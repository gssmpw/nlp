%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Materials}

\begin{figure*}[ht]
    \begin{center}
        \begin{subfigure}[c]{0.24\textwidth}
        
        \includegraphics[width=\linewidth]{images/Moments_d=100,n=1000_without_MF_exp_0.9.pdf}
        \subcaption{
        \begin{minipage}{\linewidth}
        \vspace{-0.4cm}
        \centering $d = 100, \beta=0.9$,\\  $C_1=C_2=I$
        \end{minipage}
        }
        \end{subfigure}
        \begin{subfigure}[c]{0.24\textwidth}
        \includegraphics[width=\linewidth]{images/Moments_d=100,n=1000_with_MF_exp_0.9.pdf}
        \subcaption{
        \begin{minipage}{\linewidth}
        \vspace{-0.4cm}
        \centering $d = 100, \beta=0.9$,\\  $C_1=C_2=A_{\beta}^{1/2}$
        \end{minipage}
        }
        \end{subfigure}
        \begin{subfigure}[c]{0.24\textwidth}
        \includegraphics[width=\linewidth]{images/Moments_d=100,n=1000_without_MF_window_10.pdf}
        \subcaption{
        \begin{minipage}{\linewidth}
        \vspace{-0.4cm}
        \centering $d=100, w=10,$\\  $C_1=C_2=I$
        \end{minipage}
        }
        \end{subfigure}
        \begin{subfigure}[c]{0.24\textwidth}
        \includegraphics[width=\linewidth]{images/Moments_d=100,n=1000_with_MF_window_10.pdf}
        \subcaption{
        \begin{minipage}{\linewidth}
        \vspace{-0.4cm}
        \centering $d=100, w=10,$\\  $C_1=C_2=A_w^{1/2}$
        \end{minipage}
        }
        \end{subfigure}
    \caption{Expected error of second moment estimation with JME versus PP under different workload settings. The scenarios include exponential decay ($\beta=0.9$) and sliding window ($w=10$) workloads and $d=100, n=1000$, both trivial and square root matrix factorization. 
    %
    In line with our analysis, incorporating matrix factorization significantly improves the quality of both methods, particularly in the high privacy regime.}
    \label{fig:moments_exp_window}
    \end{center}
\end{figure*}

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llcccccccccc}
\toprule
 & \textbf{Method} & \textbf{Epoch 1} & \textbf{Epoch 2} & \textbf{Epoch 3} & \textbf{Epoch 4} & \textbf{Epoch 5} & \textbf{Epoch 6} & \textbf{Epoch 7} & \textbf{Epoch 8} & \textbf{Epoch 9} & \textbf{Epoch 10} \\
\midrule
\multirow{4}{*}{$\varepsilon \approx 0.16$} 
& DP-Adam-JME     & 12.43 $\pm$ 3.95 & 19.18 $\pm$ 2.27 & 23.29 $\pm$ 1.25 & 25.83 $\pm$ 0.48 & 27.25 $\pm$ 0.20 & 27.81 $\pm$ 0.14 & 28.50 $\pm$ 0.40 & 28.89 $\pm$ 0.54 & 29.61 $\pm$ 0.51 & 30.38 $\pm$ 0.62 \\
& DP-Adam-Clip    & 13.54 $\pm$ 3.31 & 19.68 $\pm$ 2.76 & 24.92 $\pm$ 1.48 & 26.24 $\pm$ 1.04 & 26.60 $\pm$ 0.25 & 27.27 $\pm$ 0.55 & 27.56 $\pm$ 0.46 & 27.98 $\pm$ 0.26 & 27.78 $\pm$ 0.17 & 28.14 $\pm$ 0.33 \\
& DP-Adam-Debiased & 12.41 $\pm$ 4.14 & 16.91 $\pm$ 1.36 & 23.78 $\pm$ 0.70 & 24.92 $\pm$ 0.55 & 26.20 $\pm$ 0.65 & 26.96 $\pm$ 1.19 & 27.46 $\pm$ 0.90 & 28.14 $\pm$ 0.72 & 28.55 $\pm$ 0.92 & 29.14 $\pm$ 0.60 \\
& DP-Adam         & 10.29 $\pm$ 0.51 & 10.32 $\pm$ 0.56 & 10.32 $\pm$ 0.56 & 10.34 $\pm$ 0.58 & 10.39 $\pm$ 0.67 & 10.41 $\pm$ 0.70 & 10.42 $\pm$ 0.73 & 10.45 $\pm$ 0.79 & 10.45 $\pm$ 0.78 & 10.48 $\pm$ 0.83 \\
\midrule
\multirow{4}{*}{$\varepsilon \approx 1.7$} 
& DP-Adam-JME     & 32.64 $\pm$ 0.71 & 39.19 $\pm$ 1.52 & 42.28 $\pm$ 0.48 & 44.44 $\pm$ 0.57 & 45.12 $\pm$ 0.81 & 45.61 $\pm$ 0.73 & 46.91 $\pm$ 1.21 & 47.04 $\pm$ 0.98 & 47.24 $\pm$ 0.79 & 47.94 $\pm$ 0.85 \\
& DP-Adam-Clip    & 30.51 $\pm$ 0.72 & 38.03 $\pm$ 1.00 & 40.66 $\pm$ 0.75 & 42.88 $\pm$ 0.46 & 43.30 $\pm$ 0.71 & 43.52 $\pm$ 0.50 & 44.51 $\pm$ 0.70 & 44.45 $\pm$ 0.24 & 44.76 $\pm$ 0.22 & 44.34 $\pm$ 0.28 \\
& DP-Adam-Debiased & 34.21 $\pm$ 0.95 & 41.38 $\pm$ 1.54 & 45.20 $\pm$ 0.44 & 46.91 $\pm$ 1.25 & 48.06 $\pm$ 0.36 & 48.38 $\pm$ 0.83 & 48.26 $\pm$ 1.05 & 48.38 $\pm$ 1.21 & 47.81 $\pm$ 0.73 & 48.53 $\pm$ 0.99 \\
& DP-Adam         & 26.58 $\pm$ 0.43 & 31.45 $\pm$ 0.43 & 35.02 $\pm$ 1.16 & 36.39 $\pm$ 0.58 & 38.15 $\pm$ 0.95 & 39.31 $\pm$ 0.52 & 40.33 $\pm$ 0.33 & 41.47 $\pm$ 1.05 & 42.42 $\pm$ 0.94 & 43.07 $\pm$ 0.86 \\
\bottomrule
\end{tabular}
}
\caption{CIFAR-10 experiments with two different privacy budgets, $\varepsilon \approx 1.7$ and $\varepsilon \approx 0.16$ for $\delta = 10^{-6}$,  for four methods: DP-Adam with and without debiasing, JME, and Joint Clipping. The average and standard deviation errors are based on 3 runs.}
\label{tab:accuracy_results}
\end{table}

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1} % Adjust row height
\setlength{\tabcolsep}{4pt}      % Adjust column spacing
\begin{tabular}{lcclcccc}
\toprule
 & \textbf{Noise Mult} & \textbf{Batch Size} & \textbf{Method} & \textbf{lr}   & \textbf{Scaling ($\lambda, \tau)$} & \textbf{eps} & \textbf{Clip Norm} \\ 
\midrule
\multirow{4}{*}{\(\varepsilon \approx 1.7\)} & \multirow{4}{*}{1} & \multirow{4}{*}{256} 
& DP-Adam-JME      & $10^{-3}$ & 1  & $10^{-6}$ & 1 \\ 
& & & DP-Adam-Clip     & $10^{-3}$ & 0.5 & $10^{-6}$ & 1 \\ 
& & & DP-Adam-Debiased & $10^{-3}$ & -     & $10^{-6}$ & 1 \\ 
& & & DP-Adam          & $10^{-3}$ & -     & $10^{-8}$ & 1 \\ 
\midrule
\multirow{4}{*}{\(\varepsilon \approx 0.16\)} & \multirow{4}{*}{2} & \multirow{4}{*}{1} 
& DP-Adam-JME      & $10^{-7}$ & 1 & $10^{-7}$ & 1 \\ 
& & & DP-Adam-Clip     & $10^{-7}$ & 0.5 & $10^{-7}$ & 1 \\ 
& & & DP-Adam-Debiased & $10^{-7}$ & -     & $10^{-7}$ & 1 \\ 
& & & DP-Adam          & $10^{-7}$ & -     & $10^{-8}$ & 1 \\ 
\bottomrule
\end{tabular}
\caption{Hyperparameters for CIFAR-10 Experiments. Medium-privacy experiments use a batch size of $256$, compared to $1$ in the high-privacy regime, while using a noise multiplier of $\sigma_{\varepsilon, \delta} = 1$ for the medium-privacy regime and $\sigma_{\varepsilon, \delta} = 2$ for the high-privacy regime. JME and joint clipping require an additional hyperparameter—scaling—which is optimized to find the best value for those runs. We also find it helpful to clip the updates; for this, we use the same clipping norm.}
\label{tab:hyperparameters}
\end{table}




\begin{definition}[Face-Splitting Product] 
Let $A = (a_{i}^\top)_{i = 1}^{n}$ with $a_i \in \mathbb{R}^{d_1}$ and $B = (b_i^\top)_{i = 1}^{n}$ with $b_i \in \mathbb{R}^{d_2}$. 
The \emph{Face-Splitting Product} of $A$ and $B$, denoted by $A \face B$, is defined as:
\begin{equation}
    A \face B = (a_i \otimes b_i)_{i=1}^n = (\operatorname{vec}(a_ib_i^\top))_{i=1}^n,
\end{equation}
where $\otimes$ denotes the Kronecker product, and $\operatorname{vec}$ denotes vectorization.

\end{definition}
The result is a matrix of size $n \times (d_1 d_2)$, where each row corresponds to the vectorized Kronecker product of $a_i$ and $b_i$. A few properties of the Face-Splitting Product that we will use further:
\begin{itemize}
    \item  \emph{Bilinearity} $A\face (B+C)=A\face B+A\face C$ and $(A+B)\face C=A\face C+B\face C)$.
    \item \emph{Associativity} $(A\face B)\face C=A\face (B\face C)$.
    \item \emph{Frobenious norm} $\|A\face B\|_{\Fr}=\|B\face A\|_{\Fr}$, even so $A \face B \ne B \face A$.
\end{itemize}



\section{Technical Proofs}

\subsection{Bounds on the expected approximation error for PP with non-trivial factorization}\label{sec:supX_upperlowerbound}
\begin{lemma}\label{lem:supX_upperlowerbound}
\begin{align}
\|A_2 C_1^{-1}\|^2_{\Fr}
& \leq \sup_{X\in\mathcal{X}} \tr\big((A_2^\top A_2 \circ C_1^{-1}C_1^{-\top})XX^\top\big)
 \leq \sum_{ij} \big|\,[A_2^\top A_2 \circ C_1^{-1}C_1^{-\top}]_{i,j}\,\big|
\end{align}
\end{lemma}

\begin{proof}[Proof of \Cref{lem:supX_upperlowerbound}]
The proof is elementary. For the lower bound. consider the 
specific choice for $X=(x_1,\dots,x_n)$ with all rows identical 
with $\|x_i\|=1$, such that $XX^\top$ is the constant matrix 
of all $1$s. 
%
For the upper bound, we observe that $[XX^\top]_{i,j}\in[-1,1]$, so 
\begin{align}
\tr\big((A_2^\top A_2 \circ C_1^{-1}C_1^{-\top})XX^\top\big)
&=\sum_{i,j} [A_2^\top A_2 \circ C_1^{-1}C_1^{-\top}]_{i,j}[XX^\top]_{i,j}
\leq \sum_{i,j} \big|\,[A_2^\top A_2 \circ C_1^{-1}C_1^{-\top}]_{i,j}\,\big|.
\end{align}
\end{proof}


\theoremforkequalone*

\begin{proof}

We recall that the function $r_1(\lambda)$ is defined as

\begin{equation}
    r_1(\lambda) = \sup_{|x|, |y| \le 1} [(x - y)^2 + \lambda (x^2 - y^2)^2].
\end{equation}
Given the difference it is an increasing function of $x + y$ therefore without any loss of generality we can assume $y = 1$. Then the derivative of this expression would give us

\begin{equation}
    2(x - 1)(1 + 2\lambda (x^2 + x))
\end{equation}

The optimal value will depend on the roots of the quadratic polynomial. The determinant is $4\lambda^2 - 8\lambda$ therefore for $\lambda < 2$ there are no roots, the function is decreasing on the segment $[-1, 1]$ reaching its maximal value at $x = -1$ with the value $4$. Otherwise we have the following roots $\frac{-1 \pm \sqrt{1 - 2/\lambda}}{2}$. We observe that for large $\lambda \gg 1$ we have the optimal $x = 0$ which corresponds to the maximal value for the second term. The maximum could be in both $x = -1$ and $x = \frac{-1 + \sqrt{1 - 2/\lambda}}{2} $ therefore we will need to compare them. By substituting it into the expression we get $\frac{1}{8} (\tau - 3)^2 (\lambda \tau + 1 + \lambda)$, where $\tau = \sqrt{1 - 2 /\lambda}$. We should compare this expression with $4$ to find a boundary when $x = -1$ is an optimal solution. 

\begin{equation}
\frac{1}{8} (\tau - 3)^2 (\lambda \tau + 1 + \lambda) \le 4
\end{equation}
We can express $1 / \lambda = \frac{1 - \tau^2}{2}$. Therefore we get the following inequality of variable $0 \le \tau < 1$:

\begin{align}
\begin{split}
    \frac{1}{8}(\tau - 3)^2 (\tau + \frac{1 - \tau^2}{2} + 1) - 4 \frac{1 - \tau^2}{2}
     &= \frac{1}{16}(3 - \tau)^3(1 + \tau) - 2 (1 - \tau) (1 + \tau)\\
    \quad &= \frac{1}{16}(1 + \tau)\left[27 -27\tau + 9\tau^2 - \tau^3 - 32 + 32\tau\right]\\
    \quad &= -\frac{1}{16}(1 + \tau)^2(\tau^2 - 10\tau + 5)  \le 0
\end{split}
\end{align}
Which is less than $0$ for $\tau \le 5 - 2\sqrt{5} \Rightarrow \lambda \le \frac{11 + 5\sqrt{5}}{8}  \approx 2.77$ which concludes the proof.
\end{proof}

\theoremcovariancematrixsensitivity*
\begin{proof}

First, we decompose the Kronecker product as follows:
\begin{equation}
\begin{aligned}
    \|x \otimes x - y \otimes y\|_{\Fr}^2 &= \|xx^T - yy^T\|_{\Fr}^2 = \tr(xx^Txx^T) - 2\tr(xx^Tyy^T) + \tr(yy^Tyy^T)\\
    &= \|x\|_2^4 + \|y\|_2^4 - 2\langle x, y \rangle^2.
\end{aligned}
\end{equation}

Similarly, the squared norm of the difference is

\begin{equation}
    \|x - y\|_2^2 = \|x\|_2^2 + \|y\|_2^2 - 2\langle x, y\rangle.
\end{equation}

Therefore, the objective function becomes

\begin{equation}
    \|x - y\|^2 + \lambda \|x \otimes x - y \otimes y\|_{\Fr}^2 = \|x\|_2^2 + \|y\|_2^2 + \lambda \|x\|_2^4 + \lambda \|y\|_2^4 - 2\langle x, y\rangle - 2\lambda \langle x, y\rangle^2.
\end{equation}

The first four terms are maximized when $\|x\| = \|y\| = 1$, yielding $2 + 2\lambda$. The last two terms can then be optimized independently over the scalar product $\langle x, y \rangle$ in dimension $d > 1$.

Let $\beta = \langle x, y \rangle$, where $-1 \leq \beta \leq 1$. The expression $-\beta - 2\lambda\beta^2$ is maximized at $\beta = -\frac{1}{2\lambda}$ when $\lambda \geq \frac{1}{2}$, or at $\beta = -1$ when $\lambda < \frac{1}{2}$.  If $\beta = -1$, then $x = -y$ and the objective function becomes $4$. If $\beta = -\frac{1}{2\lambda}$, the objective function is equal to $2 + 2\lambda + \frac{1}{2\lambda}$, which concludes the proof.

\end{proof}

\theoremJMEvsIME*
\begin{proof}

We begin the proof with the following observation: $\lambda$-\acronym and IME introduce \textit{additive} noise to the first and second moments. Therefore, for the Frobenius approximation error, it is sufficient to compare just the variances of the noise introduced by those methods. We use an instance of a Gaussian mechanism, and the privacy guarantees are more appropriately characterized by the notion of Gaussian privacy. For the sake of the proof, we assume that $(\epsilon, \delta)$-DP is equivalent to $\mu$-GDP for a specific choice of $\mu$. The sensitivity of $\lambda$-\acronym depends on the dimensionality. Here, we assume $d > 1$ to address the hardest case, as $d = 1$ follows trivially.


To estimate $x$ and $x \otimes x$ simultaneously in a differentially private way via composition theorem, we need to split the privacy budget between the components. Using the Gaussian mechanism, we split the budget as $\mu_1$-GDP for the first component and $\mu_2$-GDP for the second component such that $\mu_1^2 + \mu_2^2 = \mu^2$. The squared sensitivity of $x$ is $4\zeta^2$, and the squared sensitivity of $x \otimes x$ is $2\zeta^4$, assuming $\|x\|_2 \le \zeta$. Therefore, the variance of noise added to the first and second components is given by:
\begin{equation}
\left(\frac{4\zeta^2}{\mu_1^2}, \frac{2\zeta^4}{\mu^2 - \mu_1^2}\right).
\end{equation}

Our analysis for \acronym yields the pair of variances, for $\lambda\zeta^2 \geq \frac{1}{2}$:
\begin{equation}
\left(\frac{\zeta^2(2 + 2\lambda\zeta^2 + \frac{1}{2\lambda\zeta^2})}{\mu^2}, \frac{\zeta^2(2 + 2\lambda\zeta^2 + \frac{1}{2\lambda\zeta^2})}{\lambda\mu^2}\right).
\end{equation}

We aim to show that for a given variance in the first component, our variance for the second component is smaller. Specifically, we need to prove:
\begin{equation}
\frac{\zeta^2(2 + 2\lambda\zeta^2 + \frac{1}{2\lambda\zeta^2})}{\lambda\mu^2} < \frac{2\zeta^4}{\mu^2 - \mu_1^2}, \quad \text{where} \quad \frac{4\zeta^2}{\mu_1^2} = \frac{\zeta^2(2 + 2\lambda\zeta^2 + \frac{1}{2\lambda\zeta^2})}{\mu^2}.
\end{equation}

By substituting $\frac{\mu^2}{\mu_1^2} = \frac{1}{2} + \frac{\lambda\zeta^2}{2} + \frac{1}{8\lambda\zeta^2}$ into the inequality, we obtain:
\begin{equation}
\frac{2\lambda\zeta^2 }{2 + 2\lambda\zeta^2 + \frac{1}{2\lambda\zeta^2}} > 1 - \frac{1}{\frac{1}{2} + \frac{\lambda\zeta^2}{2} + \frac{1}{8\lambda\zeta^2}}.
\end{equation}

Multiplying both sides by the denominator yields:
\begin{equation}
2\lambda\zeta^2 > 2 + 2\lambda\zeta^2 + \frac{1}{2\lambda\zeta^2} - 4.
\end{equation}

Simplifying, we find $\lambda\zeta^2 > \frac{1}{4}$,
which is satisfied by the initial assumption.
\end{proof}

\lemmaclippingvsjointmomentestimation*
\begin{proof}

We begin the proof with the following observation: $\lambda$-\acronym and CS introduce \textit{additive} noise to the first and second moments. Therefore, for the Frobenius approximation error, it is sufficient to compare just the variances of the noise introduced by those methods. We use an instance of a Gaussian mechanism, and the privacy guarantees are more appropriately characterized by the notion of Gaussian privacy. For the sake of the proof, we assume that $(\epsilon, \delta)$-DP is equivalent to $\mu$-GDP for a specific choice of $\mu$. The sensitivity of $\lambda$-\acronym depends on the dimensionality. Here, we assume $d > 1$ to address the hardest case, as $d = 1$ follows trivially.

We aim to show that Joint Moment Estimation (JME) introduces less noise to the $(x \otimes x)$ component than CS under the same privacy budget $\mu$-GDP. Specifically, we compare the variances in the second coordinate under the assumption that the noise variances in the first coordinate are equal.

The combined vector of $x_i$ and $x_i \otimes x_i$ can be represented as:
\begin{equation}
(x_i, \sqrt{\tau} x_i \otimes x_i),
\end{equation}
where $\tau > 0$ is a scaling parameter. If the input vectors $x_i$ are bounded by $\|x_i\|_2 \leq \zeta$, the resulting vector will have $l_2$ norm bounded by $\sqrt{\zeta^2 + \tau \zeta^4}$.
The squared sensitivity of the vector is therefore:
$4\zeta^2(1 + \tau \zeta^2)$.
This results in the following variances for the first and second coordinates under noise addition:
\begin{equation}
\left( \frac{4\zeta^2(1 + \tau \zeta^2)}{\mu^2}, \frac{4\zeta^2(1 + \tau \zeta^2)}{\tau \mu^2} \right).
\end{equation}

For \acronym, we analyze the variances and obtain the pair of variances as:
\begin{equation}
\left( \frac{\zeta^2(2 + 2\zeta^2\lambda + \frac{1}{2\zeta^2\lambda})}{\mu^2}, \frac{\zeta^2(2 + 2\zeta^2\lambda + \frac{1}{2\zeta^2\lambda})}{\lambda \mu^2} \right),
\end{equation}
where we assume \(\zeta^2 \lambda \geq \frac{1}{2}\).

To compare the noise introduced to the second component, we aim to show:
\begin{equation}
\frac{\zeta^2(2 + 2\zeta^2\lambda + \frac{1}{2\zeta^2\lambda})}{\lambda \mu^2} < \frac{4\zeta^2(1 + \tau \zeta^2)}{\tau \mu^2},
\end{equation}
under the condition that the variances in the first component are equal:
\begin{equation}
\frac{\zeta^2(2 + 2\zeta^2\lambda + \frac{1}{2\zeta^2\lambda})}{\mu^2} = \frac{4\zeta^2(1 + \tau \zeta^2)}{\mu^2}.
\end{equation}

Given the equality, inequality is equivalent to $\lambda > \tau$. Simplifying the equality in the first component gives:
\begin{equation}
2 + 2\zeta^2\lambda + \frac{1}{2\zeta^2\lambda} = 4 + 4\tau \zeta^2.
\end{equation}

Rearranging terms, we isolate \(\frac{1}{2\zeta^2\lambda}\):
\begin{equation}
\frac{1}{2\zeta^2\lambda} = 2 + 2(2\tau - \lambda)\zeta^2.
\end{equation}

From the assumption \(\zeta^2\lambda \geq \frac{1}{2}\), we know that \(\frac{1}{2\zeta^2\lambda} < 1\). For this inequality to hold, the term \(2 + 2(2\tau - \lambda)\zeta^2\) must also be less than 1. Therefore, \(2\tau - \lambda < 0\), therefore $\lambda > \tau$.

Thus, we conclude that for the same variance in the first component, JME introduces less noise variance to the second component compared to CS.
\end{proof}


\lemmaexpectedsecondmomentPostPr*
\begin{proof}
We aim to evaluate the expected squared Frobenius norm of the error:
\begin{equation*}
\begin{aligned}
&\sup_{X \in \mathcal{X}}\mathbb{E}\| A_2 ((X + C_1^{-1}Z_1) \face (X + C_1^{-1}Z_1)) - A_2 (X \face X) \|_{\Fr}^2\\
&\quad =2 \underbrace{\sup_{X \in \mathcal{X}}\mathbb{E} \|A_2 (X \face C_1^{-1} Z_1)\|_{\Fr}^2}_{\mathcal S_1} + 2 \underbrace{\sup_{X \in \mathcal{X}}\mathbb{E}\langle A_2 (X \face C_1^{-1} Z_1), A_2 ( C_1^{-1} Z_1 \face X) \rangle_{\Fr}}_{\mathcal S_2}  + \underbrace{\mathbb{E} \|A_2 ((C_1^{-1} Z_1) \face (C_1^{-1} Z_1))\|_{\Fr}^2}_{\mathcal S_3}.
\end{aligned}
\end{equation*}

We compute those terms separately.

\paragraph{Step 1. Bound $\mathcal S_1$.} 
\begin{align}
\label{eq:S_1postprocessing}
\begin{split}
\mathcal S_1 &= \sup_{X \in \mathcal{X}}\mathbb{E} \|A_2 (X \face C_1^{-1} Z_1)\|_{\Fr}^2 \\
    &=  \sup_{X \in \mathcal{X}}\mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{i, j}^{d} \left(\sum\limits_{t = 1}^{n} (A_2)_{k, t} X_{t, i} \sum\limits_{r = 1}^{n} (C_1^{-1})_{t, r} (Z_1)_{r, j}\right)^2\\
    &=  \sup_{X \in \mathcal{X}} \sum\limits_{k = 1}^{n}\sum\limits_{i, j}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_1} X_{t_1, i}X_{t_2, i} \sum\limits_{r = 1}^{n} (C_1^{-1})_{t_1, r}(C_1^{-1})_{t_2, r}  \mathbb{E}(Z_1)_{r, j}^2\\
    &= \sigma^2 \|C_1\|_{1\to2}^2\sup_{X \in \mathcal{X}} \sum\limits_{k = 1}^{n}\sum\limits_{i,j}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_2} X_{t_1, i}X_{t_2, i} \sum\limits_{r = 1}^{n} (C_1^{-1})_{t_1, r}(C_1^{-1})_{t_2, r}\\
    &= d\sigma^2 \|C_1\|_{1\to2}^2\sup_{X \in \mathcal{X}} \sum\limits_{t_1, t_2}^{n} \langle(A_2^\top)_{t_1}, (A_2^\top)_{t_2}\rangle \langle X_{t_1}, X_{t_2}\rangle  \langle(C_1^{-1})_{t_1}, (C_1^{-1})_{t_2} \rangle\\
    &= d\sigma^2 \|C_1\|_{1\to2}^2\sup_{X \in \mathcal{X}} \tr((A_2^TA_2 \circ Q)XX^T), 
\end{split}
\end{align}
where $Q = (C_1C_1^{T})^{-1}$. 

\paragraph{Step 2. Bound $\mathcal S_2$.} 
\begin{align}
\label{eq:S_2postprocessing}
\begin{split}
\mathcal S_2 &= \sup_{X \in \mathcal{X}}\mathbb{E}\langle A_2 (X \face C_1^{-1} Z_1), A_2 ( C_1^{-1} Z_1 \face X) \rangle_{\Fr} \\
&=  \sup_{X \in \mathcal{X}}\mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{i, j}^{d} \left(\sum\limits_{t = 1}^{n} (A_2)_{k, t} X_{t, i} (C_1^{-1}Z_1)_{t, j} \right)\left(\sum\limits_{t = 1}^{n} (A_2)_{k, t} X_{t, j} (C_1^{-1}Z_1)_{t, i} \right)\\
&=  \sup_{X \in \mathcal{X}}\mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{i, j}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1} X_{t_1, i} (C_1^{-1}Z_1)_{t_1, j}  (A_2)_{k, t_2} X_{t_2, j} (C_1^{-1}Z_1)_{t_2, i} \\
&= \sup_{X \in \mathcal{X}}\mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1} X_{t_1, j} (C_1^{-1}Z_1)_{t_1, j}  (A_2)_{k, t_2} X_{t_2, j} (C_1^{-1}Z_1)_{t_2, j} \\
&=\frac{1}{d}\sup_{X \in \mathcal{X}}\mathbb{E} \|A_2 (X \face C_1^{-1} Z_1)\|_{\Fr}^2 = \frac{S_1}{d}
\end{split}
\end{align}

\textbf{Step 3. Bounding $\mathcal S_3$}
We expand this term:
\begin{align}
\label{eq:S_3postprocessing}
\begin{split}
    \mathcal S_3 &= \mathbb{E} \|A_2 ((C_1^{-1} Z_1) \face (C_1^{-1} Z_1))\|_{\Fr}^2 \\
    &= \mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{i,j}^{d} \left(\sum\limits_{t = 1}^{n}(A_2)_{k, t} (C_1^{-1}Z_1)_{t, i}(C_1^{-1}Z_1)_{t, j}\right)^2\\
    &= \mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{i,j}^{d} \sum\limits_{t_1, t_2}^{n}(A_2)_{k, t_1}(A_2)_{k, t_2} (C_1^{-1}Z_1)_{t_1, i}(C_1^{-1}Z_1)_{t_1, j}(C_1^{-1}Z_1)_{t_2, i}(C_1^{-1}Z_1)_{t_2, j} \\
    &= \mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{t_1, t_2}^{n}\sum\limits_{i,j}^{d} 
    (A_2)_{k, t_1}(A_2)_{k, t_2} (C_1^{-1}Z_1)_{t_1, i}(C_1^{-1}Z_1)_{t_1, j}(C_1^{-1}Z_1)_{t_2, i}(C_1^{-1}Z_1)_{t_2, j}\\
    &= \mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_2} \sum\limits_{i,j}^{d} 
     (C_1^{-1}Z_1)_{t_1, i}(C_1^{-1}Z_1)_{t_1, j}(C_1^{-1}Z_1)_{t_2, i}(C_1^{-1}Z_1)_{t_2, j} \\
     &=  \sum\limits_{k = 1}^{n}\sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_2} \sum\limits_{i,j}^{d} \mathbb{E}  
     (C_1^{-1}Z_1)_{t_1, i}(C_1^{-1}Z_1)_{t_1, j}(C_1^{-1}Z_1)_{t_2, i}(C_1^{-1}Z_1)_{t_2, j}.
\end{split}
\end{align}

We now deal with two cases. When $i = j$, we compute:
\begin{align}
\begin{split}
\mathbb{E}(C_1^{-1}Z_1)^2_{t_1, j}(C_1^{-1}Z_1)^2_{t_2, j} 
&=\mathbb{E}\sum\limits_{r = 1}^{n} (C_1^{-1})^2_{t_1, r}(C_1^{-1})^2_{t_2, r}(Z_1)_{r, j}^4 + \mathbb{E}\sum\limits_{r_1\ne r_2}^{n} (C_1^{-1})^2_{t_1, r_1}(C_1^{-1})^2_{t_2, r_2}(Z_1)_{r_1, j}^2(Z_1)_{r_2, j}^2\\
&\quad + 2\mathbb{E}\sum\limits_{r_1\ne r_2}^{n} (C_1^{-1})_{t_1, r_1}(C_1^{-1})_{t_1, r_2}(C_1^{-1})_{t_2, r_1}(C_1^{-1})_{t_2, r_2}(Z_1)_{r_1, j}^2(Z_1)_{r_2, j}^2\\
&= 3\sigma^4 \|C_1\|_{1 \to 2}^4 \sum\limits_{r = 1}^{n} (C_1^{-1})^2_{t_1, r}(C_1^{-1})^2_{t_2, r} + \sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{r_1 \ne r_2}^{n} (C_1^{-1})^2_{t_1, r_1}(C_1^{-1})^2_{t_2, r_2}\\
&\quad +2 \sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{r_1\ne r_2}^{n} (C_1^{-1})_{t_1, r_1}(C_1^{-1})_{t_1, r_2}(C_1^{-1})_{t_2, r_1}(C_1^{-1})_{t_2, r_2}\\
&= \sigma^4 \|C_1\|_{1 \to 2}^4Q_{t_1, t_1} Q_{t_2, t_2} +2 \sigma^4 \|C_1\|_{1 \to 2}^4Q_{t_1, t_2}^2.
\end{split}
\label{eq:caseieqj}
\end{align}

On the other hand, when $i \ne j$, then 
\begin{align}
\begin{split}    
\mathbb{E}(C_1^{-1}Z_1)_{t_1, i}(C_1^{-1}Z_1)_{t_1, j}(C_1^{-1}Z_1)_{t_2, i}(C_1^{-1}Z_1)_{t_2, j} &= \sigma^4 \|C_1\|_{1 \to 2}^4 \left(\sum\limits_{r = 1}^{n}(C_1^{-1})_{t_1, r}(C_1^{-1})_{t_2, r}\right)^2 = \sigma^4 \|C_1\|_{1 \to 2}^4 Q_{t_1, t_2}^2.
\end{split}
\label{eq:caseineqj}
\end{align}


Using equation~\eqref{eq:caseieqj} and equation~\eqref{eq:caseineqj} in equation~\eqref{eq:S_3postprocessing}, we get 
\begin{align*}
     \mathcal S_3 &= \mathbb{E} \|A_2 ((C_1^{-1} Z_1) \face (C_1^{-1} Z_1))\|_{\Fr}^2 = d\sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{k = 1}^{n} \sum\limits_{t_1, t_2}^{n}(A_2)_{k, t_1}(A_2)_{k, t_2} (Q_{t_1,t_1}Q_{t_2, t_2} + (d + 1)Q_{t_1, t_2}^2)\\
     &= d\sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{k = 1}^{n} \sum\limits_{t_1, t_2}^{n}(A_2)_{k, t_1}(A_2)_{k, t_2} (Q_{t_1,t_1}Q_{t_2, t_2} + (d + 1)Q_{t_1, t_2}^2)\\
     &= d\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2E_Q) +d(d + 1)\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2(Q\circ Q)),
\end{align*}
where $E_Q = \diag(Q)\diag^{\top}(Q)$. 

Adding equation~\eqref{eq:S_1postprocessing} to equation~\eqref{eq:S_3postprocessing}, we obtain:
\begin{equation*}
\begin{aligned}
\sup_{X \in \mathcal{X}}\mathbb{E}\|S - \widehat{S}_{\text{PP}}\|^2_{\Fr} &= d(d + 1)\sigma^4 \|C_1\|_{1 \to 2}^4 \cdot \tr(A_2^\top A_2 (Q \circ Q)) + 2(d + 1)\sigma^2 \|C_1\|_{1 \to 2}^2 \cdot \sup_{X \in \mathcal{X}} \tr((A_2^TA_2 \circ Q)XX^T)\\
&\quad+d\sigma^4 \|C_1\|_{1 \to 2}^4 \cdot \tr(A_2^\top A_2 E_Q)
\end{aligned}
\end{equation*}

\textbf{Bias Correction.}

The expectation of $A_2 ((C_1^{-1}Z_1) \face (C_1^{-1}Z_1))]_{k, i, j}$ introduces a bias:
\begin{equation}
\begin{aligned}
\label{eq:PP_bias_term}
[\mathbb{E} A_2 ((C_1^{-1}Z_1) \face (C_1^{-1}Z_1))]_{k, i, j} &= \mathbb{E}\sum\limits_{t = 1}^{n} (A_2)_{k, t} \left(\sum\limits_{r =1}^{n}(C_1^{-1})_{t, r} (Z_1)_{r, i}\right)\left(\sum\limits_{r =1}^{n}(C_1^{-1})_{t, r} (Z_1)_{r, j}\right)\\
&= \sigma^2\delta_{i,j}\|C_1\|_{1\to2}^2\sum\limits_{t = 1}^{n}\sum\limits_{r =1}^{n} (A_2)_{k, t} (C_1^{-1})^2_{t, r}=\sigma^2\delta_{i,j}\|C_1\|_{1\to2}^2\sum\limits_{t = 1}^{n} (A_2)_{k, t} Q_{t, t}.
\end{aligned}
\end{equation}
%
The Frobenius norm of this bias is:
\begin{align*}
\|\mathbb{E}A_2 ((C_1^{-1}Z_1) \face (C_1^{-1}Z_1))\|_{\Fr}^2 &= \sigma^4\|C_1\|_{1\to2}^4 \sum\limits_{k = 1}^{n} \sum\limits_{j = 1}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_2} Q_{t_1, t_1}Q_{t_2, t_2}\\
&= d\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2E_Q)
\end{align*}

If we subtract this bias from the estimate, it will increase the error by the aforementioned quantity due to the Frobenius norm of the bias but will decrease the error by two scalar products with the $A_2 ((C_1^{-1} Z_1) \face (C_1^{-1} Z_1))$ term:

\begin{equation*}
    \mathbb{E}\langle A_2 ((C_1^{-1}Z_1) \face (C_1^{-1}Z_1)), \mathbb{E}A_2 ((C_1^{-1}Z_1) \face (C_1^{-1}Z_1)) \rangle = \|\mathbb{E}A_2 ((C_1^{-1}Z_1) \face (C_1^{-1}Z_1))\|_{\Fr}^2.
\end{equation*}

Thus, we can eliminate the last term ($d\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2E_Q)$) in the error sum via bias correction.
\end{proof}

We collect some useful proposition that would be useful for our analysis.

\begin{proposition}
\label{prop:normofVZ}
    Let $V$ and $X$ be a given fixed matrix and $Z$ be a Gaussian matrix of appropriate dimension. Then 
    \[
    \mathbb{E}_{Z}\|VZ \face VZ\|_{\Fr}^2 = d(d+2)\sigma^4 \sum\limits_{k = 1}^{n}  \left(\sum\limits_{t = 1}^{n} V_{k, t}^2\right)^2.
    \]    
\end{proposition}
\begin{proof}
Recalling the Face-Splitting product, we have 
\begin{equation}
\begin{aligned}
    \mathbb{E}_{Z}\|VZ \face VZ\|_{\Fr}^2 &= \mathbb{E}_{Z}\sum\limits_{k = 1}^{n} \sum\limits_{i,j}^{d} (VZ)^2_{k,i}(VZ)_{k,j}^2 
    = \mathbb{E}_{Z}\sum\limits_{k = 1}^{n} \sum\limits_{i, j}^{d} \left(\sum\limits_{t = 1}^{n}V_{k,t}Z_{t, i}\right)^2\left(\sum\limits_{t = 1}^{n}V_{k,t}Z_{t, j}\right)^2\\
    &= 3\sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{i = 1}^{d}\sum\limits_{t = 1}^{n} V_{k, t}^4 + \sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{i \ne j}^{d} \left(\sum\limits_{t = 1}^{n} V_{k, t}^2\right)^2 +  3\sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{i =1}^{d} \sum\limits_{j \ne \ell}^{n} V_{k, j}^2V_{k, \ell}^2\\
    &= d(d+2)\sigma^4 \sum\limits_{k = 1}^{n}  \left(\sum\limits_{t = 1}^{n} V_{k, t}^2\right)^2,
\end{aligned}
\end{equation}
This completes the proof of the proposition.
\end{proof}

\begin{proposition}
\label{prop:normofVZVX}
    Let $V$ and $X$ be a given fixed matrix and $Z$ be a Gaussian matrix of appropriate dimension. Then 
    \[
    \mathbb{E}_{Z} \langle(VZ) \face (VX), (VX) \face (VZ)\rangle = \frac{1}{d}\mathbb{E}_{Z}\|VZ_1 \face VX\|_{\Fr}^2.
    \]
\end{proposition}
\begin{proof}
The result follows using the following calculation:
\begin{align*}
\mathbb{E}_{Z} \langle(VZ) \face (VX), (VX) \face (VZ)\rangle 
&= \mathbb{E}_{Z}\sum\limits_{k = 1}^{n} \sum\limits_{i,j}^{d} (VZ_1)_{k,i}(VX)_{k,j}(VZ_1)_{k,j}(VX)_{k,i} 
= \mathbb{E}_{Z}\sum\limits_{k = 1}^{n} \left(\sum\limits_{i=1}^{d} (VZ_1)_{k,i}(VX)_{k,i}\right)^2\\
&=\sigma^2\sum\limits_{k = 1}^{n}  \sum\limits_{t = 1}^nV_{k,t}^2\sum\limits_{j=1}^{d}(VX)_{k,j}^2 = \frac{1}{d}\mathbb{E}_{Z}\|VZ_1 \face VX\|_{\Fr}^2
\end{align*}
completing the proof. 
\end{proof}



\theoremJMEwithbiascorrection*
\begin{proof}

We first recall that, if $Z \sim \mathcal N(\mu,\Sigma)$, then for any matrix $A$, we have $AZ \sim \mathcal N(A \mu, A \Sigma A^\top)$. This implies that, when $\Sigma=\mathbb I$ and $\mu=0$, we have 
\[
\mathbb{E}_{Z}\|AZ\|_{\Fr}^2 = \|A\|_{\Fr}^2.
\]
Recall that $\widehat{\mu} = V(X + Z_1)$ is a running mean and $\widehat{\Sigma} = V(X \face X + \lambda^{-1/2} Z_2) - (V(X + Z_1) \face V(X + Z_1))$ is a running covariance matrix, with independent noise $Z_1, Z_2 \in \mathcal{N}(0, \sigma^2)^{n \times d^2}$, the clipping norm $\zeta=1$, and $\lambda = \lambda^{*} = c_d^{-1}$ as defined in equation~\eqref{def:crit_lambda}. Using the associativity of Face-splitting product and the Pythogorean theorem, we have  
\begin{equation*}
\begin{aligned}
&\mathbb{E}_{Z_1, Z_2} \|V(X \face X + \lambda^{-1/2}Z_2) - (V(X+Z_1) \face V(X + Z_1)) - V(X\face X) + (VX)\face(VX)\|_{\Fr}^2\\
&\quad =\mathbb{E}_{Z_1, Z_2} \|\lambda^{-1/2}VZ_2 - (VZ_1) \face (VX) - (VX) \face (VZ_1) - (VZ_1) \face (VZ_1)) \|_{\Fr}^2\\
&\quad =\mathbb{E}_{Z_2}\|\lambda^{-1/2}VZ_2\|_{\Fr}^2 + 2\mathbb{E}_{Z_1}\| (VZ_1) \face (VX) \|_{\Fr}^2 + \mathbb{E}_{Z_1}\|(VZ_1) \face (VZ_1) \|_{\Fr}^2 + 2\mathbb{E}_{Z_1} \langle(VZ_1) \face (VX), (VX) \face (VZ_1)\rangle\\
&\quad = c_d\sigma^2 d^2 \|V\|_{\Fr}^2 + 2{\mathbb{E}_{Z_1}\| (VZ_1) \face (VX) \|_{\Fr}^2} + {\mathbb{E}_{Z_1}\|(VZ_1) \face (VZ_1) \|_{\Fr}^2} + 2{\mathbb{E}_{Z_1} \langle(VZ_1) \face (VX), (VX) \face (VZ_1)\rangle} \\
\end{aligned}
\end{equation*}

Using Proposition \ref{prop:normofVZ} and Proposition \ref{prop:normofVZVX}, we therefore have
\begin{align}
\begin{split}
\label{eq:simplying}
&\mathbb{E}_{Z_1, Z_2} \|V(X \face X + \lambda^{-1/2}Z_2) - (V(X+Z_1) \face V(X + Z_1)) - V(X\face X) + (VX)\face(VX)\|_{\Fr}^2 \\
&\quad =  c_d\sigma^2 d^2 \|V\|_{\Fr}^2 + 2 \left(1 + {1\over d} \right) {\mathbb{E}_{Z_1}\| (VZ_1) \face (VX) \|_{\Fr}^2}  +  d(d+2)\sigma^4 \sum\limits_{k = 1}^{n}  \left(\sum\limits_{t = 1}^{n} V_{k, t}^2\right)^2.  
\end{split}
\end{align}

Now using the properties of $V$, we have 
\begin{align}
\begin{split}
\label{eq:evaluatingV}    
\|V\|_{\Fr}^2 = \sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{n}V_{k,t}^2  = \sum\limits_{k = 1}^{n}\frac{1}{k} = H_{n,1} \quad \text{and} \quad 
\sum\limits_{k = 1}^{n}  \left(\sum\limits_{t = 1}^{n} V_{k, t}^2\right)^2 =  \sum\limits_{k = 1}^{n} \frac{1}{k^2} = H_{n,2}.
\end{split}
\end{align}

Using equation~(\ref{eq:evaluatingV}) in equation~(\ref{eq:simplying}), we get 
\begin{align}
\begin{split}
\label{eq:simplified}
&\mathbb{E}_{Z_1, Z_2} \|V(X \face X + \lambda^{-1/2}Z_2) - (V(X+Z_1) \face V(X + Z_1)) - V(X\face X) + (VX)\face(VX)\|_{\Fr}^2 \\
&\quad =   2 \left(1 + {1\over d} \right) {\mathbb{E}_{Z_1}\| (VZ_1) \face (VX) \|_{\Fr}^2} +  d(d+2)\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k^2} + 2\sigma^2 d^2\sum\limits_{k = 1}^{n}\frac{1}{k} \\
&\quad =   2 \left(1 + {1\over d} \right) {\mathbb{E}_{Z_1}\| (VZ_1) \face (VX) \|_{\Fr}^2} +  d(d+2)\sigma^4 H_{n,2} + 2\sigma^2 d^2H_{n,1}.    
\end{split}
\end{align}

Let $H_{n,c}$ denote the generalized Harmonic sum, i.e., $H_{n,c} = \sum_{i=1}^n i^{-c}$.  
Therefore, we have  
\begin{align}
\begin{split}
\label{eq:simplifiedend}
&\mathbb{E}_{Z_1, Z_2} \|V(X \face X + \lambda^{-1/2}Z_2) - (V(X+Z_1) \face V(X + Z_1)) - V(X\face X) + (VX)\face(VX)\|_{\Fr}^2 \\
&\quad =   2 \left(1 + {1\over d} \right) \underbrace{\mathbb{E}_{Z_1}\| (VZ_1) \face (VX) \|_{\Fr}^2}_{S(X)}   +  \sigma^4 d(d+2) H_{n,2} + c_d\sigma^2 d^2H_{n,1}.  
\end{split}
\end{align}

Therefore to estimate $\sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}\|_{\Fr}^2$, it suffices to estimate $\sup_{X \in \mathcal{X}} S(X)$. We do it as follows:

\begin{equation}
\label{eq:supS(X)}
\begin{aligned}
    \sup_{X \in \mathcal{X}} S(X) &= \sup_{X \in \mathcal{X}}\mathbb{E}_{Z_1}\|VZ_1 \face VX\|_{\Fr}^2 = d\sigma^2\sup_{X \in \mathcal{X}}\sum\limits_{k = 1}^{n}\frac{1}{k^3} \sum\limits_{j=1}^{d}\left(\sum\limits_{t = 1}^{k}X_{t, j}\right)^2\\
    &= d\sigma^2\sup_{X \in \mathcal{X}}\sum\limits_{k = 1}^{n}\frac{1}{k^3} \sum\limits_{t_1, t_2}^{k}\langle X_{t_1, :}, X_{t_2, :}\rangle = d\sigma^2 \sum\limits_{k = 1}^{n}\frac{1}{k} = d \sigma^2 H_{n,1},
\end{aligned}
\end{equation}

Plugging equation \eqref{eq:supS(X)} in equation (\ref{eq:simplifiedend}), we get the bound for the biased estimate:
\begin{align}
\label{eq:jme_cov_biased_error}
\sigma^2(c_d d^2 + 2d + 2)H_{n,1} + \sigma^4 d(d+2) H_{n,2}.
\end{align}

Then we need to determine the bias term, all the first-order terms will result in $0$ as the expectation is over a zero mean distribution, so the only term that introduces the bias is
\begin{equation}
\label{eq:JME_cov_bias}
    (\mathbb{E}_{Z_1} (VZ_1) \face (VZ_1))_{k, i, j} = \mathbb{E}_{Z_1}(VZ_1)_{k, i}(VZ_1)_{k, j} = \sigma^2 \delta_{i=j}\sum\limits_{t = 1}^{n}V_{k, t}^2,
\end{equation}
where $\delta_{i=j} = \begin{cases}
    1 & i=j \\
    0 & \text{otherwise}
\end{cases}$ is the Dirac-delta function. 

Then the unbiased approximation error is 
\begin{align*}
\mathbb{E}_{Z_1, Z_2} \|V(X \face X + Z_2) - (V(X+Z_1) \face V(X + Z_1)) - V(X\face X) + (VX)\face(VX) + \mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\|_{\Fr}^2 \\
\end{align*}
We have already computed the first three terms inside the expectation. We now compute the bias term error:
\begin{align*}
    \|\mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\|_{\Fr}^2 = \sigma^4\sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \left(\sum\limits_{t = 1}^{n}V_{k, t}^2\right)^2 = d\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k^2} = d \sigma^4 H_{n,2}. 
\end{align*}
Bias reduction procedure decreases the expected error of \eqref{eq:jme_cov_biased_error} by
\begin{align*}
     -2\mathbb{E}_{Z_1} \langle (VZ_1) \face (VZ_1) , \mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\rangle + \|\mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\|_{\Fr}^2 &= d\sigma^4 \sum\limits_{k = 1}^{n} \left(\sum\limits_{t = 1}^{n}V_{k, t}^2\right)^2 = d\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k^2} = d\sigma^4 H_{n,2}.
\end{align*}

Resulting in the final error of: 
\begin{align*}
    \sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}\|_{\Fr}^2 &= \sigma^2(c_d d^2 + 2d + 2)\sum\limits_{k = 1}^{n}\frac{1}{k} + d(d+1)\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k^2} \\
    &= \sigma^2(c_d d^2 + 2d + 2)H_{n,1} + d(d+1)\sigma^4  H_{n,2}.
\end{align*}
This completes the proof of the theorem.
\end{proof}



\theoremPostPrwithbiascorrection*

\begin{proof}
Let us denote the covariance matrix estimated via Post-Processing (PP) \textbf{without} bias correction as $\widehat{\Sigma}^{b}_{\text{PP}}$. Then, the approximation error  has the following form:
\begin{align}
\begin{split}
\mathbb{E}\|\Sigma - \widehat{\Sigma}^{b}_{\text{PP}}\|_{\Fr}^2 = &\mathbb{E}_{Z_1} \|V((X + Z_1) \face (X + Z_1)) - (V(X+Z_1) \face V(X + Z_1)) - V(X\face X) + (VX)\face(VX)\|_{\Fr}^2\\
&\quad =\mathbb{E}_{Z_1} \|V(Z_1 \face Z_1) + V(Z_1 \face X) + V(X \face Z_1) - (VZ_1) \face (VX) - (VX) \face (VZ_1) - (VZ_1) \face (VZ_1)) \|_{\Fr}^2\\
&\quad =\underbrace{\mathbb{E}_{Z_1}\|V(Z_1 \face Z_1)\|_{\Fr}^2}_{A_1}  - 2\underbrace{\mathbb{E}_{Z_1} \langle V(Z_1 \face Z_1), (VZ_1) \face (VZ_1)\rangle}_{A_2} + 2 \underbrace{\mathbb{E}_{Z_1}\|V(Z_1 \face X)\|_{\Fr}^2}_{A_3}  \\
&\hspace{2cm} + 2 \underbrace{\mathbb{E}_{Z_1}\langle V(Z_1 \face X), V(X \face Z_1)\rangle}_{A_4} 
 -4 \underbrace{\mathbb{E}_{Z_1}\langle V(Z_1 \face X), (VX) \face (VZ_1)\rangle}_{A_5}    \\
&\hspace{2cm} - 4\underbrace{\mathbb{E}_{Z_1}\langle V(Z_1 \face X), (VZ_1) \face (VX)\rangle}_{A_6}  +2\mathbb{E}_{Z_1}\| (VZ_1) \face (VX) \|_{\Fr}^2 + \mathbb{E}_{Z_1}\|(VZ_1) \face (VZ_1) \|_{\Fr}^2\\
&\hspace{2cm} +2\mathbb{E}_{Z_1} \langle(VZ_1) \face (VX), (VX) \face (VZ_1)\rangle
\end{split}
\label{eq:fullsumpostprocessing}
\end{align}
The last three terms in the above expression evaluates to equation~(\ref{eq:simplifiedend}). Therefore, in what follows, we bound
$A_1$ to $A_6$. 
\paragraph{Bounding $A_1$:}
\begin{align}
\begin{split}
    A_1 & = \mathbb{E}_{Z_1}\|V(Z_1 \face Z_1)\|_{\Fr}^2 = \mathbb{E}_{Z_1}\sum\limits_{k = 1}^{n}\sum\limits_{i, j}^{d} \left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, i}(Z_1)_{t, j}\right)^2\\
    & = 3\sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{j = 1}^{d}\sum\limits_{t = 1}^{n}V_{k, t}^2 + \sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{j = 1}^{d}\sum\limits_{t_1 \ne t_2}^{n}V_{k, t_1}V_{k, t_2} + \sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{i \ne j }^{d}\sum\limits_{t = 1}^{n}V_{k, t}^2\\
    &= d(d + 1)\sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{t = 1}^{n} V_{k, t}^2 + d\sigma^4 \sum\limits_{k = 1}^{n} \left(\sum\limits_{t=1}^{n} V_{k, t}\right)^2\\
\end{split}
\label{eq:A_1}
\end{align}

\paragraph{Bounding $A_2$:}
\begin{align}
\begin{split}
    A_2 &= \mathbb{E}_{Z_1}\langle V(Z_1 \face Z_1), (VZ_1)\face (VZ_1)\rangle \\
    &= \mathbb{E}_{Z_1} \sum\limits_{k = 1}^{n} \sum\limits_{i,j}^{d} \left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, i}(Z_1)_{t, j}\right) \left(\sum\limits_{t}V_{k, t}(Z_1)_{t, i}\right)\left(\sum\limits_{t}V_{k, t}(Z_1)_{t, j}\right)\\
    &= 3\sigma^4 \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d}\sum\limits_{t = 1}^{n} V_{k, t}^3 + \sigma^4 \sum\limits_{k = 1}^{n}\sum\limits_{i \ne j}^{d}\sum\limits_{t = 1}^{n} V_{k, t}^3 + \sigma^4 \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d}\sum\limits_{t_1 \ne t_2}^{n} V_{k, t_1}V_{k, t_2}^2\\
    &= d( d + 1)\sigma^4 \sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{n} V_{k, t}^3 + d\sigma^4 \sum\limits_{k = 1}^{n}\left(\sum\limits_{t = 1}^{n} V_{k, t}\right)\left(\sum\limits_{t = 1}^{n} V^2_{k, t}\right)
\end{split}
\label{eq:A_2}
\end{align}

\paragraph{Bounding $A_3$:}
\begin{align}
\label{eq:A_3}
    A_3 &= \mathbb{E}_{Z_1}\|V(Z_1 \face X)\|_{\Fr}^2 = \mathbb{E}_{Z_1}\sum\limits_{k = 1}^{n} \sum\limits_{i, j}^{d} \left(\sum\limits_{t = 1}^{n}V_{k, t} (Z_1)_{t, i}X_{t, j}\right)^2 = d\sigma^2 \sum\limits_{k = 1}^{n} \sum\limits_{t = 1}^{n} V_{k, t}^2 \sum\limits_{ j=1}^{d}X_{t, j}^2
\end{align}

\paragraph{Bounding $A_4$:}
\begin{align}
\begin{split}
    A_4 &=\mathbb{E}_{Z_1}\langle V(Z_1 \face X), V(X \face Z_1)\rangle \\
    &=  \mathbb{E}_{Z_1} \sum\limits_{k = 1}^{n}\sum\limits_{i, j}^d \left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, i}X_{t, j}\right)\left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, j}X_{t, i}\right) = \sigma^2\sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{n} V_{k, t}^2 \sum\limits_{j = 1}^{d} X_{t, j}^2
\end{split}
\label{eq:A_4}
\end{align}

\paragraph{Bounding $A_5$:}
\begin{align}
\begin{split}
    A_5 &= \mathbb{E}_{Z_1}\langle V(Z_1 \face X), (VX) \face (VZ_1)\rangle \\
    &= \mathbb{E}_{Z_1}\sum\limits_{k = 1}^{n}\sum\limits_{i, j}^{d} \left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, i}X_{t, j}\right)\left(\sum\limits_{t = 1}^{n} V_{k, t} X_{t, i}\right)\left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, j}\right)\\
    &= \sigma^2 \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \left(\sum\limits_{t = 1}^{n}V_{k, t}^2X_{t, j}\right)\left(\sum\limits_{t = 1}^{n} V_{k, t} X_{t, j}\right) 
\end{split}
\label{eq:A_5}
\end{align}

\paragraph{Bounding $A_6$:}
\begin{align}
\begin{split}
    A_6 &= \mathbb{E}_{Z_1}\langle V(Z_1 \face X), (VZ_1) \face (VX)\rangle \\
    &= \mathbb{E}_{Z_1}\sum\limits_{k = 1}^{n}\sum\limits_{i, j}^{d} \left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, i}X_{t, j}\right)\left(\sum\limits_{t = 1}^{n} V_{k, t} (Z_1)_{t, i}\right)\left(\sum\limits_{t = 1}^{n} V_{k, t} X_{t, j}\right)\\
    &= d\sigma^2 \sum\limits_{k = 1}^{n}\sum\limits_{j=1}^{d}\left(\sum\limits_{t = 1}^{n} V_{k,t}^2X_{t, j}\right) \left(\sum\limits_{t = 1}^{n} V_{k, t} X_{t, j}\right)
\end{split}
\label{eq:A_6}
\end{align}

Plugging equation~(\ref{eq:A_1}) to equation~(\ref{eq:A_6}) in equation~(\ref{eq:fullsumpostprocessing}), we get 
\begin{align*}
   \mathbb{E}\|\Sigma - \widehat{\Sigma}^{b}_{\text{PP}}\|_{\Fr}^2 &= d(d + 1)\sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{t = 1}^{n} V_{k, t}^2 + d\sigma^4 \sum\limits_{k = 1}^{n} \left(\sum\limits_{t=1}^{n} V_{k, t}\right)^2 -2d( d + 1)\sigma^4 \sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{n} V_{k, t}^3 \\
    &\quad - 2d\sigma^4 \sum\limits_{k = 1}^{n}\left(\sum\limits_{t = 1}^{n} V_{k, t}\right)\left(\sum\limits_{t = 1}^{n} V^2_{k, t}\right) \\
    &\quad +2(d + 1)\sigma^2\sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{n} V_{k, t}^2 \sum\limits_{j = 1}^{d} X_{t, j}^2 - 4(d + 1)\sigma^2 \sum\limits_{k = 1}^{n}\sum\limits_{j=1}^{d}\left(\sum\limits_{t = 1}^{n} V_{k,t}^2X_{t, j}\right) \left(\sum\limits_{t = 1}^{n} V_{k, t} X_{t, j}\right)\\
    &\quad + 2(d + 1)\sigma^2\sum\limits_{k = 1}^{n}\sum\limits_{t=1}^{n}V_{k,t}^2 \sum\limits_{j=1}^{d} (VX)_{k,j}^2  + d(d + 2)\sigma^4 \sum\limits_{k = 1}^{n}  \left(\sum\limits_{t = 1}^{n} V_{k, t}^2\right)^2
\end{align*}

Recalling that the matrix $V$ is the \emph{averaging} workload matrix $V=(a^t_i)$ with $a^t_i=\frac{1}{t}$ for $1\leq i\leq t$ and $a^t_i=0$ otherwise, we get 
\begin{align*}
     \mathbb{E}\|\Sigma - \widehat{\Sigma}^{b}_{\text{PP}}\|_{\Fr}^2 &= d(d + 1)\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k} + dn\sigma^4  -2d( d + 1)\sigma^4 \sum\limits_{k = 1}^{n}\frac{1}{k^2}- 2d\sigma^4 \sum\limits_{k = 1}^{n}\frac{1}{k} \\
    &\quad +2(d + 1)\sigma^2\sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{k} \frac{1}{k^2} \sum\limits_{j = 1}^{d} X_{t, j}^2 - 4(d + 1)\sigma^2 \sum\limits_{k = 1}^{n}\sum\limits_{j=1}^{d}\left(\sum\limits_{t = 1}^{k} \frac{1}{k^2}X_{t, j}\right) \left(\sum\limits_{t = 1}^{k} \frac{1}{k} X_{t, j}\right)\\
    &\quad + 2(d + 1)\sigma^2\sum\limits_{k = 1}^{n}\frac{1}{k} \sum\limits_{j=1}^{d} \left(\sum\limits_{t = 1}^{k}\frac{1}{k}X_{t, j}\right)^2  + d(d + 2)\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k^2}\\
    %
    &= d(d - 1)\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k} + dn\sigma^4  -d^2\sigma^4 \sum\limits_{k = 1}^{n}\frac{1}{k^2}  +2(d + 1)\sigma^2 \underbrace{\sum\limits_{k = 1}^{n}\frac{1}{k^2}\left[\sum\limits_{t = 1}^{k} \langle X_{t, :},X_{t, :}\rangle - \frac{1}{k} \sum\limits_{t_1, t_2}^{k}\langle X_{t_1, :},X_{t_1, :}\rangle\right] }_{T_n(X)} 
\end{align*}

Since every term except $T_n(X)$ is independent of $X$, to compute both upper and lower bounds on $\sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}^{b}_{\text{PP}}\|_{\Fr}^2$, it suffices to bound the supremum of the inner difference over $X$:
\begin{align*}
     T_n := \sup_{X \in \mathcal{X}} T_n(X) \sup_{X \in \mathcal{X}}\sum\limits_{k = 1}^{n}\frac{1}{k^2}\left[\sum\limits_{t = 1}^{k} \langle X_{t, :},X_{t, :}\rangle - \frac{1}{k} \underbrace{\sum\limits_{t_1, t_2}^{k}\langle X_{t_1, :},X_{t_2, :}}_{=\left\|\sum_{t} X_{t}\right\|^2_2 \geq 0}\rangle\right].
\end{align*}

Since the double sum is non-negative, this leads to a trivial upper bound  $T_n \le \sum\limits_{k = 1}^n \frac{1}{k}=H_{n,1}$. For the lower bound, consider $X_i = (-1)^i e_1$. In this case, $\|X_i\|_2 = 1$, but $\left\|\sum_{t} X_{t}\right\|^2_2 \leq 1$, so $\sum\limits_{k = 1}^n \left(\frac{1}{k} - \frac{1}{k^3}\right) \leq  T_n$. Therefore, 
\begin{equation*}
H_{n,1} - H_{n,3} \leq  T_n \leq  H_{n,1}.
\end{equation*}
 
Therefore, we have the following bounds for the approximation error \textbf{without} a bias correction:
\begin{align*}
     \sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}^{b}_{\text{PP}}\|_{\Fr}^2 &\le d(d - 1)\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k} + dn\sigma^4  -d^2\sigma^4 \sum\limits_{k = 1}^{n}\frac{1}{k^2} + 2(d + 1)\sigma^2 \sum\limits_{k = 1}^{n}\frac{1}{k}\\
    \sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}^{b}_{\text{PP}}\|_{\Fr}^2&\ge d(d - 1)\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k} + dn\sigma^4  -d^2\sigma^4 \sum\limits_{k = 1}^{n}\frac{1}{k^2} + 2(d + 1)\sigma^2 \sum\limits_{k = 1}^{n}\frac{1}{k} - 2(d + 1)\sigma^2 \sum\limits_{k = 1}^{n}\frac{1}{k^3},
\end{align*}
Now we will determine the bias term; let $\delta_{i=j}$ denote the Dirac-delta function, then the bias of $\widehat\Sigma_{\text{PP}}$ is 

\begin{equation}
\label{eq:PP_cov_bias}
    (\mathbb{E}_{Z_1} (V(Z_1 \face Z_1) - \mathbb{E}_{Z_1} (VZ_1) \face (VZ_1))_{k, i, j} = \sigma^2 \delta_{i=j}\sum\limits_{t = 1}^{n}V_{k, t} - V_{k, t}^2 
\end{equation}

We also have the following set of equalities when $Z_1$ is a Gaussian matrix. 
\begin{align*}
    \|\mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\|_{\Fr}^2 &= \sigma^4\sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \left(\sum\limits_{t = 1}^{n}V_{k, t}^2\right)^2 = d\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k^2}\\
    \|\mathbb{E}_{Z_1} V(Z_1 \face Z_1)\|_{\Fr}^2 &= \sigma^4\sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \left(\sum\limits_{t = 1}^{n}V_{k, t}\right)^2 = dn\sigma^4 \\
    \langle \mathbb{E}_{Z_1} (VZ_1) \face (VZ_1) , \mathbb{E}_{Z_1} V(Z_1 \face Z_1 )\rangle_{\Fr} &= d\sigma^4 \sum\limits_{k = 1}^{n} \left(\sum\limits_{t = 1}^{n}V_{k, t}^2\right)\left(\sum\limits_{t = 1}^{n}V_{k, t}\right) = d\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k}
\end{align*}


To remove the bias from the error we would need to add the following terms:
\begin{align*}
    &\|\mathbb{E}_{Z_1} V(Z_1 \face Z_1)\|_{\Fr}^2 + \|\mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\|_{\Fr}^2 - 2\langle \mathbb{E}_{Z_1} (VZ_1) \face (VZ_1) , \mathbb{E}_{Z_1} V(Z_1 \face Z_1 )\rangle \\
    &\quad - 2\mathbb{E}_{Z_1} \langle (VZ_1) \face (VZ_1) , \mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\rangle_{\Fr} + 4\mathbb{E}_{Z_1} \langle (VZ_1) \face (VZ_1) , \mathbb{E}_{Z_1} V(Z_1 \face Z_1 )\rangle \\
    &\quad -2\mathbb{E}_{Z_1} \langle V(Z_1 \face Z_1) , \mathbb{E}_{Z_1} V(Z_1 \face Z_1 )\rangle_{\Fr}\\
    &= -\|\mathbb{E}_{Z_1} V(Z_1 \face Z_1)\|_{\Fr}^2 - \|\mathbb{E}_{Z_1} (VZ_1) \face (VZ_1)\|_{\Fr}^2 + 2\langle \mathbb{E}_{Z_1} (VZ_1) \face (VZ_1) , \mathbb{E}_{Z_1} V(Z_1 \face Z_1 )\rangle \\
    &=-d\sigma^4\sum\limits_{k = 1}^{n} \left(\sum\limits_{t = 1}^{n}V_{k, t}\right)^2 -d\sigma^4 \sum\limits_{k = 1}^{n} \left(\sum\limits_{t = 1}^{n}V_{k, t}^2\right)^2 + 2d\sigma^4 \sum\limits_{k = 1}^{n} \left(\sum\limits_{t = 1}^{n}V_{k, t}^2\right)\left(\sum\limits_{t = 1}^{n}V_{k, t}\right) \\
    &= -dn\sigma^4 -d\sigma^4 \sum\limits_{k = 1}^{n} \frac{1}{k^2} + 2d\sigma^4\sum\limits_{k = 1}^{n}\frac{1}{k} = -dn\sigma^4 -d\sigma^4 H_{n,2} + 2d\sigma^4 H_{n,1}.
\end{align*}

Combining everything together we obtain:
\begin{align*}
   \mathbb{E}\|\Sigma - \widehat{\Sigma}_{\text{PP}}\|_{\Fr}^2 &= d(d + 1)\sigma^4 \sum\limits_{k = 1}^{n} \sum\limits_{t = 1}^{n} V_{k, t}^2  -2d( d + 1)\sigma^4 \sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{n} V_{k, t}^3 \\
    &\quad +2(d + 1)\sigma^2\sum\limits_{k = 1}^{n}\sum\limits_{t = 1}^{n} V_{k, t}^2 \sum\limits_{j = 1}^{d} X_{t, j}^2 - 4(d + 1)\sigma^2 \sum\limits_{k = 1}^{n}\sum\limits_{j=1}^{d}\left(\sum\limits_{t = 1}^{n} V_{k,t}^2X_{t, j}\right) \left(\sum\limits_{t = 1}^{n} V_{k, t} X_{t, j}\right)\\
    &\quad + 2(d + 1)\sigma^2\sum\limits_{k = 1}^{n}\sum\limits_{t=1}^{n}V_{k,t}^2 \sum\limits_{j=1}^{d} (VX)_{k,j}^2  + d(d + 1)\sigma^4 \sum\limits_{k = 1}^{n}  \left(\sum\limits_{t = 1}^{n} V_{k, t}^2\right)^2
\end{align*}

Therefore,
\begin{align*}
     \sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}_{\text{PP}}\|_{\Fr}^2 &\le d(d + 1)\sigma^4 H_{n, 1} -d(d + 1)\sigma^4 H_{n, 2} + 2(d + 1)\sigma^2 H_{n, 1} \quad \text{and} \\
     \sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}_{\text{PP}}\|_{\Fr}^2 &\ge d(d + 1 )\sigma^4 H_{n, 1} -d(d + 1)\sigma^4 H_{n, 2} + 2(d + 1)\sigma^2 H_{n, 1} - 2(d + 1)\sigma^2 H_{n, 3},
\end{align*}
which concludes the proof.
\end{proof}

\lemmadimensionreduction*
\begin{proof}
We begin by selecting indices $i$ and $j$  such that the corresponding components $x_i, x_j$ from $x$ and $y_i, y_j$ from $y$ satisfy $(x_i^2 - y_i^2)(x_j^2 - y_j^2) \geq 0$. We can always find such indices because, by the pigeonhole principle for $d \geq 3$, there will be pairs where either both $x_i^2 \geq y_i^2$ and $x_j^2 \geq y_j^2$, or both $x_i^2 \leq y_i^2$ and $x_j^2 \leq y_j^2$. We can compare the impact of these components on the sum with the values $\sqrt{x_i^2 + x_j^2}$ and $-\sqrt{y_i^2 + y_j^2}$, which correspond to vectors in a lower dimension. Consider the difference in the objective function and $f(x_,i,y_i)$ defined below:

\begin{align*}
f(x_i,y_i)&:=\left(\sqrt{x_i^2 + x_j^2} + \sqrt{y_i^2 + y_j^2}\right)^2 + \lambda \left(x_i^2 + x_j^2 - y_i^2 - y_j^2\right)^2 \\
g(x_i,y_i) &:= (x_i - y_i)^2 + (x_j - y_j)^2 + \lambda(x_i^2 - y_i^2)^2 + \lambda (x_j^2 - y_j^2)^2.
\end{align*}

Note that $f(x_i,y_i)$ is the objective function. Now, after algebraic manipulation, we get 

\begin{align}
f(x_i,y_i) - g(x_i,y_i) &=     2 \sqrt{x_i^2 + x_j^2} \sqrt{y_i^2 + y_j^2} + \lambda (x_i^2 - y_i^2)(x_j^2 - y_j^2) + 2x_iy_i + 2x_jy_j \\
&\geq \lambda (x_i^2 - y_i^2)(x_j^2 - y_j^2) \geq 0,
\end{align}
where the first inequality follows from the Cauchy-Schwarz inequality and the second inequality is from the assumption that $(x_i^2 - y_i^2)(x_j^2 - y_j^2) \ge 0$. 

For this lemma, $d = 2$ is indeed a special case since it is possible to find $x_1^2 > y_1^2 $ and $x_2^2 < y_2^2$, for which the dimension reduction argument would not work.
\end{proof}






\lemmaforkequaltwo*
\begin{proof}

First, we consider the effect of the signs of the components of $x$ and $y$. Multiplying both $x_i$ and $y_i$ by $-1$ does not change the value of the expression. If $x_i$ and $y_i$ have the same sign, then by flipping the sign of one of them, we increase the difference $\|x - y\|_2$, while $\|x \circ x - y \circ y\|_2$ remains unchanged. Therefore, without loss of generality, we can assume $x = (x_1, x_2)$ and $y = (-y_1, -y_2)$ for positive $x_i$ and $y_i$. Next, note that for a fixed difference $\|x - y\|_2$, the functional increases as the sum $x_1 + y_1$ or $x_2 + y_2$ increases. Thus, we can assume $\|x\|_2 = 1$, so $x_2 = \sqrt{1 - x_1^2}$. The norm of $y$, however, can be different. Therefore, we look for a solution of the form $x = (x_1, \sqrt{1 - x_1^2})$ and $y = (-y_1, -y_2)$.

We now consider the functional in the statement of the lemma in the following form:
\begin{equation}
    \mathcal{L}_\lambda(x_1, y_1, y_2) = (x_1 + y_1)^2 + \left(\sqrt{1 - x_1^2} + y_2\right)^2 + \lambda \left(x_1^2 - y_1^2\right)^2 + \lambda \left(1 - x_1^2 - y_2^2\right)^2.
\end{equation}



We now aim to prove that $\sup \mathcal{L}_\lambda(x_1, y_1, y_2) = r_2^{\text{diag}}(\lambda)$ in the constrained domain $y_1 \geq 0$, $y_2 \geq 0$, $y_1^2 + y_2^2 \leq 1$, $0 \leq x_1 \leq 1$. This analysis involves considering up to twenty-four different scenarios for optimization with boundaries, which, due to symmetry, can be reduced to five distinct cases.

\subsubsection*{\textbf{Case I: $y_1^2 + y_2^2 = 1$, $y_1 > 0$, $y_2 > 0$.}}

We can compute $y_2 = \sqrt{1 - y_1^2}$, then the optimization functional is 
\begin{align}
    \mathcal{L}_\lambda\left(x_1, y_1, \sqrt{1 - y_1^2}\right) &= (x_1 + y_1)^2 + \left(\sqrt{1 - x_1^2} + \sqrt{1 - y_1^2}\right)^2 + \lambda \left(x_1^2 - y_1^2\right)^2 + \lambda \left(y_1^2 - x_1^2 \right)^2\\
    &= 2 + 2x_1y_1 + 2 \sqrt{1 - x_1^2}\sqrt{1 - y_1^2} + 2\lambda \left(x_1^2 - y_1^2\right)^2.
\end{align}

The cases where $x_1 = 0$ or $x_1 = 1$ with $\|y\| = 1$ are equivalent to the scenario where $\|x\| = 1$ and $y_1 = 0$, or $y_1 = 1$, which we will consider later. For now, we proceed by computing the derivatives with respect to $y_1$ and $x_1$:
\begin{align}
    \begin{split}
        \frac{\partial \mathcal{L}_\lambda\left(x_1, y_1, \sqrt{1 - y_1^2}\right)}{\partial y_1} = 2x_1 - \frac{2y_1\sqrt{1 - x_1^2}}{\sqrt{1 - y_1^2}} + 4y_1\lambda \left(y_1^2 - x_1^2\right) = 0, \\
        \frac{\partial \mathcal{L}_\lambda\left(x_1, y_1, \sqrt{1 - y_1^2}\right)}{\partial x_1} = 2y_1 - \frac{2x_1\sqrt{1 - y_1^2}}{\sqrt{1 - x_1^2}} + 4x_1\lambda \left(x_1^2 - y_1^2\right) = 0.
    \end{split}
\end{align}

We transform this system by summing and subtracting the equalities:
\begin{align}
    \begin{split}
        \left(x_1 + y_1\right) - \frac{y_1 - y_1x_1^2 + x_1 - x_1y_1^2}{\sqrt{1 - y_1^2}\sqrt{1 - x_1^2}} + 4\lambda \left(x_1^3 + y_1^3\right) - 4\lambda y_1 x_1 \left(y_1 + x_1\right) = 0, \\
        \left(x_1 - y_1\right) - \frac{y_1 - y_1x_1^2 - x_1 + x_1y_1^2}{\sqrt{1 - y_1^2}\sqrt{1 - x_1^2}} + 4\lambda \left(y_1^3 - x_1^3\right) - 4\lambda y_1 x_1 \left(x_1 - y_1\right) = 0.
    \end{split}    
\end{align}

$x_1 + y_1 = 0$ is not a solution under the constraints we are solving. However, $x_1 = y_1$ is a solution that gives $\mathcal{L}_\lambda(x_1, x_1, \sqrt{1 - x_1^2}) = 4$ for any $\lambda$. From now on, consider $y_1 \neq x_1$; then we can divide by the difference, leading to the system:
\begin{align}
    \begin{split}
        1 - \frac{1 - x_1y_1}{\sqrt{1 - y_1^2}\sqrt{1 - x_1^2}} + 4\lambda \left(x_1 - y_1\right)^2 = 0, \\
        1 + \frac{1 + x_1y_1}{\sqrt{1 - y_1^2}\sqrt{1 - x_1^2}} - 4\lambda \left(x_1 + y_1\right)^2 = 0.
    \end{split}    
\end{align}

We  consider again the sum and difference to get:
\begin{align}
    \begin{split}
        2 + \frac{2x_1y_1}{\sqrt{1 - y_1^2}\sqrt{1 - x_1^2}} - 16\lambda x_1 y_1 = 0, \\
        -\frac{2}{\sqrt{1 - y_1^2}\sqrt{1 - x_1^2}} + 8\lambda \left(x_1^2 + y_1^2\right) = 0.
    \end{split}    
\end{align}

We solve it for $\lambda$ to get the following equation:
\begin{align}
    \frac{1}{4\left(x_1^2 + y_1^2\right) \sqrt{1 - y_1^2}\sqrt{1 - x_1^2}} &= \frac{1}{8x_1y_1} + \frac{1}{8\sqrt{1 - y_1^2}\sqrt{1 - x_1^2}}.
\end{align}

We transform it into the form
\begin{equation}
    \frac{2x_1y_1}{x_1^2 + y_1^2} - x_1y_1 = \sqrt{1 - y_1^2 - x_1^2 + x_1^2y_1^2}.
\end{equation}


By squaring both sides and subtracting $x_1^2y_1^2$, we obtain

\begin{equation}
    \frac{4x_1^2y_1^2}{(x_1^2 + y_1^2)^2}(1 - x_1^2 - y_1^2) = 1 - x_1^2 - y_1^2.
\end{equation}

So, either $x_1^2 + y_1^2 = 1$, which implies $y_1 = \sqrt{1 - x_1^2}$, or $x_1^2 + y_1^2 = 2x_1y_1$, which implies $x_1 = y_1$, which we have already discussed. We have found another potentially optimal point $y_1 = \sqrt{1 - x_1^2}$, which we will further investigate. We now consider it as a function of one variable $x_1$:

\begin{equation}
    \mathcal{L}_\lambda\left(x_1, \sqrt{1 - x_1^2}, x_1\right)  = 2 \left(x_1 + \sqrt{1 - x_1^2}\right)^2 + 2\lambda \left(1 - 2x_1^2\right)^2.
\end{equation}

Then the optimum is either $x_1 = 0$, $x_1 = 1$ with the value $4$, or when the derivative is $0$:

\begin{align}
    \frac{\partial \mathcal{L}_\lambda\left(x_1, \sqrt{1 - x_1^2}, x_1\right)}{\partial x_1} &= 4\sqrt{1 - x_1^2} - \frac{4x_1^2}{\sqrt{1 - x_1^2}} - 16\lambda x_1(1-2x_1^2)\\
    &=  \frac{4(1 - 2x_1^2)\left(1 - 4\lambda x_1 \sqrt{1 - x_1^2}\right)}{\sqrt{1 - x_1^2}} = 0.
\end{align}

The first term gives $x_1 = \frac{1}{\sqrt{2}}$, which results in $\mathcal{L}_\lambda = 4$; otherwise, $4\lambda x_1 \sqrt{1 - x_1^2} = 1$, which we can solve by first denoting $x_1^2$
 as a new variable in the equation  $-x_1^4 + x_1^2 - \frac{1}{16\lambda^2} = 0$. Using the quadratic formula, we find the optimal value $x_1^*$, which is given by:



\begin{equation}
    x_1^* = \sqrt{\frac{1}{2} + \frac{1}{2}\sqrt{1 - \frac{1}{4\lambda^2}}}, \quad \sqrt{1 - (x_1^*)^2} = \sqrt{\frac{1}{2} - \frac{1}{2}\sqrt{1 - \frac{1}{4\lambda^2}}}.
\end{equation}

This solution exists only when $\lambda \ge \frac{1}{2}$. Substituting this root back into the function gives us

\begin{equation}
    \mathcal{L}_\lambda\left(x_1^*, \sqrt{1 - (x_1^*)^2}, x_1^*\right) = 2 + \underbrace{4x_1^*\sqrt{1 - (x_1^*)^2}}_{=\frac{1}{\lambda}} + 2\lambda + \underbrace{8\lambda ((x_1^*)^4 - (x_1^*)^2)}_{=-\frac{1}{2\lambda}} = 2 + 2\lambda + \frac{1}{2\lambda}  \geq 4,
\end{equation}
which constitutes the function $r_2^{\text{diag}}(\lambda)$.

\subsubsection*{\textbf{Case II: $y_1^2 + y_2^2 < 1$, $y_1, y_2 > 0$ (Interior).}}

For the optimum, it is important that $\frac{\partial \mathcal{L}_\lambda}{\partial y_1} = 0$ and $\frac{\partial \mathcal{L}_\lambda}{\partial y_2} = 0$. But the necessary condition for the maximum would be that the Hessian is negative semi-definite: $\frac{\partial^2 \mathcal{L}_\lambda}{\partial y_1^2} \leq 0$ and $\frac{\partial^2 \mathcal{L}_\lambda}{\partial y_2^2} \leq 0$. We can compute the second derivatives explicitly:

\begin{align}
    \frac{\partial^2 \mathcal{L}_\lambda}{\partial y_1^2} = 2 + 12\lambda y_1^2 - 4\lambda x_1^2 \leq 0 \Rightarrow y_1^2 \leq \frac{x_1^2}{3}.
\end{align}

Analogously, we can derive that $y_2^2 \leq \frac{1 - x_1^2}{3}$. Then substituting this into the functional, we get:

\begin{align}
    \mathcal{L}_\lambda(x_1, y_1, y_2) &= (x_1 + y_1)^2 + \left(\sqrt{1 - x_1^2} + y_2\right)^2 + \lambda (x_1^2 - y_1^2)^2 + \lambda \left(1 - x_1^2 - y_2^2\right)^2 \\
    & \leq \left(1 + \frac{1}{\sqrt{3}}\right)^2 + \lambda + \frac{\lambda}{9} < r_2(\lambda).
\end{align}

\subsubsection*{\textbf{Case III: $y_1 = 0$, $0 < y_2 < 1$.}}

\begin{align}
    \mathcal{L}_\lambda(x_1, y_1, y_2) &= x_1^2 + \left(\sqrt{1 - x_1^2} + y_2\right)^2 + \lambda x_1^4 + \lambda \left(1 - x_1^2 - y_2^2\right)^2.
\end{align}

As a continuous function of $y_2$ on an open domain, it can reach a maximum only when the second derivative is non-positive, which leads to the same condition as in the previous case: $y_2^2 \leq \frac{1 - x_1^2}{3}$. Since $y_1 = 0 \leq \frac{x_1^2}{3}$, this leads to the same conclusion.

\subsubsection*{\textbf{Case IV: $y_1 = 0$, $y_2 = 0$.}}
\begin{align}
\begin{split}
    \mathcal{L}_\lambda(x_1, 0, 0) &= x_1^2 + 1 + \lambda x_1^4 + \lambda (1 - x_1^2)^2 \leq x_1^2 + \lambda x_1^4 + \lambda(1 - x_1^2)^2 + 1 \leq 1 + \lambda < r_2^{\text{diag}}(\lambda).
\end{split}
\end{align}

\subsubsection*{\textbf{Case V: $y_1 = 1$, $y_2 = 0$.}}

\begin{align}
    \mathcal{L}_\lambda(x_1, 1, 0) &= (x_1 + 1)^2 + 1 - x_1^2 + \lambda(1 - x_1^2)^2 + \lambda (1 - x_1^2)^2 = 2 + 2x_1 + 2\lambda(1 - x_1^2)^2.
\end{align}

First, for $\lambda \leq \frac{1}{2}$, we have:
\begin{align}
     \mathcal{L}_\lambda(x_1, 1, 0) &\leq 2 + 2x_1 + 1 - x_1^2 \leq 4.
\end{align}

For $\lambda > \frac{1}{2}$, we have the following inequality:
\begin{align}
    \mathcal{L}_\lambda(x_1, 1, 0) &\leq 2 + 2x_1 + 2\lambda(1 - x_1^2) = 2 + 2\lambda + 2x_1 - 2\lambda x_1^2 \leq 2 + 2\lambda + \frac{1}{2\lambda}.
\end{align}

This concludes the proof.
\end{proof}









\begin{restatable}[Expected Second Moment Error with PP]{lemma}{lemmaexpectedsecondmomentPostPrDPAdam}
\label{lem:PostPr_DP_Adam}
Given the private estimation of the first moment $A_1(X + C_1^{-1}Z_1)$, where $A_1 = B_1C_1$, and the second moment $ A_2 (X + C_1^{-1}Z_1) \circ (X + C_1^{-1}Z_1)$, where $A_2 = B_2C_2$, with independent noise $Z_1\in \mathcal{N}(0, \|C_1\|_{1\to 2}^{2}\sigma^2)^{n \times d}$, the clipping norm $\zeta=1$, $d > 1$, the expected squared Frobenius norm of the estimation error for the second moment satisfies:
\begin{equation}
\begin{aligned}
\sup_{X \in \mathcal{X}}\mathbb{E}_{Z} \|\widehat D_{\text{PP}}-D\|^2 :&= \sup_{X \in \mathcal{X}}\mathbb{E}\| A_2 ((X + C_1^{-1}Z_1) \circ (X + C_1^{-1}Z_1)) - A_2 (X \circ X) \|_{\Fr}^2 \\
&= 2d\sigma^4 \|C_1\|_{1 \to 2}^4 \cdot \tr(A_2^\top A_2 (Q \circ Q)) + 4\sigma^2 \|C_1\|_{1 \to 2}^2 \cdot \sup_{X \in \mathcal{X}} \tr((A_2^TA_2 \circ Q)XX^T) \\
&\quad+\underbrace{d\sigma^4 \|C_1\|_{1 \to 2}^4 \cdot \tr(A_2^\top A_2 E_Q)}_{\text{bias}}
\end{aligned}
\end{equation}
where $Q = C_1^{-1} C_1^{-\top}$ and $E_Q = \diag(Q)\diag^{\top}(Q)$.
\end{restatable}

\begin{proof}
We aim to evaluate the expected squared Frobenius norm of the error:
\begin{equation*}
\begin{aligned}
&\sup_{X \in \mathcal{X}}\mathbb{E}\| A_2 ((X + C_1^{-1}Z_1) \circ (X + C_1^{-1}Z_1)) - A_2 (X \circ X) \|_{\Fr}^2\\
&\quad =4 \underbrace{\sup_{X \in \mathcal{X}}\mathbb{E} \|A_2 (X \circ C_1^{-1} Z_1)\|_{\Fr}^2}_{S_1} + \underbrace{\mathbb{E} \|A_2 ((C_1^{-1} Z_1) \circ (C_1^{-1} Z_1))\|_{\Fr}^2}_{S_2}
\end{aligned}
\end{equation*}
We compute those terms separately.

\begin{align*}
    S_1 &= \sup_{X \in \mathcal{X}}\mathbb{E} \|A_2 (X \circ C_1^{-1} Z_1)\|_{\Fr}^2 =  \sup_{X \in \mathcal{X}}\mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \left(\sum\limits_{t = 1}^{n} (A_2)_{k, t} X_{t, j} \sum\limits_{r = 1}^{n} (C_1^{-1})_{t, r} (Z_1)_{r, j}\right)^2\\
    &=  \sup_{X \in \mathcal{X}} \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_1} X_{t_1, j}X_{t_2, j} \sum\limits_{r = 1}^{n} (C_1^{-1})_{t_1, r}(C_1^{-1})_{t_2, r}  \mathbb{E}(Z_1)_{r, j}^2\\
    &= \sigma^2 \|C_1\|_{1\to2}^2\sup_{X \in \mathcal{X}} \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_2} X_{t_1, j}X_{t_2, j} \sum\limits_{r = 1}^{n} (C_1^{-1})_{t_1, r}(C_1^{-1})_{t_2, r}\\
    &= \sigma^2 \|C_1\|_{1\to2}^2\sup_{X \in \mathcal{X}} \sum\limits_{t_1, t_2}^{n} \langle(A_2^\top)_{t_1}, (A_2^\top)_{t_2}\rangle \langle X_{t_1}, X_{t_2}\rangle  \langle(C_1^{-1})_{t_1}, (C_1^{-1})_{t_2} \rangle\\
    &= \sigma^2 \|C_1\|_{1\to2}^2\sup_{X \in \mathcal{X}} \tr((A_2^TA_2 \circ Q)XX^T),
\end{align*}
where $Q = C_1^{-1}C_1^{-T}$.

\begin{align*}
    S_2 &= \mathbb{E} \|A_2 ((C_1^{-1} Z_1) \circ (C_1^{-1} Z_1))\|_{\Fr}^2 = \mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \left(\sum\limits_{t = 1}^{n}(A_2)_{k, t} (C_1^{-1}Z_1)^2_{t, j}\right)^2\\
    &= \mathbb{E} \sum\limits_{k = 1}^{n}\sum\limits_{j = 1}^{d} \sum\limits_{t_1, t_2}^{n}(A_2)_{k, t_1}(A_2)_{k, t_2} (C_1^{-1}Z_1)^2_{t_1, j}(C_1^{-1}Z_1)^2_{t_2, j}.
\end{align*}

First, we compute:
\begin{align*}
\mathbb{E}(C_1^{-1}Z_1)^2_{t_1, j}(C_1^{-1}Z_1)^2_{t_2, j} 
&=\mathbb{E}\sum\limits_{r = 1}^{n} (C_1^{-1})^2_{t_1, r}(C_1^{-1})^2_{t_2, r}(Z_1)_{r, j}^4 + \mathbb{E}\sum\limits_{r_1\ne r_2}^{n} (C_1^{-1})^2_{t_1, r_1}(C_1^{-1})^2_{t_2, r_2}(Z_1)_{r_1, j}^2(Z_1)_{r_2, j}^2\\
&\quad + 2\mathbb{E}\sum\limits_{r_1\ne r_2}^{n} (C_1^{-1})_{t_1, r_1}(C_1^{-1})_{t_1, r_2}(C_1^{-1})_{t_2, r_1}(C_1^{-1})_{t_2, r_2}(Z_1)_{r_1, j}^2(Z_1)_{r_2, j}^2\\
&= 3\sigma^4 \|C_1\|_{1 \to 2}^4 \sum\limits_{r = 1}^{n} (C_1^{-1})^2_{t_1, r}(C_1^{-1})^2_{t_2, r} + \sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{r_1 \ne r_2}^{n} (C_1^{-1})^2_{t_1, r_1}(C_1^{-1})^2_{t_2, r_2}\\
&\quad +2 \sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{r_1\ne r_2}^{n} (C_1^{-1})_{t_1, r_1}(C_1^{-1})_{t_1, r_2}(C_1^{-1})_{t_2, r_1}(C_1^{-1})_{t_2, r_2}\\
&= \sigma^4 \|C_1\|_{1 \to 2}^4Q_{t_1, t_1} Q_{t_2, t_2} +2 \sigma^4 \|C_1\|_{1 \to 2}^4Q_{t_1, t_2}^2.
\end{align*}


Plugging it back, we get 
\begin{align*}
     \mathbb{E} \|A_2 ((C_1^{-1} Z_1) \circ (C_1^{-1} Z_1))\|_{\Fr}^2 &= d\sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{k = 1}^{n} \sum\limits_{t_1, t_2}^{n}(A_2)_{k, t_1}(A_2)_{k, t_2} (Q_{t_1,t_1}Q_{t_2, t_2} + 2Q_{t_1, t_2}^2)\\
     &= d\sigma^4 \|C_1\|_{1 \to 2}^4\sum\limits_{k = 1}^{n} \sum\limits_{t_1, t_2}^{n}(A_2)_{k, t_1}(A_2)_{k, t_2} (Q_{t_1,t_1}Q_{t_2, t_2} + 2Q_{t_1, t_2}^2)\\
     &= d\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2E_Q) +2d\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2(Q\circ Q)),
\end{align*}
where $E_Q = \diag(Q)\diag^{\top}(Q)$. 

Adding these terms together we obtain:
\begin{equation*}
\begin{aligned}
\sup_{X \in \mathcal{X}}\mathbb{E}_{Z} \|\widehat D_{\text{PP}}-D\|^2&= 2d\sigma^4 \|C_1\|_{1 \to 2}^4 \cdot \tr(A_2^\top A_2 (Q \circ Q)) + 4\sigma^2 \|C_1\|_{1 \to 2}^2 \cdot \sup_{X \in \mathcal{X}} \tr((A_2^TA_2 \circ Q)XX^\top)\\
&\quad+d\sigma^4 \|C_1\|_{1 \to 2}^4 \cdot \tr(A_2^\top A_2 E_Q)
\end{aligned}
\end{equation*}

\textbf{Bias Correction.}

The expectation of $A_2 ((C_1^{-1}Z_1) \circ (C_1^{-1}Z_1))]_{k, j}$ introduces a bias:

\begin{align*}
[\mathbb{E} A_2 ((C_1^{-1}Z_1) \circ (C_1^{-1}Z_1))]_{k, j} &= \mathbb{E}\sum\limits_{t = 1}^{n} (A_2)_{k, t} \left(\sum\limits_{r =1}^{n}(C_1^{-1})_{t, r} (Z_1)_{r, j}\right)^2\\
&= \sigma^2\|C_1\|_{1\to2}^2\sum\limits_{t = 1}^{n}\sum\limits_{r =1}^{n} (A_2)_{k, t} (C_1^{-1})^2_{t, r}\\
&=\sigma^2\|C_1\|_{1\to2}^2\sum\limits_{t = 1}^{n} (A_2)_{k, t} Q_{t, t}.
\end{align*}
%
The Frobenius norm of this bias is:
\begin{align*}
\|\mathbb{E}A_2 ((C_1^{-1}Z_1) \circ (C_1^{-1}Z_1))\|_{\Fr}^2 &= \sigma^4\|C_1\|_{1\to2}^4 \sum\limits_{k = 1}^{n} \sum\limits_{j = 1}^{d} \sum\limits_{t_1, t_2}^{n} (A_2)_{k, t_1}(A_2)_{k, t_2} Q_{t_1, t_1}Q_{t_2, t_2}\\
&= d\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2E_Q)
\end{align*}

If we subtract this bias from the estimate, it will increase the error by the aforementioned quantity due to the Frobenius norm of the bias but will decrease the error by two scalar products with the $A_2 ((C_1^{-1} Z_1) \circ (C_1^{-1} Z_1))$ term:

\begin{equation*}
    \mathbb{E}\langle A_2 ((C_1^{-1}Z_1) \circ (C_1^{-1}Z_1)), \mathbb{E}A_2 ((C_1^{-1}Z_1) \circ (C_1^{-1}Z_1)) \rangle = \|\mathbb{E}A_2 ((C_1^{-1}Z_1) \circ (C_1^{-1}Z_1))\|_{\Fr}^2.
\end{equation*}

Thus, we can eliminate the last term ($d\sigma^4 \|C_1\|_{1 \to 2}^4 \tr (A_2^\top A_2E_Q)$) in the error sum via bias correction.
\end{proof}


\clearpage
\section{Algorithms}

\begin{algorithm}[ht]
\caption{Differentially Private JME Adam}\label{alg:jme_adam} 
\begin{algorithmic}
\STATE {\bfseries Input:} Initial model $\theta_0 \in \mathbb{R}^d$, dataset $D$, batchsize $b$, matrices $C_{\beta_1}, C_{\beta_2} \in \mathbb{R}^{n \times n}$, model loss $\ell(\theta,d)$, clipnorm $\zeta$, noise multiplier $\sigma_{\epsilon, \delta} \ge 0$, learning rate $\alpha > 0$, and parameters $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\varepsilon = 10^{-8}$.


\STATE $m_0 \leftarrow 0$ \qquad// first moment initialization.
\STATE $v_0 \leftarrow 0$ \qquad// second moment initialization.

\STATE $\lambda, s_\lambda= \cdot \textit{Joint-sens}(C_{\beta_1}, C_{\beta_2})$ \qquad// joint sensitivity 
\STATE $Z_1, Z_2 \sim N(0, \sigma_{\epsilon, \delta}^2s_\lambda^2I_d) $ \qquad// noise generating
\FOR{$i = 1, 2, \dots ,n$}
\STATE $S_i \leftarrow \{d_1, \dots, d_m\} \subseteq D$\quad select a data batch
\STATE $g_j \leftarrow \nabla_\theta \ell(\theta_{i-1}, d_j)) \quad\text{for $j=1,\dots,m$}$
%\STATE $g_j^c \leftarrow  \min(1, \zeta/||g_j||)g_j $
\STATE $x_i \leftarrow \sum_{j = 1}^{m} \min(1, \zeta/||g_j||)g_j $ %g_j^c$ 
\STATE $\widehat{x}_i \leftarrow x_i + \zeta [C_{\beta_1}^{-1}Z_1]_{[i,\cdot]}$
\STATE $\widehat{x^2}_i \leftarrow x_i^2 + \lambda^{-1/2}\zeta [C_{\beta_2}^{-1}Z_2]_{[i,\cdot]}$
\STATE $m_i \leftarrow m_{i - 1} \beta_1 + (1 - \beta_1)\widehat{x}_i$

\STATE $v_i \leftarrow v_{i - 1} \beta_2 + (1 - \beta_2)\widehat{x^2}_i$
\STATE $\widehat{m}_i = m_i / (1 - \beta_1^i)$ \qquad// bias-correction
\STATE $\widehat{v}_i = v_i / (1 - \beta_2^i)$ \qquad// bias-correction

\STATE $\theta_{i} \leftarrow \theta_{i - 1} - \alpha \widehat{m}_i / (\sqrt{\widehat{v}_i} +\epsilon)$
\ENDFOR
\ENSURE $\Theta=(\theta_1,\dots,\theta_n)$

\end{algorithmic}
\end{algorithm}



\begin{algorithm}[ht]
\caption{$\lambda$-\acronym}\label{alg:lambdaJME}
\begin{algorithmic}
\INPUT input stream vectors $x_1, \dots, x_n\in \mathbb{R}^d$ with \mbox{$\|x_t\|_2 \leq \zeta$} for $\zeta > 0$
\INPUT workload matrices $A_1=(a^t_{k}), A_2=(b^t_{k}) \in \mathbb{R}^{n \times n}$
\INPUT noise shaping matrices $C_1, C_2$ (lower triangular, invertible, decreasing column norms) \quad (default: $\text{I}_{n\times n}$)
\INPUT privacy parameters $(\epsilon,\delta)$
\smallskip
\STATE $\sigma_{\epsilon, \delta}\leftarrow$ noise strength for $(\epsilon,\delta)$-dp Gaussian mechanism

\STATE $s\leftarrow \zeta \|C_1\|_{1 \to 2}r_d\left(\frac{\lambda \zeta^2 \|C_2\|_{1 \to 2}^2}{\|C_1\|_{1 \to 2}^2}\right)^{1/2}$ \hfill// joint sensitivity
\STATE $Z_1\sim \big[\mathcal{N}(0, \sigma_{\epsilon, \delta}^2 s^2) \big]^{n\times d}$ \hfill// 1st moment noise
\STATE $Z_2\sim \big[\mathcal{N}(0, \sigma_{\epsilon, \delta}^2 s^2)
\big]^{n\times d\times d}$ \hfill// 2nd moment noise 
\smallskip
\FOR{$t = 1, 2, \dots ,n$}
\STATE $\widehat{x_t} \leftarrow x_t +  [C_{1}^{-1}Z_1]_{[t,\cdot]}$
\STATE $\widehat{x_t \otimes x_t} \leftarrow x_t \otimes x_t + \lambda^{-1/2}[C_{2}^{-1}Z_2]_{[t,\cdot,\cdot]}$
\STATE \textbf{yield} \quad$\widehat{Y_t}=\sum\limits_{k = 1}^{t} a^t_k \widehat{x_k}, \quad \widehat{S_t}=\sum\limits_{k = 1}^{t} b^t_k \widehat{x_k \otimes x_k}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{$\alpha$-IME (IME with budget split parameter $\alpha\in(0,1)$)}\label{alg:alphaIME}
\begin{algorithmic}
\INPUT input stream of vectors $x_1, \dots, x_n\in \mathbb{R}^d$ with \mbox{$\|x_t\|_2 \leq \zeta$} for $\zeta > 0$
\INPUT workload matrices $A_1=(a^t_{k}), A_2=(b^t_{k}) \in \mathbb{R}^{n \times n}$
\INPUT noise shaping matrices $C_1, C_2$ (lower triangular, invertible, decreasing column norms)\quad (default: $\text{I}_{n\times n}$)
\INPUT privacy parameters $(\epsilon,\delta)$
\INPUT privacy trade-off $\alpha\in(0,1)$
\smallskip
\STATE $(\epsilon_1,\delta_1) \leftarrow (\alpha\epsilon,\alpha\delta)$ \hfill// 1st moment private budget 
\STATE $(\epsilon_2,\delta_2) \leftarrow ((1-\alpha)\epsilon,(1-\alpha)\delta)$ \hfill// 2nd moment private budget 
\STATE $\sigma_1\leftarrow$ noise strength for $(\epsilon_1,\delta_1)$-dp Gaussian mechanism
\STATE $s_1\leftarrow 2\zeta \|C_1\|_{1\to 2}$ \hfill// sensitivity of 1st moment
\STATE $Z_1\sim \big[\mathcal{N}(0, \sigma^2_1 s^2_1) \big]^{n\times d}$ \hfill// 1st moment noise
\STATE $\sigma_2\leftarrow$ noise strength for $(\epsilon_2,\delta_2)$-dp Gaussian mechanism
\STATE $s_2\leftarrow \sqrt{2}\zeta^2 \|C_2\|_{1\to 2}$ \hfill// sensitivity of 2nd moment
\STATE $Z_2\sim \big[\mathcal{N}(0, \sigma^2 s_2^2) \big]^{n\times d\times d}$ \hfill// 2nd moment noise
\smallskip
\FOR{$t = 1, 2, \dots ,n$}
\STATE $\widehat{x_t} \leftarrow x_t +  [C_{1}^{-1}Z_1]_{[t,\cdot]}$
\STATE $\widehat{x_t \otimes x_t} \leftarrow x_t \otimes x_t + [C_{2}^{-1}Z_2]_{[t,\cdot,\cdot]}$
\STATE \textbf{yield} \quad$\widehat{Y_t}=\sum\limits_{k = 1}^{t} a^t_k \widehat{x_k}, \quad \widehat{S_t}=\sum\limits_{k = 1}^{t} b^t_k \widehat{x_k \otimes x_k}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{$\tau$-CS (CS with 2nd moment rescaling parameter $\tau>0$)}\label{alg:tauCS}
\begin{algorithmic}
%\INPUT sequence of vectors $x_1, \dots, x_n\in \mathbb{R}^d$ with \mbox{$\|x_t\|_2 \leq \zeta$} for $\zeta > 0$
\INPUT input stream of vectors $x_1, \dots, x_n\in \mathbb{R}^d$ with \INPUT workload matrices $A=(a^t_{k}) \in \mathbb{R}^{n \times n}$
\INPUT input dimension $d$, bound on input norm $\zeta>0$
\INPUT privacy parameters $(\epsilon,\delta)$
\INPUT (optional) noise shaping matrix $C$ (lower triangular, invertible, decreasing column norms)\quad (default: $\text{I}_{n\times n}$)
\smallskip
\STATE $\sigma_{\epsilon,\delta}\leftarrow$ noise strength for $(\epsilon,\delta)$-dp Gaussian mechanism
\STATE $s\leftarrow 2\zeta\sqrt{1 + \tau \zeta^2}$ \hfill// sensitivity based on norm of concatenated data
\STATE $Z\sim \big[\mathcal{N}(0, \sigma_{\epsilon,\delta}^2 s^2) \big]^{n\times d(d+1)}$ \hfill // noise matrix for concatenated data
\smallskip
\FOR{$t = 1, 2, \dots ,n$}
\STATE $\widetilde{x_t} = \big(\, x_t, \sqrt{\tau} \textsf{vec}(x_t\otimes x_t)\,\big)$ %\hfill // %\chl{TODO: use $\tau,\tau^2$ rather than $\sqrt{\tau},\tau$ everywhere?}
\STATE $\widehat{\widetilde{x_t}} \leftarrow\widetilde{x_t} +  [C^{-1}Z]_{[t,\cdot]}$
%\STATE $\widehat{Y_t}=\sum\limits_{k = 1}^{t} a^t_k [\widehat{\tilde x_t}]_{1:d}$
%\STATE $\widehat{S_t}=\sum\limits_{k = 1}^{t} b^t_k [\widehat{\tilde x_t}]_{(d+1):d(d+1)}$
\STATE \textbf{yield} \quad $\widehat{Y_t}=\sum\limits_{k = 1}^{t} a^t_k [\widehat{\tilde x_t}]_{1:d}$\quad $\widehat{S_t}=\frac{1}{\sqrt{\tau}}\sum\limits_{k = 1}^{t} b^t_k [\widehat{\tilde x_t}]_{(d+1):d(d+1)}$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[ht]
\caption{PP}\label{alg:PP}
\begin{algorithmic}
\INPUT input stream of vectors $x_1, \dots, x_n\in \mathbb{R}^d$ with \mbox{$\|x_t\|_2 \leq \zeta$} for $\zeta > 0$
\INPUT workload matrices $A_1=(a^t_{k}), A_2=(b^t_{k}) \in \mathbb{R}^{n \times n}$
\INPUT noise shaping matrix $C_1$ (lower triangular, invertible, decreasing column norms)\quad (default: $\text{I}_{n\times n}$)
\INPUT privacy parameters $(\epsilon,\delta)$
\smallskip
\STATE $\sigma_{\epsilon,\delta}\leftarrow$ noise strength for $(\epsilon,\delta)$-dp Gaussian mechanism
\STATE $s\leftarrow 2\zeta \|C_1\|_{1\to2}$ \hfill// sensitivity of 1st moment
\STATE $Z\sim \big[\mathcal{N}(0, \sigma_{\epsilon,\delta}^2 s^2) \big]^{n\times d}$ \hfill// 1st moment noise
\smallskip
\FOR{$t = 1, 2, \dots ,n$}
\STATE $\widehat{x_t} \leftarrow x_t +  [C_{1}^{-1}Z]_{[t,\cdot]}$
\STATE $b_t \leftarrow I_{d \times d} \times \sigma_{\epsilon, \delta}^2 \|C_1\|_{1\to 2}^2 \sum\limits_{k = 1}^{n} (A_2)_{t, k}(C_1C_1^{\top})^{-1}_{k, k}$ \hfill// bias term (optional)
\STATE $\widehat{x_t \otimes x_t} \leftarrow \widehat{x_t} \otimes \widehat{x_t} - b_t$ 
\STATE \textbf{yield} \quad$\widehat{Y_t}=\sum\limits_{k = 1}^{t} a^t_k \widehat{x_k}, \quad \widehat{S_t}=\sum\limits_{k = 1}^{t} b^t_k \widehat{x_k \otimes x_k}$
\ENDFOR
\end{algorithmic}
\end{algorithm}



