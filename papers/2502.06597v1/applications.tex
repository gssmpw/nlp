\section{Applications}
\label{sec:applications}
To demonstrate potential uses of \acronym, we highlight two applications:
\emph{private Gaussian density estimation}, a classical probabilistic 
technique, and \emph{private Adam optimization}, which is  common in deep 
learning.
%
The proofs for all theoretical results can be found in the appendix.

\subsection{Private Gaussian density estimation}
The maximum likelihood solutions to Gaussian density estimation from \iid data,
$x_1,\dots,x_t$ famously is $\widehat p(x) = \mathcal{N}(x;\;\mu_t,\Sigma_t)$, where 
$\mu_t=\frac{1}{t}\sum_{i=1}^t x_i$ is the \emph{sample mean} and 
$\Sigma_t=\frac{1}{t}\sum_{i=1}^t (x_i-\mu_t)(x_i-\mu_t)^\top$
is the \emph{sample covariance}.
%
Using the alternative identity $\widehat\Sigma_t=\frac{1}{t}\sum_{i=1}^t x_ix^\top_i-\mu_t\mu^{\top}_t$,
one sees that the task can indeed be solved in a continuous release setting and 
that only estimates of the first and second moments are required.

\begin{figure}[t]
    \centering\scriptsize
    \begin{subfigure}[c]{.48\linewidth}\scriptsize
        \includegraphics[width=\linewidth]{images/variance_vs_sigma_d=1,n=1000.pdf}
        \subcaption{$d=1$, $n=1000$}
    \end{subfigure}
    \begin{subfigure}[c]{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/variance_vs_sigma_d=100,n=1000.pdf}
        \subcaption{$d=100$, $n=1000$}
    \end{subfigure}
    \caption{Expected error of the estimated \emph{covariance matrix} with JME versus PP
    (with trivial factorizations). For $d=1$, JME consistently achieves quality better 
    than or on par with PP. 
    %
    For $d=100$, JME is preferable to PP only in the high privacy regime.
    %
    }
    \label{fig:covariance_error}
\end{figure}



To do so privately, we use the \emph{averaging} workload matrix $V=(a^t_i)$ with $a^t_i=\frac{1}{t}$ for $1\leq i\leq t$ and $a^t_i=0$ otherwise, and we compute private estimates $\widehat\mu := \widehat{VX}$ (private mean) and $\widehat\Sigma=\widehat{V(X\face X)} - \widehat{(VX)}\face \widehat{(VX)}$ (private covariance) using JME. 

Note that $\widehat\mu$ is simply the first-moment vector as above. It is therefore
unbiased and the guarantees of Theorem~\ref{thm:main} holds for it.
%
However, $\widehat\Sigma$ is not an unbiased estimate because in its computation
the noise within $\widehat{VX}$ is squared. 
%
However, it is possible to characterize the bias analytically and subtract it if required. 
%
We provide the exact expression for the bias in equation~\eqref{eq:JME_cov_bias} in the Appendix, where the debiased version is referred to as \textbf{JME (Debiased)}.

The expected approximation error of $\widehat\mu$ is identical to the one 
in Theorem~\ref{thm:main} with $A=V$ and $C_1=C_2=\text{I}$.
%
The following Theorem establishes the approximation quality of the covariance estimate. 
%an analog result for $\widehat\Sigma$.

\begin{restatable}[Private covariance matrix estimation with \acronym]{theorem}{theoremJMEwithbiascorrection}
\label{thm:JME_error_with_bias_correction}
%
Assume that all input vectors have norm at most $1$. % Denote by % $\widehat{\mu}$ and 
Let $\widehat{\Sigma}$ be the results of the above construction, where privacy is 
obtained by running JME with noise strength $\sigma$ and debiasing. Then it holds:
%
\begin{align}
\sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}\|_{\Fr}^2 &= (c_d d^2 + 2d + 2)\sigma^2 H_{n, 1} 
\\&\quad+ d(d+1)\sigma^4 H_{n, 2}, \notag
\end{align}
with $c_d$ as in \Cref{thm:main}, and $H_{n, m} := \sum\limits_{k = 1}^{n} \frac{1}{k^m}$.
\end{restatable}
% H_{n,2} \to \frac{\pi^2}{6}=1.64...

For comparison, we also analyze the case where the PP method is 
used to privatize the covariance matrix, $\widehat{\Sigma}_{\text{PP}} := V(\widehat{X} \face \widehat{X}) - (V\widehat{X} \face V\widehat{X})$, where $\widehat{X}$ are privatized entries of $X$.
%
Again, the resulting biased estimate can be explicitly debiased, see 
Appendix~\eqref{eq:PP_cov_bias} for the expression.

The following theorem states upper and lower bounds on the expected approximation error:
%
\begin{restatable}[Private covariance matrix estimation with PP]{theorem}{theoremPostPrwithbiascorrection}
\label{thm:PostPr_with_bias_corr}
Assume the same setting as for \Cref{thm:JME_error_with_bias_correction}. 
%
Let $\widehat{\Sigma}_{\text{PP}}$ be the result of the above construction, 
where privacy is obtained by running PP with noise strength $\sigma$ and debiasing. 
%
Then, for the expected error of % mean estimate is the same in~\eqref{eq:hatmu_quality}, and for 
the covariance matrix estimate it holds:
\begin{align}
     &d(d + 1 )\sigma^4 H_{n, 1} -d(d + 1)\sigma^4 H_{n, 2} \notag\\
     &\qquad + 2(d + 1)\sigma^2 H_{n, 1}- 2(d + 1)\sigma^2 H_{n, 3}, \notag
     \\
     &\leq\sup_{X \in \mathcal{X}}\mathbb{E}\|\Sigma - \widehat{\Sigma}_{\text{PP}}\|_{\Fr}^2
     \\
     &\leq d(d + 1)\sigma^4 H_{n, 1} -d(d + 1)\sigma^4 H_{n, 2} + 2(d + 1)\sigma^2 H_{n, 1}.\notag
\end{align}
\end{restatable}

\paragraph{Comparison.}
%
In the high privacy regime ($\sigma \gg 1$), the leading term for JME is $d(d+1)H_{n, 2} \sigma^4$, and for PP it is $d(d + 1)H_{n, 1} \sigma^4$. 
%
Given that $H_{n,1}=O(\log n)$, while $H_{n,m}< \pi^2/6$ for $m\geq 2$, 
the error introduced by PP is logarithmically worse than JME. 
%
Figure \ref{fig:covariance_error} shows a numerical plot that confirms this observation. 
%
Note that our results match the lower bounds established by \citet{dp_covariance_lower_bound}, 
who proved that private covariance estimation in the Frobenius norm requires $\Omega(d^2)$ samples.

\subsection{Private Adam optimization}

The \emph{Adam} optimizer~\citep{kingma2014adam}, has become a de-facto standard 
for optimization in deep learning.
%
The defining property of Adam is its update rule, 
$\theta_{i} \leftarrow \theta_{i-1} - \alpha m_i / (\sqrt{v_i} +\epsilon)$, 
where $\alpha$ is a learning rate, and $m_i$ and $v_i$ are exponentially 
running averages of computed model gradients and 
componentwise squared model gradients, respectively.
%
In the context of our work,  these quantities correspond to a weighted 
first-moment vector and the diagonal of the weighted second-moment matrix. 

Previous attempts to make Adam differentially private relied on postprocessing~\citep{anil2021large,  li2022large}, potentially with debiasing~\citep{dp_adam_is_not_adam}, \ie 
they privatized the model gradients and derived the squared values from these.
%
We demonstrate that JME's approach of privatizing both 
quantities separately can be a competitive alternative.
%
Algorithm~\ref{alg:jme_adam} in the Appendix shows the pseudocode. 
%

It contains some modifications compared to the original JME. 
%
In particular, we adjust JME to only estimate and privatize the diagonal of 
the second moment matrix, which reduces the runtime and memory requirements.
%
Interestingly, as the next theorem shows, having to estimate only the diagonal
elements of the covariance matrix does not reduce the problem's sensitivity, 
so privatizing the estimates remains equally hard.
%
This implies that our previous analyses, including the relation to the baselines, 
remain valid. 

\begin{theorem}\label{thm:Adam_sens}
The sensitivity of JME, with the whole second-moment matrix $(C_1 X, \sqrt{\lambda} C_2 X \face X)$, and with just the diagonal terms $(C_1 X, \sqrt{\lambda} C_2 X\!\circ\!X)$, are identical.
\end{theorem}

The proof can be found in the \cref{sec:adam_sens_proof}. It does not follow from \Cref{lem:jointsensitivity} and is significantly more intricate. 


As in the previous cases, JME's \emph{for free} property ensures that its 
expected approximation error of the first moment is identical to PP.
%
The following theorem establishes the expected approximation errors of 
both methods for the computed second moments (\ie the diagonal of the second moment matrix):

\begin{lemma}[Comparison of JME and PP for DP-Adam]
\label{lem:Comparison_Errors}
%
Let $D=(v_1,\dots,v_n)$ be the matrix of second moment estimates of the Adam algorithm.
Denote by $\widehat D=(\widehat v_1,\dots,\widehat v_n)$ the private estimate of $D$ computed by Algorithm~\ref{alg:jme_adam} with trivial factorization and noise strength $\sigma$, 
and let $\widehat D_{\text{PP}}$ be the analog quantity computed by DP-Adam with postprocessing.
%
Then it holds: %
\begin{align}
\mathbb{E}_{Z} \|\widehat D-D\|^2 &= 2d \sigma^2 \cdot \| A_2 \|_{\Fr}^2,\\
\sup_{X \in \mathcal{X}}\mathbb{E}_{Z} \|\widehat D_{\text{PP}}-D\|^2 
&= (2d \sigma^4 + 4 \sigma^2) \cdot \| A_2 \|_{\Fr}^2 %+ 4 \sigma^2 \cdot \| A_2 \|_{\Fr}^2 
\notag\\
&\quad+ \underbrace{d \sigma^4 \cdot \big\| \sum_{i} (A_2)_i \big\|_2^2}_{\text{bias}},
\end{align}
where $A_2$ is the workload matrix obtained from the coefficients 
of Adam's exponentially weighted averaging operations.
%
The term marked "bias" disappears if PP is debiased.
\end{lemma}

The proof of the first part follows directly from \cref{thm:main}. The error for PP is computed separately in \Cref{lem:PostPr_DP_Adam} in the appendix.

This lemma shows that as long as $\sigma \geq 1$, the error introduced by JME is strictly lower than that of PP (\ie classic DP-Adam~\citep{li2022large}) and even the debiased version of PP (\ie DP-AdamBC~\citep{dp_adam_is_not_adam}). \Cref{fig:DP-Adam_error} illustrates the relation 
in a prototypical case.

\begin{figure}[t]
    \centering\scriptsize
    \begin{subfigure}[c]{.48\linewidth}\scriptsize
        \includegraphics[width=\linewidth]{images/DP-Adam_d=10_6,n=1000_without_MF.pdf}
        \subcaption{trivial factorization} %\scriptsize$d\!=\!10^6$, $n\!=\!1000$, $C_1\!=\!C_2\!=\!\text{I}$}
    \end{subfigure}
    \begin{subfigure}[c]{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/DP-Adam_d=10_6,n=1000_with_MF.pdf}
        \subcaption{square root factorization}
    \end{subfigure}
    \caption{Expected error of the estimated covariance vector for Adam with JME versus PP
    ($d=10^6$; $n=1000$). JME provides a better estimate in the mid to high privacy regime.}
    \label{fig:DP-Adam_error}
\end{figure}

\section{Experiments}
Our main contributions in this work are both algorithmic and theoretical. Specifically, 
\acronym is the general purpose technique for moment estimation, which is promising for some scenarios and less promising for others. 
%
However, it is also a \emph{practical} algorithm that can be easily implemented
and integrated into standard machine learning pipelines. 
%
To demonstrate this, we report on the experimental result of using \Cref{alg:JME} and \Cref{alg:jme_adam} in two exemplary settings, reflecting the application scenarios described above.

\paragraph{Private Gaussian density estimation.}
%
From a given data distribution, $p(x)=\mathcal{N}(\mu,\Sigma)$, we sample $n=200$ data 
points and use either \acronym or PP to form a private estimates, $\widehat p_t(x)=\mathcal{N}(\widehat\mu_t,\widehat\Sigma_t)$, at each step $t=1,\dots,n$, of the continuous release process.
%
To ensure positive definiteness, we symmetrize \acronym's estimated covariance 
matrices and project them onto the positive definite cone. As a postprocessing 
operation, this does not affect their privacy.

\Cref{fig:covariance_over_steps} shows the results, with the approximation quality,
measured by the Kullback-Leibler (KL) divergence, $\text{KL}(\widehat p_t\|p)$, at each 
step, $t$, as average and standard deviation over 1000 runs, with 
$\mu \sim \mathcal{N}(0, \frac{1}{2}I_d)$ and $\Sigma \sim W_d\left(\frac{1}{2}I_d, 2d\right)$
(\ie a \emph{Wishart distribution}) in each case.
%
One can see that in the high-privacy regime (here: $\sigma = 2$), on average, 
\acronym achieves a better estimate of the true density than PP, with and without 
debiasing.
%

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{images/gaussian_density_d=5_n=100.pdf}
    \caption{Approximation quality of JME and PP for private Gaussian density estimation ($d=5, n=100$) as average and standard deviation over 1000 runs. In the high-privacy regime ($\sigma = 2$), \acronym achieves lower KL divergence than PP after  $\approx 10$ samples.}
    \label{fig:covariance_over_steps}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[c]{.48\linewidth}\scriptsize
    \includegraphics[width=\linewidth]{images/CIFAR10_dp_adam_accuracy_B=256_sigma=1.pdf}
    \caption{batchsize $B\!=\!256$, $\varepsilon \approx 1.7$}
    \end{subfigure}
    \begin{subfigure}[c]{.48\linewidth}\scriptsize
    \includegraphics[width=\linewidth]{images/CIFAR10_dp_adam_accuracy_B=1_sigma=2.pdf}
    \caption{batchsize $B\!=\!1$, $\varepsilon \approx 0.16$}
    \end{subfigure}
    
    \caption{Results of private Adam training on CIFAR-10 experiments comparing four methods: DP-Adam based PP (with and without debiasing), \acronym, and joint clipping, over 10 epochs. 
    Left: for low to medium privacy, JME outperforms vanilla DP-Adam but is slightly worse than the debiased version. Right: for high privacy, \acronym is slightly better than the debiased version, where as vanilla DP-Adam cannot handle the necessarily amounts of noise.}
    \label{fig:cifar_10}
\end{figure}

\paragraph{Private model training with Adam.}
%
We train a convolutional network on the CIFAR-10 dataset with DP-Adam, 
which is privatized either with JME or PP.
%
Because gradients could have arbitrarily large number, we apply gradient clipping
to a model-selected threshold in both methods. 
%
In addition, we include a heuristic baseline that uses the concatenate-and-split 
techniques in which not the norm of the gradient vector but the norm of the 
concatenation vector is clipped. 
%
While inferior to \acronym in the worst case, this might be beneficial for real data,
so we include it in the experimental evalution.

%
The results in Figure \ref{fig:cifar_10} confirm our expectations:
%
in a setting with small noise variance (large batchsize, low privacy), 
DP-Adam-JME achieved better results than standard DP-Adam, but worse 
than the debiased variant of DP-Adam.
%
If the noise variance is large (small batchsize, high privacy), DP-Adam-JME slightly improves over the other methods.
%
For detailed accuracy results, see Table \ref{tab:accuracy_results} 
in the appendix with hyperparameters provided in Table \ref{tab:hyperparameters}.
%

