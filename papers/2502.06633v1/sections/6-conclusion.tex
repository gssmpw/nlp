\section{Conclusion}
\label{sec:conclusion}


In this paper, we explored hybrid approaches that combine knowledge-based systems (KBS) with large language models (LLMs) to enhance the automation of code review. By integrating KBS-derived insights at three stages—data preparation (Data-Augmented Training, DAT), inference (Retrieval-Augmented Generation, RAG), and post-inference (Naive Concatenation of Outputs, NCO)—we aimed to leverage the strengths of both KBS and learning-based systems (LBS) to generate more accurate and comprehensive code review comments.
Our empirical evaluation showed that combination approaches offer distinct advantages. RAG emerged as the most effective, improving both accuracy and coverage of review comments by enriching the generation process with structured, contextually relevant knowledge from static analysis tools. DAT achieved broad coverage by exposing the LLM to diverse issue types in training, sometimes at the expense of precision. NCO, while straightforward, achieved moderate improvements in coverage. 

These findings underscore the potential of combining static analysis tools with LLMs to address the limitations of individual approaches in automated code review. Future work will involve exploring more sophisticated integration of knowledge into open-weight LLMs. We also plan to expand this methodology to additional programming languages and exploring bytecode-level analysis for greater depth. Furthermore, the integration of more advanced static analyzers and dynamic analysis tools could further enhance coverage and precision, ultimately contributing to a more robust and versatile code review automation pipeline.



