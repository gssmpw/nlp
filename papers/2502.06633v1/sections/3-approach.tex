\section{Proposed approach}
\label{sec:approach}

For the task of review comment generation, Knowledge-Based Systems (KBS) draw on codified rules and expert knowledge to deliver feedback that is consistent with established coding standards and best practices. Static analyzers, a prominent example of KBS, systematically follow predefined guidelines to detect code issues, offering reliable and structured feedback. While KBS achieve high precision, they are limited in scope, covering only a subset of possible issues encountered during code changes. In contrast, Learning-Based Systems (LBS) harness the adaptive potential of language models, which, by training on historical data, can recognize intricate patterns and generate contextually relevant review comments. This adaptability allows LBS to cover a broader range of issues present in the dataset, though often at the expense of precision. In this work, we conjecture that by combining these two strategies, it is possible to achieve the best of both approaches, namely, broader issue coverage coupled with improved precision.

\subsection{Overview}

Figure~\ref{fig:combination} illustrates our approach, outlining three strategies to combine Knowledge-Based Systems (KBS) and Learning-Based Systems (LBS) to enhance code review automation. 

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=1\linewidth]{figures/combination.png}
  \caption{Different strategies to combine learning and knowledge-based systems}
  \label{fig:combination} 
\end{figure}

These strategies leverage KBS insights at different stages of the LBS pipeline, specifically during data preparation, inference, and final output. The three strategies are as follows:

\begin{itemize} 
    \item \emph{Data-Augmented Training (DAT)}: In this strategy, we enhance the training dataset by augmenting a real-world dataset with synthetic data generated from both KBS and LBS. This enriched dataset is then used to fine-tune a language model, enabling the LBS to incorporate both data-driven patterns and rule-based knowledge. This combination helps the model gain a more comprehensive understanding of code review patterns, improving robustness in varied review scenarios.
    \item \emph{Retrieval-Augmented Generation (RAG)}: Here, KBS insights are integrated directly into the LBS inference process. Through RAG, relevant information is dynamically retrieved from KBS (i.e., static analysis results) and injected into the prompts during generation. By incorporating the results of the KBS into the instruction, the LBS aligns its responses with established coding standards and practices, providing feedback grounded in structured, rule-based knowledge.
    \item \emph{Naive Concatenation of Outputs (NCO)}: This strategy merges the feedback generated by KBS and LBS after inference, combining their outputs to produce a unified code review. By consolidating KBS’s rule-based precision with LBS’s contextual depth, NCO offers a comprehensive review that covers a broader range of potential issues.
\end{itemize}

These strategies allow the LBS to benefit from the structured, rule-based insights of KBS, enhancing its ability to generate accurate, contextually appropriate, and standards-compliant code review comments.


\subsection{Baseline Model Preparation and Static Analyzers Selection}

While our approach is applicable to a wide range of LLMs and static analysis tools, we propose a specific configuration to illustrate the three strategies and establish the baselines for validating our conjecture. To set up the baseline systems, we first defined the LBS. We fine-tuned a large language model on an extensive code review dataset \cite{li2022automating}, referred to as \(\mathcal{D}_{\mathcal{M}_i}\), which pairs code changes with detailed reviews.

The selected model for fine-tuning is \emph{CodeLlama-7b}, trained for comment generation (i.e., generating review comments from code changes) with the following hyperparameter settings. The training was conducted on four \emph{NVIDIA RTX 3090} GPUs, using a batch size of $4$ per device. To boost efficiency, we applied gradient accumulation with a step size of $4$, updating the optimizer only after multiple batches. We used 4-bit quantization to improve memory and computational efficiency. Additionally, we employed Quantized Low-Rank Adaptation (QLoRA) \cite{hu2021lora}, a Parameter-Efficient Fine-Tuning (PEFT) technique, with $r = 16$, $\alpha = 32$, and $dropout = 0.05$. This method decomposes weight updates into low-rank matrices, reducing the parameters needed for fine-tuning and optimizing training efficiency \cite{hu2021lora}.

This resulted in a model, denoted as \(\mathcal{M}_i\), capable of generating detailed human-like code reviews.
Since \(\mathcal{M}_i\) represents the LBS component and was trained using the data from \(\mathcal{D}_{\mathcal{M}_i}\), we used the test set to generate reviews by both static analyzers and the fine-tuned model \(\mathcal{M}_i\).

To focus on a relevant subset of available static analyzers, we filtered the test set to include only Java code samples, producing a subset of $27,267$ entries, denoted \(\mathcal{D}_o\). Each entry in \(\mathcal{D}_o\) is a tuple \( (f,c) \), where \( f \) represents the source code file and \( c \) denotes the code change. Here, \( c \) is input to the LBS, while \( f \) serves as input to the KBS.
We limited our selection of static analyzers to tools that process Java source code directly. Although this decision excludes tools designed for Java bytecode analysis, it allows for a broader range of issue types. Specifically, we selected two well-established static analyzers: PMD \cite{pmd} and Checkstyle \cite{checkstyle}, both of which are designed to identify potential issues directly in source code.

PMD is a static code analysis tool that identifies issues in code by applying a set of rules aimed at detecting common problems, which are categorized into eight groups: best practices, coding style, design, documentation, error-prone, multi-threading, performance, and security \cite{lenarduzzi2023critical}. By analyzing source code against these rules, PMD generates detailed reports highlighting areas for improvement and enables users to create custom rules for specific analyses \cite{oskouei2018comparing}.

Checkstyle is another static code analysis tool for Java that offers predefined style configurations for standard checks, including Google Java Style and Sun Java Style. Its rules cover various aspects such as annotations, class design, coding, and naming conventions. Checkstyle also supports custom configuration files tailored to user needs~\cite{hovemeyer2004finding, balachandran2013reducing, oskouei2018comparing, lenarduzzi2023critical}.




\begin{figure}[!htbp]
\begin{subfigure}{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/approaches/approach-FTr.pdf}
  \caption{Data augmented training}
  \label{fig:approach1}
\end{subfigure}
\\ \vspace{10pt}
\begin{subfigure}{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/approaches/approach-RAG.pdf}
  \caption{Retrieval augmented generation}
  \label{fig:approach2}
\end{subfigure}
\\ \vspace{10pt}
\begin{subfigure}{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/approaches/approach-Concat.pdf}
  \caption{Naive concatenation of outputs}
  \label{fig:approach3}
\end{subfigure}
\caption{Proposed strategies to combine LBS and KBS}
    %\\The forward arrows represent the forward pass of the model, the circled shapes represent data (inputs, outputs, and ground truth), and the double arrows represent the losses.}
\label{fig:approaches}
\end{figure}





\subsection{Data Augmented Training}

As shown in Figure~\ref{fig:approach1}, this strategy involves retraining the LBS using an augmented dataset \({Da}\), which includes review comments generated by both, static analyzers and the fine-tuned model \(\mathcal{M}_i\). Through this retraining process, the LBS learns from both data sources, producing a more refined model referred to as \(\mathcal{M}_{FT}\).


A simple approach to augmenting the dataset would have been to apply static analysis to the code in \(\mathcal{D}_{\mathcal{M}_i}\) and add or concatenate the generated comments with the existing ones. However, this method does not guarantee data quality within the augmented dataset and fails to account for the insights inferred by the LBS \(\mathcal{M}_i\). 
Therefore, we employ an ensemble learning approach where the two distinct sources—the LBS and the KBS—serve as \emph{experts} to generate data for fine-tuning a model. The underlying rationale is that both KBS and LBS reviews are inherently synthetic. By combining their outputs, we achieve a more balanced and consistent fine-tuning process.

To produce the augmented dataset \(\mathcal{D}_a\), we designed a two-step process (i.e., \emph{data generation} and \emph{data filtering}), as depicted in Figure~\ref{fig:approach}.


\begin{figure}[htbp!]
  \centering
  \includegraphics[width=\linewidth]{figures/dataset.png}
  \caption{Dataset augmentation pipeline}
  \label{fig:approach} 
\end{figure}


In the Data Generation phase, we used the original dataset \(\mathcal{D}_o\) as input. For each code change \( c \), we employed our fine-tuned model \(\mathcal{M}_i\) to generate four context-aware, human-like reviews. Simultaneously, static analyzer rules were applied to each source code \( f \) to produce structured and precise feedback. Each static analyzer generated a report containing several reviews, including the start and end sections of code where each issue was identified. Since our approach focuses on code changes, we extracted the code section highlighted in each review, adding a few context lines before and after each extracted segment.
We then merged the reviews generated by both the static analyzers and \(\mathcal{M}_i\) into a single, unified dataset. Each data point in the dataset consists of tuples in the form \( (f, c, r, t) \), where \( f \) represents the source code file, \( c \) is the code change, \( r \) denotes the review comment, and \( t \) indicates the method used to generate the review (either KBS or LBS). 
%Note that for the same code \( (f,c) \) in \(\mathcal{D}_o\), we may have multiple tuples—i.e., samples—each corresponding to a generated comment.

After data generation, we applied a systematic Data Filtering step to evaluate and refine the merged dataset, ensuring that only the most relevant and meaningful reviews were retained for each source code. While the fine-tuned model \(\mathcal{M}_i\) can generate context-aware, human-like reviews, its output may sometimes include irrelevant or less meaningful feedback, particularly when handling complex or ambiguous code changes. Similarly, static analyzers, although reliable, may produce output overloaded with false positives \cite{johnson2013don, aniche2020effectiveness}, making it difficult to separate significant concerns from noise.
Therefore, filtering both \(\mathcal{M}_i\)'s and the static analyzers' reviews was essential to maintain a dataset of high-quality, meaningful feedback. This filtering process involved rating each review based on its relevance to the corresponding code. The ratings provided a quantitative measure of the review quality generated by both static analyzers and \(\mathcal{M}_i\).


\begin{figure}[hbt!]
  \centering
  \includegraphics[width=1\linewidth]{figures/llama3_rating.png}
  \caption{Judgment of review comments using \emph{Llama3-70B}}
  \label{fig:llama3_rating}
\end{figure}


To ensure a fair and scalable rating system, we leveraged large language models, which have demonstrated remarkable performance in similar assessment tasks \cite{zheng2024judging, huang2024empirical, weyssow2024codeultrafeedback}. By using these models, we achieved a more accurate and consistent evaluation of each review, enhancing the dataset's quality and making it a valuable resource for fine-tuning. As shown in Figure~\ref{fig:llama3_rating}, we used \emph{Llama3-70b}, inputting the code and its corresponding reviews. We instructed the model to rate each review on a 10-point scale. A threshold rating of 8 was set, with only reviews surpassing this threshold retained in the final dataset.
After filtering for relevance and quality, we ensured that each comment exceeding the threshold was treated separately. For a source code \(f\) with \(n\) reviews \((r_1...r_n)\), we generated \(n\) distinct data points: \(<f,r_1>\),  \(<f,r_2>\),..., \(<f,r_n>\). Additionally, for each comment, we extracted and included the specific segment of code change related to the issue being addressed, the dataset was then structered as \(<c_1,r_1>\),  \(<c_2,r_2>\),..., \(<c_n,r_n>\).

To prevent overrepresentation of specific rules, we randomly discarded reviews associated with rules that have an excessively high number of reviews. Furthermore, to maintain a balanced dataset, we randomly discarded a subset of learning-based reviews, ensuring an equal distribution between knowledge-based and learning-based reviews.
 
The final dataset \(\mathcal{D}_a\) consists of $78,776$ samples, ensuring an equal representation of reviews generated by both KBS and LBS methodologies, as shown in Figure~\ref{fig:chart}. It also ensures a balanced distribution across all KBS rules. 
\begin{figure}[!htbp]
  \centering
  \vspace{-2em}
  \includegraphics[width=0.6\linewidth]{figures/chart.png}
  \caption{Distribution of LBS and KBS Reviews in Our Dataset}
  \label{fig:chart} 
\end{figure}


To obtain the \(\mathcal{M}_{FT}\) model, we partitioned \(\mathcal{D}_a\) dataset into three subsets: 80\% of the samples were assigned to the training set, while the remaining 20\% was equally divided, with 10\% designated for validation and 10\% for testing. Each subset maintained a balanced mix of LBS and KBS reviews. We then fine-tuned the CodeLlama-7b model on this dataset with QLoRA to optimize memory efficiency \cite{hu2021lora}. 



\subsection{Retrieval Augmented Generation}

Retrieval-Augmented Generation (RAG) is a technique designed to enhance the generative capabilities of language models by incorporating external knowledge into their prompts during the inference phase \cite{jiang2023active}. In the context of code review, this strategy can be used to embed KBS-generated feedback directly into the prompts of a language-based system, as shown in Figure~\ref{fig:approach2}.



In our approach, the fine-tuned model \(\mathcal{M}_i\) takes as input the code changes from the \(\mathcal{D}_a\) dataset, along with outputs from PMD and Checkstyle. Incorporating KBS knowledge into the prompt guides the model to produce more relevant and precise reviews. This combination ensures that the generated review comments are both comprehensive and contextually informed. As a result, the reviews generated align closely with established coding standards and best practices, thereby enhancing their overall quality. The augmented prompt is illustrated in Figure~\ref{fig:rag_prompt}.

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/rag_prompt.png}
  \caption{Prompt used to generate review comments using RAG}
  \label{fig:rag_prompt}
\end{figure}



\subsection{Naive Concatenation of Outputs}

The Naive Concatenation of Outputs strategy serves as a baseline approach in which review comments generated separately by the LBS and KBS for the same code are combined to form a single review. As illustrated in Figure~\ref{fig:approach3}, the review comment generated by \(\mathcal{M}_i\) is directly concatenated with the output from the static analyzer (either PMD or Checkstyle).
This approach is straightforward, requiring minimal adjustments to the inference pipeline while ensuring that the final review delivers comprehensive feedback from both systems.











