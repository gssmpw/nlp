\section{Evaluation}
\label{sec:evaluation}

In this section, we first define the research questions that guide our study, followed by a detailed description of the experimental setup. We then present and interpret the results.
The replication package and the data of our experiments are available online \cite{github_replication, zenodo_14061110}.

\subsection{Research Questions}
The goal of our evaluation is to determine whether integrating knowledge into the LBS enhances the accuracy of generated comments while maintaining adequate coverage compared to using only the LBS or KBS. To structure our evaluation, we define the following research questions:

\textbf{RQ1: Manual evaluation of accuracy:} 
\emph{How accurate are the reviews generated by our hybrid approaches compared to those produced by the baseline?}

To address this question, we perform a manual assessment involving two reviewers on a sample of 10\% of the test set, comparing the accuracy of comments generated by our hybrid approaches against those produced by a fine-tuned language model and static analyzers.

\textbf{RQ2: Alignment of LLM judgments with human assessments (sanity check):}
\emph{To what extent do LLM judgments align with human assessments?}

Given the size of the test set and the five different comment generation options to evaluate, human assessment is impractical for the entire dataset. As an alternative, we employ an LLM to perform assessments across the whole test set. To evaluate the LLM's ability to mimic human judgment, we replicate the manual assessment procedure used for RQ1, substituting the human reviewers with the LLM and measuring the level of agreement between LLM and human evaluations.

\textbf{RQ3: LLM evaluation of generated reviews:}
\emph{How does the accuracy of the reviews generated by our hybrid approaches compare to those produced by the baseline when evaluated by an LLM?}

We use the LLM-as-a-judge approach to assess the accuracy of the generated reviews, comparing results from our hybrid approaches to those from the baseline, specifically the learning-based models and static analyzers.

\textbf{RQ4: Ranking of reviews based on coverage:}
\emph{How do our hybrid approaches compare to the baseline in terms of  coverage?}

For this question, we use an LLM-as-a-judge to conduct comparative evaluations, ranking reviews generated by our approaches and the baseline according to coverage criteria.





\subsection{Experimental Setup}
To address our research questions, we conducted a series of experiments. To enable comparison across the different hybrid approaches, we used the test set from our augmented dataset, \(Da\), ensuring that all entries were unseen by the fine-tuned model \(\mathcal{M}_{FT}\). 
Since the retrieval-augmented generation and naive concatenation of outputs approaches are applicable only to code changes with both KBS and LBS reviews, we filtered the test set to include only samples containing reviews from both sources. As illustrated in Figure~\ref{fig:overlap}, we examined the code changes in the selected test set, creating a unified code difference for analysis whenever overlaps were found. For example, if the test set contains a code difference $diff_1$ with an LBS review comment $r_1$ and a code difference $diff_2$ having a KBS review comment $r_2$, we merged both code differences into a unified difference, $diff_u$, as shown in Figure~\ref{fig:overlap}, forming a new triplet ($diff_u$, $r_1$, $r_2$) in our test set. Out of the $7,884$ total samples, we identified $1,245$ common code differences that included both KBS and LBS reviews.

For each of these selected code changes, we generated five types of reviews: the static analyzer review, the review generated by \(\mathcal{M}_i\), the review generated by \(\mathcal{M}_{FT}\), the retrieval-augmented generation review and the naive concatenated review.


For RQ1, we randomly selected $10\%$ of the $1,245$ test set examples for a manual evaluation focused on accuracy. All reviews were anonymized and presented to the evaluators in a randomized order. This approach minimizes confirmation bias, preventing evaluators from unconsciously favoring certain methods by being unaware of the source of each review.

Two evaluators, each with good expertise in code review, assessed each example by classifying reviews as \emph{accurate}, \emph{partially accurate}, or \emph{not accurate}. A review was considered accurate if it correctly addresses issues without any errors or irrelevant information. If only some parts of the review are valid, and the rest are irrelevant or incorrect, it was deemed partially accurate. Finally, a review was classified as not accurate if it fails to address the identified issues and is completely irrelevant to the context. After the initial assessments, a few conflicts were identified and resolved through discussion, leading to a consensus on the final evaluations.

For RQ2, we conducted a sanity check to evaluate the ability of LLMs, specifically \emph{Llama3-70B}, to assess review comments reliably and accurately. Using the same 10\% subset from RQ1, we tasked \emph{Llama3-70B} with categorizing each review as \emph{accurate}, \emph{partially accurate}, or \emph{not accurate}. We then measured the agreement between the ratings of human evaluators and those of \emph{Llama3-70B} by calculating Cohen’s kappa, a statistical measure that quantifies inter-rater reliability for categorical data \cite{thakur2024judging, mchugh2012interrater}. This analysis provides insight into the alignment between LLM and human judgments, helping validate the reliability of LLMs in accurately assessing review comments.

To address RQ3, we instructed \emph{Llama3-70B} to evaluate the code changes across the full dataset of $1,245$ samples, categorizing each review comment into one of three categories: \emph{accurate}, \emph{partially accurate}, or \emph{not accurate}.

For RQ4, We conducted a comparative evaluation by ranking the generated reviews based on their coverage, which measures how effectively the smaller, fine-tuned LLM detects a wide range of code issues. To assess this, we used the larger LLM, \emph{Llama3-70B}, as a reference point, assuming it can identify a comprehensive set of issues. The objective was to evaluate how well the smaller model aligns with this benchmark by capturing as many of these issues as possible. The ranking process reflects the relative effectiveness of each model in identifying coding issues, with Rank 1 assigned to the most comprehensive reviews and Rank 5 to the least.



We then analyzed the \emph{win}-\emph{tie}-\emph{loss} ratios to compare the performance of our three proposed strategies (\emph{DAT}, \emph{RAG}, \emph{NCO}) against the baseline models, $\mathcal{M}_i$ and the static analyzer. A \emph{win} was recorded if a review was ranked at least two levels higher than the baseline, indicating substantially superior coverage, while a \emph{loss} was recorded when a review ranked at least two levels lower, reflecting significantly weaker coverage. Rankings within ±$1$ level were classified as a \emph{tie}, as the difference in coverage was considered minor. This approach ensured a more meaningful evaluation by emphasizing significant differences in issue coverage rather than minor variations. 



\begin{figure}[hbt!]
  \centering
  \includegraphics[width=\linewidth]{figures/overlap.png}
  \caption{Merging LBS and KBS Changes into a Union Diff}
  \label{fig:overlap} 
\end{figure}



\subsection{Results}

\subsubsection{Results for RQ1 - Manual evaluation of reviews' accuracy}

The manual evaluation results in Figure~\ref{fig:manual_judge} reveal accuracy differences among the various code review generation approaches. As expected, static analyzers (KBS) consistently demonstrate reliability in producing accurate reviews, whereas the LBS model \(\mathcal{M}_i\) yields less accurate reviews, leading to a higher proportion of partially accurate or inaccurate feedback.
Our hybrid approaches exhibit diverse accuracy levels compared to the baseline systems (i.e., KBS and LBS). Notably, the RAG approach significantly outperforms \(\mathcal{M}_i\) in terms of accuracy. Meanwhile, the fine-tuned model \(\mathcal{M}_{FT}\) produced through the data-augmented training (DAT) approach, and the concatenation approach (NCO) achieve accuracy levels similar to that of \(\mathcal{M}_i\).

In response to \textbf{RQ1}, we conclude that an LLM utilizing the RAG approach generates comments with notably higher accuracy than when used alone. This improvement in accuracy is substantial, although it still falls short of the accuracy achieved by static analyzers. We did not observe an improvement in accuracy for the two other combination approaches. 




\begin{figure}[htbp!]
  \centering
  \includegraphics[width=\linewidth]{figures/accracy_dist.png}
  \caption{Accuracy levels for the different models based on human assessments}
  \label{fig:manual_judge} 
\end{figure}

\subsubsection{Results for RQ2 - Alignment between manual and LLM evaluations}

The alignment between the ratings of human evaluators and those provided by \emph{Llama3-70B} was measured using Cohen's kappa, yielding a score of \textbf{0.72}. This score indicates substantial agreement, highlighting the LLM’s capability to accurately categorize reviews as "accurate," "partially accurate," or "not accurate." Such a high level of concordance suggests that \emph{Llama3-70B} closely mirrors human evaluations in assessing the quality of code reviews, reinforcing its reliability as a substitute for human judgment. Moreover, this degree of agreement aligns with findings from similar studies, further validating its effectiveness as an evaluator.

To answer \textbf{RQ2}, this substantial alignment between \emph{Llama3-70B} and human evaluations enables us to confidently employ the LLM as the evaluator for the entire test set, assessing both accuracy and coverage. Given the observed agreement, we trust that the model will deliver consistent and reliable evaluations across a broader dataset, allowing for efficient and scalable assessment without compromising on quality.



\subsubsection{Results for RQ3 -  LLM evaluation of reviews' accuracy}

We employed \emph{Llama3-70B} as an evaluator to assess the review comments generated by each of the five approaches on the test set comprising $1,245$ samples. Each sample included a code change along with five review comments, one from each approach. Figure~\ref{fig:llm_judge} presents the results of this evaluation.
The findings are consistent with those from the manual evaluation in RQ1. As expected, static analyzers (KBS) maintain the highest percentage of accurate reviews due to their deterministic nature in identifying recurring and straightforward coding issues. Similarly, the Retrieval-Augmented Generation (RAG) approach outperforms the original model \(\mathcal{M}_i\) although the improvement in accuracy is slightly less pronounced than what was observed in the smaller RQ1 sample.

The Naive Concatenation of Outputs (NCO) approach achieves comparable accuracy to \(\mathcal{M}_i\), demonstrating its capability to produce reliable reviews. In contrast, the Data-Augmented Training (DAT) approach yielded the lowest accuracy scores among the methods, but still the same levels of NCO and \(\mathcal{M}_i\).

In conclusion, the answer to \textbf{RQ3} is that the RAG approach offers the most significant accuracy improvement over using the LLM alone. The other combination methods, NCO and DAT, show accuracy levels comparable to the standalone LLM model, without notable gains.


\begin{figure}[htbp!]
  \centering
  \includegraphics[width=\linewidth]{figures/accracy_dist_llm.png}
  \caption{Accuracy levels for the different models based on LLM judgments}
  \label{fig:llm_judge} 
\end{figure}


\subsubsection{Results for RQ4 - Ranking of Reviews Based on Coverage Criteria Using LLM}
Figure~\ref{fig:rating_results} presents the cumulative ranking distribution for the five code review generation approaches: the baseline methods (KBS and LBS) and the combined approaches (DAT, RAG, and NCO). Rankings assess each approach's effectiveness in achieving comprehensive issue coverage, with Rank 1 indicating the highest coverage and Rank 5 the least.

For the baseline methods, the distribution aligns with expectations. The LBS model displays a broad spread across ranks, with the highest concentration in Rank 4 and less than half of the samples in Ranks 1, 2, and 3. This pattern reflects LBS's real-world training data, likely capturing the selective coverage patterns typical in code review datasets. As a result, LBS effectively identifies common issues but lacks the systematic, rule-based coverage characteristic of KBS. Static analyzers, in contrast, are predominantly ranked in Ranks 4 and 5 due to their rule-based approach, which, while precise, lacks adaptability and the capacity to detect a broader range of issues. This rule-bound structure restricts static analyzers to specific, predictable issue types, limiting their ability to provide comprehensive coverage as LBS might.

The DAT approach has a high proportion of reviews achieving Rank 1 (49\%), demonstrating strong coverage in these cases. Notably, the DAT distribution is bimodal, with relatively few reviews in Ranks 2, 3, or 4. This suggests that DAT either ranks at the top, addressing the majority of issues in a code change, or falls to Rank 5 when its coverage is limited. This polarized performance pattern in \(\mathcal{M}_{FT}\) after data augmentation shows that it either achieves comprehensive coverage or misses key areas, rarely achieving intermediate coverage levels.
RAG performs consistently well, with most reviews in Ranks 1 and 2 (70\%). This suggests that RAG reliably covers a substantial number of issues in code changes, with a notable share in Rank 2, indicating that it frequently provides strong, if not complete, coverage.

The NCO approach primarily appears in middle ranks, especially in Rank 3, indicating moderate coverage. This outcome reflects the straightforward concatenation process, which combines reviews from LBS and KBS without advanced integration. As a result, NCO's coverage is influenced by the quality of the LBS review; when LBS has low coverage, NCO’s coverage also tends to be limited. However, the addition of static analyzer reviews offers slight improvements, giving NCO somewhat broader coverage than LBS alone in the top three ranks.

In summary, DAT and RAG emerge as the top-performing approaches, together accounting for nearly 80\% of Rank 1 reviews, each excelling in one of the top two ranks. NCO demonstrates moderate performance, with Rank 3 as its most common position.

To enable a direct comparison between the combination approaches and baselines, we conducted a pairwise win-tie-loss analysis based on rankings. As shown in Figure~\ref{fig:wtl}, DAT has a high win rate against both LBS and KBS, highlighting its effectiveness in achieving broader coverage. RAG also performs well, consistently identifying a wide range of issues and frequently surpassing LBS in coverage. In contrast, while NCO positions favorably against KBS, it shows a higher tie rate with LBS, indicating that it generally achieves moderate coverage but does not consistently outperform LBS. Cases where NCO provides less coverage than KBS can arise when LBS-generated reviews contradict or override KBS output, reducing the overall coverage of the concatenated result. For example, in one instance, the KBS produced the following comment:
"Unused import 'com.google.common.collect.ImmutableList.'"
However, the LBS-generated review effectively invalidated this observation:
"The code change adds a new import statement for com.google.common.collect.ImmutableList. This is a good addition, as it allows the class to use the ImmutableList class, which can be useful for creating immutable lists."
In such cases, the conflicting feedback from LBS can diminish the impact of KBS findings, leading to lower coverage in the NCO approach.


To answer RQ4, we conclude that two of the combination approaches, DAT and RAG, significantly enhance the coverage of issues that can be automatically identified compared to the baselines.


\begin{figure}[htbp!]
  \centering
  \includegraphics[width=\linewidth]{figures/pourcentage.png}
  \caption{Distribution of Ranks Across Models}
  \label{fig:rating_results} 
\end{figure}



\begin{figure*}[htbp!]
  \centering
  \includegraphics[width=1\linewidth]{figures/win_tie_loss_v2.png}
  \caption{Win-tie-loss ratios of our proposed approaches compared to the baseline in terms of coverage. }
  \label{fig:wtl}
\end{figure*}

\subsection{Discussion}
Based on the results from RQ3 and RQ4, our baseline models performed as expected, each with distinct strengths and limitations. The knowledge-based system (KBS) demonstrated high accuracy in detecting rule-based issues, particularly for violations that could be systematically encoded. 
However, its coverage was limited, missing many complex issues that require a nuanced understanding of context, which cannot easily be captured through static rules alone. 

In cases where the KBS was deemed inaccurate, its reliance on predefined rules is often the primary cause, as these rules may generate irrelevant or outdated reviews. Since they do not always align with the specific context of the code being reviewed, they can lead to misjudgments. For example, rules based on fixed thresholds (e.g., maximum method length or parameter list size) may result in incorrect assessments, particularly when they fail to account for context-specific factors.

For example, as seen in Table~\ref{tab:exp}, KBS generated a review comment addressing a basic violation of naming conventions, which is straightforward but ranks lowest. In contrast, the learning-based system \(\mathcal{M}_i\), ranked fourth, identified a more subtle issue that would be challenging to encode in a rule-based analyzer, highlighting the advantage of LBS in identifying context-dependent issues.


The Data Augmented Training (DAT) approach expanded the coverage beyond \(M_i\) by exposing the model to a wider variety of issues, as the augmented dataset \(Da\) allowed the model to handle both LBS and KBS issues more effectively. This exposure enabled DAT to consider contextual factors and recognize project-specific nuances that KBS would typically overlook, while also incorporating rule-based issues where appropriate. For example, in Table~\ref{tab:exp}, DAT achieved the highest rank by generating a detailed and context-sensitive review comment that covered both rule-based and nuanced observations, reflecting its expanded coverage capabilities.

The Retrieval-Augmented Generation (RAG) approach enhanced both the accuracy and coverage of generated reviews by incorporating information retrieved from KBS. This retrieval step enriched RAG's feedback, making it more comprehensive and relevant. However, it still fell short of KBS’s precision on rule-based issues due to \(\mathcal{M}_i\)’s inherent inaccuracies during inference. In Table~\ref{tab:exp}, RAG’s comment, ranked second, provides a more detailed critique than the baseline models, yet it lacks the depth of DAT’s analysis.

The Naive Concatenation of Outputs (NCO) strategy did not notably improve accuracy and offered only a marginal increase in coverage. Since part of the NCO approach’s output is directly inherited from the LBS, its accuracy is inherently tied to the quality of the LBS-generated comments. If the LBS produces a partially accurate comment, the corresponding NCO output will also reflect this partial accuracy, as the NCO does not refine or correct LBS-generated reviews.

Table~\ref{tab:exp} illustrates an NCO’s generated comment, ranked third, integrated feedback from both baselines, but without advanced synthesis, resulting in a moderate improvement. Although NCO included the LBS and KBS comments, it was unable to address the more complex issues that DAT and RAG identified.

In summary, DAT and RAG stood out for their enhanced coverage, capturing nearly 80\% of Rank 1 assessments. NCO, by contrast, demonstrated only moderate performance, frequently achieving Rank 3 due to its limited integration of baseline insights. This analysis underscores the strengths of hybrid approaches in combining rule-based precision with learning-based adaptability to achieve comprehensive code review coverage.





\begin{table*}[!htbp]
\centering
\caption{Comparison of Review Comments. Excerpts of comments are shown for brevity.}
\label{tab:exp}
\begin{tabularx}{1\linewidth}{X}
\toprule
\multicolumn{1}{c}{\textbf{Code change}} \\
\midrule
@@ -38,5 +38,7 @@ public class GoogleAuthenticatorAccount extends OneTimeTokenAccount \{

                            @JsonProperty("validationCode") final int validationCode,
                            
                            @JsonProperty("scratchCodes") final List$\langle Integer\rangle$
                            scratchCodes) \{
                            
         super(username, secretKey, validationCode, scratchCodes);
         
+        long v =  java.lang.System.currentTimeMillis();

+        this.setId(v);
     \}
 \}
\\ \midrule
\multicolumn{1}{c}{\textbf{DAT}} \\ \midrule
\#\#\#\# Potential Issues:

- The setId() method uses `System.currentTimeMillis()` in the constructor, which could lead to issues if the object is not persisted before the constructor finishes.

- The `id` field should be marked as transient to avoid unnecessary persistence.

-  It is possible to remove the final keyword to allow subclasses to override the constructor.

\#\#\#\# Areas for Improvement:

- Consider adding a `toString()` method for better readability.

- Validate the constructor parameters (username, secretKey, etc.) to ensure they are not null or empty.
\\ \midrule
\multicolumn{1}{c}{\textbf{RAG}} \\ \midrule
The use of the short variable name `v` is discouraged. Consider using more descriptive names like `currentTimestamp` or `uniqueIdentifier`. Also, `System.currentTimeMillis()` may not be the most efficient way to generate a unique identifier; alternatives like `UUID.randomUUID()` or `SecureRandom.nextLong()` could be more robust.
\\ \midrule
\multicolumn{1}{c}{\textbf{NCO}} \\ \midrule
The addition of the `id` field is unnecessary as the parent class `OneTimeTokenAccount` already has an `id` field. Using the existing field would avoid redundancy and confusion.

Avoid using short variable names like `v`. 

\\ \midrule
\multicolumn{1}{c}{\textbf{LBS}} \\ \midrule
The addition of the `id` field is unnecessary as the parent class `OneTimeTokenAccount` already has an `id` field. Using the existing field would avoid redundancy and confusion.
\\ \midrule
\multicolumn{1}{c}{\textbf{KBS}} \\ \midrule
Avoid using short variable names like `v`. 
\\ \bottomrule

\end{tabularx}
\end{table*}


\subsection{Threats to Validity}
During our evaluation process, we identified several potential threats to the validity of our results. The most notable threats are discussed below.

Evaluator Bias: While human evaluations were conducted to assess accuracy in RQ1, there is a potential for evaluator bias. To mitigate this threat, we presented the reviews in an anonymized and randomized order to reduce any unconscious preference for particular methods.

Generalizability: Our study primarily focuses on Java code, using tools and datasets specific to the Java programming language. This may limit the generalizability of our findings to other languages and contexts, where static analyzers or learning-based systems could exhibit different strengths and weaknesses.

Dependency on LLM Judgment: For RQ3 and RQ4, we relied on an LLM (\emph{Llama3-70B}) to evaluate accuracy and coverage across the entire test set. Although Cohen’s kappa indicates substantial agreement with human evaluations on a subset of data, subtle variations in the LLM's judgment across the full dataset may not fully align with human assessments, potentially impacting reliability. To address this, in addition to the kappa score, we manually inspected a random subset of evaluations for RQ4 and found them consistent and reliable.

Choice of Static Analyzers: The effectiveness of the KBS approach depends on the static analyzers selected. We chose PMD and Checkstyle, which we believe are state-of-the-art tools; however, other static analyzers might yield different results. Our confidence in these tools is based on their widespread use and established effectiveness in rule-based code analysis.

Evaluation Metrics Limitations: The ranking system (win-tie-loss) used in RQ4 considers the number of levels above or below baseline models, but it may not capture subtle differences in review quality. To account for this, we adopted a conservative approach, requiring at least a two-level ranking difference for a “win,” to ensure that only substantial improvements were counted as wins.






% \begin{table*}[!htbp]
% \centering
% \caption{Caption. \obs{I think tabularx package is not imported} \obs{Since the comments are too long, I suggest to just include an excerpt and omit unnecessary details (e.g, 'Here is a review comment that highlights...') and mention this in the caption}}
% \label{tab:exp}
% \begin{tabularx}{1\linewidth}{|X|}
% \toprule
% \multicolumn{1}{c}{\textbf{Code change}} \\\midrule
% @@ -38,5 +38,7 @@ public class GoogleAuthenticatorAccount extends OneTimeTokenAccount {

%                             @JsonProperty("validationCode") final int validationCode,
                            
%                             @JsonProperty("scratchCodes") final List<Integer> scratchCodes) {
                            
%          super(username, secretKey, validationCode, scratchCodes);
         
% +        long v =  java.lang.System.currentTimeMillis();

% +        this.setId(v);
%      }
%  }
% % \makebox[\linewidth] 
% \\\midrule
% \multicolumn{1}{c}{\textbf{DAT}}\\\midrule
% \#\#\#\# Potential Issues:
% - The setId() method is called with a value that is the current time in milliseconds. However, the setId() is called in the constructor and the constructor is called before the object is persisted to the database. It is unlikely that the object will be persisted to the data store before the constructor is called.
% - The id field is not marked as transient. It is possible that this field could be marked as transients to avoid persisting this field to the data store.
% - The constructor is marked as final. It is possible to remove the final keyword to allow subclasses to override the constructor.
% \#\#\#\# Areas for Improvement:
% - Add a toString() method to the class to provide a human-readable representation of the object.
% - Add a method to the class that returns the object as a JSON representation.
% - Add validation to the constructor to ensure that the username, secretKey, validationCode, and scratchCodes are not null or empty.
% \midrule
% \multicolumn{1}{c}{\textbf{RAG}} \\\midrule
% The code difference you provided contains a new variable v with a short name, which is not recommended. The variable name should be descriptive and meaningful, and it should not be a single letter.
% Here's a review comment that highlights the potential issue and suggests improvements:
% "The use of short variable names like v may make the code harder to read and understand. Consider using more descriptive variable names, such as currentTimestamp or uniqueIdentifier. Additionally, using System.currentTimeMillis() to generate a unique identifier may not be the most efficient way to do so. Consider using a more robust method, such as UUID.randomUUID() or SecureRandom.nextLong()."  The code difference you provided shows a new variable v being added to the constructor of the User class. The variable is assigned the value of java.lang.System.currentTimeMillis(), which is a long integer.
% \midrule
% \multicolumn{1}{c}{\textbf{NCO}} \\\midrule
% This code change adds a new field to the GoogleAuthenticatorAccount class, which is not necessary. The id field is already present in the OneTimeTokenAccount class, and it is used to identify the account. Adding a new field that is not necessary can lead to confusion and make the code harder to maintain.
% Instead of adding a new field, the code could use the existing id field to identify the account. This would be a more efficient and effective way to identify the account, and it would avoid the potential confusion that can arise from having multiple fields that serve the same purpose.
% Avoid variables with short names like v
% \midrule
% \multicolumn{1}{c}{\textbf{LBS}} \\\midrule
% This code change adds a new field to the GoogleAuthenticatorAccount class, which is not necessary. The id field is already present in the OneTimeTokenAccount class, and it is used to identify the account. Adding a new field that is not necessary can lead to confusion and make the code harder to maintain.

% Instead of adding a new field, the code could use the existing id field to identify the account. This would be a more efficient and effective way to identify the account, and it would avoid the potential confusion that can arise from having multiple fields that serve the same purpose.
% \midrule
% \multicolumn{1}{c}{\textbf{KBS}} \\\midrule
% Avoid variables with short names like v

% \bottomrule

% \end{tabularx}
% \end{table*}






