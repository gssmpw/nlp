\section{Related work}
\label{sec:literature}

\subsection{Code review automation}
Several approaches were proposed to assist code review. Gupta et al. \cite{gupta2018intelligent} introduced an LSTM-based model trained on positive and negative (code, review) pairs, selecting candidate reviews based on code similarity and predicting relevance scores. Siow et al. \cite{siow2020core} enhanced this with multi-level embeddings, leveraging word-level and character-level representations to better capture the semantics of code and reviews.

With the advent of large language models, the focus has shifted toward generative models to fully automate code review tasks. Tufano et al. \cite{tufan2021towards} developed a transformer-based model to suggest code edits before and after code review, later enhancing it by pre-training T5 on Java and technical English~\cite{tufano2022using}. Li et al. \cite{li2022automating} pre-trained CodeT5 on a multilingual dataset and fine-tuned it for code review tasks like quality estimation, review generation, and code refinement. Sghaier et al. \cite{ben2024improving} further advanced this area by applying cross-task knowledge distillation to address successive code review tasks jointly, enhancing performance and promoting tasks' interdependence.

Current research efforts have significantly advanced the automation of code review, introducing different and innovative approaches that enhance code review tasks. However, the performance of these automated approaches remains limited in terms of correctness, as indicated by low BLEU scores, suggesting that further refinement is needed to achieve higher accuracy and reliability as expected in practical software development contexts.


\subsection{LLMs and static analysis combination}

Recent research has increasingly focused on enhancing LLM-based solutions for software engineering using several techniques. One is by integrating them with static analysis tools, addressing the challenge of reducing inaccurate or incomplete results. 

In automated program repair, RepairAgent \cite{bouzenia2024repairagent} employs static analysis to gather contextual data that guides LLM-driven code correction, while PyTy \cite{chow2024pyty} relies on type-checking mechanisms to validate the accuracy of LLM-generated candidates in resolving static type inconsistencies. For software testing, approaches like TECO \cite{nie2023learning} apply static analysis to derive semantic features for training transformers in test completion, while ChatTester \cite{yuan2023no} and TestPilot \cite{schafer2023adaptive} utilize similar techniques to prepare contextual information that supports iterative LLM-based test code repair processes. For bug detection, LLMs were combined with static analysis to reduce false positives. SkipAnalyzer \cite{mohajer2023skipanalyzer} and GPTScan~\cite{sun2024gptscan} use static analysis to validate LLM predictions, while D2A~\cite{zheng2021d2a} and ReposVul \cite{wang2024reposvul} refine bug labeling and re-rank predictions.
For code completion, STALL+ \cite{liu2024stall+} integrates static analyzers with LLMs through a multi-phase approach involving prompting, decoding, and post-processing.

These studies illustrate the effectiveness of combining LLMs with static analysis across tasks like program repair, bug detection, testing, and code completion. However, this integration has yet to be explored for code review.


% ==================================================================

% Research is increasingly focusing on improving LLM-based solutions for software engineering tasks by integrating them with static analysis tools and techniques. This combination addresses one of the key challenges in using LLMs by reducing the risk of generating inaccurate or incomplete results. For instance, in automated program repair, several approaches leverage static analysis to validate or improve LLM-generated patches. RepairAgent \cite{bouzenia2024repairagent} uses static analysis to extract relevant information that guides LLM-based repair suggestions, while PyTy \cite{chow2024pyty} employs a type checker to ensure that each candidate fix generated by the LLM resolves type-related issues. Repolit \cite{wei2023copiloting} integrates a code completion engine to refine patch generation dynamically.

% For software testing, researchers have developed methods that combine traditional test generation with LLMs to improve test coverage and accuracy. TECO \cite{nie2023learning} employs a transformer model trained for test completion, which relies on semantic features obtained through static analysis. Additionally, ChatTester \cite{yuan2023no} uses static analysis to provide contextual information for iterative test repair guided by LLMs. 

% In the area of bug detection, combining LLMs with static analysis has shown promise in reducing false positives. For example, SkipAnalyzer \cite{mohajer2023skipanalyzer} filters warnings produced by both the LLM and the Infer static analysis tool, and GPTScan \cite{sun2024gptscan} applies static analysis to validate key variables and statements recognized by the LLM, increasing confidence in detecting vulnerabilities. Additionally, D2A \cite{zheng2021d2a} leverages LLMs alongside differential analysis of bug-fixing commits to improve labeling and reduce false positives in static analysis outputs. Similarly, ReposVul \cite{wang2024reposvul} employs a combination of static analysis tools and LLMs to evaluate the relevance of code changes to vulnerability fixes. During model inference, static analysis is employed to adjust the model's predicted token probabilities. After model inference, static analysis is used to filter and re-rank these candidates.

% STALL+ \cite{liu2024stall+} integrates static analyzers with LLMs for code completion through a multi-phase approach involving prompting, decoding, and post-processing. Before inference, static analysis is used to extract useful code contexts from other files in the repository. 

% These studies demonstrate the effectiveness of combining LLMs with static analysis across various software engineering tasks, including program repair, bug detection, test generation, and code completion.

% \imene{can we say that here?} While these approaches underscore the potential benefits of integrating LLMs and static analysis, specific work on using this combination for code review has not yet been explored. 




