\begin{table}[!htbp]
  \centering
  \caption{Evolution of the categories across the curated dataset}
  \label{tab:categories_distribution}
  \begin{tabular}{>{\centering\arraybackslash}p{1.2cm} >{\centering\arraybackslash}p{2cm}*{2}{r}}
    \toprule
    \textbf{Category} & \textbf{Subcategory} & \textbf{Count} & \textbf{Percentage}\\
    \midrule
    \multirow{4}{*}{\textbf{Nature}} & Descriptive & 0.95 & 1 674\down \\
    & Prescriptive & 90.20 & 159 306\up \\
    & Clarification & 29 586 & 16.75\down \\
    & Other & 17 491 & 9.90\up \\
    \midrule
    \multirow{2}{*}{\textbf{Civility}} & Civil & 176 613 & 100\up \\
    & Uncivil & 0 & 0\down \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!htbp]
  \centering
  \caption{}
  \label{tab:judge}
  \begin{tabular}{>{\centering\arraybackslash}p{1.2cm} >{\centering\arraybackslash}p{2cm} r r r r r}
    \toprule
    \textbf{} & \textbf{Rank 1} & \textbf{Rank 2} & \textbf{Rank 3} & \textbf{Rank 4} & \textbf{Rank 5} \\
    \midrule
    \multirow{1}{*}{\textbf{FTr}} & 55 & 2 & 1 & 1 & 40 \\
    \midrule
    \multirow{1}{*}{\textbf{RAGr}} & 23 & 42 & 17 & 15 & 4 \\
    \midrule
    \multirow{1}{*}{\textbf{SAr}} & 1 & 16 & 54 & 21& 9\\
    \midrule
    \multirow{1}{*}{\textbf{LBr}} & 17 & 26 & 11 &33 & 14 \\
    \midrule
    \multirow{1}{*}{\textbf{CONCATr}} & 3 & 13 & 18 & 32 & 33\\
    \bottomrule
  \end{tabular}
\end{table}

Traditionally, deep learning models are trained on datasets composed of human-written code reviews, typically sourced from platforms like GitHub, Gerrit, or issue trackers. These datasets reflect the expertise and practices of developers, offering insights into common bugs, best practices, and areas for improvement. However, such datasets are often empirical in nature, lacking the structured, rule-based rigor provided by formal static analysis tools.
To address this limitation, we introduce a fine-tuning approach that combines reviews from two distinct sources: knowledge-based systems (KBS) and learning-based systems (LBS). KBS reviews, generated by static analysis tools, are rule-based and focus on detecting coding violations and inefficiencies in a systematic, consistent manner. In contrast, LBS reviews are derived from machine learning models trained on large-scale datasets of code and developer feedback, providing more flexible, data-driven insights.

\begin{table}[!htbp]
  \centering
  \caption{}
  \label{tab:judge}
  \begin{tabular}{>{\centering\arraybackslash}p{1.2cm} >{\centering\arraybackslash}p{2cm}*{2}{r}}
    \toprule
    \textbf{} & \textbf{Accurate} & \textbf{Partially accurate} & \textbf{Not accurate}\\
    \midrule
    \multirow{1}{*}{\textbf{FTr}} & 510 & 352 & 383 \\
    
    \midrule
    \multirow{1}{*}{\textbf{RAGr}} & 832 & 355 & 58 \\
    \midrule
    \multirow{1}{*}{\textbf{SAr}} & 1178 & 47 & 20 \\
    \midrule
    \multirow{1}{*}{\textbf{LBr}} & 681 & 274 & 290 \\
      \midrule
    \multirow{1}{*}{\textbf{CONCATr}} & 657 & 307 & 281 \\
  \end{tabular}
\end{table}

\begin{table}[!htbp]
  \centering
  \caption{}
  \label{tab:judge}
  \begin{tabular}{>{\centering\arraybackslash}p{1.2cm} >{\centering\arraybackslash}p{2cm}*{2}{r}}
    \toprule
    \textbf{} & \textbf{Accurate} & \textbf{Partially accurate} & \textbf{Not accurate}\\
    \midrule
    \multirow{1}{*}{\textbf{FTr}} & 47 & 31 & 22 \\
    
    \midrule
    \multirow{1}{*}{\textbf{RAGr}} & 70 & 4 & 26 \\
    \midrule
    \multirow{1}{*}{\textbf{SAr}} & 97 & 1 & 2 \\
    \midrule
    \multirow{1}{*}{\textbf{LBr}} & 46 & 29 & 25 \\
      \midrule
    \multirow{1}{*}{\textbf{CONCATr}} & 47 & 28 & 25 \\
  \end{tabular}
\end{table}





The authors' approach in creating the D2A dataset and using it for vulnerability detection and false positive reduction can be summarized in a clear, step-by-step manner:

1. Dataset Creation
Source Selection: The authors selected multiple open-source projects to analyze.
Version Pair Analysis: They focused on bug-fixing commits, running static analysis tools on the versions of the code before and after these commits.
Differential Analysis: By comparing the static analysis results from the before-commit and after-commit versions, they identified issues that disappeared after a commit. If an issue detected in the before-commit version was no longer present in the after-commit version, it was likely a true bug that had been fixed.
Labeling: This process allowed them to label a large dataset of issues as either likely true positives (real bugs) or false positives (non-issues) based on the analysis of commit history and static analysis outputs.
2. Model Training
Feature Extraction: The authors defined features based on the outputs of static analysis, including characteristics of the detected issues and the context provided by commit history.
Machine Learning Algorithms: They trained various machine learning models (e.g., Random Forest, Extra Trees) on the labeled dataset to learn patterns that distinguish true positives from false positives.
Training Process: The models were trained using the extensive labeled dataset, which included over 1.3 million examples, allowing them to generalize well across different projects.
3. Prediction and Prioritization
Model Predictions: The trained model predicts the likelihood that a reported issue is a true positive. It assesses each issue reported by static analysis tools and assigns a probability score indicating its likelihood of being a real bug.
False Positive Reduction: By identifying issues that are more likely to be true positives, the model helps developers prioritize their investigations. This means developers can focus on the most promising issues first, improving their efficiency and trust in static analysis tools.
4. Evaluation of Effectiveness
Performance Metrics: The authors evaluated the model's performance using metrics such as the False Positive Reduction Rate (FPRR). They reported significant reductions in false positives, often exceeding 70%, across various projects.
Practical Application: The approach not only aids in vulnerability detection but also enhances the usability of static analysis tools by reducing the noise created by false positives.
In summary, the authors' approach combines differential analysis of code versions with machine learning techniques to create a robust dataset and model that effectively identifies and prioritizes real software vulnerabilities while minimizing false positives.





To effectively implement the strategies outlined, we developed a unified dataset that integrates both LBS-generated reviews and KBS-generated feedback for a wide range of code changes. This dataset, denoted as  $\mathcal{D}$, serves as the foundational resource for our hybrid approach. The dataset  $\mathcal{D}$ consists of tuples in the form \( (f, c, r, t) \), where \( f \) represents the initial version of the code file, \( c \) is the code change, \( r \) denotes the review comment, \( t \) indicates the method used for generating the review (either KBS or LBS).

\begin{table}[!htbp]
  \centering
  \caption{}
  \label{tab:judge}
  \begin{tabular}{>{\centering\arraybackslash}p{1.2cm} >{\centering\arraybackslash}p{2cm}*{2}{r}}
    \toprule
    \textbf{} & \textbf{Accurate} & \textbf{Partially accurate} & \textbf{Not accurate}\\
    \midrule
    \multirow{1}{*}{\textbf{DAT}} & 41 & 28 & 31 \\
    
    \midrule
    \multirow{1}{*}{\textbf{RAG}} & 67 & 29 & 5 \\
    \midrule
    \multirow{1}{*}{\textbf{KBS}} & 95 & 4 & 2 \\
    \midrule
    \multirow{1}{*}{\textbf{LBS}} & 53 & 25 & 23 \\
      \midrule
    \multirow{1}{*}{\textbf{NCO}} & 55 & 22 & 23 \\
  \end{tabular}
\end{table}












\begin{table*}[!htbp]
\centering
\caption{Caption.}
\label{tab:exp}
\begin{tabularx}{1\linewidth}{|X|}
\toprule

\multicolumn{1}{c}{\textbf{Code change}} \\\midrule
\makebox[\linewidth]{\includegraphics[width=0.6\linewidth]{figures/code_snippet3.png}} \\\midrule
\multicolumn{1}{c}{\textbf{Original dataset}}\\\midrule

content\midrule

\multicolumn{1}{c}{\textbf{Curated dataset}} \\\midrule

content\bottomrule

\end{tabularx}
\end{table*}






\begin{table*}[!htbp]
\centering
\caption{Caption.}
\label{tab:exp}
\begin{tabularx}{1\linewidth}{|X|}
\toprule

\multicolumn{1}{c}{\textbf{Code change}} \\\midrule
package io.confluent.ksql.util;

import java.util.Set;

+/**
 * Interface get a set of incompatible elements for an object
 * @see CompatibleSet
 */
-public interface CompatibleElement<T extends CompatibleElement> {
+public interface CompatibleElement<T extends CompatibleElement<T>> {
  Set<T> getIncompatibleWith();
}

\makebox[\linewidth] \\\midrule
\multicolumn{1}{c}{\textbf{DAT}}\\\midrule
* The code change makes the `CompatibleElement` interface generic, allowing the `T` parameter to be used in the `getIncompatibleWith()` method's return type. This change is safe and does not introduce any new bugs or issues.

* However, the code change does not introduce any type safety, as the `T` type parameter is not used in any of the methods of the interface. This means that any object implementing the interface can return any type of `Set` from the `getIncompatibleWith()` method, which may cause type-related issues in the code that uses the interface.

* To fix this issue, the `CompatibleSet` interface should be made generic, allowing the same type parameter to be used for the `CompatibleElements` in the `Set`. This will allow the `CompatibleSets` to be used safely in the code that depends on the `Compatible` interface.

\midrule

\multicolumn{1}{c}{\textbf{RAG}} \\\midrule
The Javadoc for this interface is missing an ending period. It is recommended to add a period at the end of the first sentence to make it clear that the sentence is complete. Additionally, the type parameter for the interface should be changed to `T extends CompatibleElement<T>` to ensure that the interface is self-referential and to avoid potential issues with type inference. This change will also make the interface more consistent with other interfaces in the KSQL library
\midrule

\multicolumn{1}{c}{\textbf{NCO}} \\\midrule
The method signature for getIncompatibleWith() should be changed to return a Set of T instead of a Set of CompatibleElement. This is because the method is defined to return a set of elements that are incompatible with the current element, and the type parameter T is already defined to be a subtype of CompatibleElement. By returning a Set of T, the method signature becomes more specific and easier to understand.
First sentence of Javadoc is missing an ending period.

\midrule

\multicolumn{1}{c}{\textbf{LBS}} \\\midrule
The method signature for getIncompatibleWith() should be changed to return a Set of T instead of a Set of CompatibleElement. This is because the method is defined to return a set of elements that are incompatible with the current element, and the type parameter T is already defined to be a subtype of CompatibleElement. By returning a Set of T, the method signature becomes more specific and easier to understand.
\midrule

\multicolumn{1}{c}{\textbf{KBS}} \\\midrule
First sentence of Javadoc is missing an ending period.


\bottomrule

\end{tabularx}
\end{table*}




