\section{Introduction}
\label{sec:intro}

Code review is a common software engineering practice that plays a crucial role in maintaining code quality, identifying bugs, and fostering a culture of continuous improvement within development teams \cite{ccetin2021review,xiaomeng2018survey}. By rigorously inspecting code changes, reviewers provide valuable feedback that can improve software both before and after it is integrated into the system. Modern code review goes beyond defect detection; it aims to enhance the overall quality of software changes by ensuring maintainability, reliability, and adherence to best practices \cite{hong2022commentfinder, bavota2015four}.

However, the code review process is often perceived as complex, time-consuming, and subject to various biases, especially in large-scale projects. Factors such as varying levels of developer expertise, interpersonal dynamics, and the lack of standardized guidelines can introduce inconsistencies, ultimately impacting the efficiency and robustness of the codebase~\cite{ben2024improving}. Furthermore, the evolving nature of coding standards and best practices requires constant adaptation, adding further complexity to the process.

To address these challenges, there has been growing interest in automating the code review process \cite{hovemeyer2004finding}. Initial efforts primarily involved the deployment of knowledge-based systems (KBS), particularly static analysis tools like PMD, FindBugs, and SonarQube. These tools use predefined rule sets to identify common coding issues, systematically scanning the codebase for violations and improving the early stages of development by flagging rule violations in the code. Although effective at detecting surface-level issues, static analyzers are limited by their dependence on manually defined rules, which require frequent updates to remain relevant. Their rigidity makes it difficult to handle complex, context-dependent issues that require a deeper understanding of code intent \cite{sadowski2015tricorder}. Consequently, these tools often fall short in adapting to the dynamic and context-sensitive nature of software projects, where factors like architecture, team culture, and specific project requirements demand more flexible and intelligent solutions.

Recent advances in large language models and natural language processing have sparked significant interest in using pre-trained language models to automate code review workflows. These learning-based systems (LBS) aim to overcome the limitations of knowledge-based detection by providing a more nuanced understanding of code, enabling them to identify deeper faults, recommend improvements, and even anticipate the potential impact of code changes on the overall system~\cite{wadhwa2024core}. Although these models capture a broader range of issue patterns than static analysis alone, their precision still remains below acceptable levels \cite{tufano2024code, ibtasham2024towards}.

As is common in the automation of software development tasks, some solutions achieve high precision but with limited coverage, while others offer broader coverage at the expense of precision. The most promising approaches, therefore, are those that balance these trade-offs by combining solutions to optimize both precision and coverage. We hypothesize that integrating static analysis with learning-based systems can enhance the effectiveness of automated code review generation tools.
In this paper, we explore three combination strategies aimed at integrating the structured knowledge from static analysis into the pipeline for building and operating a large language model (LLM) for code review generation. Specifically, our approach integrates knowledge at three distinct points in the pipeline: during data preparation for fine-tuning (data-augmented training, DAT), at inference time (retrieval-augmented generation, RAG), and after inference (naive concatenation of outputs, NCO).

To evaluate these strategies, we used a combination of human judgments and LLM-based assessments to compare the generated reviews from each approach with baseline systems: standalone KBS and LBS. Our results show that combining KBS and LBS captures the strengths of both: the precision of static analysis and the comprehensiveness of LLMs, resulting in more effective and human-like code reviews.

The remainder of this paper is organized as follows: Section~\ref{sec:background} provides an overview of the background. Section~\ref{sec:approach} details the components of our proposed approach. Section~\ref{sec:evaluation} presents the evaluation results. Section~\ref{sec:literature} discusses related work. Finally, Section~\ref{sec:conclusion} offers concluding remarks and suggests potential directions for future improvements to our approach.

