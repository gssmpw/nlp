\section{Background}
\label{sec:background}


\subsection{Code Review Automation}
Code review is a widely adopted practice among software developers where a reviewer examines changes submitted in a pull request \cite{hong2022commentfinder, ben2024improving, siow2020core}. If the pull request is not approved, the reviewer must describe the issues or improvements required, providing constructive feedback and identifying potential issues. This step involves review commment generation, which play a key role in the review process by generating review comments for a given code difference. These comments can be descriptive, offering detailed explanations of the issues, or actionable, suggesting specific solutions to address the problems identified \cite{ben2024improving}.


Various approaches have been explored to automate the code review comments process  \cite{tufano2023automating, tufano2024code, yang2024survey}. 
Early efforts centered on knowledge-based systems, which are designed to detect common issues in code. Although these traditional tools provide some support to programmers, they often fall short in addressing complex scenarios encountered during code reviews \cite{dehaerne2022code}. More recently, with advancements in deep learning, researchers have shifted their focus toward using large-language models to enhance the effectiveness of code issue detection and code review comment generation.

\subsection{Knowledge-based Code Review Comments Automation}

Knowledge-based systems (KBS) are software applications designed to emulate human expertise in specific domains by using a collection of rules, logic, and expert knowledge. KBS often consist of facts, rules, an explanation facility, and knowledge acquisition. In the context of software development, these systems are used to analyze the source code, identifying issues such as coding standard violations, bugs, and inefficiencies~\cite{singh2017evaluating, delaitre2015evaluating, ayewah2008using, habchi2018adopting}. By applying a vast set of predefined rules and best practices, they provide automated feedback and recommendations to developers. Tools such as FindBugs \cite{findBugs}, PMD \cite{pmd}, Checkstyle \cite{checkstyle}, and SonarQube \cite{sonarqube} are prominent examples of knowledge-based systems in code analysis, often referred to as static analyzers. These tools have been utilized since the early 1960s, initially to optimize compiler operations, and have since expanded to include debugging tools and software development frameworks \cite{stefanovic2020static, beller2016analyzing}.



\subsection{LLMs-based Code Review Comments Automation}
As the field of machine learning in software engineering evolves, researchers are increasingly leveraging machine learning (ML) and deep learning (DL) techniques to automate the generation of review comments \cite{li2022automating, tufano2022using, balachandran2013reducing, siow2020core, li2022auger, hong2022commentfinder}. Large language models (LLMs) are large-scale Transformer models, which are distinguished by their large number of parameters and extensive pre-training on diverse datasets.  Recently, LLMs have made substantial progress and have been applied across a broad spectrum of domains. Within the software engineering field, LLMs can be categorized into two main types: unified language models and code-specific models, each serving distinct purposes \cite{lu2023llama}.

Code-specific LLMs, such as CodeGen \cite{nijkamp2022codegen}, StarCoder \cite{li2023starcoder} and CodeLlama \cite{roziere2023code} are optimized to excel in code comprehension, code generation, and other programming-related tasks. These specialized models are increasingly utilized in code review activities to detect potential issues, suggest improvements, and automate review comments \cite{yang2024survey, lu2023llama}. 




\subsection{Retrieval-Augmented Generation}
Retrieval-Augmented Generation (RAG) is a general paradigm that enhances LLMs outputs by including relevant information retrieved from external databases into the input prompt \cite{gao2023retrieval}. Traditional LLMs generate responses based solely on the extensive data used in pre-training, which can result in limitations, especially when it comes to domain-specific, time-sensitive, or highly specialized information. RAG addresses these limitations by dynamically retrieving pertinent external knowledge, expanding the model's informational scope and allowing it to generate responses that are more accurate, up-to-date, and contextually relevant \cite{arslan2024business}. 

To build an effective end-to-end RAG pipeline, the system must first establish a comprehensive knowledge base. It requires a retrieval model that captures the semantic meaning of presented data, ensuring relevant information is retrieved. Finally, a capable LLM integrates this retrieved knowledge to generate accurate and coherent results \cite{ibtasham2024towards}.




\subsection{LLM as a Judge Mechanism}

LLM evaluators, often referred to as LLM-as-a-Judge, have gained significant attention due to their ability to align closely with human evaluators' judgments \cite{zhu2023judgelm, shi2024judging}. Their adaptability and scalability make them highly suitable for handling an increasing volume of evaluative tasks. 

Recent studies have shown that certain LLMs, such as Llama-3 70B and GPT-4 Turbo, exhibit strong alignment with human evaluators, making them promising candidates for automated judgment tasks \cite{thakur2024judging}

To enable such evaluations, a proper benchmarking system should be set up with specific components: \emph{prompt design}, which clearly instructs the LLM to evaluate based on a given metric, such as accuracy, relevance, or coherence; \emph{response presentation}, guiding the LLM to present its verdicts in a structured format; and \emph{scoring}, enabling the LLM to assign a score according to a predefined scale \cite{ibtasham2024towards}. Additionally, this evaluation system can be enriched with the ability to explain reasoning behind verdicts, which is a significant advantage of LLM-based evaluation \cite{zheng2023judging}. The LLM can outline the criteria it used to reach its judgment, offering deeper insights into its decision-making process.




