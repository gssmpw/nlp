\begin{abstract}
Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.

\begin{IEEEkeywords}
Code Review, Knowledge-Based Systems, Language Models, Retrieval-Augmented Generation.
\end{IEEEkeywords}

% Code review is an essential practice in software development, crucial for maintaining code quality and minimizing the risk of errors. Nonetheless, the process is often complex, subjective, and time-consuming. Recent research has explored the automation of code review through diverse methodologies. Early approaches primarily focused on knowledge-based systems that employ rule-based mechanisms to detect code issues. However, these systems have limitations in handling complex, context-dependent scenarios. Consequently, there has been a shift toward fine-tuning pre-trained language models to automate the code review process. In this paper, we propose a novel approach that combines both knowledge-based systems (KBS) and learning-based systems (LBS) to create high-quality and comprehensive code reviews. Our approach incorporates three distinct strategies: Data Augmented Training (DAT), Retrieval-Augmented Generation (RAG), and Naive Outputs Concatenation Outputs (NOC). DAT enriches the training data by combining outputs from KBS and LBS, allowing the language model to learn from both rule-driven and data-driven approaches. RAG integrates KBS-generated knowledge dynamically at inference, guiding the LBS towards producing more comprehensive and standards-aligned feedback. NCO serves as a baseline method that concatenates outputs from both approaches at post-inference for a unified review. We empirically compare the proposed models to the baseline models. 
% Our results demonstrate that our approach improves the relevance, completeness, and quality of review comments, bridging gaps between knowledge-based tools and deep learning models.
\end{abstract}
