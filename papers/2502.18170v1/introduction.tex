\section{Introduction}

\textit{Quantum state tomography}~\cite{BBMR04,Keyl06,GJK08} is the problem of learning an unknown quantum state. It is a fundamental problem with important applications in quantum computing as we often need to learn about the state of a quantum device. 
Formally, we are given $\ns$ copies of an $\nqubits$-qubit quantum
state with density matrix $\rho\in \C^{\dims\times\dims}$ where $\dims=2^{\nqubits}$. We need to perform quantum
measurements on $\rho^{\otimes \ns}$ and obtain an estimate $\hat{\rho}$ close to $\rho$ under some error metric. In this work, we focus on the trace distance $\tracenorm{\qest-\rho}$, and we want to ensure that $\tracenorm{\qest-\rho}<\eps$ with probability at least 0.99. We want to characterize the copy/sample complexity, the minimum number of copies of $\rho$ needed for the task.

There are different types of measurements we can apply. The most general is  \emph{entangled} or \emph{joint} measurement, where one can arbitrarily apply any measurement to $\rho^{\otimes \ns}$.
In \cite{HaahHJWY17,ODonnellW16,ODonnellW17}, the authors showed that the worst-case sample complexity is $n =\bigTheta{\frac{4^\nqubits}{\eps^2}}$. 
% \cite{Yuen_2023} proved a stronger lower bound when fidelity is considered as the metric.
While such measurement is powerful, it is difficult to implement on near-term quantum computers as it requires a large and coherent quantum memory.  
This leads to a line of work studying \emph{single-copy} measurements~\cite{Flammia_2012,KRT14,HaahHJWY17,chen2023does}, where we apply a separate measurement for each copy of $\rho$. 
The measurements can be chosen \emph{non-adaptively}, where all $\ns$ measurements are decided simultaneously, or \emph{adaptively}, where the copies are measured sequentially, and each measurement can be chosen based on previous outcomes. 
With non-adaptive measurements, \cite{KRT14} showed that 
tomography is possible using $\ns = O(8^\nqubits / \eps^2)$ copies, and later was shown to be optimal by \cite{HaahHJWY17}. \cite{chen2023does} further showed that adaptivity does not help. 

However, since each copy is an $\nqubits$-qubit system, single-copy measurement potentially requires entanglement over $\nqubits$ qubits. 
In particular, the optimal single-copy measurements \cite{KRT14,guctua2020fast} are highly structured POVMs (including SIC-POVM, maximal MUB, uniform POVM) that are difficult to implement in practice especially when $N$ is large.
Thus, it is important to study single-copy tomography under measurement constraints. 
As a canonical example, \emph{Pauli measurements} have attracted significant attention due to their ease of implementation. 
It only involves measuring each qubit in the eigenbasis of one of the three $2\times 2$ Pauli operators $\pauliX,\pauliY,\pauliZ$, which is arguably one of the most experiment-friendly measurements. 
Although Pauli measurement is fundamental in quantum physics, there is a large gap between the upper and lower bounds of its copy complexity. 
The best-known upper bound is $\ns=\bigO{\nqubits\frac{12^{\nqubits}}{\eps^2}}$ \cite{guctua2020fast}, and no better lower bound is known besides the single-copy lower bound of $\bigOmega{\frac
{8^{\nqubits}}{\eps^2}}$. On the other hand, empirical evidence \cite{stricker2022pauliSIC} suggests that SIC-POVM is better than Pauli measurements.



% Category 3 consists of local measurements on each qubit of each copy of the state $\rho$, whose results are to be combined to reconstruct the
% density matrix \cite{guctua2020fast}. 




\subsection{Results}
Our work makes notable progress in closing the long-standing gap for Pauli tomography.
We prove the first non-trivial lower bound showing that Pauli measurements cannot achieve the optimal sample complexity for single-copy tomography.  
\begin{theorem}
Using Pauli measurements, learning an unknown $\nqubits$-qubit state $\rho$ up to trace distance $\eps$  with probability at least 0.99 requires at least
\[
\ns = \bigOmega{\frac{9.118^{\nqubits}}{\eps^2}}
\]
copies of $\rho$.
This lower bound holds even when the measurements are chosen adaptively.
\label{thm:pauli-lower}
\end{theorem}
Since highly structured POVM such as SIC-POVM and maximal MUB achieves the $O(8^{\nqubits}/\eps^2)$ sample complexity, we formally show a separation between Pauli measurements and these structured POVMs. This is consistent with experimental observations~\cite{stricker2022pauliSIC}.

We also design a new algorithm that improves the current upper bound,
\begin{theorem}
 {Quantum state tomography} can be solved by Pauli measurements  using 
 $$\ns = \bigO{\frac{10^N\log\frac{1}{\delta}}{\eps^2}}$$ copies of $\rho$ with success with probability at least $1-\delta$.
 \label{thm:pauli-upper}
 \end{theorem}

Thus, we significantly reduce the existing gap between upper and lower bounds for Pauli tomography. We conjecture that the $10^{\nqubits}/\eps^2$ upper bound is tight.

Our main technical contribution is a new lower-bound framework for adaptive quantum tomography under measurement constraints. The constraint is generally described by a set of POVMs $\povmset$, which is the set of allowed measurements that we can apply to each copy. In our problem, $\povmset$ is the set of Pauli measurements.

Compared to the unrestricted case, there are two challenges to proving tight lower bounds. 
The first challenge is that with constraint $\povmset$, some states might be harder to learn for measurements in $\povmset$ than others. 
Thus, we need to design a measurement-dependent hard instance to capture the limitations of $\povmset$. The second challenge is to quantitatively evaluate the effect of measurement constraints on sample complexity. 
Intuitively, there should be some indicator of ``measurement capability'' that controls the hardness of learning. 

To our knowledge, existing lower-bound techniques for tomography cannot address these challenges. The hard case constructions in many works \cite{ODonnellW16, HaahHJWY17,chen2023does,lowe2022lower} are measurement-independent. Moreover, the analysis is either oblivious to measurement constraints \cite{ODonnellW16, HaahHJWY17,chen2023does}, or only applies to some specific constraint such as Pauli observables~\cite{Flammia_2012} and constant-outcome measurements~\cite{lowe2022lower}.

We address both challenges and develop a general framework using the \emph{measurement information channel},

\begin{definition}
\label{def:mic}
    Let $\POVM=\{M_x\}_{x}$ be a POVM. The \emph{measurement information channel (MIC)} $\Luders_{\POVM}:\C^{\dims\times\dims}\mapsto\C^{\dims\times\dims}$ and its matrix representation $\Choi_{\POVM}$ are defined as
\begin{equation}
    \Luders_{\POVM}(A)\eqdef\sum_{x}M_x\frac{\Tr[M_xA]}{\Tr[M_x]}, \quad\Choi_{\POVM}\eqdef \sum_{x}\frac{\vvec{M_x}\vadj{M_x}}{\Tr[M_x]} \in \C^{\dims^2\times\dims^2}.
\end{equation}
where $\vvec{M_x}=\VecOp(M_x)$ and $\vadj{M_x}=\VecOp(M_x)^{\dagger}$.
\end{definition}
The channel maps a quantum state to another quantum state. 
Intuitively, it characterizes the similarity of the outcome distributions after applying $\POVM$ to $\rho$ and the maximally mixed state $\qmm$. 
The ability to distinguish between different states is described by the eigenvalues of the channel. The eigenvectors (which are matrices in this case) with small eigenvalues indicate the directions that are hard to distinguish for the measurement. 

The MIC helps us to address both challenges. We design hard instances based on the ``weak'' directions of MIC of measurements in $\povmset$, 
and the sample complexity would depend on the eigenvalues of MIC.
Our framework not only works for Pauli measurements but can also be applied to arbitrary measurement constraints $\povmset$. 
To demonstrate the generality of our approach, we prove tight sample complexity bound for a natural family of $\ab$-outcome measurements. 
\begin{theorem} \label{thm:nearly-tight-finite-out}
    The worst-case copy complexity of single-copy tomography with $\ab$-outcome measurements is
    \[
    \ns = \tildeTheta{\frac{\dims^4}{\eps^2\min\{\ab, \dims\}}}.
    \]
    The lower bound holds for adaptive measurements, and the upper bound is achieved by non-adaptive ones.
\end{theorem}
Previously, the worst-case bound was known only for constant $\ab$~\cite{lowe2022lower,Flammia_2012}\footnote{\cite{lowe2022lower} proved the lower bound for non-adaptive measurements. Their adaptive lower bound only applies to finite constraint set $\povmset$.} and $\ab\ge\dims$~\cite{HaahHJWY17, chen2023does,guctua2020fast}. We not only recover their results but establish a complete dependence on $\ab$. We give a detailed discussion of our framework in \cref{sec:techniques}.


\subsection{Related works}
\paragraph{Quantum state tomography} We make additional remarks about previously mentioned works and discuss other works in this regime.

Many works study the tomography of low-rank states. 
For $\rho$ with rank $\rk$, it is shown that $\ns = \tildeTheta{\frac{\dims \rk}{\eps^2}}$ is necessary and sufficient~\cite{HaahHJWY17,ODonnellW16} with entangled measurement. 
For single-copy measurements, the sample complexity is $\ns=\bigTheta{\frac{\dims \rk^2}{\eps^2}}$ for non-adaptive measurements, but whether it is tight for adaptive ones is unknown. 

For Pauli measurements, \cite{guctua2020fast} showed that $\ns=\bigO{\frac{\nqubits \cdot 3^\nqubits\cdot \rk^2}{\eps^2}}$ is sufficient. 
Random Pauli measurements offer distinct advantages \cite{Elben_2022} and have been effectively applied in quantum process tomography for shallow quantum circuits \cite{yu2023learningmarginalssuffices,Huang_2024}. 

Some work~\cite{compressed,Flammia_2012} considers Pauli observables, a special class of 2-outcome Pauli measurements. The sample complexity for rank-$\rk$ state tomography is ${\tilde{{\Theta}}}(\frac{\dims^2 r^2}{\eps^2})$ using non-adaptive measurements~\cite{Flammia_2012}. \cite{lowe2022lower} showed that the lower bound also holds for adaptively chosen constant-outcome measurements\footnote{More precisely when adaptively chosen from a finite set of measurements with size at most $\exp(O(\dims))$.}. \cite{cai2016optimal} derived near-optimal error rates for Hilbert-Schmidt and operator-norm distance. However, they require that the state is sparse in the expectation values of Pauli observables, nor did they prove a lower bound for the trace distance. 


Recently, \cite{Chen0L24memory} studied the case when one can perform entangled measurement over $t>1$ copies at a time, which interpolates between single-copy and fully entangled measurements. Apart from trace distance, other metrics were considered, such as fidelity~\cite{HaahHJWY17,chen2023does, Yuen_2023}, quantum relative entropy~\cite{flamian2023tomography}, and Bures $\chi^2$-divergence~\cite{flamian2023tomography}. Extending our work to low-rank states and other error metrics is an interesting future direction.
 



\paragraph{Other quantum state inference problems} Quantum state testing/certification \cite{ODonnellW15,BadescuO019} is a closely related problem, where the goal is to test whether an unknown state $\rho$ is equal or far from a target state $\sigma$. The problem has also been considered under entangled~\cite{ODonnellW15,BadescuO019}, single-copy measurements~\cite{BubeckC020,Chen0HL22,liu2024role}, and Pauli measurements~\cite{Yu2023almost}. A concurrent work~\cite{liu2024restricted} considers single-copy testing under measurement restrictions, but their technique only applies to non-adaptive measurements.

In practice, we are often interested in partial information about the state rather than a full-state description. \cite{Cotler_2020, Garc_a_P_rez_2020, evans2019scalable} studied \textit{quantum overlapping tomography}, where the goal is to output the classical description of all $k$-qubit reduced
density matrices of an $n$-qubit system. The algorithms are based on Pauli measurements, demonstrating their wide applicability. 
Shadow tomography~\cite{Aaronson20, huang2020predicting, ChenCH021,chen2024pauli} aims to learn the expectation value of a finite set of observables. In particular, \cite{chen2024pauli} studied Pauli shadow tomography with various measurement constraints such as measurements with finite quantum memory and Clifford measurements. It would be an interesting future work to establish a formal connection between their framework and our method.


\paragraph{Distributed distribution estimation}

Quantum tomography can be thought of the quantum analogue of the classical problem of distribution estimation. Given samples from an unknown distribution $p$, the goal is to output an estimate $\hat p$ such that $d(p,\hat p)<\varepsilon$ for some distance $d$. The problem has a long history and has been studied under several different settings. 

The problem of single copy tomography is in spirit similar to the problems of distributed estimation of distributions under information constraints. 
In it, i.i.d. samples $X_1, \ldots, X_n$ from the unknown $p$ are distributed across $n$ users, who are constrained in how much information about their sample they can send. 
Well-studied information constraints include communication constraints (where each user has to compress their sample using at most $b$ bits), or privacy constraints (where each user has to add noise to their sample to preserve privacy). Several problems in distributed estimation of distribution for both discrete, continuous as well as high dimensional distributions have been studied in the past several years~\cite{duchi2013local, barnes2019lower, AcharyaCT19, acharya2020distributed}.

\section{Our techniques}
\label{sec:techniques}
\subsection{Lower bound ideas through a simple example}
Our main contribution is a novel technique to prove single-copy tomography lower bounds with measurement restrictions. Before we delve into the details, we use a simple example to illustrate why we need new ideas for the problem.

Let's say that we are only allowed to use the computational basis measurement $\{\qproj{x}\}_{x=0}^{\dims-1}$ for each copy. It is impossible to perform quantum tomography under this constraint: one can easily observe that for both the maximally mixed state $\qmm\eqdef\eye_\dims/\dims$ and the state 
\[
\qbit{\psi}=\frac{1}{\sqrt{\dims}}\sum_{x=0}^{\dims-1}\qbit{x},
\]
the measurement outcomes would be a uniform distribution over $\{0, \ldots, \dims-1\}$. Yet, the trace distance between the two states is close to 1. Thus, we cannot even distinguish two states that are nearly as far away as they can be, let alone learning any given state up to an arbitrary accuracy $\eps$.

An immediate lesson is that when the constraint set $\povmset$ is too restricted, nature would be able to design some states that are particularly hard to distinguish for measurements in this set $\povmset$. In this example, the two states $\qbit{\psi}$ and $\qmm$ are precisely chosen based on the measurement $\{\qproj{x}\}_{x=0}^{\dims-1}$. 

Note that Pauli measurements only consist of $3^{\nqubits}$ basis measurements. 
It is a fairly small set compared to the dimensionality of quantum states which is $\dims^2=4^{\nqubits}$. Furthermore, it does not have nice geometric properties of maximal MUB~\cite{klappenecker2005mutually} and SIC-POVM
~\cite{zauner1999grundzuge}.
Thus, we expect that the lower bound instance for Pauli tomography also needs to be \emph{measurement-dependent}.
However, to our knowledge, the constructions in existing works on quantum tomography are predominantly measurement-independent.
For example, \cite{chen2023does} uses Gaussian orthogonal ensembles which informally speaking apply independent Gaussian perturbations to each coordinate of the maximally mixed state. \cite{HaahHJWY17} constructs a packing set based on Haar-random unitary transformations. 
Thus, the techniques in these works are likely not optimal for Pauli measurements.

It was fairly easy to design two states that completely fool the computational basis measurement, which implies that tomography with just the computation basis is impossible.
For other measurement constraints such as Pauli measurements, we need a systematic approach to (1) design a specific hard case instance and (2) analyze the effect of the constraint on tomography. It turns out that the \emph{measurement information channel (MIC)} helps us to achieve both objectives. We illustrate the role of MIC using the computational basis example. From \cref{def:mic}, the MIC of $\POVM=\{\qproj{x}\}_{x=0}^{\dims-1}$ is
\[
\Luders_{\POVM}(\cdot) = \sum_{x=0}^{\dims-1} \qproj{x}(\cdot)\qproj{x}.
\]
The outputs of $\Luders_{\POVM}$ on $\qmm$ and $\qproj{\psi}$ are identical,
\[
\Luders_{\POVM}(\qmm)=\frac{1}{\dims}\sum_{x=0}^{\dims-1} \qproj{x}\eye_\dims\qproj{x}=\frac{1}{\dims}\sum_{x=0}^{\dims-1} \qproj{x},\quad 
\Luders_{\POVM}(\qproj{\psi})=\sum_{x=0}^{\dims-1} \qbit{x}\qdotprod{x}{\psi}\qdotprod{\psi}{x}\qadjoint{x}=\frac{1}{\dims}\sum_{x=0}^{\dims-1} \qproj{x}.
\]
Let $\Delta=\qmm-\qproj{\psi}$. Equivalently we have $\Luders_{\POVM}(\Delta)=0$, or $\Delta$ lies in the 0-eigenspace of $\Luders_{\POVM}$. Therefore, to construct the hard instance, we can perturb the reference state (normally $\qmm$) along directions where the MIC has small eigenvalues. Furthermore, the spectrum of MIC in the constraint set $\povmset$ determines the hardness of tomography.

The intuition might appear similar to how testing with fixed measurement is disadvantageous to randomized ones~\cite{liu2024role}. However, their work does not consider measurement restrictions for each copy. Moreover, tomography is a harder problem than testing which requires a different analysis. Furthermore, we allow adaptive measurements, which are much more complicated than fixed measurements where the measurement outcomes are independent. Thus, it is unclear how their arguments can extend to our problem.


\subsection{The lower bound construction} 
Informally, our construction adds independent binary perturbations to $\qmm$ along different directions,
\begin{equation}
\label{equ:quantum-construction-informal}
    \sigma_z=\qmm + \frac{\dst}{\sqrt{\dims}}\cdot\frac{\cd}{\dims}\sum_{i=1}^{\ell} z_iV_i, \quad\ell=\frac{\dims^2}{2},
\end{equation}
where $\{V_i\}_{i=1}^{\dims^2-1}$ are $\dims^2-1$ orthonormal trace-0 Hermitian matrices which satisfy $\Tr[V_iV_j]=\indic{i=j}$, $z=(z_1, \ldots, z_{\dims^2/2})$ is drawn uniformly from $\{-1, 1\}^{\dims^2/2}$, and $\cd$ is an absolute constant. 

The same construction was used for quantum state testing by \cite{liu2024role}. We can argue that with high probability over $z$, 
\eqref{equ:quantum-construction-informal} yields a valid quantum state that is $\eps$ far from the maximally mixed state $\qmm$. 
\begin{theorem}[Valid construction, informal]
\label{thm:valid-construction-informal}
    Let $\eps\le 1/200$, and $c$ be a suitably chosen absolute constant. Let $z\sim \{-1,1\}^{\dims^2/2}$ uniformly, then with probability at least $1-\exp(-\dims)$, $\sigma_z$ in \eqref{equ:quantum-construction-informal} is a valid quantum state and $\tracenorm{\sigma_z-\qmm}>\eps$. 
\end{theorem}
In the rare event that $\sigma_z$ is not a valid state (i.e., not p.s.d.), we shrink the perturbation $\Delta_z=\sigma_z-\qmm$ so that it has a maximum eigenvalue of at most $1/(2\dims)$. The formal definition is presented in \cref{def:perturbation}.

The main advantage of this construction is that it gives us the freedom to choose directions $V_1, \ldots, V_{\dims^2/2}$ that are least sensitive to the given measurement constraint. Next, we discuss the choice of these directions for Pauli measurements.

\paragraph{Construction for Pauli measurements} We choose these directions to be the (normalized) \emph{Pauli observables} with the largest weights. 
In short, a Pauli observable $P$ belongs to $\pauliObsSet=\{\pauliX, \pauliY, \pauliZ, \eye_2\}^{\otimes \nqubits}\setminus\{\eye_\dims\}$\footnote{Some literature also include $\eye_\dims$ as a Pauli observable} where 
\begin{equation}
    \pauliX = \begin{bmatrix}
    0 & 1 \\
    1 & 0
\end{bmatrix},\quad
\pauliY = \begin{bmatrix}
    0 & -i \\
    i & 0
\end{bmatrix},\quad
\pauliZ = \begin{bmatrix}
    1 & 0\\
    0 & -1
\end{bmatrix}.
\label{equ:pauli-ops}
\end{equation}
The \emph{weight} of $P$ is the number of non-identity components ($\pauliX,\pauliY,\pauliZ$) it contains. 
An important property is that the Pauli observables and $\eye_\dims$ forms an orthogonal basis for quantum states, and thus any state $\rho$ can be represented as
\begin{equation}
    \rho = \frac{\eye_\dims}{\dims} +\sum_{P\in\pauliObsSet}\alpha_PP, \quad\alpha_P=\frac{\Tr[\rho P]}{\dims}.
    \label{equ:pauli-decomposition}
\end{equation}


For each copy of the $\nqubits$-qubit state, a Pauli measurement $\POVM$ measures each qubit in the eigenbasis of either $\pauliX,\pauliY,\pauliZ$. The outcome is a $\{-1, 1\}^{\nqubits}$ binary string which reveals information about the coefficient $\alpha_P$ of all Pauli observables $P$ whose non-identity components match the choice of $\pauliX,\pauliY,\pauliZ$ for the corresponding qubit. Thus, the coefficients of $P$ with larger weights are harder to learn. 

As an example, if $P=\sigma_X^{\otimes \nqubits}$, then the only way to learn about $\alpha_P$ is to measure all qubits in the eigenbasis of $\sigma_X$. On the other hand, to learn about $\sigma_X\otimes \eye_{\dims/2}$, the only requirement is to measure the first qubit in the eigenbasis of $\sigma_X$, and we can choose any of the three choices for other qubits. In general, for a Pauli observable $P$ with weight $w$, there are $3^{\nqubits-w}$ Pauli measurements that can learn information about $\alpha_P$.

\paragraph{The role of MIC} It turns out that the measurement information channel of Pauli measurements precisely formalizes our previous intuition. Using \cref{def:mic} and the definition of Pauli measurements, we have the following result,
\begin{lemma}[MIC of Pauli measurement, informal]
\label{lem:mic-pauli-informal}
    Let $\Luders_{\POVM}$ be the measurement information channel of a Pauli measurement $\POVM$, then for all Pauli observable $P$
    \[
    \Luders(P)=P\indic{\text{The non-identity components of $P$ match the choice of basis in $\POVM$}}.
    \]
\end{lemma}
This is consistent with the fact that Pauli measurements only reveal information for Pauli observables with a matching choice of $\pauliX, \pauliY, \pauliZ$. 

To see how the eigenvalues of MIC characterize the ability of Pauli measurements, we consider a POVM $\ObsPOVM$ defined by the uniform ensemble of all $3^{\nqubits}$ Pauli measurements. In other words, we uniformly sample a Pauli measurement and observe the outcome. Together with the choice of the measurement, $\ObsPOVM$ is a POVM with $3^{\nqubits}\cdot 2^{\nqubits}=6^{\nqubits}$ outcomes. From \cref{def:mic}, the MIC of $\ObsPOVM$ is simply the linear combination of the MIC of all Pauli measurements,
\[
\Luders_{\ObsPOVM}(\cdot) = \frac{1}{3^{\nqubits}}\sum_{\POVM \text{ Pauli}}\Luders_{\POVM}(\cdot).
\]
Due to linearity, all Pauli observables $P$ are also the eigenvectors of $\Luders_{\ObsPOVM}$. Suppose $P$ has a weight of $w$, then using \cref{lem:mic-pauli-informal}, its eigenvalue for $\Luders_{\ObsPOVM}$ is,
\[
\frac{1}{3^{\nqubits}}\sum_{\POVM \textbf{ Pauli}}\indic{\text{The non-identity components of $P$ match the choice of basis in $\POVM$}}=\frac{3^{\nqubits-w}}{3^{\nqubits}}=3^{-w}.
\]
The first step is precisely because there are $3^{\nqubits-w}$ Pauli measurements that match the non-identity components of $P$. Thus $P$ with a large weight has a smaller eigenvalue,  meaning that over uniform draw of Pauli measurements, we learn less about large-weight $P$ ``on average''. This is consistent with the previous discussion that large-weight Pauli observables are hard to learn for Pauli measurements.

\subsection{Assouad's method: Hamming separation for trace distance}
The packing argument is popular among previous works \cite{HaahHJWY17, ODonnellW16} which constructs a finite set of states such that the pair-wise trace distance is $\Omega(\eps)$. 
Thus, any learning algorithm must be able to correctly identify the state chosen by nature from the packing set. 
From this Holevo's theorem can be applied. However, it is not straightforward to construct a packing set that adjusts to the measurement constraint. 

Our lower bound is based on Assouad's method~\cite{Assouad83,yu1997assouad}, which reduces the learning problem to a multiple binary hypothesis testing problem. It has been extensively used for many parametric estimation problems \cite{duchi2013local,ACLST22iiuic}. The method is more suitable for our construction in \eqref{equ:quantum-construction-informal}.

Let $L(\cdot, \cdot)$ be an error metric between quantum states.
The main argument is that if an algorithm can learn any state with a small error in terms of $L$, then given a randomly sampled $\sigma_z$, the algorithm should be able to obtain an estimate $\hat{z}\in\{-1,1\}^{\dims^2/2}$ that matches most coordinates of $z$. Traditionally \cite{yu1997assouad,duchi2013local}, this relies on a $2\tau$-Hamming separation for the error metric $L$,
\[
L(\sigma_z,\sigma_{z'})\ge 2\tau \ham{z}{z'}=2\tau\sum_{i=1}^{\dims^2/2}\indic{z_i\ne z_i'}.
\]
Given this relation, a small loss $L$ implies $\ham{z}{z'}$ must be small. We then need to compute the separation parameter $\tau$, which is easy if $L$ and $z$ have a coordinate-wise relation. This is often true for classical distribution estimation~\cite{duchi2013local,ACLST22iiuic}, where $L$ is often the $\ell_p$ norm between two distributions. In quantum tomography, if $L$ is the Hilbert-Schmidt distance $\hsnorm{\sigma_z-\sigma_{z'}}$, the distance can also be written in terms of the coordinates of $z,z'$ since $V_i$'s are orthogonal. \cite{cai2016optimal} obtained the lower bound for the Hilbert-Schmidt distance using this approach.

However, the trace distance does not have a nice geometry like the Hilbert-Schmidt norm or vector $\ell_p$ norms that yields a direct relation between $\tracenorm{\sigma_z-\sigma_{z'}}$ and each coordinate of $z,z'$. 
Partly for this reason, \cite{cai2016optimal} did not obtain a lower bound for trace distance and suggested that a new approach might be needed.

Instead of trying to prove a general coordinate-wise relation for trace distance, we show a Hamming separation only for the ``good'' $z\in\{-1,1\}^{\dims^2/2}$ such that $\sigma_z$ is a valid quantum state. 
This is sufficient for our purpose since  according to \cref{thm:valid-construction-informal} an overwhelming fraction of $z$'s are ``good''.
\begin{lemma}[Trace distance Hamming separation, informal] \label{lemma:hamm-separation-informal}
   Let $z\in\{-1, 1\}^{\dims^2/2}$ and $c'$ be an absolute constant. If $\sigma_z$ defined in \eqref{equ:quantum-construction-informal} is a valid quantum state, then for all  $z' \in \left\{-1,1\right\}^{\dims^2/2}$, 
   \begin{equation}
       \tracenorm{\sigma_z - \sigma_{z'}} \geq \frac{c' \eps}{\dims^2} \ham{z}{z'}.
   \end{equation}
\end{lemma}
\begin{proof}[Proof sketch]
    The idea is that when $\sigma_z$ is ``good'', then the perturbation $\Delta_z=\sigma_z-\qmm$ has an operator norm at most $O(\eps/\dims)$. Then we use the duality between the trace norm and operator norm,
\begin{lemma}[Duality between trace and operator norms] \label{lemma:trace-norm-dual}
    Let $A\in \C^{\dims\times\dims}$, then
    \[
    \tracenorm{A}=\sup_{B\in \C^{\dims\times\dims}:\opnorm{B}\le 1}|\Tr[B^{\dagger}A]|.
    \]
\end{lemma}
We set $A=\sigma_z-\sigma_{z'}$ and $B=\Delta_z/\opnorm{\Delta_z}$. For simplicity of presentation assume that $\sigma_{z'}$ is also a valid quantum state, then $A=\frac{2c\eps}{\dims\sqrt{\dims}}\sum_{i}\indic{z_i\ne z_i'}V_i$. Note that $\Delta_z=\frac{c\eps}{\dims\sqrt{\dims}}\sum_{i}V_i$. Then by duality,
\[
\tracenorm{A}\ge \frac{\Tr[\Delta_z A]}{\opnorm{\Delta_z}}=\bigOmega{\frac{\dims}{\eps}}\frac{2c^2\eps^2}{\dims^3}\sum_{i}\indic{z_i\ne z_i'}=\bigOmega{\frac{\eps}{\dims^2}}\ham{z}{z'}.
\]
The second step uses that $V_i$'s are orthonormal and thus $\Tr[V_iV_j]=\indic{i=j}$. 
We note that we can also prove the Hamming separation when $\sigma_{z'}$ is not a valid state and $\Delta_{z'}$ needs to be normalized. The formal lemma statement and proof are in \cref{lemma:hamm-packing}.
\end{proof}

Using \cref{lemma:hamm-separation-informal}, we can argue that a tomography algorithm must be able to guess at least 0.59 fraction of the $z_i$'s correctly,
\begin{proposition}
\label{prop:tomography-guess}
    Let $z\sim \{-1,1\}^{\ell}$ be uniform. Given $\ns$ copies of $\sigma_z$, a tomography algorithm with accuracy $\eps$ in trace distance can obtain a guess $\hat{z}\in \{-1,1\}^{\ell}$ such that
    \[
    \frac{1}{\ell}\sum_{i=1}^\ell\probaOf{z_i\ne\hat{z}_i}\le 0.41.
    \]
\end{proposition}

\subsection{Handling adaptivity via average mutual information}
Let $x_1, \ldots, x_{\ns}$ be the measurement outcomes, and denote $x^t=(x_1, \ldots x_t)$. To complete Assouad's argument, we need to analyze the outcome distributions when the $i$th coordinate is fixed $z_i=+1$ or $z_i=-1$ while other $z_j$ are still chosen uniformly. Denote these distributions as $\p_{+i}^{x^{\ns}}$ and  $\p_{+i}^{x^{\ns}}$ respectively. Using Le Cam's method \cite{LeCam73,yu1997assouad}, the total variation distance must be large to guess $z_i$ correctly,
\[
\Pr_{z_i\sim \{-1,1\}}[z_i\ne \hat{z}_i(x^\ns)]\ge \frac{1}{2}\Paren{1-\totalvardist{\p_{+i}^{x^{\ns}}}{\p_{-i}^{x^{\ns}}}}.
\]
Here $\hat{z}_i$ is an estimator that guesses $z_i$, which is produced by the tomography algorithm in our case. 
Combining with \cref{lemma:hamm-separation-informal}, it is sufficient to upper bound each $\totalvardist{\p_{+i}^{x^{\ns}}}{\p_{-i}^{x^{\ns}}}$. 

However, the total variation distance is hard to compute, especially given that $\p_{+i}^{x^{\ns}}$ are $\p_{-i}^{x^{\ns}}$ complicated mixture distributions.
Furthermore, the dependence between the outcomes $x_1, \ldots, x_{\ns}$ due to adaptivity poses another great challenge. Instead, we use the \emph{average mutual information} \cite{ACLST22iiuic} between the outcomes $x^\ns$ and $z_i$ as a bridge,
$\frac{1}{\ell}\sum_{i=1}^{\ell}\mutualinfo{z_i}{x^\ns}, \ell=\frac{\dims^2}{2}$.

First, when $z\sim \{-1, 1\}^{\ell}$, we can easily relate this quantity to the average error probability of guessing the $z_i$'s using \cite[Lemma 10]{ACLST22iiuic},
\begin{equation}
    \frac{1}{\ell}\sum_{i=1}^{\ell}\mutualinfo{z_i}{x^\ns}\ge 1-h\Paren{\frac{1}{\ell}\sum_{i=1}^\ell\probaOf{z_i\ne\hat{z}_i}}, \;h(p)=-p\log p-(1-p)\log(1-p).
    \label{equ:avg-MI-lower}
\end{equation}

From \cref{prop:tomography-guess}, and that $h$ is increasing in $[0, 1]$, the average mutual information must be lower bounded by a constant $1-h(0.41)$.

It remains to upper bound the average mutual information. Mutual information can be expressed using the KL-divergence, which enjoys a chain rule that helps us to analyze the distribution of each outcome $x_i$ separately even though it may depend on previous outcomes $x^{i-1}$. By further upper-bounding KL-divergence using chi-square divergence, we obtain an upper bound in terms of the measurement information channel. The formal statement is in \cref{thm:avg-MI-upper} and we provide an informal one below.

\begin{theorem}[Average mutual information bound, informal]
\label{thm:avg-MI-upper-informal}
    Let $\ptb\sim\{-1,1\}^{\ell}$ and $\sigma_z$ defined in \eqref{equ:quantum-construction-informal}, and $\out^\ns$ be measurement outcomes. Then,
    \begin{align}
         \frac{1}{\ell}\sum_{i=1}^{\ell}\mutualinfo{\ptb_i}{\out^\ns}&\le  \frac{c_1\ns\eps^2}{\ell^2} \sup_{\POVM\in {\povmset}}\sum_{i=1}^{\ell} \vadj{V_i} \Choi_{\POVM} \vvec{V_i}\label{equ:avg-MI-partial-informal},
    \end{align}
    where $\Choi_{\POVM}$ is the matrix representation of  $\POVM$'s MIC and $c_1$ is an absolute constant.
\end{theorem}

\paragraph{Proof of Pauli tomography lower bound} Recall that we choose $V_1, \ldots, V_{\dims^2/2}$ as the normalized Pauli observables with the largest weights. This roughly consists of all Pauli observables with weights at least $\frac{3\nqubits}{4}$. Using \cref{lem:mic-pauli-informal},  for all Pauli measurement $\POVM$, we have $\vadj{V_i} \Choi_{\POVM} \vvec{V_i}=1$ if the non-identity components of $V_i$ match with $\POVM$ and 0 otherwise. Thus,
\[
\sum_{i=1}^{\ell} \vadj{V_i} \Choi_{\POVM} \vvec{V_i}=\sum_{w=3\nqubits/4}^{\nqubits}{\nqubits\choose w}=\sum_{w=0}^{\nqubits/4}{\nqubits\choose w}= \bigO{2^{\nqubits h(1/4)}}.
\]
The first equality is because there are ${\nqubits\choose w}$  $V_i$'s of weight $w$ with matching non-identity components. 
The final step is due to Stirling's approximation (see \cref{lem:sum-binomial-coef}). 
Due to linearity, the above expression holds for all convex combinations of $\Choi_{\POVM}$. Combining with \eqref{equ:avg-MI-lower} and \eqref{equ:avg-MI-partial-informal}, and noting that $\ell=\dims^2/2$,
\[
\ns =\bigOmega{\frac{2^{\nqubits(4-h(1/4))}}{\eps^2}}.
\]
Finally note that $2^{4-h(1/4)}\ge 9.118$. This completes the lower bound proof.

\paragraph{Plug-and-play lower bound} The summation in \cref{equ:avg-MI-partial-informal} can be further bounded as $\sum_{i=1}^{\ell} \vadj{V_i} \Choi_{\POVM} \vvec{V_i}\le \tracenorm{\Choi_{\POVM}}=\tracenorm{\Luders_{\POVM}}$.
Combining with \eqref{equ:avg-MI-lower}, we have a more convenient result,
\begin{corollary}
    Let $\eps\le 1/200$ and $\povmset$ be the set of allowed measurements for each copy. Then the sample complexity of adaptive tomography with constraint $\povmset$ is
    \[
    \ns = \bigOmega{\frac{\dims^4}{\eps^2\sup_{\POVM\in {\povmset}}\tracenorm{\Luders_{\POVM}}}}.
    \]
\end{corollary}
Thus we provide a plug-and-play adaptive tomography lower bound for all measurement constraints $\povmset$ without going through the complications of adaptivity. We demonstrate it for finite-outcome measurements,
\begin{corollary} \label{cor:finite-lower}
    By \cite[Lemma 2.3]{liu2024restricted}, $\tracenorm{\Luders_{\POVM}}\le \min\{\ab, \dims\}$ for $\POVM$ with at most $\ab$ outcomes. Thus, the sample complexity lower bound for adaptive tomography with $\ab$-outcome measurements is
    \[
    \ns = \bigOmega{\frac{\dims^4}{\eps^2\min\{\ab, \dims\}}}.
    \]
\end{corollary}
This recovers the lower bound in \cite{lowe2022lower} for constant-outcome measurements (in fact improves it by a $\log\dims$ factor) and the single-copy adaptive lower bound in \cite{chen2023does}. Furthermore, we show in \cref{sec:finite-upper} that the lower bound is tight up to log factors for general $\ab$.


\subsection{Refined upper bound analysis}
Like the previous work of \cite{guctua2020fast}, we use a non-adaptive scheme where we apply each Pauli measurement to the same number of copies $m\eqdef\ns/3^{\nqubits}$. Each Pauli measurement contributes to the empirical estimate $\hat{\alpha}_P$ of $\alpha_P$ in \eqref{equ:pauli-decomposition} where $P$ has matching non-identity components. Our estimator is given by 
\[
\hat{\rho}=\frac{\eye_\dims}{\dims}+\sum_{P}\hat{\alpha_P}P.
\]
Using the fact that a weight-$w$ Pauli observable can be learned with $3^{\nqubits-w}$ Pauli measurements, 
we compute the expected squared-$\ell_2$ distance between the coefficients ${\sum_{P}|\hat{\alpha}_P-\alpha_P|^2}$,
which is exactly $\hsnorm{\hat{\rho}-\rho}^2$. 
Finally we use Cauchy-Schwarz $\hsnorm{\hat{\rho}-\rho}\ge \tracenorm{\hat{\rho}-\rho}/\sqrt{\dims}$ and Jensen's inequality to obtain the error in trace distance. 
Details can be found in \cref{sec:pauli-upper}. 