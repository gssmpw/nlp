\subsection{Mutual information lower bound}
We state some useful bounds on mutual information.
\begin{lemma}[{\cite[Lemma 10]{ACLST22iiuic}}]
\label{lem:MI-lower}
    Let $Z\in\{-1, 1\}^\ab$ be drawn uniformly and $Z-Y-\hat{Z}$ be a Markov chain where $\hat{Z}$ is an estimate of $Z$. Let $h(t)\eqdef -t\log t-(1-t)\log(1-t)$, then for each $i\in[\ab]$,
    \[
    \mutualinfo{Z_i}{Y}\ge 1-h(\probaOf{Z_i\ne \hat{Z}_i}).
    \]
\end{lemma}

The following lemma is an Assouad-type lower bound on the average mutual information. 
\begin{lemma}
\label{lem:avg-MI-lower}
    Let $\sigma_\ptb\sim\ptbDistr(\hbasis)$ where $\ptb\sim\{-1,1\}^{\ell}$, $\out^\ns$ be the outcomes after applying $\POVM^\ns$ to $\sigma_\ptb^{\otimes\ns}$, and $\qest$ be an estimator using $\out^\ns$ that achieves an accuracy of $\eps$. Then,
    \begin{equation}
        \frac{1}{\ell}\sum_{i=1}^{\ell}\mutualinfo{Z_i}{Y}\ge\frac{1}{100}.
    \end{equation}
\end{lemma}


Combining \cref{lem:avg-MI-lower} and \cref{thm:avg-MI-upper} proves the interactive lower bound for tomography.
\begin{proof}
   The idea behind this bound is that any good estimation $\qest$ of the parameterized state $\sigma_\ptb$ is close in the sense that the closest parameterized $\sigma_{\zest}$ to $\qest$ should also be sufficiently close. Then, we can relate the distance $\tracenorm{\sigma_{\ptb} - \sigma_{\zest}}$ to the hamming distance in $\sum_{i=1}^\ell \indic{z_i \neq \zest_i}$. Once this relation is established, then a optimal tomography algorithm should also have low probability of error in estimating $z$ with $\zest$. Thus, leading to lower bound of mutual information with the application of \cref{lem:MI-lower}. We begin by first bounding the error between the "parameterized version" of the estimator and $\sigma_{\hat{\ptb}}$,
   \begin{align*}
        \zest &:= \argmin_{\ptb \in \{-1,1\}^\ell } \tracenorm{\sigma_{\ptb} - \qest}\\
        \tracenorm{\sigma_{\zest} - \sigma_{\ptb}} &\leq \tracenorm{\sigma_{\ptb} - \qest} + \tracenorm{\qest-\sigma_{\zest}} \leq 2 \tracenorm{\sigma_{\ptb} - \qest},
   \end{align*}
   where the last line holds since $\tracenorm{\qest-\sigma_{\zest}} \leq \tracenorm{\qest-\sigma_{\ptb}}$ by definition of $\hat{\ptb}$. Notice $ \tracenorm{\qest-\sigma_{\ptb}} \leq \eps \implies \tracenorm{\sigma_{\zest} - \sigma_{\ptb}} \leq 2\eps $. Thus, 
   $$\Pr[\tracenorm{\sigma_{\zest} - \sigma_{\ptb}} \leq 2 \eps] \ge \Pr[\tracenorm{\sigma_{\zest} - \sigma_{\ptb}} \leq \eps] \geq 0.99.$$
   Now, we will introduce a lemma that will allow us to construct a informaton-theoretic packing around this estimator. This is done by relating the trace distance and the hamming distance between Z parameters. We present the formal version of~\cref{lemma:hamm-separation-informal}

   \begin{lemma}[Trace distance Hamming separation] \label{lemma:hamm-packing}
       Consider $z \in \mathcal{G}$, where $\mathcal{G}$ is defined from \cref{eq:concentrated-set}. For any  $\hat{z} \in \left\{-1,1\right\}^{\ell}$,
       \begin{equation}
           \tracenorm{\sigma_\ptb - \sigma_{\zest}} \geq \frac{c \eps}{2\kappa_\alpha \ell} \ham{\ptb}{\zest}.
       \end{equation}
   \end{lemma}

   \begin{proof}
       
   Let $C_{z} := \min\left\{1, \frac{1}{2\dims \opnorm{\Delta_{z}}}\right\}$ and define the matrices,
   \[\Delta_{w} := \frac{c \eps}{\sqrt{d\ell}}  \sum_{i=1}^\ell \indic{z_i \neq \hat{z}_i} z_i V_i, \;\Delta_{c} := \frac{c \eps}{\sqrt{d\ell}} \sum_{i=1}^\ell \indic{z_i = \hat{z}_i} z_i V_i.
   \]
   Notice the trace norm of distance between perturbation matrices has the following form,
   \begin{align*}   
   \tracenorm{\sigma_{\zest} - \sigma_{\ptb}} & = \tracenorm{\barDelta_{\hat{\ptb}} - \barDelta_{\ptb}} \\
   &=\tracenorm{C_{\hat{z}} \Delta_{\hat{\ptb}} - C_{z} \Delta_{\ptb}} \\
   &= \frac{c \eps}{\sqrt{d\ell}} \tracenorm{(-C_z-C_{\zest}) \sum_{i=1}^\ell \indic{z_i \neq \zest_i} z_i V_i + (C_{\zest} - C_z) \sum_{i=1}^\ell \indic{z_i = \zest_i} z_i V_i} \\
   &= \tracenorm{(C_z+C_{\zest}) \Delta_w + (C_z-C_{\zest}) \Delta_c)}.
   \end{align*}
    Now, we will take advantage of the duality between the trace and operator norm (\cref{lemma:trace-norm-dual}) to correlate the distance between perturbations to the hamming distance between $z$ and $\zest$. Let $W_z = \sum_{i=1}^\ell z_i V_i$. For $z$ such that $\opnorm{W_z} \leq \kappa_\alpha \sqrt{\dims}$, we have $C_z=1$, from \cref{eq:op-norm-bound}.
   \begin{align*}
    \tracenorm{\sigma_{\zest} - \sigma_{\ptb}} &=
     \tracenorm{((1+C_{\zest}) \Delta_w + (1-C_{\zest}) \Delta_c} = \sup_{\opnorm{B} \leq 1} |\Tr[B^{\dagger} \left[(1+C_{\zest}) \Delta_w + (1-C_{\zest}) \Delta_c\right]]| \\
     &\geq \frac{1}{\kappa_\alpha \sqrt{\dims}} 
     |\Tr[W_z^{\dagger} \left[(1+C_{\zest}) \Delta_w + (1-C_{\zest}) \Delta_c\right]]| = \frac{c \eps}{\sqrt{\dims \ell}} \frac{1}{\kappa_\alpha \sqrt{\dims}} |(1+C_{\zest}) \delta_w + (1-C_{\zest}) \delta_c| \\
     &= \frac{c \eps}{\kappa_\alpha d \sqrt{\ell}} \left[(1+C_{\zest}) \delta_w + (1-C_{\zest}) \delta_c\right] \geq \frac{c \eps}{2 \kappa_\alpha \ell} \delta_w.
   \end{align*}
   Where $\delta_w = \ham{z}{\zest}$ and $\delta_c = \ell - \delta_w = \ell - \ham{z}{\zest}$.The second inequality uses: $B = \frac{W_z}{\kappa_\alpha \sqrt{\dims}}$. The reduction $\Tr[W_z^{\dagger} \Delta_w] = \delta_w$ and $\Tr[W_z^{\dagger} \Delta_c] = \delta_c$ comes directly from the orthonormality of the peturbation matrices $\{V_i\}_{i=1}^\ell$ under the inner product: $\hdotprod{A}{B} = \vvdotprod{A}{B} = \Tr[A^\dagger B]$. With the last line, we have shown the desired bound.  
   \end{proof}
    
   Since this relation to $\ham{\cdot}{\cdot}$ only occurs for a concentrated set $\mathcal{G}$, we can show that the expected hamming distance is "approximately trace distance" for sufficiently large $\dims > \frac{\ln{5}}{\alpha}$. $\sigma_{\hat{z}}$ also has to be close to $\sigma_z$ with high probability to be a sufficient estimator of $\sigma_z$, inducing a upper bound on the error probability of estimating $Z$,
   \begin{align}
       \frac{1}{\ell} \expectDistrOf{}{\delta_w} &= \frac{1}{\ell} \expectDistrOf{}{\delta_w \mid \tracenorm{\sigma_z - \sigma_{\zest}} \leq 2 \eps} \Pr[\tracenorm{\sigma_z - \sigma_{\zest}} \leq 2 \eps] \nonumber\\
       &+ \frac{1}{\ell} \expectDistrOf{}{\delta_w | \tracenorm{\sigma_z - \sigma_{\zest}} > 2 \eps} \Pr[\tracenorm{\sigma_z - \sigma_{\zest}} > 2 \eps] \nonumber\\
       &\leq \frac{1}{\ell} \expectDistrOf{}{\delta_w \mid \tracenorm{\sigma_z - \sigma_{\zest}} \leq 2 \eps} + 0.01.  \label{eq:cond-expect-tom-lower}
   \end{align}
   It is enough to upper bound the remaining expectation term by a constant. We will case on whether $z$'s lead to a approximate hamming relationship with trace distance. When $z \in \mathcal{G}$, we apply \cref{lemma:hamm-packing}
   \begin{align*}
       \frac{c \eps}{2 \kappa_\alpha \ell} \delta_w \leq \tracenorm{\sigma_z - \sigma_{\zest}} \leq 2 \eps 
       \implies  \frac{1}{\ell} \delta_w \leq \frac{4 \kappa_\alpha}{c}.
   \end{align*}
   The conditional expectation will now be bounded by a small constant for $c \geq 10 \kappa_\alpha$,
   \begin{align*}
       \frac{1}{\ell} \expectDistrOf{}{\delta_w \mid \tracenorm{\sigma_\ptb - \sigma_{\zest}} \leq 2 \eps} &\leq \Pr[z \in \mathcal{G}]\frac{1}{\ell} \expectDistrOf{}{\delta_w \mid \tracenorm{\sigma_\ptb - \sigma_{\zest}} \leq 2 \eps \land z \in \mathcal{G}} \\
       &+\Pr[z \notin \mathcal{G}] \frac{1}{\ell} \expectDistrOf{}{\delta_w \mid \tracenorm{\sigma_z - \sigma_{\zest}} \leq 2 \eps \land z \notin \mathcal{G}} \\
       &\leq \frac{4 \kappa_\alpha}{c} +  2\exp\{-\alpha d\} \cdot 1 \leq \frac{2}{5} + \frac{2}{5} = 0.40.
   \end{align*}
   Substituting this result into \cref{eq:cond-expect-tom-lower}, we have $\frac{1}{\ell} \sum_{i=1}^\ell \Pr[Z_i \neq \hat{Z_i}] = \frac{1}{\ell} \expectDistrOf{}{\delta_w} \leq 0.41$. We can then apply \cref{lem:MI-lower} to obtain the mutual information bound,
   \begin{align*}
       \frac{1}{\ell} \sum_{i=1}^\ell \mutualinfo{Z_i}{Y} \geq 1 - h\left(\frac{1}{\ell} \sum_{i=1}^\ell \Pr[Z_i \neq \hat{Z_i}]\right) \geq 1 - h\left(0.41\right) \geq \frac{1}{100}.
   \end{align*}
   The first inequality is due to the concavity of the binary entropy function $h$.
\end{proof}