\newcommand{\MUB}{\POVM_{MUB}}
\section{Upper bound for tomography with finite outcomes}
\label{sec:finite-upper}
We will show the tightness of the adaptive tomography bounds for k-outcome POVMs by modifying the Projected Least Squares Method (PLS)~\cite{guctua2020fast} to work with k-outcome POVMs. We present these adjustments for the case when $k = d$ and $k < d$. As a result, we will have the first upper and lower bounds for adaptive tomography for k-outcome measurements, where the upper bound is achieved with non-adaptive algorithms. The key component is reducing the $\MUB$ to a k outcome measurement,
$$\MUB \eqdef \left\{\frac{1}{d+1} \ket{\psi_x^k} \bra{\psi_x^k}\right\}_{k \in [d+1], x \in [d]},$$
where each fixed $k$ corresponds to each one of the Maximally mutually unbiased bases. The reduction will follow similarly to~\cite{liu2024restricted}.
\subsection{Algorithm for $k=d$} \label{sub-keqd}
Measuring with $\MUB$ acts as a uniform sampling $i \sim Unif([d+1])$ to select one of the MUB and measuring with the POVM described by $\left\{\ket{\psi_x^i} \bra{\psi_x^i}\right\}_{x \in [d]}$. So, we can split each of the MUB bases across the multiple copies and uniformly sample amongst them to replicate the outcome distribution of measuring with $\MUB$. 

\begin{algorithm}
\caption{Finite Outcome Tomography for $k = d$}\label{alg:tom-keqd}
\hspace*{0.1cm} \textbf{Input:} $n$ copies of state $\rho$ \\
\hspace*{0.1cm} \textbf{Output:} Estimate $\hat{\rho} \in \mathcal{C}^{d \times d}$
\begin{algorithmic}
\State Divide $\MUB$ into $d+1$ groups of d-outcome measurements $\mathcal{M}_j := \left\{\ket{\psi_x^j} \bra{\psi_x^j}\right\}_{x \in [d]}$.
\State Divide $n$ copies into $d+1$ equally sized groups, each group has $n_0 = n/(d+1)$ copies.
\For {$j = 1, ..., d+1$}
\State For group $j$, apply $\mathcal{M}_j$. Let the outcomes be $x_1^{(j)}, ..., x_{n_0}^{(j)}$.
\EndFor
\State Generate n/2 i.i.d samples from $Unif([d+1])$ and  let $m_j$ be the number of times $j$ appears.
\State Let $x = (x_1, ..., x_{d+1})$ where $x_j = (x_1^{(j)}, ..., x_{\min\{n_0, m_j\}}^{(j)}$)
\State From $x$, obtain empirical frequencies $F = (f_1, ..., f_{d (d+1)})$ by obtaining group specific frequencies of each $x_i$ and concatenating the frequency vectors together.
\State \Return $\hat{\rho} = PLS(F)$
\end{algorithmic}
\end{algorithm}

For the analysis, we will use the multiplicative Chernoff Bound for sums of i.i.d random variables.
\begin{lemma}[Multiplicative Chernoff Bound]
    \label{mult_chernoff}
   Let $X_1, ..., X_n$ be i.i.d with $\expectDistrOf{}{X_1} = \mu$. Then, 
   $$\Pr\left[\sum_{i}^n X_i \geq n(1+\alpha)\mu\right] \leq \exp{\left\{-\frac{n \alpha^2 \mu}{2 + \alpha}\right\}}\;, \alpha > 0$$ 
   $$\Pr\left[\sum_{i}^n X_i \geq n(1-\alpha)\mu\right] \leq \exp{\left\{-\frac{n \alpha^2 \mu}{2}\right\}}\;, \alpha \in (0,1)$$ 
\end{lemma}
\begin{theorem} \label{thm_keqd}
    For $k=d$, Algorithm \cref{alg:tom-keqd} will give estimate $\hat{\rho}$ such that $\Pr[\tracenorm{\hat{\rho} - \rho} \leq \eps] \geq \frac{2}{3}$ with $n = O\left( \frac{d^3 \log d}{\eps^2}\right)$
\end{theorem}
\begin{proof}
Notice that each sample made will follow the outcome distribution of applying $\MUB$ to a single copy of $\rho$. 
Given $n$ copies, it will be shown that $\frac{n}{2}$ such samples will be made with sufficiently high probability. This is when $m_j \leq n_j$ for all $j \in [d+1]$. 
Using~\cref{mult_chernoff}, on the $m_j \sim Bin(\frac{n}{2}, \frac{1}{d+1})$, which is sum of $Y_1,...,Y_{\frac{n}{2}} \sim Bern(\frac{1}{d+1})$, $\mu = \expectDistrOf{}{Y_1} = \frac{1}{d+1}$,
\begin{align*}
    \Pr\left[m_j > n_j\right] = \Pr\left[\sum_{i=1}^\frac{n}{2} Y_i > 2 n \mu \right] \leq \exp{\left\{-\frac{n}{6(d+1)}\right\}}.
\end{align*}
Furthermore, by union bound,
\begin{align*}
    \Pr\left[\exists_j m_j > n_j\right] \leq \sum_{j=1}^{d+1} \Pr\left[m_j > n_j\right] \leq (d+1) \exp{\left\{-\frac{n}{6(d+1)}\right\}}.
\end{align*}
From previous work \cite{guctua2020fast}, we have the following guarantee on the estimation error using the PLS method using the outcome of $\MUB$ measurements,
\begin{align*}
    \Pr\left[\tracenorm{\hat{\rho}_n - \rho} \geq \eps \right] \leq d \exp{\left\{-\frac{n \eps^2}{86 d^3}\right\}}.
\end{align*}
With the algorithm, we can bound the probability of the estimate not being optimal,
\begin{align*}
\Pr\left[\tracenorm{\hat{\rho} - \rho} \geq \eps \right] &\leq \Pr\left[\exists_j m_j > n_j \lor \tracenorm{\hat{\rho}_{n/2} - \rho} \geq \eps\right] \\
&\leq (d+1) \exp{\left\{-\frac{n}{6(d+1)}\right\}} + d \exp{\left\{-\frac{n \eps^2}{172 d^3}\right\}}.
\end{align*}
With $d \geq 16$ and $n = \frac{172d^3 \ln{200d}}{\eps^2}  = O\left(\frac{d^3 \log{d}}{e^2} \right)$, we will have $\Pr\left[\tracenorm{\hat{\rho} - \rho} \le \eps \right] \geq \frac{99}{100}$
\end{proof}
\subsection{Algorithm for $k < d$}
For $\ab<\dims$, it is helpful to think of the problem as follows: there are $\ns$ players, each of whom holds a copy of $\rho$, but can only send $\log\ab$ classical bits to a central server that collects the messages and learn about the state. 

The idea is then to simulate each $\dims$-outcome POVM using only $\log\ab$ bits for each player. 
Using results from \cite{ACT:19:IT2}, the number of players (or copies) required to simulate the original $\dims$-outcome POVM is roughly $O(\dims/\ab)$, and thus we have a $O(\dims/\ab)$ factor blow up in the sample complexity compared to $\dims$-outcome measurements.

\begin{definition} [$\eta$-Simulation]
\label{def-simulate}
We are given $n$ players each with i.i.d sample from an unknown distribution $\p \in \Delta_d$. Each player can only send $w$ bits to the server. The server can then perform a $\eta$-simulation where $\hat{X} = [d] \cup \{\perp\}$.
\begin{equation}
    \Pr[\hat{X} = x \mid \hat{X} \neq \perp] = p_x, \; \Pr[\hat{X} = \perp] \leq \eta
\end{equation}

It can be shown that there exists an algorithm that can perform a $\eta$ simulation with $O(\dims/\ab)$ players,

\end{definition}
\begin{theorem}[\cite{ACT:19:IT2}, Theorem IV.5]
\label{thm:simulation}
   For every $\eta \in (0,1)$, there exists an algorithm that $\eta$ simulates $p \in \Delta_d$ using 
   \begin{equation}
       M = 40 \left\lceil \log{\frac{1}{\eta}} \right\rceil \left\lceil \frac{d}{2^w - 1} \right\rceil
   \end{equation}
   players from the setting in \cref{def-simulate}. The algorithm only requires private randomness for each player.
\end{theorem}
Therefore, for each MUB measurement $\POVM$ we assign $M=O(\dims/\ab)$ players. 
Each player applies $\POVM$ to $\rho$ and compresses the outcome to $\log\ab$ bits using the simulation algorithm in \cref{thm:simulation}. 
This process is a valid $\ab$-outcome POVM. 
The server then can use the $M=O(\dims/\ab)$ messages to simulate the outcome of $\POVM$ applied to $\rho$.
From \cref{thm_keqd}, we need $\tildeO{\dims^3/\eps^2}$ simulated samples, and thus the total number of copies required to simulate those samples is $M=O(\dims/\ab)$ times more. The detailed proof is given in \cref{thm:k-outcome-tomography}.
% ect $O(\frac{d}{k})$ multiplicative factor on the copy complexity from \cref{sub-keqd}, as each simulation of a per copy measurement will require $O(\frac{d}{k})$ samples. 

% \begin{algorithm}
% \caption{Finite Outcome Tomography for k < d}\label{alg:tom-klessd}
% \hspace*{0.1cm} \textbf{Input:} $n$ copies of state $\rho$ \\
% \hspace*{0.1cm} \textbf{Output:} Estimate $\hat{\rho} \in \mathcal{C}^{d \times d}$
% \begin{algorithmic}
% \State Divide $\MUB$ into $d+1$ groups of d-outcome measurements $\mathcal{M}_j := \left\{\ket{\psi_x^j} \bra{\psi_x^j}\right\}_{x \in [d]}$.
% \State Divide $n$ copies into $d+1$ equally sized groups, each group has $n_0 = n/(d+1)$ copies.
% \For {$j = 1, ..., d+1$}
% \State For group $j$, apply $\mathcal{M}_j$. Let the outcomes be $y_1^{(j)}, ..., y_{n_0}^{(j)}$.
% \State Perform distributed simulation: Splitting up into $\frac{n_0}{M}$ groups, each group producing a single sample. 
% \State Let the non-aborted samples be $x_1^{(j)}, ..., x_{n_j}^{(j)}$
% \EndFor
% \State Generate n/2 i.i.d samples from $Unif([d+1])$ and  let $m_j$ be the number of times $j$ appears.
% \State Let $x = (x_1, ..., x_{d+1})$ where $x_j = (x_1^{(j)}, ..., x_{\min\{n_j, m_j\}}^{(j)}$)
% \State From $x$, obtain empirical frequencies $F = (f_1, ..., f_{d (d+1)})$ by computing group specific frequencies of each $x_i$ and concatenating the frequency vectors together.
% \State \Return $\hat{\rho} = PLS(F)$
% \end{algorithmic}
% \end{algorithm}

\begin{theorem}
\label{thm:k-outcome-tomography}
For $\ab < \dims$, Algorithm 1 with distributed simulation will give estimate $\hat{\rho}$ such that $Pr[\tracenorm{\rho-\hat{\rho}} \le \eps ]\ge 0.99$ with  $\ns = \bigO{\frac{\dims^4\log\dims}{\ab\eps^2}}$.
\end{theorem}
\begin{proof}
   The proof will follow the same steps as \cref{thm_keqd}, but also considering $n_j \sim Bin(1-\eta,n_0/M)$ taking the role of $n_0$. Since $n_j$ and $m_j$ are both Binomial random variables, it is enough to say that $m_j$ and $n_j$ are on the opposite sides of a mean threshold with exponentially decreasing probability. Denote $\hat{n}_0 = \frac{n_0}{M}$ and $\hat{n} = \frac{n}{M}$,
   \begin{align*}
       \Pr\left[m_j \leq n_j\right] &\geq \Pr\left[m_j \leq \frac{3}{4} \hat{n}_0 \land n_j \geq \frac{3}{4} \hat{n}_0\right] \\
       \Pr\left[m_j > n_j\right] &< \Pr\left[m_j > \frac{3}{4} \hat{n}_0 \lor n_j < \frac{3}{4} \hat{n}_0\right].
   \end{align*}
   We will now bound each of the above union events with exponentially decreasing probability. We will apply \cref{mult_chernoff} on $m_j \sim Bin(\hat{n}/2, \frac{1}{d+1}), \expectDistrOf{}{m_j} = \hat{n}_0/2$ with $\alpha = 1/2$,
   \begin{align*}
       \Pr\left[m_j > \frac{3}{4} \hat{n}_0\right] \leq \exp{\left\{- \frac{\hat{n}}{10(d+1)}\right\}}.
   \end{align*}
   Now, we will apply \cref{mult_chernoff} once more for $n_j \sim Bin(1-\eta,\hat{n}_0), \expectDistrOf{}{n_j} = (1-\eta) \hat{n}_0$ and $\alpha = \frac{1/4 + \eta}{\eta} \geq \frac{1}{4}$,
   \begin{align*}
          \Pr\left[n_j > \frac{3}{4} \hat{n}_0\right] \leq \exp{\left\{- \frac{\hat{n}}{32(d+1)}\right\}}.
   \end{align*}
   Thus, 
   \begin{align*}
       \Pr\left[m_j > n_j \right] &\leq \Pr\left[m_j > \frac{3}{4} \hat{n}_0 \right] + \Pr\left[n_j < \frac{3}{4} \hat{n}_0\right] \\
       &\leq 2 \exp{\left\{- \frac{\hat{n}}{32(d+1)}\right\}}.
   \end{align*}
   By union bound,
   \begin{align*}
       \Pr\left[\exists_j m_j > n_j \right] \leq (d+1) \exp{\left\{- \frac{\hat{n}}{32(d+1)}\right\}}.
   \end{align*}
   Now we will repeat the argument from \cref{sub-keqd} for plugging in the samples into the PLS estimator,
   \begin{align*}
       \Pr\left[\tracenorm{\hat{\rho} - \rho} > \eps \right] &\leq \Pr\left[\exists_j m_j > n_j \cup \tracenorm{\hat{\rho}_{\hat{n}/2} - \rho} > \eps\right] \\
       &\leq (d+1) \exp{\left\{-\frac{\hat{n}}{32(d+1)}\right\}} + d \exp{\left\{-\frac{\hat{n} \eps^2}{172 d^3}\right\}}.
   \end{align*}
With $d \geq 16$ and $\hat{n} = \frac{172d^3 \ln{200d}}{\eps^2}$, we will have $\Pr\left[\tracenorm{\hat{\rho} - \rho} \le \eps \right] \geq \frac{99}{100}$. With $w = \log k$ and $\eta = 0.01$, we have that $\hat{n} = \Theta(\frac{k}{d}) \cdot n$, so $n = O(\frac{d^4 \log{d}}{k \eps^2} )$.
\end{proof}
Thus, the upper bound can be compactly written as $O\left(\frac{\dims^4 \log{d}}{\eps^2\min\{\ab, \dims\}}\right)$, combining the $k=d$ and $k < d$ cases. With \cref{cor:finite-lower}, we have proven \cref{thm:nearly-tight-finite-out}.

\begin{remark}
    We note that running the distributed simulation with $\log\ab$ bits requires first obtaining the $\dims$ outcomes for each qubit. Thus, the algorithm is more relevant in the distributed setting as described in this section. Nevertheless, the compression step for each copy defines a valid $\ab$-outcome measurement and thus proves that our lower bound in~\cref{cor:finite-lower} is tight.
\end{remark}
