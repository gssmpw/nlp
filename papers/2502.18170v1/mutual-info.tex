\section{Adaptive tomography lower bound}
\subsection{Lower bound construction}
\begin{definition}
 \label{def:perturbation}
     Let $\dims^2/2\le\ell\le\dims^2-1$ and $\hbasis=(V_1, \ldots, V_{\dims^2}=\frac{\eye_\dims}{\sqrt{\dims}})$ be an orthonormal basis of $\Herm{\dims}$, and $\cd$ be a universal constant. Let  $\ptb=(\ptb_1, \ldots, \ptb_\ell)$ be uniformly drawn from $\{-1, 1\}^\ell$,
     \begin{equation}
         \Delta_{\ptb} = \frac{\cd\eps}{\sqrt{\dims}}\cdot\frac{1}{\sqrt{\ell}}\sum_{i=1}^\ell \ptb_iV_i, \quad \barDelta_{\ptb}= \Delta_{\ptb}\min\left\{1, \frac{1}{2\dims \opnorm{\Delta_{\ptb}}}\right\},
         \label{equ:delta_z}
     \end{equation}
     Finally we set $\sigma_{\ptb}=\qmm + \barDelta_{\ptb}$ whose distribution we denote as $\ptbDistr(\hbasis)$.
 \end{definition}

The construction adds independent binary perturbations to $\qmm$ along $\ell$ orthogonal trace-0 directions. With appropriate constant $\cd$, $\ptbDistr(\hbasis)$ has an exponentially small probability mass outside the set $\mathcal{P}_\eps\eqdef\{\rho: \tracenorm{\rho-\qmm}>\eps\}$.
\begin{theorem}[{\cite[Corollary 4.4]{liu2024role}}]
\label{prop:perturbation-trace-distance}
    Let $\cd= 10\sqrt{2}$, $\ell\ge \dims^2/2$, $\eps<1/200$. Then for $\sigma\sim \ptbDistr(\hbasis)$,  $\|\sigma-\qmm\|_1\ge \eps$ with probability at least $1-2\exp(-\dims)$. 
\end{theorem}

This is the result of a random matrix concentration.
\begin{restatable}[{\cite[Theorem 4.2]{liu2024role}}]{theorem}{randmatopnorm}
\label{thm:rand-mat-opnorm-concentration}
    Let $V_1, \ldots, V_{\dims^2}\in\C^{\dims\times \dims}$ be an orthonormal basis of $\C^{\dims\times \dims}$ and $\ptb_1, \ldots, \ptb_{\dims^2}\in\{-1, 1\}$ be independent symmetric Bernoulli random variables. Let $W=\sum_{i=1}^{\ell}\ptb_iV_i$ where $\ell\le \dims^2$. For all $\alpha>0$, there exists $\cop_\alpha$, {which is increasing in $\alpha$} such that
    \[
    \probaOf{\opnorm{W}>\cop_\alpha\sqrt{\dims}}\le 2\exp\{-\alpha\dims\}.
    \]
\end{restatable}

Let $z\sim\{-1,1\}^{\ell}$ and $\sigma_z\sim \ptbDistr(\hbasis)$ be defined in \cref{def:perturbation}. Use the shorthand $\p_z^{\out_i|\out^{i-1}}=\p_{\sigma_z}^{\out_i|\out^{i-1}}$. 
We define the following mixtures,
\begin{equation}
    \p_{+i}^{\out^\ns}\eqdef \frac{1}{2^{\ell-1}}\sum_{\ptb:\ptb_i=+1}\p_z^{\out^\ns},\quad  \p_{-i}^{\out^\ns}\eqdef \frac{1}{2^{\ell-1}}\sum_{\ptb:\ptb_i=-1}\p_z^{\out^\ns}.
\end{equation}
Which are the distributions of outcomes $\out^\ns$ when we fix the $i$th coordinate to be $+1$ and $-1$ respectively. Then we can define,
\begin{equation}
\label{equ:out-distr}
\q^{\out^\ns}\eqdef\frac{1}{2^\ell}\sum_{z\in\{-1,1\}^\ell}\p_z^{\out^\ns}=\frac{1}{2}(\p_{+i}^{\out^\ns}+\p_{-i}^{\out^\ns}).  
\end{equation}

This is exactly the distribution of $\out^\ns$ when $\sigma_z\sim \ptbDistr(\hbasis)$ and outcomes $\out^\ns$ are obtained by measuring $\sigma_z^{\otimes\ns}$ with the adaptive scheme $\POVM^\ns$.




\subsection{Mutual information upper bound via MIC}
The following theorem bounds the mutual information in terms of the measurement information channel.
\begin{theorem}
\label{thm:avg-MI-upper}
    Let $\sigma_\ptb\sim\ptbDistr(\hbasis)$ where $\ptb\sim\{-1,1\}^{\ell}$, $\out^\ns$ be the outcomes after applying $\POVM^\ns$. Then for $\dims\ge 1024$ and all $t\in[\ns]$,
    \begin{align}
         \frac{1}{\ell}\sum_{i=1}^{\ell}\mutualinfo{\ptb_i}{\out^t}&\le \frac{8 tc^2 \eps^2}{\ell^2}  \sup_{\POVM\in {\povmset}}{\sum_{i=1}^\ell \vadj{V_i} \Choi_{\POVM} \vvec{V_i}} +16\exp\{-\alpha\dims\}tc^2\eps^2 \label{equ:avg-MI-partial}\\
         &\le \frac{16tc^2\eps^2}{\ell^2}\tracenorm{\povmset}.\label{equ:avg-MI-tracenorm}
    \end{align}
\end{theorem}

\begin{proof}
    We start with the fundamental fact that mutual information is the conditional KL-divergence between the conditional distribution given the marginal $x^t$: $\p^{\out^t}_{z_i}$ for $1 \leq i \leq n$ and the marginal distribution $\q^{\out^t}$,
\begin{align*}
    I(z_i;x^t) &= \kldiv{\p^{\out^t}_{z_i}}{\q^{\out^t} \mid z_i} = \expectDistrOf{z_i}{\kldiv{\p^{\out^t}_{z_i}}{\q^{\out^t}}} \\ &= \frac{1}{2} \kldiv{\p_{+i}^{\out^t}}{\q^{\out^t}} + \frac{1}{2} \kldiv{\p_{-i}^{\out^t}}{\q^{\out^t}} \\ 
    &= \frac{1}{2} \kldiv{\p_{+i}^{\out^t}}{\frac{\p_{+i}^{\out^t} + \p_{-i}^{\out^t}}{2}} + \frac{1}{2} \kldiv{\p_{-i}^{\out^t}}{\frac{\p_{+i}^{\out^t} + \p_{-i}^{\out^t}}{2}}.
\end{align*}
Thus, by convexity, 
\begin{align}
    I(z_i;x^t) &\leq \frac{1}{4} \left[\kldiv{\p^{\out^t}_{+i}}{\p^{\out^t}_{+i}}+\kldiv{\p^{\out^t}_{-i}}{\p^{\out^t}_{+i}} \right] = \frac{1}{2} \kldivsym{\p^{\out^t}_{+i}}{\p^{\out^t}_{-i}} \label{pf:MI-bound}.
\end{align}
Where the last inequality comes from the convexity of KL-divergence with respect to its second argument. Given this symmetric KL-divergence between the mixture distribution conditioned on the i-th perturbation, we can further narrow the correlation between the measurement outcomes and the perturbation with the change in measurement outcome distribution when $z_i$ is flipped. We apply chain rule on the symmetric KL-divergence to allow us to isolate the per measurement round divergence,
\begin{align}
    \kldivsym{\p^{\out^t}_{+i}}{\p^{\out^t}_{-i}} = \sum_{j=1}^{t} \expectDistrOf{\q^{x^\ns}}{\kldivsym{\p^{\out_{j}|\out^{j-1}}_{+i}}{\p^{\out_{j}|\out^{j-1}}_{-i}}}.
    \label{eq:per-round-divergence}
\end{align}
We bound the symmetric KL by the chi-squared divergence,
\begin{align*}
    \kldivsym{\p^{\out_{j}|\out^{j-1}}_{+i}}{\p^{\out_{j}|\out^{j-1}}_{-i}} &\le \chisquare{\p^{\out_{j}|\out^{j-1}}_{+i}}{\p^{\out_{j}|\out^{j-1}}_{-i}} \\
    &\leq \frac{1}{2^{l-1}} \sum_{z \in \{+1,-1\}^\ell} \; \chisquare{\p^{\out_{j}|\out^{j-1}}_{z}}{\p^{\out_{j}|\out^{j-1}}_{z^{\oplus i}}}  \\
    &= \frac{1}{2^{\ell-1}} \sum_{z \in \{+1,-1\}^\ell} \; \expectDistrOf{X \sim \p^{\out_{j}|\out^{j-1}}_{z^{\oplus i}}}{\delta_j(X)^2}.
\end{align*}
 Where the last inequality is from the joint convexity of f-divergences. $\delta_t(X)$ follows the definition,
\begin{align*}
    \delta_j(x) \eqdef \frac{\p^{\out_{j}|\out^{j-1}}_{z}(x)-\p^{\out_{j}|\out^{j-1}}_{z^{\oplus i}}(x)}{\p^{\out_{j}|\out^{j-1}}_{z^{\oplus i}}(x)}.
\end{align*}
Furthermore, $\delta_j$ term can be bounded by extracting the MIC channel,
\begin{align*}
    \delta_j(x) &= \frac{\Tr[M_x^j (\qmm + \barDelta_{\ptb})] - \Tr[M_x^j (\qmm + \barDelta_{\ptb^{\oplus i}})]}{\Tr[M_x^j (\qmm + \barDelta_{\ptb^{\oplus i}})]} \\
    &= \frac{\Tr[M_x^j (\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})]}{\Tr[M_x^j (\qmm + \barDelta_{\ptb^{\oplus i}})]}.
\end{align*}
Therefore, we plug $\delta_j(X)$ into the expectation and noting that $\p^{\out_{j}|\out^{j-1}}_{z^{\oplus i}}(x) = \Tr[M_x^j (\qmm + \barDelta_{\ptb^{\oplus i}})]$,
\begin{align*}
    \expectDistrOf{X \sim \p^{\out_{j}|\out^{j-1}}_{z^{\oplus i}}}{\delta_j(X)^2} &= \sum_{x \in X} \frac{\Tr[M_x^j (\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})]^2}{\Tr[M_x^j (\qmm + \barDelta_{\ptb^{\oplus i}})]} \\
    &= \sum_{x \in X} \frac{\Tr[(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}}) M_x^j] \Tr[(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}}) M_x]^{\dagger}}{\Tr[M_x^j (\qmm + \barDelta_{\ptb^{\oplus i}})]} \\
    &= \sum_{x \in X} \frac{\Tr[(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}}) M_x^j] \Tr[(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}}) M_x^j]^{\dagger}}{\Tr[M_x^j (\qmm + \barDelta_{\ptb^{\oplus i}})]} \\
    &= \sum_{\out \in \mathcal{X}} \frac{\vvdotprod{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})}{M_x^j}\vvdotprod{M_x^j}{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})}}{\Tr[M_x^j(\qmm + \barDelta_{\ptb^{\oplus i}})]}.
\end{align*}
Note that $\qmm + \barDelta_{\ptb^{\oplus i}} \succcurlyeq \frac{1}{2} \qmm \implies \Tr[M_x^j(\qmm + \barDelta_{\ptb^{\oplus i}})] \geq \Tr[M_x^j(\frac{1}{2} \qmm)] = \frac{1}{2 \dims}\Tr[M_x^j]$. This statement comes from the fact that $\opnorm{\barDelta_{\ptb^{\oplus i}}} \leq \frac{1}{2 \dims}$~\eqref{equ:delta_z},
\begin{align*}
   \expectDistrOf{X \sim \p^{\out_{j}|\out^{j-1}}_{z^{\oplus i}}}{\delta_j(X)^2} &\le  2 \dims \sum_{\out \in \mathcal{X}} \frac{\vvdotprod{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})}{M_x^j}\vvdotprod{M_x^j}{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})}}{\Tr[M_x^j]} \\
   &=  2 \dims \vadj{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})} \sum_{\out \in \mathcal{X}} \frac{\vvec{M_x^j}\vadj{M_x^j}}{\Tr[M_x^j]} \vvec{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})}.
\end{align*}
We can then apply this bound to the per-round symmmetric KL-divergence,
\begin{align*}
   \kldivsym{\p^{\out_{j}|\out^{j-1}}_{+i}}{\p^{\out_{j}|\out^{j-1}}_{-i}}&\le \frac{1}{2^{\ell-1}} \sum_{z \in \{+1,-1\}^l} \; 2 \dims \vadj{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})} \Sigma_{\out \in \mathcal{X}} \frac{\vvec{M_x^j}\vadj{M_x^j}}{\Tr[M_x^j]} \vvec{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})} \\
   &= 4 \dims \expectDistrOf{z\sim\{-1, 1\}^{\ell}}{\vadj{(\barDelta_{z} - \barDelta_{z ^{\oplus i}})} \Choi_{\POVM_j} \vvec{(\barDelta_{\ptb} - \barDelta_{z ^{\oplus i}})}},
\end{align*}
where $z$ is drawn uniformly from $\{-1,1\}^{\ell}$.
Another key to this bound is that we have a concentration on the operator norm of the perturbation matrix such that the operator norm lies in the boundary (within some constant)  with exponentially decreasing probability, see \cref{thm:rand-mat-opnorm-concentration}. 
Intuitively, this means that it is rare that all of the $z_i$ components are selected in a way where eigenvectors of the $z_i V_i$ components are aligned, thus resulting in an equal contribution to the total perturbation from each $z_i V_i$ component. 
As a result, this concentration perspective allows us to see that flipping a single $z_i V_i$ entry will dictate a perturbation outcome with high probability.
For convenience, we define the concentration set for the perturbation parameters,
\begin{align}
    \mathcal{G} := \{z \in \{1,1\}^{\ell} \; | \; \opnorm{W_z} \leq \kappa_\alpha \sqrt{d}\} \label{eq:concentrated-set},   
\end{align}
where $\alpha$ is a positive constant and $\kappa_\alpha$ is a positive constant non-decreasing in $\alpha$. By \cref{thm:rand-mat-opnorm-concentration},
\[
\probaOf{z \in \mathcal{G}} \geq 1 - 2\exp\{- \alpha d\}.
\]
For more details on the constants involved, see Lemma 21 of \cite{liu2024role}. 
We then condition between the possible cases of perturbations with law of iterative expectation,

\begin{align}
    \kldivsym{\p^{\out_{j}|\out^{j-1}}_{+i}}{\p^{\out_{j}|\out^{j-1}}_{-i}} &\le 4 \dims \expect{\expectDistrOf{z}{\vadj{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})} \Choi_{\POVM_j} \vvec{(\barDelta_{\ptb} - \barDelta_{\ptb ^{\oplus i}})} \;| \indic{z \in \mathcal{G}}}}\label{pf:KL-div-concentration}.
\end{align}
  Now, it suffices to bound the peturbation for when $z \in G$ and $z \notin G$. When $z \in G$, the following bound holds for $\eps \leq \frac{1}{4(\kappa_\alpha+1)}$,
\begin{align}
\opnorm{\Delta_z} &= \frac{c\eps}{\sqrt{\dims \ell}} \opnorm{W_z} \leq  \frac{\kappa_\alpha c\eps}{\sqrt{\ell}} \leq \frac{2 \kappa_\alpha c\eps}{\dims} \leq \frac{1}{2 \dims} .\label{eq:op-norm-bound}
\end{align}
In addition, the following holds when the i-th bit is flipped,
\begin{align*}
        \opnorm{W_{z ^{\oplus i}}} &= \opnorm{W_z - 2 z_i V_i} \leq \opnorm{W_z} + \opnorm{-2 z_i V_i} \\
    &\le \kappa_\alpha \sqrt{\dims} + 2 \leq (\kappa_\alpha + 1) \sqrt{d} \\
    \implies \opnorm{\Delta_{z^{\oplus i}}} &= \frac{c\eps}{\sqrt{\dims \ell}} \opnorm{W_{z^{\oplus i}}} \leq  \frac{(\kappa_\alpha + 1) c\eps}{\sqrt{\ell}} \leq \frac{2(\kappa_\alpha + 1) c\eps}{\dims} \leq \frac{1}{2\dims}.
\end{align*}
The second inequality follows because $ \opnorm{V_i}^2 = \|V_i\|_{S_\infty}^2 \leq\|V_i\|_{S_2}^2 = \hdotprod{V_i}{V_i} = 1$. For $z \in \mathcal{G}$, we have that $\opnorm{\Delta_{z}}, \opnorm{\Delta_{z^{\oplus i}}} \leq \frac{1}{2 \dims}$. This results in $\barDelta_{z ^{\oplus i}} = \Delta_{z ^{\oplus i}}, \;\barDelta_{z} = \Delta_{z}$, by definition of the normalization factor in \cref{equ:delta_z}. Thus,
\begin{align*}
 \vadj{(\barDelta_{z} - \barDelta_{z ^{\oplus i}})} \Choi_{\POVM_j} \vvec{(\barDelta_{z} - \barDelta_{z ^{\oplus i}})} &=  \vadj{(\Delta_{z} - \Delta_{z ^{\oplus i}})}
 \Choi_{\POVM_j} \vvec{(\Delta_{z} - \Delta_{z ^{\oplus i}})} \\
 &=\vadj{\frac{c\eps}{\sqrt{\dims \ell}} 2 z_i V_i}
 \Choi_{\POVM_j}
 \vvec{\frac{c\eps}{\sqrt{\dims \ell}} 2 z_i V_i} = \frac{4 c^2 \eps^2 z_{i}^2}{\dims \ell}  = \frac{4 c^2 \eps^2}{\dims \ell} \vadj{V_i} \Choi_{\POVM_j} \vvec{V_i}.
\end{align*}
We will later see that this will result in the trace decomposition of $\Choi_{\POVM_j}$ under the vectorized version of the orthornormal Hilbert basis $\hbasis$. Now, we will apply a more crude bound for the low-concentration set $z \notin \mathcal{G}$. We start by bounding the Hilbert-Schmidt norm of the perturbation matrix for every $z \in \{-1,1\}^\ell$,
\begin{align*}
\hsnorm{\barDelta_{z}} &= \sqrt{\vvdotprod{\barDelta_{z}}{\barDelta_{z}}} \\
&= \sqrt{\frac{c^2\eps^2}{\dims \ell}\vvdotprod{\sum_{i=1}^\ell \min\left\{1, \frac{1}{2\dims \opnorm{\Delta_{\ptb}}}\right\} z_i V_i}{\sum_{i=1}^\ell \min\left\{1, \frac{1}{2\dims \opnorm{\Delta_{\ptb}}}\right\} z_i V_i}}\\
&= \sqrt{\frac{c^2 \eps^2}{\dims \ell} \sum_{i \neq j} \min\left\{1, \frac{1}{2\dims \opnorm{\Delta_{\ptb}}}\right\}^2 z_i z_j \vvdotprod{V_i}{V_j} + \sum_{i}^\ell \min\left\{1, \frac{1}{2\dims \opnorm{\Delta_{\ptb}}}\right\}^2 z_i^2 \vvdotprod{V_i}{V_i}} \\
&= \sqrt{\frac{c\eps^2}{\dims \ell} \sum_{i}^\ell \min\left\{1, \frac{1}{2\dims \opnorm{\Delta_{\ptb}}}\right\}^2} \leq  \sqrt{\frac{c\eps^2}{\dims \ell} \sum_{i}^\ell 1} = \frac{c \eps}{\sqrt{\dims}}.
\end{align*} 
Where last line holds from the orthonormality of the perturbation basis. Now, we can use triangle inequality of the Hilbert-Schmidt norm to get the bound on the Hilbert-Schmidt norm of the difference between the perturbation matrices.
\begin{align*}
    \vadj{(\barDelta_{z} - \barDelta_{z ^{\oplus i}})} \Choi_{\POVM_j} \vvec{(\barDelta_{z} - \barDelta_{z ^{\oplus i}})} 
    &\le \opnorm{\Choi_{\POVM_j}}\hsnorm{\barDelta_{z} - \barDelta_{z ^{\oplus i}}}^2  \\
    &\leq 2(\hsnorm{\barDelta_{z}}^2 + \hsnorm{\barDelta_{z ^{\oplus i}}}^2) \\
    &\leq \frac{4c^2 \eps^2}{\dims}.
\end{align*}
The first step is due to the definition of operator norm. The second step is because $\opnorm{\Choi_{\POVM_j}} \le 1$, triangle inequality, and $(a+b)^2\le 2(a^2+b^2)$. We can further bound the symmetric KL-divergence in \cref{pf:KL-div-concentration},
 \begin{align*}
     \kldivsym{\p^{\out_{j}|\out^{j-1}}_{+i}}{\p^{\out_{j}|\out^{j-1}}_{-i}} &\le 4 \dims \left[\probaOf{z \in \mathcal{G}} \frac{4 c^2 \eps^2}{\dims\ell} \vadj{V_i} \Choi_{\POVM_j} \vvec{V_i} + (1 - \probaOf{z \in \mathcal{G}})   \frac{4 c^2 \eps^2}{\dims}\right] \\
     &= \probaOf{z \in \mathcal{G}} \frac{16 c^2 \eps^2}{\ell} \vadj{V_i} \Choi_{\POVM_j} \vvec{V_i} + (1-\probaOf{z \in \mathcal{G}}) 16 c^2 \eps^2.
 \end{align*} 

Thus combining with \eqref{pf:MI-bound} \eqref{eq:per-round-divergence},
\begin{align*}
    \frac{1}{\ell}\sum_{i=1}^{\ell}\mutualinfo{\ptb_i}{\out^t} &\le \frac{1}{2\ell}\sum_{i=1}^{\ell}\sum_{j=1}^{t} \expectDistrOf{\q^{\out^\ns}}{\kldivsym{\p^{\out_{j}|\out^{j-1}}_{+i}}{\p^{\out_{j}|\out^{j-1}}_{-i}}} \\
    &\leq \probaOf{z \in \mathcal{G}} \frac{8 c^2 \eps^2}{\ell^2} \sum_{j=1}^{t} \sum_{i=1}^{\ell} \expectDistrOf{\q^{\out^\ns}}{ \vadj{V_i} \Choi_{\POVM_j} \vvec{V_i}}
    + (1 - \probaOf{z \in \mathcal{G}}) \sum_{j=1}^{t} \sum_{i=1}^{\ell}  \frac{8c^2\eps^2}{\ell} \\
    &\le  \frac{8 tc^2 \eps^2}{\ell^2}  \expectDistrOf{\q^{\out^\ns}}{\frac{1}{t}\sum_{j=1}^{t}\sum_{i=1}^\ell \vadj{V_i} {\Choi}_{\POVM_j} \vvec{V_i}} +16\exp\{-\alpha\dims\}tc^2\eps^2\\
    &\le \frac{8 tc^2 \eps^2}{\ell^2}  \sup_{\POVM\in\povmset}\sum_{i=1}^\ell \vadj{V_i} {\Choi}_{\POVM} \vvec{V_i} +16\exp\{-\alpha\dims\}tc^2\eps^2,
\end{align*}
The second term in the final step is due to \cref{thm:rand-mat-opnorm-concentration}. This proves \eqref{equ:avg-MI-partial} in \cref{thm:avg-MI-upper}.

We continue to derive the remaining expression \eqref{equ:avg-MI-tracenorm}. We use the fact that for any matrix $A\in\C^{\dims\times\dims}$ and an orthonormal basis $\qbit{u_1}, \ldots, \qbit{u_\dims}$,
\[
\Tr[A]=\sum_{i=1}^{\dims}\matdotprod{u_i}{A}{u_i}.
\]
Combining with the fact that ${\Choi}_{\POVM}$ is p.s.d., we have
\[
\sum_{i=1}^{\ell} \vadj{V_i} {\Choi}_{\POVM} \vvec{V_i}\le \sum_{i=1}^{\dims^2}\vadj{V_i}\Choi_{\POVM} \vvec{V_i}=\Tr[\Choi_{\POVM}]=\tracenorm{\Choi_{\POVM}}.
\]
Therefore, continuing from \eqref{equ:avg-MI-partial},

\begin{align*}
     \frac{1}{\ell}\sum_{i=1}^{\ell}\mutualinfo{\ptb_i}{\out^t} 
    &\le \frac{8 t c^2 \eps^2}{\ell^2} \tracenorm{\povmset} + 16\exp\{-\alpha\dims\}tc^2\eps^2\\
    & \le \frac{16 t c^2 \eps^2}{\ell^2} \tracenorm{\povmset}.
\end{align*}
The first step is from the definition of $\tracenorm{\povmset}$ in \eqref{equ:max-povm-norm}. The second step holds as $\tracenorm{\povmset}\ge 1$ and $\exp\{-\alpha d\} \leq \frac{1}{d^4}$ when $d \geq 1024$. 
\end{proof}

