%\chapter{Compilers}\label{chap:compiler}
\begin{acronym}
  \acro{adl}[ADL]{Architecture Description Language}

  \acro{ag}[AG]{Address Generator}

  \acro{alu}[ALU]{Arithmetic Logic Unit}

  \acro{asic}[ASIC]{Application-Specific Integrated Circuit}
  \acroplural{asic}[ASICs]{Application-Specific Integrated Circuits}

  \acro{cgra}[CGRA]{Coarse-Grained Reconfigurable Array}
  \acroplural{cgra}[CGRAs]{Coarse-Grained Reconfigurable Arrays}

  \acro{cli}[CLI]{Command Line Interface}
  
  \acro{dfg}[DFG]{Data-Flow Graph}
  
  \acro{fpga}[FPGA]{Field-Programmable Gate Array}
  \acroplural{fpga}[FPGAs]{Field-Programmable Gate Arrays}
  
  \acro{fu}[FU]{Functional Unit}
  \acroplural{fu}[FUs]{Functional Units}

  \acro{gc}[GC]{Global Controller}

  \acro{gui}[GUI]{Graphical User Interface}

  \acro{ml}[ML]{Machine Learning}

  \acro{mrrg}[MRRG]{Modulo Routing Resource Graph}

  \acro{oip}[OIP]{orthogonal instruction processing}

  \acro{pe}[PE]{Processing Element}
  \acroplural{pe}[PEs]{Processing Elements}

  \acro{pla}[PLA]{Piecewise Linear Algorithm}
  \acroplural{pla}[PLAs]{Piecewise Linear Algorithms}

  \acro{resmii}[ResMII]{Resource Minimum Initiation Interval}
  \acro{recmii}[RecMII]{Recurrence Minimum Initiation Interval}

  \acro{rf}[RF]{Register File}
  
  \acro{tcpa}[TCPA]{Tightly Coupled Processor Array}
  \acroplural{tcpa}[TCPAs]{Tightly Coupled Processor Arrays}

  \acro{uda}[UDA]{Uniform Dependence Algorithm}
  \acroplural{uda}[UDAs]{Uniform Dependence Algorithms}
\end{acronym}

This chapter introduces the considered compiler frameworks for \acfp{cgra} and \acfp{tcpa}.
For \ac{cgra} architectures, CGRA-Flow (Section \ref{section:cgra-flow}) and Morpher (Section \ref{section:morpher}) are chosen as target compilers.
For \ac{tcpa} architectures, the TCPA-Compiler (Section \ref{section:tcpa-compiler}) is used as the target compiler.
Each section gives an overview of the offered functionalities, the required user input, and the returned output.
In the last section, the three compilers will be compared in terms of features and scalability.



\section{CGRA-Flow} \label{section:cgra-flow}
    % src [2]
    CGRA-Flow \cite{4_OpenCGRA} is a framework for compilation, exploration, synthesis, and development of \ac{cgra} architectures \cite{cgra-flow}.
    It is open-source and available on GitHub\footnotemark. 
    \footnotetext{\url{https://github.com/tancheng/CGRA-Flow} (Accessed: 7 May 2024).}
    The GitHub repository cites a paper by \Citeauthor{4_OpenCGRA} \cite{4_OpenCGRA} that describes a framework called OpenCGRA and links to a different repository on GitHub.
    Evidently, the OpenCGRA repository has been integrated into the CGRA-Flow framework as VectorCGRA.
    Given that CGRA-Flow extends the OpenCGRA framework and has more recent updates, this work will use CGRA-Flow \cite{cgra-flow}.

    \begin{figure}[htb]
      \centering
      \includegraphics[width=.5\textwidth]{images/ch5_CGRA-Flow_framework.png}
      \caption{%
        Overview of the CGRA-Flow framework.
        Compilation of loops is performed with LLVM \cite{LLVM} to map the innermost loop onto a user-specific \ac{cgra} design.
        Synthesizable Verilog of the given \ac{cgra} is generated via PyMTL \cite{PyMTL}.
        This is used to test each building block of the \ac{cgra} and its integration and to simulate the target \ac{cgra} \cite{4_OpenCGRA}.
      }
      \label{fig:cgra-flow_framework}
    \end{figure}

    % framework 
    Figure \ref{fig:cgra-flow_framework} shows an overview of the CGRA-Flow framework \cite{4_OpenCGRA}.
    Additionally, CGRA-Flow has a \acf{gui} (Figure \ref{fig:cgra-flow_gui}) for visualizing input, output, and intermediate results.
    % input
    As input, users select the path to a loop program written in C or C++.
    CGRA-Flow supports the mapping of up to two innermost loop nests with control flow in the loop body or up to three innermost loop nests without control flow in the loop body onto the user-specified \ac{cgra}. 
    On the top left of the \ac{gui}, users can configure the \ac{cgra} architecture by selecting the number of \acp{pe} (Rows \& Columns), the number of operations mapped to one \ac{pe} (Config Memory), the size of the memory buffer (data SPM), the operations that can be executed by the \ac{pe}, the connections to neighboring \acp{pe}, and the disablement of entire \acp{pe} so that no operation is mapped to them can be configured.
    This user-specified architecture is visualized on the left and automatically transformed into a JSON file, which is used for the mapping.
    Note that each \ac{pe} can only perform single-cycle operations.
    However, it is possible to program more complex operations, \eg the Multiply-Accumulate (MAC) operation, which multiplies two numbers and adds that product to an accumulator, but they have to be completed within one cycle. 
    % mapping
    The compilation of the user-given loop and generation of the corresponding \acf{dfg} are performed using LLVM's\cite{LLVM} intermediate representation (IR) to extract the operations and the dependency between operations.
    Some simple arithmetic optimizations are automatically applied to the IR by LLVM.
    The generated \ac{dfg} is visualized on the bottom left of the \ac{gui}.
    %
    Before starting the mapping phase, the user can select between two mapping algorithms, called \textit{exhaustive} and \textit{heuristic}.
    While the exhaustive algorithm checks all possible mappings for one given initiation interval, the heuristic approach starts with a minimal initiation interval and iteratively increments it until the mapping with the lowest cost (based on a heuristic function) for the current initiation interval is valid.
    %
    % mapping output
    After the mapping is completed, it is returned as a JSON file.
    Additionally, the mapping is visualized in the \ac{gui} on the bottom right.
    The \ac{gui} also shows the initiation interval of the mapping (Map II), the time it took to find the mapping in seconds (Time), and speedup compared to a single-issue in-order CPU (Speedup).
    % generate synth. Verilog and power & area estimation
    After mapping, the user can generate Verilog via PyMTL and verify it.
    The verification uses unit tests for each building block, integration tests for the target \acp{cgra}, and property-based random tests for further testing of the \ac{cgra} components \cite{4_OpenCGRA}.
    The Verilog description is then used for the simulation to estimate the area and power of \acp{pe} and on-chip memory.
    Currently, the power of the \acp{pe} cannot be estimated.

    \begin{figure}[htb]
      \centering
      \includegraphics[width=.5\textwidth]{images/ch5_CGRA-Flow_GUI.png}
      \caption{CGRA-Flow's \acf{gui} \cite{cgra-flow}.}
      \label{fig:cgra-flow_gui}
    \end{figure}

\section{Morpher}\label{section:morpher}
    Morpher is an integrated compilation and simulation framework \cite{morpher}.
    There are two versions of this framework, Morpher\footnote{\url{https://github.com/ecolab-nus/morpher} (Accessed: 10 May 2024).} and 
    Morpher-v2\footnote{\url{https://github.com/ecolab-nus/morpher-v2} (Accessed: 10 May 2024).}, which are both on GitHub.
    Compared to Morpher, Morpher-v2 offers an automated synthesis of Verilog.
    However, it is recommended to use Morpher if no synthesis is required.
    Therefore, this section will only focus on Morpher.

    \begin{figure}[htb]
      \centering
      \includegraphics[width=.5\textwidth]{images/ch5_morpher-v1_framework.png}
      \caption{%
        Overview of the Morpher framework.
        User-given input is colored yellow, functional components are colored blue, and output is colored gray \cite{6_MorpherWOSET}.
      }
      \label{fig:morpher-v1_framework}
    \end{figure}

    % framework
    Figure \ref{fig:morpher-v1_framework} shows an overview of the Morpher framework.
    Yellow nodes are user input, blue nodes are functional components, and gray nodes are output returned by the functional components.
    Each component of the framework is numbered for easy reference and is described in the following.
    % Morpher only offers command line interface.
    % input (1) & (4)
    As input, the user provides a description of the target \ac{cgra} architecture and a loop program written in C or C++ with an annotated loop that should be mapped onto the target architecture (\circled{4} and \circled{1} in Figure \ref{fig:morpher-v1_framework}).
    The \ac{cgra} architecture is described with Morpher's \ac{adl}, allowing the user to describe many different \acp{cgra} \cite{6_MorpherWOSET}.
    Even though the \ac{adl} would support multiple \acp{fu} per \ac{pe} (to model \ac{tcpa} \acp{pe}), the mapping algorithm only assumes one \ac{fu} per \ac{pe}. 
    Morpher's \ac{adl} provides syntactic sugar to easily connect the modules and \acp{pe} according to a given pattern of connections, eliminating the need to specify connections for each individual module/\ac{pe} in the \ac{cgra}.
    %
    The \ac{dfg} generator (\circled{2} in Figure \ref{fig:morpher-v1_framework}) begins by extracting the innermost loop of the program and generating the corresponding \ac{dfg} using LLVM \cite{LLVM}.
    It offers three schemes for mapping loop control flow onto a \ac{cgra}, influencing \ac{dfg} generation: partial predication, full predication, and dual-issue \cite{6_MorpherWOSET}, described in detail by in \cite{8_Branch_aware_loop_mapping}.
%
    Partial predication maps if-part and else-part operations to different PEs, adding a \textit{select} node if both parts update the same variable.
    Full predication schedules both parts using the same variable to the same PE, with one operation executed per cycle, avoiding the need for a select node.
    Dual-issue merges both operations into one \ac{dfg} node, scheduling them simultaneously but only executing one at run-time.
    This work uses only partial predication.
    The resulting \ac{dfg} and data layout for input/output variables on SPM is then used by the \ac{cgra} mapper (\circled{5}) to find a valid mapping using three algorithms: PathFinder, Simulated Annealing, and LISA \cite{6_MorpherWOSET,morpher}.
    PathFinder explores multiple mappings by backtracking.
    Simulated Annealing, a probabilistic optimization technique \cite{32_SA}, generates an initial mapping with conflicts, then reduces them by reassigning nodes based on a cost function.
    LISA uses a trained Graph Neural Network to assign and schedule \ac{dfg} nodes.
    The mapper returns a CSV file detailing resource allocation per cycle, and provides additional information like the cycle offset of the last operation and mapping time.
    %
    The mapping can then be verified by simulating the execution using a simulator (\circled{6}) that models a CGRA with FUs, registers, multiplexers, and memory banks \cite{6_MorpherWOSET}, supporting variations of HyCUBE and, in Morpher-v2, additional architectures.
    Here, the test data is automatically created by Morpherâ€™s data generator (\circled{3}) \cite{3_MorpherCODAI}.
    After simulation, memory values are compared to the test data, and Morpher reports the number of matches and mismatches.

\section{TCPA-Compiler}\label{section:tcpa-compiler}
    \begin{figure}[htb]
      \centering
      \resizebox{!}{.25\textwidth}{\input{images/convert_png/ch5_tcpa-compiler_framework.tikz}}
      \caption{Overview of the TCPA-Compiler toolchain with user-given input colored yellow and integrated tools as blue rectangular nodes.}
      \label{fig:tcpa-compiler_framework}
    \end{figure}

% framework overview
The TCPA-Compiler is a toolchain performing verification, compilation, and simulation of a given loop program.

As briefly mentioned in Section \ref{section:tcpa_mapping}, the mapping approach of \acp{tcpa} allows the number of \acp{pe} and iterations to be unknown at compile-time by generating a parameterized schedule.
Only at run-time, when the number of \acp{pe} and iterations are known, are the parameters replaced by concrete values.
The TCPA-Compiler implements this by splitting the mapping into two phases: \emph{symbolic mapping} for variable loop bounds and number of PEs at compile-time and \emph{instantiation} for specific loop bounds and number of PEs at run-time \cite{15_sym_comp}.
Figure \ref{fig:tcpa-compiler_framework} gives an overview of the TCPA-Compiler toolchain.
A yellow node indicates user input, and a blue node is an integrated tool.
The workflow of the TCPA-Compiler can be split into four phases: verification by \textit{unpaula}, symbolic mapping by \textit{tcpac}, instantiation by \textit{tcpaconf}, and simulation by \textit{tcpasim}.

\begin{lstlisting}[style=my_PAULA_style, float, escapechar=|, label=listing:paula, captionpos=b,
  caption=PAULA Code multiplying a matrix $A \in \mathbb{R}^{I \times J}$ with a vector $x \in \mathbb{R}^J$ and storing the result in vector $b \in \mathbb{R}^I$.]
program matrix_vector_multiplication {
  // In-/Output variables
  variable A 2 in data_type; |\label{line:declare_input1}|  
  variable x 1 in data_type;  
  variable b 1 out data_type; 
  // Internal variables
  variable xtmp 2 data_type;
  variable prod 2 data_type;
  variable psum 2 data_type; |\label{line:declare_input2}|  
  // Parameters with values assigned at instantiation
  parameter I; |\label{line:param1}|  
  parameter J; |\label{line:param2}| 
  // Loop with iteration space in brackets
  par (i >= 0 and i < I and j >= 0 and j < J) { |\label{line:par}|
    // Read input from
    xtmp[i,j] = x[j] if (i==0); // memory buffer
    xtmp[i,j] = xtmp[i-1,j] if (i>0);  // adjacent PE

    prod[i,j] = A[i,j] * xtmp[i,j]  
    psum[i,j] = prod[i,j] if (j==0);
    psum[i,j] = psum[i,j-1]+prod[i,j] if (j>0);

    b[i] = psum[i,j] if (j==J); // Output result
  }
}
\end{lstlisting}

% verification
For verification by \textit{unpaula}, the user provides a loop program written in the programming language PAULA and a data generator written in Python.
PAULA is a functional programming language specifically developed to map loop programs onto \acp{tcpa} \cite{14_PAULA}.
The PAULA language is modeled after \ac{uda} (Section \ref{section:tcpa_mapping}).
Hence, it has a single assignment form where each variable can be assigned exactly once per condition space, and the variables have to be indexed uniformly.

Listing \ref{listing:paula} shows an example of a PAULA program for a matrix-vector multiplication that multiplies a matrix $A \in \mathbb{R}^{I \times J}$ with a vector $x \in \mathbb{R}^J$ (similar to Example \ref{ex:tcpa_mapping} in Section \ref{section:tcpa_mapping}), which will be used as a running example.
This program shows a few key features of the PAULA language.


Firstly, all variables are declared at the beginning (\Crefrange{line:declare_input1}{line:declare_input2}).

Secondly, the \textit{parameter} keyword declares values that are only assigned later, during instantiation (Lines \ref{line:param1} and \ref{line:param2}).
In this example, they are used to parameterize the loop bounds.
Lastly, every PAULA program contains a loop with loop bounds declared by the keyword \textit{par} (Line \ref{line:par}).
Notice that in the \textit{par} statement, all iteration variables are declared at once without the need to define the order of iterations.

Another feature of PAULA is that the statements within a loop may not be executed in the sequential order in which they are declared.
Thus, the TCPA-Compiler can rearrange the order as much as the data dependencies allow.
The TCPA-Compiler offers a verification step so that users can check their PAULA implementation against reference values generated by a Python data generator.
%Then, the PAULA program is checked against values generated by the Python data generator

\begin{lstlisting}[language=JSON, float, escapechar=@, caption=Excerpt of build.json that specifies scanning order and which iteration variable is mapped to \ac{tcpa} row and column., captionpos=b, label=listing:build_json]
{ @\textcolor{arrowcolor}{\dots}@ 
    "ScanningOrder": ["+j", "+i"],
    "MapToColumns": "j",
    "MapToRows": "i",
    "InitiationInterval": 2,
  @\textcolor{arrowcolor}{\dots}@ }
\end{lstlisting}

% symbolic mapping phase 
The symbolic mapping phase is performed by the \textit{tcpac} compiler.
The compiler requires a PAULA program, a \ac{tcpa} architecture, and a \textit{build.json} containing further mapping information.
The target architecture is described in an XML file.
For example, this file describes the \acp{fu} within a \ac{pe}, an \ac{fu}'s instruction memory size, the registers of a \ac{pe}, the number of input and output ports of a \ac{pe} and how they are connected via an interconnect, the size of the I/O buffers, and so on.

The architecture description also contains the size of a \ac{tcpa}, meaning the number of rows and columns of \acp{pe}.

As it is also possible to allocate less \ac{pe} for execution, the number of \acp{pe} is assumed to be unknown in the symbolic mapping phase.
In the \textit{build.json}, further details for mapping, such as the scanning order of the iteration variables, the initiation interval, and which iteration variable should be mapped to the rows and columns of a \ac{tcpa}, are specified by the user.
Listing \ref{listing:build_json} is an extract of an example \textit{build.json}.
Furthermore, the user can also constrain the mapping by, \eg specifying that an operation should be executed by a certain \ac{fu} or that a variable has to be stored in or read from a certain register.

After successful compilation and mapping performed by \textit{tcpac}, it returns a symbolic configuration as an XML file.
This symbolic configuration contains the information about tiling parameters, connections between \acp{pe} and their ports, and a symbolic schedule.
More specifically, it describes which operation is performed by which \ac{fu} at offset $\tau_{op}$.
Additionally, the condition space when the operation is executed is specified because multiple operations can be assigned at the same offset.
At run-time only one operation is actually executed.
% for this thesis, we will focus on the symbolic inter-tile and intra-tile schedule found in the log file because it is used in chapter for determining the latency.
For the PAULA code (listing) and json information, the symbolic schedule is as follows:
To describe the execution order of iterations, the TCPA-Compiler returns a parameterized schedule vector  $\lambda = \begin{pmatrix}\lambda_J & \lambda_K\end{pmatrix}$, 
which is written into a log file.
Listing \ref{listing:log_schedule} is an excerpt of a possible log file for the loop program in Listing \ref{listing:paula} and the \textit{build.json} from Listing \ref{listing:build_json}.
The inter-tile and intra-tile schedule vectors are described with tiling parameters $p_1$ and $p_2$.
In this example, each component of the vector is multiplied by two, which is the initiation interval given in \textit{build.json}.

% To each component of the inter-tile vector four is added because it is the delay between two tiles, so called inter-tile delta.

\begin{lstlisting}[style=log, captionpos=b, escapechar=@,  caption=Excerpt of a log file describing schedule vector $\lambda$ consisting of intra-tile schedule vector $\lambda_J$ and inter-tile schedule vector $\lambda_K$., label=listing:log_schedule ]
Using symbolic tile sizes:
  [0] p_1
  [1] p_2
Using symbolic intra-tile schedule:
  [0] 2 * p_2
  [1] 2
 @\textcolor{arrowcolor}{\dots}@ 
Finished determining symbolic inter-tile schedule:
  [0] 2 (p_2 * p_1 - p_2) + 4
  [1] 2 (p_2 - 1) + 4
\end{lstlisting}

%the configuration describes in which subset of the iteration space the input and output variables are accessed

% instantiation output 
Next, an instantiation of the symbolic configuration is performed by the \textit{tcpaconf} tool.
In this phase, concrete values for the loop bounds and number of allocated \acp{pe} are given by a \textit{build.json}.
An example of a \textit{build.json} is shown in Listing \ref{listing:build_json_instantiation}.

The symbolic tiling parameters in a schedule vector are replaced by the concrete values from the \textit{build.json}.
For example, given the instantiation values from Listing \ref{listing:build_json_instantiation} and the schedule vector from Listing \ref{listing:log_schedule}, the tiling parameters are $p_1 = \lceil I/{Columns} \rceil = 2$ and $p_2=\lceil J/{Rows} \rceil = 2$, which are inserted into the inter- and intra-tile schedule.

\begin{lstlisting}[language=JSON, float, escapechar=@, caption={Excerpt of a build.json specifying one instantiation for loop bounds $I = 4$ and $J = 6$, and $2 \times 2$ allocated \acp{pe}.}, captionpos=b,label=listing:build_json_instantiation]
{ @\textcolor{arrowcolor}{\dots}@ 
  "Instantiations": [
    { 
      "Rows": 2,
      "Columns": 3,
      "Parameters": {"I":4, "J":6}
    } 
  ], @\textcolor{arrowcolor}{\dots}@ }
\end{lstlisting}

Hence, \textit{tcpaconf} transforms the symbolic configuration into a concrete one, which is returned as an XML file.
The concrete configuration now contains specific values for the tiled loop, a schedule considering delays for data transfers to and from the I/O buffers, address functions for \acfp{ag}, and control signals for a \acf{gc}.
During the instantiation phase, assembler code for each \ac{fu} is generated that determines which operation is performed and which source and target registers are used.

The \ac{fu}'s program also contains static branching.
The branching outcome is determined by control signals that are generated by a \ac{gc} and propagated by \acp{pe}.

% simulation & synthesis
Lastly, software simulation of the configuration can be performed with the \textit{tcpasim} tool.
After a cycle-accurate simulation, \textit{tcpasim} returns the read and write transfers and statistics detailing, for example, the total cycles needed for the execution, the number of stalled cycles, and throughput.
Furthermore, the TCPA-Compiler offers additional simulation by generating binary configuration from an XML configuration and performing a Register Transfer Level (RTL) simulation of this binary.
However, these further steps are not covered in this thesis.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%















\section{Comparison and Evaluation}\label{section:compiler-compare}

\begin{table}[htb]
  \centering
  \resizebox{!}{0.25\textwidth}{
      \begin{tabular}{l|C{.15\linewidth}|C{.15\linewidth}|C{.15\linewidth}}
        Feature & \makecell{CGRA-\\Flow} & Morpher & \makecell{TCPA-\\Compiler} \\
        \hline
        \rowcolor{lightgray}\textbf{Tool interface}& GUI & CLI & CLI \\
        \hspace{.5em} $\hookrightarrow$ Intuitive? & \cmark & \xmark & \xmark \\
        
        \rowcolor{lightgray}\textbf{Architecture description} & GUI & Morpher ADL & XML \\
        \hspace{.5em} $\hookrightarrow$ Intuitive? & \cmark & \xmark & \xmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Specify \#PE? & \cmark & \cmark & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Specify \#FU per PE? & \xmark & (\cmark) & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Configurable Interconnect? & \cmark & \cmark & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Multi-cycle operations? & \xmark & \cmark & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Multi-hop connections? & \xmark & \cmark & \xmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Configurable memory? & \cmark & \cmark & \cmark \\

        \rowcolor{lightgray}\textbf{Loop program} & C/C++ & C/C++ & PAULA \\
        \hspace{.5em} $\hookrightarrow$ Commonly used language? & \cmark & \cmark & \xmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Auto. test data generation? & \xmark & \cmark & \xmark \\
        
        \rowcolor{lightgray}\textbf{Mapping} & GUI & CSV & XML \\
        \hspace{.5em} $\hookrightarrow$ Different mapping algorithms? & \cmark & \cmark & \xmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Maps nested loops? & (\cmark) & \xmark & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Independent of \#iterations? & (\cmark) & (\cmark) & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Independent of \#PE? & \xmark & \xmark & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Data layout aware? & \xmark & \cmark & \cmark \\

        \rowcolor{lightgray}\textbf{Mapping time} & sec - min & min - h & sec \\
        \hspace{.5em} $\hookrightarrow$ Independent of \#operations? & \xmark & \xmark & \xmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Independent of \#iterations? & \cmark & \cmark & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Independent of \#PE? & \xmark & \xmark & \cmark \\

        \rowcolor{lightgray}\textbf{Simulation} & CGRA & HyCUBE & TCPA \\
        \hspace{.5em} $\hookrightarrow$ Verification of model? & \cmark & \cmark & \cmark \\
        \hline
        \hspace{.5em} $\hookrightarrow$ Simulation statistics? & \cmark & \xmark & \cmark \\
      \end{tabular}
  }
  \caption{Comparing the features of CGRA-Flow, Morpher, and TCPA-Compiler frameworks.}
  \label{table:comp_feature}
\end{table}


%all compilers offer compilation of given loop, mapping onto user-designed architecture and simulation of architecture and mapping.
Table compares different aspects of the compiler frameworks
Table \ref{table:comp_feature} gives an overview of the different features each of the above-described compiler frameworks offer.

% GUI/CLI
The most obvious difference between the three compilers is that CGRA-Flow has a \acf{gui}, visualizing input, output, and intermediate results.
Especially for beginners, CGRA-Flow is intuitive to use and easy to understand.
On the other hand, Morpher and the TCPA-Compiler offer a \acf{cli}, which may require some time to learn how to use.

% architecture description
All compiler frameworks allow the user to configure a target architecture.

CGRA-Flow's architecture can be configured via the \ac{gui}.
This visualization of the target \ac{cgra} makes the configuration straightforward and less error-prone because the corresponding architecture description (JSON) is automatically created.
However, CGRA-Flow requires the user to configure the target architecture every time the \ac{gui} is reopened because it does not offer a feature to import an already generated architecture configuration.

Furthermore, CGRA-Flow's \ac{gui} also limits the architecture design.
Generally, the user can only configure the number of \acp{pe}, the operation a \ac{pe} can perform, the connectivity to other neighboring \acp{pe}, and the size of the on-chip memory.
On the other hand, Morpher and TCPA-Compiler are more flexible.
For example, both allow the user to configure the registers within a \ac{pe} and to define multiple \acp{fu} per \ac{pe} (even though, during mapping, Morpher ignores this).
Moreover, both also support multi-cycle operations.
Morpher additionally supports single-cycle multi-hop connections to describe HyCUBE architectures\footnotemark.

\footnotetext{Due to TCPA-Compiler only supporting \acp{uda}, which have uniform dependencies, \acp{tcpa} do not need single-cycle multi-hop connections because \Citeauthor{20_Witterauf} \cite{20_Witterauf} states that uniform dependencies only result in communication between neighboring \acp{pe}.}

% Programming language
As input, Morpher and CGRA-Flow accept loops written in C or C++, which are commonly used programming languages.
On the other hand, TCPA-Compiler has its own functional programming language, PAULA, which a user has to learn.

The TCPA-Compiler tries to elevate this disadvantage by offering verification of the PAULA implementation via \textit{unpaula}.
However, it requires the user to manually write a data generator for reference values.
Morpher has a  built-in data generator, which generates test data for simulation.
%But PAULA has been especially developed for tcpas and therefore exploits the resources much better.

% Mapping Algo
For the mapping phase, CGRA-Flow and Morpher offer different search algorithms to find a mapping.

CGRA-Flow provides two algorithms.
Firstly, a heuristic algorithm that checks one mapping per initiation interval.
And secondly, an exhaustive algorithm that recursively checks multiple mappings for one target initiation interval.
The exhaustive algorithm often fails due to recursively invoking the mapping function for each node in a \ac{dfg}.

Morpher provides three algorithms.
Firstly, a heuristic algorithm that checks multiple mappings per initiation interval.
Secondly, an algorithm based on simulated annealing that is not deterministic due to choosing nodes to relocate randomly.
Lastly, a learning-induced search approach (LISA) that requires training of a Graph Neural Network (GNN) model.
In papers \cite{3_MorpherCODAI} and \cite{6_MorpherWOSET}, it is shown that Morpher is often able to find a mapping with the theoretical minimal initiation interval.

In contrast, TCPA-Compiler only has one mapping algorithm.


% mapping loop nests
Each compiler framework returns its found mapping in a different format (see Table \ref{table:comp_feature}).
If the input program has a nested loop, then the TCPA-Compiler can map all loop nests.
On the contrary, Morpher only maps the innermost loop, which it automatically extracts.
CGRA-Flow is able to map up to three loop nests if there is no control flow within the loop body or, otherwise, up to two loop nests.
Therefore, in Table \ref{table:comp_feature}, the checkmark for CGRA-Flow mapping nested loops is in brackets.
If there are more than two (or three) loop nests, then CGRA-Flow requires the user to manually extract the innermost loops.

% mapping dependency (#PE & #iterations)
The returned symbolic mapping of the TCPA-Compiler is independent of the number of iterations (\ie loop bounds) and the number of allocated \acp{pe}.
Only during instantiation is the mapping concretized.
Hence, the same symbolic mapping can be used for multiple different combinations of loop bounds and allocated \acp{pe}.
In theory, the returned mappings of Morpher and CGRA-Flow are also independent of the number of iterations.
However, if a user wants to simulate the same loop with different loop bounds, then the user has to rerun the mapping.
Thus, the checkmarks for \textit{mapping independent of \#iter} in Table \ref{table:comp_feature} are in brackets.

% data layout in mapping 
Furthermore, CGRA-Flow's mapping does not consider a \ac{pe}'s register in its mappings.
As a result, CGRA-Flow needs an infinite number of registers to make the mapping work.
Additionally, CGRA-Flow does not consider the data layout within the larger memory buffer, or rather, it always assumes that the memory address starts at zero.
The mappings of both Morpher and TCPA-Compiler consider the registers by specifying in which register values are stored.
The memory data layout is considered by creating more nodes for address calculation in a \ac{dfg} in the case of Morpher or by specifying address functions for the \acp{ag} in the case of TCPA-Compiler.

% Mapping algorithm and scalability
Out of the three compilers, the TCPA-Compiler has the most stable mapping time because it does not scale for architectures with more \acp{pe} due to symbolic mapping using parameterized expressions for tiling.
The TCPA-Compiler completes its mapping within seconds.
Often, CGRA-Flow also finds a mapping within seconds if the heuristic algorithm is used.

Even though its heuristic algorithm scales to the number of \acp{pe} and \ac{dfg} nodes, it is not very noticeable because it only checks one mapping per initiation interval and simplifies the data layout.
CGRA-Flow takes minutes to find a mapping if a generated \ac{dfg} has many nodes (hundreds) or a \ac{cgra} has many \acp{pe} (64 or more).
However, more often than not, CGRA-Flow fails to find a mapping due to only checking one per initiation interval and thus reaching the (hard-coded) maximum initiation interval of 60 rather quickly.
%If the exhaustive algorithm is used then the mapping often fails due to recursion.

%Therefore, it is ignored here.
By far, the longest mapping time is achieved by Morpher.
It takes somewhere from minutes to hours, depending on the node count in a \ac{dfg} and the \ac{pe} count in a \ac{cgra}.
This long mapping time is due to checking many mappings for one initiation interval before increasing it (heuristic algorithm).
The Simulated Annealing algorithm may be faster but may also be slower because it has random node selection.
Furthermore, Morpher also creates binaries during its mapping phase, which takes a few seconds to create, but the most time is spent on finding a mapping.


% Simulation
Lastly, all three tools offer verification and simulation.
Among those three, TCPA-Compiler returns the most statistics for the simulated execution.
For example, it returns the total cycles needed for execution, the number of read and write transfers, throughput, and utilization of a \ac{tcpa}.
CGRA-Flow returns the estimated power and area of \acp{pe} and on-chip memory.
Morpher, which is only able to simulate HyCUBE architectures, only returns the number of matches and mismatches compared to reference values generated by the integrated data generator.

% Summary
All in all, CGRA-Flow is the most intuitive to use due to its \ac{gui}.
However, as a tradeoff, the architecture design is limited.
Further disadvantages of CGRA-Flow are that it does not consider data layout and that it implements a very simple heuristic search algorithm, which can easily result in a large initiation interval.

Morpher implements a more complex heuristic search algorithm that is able to find mappings with a minimal initiation interval.
But, in exchange for a better mapping, Morpher has a very long search time that scales drastically to the size of a \ac{cgra} and the complexity of a loop (\ie the number of \ac{dfg} nodes).
This non-scalability of \ac{cgra}'s mapping approach is a known challenge, also mentioned in \cite{36_survey_mapping_approach}, \cite{19_CGRA}, and \cite{18_himap}.
Hence, most studies only map simple benchmarks onto a small \ac{cgra} \cite{19_CGRA}.
Therefore, the TCPA-Compiler's greatest advantage is the scalability of its mapping approach.
The mapping time only increases for the number of operations within a target loop body and not for the \ac{pe} count.
Additionally, the ability to map all loop nests and the ability to reuse the same symbolic mappings for different loop bounds are also advantages.
In return, however, the user has to learn a new programming language, PAULA.

