\section{Related Work}
%%CF Chelsea notes:
% robot instruction following (including non-learning based approaches)
% hierarchical methods with language, including hierarchical RL
% VLA methods

%% TODO: we should probably cite papers that do instruction following without VLMs (including older stuff and generalist policies like the MIT Lirui paper and Octo)
%%SL: list of things to cite in this paragraph:
% RT-2
% Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation
% OpenVLA
% pi-zero
Our work relates to research on VLMs for robotic control, which we can categorize into two groups: directly training VLMs for robotic control and using VLMs out-of-the-box with pre-defined robot skills. In the former category, methods fine-tune VLMs to output robotic controls based on input images and language commands~\citep{brohan2023rt,wen2024tinyvla, kim2024openvla, black2024pi_0,liu2024rdt,li2024cogact,o2024open,zawalski2024robotic,zheng2025universal,pertsch2025fast} . While such methods have demonstrated impressive generalization and instruction-following, they are trained for relatively simple commands (``put the cup on the plate''). In contrast, we demonstrate tasks with intricate prompts and human interactions that require situated reasoning.

%%SL: we can start listing the papers to cite in this paragraph:
% SayCan
% Code as policies
% Voxposer (there are a few similar papers)
% MOKA
% PIVOT
% https://arxiv.org/pdf/2409.03966 -> PIVOT-like paper (basically a PIVOT clone)
% https://arxiv.org/pdf/2410.06237
% I. Singh et al., ``Progprompt: Generating situated robot task plans using large language models,''
% S. Wang et al., ``LlmË† 3: Large language model-based task and motion planning with motion failure reasoning,''
% S. Yenamandra et al., ``Homerobot: Open vocabulary mobile manipulation,'' 2023.
%% seems to be a benchmark -> will omit it for now
% ``Ok-robot: What really matters in integrating open-knowledge models for robotics,''
% ``Open-world object manipulation using pre-trained vision-language models,''
% ``Open-vocabulary mobile manipulation in unseen dynamic environments with 3d semantic maps,''
% ``Closed-loop open-vocabulary mobile manipulation with gpt-4v,''
% Survey paper (don't necessarily need to cite, but could cite if it seems appropriate): https://arxiv.org/pdf/2406.04086
% other related work that I'm note sure where to put:
% https://openreview.net/pdf?id=S8jQtafbT3 : uses VLM to correct policy (through parameters sent to low-level controller) from failures, doesn't use human feedback/interaction
In the latter category, a number of methods use LLMs and VLMs to reason over robot observations and commands, and break up multi-stage tasks into simpler steps that can be performed by low-level controllers. Earlier methods of this sort used language models in combination with various learned or hand-designed skills~\citep{huang2022language,brohan2023can, liang2023code, shah2024bumble, singh2023progprompt, wang2024llm}, but such systems have limited ability to incorporate complex context, such as image observations, into the reasoning process. More recently, multiple works have use VLMs to output parameters for pre-defined robotic skills~\citep{huang2023voxposer, liu2024moka, nasiriany2024pivot, chen2024automating, liu2024ok, stone2023open, qiu2024open, zhi2024closed}. Such methods can process more complex commands and situate them in the context of visual observations, but these approaches have shown limited physical dexterity and limited ability to incorporate real-time language interaction with humans (with some exceptions discussed below). In contrast, our system utilizes VLMs for \emph{both} high-level reasoning and low-level control, with a flexible language interface between the two. These design choices, along with a new synthetic data generation scheme, allow our system to achieve both significant physical dexterity and detailed promptability that prior works lack.
% In the latter category, a number of methods use LLMs to parse complex instructions and break them into sub-steps or VLMs to supply parameters for pre-defined skills, thereby enabling more sophisticated reasoning over robot observations and commands \citep{huang2022language,brohan2023can,liang2023code,shah2024bumble,singh2023progprompt,wang2024llm,huang2023voxposer,liu2024moka,nasiriany2024pivot,chen2024automating,liu2024ok,stone2023open,qiu2024open,zhi2024closed}. While such methods can process more complex prompts, they often rely on fixed skill sets and struggle with real-time feedback or dexterous manipulation. In contrast, our system employs a VLM for \emph{both} high-level reasoning and fine-grained control, using a flexible language interface that supports open-ended interaction with human users. Along with a new synthetic data generation pipeline, this design enables both significant physical dexterity and detailed promptability that go beyond prior approaches.

% Most closely related papers:
% OLAF: https://ut-austin-rpl.github.io/olaf/
% Uses an LLM to edit a trajectory in order to output a better trajectory (high-level is not a VLM, reasoning is episodic)
% RACER: https://rich-language-failure-recovery.github.io/
% Uses a VLM to do something similar (also episodic)

%%SL: consider citing some of these
% https://robibutler.github.io/ -> SayCan style planner connecting to learned skills, focus is on human communication ("methods in this category have incorporated LLMs and VLMs")
Many works aim to enable robotic language interaction with users, including model-based systems that parse language instructions and feedback and ground them via a symbolic representation of the scene~\citep{swadzba2009computational, matuszek2013learning, namasivayam2023learning, patki2019inferring}, and more recent learning-based methods that process feedback directly, typically with a hierarchical architecture~\citep{liu2023interactive,xiao2024robi, shi2024yell, belkhale2024rt, singh2024lgr2, mccallum2023feedback,driess2023palm,dai2024racer}. Our work builds on the latter class of methods, where user feedback is incorporated via a high-level policy that provides atomic commands to a learned low-level policy. 
Unlike OLAF~\citep{liu2023interactive}, which uses an LLM to modify robot trajectories, our approach can incorporate situated corrections based on the robot's observations, respond to those corrections in real time, and follow complex prompts describing dexterous manipulation tasks. While YAY Robot~\citep{shi2024yell} can handle situated real-time corrections, it is limited to one prompt and to the corrections seen in the human-written data; our approach leverages VLMs and a new data generation scheme to enable diverse prompts and open-ended corrections. 
Finally, RACER~\citep{dai2024racer} can also incorporate situated corrections, but relies on a physics simulator to construct recovery behaviors; our approach only uses real robot demonstrations without intentional perturbations or corrections and is applicable to open-ended prompts.
% Many works aim to enable robotic language interaction with users, ranging from model-based systems that parse instructions into symbolic scene representations~\citep{swadzba2009computational, matuszek2013learning, namasivayam2023learning, patki2019inferring} to learning-based methods that process feedback directly, typically with a hierarchical architecture~\citep{liu2023interactive, xiao2024robi, shi2024yell, belkhale2024rt, singh2024lgr2, mccallum2023feedback, dai2024racer}. Our work builds on the latter by allowing real-time corrections and open-ended prompts in complex manipulation tasks. Unlike OLAF~\citep{liu2023interactive}, which modifies robot trajectories post-hoc via an LLM, our approach can handle on-the-fly corrections grounded in visual observations; unlike YAY Robot~\citep{shi2024yell}, we support instructions beyond those seen in training; and unlike RACER~\citep{dai2024racer}, which depends on simulation for recovery behaviors, Hi Robot relies only on real-world demonstrations without engineered perturbations and is applicable to open-ended prompts.

% Compared to these prior works, our approach is the only method that can incorporate open-ended situated corrections based on the robot's observations and follow complex prompts describing dextrous tasks, due to our novel data generation scheme. Moreover, unlike OLAF, our approach can incorporate feedback in real time.
% In contrast to OLAF~\cite{liu2023interactive}, our high-level reasoning module processes both corrections and the robot's image observations, enabling it to incorporate situated and semantic corrections. Moreover, our approach is able to process complex prompts and perform delicate manipulation tasks.
%OLAF~\citep{liu2023interactive} uses an LLM to modify the robot's trajectory after failures, parsing a language correction from a user and translating into a physical modification to the robot's motion by an LLM. In contrast, our method incorporates situated and semantic corrections (e.g., ``that's not trash'') in real time during the robot's execution, providing for more flexible and natural behavior. YAY Robot~\citep{shi2024yell} incorporates corrections from the user by directly changing the output of the high-level policy. Our method in contrast supports indirect feedback (``that's not trash''), processing it with a VLM to deduce the most suitable response. 
%RACER~\citep{dai2024racer} uses a VLM to incorporate more situated corrections. However, this method relies on a simulator to generate perturbations with synthetic corrections and corresponding language, whereas our method is trained entirely on standard real-world demonstration data, without any intentional perturbations, mistakes, or corrections. Our method also goes further by enabling the robot to parse complex prompts and incorporate diverse feedback in real time.
%%SL.11.27: I'm not completely happy with this paragraph, especially the last bit about RACER. Should consider ways to improve this