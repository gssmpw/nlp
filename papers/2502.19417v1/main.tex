\documentclass{article}

\usepackage[dvipsnames]{xcolor}
\usepackage[accepted]{icml2025}
\usepackage{algorithm}
\usepackage{algorithmic}
\bibliographystyle{icml2025}

% Custom packages
\usepackage[bookmarks=true]{hyperref}
% \usepackage{xcolor}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bbm}
\usepackage[algo2e,algoruled,boxed,lined,noend]{algorithm2e}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{graphicx}
\usepackage[font=footnotesize]{caption}
\usepackage{subcaption}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=NavyBlue,
    filecolor=NavyBlue,      
    urlcolor=NavyBlue,
    citecolor=NavyBlue,
}
\usepackage[capitalise, nameinlink]{cleveref}

% figref, algref, eqref, secref
\newcommand{\myfigref}[1]{Figure~\ref{#1}}
\newcommand{\myalgref}[1]{Algorithm~\ref{#1}}
\newcommand{\myeqref}[1]{Equation~(\ref{#1})}
\newcommand{\mysecref}[1]{Section~\ref{#1}}
\newcommand{\mytabref}[1]{Table~\ref{#1}}

% Scores
\definecolor{darkgreen}{rgb}{0.09, 0.45, 0.27}
\newcommand{\bestscore}[1]{\textcolor{darkgreen}{\mathbf{#1}}}

% Custom macros
\newcommand{\ours}[0]{Hi Robot\xspace}
%\newcommand{\ours}{\textsc{Hi Robot}\xspace}
\newcommand{\piL}[0]{\pi_{\text{L}}}
\newcommand{\piH}[0]{\pi_{\text{H}}}
\newcommand{\todo}[1]{{\color{red}{\small\sl [Todo: #1]}}}
\newcommand{\lucy}[1]{{\color{blue}{\small\sl [Lucy: #1]}}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\polhi}{p^\text{hi}}
\newcommand{\pollo}{p^\text{lo}}
\newcommand{\polgen}{p^\text{gen}}
\newcommand{\cmd}{\hat{\ell}}
\newcommand{\hist}{\bar{\ell}}

\def \pizero {$\pi_0$}

\usepackage{enumitem}
\begin{document}
\twocolumn[
\icmltitle{\ours: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models}

\begin{icmlauthorlist}
\icmlauthor{Lucy Xiaoyang Shi}{pi,stanford}  
\icmlauthor{Brian Ichter}{pi}  
\icmlauthor{Michael Equi}{pi}  
\icmlauthor{Liyiming Ke}{pi}  
\icmlauthor{Karl Pertsch}{pi,stanford,berkeley}  
\icmlauthor{Quan Vuong}{pi}  
\icmlauthor{James Tanner}{pi}  
\icmlauthor{Anna Walling}{pi}  
\icmlauthor{Haohuan Wang}{pi}  
\icmlauthor{Niccolo Fusai}{pi}  
\icmlauthor{Adrian Li-Bell}{pi}  
\icmlauthor{Danny Driess}{pi}  
\icmlauthor{Lachy Groom}{pi}  
\icmlauthor{Sergey Levine}{pi,berkeley}  
\icmlauthor{Chelsea Finn}{pi,stanford}\\
\url{https://www.pi.website/research/hirobot}
\end{icmlauthorlist}
\icmlaffiliation{pi}{Physical Intelligence, San Francisco, California, USA}
\icmlaffiliation{stanford}{Stanford University, Stanford, California, USA}
\icmlaffiliation{berkeley}{University of California, Berkeley, Berkeley, California, USA}
\icmlcorrespondingauthor{Physical Intelligence}{\texttt{research@physicalintelligence.company}}

%%SL.11.27: What do you think about using the same format as the pi-zero paper? At some point we should just have a PI tech report format, but until then, maybe it's good to use a consistent format across the board? The pi-zero tech report is just using the RSS style files, which is a pretty reasonable two-column format.
%%LS.1.22: Sure, let's port this into the RSS format for release
\icmlkeywords{Machine Learning, Robotics, Language, Vision-Language Models}
\vskip 0.3in
]

\printAffiliationsAndNotice{}  % leave blank if no notice

\begin{abstract}
Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., ``Could you make me a vegetarian sandwich?'' or ``I don't like that one'') require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands (``pick up the cup''), our system can reason through complex prompts and incorporate situated feedback during task execution (``that's not trash''). 
We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping.
% Videos are available at \href{https://hi-robot-vla.github.io/}{https://hi-robot-vla.github.io/}
\end{abstract}

\section{Introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser_v2.pdf}
    % \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \vspace{-6mm}
    \caption{\textbf{Open-ended instruction following.} \ours enables robots to follow multi-stage instructions, adapt to real-time corrections and constraints, complete unseen long-horizon tasks, and respond verbally when needed.}
    \label{fig:teaser}
    \vspace{-3mm}
\end{figure*}

A defining feature of intelligence is its flexibility: people not only excel at complex tasks but also adapt to new situations, modify behaviors in real time, and respond to diverse inputs, corrections, and feedback.
%, and ask clarification questions. 
Achieving this kind of flexibility is essential for robots in open-ended, human-centric environments. For instance, consider a robot tasked with tidying up a table after a meal: instead of rigidly following a single predefined set of steps, the robot would need to interpret dynamic prompts like ``only take away someone's dishes if they are done eating,'' respond to corrections like ``leave it alone,'' and adapt when faced with unfamiliar challenges, such as a delicate object that requires special handling. This paper aims to advance robotic intelligence by enabling robots to interpret and act on diverse natural language commands, feedback, and corrections -- a step towards creating agents that reason through tasks, integrate human feedback seamlessly, and operate with human-like adaptability.
%%SL: I think with the current structure of this paragraph, it's getting a bit long, so it's probably ok to omit this sentence
%Just as the most successful real-world deep learning systems, such as search engines~\citep{} and chatbots~\citep{}, dynamically respond to user inputs, our approach seeks to imbue robots with flexibility. 
If we can enable a robot to process and engage with complex natural language interaction, we can unlock not only better instruction following, but also the ability for users to guide a robot through new tasks and correct the robot in real time.

Achieving this level of flexibility and steerability in robotic systems is challenging. While standard language-conditioned imitation learning can follow simple, atomic instructions such as ``pick up the coke can''~\citep{brohan2022rt}, real-world tasks are rarely so straightforward. Imagine a more realistic prompt, such as: ``Could you make me a vegetarian sandwich? I’d prefer it without tomatoes. Also, if you have ham or roast beef, could you make a separate sandwich with one of those for my friend?'' This requires not only understanding the language, but also the ability to situate commands within the current context and compose existing skills (e.g., picking up the roast beef) to solve a new task. If the robot further receives corrections and feedback (``that's not how you do it, you have to get lower, otherwise you'll keep missing''), these must also be integrated dynamically into task execution. %, and if the prompt or feedback is unclear, the robot might even need to ask clarifying questions.
%Additionally, corrections and feedback often reference the robot's ongoing actions or environment, such as ``that's not how you do it, you have to get lower, otherwise you'll keep missing'' or ``try again, but use the left arm.'' These types of interactions require more than just mapping instructions to actions -- they demand sophisticated semantic reasoning to ground tasks in the robot’s observations and adjust behavior accordingly.
This challenge resembles the distinction between Kahneman's ``System 1'' and ``System 2'' cognitive processes~\citep{kahneman2011thinking}. The ``automatic'' System 1 corresponds to a policy capable of executing straightforward commands by triggering pre-learned skills, while the more deliberative System 2 involves higher-level reasoning to parse complex long-horizon tasks, interpret feedback, and decide on an appropriate course of action. Prior work in robotic instruction following has largely focused on atomic instructions~\citep{stepputtis2020language,jang2022bc,brohan2022rt}, addressing only System 1-level behaviors.
%using language-annotated robot demonstration data.

%However, enabling real-world interaction requires both physical fluency -- policies that execute precise, low-level actions -- and flexible intelligence to process, interpret, and act on intricate, evolving commands. 

%%SL.11.26: To get the length of the intro down, I think we can skip this paragraph, and instead these issues in related work
%A key challenge in enabling robots to handle complex interactions lies in bridging the gap between high-level reasoning and grounded robotic behavior. Existing solutions fall short: while training on annotated robot data through language-conditioned imitation learning, as explored by \citet{lynch2023interactive}, enables the model to understand robotic capabilities, it lacks the ability to ask clarification questions, interpret nuanced instructions, or adapt to novel scenarios. Conversely, leveraging state-of-the-art vision-language models (VLMs) like GPT-4o as high-level policies offers strong language capabilities but fails to ground responses in the robot's physical affordances~\citep{}, often requiring extensive prompt engineering to constrain outputs appropriately in every situation. Moreover, their slow inference speed makes them unsuitable for real-time closed-loop control. Addressing these limitations necessitates a unified approach that integrates the strengths of both language understanding and robotic grounding while operating efficiently in real-world environments.

In this paper, we address the more intricate reasoning needed for complex prompts and feedback by introducing a hierarchical reasoning system for robotic control based on vision-language models (VLMs). In our system, the robot incorporates complex prompts and language feedback using a VLM, which is tasked with interpreting the current observations and user utterances, and generating suitable verbal responses and atomic commands (e.g., ``grasp the cup'') to pass into the low-level policy for execution. This low-level policy is itself a vision-language model finetuned for producing robotic actions, also known as a vision-language-action (VLA) model~\citep{black2024pi_0, brohan2023rt, kim2024openvla,wen2024tinyvla}. We expect that robot demonstrations annotated with atomic commands will not be sufficient for training the high-level model to follow complex, open-ended prompts, and we therefore need representative examples of complex prompt following. To acquire this data, we propose to \emph{synthetically} label datasets consisting of robot observations and actions with hypothetical prompts and human interjections that might have been plausible for that situation. To this end, we provide a state-of-the-art vision-language model with a robot observation and target atomic command, and ask it to come up with a prompt or human interaction that may have preceded that observation and command, i.e. generating high-level policy prompts for different outcomes. By incorporating these synthetically-generated but situated examples into high-level policy training, our approach generalizes to diverse prompts and interjections while maintaining grounding in the robot's capabilities.

%%SL: rewrote this paragraph a bit
%To address the more intricate reasoning needed for complex prompts, feedback, and clarifications, we introduce \ours, a framework that equips robots with the ability to dynamically interpret diverse inputs and seamlessly adapt their behavior to user intentions, environmental context, and feedback in real time. The core of \ours is a vision-language model finetuned on \emph{conversation traces} that encapsulate rich human-robot interactions. As shown in~\myfigref{}, these traces capture human utterances, desirable robot verbal and physical responses (indexed by semantic skills), image observations, and the interaction history within an episode. By training on such contextualized data, \ours learns to generalize to unseen scenarios while maintaining grounding in the robot's affordances. This model serves as a high-level policy that predicts the robot's next verbal or physical response based on the ongoing interaction. Furthermore, to address the challenge of collecting extensive real-world conversation traces, we augment our dataset by generating synthetic traces using state-of-the-art VLMs conditioned on robot skills and image observations, ensuring high-quality, affordance-grounded data for training.

The main contribution of our paper is a \underline{\textbf{h}}ierarchical \underline{\textbf{i}}nteractive \underline{\textbf{robot}} learning system (\ours), a novel framework that uses VLMs for both high-level reasoning and low-level task execution. We show that our framework enables a robot to process much more complex prompts than prior end-to-end instruction following systems and incorporate feedback during task execution
%, and even provides for natural conversation between the robot and the user 
(\myfigref{fig:teaser}). 
While some of the individual components of this system, such as the low-level VLA policy, have been studied in prior work, the combination of these components along with our synthetic data generation scheme are novel and enable novel capabilities.
%While the individual components of this system, such as the ``System 1'' VLA, have been studied in prior work, the combination of these components and the capabilities it enables are novel. 
We evaluate \ours on diverse robots, including single-arm, dual-arm, and mobile platforms. Our evaluation requires the robots to perform a variety of tasks, including new combinations of skills seen during training, in the context of scenarios that span cleaning of messy tables, making sandwiches, and grocery shopping. Our experiments show that \ours surpasses multiple prior approaches, including using API-based VLMs and flat VLA policies, in both alignment with human intent and task success.
%surpasses VLMs trained solely on annotated robotic data in alignment with human intent and task success rates. Compared to calling API-based VLMs, \ours achieves better responsiveness, adaptability, and simplicity in eliciting desirable robotic behavior.
%%SL.11.27: My suggestion would be to move the discussion of API-based VLMS to the related work section, so as to keep this a bit more brief and to-the-point.
By grounding high-level reasoning in both verbal and physical interaction, \ours paves the way for more intuitive and steerable human-robot symbiosis, advancing the potential for flexible intelligence in real-world applications.

\section{Related Work}

%%CF Chelsea notes:
% robot instruction following (including non-learning based approaches)
% hierarchical methods with language, including hierarchical RL
% VLA methods

%% TODO: we should probably cite papers that do instruction following without VLMs (including older stuff and generalist policies like the MIT Lirui paper and Octo)
%%SL: list of things to cite in this paragraph:
% RT-2
% Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation
% OpenVLA
% pi-zero
Our work relates to research on VLMs for robotic control, which we can categorize into two groups: directly training VLMs for robotic control and using VLMs out-of-the-box with pre-defined robot skills. In the former category, methods fine-tune VLMs to output robotic controls based on input images and language commands~\citep{brohan2023rt,wen2024tinyvla, kim2024openvla, black2024pi_0,liu2024rdt,li2024cogact,o2024open,zawalski2024robotic,zheng2025universal,pertsch2025fast} . While such methods have demonstrated impressive generalization and instruction-following, they are trained for relatively simple commands (``put the cup on the plate''). In contrast, we demonstrate tasks with intricate prompts and human interactions that require situated reasoning.

%%SL: we can start listing the papers to cite in this paragraph:
% SayCan
% Code as policies
% Voxposer (there are a few similar papers)
% MOKA
% PIVOT
% https://arxiv.org/pdf/2409.03966 -> PIVOT-like paper (basically a PIVOT clone)
% https://arxiv.org/pdf/2410.06237
% I. Singh et al., ``Progprompt: Generating situated robot task plans using large language models,''
% S. Wang et al., ``Llmˆ 3: Large language model-based task and motion planning with motion failure reasoning,''
% S. Yenamandra et al., ``Homerobot: Open vocabulary mobile manipulation,'' 2023.
%% seems to be a benchmark -> will omit it for now
% ``Ok-robot: What really matters in integrating open-knowledge models for robotics,''
% ``Open-world object manipulation using pre-trained vision-language models,''
% ``Open-vocabulary mobile manipulation in unseen dynamic environments with 3d semantic maps,''
% ``Closed-loop open-vocabulary mobile manipulation with gpt-4v,''
% Survey paper (don't necessarily need to cite, but could cite if it seems appropriate): https://arxiv.org/pdf/2406.04086
% other related work that I'm note sure where to put:
% https://openreview.net/pdf?id=S8jQtafbT3 : uses VLM to correct policy (through parameters sent to low-level controller) from failures, doesn't use human feedback/interaction
In the latter category, a number of methods use LLMs and VLMs to reason over robot observations and commands, and break up multi-stage tasks into simpler steps that can be performed by low-level controllers. Earlier methods of this sort used language models in combination with various learned or hand-designed skills~\citep{huang2022language,brohan2023can, liang2023code, shah2024bumble, singh2023progprompt, wang2024llm}, but such systems have limited ability to incorporate complex context, such as image observations, into the reasoning process. More recently, multiple works have use VLMs to output parameters for pre-defined robotic skills~\citep{huang2023voxposer, liu2024moka, nasiriany2024pivot, chen2024automating, liu2024ok, stone2023open, qiu2024open, zhi2024closed}. Such methods can process more complex commands and situate them in the context of visual observations, but these approaches have shown limited physical dexterity and limited ability to incorporate real-time language interaction with humans (with some exceptions discussed below). In contrast, our system utilizes VLMs for \emph{both} high-level reasoning and low-level control, with a flexible language interface between the two. These design choices, along with a new synthetic data generation scheme, allow our system to achieve both significant physical dexterity and detailed promptability that prior works lack.
% In the latter category, a number of methods use LLMs to parse complex instructions and break them into sub-steps or VLMs to supply parameters for pre-defined skills, thereby enabling more sophisticated reasoning over robot observations and commands \citep{huang2022language,brohan2023can,liang2023code,shah2024bumble,singh2023progprompt,wang2024llm,huang2023voxposer,liu2024moka,nasiriany2024pivot,chen2024automating,liu2024ok,stone2023open,qiu2024open,zhi2024closed}. While such methods can process more complex prompts, they often rely on fixed skill sets and struggle with real-time feedback or dexterous manipulation. In contrast, our system employs a VLM for \emph{both} high-level reasoning and fine-grained control, using a flexible language interface that supports open-ended interaction with human users. Along with a new synthetic data generation pipeline, this design enables both significant physical dexterity and detailed promptability that go beyond prior approaches.

% Most closely related papers:
% OLAF: https://ut-austin-rpl.github.io/olaf/
% Uses an LLM to edit a trajectory in order to output a better trajectory (high-level is not a VLM, reasoning is episodic)
% RACER: https://rich-language-failure-recovery.github.io/
% Uses a VLM to do something similar (also episodic)

%%SL: consider citing some of these
% https://robibutler.github.io/ -> SayCan style planner connecting to learned skills, focus is on human communication ("methods in this category have incorporated LLMs and VLMs")
Many works aim to enable robotic language interaction with users, including model-based systems that parse language instructions and feedback and ground them via a symbolic representation of the scene~\citep{swadzba2009computational, matuszek2013learning, namasivayam2023learning, patki2019inferring}, and more recent learning-based methods that process feedback directly, typically with a hierarchical architecture~\citep{liu2023interactive,xiao2024robi, shi2024yell, belkhale2024rt, singh2024lgr2, mccallum2023feedback,driess2023palm,dai2024racer}. Our work builds on the latter class of methods, where user feedback is incorporated via a high-level policy that provides atomic commands to a learned low-level policy. 
Unlike OLAF~\citep{liu2023interactive}, which uses an LLM to modify robot trajectories, our approach can incorporate situated corrections based on the robot's observations, respond to those corrections in real time, and follow complex prompts describing dexterous manipulation tasks. While YAY Robot~\citep{shi2024yell} can handle situated real-time corrections, it is limited to one prompt and to the corrections seen in the human-written data; our approach leverages VLMs and a new data generation scheme to enable diverse prompts and open-ended corrections. 
Finally, RACER~\citep{dai2024racer} can also incorporate situated corrections, but relies on a physics simulator to construct recovery behaviors; our approach only uses real robot demonstrations without intentional perturbations or corrections and is applicable to open-ended prompts.
% Many works aim to enable robotic language interaction with users, ranging from model-based systems that parse instructions into symbolic scene representations~\citep{swadzba2009computational, matuszek2013learning, namasivayam2023learning, patki2019inferring} to learning-based methods that process feedback directly, typically with a hierarchical architecture~\citep{liu2023interactive, xiao2024robi, shi2024yell, belkhale2024rt, singh2024lgr2, mccallum2023feedback, dai2024racer}. Our work builds on the latter by allowing real-time corrections and open-ended prompts in complex manipulation tasks. Unlike OLAF~\citep{liu2023interactive}, which modifies robot trajectories post-hoc via an LLM, our approach can handle on-the-fly corrections grounded in visual observations; unlike YAY Robot~\citep{shi2024yell}, we support instructions beyond those seen in training; and unlike RACER~\citep{dai2024racer}, which depends on simulation for recovery behaviors, Hi Robot relies only on real-world demonstrations without engineered perturbations and is applicable to open-ended prompts.

% Compared to these prior works, our approach is the only method that can incorporate open-ended situated corrections based on the robot's observations and follow complex prompts describing dextrous tasks, due to our novel data generation scheme. Moreover, unlike OLAF, our approach can incorporate feedback in real time.
% In contrast to OLAF~\cite{liu2023interactive}, our high-level reasoning module processes both corrections and the robot's image observations, enabling it to incorporate situated and semantic corrections. Moreover, our approach is able to process complex prompts and perform delicate manipulation tasks.
%OLAF~\citep{liu2023interactive} uses an LLM to modify the robot's trajectory after failures, parsing a language correction from a user and translating into a physical modification to the robot's motion by an LLM. In contrast, our method incorporates situated and semantic corrections (e.g., ``that's not trash'') in real time during the robot's execution, providing for more flexible and natural behavior. YAY Robot~\citep{shi2024yell} incorporates corrections from the user by directly changing the output of the high-level policy. Our method in contrast supports indirect feedback (``that's not trash''), processing it with a VLM to deduce the most suitable response. 
%RACER~\citep{dai2024racer} uses a VLM to incorporate more situated corrections. However, this method relies on a simulator to generate perturbations with synthetic corrections and corresponding language, whereas our method is trained entirely on standard real-world demonstration data, without any intentional perturbations, mistakes, or corrections. Our method also goes further by enabling the robot to parse complex prompts and incorporate diverse feedback in real time.
%%SL.11.27: I'm not completely happy with this paragraph, especially the last bit about RACER. Should consider ways to improve this

\section{Preliminaries and Problem Statement}
\label{sec:prelim}

A learned policy controls a robot by processing observation inputs, which we denote $\bo_t$, and producing one or more actions $\bA_t = [\ba_{t}, \ba_{t+1}, ..., \ba_{t+H-1}]$, where we use $\bA_t$ to denote an \emph{action chunk} consisting of the next $H$ actions to execute~\citep{zhao2023learning}. Our system takes as input the images from multiple cameras $\bI^1_t, ... , \bI^n_t$, the robot's configuration (i.e., joint and gripper positions) $\bq_t$, and a language prompt $\ell_t$. 
% We'll discuss in Section~\ref{sec:method} how this prompt can also include past language interactions with the user. 
Thus, we have $\bo_t = [\bI^1_t, ... , \bI^n_t, \ell_t, \bq_t]$, and the policy represents the distribution $p(\bA_t | \bo_t)$. Prior works have proposed various methods for representing and training such policies~\citep{zhao2023learning, chi2023diffusionpolicy, octo_2023, pertsch2025fast}.

Since our focus will be specifically on complex, multi-stage tasks that require parsing intricate prompts and even dynamic user feedback, we need our policies to be able to interpret complex language and ground it via observations of the environment. A particularly powerful approach for handling such complex semantics is provided by vision-language-action (VLA) models~\citep{black2024pi_0, brohan2023rt, kim2024openvla,wen2024tinyvla}, which use vision-language model (VLM) pre-training to initialize the policy $p(\bA_t | \bo_t)$. A VLM is a language model that has also been trained to process image inputs, and represents a distribution $p(\ell^\prime | \bI, \ell)$ -- the probability of a language \emph{suffix} $\ell^\prime$ (e.g., an answer to a question) in response to an image-language prefix consisting of an image $\bI$ and a prompt $\ell$ (e.g., a visual question). The most commonly used VLMs represent $p(\ell^\prime | \bI, \ell)$ via an autoregressive decoder-only Transformer model, factorizing the distribution into a product of autoregressive token probabilities $p(\bx_{t+1} | \bx_1, ..., \bx_t, \bI)$, where $\bx_t$ denotes the $t^\text{th}$ token (not to be confused with a physical time step), and we have $\ell = [\bx_1, ..., \bx_{t_p}]$ and $\ell^\prime = [\bx_{t_p+1}, ..., \bx_{t_p + t_s}]$, with $t_p$ the length of the prefix and $t_s$ the length of the suffix~\citep{beyer2024paligemma}. We also use such Transformer-based VLMs, but since we do not modify their architecture and their autoregressive structure is therefore not relevant to our discussion, we will use the more concise $p(\ell^\prime | \bI, \ell)$ notation to represent a standard VLM.

A standard VLA is produced by fine-tuning the VLM $p(\ell^\prime | \bI, \ell)$ such that the actions $\bA_t$ are represented by tokens in the suffix $\ell^\prime$, typically by tokenizing the actions via discretization. We build on the \pizero\ VLA~\citep{black2024pi_0}, which additionally handles multiple images and continuous state observations $\bq_t$, and modifies the VLM to output continuous action chunk distributions via flow-matching, but the high-level principles are similar. While such VLA models can follow a wide variety of language prompts~\citep{brohan2023rt}, by themselves they are typically limited to simple and atomic commands, and do not handle the complex prompts and feedback that we study in this paper. 
% Our full system will use two VLMs, one that serves as a high-level policy, producing intermediate substeps represented via language, and a second one that serves as the low-level policy, producing the continuous action chunks.

\section{\ours}
\label{sec:method}

%%SL.11.27: My recommendation for a more top-down organization would be instead to start by explaining what the model does at inference time *first*, and only then explain how it is trained. The reason for this is that, after you explain the inference time procedure, it will be much clearer to readers what you are trying to accomplish, and therefore the explanation of the training process will be much more intuitive. The other reason is that, as a system paper of this type, I expect that the overall inference time procedure will be much more the thing that makes an impact and that people will remember, vs. the synthetic data generation is something that will be refined and changed in future works. If the story is "we do inference via a System 1/System 2 decomposition" (which I think is the most compelling version of this story), then it makes sense to lead with the inference-time stuff. I would therefore recommend this organization:
% Sec 3: Preliminaries
%  -> this section covers VLMs, VLAs, etc., focus on the basics and on notation, do not introduce anything novel
% Sec 4: [\ours]
%  -> start with an overview and a figure, introducing notation and the overall architecture; say nothing about training
%  Sec 4.1: Hierarchical inference with VLAs
%   -> discuss the inference procedure for fully "autonomous mode"
%  Sec 4.2: Incorporating human interaction
%   -> discuss how interaction happens
% Sec 5: training [\ours]
%  -> this section covers all the training, data generation, etc

%%SL.1.29: any way we can get Fig 2 onto the same page as this?
\begin{figure}[t]
    \centering
    % \includegraphics[width=\linewidth]{figures/block_diagram.png}
    % \includegraphics[width=\linewidth]{figures/block_diagram_v3.png}
    \includegraphics[width=\linewidth]{figures/block_diagram_v2.pdf}
    %%SL.1.29: This is generally a good figure, but it makes it a bit unclear what exactly is the distinction between this and prior methods. I wonder if we can add to this figure a little bit to make it clearer what new parts are? Some things we could add (not necessarily all of these): (1) the dialogue history; (2) interventions/feedback; (3) the robot talking back
    %%SL.1.29: also consider if this figure can use the same notation as what we use in Sec 4
    % \caption{\textbf{Overview of hierarchical VLA.} We decompose the policy $p(\bA_t | \bo_t)$ into a high-level policy $\polhi(\cmd_t | \bI^1_t, ... , \bI^n_t, \ell_t)$ and a low-level policy $\pollo(\bA_t | \bI^1_t, ... , \bI^n_t, \cmd_t, \bq_t)$. 
    \vspace{-6mm}
    \caption{\textbf{Overview of hierarchical VLA.} The policy consists of a high-level and a low-level policy.
    The high-level policy processes open-ended instructions and images from base and wrist-mounted cameras to generate low-level language commands. The low-level policy uses these commands, images, and robot states to produce actions and optionally verbal responses.}
    \label{fig:overview}
    \vspace{-6mm}
\end{figure}
We provide an overview of our method in Figure~\ref{fig:overview}. Our approach decomposes the policy $p(\bA_t | \bo_t)$ into a low-level and high-level inference process, where the low-level policy consists of a VLA that produces the action chunk $\bA_t$ in response to a simpler, low-level language command, and the high-level policy consists of a VLM that processes the open-ended task prompt, and outputs these low-level language commands for the low-level inference process. 
% The two processes run at different rates, with the low-level process producing action chunks at a high frequency, and the high-level process being invoked less often, either when enough time has passed or when new language feedback from the user is received. 
The two processes run at different rates: the low-level process produces action chunks at a high frequency, while the high-level process is invoked less often, either after a set time or upon receiving new language feedback.
Thus, the high-level process essentially ``talks'' to the low-level process, breaking down complex prompts and interactions into bite-sized commands that can be converted into actions.

\subsection{Hierarchical Inference with VLAs}
%%SL.1.29: could this section somehow reference Figure 1 and use it as a map to present this?

Formally, the high-level policy $\polhi(\cmd_t | \bI^1_t, ... , \bI^n_t, \ell_t)$ takes in the image observations and an open-ended prompt $\ell_t$, and produces an intermediate language command $\cmd_t$. The low-level policy $\pollo(\bA_t | \bI^1_t, ... , \bI^n_t, \cmd_t, \bq_t)$ takes in the same type of observation as the standard VLA described in Section~\ref{sec:prelim}, except that the language command $\ell_t$ is replaced by the output from the high-level policy $\cmd_t$. Thus, following the System 1/System 2 analogy, the job of the high-level policy is to take in the overall task prompt $\ell_t$ and accompanying context, in the form of images and user interactions, and translate it into a suitable task for the robot to do at this moment, represented by $\cmd_t$, that the low-level policy is likely to understand. Of course, for simple and familiar tasks, this is not necessary -- if we simply want the robot to perform a task that the low-level policy was directly trained for, we could simply set $\cmd_t = \ell_t$ and proceed as in prior work~\citep{brohan2022rt}. The benefit of this hierarchical inference process is in situations where either the prompt $\ell_t$ is too complex for the low-level policy to parse, too unfamiliar in the context of the robot data, or involves intricate interactions with the user.

The high-level policy is represented by a VLM that uses the images and $\ell_t$ as the prefix, and produces $\cmd_t$ as the suffix. We describe how this model is trained in Section~\ref{sec:training}.

Since high-level inference is slower but also less sensitive to quick changes in the environment, we can comfortably run it at a lower frequency. A variety of strategies could be used to instantiate this, including intelligent strategies where the system detects when the command $\cmd_t$ has been completed before inferring the next suitable command. In our implementation, we found a very simple strategy to work well: we rerun high-level inference and recompute $\cmd_t$ either when one second has elapsed, or when a new interaction with the user takes place. This provides reactive behavior when the user provides feedback or corrections, while maintaining simplicity.

\subsection{Incorporating User Interaction}

The user can intervene at any point during policy execution and provide additional information and feedback, or even change the task entirely. In our prototype, these interventions take the form of text commands or spoken language (which is then transcribed into text). When the system receives a user intervention, the high-level inference is triggered immediately to recompute $\cmd_t$. The high-level policy has the option to include a verbal utterance $u_t$ in the command $\cmd_t$, which can be confirmations or clarifications from the robot. When $u_t$ is included, we use a text to speech system to play the utterance to the user, and remove it from $\cmd_t$ before passing it into the low-level policy. 
%%SL.1.29: is there some way to include the utterance in Fig 1 (including with notation that matches the paper text) to help explain this?
% This utterance is also concatenated to the transcript $\hist_t$ [is that right?].

When an interjection (``leave it alone’’) has been fulfilled, the user can signal to the robot that it may switch back to the previous command and continue the task execution.
Notably, the responses of the high-level policy are \emph{contextual}, because it observes not only the prompt $\ell_t$, but also the current image observations. Therefore, it can correctly ground feedback like ``that's not trash,'' which is not possible with language-only systems. 
% However, besides providing the high-level policy with the images and transcript, we do not provide for any additional machinery at test-time to handle multi-turn interactions with the user -- dialogues, responses, and questions are all emergent properties of the system.
%%SL: adjust the above sentence depending on what the results actually show
%%SL.1.29: might need to adjust the above depending on what the current results show

\begin{figure}[t]
    \centering
    % \includegraphics[width=0.9\linewidth]{figures/data_gen.png}
    % \includegraphics[width=\linewidth]{figures/data_gen_v5.png}
    \includegraphics[width=\linewidth]{figures/data_gen_v3.pdf}
    \vspace{-7mm}
    \caption{\textbf{Data collection and generation for training the high-level policy.} We first collect teleoperated robot demonstrations and segment them into short skills (e.g., \emph{pick up KitKat}). Using this labeled data, we prompt a vision-language model (VLM) to generate synthetic user instructions (e.g., \emph{``Can you get me something sweet?''}) and robot responses. The resulting dataset is used to train the high-level policy, which maps image observations and user commands to verbal responses and skill labels.}
    \label{fig:data_generation}
    \vspace{-4mm}
\end{figure}

\subsection{Data Collection and Training \ours}
\label{sec:training}

To train \ours in a scalable manner, we employ both human-labeled and synthetically generated interaction data, as illustrated in \myfigref{fig:data_generation}. First, we collect robot demonstration data $\mathcal{D}_{demo}$ via teleoperation. This yields trajectories with coarse language annotations of the overall goal (e.g., \emph{make a sandwich}). We then segment these full demonstration episodes into short skills, $\cmd_t$, such as \emph{pick up one piece of lettuce}, which generally last between one and three seconds. We also heuristically extract basic movement primitives (e.g., small corrective motions) such as \emph{move the right arm to the left} from the raw robot actions. The resulting dataset $\mathcal{D}_{labeled}$ contains a set of $(\cmd_t , \bI^1_t, ... , \bI^n_t)$ tuples that describe robot skills.
%%SL.1.29: can we use the same notation as in Sec 4.1? I think \cmd_t is the right label for "short skill label", so this should be (\cmd_t , \bI^1_t, ... , \bI^n_t) tuples (which conveniently is almost everything that we need for the high level policy as defined at the top of Sec 4.1 except the high-level language, so we can introduce that symbol in the next paragraph)

Next, we use a large vision-language model (VLM) $\polgen$ to produce synthetic user prompts and interjections $\ell_t$, and corresponding robot utterance $u_t$.
%%SL.1.29: can we make sure the term "interaction traces" appears in Sec 4.2 and it's clear what it means? ideally we would have a symbol that weu se consistenly for it (which I *think* should be \ell_t based on the definitions in the preceding subsections)
Given $\mathcal{D}_{labeled}$, we prompt $\polgen$ with both the visual context 
% (from all relevant camera views)
%%SL.1.29: use symbol
$\bI^1_t, ... , \bI^n_t$
and the skill label $\cmd_t$ (e.g., \emph{pick up the lettuce}).
%%SL.1.29: use symbol
$\polgen$ then imagines an appropriate interaction that might have led to $\cmd_t$ in a real user interaction: it generates possible user prompts $\ell_t$ (e.g., \emph{``Can you add some lettuce for me?''}) along with the robot’s verbal responses and clarifications $u_t$.
%%SL.1.29: use symbol, e.g.: These are then use to provide a synthetic context $\ell_t$ that, when combined with the images $\bI^1_t, ... , \bI^n_t$ associated with the example and the correct low-level command $\cmd_t$ provide us with everything we need to train the high-level policy $whatever$.
% These are then used to provide a synthetic context that, when combined with the images $\bI^1_t, ... , \bI^n_t$ associated with the example and the correct low-level command $\cmd_t$, provide us with a synthetic dataset $\mathcal{D}_{syn}$ for training the high-level policy $\polhi(\cmd_t | \bI^1_t, ... , \bI^n_t, \ell_t)$.
We detail the generation of the synethetic dataset $\mathcal{D}_{syn}$ in Appendix~\ref{sec:data_gen}.

% During data generation, we also condition $\polgen$ on previous skill labels within the same episode $\cmd_0, ..., \cmd_{t-1}$ to maintain coherence regarding which actions have already been performed and to better ground each successive skill in context.
%%SL.1.29: can we express this with the same symbols?
% Additionally, we incorporate scenario classification (e.g., \emph{negative task}, \emph{ambiguous intention}) and categories of robot responses (e.g., \emph{simple confirmations}, \emph{clarifications}, \emph{error handling}) and provide examples to further guide the synthetic generation process to ensure data quality and diversity, following strategies suggested in prior work~\cite{}.
%%SL.1.29: this last "Additionally" paragraph contains a lot of nuance. It's not really clear (not reproducible) from this sentence alone what exactly we are doing -- could it make sense to make this an entire separate paragraph, or at least expand it a bit to explain it more fully and more precisely?

% When the amount of synthetic data is relatively small,
%%SL.1.29: do we always do this? if so let's cut "When", but if we only sometimes do it, we should clearly explain what "relatively small" means. Maybe better to just embrace the ambiguity here and say: We can train on either only the synthetic data or, optionally, on both the synthetic and human-labeled data (but if we always do both, let's just say that and keep it simple)
We train the high-level policy $\polhi(\cmd_t | \bI^1_t, ... , \bI^n_t, \ell_t)$ on  $\mathcal{D}_{syn} \cup \mathcal{D}_{labeled}$ using the cross-entropy loss for next-token prediction. 
% Specifically, we use the combined dataset to train our high-level policy, which takes as input image observations and a user command (or conversation snippet) and outputs the robot’s next verbal response and the corresponding skill label.
%%SL.1.29: use symbols, they were introduced already
To train 
% the low-level policy (the controller that executes the predicted skill in terms of joint positions, gripper commands, base movements, etc.),
%%SL.1.29: instead of a big parenthetical, let's just use the symbol, it will make it much less ambiguous and more concise
the low-level policy $\pollo(\bA_t | \bI^1_t, ... , \bI^n_t, \cmd_t, \bq_t)$,
we use $\mathcal{D}_{labeled} \cup \mathcal{D}_{demo}$ using a flow-matching objective, following~\citet{black2024pi_0}.
%%SL.1.29: can we introduce symbols for these datasets earlier and use these symbols throughout? It would also help to explain precisely how we use it. The high-level policy is trained on tuples \ell_t, images, \cmd_t -- can we say precisely what we set these to when we use the human-labeled data, using the same notation we introduced before?
% These larger unlabeled segments are used to refine the low-level skill execution without relying on synthetic dialogues, which primarily focus on language grounding and high-level decision-making.

% While in this work we train a separate high-level policy for each task (e.g., sandwich making vs.\ table cleaning) for clarity and ease of benchmarking, the architecture is readily amenable to a unified multi-task formulation. In principle, the same hierarchical approach could be used to train a single high-level policy across a multitude of tasks, facilitating knowledge transfer between task domains and more robust, open-ended robot behavior.

% \subsection{Synthetic Data Generation}
% \label{sec:data_gen}
% To ensure the quality and diversity of the synthetic data, we incorporate structured scenario classification and response categorization into the prompt design for $\polgen$, following~\cite{stephan2024rlvf}. Specifically, we classify interactions into different scenario types, such as \emph{negative task} (where the user instructs the robot what \emph{not} to do),  \emph{situated correction} (where the user adjusts an earlier command based on the evolving task state), and \emph{specific constraint} (where the user specifies particular constraints, such as dietary preferences). In addition, we categorize the robot’s responses into types such as \emph{simple confirmations}, \emph{clarifications}, and \emph{error handling}. These classifications guide the generation process to ensure a broad range of user-robot interactions.

% In prompt $\mathcal{P}$, we include a detailed description of the task (e.g., bussing a table, making a sandwich, grocery shopping) and instruct the model to ground responses in visual observations and prior context. A key advantage of leveraging large pretrained VLMs is their ability to incorporate world knowledge when generating interactions. For instance, the model can infer dietary constraints when generating prompts for sandwich-making, producing user commands such as ``Can you make a sandwich for me? I’m lactose intolerant'' and an appropriate robot response like ``Sure, I won’t put cheese on it.'' Similarly, it can reason over ambiguous or implicit requests, such as inferring that ``I want something sweet'' in a grocery shopping scenario should lead to suggestions like chocolate or candy.

% To maintain consistency in multi-step tasks, we condition $\polgen$ on prior skill labels within an episode $\cmd_0, ..., \cmd_{t-1}$, allowing it to generate coherent user commands that account for past actions. For instance, if the robot has already placed lettuce and tomato on a sandwich, the generated user prompt might request additional ingredients that logically follow. This ensures that the synthetic interactions reflect realistic task progression rather than isolated commands.
% As such, we leverage $\polgen(\ell_t, u_t | \bI^1_t, ... , \bI^n_t, \cmd_0, ..., \cmd_{t-1}, \cmd_t, \mathcal{P})$ to produce a richer, more diverse synthetic dataset $\mathcal{D}_{syn}$ that provides meaningful supervision for training our high-level policy.

\subsection{Model Architecture and Implementation}

% Our system employs a hierarchical vision-language-action (VLA) architecture comprising a high-level policy and a language-conditioned low-level policy.
% For the low-level policy, we use the $\pi_0$ VLA~\cite{}, which generates continuous robotic actions through flow matching~\cite{}. While we employ $\pi_0$ for our experiments, our framework is inherently modular, allowing for the integration of alternative language-conditioned policies as needed.

% The high-level policy is trained by fine-tuning PaliGemma-3B without freezing model weights. While we chose PaliGemma-3B for its relatively small size and efficiency, our framework remains compatible with any pre-trained vision-language model (VLM), offering flexibility in adapting to more advanced or specialized VLMs. Further details on the implementation and training setup can be found in Appendix~\ref{sec:appendix:implementation}.

%%SL.1.29: This is OK, but we could probably emphasize more than the high-level and low-level are almost the same model. E.g., we could write something like:
In our implementation, the low-level and high-level policies use the same base VLM as a starting point, namely the PaliGemma-3B VLM~\citep{beyer2024paligemma}. The low-level policy is the $\pi_0$ VLA~\citep{black2024pi_0}, which is trained by finetuning PaliGemma-3B with an additional flow matching ``action expert'' to produce continuous actions, while the high-level policy is fine-tuned on the image-language tuples described in \mysecref{sec:training} to predict commands. While we employ $\pi_0$ for our experiments, our framework is inherently modular, allowing for the integration of alternative language-conditioned policies as needed. 
% In fact the two models do not even need to be separated, and conceivably the same model weights could be used for \emph{both} the high-level and low-level model in the future -- the main concept behind our approach is rather to employ a hierarchical \emph{inference} procedure, where the high-level inference process makes semantic inferences, and the low-level inference process takes care of motor control.

%%SL: these are the old subsections

%\subsection{High-Level Policy}

%\subsection{Low-Level Policy}

%\subsection{Data Collection and Annotation}

%\subsection{Conversation Traces}

%\subsection{Synthetic Data Generation}

%\subsection{VLM Finetuning}

%\subsection{Inference-Time Interaction}

%%LS: moved to appendix
% \section{System and Robot Overview}

% This describes the robot, robot system, etc. This would be the place to explain microphone input, GPUs, inference timing, etc


\section{Experiments}

In our experimental evaluation, we study a range of problems that combine challenging physical interactions with complex user interaction, including multi-stage instructions, live user feedback in the middle of the task, and prompts that describe novel task variations.
% , such as asking a policy that was trained to clean up a table to clear only the dishes. 
% These tasks challenge \emph{both} the low-level control and high-level reasoning capabilities of our system, by requiring well-trained policies to execute complex and dexterous behaviors (e.g., delicately manipulating the ingredients for a sandwich), and by requiring the system to interpreting nuanced instructions that require a significant deviation from the default behavior (e.g., picking up only the dishes when cleaning a table). 
We compare our full method to prior approaches and to alternative designs that use other high-level policy training methods. The aims of our experiments are:

%%CF.1.29: Revised this to line up more closely with the experiments section
\setlist{nolistsep}
\begin{enumerate}[noitemsep,leftmargin=*]
  \item Evaluate the ability of our method to follow a variety of complex textual prompts and live user feedback.
  \item Compare our full method to prior approaches that train a flat instruction-following VLA policy or that use foundation models out-of-the-box for high-level reasoning.
  \item Evaluate the importance of synthetic data and hierarchy for task performance and language following.
\end{enumerate}

\subsection{Tasks and Baseline Methods}

\begin{figure*}[h!]
    \centering
    % \includegraphics[width=0.95\linewidth]{figures/tasks.png}
    \includegraphics[width=\linewidth]{figures/tasks_v2.pdf}
    % \includegraphics[width=\linewidth]{figures/tasks.pdf}
    \vspace{-6mm}
    \caption{\textbf{Task domains used in our evaluation}. Across three domains, we evaluate complex instructions, intermediate feedback, and user interruptions. For example, in Table Bussing, when the user says, ``that's not trash,'' the robot correctly puts the bowl back down instead of putting it away. All images are from policy rollouts.}
    \label{fig:tasks}
    \vspace{-3mm}
\end{figure*}

\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\linewidth]{figures/baselines.png}
    % \includegraphics[width=\linewidth]{figures/baselines_v3.png}
    \includegraphics[width=\linewidth]{figures/baselines_v2.pdf}
    \vspace{-8mm}
    % \caption{\textbf{Baseline Comparisons.} \ours outperforms GPT-4o and flat VLA across three open-ended robotic tasks—Table Bussing, Sandwich Making, and Grocery Shopping—each requiring instruction following, situated reasoning, and multi-step interactions. On average, \ours achieves over 40\% higher instruction accuracy than GPT-4o, demonstrating stronger alignment with user prompts and real-time observations. While expert human guidance remains the upper bound, \ours closes the gap by leveraging its high-level policy for robust decision-making.}
    \caption{\textbf{Comparisons to Prior Methods.} \ours outperforms GPT-4o and flat VLA on Table Bussing, Sandwich Making, and Grocery Shopping. \ours averages over 40\% higher instruction accuracy than GPT-4o, showing stronger alignment with user prompts and real-time observations, and approaches expert human guidance by leveraging its high-level policy.}
    \label{fig:baselines}
    \vspace{-5mm}
\end{figure*}

We use three complex problem domains in our experiments, 
% each intended to provide different types of challenges. In each domain, we evaluate with multiple different language prompts and interjections, 
% as described below. Illustrations of each problem are shown in Figure~\ref{fig:tasks}.
as shown in Figure~\ref{fig:tasks}.

\noindent \textbf{Table bussing} involves cleaning up a table, placing dishes and utensils into a bussing bin and trash items into the trash. The training data consists of full table cleaning episodes.
%, where the robot places all dishes and utensils into the bussing bin and all trash into the trash bin.
% , with detailed language annotations for each segment (e.g., ``pick up the plate''). 
This task is physically challenging because some items require nuanced grasping strategies (e.g., grasping a plate by the edge), 
the robot must pick up and singulate different objects, 
and in some cases might even manipulate some objects using others (e.g., picking up a plate with trash on it and tilting the plate to dump the trash into the trash bin). 
In our evaluation, the robot receives prompts that substantively alter the goal of the task, such as ``can you clean up only the trash, but not dishes?'', ``can you clean up only the dishes, but not trash?'', and ``bus all the yellowish things''. This requires the high-level model to reason about the task and each object (e.g., recognizing that reusable plastic cups are dishes, while paper cups are trash), then modify the robot's ``default'' behavior of always putting away all items. 
This includes understanding what to do and also what \emph{not} to do (e.g., avoid touching dishes when asked to collect only trash). 
The robot might also receive contextual feedback \emph{during} the task, such as ``this is not trash'', ``leave the rest'', or ``leave it alone,'' which require it to understand the interjection and respond accordingly. \\
%%CF: below is redundant. also revised above to make this more clear that it is the full evaluation
%In all these cases, the system has to \emph{alter} the task from its default formulation, presenting a considerable ``System 2'' challenge.
%Test prompts: ``can you clean up only the trash, but not dishes?'', ``can you clean up only the dishes, but not trash?'', ``this is not trash'', ``leave the rest'', ``bus all the yellow-ish things'', ``leave it alone''
\noindent \textbf{Sandwich making} requires the robot to make a sandwich, using up to six ingredients as well as bread.
%The ingredients start out in bins on the right side of the table, and the bread is placed on the left side.
This task is physically difficult, because the robot has to manipulate deformable and delicate ingredients that have to be grasped carefully and placed precisely.
%(e.g., the cheese slices can break, the meat deforms into clumps, and the lettuce leaves are large and fragile).
The data contains examples of different types of sandwiches, with segment labels (e.g., ``pick up one slice of bread''). We use this task to evaluate complex prompts, such as ``hi robot, can you make me a sandwich with cheese, roast beef, and lettuce?'' or ``can you make me a vegetarian sandwich? I’m allergic to pickles'', and live corrections, like ``that's all, no more''. \\
%Test prompts: ``hi robot, can you make me a sandwich with cheese, roast beef, and lettuce?'', ``can you get me another slice of bread?'', ``hi robot, can you make me a sandwich? i’m allergic to pickles''
\noindent \textbf{Grocery shopping} entails picking up a combination of requested items from a grocery shelf, placing them into a basket, and placing the basket on a nearby table. This task requires controlling a bimanual mobile manipulator (see Figure~\ref{fig:tasks}) and interpreting nuanced semantics that involve variable numbers of objects. Examples of prompts include ``hey robot, can you get me some chips? I'm preparing for a movie night'', ``can you get me something sweet?'', ``can you grab me something to drink?'', ``hey robot, can you get me some Twix and Skittles?", as well as interjections such as ``I also want some Kitkat".
%Test prompts: ``hey robot, can you get me some chips? i’m preparing for a movie night'', ``can you get me something sweet?'', ``can you grab me something to drink?'', ``hey robot, can you get me some twix and skittles?'', ``i also want some kitkat''

\begin{figure*}[t]
    \centering
    \vspace{-1mm}
    % \includegraphics[width=\linewidth]{figures/qualitative_comparison_v3.png}
    \includegraphics[width=\linewidth]{figures/qualitative_comparison_v2.pdf}
    % \caption{\textbf{Comparison of low-level language command predictions.} GPT-4o high-level struggles with (a) recognizing the object the robot is interacting with rather than background objects, (b) issuing commands that correspond to the ongoing subtask instead of prematurely jumping to the next one, and (c) predicting commands that align with human intent. In contrast, \ours produces commands that better match the robot's current behavior and human expectations. The high-level policy trained without synthetic data makes language predictions that align well with image observations, but sometimes ignores user requests.}
    \vspace{-8mm}
    \caption{\textbf{Qualitative Command Comparisons.} GPT-4o often (a) misidentifies objects, (b) skips subtasks, or (c) ignores user intent. \ours consistently produces commands aligned with the robot’s ongoing actions and user requests. Without synthetic data, the high-level policy aligns well with image observations but ignores user constraints.}
    \label{fig:qualitative_comparison}
    \vspace{-5mm}
\end{figure*}

\noindent \textbf{Comparisons and ablations.}
Our comparisons evaluate our full method and a number of alternative approaches, which either employ a different type of high-level strategy, or do not utilize a hierarchical structure. These include:

\textbf{Expert human high level}: This \textbf{oracle} baseline uses an expert human in place of the high-level model, who manually enters language commands for low-level behaviors that they believe are most likely to succeed at the task. This allows us to understand how much performance is limited by the low-level policy, with ideal high-level commands.\\
\textbf{GPT-4o high-level model}: This method uses the same high-level/low-level decomposition as \ours, but queries the GPT-4o API-based model for the high level, while using the same low-level policy. GPT-4o is a significantly larger VLM than the one we use, but it is not finetuned with our real and synthetic datasets. This comparison is similar to an advanced version of SayCan~\citep{brohan2023can}, which uses an out-of-the-box LLM as a high-level policy, while this baseline uses a VLM. To align GPT-4o with the robot's affordances, we carefully engineer the prompt to include task-relevant instructions that the low-level policy can follow, determined by ranking the most common skill labels in the human-annotated dataset, and ask GPT-4o to choose among them.\\
\textbf{Flat VLA}: This comparison directly uses the same $\pi_0$ low-level policy as in \ours, but without any high level or synthetic data, representing a state-of-the-art approach for instruction following~\cite{black2024pi_0}.\\
\textbf{Flat VLA with synthetic data}: This ablation uses the $\pi_0$ low-level policy by itself, without a high-level model, but includes the synthetic data in the training data for the low-level policy, such that it can still process the complex prompts used in our evaluation. This baseline allows us to evaluate the benefit of hierarchy independent from the effect of synthetic data.\\
\textbf{\ours without synthetic data}: This ablation corresponds to our method without synthetic training data, evaluating the importance of including diverse synthetically-generated prompts in training. This ablation can be seen as an advanced VLM-based version of YAY Robot~\citep{shi2024yell}, a prior system that uses a high-level model to predict language commands for a low-level model.


\subsection{Metrics and Evaluation Protocol}
We report two complementary metrics, measured by a human evaluator who is blind to the method being run. Each evaluation consists of 20 trials per task per method.

\textbf{Instruction Accuracy (IA).} This score measures how well the high-level policy’s predicted instruction aligns with human intent, requiring multi-modal understanding of the current environment and prompt. If the prediction from the high-level model is consistent with both the user’s command and the current observation, the evaluator marks it as a correct prediction; otherwise, it is labeled as incorrect. The Instruction Accuracy for a trial is then computed as the proportion of correct predictions out of the total number of predictions. For flat baselines, which lack interpretable language predictions, scoring is based on the evaluator's interpretation of the intent of the policy behavior.

\textbf{Task Progress (TP).} Since all tasks we evaluate are complex and long-horizon, we record task progress to provide a granular view of task completion. Task progress quantifies how closely the robot matches the intended goal and is computed by the proportion of objects that are successfully placed in their correct locations or configurations.  
% Each trial ends as soon as either the entire goal is achieved or it becomes clear that the system can no longer recover from an incorrect sequence of actions. 

\subsection{Core Results}


% We first present the results for our full system and two key prior methods: a GPT-4o-based high-level policy and a flat VLA method. We include quantitative results in \myfigref{fig:baselines} and qualitative results in \myfigref{fig:qualitative_comparison}, and we summarize our findings below.
We present results for our system and two key baselines: a GPT-4o policy and a flat VLA method. Quantitative and qualitative results are in \myfigref{fig:baselines} and \myfigref{fig:qualitative_comparison}, and we summarize our findings below.



% \paragraph{(1) \ours demonstrates robust open-ended instruction following and free-form language understanding.} 
% Across all tasks, \ours exhibits substantially higher Instruction Accuracy and Task Progress compared to GPT-4o. For instance, in Table Bussing, when instructed to ``clean only the dishes,'' \ours accurately identifies plates and utensils, places them in the correct bin, and leaves trash items behind—adjusting mid-task to feedback like ``that’s not trash.'' In Sandwich Making, \ours carefully selects ingredients while omitting others upon user request (e.g., ``I’m allergic to pickles''). Similarly, in Grocery Shopping, \ours retrieves multiple items in line with evolving prompts (``I also want some drinks'') and places them correctly in a basket.  
% By contrast, GPT-4o often loses context after the robot starts interacting with objects—issuing nonsensical commands (e.g., ``pick up bermuda triangle'') or sometimes labeling everything as ``plate'' or ``spoon.'' These errors disrupt long-horizon planning and frequently lead to ignoring user constraints. 
\textbf{(1) \ours excels at open-ended instruction following.}
Across all tasks, \ours exhibits substantially higher Instruction Accuracy and Task Progress, compared to GPT-4o and the flat baseline. It properly identifies, picks up, and places the correct items -- even when prompted to handle only certain objects or omit ingredients (e.g., ``I’m allergic to pickles''). In contrast, GPT-4o frequently loses context once physical interaction begins, issuing nonsensical commands (e.g., ``pick up bermuda triangle'') or sometimes labeling everything as ``plate'' or ``spoon,'' which disrupts long-horizon planning.

\textbf{(2) \ours shows strong situated reasoning and adaptation to feedback.} 
% A core requirement of open-ended instruction following is the ability to adapt to user clarifications mid-task. In all three tasks, \ours cleanly incorporates feedback such as ``leave the rest'' or ``I want more lettuce,'' issuing updated commands that adjust the ongoing plan. For example, in Grocery Shopping, if the user changes the request from ``grab me some chips'' to ``I also want a KitKat,'' \ours smoothly integrates the extra item. GPT-4o, on the other hand, struggles to maintain a coherent internal state of what the robot is holding, often attempting to pick up new objects when the gripper is occupied, or abruptly switching to ``put away'' commands before an item is secured. 
% The human expert baseline naturally handles these corrections and clarifications, providing an upper bound on success rates.
When users modify requests mid-task (e.g., ``leave the rest,'' ``I also want a KitKat''), \ours updates low-level commands accordingly. GPT-4o, however, often fails to maintain a coherent internal state, leading to commands like picking up new objects when the gripper is still occupied or prematurely switching tasks. The flat baseline, on the other hand, does not react to real-time feedback.

% \paragraph{(3) \ours is effective across diverse tasks, robot embodiments, and user constraints.}
% We evaluate \ours on single-arm and dual-arm setups (Table Bussing, Sandwich Making) and on a mobile bimanual robot (Grocery Shopping). In each setup, \ours consistently outperforms GPT-4o in identifying, picking up, and placing physically distinct objects—from fragile cheese slices to tall beverage bottles—while respecting user-defined constraints. Further, \ours handles unexpected queries like ``bus only the yellowish items'' or ``don’t add tomatoes'' without requiring a new round of training. By contrast, the flat baseline and GPT-4o often revert to default behaviors (e.g., picking up every object in sight, or including all ingredients in a sandwich) when the prompt changes mid-episode.
\textbf{(3) \ours is effective across diverse tasks, robots, and user constraints.}
On single-arm, dual-arm, and mobile bimanual platforms, \ours is able to handle distinct objects (from fragile cheese slices to tall bottles) while respecting dynamic constraints (e.g., ``bus only yellowish items,'' ``don’t add tomatoes''). By contrast, the flat baseline and GPT-4o often revert to default behaviors (e.g., picking up every object in sight, or including almost all ingredients in a sandwich) when the prompt changes mid-episode.

\textbf{(4) Expert human guidance reveals the low-level policy’s strengths but underscores the need for high-level reasoning.}  
% When guided by human-provided high-level instructions, the low-level policy executes tasks nearly flawlessly, demonstrating its robustness. Errors stem primarily from inadequate high-level reasoning rather than physical limitations. However, relying solely on human high-level input is not scalable. \ours bridges the gap to oracle performance by leveraging a high-level VLM that better aligns with user prompts and real-time observations. In contrast, GPT-4o’s lack of physical grounding degrades performance, while the flat baseline's absence of high-level reasoning leads to poor alignment.
With human high-level instructions, the low-level policy executes nearly flawlessly, showing that failures stem more from reasoning than actuation. However, solely relying on human input is not scalable. \ours bridges this gap via a high-level VLM that aligns with user prompts and real-time observations, whereas GPT-4o’s lack of physical grounding and the flat baseline’s lack of high-level reasoning hinder performance.


\subsection{Ablation Studies}

We conduct two key ablations to isolate the contributions of (1) synthetic data for high-level reasoning, and (2) hierarchical decomposition vs. a single ``flat'' policy.

\begin{figure}[t]
    \centering
    % \includegraphics[width=\linewidth]{figures/ablations_data.png}
    % \includegraphics[width=\linewidth]{figures/ablation_data_v2.png}
    \includegraphics[width=\linewidth]{figures/ablation_data_v2.pdf}
    \vspace{-8mm}
    \caption{\textbf{Ablation on synthetic data.} Synthetic data is essential for handling open-ended instructions, as the model trained without it struggle with user-driven deviations, failing to integrate clarifications and constraints, whereas \ours adapts seamlessly by leveraging diverse, compositional language prompts. (IA = Instruction Accuracy, TP = Task Progress)}
    % \caption{\textbf{Ablation on Synthetic Data.} Models trained without synthetic data fail to incorporate clarifications and constraints in open-ended instructions. In contrast, \ours adapts seamlessly using diverse language prompts. (IA=Instruction Accuracy, TP=Task Progress)}
    % \caption{\textbf{Ablation on Synthetic Data.} Without synthetic data, models struggle to incorporate clarifications and constraints in open-ended instructions, while \ours seamlessly adapts using diverse language prompts. (IA = Instruction Accuracy, TP = Task Progress)}
    \label{fig:ablations_data}
    \vspace{-4mm}
\end{figure}

\textbf{(A) Synthetic data is critical for open-ended instruction following.}
% As shown in \myfigref{fig:ablations_data}, we compare \ours (trained on both human-labeled data and synthetic conversation traces) to \ours without synthetic data (trained only on human-labeled data). Without the diverse, compositional language prompts supplied by synthetic data, the ablated model struggles with user-driven deviations from its training distribution. In Table Bussing, it ignores clarifications like ``this is not trash'' and continues to discard all items. In Sandwich Making, it routinely includes forbidden ingredients (e.g., pickles) despite explicit user warnings. In contrast, \ours integrates these corrections seamlessly because synthetic examples expose the high-level VLM to a wider variety of requests (e.g., partial clearing, negative instructions, conditional modifications), building the language flexibility needed to adapt in real time.
Comparing \ours (trained on human-labeled + synthetic data) to a variant trained solely on human-labeled data shows that synthetic interactions significantly boost language flexibility (\myfigref{fig:ablations_data}). Without them, the ablated model ignores clarifications (e.g., ``this is not trash'') or includes forbidden items (e.g., pickles), while \ours smoothly adapts to such feedback, due to the broader coverage of compositional language in synthetic data.

\begin{figure}[t]
    \centering
    % \includegraphics[width=\linewidth]{figures/ablations_hierarchy.png}
    % \includegraphics[width=\linewidth]{figures/ablation_hierarchy_v2.png}
    \includegraphics[width=\linewidth]{figures/ablation_hierarchy_v2.pdf}
    \vspace{-11mm}
    \caption{\textbf{Hierarchical policy vs. flat policy.} The hierarchical approach outperforms the flat variant trained on the same data, as it effectively integrates user feedback and partial instructions, whereas the flat model struggles with mid-task clarifications and nuanced task variations. (IA = Instruction Accuracy, TP = Task Progress)}
    % \caption{\textbf{Hierarchical vs. Flat Policy.} Our hierarchical approach better integrates user feedback and partial instructions, while the flat variant struggles with mid-task clarifications and nuanced task variations. (IA=Instruction Accuracy, TP=Task Progress)}
    \label{fig:ablations_hierarchy}
    \vspace{-6mm}
\end{figure}

% \paragraph{(B) Hierarchical structure outperforms a flat policy for situated reasoning and task generalization.}
% We next compare \ours (hierarchical) against a flat policy with synthetic data, which receives the same textual variety but directly outputs motor actions without an explicit deliberation step (\myfigref{fig:ablations_hierarchy}). The flat model sometimes ignores user feedback mid-task, such as ``leave the rest.'' It also struggles with partial instructions like ``bus only the yellowish things,'' often reverting to fully clearing the workspace. \ours, by contrast, re-checks the prompt in every high-level inference step, enabling coherent updates in response to dynamic inputs.  
% This suggests that a hierarchical design that separates language/vision reasoning from motion control better maintains multi-step coherence, handles live user clarifications, and supports more nuanced task variations.
\textbf{(B) Hierarchical structure outperforms a flat policy.} We next compare \ours to a flat policy trained on the same synthetic data but without a separate reasoning step (\myfigref{fig:ablations_hierarchy}). The flat model often reverts to clearing all items or fails to handle partial instructions (``bus only the yellowish things''), whereas \ours re-checks the prompt at each high-level step and responds coherently to mid-task updates. This suggests separating high-level reasoning from low-level control is benficial for multi-step coherence and adapting to dynamic user inputs.


\section{Discussion and Future Work}

We presented \ours, a system that uses vision-language models (VLMs) in a hierarchical structure, first reasoning over complex prompts, user feedback, and language interaction to deduce the most appropriate next step to fulfill the task, and then performing that step by directly outputting low-level action commands. Our system can be thought of as a VLM-based instantiation of the ``System 1'' and ``System 2'' architecture~\citep{kahneman2011thinking}. The deliberative ``System 2'' layer takes the form of a high-level VLM policy, which leverages semantic and visual knowledge from web-scale pre-training to reason through complex prompts and user interactions. The physical, reactive ``System 1'' layer also takes the form of a VLM, trained to directly output robot actions in response to simpler commands that describe atomic behaviors.

The two VLMs have nearly identical architectures, with the only difference being that the low-level policy uses flow matching to output the actions. Indeed, the separation of roles at the model level is not fundamental to this design: a natural step for future work is to combine both systems into one model, and draw the ``System 1'' vs ``System 2'' distinction purely at inference time. Future work could also interleave high-level and low-level processing more intricately -- while our system simply runs high-level inference at a fixed but lower frequency, an adaptive system might simultaneously process inputs and language asynchronously at multiple different levels of abstraction, providing for a more flexible multi-level reasoning procedure.

Our system also has a number of limitations that could be studied in future work. While we show that our high-level policy can often break down complex commands into low-level steps that the robot can perform physically, the training process for this high level model relies in some amount of prompt engineering to produce synthetic training examples that induce this behavior. The training process decouples the high-level and low-level models, and they are not aware of one another's capabilities except through the training examples. Coupling these two layers more directly, e.g. by allowing the high-level policy to be more aware of how successfully the low-level policy completes each command, would be an exciting direction for future work. More generally, by instantiating both high-level and low-level reasoning via VLMs, we believe that this design opens the door for much more intricate integration of these components, such that future work might create robotic vision-language-action models that dynamically reason about inputs, feedback, and even their own capabilities to produce suitable situated response in complex open-world settings.

% We presented \ours, a hierarchical system using vision-language models (VLMs) that first deduces the appropriate next steps from complex prompts and user feedback, then outputs low-level action commands. This design can be viewed as a VLM-based version of ``System 1'' and ``System 2''~\citep{kahneman2011thinking}. The deliberative ``System 2'' layer takes the form of a high-level VLM policy, which leverages semantic and visual knowledge from web-scale pre-training to reason through complex prompts and user interactions. The physical, reactive ``System 1'' layer also takes the form of a VLM, trained to directly output robot actions in response to simpler commands that describe atomic behaviors.
% %, where a high-level VLM (System 2) reasons about complex requests, and a reactive VLM (System 1) generates lower-level actions.

% %Both VLMs share similar architectures, except that the low-level policy uses flow matching for action outputs. This separation is not fundamental to this design: one future direction is to merge both systems into a single model, distinguishing ``System 1'' vs. ``System 2'' only at inference. Another direction is tighter integration between high-level and low-level processing, allowing adaptive multi-level reasoning at different time scales.

% Our approach has some limitations. Although the high-level policy can break down complex commands into executable steps, the training process for this high level model relies on some amount of prompt engineering to produce synthetic training examples that induce this behavior. The high-level and low-level models are also decoupled, interacting only through the training examples. More direct coupling -- e.g., allowing the high-level policy to monitor the success of low-level actions -- could improve performance. More generally, by using VLMs for both planning and action, \ours\ paves the way for future vision-language-action models that dynamically reason about inputs, feedback, and their own capabilities to handle complex open-world tasks.

% \section*{Impact Statement}
% This paper presents work where the goal is to advance the fields of machine learning and robotics. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


\section*{Acknowledgments}
We thank Ury Zhilinsky and Kevin Black for their help in setting up the data and training infrastructure. We thank Karol Hausman for valuable feedback and discussions on video demonstration and language-following evaluation. 
We are also grateful to Noah Brown, Szymon Jakubczak, Adnan Esmail, Tim Jones, Mohith Mothukuri, and Devin LeBlanc for their support in robot maintenance.
We appreciate Suraj Nair and Laura Smith for their insightful discussions that helped with policy debugging. We also thank Claudio Guglieri for help in creating visualizations used in this paper and on the project website.
Finally, we extend our deepest gratitude to the entire team of robot operators at Physical Intelligence for their immense contributions to data collection, annotation, and policy evaluations.

\bibliography{references}

\newpage
\include{appendix}

\end{document}
