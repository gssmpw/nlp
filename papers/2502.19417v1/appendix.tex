\appendix

% \section{Data}
\section{Synthetic Data Generation}
\label{sec:data_gen}
\subsection{Scenario and Response Categorization}
To ensure the quality and diversity of the synthetic data, we incorporate structured scenario classification and response categorization into the prompt design for $\polgen$, following~\cite{stephan2024rlvf}. Specifically, we classify interactions into different scenario types, such as \emph{negative task} (where the user instructs the robot what \emph{not} to do),  \emph{situated correction} (where the user adjusts an earlier command based on the evolving task state), and \emph{specific constraint} (where the user specifies particular constraints, such as dietary preferences). In addition, we categorize the robot’s responses into types such as \emph{simple confirmations}, \emph{clarifications}, and \emph{error handling}. These classifications guide the generation process to ensure a broad range of user-robot interactions.

\subsection{Prompt Construction for Contextual Grounding} 
In prompt $\mathcal{P}$, we include a detailed description of the task (e.g., bussing a table, making a sandwich, grocery shopping) and instruct the model to ground responses in visual observations and prior context. A key advantage of leveraging large pretrained VLMs is their ability to incorporate world knowledge when generating interactions. For instance, the model can infer dietary constraints when generating prompts for sandwich-making, producing user commands such as ``Can you make a sandwich for me? I’m lactose intolerant'' and an appropriate robot response like ``Sure, I won’t put cheese on it.'' Similarly, it can reason over ambiguous or implicit requests, such as inferring that ``I want something sweet'' in a grocery shopping scenario should lead to suggestions like chocolate or candy.

To maintain consistency in multi-step tasks, we condition $\polgen$ on prior skill labels within an episode $\cmd_0, ..., \cmd_{t-1}$, allowing it to generate coherent user commands that account for past actions. For instance, if the robot has already placed lettuce and tomato on a sandwich, the generated user prompt might request additional ingredients that logically follow. This ensures that the synthetic interactions reflect realistic task progression rather than isolated commands.
As such, we leverage $\polgen(\ell_t, u_t | \bI^1_t, ... , \bI^n_t, \cmd_0, ..., \cmd_{t-1}, \cmd_t, \mathcal{P})$ to produce a richer, more diverse synthetic dataset $\mathcal{D}_{syn}$ that provides meaningful supervision for training our high-level policy.

While in this work we generate a separate $\mathcal{D}_{syn}$ and train a separate high-level policy for each task (e.g., sandwich making vs.\ table cleaning) for clarity and ease of benchmarking, the architecture is readily amenable to a unified multi-task formulation. In principle, the same hierarchical approach could be used to train a single high-level policy across a multitude of tasks, facilitating knowledge transfer between task domains and more robust, open-ended robot behavior.

\section{System and Robot Overview}

% This describes the robot, robot system, etc. This would be the place to explain microphone input, GPUs, inference timing, etc

Our system integrates speech-based interactions and real-time robotic control. Below, we detail the components of our system, including audio processing, GPU-based inference, and the robot configurations.

\subsection{Perception and Language Processing}

For speech-based interaction, we use a consumer-grade lavalier microphone for audio input. Speech-to-text transcription is handled locally using Whisper large-v2~\citep{radford2023robust}. For text-to-speech synthesis, we employ the Cartetia API to generate natural and expressive speech outputs.

\subsection{Inference Hardware}

To support real-time inference, we utilize one to two NVIDIA GeForce RTX 4090 consumer-grade GPUs. 

\subsection{Robot System Details}

We employ three different robot configurations with various manipulation and mobility capabilities.

\paragraph{UR5e.} This setup features a 6-DoF robotic arm equipped with a parallel jaw gripper. It includes two cameras: a wrist-mounted camera and an over-the-shoulder camera. The system operates within a 7-dimensional configuration and action space.

\paragraph{Bimanual ARX.} This configuration consists of two 6-DoF ARX arms. The system is equipped with three cameras: two wrist-mounted cameras and one base camera. The combined system has a 14-dimensional configuration and action space, enabling dextrous bimanual manipulation tasks.

\paragraph{Mobile ARX.} Built on the Mobile ALOHA~\citep{fu2024mobile} platform, this system integrates two 6-DoF ARX robotic arms mounted on a mobile base. The nonholonomic base introduces two additional action dimensions, resulting in a 14-dimensional configuration space and a 16-dimensional action space. Similar to the bimanual setup, it includes two wrist-mounted cameras and a base camera, providing robust visual feedback for navigation and manipulation.

% \section{Model Implementation Details}
% \label{sec:appendix:implementation}

% Low-level policy

% High-level policy

% Learning rate, warm-up
