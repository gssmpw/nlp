@article{belkhale2024rt,
  title={Rt-h: Action hierarchies using language},
  author={Belkhale, Suneel and Ding, Tianli and Xiao, Ted and Sermanet, Pierre and Vuong, Quon and Tompson, Jonathan and Chebotar, Yevgen and Dwibedi, Debidatta and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2403.01823},
  year={2024}
}

@article{black2024pi_0,
  title={$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control},
  author={Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others},
  journal={arXiv preprint arXiv:2410.24164},
  year={2024}
}

@inproceedings{brohan2023can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others},
  booktitle={Conference on robot learning},
  pages={287--318},
  year={2023},
  organization={PMLR}
}

@article{brohan2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2307.15818},
  year={2023}
}

@article{chen2024automating,
  title={Automating Robot Failure Recovery Using Vision-Language Models With Optimized Prompts},
  author={Chen, Hongyi and Yao, Yunchao and Liu, Ruixuan and Liu, Changliu and Ichnowski, Jeffrey},
  journal={arXiv preprint arXiv:2409.03966},
  year={2024}
}

@article{dai2024racer,
  title={RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning},
  author={Dai, Yinpei and Lee, Jayjun and Fazeli, Nima and Chai, Joyce},
  journal={arXiv preprint arXiv:2409.14674},
  year={2024}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

@article{huang2023voxposer,
  title={Voxposer: Composable 3d value maps for robotic manipulation with language models},
  author={Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li},
  journal={arXiv preprint arXiv:2307.05973},
  year={2023}
}

@article{kim2024openvla,
  title={OpenVLA: An Open-Source Vision-Language-Action Model},
  author={Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others},
  journal={arXiv preprint arXiv:2406.09246},
  year={2024}
}

@article{li2024cogact,
  title={CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation},
  author={Li, Qixiu and Liang, Yaobo and Wang, Zeyu and Luo, Lin and Chen, Xi and Liao, Mozheng and Wei, Fangyun and Deng, Yu and Xu, Sicheng and Zhang, Yizhong and others},
  journal={arXiv preprint arXiv:2411.19650},
  year={2024}
}

@inproceedings{liang2023code,
  title={Code as policies: Language model programs for embodied control},
  author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9493--9500},
  year={2023},
  organization={IEEE}
}

@article{liu2023interactive,
  title={Interactive robot learning from verbal correction},
  author={Liu, Huihan and Chen, Alice and Zhu, Yuke and Swaminathan, Adith and Kolobov, Andrey and Cheng, Ching-An},
  journal={arXiv preprint arXiv:2310.17555},
  year={2023}
}

@inproceedings{liu2024moka,
  title={Moka: Open-vocabulary robotic manipulation through mark-based visual prompting},
  author={Liu, Fangchen and Fang, Kuan and Abbeel, Pieter and Levine, Sergey},
  booktitle={First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024},
  year={2024}
}

@article{liu2024ok,
  title={Ok-robot: What really matters in integrating open-knowledge models for robotics},
  author={Liu, Peiqi and Orru, Yaswanth and Vakil, Jay and Paxton, Chris and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2401.12202},
  year={2024}
}

@article{liu2024rdt,
  title={Rdt-1b: a diffusion foundation model for bimanual manipulation},
  author={Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2410.07864},
  year={2024}
}

@inproceedings{matuszek2013learning,
  title={Learning to Parse Natural Language Commands to a Robot Control System},
  author={Matuszek, Cynthia and Herbst, Evan and Zettlemoyer, Luke and Fox, Dieter},
  booktitle={Experimental Robotics: The 13th International Symposium on Experimental Robotics},
  volume={88},
  pages={403},
  year={2013},
  organization={Springer}
}

@inproceedings{mccallum2023feedback,
  title={Is feedback all you need? Leveraging natural language feedback in goal-conditioned RL},
  author={McCallum, Sabrina and Taylor-Davies, Max and Albrecht, Stefano and Suglia, Alessandro},
  booktitle={NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning}
}

@inproceedings{namasivayam2023learning,
  title={Learning neuro-symbolic programs for language guided robot manipulation},
  author={Namasivayam, K and Singh, Himanshu and Bindal, Vishal and Tuli, Arnav and Agrawal, Vishwajeet and Jain, Rahul and Singla, Parag and Paul, Rohan},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7973--7980},
  year={2023},
  organization={IEEE}
}

@article{nasiriany2024pivot,
  title={Pivot: Iterative visual prompting elicits actionable knowledge for vlms},
  author={Nasiriany, Soroush and Xia, Fei and Yu, Wenhao and Xiao, Ted and Liang, Jacky and Dasgupta, Ishita and Xie, Annie and Driess, Danny and Wahid, Ayzaan and Xu, Zhuo and others},
  journal={arXiv preprint arXiv:2402.07872},
  year={2024}
}

@inproceedings{o2024open,
  title={Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0},
  author={Oâ€™Neill, Abby and Rehman, Abdul and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and others},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={6892--6903},
  year={2024},
  organization={IEEE}
}

@inproceedings{patki2019inferring,
  title={Inferring compact representations for efficient natural language understanding of robot instructions},
  author={Patki, Siddharth and Daniele, Andrea F and Walter, Matthew R and Howard, Thomas M},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={6926--6933},
  year={2019},
  organization={IEEE}
}

@article{pertsch2025fast,
  title={FAST: Efficient Action Tokenization for Vision-Language-Action Models},
  author={Pertsch, Karl and Stachowicz, Kyle and Ichter, Brian and Driess, Danny and Nair, Suraj and Vuong, Quan and Mees, Oier and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:2501.09747},
  year={2025}
}

@article{qiu2024open,
  title={Open-vocabulary mobile manipulation in unseen dynamic environments with 3d semantic maps},
  author={Qiu, Dicong and Ma, Wenzong and Pan, Zhenfu and Xiong, Hui and Liang, Junwei},
  journal={arXiv preprint arXiv:2406.18115},
  year={2024}
}

@article{shah2024bumble,
  title={BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation},
  author={Shah, Rutav and Yu, Albert and Zhu, Yifeng and Zhu, Yuke and Mart{\'\i}n-Mart{\'\i}n, Roberto},
  journal={arXiv preprint arXiv:2410.06237},
  year={2024}
}

@article{shi2024yell,
  title={Yell at your robot: Improving on-the-fly from language corrections},
  author={Shi, Lucy Xiaoyang and Hu, Zheyuan and Zhao, Tony Z and Sharma, Archit and Pertsch, Karl and Luo, Jianlan and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2403.12910},
  year={2024}
}

@inproceedings{singh2023progprompt,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={11523--11530},
  year={2023},
  organization={IEEE}
}

@article{singh2024lgr2,
  title={LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning},
  author={Singh, Utsav and Bhattacharyya, Pramit and Namboodiri, Vinay P},
  journal={arXiv preprint arXiv:2406.05881},
  year={2024}
}

@article{stone2023open,
  title={Open-world object manipulation using pre-trained vision-language models},
  author={Stone, Austin and Xiao, Ted and Lu, Yao and Gopalakrishnan, Keerthana and Lee, Kuang-Huei and Vuong, Quan and Wohlhart, Paul and Kirmani, Sean and Zitkovich, Brianna and Xia, Fei and others},
  journal={arXiv preprint arXiv:2303.00905},
  year={2023}
}

@inproceedings{swadzba2009computational,
  title={A computational model for the alignment of hierarchical scene representations in human-robot interaction},
  author={Swadzba, Agnes and Vorwerg, Constanze and Wachsmuth, Sven and Rickheit, Gert},
  booktitle={Twenty-First International Joint Conference on Artificial Intelligence},
  year={2009},
  organization={Citeseer}
}

@article{wang2024llm,
  title={LLM\^{} 3: Large Language Model-based Task and Motion Planning with Motion Failure Reasoning},
  author={Wang, Shu and Han, Muzhi and Jiao, Ziyuan and Zhang, Zeyu and Wu, Ying Nian and Zhu, Song-Chun and Liu, Hangxin},
  journal={arXiv preprint arXiv:2403.11552},
  year={2024}
}

@article{wen2024tinyvla,
  title={Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation},
  author={Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and others},
  journal={arXiv preprint arXiv:2409.12514},
  year={2024}
}

@article{xiao2024robi,
  title={Robi Butler: Remote Multimodal Interactions with Household Robot Assistant},
  author={Xiao, Anxing and Janaka, Nuwan and Hu, Tianrun and Gupta, Anshul and Li, Kaixin and Yu, Cunjun and Hsu, David},
  journal={arXiv preprint arXiv:2409.20548},
  year={2024}
}

@article{zawalski2024robotic,
  title={Robotic control via embodied chain-of-thought reasoning},
  author={Zawalski, Micha{\l} and Chen, William and Pertsch, Karl and Mees, Oier and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:2407.08693},
  year={2024}
}

@article{zheng2025universal,
  title={Universal Actions for Enhanced Embodied Foundation Models},
  author={Zheng, Jinliang and Li, Jianxiong and Liu, Dongxiu and Zheng, Yinan and Wang, Zhihao and Ou, Zhonghong and Liu, Yu and Liu, Jingjing and Zhang, Ya-Qin and Zhan, Xianyuan},
  journal={arXiv preprint arXiv:2501.10105},
  year={2025}
}

@article{zhi2024closed,
  title={Closed-loop open-vocabulary mobile manipulation with gpt-4v},
  author={Zhi, Peiyuan and Zhang, Zhiyuan and Han, Muzhi and Zhang, Zeyu and Li, Zhitian and Jiao, Ziyuan and Jia, Baoxiong and Huang, Siyuan},
  journal={arXiv preprint arXiv:2404.10220},
  year={2024}
}

