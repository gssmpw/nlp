\documentclass[manuscript, screen]{acmart}
\input{utils}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}






\begin{document}
\input{aliases}

\title{\tool: Scaffolding Spreadsheet Development with a Language Agent}

\author{Jenny T. Liang}
\email{jtliang@cs.cmu.edu}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{PA}
  \country{USA}
}

\author{Aayush Kumar}
\email{t-aaykumar@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Banglore}
  \country{India}
}

\author{Yasharth Bajpai}
\email{ybajpai@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Banglore}
  \country{India}
}


\author{Sumit Gulwani}
\email{sumitg@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \city{WA}
  \country{USA}
}

\author{Vu Le}
\email{levu@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \city{WA}
  \country{USA}
}

\author{Chris Parnin}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \city{WA}
  \country{USA}
}
\email{chrisparnin@microsoft.com}

\author{Arjun Radhakrishna}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \city{WA}
  \country{USA}
}
\email{arradha@microsoft.com}

\author{Ashish Tiwari}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \city{WA}
  \country{USA}
}
\email{ashish.tiwari@microsoft.com}

\author{Emerson Murphy-Hill}
\authornote{Denotes equal contribution. Author names are listed alphabetically.}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \city{WA}
  \country{USA}
}
\email{emerson.rex@microsoft.com}

\author{Gustavo Soares}
\authornotemark[1]
\email{gustavo.soares@microsoft.com}
\affiliation{%
  \institution{Microsoft}
  \city{Redmond}
  \city{WA}
  \country{USA}
}

\renewcommand{\shortauthors}{Liang et al.}

\begin{abstract}
Despite spreadsheet programming being an ubiquitous activity in the modern workforce, creating spreadsheets remains challenging.
This is in part due to two reasons: first, programmers must have spreadsheet programming knowledge (e.g., API understanding) to perform fundamental actions like writing formulas, and second, they must develop problem-solving knowledge to compose these actions together to achieve complex tasks.
\added{
Given these challenges, approaches that automate aspects of the spreadsheet development process, such as large language models (LLMs), show promise in alleviating the challenges these programmers face.
}
Recent advances in planning and reasoning in LLMs have enabled a more advanced approach known as \emph{language agents} to solve spreadsheet programming tasks.
Compared to traditional LLMs that generate text, language agent-based (i.e., agentic) approaches emphasize dynamic planning, tool use, and iterative action-taking to complete complex tasks in interactive environments. 
Agents achieve a specific goal by observing the environment and planning and proactively performing predefined actions on the environment.
Thus, we reason that language agents can scaffold programmers through creating spreadsheets by following expert processes.

We present \tool, a language agent that allows spreadsheet programmers to build spreadsheets in a conversational manner.
\added{
\tool's design reifies three design principles for tools to support spreadsheet programming------which we derive from two formative studies of 7 spreadsheet programmers and 62 Excel spreadsheet templates.
}
\tool enables the creation of spreadsheet programs by guiding programmers through a structured plan to develop spreadsheets, based on expert processes.
During each interaction with \tool, the tool generates three potential next steps \added{the user can select to adapt the plan to their context}.
In addition, \tool has access to tools that produce atomic spreadsheet features so that \tool can develop spreadsheets incrementally with the programmer. 
\added{
Based on a user study with 20 spreadsheet programmers, we find that our approach produces higher quality spreadsheets that are 2.3 times more likely to be preferred by other programmers compared to a \baseline. 
It also reduces the cognitive load of spreadsheet programmers and the amount of time spent thinking about spreadsheet programming actions by 12.6\% compared to the baseline.
}
\tool has implications on the design of human-agent collaboration systems. 
This includes providing persistent direct manipulation interfaces for stopping or undoing agent actions, while ensuring that such interfaces for accepting actions can be deactivated.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003129</concept_id>
       <concept_desc>Human-centered computing~Interactive systems and tools</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10011007.10011006.10011050.10011023</concept_id>
       <concept_desc>Software and its engineering~Specialized application languages</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
  <concept>
       <concept_id>10010147.10010178.10010199</concept_id>
       <concept_desc>Computing methodologies~Planning and scheduling</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Interactive systems and tools}
\ccsdesc[500]{Software and its engineering~Specialized application languages}
\ccsdesc[300]{Computing methodologies~Planning and scheduling}

\keywords{Spreadsheet programming, language agents, human-agent collaboration}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle

\begin{figure}[t!]
\centering
\includegraphics[trim=0 350 200 0, clip, width=\linewidth, keepaspectratio]{figures/intro-figure.pdf} 
\caption{
\added{A screenshot of the \tool interface.}
}
\label{fig:intro}
\end{figure}

\section{Introduction}
\label{sec:introduction}

\scenario{
Alma is creating a spreadsheet to track expenses to understand the spending trends of her business with her co-owner.
Alma is unsure on how to solve this problem, so she begins by defining a table whose schema represents the information she wants to track. 
This includes columns for the transaction date, cost, and spending category.
Then, she manually tabulates the data from various sources, such as bank statements or receipts.
Alma now wants to understand her top spending categories, which requires completing multiple steps.
First, Alma must produce a second table by filtering rows from the past month, aggregating the rows by spending category, summing their costs, and saving the results in a second table.
She then must sort the second table by the total cost column.
However, Alma does not know how to decompose the problem into these steps.
She also struggles to implement the formula due \added{to having difficulty understanding complex spreadsheet programming APIs}, and eventually gives up on implementing the spreadsheet.
}

\added{
Like Alma, many professionals develop spreadsheet programs~\cite{scaffidi2005estimating, abraham2008spreadsheet, ko2011state}, from administrators and retail managers~\cite{glass2024skills} to teachers and analysts~\cite{ko2011state}.
We refer to these professionals as \emph{spreadsheet programmers}.
}
Spreadsheet programming is an important skill in the modern workforce~\cite{scaffidi2005estimating, scaffidi2017workers, glass2024skills}.
Based on a 2023 longitudinal analysis of millions of job postings, spreadsheet programming remains a significant and enduring skill~\cite{glass2024skills}.
Despite the prevalence of spreadsheet programs, creating these programs is challenging.
As illustrated by Alma's experience, what makes spreadsheet programming challenging is two-fold.
Firstly, spreadsheet programmers must have a background in programming ~\cite{nardi1991twinkling} and prerequisite understanding of APIs~\cite{ko2011state} to perform actions.
Secondly, spreadsheet programmers must have problem-solving knowledge to understand how to approach the problem and decompose the task into individual actions that achieve the intended goal. 
Programmers often achieve this by developing and following systematic processes~\cite{pirolli2005sensemaking, latoza2020explicit, arab2022exploratory}.
\added{
Without adequate support for these kinds of knowledge, spreadsheet programmers struggle to understand and manage spreadsheets~\cite{reschenhofer2015empirical} and introduce errors into the program~\cite{abraham2008spreadsheet}.
}

The emergence of large language models (LLMs), such as GPT-4~\cite{achiam2023gpt}, offer a promising avenue to assist spreadsheet programmers as they address both of the aforementioned challenges.
These models have sufficient spreadsheet programming knowledge to complete individual spreadsheet programming tasks (e.g., formula prediction~\cite{joshi2024flame, chen2021spreadsheetcoder}).
Further, LLMs can solve complex tasks via strong planning capabilities developed through advanced reasoning techniques~\cite{shinn2024reflexion, yao2024tree}, which have enabled LLM-based \emph{language agents} to become effective planners and problem solvers for spreadsheet programming~\cite{li2024sheetcopilot}. 
Compared to traditional LLMs, which focus on generating text, language agents can also collect observations of the environment and plan and perform actions to update their environment~\cite{andreas2022language, wang2024survey, xi2023rise}.

\added{Humans and language agents assume complementary roles in completing tasks.}
Language agents can scaffold and guide humans through complex tasks by following expert processes~\cite{sun2024reviewflow, bajpai2024lets}.
At the same time, research shows that language agents benefit from human intelligence, such as learning from human problem-solving processes~\cite{wang2024survey, bajpai2024lets}.
Therefore, humans could benefit language agents by improving the agent's planning and task performance via human expertise, while language agents could guide humans through expert processes to complete complex tasks the individual would struggle with on their own.
This motivates the need to study human-agent interaction from a planning and problem-solving perspective, which remains understudied.
Better understanding how humans and language agents interact could improve their alignment and could also inform our understanding of usable agent systems, which is vital for useful and practical tools~\cite{myers2016programmers}.
This becomes increasingly important given the growing capabilities of agents to complete a wider range of tasks (e.g., software engineering~\cite{li2024sheetcopilot, yang2024swe} or real-life scenarios~\cite{xi2023rise}) \added{and the adoption of agents in programming systems like Devin~\cite{devin2025devin}.}

In this work, we explore the potential for collaboration between language agents and humans to plan and solve complex tasks together.
We use spreadsheet programming as a domain to study human-agent collaboration, given the ability of language agents to perform the task~\cite{li2024sheetcopilot}; the semi-structured nature of the task~\cite{pirolli2005sensemaking}, indicating the need for planning and problem solving by spreadsheet programmers; and the difficulty of the task for humans~\cite{ko2011state}.
We explore this idea with our tool, \tool (see Figure~\ref{fig:intro}).
\added{\tool implements three design principles for tools to support spreadsheet programming---scaffolding, flexibility, and incrementality.
We derive these principles by understanding the design space of scaffolding tools and how developers use them through a study of 62 spreadsheet templates and a user study with 7 spreadsheet programmers respectively.}
\tool supports the creation of spreadsheet programs by having the programmer participate in the planning process and provide feedback on potential table schema through a chat interface.
\tool is implemented as a language agent that 1) follows a plan for effective spreadsheet creation identified by \citet{pirolli2005sensemaking} and 2) has access to tools to implement atomic spreadsheet features, enabling the incremental development of spreadsheets.
In each interaction with \tool, the tool generates three potential next steps that the programmer can select to adapt the plan to their context.
\added{We demonstrate the benefits of the design principles via a user study with 20 participants.} 
We find that \tool produces higher-quality spreadsheets that are 2.3 times more likely to be preferred by other spreadsheet programmers compared to \added{a \baseline's}.
\added{
It also enables better conversation, reduces cognitive load, and decreases the amount of time spent thinking about spreadsheet programming actions by 1.9 minutes (112 seconds)---12.6\% of the total task time---compared to the baseline.
}

In summary, the contributions of this work are:
\begin{itemize}
    \item \added{A list of design principles (Section~\ref{sec:design-goals}) for tools to support spreadsheet programming based on a formative study of 62 spreadsheet templates (Section~\ref{sec:spreadsheet-template-analysis}) and a user study with 7 spreadsheet programmers (Section~\ref{sec:formative-user-study});}
    \item \tool, a novel system that \added{reifies these principles} to provide iterative, flexible, and scaffolded guidance for programmers to build spreadsheets in Excel with a language agent (Section~\ref{sec:tool});
    \item Insights on AI-assisted spreadsheet programming and human-agent collaboration via an evaluation of \tool with 20 spreadsheet programmers (Section~\ref{sec:evaluation-user-study}); and
    \item Design guidelines and recommendations for future human-agent collaborative systems (Section~\ref{sec:discussion}).
\end{itemize}

\section{Related Work}
\added{Below, we discuss prior work on human factors of spreadsheet programming (Section~\ref{sec:human-factors-of-spreadsheet-programming}), scaffolding tools for programming tasks (Section~\ref{sec:scaffolding-processes}), and AI for spreasheet programming (Section~\ref{sec:llm-agents}).
}
Given the rapid advances in LLMs, our discussions offer a snapshot of the field as of December 2024.

\subsection{Human Factors of Spreadsheet Programming}
\label{sec:human-factors-of-spreadsheet-programming}
Prior work has investigated the process that spreadsheet programmers follow to create spreadsheets, revealing its complex and iterative nature.
In a field study of 11 spreadsheet programmers, \citet{nardi1991twinkling} found that spreadsheet development was collaborative by nature and required both programming and domain expertise.
This expertise was often distributed among multiple individuals who had different specialties.
Closely related is \citet{pirolli2005sensemaking}'s cognitive task analysis, which found that professionals who developed knowledge products from data like spreadsheets followed a defined process.
This involved gathering information, developing a schema for analysis, generating insights by manipulating the schema, and creating some knowledge product or action.
Further, the authors found that the expert process involved a foraging loop (i.e., finding, searching, filtering, and extracting information) and a sense making loop (i.e., developing a mental model for a schema).

Other studies have revealed the challenges associated with spreadsheet development.
Spreadsheet programmers often struggle to find the correct abstractions to use and to know how to reuse existing code to accomplish a task~\cite{ko2011state}.
In addition, spreadsheet programs can be difficult to understand, extend, and manage~\cite{reschenhofer2015empirical}. 
As a result, they can be riddled with errors, including mechanical and logic errors~\cite{abraham2008spreadsheet}.
Although automating aspects of spreadsheet programming could be helpful~\cite{reschenhofer2015empirical}, it can also come at a cost to the programmer.
\citet{pandita2018no} found that there was no clear benefit in using tools in spreadsheet programming: while tool usage could increase task correctness, it could also result in longer task completion times.

The prior literature on human factors of spreadsheet programming reveals rich insights on how programmers develop spreadsheets.
However, these works do not study how spreadsheet programmers use scaffolding tools like spreadsheet templates, which is vital to understanding how AI can guide spreadsheet development.
We corroborate and extend the findings of these studies by examining the benefits and challenges of using scaffolding tools in spreadsheet development in our formative user study (see Section~\ref{sec:formative-user-study}).
In addition, we leverage the insights from these studies in the design principles (see Section~\ref{sec:design-goals}) and implementation of \tool (see Section~\ref{sec:tool}),
such as having the agent follow the process from \citet{pirolli2005sensemaking} for spreadsheet programmers.

\subsection{Scaffolding Tools for Programming}
\label{sec:scaffolding-processes}
Scaffolding tools help users complete complex tasks by structuring tasks in a way such that they can accomplish tasks that they struggle to attain on their own~\cite{reiser2018scaffolding}.
Examples of scaffolding tools include templates, examples of other work, hints, or links to resources~\cite{saye2002scaffolding}, as well as step-by-step instructions to complete a task~\cite{latoza2020explicit}.
In programming, scaffolding tools can help programmers with problem solving and planning. 
They have been applied to a variety of complex programming tasks, such as decision making~\cite{liu2019unakite}, debugging~\cite{bajpai2024lets, latoza2020explicit}, software architecture, and testing~\cite{arab2021howtoo}.
However, scaffolding tools struggle to offer the right level of support by being too general or too specific~\cite{reiser2018scaffolding}.
For example, when a programmer follows expert-defined instructions to solve programming problems---known as programming strategies~\cite{latoza2020explicit}---it can be difficult to apply the strategy because it may not consider the programmer's unique context or be written too abstractly~\cite{arab2022exploratory}.

LLMs have shown promise in alleviating these challenges by imbuing additional flexibility and user context into scaffolding tools for complex tasks, such as for the academic peer review process~\cite{sun2024reviewflow} and information sense making~\cite{suh2023sensecape, liu2024selenite}.
In programming, LLM-based scaffolding approaches have shown promise in helping programmers.
The most closely related tool to \tool is ROBIN~\cite{bajpai2024lets}, a tool that guides programmers through a predefined debugging programming strategy.
ROBIN is implemented with four main LLM-based components, including one for generating follow-ups in the conversation.
With this approach, ROBIN led to a 150\% increase in defect localization rates and a 250\% increase in defect resolution rates compared to the baseline.

Overall, this body of literature points to the promise of scaffolding tools assisting spreadsheet programmers in creating spreadsheets.
However, to the best of our knowledge, scaffolding tools for spreadsheet development remain understudied.
This is because, rather than guiding programmers through complex tasks, many intelligent spreadsheet tools have focused on developing improved interfaces, algorithms, or machine learning techniques for a range of spreadsheet development tasks.
This includes writing formulas~\cite{srinivasa2022gridbook}, prototyping tables~\cite{huang2024table}, debugging~\cite{abraham2007goaldebug, myers1991graphical} as well as defining test cases~\cite{burnett2002testing}, assertions~\cite{burnett2003end}, and constraints~\cite{myers1991graphical}.
Of these tools, perhaps the most related to \tool is Table Illustrator~\cite{huang2024table} and GridBook~\cite{srinivasa2022gridbook}.
Table Illustrator~\cite{huang2024table} is an interactive system that facilitates the rapid prototyping of different table layouts by allowing programmers to arrange puzzle pieces as a metaphor for table construction.
In the Table Illustrator user study, \citet{huang2024table} found that the tool performed similarly to Excel, while decreasing cognitive load and task completion time.
Meanwhile, GridBook~\cite{srinivasa2022gridbook} is an interactive system that allows programmers to provide input in natural language within a spreadsheet cell and uses deep learning techniques to generate formulas.
In the GridBook user study, \citet{srinivasa2022gridbook} found that the tool performed comparably to Excel, but reduced task completion time compared to Jupyter notebooks.

However, some work has studied scaffolding spreadsheet programming, namely, detecting and producing spreadsheet templates~\cite{abraham2004header, abraham2006inferring, abraham2005visual, erwig2006gencel}.
ViTSL~\cite{abraham2005visual} is a visual programming language that defines abstractions for the spreadsheet structure to reduce spreadsheet programming errors.
It offers abstractions on cells, references, vertical groups, and horizontal groups.
Gencel~\cite{erwig2006gencel}, an Excel add-on, then generates and edits tables that conform to the ViSTL template structure.
In Gencel's user study, \citet{erwig2006gencel} found that the participants wanted more flexibility in ViSTL templates and expressed concern in being able to adapt ViSTL templates due to the required programming knowledge.

To our knowledge, \added{\tool is the first conversational tool that uses LLM language agents to scaffold the spreadsheet creation process}.
Compared to Table Illustrator~\cite{huang2024table} and GridBook~\cite{srinivasa2022gridbook}, \tool follows a scaffolding approach to develop spreadsheets. 
Unlike GridBook, \tool uses conversation rather than annotations in spreadsheet cells to develop speadsheets and further, can manipulate spreadsheet presentation (e.g., themes) in addition to complex formulas; unlike Table Illustrator, \tool uses state-of-the-art LLMs to create spreadsheets and supports the development of complex formulas, rather than being limited to simple summations.
Next, we build on previous work on spreadsheet programming scaffolding tools, such as ViSTL~\cite{abraham2005visual} and Gencel~\cite{erwig2006gencel}, by using language agents to provide additional flexibility in the spreadsheet creation process.
Finally, we extend the existing body of literature on LLM scaffolding tools such as ROBIN~\cite{bajpai2024lets} by studying the collaboration between programmers and agents that can modify the development environment, which is becoming increasingly important with the adoption of agentic approaches in practice with tools like Devin~\cite{devin2025devin}. 

\subsection{AI for Spreadsheet Programming}
\label{sec:llm-agents}
Prior work has investigated multiple approaches for LLMs to solve spreadsheet programming tasks.
Previously, LLMs solved a range of self-contained spreasheet programming tasks through pre-training and fine-tuning, such as formula prediction~\cite{joshi2024flame, chen2021spreadsheetcoder} and cell role prediction~\cite{du2021tabularnet}.
Recently, a budding approach has allowed LLMs to complete more complex tasks and increase task performance in spreadsheet programming: \emph{language agents}, also known as agents.
Agents achieve a specific goal by observing the environment, performing actions, generating utterances, modeling internal state, and inferring intentions from others~\cite{andreas2022language, xi2023rise, wang2024survey}.
Compared to traditional LLMs, which focus on generating text autoregressively (i.e., from left to right) given the previous text, agents put special emphasis on dynamic planning and proactive action-taking within an environment to complete tasks.
Following the trend of using language agents to achieve state-of-the-art performance in software development tasks (e.g., closing issues~\cite{yang2024swe} and generating code and tests~\cite{tufano2024autodev}), \citet{li2024sheetcopilot} developed an agent to manipulate spreadsheets called SheetCopilot. 
SheetCopilot can understand spreadsheet manipulation requests in natural language and construct and execute plans to achieve the request.
Based on a benchmark of 221 spreadsheet programming tasks, SheetCopilot completed 44.3\% of the tasks in a single generation.

Although these advances have enabled state-of-the-art performance, agentic systems do not typically consider the role of human interaction, as they restrict human involvement to providing annotations on agent actions~\cite{xi2023rise} or limiting interactions to the beginning of the task rather than throughout the task (e.g.,~\cite{yang2024swe, li2024sheetcopilot}).
This reduces the usability of agent-based systems, as these approaches violate long-established principles in human-AI collaboration that underscored the importance of granular user feedback~\cite{amershi2019guidelines} and the refinement of AI output~\cite{horvitz1999principles}.
Thus, compared to other agentic approaches, \tool explores human-agent collaboration by being designed to work interactively with spreadsheet programmers to perform their tasks.
Rather than trying to complete a task in a single user interaction as in previous work~\cite{li2024sheetcopilot, yang2024swe, tufano2024autodev}, \tool scaffolds the spreadsheet creation process and allows programmers---rather than models---to provide feedback on intermediate states and evaluate multiple next steps for planning.

\section{Spreadsheet Template \added{Study}}
\label{sec:spreadsheet-template-analysis}
\added{
To better understand existing scaffolding tools in spreadsheet programming, we analyzed 62 Excel spreadsheet templates.
We studied spreadsheet templates because they are tools to scaffold the spreadsheet creation process for programmers.
}
Therefore, \added{to understand spreadsheet templates analytically}, our research question in this study is:

\begin{description}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{2pt}
    \item[\textbf{RQ1}] What is the design space of Excel spreadsheet templates?
\end{description}

\label{sec:template-analysis}
\added{
We complement \citet{huang2024table}'s analysis on the design space of LaTeX and Microsoft Word tables by studying Excel spreadsheet templates.
Through a broad sample of 2,500 tables across a variety of domains, their analysis revealed rich insights into the different physical aspects of individual tables, including the structure and style of tables.
Our study extends this work by focusing on templates from a singular domain to understand the diverse problem-solving approaches that programmers employ to build spreadsheets for similar problems.
}
Therefore, to answer RQ1, we gathered a dataset of budget-related Excel spreadsheet templates (Section~\ref{sec:template-analysis-data}) and qualitatively analyzed it (Section~\ref{sec:template-analysis-analysis}). 
We used the results (Section~\ref{sec:template-analysis-results}) of this study to inform the design principles of \tool and understand what spreadsheet components to generate.

\subsection{Data}
\label{sec:template-analysis-data}
For this study, we analyzed Excel templates since they are widely accessible scaffolding tools for spreadsheet programmers and are a sample of spreadsheets of higher quality and intended for public use.
These spreadsheets were representative of what we wanted \tool to produce.
The budget domain was selected \added{as the study domain} as it was a prominent predefined category of spreadsheet templates in the Excel application.

To obtain the Excel template data set, \added{we used the Excel template library from Excel version 2409} and selected  the templates under the "Budget" filter.
This yielded 91 templates.
Because this set included spreadsheets that were not purely budgets (e.g., planners), we further filtered this set down by only looking at templates that contained the keyword "budget".
This produced the final dataset of 62 spreadsheets, which were then manually extracted for analysis.
The dataset is available in the supplemental materials~\cite{supplemental-materials}.

\subsection{Analysis}
\label{sec:template-analysis-analysis}
To analyze the Excel template dataset, two authors performed open coding on the dataset.
The authors independently coded 15 spreadsheets (approximately 25\% of the samples) for common atomic spreadsheet features and inductively generated codes in an individual codebook.
The authors then gathered to merge their individual codebooks.
Codes with similar themes were first merged.
Each of the remaining codes were added or removed based on unanimous agreement. 
The authors then grouped the codes into similar themes in a round of axial coding.
Each category was created only by unanimous agreement.

To validate the codebook, one author coded an additional spreadsheet by \added{selecting areas within the spreadsheet and applying a code from the shared codebook}. 
The codes were then masked, with the original highlighted areas corresponding to the code remaining intact.
The other author then recoded the same spreadsheets by applying the shared codebook.
We then computed an inter-rater reliability (IRR) score using Cohen's $\kappa$, which yielded a score of 0.87---near perfect agreement~\cite{landis1977measurement}.
After the codebook was validated, the first author coded the remaining spreadsheets.
No new codes were discovered in this process.
The codebook is available in the supplemental materials~\cite{supplemental-materials}.

\begin{figure}[t!]
\centering
\includegraphics[trim=0 250 725 0, clip, width=0.85\linewidth, keepaspectratio]{figures/template-results-figure.pdf} 
\caption{
An overview of the themes and codes of Excel spreadsheet template features from the spreadsheet template study.
The requirements of the spreadsheet (left) influence its physical structure (middle), defining the architecture and data organization.
This structure, in turn, serves as the foundation for its usability (right).
The themes associated with the requirements, structure, and usability are \underline{underlined}, with their corresponding codes listed directly below.
}
\label{fig:template-results-figure}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[trim=0 25 700 0, clip, width=\linewidth, keepaspectratio]{figures/template-figure.pdf} 
\caption{
An example Excel budget spreadsheet template.
This template contains one \ilabel{1} \theme{insight table}{\faLightbulb[regular]}, which summarizes the data in the sheet, and two \ilabel{2} \theme{data tables}{\faDatabase} that define the underlying data within the sheet. 
Within the data table, the last row is an \ilabel{5} \formativecode{aggregation cell}, which compiles the data within the single table.
Within the insight table, each row contains an \ilabel{3} \formativecode{aggregation reference cell} linking to the category's overall total via the aggregation cell from the corresponding data table.
The aggregation reference cells are then aggregated in the \ilabel{4} \formativecode{table aggregation cell} to summarize the costs across all the tables in the sheet.
}
\label{fig:template}
\end{figure}

\subsection{Results}
\label{sec:template-analysis-results}
Based on the qualitative analysis, we extracted 21 codes on common atomic features from Excel spreadsheet templates that were grouped into 7 different themes.
\added{
We provide an overview in Figure~\ref{fig:template-results-figure}.
}
The themes covered three topics: requirements, structure, and usability.
The spreadsheet \theme{requirements}{\faListUl} shaped the physical structure of the table as the requirements would inform the table schema.
Similar to \citet{huang2024table}'s study, we found the structure of the spreadsheet was hierarchical and was dived at the \theme{spreadsheet level}{\faNewspaper} and \theme{table level}{\faTable}.
We also elucidated new themes related to different types of tables that provided organization within the spreadsheet.
This included \theme{data tables}{\faDatabase} (see Figure~\ref{fig:template}-2), which contained the fundamental information about the data schema, and \theme{insight tables}{\faLightbulb[regular]} (see Figure~\ref{fig:template}-1), which used the data tables to generate insights.
These types of tables reflect how professionals create spreadsheets, which involves developing the underlying schema to represent the data and then generating insights based on the defined schema~\cite{pirolli2005sensemaking}.
Finally, the user experience of the spreadsheet, which included themes like \theme{ease of use}{\faThumbsUp[regular]} and \theme{presentation}{\faPalette}), enhanced the programmer's experience of using spreadsheets.
An example Excel spreadsheet template is shown in Figure~\ref{fig:template}.
We now discuss the codes for each theme in detail below.

\subsubsection{\faThumbsUp[regular]\xspace Ease of Use}
The Excel templates included features to make them usable by other programmers. 
This included \formativecode{documentation}, such as notes, tool tips, or guides on how to use the spreadsheet.
Written guides were commonly included as a separate sheet within the Excel workbook.
Templates were often explicitly denoted as \formativecode{simple} by including terms such as "basic" or "simple" in the title.

\subsubsection{\faPalette\xspace Presentation}
As noted in prior literature~\cite{huang2024table}, we identified a theme related to the presentation of the template.
This included the \formativecode{theme} of the template and \formativecode{conditional formatting} of the cell data.
We also noted \formativecode{interactive elements} of the spreadsheets, such as navigation elements.

\subsubsection{\faListUl\xspace Requirements}
One theme was the requirements for the spreadsheet, \added{which was not discussed in prior work}.
The templates differed according to \formativecode{audience} (e.g., for personal use versus business use) and \formativecode{timescale} (e.g., budgets on a monthly versus a yearly basis).
The templates also varied by additional \formativecode{context} about the purpose of the spreadsheet (e.g., a budget for a wedding, marketing team, family meal plan, or landscaping project).
The requirements influenced the spreadsheet table schema.
For example, a spreadsheet for a gardening project could require columns for the name, type, and quantity of each plant, while a spreadsheet for a yearly business budget could require columns for each month of the year.

\subsubsection{\faNewspaper\xspace Spreadsheet Level}
Template features occurred at the spreadsheet level, such as each sheet having a unique \formativecode{spreadsheet name}.
There were also differences in the overall \formativecode{architecture} of the spreadsheet.
Some spreadsheets put all the tables on a single sheet (e.g., in Figure~\ref{fig:template}), while others separated the data and insight tables in different sheets.

\subsubsection{\faTable\xspace Table Level}
Template features also occurred at the table level.
Each table was given a \formativecode{table name} and a \formativecode{schema} of its underlying data.
\formativecode{Example rows} and data were often provided to illustrate the schema and to show how the formulas worked.
For example, in Figure~\ref{fig:template}, there are example data in the data tables and the insight table that demonstrate the behavior of the aggregation in the last row, as well as the conditions under which an up or down arrow appears.
In addition, tables often included \formativecode{transforms}, which were cells that had formulas that encoded computational relationships between multiple columns in the schema.
For example, Figure~\ref{fig:template} includes the "Over / Under" column that takes the difference between the estimated and actual costs.

\subsubsection{\faDatabase\xspace Data Tables}
Data tables, which represented the underlying data for the spreadsheet, had unique features.
Two data tables are included in the example template in Figure~\ref{fig:template}.
Data within the data tables were either \formativecode{categorical values} (i.e., represented by categorical data) or \formativecode{literals} (i.e., plain values).
As noted in \citet{huang2024table}'s study, at the bottom of the data tables, there were often \formativecode{aggregation cells} that aggregated raw data (e.g., total cost for column A) to summarize the information in the table.
Finally, these tables had \formativecode{parameter cells} that contained related information that was not incorporated into the schema (e.g., a checkbox to track whether the data corresponding to a table were paid).

\subsubsection{\faLightbulb[regular]\xspace Insight Tables}
Insight tables, designed to transform data tables into actionable insights, featured distinct elements from the data tables themselves. 
An example of an insight table can be found in the template shown in Figure~\ref{fig:template}. 
Complex spreadsheets with multiple tables often included an \formativecode{aggregation reference cell}, which directly linked to an aggregation cell within a data table. 
These could be further aggregated using a \formativecode{table aggregation cell} to provide an overall summary of the spreadsheet. 
For instance, in Figure~\ref{fig:template}, the insight table connects the cost of each category through aggregate reference cells, which are then summarized in the bottom row via a table aggregation cell. 
Additionally, insight tables were often enhanced with \formativecode{charts} to visually represent the data.

\mybox{
\faArrowCircleRight\xspace\textbf{Key findings (RQ1):}
\added{
The design space of spreadsheet templates include the structure and design of the spreadsheet.
}
The spreadsheet's design is influenced by the requirements of the problem and usability aspects, such as ease of use and presentation. 
\added{
In terms of spreadsheet structure, spreadsheet template features span at the spreadsheet level and table level.
}
Tables come in two variants: data tables that define the underlying data within the sheet and insight tables that summarize the data contained in the data tables.
}

\section{Formative User Study}
\label{sec:formative-user-study}
\added{
While \citet{erwig2006gencel}'s user study of Gencel provided some insights on scaffolding support through spreadsheet templates, it is unclear what the benefits and drawbacks of scaffolding tools are compared to having no guidance.
}
\added{To support programmers in creating spreadsheets through scaffolding}, we conducted a formative user study \added{to understand current practices in using scaffolding tools in spreadsheet programming}.
In particular, we wanted to understand programmers' experiences of using existing scaffolding tools (i.e., spreadsheet templates) and how this compared to not receiving any scaffolding assistance. 
We also wanted to understand spreadsheet programmers' needs for how an AI could assist them in creating a spreadsheet.
Therefore, the research questions in this study were:
\begin{description}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{2pt}
    \item[\textbf{RQ2}] What is the experience of spreadsheet programmers while developing spreadsheets from scratch?
    \item[\textbf{RQ3}] What is the experience of spreadsheet programmers while developing spreadsheets from a template?
    \item[\textbf{RQ4}] What do spreadsheet programmers want from an AI that assists with creating spreadsheets?
\end{description}

To answer RQ2, RQ3, and RQ4, we recruited 7 spreadsheet programmers (Section~\ref{sec:formative-user-study-participants}) who completed a spreadsheet programming task (Section~\ref{sec:formative-user-study-task}) during an hour-long user study (Setion~\ref{sec:formative-user-study-protocol}).
We then analyzed the data (Section~\ref{sec:formative-user-study-analysis}) to understand how programmers developed spreadsheets from scratch and from templates as well as their needs for an AI spreadsheet creation tool (Section~\ref{sec:formative-user-study-results}).
Using the findings from this study, we define design principles (Section~\ref{sec:design-goals}) for our tool for building spreadsheets, \tool.
our research questions (Section~\ref{sec:evaluation-user-study-results}).
The materials used in this study, including the protocols and codebooks, are included in the supplemental materials~\cite{supplemental-materials}.

\subsection{Participants}
\label{sec:formative-user-study-participants}
We recruited 7 \added{active} spreadsheet programmers through an online user study platform. 
This ensured participants were familiar with spreadsheet software, as the study focused on usage rather than learning barriers.
Therefore, the inclusion criteria were Excel users who were working professionals from English-speaking countries and used Excel for at least 5 days in the previous month.
Our participants were men ($N=3$) and women ($N=4$) aged 26 to 49 ($\mu=35$) located in the United Kingdom ($N=2$), United States ($N=2$), and Canada ($N=3$).
Participants used Excel for 11-20 days ($N=4$) and 21+ days ($N=3$) in the previous month.

\subsection{Task}
\label{sec:formative-user-study-task}
We developed the user study task so that it included elements of the process that professionals follow to create knowledge products such as spreadsheets~\cite{pirolli2005sensemaking}: gathering information about the task, schematizing the task data, and developing insights based on the schema.

\paragraph{Task Description}
The user study task was in the domain of task tracking.
We selected this domain because \added{similar to the budget domain in the spreadsheet template study (see Section~\ref{sec:template-analysis})}, it was a predefined category of templates in the Excel application.
The user study task presented participants with a list of food drive tasks and asked them to determine who was doing the most work.
Tasks varied by frequency (e.g., twice a week versus once a quarter) and duration.

\subsection{Protocol}
\label{sec:formative-user-study-protocol}
The user study was conducted remotely on a teleconferencing platform, where the participant's screen was recorded.
During the user study, the participants created spreadsheets to complete the task twice: once from scratch and once from a template retrieved from the task tracking template category in Excel version 2409.
This is because we wanted to observe participants creating spreadsheets with and without the assistance of scaffolding tools.
To reduce learning effects, the order of the conditions was roughly counterbalanced.

\paragraph{Overview}
To begin the study, the interviewer introduced the user study task.
The participant then completed the task either by creating a table from scratch or from a template in Excel.
Participants were allowed to access the Internet or use any Excel feature to replicate a typical work environment.
Participants were initially given 15 minutes to complete the task and were provided extra time to edit the spreadsheet if needed.


Participants who created a spreadsheet from scratch were given a blank Excel workbook, while those who used a template received a copy of the task planning template. 
After completing the first task, participants repeated it under the opposite condition. 
Following each round, they debriefed about their experience and, after both rounds, compared and contrasted the two approaches.

At the end of the study, participants were shown three designs of an AI that created spreadsheets which that varied in the level of guidance provided by the tool.
The designs were presented as high-fidelity screenshot prototypes based on Excel and Excel Copilot. 
The first design involved interacting with the AI from the Excel canvas as a formula, while the second option used chat-based interaction. 
The most structured option featured an AI-driven setup wizard with static UI elements like drop-down menus. 
Participants were asked to share their impressions of these designs.

\paragraph{Piloting}
Following best practices for user studies in software engineering~\cite{ko2015practical}, we piloted the user study protocol with two spreadsheet programmers to: 1) ensure the quality of the interview protocol by identifying and reducing potential confounding factors and 2) verify that the task allowed participants to follow the process detailed by \citet{pirolli2005sensemaking}.
We updated the wording of the interview protocol based on participants' feedback. 
We found that all pilot participants completed the task in the 15-minute time frame and engaged in gathering information about the task, schematizing the data, and developing insights.

\subsection{Analysis}
\label{sec:formative-user-study-analysis}
After the interviews, \added{one author performed transcription} and \added{two authors qualitatively analyzed the interviews}, following the same qualitative analysis process in Section~\ref{sec:template-analysis-analysis}.
The authors developed an initial codebook on 2 transcripts (25\% of the samples) and after validation obtained a Cohen $\kappa$ score of 0.84---near perfect agreement~\cite{landis1977measurement}.
No new codes were discovered while applying the codes to the remaining data.

We also performed an analysis of the participants' development activities to study their development process. 
The second author reviewed footage of the spreadsheet development process and applied a codebook on programmer implementation actions from~\citet{liang2023qualitative} at the category level.
Each action was labeled with a start time, a stop time, and a code label, resulting in a dataset of 217 actions.
To understand the effect of the task on the amount of time spent on each action, we aggregated the time each participant spent on every action.
We then ran a mixed-effects model that modeled participants as a random effect, with an interaction term between the conditions and actions: 

\begin{equation*}
\texttt{Total Action Time} \sim \texttt{Condition} + \texttt{Action} + \texttt{Condition} \times \texttt{Action} + (\texttt{1|Participant})
\end{equation*}

\begin{figure}[t!]
\centering
\includegraphics[trim=75 1650 75 0, clip, width=\linewidth, keepaspectratio]{figures/formative_study-activity-figure.pdf} 
\caption{
A timeline of the participants' spreadsheet programming activities from the formative study for creating a spreadsheet from scratch (left) and from a template (right).
The red vertical line denotes the 15-minute mark.
}
\label{fig:formative-study-activity}
\end{figure}

\subsection{Results}
\label{sec:formative-user-study-results}
An overview of the activities of the participants is in Figure~\ref{fig:formative-study-activity}.
We find that creating spreadsheets from scratch and from templates both have benefits and drawbacks.
Creating from scratch affords flexibility and customization of the spreadsheets, but can reduce the overall quality of the final spreadsheet due to \added{minimal guidance on performing Excel actions (e.g., writing formulas)}.
Meanwhile, starting from a template provides structure to the problem solving process and produces more polished spreadsheets. 
However, using a template requires significant adaptation that can interfere with the programmer's ability to complete the task.
We now discuss the results in further detail.

\subsubsection{From Scratch}
While creating spreadsheets from scratch, we found that 4 of the 7 participants completed the task in the 15 minutes allotted.
2 participants completed the task given extra time; only P5 did not complete the task.
The study participants spent most of their time on implementation.
Each participant followed their own unique process to implement a spreadsheet (see Figure~\ref{fig:formative-study-activity}), which aligns with traditional software engineering~\cite{liang2023qualitative}.
All participants completed multiple rounds of prototyping and implementation, supporting previous findings that the spreadsheet creation process is highly iterative~\cite{huang2024table, pirolli2005sensemaking}.
In addition, we noted that P2---the only participant who created a polished spreadsheet with formulas and themes---modeled a professional's process of first prototyping the table schema and then implementing the rest of the spreadsheet~\cite{pirolli2005sensemaking}.

Creating spreadsheets from scratch enabled \formativecode{customized} ($N=5$) spreadsheets that fit the programmer's context, as it was not constrained by a predefined structure.
This allowed the spreadsheet to be \pquote{[designed] in a way that naturally makes sense to me}{7}, making it \pquote{quite easy...to move stuff around}{6}.
However, the resulting sheets were \formativecode{quick-and-dirty} ($N=3$) that were not well-designed: \pquote{The table is not structured. It's more like a note thing rather than a table}{1}.
Participants ($N=5$) did not include any formatting elements (e.g., table themes or borders) in their spreadsheets.
Furthermore, \added{starting from scratch provided \formativecode{limited support on structuring problems}} ($N=7$).
Participants noted difficulties knowing \pquote{how...to present [information] in the most natural way}{7} and \pquote{putting my thought process into the spreadsheets}{4}.
This echos previous work that indicates that knowing how to schematize data is a skill to develop spreadsheets~\cite{pirolli2005sensemaking, ko2011state, chalhoub2022s} \added{and that problem-solving knowledge is an important skill in programming~\cite{baltes2018towards, liang2022understanding}}.
\added{Finally, participants had \formativecode{minimal guidance on using advanced Excel features}} ($N=5$), especially formulas: \pquote{I'm not too familiar with the formulas... That's why it took me a bit longer to get the calculations}{3}.
Rather than writing Excel formulas, P1 resorted to using their computer's calculator, while P3 performed arithmetic mentally.
\added{
This corroborates prior findings that programmers struggle with understanding spreadsheet programming APIs~\cite{ko2011state, nardi1991twinkling}.
}

\mybox{
\faArrowCircleRight\xspace\textbf{Key findings (RQ2):} 
\added{
Programmers who develop spreadsheets from scratch create spreadsheets that match the programmer's context, but the resulting spreadsheets are unpolished.
}
Programmers struggle to structure the table to solve the task and lacked guidance on using advanced Excel features to implement the spreadsheet.
In addition, a vast majority of programmers completed the task by creating spreadsheets from scratch.
}

\subsubsection{From a Template}
Only a single participant (P7) completed the task in the 15 minutes allotted when using an Excel template to create a spreadsheet.
Although 4 of the 7 participants required additional time, these participants were still unable to complete the task.
This suggests that the usage of templates hindered the completion of the task compared to creating from scratch.
This aligns with previous work, which finds that the use of programming tools can increase task completion time~\cite{pandita2018no} and increase task failure rates~\cite{vaithilingam2022expectation}.

In addition, \added{we observed that participants spent additional time on all actions compared to creating spreadsheets from scratch, with the increase in time of each action ranging from 1.4 minutes (83 seconds) to 4.0 minutes (237 seconds).
The largest differences in time spent occurred in assessing the spreadsheet ($p=0.003$) and updating knowledge ($p=0.40$), where study participants spent an additional 4.0 minutes (237 seconds) understanding the template and 2.0 minutes (123 seconds) looking up documentation.
}
In terms of process (see Figure~\ref{fig:formative-study-activity}), several participants ($N=4$) entered multiple rounds of implementation and prototyping, indicating that spreadsheet development is iterative~\cite{huang2024table} even while working with templates.
However, fewer participants performed these iterations compared to creating spreadsheets from scratch, suggesting that some participants adopted the spreadsheet template schema with minimal modification of the columns.

Participants noted that templates were \formativecode{structured} ($N=4$), which could help spreadsheet programmers with problem-solving: \pquote{These templates are set up to be quick and easy if you cant think of how you want to do something}{6}.
However, study participants found that creating spreadsheets from templates was difficult for a variety of reasons.
For example, the structure imposed by the template was too rigid and \formativecode{required adaptation} ($N=7$) to fit the programmer's context, such as removing extraneous columns: \pquote{Templates [are] quite annoying because they're not necessarily catered 100\% to my needs}{6}.
\added{
This has been observed in prior studies on programming scaffolding tools, which find that the tool needs to be adapted to the programmer's context~\cite{erwig2006gencel, latoza2020explicit}.
}
Because spreadsheet templates required significant effort for \pquote{modification than it is actually worth}{7}, templates were useful only when they closely matched the problem: \pquote{The template...feels very specific.  You can only use it for work that is exactly the same as the one here}{3}.
In addition, templates were often \formativecode{hard to understand} ($N=5$), \added{as noted in prior work~\cite{reschenhofer2015empirical}}: \pquote{I'm not familiar enough...to interpret at a glance what someone else has created}{5}.
Since the participants \pquote{did not set up [the spreadsheet] myself}{5} and struggled to understand \pquote{how calculations works}{1}, they \pquote{need[ed] to spend more time...working out if the template is designed for me}{6}.
However, participants noted several benefits of templates.
This included offering \formativecode{reusable features} ($N=3$) like drop-down lists for validation, filtering, or formulas.
Participants also noted templates were more \formativecode{polished} ($N=6$): \pquote{I do like the visual presentation and the layout of the template}{3}.

\mybox{
\faArrowCircleRight\xspace\textbf{Key findings (RQ3):} 
\added{
Spreadsheet programmers who develop spreadsheets from templates have more structure to solve the problem and produce more polished spreadsheets. 
However, the templates require heavy adaptation and are difficult to understand.
}
Finally, creating spreadsheets from templates requires more time to assess the templates, which can result in less task completion.
}


\subsubsection{Spreadsheet Programmer Needs for AI}
The majority of the participants preferred the chat bot ($N=6$) compared to the setup wizard ($N=1$) and the in-canvas formula ($N=0$), since the chat bot was more \formativecode{flexible} ($N=7$) and could be used to \pquote{update and rectify [outputs]}{1} via natural language.
The participants felt that the setup wizard interface was \pquote{too structured and too limited}{2} since a programmer would be dependent on \pquote{how extensive the options are}{5}, whereas for the in-canvas interface, there was no \pquote{interaction capability}{1}.

The participants wanted to \formativecode{describe tasks and goals} ($N=5$) to the AI, since their \pquote{concerns are always more related to the high level}{7}: \pquote{You don't want to say, 'I want cell A1 + B12`... You want to explain the scenario}{1}.
Next, participants said the AI should have \formativecode{understandable outputs} ($N=3$) for easy modification: \pquote{If [the AI] was not transparent about what it created, I'd have a hard time...making little tweaks}{5}.
Further, participants said the tool should be \formativecode{usable} ($N=6$) and not require technical knowledge: \pquote{I have a decent base of coding, but I want to explain what I want without using any technical terms}{1}.
Participants also noted the tool should provide suggestions on \formativecode{insights} ($N=5$) in the data, like specific analyses, \pquote{graphs}{3}, or \pquote{ways to summarize the data}{5}.

\mybox{
\faArrowCircleRight\xspace\textbf{Key findings (RQ4):}
\added{For an AI that assists with creating spreadsheets, spreadsheet programmers} prefer a chat-based interface for its flexibility in expressing intent and correcting outputs.
Programmers want to work with the AI to express high-level intents and understand the AI's outputs for a seamless modification of the spreadsheet.
}

\section{Design Principles}
\label{sec:design-goals}
Based on the findings from the spreadsheet template study (Section~\ref{sec:template-analysis}) and the formative user study (Section~\ref{sec:formative-user-study}), we reason that an effective tool for developing spreadsheets should follow these design principles (labeled "DP"):

\paragraph{\textbf{\dgA}}
\added{
Tools that support spreadsheet programming should guide users through the process of creating spreadsheets.
Spreadsheet programmers struggle with \formativecode{limited support on structuring problems} and having \formativecode{minimal guidance on using advanced Excel features} (e.g., writing formulas) while creating spreadsheets from scratch, which results in \formativecode{quick-and-dirty} spreadsheets.
}
In contrast, templates offer welcome \formativecode{structure}.

Because prior literature shows that scaffolding tools can improve programmer task performance in a variety of activities~\cite{latoza2020explicit, arab2022exploratory, bajpai2024lets}, we infer that guiding spreadsheet programmers through established processes can achieve their goals more effectively, especially since professional spreadsheet programmers follow \added{a defined process of gathering information, representing information in a schema, developing insight on the data, and creating a knowledge product~\cite{pirolli2005sensemaking}.
For spreadsheet programming, whereby the process inherently creates a knowledge product (i.e., a spreadsheet), spreadsheet programming tools should guide users through the process of: 1) gathering \theme{requirements}{\faListUl} about the spreadsheet table, 2) defining the data schema in \theme{data tables}{\faDatabase}, and 3) extracting insights by creating \theme{insight tables}{\faLightbulb[regular]}.
For gathering requirements, the tool should elicit the \formativecode{audience} and additional \formativecode{context} of the spreadsheet such as \formativecode{timescale}.
To create data tables, the tool should create a \formativecode{schema} with the given requirements and generate \formativecode{example rows}.
To develop insight tables, the tool should suggest and generate analyses to obtain insights from the data.
}

\paragraph{\textbf{\dgB}}
\added{
Tools supporting spreadsheet programming should imbue flexibility within the structured process.
}
They should have \formativecode{flexible} interactions because spreadsheet development is a highly iterative and nonlinear process.
However, spreadsheet programming scaffolding tools are inflexible and \formativecode{require adaptation} to become \formativecode{customized} to fit the \theme{requirements}{\faListUl} of the programmer.
Therefore, we reason that it is important for tools that support spreadsheet programming to provide flexibility to perform actions that fit a programmer's specific context for better task performance.

\paragraph{\textbf{\dgC}}
\added{
Tools that support spreadsheet programming should build spreadsheets iteratively and incrementally.
}
A drawback to creating spreadsheets from a template is that templates are \formativecode{hard to understand} due to their complexity, requiring programmers to spend more time understanding the template.
\added{
Therefore, tools that support spreadsheet programming should produce \formativecode{understandable outputs} that are \formativecode{simple}.
}
We infer that incrementally building spreadsheets with programmers in smaller units can result in less manual adaptation of the generated spreadsheets and make the spreadsheet creation easier for the programmer.

\begin{figure}[t!]
\centering
\includegraphics[trim=0 175 825 0, clip, width=0.95\linewidth, keepaspectratio]{figures/tool-figure.pdf} 
\caption{
The interface of \tool, an agent-based tool that helps programmers build spreadsheets in Excel by \ilabel{1} guiding programmers through a structured plan of developing spreadsheets (gathering requirements, defining the data schema, and extracting insights) based on expert processes; \ilabel{2} prototyping tables in the chat using Markdown; \ilabel{3} leveraging tools that produce spreadsheet components can be composed to build spreadsheets; and \ilabel{4} suggesting next steps based on the plan and the current state to adapt the plan to the programmer's context.
}
\label{fig:tool}
\end{figure}

\input{tables/comparison}

\section{\tool System}
\label{sec:tool}
We implement \tool, a tool that assists programmers in creating spreadsheet programs in Excel (see Figure~\ref{fig:tool}).
We compare \tool with other state-of-the-art spreadsheet programming tools in research (i.e., SheetCopilot~\cite{li2024sheetcopilot}) and practice (i.e., Excel Copilot, spreadsheet templates) based on the design principles (labeled "DP") and features (labeled "F") elaborated below (see Table~\ref{tab:tool-comparison}).
\tool is the only tool that achieves all design principles.
For example, templates provide scaffolding, but do not offer flexibility or enable incremental spreadsheet development.
Agentic tools such as SheetCopilot~\cite{li2024sheetcopilot} and Excel Copilot can perform spreadsheet development actions, but do not provide scaffolding for the programmer.
We note that Excel Copilot offers Markdown table previews in the chat and while it offers suggestions, the suggestions are not based on the spreadsheet creation plan.
Now, we introduce \tool in a motivating example (Section~\ref{sec:motivating-example}) and then discuss the design of \tool (Section~\ref{sec:design}) and its implementation details (Section~\ref{sec:implementation}).

\subsection{Motivating Example}
\label{sec:motivating-example}
We revisit Alma from Section~\ref{sec:introduction}, who wants to analyze her small business expenses to understand her top spending categories for her business co-owner.
Due to Alma's earlier struggles to implement this spreadsheet, she tries \tool:

\scenario{
Alma opens Excel and launches \tool's chat pane, where it asks about her spreadsheet goals. 
She replies, "I need a spreadsheet to track spending categories for my small business." 
\tool guides Alma through the requirements by asking clarifying questions about task details (Figure~\ref{fig:tool}-1). 
Alma responds, "Its for me and my co-owner to identify top spending categories. Heres a recent credit card statement..." and pastes the statement into the chat.
\newline\newline
\tool prototypes a sample spreadsheet as a Markdown table in the chat (Figure~\ref{fig:tool}-2), with columns for date (A), category (B), amount (C), and notes (D), populated using Almas credit card data. 
Although Alma could continue to edit the table in chat, she is now satisfied with the schema, she and tells \tool to transfer the table to Excel.
In response, \tool runs the action to create a table (see Figure~\ref{fig:tool}-3), adding it to the Excel canvas.
\newline\newline
With the table now added to Excel, \tool presents three potential next steps that are displayed as rounded, clickable UI elements known as "suggestion pills": \emph{"Summarize the total amount spent in April."}, \emph{"Create a pie chart showing expenses by category for April."}, an \emph{"Add more example data for April expenses."} (see Figure~\ref{fig:tool}-4).
Alma selects the first option as it best aligns with her goal. 
The tool prototypes a table with two columns---category and total cost. 
The total cost column includes an Excel formula, such as \texttt{=SUMIFS(C:C, A:A, ">=2023-04-01", A:A, "<=2023-04-30"}, B:B, "Operational"), to calculate costs by category for the past month. 
Alma accepts, and the table is added below the first one and sorted by total cost.
Next, Alma selects the suggestion \emph{"Create a pie chart showing expenses by category for April."}, which generates a pie chart using the new tables data. 
Satisfied with the results, she shares the spreadsheet with her co-owner for review.
}

\subsection{Design}
\label{sec:design}
\added{
To reify the design principles in \tool, we leverage language agents since they can achieve the design goals of scaffolding, flexibility, and incrementality. 
This is because LLMs can autonomously reason through complex tasks and can adapt to feedback in their environment~\cite{shinn2024reflexion, gupta2024metareflection, paul2023refiner, madaan2024self}, which can provide both scaffolding and flexibility.
By leveraging tools (i.e., external modules that complete specific actions) to build atomic components of spreadsheets, agents can construct spreadsheets incrementally.
}
We further discuss the main features of \tool and how they support the design principles below.

\subsubsection{\dgA}
We designed \tool to scaffold programmers through the structured process that professionals follow to develop knowledge products from data, such as spreadsheets (\emph{F1}).

\paragraph{\fA} 
\added{
Following prior LLM tools that guide users through expert processes~\cite{sun2024reviewflow, bajpai2024lets}, \tool scaffolds programmers through the spreadsheet creation process \added{enumerated in Section~\ref{sec:design-goals}}}.
We translated each step as a natural language instruction and included these instructions in the tool's system prompt as a step-by-step recipe (see Section~\ref{sec:implementation}).
For the first step of gathering requirements, the agent elicits the programmer's spreadsheet needs by asking questions about the audience, context, and timescale of the spreadsheet as well as requesting related documents.
For the second step of creating a data table, the agent proposes a schema with the given information and generates a table with example rows.
Finally, to develop insight tables, \tool suggests different insights that a programmer could obtain from the data and implements an analysis that the programmer requested.

\subsubsection{\dgB}
\tool is designed to offer flexibility in the structured process, ensuring programmers are not constrained by rigid scaffolding. 
The chat interface allows programmers to freely interact with the tool and express their needs without limitations. 
Additionally, suggestion pills (\emph{F2}) reveal potential next steps, enabling users to adapt the process based on their personal context for human-in-the-loop planning. 
The tool also supports rapid prototyping of spreadsheet tables in Markdown (\emph{F3}), providing programmers with an efficient way to iterate and refine their ideas.

\paragraph{\fB}
\added{
LLMs can perform better decision-making by considering multiple reasoning paths and looking ahead to the next step~\cite{yao2024tree}.
}
\tool provides multiple suggestions to help programmers explore the solution space, leveraging their problem-solving expertise~\cite{baltes2018towards, liang2022understanding} to select the most suitable action.
This approach ensures the language agent performs actions that align closely with the programmer's goals.
Similar to prior work using LLMs to generate follow-up suggestions~\cite{bajpai2024lets}, \tool employs LLM-generated follow-ups during each interaction.
However, unlike previous systems, \tool's follow-up responses are designed to guide programmers through the expert process the agent is following, advancing their progress in a structured yet adaptable manner.
To guide the programmer through the agent's expert process, \tool provides three next-step suggestions displayed as suggestion pills. 
These are generated based on the conversation history, spreadsheet context, and the programmer's goal, ensuring the most efficient path to achieving the objective.

\paragraph{\fC}
Prototyping is vital in software development to evaluate solution viability~\cite{liang2022understanding}. 
Given that spreadsheet development is iterative~\cite{pirolli2005sensemaking, huang2024table}, rapid spreadsheet prototyping is crucial for testing schemas. 
\tool enables this through Markdown because it is human-readable and is a widely supported format that LLMs can generate~\cite{liu2024we}.
While direct manipulation in Excel would be intuitive, it is technically impractical due to the slow inference speed of language agents, which requires planning and executing actions~\cite{wang2024survey}. 
Instead, \tool uses chat-based prototyping, offering faster iterations and the ability to test solutions without modifying the spreadsheet since text generation is more efficient than generating and performing actions.
\added{In the future, improved agent reasoning speeds could make direct manipulation a feasible and desirable option.}

\input{tables/tools}

\subsubsection{\dgC}
\label{sec:dgC}
\tool supports the incremental development of spreadsheets to produce changes that are easy for spreadsheet programmers to understand. 
We do so by creating tools that the language agent can use to implement the atomic components of spreadsheets (\emph{F4}).
The output of these tools can be combined to implement more complex spreadsheet program behaviors.

\paragraph{\fD}
\added{Because language agents struggle to perform certain tasks (e.g., mathematical reasoning~\cite{lu2023survey})}, language agents are commonly equipped with "tools" like calculators and code interpreters for additional capabilities~\cite{xi2023rise, wang2024survey, schick2024toolformer, gao2023pal}.
These tools function like API calls, where the agent populates parameters to execute tasks (e.g., running code with a code interpreter).
\tool uses tools to implement atomic spreadsheet components (e.g., conditional formatting, charts) by leveraging OfficeScript, an Excel-specific scripting language with APIs for direct canvas manipulation~\cite{microsoft2024officescript}. 
A tool-based approach is required since state-of-the-art LLMs like \texttt{GPT-4} have limited OfficeScript generation capabilities~\cite{payan2023instructexcel}. 
In \tool, pre-written OfficeScript scripts handle tasks, with the agent supplying parameters to execute desired behaviors and Excel executing these scripts to update the spreadsheet.
\tool tools include creating tables, changing colors, highlighting rows and cells, visualizations, sorting, filtering, and renaming sheets. 
These tools, summarized in Table~\ref{tab:tools}, were designed to cover spreadsheet structure-related themes identified in the template study (see Section~\ref{sec:template-analysis}) for comprehensive component coverage.

\begin{figure}[t!]
\centering
\includegraphics[trim=0 250 275 0, clip, width=\linewidth, keepaspectratio]{figures/overview-figure.pdf} 
\caption{
An overview of \tool. 
\textbf{Interact/Observe:} A spreadsheet programmer interacts with \tool and \tool observes the current state of the spreadsheet. 
\textbf{Plan (AI):} Based on this information, \tool plans which tool to use to address the programmer's query. \
\textbf{Update:} After selecting a tool, \tool executes the tool to update the spreadsheet. 
\textbf{Observe:} The result of running the tool, including the new workbook state, is sent back to \tool. 
\textbf{Respond:} \tool responds with a summary of the changes that it made. 
\textbf{Plan (human):} The spreadsheet programmer assists with planning by selecting the next step through the suggestion pill. 
\tool reads the new spreadsheet state, and decides its next step. 
At any given interaction, \tool can continue to execute tools (\textbf{Observe}, \textbf{Plan (AI)}, \textbf{Update}) or respond to the programmer (\textbf{Respond}).
}
\label{fig:overview}
\end{figure}


\subsection{Implementation}
\label{sec:implementation}
\tool adapts the front end of Excel Copilot in Excel version 2409. 
It has a custom Flask server as the back end which contains the language agent implemented using the OpenAI Assistants API~\cite{openai-api-assistants} with \gpt.
We describe different implementation details of \tool, including the architecture (Section~\ref{sec:tool-architecture}), as well as the agents and their prompt structures (Section~\ref{sec:tool-agents}) below.

\subsubsection{Architecture}
\label{sec:tool-architecture}
The front end gathers input, captures current workbook state, executes tools and records the result of the execution, and finally renders the back end's response to the programmer.
Meanwhile, the back end's responsibility is to run the language agent. 
The agent considers the programmer's query and the current workbook state, generates a response to the programmer, and selects tools that will complete the programmer's request.

Figure~\ref{fig:overview} illustrates the interaction between the front end and back end. 
When a user sends a message, the front end generates a textual representation of the workbook's current state and sends it, along with the user's message, to the back end. 
The back end uses the OpenAI Assistants API to determine the best tool for the query and returns a response with tool calls, which are executed in the front end.
The updated workbook state is saved and sent back to the back end to keep the API informed. If additional tools are required, the back end requests further executions, and this loop continues until no tools remain to run.

\subsubsection{Agents}
\label{sec:tool-agents}
\tool is implemented as a single agent system.
It operates in two steps: one to generate utterances and plan actions based on the current state and another to generate suggested follow-up responses to the programmer.

The agent analyzes the current spreadsheet state and chat history to generate an utterance or select appropriate tools. 
If an OfficeScript tool is chosen, the agent populates its parameters according to the tool's specification, which includes details on its behavior, parameter descriptions, and expected output. 
Each parameter is validated to ensure it is well-formed. If an error is detected, the agent receives an error message and regenerates the parameter.
Once all parameters are error-free, the tool is sent to the front end for execution.
If an utterance is generated, the agent generates three suggestions for potential next steps in a separate API call.
The agent considers the current spreadsheet state, chat history, the most recent message, and the goals of the programmer by generating three suggestions for the programmer.
For this prompt, we adopt the ReAct prompting approach~\cite{yao2023react}, where the model simulates the programmer's thought process at each step before producing a suggestion. 
This encourages deeper reasoning by the model, leading to more contextually relevant suggestions.

The agent's prompt begins with a description of the Excel spreadsheet programming context.
It then outlines a JSON representation of each table, structured as a JSON array containing metadata for each cell (e.g., cell address, value, formula). 
Next, the prompt details the step-by-step spreadsheet creation process, including instructions for prototyping tables in Markdown when defining data and insight tables. 
The agent is also tasked with summarizing the programmer's current overall goal. 
Finally, additional guidelines are provided, such as using Excel formulas and ensuring constraints like non-overlapping tables.
Meanwhile, the suggestion prompt begins with a description of the Excel spreadsheet programming context and the overall task.
Guidelines for writing the suggestions are provided (e.g., being concise or suggesting actions for only existing tables) and finally, the JSON output format of the suggestion is described.

\section{Evaluation}
\label{sec:evaluation-user-study}
\added{To evaluate \tool's agentic approach to implementing the design principles}, we recruited 20 spreadsheet programmers (Section~\ref{sec:evaluation-user-study-participants}) to perform two tasks (Section~\ref{sec:evaluation-user-study-tasks}) in a controlled study (Section~\ref{sec:evaluation-user-study-protocol}) using \tool and \added{a \baseline.
The baseline for this study is the Excel Copilot available in Excel version 2409 (see Table~\ref{tab:tool-comparison}).
}
We then analyzed these data quantitatively and qualitatively (Section~\ref{sec:evaluation-user-study-analysis}) to better understand the results of our research questions (Section~\ref{sec:evaluation-user-study-results}).
The materials used in this study, including the protocols and codebooks, are included in the supplemental materials~\cite{supplemental-materials}.

\added{
One objective of this study was to derive lasting insights into human-agent collaboration that would remain relevant over time. 
While current agentic approaches like \tool~\cite{wang2024survey} are characterized by high latency, we anticipate future technological advancements will reduce these delays. 
To address this, our evaluation focused on spreadsheet quality rather than task completion, with study protocols and analyses designed to consider latency effects.
}
Therefore, our research questions for the evaluation are as follows:

\begin{description}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{2pt}
    \item[\textbf{RQ5}] How does using \tool affect the quality of the created spreadsheets?
    \item[\textbf{RQ6}] How does using \tool affect the way a spreadsheet programmer creates spreadsheets?
    \item[\textbf{RQ7}] How does using \tool affect a spreadsheet programmer's perceived experience in creating spreadsheets?
\end{description}

\subsection{Participants}
\label{sec:evaluation-user-study-participants}
We recruited 20 spreadsheet programmers via an online user study recruitment platform.
We applied the same inclusion criteria as the formative user study (see Section~\ref{sec:formative-user-study-participants}).
Our participants were men ($N=12$) and women ($N=8$) aged 28 to 60 ($\mu=40$) located in the United Kingdom ($N=4$), United States ($N=9$), Canada ($N=6$), and Australia ($N=1$).
Participants used Excel 5-10 days ($N=3$), 11-20 days ($N=7$) or 21+ days ($N=10$) in the previous month.

\subsection{Tasks}
\label{sec:evaluation-user-study-tasks}
Participants completed two tasks: 1) analyzing student grades and 2) tracking hours worked and client payments. 
These tasks were selected from \citet{chalhoub2022s}'s study of spreadsheet programmers and aligned with common occupations requiring spreadsheet skills~\cite{glass2024skills}. 
Each task had its own reference solution with example data.
\added{The written task instructions directed} participants to create spreadsheets polished enough for reuse by others and visually appealing to encourage high-quality table creation. 
To promote ecological validity, the written task instructions included only the problem statement, without specific data. 
\added{This reflects real-world spreadsheet programming, where users often start with broad problem definitions and refine the data as they work~\cite{pirolli2005sensemaking}.}

Like the formative study task (see Section~\ref{sec:formative-user-study-task}), these tasks were designed to reflect the professional process of creating knowledge products~\cite{pirolli2005sensemaking}. 
Since spreadsheet programmers struggle to schematize and implement tables~\cite{pirolli2005sensemaking, ko2011state, chalhoub2022s}, we designed the task to be difficult in problem solving (i.e., knowing how to approach the problem) and implementation (i.e., writing Excel formulas), which we reason that AI tools such as \tool and \added{the \baseline} could assist with.
The tasks were intentionally difficult to complete within the 15-minute time limit to \added{focus on capturing spreadsheet quality rather than task completion}.
Participants were expected to make substantial progress but not necessarily finish the tasks, which we confirmed through piloting (see Section~\ref{sec:evaluation-user-study-protocol}).

\paragraph{Task Descriptions}
The student grades analysis task required participants to count the overall grades of all students in a course based on their raw grades for various classroom activities. 
To prevent \tool and \added{the \baseline} from automatically using a standard classification scale (e.g., AF), the overall grades were on a custom scale of Excellent ($\geq$ 95\%), Very good ($\geq$ 85\%), Satisfactory ($\geq$ 75\%) and Needs improvement ($<$ 75\%).
The overall grade for each student was calculated as a weighted average of their performance across classroom activities, which were divided into assignments, participation, and exams. 
Each category had a unique weight and a varying number of activities.

The task of tracking hours worked and client payments required participants to calculate the total hours worked in September based on a list of tasks completed for various clients. 
Tasks spanned multiple months, and each task type (e.g., consulting) had an associated hourly rate. 
For hours exceeding 10, an overtime rate of 10\% above the base hourly rate applied. 
For instance, 11 hours of work at \$100 per hour would result in the first 10 hours charged at \$100 each and the 11th hour charged at \$110.

\subsection{Protocol}
\label{sec:evaluation-user-study-protocol}
The user study was 90 minutes long and was conducted remotely on a teleconferencing platform.
To reduce bias, the two tools were framed as different versions of Excel Copilot, given that \tool was adapted from the baseline.
To reduce the confounding factors and learning effects associated with having two tools and two tasks, the order of the tasks and the tools was counterbalanced in four conditions.
Each participant was assigned to a single condition, resulting in each condition being associated with 5 data points. 

\paragraph{Overview}
At the start of the study, participants were introduced to the study context, framed as spreadsheet programmers tasked with creating a spreadsheet for a client. 
Then, the interviewer shared their screen and gave the participant control of the interviewer's computer to ensure access to the tool.

The participant then completed a 10-minute warm-up activity in which the participant created a simple budget spreadsheet.
The goal of the warm-up activity was to teach the participant the tool's capabilities and how to interact with the tool via open-ended text and suggestion pills.
In addition, for \added{the \baseline}, we instructed participants to include a table with headers in the spreadsheet for the best results.
During the tutorial, the participant developed a preliminary spreadsheet with data and then completed four sub-tasks: changing the color of the table, highlighting the cells, generating graphs, and sorting the table.
After completing the tutorial, the participant was briefed on the task and given time to read the specific task instructions in a separate document.
After answering any questions about the task, the interviewer provided additional instructions.
To promote the use of the AI tool, the participant was instructed to first try performing an action with the AI and then manually if the tool failed.
The participant was also encouraged to clarify the spreadsheet requirements with the interviewer.

Participants were then given 15 minutes to complete each task which was enforced with a timer. 
Midway through, the interviewer provided an example row from the reference solution to help participants evaluate their spreadsheet and maintain consistency. 
After the allotted time, participants completed a questionnaire assessing perceived workload, system usability, and conversation quality.
The participant was then given 15 minutes to complete the task.
Halfway through the task, the interviewer provided the participant with an example row from the reference solution to allow the participant to evaluate their spreadsheet and ensure consistency in the resulting spreadsheet.
At the end, the participant completed a questionnaire to collect data on perceived workload, system usability, and conversation quality.
The questionnaire contained the NASA Task Load Index (TLX)~\cite{hart1988development} instrument on a 10-point scale, System Usability Scale (SUS)~\cite{brooke1996sus} instrument on a 5-point scale, and two questions on conversation quality.
These questions were related to the tool's \emph{proactivity} and \emph{relevance} in conversation, which are the strongest predictors of overall conversation quality~\cite{finch-choi-2020-towards}.
After completing the questionnaire, the warm-up exercise and task protocol were repeated for the next task and tool. 

After completing both tasks with \tool and \added{the baseline}, the interviewer conducted a \added{a semi-structured interview} where the participant answered questions about the benefits and drawbacks of each tool and selected which tool they preferred overall.
\added{To account for the latency of \tool,} if the participant mentioned \tool's response time, the interviewer asked whether the participant's decision would change if the response times between the tools were equal.

\paragraph{Piloting}
\added{
We piloted the interview protocol with four spreadsheet programmers, following the same process as the formative user study (see Section~\ref{sec:formative-user-study-protocol}). 
The pilot confirmed that the tasks allowed participants to follow the professional process of working with open-ended data~\cite{pirolli2005sensemaking}.
Participants made significant progress within the 15-minute time limit, despite not completing the tasks due to their designed difficulty. 
All participants successfully created well-formed spreadsheets for the task using at least one of the tools.
}

\subsection{Analysis}
\label{sec:evaluation-user-study-analysis}
We performed quantitative and qualitative analyses on the questionnaire data, interview transcripts, interview recordings, and participant spreadsheets.

\paragraph{Spreadsheet Analysis}
To assess the quality of participant spreadsheets (RQ5), we conducted a convenience sampling of six spreadsheet programmers who had performed spreadsheet programming in the past month. 
Each evaluator was assigned a task and provided with the corresponding task instructions.
Evaluators compared 10 pairs of spreadsheets, with each pair consisting of one created using \tool and one from \added{the baseline. 
The order of the pairs was randomized to prevent evaluators from knowing which tool was used for each spreadsheet.}
For each pair, the evaluator selected the spreadsheet they preferred to reuse for continuing the task and provided open-ended feedback explaining their choice.
Each task had 3 evaluators (30 comparisons total), with each pairing being unique across each task.

We quantitatively analyzed these data by reporting descriptive statistics on evaluator preferences and modeling the strength of \tool- and \added{\baseline}-generated spreadsheets using a Bradley-Terry model.
Bradley-Terry statistical models can estimate an ability score (i.e., relative strength) of teams in competitions using pairwise comparisons~\cite{agresti2012categorical}.
In our Bradley-Terry model, each team corresponds to a tool (\tool or \added{the baseline}) and the resulting ability score of \tool represents the log-likelihood that \tool~\added{is preferred to the baseline}.

We triangulated our quantitative results with a qualitative analysis of the 60 written responses from spreadsheet evaluators. 
Due to the simplicity of the data, one author conducted the qualitative analysis. 
The first author extracted all comments about participant spreadsheets from the open-ended responses, yielding 166 total comments. 
These comments were open-coded based on the aspects of the spreadsheets they addressed, with each assigned a single code and labeled as either "positive" or "negative."
To validate the codebook, a second author applied it to a random 25\% subset of the data. 
This process resulted in near-perfect agreement~\cite{landis1977measurement}, with a Cohen's $\kappa$ score of 0.88. 
To assess whether \tool influenced the evaulators' comments, we conducted a Pearson's chi-square test on the counts of the positive and negative codes between the two tools.
To understand which codes contributed to this difference, we examined the residuals of the chi-square test and identified those whose value exceeded $\pm2$~\cite{sharpe2015your}.

\paragraph{Task Analysis}
To understand how participants created spreadsheets (RQ6), we analyzed interview video recordings. 
An author reviewed the recordings and applied \citet{mozannar2024reading}'s CodeRec User Programming States (CUPS) taxonomy, \added{which includes codes for a tool's response time}. 
Since the taxonomy was designed for traditional programming contexts, we added a new code for manually entering data, a common activity in spreadsheet programming.
Each activity was labeled with its start time, stop time, and a code, resulting in a dataset of 976 actions. 
To determine if there were differences between the tools, we conducted a Pearson's chi-square test on the activity code frequencies and examined the standard residuals. 
Additionally, we used a mixed-effects modeling approach, including an interaction term between each tool and action, following the analysis approach from the formative user study (Section~\ref{sec:formative-user-study-analysis}).
\begin{equation*}
\texttt{Total Action Time} \sim \texttt{Tool} + \texttt{Action} + \texttt{Tool} \times \texttt{Action} + (\texttt{1|Participant})
\end{equation*}

We also conducted a qualitative analysis of the chat logs. 
After each interview, the first author extracted chat messages from each tool, resulting in 257 messages. 
Using the same single-author coding and validation procedure as in the spreadsheet analysis, the codebook validation achieved a Cohen's $\kappa$ score of 0.80, indicating substantial agreement~\cite{landis1977measurement}.
During the analysis, we identified six non-substantive confirmation messages (e.g., "Yes" or "This looks good") that were excluded due to their lack of qualitative insight, allowing the analysis to focus on substantive interactions. 
To explore differences in the types of messages sent between the two tools, we performed a Pearson's chi-square test on the message codes and examined the standard residuals.

\paragraph{Semi-Structured Interview Analysis}
To understand participants' experiences creating spreadsheets (RQ7), we transcribed their responses to the semi-structured interview questions about their tool usage. 
Two authors conducted a qualitative analysis of the transcripts, following the same procedure used in the template study (see Section~\ref{sec:template-analysis-analysis}) and the formative user study (see Section~\ref{sec:formative-user-study-analysis}).
The IRR of the codebook using Cohen's $\kappa$ was 0.79, indicating substantial agreement~\cite{landis1977measurement}. 
The first author found no new codes after applying the codebook to the remaining data.

\paragraph{Post-Task Questionnaire Analysis}
To study participants' experiences creating spreadsheets (RQ7), we conducted a quantitative analysis of the post-task questionnaire data. 
We calculated an aggregated SUS score using the standard SUS scoring procedure~\cite{lewis2018system}.
We then used Wilcoxon signed-rank tests to compare the aggregated SUS scores, individual TLX items, and individual conversational quality items between \tool and \added{the baseline}. 
\added{For constructs with five or more comparisons, we applied Benjamini-Hochberg corrections, which affected only the \emph{p}-values of the NASA TLX items.}

\input{tables/spreadsheet-results}

\subsection{Results}
\label{sec:evaluation-user-study-results}
\added{During the study, no participant completed any task}, as anticipated due to the difficulty of the task. 
However, P1, P4, and P18 progressed to summarizing the total counts of grades in the gradebook task using \tool ($N=2$) and \added{the \baseline} ($N=1$).
We now discuss the results on how \tool affected the quality of the created spreadsheets (RQ5; Section~\ref{sec:evaluation-user-study-results-spreadsheets}), how spreadsheet programmers created spreadsheets (RQ6; Section~\ref{sec:evaluation-user-study-results-actions}), and the experience of spreadsheet programmers (RQ7; Section~\ref{sec:evaluation-user-study-results-experience}).

\subsubsection{Spreadsheet Quality}
\label{sec:evaluation-user-study-results-spreadsheets}
Spreadsheet evaluators expressed a variety of comments on the generated spreadsheets (see Table~\ref{tab:spreadsheet-results}). 
We observed a difference in \added{attitudes towards the spreadsheets created by \tool compared to the \baseline} ($\chi^2=25.4$, $p<0.001$).
\added{
We further elaborate on these findings below.
}

\paragraph{\textbf{\tool produces higher quality spreadsheets compared to the baseline.}}
Across both tasks, spreadsheet evaluators preferred the spreadsheets created by \tool over \added{the baseline} 42 out of 60 times (70\%).
The general preferences of the individual evaluator towards \tool ranged from 50\% to 90\%.
Based on the Bradley-Terry model, \tool had an ability score of $0.85$, indicating that \tool's spreadsheets had 2.3 times \added{higher odds of being preferred over} \added{the baseline}'s (\added{$p<0.001$}).

\added{
Evaluators expressed a marked difference in the spreadsheet \evalcode{polish}, a general-purpose code for the overall completeness of the spreadsheet.
Evaluators were much less negative ($r=-2.4$) for \tool ($p=0.02$) and much more negative ($r=2.8$) for the baseline ($p=0.005$) than expected.
We find support for this in the qualitative data.
}
While both tools received similar positive feedback, such as the spreadsheets having \equote{nicer formatting}{1} or being \equote{filled out}{1, E3, E5} and \equote{complete}{4, E5, E6},
the evaluators noted a distinct lack of polish for \added{the baseline's} spreadsheets, as some participants \equote{couldn't get anywhere with the task}{4}. 

\paragraph{\textbf{The \baseline's spreadsheets were rated negatively on polish, schema, correctness, and usability, while \tool's spreadsheets had positive or mixed ratings.}}
For \tool, the evaluator comments were strongly positive for \evalcode{polish} (86\%) and more mixed for \evalcode{usability} (56\%), \evalcode{schema} (56\%), and \evalcode{correctness} (50\%).
However, for \added{the baseline}, the comments trended strongly negative for \evalcode{polish} (75\%) and \evalcode{usability} (71\%) and slightly negative for \evalcode{schema} (60\%) and \evalcode{correctness} (63\%).

In terms spreadsheet \evalcode{correctness}, evaluators praised spreadsheets from both tools for having correct formulas, but noted several instances when the spreadsheets did not have functionality associated with the end of the task, likely due to its difficulty. 
However, the spreadsheet evaluators noted that \added{the \baseline's} spreadsheets were \equote{completely wrong}{4}, such as not accounting for weighted averages.
In contrast, evaluators noted that the \tool spreadsheets also contained errors that were easy to fix ($N=4$) such as \equote{chang[ing] the range}{4}.

Spreadsheet evaluators discussed the \evalcode{usability} of the spreadsheets. %
They described instances where the spreadsheets of both tools were \equote{transparent}{3} on how the calculations were derived.
\added{
However, there were fewer comments ($N=4$, $r=-1.27$) of the baseline's spreadsheets being usable ($p=0.20$).
}
While the spreadsheets from both tools contained unexplained columns or values, \added{the baseline's} spreadsheets often contained hard-coded values ($N=4$) or complicated formulas, causing evaluators to struggle to understand them ($N=2$).

Finally, in terms of how the spreadsheet represented the information in the table \evalcode{schema}, the evaluators had mixed comments for \tool (56\%) and slightly negative comments for the baseline (60\%).
Both tools received positive comments on \equote{the overall columns mak[ing] sense}{6} and negative comments on creating spreadsheets where the \equote{task requirements are not there}{6}. 
However, the evaluators said only \added{the baseline} created tables where the units did not make sense ($N=3$).


\mybox{
\faArrowCircleRight\xspace\textbf{Key findings (RQ5):}
\added{
\tool affects the quality of spreadsheets by producing better and more polished spreadsheets compared to the \baseline, leading to a strong preference of spreadsheets made by \tool.
}
Although the accuracy of \tool received mixed feedback, many of the errors could easily be fixed.
In contrast, the feedback on \added{the baseline's} tables was generally negative on their polish, schema, correctness, and usability.
}

\input{tables/chat-results}

\begin{figure}[t!]
\centering
\includegraphics[trim=0 1000 0 0, clip, width=0.8\linewidth, keepaspectratio]{figures/evaluation_study-activity-figure-overview.pdf} 
\caption{
The total number of seconds spent on different CodeRec User Programming States taxonomy~\cite{mozannar2024reading}  actions across all programmers for \tool (orange) and \added{the \baseline} (blue).
}
\label{fig:evaluation-study-activity-overview}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[trim=225 700 225 25, clip, width=0.9\linewidth, keepaspectratio]{figures/evaluation_study-activity-figure.pdf} 
\caption{
A timeline of the participants' activities based on the CodeRec User Programming States (CUPS)~\cite{mozannar2024reading} for \tool (top) and for \added{the \baseline} (bottom).
}
\label{fig:evaluation-study-activity}
\end{figure}

\subsubsection{Spreadsheet Programmer Activity}
\label{sec:evaluation-user-study-results-actions}
\added{
We provide an overview of the time spent in each programming state based on the CUPS taxonomy (Figure~\ref{fig:evaluation-study-activity-overview}) and a summary of chat messages (Table~\ref{tab:chat-results}). 
\added{Participants manually sent 131 chat messages to the baseline and 82 to \tool.}
As expected, \tool programmers collectively spent 61.0 more minutes (3,657 seconds) waiting for suggestions compared to \added{the baseline}, averaging an additional 2.7 minutes (164.2 seconds) of waiting per participant ($p<0.001$). 
We also observed significant differences in overall activities ($\chi^2=31.1$, $p<0.001$) and the types of chat messages sent ($\chi^2=21.0$, $p<0.001$) between the tools.}
We elaborate on these differences below.

\paragraph{\textbf{Programmers using \tool focus more on the requirements of the problem compared to the baseline.}}
Based on Figure~\ref{fig:evaluation-study-chats}, we observed a much higher presence of \evalcode{requirements} messages ($r=2.2$) sent to \tool ($p=0.03$) and a smaller amount ($r=-1.7$) of these messages sent to the baseline ($p=0.09$) than expected.
Examining the CUPS activity (Figure~\ref{fig:evaluation-study-activity}), participants spent longer continuous periods crafting their prompts, even though the total time spent prompting was similar across the two tools. 
\added{This indicates that focusing on spreadsheet requirements can be labor-intensive, requiring sustained bursts of effort.}

\paragraph{\textbf{Programmers using the baseline focus more on problem solving than compared to programmers using \tool.}}
Participants collectively spent \added{an additional 31.1 minutes (1,871 seconds)} thinking or verifying suggestions while using the baseline.
This translated into a \added{1.9 minute (112 seconds) reduction in} thinking about spreadsheet programming actions or verifying the code while using \tool relative to the baseline ($p=0.007$).
\added{In addition, 32 of the 39 accepted suggestions from \tool were \evalcode{high-level commands} (see Figure~\ref{fig:evaluation-study-chats}),} indicating that the suggestions offloaded some of the problem solving from the programmer.


\paragraph{\textbf{Programmers using the baseline focus more on implementation details than compared to programmers using \tool.}}
While using the baseline, programmers spent an additional \added{35.0 minutes (2,100 seconds)} writing new functionality across all participants.
Study participants wrote more new functionality ($r=3.0$) with the baseline ($p=0.003$) and less ($r=-3.4$) with \tool ($p<0.001$) than expected, resulting in an additional 2.0 minutes (118 seconds) of implementation while using the baseline ($p=0.07$).
In addition, the programmers sent \added{significantly more \evalcode{low-level command} messages ($r=2.8$) using the baseline ($p=0.006$) and less ($r=-2.1$) for \tool ($p=0.04$) than expected (see Figure~\ref{fig:evaluation-study-chats})}.
Finally, based on Figure~\ref{fig:evaluation-study-activity}, the participants went through multiple rounds of iterations to create prompts and implement table functionality while working with \added{the baseline}.
Although study participants spent less continuous time crafting prompts, they spent longer periods of time writing new functionality, indicating that working with \tool required more intensive manual effort.

\begin{figure}[t!]
\centering
\includegraphics[trim=20 1550 25 20, clip, width=0.90\linewidth, keepaspectratio]{figures/evaluation_study-chats-figure.pdf} 
\caption{
The frequency of the different types of chat messages manually sent by participants between \tool and \added{the \baseline}.
The frequency of the chat message type is displayed within its bar.
}
\label{fig:evaluation-study-chats}
\end{figure}

\mybox{
\faArrowCircleRight\xspace\textbf{Key findings (RQ6):} 
\added{
\tool affects how programmers creates spreadsheets by allowing programmers to focus more on the requirements of the spreadsheet instead of problem solving and implementation.
}
Programmers using \tool overall spend less time thinking about next steps and verifying suggestions, but spend more time waiting for suggestions.
They also write less new functionality.
Messages sent to \tool are less related to low-level commands and more related to requirements, while the suggestions are high-level commands.
}

\input{tables/survey-results}

\subsubsection{Experience of Creating Spreadsheets}
\label{sec:evaluation-user-study-results-experience}
\added{The questionnaire results are summarized in Table~\ref{tab:survey-results}. 
Overall, \tool was rated higher than the baseline across all conversational quality, NASA TLX constructs, and SUS scores. 
The mean SUS score for \tool was 74.0, indicating good usability, compared to the baseline's mean score of 59.8, which suggests OK usability~\cite{bangor2009determining}.}
Out of 20 participants, 14 preferred \tool over the baseline, and 16 preferred \tool if response times were equal. 
Below, we discuss participants' experiences while developing spreadsheets with \tool and the baseline, providing insight on the reasons for their preferences.

\paragraph{\textbf{\tool proactively performs actions on its own, while the baseline defers more control to the programmer.}}
Participants observed \tool being more \evalcode{proactive} ($N=9$) in performing actions which \pquote{narrowed down}{11} the problem space and \pquote{took out the need for [programmers] to think}{19}.
\added{This resulted in lower mental ($p<0.05$) and physical ($p=0.12$) demand compared to the baseline (see Table~\ref{tab:survey-results}).}
In contrast, \added{the baseline} delegated more control to programmers by providing \evalcode{direct manipulation} ($N=11$) to control actions performed on the spreadsheet in the chat window (e.g., accept action and undo buttons).
It also provided \evalcode{more information} ($N=5$) on how to approach the task (e.g., more explanations on how to solve the problem (P9) and multiple example tables (P2)), causing it to generate longer answers and making it difficult to read (P1).
As a result, P11 compared \added{the baseline} to a \pquote{Google search engine, just bringing up information that was helpful to a bit}{11}.

The participants expressed different opinions on which approach they preferred. 
Some participants appreciated the approach of \tool in directly executing actions since it was more efficient (P1, P5, P10, P13, P15, P20): \pquote{I find it laborious and a little too repetitious to continually have to tell it, 'Yes, I want you to do what I just told you to do'}{20}.
However, many participants preferred the controls that \added{the \baseline} provided in the chat window (P3, P11, P12, P15, P16, P18, P19) since it was important to \pquote{feel like you're in control}{19} to maintain ownership over the spreadsheet.
P20 also suggested the addition of a stop button to terminate the agent's actions midway through.
While P6 and P10 both described the actions of \tool as going \pquote{above and beyond}{6, P10}, only P10 appreciated how it thought about \pquote{those details that are overlooked}{10}.
However, P6 felt this was \pquote{invasive}{6} and an \pquote{overreach}{6}: \pquote{If I ask [the AI] something, I feel like I need it to be very specific, and I'd want the tool to also be that specific}{6}.

\paragraph{\textbf{Interactions with \tool were more conversational and collaborative, while interactions with the baseline were more efficient.}}
Participants found \tool to be more \evalcode{collaborative} in its approach ($N=11$), which required conversational interaction throughout developing the spreadsheet.
Meanwhile, study participants described \added{the baseline} as more \evalcode{straightforward} in its interaction ($N=3$) and required less programmer input to perform actions in the spreadsheet.
Although several participants felt positively about \tool's approach (P3, P10, P17), others felt that it was cumbersome (P2, P16).
Some participants found \tool to require \pquote{more input}{19} which could make using it more \pquote{taxing}{2} because the programmers \pquote{had to think about what to put}{16} in the spreadsheet.
Other participants found that the questions \tool asked were more \pquote{helpful}{2} and \pquote{engaging}{10} and resulted in personalized spreadsheets (P2, P7).
For \added{the \baseline}, participants found its approach more \pquote{technical}{7, P14}, since programmers \pquote{have to know what you're doing to guide [\added{the baseline}] where you want it to go}{11}.

\pblockquote{[\added{The baseline}] was more of a command interface. It made me feel as though I was giving it things to do, and it was working.
For me, [\tool felt] like we were working collaboratively...
It's going step by step, making sure that I'm on point with every step that it's doing and making sure it's communicating. 
The way the tool was designed did not take anything away from me. 
What it really took away from me was headaches involved with spreadsheet creation and management and data points.}{10}

\paragraph{\textbf{\tool provides better results for more complex tasks, while the baseline performs well for simple tasks.}}
\added{Participants reported feeling more successful in accomplishing the task using \tool compared to the baseline ($p=0.12$, see Table~\ref{tab:survey-results}).}
Many participants felt that \tool produced \evalcode{better outcomes} in the main task and performed the required actions better ($N=14$).
Study participants described \tool performing the exact actions they wanted it to do (P6, P10, P18, P19, P20), which made it easier to control or modify the resulting spreadsheet (P11, P12, P14) and perform more actions (P13).
Participants felt that they \pquote{got quite a lot further into the task}{4} and that the spreadsheet \pquote{look[ed] more reliable}{11}.
As a result, P3 noted that this \pquote{eliminates trial and error because it builds it right [the] first [time]}{3}.
Despite the observed \evalcode{lag} ($N=7$), participants felt \tool \pquote{performed beyond...expectations}{8} and was still willing to use the tool:
\pblockquote{Even though [\tool] took a little bit longer for the generation, the quality of the output was so much better that I was quite happy to wait that little bit extra.}{4}

Meanwhile, numerous participants ($N=9$) described instances in which using \added{the baseline} resulted in \evalcode{worse outcomes} on the main task.
Study participants noted times where \added{the \baseline} did not complete the action the participant wanted (P1, P4, P8, P12, P17).
However, P4, P6, P7, P10, P13 noted that \added{the baseline} performed well on \pquote{simple, structured tasks}{4}, such as the tutorial task, and produced a less complex spreadsheet (P7).
As a result, P10 and P13 felt that they needed to break down the problem for \added{the baseline}, rather than letting the tool problem solve on its own: \pquote{I found that [with TableTalk] I tried to throw everything all at once...versus [for \added{the baseline}] I tried to feed it a little bit less, like one piece at a time of information}{13}.

\paragraph{\textbf{\tool provides better conversational interactions.}}
For conversational quality metrics~\cite{finch-choi-2020-towards} (see Table~\ref{tab:survey-results}), \added{\tool had improved scores in \emph{relevance} ($p=0.009$) and \emph{proactivity} ($p=0.07$) compared to \added{the baseline}}.
In addition, study participants also noted that \tool provided \evalcode{higher-quality conversation} ($N=9$).
The participants felt that the tool better understood their intentions (P4, P8, P20), making the overall conversation smoother (P7, P14): \pquote{I had a lot of typos in that prompt and it...understood my demands very well}{8}.
Study participants also felt that \tool had \pquote{more humanness}{4} and, as a result, \pquote{on an emotional level...I could understand [it] a lot more}{14}.
In contrast, participants observed that \added{the \baseline} provided \evalcode{lower-quality conversation} ($N=10$).
Multiple participants felt that the tool misunderstood their intentions (P3, P8, P9, P11, P18) or had difficulty understanding why the tool performed certain actions without clear explanations (P1, P14), which \added{increased frustration ($p<0.05$)} and \added{decreased SUS scores ($p<0.05$)} when using the baseline. 


\paragraph{\textbf{\tool's suggestions are more relevant to the programmer's context.}}
16 of 20 participants used \tool's suggestions, while 4 participants used the suggestions from \added{the \baseline}, implying that \tool suggestions were a useful feature.
Study participants commented on \tool having \evalcode{pertinent suggestions} ($N=14$).
Even if the suggestions were not used in the interaction, the participants felt that they served as a useful reference to drive future interactions (P3, P4, P10, P11, P16) since \pquote{sometimes [\tool's suggestions] gives you things you might not have even thought about}{17}.
One participant also mentioned that the suggestions contained \pquote{language that the AI bot...wanted to use...[to] train me this is the way I word things}{4}.
While \tool's suggestions were generally relevant, P13 felt they could be even more individualized to their personal circumstances.
Meanwhile, study participants commented on \added{the baseline} having \evalcode{generic suggestions} ($N=4$), causing them to ignore the suggestions (P1, P3) or find them unhelpful (P9).

\mybox{
\faArrowCircleRight\xspace\textbf{Key findings (RQ7):}
\added{Using \tool affects the programmer's experience} by taking a more proactive and collaborative approach, which decreases mental demand compared to \added{the \baseline}.
While participants felt that \tool achieves better results in more complex tasks, they had mixed reactions about its approach because it reduces user control in implementing the spreadsheet.
}

\section{Discussion}
\label{sec:discussion}
Based on our findings from \tool's evaluation study, we discuss the design implications on future human-agent collaboration and spreadsheet programming systems (Section~\ref{sec:design-implications}). 
We then consider the impact of agents on spreadsheet programming (Section~\ref{sec:agentic-spreadsheet-programming}) and conclude with the limitations of our studies (Section~\ref{sec:limitations}).

\subsection{Design Implications}
\label{sec:design-implications}

\subsubsection{Apply Scaffolding Techniques to Complex Tasks}
While \tool's flexible scaffolding approach improved task outcomes over \added{the baseline}'s non-scaffolded approach, scaffolding techniques are not a silver bullet for all programming problems.
One challenge was that the \evalcode{proactive} and \evalcode{collaborative} nature of \tool was taxing, especially for simple tasks they could perform themselves:
\pblockquote{Because at the time when [TableTalk] asks questions, it still doesn't know how complex the task is when it starts building. 
For the simple stuff, I would prefer simplistic answers with not many options, kind of like, `Here's what it is. Do you like it? Yes, no.'}{3}
This suggests that scaffolding is most effective for complex problems that spreadsheet programmers \added{struggle to solve without adequate support}, such as debugging~\cite{bajpai2024lets}, but may introduce excessive overhead for simpler tasks. 
This aligns with findings on AI programming assistants like GitHub Copilot, which show that programmers often know how to solve the problem and use AI to complete tasks more efficiently~\cite{barke2023grounded, liang2024large}.

Future AI programming systems should balance light-weight interventions, such as AI code completion, with more intensive scaffolding from language agents. 
Future work could develop heuristics and automatic detection methods to determine when each technique is most appropriate. 
Additionally, since scaffolding tools are designed to aid novices~\cite{reiser2018scaffolding}, future studies could replicate the evaluation with both expert and novice spreadsheet programmers to better understand their differing support needs.

\subsubsection{Provide Direct Manipulation Interfaces to Control the Agent}
A key challenge identified by participants in \tool was the lack of \evalcode{direct manipulation} interfaces to control the agent, a feature that \added{the baseline} offered. 
This was the most frequently cited reason for preferring the baseline, with three participants mentioning it in their rationale.
This indicates that the direct manipulation provided by the Excel canvas alone is insufficient when working with a language agent. 
Programmers desire interfaces to assert control over the agent, such as the ability to stop or undo its actions. 
This aligns with \citet{amershi2019guidelines}'s human-AI interaction guidelines, which highlight the importance of global controls for AI systems.
Therefore, we recommend future human-agent systems to offer direct manipulation interfaces to support stopping and undoing agent actions.

\subsubsection{Make the Level of Proactivity Customizable for the Programmer}
While evaluation study participants appreciated having \evalcode{direct manipulation} interfaces to accept agent actions, several also valued \tool's \evalcode{proactive} approach in performing actions. 
Some felt that direct manipulation interfaces to accept agent actions were repetitive when the action had already been requested.
To address this, human-agent collaboration systems should allow programmers to customize the degree of agent proactivity, enabling a balance between automation and user control based on individual preferences.

Future work should explore automatic methods to determine when direct manipulation interfaces should be offered to accept agent actions.
Participants noted that this preference could vary by task context or programmer expertise.
For example, P15 expressed a strong preference for the direct manipulation interface to accept agent actions and described the importance of expert spreadsheet programmers to perform actions themselves, while P6 described business contexts where proactivity would be unwelcome:
\pblockquote{Maybe I'm just speaking for me, for the older users that have got 20 plus years of doing it yourself. They might be like...`No, I'm doing this' [when they receive a suggestion].}{15}
\pblockquote{I wonder, though, if you've got a really defined business and your table is very secure in its formatting. I didn't like the fact that it seemed to make changes to my table because I think I'd probably go crazy if I had a really robust table where I'm just entering for that month, and I don't want changes.}{6}

\subsubsection{Incorporate Features to Evaluate the Agent's Output to Minimize Overreliance}
In the user study, participants engaged in minimal debugging or testing behaviors for both \tool and \added{the baseline}. 
However, spreadsheet evaluators identified formula errors in the outputs from both tools, indicating that participants over-relied on AI-generated formulas. 
This reflects a broader concern with AI programming assistants, particularly for novice users~\cite{becker2023programming, kazemitabaar2023novices}.
To address this, future human-agent collaboration systems should incorporate features that help evaluate the AI's output, such as generation quality indicators. 
These features can promote appropriate trust between humans and AI~\cite{wang2024investigating}, encouraging users to critically assess and validate AI-generated content.

\subsubsection{Promote Code Comprehension for Better Debugging}
We observed that a lack of support for code comprehension caused some participants to struggle to debug the generated Excel formulas. 
End-user programmers often use haphazard debugging strategies, which can introduce further errors~\cite{ko2011state}. 
For example, P9 requested \tool to generate a formula that displayed "Excellent" for an overall grade of 100\%. 
Instead of inspecting the formula to identify the error, the participant repeatedly described the desired behavior: \pquote{if score is below 100, do not display excellent}{10}.
In response, \tool generated the formula:
\texttt{=IF(SUM(B2:H2)=100, "Excellent", SUM(B2:H2))},
which displays "Excellent" if the sum of grades equals 100 and the summed scores otherwise. 
However, since the resulting column values did not change, P9 was unaware that \tool had adapted the formula. 
This led P9 to prefer the baseline over \tool, citing that it felt \pquote{more advanced}{9}.
To address this, we recommend that human-agent collaboration systems promote code comprehension by providing detailed explanations of generated formulas whenever they are added to the spreadsheet. 

Future research should explore techniques to enhance code comprehension of spreadsheet formulas. 
Approaches like \citet{ferdowsi2023coldeco}'s COLDECO have tackled this by decomposing formulas into helper columns, highlighting representative rows in a summary table, and providing natural language explanations of functions.
Additionally, as demonstrated by P9s experience, end-user debugging processes can be ineffective and could benefit from scaffolding tools. 
Future work could investigate whether LLM-augmented scaffolding techniques---similar to \citet{bajpai2024lets}'s ROBIN for traditional programming---might effectively guide users through spreadsheet debugging.

\subsection{Language Agents with Proactive Support: The Future of Spreadsheet Programming Assistance?}
\label{sec:agentic-spreadsheet-programming}
In the machine learning literature, numerous approaches have been proposed for language agents---proactive LLMs that perform actions in their environment---to carry out software engineering tasks autonomously~\cite{yang2024swe, li2024sheetcopilot, tufano2024autodev}. 
However, these works often overlook the role of humans. 
This leaves the dynamics of how programmers collaborate with language agents underexplored, particularly in the context of spreadsheet programming.

\tool is a prototype system that demonstrates human-agent collaboration in spreadsheet programming. 
Our results show that language agents can significantly benefit spreadsheet programmers by addressing key challenges they face.
Returning to \tool's design principles (Section~\ref{sec:design-goals}), its flexible scaffolding (DP1, DP2) enabled programmers to focus on the \evalcode{requirements} of the spreadsheet rather than on low-level implementation details by reducing the need to send \evalcode{low-level commands}. 
Additionally, \tools proactive suggestions alleviated the burden of problem-solving by automatically providing and sending \evalcode{high-level commands}. 
This addressed challenges such as \formativecode{minimal guidance on using advanced Excel features} and \added{\formativecode{limited support on structuring problems}}, as identified in the formative study and prior literature~\cite{pirolli2005sensemaking, chalhoub2022s, baltes2018towards, liang2022understanding, ko2011state, nardi1991twinkling}.
Furthermore, the incremental development approach (DP3) resulted in less manual adaptation, addressing templates being \formativecode{hard to adapt}. 
Collectively, these features reduced the mental demand of building spreadsheets and enabled programmers to produce \evalcode{better outcomes}, with higher-quality and more \evalcode{polished} spreadsheets that were preferred over those from \added{the baseline}.

Regardless of the tool, many participants in the evaluation study expressed their excitement for proactive AI assistance to improve spreadsheet programming, as they found the automation helpful ($N=6$).
Participants described the time-saving effects (P19) and effort reduction (P17) that AI provided, such as eliminating the need to reference online documentation (P3) and reducing small spreadsheet errors (P10).
These benefits align with those observed in traditional code completion assistants~\cite{barke2023grounded, liang2024large}.
Beyond the productivity improvements, participants (P8, P15) also noted the educational potential of these tools, which future work could investigate:
\pblockquote{[Having an AI] just asking what you want, would, for me personally, free up time for the senior analysts.
You've got a junior analyst...sitting with you, and they're asking all the time, `How do I do this? How do I do that?'
You give them a course to do in Excel...but they're still asking, `How do I do this?'
It's easy now [since] having an [AI] is better at training someone than a training course. [Before AI], if you're training, you have to take it in, write notes, and keep going back.}{15}

Despite the excitement, participants expressed concerns that proactive AI assistance might undermine their ownership of the spreadsheet: \pquote{[I want the AI to] make it feel a bit more personal, that it's my co-pilot on my journey, and this is my journey to create this one little project among many others. It's important to me}{19}.

These varying---and sometimes conflicting---perspectives highlight the need for future research in this area by both HCI and software engineering communities. 
This is especially important as agentic programming systems become increasingly popular, with emerging tools like Devin~\cite{devin2025devin} gaining traction in traditional programming.
Our goal with \tool, its evaluation study, and the resulting design implications is to provide a prototype tool and insights into human-agent collaboration in spreadsheet programming. 
These findings can serve as a foundation for future advancements in human-agent systems.

\subsection{Limitations}
\label{sec:limitations}
We now discuss the limitations of \tool, the template study, and formative and evaluation user studies.

\paragraph{Impact of \tool on Spreadsheet Programming}
The current design of \tool serves as a scaffolding tool to assist spreadsheet programmers in creating spreadsheets. 
We do not claim that all spreadsheet programming tasks should be fully automated. 
Instead, we advocate for a collaborative approach where LLMs assist programmers in tackling the most challenging aspects of spreadsheet programming, a benefit demonstrated by the evaluation study results.
However, as with any LLM-based technology, potential negative impacts deserve further investigation. 
In traditional programming, tools like GitHub Copilot can generate code that is difficult to understand or debug~\cite{liang2024large}. 
Spreadsheet programmers may face similar challenges, such as overlooking critical errors generated by \tool or experiencing reduced learning due to overreliance on the tool. 
Future work should explore these risks to ensure that such tools effectively support users without undermining their skills or accountability.

\paragraph{Study Limitations}
Our studies have several limitations. 
First, the research team \added{likely had confirmation biases that could influence the qualitative analyses}. 
We mitigated this threat by involving multiple coders to develop the codebook and measuring inter-rater reliability.
Additionally, the template selection process (e.g., studying a single domain) may have introduced biases, so we do not claim that our sample of templates is representative of all spreadsheets. 
Instead, this study serves as the foundation for the design of \tool.
Finally, in the evaluation, we focused on two tasks, but these may not encompass the full range of spreadsheet programming activities. 
Furthermore, our user study participants---English speakers from the United States, Canada, Australia, and the United Kingdom----are not fully representative of all spreadsheet programmers. 
While our findings provide insights into spreadsheets and AI-assisted programming, these factors limit the generalizability of our results.
Future work could explore the effects of \tool in different contexts, with more complex tasks or larger-scale datasets, such as through a longitudinal field study. 

\paragraph{Researcher Positionality}
\added{
As researchers, our lived experiences and positionality influence our perspectives and, consequently, how we approach our research~\cite{savin2023qualitative, holmes2020researcher}. 
Below, we outline aspects of our positionality that are most relevant to this study.
The research team comprises computer science researchers, including full-time scientists, research fellows, and research interns on the Excel team, with expertise in human-computer interaction, software engineering, and artificial intelligence. 
We approach spreadsheet tools from software-oriented, user-centered, and machine learning perspectives, which may lead to an underemphasis on cultural or sociological aspects of spreadsheet use.
While our close ties to Excel and its research and development provide a deep understanding of spreadsheet programming tools, we acknowledge that this proximity can influence our experiments and introduce implicit biases in how we frame Excel's strengths and limitations. 
Additionally, our affiliation with Excel might create a power dynamic in user studies, making participants reluctant to provide critical feedback.
To address potential courtesy bias, we requested participants were encouraged to discuss the potential limitations of each tool.
}

\section{Conclusion}
Spreadsheet programming is a challenging activity that requires programmers to schematize data, apply problem-solving skills to break down complex tasks, and effectively use APIs to implement spreadsheet formulas.
Research has shown that LLMs can guide programmers through complex tasks through scaffolding approaches, which re-structure and decompose tasks in a way such that they can accomplish them independently.
However, to our knowledge, no prior work has explored scaffolding specifically for spreadsheet programming.

In this work, we address the challenges of spreadsheet programmers by leveraging the increasing capabilities of LLM agents to solve complex problems and generate spreadsheet formulas.
We introduce \tool, a language agent-based system that helps spreadsheet programmers create spreadsheets, informed by a user study of 7 spreadsheet programmers and a study of 62 Excel spreadsheet templates.
\tool implements a flexible scaffolding approach through language agents to help programmers build spreadsheets incrementally.
It has access to tools (i.e., external modules that are used to complete specific operations) to help implement atomic spreadsheet features that can be composed to build spreadsheets.
We find that compared to \added{the \baseline}, \tool enables programmers to build more polished spreadsheets and reduces mental load.
It allows programmers to focus more on the requirements of the spreadsheet rather than on the problem solving and implementation details.
Therefore, \tool serves as a proof-of-concept that LLM-based scaffolding is a promising avenue to support spreadsheet programmers through complex tasks.
To facilitate replication of this work, we include the protocols for the formative and evaluation user studies, the spreadsheet template dataset, all codebooks used across the analyses, and a video demo of \tool in the supplemental materials~\cite{supplemental-materials}.

\begin{acks}
We thank our study participants for their insights.
We also thank Albert Dodson for technical assistance in implementing \tool, as well as Brad A. Myers and Mukhul Singh for valuable insight into the project.
Finally, we extend our gratitude to Soham Pardeshi, Millicent Li, Christopher Kang, and Michelle Lin for the feedback on \tool.
Last but not least, we give special thanks to Mei \meiicon, an outstanding canine software engineering researcher, for providing support and motivation throughout this study.
Jenny T. Liang conducted this work as an Applied Scientist Intern in Microsofts PROSE Team (\url{https://www.microsoft.com/en-us/research/group/prose}).
She is supported by the National Science Foundation under grants DGE1745016 and DGE2140739.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}

\appendix
\section{Formative User Study}












\subsection{Food Drive Task Tracking Instructions}
You are working with your colleagues, Arya and Zarah, to coordinate project tasks for your local food drive. Your goal is to understand who is doing the most work between you three for the food drive. Here are the tasks that need to be tracked and who is assigned to them.
\begin{enumerate}
    \item Collecting food donations: Done once a week by Zarah and Arya. This takes 3 hours to complete.
    \item Coordinating volunteers: Done by you every 2 weeks. This takes an hour to complete.
    \item Counting food inventory: Done by you every two days. This task takes 45 minutes to complete.
    \item Donation box placement: Done by you once quarterly. This takes about 3 hours to complete.
    \item Donation delivery: Done twice a week by Arya. This task takes about 30 minutes to complete. 
    \item Local food bank distribution planning: Done once a week by Zarah. This takes about an hour.
    \item Local shelter distribution planning: Done once a week by Arya. This takes about an hour.
    \item Marketing on social media: Done by Arya once a month. This takes 2 hours to complete.
\end{enumerate}

\subsection{Design Probe}
Imagine you had a system that could help you create Excel tables from scratch. Suppose this system had access to your working context, such as your files and your browser. Also suppose that this system could generate Excel formulas correctly every time.

\subsubsection*{Prototype Screenshots}
To view the screenshots of the different prototypes that were  to participants during the design investigation in the formative study, refer to Figures~\ref{fig:prototype1},~\ref{fig:prototype2}, and~\ref{fig:prototype3}.

\begin{figure}[t!]
\begin{center}
\includegraphics[trim=0 175 0 175, clip, width=\linewidth, keepaspectratio]{figures/Prototype1.pdf} 
\end{center}
\caption{A screenshot shown to the formative participants during the design probe for Option 1: Code snippets on the Excel canvas.}
\label{fig:prototype1}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[trim=0 170 0 170, clip, width=\linewidth, keepaspectratio]{figures/Prototype2.pdf} 
\end{center}
\caption{A screenshot shown to the formative participants during the design probe for Option 2: Chat with an AI chatbot.}
\label{fig:prototype2}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[trim=0 150 0 170, clip, width=\linewidth, keepaspectratio]{figures/Prototype3.pdf} 
\end{center}
\caption{A screenshot shown to the formative participants during the design probe for Option 3: Setup wizard to scaffold table creation.}
\label{fig:prototype3}
\end{figure}

\subsection{Template}
To view the screenshots of the template that was used during the formative user study, refer to Figure~\ref{fig:spreadsheet-template}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{\textwidth}
        \includegraphics[trim=0 275 0 25,clip,width=\textwidth]{figures/Template1.pdf}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[trim=25 250 150 75,clip,width=\textwidth]{figures/Template2.pdf}
    \end{subfigure}

    \caption{
        A screenshot of the Excel spreadsheet template used during the formative user study.
        The template contained two sheets: one titled "Project Tracker" (top) and one titled "Setup" (bottom).
    }
    \label{fig:spreadsheet-template}
\end{figure}

\section{Evaluation User Study}













\subsection{Student Grades Analysis Instructions}
\paragraph{Instructions} Work with the AI tool to develop a spreadsheet that addresses the given scenario. You can think of yourself as a spreadsheet programmer, and the moderator as your client for the spreadsheet. 
While developing the spreadsheet, you should ask questions about the requirements of the spreadsheets. You can think of your own questions or use the results from the AI to think of questions to ask.

\paragraph{Scenario} 
You are a teacher who taught a course in geometry to 3 high school students. Your job is to help the principal (the moderator) count how many times each of the overall grades occur (e.g., the number of students who earned an "Excellent" overall grade) based on graded items in the course.
Details. The scenario includes the following details.
\begin{itemize}
    \item In your course, there were 4 assignments, 2 exams, and 1 participation activity. 
    \item Grades are expressed as a percentage.
    \item The assignments are worth 40\% of the grade; the exams are worth 50\% of the grade; the participation points are worth 10\% of the grade. 
    \item The overall grades go from Excellent (95+\%), Very good (85\%+), Satisfactory (75\%+), Needs improvement (below 75\%).
\end{itemize}
\paragraph{Data} 
There is existing data for the grades of the 3 students. You can ask the principal (moderator) for the specific grades once you work with the AI to develop a table and are satisfied with its schema.
Spreadsheet quality. Your final spreadsheet should be: 1) polished enough for a coworker to reuse with minimal effort to get the same functionality on different data and 2) be visually appealing.

\subsection{Hours Worked \& Client Payments Tracking Instructions}
\paragraph{Instructions}
Work with the AI tool to develop a spreadsheet to address the given scenario. You can think of yourself as a spreadsheet programmer and the moderator as your client for the spreadsheet. 
While developing the spreadsheet, you should ask questions about the requirements of the spreadsheets. You can think of your own questions or use the results from the AI to think of questions to ask.

\paragraph{Scenario}
You are an assistant to an entrepreneur (the moderator). The entrepreneur needs help calculating how much money she earned from different kinds of tasks done in September (e.g., earning \$500 from consulting-related work) by working with clients.

\paragraph{Details} 
The scenario includes the following details.
\begin{itemize}
    \item The entrepreneurs work has different hourly rates based on the service provided. 
    \item When the job takes more than 10 hours, each hour past the first 10 hours costs 10\% more than the hourly base rate. 
    \item Some clients also receive discounts at a flat amount (e.g., a \$100 discount).
    \item The services provided were completed on different dates.
\end{itemize}

\paragraph{Data}
There is existing data for the 4 clients. You can ask the entrepreneur (moderator) for the specific data once you work with the AI to develop a table and are satisfied with its schema.

\paragraph{Spreadsheet quality}
Your final spreadsheet should be: 1) polished enough for a coworker to reuse with minimal effort to get the same functionality on different data and 2) be visually appealing.  



\end{document}
