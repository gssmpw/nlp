\begin{table*}
    \centering

    \begin{tabular}{l|cccc|cccc}
        %\hline
        & \multicolumn{4}{c|}{\s{Automated Metrics}} & \multicolumn{4}{c}{\s{Gold Annotations}} \\
        {Model}                                           & $A$        & $G$      & $D$      & $Faith$ & $A_g$     & $G_g$    & $D_g$    & $F_g$    \\
        \hline           
        \footnotesize{\texttt{qacg}}                      & 0.89       & 0.69     & 0.70     & 0.88    & 0.99      & 0.85     & 0.91     & 0.76     \\
        \footnotesize{\texttt{gpt-4-turbo-3-shot}}        & 0.92       & 0.70     & 0.77     & \s{0.99}& 0.98      & \s{0.97} & \s{0.96} & \s{0.93} \\
        \footnotesize{\texttt{qlora-mistral-instruct}}    & 0.95       & 0.75     & 0.80     & 0.95    & \s{1.00}  & 0.96     & \s{0.96} & 0.90     \\
        \footnotesize{\texttt{t5\_sm\_diverse\_7\_beams}} & 0.95       & 0.74     & 0.80     & 0.91    & 0.99      & 0.89     & 0.95     & 0.79     \\
        \footnotesize{\texttt{t5\_sm\_multi-claim}}       & \s{0.96}   & \s{0.76} & \s{0.82} & 0.95    & \s{1.00}  & 0.91     & 0.94     & 0.88     \\
        \hline
    \end{tabular}
    \caption{\label{model-comparison-bin}
        Model comparison on means of model metrics over all samples (or over all annotated samples). In bold are the best values for each metric.}
\end{table*}