%!TEX ROOT=../acl2023.tex
\begin{comment}
    \section{Data Analysis}\label{sec:data}
In order to perform practical experiments on the task of claim generation, we need to establish sets of gold data for such task.
For this purpose, we collect two sorts of datasets -- a \textit{dedicated} dataset annotated in a controlled \q{lab} environment with annotators being instructed to follow the exact definition of the task, and a couple of \textit{domain-related} datasets featuring the real-world sources of factual claims in their various forms.
\end{comment}

\section{\ds{} Dataset}\label{sec:feverfact}
In order to obtain a dedicated dataset, we utilize the \textbf{FEVER Error Correction} data published in~\cite{thorne-vlachos-2021-evidence}.
Despite the dataset being designed for a different task, it releases a set of 17.5K truthful claims \textit{directly extracted} from a sample of sentences of the 2017 Wikipedia corpus.
The annotation of these claims dates back to the WF1a task of the original \textbf{FEVER}~\cite{thorne-etal-2018-fever}, in which the annotators were instructed to extract 2--5 claims from a source sentence, being provided with its neighbouring sentences and page title for context, as shown in Figure~\ref{fig:feverfact} -- hereinafter, we refer to this whole body of text as to the \textit{\ds{} document}.
Years later, the original source sentence each claim was extracted from, however, may not be reproduced from the data, as each sentence is only represented by an integer ID pointing to a mapping file within a deleted Amazon bucket, lost to its authors\footnote{As per direct Slack communication with FEVER authors.}. 

With a simple idea, we manage to reverse-engineer this mapping:
we first group the claims by the ID of their (unknown) source sentence, taking the most common Wikipedia article among each group's FEVER gold evidence.
This should be the article group was extracted from, since all the WF1a claims were annotated to be true and only contain the information from their source sentence, without any additional knowledge.
We then, sentence-by-sentence, scan this article using a \textit{Natural Language Inference (NLI)} model, looking for a sentence which entails the most of the claims in the group, tossing the groups where no claim is entailed by any sentence due to noise in our method.
Doing so using a \texttt{nli-deberta-v3-small}~\cite{he2021deberta,he2021debertav3,reimers-gurevych-2019-sentence} pre-trained CrossEncoder yields near-ideal results -- our annotation on a sample of 2\% (89 sentences, 350 claims) datapoints shows that in 94.4\% cases the \textit{entire} group of claims could be directly extracted from the reverse-engineered source sentence, only using its neighbouring sentences and page title to resolve coreferences.

Thus we release dataset we call \textbf{\ds{}}: 4.4K 3-sentence documents (plus a page title), each annotated with a set of claims that may be extracted from its middle sentence for a total of 17K atomic factual claims.
We split it into train, dev and test set in an 80:10:10 ratio, preventing the same page title from appearing in two different splits.

\subsection{\ds{} recall}
In the wild, an ideal dataset for claim extraction would feature \textit{all} check-worthy claims that can be extracted from given source sentence or even from the whole \ds{} document.
Such data is extremely hard to annotate due to the requirement for claim atomicity\footnote{Each claim describing a single entity or relation, see Section~\ref{atomicity}} -- even relatively simple grammatical paralellisms such as zeugmata or compound sentences can explode the number of relations that can be extracted from the sentence.

Even so, having a large-enough sample of claims matched to a source text would yield a good approximation on which information within the text is check-worthy enough to be extracted.

The original annotators behind data used in \ds{} were tasked to extract 2--5 check-worthy claims from the source sentence, producing a median of 4 claims per source.
To probe whether this number is enough, we perform a Named Entity Recognition (NER) on both \ds{} claims and their source sentences using NameTag~3~\cite{strakova-etal-2019-neural}, to find that the recall of named entity words in FEVER claims is about 67\% (taking source entities as a reference).
While such metric is on the rough side and future annotation is desirable, we conclude that the \ds{} claims cover most of the information and are suitable enough to be used as a reference for reference-based claim extraction metrics (Section~\ref{reference-based}).

\begin{comment}
    
    \subsection{Domain-related datasets}
    While having a gold-labeled dataset for the task of claim generation is certainly desirable, the data sources in the wild may vary dramatically from the simplicity of \ds{} sentences.
    In order to validate our formulation of the claim-generation task and data, as well as of the proposed methods, we pick
    
    In order to perform an exploratory analysis of the effect of summarization methods to extract claims from real-world data we experiment with the following datasets.
    
    \begin{enumerate}
        \item \textbf{XSum}~\cite{narayan-etal-2018-dont} (BBC articles with a summarizing leading sentence) has a general structure of extracting 1 factual claim per document, yielding a solid large-scale pretraining task. Models trained on it should be considered.
        \item \textbf{CLEF CheckThat}~\cite{barroncedeno2020overview} -- verified claim retrieval dataset gives about 10K samples of a single-sentence factual claim and a document it was retrieved from
        %\item \textbf{CNN/DailyMail} dataset contains multiple-sentence summarization. This promotes interest for using each sentence as a separate claim, which, however, is not feasible as the sentences are not self-contained and do reference each other using pronouns etc. Could be used for some fine-tuning further upstream.
        \item \textbf{\ds}\footnote{\url{https://huggingface.co/datasets/ctu-aic/fever-sum}} is a dataset we have extracted from the results of FEVER error correction paper, mapping each of the FEVER claims to the concatenation of Wikipedia abstracts it was extracted from.~\cite{thorne-vlachos-2021-evidence}
    \end{enumerate}
\end{comment}
