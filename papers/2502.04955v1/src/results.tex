%!TEX ROOT=../acl2023.tex
\begin{table*}[h]
    \centering
    \begin{tabular}{@{}l|cccc|cccc@{}}
        Model & \footnotesize{Atomicity}   & \footnotesize{Fluency}    & \footnotesize{Decontext.}    & \footnotesize{Faith.} & \footnotesize{Focus} & \footnotesize{Coverage}  & $F_{fact}$& \tiny{Redundancy\footnoteref{fn:redundancy}}  \\
        \hline
        \footnotesize{\texttt{qacg}}                      & 0.89   & 0.69   & 0.70   & 0.88      & 0.20  & 0.67        & 0.30     & 0.44    \\
        \footnotesize{\texttt{gpt-4-turbo-3-shot}}        & 0.92   & 0.70   & 0.77   & \s{0.99} & 0.21  & \s{0.81}     & 0.34     & \s{0.14}\\
        \footnotesize{\texttt{qlora-mistral-instruct}}    & 0.95   & 0.75   & 0.80   & 0.95      & \s{0.58}  & 0.63    & \s{0.61} & 0.19    \\
        \footnotesize{\texttt{t5\_sm\_diverse\_7\_beams}} & 0.95   & 0.74   & 0.80   & 0.91      & 0.55  & 0.58        & 0.56     & 0.59    \\
        \footnotesize{\texttt{t5\_sm\_multi-claim}}       & \s{0.96} & \s{0.76} & \s{0.82} & 0.95  & \s{0.58}  &0.51   & 0.55     & 0.54    \\
        \hline
        \hline
        \multirow{2}{*}{\footnotesize{\textit{Evaluation method}}}      & {\small{\textit{REBEL}}} & \small{\textit{CoEdIT}} & \footnotesize{\textit{T5}} & \footnotesize{\textit{Align-}} & \footnotesize{\textit{Concat}} & \footnotesize{\textit{Concat}} & \footnotesize& \\
        \noalign{\vskip -5pt}
        &   \tiny{$|rel|\leq1$}    & \tiny{\textit{+Scribendi}} &\tiny{\textit{Decontext}} & \footnotesize{\textit{Score}}        & \footnotesize{\textit{AlignS.}} &  \footnotesize{\textit{AlignS.}}  & &   \\
        \footnotesize{\textit{Validation against human}}      & \textit{0.96} & \textit{0.80} & \textit{0.86} & \textit{0.92}  & \textit{0.23} & \textit{0.22}  && \\
        \footnotesize{\textit{Validation method}}      &\multicolumn{4}{c|}{\footnotesize{\textit{$F_1$ --  higher is better}}}   &\multicolumn{4}{c}{\footnotesize{\textit{Root mean squared error --  lower is better}}} \\ %{$\sqrt{mean~square~error}$ 

    \end{tabular}
    \caption{\label{auto-metrics}
        \textbf{Automated claim metric} averages across model-generated claims on \ds{} test set. The best value for each metric marked bold. Model choices and training procedures described in Section~\ref{sec:methods}, claim quality metrics in Section~\ref{sec:metrics}. Automated scoring methods and their validation against human grading (Table~\ref{gold-metrics}) in italic.}
\end{table*}

\begin{table*}
    \centering
    \begin{tabular}{@{}l|cccc|cccc@{}}
        Model & \footnotesize{Atomicity}   & \footnotesize{Fluency}    & \footnotesize{Decontext.}    & \footnotesize{Faith.} & \footnotesize{Focus} & \footnotesize{Coverage} & $F_{fact}$& ~~~~~~~~~~~~~~~~   \\
        \hline
        \footnotesize{\texttt{qacg}}                      & 0.99   & 0.85   & 0.91   & 0.76   & 0.19     & 0.60             & 0.28     &\\
        \footnotesize{\texttt{gpt-4-turbo-3-shot}}        & 0.98   & \s{0.97} & \s{0.96} & \s{0.93} & 0.24     & \s{0.79}   & 0.37     &\\
        \footnotesize{\texttt{qlora-mistral-instruct}}    & \s{1.00} & 0.96   & \s{0.96} & 0.90   & \s{0.60} & 0.69         & \s{0.64} &\\
        \footnotesize{\texttt{t5\_sm\_diverse\_7\_beams}} & 0.99   & 0.89   & 0.95   & 0.79   & 0.51     & 0.62             & 0.56     &\\
        \footnotesize{\texttt{t5\_sm\_multi-claim}}       & \s{1.00} & 0.91   & 0.94   & 0.88   & 0.47     & 0.50           & 0.49     &\\
        \hline
        \hline
        \multicolumn{9}{c}{\footnotesize{\textit{Inter-annotator agreement (sample of $\sim23\%$ annotated claims, $\leq$ 5 annotators per claim, at least 2)}}}\\
        \footnotesize{\textit{Krippendorff's $\alpha$}} & \textit{0.27} & \textit{0.41} & \textit{-0.01} & \textit{0.53} & \textit{0.75} & \textit{0.64} && \\
        \footnotesize{\textit{GWET's AC1}}              & \textit{0.95} & \textit{0.86} & \textit{0.85}  & \textit{0.83} & \textit{0.80} & \textit{0.66} && \\
        \footnotesize{\textit{\%-agreement}}            & \textit{0.87} & \textit{0.86} & \textit{0.97}  & \textit{0.84} & \textit{0.88} & \textit{0.82} && \\
    

    \end{tabular}
    \caption{\label{gold-metrics}
        \textbf{Blinded human annotation} averages across a sample of 1110 claims generated from 40 (9\%) \ds{} test documents. Annotation of reference-free metrics (left) was done using grading scales (Appendix~\ref{sec:annotation}) adapted from~\citealt{wright-etal-2022-generating} binarizing the best grade to 1, others to 0. Reference-based metrics (right) annotated using a checkbox interface over gold and pred. claims (Appendix~\ref{sec:annotation}). The best value for each metric marked bold. Inter-annotator agreement experiment results in italic.
        Krippendorff's $\alpha$ left in for completeness, albeit very inappropriate for tasks with such class imbalance, as also noted by~\citealt{wright-etal-2022-generating}, Gwet's AC1 being the more appropriate agreement metric for our task.}
\end{table*}

\section{Results}\label{sec:results}

After training the models described in Section~\ref{sec:methods} on \ds{} dataset, we had each model extract a set of claims from each of 444 \ds{}-test documents.
We evaluated the claims using the automated metrics described in previous Section~\ref{sec:metrics}.
To validate the metrics, we have also annotated a subset of 9\% (40) \ds{}-test documents (and 1110 claims extracted from them by the models in total), using a custom annotation platform described in Appendix~\ref{sec:annotation}.


\subsection{Model comparison}
In tables~\ref{auto-metrics} and~\ref{gold-metrics} we use the automated and human-annotated metrics to benchmark our models, on full \ds{}-test set and a sample of 40 documents, respectively.
The benchmarks reveal that all models score high in Atomicity, Fluency, Decontextualization, and Faithfulness reference-free metrics -- confirmed by the human annotations used to compute the Table~\ref{gold-metrics}, these do not appear to be the challenges modern NLP claim-extractors would struggle with.

The reference-based metrics of Focus and Coverage are where significant differences can be found and tradeoffs can be seen -- models with highest coverage focus too little and vice versa. 
Taking their harmonic mean, $F_{fact}$, the fine-tuned models \texttt{qlora-mistral-instruct} and \texttt{t5\_sm} take a significant lead, showing that the models were able to learn that the \ds{} claims come from the middle sentence on input, which is the information annotators were tasked to focus on in the original~\citealt{thorne-etal-2018-fever}'s annotation experiment -- hopefully, this could mean the models could be tuned for trickier notions of check-worthiness, such as those studied by~\citealt{10.1007/978-3-031-28241-6_59}, in future works.

The T5 coupled with \textit{diverse beam search} decoding strategy described in Section~\ref{sec:methods} shows promising results, despite it being essentially an abstractive summarization model.
Its redundancy is the highest (diverse outputs only assured through a decoding strategy), but not a clear outlier.
We find this very encouraging for further use of the diverse beam search predictioning in settings where only one claim per source document is annotated, such as in AVeriTeC-DCE~\cite{deng-etal-2024-document}, to produce multiple interesting results.
Datasets like XLSum~\cite{hasan-etal-2021-xl} with an objective highly similar to claim extraction and good availability of summarization models across languages make it an accessible choice with competitive performance.
While the GPT-4 could have certainly been prompted to fit to the \ds{} claim extraction better than in our study, we still recommend to use the provided \texttt{t5}'s or \texttt{Mistral} as the baseline for future investigations for their size, ease of use, and trainability for whichever claims one wants to extract through examples.   

\begin{comment}
    \begin{table*}
    \centering

    \begin{tabular}{l|cccc|cccc}
        %\hline
        & \multicolumn{4}{c|}{\s{Automated Metrics}} & \multicolumn{4}{c}{\s{Gold Annotations}} \\
        {Model}                                           & $A$        & $G$      & $D$      & $Faith$ & $A_g$     & $G_g$    & $D_g$    & $F_g$    \\
        \hline           
        \footnotesize{\texttt{qacg}}                      & 0.89       & 0.69     & 0.70     & 0.88    & 0.99      & 0.85     & 0.91     & 0.76     \\
        \footnotesize{\texttt{gpt-4-turbo-3-shot}}        & 0.92       & 0.70     & 0.77     & \s{0.99}& 0.98      & \s{0.97} & \s{0.96} & \s{0.93} \\
        \footnotesize{\texttt{qlora-mistral-instruct}}    & 0.95       & 0.75     & 0.80     & 0.95    & \s{1.00}  & 0.96     & \s{0.96} & 0.90     \\
        \footnotesize{\texttt{t5\_sm\_diverse\_7\_beams}} & 0.95       & 0.74     & 0.80     & 0.91    & 0.99      & 0.89     & 0.95     & 0.79     \\
        \footnotesize{\texttt{t5\_sm\_multi-claim}}       & \s{0.96}   & \s{0.76} & \s{0.82} & 0.95    & \s{1.00}  & 0.91     & 0.94     & 0.88     \\
        \hline
    \end{tabular}
    \caption{\label{model-comparison-bin}
        Model comparison on means of model metrics over all samples (or over all annotated samples). In bold are the best values for each metric.}
\end{table*}


\end{comment}





\subsection{Metric validation}
To validate our evaluation framework, we have annotated the claim properties on grading scales with instructions adapted from~\cite{wright-etal-2022-generating} and the Focus and Coverage using a simple checkbox interfaces shown in attachment~\ref{sec:annotation}.

The human annotations confirm that the reference-free methods (Section~\ref{atomicity}) are easy for today's models.
They, surprisingly, also preserve the leaderboard of models based on $F_{fact}$, the hardest metric, requiring an understanding of which claims are relevant in \ds{} data and punishing greedy approaches.
We evaluate the reference-free metric estimation methods using the $F_1$ score, chosen due to the imbalance of classes on real generated claims (this goes hand in hand with metrics being too easy for models).
For faithfulness and fluency, we binarize the human-annotated grades (the highest grade to 1, others to 0), to be directly comparable with automated metrics (Section~\ref{sec:metrics}) -- AlignScore used for Faithfulness was simply rounded to 0 or 1 (threshold 0.5) for this experiment.
All metrics score above $80\%$ $F_1$, testifying to their soundness and usability at a scale in scenarios where a good approximation is enough.

We proceed to validate our Focus and Coverage metrics against human annotation, using the root mean squared error to obtain values around 0.22, which is also encouraging for further use.
To see whether this value is not accidental (correct proportion, but erroneously distinguished gold claims), we also measure the \textit{claimwise} $Foc$ and $Cov$ to see the per-claim contribution to metrics as a probability $Foc(G,\{c\})$, $Cov(\{g\},C)$,  as shown in Section~\ref{reference-based}.
We compare them to claimwise annotations, and asses their quality using a Brier score as suggested in~\citealt{rufibach2010use}. 
We measure 0.15 for $Foc$ and 0.12 for $Cov$ (lower Brier score is better), in other research sometimes interpreted~\cite{fernandes} as a \q{superior} agreement.

\subsection{Inter-Annotator Agreement}
The inter-annotation aggreement study is presented in Table~\ref{gold-metrics}.
Due to missing annotations, our first choice for it was the Krippendorff's $\alpha$ coefficient \cite{krippendorff2011computing}.
Its results are, however, abysmally low on our annotations, even reaching negative values reserved for annotation vandalism -- the cause of this are very high levels of class prevalence in our annotations\footnote{Prevalence of the most common class in faithfulness, fluency, atomicity and decontextualisation grading is 0.84, 0.9, 0.98, and 0.91, respectively.}, overly increasing the chance of accidental agreement Krippendorff's alpha targets to punish.
This is a known pitfall of Krippendorff's alpha, also encountered in the~\citealt{wright-etal-2022-generating}'s paper which proposed the reference-free metrics.
To overcome it, we also report the Gwet's Agreement Coefficient 1 (AC1) that was designed to be less sensitive to trait prevalence.~\cite{gwet2010handbook}
We report the values\footnote{Values were computed using Python port of R library \textit{irrCAC} - https://irrcac.readthedocs.io/en/latest/} for AC1 coefficient in table \ref{gold-metrics}.
From the results, we can see that the AC1 coefficient is high in all cases\footnote{The Gwet's AC1 coefficient ranges from 0 to 1.} thus, we conclude that our annotation process was reliable. To provide a complete measurement, we also report \%-agreement -- a proportion of labels on which all annotators agree~\cite{Artstein2017}.

