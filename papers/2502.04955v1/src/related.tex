%!TEX ROOT=../acl2023.tex
\section{Related work}\label{sec:related}

\begin{enumerate}
    \item \textbf{XSum}~\cite{narayan-etal-2018-dont} -- single sentence summaries on a large scale -- match the claim structure very close, except for atomicity.
    \item \textbf{FEVER}~\cite{fever-2022-fact,thorne-vlachos-2021-evidence} -- recently republished along with Wikipedia abstracts that were used to extract claims -- may be a great testbed for our specific summarization task.
    \item \textbf{CLEF CheckThat}~\cite{barroncedeno2020overview} -- series of shared task that published numerous datasets on the adjacent task of checkworthiness estimation and evaluation schemes for the solution, also, generating claims from social media
    \item \textbf{BERTScore}~\cite{bert-score} proposes a precision, recall and F1 score for the seq2seq tasks based on the BERT-model embeddings
    \item \textbf{FFCI}~\cite{ffci} takes these scores to estimate \textit{faithfulness}, \textit{coverage}, \textit{focus}, \textit{coherence} and other scores typical for abstractive summarization
    \item \textbf{Scientific Claim Generation}~\cite{wright-etal-2022-generating} gives a functioning scheme of transducing source \textit{citances}\footnote{Self-contained (citable) factual sentence.} to atomic claims -- this could either be used for comparison, or later in our pipeline to \q{atomize} our extracted claims, as those have a form of a citance.
    \item \textbf{QAGS}~\cite{wang-etal-2020-asking} gives methods for examining claim \textit{focus} and \textit{coverage} using QA methods, and proposes a method for extracting claims from source texts using NERs and QA
    \item \textbf{AlignSCORE}~\cite{zha-etal-2023-alignscore} proposes a method for checking factual consistency of various text2text tasks using a function aligning factual information in two arbitrary texts
    \item \todo{More recent works in field? (the above was updated in April)}
\end{enumerate}

