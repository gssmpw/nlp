%!TEX ROOT=../acl2023.tex
\section{Evaluation Framework: Claim Metrics}\label{sec:metrics}
To evaluate our models, we compile claim quality criteria introduced in other research.
Each predicted claim is checked to be \textit{faithful}, \textit{fluent}, \textit{atomic} and \textit{decontextualized}.
A set of claims extracted from the same document is measured to \textit{focus} solely on its relevant information andÂ \textit{cover} all of it.
While the criteria above can be checked by human grading as in~\cite{wright-etal-2022-generating}, research such as~\cite{wang-etal-2020-asking,ffci} already proposes ways of how to evaluate some of the qualities automatically, alleviating the need of human grading of model outputs.
In this section, we attempt to compile known evaluators for our task, reducing each metric to an already well-explored NLP challenge, proceeding to validate our metrics against real human annotations on generated claims.

\subsection{Reference-free evaluation metrics}
\label{atomicity}
For the reference-free metrics which score each claim on its own, we use the criteria named by~\citealt{wright-etal-2022-generating}: Atomicity, Fluency, Decontextualization, and Faithfulness, adapted from \citealt{10.1007/978-3-642-38288-8_33}'s AIDA (Atomic, Declarative, Independent, Absolute).
To allow automated evaluation at a scale, we find following reductions to known NLP tasks to replace human grading:

\begin{enumerate}
    \item \textbf{Atomicity} -- \textit{does the claim describe a single entity, relation, or process?}

    While the real-world factual claims are not often atomic in the true sense (i.e., the claims are typically more convoluted than \q{A is B} or \q{C does D}), breaking a more complex factual statement into a set of atomic claims trivializes the inference on top of such claims, as well as allows certain level of explainability~\cite{dammu-etal-2024-claimver}, such as which parts of complex statement disinterpret which facts.
    
    We propose the following scheme of atomicity classification of claim $c$, reducing it to a Relationship Extraction task as:

    $$A(c) := \left\{ 
        \begin{array}{ c l }
            1 & \quad \textrm{if } |RE(c)| \leq 1 \\
            0                 & \quad \textrm{otherwise}
        \end{array}
    \right.$$

    Where the relation-extraction result is interpreted as a set of undirected relations $RE(c) = \{\{s_1,t_1\}\dots\}$, in order to avoid counting symmetrical relationships like $(Trump, president\_of, USA)$, $(USA, governed\_by, Trump)$ twice.

    For the $RE$ solver, we recommend using $\textrm{REBEL}$~\cite{huguet-cabot-navigli-2021-rebel-relation} due to its manageable size and end-to-end approach -- other models may rely on entity-pair extraction and classification~\cite{yamada-etal-2020-luke} -- we proceed to use its Large variant.

    We also experimented with a non-binary metric $A'(c)=\frac{1}{max\{1,|RE(c)|\}}$, but during our experiments, non-atomic model outputs were quite rare, so punishing them by zeroing-out their $A$ score worked best for averaging across large number of claims.

    \item \textbf{Fluency} -- \textit{is the claim grammatically correct and intelligible?}
    
    The task of fluency, also referred to as \textit{grammaticallity}, is well studied in literature, with most recent research modelling it as a grammatical error detection (GED) and correction (GEC) tasks.
    
    The available techniques range from simpler ridge regression models based on linguistic features~\cite{heilman-etal-2014-predicting} through using syntactic log-odds ratio (SLOR)~\cite{kann-etal-2018-sentence} or perturbing the claims words and characters to find local optima in the output probability using a language model such as GPT-2 as its reference~\cite{yasunaga-etal-2021-lm} to promping a LLM (such as GPT-3) to obtain a model-inferred score using few- or zero-shot learning~\cite{fu-etal-2024-gptscore}. 
    
    The best performing approach we studied revolves around CoEdIT~\cite{raheja-etal-2023-coedit} GEC model, coupled with Scribendi score~\cite{islam-magnani-2021-end} to rate the improvement between each claim and its CoEdIT correction.
    Scribendi score combines perplexity scores and Levenshtein distance, yielding -1 for bad correction, 0 for no improvement, and +1 for correction improving the claim.
    While this metric is originally designed for the evaluation of GEC systems, it performs well on rough fluency rating, looking for zero or negative improvement in CoEdIT corrections:
    $$G(c):=\left\{ 
        \begin{array}{ c l }
        1 & \textrm{if } \textrm{\footnotesize{Scribendi}}(c,\textrm{\footnotesize{CoEdIT}}(c)) \leq 0 \\
        0 & \textrm{otherwise}
    \end{array}
    \right.$$
    
    
    \item \textbf{Decontextualization} -- \textit{can the claim be correctly interpreted without any additional context from the source document or elsewhere?}
    \citealt{choi-etal-2021-decontextualization} proposes the decontextualization as a text-to-text task, training a T5 to receive context and a sentence on its input, outputting the decontextualizated sentence, resolving its pronouns into proper nouns and relative terms into absolute according to the given context marked with separators.

    As with atomicity, a strict binary classification worked best, as the non-decontextualized model outputs were rare.
    For context, we use the full \ds{} document $d$:
    $$D(d,c) := \left\{ 
        \begin{array}{ l l }
            1 &  \textrm{if } \textrm{ T5}_{\textrm{\tiny{d}}}(d,c) = c \\
            0                 &  \textrm{otherwise}
        \end{array}
    \right.$$
    where $\textrm{T5}_{d}(d,c)$ denotes the output of $\textrm{T5}_{\textrm{\tiny{large}}}$ model trained by~\citealt{choi-etal-2021-decontextualization}.
    
    

    \item \textbf{Faithfulness} -- \textit{does the claim contain only information consistent with its source?}
    
    Faithfulness has been extensively studied on its own to detect hallucination in text-to-text tasks. \textsc{AlignScore} claimed a state of the art in~\citealt{zha-etal-2023-alignscore}, looking for optimum alignment of output and input parts, in terms of a RoBERTa~\cite{liu2019roberta} classifier with a \textit{unified alignment function}.
    While compact (125M parameters in \textit{base} version), it outperforms metrics based on GPT-4 that is orders of magnitude larger~\cite{zha-etal-2023-alignscore}.
    $$Faith(d,c) := \textsc{AlignScore}_{base}(d,c)$$

\end{enumerate}

\subsection{Reference-based evaluation metrics}\label{reference-based}
The evaluation metrics introduced above do not rely on any gold data for their reference.
But what if we need to evaluate the whole group of claims, to see if it captured all the information it should and nothing more?

The notion of what a good set of claims is varies task-to-task, so engineering rules may do more harm than good.
Let us therefore use examples, like~\citealt{ffci} suggests in their FFCI framework for interpretable summarization metrics, measuring \textit{Focus} and \textit{Coverage} to see whether the model extracts what a human would.
Besides the predicted set of claims $C$, which serves as the input for multi-claim metrics, assume a set of gold claims $G$ extracted from the same document -- in \ds{}, those would be the full sets of claims (green in Figure~\ref{fig:feverfact}) obtained from annotation.

\begin{enumerate}
    \setcounter{enumi}{4}
    \item \textbf{Focus} -- \textit{what is the proportion of gold (relevant) information among all the information listed in the generated claims?}
    
    The metric is analogous to the concept of \textit{precision}: $\frac{|true~positives|}{|positives|}$, but rather then exact matching, we seek a measure of semantic overlap.
    FFCI uses QAGS~\cite{wang-etal-2020-asking}, which uses a Question Generation model (QG) to formulate questions in natural language based on all $|C|$ predicted claims. The questions are then twice answered using a Question Answering (QA) model, giving it knowledge from (i.) the predicted claims (ii.) the gold claims written by a human.
    Focus is then defined as the proportion of questions with the same answers extracted from the gold and predicted claims among all questions the model can generate from the predicted claims.
    In our experiments, this metric was too noisy, with QG, QA and aggregation faults all propagating into the final result.

    To propose a simpler method, we exploit that the gold claims in $G$ can be concatenated into a mock-document.
    Since gold claims are decontextualized by annotation, this preserves their meaning, reducing Focus into $|C|$ independent tasks of deciding Faithfulness:
    $$Foc(G,C):=\frac{1}{|C|}\sum_{c\in C}Faith(concat(G), c)$$
    We also encourage experiments with \textit{claimwise focus}, computing the single probability $Foc(G,\{c\})$ for each pred. claim $c$ separately, to see \textit{which exact} claim in $C$ should be extracted according to $G$ and which should not.
    \item \textbf{Coverage} -- \textit{what proportion of gold (relevant) information from the source is featured in $C$?}
    
    Can be simply adapted from focus: 
    $$Cov(G,C) := Foc(C,G)$$

    \citealt{ffci} proposes this trick, and while our underlying $Foc$ method differs, the argument swap still works to yield proportion of gold information extracted into $C$. 
    Whether the claims in $C$ are non-decontextualized by a faulty prediction and could influence each other's meanings upon concatenation can be checked for using the metric from Section~\ref{atomicity}, marking the claims to toss, or leave them in if the level of noise is tolerable.
    We went with the latter as $>94\%$ claims were decontextualized upon human grading (Table~\ref{gold-metrics}).
    
    \item \textbf{Redundancy}\footnote{\label{fn:redundancy}Unlike Focus and Coverage, we did not validate this Redundancy metric with annotations, but as its validity stems from the same principles as that of $Foc$ and $Cov$, we feature it anyways for added insights. The metric was motivated by feedback from the annotators of data used for Table~\ref{gold-metrics}} -- \textit{What is the proportion of duplicate information among generated claims?}
    
    the definitions of $Foc$ and $Cov$ inspire an ellegant estimate of \textit{model redundancy} -- if $|C|$ claims are generated by a one-to-many seq2seq model, what proportion of them can be expressed from the \textit{other} model outputs:
    $$Red(C) := \frac{1}{|C|}\sum_{c\in C}Faith(concat(C\setminus c), c)$$ 
\end{enumerate}

\subsection{\textbf{$F_{fact}$-value}}
\label{ffact}
As our experiments in Section~\ref{sec:results} suggest, the $Foc$ and $Cov$ do indeed behave like Precision and Recall metrics in the sense of their mutual tradeoff -- if a na\"ive model scores very high in one (such as QACG covering \textit{every part} of the \ds{} document), a significant decrease can be observed in the other.
    
In our experience, the reference-free single-claim metrics (Section~\ref{atomicity}) did not pose a tough challenge to modern NLP methods, most of their corresponding mean scores in both Table~\ref{auto-metrics} and~\ref{gold-metrics} being very high, raising a concern they may be a too-easy benchmark nowadays.
Provided with the gold annotations for each source document, we therefore suggest that the most important aggregate metric to compare the future claim extraction SotA models should be the \textbf{$F_{fact}$ score} -- harmonic mean of $Foc$ and $Cov$.