%!TEX ROOT=../acl2023.tex
\section{Claim Extraction Models}\label{sec:methods}
Using the \ds{} dataset to train and evaluate claim generators, we model the automated claim extraction as a \q{one-to-many} sequence-to-sequence task~\cite{sutskever-2014} -- given a single \ds{} document, predict a set of facts it claims.
We experiment with prompting, transfer learning and a NER-based baseline used on FEVER data previously in literature.

To all the models, we do not provide any distinction (such as separators) between the \ds{} source sentence and its context, feeding the whole \ds{} document (Figure~\ref{fig:feverfact}) to each model's input.
This is done experimentally, to give idea whether different models are able to learn to only focus on the information relevant to the user (in our case, arbitrarily, only the information claimed by the middle sentence) just from its training/few-shot examples.
If so, this primitive example of \q{positional} relevance of input information could motivate collection of claim extraction datasets capturing more challenging notions of relevant information to extract, such as check-worthiness of claims scattered through a political debate or Twitter discourse, where, unlike in encyclopedic style, only a small proportion of text is interesting for checks~\cite{10.1007/978-3-031-28241-6_59}. 

We experiment with the following models:

\begin{enumerate}
    \item \textbf{Question Answering for Claim Generation (QACG)}~\cite{pan-etal-2021-zero} is an off-the-shelf baseline we use.
    It has been built dedicatedly to generate claims: given an input text, its named entities are extracted, fixing each entity $e$ as an answer for a question $Q$ to be generated.
    The $(Q,e)$ is then converted into a declarative sentence using a QA-to-claim model, arriving to one factual claim for each of the source-text entities.

    While being a valuable baseline, the entity-centric approach has its caveats: the QACG pipeline has many steps, each a language model and, therefore, a point of failure -- mistake in any step propagates to the result.
    The QACG also lacks trainable parameters for \textit{choosing the appropriate} claims.
    If, for example, one wants to generate claims from a debate, it does not have mechanisms to learn to omit recurring irrelevant, entity-dense guest introductions. 
    \item \textbf{LLMs and few-shot learning}~\cite{NEURIPS2020_1457c0d6} has been recently a popular, universally well-performing solution to be reckoned with.
    Models such as GPT-4~\cite{openai2023gpt4} or Mistral-instruct~\cite{jiang2023mistral} have been shown to adapt well for similar tasks.

    We examine \texttt{gpt-4-turbo} in a 3-shot setting and finetune \texttt{Mistral-7b-instruct-v0.2} model on our \ds{} train data using the quantized QLoRA~\cite{dettmers2023qlora} approach with $r=64, \alpha=32$ on 4 bits.

    The GPT-4, however, is a blackbox, and the open-source LLMs are computationally expensive.
    This motivates an examinaton of other methods to be reproducible on much lower resources using easy-to-obtain data.
    \item \textbf{T5 transfer learning}~\cite{2020t5}: our small model of choice (based on the preliminary experiments) is the \texttt{t5-small-finetuned-xsum}\footnote{Initial weights loaded from openly available \url{https://huggingface.co/darshkk/t5-small-finetuned-xsum}}
    
    The T5 was pre-trained on various text-to-text tasks using a \textit{span-corruption} objective, making it adapt well to our task despite a relatively small (3.5K \ds{} documents, 13.5K claims) train size.
    We have found that fine-tuning it on a single-sentence summarization dataset such as XSum~\cite{narayan-etal-2018-dont} \textit{and then} on \ds{} yields even better results, possibly due to similar task definition.

    Two approaches were examined: first, where T5 was trained to output a concatenation of all claims in a single prediction\footnote{The claims were then separated using PySBD~\cite{sadvilkar-neumann-2020-pysbd}}, second, where the T5 was tuned to output a \textit{single} claim as a sentence (using our data as 13.5K document-claim pairs).
    In the prediction stage, the latter approach was coupled with \textit{diverse beam search}~\cite{vijaykumar_beam} decoding technique to generate arbitrary number ($k$) of single-sentence claims per text, using $k$ beam groups and a diversity penalty of $1$.
    Its results on our task being competitive with the other models are particularly encouraging, as the single-sentence summarization data is easily available in other settings and languages (just XLSum~\cite{hasan-etal-2021-xl} features data in 45 languages expertly annotated for the task). 
\end{enumerate}