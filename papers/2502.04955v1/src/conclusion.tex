%!TEX ROOT=../acl2023.tex
\section{Conclusion}\label{sec:conclusion}
Our study has examined the problematic of automated claim extraction in its full scope: from data gathering (publishing \ds{} dataset) through model training (suggesting low- and high-resource options), benchmarking (publishing a Python framework for automated evaluation) to its validation with annotators, suggesting the hardest and most appropriate score for claim extraction being the $F_{fact}$ we introduce in Section~\ref{ffact}, requiring gold claims for reference.

Interestingly, we have shown that the task of claim extraction can be to a significant extent solved using single-sentence abstractive summarization methods and a diverse beam search decoding strategy, which is particularly interesting for low-resource languages and environments.

Overall, we have aimed this aticle as a breadth-first exploration of the claim extraction task using abstractive methods, pointing directions which to explore in more depth.
\subsection{Future works}
\begin{comment}
\begin{enumerate}
\item 
\end{comment}
    
A central challenge to claim extraction is the notion of claim \textit{check-worthiness}, now explored as a classification task, for example by~\citealt{10.1007/978-3-031-28241-6_59}.
Our paper models a dummy version of it, training the models to only focus on the middle sentence claims like FEVER annotators did.
Adapting existing data such as CLEF-CheckThat! datasets for text generation, rather than classification task, could capitalize on our findings to train generative models which only focus on check-worthy claims.

We also encourage the use of our evaluation metric framework to yield additional insights into any system for knowledge-intensive tasks, especially where multiple gold answers can be found or synthesized, such as Summarization or Retrieval-Augmented Generation (RAG).
\begin{comment}
    \item Propose more datasets
    \item Validate the claim quality metrics using human annotations and tune the choice of models as well as weights of each metric to obtain the best match
\end{enumerate}
\end{comment}

