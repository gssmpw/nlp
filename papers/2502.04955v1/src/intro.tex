%!TEX ROOT=../acl2023.tex
\section{Introduction}\label{sec:intro}

Recent research in Natural Language Processing has extensively covered the case of automated fact-checking as a pipeline of retrieval and inference tasks~\cite{fever-2022-fact,thorne-vlachos-2021-evidence}.
As noted in other research~\cite{guo-etal-2022-survey}, these steps alone do not represent the whole challenge human fact-checkers face, notably omitting the step of coming up with the claims to be checked in the first place.
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/feverfact.pdf}
    \caption[An example from \ds{} dataset]{An example of claim extraction from the \ds{} dataset constructed from data published by~\citealt{thorne-vlachos-2021-evidence}}
    \label{fig:feverfact}
\end{figure}


The real-world factual claims may be scattered throughout a politician's social network post, pieced together from a debate transcript, distilled from a lengthy news article, etc. 
Research such as~\cite{barroncedeno2020overview} therefore proposes augmenting the pipeline by the step of \textit{claim detection}, commonly modelled as classification of \textit{check-worthy} claims within a real-world text segmented into its sentences.

The claim detection paradigm, however, only kicks the can down the road -- the detected \q{check-worthy} parts of text still need to be processed by a human fact-checker~\cite{sheikhi-etal-2023-automated} and re-written into the form of a \textit{claim}: a factual statement presenting the source text information without the need of additional context in a single sentence.

The automated generation of such claims is desirable not only for the fact-checkers themselves~\cite{deng-etal-2024-document}, but also for further research in the field~\cite{drchal2023pipeline} where the generation of claims on top of large-scale corpora was shown to yield valuable data to train models~\cite{pan-etal-2021-zero}.

In this paper, we explore the abstractive methods of claim extraction, modelled as one-to-many text-to-text task: after seeing the whole input text, the model is tasked to formulate a set of claims it makes, as simple self-contained sentences faithful\footnote{An interesting close parallel for this task are the single-sentence summarization methods (also known as the \textit{extreme summarization}) explored before in~\citealt{narayan-etal-2018-dont,mao-etal-2022-citesum}, finetuning of which we examine as one of the approaches in Section~\ref{sec:methods}.} to the source. 
We construct a downstream task for the models -- the \textbf{\ds} dataset -- and compile the criteria previously studied for extracting factual claims~\cite{wright-etal-2022-generating,10.1007/978-3-031-28241-6_59}, proposing methods of their automatic evaluation at a scale.

This paper explores the claim extraction using generative methods in breadth, accumulating a set of data, automated reference-based and reference-free metrics and proposing solvers for the task, yielding a complete scheme of the automated generation of check-worthy claims and measuring their qualitative properties to identify possible problems of each claim.

\subsection{Main Contributions}
\begin{enumerate}
    \item We publish the \ds{} dataset (Figure~\ref{fig:feverfact}) with 4.4K contextualized Wikipedia sentences, and a total of 17K check-worthy claims extracted from them by annotators.
    \item We propose an automated evaluation framework for claim extraction reusable for other generative fact-critical tasks, consisting of 6 metrics: Atomicity, Fluency, Decontextualization, Faithfulness, Focus and Coverage.
    We compile current research in the field to name the relevant metrics, as well as to find their scoring methods, combined with novel ideas where appropriate (see Section~\ref{sec:metrics}).
    \item We explore generative methods for extracting the \ds{} claims -- QACG from~\cite{pan-etal-2021-zero}, LLM prompting, and LM transfer learning, publishing our tuned models.  
    \item We annotate a blinded sample of generated claims to validate our evaluation framework and challenge the benchmarks it produces.
\end{enumerate}

%In accordance with other research, we have identified the evaluation of claim atomicity and %contextualization faithfulness as important criteria for factual claim extraction~\cite%{guo-etal-2022-survey}.
%
%In this paper, we propose a scheme of fine-tuning summarization models for the %claim-extraction task, as well as a set of automated evaluation tools and a discussion of %common issues such as model \textit{hallucination} and an example of scenarios for an usage of %such models.

%\subsection{Main todos}
%\begin{enumerate}
%    \item Introduce the claim extraction problem (as a combo of check-worthiness \& hallucination-free sum)
%    \item Adapt existing dataset(s) for the claim extraction task:
%     \begin{itemize}
%        \item CNN-Dailymail -- Each point is a pair of article and several-sentences-long \q{highlights}
%        \item FEVER -- let's start from the error correction paper -- \url{https://aclanthology.org/2021.acl-long.256/}%\\
%    \end{itemize}
%    \item Train a strong baseline for the problem (based on BRIO, Pegasus 2B or at least T5/BART)
%    \item Maybe publish a list of novel claims extracted over some FEVER datapoints or sth? maybe blbost?
%\end{enumerate}
%
%\subsection{Todos after Marian}
%\begin{enumerate}
%    \item Read Å imon fully
%    \item Mention interesting hallucinations (some unpunished by ROUGE):  i.e., switching the victim and perpetrator of %a crime, active/passive verb, numbers are a pitfall
%    \item See arrow datasets -- use dataset\_handler.py from Marian's codebase
%\end{enumerate}