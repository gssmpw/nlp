% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{comment}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\newcommand{\q}[1]{``#1''}
\newcommand{\s}[1]{\textbf{#1}}
\newcommand{\ds}{FEVERFact}
\newcommand{\qit}[1]{\textit{``#1''}}
\newcommand{\revise}[1]{{\color{blue}#1}}
\newcommand{\rewrite}[1]{{\color{red}#1}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\revision}[1]{{\color{purple}#1}}
\newcommand{\jd}[1]{{\color{orange}\textbf{jd: }#1}}
\newcommand{\hu}[1]{{\color{blue}\textbf{hu: }#1}}
\newcommand{\mr}[1]{{\color{green}\textbf{mr: }#1}}
\newcommand{\todo}[1]{{\color{red}\colorbox{yellow}{\textbf{TODO: }}#1}}

%figures package
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}
\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother
%\usepackage{emoji}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Herbert Ullrich \\
AI Center @ CTU FEE\\
Charles Square 13,\\
Prague, Czech Republic\\
\texttt{ullriher@fel.cvut.cz} \\\And
Tomáš Mlynář \\
AI Center @ CTU FEE\\
Charles Square 13,\\
Prague, Czech Republic\\
\texttt{mlynatom@fel.cvut.cz} \\ \\\And
Jan Drchal \\
AI Center @ CTU FEE\\
Charles Square 13,\\
Prague, Czech Republic\\
\texttt{drchajan@fel.cvut.cz} \\}
\begin{document}
\maketitle

\begin{abstract}
  In this paper, we explore\footnote{Full code and data supplement available at \url{https://github.com/aic-factcheck/claim_extraction}} the problem of Claim Extraction using one-to-many text generation methods, comparing LLMs, small summarization models finetuned for the task, and a previous NER-centric baseline QACG.
  As the current publications on Claim Extraction, Fact Extraction, Claim Generation and Check-worthy Claim Detection are quite scattered in their means and terminology, we compile their common objectives, releasing the \ds{} dataset, with 17K atomic factual claims extracted from 4K contextualised Wikipedia sentences, adapted from the original FEVER.
  We compile the known objectives into an Evaluation framework of: \textit{Atomicity, Fluency, Decontextualization, Faithfulness} checked for each generated claim separately, and \textit{Focus} and \textit{Coverage} measured against the full set of predicted claims for a single input.
  For each metric, we implement a scale using a reduction to an already-explored NLP task.
  We validate our metrics against human grading of generic claims, to see that the model ranking on $F_{fact}$, our hardest metric, did not change and the evaluation framework approximates human grading very closely in terms of $F_1$ and RMSE.
\end{abstract}

\input{src/intro}
%\input{src/related}
\input{src/data}
\input{src/methods}
\input{src/metrics}
\input{src/results}
\input{src/conclusion}
%\input{src/generic}


\section*{Limitations}
The main limitation of the work is that it does not explore a specific sound notion of check-worthiness for measuring the claim \textit{focus} and \textit{coverage}, but model it after the FEVER WF1a annotation objective, which instructs the annotators to extract 2-5 relevant claims from the \textit{middle} sentencence of shown context.
This is a sort of a dummy objective to optimize for (albeit, has its sense, since every sentence in encyclopedic style is supposed to contain some check-worthy claim), and the fact that our SotA model scores $F_{fact} = 0.64$ expressing the extent to which it extracts the same claims human would only motivates the research on \textit{what other objectives of claim relevancy} could the models be trained for, rather than marking the claim extraction challenge as resolved.

So, while we have the approaches and metrics that have been validated in vitro against human gradings of the same task, the generalization of the scheme to claim extraction in the wild remains largely to be explored.

\section*{Ethics Statement}
While our data and evaluation framework seems to be disambiguous enough to use, our proposed method of generating the factual claims using a one-to-many abstractive text-to-text approach raises critical requirements on the deployment of such models.
The models are suitable to produce claims at a scale, and the evaluation framework can pinpoint most of their hallucinations.
However, the use of such models to intepret individual person's claims in language forms such as Tweets and debate transcripts, like~\citealt{barroncedeno2020overview} do in their check-worthy claim detection tasks, is ethically problematic, as the abstractive models have no hard guarantee of aligning to the facts and semantical nuances of the speaker.
The models should therefore be avoided to use without a human in the loop in such scenario, and their use should be disclosed.

Furthermore, we declare that we have used LLM-based code assistants when producing the code supplement of this paper, to generate boilerplate Python code.

\begin{comment}
\section*{Acknowledgements}
Acknowledgements shall be revealed after reviews, so as to maintain the authors' anonymity.
\begin{comment}

This article was produced with the support of the Technology Agency of the Czech Republic under the ÉTA Programme, project TL02000288. The access to the computational infrastructure of the OP VVV funded project CZ.02.1.01/0.0/0.0/16\_019/0000765 ``Research Center for Informatics'' is also gratefully acknowledged.
\end{comment}


% Entries for the entire Anthology, followed by custom entries
\bibliography{custom, anthology}

%TODO added to start appendix on a new page - should it be this way?
\clearpage
\appendix
\begin{comment}
  
\section{Overview of Considered Methods}
\label{sec:appendixA_all_metrics}
In this appendix section we present results for some of the methods that were implemented and evaluated in our work as in the main text we mention only the best performing ones. The results are presented in Table~\ref{table:metric-validation-bin-all}.

For Fluency, we have evaluated a wide range of methods. Preliminary experiments were made with ridge regression models. However, due to the unavailability of libraries needed to extract linguistic features from the text, we could not replicate results from~\cite{heilman-etal-2014-predicting}. The tested methods in the Table~\ref{table:metric-validation-bin-all} can be divided into two groups. One is reference-less methods, and the other one contains methods dependent on reference grammatically corrected claims made by SoTA CoEdIT encoder-decoder model\footnote{https://huggingface.co/collections/grammarly/coedit-653a08b9a693d907fadaffb9}~\cite{raheja-etal-2023-coedit}. One of the reference-less methods is a standard GPT-2 perplexity that is described in the main text as it was one of the best-performing ones. The other one is the BPE-SLOR (Syntactic log-odds ratio made on byte-pair encoding)~\cite{kann-etal-2018-sentence} also based on the GPT-2 model. The reference-based methods compare the claims with their hypothetically correct versions. The first comparison method was simple identity (evaluating whether the two strings are identical). Then we tried other basic metrics such as Levenshtein distance (in the experiments also in the form normalized by distance\footnote{In our case it did not bring any improvement as the compared text strings were in almost all cases of similar length.}) or Jaccard similarity (here we tried 1 to 4 grams - bigrams proved to be the best choice). The last method was the Scribendi Score~\cite{islam-magnani-2021-end}, which is closely described in the main text as it also performed well throughout the experiments and is not dependent on an optimized threshold. Other tried methods were Damerau-Levenhtein distance, Jaro-Winkler distance, Jaro Similarity, Python's difflib string similarity\footnote{https://docs.python.org/3/library/difflib.html}, Language Tool~\cite{napoles-etal-2016-theres}, Needleman-Wunsh distance and Smith-Waterman distance. However, these methods performed poorly even in preliminary experiments made on GUG dataset\footnote{https://github.com/EducationalTestingService/gug-data}~\cite{heilman-etal-2014-predicting} and by manual evaluation and were not further evaluated.


\begin{table*}[!h]
  \centering
  \begin{tabular}{lrr}

      \hline
      metric & model & $F_1$ \\
      \hline
      \hline
      Faithfulness & \footnotesize{\texttt{AlignScore Base F }} & 0.92 \\
       & \footnotesize{\texttt{AlignScore Large F\_lg }} & 0.93 \\
       & \footnotesize{\texttt{BertScore F\_bert}} & 0.91 \\
      \hline
      Fluency & \footnotesize{\texttt{CoEdIt Large + Scribendi score}} & 0.80 \\
       & \footnotesize{\texttt{CoEdIt XL + Scribendi score}} & 0.74 \\
       & \footnotesize{\texttt{CoEdIt Large + Identity}} & 0.47 \\
       & \footnotesize{\texttt{CoEdIt XL + Identity}} & 0.39 \\
       & \footnotesize{\texttt{GPT2 Perplexity (threshold 10.68)}} & 0.94 \\
       & \footnotesize{\texttt{BPE-SLOR (threshold 2.88)}} & 0.92 \\
       & \footnotesize{\texttt{CoEdIt Large + Jaccard Similarity (Bigrams) (threshold 0.03)}} & 0.94\\
       & \footnotesize{\texttt{CoEdIt XL + Jaccard Similarity (Bigrams) (threshold 0.07)}} & 0.94\\
       & \footnotesize{\texttt{CoEdIt Large + Levenshtein distance (threshold 0.54)}} & 0.78\\
       & \footnotesize{\texttt{CoEdIt XL + Levenshtein distance (threshold 2.33)}} & 0.81\\
      \hline
      Atomicity & \footnotesize{\texttt{REBEL}} & 0.96 \\
       &  \footnotesize{\texttt{LUKE}} & 0.94\\
      \hline
      Decontextualization & \footnotesize{\texttt{t5-large}} & 0.86 \\
      \hline
  \end{tabular}
  \caption[Results for all evaluated metrics.]{\label{table:metric-validation-bin-all}$F_1$ scores for all tested metrics. Thresholds were computed using hold out test sets (trained on 10 \% of annotations and tested on 90 \% of unseen annotations)
  }
\end{table*}
\end{comment}

\section{Annotation Platform}
\label{sec:annotation}

To facilitate the human annotations, we have created a PHP annotation platform. It supports the annotation of the quality of the claim (example in Figure~\ref{fig:platform_claim_quality}), multi-claim annotation (example in Figure~\ref{fig:platform_multi-claims}) and also cross-annotations providing data for the inter-annotator agreement evaluation.

The claim quality annotation page first presents the grading scale to let the annotators familiarize themselves with it. Then, a sample was provided with a highlighted sentence from which most of the claims should have been extracted. The claims obtained with various models are presented below. These claims were shuffled to avoid bias towards any model. Moreover, the annotators did not know which model generated/extracted each claim. Given the context sample and a claim, the annotators were asked to grade the quality by assigning scores\footnote{Scores are from range 0-3 for faithfulness and fluency, and either 0 or 1 for contextualized and atomicity.}. To help the annotators fill scores, we have provided a help popup with information on which metric they are considering now. An example of the popups is shown in Figure~\ref{fig:platform_popup}.

The second type of annotation was the multi-claim annotation, where the annotators were first asked to evaluate coverage and then the focus. These annotations were done per model. However, the model type was hidden so as not to induce any bias.

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/platform_popup_faithfullness.png}

  \caption[Platform - popup help example]{Example of popup help for the claim quality annotation. This example shows the help for the faithfullness metric and simillar popups were provided also for the other ones.}
  \label{fig:platform_popup}
\end{figure}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{figures/platform_claim_quality.pdf}
  \caption[Annotation platform - claim quality annotation screenshot]{Example of the claim quality annotation page. The annotators were asked to grade the quality of the claimss.}
  \label{fig:platform_claim_quality}
\end{figure*}

\begin{figure*}[h]
  \includegraphics[width=\textwidth]{figures/platform_multi-claims.pdf}
  \caption[Annotation platform - multi-claim annotation screenshot]{Example of the multi-claim annotation page. The annotators were asked to evaluate the coverage and focus of the claims.}
  \label{fig:platform_multi-claims}
\end{figure*}

\end{document}
