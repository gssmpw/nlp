\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021


\usepackage{subfig}
\usepackage{stfloats}
\usepackage{multirow}

\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{cuted}
\usepackage{capt-of}
\usepackage{textcomp} % textrightarrow 

\usepackage{color}
\usepackage[export]{adjustbox}

\newcommand{\ssection}[1]{\noindent {\bf #1} }


\usepackage{color}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\green}[1]{{\color{green}#1}}

% refer: https://tex.stackexchange.com/questions/657210/place-a-double-column-figure-right-underneath-the-title
%************************************* added <<<<<<<<<<<<<
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{caption}

\usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref}

\newcommand{\insertfig}{\includegraphics[width=\textwidth]{./figure/teaser.png}\captionof*{figure}{Our approach empowers augmented reality games to adapt to dynamic scenes with changing real object layouts. In this illustration, we begin by scanning the layout and creating a farm-to-table AR game. The initial game scene, depicted in (a), features labels in the format of \textit{animal building (count)}, where \textit{animal building} indicates the type of colored animals attracted to the building, and \textit{count} represents the number of such colored animals that have entered the building. Coops and hutches cater to chickens and bunnies, respectively, and animals move toward their designated buildings. For instance, the label \textit{Yellow Chicken Coop (22)} in (a) signifies that this coop is for yellow chickens, and 22 yellow chickens have entered the coop. As shown in (b), the real objects' layout is being manipulated by the player. (c) Our approach keeps track of the identities of the buildings so that the AR game continues to function seamlessly after the layout change. For example, the blue chicken coop from (a) is still recognized after it is moved to the left as shown in (c) even though it looks identical to the other coops.}}

\makeatletter
\apptocmd{\@maketitle}{\centering\insertfig}{}{}% insert the figure after authors
\makeatother
%*************************************

\begin{document}

\title{Enriching Physical-Virtual Interaction in\\ AR Gaming by Tracking Identical Real Objects}

\author{Liuchuan Yu,~Ching-I Huang,~Hsueh-Cheng Wang,~Lap-Fai Yu
\thanks{Liuchuan Yu is with the Computer Science Department, George Mason University, Fairfax, Virginia, USA.}
\thanks{Ching-I Huang is with the Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Taiwan.}
\thanks{Hsueh-Cheng Wang is with the Faculty of Electrical and Computer Engineering Department and Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Taiwan.}
\thanks{Lap-Fai Yu is with the Faculty of Computer Science Department, George Mason University, Fairfax, Virginia, USA.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8,August~2024}
{Liuchuan \MakeLowercase{\textit{et al.}}: Enriching Physical-Virtual Interaction in AR Gaming by Tracking Identical Real Objects}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2024 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle


\begin{abstract}
Augmented reality (AR) games, particularly those designed for headsets, have become increasingly prevalent with advancements in both hardware and software. However, the majority of AR games still rely on pre-scanned or static scenes, and interaction mechanisms are often limited to controllers or hand-tracking. Additionally, the presence of identical objects in AR games poses challenges for conventional object tracking techniques, which often struggle to differentiate between identical objects or necessitate the installation of fixed cameras for global object movement tracking. In response to these limitations, we present a novel approach to address the tracking of identical objects in an AR scene to enrich physical-virtual interaction. Our method leverages partial scene observations captured by an AR headset, utilizing the perspective and spatial data provided by this technology. Object identities within the scene are determined through the solution of a label assignment problem using integer programming. To enhance computational efficiency, we incorporate a Voronoi diagram-based pruning method into our approach. Our implementation of this approach in a farm-to-table AR game demonstrates its satisfactory performance and robustness. Furthermore, we showcase the versatility and practicality of our method through applications in AR storytelling and a simulated gaming robot.

Our video demo is available at: \url{https://youtu.be/rPGkLYuKvCQ}.
\end{abstract}


\begin{IEEEkeywords}
Augmented reality game, physical-virtual interaction, integer programming
\end{IEEEkeywords}


\section{Introduction}


\IEEEPARstart{A}{ugmented} reality (AR) integrates 3D virtual objects into the real world in real time~\cite{azuma1997survey}. AR games have become increasingly prevalent in recent years as AR hardware and software advance. AR games can generally be divided into two categories based on the devices they run on: mobile AR games and headset AR games. Mobile AR games are designed to be played on mobile devices such as smartphones and tablets. They use the device's camera to scan the real world and the device's projector to overlay digital content onto the real world, enhancing the user's experience. A notable example of mobile AR games is Pokémon GO (Figure~\ref{fig:pokemon_go}). Headset AR games require a headset or other wearable devices to provide an immersive AR experience. The headset typically has a display that visualizes digital content within the user's field of vision, allowing for interactive and engaging gameplay. Figure~\ref{fig:ar_games} shows some examples.

\IEEEpubidadjcol
AR games have found applications across diverse domains, including education~\cite{pellas2019augmenting, nincarean2013mobile, Weerasinghe2019, amanatidis2022augmented}, accessible learning~\cite{cai2021exploring}, rehabilitation and exercise~\cite{nelson2023augmented}, medicine and healthcare~\cite{Ducasse2019}, and digital art~\cite{Lichty2019}. Further insights can be gleaned from comprehensive reviews on the subject~\cite{geroimenko2019augmented, marto2022augmented}. Traditionally, AR games follow the practice of scanning the scene first and subsequently overlaying virtual objects. However, the real scene layout is susceptible to intentional or unintentional manipulation. Additionally, prevalent interaction methods in AR games involve controllers or hand-tracking, with limited exploration of physical-virtual interaction. While virtual proxy is feasible, as highlighted in~\cite{simeone2015substitutional}, the disparity between real and virtual objects can impact user experience, a critical aspect in gaming. Therefore, there is a compelling need to delve into enhancing physical-virtual interaction in AR games and facilitating continuous gameplay even after changes in the physical layout, without the necessity for re-scanning.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/pgo.png}
    \caption{Mobile AR game: Pokémon GO\protect\footnotemark.}
    \label{fig:pokemon_go}
\end{figure}
\footnotetext{Pokémon GO: \url{https://pokemongolive.com}}

Identical objects are ubiquitous, both in the physical and virtual realms. Whether they are identical tables and chairs in offices, classrooms, or restaurants, or identical items in real toy sets, and even in games where numerous non-player characters (NPCs) and props share identical characteristics, tracking the identities of these objects poses a non-trivial challenge. While common computer vision techniques like object identification or tracking can address this challenge to some extent, they often struggle to differentiate and track identical objects. Moreover, they may necessitate the installation of fixed cameras in the scene for comprehensive object movement tracking, making them less suitable for real-time AR experiences. Although some marker-based AR games offer a solution for tracking identical objects, they require the cumbersome process of printing and affixing markers, which is not an ideal solution for widespread application. Additionally, considering the importance of portability and energy efficiency, it might be impractical to introduce additional devices that continuously observe the scene, potentially hindering the gaming experience. Hence, it is worth exploring a cost- and power-efficient approach to track identical objects seamlessly, without relying on additional devices.

In response to these challenges, we present a streamlined optimization-based approach designed to differentiate visually identical object instances within a dynamic scene, accounting for changes in their positions and orientations. Rather than relying on fixed cameras capturing global scene changes, our approach exclusively leverages partial observations from the user's AR headset. Such observations enable our integer programming-based approach to resolve object identities and update object poses, ensuring an accurate understanding of the evolving scene layout. Notably, our approach operates without the need for continuous observation. Furthermore, our method incorporates the Voronoi diagram to expedite computations, enhancing scalability for large game scenes. Quantitative experiments were conducted to validate the effectiveness of our approach. The implementation of an AR farm-to-table game served as a practical test, confirming its performance and robustness. Furthermore, we showcased its versatility through potential applications in other game-related experiences, including AR storytelling and gaming robot.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/ar_games.png}
    \caption{Example headset AR games. Image courtesy of the game developers.}
    \label{fig:ar_games}
\end{figure}


\section{RELATED WORK}

Augmented reality serves as an effective tool to blend physical and virtual worlds. By overlaying 3D models, animations, and texts that align with a user's physical surroundings, AR provides an immersive gaming experience. Interactions with physical objects using AR predominantly involve object identification and tracking. These methodologies are crafted to identify specific objects and track their positions and orientations. We discuss the related techniques in this section.


\subsection{Object Detection for AR Content Authoring}
A fundamental challenge of augmented reality pertains to showing realistic content around the user. This challenge, in the realm of computer vision, largely relies on semantic segmentation and object detection~\cite{lee2021all} for preprocessing. For an immersive AR experience that provides real-time augmentation, such preprocessing algorithms need to run fast enough to catch up with the moving speed of humans~\cite{ko2020novel}. Conventional methods that apply feature tracking algorithms such as SIFT, or pixel-by-pixel classification of handcrafted features, such as support vector machines, faced challenges in achieving optimal segmentation performance~\cite{lee2021all}. Conversely, neural networks, particularly CNNs such as FasterRCNN~\cite{ren2015faster}, the YOLO series~\cite{redmon2018yolov3,simony2018complex}, and SSD detectors~\cite{liu2016ssd}, have witnessed substantial progress and are now seamlessly integrated into AR applications~\cite{zhang2020slimmer, tanzi2021real}. They are proficient in handling occlusion challenges within the AR domain such as overlaying virtual objects~\cite{roxas2018occlusion}. 

Furthermore, AR applications such as gaming and storytelling~\cite{liang2021scene,arstory,locationAR,snippet} that employ virtual characters need to place the characters with respect to the semantics and poses of objects in the scene to deliver realistic experiences. To ensure logical interaction with objects, considering object poses is critical. Object orientations might be deduced from the bounding boxes of scene objects integrated within a volumetric map derived from RGB-D streams~\cite{tahara2020retargetable}. 

However, object detection, even with pose information, is not inherently designed to distinguish between object instances of the same type, thereby constraining its suitability for more expansive AR applications.

\subsection{Physical-Virtual Interaction}
Due to the nature that AR overlays virtual objects onto the physical environment, the interaction between the physical environment and virtual objects has attracted increasing research interests. Similar to VR, most headset-based AR applications, including games, use controllers, eye-tracking, haptics, voice control, or hand-tracking to enable interaction with virtual objects~\cite{balakrishnan2021interaction}. AR games that involve real object interactions are relatively less explored but promise to gain traction.%, especially in industry. 

There are research efforts in the physical-virtual interaction direction. For example, Simeone et al.~\cite{simeone2015substitutional} investigated substitutional reality where virtual counterparts substitute the physical world and found that mismatches between virtual and physical objects would become an obstacle to the interaction and user experiences. Lee et al.~\cite{lee2019mixed} designed an actuator system underneath the table to synchronize physical and virtual objects in a face-to-face mixed reality environment. Min et al.~\cite{min2018virtual} presented a Virtual-Physical Interaction System (VPIS) that provided a realistic product experience in mixed reality by enabling users to manipulate a physical tangible product model. Hu et al.~\cite{hu2020enhancing} proposed a prototype to let users directly interact with physical objects, which provided active reactions in AR interactions. Hartmann et al.~\cite{hartmann2019realitycheck} presented RealityCheck to blend the player's real-world surroundings with the virtual world. Kaimoto et al.~\cite{kaimoto2022sketched} proposed Sketched Reality to support bi-directional interactions between AR-based virtual sketches and actuated tangible UIs. There are also some works~\cite{nagendran2012continuum, kim2018improving} in physical-virtual avatar interactions.

However, prior works require devices in addition to the headset to enable physical-virtual interaction. Some devices are customized such as in~\cite{lee2019mixed, hu2020enhancing}. Other devices are commercially available such as in~\cite{min2018virtual, hartmann2019realitycheck, kaimoto2022sketched}. To bypass the need of installing an extra device in addition to the AR headset in running a game, which could be inconvenient and unscalable, we proposed a computer vision-based approach to support physical-virtual interaction, which does not require any additional device. Based on the deduced identities of the real objects, our approach fuses virtual content onto the real objects. 

\subsection{Object Tracking in Augmented Reality}

Object tracking in AR encompasses both marker-based and markless methods. Fiducial marker tracking, a fundamental approach, relies on artificial markers affixed to an object's surface for user-friendly tracking purposes~\cite{kato1999marker}. Despite the widespread use of marker-based methods, such as QR code-based tracking, it is generally impractical to put markers on every object in a scene. Depending on the device used for tracking, object tracking for AR can be categorized into mobile AR tracking and headset AR tracking. While numerous works focus on mobile AR tracking, less research has been conducted on AR tracking methods that use headsets alone.

In the realm of mobile AR, various approaches have been proposed. Mooser et al.~\cite{mooser2007real} introduced an efficient and accurate object-tracking algorithm based on graph cut segmentation, eliminating the need for a preexisting 3D model. Park et al.~\cite{park2008multiple} presented a method to simultaneously track multiple 3D objects by combining object detection and tracking. Rambach et al.~\cite{rambach2017poster} introduced the concept of Augmented Things, where objects carry necessary tracking and augmentation information. Le et al.~\cite{le2021augmented} incorporated machine learning for detecting and tracking AR marker targets. Lee et al.~\cite{lee2022study} proposed a system enabling camera tracking in the real world, visualizing virtual information through object recognition and positioning. Arifitama et al.~\cite{arifitama2021mobile} investigated markerless-based tracking as a potential substitute for marker-based tracking in AR problems. For a comprehensive overview of mobile AR tracking, please refer to the review by Syed et al.~\cite{syed2022depth}. On the other hand, research on AR tracking methods based on headsets alone is relatively scarce. Frantz et al.~\cite{frantz2018augmenting} explored the augmentation of HoloLens with the Vuforia image processing SDK for neuronavigational use. Radkowski et al.~\cite{radkowski2018hololens} integrated the HoloLens into a point cloud-based tracking system using Kinect range cameras.

These AR tracking methods neither coped with multiple identical objects nor monitored the identities of tracked objects, while real environments such as offices, apartments and classrooms commonly consist of multiple identical objects (e.g., chairs, desks). Given the prevalence of AR headsets, it is important to investigate tracking methods, particularly markerless tracking methods, that use AR headsets only and are capable of deducing the object identities of multiple visually-identical objects in an environment.

\section{OVERVIEW}

\begin{figure*}[htp!]
    \centering
    \includegraphics[width=\textwidth]{./figure/overview.png}
    \caption{An overview of our approach.}
    \label{fig:overview}
    \vspace{0mm}
\end{figure*}


Figure~\ref{fig:overview} shows an overview of our technical approach. At the beginning, we partition the scene via a Vonoroi diagram, which is regarded as the search space for possible labels of detected objects in the changed layout. As objects are moved in the scene, an AR camera captures images of the scene based on which objects are detected. The pose estimation algorithm computes objects' 6DoF poses. Since the pose of the AR camera is known, we can project the detected objects back into the real world. After that, we can calculate the costs: translation cost, rotation cost, and dimension cost between possible objects in the initial layout and detected objects in the changed layout. These costs will be input into our integer programming approach, which determines the identities of the detected objects.
 

\subsection{Object Detection and Pose Estimation}

The initial stage in accurately assigning labels involves estimating the poses of objects. Previous studies have relied on either RGB, RGB-D, or point cloud data. Notable off-the-shelf implementations, such as Objectron~\cite{ahmadyan2021objectron}, are available. If a pre-existing solution for pose estimation is unavailable, we have developed a pipeline employing state-of-the-art computer vision techniques as shown in Figure~\ref{fig:pose_pipeline}. Detailed explanations of this pipeline are provided in Section~\ref{farm_to_table_game} Farm-to-Table AR Game. Subsequent to the calculation of object poses, our object label assignment algorithm is applied to ascertain their respective labels.



\subsection{Object Label Assignment Algorithm}
\ssection{Voronoi Diagram Partitioning.} Considering the efficiency and spatial structure of the environment, we utilize the Voronoi diagram to split the whole space into several partitions. Let $S_k$ be the $k$-th site in the Voronoi diagram, $C_k$ be the center point of the $k$-th site, and $P$ be the AR camera's position. Let $D_p^k$ be the distance between the camera's position and the center point of the $k$-th site, we have:

\begin{equation}
    D_p^k = |P-C_k|.
\end{equation}

According to the Voronoi diagram's properties, if the camera is within some site $S_k$, it means that among all site centers, site $S_k$'s center $C_k$ is the nearest to the camera. We define an indicator function $within$ as:

\begin{equation}
    within(P, S_k) = 
    \begin{cases}
    1 & \text{if $D_p^k$ is the minimum among all sites,}\\
    0 & \text{otherwise}.
    \end{cases}
\end{equation}

Based on the current AR camera's position, we define the probability of considering site $S_k$ as:

\begin{equation}
    Prob(S_k) = 
    \begin{cases}
    1 & \text{if $within(P,S_k)=1$,}\\
    \frac{\frac{1}{D_{p}^k}}{\sum_t{\frac{1}{D_{p}^t}}} & \text{if $within(P,S_k)=0$.}
    \end{cases}
\end{equation}


In the following, possible objects represent the objects in the initial layout, while detected objects mean the objects detected by the camera in the changed layout. As we formulate the object tracking problem as a label assignment problem solved by integer programming, we define the translation, rotation, and dimension costs for label assignment evaluations.

\ssection{Translation Cost.} For simplicity, we assume that objects are only moved horizontally, not vertically. For example, a chair is pushed around the ground plane, but not lifted. In other words, objects are moved in the x and z directions, x and z, but not the y direction. We define the translation cost as:
\begin{equation}
\label{eq:translation_cost}
    C_{\textrm{t}} = \frac{\sqrt{(x - \hat{x})^2 + (z - \hat{z})^2}}{l},
\end{equation}

\noindent where $(x, z)$ and $(\hat{x}, \hat{z})$ denote the positions of the possible object and the detected object, respectively on the xz-plane. $l$ denotes the diagonal length of the scene, which is used for normalizing the cost.

\ssection{Rotation Cost.} We assume that an object is only rotated about its $y$-axis. Accordingly, the rotation cost is defined as:
\begin{equation}
\label{eq:rotation_cost}
    C_{\textrm{r}} = \sin{\frac{|r_y - \Hat{r_y}| \pi}{360}},
\end{equation}

\noindent where $r_y$ and $\hat{r_y}$ represent the rotation (in degree) about the $y$-axis of the possible object and the detected object, respectively. We use the sine function to normalize the cost to $[0, 1]$ because (1) the maximal rotation cost happens when the object is rotated by 180° and (2) the rotation cost is symmetrical, meaning that it should have the same cost if the object is rotated by 90° or 270° (equivalent to rotating by 90° in the counter-clockwise direction).


\ssection{Dimension Cost.} We assume that the detected objects may belong to different categories (e.g., chairs, desks) with different dimensions. So we define a cost term named dimension cost $C_{\textrm{d}}$. We use $w$ for width, $h$ for height, and $d$ for depth to describe the dimension information of the bounding box of a detected 3D object. Then, we define the dimension cost $C_{\textrm{d}}$ as follows:

\begin{equation}
\label{eq:dimension_cost}
    C_{\textrm{d}} = \frac{\max(w, \hat{w})}{\min(w, \hat{w})}\times\frac{\max(h, \hat{h})}{\min(h, \hat{h})}\times\frac{\max(d, \hat{d})}{\min(d, \hat{d})},
\end{equation}

\noindent where $w$ denotes the width of the possible object and $\hat{w}$ denotes the width of the detected object. Similar for $h$ and $d$ which refer to the height and depth.

In~\eqref{eq:dimension_cost}, $C_{\textrm{d}}$ is always greater or equal to 1. Because of the possible rotation of the 3 axes in the object detection results (represented as bounding boxes), the width, height, and depth of possible objects and detected objects could be misaligned. Therefore, we use the minimal value to represent this cost.

\ssection{Integer Programming.} We formulate this object label assignment problem as an integer programming problem. Each detected object should be assigned a proper label and each label should be used at most once. In other words, one label cannot be assigned to more than one detected object.

Let $N$ be the total number of all detected objects and $M$ be the total number of possible labels. Let $i\to j$ refer to assigning the $i$-th detected object (within the field of view of the camera) with the $j$-th possible label (within all possible Voronoi sites).

Let $C_{\textrm{t}}$ be the translation cost; $C_{\textrm{r}}$ be the rotation cost; and $C_{\textrm{d}}$ be the dimension cost. Let $w_{\textrm{t}}$ be the weight of the translation cost and $w_{\textrm{r}}$ be the weight of the rotation cost. Then, we use $C^{i\to j}$ to denote the total cost of assigning the $i$-th detected object  with the $j$-th possible label:

\begin{equation}
    C^{i\to j} = C_{\textrm{d}}^{i\to j}(w_{\textrm{t}} C_{\textrm{t}}^{i\to j}  + w_{\textrm{r}} C_{\textrm{r}}^{i\to j}).
\end{equation}

The objective function refers to minimizing the total cost of assigning a label to every detected object:


\begin{equation}
\label{eq:integer_programming}
\begin{gathered}
    \min\sum\limits_{i=1}^N \sum\limits_{j=1}^M C^{i\to j} A^{i\to j}, \\ 
    s.t.
     \sum\limits_{j=1}^M A^{i\to j} = 1,\forall i \in N, \\
      \sum\limits_{i=1}^N -A^{i\to j} >= -1,\forall j \in M.
\end{gathered}
\end{equation}

Here, $A^{i\to j}$ denotes whether the assignment exists or not: 
\begin{equation}
    \ A^{i\to j} = 
    \begin{cases}
        1 & \text{if $i\to j$ exists,} \\
        0 & \text{otherwise.}
    \end{cases}
\end{equation}

\ssection{Search Space Pruning.} We can solely use~\eqref{eq:integer_programming} to solve the label assignment problem. However, considering the fact that the number of detected objects is less than the number of possible labels, it would be not efficient to search all labels and assign them to detected objects. The camera's position is helpful in pruning the possible label space since the detected results of objects farther away from the camera tend to be less accurate. Therefore, we leverage the Voronoi diagram partitioning result to help filter the possible labels. First, we sort the sites' probabilities in descending order. Second, we apply a threshold to filter out sites that are far away from the camera. Third, we only use those objects within the filtered result for integer programming. Detailed explanations of this method are provided in the threshold experiment of Section~\ref{threshold_experiment} and Fig.~\ref{fig:thresholds}.

\section{EXPERIMENTS}
To assess the effectiveness of our approach, we undertake a comprehensive examination through both quantitative and qualitative experiments. Our quantitative experiments involve the simulations of scenes with varying complexity levels and randomized translation/rotation manipulation using Gaussian noise models. Specifically, we utilize office and restaurant layouts, which are typically characterized by a multitude of identical objects such as tables and chairs. In the qualitative experiments, we implemented a Farm-to-Table AR game. Furthermore, we showcased the potential applications in AR storytelling and gaming robot, providing a broader perspective on the general applicability of our approach.

The code of our approach is available at: \url{https://github.com/gmudcxr/EnrichingARInteraction}. 

We also provide a video that shows our experiment results: \url{https://youtu.be/rPGkLYuKvCQ}.

\subsection{Quantitative Experiments}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{./figure/layouts.png}
    \caption{Screenshots of the six scenes with different complexity levels. Note that although shown in approximately the same size for visual clarity, the layouts are scaled down by different amounts due to different original sizes.}
    \label{fig:layouts}
    \vspace{0mm}
\end{figure*}

\ssection{Environment.} The system prototype for synthetic quantitative experiments was developed on Windows 11 and Unity 2020.3.20 using an Alienware Aurora R12 with 32GB RAM, Intel i7-11700F CPU, and Nvidia RTX 3070. For integer programming, we use lp\_solver\footnote{	
lp\_solve: \url{https://lpsolve.sourceforge.net/5.5/}} as the solver. 

\ssection{Simulation.} We simulate two kinds of layouts, a restaurant and an office, which are typically full of identical objects. Based on the number of sites, the number of object types, the number of objects, and the scene size, we create 3 different complexity levels, (L)ow, (M)edium, and H(igh). Table~\ref{table:scene_stat_summary} shows their statistics. Figure~\ref{fig:layouts} shows their top-down views.

\ssection{Movement Simulation.} We use the Unity asset, Bézier Path Creator, to create the wandering path and set a speed to simulate the movement of the AR camera.

\ssection{Gaussian Noise Model.} If the initial scene layout does not change, the assignment will always succeed by returning the initial label assignment as the solution, which corresponds to a cost of zero. To simulate object manipulations in the layout, we apply Gaussian noise to every object. 

We assume that one object can be moved by translation and rotation, corresponding to the aforementioned translation cost and rotation cost. For translation, we assume that the object can move along the $x$ and $z$ directions. Since we conduct simulations with Unity, we use the Unity coordinate system here, where the $y$-axis points up. As for rotation, we assume that an object only rotates about the $y$-axis. 

We use $T(t_{\textrm{m}}, t_{\textrm{s}})$ to denote the translation noise model, where $(t_{\textrm{m}}, t_{\textrm{s}})$ stands for the mean and standard deviation (SD) of the Gaussian noise model for translation $x$ and $z$. Although $x$ and $z$ share the same noise model parameters, they are independent models and work independently. We use $R(r_{\textrm{m}}, r_{\textrm{s}})$ to denote the rotation noise model with a mean of $r_{\textrm{m}}$ and a standard deviation of $r_{\textrm{s}}$. We add constraints to the translation noises and rotation noises to mimic realistic object movements. If the position of an object after applying translation noise is out of the room boundary, we will force resetting its position to be within the boundary. As for the rotation noise for object rotation, we restrict it to be within $[-360\text{°}, 360\text{°}]$. 


\begin{table}[!t]
    \caption{A summary of the six synthetic scenes.}
    \label{table:scene_stat_summary}
    \vspace{-1mm}

    \begin{adjustbox}{width=0.48\textwidth}
    \begin{tabular}{llllll}
        \hline
        \begin{tabular}[l]{@{}l@{}}\textbf{Complexity}\\ \textbf{Level}\end{tabular} & \begin{tabular}[l]{@{}l@{}}\textbf{Scene}\\ \textbf{Name}\end{tabular} & \begin{tabular}[l]{@{}l@{}}\textbf{No.}\\ \textbf{Sites}\end{tabular} & \begin{tabular}[l]{@{}l@{}}\textbf{No. Object} \\ \textbf{Types}\end{tabular} & \begin{tabular}[l]{@{}l@{}}\textbf{No.}\\ \textbf{Objects}\end{tabular} & \begin{tabular}[l]{@{}l@{}}\textbf{Scene}\\ \textbf{Size($m^2$)}\end{tabular} \\ \hline
			Low & L1 & 2 & 1 & 16 & 25  \\ 
			Low & L2 & 4 & 1 & 8  & 64  \\ 
			Medium & M1 & 5 & 3 & 35 & 64  \\
			Medium & M2 & 6 & 2 & 30 & 25  \\ 
			High & H1 & 10 & 5 & 52 & 49  \\
			High & H2 & 14 & 11 & 48 & 225 \\ 
        \hline
    \end{tabular}
    \end{adjustbox}
    \vspace{-2mm}
\end{table}




\ssection{Experiment Procedure.} For each scene, we execute the following procedure. \textbf{(a)} Pick a translation SD value $a$ from $TList=\{0.1,0.2,0.3,...,1,2,3,...,\sqrt{scene\_size}\}$ to fill the translation noise model $T(0,a)$; \textbf{(b)} Pick a rotation SD value $b$ from $RList=\{0,5,10,...,120\}$ to fill the rotation noise model $R(0,b)$, using $360/3=120$ as the maximum value of rotation model SD as data falling outside of three SDs is rare; \textbf{(c)} Use $T(0,a)$ and $R(0,b)$ to generate a noisy layout; \textbf{(d)} Let the camera move along the pre-defined path with a constant speed, which stops every 100 frames to run our approach to calculate objects' identities; \textbf{(e)} Reset the layout once the camera reaches the end; \textbf{(f)} Loop the previous steps until all values from $TList$ and $RList$ have been used once. The translation cost weight $w_{\textrm{t}}$ and the rotation cost weight $w_{\textrm{r}}$ were set to $0.36\sqrt{scene\_size}$ and $1$, respectively.


\ssection{Result Analysis.} Figure~\ref{fig:result_6scenes} shows the results. We observe the following from the results:


\begin{table*}[!ht]
    \centering
    \caption{Sites' probabilities and cumulative probabilities. Note that site S5's probability is 1.}
    \label{table:sites_prob}
    \vspace{-0.5mm}
        % \begin{adjustbox}{width=1\textwidth}
        \begin{tabular}{lllllllllll}
        \hline
        \textbf{Site}                    & \textbf{S6}    & \textbf{S3}    & \textbf{S10}   & \textbf{S9}    & \textbf{S2}    & \textbf{S4}    & \textbf{S7}    & \textbf{S1}    & \textbf{S8}    \\ \hline
        \textbf{Probability}            & 0.214 & 0.150 & 0.129 & 0.110 & 0.109 & 0.080 & 0.076 & 0.067 & 0.065 \\
        \textbf{Cumulative Probability} & 0.214 & 0.364 & 0.494 & 0.604 & 0.713 & 0.793 & 0.868 & 0.935 & 1.000 \\ \hline
        \end{tabular}
        % \end{adjustbox}
\vspace{-3mm}
\end{table*}




\begin{itemize}
\setlength\itemsep{1mm}
\item Overall, the accuracy drops as the SD of the translation noise model or rotation noise model increases, which is reasonable because the SD value increases the messiness. 

\item Even using the same translation noise model, the accuracies of the same complexity level vary. For example, as shown in Fig.~\ref{fig:result_6scenes}~(a1) and Fig.~\ref{fig:result_6scenes}~(b1), the accuracy curves in (b1) are above those in (a1). An obvious difference between scene L1 (Fig.~\ref{fig:layouts}~(a)) and scene L2 (Fig.~\ref{fig:layouts}~(b)) is that chairs in L1 are more clustered than those in L2.

\item A more complex scene does not necessarily imply a lower accuracy compared to a less complex scene. For example, medium-complexity scene M1 (Fig.~\ref{fig:layouts}~(c)) has a better accuracy than low-complexity scene L1 (Fig.~\ref{fig:layouts}~(a)) at the low translation noise level. The reason is that objects in M1 are sparser than those in L1. A similar trend is observed when comparing high-complexity scene H2 with low-complexity scene L1 as shown in Fig.~\ref{fig:result_6scenes}~(f1) and Fig.~\ref{fig:result_6scenes}~(a1).

\item When the SD of translation noises increases, the results will become steady when the accuracy does not keep dropping as shown in Fig.~\ref{fig:result_6scenes}~(a2), (c2), and (e2). This is because the objects will reach the boundaries of the scene due to a large translation noise. Therefore, if a lower SD pushes objects to the boundaries, a higher SD also pushes objects to the boundaries. This means that applying the two different translation noise models makes little difference.

\item Obviously, a low translation noise always leads to better accuracy compared to a high translation noise in the same scene.

\item In general, our approach has a better performance in the office scenes (L2, M1, and H2) than in the restaurant scenes (L1, M2, and H1). It makes sense because an office is usually sparser than a restaurant. For the office scenes (L2, M1, and H2), if the rotation noise SD is 50°, the average accuracy of low translation noise levels is 77.39\%; if the translation noise SD is 0.5 meters, the average accuracy of all rotation noise levels is 77.23\%.

\vspace{-1mm}
\end{itemize}

\begin{figure}[t]
    \vspace{0mm}
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/threshold_demo.png}
    \caption{A demonstration of the threshold experiment.}
    \label{fig:threshold_demo}
    \vspace{0mm}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{./figure/result_6scenes.png}
    \caption{Accuracies of the six scenes with different noise levels. Each scene is associated with two figures, e.g., low-complexity-level scene L1 has two figures, (a1) and (a2). The first figure (a1) depicts the accuracy across different rotation noise levels. Low translation noise refers to noise models from $T(0, 0.1)$ to $T(0, 1.0)$ with an SD increment of 0.1, while high translation noise refers to noise models from $T(0, 1.0)$ to $T(0, \sqrt{scene\_size})$ with an SD increment of 1.0. Each point represents the mean accuracy of different translation noise models. The second figure (a2) shows the accuracy of different translation noise levels. Each point represents a mean value of mean accuracies of rotation models from $R(0,0)$ to $R(0,120)$ with an SD increment of 5.}
    \label{fig:result_6scenes}
    \vspace{0mm}
\end{figure*}




The results show that our approach works reasonably well at different layout complexity levels. If the layout of objects is sparse and less clustered, our approach performs better.


\ssection{Threshold Experiment.}\label{threshold_experiment}
We conduct a threshold experiment to validate the Voronoi diagram-based pruning method. We use the same procedure as the aforementioned synthetic experiment except that \textbf{(1)} we only use noise models $T(0, 0.1)$ and $R(0, 15)$; \textbf{(2)} the wandering path is traveled once; and \textbf{(3)} when the camera stops, we will iterate each threshold from 0 to 1 by a step of 0.05, set the threshold, get the possible objects, and count the time cost of running the integer programming 100 times. We conducted this experiment on the H1 scene using $w_{\textrm{t}} = 2.52$ and $w_{\textrm{r}} = 1$.




Figure~\ref{fig:threshold_demo} shows an example. Black lines mark the boundaries of the Voronoi diagram. S1 to S10 refer to the sites of the Voronoi diagram. The red dot shows the AR camera's position. The two long pink lines indicate the camera's FOV and the short pink line indicates the camera's orientation. At this moment, the sites' probabilities are shown in Table~\ref{table:sites_prob}.




Because the thresholds are set by fixed intervals and sites' cumulative probabilities do not fit those intervals well, we take the following measurements. Those sites whose cumulative probability is firstly greater or equal to the threshold $t(t>0)$ are regarded as the sites within the threshold. For example, as shown in Table~\ref{table:sites_prob} and Fig.~\ref{fig:thresholds}, when $t$ equals 0, only site S5 is considered. When $t$ equals 0.50, sites S5, S6, S3, S10, and S9 are considered because the cumulative probability is 0.604, which is firstly greater than 0.50.

We use average accuracy and average time cost to represent the accuracy and time cost of one threshold. Figure~\ref{fig:threshold_acc_time} shows the result.





This experiment is intuitive. As shown in~\eqref{eq:integer_programming}, whatever the threshold is changed to, the total number of all detected objects, $N$, is always the same. The difference depends on $M$ which is the total number of possible labels. When the threshold increases, more sites will be considered, leading to more possible labels. In the objective function, the cost terms increase and so do the constraints. Therefore, the approach needs more time to solve the integer programming problem. Since the camera has a limited effective range, considering those objects far away from the camera does not contribute to the accuracy. That is why as the threshold reaches 0.75, the accuracy already attains 1.0.

We show that the threshold method based on the Voronoi diagram works for this task. When the threshold is properly set, our approach can take less time to achieve the same performance. This trade-off between time and accuracy could facilitate applying our approach to a large scene with many objects.

\subsection{Qualitative Experiments}\label{farm_to_table_game}

We developed a prototype Farm-to-Table AR game utilizing the Microsoft HoloLens 2 headset to validate our proposed approach. Additionally, we presented two potential applications that could benefit from our approach. One application pertains to AR storytelling, while the other uses a gaming robot. Our experiments demonstrate that our approach is not limited to tabletop AR games; it is also well-suited for room-scale AR games, particularly in scenes featuring identical objects. Furthermore, our approach proves beneficial in gaming robot, especially when the task involves identifying identical objects.


\subsubsection{\textbf{Farm-to-Table AR Game}}

In reference to the mobile farm-to-table game, Egg Inc AR Experience (Fig.~\ref{fig:farm_to_table}), we developed a headset-based Farm-to-Table AR game to showcase our approach. The game incorporates various elements, including environmental animals (goats and cattles), controlled animals (chickens and bunnies), environmental props (silos, troughs, grasses, wood stacks, and flowers), and buildings (chicken coops, bunny hutches, and an animal shed).



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{./figure/thresholds.png}
    \caption{An illustration of the threshold experiment. This is plotted based on Fig.~\ref{fig:threshold_demo}. S1-S10 refer to sites. The red star indicates the camera's position. The cell filled with blue color is considered under the corresponding threshold value. For example, as shown in the top-left sub-figure, only S5 is considered when the threshold is 0. As shown in the top-right sub-figure, S3, S5, S6, S9, and S10 are considered when the threshold is 0.50, 0.55, or 0.60.}
    \label{fig:thresholds}
    \vspace{0mm}
\end{figure*}

Figure~\ref{fig:board_voronoi} illustrates the game settings where we divide the region into 4 Voronoi sites, namely S1 - S4. The player, wearing an AR headset (Microsoft HoloLens 2), is positioned in S3. The animal shed serves as the starting point for chickens and bunnies. Chickens move towards chicken coops, while bunnies head towards bunny hutches. Both chickens and bunnies come in 5 different colors, and the corresponding coops and hutches also have matching colors. Each colored chicken or bunny moves toward the coop or hutch with the same color. For instance, a yellow chicken heads to the yellow chicken coop, while a white bunny moves to the white bunny hutch. Each coop or hutch has a label floating above it in the format of \textit{animal building (count)}. Here, \textit{animal building} indicates the type of colored animal attracted to the building, and \textit{count} represents the number of such colored animals present. The \textit{count} increases by one each time an animal with the corresponding color enters.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/threshold_acc_time_v2.png}
    \caption{Accuracy vs time cost. When the threshold reaches 0.50, the average accuracy reaches 91\%, which only takes about 50\% of time compared with the no-threshold method (i.e. threshold = 1.0). When the threshold reaches 0.75, the average accuracy reaches 1.00, which saves about 20\% of the time cost compared to not using a threshold for pruning.}
    \label{fig:threshold_acc_time}
    \vspace{-4mm}
\end{figure}


In our experiment, only the chicken coops, bunny hutches, and the animal shed physically exist. The player has the ability to reposition chicken coops and bunny hutches, but the animal shed remains fixed in its location. This restriction is due to the singular identity of the animal shed, which remains constant regardless of the layout changes.

At the implementation level, we estimate the poses of actual chicken coops and bunny hutches and then register a virtual coop or hutch to its corresponding real counterpart. This ensures that the virtual chicken aligns with the virtual coop as it reaches the real coop. Consequently, collision detection techniques are employed to trigger an increase in the counter. The running paths of chickens and bunnies are defined using Bézier curves as used in quantitative experiments, originating from the animal shed and terminating at each coop and hutch. Each chicken or bunny follows its respective path. In cases where the real coop or hutch is manipulated, we update only the endpoint of the Bézier curve after resolving their identities.


Note that while we initialize a Voronoi diagram, we set the pruning threshold to 1.0. This choice implies that all sites will be considered for integer programming. This decision is deemed reasonable given the relatively small size of the scene, where the trade-off between time and accuracy is deemed unnecessary.


Regarding the resolution of the label assignment problem, we address coops and hutches separately since a recognized coop cannot be a hutch, and vice versa. In this process, we set the weight ratio from translation cost $w_{\textrm{t}}$ to rotation cost $w_{\textrm{r}}$ at a 1:1 ratio and the dimension cost $C_{\textrm{d}}$ to 1.


In this game setting, we follow the typical AR game procedure by initially scanning and reconstructing the table. Subsequently, the game begins, allowing the player to reposition the physical coops and hutches. The animation of chickens and bunnies freezes when the player's hand becomes visible in the camera view. After the player completes the repositioning and his hand is no longer in view, the headset camera captures an image and records the camera pose. This pose is later used for reconstructing the camera pose when capturing the image. The poses of coops and hutches in the captured image are then resolved. Following this, our object label assignment algorithm identifies their respective identities and updates the game accordingly.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{./figure/pose_pipeline.png}
    \caption{The pipeline of pose estimation for our Farm-to-Table game. Please refer to the main text for a description.}
    \label{fig:pose_pipeline}
\end{figure*}


\begin{figure}[t]
    \centering
    \includegraphics[height=0.48\textwidth, keepaspectratio]{./figure/farm_to_table.png}
    \caption{A screenshot of the Egg Inc AR Experience, a mobile farm-to-table game.\protect\footnotemark.}
    \label{fig:farm_to_table}
\end{figure}

\footnotetext{Original gameplay video: \url{https://www.youtube.com/shorts/3S6_ZNuXzU4}}

Given the absence of an off-the-shelf solution for estimating the poses of coops and hutches in our experiment, we devised a pipeline using state-of-the-art computer vision techniques. Figure~\ref{fig:pose_pipeline} illustrates the pipeline. Initially, we preprocess the coops and hutches to obtain their meshes and partial textures. Using the captured image, we employ the SAM (Segment Anything Model)~\cite{kirillov2023segany} to obtain segmentations. CLIP (Contrastive Language-Image Pre-Training)~\cite{radford2021learning} is then utilized to calculate dense vector representations for each segmentation region, using the pre-trained \textit{clip-ViT-B-32} model. The same process is applied to the partial textures (templates) of the coop and hutch. Subsequently, a similarity query approach is employed to identify regions similar to the templates. Once the coops and hutches are located, their corresponding binary masks in segmentation are utilized for the next step. Differentiable rendering, bridging the gap between 2D and 3D by linking 2D image pixels to the 3D properties of a scene~\cite{keselman2022approximate}, is applied to solve the poses of coops based on the coop binary mask, the headset camera pose during image capture, and the coop mesh. The same procedure is followed for hutch masks. Here PyTorch3D\footnote{PyTorch3D: \url{https://github.com/facebookresearch/pytorch3d}} is used for differentiable rendering. Once all poses are determined, our object label assignment algorithm is executed to identify their respective object identities.


\underline{Gameplay Demonstration \#1:} In this demonstration, the player is positioned facing the Vonoroi site S1 as depicted in Figure~\ref{fig:board_voronoi}. Figure~\ref{fig:farm_exp_1} provides a visual representation of this demonstration. The virtual environment includes objects such as environmental props (e.g., grasses, flowers, troughs, wood stacks), environmental animals (e.g., goats grazing on grass, a wandering dog, and a cattle drinking water from the trough), controlled animals with various colors (chickens and bunnies), and text labels with diverse colors. On the other hand, real objects are also present, featuring an animal shed, three chicken coops, and three bunny hutches. Chickens and bunnies are observable in the scene, moving towards their respective coops and hutches. Figure~\ref{fig:farm_exp_1} (a) and Figure~\ref{fig:farm_exp_1} (b) depict the initial game scene and the updated game scene, respectively.


Analyzing the output of our object label assignment algorithm, presented in Fig.~\ref{fig:farm_exp_1} (b), we can infer the following layout changes: (1) swapping the positions of the \textbf{Blue Chicken Coop} and the \textbf{Cyan Bunny Hutch}; (2) relocating the \textbf{Magenta Bunny Hutch} to the left of the \textbf{Cyan Bunny Hutch}; (3) slightly pushing the \textbf{Yellow Chicken Coop} further away from the headset. Considering our proposed cost terms, namely translation cost and rotation cost, this assignment result appears reasonable from a cost perspective.

\underline{Gameplay Demonstration \#2:} In this demonstration, the player is facing the Voronoi site S4 as depicted in Figure~\ref{fig:board_voronoi}. Figure~\ref{fig:farm_exp_2} provides a visual representation of this demonstration. The virtual environment includes objects such as environmental props (e.g., silo, grasses, flowers, troughs, wood stacks, tractors), environmental animals (e.g., goats grazing on grass, a wandering dog, and a goat drinking water from the trough), controlled animals with various colors (chickens and bunnies), and text labels with diverse colors. On the other hand, real objects are visible, featuring part of the animal shed, two chicken coops, and two bunny hutches. Chickens and bunnies can be observed in the scene, moving towards their respective coops and hutches. Figure~\ref{fig:farm_exp_2} (a) and Figure~\ref{fig:farm_exp_2} (b) depict the initial game scene and the updated game scene, respectively.


Examining the output of our object label assignment algorithm, presented in Figure~\ref{fig:farm_exp_2} (b), we can deduce the layout changes: (1) exchanging the positions of the \textbf{Orange Chicken Coop} and the \textbf{White Bunny Hutch}; and (2) rotating these objects. Evaluating these changes from the perspective of our proposed cost terms—translation cost and rotation cost—this assignment result is reasonable in terms of minimizing the associated costs.

\subsubsection{\textbf{AR Storytelling Showcase}}



We use another scene, a partially scanned lab layout as shown in Figure~\ref{fig:init_exp} to showcase our approach's applications on AR storytelling and gaming robot. First, we scanned and reconstructed the partial layout. We also modeled a 3D chair, similar to the lab's real chair. In the experiment, we moved the chairs in the lab. There were 7 chairs in the scene. For pose estimation, here we used the off-the-shelf implementation, \textit{MediaPipe Objectron}~\cite{ahmadyan2021objectron}\footnote{MediaPipe Objectron: \url{https://github.com/google/mediapipe/blob/master/docs/solutions/objectron.md}}.


Considering the small layout size and the small number of possible objects (i.e. chairs), we did not apply the Voronoi diagram partitioning algorithm here as a trade-off between time and accuracy was unnecessary.



We implemented an AR storytelling scenario as shown in Figure~\ref{fig:story_demo}. A user was watching an AR story, where two virtual characters sat on two real chairs (Fig.~\ref{fig:story_demo} (a)). More specifically, the male character sat on Chair 1 and the female character sat on Chair 2. After some time, Chair 1 and Chair 2 were manipulated by a person passing by. It would look unnatural if the program did not adjust the two virtual characters as they would then be sitting in the air, not on the real chairs.

The AR headset that the person used for watching the AR story captured an image of the scene. The pose estimation algorithm took this image as input and output 6DoF poses. Our approach utilized the AR headset's camera pose and the pose estimation results to determine the chairs' identities, based on which the two virtual characters' positions and rotations (Fig.~\ref{fig:story_demo} (b)) were updated. As a result, the two virtual characters still appeared sitting on the same chairs as before.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/board_voronoi.png}
    \caption{An illustration of the game settings.}
    \label{fig:board_voronoi}
\end{figure}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{./figure/farm_exp_1.png}
    \caption{A demonstration of our AR headset Farm-to-Table game when the player faces Voronoi site S1 (Fig.~\ref{fig:board_voronoi}). (a) illustrates the initial game scene before any layout change, while (b) portrays the updated game scene after executing the layout changes and running our object label assignment algorithm. We can deduce the following layout adjustments from (b): (1) swapping Blue Chicken Coop and Cyan Bunny Hutch positions; (2) moving Magenta Bunny Hutch left of Cyan Bunny Hutch; (3) slightly shifting Yellow Chicken Coop away from the headset.}
    \vspace{0mm}
    \label{fig:farm_exp_1}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{./figure/farm_exp_2.png}
    \caption{A demonstration of our AR headset Farm-to-Table game when the player faces Voronoi site S4 ( Fig.~\ref{fig:board_voronoi}). (a) illustrates the initial game scene before any layout change, while (b) portrays the updated game scene after executing the layout changes and running our object label assignment algorithm. From the label assignment result, We can infer two layout changes: (1) swapping Orange Chicken Coop and White Bunny Hutch positions, and (2) rotating these objects.}
    \vspace{0mm}
    \label{fig:farm_exp_2}
\end{figure*}


\subsubsection{\textbf{Gaming Robot Showcase}}


We simulated a gaming robot. The objective for the robot is quite simple, i.e., making a delivery to a designated chair. We used LoCoBot\footnote{LoCoBot: \url{http://www.locobot.org/}} as the robot platform. It was equipped with an Intel Realsense Depth Camera D435, which captured images for pose estimation. We set Chair 4 as the target that the LocoBot should move to. In the beginning, we made Chair 4 offset from its original position as shown in Figure~\ref{fig:robot_exp}(a).



\begin{figure}[btp!]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/init_exp.png}
    \vspace{3pt}
    \caption{A screenshot of the scanned lab environment. We use this as the initial layout. The numbers from 1 to 7 indicate movable chairs. The chair model is manually created according to the real chair.}
    \label{fig:init_exp}
    % \vspace{mm}
\end{figure}

\begin{figure}[btp!]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/story_demo.png}
    \vspace{5pt}       
        \caption{An AR storytelling example. (a) The initial view of an AR story where two virtual characters sit on real chairs. (b) The layout's 3D visualization. (c) An updated AR view showing that the two virtual characters still sat on the same chairs as before in the changed layout. 
        }
    \vspace{5mm}               
    \label{fig:story_demo}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/robot_pipeline.png}
    \caption{The pipeline of the gaming robot showcase. First, our system subscribed to the aligned RGBD image and odometry topics. Then, the pose estimation algorithm solves the 6DoF poses. Considering both the initial and changed layouts, our approach determines the objects' identities. If the target object is within the result, its position is sent to the robot, which then moves to the target.}
    \label{fig:robot_pipeline}
    \vspace{0mm}
\end{figure}


As Figure~\ref{fig:robot_pipeline} shows, our system subscribed to the aligned RGBD image and odometry topics. The pose estimation was run on the RGB image. If it outputs a valid 6DoF pose, its corresponding depth image will be used to estimate the distance. After that, the world transformation of the object can be calculated using the camera's pose. Our object label assignment algorithm is then run to determine the object's identity label. If the target object is within the result, its position is sent to the robot which then moves to the target.




For the experiment procedure, we first set up the robot, gave it a delivery target, and let it move forward. As our approach detected the target after pose estimation and label assignment, a target position was issued to the robot. Because of that, the robot would cancel the current goal first and execute the new goal, i.e., moving toward the target. 

As Figure~\ref{fig:robot_exp} shows, the robot reached the expected target, showing that our approach performed as expected in the real environment.

\section{DISCUSSION}

Our approach excels in distinguishing visually identical object instances. It can also incorporate a pruning algorithm to process a large environment with numerous objects more efficiently. Its effectiveness has been successfully validated across a spectrum of simulated environments, spanning various levels of complexity. We implemented a Farm-to-Table game based on an AR headset, demonstrating the practical application of our approach. Moreover, our approach can smoothen AR storytelling experiences by leveraging tracked real objects. Our approach can also be applied to the domain of gaming robot.




\subsection{Limitations}

Within the realm of AR games, user experience holds paramount importance, with immersiveness serving as a key contributing factor. While our approach proficiently resolves the identities of identical objects following layout changes, it does not generate dynamic AR visual effects introduced by such changes, such as light variations and shadows. Beyond object identity resolution, it is useful to extend the AR gaming pipeline to incorporate real-time visual updates based on the tracked objects during the AR gameplay, which would contribute to a more compelling and immersive AR gameplay experience.

Our approach hinges on the precision of pose estimation. Any inaccuracy in this aspect can lead to mislabeled results. At the hardware level, the HoloLens 2 has a limited field of view, restricting its ability to capture multiple objects simultaneously, especially in large scenes. The more objects it captures, the more constraints are introduced, resulting in better integer programming outcomes in general. While our algorithm can handle the simultaneous assignment of multiple object identities, its effectiveness depends on the number of objects that can be detected. Furthermore, in terms of the pose estimation algorithm, utilizing a versatile pose estimation approach that does not require scanned meshes and texture could broaden the applicability of our method.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{./figure/robot_exp.png}
        
        \caption{The gaming robot simulation. (a) The changed layout in the real world in the beginning; (b) the scene when the robot reaches the target in the end.
        }
    \label{fig:robot_exp}
\end{figure}


In this paper, our assumptions revolve around the movement and rotation capabilities of objects within our model. We consider objects capable of movement along two primary directions: the $x$-axis, corresponding to front-and-back motion, and the $z$-axis, corresponding to left-and-right motion. We do not account for movements in the up-and-down direction in our model. Furthermore, our assumption restricts the rotation of objects to occur solely about the $y$-axis. In some real-world scenarios, objects may have more degrees of freedom when it comes to movement and rotation. In reality, objects can rotate freely about various axes, leading to a more complex and versatile range of spatial transformations. Our simplified assumptions are made for the sake of modeling simplicity and computational efficiency but should be understood in the context of these real-world limitations.


\subsection{Future Work}

In seeking a better pose estimation result, currently, a game pause is required to await the completion of layout changes, perform pose estimation, and subsequently update the game in our implementation. However, this pause in gameplay may impact user experience. To refine this aspect, a real-time update is preferable. This entails synchronizing the game updates with the ongoing changes in the real layout, eliminating the need for pauses. By adopting a real-time update strategy, it is possible to seamlessly integrate alterations to the physical environment with the virtual elements, enhancing the user experience by providing a more fluid and continuous gameplay interaction.


In our setup, real-world objects function as NPCs, with virtual objects interacting with them dynamically. NPCs often host events in games, and enhancing player immersion involves detecting their actions and updating the game scene accordingly. For instance, imagine a scenario that triggers virtual chickens and bunnies to emerge right after the player opens the gate of the real animal shed. Similarly, it could be designed to prompt designated chickens to alter their routine and relocate to another coop once the player removes the stairs from a coop, which will add depth to the AR gaming experience.


In our experiment, physical objects remain stationary during the gameplay. However, future AR games may involve scenarios where physical objects exhibit the same dynamism as virtual ones. Consider a cooking AR game where players manipulate real kitchen tools such as solid turners. The AR gaming experience would become more realistic and immersive if players could use authentic kitchen tools such as turners and pans to interact with virtual food elements. Such an experience would surpass the realism attained through hand-tracking or controller-based simulations.


\section{Conclusion}
We presented a novel approach to enrich physical-virtual interaction in AR gaming by tracking identical real objects. Our method leverages partial scene observations captured by an AR headset and solves object identities by integer programming. First, we explored its effectiveness through comprehensive quantitative experiments. Second, we implemented a Farm-to-Table AR game to validate our approach. Moreover, we presented two potential applications, AR storytelling and gaming robot, that could benefit from our approach.


\bibliographystyle{IEEEtran}
\bibliography{main}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./bio/bio_liuchuan_yu.jpg}}]{Liuchuan Yu} is currently pursuing a Ph.D. in Computer Science in the Design Computing and Extended Reality (DCXR) group supervised by Professor Lap-Fai (Craig) Yu at George Mason University. Prior to embarking on his doctoral studies, he earned his bachelor's degree in Remote Sensing Science and Technology from Shandong University of Science and Technology in 2015. Subsequently, he attained his master's degree in Software Engineering from Beijing Jiaotong University in 2020. His academic focus centers on Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), and Human-Computer Interaction (HCI). His overarching research objective is to leverage the synergy of Artificial Intelligence (AI) and Extended Reality (XR) technologies to enhance human performance.
\end{IEEEbiography}

\vspace{11pt}

\begin{IEEEbiography}[{\vspace{-6mm} \includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./bio/bio_isabella_huang.jpg}}]{Ching-I Huang} is a Ph.D. candidate at the Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University, Taiwan. 
Her research centers on human-robot collaboration, emphasizing learning-based autonomy and VR applications for service robot system development.
\end{IEEEbiography}

\vspace{11pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./bio/bio_prof_nick_wang.jpg}}]{Hsueh-Cheng Wang} is an associate professor in the Department of Electrical and Computer Engineering and Institute of Electrical and Control Engineering at National Yang Ming Chiao Tung University, Taiwan. Dr. Wang's group concentrates on creating field and service robotic systems to address real-world challenges. His research areas include robot navigation using deep reinforcement learning, robust perception with multi-modal sensors, and human-robot interaction.
\end{IEEEbiography}

\vspace{11pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./bio/bio_prof_craig_yu.jpg}}]{Lap-Fai (Craig) Yu} is an associate professor of computer science at George Mason University, where he leads the Design Computing and Extended Reality (DCXR) group to pursue research in virtual reality, computer graphics, and human-computer interaction. He obtained his Ph.D. degree in Computer Science from UCLA with an Outstanding Recognition in Research Award. His research has been featured by New Scientist, the UCLA Headlines, and the IEEE Xplore Innovation Spotlight; and has won Best Paper Honorable Mention Awards at 3DV and CHI conferences. He received an NSF CRII award and an NSF CAREER award for his research achievements in computational design and virtual reality. He served as a member of the Panel on Assessment of Humans in Complex Systems of the National Academies. He regularly serves on the technical program committees of ACM SIGGRAPH, ACM CHI, and IEEE VR, and as an Associate Editor for the ACM Transactions on Graphics.
\end{IEEEbiography}

\vfill

\end{document}
