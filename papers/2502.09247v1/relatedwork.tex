\section{Related Work}
Entity recognition and relation extraction stand as pivotal sub-tasks within the realm of natural language processing(NLP). The initial two-stage model was gradually abandoned due to the difficulty in achieving interaction between two sub-tasks and the problem of error propagation. Currently, the joint extraction model for unified modeling the two tasks is used, and the extracted structured triplets are convenient for constructing large-scale knowledge graphs(KG) or completing other downstream tasks.    

\textbf{Parameter sharing-based methods} The shared-parameter joint extraction framework has been shown to enhance the performance of relation extraction through the synergistic optimization of entity recognition processes. Wei et al.\cite{wei-etal-2020-novel} utilizes the binary taggers to predict all potential head entities, and subsequently utilizes these predictions to infer their corresponding tail entities and relations. Fu et al.\cite{fu2019graphrel} innovatively constructs entities and relations as graphs, where entities are nodes and relations are edges. The method Bekoulis et al.\cite{bekoulis2018joint} proposed is similar to the work Huang et al.\cite{huang2019bert} proposed, they both use BIO annotation combined with CRF for entity recognition and allow for the prediction of multiple potential relations between a word and other words during relation extraction. The only difference lies in the selection of the basic encoder. However, the multi-head selection method lacks the use of semantic information for judgment. STER\cite{zhao2022exploring} is a model that uses knowledge distill, it has two teachers that study the information for entity recognition and relation extraction respectively. Our work, similar to SpERT\cite{Eberts2019SpanbasedJE} and SPAN\cite{ji2020span}, utilizes spans for overlapping entity recognition. However, neither of these methods can effectively achieve task interaction, and SpERT lacks effective acquisition of complex semantics. On the other hand, SPAN achieves sentence-level semantic supplementation through multiple attention mechanisms, which often require more storage of attention parameters.

\textbf{Joint decoding algorithm-based methods} The methods based on the decoding algorithm often blur the distinction between entity recognition and relation extraction, enabling the direct extraction of triplets in a single step. Common approaches that use joint decoders include tag-based and table-based. Zheng et al.\cite{zheng-etal-2017-joint} proposed a method that involves assigning a tag to each input token to denote the head or tail entities along with the relations, and subsequently inferring the triplet through tag prediction and parsing. However, the method based on tables often generates one table for each relation. Feiliang Ren et al.\cite{ren-etal-2021-novel} introduces a global feature-oriented relational triple extraction model, Wang et al.\cite{wang-etal-2020-tplinker} proposed a method that formulates joint extraction as a token pair linking problem and introduces a novel handshaking tagging scheme that aligns the boundary tokens of entity pairs under each relation type. Shang Y M et al.\cite{shang2022onerel} introduce a novel scoring based classifier to parallel tag and a Rel-Spec Horns Tagging strategy to ensure efficient decoding. Nevertheless, these methods often rely on predefined encoding and decoding strategies, and these methods often lack entity type constraints.

\textbf{Generative models-based methods} With the widespread application of generative models such as Transformer\cite{vaswani2017attention} in multiple fields, some researchers have also attempted to use generative models for entity relation extraction. Dianbo Sui et al.\cite{sui2023joint} used generative models to generate sets, where the elements in the set are the desired triplets. However, the output produced by the generative model is more stochastic, and this model often relies on richer training data. LinkNER\cite{zhang2024linkner} is the first work exploring how to fine-tune models synergistically with LLMs for NER tasks. Asada M et al.\cite{asada2024enhancing} Proposed a novel relation extraction method enhanced by large language models (LLMs).