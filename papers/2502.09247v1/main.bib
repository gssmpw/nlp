@article{nasar2021named,
  title={Named entity recognition and relation extraction: State-of-the-art},
  author={Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={1},
  pages={1--39},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@inproceedings{li2022unified,
  title={Unified named entity recognition as word-word relation classification},
  author={Li, Jingye and Fei, Hao and Liu, Jiang and Wu, Shengqiong and Zhang, Meishan and Teng, Chong and Ji, Donghong and Li, Fei},
  booktitle={proceedings of the AAAI conference on artificial intelligence},
  volume={36},
  number={10},
  pages={10965--10973},
  year={2022}
}
@inproceedings{wei-etal-2020-novel,
    title = "A Novel Cascade Binary Tagging Framework for Relational Triple Extraction",
    author = "Wei, Zhepei  and
      Su, Jianlin  and
      Wang, Yue  and
      Tian, Yuan  and
      Chang, Yi",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.136",
    doi = "10.18653/v1/2020.acl-main.136",
    pages = "1476--1488",
    abstract = "Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online.",
}
@inproceedings{fu2019graphrel,
  title={Graphrel: Modeling text as relational graphs for joint entity and relation extraction},
  author={Fu, Tsu-Jui and Li, Peng-Hsuan and Ma, Wei-Yun},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={1409--1418},
  year={2019}
}
@inproceedings{Eberts2019SpanbasedJE,
  title={Span-based Joint Entity and Relation Extraction with Transformer Pre-training},
  author={Markus Eberts and Adrian Ulges},
  booktitle={European Conference on Artificial Intelligence},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:202583766}
}
@article{bekoulis2018joint,
  title={Joint entity recognition and relation extraction as a multi-head selection problem},
  author={Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},
  journal={Expert Systems with Applications},
  volume={114},
  pages={34--45},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{mintz2009distant,
  title={Distant supervision for relation extraction without labeled data},
  author={Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  booktitle={Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
  pages={1003--1011},
  year={2009}
}
@article{zelenko2003kernel,
  title={Kernel methods for relation extraction},
  author={Zelenko, Dmitry and Aone, Chinatsu and Richardella, Anthony},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1083--1106},
  year={2003}
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{zheng-etal-2017-joint,
    title = "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
    author = "Zheng, Suncong  and
      Wang, Feng  and
      Bao, Hongyun  and
      Hao, Yuexing  and
      Zhou, Peng  and
      Xu, Bo",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1113",
    doi = "10.18653/v1/P17-1113",
    pages = "1227--1236",
    abstract = "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What{'}s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",
}
@inproceedings{miwa2014modeling,
  title={Modeling joint entity and relation extraction with table representation},
  author={Miwa, Makoto and Sasaki, Yutaka},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1858--1869},
  year={2014}
}
@article{su2022global,
  title={Global pointer: Novel efficient span-based approach for named entity recognition},
  author={Su, Jianlin and Murtadha, Ahmed and Pan, Shengfeng and Hou, Jing and Sun, Jun and Huang, Wanwei and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2208.03054},
  year={2022}
}
@inproceedings{wang-etal-2020-tplinker,
    title = "{TPL}inker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking",
    author = "Wang, Yucheng  and
      Yu, Bowen  and
      Zhang, Yueyang  and
      Liu, Tingwen  and
      Zhu, Hongsong  and
      Sun, Limin",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.138",
    doi = "10.18653/v1/2020.coling-main.138",
    pages = "1572--1582",
    abstract = "Extracting entities and relations from unstructured text has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in identifying overlapping relations with shared entities. Prior works show that joint learning can result in a noticeable performance gain. However, they usually involve sequential interrelated steps and suffer from the problem of exposure bias. At training time, they predict with the ground truth conditions while at inference it has to make extraction from scratch. This discrepancy leads to error accumulation. To mitigate the issue, we propose in this paper a one-stage joint extraction model, namely, TPLinker, which is capable of discovering overlapping relations sharing one or both entities while being immune from the exposure bias. TPLinker formulates joint extraction as a token pair linking problem and introduces a novel handshaking tagging scheme that aligns the boundary tokens of entity pairs under each relation type. Experiment results show that TPLinker performs significantly better on overlapping and multiple relation extraction, and achieves state-of-the-art performance on two public datasets.",
}

@inproceedings{ren-etal-2021-novel,
    title = "A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling",
    author = "Ren, Feiliang  and
      Zhang, Longhui  and
      Yin, Shujuan  and
      Zhao, Xiaofeng  and
      Liu, Shilei  and
      Li, Bochao  and
      Liu, Yaduo",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.208",
    doi = "10.18653/v1/2021.emnlp-main.208",
    pages = "2646--2656",
    abstract = "Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a table feature for each relation. Then two kinds of global associations are mined from the generated table features. Next, the mined global associations are integrated into the table feature of each relation. This {``}generate-mine-integrate{''} process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation{'}s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed model on three benchmark datasets. Experimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: \url{https://github.com/neukg/GRTE}.",
}
@article{zhong2020frustratingly,
  title={A frustratingly easy approach for entity and relation extraction},
  author={Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2010.12812},
  year={2020}
}
@article{wang2019extracting,
  title={Extracting multiple-relations in one-pass with pre-trained transformers},
  author={Wang, Haoyu and Tan, Ming and Yu, Mo and Chang, Shiyu and Wang, Dakuo and Xu, Kun and Guo, Xiaoxiao and Potdar, Saloni},
  journal={arXiv preprint arXiv:1902.01030},
  year={2019}
}
@inproceedings{shang2022onerel,
  title={Onerel: Joint entity and relation extraction with one module in one step},
  author={Shang, Yu-Ming and Huang, Heyan and Mao, Xianling},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={36},
  number={10},
  pages={11285--11293},
  year={2022}
}
@inproceedings{cao2021generativere,
  title={GenerativeRE: Incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction},
  author={Cao, Jiarun and Ananiadou, Sophia},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2119--2126},
  year={2021}
}
@article{sui2023joint,
  title={Joint entity and relation extraction with set prediction networks},
  author={Sui, Dianbo and Zeng, Xiangrong and Chen, Yubo and Liu, Kang and Zhao, Jun},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}
@article{lu2022unified,
  title={Unified structure generation for universal information extraction},
  author={Lu, Yaojie and Liu, Qing and Dai, Dai and Xiao, Xinyan and Lin, Hongyu and Han, Xianpei and Sun, Le and Wu, Hua},
  journal={arXiv preprint arXiv:2203.12277},
  year={2022}
}
@inproceedings{zeng2014relation,
  title={Relation classification via convolutional deep neural network},
  author={Zeng, Daojian and Liu, Kang and Lai, Siwei and Zhou, Guangyou and Zhao, Jun},
  booktitle={Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers},
  pages={2335--2344},
  year={2014}
}
@article{miwa2016end,
  title={End-to-end relation extraction using lstms on sequences and tree structures},
  author={Miwa, Makoto and Bansal, Mohit},
  journal={arXiv preprint arXiv:1601.00770},
  year={2016}
}
@article{zheng2017joint1,
  title={Joint entity and relation extraction based on a hybrid neural network},
  author={Zheng, Suncong and Hao, Yuexing and Lu, Dongyuan and Bao, Hongyun and Xu, Jiaming and Hao, Hongwei and Xu, Bo},
  journal={Neurocomputing},
  volume={257},
  pages={59--66},
  year={2017},
  publisher={Elsevier}
}
@inproceedings{ren2017cotype,
  title={Cotype: Joint extraction of typed entities and relations with knowledge bases},
  author={Ren, Xiang and Wu, Zeqiu and He, Wenqi and Qu, Meng and Voss, Clare R and Ji, Heng and Abdelzaher, Tarek F and Han, Jiawei},
  booktitle={Proceedings of the 26th international conference on world wide web},
  pages={1015--1024},
  year={2017}
}
@inproceedings{yu2010jointly,
  title={Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach},
  author={Yu, Xiaofeng and Lam, Wai},
  booktitle={Coling 2010: Posters},
  pages={1399--1407},
  year={2010}
}
@inproceedings{carreras-marquez-2004-introduction,
    title = "Introduction to the {C}o{NLL}-2004 Shared Task: Semantic Role Labeling",
    author = "Carreras, Xavier  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004",
    month = may # " 6 - " # may # " 7",
    year = "2004",
    address = "Boston, Massachusetts, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-2412",
    pages = "89--97",
}
@inproceedings{huang2019bert,
  title={Bert-based multi-head selection for joint entity-relation extraction},
  author={Huang, Weipeng and Cheng, Xingyi and Wang, Taifeng and Chu, Wei},
  booktitle={Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9--14, 2019, Proceedings, Part II 8},
  pages={713--723},
  year={2019},
  organization={Springer}
}
@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}
@inproceedings{dixit2019span,
  title={Span-level model for relation extraction},
  author={Dixit, Kalpit and Al-Onaizan, Yaser},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={5308--5314},
  year={2019}
}
@article{zhao2022exploring,
  title={Exploring privileged features for relation extraction with contrastive student-teacher learning},
  author={Zhao, Xiaoyan and Yang, Min and Qu, Qiang and Xu, Ruifeng and Li, Jieke},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}
@article{wang2023gpt,
  title={Gpt-ner: Named entity recognition via large language models},
  author={Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  journal={arXiv preprint arXiv:2304.10428},
  year={2023}
}

@inproceedings{chan2011exploiting,
  title={Exploiting syntactico-semantic structures for relation extraction},
  author={Chan, Yee Seng and Roth, Dan},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies},
  pages={551--560},
  year={2011}
}
@article{liu2023novel,
  title={A novel pipelined end-to-end relation extraction framework with entity mentions and contextual semantic representation},
  author={Liu, Zhaoran and Li, Haozhe and Wang, Hao and Liao, Yilin and Liu, Xinggao and Wu, Gaojie},
  journal={Expert Systems with Applications},
  volume={228},
  pages={120435},
  year={2023},
  publisher={Elsevier}
}
@article{tang2023boundary,
  title={Boundary regression model for joint entity and relation extraction},
  author={Tang, Ruixue and Chen, Yanping and Qin, Yongbin and Huang, Ruizhang and Zheng, Qinghua},
  journal={Expert Systems with Applications},
  volume={229},
  pages={120441},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{ji2020span,
  title={Span-based joint entity and relation extraction with attention-based span-specific and contextual semantic representations},
  author={Ji, Bin and Yu, Jie and Li, Shasha and Ma, Jun and Wu, Qingbo and Tan, Yusong and Liu, Huijun},
  booktitle={Proceedings of the 28th international conference on computational linguistics},
  pages={88--99},
  year={2020}
}
@inproceedings{zhang2024linkner,
  title={Linkner: Linking local named entity recognition models to large language models using uncertainty},
  author={Zhang, Zhen and Zhao, Yuhua and Gao, Hang and Hu, Mengting},
  booktitle={Proceedings of the ACM on Web Conference 2024},
  pages={4047--4058},
  year={2024}
}
@inproceedings{asada2024enhancing,
  title={Enhancing Relation Extraction from Biomedical Texts by Large Language Models},
  author={Asada, Masaki and Fukuda, Ken},
  booktitle={International Conference on Human-Computer Interaction},
  pages={3--14},
  year={2024},
  organization={Springer}
}
@inproceedings{xiong2022multi,
  title={A multi-gate encoder for joint entity and relation extraction},
  author={Xiong, Xiong and Liu, Yunfei and Liu, Anqi and Gong, Shuai and Li, Shengyang},
  booktitle={China National Conference on Chinese Computational Linguistics},
  pages={163--179},
  year={2022},
  organization={Springer}
}