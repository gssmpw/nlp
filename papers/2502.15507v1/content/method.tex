\section{Methodology}
\subsection{Constructing Steering Dataset}

\textbf{Model Choice} \quad
We build steering datasets for 7B parameter Llemma \cite{llemma} and 7B parameter InternLM2 \cite{internlm}. These models are trained to generate formal theorems  and code, which is important for the tactic prediction task.

\textbf{Source Dataset}\quad We constructed steering pairs from a randomly sampled subset $S$ from \textit{Lean-STaR} data. The subset consists of approximately ten thousand unique proof stages and tactics. We treat this subset as natural prompt data $p_{2}$ and generate a new set  $p_{1}$ by adding reasoning steps in $p_{2}$ representing a lean stage $l$. This generates a set of pairs ($p_{1}$, $p_{2}$) which we then prompt InternLM2 to predict the next tactic $t_{1}$ and $t_{2}$ respectively. We then take pairs where $t_{1} \neq t_{2}$ creating a subset $s$
of prompt pairs. We then validate tactics $t_1, t_2 \in s$ with Lean Prover to generate $s^{\prime}$.

This process generates a quadruple $\{p_1, p_2, t_1, t_2\}$ where $t_1$ and $t_2$ are valid tactics for a lean stage $l$. We assume that $t_1$ is a more optimal tactic for $l$. 

\textbf{Generating \( p_1 \) from \( p_2 \)}\quad In Fig. \ref{fig:data_creation}, we illustrate the process of improving theorem-proving LLMs using \textit{StepBackReasoning} \cite{zheng2024stepbackevokingreasoning}. Initially, the model is given a prompt asking it to predict the next tactic in a Lean 4 proof. Without additional reasoning guidance, the model produces an incorrect output, such as $rfl$. which does not align with the required proof strategy. To address this, we introduce a step-back reasoning mechanism. First, we extract the proof state from the given Lean prompt, as shown in the \textit{StepBackAbstraction} module. This abstraction step helps identify the core mathematical principles underlying the theorem. Using GPT-4, we generate step-back reasoning prompts, which explicitly highlight key mathematical structures, such as exponentiation in a division monoid and properties like associativity and commutativity. These enriched insights are then incorporated into the original theorem-proving prompt, providing the model with a more structured understanding of the problem. As a result, when the theorem-proving LLM is queried again, it produces the correct output $rw pow_mul$ which correctly applies the relevant exponentiation rule. This process demonstrates how structured reasoning about the problem enhances logical coherence and guides LLMs toward more accurate proof generation in Lean 4.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{imgs/data_creation.pdf}
    \caption{Steering Vectors are computed as difference of activations of $p_{1}$ and $p_{2}$}
    \label{fig:data_creation}
\end{figure}



\subsection{Constructing Steering Vectors}
Given dataset of steering pairs and tactics $\{p_1, p_2, t_1, t_2\} \in s^{\prime}$ and a model $M$, we apply a forward pass to every $M(p_1)$,$M(p_2)$ to collect values of the \textit{residual stream} vector on queries at the last token of the input layer $\ell \in \{1,...,L\}$. We isolate the internal representation corresponding to the reasoning by computing the difference in residual stream vectors $v_{1,\ell}$ and $v_{2,\ell}$. More formally, we compute the vector $u_\ell$ representing the direction of the steering at layer $\ell$:
\[
\mathbf{u}_\ell = \frac{\mathbf{v}_\ell}{\|\mathbf{v}_\ell\|}, \quad \text{where} \quad \mathbf{v}_\ell = \frac{1}{N} \sum_{i}^{N} \left( \mathbf{p}_{1,\ell,i} - \mathbf{p}_{2,\ell,i} \right)
\]

Averaging our different proof states to capture activation values most closely associated with the structured reasoning step independent of the query.  The calculation of the direction of the steering is carried out using the representations in the last token of the input, which effectively encapsulates the behavior of the model not only for the next token prediction task, but also for the entire generation following \cite{todd2024function, Scalena_2024}. After identifying steering direction, we compute steering vector by re-scailing unit vector $u_\ell$ by a coefficient $c$. We use a systematic scaling approach where the value of c is selected to ensure that residual stream activations are assigned to their mean value on inputs that contain the structured reasoning steps.  In particular, we compute a new example with residual stream values $p^\prime$ at a given token. 

\[
c = \bar{z} - p_{\ell}^{\prime} u_\ell, \quad \text{where} \quad \bar{z} = \frac{1}{N} \sum_{i}^{N} p_{1,i,\ell} u_\ell.
\]
The steering vector $cu_\ell$ is then added to the corresponding residual stream layer and the forward pass is resumed with the updated residual stream value  $\tilde{x}_\ell^\prime = x_\ell^\prime + cu_\ell$. This procedure is carried out at a single layer across all token positions, motivated by previous findings that show models tend to deviate from instructions as they generate more tokens \cite{stolfo2024improvinginstructionfollowinglanguagemodels, li2024measuring}

