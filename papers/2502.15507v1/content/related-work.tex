\section{Related Work}
\subsection{Formal Theorem Proving}
Formal theorem proving encodes theorems and proofs in a machine-verifiable format, ensuring correctness through rigid logical rules. A key component of this field is Interactive Theorem Proving (ITP), where humans collaborate with 	\textit{proof assistants} such as Isabelle~\cite{wenzel2008isabelle}, Lean~\cite{de2015lean}, and Coq~\cite{coq} to formally verify proofs. These assistants allow users to express theorems in higher-order logic and construct verifiable proofs.

In Lean~\cite{de2015lean}, proofs are built using \textit{tactics}, which either solve a goal or decompose it into sub-goals.

\subsection{Proofstep Generation with Large Language Models}  
Generating intermediate proof steps is a fundamental challenge in theorem proving, particularly in tactic-based automated theorem provers (ATPs). Early neural approaches~\citep{holophrasm, gamepad, holist, graph_representation, proverbot} framed proofstep generation as a classification task, employing models like TreeLSTM and RNN to predict tactics and their arguments. ASTactic~\citep{coqgym} later introduced a grammar-constrained decoder for structured tactic generation.

Recent advances leverage large language models (LLMs) for proof generation, casting tactic prediction as an 	extit{auto-regressive sequence modeling} problem. GPT-$f$~\citep{gptf} pioneered this approach by training transformers only with decoders to generate structured proof steps. Baldur~\citep{baldur} extended this by producing entire proofs, while POETRY~\citep{recursive} adopted a recursive decomposition strategy. Other works~\citep{rag_tactic, ps_formal, naturalprover, thor, leandojo} integrate the selection of premises with tactic prediction, employing retrieval-augmented methods and constrained decoding to improve the coherence of the proof.

Lean-Star~\cite{lean-star} introduced Self-Taught Reasoning, incorporating Chain-of-Thought~\cite{wei2023chainofthoughtpromptingelicitsreasoning} reasoning before each tactic to generate synthetic data for fine-tuning LLMs via self-play~\cite{chen2024selfplayfinetuningconvertsweak}. Our work leverages these randomly sampled data points from \textit{Lean-STaR-base} as natural data.

\subsection{Mechanistic Interpretability}

Previous work has focused on localizing and editing factual associations within transformers~\citep{meng_locating_2022} and probing hidden representations for high-level knowledge~\citep{li2024inference,dong2023probing}. Such studies perform \emph{implicit evaluations} of model ability, complementing explicit benchmarks~\citep{dong2023probing}. A key technique in mechanistic interpretability is activation patching~\citep{vig2020causal,variengien2023look}, which modifies model activations to influence outputs. This research has suggested the existence of task vectors~\citep{hendel2023context, ilharco2022editing}—representations encoding abstract task information. Activation steering has been employed to mitigate model deceitfulness and sycophancy~\citep{rimsky2023steering, li2024inference}, further supporting the presence of task vectors. Steering is based on the linear representation hypothesis~\citep{park2023linear}, which posits that concepts exist as directions in the embedding space of the model. 

For theorem proving, mechanistic interpretability provides insights into how LLMs represent logical structures and reasoning processes. By dissecting these representations, we can identify failure cases, refine tactic prediction, and enhance proof generation. We hypothesize that effective steering transforms activations to align the model’s reasoning trajectory with a more structured and verifiable direction.

\subsection{Model Steering}
Activation-based interventions can directly influence the language model output during inference~\cite{Dathathri-etal, subramani-etal-2022-extracting}. Recent studies demonstrate that activation steering enhances truthfulness~\cite{}, mitigates sycophancy, and improves instruction-following~\cite{stolfo2024improvinginstructionfollowinglanguagemodels}, as well as type prediction in code~\cite{lucchetti2024understandingcodellmsmispredicttypes}.

Building on these ideas, we investigate steering vectors in the context of theorem proving. Following prior work~\cite{burns2024discoveringlatentknowledgelanguage, turner2024steeringlanguagemodelsactivation, arditi2024refusallanguagemodelsmediated, vanderweij2024extendingactivationsteeringbroad}, we compute steering vectors based on input pairs differing by a specific feature—here, the presence or absence of synthetic metadata. Unlike previous studies that focused on broad linguistic properties such as sentiment and style, our approach seeks to refine the logical inference pathways of LLMs.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{imgs/flowchart.pdf}
    \caption{Steering Vectors are computed as difference of activations of $p_{1}$ and $p_{2}$}
    \label{fig:flowchart}
\end{figure}

For theorem proving, activation steering provides a mechanism to guide the model towards structured reasoning, improving its ability to generate valid proof steps. By leveraging task vectors, we aim to shift model activations to align with correct logical deductions, thereby enhancing both proof coherence and model interpretability. This intervention is particularly valuable in interactive theorem proving, where fine-grained control over reasoning steps can lead to more reliable and verifiable proofs.

