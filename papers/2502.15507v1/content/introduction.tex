\section{Introduction}
Interactive proof assistants such as Lean \cite{de2015lean}, Isabelle \cite{wenzel2008isabelle}, and Coq \cite{coq} enable the formal verification of mathematical proofs and software by leveraging specialized programming languages \cite{Avigad,Ringer2019}. Neural theorem proving, which integrates neural language models with interactive proof assistants, has emerged as a promising approach to automating formal reasoning \cite{baldur,polu2020generative,polu2022formal,yang2023leandojo,ntptutorial}. This integration is mutually beneficial: proof assistants enforce formal correctness, while language models assist in proof construction by predicting and suggesting logical steps. A central challenge in this setting is tactic prediction—determining the appropriate next step at each proof state.

In this work, we investigate activation steering \cite{panickssery2024steeringllama2contrastive, turner2024steeringlanguagemodelsactivation, lucchetti2024understandingcodellmsmispredicttypes} as a technique to enhance tactic prediction in Llemma \cite{azerbayev2024llemmaopenlanguagemodel} and InternLM2 \cite{ying2024internlmmathopenmathlarge}. These language models  are designed for theorem proving by training and fine-tuning on mathematical data. Activation steering is an inference-time model editing method that modifies a model’s internal representations to guide its behavior toward desired outputs. We propose its application in refining tactic selection, aiming to improve both the accuracy and interpretability of proof automation. By systematically influencing LLMs' reasoning process, our approach enables structured interventions that enhance model-driven theorem proving, leading to more reliable and controllable predictions.

We present an approach for steering tactic selection from a pair of prompts $(p_{1}, p_{2})$ that contain a LEAN state $s$ to generate the next step (or tactic) $t$. However, the LLM successfully predicts $t$ for $p_{1}$ but mispredicts for $p_{2}$.
In each pair $p_{2}$ is natural data and $p_{1}$ is synthetically generated using $p_{2}$. We systematically add the attributes and a high level of structure the proof should follow. These additional attributes guide the model to a specific and more grounded chain of thought to follow while predicting the tactics. These abstractions over the proof help the LLM to do critical decision making while predicting the next step. 
