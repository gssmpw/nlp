% \section{Preliminaries}
% \vspace{-12pt}
\section{Problem Formulation}
% \vspace{-6pt}

% \subsection{Problem Definition}

We consider MO-MAB with $K$ arms and $D$ objectives.
At each round $t \in [T]$, the learner chooses an arm $a_t$ to play and observes a stochastic $D$-dimensional \emph{reward vector} $\boldsymbol{r}_{a_t,t} \in \mathcal{R} \subseteq \mathbb{R}^D$ for action $a_t$, which we refer to as \emph{reward}.
For the reward, we make the following standard assumption:

\begin{assumption}
[Bounded stochastic reward]
\label{assmp: all_1}
For $i \in [K], t \in [T], d \in [D]$, each reward entry $\boldsymbol{r}_{i,t}(d)$ is independently drawn from a \textbf{fixed} but \textbf{unknown} distribution with mean $\boldsymbol{\mu}_{i}(d)$ and variance $\sigma_{r,i,d}^2$, satisfying
$\boldsymbol{r}_{i,t} (d) \in [0,1]$, and $\sigma_{r,i,d}^2 \in [\sigma^2_{r \downarrow}, \sigma^2_{r \uparrow}] $, where $\sigma^2_{r \downarrow}, \sigma^2_{r \uparrow} \in \mathbb{R}^{+}$.
\end{assumption}

\textbf{User preferences.}
At each round $t$, we consider the user to be associated with a stochastic $D$-dimensional \emph{preference vector} $\boldsymbol{c}_t \in \mathcal{C} \subseteq \mathbb{R}^D$, indicating the user preferences across the $D$ objectives. 
We refer to this vector as \emph{preference} for short. Specifically, we make the following assumptions:

\begin{assumption}[Bounded stochastic preference]
\label{assmp: all_2}
For $t \in [T], d \in [D]$, each preference entry $\boldsymbol{c}_t(d)$ is independently drawn from a fixed distribution \textbf{(either known or unknown)} with mean $\boldsymbol{\overline{c}}(d)$ and variance $\sigma_{c,d}^2$, satisfying
$\boldsymbol{c}(d) \geq 0$, $\Vert \boldsymbol{c}_t \Vert_1 \leq \delta$, $\sigma_{c,d}^2 \in [0, \sigma^2_{c}]$.
\end{assumption}

\begin{assumption}
[Independence]
\label{assmp: all_3}
% All entries of $\boldsymbol{c}_t$ and $\boldsymbol{r}_t$ are independent.
For any $t \!\in\! [T]$, $i \!\in\! [K]$, $d_1,d_2 \!\in\! [D]$, $\boldsymbol{r}_{i,t}(d_1)$, $\boldsymbol{c}_t(d_2)$ are independent.
\end{assumption}

Assumption \ref{assmp: all_3} is common in real applications since $\boldsymbol{c}_t$ and $\boldsymbol{r}_t$ are inherently determined by independent factors: user characteristics and arm properties. For example, an individual user's preferences do not influence a restaurant's location, environment, pricing level, etc., and vice versa.

% Optimizing $D$ objectives simultaneously is challenging, as optimizing one often leads to suboptimal outcomes for others. A common approach is to use Pareto optimality \cite{drugan2013designing} for arm selection, where the arms whose expected reward not dominated by that of others are deemed Pareto optimal. 
% However, as shown in Fig.~\ref{fig:intro}, achieving Pareto optimality may not ensure optimal user outcomes. We further illustrate this with a lower bound (Proposition \ref{prop: lower_bd}) in Section \ref{sec:lower_bd}. Thus, MO-MAB optimization should be tailored to user preferences for more favourable results.


% Since the optimal solution for one objective often leads to sub-optimal for others, optimizing $D$ objectives simultaneously is tricky.
% One common solution is using Pareto optimality in the reward vector space to define optimality~\cite{drugan2013designing}, which utilizes Pareto ordering to compare arms based on their reward vectors. Arms that are not outperformed by any others in all objectives are considered as Pareto optimal.
% However, as illustrated in Figure~\ref{fig:intro}, merely achieving Pareto Optimality would result in suboptimal feedback from users. At the end of this section, we'll also provide a lower bound (Proposition \ref{prop: lower_bd}) to further illustrate this point. Therefore, the optimization of MO-MAB should be customized according to user preferences to ensure more favourable outcomes.

\textbf{Preference-aware reward.}
We define an \emph{overall-reward} as the \emph{inner product} of arm's reward and user's preference, which models the user reward rating under their preferences.
Specifically, we refer to the inner product mapping $\Phi: \mathcal{C} \times \mathcal{R} \rightarrow \mathbb{R}$ as the \emph{aggregation function}.
In each round $t$, the overall-reward $g_{a_t,t}$ for the chosen arm $a_t$ is defined as: 
$\textstyle
g_{a_t,t} = \Phi (\boldsymbol{c}_t, \boldsymbol{r}_{a_t, t}) 
= \boldsymbol{c}_t^{\top} \boldsymbol{r}_{a_t, t}$.


To evaluate the learner’s performance, we define regret as the cumulative difference between the expected overall-reward by selecting the arm with the highest expected overall-reward at each time $t$ and the expected overall-reward under the learner’s policy:
% \[
\begin{equation}
% \label{eq: regret_def}
\textstyle
R(T) 
 =
\sum^{T}_{t=1} \mathbb{E}[g_{a^{*}, t} - g_{a_t, t}]  =
\sum^{T}_{t=1} \boldsymbol{\overline{c}}^{\top} (\boldsymbol{\mu}_{a^*} - \boldsymbol{\mu}_{a_t} )
\end{equation}
% \]
where $a^{*} \!=\! \argmaxA_{i \in [K]} \boldsymbol{\overline{c}}^{\top} \boldsymbol{\mu}_{i}$ is the optimal arm.
The goal is to minimize the regret $R(T)$.
We term this problem as \emph{Preference-Aware MO-MAB} (PAMO-MAB).


% \begin{remark}
% Despite the linear model of overall reward, PAMO-MAB differs fundamentally from linear (contextual) bandits \cite{abbasi2011improved, chu2011contextual} for the following reasons:
% \begin{itemize}[leftmargin=*]
% \vspace{-9pt}
% \item
% In linear bandits, the input features are observable before making decisions, whereas in PAMO-MAB, both the random reward and preference can be unknown and must be estimated.
% \vspace{-7pt}
% \item
% In linear bandits, the feedback is a scalar reward, whereas in PAMO-MAB, the feedback can take on various forms: a $D$-dimensional reward, a $D$-dimensional reward with a $D$-dimensional preference, or a $D$-dimensional reward with an overall-reward, depending on the interaction protocols.
% \end{itemize}
% \end{remark} 



% \vspace{-8pt}
\section{A Lower Bound}
\label{sec:lower_bd}
% \vspace{-6pt}
In the following, we develop a lower bound (Proposition \ref{prop: lower_bd}) on the defined regret for PAMO-MAB.
Such a lower bound will quantify how difficult it is to control regret without preference-adaptive policies under PAMO-MAB. 
Firstly, we present a definition characterizing a class of MO-MAB algorithms that are "preference-free".
% of which the sequential decision-making is independent of the preference information.


% \begin{definition}[Preference-Free Algorithm]
% \label{def: pref_free_alg}
% Let
% $\boldsymbol{\mathrm{c}}^{t} = \{\boldsymbol{c}_1, \boldsymbol{c}_2, ..., \boldsymbol{c}_{t}\} \in \mathbb{R}^{D \times t}$ be the preference sequence up to $t$ episode with mean $\boldsymbol{\overline{c}}$. 
% Let $\pi_t^{\mathcal{A}}$ be the policy of algorithm $\mathcal{A}$ at time $t$ for selecting arm $a_t$ in a PAMO-MAB problem. 
% Then $\mathcal{A}$ is defined as a preference-free algorithm if its policy 
% $\pi_t^{\mathcal{A}}$ is independent of $\boldsymbol{\mathrm{c}}^{t}$ and $\boldsymbol{\overline{c}}$, i.e., 
% $\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t = i | \boldsymbol{\mathrm{c}}^{t}, \boldsymbol{\overline{c}}^{t}) = \mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t = i)$
% for all arms $i \in [K]$ and all episodes $t \in (0,T]$.
% \end{definition} 

\begin{definition}[Preference-Free Algorithm]
\label{def: pref_free_alg}
Let
$\boldsymbol{\mathrm{c}}^{t} = \{\boldsymbol{c}_1, \boldsymbol{c}_2, ..., \boldsymbol{c}_{t}\} \in \mathbb{R}^{D \times t}$ be the preference sequence up to $t$ episode with mean $\boldsymbol{\overline{c}}$. 
Let $\pi_t^{\mathcal{A}}$ be the policy of algorithm $\mathcal{A}$ at time $t$ for selecting arm $a_t$. 
Then $\mathcal{A}$ is defined as preference-free if its policy 
$\pi_t^{\mathcal{A}}$ is independent of $\boldsymbol{\mathrm{c}}^{t}$ and $\boldsymbol{\overline{c}}$, i.e., 
$\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t = i \mid \boldsymbol{\mathrm{c}}^{t}, \boldsymbol{\overline{c}}) = \mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t = i)$
for all arms $i \in [K]$ and all episodes $t \in [T]$.
\end{definition} 

To our knowledge, most existing algorithms in theoretical MO-MAB studies \cite{drugan2013designing, busa2017multi, xu2023pareto, huyuk2021multi, cheng2024hierarchize} fall within the class of preference-free algorithms, which employ a global policy for arm selection, while neglecting users' preferences.


\begin{proposition}
\label{prop: lower_bd}
Assume an MO-MAB environment contains multiple objective-conflicting arms, i.e., $\vert \mathcal{O}^{*} \vert \geq 2$, where $\mathcal{O}^{*}$ is the Pareto Optimal front. 
Then, for any preference-free algorithm, there exists a subset of preference such that the regret $R(T) = \Omega(T)$. 
\end{proposition} 

Proposition \ref{prop: lower_bd} shows that for PAMO-MAB problem with $|\mathcal{O}^*| \geq 2$, sub-linear regret is unachievable for preference-free algorithms.
This is because, for any arm $i \in \mathcal{O}^*$ that is optimal in one preference subset $\mathcal{C}^{+}$, there exists another preference subset $\mathcal{C}^{-}$ where arm $i$ becomes suboptimal,
while preference-free algorithms cannot adapt their policies to varying preference across the entire space $\mathcal{C}$. 
Please see Appendix \ref{sce: app_pf_lower_bd} for the  detailed proof. 
We therefore ask the following question:
\textbf{\emph{Can we design preference-adaptive algorithms that achieve sub-linear regret for PAMO-MAB?}}
The answer is {\bf yes.}
In the following, we analyze PAMO-MAB under two scenarios: preference feedback provided and hidden preference, demonstrating that with preference adaptation, sub-linear regret can indeed be achieved.





