\section{Analyses for Section \ref{sec: hidden} (Hidden Preference) }

Our main result of Theorem \ref{theorem:up_bd_hiden} in Section \ref{sec: hidden} indicates that the proposed PUCB-HPM under hidden preference environment achieves sublinear expected regret $R(T) \leq \tilde{\mathcal{O}}(D \sqrt{T})$. 
To prove this, we need two key components. 
The first is to show that the value of $\hat{\boldsymbol{r}}_{i,t}$, the matrix of $\boldsymbol{V}_t$, and the region of $\Theta_{t}$ are good estimators of $\boldsymbol{\mu}_i$, $\mathbb{E}[\boldsymbol{V}_t]$ and $\overline{\boldsymbol{c}}$ respectively.
The second is to show that as long as the aforementioned high-probability event holds, we have some control on the growth of the regret.
We show the analyses regarding these two components in the following sections.

\subsection{Proof of Lemma \ref{lemma: R_normed}}
The following lemma directly come from the definition of the sub-Gaussian variables.

\begin{lemma}[\cite{lamprier2018profile}]
\label{lemma: sub_gaussian_addity}
Let $X_1$ and $X_2$ be two sub-Gaussian variables with respective constant $R_1$ and $R_2$, let $\alpha_1$ and $\alpha_2$ be two real scalars. Then the variable $\alpha_1 X_1 + \alpha_2 X_2$ is sub-Gaussian too, with constant $\sqrt{\alpha_1^2 X_1^2 + \alpha_2^2 X_2^2}$.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma: R_normed}]
\label{sec: app_pf_lemma_R_normed}

By Assumption \ref{assmp: hpm_3} and Lemma \ref{lemma: sub_gaussian_addity}, we have that that for any $d \in [D]$ and any $t$, $\boldsymbol{\zeta}_{t}(d) = \boldsymbol{c}_t - \boldsymbol{\overline{c}}$ is also $R$ sub-Gaussian:
$\mathbb{E}[e^{x\boldsymbol{\zeta}_{t}(d)}] \leq e^{\frac{x^2 R^2}{2}}$.
For $\boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t}$, the independence implies:
\[
\mathbb{E}[e^{\lambda \boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t} }]
=
\mathbb{E}[e^{\lambda \sum_{d \in [D]} \boldsymbol{\zeta}_{t}(d)) \boldsymbol{r}_{a_{t}, t}(d) }]
= 
\prod_{d \in [D]} \mathbb{E}[e^{\lambda \boldsymbol{\zeta}_{t}(d)) \boldsymbol{r}_{a_{t}, t}(d) }]
\leq
e^{\frac{\lambda^2 R^2 \sum_{d \in [D]} \boldsymbol{r}_{a_{t}, t}(d)^2 }{2}}
=
e^{\frac{\lambda^2 R^2 \Vert \boldsymbol{r}_{a_{t}, t} \Vert_2^2 }{2}},
\]
where the inequality holds by sub-Gaussian definition and Lemma \ref{lemma: sub_gaussian_addity} that if 
$a$ is $\sigma$-subgaussian, then $ba$ is $\vert b \vert R$-subgaussian. 
The result above implies that the overall residual noise $\boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t}$ is $\Vert \boldsymbol{r}_{a_{t}, t} \Vert_2 R$-subgaussian, which is collinear with the input reward sample $\boldsymbol{r}_{a_{t}, t}$. 

By applying the the weight $w_{t}$ and using Lemma \ref{lemma: sub_gaussian_addity}, we can derive that the normed overall residual term $\sqrt{w_t} \boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t} = \frac{\sqrt{\omega}}{\Vert \boldsymbol{r}_{a_{t}, t} \Vert_2} \boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t}$ is $\sqrt{\omega} R$-subgaussian, which eliminates the heteroscedasticity of the residual error induced by selected arm.
\end{proof}


\subsection{Proof of Lemma \ref{lemma:c_estimator_conf_bd} (Confidence Ellipsoid for $\boldsymbol{\overline{c}}$)}
\label{sec:app_proof_lemma_c_estimator_conf_bd}


First we state two lemmas from~\cite{abbasi2011improved} that will be utilized in our confidence analysis of preference estimator:

% \vspace{4pt}
% \hrule \vspace{1mm}   \hrule
\begin{lemma}[Self-Normalized Bound for Vector-Valued Martingales~\cite{abbasi2011improved}, Theorem 1]
\label{lemma: self_norm_bound}
Let $\{\mathcal{F}_t\}_{t=0}^{\infty}$ be a filtration, and let $\{\zeta_t\}_{t=1}^{\infty}$ be a real-valued stochastic process such that $\zeta_t$ is $\mathcal{F}_t$-measurable, $\mathbb{E}[\zeta_t \mid \mathcal{F}_{t-1}]$ = 0 and $\zeta_t$ is conditionally $R$-sub-Gaussian for some $R \geq 0$.
Let $\{\boldsymbol{X}_t\}_{t=1}^{\infty}$ be an $\mathbb{R}^d$-valued stochastic process such that $\boldsymbol{X}_t$ is $\mathcal{F}_{t-1}$-measurable. 
Assume that $\boldsymbol{V} \in \mathbb{R}^{d \times d}$ is a positive definite matrix, and define $\boldsymbol{\overline{V}}_{t} = \boldsymbol{V} + \sum_{\ell=1}^{t} \boldsymbol{X}_{\ell} \boldsymbol{X}_{\ell}^{\top}$.
Then for any $\alpha \geq 0$, with probability at least $1 - \alpha$, for all $t \geq 1$, we have
\[
\Vert \sum_{\ell=1}^{t} \zeta_{\ell} \boldsymbol{X}_{\ell} \Vert_{\boldsymbol{\overline{V}}_{t}^{-1}}^2
\leq
2R^2 \log \left( \frac{ \det \left( \boldsymbol{\overline{V}}_{t} \right)^{\frac{1}{2}} \det \left( \boldsymbol{V} \right)^{-\frac{1}{2}} }{\alpha} \right).
\]
\end{lemma}

\begin{lemma}[Determinant-Trace Inequality~\cite{abbasi2011improved}, Lemma 10]
\label{lemma: det_trace_ineq}
Suppose $\boldsymbol{X}_{1}, ... , \boldsymbol{X}_{t} \in \mathbb{R}^d$ and $\Vert \boldsymbol{X}_{\ell} \Vert_2 \leq L, \forall \ell \in [1,t]$. 
Let $\boldsymbol{\overline{V}}_{t} = \lambda \boldsymbol{I} + \sum_{\ell=1}^{t} \boldsymbol{X}_{\ell} \boldsymbol{X}_{\ell}^{\top}$ for some $\lambda > 0$, then
\[
\det \left( \boldsymbol{\overline{V}}_t \right) \leq \left(\lambda + \frac{t L^2}{d} \right)^d.
\]
\end{lemma}

% \hrule \vspace{1mm}   \hrule
% \vspace{10pt}

\begin{proof}[Proof of Lemma \ref{lemma:c_estimator_conf_bd}]

According to the definition of estimated vector $\hat{\boldsymbol{c}}_{t}$ in Algorithm \ref{alg:PRUCB_HP}, we have 
\[
\hat{\boldsymbol{c}}_{t} 
=
\boldsymbol{V}_{t-1}^{-1} \sum_{\ell = 1}^{t-1} w_{\ell} g_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell}, \ell} 
= 
\boldsymbol{V}_{t-1}^{-1} \sum_{\ell = 1}^{t-1} w_{\ell} (\boldsymbol{\overline{c}}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} + \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell}) \boldsymbol{r}_{a_{\ell}, \ell},
\]
where the second equality followed by the definition of overall reward $g_{a_t,t} = (\boldsymbol{\overline{c}} + \boldsymbol{\zeta}_t)^{\top} \boldsymbol{r}_{a_t,t} = \boldsymbol{\overline{c}}^{\top} \boldsymbol{r}_{a_t,t} + \boldsymbol{\zeta}_t^{\top} \boldsymbol{r}_{a_t,t}$, $\boldsymbol{\zeta}_t^{\top} \in \mathbb{R}^{D}$ is an independent noise term over $\overline{\boldsymbol{c}}$ to denote the randomness of $\boldsymbol{c}_t$.

This equation further implies that the difference between estimated vector $\hat{\boldsymbol{c}}_{t}$ and the unknown vector $\overline{\boldsymbol{c}}$ can be decomposed as:
\begin{equation}
\begin{aligned}
\label{eq:hidden_conf_c}
\Vert \hat{\boldsymbol{c}}_{t} -\overline{\boldsymbol{c}} \Vert_{\boldsymbol{V}_{t-1}}
& = 
\Big \Vert 
\boldsymbol{V}_{t-1}^{-1} \sum_{\ell = 1}^{t-1} w_{\ell} (\boldsymbol{\overline{c}}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} + \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell}) \boldsymbol{r}_{a_{\ell}, \ell} - \overline{\boldsymbol{c}}
\Big \Vert_{\boldsymbol{V}_{t-1}} \\
& = 
\Big \Vert 
\boldsymbol{V}_{t-1}^{-1} \sum_{\ell = 1}^{t-1} w_{\ell} (\boldsymbol{\overline{c}}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} + \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell}) \boldsymbol{r}_{a_{\ell}, \ell} - 
\boldsymbol{V}_{t-1}^{-1} \big( \sum_{\ell = 1}^{t-1} w_{\ell} \boldsymbol{r}_{a_{\ell},\ell} \boldsymbol{r}_{a_{\ell},\ell}^{\top} + \lambda \boldsymbol{I} \big)
\overline{\boldsymbol{c}}
\Big \Vert_{\boldsymbol{V}_{t-1}} \\
& = 
\Big \Vert 
\boldsymbol{V}_{t-1}^{-1} \sum_{\ell = 1}^{t-1} w_{\ell} \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell}, \ell} - 
\lambda \boldsymbol{V}_{t-1}^{-1} \overline{\boldsymbol{c}}
\Big \Vert_{\boldsymbol{V}_{t-1}} \\
& \underset{(a)}{\leq}
\underbrace{
\Big \Vert 
\boldsymbol{V}_{t-1}^{-1} \sum_{\ell = 1}^{t-1} w_{\ell} \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell}, \ell}
\Big \Vert_{\boldsymbol{V}_{t-1}} 
}_{\text{Stochastic error: } I_1}
+
\underbrace{
\Big \Vert 
\lambda \boldsymbol{V}_{t-1}^{-1} \overline{\boldsymbol{c}}
\Big \Vert_{\boldsymbol{V}_{t-1}} 
}_{\text{Regularization error: } I_2},
\end{aligned}
\end{equation}

where (a) holds followed by the triangle inequality that $\Vert \boldsymbol{X} + \boldsymbol{Y} \Vert_{\boldsymbol{A}} \leq \Vert \boldsymbol{X} \Vert_{\boldsymbol{A}} + \Vert \boldsymbol{Y} \Vert_{\boldsymbol{A}}$.

\textbf{Bounding term $I_1$.}
For the stochastic error term $I_1$, we first construct two auxiliary terms: 
\[
\zeta_{\ell}^{\prime} = \frac{\sqrt{\omega} \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell}}{\Vert \boldsymbol{r}_{a_{\ell}, \ell} \Vert_2}, 
\text{ and }
\boldsymbol{r}_{a_{\ell}, \ell}^{\prime} = \frac{\sqrt{\omega} \boldsymbol{r}_{a_{\ell}, \ell}}{\Vert \boldsymbol{r}_{a_{\ell}, \ell} \Vert_2}.
\]

% By Assumption \ref{assmp: hpm_3}, we have that that for all $d \in [D]$, $\boldsymbol{\zeta}_{\ell}(d)$ is $R$ sub-Gaussian:
% $\mathbb{E}[e^{x\boldsymbol{\zeta}_{\ell}(d)}] \leq e^{\frac{x^2 R^2}{2}}$.
% For $\boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell}$, the independence implies:
% \[
% \mathbb{E}[e^{x \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} }]
% =
% \mathbb{E}[e^{x \sum_{d \in [D]} \boldsymbol{\zeta}_{\ell}(d)) \boldsymbol{r}_{a_{\ell}, \ell}(d) }]
% = 
% \prod_{d \in [D]} \mathbb{E}[e^{x \boldsymbol{\zeta}_{\ell}(d)) \boldsymbol{r}_{a_{\ell}, \ell}(d) }]
% \leq
% e^{\frac{x^2 R^2 \sum_{d \in [D]} \boldsymbol{r}_{a_{\ell}, \ell}^2 }{2}},
% \]
% where the last inequality holds due to the fact that if 
% $a$ is $\sigma$-subgaussian, then $ba$ is $\vert b \vert R$-subgaussian. The result above implies that the overall residual noise $\boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell}$ is $\Vert \boldsymbol{r}_{a_{\ell}, \ell} \Vert_2 R$-subgaussian, which is collinear with the input reward sample $\boldsymbol{r}_{a_{\ell}, \ell}$. By applying the the weight $w_{\ell}$, we eliminate the heteroscedasticity of the residual error induced by selected arm and show that
% \[
% \text{normed overall residual term }
% \zeta_{\ell}^{\prime} \text{ is } \sqrt{\omega} R-\text{subgaussian}.
% \]

For $\zeta_{\ell}^{\prime}$, by Lemma \ref{lemma: R_normed}, we have $\zeta_{\ell}^{\prime}$ is $\sqrt{\omega} R$-subgaussian.


For $\boldsymbol{r}_{a_{\ell}, \ell}^{\prime}$, we have 
\[\Vert \boldsymbol{r}_{a_{\ell}, \ell}^{\prime} \Vert _2 = \sqrt{\omega} \frac{\Vert \boldsymbol{r}_{a_{\ell}, \ell} \Vert_2}{\Vert \boldsymbol{r}_{a_{\ell}, \ell} \Vert_2}=\sqrt{\omega}.
\]

With the notations of $\zeta_{\ell}^{\prime}$ and $\boldsymbol{r}_{a_{\ell}, \ell}^{\prime}$, we have 

\begin{equation}
\begin{aligned}
\label{eq: hidden_conf_c_I_1_error}
I_1 
& = 
\Big \Vert 
\boldsymbol{V}_{t-1}^{-1} \sum_{\ell = 1}^{t-1} w_{\ell} \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell}, \ell}
\Big \Vert_{\boldsymbol{V}_{t-1}} \\
& = 
\Big \Vert 
\sum_{\ell = 1}^{t-1} \frac{\omega}{\Vert \boldsymbol{r}_{a_{\ell}, \ell} \Vert_2^2} \boldsymbol{\zeta}_{\ell}^{\top} \boldsymbol{r}_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell}, \ell}
\Big \Vert_{\boldsymbol{V}_{t-1}^{-1}} \\
& = 
\Big \Vert 
\sum_{\ell = 1}^{t-1} \zeta_{\ell}^{\prime} \boldsymbol{r}_{a_{\ell}, \ell}^{\prime}
\Big \Vert_{\boldsymbol{V}_{t-1}^{-1}} \\
& \underset{(a)}{\leq}
\sqrt{2 \omega R^2 \log \left(\frac{\det(\boldsymbol{V}_{t-1})^{1/2} \det(\boldsymbol{V}_{0})^{-1/2}}{\alpha} \right)} \\
& \qquad \qquad \text{(with probability at least }1-\alpha) \\
& \underset{(b)}{\leq}
R \sqrt{ \omega D \log \left(\frac{1 + \omega T/\lambda}{\alpha} \right)},
\end{aligned}
\end{equation}

where (a) holds by Lemma \ref{lemma: self_norm_bound}, and (b) holds by Lemma \ref{lemma: det_trace_ineq} with $\boldsymbol{V}_{t-1} = \lambda \boldsymbol{I} + \sum_{\ell   =1}^{t-1} \boldsymbol{r}_{a_{\ell}, \ell}^{\prime} {\boldsymbol{r}_{a_{\ell}, \ell}^{\prime}}^{\top}$ and $\Vert \boldsymbol{r}_{a_{\ell}, \ell}^{\prime} \Vert _2 = \sqrt{\omega}$.

\textbf{Bounding term $I_2$.}
Note $\overline{\boldsymbol{c}}^{\top} \boldsymbol{r}_t \in [0,1]$ and $\boldsymbol{r}_t \in [0,1]^D$, we have $\sum_{d \in [D]} \overline{\boldsymbol{c}}(d) \leq 1$ with $\overline{\boldsymbol{c}}(d) \geq 0, \forall d \in [D]$, which implies
\[
\Vert \overline{\boldsymbol{c}} \Vert_2
\leq 
\Vert \overline{\boldsymbol{c}} \Vert_1
\leq 
1.
\]

Thus for the regularization error term $I_2$, we have
\[
I_2 = \left \Vert 
\lambda \boldsymbol{V}_{t-1}^{-1} \overline{\boldsymbol{c}}
\right \Vert_{\boldsymbol{V}_{t-1}} = 
\lambda \left \Vert 
\overline{\boldsymbol{c}} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}} 
\underset{(a)}{\leq}
\lambda \left \Vert 
\overline{\boldsymbol{c}} \right \Vert_{\frac{1}{\lambda}\boldsymbol{I}}
=
\sqrt{\lambda} \left \Vert 
\overline{\boldsymbol{c}} \right \Vert_{2}
\leq 
\sqrt{\lambda},
\]
where (a) holds since $\boldsymbol{V}_{t-1}^{-1} \preceq \boldsymbol{V}_{0}^{-1} = \frac{\boldsymbol{I}}{\lambda}$.

Combining term $I_1$ and term $I_2$ with Eq. \ref{eq:hidden_conf_c} completes the proof of Lemma \ref{lemma:c_estimator_conf_bd}.
\end{proof}

% [Upper Confidence Bound of Expected Overall Reward]
\subsection{Proof of Lemma \ref{lemma:g_estimator_upper_conf_bd} (Upper Confidence Bound for Expected Overall Reward)}
\label{sec:app_pf_lemma_g_estimator_upper_conf_bd}

\begin{proof}

For any $i \in [K], d \in [D], t>0$, by Hoeffding’s Inequality (Lemma~\ref{lemma: Hoeffding}), we have:

\begin{equation}
\begin{aligned}
\mathbb{P} \left( | \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d) | > \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right)
& \leq
2\exp \left( \frac{-2 N_{i,t}^2 \log(t/\alpha) }{ N_{i,t} \sum_{\ell=1}^{N_{i,t}}(1-0)^2 } \right) \\
& =
2\exp \left( -2 \log(t/\alpha) \right) \\
& =
2 \left( \frac{\alpha}{t} \right)^2,
\end{aligned}
\end{equation}

Thus for any $i \in [K] $ and $t > 0$, with at least probability $1 - 2D \left( \frac{\alpha}{t} \right)^2$, we have 

\[
\boldsymbol{\hat{r}}_{i,t} - \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \preceq \boldsymbol{\mu}_{i} \preceq \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e}.
\]

And thus we can derive 

\begin{equation}
\begin{aligned}  
\label{eq: c_hidden_confidence_upper_bd2}
\boldsymbol{\hat{c}}_t^{\top} \boldsymbol{\hat{r}}_{i,t} + B_{i,t}^{r} + B_{i,t}^{c} -\boldsymbol{\overline{c}}^{\top} \boldsymbol{\mu}_{i} 
& \geq
\boldsymbol{\hat{c}}_t^{\top} \boldsymbol{\hat{r}}_{i,t} + B_{i,t}^{r} + B_{i,t}^{c} - \boldsymbol{\overline{c}}^{\top} \left( \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right)  \\
& =
\boldsymbol{\hat{c}}_t^{\top} \left( \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right) + B_{i,t}^{c} - \boldsymbol{\overline{c}}^{\top} \left( \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right)  \\
& = 
(\boldsymbol{\hat{c}}_t - \boldsymbol{\overline{c}})^{\top}
\left( \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right) + B_{i,t}^{c} \\
& \underset{(a)}{\geq}
- \Vert \boldsymbol{\hat{c}}_t - \boldsymbol{\overline{c}}_t \Vert_{\boldsymbol{V}_{t-1}} \cdot \left\Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right\Vert_{\boldsymbol{V}_{t-1}^{-1}} 
+
B_{i,t}^{c} \\
& \underset{(b)}{\geq}
- \beta_t \cdot \left\Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right\Vert_{\boldsymbol{V}_{t-1}^{-1}} 
+
B_{i,t}^{c} \\
& \qquad \qquad \qquad (\text{with probability at least } 1-\vartheta) \\
& = 0,
\end{aligned}
\end{equation}
where (a) holds by Cauchy-Schwarz inequality, (b) holds by Lemma \ref{lemma:c_estimator_conf_bd}. 
\end{proof}


\subsection{Uniform Confidence Bound for Estimators}
\label{sec: proof_proposition_uniform_confidence_bound}

% In this section, we show that the confidence sets designed for both preferences estimations and reward estimations in PUCB-HPM can be 



\begin{proposition}
\label{proposition: uniform_confidence_bound}
Let $\alpha = \sqrt{12 \vartheta/(KD(D+3) \pi^2)}$, for all $t\in [1,T]$, with probability at least $1 - \vartheta$, we have following events hold simultaneously:
\[
\begin{aligned}
& \text{Event A:} \Bigg\{ |\boldsymbol{\mu}_{i}(d) - \boldsymbol{\hat{r}}_{i,t}(d)| \leq \sqrt{\frac{\log \left( \frac{t}{\alpha} \right)}{N_{i,t}}}, \forall i \in [K], \forall d \in [D] \Bigg\}, \\
& \text{Event B:} \Bigg\{ \mathbb{E} \left[ \sum_{\ell \in \mathcal{T}_{i,t-1}} w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right](m,n)
-
\sum_{\ell \in \mathcal{T}_{i,t-1}} \left( w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right)(m,n)
\leq 
\omega \sqrt{ N_{i,t} \log \left( \frac{t}{\alpha} \right)}, \\ 
& \qquad \qquad \quad \forall i \in [K], \forall m \in [D], \forall n \in [m,D] \Bigg\}, \\
\end{aligned}
\]
where $\mathcal{T}_{i,t}$ is the set of episodes that arm $i$ is pulled within $t$ steps.
% $V_{\sigma_{r}} = 2 \cdot \mathds{1}_{\{\sigma_r^2>0\}} + \sigma_r^2 \mathds{1}_{\{\sigma_r^2=0\}}$. 
% where $\{ \boldsymbol{r}_{i,\ell} \}_{\ell}^{N_{i,t}}$ denotes the sequence of  historical reward obtained by pulling arm $i$ within $t$ steps.
\end{proposition}

\begin{proof}

\textbf{Step-1 (Confidence analysis of Event A):} 

For any $i \in [K], d \in [D], t \in (0,T]$, by Hoeffding’s Inequality (Lemma~\ref{lemma: Hoeffding}), we have the instantaneous failure probability of Event B can be bounded as:

\begin{equation}
\begin{aligned}
\mathbb{P} \left( | \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d) | > \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right)
& \leq
2\exp \left( \frac{-2 N_{i,t}^2 \log(t/\alpha) }{ N_{i,t} \sum_{\ell=1}^{N_{i,t}}(1-0)^2 } \right) \\
& =
2\exp \left( -2 \log(t/\alpha) \right) \\
& =
2 \left( \frac{\alpha}{t} \right)^2,
\end{aligned}
\end{equation}

which yields the upper bound of $\mathbb{P} (B^\mathsf{c})$ by union bound as 

\begin{equation}
\begin{aligned}
\label{eq: conf_B_upbd}
\mathbb{P} (B^\mathsf{c}) 
& = 
\mathbb{P} \left( \exists \{i,d,t\}, | \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d) | > \sqrt{\frac{2 \log(t/\alpha)}{N_{i,t}}} \right) \\
& \leq
2 \sum_{t=1}^{T}
\sum_{i=1}^{K}
\sum_{d=1}^{D}
\mathbb{P} \left( | \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d) | > \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right) \\
& \leq 
2 \sum_{t=1}^{T} \sum_{i=1}^{K} \sum_{d=1}^{D} \left( \frac{\alpha}{t} \right)^2
\underset{(Eq. \ref{eq: riemann_zeta})}{\leq }
\frac{KD \alpha^2 \pi^2}{3},
\end{aligned}
\end{equation}


\textbf{Step-2 (Confidence analysis of Event B):} 

The proof follows similar lines as above.
Note that for any $i \in [K], t \in (1,T], m \in [1, D], n \in [m, D]$, we have the instantaneous failure probability of Event C can be bounded as

\begin{small}
\[
\begin{aligned}
& \mathbb{P} \left( \mathbb{E} \left[ \sum_{\ell \in \mathcal{T}_{i,t-1}} w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right](m,n)
-
\sum_{\ell \in \mathcal{T}_{i,t-1}} \left( w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right)(m,n)
>
\omega \sqrt{ N_{i,t} \log \left( \frac{t}{\alpha} \right)} \right) \\
& \quad = 
\mathbb{P} \left( \mathbb{E} \left[ \frac{\omega}{\Vert \boldsymbol{r}_{i} \Vert_2^2} \boldsymbol{r}_{i} \boldsymbol{r}_{i}^{\top} \right](m,n)
-
\frac{1}{N_{i,t}} \sum_{\ell \in \mathcal{T}_{i,t-1}} \left( 
\frac{\omega}{\Vert \boldsymbol{r}_{i,\ell} \Vert_2^2} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right)(m,n)
>
\omega \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right) \\
& \quad \leq 
\exp \left( - \frac{ 2 \omega^2 N_{i,t}^2 \log(t/\alpha) }{ N_{i,t}^2 (\omega - 0)^2 } \right) 
=
(\frac{\alpha}{t})^2.
\qquad \text{\big(by Lemma \ref{lemma: Hoeffding} and $( \frac{\omega}{\Vert \boldsymbol{r}_{i,\ell} \Vert_2^2} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top})(m,n) \in [0,\omega]$ \big)}
\end{aligned}
\]
\end{small}

Using union bound, we have $\mathbb{P} (C^{\mathsf{c}})$ as
\begin{small}
\begin{equation}
\begin{aligned}
\label{eq: conf_C_upbd}
\mathbb{P} (C^\mathsf{c}) 
& = 
\mathbb{P} \left( \exists \{i,t,m,n\}, \mathbb{E} \left[ \sum_{\ell \in \mathcal{T}_{i,t-1}} w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right](m,n)
-
\sum_{\ell \in \mathcal{T}_{i,t-1}} \left( w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right)(m,n)
>
\omega \sqrt{ N_{i,t} \log \left( \frac{t}{\alpha} \right)} \right) \\
& \leq
\sum_{t=1}^{T}
\sum_{i=1}^{K}
\sum_{m=1}^{D}
\sum_{n=m}^{D}
\mathbb{P} \left( \mathbb{E} \left[ \sum_{\ell \in \mathcal{T}_{i,t-1}} w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right](m,n)
-
\sum_{\ell \in \mathcal{T}_{i,t-1}} \left( w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right)(m,n)
>
\omega \sqrt{ N_{i,t} \log \left( \frac{t}{\alpha} \right)} \right) \\
& \leq 
\sum_{t=1}^{T}
\sum_{i=1}^{K}
\sum_{m=1}^{D}
\sum_{n=m}^{D} \left( \frac{\alpha}{t} \right)^2
\underset{(Eq.~\ref{eq: riemann_zeta})}{\leq }
\frac{KD(D-1) \alpha^2 \pi^2}{12}.
\end{aligned}
\end{equation}
\end{small}

\textbf{Step-3 (Union confidence on three Events):} 

Combining Eq.~\ref{eq: conf_B_upbd} and Eq.~\ref{eq: conf_C_upbd}, and setting $\alpha = \sqrt{\frac{12 \vartheta}{KD(D+3) \pi^2}}$, by union bound, we can have the overall failure probability bound of three Events as

\[
\begin{aligned}
\mathbb{P} (A^\mathsf{c} \cup B^\mathsf{c}) 
\leq 
\mathbb{P} (A^\mathsf{c}) + \mathbb{P} (B^\mathsf{c})
= 
\left( \frac{KD(D-1) \pi^2}{12} + \frac{4KD \pi^2}{12} \right) \left(\frac{12 \vartheta}{KD(D+3) \pi^2} \right)
= 
\vartheta.
\end{aligned}
\]

This concludes the proof of Proposition~\ref{proposition: uniform_confidence_bound}.
\end{proof}


\subsection{Proof of Theorem \ref{theorem:up_bd_hiden} (Regret Analysis of Algorithm \ref{alg:PRUCB_HP})}
\label{sec: app_pf_thm_up_bd_hiden}

\begin{proof}

Based on the assumptions in Proposition \ref{proposition: uniform_confidence_bound}, we next show that when Events of A, B, C in Proposition \ref{proposition: uniform_confidence_bound} hold (detailed definitions of Events of A, B, C refer to Appendix \ref{sec: proof_proposition_uniform_confidence_bound}), the sub-linear regret of PUCB-HPM can be achieved. Please see the detailed proof steps below.

% Using the above two propositions, we can easily get the regret bound. 
% Specifically, by choosing 
% \[
% \beta_t = \left( \sqrt{\lambda} + \sqrt{\frac{1}{2} D \log\left( 1 + \frac{t-1}{\lambda} \right) + 4 \log \left( \frac{\pi t}{(30 \vartheta)^{\frac{1}{4}}} \right) } \right)^2 ,
% \]
% and 
% \[
% \alpha = \left( \frac{120 \vartheta}{KD(D+3) \pi^4} \right)^{\frac{1}{4}}, 
% \] 
% with probability at least $1 - \vartheta$, we have the expected regret of PUCB-HPM satisfies

% \begin{equation}
% \begin{aligned}
% \label{eq: PUCB_HPM_regret}
% R(T) &\leq R(M+1:T) + R(M) \\
% & \underset{(a)}{\leq}
% 2 \sqrt{\frac{\beta_T D}{2 \log(\frac{5}{4})} \log \left(1 + \frac{(1 + \sigma^2_{\boldsymbol{r}}) (T-M)}{\lambda} \right) (T - M) } + 
% 4 \sqrt{2 K \log \left( \frac{T}{\alpha} \right) (T-M) } + M ,
% \end{aligned}
% \end{equation}

% where (a) trivially holds since $R(M) \leq M$ due to the 1-bounded instantaneous regret.
% \end{proof}


% \subsection{Proof of Proposition \ref{proposition: regret_hidden_control}}
% \label{sec: proof_proposition_regret_hidden_control}



% \begin{lemma}
% \label{lemma: upbd_term2_hidden}
% Let $M \in (0,T)$, and the assumptions follow those outlined in Proposition \ref{proposition: uniform_confidence_bound}, then we have:
% \[
% \sum_{t=M+1}^{T} \sqrt{\frac{2 \log \left( \frac{t}{\alpha} \right)}{N_{a_t,t}}} 
% \leq 
% 2 \sqrt{2 K \log \left( \frac{T}{\alpha} \right) (T-M) }.
% \]
% \end{lemma}


\textbf{Step-1 (Regret Analysis and Decomposition)}

Let $M$ be an arbitrary positive integer, we can express $R(T)$ in a truncated form with respect to $M$ as follows:
\begin{equation}
\begin{aligned}
\label{eq: trunc_regret_0}
R(T) = \sum_{t=1}^{T} \text{regret}_{t} \leq M + \sum_{t=M+1}^{T} \text{regret}_{t},
\end{aligned}
\end{equation}

where $\text{regret}_{t}$ denotes the instantaneous regret of PRUCB-HPM at step $t \in \left[ T \right]$, and the last inequality holds since the fact that the instantaneous regret is upper-bounded by 1 (by Assumption \ref{assmp: hpm_2}).

Next, we analyze the instantaneous regret over the truncated time horizon $[M+1, T]$.
Note that since event B holds, we have 

\begin{equation}
\begin{aligned}
\label{eq: a^*_confidence_hidden}
\boldsymbol{\mu}_{a^*} \preceq \boldsymbol{\hat{r}}_{a^*,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e}, 
\text{ and }
\boldsymbol{\mu}_{a_t} \succeq  \boldsymbol{\hat{r}}_{a_t, t} - \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e}.
\end{aligned}
\end{equation}

By the definition of regret and fact above, we can derive the upper-bound of expected instantaneous regret as follows:

\[
\begin{aligned}
\text{regret}_{t} 
& = \boldsymbol{\overline{c}}^{\top}  \boldsymbol{\mu}_{a^*} - \boldsymbol{\overline{c}}^{\top}  \boldsymbol{\mu}_{a_t} \\
& \underset{(a)}{\leq}
\boldsymbol{\hat{c}}^{\top} \boldsymbol{\hat{r}}_{a^*,t} + B_{a^*,t}^{r} + B_{a^*,t}^{c} - \boldsymbol{\overline{c}}^{\top}  \boldsymbol{\mu}_{a_t} \\
& \underset{(b)}{\leq}
\boldsymbol{\hat{c}}^{\top} \boldsymbol{\hat{r}}_{a^*,t} + B_{a^*,t}^{r} + B_{a^*,t}^{c} - \boldsymbol{\overline{c}}^{\top} \left( \boldsymbol{\hat{r}}_{a_t,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right) + 2\Vert\boldsymbol{\overline{c}}\Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \\
& \underset{(c)}{\leq}
\boldsymbol{\hat{c}}^{\top} \boldsymbol{\hat{r}}_{a_t,t} + B_{a_t,t}^{r} + B_{a_t,t}^{c} - \boldsymbol{\overline{c}}^{\top} \left( \boldsymbol{\hat{r}}_{a_t,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right) + 2\Vert\boldsymbol{\overline{c}}\Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}  \\
& = 
(\boldsymbol{\hat{c}} - \boldsymbol{\overline{c}})^{\top} \left( \boldsymbol{\hat{r}}_{a_t,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right) + B_{a_t,t}^{c} + 2\Vert\boldsymbol{\overline{c}}\Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \\
& \underset{(d)}{\leq}
\Vert \boldsymbol{\hat{c}}_t - \boldsymbol{\overline{c}}_t \Vert_{\boldsymbol{V}_{t-1}} \cdot \left \Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}} + B_{a_t,t}^{c} + 2\Vert\boldsymbol{\overline{c}}\Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \\
& \underset{(e)}{\leq}
\underbrace{
\min \left( 2 B_{a_t,t}^{c}, 1 \right)
}_{\text{regret}_{t}^{\tilde{\boldsymbol{c}}}} 
+ 
\underbrace{
2 \Vert\boldsymbol{\overline{c}}\Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
}_{\text{regret}_{t}^{\tilde{\boldsymbol{r}}}},
\end{aligned}
\]

where (a) followed by Lemma \ref{lemma:g_estimator_upper_conf_bd}, (b) followed by Eq.~\ref{eq: a^*_confidence_hidden}, (c) holds by the definition of optimization policy for arm selection, (d) holds by Cauchy-Schwarz inequality, (e) followed by Lemma \ref{lemma:c_estimator_conf_bd} with the definition of $B_{a_t,t}^{c}$, and the fact that the instantaneous regret is at most 1.
Interestingly, the derived instantaneous regret above can also be interpreted as the sum of two components: 
\begin{itemize}
  \item $\text{regret}_{t}^{\tilde{\boldsymbol{c}}}$: Regret caused by the imprecise estimation of preference $\boldsymbol{\overline{c}}$.
  \item $\text{regret}_{t}^{\tilde{\boldsymbol{r}}}$: Regret caused by the imprecise estimation of expected reward of arms.
\end{itemize}

Plugging above results back to Eq. \ref{eq: trunc_regret_0}, we have 

\begin{equation}
\begin{aligned}
\label{eq: trunc_regret}
R(T) 
& \leq
M
+
\sum_{t=M+1}^{T} \text{regret}_t \\
& \leq
M 
+
\sum_{t=M+1}^{T} \Big( \text{regret}_{t}^{\tilde{\boldsymbol{c}}} 
+
\text{regret}_{t}^{\tilde{\boldsymbol{r}}}
\Big)
\\ 
& \leq
M
+
\underbrace{
\sum_{t=M+1}^{T} 
\min \left( 2 B_{a_t,t}^{c}, 1 \right)
}_{R_{M+1:T}^{\tilde{\boldsymbol{c}}}}+
\underbrace{
\sum_{t=M+1}^{T} 2 \Vert\boldsymbol{\overline{c}}\Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
}_{R_{M+1:T}^{\tilde{\boldsymbol{r}}}},
\end{aligned}
\end{equation}

which also yields two components of $R_{M+1:T}^{\tilde{\boldsymbol{c}}}$ and $R_{M+1:T}^{\tilde{\boldsymbol{r}}}$, denoting the accumulated truncated expected errors caused by the imprecise estimations of preference and reward respectively. 
Next we analyze two components of $R_{M+1:T}^{\tilde{\boldsymbol{c}}}$ and $R_{M+1:T}^{\tilde{\boldsymbol{r}}}$ separately.


\textbf{Step-2 (Upper-Bound over $R_{M+1:T}^{\tilde{\boldsymbol{c}}}$)}

Before analyzing $R_{M+1:T}^{\tilde{\boldsymbol{c}}}$, we first show three lemmas that will be utilized for proof:

\begin{lemma}
\label{lemma: hidden_sum_reg_c_expectation_bd}
Let $M = \left\lfloor \min \big \{ t^{\prime} \mid t  \sigma^2_{r \downarrow} + \lambda \geq 2D \omega \sqrt{Kt\log \frac{t}{\alpha} }, \forall t \geq t^{\prime} \big \} \right \rfloor$, follow the assumptions outlined in Proposition \ref{proposition: uniform_confidence_bound}, for any $t \geq M+1$, and any $\boldsymbol{\mu} \in \mathbb{R}^{D}$, we have 
\[
\boldsymbol{\mu}^{\top} \boldsymbol{V}_{t-1}^{-1} \boldsymbol{\mu}
\leq
2 \boldsymbol{\mu}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-1} \boldsymbol{\mu}.
\]
\end{lemma}
Please see Appendix \ref{sec: proof_lemma_hidden_sum_reg_c_expectation_bd} for the proof of Lemma \ref{lemma: hidden_sum_reg_c_expectation_bd}.



\begin{lemma}
\label{lemma: hidden_sum_reg_c_1}
Follow the assumptions outlined in Proposition \ref{proposition: uniform_confidence_bound}, then for any $M \geq 0$, we have
\[
\sum_{t=M+1}^{T}
\min \left(
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}}, 
\frac{1}{2\sqrt{2}}
\right)
\leq
\sqrt{\frac{D}{2 \log(9/8)} (T-M) \log \Big( 1 + \frac{\omega}{\lambda}(T-M) \Big) }.
\]
\end{lemma}
Please see Appendix \ref{sec: proof_lemma_upbd_term1_hidden} for the proof of Lemma \ref{lemma: hidden_sum_reg_c_1}.

\begin{lemma}
\label{lemma: hidden_sum_reg_c_2}
Let assumptions follow those outlined in Proposition \ref{proposition: uniform_confidence_bound}, then for any $M \geq 0$, we have
\[
\sum_{t=M+1}^{T} \sqrt{\frac{ \log \left( \frac{t}{\alpha} \right)}{N_{a_t,t}}}
\leq
2 \sqrt{ K(T-M) \log \left( \frac{T}{\alpha} \right)}.
\]
\end{lemma}
Please see Appendix \ref{sec: proof_lemma_hidden_sum_reg_c_2} for the proof of Lemma \ref{lemma: hidden_sum_reg_c_2}.


Since we assume the Event B always holds, for any $t>1$ and $i \in [K]$, we have  
\[
\left \Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} - \boldsymbol{\mu}_{i} \right \Vert_2
\leq
\left \Vert 2 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \boldsymbol{e} \right \Vert_2
=
2 \sqrt{D \frac{ \log(t/\alpha)}{N_{i,t}}} 
=
\rho_{i,t}.
\]

Consequently,
\[
\begin{aligned}
\left \Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\log(t/\alpha)/N_{i,t}} \boldsymbol{e} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}}
-
\Vert \boldsymbol{\mu}_{i} \Vert_{\boldsymbol{V}_{t-1}^{-1}}
& \leq
\left \Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\log(t/\alpha)/N_{i,t}} \boldsymbol{e} 
-
\boldsymbol{\mu}_{i} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}} \\
& \leq
\frac{1}{\sqrt{\lambda}} 
\left \Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\log(t/\alpha)/N_{i,t}} \boldsymbol{e} 
-
\boldsymbol{\mu}_{i} \right \Vert_{2} \\
& \leq
\frac{1}{\sqrt{\lambda}} \rho_{i,t}, \\
\end{aligned}
\]
\begin{equation}
\label{eq:expect_r_gap_hidden}
\implies
\left \Vert \boldsymbol{\hat{r}}_{i,t} + \sqrt{\log(t/\alpha)/N_{i,t}} \boldsymbol{e} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}}
\leq 
\Vert \boldsymbol{\mu}_{i} \Vert_{\boldsymbol{V}_{t-1}^{-1}}
+
\frac{1}{\sqrt{\lambda}} \rho_{i,t}.
\end{equation}


Define $M = \left\lfloor \min \big \{ t^{\prime} \mid t  \sigma^2_{r \downarrow} + \lambda \geq 2D \omega \sqrt{Kt\log \frac{t}{\alpha} }, \forall t \geq t^{\prime} \big \} \right \rfloor$. 
Please note that for $\sigma^2_{r \downarrow} >0$, we have $\lim_{t \rightarrow \infty}\frac{2D \omega \sqrt{Kt \log \frac{t}{\alpha} }}{\sigma^2_{r \downarrow}t} = \lim_{t \rightarrow \infty} C_1 \sqrt{ \frac{ \log (t) - C_2 }{t}} = 0$ since as $t$ increase, $\sqrt{\log t}$ grows much slowly compared to $\sqrt{t}$. Hence for sufficiently large $t^{\prime}$, the inequality $t \sigma^2_{r \downarrow} + \lambda \geq 2D\omega  \sqrt{Kt\log \frac{t}{\alpha} }, \forall t \geq t^{\prime}$ holds, which implies that such an $M$ does indeed exist.
By Lemma~\ref{lemma: hidden_sum_reg_c_expectation_bd}, for any $t \in \left[ M+1, T \right]$, we have 

\begin{equation}
\begin{aligned}
\label{eq: hidden_regret_c}
\text{regret}_{t}^{\tilde{\boldsymbol{c}}}
& =
\min \left(
2 B_{a_t,t}^{c}, 1 \right)  \\
& =
\min \left( 
2 
\beta_t 
\left \Vert \boldsymbol{\hat{r}}_{a_t,t} + \sqrt{ \log(t/\alpha)/N_{a_t,t}} \boldsymbol{e} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}},
1 \right) \\
& \underset{(a)}{\leq}
\min \left( 
2 \beta_t 
\left(
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\boldsymbol{V}_{t-1}^{-1}}
+
\frac{1}{\sqrt{\lambda}} \rho_{a_t,t}
\right), 
1 \right)\\
& \underset{(b)}{\leq}
\min \left( 
2 \beta_t 
\left(
\sqrt{2} \Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}}
+
\frac{1}{\sqrt{\lambda}} \rho_{a_t,t}
\right), 
1 \right)\\
& \underset{(c)}{\leq}
\underbrace{
2\sqrt{2} \beta_T 
\min \left( 
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}},
\frac{1}{2\sqrt{2}}  \right)
}_{\text{reg}_{t}^{(1)}}
+
\underbrace{
2\beta_T \frac{1}{\sqrt{\lambda}} 
\rho_{a_t,t}
}_{\text{reg}_{t}^{(2)}},
\end{aligned}
\end{equation}

where (a) follows by Eq. \ref{eq:expect_r_gap_hidden}, (b) follows by Lemma \ref{lemma: hidden_sum_reg_c_expectation_bd}, (c) holds due to $\beta_t \geq 1$ and increasing with $t$. By applying Lemma \ref{lemma: hidden_sum_reg_c_1} on $\text{reg}_{t}^{(1)}$ and Lemma \ref{lemma: hidden_sum_reg_c_2} on $\text{reg}_{t}^{(2)}$, we have  

\begin{equation}
\begin{aligned}
\label{eq: hidden_regret_c}
R_{M+1:T}^{\tilde{\boldsymbol{c}}}
& \leq 
\sum_{t=M+1}^{T} \text{reg}_{t}^{(1)} +
\sum_{t=M+1}^{T} \text{reg}_{t}^{(2)} \\
& \leq
2\sqrt{2} \beta_T 
\sum_{t=M+1}^{T}
\min \left( 
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}},
\frac{1}{2\sqrt{2}}  \right)
+
\frac{2\beta_T}{\sqrt{\lambda}}
\sum_{t=M+1}^{T} \rho_{a_t,t} \\
& \leq
2\beta_T \sqrt{\frac{D}{\log(9/8)} (T-M) \log\Big( 1 + \frac{\omega}{\lambda} (T-M) \Big) } \\
& \qquad \qquad +
\frac{8\beta_T}{\sqrt{\lambda}}
\sqrt{ DK(T-M) \log \left( \frac{T}{\alpha} \right)}.
\end{aligned}
\end{equation}


\textbf{Step-3 (Upper-Bound over $R_{M+1:T}^{\tilde{\boldsymbol{r}}}$)}

For the truncated regret component $R_{M+1:T}^{\tilde{\boldsymbol{r}}}$ caused by imprecise estimation of reward, we have

\begin{equation}
\label{eq: hidden_regret_r}
\begin{aligned}
R_{M+1:T}^{\tilde{\boldsymbol{r}}}
= 
2 \sum_{t=M+1}^{T} \Vert\boldsymbol{\overline{c}}\Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
\underset{(a)}{\leq}
2 \sum_{t=M+1}^{T} \sqrt{\frac{ \log \left( \frac{t}{\alpha} \right)}{N_{a_t,t}}} 
\underset{(b)}{\leq}
4 \sqrt{ K (T-M) \log \left( \frac{T}{\alpha} \right)},
\end{aligned}
\end{equation}

where (a) holds by the fact that $\Vert \boldsymbol{\overline{c}}\Vert_1 \leq 1$, (b) follows by Lemma \ref{lemma: hidden_sum_reg_c_2}.

\textbf{Step-4 (Deriving final regret)}

Finally, we can derive the final regret $R(T)$.
Specifically, combining Eq. \ref{eq: hidden_regret_c} and Eq. \ref{eq: hidden_regret_r} gives
\[
\begin{aligned}
R(T) 
& \leq 
M +
R_{M+1:T}^{\tilde{\boldsymbol{c}}} +
R_{M+1:T}^{\tilde{\boldsymbol{r}}} \\
& \leq
2\beta_T \sqrt{\frac{D}{\log(9/8)} (T-M) \log \Big( 1 + \frac{\omega}{\lambda} (T-M) \Big) } \\
& \qquad +
\frac{8\beta_T}{\sqrt{\lambda}}
\sqrt{ DK(T-M) \log \left( \frac{T}{\alpha} \right)}
+
4 \sqrt{ K (T-M) \log \left( \frac{T}{\alpha} \right)}
+ M \\
& =
O\Bigg(
DR \sqrt{ \omega T \log^{2} \Big( \frac{1 + \omega T/\lambda}{\vartheta} \Big)}
+
\frac{DR}{\sqrt{\lambda}} \sqrt{ \omega DK T \log^2 \left(\frac{1 + \omega T/\lambda}{\vartheta} \right)} \\
& \qquad + 
\sqrt{ KT \log \left( \frac{T}{\vartheta} \right)}
\Bigg).
\end{aligned}
\]

This concludes the proof of Theorem \ref{theorem:up_bd_hiden}.
\end{proof}




\subsection{Proof of Lemma \ref{lemma: hidden_sum_reg_c_expectation_bd}}
\label{sec: proof_lemma_hidden_sum_reg_c_expectation_bd}

Before the proof, we state two lemmas that will be utilized in the derivation as follows.

\begin{lemma}[Eigenvalues of Sums of Hermitian Matrices~\cite{fulton2000eigenvalues}, Eq.(11)]
\label{lemma: eigen_rank}
Let $\boldsymbol{A}$ and $\boldsymbol{B}$ are $n \times n$ Hermitian matrices with eigenvalues $a_1 > a_2 > ... > a_n$ and $b_1 > b_2 > ... > b_n$. 
Let $\boldsymbol{C} = \boldsymbol{A} + \boldsymbol{B}$ and the eigenvalues of $\boldsymbol{C}$ are $c_1 > c_2 > ... > c_n$, then we have 
\[
c_{n-i-j} \geq a_{n-i} + b_{n-j}, \forall i,j \in [0, n-1].
\]
\end{lemma}

\begin{lemma}[Eigenvalue Bounds on Quadratic Forms]
\label{lemma: eigen_bound}
Assuming $A \in \mathbb{R}^{n \times n}$ is symmetric, then for any $\boldsymbol{x} \in \mathbb{R}^n$, the quadratic form is bounded by the product of the minimum and maximum eigenvalues of $A$ and the square of the norm of $\boldsymbol{x}$:
\[
\max \left( {\boldsymbol{\lambda_{A}}} \right) \Vert \boldsymbol{x} \Vert_2^2
\geq
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} 
\geq
\min \left( {\boldsymbol{\lambda_{A}}} \right) \Vert \boldsymbol{x} \Vert_2^2,
\]
where $\boldsymbol{\lambda_{A}}$ is the eigenvalues of $\boldsymbol{A}$.
\end{lemma}

\begin{proof}
The quadratic form $\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x}$ can be analyzed by decomposing $\boldsymbol{A}$ using its eigenvalues and eigenvectors. Since $\boldsymbol{A}$ is a symmetric matrix, we can write it as:
\[
\begin{aligned}
\boldsymbol{A} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top},
\end{aligned}
\]
where $\boldsymbol{Q}$ is an orthogonal matrix whose columns are the eigenvectors of $\boldsymbol{A}$, and $\boldsymbol{\Lambda}$ is a diagonal matrix with the eigenvalues $\boldsymbol{\lambda_{A}}(i)$ on its diagonal.
By substituting the eigen-decomposition of $\boldsymbol{A}$, we have
\[
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} = \boldsymbol{x}^{\top} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{x}.
\]

Let $\boldsymbol{y} = \boldsymbol{Q}^{\top} \boldsymbol{x}$, then we have
\[
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} = \boldsymbol{y}^{\top} \boldsymbol{\Lambda} \boldsymbol{y}
=
\sum_{i=1}^{n} \boldsymbol{\lambda_{A}}(i) \boldsymbol{y}(i)^2
\geq 
\min \left( {\boldsymbol{\lambda_{A}}} \right) \sum_{i=1}^{n} \boldsymbol{y}(i)^2
=
\min \left( {\boldsymbol{\lambda_{A}}} \right) \Vert \boldsymbol{y} \Vert_2^2
\underset{(a)}{=}
\min \left( {\boldsymbol{\lambda_{A}}} \right) \Vert \boldsymbol{x} \Vert_2^2.
\]

where (a) follows since $\Vert \boldsymbol{y} \Vert_2^2 = \Vert \boldsymbol{Q}^{\top} \boldsymbol{x} \Vert_2^2 = \Vert \boldsymbol{x} \Vert_2^2$ as 
$\boldsymbol{Q}$ is orthogonal and preserves the norm.
For $\max \left( {\boldsymbol{\lambda_{A}}} \right) \Vert \boldsymbol{x} \Vert_2^2,
\geq
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x}$, the proof follows similarly and is therefore omitted.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma: hidden_sum_reg_c_expectation_bd}]

First, let's recall the definitions of $\mathbb{E}[\boldsymbol{V}_t]$ and $\boldsymbol{V}_t$ for $t \in [1,T]$:

\begin{equation}
\begin{aligned}
\label{eq: expected_upsilon_def}
\mathbb{E}[\boldsymbol{V}_t] & = \sum_{\ell=1}^{t} \mathbb{E}[w_{\ell} \boldsymbol{r}_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell}, \ell}^{\top}] + \lambda \boldsymbol{I} 
 =
\sum_{i=1}^{K} \mathbb{E} \left[ \sum_{\ell \in \mathcal{T}_{i,t}} w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right] + \lambda \boldsymbol{I}
% & =
% \sum_{i=1}^{K} N_{i,t+1} 
% \mathbb{E} \left[ w_{i} \boldsymbol{r}_{i} \boldsymbol{r}_{i}^{\top} \right] + \lambda \boldsymbol{I}
% =
% \sum_{i=1}^{K} N_{i,t+1} \left( \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\top} + \Sigma_{\boldsymbol{r,i}} \right) + \lambda \boldsymbol{I}
,
\end{aligned}
\end{equation}
\[
\begin{aligned}
\boldsymbol{V}_t 
=
\sum_{i=1}^{K} \sum_{\ell=1}^{N_{i,t+1}} w_{\ell} \boldsymbol{r}_{i, \ell} \boldsymbol{r}_{i, \ell}^{\top} + \lambda \boldsymbol{I} .
\end{aligned}
\]

where 
$\mathcal{T}_{i,t}$ denotes the set of episodes when arm $i$ was pulled.
% \begin{small}
% $\Sigma_{\boldsymbol{r},i} = 
% \begin{bmatrix}
% \sigma_{r,i,1}^2 & & 0\\
%   & \ddots &\\
% 0 & & \sigma_{r,i,D}^2
% \end{bmatrix}_{D \times D}$ 
% \end{small}
% denotes the covariance matrix of reward.

Due to the assumption that event C holds, we have $\forall i \in [K]$,

\[
\begin{aligned}
\mathbb{E} \left[ \sum_{\ell \in \mathcal{T}_{i,t}} w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
-
\omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\preceq
\sum_{\ell \in \mathcal{T}_{i,t}} \left( w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right),
\end{aligned}
\]

implying that for any $\boldsymbol{x} \in \mathbb{R}^{D}$, we can get

\begin{equation}
\begin{aligned}
\label{eq: expected_with_beta}
\chi
& = 
\boldsymbol{x}^{\top} 
\boldsymbol{V}_{t}
\boldsymbol{x} \\
& \geq
\boldsymbol{x}^{\top} 
\left( 
\sum_{i=1}^{K} \left(
\sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[ w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
-
\omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\right)
+ \lambda \boldsymbol{I}
\right)
\boldsymbol{x} \\
& = 
\boldsymbol{x}^{\top} 
\left(
\mathbb{E} \left[\boldsymbol{V}_{t} \right]
- 
\sum_{i=1}^{K}
\omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\right)
\boldsymbol{x}
.
\end{aligned}
\end{equation}

Next we make a preliminary analysis over the norm-distances of $\Vert \boldsymbol{x} \Vert^2_{ \sum_{i=1}^{K} \left( \sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[ w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
\right) }$.

Let $w^{\downarrow} = \omega / \max_{i\in[K], t\in[T]}(\Vert \boldsymbol{r}_{i,t} \Vert_2^2)$, $w^{\uparrow} = \omega/\min_{i\in[K], t\in[T]}(\Vert \boldsymbol{r}_{i,t} \Vert_2^2)$, we have 

\[
w^{\downarrow}
\sum_{i=1}^{K} \left( \sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[\boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
\right) 
\preceq
\sum_{i=1}^{K} \left( \sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[ w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
\right) 
\preceq
w^{\uparrow}
\sum_{i=1}^{K} \left( \sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[\boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
\right).
\]

By the continuity of norm-distance, there must be a weight $ \tilde{w} \in (w^{\downarrow}, w^{\uparrow})$ such that

\[
\boldsymbol{x}^{\top}
\left( 
\sum_{i=1}^{K} \Big( \sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[ w_{\ell} \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
\Big)
\right)
\boldsymbol{x}
=
\boldsymbol{x}^{\top}
\left( 
\tilde{w}
\sum_{i=1}^{K} \Big( \sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[ \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
\Big)
\right)
\boldsymbol{x}.
\]

Substituting it in Eq. \ref{eq: expected_with_beta}, we have 

\begin{equation}
\begin{aligned}
\label{eq: expected_with_beta_2}
& \boldsymbol{x}^{\top} 
\left(
\mathbb{E} \left[\boldsymbol{V}_{t} \right]
- 
\sum_{i=1}^{K}
\omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\right)
\boldsymbol{x} \\
& \qquad = 
\boldsymbol{x}^{\top} 
\left( 
\tilde{w}
\sum_{i=1}^{K}
\sum_{\ell \in \mathcal{T}_{i,t}} \mathbb{E} \left[ \boldsymbol{r}_{i,\ell} \boldsymbol{r}_{i,\ell}^{\top} \right]
+ \lambda \boldsymbol{I}
-
\sum_{i=1}^{K} \omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\right)
\boldsymbol{x} \\
& \qquad = 
\boldsymbol{x}^{\top} 
\left( 
\tilde{w}
\sum_{i=1}^{K} N_{i,t+1} \left(
 \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^T + \Sigma_{\boldsymbol{r,i}} \right)
+ \lambda \boldsymbol{I}
-
\sum_{i=1}^{K} \omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\right)
\boldsymbol{x}, \\
\end{aligned}
\end{equation}

where the final line follows by the definition of outer product expectation, and
\begin{small}
$\Sigma_{\boldsymbol{r},i} = 
\begin{bmatrix}
\sigma_{r,i,1}^2 & & 0\\
  & \ddots &\\
0 & & \sigma_{r,i,D}^2
	%  \sigma_{r}^2 & &  \\
	%  & ... &  \\
	%  & &  \sigma_{r}^2
\end{bmatrix}_{d \times d}$ 
\end{small}
denotes the covariance matrix of reward.

Similarly, let $p = \argminA_{i \in [K]} \boldsymbol{x}^{\top} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\top} \boldsymbol{x}$ and $
q = \argmaxA_{j \in [K]} \boldsymbol{x}^{\top} \boldsymbol{\mu}_{j} \boldsymbol{\mu}_{j}^{\top} \boldsymbol{x}$, we can obtain

\[
\boldsymbol{x}^{\top} \left( t \boldsymbol{\mu}_{p} \boldsymbol{\mu}_{p}^{\top}
\right) \boldsymbol{x} 
\leq
\boldsymbol{x}^{\top} \left( \sum_{i=1}^{K} N_{i,t+1} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\top}
\right) \boldsymbol{x} 
\leq 
\boldsymbol{x}^{\top} \left( t \boldsymbol{\mu}_{q} \boldsymbol{\mu}_{q}^{\top}
\right) \boldsymbol{x}. 
\]

By the continuity of norm-distance, result above implies that $\exists a \in [0,1]$, such that
\begin{equation}
\begin{aligned}
\label{eq: eq_mu}
\boldsymbol{x}^{\top} \left( \sum_{i=1}^{K} N_{i,t+1} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\top}
\right) \boldsymbol{x}
=
\boldsymbol{x}^{\top} \left( t \boldsymbol{\tilde{\mu}} \boldsymbol{\tilde{\mu}}^{\top}
\right) \boldsymbol{x}, \\
\end{aligned}
\end{equation}
where $\boldsymbol{\tilde{\mu}} = a \boldsymbol{\mu}_{p} + (1-a) \boldsymbol{\mu}_{q}$.

Similarly, for $\Vert\boldsymbol{x} \Vert^2_{ \sum_{i=1}^{K} \left( N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right) }$, since the covariance matrices $\Sigma_{\boldsymbol{r,i}}, \forall i \in [K]$ are diagonal, by Lemma \ref{lemma: eigen_bound}, we have 
\begin{small}
\[
\xi_{\min} \left( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right)
\Vert \boldsymbol{x} \Vert_2^2
\leq
\boldsymbol{x}^{\top}
\left( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right)
\boldsymbol{x}
\leq
\xi_{\max} \left( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right)
\Vert \boldsymbol{x} \Vert_2^2,
\]
\end{small}

where 
$\xi_{\min} ( \sum_{i=1}^{K} N_{i,t} \Sigma_{\boldsymbol{r,i}})$
denotes the minimum eigenvalue of matrix 
$\sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} $, while 
$\xi_{\max} ( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} )$
denotes the corresponding maximum one.
We will also use $\xi (\cdot)$ to denote the eigenvalue calculator for a matrix in the following part.
By the continuity of nor-distance, result above implies that there exist a constant 
$\tilde{\xi}_t \in \big[\xi_{\min} ( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} ), \xi_{\max} ( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} ) \big]$, such that
\[
\begin{aligned}
\xi_{\min} \left( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right) 
\Vert\boldsymbol{x} \Vert_2^2 
\leq
\tilde{\xi}_t \Vert \boldsymbol{x} \Vert_2^2
=
\boldsymbol{x} ^{\top}
\left( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right)
\boldsymbol{x}
\leq
\xi_{\max} \left( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right)
\Vert \boldsymbol{x} \Vert_2^2,
\end{aligned}
\]

% where $\tilde{\sigma}^2_r = w_2 \xi_{\min} ( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} ) + (1- w_2) \xi_{\max} ( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} )  $.
Note that $\sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}}$ is diagonal, we have $\xi_{\min} ( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} ) = \min_{d \in [D]} \sum_{i=1}^{K} N_{i,t+1} \sigma_{r,i,d} \geq t \sigma^2_{r \downarrow}$, and similarly, $\xi_{\max} ( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} ) \leq t \sigma^2_{r \uparrow}$.
Define $\tilde{\sigma}^2_{r,t} = \frac{\tilde{\xi}_t}{t}$,
we have $\tilde{\sigma}^2_{r,t} \in [\sigma^2_{r \downarrow},\sigma^2_{r \uparrow}]$ satisfying 
\begin{equation}
\label{eq: eq_sigma}
t \tilde{\sigma}^2_{r,t} \Vert \boldsymbol{x} \Vert_2^2
=
\boldsymbol{x}^{\top}
\left( \sum_{i=1}^{K} N_{i,t+1} \Sigma_{\boldsymbol{r,i}} \right)
\boldsymbol{x}.
\end{equation}


By plugging above result back to Eq. \ref{eq: expected_with_beta_2}, we have
\begin{equation}
\begin{aligned}
\label{eq: expected_with_beta2}
\chi
& \geq
\boldsymbol{x}^{\top} 
\left( 
\underbrace{
\tilde{w}
\sum_{i=1}^{K} N_{i,t+1} \left(
 \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^T + \Sigma_{\boldsymbol{r,i}} \right)
+ \lambda \boldsymbol{I}
}_{\mathbb{E}[\boldsymbol{V}_t] }
-
\sum_{i=1}^{K} \omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\right)
\boldsymbol{x} \\
& \underset{(a)}{=}
\boldsymbol{x}^{\top} 
\left( 
\tilde{w} t \boldsymbol{\tilde{\mu}} \boldsymbol{\tilde{\mu}}^{\top} + \left( \tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda \right) \boldsymbol{I}
-
\sum_{i=1}^{K} \omega \sqrt{ N_{i,t+1} \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
\right)
\boldsymbol{x} \\
& \underset{(b)}{\geq}
\boldsymbol{x}^{\top} 
\bigg (
\underbrace{
\tilde{w} t \boldsymbol{\tilde{\mu}} \boldsymbol{\tilde{\mu}}^{\top} 
}_{\boldsymbol{A}_t}
+ 
\underbrace{
\left( \tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda \right) \boldsymbol{I}
}_{\boldsymbol{B}_t}
-
\underbrace{
\omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)} \boldsymbol{e e}^{\top}
}_{\boldsymbol{C}_t}
\bigg )
\boldsymbol{x}.
\end{aligned}
\end{equation}

where (a) holds by Eq. \ref{eq: eq_mu} and Eq. \ref{eq: eq_sigma}, (b) holds since the squared root term is maximized when $N_{i,t+1} = t/K, \forall i \in [K]$.
Note that $\boldsymbol{B}_t$ is diagonal matrix, and $-\boldsymbol{C}_t$ is rank-1 matrix yields one eigenvalue of $- \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)} \Vert \boldsymbol{e} \Vert_2^2 = -D \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)}$ and $D-1$ eigenvalues of 0, we have 

\[
\xi_{\min} (\boldsymbol{B}_t - \boldsymbol{C}_t) = \tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda -D \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)}.
\]

Since $t \geq M$, we can trivially derive $\tilde{w} t\tilde{\sigma}^2_{r,t} + \lambda \geq t \sigma^2_{r \downarrow} + \lambda \geq 2D \omega \sqrt{Kt\log \frac{t}{\alpha}}$ due to $\tilde{w}\geq 1$ and $\tilde{\sigma}^2 \geq \sigma^2_{r \downarrow}$, implying that the minimum eigenvalue $\xi_{\min} (\boldsymbol{B}_t - \boldsymbol{C}_t) \geq 0$ and
the matrix $\boldsymbol{B}_t - \boldsymbol{C}_t $ is a positive semi-definite matrix, and thus $\boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t$ is positive-definite. Also note that $\boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t$ is symmetric, by Lemma~\ref{lemma: eigen_bound}, we can derive that 

\begin{equation}
\begin{aligned}
\label{eq: ball_width_bd}
& \xi_{\min} \left( \boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t \right) \Vert \boldsymbol{x} \Vert_2^2
\leq
\left( \boldsymbol{x} \right)^{\top} \left( \boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t \right) \left( \boldsymbol{x} \right)
\leq
\chi \\
& \underset{(a)}{\implies}
\Vert \boldsymbol{x} \Vert_2^2 
\leq
\frac{\chi}{\xi_{\min} \left( \boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t \right)},
\end{aligned}
\end{equation}

where $\xi_{\min}\left( \boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t \right)$ is the minimum eigenvalue of $\boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t$, and the implication (a) holds since $\xi_{\min}\left( \boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t \right) > 0$ due to the positive-definite of $\boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t$.

Note that $\boldsymbol{A}_t$ and $-\boldsymbol{C}_t$ are rank-1 matrices and $\boldsymbol{B}_t$ is diagonal matrix, we can trivially derive that:

\begin{itemize}
\item 
$\boldsymbol{A}_t + \boldsymbol{B}_t$ has one eigenvalue of $\tilde{w} t(\Vert \boldsymbol{\tilde{\mu}} \Vert_2^2 + \tilde{\sigma}^2_{r,t}) + \lambda$ and $D-1$ eigenvalues of $ \tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda$. 
\item $-\boldsymbol{C}_t$ has one eigenvalue of $- \omega \sqrt{ Kt\log \left( \frac{t}{\alpha} \right)} \Vert \boldsymbol{e} \Vert_2^2 = -D \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)}$ and $D-1$ eigenvalues of 0.
\end{itemize}

Since $\boldsymbol{A}_t+\boldsymbol{B}_t$ and $-\boldsymbol{C}_t$ are both symmetric, by applying Lemma~\ref{lemma: eigen_rank}, we have 

\[
\begin{aligned}
\xi_{\min} \left( \boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t \right) 
& \geq
\xi_{\min} \left( \boldsymbol{A}_t + \boldsymbol{B}_t \right) 
+
\xi_{\min} \left( -\boldsymbol{C}_t \right) \\
& = 
\tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda - D \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)}
\end{aligned}
\]

Plugging above result back into Eq.~\ref{eq: ball_width_bd}, we have

\begin{equation}
\begin{aligned}
\label{eq: ball_width_bd2}
\Vert \boldsymbol{x} \Vert_2^2 
\leq
\frac{\chi}{\xi_{\min} \left( \boldsymbol{A}_t + \boldsymbol{B}_t - \boldsymbol{C}_t \right)}
\leq
\frac{\chi}{\tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda - D \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)}}.
\end{aligned}
\end{equation}

Again, since $t \geq M$ holds, the denominator of the final term is strictly positive.
Combining above result with Eq.~\ref{eq: expected_with_beta2} and rearranging the terms, for $t \geq M$, we can obtain

\begin{equation}
\begin{aligned}
\boldsymbol{x}^{\top} 
\mathbb{E} [\boldsymbol{V}_t]
\boldsymbol{x}
& =
\boldsymbol{x} ^{\top} 
\big ( \boldsymbol{A}_t + \boldsymbol{B}_t \big )
\boldsymbol{x} \\
& \leq 
\chi + \boldsymbol{x}^{\top} 
\boldsymbol{C}_t
\boldsymbol{x} \\
& \underset{(a)}{\leq}
\chi + \xi_{\max} \left(\boldsymbol{C}_t \right) \Vert \boldsymbol{x} \Vert_2^2 \\
& \underset{(b)}{\leq}
\chi + \frac{\chi D \omega \sqrt{ Kt \log \left( \frac{t}{\alpha} \right)}}{ \tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda - D \omega \sqrt{ Kt \log \left( \frac{t}{\alpha} \right)}} \\
& = 
\chi + \frac{\chi}{\frac{ \tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda}{ D \omega \sqrt{ Kt \log \left( \frac{t}{\alpha} \right)}} - 1} \\
& \underset{(c)}{\leq}
2 \chi 
= 
\boldsymbol{x}^{\top} \big( 2\boldsymbol{V}_t \big) \boldsymbol{x},
\end{aligned}
\end{equation}

where (a) follows from Lemma~\ref{lemma: eigen_bound}, (b) holds since Eq.~\ref{eq: ball_width_bd} and $\xi_{\max} \left(\boldsymbol{C}_t \right) = -\xi_{\min} \left(-\boldsymbol{C}_t \right) = D \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)}$, (c) holds since $\tilde{w} t \tilde{\sigma}^2_{r,t} + \lambda \geq 2 D \omega \sqrt{ K t \log \left( \frac{t}{\alpha} \right)} $ for $t \geq M$.

Since for $t \geq M$, the above result $\boldsymbol{x}^{\top} 
\mathbb{E} [\boldsymbol{V}_t]
\boldsymbol{x} \leq \boldsymbol{x}^{\top} \big( 2\boldsymbol{V}_t \big) \boldsymbol{x}$ holds for all $\boldsymbol{x} \in \mathbb{R}^{D}$, we have $2\boldsymbol{V}_t - \mathbb{E} [\boldsymbol{V}_t]$ is positive definite, and thus we can trivially derive that for for $t \geq M$,
\[
\boldsymbol{x}^{\top} 
\mathbb{E} [\boldsymbol{V}_t]^{-1}
\boldsymbol{x} 
\geq
\boldsymbol{x}^{\top} \big( 2\boldsymbol{V}_t \big)^{-1} \boldsymbol{x} 
= 
\frac{1}{2} \boldsymbol{x}^{\top} \boldsymbol{V}_t^{-1} \boldsymbol{x}.
\]

Therefore, we complete the proof of Lemma \ref{lemma: hidden_sum_reg_c_expectation_bd}.
\end{proof}



\subsection{Proof of Lemma \ref{lemma: hidden_sum_reg_c_2}}
\label{sec: proof_lemma_hidden_sum_reg_c_2}

\begin{proof}[Proof of Lemma \ref{lemma: hidden_sum_reg_c_2}]
\begin{equation}
\label{eq: trunc_regret_r}
\begin{aligned}
 \sum_{t=M+1}^{T} \sqrt{\frac{ \log \left( \frac{t}{\alpha} \right)}{N_{a_t,t}}}
& \underset{(a)}{\leq }
\sqrt{ \log \left( \frac{T}{\alpha} \right)} \sum_{i=1}^{K} \sum_{n=N_{i,M+1}}^{N_{i,T}} \sqrt{\frac{1}{n}}\\
& \underset{(b)}{\leq }
\sqrt{ \log \left( \frac{T}{\alpha} \right)} \sum_{i=1}^{K} \sum_{n=N_{i,1}}^{N_{i,T-M}} \sqrt{\frac{1}{n}}\\
& \underset{(c)}{\leq }
\sqrt{ \log \left( \frac{T}{\alpha} \right)} \sum_{i=1}^{K} \sum_{n=1}^{\frac{T-M}{K}} \sqrt{\frac{1}{n}}\\
& \underset{(d)}{\leq }
\sqrt{ \log \left( \frac{T}{\alpha} \right)} \sum_{i=1}^{K} 2 \sqrt{\frac{T-M}{K}}\\
& =
2 \sqrt{ K \log \left( \frac{T}{\alpha} \right) (T-M)}.
\end{aligned}
\end{equation}

Specifically, in step (a), we breakdown the totally truncated horizon by the episodes that each individual arm $i \in [K]$ was pulled, and replace $t$ with upper-bound $T$ in the original numerator.
Step (b) trivially holds since $\frac{1}{N_{i,t+M}} \leq \frac{1}{N_{i,t}}$ is strictly true for all $i \in [K]$. Step (c) follows from the fact that the entire sum is maximized when all arms are pulled an equal number of times. (d) holds since the fact that $ 2\sqrt{n}-2 \leq \sum_{x=1}^{n} \frac{1}{\sqrt{x}} \leq 2\sqrt{n} $.
\end{proof}













% \begin{lemma}
% \label{lemma: confidence_ball_width}
% Let $\boldsymbol{\mu} \in \mathbb{R}^D$, $M = \left\lfloor \argminA_{t} \left( (t-1) \sigma^2_{\boldsymbol{r}} + \lambda \geq 2D \sqrt{2K(t-1)\log \frac{t}{\alpha} }  \right) \right \rfloor$. Then for $t \geq M+1$, and any $c \in \Theta_t$, 
% \[
% \left| (\boldsymbol{c} - \boldsymbol{\hat{c}}_t)^{\top} \boldsymbol{\mu} \right| \leq \sqrt{2 \beta_t \boldsymbol{\mu}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}}.
% \]
% \end{lemma}



\subsection{Proof of Lemma \ref{lemma: hidden_sum_reg_c_1}}
\label{sec: proof_lemma_upbd_term1_hidden}

To derive the upper-bound of term 
$\sum_{t=M+1}^{T}
\min \left(
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}}, 
\frac{1}{2\sqrt{2}}
\right)$, 
we follow the similar techniques for analyzing the sum of instantaneous regret in OFUL~\cite{abbasi2011improved}. Specifically, we first show that the sum of squared terms $\min \left(
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}}, 
\frac{1}{2\sqrt{2}}
\right)^2$ is optimal up to $\mathcal{O}(\log T)$, and then extend the result to the sum of $\min \left(
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}}, 
\frac{1}{2\sqrt{2}}
\right)$ using Cauchy-Schwarz inequality. 


We begin with stating the following lemmas for proof. 
\begin{lemma}
\label{lemma: iter_E_upsilon}
For any action sequence of ${a_1},...,{a_T}$ and any $M \in [0,1]$, we have
\[
\emph{det} \left( \mathbb{E}[\boldsymbol{V}_{T}] \right) 
\geq
\emph{det} \left( \mathbb{E}[\boldsymbol{V}_{M}] \right) 
\prod_{t=M+1}^{T} 
\left(
1 + \frac{\emph{det} \left( \Sigma_{r,a_t} \right)}{\emph{det} \left( \mathbb{E}[\boldsymbol{V}_{t-1}] \right) }
+ 
\boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-1} \boldsymbol{\mu}_{a_t}
\right).
\]
\end{lemma}
Please see Appendix~\ref{sec: proof_lemma_iter_E_upsilon} for the detailed proof of Lemma~\ref{lemma: iter_E_upsilon}.


\begin{lemma}
\label{lemma: log_E_upsilon}
For any action sequence of ${a_1},...,{a_T}$, any weight $w_t = \frac{\omega}{\Vert \boldsymbol{r}_{a_t,t} \Vert_2^2}$, then for any $M \in [1,T]$, we have
\[
\log \left( \frac{ \emph{det} \left(\mathbb{E}[\boldsymbol{V}_{T}] \right) }{ \emph{det} \left(\mathbb{E}[\boldsymbol{V}_{M}] \right)} \right)
\leq 
D \log \left( 1 + \frac{\omega}{D \lambda}(T-M) \right).
\]
\end{lemma}

Please see Appendix~\ref{sec: proof_lemma_log_E_upsilon} for the detailed proof of Lemma~\ref{lemma: log_E_upsilon}.

\begin{proof}[Proof of Lemma\ref{lemma: hidden_sum_reg_c_1}]

\textbf{Step-1:}
We first show that the sum of squared terms is optimal up to $\mathcal{O}(\log(T-M))$. Specifically,

\begin{equation}
\begin{aligned}
\label{eq: term1_squared_hidden}
& \sum_{t=M+1}^{T} \min \left(
\Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}\left[\boldsymbol{V}_{t-1}\right]^{-1}}, 
\frac{1}{2\sqrt{2}}
\right)^2 \\
& \qquad \qquad =
\sum_{t=M+1}^{T} \min \left( \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}_{a_t}, \frac{1}{8} \right) \\
& \qquad \qquad \underset{(a)}{\leq} 
\sum_{t=M+1}^{T} \frac{1}{8 \log(9/8)} \log \left( 1 + \min \big( \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}_{a_t}, \frac{1}{8} \big) \right) \\
& \qquad \qquad \leq 
\sum_{t=M+1}^{T} \frac{1}{8 \log(9/8)} \log \left( 1 + \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}_{a_t} \right) \\
\end{aligned}
\end{equation}

where (b) holds since the fact that $\log(1+x) \geq 8 \log \left( \frac{9}{8} \right)x$ for $x \leq \frac{1}{8}$.

On the other hand, Lemma~\ref{lemma: iter_E_upsilon} implies that 

\begin{equation}
\begin{aligned}
\log \left( \frac{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{T}] \right) }{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{M}] \right)} \right) 
\geq 
\sum_{t=M+1}^{T} 
\log \left(
1 + \frac{\text{det} \left( \Sigma_{r,a_t} \right)}{\text{det} \left( \mathbb{E}[\boldsymbol{V}_{t-1}] \right) }
+ 
\boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-1} \boldsymbol{\mu}_{a_t}
\right).
\end{aligned}
\end{equation}

Additionally, since $\text{det} \left( \Sigma_{r,a_t} / \mathbb{E}[\boldsymbol{V}_{t-1}] \right) > 0$ and $\Vert \boldsymbol{\mu}_{i} \Vert_2^2 \leq \Vert \boldsymbol{\mu}_{i} \Vert_1^2 \leq D, \forall i \in [K]$, by Lemma~\ref{lemma: log_E_upsilon}, we have 

\begin{equation}
\begin{aligned}
\label{eq: term1_log_sum_hidden}
D \log \left( 1 + \frac{\omega}{\lambda}(T-M) \right)
\geq 
\log \left( \frac{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{T}] \right) }{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{M}] \right)} \right) 
\geq
\sum_{t=M+1}^{T} 
\log \left(
1 + 
\boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}_{a_t}
\right).
\end{aligned}
\end{equation}

Plugging the above result back into Eq.~\ref{eq: term1_squared_hidden}, we can derive a bound up to $\mathcal{O}(\log(T-M))$ on the sum of squared instantaneous regrets as:

\begin{equation}
\begin{aligned}
\label{eq: term1_squared_upbd_hidden}
\sum_{t=M+1}^{T} \min \left( \sqrt{ \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}_{a_t}}, \frac{1}{2\sqrt{2}} \right)^2 
& \leq 
\frac{D}{8 \log(9/8)} \log \left( 1 + \frac{\omega}{ \lambda}(T-M) \right). \\
\end{aligned}
\end{equation}

\textbf{Step-2:}
Given the upper-bound on the sum of squared instantaneous regrets , we next extend it to the sum of instantaneous regrets by using Cauchy-Schwarz inequality.
Specifically, 

\begin{equation}
\begin{aligned}
\sum_{t=M+1}^{T} \min \Big( \sqrt{ \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}_{a_t}}, \frac{1}{2\sqrt{2}} \Big)
& \leq
\sqrt{(T-M) \sum_{t=M+1}^{T} \min \Big( \sqrt{ \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_t]^{-1} \boldsymbol{\mu}_{a_t}}, \frac{1}{2\sqrt{2}} \Big)^2 } \\
& \leq
\sqrt{\frac{D}{8 \log(9/8)} (T-M) \log \Big( 1 + \frac{\omega}{ \lambda}(T-M) \Big)}. \\
\end{aligned}
\end{equation}

Therefore we complete the proof of Lemma \ref{lemma: hidden_sum_reg_c_1}.  
\end{proof}


\subsubsection{Proof of Lemma~\ref{lemma: iter_E_upsilon}}
\label{sec: proof_lemma_iter_E_upsilon}

We begin with a lemma that will be utilized in the derivations of Lemma~\ref{lemma: iter_E_upsilon}:

\begin{lemma}[Determinant of Symmetric PSD Matrices Sum]
\label{lemma: symmetric_det}
Let $\boldsymbol{A} \in \mathbb{R}^{n \times n}$ be a symmetric and positive definite matrix, and $\boldsymbol{B} \in \mathbb{R}^{n \times n}$ be a symmetric and positive (semi-) definite matrix. Then we have 

\[
\emph{det} \left( \boldsymbol{A} + \boldsymbol{B} \right) 
\geq
\emph{det} \left( \boldsymbol{A}\right) + \emph{det} \left( \boldsymbol{B}\right) 
\]
\end{lemma}

\begin{proof}

\begin{equation}
\begin{aligned}
\label{eq: symmetric_det1}
\text{det} \left( \boldsymbol{A} + \boldsymbol{B} \right) 
=
\text{det} \left( \boldsymbol{A}\right) 
\text{det} \left( \boldsymbol{I} + \boldsymbol{A}^{-\frac{1}{2}} \boldsymbol{B} \boldsymbol{A}^{-\frac{1}{2}} \right).
\end{aligned}
\end{equation}

Let $\lambda_1, ..., \lambda_n$ be the eigenvalues of $\boldsymbol{A}^{-\frac{1}{2}} \boldsymbol{B} \boldsymbol{A}^{-\frac{1}{2}}$. 
Since $\boldsymbol{A}^{-\frac{1}{2}} \boldsymbol{B} \boldsymbol{A}^{-\frac{1}{2}}$ is positive (semi-) definite, we have $\lambda_i \geq 0, \forall i \in [n]$, which implies

\begin{equation}
\begin{aligned}
\label{eq: symmetric_det2}
\text{det} \left( \boldsymbol{I} + \boldsymbol{A}^{-\frac{1}{2}} \boldsymbol{B} \boldsymbol{A}^{-\frac{1}{2}} \right)
=
\prod_{i=1}^{n} (1 + \lambda_i) 
\geq 
1 + \prod_{i=1}^{n} \lambda_i
= 
\text{det} (\boldsymbol{I}) + \text{det} \left( \boldsymbol{A}^{-\frac{1}{2}} \boldsymbol{B} \boldsymbol{A}^{-\frac{1}{2}} \right).
\end{aligned}
\end{equation}

Combining Eq.\ref{eq: symmetric_det1} with Eq.~\ref{eq: symmetric_det2} concludes the proof.

\end{proof}


\begin{proof}[Proof of Lemma~\ref{lemma: iter_E_upsilon}]

For $\boldsymbol{V}_t$ and $\mathbb{E}[\boldsymbol{V}_t]$, by definition and $w_t \geq 1$,

\[
\boldsymbol{V}_{t}
=
\boldsymbol{V}_{t-1} + w_{t} \boldsymbol{r}_{a_t, t} \boldsymbol{r}_{a_t, t}^{\top}
\quad \text{and} \quad
\boldsymbol{V}_{0} = \lambda \boldsymbol{I}
\]
\[
\mathbb{E}[\boldsymbol{V}_{t}]
=
\mathbb{E}[\boldsymbol{V}_{t-1} + w_{t} \boldsymbol{r}_{a_t, t} \boldsymbol{r}_{a_t, t}^{\top}] 
\succeq
\mathbb{E}[\boldsymbol{V}_{t-1} + \boldsymbol{r}_{a_t, t} \boldsymbol{r}_{a_t, t}^{\top}] 
=
\mathbb{E}[\boldsymbol{V}_{t-1}] 
+ \boldsymbol{\mu}_{a_t} \boldsymbol{\mu}_{a_t}^{\top} + \Sigma_{r, a_t}.
\]

Since $\mathbb{E}[\boldsymbol{V}_{t}]$ is symmetric and positive definite, we have 

\begin{small}
\begin{equation}
\begin{aligned}
\label{eq: iter_E_upsilon1}
\text{det} \left( \mathbb{E}[\boldsymbol{V}_{t}] \right)
& \geq 
\text{det} \left( \mathbb{E}[\boldsymbol{V}_{t-1}] + \boldsymbol{\mu}_{a_t} \boldsymbol{\mu}_{a_t}^{\top} + \Sigma_{r, a_t} \right) \\
& = 
\text{det} \left( 
\mathbb{E}[\boldsymbol{V}_{t-1}]^{\frac{1}{2}} \left( 
\boldsymbol{I} + \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \left (\boldsymbol{\mu}_{a_t} \boldsymbol{\mu}_{a_t}^{\top} + \Sigma_{r,a_t} \right) \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right)  \mathbb{E}[\boldsymbol{V}_{t-1}]^{\frac{1}{2}}
\right) \\
& = 
\text{det} \left( 
\mathbb{E}[\boldsymbol{V}_{t-1}] \right) 
\text{det} \left( 
\boldsymbol{I} + \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \left (\boldsymbol{\mu}_{a_t} \boldsymbol{\mu}_{a_t}^{\top} + \Sigma_{r,a_t} \right) \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right) \\
& \underset{(a)}{\geq}
\text{det} \left( 
\mathbb{E}[\boldsymbol{V}_{t-1}] \right) 
\left(
\text{det} \left( 
\boldsymbol{I} + \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \boldsymbol{\mu}_{a_t} \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right)
+ \text{det} \left( 
\mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}  \Sigma_{r,a_t} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right)
\right) \\
\end{aligned}
\end{equation}
\end{small}

where (a) holds since $\left( 
\boldsymbol{I} + \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \boldsymbol{\mu}_{a_t} \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right)$ and $\left( 
\mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}  \Sigma_{r,a_t} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right)$ are positive definite and applying Lemma~\ref{lemma: symmetric_det} yields the result.

Let $\mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \boldsymbol{\mu}_{a_t} = \boldsymbol{v}_t$, and we observe that 
\[
\left( \boldsymbol{I} + \boldsymbol{v}_t \boldsymbol{v}_t^{\top} \right) \boldsymbol{v}_t
=
\boldsymbol{v}_t + \boldsymbol{v}_t \left( \boldsymbol{v}_t^{\top} \boldsymbol{v}_t \right)
=
\left(1 + \boldsymbol{v}_t^{\top} \boldsymbol{v} \right) \boldsymbol{v}_t.
\]

Hence, $1 + \boldsymbol{v}_t^{\top} \boldsymbol{v}$ is an eigenvalue of $\boldsymbol{I} + \boldsymbol{v}_t \boldsymbol{v}_t^{\top}$. And since $\boldsymbol{v}_t \boldsymbol{v}_t^{\top}$ is a rank-1 matrix, all other eigenvalue of $\boldsymbol{I} + \boldsymbol{v}_t \boldsymbol{v}_t^{\top}$ equal to 1, implying

\begin{equation}
\begin{aligned}
\label{eq: iter_E_upsilon2}
\text{det} \left( 
\boldsymbol{I} + \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \boldsymbol{\mu}_{a_t} \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right) 
& =
\text{det} \left( \boldsymbol{I} + \boldsymbol{v}_t \boldsymbol{v}_t^{\top} \right)\\
& =
1 + \boldsymbol{v}_t \boldsymbol{v}_t^{\top} \\
& =
1 + \left( \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \boldsymbol{\mu}_{a_t} \right)^{\top} \left( \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}} \boldsymbol{\mu}_{a_t} \right) \\
& = 
1 + \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-1} \boldsymbol{\mu}_{a_t}.
\end{aligned}
\end{equation}

Combining Eq.~\ref{eq: iter_E_upsilon1} and Eq.~\ref{eq: iter_E_upsilon2}, we have

\[
\begin{aligned}
\text{det} \left( \mathbb{E}[\boldsymbol{V}_{t+1}] \right)
& \geq
\text{det} \left( 
\mathbb{E}[\boldsymbol{V}_{t-1}] \right) 
\left(
1 + \boldsymbol{\mu}_{a_t}^{\top} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-1} \boldsymbol{\mu}_{a_t}
+ \text{det} \left( 
\mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}  \Sigma_{r,a_t} \mathbb{E}[\boldsymbol{V}_{t-1}]^{-\frac{1}{2}}
\right)
\right) \\
\end{aligned}
\]

The solution of Lemma~\ref{lemma: iter_E_upsilon} follows from induction.
\end{proof}


\subsubsection{Proof of Lemma~\ref{lemma: log_E_upsilon}}
\label{sec: proof_lemma_log_E_upsilon}

\begin{proof}

For the proof of this lemma, we follow the main idea of Determinant-Trace Inequality in OFUL~\cite{abbasi2011improved} (Lemma 10).
Specifically, by the definition of $\boldsymbol{V}_{t}$, we have 

\begin{equation}
\begin{aligned}
\label{eq: log_E_upsilon1}
\log \left( \frac{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{T}] \right) }{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{M}] \right)} \right)
& =
\log \left( \text{det} \left( \frac{ \mathbb{E}[\boldsymbol{V}_{M}] + \sum_{t=M+1}^{T} \mathbb{E}[ w_t \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top}] }{ \mathbb{E}[\boldsymbol{V}_{M}] } \right) \right) \\
& \underset{(a)}{\leq}
\log \left( \text{det} \left( 1 + \frac{ \sum_{t=M}^{T} \mathbb{E}[ w_t \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top}] }{ \lambda \boldsymbol{I} } \right) \right) \\
& = 
\log \left( \text{det} \left( 1 +  \frac{1}{\lambda} \sum_{t=M+1}^{T} \mathbb{E}[ w_t \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top}]  \right) \right), \\
\end{aligned}
\end{equation}


where (a) holds since $\text{det}(\mathbb{E}[\boldsymbol{V}_{M}]) \geq \text{det}(\mathbb{E}[\boldsymbol{V}_{0}]) = \lambda \boldsymbol{I}$. Let $\xi_1, ... , \xi_D$ denote the eigenvalues of $\sum_{t=M+1}^{T} \mathbb{E}[ w_t \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top}]$, and note:

\begin{equation}
\begin{aligned}
\label{eq: log_E_upsilon2}
\sum_{d=1}^{D} \xi_d 
& =
\text{Trace} \left( \sum_{t=M+1}^{T} \mathbb{E}[ w_t \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top}] \right) \\
& =
\sum_{t=M+1}^{T} \mathbb{E} \left[\text{Trace} \left( w_t \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top} \right) \right] \\
& =
\sum_{t=M+1}^{T} \mathbb{E} \left[\text{Trace} \left( \frac{\omega}{\Vert \boldsymbol{r}_{a_t,t} \Vert_2^2} \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top} \right) \right] \\
& =
\sum_{t=M+1}^{T} \mathbb{E} \left[ \frac{\omega \Vert \boldsymbol{r}_{a_t,t} \Vert_2^2}{\Vert \boldsymbol{r}_{a_t,t} \Vert_2^2} \right] \\
& = 
\omega (T-M).
\end{aligned}
\end{equation}

Combining Eq.~\ref{eq: log_E_upsilon1} and Eq.~\ref{eq: log_E_upsilon2} implies 

\[
\begin{aligned}
\log \left( \frac{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{T}] \right) }{ \text{det} \left(\mathbb{E}[\boldsymbol{V}_{M}] \right)} \right)
& \leq
\log \left( \text{det} \left( 1 +  \frac{1}{\lambda} \sum_{t=M+1}^{T} \mathbb{E}[ w_t \boldsymbol{r}_{a_t,t} \boldsymbol{r}_{a_t,t}^{\top}]  \right) \right) \\
& =
\log \left( \prod_{i=1}^{D} \left( 1 + \frac{\xi_i}{\lambda} \right) \right) \\
& =
D \log \left( \prod_{i=1}^{D} \left( 1 + \frac{\xi_i}{\lambda} \right) \right)^{\frac{1}{D}} \\
& \underset{(a)}{\leq}
D \log \left( \frac{1}{D} \sum_{i=1}^{D} \left( 1 + \frac{\xi_i}{\lambda} \right) \right) \\
& \underset{(b)}{\leq}
D \log \left( 1 +  \frac{\omega (T-M)}{D \lambda} \right), \\
\end{aligned}
\]

where (a) follows from the inequality of arithmetic and geometric means, and (b) follows from Eq.~\ref{eq: log_E_upsilon2}.
\end{proof}



