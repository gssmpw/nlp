\section{Analyses for Section \ref{sec: knonw}}

\subsection{Proof of Theorem \ref{theorem:up_bd_stat} (Unknown Preference Case: Regret Analysis of Algorithm \ref{alg:PRUCB_UP})}
\label{sec:app_up_bd_stat}


The presented Theorem \ref{theorem:up_bd_stat} establishes the upper bound of regret $R(T)$ for PRUCB-HP under unknown preference environment.
For the convenience of the reader, we re-state some notations that will be used in the following before going to proof.
In the case where both reward $\boldsymbol{r}_t$ and preference $\boldsymbol{c}_t$ follow fixed distributions with mean vectors of $\boldsymbol{\mu}$ and $\boldsymbol{\overline{c}}$, the optimal arm 
$a^{*}_{t} = \argmaxA_{i \in [K]} \boldsymbol{\overline{c}}^T \boldsymbol{\mu}_i $ 
remains the same in each step, and thus we use $a^{*}$ to denote the optimal arm for simplicity.
Let 
$\eta_{i} = \boldsymbol{\overline{c}}^{T} \Delta_{i} $ 
denote the expected overall-reward gap between arm $i$ and best arm $a^{*}$,
where
$\Delta_{i} = \mu_{a^{*}} - \mu_{i} \in \mathbb{R}^D$.

\subsubsection{Proof Sketch of Theorem \ref{theorem:up_bd_stat}}
\label{sec:app_pr_sketch_up_bd_stat}

We analyze the expected number of times in $T$ that one suboptimal arm $i \neq a^*$ is played, denoted by $N_{i,T}$.
Since regret performance is affected by both reward and preference estimates, we introduce a hyperparameter $\epsilon$ to quantify the accuracy of the empirical estimation $\boldsymbol{\hat{c}}_t$.

The key idea is that by using $\epsilon$ to measure the closeness of the preference estimation $\hat{\boldsymbol{c}}_t$ to the true expected vector $\overline{\boldsymbol{c}}$, the event of pulling a suboptimal arm can be decomposed into two disjoint sets based on whether $\hat{\boldsymbol{c}}_t$ is sufficiently accurate, as determined by  $\epsilon$. And the parameter $\epsilon$ can be tuned to optimize the final regret.
This decomposition allows us to address the problem of joint impact from the preference and reward estimate errors,  analyzing the undesirable behaviors of leaner caused by estimation errors of reward $\boldsymbol{\hat{r}}$ and preference $\boldsymbol{\hat{c}}$ independently. 

For suboptimal pulls induced by error of $\boldsymbol{\hat{r}}$, 
we show that the pseudo episode set $\mathcal{M}_i$ where the suboptimal arm $i$ is considered suboptimal under the preference estimate align with the true suboptimal episode set $[T]$, and the best arm within $\mathcal{M}_i$ is consistently identified as better than arm $i$. Using this insight, we show that this case can be transferred to a new preference known instance with a narrower overall-reward gap w.r.t $\epsilon$. 

% we show that the best arm $a^*$ still acts as a better arm compared to other suboptimal arms based on the estimated preference $\boldsymbol{\hat{c}}$. 
% In this case, we can leverage our previously derived Proposition \ref{prop: N_known_changing} to show that the regret caused in the case can be upper-bounded by order of $\log(T)$.

% For suboptimal pulls due to error of $\boldsymbol{\hat{c}}$, we first relax the original event set to a pure imprecise preference estimation event set. 
% By comparing the expected overall-reward gap (arm $i$ with best arm) with the estimated gap, and using a tailored-made distance bound, we derive a lower bound on the error between the true and estimated preference vectors. The probability of this error can then be upper-bounded using a tail bound.

For suboptimal pulls due to error of $\boldsymbol{\hat{c}}$, 
we first relax the suboptimal event set to an overall-reward estimation error set, eliminating the joint dependency on reward and preference from action $a_t$. Then we develop a tailored-made error bound (Lemma \ref{lemma: error_distance}) on preference estimation, which transfers the original error set to a uniform imprecise estimation set on preference, such that a tractable formulation of the estimation deviation can be constructed.


% \begin{proposition}
% \label{proposition:N_up_bd_stat}
% Let the objective preference coefficients $\boldsymbol{c}_t$ follow a stationary distribution and the true value is revealed after arm pulling at each time step,
% then for any suboptimal arm $i \neq a^* $, any $\epsilon \in (0, \eta_{i})$, $\alpha \in (0,1]$, we have,
% \[
% \mathbb{E}_{\epsilon}[N_{i,T}] 
% \leq
% \underbrace{
% \frac{4 \delta^2\log{(\frac{T}{\alpha})}}{(\eta_{i} - \epsilon)^2}  
% + 
% \frac{D \pi^2 \alpha^2} {3}
% }_{\text {\parbox{110pt}{\centering \emph{Suboptimal pulls caused by imprecise \textbf{reward estimation} }} } }
% +
% \underbrace{
% \big( \frac{ \sqrt{D} \delta \Vert \Delta_{i} \Vert_2 }{\epsilon} \big)^{\frac{5}{2}}
% +
% \frac{D\pi^2}{3}
% }_{\text {\parbox{100pt}{\centering \emph{Suboptimal pulling caused by \\ imprecise \textbf{preference estimation} }} } },
% \]

% % where 
% % $\gamma \approx 0.5772 $ is the Eulerâ€“Mascheroni constant~\cite{murty2010euler}, 
% % $ 0\leq q_{T}\leq 1/8T^{2}$ 
% % which approaches 0 as $T$ goes to infinity~\cite{boas1971partial}.
% \end{proposition}

% \begin{corollary}
% \label{corollary: proposition:N_up_bd_stat_episilon}
% For any $i \neq a^*$, the parameter $\epsilon \in (0, \eta_{i})$ can be optimally selected so as to minimize the RHS in Proposition ~\ref{proposition:N_up_bd_stat}. 
% For simplicity, taking $\epsilon = \frac{1}{\sqrt{D}+1} \eta_{i}$ yields
% \[
% \mathbb{E}[N_{i,T}] 
% \leq
% \frac{4 (\delta + \frac{\delta}{\sqrt{D}} )^2 \log{(\frac{T}{\alpha})}}{\eta_{i}^2} 
% +
% \frac{D \pi^2 \alpha^2} {3}
% +
% \big( \frac{ (D + \sqrt{D}) \delta \Vert \Delta_{i} \Vert_2 }{\eta_i} \big)^{\frac{5}{2}}
% +
% \frac{D\pi^2}{3}
% .
% \]

% \end{corollary} 

% Since $\sqrt{D}+D \leq 2D$ holds for all $D \geq 1$, we can replace $\sqrt{D}+D$ with $2D$ in Corollary \ref{corollary: proposition:N_up_bd_stat_episilon} for a simpler form. 
% Multiplying the results above by the expected overall-reward gap $\eta_i$ for all suboptimal arms $i \neq a^*$ and summing them up, we can derive the regret of PRUCB-HP follows the upper bound below,
% \[
% R(T) \leq 
% \sum_{i \neq a^*}
% \frac{4 (\delta + \frac{\delta}{\sqrt{D}} )^2 \log{(\frac{T}{\alpha})}}{\eta_{i}} 
% +
% \frac{D \pi^2 \alpha^2 \eta_{i}} {3}
% +
% \frac{4 \sqrt{2} (D \delta \Vert \Delta_{i} \Vert_2)^{\frac{5}{2}} }{\eta_i^{3/2}}
% +
% \frac{D\pi^2 \eta_i}{3}.
% \]

% Combining above results concludes the proof of Theorem \ref{theorem:up_bd_stat}.



\subsubsection{Proof of Theorem \ref{theorem:up_bd_stat}}
\label{sec:app_pr_up_bd_stat}

We begin with a more general upper bound (Proposition \ref{prop: N_known_changing}) for the learner's behavior using a policy that optimizes the inner product between the reward upper confidence bound (UCB) of arms and an arbitrary dynamic vector $\boldsymbol{b}_t$. It demonstrates that after a sufficiently large number of samples (on the order of $\mathcal{O}(\log T)$) for each arm $i$, for the episodes where the inner product of its rewards expectations with $\boldsymbol{b}_t$ is not highest, the expected number of times arm $i$ is pulled can be well controlled by a constant.
The proof of Proposition \ref{prop: N_known_changing} is provided in Appendix~\ref{sec: app_pr_prop_N_known_changing}.


\begin{proposition}
\label{prop: N_known_changing}
Let $\boldsymbol{b}_t \in \mathbb{R}^D$ be an arbitrary bounded vector at time step $t$ with $\Vert \boldsymbol{b}_t \Vert_1 \leq M$, define $\mathcal{M}_i := \{ t \in [T] \mid i \neq \argmaxA_{j \in [K]} \boldsymbol{b}_t^T \boldsymbol{\mu}_j \}, \forall i \in [K]$. For the policy of $a_t = \argmaxA \Phi ( \boldsymbol{b}_t, \hat{\boldsymbol{r}}_{i,t}+\sqrt{\frac{\log(t/\alpha)}{\max \{1, N_{i, t } \} }} \boldsymbol{e})$, for any arm $i \in [K]$, any subset $\mathcal{M}^{o}_i \subset \mathcal{M}_i$, we have
\[
\mathbb{E} \left[ \sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i \} } \right] 
\leq
\frac{4 M^2 \log{(\frac{T}{\alpha})}}{L_i^2}
+
\frac{ |\mathcal{B}^{+}_{T}| \pi^2 \alpha^2} {3},
\]
where $L_i = \min_{t \in \mathcal{M}^{o}_i} \{ \max_{j \in [K] \setminus i } \{\boldsymbol{b}_t^T (\boldsymbol{\boldsymbol{\mu}_j - \boldsymbol{\mu}_{i}
} )\} \}$, $\mathcal{B}^{+}_{T} := \{{[\boldsymbol{b}_{1}(d), \boldsymbol{b}_{2}(d), ..., \boldsymbol{b}_{T}(d)]} \neq \boldsymbol{0}, \forall d \in [D]\}$ is the collection set of non-zero $[\boldsymbol{b}(d)]^T$ sequence. 
\end{proposition}

\begin{proof}[Proof of Theorem \ref{theorem:up_bd_stat}]
% \label{pf: Theorem_N_up_bd_stat}
Let $N_{i,T}$ denote the expected number of times in $T$ that the suboptimal arm $i \neq a^*$ is played. We first analyze the upper-bound over $N_{i,T}$, and then derive the final regret $R(T)$ by $R(T) = \sum_{i \neq a^*} \Delta_{i} N_{i,T}$.
The proof consists of several steps.

\textbf{Step-1 ($N_{i,T}$ Decomposition with Parameter $\epsilon$):}


For any $i \neq a^*$, any time step $t \in [T]$, with a hyper-parameter $0 < \epsilon \leq \eta_i$ introduced, we can formulate the the number of times the suboptimal arm $i$ is played as follows:

\begin{equation}
\label{eq: N_up_bd_stat}
\begin{aligned}
N_{i,T} &= \sum_{t=1}^{T} \mathds{1}_{\{a_t = i\}} 
= 
\underbrace{\sum_{t=1}^{T} \mathds{1}_{\{a_t = i, \hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} > \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \}}}
_{N_{i,T}^{\widetilde{\boldsymbol{r}}}: \text { \parbox{110pt}{\centering \emph{Suboptimal pulls caused by imprecise \textbf{reward estimation} }} } }
+ 
\underbrace{\sum_{t=1}^{T} \mathds{1}_{\{a_t = i, \hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \}}}
_{N_{i,T}^{\widetilde{\boldsymbol{c}}}: \text {\parbox{100pt}{\centering \emph{Suboptimal pullings caused by \\ imprecise \textbf{preference estimation} }} } }.
\end{aligned}
\end{equation}

The technical idea behind is that by introducing $\epsilon$ to measure the closeness of the preference estimate $\hat{\boldsymbol{c}}_t$ to the true expected vector $\overline{\boldsymbol{c}}$ (i.e., the gap between $\hat{\boldsymbol{c}}_t^T \Delta_i$ and $\boldsymbol{\overline{c}}^T \Delta_i$), we can decouple the undesirable behaviors caused by either reward estimation error or preference estimation error.
Let $N_{i,T}^{\widetilde{\boldsymbol{r}}}$ and $N_{i,T}^{\widetilde{\boldsymbol{c}}}$ denote the times of suboptimal pulling induced by imprecise reward estimation and preference estimation (shown in Eq.~\ref{eq: N_up_bd_stat}).
We use $\mathbb{E}_{\epsilon}$ and $\mathbb{P}_{\epsilon}$ to denote the probability distribution and expectation under parameter $\epsilon$. Next, we will study these two terms separately.

\textbf{Step-2 (Bounding $N_{i,T}^{\hat{\boldsymbol{r}}}$):}

Define $\mathcal{M}_i$ as the set of episodes that arm $i$ achieves suboptimal expected overall-reward under preference estimation $\hat{\boldsymbol{c}}_t$, i.e., 
$\mathcal{M}_i := \{ t \in [T] \mid i \neq \argmaxA_{j \in [K]} \hat{\boldsymbol{c}}_t^T \boldsymbol{\mu}_j \}$.
Since for the event regarding $N_{i,T}^{\hat{\boldsymbol{r}}}$, we have $\hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} > \eta_i - \epsilon \geq 0$ holds for all $t \in [T]$, which implies that $a^{*}$ still yields a better result than $i$ given the estimated preference coefficient $\hat{\boldsymbol{c}}_{t}$ over time horizon $T$.
Thus the suboptimal pulling of arm $i$ is attributed to the imprecise rewards estimations of arms. 
Additionally, we have $\mathcal{M}_i = [T]$ since arm $i$ is at least worse than $a^*$ under the preference estimation $\hat{\boldsymbol{c}}_{t}$ for all episode $t \in [T]$. Hence for $N_{i,T}^{\widetilde{\boldsymbol{r}}}$ we have
\begin{equation}
N_{i,T}^{\widetilde{\boldsymbol{r}}} = 
\sum_{t=1}^{T} \mathds{1}_{\{a_t = i, \hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} > \eta_{i} - \epsilon\}}
=
\sum_{t \in \mathcal{M}_i} \mathds{1}_{\{a_t = i, \hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} > \eta_{i} - \epsilon\}}
% \leq
% \sum_{t \in \mathcal{M}_i} \mathds{1}_{\{a_t = i \}}
\end{equation}

Let $L_i = \min_{t \in \mathcal{M}_i} \{ \max_{j \in [K] \setminus i } \{\hat{\boldsymbol{c}}_t^T (\boldsymbol{\boldsymbol{\mu}_j - \boldsymbol{\mu}_{i} } )\} \}$, 
$\hat{\mathcal{C}}^{+}_{T}:= \{ [\boldsymbol{\hat{c}}_{1}(d), ..., \boldsymbol{\hat{c}}_{T}(d)] \neq \boldsymbol{0}, \forall d \in [D]\}$ be the collection set of non-zero preference estimation sequence. 
Recall that PRUCB-HP leverages $\boldsymbol{\hat{c}}_t$ for overall-reward UCB optimization, i.e., $a_t = \argmaxA \Phi( \boldsymbol{\hat{c}}_t, \hat{\boldsymbol{r}}_{i,t}+\sqrt{\frac{ \log(t/\alpha)}{\max \{1, N_{i, t } \} }} \boldsymbol{e})$.
By Proposition \ref{prop: N_known_changing}, we have 

\begin{equation}
\label{eq:N_unkown_stat}
\mathbb{E}_{\epsilon} \left[ \sum_{t \in \mathcal{M}_i} \mathds{1}_{\{a_t = i, \hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} > \eta_{i} - \epsilon\}} \right] 
\leq
\mathbb{E} \left[ \sum_{t \in \mathcal{M}_i} \mathds{1}_{\{a_t = i \} } 
\right] 
\leq
\frac{4 \delta^2 \log{(\frac{T}{\alpha})}}{L_i^2}
+
\frac{ |\hat{\mathcal{C}}^{+}_{T}| \pi^2 \alpha^2 } {3}.
\end{equation}

Additionally, since $\hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} > \eta_i - \epsilon \geq 0$ holds for all $t \in [T]$, it implies that

\[
L_i = \min_{t \in \mathcal{M}_i} \{ \max_{j \in [K] \setminus i } \{\boldsymbol{\hat{c}}_t^T (\boldsymbol{\boldsymbol{\mu}_j - \boldsymbol{\mu}_{i} } )\} \} 
\geq 
\min_{t \in \mathcal{M}_i} \hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} 
>
\eta_i - \epsilon
\geq
\eta_i - \epsilon
.
\]

Plugging above result into Eq. \ref{eq:N_unkown_stat}, and by $|\hat{\mathcal{C}}^{+}_{T}| \leq D$,
we have the expectation of $N_{i,T}^{\widetilde{\boldsymbol{r}}}$ in Eq.~\ref{eq: N_up_bd_stat} can be upper-bounded as follows:

\begin{equation}
\begin{aligned}
\label{eq: upbd_term_1_stat}
\mathbb{E}_{\epsilon} \left[N_{i,T}^{\widetilde{\boldsymbol{r}}} \right]
& = 
\mathbb{E}_{\epsilon} \left[ \sum_{t \in \mathcal{M}_i} \mathds{1}_{\{a_t = i, \hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} > \eta_{i} - \epsilon\}} \right]  \\
& \leq 
\frac{4 \delta^2 \log (T/\alpha)}{(\eta_{i} - \epsilon)^2}
+
D \frac{\pi^2 \alpha^2}{3}.
\end{aligned}
\end{equation}


\textbf{Step-3 (Bounding $N_{i,T}^{\widetilde{\boldsymbol{c}}}$):}

We begin with stating one tailored-made preference estimation error bound which will be utilized in our derivation.

% \vspace{4pt}
% \hrule \vspace{1mm}   \hrule
\begin{lemma}
\label{lemma: error_distance}
For any non-zero vectors $\Delta, \boldsymbol{\overline{c}} \in \mathbb{R}^k$, and all $\epsilon \in \mathbb{R}$, if $\boldsymbol{\overline{c}}^T \Delta > \epsilon$, then for any vector $\boldsymbol{c^{\prime}}$ s.t, $\boldsymbol{c^{\prime}}^T \Delta = \epsilon$, we have
\[
\Vert \boldsymbol{\overline{c}}- \boldsymbol{c^{\prime}}  \Vert_2 \geq \frac{ \boldsymbol{\overline{c}}^T \Delta  - \epsilon}{\Vert \Delta \Vert_2}.
\]
\end{lemma}

Please see Appendix \ref{sec: proof_lemma_error_distance} for the proof of Lemma \ref{lemma: error_distance}

% \hrule \vspace{1mm}   \hrule


Firstly we relax the instantaneous event set of $N_{i,T}^{\widetilde{\boldsymbol{c}}}$ in Eq.~\ref{eq: N_up_bd_stat} into a pure estimation error case as:
\begin{equation}
\begin{aligned}
\left\{ a_t = i \neq a^*, \hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \right\}
\subset
\left\{\hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_{i} - \epsilon \right\}
= 
\left\{\hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} \leq \eta_{i} - \epsilon \right\}.
\end{aligned}
\end{equation}

Then, according to Lemma \ref{lemma: error_distance} above, we can transfer the original overall-reward gap estimation error to the preference estimation error.
More specifically, since ${\boldsymbol{\overline{c}}^T \Delta_{i} > \eta_{i} - \epsilon }$ always holds, for any $t \in (0, T]$, by applying Lemma~\ref{lemma: error_distance}, we have 
\begin{equation}
\begin{aligned}
\label{eq: term_2_set_stat}
\left\{\hat{\boldsymbol{c}}_{t}^{T} \Delta_{i} \leq \eta_{i} - \epsilon \right\} 
& \subset
\left\{ \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 \geq \frac{\overline{\boldsymbol{c}}^{T} \Delta_{i} - ( \eta_{i} - \epsilon ) }{ \Vert \Delta_{i} \Vert_2 } \right\} \\
& \subset
\left\{ \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
\geq
\frac{ \epsilon }{ \Vert \Delta_{i} \Vert_2 } \right\}.
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\label{eq: term_2_set_prob_stat}
& \implies
\mathbb{P}_{\epsilon} \left( a_t = i \neq a^*, \hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \right)
\leq
\mathbb{P}_{\epsilon} \left( \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 \geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } \right). \\
\end{aligned}
\end{equation}

% Recall that the preferences estimation solution $\hat{\boldsymbol{c}}_{t}$ is located within the confidence region $\Theta_t$ centered at empirical average $\hat{\boldsymbol{c}}_t$, with radius of $\delta \sqrt{\frac{D\log(t)}{t}}$. Thus we have,
% \begin{equation}
% \begin{aligned}
% \Vert \overline{\boldsymbol{c}} - \widetilde{\boldsymbol{c}}_t \Vert_2 
% =
% \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t + \hat{\boldsymbol{c}}_t - \widetilde{\boldsymbol{c}}_t \Vert_2 
% & \underset{(a)}{\leq}
% \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
% +
% \Vert \hat{\boldsymbol{c}}_t - \widetilde{\boldsymbol{c}}_t \Vert_2 \\
% & \leq
% \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
% +
% \delta \sqrt{\frac{D\log(t)}{t}}.
% \end{aligned}
% \end{equation}

% where (a)) holds by triangle inequality. Combining above result with Eq. \ref{eq: term_2_set_stat} yields that
% \begin{equation}
% \begin{aligned}
% \label{eq: term_2_set_prob_stat}
% \mathbb{P}_{\epsilon} \left( a_t = i \neq a^*, \widetilde{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \widetilde{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \right)
% & \leq  
% \mathbb{P}_{\epsilon} \left( \Vert \overline{\boldsymbol{c}} - \widetilde{\boldsymbol{c}}_t \Vert_2 \geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } \right)\\
% & \underset{(a)}{\leq}
% \mathbb{P}_{\epsilon} \left( 
% \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
% +
% \delta \sqrt{\frac{D\log(t)}{t}} \geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } \right)\\
% & =
% \mathbb{P}_{\epsilon} \left( 
% \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
% \geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } - \delta \sqrt{\frac{D \log(t)}{t}} \right).\\
% \end{aligned}
% \end{equation}

% Please note that the confidence radius of $\delta \sqrt{\frac{D\log(t)}{t}}$ for preferences estimation ellipse asymptotically converges to 0 as $t$ increases. However for some small episodes $t$, the confidence radius might be too wide to exceeds the estimation gap error, i.e., $\frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } - \delta \sqrt{\frac{D\log(t)}{t}} < 0$, leading to the instantaneous probability upper-bound
% \begin{small}
% \[
% \mathbb{P}_{\epsilon} \left( a_t = i \neq a^*, \widetilde{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \widetilde{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \right)
% < \mathbb{P}_{\epsilon} \left( 
% \Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
% \geq 0^{-} > \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } - \delta \sqrt{\frac{D\log(t)}{t}} \right) = 1.
% \]
% \end{small}

% In the following, we will have a closer look at it and show that the episodes $t$ such that $\frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } < \delta \sqrt{\frac{D\log(t)}{t}}$ can be well bounded by a constant controlled by $\epsilon$.

For the RHS term in Eq. (\ref{eq: term_2_set_prob_stat}), we have 
\begin{equation}
\begin{aligned}
\label{eq: term_2_prob_2_stat}
% \sum_{t=\lfloor t_{\epsilon} \rfloor+1}^{T}
\mathbb{P}_{\epsilon} \bigg( 
\Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
\geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } \bigg) 
& =
\mathbb{P}_{\epsilon} \left( \sum_{d = 1}^{D} \left( \overline{\boldsymbol{c}} (d) - \hat{\boldsymbol{c}}_t (d) \right)^2 
\geq
\frac{\epsilon^2}{ \Vert \Delta_{i} \Vert_2^2 } \right) \\
& \underset{(a)}{\leq}
\sum_{d = 1}^{D} \mathbb{P}_{\epsilon} \left( | \overline{\boldsymbol{c}} (d) - \hat{\boldsymbol{c}}_t (d) | \geq 
\frac{\epsilon}{\sqrt{D} \Vert \Delta_{i} \Vert_2 } \right)
\end{aligned}
\end{equation}

where (a) holds by the union bound and the fact that there must be at least one objective $d \in [D]$ satisfying $\left( \overline{\boldsymbol{c}} (d) - \hat{\boldsymbol{c}}_t (d) \right)^2 \geq \frac{1}{D} \frac{\epsilon^2}{ \Vert \Delta_{i} \Vert_2^2 }$, otherwise the event would fail.
Note that for all $t \in (0, T]$, ${\boldsymbol{c}}_t$ follows same the distribution, and the deviation is exactly the radius of the preference confidence ellipse, thus we can use a tail bound for the confidence interval on empirical mean of i.i.d. sequence. 
Applying the the Hoeffding's inequality (Lemma~\ref{lemma: Hoeffding}), the probability for each objective $d \in [D]$ can be upper-bounded as follows:
\begin{equation}
\begin{aligned}
\mathbb{P}_{\epsilon} \left( | \overline{\boldsymbol{c}} (d) - \hat{\boldsymbol{c}}_t (d) | \geq 
\frac{\epsilon}{\sqrt{D} \Vert \Delta_{i} \Vert_2 } \right)
& \leq
2 \exp \left( - \frac{ 2 \epsilon^2 t^2 }{ D \Vert \Delta_{i} \Vert_2 \sum_{\tau=1}^{t} \delta^2 } \right)
=
2 \exp \left( - \frac{ 2 \epsilon^2 }{ D \Vert \Delta_{i} \Vert_2^2 \delta^2 } t \right).
\end{aligned}
\end{equation} 


% where $\sigma_{c}^2$ is the variance upper-bound of preference's distribution for each objective, $\delta$ is the upper-bound of $\Vert \boldsymbol{c}_t \Vert_1$.
Plugging above result back to Eq. \ref{eq: term_2_prob_2_stat} and combining with Eq. \ref{eq: term_2_set_prob_stat}, we can obtain the upper-bound for the expectation of $N_{i,T}^{\widetilde{\boldsymbol{c}}}$ in Eq.~\ref{eq: N_up_bd_stat} as follows:
\begin{equation}
\begin{aligned}
\label{eq: upbd_term_2_stat}
% \sum_{t=1}^{T} \mathds{1}_{\{\hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_{i} - \epsilon\}}
% \leq
% \psi_T \tau 
% +
% \frac{ |\mathcal{D}^{+}_{T}|^3 (\Delta_{i}^{\uparrow})^2 \delta^2 T}{ \eta^2 \tau }
\mathbb{E}_{\epsilon} \left[ N_{i,T}^{\widetilde{\boldsymbol{c}}} \right]
& = 
\mathbb{E}_{\epsilon} \left[ \sum_{t=1}^{T} \mathds{1}_{\{a_t = i \neq a^*, \hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \}} \right] \\
& = 
\sum_{t=1}^{T} \mathbb{P}_{\epsilon} \left( a_t = i \neq a^*, \hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \right) \\
& \leq
\sum_{t=1}^{T}
\mathbb{P}_{\epsilon} \bigg( 
\Vert \overline{\boldsymbol{c}} - \hat{\boldsymbol{c}}_t \Vert_2 
\geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } \bigg) \\ 
& \leq
2 D \sum_{t=1}^{T} \exp \left( - \frac{ 2 \epsilon^2 }{ D \Vert \Delta_{i} \Vert_2^2 \delta^2 } t \right) \\
& \underset{(a)}{\leq}
\frac{2 D}{ \exp \left( \frac{ 2 \epsilon^2 }{ D \Vert \Delta_{i} \Vert_2^2 \delta^2 } \right) -1 }\\
& \leq
\frac{ D^2 \Vert \Delta_{i} \Vert_2^2 \delta^2 }{ \epsilon^2 },
\quad (\text{by } e^x \geq x+1, \forall x \geq 0 )
\end{aligned}
\end{equation} 

where (a) holds since for any $a>0$, we have
\begin{equation}
\begin{aligned}
\label{eq: geometric_converge}
\sum_{t=1}^{T} \left( e^{-a} \right)^{t} 
= 
\sum_{t=0}^{T-1} e^{-a} \cdot \left( e^{-a} \right)^{t} 
& \leq 
\sum_{t=0}^{\infty} e^{-a} \cdot \left( e^{-a} \right)^{t} \\
& = \frac{e^{-a}}{1 - e^{-a}} \quad \text{(by closed form of the geometric series)} \\
& = 
\frac{1}{e^a-1}.
\end{aligned}
\end{equation}

\textbf{Step-4 (Final $R(T)$ Derivation and Optimization over $\epsilon$):}

Combining Eq.\ref{eq: N_up_bd_stat} with the corresponding upper-bounds of $\mathbb{E}_{\epsilon} \left[ N_{i,T}^{\widetilde{\boldsymbol{r}}} \right]$ (Eq.\ref{eq: upbd_term_1_stat}) and $\mathbb{E}_{\epsilon} \left[ N_{i,T}^{\widetilde{\boldsymbol{c}}} \right]$ (Eq.\ref{eq: upbd_term_2_stat}), we can get
\begin{equation}
\begin{aligned}
\label{eq: E_N_up_bd_stat}
\mathbb{E}_{\epsilon} [N_{i,T}]
\leq
\frac{4 \delta^2 \log (T/\alpha)}{(\eta_{i} - \epsilon)^2}
+
\frac{D \pi^2 \alpha^2}{3}
+
\frac{ D^2 \Vert \Delta_{i} \Vert_2^2 \delta^2 }{ \epsilon^2 }.
\end{aligned}
\end{equation} 


Note that for any $i \neq a^*$, the parameter $\epsilon \in (0, \eta_{i})$ can be optimally selected so as to minimize the RHS of Eq. \ref{eq: E_N_up_bd_stat}. 
For simplicity, taking $\epsilon = \frac{1}{2} \eta_{i}$ yields
\[
\mathbb{E}[N_{i,T}] 
\leq
\frac{16 \delta^2 \log{(\frac{T}{\alpha})}}{\eta_{i}^2} 
+
\frac{D \pi^2 \alpha^2} {3}
+
\frac{ 4 D^2 \Vert \Delta_{i} \Vert_2^2 \delta^2 }{ \eta_{i}^2 }
.
\]


Multiplying the results above by the expected overall-reward gap $\eta_i$ for all suboptimal arms $i \neq a^*$ and summing them up, we can derive the regret of PRUCB-HP in Theorem \ref{theorem:up_bd_stat}.
\end{proof}

\subsubsection{Proof of Proposition \ref{prop: N_known_changing}}
\label{sec: app_pr_prop_N_known_changing}

\begin{lemma}[Hoeffdingâ€™s inequality for general bounded random variables~\cite{vershynin2018high} (Theorem
2.2.6)]
\label{lemma: Hoeffding}
 Given independent random variables $\{X_1, ..., X_m \}$ where $a_i \leq X_i \leq b_i$ almost surely (with probability 1) we have:
\[
\mathbb{P} \left(\frac{1}{m} \sum_{i=1}^{m} X_i - \frac{1}{m} \sum_{i=1}^{m} \mathbb{E}[X_i] \geq \epsilon \right) \leq \exp \left(\frac{-2 \epsilon^2 m^2}{\sum_{i=1}^{m} (b_i-a_i)^2} \right).
\]
\end{lemma}

\begin{proof}[Proof of Proposition \ref{prop: N_known_changing}]

Define $\tilde{a}_{t}^{*} = \argmaxA_{j \in [K]} \boldsymbol{b}_t^T \boldsymbol{\mu}_j, \forall t \in (0,T]$, for any $\beta \in (0, T]$, we have

\begin{equation}
\begin{aligned}
\label{eq: term_1_stat}
\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i \} }
& \leq 
\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i, N_{i,t} \leq \beta \}}
+
\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i, N_{i,t} > \beta \}} \\
& \leq 
\beta
+
\sum_{t \in [T]} \mathds{1}_{\{a_t = i \neq \tilde{a}_{t}^{*}, N_{i,t} > \beta \}}.
\end{aligned}
\end{equation}



where the first term refers to the event of insufficient sampling (quantified by $\beta$) of arm $i$. 
, then for the event of second term, we have 
\begin{equation}
\begin{aligned}
\label{eq: event_ABC_t_stat1}
&{\{a_t = i \neq \tilde{a}_{t}^{*}, N_{i,t} > \beta \}} \\
& \subset
\Bigg\{ 
\underbrace{
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} > \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
}_{\tilde{A}_t}, 
N_{i,t} > \beta \Bigg\} \\
& \qquad \qquad \cup
\Bigg\{ 
\underbrace{
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} < \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}
}_{\tilde{B}_t}, 
N_{i,t} > \beta \Bigg\} \\
& \qquad \qquad \cup
\Bigg\{ 
\underbrace{
\tilde{A}_t^{\mathsf{c}}, \tilde{B}_t^{\mathsf{c}}, 
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}} , N_{i,t} > \beta }_{\tilde{\Gamma}_t} \Bigg\}.  \\
\end{aligned}
\end{equation}

Specifically, $\tilde{A}_t$ and $\tilde{B}_t$ denote the events where the constructed upper confidence bounds (UCBs) for arm $i$ or the optimal arm $a^{}$ fail to accurately bound their true expected rewards, indicating imprecise rewards estimation. Meanwhile, $\tilde{\Gamma}_t$ represents the event where the UCBs for both arms effectively bound their expected rewards, yet the UCB of arm $i$ still exceeds that of the arm $\tilde{a}_{t}^{*}$ though it yields the maximum value of $\boldsymbol{b}_t^T \boldsymbol{\mu}_{\tilde{a}_{t}^{*}}$, leading to pulling of arm $i$. According to~\cite{auer2002finite}, at least one of these events must occur for an pulling of arm $i$ to happen at time step $t$.

For event $\tilde{\Gamma}_t$, the $\tilde{A}_t^{\mathsf{c}}$ and $\tilde{B}_t^{\mathsf{c}}$ imply
\[
\boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t}
\quad \text{and} \quad
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} \geq \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}, \\
\]

indicating 
\[
\begin{aligned}
& \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + 2 \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}
\geq
\boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} \\
& \qquad \qquad \implies
2 \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
\geq
\boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i}.
\end{aligned}
\]

Combining above result and relaxing the first and second union sets in Eq.~\ref{eq: event_ABC_t_stat1} gives:
\begin{equation}
\begin{aligned}
&{\{a_t = i \neq \tilde{a}_{t}^{*}, N_{i,t} > \beta \}} \\
& \qquad \subset
\left\{ \boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} > \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right\} 
\cup
\left\{ \boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} < \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}} \right\} \\
& \qquad \qquad \cup
\left\{ \boldsymbol{b}_{t}^{T} (\boldsymbol{\mu}_{\tilde{a}_{t}^{*}}-\boldsymbol{\mu}_{i}) < 2 \Vert \boldsymbol{b}_{t} \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}, N_{i,t} > \beta \right\}  \\
& \qquad \subset
\underbrace{
\left\{ 
\underset{d \in \mathcal{D}^{+}_{T}}{\cup} \left\{ \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{i,t}(d) > \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{i}(d) + \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right\} \right\}}_{A_t} \\
& \qquad \qquad \cup
\underbrace{
\left \{
\underset{d \in \mathcal{D}^{+}_{T}}{\cup} \left\{ \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t}(d) < \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{\tilde{a}_{t}^{*}}(d) - \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}\right\}\right\}}_{B_t} \\
& \qquad \qquad \cup
\underbrace{
\left\{ \boldsymbol{b}_{t}^{T} (\boldsymbol{\mu}_{\tilde{a}_{t}^{*}}-\boldsymbol{\mu}_{i}) < 2 \Vert \hat{\boldsymbol{c}}_t \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}, N_{i,t} > \beta, \boldsymbol{b}_{t}^{T} \Delta_{i} > \eta_{i} - \epsilon\right\}}_{\Gamma_t},
\end{aligned}
\end{equation}

where $\mathcal{D}^{+}_{T} :=\left\{ d| [\boldsymbol{b}_{1}, \boldsymbol{b}_{2}, ..., \boldsymbol{b}_{T}](d) \in \mathcal{B}^{+}_{T} \right\}$, and $\mathcal{B}^{+}_{T}:= \{{[\boldsymbol{b}_{1}(d), \boldsymbol{b}_{2}(d), ..., \boldsymbol{b}_{T}(d)]} \neq \boldsymbol{0}, \forall d \in [D]\}$ is the collection set of non-zero $[\boldsymbol{b}(d)]^T$ sequence.

Then on event $A_t$, by applying Hoeffdingâ€™s Inequality (Lemma~\ref{lemma: Hoeffding}), for any $d \in [D]$, we have

\begin{equation}
\begin{aligned}
\mathbb{P} \left( \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{i,t}(d) > \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{i}(d) + \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}\right) 
& = 
\mathbb{P} \left( \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d) > \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right) \\
& \leq
\exp \left( \frac{-2 N_{i,t}^2 \log(t/\alpha) }{ N_{i,t} \sum_{\iota=1}^{N_{i,t}}(1-0)^2 } \right) \\
& =
\exp \left( -2 \log(t/\alpha) \right) 
=
\left( \frac{\alpha}{t} \right)^2,
\end{aligned}
\end{equation}

which yields the upper bound of $\mathbb{P} (A_t)$ as 
\begin{equation}
\begin{aligned}
\label{eq: P_A_t_stat}
\mathbb{P} (A_t) \leq \sum_{d \in \mathcal{D}^{+}_{T}}
\mathbb{P} \left( \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{i,t}(d) > \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{i}(d) + \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}\right) 
& \leq 
|\mathcal{B}^{+}_{T}| \left( \frac{\alpha}{t} \right)^2,
\end{aligned}
\end{equation}

and similarly,
\begin{equation}
\begin{aligned}
\label{eq: P_B_t_stat}
\mathbb{P} (B_t) \leq \sum_{d \in \mathcal{D}^{+}_{T}}
\mathbb{P} \left( \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t}(d) < \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{\tilde{a}_{t}^{*}}(d) - \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}} \right) 
& \leq 
|\mathcal{B}^{+}_{T}| \left( \frac{\alpha}{t} \right)^2.
\end{aligned}
\end{equation}

Next we investigate the event $\Gamma_t := \left\{ \boldsymbol{b}_{t}^{T} \Delta_i < 2 \Vert \boldsymbol{b}_{t} \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}, N_{i,t} > \beta \right\}$. 
Let $\beta = \frac{4 M^2 \log (T/\alpha)}{L_i^2}$. Since $N_{i,t} \geq \beta$ and recall that $\boldsymbol{b}_t^T (\boldsymbol{\mu}_{\tilde{a}^{*}_{t}} - \boldsymbol{\mu}_{i}) \geq L_i$, we have,

\begin{equation}
\begin{aligned}
2 \Vert \boldsymbol{b}_t \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
\leq
2 \Vert \boldsymbol{b}_t \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{\beta}}
\leq
2 M \sqrt{\frac{ \log(T/\alpha)}{\beta}} = L_i 
\leq 
\boldsymbol{b}_t^T (\boldsymbol{\mu}_{\tilde{a}^{*}_{t}} - \boldsymbol{\mu}_{i}), 
\end{aligned}
\end{equation}

implying that the event $\Gamma_t$ has $\mathbb{P}$-probability 0.
By combining Eq.~\ref{eq: term_1_stat} with Eq.~\ref{eq: event_ABC_t_stat1}, \ref{eq: P_A_t_stat} and \ref{eq: P_B_t_stat}, the expectation of LHS term in Eq.~\ref{eq: N_up_bd_stat} can be upper-bounded as follows:

\begin{equation}
\begin{aligned}
\label{eq: upbd_term_1_stat}
\mathbb{E} \left[\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i \} } \right]
& = 
\mathbb{E} \left[ \sum_{t=1}^{T} \mathds{1}_{\{a_t = i \neq \tilde{a}^{*}_{t} \}} \right] \\
& \leq 
\frac{4 M^2 \log (T/\alpha)}{L_i^2}
+
|\mathcal{B}^{+}_{T}| \alpha^2 \sum_{t=1}^{T} t^{-2}  \\
& \underset{(a)}{\leq}
\frac{4 M^2 \log (T/\alpha)}{L_i^2}
+
|\mathcal{B}^{+}_{T}| \frac{\pi^2 \alpha^2}{3},
\end{aligned}
\end{equation}

where (a) holds by the convergence of sum of reciprocals of squares that 
\begin{equation}
\label{eq: riemann_zeta}
\sum_{t=1}^{\infty} t^{-2} = \frac{\pi^2}{6}.
\end{equation}
This concludes the proof.
\end{proof}

\subsubsection{Proof of Lemma~\ref{lemma: error_distance}}
\label{sec: proof_lemma_error_distance}

\begin{proof}[Proof of Lemma~\ref{lemma: error_distance}]

Let $\phi_\epsilon$ be the set of solution such that $\boldsymbol{x}^T \Delta = \epsilon$,
$\phi_{\boldsymbol{\overline{c}}^T \Delta}$ be the solution set of $\boldsymbol{x}^T \Delta = \boldsymbol{\overline{c}}^T \Delta$,
i.e., 
\[
\begin{aligned}
\phi_{\epsilon} & :=\left\{ \boldsymbol{x} \mid \boldsymbol{x}^T \Delta = \epsilon\right\}\\
\phi_{\boldsymbol{\overline{c}}^T \Delta} &:=\left\{ \boldsymbol{x} \mid \boldsymbol{x}^T \Delta = \boldsymbol{\overline{c}}^T \Delta \right\},
\end{aligned}
\]

where $\phi_{\epsilon}$ and $\phi_{\boldsymbol{\overline{c}}^T \Delta}$ can be viewed as two hyperplanes share the same normal vector of $\Delta$. Let $\boldsymbol{\overline{c}}_{\phi_{\epsilon}}$ be the projection of vector $\boldsymbol{\overline{c}}$ on hyperplane $\phi_{\epsilon}$. Apparently, $\left( \boldsymbol{\overline{c}}_{\phi_{\epsilon}} - \boldsymbol{\overline{c}} \right) \perp \phi_{\epsilon}$, and thus we have
\begin{equation}
\begin{aligned}
\Vert \boldsymbol{\overline{c}}_{\phi_{\epsilon}} - \boldsymbol{\overline{c}} \Vert_2 = \frac{\boldsymbol{\overline{c}}^T \Delta}{\Vert \Delta \Vert_2}
-
\frac{\epsilon}{\Vert \Delta \Vert_2},
\end{aligned}
\end{equation}

which is also the distance between the parallel hyperplanes $\phi_{\epsilon}$ and $\phi_{ \boldsymbol{\overline{c}}^T \Delta }$.
By the principle of distance between points on parallel hyperplanes, we have for any $\boldsymbol{\hat{c}} \in \phi_{\epsilon}$, the distance between 
$\boldsymbol{\hat{c}}$ and $\boldsymbol{\overline{c}}$ is always greater than or equal to the shortest distance between the hyperplanes $\phi_{\epsilon}$ and $\phi_{\boldsymbol{\overline{c}}^T \Delta }$, i.e.,
\begin{equation}
\begin{aligned}
\Vert \boldsymbol{\hat{c}} - \boldsymbol{\overline{c}} \Vert_2
\geq
\Vert \boldsymbol{\overline{c}}_{\phi_{\epsilon}} - \boldsymbol{\overline{c}} \Vert_2 = \frac{\boldsymbol{\overline{c}}^T \Delta - \epsilon }{\Vert \Delta \Vert_2}
\end{aligned}
\end{equation}
\end{proof}


\subsection{Instance-Independent Regret}
\label{sec:app_unknown_inst_indep_bound}

We show that PRUCB-UP also enjoys an instance-independent regret bound.
Specifically, let $S := \{ i \mid 0<\eta_i\leq\eta\}$, we have
\[
\begin{aligned}
R(T) &= \sum_{i \in [K]} \eta_i \mathbb{E} [N_{a_i,T}] = \sum_{i \in S} \eta_i \mathbb{E} [N_{a_i,T}] + \sum_{i \in [K] \setminus S} \eta_i \mathbb{E} [N_{a_i,T}] \\
& \leq \eta\sum_{i \in S} \mathbb{E} [N_{a_i,T}] + \sum_{i \in [K] \setminus S} \eta_i \mathbb{E} [N_{a_i,T}] \\
& \leq \eta T + C_1\frac{K \delta^2 \log T}{\eta} + C_2 \frac{K D^2 \delta^2}{\eta} + C_3,
\end{aligned}
\]

where the last inequality is derived by substituting the second term ($\sum_{i \in [K] \setminus S} \eta_i \mathbb{E} [N_{a_i,T}]$) in the second line with the result in Theorem \ref{theorem:up_bd_stat}. 

By taking $\eta = \sqrt{ \frac{C_1 K \delta^2 \log T + C_2 K D^2 \delta^2}{T} } = O(\sqrt{\frac{K \delta^2 \log T}{T}})$, we have 
\[
R(T) = O( \delta \sqrt{K T \log T}),
\]
which matches the MAB minimax lower bound up to logarithmic factors.


\subsection{Known Preference as A Special Case }
\label{sec:app_up_bd_stat_known}

\begin{wrapfigure}{r}{0.45\textwidth} % Adjust the width as needed
\begin{minipage}{0.45\textwidth}
\begin{algorithm}[H]
\caption{Preference UCB with Known Preference}
\label{alg:PRUCB_KP}
\begin{algorithmic}
\State \textbf{Parameters:} $\alpha$.
\State \textbf{Initialization:} 
$N_{i, 1} \!\leftarrow\! 0$; $\boldsymbol{\hat{r}}_{i,1} \!\leftarrow\! [0]^{D}, \forall i \!\in\! [K]$.
\For{$t=1,\cdots,T$ }
    \State Receive user preference expectation $\boldsymbol{c}_t$
    \State $\hat{\boldsymbol{c}}_{t} \leftarrow \boldsymbol{c}_t$
    \State Perform one step of PRUCB-UP.
\EndFor
\end{algorithmic}
\end{algorithm}
 \end{minipage}
\end{wrapfigure}

In this section, we use a simple variant of Algorithm \ref{alg:PRUCB_UP} to solve PAMO-MAB with known preference case and show the regret upper-bound. 

Specifically, the known preference environment can be \emph{viewed as a special case} of unknown preference environment, where the preference estimation $\boldsymbol{\hat{c}}_t$ is exactly the user's preference expectation $\boldsymbol{\overline{c}}$ provided before hand. Hence, we can simply replace the estimation value $\boldsymbol{\hat{c}}_t$ in Algorithm \ref{alg:PRUCB_UP} with $\overline{\boldsymbol{c}}$ we obtained in advance. We present this variant in Algorithm \ref{alg:PRUCB_KP}.


Define $\tilde{a}_t = \argmaxA_{k \in [K]} \boldsymbol{c}_t^{\top} \boldsymbol{\mu}_k$ and $\mathcal{T}_{i} = \{ t \in [T] \mid \tilde{a}_t \neq i \}$ be the set of episodes when $i$ serving as a suboptimal arm conditioned on the given preference $\boldsymbol{c}_t$ over $T$ horizon.
Let 
$\tilde{\Delta}_{i,t} = \mu_{\tilde{a}_t} - \mu_{i} \in \mathbb{R}^D, \forall t \in [1,T]$ 
be the gap of expected rewards between arm $i$ and best $\boldsymbol{c}_t$ conditioned arm $\tilde{a}_t$ at time step $t$, 
$\eta_{i}^{\downarrow} = \min_{t \in \mathcal{T}_{i}}\{\boldsymbol{c}_{t}^{T} \tilde{\Delta}_{i,t} \}$ and 
$\eta_{i}^{\uparrow} = \max_{t \in \mathcal{T}_{i}}\{\boldsymbol{c}_{t}^{T} \tilde{\Delta}_{i,t} \}$ refer to the lower and upper bounds of the expected overall-reward gap between $i$ and $a_t^*$ over $T$ when $i$ serving as a suboptimal arm.
For preference known case with Algorithm \ref{alg:PRUCB_KP}, the following corollary of Theorem \ref{theorem:up_bd_stat} characterizes the performance. 

\begin{corollary}
Assume $\overline{\boldsymbol{c}}$ is given before decision making, let $\eta_i = \boldsymbol{\overline{c}} \Delta_i$, $\Delta_i = \boldsymbol{\mu_{a^*}} - \boldsymbol{\mu_{i}}$, Algorithm \ref{alg:PRUCB_KP} has 
\[
R(T) 
=
O
\Bigg(
\sum_{i \neq a^{*}}
\Big(
\frac{\delta^2 \eta_{i}^{\uparrow} \log T}{{\eta_{i}^{\downarrow}}^2} 
+
D \pi^2 \alpha^2 \eta_{i}^{\uparrow}
\Big)
\Bigg).
\]
\end{corollary}

\begin{proof}
The proof follows the same path of Theorem \ref{theorem:up_bd_stat} but with some slight modifications.
Let $\tilde{N}_{i,T}$ denotes the number of times that arm $i$ is played as a $\boldsymbol{c}_t$ conditioned suboptimal arm, i.e.,
$\tilde{N}_{i,T} = \sum_{t \in \mathcal{T}_i} \mathds{1}_{\{a_t = i \neq \tilde{a}_t\}}$.

Then we can apply Proposition \ref{prop: N_known_changing} on $\tilde{N}_{i,T}$ for analysis. 
Specifically, by directly substituting $\boldsymbol{b}_t$ with $\boldsymbol{c}_t$, the policy of $a_t$ aligns with that of Algorithm \ref{alg:PRUCB_KP}, and it is easy to verify that $\mathcal{M}_i = \mathcal{T}_i$, $L_i = \eta_{i}^{\downarrow}$. And thus by Proposition \ref{prop: N_known_changing}, we have

\[
\mathbb{E} [\tilde{N}_{i,T}] 
=
\mathbb{E} \left[ \sum_{t \in \mathcal{T}_i} \mathds{1}_{\{a_t = i \} } \right] 
\leq
\frac{4 \delta^2 \log{(\frac{T}{\alpha})}}{\eta_{i}^{\downarrow 2}}
+
\frac{D \pi^2 \alpha^2} {3}.
\]
Thus we have 
\[
\begin{aligned}
R(T) & = \sum_{t\in[T]} \max_{i\in[K]}\mathbb{E}[\boldsymbol{c}_t^{\top} (\boldsymbol{r}_i - \boldsymbol{r}_{a_t} ) ]
\leq 
\sum_{t\in[T]} \mathbb{E}[\max_{i\in[K]} \boldsymbol{c}_t^{\top} (\boldsymbol{r}_i - \boldsymbol{r}_{a_t} ) ]
=
\sum_{i \in [K]} \sum_{t \in \mathcal{T}_{i}}
\mathbb{E} \left[
\mathds{1}_{\{a_t = i \neq \tilde{a}_t\}} \boldsymbol{c}_{t}^{T} \tilde{\Delta}_{i,t}
\right] \\
& \leq
\sum_{i \in [K]} 
\mathbb{E} [\tilde{N}_{i,T}] \eta_{i}^{\uparrow}
\leq
\sum_{i \in [K]} 
\eta_{i}^{\uparrow} 
( \frac{4 \delta^2 \log{(\frac{T}{\alpha})}}{\eta_{i}^{\downarrow 2}}
+
\frac{D \pi^2 \alpha^2} {3} ).
\end{aligned}
\]





% Specifically, since in Algorithm \ref{alg:PRUCB_KP}, we replace the preference estimation $\boldsymbol{\hat{c}}_T$ with known preference expectation $\boldsymbol{\overline{c}}$, for Eq. (\ref{eq: term_2_set_prob_stat}), for any $\epsilon > 0$ we have 
% \[
% \mathbb{P}_{\epsilon} \left( a_t = i \neq a^*, \hat{\boldsymbol{c}}_{t}^{T} \mu_{a^{*}} \leq \hat{\boldsymbol{c}}_{t}^{T} \mu_{i} + \eta_i - \epsilon \right)
% \leq
% \mathbb{P}_{\epsilon} \left( \Vert \overline{\boldsymbol{c}} - \overline{\boldsymbol{c}} \Vert_2 \geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } \right)
% =
% \mathbb{P}_{\epsilon} \left( 0 \geq \frac{\epsilon}{ \Vert \Delta_{i} \Vert_2 } \right)
% = 0.
% \]

% Hence the Eq. (\ref{eq: E_N_up_bd_stat}) can be rewritten as 
% \[
% \mathbb{E}_{\epsilon} [N_{i,T}]
% \leq
% \frac{4 \delta^2 \log (T/\alpha)}{(\eta_{i} - \epsilon)^2}
% +
% \frac{D \pi^2 \alpha^2}{3}.
% \]

% By setting $\epsilon$ to be a sufficiently small positive number, we have 
% $
% \mathbb{E}_{\epsilon} [N_{i,T}]
% \leq
% \frac{4 \delta^2 \log (T/\alpha)}{\eta_{i}^2}
% +
% \frac{D \pi^2 \alpha^2}{3}.$
% Multiplying the results by the expected overall-reward gap $\eta_i$ and summing all suboptimal arms $i \neq a^*$ concludes the proof.

\end{proof}


