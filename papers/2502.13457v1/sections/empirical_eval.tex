\section{Experiments}
\label{sec:app_exp}

In this section, we conduct numerical experiments to evaluate the effectiveness of our proposed algorithms under different user preference environments.

\subsection{Experiments in Unknown Preference Environment}
\label{sec:app_exp_prucb_static}

In this section, we verify the capability of PRUCB and PRUCB-UP to model user preference $\boldsymbol{c}_t$ and optimize the overall reward in a stationary preference environment. 
We compare these two algorithms in terms of regret with the following multi-objective bandits algorithms.
\vspace{-5pt}
\begin{itemize}[leftmargin=*]
\item 
S-UCB~\cite{drugan2013designing}:
the scalarized UCB algorithm, which scalarizes the multi-dimensional reward by assigning weights to each objective and then employs the single objective UCB algorithm \cite{auer2002finite}. Throughout the experiments, we assign each objective with equal weight. 
\item 
S-MOSS:
the scalarized UCB algorithm, which follows the similar way with S-UCB by scalarizing the multi-dimensional reward into a single one, but uses MOSS \cite{audibert2009minimax} policy for arm selection.
\item 
Pareto-UCB~\cite{drugan2013designing}:
the Pareto-based algorithm, which compares different arms by the upper confidence bounds of their expected multi-dimensional reward by Pareto order and pulls an arm uniformly from the approximate Pareto front.
\item 
Pareto-TS~\cite{yahyaa2015thompson}:
the Pareto-based algorithm, which makes use of the Thompson sampling technique to estimate the expected reward for every arm and selects an arm uniformly at random from the estimated Pareto front.
\item 
GT:
The PRUCB-UP variant with known preference as a ground truth, where we replace the preference estimation in Algorithm \ref{alg:PRUCB_UP} with the true preference estimation.
\end{itemize}


\textbf{Experimental settings.}
For evaluation, we use a synthetic dataset. Specifically, we consider the MO-MAB with $K$ arms, each arm $i \in [K]$ associated with a $D$-dimensional reward, where the reward of each objective $d$ follows a Gaussian distribution with a randomized mean $\boldsymbol{\mu}_i(d) \in [0,1]$ and variance of 0.01.
For user preference, we consider two settings including predefined preference and randomized preference. For predefined preference-aware structure, we define the mean preference $\boldsymbol{\overline{c}}$ as 
% $ \boldsymbol{\overline{c}}(d) = 
% \begin{cases}
% 2.0 & \text{ if } d=j\\
% 0.5 & \text{ otherwise}
% \end{cases}$
$ \boldsymbol{\overline{c}}(d) = 
2.0 \text{ if } d=j; 0.5 \text{ otherwise}
$, 
where $j \in [D]$ is randomly selected. The practical implication of this structure is that it represents a common scenario in which the user exhibits a markedly higher preference for one particular objective while showing little interest in others.
For randomized preference, the values of mean preference $\boldsymbol{\overline{c}}$ are randomly defined within $[0,5]$. For both setups, the instantaneous preference is generated under Gaussian distributions with corresponding means and variance of 0.5. 
% To guarantee the non-negative preference, we clip the generated instantaneous preference within $[0, 2\boldsymbol{\overline{c}}]$. 

\textbf{Implementations.}
For the implementations of the algorithms, following previous studies~\cite{auer2002finite, audibert2007tuning}, we set $\alpha=1$.
The time horizon is set to $T = 5000$ rounds, and we repeat 10 trials for each set of evaluation due to the randomness from both environment and algorithms.

\begin{figure*}[ht]
    \centering    
    \includegraphics[width=1\textwidth]{figures/expriments_1.pdf}
    \caption{Regrets of different algorithms under unknown preference environment. 
}
\label{fig: experiments_1}
\end{figure*}


\textbf{Results.}
We report the averaged regret performance of the algorithms in Fig.~\ref{fig: experiments_1}.
It is evident that our algorithm significantly outperform other competitors in all experiments. This is expected since the competing algorithms are designed for Pareto-optimality identification and do not utilize the preference structure of users considered in this paper, which our algorithm explicitly exploits.
Additionally, from the zoom-in window, we observe that PRUCB-UP exhibits only a very slight performance degradation compared to GT, which knows the preference expectation in advance. This indicates that the proposed PRUCB-UP can effectively model user preference via empirical estimation in stationary preference environments.






\subsection{Experiments in Hidden Preferences Environment}
\label{sec:app_exp_hidden}
In this section, we evaluate the performance of PRUCB-HP in modeling user preference \(\boldsymbol{c}_t\) and optimizing the overall reward when explicit user preference is not visible, but overall reward $g_{a_t,t}$ and reward $\boldsymbol{r}_{a_t, t}$ are revealed after each episode. 

\textbf{Experimental protocol.}
Given that PRUCB-HP models both the expected arms reward and user preference, we designed a new \emph{user-switching protocol} for evaluation. Figure \ref{fig:experiment3_protocol} illustrates this protocol with 3 users and 9 arms. Specifically, at each episode, one user is exposed to a block of arms (3 in our illustration). Only the arms within this block can be selected for this user. After one arm has been pulled, the system observes the reward $\boldsymbol{r}_{a_t, t}$ and user’s overall ratings $g_{a_t,t}$ corresponding to the pulled arm $a_t$. In the next episode, the arm block rotates to another user. The goal is to maximize the cumulative overall ratings from all users.

This protocol simulates real-world applications, such as recommender systems, where empirical multi-objective rewards (ratings) of arms (recommendation candidates) are obtained from a diverse set of users rather than a single fixed user. Additionally, users are not always exposed to a fixed set of arms (recommendation candidates). This user-switching protocol allows us to evaluate the algorithm’s ability to model arm reward and user preference, thus enabling the customized optimization of users’ overall ratings.
In Figure \ref{fig:experiment3_protocol}(b), we present an intuitive example of the protocol in the context of real-world hotel recommendations. Specifically, the blocks represent different cities (e.g., NYC, LA, CHI), and the hotel candidates within these cities correspond to the arms within the blocks. At each time step, a customer travels to a city, stays in a hotel recommended by the system, and leaves feedback (both objective and overall ratings) after her or his stay. In the next episode, the customer travels to a different city and encounters a new set of hotel options. The hotel recommender system needs to learn the multi-objective rewards of all hotel candidates from various customers and model each customer’s preference based on their multi-objective and overall feedback. This enables the system to customize optimal hotel recommendations tailored to individual user preference.

\begin{figure*}[t]
    \centering    
    \includegraphics[width=1\textwidth]{figures/expriment_protocol.pdf}
    \caption{(a) Users switching protocol for experimental evaluation of hidden preference and multi-objective reward modelings. (b) One real-world example of the experimental protocol.
}
    \label{fig:experiment3_protocol}
\end{figure*}

\begin{figure*}[t]
    \centering    
    \includegraphics[width=1\textwidth]{figures/expriments_3.pdf}
    \caption{Regrets of different algorithms under hidden preference environment.
}
    \label{fig:experiment_3}
\end{figure*}


\begin{figure*}[t]
    \centering    
    \includegraphics[width=1\textwidth]{figures/lr_exp_2.pdf}
    \caption{A simple 2D hidden preference PAMO-MAB for preference estimation comparison between standard LR estimator and our tailored WLS-estimator under different samples.}
    \label{fig:lr_exp_2}
\end{figure*}

\textbf{Baselines.}
For performance comparison, we choose the MO-MAB baselines used in unknown environment (Appendix \ref{sec:app_exp_prucb_static}, including S-UCB~\cite{drugan2013designing}, S-MOSS, Pareto-UCB~\cite{drugan2013designing} and Pareto-TS~\cite{yahyaa2015thompson}). 
Besides, we develop a variant of OFUL \cite{abbasi2011improved}, a widely used linear bandit benchmark, for comparison. This variant estimates user preferences using ridge regression based on reward and overall reward feedback, replaces the input feature with the empirical reward estimate, and applies an $\epsilon = 0.05$ rate for exploration.
Additionally, note that the scale overall score is also provided, it is feasible to use standard MAB methods by leveraging historical overall rewards for optimization. Hence we also choose classic MAB algorithms including UCB~\cite{auer2002finite} and MOSS~\cite{audibert2009minimax} for comparison.

\textbf{Experimental settings.}
In our experiment, we set $N$ users and $3N$ arms in total, and each arm associates with $D$-dimensional reward. The generations of instantaneous reward $\boldsymbol{r}_{i,t}$ of arms and user preference $\boldsymbol{c}_{t}$ follow the same settings as unknown environment.  

For user-switching protocol, we set $N$ blocks in total, with each block containing 3 fixed arms. At each episode, each user will be randomly assigned one block without replacement. The learner can only select the arm within assigned block for each user. 

\textbf{Implementation.}
Similarly, we set $\alpha = 1$ in PRUCB-HP. For regularization coefficient, we set $\lambda = 1$. For confidence radius, we set $\beta_t = 0.1\sqrt{D \log(t)}$. For weight scalar, we set $\omega = D$.
We perform 10 trials up to round $T = 5000$ for each set of evaluation.

\textbf{Results.}
We report average performance of the algorithms in Fig.~\ref{fig:experiment_3}. 
As shown, our proposed PRUCB-HP achieves superior results in terms of regret under all experimental settings compared to other competitors. This empirical evidence suggests that modeling user preference and leveraging this information for arm selection significantly enhances the performance of customized bandits optimization.



\subsubsection{Analysis on WLS Estimator}
In this section, we investigate the effectiveness of the proposed WLS-preference estimator with the carefully designed weight $w_t$. 
Specifically, we consider the 2D PAMO-MAB toy instance shown in Section \ref{sec:uniq_chllenge}. This instance contains two arms: Arm-1 with dominated mean reward $[0.2,0.2]^{\top}$, and Arm-2 with Pareto-optimal mean reward $[0.8,0.8]^{\top}$. The preference $\boldsymbol{c}_t$ at each step follows a Gaussian distribution with a mean of $[0.5, 0.5]$ and variance of $0.05$ for each objective dimension. 

We compare the preference estimation performance with the standard linear regression model under different sample numbers, i.e., 20, 50, 80, 200 (half for each arm).
The results are shown in Figure \ref{fig:lr_exp_2}. Notably, our proposed WLS-preference estimator consistently achieves better preference estimation performance (lower error to the ground-truth $[0.5, 0.5]$) than standard linear regression, verifying the effectiveness of our method for preference learning in hidden preference PAMO-MAB problem.

