\section{Analyses for Section \ref{sec: knonw} (Known Preference)}

% \subsection{Proof of Theorem \ref{thm: knonwn1} (Stationary Preferences Distributions)}
% \label{sec: pf_knwon1}

% This proof follows the corresponding proof from \citet{auer2002finite}.
% We begin with stating a useful central bound below.
% \begin{lemma}[Hoeffding’s inequality for general bounded random variables~\citep{vershynin2018high} (Theorem
% 2.2.6)]
% \label{lemma: Hoeffding}
%  Given independent random variables $\{X_1, ..., X_m \}$ where $a_i \leq X_i \leq b_i$ almost surely (with probability 1) we have:
% \[
% \mathbb{P} \left(\frac{1}{m} \sum_{i=1}^{m} X_i - \frac{1}{m} \sum_{i=1}^{m} \mathbb{E}[X_i] \geq \epsilon \right) \leq \exp \left(\frac{-2 \epsilon^2 m^2}{\sum_{i=1}^{m} (b_i-a_i)^2} \right).
% \]
% \end{lemma}

% \begin{proof}[Proof of Theorem \ref{thm: knonwn1}]
% By the regret definition in Eq. \ref{eq: regret_def}, for the known constant objective preferences $\boldsymbol{c} \in \mathbb{R}^D$, we have 

% \begin{equation}
% \label{eq: regret_def_known}
% \begin{aligned}
% R(T) 
% &=
% \sum^{T}_{t=1} \left(\mathbb{E}[f(\boldsymbol{c}_t, \boldsymbol{r}_{a^{*}_t, t})] - \mathbb{E}[f(\boldsymbol{c}_t, \boldsymbol{r}_{a_t, t})] \right)
% =
% \sum_{i \neq a^*} \boldsymbol{c}^T \Delta_i \mathbb{E}[N_{i,T}],
% \end{aligned}
% \end{equation}
% where $\Delta_i = \boldsymbol{\mu}_{a^*} - \boldsymbol{\mu}_{i} \in \mathbb{R}^D$.

% By the analysis in \citet{auer2002finite}, each suboptimal arm $i$ will only be played at time step $t$ if its upper confidence bound exceeds that of best arm $a^*$, i.e., 

% \begin{equation}
% \label{eq: suboptimal_pull_known}
% \begin{aligned}
% \boldsymbol{c}^T \left(  \hat{\boldsymbol{r}}_{i,t}+\sqrt{\frac{\log(t/\alpha)}{N_{i, t}}} \boldsymbol{e} \right)
% \geq 
% \boldsymbol{c}^T \left(  \hat{\boldsymbol{r}}_{a^*,t}+\sqrt{\frac{\log(t/\alpha)}{N_{a^*, t}}} \boldsymbol{e} \right).
% \end{aligned}
% \end{equation}

% % We begin by bounding the probability of pulling a suboptimal arm due to insufficient sampling.

% Let 
% $A_t:= \{\hat{\boldsymbol{r}}_{i,t}(d) \leq \boldsymbol{\mu}_i(d) + \sqrt{\frac{\log(t/\alpha)}{N_{i, t}}}, \forall d \in [D] \} $,
% $B_t:= \{\hat{\boldsymbol{r}}_{a^*,t}(d) \geq \boldsymbol{\mu}_{a^*}(d) - \sqrt{\frac{\log(t/\alpha)}{N_{a^*, t}}},  \forall d \in [D] \} $. 
% Suppose that both $A_t$ and $B_t$ both hold. Due to the non-negativity of $\boldsymbol{c}$, by $A_t$ and $B_t$, we can easily obtain 
% \begin{equation}
% \label{eq: A_t_known}
% \begin{aligned}
% \boldsymbol{c}^T \hat{\boldsymbol{r}}_{i,t} \leq \boldsymbol{c}^T \boldsymbol{\mu}_i + \boldsymbol{c}^T \sqrt{\frac{\log(t/\alpha)}{N_{i, t}}} \boldsymbol{e},
% \end{aligned}
% \end{equation}

% \begin{equation}
% \label{eq: B_t_known}
% \begin{aligned}
% \boldsymbol{c}^T \hat{\boldsymbol{r}}_{a^*,t} 
% \geq
% \boldsymbol{c}^T \boldsymbol{\mu}_{a^*} - \boldsymbol{c}^T \sqrt{\frac{\log(t/\alpha)}{N_{a^*, t}}} \boldsymbol{e}. 
% \end{aligned}
% \end{equation}


% Chaining Eq.~\ref{eq: suboptimal_pull_known}, Eq.~\ref{eq: A_t_known} and Eq.~\ref{eq: B_t_known}, and rearranging results yiled

% \begin{equation}
% \label{eq: A_t_B_t_known}
% \begin{aligned}
% & \quad \quad \boldsymbol{c}^T \boldsymbol{\mu}_i + 2 \boldsymbol{c}^T \sqrt{\frac{\log(t/\alpha)}{N_{i, t}}} \boldsymbol{e}
% \geq
% \boldsymbol{c}^T \boldsymbol{\mu}_{a^*} \\
% & \implies 
% 2 \sqrt{\frac{\log(t/\alpha)}{N_{i, t}}} \boldsymbol{c}^T \boldsymbol{e}
% \geq
% \boldsymbol{c}^T ( \boldsymbol{\mu}_{a^*} - \boldsymbol{\mu}_i )
% =
% \boldsymbol{c}^T \Delta_i \\
% & \implies 
% 2 \sqrt{\frac{\log(t/\alpha)}{N_{i, t}}} \Vert \boldsymbol{c} \Vert_1
% \geq
% \boldsymbol{c}^T \Delta_i \\
% & \implies 
% \frac{ 4 \Vert \boldsymbol{c} \Vert_1^2 \log(t/\alpha) }{ (\boldsymbol{c}^T \Delta_i)^2 }
% \geq
% N_{i,t}.
% \end{aligned}
% \end{equation}


% Then we analyze the case when $A_t$ or $B_t$ fails. We bound the probabilities of the complements of events $A_t$ and $B_t$ occurring by applying Lemma \ref{lemma: Hoeffding} (Hoeffding’s inequality). Specifically, $A_t$ fails when $\exists d \in [D]$ such that $\hat{\boldsymbol{r}}_{i,t}(d) \leq \boldsymbol{\mu}_i(d) + \sqrt{\frac{\log(t/\alpha)}{N_{i, t}}}$.
% Let $\mathcal{C}^{+}:= \{\boldsymbol{\overline{c}}(d) \neq 0, \forall d \in [D]\}$ be the set of positive preferences entries. 
% By Lemma \ref{lemma: Hoeffding} and union bound, we have 

% \begin{equation}
% \label{eq: A_complement_known}
% \begin{aligned}
% \mathbb{P} (A^\mathsf{c}) 
% & = 
% \mathbb{P} \left( \exists d,  \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d)  > \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right) \\
% & \leq
% \sum_{d \in \mathcal{C}^{+}}
% \mathbb{P} \left( \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d) > \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right) \\
% & \leq
% \sum_{d \in \mathcal{C}^{+}}
% \exp \left( \frac{-2 N_{i,t}^2 \log(t/\alpha) }{ N_{i,t} \sum_{\iota=1}^{N_{i,t}}(1-0)^2 } \right) \\
% & =
% |\mathcal{C}^{+}| \exp \left( -2 \log(t/\alpha) \right) 
% =
% |\mathcal{C}^{+}| \left( \frac{\alpha}{t} \right)^2. \\
% \end{aligned}
% \end{equation}

% Similarly, we have
% \begin{equation}
% \label{eq: B_complement_known}
% \begin{aligned}
% \mathbb{P} (B^\mathsf{c}) 
% & \leq
% |\mathcal{C}^{+}| \left( \frac{\alpha}{t} \right)^2. \\
% \end{aligned}
% \end{equation}

% Combining Eq.~\ref{eq: A_t_B_t_known}, Eq.~\ref{eq: A_complement_known} and Eq.~\ref{eq: B_complement_known}, 
% we have 

% \begin{equation}
% \label{eq: N_known}
% \begin{aligned}
% \mathbb{E}[N_{i,T}]
% & =
% \sum_{t=1}^{T} \mathbb{E}[\mathds{1}_{\{a_t = i\}}] \\
% & \leq
% \frac{ 4 \Vert \boldsymbol{c} \Vert_1^2 \log(T/\alpha) }{ (\boldsymbol{c}^T \Delta_i)^2 } + 
% \sum_{t=1}^{T} \mathbb{E}[\mathds{1}_{\{ A^\mathsf{c} \cup B^\mathsf{c} \}}]  \\
% & \leq
% \frac{ 4 \Vert \boldsymbol{c} \Vert_1^2 \log(T/\alpha) }{ (\boldsymbol{c}^T \Delta_i)^2 } + 
% 2 |\mathcal{C}^{+}| \sum_{t=1}^{T} \left( \frac{\alpha}{t} \right)^2 \\
% & \underset{(a)}{\leq}
% \frac{ 4 \Vert \boldsymbol{c} \Vert_1^2 \log(T/\alpha) }{ (\boldsymbol{c}^T \Delta_i)^2 } + 
% |\mathcal{C}^{+}| \frac{\pi^2 \alpha^2}{3}, 
% \end{aligned}
% \end{equation}

% % where (a) holds since the convergence of Riemann zeta function that 
% % $\sum_{t=1}^{\infty} t^{-4} = \frac{\pi^4}{90}$.
% where (a) holds by the convergence of sum of reciprocals of squares that 
% $\sum_{t=1}^{\infty} t^{-2} = \frac{\pi^2}{6}$.
% Plugging above result into Eq. \ref{eq: regret_def_known} concludes the proof.


% \end{proof}




\subsection{Proof of Theorem \ref{thm: up_bd_known}}
\label{sec:app_pf_known_change}

For analyzing PRUCB's behaviours in the environment where the preference distribution is possibly dynamic, the main difficulty lies in tracking the potentially changes of the best arm.
Specifically, in preference changing environments, the optimal arm is not fixed any more and would change with the changing preference distributions. 

We begin with a more general upper bound (Proposition \ref{prop: N_known_changing}) for the learner's behavior using a policy that optimizes the inner product between the reward upper confidence bound (UCB) of arms and an arbitrary dynamic vector $\boldsymbol{b}_t$. It demonstrates that after a sufficiently large number of samples (on the order of $\mathcal{O}(\log T)$) for each arm $i$, for the episodes where the inner product of its rewards expectations with $\boldsymbol{b}_t$ is not highest, the expected number of times arm $i$ is pulled can be well controlled by a constant.
The proof of Proposition \ref{prop: N_known_changing} is provided in Appendix~\ref{sec: app_pr_prop_N_known_changing}.



\begin{proposition}
\label{prop: N_known_changing}
Let $\boldsymbol{b}_t \in \mathbb{R}^D$ be an arbitrary bounded vector at time step $t$ with $\Vert \boldsymbol{b}_t \Vert_1 \leq M$, define $\mathcal{M}_i := \{ t \in [T] \mid i \neq \argmaxA_{j \in [K]} \boldsymbol{b}_t^T \boldsymbol{\mu}_j \}, \forall i \in [K]$. For the policy of $a_t = \argmaxA \Phi ( \boldsymbol{b}_t, \hat{\boldsymbol{r}}_{i,t}+\sqrt{\frac{\log(t/\alpha)}{\max \{1, N_{i, t } \} }} \boldsymbol{e})$, for any arm $i \in [K]$, any subset $\mathcal{M}^{o}_i \subset \mathcal{M}_i$, we have
\[
\mathbb{E} \left[ \sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i \} } \right] 
\leq
\frac{4 M^2 \log{(\frac{T}{\alpha})}}{L_i^2}
+
\frac{ |\mathcal{B}^{+}_{T}| \pi^2 \alpha^2} {3},
\]
where $L_i = \min_{t \in \mathcal{M}^{o}_i} \{ \max_{j \in [K] \setminus i } \{\boldsymbol{b}_t^T (\boldsymbol{\boldsymbol{\mu}_j - \boldsymbol{\mu}_{i}
} )\} \}$, $\mathcal{B}^{+}_{T} := \{{[\boldsymbol{b}_{1}(d), \boldsymbol{b}_{2}(d), ..., \boldsymbol{b}_{T}(d)]} \neq \boldsymbol{0}, \forall d \in [D]\}$ is the collection set of non-zero $[\boldsymbol{b}(d)]^T$ sequence. 
\end{proposition}

\begin{proof}[Proof of Theorem \ref{thm: up_bd_known}]

Define $\mathcal{T}_{i} = \{ t \in [T] | a^*_t \neq i \}$ be the set of episodes when $i$ serving as a suboptimal arm over $T$.
Let 
$\Delta_{i,t} = \mu_{a_{t}^{*}} - \mu_{i} \in \mathbb{R}^D, \forall t \in [1,T]$ 
be the gap of expected rewards between suboptimal arm $i$ and best arm $a_t^*$ at time step $t$, 
$\eta_{i}^{\downarrow} = \min_{t \in \mathcal{T}_{i}}\{\boldsymbol{\overline{c}}_{t}^{T} \Delta_{i,t} \}$ and 
$\eta_{i}^{\uparrow} = \max_{t \in \mathcal{T}_{i}}\{\boldsymbol{\overline{c}}_{t}^{T} \Delta_{i,t} \}$ refer to the lower and upper bounds of the expected overall-reward gap between $i$ and $a_t^*$ over $T$ when $i$ serving as a suboptimal arm.
Let $\tilde{N}_{i,T}$ denotes the number of times that arm $i$ is played as a suboptimal arm, i.e.,
\[
\begin{aligned}
\tilde{N}_{i,T} &= \sum_{t=1}^{T} \mathds{1}_{\{a_t = i \neq a^*_t\}}.
\end{aligned}
\]

Then we can apply Proposition \ref{prop: N_known_changing} on $\tilde{N}_{i,T}$ for analysis. 
Specifically, by directly substituting $\boldsymbol{b}_t$ with $\boldsymbol{\overline{c}}_t$, the policy of $a_t$ aligns with that of PRUCB, and it is easy to verify that $\mathcal{M}_i = \mathcal{T}_i$, $L_i = \eta_{i}^{\downarrow}$. And thus by Proposition \ref{prop: N_known_changing}, we have

\[
\mathbb{E} [\tilde{N}_{i,T}] 
=
\mathbb{E} \left[ \sum_{t \in \mathcal{T}_i} \mathds{1}_{\{a_t = i \} } \right] 
\leq
\frac{4 \delta^2 \log{(\frac{T}{\alpha})}}{\eta_{i}^{\downarrow 2}}
+
\frac{|\mathcal{C}^{+}_{T}| \pi^2 \alpha^2} {3},
\]

where $\mathcal{\overline{C}}^{+}_{T}:= \{ d \in [D] \mid {[\boldsymbol{\overline{c}}_{1}(d), \boldsymbol{\overline{c}}_{2}(d), ..., \boldsymbol{\overline{c}}_{T}(d)]} \neq [0]^T \}$ is the set of non-zero expected preference sequence on each dimension (objective).
By multiplying above result with the corresponding upper-bound of expected gap $\eta_{i}^{\uparrow}$ and sum over $K$ arms concludes the proof of Theorem \ref{thm: up_bd_known}.
\end{proof} 


\subsubsection{Proof of Proposition \ref{prop: N_known_changing}}
\label{sec: app_pr_prop_N_known_changing}

We begin with stating a useful central bound below.
\begin{lemma}[Hoeffding’s inequality for general bounded random variables~\citep{vershynin2018high} (Theorem
2.2.6)]
\label{lemma: Hoeffding}
 Given independent random variables $\{X_1, ..., X_m \}$ where $a_i \leq X_i \leq b_i$ almost surely (with probability 1) we have:
\[
\mathbb{P} \left(\frac{1}{m} \sum_{i=1}^{m} X_i - \frac{1}{m} \sum_{i=1}^{m} \mathbb{E}[X_i] \geq \epsilon \right) \leq \exp \left(\frac{-2 \epsilon^2 m^2}{\sum_{i=1}^{m} (b_i-a_i)^2} \right).
\]
\end{lemma}

\begin{proof}[Proof of Proposition \ref{prop: N_known_changing}]

Define $\tilde{a}_{t}^{*} = \argmaxA_{j \in [K]} \boldsymbol{b}_t^T \boldsymbol{\mu}_j, \forall t \in (0,T]$, for any $\beta \in (0, T]$, we have

\begin{equation}
\begin{aligned}
\label{eq: term_1_stat}
\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i \} }
& \leq 
\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i, N_{i,t} \leq \beta \}}
+
\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i, N_{i,t} > \beta \}} \\
& \leq 
\beta
+
\sum_{t \in [T]} \mathds{1}_{\{a_t = i \neq \tilde{a}_{t}^{*}, N_{i,t} > \beta \}}.
\end{aligned}
\end{equation}



where the first term refers to the event of insufficient sampling (quantified by $\beta$) of arm $i$. 
, then for the event of second term, we have 
\begin{equation}
\begin{aligned}
\label{eq: event_ABC_t_stat1}
&{\{a_t = i \neq \tilde{a}_{t}^{*}, N_{i,t} > \beta \}} \\
& \subset
\Bigg\{ 
\underbrace{
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} > \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
}_{\tilde{A}_t}, 
N_{i,t} > \beta \Bigg\} \\
& \qquad \qquad \cup
\Bigg\{ 
\underbrace{
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} < \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}
}_{\tilde{B}_t}, 
N_{i,t} > \beta \Bigg\} \\
& \qquad \qquad \cup
\Bigg\{ 
\underbrace{
\tilde{A}_t^{\mathsf{c}}, \tilde{B}_t^{\mathsf{c}}, 
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}} , N_{i,t} > \beta }_{\tilde{\Gamma}_t} \Bigg\}.  \\
\end{aligned}
\end{equation}

Specifically, $\tilde{A}_t$ and $\tilde{B}_t$ denote the events where the constructed upper confidence bounds (UCBs) for arm $i$ or the optimal arm $a^{}$ fail to accurately bound their true expected rewards, indicating imprecise rewards estimation. Meanwhile, $\tilde{\Gamma}_t$ represents the event where the UCBs for both arms effectively bound their expected rewards, yet the UCB of arm $i$ still exceeds that of the arm $\tilde{a}_{t}^{*}$ though it yields the maximum value of $\boldsymbol{b}_t^T \boldsymbol{\mu}_{\tilde{a}_{t}^{*}}$, leading to pulling of arm $i$. According to~\citep{auer2002finite}, at least one of these events must occur for an pulling of arm $i$ to happen at time step $t$.

For event $\tilde{\Gamma}_t$, the $\tilde{A}_t^{\mathsf{c}}$ and $\tilde{B}_t^{\mathsf{c}}$ imply
\[
\boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t}
\quad \text{and} \quad
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} \geq \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}, \\
\]

indicating 
\[
\begin{aligned}
& \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + 2 \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
\geq
\boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}
\geq
\boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} \\
& \qquad \qquad \implies
2 \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} 
\geq
\boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i}.
\end{aligned}
\]

Combining above result and relaxing the first and second union sets in Eq.~\ref{eq: event_ABC_t_stat1} gives:
\begin{equation}
\begin{aligned}
&{\{a_t = i \neq \tilde{a}_{t}^{*}, N_{i,t} > \beta \}} \\
& \qquad \subset
\left\{ \boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{i,t} > \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{i} + \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right\} 
\cup
\left\{ \boldsymbol{b}_{t}^{T} \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t} < \boldsymbol{b}_{t}^{T} \boldsymbol{\mu}_{\tilde{a}_{t}^{*}} - \boldsymbol{b}_{t}^{T} \boldsymbol{e} \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}} \right\} \\
& \qquad \qquad \cup
\left\{ \boldsymbol{b}_{t}^{T} (\boldsymbol{\mu}_{\tilde{a}_{t}^{*}}-\boldsymbol{\mu}_{i}) < 2 \Vert \boldsymbol{b}_{t} \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}, N_{i,t} > \beta \right\}  \\
& \qquad \subset
\underbrace{
\left\{ 
\underset{d \in \mathcal{D}^{+}_{T}}{\cup} \left\{ \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{i,t}(d) > \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{i}(d) + \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right\} \right\}}_{A_t} \\
& \qquad \qquad \cup
\underbrace{
\left \{
\underset{d \in \mathcal{D}^{+}_{T}}{\cup} \left\{ \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t}(d) < \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{\tilde{a}_{t}^{*}}(d) - \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}}\right\}\right\}}_{B_t} \\
& \qquad \qquad \cup
\underbrace{
\left\{ \boldsymbol{b}_{t}^{T} (\boldsymbol{\mu}_{\tilde{a}_{t}^{*}}-\boldsymbol{\mu}_{i}) < 2 \Vert \hat{\boldsymbol{c}}_t \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}, N_{i,t} > \beta, \boldsymbol{b}_{t}^{T} \Delta_{i} > \eta_{i} - \epsilon\right\}}_{\Gamma_t},
\end{aligned}
\end{equation}

where $\mathcal{D}^{+}_{T} :=\left\{ d| [\boldsymbol{b}_{1}, \boldsymbol{b}_{2}, ..., \boldsymbol{b}_{T}](d) \in \mathcal{B}^{+}_{T} \right\}$, and $\mathcal{B}^{+}_{T}:= \{{[\boldsymbol{b}_{1}(d), \boldsymbol{b}_{2}(d), ..., \boldsymbol{b}_{T}(d)]} \neq \boldsymbol{0}, \forall d \in [D]\}$ is the collection set of non-zero $[\boldsymbol{b}(d)]^T$ sequence.

Then on event $A_t$, by applying Hoeffding’s Inequality (Lemma~\ref{lemma: Hoeffding}), for any $d \in [D]$, we have

\begin{equation}
\begin{aligned}
\mathbb{P} \left( \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{i,t}(d) > \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{i}(d) + \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}\right) 
& = 
\mathbb{P} \left( \hat{\boldsymbol{r}}_{i,t}(d) - \boldsymbol{\mu}_{i}(d) > \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}} \right) \\
& \leq
\exp \left( \frac{-2 N_{i,t}^2 \log(t/\alpha) }{ N_{i,t} \sum_{\iota=1}^{N_{i,t}}(1-0)^2 } \right) \\
& =
\exp \left( -2 \log(t/\alpha) \right) 
=
\left( \frac{\alpha}{t} \right)^2,
\end{aligned}
\end{equation}

which yields the upper bound of $\mathbb{P} (A_t)$ as 
\begin{equation}
\begin{aligned}
\label{eq: P_A_t_stat}
\mathbb{P} (A_t) \leq \sum_{d \in \mathcal{D}^{+}_{T}}
\mathbb{P} \left( \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{i,t}(d) > \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{i}(d) + \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}\right) 
& \leq 
|\mathcal{B}^{+}_{T}| \left( \frac{\alpha}{t} \right)^2,
\end{aligned}
\end{equation}

and similarly,
\begin{equation}
\begin{aligned}
\label{eq: P_B_t_stat}
\mathbb{P} (B_t) \leq \sum_{d \in \mathcal{D}^{+}_{T}}
\mathbb{P} \left( \boldsymbol{b}_{t}(d) \hat{\boldsymbol{r}}_{\tilde{a}_{t}^{*},t}(d) < \boldsymbol{b}_{t}(d) \boldsymbol{\mu}_{\tilde{a}_{t}^{*}}(d) - \boldsymbol{b}_{t}(d) \sqrt{\frac{ \log(t/\alpha)}{N_{\tilde{a}_{t}^{*},t}}} \right) 
& \leq 
|\mathcal{B}^{+}_{T}| \left( \frac{\alpha}{t} \right)^2.
\end{aligned}
\end{equation}

Next we investigate the event $\Gamma_t := \left\{ \boldsymbol{b}_{t}^{T} \Delta_i < 2 \Vert \boldsymbol{b}_{t} \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}, N_{i,t} > \beta \right\}$. 
Let $\beta = \frac{4 M^2 \log (T/\alpha)}{L_i^2}$. Since $N_{i,t} \geq \beta$ and recall that $\boldsymbol{b}_t^T (\boldsymbol{\mu}_{\tilde{a}^{*}_{t}} - \boldsymbol{\mu}_{i}) \geq L_i$, we have,

\begin{equation}
\begin{aligned}
2 \Vert \boldsymbol{b}_t \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{N_{i,t}}}
\leq
2 \Vert \boldsymbol{b}_t \Vert_1 \sqrt{\frac{ \log(t/\alpha)}{\beta}}
\leq
2 M \sqrt{\frac{ \log(T/\alpha)}{\beta}} = L_i 
\leq 
\boldsymbol{b}_t^T (\boldsymbol{\mu}_{\tilde{a}^{*}_{t}} - \boldsymbol{\mu}_{i}), 
\end{aligned}
\end{equation}

implying that the event $\Gamma_t$ has $\mathbb{P}$-probability 0.
By combining Eq.~\ref{eq: term_1_stat} with Eq.~\ref{eq: event_ABC_t_stat1}, \ref{eq: P_A_t_stat} and \ref{eq: P_B_t_stat}, the expectation of LHS term in Eq.~\ref{eq: N_up_bd_stat} can be upper-bounded as follows:

\begin{equation}
\begin{aligned}
\label{eq: upbd_term_1_stat}
\mathbb{E} \left[\sum_{t \in \mathcal{M}^{o}_i} \mathds{1}_{\{a_t = i \} } \right]
& = 
\mathbb{E} \left[ \sum_{t=1}^{T} \mathds{1}_{\{a_t = i \neq \tilde{a}^{*}_{t} \}} \right] \\
& \leq 
\frac{4 M^2 \log (T/\alpha)}{L_i^2}
+
|\mathcal{B}^{+}_{T}| \alpha^2 \sum_{t=1}^{T} t^{-2}  \\
& \underset{(a)}{\leq}
\frac{4 M^2 \log (T/\alpha)}{L_i^2}
+
|\mathcal{B}^{+}_{T}| \frac{\pi^2 \alpha^2}{3},
\end{aligned}
\end{equation}

where (a) holds by the convergence of sum of reciprocals of squares that 
$\sum_{t=1}^{\infty} t^{-2} = \frac{\pi^2}{6}$. This concludes the proof.

\end{proof}

\subsection{Remarks of Theorem \ref{thm: up_bd_known}}
\vspace{-10pt}
\textcolor[rgb]{0.4, 0.4, 1}{
\begin{remark}
If the distribution of $\boldsymbol{c}_t$ is stationary with known $\boldsymbol{\overline{c}}$, each arm can be viewed as having a stationary reward distributed with mean of $\boldsymbol{\overline{c}}^T \boldsymbol{\mu}_i \in \mathbb{R}$, and the goal is to maximizing accumulative reward. This reduces the problem to a standard MAB framework. By treating $\eta_{i}^{\uparrow} = \eta_{i}^{\downarrow} = \boldsymbol{\overline{c}}^T \Delta_i$ as the reward gap between arm $i$ and the best arm $a^*$, and $\delta$ as the upper-bound of reward $\boldsymbol{c}_t^T \boldsymbol{r}_{a_t,t}$ in each round $t$, our result in Theorem~\ref{thm: up_bd_known} matches the typical UCB bounds~\citep{auer2002finite}. 
% Interestingly, the expected performance depends only on the mean values and not on the variance or distribution of $\boldsymbol{c}_t$. A high-level interpretation is that, the expected regret can be decoupled into the sum of the product of expected preferences $\boldsymbol{\overline{c}}$ and expected reward $\boldsymbol{\mu}_i$. Since $\boldsymbol{\overline{c}}$ is known, the only randomness comes from estimating $\hat{\boldsymbol{r}}_i$.
\end{remark} 
}
\vspace{-15pt}

\textcolor[rgb]{0.4, 0.4, 1}{
\begin{remark}
Interestingly, the standard stochastic MAB can also be seen as a special case of PAMO-MAB with known preferences. 
Specifically, a $K$-armed stochastic bandit with reward means $x_1, \dots, x_K$ is equivalent to the MO-MAB case where $\exists j \in [D]$ s.t., $\boldsymbol{\mu}_i(j) = x_i$, $\forall i \in [K]$ and $\overline{\boldsymbol{c}}_t = \boldsymbol{e}_j$ (the $j$-th standard basis vector). 
In this case, obviously the best arm $a_t^* = \argmaxA_{i \in [K]} x_i$. 
Note 
$|\mathcal{\overline{C}}^{+}_{T}| \!=\! 1$, 
$\eta_{i}^{\downarrow} \!=\! \eta_{i}^{\uparrow} \!=\! \Delta_i(j) \!=\! x_{a_{t}^*} \!-\! x_{i}$,
the result in Theorem~\ref{thm: up_bd_known} can be rewrite as:
$
\textstyle
R(T) \leq \sum_{i=1}^{K}  \frac{4}{x_{a_{t}^*} - x_{i}} \log{(\frac{T}{\alpha})} + O(1)
$,
which recovers the bound in standard MAB~\citep{auer2002finite}.
\end{remark} 
}

\textcolor[rgb]{0.4, 0.4, 1}{
Specifically, the remarks above illustrate that under stationary and known preference environments, by introducing the preference-aware optimization, PAMO-MAB can be related to a standard MAB and is solvable using conventional techniques.
This insight also provides a foundation for the algorithm design and regret analysis in the unknown preference cases, where we will show that under precise preference estimation, the unknown preference problem can be reduced to the known case but narrowed overall-reward gap.
}