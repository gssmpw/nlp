\vspace{-8pt}
\section{Related Work}
% \vspace{-5pt}
\textbf{Multi-Objective Multi-Armed Bandits.} 
% \textbf{MO-MAB.} 
MO-MAB extends scalar rewards in the standard MAB problem to multi-dimensional vectors. 
The Pareto-UCB \cite{drugan2013designing} introduced the MO-MAB framework and Pareto regret as a metric, achieving $O(\log T)$ Pareto regret using the UCB technique.
Other techniques, including Knowledge Gradient \cite{yahyaa2014knowledge} and Thompson Sampling \cite{yahyaa2015thompson}, have subsequently been adapted for MO-MAB.
Additionally, researchers have extended the contextual setup to MO-MAB \cite{turgay2018multi, lu2019multi}. 
These studies aim to approximate the entire Pareto front 
$\mathcal{O}^*$, and employ a \emph{random arm selection policy} on the estimated Pareto front for Pareto optimality.
Considering computing the full Pareto front is expensive, another line of work propose to converts the multi-dimensional reward into a scalar value through a scalarization function, targeting a specific Pareto optimal solution while not the entire Pareto front. Our work also falls within the realm of this framework.
The scalarization function can either be randomly initialized (chosen) \cite{drugan2013designing, xu2023pareto}, or optimized based on a fixed metric, such as the Generalized Gini Index score \cite{busa2017multi, mehrotra2020bandit}.
Nonetheless, existing studies primarily achieve Pareto optimality through a \emph{global policy} for arm selection across all users. 
As discussed in Section \ref{sec: intro}, merely achieving Pareto optimality with a global policy may not yield favorable outcomes, as users have diverse preferences on different objectives. 
% Therefore, customized MO-MAB optimization under user preferences is essential, which is the goal of our work.


% We first begin with a quick overview of works that aim to minimize the Pareto regret over some horizon $T$. 
% One representative line of research is strategically approximating the entire Pareto front $\mathcal{O}^*$ and employing a \emph{uniform arm selection policy} upon estimated Pareto front for Pareto optimality and fairness \cite{drugan2013designing, turgay2018multi, lu2019multi, drugan2018covariance, balef2023piecewise}.
% In practice, determining the entire Pareto front can be costly. An alternative approach is to directly target a specific solution on the Pareto front through scalarization, which transforms the $D$-dimensional reward into a scalar. Scalarized-UCB \cite{drugan2013designing} addresses this by uniformly selecting a scalarization function from a set of weighted sum functions in each round, aiming to achieve Pareto optimality and fairness. 
% cite{busa2017multi} transfer the $D$-dimensional reward into Generalized Gini Index (GGI) through weighted-scalarization function, of which the weights can be computed by solving a linear program problem.
% The arm minimizes the GGI score is selected as it guarantees the Pareto optimality.
% cite{mehrotra2020bandit} adopt this GGI-based approach for music streaming recommendation. 
% cite{xu2023pareto} proposes to randomly select one dimension $d \in [D]$ as the optimization objective at the outset, reducing the problem to a standard MAB, since the arm that achieves optimality in one dimension must also be Pareto optimal.

\vspace{-2pt}
\textbf{Preference-based MO-MAB optimization.}
Recent studies have explored MO-MAB optimization using lexicographic order \cite{ehrgott2005multicriteria} to reflect user preferences. In lexicographic order, objectives are prioritized hierarchically, where the first objective takes absolute precedence over the second, and so on. cite{huyuk2021multi} first introduced lexicographic order to MO-MAB, and cite{cheng2024hierarchize} extended it to mixed Pareto-lexicographic environments.
However, lexicographic order may not adequately capture a user's overall satisfaction in real-world applications, where preferences often involve trade-offs rather than strict prioritization. For example, a user may prefer a \$10 meal with good taste over a \$9.5 meal with poor taste, even though cost is a priority.
Our work proposes a more general framework that incorporates a weighted order based on the user's explicit preference space. Notably, the lexicographic order becomes a special case of our proposed PAMO-MAB framework.


% Another line of MO-MAB research arms to convert multi-dimensional reward into a scalar by aggregation functions \cite{liu2014multiobjective}. 
% Scalarized UCB uniformly selects aggregation function from a set of weighted sum functions \cite{drugan2014pareto}, achieving $\mathcal{O}(S \log(T/S))$ regret, where $S$ is the set size.
% cite{busa2017multi} uses the Generalized Gini Index (GGI) aggregation function to fairly optimize objectives through online convex optimization.
% Our problem formulation is more akin to these scalarized MO-MABs, but the setting, motivation and the choice of weights are fundamentally different.
% Specifically, our aggregation functions are based on explicit user preferences, not predefined or optimized for maximum weighted sum reward. Our aim is to capture user preferences and implement customized optimization, not Pareto set estimation or fair optimization cross objectives. This requires a distinct analytical approach.

% Another line of MO-MAB research utilizes scalarization techniques to convert multiple objectives into a single one using aggregation functions \cite{liu2014multiobjective}. Different aggregation functions can be employed depending on the problem at hand. Scalarized UCB \cite{drugan2014pareto} uniformly chooses the aggregation function from a set of weighted sum functions, achieving $\mathcal{O}(S \log(T/S))$ scalarized regret, where $S$ is the size of functions set used by the algorithm. 
% cite{busa2017multi} leverage the Generalized Gini Index (GGI) aggregation function with online convex optimization to fairly optimize the objectives of MO-MAB.
% Our problem formulation is more akin to these scalarized MO-MAB problems. However, the essential difference is that our scalarization functions are formulated based on explicit user preferences rather than being manually set or optimized for maximizing the best-case weighted sum reward. Under this setup, our goal is to accurately reflect overall user satisfaction and implement customized optimization, rather than estimating the Pareto set and optimizing all objectives in a fair manner as in the studies mentioned above.
% This leads to a completely distinct analytic approach from others.

% === appendix ===
% \textbf{Lexicographic optimality for MO-MAB.}
% Recent studies explore multi-objective optimization in MO-MAB using lexicographic order to reflect user preferences \cite{ehrgott2005multicriteria}, which aligns with our goals. Lexicographic order prioritizes objectives hierarchically, where the first objective has absolute precedence over the second, and so on. cite{huyuk2021multi} introduced priority-based regret for MO-MAB under lexicographic order, while cite{cheng2024hierarchize} extended this to mixed Pareto-lexicographic environments with a new optimality measure.
% However, we argue that lexicographic order may not capture overall user satisfaction in real-world applications, as user preferences often involve trade-offs rather than strict prioritization. For instance, a user may prefer a \$10 meal with good taste over a \$9 meal with poor taste, despite cost being a priority. Our approach, which incorporates explicit user preferences, provides a more general and practical framework of weighted order MO-MAB optimization under preference space.
% === appendix ===


% Recently, some studies explore the multi-objective optimization of MO-MAB under lexicographic order \cite{ehrgott2005multicriteria} for reflecting user preferences, which share the similar goal with our work. Specially, lexicographic orders defines that the first objective holds absolute priority over the second one, and in turn has higher precedence over the third one, etc.
% cite{huyuk2021multi} first investigate the MO-MAB under lexicographic order and develop a priority-based regret to assess the bandit algorithm under this environment. 
% cite{cheng2024hierarchize} extend the problem to the mixed Pareto-lexicographic orders environment and propose a notion of Pareto-lexicographic optimality to evaluate the learnersâ€™ performance.
% However, we augue that lexicographic orders would be limited to reflect the user's overall satisfactions in many real-world applications since the user preferences on objectives is a complex trade-off rather than an indefinitely over-weighting. For example, even though a user want a cheap dinner, meal A of \$10 with good taste is more likely to be preferred than meal B of \$9 but with very bad taste. 
% On the other hand, by introducing the explicit user preferences, our problem formulation enjoys a more general and practical setup for the preference order MO-MAB.