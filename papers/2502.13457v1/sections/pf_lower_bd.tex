\section{Proof of Proposition \ref{prop: lower_bd}}
\label{sce: app_pf_lower_bd}

\begin{lemma}[Variant of Lemma 7 in \cite{jun2018adversarial}]
\label{lemma: R_N_relation}
Assume that a bandit algorithm enjoys a sub-linear regret bound, then 
$\mathbb{E}[ N_{i,T} ] = o(T), \forall i \neq a^*$.
\end{lemma}

\begin{proof}
The sub-linear regret bound implies that for a sufficiently large $T$ there exists a constant $C > 0$ such that 
$\sum_{i=1}^{K} \mathbb{E}[ N_{i,T} ] \boldsymbol{\overline{c}}_t^T (\mu_{a^*} - \mu_{i}) < CT $. 
Hence we have 
$\mathbb{E}[ N_{i,T} ] \boldsymbol{\overline{c}}_t^T (\mu_{a^*} - \mu_{i}) \leq CT, \forall i \neq a^*$, implying 
$\mathbb{E}[ N_{i,T} ] < \frac{CT}{\boldsymbol{\overline{c}}_t^T (\mu_{a^*} - \mu_{i})}$.
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop: lower_bd}]  
We proof the Proposition \ref{prop: lower_bd} using a simple toy instance with 2 arms and 2 objectives. For the derivation based on the general instance, please refer to Appendix \ref{sce: app_pf_lower_bd_general}.

Let us consider an instance with two arms, where arm-1 has fixed reward $\boldsymbol{r}_{1} = [1,0]^{\top}$, arm-2 has fixed reward $\boldsymbol{r}_{2} = [0,1]^{\top}$. There two preference vectors $\boldsymbol{c}_1 = [1,0]^{\top}$ and $\boldsymbol{c}_2 = [0,1]^{\top}$.
Apparently, under preference $\boldsymbol{c}_1$, arm-1 is the optimal arm, while for preference $\boldsymbol{c}_2$, arm-2 is the optimal.

Assume there exists a preferences-free algorithm $\mathcal{A}$ (i.e., Pareto-UCB \cite{drugan2013designing}) achieving sub-linear regret under preference $\boldsymbol{c}_1$.
By Lemma \ref{lemma: R_N_relation}, we have 
\[
\mathbb{E}_{\boldsymbol{c}_1}[N_{2,T}] = \sum_{t \in [T]} \mathbb{P}_{\pi^{\mathcal{A}}_t \mid \boldsymbol{c}_1} (a_t = 2 \mid \boldsymbol{c}_1) = o(T),
\]
where $N_{2,T}$ denotes the number of pulls of arm-2 (suboptimal).


Since the policy $\pi^{\mathcal{A}}_t$ of $\mathcal{A}$ is  independent on the sequences of instantaneous preferences and preferences means, thus for algorithm $\mathcal{A}$ under preference $\boldsymbol{c}_2$, we have 
\[
\mathbb{E}_{\boldsymbol{c}_2}[N_{2,T}] = \sum_{t \in [T]} \mathbb{P}_{\pi^{\mathcal{A}}_t \mid \boldsymbol{c}_2} (a_t = 2 \mid \boldsymbol{c}_2) 
= \sum_{t \in [T]} \mathbb{P}_{\pi^{\mathcal{A}}_t \mid \boldsymbol{c}_1 } (a_t = 2 \mid \boldsymbol{c}_1)
= \mathbb{E}_{\boldsymbol{c}_1}[N_{2,T}] = o(T),
\]
where the second equality holds by the the definition of preferences-free algorithm in Definition \ref{def: pref_free_alg}.

However, recall that under preference $\boldsymbol{c}_2$, arm-2 is the optimal arm, which implies that the regret of $\mathcal{A}$ under preference $\boldsymbol{c}_2$ would be at least $\Omega(T)$, i.e.,
\[
R_{\boldsymbol{c}_2}(T) 
= 
1 \cdot \mathbb{E}_{\boldsymbol{c}_2}[N_{1,T}]
=
1 \cdot (T - \mathbb{E}_{\boldsymbol{c}_2}[N_{2,T}])
>
T - o(T)
=
\Omega(T).
\]
\end{proof}

\subsection{General Version}
\label{sce: app_pf_lower_bd_general}

\begin{definition}[Pareto order, \cite{lu2019multi}]
Let $\boldsymbol{u}, \boldsymbol{v} \in \mathbb{R}^{D}$ be two vectors.
\begin{itemize}[leftmargin=*]
    \item $\boldsymbol{u}$ dominates $\boldsymbol{v}$, denoted as $\boldsymbol{u} \succ \boldsymbol{v}$, if and only if $\forall d \in [D], \boldsymbol{u}(d) > \boldsymbol{v}(d)$.
    \item $\boldsymbol{v}$ is not dominated by $\boldsymbol{u}$, denoted as by $\boldsymbol{u} \not\succ \boldsymbol{v}$, if and only if $\boldsymbol{u} = \boldsymbol{v}$ or $\exists d \in [D], \boldsymbol{v}(d) > \boldsymbol{u}(d)$.
    \item $\boldsymbol{u}$ and $\boldsymbol{v}$ are incomparable, denoted as $\boldsymbol{u}||\boldsymbol{v}$, if and only if either vector is not dominated by the other, i.e., $\boldsymbol{u} \not\succ \boldsymbol{v}$ and $\boldsymbol{v} \not\succ \boldsymbol{u}$.
\end{itemize}
\end{definition}

\begin{proof}[Proof of Proposition \ref{prop: lower_bd} (General Version)]  
We first construct an arbitrary $K$-armed $D$-objective MO-MAB environment with conflicting reward objectives. 
Let each objective reward of each arm follow a distribution, i.e., $\boldsymbol{r}_{i,t}(d) \sim {\rm Dist}_{i,d}, \forall i \in [K], \forall d \in [D]$, with mean of $\mu_i(d)$.
Define $\mathcal{P} := \{ [{\rm Dist}_{1,d}]^D, [{\rm Dist}_{2,d}]^D, ..., [{\rm Dist}_{K,d}]^D  \} $ be the set of $K$-armed $D$-dimensional reward distributions.

We start with a simple case where the MO-MAB environment has two conflicting objective arms. 
Specifically, assume that $\exists u,v \in [K]$, s.t., 
\[
\boldsymbol{\mu}_{u} \neq \boldsymbol{\mu}_{v}; 
\quad \boldsymbol{\mu}_{u} || \boldsymbol{\mu}_{v}
\]
and 
\[
\boldsymbol{\mu}_{u} \succ \boldsymbol{\mu}_{i}, \boldsymbol{\mu}_{v} \succ \boldsymbol{\mu}_{i}, \forall i \in [k] \setminus \{u, v\}.
\]


Due to $\boldsymbol{\mu}_{u} \neq \boldsymbol{\mu}_{v}$, by taking the orthogonal complement of $\boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v}$, we can construct a subset $\mathcal{C}_{\varsigma^{+}} := \{ \boldsymbol{c} \in \mathbb{R}^D | \boldsymbol{c}^T ( \boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v} ) = 0 \}$.
Next we consider two different constant preferences vector sets as the user's preferences, to construct two sets of preferences-aware MO-MAB scenarios.

\textbf{Scenarios $\mathcal{S}_{\varsigma^{+}}$}.
For any $\varsigma^{+} > 0$, we can construct a subset $\mathcal{C}_{\varsigma^{+}} := \{ \boldsymbol{c} \in \mathbb{R}^D | \boldsymbol{c}^T ( \boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v} ) = \varsigma^{+} \}$. Specifically, the general form of $\boldsymbol{c}_{\varsigma^{+}} \in \mathcal{C}_{\varsigma^{+}}$ can be written as $\boldsymbol{c}_{\varsigma^{+}} = \frac{\varsigma^{+}}{ \Vert \boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v} \Vert_2^2 } (\boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v}) + \boldsymbol{c}_0$, where $\boldsymbol{c}_0$ is any vector such that $\boldsymbol{c}_0 \in \mathcal{C}_0$.
Then for the preferences-aware MO-MAB scenarios $\mathcal{S}_{\varsigma^{+}} := \{\mathcal{P} \times \mathcal{C}_{\varsigma^{+}} \}$ under the sets of arm reward distributions $\mathcal{P}$ and user preferences $\mathcal{C}_{\varsigma^{+}}$, it is obvious that arm $u$ is the optimal arm since 
$\boldsymbol{\mu}_{u} \succ \boldsymbol{\mu}_{i}, \forall i \in [K] \setminus \{u,v\}$ and $\boldsymbol{c}_{\varsigma^{+}}^T  \boldsymbol{\mu}_{u} > \boldsymbol{c}_{\varsigma^{+}}^T \boldsymbol{\mu}_{v}, \forall \boldsymbol{c}_{\varsigma^{+}} \in \mathcal{C}_{\varsigma^{+}}$.

\textbf{Scenarios $\mathcal{S}_{\varepsilon^{-}}$}.
Similarly, for any $\varepsilon^{-} < 0$, we can construct a subset $\mathcal{C}_{\varepsilon^{-}} := \{ \boldsymbol{c} \in \mathbb{R}^D | \boldsymbol{c}^T ( \boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v} ) = \varepsilon^{-} \}$, with the general form of $\boldsymbol{c}_{\varepsilon^{-}} = \frac{\varepsilon^{-}}{ \Vert \boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v} \Vert_2^2 } (\boldsymbol{\mu}_{u} - \boldsymbol{\mu}_{v}) + \boldsymbol{c}_0$, where $\boldsymbol{c}_0$ is any vector such that $\boldsymbol{c}_0 \in \mathcal{C}_0$.
For scenarios $\mathcal{S}_{\varepsilon^{-}} := \{\mathcal{P} \times \mathcal{C}_{\varepsilon^{-}} \}$ with same arm rewards distributions $\mathcal{P}$ but modified user preferences $\mathcal{C}_{\varepsilon^{-}}$ sets, we have the arm $v$ to be the optimal.


We use $\mathbb{P}_{\varsigma^{+}}$ to denote the probability with respect to the scenarios $\mathcal{S}_{\varsigma^{+}}$, and use $\mathbb{P}_{\varepsilon^{-}}$ to denote the probability conditioned on $\mathcal{S}_{\varepsilon^{-}}$. Analogous expectations $\mathbb{E}_{\varsigma^{+}}[\cdot]$ and $\mathbb{E}_{\varepsilon^{-}}[\cdot]$ will also be used.
Let $\boldsymbol{\mathrm{a}}^{t-1} = \{ A_1,..., A_{t-1} \}$ and $\boldsymbol{\mathrm{r}}^{t-1} = \{ \boldsymbol{x}_{1}, ..., \boldsymbol{x}_{t-1} \}$ be the actual sequence of arms pulled and the sequence of received rewards up to episode $t-1$, and $\boldsymbol{\mathrm{H}}^{t-1} = \{ \langle A_1, \boldsymbol{x}_{1} \rangle, ..., \langle A_{t-1}, \boldsymbol{x}_{t-1} \rangle \}$ be the corresponding historical rewards sequence. 
For consistency, we define $\boldsymbol{\mathrm{a}}^{0}$, $\boldsymbol{\mathrm{r}}^{0}$ and $\boldsymbol{\mathrm{H}}^{0}$ as the empty sets.
Assume there exists a preferences-free algorithm $\mathcal{A}$ (i.e., Pareto-UCB \cite{drugan2013designing}) that is possibly dependent on historical rewards sequence $\boldsymbol{\mathrm{H}}^{t-1}$ at episode $t$ (classical assumption in MAB), 
achieving sub-linear regret in scenarios $\mathcal{S}_{\varsigma^{+}}$. Let $N_{i,T}$ be the number of pulls of arm $i$ by $\mathcal{A}$ up to $T$ episode. By Lemma \ref{lemma: R_N_relation}, we have 
\begin{equation}
\label{eq: E_N_+}
\mathbb{E}_{\varsigma^{+}}[N_{*,T}] = \mathbb{E}_{\varsigma^{+}}[N_{u,T}] = T - o(T).
\end{equation}

Since the policy $\pi^{\mathcal{A}}_t$ of $\mathcal{A}$ is possibly dependent on $\boldsymbol{\mathrm{H}}^{t-1}$ but independent on the sequences of instantaneous preferences $\boldsymbol{\mathrm{c}}^{t}$ and preferences means $\boldsymbol{\overline{c}}$, for $t \in (0,T\}$, $i \in [K]$ we have 

\begin{equation}
\label{eq: E_0-E_epsilon}
\begin{aligned}
& \mathbb{E}_{\varsigma^{+}}[ \mathds{1}_{a_t = i} ] - \mathbb{E}_{\varepsilon^{-}}[ \mathds{1}_{a_t = i} ] \\
& \quad = 
\sum_{\substack{\boldsymbol{\mathrm{a}}^{t-1} \in [K]^{t-1}}} \int_{\substack{\boldsymbol{\mathrm{r}}^{t-1} \in [0,1]^{D \times (t-1)}}}
\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t=i | \boldsymbol{\mathrm{H}}^{t-1}, [\boldsymbol{c}_0]^{t}, \boldsymbol{c}_0)
\cdot
\mathbb{P}_{\varsigma^{+}}(\boldsymbol{\mathrm{H}}^{t-1} ) 
d\boldsymbol{\mathrm{r}}^{t-1} \\
& \quad \quad \quad \quad -
\sum_{\substack{\boldsymbol{\mathrm{a}}^{t-1} \in [K]^{t-1}}} \int_{\substack{\boldsymbol{\mathrm{r}}^{t-1} \in [0,1]^{D \times (t-1)}}}
\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t=i | \boldsymbol{\mathrm{H}}^{t-1}, [\boldsymbol{c}_{\varepsilon^{-}}]^{t}, \boldsymbol{c}_{\varepsilon^{-}})
\cdot
\mathbb{P}_{\varepsilon^{-}}(\boldsymbol{\mathrm{H}}^{t-1} ) 
d\boldsymbol{\mathrm{r}}^{t-1} \\
& \quad \underset{(a)}{=}
\sum_{\substack{\boldsymbol{\mathrm{a}}^{t-1} \in [K]^{t-1}}} \int_{\substack{\boldsymbol{\mathrm{r}}^{t-1} \in [0,1]^{D \times (t-1)}}}
\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t=i | \boldsymbol{\mathrm{H}}^{t-1})
\cdot
\bigg(
\mathbb{P}_{\varsigma^{+}}(\boldsymbol{\mathrm{H}}^{t-1} )
-
\mathbb{P}_{\varepsilon^{-}}(\boldsymbol{\mathrm{H}}^{t-1} )
\bigg )
d\boldsymbol{\mathrm{r}}^{t-1}, \\
\end{aligned}
\end{equation}

with 
\begin{equation}
\label{eq: P_0=P_epsilon}
\begin{aligned}
& \mathbb{P}_{\varsigma^{+}}(\boldsymbol{\mathrm{H}}^{t-1} ) 
= \prod_{\tau=1}^{t-1}
\left(
\mathbb{P}_{\varsigma^{+}}(\boldsymbol{\mathrm{H}}^{\tau-1})
\cdot
\mathbb{P}_{\pi_{\tau}^{\mathcal{A}}}(a_{\tau} = A_{\tau}| \boldsymbol{\mathrm{H}}^{\tau-1} )
\cdot
\mathbb{P}_{\varsigma^{+}}(r_{a_{\tau}} = \boldsymbol{x}_{\tau} | a_{\tau}=A_{\tau})
\right) , \\
& \mathbb{P}_{\varepsilon^{-}}(\boldsymbol{\mathrm{H}}^{t-1} ) 
= \prod_{\tau=1}^{t-1}
\left(
\mathbb{P}_{\varepsilon^{-}}(\boldsymbol{\mathrm{H}}^{\tau-1})
\cdot
\mathbb{P}_{\pi_{\tau}^{\mathcal{A}}}(a_{\tau} = A_{\tau}| \boldsymbol{\mathrm{H}}^{\tau-1} )
\cdot
\mathbb{P}_{\varepsilon^{-}}(r_{a_{\tau}} = \boldsymbol{x}_{\tau} | a_{\tau}=A_{\tau})
\right).
\end{aligned}
\end{equation}

where $\boldsymbol{c}_0, \boldsymbol{c}_{\varepsilon^{-}}$ can be any constant vectors such that $\boldsymbol{c}_0 \in \mathcal{C}_0$ and $\boldsymbol{c}_0 \in \mathcal{C}_{\varepsilon^{-}}$. (a) holds since the policy $\pi^{\mathcal{A}}_t$ is independent of $\boldsymbol{\mathrm{c}}^{t}$ and $\boldsymbol{\overline{c}}$. Hence 
$
\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t=i | \boldsymbol{\mathrm{H}}^{t-1})
=
\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t=i | \boldsymbol{\mathrm{H}}^{t-1}, [\boldsymbol{c}_0]^{t}, \boldsymbol{c}_0)
= \mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t=i | \boldsymbol{\mathrm{H}}^{t-1}, [\boldsymbol{c}_{\varepsilon^{-}}]^{t}, \boldsymbol{c}_{\varepsilon^{-}})$
(recall the definition of preferences-free algorithm in Definition \ref{def: pref_free_alg}).

Additionally, please note that both scenarios $\mathcal{S}_{\varsigma^{+}}$ and $\mathcal{S}_{\varepsilon^{-}}$ share the same arm reward distributions $\mathcal{P}$, which implies that for any $t \in (0,T]$ and $A \in [K]$, we have

\[
\mathbb{P}_{\varsigma^{+}}(r_{a_{t}} = \boldsymbol{x}_{t} | a_{t}=A)
=
\mathbb{P}_{\varepsilon^{-}}(r_{a_{t}} = \boldsymbol{x}_{t} | a_{t}=A).
\]

Combining result above with Eq. \ref{eq: P_0=P_epsilon} and using the fact that $\boldsymbol{\mathrm{H}}^{0}:= \emptyset$ for both  $\mathcal{S}_{\varsigma^{+}}$ and $\mathcal{S}_{\varepsilon^{-}}$, it can be easily verified by induction that 
$\mathbb{P}_{\varsigma^{+}}(\boldsymbol{\mathrm{H}}^{t-1} ) 
= 
\mathbb{P}_{\varepsilon^{-}}(\boldsymbol{\mathrm{H}}^{t-1} )$.
Plugging this back to Eq \ref{eq: E_0-E_epsilon} yields

\begin{equation}
\label{eq: E_0=E_epsilon}
\begin{aligned}
& \mathbb{E}_{\varsigma^{+}}[ \mathds{1}_{a_t = i} ] - \mathbb{E}_{\varepsilon^{-}}[ \mathds{1}_{a_t = i} ] \\
& \quad =
\sum_{\substack{\boldsymbol{\mathrm{a}}^{t-1} \in [K]^{t-1}}} \int_{\substack{\boldsymbol{\mathrm{r}}^{t-1} \in [0,1]^{D \times (t-1)}}}
\mathbb{P}_{\pi_t^{\mathcal{A}}} (a_t=i | \boldsymbol{\mathrm{H}}^{t-1})
\cdot
\bigg(
\cancelto{0}
{
\mathbb{P}_{\varsigma^{+}}(\boldsymbol{\mathrm{H}}^{t-1} )
-
\mathbb{P}_{\varepsilon^{-}}(\boldsymbol{\mathrm{H}}^{t-1} )
}
\bigg )
d\boldsymbol{\mathrm{r}}^{t-1}
= 0.
\end{aligned}
\end{equation}

By summing over $T$ we can derive that 

\[
\mathbb{E}_{\varsigma^{+}}[ N_{i,T} ]
=
\sum_{t=1}^{T} \mathbb{E}_{\varsigma^{+}}[ \mathds{1}_{a_t = i} ]
=
\sum_{t=1}^{T} \mathbb{E}_{\varepsilon^{-}}[ \mathds{1}_{a_t = i} ]
=
\mathbb{E}_{\varepsilon^{-}}[ N_{i,T} ].
\]

Combining above result with Eq. \ref{eq: E_N_+} gives that= 
\[
\mathbb{E}_{\varsigma^{+}}[N_{u,T}] = \mathbb{E}_{\varepsilon^{-}}[N_{u,T}] = T - o(T) = \Omega(T).
\]

However, recall that in scenarios $\mathcal{S}_{\varepsilon^{-}}$, $u$ is a suboptimal arm, which implies that the regret of $\mathcal{A}$ in $\mathcal{S}_{\varepsilon^{-}}$ would be at least $\Omega(T)$, i.e.,
\[
\begin{aligned}
R(T) 
& = 
\sum_{i \neq v} \boldsymbol{c}_{\varepsilon^{-}}^T (\mu_{v} - \mu_{i}) \mathbb{E}_{\varepsilon^{-}}[ N_{i,T}] \\
& >
|\varepsilon^{-}| \mathbb{E}_{\varepsilon^{-}}[ N_{u,T}]
=
\Omega(T).
\end{aligned}
\]

The analysis above indicates that for the case with two objective-conflicting arms $u, v$, for any preferences-free algorithm $\mathcal{A}$, if there exists a $ \varsigma^{+}>0$ such that $\mathcal{A}$ can achieve sub-linear regret in scenarios $\mathcal{S}_{\varsigma^{+}}$, then it will suffer the regret of the order $\Omega(T)$ in scenarios $\mathcal{S}_{\varepsilon^{-}}$ for all $\varepsilon^{-}<0$, and vice verse (i.e., sub-linear regret in $\varepsilon^{-}>0$ while $\Omega(T)$ regret in $\mathcal{S}_{\varsigma^{+}}$).

Next we extend the solution to the MO-MAB environment containing more than two objective-conflicting arms.
% The solution can be easily extended to the MO-MAB environment containing more than two objective-conflicting arms.
Specifically, for each conflicting arm $i$, we can simply select another conflicting arm $j$ to construct a pair, and apply the solution we derived in two-conflicting arms case. 
By traversing all conflicting arms, we have that for any preferences-free algorithm $\mathcal{A}$ achieving sub-linear regret in a scenarios set $\mathcal{S}_{0}$ with a subset of conflicting arms $\{a^*\}$ as the optimal, there must exists another scenarios set $\mathcal{S}^{\prime}_0$ for each arm $i \in \{a^*\}$ such that the arm $i$ is considered as suboptimal and lead to the regret of order $\Omega(T)$.
This concludes the proof of Proposition \ref{prop: lower_bd}.
\end{proof}


\begin{remark}
\label{remark:lower_bd}
As a side-product of the analysis above, we have:
If one MO-MAB environment contains multiple objective-conflicting arms, i.e., $\vert \mathcal{O}^{*} \vert \geq 2$, where $\mathcal{O}^{*}$ is the Pareto Optimal front. 
Then for any Pareto-Optimal arm $i \in \mathcal{O}^{*}$, there exists preferences subsets such that the arm $i$ is suboptimal.
\end{remark}    

