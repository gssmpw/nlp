% \newpage
\section{The Case when the Preference Is Provided}
\label{sec: knonw}

% \captionsetup[figure]{skip=5pt}

\begin{figure}[t]
\vskip -0.1in
\begin{center}
\centerline{\includegraphics[width=0.55\textwidth]{figures/s1_2.pdf}}
\caption{
An example where user explicitly provides her preference to LLM system before (\ding{182}) or after (\ding{183}) the response of movie recommendation (decision making).}
\label{fig: s1}
\end{center}
\end{figure}

% \begin{wrapfigure}{r}{0.47\textwidth} % Adjust the width as needed
%     \vspace{-20pt} % Adjust vertical space before the algorithm
%     \begin{minipage}{0.47\textwidth}
% \begin{algorithm}[H]
% \small
% \caption{Preference UCB (PRUCB)}
% \begin{algorithmic}[1]
% \STATE \textbf{Parameters:} $\alpha$.
% \STATE \textbf{Initialization:} 
% $N_{i, 1} \!\leftarrow\! 0$; $\boldsymbol{\hat{r}}_{i,1} \!\leftarrow\! [0]^{D}, \forall i \!\in\! [K]$.

% \For{$t=1,\cdots,$ T}
%     \STATE Obtain user expected preference $\boldsymbol{\overline{c}}_t$, $\boldsymbol{\hat{c}}_t \!\leftarrow\! \boldsymbol{\overline{c}}_t$.
%     % \STATE $a_t = \argmaxA_{i \in [K]} f ( \boldsymbol{\hat{c}}_t,  \hat{\boldsymbol{r}}_{i,t}+\sqrt{\frac{ \log(t/\alpha)}{\max \{1, N_{i, t } \} }} \boldsymbol{e} )$.
%     \STATE Draw $a_t$ by Eq. \ref{eq:prucb_at}, observe reward $\boldsymbol{r}_{a_t, t}$.
%     % \Comment{Computation of Eq.~\eqref{beta_update}}
%     % \STATE Update 
%     % \begin{small}
%     %     $N_{i, t+1} \!=\! N_{i, t} \!+\! \mathds{1}_{ \{a_t = i \}}$
%     % \end{small}, 
%     % reward estimation $\hat{\boldsymbol{r}}_{i,t+1} = \frac{\hat{\boldsymbol{r}}_{i,t} N_{i, t} + \boldsymbol{r}_{a_t, t} \cdot \mathds{1}_{ \{a_t = i \}} }{N_{i, t+1}}, \forall i \in [K]$.
%     \STATE Update $N_{i, t+1}$ and $\hat{\boldsymbol{r}}_{i,t+1}, \forall i \!\in\! [K]$ by Eq.\ref{eq:prucb_r_t}.
%     % \STATE Update estimated reward $\hat{\boldsymbol{r}}_{i,t+1} = \frac{\hat{\boldsymbol{r}}_{i,t} N_{i, t} + \boldsymbol{r}_{i, t}}{N_{i, t+1}}$ if $i==a_t$ else $\hat{\boldsymbol{r}}_{i,t+1} = \hat{\boldsymbol{r}}_{i,t}, \forall i \in [K]$.
% \EndFor
% \end{algorithmic}
% \label{alg:PRUCB}
% \end{algorithm}
%  \end{minipage}
%     \vspace{-10pt} % Adjust vertical space after the algorithm
% \end{wrapfigure}



% for understanding the structure of the problem and tackling the subsequent more challenging case where preferences are unknown
As a warm-up, we begin with a simpler case where the user's preferences are explicitly provided to the agent either before or after decision making. 
% Formally, at each round $t$, the learner obtains $\overline{\boldsymbol{c}}_t \in \mathbb{R}^D$ from user's input and selects an arm $a_t \in [K]$, then observes $\boldsymbol{r}_{a_t,t} \in \mathbb{R}^D$. 
This setup is prevalent in numerous real-world applications. 
% In personalized recommender, systems are typically informed of user preferences (e.g., quality, price, style) before or after recommendation.
Many online systems now allow users to express their preferences through interactive techniques, such as conversations and prompt design, either before or after taking action. For example, as shown in Fig. \ref{fig: s1}, a user can explicitly share her movie preferences with an LLM-based chat system either prior to receiving a recommendation (\textsf{\ding{182}} in the initial input) or after receiving one (\textsf{\ding{183}} in the follow-up conversation).

We observe that when the preference $\boldsymbol{c}_t$ is given before decision-making, by simply using the preference $\boldsymbol{c}_t$ as a weight vector on the reward estimate, the 
problem collapses into a single-objective MAB. 
Therefore, we focus on the case where the preference is unknown and only revealed after decision-making. 
Formally, at each round $t$, the learner first selects an arm $a_t$, and then observes the reward $\boldsymbol{r}_{a_t}$ and user's preference $\boldsymbol{c}_t$.
As we will show later, the known preference case can be handled as a special instance within our developed framework for unknown preference.

As discussed in Section \ref{sec:lower_bd}, policy adapting to user preference is crucial. 
% For the unknown preference case, the inaccessibility of the true preference expectation raises two fundamental questions for algorithm design:
% \emph{(1) How to estimate the unknown preference?
% (2) How to handle the uncertainty of preference estimation in decision making?}
To achieve this, we develop Preference-UCB for Unknown Preference (PRUCB-UP, presented in Algorithm \ref{alg:PRUCB_UP}). 
At a high level, PRUCB-UP is an extension of the UCB approach \cite{auer2002finite} with two new components.

\textbf{Preference estimation.}
Capturing user preferences is a fundamental step toward preference adaptation. Due to the unknown preference expectation, we leverage the empirical average of historical preference feedback as the preference estimate. For $t \geq 1$, preference estimate is updated as
\vspace{-3pt}
\begin{equation}
\label{eq:PRUCB_SPM_c_t}
\textstyle
\hat{\boldsymbol{c}}_{t+1} = \big( (t-1) \boldsymbol{\hat{c}}_{t} + \boldsymbol{c}_{t} \big)/t.
\vspace{-3pt}
\end{equation}
Similarly, the reward estimate $\hat{\boldsymbol{r}}_{i,t}$ is defined as empirical estimation. For $t \in [1,T]$, it is updated as:
\vspace{-3pt}
\begin{equation}
\begin{aligned}
\textstyle
\label{eq:prucb_r_t}
N_{i, t+1} &= N_{i, t} + \mathds{1}_{ \{a_{t} = i \}}, \\
\hat{\boldsymbol{r}}_{i,t+1} & = (\hat{\boldsymbol{r}}_{i,t} N_{i, t} + \boldsymbol{r}_{a_{t}, t} \cdot \mathds{1}_{ \{a_{t} = i \}} )/N_{i, t+1},
\vspace{-5pt}
\end{aligned}
\end{equation}
with $N_{i, 1} \leftarrow 0, \boldsymbol{\hat{r}}_{i, 1} \leftarrow [0]^D, \forall i \in [K]$. 
% $N_{i,t}$ denotes the number of pulls of arm $i$ within the first $t-1$ rounds.


\textbf{Preference-aware optimization.}
To adapt the policy to the estimated preference, one might consider following the "optimism in the face of uncertainty" principle \cite{auer2002finite} by constructing confidence intervals on both preference and reward estimates, and incorporating them into the aggregation function $\Phi$ for optimization.

% Due to the deviation of preference estimate from the true expectation, an intuitive approach could involve constructing a confidence region $\Theta_t$ for $\hat{\boldsymbol{c}}_t$, similar to how the UCB method handles reward estimates $\hat{\boldsymbol{r}}_t$. 
% The solution would then be to choose the pair $( \hat{\boldsymbol{c}}_t^{\prime}, a_t) \in \Theta_t \times [K]$ that jointly maximizes the upper-confidence bound of the overall-reward.

However, in this case, we claim that despite the deviation from the true expectation, \emph{a confidence term for the preference estimate $\hat{\boldsymbol{c}}_t$ is unnecessary} 
The fundamental reason is that the preference estimation does not involve \emph{sequential action decision-making} component, where the preference feedback ${\boldsymbol{c}}_t$ observed is independent of the chosen action $a_t$.
Consequently, a bonus term for exploration is not required.
In contrast, for reward estimation, the action $a_t$ determined by $\hat{\boldsymbol{r}}_t$ will also influence the future estimate $\hat{\boldsymbol{r}}_{t+1}$, making a confidence term on $\hat{\boldsymbol{r}}_t$ is necessary to encourage exploration.

\input{pseudo_code/algo1}


Building upon this, the arm selection policy is designed as:
\vspace{-5pt}
\begin{equation}
\label{eq:prucb_spm_at}
\textstyle
a_t = \argmaxA_{i \in [K]} \Phi ( \boldsymbol{\hat{c}}_t,  \hat{\boldsymbol{r}}_{i,t} + \rho_{i,t}^{\alpha} \boldsymbol{e} ).
\vspace{-2pt}
\end{equation}
where $\Phi$ is the inner product aggregation function, $\rho_{i,t}^{\alpha} = \sqrt{ \log(t/\alpha) / \max \{1, N_{i, t } \} }$ is the standard Hoeffding bonus term.
We characterize the regret of PRUCB-UP below.

\begin{theorem}
\label{theorem:up_bd_stat}
Assume the preference follows unknown distribution with the value being revealed after each arm pull. 
Let $\eta_{i} = \boldsymbol{\overline{c}}^{T} \Delta_{i}$, 
$\Delta_{i} = \mu_{a^{*}} - \mu_{i} \in \mathbb{R}^D$,
Algorithm \ref{alg:PRUCB_UP} has
\[
R(T) 
=
O
\Bigg(
\sum_{i \neq a^{*}}
\Big(
\underbrace{
\textcolor{cyan}{
\frac{\delta^2 \log T}{\eta_{i}} 
+
D \pi^2 \alpha^2 \eta_{i}
}}_{R^{r}_T}
 +
\underbrace{
\textcolor{orange}{
\frac{ D^2 \Vert \Delta_{i} \Vert_2^2 \delta^2 }{ \eta_{i} }
}}_{R^{c}_T}
\Big)
\Bigg),
\]
where $R^{r}_T$ and $R^{c}_T$ refer to the regrets caused by reward estimate error and preference estimate error respectively.
\end{theorem}


\begin{remark}
Theorem \ref{theorem:up_bd_stat} shows that with preference feedback, PRUCB-UP achieves a regret of $O(K \delta \log T + K \delta D^2)$, demonstrating near-optimal performance. Notably, the regret caused by additional preference estimation error is bounded by a constant related to objective dimension $D$ and preference $\ell_1$-norm bound $\delta$. 
This implies that the impact of preference estimation error on the regret is small.

PRUCB-UP also admits an instance-independent regret bound. Due to space limit, we defer it to Appendix \ref{sec:app_unknown_inst_indep_bound}.
Additional, we show that the preference known case can be solved by PRUCB-UP as a special case, achieving a regret of $O(K \delta \log T)$, please see Appendix \ref{sec:app_up_bd_stat_known} for details. 

% The regret term by preference estimation errors is controlled by a constant related to objective dimension $D$ and $\ell_1$-norm bound $\delta$ of preferences.
% Theorem \ref{theorem:up_bd_stat} implies that the PRUCB-SPM without any prior knowledge of user preferences involves the upper-bounded regret of $\mathcal{O} \left( \delta \log T \right)$.
% Notably, the dominant term of regret achieves very close performance ($ (1 + 1/\sqrt{D})^2 $ times worse) with that in known-case. For regret term induced by preferences estimation error, it can be controlled by constant, which is related to the dimension $D$ and $L_1$-norm bound $\delta$ of preferences.
\end{remark}


To prove Theorem \ref{theorem:up_bd_stat}, the main difficulty lies in decoupling and capturing the effect of the joint error from both reward estimation and preference estimation on the final regret. 
To address this, we introduce a tunable parameter $\epsilon$ to quantify the accuracy of preference estimation $\boldsymbol{\hat{c}}_t$, and decompose suboptimal actions into two disjoint sets: 
(1) suboptimal pulls under sufficiently precise preference estimation and (2) suboptimal pulls under imprecise preference estimation.
For set (1), we demonstrate that it can be transferred to a preference-known instance with a diminished overall-reward gap to the optimal arm. For set (2), we transfer the original set with joint error to a preference estimation deviation event using Lemma \ref{lemma: error_distance} (Appendix \ref{sec:app_pr_up_bd_stat}), making it more tractable. 
% accounting for two regret terms of $R^r_T$ and $R^c_T$. 
% The derivation of $R^r_T$ relies on Proposition \ref{prop: N_known_changing} in Appendix \ref{sec:app_pr_up_bd_stat}, which characterizes the policy behavior under accurate preference estimation updates. The derivation of $R^c_T$ relies on Lemma \ref{lemma: error_distance} in Appendix \ref{sec:app_pr_up_bd_stat} to 
% transfer the original set with joint error to a preference estimation deviation event, making it more tractable. 
Please refer to Appendix
\ref{sec:app_up_bd_stat} for the full proof.