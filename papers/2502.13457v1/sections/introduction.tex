\section{Introduction}
\label{sec: intro}

% The multi-armed bandit (MAB) problem is a sequential decision-making framework where users choose arms from a set and receive rewards based on their selections at each time step. 
% The goal is to maximize cumulative rewards or minimize regret, defined as the difference between the rewards from the optimal arm and those actually received.
% Algorithms for MAB must manage the balance between exploration (testing various arms) and exploitation (favoring the best-performing arm) to optimize performance, as excessive exploration can reduce returns, while too much exploitation can overlook potentially better options.
% The MAB framework has emerged as a standard model for a range of practical applications, including online advertising~\cite{nuara2018combinatorial, geng2020online, avadhanula2021stochastic}, recommendation systems~\cite{zeng2016online, ding2021hybrid}, and the design of medical treatments~\cite{duran(D2)018contextual, varatharajah2022contextual}.
Multi-objective multi-armed bandit (MO-MAB) problem is an important extension of the multi-armed bandits (MAB)~\cite{drugan2013designing}. In MO-MAB problems each arm is associated with a $D$-dimensional reward vector.
In this environment, objectives could conflict, leading to arms that are optimal in one dimension, but suboptimal in others.
% This setup is particularly relevant in a variety of real-world applications where user preferences may vary. Consider the problem in which users would like to select a hotel. In such a problem, luxury hotels offer excellent facilities but come with high prices, while ordinary hotels have more modest facilities but are cheaper. 
A natural solution is utilizing Pareto ordering to compare arms based on their rewards~\cite{drugan2013designing}. Specifically, for any arm $i \in [K]$, if its expected reward $\boldsymbol{\mu}_i$ is non-dominated by that of any other arms, arm $i$ is deemed to be Pareto optimal. 
The set containing all Pareto optimal arms is denoted as Pareto front $\mathcal{O}^*$.
Formally, $\mathcal{O}^* = \{i \mid \boldsymbol{\mu}_j \not\succ \boldsymbol{\mu}_i, \forall j \in [K]\setminus i \}$, where 
$\boldsymbol{u} \succ \boldsymbol{v}$ holds if and only if $\boldsymbol{u}(d) > \boldsymbol{v}(d), \forall d \in [D]$.
% $\mathcal{O}^*$, i.e., $\mathcal{O}^* = \{i \mid \forall j \in [K]\setminus i, \exists d \in [D], s.t., \mu_i(d) \geq \mu_i(d) \}$
The performance is then evaluated by Pareto regret, which measures the cumulative minimum distance between the learner's obtained rewards and rewards of arms within $\mathcal{O}^*$ \cite{drugan2013designing}. 
% However, simply obtaining a solution that has good Pareto regret does not take into account the fact that individual users would like to pick the hotel that matches their specific needs (e.g., one user may give a higher preference to quality, while another user may give a higher preference to affordibility). This means that user preferences need to be accounted for in the problem set up in order to choose the right solution on the Pareto front. This is the focus of this paper. 
However, simply obtaining a solution that has good Pareto regret does not take into account the fact that individual users would like to pick the choice that matches their specific needs.
As the example depicted in Fig. \ref{fig:intro}, given multiple Pareto optimal restaurants, one user may give a higher preference to quality, while another user may give a higher preference to affordibility. This means that \emph{user preferences} need to be accounted for in the MO-MAB problem set up in order to choose the right solution on the Pareto front $\mathcal{O}^*$. This is the focus of this paper. 

\begin{figure*}[t]
    \centering    
    \includegraphics[width=\textwidth]{figures/intro.pdf}
    \caption{
    A scenario of users interacting with a conversational recommender for restaurant recommendation. 
    (a) Recommender achieves Pareto optimality but receives low rating from user. 
    (b) Recommendations with high users' ratings when the recommender captures users' preferences and aligns optimization with preferences.
    }
    \label{fig:intro}
\end{figure*}

Numerous MO-MAB studies have been conducted but \textbf{most of them achieve Pareto optimality via an arm selection policy that is uniform across all users}, which we refer to as a \emph{global policy}. 
Specifically, one representative line of research focuses on efficiently estimating the entire Pareto front $\mathcal{O}^*$, and the action is \emph{randomly} chosen on the estimated Pareto front \cite{drugan2013designing, turgay2018multi, lu2019multi, drugan2018covariance, balef2023piecewise}.
Another line of research transforms the $D$-dimensional reward into a scalar using a scalarization function, which targets a specific Pareto optimal arm solution without the costly estimation of entire Pareto front \cite{drugan2013designing, busa2017multi, mehrotra2020bandit, xu2023pareto}. These studies construct the scalarization function in a user-agnostic manner, causing the target arm solution to remain the same across different users.
% We first begin with a quick overview of works that aim to minimize the Pareto regret over some horizon $T$. 
% One representative line of research is strategically approximating the entire Pareto front $\mathcal{O}^*$ and employing a \emph{uniform arm selection policy} upon estimated Pareto front for Pareto optimality and fairness \cite{drugan2013designing, turgay2018multi, lu2019multi, drugan2018covariance, balef2023piecewise}.
% In practice, determining the entire Pareto front can be costly. An alternative approach is to directly target a specific solution on the Pareto front through scalarization, which transforms the $D$-dimensional reward into a scalar. Scalarized-UCB \cite{drugan2013designing} addresses this by uniformly selecting a scalarization function from a set of weighted sum functions in each round, aiming to achieve Pareto optimality and fairness. 
% \cite{busa2017multi} transfer the $D$-dimensional reward into Generalized Gini Index (GGI) through weighted-scalarization function, of which the weights can be computed by solving a linear program problem.
% The arm minimizes the GGI score is selected as it guarantees the Pareto optimality.
% \cite{mehrotra2020bandit} adopt this GGI-based approach for music streaming recommendation. 
% \cite{xu2023pareto} proposes to randomly select one dimension $d \in [D]$ as the optimization objective at the outset, reducing the problem to a standard MAB, since the arm that achieves optimality in one dimension must also be Pareto optimal.
% All the aforementioned studies achieve Pareto optimality via a \emph{global arm selection policy} across all users. 
However, \emph{simply achieving Pareto optimality using a global policy may not yield favorable outcomes, since, as mentioned earlier, users often have diverse preferences across different objectives. }
Consider the following scenario depicted in Fig.~\ref{fig:intro}(a), where two users with distinct preferences interact with a conversational recommender to find a nearby restaurant for dinner. The upper section lists restaurant options, each associated with multi-dimensional rewards (e.g., price, taste, service), while the lower section shows the dialogues and users' reward ratings for the recommendations.
Clearly, restaurants A, B, and C are Pareto optimal, as none of their rewards are dominated by others. 
Previous research using a global policy would either randomly recommend a restaurant from A, B, or C, or select one based on a fixed global criterion to achieve Pareto optimality. 
However, while recommending a restaurant like B might lead to positive feedback from user-1, it is likely to result in a low reward rating from user-2, who prefers an economical meal, since restaurant B is expensive.
In contrast, Fig.~\ref{fig:intro}(b) illustrates that when the system accurately captures user preferences (e.g., user-1 prefers a tasty meal, while user-2 prefers a cheap meal), it can select options more likely to receive positive reward ratings from both users.
\emph{Therefore, we argue that optimizing MO-MAB should be customized based on the user preferences rather than solely aiming for Pareto optimality with a global policy.}


To fill this gap, we introduce a formulation of MO-MAB problem, where each user is associated with a $D$-dimensional \emph{preference vector}, with each element representing the user's preference for the corresponding objective. 
Formally, in each round $t$, user incurs a stochastic preference $\boldsymbol{c}_t \!\in\! \mathbb{R}^D$.
The player selects an arm $a_t$ and observes a stochastic reward $\boldsymbol{r}_{a_t,t} \!\in\! \mathbb{R}^D$.
We define the scalar \emph{overall-reward} as the inner product of arm reward $\boldsymbol{r}_{a_t,t}$ and user preference $\boldsymbol{c}_t$. The learner's goal is to maximize the overall-reward accrued over a given time horizon.
For performance evaluation, we define the regret metric as the cumulative expected gap related to the overall-reward. 
% which is more general than the conventional Pareto regret.
We term this problem as \emph{Preference-Aware MO-MAB} (PAMO-MAB).
% \emph{To our best knowledge, this is the first work explicitly showcase the fundamental impact of user's preferences in the regret optimization and the necessity of developing preference-aware methods.}


% This setup further prompts three fundamental questions: 

% \emph{1. If user's preferences are known to the learner, how would they effect the learning process?
% 2. If preferences information is unknown, how to effectively model the user's preferences and minimize the regret?
% 3. Can we design algorithms with sub-linear regret for PAMO-MAB? 
% }

% In this paper, we explore stochastic PAMO-MAB under three practical scenarios and provide affirmative answers to the above questions. 


While interactive user modeling and customized optimization cross multiple objectives present promising experimental results in some areas including recommendation \cite{xie2021personalized}, ranking \cite{wanigasekara2019learning}, and more \cite{reymond2024interactively}, there are no theoretical studies on MO-MAB customization under explicit user preferences. 
Particularly, two open questions remain:
\emph{(1) how to develop provably efficient algorithms for customized optimization under different preference structure (e.g., unknown preference, hidden preference)?}
\emph{(2) how does the additional user preferences impact the overall performance?}

Our contributions are summarized as follows.
\begin{itemize}[leftmargin=*]
\item
We make the first effort to address the open questions above. 
Motivated by real applications, we consider the PAMO-MAB problem under two preference structures: unknown preference with feedback, and unknown preference without feedback (hidden preference), with tailored algorithms that are proven to achieve near-optimal regret in each case.
The expressions of our results are in an explicit form that capture a clear dependency on preference.
\emph{To the best of our knowledge, this is the first work that explicitly showcases the fundamental impact of user preference in the regret optimization of MO-MAB problems.}

\item 
For the unknown preference case, we propose an near-optimal algorithm PRUCB-UP incorporating two new designs: \emph{Preference estimation} and \emph{Preference-aware optimization}, to enable effective preference capture and optimization under preference-centric customization.
For regret analysis, we introduce a novel approach using a tunable parameter $\epsilon$ to decompose the suboptimal actions into two disjoint sets, addressing the joint impact of preference and reward estimation errors on regret.


\item 
For the hidden preference case, we propose PRUCB-HP, a novel near-optimal algorithm that addresses the unique challenges of hidden PAMO-MAB with two key designs: (1) A weighted least squares-based hidden preference learner, with weights set as the inverse squared $\ell_2$-norm of reward observations, to resolve the random mapping issue caused by random preferences, and (2) A \emph{dual-exploration} policy with novel bonus design to balance the trade-off between \emph{local exploration} for identifying better reward arms and \emph{global exploration} for refining preference learning.



% \item 
% \vspace{-2pt}
% \textbf{Extensive experimental evaluations.}
% % We propose different algorithms to handle PAMO-MAB under corresponding structures, which have the potential to be used in various online applications, by capturing the user's preferences structures therein in an online manner. \ms{How is this sentence related to "Extensive experimental evaluations"?}
% We conduct extensive experiments of our algorithms under three difference preference structures. The results clearly validate the effectiveness of our algorithms in online preference and rewards estimations, and overall-reward optimization.

\end{itemize}
