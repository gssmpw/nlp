\section{The Case with Hidden Preference}
\label{sec: hidden}

\begin{figure}[t]
% \vskip -0.15in
\begin{center}
\centerline{\includegraphics[width=0.55\columnwidth]{figures/s3.pdf}}
\end{center}
\vskip -0.2in
\caption{A scenario of user's preference feedback is not explicitly provided.}
\label{fig: s3}
\end{figure}

Next, we consider another practical scenario where only feedback on the reward and overall reward is observable, while preference feedback is not provided. For instance, in hotel surveys, customers often provide ratings on specific objectives (e.g., price, location, environment, amenities) along with an overall rating (as depicted in Fig. \ref{fig: s3}). In such cases, user preferences can be inferred from the latent relationship between the overall rating and the individual objective ratings.
Formally, in each round $t$, the learner selects an arm $a_t \!\in\! [K]$, and observes the reward vector $\boldsymbol{r}_{a_t} \!\in\! \mathbb{R}^{D}$, and the corresponding overall-reward score:
\begin{equation}
\label{eq:g_at}
\textstyle
g_{a_t,t} = \Phi(\boldsymbol{c}_t, \boldsymbol{r}_{a_t, t}) = \boldsymbol{c}_t^{\top} \boldsymbol{r}_{a_t, t} \in \mathbb{R}
\end{equation}
Within this framework, we adhere to the original Assumption~\ref{assmp: all_1} regarding rewards. It is worth noting that, in many real-world applications like hotel rating systems, the overall rating often shares the same scale as individual objective ratings.  
Therefore, we assume in this problem that the bound on the overall reward is identical to that of the individual rewards. This introduces one additional assumption and one revised assumption, as outlined below:
\begin{assumption}
\label{assmp: hpm_2}
For $t \in [T]$, $a_t \in [K]$, the overall-reward score satisfies 
$g_{a_t,t} \in [0,1]$.
\end{assumption}

\begin{assumption}
\label{assmp: hpm_3}
For $t \in [T]$, the stochastic preference is bounded and satisfies $\Vert \boldsymbol{c}_t \Vert_1 \leq 1$. Without loss of generality, we assume $\boldsymbol{c}_t(d)$ is $R$-sub-Gaussian\footnote{By Hoeffdingâ€™s lemma, for any $X \in [a,b]$ almost surely, $X$ is a $R$-sub-Gaussian random variable with $R$ at most $(b-a)/2$.}, $\forall d \in [D]$.
\end{assumption}


% \begin{assumption}
% \label{assmp: hpm}
% The PAMO-MAB with hidden preference satisfies the following conditions:
% \begin{itemize}[leftmargin=*]
% \item 
% For $t \in [T]$, $a_t \in [K]$, the overall-reward score satisfies 
% $g_{a_t,t} \in [0,1]$.
% \item 
% For $t \in [T]$, the stochastic preference satisfies $\Vert \boldsymbol{c}_t \Vert_1 \leq 1$ with 
% $\boldsymbol{c}_t(d) \in [0,1], \forall d \!\in\! [D]$.
% \end{itemize}
% \end{assumption}



\subsection{Unique Challenges}
\label{sec:uniq_chllenge}
This problem introduces two unique challenges that distinguish us from previous MAB studies:

\textbf{Local Exploration \emph{vs} Global Exploration.}
Unlike traditional bandit algorithms that focus on a single goal (e.g., identifying the arm with the highest reward), the uncertainty in both preference and reward, combined with the need to infer latent preference, introduces a novel trade-off challenge: balancing \emph{global exploration} for better preference estimation and \emph{local exploration} of arm rewards:
\begin{itemize}[leftmargin=*]
\item \emph{Global exploration for preferences:}
% selecting arms that diversify $r_{a_t}$ to refine feature space for preference learning.
Selecting arms that reduce uncertainty in poorly explored direction of the feature space, refining the model for preference learning.
% Selecting diverse arms to gather information about the relationship between $\boldsymbol{r}_{a_t}$ and $g_{a_t}$, reducing uncertainty across less-explored directions in feature space to refine preference learning.
\item \emph{Local exploration for rewards:}
% Selecting arms that balance exploring their rewards and exploiting empirically good arms.
Selecting arms to reduce uncertainty for specific individual arm reward estimate,  while balancing exploiting empirically high reward arms.
\end{itemize}


\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.65\columnwidth]{figures/lr_exp_1.pdf}}
\end{center}
\vskip -0.1in
\caption{A 2-dimensional hidden preference PAMO-MAB toy example with mean preference $\overline{\boldsymbol{c}} = [0.5, 0.5]$, illustrating preference estimate $\hat{\boldsymbol{c}}$ via linear regression using reward data from (a) Arm-1 (dominated mean reward: $[0.2,0.2]$) and (b) Arm-2 (Pareto-optimal mean reward: $[0.8,0.8]$).
}
\label{fig: lr_1}
\end{figure}

Note that these two learning objectives may conflict, as arms with high rewards might lack sufficient information for latent preference learning and could even degrade estimation performance (further discussed in the second challenge). 
This can also be verified by Fig. \ref{fig: lr_1}, where 80 samples of $[\boldsymbol{r}_{t}, g_{t}]$ are collected by repeatedly pulling an arm, and preference $\boldsymbol{\hat{c}}$ is estimated using linear regression. Here, $\boldsymbol{c}_t$ at each step follows a Gaussian distribution with a mean of $[0.5, 0.5]$. The results demonstrate that samples from suboptimal Arm-1 (Fig. \ref{fig: lr_1}a) significantly outperform those from Pareto-optimal Arm-2 (Fig. \ref{fig: lr_1}b) in preference estimation.
This necessitates an exploration policy that effectively addresses both global and local learning objectives.

\textbf{Random Mapping from $\boldsymbol{r}_t$ to $g_t$.}
In the hidden preference case, the observed overall rewards are generated through a \emph{random mapping} of rewards.
Specifically,
$g_{a_t,t} = (\boldsymbol{\overline{c}} + \boldsymbol{\zeta}_t)^{\top} \boldsymbol{r}_{a_t,t} = \boldsymbol{\overline{c}}^{\top} \boldsymbol{r}_{a_t,t} + \boldsymbol{\zeta}_t^{\top} \boldsymbol{r}_{a_t,t}$, where $\boldsymbol{\zeta}_t = \boldsymbol{c}_t - \boldsymbol{\overline{c}} \in \mathbb{R}^D$ is an independent random noise vector. 
This formulation implies that the overall residual noise term $\zeta_{g,t} = \boldsymbol{\zeta}_t^{\top} \boldsymbol{r}_{a_t,t}$ is no longer independent of the input.
Consequently, standard regression models become infeasible for preference estimation, as they rely on the assumption that the residual noise in the output is independent of the input.

Additionally, the magnitude of overall residual noise is a monotonically non-decreasing function w.r.t each reward objective, i.e., 
$\Vert \zeta_{g,t}(\boldsymbol{r}_{i}) \Vert \leq \Vert \zeta_{g,t}(\boldsymbol{r}_{j}) \Vert$ iff $\boldsymbol{r}_{i} \preceq \boldsymbol{r}_{j}$.
This explains why suboptimal arms typically outperform Pareto-optimal arms for preference estimation in Fig. \ref{fig: lr_1}, as selecting profitable arms tends to amplify the residual error, thereby degrading preference learning.
Thus, a tailored latent preference estimator is essential to mitigate the expanding error w.r.t the reward and ensure effective preference learning.



\subsection{Our Algorithm}
To this end, we propose a novel PRUCB-HP method (Algorithm \ref{alg:PRUCB_HP}) involving two key designs as follows.

\textbf{Key design I: WLS-Preference Estimator.}
As we have seen before, the randomness of preference $\boldsymbol{c}_t$ leads to the overall residual noise $\zeta_{g,t}$ be a function w.r.t, input reward $\boldsymbol{r}_t$. Moreover, larger input rewards $\boldsymbol{r}_t$ result in greater corruption from the residual noise.
To resolve this, we employ a weighted least-squares (WLS) estimator for preference learning. Specifically, our algorithm assigns a weight $w_t$ to each observed sample and estimates the unknown preference using weighted ridge regression:
\[
\textstyle
\hat{\boldsymbol{c}}_t \xleftarrow{} \argminA_{\boldsymbol{c} \in \mathbb{R}^{D}} \lambda \Vert \boldsymbol{c} \Vert_2^2 
+
\sum_{\ell=1}^{t-1} w_{\ell} ( {\boldsymbol{c}}^{\top} \boldsymbol{r}_{a_{\ell},\ell} - g_{a_{\ell},\ell})^2,
\]
where $\lambda$ is the regularization parameter. Above optimization problem has a closed-form solution as:
\begin{equation}
\label{eq:hpm_c_t}
\textstyle
\boldsymbol{\hat{c}}_{t} = \boldsymbol{V}_{t-1}^{-1} \sum_{\ell=1}^{t-1} w_{\ell} g_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell},\ell},
% , \text{and } \boldsymbol{V}_{1} = \lambda \boldsymbol{I}
\end{equation}
where the Gram matrix $\boldsymbol{V}_{t-1} = \lambda \boldsymbol{I} + \sum_{\ell=1}^{t-1} w_{\ell} \boldsymbol{r}_{a_{\ell},\ell} \boldsymbol{r}_{a_{\ell},\ell}^{\top}$.

\input{pseudo_code/algo3}

Inspired by \cite{zhou2021nearly} using the inverse of the noise variance as weight 
% to normalize the noise 
for tight variance-dependent regret guarantee, 
% To this end, 
we define the weight as the inverse of the squared $\ell_2$-norm of the reward: $w_t = \omega / \Vert \boldsymbol{r}_{a_t,t} \Vert_2^2$, where $\omega > 0$ is a threshold parameter guaranteeing $w_t \geq 1$. 
Intuitively, it ensures samples with high rewards will be assigned smaller weights to reduce the influence of potentially large residual noises, while samples with low rewards receive larger weights to ensure their contribution to the estimation.

To see how our choice of weight can tackle the \emph{random mapping} issue, we first define 
$\boldsymbol{r}_{a_t,t}^{\prime} = \sqrt{w_t} \boldsymbol{r}_{a_t,t}$, 
$g_{a_t,t}^{\prime} = \sqrt{w_t} g_{a_t,t}$, then the original formula Eq. \ref{eq:g_at} can be rewrite as
\[
\textstyle
\sqrt{w_t} \cdot g_{a_t,t} = \sqrt{w_t} \cdot \boldsymbol{c}_t^{\top} \boldsymbol{r}_{a_t, t} = \sqrt{w_t} \cdot (\boldsymbol{\overline{c}} + \boldsymbol{\zeta_t})^{\top} \boldsymbol{r}_{a_t, t}
\]
\begin{equation}
\label{eq:g_t_2}
\textstyle
\implies
g_{a_t,t}^{\prime} = \boldsymbol{\overline{c}}^{\top} \boldsymbol{r}_{a_t, t}^{\prime} + \sqrt{w_t} \cdot \boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t}.
\end{equation}
For term $\sqrt{w_t} \boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t}$, we have the following lemma:
\begin{lemma}
\label{lemma: R_normed}
Under Assumption \ref{assmp: hpm_3}, the random variable $\sqrt{w_t} \boldsymbol{\zeta}_{t}^{\top} \boldsymbol{r}_{a_{t}, t}$ is sub-Gaussian with constant $R^{\prime} = \sqrt{\omega} R$.
\end{lemma}
The proof is available in Appendix \ref{sec: app_pf_lemma_R_normed}. By above lemma, we observe that with the designed weight, the original random mapping regression problem is transferred into a new formula as (\ref{eq:g_t_2}). Specifically, the output $g_{a_t,t}^{\prime}$ is mapped from $\boldsymbol{r}_{a_t,t}^{\prime}$ via a \emph{fixed} vector $\boldsymbol{\overline{c}}$ with a normed $R^{\prime}$-sub-Gaussian residual noise, where $R^{\prime} = \sqrt{\omega} R$, independent of the input. 


% While weighted ridge regression is not new and has been used in prior work on bandits \cite{russac2019weighted, zhou2021nearly, he2022nearly}, our setting, motivation and weight design are fundamentally different.

% Assign weights to each sample inversely proportional to the noise variance, which is proportional to the 2-norm of samples $\Vert \boldsymbol{r} \Vert_2^2$.

\textbf{Key design II: Dual-Exploration Policy.}
As discussed earlier, there is a new global-local exploration dilemma in our setting. 
On the one hand, the algorithm must focus on local exploration by selecting optimistically profitable arms to discover better ones. Simultaneously, it must globally explore diverse arms to gather information about the relationship between $\boldsymbol{r}_t$ and $g_t$ for modeling the hidden preference.

To resolve this, we design an \emph{optimistic dual-exploration policy} by incorporating a \emph{preference-driven bonus} and \emph{reward-driven bonus} within the preference-aware optimization framework for trade-off.
The optimistic policy is defined as
\begin{equation}
\label{eq:a_t_hidden}
\textstyle
a_t \leftarrow \argmaxA_{i \in [K]} \Phi ( \boldsymbol{\hat{c}}_t,  \hat{\boldsymbol{r}}_{i,t}) + B_{i,t}^{r} + B_{i,t}^{c},
\end{equation}
where $\Phi$ is the inner product aggregation function, $B_{i,t}^{r}$ and $B_{i,t}^{c}$ are the dual-exploration bonus terms. We detail the design of these bonus terms below and will later theoretically demonstrate in Section \ref{sec:theoretical_result_hidden} how they establish a tight UCB for the expected overall reward, ensuring the effectiveness of the optimistic policy in (\ref{eq:a_t_hidden}).

\underline{\emph{Reward Bonus}} $B_{i,t}^{r}$.
The reward bonus term explicitly encourages local exploration of arms with potentially high rewards, for the principle of optimism in face of uncertainty. Specifically, the bonus $B_{i,t}^{r}$ is formulated as a reward uncertainty-aware regularization term:
\begin{equation}
B_{i,t}^{r} = \rho_{i,t}^{\alpha} \Vert \hat{\boldsymbol{c}}_t \Vert_1.
\end{equation}
$\rho_{i,t}^{\alpha} = \sqrt{ \log(t/\alpha) / \max \{1, N_{i, t } \} }$ represents the standard Hoeffding bonus that quantifies the uncertainty in the reward estimates for each arm, ensuring that arms with higher uncertainty or lower exploration counts will be prioritized.

\underline{\emph{Preference Bonus}} $B_{i,t}^{c}$.
The preference bonus term aims to encourage the exploration of arms that reduce uncertainty in preference estimation.
In previous bandit studies \cite{abbasi2011improved, zhao2020simple, he2022nearly} involving linear coefficient ($\theta^*$) learning, it has been shown that $\beta \Vert \boldsymbol{x}_i \Vert_{\boldsymbol{V}^{-1}}$ provides a tight confidence bonus for the payoff of arm $i$, where $\beta$ is the confidence set radius for coefficient estimation, and $\boldsymbol{x}_i$ is the observable arm feature. In information theory, $\Vert \boldsymbol{x}_i \Vert_{\boldsymbol{V}^{-1}}$ also reflects entropy reduction in the model posterior, and is used to measure the estimator uncertainty improvement contributed by the chosen action $\boldsymbol{x_i}$ \cite{li2010contextual}.

However, such design is not feasible in our setting, as the exact reward $\boldsymbol{r}_{a_t,t}$ is revealed only after pulling the arm $a_t$, making the actual information gain $\Vert \boldsymbol{r}_{a_t,t} \Vert_{\boldsymbol{V}^{-1}_{t-1}}$ from arm $a_t$ unpredictable beforehand.
To resolve this problem, we introduce a \emph{pseudo information gain} term, defined as $\Vert \boldsymbol{\hat{r}}_{i,t} + \rho_{i,t}^{\alpha} \boldsymbol{e} \Vert_{\boldsymbol{V}^{-1}_{t-1}}$, where $\rho_{i,t}^{\alpha}$ is the standard Hoeffding bonus. And then the preference bonus is set as 
\begin{equation}
B_{i,t}^{r} = \beta_t \cdot \Vert \boldsymbol{\hat{r}}_{i,t} + \rho_{i,t}^{\alpha} \boldsymbol{e} \Vert_{\boldsymbol{V}^{-1}_{t-1}},
\end{equation}
where the scalar $\beta_t$ is the confidence radius of the preference estimation we will give in Lemma \ref{lemma:g_estimator_upper_conf_bd}.
Intuitively, this pseudo information gain term captures the potential improvement in the preference estimator that could be achieved by \emph{optimistically} selecting arm $i$ based on its reward estimation.
In this way, it explicitly encourages global exploration of arms that reduce uncertainty in preference estimation while performing local exploration.

\subsection{Theoretical Results}
\label{sec:theoretical_result_hidden}

In this section, we provide theoretical guarantees for the PRUCB-HP algorithm.
We first characterize the estimation error of $\boldsymbol{\hat{c}}_t$ w.r.t $\overline{\boldsymbol{c}}$ by WLS preference estimator below.

\begin{lemma}
\label{lemma:c_estimator_conf_bd}
Under Assumption \ref{assmp: hpm_3}, for any $0 < \alpha <1$, $\omega>0$, with a probability at least $1-\vartheta$, the preference estimator $\hat{c}_t$ in Algorithm \ref{alg:PRUCB_HP} verifies for all $t \in [1,T]$:
\[
\Vert \hat{\boldsymbol{c}}_t - \boldsymbol{\overline{c}} \Vert_{\boldsymbol{V}_{t-1}} 
\leq
R \sqrt{\omega D \log\big((1 + \omega t/\lambda)/\vartheta\big)} + \sqrt{\lambda}.
\]
\end{lemma}
Please see Appendix \ref{sec:app_proof_lemma_c_estimator_conf_bd} for the proof. This estimate confidence bound essentially implies the effectiveness of WLS preference estimator with the designed weight to handle the random-mapping issue in our problem. With this in hand, we can then derive an upper confidence bound for the expected overall reward.

\begin{lemma}
% [Upper Confidence Bound of Expected Overall Reward]
\label{lemma:g_estimator_upper_conf_bd}
Set $\beta_t = \sqrt{\omega D \log\big((1 + \omega t/\lambda)/\vartheta\big)} + \sqrt{\lambda}$, then for any $i \in [K]$ and any $t > 0$, with probability at least equal to $1 - \vartheta - D\alpha^2/t^2$, we have
\begin{equation}
\label{eq:ucb_hidden}
\boldsymbol{\overline{c}}^{\top} \boldsymbol{\mu}_{i} 
\leq
\boldsymbol{\hat{c}}_{t}^{\top} \boldsymbol{\hat{r}}_{i,t} 
+
B_{i,t}^{r}
+
B_{i,t}^{c},
\end{equation}
with 
$B_{i,t}^{r} = \rho_{i,t}^{\alpha} \Vert \hat{\boldsymbol{c}}_t \Vert_1$
and 
$B_{i,t}^{c} = \beta_t \left \Vert \hat{\boldsymbol{r}}_{i,t} \!+\! \rho_{i,t}^{\alpha} \boldsymbol{e} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}}$
as the bonus terms for dual-exploration.
\end{lemma}

Please see Appendix \ref{sec:app_pf_lemma_g_estimator_upper_conf_bd} for the proof. Lemma \ref{lemma:g_estimator_upper_conf_bd} essentially suggests an upper confidence bound of the expected reward, which is adopted in our dual-exploration policy (\ref{eq:a_t_hidden}).
Notably, the two bonus terms strike a balance between local and global explorations while guaranteeing optimization under the principle of optimism in face of uncertainty.

% \label{eq:ucb_hidden}

% \begin{figure}[t]
% \vspace{-8pt}
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figures/lr_exp_2.pdf}}
% \end{center}
% \vspace{-12pt}
% \caption{A scenario of user's preference feedback is not explicitly provided.}
% \label{fig: s3}
% \vskip -0.1in
% \end{figure}


\begin{theorem}
\label{theorem:up_bd_hiden}
For PAMO-MAB with hidden preference, for any $\lambda > 0$, by setting
$\alpha = \sqrt{\frac{12 \vartheta}{KD(D+3) \pi^2}}$,
$\omega \geq D$,
$\beta_t = \sqrt{\omega D \log\big((1 + \omega T/\lambda)/\alpha\big)} + \sqrt{\lambda}$, 
with probability greater than $1- 2\vartheta$, Algorithm \ref{alg:PRUCB_HP} has
\[
\small
\begin{aligned}
\textstyle
R(T) 
=
& O\Big(
\textcolor{cyan}{
DR \sqrt{ \omega T \log^{2} \Big( \big(1 + \omega T/\lambda\big)/\vartheta \Big)}
}
+ 
\textcolor{teal}{
\frac{DR}{\sqrt{\lambda}} \sqrt{ \omega DK T \log^2 \left(\big( 1 + \omega T/\lambda \big)/\vartheta) \right)}
}
+ 
\textcolor{orange}{
\sqrt{ KT \log \left( T/\vartheta \right)}
}
+
M
\Big),
\end{aligned}
\]
\begin{small}
with $M = \left\lfloor \min \big \{ t^{\prime} \mid t  \sigma^2_{r \downarrow} + \lambda \geq 2D \omega \sqrt{Kt\log \frac{t}{\alpha} }, \forall t \geq t^{\prime} \big \} \right \rfloor$\footnote{Since $\sigma^2_{\boldsymbol{r} \downarrow} \!\in\! \mathbb{R}^{+}$, 
$\lim_{t \rightarrow \infty} 2D \omega \sqrt{Kt \log \frac{t}{\alpha} }/\big(\sigma^2_{r \downarrow}t\big) = \lim_{t \rightarrow \infty} C_1 \sqrt{ (\log t - C_2)/t} = 0$, as $\sqrt{\log t}$ grows much slowly compared to $\sqrt{t}$. Hence $M$ exists for sufficiently large $t^{\prime}$.}.
\end{small}
Here the first term represents regret from preference estimation error, the third from reward estimation error, and the second from the combined error of both.
\end{theorem} 

Please see Appendix \ref{sec: app_pf_thm_up_bd_hiden} for the proof. The key difficulty of the proof is to upper-bound the accumulative preference bonus $\sum_{t}B_{i,t}^{c}$. Specifically, we need to quantify the weighted $\ell_2$-norm of the empirical estimation $\boldsymbol{\hat{r}}_{a_t,t}$ with weighting matrix $\boldsymbol{V}_{t}$ constructed by the true reward $\boldsymbol{r}_{a_t,t}$ instead. 
This inconsistency renders the classical induction method \cite{abbasi2011improved} for deriving $\log (\frac{\det \boldsymbol{V}_{T}}{\det \boldsymbol{V}_{0}})$ infeasible for upper-bounding $\sum_{t} \Vert \boldsymbol{\hat{r}}_{a_t,t} \Vert_{\boldsymbol{V}_{t-1}^{-1}}^2$. 
To resolve this, we first transfer $\Vert \boldsymbol{\hat{r}}_{a_t,t} \Vert_{\boldsymbol{V}_{t-1}^{-1}}$ to $\Vert \boldsymbol{\mu}_{a_t} \Vert_{\boldsymbol{V}_{t-1}^{-1}}$. Then we show that for sufficiently large $t$, $a \Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}[\boldsymbol{V}_{t-1}]^{-1}}$ serves as an upper bound for $\Vert \boldsymbol{\mu}_{a_t} \Vert_{\boldsymbol{V}_{t-1}^{-1}}$ with constant $a > 1$ (see Lemma \ref{lemma: hidden_sum_reg_c_expectation_bd}).
This allows us to use $a \Vert \boldsymbol{\mu}_{a_t} \Vert_{\mathbb{E}[\boldsymbol{V}_{t-1}]^{-1}}$ as an upper-bound, where a new recursion relationship between $\mathbb{E}[\boldsymbol{V}_{t-1}]$ and $\boldsymbol{\mu}_{a_t}$ can be guaranteed, enabling us to bound $\log (\frac{\det \mathbb{E}[\boldsymbol{V}_{T}]}{\det \mathbb{E}[\boldsymbol{V}_{0}]})$ via induction, which can further be bounded by slightly modifying existing techniques in linear bandits.

\begin{remark}
Theorem \ref{theorem:up_bd_hiden} shows that, even without explicit preference feedback, PRUCB-HP achieves sub-linear regret through carefully designed mechanisms for preference adaptation.
In particular, for $t \geq M$, where $M$ is a constant independent of $T$, the regret asymptotically scales as $\tilde{O} ( D \sqrt{T} )$.
\emph{To the best of our knowledge, this is the first result characterizing the performance of PAMO-MAB with hidden preference in the literature.}
\end{remark}


% \begin{figure}[t]
% \vspace{-5pt}
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=1\columnwidth]{figures/expriments_main.pdf}}
% \end{center}
% \vspace{-12pt}
% \caption{(a) Regret comparison under unknown preference. (b) Regret comparison under hidden preference. 
% }
% \label{fig: exp1_sample}
% \vskip -0.1in
% \end{figure}


\begin{figure}[t]
\begin{center}
    \subfigure[Unknown preference case]{
        \includegraphics[width=0.35\columnwidth]{figures/expriments_main_1.pdf}
        \label{fig: exp1_sample_1}
    }
    % \hfill
    \subfigure[Hidden preference case]{
        \includegraphics[width=0.35\columnwidth]{figures/expriments_main_2.pdf}
        \label{fig: exp1_sample_1}
    }
    \caption{Regret comparison of our proposed PRUCB with other benchmarks under different preference environments, where our methods outperforms other methods significantly.}
\label{fig: exp1_sample}
\end{center}
\end{figure}


% \begin{figure}[t]
% \vspace{-5pt}
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=1\columnwidth]{figures/expriment_protocol.pdf}}
% \end{center}
% \vspace{-12pt}
% \caption{A 2-dimensional hidden preference PAMO-MAB toy example
% }
% \label{fig: exp1_sample}
% \vskip -0.1in
% \end{figure}

% \begin{figure*}[t]
%     \centering    
%     \includegraphics[width=1\textwidth]{figures/expriment_protocol.pdf}
%     \caption{(a) Users switching protocol for experimental evaluation of hidden preference and multi-objective reward modelings. (b) One real-world example of the experimental protocol.
% }
% \vspace{-10pt}
%     \label{fig:experiment3_protocol}
% \end{figure*}

\section{Numerical Analysis}
We evaluate the performance of PRUCB-UP and PRUCB-HP in unknown and hidden preference environments, respectively. The PAMO-MAB instance includes 
$K$ arms and $D$ objectives, with preferences and rewards following Gaussian distributions with randomly initialized means..
(Detailed settings refer to Appendix \ref{sec:app_exp_prucb_static}). 
For the hidden preference case, we introduce a user-switching protocol to simulate practical scenarios. The environment features multiple users, each exposed to a block of arms (5 in our setup) per round. Only arms within the current block can be selected for that user. In the next round, the arm block rotates to a another user. The objective is to maximize cumulative overall ratings across all users, and performance is measured by the averaged users' regrets. A more detailed illustration is provided in Appendix \ref{sec:app_exp_hidden}.

We compare our results with other baselines including S-UCB, Pareto-UCB \cite{drugan2013designing}, S-MOSS, Pareto-TS~\cite{yahyaa2015thompson}), UCB~\cite{auer2002finite}, MOSS~\cite{audibert2009minimax} and OFUL \cite{abbasi2011improved}.
The regret is averaged across 10 trials with round $T = 5000$.
Figure \ref{fig: exp1_sample} shows that our proposed algorithms significantly outperform other competitors under both environments.
It is worth noting that for all the preference-free competitors exhibit linear regret, aligning with Proposition \ref{prop: lower_bd}, demonstrating that approaches agnostic to user preferences cannot align their outputs with user preferences, even if they achieve Pareto optimality.
For more comprehensive experimental analyses, please refer to Appendix \ref{sec:app_exp}.
