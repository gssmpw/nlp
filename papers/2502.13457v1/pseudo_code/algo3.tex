\begin{algorithm}[t]
\caption{Preference UCB with Hidden Preference (PRUCB-HP)}
\label{alg:PRUCB_HP}
\begin{algorithmic}
\State \textbf{Parameters:}
$\alpha$, $\lambda$, $\beta_t$, $\omega$.
\State \textbf{Initialization:}
$\boldsymbol{\hat{r}}_{i,1} \!\leftarrow\! [0]^D, 
N_{i, 1} \!\leftarrow\! 0, \forall i \!\in\! [K]$, 
$\boldsymbol{\hat{c}}_1 \!\leftarrow\! [\frac{1}{D}]^D$, 
$\boldsymbol{V}_0 \!\leftarrow\! \lambda \boldsymbol{I}$.
\For{$t=1, \cdots, T$}
% \vspace{-5pt}
    \State Compute reward bonus term: $B_{i,t}^{r} = \Vert \hat{\boldsymbol{c}}_t \Vert_1 \sqrt{\frac{\log{t/\alpha}}{\max\{N_{i,t}, 1\}}}, \forall i \in [K]$.
    \State Compute pseudo information gain term: $B_{i,t}^{c} = \beta_t \left \Vert \hat{\boldsymbol{r}}_{i,t} \!+\! \sqrt{\frac{\log{t/\alpha}}{\max\{N_{i,t}, 1\}}} \boldsymbol{e} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}}, \forall i \in [K]$.
    \State \textbf{Draw arm} $a_t \leftarrow \argmaxA_{i \in [K]} \hat{\boldsymbol{c}}_t^\top \hat{\boldsymbol{r}}_{i,t} + B_{i,t}^{r} + B_{i,t}^{c}$.
    \State \textbf{Observe} reward $\boldsymbol{r}_{a_t,t}$ and overall-reward $g_{a_t,t}$.
    \State \textbf{Updating phase:}
    % \Comment{(Preference-aware optimization)}
    \State \indent $N_{i, t+1} \!=\! N_{i, t} \!+\! \mathds{1}_{ \{a_{t} = i \}}$.
    \State \indent $\hat{\boldsymbol{r}}_{i,t+1} \!=\! (\hat{\boldsymbol{r}}_{i,t} N_{i, t} + \boldsymbol{r}_{a_{t}, t} \cdot \mathds{1}_{ \{a_{t} = i \}} )/N_{i, t+1}, \forall i \in [K]$.
    \State \indent $w_t = \frac{\omega}{\Vert \boldsymbol{r}_{a_t,t} \Vert_2^2}$, $\boldsymbol{V}_{t} = \boldsymbol{V}_{t-1} + w_t \boldsymbol{r}_{a_t,t}\boldsymbol{r}_{a_t,t}^{\top}$.
    \State \indent $\boldsymbol{\hat{c}}_{t+1} = \boldsymbol{V}_{t}^{-1} \sum_{\ell=1}^{t} w_{\ell} g_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell},\ell}$.
    % \Comment{(Reward estimation)}    
\EndFor
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[t]
% \caption{Preference UCB with Unknown Preference (PRUCB-UP)}
% \label{alg:PRUCB_HP}
% \begin{algorithmic}
% \State \textbf{Parameters:}
% $\alpha$, $\lambda$, $\beta_t$, $\omega$.
% \State \textbf{Initialization:}
% $\boldsymbol{\hat{r}}_{i,1} \!\leftarrow\! [0]^D, 
% N_{i, 1} \!\leftarrow\! 0, \forall i \!\in\! [K]$, 
% $\boldsymbol{\hat{c}}_1 \!\leftarrow\! [\frac{1}{D}]^D$, 
% $\boldsymbol{V}_0 \!\leftarrow\! \lambda \boldsymbol{I}$.
% \FOR{$t=1, \cdots, T$}
% % \vspace{-5pt}
%     \State Compute reward bonus term:\\ $B_{i,t}^{r} = \Vert \hat{\boldsymbol{c}}_t \Vert_1 \sqrt{\frac{\log{t/\alpha}}{\max\{N_{i,t, 1\}}}}, \forall i \in [K]$.
%     \State Compute pseudo information gain term:\\ $B_{i,t}^{c} = \beta_t \left \Vert \hat{\boldsymbol{r}}_{i,t} \!+\! \sqrt{\frac{\log{t/\alpha}}{\max\{N_{i,t, 1\}}}} \boldsymbol{e} \right \Vert_{\boldsymbol{V}_{t-1}^{-1}}, \forall i \in [K]$.
%     \State \textbf{Draw arm} $a_t \leftarrow \argmaxA_{i \in [K]} \hat{\boldsymbol{c}}_t^\top \hat{\boldsymbol{r}}_{i,t} + B_{i,t}^{r} + B_{i,t}^{c}$.
%     \State \textbf{Observe} reward $\boldsymbol{r}_{a_t,t}$ and overall-reward $g_{a_t,t}$.
%     \State \textbf{Updating phase:}
%     % \Comment{(Preference-aware optimization)}
%     \State \indent $N_{i, t+1} \!=\! N_{i, t} \!+\! \mathds{1}_{ \{a_{t} = i \}}$.
%     \State \indent $\hat{\boldsymbol{r}}_{i,t+1} \!=\! (\hat{\boldsymbol{r}}_{i,t} N_{i, t} + \boldsymbol{r}_{a_{t}, t} \cdot \mathds{1}_{ \{a_{t} = i \}} )/N_{i, t+1}, \forall i \in [K]$.
%     \State \indent $w_t = \frac{\omega}{\Vert \boldsymbol{r}_{a_t,t} \Vert_2^2}$, $\boldsymbol{V}_{t} = \boldsymbol{V}_{t-1} + w_t \boldsymbol{r}_{a_t,t}\boldsymbol{r}_{a_t,t}^{\top}$.
%     \State \indent $\boldsymbol{\hat{c}}_{t+1} = \boldsymbol{V}_{t+1}^{-1} \sum_{\ell=1}^{t} w_{\ell} g_{a_{\ell}, \ell} \boldsymbol{r}_{a_{\ell},\ell}$.
%     % \Comment{(Reward estimation)}    
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

