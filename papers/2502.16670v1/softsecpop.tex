\documentclass[5p, twocolumn, numbers, sort]{elsarticle}

\usepackage[colorlinks = true,
  linkcolor = blue,
  citecolor = blue,
  urlcolor = blue]{hyperref}

\usepackage{url}
\usepackage{balance}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{threeparttable}

\newcommand{\e}[1]{\textcolor{blue}{#1}}

\begin{document}

\title{The Popularity Hypothesis in Software Security: \\ A Large-Scale Replication
  with PHP Packages}

\author[sdu]{Jukka Ruohonen\corref{cor}}
\ead{juk@mmmi.sdu.dk}
\author[sdu]{Qusai Ramadan}
\cortext[cor]{Corresponding author.}
\address[sdu]{University of Southern Denmark, S\o{}nderborg, Denmark}

\begin{abstract}
There has been a long-standing hypothesis that a software's popularity is
related to its security or insecurity in both research and popular
discourse. There are also a few empirical studies that have examined the
hypothesis, either explicitly or implicitly. The present work continues with and
contributes to this research with a replication-motivated large-scale analysis
of software written in the PHP programming language. The dataset examined
contains nearly four hundred thousand open source software packages written in
PHP. According to the results based on reported security vulnerabilities, the
hypothesis does holds; packages having been affected by vulnerabilities over
their release histories are generally more popular than packages without having
been affected by a single vulnerability. With this replication results, the
paper contributes to the efforts to strengthen the empirical knowledge base in
cyber and software security.
\end{abstract}

\begin{keyword}
Software security, software vulnerability, replication, web applications,
software ecosystems, dependencies, downloads, installs, stars, forks
\end{keyword}

\maketitle

\section{Introduction}

The paper examines what can be called a popularity hypothesis in empirical
software security research; an assumption that a software's popularity affects
its security or insecurity. The hypothesis has been previously examined with
Java packages, the conclusion being that popularity is not a reliable indicator
of software security~\cite{Siavvas18}. The present paper revisits this
hypothesis with a large-scale analysis of open source software packages written
in the PHP programming language. Given the existing work, the paper is by
definition a replication.

% By following existing recommendations~\cite{Monin14}, one of the authors of the original, replicated paper is among the authors of the present work too.

Within the domain of replication studies, the paper can be classified as a
dependent or a conceptual replication: although both the data and methods are
different, the paper is designed with an explicit reference to previous work in
mind~\cite{Fabrigar16, Gomez14, Ruohonen15COSE}. The methodological difference
can be justified on the grounds that the replicated study was based only on
correlation analysis~\cite{Siavvas18}. In contrast, the paper uses also
regression analysis and classification algorithms, which can be argued to yield
stronger empirical evidence in general. The recently discussed generalizability
problems in empirical software engineering~\cite{Baltes22} justify the use of a
different dataset. If the hypothesis holds, or does not hold, with two distinct
datasets, there is either stronger or weaker evidence for it, respectively.

The underlying replication logic focuses on the previous work's conclusion---it
is important to know whether or not the hypothesis holds with a different
dataset and different methods because the hypothesis is rather fundamental in
many ways. It also frequently appears in popular discourse. The primary
historical example would be Microsoft Windows; not so long ago, it was commonly
discussed and speculated by both laypeople and professionals that the popularity
of the operating system was a definite factor behinds its historical
insecurity. To quote from a famous security engineering textbook, most ``malware
writers targeted Windows rather than Mac or Linux through the 2000s and 2010s as
there are simply more Windows machines to infect'', and, furthermore, the
``model replicated itself when smartphones took over the
world''~\cite[p.~296]{Anderson08}. Although there is no particular reason to
question these assertions, it is still worth remarking that they are not
actually backed by any empirical evidence in the textbook. This point motivates
a need to examine the hypothesis further empirically.

Given a distinction between ``hard'' and ``soft'' theories in empirical software
engineering~\cite{Russo19}, the popularity hypothesis is on the side of hard
theories because it presumes a causal relation between a cause (popularity) and
effect (insecurity), but the theorizing involved is still rather soft. In other
words, assuming that the hypothesis holds, the underlying explanations remain
open to a debate.

The starting point for an explanation given by what is now known as (information
or cyber) security economics would be incentives~\cite{Anderson08}. That is,
cyber criminals and other adversaries have an incentive to target popular
software products, packages, and projects, whether the targeting is about
finding vulnerabilities from those or explicitly attacking them. Then: even
though both commercial software vendors and open source software projects have
an incentive to develop secure code and provide defenses, it is frequently
discussed in the literature that software security and cyber security in general
are not prioritized because there are no pressing incentives to do
so~\cite{Ruohonen24JSS}. A slightly different theorization path involves the
famous so-called Linus law; that all bugs are shallow when there are many eyes
involved to look at code~\cite{Raymond01}. From this alternative perspective,
there are also empirical studies indicating that widely used and hence popular
open source software packages often contain more \textit{reported} bugs and
vulnerabilities than less popular packages~\cite{Davies10, Herraiz11,
  Ruohonen19EASE}. These empirical studies further reinforce the need and
motivation for a replication of the popularity hypothesis study already
mentioned~earlier. The italics placed upon the word \textit{reported} are also
worth briefly explaining. The reason for the italics is that empirical
observations cannot generally prove that something is secure, although these can
be used to make claims about insecurity~\cite{Herley17}. This fundamental point
should be kept in mind throughout the paper; only reported security issues are
observed. By implication, the popularity hypothesis in itself may not
necessarily tell everything, or in some cases even anything, about software
security~\textit{per~se}.

The following three contributions are made:
%
\begin{enumerate}
%
\itemsep 5pt
%
\item{The paper contributes to strengthening of the empirical knowledge base in
  cyber and software security by evaluating the robustness of existing knowledge.}
%
\item{The paper's replication advances the use of good practices in empirical
  cyber and software security research.}
%
\item{With respect to the replicated study~\cite{Siavvas18}, the paper examines
  a substantially larger dataset and uses more sophisticated methods for its
  analysis.}
%
\end{enumerate}

The paper's remainder is structured into a conventional format. The opening
Section~\ref{sec: related work} further frames the paper with a larger scope of
related work. Then, the empirical materials and the methods for examining these
are elaborated in Section~\ref{sec: materials and methods}. This section also
clarifies what is meant by software popularity; in general, it is
operationalized as a degree to which a package is widely adopted and actively
engaged within its ecosystem. The subsequent Section~\ref{sec: results} presents
the replication results after which conclusions follow in the final
Section~\ref{sec: conclusion}.

\section{Related Work}\label{sec: related work}

The paper's reference domain is empirical software engineering focusing on
software security. The domain is large, but with some suitable framings, it is
possible to narrow it down. The first obvious framing is toward empirical
research. The second equally obvious framing is toward the PHP programming
language. With these two framings, a few examples can be given about implicitly
related work. For instance, technical debt of PHP packages for web applications
has been investigated empirically~\cite{Amanatidis17}. Regarding web
applications and websites in general, another example would be a large-scale
empirical study investigating the adoption of PHP interpreter releases among
website deployments~\cite{Ruohonen17APSEC}. A third framing can be done toward
software security. The previous two examples are in the scope also with this
further software security \text{framing---technical} debt is related to software
quality, which is a superset for software security, while the adoption (or a
lack thereof) of new PHP interpreter releases contains security risks in case
many (or some important) websites do not update their interpreters for a reason
or another. The reason for the risks is that also the PHP interpreter has seen
many vulnerabilities over the years.

A lot of empirical research has also been done to examine the security of
software written in PHP more generally. For instance, the security of plugins
for popular PHP-based web frameworks has been investigated~\cite{Niemietz21,
  Ruohonen19EASE}. Regarding software vulnerabilities and their underlying
weaknesses, and despite the availability of defensive solutions~\cite{Dahse15},
cross-site scripting and input validation more generally have been the most
typical weaknesses in PHP software~\cite{Ruohonen19EASE, Santos17}. Though, it
can be noted that this observation is hardly unique to PHP. In other words, the
same weaknesses typically lead the scoreboards also with other popular
interpreted programming languages used in the web
domain~\cite{Ruohonen18IWESEP}. Having said that, it should be emphasized that
empirical software security research on PHP applications is not limited only on
vulnerabilities and weaknesses. A good example would be a forensics
investigation of PHP applications in containerized cloud environments based on
empirical log analysis~\cite{Hyder24}. Furthermore, a third framing can be done
toward replication research.

Despite a long debate on a real, perceived, or alleged need to align with
empirical sciences in cyber security research~\cite{Herley17}, the empirical
foundations are arguably still rather limited. Like in empirical software
engineering~\cite{Shepperd13}, which also partially frames the current paper,
systematic literature reviews have improved the systematization of knowledge but
actual cumulation of empirical evidence, whether done through meta-analyses or
by other means, has been limited. Many reasons for this limitation could be
pointed out and further speculated. Among other things: like in many fields,
there has been a lack of incentives in cyber security research to share
datasets~\cite{Zheng18}. Although there are replications also in cyber
security~\cite{Ruohonen15COSE}, the data sharing limitation has supposedly
contributed to their volume. To this end, the paper contributes to the efforts
to strengthen the empirical knowledge base in cyber security.

A fourth and final clarifying framing can be done toward research on software
ecosystems and particularly the security or insecurity of software distributed
via them. The empirical security-oriented ecosystem research is again vast. A
recurring theme is that software packages distributed in software ecosystems via
language-specific package managers are generally of poor equality, often
containing security issues of various kinds, including concrete
vulnerabilities~\cite{Ruohonen21PST}. In addition, the ecosystems exhibit a risk
of malware being uploaded, as is often done together with so-called
typo-squatting \cite{Ruohonen18IWESEP, Vaidya19}. The underlying security risks
are intensified by the heavy use of software dependencies in these
ecosystems~\cite{Zerouali22}. Given this background, it is understandable that
there are also various ongoing funded projects to improve the ecosystems and the
supply chain security of open source software in
general~\cite{Ruohonen24JSS}. The software ecosystem research and dependencies
are also important to underline because the concept of (software) popularity is
closely related to them~\cite{Kula18, Qiu18}. The framing toward software
ecosystems is also relevant as it is also directly related to the dataset
examined. On that note, the materials and methods should also be elaborated.

\section{Materials and Methods}\label{sec: materials and methods}

In what follows, the large-scale PHP dataset for the replication is first
briefly elaborated. Afterwards, the computational methods for analyzing it are
described.

\subsection{Data}\label{subsec: data}

The dataset was collected on December 2024 from Packagist~\cite{Packagist},
a~repository for PHP software packaged with the Composer package
manager~\cite{Composer}. The dataset is available online for any interested
party, possibly regarding further replication work~\cite{Ruohonen25a}. In total,
$n = 381,993$ packages are covered in the dataset. The dataset is composed of
eight simple meta-data metrics provided on the Packagist's website:
\textit{security}, \textit{installs}, \textit{dependents}, \textit{suggesters},
\textit{stars}, \textit{watchers}, \textit{forks}, and \textit{releases}. Of
these metrics, all except \textit{security} and \textit{releases} can be
interpreted to proxy slightly different dimensions of software popularity. The
outlying \textit{releases} metric counts the number of releases made for a given
open source PHP package, as reported on the Packagist's website. All metrics
have a continuous scale.

While installation amounts, \textit{installs}, provide a relatively
straightforward and widely used metric for software popularity~\cite{Herraiz11,
  Kula18, Qiu18, Ruohonen19EASE}, the \textit{dependents} metric conveys a
package's popularity in terms of incoming edges in a dependency network between
packages~\cite{Kula18}. With this metric a package is seen as popular when many
other packages depend on it. Both installation (or download) counts and
dependencies have also been used to help security-related decision-making about
funding applications for open source software
projects~\cite{Ruohonen24JSS}. Although documentation is unfortunately lacking,
the \textit{suggesters} metric generally proxies ``soft'' dependencies; for each
package, Packagist recommends also further packages in addition to the ``hard''
dependencies, as captured by \textit{dependents}, without which a package does
not install or function properly.

The \textit{stars}, \textit{watchers}, and \textit{forks} metrics are explicitly
tied to functionality provided by GitHub on which almost all of the actual
development of the PHP packages occurs today. In general, with ``stars''
developers can flag packages and projects they perceive as interesting or
relevant. By adding oneself as a ``watcher'' to a project, one can get
information about changes in the project. The \textit{forks} metric, in turn,
refers to the number of times a given project was forked, as has been common in
the open source world throughout the decades~\cite{Gencer12}. According to
practitioner surveys, all three metrics have been seen as relevant for proxying
popularity~\cite{Borges18}. Finally, the dependent metric is \textit{security},
which counts and records vulnerabilities reported for a package. Many (but not
all) of the vulnerabilities tracked on Packagist are identified with Common
Exposures and Vulnerabilities~(CVEs), while the associated security advisories
typically point toward GitHub. In this regard, it can be noted that no
additional data was retrieved to validate the CVE-referenced vulnerability
information provided on Packagist. Nor was GitHub queried to retrieve further
software development~data.

\subsection{Methods}

The primary methodology is based on binary classification. In addition, a few
descriptive statistics and a brief ordinary least squares (OLS) regression
analysis are presented. As these do not require a particular exposition, the
following discussion concentrates on the classification.

\subsubsection{Classification}

Imbalance is a highly typical problem in computational cyber security
applications~\cite{Singh22, Wheelus18}. Although not a problem for the current
paper, not only is cyber security data often imbalanced but it also often
contains missing values~\cite{Tahir25}. Even without a problem of missing
values, as will be soon seen in Section~\ref{sec: results}, the dataset examined
can be characterized to contain extreme imbalance. In other words, only a small
minority of the PHP packages examined has been affected by one or more
vulnerabilities.

To tackle the issue, four computational solutions are used. The first is
oversampling or upsampling, as it is also known. It balances a training set by
replicating the minority class, which in the present context refers to packages
that have been affected by one or more vulnerabilities during their entire
release histories. The second is undersampling or downsampling; it eliminates
observations from the majority class (non-vulnerable packages) until a training
set is balanced. Although there are variants that use synthetic
data~\cite{Singh22}, both the oversampling and undersampling solutions are
computed with random samples.

Then, the third and fourth solutions are based on the SMOTE~\cite{Chawla02,
  Torgo13} and ROSE~\cite{Lunardon14, Menardi14} algorithms. The former uses
synthetic data to mimic observations in the minority class. The $k$-nearest
neighbors algorithm is used to interpolate the synthetic observations. The
latter uses similar ideas with bootstrapping methods. In general, particularly
the SMOTE algorithm is commonly seen as a today's \textit{de~facto} solution
for handling imbalanced data~\cite{Fernandez18}. Although numerous competing
algorithms have been developed over the years, SMOTE therefore suffices as a
good baseline for the classification computations. Furthermore, it should be
emphasized that the goal is \textit{not} about seeking maximum performance but
about testing of a hypothesis.

In the same vein, actual classification is done with three conventional
algorithms: na\"ive Bayes~\cite{Majka24}, (boosted) logistic
regression~\cite{Tuszynski24}, and random forest~\cite{Liaw02}. All are
well-known and thus require no particular elaboration. Instead, the
implementations used are worth remarking; these are all R packages implemented
to work with the \textit{caret} package~\cite{Kuhn08}. Default parameters were
used for all algorithms. The only exception is the random forest classifier
implementation for which the maximum number of trees had to be restricted to one
hundred due to memory constraints.

% In addition, a visualization package~\cite{Robin11} is used for receiver operating characteristic (ROC) curves.

\subsubsection{Performance Metrics}\label{subsec: performance metrics}

Imbalance affects also metrics for evaluating classification performance. Among
other things~\cite{Menardi14}, the traditional accuracy metric is biased and
misleading in this context. Although the $F_1$ metric is often used as an
alternative, it too can be problematic in cyber security
applications~\cite{Marwah24}. Therefore, three alternative performance metrics
are used.

First, a simple balanced accuracy (BA) has been used in existing
work~\cite{Brodersen10}. It is given by
%
\begin{equation}
\textmd{BA} = (\textmd{TPR} + \textmd{TNR})~/~2 ,
\end{equation}
%
where the true positive rate (TPR) and the true negative rate (TNR) are defined as
%
\begin{align}\label{eq: tpr tnr}
\textmd{TPR} &= \textmd{TP}~/~(\textmd{TP} + \textmd{FN})
%
~~~~~\textmd{and}~\\
%
\textmd{TNR} &= \textmd{TN}~/~(\textmd{TN} + \textmd{FP}) ,
\end{align}
%
where, in turn, TP refers to true positives, FN to false negatives, TN to true
negatives, and FP to false positives. The TPR metric is also known as a recall
or a sensitivity and the TNR metric as a specificity.

Second, a G-mean metric has also been used in the imbalanced classification
context~\cite{Kaleeswari23, Susan20}. It is defined via the conventional metrics
in \eqref{eq: tpr tnr} as
%
\begin{equation}
\textmd{G-mean} = \sqrt{\textmd{TPR} + \textmd{TNR}} .
\end{equation}
%
The BA and G-mean metrics both vary in the unit interval. Higher values are
better.

Third, the conventional mean square error (MSE), which is sometimes also known
as the Brier score~\cite{Kaleeswari23}, has further been used in the imbalanced
classification context. It is given by the usual formula:
%
\begin{equation}
\textmd{MSE} = \frac{1}{n}\sum^n_{i=1} (\widehat{p}_i - p_i)^2 ,
\end{equation}
%
where, in the present context, $\widehat{p}_i$ is an estimated probability for a
package's release history having been affected by at least one vulnerability,
while $p_i$ is the actual probability; $p_i = 1$ in case the $i$:th package has
been affected by at least one vulnerability. Unlike with BA and G-mean, lower
values are better.

\subsubsection{Computation}\label{subsec: computation}

Three models are computed for each classifier and each of the four balancing
solutions. Thus, in total, \text{$3 \times 3 \times 4 = 36$} computations are
carried out. Evaluation is done according to the three performance metrics.

The first model contains only \textit{releases}. As was discussed in
Subsection~\ref{subsec: data}, it is the only explanatory metric that is not
explicitly related to popularity of open source software. Therefore, the first
model serves as a baseline for comparing the other two models. If the hypothesis
holds, adding the popularity metrics should increase performance.

The second model contains two explanatory metrics: again \textit{releases} and a
sum variable based on the arithmetic mean of the six popularity
metrics. Cronbach's standardized $\alpha$-coefficient~\cite{Cronbach51} is
$0.64$ for this sum variable. Given the maximum of one, the value is not
particularly high but still sufficient for the present purposes. Adjectives such
as acceptable, satisfactory, and sufficient have been used to describe
comparable values in existing research~\cite{Taber17}. The third full or
unrestricted model contains all available information in the dataset. In other
words, the seven metrics described earlier in Subsection~\ref{subsec: data} are
used in this full model. Alongside the hypothesis, the prior expectation is that
the model's performance is better compared to the barebone first~model.

In addition, for a reason soon discussed, a $\ln(x + 1)$ transformation is
applied to all metrics, including the \textit{security} metric that is truncated
into a binary-valued metric later on for the classifications. Regarding other
computational details, a $10$-fold cross-validation is used. Testing is done
with a random sample containing 25\% of the dataset. Due to the logarithm
transformation, neither scaling nor centering are used for the explanatory
metrics. With these notes in mind, the results can be disseminated next.

\section{Results}\label{sec: results}

The presentation of the results is done in three steps. A~few descriptive
statistics are first presented. Then, the hypothesis is briefly further probed
with regression analysis. The classification results end the presentation.

\subsection{Descriptive Statistics}

The presentation of the results can be started by reiterating the methodological
points. Thus, the extreme imbalance is best illustrated with plain numbers: of
the over $381$ thousand PHP packages, only $777$, or about $0.2\%$, have been
affected by one or more vulnerabilities over their release histories. When
taking a look at Table~\ref{tab: vulnerabilities}, it can be further concluded
that the majority (57\%) of the $777$ packages with vulnerabilities has been
affected by a single vulnerability. As has also been previously observed with
PHP packages~\cite{Ruohonen19EASE}, the tail is relatively long; there are
outlying packages that have been affected by nine or more
vulnerabilities. A~similar observation applies to the remaining metrics; they
all have long tails. Thus, the $\ln(x + 1)$ transformation is generally
justified for analyzing the dataset.

\begin{table*}[th!b]
\centering
\caption{Vulnerabilities Across the PHP Packages}
\label{tab: vulnerabilities}
\begin{tabularx}{\linewidth}{llrcrcrcrcrcrcrcrcrcr}
\toprule
&& \multicolumn{19}{c}{Number of vulnerabilities} \\
\cmidrule{3-21}
&& 0 && 1 && 2 && 3 && 4 && 5 && 6 && 7 && 8 && $\geq$ 9 \\
\hline
Frequency && $381,216$ && $433$ && $114$ && $51$ && $27$ && $23$ && $15$ && $7$ && $14$ && $93$ \\
Share (\%) && $99.797$ && $0.113$ && $0.030$ && $0.013$ && $0.007$ && $0.006$ && $0.004$ && $0.002$ && $0.004$ && $0.024$ \\
\bottomrule
\end{tabularx}
\end{table*}

\begin{figure}[th!b]
\centering
\includegraphics[width=\linewidth, height=7cm]{fig_cor.pdf}
\caption{Correlations in the Dataset}
\label{fig: correlations}
\end{figure}

\begin{figure*}[th!b]
\centering
\includegraphics[width=\linewidth, height=2.5cm]{fig_tt.pdf}
\caption{Means Across Vulnerability Groups (Welch's \cite{Welch47} approximation
  for unequal variances; all $t$-tests statistically significant at \text{$p <
    0.001$})}
\label{fig: tt}
\end{figure*}

\begin{figure*}[th!b]
\centering
\includegraphics[width=\linewidth, height=5cm]{fig_ols.pdf}
\caption{Regression Results (OLS, full sample)}
\label{fig: ols}
\end{figure*}

Regarding multicollinearity, Fig.~\ref{fig: correlations} displays (Pearson's)
correlation coefficients across all metrics in the dataset by using the
logarithm transformation. As can be seen, all coefficients have positive signs
and some of these are relatively large in their magnitudes. These correlations
are nothing surprising as such. Particularly when operating with large
behavioral datasets---a domain to which the dataset examined can also be seen to
belong, everything tends to be correlated with everything~\cite{Meeh90}. In any
case, \textit{stars}, \textit{watchers}, and \textit{installs} are moderately or
even strongly correlated with each other on one hand and \textit{installs} and
\textit{releases} on the other, to use the thresholds and adjectives for these
from the replicated study~\cite{Siavvas18}. Of these correlations, particularly
the latter two seem sensible in a sense that more releases may cause more
installs. These correlations may affect the classifications because particularly
the na\"ive Bayes and logistic regression rely on an independence
assumption. Though, even na\"ive Bayes seems to still perform well under
multicollinearity~\cite{Araveeporn24}. Therefore, it is more relevant to
continue by pointing out the only modest magnitudes between \textit{security}
and rest of the metrics. This observation might be taken as a prior expectation
that the hypothesis may not hold.

However, the descriptive results in Fig.~\ref{fig: tt} tell a different
story. These are based on conventional $t$-test estimates using a correction for
unequal variances, which are evident also according to the Leneve's classical
test~\cite{Leneve60}. As can be seen, all means are different in the two groups
separated by the truncated, binary-valued \textit{security} metric. In other
words, both popular packages and packages with long release histories seem to
have been affected by vulnerabilities over the years. This observation provides
a good motivation to briefly also report regression analysis results before
continuing to the classification results.

\subsection{Regression Analysis}

Regression results can shed a little more light on the earlier correlation
results in Fig.~\ref{fig: correlations}. Thus, coefficients from three OLS
regressions for the three models noted in Subsection~\ref{subsec: computation}
are shown in Fig.~\ref{fig: ols}. The logarithm transformation is again used for
all metrics. All coefficients are statistically significant at a 99\% confidence
level, which is hardly surprising due to the sample size~\cite{Bakan66}. As
could be furthermore expected, the performance increases the more there are
metrics. The full model yields an adjusted $R^2 = 0.045$, meaning that roughly
about five percent of the total variance is explained by the seven metrics. When
keeping in mind that there are only seven hundred seventy seven observations for
which \textit{security} attains a value larger than zero, the performance is not
necessarily that bad. Of the coefficients for the individual metrics, the one
for \textit{suggesters} stands out. Interestingly, the coefficient for
\textit{installs} has a negative sign, which seems to contradict the earlier
results in Fig.~\ref{fig: tt}. All in all, nevertheless, the evidence is again
on the side of supporting the hypothesis. The magnitudes and signs of the
coefficients are presumably linked to the correlations between the explanatory
metrics.

\subsection{Classification}

The classification results are summarized in Tables~\ref{tab: classification
  naive bayes}, \ref{tab: classification logistic regression}, and \ref{tab:
  classification random forest} for the three classifiers. In each table, the
second, third, and fourth columns denote the three models estimated. To recall:
the first model is the barebone one with just the \textit{releases} metric, the
second model adds the sum variable of the popularity metrics, and the third
model uses all seven metrics individually. Then, the rows are arranged according
to the three performance metrics described in Subsection~\ref{subsec:
  performance metrics}. For each metric, the results are reported according to
the four balancing solutions.

\begin{table}[th!b]
\centering
\caption{Na\"ive Bayes Classification Results$^1$}
\label{tab: classification naive bayes}
\begin{threeparttable}
\begin{tabularx}{\linewidth}{lrrr}
\toprule
& Model 1. & Model 2. & Model 3. \\
\cmidrule{2-4}
BA \\
\cmidrule{1-1}
Oversampling & 0.771 & 0.726 & 0.749 \\
Downsampling & 0.771 & 0.736 & 0.803 \\
SMOTE & 0.771 & 0.726 & \e{0.820} \\
ROSE & 0.771 & 0.726 & 0.809 \\
\cmidrule{2-4}
G-mean \\
\cmidrule{1-1}
Oversampling & 0.759 & 0.678 & 0.747 \\
Downsampling & 0.759 & 0.693 & 0.803 \\
SMOTE & 0.759 & 0.678 & \e{0.819} \\
ROSE & 0.759 & 0.678 & 0.807 \\
\cmidrule{2-4}
MSE & \\
\cmidrule{1-1}
Oversampling & 0.283 & 0.463 & \e{0.178} \\
Downsampling & 0.292 & 0.442 & 0.206 \\
SMOTE & 0.284 & 0.463 & 0.199 \\
ROSE & 0.278 & 0.460 & 0.219 \\
\bottomrule
\end{tabularx}
\begin{tablenotes}
\begin{scriptsize}
\item{$^1$~The best values are colored in each of the three rowwise panels.}
\end{scriptsize}
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{table}[th!b]
\centering
\caption{Boosted Logistic Regression Classification Results$^1$}
\label{tab: classification logistic regression}
\begin{threeparttable}
\begin{tabularx}{\linewidth}{lrrr}
\toprule
& Model 1. & Model 2. & Model 3. \\
\cmidrule{2-4}
BA \\
\cmidrule{1-1}
Oversampling & 0.731 & 0.754 & 0.805 \\
Downsampling & 0.793 & 0.633 & 0.761 \\
SMOTE & 0.731 & 0.759 & 0.816 \\
ROSE & 0.744 & 0.734 & \e{0.840} \\
\cmidrule{2-4}
G-mean \\
\cmidrule{1-1}
Oversampling & 0.695 & 0.722 & 0.796 \\
Downsampling & 0.788 & 0.522 & 0.732 \\
SMOTE & 0.695 & 0.727 & 0.814 \\
ROSE & 0.717 & 0.692 & \e{0.840} \\
\cmidrule{2-4}
MSE & \\
\cmidrule{1-1}
Oversampling & 0.492 & 0.383 & 0.260 \\
Downsampling & 0.299 & 0.593 & 0.378 \\
SMOTE & 0.492 & 0.366 & 0.194 \\
ROSE & 0.455 & 0.425 & \e{0.118} \\
\bottomrule
\end{tabularx}
\begin{tablenotes}
\begin{scriptsize}
\item{$^1$~The best values are colored in each of the three rowwise panels.}
\end{scriptsize}
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{table}[th!b]
\centering
\caption{Random Forest Classification Results$^1$}
\label{tab: classification random forest}
\begin{threeparttable}
\begin{tabularx}{\linewidth}{lrrr}
\toprule
& Model 1. & Model 2. & Model 3. \\
\cmidrule{2-4}
BA \\
\cmidrule{1-1}
Oversampling & 0.505 & 0.543 & 0.508 \\
Downsampling & 0.704 & 0.689 & 0.772 \\
SMOTE & 0.524 & 0.599 & 0.525 \\
ROSE & 0.625 & 0.696 & \e{0.825} \\
\cmidrule{2-4}
G-mean \\
\cmidrule{1-1}
Oversampling & 0.484 & 0.319 & 0.125 \\
Downsampling & 0.694 & 0.622 & 0.746 \\
SMOTE & 0.405 & 0.525 & 0.231 \\
ROSE & 0.586 & 0.637 & \e{0.824} \\
\cmidrule{2-4}
MSE & \\
\cmidrule{1-1}
Oversampling & 0.353 & 0.018 & \e{0.007} \\
Downsampling & 0.327 & 0.496 & 0.314 \\
SMOTE & 0.145 & 0.094 & 0.008 \\
ROSE & 0.464 & 0.464 & 0.104 \\
\bottomrule
\end{tabularx}
\begin{tablenotes}
\begin{scriptsize}
\item{$^1$~The best values are colored in each of the three rowwise panels.}
\end{scriptsize}
\end{tablenotes}
\end{threeparttable}
\end{table}

With these notes, the dissemination of the classification results can be started
by noting that the balancing solutions improve the performance
significantly. Although not reported, plain unbalanced classifications yield
much worse performance, regardless of the three classifiers and the three
models. Regarding the balancing solutions, it seems that the ROSE algorithm is
better than the rest, although SMOTE does well with the Na\"ive Bayes
classifier, and plain oversampling yields the best performance in one outlying
case. As for the classifiers, the boosted logistic regression outperforms the
Na\"ive Bayes and random forest classifiers according to the BA and G-mean
metrics. According to these two performance metrics, the highest value reached
is~$0.840$, which can be interpreted as a decent value in the extreme imbalance
context operated.

Most importantly, the best values colored in each table refer to the third
model. In all cases these are much better than with the first model. The
popularity hypothesis can be taken to hold also according to the classification
results.

\section{Conclusion}\label{sec: conclusion}

The paper revisited a popularity hypothesis in empirical software security
research; an assumption that a software's popularity can explain to some degree
the software's insecurity. According to the results presented, the hypothesis can
be concluded to hold: popular PHP packages have been affected by vulnerabilities
throughout their release histories more often than unpopular PHP packages. By
implication, the paper also fails to replicate a previous study's conclusion,
although it should be noted that the study strictly speaking only concluded that
popularity is not a reliable indicator of software
security~\cite{Siavvas18}. Although a decent classification performance was
obtained even with the limited amount of explanatory information used, the
results reported should not be taken to mean that popularity alone could explain
the security or insecurity of packages written in the PHP programming language.

The confirmation of the hypothesis has implications also for empirical research
more broadly. Recently, it has been argued that probabilistic random sampling
should be preferred in empirical software engineering to fix a real or perceived
generalizability crisis~\cite{Baltes22}. The results presented cast a doubt upon
the argument. The unconditional probability of picking a non-vulnerable PHP
package from the almost entire Packagist population observed is $0.998$. In
other words, it would be very likely that only non-vulnerable packages would end
up in a sample picked randomly. By implication, it is necessary to return to the
concept of \textit{reported} vulnerabilities discussed in the
introduction. Thus, should one consequently believe that the $381,216$ packages
in the sample without \textit{reported} vulnerabilities are free of security
issues and generally of high quality? Although no definite answers can be given,
many would likely prefer a negative answer to the question. If this answer is
accepted, the why-question subsequently emerges. As was noted, the famous Linus
law probably has something to do also with reporting of vulnerabilities in large
software ecosystems. If a package is particularly popular, it may be more likely
that some developers actually also review and even audit its code base.

The confirmation of the popularity hypothesis has also other implications. As
was noted, popularity metrics have also been used to grant cyber security
funding for open source software projects. As the funding grants have involved
also testing and security audits~\cite{Ruohonen24JSS}, it may be that the
evidence for the popularity hypothesis also strengthens in the future in a sense
that even more vulnerabilities are \textit{reported} for popular open source
software projects. The same point applies with respect to automated tools. For
instance, also large-scale security scanning efforts of open source software
projects have used popularity and related metrics to select preferable samples
for scanning~\cite{AlphaOmega25}. Audits and scanning are also important topics
because they are arguably closer to ``real'' software security than what is
available through observing \textit{reported} vulnerabilities. To this end, it
can be argued that also construct validity should be revisited, discussed, and
debated in empirical software security research. In the meanwhile, it suffices
to reiterate an argument that \textit{reported} vulnerability counts should be
approached with care when making practical choices about open source software
packages~\cite{Ruohonen18IWESEP}. This argument reinforces the earlier remark
about long-standing philosophical and theoretical problems in applying
principles of empirical sciences to cyber security research.

Finally, both the replicated study and this replication suffer from potential
generalizability issues. In other words, it remains unclear whether the
hypothesis would, or would not, hold with a further different dataset. As said,
however, it is also generally unclear how the generalizability problem should be
addressed---and whether it is even possible to address it in empirical software
security research and empirical software engineering in general. Regardless, as
it stands, there is now conflicting evidence about the popularity hypothesis. It
is up to a reader to make the final verdict about whether software's popularity
affects its insecurity in light of the evidence put forward.

\balance
\bibliographystyle{ieeetr}
%\bibliography{softsecpop}

\begin{thebibliography}{10}

\bibitem{Siavvas18}
M.~Siavvas, M.~Jankovic, D.~Kehagias, and D.~Tzovaras, ``{I}s {P}opularity an
  {I}ndicator of {S}oftware {S}ecurity?,'' in {\em Proceedings of the
  International Conference on Intelligent Systems (IS 2018)}, (Funchal),
  pp.~692--697, IEEE, 2018.

\bibitem{Fabrigar16}
L.~R. Fabrigar and D.~T. Wegener, ``{C}onceptualizing and {E}valuating the
  {R}eplication of {R}esearch {R}esults,'' {\em Journal of Experimental Social
  Psychology}, vol.~66, pp.~68--80, 2016.

\bibitem{Gomez14}
O.~S. G\'omez, N.~Juristo, and S.~Vegas, ``{U}nderstanding {R}eplication of
  {E}xperiments in {S}oftware {E}ngineering: {A} {C}lassification,'' {\em
  Information and Software Technology}, vol.~56, no.~8, pp.~1033--1048, 2014.

\bibitem{Ruohonen15COSE}
J.~Ruohonen, S.~Hyrynsalmi, and V.~Lepp\"anen, ``{T}he {S}igmoidal {G}rowth of
  {O}perating {S}ystem {S}ecurity {V}ulnerabilities: {A}n {E}mpirical
  {R}evisit,'' {\em Computers \& Security}, vol.~55, pp.~1--20, 2015.

\bibitem{Baltes22}
S.~Baltes and P.~Ralph, ``{S}ampling in {S}oftware {E}ngineering {R}esearch:
  {A} {C}ritical {R}eview and {G}uidelines,'' {\em Empirical Software
  Engineering}, vol.~27, no.~94, pp.~1--38, 2022.

\bibitem{Anderson08}
R.~Anderson, {\em {S}ecurity {E}ngineering}.
\newblock New York: Wiley, second~ed., 2008.

\bibitem{Russo19}
D.~Russo and K.-J. Stol, ``{S}oft {T}heory: {A} {P}ragmatic {A}lternative to
  {C}onduct {Q}uantitative {E}mpirical {S}tudies,'' in {\em Proceedings
  IEEE/ACM Joint 7th International Workshop on Conducting Empirical Studies in
  Industry (CESI 2019) and 6th International Workshop on Software Engineering
  Research and Industrial Practice (SER\&IP 2019)}, (Montreal), pp.~30--33,
  IEEE, 2019.

\bibitem{Ruohonen24JSS}
J.~Ruohonen, G.~Choudhary, and A.~Alami, ``{A}n {O}verview of {C}yber
  {S}ecurity {F}unding for {O}pen {S}ource {S}oftware.'' {A}rchived manuscript,
  available online: \url{https://arxiv.org/abs/2412.05887}, 2024.

\bibitem{Raymond01}
E.~S. Raymond, {\em {T}he {C}athedral and the {B}azaar: {M}usings on {L}inux
  and {O}pen {S}ource by an {A}ccidental {R}evolutionary}.
\newblock Sebastobol: O'Reilly, revised~ed., 1999.

\bibitem{Davies10}
J.~Davies, H.~Zhang, L.~Nussbaum, and D.~M. German, ``{P}erspectives on {B}ugs
  in the {D}ebian {B}ug {T}racking {S}ystem,'' in {\em Proceedings of the 7th
  IEEE Working Conference on Mining Software Repositories (MSR 2010)}, (Cape
  Town), pp.~86--89, IEEE, 2010.

\bibitem{Herraiz11}
I.~Herraiz, E.~Shihab, T.~H. Nguyen, and A.~E. Hassan, ``{I}mpact of
  {I}nstallation {C}ounts on {P}erceived {Q}uality: {A} {C}ase {S}tudy on
  {D}ebian,'' in {\em Proceedings of the Working Conference on Reverse
  Engineering (WCRE 2011)}, (Limerick), pp.~219--228, IEEE, 2011.

\bibitem{Ruohonen19EASE}
J.~Ruohonen, ``{A} {D}emand-{S}ide {V}iewpoint to {S}oftware {V}ulnerabilities
  in {W}ord{P}ress {P}lugins,'' in {\em Proceedings of the 23rd Conference on
  the Evaluation and Assessment in Software Engineering (EASE 2019)},
  (Copenhagen), pp.~222--228, ACM, 2019.

\bibitem{Herley17}
C.~Herley and P.~C. {Van Oorschot}, ``{S}o{K}: {S}cience, {S}ecurity and the
  {E}lusive {G}oal of {S}ecurity as a {S}cientific {P}ursuit,'' in {\em
  {P}rocceedings of the {IEEE} {S}ymposium on {S}ecurity and {P}rivacy (S\&P)},
  (San Jose), pp.~99--120, IEEE, 2017.

\bibitem{Amanatidis17}
T.~Amanatidis, A.~Chatzigeorgiou, and A.~Ampatzoglou, ``{T}he {R}elation
  {B}etween {T}echnical {D}ebt and {C}orrective {M}aintenance in {PHP} {W}eb
  {A}pplications,'' {\em Information and Software Technology}, vol.~90,
  pp.~70--74, 2017.

\bibitem{Ruohonen17APSEC}
J.~Ruohonen and V.~Lepp\"anen, ``{H}ow {PHP} {R}eleases {A}re {A}dopted in the
  {W}ild?,'' in {\em Proceedings of the 24th Asia-Pacific Software Engineering
  Conference (APSEC 2017)}, (Nanjing), pp.~71--80, IEEE, 2017.

\bibitem{Niemietz21}
M.~Niemietz, M.~Korth, C.~Mainka, and J.~Somorovsky, ``{O}ver 100 {B}ugs in a
  {R}ow: {S}ecurity {A}nalysis of the {T}op-{R}ated {J}oomla {E}xtensions.''
  {A}rchived manuscript, available online:
  \url{https://arxiv.org/abs/2102.03131}, 2021.

\bibitem{Dahse15}
J.~Dahse and T.~Holz, ``{E}xperience {R}eport: {A}n {E}mpirical {S}tudy of
  {PHP} {S}ecurity {M}echanism {U}sage,'' in {\em Proceedings of the 2015
  International Symposium on Software Testing and Analysis (ISSTA 2015)},
  (Baltimore), pp.~60--70, ACM, 2015.

\bibitem{Santos17}
J.~C.~S. Santos, A.~Peruma, M.~Mirakhorli, M.~Galstery, J.~V. Vidal, and
  A.~Sejfia, ``{U}nderstanding {S}oftware {V}ulnerabilities {R}elated to
  {A}rchitectural {S}ecurity {T}actics: {A}n {E}mpirical {I}nvestigation of
  {C}hromium, {PHP} and {T}hunderbird,'' in {\em Proceedings of the IEEE
  International Conference on Software Architecture (ICSA 2017)}, (Gothenburg),
  pp.~69--78, IEEE, 2017.

\bibitem{Ruohonen18IWESEP}
J.~Ruohonen, ``{A}n {E}mpirical {A}nalysis of {V}ulnerabilities in {P}ython
  {P}ackages for {W}eb {A}pplications,'' in {\em Proceedings of the 9th
  International Workshop on Empirical Software Engineering in Practice (IWESEP
  2018)}, (Nara), pp.~25--30, IEEE, 2018.

\bibitem{Hyder24}
M.~F. Hyder, S.~H. Ahmed, M.~Latif, K.~Aslam, A.~U. Rab, and M.~T. Siddiqui,
  ``{T}owards {D}igital {F}orensics {I}nvestigation of {W}ord{P}ress
  {A}pplications {R}unning {O}ver {K}ubernetes,'' {\em IETE Journal of
  Research}, vol.~70, no.~4, pp.~3856--3871, 2024.

\bibitem{Shepperd13}
M.~Shepperd, ``{C}ombining {E}vidence and {M}eta-{A}nalysis in {S}oftware
  {E}ngineering,'' in {\em Proceedings of the International Summer Schools on
  Software Engineering (ISSSE 2009-2011)} (A.~Lucia and F.~Ferrucci, eds.),
  (Cham), pp.~46--47, Springer, 2023.

\bibitem{Zheng18}
M.~Zheng, H.~Robbins, Z.~Chai, P.~Thapa, and T.~Moore, ``{C}ybersecurity
  {R}esearch {D}atasets: {T}axonomy and {E}mpirical {A}nalysis,'' in {\em
  Proceedings of the 11th USENIX Workshop on Cyber Security Experimentation and
  Test (CSET 2018)}, (Baltimore), pp.~1--8, USENIX, 2018.

\bibitem{Ruohonen21PST}
J.~Ruohonen, K.~Hjerppe, and K.~Rindell, ``{A} {L}arge-{S}cale
  {S}ecurity-{O}riented {S}tatic {A}nalysis of {P}ython {P}ackages in {PyPI},''
  in {\em Proceedings of the 18th Annual International Conference on Privacy,
  Security and Trust (PST 2021)}, (Auckland (online)), pp.~1--10, IEEE, 2021.

\bibitem{Vaidya19}
R.~K. Vaidya, L.~{De Carli}, D.~Davidson, and V.~Rastogi, ``{S}ecurity {I}ssues
  in {L}anguage-{B}ased {S}oftware {E}cosystems.'' {A}rchived manuscript,
  available online: \url{https://arxiv.org/abs/1903.02613}, 2019.

\bibitem{Zerouali22}
A.~Zerouali, T.~Mens, A.~Decan, and C.~{De Roover}, ``{O}n the {I}mpact of
  {S}ecurity {V}ulnerabilities in the npm and {R}uby{G}ems {D}ependency
  {N}etworks,'' {\em Empirical Software Engineering}, vol.~27, pp.~1--53, 2022.

\bibitem{Kula18}
R.~G. Kula, C.~{De Roover}, D.~M. German, T.~Ishio, and K.~Inoue, ``{A}
  {G}eneralized {M}odel for {V}isualizing {L}ibrary {P}opularity, {A}doption,
  and {D}iffusion {W}ithin a {S}oftware {E}cosystem,'' in {\em Proceedings of
  the IEEE 25th International Conference on Software Analysis, Evolution and
  Reengineering (SANER 2018)}, (Campobasso), pp.~288--299, 2018.

\bibitem{Qiu18}
S.~Qiu, R.~G. Kula, and K.~Inoue, ``{U}nderstanding {P}opularity {G}rowth of
  {P}ackages in {J}ava{S}cript {P}ackage {E}cosystem,'' in {\em Proceedings of
  the IEEE International Conference on Big Data, Cloud Computing, Data Science
  \& Engineering (BCD 2018)}, (Yonago), pp.~55--60, 2018.

\bibitem{Packagist}
{P}ackagist, ``{T}he {PHP} {P}ackage {R}epository.'' {A}vailable online in
  December: \url{https://packagist.org/}, 2024.

\bibitem{Composer}
N.~Adermann, J.~Boggiano, {\em et~al.}, ``{C}omposer: {A} {D}ependency
  {M}anager for {PHP}.'' {A}vailable online in December:
  \url{https://getcomposer.org/}, 2024.

\bibitem{Ruohonen25a}
J.~Ruohonen, ``{A} {D}ataset for a {P}aper {E}ntitled ``{T}he {P}opularity
  {H}ypothesis in {S}oftware {S}ecurity: {A} {L}arge-{S}cale {R}eplication with
  {PHP} {P}ackages''.'' Zenodo, available online:
  \url{https://doi.org/10.5281/zenodo.14852281}, 2025.

\bibitem{Gencer12}
M.~Gen\c{c}er and B.~\"Ozel, ``{F}orking the {C}ommons: {D}evelopmental
  {T}ensions and {E}volutionary {P}atterns in {O}pen {S}ource {S}oftware,'' in
  {\em Proceedings of the 8th IFIP WG 2.13 International Conference on Open
  Source Systems: Long-Term Sustainability (OSS 2012)}, (Hammamet),
  pp.~310--315, Springer, 2012.

\bibitem{Borges18}
H.~Borges and M.~T. Valente, ``{W}hat's in a {G}it{H}ub {S}tar? {U}nderstanding
  {R}epository {S}tarring {P}ractices in a {S}ocial {C}oding {P}latform,'' {\em
  The Journal of Systems and Software}, vol.~146, pp.~112--129, 2018.

\bibitem{Singh22}
A.~Singh, R.~K. Ranjan, and A.~Tiwari, ``{C}redit {C}ard {F}raud {D}etection
  {U}nder {E}xtreme {I}mbalanced {D}ata: {A} {C}omparative {S}tudy of
  {D}ata-{L}evel {A}lgorithms,'' {\em Journal of Experimental \& Theoretical
  Artificial Intelligence}, vol.~34, no.~4, pp.~571---598, 2022.

\bibitem{Wheelus18}
C.~Wheelus, E.~Bou-Harb, and X.~Zhu, ``{T}ackling {C}lass {I}mbalance in
  {C}yber {S}ecurity {D}atasets,'' in {\em Proceedings of the IEEE
  International Conference on Information Reuse and Integration (IRI 2008)},
  (Salt Lake City), pp.~229--232, IEEE, 2018.

\bibitem{Tahir25}
M.~Tahir, A.~Abdullah, N.~I. Udzir, and K.~A. Kasmiran, ``{A} {N}ovel
  {A}pproach for {H}andling {M}issing {D}ata to {E}nhance {N}etwork {I}ntrusion
  {D}etection {S}ystem,'' {\em Cyber Security and Applications}, vol.~3,
  p.~100063, 2025.

\bibitem{Chawla02}
N.~V. Chawla, K.~W. Bowyer, L.~O. Hall, and W.~P. Kegelmeyer, ``{SMOTE}:
  {S}ynthetic {M}inority {O}ver-{S}ampling {T}echnique,'' {\em Journal of
  Artificial Intelligence Research}, vol.~16, no.~1, pp.~321--357, 2002.

\bibitem{Torgo13}
L.~Torgo, ``{DMwR}: {F}unctions and {D}ata for ``{D}ata {M}ining with {R}''.''
  {R} package version 0.4.1, available online in February 2025:
  \url{https://cran.r-project.org/src/contrib/Archive/DMwR/}, 2013.

\bibitem{Lunardon14}
N.~Lunardon, G.~Menardi, and N.~Torelli, ``{ROSE}: {A} {P}ackage for {B}inary
  {I}mbalanced {L}earning,'' {\em The R Journal}, vol.~6, no.~1, pp.~79--89,
  2014.

\bibitem{Menardi14}
G.~Menardi and N.~Torelli, ``{T}raining and {A}ssessing {C}lassification
  {R}ules {W}ith {I}mbalanced {D}ata,'' {\em Data Mining and Knowledge
  Discovery}, vol.~28, no.~1, pp.~92--122, 2014.

\bibitem{Fernandez18}
A.~Fern\'andez, S.~Garc\'ia, F.~Herrera, and N.~V. Chawla, ``{SMOTE} for
  {L}earning from {I}mbalanced {D}ata: {P}rogress and {C}hallenges, {M}arking
  the 15-year {A}nniversary,'' {\em Journal of Artificial Intelligence
  Research}, vol.~61, pp.~863--905, 2018.

\bibitem{Majka24}
M.~Majka, ``naivebayes: {H}igh {P}erformance {I}mplementation of the {N}aive
  {B}ayes {A}lgorithm in {R}.'' R package version 1.0.0, avilable online in
  February 2025:
  \url{https://cran.r-project.org/web/packages/naivebayes/index.html}, 2024.

\bibitem{Tuszynski24}
J.~Tuszynski and M.~Dietze, ``ca{T}ools: {T}ools: {M}oving {W}indow
  {S}tatistics, {GIF}, {B}ase64, {ROC} {AUC}, etc..'' {R} package version
  1.18.3, available online in February 2025:
  \url{https://cran.r-project.org/web/packages/caTools/index.html}, 2024.

\bibitem{Liaw02}
A.~Liaw and M.~Wiener, ``{C}lassification and {R}egression by random{F}orest,''
  {\em R News}, vol.~2, no.~3, pp.~18--22, 2002.

\bibitem{Kuhn08}
M.~Kuhn, ``{B}uilding {P}redictive {M}odels in {R} {U}sing the caret
  {P}ackage,'' {\em Journal of Statistical Software}, vol.~28, no.~5,
  pp.~1--26, 2008.

\bibitem{Marwah24}
M.~Marwah, A.~Narayanan, S.~Jou, M.~Arlitt, and M.~Pospelova, ``{I}s ${F}_1$
  {S}core {S}uboptimal for {C}ybersecurity {M}odels? {I}ntroducing
  ${C}_{score}$, a {C}ost-{A}ware {A}lternative for {M}odel {A}ssessment,'' in
  {\em Proceedings of the Conference on Applied Machine Learning for
  Information Security (CAMLIS 2024)}, (Arlington), pp.~1--19, CEUR-WS, 2024.

\bibitem{Brodersen10}
K.~H. Brodersen, C.~S. Ong, K.~E. Stephan, and J.~M. Buhmann, ``{T}he
  {B}alanced {A}ccuracy and {I}ts {P}osterior {D}istribution,'' in {\em
  Proceedings of the 20th International Conference on Pattern Recognition},
  (Istanbul), pp.~3121--3124, IEEE, 2010.

\bibitem{Kaleeswari23}
C.~Kaleeswari, K.~Kuppusamy, and A.~Senthilrajan, ``{P}erformance {A}nalysis of
  {S}amplers and {C}alibrators {W}ith {V}arious {C}lassifiers for {A}symmetric
  {H}ydrological {D}ata,'' {\em International Journal of Advanced Technology
  and Engineering Exploration}, vol.~10, no.~107, pp.~1316--1335, 2023.

\bibitem{Susan20}
S.~Susan and A.~Kumar, ``{T}he {B}alancing {T}rick: {O}ptimized {S}ampling of
  {I}mbalanced {D}atasets---{A} {B}rief {S}urvey of the {R}ecent {S}tate of the
  {A}rt,'' {\em Engineering Reports}, vol.~3, no.~4, p.~e12298, 2020.

\bibitem{Cronbach51}
L.~J. Cronbach, ``{C}oefficient {A}lpha and the {I}nternal {S}tructure of
  {T}ests,'' {\em Psychometrika}, vol.~16, pp.~297--334, 1951.

\bibitem{Taber17}
K.~S. Taber, ``{T}he {U}se of {C}ronbach's {A}lpha {W}hen {D}eveloping and
  {R}eporting {R}esearch {I}nstruments in {S}cience {E}ducation,'' {\em
  Research in Science Education}, vol.~48, pp.~1273--1296, 2017.

\bibitem{Welch47}
B.~L. Welch, ``{T}he {G}eneralization of `{S}tudent's' {P}roblem when {S}everal
  {D}ifferent {P}opulation {V}ariances are {I}nvolved,'' {\em Biometrika},
  vol.~34, no.~1/2, pp.~28--35, 1947.

\bibitem{Meeh90}
P.~E. Meeh, ``{A}ppraising and {A}mending {T}heories: {T}he {S}trategy of
  {L}akatosian {D}efense and {T}wo {P}rinciples {T}hat {W}arrant {I}t,'' {\em
  Psychological Inquiry}, vol.~1, no.~2, pp.~108--141, 1990.

\bibitem{Araveeporn24}
A.~Araveeporn and P.~Wanitjirattikal, ``{C}omparison of {M}achine {L}earning
  {M}ethods for {B}inary {C}lassification of {M}ulticollinearity {D}ata,'' in
  {\em Proceedings of the 2024 7th International Conference on Mathematics and
  Statistics (ICoMS 2024)}, (Amarante), pp.~44--49, ACM, 2024.

\bibitem{Leneve60}
H.~Levene, ``{R}obust {T}ests for {E}quality of {V}ariances,'' in {\em
  {C}ontributions to {P}robability and {S}tatistics} (I.~Olkin and
  H.~Hotelling, eds.), pp.~278--922, Palo Alto: Stanford University Press,
  1960.

\bibitem{Bakan66}
D.~Bakan, ``{T}he {T}est of {S}ignificance in {P}sychological {R}esearch,''
  {\em Psychological Bulletin}, vol.~66, no.~6, pp.~423--437, 1966.

\bibitem{AlphaOmega25}
{Alpha Omega}, ``{A}lpha {E}ngagement: {O}pen{R}efactory.'' {A}vailable online
  in February 2025:
  \url{https://github.com/ossf/alpha-omega/tree/main/alpha/engagements/2023/OpenRefactory},
  2025.

\end{thebibliography}

\end{document}
