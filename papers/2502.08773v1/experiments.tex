
The setting of observing new LLMs at test time is a core focus of this work.
We demonstrate the effectiveness of our 
proposed methods in this setting
on four benchmark datasets: 
for evaluating routing algorithms: EmbedLLM \citep{ZhuWuWen2024}, MixInstruct \citep{Jiang:2023},   RouterBench \citep{HuBieLi2024}, and Chatbot Arena \citep{ZheChiShe2023}.


\begin{figure*}[!t]
  \begin{minipage}[b]{.98\linewidth}
  \hspace{4mm}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figs/embedllm/embedllm-alldatasets-k20.pdf}
        \caption{EmbedLLM}
        \label{fig:embedllm}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figs/mixinstruct/mixinstruct-alldatasets-k10.pdf}
        \caption{MixInstruct}
        \label{fig:mixinstruct}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figs/routerbench/routerbench-alldatasets-k50.pdf}
        \caption{RouterBench}
        \label{fig:routerbench}
    \end{subfigure}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{.98\linewidth}
  \centering
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{lrrrrrrrrr@{}}
    \toprule
    \multirow{2}{*}{
    \diaghead(-3,1){justpadspaceeee}{Method}{Dataset}
    } 
    & \multicolumn{3}{c}{EmbedLLM}  & \multicolumn{3}{c}{MixInstruct} & \multicolumn{3}{c}{RouterBench} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
     & Area $\uparrow$ & QNC $\downarrow$ & Peak Acc.$\uparrow$  
     & Area $\uparrow$ & QNC $\downarrow$ & Peak Acc.$\uparrow$
     & Area $\uparrow$ & QNC $\downarrow$ & Peak Acc.$\uparrow$\\
    \midrule
    Pareto-random router 
       & .611 & 1.0 & 65.7\% 
       & .0477 & 1.0 & .0510 
       & .692 & 1.0 & 78.2\% \\
    K-NN
      & .634 & .51 & 68.3\% 
        & .0488 & .94 & .0528 
        & .717 & .962 & \best{78.3\%} \\
    K-means (Gecko) 
        & \best{.650} & \best{.35} & \best{69.3\%}
        & .0489 & .96 & .0524 
        & \best{.720} & \best{.961} & 78.2\% \\
    K-means (Attributes) 
       & .637 & .50 & 67.6\% 
         & \best{.0491} & \best{.90} & \best{.0529}
        & .716 & .970 & 78.2\% \\
    Learned cluster map
        & .638 & .44 & 67.8\%
         & .0489 & .93 & .0516 
        & .715 & .971 & 78.2\% \\
    \midrule
    Clairvoyant fixed-pool router 
        & .657 & .32 & 69.9\% 
        & .0497 & .85 & .0534 
        & .723 & .989 & 78.2\% \\
    \bottomrule
    \end{tabular}
    
   }
  \end{minipage}
\vspace{-3pt}
  \caption{\textbf{Top}: Accuracy-cost trade-off curves (deferral curves) of the six methods (\S\ref{sec:experiments}) on %
  on unseen, testing LLMs.
    Our proposed routing approaches \emph{K-means} are able to generalize to new LLMs, and deliver a good quality-cost trade-off.
    \textbf{Bottom}: Summary of each deferral curve. On all datasets, our proposal yields the highest area under the curve, and lowest Quality-Neutral Cost (QNC) i.e., the minimum relative cost to achieve the same performance as the most accurate LLM. Pareto-random router and K-NN are the main baselines. The Clairvoyant fixed-pool router represents an upper bound on the performance achievable had \emph{all} LLMs been observed during training. 
    }
\vspace{-7pt}
\label{fig:three_experiments}
\end{figure*}


\textbf{Data pre-processing.}
With EmbedLLM, MixInstruct, and RouterBench, we partition the set of LLMs available into two disjoint sets: training models ($\mset_\mathrm{tr}$ in \S\ref{sec:proposal}) and testing models ($\mset_\mathrm{te}$). For EmbedLLM which contains responses from as many as $112$ LLMs, we use a random subset of $\sfrac{2}{3}$ for training and $\sfrac{1}{3}$ for testing. For MixInstruct (11 LLMs in total) and RouterBench (11 LLMs in total), we use a random 50\% 
for training and the rest for testing. 

We randomly split examples into 60\%/10\%/30\% for training, validation, and testing, respectively. All baselines are evaluated on the test portion and \emph{only} on the test LLMs. The validation split is the small dataset used to represent each test-time LLM as a feature vector, as described in \S\ref{sec:cluster_router}. The training portion is for training a router, and only has correctness labels of training models.


\textbf{Per-example metrics.}\
EmbedLLM and  RouterBench aggregate examples from publicly available benchmark datasets, where the evaluation metric is binary accuracy (i.e., an LLM is either correct or wrong on a query). Thus, all the baselines we consider seek to estimate $\gamma^{(m)}(\bx) = \mathbb{P}\left[\by\ne h^{(m)}(\bx)\mid\bx\right]$ and rely on the same deferral rule described in \eqref{eq:opt_rule01}.
For MixInstruct, we use (exponentiated) BARTScore \citep{YuaNeuLiu2021} as the evaluation metric, following \citet{Jiang:2023}. 




\textbf{Routing methods.}\
For all the baselines that require a query embedder, we use the Gecko 1B  model  \citep{LeeDaiRen2024} to produce a 768-dimensional embedding.\footnote{Specifically, we use \texttt{text-multilingual-embedding-002} from Google Cloud: \url{https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings}.}
The Gecko checkpoint is used as is without further fine-tuning. The following are the routing methods we evaluate: 
\begin{enumerate}[itemsep=0pt,topsep=0pt,label=(\arabic*),,leftmargin=16pt]
    \item \textbf{Clairvoyant fixed-pool router} 
    \citep{HuBieLi2024,OngAlmWu2024,Ding:2024}. 
    This is a representative baseline for the multi-model routing strategies described in \S\ref{sec:background} for a fixed (non-dynamic) pool of LLMs.
    Since these methods cannot route to unseen LLMs, we provide access to  
    both the training and testing models  during training (hence clairvoyant), train a separate output head for each LLM (see \eqref{eqn:bert-router}) to predict correctness labels, and route via \eqref{eqn:post-hoc}. We use the sigmoid cross entropy loss on EmbedLLM and RouterBench, and the squared loss on MixInstruct. This baseline provides an estimate %
     of the \emph{performance achievable when all LLMs are observed}. 
    
    \item \textbf{Pareto-random router.}\
    Implements the rule in $\eqref{eq:cluster-routing-k-equals-1}$.
    For each prompt, the router routes to the same test LLM (or a randomized combination of two LLMs). %
    
    \item \textbf{K-NN} \citep{HuBieLi2024}. For each test prompt, this method looks up the $K$ nearest prompts in the validation set in the space of Gecko embeddings and computes $\hat{\gamma}$ of each test LLM using \eqref{eqn:knn-router} (with the 0-1 loss). 
    This method does not use the training sample, 
    as it does not contain correctness labels for the test models.
    
    \item \textbf{K-means (Gecko).}\
    This is our proposed cluster-based routing rule in \S\ref{sec:cluster_router}.
    We apply K-means to the training prompts to construct the clusters.
    
    \item \textbf{K-means (Attributes).}\
    Same as the above proposed cluster-based routing, except that the query representation is based on the query attributes described in \S\ref{sec:new_llms}. 
    We use the seven binary difficulty attributes proposed in \citet{Li:2024c}, and prompt Gemini 1.5 Pro 002 to annotate each attribute for each training prompt.\footnote{See \texttt{gemini-1.5-pro-002} on \url{https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions}.} 
    We then construct a 
    query embedder $\Phi(\bx) = \sigma(\mathbf{V}^\top \mathrm{Gecko}(\bx)) \in [0,1]^7$, where $\mathbf{V} \in \mathbb{R}^{768 \times 7}$ %
    is distilled using the training set to predict the 7-category attributes for any new prompt $\bx$. 
    
    \item \textbf{Learned cluster map.}\ Our proposed cluster-based routing method in \S\ref{sec:two_tower}, where the cluster assignment map is learned using the training set using the correctness labels for the training models.
\end{enumerate}



\textbf{Deferral curve.}\ We evaluate each method with a deferral curve as described in \S\ref{sec:background}, which plots the average quality against the overall cost.
The trade-off is realized by varying the $\lambda_\mathfrak{H}$ parameter in the routing rule in \eqref{eq:opt_rule}.
For EmbedLLM and MixInstruct, we use the number of parameters of the LLM as the cost of processing one prompt. 
This cost definition is a convenient proxy for the amount of computation required to call each LLM.
For RouterBench, we plot LLMs' API calling costs (USD) as available in the dataset.


\textbf{Hyper-parameter tuning.}\ We apply the following procedure to pick the parameter $K$ for K-NN, K-means (Gecko), K-means (Attributes) and the Learned cluster map: for each $K$, we represent the training LLM using correctness labels in the training set, evaluate the routing rule for the training LLMs on the validation set, and measure the area under the deferral curve. This evaluation metric can be seen as the average improvement in quality per unit cost (analogous to the AIQ metric used in \citet{HuBieLi2024}). The parameter $K$ with the maximum area is then chosen. See \S\ref{sec:validate_k} for details.


\textbf{Results.}\ We present deferral curves for different methods in \cref{fig:three_experiments}. Each isolated  point \textcolor{gray}{\texttt{x}} represents the cost and average test accuracy of one testing LLM.
In the table, we report three evaluation metrics for each method: (i) the area under the deferral curve (Area); (ii) the quality-neutral cost (QNC) or the minimum relative cost needed to achieve the same performance as the most accurate testing LLM; and (iii) the peak accuracy (Peak Acc.) achievable across the entire cost range. 
Note that the QNC is analogous to the  call-performance threshold %
reported in \citet{OngAlmWu2024}. %

We observe that our proposed  method \emph{K-means (Gecko)} yields the best quality-cost trade-off in most cases. Interestingly, the \emph{K-means (Attributes)}, which relies on only a 7-dimensional input representation, performs remarkably well 
on MixInstruct.
Notably, on EmbedLLM, our K-means (Gecko) almost matches the performance of Clairvoyant fixed-pool router, \emph{despite not observing testing models during training}.
An important comparison point is 
the Pareto-random router, which was noted as a strong baseline in \citet{HuBieLi2024} (referred to as the Zero Router). On both EmbedLLM and RouterBench, our methods are able to exceed the performance of this baseline on a large range of costs. %


The surprising underperformance of K-NN is due, in large part, to the requirement that only the retrieved neighbors from the validation set can be used to estimate test models' performance. It is hence unable to exploit the training set in any way. In contrast, our methods are able to fully exploit the training data either in an unsupervised manner (K-means), or through the use of correctness label supervision for the training LLMs (Learned cluster map). 


\textbf{Robustness to distribution shift.}\ We also evaluate the robustness of our methods to shifts in the query distribution by constructing routers using the Chatbot Arena dataset and evaluating them on EmbedLLM. Owing to its more compact representation, we find K-means (Attributes) to be the most robust and yield the best trade-offs (details in \S\ref{sec:chatbot_exp_details}). %
\begin{table}[H]
\centering
\caption{Generalizing from Chatbot Arena to EmbedLLM.}
\begin{tabular}{lrrr}
\toprule
Method & Area$\uparrow$ & QNC$\downarrow$ & Peak Acc.$\uparrow$\\
\midrule
Pareto-random router & .507 & $\infty$ & 51.9\% \\
K-NN & .472 & $\infty$ & 52.1\% \\
K-means (Gecko) & .529 & $\infty$ & 54.5\% \\
K-means (Attributes) & \best{.545} & \best{.97} & \best{55.8\%} \\
\bottomrule
\end{tabular}
\end{table}




    




    
    
