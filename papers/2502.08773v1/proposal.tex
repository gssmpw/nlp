

We now propose a means to represent LLMs via their
\emph{correctness vector} on a small set of labelled validation prompts.
This naturally leads to certain \emph{cluster-based} representations, involving either unsupervised or supervised cluster assignments based on a large set of unlabelled training prompts.



\subsection{The Correctness Vector Representation}
\label{sec:correctness_representation}

To construct our LLM representation $\LLMEmbed$,
it is useful to consider the properties a ``good'' representation ought to satisfy.
One intuitive requirement is that $\LLMEmbed( h )^\top \LLMEmbed( h' )$ should be large for a pair $(h, h')$ of ``similar'' LLMs,
and small for a pair of ``dissimilar'' LLMs.
A reasonable definition of ``similar'' would thus enable the design of $\LLMEmbed$.

We posit that two LLMs are similar if they 
\emph{have comparable performance on a set of representative prompts},
following similar proposals in~\citet{Thrush:2024,ZhuWuWen2024}.
Concretely, suppose 
that
we have access to a small validation set $S_{\val} = \{(\bx^{(i)},\by^{(i)})\}_{i=1}^{N_\val}$ of labelled prompts.
Further, suppose that 
\emph{any new LLM 
$\hNew^{( n )} \in \msetNew$
can be evaluated on these prompts}.
Then, 
one may construct
$$ \LLMEmbed( \hNew^{( n )} ) = \begin{bmatrix} 1( \by^{(i)} = \hNew^{( n )}( \bx^{(i)} ) ) \end{bmatrix}_{i \in [ N_{\val} ]} \in \{ 0, 1 \}^{N_{\val}}, $$ 
denoting the accuracy of an LLM on each prompt in $S_{\val}$.
We term this the \emph{correctness vector representation}.

The choice of prompts in $S_{\val}$ is of clear import.
These prompts could be either hand curated based on domain knowledge,
or simply drawn from a standard benchmark suite.
Further,
since $S_{\val}$ is assumed to be of modest size, 
evaluation of any new LLM's predictions on $S_{\val}$ is not prohibitive;
indeed, if $S_{\val}$ comprises a subset of a standard benchmark suite,
these results may already be available as part of the LLMs' standard evaluation protocol.

While the above provides a reasonable starting point,
it may introduce a risk of overfitting if $N_{\rm val}$ is moderately large (say, $\geq 1000$).
To mitigate this, we consider a variant which relies on \emph{aggregate} performance on \emph{subsets} of $S_{\val}$.


\ifarxiv
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/illus_cluster_routing.pdf}
    \caption{An illustration of our proposed cluster-based router (see \S\ref{sec:cluster_router}). 
    We first perform $K$-means on an unlabeled training set to find $K$ centroids, which allow us to partition the validation set to $K$ representative clusters. 
    Each test-time LLM can then be represented as a $K$-dimensional feature vector of per-cluster errors. For each test query, we route to the LLM which has the smallest cost-adjusted average error on the cluster the query belongs to.
    }
    \label{fig:illus_cluster}
\end{figure}
\fi


\subsection{Cluster-Based LLM Representation}
\label{sec:cluster_router}



To extend the above, we propose
to represent any new LLM $\hNew^{( n )}$ through its average errors 
$\hat{\LLMEmbed}( \hNew^{( n )} ) \in [0,1]^K$
on $K$ pre-defined \emph{clusters}. 
We then approximate 
\eqref{eq:opt_rule01} via 
$$ \hat{\gamma}( \bx, \hNew^{( n )} ) = \mathbf{z}( \boldsymbol{x} )^\top \hat{\LLMEmbed}( \hNew^{( n )} ), $$
where $\mathbf{z}( \boldsymbol{x} ) \in \{ 0,1 \}^K$ indicates the cluster membership.


One challenge with the above
is that clustering $S_{\val}$ itself is prone to overfitting,
since (by assumption) the set is of modest size.
To overcome this,
we assume we have access to a large
\emph{unlabeled} training set consisting of input prompts $S_\tr = \{\bx^{(i)}\}_{i=1}^{N_\tr}$.
We now
use the training set to  group the prompts into $K$ disjoint clusters, and compute per-cluster errors for a new LM using the validation set $S_{\val}$. %

Concretely, our proposed LLM representation is as follows:
\begin{enumerate}[label=(\roman*),itemsep=0pt,topsep=0pt,leftmargin=16pt]
\item Given a pre-trained query embedder 
$\Phi \colon \XCal \to \Real^D$, 
apply a clustering algorithm to the \emph{training} set embeddings 
$\{\Phi(\bx^{(i)})\}_{i=1}^{N_\tr}$
to construct $K$ non-overlapping clusters.
This yields a cluster assignment map
$\mathbf{z}\colon\mathscr{X}\to\{0,1\}^K$, where $z_k(\bx) = 1$ indicates that $\bx$ belongs to cluster $k$.
\item Assign each prompt in the \emph{validation} sample to a cluster. Let $C_{k}\defeq\{(\bx, \by)\,:\,(\bx, \by) \in S_\val,\, z_{k}(\bx)=1\}$ be the subset of the validation set that belongs to cluster $k$.
\item For each new LLM $\hNew^{( n )} \in \msetNew$, compute a feature representation $\hat{\LLMEmbed}( \hNew^{( n )} ) \in [0,1]^{K}$ 
using its per-cluster error on the  validation set:
\begin{align}
\hat{\LLMEmbedScalar}_{k}( \hNew^{( n )} )
\defeq\frac{1}{|C_{k}|}\sum_{(\bx,\by)\in C_{k}}\1\big[ \by \ne \hNew^{( n )}(\bx) \big].
\label{eq:cluster_accs}
\end{align}
\vspace{-15pt}
\end{enumerate}

We may now approximate the expected loss for $\hNew$ on an input prompt $\bx$ using the average error of the LLM on the cluster the prompt is assigned to, and route via:
\begin{equation}
\begin{aligned}
\hat{r}(\bx, \msetNew) & = \underset{n \in [ \numSetNew ]}{\argmin} \, \big[ \hat{\gamma}_{\cluster}(\bx, \hNew^{( n )} ) + \lambda\cdot c( \hNew^{( n )} ) \big]
\label{eq:cluster-routing}\\
\hat{\gamma}_{\cluster}(\bx, \hNew^{(n)}) &\defEq
\mathbf{z}( \boldsymbol{x} )^\top \hat{\LLMEmbed}(\hNew^{(n)}).%
\end{aligned}%
\end{equation}
Intuitively,
$\hat{\gamma}_{\cluster}(\bx, \hNew^{( n )} )$ 
estimates the performance of a given LLM on $\bx$
by examining the performance of the LLM on \emph{similar} prompts,
i.e.,
those prompts belonging to the same cluster.
Note that to add a new LM to
the serving pool, we simply need to
compute per-cluster errors $\hat{\LLMEmbed}( \hNew^{( n )} )$ %
by generating responses
from the LM on a small set of validation prompts. 
Importantly, this operation
does not require any expensive gradient updates.






A common choice for the clustering algorithm in step (ii) is the $K$-means algorithm~\citep{Mac1967}, which would return a set of $K$ centroids $\{\mu_{1},\ldots,\mu_{K}\}\subset\mathbb{R}^{D}$, and an assignment map $\mathbf{z}$ that assigns a new prompt to the cluster with the nearest centroid, i.e., $z_k(\bx)  = 1$ iff $k = \arg\min_{j\in[K]}\|\Phi(\bx)-\mu_{j}\|_{2}$.
\ifarxiv
An illustration of our proposal is shown in \cref{fig:illus_cluster}.
\fi






\textbf{Special case: Pareto-random routing}.
When the number of clusters $K=1$, the  routing rule in \eqref{eq:cluster-routing} returns the same LLM for all queries $\bx$, and is given by:
\begin{align}
\hat{r}(\bx, \msetNew) & =
\underset{n \in [ \numSetNew ]}{\argmin} \, \big[ \hat{\LLMEmbedScalar}( \hNew^{(n)} ) + \lambda\cdot c( \hNew^{(n)} ) \big],
\label{eq:cluster-routing-k-equals-1}
\end{align}
where $\hat{\LLMEmbedScalar}( \hNew^{( n )} ) \defeq\frac{1}{N_{\rm val}}\sum_{(\bx, \by) \in S_{\rm val}} \1[\hNew^{( n )}(\bx)\ne\by].$  
This rule is closely aligned with the Pareto-random router (\S\ref{sec:background}). 

\begin{prop}
{For any $\lambda \in \mathbb{R}_{\geq 0}$, the routing rule in \eqref{eq:cluster-routing-k-equals-1} returns an LLM on the  \emph{Pareto-front} of the set of cost-risk pairs 
$\{(c( \hNew^{( n )} ), \hat{R}_{01}( \hNew^{( n )} ): n \in [ \numSetNew ]\}$, 
where 
$\hat{R}_{01}( \hNew^{( n )} ) = \frac{1}{N_{\rm val}}\sum_{(\bx, \by) \in S_{\rm val}} \1[\hNew^{( n )}(\bx)\ne\by]$.}
\label{prop:cluster-routing-k-equals-1}
\end{prop}












\subsection{Learned Cluster Assignment Map}
\label{sec:two_tower}
One may further improve the cluster-based routing strategy by replacing the assignment map in \eqref{eq:cluster-routing} with a \emph{learned} map
$\mathbf{z}( \cdot; \boldsymbol{\theta} ) \in [ 0, 1 ]^{K}$ 
parameterised by $\boldsymbol{\theta}$,
that can better map an input query to a distribution over clusters. 

To achieve this, we assume access to a training set containing both prompts and labels $S_\tr = \{(\bx^{(i)}, \by^{(i)})\}_{i=1}^{N_\tr}$, and a set of  LMs during training $\mset_\tr$, %
which could potentially be different from the ones we see during deployment time.

Suppose we produce a cluster assignment $\mathbf{z} \colon \XCal \to \{ 0, 1 \}^K$ as in the previous section.
We may then model the assignment function 
$\mathbf{z}( \cdot; \boldsymbol{\theta} )$ %
via a log-linear model:
\begin{align*}
 \hat{z}_k( \boldsymbol{x}, \boldsymbol{\theta} ) \propto \exp\left(\theta_{k}^\top \Phi( \boldsymbol{x} ) \right),
\end{align*}
for $\boldsymbol{\theta} \in \mathbb{R}^{K\times D}$,
and similar to \eqref{eq:cluster-routing},
estimate the probability of error $\mathbb{P}\big[\by \ne h(\bx) \mid \bx\big]$ for an LLM $h$ via:
\begin{align*}
\hat{\gamma}\left( \bx, h; \boldsymbol{\theta} \right) 
&= \mathbf{z}( \bx; \boldsymbol{\theta} )^\top \hat{\LLMEmbed}( h ), %
\end{align*}
where $\hat{\LLMEmbed}( h  )$ denotes the per-cluster errors for the LM estimated from the validation sample, analogous to~\eqref{eq:cluster_accs}.
As before,
the routing rule for the new LMs $\msetNew$ is:
\begin{equation}
    \hat{r}(\bx, \msetNew; \boldsymbol{\theta} ) = \underset{n \in [ \numSetNew ]}{\argmin}\,\, 
    \left[
    \hat{\gamma}\big( \bx, \hNew^{(n)}; \boldsymbol{\theta} \big) + \lambda\cdot c( \hNew^{(n)} )
    \right].
    \label{eq:cluster-routing-learned}
\end{equation}

To pick the parameters $\boldsymbol{\theta}$, we minimize the log loss on the training set against correctness labels for LMs $\mset_\tr$: %
\begin{align*}
    -\sum_{(\bx, \by) \in S_\tr} &\sum_{h \in \mset_\tr}
        \1\big[ \by \ne h(\bx)\big] \cdot 
            \log\,\hat{\gamma} \left(
                    \bx, h; \theta \right) 
            +
        \1\big[ \by = h(\bx)\big] \cdot 
            \log \left( 
                1 - \hat{\gamma}\left(
                    \bx, h; \theta \right) 
                    \right), 
\end{align*}
where $\hat{\LLMEmbed}( h )$ is evaluated using the \emph{validation set}. 





\subsection{Excess Risk Bound}
\label{sec:excess-risk}
We now present an excess risk bound for our cluster-based routing strategy.
Suppose we represent the underlying data distribution over $(\bx,\by)$ by a mixture of $K$ latent components:
$\mathbb{P}(\bx,\by) = \sum_{k=1}^{K}\pi_{k} \cdot \mathbb{P}(\bx,\by\,|\,z=k),$
where $z$
denotes a latent random variable that identifies the mixture
component and $\pi_{k}=\mathbb{P}(z=k)$. For a fixed component $k$, we may denote the probability of incorrect predictions
for $\hNew\in \mathscr{H}$ conditioned on $z=k$ by:
\[
\LLMEmbedScalar_{k}( \hNew^{( n )} ) \defeq \mathbb{P}_{\bx, \by|z=k}\left[\hNew^{( n )}(\bx)\ne\by\right].
\]
Then the cluster-based routing rules in \eqref{eq:cluster-routing} and \eqref{eq:cluster-routing-learned} seek to mimic the following population routing rule:
\begin{equation}
    \begin{aligned}
    \label{eq:cluster-rule-population}
    \lefteqn{\tilde{r}^{*}(\bx, \msetNew)  =}
    \\
    & ~~~
    \underset{n \in [ \numSetNew ]}{\argmin} \, \sum_{k\in[K]}\mathbb{P}(z=k|\bx) \cdot \LLMEmbedScalar_{k}(\hNew^{(n)}) +\lambda \cdot c( \hNew^{(n)} ).
    \end{aligned}%
\end{equation}



\begin{prop}
\label{prop:cluster-regret-bound}
Let $r^*$ denote the Bayes-optimal routing rule in Proposition \ref{prop:optimal_rule}.
For any $\msetNew = \{ \hNew^{( n )} \}_{n \in [ N ]} \in \mathbb{H}$, 
$\hNew^{( n )} \in \msetNew$,
and
$\bx \in \XCal$, let:
\begin{align*}
    \Delta_{k}(\bx, \hNew^{( n )} ) \defEq \left|\mathbb{P}_{\by|\bx,z=k}\left[\hNew^{( n )}(\bx)\neq\by\right] \,-\, \LLMEmbedScalar_{k}( \hNew^{( n )} ) \right|.
\end{align*}
Let 
$R_{01}(r, \msetNew) \defEq \sum_{n} \mathbb{P} \left[ \hNew^{( n )}(\bx)\neq\by \land r(\bx, \msetNew)=m \right]$ 
denote the 0-1 risk. %
Then under a regularity condition on $\mathbb{P}$,
the difference in 0-1 risk between $\tilde{r}^*$ and $r^*$ is bounded by:
\begin{align*}
\lefteqn{\mathbb{E}_{\msetNew}\left[R_{01}(\tilde{r}^*,\msetNew)\right] ~-~ \mathbb{E}_{\msetNew}\left[{R}_{01}(r^*,\msetNew)\right]} \\
&\hspace{2cm}
 \,\leq\,
\mathbb{E}_{\msetNew, \bx}\left[\max_{\hNew^{( n )} \in \msetNew, k\in[K]}\,\Delta_{k}(\bx, \hNew^{( n )} )\right].
\end{align*}
\end{prop}

Proposition~\ref{prop:cluster-regret-bound} suggests that the difference in quality between the cluster-based routing rule  (\eqref{eq:cluster-routing} or \eqref{eq:cluster-routing-learned}) and the optimal rule in \eqref{eq:opt_rule01} is bounded by the discrepancy between the per-cluster error and the per-query error.
That is, 
we require that %
for any query $\bx$, 
the average error of an LLM on the cluster $\bx$ belongs to closely reflects the error on $\bx$ itself. %


\subsection{Discussion and Relation to Existing Work}
\label{sec:discussion_related}

Our proposal relates to several recent strands of work~\citep{Tailor:2024,Thrush:2024,ZhuWuWen2024,Feng:2024,Li:2025,Zhao:2024}, which merit individual discussion.

The idea of representing an LLM via a correctness vector has close relation to some recent works.
In~\citet{Tailor:2024}, the authors proposed to represent ``experts'' via predictions on a small \emph{context set},
so as to enable deferral to a new, randomly selected expert at evaluation time.
While not developed in the context of LLMs (and for a slightly different problem),
this bears similarity to our proposal of representing LLMs via a correctness vector on a validation set; 
however, note that the mechanics of routing based on this vector (via suitable clustering) are novel.
\citet{Thrush:2024} considered representing LLMs via their perplexity on a set of public benchmarks,
for subsequent usage in pre-training data selection.
This shares the core idea of using LLM performance on a set of examples to enable subsequent modelling, albeit for a wholly different task.
\citet{ZhuWuWen2024} proposed to construct a generic
embedding for LLMs based on performance on public benchmarks.
This embedding is constructed via a form of matrix factorisation, akin to~\citet{OngAlmWu2024}.
While~\citet{ZhuWuWen2024} discuss model routing as a possible use-case for such embeddings, there is no explicit evaluation of embedding generation in the case of \emph{dynamic} LLMs;
note that this setting would na\"{i}vely require full re-computation of the embeddings, or some version of incremental matrix factorisation~\citep{Brand:2002}.

Some recent works have considered the problem of routing with a dynamic pool of LLMs.
\citet{Feng:2024} proposed a graph neural network approach,
wherein LLMs are related to prompts and \emph{tasks} (e.g., individual benchmarks that a prompt is drawn from).
Such 
pre-defined \emph{task labels} for input prompts
may be unavailable in some practical settings.
\citet{Li:2025} proposed to use LLM performance on benchmark data to construct a \emph{model identity vector},
which is trained using a form of variational inference.
This has conceptual similarity with our correctness vector proposal (and the works noted above);
however, our mechanics of learning based on this vector 
(i.e., the cluster-based representation)
are markedly different.
Further,~\citet{Li:2025} consider an online routing setting wherein bandit approaches are advisable,
whereas we consider and analyse a conventional supervised learning setting.
Finally, we note that~\citet{Zhao:2024} consider the problem of routing to a dynamic \emph{LoRA pool}, where LoRA modules are natively represented by aggregating (learned) embeddings on a small subset of training examples.
These embeddings are learned via a contrastive loss,
constructed based on certain pre-defined task labels.
As such labels may not be provided in many settings,~\citet{Chen:2024} implemented a variant wherein these are replaced by unsupervised cluster assignments.
While similar in spirit to our proposals, the details of the mechanics are different; e.g., we use the clustering to compute a set of average accuracy scores for each LLM,
rather than training an additional embedding.

Finally, we note that~\citet{Chen:2024} also consider the use of clustering as part of router training.
However, the usage is fundamentally different:
~\citet{Chen:2024}
regularise the learned query embedding $\QueryEmbed$
based on
cluster information,
while we use clustering to construct the LLM embedding $\LLMEmbed$.

Our proposed approach is similar in spirit to methods such as  K-NN, where the prediction for a query is based on the labels associated with queries in its neighborhood; in our case, we consider pre-defined clusters instead of example-specific neighborhoods.

 



