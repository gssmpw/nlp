

We now formalise the model routing problem when the set of LLMs may vary \emph{dynamically},
drawing upon a formulation in~\citet{Tailor:2024} for a related problem.
By studying the 
corresponding
\emph{Bayes-optimal} solution,
we present a general framework
that relies on constructing \emph{LLM feature representations}.

\subsection{Problem Setup}

The formal model routing setup in~\eqref{eqn:static-router}
assumed a \emph{fixed} set of LLMs.
Consequently, the post-hoc routing approaches of~\eqref{eqn:knn-router},~\eqref{eqn:bert-router}
fundamentally rely on estimating parameters for each LLM observed during training.
In practice, the set of LLMs may frequently grow or shrink,
as new models are released and old models are deprecated.
To extend the routing setup to such settings,
we closely follow a formulation in~\citet{Tailor:2024},
devised for the related problem of learning to defer to an expert (cf.~\S\ref{sec:related}).

Concretely, 
suppose 
$\mset_{\rm all}$ denotes the set of all possible feasible LLM predictors,
where for simplicity we assume $| \mset_{\rm all} | < +\infty$.
Let $\mathbb{H} \defEq 2^{\mset_{\mathrm{all}}}$ 
denote the set of all subsets of $\mset$. 
Let
$\mset_{\rm tr} = \{ h^{(1)}, \ldots, h^{(M)} \} \in \mathbb{H}^M$ 
denote the set of $M$ LLM predictors observed during training.
During evaluation,
we seek to route amongst the LLM predictors in some set 
$\mset_{\rm te} = \{ \hNew^{(1)}, \ldots, \hNew^{(N)} \} \in \mathbb{H}^N$.
If $\mset_{\rm tr} = \mset_{\rm te}$, 
we obtain the original model routing problem in~\eqref{eqn:static-router}.
However, we aim to support settings where
$\mset_{\rm tr} \neq \mset_{\rm te}$;
indeed, in an extreme case, it may be that 
$\mset_{\rm tr} \cap \mset_{\rm te} = \varnothing$.

To accommodate the dynamic nature of LLMs observed during evaluation, 
we first modify our router to accept both an input \emph{and} a set of candidate LLMs,
with the goal to pick the best option from this set;
i.e.,
we consider 
\emph{dynamic routers}
$\mathscr{R} \defEq \{ r( \cdot, {\mset} ) \colon \XCal \to [| \mset |] \mid \mset \in \mathbb{H} \}$.
Next, we assume that the set of LLMs observed during training is itself drawn from some \emph{meta-distribution} $\mathfrak{H}$ over $\mathbb{H}$. 
Rather than perform well on the specific set of training LLMs,
we would like to generalise to \emph{any} set of LLMs drawn from $\mathfrak{H}$.

We now summarise the dynamic LLM routing problem as:
\begin{align}
    \nonumber
    \min_{r \in \mathscr{R}} &\thinspace \mathbb{E}_{(\bx, \by, \mset)}\left[\sum_{m\in[|\mset|]} \1( r( \bx, \mset ) = m ) \cdot \ell(\bx,\by,h^{(m)}) \right] \colon \\
    &\thinspace 
    \hspace{-10pt}
    \mathbb{E}_{(\bx, \by, \mset)}\left[\sum_{m\in[|\mset|]} \1( r( \bx, \mset ) = m ) \cdot c( h^{(m)} ) \right] \le B,
    \label{eq:risk}
\end{align}
where 
as before $B \in \mathbb{R}_{+}$ denotes a cost budget,
$\mset \defeq \{ h^{(1)}, \ldots, h^{(M)} \} \sim \mathfrak{H}$ denotes a sample of $M$ LLMs,  
and $c \colon \mset_{\rm all} \to \Real_+$ denotes the cost of a given LLM.



\subsection{Optimal Routing with a Dynamic Pool}

To guide the design of a suitable dynamic router, 
we begin by studying the nature of the \emph{Bayes-optimal} rule for~\eqref{eq:risk}.
Interestingly, this rule decomposes across each of the constituent LLMs.
Note that the result closely mirrors~\citet[Eq.\ 6]{Tailor:2024} (which was derived in the related setting of learning to defer to an expert),
and may be seen as a generalization of Lemma F.1 of \citet{Jitkrittum:2023} to an arbitrary loss.

\begin{prop}[Optimal dynamic routing]
\label{prop:optimal_rule} 
Under a mild regularity condition on $\mathbb{P}$,
for any 
input
$\bx \in \XCal$,
LLM candidate set $\mset \in \mathbb{H}$,
and budget $B > 0$,
the optimal dynamic router $r^{*}$ for the constrained optimization
in \eqref{eq:risk} is
\begin{equation}
\label{eq:opt_rule}
r^{*}(\bx, \mset) = \underset{m\in[ | \mset | ]}{\argmin} \, \left[ \mathbb{E}_{\by\mid\bx}\left[\ell(\bx,\by,h^{(m)})\right]+\lambda_{\mathfrak{H}} \cdot c( h^{(m)} ) \right],
\end{equation}
where $\lambda_{\mathfrak{H}} \ge 0$ is a Lagrange multiplier.
\end{prop}



Intuitively, 
it is optimal to route to the model that has the lowest expected loss
on the given input $\bx$, 
after applying a \emph{cost adjustment} 
of $\lambda_{\mathfrak{H}} \cdot c( h^{(m)} )$ to the loss.
The hyperparameter $\lambda_{\mathfrak{H}} \ge0$
allows one to trade off the expected quality and the average cost.
Setting $\lambda_{\mathfrak{H}} =0$ corresponds to having an unlimited cost budget
($B=\infty$ in \eqref{eq:risk}), and the rule always routes to the
model that has the lowest expected loss, regardless of its cost. On
the other extreme, setting a high value of $\lambda_{\mathfrak{H}} $ will encourage
the rule to prioritize having a low average cost over high accuracy. 

The dependence of $\lambda$ on $\mathfrak{H}$ poses complications if one wishes to \emph{exactly} solve the objective for a fixed $B$.
However, if one wishes to \emph{sweep} $B$ to trace a deferral curve (\S\ref{sec:background}),
one may equally treat $\lambda_{\mathfrak{H}}$
as a constant to be swept from $[ 0, +\infty )$.

\paragraph{Special case: 0-1 Loss}
For ease of exposition, consider a setting where an LLM response 
may be compared 
to a ground-truth
target
based on an exact match criteria. 
In this case, one
may set the loss $\ell(\bx,\by,h^{(m)})=\1[h^{(m)}(\bx)\neq\by]$
i.e., the 0-1 loss,
with
values either 
0 (incorrect) or 1 (correct).
Here, the optimal rule in \eqref{eq:opt_rule} becomes
\begin{align}
r^{*}(\bx, \mset) &= 
\underset{m\in[ |\mset| ]}{\argmin} \, \left[ \gamma( \bx, h^{(m)} ) +\lambda\cdot c( h^{(m)} ) \right] \label{eq:opt_rule01} \\
\gamma( \bx, h ) &\defEq \mathbb{P}\left[\by\ne h(\bx)\mid\bx\right].\nonumber
\end{align}
For simplicity, we will focus on the the 0-1 loss henceforth, and consider
\eqref{eq:opt_rule01} as the optimal routing rule;
our results can be readily adapted to other loss functions
by considering \eqref{eq:opt_rule}.
Example problems where the 0-1 loss are appropriate include
BoolQ \citep{ClaLeeCha2019}
with yes/no answers, 
MMLU (Massive Multitask Language Understanding,
\citet{HenBurBas2021}) with four-choice answers, and 
problems in
the SuperGLUE benchmark~\citep{WanPruNan2019}. 


\subsection{Parameterising a Dynamic Router}
\label{sec:new_llms}

Recall from \S\ref{sec:background} 
(e.g.,~\eqref{eqn:post-hoc},~\eqref{eqn:bert-router})
that one standard routing approach is to
estimate per-LLM quality scores,
e.g., via
$$ \hat{\gamma}^{(m)}( \bx ) = \mathbf{w}_{m}^\top \QueryEmbed( \bx ). $$
Such routing is tied to the set 
$\msetTrain = \{ h^{(m)} \colon m \in [ M ] \}$
of LLMs observed during training:
we have a single parameter vector $\mathbf{w}_{m}$ for each $m \in [ M ]$.
To accommodate potentially new LLMs
$\msetNew = \{ \hNew^{(n)} \colon n \in [ N ] \}$
during evaluation,
a na\"{i}ve recourse would be to retrain the router;
however, per earlier discussion,
this may prove prohibitively expensive.

To move beyond this, a natural idea is to 
construct a generic \emph{LLM feature map}
$\LLMEmbed \colon \mset_{\rm all} \to \Real^{D'}$.
We may then compute
$$ \hat{\gamma}( \bx, \hNew^{( n )} ) = F( \QueryEmbed( \bx ), \LLMEmbed( \hNew^{( n )} ) ), $$
for suitable $F \colon \Real^{D} \times \Real^{D'} \to \Real$;
e.g.,
$F( \QueryEmbed( \bx ), \LLMEmbed( \hNew^{( n )} ) ) = \LLMEmbed( \hNew^{( n )} )^\top \mathbf{A} \QueryEmbed( \bx )$,
with $\mathbf{A} \in \Real^{D' \times D}$.
Such a model 
no longer requires $\hNew^{( n )} \in \msetTrain$;
this is analogous to classic semantic output codes for zero-shot image classification~\citep{Palatucci:2009}.
Concretely, for any set of test LLMs $\msetNew = \{ \hNew^{(n)} \}_{n \in [ N ]}$,
we may estimate~\eqref{eq:opt_rule01} via
$$ \hat{r}(\bx, \msetNew) = 
\underset{n \in [ N ]}{\argmin} \, \left[ \hat{\gamma}( \bx, \hNew^{(n)} ) +\lambda\cdot c( \hNew^{(n)} ) \right]. $$
We now consider
two 
critical 
unanswered
questions:
what are reasonable choices of 
$\QueryEmbed( \bx )$ and
$\LLMEmbed( \hNew^{( n )} )$? 


\textbf{Input prompt representation.}\ 
Following prior work~\citep{HuBieLi2024},
a natural choice 
for $\QueryEmbed( \bx )$
is a frozen general-purpose text embedding,
such as 
\texttt{text-embedding-3}~\citep{OpenAI:2025},
NV-Embed~\citep{Lee:2025},
E5-Mistral-7B~\citep{Wang:2024},
and
Gecko~\citep{LeeDaiRen2024}.

An alternate approach is to construct 
a binary vector of \emph{query attributes},
denoting whether a query possesses certain characteristics~\citep{Li:2024c,Li:2024d},
e.g.,
whether it 
requires multi-step reasoning,
seeks a single correct answer,
and so on.
These can be seen as a generalised 
``task''.
Compared to a general purpose text embedding,
such a representation is a coarser representation;
on the other hand,
for the purposes of model routing,
this can help mitigate overfitting.

\textbf{LLM representation.}\ 
A good choice for $\LLMEmbed( \hNew^{( n )} )$ is less apparent than that for $\QueryEmbed( \bx )$.
A na\"{i}ve idea 
is to simply flatten the LLM's trained parameters.
However,
these would be in excess of billions of dimensions 
(exacerbating the risk of overfitting),
and 
are inadmissible for many proprietary LLMs.
Similarly,
a one-hot encoding of the training LLMs
would take us back to the original proposal~\eqref{eqn:bert-router},
which does not generalise to new LLMs.
This is analogous to the cold-start problem in collaborative filtering~\citep{Schein:2002}.

While these na\"{i}ve ideas are inadmissible, 
we now examine an alternative that represents each LLM based on \emph{its performance on a subset of prompts}.


