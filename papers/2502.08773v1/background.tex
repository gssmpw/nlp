
We now describe the relevant background for LLM routing.

\textbf{Language models (LMs)}.
Given a finite, non-empty vocabulary of \emph{tokens} $\mathscr{V}$,
a \emph{language model} (\emph{LM})
is a distribution 
$\pLM \in \Delta( \mathscr{V}^* )$,
where
$\mathscr{V}^* \defEq \bigcup_{n = 0}^{\infty} \mathscr{V}^n$
and $\Delta( \cdot )$ denotes the set of distributions over a set.
Often, 
an LM is specified via a family of \emph{next-token probabilities}
$\{ p_{\rm NT}( \cdot \mid \boldsymbol{x} ) \in \Delta( \mathscr{V} \cup \{ \perp \} ) \colon \boldsymbol{x} \in \mathscr{V}^* \}$,
where $\perp \notin \mathscr{V}$ denotes a special end-of-sequence symbol.
Concretely, 
for any {sequence} $\boldsymbol{y} = ( y_1, \ldots, y_n ) \in \mathscr{V}^n$,
one constructs
$\pLM( y_1, \ldots, y_n ) = p_{\rm NT}( \perp \mid \boldsymbol{y} ) \cdot \prod_{i = 1}^{n} p_{\rm NT}( y_{i} \mid \boldsymbol{y}_{< i} )$~\citep{Cotterell:2024}.
Here, $\boldsymbol{y}_{< i} \defEq ( y_1, \ldots, y_{i - 1} )$.



\emph{Large} language models (\emph{LLMs})
are LMs
with a high parameter-count.
LLMs are typically (though not exclusively~\citep{Gu:2024b}) based on Transformers~\citep{Vaswani:2017},
and
have proven highly versatile.

\textbf{LLMs for predictive tasks}.
We consider the following prediction problem.
Let $\mathscr{X} \subset \mathscr{V}^*$ be a set of \emph{input prompts},
and $\mathscr{Y}$ be a set of \emph{targets}.
The precise semantics of $\mathscr{Y}$ may vary based on the application;
e.g., we may have 
a simple classification problem
wherein
$\mathscr{Y} = \{ 0, 1 \}$,
or 
a code generation problem wherein
$\mathscr{Y} \subset \mathscr{V}^*$.
We assume there is some underlying (unknown) distribution $\Pr$ over $\mathscr{X} \times \mathscr{Y}$.

Let
$\ell \colon \mathscr{X} \times \mathscr{Y} \times \mathscr{Y} \to \Real_+$ denote a \emph{loss function},
where
$\ell( \boldsymbol{x}, \boldsymbol{y}, \hat{\boldsymbol{y}} )$
measures the \emph{loss} (or \emph{disutility}) of a \emph{predicted} response $\hat{\boldsymbol{y}}$ on a given (prompt, target) pair $( \boldsymbol{x}, \boldsymbol{y} )$.
For example, 
$\ell( \boldsymbol{x}, \boldsymbol{y}, \hat{\boldsymbol{y}} ) = \1( \boldsymbol{y} \neq \hat{\boldsymbol{y}} )$ stringently measures whether the response is an exact string match of the target.
Our goal is to construct a \emph{predictor}
$h \colon \mathscr{X} \to \mathscr{Y}$ with low expected loss or \emph{risk}
$R( h ) \defEq \mathbb{E}_{( \boldsymbol{x}, \boldsymbol{y} ) \sim \Pr}\left[ \ell( \boldsymbol{x}, \boldsymbol{y}, h( \boldsymbol{x} ) ) \right]$.

An LLM natively provides a distribution over $\mathscr{V}^*$.
To
convert
such a distribution to a predicted target,
we assume there is some (task-specific, and possibly randomised) \emph{prediction function} $\texttt{predict} \colon \Delta( \mathscr{V}^* ) \to \mathscr{Y}$;
e.g., in the simple case where $\mathscr{Y} \subset \mathscr{V}^*$, 
we may employ a standard decoding 
algorithm~\citep{Ficler:2017,Fan:2018,Holtzman:2020}.
More generally, $\texttt{predict}$ may involve some non-trivial processing 
(e.g., stripping non-numeric tokens).
Given such a function, we may construct
$h( \boldsymbol{x} ) \defEq \texttt{predict}( \pLM( \cdot \mid \boldsymbol{x} ) )$,
and seek to minimise $R( h )$.




\textbf{Model routing}.
Model routing is a means for achieving efficiency at inference time
by selecting an appropriate LLM 
for each input prompt. 
Inference efficiency is gained by sparingly calling a large
model only on complex input prompts.



We consider routing in the context of the above prediction setup.
Suppose we have a collection 
of $M \geq 2$ LLMs
$\{ \pLM^{(m)} \}_{m \in [M]}$,
with corresponding \emph{inference costs}
$\{ c^{(m)} \}_{m \in [M]}$
denoting,
e.g.,
the average latency of invoking each LLM.
We assume $c^{(1)} \leq c^{(2)} \leq \ldots \leq c^{(M)}$.
Let $r \colon \mathscr{X} \to [ M ]$ be a router that,
given a prompt, predicts the most suitable LLM.
We seek a router which achieves
\begin{align}
    \nonumber
    \min_{r \colon \mathscr{X} \to [ M ]} 
    &\sum_{m \in [ M ]} \mathbb{E}_{( \boldsymbol{x}, \boldsymbol{y} )}\left[ \1( r( \boldsymbol{x} ) = m ) \cdot \ell( \boldsymbol{x}, \boldsymbol{y}, h^{(m)}) \right] \colon \\
    \label{eqn:static-router}
    &\sum_{m \in [ M ]} \mathbb{E}_{( \boldsymbol{x}, \boldsymbol{y} )}\left[ \1( r( \boldsymbol{x} ) = m ) \cdot c^{(m)} \right] \leq B.
\end{align}
Here, $B \in \Real_+$ is some fixed budget on the cost of the routed solution.
We use $h^{(m)}( \boldsymbol{x} ) \defEq \texttt{predict}( \pLM^{(m)}( \cdot \mid \boldsymbol{x} ) )$ for some fixed prediction function $\texttt{predict}$.

The above can equally be seen as constructing a \emph{routed} model $h_{\rm RM}( \bx, r ) \defEq \sum_{m \in [ M ]} \1( r( \boldsymbol{x} ) = m ) \cdot h^{(m)}( \bx )$.



\textbf{Evaluation: deferral curve}.
We generally measure performance via a \emph{deferral curve}~\citep{Jitkrittum:2023,WanAugRus2024,HuBieLi2024}.
This is a curve 
$\mathscr{C} = \{ ( B, R( h_{\rm RM}( \cdot, r_B ) ) \colon B \in [ c^{(1)}, c^{(M)} ] \}$
tracing the tradeoff between the cost budget $B$ and loss of the resulting routed model.
Specifically, one varies the cost budget $B \in [ c^{(1)}, c^{(M)} ]$;
computes a router $r_B( \cdot )$ for this budget;
and plots
the resulting expected loss  $R( h_{\rm RM}( \cdot, r_B ) )$.
We may also use a quality metric (e.g., accuracy) in place of the loss to 
capture quality-cost trade-offs.


For budget $B = c^{(1)}$,
the constraints require always picking the smallest LLM (assuming distinct costs),
and thus one has the expected loss of the smallest LLM.
For budget $B = c^{(M)}$, 
it is admissible to always pick the largest LLM,
which is typically the LLM which achieves lowest expected loss.

\textbf{Pareto-random routing.}{ 
An elementary approach to multi-model routing is to  identify the \emph{Pareto-optimal} points %
from the set of cost-risk pairs $\{(c^{(m)}, R( h^{(m)}): m \in [M]\}$, and to route amongst the LLMs that lie on the Pareto-front. Specifically, given a budget $B \in ( c^{(1)}, c^{(M)} )$, we may pick the two nearest costs $c^{(m_1)} \leq B <  c^{(m_2)}$ from the Pareto-front, and route a query to LLM $h^{(m_1)}$ with probability $\frac{c^{(m_2)} - B}{c^{(m_2)} - c^{(m_1)}}$ and to $h^{(m_2)}$ with probability $\frac{B - c^{(m_1)}}{c^{(m_2)} - c^{(m_1)}}$. Despite its simplicity (i.e., input-agnostic routing), this approach is often seen to be a strong baseline \citep{HuBieLi2024}.
}

\textbf{Model routing strategies}.
\citet{NarJitMen2022,HuBieLi2024} proposed a 
post-hoc approach to model routing.
Here, one constructs an estimator 
$\hat{\gamma}^{(m)} \colon \mathscr{X} \to \Real_+$ of the expected loss incurred by each LLM,
and route via
\begin{equation}
    \label{eqn:post-hoc}
    r( \boldsymbol{x} ) = \underset{m \in [M]}{\argmin} \, \left[ \hat{\gamma}^{(m)}( \boldsymbol{x} ) + \lambda \cdot c^{(m)} \right].
\end{equation}
Here, 
$\lambda \geq 0$ is a hyper-parameter trading between cost and quality.
In \S\ref{sec:proposal}, we show formally that the routing rule in \eqref{eqn:post-hoc} is in fact a plug-in estimator of a theoretically optimal routing rule, complementing \citet{HuBieLi2024}'s findings.


One may consider different approaches to constructing $\hat{\gamma}$
given a training sample 
$\{ ( \boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)} ) \}_{i = 1}^{N}$.
For example, a $K$-NN estimator can be used~\citep[Section 5.1]{HuBieLi2024}:
\begin{equation}
    \label{eqn:knn-router}
    \hat{\gamma}^{(m)}( \boldsymbol{x} ) = \frac{1}{k} \sum_{i \in {\rm NN}( \boldsymbol{x} )} \ell( \boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)}, h^{(m)} ),    
\end{equation}
where ${\rm NN}( \boldsymbol{x} )$ denotes the set of $K$-nearest neighbours in the training sample, as measured in a suitable embedding space.
An alternative is a linear model:
\begin{equation}
    \label{eqn:bert-router}
    \hat{\gamma}^{(m)}( \boldsymbol{x} ) = \mathbf{w}_{m}^\top \QueryEmbed( \boldsymbol{x} ),
\end{equation}
where
$\QueryEmbed \colon \mathscr{X} \to \Real^{D}$ is a sequence embedding model (e.g., BERT, Sentence-T5),
and each $\mathbf{w}_m \in \Real^D$.
Here, each $\mathbf{w}_m$ (and possibly $\QueryEmbed$)
are fit to minimise a suitable empirical loss 
such as the mean squared error:
$$ \frac{1}{N} \sum_{i \in [ N ]} \sum_{m \in [ M ]} ( \ell( \bx^{(i)}, \by^{(i)}, h^{(m)} ) - \hat{\gamma}^{(m)}( \bx^{(i)} )^2 )^2. $$
\citet{OngAlmWu2024} proposed a matrix factorisation approach:
$$ \hat{\gamma}^{(m)}( \boldsymbol{x} ) = \mathbf{v}_{m}^\top \mathbf{A} (\QueryEmbed( \boldsymbol{x} ) + \mathbf{b}) $$
for a \emph{frozen} text embedding $\QueryEmbed \colon \XCal \to \Real^{D}$, 
$\mathbf{v}_m \in \Real^{D'}$,
and $\mathbf{A} \in \Real^{D' \times D}$.
This score was proposed to be fit on a training sample comprising \emph{pairwise} comparisons only.
Compared to~\eqref{eqn:bert-router},
the key distinctions are the freezing of the input embedding,
and the possibility that $D \neq D'$.





