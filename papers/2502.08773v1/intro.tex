Large language models (LLMs) have seen a flurry of development in recent years, 
with consistent advances in their capabilities~\citep{Radford:2018,Radford:2019,Brown:2020,Touvron:2023,Anil:2023,Grattafiori:2024,DeepSeekAI:2024}.
Their impressive abilities notwithstanding,
the inference cost of LLMs can be prohibitive~\citep{Li:2024e,Wan:2024,Zhou:2024b}.
This has motivated a range of techniques to improve inference efficiency,
such as 
speculative decoding~\citep{Stern:2018,chen2023accelerating,leviathan2023fast}, 
early-exiting~\citep{SchFisGup2022}, 
quantisation~\citep{Chee:2023}, 
pruning~\citep{Frantar:2023},
distillation~\citep{Agarwal:2024,Rawat:2024}, 
and others~\citep{Pope:2023,Aishwarya:2024,Menghani:2022}.

Our interest is in \emph{model routing} for efficient inference.
Here,
one maintains a pool of candidate LLMs of various sizes and capabilities.
Given a query, 
one learns to predict the lowest-cost LLM which can reasonably address the query.
In doing so, one can learn to use high-cost LLMs sparingly, only on the (relatively) few ``hard'' inputs.
This is a conceptually simple but effective technique,
which has seen a surge of recent interest~\citep{Hendy:2023,Narayanan:2023,Ding:2024,Sakota:2024,CheJiaLin2024,HuBieLi2024,Shnitzer:2023,Wang:2023,Stripelis:2024,OngAlmWu2024,ZhuWuWen2024,Feng:2024,LuYuaLin2024,Zhao:2024,Dann:2024,Aggarwal:2024,Lee:2024}.

Existing works largely focus on routing over a \emph{fixed} pool of LLMs, typically two.
In practice, however, the pool of candidate LLMs can constantly change;
e.g., older LLMs may be deprecated in favor of new, performant LLMs.
Ideally, a router ought to leverage these new LLMs.
To achieve this,
perhaps the simplest approach is to retrain the router as the candidate pool varies.
However, 
with frequent changes to the LLM pool,
such retraining may be impractical 
owing to the non-trivial costs
of 
both model retraining,
as well as 
obtaining suitable \emph{training labels} for each new LLM.

In this paper, we 
formalise this \emph{dynamic} routing problem, wherein unobserved LLMs are available at test time.
We propose a new approach to this problem that relies on representing each LLM as a \emph{feature vector}, derived based on 
\emph{prediction correctness} 
on a set of representative prompts.
This mirrors recent trends in using 
vectors of model predictions to improve pre-training data selection~\citep{Thrush:2024},
enable generalisation of \emph{deferral to an expert}~\citep{Tailor:2024},
and constructing generic LLM embeddings~\citep{ZhuWuWen2024}.
Based on this, we detail two effective 
routing
strategies, relying on \emph{cluster-based} routing and \emph{learned cluster map} respectively.

Our solution allows a trained router to make use of these test-time LLMs without retraining.
This is unlike most, though not all existing solutions to the problem;
indeed, recent works~\citet{Feng:2024,Zhao:2024,Li:2025} have also proposed solutions to dynamic routing.
Compared to these works, we provide a formally-grounded approach that 
avoids dependence on exogenous \emph{task label} information for prompts, and relies on simple statistical learning primitives; 
we defer a complete discussion to~\S\ref{sec:discussion_related}.



In sum, our contributions are:
\begin{enumerate}[label=(\roman*),itemsep=0pt,topsep=0pt,leftmargin=16pt]
    \item we 
    formalise the
    problem setting of model routing with a \emph{dynamic} pool of LLMs (\S\ref{sec:dynamic_routing});

    \item we propose
    a new approach to routing,
    relying on representing each LLM with a suitable \emph{feature vector} derived from its \emph{prediction correctness} (\S\ref{sec:correctness_representation});
    equipped with this, 
    we propose two effective techniques 
    based on unsupervised clustering or supervised learning (\S\ref{sec:cluster_router}, \S\ref{sec:two_tower}), with an accompanying excess risk bound (\S\ref{sec:excess-risk})

    \item we present experiments on several benchmark datasets, including 
    EmbedLLM \citep{ZhuWuWen2024}, MixInstruct \citep{Jiang:2023}, RouterBench~\citep{HuBieLi2024}, and Chatbot Arena~\citep{OngAlmWu2024}, and effectively route amongst more than 30 unseen LLMs.
\end{enumerate}
