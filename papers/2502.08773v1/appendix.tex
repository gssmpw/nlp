\input{proofs}

\section{Experimental Setup}

We provide more details on the experiments discussed in \cref{sec:experiments}.

\subsection{Splitting Data and LLMs}

In the experiment on each of the three datasets (EmbedLLM \citep{ZhuWuWen2024}, MixInstruct \citep{Jiang:2023},  and RouterBench \citep{HuBieLi2024}), we split the data into three disjoint portions:  train (60\%), validation (10\%), and test (30\%). 
The set of all LLMs available in each dataset is also split into two disjoint sets: training models and testing models. The relationship of data splits and model splits is summarized in the following table.


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\shadecell}{ \cellcolor{gray!25}}
\begin{center}
\begin{tabular}{l|>{\centering\arraybackslash}p{49mm}|>{\centering\arraybackslash}p{14mm}|>{\centering\arraybackslash}p{28mm}}
\toprule
    & Train (60\%) & Validation (10\%) & Test (30\%) \\
\midrule 
  Training models & \shadecell \cmark & \shadecell \cmark & \xmark \\
  Testing models & \xmark    & \shadecell \cmark & \shadecell \cmark \\ 
\bottomrule
\end{tabular}
\end{center}

\begin{itemize}
    \item \textbf{Training set}. The training examples are meant for router training. Only information of the training models (not testing models) is available in this data portion.  The only exception is the clairvoyant fixed-pool router baseline which is allowed access to correctness labels of testing models  on training examples. In other words, unlike other baselines, the clairvoyant fixed-pool router observes all models during training, and is trained on both training and validation portions. This baseline is meant to establish performance achievable if a router has access to all models.
    \item \textbf{Validation set}. The validation examples are meant to be used to represent new LLMs. For instance, for our proposed K-means (Gecko) approach, the validation set is used to compute per-cluster performance metrics of each testing LLM observed at test time, to represent it as a feature vector.
    \item \textbf{Test set}. The test examples are only used for evaluating routing methods by evaluating their deferral curves.
\end{itemize}

Testing models represent new models that arrive at deployment time, and are not available for training (except to the clairvoyant fixed-pool router baseline).
Training models are meant for router training. For instance, our proposed K-means (Gecko) approach learns to route among the training models, and is tested on the test set to route among the testing models.







\section{Additional Experimental Results}
We present additional experimental results we omitted in the main text.

\subsection{Selecting K}
\label{sec:validate_k}
There are four baselines that we consider in the experiments in \cref{sec:experiments} that depend on a hyperparameter $K$.
Specifically, $K$ in K-NN refers to the number of nearest neighbors, and $K$ in K-means (Gecko), K-means (Attributes) and Learned cluster map refers to the number of clusters. In \cref{fig:three_experiments}, we report the performance of these methods with the best $K$ found on each dataset separately for each method.
We now describe the validation procedure we used to select the best $K$.

\paragraph{K-NN} 
For each candidate $K$, and each query in the validation set, find the $K$ nearest queries in the training set (in the Gecko embedding space). Route each query in the validation set to the most appropriate \emph{training} LLM according to the routing rule \eqref{eq:opt_rule01} where $\gamma^{(m)}$ is estimated with \eqref{eqn:knn-router}. Produce a deferral curve on the validation set, and compute the normalized area under such curve. Select $K$ that maximizes the area.

\paragraph{K-means (Gecko)} 
For each candidate $K$, perform $K$-means on the training set using Gecko embeddings \citep{LeeDaiRen2024}.
Compute the feature vector representation of each \emph{training} LLM on the training set using \eqref{eq:cluster_accs}. 
For each query in the validation set, find the nearest cluster, and route the query to the most appropriate \emph{training} LLM according to the routing rule \eqref{eq:cluster-routing}. Produce a deferral curve on the validation set, and compute the normalized area under such curve. Select $K$ that maximizes the area.

\paragraph{K-means (Attributes)}
Parameterize the query embedding model to be  $\Phi(\bx) = \sigma(\mathbf{V}^\top \mathrm{Gecko}(\bx))$ where $\mathbf{V} \in \mathbb{R}^{768 \times 7}$, and $\sigma$ denotes the sigmoid function.
Train each head $\mathbf{v}_j \in \mathbb{R}^{768}$  (with $\mathrm{Gecko}$ frozen) by minimizing the sigmoid cross entropy to predict whether the $j$-th semantic attribute is active on each input query.  We use the seven prompt difficulty attributes as described in \citet{Li:2024c}, and prompt Gemini 1.5 Pro 002 to annotate each binary attribute on each training example.
Once the query embedding model $\Phi$ is trained, we freeze it, and perform the same hyperparameter selection procedure as used for K-means (Gecko) by replacing the Gecko embedding function with $\Phi$.

\paragraph{Learned cluster map}
For each candidate $K$, perform $K$-means on the training set using Gecko embeddings.
Compute the feature vector representation of each \emph{training} LLM on the training set using \eqref{eq:cluster_accs}. 
Parameterize the cluster assignment map with a softmax-dense layer as described in  \cref{sec:two_tower}, resulting in a parameter $\boldsymbol{\theta}$ to learn. Learn the parameter by minimizing the binary sigmoid cross entropy on the training with the labels given by the correctness labels of the \emph{training} LLMs.
With the cluster map trained, for each query in the validation set, route the query to the most appropriate training LLM according to the routing rule \eqref{eq:cluster-routing-learned}.
Produce a deferral curve on the validation set, and compute the normalized area under such curve. Select $K$ that maximizes the area.


\cref{fig:validate_k} shows the area under the deferral curve (on the validation set) vs candidate parameter $K$. Importantly, the testing models and the test set are never used in the above hyperparameter selection process.

\newcommand{\imgw}{0.9\textwidth}
\begin{figure*}[th]
  \begin{minipage}[b]{.98\linewidth}
  
\begin{center}
 EmbedLLM \hspace{30mm} MixInstruct \hspace{28mm} RouterBench
 \end{center}
 \vspace{2mm}
 
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/embedllm/embedllm-model_select-knn_gecko-7ks.pdf}
        \caption{K-NN}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/mixinstruct/mixinstruct-model_select-knn_gecko-7ks.pdf}
        \caption{K-NN}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/routerbench/routerbench-model_select-knn_gecko-7ks.pdf}
        \caption{K-NN}
    \end{subfigure}
    
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/embedllm/embedllm-model_select-kmeans_gecko-5ks.pdf}
        \caption{K-means (Gecko)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/mixinstruct/mixinstruct-model_select-kmeans_gecko-5ks.pdf}
        \caption{K-means (Gecko)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/routerbench/routerbench-model_select-kmeans_gecko-5ks.pdf}
        \caption{K-means (Gecko)}
    \end{subfigure}
    
   \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/embedllm/embedllm-model_select-kmeans_attributes-5ks.pdf}
        \caption{K-means (Attributes)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/mixinstruct/mixinstruct-model_select-kmeans_attributes-5ks.pdf}
        \caption{K-means (Attributes)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/routerbench/routerbench-model_select-kmeans_attributes-5ks.pdf}
        \caption{K-means (Attributes)}
    \end{subfigure}
    
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/embedllm/embedllm-model_select-twotower_gecko-5ks.pdf}
        \caption{Learned cluster map}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/mixinstruct/mixinstruct-model_select-twotower_gecko-4ks.pdf}
        \caption{Learned cluster map}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\imgw]{figs/routerbench/routerbench-model_select-twotower_gecko-5ks.pdf}
        \caption{Learned cluster map}
    \end{subfigure}
  \end{minipage}
  \caption{Validation performance of the four methods considered in \cref{fig:three_experiments}: K-NN, K-means (Gecko), K-means (Attributes), and Learned cluster map. See \cref{sec:validate_k} for more details.
  }
  \label{fig:validate_k}
\end{figure*}


\clearpage
\newpage
\subsection{Results on Individual Datasets in EmbedLLM}
To supplement results on EmbedLLM in \cref{fig:embedllm}, 
we further evaluate  the same router models separately on the 10 datasets contained in EmbedLLM. 
The results are shown in \cref{fig:embedllm_each_dataset}.

\newcommand{\iw}{0.99\textwidth}
\begin{figure}[ht]
  \begin{minipage}[b]{.98\linewidth}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_asdiv.pdf}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_gpqa.pdf}
    \end{subfigure}
     \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_gsm8k.pdf}
    \end{subfigure}
    
     \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_logiqa.pdf}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_mathqa.pdf}
    \end{subfigure}
     \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_medmcqa.pdf}
    \end{subfigure}
    
    
     \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_mmlu.pdf}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_piqa.pdf}
    \end{subfigure}
     \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_social_iqa.pdf}
    \end{subfigure}
  
  \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/embedllm-alldatasets-k_val_optimal_truthfulqa_mc1.pdf}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=\iw]{figs/jeevesh/embedllm/legend.pdf}
    \end{subfigure}
    
  \end{minipage}
  \caption{Evaluation of the router models used in \cref{fig:embedllm}  on the 10 datasets contained in EmbedLLM.} 
  \label{fig:embedllm_each_dataset}
\end{figure}

\clearpage
\newpage
\subsection{Train on Chatbot Arena and Test on EmbedLLM}
\label{sec:chatbot_exp_details}

\begin{figure}[th]
\vspace{4mm}
  \begin{minipage}[t]{.49\linewidth}
  \vspace{-\topskip}
\includegraphics[width=0.9\textwidth]{figs/chatbot_arena/chatbot_arena-test_embedllm.pdf}
  \end{minipage}%
  \hfill%
  \begin{minipage}[t]{.49\linewidth}
  \vspace{-\topskip}
\begin{tabular}{lrrr}
\toprule
Method & Area$\uparrow$ & QNC$\downarrow$ & Peak Acc.$\uparrow$\\
\midrule
Pareto-random router & .507 & $\infty$ & 51.9\% \\
K-NN & .472 & $\infty$ & 52.1\% \\
K-means (Gecko) & .529 & $\infty$ & 54.5\% \\
K-means (Attributes) & \best{.545} & \best{.97} & \best{55.8\%} \\
\bottomrule
\end{tabular}
  \end{minipage}
\caption{Deferral curves of routers trained on Chatbot Arena with pairwise comparison labels, and tested on EmbedLLM with per-query correctness labels.}
\label{fig:chatbot_arena}
\end{figure}

Representing each prompt with a small number of attributes that capture its inherent hardness shines when there is a query distribution shift at test time.
To illustrate this, we compare  Gecko-based query representation and attribute-based representation by training on Chatbot Arena conversation data \citep{ZheChiShe2023}, and testing on EmbedLLM, which contains mostly Q\&A prompts.
To reduce confounding factors, we train on all LLMs that are present in both datasets (26 LLMs), and test on the same set of LLMs (i.e., no unseen LLMs at test time).   After appropriate filtering, the Chatbot Arena dataset has 8447 records left. The filtering step ensures that we only deal with LLMs that are present in both datasets. These examples are split further into 90\% training and 10\% validation splits.

The Chatbot Arena dataset contains pairwise comparison labels: each user query is responded to by two random LLMs, to which the user selects the better response. To evaluate per-cluster performance for representing each LLM, we fit the Bradley-Terry-Luce model \citep{bradley1952rank} to the pairwise comparison labels within a cluster and estimate the pointwise quality scores for each LLM for that cluster. We use the full EmbedLLM dataset for testing.


The results are shown in \cref{fig:chatbot_arena}.
We observe that K-means (Attributes) in this case performs better than K-means (Gecko), suggesting that using prompt hardness attributes helps improve robustness to query distribution shifts. 
In fact, this routing approach is the only method that can reach the performance of the most accurate model in the pool, thus attaining a finite quality-neutral cost (QNC). The reason the Pareto-random router has a decreasing trend is because the Pareto-optimal LLMs are chosen using the validation set, and turn out to be not optimal for the test set.

\ifarxiv
\else
\section{Additional Related work}
\label{app:related}
\input{additional_related}
\fi

