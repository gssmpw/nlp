\section{Proofs of results in main body}
\todoakm{fix proofs to not use subscript}

In what follows, we use $r_\mset(\bx)$ and $r(\bx, \mset)$ interchangeably.

\subsection{Proof of Proposition \ref{prop:optimal_rule}}

\begin{prop*}[Restated]
For $\bx \sim \mathbb{P}$, assume $\eta(\bx) = \mathbb{P}(\by \mid \bx)$ is a continuous random variable. Then 
for any 
input
$\bx \in \XCal$,
LLM candidate set $\mathscr{H} \in \mathbb{H}$,
and budget $B > 0$,
the optimal dynamic router $r^{*}$ for the constrained optimization
in \eqref{eq:risk} is
\begin{equation}
r^{*}(\bx, \mathscr{H}) = \underset{m\in[ | \mathscr{H} | ]}{\argmin} \, \left[ \mathbb{E}_{\by\mid\bx}\left[\ell(\bx,\by,h^{(m)})\right]+\lambda_{\mathfrak{H}} \cdot c^{(m)} \right],
\end{equation}
where $\lambda_{\mathfrak{H}} \ge 0$ is a Lagrange multiplier.
\end{prop*}

\begin{proof}
Under the assumption on $\mathbb{P}$, the constrained problem in \eqref{eq:risk}
is equivalent to minimizing the following Lagrangian objective for
some Lagrange multiplier $\lambda_{\mathfrak{H}}\ge0$ \citep{neyman1933ix}:
\begin{align*}
\mathcal{L} & =\mathbb{E}_{(\bx,\by,\mset)}\left[\sum_{m\in[|\mathscr{H}|]}\1(r(\bx, \mathscr{H})=m)\cdot\ell(\bx,\by,h^{(m)})\right]+\lambda_{\mathfrak{H}}\cdot\mathbb{E}_{(\bx,\by,\mathscr{H})}\left[\sum_{m\in[|\mathscr{H}|]}\1(r(\bx, \mathscr{H})=m)\cdot c^{(m)}\right]\\
 & \stackrel{(a)}{=}\mathbb{E}_{\mset}\mathbb{E}_{\bx}\mathbb{E}_{\by|\bx}\left[\sum_{m\in[|\mathscr{H}|]}\1(r(\bx, \mathscr{H})=m)\cdot\left\{ \ell(\bx,\by,h^{(m)})+\lambda_{\mathfrak{H}}\cdot c^{(m)}\right\} %
 \right]\\
 & =\mathbb{E}_{\mset}\mathbb{E}_{\bx}\bigg[
 \underbrace{
 \sum_{m\in[|\mathscr{H}|]}\1(r(\bx, \mathscr{H})=m)\cdot\left\{ \mathbb{E}_{\by|\bx}\left[\ell(\bx,\by,h^{(m)})%
 \right]+\lambda_{\mathfrak{H}}\cdot c^{(m)}\right\}
 }_{\mathcal{L}_{\mset,\bx}}
 \bigg],
\end{align*}
where (a) uses the fact that the draw of $\mset$ is independent of the draw of $(\bx,\by)$. 
The last line makes it clear that for any fixed $\mset$ %
and any fixed $\bx$, to minimize the overall loss, the router ought to
route to the model that has the lowest cost-adjusted loss $\mathcal{L}_{\mset,\bx}$. Thus, 
\begin{align*}
r^{*}(\bx, \mset) & =\argmin_{m\in[|\mset|]}\mathbb{E}_{\by|\bx}\left[\ell(\bx,\by,h^{(m)})\right]+\lambda_{\mathfrak{H}}\cdot c^{(m)}.
\end{align*}
\end{proof}




\subsection{Proof of Proposition \ref{prop:cluster-routing-k-equals-1}}
\begin{proof}

Suppose there exists a $\lambda_1 \in \mathbb{R}_{\geq 0}$ and $m_1 \in \argmin_m \, \hat{\LLMEmbedScalar}_{m}+\lambda_1\cdot c^{(m)}$ such that $(c^{(m_1)}, \hat{R}_{01}( h^{(m_1)}))$ is not Pareto-optimal. Then there exists $h^{(m_2)} \in \mathscr{H}$ such that either $c^{(m_2)} < c^{(m_1)}$ and $\hat{R}_{01}( h^{(m_2)})) \leq \hat{R}_{01}( h^{(m_1)}))$, or $c^{(m_2)} \leq c^{(m_1)}$ and $\hat{R}_{01}( h^{(m_2)})) < \hat{R}_{01}( h^{(m_1)}))$. In either case, $\hat{\LLMEmbedScalar}_{m_2}+\lambda_1\cdot c^{(m_2)} = \hat{R}_{01}( h^{(m_2)}) +\lambda_1\cdot c^{(m_2)} < \hat{R}_{01}( h^{(m_1)}) +\lambda_1\cdot c^{(m_1)} = \hat{\LLMEmbedScalar}_{m_1}+\lambda_1\cdot c^{(m_1)}$, which contradicts the fact that $m_1 \in \argmin_m\, [ \hat{\LLMEmbedScalar}_{m}+\lambda_1\cdot c^{(m)} ]$. 
\end{proof}

\subsection{Proof of Proposition \ref{prop:cluster-regret-bound}}
\begin{prop*}[Restated]
For $\bx \sim \mathbb{P}$, assume $\eta(\bx) = \mathbb{P}(\by \mid \bx)$ is a continuous random variable. 
For a set of LLMs $\mathscr{H}$, 
let $r^*$ denote the Bayes-optimal routing rule in Proposition \ref{prop:optimal_rule}. 
For any $\bx \in \XCal$ and $h^{(m)} \in \mathscr{H}$, let:
\begin{align*}
    \Delta_{k}(\bx, h^{(m)}) = \left|\mathbb{P}_{\by|\bx,z=k}\left[h(\bx)\neq\by\right] \,-\, \LLMEmbedScalar_{k}(h^{(m)}) \right|.
\end{align*}
Let $R_{01}(r,\mathscr{H}) = \sum_{m} \mathbb{P} \left[h^{(m)}(\bx)\neq\by\land r_{\mset}(\bx)=m\right]$ denote the 0-1 risk.
Then the difference in 0-1 risk between $\tilde{r}^*$ and $r^*$ can be bounded as:
\begin{align*}
{\mathbb{E}_{\mathscr{H}}\left[R_{01}(\tilde{r}^*,\mathscr{H})\right] ~\leq~ \mathbb{E}_{\mathscr{H}}\left[{R}_{01}(r^*,\mathscr{H})\right]}  
 \,+\,
\mathbb{E}_{(\bx\mathscr{H})}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right].
\end{align*}
\end{prop*}

We define a proxy risk objective:
\begin{align}
\tilde{R}_{01}(r,\mathscr{H})
& =
\mathbb{E}_{\bx,z}\left[
\sum_{m \in [|\mathscr{H}|]} \LLMEmbedScalar_{z}(h^{(m)})\cdot\1\left[r(\bx, \mset)=m\right]\right],
\label{eq:proxy-risk}
\end{align}
where the expectation is over the joint distribution over $(\bx,z)$ (and not $(\bx, \by)$). 

Consider solving a variant of the constrained optimization problem in \eqref{eq:risk} where the original risk objective is replaced with the proxy risk in \eqref{eq:proxy-risk}:
\begin{align}
 & \min_{r}\thinspace\mathbb{E}_{\mathscr{H}}\left[ \tilde{R}_{01}(r,\mathscr{H}) \right]\nonumber \\
 & \text{s.t.}~~~\thinspace\mathbb{E}_{(\bx, \by, \mathscr{H})}\left[\sum_{m\in[|\mathscr{H}|]}c^{(m)} \cdot \1[r(\bx, \mset)=m]\right]\le B.\label{eq:proxy-optimization}
 \end{align}


We can then show that the optimal solution to
above proxy constrained optimization problem admits the same form as the cluster-based routing rule $\tilde{r}$ in \eqref{eq:cluster-rule-population}:
\begin{lemma}
\label{lem:cluster-opt-rule}
Under the assumption on $\mathbb{P}$ in Proposition \ref{prop:cluster-regret-bound}, for any set of models $\mathscr{H}$, the minimizer of the proxy constrained optimization problem in \eqref{eq:proxy-optimization} is given by:
\begin{align*}
\tilde{r}^{*}(\bx, \mset) 
 & =\underset{m\in[|\mathscr{H}|]}{\argmin}\sum_{k\in[K]}\mathbb{P}(z=k|\bx) \cdot \LLMEmbedScalar_{k}(h^{(m)}) +\lambda \cdot c^{(m)},
\end{align*}
for some $\lambda \geq 0$.
\end{lemma}

We will also find it useful to bound the difference between the original risk $R_{01}(r, \mset)$ and the proxy risk in \eqref{eq:proxy-risk}:
\begin{lemma}
\label{lem:cluster-Delta-bound}
For any routing rule $r$ and fixed $\mset$, 
\[
\left|R_{01}(r,\mathscr{H}) - \tilde{R}_{01}(r,\mathscr{H})\right| \,\leq\,
\mathbb{E}_{\bx}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right].
\]
\end{lemma}

We are now ready to prove Proposition \ref{prop:cluster-regret-bound}:
\begin{proof}
The excess risk we wish to bound is given by:
\begin{align*}
    \lefteqn{
    \mathbb{E}_{\mathscr{H}}\left[ 
    R_{01}(\tilde{r}^*,\mathscr{H}) \,-\, {R}_{01}(r^*,\mathscr{H})
    \right]
    }
    \\ 
    &=  
    \mathbb{E}_{\mathscr{H}}\left[ 
    \Big\{R_{01}(\tilde{r}^*,\mathscr{H})
    \,-\, \tilde{R}_{01}(\tilde{r}^*,\mathscr{H})\Big\}
    \,+\, \tilde{R}_{01}(\tilde{r}^*,\mathscr{H})
    \,-\, \tilde{R}_{01}({r}^*,\mathscr{H})
    \,+\, \Big\{\tilde{R}_{01}({r}^*,\mathscr{H})
    \,-\, {R}_{01}(r^*,\mathscr{H})\Big\} 
    \right]
    \\
    &\stackrel{(a)}{\leq}
    \mathbb{E}_{\mathscr{H}}\left[ \tilde{R}_{01}(\tilde{r}^*,\mathscr{H}) \right]
    \,-\, 
    \mathbb{E}_{\mathscr{H}}\left[ \tilde{R}_{01}({r}^*,\mathscr{H}) \right] + 2\cdot\mathbb{E}_{(\bx, \mathscr{H})}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right]\\
    &\stackrel{(b)}{\leq}
    0 + 2\cdot\mathbb{E}_{(\bx, \mathscr{H})}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right],
\end{align*}
as desired. 
To derive (a), we apply Lemma \ref{lem:cluster-Delta-bound} to bound the first and third terms. To derive (b), we use the fact that $\tilde{r}^*$  is the minimizer of the proxy-risk $ \mathbb{E}_{\mathscr{H}}\left[ \tilde{R}_{01}(\cdot,\mathscr{H}) \right] $ subject to the budget constraint in  \eqref{eq:proxy-optimization}; since $r^*$ also  satisfies the same budget constraint, it has an equal or higher expected risk than $\tilde{r}^*$.
\end{proof}



We now prove Lemmas \ref{lem:cluster-opt-rule} and \ref{lem:cluster-Delta-bound}.


\begin{proof}[Proof of Lemma \ref{lem:cluster-opt-rule}]
Under the assumption on $\mathbb{P}$, the constrained problem in \eqref{eq:proxy-optimization}
is equivalent to minimizing the following Lagrangian objective for
some Lagrange multiplier $\lambda$ \citep{neyman1933ix}:
\begin{align*}
\mathcal{L} & =\mathbb{E}_{(\bx,z,\mset)}\left[\sum_{m \in [|\mathscr{H}|]}\LLMEmbedScalar_{z}(h^{(m)}) \cdot\1\left[r(\bx, \mset)=m\right]\right]+\lambda\cdot\mathbb{E}_{(\bx,\by,\mathscr{H})}\left[\sum_{m\in[|\mathscr{H}|]}\1(r(\bx,\mset)=m)\cdot c^{(m)}\right]\\
 & \stackrel{(a)}{=}\mathbb{E}_{\mset}\mathbb{E}_{\bx}\mathbb{E}_{z|\bx}\left[\sum_{m\in[|\mathscr{H}|]}\1(r(\bx, \mset)=m)\cdot\left\{ \LLMEmbedScalar_{z}(h^{(m)}) +\lambda\cdot c^{(m)}\right\} %
 \right]\\
 & =\mathbb{E}_{\mset}\mathbb{E}_{\bx}\bigg[
 \underbrace{
 \sum_{m\in[|\mathscr{H}|]}\1(r(\bx,\mset)=m)\cdot\Big\{ 
 \sum_{k\in[K]}\mathbb{P}(z=k|\bx) \cdot  \LLMEmbedScalar_{k}(h^{(m)}) 
 +\lambda\cdot c^{(m)}\Big\}
 }_{\mathcal{L}_{\mset,\bx}}
 \bigg],
\end{align*}
where (a) uses the fact that the draw of $\mset$ is independent of the draw of $(\bx,\by)$. 
The last line makes it clear that for any fixed $\mset$ %
and any fixed $\bx$, to minimize the overall loss, the router ought to
route to the model that has the lowest cost-adjusted loss $\mathcal{L}_{\mset,\bx}$. Thus, 
\begin{align*}
\tilde{r}^{*}(\bx,\mset) 
 & =\underset{m\in[|\mathscr{H}|]}{\argmin}\sum_{k\in[K]}\mathbb{P}(z=k|\bx) \cdot \LLMEmbedScalar_{k}(h^{(m)}) +\lambda \cdot c^{(m)}.
\end{align*}
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:cluster-Delta-bound}]
Expanding the original risk, we have:
\begin{align}
R_{01}(r,\mathscr{H})
& =\mathbb{E}_{(\bx, \by)}\left[\sum_{m\in[|\mathscr{H}|]}\1\left[h^{(m)}(\bx)\neq\by\right]\cdot\1\left[r_{\mset}(\bx)=m\right]\right]
\nonumber
\\
& = \mathbb{E}_{\bx}\bigg[\sum_{m\in[|\mathscr{H}|]}\mathbb{E}_{\by|\bx}\left[\1\left[h^{(m)}(\bx)\neq\by\right]\cdot\1\left[r_{\mset}(\bx)=m\right]\right]\bigg]
\nonumber
\\
& = \mathbb{E}_{\bx}\left[\sum_{m\in[|\mathscr{H}|]}\mathbb{P}_{\by|\bx}\left[h^{(m)}(\bx)\neq\by\right]\cdot\1\left[r_{\mset}(\bx)=m\right]\right]
\nonumber
\\
& = \mathbb{E}_{\bx}\left[\sum_{m\in[|\mathscr{H}|]}\sum_{k \in [K]}\pi_k \cdot \mathbb{P}_{\by|\bx, z=k}\left[h^{(m)}(\bx)\neq\by\right]\cdot\1\left[r_{\mset}(\bx)=m\right]\right].
\label{eq:R01-expand}
\end{align}

Recall that:
\begin{align*}
\Delta_{k}(\bx, h^{(m)}) &= 
\left|\mathbb{P}_{\by\,|\,\bx, z=k} \left[h^{(m)}(\bx)\neq\by \right] \,-\, \LLMEmbedScalar_{k}(h^{(m)})\right|
\\
&=\left|\mathbb{P}_{\by\,|\,\bx, z=k} \left[h^{(m)}(\bx)\neq\by \right] \,-\, \mathbb{P}_{\bx', \by'\,|\,z=k}\left[h^{(m)}(\bx')\neq\by'\right] \right|.
\end{align*}

We may next bound \eqref{eq:R01-expand} in terms of $\Delta_{m}(\bx, h^{(m)})$:
\begin{align*}
\lefteqn{R_{01}(r,\mathscr{H})} \\
&\leq
\mathbb{E}_{\bx}\left[\sum_{m\in[|\mathscr{H}|]}\sum_{k \in [K]}\pi_k \cdot \Big( 
\mathbb{P}_{\bx',\by'\,|\,z=k}\left[
    h^{(m)}(\bx')\neq\by'
\right]
+ \Delta_{k}(\bx, h^{(m)})
\Big)
\cdot\1\left[r_{\mset}(\bx)=m\right]\right]
\\
&=
\mathbb{E}_{\bx}\left[\sum_{k \in [K]}\pi_k \cdot\sum_{m\in[|\mathscr{H}|]} \mathbb{P}_{\bx', \by'|z=k}\left[h^{(m)}(\bx')\neq\by'\right]\cdot\1\left[r_{\mset}(\bx)=m\right]\right] \\
&\hspace{5cm}
\,+\,
\mathbb{E}_{\bx}\left[\sum_{k \in [K]}\pi_k \cdot \sum_{m\in[|\mathscr{H}|]}\Delta_{k}(\bx, h^{(m)}) \cdot\1\left[r_{\mset}(\bx)=m\right]\right]
\\
& \stackrel{(a)}{\leq}
\mathbb{E}_{\bx}\left[\sum_{k \in [K]}\pi_k \cdot \sum_{m\in[|\mathscr{H}|]} \mathbb{P}_{\bx', \by'|z=k}\left[h^{(m)}(\bx')\neq\by'\right]\cdot\1\left[r_{\mset}(\bx)=m\right]\right] 
\,+\,
\mathbb{E}_{\bx}\left[\sum_{k \in [K]}\pi_k \cdot \max_{m\in[|\mathscr{H}|]}\,\Delta_{k}(\bx, h^{(m)})\right] \\
& \stackrel{(b)}{\leq}
\mathbb{E}_{\bx}\left[\sum_{k \in [K]}\pi_k \cdot \sum_{m\in[|\mathscr{H}|]} \mathbb{P}_{\bx', \by'|z=k}\left[h^{(m)}(\bx')\neq\by'\right]\cdot\1\left[r_{\mset}(\bx)=m\right]\right] 
\,+\,
\mathbb{E}_{\bx}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right]\\
&=
\mathbb{E}_{(\bx, z)}\left[ \sum_{m\in[|\mathscr{H}|]}\LLMEmbedScalar_{z}(h^{(m)})\cdot\1\left[r_{\mset}(\bx)=m\right]\right] 
\,+\,
\mathbb{E}_{\bx}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right]\\
&= \tilde{R}_{01}(r, \mathscr{H}) \,+\,
\mathbb{E}_{\bx}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right].
\end{align*}
where $(a)$ uses the fact that $\sum_m \1\left[r_{\mset}(\bx)=m\right] = 1$ and $(b)$ follows from the fact that $\sum_k \pi_k = 1$.

One can similarly show that:
\[
R_{01}(r,\mathscr{H}) \,\geq\, \tilde{R}_{01}(r,\mathscr{H}) \,-\,
\mathbb{E}_{\bx}\left[\max_{m\in[|\mathscr{H}|], k\in[K]}\,\Delta_{k}(\bx, h^{(m)})\right],
\]
which completes the proof.
\end{proof}







