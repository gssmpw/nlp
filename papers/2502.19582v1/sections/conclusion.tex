\section{Conclusion}\label{sec:conc}
In this work, we set out to unravel how current African language policies, by dictating data availability and accessibility, directly influence model performance across the continentâ€™s diverse linguistic spectrum. To achieve this, we conducted a comprehensive empirical evaluation of leading large language models (LLMs) on \ourbenchmark, an extensive benchmark assembled primarily from existing datasets. \ourbenchmark~consists of $30$ publicly accessible datasets, covering $16$ classification, generation, MCCR, and token level tasks. We also introduced the first publicly available leaderboard for African languages. Our evaluations revealed critical gaps in task performance and language coverage, underscoring pronounced disparities rooted in policy-induced data inequities. Moreover, our findings highlight the importance of integrating theoretical insights on language policies with empirical evaluations to inform forward-thinking policy reforms and promote inclusive data practices. We believe that \ourbenchmark~will serve as essential benchmarking standards, catalyzing future research and development efforts aimed at bridging the digital divide and fostering linguistic diversity in artificial intelligence for African communities.




