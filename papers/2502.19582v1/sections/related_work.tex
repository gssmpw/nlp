\section{Related Work}\label{sec:litreview}
The development of NLP for African languages is hindered by data scarcity, policy-induced disparities, and suboptimal model performance. Recent initiatives have sought to mitigate these challenges by curating diverse datasets, rigorously evaluating multilingual models, and advocating for more inclusive language policies. For example, several initiatives have contributed to expanding datasets for African \citet{yimametalcoling2020, muhammad-etal-2022-naijasenti, aliyu2022herdphobia, muhammadSemEval2023, muhammad-etal-2023-afrisenti, ilevbare2024ekohate, elmadany-etal-2024-toucan}. Some notable ones like \citet{adelani-etal-2021-masakhaner, adelani-etal-2022-masakhaner} introduced MasakhaNER, a named entity recognition dataset for 10 African languages, highlighting the limitations in existing resources. Similarly, IROKOBench \cite{adelani2024irokobench} evaluated large language models (LLMs) on African languages, revealing stark performance gaps between high- and low-resource languages. \cite{adebara2022afrolid} developed AfroLID, a neural language identification model covering 517 African languages, showing that most languages lack NLP datasets beyond language identification.
\input{Tables/sahara_data}

Language policies play a crucial role in determining resource availability.~\citet{adebara-abdul-mageed-2022-towards, hershcovich-etal-2022-challenges} emphasized the sociolinguistic factors affecting NLP development, noting that languages with official status receive more institutional support, fostering greater digital representation.~\citet{bird-2020-decolonising} further critiqued the Western-centric approach in speech and language technologies, calling for AI models that align with Indigenous linguistic contexts. Model performance in African languages remains uneven, largely due to the dominance of high-resource languages in multilingual training. \cite{conneau2018xnli} introduced XLM-R, demonstrating the limitations of cross-lingual transfer learning for underrepresented languages. Meta AIâ€™s No Language Left Behind (NLLB) project  \cite{nllb2022} attempted to improve multilingual models for low-resource languages, but performance remained inconsistent, particularly in generation tasks. Despite these advances, African NLP still requires increased investment in dataset curation, inclusive policy frameworks, and model development tailored to the linguistic diversity of the continent. We now introduce~\ourbenchmark.


