\section{Data-Performance-Policy Analysis}~\label{sec:analysis}
 \noindent To understand the relationship between policy, data availability, and performance, we carry out a number of analyses inspired by~\ourbenchmark~ data and empirical results on it. 
\input{figures/analysis}
\paragraph{Data Availability and Diversity.}
As shown in Figure \ref{fig:cluster_lang}, among the $517$ languages in our benchmark, only $45$ have more than one dataset, while the majority are represented solely by language identification data. Amharic leads with $11$ unique datasets across all clusters, followed by Yor\`{u}b\'{a} with ten and Hausa with nine. A total of six languages have only one dataset beyond language identification, while the remaining $36$ have between two and eight datasets each. \textit{It is important to note that the disparity in dataset availability is closely linked to language policy.} The $45$ languages with multiple datasets generally hold official status or are spoken by a significant majority in their respective countries. Government policies that designate certain languages as official often yield greater institutional support, driving increased documentation, education, and media presence. For example, Amharic’s official status in Ethiopia contributes to its relatively rich dataset availability, while Yor\`{u}b\'{a} and Hausa, both widely used in Nigeria for regional governance and media, similarly benefit from enhanced linguistic resources.

Conversely, languages lacking official recognition or widespread institutional support receive minimal investment in digital and linguistic infrastructure. The dominance of a few languages within policy frameworks perpetuates data scarcity for many Indigenous African languages, thereby limiting their inclusion in NLP advancements and reinforcing digital inequalities. Addressing this imbalance requires policy shifts that prioritize multilingual data collection, increased funding for low-resource language research, and the integration of diverse languages into education, media, and digital communication.

\paragraph{Data Quality.}
Most of the downstream task datasets  are limited to simple tasks that require merely an atomic class label at the word, sentence, or document level (e.g., sentiment analysis, named entity recognition, topic classification). Furthermore, many of these datasets are translations of existing resources or content from high-resource languages like English and French, and they do not always reflect authentic language usage within the respective communities. For instance, AfriXLNI, AfriMMLU, and AfriMGSM \cite{adelani2024irokobench} were created by translating XLNI, MMLU, and MGSM into African languages. In these datasets, the frequent use of borrowed words and phonologized variants of foreign terms \cite{adebara-abdul-mageed-2022-towards} highlights the challenges posed by cross-lingual concept gaps. When translations are applied to classification tasks, misalignments in the attached labels can occur because labels are not always equivalent across languages. Some labels may not exist in the target language or may have restricted usage. For example, while English possesses a productive set of adjectives, languages like Yor\`{u}b\'{a} have a more limited adjective inventory that is context-dependent, meaning that an English adjective may be translated as a noun or verb in Yor\`{u}b\'{a} rather than as an adjective. 

\paragraph{Model Performance}
Here, we analyze performance of our best model, Claude 3.5-Sonnet. As shown in Figure~\ref{fig:performance_analysis_scatterplot}, the model faces varying degrees of difficulty when handling African languages. Sub-figure~\ref{fig:preform_sub1} demonstrates that for generation tasks, even the strongest models struggle to produce high-quality text in African languages, with most scores falling below 15 BLEU/ROUGE. In contrast, sub-figure~\ref{fig:preform_sub2} shows that while many languages achieve relatively strong classification performance (over 80\% accuracy), a substantial portion still falls into the 60\%–80\% range, underscoring a clear performance gap. Finally, sub-figures~\ref{fig:preform_sub3} and~\ref{fig:preform_sub4} reveal that translation into English is notably easier than translation out of English or French into African languages, where BLEU scores drop from an average of around $19$ down to nine or $11$, respectively. This finding suggests that even the best model struggles more with producing fluent, faithful output in African languages than with understanding them—underscoring the urgent need for better data and model development tailored to these languages. Refer to Appendix~\ref{appdx_sec:analysis} for a similar analysis leveraging our empirical language identification evaluation.

Apart from our empirical analyses, the limited availability and diversity of datasets for African languages in NLP benchmarks has significant implications for developing robust language technologies. \textit{Limited Model Performance and Generalization} – Models trained on a single dataset per language risk overfitting to specific domains or styles, reducing their ability to generalize across different contexts, which is especially problematic for African languages with high dialectal variation. \textit{Bias Toward Well-Resourced Languages} – Languages with more datasets naturally perform better in multilingual models, reinforcing disparities and leading to poor performance for underrepresented African languages. \textit{Restricted Downstream Applications} – Many NLP applications, such as machine translation and question answering, require diverse datasets; if a language is represented only by language identification data, it cannot support the development of more complex language technologies. \textit{Challenges in Transfer Learning} – Transfer learning techniques, like multilingual fine-tuning, rely on cross-lingual similarities and sufficient data; when African languages suffer from sparse training data, they are unable to effectively leverage knowledge transfer from resource-rich languages.

%\hl{It is important to mention that despite the} 

\section{Recommendations}\label{sec:rec} 
Policy determines how languages are used across various domains, influencing their visibility and viability in digital spaces. Currently, the limited online presence of many Indigenous African languages restricts the availability of training data, which is crucial for building effective NLP models. We provide the following recommendations:

\noindent\textbf{Policy matters.} Governments and policymakers should enact national language policies that mandate the inclusion of Indigenous and underrepresented languages across digital infrastructure, education, and media. Countries with multilingual populations should adopt policies that support the systematic documentation, transcription, and digitization of African languages to enhance their presence in AI applications. Such efforts will enrich the diversity of available data, paving the way for the development of more robust and inclusive models.

\noindent\textbf{Expand data collection and annotation.} Given that only $45$ of the $517$ languages in the benchmark have more than one dataset based on our collection, it is essential to increase the availability of high-quality data for underrepresented African languages across a broad range of downstream tasks. Moreover, prioritizing community-driven data annotation efforts will be crucial in ensuring both linguistic diversity and accuracy, especially for languages that lack institutional support.

\noindent\textbf{Develop contextually relevant and culturally accurate datasets.} Many existing African language datasets are derived from translations of high-resource languages, which can introduce misalignments and fail to capture authentic linguistic expressions. To improve NLP performance, datasets should be created using native speakers and sourced from real-world interactions, literature, and media in African languages. This will help preserve linguistic authenticity and avoid the overreliance on loanwords or structures that do not naturally occur in the language. Specialized corpora should also be developed to capture dialectal variations and culturally specific language use.
