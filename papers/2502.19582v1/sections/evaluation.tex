\section{Experiments and Results}\label{sec:evaluation}
\subsection{Experimental Setup}

To provide an overview of the state of African NLP, we evaluated the performance of popular instruction-based LLMs in a few-shot setting using the \ourbenchmark~benchmark. The models tested include Phi-3.5~\cite{abdin2024phi3technicalreporthighly}, Gemma-2~\cite{gemmateam2024gemma2improvingopen}, Llama-3.1~\cite{dubey2024llama}, Llama-3.2, Aya-23~\cite{aryabumi2024aya23openweight}, and command-r-plus from open LLMs, as well as GPT-4o and Claude-3.5-Sonnet from closed LLMs. We also considered variations of these models based on their size, categorizing the LM as small, \texttt{SLM} for those with fewer than 8B parameters and large, \texttt{LLM} for those with more than 8B parameters to analyze the effect of model size on performance. For clearer visualization, we aggregated the test datasets for each task cluster identified in Appendix~\ref{appendix: task_cluster_detail}. We randomly chose a sample of $600$ data points for each task for a few-shot evaluation. The evaluation used well-crafted prompts that are both concise and straightforward, with examples of these prompts provided in Appendix \ref{fig:few-shot}. For tasks available in the lm-eval tool \cite{eval-harness}, we use the tool for evaluating the open-source LMs.


\subsection{Results and Discussion}

We evaluate 13 LMs across four task clusters, maintaining a consistent assessment approach while varying evaluation metrics. Depending on the task, we employ \textit{F\textsubscript{1}}, \textit{Accuracy}, \textit{Bleu}, and \textit{RougeL} scores.
% The detailed results for each task, along with the corresponding evaluation metrics, are presented in Table \ref{tab:results-subset}.

As shown in Table~\ref{tab:results-subset}, in the \textit{classification tasks}, the closed models Claude-3.5-Sonnet and GPT-4o consistently led in performance, followed by larger LLMs such as Llama3.1-70B and CommandR-Plus-104B. In the \textit{language identification task}, most models struggled to grasp the task effectively, with only the larger LLMs demonstrating some understanding, suggesting that model scale is crucial for achieving higher accuracy across various multilingual tasks. In the \textit{generation task cluster}, the larger models performed as expected, with Claude-3.5 and CommandR-Plus leading. There was a noticeable performance increase across all models in the paraphrase task, while the summarization task exhibited a more balanced performance among all models. Considering their size, Llama3.2-3B and Llama3.1-8B performed well within this task cluster. In the \textit{MCCR tasks}, Llama3.1-70B performed competitively against the closed Claude-3.5 model. In the MGSM task, which involves solving mathematical word problems, most open-source models performed poorly, while Claude-3.5 excelled, achieving an exact match score of 22.34\%. In token-level tasks, GPT-4o and CommandR+ consistently delivered strong performances, whereas the Aya-23 model family struggled to grasp the tasks. %Interestingly, the Gemma-2 family models performed well, even leading in the named entity recognition task.









The evaluation indicates a landscape of competitive and specialized performance among multilingual and African LLMs, with certain models standing out in some of the tasks. In few-shot evaluation, the closed models followed by Llama3.1-70B and Command-R-Plus, performed significantly better than the other models across multiple tasks. Given their size, the Llama models, especially Llama3.1-8B, perform better than the other models. %In fine-tuned evaluation, performance significantly varies across languages, notably lower for Horn of Africa languages due to limited training data. In contrast, well-represented languages like Nigerian Pidgin and Swahili show better model performance. 
Additionally, translating \textit{into} English proved easier than translating \textit{from} English, reflecting the impact of language representation and data availability on model efficacy (especially at the \textit{target} side).


\input{figures/lang_cluster}

\paragraph{Which LM should I use for reasoning tasks?} In a limited resource setting, for mathematical QA tasks that fall in the \texttt{MCCR} cluster, Phi3.5 \cite{abdin2024phi3technicalreporthighly} ranks top amongst 4 other SLMs for 16 African Languages. In the LLMs category, Gemma 27B has better performance on a higher number of African languages. For reading comprehension tasks, Llama 3.2-3B \cite{dubey2024llama} is most suitable in constrained compute resource settings while in scenarios without limits in compute resources, Llama 3.1-70B gives better performance.

\paragraph{Which African language do LMs perform best on?} 
Most of lama3 models perform best on Swahili. It ranks first in two models and second in one of three models tested. In the Gemma2 models,  Aya-x and CommandR+, Swahili and Afrikaans are generally well known. We believe this is due to the availability of data for both languages, with both languages being among the highest resourced languages in Africa.


