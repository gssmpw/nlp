\section{\includegraphics[scale=0.04]{Images/sahara_logo.png} Sahara Benchmark}\label{sec:benchmark}

Our objective is to create a comprehensive benchmark for African NLP that enables (i) analysis of existing language resources and (ii) the assessment of language models and tracking of the progress of African NLP. To achieve this goal, we develop \ourbenchmark, adhering to several key design principles that we will now elucidate. \ourbenchmark~establishes a comprehensive and adaptable benchmark for African NLP using publicly available datasets. Figure~\ref{fig:map_data} shows coverage of~\ourbenchmark.%Its primary goal is to enable accurate and 

\paragraph{Wide and Diverse Coverage.}
\ourbenchmark~is the most comprehensive benchmark for African NLP, covering a vast range of languages. We achieve this by collecting high-quality, publicly available datasets from as many African languages as possible, ensuring easy accessibility for researchers evaluating their models. As a result, \ourbenchmark~supports $517$ languages across various tasks, making it the most extensive and representative benchmark for African NLP. It encompasses languages from $50$ out of $54$ African countries and includes data written in five different scripts: \textit{Arabic, Coptic, Ethiopic, Latin}, and \textit{Vai}, spanning five language families across the continent. 



\paragraph{Tasks and Task Clusters.}  %While incorporating a wide range of languages in a multilingual benchmark is essential, the diversity of NLP tasks is equally critical—especially given the rapid advancements in LLMs in understanding multiple languages and complex tasks.
\ourbenchmark~is designed to support a broad spectrum of NLP tasks, which we systematically organize into coherent task clusters. The inclusion of well-defined, challenging clusters enables a more meaningful assessment of a model’s capabilities and allows researchers to evaluate performance on specific clusters. As shown in Table\ref{tab:benchmark_tasks}, \ourbenchmark~comprises four clusters: \textit{text classification}, \textit{text generation}, \textit{multiple-choice, comprehensive and reasoning (MCCR)}, and \textit{token-level classification}. For further details on the datasets used for each task, please refer to Appendix \ref{appendix: task_cluster_detail}.

\input{Tables/result_new_subset}
\paragraph{Modular Public Leaderboard.}  
We implement a \textit{user-friendly leaderboard} for model evaluation on \ourbenchmark, hosted on Hugging Face (HF) Spaces. Evaluated models must be publicly available in HF repositories to ensure seamless integration. When submitting a model, users are required to provide detailed \textit{metadata}—including whether the model is pretrained, further pretrained, or instructed—and then submit an evaluation request. The submission enters a processing queue, and once evaluation is complete, the results are automatically displayed on the leaderboard. Our team will actively maintain the leaderboard by continuously integrating new publicly available tasks and datasets to keep it \textit{up-to-date}. \ourbenchmark\ thus serves as a valuable tool for assessing model performance in African NLP, accelerating the development of high-performance models, and contributing to the field’s growth. In addition to evaluating models on the entire benchmark, users have the flexibility to assess performance on specific task clusters, encouraging the development of specialized models tailored to address the unique challenges of African NLP.



