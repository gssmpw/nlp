\section{Introduction}

Sequences of events are a widely used data type representing an irregular multidimensional time series~\cite{yan2019recent}.
This complex domain can describe many application areas, such as bank transaction data or store purchases. 
For a single client, there are typically hundreds or even thousands of purchase events in a corresponding sequence.
Among different applied fields, modeling of financial transactions is one of the most important among sequences of the event (transaction) data~\cite{babaev2019rnn}.
There, on one hand, transactions reflect complex purchase behavior, on the other, vast data on past event sequences for clients are available.

\begin{figure}[!t]
     \centering
     \includegraphics[width=0.9\columnwidth]{figures/mean_rank_colors.pdf}
     \caption{Mean rank ($\downarrow$) with respect to ROC-AUC values for different pooling aggregation methods over datasets Churn, Default, HSBC and global and event type validation. The proposed Kernel attention approach has the lowest mean rank.}
     \label{fig:mean_rank}
\end{figure}

Given these circumstances, a neural network seems a reasonable choice methodologically ~\cite{shchur2021neural} with confirmed strong empirical performance~\cite{babaev2019rnn}.
There, a trained neural network encoder provides a single representation vector for a sequence of events at a selected time moment, suitable for diverse downstream tasks~\cite{bin2022review}. 
Moreover, as most event sequences are unlabeled, self-supervised representation learning for this modality enables more powerful and universal representations~\cite{babaev2022coles},
reducing costs for creating and maintaining models~\cite{bazarova2024universal}. 
With enhanced quality~\cite{babaev2022coles} and robustness, these models cover multiple applied tasks: loan defaults prediction~\cite{zaytsev2023designing}, churn prediction, fraud detection - and many others.



While modeling separate sequences alone provides significant benefits, understanding the interdependencies between sequential events in complex systems further enhances overall quality and performance~\cite{farajtabar2017coevolve, farajtabar2014shaping}. 
Moreover, if a last event happened for a specific time, long time ago --- we can face outdated embeddings in a standard recurrent or attention-based architecture.
Temporal point processes are often used to describe for interaction prediction~\cite{de2016social, ijcai2021p623,passino2023mutually}, but for transactional data, more general models and loss functions are usually more beneficial and robust~\cite{zhuzhel2023continuous,bazarova2024universal}. 
These models, however, lack accounting for interactions.
So, including the aggregation of other sequences can incorporate external information missed in a single sequences - and thus increase the quality of model prediction.  

To capture interactions between sequences of financial transactions, we introduce an efficient sequence embedding aggregation method. This approach enhances both training and inference performance while significantly improving model quality by integrating additional information into the embeddings. Our key novelty lies in leveraging straightforward yet powerful attention-based techniques for representation learning in sets of co-occurring event sequences.
Thus, our main contributions are:
\begin{itemize}
    \item 
    \emph{A pioneering use of aggregated sequence embeddings as an external context vector.} Our study explores various methods to construct these aggregations, enabling the enrichment of sequence representations without requiring additional data and significant additional computational resources.
    \item \emph{A novel Kernel attention method}, that stand out among considered methods. It leverages learnable kernel dot products to calculate similarities between sequence embeddings. For it, as well as other methods, we can fine-tune a backbone encoder model, improving the model quality even further.
    \item \emph{An empirical evaluation for external information aggregation.} We conducted extensive experiments demonstrating that the addition of the external context vector significantly enhances the performance of downstream tasks applied to sequence representations if we either fine-tune an encoder or use the encoder as it is. The proposed \textit{Kernel attention} outperforms other considered methods, as evident from Figure~\ref{fig:mean_rank}, while other aggregation techniques also lead to quality boost if compared to a standard approach that processes event sequences independently.
\end{itemize}

The reproduction code used to produce the results is available on our GitHub page: 

\href{https://anonymous.4open.science/r/external_information-3681/}{https://anonymous.4open.science/r/external\_information-3681/}.

% Different approaches are used for embedding construction, but none of them take into account the external context. 
% For example, in the domain of transactional data, models don't consider representations of other bank clients or macroeconomic parameters~\cite{begicheva2021bank}.
% Accounting for this information is a new task that will potentially improve the quality of the models~\cite{ala2022deep}. Since the external context influences the behaviour of all client sequences, our main assumption is that the external context can be identified through some aggregation of all representation vectors of sequences at a given time point.
% The addition of this aggregation into representation models can improve the quality of the applied tasks.

% So, the main purpose of this work is to improve models for representing event sequences in general and sequences of bank client transactions in particular by adding external information. 
% Although this work examines problems in the banking sector, the proposed approach can be generalized to other areas of data. 
% Our main contributions are:
% \begin{itemize}
% \item We develop and test different methods of context aggregation and analyze their performance on applied tasks. 
% \item We show that adding external information improves event sequence representation in different applied tasks.
% \item We present the attention TODO (the best method)
% \end{itemize}

% The rest of the paper is organized as follows: Section~\ref{lit_rewiev} presents the literature review of the examined models and approaches, in Section~\ref{sec:methods}, we discuss in detail the methods used in this work, and in Section~\ref{sec:results} we show the obtained results.


