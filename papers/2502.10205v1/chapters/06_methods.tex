\section{Methodology}\label{sec:methods}

\begin{figure*}[!t]
     \centering
     \includegraphics[width=0.85\textwidth]{figures/Picture_pipeline_v2.pdf}
     \caption{General pipeline for external context generation to integrate it with a vanilla internal context followed by the considered validation procedures.}
     \label{fig:pipeline}
\end{figure*}

The general experiment pipeline is depicted in Figure~\ref{fig:pipeline} and structured as follows:
\begin{enumerate}
    \item Sequences of the transactions are preprocessed and passed through the pretrained SSL encoder to get embeddings.
    \item Various external context aggregation methods combine embeddings from different sequences and produce an external context vector at the current time moment. After that, the external context vector is concatenated with the embedding of the sequence under consideration. 
    \item The concatenated vector is the current representation of the sequence. It is used in two following downstream tasks. 
\end{enumerate}
The corresponding subsections discuss in detail all the presented steps of the general pipeline. 
We begin with describing transactional data and its preprocessing in general. 
Next, we describe the given models from the baseline approach. 
Then, we present the approaches used to account for external representations. 
Lastly, we discuss the methods of validating obtained representations.



\subsection{Transactional data description and its preprocessing}

In this work, we consider several samples of transactional data, all of which have common and different features. The dataset of transactions $D$ is a set of $n$ sequences with length $T_i$ for every sequence with available features $\vecx$ and timestamps $t$ for every timestamp.
Formally, each sequences $s_i = \{(\vecx_{ij}, t_{ij})\}_{j = 1}^{T_i}$ has a single label for the whole sequence $y_i$.
So, our sample $D = \{(s_i, y_i)\}_{i = 1}^n$.
For a set of event sequences with available labels the notation would be the~same.

To standardize the pipeline, we focus on two main features, accompanied by timestamps, that are presented for all events in all datasets: the transaction's merchant category code (MCC code --- the type of transaction, a categorical variable with $\sim 1000$ possible codes), its volume of money (Amount). 
The time marks of each transaction are also used later for representation construction, not being input to an encoder as proposed in ~\cite{babaev2022coles}. 
More preprocessing details can be found in Appendix~\ref{sec:details}, while Appendix~\ref{sec:app_datasets} provides more details on the used datasets.

\subsection{Models for building representations}

\paragraph{Input encoder.} A special transaction encoder processes a transaction sequence before being put into the main part of a parametric model.
The basic procedure involves obtaining representations of MCC codes of some fixed dimension $d_{\mathrm{mcc}}$, where each transaction type has its vector from a dictionary~\cite{pennington2014glove}. 
Simple preprocessing of numerical characteristics, such as normalization of the amount, is also performed.

Thus, after this preprocessing, we have a sequence of dimension $(T, d_{\mathrm{mcc}} + 1)$, where $1$ is added because of the Amount feature, and $T$ is the length of the transactions sequence.

\paragraph{Sequence encoder.} The resulting sequence of the encoded transactions is fed to the main model of sequential data processing (SeqEncoder), which determines the structure of the considered representations. Any neural network that operates in Sequence-to-Sequence mode can be used as a SeqEncoder. In other words, when the model receives a certain sequence of observations as input, it produces a sequence of representations of the same length of dimension $(T, m)$, where $m$ is the embedding size.
We use recurrent neural networks with a Long-Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) architecture, depending on the dataset. It was shown in~\cite{babaev2022coles}, RNN-based models outperform transformers for transaction data. 

\paragraph{Self-supervised representation learning for event sequences data.}
% \label{chap:methods}
Our basic representation learning method is the contrastive method CoLES. We also use it as a self-supervised baseline approach.
%and the autoregressive method. 

Contrastive Learning for Event Sequences with Self-Supervision, or shortly CoLES, is a contrastive method for representation learning, proposed in the work~\cite{babaev2022coles}.
CoLES shows high quality on several tasks of event sequence processing, including transactional data. 

In the case of CoLES, two subsets of transactions obtained from one bank client are used as positive pairs, and two subsets of transactions obtained from different clients are used as negative pairs.
A parametric encoder model is used to obtain representations of these subsequences. This encoder is trained using a specific contractive loss function. 
The concept allows the use of different encoders to obtain transaction representations. 



% \textbf{Autoregressive models} (AR)  predict the next item in a sequence. A recurrent neural network similar to the CoLES model is implemented in our work. The model's last hidden state represents the sequence because it contains complete knowledge about the whole sequence.
% The model is trained to predict the next transaction, encompassing the MCC code and transaction amount.

% We add two linear heads to the recurrent neural network output for amounts and MCC prediction to train the AR model. Since transactions contain both categorical (MCC codes) and continuous (amounts) information, the reconstruction loss is also divided into two parts. 

% We use cross-entropy for the categorical features and mean squared error for the continuous features. 
% The final loss function is a weighted sum of the two intermediate ones, with the weights set to five and one for the amount and MCC part, respectively.  

% We found that the preprocessing of transaction amounts defined below is essential for the model to train successfully:
% \begin{equation} \label{eq:ae_amount_transform}
%     f(a) = \mathrm{sign}(a) \ln{(1 + a)}.
% \end{equation}
% This transformation allows stabilization of the loss functions for amounts and MCCs without using dramatically different weights. It is also closer to human perception. People tend to focus on the order of magnitude, ignoring the exact value.

% Optimal complexity for the training objective is very important for self-supervised approaches ~\cite{he2022masked,kenton2019bert}. 
% In our case, we found out that prediction of rare MCC code is too complicated and unnecessary task.
% So, we reduce the number of unique MCC codes to 100 for all datasets, clipping all less frequent MCC codes.

\subsection{Usage of external information based on local representations}
\label{sec:global_context_methods}

An additional external context representation vector (or global embedding) is built from the local representation vectors from all or some selected users by aggregating them. 
The global embedding model works on top of the original encoder.

The procedure for constructing a context vector at a specific point in time is presented in Figure~\ref{fig:global_pooling} and is described as follows:
\begin{itemize}
     \item [1.] Collect a sample of all possible local customer representations for all users and each unique moment of the transaction.
     \item [2.] Select local representations that precede the current time point but are close to it in terms of temporal proximity.
     \item [3.] Apply aggregation to the resulting set of vectors. The resulting vector is the vector of external context.
\end{itemize}

\begin{figure}[t!]
     \centering
     \includegraphics[width=0.9\columnwidth]{figures/scheme_v5.pdf}     
     \caption{External context aggregation that outputs the external representation. Here we don't show encoding of external sequences}
     \label{fig:global_pooling}
\end{figure}

As mentioned earlier, accounting of external context vector  can improve the quality of models in applied problems. To check this, we concatenate the resulting context embedding vector with the user’s local embedding and validate the extended representation.

\subsubsection{Straightforward aggregation}

Averaging and maximization pooling provide a natural way to aggregate external information. 
In the first case, the vector of external representation is obtained by componentwise averaging the local representation vectors for all users in the stored dataset.
In the second case, the maximum value for each component is taken. 
They are close to Mean and Max Pooling operations in convolutional neural networks~\cite{boureau2010theoretical} and language models~\cite{xing2024comparative}. Like global pooling in convolutional neural networks, such aggregation methods are designed to generalize the environment before the following transformations, while taking a new perspective: instead of aggregating events for a specific sequence, we aggregate representations for a set of sequences.

\subsubsection{Methods based on the attention mechanism}

The aggregation methods described in the previous paragraph ignore the interaction between the local representations of all users from the dataset and the considered one and produce similar external context for all sequences.
However, some users in the dataset may behave more like the user than others. 
While we don't have a direct links like for spatial graphs~\cite{huang2024temporal},
we can try to extract these connections from embeddings itself.
Similar clients determine the user’s closest environment and, correspondingly, help describe her or his behaviour better.

Thus, we need an aggregation method that take into account the similarity of users. 
The attention mechanism, originally used to describe the similarity of the word embedding vectors~\cite{vaswani2017attention}, is natural for such a problem.

In this work, different variants of the attention mechanism are used.

\textbf{Simple attention without learnable attention matrix}

In the version without a training matrix, the external context vector for a given point in time has the form:
\begin{equation}
     \vecg_t = X \mathrm{softmax} (X^T \vech_t),
\end{equation}
where $\vech_t \in \mathbb{R}^m$ is a vector of local (internal) representation for the considered user, $\vecg_t \in \mathbb{R}^{m}$ is a vector of external (global) representation for the considered user, and $X \in \mathbb{R}^{m \times n}$ is a matrix, which rows are embeddings of size $m$ for all $n$ users from dataset at a given time point.
In this case, the user similarity metric is normalized by the softmax of the dot product.

\textbf{Attention with learnable attention matrix}

For the method with a trained matrix, the formula is similar:
\begin{equation}
     \vecg_t = X \mathrm{softmax} (X^T A \vech_t),
\end{equation}
here $A \in \mathbb{R}^{m \times m}$ -- is the matrix to be trained.
There, before calculating the scalar product, vectors of representations from the dataset are additionally passed through a trained linear layer. 

\textbf{Attention with symmetric attention matrix}

In this approach, we model the attention matrix as a product of one matrix with  its transposed version $A = S^T S$.
That makes the resulting formula similar to the calculation of kernel the <<dot product>> between the current representation vector and vectors from the train set with the linear kernel:
\begin{equation}
\begin{split}
     % \mathbf{b}_t = X \text{softmax} (X^T S^TS \mathbf{h}_t) = X \text{softmax} ((SX) S\mathbf{h}_t) =  \\ X \text{softmax} (<SX, S\mathbf{h}_t>),
      \mathbf{g}_t = X \mathrm{softmax} (X^T S^T S \mathbf{h}_t) = X \mathrm{softmax} (\langle S X, S \mathbf{h}_t \rangle),
\end{split}
\end{equation}
where $\langle \cdot, \cdot \rangle$ is the notation for the <<dot product>> between all the vectors from $SX$ and $S\mathbf{h}_t$ vector combined in one vector.

\textbf{Kernel attention}

We generalize the approach with a symmetric attention matrix and propose a kernel attention method. The main idea here is that we can use the general kernel dot product to calculate attention scores:
\begin{equation}
     \vecg_t = X \mathrm{softmax} (\langle \phi(X), \phi(\vech_t) \rangle),
\end{equation}
here $\phi(\cdot)$ is a learnable function, that is applied to a row or independently to all rows, if the input is the matrix. In our case, it is parameterized by a two-layer fully-connected neural network.

\subsubsection{Methods inspired by Hawkes process}
~

Previous proposed methods do not take into account the time of the embeddings explicitly. This factor can affect the result as it is natural that the events that happened a long time ago should have less influence on the current moment compared to recent events, as various temporal process models state. To consider this, we turn to the analogues to Multivariate Hawkes processes ~\cite{hawkes1971spectra,hawkes2018hawkes}. 
Formally, Alan Hawkes proposed a generalization of Poisson point process with the intensity that is conditional on past events.

We propose three options based on this intuition, \emph{Exponential Hawkes}, \emph{Learnable exponential Hawkes}, and \emph{Attention Hawkes}. 
They are described in more details in Appendix~\ref{sec:hawkes_insprired_methods}.


\subsection{Production-ready downstream problems} 
\label{sec:validation_methods}

\textbf{Global properties} of representations are validated using the following approach used in the paper~\cite{babaev2022coles}, in which the CoLES model was proposed.

All the datasets considered in this work contain the binary classification mark for each sequence. 
For example, this target mark can represent the clients who left the bank or did not repay the loan. This procedure consists of three steps, which are described below. 
For an initial sequence of transactions of length $T_{i}$ related to the $i$-th user, we obtain the representation $\mathbf{h}^{i}\in\mathbb{R}^{m}$, which characterizes the entire sequence of transactions as a whole. 
Given fixed representations $\mathbf{h}^{i}$, we predict the binary target label $y^{i}\in\{0, 1\}$ using gradient boosting. 
As a specific implementation, the LightGBM~\cite{ke2017lightgbm} model is used, which works fast enough for large data samples and allows obtaining results of sufficiently high quality. 
The specific hyperparameters of the gradient boosting model are fixed (corresponding to ~\cite{babaev2022coles}) and are the same for all base models under study. The quality of the solution of a binary classification problem is measured using a standard set of metrics.

This procedure allows us to evaluate how well the representations capture the client's "global" pattern across its history. 
We call this task \textit{Global target} prediction or \textit{Global validation} from here and below.

\textbf{Local properties} of sequences differ from global ones in that they change over time, even for one sequence. 
We use a sliding window procedure of size $w$ to obtain and evaluate local embeddings. To do this, for the $i$-th user at time $t_{j}\in[t_{w}, T_{i}]$ the subsequence of his transactions $\mathbf{S}_{j-w:j}^{i }$ is taken.
Next, this interval is passed through the encoder model under consideration to obtain a local representation $\mathbf{h}_{j}^{i}\in\mathbb{R}^{m}$. 
% An illustration of this approach can be found in Figure~\ref{fig:local_validation_coles}.

% \begin{figure}[!ht]
%      \centering
%      \includegraphics[width=\columnwidth]{figures/local_val.pdf}
%      \caption{Usage of the sliding window for local validation approaches}
%      \label{fig:local_validation_coles}
% \end{figure}

The longer the time interval the model uses, the greater the risk that the data it relies on will become outdated. Our artificial limitation allows us to reduce this effect for all models and also strengthen their local properties only due to the “relevance” of the data.

An obvious limitation of this approach is that it does not allow obtaining local representations at times $t\leq t_{w}^{i}$.
In addition, the window size $w$ is an additional hyperparameter that must be chosen, taking into account the fact that for small values of $w$, the resulting representations do not contain enough information to solve local problems and for large values of $w$ the representations lose local properties and become global in the limit $w\to T_{i}$.

As local validation problems, we explored the prediction of the next transaction's MCC code. We call this task \textit{Event type} prediction from here and below. This validation approach was inspired by the work~\cite{zhuzhel2023continuous}, in which it was proposed to predict the type of the next event based on the history of observations --- in our case, the MCC code of the next transaction. Formally, in this case, the multiclass classification problem is solved: given the local representation $\mathbf{h}_{j}^{i}$, we predict the MCC code of the transaction of the $i$-th client, completed at time $t_{j+1}$.

Note that there are many rare MCC types in datasets. From a business perspective, such categories are often less interesting and meaningful. Therefore, to simplify the task, in this procedure for testing local properties, it was decided to leave only transactions corresponding to the $100$ most popular codes.

\subsection{Efficient work in production} 
\label{sec:prod_efficient}

% In our work, we propose a solution for an applied problem in the Financial domain, like credit default prediction. Our approach increases the quality of a downstream task by enriching user representation vectors with an external context. We provide a description of the system for our solution and quantification of system performance in a special section of the paper. 

To integrate our method of external aggregation in the production system to conduct inference, 
we maintain a database of embedding vectors that allows on-the-fly aggregation.
Let us consider all the steps of the proposed procedure below.

The vector database consists of embedding vectors with actual users $D \in \mathbb{R}^{n_0 \times m}$: $n$ user vectors of size $m$, that also appear as a pre-computing embeddings database in the recommendation system domain~\cite{zanardi2011dynamic}. 
To reduce the computational time, we randomly select $n_0 \ll n$ that is significantly smaller than the whole set of users.
In our experiments, $n_0 = 100$ users for a dataset of $n = 5000$ users is enough for aggregation to perform well. 

An update of the database happens daily, to account for new events, such as transactions, as typical user performs no more than three transactions per day. 
An RNN-based model there can easily be updated without rerunning the model on the whole event sequence by using the current embedding as an input. 

The external context vector can be independent and same for all users (mean aggregation) or dependent from user embedding and different for all users (attention aggregation) of the specific user embeddings.  
In the first case, we easily aggregate vectors of selected users in a single vector $\mathbf{g}$ and use it throughout the day.
In the second case, we compute the necessary aggregation on request with a single attention layer.
In both cases, we have a linear in the $n_0$ procedure.

The concluding prediction takes a close time to the usual prediction, as we concatenate the external context embedding and a user-local embedding and process it with a linear or a gradient boosting layer.  

So, our method is easy to develop and fast to employ in real-life environments with little computational overhead.



%scenario is also not computationally expensive. If you need to get predictions for users from the external vector database, you don't need to rerun the model in offline mode. You can just take these users' embeddings from the vector database and concatenate them with external context embedding. It can even speed up computations for  offline model usage scenario with CPU computation for neural networks. If the user is not in the vector database, you need to run the encoder and just compute an actual external context vector or get it from the database with unified computational complexity and get a prediction with an NN head or another model without external aggregation.  


% The size of this database will include  plus one external context vector of the same size.

