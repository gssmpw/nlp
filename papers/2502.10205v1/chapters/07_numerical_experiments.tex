% \section{Results and Discussion}

% We start this chapter with methods for validating the resulting embeddings in terms of global and local properties and discussion of used datasets and metrics.
% Next, the chapter provides obtained results of different methods used to take into account external representations. The discussion of results and the proposition for further development of the project are also presented in this part.

% All experiments were carried out in the Python programming language using the models and hyperparameters described in the methods chapter. 

\section{Results}\label{sec:results}

In this section, we describe the results of experiments on obtaining an external context representation and using it to improve existing~models.

\subsection{Methods}

In the experiments, we consider three types of methods: the one without external context inclusion and unlearnable and learnable approaches to aggregate information from other event sequences.
A conventional CoLES encoder~\cite{babaev2022coles} provides strong performance in the considered problems, so we use it in the case without adding external context information (\emph{Without context}).
For aggregation approaches, all learnable parts of these methods come from this CoLES pipeline with fixed original encoders. 
We investigated the following types of aggregation of representations to obtain a context~vector:
\begin{itemize}
    \item straightforward aggregation methods: Averaging (\emph{Mean}), Maximization (\emph{Max}) poolings;
    \item methods inspired by Hawkes process: Exponential Hawkes (\emph{Exp Hawkes}), Exponential learnable Hawkes (\emph{Learnable exp Hawkes*});
    \item attention-based aggregation methods: Attention mechanism without learning (\emph{Attention}), with learning (\emph{Learnable attention*}), Attention with symmetric matrix (\emph{Symmetrical attention*}), Kernel attention (\emph{Kernel attention*});
    \item combined methods: Hawkes with attention (\emph{Attention Hawkes}).
\end{itemize}
All learnable methods are marked with the asterisk sign~"*".



\begin{figure*}[!th]
      \centering
          \includegraphics[width=0.9\textwidth]{figures/star_plot.pdf}
          \caption{\selectfont Quality of the models regarding their global and local properties on the \textit{Churn} dataset (left), \textit{Default} dataset (central), and \textit{HSBC} dataset (right). The $x$-axis corresponds to the global validation ROC-AUC, while the $y$-axis shows the next event type prediction ROC-AUC. Thus, the upper and righter the dot is, the better the model is. A dot in the plot is mean, and lines are std for experiments with 3 seeds. }\label{fig:coles}
\end{figure*}


\begin{figure}[!th]
     \centering
     \includegraphics[width=\columnwidth]{figures/multi_user.pdf}
     \caption{Dependencies between the number of event sequences in external context and forthe  global target (left) and event type (right) ROC-AUC for \textit{Churn} dataset. }
     \label{fig:iters}
\end{figure}

\subsection{Validation}

\paragraph{Procedure.} We consider two types of downstream problems: local and global, defined for a current timestamp for a client or for an event sequences as a whole correspondingly. More detailed problem definitions are in Subsection~\ref{sec:validation_methods}. For global validation, we follow the procedure described in~\cite{babaev2022coles}.
There, self-supervised learning is followed by adding a gradient boosting on top of learned embedding features. 
We use the same hyperparameters for the boosting model.
A more common for computer vision~\cite{grill2020bootstrap} linear probing instead of boosting probing performs worse, so the experiments use the latter one. 
For local validation, we use the procedure described in the corresponding section~\ref{sec:validation_methods}.
All results were averaged across $3$ separate training runs for pre-trained encoder models. 

\paragraph{Datasets.} % \label{sec:data}
To compare the models and methods, we work with three open samples of transactional data: \href{https://boosters.pro/championship/rosbank1}{Churn}, \href{https://boosters.pro/championship/alfabattle2}{Default} and \href{https://www.kaggle.com/datasets/ashisparida/hsbc-ml-hackathon-2023}{HSBC}.
Details on the datasets are in Appendix~\ref{sec:app_datasets}





\subsection{Main results}

% All models were tested in the way described in section \ref{sec:methods} on two applied tasks. 
The results for the Churn, Default, and HSBC datasets are presented in Figure \ref{fig:coles}.
Also, Figure \ref{fig:mean_rank} summarizes these results by presenting ranks of all methods averaged by the mean ROC-AUC metric for all datasets.
Results for two additional non-financial datasets are provided in Appendix~\ref{sec:other-data-results}. We also provide more detailed experiment metrics and rank data results in Table~\ref{tab:metrics} in Appendix~\ref{sec:AppMainTable} and and Table~\ref{tab:metrics} in Appendix~\ref{sec:AppMainTable}.

External context improves metrics in most cases. This is especially noticeable in the balanced Churn sample. In the unbalanced Default and HSBC samples, contextual representations also help models solve local and global problems, but this is not as explicit as in the case of a balanced dataset. Figures also allows us to note, that aggregation methods provides less variation among different seeds, thus, the produced models are more robust. 

According to Figure~\ref{fig:mean_rank} and Table~\ref{tab:ranks}, the best method is the Kernel attention, which performs best or is near the best for all tasks.
However, all attention-based methods with learnable parts (Learnable attention, Symmetrical attention, Kernel attention) often end up among the leaders, especially in the event-type prediction task. 
It is natural, as attention-based probing often leads to stronger models that account for more intricate connections between users.

On the other hand, approaches inspired by the Hawkes process (Exp Hawkes, Exp learnable Hawkes, Attention Hawkes) show inferior results even though these methods take into account the temporal distance of sequence embeddings and, because of this, should respond better to local temporal changes in the data. 
So, accounting for diverse time differences in the context of conditional intensity modelling, given the history of events, remains open for the task of external context addition and requires further research.

Among the methods without learnable parts (Mean, Max, Attention, Exp Hawkes, Attention Hawkes), the Mean and Max methods perform best in local and global validation tasks --- in this case, we can capture average external context, which also should be helpful to specific prediction tasks. 
%This says a lot about society :)
So, default methods of aggregation can be reasonable in the absence of computational resources. 
However, the better choice is to build an external context accounting method with learnable parts, like \emph{Kernel attention}. 

\input{tables/time}

\begin{figure*}[!t]
\centering
\begin{subfigure}{.46\textwidth}
  \centering
  \includegraphics[width=.97\linewidth]{figures/churn_ft.pdf}
  %\captionsetup{justification=raggedright,singlelinecheck=false}
  \caption{Churn dataset}
  \label{fig:scatter_fgsm}
\end{subfigure}
\begin{subfigure}{.46\textwidth}
  \centering
  \includegraphics[width=.97\linewidth]{figures/default_ft.pdf}
  %\captionsetup{justification=raggedright,singlelinecheck=false}
  \caption{Default dataset}
  \label{fig:scatter_kll2}
\end{subfigure}
\caption{ROC-AUC for the task next event type prediction with and without fine-tuning encoder.}
\label{fig:fine-tune}
\end{figure*}

\subsection{Dependence on the number of external sequences}

To show how the number of users influences the construction of external context aggregation, we measured how the quality of applied tasks changes depending on the change.
Considered context sizes included $10$, $50$, $100$, $500$ and $1000$ external sequences.
In this experiment, we select the users whose last event was the most recent for the considered time point.
There, the results are for the best approaches from each cohort: from the pooling methods, it was the Max; from the attention-based methods --- our best Kernel attention, and from methods inspired by the Hawkes method --- Exp Hawkes.  

The results for the Churn dataset are presented in Figure ~\ref{fig:iters}.
The increase in the number of users positively affects the quality of global target and event-type prediction tasks. This statement is not valid for the Exp Hawkes method in the event type prediction task, but as stated above, the Hawkes method group did not work for us. 
On the other hand, $500$ sequences in a context should be enough for the performance sufficiently close to the top one.

\subsection{Fine-tuning backbone for local validation}

To improve the quality of our aggregation methods, we unfreeze the backbone in the process of model head event type prediction training. 
This procedure runs only for the local validation because using a differentiable linear head instead of gradient boosting significantly decreases the quality of the global validation task. 
We compare training the neural network in two modes: training both the encoder and head (unfreeze mode) and training only the head with a freezed encoder, as in all experiments before. 

The results are presented in Figure \ref{fig:fine-tune}. 
Unfreezing of the encoder increases the quality of pooling aggregation, especially for the Churn dataset. 
Besides, the ROC-AUC of all pooling grew substantially compared the encoder without external context.  



\subsection{Computational overhead for the external aggregation}

As discussed in Section~\ref{sec:prod_efficient}, during Inference, the inclusion of external context leads to minor computational overhead if an overall procedure is implemented properly.
However, during training, additional expenses can outweigh potential benefits.

We measured the time required to complete four different training stages.
\begin{itemize}
    \item \textit{Pretrain CoLES SSL} is training encoder with self-supervised loss. In this stage, we get a pre-trained encoder for getting only local representation vectors.
    \item \textit{Pretrain Pool SSL} is training pooling learnable parameters only for learnable pooling methods with self-supervised loss. In this stage, we get a pre-trained encoder with pooling layers for getting both local and external representation vectors.
    \item \textit{Fine-tune, freeze} and \textit{Fine-tune, unfreeze} is training head over freezed or unfreezed backbone
 with pooling on the next event prediction task, respectively. In this stage, we train the head of the encoder for the local validation task.
\end{itemize}
The first stage \textit{Pretrain CoLES SSL} is exactly the training of a basic model, all others are additional costs that can be required to use external aggregation approaches.

The training time in minutes is in Table~\ref{tab:time}.
We see, that all training time are comparable, while additional training with unfreezing requires significant additional resource, while simultaneously boosting the model quality.



