\section{Hawkes-process inspired methods}
\label{sec:hawkes_insprired_methods}

The original Hawkes process is described by the following formula for the conditional intensity function $\lambda_u(t)$:
\begin{align*}
\lambda_u(t) &=
\mu_u(t) + \sum_{v} b_{uv} \sum_{e_i \in \mathcal{H}_v(t)} \kappa(t-t_i) = \\
&= \mu_u(t) + \sum_{v} \sum_{e_i \in \mathcal{H}_v(t)} b_{uv}  \kappa(t-t_i),
\end{align*}
where the first term, $\mu_u(t)$, models the current event sequence behaviour, and the second term, with $f_{uv}$ models interaction between sequence $u$ and sequence $v$,  $\kappa(t - t_i)$ is a kernel function of time, usually exponential. $e_i \in \mathcal{H}_v(t)$ are all events from sequence $v$ before time $t$. 

In the original Hawkes process, events from one sequence have the same type. In our case, this is not true. Also, as we work not with original sequences but with their representations, we can simplify the equation:
\[
\lambda_u(t) = \vecmu_u + \sum_{v} b(\vecmu_u, \vecmu_v) \kappa(t - t_v).
\]
Here, $\vecmu_u$ is a representation vector of the sequence under consideration, $\vecmu_v$ is a representation vector of the sequence from the training dataset,  and $t_v$ is the time moment of the last event in the $v$ sequence at the current time point, and $b$ and $\kappa$ are learnable kernels. The second term in the equation is the target external representation. 

In our notation, the equation can be rewritten:
\[
     \mathbf{g}_t = b(X, \mathbf{h}_t)\cdot \kappa(t \cdot \mathbf{1} - \mathbf{T}),
\]
where $\mathbf{T}$ is a vector of  last event times in $X$ for the current time~$t$.

We simplify this general formulation to a set of different special cases. 
Mostly, this step is taken due to the instability of learning in a general case.
The different variations of the presented formulation of the problem are enumerated below.

\paragraph{Exponential Hawkes.} In this simple variation of the Hawkes method, we use an exponential kernel for time transformation and identical transformation for matrix $X$.
\begin{equation}
     \mathbf{g}_t = X \exp{(-(t \cdot \mathbf{1} - \mathbf{T}))}.
\end{equation}
In this case, we weigh the vectors from $X$ with exponential time-dependent weights.

\textbf{Learnable exponential Hawkes}

The learnable transformation was used in this method. Firstly, we concatenate each vector from  $X$ with $\mathbf{h}_t$ and get matrix $Y \in \mathbb{R}^{2m \times n}$. Secondly, we pass the concatenated matrix through feed-forward neural network $\phi_{NN}(\cdot)$ and get the matrix  $X'\in \mathbb{R}^{m \times n}$. 
\begin{align*}
        \mathbf{g}_t &= \phi_{\mathrm{NN}}(\mathrm{concatenate}(X, \mathbf{h}_t))\exp{(-(t \cdot \mathbf{1} - \mathbf{T}))} =\\ 
     &= \phi_{\mathrm{NN}}(Y)\exp{(-(t \cdot \mathbf{1} - \mathbf{T}))} = \\
     &= X'\exp{(-(t \cdot \mathbf{1} - \mathbf{T}))}.
\end{align*}

Here, we additionally consider the dependencies between the current embedding vector and embedding vectors from the dataset.

\paragraph{Learnable Hawkes.}
This variation of the method is learnable. The embeddings transformation works exactly the same way as in the previous exponential learnable Hawkes method, but here we also add a learnable transformation for times deltas using a feed-forward neural network~$\kappa_{NN}(\cdot)$.
\[
    \mathbf{g}_t = \phi_{\mathrm{NN}}(\text{concatenate}(X, \mathbf{h}_t))\kappa_{\mathrm{NN}}(t \mathbf{1} - \mathbf{T}). 
\]
This method shows instability during learning, as the loss tends to go to infinity. So, it requires more in-depth study, and its results are not presented in this work.

\paragraph{Attention Hawkes.} In this method, we combine the usual exponential Hawkes and the attention methods:
\begin{equation}
     \mathbf{g}_t = X\mathrm{softmax}(X^T \mathbf{h}_t)\exp{(-(t \cdot \mathbf{1} - \mathbf{T}))},
\end{equation}
This approach has double weighting: the first accounts for user similarity, and the second accounts for the time delta.

\section{Implementation details}~\label{sec:details}

\paragraph{Backbone pretrain.} In this research, the pipeline follows that of the \href{https://github.com/dllllb/pytorch-lifestream/tree/main}{pytorch-lifestream} package, which contains an implementation of the CoLES model. 
Following their original article, we use an LSTM block with a hidden layer dimension of $1024$ for the Churn and HSBC datasets.
For Default, absent in that article, we select GRU with a hidden size of $800$ that provides competitive results on numerical experiments.
The models were trained for $60$ epochs with a batch size of $128$.
The model takes two features as input: MCCs and amounts of transactions. 
As MCC is a categorical variable, we use its embedding of size $d_{\mathrm{mcc}} = 24$ for Churn and HSBC or $16$ for Default.


\paragraph{Learnable aggregations.}
The learnable attention matrix $A$ from the learnable attention method, the learnable matrix $S$ from the symmetrical attention method, and the learnable functions $\phi(*)$ and $\phi_{\mathrm{NN}}(*)$ from the kernel attention and exponential learnable Hawkes methods were trained in the CoLES learning pipeline with fixed encoder. 

For the  $S \in \mathbb{R}^{r \times m}$ matrix, internal size $r$ was taken to be equal to $100$. For the learnable functions $\phi(*)$ and  $\phi_{\mathrm{NN}}$, we use a two-layer neural network with a hidden size equal to $100$. 

In the CoLES pipeline, all the learnable elements were trained for $60$ epochs with a batch size $128$.

A model using external information requires considerable computing resources since it needs to store local representations of all users from the training set for all time points.
Due to available memory requirements, we use only a random part of the set of local representations: for the Churn dataset, the number of clients to train in all experiments was $1000$; for the Default dataset, $300$; and for HSBC, $1000$.


\paragraph{Validation.} 


For the global validation task, we train the LGBM model with $500$ estimators, learning rate equal to $0.02$, regularization coefficients for $l_1$ and $l_2$ regularization equal to $1$. For the local validation task, we train linear head over encoder. There, the hyperparameters are the following: window size equal to $32$, stride equal to $16$, batch size equal to $512$, and a maximum number of epochs equal to $20$ for Churn and Default datasets and $10$ for HSBC dataset, learning rate equals to $0.001$, optimizer is Adam. The local task validation was conducted in two modes: freeze and unfreeze. In freeze mode, we train only the local validation head without updating encoder weights. In unfrozen mode, we train both the encoder and head parts on the neural networks.

\section{Description of datasets}
\label{sec:app_datasets}

\emph{Churn} dataset contains transactional data of bank customers and has been used previously~\cite{babaev2022coles}.
For global validation, it is proposed that the problem of binary client classification be solved depending on whether the client left the bank (churn case).
The classes here are almost balanced.

\emph{Default} dataset~\cite{bazarova2024universal} also contains transactional data of bank customers.
For global validation, it is proposed that the problem of binary classification of clients be solved depending on whether the client could repay the loan to the bank.
There is a significant class imbalance in this sample.
Unlike most other open transaction data in this dataset, the target variable is close to a real business problem, namely future credit default identification in credit scoring.

\input{tables/dataset}

\input{tables/new_datasets}

\input{tables/metric_table}
\input{tables/ranks}

\emph{HSBC} dataset~\cite{bazarova2024universal} originates from a competition on the fraud identification task.
Each transaction in the dataset has a mark that indicates whether this transaction was made by the client personally or maliciously. 
We add the global target, which identifies whether at least one transaction in sequence was fake for each user.
With fewer identified fraudulent transactions, the target labels are imbalanced, similarly to \emph{Default}.

Table~\ref{tab:datasets} summarizes the main characteristics of the dataset.


\section{Detailed metrics for main experiments} 
\label{sec:AppMainTable}
% TODO: if we have time, make text nicer

Here, we provided an expanded version of the results for financial datasets with additional metrics.
ROC AUC and PR AUC values are available in Table~\ref{tab:metrics}.


Precise metrics and measurements of PR AUC values confirm our finding from the main part.
We also complement these results with ranks of the considered approach and their average in Table~\ref{tab:ranks}.
They also confirm the lead of \emph{Kernel attention}, while simpler approaches like \emph{Mean} provide a strong alternative.



\section{Non-financial domain dataset}
\label{sec:other-data-results}
We also conducted experiments for several non-transactional datasets. 

% \input{tables/new_datasets}

\href{https://www.dunnhumby.com/source-files/}{\textbf{Dunnhumby-Carbo.}} This dataset includes transactional data of households in a retail store. It contains more than 500k transactions over two years.

For this dataset, we consider only the next event-type prediction task. Results are presented in the Table~\ref{tab:dunnhumby}. Mean, max and different variations of the learnable attention methods outperform the baseline, while differences are pretty small --- we assume, that the data nature prevents us from significantly benefiting from aggregation.

\href{https://www.kaggle.com/c/data-science-bowl-2019}{\textbf{Assessment.}}
The dataset contains the history of childrenâ€™s gameplay data. 
It consists of 12M gameplay events combined into 330K gameplays representing 18K children, but only 17.7K gameplays are labelled. 
Each gameplay event is characterized by a timestamp, an event code, an incremental counter of events within a game session, the time since the start of the game session, etc.

For this dataset, we have considered two tasks. The first is the next-event type prediction, and the second is the prediction of the in-game assessment results. For the second task, the target is one of four grades, with proportions of $0.50$, $0.24$, $0.14$, and $0.12$. For the first task, we use the full dataset for validation, and for the second, only its labelled part.

For external information aggregation, we consider each gameplay as a separate user.

% \begin{figure}[!ht] 
%       \centering
%           \includegraphics[width=1.0\columnwidth]{figures/assessment.pdf}
%           \caption{\selectfont ROC AUC for next event prediction task for \textit{Assessment} dataset. }\label{fig:assessment}
% \end{figure}

Results are also presented in Table \ref{tab:dunnhumby}. All aggregation techniques provide a significant boost for global target validation but a very insignificant difference for the next event-type prediction task. This can be explained by the fact that different children play individually; that is, they do not interact in any way and do not influence each other during the game. Therefore, the results of the event type prediction task, which depends on the time component, remains unchanged. 
On the other hand, the change of metrics for the global target validation task may be due to the fact that this target is a grade for a game, and the addition of other peopleâ€™s embeddings allows comparing them. If the model can highlight the difference between the grades of the current person and the average grade, then the assessment of absolute values becomes easier to predict.

% \begin{table}[!t]
% \caption{Results for Assessment dataset}
% \label{tab:assessment}
% \centering
% \begin{tabular}{lcc}
% \hline
% ~ & \begin{tabular}{l}
%      Event type\\
%      ROC AUC
% \end{tabular}  & \begin{tabular}{l}
%      Global target\\
%      ROC AUC
% \end{tabular} \\
% \hline
% Without context & 0.8588 $\pm$ 0.0020 & 0.8711 $\pm$ 0.0182 \\
% Max &   & \textbf{0.9962 $\pm$ 0.0012} \\
% Mean &  0.8586 $\pm$ 0.0024 & \textbf{0.9962 $\pm$ 0.0012} \\
% Exp Hawkes  &  0.8586 $\pm$ 0.0023 & \textbf{0.9962 $\pm$ 0.0012} \\
% Learnable Exp Hawkes &   & 0.9958 $\pm$ 0.0014 \\
% Attention & 0.8602 $\pm$ 0.0025 & 0.9958 $\pm$ 0.0013 \\
% Learnable attention &   & 0.9957 $\pm$ 0.0012 \\
% Symmetric attention &   & 0.9959 $\pm$ 0.0013 \\
% Attention Hawkes & 0.8593 $\pm$ 0.0016  & 0.9958 $\pm$ 0.0013 \\
% Kernel attention &   & 0.9960 $\pm$ 0.0013 \\

% \hline
% \end{tabular}
% \end{table}