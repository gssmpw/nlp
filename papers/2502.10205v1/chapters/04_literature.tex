\section{Related work}
\label{lit_rewiev}

We review the approaches to external context aggregation in various data modalities and methods for evaluating sequence representations. 
The review also considers existing representation learning methods for event sequences, including self-supervised methods for general and bank transaction domains, to provide further background.

\subsection{Event sequence representation learning}
Neural networks have been shown to perform well when faced with the task of event sequence representation learning~\cite{babaev2022coles}.
Obtained representations are helpful both for tasks describing local in-time properties, such as next event prediction~\cite{zhuzhel2023continuous} or change point detection~\cite{deldari2021time,ala2022deep} and whole sequence classification~\cite{bin2022review,jaiswal2020survey}.

\paragraph{Self-supervised learning} (SSL) is a solid representation learning paradigm that learns an encoder --- neural network, providing representation vector, without labelled data. 
This allows researchers to skip costly and possibly limited human expert annotation and leverage the large bodies of low-cost unlabeled data, leading to more universal representations~\cite{caron2021emerging}.
This paradigm is often implemented within contrastive and generative learning frameworks~\cite{zhang2020self,liu2023ssl}. 

In contrastive learning, the encoder learns to produce embeddings that are close if objects are labelled as similar and distant, and vice versa. 
%allow distinguishing between objects that are labelled as close or distant in a self-supervised manner.
It originated in computer vision in the form of Siamese loss~\cite{hoffer2015deep} and SimCLR~\cite{chen2020simple} with subsequent DINO \cite{caron2021emerging} and BarlowTwins~\cite{zbontar2021barlow} among others.
It also allows the production of meaningful representations for time series and event sequence data~\cite{zhang2024self}.
% These approaches are also gaining popularity in the field of multidimensional event sequences and time series~\cite{babaev2022coles, li_deep_2020, romanenkova2022similarity, moskvoretskii2024self, li2023new}. 
Our work also uses a contrastive representation model as it performs well~\cite{babaev2022coles} for event sequences data, producing universal representations. 
The authors of the method studied various ways to define positive and negative pairs during training. 
They came up with the split strategy, which considers two transaction subsequences from the same client to be a positive pair and from different clients to be negative. 
There, representations of the subsequences were obtained via a Long Short-Term Memory network~\cite{hochreiter1997long} (LSTM) as an encoder, as transformers don't always perform stronger for sequential data, similarly to~\cite{yue2022ts2vec}. We will use the CoLES~\cite{babaev2022coles} model with an RNN-based encoder as a basic encoder in our work. The limitations of CoLES are strongly connected to the method, which is used to obtain positive and negative pairs of subsequences. This algorithm explicitly encourages the model to ensure that the resulting representations show information about the user as a whole rather than about his local state at a specific time.
In addition, the authors use client classification as the main task in the article to demonstrate the quality of their model.
% TODO: add few articles about Neural Hawkes-based models as another form of SSL for event sequences data (we use these ideas later!) 

Another powerful tool for constructing representation vectors can be Hawkes processes~\cite{hawkes1971spectra, zhang2020self, hawkes1971spectra, hawkes2018hawkes,  hawkes2016de}. Hawkes processes, a class of self-exciting point processes, offer a powerful framework for modelling event sequences where the occurrence of one event increases the probability of subsequent events. This makes them well-suited for analyzing transactional data, where events such as purchases often exhibit temporal dependencies. Hawkes processes incorporate the temporal dynamics and self-exciting nature of transactional events.

\subsection{Accounting for external information}
All the methods mentioned above consider only one sequence at a time. It is sometimes beneficial to consider the overall context formed from the actions of other clients~\cite{ala2022deep}.

External context may consist of the behaviour of other specific clients, or it may reflect the current macroeconomic state. It has been shown that the two strongly correlate~\cite{thomas2000survey,begicheva2021bank}. This implies that the external context may contain useful information for model training.

Since choosing macroeconomic indicators is difficult without the proper education and thus requires bringing in a human expert in the field, it makes more sense to try different ways to aggregate the actions of all bank clients (mean, max, etc.). This approach allows the extraction of more data from the dataset without additional annotation.

Preliminary experiments in~\cite{bazarova2024universal} show that even naive aggregation within an external context provides superior results in some event sequences modelling problems, while their results lacked stability and were limited within two datasets.
For graph representation, learning links between objects is the essential part of a model, being implemented in SOTA models~\cite{kipf2022semi}.
This idea extends to temporal graphs where attention-based approaches~\cite{velivckovic2018graph} via self-supervised learning help to handle complex interactions between objects~\cite{liu2024self}.
Similarly, this line of thought has been developing in temporal point processes, Hawkes mutually-exciting point processes in particular, where one can either derive or use provided node links~\cite{dizaji2022comparative,passino2023mutually}. 
However, in most cases, exact information between connections even from graph data, is absent, and their restoration is of limited quality~\cite{shumovskaia2021linking}.

Other examples of simultaneous accounting for the behaviour of different users and characteristics of a particular moment in time occur in recommendation systems. While classical algorithms consider exclusively the similarity of users or items for recommendations, session-based~\cite{wang2021session} and time-based~\cite{ghiye2023timedecay, JAIN20231834, xia2010timedecay} recommendation systems integrates information from a flow of purchases from different users. Thus, we account for dynamic user preferences and ongoing external context.
The system in this case balances user and group recommendation, as a review~\cite{ceh2022performance} notes. 
Further papers included attention mechanism in this workflow~\cite{guo2020group}. 
Limited scope of this works is constrained by efficiency constraints, that existing works solve by considering recommendations for predefined group of multiple users.
However, a single sequence perspective can be crucial given the diversity of possible life paths.


\subsection{Sequence embedding evaluation}
Evaluation and comparison of proposed approaches are crucial for obtaining evidence on the topic. They allow the researcher to test the applicability of the considered methods.
For event sequences, we require tests that consider both local and global embedding properties.
The local ones reflect the ability of a model to capture an instant state.
For example, in this case, we can test whether the model can predict the next event type~\cite{shchur2021neural}.
Additionally, the global properties consider questions about a sequence as a whole. 
To test these properties, we make a model to predict a sequence label, following~\cite{babaev2022coles}, e.g. if a client will default or leave a bank.
While the papers~\cite{osin2024ebes,xue2023easytpp} use a wider selection of datasets, they consider either specific downstream targets or focus on the prediction of the next event type and time. 
We expand these statements by focusing on the universality of embedded embeddings.
So, our work expands the benchmark from~\cite{bazarova2024universal}, which simultaneously evaluates the quality of a representation for a simultaneous classification of a whole sequence and predicts upcoming event characteristics by using more relevant datasets. 


% On the other hand, CoLES and some of the other mentioned before models have a bunch of drawbacks. Firstly, CoLES, trained on whole sequences, performs poorly on smaller sequence slices. Secondly, it cannot be used to track the changes in the behavior of a single user: all slices from a common sequence will have close representations. Besides that, it could be argued that similar subsequences need to have close representations, even if they come from different users, which is definitely not the case for CoLES. All in all, CoLES representations show good performance when faced with global, inter-user tasks, but local in time tasks require a different training procedure.


% \textbf{Generative models} use different learning approaches to learn the distribution of hidden data.
% The knowledge they gain can then be used to generate plausible data. These approaches often originate from neural language processing (NLP)~\cite{kenton2019bert, radford2019language}.

% Autoencoder is a popular generative model due to its efficiency and simplicity in different domains~\cite{tschannen2018recent}. 
% The paper~\cite{mancisidor2021learning} learns bank client representations useful for downstream tasks using variational autoencoder. 
% Masked language models take the autoencoder idea one step further. 
% Such models recover randomly changed tokens and perform well on various benchmarks~\cite{kenton2019bert, he2022masked}.

% According to the latest research, generative methods outperform contrastive ones at missing value prediction~\cite{jaiswal2020survey}. 


% Autoregressive (AR) modeling is extensively used in CV~\cite{van2016pixel, chen2018pixelsnail, razavi2019generating, esser2021taming}, as well as in sequential domains like NLP~\cite{radford2018improving, brown2020language, black2022gpt} and Audio~\cite{oord2016wavenet, borsos2023audiolm}. 
% Such models are trained to predict the next item in a sequence. Unlike token embeddings in autoencoders, autoregressive models do not use information about future tokens.
% Because of this feature, autoregressive models can capture more complex patterns.  It's confirmed by their superior performance on text generation tasks~\cite{ethayarajh2019contextual}.
