\newcommand{\CLASSINPUTtoptextmargin}{0.77in}
\newcommand{\CLASSINPUTbottomtextmargin}{1.075in}
\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024
%\addtolength{\topmargin}{+0.05in}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,fourier}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{enumitem}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{hyperref}

\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bbeta}{\boldsymbol{{\beta}}}
\newcommand{\bphi}{\boldsymbol{{\phi}}}
\newcommand{\bpsi}{\boldsymbol{{\psi}}}
\newcommand{\ndim}{{d}}
\newcommand{\Rf}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\ST}{{\mathcal{S}}}
\newcommand{\subsamp}{{\mathcal{T}}}
\newcommand{\defined}{\, := \,}
\newcommand{\fr}[2]{{ \displaystyle \frac{#1}{#2} }}
\newcommand{\Sm}[2]{{\displaystyle \sum_{#1}^{#2}}}
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\newcommand{\bdm}{\begin{displaymath}}
\newcommand{\edm}{\end{displaymath}}
\newcommand{\transp}{{\scriptscriptstyle{\mathsf{T}}}}


\begin{document}

\title{Explainable and Class-Revealing Signal Feature Extraction
  via Scattering Transform and Constrained Zeroth-Order Optimization}

\author{\IEEEauthorblockN{Naoki Saito}
\IEEEauthorblockA{\textit{Department of Mathematics} \\
\textit{University of California}\\
Davis, CA 95616 USA \\
nsaito@ucdavis.edu}
\and
\IEEEauthorblockN{David Weber}
\IEEEauthorblockA{\textit{The Delphi Group} \\
\textit{Carnegie Mellon University}\\
Pittsburgh, PA 15213 USA \\
dsweber2@protonmail.com}
}

\maketitle

\begin{abstract}
  We propose a new method to extract discriminant and explainable features from a particular machine learning model, i.e., a combination of the scattering transform and the multiclass logistic regression. Although this model is well-known for its ability to learn various signal classes with high classification rate, it remains elusive to understand why it can generate such successful classification,
  mainly due to the nonlinearity of the scattering transform. In order to uncover the meaning of the scattering transform coefficients selected by the multiclass logistic regression (with the Lasso penalty), we adopt zeroth-order optimization algorithms to search an input pattern that maximizes the class probability of a class of interest given the learned model. In order to do so, it turns out that imposing sparsity and smoothness of input patterns is important. 
We demonstrate the effectiveness of our proposed method using a couple of synthetic time-series classification problems.
\end{abstract}

\begin{IEEEkeywords}
Nonlinear discriminant feature extraction, scattering transform, wavelets, zeroth-order optimization, sparsity and smoothness of signals
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
In signal and image classifications, the \emph{Scattering Transform} (ST)
\cite{MALLAT-SCAT, BRUNA-MALLAT}, which cascades wavelet transform convolutions
with modulus nonlinearities (i.e., absolute values) and averaging operators,
has emerged as an alternative to the popular Convolutional Neural Networks
(CNNs)/Deep Learning (DL)~\cite{LECUN-BENGIO-HINTON, Goodfellow-et-al-2016}.
Since only two or three layers of the cascades in the ST are sufficient and
since it uses the predefined wavelet convolution filters,
it has a number of advantages over CNNs/DL for signal classification problems:
1) its training process is computationally faster;
2) it does not require a large number of training samples;
3) it automatically generates multiscale/multifrequency feature representations
of input data in an \emph{explicit} manner via wavelet filters;
4) the computed ST coefficients can be fed to any classifier of choice, 
e.g., Multiclass Logistic Regression (MLR), Support Vector Machine (SVM), $k$-Nearest Neighbors ($k$NN), etc.; and 
5) it is more mathematically tenable.
Yet, it still extracts robust and quasi-invariant features of input signals
relative to certain types of deformations or perturbations (e.g., a small amount
of shifts, warps, noise, etc.) so that it provides comparable classification
performance as DL models.
In fact, for a small number of training samples, it outperforms DL
models~\cite{BRUNA-MALLAT-CVPR11, BRUNA-MALLAT, ANDEN-MALLAT}.

Our earlier work has obtained excellent results using the ST and its
variant in a number of signal classification problems including:
underwater object classification from acoustic wavefields~\cite{SAITO-WEBER-SPIE, WEBER-PHD}; texture image classification~\cite{CHAK-SAITO-MWSN}; and music
genre classification~\cite{CHAK-SAITO-WEBER}.
However, one critical thing remains to be understood: \emph{Why does this combination of the features and the classifier work so well? What are the features
\underline{in the original time domain} that contributed to these excellent
classification results?} Because the ST is a nonlinear transform like DL, even
if we can identify which ST coefficients significantly contributed to the
correct classification, the true meaning of such coefficients in the original
time domain has been elusive: the only explicitly available information is the
so-called \emph{path} information, i.e., a set of indices of the wavelet filters
(e.g., passband info) used in all the previous layers and the current layer
in order to generate that particular ST coefficient.

Hence, our aim here is to \emph{extract explainable or class-revealing features
using the logistic regression coefficients learned on the ST coefficients of
given training samples.}
More specifically, we plan to \emph{estimate signals that maximize the class
probability using their ST coefficients and the trained MLR classifier for a signal class of interest}.
If we are successful, then we can deepen our understanding of the features
in the original time domain that are responsible for discriminating one class
from the other.
Such understanding and insight may uncover the secret of physical nature of
a given problem, e.g., detection and classification of underwater objects via
scattered acoustic wavefields; see \cite{SAITO-WEBER-SPIE, WEBER-PHD} for
the background information and our previous effort.
This is quite important since it may provide us with an opportunity to
\emph{design new transmitter signal patterns and/or sensors that specifically
focus on discriminating a particular class of objects}.

\section{Our Proposed Method}
\label{sec:method}
To be concrete, let us consider a typical signal classification problem.
Let $\{\bx_i\}_{i=1:N}$ be available training signals each of which has $\ndim$
time samples. Each $\bx_i \in \Rf^\ndim$ has a class label,
$k \in \{1, \ldots, K\}$, i.e., $\bx_i \in X_k$, where $X_k \subset \Rf^\ndim$ is
a space of class $k$ signals.
Our proposed procedure is the following.
\begin{description}[labelsep=0.5em]
\item[Step 1:] Apply the ST to the training samples;
\item[Step 2:] Train the GLMNet classifier (= the MLR classifier with the Lasso penalty~\cite{HASTIE-TIB-WAINWRIGHT}), which can efficiently
  select a small number of the ST coefficients as key features; let $(\alpha_k, \bbeta_k)$, $k=1:K$ be the resulting intercepts and regression coefficient vectors;
\item[Step 3:] Find an input pattern $\hat{\!\bx} \in \Rf^\ndim$ for class $k$
  that minimizes the following
  criterion:
  \begin{equation}
    \label{eqn:opt}
    \hat{\!\bx} = \arg\min_{\bx \in \Rf^\ndim} \frac{1}{p_k(\bx)} + \mu \| \bx \|_1 + \nu \| \nabla \bx \|_2,
  \end{equation}
  where $p_k(\bx) \defined \exp(\alpha_k+\bbeta_k \cdot \ST[\bx])/\sum_{j=1}^{K} \exp(\alpha_j + \bbeta_j \cdot \ST[\bx])$ is the probability of a signal $\bx$ belonging to class $k$ (according to the trained GLMNet classifier); $\ST[\bx]$ is the ST coefficient vector computed from $\bx$; and  $\mu > 0$, $\nu > 0$ are the Lagrange multiplier parameters to be adjusted.
  Note that the minimization of the first term $1/p_k(\bx)$ is clearly equivalent
  to the maximization of $p_k(\bx)$ while the second and third terms promote
  \emph{sparsity} and \emph{smoothness} of $\bx$, respectively.
\end{description}
We now describe the ST a bit more precisely.
Since we only consider the second-layer ST coefficients, we define it for a given
input 1D signal $\bx \in \Rf^\ndim$ as follows:
\begin{equation}
\label{eqn:ST}
\ST[\bx] \defined  \left\{ \rho\( \rho\(\bx \circledast_{r_1} \bpsi^{(1)}_{\lambda_1}\) \circledast_{r_2} \bpsi^{(2)}_{\lambda_2} \) \circledast_{r_a} \bphi_J \right\}_{\lambda_1 \in \Lambda_1; \lambda_2 \in \Lambda_2},
\end{equation}
where %$\subsamp_r$ is the subsampling operator with rate $r$, i.e.,
%$\subsamp_r: \by=(y_1, \ldots, y_\ndim)^\transp \in \Rf^n \mapsto (y_1, y_{r+1}, y_{2r+1}, \ldots, y_{\lceil \ndim/r \rceil r + 1})^\transp \in \Rf^{\lceil \ndim/r \rceil}$;
$\rho$ is the elementwise modulus operator,
$\rho: \by=(y_1, \ldots, y_\ndim)^\transp \in \Rf^\ndim \mapsto |\by| \defined (|y_1|, \ldots, |y_\ndim|)^\transp \in \Rf_{\geq 0}^\ndim$;
$\circledast_{r}$ indicates a 1D circular convolution (typically done via multiplication in the Fourier domain) \emph{followed by subsampling with rate $r \geq 1$};
$\bpsi^{(l)}_{\lambda_l}$, is the mother wavelet filter with frequency-band index $\lambda_l \in \Lambda_l$ for the $l$th layer, $l=1, 2$; and finally $\bphi_J$ is
the father wavelet (i.e., lowpass) filter at scale $J \in \N$. Note that
we allow different subsampling rates for each layer, i.e., $r_1, r_2$ and
for the final averaging process $r_a$.
For the details, see \cite{MALLAT-SCAT, ANDEN-MALLAT, KYMATIO} as well as
\cite[Chap.~3]{WEBER-PHD}.

Step 3 needs a bit more explanation here.
To solve the minimization problem Eq.~\eqref{eqn:opt}, we use the so-called
\emph{zeroth-order (ZO) or derivative-free optimization}~\cite{CONN-ETAL_ZO-BOOK, LARSON-ETAL_ZO, LIU-ETAL_ZO}. For our proposed method, it is better to use ZO
optimization algorithms than the popular first-order (FO) optimization algorithms
that require computing the gradient of the objective function to be minimized.
This is because the gradient $\nabla \ST[\bx]$ is highly discontinuous, leading
update steps to converge slowly, as shown in \cite[Chap.~4]{WEBER-PHD}.
%although computing $\nabla \bx$ is easy if $\bx$ is a signal sampled on a regular grid.
In this paper, we use the popular ZO method called \emph{Differential Evolution} (DE)~\cite{AHMAD-etal_DE-Rev, BILAL-etal_DE-Rev, OPARA-ARABAS_DE-Rev}.
We also note that we perform the updates of solution candidates in the frequency
domain (via DCT) with the \emph{pink} (a.k.a.\ $1/f$) noise as the initial
randomized candidates instead of time domain updates starting from white noise.
This is because the updates of solution candidates in the time domain
are too local in time and many naturally-occurring signals follow the $1/f$
power spectra~\cite{PRESS}; see also \cite[Chap.~4]{WEBER-PHD}.
The DE is certainly not the only ZO method to use for our problems;
see more discussion of the choice of ZO methods in Section~\ref{sec:summary}.
Finally, we point out the importance of using the sparsity and the smoothness
constraints in Eq.~\eqref{eqn:opt}:
%There are two reasons why we use both these constraints in the ZO optimization:
%We use both these constraints in the ZO optimization because:
1) the landscape of Eq.~\eqref{eqn:opt} with $\mu=\nu=0$ becomes too rough for the ZO optimization to converge; and 2) the sparsity and the smoothness provide us with easy and intuitive interpretation of signal features.

\section{Examples}
\label{sec:ex}
%In this section, we will demonstrate the usefulness of our proposed method
%using two well-known synthetic signal classification problems.
We now demonstrate the usefulness of our proposed method
using two %well-known
synthetic signal classification problems.

\subsection{``Cylinder-Bell-Funnel'' Signal Classification}
\label{subsec:cbf}
\begin{figure}
  \begin{subfigure}{0.45\textwidth}
  \centering\includegraphics[width=\textwidth]{bcfsample5.png}
  \caption{Five samples/class in the ``CBF'' classification problem:
    ``Cylinder'' (bottom 5); ``Bell'' (middle 5); ``Funnel'' (top 5)}
  \label{fig:bcf-samples}
  \end{subfigure}

  \begin{subfigure}{0.45\textwidth}
  \centering\includegraphics[width=\textwidth]{bcfbeta.png}
  %\caption{$\bbeta$ coefficients of ``CBF'' classes}
  \caption{$\bbeta$ coefficients: ``Cylinder'' (bottom);
    ``Bell'' (middle); ``Funnel'' (top)}
  \label{fig:bcf-betas}
  \end{subfigure}

  \begin{subfigure}{0.45\textwidth}
    \centering\includegraphics[width=\textwidth]{bcfpseudoinv2.png}
%    \caption{Solutions of Eq.~\eqref{eqn:opt} for the ``CBF'' classification problem}
    \caption{Solutions of Eq.~\eqref{eqn:opt}: ``Cylinder'' (bottom);
    ``Bell'' (middle); ``Funnel'' (top)}

  \label{fig:bcf-sols}
  \end{subfigure}
  \caption{Extracted features via Eq.~\eqref{eqn:opt} reveal the decisive
    characteristics of each class in the ``CBF'' signal classification problem}
\end{figure}
We first tested our idea described above
%, in particular, the optimization problem Eq.~\eqref{eqn:opt} using the DE algorithm
on a simple yet well-known time-series classification dataset, called
the ``Cylinder-Bell-Funnel'' dataset that was introduced by one of us~\cite{SAITO-COIF-JMIV, SAITO-PHD} and has become quite well known in the time-series
analysis literature; see, e.g., \cite{GEURTS, KEOGH-KASETTY}.
In this example, we want to classify synthetic noisy signals with various shapes,
amplitudes, lengths, and positions into three possible classes ($K=3$).
More precisely, sample signals of the three classes were generated by:
\begin{align*}
c(i) &= (6 + \eta) \cdot \chi_{[a,b]}(i) + \epsilon(i) &&\text{for ``cylinder''}\\
b(i) &= (6 + \eta) \cdot \chi_{[a,b]}(i) \cdot (i-a)/(b-a) + \epsilon(i) && \text{for ``bell''}\\
f(i) &= (6 + \eta) \cdot \chi_{[a,b]}(i) \cdot (b-i)/(b-a) + \epsilon(i) && \text{for ``funnel''}
\end{align*}
where $i=1,\ldots,128$, $a$ is an integer-valued uniform random variable
on the interval $[16,32]$, $b-a$ also obeys an integer-valued uniform
distribution on $[32,96]$, $\eta$ and $\epsilon(i)$ are the standard normal
variates, and $\chi_{[a,b]}(i)$ is the characteristic (or indicator) function
on the interval $[a,b]$.
Figure~\ref{fig:bcf-samples} shows five sample waveforms from each class.
If there is no noise, we can characterize the ``cylinder'' signals by two step
edges and constant values around the center,
the ``bell'' signals by one ramp and one step edge in this order with
positive slopes around the center, and the ``funnel'' signals by
one step edge and one ramp in this order with negative slopes around the center.
In our numerical experiments below, we used the Julia programming language~\cite{JULIA};
more specifically, for the ST computation and the MLR with the Lasso penalty,
we used our own \texttt{ScatteringTransform.jl}~\cite{ScatteringTransform} and
the publicly-available \texttt{GLMNet.jl}~\cite{GLMNet}.
Then, for the DE algorithm, we used the \texttt{BlackBoxOptim.jl}~\cite{BBO} package.


We generated $100$ training signals per class % (see Fig.~\ref{fig:bcf}a),
and used the ST with the famous ``Mexican-hat'' wavelet function (i.e., the 2nd
derivative of the Gaussian) as its base filter. Finally, we trained the GLMNet
classifier on the 2nd layer ST coefficients $\left\{\ST[\bx_i] \in \Rf_{\geq 0}^{1078}\right\}_{i=1:300}$ of these training signals $\left\{\bx_i \in \Rf^{128}\right\}_{i=1:300}$,
to obtain the regression coefficient vector $\bbeta_k \in \Rf^{1078}$ and
the intercept, $\alpha_k \in \Rf^1$, $k=1, 2, 3$,
corresponding to cylinder, bell, and funnel classes.
The subsampling rates $r_1=r_2=1.5$ and $r_a=8$ were used while
the number of wavelet filters was set as $| \Lambda_1 |=14$ and $|\Lambda_2|=11$.
Hence, the final output size at the 2nd layer of the ST for each input signal
of length $128$ \emph{per filter pair $(\lambda_1, \lambda_2)$} was
$\lfloor 128/1.5/1.5/8 \rfloor = 7$, which led to $7 \times 14 \times 11 = 1078$
ST coefficients in total.
The classification accuracy of this method was almost perfect ($\approx 99$\%)
on newly-generated test dataset of size $3000$.
Figure~\ref{fig:bcf-betas} displays the learned $\bbeta_k$ coefficient vectors
that are quite sparse thanks to the Lasso penalty while Fig.~\ref{fig:bcf-sols}
shows the solutions of Eq.~\eqref{eqn:opt} using the DE algorithm.
It is amazing to see that \emph{those estimated signals pinpoint the
distinguishing features of those three classes}: the one for the bell class and
that for the funnel class are measuring the local activities around the
discontinuous regions where those classes abruptly ends (bell) or starts (funnel)
while it checks \emph{both} edge locations for the cylinder class.
They are the \emph{class-revealing} features and can be viewed as the
\emph{essence} of the prototype signals.

\subsection{Triangular Waveform Classification}
\label{subsec:tri}
\begin{figure}
  \begin{subfigure}{0.45\textwidth}
  \centering\includegraphics[width=\textwidth]{trisample5.png}
  \caption{Five samples/class of the triangular waveform problem:
  Class 1 (bottom 5); Class 2 (middle 5); Class 3 (top 5)}
  \label{fig:tri-samples}
  \end{subfigure}

  \begin{subfigure}{0.45\textwidth}
  \centering\includegraphics[width=\textwidth]{tribeta.png}
  \caption{$\bbeta$ coefficients: Class 1 (bottom); Class 2 (middle); Class 3 (top)}
  \label{fig:tri-betas}
  \end{subfigure}

  \begin{subfigure}{0.45\textwidth}
    \centering\includegraphics[width=\textwidth]{tripinv2.png}
    \caption{Solutions of Eq.~\eqref{eqn:opt}: Class 1 (bottom); Class 2 (middle); Class 3 (top)}
  \label{fig:tri-sols}
  \end{subfigure}
  \caption{Extracted features via Eq.~\eqref{eqn:opt}
    reveal the significant characteristic of each class
  in the triangular signal classification problem}
\end{figure}
This is another well-known classification problem, first appeared in
the classic book~\cite[Sec.~2.6.2]{CART}.
The dimensionality (or length) of each signal was extended from the original
$21$ in \cite{CART} to $128$ in order to fully utilize the power of the ST:
$21$ was just too short for cascades of wavelet convolutions.
Three classes of signals were generated by: % the following formulas:
\begin{align*}
x^{(1)}(i) &= uh_1(i) + (1 - u)h_2(i) + \epsilon(i) &&\text{for Class 1}, \\
x^{(2)}(i) &= uh_1(i) + (1 - u)h_3(i) + \epsilon(i) &&\text{for Class 2}, \\
x^{(3)}(i) &= uh_2(i) + (1 - u)h_3(i) + \epsilon(i) &&\text{for Class 3},
\end{align*}
where $i = 1, \ldots, 128$, $h_1(i) = \max(6 - |i - 43|/7, 0)$, 
$h_2(i) = h_1(i - 21)$, $h_3(i) = h_1(i - 42)$, $u$ is a uniform random variable
on the interval $(0, 1)$, and $\epsilon(i)$ are the standard normal variates.
Simply speaking, each class consists of random convex linear combination
of two triangles with additive white Gaussian noise.
Notice that the noiseless version of each class forms an edge of
a triangular manifold in $\Rf^{128}$ whose vertices are those three triangles
$\bh_k$, $k=1, 2, 3$. Hence, the noisy versions are distributed within Gaussian
balls around this triangular manifold.
Figure~\ref{fig:tri-samples} shows five sample waveforms from each class;
Fig.~\ref{fig:tri-betas} shows the sparse $\bbeta_k$ coefficient vectors; and
Fig.~\ref{fig:tri-sols} displays the extracted feature/the best input pattern
for each class; the red, green, and blue triangles in the background indicate
the vectors $\bh_1$, $\bh_2$, and $\bh_3$, respectively.
We used the same parameter settings as those of the ``CBF'' signal classification
problem.
%According to \cite{CART}, the Bayes error of this classification
%problem is about $14$\%, and using the ST plus MLR model, we get the classification error about $xx$\% on the test dataset.
The classification rate using the ST plus MLR was $88.9$\% using $1000$ newly generated signals per class as the test dataset while each of the estimated features in Fig.~\ref{fig:tri-sols} gave us essentially perfect classification rate of
$99.99$\%. In other words, these estimated features essentially discovered the
nature of class signal generation mechanism as in the ``CBF'' signal
classification problem.  The Class 1 and the Class 3 features in the bottom and
top rows in Fig.~\ref{fig:tri-sols} indicate the signal energy concentrates
around the apices of $\{\bh_1, \bh_2\}$ and $\{\bh_2, \bh_3\}$, respectively
while the Class 2 feature shown in the middle row of Fig.~\ref{fig:tri-sols}
does not have such energy concentration around $\bh_2$, indicating that the
signals of this class do not have the contribution from $\bh_2$.
This is quite interesting: if the Class 2 feature had energy concentration
around $\{\bh_1, \bh_3\}$, it would have generated worse classification rate
because it would confuse some Class 1 and Class 3 signals; hence our algorithm
decided to use the ``nonexistence'' of the $\bh_2$ component. This has given us
a serendipity: %in order to understand some competing class signals, it would be
in certain situations, it would be better to consider feature ``suppressors''
instead of feature extractors!


\section{Summary and Discussion}
\label{sec:summary}
%In this paper, we described our effort to understand nonlinear signal features
We have described our effort to understand nonlinear signal features
computed by ST and the MLR classifier % with the Lasso penalty
%with the help of
using the ZO optimization with constraints on sparsity and smoothness
of the features in the original time domain.
Our numerical results %on the two well-known synthetic signal classification problems
in Section~\ref{sec:ex}
were quite promising.

We have used the DE algorithm as our ZO method to solve Eq.~\eqref{eqn:opt}.
Clearly, other methods may work well with even faster computational speed than
the DE algorithm. Hence, it is important to evaluate various ZO algorithms that
is robust and computationally efficient.
There are two different strategies in ZO optimization: \emph{single-particle}
methods and \emph{multi-particle} methods.
The advantage of multi-particle methods~\cite{CARRILLO-ETAL_CONSENSUS, FORNASIER-ETAL_CONSENSUS, HUANG-ETAL_PSO} is its population-based adaptive and evolutionary
solution-search capability while the disadvantage is their difficulty to derive
theoretical guarantee of convergence. Hence, we will also investigate the
single-particle methods, which are theoretically more tenable and can fully
utilize the advanced FO %optimization
algorithms once the gradient estimation is done at each iteration. 
In particular, we plan to investigate the so-called \emph{ZO proximal methods}~\cite{POUGKAKIOTIS-KALOGERIAS_ZO-PRIMAL, KAZEMI-WANG_ZO-PRIMAL} because these
can handle the constraints consisting of a nonconvex and smooth function and
a nonsmooth and convex function like our Eq.~\eqref{eqn:opt}.
Our preliminary numerical experiments using the \texttt{PRIMA.jl}~\cite{PRIMA}
is quite promising: it generated similar results as those in Section~\ref{sec:ex}
with an order of magnitude faster than the DE algorithm.

Furthermore, we will investigate:
%1) semi-automatic determination of the optimal values of $\mu$ and $\nu$ in
1) automatic determination of the optimal values of $\mu$ and $\nu$ in
Eq.~\eqref{eqn:opt};
2) other effective constraints beyond sparsity and smoothness;
%measured by $\ell_1$-norm of the signal and $\ell_2$-norm of its gradient;
and
%3) the influence of the setup of the wavelet filters (e.g., type of wavelets,
3) the influence of the wavelet filter parameters (e.g., type of wavelets,
number of filters, their frequency overlaps, etc.). 
% used in the ST to the estimated class-revealing features.

Finally, we note that our optimization strategy Eq.~\eqref{eqn:opt} should work in principle with any other learning model as long as it outputs the class probability $p_k(\bx)$ for a given input signal $\bx \in \Rf^\ndim$, e.g.,
a DL model equipped with ``softmax'' output units~\cite[Sec.\ 6.2.2.3]{Goodfellow-et-al-2016}.

\section*{Acknowledgment}
This research was partially supported by the US National Science Foundation grants DMS-1912747 and CCF-1934568 as well as the US Office of Naval Research grant N00014-20-1-2381.

\newpage
\begin{thebibliography}{10}

\bibitem{MALLAT-SCAT}
S.~Mallat,
\newblock ``Group invariant scattering,''
\newblock {\em Comm. Pure Appl. Math.}, vol. 65, no. 10, pp. 1331--1398, 2012.

\bibitem{BRUNA-MALLAT}
J.~Bruna and S.~Mallat,
\newblock ``Invariant scattering convolution networks,''
\newblock {\em IEEE Trans. Pattern Anal. Machine Intell.}, vol. 35, no. 8, pp.
  1872--1886, 2013.

\bibitem{LECUN-BENGIO-HINTON}
Y.~LeCun, Y.~Bengio, and G.~Hinton,
\newblock ``Deep learning,''
\newblock {\em Nature}, vol. 521, no. 7553, pp. 436--444, 2015.

\bibitem{Goodfellow-et-al-2016}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville,
\newblock {\em Deep Learning},
\newblock MIT Press, 2016,
\newblock \url{http://www.deeplearningbook.org}.

\bibitem{BRUNA-MALLAT-CVPR11}
J.~Bruna and S.~Mallat,
\newblock ``Classification with scattering operators,''
\newblock in {\em Proc. IEEE Conf. on Computer Vision and Pattern Recognition
  (CVPR)}, 2011, pp. 1561--1566.

\bibitem{ANDEN-MALLAT}
J.~And\`{e}n and S.~Mallat,
\newblock ``Deep scattering spectrum,''
\newblock {\em IEEE Trans. Signal Process.}, vol. 62, no. 16, pp. 4114--4128,
  2014.

\bibitem{SAITO-WEBER-SPIE}
N.~Saito and D.~S. Weber,
\newblock ``Underwater object classification using scattering transform of
  sonar signals,''
\newblock in {\em Wavelets and Sparsity XVII, Proc. SPIE 10394}, Y.~M. Lu,
  D.~{Van De Ville}, and M.~Papadakis, Eds., 2017,
\newblock Paper \# 103940K.

\bibitem{WEBER-PHD}
David~S. Weber,
\newblock {\em On Interpreting Sonar Waveforms via the Scattering Transform},
\newblock {PhD} dissertation, Appl. Math., Univ. California, Davis, Dec. 2021.

\bibitem{CHAK-SAITO-MWSN}
Wai~Ho Chak and Naoki Saito,
\newblock ``Monogenic wavelet scattering network for texture image
  classification,''
\newblock {\em JSIAM Letters}, vol. 15, pp. 21--24, 2023.

\bibitem{CHAK-SAITO-WEBER}
Wai~Ho Chak, Naoki Saito, and David Weber,
\newblock ``The scattering transform network with generalized {M}orse wavelets
  and its application to music genre classification,''
\newblock in {\em Proc. 2022 International Conference on Wavelet Analysis and
  Pattern Recognition (ICWAPR)}, Toyama, Japan, 2022, pp. 25--30.

\bibitem{HASTIE-TIB-WAINWRIGHT}
Trevor Hastie, Robert Tibshirani, and Martin Wainwright,
\newblock {\em Statistical Learning with Sparsity: The Lasso and
  Generalizations}, vol. 143 of {\em Monographs on Statistics and Applied
  Probability},
\newblock CRC Press, Boca Raton, FL, 2015.

\bibitem{KYMATIO}
Mathieu Andreux et~al.,
\newblock ``Kymatio: Scattering transforms in python,''
\newblock {\em Journal of Machine Learning Research}, vol. 21, no. 60, pp.
  1--6, 2020.

\bibitem{CONN-ETAL_ZO-BOOK}
Andrew~R. Conn, Katya Scheinberg, and Luis~N. Vicente,
\newblock {\em Introduction to Derivative-Free Optimization},
\newblock MPS-SIAM Series on Optimization. SIAM, 2009.

\bibitem{LARSON-ETAL_ZO}
Jeffrey Larson, Matt Menickelly, and Stefan~M. Wild,
\newblock ``Derivative-free optimization methods,''
\newblock {\em Acta Numerica}, vol. 28, pp. 287--404, 2019.

\bibitem{LIU-ETAL_ZO}
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred~O. Hero~III,
  and Pramod~K. Varshney,
\newblock ``A primer on zeroth-order optimization in signal processing and
  machine learning: Principals, recent advances, and applications,''
\newblock {\em IEEE Signal Processing Magazine}, vol. 37, no. 5, pp. 43--54,
  2020.

\bibitem{AHMAD-etal_DE-Rev}
Mohamad~Faiz Ahmad, Nor Ashidi~Mat Isa, Wei~Hong Lim, and Koon~Meng Ang,
\newblock ``Differential evolution: {A} recent review based on state-of-the-art
  works,''
\newblock {\em Alexandria Eng. J.}, vol. 61, no. 5, pp. 3831--3872, 2022.

\bibitem{BILAL-etal_DE-Rev}
Bilal, Millie Pant, Hira Zaheer, Laura-Hernandez Garcia, and Ajith Abraham,
\newblock ``Differential {E}volution: {A} review of more than two decades of
  research,''
\newblock {\em Eng. Appl. Artif. Intell.}, vol. 90, pp. Article \#103479, 2020.

\bibitem{OPARA-ARABAS_DE-Rev}
Karol~R. Opara and Jaros≈Çaw Arabas,
\newblock ``Differential {E}volution: {A} survey of theoretical analyses,''
\newblock {\em Swarm Evol. Comput.}, vol. 44, pp. 546--558, 2019.

\bibitem{PRESS}
W.~H. Press,
\newblock ``Flicker noises in astronomy and elsewhere,''
\newblock {\em Comments on Modern Physics, Part C - Comments on Astrophysics},
  vol. 7, no. 4, pp. 103--119, 1978.

\bibitem{SAITO-COIF-JMIV}
N.~Saito and R.~R. Coifman,
\newblock ``Local discriminant bases and their applications,''
\newblock {\em J. Math. Imaging Vis.}, vol. 5, no. 4, pp. 337--358, 1995,
\newblock Invited paper.

\bibitem{SAITO-PHD}
N.~Saito,
\newblock ``Local feature extraction and its applications using a library of
  bases,''
\newblock in {\em Topics in Analysis and Its Applications: Selected Theses},
  R.~Coifman, Ed., pp. 269--451. World Scientific Pub. Co., Singapore, 2000.

\bibitem{GEURTS}
Pierre Geurts,
\newblock ``Pattern extraction for time series classification,''
\newblock in {\em Principles of Data Mining and Knowledge Discovery (PKDD
  2001)}, Luc De~Raedt and Arno Siebes, Eds., 2001, vol. 2168 of {\em Lecture
  Notes in Computer Science}, pp. 115--127.

\bibitem{KEOGH-KASETTY}
Eamonn Keogh and Shruti Kasetty,
\newblock ``On the need for time series data mining benchmarks: A survey and
  empirical demonstration,''
\newblock {\em Data Mining and Knowledge Discovery}, vol. 7, pp. 349--371,
  2003.

\bibitem{JULIA}
J.~Bezanson, A.~Edelman, S.~Karpinski, and V.~B. Shah,
\newblock ``Julia: {A} fresh approach to numerical computing,''
\newblock {\em SIAM Review}, vol. 59, no. 1, pp. 65--98, 2017.

\bibitem{ScatteringTransform}
David Weber and Naoki Saito,
\newblock ``Scattering{T}ransform.jl,''
  \url{https://github.com/dsweber2/ScatteringTransform.jl}, 2022--25.

\bibitem{GLMNet}
Julia Statistics,
\newblock ``G{LMNet}.jl,'' \url{https://github.com/JuliaStats/GLMNet.jl},
  2015--25.

\bibitem{BBO}
Robert Feldt,
\newblock ``Black{B}ox{O}ptim.jl,''
  \url{https://github.com/robertfeldt/BlackBoxOptim.jl}, 2015--25.

\bibitem{CART}
L.~Breiman, J.~H. Friedman, R.~A. Olshen, and C.~J. Stone,
\newblock {\em {Classification and Regression Trees}},
\newblock Chapman \& Hall, Inc., New York, 1993,
\newblock previously published by Wadsworth \& Brooks/Cole in 1984.

\bibitem{CARRILLO-ETAL_CONSENSUS}
Jos\'e~A. Carrillo, Shi Jin, and Yuhua Zhu,
\newblock ``A consensus-based global optimization method for high dimensional
  machine learning problems,''
\newblock {\em ESAIM: Control, Optimisation and Calculus of Variations}, vol.
  27, pp. \# S5, 2021.

\bibitem{FORNASIER-ETAL_CONSENSUS}
Massimo Fornasier, Lorenzo Pareschi, Hui Huang, and Philippe S\"{u}nnen,
\newblock ``Consensus-based optimization on the sphere: {C}onvergence to global
  minimizers and machine learning,''
\newblock {\em Jour. Mach. Learn. Res.}, vol. 22, no. 237, pp. 1--55, 2021.

\bibitem{HUANG-ETAL_PSO}
Hui Huang, Jinniao Qiu, and Konstantin Riedl,
\newblock ``On the global convergence of particle swarm optimization methods,''
\newblock {\em Appl. Math. Optim.}, vol. 88, pp. Article \#30, 2023.

\bibitem{POUGKAKIOTIS-KALOGERIAS_ZO-PRIMAL}
Spyridon Pougkakiotis and Dionysis Kalogerias,
\newblock ``A zeroth-order proximal stochastic gradient method for weakly
  convex stochastic optimization,''
\newblock {\em SIAM J. Sci. Comput.}, vol. 45, no. 5, pp. A2679--A2702, 2023.

\bibitem{KAZEMI-WANG_ZO-PRIMAL}
E.~Kazemi and L.~Wang,
\newblock ``Efficient zeroth-order proximal stochastic method for nonconvex
  nonsmooth black-box problems,''
\newblock {\em Mach. Learn.}, vol. 113, pp. 97--120, 2024.

\bibitem{PRIMA}
Zaikun Zhang and Alexis Montoison,
\newblock ``{PRIMA.jl},'' \url{https://github.com/libprima/PRIMA.jl}, 2023--25.

\end{thebibliography}

\end{document}
