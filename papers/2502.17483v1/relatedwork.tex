\section{Related Work}
% WiFi-based HAR utilizes either the received signal strength indication (RSSI) or channel state information (CSI) of WiFi. Abdelnasser et al. \cite{abdelnasser2015wigest} and Gu et al. \cite{gu2014wifi} use RSSI for gesture and activity recognition.
% However, the effectiveness of RSSI is constrained by multipath and fading effects.
% In contrast, CSI can capture the characteristics of multiple subcarriers, including phase and amplitude information, thus enabling a more comprehensive understanding of the signal propagation and changes. In addition, CSI is less susceptible to multipath and fading effects.
% Wang et al. \cite{wang2019can} employed a 1D CNN model for indoor positioning, while Moshiri et al. \cite{moshiri2021csi} and Chahoushi et al. \cite{chahoushi2023csi} used CNNs for activity classification. 
% Chen et al. \cite{chen2018wifi} and Shang et al. \cite{shang2021lstm} found that LSTM networks don't perform well when dealing with bidirectional sequential data.
% To address this, Yadav et al. \cite{yadav2022csitime} introduced the InceptionTime structure, and Li et al. \cite{li2021two} implemented Transformer architecture to enhance temporal data analysis.

Static DL models for WiFi cannot adapt to the dynamic environment where new activities are constantly introduced.
In computer vision, some works proposed class incremental learning to solve this aspect.
Class-incremental learning (CIL) is classified as three categories: 1) rehearsal-based method, which preserves knowledge by replaying exemplars from past tasks \cite{rebuffi2017icarl, hou2019learning, wu2019large}; 2) regularization-based method, which uses penalties to maintain critical parameters but struggles with lengthy task sequences \cite{kirkpatrick2017overcoming, yang2021cost, saha2021gradient}, and 3) dynamic architecture-based method, which expands the model for new tasks but can be resource-intensive \cite{rusu2016progressive, verma2021efficient, douillard2022dytox}. 
Many existing CIL methods rely on storing exemplars from previous tasks to address catastrophic forgetting. However, the limited storage and computational power of commonly used edge devices, combined with privacy concerns around retaining user data, restrict the applicability of these methods in real-world scenarios.

Some works propose exemplar-free CIL (EFCIL) to avoid the need to retain exemplars.
Li \textit{et al.} \cite{li2017learning} introduced knowledge distillation (KD) for CIL.
However, KD has limited effects when only new data is used.
Gao \textit{et al.} \cite{gao2022r} proposed a new framework that separates representation and classifier learning, thus improving model inversion to synthesize data for previous tasks.
Asadi \textit{et al.} \cite{asadi2023prototype} introduced prototype-sample relation distillation by combining supervised contrastive loss \cite{khosla2020supervised}, self-supervised learning \cite{liu2021self}, and class prototype evolution techniques \cite{de2021continual}. 
By jointly learning representations and class prototypes, they effectively reduce catastrophic forgetting and maintain the relevance as well as the embedding similarity of old class prototypes.
The above methods mainly used ResNet and other convolutional neural network (CNN)-based models.
In general, the two dimensions of WiFi (time stamps and channel state) are fundamentally different from the two dimensions of images that contains spatial information.
Using a two-dimensional kernel to extract spatial patterns from WiFi data would result in poor feature extraction.


Although many CNN and ResNet-based approaches have been developed for EFCIL, transformer-based EFCIL remains a relatively unexplored area.
Roy \textit{et al.} \cite{roy2023exemplar} adapted the transformer's MHSA layers with convolution operations for new tasks. However, their proposed method is unsuitable for WiFi data, as it relies on image augmentation, whereas WiFi augmentation strategies involve temporal delays and frequency shifts, leading to performance degradation.
Zhang \textit{et al.} \cite{zhang2023csi} and Ding \textit{et al.} \cite{ding2023passive} investigated WiFi-based HAR using incremental learning. Zhang \textit{et al.} employed retained exemplars and distillation loss to preserve activity knowledge, while Ding \textit{et al.} introduced an enhancement CNN with attention and dual-loss functions. However, Ding's approach processes only one category at a time, limiting its applicability.
%Existing methods like \cite{roy2023exemplar} are limited by their reliance on visual enhancements unsuitable for WiFi's subtle changes. Additionally, systems like Zhang et al.'s and Ding et al.'s face restrictions due to their storage requirements and scalability when handling multiple categories. 
To overcome the limitations of the above methods, we propose a new model specifically designed for WiFi-based HAR, which can more effectively adapt to the dynamic changes in WiFi data while reducing the need for data storage.
% Roy et al. \cite{roy2023exemplar} proposed a framework that  dynamically adjusts the weights in the multi-head self-attention (MHSA) layers of Transformer through convolution operations to  accommodate new tasks. 
% During the inference phase, they proposed an entropy-based task prediction method which generates multiple enhanced views of the input image and calculates the average prediction entropy of these views to determine the task id. 
% This framework is unsuitable for WiFi, as variations, such as temporal delays and frequency shifts, are not visually intuitive. 
% A few studies have explored class incremental learning for WiFi-based HAR.
% Zhang et al. \cite{zhang2023csi} proposed an incremental learning system for WiFi and designed a cross-scenario HAR. This system employs a subset of retained representative exemplar and distillation loss to enhance the model's memory of learned activities. 
% Ding et al. \cite{ding2023passive} proposed WiCI-HAR, 
% a class incremental learning model for WiFi-based HAR, 
% which features an amplitude-phase enhancement CNN.
% This system integrates attention mechanisms and a dual-loss function. 
% However, it processes only one category at a time, 
% thereby limiting its learning efficiency and potential for broader applications.

\begin{figure*}[t]
\centering
\includegraphics[width=0.86\textwidth]{model.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\vspace{-0.25cm} %调整图片与上文的垂直距离
\caption{Architecture of ConSense. Left part contains the framework. Right part details how the model dynamically expands and selectively retrains during continual learning from training session $1$ to training session $t$.
As new tasks are introduced, the model dynamically expands with new prefixes in the MHSA layer.
% , which are specifically trained for new task data. 
In the MLP, a selective retraining strategy is implemented to adjust neuron weights, preserving learned outcomes from stable neurons while updating unstable neurons to accommodate new tasks.}
\label{fig_architecture}
\vspace{-0.45cm} %调整图片与上文的垂直距离
\end{figure*}

\vspace{-0.45cm} %