% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{xcolor}
% \usepackage{algorithmicx}
\usepackage{subfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{Leveraging band diversity for feature selection in EO data}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Sadia Hussain\inst{1}\orcidID{0000-0002-5507-1063} \and
Brejesh Lall\inst{2}\orcidID{0000-0003-2677-3071}}
%
\authorrunning{S. Hussain et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Bharti School of Telecommunication Technology and Management , IIT Delhi, Hauz Khas, New Delhi, India 
\email{bsz198107@dbst.iitd.ac.in}\\ \and
Electrical engineering department, IIT Delhi, Hauz Khas, New Delhi, India\\
\email{brejesh@ee.iitd.ac.in}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% In hyperspectral imaging (HSI) the bands provide information about the type of material and the composition. To handle this rich and dense information, it often becomes difficult to extract this information. As a result of which management of these bands become challenging and essential at the same time. To address the challenge of managing large number of bands, one of the simplest solution is to group bands. Grouping techniques based on correlations, band averaging, spectral response function have already been developed. However, these techniques address the classification problem wherein each representative band is selected from a group of adjacent bands which show high spectral similarity. These representative bands only consider essential features or attributes to map the essential detail for classifying the object. However, this technique can neglect the information pertaining to detail and texture in reconstruction problem. In reconstruction problem, such as super-resolution, each pixel contributes to a particular detail and constitutes texture. To overcome this limitation, we introduce leveraging information from uncorrelated bands to strengthen the interpolation mapping. Our proposed method, incorporates information from uncorrelated bands which are not categorised as \textit{representative bands}. Our study proves that these bands prove helpful in feature learning in reconstruction problems and should not be entirely discarded. These uncorrelated bands offer interesting details to the output which helps in making the interpolation learning stronger with better generalisation.
Hyperspectral imaging (HSI) is a powerful earth observation technology that captures and processes information across a wide spectrum of wavelengths. Hyperspectral imaging provides comprehensive and detailed spectral data that is invaluable for a wide range of reconstruction problems. However due to complexity in analysis it often becomes difficult to handle this data. To address the challenge of handling large number of bands in reconstructing high quality HSI, we propose to form groups of bands. In this position paper we propose a method of selecting diverse bands using determinantal point processes in correlated bands. To address the issue of overlapping bands that may arise from grouping, we use spectral angle mapper analysis. This analysis can be fed to any Machine learning model to enable detailed analysis and monitoring with high precision and accuracy. 

\keywords{Earth Observation  \and Hyperspectral Super-resolution \and Machine Learning.}
\end{abstract}


\section{Introduction}
A hyperspectral imaging device collects spectral information using hundreds of narrow bands and combines it with digital imagery. When an object interacts with light at different wavelengths, this technology captures the unique physical and chemical characteristics of the material. As a result, hyperspectral imaging finds numerous applications in earth observation, including precision agriculture, climate monitoring, and remote sensing. However, this technology comes with several challenges. With information spread across a wide range of narrow bands, similar yet distinct objects can be mistakenly categorized as the same. Additionally, there is a bottleneck in transmitting these images when captured by a sensor, which often necessitates processing these extensive bands. This processing may degrade either the spatial resolution, the spectral resolution, or both. The large size of hyperspectral imaging (HSI) data introduces multiple processing issues, such as increased computational cost, complexity in image analysis, and a scarcity of available training data. Collecting large datasets can be difficult due to limitations of acquisition devices, and storing this vast amount of information can also be cumbersome. Consequently, in restoration-based problems, machine learning or computer vision-based solutions often prove inadequate at these early stages due to the need to process a large number of bands. In hyperspectral images, the high dimensionality results from the large number of bands. Effective hyperspectral band management is crucial for revealing the unique features of objects. Consequently, two main approaches are commonly found in the literature: band selection (or feature selection) and band extraction (or feature extraction). Band selection involves using significant or representative bands. Based on the physical and chemical characteristics of the material, these representative bands are selected to compactly represent hyperspectral images. On the other hand, band extraction reduces the higher dimensionality of hyperspectral bands into a lower dimensionality, which can cause hyperspectral images to lose their physical significance. 
In this position paper, we devise a unified band selection approach for controlling hyperspectral bands by applying a grouping strategy. This grouping strategy enhances the performance of any machine learning-based method used for resolution enhancement. Our paper outlines three important areas of grouping diversity by employing: (1) Sampling Method: We use Determinantal Point Processes to provide insights for the selection of diverse bands. (2) Spectral Correlations: We group strongly correlated bands together based on their spectral characteristics. (3) Spectral Angle Mapping analysis: For overlapping bands within a group, we use the spectral angle mapper to disentangle bands based on more precise similarity measurements.

This unified approach aims to optimize the use of hyperspectral bands, improving the accuracy and efficiency of hyperspectral image analysis in various applications.
\begin{figure}
% \includegraphics[width=\textwidth, height=8cm, keepaspectratio]{Maclean.png}
\includegraphics[width=\textwidth]{Maclean.png}
  \caption{Schematic view of the proposed approach. }
  \label{fig:FIG2}
\end{figure}
\section{Related Works}
A lot of literature is found in Image classification using the above two approaches. In band selection methods further three sub-categories have been devised based on the derivation of subset of bands~\cite{sawant2020survey}. These are as follows: subsets derived on the basis of subset evaluation criteria~\cite{chang1999joint,tschannerl2019mimr,bhardwaj2018unsupervised,keshava2004distance,zhang2018fast}, availability of prior information further sub-categorized on the basis of supervised selection criteria~\cite{guo2014improving,yang2010efficient}, unsupervised selection criteria~\cite{sawant2020unsupervised,jia2012unsupervised} and selection strategy (individual~\cite{datta2015combination,jia2015novel} and other evaluation techniques) used to create the band subset.

Efficient band selection plays a crucial role in extracting meaningful information from vast datasets. By selecting a subset of relevant spectral bands, researchers can reduce data dimensionality, enhance computational efficiency, and improve the performance of downstream analysis tasks such as restoration tasks, classification and target detection. However, achieving optimal band selection poses significant challenges, necessitating innovative approaches that leverage spectral grouping techniques.~\cite{9153939},~\cite{10036355} address this need with novel methodologies. The former introduces a method based on neighborhood grouping to efficiently identify relevant bands, while the latter proposes leveraging differences between inter-groups for band selection.~\cite{ayna2023learning} presents a learning-based optimization approach for band selection tailored specifically for classification tasks. Despite their innovative approaches, both approaches face challenges interms of computational efficiency, interpretability, and scalability. Furthermore, in the context of hyperspectral image restoration, particularly superresolution, no definitive solutions have emerged regarding grouping methodologies. This underscores a critical gap in current research highlighting the need for further exploration and development in this area. Addressing these challenges is crucial for an efficient grouping algorithm, advancing hyperspectral imaging capabilities and realizing the full potential of band selection techniques in various applications.
\begin{figure}[h]
    \centering
    \subfloat{\includegraphics[width=0.20\textwidth]{ntire_corr.png}} 
    \subfloat{\includegraphics[width=0.20\textwidth]{cave_corr.png}} 
    \subfloat{\includegraphics[width=0.20\textwidth]{chikusie_50dpi.png}}
    \subfloat{\includegraphics[width=0.20\textwidth]{landsat_corr.png}}
    \subfloat{\includegraphics[width=0.20\textwidth]{sentinel_corr.png}}
    \caption{(a) Correlation matrix for NTIRE2022 Dataset (b) CAVE Dataset (c) Chikusei Dataset (d) Sentinel Dataset (e) Landsat Dataset}
    \label{fig:foobar}
\end{figure}
Recent advancements in hyperspectral image analysis focus on correlation matrices, which reveal spectral dependencies and enable high-texture detail within groups of spectrally dependent vectors. Traditionally, linear predictions~\cite{manolakis2008spectral} have dominated hyperspectral data analysis, but recent studies advocate for interval sampling~\cite{wang2022group} for superior group formation and enhanced interpolation outcomes. Our study proposes an explicit grouping method based on correlation coefficients to establish a standardized framework for hyperspectral super-resolution. By integrating correlation analysis into our grouping strategy, each band within a correlated group is rigorously evaluated for its significance in super-resolution. These explicit groups, guided by correlation analysis, feed into deep neural network architectures, optimizing interpolation learning efficiency. Our innovation lies in combining Determinantal Point Process (DPP) with correlation analysis to form coherent band subsets, minimizing redundancy and maximizing diversity. This integration enriches interpolation learning by capturing intricate spectral relationships effectively. Challenges arise when grouped bands still overlap significantly despite spectral correlation. To mitigate this, our approach employs Spectral Angle Mapper (SAM) to resolve overlaps based on the lowest SAM values, refining the grouping strategy. This modular integration enhances cohesion, learning robustness, and efficiency in handling complex hyperspectral data, promoting seamless interaction within the network and improving restoration outcomes.
\begin{algorithm}
\caption{Spectral correlation estimation}
\label{algo: Algorithm1}
\begin{algorithmic}[1]
\State \textbf{Input:} \(X\), \(S\)
\State \textbf{Output:} \(Z\)
\State \(Z \in \mathbb{R}^{N \times WH}\)
\State \(X \in \mathbb{R}^{N \times wh}\)
\State Initialize \(B \in \mathbb{R}^{N \times n}\)
\State Initialize \(M \in \mathbb{R}^{n \times WH}\)
\State \(X = ZS\)
\For{\textbf{each band} \(i, j\)}
    \State \(b_i = f_i(t)\)
    \State \(b_j = f_j(t)\)
    \State \(R_{b,b}(i, j) = \frac{E[b_i, b_j]}{\sigma_i \sigma_j}\)
\EndFor
\State \(Z = B \cdot M\)
\State Solve for \(B\) and \(M\)
\end{algorithmic}
\end{algorithm}
\section{Methodology}
\begin{algorithm}
\caption{k-DPP Sampling}
\label{algo: Algorithm 2}
\begin{algorithmic}[1]
\State \textbf{Input:} Hyperspectral data $L$
\State \textbf{Output:} Subset of bands $Y$ of size $k$
% \State Compute spectral correlation functions for hyperspectral data
% \State Identify patterns of spectral similarity or dissimilarity
\State Initialize \( k \)-DPP for sampling band subsets
\State Set cardinality constraint $k$
\State Compute the probability $P^k_L(Y)$ of selecting a subset $Y$ of size $k$ from $L$ using k-DPP:
\[
    P_{L} (\mathbf{Y} = Y ) = \frac{\det(L_{Y})}{\sum_{\mathbf{Y} \subseteq \gamma} \det(L_Y)} = \frac{\det(L_{Y})}{\det(L + I_{N})}
\]
\State Decompose $L_{Y}$ into eigenvectors $S$ and eigenvalues $\lambda_{n}$
\State Select subset of eigenvectors based on eigenvalues:
\[
    P(n \in S) = \frac{\lambda_{n}}{e_{k}^{n}} \sum_{|S'| = k-1} \prod_{n' \in S'} \lambda_{n'} = \lambda_{n} \frac{e_{k-1}^{n-1}}{e_{k}^{n}}
\]
\State Ensure selected subset represents the most relevant and informative components
\State \textbf{return} Subset of bands $Y$ of size $k$
\end{algorithmic}
\end{algorithm}
This section elaborates our proposed approach for Band grouping. Our method comprises of three primary components for grouping bands in a data: extracting primary grouping information based on correlation analysis, extracting critical band information using DPP, solving for overlapping bands using spectral angle mapper information. The technical specifics of these components are elucidated in the following discussion.
\subsection{Spectral Correlation in HSI}
Correlation functions in hyperspectral imaging provide valuable insights into the relationships between spectral bands and spatial locations in hyperspectral data. These correlation functions can help understand how the spectral characteristics of different bands are related and can be used for various purposes, including feature selection, dimensionality reduction, and image interpretation. 
As illustrated in Algorithm~\ref{algo: Algorithm1}, we denote the high-resolution hyperspectral image (HR-HSI) data as \( Z \in \mathbb{R}^{W \times H \times N} \) and the low-resolution counterpart (LR-HSI) as \( X \in \mathbb{R}^{w \times h \times N} \). The LR-HSI \( X \) is obtained from \( Z \) through spatial downsampling using matrix \( S \) such that \( X = ZS \), where \( S \in \mathbb{R}^{WH \times wh} \). To recover \( Z \), which captures the spatial and spectral details of the original HR-HSI, we propose a method involving spectral basis \( B \in \mathbb{R}^{N \times n} \) and a correlation matrix \( M \in \mathbb{R}^{n \times WH} \): \( Z = BM \). Here, \( B \) represents spectral basis vectors, and \( M \) incorporates correlation coefficients. This formulation aims to find optimal values for \( B \) and \( M \) to reconstruct \( Z \). Understanding the spatial and spectral correlations inherent in hyperspectral images is crucial. Bands that are closer together often exhibit similar patterns due to underlying scene properties. This correlation structure is assessed through auto-correlation measures \( R_{b,b}(i, j) \) in Algorithm 1.

As illustrated in Algorithm 1, each element in \( R_{b,b}(i, j) \) represents the correlation between band \( i \) and band \( j \). This analysis aids in identifying which bands carry similar information essential for feature selection. A higher value indicates that these bands change in a similar manner across pixels, suggesting they may capture similar spectral information. Conversely, a lower value or near-zero covariance indicates that the bands change independently.
%%%%%%%%%%%%%%%%%%%%%%% Commented for MACLEAN %%%%%%%%%%%%%%%%%%%%%%
% Correlation functions can be defined and used in the context of hyperspectral data as \textbf{(i) Spectral Correlation Function}: In hyperspectral imaging, each pixel in the image is represented by a spectrum, which consists of intensity values across multiple spectral bands. The spectral correlation function measures the correlation between the intensity values of different spectral bands for a given pixel or group of pixels. This correlation function can be used to identify spectral bands that are highly correlated, indicating redundancy in information, or bands that are uncorrelated or negatively correlated, indicating complementary information. Understanding spectral correlations can help in feature selection by identifying the most informative spectral bands for classification or analysis tasks. In the context of super-resolution, spectral correlations are crucial as they help identify redundancy or complementary information across spectral bands.
% By analyzing spectral correlations, one can select the most informative spectral bands for super-resolution reconstruction.\textbf{(ii) Spatial-Spectral Correlation Function}: In addition to spectral correlations, the spatial-spectral correlation function considers both spectral and spatial relationships in hyperspectral data. It measures the correlation between spectral bands at different spatial locations within the image. This correlation function can reveal spatial patterns of spectral similarity or variability across the image. Spatial-spectral correlations can be used for tasks such as image segmentation, anomaly detection, and change detection, where both spectral and spatial information are important. \textbf{(iii) Cross-Correlation Function}: The cross-correlation function measures the correlation between different spectral bands across the entire image. It provides a global measure of similarity or dissimilarity between spectral bands. Cross-correlation can be used to identify bands that are strongly correlated across the image, indicating consistent spectral patterns, or bands that are weakly correlated, indicating diverse spectral information. This function is useful for identifying redundant or complementary spectral bands and for assessing the overall similarity structure of hyperspectral data.

% Overall, correlation functions in hyperspectral imaging provide valuable insights into the relationships between spectral bands and spatial locations in hyperspectral data. By analyzing these correlation functions, researchers can better understand the structure of hyperspectral images and extract useful information for various applications in remote sensing, environmental monitoring, agriculture, and other fields.
% We represent data HR-HSI, LR-HSI using $Z$ and $X$ respectively where $Z \in R^{W \times H \times N}$ and $X \in R^{w \times h \times N}$. Converting the data into a tensor to be represented as matrices in the form of $Z \in R^{N \times W H}$ and similarly for $X \in R^{N \times w h}$. The input to this super-resolution problem will is a downsampled HR-HSI which is given as follows:
% \begin{equation}
%     X = ZS
% \end{equation}
% where $Z$ is downsampled by a spatial downsampling operation, S, such that $S \in R^{W H \times w h}$. In hyperspectral images, the bands which are adjacent show high correlation~\cite{bioucas2008hyperspectral} and is visually depicted in Figure~\ref{fig:foobar}. Therefore, to solve for $Z$, its assumed that the correlation matrix can be the low-cost solution as $Z$ can be estimated using a set of spectral basis with the correlation matrix coefficients as:
% \begin{equation}
%     Z = B^{N \times n} M^{n \times W H}
% \end{equation}
% Where $B \in R^{N \times n}$ is the spectral basis and M represents the correlation matrix such that $M \in R^{n \times WH}$. Therefore, the problem can be solved by finding values for $B$ and $M$. We consider solving $M$ as grouping strategy where number of $n$ will effectively determine the number of groups of bands to be dealt with.

% To understand the underlying structure of hyperspectral images and emergence of grouping strategy, we map the structures and patterns in HSI. This mapping suggests that bands which are closer to each other exhibit similar patterns as compared to the bands which are far away from each other. We further analyse this using correlation structure of images by depicting correlation structure of five different datasets considered (two synthetic datasets and three remote sensing datasets) in ~\ref{fig:foobar}.
% Correlated values in a matrix are based on the relationship between different bands. We consider $i^{th}$ and $j^{th}$ band in a hyperspectral image as bands estimated based on reflectance or properties of a scene, then we define a relationship such that 
% \begin{equation}
%     b_{i} = f_{i}(t)
% \end{equation}
% where t denotes the relationship that a band has with the material, scene, temperature etc. Similarly $b_{j} = f_{j}(t)$ will depict the relationship with some other $j_{th}$ band in the same $t$ scene. We define this using auto-correlation using random processes, where random processes capture the essence of band location. 
% \begin{equation}
%     R_{b,b}(i,j) = \frac{E[b_{i},b_{j}]}{\sigma_{i}\sigma_{j}}
% \end{equation}
% Here, $\sigma_{i}$, $\sigma_{j}$ are the standard deviations related to $b_{i}$, $b_{j}$.  We say the bands are correlated with each other if $b_{i}$ and $b_{j}$ can have some non-linear relationship. When bands $b_{i}$ and $b_{j}$ are closer to each other, the value of autocorrelation is positive. We consider the estimation of this relationship using a neural network by introducing some of its architectures which also include non-linear activation function. Correlations in hyperspectral imagery has been assessed in classification vigorously. The use of correlation for classification leveraging representative bands based on ranking, clustering, searching and embedded based methods have been extensively used~\cite{aymanbands, wang2019hyperspectral, xuejun2012spectral, manolakis2008spectral}. However, these learnings cannot be directly applied to interpolation. A representative band cannot offer information regarding other correlated pixels, while as in interpolation technique, more the number of correlated bands, more details to produce preferable high resolution. In an interpolation problem,~\cite{wang2022group} paper suggests that a band pertaining to a different group learns detail information which increases super-resolution ability of the network from other group which are uncorrelated. However we draw following conclusions from this learning, (1) an interpolation method cannot generalize well when information is chosen from uncorrelated bands, this can increase detail but will lose the context of the features to be reconstructed which might be irrelevant. (2) this also leads to wasted computation and too much structure in the output which leads to unnecessary artifacts in the generated output image.
\subsection{Determinantal Point Processes}
Our proposed method extends the original k-DPP approach~\cite{kulesza2011k,kulesza2012determinantal} for sampling diverse band subsets from hyperspectral data, represented by the correlation matrix $L$ as illustrated in Algorithm~\ref{algo: Algorithm 2}. It ensures a subset of size $k$ with diverse bands via the probability $P^{k}_{L}(Y)$, which conditions on subsets' diversity captured by $L_{Y}$ and the eigenvalues $\lambda_{n}$, ensuring relevance and diversity in the selected subsets. Eigenvectors $S$ are selected based on eigenvalues $\lambda_{n}$ to capture dataset variability effectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% This section is commented for Maclean %%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Spectral correlation functions in point processes offer insights into the spectral dependencies among points in hyperspectral data, helping to characterize the band structure of the underlying stochastic process. By analyzing these correlations, researchers can gain valuable information about spectral relationships, spatial structures, and material distributions within hyperspectral images. Spectral correlation functions quantify the statistical relationships between spectral bands at different points within the hyperspectral image. By analyzing spectral correlation functions, researchers can identify patterns of spectral similarity or dissimilarity among points. Strong spectral correlations indicate that points tend to have similar spectral signatures, suggesting spatial regions with consistent spectral characteristics. Weak or negative spectral correlations suggest variability or diversity in spectral signatures among points, indicating spatial regions with heterogeneous spectral properties.
% \subsubsection{Sampling Key bands as Eigenvector subset}
% The Corr-DPP method extends the original DPP approach to handle sampling from correlation matrix and combines this information with the grouping information to select the final band subsets. This algorithm serves as modification to the original k-DPP method~\cite{kulesza2011k} for sampling band subsets from hyperspectral data. The k-DPP will ensure that the final band subset maintains diversity while respecting the specified cardinality constraint (i.e., the desired resolution increase). By setting the cardinality to \textit{k}, the DPP algorithm ensures that the resulting band subset contains \textit{k} spectral bands that are diverse and representative of the overall spectral variability present in the dataset. These \textit{k} bands are selected based on their mutual diversity, aiming to capture the essential spectral information while minimizing redundancy. Here, the hyperspectral data is represented as a \textit{L}, which captures the correlations between different bands. The k-DPP is a probabilistic model that selects subsets of a fixed size \textit{k} based on their diversity. In ~\cite{kulesza2011k}, $P^{k}_{L}(Y)$ denotes the probability of selecting a subset \textit{Y} of size \textit{k} from the hyperspectral data \textit{L} using the k-DPP. This probability is computed based on the determinant of submatrices of \textit{L}, where $\mid Y' \mid=k$ represents all possible subsets of size \textit{k}. By conditioning the standard DPP on the event that the selected subset \textit{Y} has a cardinality of \textit{k}, the k-DPP ensures that the selected bands exhibit diversity while maintaining a fixed size. The selection process in the DPP algorithm is governed by the probability distribution as given in equation~\ref{eq1}, where \textit{L} is a real, symmetric matrix representing the similarities between elements in the dataset. This distribution ensures that subsets are selected probabilistically based on their diversity, as quantified by the determinant of the submatrix $L_{Y}$, while also considering the overall structure of the dataset captured by $L + I_{N}$ The process of selecting a band subset \textit{Y} is probabilistic and we use sampling lemma from~\cite{kulesza2012determinantal} in the following equation:
% \begin{equation}\label{eq1}
% P_{L} (\textbf{Y} = Y ) = \frac{det(L_{Y})}
% {\sum_{\textbf{Y} \subseteq \gamma} det(L_Y)}
% = \frac{det(L_{Y} )}{
% det(L + I_{N} )}     
% \end{equation}
% The matrix $L_{Y}$ is decomposed into the corresponding eigenvector \textit{S} and eigenvalue $\lambda_{n}$. A subset of eigenvectors capture the underlying structure or variability in the dataset, \textit{S}, will be selected according to their corresponding eigenvalues $\lambda_{n}$,indicate the importance of each eigenvector in representing this variability, where n $\in (1, 2, 3, \hdots , l)$. Eigenvectors with larger eigenvalues are more likely to be selected, reflecting their greater significance in capturing the diversity of the dataset. This step ensures that the selected subset represents the most relevant and informative components of the data. For selecting band subset, the information of \textit{S} is calculated to determine the selected bands. The likelihood of including the $k^{th}$ eigenvector index \textit{n} in the subset \textit{S} based on the eigenvalue $\lambda_{n}$ is given as~\cite{kulesza2012determinantal}
% \begin{equation}
%     P(n \in S) = \frac{\lambda_{n}} {e_{k}^{n}} \sum_{|S'| = k-1} \prod_{n' \in S'} \lambda_{n'} = \lambda_{n} \frac{e_^{n-1}_{k-1}} {e^{n}_{k}}
% \end{equation}
% where $e_{k}^{n}$ represents the $k^{th}$ element of the eigenvector \textit{S}. This distribution ensures that elements are selected probabilistically based on their relevance, as quantified by the eigenvalues $\lambda_{n}$, while also considering the diversity of the selected subset. Thus, the DPP algorithm effectively balances diversity and relevance in the sampled subsets, leading to informative and representative samples for various data analysis tasks.
%%%%%%%%%%%%%%%%%% for eigen representation(commented before Maclean) %%%%%%%%%%%%%%%%%%%%%%%%%%%
% To translate the algorithm into equations for the method, we need to define the mathematical operations involved in each step. Let's break down the algorithm and identify the corresponding equations:
% \begin{enumerate} 

%     \item Compute the correlation matrix C:
%         Each element cijcij of the correlation matrix CC is computed using the formula for correlation coefficient between bands ii and jj.


%    \item Perform eigen decomposition on C:
%         Eigen decomposition of CC yields eigenvalues λλ and eigenvectors νν. This can be represented as C=VΛV−1C=VΛV−1, where VV is the matrix of eigenvectors and ΛΛ is the diagonal matrix of eigenvalues.

%    \item  Initialize an empty set S:
%         No equation needed.

%     \item  Iterate over the eigenvalues λ:
%         If the eigenvalue λiλi​ is above a certain threshold, add the index ii of the corresponding eigenvector to set SS. This step involves comparing eigenvalues to a threshold value.

%     \item  Output the set S:
%         No equation needed.
% \end{enumerate}
% Based on this breakdown, the equations required for the method would include:

%     Equation(s) for computing the correlation matrix CC.
%     Equation(s) for performing eigen decomposition on CC.
%     Equation(s) for selecting eigenvectors based on a threshold.

% The specific number of equations needed depends on the complexity of the computations involved in each step. Typically, you would need a few equations for computing the correlation matrix, several equations for eigen decomposition (depending on the method used), and one or more equations for selecting eigenvectors based on the threshold. Overall, you might need around 5-10 equations to fully describe the method, but this can vary based on implementation details and any additional considerations.


%     Sampling from Multiple Matrices:
%         Unlike the original k-DPP method, which samples band subsets from a single matrix, MIMN-DPP samples from both LBLB​ and LB^LB^​ matrices.
%         This requires decomposing both matrices into their corresponding eigenvectors and eigenvalues.

%     Selection of Eigenvector Subsets:
%         Two subsets of eigenvectors, denoted as SS and S^S^, are selected based on their corresponding eigenvalues {λBn}{λBn​} and {λB^n}{λB^n​}.
%         The selection process involves combining information from both SS and S^S^ to determine the final set of selected bands.

%     Probability of Selecting Eigenvectors:
%         The probability of selecting the kk-th eigenvector index nBnB​ to add to SS is determined using Equation (22), where λBnλBn​ represents the eigenvalue and eB,kneB,kn​ denotes the eigenvector.
%         Similarly, the probability of selecting the kk-th eigenvector index nn from the adjacent matrix LB^LB^​ to add to S^S^ is computed using a similar equation.

%     Algorithm Overview:
%         The provided algorithm outlines the steps for selecting eigenvector index subsets SS and S^S^ with a cardinality of kk.
%         This involves eigen decomposition of both LBLB​ and LB^LB^​, followed by iterative selection based on the computed probabilities.
\subsection{SAM for overlapping Bands}
In the context of grouping strategies in HSI, SAM is employed to handle overlapping bands within a group. Overlapping bands can complicate the analysis as they may contain redundant information or obscure the distinct spectral features. By applying SAM as illustrated in Algorithm 3, we can measure the precise similarity between bands within a group and disentangle those that are too similar.
\begin{algorithm}
\caption{Calculate Spectral Angle Mapper (SAM)}
\begin{algorithmic}[1]
\Function{CalculateSAM}{$\vec{v}_1, \vec{v}_2$}
    \State $numerator \gets \vec{v}_1 \cdot \vec{v}_2$ \Comment{Dot product of vectors}
    \State $denominator \gets \|\vec{v}_1\| \cdot \|\vec{v}_2\|$ \Comment{Product of norms}
    \State $angle \gets \arccos\left(\frac{numerator}{denominator}\right)$ \Comment{Calculate angle}
    \State \Return $angle$
\EndFunction
\State $diverse\_set \gets \{ k\}$ \Comment{Diverse set elements}
\State $sam\_values \gets \mathbf{0}_{|diverse\_set| \times 31}$ \Comment{Initialize SAM values matrix}

\For{$i \in \{1, \dots, |diverse\_set|\}$}
    \For{$j \in \{1, \dots, 31\}$}
        \If{$diverse\_set[i] \neq j$}
            % \State $band\_data_1 \gets \text{flatten}(data1[:, diverse\_set[i]])$
            % \State $band\_data_2 \gets \text{flatten}(data1[:, j])$
            \State $sam\_values[i, j] \gets \text{CalculateSAM}(band\_data_1, band\_data_2)$
        \Else
            \State $sam\_values[i, j] \gets \text{NaN}$
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
% Band selection hints at band grouping implicitly. Band selection has been formerly studied as one of the most important topics in HSI. Band selection is done to remove redundancy within bands so that noise-free reconstruction or classification of HSI takes place. Band selection selects a group of bands among the actual bands which have similar properties in-terms of structure or spectral response, thus reducing the spectral dimensionality for a more manageable feature reconstruction. The idea behind band grouping was to help in restricting more information to a subset of bands such that this group will have lesser similarity and hence provides sufficient knowledge about objects~\cite{guyon2003introduction,wang2019hyperspectral,li2019efficient}.

% We now look at band grouping from three main perspectives: \textbf{(1) Implicit grouping:} This type of grouping isnot limited to grouping bands as subsets, it groups bands across all dimensions~\cite{rodarmel2002principal,uddin2021pca,greenacre2022principal}. This is based on the fact that either the group of bands are chosen to reduce the dimensionality of the data or select only relevant bands. This is mostly suitable for classification based problems in general. However one must ensure that with reduced number of bands, the classification accuracy should be maintained. \textbf{(2) Pseudo implicit grouping:} The aim of this group is to first explicitly map the subsets into smaller groups and then based on clustering or ranking based methods this form reduces to an implicit grouping form. To simply put this form can be regarded as an indirect implicit grouping. This class mainly includes unsupervised based algorithms. This further entails two categories of methods based on the fact that when bands are considered as points, clustering based methods~\cite{jia2015novel,martinez2007clustering,yuan2015dual} will be considered. These points when considered are grouped, are based upon similarity matrix such as divergence, entropy or other criteria. Where as when a certain criterion is considered to group bands based on the ascending, descending, or some order, a score is established and based on this score subset of bands is selected~\cite{chang1999joint,datta2015combination,wang2016salient,martinez2006clustering}. Both these methods incorporate learning based methods which obtain only relevant or representative bands. If this grouping strategy is applied to interpolation technique, it tries to extract the information among the selected group in such a way that such representative groups map only the strong matches in the output image. \textbf{(3) Explicit grouping:} Both implicit grouping and pseudo implicit grouping in a way discretize the imaging data received at the camera sensor such that the actual scene information is already compromised. Also, the rich literature that exists in further discretizing the processing is done in the form of band selection and band grouping which leads to limited representation in the form of few significant bands. However, in imaging system, the high-resolution output is perceived as the combination of sub-pixels interpolated as a result of adjacent pixels~\cite{shi2016real}. Similarly one can think of the combination of pixels from the bands in the group as a representation of a sub-pixel which extends in the spectral domain and hence we term this as spectral sub-pixel. This matching spectral sub-pixel from another band in a group acts as the sub-pixel in the low resolution HSI (LR-HSI) as it can be mapped to a larger pixel as a rearrangement in high-resolution (HR-HSI). This spectral sub-pixel has inherent significance when it comes to reconstruction problem in HSI. We, therefore lay a fundamental end-to-end mapping technique for super-resolution in HSI, such that instead of using implicit or pseudo implicit grouping, we use all bands as spectral sub-pixel because each spectral sub-pixel of a group is equivalent to the sub-pixel which is adjusted depending on the interpolation between adjacent pixels in HR-HSI. Hence, the novelty lies in mapping information from spectral sub-pixel groups to the generated output HSI. This concept is similar to redundancy of pixels and band information being converted into sub-pixel and spectral sub-pixel information for super-resolution.

% Several grouping techniques have been covered~\cite{liu2021spectral,zhou2016deep,khan2015joint} in literature. Adaptive spectral grouping technique~\cite{hussain2023extending} is another method explored in HSI super-resolution (SR). In this work, we build upon the correlation matrix as a means of grouping technique (Figure~\ref{fig:foobar}). Using insights from correlation matrix we group bands based on the group numbers obtained from the matrix. In Figure~\ref{fig:FIG2} we illustrate our proposed method using Ntire2022 dataset. The low-resolution input upon analysis using correlation matrix transformation gives information regrading the subset of bands to be taken as groups. The number of groups can vary based on the user, as we can further divide a group into other sub-groups and hence the number varies from $1 \hdots n$ where n is the number of band-groups considered. 
\section{Conclusion}
In conclusion, our proposed approach, which utilizes Determinantal Point Processes (DPP) and spectral angle mapping (SAM), offers a promising direction for addressing the complexities of hyperspectral imaging (HSI). By applying these methodologies, we aim to enhance the effectiveness of spectral band selection and optimize hyperspectral image reconstruction. This approach seeks to mitigate redundancy while striving to improve the efficiency and accuracy of analysis techniques. Future research into advanced grouping strategies will be essential for overcoming remaining computational and interpretative hurdles, thereby unlocking the full potential of HSI in various earth observation applications.
\bibliography{samplepaper}
\bibliographystyle{splncs04}
%
%
%
% \section{First Section}
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

% \begin{credits}
% \subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
% used for general acknowledgments, for example: This study was funded
% by X (grant number Y).

% \subsubsection{\discintname}
% It is now necessary to declare any competing interests or to specifically
% state that the authors have no competing interests. Please place the
% statement with a bold run-in heading in small font size beneath the
% (optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
% system, is used, then the disclaimer can be provided directly in the system.},
% for example: The authors have no competing interests to declare that are
% relevant to the content of this article. Or: Author A has received research
% grants from Company W. Author B has received a speaker honorarium from
% Company X and owns stock in Company Y. Author C is a member of committee Z.
% \end{credits}
% %
% % ---- Bibliography ----
% %
% % BibTeX users should specify bibliography style 'splncs04'.
% % References will then be sorted and formatted in the correct style.
% %
% % \bibliographystyle{splncs04}
% % \bibliography{mybibliography}
% %
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
% \end{thebibliography}
\end{document}
