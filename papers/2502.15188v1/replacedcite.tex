\section{Related work}
\subsection{Traditional image compression methods}
Image compression has been studied for decades and has resulted in a series of well-known standards, such as JPEG ____, JPEG2000 ____, and H.266/VVC ____. Traditional methods use transforms, quantization, and entropy coding. The main techniques include the discrete cosine transform (DCT), the discrete wavelet transform (DWT), context-adaptive variable-length coding (CAVLC), and context-adaptive binary arithmetic coding (CABAC). The works in ____ have achieved significant performance gains by refining traditional methods. However, joint optimization of different techniques remains challenging with these traditional approaches.

\subsection{Learned image compression methods}
LIC methods achieve excellent performance through end-to-end joint optimization of transform networks and entropy coding.

\textbf{Transform Networks.} Toderici \textit{et al}. ____ proposed a recurrent neural network (RNN)-based image compression method that outperformed JPEG. Due to their excellent feature extraction capabilities, convolutional neural networks (CNNs) have also been widely used in image encoding ____. Ballé \textit{et al}. ____ first proposed an end-to-end image compression framework using a CNN to implement nonlinear transformations. Subsequently, the work in ____ proposed a hyperprior to capture spatial dependencies in the latent representation. Minnen \textit{et al}. ____ jointly optimized hierarchical hyperpriors and autoregressive models to achieve better RD performance. Chen \textit{et al}. ____ proposed a non-local attention module (NLAM) to capture long-range correlations. Cheng \textit{et al}. ____ later simplified the NLAM. Ma \textit{et al}. ____ proposed iWave, a framework tailored for deriving a wavelet-like transform optimized for natural image compression. Akbari \textit{et al}. ____ proposed a multi-resolution variable-rate image compression framework that uses generalized octave convolutions (GoConv) to factorize the feature maps into high and low resolutions. In recent years, with the explosion of self-attention, Vision Transformers (ViT) have also received widespread attention ____. Zhu \textit{et al}. ____ used the Swin transformer ____ to construct nonlinear transformations. Lu \textit{et al}. ____ used a neural transformation unit (NTU) based on a VAE architecture to extract features. Subsequently, the work in ____ further adopted an integrated convolution and self-attention unit (ICSA) to achieve content-adaptive transforms.

\textbf{Entropy Models.} Building an accurate entropy model is essential for improving coding performance. Ballé \textit{et al}. ____ proposed a zero-mean Gaussian scale mixture (GSM) model for entropy coding, which achieved better performance than BPG. In the proposed GSM, the scale parameters are conditioned on a hyperprior. 
Minnen \textit{et al}. ____ extended the work in ____ by generalizing the GSM model to a conditional Gaussian mixture model (GMM). They also combined an autoregressive model with the hyperprior. Zhou \textit{et al}. ____ modeled the entropy model using a Laplacian distribution, which outperformed the Gaussian model. Cheng \textit{et al}. ____ used a discrete GMM, and Fu \textit{et al}. ____ proposed a Gaussian-laplacian-logistic mixture model (GLLMM). \textcolor{black}{In addition, to enhance the encoding efficiency of the joint hyperprior and autoregressive model, some works explored parallel upper-lower modeling, such as channel-wise techniques ____, checkerboard context ____, and multistage context model ____. These approaches leverage parallelization to reduce the high time complexity caused by serial scanning methods.}
\begin{table}
  \caption{Notations}
  \label{tab:Notations}
  \begin{tabular}{ccl}
    \toprule
    Abbr./Symbol &Description\\
    \midrule
    FExM & Feature Extraction Module\\
	FRM & Feature Refinement Module\\
	FEnM & Feature Enhancement Module\\
	ARB & Attention Refinement Block\\
	ICRSA & Integrated Concatenated Residual and Self-Attention\\
	CRM & Concatenated Residual Module\\
	MCM  & Multi-stage Context Model\\
	DB & Dense Block\\
	QECM & Quantization Error Compensation Module\\
	U$\mid$Q & Quantization process\\
	AE/AD & Arithmetic Encoding and Arithmetic Decoding\\
	\midrule
	$\bm{S}_{i},i=1,2,...,b^2$ & Sub-images\\
	$\bm{f_x}$ & Feature corresponding to input image \bm{$x$}\\
	$\bm{f}_{\bm{S}_{i}},i=1,2,...,b^2$ & Feature corresponding to Sub-image $\bm{S}_{i}$\\
	$\bm{f'}$ & Stacked feature of $\bm{f}_{\bm{S}_1}$,$\bm{f}_{\bm{S}_2}$,$...$,$\bm{f}_{\bm{S}_{b^2}}$ and $\bm{f_x}$ \\
	$\bm{f}$ & Refined feature\\
	$\bm{\hat{f}_e}$ & Reconstructed enhanced refined feature\\
	$g_a/g_s$ & Main Encoder / Decoder\\
	$h_a/h_s$ & Hyper Encoder / Decoder\\
	$\text{H}_{qc}/\text{H}_{iqc}$ & (Inverse) Quantization Compensation Block \\
	$\bm{y}(\bm{\tilde{y}/\bm{\hat{y}}})$ & Latent (soft / hard quantized latent)\\
	$\bm{z}(\bm{\tilde{z}}/\bm{\hat{z}})$ & Hyper latent (soft / hard quantized hyper latent)\\
	$\bm{x}/ \bm{\hat{x}} $ & Input image / Reconstructed image\\
	$\bm{\psi}$ & Hyperpriors\\
    \bottomrule
	\end{tabular}
\end{table}

\begin{figure*}
	\centering
	\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figures/framework.pdf}
	\caption{\textcolor{black}{Framework. The codec uses a main and a hyper encoder-decoder architecture based on VAE. FExM applies image split and feature extraction. FRM refines the stacked features with ARB, which uses an attention block (AB) to refine the features in the channel, spatial, temporal and feature dimensions. FEnM uses two dense blocks (DB) to enhance the distorted features. In the codec, each ICRSA consists of a conv layer, a concatenated residual module (CRM), and an RNAB. RNAB ____ aggregates neighborhood information based on attention. $d_i(i=1,...,6)$ is the number of RNABs used at the $i$-th stage. $\mathrm{St}$, $\mathrm{Sr}$ represent stacking and re-arranging. $k5s2$ denotes a kernel with a size of $5 \times 5$ and a stride of 2. }}
	\label{fig:framework}
	\end{minipage}
\end{figure*}

\textbf{Quantization.} One of the primary challenges is maintaining a differentiable and continuous process for backpropagation-based training, which contrasts with the non-differentiable nature of hard quantization operations. To tackle this issue, non-differentiable quantization is approximated using differentiable techniques. This approximation can be achieved by introducing additive uniform noise____, straight-through estimator (STE)____, and soft-to-hard annealing____. Most prevalent approaches opt for soft quantization with additive uniform noise as a replacement for hard quantization during training. However, this approach can lead to a mismatch between training and testing. Eirikur \textit{et al}.____ introduced universal quantization and soft rounding methods to bridge the gap between training and testing. Guo \textit{et al}.____ proposed a soft-then-hard strategy to learn latent representations and address the aforementioned mismatch issue. Fu \textit{et al}.____ introduced a post-quantization filter to minimize the difference between the latent and dequantized latent representations. However, these methods often overlook the characteristics and statistical distribution of quantization errors. To alleviate the mismatch problem, we propose a quantization error compensation method based on Fourier series approximation and Laplacian noise. Our paper extends our previous work____ by adding feature extraction, refinement, and enhacement modules.