\section{Related work}
\subsection{Traditional image compression methods}
Image compression has been studied for decades and has resulted in a series of well-known standards, such as JPEG \cite{6}, JPEG2000 \cite{7}, and H.266/VVC \cite{8}. Traditional methods use transforms, quantization, and entropy coding. The main techniques include the discrete cosine transform (DCT), the discrete wavelet transform (DWT), context-adaptive variable-length coding (CAVLC), and context-adaptive binary arithmetic coding (CABAC). The works in \cite{8962013,9762060,8744274,9899414,8447515} have achieved significant performance gains by refining traditional methods. However, joint optimization of different techniques remains challenging with these traditional approaches.

\subsection{Learned image compression methods}
LIC methods achieve excellent performance through end-to-end joint optimization of transform networks and entropy coding.

\textbf{Transform Networks.} Toderici \textit{et al}. \cite{toderici2015variable,toderici2017full} proposed a recurrent neural network (RNN)-based image compression method that outperformed JPEG. Due to their excellent feature extraction capabilities, convolutional neural networks (CNNs) have also been widely used in image encoding \cite{theis2017lossy,balle2018variational,cheng2020learned,chen2021end,jiang23}. Ballé \textit{et al}. \cite{balle2016end} first proposed an end-to-end image compression framework using a CNN to implement nonlinear transformations. Subsequently, the work in \cite{balle2018variational} proposed a hyperprior to capture spatial dependencies in the latent representation. Minnen \textit{et al}. \cite{minnen2018joint} jointly optimized hierarchical hyperpriors and autoregressive models to achieve better RD performance. Chen \textit{et al}. \cite{chen2021end} proposed a non-local attention module (NLAM) to capture long-range correlations. Cheng \textit{et al}. \cite{cheng2020learned} later simplified the NLAM. Ma \textit{et al}. \cite{8931632} proposed iWave, a framework tailored for deriving a wavelet-like transform optimized for natural image compression. Akbari \textit{et al}. \cite{9385968} proposed a multi-resolution variable-rate image compression framework that uses generalized octave convolutions (GoConv) to factorize the feature maps into high and low resolutions. In recent years, with the explosion of self-attention, Vision Transformers (ViT) have also received widespread attention \cite{zhu2022transformer,lu2021transformer,lu2022high}. Zhu \textit{et al}. \cite{zhu2022transformer} used the Swin transformer \cite{liu2021swin} to construct nonlinear transformations. Lu \textit{et al}. \cite{lu2021transformer} used a neural transformation unit (NTU) based on a VAE architecture to extract features. Subsequently, the work in \cite{lu2022high} further adopted an integrated convolution and self-attention unit (ICSA) to achieve content-adaptive transforms.

\textbf{Entropy Models.} Building an accurate entropy model is essential for improving coding performance. Ballé \textit{et al}. \cite{balle2018variational} proposed a zero-mean Gaussian scale mixture (GSM) model for entropy coding, which achieved better performance than BPG. In the proposed GSM, the scale parameters are conditioned on a hyperprior. 
Minnen \textit{et al}. \cite{minnen2018joint} extended the work in \cite{balle2018variational} by generalizing the GSM model to a conditional Gaussian mixture model (GMM). They also combined an autoregressive model with the hyperprior. Zhou \textit{et al}. \cite{zhou2018variational} modeled the entropy model using a Laplacian distribution, which outperformed the Gaussian model. Cheng \textit{et al}. \cite{cheng2020learned} used a discrete GMM, and Fu \textit{et al}. \cite{fu2023learned} proposed a Gaussian-laplacian-logistic mixture model (GLLMM). \textcolor{black}{In addition, to enhance the encoding efficiency of the joint hyperprior and autoregressive model, some works explored parallel upper-lower modeling, such as channel-wise techniques \cite{minnen2020channel}, checkerboard context \cite{He_2021_CVPR}, and multistage context model \cite{lu2022high}. These approaches leverage parallelization to reduce the high time complexity caused by serial scanning methods.}
\begin{table}
  \caption{Notations}
  \label{tab:Notations}
  \begin{tabular}{ccl}
    \toprule
    Abbr./Symbol &Description\\
    \midrule
    FExM & Feature Extraction Module\\
	FRM & Feature Refinement Module\\
	FEnM & Feature Enhancement Module\\
	ARB & Attention Refinement Block\\
	ICRSA & Integrated Concatenated Residual and Self-Attention\\
	CRM & Concatenated Residual Module\\
	MCM  & Multi-stage Context Model\\
	DB & Dense Block\\
	QECM & Quantization Error Compensation Module\\
	U$\mid$Q & Quantization process\\
	AE/AD & Arithmetic Encoding and Arithmetic Decoding\\
	\midrule
	$\bm{S}_{i},i=1,2,...,b^2$ & Sub-images\\
	$\bm{f_x}$ & Feature corresponding to input image \bm{$x$}\\
	$\bm{f}_{\bm{S}_{i}},i=1,2,...,b^2$ & Feature corresponding to Sub-image $\bm{S}_{i}$\\
	$\bm{f'}$ & Stacked feature of $\bm{f}_{\bm{S}_1}$,$\bm{f}_{\bm{S}_2}$,$...$,$\bm{f}_{\bm{S}_{b^2}}$ and $\bm{f_x}$ \\
	$\bm{f}$ & Refined feature\\
	$\bm{\hat{f}_e}$ & Reconstructed enhanced refined feature\\
	$g_a/g_s$ & Main Encoder / Decoder\\
	$h_a/h_s$ & Hyper Encoder / Decoder\\
	$\text{H}_{qc}/\text{H}_{iqc}$ & (Inverse) Quantization Compensation Block \\
	$\bm{y}(\bm{\tilde{y}/\bm{\hat{y}}})$ & Latent (soft / hard quantized latent)\\
	$\bm{z}(\bm{\tilde{z}}/\bm{\hat{z}})$ & Hyper latent (soft / hard quantized hyper latent)\\
	$\bm{x}/ \bm{\hat{x}} $ & Input image / Reconstructed image\\
	$\bm{\psi}$ & Hyperpriors\\
    \bottomrule
	\end{tabular}
\end{table}

\begin{figure*}
	\centering
	\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figures/framework.pdf}
	\caption{\textcolor{black}{Framework. The codec uses a main and a hyper encoder-decoder architecture based on VAE. FExM applies image split and feature extraction. FRM refines the stacked features with ARB, which uses an attention block (AB) to refine the features in the channel, spatial, temporal and feature dimensions. FEnM uses two dense blocks (DB) to enhance the distorted features. In the codec, each ICRSA consists of a conv layer, a concatenated residual module (CRM), and an RNAB. RNAB \cite{lu2022high} aggregates neighborhood information based on attention. $d_i(i=1,...,6)$ is the number of RNABs used at the $i$-th stage. $\mathrm{St}$, $\mathrm{Sr}$ represent stacking and re-arranging. $k5s2$ denotes a kernel with a size of $5 \times 5$ and a stride of 2. }}
	\label{fig:framework}
	\end{minipage}
\end{figure*}

\textbf{Quantization.} One of the primary challenges is maintaining a differentiable and continuous process for backpropagation-based training, which contrasts with the non-differentiable nature of hard quantization operations. To tackle this issue, non-differentiable quantization is approximated using differentiable techniques. This approximation can be achieved by introducing additive uniform noise~\cite{balle2016end,balle2018variational,minnen2018joint,fu2023learned,kim2022joint}, straight-through estimator (STE)~\cite{bengio2013estimating}, and soft-to-hard annealing~\cite{agustsson2017soft,yang2020improving}. Most prevalent approaches opt for soft quantization with additive uniform noise as a replacement for hard quantization during training. However, this approach can lead to a mismatch between training and testing. Eirikur \textit{et al}.~\cite{agustsson2020universally} introduced universal quantization and soft rounding methods to bridge the gap between training and testing. Guo \textit{et al}.~\cite{guo2021soft} proposed a soft-then-hard strategy to learn latent representations and address the aforementioned mismatch issue. Fu \textit{et al}.~\cite{fu2022asymmetric} introduced a post-quantization filter to minimize the difference between the latent and dequantized latent representations. However, these methods often overlook the characteristics and statistical distribution of quantization errors. To alleviate the mismatch problem, we propose a quantization error compensation method based on Fourier series approximation and Laplacian noise. Our paper extends our previous work~\cite{jiang23} by adding feature extraction, refinement, and enhacement modules.