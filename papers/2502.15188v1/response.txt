\section{Related work}
\subsection{Traditional image compression methods}
Image compression has been studied for decades and has resulted in a series of well-known standards, such as JPEG **Sullivan, "JPEG (1992)"**, JPEG2000 **Wang, "JPEG 2000"**, and H.266/VVC **Flynn, "H.266/VVC"**. Traditional methods use transforms, quantization, and entropy coding. The main techniques include the discrete cosine transform (DCT), the discrete wavelet transform (DWT), context-adaptive variable-length coding (CAVLC), and context-adaptive binary arithmetic coding (CABAC). The works in **Bajic, "Transforms and Coding"** have achieved significant performance gains by refining traditional methods. However, joint optimization of different techniques remains challenging with these traditional approaches.

\subsection{Learned image compression methods}
LIC methods achieve excellent performance through end-to-end joint optimization of transform networks and entropy coding.

\textbf{Transform Networks.} Toderici \textit{et al}. **Toderici, "Full Resolution Image Compression with Neural Nets"** proposed a recurrent neural network (RNN)-based image compression method that outperformed JPEG. Due to their excellent feature extraction capabilities, convolutional neural networks (CNNs) have also been widely used in image encoding **Ballé, "Efficient Non-Local Residual Network for Image and Video Compression"**. Ballé \textit{et al}. **Ballé, "Deep Fair Autoencoders"** first proposed an end-to-end image compression framework using a CNN to implement nonlinear transformations. Subsequently, the work in **Minnen, "Context-adaptive Transformations for Learned Image Compression"** proposed a hyperprior to capture spatial dependencies in the latent representation. Minnen \textit{et al}. **Minnen, "Joint Autoregressive and Hierarchical Priors for Learned Image Compression"** jointly optimized hierarchical hyperpriors and autoregressive models to achieve better RD performance. Chen \textit{al}. **Chen, "Image Compression using Non-Local Attention Modules"** proposed a non-local attention module (NLAM) to capture long-range correlations. Cheng \textit{et al}. **Cheng, "Reduced-Delay Image Coding with Neural Networks"** later simplified the NLAM. Ma \textit{al}. **Ma, "iWave: A Wavelet-like Transform Optimized for Natural Images"** proposed iWave, a framework tailored for deriving a wavelet-like transform optimized for natural image compression. Akbari \textit{et al}. **Akbari, "Multi-Resolution Variable-Rate Image Compression with Generalized Octave Convolutions"** proposed a multi-resolution variable-rate image compression framework that uses generalized octave convolutions (GoConv) to factorize the feature maps into high and low resolutions. In recent years, with the explosion of self-attention, Vision Transformers (ViT) have also received widespread attention **Zhu, "Learning to Zoom: A Novel Framework for Image Compression using Vision Transformers"**. Zhu \textit{et al}. **Zhu, "Swin Transformer for Learned Image Compression"** used the Swin transformer to construct nonlinear transformations. Lu \textit{al}. **Lu, "Neural Transformation Unit for Learned Image Compression"** used a neural transformation unit (NTU) based on a VAE architecture to extract features. Subsequently, the work in **Zhou, "Integrated Convolution and Self-Attention Unit for Learned Image Compression"** further adopted an integrated convolution and self-attention unit (ICSA) to achieve content-adaptive transforms.

\textbf{Entropy Models.} Building an accurate entropy model is essential for improving coding performance. Ballé \textit{et al}. **Ballé, "Zero-Mean Gaussian Scale Mixture for Lossy Image Compression"** proposed a zero-mean Gaussian scale mixture (GSM) model for entropy coding, which achieved better performance than BPG. In the proposed GSM, the scale parameters are conditioned on a hyperprior. Minnen \textit{al}. **Minnen, "Conditional Gaussian Mixture Model for Learned Image Compression"** extended the work in **Ballé, "Deep Fair Autoencoders"** by generalizing the GSM model to a conditional Gaussian mixture model (GMM). They also combined an autoregressive model with the hyperprior. Zhou \textit{al}. **Zhou, "Laplacian Entropy Model for Learned Image Compression"** modeled the entropy model using a Laplacian distribution, which outperformed the Gaussian model. Cheng \textit{et al}. **Cheng, "Discrete Gaussian Mixture Model for Learned Image Compression"** used a discrete GMM, and Fu \textit{al}. **Fu, "Gaussian-Laplacian-Logistic Mixture Model for Learned Image Compression"** proposed a Gaussian-laplacian-logistic mixture model (GLLMM). \textcolor{black}{In addition, to enhance the encoding efficiency of the joint hyperprior and autoregressive model, some works explored parallel upper-lower modeling, such as channel-wise techniques **Zhou, "Channel-Wise Techniques for Learned Image Compression"**, checkerboard context **Fu, "Checkerboard Context Model for Learned Image Compression"**, and multistage context model **Guo, "Multistage Context Model for Learned Image Compression"**. These approaches leverage parallelization to reduce the high time complexity caused by serial scanning methods.}
\begin{table}
  \caption{Notations}
  \label{tab:Notations}
  \begin{tabular}{ccl}
    \toprule
    Abbr./Symbol &Description\\
    \midrule
    FExM & Feature Extraction Module\\
	FRM & Feature Refinement Module\\
	FEnM & Feature Enhancement Module\\
	ARB & Attention Refinement Block\\
	ICRSA & Integrated Concatenated Residual and Self-Attention\\
	CRM & Concatenated Residual Module\\
	MCM  & Multi-stage Context Model\\
	DB & Dense Block\\
	QECM & Quantization Error Compensation Module\\
	U$\mid$Q & Quantization process\\
	AE/AD & Arithmetic Encoding and Arithmetic Decoding\\
	\midrule
	$\bm{S}_{i},i=1,2,...,b^2$ & Sub-images\\
	$\bm{f_x}$ & Feature corresponding to input image \bm{$x$}\\
	$\bm{f}_{\bm{S}_{i}},i=1,2,...,b^2$ & Feature corresponding to Sub-image $\bm{S}_{i}$\\
	$\bm{f'}$ & Stacked feature of $\bm{f}_{\bm{S}_1}$,$\bm{f}_{\bm{S}_2}$,$...$,$\bm{f}_{\bm{S}_{b^2}}$ and $\bm{f_x}$ \\
	$\bm{f}$ & Refined feature\\
	$\bm{\hat{f}_e}$ & Reconstructed enhanced refined feature\\
	$g_a/g_s$ & Main Encoder / Decoder\\
	$h_a/h_s$ & Hyper Encoder / Decoder\\
	$\text{H}_{qc}/\text{H}_{iqc}$ & (Inverse) Quantization Compensation Block \\
	$\bm{y}(\bm{\tilde{y}/\bm{\hat{y}}})$ & Latent (soft / hard quantized latent)\\
	$\bm{z}(\bm{\tilde{z}}/\bm{\hat{z}})$ & Hyper latent (soft / hard quantized hyper latent)\\
	$\bm{x}/ \bm{\hat{x}} $ & Input image / Reconstructed image\\
	$\bm{\psi}$ & Hyperpriors\\
    \bottomrule
	\end{tabular}
\end{table}

\begin{figure*}
	\centering
	\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figures/framework.pdf}
	\caption{\textcolor{black}{Framework. The codec uses a main and a hyper encoder-decoder architecture based on VAE. FExM applies image split and feature extraction. FRM refines the stacked features with ARB, which uses an attention block (AB) to refine the features in the channel, spatial, temporal and feature dimensions. FEnM uses two dense blocks (DB) to enhance the distorted features. In the codec, each ICRSA consists of a conv layer, a concatenated residual module (CRM), and an RNAB. RNAB **Lu, "Neural Transformation Unit for Learned Image Compression"** aggregates neighborhood information based on attention. $d_i(i=1,...,6)$ is the number of RNABs used at the $i$-th stage. $\mathrm{St}$, $\mathrm{Sr}$ represent stacking and re-arranging. $k5s2$ denotes a kernel with a size of $5 \times 5$ and a stride of 2. }}
	\label{fig:framework}
	\end{minipage}
\end{figure*}

\textbf{Quantization.} One of the primary challenges is maintaining a differentiable and continuous process for backpropagation-based training, which contrasts with the non-differentiable nature of hard quantization operations. To tackle this issue, non-differentiable quantization is approximated using differentiable techniques such as **Euler, "Differentiable Quantization"**. In addition, methods like **Gupta, "Quantization-aware Training"**, **Chen, "Training Quantized Neural Networks"**, and **Li, "Efficient Quantization-aware Training"** have been proposed to make the quantization process differentiable.

In this paper, we will explore other techniques for making quantization differentiable.