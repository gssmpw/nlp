\section{Experiments}\label{sec:experiments}
We now evaluate SAPS using both qualitative and quantitative analyses. We first compare its zero-shot performance to R3L on benchmark tasks, then 
%perform delve into ablation studies and
an analysis of how our alignment approach behaves under different conditions.

\paragraph{Environments}
Our agents act by receiving pixel images as input observation, consisting of four consecutive $84 \times 84$ RGB images, stacked along the channel dimension to capture dynamic information such as velocity and acceleration.
We consider environments where we can freely change visual features (background color, camera perspective) or task (rewards, dynamics), therefore we use CarRacing \citep{klimov2016carracing} and LunarLander as both implemented in R3L.
CarRacing requires the agent to drive in a track using pixel observations, whose variations can be in the background color or the target speed, while LunarLander requires the agent to land on a platform, with variations comprising background color and different gravities.
% \AR{appendice per dettagli approfonditi su variazioni}.
No context is provided, hence the agents do not receive any information about the task.
% In the appendix we have other tests with atari env: \Cref{appendix:atari} \AR{riscrivi frase}

\paragraph{Baselines}
We mainly compare SAPS to (R3L), another zero-shot stitching method using relative representations whose approach is similar to ours.
For an additional baseline we also compare to naive zero-shot stitching, where we stitch encoders and controllers with no additional processing, to showcase the progress reached by the methods performing latent alignment techniques.