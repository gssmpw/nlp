\section{Related Work}

\paragraph{Domain Adaptation and Generalization in RL}

One line of work tackling the mismatch between training and deployment environments focuses on domain adaptation. Early solutions often rely on domain randomization, which exposes agents to a broad range of visual or dynamical variations during training so that they learn features robust to such changes \citep{tobin2017domain, sadeghi2016cad2rl}. Similarly, data augmentation techniques \citep{yarats2021drqv2, laskin2020reinforcement} modify raw input frames (e.g., random crops, color jitter) to improve out-of-distribution performance \citep{kostrikov2020image, yarats2021drqv2}.
However, these methods either demand extensive coverage of possible variations or can be extremely resource intensive.



\paragraph{Multi-task and Modular RL}
A common strategy to reuse knowledge across related tasks is multi-task or meta-RL, where a single agent is trained on multiple environments \citep{teh2017distral, finn2017model}. While this can yield more robust representations, it often demands joint training on all tasks, which might be impractical when new tasks appear over time. Transfer RL \citep{taylor2009transfer} instead aims to expedite learning in a target task by leveraging agents trained on a source task, reusing features or parameters \citep{barreto2017successor, killian2017robust}, value functions (\cite{tirinzoni2018transfer, liu2021learning}, sub-policies \citep{fernandez2006probabilistic, devin2017learning}.
Other approaches exploit networks modularity, by designing agents as composition of planner and actuator modules \citep{karkus2020beyond} or by combining submodules to solve harder tasks \citep{mendez2022modular, russell2003q, simpkins2019composable}. These, however, require defining ad-hoc architectures to work. Our method instead focuses on zero-shot reuse of already existing models, while also eliminating the need for environment interaction or fine-tuning to adapt an already-trained module to a new domain.

\paragraph{Representation Learning}
Exploiting representation learning techniques is another approach to mitigate the complexity in RL. Some approaches propose to isolate visual or observational factors from task-dependent decision-making \citep{oord2018representation}, or learning a robust policy given a source domain \citep{higgins2017darla}. Other ignore task-irrelevant features via invariant encoders, using bisimulation metrics as training constraints \citep{zhang2020learning}, using supervised learning o train inverse dynamics models \citep{hansen2020self} or finetuning to match prior latent distributions \citep{yoneda2021invariance}.
However, these approaches focus exclusively on visual variations, leaving shifts in the underlying task unaddressed. While such fine-tuning can be more efficient than learning from scratch, it still requires re-optimization whenever the environment changes. In contrast, our proposed approach does not need re-training or fine-tuning. Instead, it only needs to encode small transformation between existing latent spaces.


\paragraph{Model Stitching}
Model stitching refers to the process of combining separate neural modules (e.g. encoders, decoders, or intermediate layers from independently trained models) to create a new, fully functional model. Initially studied in supervised learning as means to measure latent space similarity across different models, \citep{Lenc2014-gy, Bansal2021-oj, Csiszarik2021-yi}, models stitching recently is being used for zero-shot model reuse \citep{Moschella2022-yf, norelli2022b, maiorca2023latent, cannistraci2023bricks} by using sets of parallel data called \textit{anchors}, which establish a semantic correspondence between models. These are used in the relative representation \citep{Moschella2022-yf} framework to project latent spaces to a common space, and the semantic alignment framework \citep{maiorca2023latent} to directly estimate a mapping between latent spaces of different models.

Recently, the relative representation framework has been used to perform stitching between encoders and controllers in the context of vision-based imitation learning \citep{jian2024perception}, while in RL it has been used to perform few-shot and zero-shot stitching in robotic control from low dimensional proprioceptive states \citep{jian2023policy} or control from pixels in the Gymnasium suite \citep{ricciardi2025r3lrelativerepresentationsreinforcement}.
Although powerful, these methods require training or fine-tuning models or the decoder, so that it learns to act given \say{relative} inputs, while also requiring additional training constraints to not degrade end-to-end performance.
By contrast, our approach applies the idea of creating a mapping between layers by using anchor samples \citep{maiorca2023latent} and is directly applicable to already trained models, learning a transformation that maps between modules of different policies without further constraints or fine-tuning.


\iffalse
\paragraph{Summary}
Most existing methods for RL generalization demand new optimization phases or specialized architectures. The semantic alignment approach described here departs from that paradigm by casting domain adaptation as a problem of matching latent spaces. By collecting a modest number of paired observations (or anchor states) across domains, we empirically show that an alignment transform is sufficient to fuse an existing encoder with a different controller, yielding robust policies without additional environment interactions. This opens up new avenues for RL research, where large libraries of pre-trained agents can be combined in zero-shot fashion to tackle unseen variations in observation or task.
\AR{rivedi}
\fi