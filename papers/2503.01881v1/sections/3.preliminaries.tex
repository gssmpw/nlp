\newcommand{\enc}[0]{\phi}
\newcommand{\con}[0]{\psi}
\newcommand{\encmap}[2]{\mathcal{O}_{#1}^{#2} \mapsto \mathcal{X}_{#1}^{#2}}
\newcommand{\conmap}[3]{\mathcal{X}_{#1}^{#2} \mapsto \mathcal{A}_{#3}}
\newcommand{\ours}[0]{Trasl. \textbf{(Ours)}}


\section{Preliminaries}

We assume the underlying environment is a Markov decision process $\mathcal{M} = (\mathcal{S, A}, \mathcal{O}, R, P, \gamma)$, with state space $\mathcal{S}$, action space $\mathcal{A}$, input observations $o \in \mathcal{O}$ and the transition
function $P : \mathcal{S} \times \mathcal{A} \mapsto \mathcal{S}$ that defines a probability distribution over the next state given the current state and action. The function $\mathcal{R} : \mathcal{S} \times \mathcal{A} \mapsto \mathcal{R}$ assigns rewards, and $\gamma$ is the discount factor that reduces the importance of delayed rewards. The agent’s behavior is dictated by a policy $\pi : \mathcal{O} \rightarrow \mathcal{A}$, which receives an observation and selects an action at each state, and is trained to maximize the discounted returns
$\mathbb{E}\Bigl[\sum_{i=0}^{\infty} \gamma^{i} \mathcal{R}(\mathbf{s}_{i}, \mathbf{a}_{i})\Bigr]$.

\subsection{Background}
We are interested in scenarios where both training setups and agent behaviors can vary. We find it convenient to use the same notation introduced in \textsc{R3L} (Relative Representations for Reinforcement Learning) \citep{ricciardi2025r3lrelativerepresentationsreinforcement} to formalize these variations.

\paragraph{Environment variations}
We denote an environment by $\mathcal{M}{u}^{i} = (\mathcal{O}_{u}, T_{i})$. Here, $\mathcal{O}_{u}$ is the distribution of observations $o_{u}$, and $T_{i} : \mathcal{S}{i} \times \mathcal{A}_{i} \times \mathcal{R}_{i} \times P{i} \mapsto \mathcal{R}_{i}$  specifies the task. Two environments can differ in the distribution of observations (e.g., background color, camera perspective) or in the task itself (e.g., transition dynamics, action spaces, reward definitions).
Since agents must discover the task solely through reward feedback, any shift—whether in observations or task—can significantly alter their learned representations.

\paragraph{Agents}
Following \textsc{R3L}, each policy $\pi_{u}^{i}$ is typically obtained by end-to-end training on $\mathcal{M}{u}^{i}$. However, we emphasize a modular view of this policy:
\begin{align}
    \pi_{u}^{i}(o_{u}) \;=\; \con_{u}^{i}\bigl[\enc_{u}^{i}(o_{u})\bigr] \;=\; \con_{u}^{i}\bigl(\mathbf{x}_{u}^{i}\bigr)
\end{align}
where $\enc_{u}^{i} : \mathcal{O}_{u} \mapsto \mathcal{X}_{u}^{i}$ serves as the \textit{encoder} that processes raw observations (e.g., images), and $\con_{u}^{i} : \mathcal{X}_{u}^{i} \mapsto \mathcal{A}_{i}$ is the \textit{controller} that outputs actions based on the latent embedding $\mathbf{x}{u}^{i}$. This factorization disentangles observation-specific features (in $\enc_{u}^{i}$) from task-specific decision rules (in $\con_{u}^{i}$).

\paragraph{Latent Representation}
Now consider a second environment $\mathcal{M}_{v}^{j} = (\mathcal{O}_{v}, T_{j})$, where $\mathcal{O}_{v}$ differs from $\mathcal{O}_{u}$ only in visual style (e.g., a shifted color scheme), and suppose we have a policy $\pi_{v}^{j}$ trained on that environment. For two semantically corresponding observations $o_{u} \in \mathcal{O}_{u}$ and $o_{v} \in \mathcal{O}_{v}$, the respective latent embeddings differ:
\begin{align}
\enc_{u}^{i}(o_{u}) \; \neq \; \enc_{v}^{j}(o_{v}) \quad \Longrightarrow \quad \mathbf{x}_{u}^{i} \;\neq\; \mathbf{x}_{v}^{j}
\end{align}
In the next section, we describe how to map one latent space onto another to enable zero-shot stitching of encoders and controllers trained in different visual and task domains, without additional training. %\AR{sposta Mv in environm variations, and la parte di latent differenze in Latent alignment (forse paragrafdetto li)k}
