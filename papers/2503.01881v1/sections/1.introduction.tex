\section{Introduction}

\begin{figure}[h]
    \centering
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,angle=0,origin=c,width=.4\linewidth]{images/teaser_absolute.png}
        %  trim={<left> <lower> <right> <upper>}
        %  \put(horiz, vert)
        %  \put(horiz, vert){\rotatebox{90}{Text}}
        %
        \put(107, 32){$\mathbf{\to}$}
    \end{overpic}\hspace{1cm}
    \begin{overpic}[trim=0cm 0cm 0cm 0cm,clip,angle=0,origin=c,width=.4\linewidth]{images/teaser_translated_yellow.png}
        %  trim={<left> <lower> <right> <upper>}
        %  \put(horiz, vert)
        %  \put(horiz, vert){\rotatebox{90}{Text}}
        %
    \end{overpic}
    \caption{Using translation methods, a controller trained on an environment with a given visual variation \textit{(left)} can be reused without any training or fine-tuning on a different environment (\textit{right}) with comparable performance. In red we see the trajectory of a car driven by the same controller when connected to two different encoders, one for each visual variation.
    }
    \label{fig:teaser}
\end{figure}

Deep Reinforcement Learning (RL) has enabled agents to achieve remarkable performance in complex decision-making tasks, from robotic manipulation to high-dimensional games (Mnih et al., 2015; Silver et al., 2017). 
Although recent RL techniques achieved strong improvements over sample efficiency \citep{yarats2021drqv2, kostrikov2020image}, training new agents remains a costly process, both in computational and temporal terms.
Despite these advances, most methods still require at least partial retraining when dealing with domain shifts such as visual appearance, reward functions, or action spaces \citep{pmlr-v97-cobbe19a, zhang2020learning}. These domain changes typically require expensive retraining, which can be prohibitive for real-world settings that require millions of interactions.

A variety of approaches have been proposed to address these shifting conditions. Domain randomization \citep{tobin2017domain, sadeghi2016cad2rl} trains agents across diverse visual styles or physics settings, promoting invariant features but demanding broader coverage of possible variations. Multi-task RL \citep{parisotto2015actor, teh2017distral} attempts to learn shared representations across multiple tasks.

In the supervised setting, recent representation learning techniques \citep{Moschella2022-yf,maiorca2023latent, norelli2022b, cannistraci2023bricks}, show that it is possible to zero-shot recombine encoders and decoders to perform new tasks across different modalities (images, text..) and tasks (classification, reconstruction) and even architectures.
In RL, methods adopting the relative representation framework \citep{Moschella2022-yf} have shown promising results in adapting encoders to different controllers with zero or few-shots adaptation, for robotic control from proprioceptive states \citep{jian2021adversarial} or for playing games in the Gymnasium suite \citep{towers2024gymnasium} from pixels \citep{ricciardi2025r3lrelativerepresentationsreinforcement}.
These methods, however, still require training models to use the new relative representations.

By contrast, \cite{maiorca2023latent} suggest that modules from independently trained neural networks can be connected via a simple linear or affine transformation, with no training constraint or fine-tuning required, if such transformations can be reliably estimated from a small set of “anchor” samples, pairs of states or observations deemed semantically equivalent.

Our main contribution is the implementation of a RL method based on semantic alignment to map between latent spaces of different neural models, so that their encoders and controllers can be stitched with the goal of creating new agents that can act on visual-task combinations never seen together in training. This includes the use of the transformations to map modules from different networks, and the collection of anchor samples used to estimate these transformations. We call our method Semantic Alignment for Policy Stitching (\textbf{SAPS}).
We perform analyses and empirical tests on the CarRacing and LunarLander environments to show the performance of new agents created via zero-shot stitching of encoders and controllers trained on different visual-task variations, demonstrating significant gains compared to existing zero-shot methods.