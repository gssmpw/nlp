\section{SAPS: Semantic Alignment for Policy Stitching}\label{sec:method-alignment}
Relative representations \citep{Moschella2022-yf}, used as a base for zero-shot stitching in R3L, involve computing a distance function between a set of samples, called \say{anchors}, to project the output of each encoder to a shared latent space, enabling the subsequent training of a universal policy. Semantic alignment, instead, estimates a direct mapping between latent spaces.

Consider the environment $\mathcal{M}_u^j$ for which no dedicated policy exists. However, we do have an encoder $\phi_u^i$ and a controller $\psi_v^j$, extracted from policies $\pi_u^i$ and $\pi_v^j$, respectively. 
We estimate an affine transformation $\tau_u^v$: $\mathcal{X}_{u}^{i} \mapsto \mathcal{X}_{v}^{j}$, mapping embeddings produced by $\phi_u^i$ into the space of $\pi_v^j$. This yields a new latent space:

\begin{align}
    & \tau_u^v(\enc_u^i(\mathbf{o}_u)) \approx \enc_v^j(\mathbf{o}_v)\\
    & \tau_u^v(\mathbf{x}_{u}^i) \approx \mathbf{x}_{v}^j
\end{align}

that is compatible with the existing $\psi_v^j$.
This enables the stitching of encoders and controllers from $\pi_u^i$ and $\pi_v^j$, respectively, to obtain a new policy $\tilde{\pi}_u^j$ that can act in $\mathcal{M}_u^j$, without additional training:
\begin{equation}\label{eq:2}
    \tilde{\pi}_u^j(o_u) = \con_v^j[\tau_u^v(\enc_u^i(\mathbf{o}_u))]
\end{equation}

\paragraph{Estimating $\tau$}
As in \cite{maiorca2023latent}, assume to be given latent spaces $\mathbf{X}_u$ and $\mathbf{X}_v$ which here correspond to the embedding of two visual variations in the space of observations.
We use SVD to obtain an affine transformation $\tau_u^v(\mathbf{x}_u) = \mathbf{R} \mathbf{X}_u + \mathbf{b}$.


\paragraph{Collecting the Dataset.}
The anchor embeddings $\mathbf{X}_u$ and $\mathbf{X}_v$ derive from sets of anchor points $\mathbf{A}_u$ and $\mathbf{A}_v$. Following previous works \citep{maiorca2023latent, Moschella2022-yf, ricciardi2025r3lrelativerepresentationsreinforcement} anchor pair ($\mathbf{a}_u$, $\mathbf{a}_v$) must share a semantic correspondence, meaning both samples represent the same underlying concept (e.g., the same spatial position in a racing track, viewed under two different visual styles).
In supervised learning contexts, anchor pairs can come from paired datasets (e.g., bilingual corpora). In the context of online RL, however, such datasets do not naturally exist. Hence, we collect datasets sharing a correspondence.
This correspondence can be obtained by either rolling out a policy and replaying the same set of actions with different visual variations, as already done in \cite{jian2023policy, ricciardi2025r3lrelativerepresentationsreinforcement}, or by simply applying visual transformations to the image in pixel space. This yields corresponding observation sets $\mathbf{A}_u$ and $\mathbf{A}_v$ that can be embedded by each domainâ€™s encoder to create $\mathbf{X}_u$ and $\mathbf{X}_v$. Finally, we solve for $\tau_u^v$ using the SVD-based procedure above.


%\AR{da inserire forse: Specifically, we estimate $\tau_u^v$, following the technique used in \cite{maiorca2023latent}. which suggests that, given two latent spaces $\mathbf{X} \in \mathbb{R}^{n \times d1}$ and $\mathbf{Y} \in \mathbb{R}^{m \times d2}$ from independently trained deep neural networks, the transformation $\tau$ that directly maps $\mathbf{X}$ to $\mathbf{Y}$: (i) is mostly orthogonal and (ii) can be estimated from a few corresponding elements between the two spaces. In our work, $\mathbf{X}$ and $\mathbf{Y}$ are produced by $\enc_u$ and $\enc_v$, respectively. As in \cite{maiorca2023latent}, we use \textit{Singular Value Decomposition} (SVD) to estimate the optimal orthogonal transformation.}

In our context, we assume that an agent trained end-to-end to solve a specific task in a specific environment will generate a comprehensive set of observations, providing a reasonable approximation of the entire latent space. Nevertheless, forcing the agent to explore more could be beneficial in this context.
In our experiments, we gather parallel samples either by directly translating the observation in pixel space, when there is a well-defined known visual variation between the environments, or by replaying the same sequence of actions in both environments, that in this case must be deterministic and initialized with the same random seed. We leave to future research other possible approximation techniques for translating observations between different environments.