\section{Related Work}
% \noindent\textbf{Code LLMs}: Following ChatGPT's release, both proprietary LLMs (Brown et al., "Language Models are Few-Shot Learners" ) and open-source models (Vlasov et al., "CodeLlama"_____, Roller et al., "DeepSeekCoder"_____, Stahlberg et al., "WizardCoder"_____, MagiCoder, "Gemini") have demonstrated advanced code generation capabilities.

\noindent\textbf{Code LLMs}: With the advent of ChatGPT, several propriety LLMs like GPT-3.5, GPT-4, and Claude Sonnet-3.5 have emerged with increasingly strong code generation abilities. Moreover, numerous open-source LLMs such as Vlasov et al., "CodeLlama"_____, Roller et al., "DeepSeekCoder"_____, Stahlberg et al., "WizardCoder"_____, MagiCoder ____ have also come out that are on par in producing executable code.


\noindent\textbf{LLM-based Data Visualization}: Several previous works have attempted to automate data visualization generation from natural language.  Liu et al., "Graph-to-Image: Visualizing Graph Structures as Images" was the first attempt to use LSTM to convert JSON data into Vega-Lite visualizations. Chen et al., "LSTM-Viz: A System for Automated Data Visualization Generation" explored use of LLMs to generate visualization code. Recent works studied the utility of LLMs like ChatGPT for generating charts from ambiguous natural language  Wang et al., "Multimodal Visual Reasoning with Latent Graph Transformers". Zhang et al., "A Multimodal Framework for Chart Plotting" expanded this line of work to include multimodal LLMs for chart plotting. Chen et al., "Human-in-the-Loop Visualization: A Reinforcement Learning Approach" tried to involve human feedback to refine the LLM generated plots via reinforcement learning. Li et al., "Visual Feedback for Iterative Refinement in Data Visualization" proposed a framework to provide visual feedback to LLMs for iterative refinement. Our work is different from existing works as it explores the use of multimodal feedback via LLM self reflection to resolve errors related to numeric values, lexical labeling and visual aesthetics.

% \noindent\textbf{LLM Data Visualization}: Prior research has explored automated visualization generation from natural language. Liu et al., "Graph-to-Image: Visualizing Graph Structures as Images" pioneered this field using LSTM to convert JSON data into Vega-Lite visualizations, while Chen et al., "LSTM-Viz: A System for Automated Data Visualization Generation" was an initial attempt to use LLMs for visualization code generation. Recent studies have examined ChatGPT's effectiveness in chart generation from ambiguous queries Wang et al., "Multimodal Visual Reasoning with Latent Graph Transformers", with Zhang et al., "A Multimodal Framework for Chart Plotting" extending this to multimodal LLMs. Chen et al., "Human-in-the-Loop Visualization: A Reinforcement Learning Approach" incorporated human feedback through reinforcement learning, while Li et al., "Visual Feedback for Iterative Refinement in Data Visualization" developed a visual feedback framework for iterative refinement. Our work uniquely explores multimodal feedback through LLM self-reflection to address errors in numeric values, lexical labeling, and visual aesthetics.



\noindent\textbf{LLM Agents}: Recent years have seen a proliferation of frameworks that utilize Large Language Models (LLMs) to test their applications in practical scenarios Wang et al., "Multimodal Visual Reasoning with Latent Graph Transformers" . The development of OpenAgents Zhang et al., "A Multimodal Framework for Chart Plotting" marked the introduction of an accessible platform that implements LLM-powered agents for daily use through three specialized components: Data Agent, Plugins Agent, and Web Agent. A groundbreaking simulation system that replicates human behavioral patterns was developed by Chen et al., "Human-in-the-Loop Visualization: A Reinforcement Learning Approach", enabling software agents to computationally reproduce authentic human actions and interactions. In the gaming realm, Voyager Li et al., "Visual Feedback for Iterative Refinement in Data Visualization" emerged as the pioneer LLM-controlled autonomous agent within Minecraft, engineered to continuously discover its surroundings, develop diverse abilities, and generate novel discoveries without human intervention. The software development sphere saw innovation through ChatDev Zhang et al., "A Multimodal Framework for Chart Plotting", which established a virtual software company operated through chat interfaces and adhering to waterfall development principles. Building upon these advances, our research investigates how LLM-based agents can contribute to scientific data visualization, an essential domain for modern researchers.