\section{Related Work}
% \noindent\textbf{Code LLMs}: Following ChatGPT's release, both proprietary LLMs (GPT-3.5, GPT-4, Gemini Pro) and open-source models (CodeLlama \cite{roziere2023code}, DeepSeekCoder \cite{Guo2024DeepSeekCoderWT}, WizardCoder \cite{Luo2023WizardCoderEC}, MagiCoder \cite{Wei2023MagicoderSC}) have demonstrated advanced code generation capabilities.

\noindent\textbf{Code LLMs}: With the advent of ChatGPT, several propriety LLMs like GPT-3.5, GPT-4, and Claude Sonnet-3.5 have emerged with increasingly strong code generation abilities. Moreover, numerous open-source LLMs such as CodeLlama \cite{roziere2023code}, DeepSeekCoder \cite{Guo2024DeepSeekCoderWT}, \cite{Luo2023WizardCoderEC}, \cite{Wei2023MagicoderSC} have also come out that are on par in producing executable code. 


\noindent\textbf{LLM-based Data Visualization}: Several previous works have attempted to automate data visualization generation from natural language. \cite{Dibia2018Data2VisAG} was the first attempt to use LSTM to convert JSON data into Vega-Lite visualizations. \cite{dibia-2023-lida} explored use of LLMs to generate visualization code. Recent works studied the utility of LLMs like ChatGPT for generating charts from ambiguous natural language  \cite{Cheng2023IsGA,tian2024chartgpt}. \cite{berger2024visualization} expanded this line of work to include multimodal LLMs for chart plotting. \cite{xie2024haichart} tried to involve human feedback to refine the LLM generated plots via reinforcement learning. \cite{Yang2024MatPlotAgentMA} proposed a framework to provide visual feedback to LLMs for iterative refinement. Our work is different from existing works as it explores the use of multimodal feedback via LLM self reflection to resolve errors related to numeric values, lexical labeling and visual aesthetics.

% \noindent\textbf{LLM Data Visualization}: Prior research has explored automated visualization generation from natural language. \cite{Dibia2018Data2VisAG} pioneered this field using LSTM to convert JSON data into Vega-Lite visualizations, while \cite{dibia-2023-lida} was an initial attempt to use LLMs for visualization code generation. Recent studies have examined ChatGPT's effectiveness in chart generation from ambiguous queries \cite{Cheng2023IsGA,tian2024chartgpt}, with \cite{berger2024visualization} extending this to multimodal LLMs. \cite{xie2024haichart} incorporated human feedback through reinforcement learning, while \cite{Yang2024MatPlotAgentMA} developed a visual feedback framework for iterative refinement. Our work uniquely explores multimodal feedback through LLM self-reflection to address errors in numeric values, lexical labeling, and visual aesthetics.



\noindent\textbf{LLM Agents}: Recent years have seen a proliferation of frameworks that utilize Large Language Models (LLMs) to test their applications in practical scenarios \cite{Nakano2021WebGPTBQ, yao2022webshop, qin2023webcpm, zhou2023webarena}. The development of OpenAgents \cite{xie2023openagents} marked the introduction of an accessible platform that implements LLM-powered agents for daily use through three specialized components: Data Agent, Plugins Agent, and Web Agent. A groundbreaking simulation system that replicates human behavioral patterns was developed by \cite{park2023generative}, enabling software agents to computationally reproduce authentic human actions and interactions. In the gaming realm, Voyager \cite{Wang2023VoyagerAO} emerged as the pioneer LLM-controlled autonomous agent within Minecraft, engineered to continuously discover its surroundings, develop diverse abilities, and generate novel discoveries without human intervention. The software development sphere saw innovation through ChatDev \cite{Qian2023ChatDevCA}, which established a virtual software company operated through chat interfaces and adhering to waterfall development principles. Building upon these advances, our research investigates how LLM-based agents can contribute to scientific data visualization, an essential domain for modern researchers.