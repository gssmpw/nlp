\section{Related Work}
\subsubsection{Surveys of Time Series Anomaly Detection, Metrics and Definitions}\label{sec:surveys}
In recent years, there has been a wealth of papers reviewing, classifying and benchmarking TAD algorithm and datasets, providing a wide array of insights into the characteristics of algorithms and dataset.
This wave of reviews was started by the seminal work **Keogh, "Anomaly Detection in Time Series Data"** that proclaimed ``Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress'' and showed how simple heuristics can solve most popular benchmark datasets used in the literature at that point.
**Aminikhanghahi, Chiu, "Time Series Anomaly Detection: A Survey"** provide a qualitative definition of different types of anomalies, and introduce formal characterizations of different types of anomalies in an elegant but limited framework.
Given the criticism of existing benchmarks, **Chandola, Banerjee, Kumar, "Anomaly Detection: A Survey on Approaches and Techniques"** and **Hodge, Austin, "A Survey of Outlier Detection Methodologies"** concurrently proposed more expansive benchmarks and provided an extensive evaluation of existing methods, often finding classical and relatively simple methods to work well. **Ruff et al., "Deep Anomaly Detection with Deviation Networks"** also provides a taxonomy and survey of current methods, and evaluates their performance given several synthetic anomaly scenarios. **Liu et al., "Early Anomaly Detection in Streaming Time Series Data"** showed that very simple baselines can not only match, but even outperform complex deep learning solutions, and that the commonly used adjusted F1 metric is flawed. **Khan, Madden, "Algorithms for Anomaly and Outlier Detection"** further disseminate this metric and propose alternatives, which is further built upon by **Chen et al., "Anomaly Detection in Time Series Data Using Autoencoders"**.
**Ruff et al., "Deep Anomaly Detection with Deviation Networks"** updated the benchmark provided by **Khan, Madden, "Algorithms for Anomaly and Outlier Detection"** and used human annotations and consistency rules to curate a diverse subset of the original benchmark that contains consistent labels. 
**Khasahmadi et al., "TAD-Emporium: A Comprehensive Evaluation Framework for Time Series Anomaly Detection"** provides another taxonomy of recently proposed algorithms. **Chandola, Banerjee, Kumar, "Anomaly Detection: A Survey on Approaches and Techniques"** provide a meta-survey, comparing recent benchmarking papers, and introduce a novel experimental protocol.
**Zimek et al., "On Evaluating Clustering in Subspace Projection"** provide a thorough review of different evaluation metrics and their effects on algorithm ranking.

We want to highlight some common themes in these works; first, there is no accepted definition of anomaly, though several definitions have been proposed. However, most definitions are rather qualitative and not formally specified, with the exception of **Aminikhanghahi, Chiu, "Time Series Anomaly Detection: A Survey"**. At the same time, datasets have been criticized either for being overly simple, unrealistic, or inconsistently annotated. We posit that these issues are interrelated, as the vague definition of the problem makes it difficult to define what an appropriate benchmark should look like. Several synthetic benchmarks have been proposed **Chandola, Banerjee, Kumar, "Anomaly Detection: A Survey on Approaches and Techniques"** but without consistent definitions, it's unclear whether the assumptions made there are realistic for real-world scenarios.
A possible explanation for the continued disagreement about evaluation and datasets could be our tenet \applicationspecific, which states that without additional information or assumptions, the problem is ill-specified and potentially unsolvable.

We also want to point out that despite this flood of benchmarks and surveys, drawing conclusions is still extremely difficult **Keogh, "Anomaly Detection in Time Series Data"**; two of the most comprehensive benchmarks, **Ruff et al., "Deep Anomaly Detection with Deviation Networks"** and **Zimek et al., "On Evaluating Clustering in Subspace Projection"** come to very different conclusions: **Chandola, Banerjee, Kumar, "Anomaly Detection: A Survey on Approaches and Techniques"** finds LSTM-AD to be one of the highest ranked algorithms, while **Aminikhanghahi, Chiu, "Time Series Anomaly Detection: A Survey"** finds it to be ranked in the middle, outperformed by simple methods like $k$-means (and **Ruff et al., "Deep Anomaly Detection with Deviation Networks"** ranks is near the bottom), while **Khasahmadi et al., "TAD-Emporium: A Comprehensive Evaluation Framework for Time Series Anomaly Detection"** showed good success with MCD, which was excluded from the analysis in **Chen et al., "Anomaly Detection in Time Series Data Using Autoencoders"** for common failure.

\subsubsection{Streaming}\label{sec:related_streaming}
Despite the extreme prevalence of streaming applications of TAD, we are aware of only two recent works addressing the streaming setting, **Khasahmadi et al., "TAD-Emporium: A Comprehensive Evaluation Framework for Time Series Anomaly Detection"**, who propose a simple score based on spectral residuals, which are then optionally re-calibrated using a CNN trained on synthetic data, and **Liu et al., "Early Anomaly Detection in Streaming Time Series Data"**, who propose a clustering based algorithm using shape-based distances. **Ruff et al., "Deep Anomaly Detection with Deviation Networks"** discuss the ability to operate on streams in their taxonomy, but do not evaluate the streaming setting. Unfortunately, **Chandola, Banerjee, Kumar, "Anomaly Detection: A Survey on Approaches and Techniques"** and **Aminikhanghahi, Chiu, "Time Series Anomaly Detection: A Survey"** only provide limited benchmarks, both in terms of datasets considered and in terms of baseline algorithms.

\subsubsection{Human In The Loop}
Somewhat surprisingly, there is little work on using human annotations in TAD in the academic literature. **Zimek et al., "On Evaluating Clustering in Subspace Projection"** describe a system that is used in a production setting and uses human annotations. However, the system is purely supervised, and requires a significant amount of human annotation. Evaluation is limited to the KPI dataset; however, the work includes many aspects of TAD that are extremely relevant in practical applications, but have been neglected by the academic community. A more recent approach focusing on the user interface, is described in **Khasahmadi et al., "TAD-Emporium: A Comprehensive Evaluation Framework for Time Series Anomaly Detection"**.

\subsubsection{Root Cause Analysis}
Traditionally, root cause analysis (RCA), i.e. finding the cause of an anomaly, has been treated as a separate problem from anomaly detection **Zimek et al., "On Evaluating Clustering in Subspace Projection"**. However, **Khasahmadi et al., "TAD-Emporium: A Comprehensive Evaluation Framework for Time Series Anomaly Detection"** proposed an integrated causal framework for TAD and RCA, which allows for addressing \conditionalanomalies. This work is likely motivated by real applications, given the industry affiliation; however it has received little attention. While there is a large array of work in the operations and BI community, as reviewed by **Zimek et al., "On Evaluating Clustering in Subspace Projection"**, the area seems not well studied within the ML community.

\subsubsection{Signal processing for TAD}
There are two critical signal processing components involved in most TAD applications: resampling or aggregating to a time series, and detecting periodicity lengths. While resampling signals is a well-studied problem in signal processing, the usual goal is to remove artifacts and obtain smooth signals **Khasahmadi et al., "TAD-Emporium: A Comprehensive Evaluation Framework for Time Series Anomaly Detection"** which at odds with the goal of anomaly detection, that usually seeks extreme and unusual values. To the best of our knowledge, this interaction has not been studied.
Finding the dominant periodicity of a signal has been studied in the data mining community **Khasahmadi et al., "TAD-Emporium: A Comprehensive Evaluation Framework for Time Series Anomaly Detection"** and the database community **Khan, Madden, "Algorithms for Anomaly and Outlier Detection"**.
However, despite many TAD algorithms requiring a window length, the impact of this choice has not been systematically studied in any of the surveys on TAD~\ref{sec:surveys}, and this length has been either manually specified, or set using simple heuristics. Recently, **Ruff et al., "Deep Anomaly Detection with Deviation Networks"** evaluated several methods for window size selection; however, the benchmark considers only a small subset of the algorithms considered in the other review articles, and uses the discredited adjusted F1-score.