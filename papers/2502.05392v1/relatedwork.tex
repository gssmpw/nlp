\section{Related Work}
\subsubsection{Surveys of Time Series Anomaly Detection, Metrics and Definitions}\label{sec:surveys}
In recent years, there has been a wealth of papers reviewing, classifying and benchmarking TAD algorithm and datasets, providing a wide array of insights into the characteristics of algorithms and dataset.
This wave of reviews was started by the seminal work \cite{wu2021current} that proclaimed ``Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress'' and showed how simple heuristics can solve most popular benchmark datasets used in the literature at that point.
\citet{lai2021revisiting} provide a qualitative definition of different types of anomalies, and introduce formal characterizations of different types of anomalies in an elegant but limited framework.
Given the criticism of existing benchmarks, \citet{schmidl2022anomaly} and \citet{paparrizos2022tsb} concurrently proposed more expansive benchmarks and provided an extensive evaluation of existing methods, often finding classical and relatively simple methods to work well. \citet{schmidl2022anomaly} also provides a taxonomy and survey of current methods, and evaluates their performance given several synthetic anomaly scenarios. \citet{sarfrazposition} showed that very simple baselines can not only match, but even outperform complex deep learning solutions, and that the commonly used adjusted F1 metric is flawed. \citet{kim2022towards} further disseminate this metric and propose alternatives, which is further built upon by \citet{paparrizos2022volume}.
\citet{liu2024elephant} updated the benchmark provided by \citet{paparrizos2022tsb} and used human annotations and consistency rules to curate a diverse subset of the original benchmark that contains consistent labels. 
\citet{liu2024time} provides another taxonomy of recently proposed algorithms. \citet{mejri2024unsupervised} provide a meta-survey, comparing recent benchmarking papers, and introduce a novel experimental protocol.
\citet{sorbo2024navigating} provide a thorough review of different evaluation metrics and their effects on algorithm ranking.

We want to highlight some common themes in these works; first, there is no accepted definition of anomaly, though several definitions have been proposed. However, most definitions are rather qualitative and not formally specified, with the exception of \citet{lai2021revisiting}. At the same time, datasets have been criticized either for being overly simple, unrealistic, or inconsistently annotated. We posit that these issues are interrelated, as the vague definition of the problem makes it difficult to define what an appropriate benchmark should look like. Several synthetic benchmarks have been proposed~\citep{schmidl2022anomaly, lai2021revisiting} but without consistent definitions, it's unclear whether the assumptions made there are realistic for real-world scenarios.
A possible explanation for the continued disagreement about evaluation and datasets could be our tenet \applicationspecific, which states that without additional information or assumptions, the problem is ill-specified and potentially unsolvable.

We also want to point out that despite this flood of benchmarks and surveys, drawing conclusions is still extremely difficult~\citep{sorbo2024navigating}; two of the most comprehensive benchmarks, \citet{schmidl2022anomaly} and \citet{liu2024elephant} come to very different conclusions: \citet{schmidl2022anomaly} finds LSTM-AD to be one of the highest ranked algorithms, while \citet{liu2024elephant} finds it to be ranked in the middle, outperformed by simple methods like $k$-means (and \citet{paparrizos2022tsb} ranks is near the bottom), while \citet{liu2024elephant} showed good success with MCD~\citep{rousseeuw1999fast}, which was excluded from the analysis in \citet{schmidl2022anomaly} for common failure.
%\footnote{In our own experiments, based on code provided by \citet{liu2024elephant}, LSTM-AD is outperformed by a simple windowing function, while MCD from scikit-learn~\citep{pedregosa2011scikit} is among top algorithms.}


\subsubsection{Streaming}\label{sec:related_streaming}
Despite the extreme prevalence of streaming applications of TAD, we are aware of only two recent works addressing the streaming setting, \citet{ren2019time}, who propose a simple score based on spectral residuals, which are then optionally re-calibrated using a CNN trained on synthetic data, and \citet{boniol2021sand}, who propose a clustering based algorithm using shape-based distances. \citet{schmidl2022anomaly} discuss the ability to operate on streams in their taxonomy, but do not evaluate the streaming setting. Unfortunately, \citet{ren2019time} and \citet{boniol2021sand} only provide limited benchmarks, both in terms of datasets considered and in terms of baseline algorithms.

\subsubsection{Human In The Loop}
Somewhat surprisingly, there is little work on using human annotations in TAD in the academic literature. \citet{liu2015opprentice} describe a system that is used in a production setting and uses human annotations. However, the system is purely supervised, and requires a significant amount of human annotation. Evaluation is limited to the KPI dataset; however, the work includes many aspects of TAD that are extremely relevant in practical applications, but have been neglected by the academic community. A more recent approach focusing on the user interface, is described in \citet{deng2024reliable}.

\subsubsection{Root Cause Analysis}
Traditionally, root cause analysis (RCA), i.e. finding the cause of an anomaly, has been treated as a separate problem from anomaly detection~\citep{soldani2022anomaly}. However, \citet{yang2022causal} proposed an integrated causal framework for TAD and RCA, which allows for addressing \conditionalanomalies. This work is likely motivated by real applications, given the industry affiliation; however it has received little attention. While there is a large array of work in the operations and BI community, as reviewed by \citet{soldani2022anomaly}, the area seems not well studied within the ML community.


\subsubsection{Signal processing for TAD}
There are two critical signal processing components involved in most TAD applications: resampling or aggregating to a time series, and detecting periodicity lengths. While resampling signals is a well-studied problem in signal processing, the usual goal is to remove artifacts and obtain smooth signals~\citep{oppenheim1999discrete} which at odds with the goal of anomaly detection, that usually seeks extreme and unusual values. To the best of our knowledge, this interaction has not been studied.
Finding the dominant periodicity of a signal has been studied in the data mining community~\citep{vlachos2005periodicity} and the database community~\citep{elfeky2005periodicity, wen2021robustperiod}.
However, despite many TAD algorithms requiring a window length, the impact of this choice has not been systematically studied in any of the surveys on TAD~\ref{sec:surveys}, and this length has been either manually specified, or set using simple heuristics. Recently, \cite{ermshaus2023window} evaluated several methods for window size selection; however, the benchmark considers only a small subset of the algorithms considered in the other review articles, and uses the discredited adjusted F1-score.