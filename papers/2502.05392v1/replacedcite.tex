\section{Related Work}
\subsubsection{Surveys of Time Series Anomaly Detection, Metrics and Definitions}\label{sec:surveys}
In recent years, there has been a wealth of papers reviewing, classifying and benchmarking TAD algorithm and datasets, providing a wide array of insights into the characteristics of algorithms and dataset.
This wave of reviews was started by the seminal work ____ that proclaimed ``Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress'' and showed how simple heuristics can solve most popular benchmark datasets used in the literature at that point.
____ provide a qualitative definition of different types of anomalies, and introduce formal characterizations of different types of anomalies in an elegant but limited framework.
Given the criticism of existing benchmarks, ____ and ____ concurrently proposed more expansive benchmarks and provided an extensive evaluation of existing methods, often finding classical and relatively simple methods to work well. ____ also provides a taxonomy and survey of current methods, and evaluates their performance given several synthetic anomaly scenarios. ____ showed that very simple baselines can not only match, but even outperform complex deep learning solutions, and that the commonly used adjusted F1 metric is flawed. ____ further disseminate this metric and propose alternatives, which is further built upon by ____.
____ updated the benchmark provided by ____ and used human annotations and consistency rules to curate a diverse subset of the original benchmark that contains consistent labels. 
____ provides another taxonomy of recently proposed algorithms. ____ provide a meta-survey, comparing recent benchmarking papers, and introduce a novel experimental protocol.
____ provide a thorough review of different evaluation metrics and their effects on algorithm ranking.

We want to highlight some common themes in these works; first, there is no accepted definition of anomaly, though several definitions have been proposed. However, most definitions are rather qualitative and not formally specified, with the exception of ____. At the same time, datasets have been criticized either for being overly simple, unrealistic, or inconsistently annotated. We posit that these issues are interrelated, as the vague definition of the problem makes it difficult to define what an appropriate benchmark should look like. Several synthetic benchmarks have been proposed____ but without consistent definitions, it's unclear whether the assumptions made there are realistic for real-world scenarios.
A possible explanation for the continued disagreement about evaluation and datasets could be our tenet \applicationspecific, which states that without additional information or assumptions, the problem is ill-specified and potentially unsolvable.

We also want to point out that despite this flood of benchmarks and surveys, drawing conclusions is still extremely difficult____; two of the most comprehensive benchmarks, ____ and ____ come to very different conclusions: ____ finds LSTM-AD to be one of the highest ranked algorithms, while ____ finds it to be ranked in the middle, outperformed by simple methods like $k$-means (and ____ ranks is near the bottom), while ____ showed good success with MCD____, which was excluded from the analysis in ____ for common failure.
%\footnote{In our own experiments, based on code provided by ____, LSTM-AD is outperformed by a simple windowing function, while MCD from scikit-learn____ is among top algorithms.}


\subsubsection{Streaming}\label{sec:related_streaming}
Despite the extreme prevalence of streaming applications of TAD, we are aware of only two recent works addressing the streaming setting, ____, who propose a simple score based on spectral residuals, which are then optionally re-calibrated using a CNN trained on synthetic data, and ____, who propose a clustering based algorithm using shape-based distances. ____ discuss the ability to operate on streams in their taxonomy, but do not evaluate the streaming setting. Unfortunately, ____ and ____ only provide limited benchmarks, both in terms of datasets considered and in terms of baseline algorithms.

\subsubsection{Human In The Loop}
Somewhat surprisingly, there is little work on using human annotations in TAD in the academic literature. ____ describe a system that is used in a production setting and uses human annotations. However, the system is purely supervised, and requires a significant amount of human annotation. Evaluation is limited to the KPI dataset; however, the work includes many aspects of TAD that are extremely relevant in practical applications, but have been neglected by the academic community. A more recent approach focusing on the user interface, is described in ____.

\subsubsection{Root Cause Analysis}
Traditionally, root cause analysis (RCA), i.e. finding the cause of an anomaly, has been treated as a separate problem from anomaly detection____. However, ____ proposed an integrated causal framework for TAD and RCA, which allows for addressing \conditionalanomalies. This work is likely motivated by real applications, given the industry affiliation; however it has received little attention. While there is a large array of work in the operations and BI community, as reviewed by ____, the area seems not well studied within the ML community.


\subsubsection{Signal processing for TAD}
There are two critical signal processing components involved in most TAD applications: resampling or aggregating to a time series, and detecting periodicity lengths. While resampling signals is a well-studied problem in signal processing, the usual goal is to remove artifacts and obtain smooth signals____ which at odds with the goal of anomaly detection, that usually seeks extreme and unusual values. To the best of our knowledge, this interaction has not been studied.
Finding the dominant periodicity of a signal has been studied in the data mining community____ and the database community____.
However, despite many TAD algorithms requiring a window length, the impact of this choice has not been systematically studied in any of the surveys on TAD~\ref{sec:surveys}, and this length has been either manually specified, or set using simple heuristics. Recently, ____ evaluated several methods for window size selection; however, the benchmark considers only a small subset of the algorithms considered in the other review articles, and uses the discredited adjusted F1-score.