% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{makecell}
\usepackage{fvextra}
\usepackage{listings}
\usepackage{times}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{lipsum}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{multirow}
% \usepackage{algpseudocode}
% \usepackage{algorithm}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{pbox}
\usepackage{float}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{diagbox}
\usepackage{float}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{mathtools}
% \usepackage{color, colortbl}
\definecolor{grey}{rgb}{0.898,0.898,0.898}
\definecolor{ForestGreen}{rgb}{0.133,0.545,0.133}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\linespread{0.97}
% \usepackage[usenames, dvipsnames]{xcolor}
% \usepackage[table]{xcolor} % For row colors
% \usepackage{colortbl}      % For cell colors
\usepackage{booktabs}      % For table formatting
\usepackage{array}         % For column formatting
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{adjustbox}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% Feel free to remove
% \usepackage[dvipsnames]{xcolor} 


\newcommand{\todo}[1]{\textcolor{red}{~TODO:~#1}}

\title{Detecting Ambiguities to Guide Query Rewrite for Robust Conversations in Enterprise AI Assistants}



\author{Md Mehrab Tanjim$^1$, Xiang Chen$^1$, Victor S. Bursztyn$^1$, Uttaran Bhattacharya$^1$, Tung Mai$^1$,\\
\textbf{Vaishnavi Muppala$^2$, Akash Maharaj$^2$, Saayan Mitra$^1$, Eunyee Koh$^1$, Yunyao Li$^2$, Ken Russell$^2$}\\
$^1$Adobe Research, $^2$Adobe Inc.\\
  \texttt{\{tanjim, xiangche, soaresbu, ubhattac, tumai, mvaishna, maharaj,}\\ 
  \texttt{smitra, eunyee, yunyaol, kenrusse\}@adobe.com} \\}
  

\begin{document}
\maketitle
\begin{abstract}
Multi-turn conversations with an Enterprise AI Assistant can be challenging due to conversational dependencies in questions, leading to ambiguities and errors. To address this, we propose an NLU-NLG framework for ambiguity detection and resolution through reformulating query automatically and introduce a new task called ``Ambiguity-guided Query Rewrite.'' To detect ambiguities, we develop a taxonomy based on real user conversational logs and draw insights from it to design rules and extract features for a classifier which yields superior performance in detecting ambiguous queries, outperforming LLM-based baselines. Furthermore, coupling the query rewrite module with our ambiguity detecting classifier shows that this end-to-end framework can effectively mitigate ambiguities without risking unnecessary insertions of unwanted phrases for clear queries, leading to an improvement in the overall performance of the AI Assistant. Due to its significance, this has been deployed in the real world application, namely Adobe Experience Platform AI Assistant\footnote{\href{https://business.adobe.com/products/sensei/ai-assistant.html}{https://business.adobe.com/products/sensei/ai-assistant.html}}.
% \footnote{Deployed in June, 2024 in \href{https://business.adobe.com/products/sensei/ai-assistant.html}{AEP AI Assistant}}.

\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have become increasingly popular and are now being integrated into many applications, such as document summarization \cite{kurisinkel2023llm, zakkas2024sumblogger},
information retrieval \cite{anand2023context, ma2023query}, conversational question answering (CQA) \cite{zhang2020dialogpt, thoppilan2022lamda, xu2023baize}, and so on.
Particularly in marketing and data analytics, LLMs can provide valuable information from documentation or general SQL queries to reveal data-related insights.
These digital experiences can be commonly referred to as AI Assistants \cite{maharaj2024evaluation}. 
These systems often involve multi-turn conversations that have conversational dependencies, including omissions, ambiguities, and coreferences \cite{anantha2021open, adlakha2022topiocqa, zhang2024clamber}, as illustrated in Figure 1\footnote{Please note throughout the paper we have used dummy names instead of the original ones for the customer specific dataset names and IDs for confidentiality.} (middle), where the second query, ``How many do I have?'' may not yield the correct answer without additional context from the first query ``What is a segment?" from the history.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.pdf} %,height=4cm
    \caption{Multi-turn conversations can have dependencies in prior chats, leading to ambiguities and errors (middle). While LLM-based rewriting can resolve some ambiguities, it may also introduce errors through unwanted rephrasing (left). Our proposed NLU-NLG framework, Ambiguity-guided Query Rewrite, rewrites only predicted unclear queries to prevent unnecessary rewrites leading to correct answers (right).}
    \label{fig:teaser}
    % \vspace{-0.4cm}
\end{figure}

To disambiguate, an emerging approach is to prompt an LLM to rewrite the query based on previous chat history \cite{wang2023query2doc, jagerman2023query}.  
In the example in \Cref{fig:teaser} 
% a simple prompt to
a prompted \texttt{GPT-3.5-Turbo}\footnote{\href{https://openai.com/}{https://openai.com/}}
% , such as ``\textit{Rewrite the current query based on the previous chat history to remove any ambiguities},''
for reformulating the query based on chat history can yield ``How many segments do I have?,'' which is a specific query that the AI assistant can answer correctly. 
This is known as Query Rewrite (QR) \cite{anand2023context, ma2023query}.
Prompting LLM for QR is a simple, yet effective solution to mitigate ambiguities in the query. However, rewriting all queries using LLM can lead to another problem. If we rewrite all queries by default, it can increase the risk of errors due to LLMs' tendency to hallucinate. This is shown in \Cref{fig:teaser} (left).
% For example, in our internally deployed AI Assistant for our B2B product, we have seen ? out of ? queries were clear enough which did not need a rewrite and when rewriting them, ? of them lead to incorrect behavior/answer, which is quite a significant number.
To mitigate this problem it is important to determine if a query rewrite is necessary or not. 
Especially if query is specific to begin with, it might be unnecessary to rewrite a query. This is illustrated in \Cref{fig:teaser} (right), where only `ambiguous' queries are rewritten and `clear' queries are bypassed leading to correct behavior for both of the queries.  By detecting unambiguous queries and skipping unnecessary rewrites, the chance of errors 
% due to hallucinations
can be reduced ensuring an improvement in overall performance. 


In this paper, we address this challenge
% we present a novel framework to first determine
by first determining whether an incoming query is ambiguous or not using an ambiguity detecting classifier, which is our Natural Language Understanding (NLU) component and, if ambiguous, resolving it automatically by reformulating the query using a Natural Language Generation (NLG) component (e.g., an LLM). 
% Our proposed NLU-NLG framework for this task called Ambiguity-guided Query Rewrite. By using our proposed approach, AI Assistants can more accurately and effectively detect and resolve ambiguities in user queries, leading to an overall improvement in their performance. This is illustrated in \Cref{fig:teaser}.
Specifically, our contributions in this paper are as follows:
% \textbf{NLU-NLG Framework for Ambiguity Detection and Resolution:}
\textbf{1)} We propose a novel NLU-NLG framework that addresses ambiguity detection and resolution through query rewriting and introduce a new task called ``Ambiguity-guided Query Rewrite'' (as shown in the right of \Cref{fig:teaser}) for robust multi-turn conversations in Enterprise AI Assistants. This task can serve as a standard approach for practitioners to build assistants using LLMs that can effectively handle ambiguous user queries. 
To the best of our knowledge, such a pipeline for disambiguation has not been explored before. 
% By using our proposed approach, AI Assistants can more accurately and effectively detect and resolve ambiguities in user queries, leading to an overall improvement in their performance. 
% \textbf{Taxonomy of Ambiguities in Conversational Logs:}
\textbf{2)} We analyze the types of ambiguities that can arise in user conversational logs with AI Assistants and develop a taxonomy to understand the nature of these ambiguities. This taxonomy helps in creating specific detection systems tailored for each category of ambiguity.
% \textbf{Hybrid Approach for Ambiguity Detection:}
\textbf{3) }
% During the development of the ambiguity detection module, we observed that only about 12\% of user queries are ambiguous, which poses a challenge in training an NLU model for ambiguity detection. To address this, we 
We use insights from our taxonomy to find useful features beyond text and devise rules for generating synthetic data points and improving detection. Our hybrid approach, which includes rule-based detection, data augmentation, and feature extraction, outperforms existing methods, including LLM-based baselines.
It is important to note that our proposed ambiguity-guided query rewrite pipeline is agnostic to any specific instance of the underlying QR model, making our solution 
% generalized and 
ready to adopt in any industry setting. Because of its importance, our proposed pipeline has been integrated into Adobe Experience Platform (AEP) AI Assistant.
% , one of the flagship products by Adobe.
% \todo{mention deployed and when did we deploy}

% However, we encountered an issue when using an LLM, specifically GPT 3.5 Turbo, to detect ambiguities. It tended to label even clear queries as "ambiguous," making it less applicable in real-world scenarios. To solve this problem, we had to analyze user logs and discover patterns to train a small language model to detect common ambiguities found in multi-turn queries that could benefit from query rewriting.  

\section{Related Work}
\textbf{Query Rewrite.} 
% Query rewrite or reformulation (QR) encompasses a range of methods designed to handle underspecified and ambiguous queries by reformulating them into self-contained versions suitable for retrieval or question answering systems. 
Historically, Query rewrite or reformulation (QR) methods have included the addition of terms to the original query, known as query expansion \cite{lavrenko2017relevance}, or iteratively rephrasing the query using similar phrases \cite{zukerman2002lexical}, or synonymous terms \cite{jones2006generating}. 
% When human-generated rewrites or reward signals are available, language models (LMs) are trained for question rewriting (QR) also \cite{elgohary2019can, anantha2021open, vakulenko2021question, qian2022explicit, ma2023query}.
Recently, the advent of large language models (LLMs) has spurred exploration into using these generative models to automatically resolve ambiguities during query processing.
% , thereby enhancing query modeling for downstream tasks. These tasks often involve improving information retrieval in a single QA setting. 
For instance, recent studies have prompted LLMs to provide detailed information, such as expected documents or pseudo-answers \cite{wang2023query2doc, jagerman2023query}. These techniques are particularly effective when a golden dataset for a specific domain is unavailable, necessitating the use of off-the-shelf LLMs tailored for the specific use-case.
% In our deployed system, we employ a similar approach by prompting LLMs, specifically \texttt{GPT-3.5 Turbo}, due to its clearance by our legal team for our use-cases and comparatively lower cost than GPT-4. 
However, a
% s highlighted in other studies , there are two primary limitations of using LLMs for query rewriting: 
LLM-based QR can suffer from concept drift when using only queries as prompts \cite{anand2023context} and also has high inference costs during query processing. 
% Our system has encountered similar issues, where the model undesirably inserted or modified parts of the text. 
To address both, we introduce an ambiguous query understanding component to guide the rewrite process, ensuring that only unclear queries are rewritten, thereby limiting the undesired rewriting problem and saving cost at the same time. 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/pipeline_and_architecture.pdf} %,height=4cm
    % \vspace{-0.35cm}
    \caption{Left: Proposed pipeline. Right: Architecture of our proposed ambiguity detection model.}
    \label{fig:pipline_and_architecture}
    \vspace{-0.5cm}
    %%\squeezeup
\end{figure*}

\textbf{Ambiguity Detection.} 
\label{sec:related_work}
Ambiguous queries are typically those that have multiple distinct meanings, insufficiently defined subtopics \cite{clarke2009overview}, syntactic ambiguities \cite{schlangen2004causes}, for which a system struggles to interpret accurately, resulting in inappropriate or unclear answers \cite{keyvan2022approach}. Detecting ambiguity in user queries, however, is a challenging task, as highlighted by several studies \cite{braslavski2017you, trienes2019identifying, guo2021abgcoqa}. Notably, \cite{trienes2019identifying} conducted the first comprehensive study on classifying questions as clear or unclear using logistic regression, especially in community platforms like Stack Exchange.
% However, their findings are not fully applicable to enterprise use-cases, as their research was limited to single questions and answers in community settings and they did not delve into the types of ambiguities that may arise in conversational agents or dialogue systems. 
% That being said, their research indicated that deep learning-based models outperform logistic regression models in text classification tasks, although logistic regression was ultimately used in their work for its interpretability and explainability in user interfaces. 
% A related study \cite{dhole2020resolving} tackled the task of detecting ambiguous queries on task-oriented dialogue systems (such as opening a bank account). However, these studies did not analyze the different types of ambiguities that might occur in conversational question answering (CQA). 
In CQA, \citet{guo2021abgcoqa} examined various ambiguities from a given text or story and also proposed methods for ambiguity detection and generating clarifying questions. 
% Interestingly, they framed ambiguity detection as a traditional question-answering task, adding ``ambiguous'' as a potential output, and built models based on BERT \cite{devlin2019bert}. Their best performing model achieved an F1 score of only 23.6\% on their dataset, which highlights the difficulty of detecting ambiguities in CQA task.
In the era of large language models (LLMs), researchers like \cite{kuhn2022clam, zhang2024clamber} have shown that these models can be prompted to detect ambiguous questions. However, \citet{zhang2024clamber} pointed out the limitations of LLMs in accurately detecting them, which our experimental results also validate. 
% Similar to these studies, we also experience a lower performance when we use LLM, namely GPT-3.5 Turbo, for ambiguity detection.  
% \citet{zhang2024clamber} also developed a taxonomy of ambiguities when conversing with LLM for information retrieval but omitted certain categories, such as syntactic and pragmatic ambiguities, which we found to be common in enterprise conversational agents. 
In addition, all these research works focus on ambiguity detection for generating clarifying questions rather than query rewriting. In enterprise AI Assistants, however, avoiding clarifying questions is crucial for a smooth user experience, making automatic ambiguity resolution using QR
% , such as query rewriting, 
highly desirable. Furthermore, existing works share insights from open-domain datasets which might not always translate to industries as enterprise datasets are often close-domain. To that end, this paper investigates the effectiveness of combining ambiguity detection with query rewriting to resolve ambiguities automatically, sharing best practices for industry settings.

\section{Approach}

\subsection{Proposed Pipeline}

\Cref{fig:pipline_and_architecture} (left) illustrates our proposed ambiguity-guided query rewrite framework. In this system, the incoming query first passes through a Natural Language Understanding (NLU) component, i.e., an ambiguity detection classifier, to determine if it is ambiguous. If the query is clear, it is routed directly to the agent for conversational question answering. If ambiguous, it is rewritten by a Natural Language Generation (NLG) component for disambiguation. In our case, the NLG is a prompted LLM-based ``Query Rewrite'' (QR) module. 
% However it is important to note that our NLU task for detecting ambiguous queries is independent of any specific NLG component, e.g., there could be an NLG for ``clarifying question generation'' module instead of QR, making our solution easily integratable with existing conversational question-answering systems or AI assistants. Next, we discuss the technical details of the classifier, beginning with the taxonomy of ambiguity types. In this paper, we will also discuss some key elements that a QR prompt should include for optimal performance, while maintaining the confidentiality of our internal prompt.

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{p{1.5cm}p{6.5cm}p{3.5cm}p{7.5cm}}
\toprule
\textbf{Type} & \textbf{Definition} & \textbf{Example} & \textbf{Explanation} \\ 
\midrule
\multirow{2}{2cm}{Pragmatic\\(63.55\%)} & The meaning of a sentence depends on the context, reference, or scope. & \textit{``How many do I have?''} &  It is not clear what the user is referring to for the total count.\\ \hline
\multirow{2}{2cm}{Syntactic\\(31.90\%)} & The structure of a sentence is incomplete or allows for multiple interpretations. & \textit{``Business event''} &  It is not clear what the user is asking about ``business event.''\\ \hline
\multirow{2}{2cm}{Lexical\\(4.55\%)} & The meaning of the word/term is not clear or has multiple interpretations. & \textit{``Are we removing abc123 from XYZ?''} &  It is not clear what kinds of business objects (e.g., segment or dataset) \textit{abc123}/\textit{XYZ} are referring to.\\ 
\bottomrule
\end{tabular}}
\caption{The proposed taxonomy of ambiguous queries along with examples and explanations.}
\label{tab:taxonomy}
\vspace{-0.4cm}
\end{table*}

\subsection{Taxonomy of Ambiguous Queries}
To design a classifier that can detect unclear queries requiring rewriting, we need to first analyze the types of ambiguities that arise in real-world CQA systems. Categorizing these ambiguities can help us understand their nature and frequency, allowing us to prioritize the most common types and identify key signals for better feature design. Additionally, this analysis provides insights into generating synthetic data points to enhance model training. To that end, we analyzed about $3k$ user logs, with $400$ annotated as ambiguous, and developed a simple, yet effective taxonomy of three major ambiguity types: Pragmatic, Syntactical and Lexical, which are presented in \Cref{tab:taxonomy}. It is worth noting that while some taxonomies exist in the literature, such as \citet{zhang2024clamber}, these are often from open domains
% dataset-specific
and not derived from real-world deployed systems. Therefore, they may not provide relevant insights for an industry CQA system. For instance, they overlook syntactical and pragmatic ambiguities, which are predominant in our system. 
% We further divide the major types into sub-types for more granularity.
Our findings indicate that the most frequent ambiguities are pragmatic in nature, often referring to previous chats or responses. This insight allows us to create rules for synthetically generating additional data points for ambiguous queries, 
% augmenting the dataset given
to circumvent the problem of having a low number of ambiguous queries (more on this later). We also notice that most of the lexical ambiguities follow a set of specific regular expression patterns for lexicons as well as a simple rule which makes the query ambiguous. We will touch on this next in our ambiguity detection discussion.

\subsection{Detecting Ambiguities}
One straightforward approach to detect ambiguities is to prompt-engineer another LLM to detect ambiguities. In our experiments, however, we have discovered that using an LLM to automatically detect ambiguities tended to label clear queries as ``ambiguous,'' lowering the Precision and limiting its applicability to resolve the aforementioned problem. To address this issue, we first draw some insights from our proposed taxonomy in \Cref{tab:taxonomy} and discover patterns.
% to overcome data imbalance and train a small language model to detect ambiguous queries, which outperforms LLM-based alternatives for ambiguity detection.
Our developed taxonomy in \Cref{tab:taxonomy} enables us to devise two different approaches to handle the top-level types of ambiguities. Specifically, we discover that a small language model is capable of understanding whether a query has syntactical or pragmatic ambiguities, while rule-based detection of lexical ambiguities work quite well as from language modeling perspective there is nothing wrong with lexical ambiguous queries.

\subsubsection{Pragmatic and Syntactical Ambiguities}
% \cite{trienes2019identifying}, we first explore the lengths of the ambiguous queries as a dominant feature.
% To detect these types of ambiguous queries, we initially explored the features proposed by \citet{trienes2019identifying}, which primarily focused on the length of the query. However, we discovered that this approach was not always effective, as some short queries, especially related to definitions (e.g., ``What is a segment?'') can be unambiguous. However, o
To detect these types of ambiguous queries, our analysis of conversational logs led to an important insight that pragmatic ambiguities often arise due to usage of referential words
% (e.g., \textit{it, them, those, above,} etc.) and their usage, 
while syntactic ambiguities stem from inherent faults in the sentence structure. 
To capture both, we explore the following features: 1) Query Length ($f_{QL}$): total number of words in a query,  2) Referential Count ($f_{RC}$): total count of words from this list: [`this', `that', `those', `it', `its', `some', `others', `another', `other', `them', `above', `previous'], 3) Readability ($f_{CLI}$): Coleman-Liau Index (CLI) \cite{coleman1975computer} is a readability test designed to gauge the structure of a text, calculated by the following: $\text{CLI} = 5.89L/W - 30S/W - 15.8 $, where $L, S, W$  are the number of letters, words, and sentences respectively. \Cref{fig:statistics_of_features} shows the statistics which clearly show a pattern between clear and ambiguous queries. 

In addition to these features, the textual feature of the query itself plays a vital role.
% We have found that both of these types of ambiguity can be detected by training a small language model.
% , even without having application-specific knowledge. 
For example, even readers who are not familiar with our system can distinguish between ``What is a segment?'' (clear) and ``What is it?'' (pragmatic ambiguity due to the usage of ``it'') or ``segment?'' (syntactic ambiguity due to grammatical structure). Therefore, we hypothesize that a language model would be better equipped to discern the difference, and our experiments have validated this hypothesis. 

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\columnwidth]{figures/statistics_of_features.png} %,height=4cm
    \caption{The box plots of the features from our training set show a clear difference in distribution between clear and ambiguous queries\protect\footnotemark.}
    \label{fig:statistics_of_features}
    %%\squeezeup
    \vspace{-.3cm}
\end{figure}
\footnotetext{To account for outliers, we use robust scaling from \texttt{scikit-learn} \cite{scikit-learn} which removes the median and scales the data according to the quantile range.}

To incorporate both of these modalities of features, we first use a pretrained Sentence-Transformer \cite{reimers-2019-sentence-bert}, $ST_{\phi}$, where $\phi$ denotes the pretrained parameters as the underlying language model to get the text embedding, $\textbf{t}=ST_\phi(q)$, where $q$ is the query text. Then we use fully connected layers, $FC_{\theta}$ 
with $\theta$ parameters, 
% (depending on the dimension of the input features)
which take both text embedding, $\textbf{t}$ and the robust normalized numerical feature vector, $\textbf{f}=[f_{QL}, f_{RC}, f_{CLI}]$, to get a final prediction, $\hat{y}=FC_{\theta}(\textbf{t}, \textbf{f})$. We train both $ST_\phi$ and $FC_{\theta}$ via backpropagation using cross entropy loss. Finally, since there is a class-imbalance, we use a weighted sampling during training. \Cref{fig:pipline_and_architecture} (right) shows the architecture of our ambiguity classifier. For more details, please see \Cref{sec:config}.
% detecting model.

% and fine-tune it on our training dataset. Additionally, inspired by \citet{trienes2019identifying}, we use sentence length as an additional feature, but we can potentially use any useful features. This constitute as the NLU model for detecting ambiguous queries which is shown in Figure \ref{fig:pipline_and_architecture} (right). Since both pragmatic and syntactical ambiguities can be modeled by a language model, this architecture gives us good support to detect both types well. However, the same cannot be said about lexical ambiguities as these queries can be structurally correct without usage of any referential words, making them challenging to detect with this approach. 

\subsubsection{Lexical Ambiguities}
To detect lexical ambiguities,
% we first need to identify the lexicon or unknown terms. In the case of a Marketing AI Assistant, this often relates to customer data-related queries. To detect the terms or words that belong to specific databases, a linking-to-database component is necessary, which can detect these types of entities (such as the name of the dataset) and their types. However, during our analysis of the query logs, 
we observed that simple cases of data-related entities can easily be detected by a set of regular expressions, which led us to design simple, yet effective rules. Specifically,  
\begin{itemize}[noitemsep]
    \item Mask data-related entities by: 1) removing any weblinks, 2) filtering any ordinal numbers (like 1st, 2nd, etc.) and hyphen-separated words if they are commonly used in English (like pre-requisite), 3) matching words/phrases within a single or double quotation mark or with digits, periods, colon, underscore and dash, and finally 4) masking the matched words/phrases with ``ENTITY.''
    \item If the prediction from the model is ``clear'' but the entity types (which are pre-defined words from our business objects) are missing after masking, then label it as ``ambiguous.''
\end{itemize}
% For example, we observed that entities often appear in single or double quotation marks and are often a phrase with numbers and special characters (such as underscore, dash, color, question mark, etc.). Based on these observations, we derived a set of regular expressions to capture such cases. We then mask these terms and replace them with ``ENTITY.''  We observed that after masking, ambiguous queries often lack the required business objects needed to identify the type of the masked object. For example, ``What is abc123?'' would be transformed after applying the rule as ``What is ENTITY?'' Since we do not know the entity type (e.g., it could be the name of a dataset or an attribute of a dataset), a simple rule to detect the missing object type is quite effective in detecting these types of queries. For brevity, we describe the details of the set of regular expressions as well as rules in the \Cref{sec:lexicon}. 
We show this rule in action with an example in \Cref{sec:lexicon}. A key takeaway from this exercise is that these rules can be modified and changed as per the requirements of the specific industry setting and based on customer data.

\subsection{Query Rewrite}
% \todo{mentioned GPT version and also indicate that we have included chat history}
For the QR, we use
% compare with two LLMs, namely 
\texttt{GPT-3.5-Turbo} (version 1106)
% \footnote{We do not compare with GPT-4 as we do not use it our products due to its higher operational costs.} for deployment. 
However, it is important to note that our framework is not dependent on any specific LLM. Any prompted LLM 
% that uses previous chat history 
can be used. Therefore, for generalizability of our solution, we also experiment with an open-source LLM, namely \texttt{Llama-3.1-70B}\footnote{\href{https://huggingface.co/meta-llama/Meta-Llama-3.1-70B}{https://huggingface.co/meta-llama/Meta-Llama-3.1-70B}}.
% for their clearance from our legal team.
% as it has been cleared for our specific use-case by our legal team. 
 % One simple prompt for QR could be as follows: \textcolor{ForestGreen}{``Rewrite the current query based on the previous chat history to remove any ambiguities.''} But t
 There can be multiple approaches to designing a QR module. One simple prompt for QR could be as follows: \textcolor{ForestGreen}{``Rewrite the current query based on the previous chat history to remove any ambiguities.''}  While the specifics of our internal approach could not be shared due to legal constraints, we can offer valuable insights based on our model and prompt experimentation. In our prompt, we have used surrounding context, including the chat history of the past five interactions and relevant passage snippets. Then we have included 
% A robust QR module should include 
instructions to transform the current user query into a fully specified query based on the context (e.g, chat history). We also have included instruction to
% It should also 
specifically address and clarify any ambiguous pronouns for co-reference resolution, correct typographical errors, and accurately preserve user-provided entity values. By following these guidelines, we enhance the accuracy and relevance of rewritten query in our AI Assistant.
% , and while we cannot share our internal prompt due to legal reasons, we can offer insights on best practices. Our experiments with multiple prompts have shown that a good QR prompt should include instructions to transform the current user query into a fully specified query, consider the previous chat history, resolve any vague pronouns, correct typos, and preserve the user-provided entity values.


\section{Experimental Setup}

\textbf{Data.} We collect user queries from the conversational logs with our AI Assistant and ask our domain experts to annotate these queries either `clear' or `ambiguous.'
% The annotation guideline was as follows:
Based on 
% the real conversations with user in our production 
the conversational logs, our training set originally contained $3402$ queries, with $414$ being ambiguous,
% (only $12\%$)
and the test set had $489$ queries, of which only $84$ were ambiguous.
% Only 12\% of the training queries are ambiguous.
% From the annotated data, our training set has $3402$ queries of which $414$ ambiguous queries and for testing, we hold out an additional $1036$ queries of which $403$ are unclear. As can be seen, the number of ambiguous queries in training is quite low, accounting for only 12\% of total queries.   
%This can pose some challenge to train our model. To address the minority class imbalance problem, we take advantage of our insight from our taxonomy that most of the ambiguous queries are pragmatic in nature, where they either have missing context, use referential words, or use indefinite phrases. We also leverage the rules in our lexical ambiguity detection. Based on these insights, we augment our training dataset by synthetically generating a total of 1372 ambiguous queries following these rules: 1) omit any proper nouns, 2) randomly insert referential words, 3) create vague statements, 4) Mask the queries and remove the entity types. More details on rule-based synthesis, including the regular expressions and the list of all the referential words and indefinite phrases, can be found in the  \Cref{sec:aug}. 
To address minority class imbalance in ambiguous queries for training,
% In addition, 
we augment our training dataset by synthetically generating $1372$ queries based on insights from our taxonomy and rules from our lexical ambiguity detection. These rules include omitting proper nouns, randomly inserting referential words, creating vague statements, and masking queries by removing entity types. More details on this rule-based synthesis can be found in \Cref{sec:aug}.
% The main takeaway is that while our underlying dataset may be different from others, our rules are generic enough to be applicable in any industry setting to augment the dataset, and not limited to ours. 
For stress-testing our ambiguity detecting classifier for real-world deployment,  
we similarly augment our test set, but instead of using rules, human annotators were employed to come up with more ambiguous questions. The final test dataset contained a balanced set of $1036$ queries, with $403$ being unclear.
% \todo{mention the real distribution and then sysntheticall generate by user}. 
Finally, for comparing the query rewrite framework, we collect $366$ multi-turn conversations with golden rewrites, i.e., rewritten queries by human experts.
% to make them clear. 
% In addition, these queries contain `intent' labels for measuring the performance of downstreaming task.
\begin{table}[tbp]  
    \centering  
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lcc}  
        \toprule
        \textbf{Model} & \textbf{F1} & \textbf{Accuracy}  \\
        \midrule
        SimQ\textsuperscript{$\star$} & 32.4 & 67.08 \\ 
        Abg-CoQA\textsuperscript{$\star\star$} & 73.44 & 83.25 \\
        % TC w/ BERT & 85.71 & 83.37 & \underline{84.53} & \underline{88.12} \\
        Few-shot \texttt{GPT-3.5-Turbo}  & 71.58 & 72.68 \\
        Few-shot \texttt{Llama-3.1-70B}  & \underline{78.02} & \underline{79.14}\\
        \midrule
        \midrule
        Ours  & \textbf{90.19}$^\dagger$& \textbf{92.16}$^\dagger$ \\
        \bottomrule 
        \multicolumn{3}{l}{$\star$ based on \citet{trienes2019identifying}}\\
        \multicolumn{3}{l}{$\star\star$ based on \citet{guo2021abgcoqa}} \\
        \multicolumn{3}{l}{$\dagger$ results are statistically significant ($p<0.001$)} 
        \end{tabular}  
    }
\vspace{-0.2cm}
\caption{Comparison of performance across various models for detecting ambiguous queries.
}
\label{tab:comparison_ambiguity}
\vspace{-0.4cm}
\end{table}

\textbf{Baselines.}
% We compare the performance of our ambiguity detection model with prior works that detect ambiguities based on either manual or textual features. The
For ambiguity detection, first baseline is a logistic regression model from \citet{trienes2019identifying} called ``SimgQ,'' which uses hand-selected features (such as query length).
% among others. 
The other baseline is referred to as ``Abg-CoQA'' from \citet{guo2021abgcoqa}, which treats ambiguity detection as a question answering task (mentioned in \Cref{sec:related_work}).
% which is based on \citet{guo2021abgcoqa}, where the authors uses text classification models, such as BERT, to detect ambiguities from textual features. However, our design of this baseline differs from theirs, as we used BERT to extract textual features and introduced additional layers for text classification, while they used it for question answering task. 
Additionally, inspired by \citet{zhang2024clamber}, we compare with LLM-based alternatives, where we prompt \texttt{GPT-3.5-Turbo} and \texttt{Llama-3.1-70B} with few-shot examples to detect ambiguities. 
% (full prompt in \Cref{app:full_prompt})
% for this task. 
More details on model configurations including ours and full prompt appear in \Cref{sec:config}.
% with few-shot examples to output either `VAGUE' or `CLEAR.' 
% We also present various variations of our model for comparison: 1) Base: we use the base sentence transformer model, 2) Rule: we use rules for masking and lexical ambiguity detection, 3) AD: train with \textbf{A}ugmented \textbf{D}ata, 4) AF: include the query length, referential count and readability (\Cref{fig:statistics_of_features}) as an \textbf{A}dditional \textbf{F}eatures, and finally 5) WS: train with \textbf{W}eighted \textbf{S}ampling to account for class imbalance. 
To compare our proposed framework ``Ambiguity-guided Query Rewrite,'' we used two baselines: ``No Query Rewrites,'' where we do not have any QR, and ``Always Rewriting Query,'' where we always rewrite the query irrespective of its clarity.
% whether it is clear or not.
% in multi-turn conversations.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{p{2.5cm}p{8.5cm}p{10cm}}
\toprule
\textbf{Original Query} & \textbf{Always Rewriting Query} & \textbf{Ambiguity-guided Query Rewrite} \\ 
\midrule
% What is a schema? keep it short & \texttt{GPT-3.5-Turbo:}What is a schema?, \texttt{Llama-3.1-70B:}What is a schema? & Predicted Ambiguity: \textcolor{ForestGreen}{clear}; What is a schema? \textcolor{ForestGreen}{keep it short}\\ \hline
\textcolor{orange}{What is the id of ``ABC Dataset (created on)''?} 
& \texttt{\underline{GPT-3.5-Turbo:}}What is the id of the \textcolor{red}{``ABC Dataset''}?, \texttt{\underline{Llama-3.1-70B:}}What is the id of the \textcolor{red}{``ABC Dataset''?}; \underline{Response: }\textcolor{red}{Sorry there is no such dataset.} 
\vspace{0.2cm}
&  \underline{Predicted Ambiguity:} \textcolor{blue}{clear}; \textcolor{orange}{What is the id of ``ABC Dataset (created on)''?}; \underline{Response:} \textcolor{ForestGreen}{The id is 1234.} 
\\
% & \vspace{0.4cm} & \vspace{0.4cm} 
\textcolor{orange}{What are its attributes?} 
& \texttt{\underline{GPT-3.5-Turbo:}} What attributes does the dataset \textcolor{red}{with the id} ``ABC Dataset (created on)'' have?, \texttt{\underline{Llama-3.1-70B:}} \textcolor{red}{What attributes would a dataset have?} 
&  \underline{Predicted Ambiguity:} \textcolor{blue}{ambiguous}; \texttt{\underline{GPT-3.5-Turbo:}} What are the attributes of \textcolor{ForestGreen}{dataset 1234?}, \texttt{\underline{Llama-3.1-70B:}} What are the attributes of dataset \textcolor{ForestGreen}{``ABC Dataset (created on)'' with id 1234?}\\
% \midrule \midrule
% \makecell{Turn 1: \\ Turn 2: } & \makecell{Turn 1: \\ Turn 2: } &  \makecell{Turn 1: \\ Turn 2: }\\ 
\bottomrule
\end{tabular}}
\vspace{-0.2cm}
\caption{Illustrative examples on how our ambiguity-guided query rewrite leads to the correct behavior.}
\label{tab:qualitative}
\vspace{-0.4cm}
\end{table*}

\begin{table}[tbp]  
    \centering  
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ccccc|c|c}  
        \toprule
        \multicolumn{5}{c|}{\textbf{\texttt{all-mpnet-base-v2}}} & \makecell{\textbf{\texttt{all-distil}}\\\textbf{\texttt{roberta-v1}}} &  \makecell{\textbf{\texttt{all-MiniLM}}\\\textbf{\texttt{-L12-v2}}} \\
        % \cmidrule(lr){2-6} \cmidrule(lr){7-7} & \cmidrule(lr){8-8}
         \textbf{Base} & \textbf{Base+R} & \makecell{\textbf{Base+R}\\\textbf{+AD}} & \makecell{\textbf{Base+R}\\\textbf{+AD+AF}} & \makecell{\textbf{All}\textsuperscript{$\star$}} & \makecell{\textbf{All}\textsuperscript{$\star$}} & \makecell{\textbf{All}\textsuperscript{$\star$}}\\
        \midrule
         88.29 & 89.92  & 91.91 & 92.04 & \textbf{93.37} & 92.33 & 92.36 \\ 
        \bottomrule 
        \multicolumn{5}{l}{$\star$ All=Base+R+AD+AF+WS}\\
        \end{tabular}  
    }
% \vspace{-0.3cm}
\caption{Recall for ``ambiguous'' queries shows the contribution of using rules (R), augmented data (AD), additional feature (AF) and weighted sampling (WS).}
\label{tab:ablation_study}
\vspace{-0.5cm}
\end{table}


% \begin{table}[t] 
%     \vspace{-0.25cm}
%     \centering  
%     % \setlength{\tabcolsep}{2pt} % Reduce horizontal padding  
%     % \renewcommand{\arraystretch}{0.75} % Reduce vertical padding
%     % \resizebox{\linewidth}{!}
%     \begin{adjustbox}{max width=\linewidth, max totalheight=\textheight}
%     {
%         \begin{tabular}{lccc|ccc}  
%         \toprule
%          \multirow{2}{*}{\makecell{\textbf{Disambiguation} \textbf{Frameworks}}} & \multicolumn{3}{c|}{\textbf{\texttt{GPT-3.5-Turbo}}} & \multicolumn{3}{c}{\textbf{\texttt{Llama-3.1-70B}}} \\
%         % \cmidrule(lr){2-6} \cmidrule(lr){7-7} & \cmidrule(lr){8-8}
%          & \rotatebox{0}{\textbf{BLEU Score}} & \rotatebox{0}{\textbf{Cosine Sim.}} & \rotatebox{0}{\textbf{BERTScore}}   & \rotatebox{0}{\textbf{BLEU Score}} & \rotatebox{0}{\textbf{Cosine Sim.}} & \rotatebox{0}{\textbf{BERTScore}} \\
%         \midrule
%         No Query Rewrite & 0.47 & 0.6863 & 0.8187  & 0.47 & 0.6863 & 0.8187 \\
%         Always Rewriting Query & 0.4755 & 0.8274 & 0.8529 & 0.4301 & 0.8089 & 0.8441 \\
%         Ambiguity-guided Rewrite  & \textbf{0.5417} & \textbf{0.847} & \textbf{0.8680 } & \textbf{0.5314}  & \textbf{0.8410} & \textbf{0.8653} \\
%         \bottomrule 
%         \end{tabular}  
%     }
%     \end{adjustbox}
% \vspace{-0.2cm}
% \caption{Quantitative comparison with golden rewrites show the effectiveness of our proposed framework. For fair comparison, we use the same prompt in both LLMs.}
% \label{tab:framework_comparison}
% \vspace{-0.1cm}
% \end{table}

\textbf{Metrics.} For ambiguity detection, we use F1, and accuracy. For experiments related to the framework, we use cosine similarity\footnote{using \texttt{text-embedding-3-large} from OpenAI}, BERTScore \cite{zhang2019bertscore}, and BLEU score (average of 1 \& 2-gram) to measure similarities with ground truth rewritten queries. 
% We also use Entities IoU to measure how much of the customer-specific words are retained after the rewrite. 
% Additionally, we use the accuracy of the ground truth intents compared to  detected ``intent'' on the rewritten queries by an intent classifier, which is one of our downstream tasks to route queries to the proper agents (if properly rewritten, it would be accurately routed). For all of these metrics, higher values indicate better performance.

\section{Experiments and Results}


% \begin{figure}[tbp]
%     \centering
%     \includegraphics[width=\linewidth]{figures/framework_results.png} %,height=4cm
%     \caption{Our proposed ``Ambiguity-guided Query Rewrite'' for disambiguation improves the performance across all metrics.}
%     \label{fig:framework_results}
%     \vspace{-0.3cm}
% \end{figure}

% \begin{table}[t]  
%     \centering  
%     \resizebox{\columnwidth}{!}{
%         \begin{tabular}{lccc}  
%         \toprule
%          & \textbf{all-MiniLM-L12-v2} & \textbf{all-distilroberta-v1} & \textbf{all-mpnet-base-v2}\\
%         \midrule
%         Recall & 92.36 & 92.33  & \textbf{93.37}\\ 
%         \bottomrule 
%         \end{tabular}  
%     }
% \caption{Ablation study with different backbones.}
% \label{tab:ablation_study}
% \vspace{-0.1cm}
% \end{table}

% \begin{figure}[tbp]
%     \centering
%     \includegraphics[width=0.8\linewidth, height=4cm]{figures/ablation_study.png} %,height=4cm
%     \caption{}
%     \label{fig:ablation_study}
%     % \vspace{-0.3cm}
% \end{figure}

We compare the performance of our ambiguity detection model with several baselines and show the results in \Cref{tab:comparison_ambiguity} (average numbers are reported over 10 runs).
% Please note in our use-case it is important to have a \textit{high Recall} as otherwise the system will miss important rewrites for the ambiguous queries. 
Our findings indicate that SimQ, which uses hand-picked features, resulted in poorest performance. This is expected, as textual features which play a vital role to understand the query, were not used by \citet{trienes2019identifying}. On the other hand, Abg-CoQA saw a slight improvement in using textual features, but its performance in both F1 and accuracy is still lower overall. This result aligns with their original finding \cite{guo2021abgcoqa} that ambiguity detection as a question answering task has its limitations. 
% Interestingly, their approach has the highest Precision, but since they have a low F1 and accuracy, it means their model tends to incorrectly predict ``clear'' for most of the ambiguous queries. Conversely, LLM-based baselines tend to predict ``ambiguous'' for most clear queries as evidenced by their higher Recall but lower F1 and accuracy, especially \texttt{Llama-3.1-70B}, which limits their usefulness in solving the problem discussed in our paper. 
LLM-based baselines did better than previous two but still the overall performance is not satisfactory, especially for deployment. This relatively lower
% F1 and accuracy score in 
performance of LLM-based ambiguity detection in our case
% We also observe that the LLM performed poorly in detecting ambiguous queries, which is not surprising, given that they are trained on structured, well-aligned, and clean datasets for generating coherent sentences. 
% These finding of ours 
% is also consistent with 
also aligns with the study in
\citet{zhang2024clamber}. Our proposed ambiguity detection model, on the other hand, outperforms all of the baselines under both F1 and Accuracy. \Cref{tab:ablation_study} demonstrates that each introduced component, such as rules for masking and detecting lexical ambiguities, additional features, and weighted sampling for class imbalance, contributed to performance gains. In addition, \texttt{all-mpnet-base-v2} provided the best performance compared to other pre-trained sentence-transformers, consistent with its leaderboard ranking (\href{https://www.sbert.net/docs/sentence_transformer/pretrained_models.html}{sbert.net}).
% \footnote{\href{https://www.sbert.net/docs/sentence_transformer/pretrained_models.html}{https://www.sbert.net}}.
% This shows how following the guidelines presented in paper can help practitioners to have a state-of-the-art ambiguity detecting classifier for their own use-cases.
% We provide further ablation study of the impact of different features and different backbone Sentence-Transformer architectures 
% % (e.g., \texttt{all-mpnet-base-v2}, \texttt{all-distilroberta-v1}, and \texttt{all-MiniLM-L12-v2}) 
% in the \Cref{sec:ablation}.
Since ambiguity-detecting has a higher performance, we can expect the ambiguity-guided rewrite to reduce hallucinations or unwanted rewrites, which we find to be the case in our experiments; \Cref{tab:framework_comparison} shows our proposed framework indeed outperforms all alternatives across all metrics.
% Since our ambiguity detecting classifier has a high performance, we can expect the ambiguity-guided rewrite to alleviate the problem of hallucinations or unwanted rewrite.
% % for the clear questions. 
% % as Finally, 
% % we compare our proposed ``Ambiguity-guided Query Rewrite'' framework with two alternative approaches, ``No Rewrite'' and ``Always Rewriting Query'', as shown in 
% This is indeed the behavior we see in \Cref{tab:framework_comparison} where the results show that our proposed framework outperforms the other alternatives across all metrics. 
% For example, our framework improved the intent accuracy, which is a downstream task that ensures the correct behavior of the system and leads to correct answers. 
This is because the ground truth rewritten query is close to the original query if it is clear and thus by skipping QR if detected clear, the output semantically matches more with ground truths.
% s are due to the ambiguity detecting classifier preventing unnecessary rewrites. 
Since similarity metrics alone cannot capture the cascading effect of wrong rewrites, we also demonstrate an illustrative example inspired by a real conversation from one of our users in \Cref{tab:qualitative}. 
% The original query is in orange, and the incorrect rewrites and responses are highlighted in red, where correct portions are shown in green. 
In these examples, the first query asks for the ID of the dataset named ``ABC Dataset (created on),'' but QR without guidance incorrectly removed ``(created on)'' from the name, leading to an incorrect response. Normally such small 
% re-ordering or 
rephrasing would not be an issue, however, in the context of generating the correct SQL query, this can lead to a significant error, like dataset not found. 
% For example, t
The previous mistake of the incorrect rewriting of the dataset name then leads to another incorrect rewrite in the second turn, where the LLM mistakes the name as ID (\texttt{GPT-3.5-Turbo}) or leads to a wrong question all together (\texttt{Llama-3.1-70B}). Our proposed framework avoids such mistakes through guided rewriting as highlighted in green, resulting in correct behavior in all cases.
% For example, `dataset 1234' can still be looked up using its id to find its attributes. 
Not surprisingly, after productionizing our proposed pipeline, the error related to downstream tasks (such as routing to proper agents, generating SQL queries, etc.) in our application reduced from \textbf{18\% to 8\%}. As an additional outcome, since the latency of our ambiguity classifier is merely \textbf{0.01 seconds}, our ambiguity-guided rewrite approach lowers both costs and latency associated with LLM calls by bypassing QR for clear queries. 
Overall, our quantitative and qualitative results highlight the importance of 
% our proposed ambiguity-guided approach for disambiguation tasks. By 
providing guidance for query rewrites for improve the performance of the overall system.
% and provide more accurate answers.
% to users.



\begin{table}[t]  
    \centering  
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccc|ccc}  
        \toprule
         \multirow{5}{*}{\makecell{\textbf{Disambiguation} \\\textbf{Frameworks}}} & \multicolumn{3}{c|}{\textbf{\texttt{GPT-3.5-Turbo}}} & \multicolumn{3}{c}{\textbf{\texttt{Llama-3.1-70B}}} \\
        % \cmidrule(lr){2-6} \cmidrule(lr){7-7} & \cmidrule(lr){8-8}
         & \rotatebox{90}{\textbf{BLEU Score}} & \rotatebox{90}{\textbf{Cosine Sim.}} & \rotatebox{90}{\textbf{BERTScore}} & \rotatebox{90}{\textbf{BLEU Score}} & \rotatebox{90}{\textbf{Cosine Sim.}} & \rotatebox{90}{\textbf{BERTScore}}  \\
        \midrule
        No Query Rewrite & 0.47 & 0.6863 & 0.8187  & 0.47 & 0.6863 & 0.8187 \\
        Always Rewriting Query & 0.4755 & 0.8274 & 0.8529 & 0.4301 & 0.8089 & 0.8441 \\
        Ambiguity-guided Rewrite  & \textbf{0.5417} & \textbf{0.847} & \textbf{0.8680 } & \textbf{0.5314}  & \textbf{0.8410} & \textbf{0.8653} \\
        \bottomrule 
        \end{tabular}  
    }
\vspace{-0.3cm}
\caption{Quantitative comparison with golden rewrites show the effectiveness of our proposed framework. For fair comparison, we use the same prompt in both LLMs.}
\label{tab:framework_comparison}
\vspace{-0.5cm}
\end{table}

\vspace{-0.1cm}
\section{Conclusion}
\vspace{-0.1cm}
In this paper, we have explored the challenges of ambiguity in conversations with LLM-based agents
% , referred to as AI Assistants,
in the enterprise settings. To mitigate this challenge and ensure the correct behavior of such intelligent systems, we have proposed 
% a simple yet effective for ambiguity detection and 
an NLU-NLG framework for disambiguation through ambiguity-guided query rewriting. Our approach, which combines rules and models with a mix of hand-picked and textual features, effectively detects ambiguous queries. 
% We believe the insights we collect from our proposed taxonomy of ambiguities can also be a valuable best practice for designing an in-house classifier to detect application-specific ambiguities in multi-turn conversations. 
Furthermore, our findings demonstrate that query rewriting guided by detected ambiguity results in superior performance for disambiguation and robust conversations with correct behavior, ultimately leading to its deployment into our product. Overall, we hope our insights will provide valuable guidance for researchers and practitioners working on conversational systems and AI Assistants. 
% Our hybrid approach and proposed framework have shown promising results in detecting ambiguous queries and mitigating the risk of errors due to hallucinations. 

% Overall,  We believe that this insight can serve as a valuable best practice for designing an in-house classifier to detect ambiguities in multi-turn conversations. 
% Lexical ambiguities, future work
% parts-of-text detection
% where the ambiguities arises.

\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Adlakha et~al.(2022)Adlakha, Dhuliawala, Suleman, de~Vries, and Reddy}]{adlakha2022topiocqa}
Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de~Vries, and Siva Reddy. 2022.
\newblock Topiocqa: Open-domain conversational question answering with topic switching.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:468--483.

\bibitem[{Anand et~al.(2023)Anand, Setty, Anand et~al.}]{anand2023context}
Abhijit Anand, Vinay Setty, Avishek Anand, et~al. 2023.
\newblock Context aware query rewriting for text rankers using llm.
\newblock \emph{arXiv preprint arXiv:2308.16753}.

\bibitem[{Anantha et~al.(2021)Anantha, Vakulenko, Tu, Longpre, Pulman, and Chappidi}]{anantha2021open}
Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021.
\newblock Open-domain question answering goes conversational via question rewriting.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 520--534.

\bibitem[{Braslavski et~al.(2017)Braslavski, Savenkov, Agichtein, and Dubatovka}]{braslavski2017you}
Pavel Braslavski, Denis Savenkov, Eugene Agichtein, and Alina Dubatovka. 2017.
\newblock What do you mean exactly? analyzing clarification questions in cqa.
\newblock In \emph{Proceedings of the 2017 conference on conference human information interaction and retrieval}, pages 345--348.

\bibitem[{Clarke et~al.(2009)Clarke, Craswell, and Soboroff}]{clarke2009overview}
Charles~LA Clarke, Nick Craswell, and Ian Soboroff. 2009.
\newblock Overview of the trec 2009 web track.
\newblock In \emph{Trec}, volume~9, pages 20--29.

\bibitem[{Coleman and Liau(1975)}]{coleman1975computer}
Meri Coleman and Ta~Lin Liau. 1975.
\newblock A computer readability formula designed for machine scoring.
\newblock \emph{Journal of Applied Psychology}, 60(2):283.

\bibitem[{Guo et~al.(2021)Guo, Zhang, Reddy, and Alikhani}]{guo2021abgcoqa}
Meiqi Guo, Mingda Zhang, Siva Reddy, and Malihe Alikhani. 2021.
\newblock \href {https://openreview.net/forum?id=SlDZ1o8FsJU} {Abg-co{QA}: Clarifying ambiguity in conversational question answering}.
\newblock In \emph{3rd Conference on Automated Knowledge Base Construction}.

\bibitem[{Jagerman et~al.(2023)Jagerman, Zhuang, Qin, Wang, and Bendersky}]{jagerman2023query}
Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023.
\newblock Query expansion by prompting large language models.
\newblock \emph{arXiv preprint arXiv:2305.03653}.

\bibitem[{Jones et~al.(2006)Jones, Rey, Madani, and Greiner}]{jones2006generating}
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006.
\newblock Generating query substitutions.
\newblock In \emph{Proceedings of the 15th international conference on World Wide Web}, pages 387--396.

\bibitem[{Keyvan and Huang(2022)}]{keyvan2022approach}
Kimiya Keyvan and Jimmy~Xiangji Huang. 2022.
\newblock How to approach ambiguous queries in conversational search: A survey of techniques, approaches, tools, and challenges.
\newblock \emph{ACM Computing Surveys}, 55(6):1--40.

\bibitem[{Kuhn et~al.(2022)Kuhn, Gal, and Farquhar}]{kuhn2022clam}
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2022.
\newblock Clam: Selective clarification for ambiguous questions with generative language models.
\newblock \emph{arXiv preprint arXiv:2212.07769}.

\bibitem[{Kurisinkel and Chen(2023)}]{kurisinkel2023llm}
Litton~J Kurisinkel and Nancy~F Chen. 2023.
\newblock Llm based multi-document summarization exploiting main-event biased monotone submodular content extraction.
\newblock \emph{arXiv preprint arXiv:2310.03414}.

\bibitem[{Lavrenko and Croft(2017)}]{lavrenko2017relevance}
Victor Lavrenko and W~Bruce Croft. 2017.
\newblock Relevance-based language models.
\newblock In \emph{ACM SIGIR Forum}, volume~51, pages 260--267. ACM New York, NY, USA.

\bibitem[{Ma et~al.(2023)Ma, Gong, He, Zhao, and Duan}]{ma2023query}
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.
\newblock Query rewriting for retrieval-augmented large language models.
\newblock \emph{arXiv preprint arXiv:2305.14283}.

\bibitem[{Maharaj et~al.(2024)Maharaj, Qian, Bhattacharya, Fang, Galatanu, Garg, Hanessian, Kapoor, Russell, Vaithyanathan, and Li}]{maharaj2024evaluation}
Akash Maharaj, Kun Qian, Uttaran Bhattacharya, Sally Fang, Horia Galatanu, Manas Garg, Rachel Hanessian, Nishant Kapoor, Ken Russell, Shivakumar Vaithyanathan, and Yunyao Li. 2024.
\newblock \href {https://aclanthology.org/2024.dash-1.3} {Evaluation and continual improvement for an enterprise {AI} assistant}.
\newblock In \emph{Proceedings of the Fifth Workshop on Data Science with Human-in-the-Loop (DaSH 2024)}, pages 17--24, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[{Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay. 2011.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:2825--2830.

\bibitem[{Reddy et~al.(2019)Reddy, Chen, and Manning}]{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D Manning. 2019.
\newblock Coqa: A conversational question answering challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:249--266.

\bibitem[{Reimers and Gurevych(2019)}]{reimers-2019-sentence-bert}
Nils Reimers and Iryna Gurevych. 2019.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 3982--3992.

\bibitem[{Schlangen(2004)}]{schlangen2004causes}
David Schlangen. 2004.
\newblock Causes and strategies for requesting clarification in dialogue.
\newblock In \emph{Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue at HLT-NAACL 2004}, pages 136--143.

\bibitem[{Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du et~al.}]{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du, et~al. 2022.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}.

\bibitem[{Trienes and Balog(2019)}]{trienes2019identifying}
Jan Trienes and Krisztian Balog. 2019.
\newblock Identifying unclear questions in community question answering websites.
\newblock In \emph{Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14--18, 2019, Proceedings, Part I 41}, pages 276--289. Springer.

\bibitem[{Wang et~al.(2023)Wang, Yang, and Wei}]{wang2023query2doc}
Liang Wang, Nan Yang, and Furu Wei. 2023.
\newblock Query2doc: Query expansion with large language models.
\newblock \emph{arXiv preprint arXiv:2303.07678}.

\bibitem[{Xu et~al.(2023)Xu, Guo, Duan, and McAuley}]{xu2023baize}
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023.
\newblock Baize: An open-source chat model with parameter-efficient tuning on self-chat data.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 6268--6278.

\bibitem[{Zakkas et~al.(2024)Zakkas, Verberne, and Zavrel}]{zakkas2024sumblogger}
Pavlos Zakkas, Suzan Verberne, and Jakub Zavrel. 2024.
\newblock Sumblogger: Abstractive summarization of large collections of scientific articles.
\newblock In \emph{European Conference on Information Retrieval}, pages 371--386. Springer.

\bibitem[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2019bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019.
\newblock Bertscore: Evaluating text generation with bert.
\newblock \emph{arXiv preprint arXiv:1904.09675}.

\bibitem[{Zhang et~al.(2024)Zhang, Qin, Deng, Huang, Lei, Liu, Jin, Liang, and Chua}]{zhang2024clamber}
Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, and Tat-Seng Chua. 2024.
\newblock Clamber: A benchmark of identifying and clarifying ambiguous information needs in large language models.

\bibitem[{Zhang et~al.(2020)Zhang, Sun, Galley, Chen, Brockett, Gao, Gao, Liu, and Dolan}]{zhang2020dialogpt}
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and William~B Dolan. 2020.
\newblock Dialogpt: Large-scale generative pre-training for conversational response generation.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations}, pages 270--278.

\bibitem[{Zukerman and Raskutti(2002)}]{zukerman2002lexical}
Ingrid Zukerman and Bhavani Raskutti. 2002.
\newblock Lexical query paraphrasing for document retrieval.
\newblock In \emph{COLING 2002: The 19th International Conference on Computational Linguistics}.

\end{thebibliography}


\appendix

\section{Model Configurations and Prompt }
\label{sec:config}
\textbf{Baselines.} For SimQ, we use the code provided by \citet{trienes2019identifying}. Their model takes the title and body of a Stack Exchange post as well as corresponding clarifying questions. Since we only have query texts, we provide the query text as input to the title and leave other fields empty. For Abg-CoQA, we use the setup mentioned in \citet{guo2021abgcoqa} and pretrain BERT-base model on CoQA dataset \cite{reddy2019coqa} and their proposed Abg-CoQA dataset and then fine tune on our dataset. They frame ambiguity detection as question answering task which needs a question, chat history, and input passage, where they insert ``ambiguous'' in the end for the ambiguous query. Following their setup, we also give the query and chat history as inputs, but since we do not have any passage, we use `unknown. clear. ambiguous' as input passage so that it would extract `clear' for clear queries, `ambiguous' for ambiguous queries, and `unknown' if the query does not have label (which happens for queries in the chat history).

\textbf{Our model.} For our model, we use three different backbones from the pretrained Sentence Transformer models which have the highest sentence embeddings performance in the leaderboard on \href{https://www.sbert.net/docs/sentence_transformer/pretrained_models.html}{https://www.sbert.net/docs/ sentence\_transformer/pretrained\_models.html}: \texttt{all-mpnet-base-v2}, \texttt{all-distilroberta-v1} and \texttt{all-MiniLM-L12-v2}. From them, the first two outputs $768$ dimensional vector for the text while the last one outputs $384$ dimensional vector. As such the input dimension for our fully connected layers $FC_\theta$ also change accordingly based on the backbone. $FC_\theta$ has two layers, the input-output size of first layer is ($768+3$, $384$) for \texttt{all-mpnet-base-v2} and \texttt{all-distilroberta-v1} or ($384+3$, $384$) for \texttt{all-MiniLM-L12-v2} (the additional $3$ is due to the $3$-dimensional feature vector for query length, referential count, and readability), followed by a \texttt{Tanh} activation, a dropout layer and a final fully connected layer  of ($384$, $2$) for the prediction. For training we use a learning rate of $2e-05$, batch size of $4$, and $3$ epochs with Adam optimizer. We also use a weighted sampling to have a balanced number of `clear' and `ambiguous' queries in each batch ($1:1$ ratio). We calculate validation loss every $50$ steps and save the best model which gives the highest average of both Recall and F1 for the ``ambiguous'' class.

% \section{Prompt for LLM-based Ambiguity Detection}
% \label{app:full_prompt}
\textbf{Prompt template for LLM-based detection.} We use the following prompt to detect ambiguity in query. Please note any specific word/phrase belonging to our internal data in few-shot examples has been censored with `***' for confidentiality. 

% \textbf{Prompt}: 
\begin{Verbatim}[breaklines=true]
prompt = f'''You are an expert linguist. You are provided with a question that a customer is asking to a conversational assistant, your task is to evaluate the clarity of question. Note that: 

* If the current question is complete and customer's intent is clear, output "RESPONSE: CLEAR" on a separate line. 

* If the current question is smart-talk or chit-chat, output "RESPONSE: CLEAR" on a separate line.

* If the current question is ambiguous or need clarification from the customer, output "RESPONSE: VAGUE" on a separate line. 

* If the current question contains coreference and becomes clear after coreference resolution, output "RESPONSE: VAGUE" on a separate line. 

* if the current question contains missing pronouns or conditions based on the conversation history, output "RESPONSE: VAGUE" on a separate line. 

* Do not include any explanation. 

Examples: 

QUESTION: How can I connect *** with ***? 
RESPONSE: CLEAR 

QUESTION: What is the *** from *** to populating in ***? 
RESPONSE: CLEAR 

QUESTION: When should I use the product? 
RESPONSE: VAGUE 

QUESTION: what is the data retention policy in ***? 
RESPONSE: CLEAR 

QUESTION: What is the license in this case? 
RESPONSE: VAGUE 

QUESTION: what's ***? 
RESPONSE: CLEAR 

QUESTION: what does this page do? 
RESPONSE: VAGUE 

CURRENT QUESTION:{curr_query}'''
\end{Verbatim}

\section{Lexical Ambiguity Detection with an Example}
\label{sec:lexicon}
% We apply following steps to detect entities: 
% \begin{itemize}
%     \item We have observed that when data related queries occur, it often is within a single or double quotation mark. This is our first rule. 
%     \item Then we look for the parts of the text where special characters and numbers appear along with characters. Please note there are some special cases, where these are not always indicative of entities, such as ordinal numbers (like 3rd party) or dash separated valid words (like re-entry). We filter those terms from our matched patterns.  
% \end{itemize}


% These enable to mask a query with entities. 
When we match any term that fit into our patterns, we mask them by ``ENTITY.'' An example of this conversion is: \textit{``What is the total size of 124abcde?''}, which will be converted to \textit{``What is the total size of ENTITY?''} This further enable us to devise a simple rule to detect lexical ambiguities. The rule is as mentioned in the paper: \textcolor{orange}{If a masked query has an ENTITY in it but does not have any words for the ENTITY types, then it has a lexical ambiguity.} 

For looking up entity types, we have a pre-defined list of words, which is internal to our company, but for example's sake, we can use the entity types mentioned in the paper such as `segment', `schema' or `dataset.'  
% use the following types: [``webpage'', ``batch'', ``profile'', ``attribute'', ``schema'', ``dataset'', ``source'', ``destination'', ``segment'', ``audience'', ``campaign'', ``journey'', ``offer''] 

We have found this rule to be quite effective for detecting simple cases of lexical ambiguities. For example, in the above case, the query will be detected as `ambiguous' as it is not clear what the entity is referring to (it could be a dataset or segment or something else). 

% That being said, it cannot handle some complex queries with multiple entity types, such as ``Is there any segment with abc123?'', which will be converted to ``Is there any segment with ENTITY?''. Here the rule will fail, as there are both ENTITY and entity type in the query, but unfortunately, the ENTITY is not the entity type itself, rather some property of it (e.g., abc123 could be the name of the segment or an attribute of it).   
  
% To handle such hard cases, we can definitely benefit from an advanced linking module, which can further validate each entity against an underlying database. However, in its absence, we can still leverage our current solution in the following way: as can be seen, the entity masking enable us to convert complicated data related questions into an easy-to-follow templated natural queries, on which a language model can be applied. If we can simply annotate the masked questions, the language model should be able to capture the pattern for the hard cases. To that end, we have annotated ~250 such masked queries to train our language model to handle such hard queries.  

\section{Data Augmentation}
\label{sec:aug}
We synthetically generate total $1372$ queries that needs a rewrite by following these rules: 1) omit any proper nouns, 2) insert referential pronouns randomly and 3) creating vague statements using phrases like `there is no such', `there is not any' etc. Specifically: 

1) Omitting Details: In this rule, we first match any sentences with the regex match `the (\textbackslash w+) of', which will find the patter ``the \{\} of''. Then we remove the words after ``of'' (inclusive), to make the sentence vague. For example, ``What is the name of my largest dataset?'' to ``What is the name?'' In this fashion, we generate $80$ unclear queries. 

2) Adding Referential Pronouns: In this rule, we first use the $80$ generated unclear queries above where it has the word ``the'' followed by a noun. Then we find the occurrences of ``the'' and replace it randomly with any of the referential words from this list:  \textcolor{orange}{[`this,' `that, `those,' `it,' `its,' `some,' `others,' `another,' `other,' `above', `previous']}. For example, applying this rule to ``What is the name?'' can generate ``What is this name?.'' We repeat the process $5$ times yielding $270$ unclear queries. Furthermore, we take all the clear queries with sentence length less than or equal to $7$ and then apply the same rule $5$ times giving us $353$ more unclear queries. Total unclear queries from this rule is: $623$. 

3) Vague Statements: For this rule, we first filter all the queries which are non-questions (detected by the absence of `?' in the query). Then, using parts-of-speech tagger, we find all the sentences that start with a verb, followed by a pronoun. Then we replace both verb and the pronoun randomly with any of the following phrases: \textcolor{orange}{[`there is', `there are', `there is no such', `there is no', `there are no such', `there are no', `there is not any,' `it is', `it is not', `this is not', `this is', `that is', `that is not']}. For example, applying this rule will find statements like ``Tell me about `ABC' dataset'' and generate vague statement like ``There is no such `ABC' dataset''. Repeating this process $5$ times for each sentence yields $669$ vague statements. 

    
%ablation study
% all-mpnet-base-v2
% - % Precision & Recall & F1 & Accuracy (All for ambiguous class)
% Ours (base model) & 0.91629 &  0.88289 &  0.89918 & 0.9236363636363636 \\% stds: 0.01233422 0.00966928 0.00631376 005005434193539539
%         +Augmented Data & 91.33    & 88.44 & 89.84 &  \textbf{92.21} \\
%         +Additional Feature & 90.68 & 89.13 & \textbf{89.87} & 92.18 \\
%         +Rules for Lexical & 87.97 & \textbf{91.39} & 89.64 & 92.04 \\


% \begin{table}[tbp]  
%     \centering  
%     \resizebox{\columnwidth}{!}{
%         \begin{tabular}{lcccc}  
%         \toprule
%         \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Accuracy}  \\
%         \midrule
%         SimQ\textsuperscript{$\star$} & 80.39 & 20.34 & 32.4 & 67.08 \\ 
%         Abg-CoQA\textsuperscript{$\star\star$} & \textbf{94.53} & 60.05 & 73.44 & 83.25 \\
%         % TC w/ BERT & 85.71 & 83.37 & \underline{84.53} & \underline{88.12} \\
%         Few-shot \texttt{GPT-3.5-Turbo} & 61.87 & 84.91 & 71.58 & 72.68 \\
%         Few-shot \texttt{Llama-3.1-70B} & 73.49  & \underline{91.84} & \underline{81.65} & \underline{84.08}\\
%         \midrule
%         \midrule
%         Ours & \underline{87.24} & \textbf{93.37} & \textbf{90.19}& \textbf{92.16} \\
%         \bottomrule 
%         \multicolumn{5}{l}{$\star$ based on \citet{trienes2019identifying}}\\
%         \multicolumn{5}{l}{$\star\star$ based on \citet{guo2021abgcoqa}}
%         \end{tabular}  
%     }
% \vspace{-0.3cm}
% \caption{Comparison of performance across various models for detecting ambiguous queries.
% }

% \label{tab:comparison_ambiguity}
% \vspace{-0.6cm}
% \end{table}

% \section{Ablation Study}
% \label{sec:ablation}
% We further break down the performance of our individual components in \Cref{tab:ablation_study}, where each introduced component contributed to additional performance gain. That is by using rules to mask and detect lexical ambiguities with more data points and additional features, we have observed an improvement in performance. As there is class imbalance, weighted sampling further helps boost the performance. We also compare three different pretrained sentence-transformer models as backbones
% % : \texttt{all-mpnet-base-v2}, \texttt{all-distilroberta-v1}, and \texttt{all-MiniLM-L12-v2}. 
% and \texttt{all-mpnet-base-v2} seems to give use better performance, which aligns with its rank among the three in the leaderboard\footnote{\href{https://www.sbert.net/docs/sentence_transformer/pretrained_models.html}{https://www.sbert.net}}. 
\end{document}