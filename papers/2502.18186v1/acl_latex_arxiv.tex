% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{url}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% my append
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{amsmath} 
\usepackage{tabularray} % 用于创建表格
\UseTblrLibrary{booktabs} % 用于更好的表格线条样式
\usepackage{caption} % 用于更好的标题控制

\usepackage{tabularx}       % 自适应列宽
\usepackage{ragged2e}       % 优化长文本换行


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{FleSpeech: Flexibly Controllable Speech Generation with Multimodal Prompt}
\title{Steering Language Model to Stable Speech Emotion Recognition via Contextual Perception and Chain of Thought}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Zhixian Zhao\textsuperscript{1$\ast$}},
 \textbf{Xinfa Zhu\textsuperscript{1}}\thanks{Equal contribution.},
 \textbf{Xinsheng Wang\textsuperscript{2}},
 \textbf{Shuiyuan Wang\textsuperscript{1}},
\\
 \textbf{Xuelong Geng\textsuperscript{1}},
 \textbf{Wenjie Tian\textsuperscript{1}},
 \textbf{Lei Xie\textsuperscript{1}}\thanks{Corresponding authors.}
\\
 \textsuperscript{1}Audio, Speech and Language Processing Group (ASLP@NPU), \\ School of Computer Science, Northwestern Polytechnical University, Xi’an, China
 \\
 \textsuperscript{2}Hong Kong University of Science and Technology, Hong Kong, China \\ 
 \small{
   \{\href{zxzhao@mail.nwpu.edu.cn}{zxzhao}, \href{xfzhu@mail.nwpu.edu.cn}{xfzhu}\}@mail.nwpu.edu.cn,
   \href{w.xinshawn@gmail.com}{w.xinshawn@gmail.com},\
 } \\
 \small{
    \{\href{wangshuiyuan@mail.nwpu.edu.cn}{wangshuiyuan}, \href{xlgeng@mail.nwpu.edu.cn}{xlgeng}, \href{twj@mail.nwpu.edu.cn}{twj}\}@mail.nwpu.edu.cn, \
   \href{lxie@nwpu.edu.cn}{lxie@nwpu.edu.cn} \ 
   % \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
 }
}

\begin{document}
\maketitle
\begin{abstract}
% Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of processing diverse audio inputs to generate analytical insights and textual responses. 
Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of comprehending diverse audio signal, performing audio analysis and generating textual responses. 
However, in speech emotion recognition (SER), ALMs often suffer from hallucinations,  resulting in misclassifications or irrelevant outputs. To address these challenges, we propose $\mathbf{C^2SER}$, a novel ALM designed to enhance the stability and accuracy of SER through \textbf{C}ontextual perception and \textbf{C}hain of Thought (CoT). C$^2$SER integrates the Whisper encoder for semantic perception and Emotion2Vec-S for acoustic perception, where Emotion2Vec-S extends Emotion2Vec with semi-supervised learning to enhance emotional discrimination. Additionally, C$^2$SER employs a CoT approach, processing SER in a step-by-step manner while leveraging speech content and speaking styles to improve recognition. To further enhance stability, C$^2$SER introduces self-distillation from explicit CoT to implicit CoT, mitigating error accumulation and boosting recognition accuracy. Extensive experiments show that C$^2$SER outperforms existing popular ALMs, such as Qwen2-Audio and SECap, delivering more stable and precise emotion recognition. We release the training code, checkpoints, and test sets to facilitate further research\footnote{\href{https://huggingface.co/collections/ASLP-lab/c2ser-67bc735d820403e7969fe8a0}{Hugging Face Collection}, \href{https://github.com/zxzhao0/C2SER}{GitHub Repository}}.

\end{abstract}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.98\linewidth]{pic/overview_of_C2SR.pdf}
  \caption {Overview of C$^2$SER. C$^2$SER overcomes the issue of hallucinations through contextual perception and chain of thought.}
  \label{fig-1}
\end{figure}

\section{Introduction}

Audio is a multifaceted medium for communication, conveying speech prosody, vocal tone, and paralinguistic cues through its acoustic features. Large-scale audio-language models (ALMs)~\cite{alm_AudioBench} have demonstrated substantial progress in understanding diverse forms of audio signals, which is crucial for advancing Artificial General Intelligence (AGI). With increasing data availability, computational power, and model size, significant strides have been made in speech signal comprehension, analysis, and reasoning, leading to more natural and human-like text responses.

Speech emotions are crucial in communication, influencing how individuals interact and respond through variations in tone, rhythm, and intensity. However, ALMs still face challenges in speech emotion recognition (SER)~\cite{alm2ser,alm2ser2}, which aims to accurately detect emotional states from spoken language. Speech emotions are inherently complex, encompassing diverse affective states, and their precise recognition is essential for enhancing communication effectiveness across diverse domains. Despite advancements in general audio understanding, ALMs often struggle with hallucinations in the SER task due to the intricate context of speech. These hallucinations lead to misclassifications, where the model may incorrectly interpret the emotional tone or generate irrelevant responses, undermining the model’s reliability in real-world applications.

% In this work, we aim to address the hallucinations in ALM-based SER by providing detailed speech information and expanding the thought chain. To this end, as shown in Figure~\ref{fig-1}, we introduce C$^2$SER, a novel ALM designed to improve the stability and accuracy of SER. C$^2$SER integrates two key components: contextual perception and a chain of thought (CoT), incorporating speech content and speaking styles to aid SER. Specifically, we utilize the Whisper encoder for semantic perception, and Emotion2Vec-S for acoustic perception. Whisper adopts an encoder-decoder Transformer architecture, which is trained with 680,000 hours of weakly supervised multilingual data, exhibiting powerful capability of speech content understanding. Emotion2Vec-S builds on Emotion2Vec, an emotional self-supervised learning (SSL) model that uses utterance-level loss and frame-level loss for self-supervised training, and introduces a semi-supervised contrastive loss at the category level, improving its emotional discriminations.

In this work, we address the hallucination problem in ALM-based SER by incorporating detailed speech information and expanding the model's reasoning length and depth. As illustrated in Figure~\ref{fig-1}, we introduce C$^2$SER, a novel ALM designed to improve both the stability and accuracy of SER. % As shown in Figure~\ref{fig-1}, 
C$^2$SER integrates two critical components: contextual perception and a chain of thought (CoT), leveraging both speech content and speaking styles (e.g., speaking rate, pitch, energy) to facilitate emotion recognition. 
The encoder of Whisper~\cite{whisper}, trained for automatic speech recognition (ASR), speech translation, and language identification, is employed for semantic perception. For acoustic perception, we introduce Emotion2Vec-S, a refined extension of Emotion2Vec~\cite{Emotion2Vec}, designed to enhance the extraction of emotion-related information from audio. Emotion2Vec-S incorporates semi-supervised contrastive loss at the category level, improving emotional discrimination by combining self-supervised and semi-supervised learning.

Recognizing that speech emotions are influenced by both speech content and speaking styles, such as aggressive speech characterized by a loud volume potentially indicating anger, C$^2$SER employs a CoT~\cite{deepseekr1} training approach to incentivize reasoning capability. This approach decomposes the SER task into sequential steps: first perceiving speech content and speaking style, followed by emotion inference, with the assistance of prior context. This structured method imitates human thinking and reduces the possibility of hallucinations. To further enhance stability and prevent error propagation, especially in longer thought chains, C$^2$SER introduces self-distillation, transferring knowledge from explicit to implicit CoT. This process helps minimize error accumulation, improving the model's overall performance.

We validate the effectiveness of C$^2$SER through extensive experiments using multiple speech corpora and compare C$^2$SER with state-of-the-art models, including SECap~\cite{SECap} and Qwen2-Audio~\cite{Qwen2-Audio}.
To better simulate real-world context, we introduce a new SER test set, Emo-Emilia. Emo-Emilia is created through an automated labeling approach on the in-the-wild Emilia corpus and manually verified to ensure quality and diversity across various scenarios.
Our experimental results demonstrate that C$^2$SER significantly outperforms existing models on public test sets and Emo-Emilia in terms of weighted accuracy (WA), unweighted accuracy (UA), and Macro F1 score, while notably reducing hallucination-related errors. These findings highlight the potential of C$^2$SER to provide stable and reliable emotion recognition in diverse contexts.


The key contributions of our work are summarized as follows:

\begin{itemize} 
\item We propose C$^2$SER, a novel ALM that integrates contextual perception and chain of thought to mitigate hallucinations in SER. 
\item We introduce Emotion2Vec-S, working as the acoustic perception module in C$^2$SER, which enhances the original Emotion2Vec model by incorporating semi-supervised contrastive loss at the category level, greatly improving emotional discrimination. 
\item We conduct comprehensive experiments and introduce a new emotional speech test set Emo-Emilia. The results demonstrate that C$^2$SER outperforms several state-of-the-art ALMs, achieving a more stable SER. 
\item We release the code, checkpoints, and test sets to promote further research in the field of SER. 
\end{itemize}

\section{Related Work}

\subsection{Audio Language Model}

Large language models (LLMs) have made significant strides in natural language processing (NLP), showcasing remarkable capabilities across a variety of tasks~\cite{LLMsurvey}. As audio is a critical medium of communication in human interactions and human-computer engagement, recent research has extended LLMs to integrate the audio modality, leading to the development of audio-language models (ALMs)~\cite{alm_AudioBench}. ALMs tackle tasks such as audio event detection, audio captioning, and speech recognition, serving as a cornerstone for comprehensive audio understanding~\cite{qwenaudio}.

With the rapid advancements in both LLMs and the audio domain, ALMs have gained significant attention for their powerful general audio comprehension abilities. A typical ALM architecture consists of three core components: an audio encoder for modality-specific feature extraction, an LLM for text generation, and a projection layer to bridge the gap between the audio and text modalities. In addition to these foundational components, several studies have focused on refining ALM performance through innovative model architectures. For example, SALMONN~\cite{SALMONN} utilizes dual encoders to separately process speech and non-speech audio signals, effectively mitigating potential conflicts between different types of audio input. Other approaches have explored training strategies to enhance ALMs' capabilities, with Qwen2-Audio~\cite{Qwen2-Audio} being a notable example. This model employs a comprehensive training pipeline that includes pretraining, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF).

While significant progress has been made in improving the generalization and intelligence of ALMs, their performance in speech emotion recognition (SER) remains unsatisfactory, primarily due to hallucinations. SER is particularly challenging because speech emotions are inherently complex and context-dependent~\cite{alm_AIR-Bench}, making it difficult for ALMs to interpret emotional states accurately. In many cases, the model may be misled by the content of the speech, leading to incorrect classifications or irrelevant responses.

\subsection{Speech Emotion Recognition}

Speech emotion is a key form of paralinguistic information that effectively conveys the speaker’s intent. Speech Emotion Recognition (SER) aims to automatically identify the emotional state of a speaker from their speech. The typical SER pipeline consists of three stages: speech preprocessing, feature extraction, and emotion classification~\cite{review-mer1}. Early studies relied on manually engineered feature sets, such as MFCC, and simple neural network architectures like CNN and RNN, achieving basic performance on laboratory datasets (e.g., CREMA-D~\cite{CREMA-D}, IEMOCAP~\cite{IEMOCAP}).

To address the challenge of recognizing diverse emotional expressions in real-world environments, recent research has shifted towards self-supervised learning (SSL) models~\cite{ssl_ser,ssl_ser2}, known for their powerful generalization capabilities. SSL models are trained on large-scale unlabeled speech data in an unsupervised manner, allowing them to extract rich, generalizable representations directly from raw speech waveforms. Popular SSL models, such as HuBERT~\cite{Hubert} and wav2vec 2.0~\cite{wav2vec2.0}, have demonstrated significant effectiveness in extracting emotional features, serving as robust encoders for SER tasks. Additionally, researchers have explored emotion-specific SSL models designed to capture emotion-relevant features. A popular approach involves fine-tuning SSL models on emotionally labeled data for specific emotional tasks. A prominent example is Emotion2Vec~\cite{Emotion2Vec}, which is pre-trained on emotional data through self-supervised online distillation. Emotion2Vec uses both utterance-level and frame-level loss as supervision, demonstrating remarkable improvements in emotion recognition across different languages.

Despite the advancements made in SER, mainstream emotional SSL models typically employ single-level constraints, such as utterance-level or category-level constraints. While Emotion2Vec combines utterance-level and frame-level losses that are actually constraints at the utterance level, it still struggles to distinguish similar emotional expressions, such as fear and sadness, potentially leading to confusion in emotion recognition.

\section{Method}
 
\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.8\linewidth]{pic/details_of_CSER.drawio.pdf}
  \caption {Detailed structure of C$^2$SER. C$^2$SER leverages speech content and speaking styles through chain-of-thought to assist speech emotion recognition.}
  \label{fig-2}
\end{figure*}

\subsection{Framework Overview}
C$^2$SER is designed to mitigate hallucinations in speech emotion recognition (SER) and to deliver stable emotion recognition. As illustrated in Figure~\ref{fig-2}, the C$^2$SER architecture consists of two primary components: a contextual perception module and a text-based large language model (LLM). The contextual perception module extracts detailed information regarding both the semantic and acoustic aspects, which the text LLM subsequently leverages via a chain-of-thought process to make final predictions.

More specifically, the contextual perception module comprises the following elements: a Whisper~\cite{whisper} encoder for semantic perception, Emotion2Vec-S for acoustic perception, and a connection model designed to align the feature dimensions with those required by the text LLM. Formally, given a speech waveform $X$, the Whisper encoder extract semantic representations $S={s_1, s_2, ..., s_N}$ and the Emotion2Vec-S extracts acoustic representations $A={a_1, a_2, ..., a_M}$ from $X$. Let $Y={y_1, y_2, ... , y_T}$ be the text descriptions and $P={p_1, p_2, ... , p_L}$ be the text prompts. The text LLM, parameterized by $\theta$, takes $S$ and $A$ as input and predicts $Y$ in an autoregressive manner. The overall process can be formulated as a conditional probability:
\begin{equation}
    \begin{aligned}
        P(Y | S, A, P; \theta) & = \prod_{t=1}^T P(y_t | s_1,\ldots,s_N,a_1,\ldots,\\
        & a_M, p_1, \ldots, p_L, y_1, \ldots, y_{t-1}; \theta).
    \end{aligned}
\end{equation}

\subsection{Contextual Perception}
Our contextual perception module is designed to extract both semantic and acoustic representations from speech, and it comprises a Whisper encoder and Emotion2Vec-S. Specifically, C$^2$SER employs the Whisper-Medium model as its speech encoder. This model features two one-dimensional convolutional layers with a 2× downsampling factor, followed by 24 Transformer layers. Since Whisper is a supervised model trained for speech recognition and translation, its encoded representations $S$ capture rich semantic information.

Emotion2Vec-S is built upon the universal speech emotion representation model, Emotion2Vec. Emotion2Vec follows the architecture of data2vec~\cite{data2vec}, consisting of several standard Transformer blocks, and is pre-trained on open-source, unlabeled emotion data using self-supervised online distillation. It combines an utterance-level loss to capture global emotional features with a frame-level loss to capture contextual emotion nuances. Emotion2Vec has demonstrated superior performance compared to previous state-of-the-art pre-trained universal models and emotion specialist models, which often require only the training of linear layers for the SER task on emotional speech corpora. However, because Emotion2Vec focuses solely on the emotional expressions within individual utterances, it potentially struggles to distinguish similar utterances that express different emotions, such as fear and sadness. 

With the above observation, Emotion2Vec-S introduces a coarse-level supervision to Emotion2Vec. Vanilla Emotion2Vec expands data2vec2.0~\cite{data2vec2.0} with a fixed number of utterance tokens and is trained with $\mathcal{L}_{Utt}$ to learn the global emotion and $\mathcal{L}_{Frm}$ to learn the context emotion. Inspired by CLIP~\cite{clip}, Emotion2Vec-S extends Emotion2Vec with a category-level contrastive loss $\mathcal{L}_{Cate}$. Specifically, let $G$ be the global embedding of Emotion2Vec after average pooling. Emotion2Vec-S applies a contrastive loss on $G$ by treating embeddings from utterances of the same emotion category as positive pairs and those from different categories as negative pairs. The model calculates cosine similarities between these embeddings, maximizing the similarity of positive pairs while minimizing that of negative pairs. The overall loss function for Emotion2Vec-S is formulated as follows:
\vspace{-5pt}
\begin{equation}
    \mathcal{L}_{e2v} = \mathcal{L}_{Frm} + \lambda_{utt} \mathcal{L}_{Utt} + \lambda_{cate} \mathcal{L}_{Cate},
\end{equation}
where $\lambda_{utt}$ and $\lambda_{cate}$ are hyperparameters that balance the contributions of the utterance-level and category-level losses, respectively.
\vspace{-5pt}

\subsection{Explicit Chain-of-Thought}

Explicit CoT reasoning enhances the ability of LLMs to handle specific tasks by detailing intermediate steps, thereby guiding the model through its reasoning process~\cite{excot1,excot2}. In C$^2$SER, explicit CoT is employed to sequentially address the SER task. After the contextual perception module extracts detailed information regarding speech content and speaking styles, C$^2$SER first generates speech transcripts and descriptive captions of speaking styles and then infers the final speech emotion based on the aggregated context.

% we follow the methodology outlined by Wang et al. [citation needed] to extract key attributes—such
\textbf{Explicit CoT data.} We leverage signal processing tools and text LLMs to construct training data for explicit CoT. Given an emotional speech corpus that includes text transcripts and emotion labels, we extract key attributes—such as speaking rate, pitch, and energy—and map these to discrete levels (e.g., low, medium, high). Next, we employ the GLM-4-9B-Chat model~\cite{glm} to generate a detailed reasoning path that combines text transcripts, extracted speech attributes, and corresponding emotion labels. This reasoning path serves as the explicit CoT training data.

\textbf{Explicit CoT training.} In the explicit CoT training stage, the text LLM is integrated with the contextual perception module, and the entire system is fine-tuned using the explicit CoT data. To further improve the reasoning capabilities of the model, structured text prompts are used, as illustrated in Figure~\ref{fig-2} (stage 1), to guide the model through each intermediate reasoning step. As a result, after this stage of training, C$^2$SER is able to recognize speech content and speaking styles, and subsequently infer the final emotion categories based on the complete speech context.

\subsection{Implicit Chain-of-Thought}

Although the explicit CoT approach enables C$^2$SER to address SER step by step using detailed intermediate representations of speech content and speaking styles, it also introduces inefficiencies during inference and increases the risk of error accumulation~\cite{imcot1,imcot2}. To overcome these limitations, we propose a self-distillation strategy that transitions C$^2$SER from explicit CoT to implicit CoT.

\textbf{Implicit CoT Data.} At this stage, we continue to use the same speech dataset as used in the explicit CoT training; however, the processing of the reasoning path is simplified. Rather than generating detailed intermediate descriptions, the GLM-4-9B-Chat model directly produces descriptions only in terms of emotion labels for each speech segment.

\textbf{Implicit CoT Training.} During this phase, we fine-tune the model on a combination of explicit and implicit CoT data. To ensure that the model maintains its ability to infer emotion categories from rich speech context during the self-distillation process, we gradually transition the training data from explicit to implicit CoT data using a linear schedule. Furthermore, we employ customized text prompts, illustrated in Figure~\ref{fig-2} (stage 2), to guide the model reasoning process under the implicit framework. This approach enables C$^2$SER to efficiently generate accurate emotion predictions while addressing the inefficiencies and error propagation associated with explicit CoT.

\section{Experiment Setup}

\subsection{Datasets}

\begin{table*}[htb]
\centering
\footnotesize
\caption{Speech corpora used to train C$^2$SER.}
\label{tab:dataset}
\begin{tabular}{llllll}
\toprule
Dataset                & Source  & Lang    & \#Emotion   & \#Utterances & $\sim$ \#Hours   \\ \midrule
IEMOCAP~\cite{IEMOCAP}                & Act     & English & 5   & 5331   & 7.0     \\
ESD~\cite{ESD}                    & Act     & Mix     & 5   & 31500   & 26.2    \\
MER2024~\cite{mer2024}                & TV      & Chinese & 5   & 4414   & 5.9     \\
MELD~\cite{meld}                & TV      & English & 7   & 8851   & 7.4     \\
BIIC-Podcast(V1.01)~\cite{biic}    & Podcast & Chinese & 6   & 67186  & 141.3  \\
MSP-Podcast(V1.12)~\cite{msp}     & Podcast & English & 6   & 156017 & 242.6  \\ \hline
Internal dataset(Ours) & /       & Mix & 7   & 399369 & 785.3  \\ \hline
Total                  & -       & Mix       & 7  & 672668 & 1215.7 \\ \bottomrule
\end{tabular}
\end{table*}

The statistics of the training corpora are summarized in Table~\ref{tab:dataset}, which covers seven emotions: anger, happiness, neutral, sadness, surprise, disgust and fear. We utilize six open-source corpora that contain both emotion and text labels, including IEMOCAP~\cite{IEMOCAP}, MELD~\cite{meld}, MSP-Podcast~\cite{msp}, BIIC-Podcast~\cite{biic}, ESD~\cite{ESD}, and MER2024~\cite{mer2024}, alongside an internal corpus containing text labels for model training. To obtain emotion labels for the internal corpus, we apply using an efficient automated labeling method using Emotion2Vec\footnote{\url{https://huggingface.co/Emotion2Vec/emotion2vec_plus_large}} for annotating speech emotions and GLM-4-9B-Chat\footnote{\url{https://huggingface.co/THUDM/glm-4-9b-chat}}~\cite{glm} for annotating text emotions. We then take the intersection of the two annotations to ensure consistency and reliability. Additionally, we incorporate an internal speech corpus containing approximately 2400 hours of unlabeled data during the training of Emotion2Vec. The distributions of each emotion and the label construction for both explicit and implicit CoT are provided in Appendix~\ref{append:data_distribtuion} and Appendix~\ref{append:data_cot}.

To comprehensively evaluate the model's performance in downstream tasks, we follow the data allocation strategy of EmoBox~\cite{emobox} and use multiple publicly available datasets, including MELD~\cite{meld}, ASVP-ESD~\cite{asvp-esd}, EmoV-DB~\cite{emov-db}, EMOVO~\cite{emovo}, ESD, MESD~\cite{mesd}, M3ED~\cite{m3ed}, and CASIA~\cite{casia}. Furthermore, considering the complexity of speech emotions in real-world scenarios, we introduce a diverse speech emotion test set. Specifically, we apply the automated labeling approach to annotate Emilia~\cite{emilia}, a large-scale multilingual and diverse speech generation resource with over 100,000 hours of speech data that captures a wide range of emotional contexts. We then manually verify the accuracy of the emotion labels. Each utterance is checked by at least two experts to ensure both accuracy and reliability. The final proposed test set, Emo-Emilia, consists of 1400 test samples, with 100 samples per emotion category across seven types in both Chinese and English (700 samples per language).


\subsection{Implement Details}

The architecture of Emotion2Vec-S is based on the original Emotion2Vec\footnote{\url{https://huggingface.co/emotion2vec/emotion2vec_base}} model, with the addition of a classifier consisting of three fully connected layers. $\lambda_{utt}$ and $\lambda_{cate}$ are set to 0.1 and 100, respectively. We employ the Whisper-medium\footnote{\url{https://huggingface.co/openai/whisper-medium}} encoder for semantic feature extraction. The connection module is composed of a 4-layer Transformer followed by a linear layer, with intermediate feature dimensions set to 2,560 in the feed-forward module. For the text LLM component, we utilize the Qwen2-7B-Instruct\footnote{\url{https://huggingface.co/Qwen/Qwen2-7B-Instruct}} model~\cite{Qwen2-Audio} and fine-tune it using Low-Rank Adaptation (LoRA)~\cite{lora}. The LoRA rank is set to 8, the scaling factor is 32, and the dropout rate for LoRA matrices is 0.1.

To train Emotion2Vec-S, we employ 8 Nvidia 4090 GPUs, with a gradient accumulation step set to 2. The optimizer used is Adam, with a learning rate of \(7.5 \times 10^{-5}\) and a weight decay of \(1 \times 10^{-2}\). The learning rate scheduler follows a cosine annealing strategy with a warm-up ratio of 5\%. The remaining hyperparameters are consistent with those used in the vanilla Emotion2Vec model.
To train the entire C$^2$SER model, we utilize 2 Nvidia A6000 GPUs and employ the AdamW optimizer with the following parameters: \(\beta_1 = 0.9\) and \(\beta_2 = 0.99\). The initial learning rate is \(5.0 \times 10^{-5}\), with a weight decay coefficient of 0.01. The learning rate scheduler uses WarmupLR, with the warm-up steps set to 15\% of the total training steps. During C$^2$SER training, Emotion2Vec-S is frozen to retain its ability to effectively extract emotional features from speech.


\begin{table*}[]
\centering
\footnotesize
\setlength\tabcolsep{5pt}
\caption{Emotion2Vec-S performance on datasets of Chinese, English, Italian, Mexican and mixed languages. The best and the second best result is shown in \textbf{bold} and by \underline{underlined}.}
\label{tab:ssl_result}
% \resizebox{\textwidth}{!}
{
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Model} & UA(\%) ↑             & WA(\%) ↑             & F1(\%) ↑              & UA(\%) ↑             & WA(\%) ↑             & F1(\%) ↑              & UA(\%) ↑             & WA(\%) ↑             & F1(\%) ↑             \\ \cline{2-10} 
                       & \multicolumn{3}{c|}{M3ED (Chinese)}                                       & \multicolumn{3}{c|}{MELD (English)}                                       & \multicolumn{3}{c}{ESD (Mixlingual)}                                       \\ \hline
WavLM-base             & 22.76    & 42.79    & 22.03    & 23.44          & 44.71          & 24.25          & 72.90    & 72.90    & 72.55                    \\
data2vec base              & 19.44    & 37.32    & 19.24    & \underline{23.82}          & \underline{45.57}          & \underline{24.37}          & 65.05    & 65.05    & 64.55                    \\
data2vec2.0 base          & \underline{22.82}    & 41.42    & \underline{22.89}    & \textbf{24.79}          & \textbf{46.65}          & \textbf{25.28}          & \underline{73.40}    & \underline{73.40}    & \underline{73.10}                    \\
Emotion2Vec            & 22.04     & \underline{48.28}     & 20.79     & 23.20          & 44.96          & 24.05          & 70.22    & 70.22    & 70.06                    \\
Emotion2Vec-F          & 20.80    & 48.14    & 18.35    & 19.91          & 41.82          & 20.35          & 64.18    & 64.18    & 63.96                    \\
Emotion2Vec-S          & \textbf{23.82}     & \textbf{50.21}     & \textbf{23.13}     & 21.31          & 45.38          & 21.77          & \textbf{79.84}    & \textbf{79.84}    & \textbf{79.72}                    \\ \hline
Model                  & \multicolumn{3}{c|}{CASIA (Chinese)}                                      & \multicolumn{3}{c|}{EmoV-DB (English)}                                    & \multicolumn{3}{c}{ASVP-ESD (Mixlingual)}                                  \\ \hline
WavLM-base             & 47.25    & 47.25    & 41.78    & \textbf{98.38}     & \textbf{98.49}     & \textbf{98.39}    & 46.38     & 58.05     & 47.35     \\
data2vec base              & 34.72    & 34.72    & 30.88    & 93.26     & 93.61     & 93.23    & 37.66     & 50.79     & 38.26     \\
data2vec2.0 base          & 43.31    & 43.31    & 38.90    & 95.81     & 96.09     & 95.80    & 46.0        & 57.57     & 46.62     \\
Emotion2Vec            & \underline{47.58}    & \underline{47.58}    & \underline{43.55}    & 96.71     & 96.90     & 96.74    & \textbf{48.60}      & \underline{58.30}      & \textbf{49.60}     \\
Emotion2Vec-F          & 43.18    & 43.18    & 39.28    & 96.68     & 96.94     & 96.70    & 47.05     & 57.77     & \underline{48.25}     \\
Emotion2Vec-S          & \textbf{62.95}    & \textbf{62.95}    & \textbf{60.2}     & \underline{97.04}     & \underline{97.30}     & \underline{97.08}    & \underline{48.20}      & \textbf{58.88}      & 45.63      \\ \hline
Model                  & \multicolumn{3}{c|}{EMOVO (Italian)}                                      & \multicolumn{3}{c|}{MESD (Mexican)}                                       & \multicolumn{3}{c}{Emo-Emilia (Mixlingual)}                                     \\ \hline
WavLM-base             & 42.39    & 42.39    & 37.33    & 42.58     & 43.52     & 42.94    & 67.26          & 67.26          & 67.28          \\
data2vec base          & 32.47    & 32.47    & 29.22    & 34.37     & 34.35     & 33.24    & 63.80          & 63.80          & 63.72          \\
data2vec2.0 base      & \textbf{42.96}    & \textbf{42.96}    & \textbf{41.01}    & 44.86     & 44.85     & 43.60     & 64.60          &64.60           & 64.46          \\
Emotion2Vec            & 41.02    & 41.02    & 38.60    & 50.56     & 50.48     & 50.10    & \underline{68.02}          & \underline{68.02}          & \underline{68.00}          \\
Emotion2Vec-F          & 38.65    & 38.65    & 34.83    & \underline{55.38}     & \underline{55.36}     & \underline{55.18}    & 59.24          & 59.24         & 59.00          \\
Emotion2Vec-S          & \underline{42.88}    & \underline{42.88}    & \underline{40.87}    & \textbf{59.57}     & \textbf{59.62}     & \textbf{59.28}    & \textbf{80.66}          & \textbf{80.66}          & \textbf{80.58}          \\ \hline
\end{tabular}}
\end{table*}

\subsection{Comparison Systems and Evaluation Metrics}

We conduct comparative experiments with several advanced models, including self-supervised learning (SSL) models (e.g., WavLM~\cite{wavlm}, Data2Vec~\cite{data2vec}, Data2Vec 2.0~\cite{data2vec2.0}), and audio-language models (e.g., SECap~\cite{SECap}, Qwen2-Audio~\cite{Qwen2-Audio}, and SenseVoice-Small~\cite{FunAudioLLM}).
For SSL models, we follow the methodology of EmoBox~\cite{emobox}. First, features are extracted from the last Transformer layer of the pre-trained models and undergo uniform layer normalization to accelerate convergence. Then, a downstream network is applied to perform the SER task, which consists of a simple linear hidden layer, a ReLU activation function, a pooling layer, and a classification head. For ALMs, we train SECap on our speech corpora while directly using official checkpoints of Qwen2-Audio and SenseVoice-Small.

We evaluate the models using three key metrics: weighted average accuracy (WA), unweighted average accuracy (UA), and the Macro F1 score. WA represents the overall accuracy of the model, UA corresponds to the average class-wise accuracy, and the Macro F1 score provides a balanced evaluation, particularly useful in cases of class imbalance. 

\section{Experimental Results}

\subsection{Evaluation of Emotion2Vec-S}

The results are presented in Table~\ref{tab:ssl_result}, where we compare Emotion2Vec-S with various SSL pre-trained models of similar model size and training corpora. Notably, Emotion2Vec-F refers to the Emotion2Vec model trained directly on the same corpora as Emotion2Vec-S, allowing us to investigate the impact of different datasets on model performance. The results demonstrate that Emotion2Vec-S consistently outperforms other models across most datasets. Interestingly, the performance gap between Emotion2Vec and Emotion2Vec-S indicates that the training corpus does influence the results, but it does not always lead to significant improvements. Nonetheless, Emotion2Vec-S consistently shows steady improvement compared to both Emotion2Vec-F and Emotion2Vec, validating the effectiveness of semi-supervised contrastive learning.

Furthermore, we observe that the models exhibit varying performance across different test sets and languages. Specifically, Emotion2Vec-S outperforms comparison models by a significant margin on Chinese test sets while achieving competitive results on English datasets. Additionally, Emotion2Vec-S excels in multilingual test sets, particularly in ESD and Emo-Emilia. When extended to other languages, Emotion2Vec-S achieves the best results on the Mexican test set and ranks second on the Italian test set. Although Emotion2Vec-S is trained primarily on Chinese and English speech corpora, these results highlight its impressive generalization capabilities across different languages. Overall, these findings suggest that Emotion2Vec-S offers superior emotion discrimination compared to the original Emotion2Vec model, establishing it as a robust foundation model for extracting speech emotion representations.

\begin{table*}[]
\centering
\footnotesize
\setlength\tabcolsep{4pt}
\caption{C$^2$SER performance on datasets of Chinese, English, Italian, Mexican and mixed languages. Models are tested in a zero-shot fashion. The best and the second best result is shown in \textbf{bold} and by \underline{underlined}.}
\label{tab:llm_result}
% \resizebox{\textwidth}{!}
{
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Model} & UA(\%) ↑             & WA(\%) ↑             & F1(\%) ↑              & UA(\%) ↑             & WA(\%) ↑             & F1(\%) ↑              & UA(\%) ↑             & WA(\%) ↑             & F1(\%) ↑             \\ \cline{2-10} 
                       & \multicolumn{3}{c|}{M3ED (Chinese)}                                       & \multicolumn{3}{c|}{MELD (English)}                                       & \multicolumn{3}{c}{ESD (Mixlingual)}                                       \\ \hline
Qwen2-Audio            & 19.53                & 41.38                & 15.50                 & \underline{36.82}                & \textbf{54.56}                & \textbf{37.33}                 & 56.26                & 56.26                & 33.06                \\
SenseVoice-S           & 23.13    & 23.09    & 21.11    & 0.99      & 1.95      & 1.68     & 52.23                & 52.23                & 42.20                \\
SECap                  & 23.74    & 29.90    & 18.31    & 19.41          & 22.53          & 13.91         & 42.51     & 42.51     & 25.55     \\
C$^2$SER(Explicit CoT) & \underline{32.29}    & \underline{47.59}    & \underline{26.99}    & 30.36     & 51.39     & 27.45    & \underline{93.81}     & \underline{93.86}     & \underline{68.19}     \\
C$^2$SER(Implicit CoT) & \textbf{36.68 }   & \textbf{50.57}    & \textbf{29.40}    & \textbf{38.66}     & \underline{53.10}     & \underline{33.10}    & \textbf{96.33}     & \textbf{96.34}     & \textbf{81.62}     \\ \hline
Model                  & \multicolumn{3}{c|}{CASIA (Chinese)}                                      & \multicolumn{3}{c|}{EmoV-DB (English)$^{*}$}                                    & \multicolumn{3}{c}{ASVP-ESD (Mixlingual)}                                  \\ \hline
Qwen2-Audio            & \underline{48.17}                & \underline{48.17}                & 35.58                 & \textbf{99.28}                & \textbf{99.38}                & \textbf{79.51}                 & \underline{43.44}                & \underline{48.01}                & \textbf{36.53}                \\
SenseVoice-S           & 33.58    & 33.58    & 24.32    & 39.01     & 42.13     & 33.11    & 16.55                & 16.19                & 21.57                \\
SECap                  & 33.75    & 33.75    & 23.54    & 28.85     & 30.26     & 17.10    & 25.07     & 27.95     & 19.42     \\
C$^2$SER(Explicit CoT) & 46.62    & 46.62    & \underline{37.51}    & 59.07     & 63.18     & 36.55    & 41.62     & 47.34     & 32.58     \\
C$^2$SER(Implicit CoT) & \textbf{53.33}    & \textbf{53.33}    & \textbf{42.85}    & \underline{59.66}     & \underline{63.38}     & \underline{41.63}    & \textbf{43.86}     & \textbf{48.54}     & \underline{34.06}     \\ \hline
Model                  & \multicolumn{3}{c|}{EMOVO (Italian)}                                      & \multicolumn{3}{c|}{MESD (Mexican)}                                       & \multicolumn{3}{c}{Emo-Emilia (Mixlingual)}                                     \\ \hline
Qwen2-Audio            & 35.88                & 35.88                & 26.22                 & 23.60                & 23.55                & 21.62                 & 39.07   & 39.07     & 31.91     \\
SenseVoice-S           & 14.12    & 14.12    & 14.42    & 23.13     & 23.09     & 21.11    & 63.31                 & 63.31                & 56.84                   \\
SECap                  & 26.36    & 26.36    & 17.31    & \underline{28.40}     & \underline{28.39}     & 21.24    & 32.50     & 32.50     & 23.62     \\
C$^2$SER(Explicit CoT) & \underline{37.59}    & \underline{37.59}    & \underline{27.33}    & 28.15     & 28.09     & \textbf{21.75}    & \underline{68.29}     & \underline{68.29}     & \underline{61.28}     \\
C$^2$SER(Implicit CoT) & \textbf{41.67}    & \textbf{41.67}    & \textbf{35.93}    & \textbf{28.60}     & \textbf{28.54}     & \underline{21.66}    & \textbf{69.00}     & \textbf{69.00}     & \textbf{61.61}     \\ \hline
\end{tabular}}
\vspace{-10pt}
\noindent % 取消段落缩进
    \makebox[\textwidth][l]{\footnotesize{$^{*}$Qwen2-Audio's results on EmoV-DB may indicate data leakage (i.e., inclusion of the dataset in training).}} % 强制左对齐
\end{table*}

\subsection{Evaluation of C$^2$SER}

We compare C$^2$SER with several leading audio-language models (ALMs) across various test sets. As shown in Table~\ref{tab:llm_result}, C$^2$SER outperforms most comparison models on the majority of datasets. Notably, even with only 3,500 randomly preserved utterances for evaluation in the ESD dataset, C$^2$SER still achieves exceptional performance. We speculate that this can be attributed to the shared origin of the training and test sets, a factor that may also explain the Qwen2-Audio results on EmoV-DB. However, when compared to SECap, which is trained on the same speech corpora, C$^2$SER consistently outperforms SECap across all test datasets except MESD, where the performance of both models is similar. These results validate that incorporating speech context through the chain-of-thought approach effectively improves SER.
Additionally, we observe significant improvements when comparing C$^2$SER with explicit CoT to C$^2$SER with implicit CoT. This performance gain highlights the success of self-distillation, which helps preserve reasoning capabilities while significantly reducing potential error accumulation in longer thought chains.

ALMs, including C$^2$SER, also exhibit notable domain shifts across different languages. Specifically, C$^2$SER outperforms comparison models on the M3ED and CASIA datasets, while achieving comparable results on MELD. Moreover, C$^2$SER excels across all multilingual test sets. When extended to other languages, such as Italian, C$^2$SER continues to lead. These findings demonstrate the strong language generalization capabilities of C$^2$SER, indirectly suggesting that both contextual perception and chain-of-thought contribute to improving emotion recognition across diverse speech data.

\vspace{-5pt}
\subsection{Ablation Study}

We conduct an ablation study to evaluate the contribution of each component in C$^2$SER. The experimental results are presented in Table~\ref{tab:ablation}.
Firstly, removing the Whisper encoder leads to a significant degradation in performance, with the model failing to converge during explicit CoT training due to the lack of semantic perception. 
Secondly, the model incorporating Emotion2Vec-S outperforms the version without it, demonstrating that acoustic perception is crucial for capturing emotional expressions effectively.
Finally, excluding CoT causes a substantial drop in performance. This result suggests the reasoning capability of C$^2$SER is improved after CoT training, which leads to a more accurate and stable emotion recognition.

\begin{table}[htb]
\centering
\footnotesize
\caption{Ablation study of C$^2$SER on the Emo-Emilia test set.}
\label{tab:ablation}
% \resizebox{\linewidth}{!}
{
\begin{tabular}{llll}
\toprule
Model                                 & WA(\%)↑ & UA(\%)↑ & F1(\%)↑ \\ \midrule
C$^2$SER              & \textbf{69.00}        & \textbf{69.00}       & \textbf{61.61}        \\
w/o Whisper encoder                         & 32.07        &  32.07      & 34.56        \\
w/o Emotion2Vec-S           & 57.93        &  57.93      & 51.10        \\
w/o CoT & 43.14        & 43.14       & 36.15        \\
\bottomrule
\end{tabular}}
\end{table}

\vspace{-10pt}
\section{Conclusion}
% \vspace{-5pt}

This paper proposes C$^2$SER, a novel audio-language model designed to address hallucinations in speech emotion recognition. Specifically, C$^2$SER introduces a contextual perception module of Whisper and Emotion2Vec-S, providing detailed semantic and acoustic information for the LLM decoder. Additionally, C$^2$SER introduces a chain-of-thought to incorporate speech context for emotion recognition, incentivizing reasoning capability. Furthermore, C$^2$SER proposes self-distillation, maintaining reasoning capability while minimizing error accumulation. Extensive experiments demonstrate that Emotion2Vec-S effectively captures emotion-related information, and C$^2$SER achieves an accurate and stable SER compared to existing models. 

\clearpage
\section*{Limitation}
Although C$^2$SER effectively addresses the hallucination problem in ALM-based SER,  we identified the need for further improvements in the performance across different languages. This is mainly due to the imbalance in the training corpora of C$^2$SER. We believe that scaling up data size and leveraging balanced corpora to train C$^2$SER will effectively improve performance across different languages. 

% \section*{Acknowledgments}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}


\clearpage
\appendix

\section{Data Distribution}
\label{append:data_distribtuion}

The emotion and language distributions of the training corpora are shown in Figures~\ref{fig-3} and \ref{fig-4}. As observed, neutral emotions account for nearly half of the dataset, while fear and disgust constitute less than 2\%. The scarcity of fear and disgust data arises from challenges such as subjective annotation (e.g., difficulty in accurate identification), limited natural occurrences in contextual expressions, and technical barriers in detecting these emotions from speech or text. Additionally, the proportion of Chinese speech is approximately double that of English speech. We plan to develop more balanced training corpora to address these disparities.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{pic/pie_dataset.png}
  \caption {Distribution of each emotions.}
  \label{fig-3}
\end{figure}

\label{append:data_distribtuion}
\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{pic/pie_lang_dataset_new.png}
  \caption {Distribution of different language.}
  \label{fig-4}
\end{figure}


\section{Explicit CoT Construction}
\label{append:data_cot}

\begin{table*}[htbp]
  \centering
  \caption{Template for Explicit CoT data construction, \textcolor{red}{label} will be replaced with the corresponding value.}
  \label{tab:cot-example}
  \begin{tabularx}{\textwidth}{>{\RaggedRight}X|>{\RaggedRight}X}
    \hline
    \textbf{Prompt}: Based on the provided speech features—including a speaking rate of <\textcolor{red}{speaking rate label}>, a volume level of <\textcolor{red}{energy label}>, and a pitch of <\textcolor{red}{pitch label}>—along with the text content '<\textcolor{red}{text label}>' and the emotion <\textcolor{red}{emotion label}>, generate a natural and logical emotional description. Here is an example: 'The speaker spoke at a <\textcolor{red}{speaking rate label}> pace, with a <\textcolor{red}{pitch label}> tone and <\textcolor{red}{energy label}> level: “<\textcolor{red}{text label}>”. Based on the analysis of speech characteristics, the emotion was inferred to be <\textcolor{red}{emotion label}>.' Ensure including all speech features and logic of the description."
    &
    \textbf{Generated Example}: The speaker spoke at a moderate pace, with a low-pitched tone and a soft volume: "together you sort of get this whole narrative of feedback where students are constantly feeling like you're watching and you're paying attention and you're giving them suggestions I often think about like a basketball." Based on the speech characteristics, the emotion was inferred to be disgust, revealing a sense of resentment or aversion towards the described situation. \\
    \hline
  \end{tabularx}
\end{table*}

To construct training data for both explicit and implicit CoT, we first use signal processing tools to collect relevant speech attributes, followed by the generation of reasoning paths via a text-based LLM. Specifically, we compute pitch contours from speech using the PENN library~\footnote{\url{https://github.com/interactiveaudiolab/penn}}, and calculate the utterance-level mean. We extract energy from the speech using the pyloudnorm library~\footnote{\url{https://github.com/csteinmetz1/pyloudnorm}}, which returns a value representing the loudness level. The speaking rate is determined by dividing the number of phonemes in the transcript by the utterance length. Silences at both the beginning and end of the speech waveforms are trimmed for accuracy. After collecting statistics for all utterances, we calculate the mean $\mu$ and standard deviation $\sigma$, and divide the values into three levels based on the Central Limit Theorem: 'Low' corresponds value below $\mu-\sigma$, 'Medium' corresponds values between $\mu-\sigma$ and $\mu+\sigma$, and 'High' corresponds values above $\mu+\sigma$.

After gathering speech attributes, we use GLM-4-9B-Chat to generate a CoT path conditioned on the speech attributes, original text transcripts, and emotion labels. A simple prompt is designed to guide GLM-4-9B-Chat to first produce speaking styles and speech content, followed by the final emotion. The template for this prompt is shown in Table~\ref{tab:cot-example}.

\section{Evaluation Details}
\label{append:eval_details}

For all test sets, we retain utterances within the seven emotion categories: anger, happiness, neutral, sadness, surprise, disgust, and fear. Similar emotion labels, such as Amused, Joy, and Happy, are merged into the happiness category. During evaluation, SSL models are tested using leave-one-session-out five-fold cross-validation, with 20\% of the training set used as the validation set for the Emo-Emilia test set. Evaluation on other test sets follows the EmoBox. Additionally, since the output of ALMs may include more than just emotion categories, we use Qwen2.5-14B-Chat\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-14B-Instruct}} to directly extract emotion labels from the descriptions generated by ALMs. The evaluation script is available in our repository.

To assess the performance of C$^2$SER, we compare with the following systems.

\textbf{Qwen2-Audio}: A multimodal framework for comprehensive audio understanding and generation. Qwen2-Audio employs Whisper-large-V3 as the audio encoder to capture subtle acoustic features and integrates the Qwen-7B LLM as the foundational component, enabling efficient alignment and generation between audio and text.

\textbf{SenseVoice-Small}: An encoder-only speech foundation model designed for rapid voice understanding. It employs a memory-equipped self-attention network (SAN-M) to enable fast and efficient inference.

\textbf{SECap}: A framework that generates high-quality style captions. It uses HuBERT to extract speech features, Q-Former as the Bridge-Net, and LLaMA as the text decoder to produce coherent style captions. We train SECap on the same data as C$^2$SER.

\section{Category Accuracy of Emotion2Vec-S}
\label{append:emo2vec_details}

We evaluate the category accuracy of Emotion2Vec-S using five-fold cross-validation, with 20\% of the training set used as the validation set. The average results across each fold are shown in Figure~\ref{fig-5}. Emotion2Vec-S outperforms Emotion2Vec in recognition accuracy for all emotion categories. Disgust achieves the highest recognition accuracy, while happiness has relatively lower recognition accuracy. In all cases, the accuracy of Emotion2Vec-S is higher than that of Emotion2Vec. Overall, the performance is relatively balanced across the emotions.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{pic/acc_emo2vec-s.pdf}
  \caption {Category Accuracy (\%) of Emotion2Vec-S on the Emo-Emilia test set.}
  \label{fig-5}
\end{figure}

\section{Category Accuracy of C$^2$SER}
\label{append:c2ser_details}

We evaluate the category accuracy of C$^2$SER through direct inference on the Emo-Emilia test set. The results are displayed in Figure~\ref{fig-6}. C$^2$SER achieves higher accuracy than Qwen2-audio across all emotion categories. The recognition accuracy for anger, happiness, neutral, sadness, and surprise is above 90\%, while the recognition accuracy for disgust and fear is below 20\%. This imbalance in performance is likely attributed to the skewed distribution of the training corpus, as shown in Figure~\ref{fig-3}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{pic/acc_c2ser_qwen.pdf}
  \caption {Category Accuracy (\%) of C$^2$SER on the Emo-Emilia test set.}
  \label{fig-6}
\end{figure}

\section{Effectiveness of CoT training}
\label{append:eff_cot_training}

\begin{table*}[htb]
  \centering
  \footnotesize
  % \setlength\tabcolsep{5pt}
  \caption{Effectiveness of CoT training on the Emo-Emilia test set.}
  \label{tab:cot-training}
\begin{tabular}{lllll}
\toprule
CoT Phase                       & Model                  & UA(\%) ↑ & WA(\%) ↑ & F1(\%) ↑ \\ \midrule
\multirow{3}{*}{Inference} & Qwen2-Audio  & 39.07    & 39.07    & 31.91    \\
                           & Qwen2-Audio (Explicit CoT)  & 32.57    & 32.57    & 38.12    \\
                           & Qwen2-Audio (Implicit CoT)  & 25.79    & 25.79    & 33.21    \\ \hline
\multirow{2}{*}{Training}  & C$^2$SER (Explicit CoT) & 68.29    & 68.29    & 61.28    \\
                           & C$^2$SER (Implicit CoT) & 69.00    & 69.00    & 61.61    \\ \bottomrule
\end{tabular}
\end{table*}

To validate the effectiveness of the CoT training of C$^2$SER, we use the exact text instructions on Qwen2-Audio to conduct CoT inference. The comparison results are shown in Table~\ref{tab:cot-training}. Obviously, Qwen2-Audio is not well capable of CoT reasoning, whose performance is significantly lower than that of C$^2$SER. This result reveals that our CoT training boosts the reasoning capabilities of ALMs, enabling them to incorporate speech context for more accurate emotion recognition. Furthermore, Qwen2-Audio with implicit CoT performs worse than with explicit CoT, as it suffers from severe hallucinations that generate irrelevant results. This suggests that without explicit CoT training, implicit CoT fails to effectively guide reasoning and emotion recognition.

\end{document}
