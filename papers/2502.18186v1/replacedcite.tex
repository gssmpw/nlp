\section{Related Work}
\subsection{Audio Language Model}

Large language models (LLMs) have made significant strides in natural language processing (NLP), showcasing remarkable capabilities across a variety of tasks____. As audio is a critical medium of communication in human interactions and human-computer engagement, recent research has extended LLMs to integrate the audio modality, leading to the development of audio-language models (ALMs)____. ALMs tackle tasks such as audio event detection, audio captioning, and speech recognition, serving as a cornerstone for comprehensive audio understanding____.

With the rapid advancements in both LLMs and the audio domain, ALMs have gained significant attention for their powerful general audio comprehension abilities. A typical ALM architecture consists of three core components: an audio encoder for modality-specific feature extraction, an LLM for text generation, and a projection layer to bridge the gap between the audio and text modalities. In addition to these foundational components, several studies have focused on refining ALM performance through innovative model architectures. For example, SALMONN____ utilizes dual encoders to separately process speech and non-speech audio signals, effectively mitigating potential conflicts between different types of audio input. Other approaches have explored training strategies to enhance ALMs' capabilities, with Qwen2-Audio____ being a notable example. This model employs a comprehensive training pipeline that includes pretraining, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF).

While significant progress has been made in improving the generalization and intelligence of ALMs, their performance in speech emotion recognition (SER) remains unsatisfactory, primarily due to hallucinations. SER is particularly challenging because speech emotions are inherently complex and context-dependent____, making it difficult for ALMs to interpret emotional states accurately. In many cases, the model may be misled by the content of the speech, leading to incorrect classifications or irrelevant responses.

\subsection{Speech Emotion Recognition}

Speech emotion is a key form of paralinguistic information that effectively conveys the speakerâ€™s intent. Speech Emotion Recognition (SER) aims to automatically identify the emotional state of a speaker from their speech. The typical SER pipeline consists of three stages: speech preprocessing, feature extraction, and emotion classification____. Early studies relied on manually engineered feature sets, such as MFCC, and simple neural network architectures like CNN and RNN, achieving basic performance on laboratory datasets (e.g., CREMA-D____, IEMOCAP____).

To address the challenge of recognizing diverse emotional expressions in real-world environments, recent research has shifted towards self-supervised learning (SSL) models____, known for their powerful generalization capabilities. SSL models are trained on large-scale unlabeled speech data in an unsupervised manner, allowing them to extract rich, generalizable representations directly from raw speech waveforms. Popular SSL models, such as HuBERT____ and wav2vec 2.0____, have demonstrated significant effectiveness in extracting emotional features, serving as robust encoders for SER tasks. Additionally, researchers have explored emotion-specific SSL models designed to capture emotion-relevant features. A popular approach involves fine-tuning SSL models on emotionally labeled data for specific emotional tasks. A prominent example is Emotion2Vec____, which is pre-trained on emotional data through self-supervised online distillation. Emotion2Vec uses both utterance-level and frame-level loss as supervision, demonstrating remarkable improvements in emotion recognition across different languages.

Despite the advancements made in SER, mainstream emotional SSL models typically employ single-level constraints, such as utterance-level or category-level constraints. While Emotion2Vec combines utterance-level and frame-level losses that are actually constraints at the utterance level, it still struggles to distinguish similar emotional expressions, such as fear and sadness, potentially leading to confusion in emotion recognition.