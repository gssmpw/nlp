@article{LLMsurvey,
  author       = {Wayne Xin Zhao and
                  Kun Zhou and
                  Junyi Li and
                  Tianyi Tang and
                  Xiaolei Wang and
                  Yupeng Hou and
                  Yingqian Min and
                  Beichen Zhang and
                  Junjie Zhang and
                  Zican Dong and
                  Yifan Du and
                  Chen Yang and
                  Yushuo Chen and
                  Zhipeng Chen and
                  Jinhao Jiang and
                  Ruiyang Ren and
                  Yifan Li and
                  Xinyu Tang and
                  Zikang Liu and
                  Peiyu Liu and
                  Jian{-}Yun Nie and
                  Ji{-}Rong Wen},
  title        = {A Survey of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2303.18223},
  year         = {2023},
}

@inproceedings{alm_AIR-Bench,
  author       = {Qian Yang and
                  Jin Xu and
                  Wenrui Liu and
                  Yunfei Chu and
                  Ziyue Jiang and
                  Xiaohuan Zhou and
                  Yichong Leng and
                  Yuanjun Lv and
                  Zhou Zhao and
                  Chang Zhou and
                  Jingren Zhou},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {AIR-Bench: Benchmarking Large Audio-Language Models via Generative
                  Comprehension},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {1979--1998},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
}

@article{alm_AudioBench,
  author       = {Bin Wang and
                  Xunlong Zou and
                  Geyu Lin and
                  Shuo Sun and
                  Zhuohan Liu and
                  Wenyu Zhang and
                  Zhengyuan Liu and
                  AiTi Aw and
                  Nancy F. Chen},
  title        = {AudioBench: {A} Universal Benchmark for Audio Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2406.16020},
  year         = {2024},
}


@article{Qwen2-Audio,
  author       = {Yunfei Chu and
                  Jin Xu and
                  Qian Yang and
                  Haojie Wei and
                  Xipin Wei and
                  Zhifang Guo and
                  Yichong Leng and
                  Yuanjun Lv and
                  Jinzheng He and
                  Junyang Lin and
                  Chang Zhou and
                  Jingren Zhou},
  title        = {Qwen2-Audio Technical Report},
  journal      = {CoRR},
  volume       = {abs/2407.10759},
  year         = {2024},
}

@inproceedings{alm2ser,
  author       = {Jaime Bellver{-}Soler and
                  Iv{\'{a}}n Mart{\'{\i}}n{-}Fern{\'{a}}ndez and
                  Jose M. Bravo{-}Pacheco and
                  Sergio Esteban Romero and
                  Fernando Fern{\'{a}}ndez Mart{\'{\i}}nez and
                  Luis Fernando D'Haro},
  editor       = {Najim Dehak and
                  Patrick Cardinal},
  title        = {Multimodal Audio-Language Model for Speech Emotion Recognition},
  booktitle    = {Odyssey 2024: The Speaker and Language Recognition Workshop, Quebec
                  City, Canada, June 18-21, 2024},
  pages        = {288--295},
  publisher    = {{ISCA}},
  year         = {2024},
}

@ARTICLE{alm2ser2,
  author={Akman, Alican and Sun, Qiyang and Schuller, Bj√∂rn W.},
  journal={IEEE Signal Processing Letters}, 
  title={Improving Audio Explanations Using Audio Language Models}, 
  year={2025},
  volume={32},
  number={},
  pages={741-745},
  doi={10.1109/LSP.2025.3532218},
}

@article{deepseekr1,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@inproceedings{SECap,
  author       = {Yaoxun Xu and
                  Hangting Chen and
                  Jianwei Yu and
                  Qiaochu Huang and
                  Zhiyong Wu and
                  Shi{-}Xiong Zhang and
                  Guangzhi Li and
                  Yi Luo and
                  Rongzhi Gu},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {SECap: Speech Emotion Captioning with Large Language Model},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {19323--19331},
  publisher    = {{AAAI} Press},
  year         = {2024},
}

@inproceedings{SALMONN,
  author       = {Changli Tang and
                  Wenyi Yu and
                  Guangzhi Sun and
                  Xianzhao Chen and
                  Tian Tan and
                  Wei Li and
                  Lu Lu and
                  Zejun Ma and
                  Chao Zhang},
  title        = {{SALMONN:} Towards Generic Hearing Abilities for Large Language Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
}

@article{FunAudioLLM,
  author       = {Keyu An and
                  Qian Chen and
                  Chong Deng and
                  Zhihao Du and
                  Changfeng Gao and
                  Zhifu Gao and
                  Yue Gu and
                  Ting He and
                  Hangrui Hu and
                  Kai Hu and
                  Shengpeng Ji and
                  Yabin Li and
                  Zerui Li and
                  Heng Lu and
                  Haoneng Luo and
                  Xiang Lv and
                  Bin Ma and
                  Ziyang Ma and
                  Chongjia Ni and
                  Changhe Song and
                  Jiaqi Shi and
                  Xian Shi and
                  Hao Wang and
                  Wen Wang and
                  Yuxuan Wang and
                  Zhangyu Xiao and
                  Zhijie Yan and
                  Yexin Yang and
                  Bin Zhang and
                  Qinglin Zhang and
                  Shiliang Zhang and
                  Nan Zhao and
                  Siqi Zheng},
  title        = {FunAudioLLM: Voice Understanding and Generation Foundation Models
                  for Natural Interaction Between Humans and LLMs},
  journal      = {CoRR},
  volume       = {abs/2407.04051},
  year         = {2024},
}

@inproceedings{emotion2vec,
  author       = {Ziyang Ma and
                  Zhisheng Zheng and
                  Jiaxin Ye and
                  Jinchao Li and
                  Zhifu Gao and
                  Shiliang Zhang and
                  Xie Chen},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation},
  booktitle    = {Findings of the Association for Computational Linguistics, {ACL} 2024,
                  Bangkok, Thailand and virtual meeting, August 11-16, 2024},
  pages        = {15747--15760},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
}

@article{wavlm,
  author       = {Sanyuan Chen and
                  Chengyi Wang and
                  Zhengyang Chen and
                  Yu Wu and
                  Shujie Liu and
                  Zhuo Chen and
                  Jinyu Li and
                  Naoyuki Kanda and
                  Takuya Yoshioka and
                  Xiong Xiao and
                  Jian Wu and
                  Long Zhou and
                  Shuo Ren and
                  Yanmin Qian and
                  Yao Qian and
                  Jian Wu and
                  Michael Zeng and
                  Xiangzhan Yu and
                  Furu Wei},
  title        = {WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech
                  Processing},
  journal      = {{IEEE} J. Sel. Top. Signal Process.},
  volume       = {16},
  number       = {6},
  pages        = {1505--1518},
  year         = {2022},
}

@inproceedings{whisper,
  author       = {Alec Radford and
                  Jong Wook Kim and
                  Tao Xu and
                  Greg Brockman and
                  Christine McLeavey and
                  Ilya Sutskever},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Robust Speech Recognition via Large-Scale Weak Supervision},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {28492--28518},
  publisher    = {{PMLR}},
  year         = {2023},
}

@inproceedings{clip,
  author       = {Alec Radford and
                  Jong Wook Kim and
                  Chris Hallacy and
                  Aditya Ramesh and
                  Gabriel Goh and
                  Sandhini Agarwal and
                  Girish Sastry and
                  Amanda Askell and
                  Pamela Mishkin and
                  Jack Clark and
                  Gretchen Krueger and
                  Ilya Sutskever},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning,
                  {ICML} 2021, 18-24 July 2021, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {8748--8763},
  publisher    = {{PMLR}},
  year         = {2021},
}

@inproceedings{lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
}

@inproceedings{wav2vec2.0,
  author       = {Alexei Baevski and
                  Yuhao Zhou and
                  Abdelrahman Mohamed and
                  Michael Auli},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech
                  Representations},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
}

@article{Hubert,
  author       = {Wei{-}Ning Hsu and
                  Benjamin Bolte and
                  Yao{-}Hung Hubert Tsai and
                  Kushal Lakhotia and
                  Ruslan Salakhutdinov and
                  Abdelrahman Mohamed},
  title        = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction
                  of Hidden Units},
  journal      = {{IEEE} {ACM} Trans. Audio Speech Lang. Process.},
  volume       = {29},
  pages        = {3451--3460},
  year         = {2021},
}

@inproceedings{data2vec,
  author       = {Alexei Baevski and
                  Wei{-}Ning Hsu and
                  Qiantong Xu and
                  Arun Babu and
                  Jiatao Gu and
                  Michael Auli},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {data2vec: {A} General Framework for Self-supervised Learning in Speech,
                  Vision and Language},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {1298--1312},
  publisher    = {{PMLR}},
  year         = {2022},
}

@inproceedings{data2vec2.0,
  author       = {Alexei Baevski and
                  Arun Babu and
                  Wei{-}Ning Hsu and
                  Michael Auli},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Efficient Self-supervised Learning with Contextualized Target Representations
                  for Vision, Speech and Language},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {1416--1429},
  publisher    = {{PMLR}},
  year         = {2023},
}

@article{CREMA-D,
  author       = {Houwei Cao and
                  David G. Cooper and
                  Michael K. Keutmann and
                  Ruben C. Gur and
                  Ani Nenkova and
                  Ragini Verma},
  title        = {{CREMA-D:} Crowd-Sourced Emotional Multimodal Actors Dataset},
  journal      = {{IEEE} Trans. Affect. Comput.},
  volume       = {5},
  number       = {4},
  pages        = {377--390},
  year         = {2014},
}

@inproceedings{ssl_ser,
  author       = {Abinay Reddy Naini and
                  Mary A. Kohler and
                  Elizabeth Richerson and
                  Donita Robinson and
                  Carlos Busso},
  title        = {Generalization of Self-Supervised Learning-Based Representations for
                  Cross-Domain Speech Emotion Recognition},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2024, Seoul, Republic of Korea, April 14-19, 2024},
  pages        = {12031--12035},
  publisher    = {{IEEE}},
  year         = {2024},
}

@inproceedings{ssl_ser2,
  author       = {Zhi Zhu and
                  Yoshinao Sato},
  title        = {Deep Investigation of Intermediate Representations in Self-Supervised
                  Learning Models for Speech Emotion Recognition},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech, and Signal Processing,
                  {ICASSP} 2023 - Workshops, Rhodes Island, Greece, June 4-10, 2023},
  pages        = {1--5},
  publisher    = {{IEEE}},
  year         = {2023},
}

@article{asvp-esd,
  title={ASVP-ESD: A dataset and its benchmark for emotion recognition using both speech and non-speech utterances},
  author={Landry, Dejoli and He, Qianhua and Yan, Haikang and Li, Yanxiong},
  journal={Global Scientific Journals},
  volume={8},
  pages={1793--1798},
  year={2020}
}

@inproceedings{mer2024,
  author       = {Zheng Lian and
                  Haiyang Sun and
                  Licai Sun and
                  Zhuofan Wen and
                  Siyuan Zhang and
                  Shun Chen and
                  Hao Gu and
                  Jinming Zhao and
                  Ziyang Ma and
                  Xie Chen and
                  Jiangyan Yi and
                  Rui Liu and
                  Kele Xu and
                  Bin Liu and
                  Erik Cambria and
                  Guoying Zhao and
                  Bj{\"{o}}rn W. Schuller and
                  Jianhua Tao},
  editor       = {Jianhua Tao and
                  Shreya Ghosh and
                  Zheng Lian and
                  Zhixi Cai and
                  Bj{\"{o}}rn W. Schuller and
                  Abhinav Dhall and
                  Guoying Zhao and
                  Dimitrios Kollias and
                  Erik Cambria and
                  Roland Goecke and
                  Tom Gedeon},
  title        = {{MER} 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary
                  Multimodal Emotion Recognition},
  booktitle    = {Proceedings of the 2nd International Workshop on Multimodal and Responsible
                  Affective Computing, {MRAC} 2024, Melbourne VIC, Australia, 28 October
                  2024- 1 November 2024},
  pages        = {41--48},
  publisher    = {{ACM}},
  year         = {2024},
}

@inproceedings{biic,
  author       = {Shreya G. Upadhyay and
                  Woan{-}Shiuan Chien and
                  Bo{-}Hao Su and
                  Lucas Goncalves and
                  Ya{-}Tse Wu and
                  Ali N. Salman and
                  Carlos Busso and
                  Chi{-}Chun Lee},
  title        = {An Intelligent Infrastructure Toward Large Scale Naturalistic Affective
                  Speech Corpora Collection},
  booktitle    = {11th International Conference on Affective Computing and Intelligent
                  Interaction, {ACII} 2023, Cambridge, MA, USA, September 10-13, 2023},
  pages        = {1--8},
  publisher    = {{IEEE}},
  year         = {2023},
}

@article{IEMOCAP,
  author       = {Carlos Busso and
                  Murtaza Bulut and
                  Chi{-}Chun Lee and
                  Abe Kazemzadeh and
                  Emily Mower and
                  Samuel Kim and
                  Jeannette N. Chang and
                  Sungbok Lee and
                  Shrikanth S. Narayanan},
  title        = {{IEMOCAP:} interactive emotional dyadic motion capture database},
  journal      = {Lang. Resour. Evaluation},
  volume       = {42},
  number       = {4},
  pages        = {335--359},
  year         = {2008},
}

@inproceedings{msp,
  author       = {Luz Martinez{-}Lucas and
                  Mohammed Abdelwahab and
                  Carlos Busso},
  editor       = {Helen Meng and
                  Bo Xu and
                  Thomas Fang Zheng},
  title        = {The MSP-Conversation Corpus},
  booktitle    = {21st Annual Conference of the International Speech Communication Association,
                  Interspeech 2020, Virtual Event, Shanghai, China, October 25-29, 2020},
  pages        = {1823--1827},
  publisher    = {{ISCA}},
  year         = {2020},
}

@inproceedings{ESD,
  author       = {Kun Zhou and
                  Berrak Sisman and
                  Rui Liu and
                  Haizhou Li},
  title        = {Seen and Unseen Emotional Style Transfer for Voice Conversion with
                  {A} New Emotional Speech Dataset},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages        = {920--924},
  publisher    = {{IEEE}},
  year         = {2021},
}

@inproceedings{casia,
  title={Design of speech corpus for mandarin text to speech},
  author={Zhang, JTFLM and Jia, Huibin},
  booktitle={The blizzard challenge 2008 workshop},
  year={2008},
}

@inproceedings{meld,
  author       = {Soujanya Poria and
                  Devamanyu Hazarika and
                  Navonil Majumder and
                  Gautam Naik and
                  Erik Cambria and
                  Rada Mihalcea},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {{MELD:} {A} Multimodal Multi-Party Dataset for Emotion Recognition
                  in Conversations},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {527--536},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}

@inproceedings{cot_task,
  author       = {Lei Wang and
                  Yi Hu and
                  Jiabang He and
                  Xing Xu and
                  Ning Liu and
                  Hui Liu and
                  Heng Tao Shen},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language
                  Model Signals for Science Question Answering},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {19162--19170},
  publisher    = {{AAAI} Press},
  year         = {2024},
}

@article{ravdess,
  title={The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
  author={Livingstone, Steven R and Russo, Frank A},
  journal={PloS one},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA},
}

@article{savee,
  title={Surrey audio-visual expressed emotion (savee) database},
  author={Jackson, Philip and Haq, SJUoSG},
  journal={University of Surrey: Guildford, UK},
  year={2014}
}

@inproceedings{m3ed,
  author       = {Jinming Zhao and
                  Tenggan Zhang and
                  Jingwen Hu and
                  Yuchen Liu and
                  Qin Jin and
                  Xinchao Wang and
                  Haizhou Li},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {{M3ED:} Multi-modal Multi-scene Multi-label Emotional Dialogue Database},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {5699--5710},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
}

@inproceedings{emobox,
  title     = {EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark},
  author    = {Ziyang Ma and Mingjie Chen and Hezhao Zhang and Zhisheng Zheng and Wenxi Chen and Xiquan Li and Jiaxin Ye and Xie Chen and Thomas Hain},
  year      = {2024},
  booktitle = {Interspeech 2024},
  pages     = {1580--1584},
  doi       = {10.21437/Interspeech.2024-788},
  issn      = {2958-1796},
}

@article{emov-db,
  author       = {Adaeze Adigwe and
                  No{\'{e}} Tits and
                  Kevin El Haddad and
                  Sarah Ostadabbas and
                  Thierry Dutoit},
  title        = {The Emotional Voices Database: Towards Controlling the Emotion Dimension
                  in Voice Generation Systems},
  journal      = {CoRR},
  volume       = {abs/1806.09514},
  year         = {2018},
}

@inproceedings{emovo,
  author       = {Giovanni Costantini and
                  Iacopo Iaderola and
                  Andrea Paoloni and
                  Massimiliano Todisco},
  editor       = {Nicoletta Calzolari and
                  Khalid Choukri and
                  Thierry Declerck and
                  Hrafn Loftsson and
                  Bente Maegaard and
                  Joseph Mariani and
                  Asunci{\'{o}}n Moreno and
                  Jan Odijk and
                  Stelios Piperidis},
  title        = {{EMOVO} Corpus: an Italian Emotional Speech Database},
  booktitle    = {Proceedings of the Ninth International Conference on Language Resources
                  and Evaluation, {LREC} 2014, Reykjavik, Iceland, May 26-31, 2014},
  pages        = {3501--3504},
  publisher    = {European Language Resources Association {(ELRA)}},
  year         = {2014},
}

@inproceedings{mesd,
  author       = {Mathilde Marie Duville and
                  Luz Mar{\'{\i}}a Alonso{-}Valerdi and
                  David Ibarra{-}Zarate},
  title        = {The Mexican Emotional Speech Database {(MESD):} elaboration and assessment
                  based on machine learning\({}^{\mbox{*}}\)},
  booktitle    = {43rd Annual International Conference of the {IEEE} Engineering in
                  Medicine {\&} Biology Society, {EMBC} 2021, Mexico, November 1-5,
                  2021},
  pages        = {1644--1647},
  publisher    = {{IEEE}},
  year         = {2021},
}

@inproceedings{emilia,
  title={Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation},
  author={He, Haorui and Shang, Zengqiang and Wang, Chaoren and Li, Xuyuan and Gu, Yicheng and Hua, Hua and Liu, Liwei and Yang, Chen and Li, Jiaqi and Shi, Peiyang and others},
  booktitle={2024 IEEE Spoken Language Technology Workshop (SLT)},
  pages={885--890},
  year={2024},
  organization={IEEE},
}

@article{glm,
  author       = {Aohan Zeng and
                  Bin Xu and
                  Bowen Wang and
                  Chenhui Zhang and
                  Da Yin and
                  Diego Rojas and
                  Guanyu Feng and
                  Hanlin Zhao and
                  Hanyu Lai and
                  Hao Yu and
                  Hongning Wang and
                  Jiadai Sun and
                  Jiajie Zhang and
                  Jiale Cheng and
                  Jiayi Gui and
                  Jie Tang and
                  Jing Zhang and
                  Juanzi Li and
                  Lei Zhao and
                  Lindong Wu and
                  Lucen Zhong and
                  Mingdao Liu and
                  Minlie Huang and
                  Peng Zhang and
                  Qinkai Zheng and
                  Rui Lu and
                  Shuaiqi Duan and
                  Shudan Zhang and
                  Shulin Cao and
                  Shuxun Yang and
                  Weng Lam Tam and
                  Wenyi Zhao and
                  Xiao Liu and
                  Xiao Xia and
                  Xiaohan Zhang and
                  Xiaotao Gu and
                  Xin Lv and
                  Xinghan Liu and
                  Xinyi Liu and
                  Xinyue Yang and
                  Xixuan Song and
                  Xunkai Zhang and
                  Yifan An and
                  Yifan Xu and
                  Yilin Niu and
                  Yuantao Yang and
                  Yueyan Li and
                  Yushi Bai and
                  Yuxiao Dong and
                  Zehan Qi and
                  Zhaoyu Wang and
                  Zhen Yang and
                  Zhengxiao Du and
                  Zhenyu Hou and
                  Zihan Wang},
  title        = {ChatGLM: {A} Family of Large Language Models from {GLM-130B} to {GLM-4}
                  All Tools},
  journal      = {CoRR},
  volume       = {abs/2406.12793},
  year         = {2024},
}

@inproceedings{excot1,
  author       = {Xiang Yue and
                  Xingwei Qu and
                  Ge Zhang and
                  Yao Fu and
                  Wenhao Huang and
                  Huan Sun and
                  Yu Su and
                  Wenhu Chen},
  title        = {MAmmoTH: Building Math Generalist Models through Hybrid Instruction
                  Tuning},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
}

@inproceedings{excot2,
  author       = {Longhui Yu and
                  Weisen Jiang and
                  Han Shi and
                  Jincheng Yu and
                  Zhengying Liu and
                  Yu Zhang and
                  James T. Kwok and
                  Zhenguo Li and
                  Adrian Weller and
                  Weiyang Liu},
  title        = {MetaMath: Bootstrap Your Own Mathematical Questions for Large Language
                  Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
}

@article{imcot1,
  author       = {Robin Shing{-}Hei Yuen and
                  Timothy Tin{-}Long Tse and
                  Jian Zhu},
  title        = {Internalizing {ASR} with Implicit Chain of Thought for Efficient Speech-to-Speech
                  Conversational {LLM}},
  journal      = {CoRR},
  volume       = {abs/2409.17353},
  year         = {2024},
}

@article{imcot2,
  author       = {Yuntian Deng and
                  Yejin Choi and
                  Stuart M. Shieber},
  title        = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step
                  by Step},
  journal      = {CoRR},
  volume       = {abs/2405.14838},
  year         = {2024},
}

@article{qwenaudio,
  author       = {Yunfei Chu and
                  Jin Xu and
                  Xiaohuan Zhou and
                  Qian Yang and
                  Shiliang Zhang and
                  Zhijie Yan and
                  Chang Zhou and
                  Jingren Zhou},
  title        = {Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale
                  Audio-Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.07919},
  year         = {2023},
}

@article{review-mer1,
  author       = {Smith K. Khare and
                  Victoria Blanes{-}Vidal and
                  Esmaeil S. Nadimi and
                  U. Rajendra Acharya},
  title        = {Emotion recognition and artificial intelligence: {A} systematic review
                  {(2014-2023)} and research recommendations},
  journal      = {Inf. Fusion},
  volume       = {102},
  pages        = {102019},
  year         = {2024},
}