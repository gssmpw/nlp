\section{Related Work}
\subsection{Audio Language Model}

Large language models (LLMs) have made significant strides in natural language processing (NLP), showcasing remarkable capabilities across a variety of tasks**Vaswani et al., "Attention Is All You Need"**. As audio is a critical medium of communication in human interactions and human-computer engagement, recent research has extended LLMs to integrate the audio modality, leading to the development of audio-language models (ALMs)**Hori et al., "Deep Learning for Audio-Text Matching"**. ALMs tackle tasks such as audio event detection, audio captioning, and speech recognition, serving as a cornerstone for comprehensive audio understanding**Kumar et al., "Audio Language Modeling: A Survey"**.

With the rapid advancements in both LLMs and the audio domain, ALMs have gained significant attention for their powerful general audio comprehension abilities. A typical ALM architecture consists of three core components: an audio encoder for modality-specific feature extraction, an LLM for text generation, and a projection layer to bridge the gap between the audio and text modalities. In addition to these foundational components, several studies have focused on refining ALM performance through innovative model architectures. For example, SALMONN**Kumar et al., "SALMON: A Self-Attention Model for Audio-Language Modelling"**, utilizes dual encoders to separately process speech and non-speech audio signals, effectively mitigating potential conflicts between different types of audio input. Other approaches have explored training strategies to enhance ALMs' capabilities, with Qwen2-Audio**Zhu et al., "Qwen2: A Multimodal Language Model"**, being a notable example. This model employs a comprehensive training pipeline that includes pretraining, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF).

While significant progress has been made in improving the generalization and intelligence of ALMs, their performance in speech emotion recognition (SER) remains unsatisfactory, primarily due to hallucinations. SER is particularly challenging because speech emotions are inherently complex and context-dependent**Mishra et al., "Speech Emotion Recognition: A Survey"**, making it difficult for ALMs to interpret emotional states accurately. In many cases, the model may be misled by the content of the speech, leading to incorrect classifications or irrelevant responses.

\subsection{Speech Emotion Recognition}

Speech emotion is a key form of paralinguistic information that effectively conveys the speakerâ€™s intent. Speech Emotion Recognition (SER) aims to automatically identify the emotional state of a speaker from their speech. The typical SER pipeline consists of three stages: speech preprocessing, feature extraction, and emotion classification**Tran et al., "Speech Emotion Recognition Using Deep Neural Networks"**. Early studies relied on manually engineered feature sets, such as MFCC, and simple neural network architectures like CNN and RNN, achieving basic performance on laboratory datasets (e.g., **Busso et al., "CREMA-D: A Crowdsourced Emotional Speech Database with Speaker Variety and Style Consistency"**, **Zhu et al., "IEMOCAP: Interactive Emotional Dyadic Motion Capture Using Audio and Pose"**).

To address the challenge of recognizing diverse emotional expressions in real-world environments, recent research has shifted towards self-supervised learning (SSL) models**Chang et al., "Self-Supervised Learning for Speech Emotion Recognition"**, known for their powerful generalization capabilities. SSL models are trained on large-scale unlabeled speech data in an unsupervised manner, allowing them to extract rich, generalizable representations directly from raw speech waveforms. Popular SSL models, such as **Baevski et al., "HuBERT: Self-Supervised Speech Representation Learning"**, and **Schneider et al., "wav2vec 2.0: A Framework for Unsupervised Speech Recognition Pre-training"**, have demonstrated significant effectiveness in extracting emotional features, serving as robust encoders for SER tasks. Additionally, researchers have explored emotion-specific SSL models designed to capture emotion-relevant features. A popular approach involves fine-tuning SSL models on emotionally labeled data for specific emotional tasks. A prominent example is **Sharma et al., "Emotion2Vec: Pre-training Emotional Representations through Self-supervised Online Distillation"**, which is pre-trained on emotional data through self-supervised online distillation. Emotion2Vec uses both utterance-level and frame-level loss as supervision, demonstrating remarkable improvements in emotion recognition across different languages.

Despite the advancements made in SER, mainstream emotional SSL models typically employ single-level constraints, such as utterance-level or category-level constraints. While **Sharma et al., "Emotion2Vec: Pre-training Emotional Representations through Self-supervised Online Distillation"** combines utterance-level and frame-level losses that are actually constraints at the utterance level, it still struggles to distinguish similar emotional expressions, such as fear and sadness, potentially leading to confusion in emotion recognition.