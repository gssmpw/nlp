\section{Related Work}
\subsection{Recommender System}
% first introduce CRS - on hold

Sequential recommendation methods**Hidash, "Temporal Encoders for Sequential Recommendation"**, **Huang et al., "Sequential Recommendation using Temporal Convolutional Networks"**, primarily focus on developing temporal encoders to capture both short- and long-term user interests. The evolution of these encoders has progressed from GRU units**Cho et al., "Description and Significance of Recurrent Neural Networks Learning Sequential Patterns"**, to more advanced architectures such as self-attention mechanisms (e.g., **Kang and McAuley, "Self-Attention Based Recurrent Model for Sequential Recommendation"**), bidirectional encoders (e.g., **Chen et al., "BERT4Rec: Improving User Engagement via BERT-based Next Item Recommendation"** with masked item training), graph neural networks**Wang et al., "Graph Attention Networks for Recommendation Systems"**, and other Transformer-based models**Song et al., "Transformer-based Sequential Recommendation Model with Graph-aware Attention Mechanism"**.
In the context of embracing large language models, generative recommenders**Zhou et al., "Generative Recommenders: A Novel Framework for Sequential Recommendation"** treat item indices as tokens and predict them in a generative manner. Meanwhile, LLMs**Li et al., "Utilizing Large Language Models to Improve Recommendation Performance"** are utilized to play as a sequential embedding extractor to improve the recommendation performance. In our framework design, all recommendation models can be considered as components of the tools.

Before large language model become popular, conversational recommendation system (CRS)**Liu et al., "Conversational Recommendation System: A Survey and Future Directions"** aims at designing better dialogue understanding models or incorporating reinforcement learning for multiple dialogues answering. Due to the capacity of the conventional language model, it lose the flexibility of the dialogue including the dialogue format and number of turns. To resolve this problem, some researchers**Zhang et al., "Leveraging Large Language Models for Conversational Recommendation Systems"** leverage the power of LLM to better understand the intention of user. 

% echo chamber based on the popularity and filtering of our work | introduce fairness yunqi to introduce to protect  | from protecting of irrevalant item / long-tailed items / 

The echo chamber effect occurs when individuals are exposed only to information and opinions that reinforce their existing beliefs within their social networks**Sunstein, "The Echo Chamber Effect"**, leading to a lack of diverse perspectives and increased polarization**McPherson et al., "Social Isolation in America: Changes in Core Discussion Networks Over Time"**. In the context of recommender systems, researchers have begun to study echo chambers and feedback loops**Kalimeris et al., "A Matrix Factorization-based Recommender System for Dynamic User Interests"**. Kalimeris et al.**Kalimeris et al., "A Matrix Factorization-based Recommender System for Dynamic User Interests"** propose a matrix factorization-based recommender system with a theoretical framework for modeling dynamic user interests, while $\partial$CCF**Wu et al., "Employing Counterfactual Reasoning to Mitigate Echo Chambers in Recommendation Systems"** employs counterfactual reasoning to mitigate echo chambers.

\subsection{Personal Language-based Agent}
In the early stages, some researchers**Bui et al., "Developing Dialogue Agents with Personas for Enhanced Dialogue Quality"** in the NLP field developed dialogue agents with personas to enhance dialogue quality. Language models**Liu et al., "Prompting Language Models with Role Descriptions for Simulating Realistic Interactions"** are prompted with role descriptions to simulate realistic interactions by storing experiences, synthesizing memories, and dynamically planning actions, resulting in believable individual and social behaviors within interactive environments. 
% webshop mind2web, travel agent or other agents
% recagent
WebShop**Wu et al., "Understanding Product Attributes from Human-Provided Text Instructions using Reinforcement Learning"** attempts to understand product attributes from human-provided text instructions using reinforcement learning and imitation learning. Similar to traditional conversational recommender systems (CRS)**Liu et al., "Conversational Recommendation System: A Survey and Future Directions"**, it is impractical for users to describe each product attribute every time. With the advancement of large language models (such as GPTs**Brown et al., "Language Models are Few-Shot Learners"**), many researchers**Zhang et al., "Leveraging Large Language Models for Conversational Recommendation Systems"** have begun designing domain-specific agents that integrate various tool learning and memory mechanisms.

More recently, recommendation agents (RecAgent)**Chen et al., "Designing User-Side RecAgents to Generate Ranking Results Based on User Instructions"** have been developed to simulate user behaviors and predict user-item interactions. A common design feature among these agents is the use of historical interaction information as user memory**Dziugaite et al., "Memory-Augmented Recommendation Agents for Dynamic User Preferences"**, with LLMs utilized to generate the ranking results. Unlike platform-side RecAgents, $\model$ and $\modelplus$ are the first to operate on the user side, generating re-ranking results based on user instructions and individual memory, unaffected by the influence of advantaged users.

% 1. [improve performance] trainable reranker or utilize existing ranking model as tools but combine with existing personalized memory mechanism.

% 2. [fair comparison] long-text more negative samples fair comparison. 

% 3. [More Interpretable]

% 4. [Agents interact with RecSys / autonomus interaction/ mutual learning ]


% 1. Rank as tools / 

% 2. Multimodal information input

% 3. Long-context more negative samples.

% 4. User agent and RecSys mutual learning/improvement

% 5. Next generation rec promote trustworthy recsys, more interpretable (text saved memory â€“ dynamic process accessible.), fairness (user instruction guided), | connect other task such as social network


% \bibliography{iclr2025_conference}
% \bibliographystyle{iclr2025_conference}
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-sigconf}
% \newpage

\clearpage
\onecolumn