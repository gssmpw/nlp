\section{Additional Experiments: Parameter Variations and Ablation Study}
\label{sec:other-experiments}

We describe additional experimental results with parameter variations and ablation studies. 
We describe the results by varying $\varepsilon$ and $\xi$ in~\cref{sec:vary-epsilon} and \cref{sec:vary-xi} respectively. We describe ablation studies for logarithmic-scaling quantization and progressive error bound tightening in~\cref{sec:ablation}. We only include results for the Earthquake and Ionization datasets due to space constraints. The trends reported are consistent across all datasets tested. In all plots of compression and decompression times, we omit the results of augmented Neurcomp as it is much slower than the other augmented compressors.

\subsection{Experiments with Varying Persistence Threshold}
\label{sec:vary-epsilon}

We measure the effect of varying the persistence threshold $\varepsilon$ on each of our six evaluation metrics for each augmented compressor on each dataset (except Nyx). We fix $\xi = 0.012$ and vary $\varepsilon \in \{0.02,0.03,0.04,0.05,0.06,0.07\}$. The results for the Earthquake and Ionization datasets are shown in~\cref{fig:epsilon-table}.

We observe that there is a clear positive correlation between $\varepsilon$ and compression ratio. This trend is reasonable: when $\varepsilon$ is larger, the simplified contour tree becomes simpler, and thus less precision is required to preserve the contour tree, leading to a high compression ratio. There is no clear correlation between $\varepsilon$ and any other evaluation metrics.

\begin{figure*}[!ht]
\centering
\includegraphics[width=\linewidth]{fig-epsilon_table_small.png}
\vspace{-6mm}
\caption{Each of the six evaluation metrics measured for the Earthquake and Ionization datasets as $\varepsilon$ varies. We fix $\xi = 0.012$. The augmented compressors are given as A-ZFP, A-SZ3, etc.}
\label{fig:epsilon-table}
\end{figure*}

\subsection{Experiments with Varying Pointwise Error Bound}
\label{sec:vary-xi}

We measure the effect of varying the pointwise error bound $\xi$ on each of our six evaluation metrics for each augmented compressor on four datasets (QMCPACK, Tangaroa, Earthquake, and Ionization). We fix $\varepsilon = 0.04$ and vary $\xi$ using the same values that were used to measure the trade-off between reconstruction quality and compression ratio. See \cref{sec:reconstruction-quality-extra} for specific parameter values.

The results are shown in \cref{fig:xi-table}. There is a clear positive correlation between $\xi$ and compression ratio. This trend is logical, as less information should need to be stored in order to maintain a higher error bound. Likewise, there is a clear negative trend between $\xi$ and reconstruction quality in terms of each PSNR, Wasserstein distance, and bottleneck distance. This trend is also logical as a higher error bound means that the reconstructed data does not need to be as faithful to the ground truth data. There is no clear trend between $\xi$ and compression or decompression time.

\begin{figure*}[]
    \includegraphics[width=\linewidth]{fig-xi_table_small.png}
    \vspace{-6mm}
    \captionof{figure}{Each of the six evaluation metrics measured for the Earthquake and Ionization datasets for each lossy compressor as $\xi$ varies. We fix $\varepsilon = 0.04$. The augmented compressors are given as A-ZFP, A-SZ3, etc.}
    \label{fig:xi-table}    
\end{figure*}

\subsection{Ablation Study}
\label{sec:ablation}

In our framework, there are two main components: logarithmic-scaling quantization and progressive bound tightening. We perform an ablation study to separately justify the benefits of these two components, in comparison with TopoSZ, which utilizes linear-scaling quantization and iterative bound tightening. 
We independently analyze each of the six evaluation metrics across five datasets, for all augmented compressors. 

In addition to studying iterative bound tightening based on the contour tree (as is used by TopoSZ), we also implement an iterative bound tightening based on the merge tree for the ablation study, where we iteratively tighten the upper and lower bounds until there are no false cases in the join and split trees respectively. For linear-scaling quantization, we use quantization intervals of width $\xi$ (like in TopoSZ), rather than the standard width of $2\xi$.

The results of the ablation study are shown in~\cref{fig:ablation-table}. The augmented compressors are given by A-ZFP, A-SZ3, etc. In each line chart, columns whose label start with `Lin' use linear-scaling quantization, whereas columns whose label starts with `Log' use logarithmic-scaling quantization. Columns whose labels end with `ItrCT,' `ItrMT,' and `Prog' refer to trials that use contour-tree-based iterative tightening, merge-tree-based iterative tightening, and progressive tightening respectively. For compression times, we omit the time collected for Ionization dataset using CSI and Log/ItrCT as it was much higher than the times reported by other compressors and configurations.

In \cref{fig:ablation-table}, we can see that logarithmic-scaling quantization always produces a higher compression ratio than linear-scaling quantization. Progressive or iterative tightening do not noticeably affect the compression ratio. The PSNR is essentially constant across each trial, although logarithmic-scaling quantization typically produces slightly lower PSNR values than linear-scaling quantization. The Wasserstein distance is similar across trials, but sometimes we observe slight increases to Wasserstein distance when using logarithmic-scaling quantization rather than linear-scaling quantization. We do see large fluctuations in the bottleneck distance between trials, and sometimes logarithmic-scaling quantization leads to an increase. However, across all trials there is no clear pattern.

In terms of compression time, we can see that progressive bound tightening outperforms iterative bound tightening in every trial. Also, it appears that iterative tightening based on the contour tree outperforms iterative tightening based on the merge trees in most trials. This trend suggests that the primary benefit of tightening based on the join and split trees is that doing so enables progressive tightening strategies. When progressive tightening is used, using logarithmic-scaling quantization sometimes leads to slightly longer run times than linear-scaling quantization. Neither the tightening strategy or the quantization strategy appear to offer any advantage or disadvantage in terms of decompression time.

\begin{figure*}[!ht]
    \includegraphics[width=\linewidth]{fig-ablation_small.png}
    \vspace{-6mm}
    \captionof{figure}{Each of the six evaluation metrics measured for the Earthquake and Ionization datasets for each lossy compressor in an ablation study. Augmented compressors are given as A-ZFP, A-SZ3, etc. Run times from augmented Neurcomp, and those from augmented CSI are excluded as they are much higher compared to the others. Each point on the $x$-axis represents a combination of one quantization strategy (linear-scaling vs logarithmic-scaling) and one tightening strategy: iterative with contour trees such as in TopoSZ; iterative with merge trees; or progressive bound tightening.
    Lin: linear scaling quantization. 
    Log: logarithmic-scaling quantization. 
    Prg: progressive upper and lower bound tightening. 
    ItrCT: contour-tree based iterative upper and lower bound tightening. 
    IterMT: merge-tree based iterative upper and lower bound tightening.}
    \label{fig:ablation-table}
\end{figure*}