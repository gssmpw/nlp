\section{Technical Background}
\label{sec:background}

\subsection{Merge Tree and Contour Tree}
\label{sec:merge-and-contour-tree}
\para{Merge Tree.} 
Let $f:\X \rightarrow \R$ be a continuous scalar field defined on a simply connected domain $\X$. 
The \emph{sublevel set} of $f$ at a threshold $t \in \R$ is defined as 
$\X_t = f^{-1}(-\infty,t] := \{ x \in X \mid f(x) \leq t \}$. 
The \emph{merge tree} of $f$ tracks when (connected) components of $\X_t$ appear and merge as $t$ increases. 
$\X_t$ evolves from being an empty set to contain components surrounding various local minima; these components then merge into one other until eventually there is only a single component.
Leaves of the merge tree correspond to local minima, and interior nodes correspond to saddles where components merge. 
\Cref{fig:contour-tree}(A) and (C) visualize a scalar field and its merge tree.
Formally, we define an equivalence relation $\sim$ on $\X$. We say that $x \sim y$ if and only if $f(x) = f(y)=t$ and $x$ belongs to the same component of $\X_t$ as $y$. The merge tree of $f$ is defined by the quotient space $\X/{\sim}$. 

The merge tree of $f$ defined above is sometimes referred to as the \emph{join tree}, whereas the merge tree of $-f$ is called a \emph{split tree} (see~\cref{fig:contour-tree}(B) for its visualization). The merge tree naturally induces a segmentation of the domain. Let $\phi$ be the canonical map that maps each $x \in \X$ to its equivalence class $[x]$ under $\sim$. Then for each edge $e$ of the contour tree, $\phi^{-1}(e)$ is a monotonic region in $\X$. The inverse image of each edge partitions the domain, which is called the merge-tree-induced segmentation. See \cref{fig:contour-tree}(D) (cf.,~(E)) for a split-tree-induced segmentation.

\para{Contour Tree.} 
The \emph{level set} of $f$ at a threshold $t \in \R$ is $f^{-1}(t):= \{ x \in X \mid f(x) = t \}$.
Each component of $f^{-1}(t)$ is called a \emph{contour}. 
A \emph{contour tree} tracks the relations among contours as $t$ increases. 
Analogous to a merge tree, as $t$ increases, components of $f^{-1}(t)$ appear at local minima, disappear at local maxima, and join or split at saddles. 
Formally, we define another equivalence relation $\sim$ on $\X$, where $x \sim y$ if and only if $f(x) = f(y)$ and $x$ belongs to the same contour as $y$. 
The contour tree is the quotient space $T=T(\X,f):= \X / \sim$. 
There is a 1-1 correspondence between the local extrema of $\X$ and the leaves of $T$. The interior nodes of $T$ correspond to a subset of the saddle points of $\X$ \cite{reeb1946points}. \cref{fig:contour-tree}(F) visualizes the contour tree. Similar to the merge tree, we can define a contour-tree-induced segmentation.

The classic algorithm for computing the contour tree~\cite{carr2003computing} combines join and split trees together to form a contour tree, and comes with a state-of-the-art multi-core implementation~\cite{gueunet2017task}.
In this paper, we compute the contour tree of the input data to determine the initial point-wise error bound. We use the algorithm of Gueunet et al.~\cite{gueunet2017task} (with an in-house implementation) to compute join and split trees, and the algorithm of Carr et al.~\cite{carr2003computing} to combine them into the contour tree. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{fig-ct_example.png}
    \vspace{-6mm}
    \caption{(A) Visualizing the graph of a 2D scalar field $f$ defined on a square domain. Local minima are in blue, saddles are in white, and local maxima are in red. (B) Split tree of f.  (C) Merge tree of $f$ (a.k.a.,~join tree). (D) The domain is colored according to split-tree-induced segmentation. (E) Split tree is colored according to the segmentation in (D). (F) Contour tree of $f$. (G) The graph of $f$ after its persistence simplification where a peak is removed. (H) Split tree of $f$ after persistence simplification. Edge $ab$ is removed. (F) Contour tree of $f$ after the merge tree in (B) has been simplified. }
    \label{fig:contour-tree}
    \vspace{-6mm}
\end{figure}

The algorithm by Gueunet et al.~\cite{gueunet2017task} constructs the merge tree one edge at a time. First, starting from each minimum $m$, the algorithm visits points surrounding $m$ in an increasing order until a saddle $s$ is reached. The edge $ms$ is discovered and added to the merge tree; in other words, $m$ is \emph{grown} during this process.  
For each saddle $s$, once all edges that terminate at $s$ have been discovered, $s$ is grown until some new saddle $s'$ is reached, and the edge $ss'$ is discovered and added to the merge tree. This process repeats until the merge tree has been completely discovered. 
To grow from a minimum or a saddle, the points surrounding that minimum or saddle are stored in a heap to ensure that they are processed in an increasing order. The use of heaps presents the main performance bottleneck of the algorithm.

\para{Persistence simplification.} 
In practice, real-world data typically contain noise that creates many small branches in the merge or contour tree, where persistence simplification~\cite{edelsbrunner2001hierarchical} can be used to eliminate these small branches and thereby separate topological features from noise. 
In the context of merge trees, ordinary persistent homology pairs a local extremum (i.e.,~a peak or a valley in \cref{fig:contour-tree}(A)) with a nearby saddle and assigns the pair a value of \emph{persistence}, which describes the scale at which the pair disappears via a perturbation to the function. The persistence is equal to the absolute difference in function value between an extremum and its paired saddle. A function $f$ is simplified by perturbing the function values in order to cancel pairs of critical points below a certain persistence threshold $\varepsilon$. See \cref{fig:contour-tree}(G) for an example. The cancellation of one pair of critical points of $f$ corresponds to a branch being removed from the merge tree, giving rise to its simplification.
The simplified contour tree is more complex \cite{hristov2021w}, but it can be computed by combining the simplified join and split trees. 

In~\cref{fig:contour-tree}, (G) depicts the graph of a function after persistence simplification. For the split tree shown in (B), assuming a persistence threshold of $\varepsilon \geq |f(a)-f(b)|$, the edge $ab$ will be removed after a persistence simplification at $\varepsilon$. As a result, node $c$ is directly connected to node $d$ in the split tree (H), and the join tree in (C) remains unchanged. (I) depicts the contour tree produced by combining the simplified split tree in (H) with the join tree in (C).

\subsection{A Review on TopoSZ}
\label{sec:TopoSZ}

Our framework builds upon a few ingredients from TopoSZ~\cite{yan2023toposz}. 
TopoSZ, in turn, modifies the pipeline from the error-bounded lossy compressor SZ version 1.4 \cite{tao2017significantly}. 

Let $f$ represent the input scalar field, and $f'$ be the reconstructed scalar field (after compression and decompression). 
Let $T$ be the contour tree of $f$ and $T_\varepsilon$ the persistence simplified contour tree at a threshold of $\varepsilon$. Let $T'$ and $T'_\varepsilon$ be defined analogously for $f'$.

SZ1.4 allows the user to specify $\xi$, a pointwise error bound during compression. In turn, there are two user-defined parameters in TopoSZ: a persistence threshold $\varepsilon$, and a pointwise error bound $\xi$. Unlike TopoQZ, TopoSZ does not require that $\varepsilon < \xi$. 
TopoSZ guarantees the preservation of the persistence simplified contour tree during compression while maintaining the pointwise error bound. 
That is, it guarantees that $T_\varepsilon = T'_\varepsilon$, and $|f(x)-f'(x)| \leq \xi$ for each $x \in \X$.

\para{Linear-scaling quantization.} 
SZ version 1.4 introduces a linear-scaling quantization technique to ensure that a strict absolute error bound $\xi$ is maintained. This technique is implemented in TopoSZ.

For each point $x \in \X$ with a ground truth value $f(x)$, an initial guess for its value $g(x)$ (e.g., from a Lorenzo predictor; see below) is shifted by an integer multiple of $2\xi$ to obtain a new value $f'(x)$ such that $|f'(x) - f(x)| \leq \xi$. 

This process can be conceptualized as follows: divide the real line into intervals of length $2\xi$, where one interval is centered on $g(x)$. 
The compressor then calculates how many intervals to shift $g(x)$, so that it can assign a value to $f'(x)$ that is a distance less than $\xi$ from $f(x)$. By construction, if $f(x)$ lies in an interval of length $2\xi$ centered on $f'(x)$, then $|f(x)-f'(x)| \leq \xi$. 
This process is illustrated in \cref{fig:linear-scaling-quantization}.
\begin{figure}[!ht]
  \vspace{-3mm}
  \centering
  \includegraphics[width=\columnwidth]{fig-linear-scaling-quantization.pdf}
  \vspace{-8mm}
  \caption{A standard implementation of a linear-scaling quantization.}
  \label{fig:linear-scaling-quantization}
  \vspace{-4mm}
\end{figure}
\noindent Following this construction, each $x \in \X$ is assigned an integer $n_x$ (corresponding to how many intervals $g(x)$ is shifted) such that $f'(x) = g(x) + 2\xi n_x$. These quantization numbers $\{n_x\}$ are encoded and stored in the compressed file.

If the distribution of $\{n_x\}$ has low entropy (e.g.,~if $\{n_x\}$ are mostly zeros), then the quantization numbers can be compressed to small size using an entropy-based compression algorithm, such as Huffman coding. 
More accurate predictions $g(x)$ generally lead to $\{n_x\}$ with lower entropy.

\para{False Cases.} Yan et al.~\cite{yan2023toposz} introduced three types of false cases to quantify the level of contour tree preservation: false positives, false negatives, and false types, which are illustrated in \cref{fig:false-cases}.
A false positive occurs when a new edge appears in the contour tree of the reconstructed data that does not exist in the same position of the contour tree of the original data. A false negative occurs when an edge of the contour tree from the original data is missing from the contour tree of the reconstructed data. 
A false type occurs when the critical type (maximum, minimum, saddle) of one or both endpoints of an edge of the contour tree does not match between the original and reconstructed data.
TopoSZ focuses on false cases involving extremum-saddle pairs and its algorithm terminates when there are no such false cases. 

\begin{figure}[!ht]
    \vspace{-2mm}
    \centering
    \includegraphics[width=\linewidth]{fig-false-cases.pdf}
    \vspace{-6mm}
    \caption{Three types of false cases. (A) The original contour tree. (B) A false positive: an extra edge is added. (C) A false negative: an edge is missing. (D) A false type: an edge contains a critical point as its endpoint that changes its type.}
    \label{fig:false-cases}
    \vspace{-4mm}
\end{figure}


\para{TopoSZ Pipeline.} The TopoSZ pipeline is as follows:

\para{\underline{Step 1: Upper and lower bound calculation.}}
TopoSZ first applies persistence simplification to $f$ in order to calculate $T_\varepsilon$. For each point $x \in \X$, a lower bound $L(x)$ and an upper bound $U(x)$ are assigned to $x$ according to the contour-tree-induced segmentation.
If $x$ belongs to the segmented region corresponding to edge $ab \in T_\varepsilon$, then $L(x) = \min(f(a),f(b))$ and $U(x) = \max(f(a),f(b))$. The nodes of the contour tree are stored losslessly.

\para{\underline{Step 2: Prediction.}} 
TopoSZ uses a Lorenzo predictor \cite{ibarria2003out} to predict the values of each data point. For each point $x$, the Lorenzo predictor predicts $f(x)$ as a weighted sum of the values from previously predicted points $x'$ satisfying $\|x-x'\|_\infty = 1$. The weights are fixed, and are chosen such that quadratic functions will be perfectly predicted.

\para{\underline{Step 3: Linear-scaling quantization.}} TopoSZ uses linear-scaling quantization with a decreased interval size to ensure that the pointwise upper and lower bounds, as well as the global error bound $\xi$, are maintained for each $x \in \X$. For any $x \in \X$ where no possible quantization code $n_x$ satisfies these conditions, $f(x)$ is stored losslessly.

\para{\underline{Step 4: Iterative upper and lower bound tightening.}} 
If the results from Step 3 do not perfectly preserve the contour tree, that is, if there are false cases presented in the reconstructed data, then the upper and lower bounds are tightened around points corresponding to those false edges, and then Step 3 is repeated. This cycle repeats until there are no false cases. 
%The specifics of this step are reviewed in \cref{sec:topoSZ-detail}.

\para{\underline{Step 5: Lossless compression.}} The numbers from linear-scaling quantization are encoded using Huffman Coding. The relevant information is then stored in a binary file that is further compressed using ZSTD~\cite{collet2018zstandard}.