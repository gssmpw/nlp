\section{Method}
\label{sec:method}
We give an overview of our framework in~\cref{sec:method-overview}.
We then describe two novel and technical ingredients in our framework: the logarithmic-scaling quantization (\cref{sec:augment-quantization}) and the progressive upper and lower bound tightening (\cref{sec:augment-tightening}).

\subsection{Overview}
\label{sec:method-overview}

We now describe our framework for augmenting any lossy compressor (called a \emph{base compressor}) to preserve contour trees and maintain strict error bounds. 
Our framework requires two user-specified parameters, a persistence threshold $\varepsilon$ and a pointwise absolute error bound $\xi$. 
It also requires user-specified parameters associated with the specific base compressor being augmented. Our implementation works with rectilinear meshes, and it could easily be modified to work with any simply-connected tetrahedral mesh.

Our framework guarantees that, for any augmented compressor, $T_\varepsilon = T_\varepsilon'$ and $|f(x)-f'(x)| \leq \xi$ for every $x \in \X$. Starting with a standard compressor as the base compressor, we start with a step-by-step overview of our framework. 

\para{\underline{Step 1: Upper and lower bound calculation.}}~We store critical points of the simplified contour tree $T_\varepsilon$ losslessly. We calculate the initial pointwise upper and lower bounds for other point $x \in \X$. The key idea is to locate an edge $ab$ in $T_{\varepsilon}$ whose corresponding range of function values contains $f(x)$. This requires a careful computation using the join and split trees of $T_{\varepsilon}$; see \cref{sec:algorithm-details} for details. 
We let $L(x) = \min(f(a),f(b)) + \zeta$ and $U(x) = \max(f(a),f(b))-\zeta$, where $\zeta = 10^{-5}|f(b)-f(a)|$.
If we allow $x$ to have the same function value as $a$ or $b$, the topology may be altered (e.g., along the boundary of the induced region), resulting in more false cases. Adjusting the error bound by $\zeta$ prevents such issues. We also adjust $L(x)$ and $U(x)$ as needed to ensure that if $L(x) \leq f'(x) \leq U(x)$ then $|f(x)-f'(x)| \leq \xi$. 
 
When computing $T_\varepsilon$, we compute the join and split trees of $f$ and simplify the trees directly with persistence threshold $\varepsilon$. We then combine them to obtain $T_\varepsilon$. During this construction, we track which edge of $T_\varepsilon$ each point $x \in X$ corresponds to. Compared to simplifying the entire scalar field $f$ and then computing the contour tree of the simplified field (like TopoSZ), our strategy leads to equivalent results in less time.

\para{\underline{Step 2: Base compressor.}} 
We apply the base compressor to the input data $f$. 
We compress and then decompress the data to assess changes that need to be made during decompression. 
We refer to the compressed-then-decompressed data as the \emph{intermediate data}.

\para{\underline{Step 3: Logarithmic-scaling quantization.}} 
We introduce a novel quantization technique that respects the pointwise upper and lower bounds imposed in Step 1. 
If possible, the entropy of the quantization numbers $\{n_x\}$ will be identical to that of standard linear-scaling quantization.
However, when linear-scaling quantization cannot produce a prediction for a point $x$ that respects $L(x)$ and $U(x)$, $x$ will be quantized with more precision (i.e.,~$\xi \leftarrow \xi/2$) to satisfy those bounds.

\para{\underline{Step 4: Progressive upper and lower bound tightening.}} 
We introduce a novel technique for calculating adjustments to the intermediate data to guarantee that the contour tree is preserved.
We compute the join and split trees directly. If a false edge is detected during computation, the upper and lower bounds are tightened around points in the segmentation region corresponding to the edge (see \cref{sec:merge-and-contour-tree}). All edges whose growth involved these points are recomputed.
We continue until the join and split trees of the decompressed data match those of the ground truth. We do not compute the contour tree directly as the preservation of the join and split trees guarantees the preservation of the contour tree.

\para{\underline{Step 5: Lossless compression.}} 
We encode the quantization numbers using Huffman coding. The output of the base compressor, the encoded quantization numbers, and any losslessly stored values are written to a binary file which is further losslessly compressed using xz, a general-purpose data compression tool available via {XZ Utils}~\cite{XZUtils}.

\subsection{Logarithmic-Scaling Quantization}
\label{sec:augment-quantization}

We now describe the first novel ingredient in our framework: a variable precision quantization technique that preserves tight pointwise upper and lower bounds. %without significantly compromising the entropy of the overall distribution of quantization numbers. 
For each $x \in \X$, the intermediate data contains an estimated value $g(x)$ for the ground truth value $f(x)$. 
Let $L(x)$ and $U(x)$ denote the lower and upper bounds assigned to $x$.
To ensure that $L(x) \leq f'(x) \leq U(x)$, we assign to each $x \in \X$ a numerator $a_x \in \Z$ and a precision $p_x \in \N$ that indicates the number of iterations. 
Our reconstructed value is 
\begin{equation}
f'(x) = g(x) + \frac{2\xi \cdot a_x}{2^{p_x}}.
\label{eq:fprime-original}
\end{equation}

To calculate each $a_x$ and $p_x$, we first set $p_x=0$. 
We then look for the value of $a_x$ satisfying 
\begin{equation*}
L(x) \leq g(x) + \frac{2\xi \cdot a_x}{2^{p_x}} \leq U(x)
\label{eq:Bounds}
\end{equation*}
such that $|a_x|$ is minimized. If there is no valid value of $a_x$, we increase $p_x$ by $1$ and search again. This process is repeated until a valid $a_x$ is found. If $p_x$ reaches an arbitrary threshold, we stop searching and instead store $f(x)$ losslessly. We set this threshold equal to $11$.

When $p_x = 0$, the above process is the same as the standard linear-scaling quantization, except that we also seek to maintain the upper and lower bounds. 
Each time a linear-scaling quantization fails to identify a valid choice for $a_x$ that yields a value of $f'(x)$ within the upper and lower bounds for $x$, we cut the interval lengths in half by increasing $p_x$ by $1$ and continue searching.
When the interval lengths are smaller, it is more likely that a valid choice of $a_x$ exists. 
It is also possible that during an iteration, multiple valid choices of $a_x$ exist, so we choose the one with the smallest absolute value to minimize the entropy of $\{a_x\}$. 

\begin{figure}[!ht]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{fig-log-scale-quantization.pdf}
    \vspace{-6mm}
    \caption{(A) If $p_x = 0$, there are no valid quantization intervals. (B) Increasing $p_x$ to $1$ allows for a valid quantization interval.}
    \label{fig:log-scale-quantization}
    \vspace{-2mm}
\end{figure}

This process is illustrated in \cref{fig:log-scale-quantization}. 
(A) contains an example where there are no quantization intervals where we can place $f'(x)$ to respect the upper and lower bounds. 
In (B), by raising the precision $p_x$ by 1, the quantization intervals are halved, giving a valid choice for $f'(x)$.

When encoding the data, we store a single quantization number $n_x$ for each $x \in \X$. 
To calculate each $n_x$, we first find the maximum precision $p_m$ used for any single point. The points are assigned the single quantization number $n_x = a_x \cdot 2^{p_m-p_x}$ and the max precision $p_m$ is stored in the compressed output. 
During decompression, the point $x$ is assigned the value 

\begin{equation}
f'(x) = g(x) + \frac{2\xi \cdot n_x}{2^{p_m}}.
\label{eq:fprime}
\end{equation}
Setting $n_x = a_x \cdot 2^{p_m-p_x}$ in Eq.~\eqref{eq:fprime} means that
\begin{equation*}
  g(x) + \frac{2\xi \cdot n_x}{2^{p_m}} = g(x) + \frac{2\xi \cdot a_x \cdot 2^{p_m-p_x}}{2^{p_m}} = g(x) + \frac{2\xi \cdot a_x}{2^{p_x}}.
  \label{eq:logscale}  
\end{equation*}
Therefore, the formulation in Eq.~\eqref{eq:fprime} is equivalent to the original formulation of $f'$ in Eq.~\eqref{eq:fprime-original}.

In comparison with TopoSZ, the above variable precision technique allows us to store fewer points losslessly.
In order to ensure the quantization numbers do not get too large, if any point has a precision greater than $10$ it is stored losslessly. This ensures that $p_m \leq 10$ for all trials.

\subsection{Progressive Upper and Lower Bound Tightening}
\label{sec:augment-tightening}

We now describe the second novel ingredient in our framework, namely, a \emph{progressive error bound tightening} process. 
Specifically, the process computes the join and split trees of the decompressed data. During the computation, it detects false cases, and tightens the upper and lower bounds in the neighborhoods of false cases. The algorithm progresses through merge tree computation, checking the correctness of each edge and tightening when needed, until every edge is correctly preserved.
The process allows us to bypass iteratively recomputing the entire contour tree (in the case of TopoSZ), significantly speeding up the compression process. During the tightening process, we work with merge trees (instead of contour trees), since the persistence of a leaf (local extremum) can be computed from its nearby saddle based on branch decomposition (i.e.,~local information), thereby allowing for our progressive tightening strategy. By contract, computing the persistence of a leaf of a contour tree may require global information from the whole contour tree due to the existence of V and W structures~\cite{hristov2021w}.

We describe this process for the join tree, which works analogously for the split tree. We only consider false cases involving extremum-saddle pairs. 

\para{False case detection}. To detect false cases, we construct $T'$. Doing so allows us to locate mismatches between edges in $T'_\varepsilon$ and those in $T_\varepsilon$.
We construct $T'$ using a modified version of the edge growing procedure from local minima and saddles (see~\cref{sec:merge-and-contour-tree}).
To start, we extract a list of local minima of $f'$ sorted by decreasing function values. Then, proceeding in sorted order, we grow an edge from each local minimum $m$ to a saddle $s$, and check two cases for $s$; see \cref{sec:algorithm-details} for illustrations: 

\underline{Case (I).} If $s$ is unpaired, i.e., $m$ is the first local minimum (among all local minima) whose growth terminates at $s$, then $m$ and $s$ form a persistence pair, with a persistence $p =|f'(s)-f'(m)|$. 
If $p < \varepsilon$, then the edge $ms$ does not belong to $T'_\varepsilon$; otherwise, $ms$ belongs to $T'_\varepsilon$.  

\underline{Case (II).} If $s$ is already paired, then $m$ must pair with some other saddle $s'$, and $s'$ must be an ancestor of $s$ in the join tree. A paired $s$ means that $s$ has been discovered earlier during the growth of another local minimum $m'$ such that $m'$ and $s$ form a persistence pair with persistence $p'$, and the edge $m's$ belongs to $T'$. 

\underline{Case (II.a).} 
Suppose that $p' \geq \varepsilon$. Since $m'$ preceds $m$ in the sorted order, $f'(m') > f'(m)$. Since $s'$ is an ancestor of $s$, $f'(s') > f'(s)$. Therefore $|f'(s') - f'(m)| > |f'(s) - f'(m')| = p' \geq \varepsilon$. 
Thus, the pair $(m,s')$ has a persistence above $\varepsilon$, and $ms$ must be an edge in $T'_\varepsilon$.

\underline{Case (II.b).} 
Now suppose that $p' < \varepsilon$. In this case, we do not have enough information to determine the persistence of $(m,s')$. Therefore, we grow from saddle $s$ to reach a new saddle $s''$. We then check cases (I) and (II) again, using $s''$ in place of $s$. 

Once we are done checking cases (I) and (II), if $m \notin T'_{\varepsilon}$ but $m \in T_{\varepsilon}$, then $m$ is a false negative. 
Likewise, if $ms \in T'_{\varepsilon}$ but $ms \notin T_{\varepsilon}$, then $ms$ is a false positive. 

Growing the global minimum will never produce a false case as long as the rest of $T'_\varepsilon$ is correctly predicted. Thus, we skip the growth at the global minimum, denoted as $\hat{m}$. 
Because $\hat{m}$ is the last growth that remains active, its growth will form the \textit{trunk}, a monotone sequence of edges to the root that links $\hat{m}$ to the remaining saddles~\cite{gueunet2017task}. Since $\hat{m}$ and the remaining saddles are already correctly predicted, so is the trunk, therefore no further false cases are possible, and we skip growing $\hat{m}$. 
This algorithm also admits a number of special cases; see~\cref{sec:algorithm-details}.

\para{Progressive false case correction.} 
If there is a false case, we first tighten the upper and lower bounds of points in some region $R$ to correct it. If $ms$ is a false positive, then $R$ is the region of the merge-tree-induced segmentation of $f'$ corresponding to $ms$. If $m$ is a false negative, and edge $m\hat{s}$ belongs to $T_\varepsilon$ (for some saddle $\hat{s}$), then $R$ is the region of the merge-tree-induced segmentation of $f$ corresponding to $m\hat{s}$. If the same false case occurs multiple times, we grow the region $R$. We tighten the upper and lower bounds of each $x \in R$ similarly to TopoSZ, but we tighten more aggressively to speed up compression. 
We then update the decompressed data $f'$ to respect the new bounds; see~\cref{sec:algorithm-details} for numerical specifics and a comparison with TopoSZ.

Once we update $f'$, these updates may affect parts of the join and split trees beyond the false cases, thus we must recompute those areas to ensure correctness. Specifically, we must check for any extrema bordering $R$ that may have appeared or disappeared as a result of the tightening process and update the trees accordingly. Let $E$ be the set of edges whose segmentation regions border $R$. Then the tightening also may have affected each edge $e \in E$ and every ancestor of $e$ (i.e.,~edges
connecting $e$ to the root of the tree). We recompute all such edges to ensure correctness. As before, we recompute parts of the tree in order of the function values. 

