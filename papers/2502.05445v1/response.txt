\section{Related Work}
\subsection{Implicit Neural Representation for CT}
Consider a 2D CT scanner, the image intensity of scanned object can be formulated as a function of spatial coordinates, defined by
\begin{equation}
    f: \mathbf{p} = (x,y) \in \mathbb{R}^2 \longrightarrow \mu(\mathbf{p}) \in \mathbb{R},
\label{eq:inr_ct}
\end{equation}
where $\mathbf{p}$ is an arbitrary coordinate and $\mu(\mathbf{p})$ is the corresponding intensity of the object. However, the function $f$ is highly complex, making an analytical solution intractable. Previous works **Ulyanov et al., "Deep Image Prior"**__**Soltani et al., "Learning to Denoise"** have leveraged the consistency bias inherent in INR by employing an MLP network $\mathcal{F}_{\mathbf{\Phi}}$ to approximate the function $f$.
The MLP network can learn the implicit function by incorporating the forward model (\textit{e.g.} Radon transform) to minimize the discrepancy between the predicted and acquired measurement data:
\begin{equation}
    \mathbf{\Phi}^* = \underset{\mathbf{\Phi}}{\arg\min} \mathcal{L}(\mathbf{A}\mathcal{F}_\mathbf{\Phi}, \mathbf{y}),
\end{equation}
where $\mathbf{A}$ is the forward model and $\mathbf{y}$ is the acquired measurement data.
By adopting the novel coordinate encoding strategy **Bhatnagar et al., "Deep Learning for Image Reconstruction"** , these INR-based methods can effectively capture high-frequency signals, providing high-quality reconstructions while maintaining data consistency. However, when the measurement data is undersampled or noisy, these methods tend to generate high-frequency artifacts, resulting in performance degradation.
\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{Fig/Pipeline.pdf} 
\caption{\textbf{Overview of Spener model}, including (a) iterative reconstruction using an image embedding neural network $\mathcal{F}_\Phi$, (b) architecture of the image embedding neural network $\mathcal{F}_\Phi$, (c) solving the data fidelity subproblem via the image embedding neural network $\mathcal{F}_\Phi$, and (d) solving regularization subproblem via a denoiser $\mathcal{D}_\sigma$.}
\label{fig:pipeline}
\end{figure*}
% 自监督的方式

\subsection{Plug-and-Play Half-Quadratic Splitting}
The classical approach for solving the ill-posed problem in Eq.~(\ref{eq:forward}) is by formulating an optimization problem:
\begin{equation}
    \hat{\mathbf{x}} = \underset{\mathbf{x}}{\arg \min}\frac{1}{2} \| \mathbf{y} -\mathbf{A} \mathbf{x}\|_2^2 + \lambda \mathcal{R}(\mathbf{x}),
\label{eq1}
\end{equation}
where the objective function is composed of a data fidelity term $ \| \mathbf{y} -\mathbf{A} \mathbf{x}\|_2^2/2$ and a regularization term $\lambda \mathcal{R}(\mathbf{x})$. 
The data fidelity term ensures that the solution conforms to the forward process, maintaining consistency with the measurement data, while the regularization enforces prior knowledge, narrowing the solution space. 

Plug-and-play half-quadratic splitting (PnP-HQS) is a widely used iterative framework for solving inverse problems **Wang et al., "Plug-and-Play Priors"**. 
The conventional HQS **Vonesch and Unser, "A Blind Deconvolutional Algorithm for Images"** introduces an auxiliary variable $\mathbf{z}$ to reformulate the original optimization problem Eq.~(\ref{eq1}) in a constrained manner:
\begin{equation}
    \hat{\mathbf{x}} = \underset{\mathbf{x}}{\arg\min}\frac{1}{2}\|\mathbf{y} - \mathbf{A}{\mathbf{x}} \|^2 + \lambda\mathcal{R}(\mathbf{z})\quad \textit{s.t.}\quad \mathbf{z} = \mathbf{x}.
\end{equation}
HQS solves the objective function by decoupling a data fidelity term and a regularization term, and alternatively solving them in iterative ways:
\begin{subequations}
    \label{eq:m1.hqs}
    \begin{numcases}{}
        \mathbf{x}_{t}=\underset{\mathbf{x}}{\mathrm{arg} \min}\left\| \mathbf{y}-\mathbf{A}\mathbf{x} \right\| ^2+\frac{\mu}{2} \left\| \mathbf{x}-\mathbf{z}_{t-1} \right\| ^2\label{eq:m1.data_sub}\\
        \mathbf{z}_{t}=\underset{\mathbf{z}}{\mathrm{arg} \min}\frac{1}{2(\sqrt{\lambda /\mu})^2}\left\| \mathbf{z}-\mathbf{x}_{t} \right\| ^2+\mathcal{R} \left( \mathbf{z} \right)\label{eq:m1.prior_sub}.
    \end{numcases}
\end{subequations}
The PnP-HQS proposes utilizing a general denoising algorithm to provide the solution of Eq~(\ref{eq:m1.prior_sub}), given by
\begin{equation}
    \mathbf{z}_t = \mathcal{D}_{\sigma}(\mathbf{x}_{t}),
    \label{eq:pnp_reg}
\end{equation}
where $\mathcal{D}_{\sigma}$ represents the denoising operation with a noise level parameter 
$\sigma$.

In Spener, we introduce an INR-based approach to solve the data fidelity sub-problem.
Subsequently, we apply an effective denoiser to regularize the reconstruction obtained from the INR. By iteratively solving these two sub-problems, we achieve a high-fidelity reconstruction.