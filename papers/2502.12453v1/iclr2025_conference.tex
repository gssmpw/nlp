
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\iclrfinalcopy 
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}
\usepackage{multirow} 
\usepackage{tabularx}

\usepackage{makecell}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{graphicx} 
\usepackage{bm}
\usepackage[T1]{fontenc}
\usepackage{booktabs} 


\usepackage{colortbl}
\usepackage{xcolor}
\definecolor{skyblue}{RGB}{224, 255, 255}
\definecolor{citeblue}{RGB}{54, 158, 252}
\usepackage{ulem} 

\usepackage{xspace}
\usepackage{enumitem}


\newcommand{\method}{UniMatch\xspace}
% \newcommand{\method}{UniMatch\xspace}

\usepackage[colorlinks=true, citecolor=citeblue]{hyperref}


% \title{Formatting Instructions for ICLR 2025 \\ Conference Submissions}
% \title{HierMatch: Boosting Few-Shot Learning with Molecular Hierarchical Matching}
% \title{Hierarchical Matching from Atoms to Tasks: Boosting Few-Shot Learning in Drug Discovery}
\title{UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{Ruifeng Li\textsuperscript{\rm 1,2}, Mingqian Li\textsuperscript{\rm 2}, Wei Liu\textsuperscript{\rm 3}, Yuhua Zhou\textsuperscript{\rm 1,2}, Xiangxin Zhou\textsuperscript{\rm 4}, \\
\textbf{Yuan Yao\textsuperscript{\rm 1,5}, Qiang Zhang\textsuperscript{\rm 1,5}$^\dagger$, Hongyang Chen\textsuperscript{\rm 2}$^\dagger$}\\
\textsuperscript{\rm 1} {\small Zhejiang University}
    \textsuperscript{\rm 2} {\small Zhejiang Laboratory}
    \textsuperscript{\rm 3} {\small Shanghai Jiao Tong University}\\
    \textsuperscript{\rm 4} 
    {\small University of Chinese Academy of Sciences}\\
    \textsuperscript{\rm 5} {\small ZJU-Hangzhou Global Scientific and Technological Innovation Center}
    \\
    % {\tt \small \{zonglin001,cambria\}@ntu.edu.sg, liuwanhao@mail.ustc.edu.cn} \\
    % {\tt \small gaoben@whu.edu.cn, tong.xie@unsw.edu.au, sporia@sutd.edu.sg} \\
    % {\tt \small \{liyuqiang, ouyangwanli, zhoudongzhan\}@pjlab.org.cn}
    {\tt \small 
    \{lirf, qiang.zhang.cs\}@zju.edu.cn, 
    dr.h.chen@ieee.org}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% \input{00_abstract}
% Drug discovery is essential for identifying candidate drugs for various diseases. However, its low success rate often results in scarce annotations, leading to the few-shot learning problem.
% Existing approaches primarily focus on single-scale structural features, which may overlook crucial hierarchical information within molecular structures. 
% Considering that different-scale structural information determine different molecular properties,
% we introduce \underline{\textbf{Hier}}archical \underline{\textbf{Match}}ing Networks (\textbf{HierMatch}). This approach utilizes hierarchical pooling and matching techniques to enhance the representation and comparison of molecular features across multiple scales. 
% Specifically, hierarchical pooling aggregates node features from local neighborhoods to global structures, preserving essential details that single-scale methods may overlook. Hierarchical matching then performs pairwise comparisons at these different structural levels, allowing more precise assessments of molecular similarities.
% % To further enhance our model, we incorporate a meta-learning strategy that independently updates general and task-specific parameters, improving feature alignment and matching across hierarchical levels.
% \textcolor{blue}{Additionally, we incorporate a meta-learning strategy for implicit task-level matching, enabling the model to recognize and learn task-specific information, dynamically adjust parameters to handle the complexity and differences in new tasks.} 
% Our \textcolor{blue}{approach} adaptively captures subtle structural similarities and distinctions, improving both predictive accuracy and generalization. 
% Experimental results show that HierMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, with improvements of 2.87\% in AUROC and 6.52\% in $\Delta$AUPRC. Additionally, HierMatch demonstrates excellent generalization performance on the Meta-MolNet benchmark.

% Drug discovery is critical for identifying candidate drugs for various diseases, but its low success rate often leads to a scarcity of annotations, resulting in a few-shot learning challenge. Existing methods primarily focus on single-scale structural features, overlooking important hierarchical information within molecular structures. 
% In this paper, we introduce \textbf{\underline{Hier}archical \underline{Match}ing Networks (HierMatch)}, which performs matching across multiple levels, from atoms to tasks. 
% This approach captures structural features at the atom-level, substructure-level, and molecule-level via hierarchical pooling and matching techniques, enhancing the representation and comparison of molecular features at different scales. 
% Specifically, hierarchical pooling aggregates node features from local neighborhoods to global structures, preserving essential details that single-scale methods may miss. Hierarchical matching then conducts pairwise comparisons at these structural levels, improving the precision of molecular similarity assessments.
% Additionally, we incorporate a meta-learning strategy for implicit task-level matching, allowing the model to adapt to different tasks by learning task-specific information.
% This hierarchical framework, spanning from atomic to task-level matching, enables the model to dynamically adjust parameters to handle the complexity and differences across tasks, significantly improving both predictive accuracy and generalization. 
% Our experimental results show that HierMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87\% in AUROC and 6.52\% in $\Delta$AUPRC. HierMatch also demonstrates excellent generalization performance on the Meta-MolNet benchmark.


% Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, leading to the few-shot learning problem.
% Existing methods primarily focus on single-scale features, overlooking important hierarchical molecular structures. 
% Given that structures at different scales determine different molecular properties,
% we introduce \underline{\textbf{Uni}}versal \underline{\textbf{Match}}ing Networks (\textbf{UniMatch}), which performs multi-level matching from atoms to tasks. 
% Our approach explicitly captures structural features at multiple levels—atoms, substrctures, and molecules—using hierarchical pooling and matching, allowing for precise representation and comparison of molecular structures. 
% Hierarchical pooling preserves important details by aggregating features from local to global structures, while hierarchical matching refines molecular similarity assessments via attention-based comparisons at these structural levels.
% Furthurmore, we incorporate a meta-learning strategy for implicit task-level matching, enabling the model to adapt to new tasks and handle both structural complexity and task-specific variations, significantly boosting predictive accuracy and generalization.
% Our experimental results show that \method outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87\% in AUROC and 6.52\% in $\Delta$AUPRC. \method also \textcolor{red}{demonstrates} excellent generalization ability on the Meta-MolNet benchmark.


Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem.
Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties.
% Given that structures at different scales determine different molecular properties,
To address these issues, we introduce \underline{\textbf{Uni}}versal \underline{\textbf{Match}}ing Networks (\textbf{UniMatch}), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization.
Specifically, our approach explicitly captures structural features across multiple levels—atoms, substructures, and molecules—via hierarchical pooling and matching, facilitating precise molecular representation and comparison.
Additionally, we employ a meta-learning strategy for implicit task-level matching, 
allowing the model to capture shared patterns across tasks and quickly adapt to new ones.
This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation.
Our experimental results demonstrate that \method outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87\% in AUROC and 6.52\% in $\Delta$AUPRC. \method also shows excellent generalization ability on the Meta-MolNet benchmark. The code is available at \url{https://github.com/Lirain21/UniMatch.git}
\end{abstract}

\section{Introduction}\label{intorduction}
% \input{01_intro}
Drug discovery is pivotal for human health, involving the screening and optimization of numerous compounds to identify potential drug candidates that satisfy both pharmacological efficacy and toxicological safety criteria 
 \citep{drews2000drug, renaud2016biophysics, atanasov2021natural}.
 The traditional drug development cycle typically spans over a decade, incurs costs exceeding \$1 billion, yet achieves a success rate of less than 10\%
\citep{sliwoski2014computational, adelusi2022molecular}.
Artificial Intelligence-Driven Drug Discovery (AIDD) has emerged as a promising solution to address this challenge \citep{mak2023artificial, macalino2015role, gawehn2016deep}. 
Within AIDD, Quantitative Structure-Activity/Property Relationships (QSAR/QSPR) \citep{cherkasov2014qsar, liu2009current} models are crucial for predicting the relationships between molecular structures and their activities. 
These methods rely heavily on extensive datasets due to the complexity of understanding and modeling molecular geometries \citep{zhang2021motif, fabian2020molecular, wang2021propertyaware, chen2023metalearning}.
However, the lengthy durations, high costs, and low success rates of chemical wet experiments limit the availability of labeled experimental data. 
% resulting in a scarcity of labeled data.


Few-shot learning \citep{li2023deep, song2023comprehensive} has demonstrated substantial potential in addressing data scarcity by enabling models to generalize rapidly from minimal data to new tasks.  
Recently, several approaches have been proposed to address this challenge in few-shot scenarios \citep{wang2020generalizing}.
 Most approaches are based on molecular graphs with atoms as nodes and chemical bonds as edges,  leveraging Graph Neural Networks (GNNs) \citep{zhou2020graph} to capture molecular topologies.
In particular, such models as IterRefLSTM \citep{altae2017low}, Meta-MGNN \citep{guo2021few}, PAR \citep{wang2021propertyaware}, ADKF-IFT \citep{chen2023metalearning}, and Meta-GAT \citep{lv2024meta} employ GNNs as encoders to learn molecular representations for label prediction.
Additionally, several sequence-based methods, such as CHEF \citep{adler2020cross}, MHNfs \citep{schimunek2023contextenriched} and CRA \citep{li2024contextual}, utilize Multilayer Perceptrons (MLPs) as encoders to compress molecular fingerprints or descriptors for predictive modeling.


\begin{figure*}
\centering\includegraphics[width=0.95\textwidth]{introduction.pdf}
    \caption{
    Different levels of molecular structures affect different properties:
   (a) at the atomic level, fluorine and nitrogen affect molecular acidity and basicity, respectively; (b) at the substructural level, hydroxyl groups affect the hydrophobicity of ethanol and dodecane; and (c) at the molecular level, the overall structures influence boiling points. Key molecular structures are highlighted in red.}
    \label{fig:introduction}
    % \vspace{-10pt}
\end{figure*}


However, existing approaches often overlook a crucial aspect: \textbf{different levels of structural information---ranging from atoms to substructures to the entire molecule---contribute to distinct molecular properties}.
Some properties are influenced by atomic composition, while others depend on substructures or the overall molecular configuration. 
Figure~\ref{fig:introduction} illustrates this with examples: (a) fluorine and nitrogen affect molecular acidity and basicity, respectively; (b) hydroxyl groups influence the hydrophobic properties of ethanol and dodecane; and (c) the overall molecular structure affects boiling points.
In graph-based methods, 
% like Meta-MGNN \citep{guo2021few}, PAR \citep{wang2021propertyaware}, ADKF-IFT \citep{chen2023metalearning}, and Meta-GAT \citep{lv2024meta}, 
the use of multiple GNN layers may cause over-smoothing, where the receptive fields of nodes expand excessively,
% to cover the entire molecular graph, 
thus obscuring substructural details \citep{chen2020measuring}. As a result, GNNs are more suitable for predicting properties related to the overall structure of molecules. 
In contrast, fingerprint-based methods
% like CHEF and MHNfs 
rely on only fragmented local features, potentially overlooking critical information about the overall molecular structure. Although CHEF \citep{adler2020cross} introduces a representation fusion strategy, its reliance on ECFP6 \citep{rogers2010extended}---which is based on fixed local features---limits its ability to capture hierarchical molecular structures.
 Therefore, effectively capturing different levels of molecular structures is crucial for accurately predicting a wide range of molecular properties.

To address this challenge, we propose \textbf{Universal} \textbf{Matching Networks}  (\textbf{\method}), 
a framework that facilitates universal matching across multiple levels—from atoms to tasks—enhancing the few-shot molecular property prediction task. Our main contributions are summarized as follows:
\begin{itemize}
    \item 
    To the best of our knowledge, we are pioneers to introduce a universal matching approach that spans from the atomic level to the task level. This framework employs explicit hierarchical molecular matching and implicit task-level matching at distinct levels to align molecular structures with tasks. The dual matching mechanism complements itself, forming a synergistic framework that enhances the model’s adaptability and generalization across various tasks 
    (Section~\ref{method}).
    \item 
   We propose an explicit hierarchical molecular matching mechanism that integrates information from atoms to higher-level structures, capturing complex molecular features. By utilizing an attention-based matching module, the model aligns representations across multiple levels, selecting the most relevant features for improved prediction (Sections~\ref{encoding_module}).
    \item 
    We incorporate a meta-learning strategy to achieve implicit task-level matching, learning shared parameters that generalize across tasks. This matching occurs at an abstract level, capturing task similarities through optimization and enabling rapid adaptation and improved generalization
    (Section~\ref{subsction: training_and_inference}).
    \item
    Our \method outperforms state-of-the-art methods on both the MoleculeNet (Section~\ref{subsetion:moleculenet}) and FS-Mol (Section~\ref{subsection:fs-mol}) benchmarks, achieving improvements of 2.87\% in AUROC and 6.52\% in $\Delta$AUPRC, respectively. Additionally, we evaluate the generalization ability of \method on the Meta-Mol benchmark, where it demonstrates outstanding performance (Section~\ref{subsection:meta-molnet}).
\end{itemize}


\section{Background}
\subsection{Problem Definition}
\label{problem}
The few-shot molecular property prediction problem, as defined by ADKF-IFT \citep{chen2023metalearning} and MHNfs \citep{schimunek2023contextenriched}, involves training models on a set of tasks $\left\{\mathcal{T}_{\tau}\right\}_{\tau=1}^{N_{t}}$ sampled from the training set $\mathcal{D}_{train}$ to improve generalization to new tasks.
Each task $\mathcal{T}_{\tau}$ includes a support set $\mathcal{S}_{\tau}=\left\{\left(\mathbf{x}_{\tau, i}, y_{\tau, i}\right)\right\}_{i=1}^{N_{\tau}^{s}}$ and a query set  $\mathcal{Q}_{\tau}=\left\{\left(\mathbf{x}_{\tau, j}, y_{\tau, j}\right)\right\}_{j=1}^{N_{\tau}^{q}}$, where  $\mathbf{x}_{\tau, i} \in \mathbb{R}^{d}$ and $\mathbf{x}_{\tau, j} \in \mathbb{R}^{d}$ represent molecular features, and $y_{\tau, i}$,  $y_{\tau, j} \in \{0, 1\}$ indicate the molecular properties or activities. 
The support set $\mathcal{S}_{\tau}$ provides a few labeled examples for task-specific adaptation, while the query set $\mathcal{Q}_{\tau}$ is utilized to evaluate the model's performance on unseen examples.


\subsection{Preliminaries}
\label{preliminaries}
Graph neural networks (GNNs) are designed to handle graph-structured data (non-Euclidean data) by aggregating information from neighboring nodes to learn effective representations \citep{zhou2020graph}. Models such as GCN \citep{kipf2017semisupervised}, GIN \citep{xu2018how},  and GAT \citep{velivckovic2018graph} are widely used for graph classification and other related applications. In a graph $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$, $\mathcal{V}$ represents the set of nodes, and $\mathcal{E}$  the set of edges. $\mathbf{h}_{v}^{(0)}$ 
represents the initial features of node $v$, and $\mathbf{b}_{u,v}$ denotes the features of the edge $e_{u,v}$ between nodes $u$ and $v$. At the $l^{\text{th}}$ layer, the representation $\mathbf{h}_{v}^{(l)}$ of node $v$ is updated in GNNs as follows: 
\begin{equation}
\label{node_update}
\mathbf{h}_v^{(l)}=\operatorname{UPDATE}^{(l)}\left(\mathbf{h}_v^{(l-1)}, \operatorname{AGGREGATE}^{(l)}\left(\left\{\left(\mathbf{h}_v^{(l-1)}, \mathbf{h}_u^{(l-1)}, \mathbf{b}_{v, u}\right) \mid u \in \mathcal{N}(v)\right\}\right)\right)\text{,} % \nonumber
\end{equation}
where $\mathcal{N}(v)$ is the set of neighboring nodes of $v$. The $\operatorname{AGGREGATE}$ function combines features from neighboring nodes, and the $\operatorname{UPDATE}$ function updates the node features for the next layer.

% ============

\section{Method}\label{method}

This section presents the model architecture (Section 4.1) and the meta-learning strategy (Section 4.2) of Universal Matching Networks, which respectively achieve explicit hierarchical molecular matching and implicit task-level matching. Figure 2 illustrates an overview of \method framework.


\begin{figure*} % [htb]
\centering\includegraphics[width=1.0\textwidth]{HieMatching_method.pdf}
    \caption{The overview of \method. \textbf{Left:} Our model follows a hierarchical pooling-matching architecture comprising two components: an encoding module (including pooling) and a matching module. 
    First, mean pooling is applied at each GNN layer to generate multi-level molecular representations. Then, an attention mechanism is utilized to align representations between the support set and query set across different levels. Finally, predictions from different GNN layers are integrated to obtain the final results.
    \textbf{Right:} The detailed process of the matching module.}
    \label{fig:method_pipeline}
    % \vspace{-7pt}
\end{figure*}


\subsection{Architecture: Explicit hierarchical molecular matching}
\label{model_architecture}

We propose a novel architecture for explicit hierarchical molecular matching, which captures and aligns complex molecular structures across multiple levels (atomic, substructural, and molecular) via hierarchical pooling and matching, enabling fine-grained comparison and similarity assessment from local to global scales. It establishes the foundation for implicit task-level matching.

\paragraph{Encoding Module.}
\label{encoding_module}

Following the mainstream graph-based few-shot molecular property prediction approaches \citep{wang2021propertyaware, guo2021few, lv2024meta, chen2023metalearning}, we adopt the widely used GIN \citep{xu2018how} as the backbone of our method.
In GNNs, each layer aggregates local information from nodes and their neighboring hops. As the network depth increases, the model incrementally aggregates hierarchical information, progressing from individual nodes to substructures and ultimately capturing the entire molecule.


To capture molecular representations at different levels, we employ mean pooling to aggregate node representations at each layer of the GNN. For a given task $\tau$, we first obtain the node representations $\mathbf{h}_{\tau,s}^{(l)}\footnote{the subscript s represents it belongs to support set.}  \in \mathbb{R}^{n_s \times d}$ for the support set $\mathcal{S}_{\tau}$ and $\mathbf{h}_{\tau,q}^{(l)}\footnote{the subscript q represents it belongs to query set.} \in \mathbb{R}^{n_q\times d}$ for the query set $\mathcal{Q}_{\tau}$. Then, we utilize mean pooling to derive the molecular representations $\mathbf{z}_{\tau,s}^{(l)} \in \mathbb{R}^{N_{\tau}^{s}\times d}$ for the support set $\mathcal{S}_{\tau}$ and $\mathbf{z}_{\tau,q}^{(l)}  \in \mathbb{R}^{N_{\tau}^{q}\times d}$ for the query set $\mathcal{Q}_{\tau}$, as follows: % \lw{in the equation, V should be different. }
\begin{equation}
    \mathbf{z}_{\tau,s}^{(l)} = \operatorname{Pooling}(\mathbf{h}_{\tau,s,v}^{(l)}, v \in \mathcal{V}_{\tau, s})\text{,}\: \mathbf{z}_{\tau,q}^{(l)} = \operatorname{Pooling}(\mathbf{h}_{\tau,q,v}^{(l)}\, v \in \mathcal{V}_{\tau, q})\text{,}
    \label{eq:graph_representation}
\end{equation}
where the $\operatorname{Pooling}$ function denotes mean pooling, and $l$ refers to the $l^\text{th}$ layer of the GNN. 

\paragraph{Matching Module.}
\label{matching_module}
% In few-shot learning tasks, match learning is widely used to predict results by evaluating the similarity between a query sample and samples in the support set \citep{vinyals2016matching, wang2020graph}. Given that the size of training data in few-shot learning tasks is extremely limited, using feature similarity measurement can effectively utilize support set knowledge, thereby effectively distinguishing samples from different categories. 

% In this paper, hierarchical matching focuses on specific neighborhood information of molecular structures at each layer of GNN, deliberately ignoring other features. This approach allows for more accurate identification of similarities within specific hierarchical structures of molecules, resulting in efficient matching. 
% In few-shot learning tasks, matching learning evaluates the similarity between a query sample and  samples in the support set \citep{vinyals2016matching, wang2020graph}, effectively utilizing sample similarities to achieve accurate predictions.  We explore hierarchical matching in GNNs, emphasizing the importance of neighborhood information in molecular structures with different receptive fields at each layer while disregarding other features. By concentrating on molecular structures at specific layers, this method enhances the precision of identifying similarities among molecules, consequently improving matching efficiency. 

% In few-shot learning, matching learning evaluates the similarity between a query sample and those in the support set \citep{vinyals2016matching, wang2020graph}. 

% \textcolor{blue}{We implement hierarchical matching in GNNs, focusing on molecular structures at different layers to improve the accuracy of similarity identification among molecules.
% To measure similarity, we adopt the attention mechanism proposed by \citet{vaswani2017attention}.} 

Hierarchical matching plays a critical role in our approach, as it allows the model to capture structural details at different levels—from atoms to entire molecules—ensuring a more accurate and fine-grained similarity identification. 
To achieve this, we adopt the attention mechanism introduced by \citet{vaswani2017attention}, which dynamically weighs the contributions of molecular features at multiple scales.
Specifically, we designate the molecular representations $\mathbf{z}_{\tau,s}^{(l)} \in \mathbb{R}^{N_{\tau}^{s}\times d}$ in the support set $\mathcal{S}_{\tau}$ as the key, and the molecular representations $\mathbf{z}_{\tau,q}^{(l)} \in \mathbb{R}^{N_{\tau}^{q}\times d}$ in the query set $\mathcal{Q}_{\tau}$ as the query. The corresponding ground-truth labels $\mathbf{y}_{\tau,s} \in \mathbb{R}^{N_{\tau}^{s}\times 1}$  
in the support set serve as the value. This attention-based approach allows for precise matching at the specified level: 
\begin{equation}
    \mathbf{\hat{y}}_{\tau, q}^{(l)} = \operatorname{Softmax}(\dfrac{(\mathbf{z}_{\tau,q}^{(l)}\mathbf{W}_{q})(\mathbf{z}_{\tau,s}^{(l)}\mathbf{W}_{k})^\top}{\sqrt{d}})\mathbf{y}_{\tau,s}\text{,}
    \label{eq:each_layer_y}
\end{equation}
where $d$ is the dimension of molecular representations, and $\mathbf{W}_{q}$,$\mathbf{W}_{k}$ $\in \mathbb{R}^{d \times d}$.  


\paragraph{Fusion.} 
We repeat the steps above to obtain the matching results $\mathbf{\hat{y}}_{\tau, q}^{(l)}$ for each GNN layer. 
These results are concatenated into a joint representation, which is then passed through a linear layer to generate the final prediction $\mathbf{\hat{y}}_{\tau, q} \in \mathbb{R}^{N_{\tau}^{q}\times 2}$:
\begin{equation}
    \mathbf{\hat{y}}_{\tau, q} = \operatorname{Linear_{\mathbf{W}_{o}}(\operatorname{Concat(
    \mathbf{\hat{y}}_{\tau,q}^{(1)},\mathbf{\hat{y}}_{\tau,q}^{(2)}, \cdots,\mathbf{\hat{y}}_{\tau,q}^{(L)}
    )})}\text{,}
    \label{eq:total_y}
\end{equation}
where L represents the total number of layers in the GNN, and $\mathbf{W}_{o} \in \mathbb{R}^{L \times 2}$ are the parameters of the $\operatorname{Linear}$ function.  
For classification tasks, $\mathbf{\hat{y}}_{\tau, q}$ is normalized into a probability distribution using Softmax.
By fusing multi-level features, the model captures the complex relationships within molecular structures, resulting in more robust and comprehensive predictions.

\subsection{% \textcolor{blue}{Implicit Task-level Matching: Meta-Learning} 
Meta-Learning}
\label{subsction: training_and_inference}
We employ a meta-learning  strategy to facilitate implicit task-level matching. To further clarify why meta-learning functions as implicit task-level matching, we introduce a task relationship matrix that captures task similarities at a higher level of abstraction. This matrix allows for efficient adaptation and enhances generalization through optimized learning. 

\subsubsection{% \textcolor{blue}{Meta-Learning: Training and Inference} 
Training and Inference}

For simplicity, we define \method as $f_{\bm{\theta},\mathbf{w}}$, where $\bm{\theta}$ denotes the parameters of the molecular encoder, and $\mathbf{w} = \{\mathbf{W}_{q}, \mathbf{W}_{k}, \mathbf{W}_{o}\}$ represents the parameters of the matching and fusion modules. 

\paragraph{Training Phase.} 
During the training phase, we employ a standard meta-learning process to improve the model's generalization ability using the training set $\mathcal{D}_\text{train}$, as illustrated in Algorithm~\ref{alg:meta_training}. Parameter learning involves a combination of inner and outer optimization. To facilitate dual optimization,  we split the support set $\mathcal{S}_{\text{train}, \tau}$ for each task $\mathcal{T}_{\text{train}, \tau}$ into $\mathcal{S}_{\tau}^{\prime}$ and $\mathcal{Q}_{\tau}^{\prime}$. The training loss $\mathcal{L}\left(\mathcal{Q}_\tau^\prime, f_{\boldsymbol{\theta}, \mathbf{w}}\right)$ evaluated on $\mathcal{Q}_\tau^\prime$ is defined as: 
\begin{equation}\label{eq:loss_function}
\mathcal{L}\left(\mathcal{Q}_\tau^\prime, f_{\boldsymbol{\theta}, \mathbf{w}}\right)=\sum_{\left(\mathbf{x}_{\tau, i}, y_{\tau, i}\right) \in \mathcal{Q}_\tau^\prime}-\mathbf{y}_{\tau, i}^{\top} \cdot \log \left(\hat{\mathbf{y}}_{\tau, i}\right) \text{,}
\end{equation}
where $\mathbf{y}_{\tau, i} \in \mathbb{R}^2$ is a one-hot vector representing the class of the sample, where a positive sample is denoted by $[1,0]$ and a negative sample is denoted by $[0,1]$.  

\setlength{\topsep}{-2pt}
\setlength{\partopsep}{-2pt}
\setlength{\leftmargini}{1em} 
\begin{itemize}
\item\textbf{Inner Loop.}
During the inner optimization, task-specific parameters $\mathbf{w}_{\tau}$ are updated for each task $\mathcal{T}_{\text{train},\tau}$, enabling the model to adapt quickly to the current task, as follows:
\begin{equation}\label{eq:inner_loop} 
\mathbf{w}_\tau=\mathbf{w}-\alpha \nabla_{\mathbf{w}} \mathcal{L}\left(\mathcal{Q}_\tau^\prime, f_{\boldsymbol{\theta}, \mathbf{w}}\right)\text{,}
\end{equation}
where $\boldsymbol{\theta}$ denotes the fixed parameters in the inner optimization and $\alpha$ is the inner learning rate. 
    
\item\textbf{Outer Loop.}
The outer optimization aims to update the meta-parameter  ($\bm{\theta}$ and $\mathbf{w}$) to improve generalization across all tasks. During training, this is achieved by minimizing the aggregated loss over all tasks:
\begin{equation}\label{eq:outer_loop}
\bm\theta^*, \mathbf{w}^*=\arg \min _{\boldsymbol{\theta}, \mathbf{w}} \sum_{\tau=1}^{N_t} \mathcal{L}\left(\mathcal{Q}_{\text{train}, \tau}, f_{\bm\theta, \mathbf{w}_\tau}\right)\text{,}
\end{equation}
where $N_t$ is the total number of training tasks. 
\end{itemize}


\paragraph{Inference Phase.}
After training, we evaluate the model on a set of test tasks $\mathcal{T}_\text{test}$ drawn from the test set  $\mathcal{D}_{\text{test}}$. 
% To capture task-specific knowledge for each test task $\mathcal{T}_{\text{test}, \tau}$, 
The support set $\mathcal{S}_{\text{test}, \tau}$ is split into $\mathcal{S}_{\tau}^{\prime\prime}$ and $\mathcal{Q}_{\tau}^{\prime\prime}$.
With $\bm{\theta}$ fixed, we fine-tune the task-specific parameters  $\mathbf{w}$ using $\mathcal{Q}_{\tau}^{\prime\prime}$ as defined in Eq.~\ref{eq:inner_loop}. After fine-tuning, the model is evaluated on $\mathcal{S}_{\text{test}, \tau}$, which serves as the support set to predict the labels for the unknown query molecules.


\subsubsection{
% \textcolor{blue}{Meta-Learning Mechanism: Implicit Task-Level Matching} 
Implicit Task-Level Matching}

% \textcolor{blue}{In the \method framework, meta-learning, as a form of implicit task-level matching, enables the model to generalize across tasks without relying on direct feature comparisons. Instead, it learns shared meta-parameters that facilitate rapid adaptation to new tasks, achieving matching at a more abstract, parameter-based level. This indirect, high-level matching not only reduces the computational cost associated with explicit task alignments but also increases scalability, allowing the model to handle diverse and complex task distributions. By focusing on common patterns across tasks, meta-learning enhances the model’s ability to generalize and adapt, making it essential for cross-task learning and task-level adaptation, particularly in data-scarce or highly variable environments.}

% \textcolor{blue}{While explicit molecular matching focuses on aligning molecular structures within individual tasks, implicit task-level matching (via meta-learning) ensures cross-task generalization. Together, these mechanisms allow the model to capture task-specific details (via explicit matching) and enable cross-task adaptation (via implicit matching), creating a more robust framework overall.}

% The core idea of implicit matching is that the model’s meta-parameter $\boldsymbol{\theta}$ gains the ability to generalize across tasks through explicit training, allowing it to rapidly adapt to new tasks without the need to explicitly model matching relationships.

In our UniMatch framework, we consider meta-learning as an implicit task-level matching mechanism. During the training phase, the model autonomously captures and internalizes inter-task relationships and features, embedding them into its meta-parameters. As a result, even without explicitly modeling task relationships, the model can rapidly adapt to new tasks, demonstrating a form of implicit matching.

Implicit matching primarily captures the model's adaptability across multiple tasks through latent relationships, rather than direct parameter updates. To quantify these relationships, we define a task relationship matrix $\mathbf{M} \in \mathbb{R}^{N_{t} \times N
_{t}}$, where each element $\mathbf{M}_{i,j}$ represents the relationship (e.g., similarity or distance) between task $\mathcal{T}_{\text{train},i}$ and task $\mathcal{T}_{\text{train},j}$, which is defined as follows:
\begin{equation}
\mathbf{M}_{i,j}=g_{\boldsymbol{\theta}, \mathbf{w}}\left(\mathcal{T}_{\text{train},i}, \mathcal{T}_{\text{train},j}\right)\text{,}
\end{equation}
where $g_{\boldsymbol{\theta}, \mathbf{w}}$ is a relation function based on the meta-parameter ($\boldsymbol{\theta}$ and $\mathbf{w}$), measuring the relationship between task $\mathcal{T}_{\text{train},i}$ and task $\mathcal{T}_{\text{train},j}$.

\setlength{\leftmargini}{1em} 
\begin{itemize}
    \item \textbf{Inner Loop.} We further extend the concept of implicit matching to directly shape the representation of task-specific adaptation parameters $\mathbf{w}_\tau$. In traditional explicit training, task parameters $\mathbf{w}_\tau$ are determined via gradient updates of the meta-parameter as shown in Eq.~\ref{eq:inner_loop}. By contrast, implicit matching enables a new formulation of these parameters as follows: 
\begin{equation}\label{eq:m_inner_loop}
\mathbf{w}_\tau=\mathbf{w}_\tau+\sum_{j=1}^{N_{t}} \mathbf{M}_{\tau, j} \cdot\left(\mathbf{w}_j-\mathbf{w}_\tau\right)\text{.}
\end{equation}

This approach replaces explicit gradient updates with implicit parameter matching, aligning task-specific parameters to the optimal solution based on inter-task relationships.
    
    \item \textbf{Outer Loop.}  After completing the inner-loop training, the implicit relationship matrix $\mathbf{M}$ is updated. Using the updated $\mathbf{M}$, the general parameter $\boldsymbol{\theta}$ is further refined. The update formula can be rewritten as:
\begin{equation}\label{eq:m_outer_loop}
{\boldsymbol{\theta}}={\boldsymbol{\theta}}+\eta \sum_{i=1}^{N_{t}} \sum_{j=1}^{N_t} \mathbf{M}_{i,j} \cdot\left(\mathbf{w}_j-\mathbf{w}_i\right)\text{,}
\end{equation}
where $\eta$ is the outer learning rate. This formula uses 
$\mathbf{M}$ to guide parameter updates, emphasizing the core role of implicit matching in training phase. Simultaneously, the task-specific parameters $\mathbf{w}$ can be updated using Eq.~\ref{eq:m_inner_loop}.
    
\end{itemize}

Further details on the training and inference phase are provided in Appendix~\ref{ap:training and inference}.








%We adopt a meta-learning strategy \citep{finn2017model} to enhance the generalization of our model, enabling it to quickly learn and adjust across different tasks $\left\{\mathcal{T}_{\tau}\right\}_{\tau=1}^{N_{t}}$. To ensure the model effectively captures task-specific information and exhibits robust generalization, we employ nested parameter updates. This method involves selectively updating specific parameters $\mathbf{w}_{\tau}$ on the support set $\mathcal{S}_{\tau}$ for each task $\mathcal{T}_{\tau}$ while maintaining $\bm{\theta}$ fixed. We adopt the cross entropy (CE) loss as the training loss to evaluate on support set $\mathcal{T}_{\tau}$:
% \begin{equation}
% \label{eq:loss_function}
% \mathcal{L}\left(\mathcal{S}_\tau, f_{\boldsymbol{\theta},\mathbf{w}}\right)=\sum_{\left(\mathbf{x}_{\tau, i}, {y}_{\tau, i}\right) \in \mathcal{S}_\tau}-{y}_{\tau, i}^{\top} \cdot \log \left(\hat{{y}}_{\tau, i}\right),
% \end{equation}
% where $y_{\tau, i} \in \{0, 1\}$ is the ground-truth label. 

%In the outer updating layer, the $\bm{\theta}^{*}$ and $\mathbf{w}^{*}$ are learned on $\mathcal{Q}_{\tau}$ by optimizing the following objective:
% \begin{equation}
% \min _{\boldsymbol{\theta}, \mathbf{w}} \sum_{\tau=1}^{N_t} \mathcal{L}\left(\mathcal{Q}_\tau, f_{\boldsymbol{\theta}, \mathbf{w}_\tau}\right),
% \end{equation}
% where the loss $\mathcal{L}\left(\mathcal{Q}_\tau, f_{\boldsymbol{\theta}, \mathbf{w}_\tau}\right)$ is calculated by the Equation~\ref{eq:loss_function}.


% \input{03_method}

\section{Experiment}\label{experiment}
% \input{04_experiment}
In this section, we evaluate the empirical performance of \method, as outlined in Section~\ref{method}.
We validate our \method on the MoleculeNet (Section~\ref{subsetion:moleculenet}) and FS-Mol (Section~\ref{subsection:fs-mol}) benchmarks. Additionally, we perform an ablation study of \method in Section~\ref{subsection:ablation_study}. To demonstrate the generalization of \method, we further test it on seven datasets from the Meta-MolNet benchmark in Section~\ref{subsection:meta-molnet}, covering both single-task and multi-task scenarios. Finally, we conduct visualization experiments in Section~\ref{subsection:visualization} to demonstrate the importance of the dual matching mechanism in \method. 
% For a more detailed set of experimental results, please refer to Appendix~\ref{ap:further_experiments_results_on_fs_mol}.
Additional experiments and analyses are provided in Appendix~\ref{ap:further_experiments_results_on_fs_mol}.
All experiments are run on an NVIDIA RTX A6000 GPU.

\subsection{Few-shot Molecular Property Prediction on the MoleculeNet Benchmark}
\label{subsetion:moleculenet}

\begin{table}[htb]
  \caption{Comparison of all methods on the MoleculeNet benchmark with a support set size of 20. The mean test performance is reported as AUROC\% along with the standard deviations.
 }
  \label{Table:moleculenet}
  % \centering
  % \begin{tabular*} %{lcccc}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
     Method  & Tox21 (12) $\uparrow$   & SIDER (27) $\uparrow$    & MUV (17) $\uparrow$   & ToxCast (617) $\uparrow$  \\
    \midrule
    CHEF \citep{adler2020cross} & 61.97 $\pm$ 0.65 & 57.34 $\pm$ 0.82  &  53.17 $\pm$ 4.21 & 56.52 $\pm$ 1.24\\
    MixHop \citep{abu2019mixhop} & 78.14 $\pm$ 0.33 & 72.01 $\pm$ 0.87& 78.04 $\pm$ 3.01 &  77.19 $\pm$ 0.93 \\
    Siamese \citep{koch2015siamese}  & 80.40 $\pm$ 0.35  & 71.10 $\pm$ 4.32  & 59.59 $\pm$ 5.13  & - \\
    ProtoNet \citep{snell2017prototypical} & 74.98 $\pm$ 0.32  &  64.54 $\pm$ 0.89  & 65.88 $\pm$ 4.11  & 63.70 $\pm$ 1.26 \\
    MAML \citep{finn2017model}    & 80.21 $\pm$ 0.24   & 70.43 $\pm$ 0.76  & 63.90 $\pm$ 2.28 & 66.79 $\pm$ 0.85  \\
    TPN  \citep{liu2018learning}   & 76.05 $\pm$ 0.24   & 67.84 $\pm$ 0.95  & 65.22 $\pm$ 5.82 & 62.74 $\pm$ 1.45 \\
    EGNN  \citep{kim2019edge}  & 81.21 $\pm$ 0.16   & 72.87 $\pm$ 0.73  & 65.20 $\pm$ 2.08 & 63.65 $\pm$ 1.57 \\ 
    IterRefLSTM \citep{altae2017low} & 81.10 $\pm$ 0.17 & 69.63 $\pm$ 0.31 & 45.56 $\pm$ 5.12  & - \\ 
    PAR \citep{wang2021propertyaware} & 82.06 $\pm$ 0.12   &   \textbf{74.68} $\pm$ \textbf{0.31}   &   66.48 $\pm$ 2.12   & 69.72 $\pm$ 1.63 \\
    ADKF-IFT \citep{chen2023metalearning} & 82.43 $\pm$ 0.60 & 67.72 $\pm$ 1.21  & \textbf{98.18} $\pm$ \textbf{3.05} & 72.07 $\pm$ 0.81 \\ 
    MHNFs \citep{schimunek2023contextenriched} &   80.23 $\pm$ 0.84    &  65.89 $\pm$ 1.17    & 73.81 $\pm$ 2.53  &
    74.91 $\pm$ 0.73 \\
    \rowcolor{skyblue}
    \textbf{\method (Ours)} & \textbf{82.62} $\pm$ \textbf{0.43} & 68.13 $\pm$ 1.54 & 79.40 $\pm$ 3.14 & \textbf{77.74} $\pm$ \textbf{0.75} \\
    \midrule
    Pre-GNN \citep{hu2020strategies}  & 82.14 $\pm$ 0.08  & 73.96 $\pm$ 0.08  & 67.14 $\pm$ 1.58 & 73.68 $\pm$ 0.74 \\ 
    GNN-MAML \citep{guo2021few}  & 82.97 $\pm$ 0.10 & 75.43 $\pm$ 0.21 & 68.99 $\pm$ 1.84 & - \\ 
    Pre-PAR \citep{wang2021propertyaware} & 84.93 $\pm$ 0.11 & 78.08 $\pm$ 0.16 & 69.96 $\pm$ 1.37 & 75.12 $\pm$ 0.84 \\ 
    Pre-ADKF-IFT \citep{chen2023metalearning} & 86.06 $\pm$ 0.35 & 70.95 $\pm$ 0.60 & \textbf{95.74} $\pm$ \textbf{0.37} & 76.22 $\pm$ 0.13 \\ 
    \rowcolor{skyblue}
    \textbf{Pre-\method (Ours)} & \textbf{86.35} $\pm$ \textbf{0.13} & \textbf{80.34} $\pm$ \textbf{0.45} & 86.35 $\pm$ 0.76 & \textbf{81.63} $\pm$ \textbf{0.73} \\
    \bottomrule
  \end{tabular}}
\end{table}

\paragraph{Benchmark and Baselines.}
MoleculeNet \citep{wu2018moleculenet} serves as a benchmark for few-shot molecular property prediction, focusing on small molecules with molecular weights below 900 Daltons. This benchmark includes 4 datasets—Tox21, SIDER, MUV, and ToxCast—which contain 12, 27, 17, and 617 tasks, respectively. % (see Appendix~\ref{ap:moleculeNet_datasets}). 
% Further details of datasets refer to Appendix~\ref{ap:moleculeNet_datasets}.
We compare \textbf{\method} with two types of baselines: 1) Methods trained from scratch, including 
\textbf{CHEF} \citep{adler2020cross}, 
\textbf{MixHop} \citep{abu2019mixhop},
\textbf{Siamese} \citep{koch2015siamese}, \textbf{ProtoNet} \citep{snell2017prototypical}, \textbf{MAML} \citep{ren2018meta}, \textbf{TPN} \citep{liu2018learning}, \textbf{EGNN} \citep{kim2019edge}, \textbf{IterRefLSTM} \citep{altae2017low}, \textbf{PAR} \citep{wang2021propertyaware}, \textbf{MHNfs} \citep{schimunek2023contextenriched}, and \textbf{ADKF-IFT} \citep{chen2023metalearning}; 2) Methods that fine-tune pretrained models, including \textbf{Pre-GNN} \citep{hu2020strategies}, \textbf{GNN-MAML} \citep{guo2021few}, \textbf{Pre-PAR} \citep{wang2021propertyaware}, and \textbf{Pre-ADKF-IFT} \citep{chen2023metalearning}. \textbf{Pre-\method} is \textbf{UniMatch} that utilizes pretrained parameters from the Pre-GNN model \citep{hu2020strategies}. More details  are provided in Appendix~\ref{ap:moleculeNet_benchmark}.


\paragraph{Evaluation Procedure.}
Following the procedural framework of \citet{wang2021propertyaware}, we adopt AUROC (the area under the receiver operating characteristic curve) 
% \textcolor{red}{MQ: (the ... curve)} 
as the evaluation metric and set the support set size at  20 (i.e., 2-way 10-shot). The model is trained using the Adam optimizer \citep{kingma2014adam}. During testing, results are averaged from 10 repeated experiments with different random seeds. For the baselines, we replicate the results for CHEF, MixHop, and MHNfs, while results for the other baselines are cited from \citet{chen2023metalearning}. Further details of the experimental setup can be found in Appendix~\ref{ap:experimental_setup}.

\paragraph{Performance.}
Table~\ref{Table:moleculenet} demonstrates that both \textbf{\method} and \textbf{Pre-\method} outperform existing state-of-the-art methods on the Tox21, SIDER (pre-training stage only), and ToxCast datasets, achieving an average improvement of \textbf{2.87}\%. 
Compared to CHEF, \method demonstrates superior performance, suggesting that graph structures are more effective than fixed fingerprints for hierarchical representation learning in this context.
Additionally, our \method outperforms MixHop, highlighting the importance of hierarchical matching for molecular property prediction, especially in few-shot scenarios.
On the MUV dataset, \method ranks second among all baselines, possibly due to the severe distribution imbalance inherent in the MUV dataset. 
% The balanced training setup used in the experiments (2-way 10-shot) may cause the model to not adapt well to the data.


\subsection{Few-shot Molecular Property Prediction on the FS-Mol Benchmark}
\begin{figure*}
\centering\includegraphics[width=1.0\textwidth]{fs_mol_all_2.pdf}
    \caption{Mean performance with standard errors on the FS-Mol test tasks. (a) Performance of all compared approaches on the FS-Mol benchmark. (b) Ablation study of the dual matching mechanism in \method across different backbones.} %\textcolor{blue}{MQ: rearrange font and layout}}
    \label{fig:fs_mol_ex}
    \vspace{-10pt}
\end{figure*}
\label{subsection:fs-mol}
\paragraph{Benchmark and Baselines.}
FS-Mol, introduced by ~\citet{stanley2021fsmol},  serves as a benchmark for few-shot molecular property prediction. 
It comprises 5,120 tasks, partitioned into 4,938 for training, 40 for validation, 157 for testing, covering a total of  233,786 compounds (see Appendix~\ref{ap:fs_mol_benchmarks}). 
% Further details are provided in Appendix~\ref{ap:fs_mol_benchmarks}.
To evaluate \textbf{\method}, we compare it against four types of baselines: 1) Single-task methods: single-task GP with Tanimoto kernel (\textbf{GP-ST}) \citep {ralaivola2005graph}, single-task GNN (\textbf{GNN-ST}) \citep{gilmer2017neural}, \textbf{MixHop} \citep{abu2019mixhop}, and \textbf{CHEF} \citep{adler2020cross}; 2) Multi-task pre-training: Multi-task GNN (\textbf{GNN-MT}) \citep{stanley2021fsmol}; 3) Self-supervised pre-training: Molecule Attention Transformer (\textbf{MAT}) \citep{maziarka2020molecule}; and 4) Meta-learning methods:  \textbf{PAR} \citep{wang2021propertyaware}, \textbf{ProtoNet} \citep{snell2017prototypical}, \textbf{GNN-MAML} \citep{guo2021few}, \textbf{ADKF-IFT} \citep{chen2023metalearning}, and \textbf{MHNfs} \citep{schimunek2023contextenriched}.
% The experimental results of all baselines are reproduced by \citet{chen2023metalearning}. 
Further details can be found in Appendix~\ref{ap:fs_mol_baselines}.

\paragraph{Evaluation Procedure.}
We follow the experimental setup of the FS-Mol benchmark \citep{stanley2021fsmol}. For each task, we employ unbalanced sampling to create an uneven distribution of positive and negative samples within the support set. The evaluation metric, $\Delta$AUPRC, is used to effectively assess the model’s ability to improve minority class prediction, which is critical in imbalanced datasets (see Appendix~\ref{ap:fs_mol_metrics}). 
% More details of evaluation refer to Appendix~\ref{ap:fs_mol_metrics}. 
% Further details of the evaluation can be found in Appendix~\ref{ap:fs_mol_metrics}. 
During testing, we set five different sizes for the support set: 16, 32, 64, 128, and 256. For each size, we perform 10 repeated random splits of the support/query sets for the test tasks under these settings and take their averages as the final results.

\paragraph{Performance.} 
Figure~\ref{fig:fs_mol_ex}\textcolor{red}{a} displays the results of all compared methods. The results indicate that \textbf{\method} outperforms all benchmarks across various support set sizes. It achieves substantial performance gains of \textbf{4.27}\%, \textbf{8.53}\%, \textbf{4.40}\%, \textbf{6.07}\%, and \textbf{4.26}\% with support set sizes of 16, 32, 64, 128, and 256, respectively. 
These findings highlight the effectiveness of UniMatch’s dual matching mechanism in enhancing  generalization and robustness. Additionally, \method demonstrates strong adaptability, consistently improving performance across different support set sizes.


\paragraph{Ablation Study.}
\label{subsection:ablation_study}
1) To explore the importance of explicit hierarchical molecular matching (i.e., hierarchical pooling and matching) and implicit task-level matching (i.e., meta-learning) in capturing complex molecular structures, we use several common GNNs as baselines. \method extends these baselines by incorporating these hierarchical mechanisms and a meta-learning strategy. 
2) To evaluate the transferability of \method, we test it on several common GNN architectures, including GIN~\citep{xu2018how}, GCN~\citep{kipf2017semisupervised}, GAT~\citep{velivckovic2018graph}, and GraphSAGE~\citep{hamilton2017inductive}.
Experimental results, as shown in Figure~\ref{fig:fs_mol_ex}\textcolor{red}{b}, 
highlight the significant advantages of the dual matching mechanism in effectively processing complex molecular structures and its strong adaptability and transferability across different GNNs.

\paragraph{Sub-benchmark Performance.}
\label{subsection:subbenchmark_performance}
% The FS-Mol benchmark (157 tasks) is divided into 7 subset tasks \citep{stanley2021fsmol}. In Appendix~\ref{ap:sub_benckmark}, we shows the performance of our \method and all compared baselines. Figure~\ref{} demonstrates \method outperforms SODA and 
The FS-Mol benchmark (157 test tasks) is divided into 7 subset tasks \citep{stanley2021fsmol}. The results of \method and baselines on these subset tasks are presented in Appendix~\ref{ap:sub_benckmark}. Table~\ref{table: sub_benchmark} shows the \method  outperforms state-of-the-art methods.

\begin{figure*}[htb]
\centering\includegraphics[width=1.0\textwidth]{meta-molnet.pdf}
    \caption{The performance of all compared methods on the seven classification tasks  with a support set of size 2 on the Meta-MolNet benchmark. Each colored sector represents a method, with the height of the sector indicating its effectiveness. Starting from the black arrow, the methods are listed in the legend in a counterclockwise direction. \textbf{\method} corresponds to the \textcolor{orange}{orange} sector. The dashed orange circle marks the results of \method. Methods with sectors below this line do not surpass \method, while those above it show superior performance. }
    \label{fig:meta_molnet_ex}
    \vspace{-10pt}
\end{figure*}

\subsection{Cross-domain Drug Discovery on the Meta-MolNet Benchmark}
\label{subsection:meta-molnet}
\paragraph{Benchmark and Baselines.}
% Meta-MolNet \citep{lv2024meta} provides a unified standard for evaluating the performance of existing methods in terms of generalization ability and uncertainty quantification for new scaffold molecules, which is crucial for advancing the development of computational chemistry.
% Meta-MolNet combines multiple sub-benchmarks into a diverse benchmark, improving the quality and challenge of few-shot molecular data. We choose the classification tasks (GSK3, JNK3, HIV, Tox21, ToxCast, PCBA, and MUV) of benckmark to evaluate our model. 
% By excluding scaffold collections with a molecular/scaffold ratio higher than a specific threshold, Meta-MolNet significantly improves data quality and testing challenges.
% Meta-MolNet \citep{lv2024meta} sets a standard for evaluating generalization and uncertainty quantification in computational chemistry by combining multiple sub-benchmarks into a comprehensive and challenging benchmark. We evaluate our model on classification tasks from GSK3, JNK3, HIV, Tox21, ToxCast, PCBA, and MUV. 
Meta-MolNet \citep{lv2024meta} sets a standard for evaluating generalization in computational chemistry by improving data quality and testing rigor.
% , achieved by excluding scaffolds with high molecular/scaffold ratios. 
We evaluate our model on classification tasks including GSK3, JNK3, HIV, Tox21, ToxCast, PCBA, and MUV.
For comparison, we consider four types of baselines: 1) Classical machine learning methods: support vector machine (\textbf{SVM}) \citep{bao2016lbsizecleav}, extreme gradient boosting (\textbf{XGBoost}) \citep{deng2021xgraphboost}, and random forests (\textbf{RF}) \citep{fabris2018new}. 2) Supervised learning methods: \textbf{GCN} \citep{kipf2016semi}, \textbf{CMPNN} \citep{song2020communicative}, \textbf{DMPNN} \citep{yang2019analyzing}, \textbf{Attentive FP} \citep{xiong2019pushing}, and \textbf{TrimNet} \citep{10.1093/bib/bbaa266}. 3) Self-supervised learning methods: \textbf{CDDD} \citep{winter2019learning}, \textbf{Mol2Context-vec} \citep{10.1093/bib/bbab317}, \textbf{MolBERT} \citep{fabian2020molecular}, \textbf{N-gram} \citep{liu2019n}, and \textbf{Pre-GNN} \citep{hu2020strategies}. 4) Meta-learning methods: \textbf{ADKF-IFT} \citep{chen2023metalearning} and \textbf{Meta-GAT} \citep{lv2024meta}. All baseline results are reproduced according to  \citet{lv2024meta}. Due to the sub-task settings of Meta-MolNet, prototype-based methods are no longer applicable. Further details can be found in Appendix~\ref{ap:meta_molnet_benchmark}.


\vspace{-5pt}
\paragraph{Evaluation Procedure.}
%To maintain fairness, we adopt the same experimental setup as the Meta-MolNet benchmark \citep{lv2024meta}. 
To evaluate the generalization ability of \method, we follow a higher ratio of molecules/scaffolds by \citet{lv2024meta}. For classification tasks, we use AUROC and AUPRC as evaluation metrics. Specifically, AUROC is used to measure the performance of binary classification tasks (GSK3, JNK3, HIV, Tox21, and ToxCast), while AUPRC is more suitable for tasks with severely imbalanced distributions (PCBA, MUV). 
All experimental results are averaged over three independent runs with different random seeds, using a support set size of 2. Additional details on the evaluation metrics can be found in Appendix~\ref{ap:meta_molnet_mryrics}.

\vspace{-5pt}
\paragraph{Performance.}


Figure~\ref{fig:meta_molnet_ex} presents a comparison of different methods across the seven classification datasets in Meta-MolNet benchmark. The results indicate that \method performs exceptionally well on the GSK3, JNK3, Tox21, and ToxCast datasets, while showing less well on the HIV and PCBA datasets. Our method encounters significant challenges on the MUV dataset, likely due to distributional biases. Overall, \method exhibits excellent generalization capabilities across most datasets for new molecular scaffolds, but struggles in specific cases, such as the MUV dataset.


\subsection{Visualization}
\label{subsection:visualization}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{mol_visualization.pdf}
    \caption{Layer-wise visualization for NR-AhR toxicity prediction. The first row presents PCA projections of 10 molecules, distinguishing  active (blue) from inactive (pink) molecules. The second row displays an internal visualization of a selected molecule across different layers, where color intensity indicating shifts in the model's attention as the layers deepen.}
    \label{fig:mol_visualization}
    % \vspace{-3pt}
\end{figure}

To validate the importance of hierarchical representations, we visualize 10 molecules from the tox21 dataset for the NR-AhR toxicity prediction task, as shown in Figure~\ref{fig:mol_visualization}. In the second row, we select one molecule (SMILES: ``CCOc1ccc2nc(S(N)(=O)=O)sc2c1'') to illustrate how each GNN layer captures distinct structural levels, ranging from atoms and substructures to the entire molecule. Additionally, PCA projections of these 10 molecules are used to examine the distribution of active and inactive molecules. This analysis enhances our understanding of the model's ability to distinguish molecular structures across layers, offering insights into the role of hierarchical feature extraction and its interpretability in toxicity prediction. Further details can be found in Appendix~\ref{ap:visualization_exp}.



\section{Related work}\label{releted work}
% \input{02_releted_work}
\subsection{Graph-based Molecular Property Prediction}
\label{subsection:releted_work_graph_based_MPP}

Graph-based methods are a mainstream approach for the few-shot molecular property prediction task. PAR \citep{wang2021propertyaware} and ADKF-IFT \citep{chen2023metalearning} employ GIN \citep{xu2018how} as the molecular encoder, while Meta-MGNN \citep{guo2021few} utilizes Pre-GIN \citep{hu2020strategies}. Meta-GAT \citep{lv2024meta} adopts GAT \citep{velivckovic2018graph} to learn molecular representations. However, these methods typically focus on single-scale molecular features and overlook the hierarchical nature of molecular structures \citep{altae2017low, ren2018meta, zhuang2023graph}.
In addition, several approaches \citep{zhao2023gimlet, liu2024moleculargpt} combine the strengths of Large Language Models (LLMs) to tackle the few-shot problem, but these methods often incur high computational costs.
Our method differs by incorporating molecular hierarchical structures through hierarchical pooling and matching, enabling more effective alignment of complex structures. 


\subsection{Matching Learning}
\label{matching_learning}

To address the few-shot learning problem, matching learning compares new instances with a small set of labeled examples to facilitate accurate predictions.
Common methods include Matching Networks \citep{vinyals2016matching}, ProtoNet \citep{snell2017prototypical}, Relation Networks \citep{sung2018learning}, and LGM-Net \citep{li2019lgm}.
While these methods perform well in Natural Language Processing (NLP) and Computer Vision (CV), they struggle with the inherent complexity of molecular graphs, which feature non-Euclidean structures and intricate relationships between nodes and edges. Hierarchical matching can mitigate this issue by capturing multi-level representations, but existing approaches still face limitations when applied to molecular data due to its unique topological complexity.
Specifically, 
AMN \citep{mai2019attentive} and SSF-HRNet \citep{zhong2023self}, despite their improvements in feature robustness and hierarchical relationships, struggle to fully represent global structural information and generalize across complex, varied molecular graphs.
Similarly, VTM \citep{kim2023universal} and HCL \citep{zheng2022few} integrate hierarchical matching with patch-level techniques in CV, but their effectiveness diminishes when handling the structural diversity of molecular graphs.
To overcome these challenges, our \method combines explicit intra-molecular hierarchical learning with attention mechanisms at atomic, substructural, and molecular levels, along with implicit task-level hierarchical learning via meta-learning, enhancing the model’s ability to capture task-specific molecular information and improve generalization.






\section{Conclusion}\label{conlusion}
% \input{05_conclusion}
We propose Universal Matching Networks (\method) to address the limitations of existing few-shot learning methods in drug discovery. 
\method employs a dual matching framework that integrates explicit molecular matching with implicit task-level matching.
Explicit hierarchical molecular matching provides contextual representations that support implicit task-level matching, enabling better knowledge sharing across tasks. The complementary nature of these two mechanisms further enhances model performance and adaptability to new tasks.
Experimental results show that \method improves AUROC and $\Delta$AUPRC by 2.87\% and 6.52\%, respectively, on the MoleculeNet, and FS-Mol benchmarks and demonstrates excellent generalization on the Meta-MolNet benchmark. Future work will focus on improving the fusion mechanism of \method by adopting advanced techniques such as attention fusion or multi-scale feature aggregation to better capture the complex relationships between structural levels. Additional details and discussions are provided in Appendix~\ref{ap:conclusion}.

% \paragraph{Limitation.} In our UniMatch framework, we employ a standard mean pooling and a basic concatenation-based fusion methods, which may not fully capture hierarchical feature interactions. While these strategies have showed some effectiveness, more advanced pooling and fusion techniques have not been explored. Additionally, our UniMatch model exhibits underfitting on regression tasks, likely due to the linear layer used in the fusion module. Further exploration and refinement are needed to improve performance, especially for regression tasks.

\paragraph{Acknowledgement.} 
This work was supported by the National Science and Technology Major Project 2023ZD0120802 and the National Key Research and Development Program of China 2022YFB4500300.
 
\newpage

% \paragraph{Ethics Statement.}
% This paper addresses few-shot molecular representation learning without introducing new datasets or requiring human annotation. To the best of our knowledge, no additional ethical concerns arise beyond those commonly associated with research in this field.

% \paragraph{Reproducibility Statement.}
% We have detailed our model design in the paper, covering the architecture (explicit hierarchical molecular matching in Section~\ref{model_architecture}) and meta-learning-based training and inference (implicit task-level matching in Section~\ref{subsction: training_and_inference}). Further training details are provided in the Appendix. We also outline the exact implementation of UniMatch in Section~\ref{experiment} and Appendix. Lastly, we will involve in the discussion regarding implementation details of our paper. 






\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.

\appendix
\newpage

% \section{Meta-testing Algorithm for HierMatch}
% \label{ap:meta-testing}
% \input{sections/A0-appendix}

\section{Details of Training and Inference}
\label{ap:training and inference}
% \input{0A_appedix}

% \begin{equation}
% \label{eq:training}
% \min_{f_{\mathcal{\tau},\bm{\theta}, \textbf{w}}} \mathbb{E}_{\mathcal{S}_{\tau}, \mathcal{Q}_{\tau} \sim \mathcal{D}_{\text{train}}}\left[\frac{1}{\left|\mathcal{Q}_{\tau}\right|} \sum_{\left(\mathbf{x}_{\tau, j}, {y}_{\tau,j}\right) \in \mathcal{Q}_{\mathcal{T}}} \mathcal{L}\left({y}_{\tau, j}, \hat{{y}}_{\tau, j}\right)\right],
% \end{equation}
% where $\mathcal{L}$ represents the cross-entropy (CE) loss function, and $y_{\tau, j} \in \{0, 1\}$ 
% represents the ground-truth label. 


% \begin{equation}
% \label{eq:fine_tune}
% \min _{\mathbf{w}_{\tau}} \mathbb{E}_{\tilde{\mathcal{S}}_\tau, \tilde{\mathcal{Q}}_\tau \sim \mathcal{S}_{\tau, \text{test}}}\left[\frac{1}{|\tilde{\mathcal{Q}}_\tau|} \sum_{\left(\mathbf{x}_{\tau,j}, {y}_{\tau,j}\right) \in \tilde{\mathcal{Q}}_{\tau}} \mathcal{L}\left({y}_{\tau, j}, \hat{{y}}_{\tau, j}\right)\right],
% \end{equation}
% where $\mathbf{w}_\tau$ denotes the task-specific parameters for each task. 


\subsection{Algorithm}

\begin{algorithm}
    \caption{Meta-training procedure for \method.}
    \label{alg:meta_training}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE The few-shot training tasks $\left\{\mathcal{T}_\tau\right\}_{\tau=1}^{N_{t}}$ of molecular property prediction;
        \ENSURE Trained model $f_{\bm{\theta}, \mathbf{w} }$;
        \STATE Randomly initialize $\bm{\theta}$ and $\mathbf{w}$;
        \WHILE {not converged}
            \STATE Sample a batch $\mathcal{B}$ of tasks $\{\mathcal{T}_\tau | \tau \in \mathcal{B}\}$;
            \FOR{all $\{\mathcal{T}_\tau | \tau \in \mathcal{B}\}$}
                \STATE Sample $N_{\tau}^{s}$ and $N_{\tau}^{q}$ molecules to form $\mathcal{S}_{\text{train}, \tau}$ and $\mathcal{Q}_{\text{train}, \tau}$;
                \STATE Split  $\mathcal{S}_{\text{train}, \tau}$ into $\mathcal{S}_{\tau}^\prime$ and $\mathcal{Q}_{\tau}^\prime$;
                \FOR{$l = 1, \dots, L$} 
                    \STATE Obtain node representations $\mathbf{h}_{\tau,s^\prime}^{(l)}$,
                    $\mathbf{h}_{\tau,q^\prime}^{(l)}$, and
                    $\mathbf{h}_{\tau,q}^{(l)}$ of $l^{\text{th}}$ GNN layer by Eq.~\ref{node_update};
                    \STATE Obtain molecular representations $\mathbf{z}_{\tau,s^\prime}^{(l)}$,   $\mathbf{z}_{\tau,q^\prime}^{(l)}$, and $\mathbf{z}_{\tau,q}^{(l)}$ of $l^{\text{th}}$ GNN layer by Eq.~\ref{eq:graph_representation};
                    \STATE Evaluate prediction $\mathbf{\hat{y}}_{\tau, q^\prime}^{(l)}$ and $\mathbf{\hat{y}}_{\tau, q}^{(l)}$ of $l^{\text{th}}$ GNN layer by Eq.~\ref{eq:each_layer_y};
                \ENDFOR
                \STATE Evaluate the final prediction $\mathbf{\hat{y}}_{\tau,q^\prime}$ and $\mathbf{\hat{y}}_{\tau,q}$ by Eq.~\ref{eq:total_y};
                \STATE Evaluate training loss $\mathcal{L}\left(\mathcal{Q}_\tau^{\prime}, f_{\boldsymbol{\theta}, \mathbf{w}}\right)$ by Eq.~\ref{eq:loss_function};
                \STATE Fine-tune $\boldsymbol{\theta}$ as $\boldsymbol{\theta}_\tau$ by Eq.~\ref{eq:inner_loop};
                \STATE Evaluate testing loss $\mathcal{L}\left(\mathcal{Q}_{\text {train }, \tau}, f_{\theta, \mathbf{w}_\tau}\right)$ by Eq.~\ref{eq:loss_function};
            \ENDFOR 
            \STATE Update $\bm{\theta},\mathbf{w}$ by Eq.~\ref{eq:outer_loop};
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}


Algorithm~\ref{alg:meta_training} outlines the meta-training procedure for \textbf{UniMatch}, designed to optimize the model for few-shot molecular property prediction tasks. The algorithm starts by initializing the parameters $\boldsymbol{\theta}$ and $\mathbf{w}$ and iteratively updates them through a combination of inner and outer optimization steps. Within each iteration, a batch of tasks $\mathcal{T}_{\tau}$ is sampled, and the support set $S_{\text{train}, \tau}$ is split into $S_{\tau}'$ and $Q_{\tau}'$. For each GNN layer, node and molecular representations are computed, and predictions are evaluated using the specified loss functions. The meta-parameters are fine-tuned and updated based on the performance on the query sets, ensuring effective model generalization across tasks.

\subsection{Details of Implicit Task-level Matching}
\paragraph{Training Phase.}

\setlength{\leftmargini}{1em} 
\begin{itemize}
    \item \textbf{Task Vector \( \mathbf{p}_\tau \) Expansion and Analysis}. Each task $\mathcal{T}_{\text{train},\tau}$ has its own internal task vector $\mathbf{p}_{\tau}$, which can be represented as an expansion derived from the general parameter $\boldsymbol{\theta}$, the task-specific parameter $\mathbf{w}$, and their relationships with other tasks. The relationship matrix $\mathbf{M}$ governs parameter sharing and task matching.
    \item \textbf{Using \( \mathbf{p}_\tau \) to Construct Task Relationship Matrix \( \mathbf{M} \)}. We want to use the task vector \( \mathbf{p}_\tau \) to construct the task relationship matrix \( \mathbf{M} \). The following methods can be used to measure the similarity or relationship between task vectors:
    \begin{itemize}
    \item \textbf{Dot Product Similarity}:
    \begin{equation}
    \mathbf{M}_{\tau, j} = \mathbf{p}_\tau^\top \mathbf{p}_j\text{.}
    \end{equation}
    
    The dot product similarity measures the inner product of task vectors \( \mathbf{p}_\tau \) and \( \mathbf{p}_j \) in vector space. The larger the value, the higher the similarity between the two tasks.

    \item \textbf{Cosine Similarity}:
    \begin{equation}
    \mathbf{M}_{\tau, j} = \frac{\mathbf{p}_\tau^\top \mathbf{p}_j}{\|\mathbf{p}_\tau\| \|\mathbf{p}_j\|}\text{.}
    \end{equation}
    
    The cosine similarity measures the cosine of the angle between task vectors \( \mathbf{p}_\tau \) and \( \mathbf{p}_j \). The value is in the range of \([-1, 1]\), and the closer it is to 1, the more similar the two tasks are.

    \item \textbf{Euclidean Distance}:
    \begin{equation}
    \mathbf{M}_{\tau, j} = -\|\mathbf{p}_\tau - \mathbf{p}_j\|^2\text{.}
    \end{equation}
    
    The Euclidean distance measures the difference between two task vectors. The smaller the value, the closer the relationship between the two tasks.
    \end{itemize}
    \item \textbf{Using Relationship Matrix \( \mathbf{M} \) for Task Matching}. Using the above similarity metrics, we can obtain a task relationship matrix \( \mathbf{M} \), where each element \( \mathbf{M}_{\tau, j} \) represents the similarity or matching degree between task \( \mathcal{T}_{\text{train},\tau} \) and task \( \mathcal{T}_{\text{train},j} \). Based on this relationship matrix, we can introduce inter-task information sharing in the inner loop and outer loop optimization.
    \item \textbf{Inner Loop Optimization Using Task Vector \( \mathbf{w}_\tau \)}. During the inner loop optimization, we can use the relationship matrix \( \mathbf{M} \) to update the task-specific parameters \( \mathbf{w}_\tau \) of task \( \mathcal{T}_{\text{train},\tau} \). The inner update formula is as defined in Eq.~\ref{eq:m_inner_loop}.
    
    \item \textbf{Outer Loop Optimization Using Task Vector \( \mathbf{p}_\tau \)}.  After $\mathbf{w}_\tau$ updated, task vectors $\mathbf{p}$ can be further refined, which in turn can be used to update the relationship matrix $\mathbf{M}$. During the outer loop optimization, we use the task relationship matrix \( \mathbf{M} \) to update the general parameter \( \boldsymbol{\theta} \), so that the model's performance on all tasks can be improved. The outer update formula is as defined in  Eq.~\ref{eq:m_outer_loop}. 
    \item \textbf{Relationship Matrix \( \mathbf{M} \) Update}.
     Next, the task vectors $\mathbf{p}$ are further refined, and the relationship matrix $\mathbf{M}$ is updated based on the differences between these task vectors.
    % , for example, by minimizing the total difference between all task vectors
% \begin{equation}
% \mathbf{M}_{\tau, j} = \mathbf{M}_{\tau, j} - \beta \cdot \frac{\partial}{\partial \mathbf{M}_{\tau, j}} \sum_{i=1}^{N_t} \left\| \mathbf{w}_\tau - \mathbf{w}_i \right\|^2\text{.}
% \end{equation}

\end{itemize}



\paragraph{Inference Phase.}
In the traditional inference phase, models typically employ explicit gradient descent strategies to update the parameters for a new task. This explicit update process relies on the new task's loss value and gradient calculations. However, under the framework of implicit task matching, we aim to directly generate adaptive parameters for the new task based on its matching relationship with training tasks, thereby avoiding explicit gradient update processes.

Assuming that the current model needs to handle a new task $\mathcal{T}_{\text{test},\tau}$, its adaptive parameters can be represented using the following implicit matching formula:
\begin{equation}
\mathbf{w}_j=\mathbf{w}+\sum_{k=1}^{N_{\text{test}}} \mathbf{M}_{j, k} \cdot\left(\mathbf{w}_k-\mathbf{w}_j\right)\text{,}
\end{equation}
where $N_\text{test}$ denotes the total number of test tasks. 

Then, direct inference and prediction are performed using the fine-tuned new task parameters $\mathbf{w}_j$, thereby improving the sample prediction accuracy for the new task.



\section{Details of MoleculeNet Benchmark}
\label{ap:moleculeNet_benchmark}
% \input{0B_appendix}
In this section, we introduce the details of datasets that are included in the MoleculeNet benchmark in Section~\ref{ap:moleculeNet_datasets}. In addition, we show the details of the experimental setup~\ref{ap:experimental_setup}. 
\subsection{Details of Datasets}
\label{ap:moleculeNet_datasets}
\begin{table}[H]
    \centering
     \caption{Summary of datasets included in MoleculeNet.}
    \begin{tabular}{l|cccc}
       \toprule
         Dataset & Tox21 & SIDER & MUV & ToxCast \\
         \midrule
         Compounds & 8,014 & 1,427 & 93,127 & 8,615 \\
         Tasks & 12 & 27 & 17 & 617 \\ 
         Meta-Training Tasks & 9 & 21 & 12 & 450 \\
         Meta-Testing Tasks & 3 & 6 & 5 & 167 \\
         \toprule
    \end{tabular}
    \label{tab:summary_datasets_in_moleculenet}
\end{table}

In the MoleculeNet benchmark, we perform experiments on 4 datasets in Tabel~\ref{tab:summary_datasets_in_moleculenet}, which include Tox21~\citep{richard2020tox21}, SIDER~\citep{kuhn2016sider}, MUV~\citep{rohrer2009maximum}, and ToxCast~\citep{richard2016toxcast}.
Widely utilized in the assessment of compound toxicity for drug development and environmental risk evaluation, the Tox21 dataset, as described in \citet{richard2020tox21}, contains 8,014 compounds categorized into 12 tasks. By analyzing this dataset, researchers can identify environmental pollutants and potential drug candidates, offering crucial insights into their impact on human health. 
% These efforts are crucial for progress in drug development and environmental risk assessment. 
The SIDER dataset, introduced in~\citet{kuhn2016sider} , serves as a crucial database of drug side effects, encompassing extensive information on medications and their associated adverse responses. This dataset encompasses 1427 compounds distributed among 27 categories. Utilizing the SIDER dataset provides researchers with valuable insights into drug safety profiles and potential side effects.
% This information is crucial for informing drug development, selecting appropriate therapies, and monitoring drug safety. 
The MUV dataset~\citep{rohrer2009maximum}, which includes 93,127 compounds distributed among 17 tasks showcasing a range of biological activities, is widely acknowledged as a key standard for evaluating the multifaceted functions of drug compounds. 
% By assessing these compounds, researchers can identify promising drug candidates, thereby playing a pivotal role in advancing drug discovery and design processes and expediting the exploration and development of innovative pharmaceutical options.
A fundamental resource in toxicology research, the ToxCast dataset \citep{richard2016toxcast} is a critical high-throughput screening database used to evaluate the potential health hazards posed by various compounds. With a compilation of 8,615 compounds and 617 tasks, this dataset significantly contributes to the field of toxicology. 
% With the use of this tool, researchers may quickly and effectively screen and evaluate a compound's toxicological characteristics, which expedites chemical evaluations and risk assessments and ultimately supports drug safety and environmental protection.

%\subsection{Details of Benchmarks}
%\label{ap:moleculeNet_benchmarks}
% In this paper, we compare out \textbf{UniMatch} (Section~\ref{model_architecture} with two types of baselines: 
% \newline a) Few-shot learning method for molecular property prediction learned from scratch: 
% \begin{itemize}
%     \item \textbf{Siamese} \citep{koch2015siamese} utilizes dual conventional neural networks to precisely ascertain and quantify the resemblance of input molecule pairs. 
%     \item \textbf{ProtoNet} \citep{koch2015siamese} employs instances from every category for computing class prototypes and then assigns each query molecule to all class prototypes depending on their similarity.
%     \item \textbf{MAML} \citep{ren2018meta} utilizes a meta-learning approach for model training and adjusts the meta-learned parameters for novel tasks. 
%     \item \textbf{TPN} \citep{liu2018learning} uses a relation graph for implementing label propagation in the context of transductive learning.
%     \item \textbf{EGNN} \citep{kim2019edge} is based on an edge-labeling graph that facilitates the explicit clustering evolution through iterative updates of edge labels. 
%     \item \textbf{IterRefLSTM} \citep{altae2017low}  proposed enhancing the capacity to acquire distance metrics for small molecules by integrating residual LSTM embedding with graph convolutional neural networks.  
%     \item \textbf{PAR} \citep{wang2021propertyaware} leverages prototypes to obtain property-aware representations and realize label propagation by relation graph. 
%     \item \textbf{ADKF-IFT}, as introduced by \citet{chen2023metalearning} in the field of meta-learning, utilizes deep kernel Gaussian processes (GPs) to achieve adaptive deep kernel fitting, particularly advantageous in the context of drug discovery challenges. 
%     \item \textbf{MHNfs} \citep{schimunek2023contextenriched} utilizes a large number of reference to enhance the molecular representation in few-shot tasks, then leverage attention mechanism to fuse enriched molecular representations.  
% \end{itemize}

% b) Methods which utilize pre-trained graph encoder including \textbf{Pre-GNN} \citep{hu2020strategies}, \textbf{Meta-MGNN} \citep{guo2021few}, \textbf{Pre-PAR} \citep{wang2021propertyaware}, and \textbf{ADKF-IFT} \citep{chen2023metalearning}. All of these methods are equipped with \textbf{Pre-GNN} \citep{hu2020strategies}.


\subsection{Details of Experimental Setup}
\label{ap:experimental_setup}

In HieMatch (and Pre-HirMatch), GIN used in Eq.~\ref{node_update} which consists of 5 layers with hidden size 300. In addition, attention mechanism used in Eq.~\ref{eq:each_layer_y} consist of 1 layer with 1 head. We implement UniMatch in PyTorch~\citep{paszke2019pytorch} and Pytorch Geometric library \citep{fey2019fast}. We train the model for a maximum number of 5000 epoches. We employ the Adam optimizer~\citep{kingma2014adam} with a learning rate of 0.001 for meta-learning, while using a higher learning rate of 0.05 for fine-tuning the matching module and fusion module within each task. The dropout rate is maintained at 0.1 for all components, except for the graph-based molecular encoder. 
We summarize the hyperparameters used by UniMatch in Table~\ref{tab:UniMatch hyperparameters}.


\begin{table}[htb]
\centering
\caption{Hyperparameters used by UniMatch}
\label{tab:UniMatch hyperparameters}
    \begin{tabular}{l|c|c}
    \toprule[1pt]
    %\hline
    Hyperparameter          &  Explored values   &  Selected \\ \midrule
    learning rate for meta-learning        & 0.001 & 0.001    \\
    learning rate for fine-tuning  &  0.01$\sim$0.5 & 0.05         \\ 
    number of update steps for fine-tuning      &  1$\sim$5 & 5 \\
    number of layer of GNN in Eq.\ref{node_update}  & 5  & 5  \\
    number of layer of matching module in Eq.\ref{eq:each_layer_y} & 1 & 1 \\ 
    number of head of matching module in Eq.\ref{eq:each_layer_y} & 1 & 1 \\ 
    dropout & 0.0$\sim$0.5 & 0.1 \\
    hidden dimension for GNN in Eq.\ref{node_update} & 300 & 300 \\ 
    \bottomrule[1pt]
    \end{tabular}
\vspace{-5pt}
\end{table}





\section{Details of FS-Mol Benchmark}
% \input{0C-appendix}
\label{ap:fs-mol}

% In this section, we first introduce the details of FS-Mol benchmark \citep{stanley2021fsmol} in Section ~\ref{ap:fs_mol_benchmarks}. Then, we introduce the details of all the compared baselines. 
In this section, we first introduce the details of FS-Mol benchmark \citep{stanley2021fsmol} in Section ~\ref{ap:fs_mol_benchmarks}. The subsequent discussion delves into the details of the compared baselines on FS-Mol benchmark in Section~\ref{ap:fs_mol_benchmarks}. In addition, further details regarding the evaluation metric $\Delta$AUPRC is presented in Section~\ref{ap:fs_mol_metrics}. Finally, the details of experimental setup on FS-Mol benchmark is presented in Section~\ref{ap:fs_mol_experimental_setup}. 


\subsection{Details of Benchmarks}
\label{ap:fs_mol_benchmarks}
% Few-Shot Learning Dataset of Molecules (\textbf{FS-Mol}) \citep{stanley2021fsmol} is introduced to serve for machine learning in the Quantitative Structure-Activity Relationships (QSAR) domain, with a focus on the few-shot learning setting. It consists of a total of 5120 separate assays, with 233,786 unique compounds. Additionally, it was split into three subsets: $D_{train}$ for pre-training tasks, $D_{test}$ for few-shot learning tasks, and $D_{valid}$ for validation. In total, $D_{test}$ comprises 157 tasks, $D_{train}$ has 4938 tasks, and $D_{valid}$ consists of 40 tasks. Moreover, the mean number of compounds per task is 94, far below alternative datasets, reflecting the highly specific nature of the protein targets and the assays used to explore them, which is a key challenge in the QSAR domain. 

The Few-Shot Learning Dataset of Molecules (\textbf{FS-Mol}) \citep{stanley2021fsmol} is designed for machine learning applications in the Quantitative Structure-Activity Relationships (QSAR) field \citep{tropsha2023integrating}, specifically focusing on few-shot learning scenarios. It comprises a total of 5120 distinct assays, encompassing 233,786 unique compounds. The dataset is partitioned into three subsets: $D_{train}$ for training, $D_{test}$ for testing, and $D_{valid}$ for validation purposes. $D_{test}$ contains 157 tasks, $D_{train}$ includes 4938 tasks, and $D_{valid}$ is composed of 40 tasks. Notably, each task in the dataset contains an average of 94 compounds, a notably lower figure compared to other similar datasets. This characteristic reflects the high specificity of the protein targets and the corresponding assays, posing a significant challenge in the QSAR domain.



\subsection{Details of Baselines.}
\label{ap:fs_mol_baselines}
In the comparative analysis of the FS-Mol benchmark \citep{stanley2021fsmol}, four types of baselines have been chosen: Single-task methods, Multi-task pre-training methods, Self-supervised pre-training methods, and Meta-learning methods.

\paragraph{Single-task Methods.} 
% The single-task methods include Random Forest (\textbf{RF}) \citep{fabris2018new}, single-task GP with Tanimoto kernel (\textbf{GP-ST}) \citep {ralaivola2005graph}, single-task GNN (\textbf{GNN-ST}) \citep{gilmer2017neural}, \textbf{MHNfs} \citep{schimunek2023contextenriched}.

The single-task methods are single-task GP with Tanimoto kernel (\textbf{GP-ST}) \citep {ralaivola2005graph}, single-task GNN (\textbf{GNN-ST}) \citep{gilmer2017neural}, \textbf{MixHop} \citep{abu2019mixhop} and \textbf{CHEF} \citep{adler2020cross} for context-enriched information.

% \textbf{MHNfs} \citep{schimunek2023contextenriched}

% Specifically, \textbf{RF} \citep{fabris2018new} is an ensemble learning technique that creates and merges numerous Decision Trees to make predictions. Its primary objective is to enhance prediction accuracy and model stability through the aggregation of multiple models' predictions. 
% \textbf{GP-ST} \citep{ralaivola2005graph} presents several graph kernel methods that can be used to compute the similarity between chemical compound graphs. These include the random walk kernel, the shortest-path kernel, and the subtree kernel. 
% \textbf{GNN-ST} \citep{gilmer2017neural} introduces a novel neural network architecture called ``Message Passing Neural Networks'' (MPNNs) that can effectively learn from the graph-structured representations of molecules and demonstrates its application to the domain of quantum chemistry. 
% \textbf{MHNfs} \citep{schimunek2023contextenriched} uses a novel concept to enrich molecule representations by associating them with a large set of reference (context) molecules through a Modern Hopfield Network (MHN). This enrichment step amplifies the covariance structure of the data while removing spurious correlations.

% \textbf{RF} method, as proposed by \citep{fabris2018new}, is an ensemble learning approach that amalgamates multiple Decision Trees to enhance predictive accuracy and model robustness. 
% \textbf{GP-ST}, as delineated in the study by \citep{ralaivola2005graph}, encompassing the random walk kernel, shortest-path kernel, and subtree kernel, are employed within the \textbf{GP-ST} to evaluate the resemblance between graphs of chemical compounds.
% \citet{gilmer2017neural} introduces the \textbf{GNN-ST}, particularly focusing on Message Passing Neural Networks (MPNNs), for proficient learning from graph-based representations of molecules in quantum chemistry. 
% \citet{schimunek2023contextenriched} proposes \textbf{MHNfs} approach, utilizing a Modern Hopfield Network (MHN) to link molecules with an extensive array of reference molecules, thereby enhancing the covariance structure of the data and eliminating spurious correlations.

% \textbf{RF}, as proposed by \citep{fabris2018new}, is an ensemble learning approach that amalgamates multiple Decision Trees to enhance predictive accuracy and model robustness.
\textbf{GP-ST}, as delineated in the study by \citep{ralaivola2005graph}, encompassing the random walk kernel, shortest-path kernel, and subtree kernel, are employed to evaluate the resemblance between graphs of chemical compounds.
\citet{gilmer2017neural} introduces \textbf{GNN-ST}, particularly focusing on MPNNs for proficient learning from graph-based representations of molecules in quantum chemistry.
\textbf{CHEF} \citep{adler2020cross}  leverages fingerprint-based features to capture chemical information, thereby enhancing the performance of molecular property prediction tasks.
\textbf{MixHop} \citep{abu2019mixhop} is a novel graph convolutional architecture that enables higher-order neighborhood mixing in Graph Neural Networks (GNNs). By incorporating multiple neighborhood feature mixing operations, including neighborhood difference operators, the MixHop model can learn a broader range of graph structural representations without increasing computational complexity.

% footnote{https://github.com/microsoft/FS-Mol} 
\paragraph{Multi-task Pre-training Method.} 
% Multi-task GNN (\textbf{GNN-MT}) \citep{stanley2021fsmol} uses a pre-trained shared GNN model with 10 layers and a hidden dimension of 128, and employs a ``principal neighborhood message aggregation'' technique for information propagation across the graph, and then uses task-specific ``gated graph readout'' functions and a task-specific MLP with one hidden layer of dimension 512 to produce the final activity label predictions. The entire model is trained using a multi-task learning approach, where the pre-trained GNN model is fine-tuned on the support sets of all tasks in $D_{train}$ over multiple epochs.
Multi-task GNN (\textbf{GNN-MT}) \citep{stanley2021fsmol} 
% uses a pre-trained shared GNN model with 10 layers and a hidden dimension of 128, and employs a ``principal neighborhood message aggregation'' technique for information propagation across the graph, and then uses task-specific ``gated graph readout'' functions and a task-specific MLP with one hidden layer of dimension 512 to produce the final activity label predictions. The entire model is trained using a multi-task learning approach, where the pre-trained GNN model is fine-tuned on the support sets of all tasks in $D_{train}$ over multiple epochs.
employs a 10-layer pre-trained GNN with 128 hidden dimensions and "principal neighborhood message aggregation." Task-specific readout functions and an MLP with a 512-dimensional hidden layer produce activity label predictions. The model is fine-tuned on all tasks in $\mathcal{D}_{train}$ using multi-task learning.

% \footnote{https://github.com/ardigen/MAT.git} 
\paragraph{Self-supervised Pre-training Method.} 
% Molecule Attention Transformer (\textbf{MAT}) \citep{maziarka2020molecule} adapts the Transformer architecture by augmenting the self-attention mechanism with information about inter-atomic distances and the molecular graph structure.
The Molecule Attention Transformer (\textbf{MAT}) \citep{maziarka2020molecule} modifies the Transformer architecture \citep{vaswani2017attention} by incorporating insights on inter-atomic distances and the molecular graph structure into the self-attention mechanism.


\paragraph{Meta-learning Methods.} 
Property-Aware Relation Networks (\textbf{PAR}) \citep{wang2021propertyaware}, Prototypical Networks (\textbf{ProtoNet}) \citep{snell2017prototypical}, \textbf{GNN-MAML} \citep{guo2021few}, and \textbf{ADKF-IFT} \citep{chen2023metalearning} are four typical meta-learning methods.
%
% \footnote{https://github.com/tata1661/PAR-NeurIPS21.git}
% Specifically, \textbf{PAR} \citep{wang2021propertyaware}, introduces a property-aware embedding function that transforms generic molecular embeddings into a substructure-aware representation which relevant to the target property, and designs an adaptive relation graph learning module to jointly estimate the molecular relation graph and refine the molecular embeddings with respect to the target property.
Specifically, \textbf{PAR} \citep{wang2021propertyaware}, introduces a property-aware embedding function that transforms generic molecular embeddings into a substructure-aware representation which relevant to the target property, and designs an adaptive relation graph learning module to jointly estimate the molecular relation graph and refine the molecular embeddings with respect to the target property.
\citet{schimunek2023contextenriched} proposes \textbf{MHNfs} approach, utilizing a Modern Hopfield Network (MHN) \citep{ramsauer2020hopfield} to link molecules with an extensive array of reference molecules, thereby enhancing the covariance structure of the data and mitigating spurious correlations of molecules.
%
% \footnote{https://github.com/jakesnell/prototypical-networks} 
\textbf{ProtoNet} \citep{snell2017prototypical}, a simple approach to few-shot classification, learns an embedding where each class is represented by a prototype, computed as the mean of the embedded support examples for that class. Classification is then done by computing distances from the query example to each class prototype.
%
% \footnote{https://github.com/zhichunguo/Meta-MGNN.git}
\textbf{GNN-MAML} \citep{guo2021few} uses graph neural networks to learn molecular representations, and employs a meta-learning framework for model optimization. It also incorporates molecular structure, self-supervised modules, and self-attentive task weights to exploit unlabeled data and address task heterogeneity.
%
% \footnote{https://github.com/Wenlin-Chen/ADKF-IFT.git} 
\textbf{ADKF-IFT} \citep{chen2023metalearning} combines the representational power of deep learning with the probabilistic modeling capabilities of gaussian processes, enabling efficient and uncertainty-aware molecular property prediction through meta-learning.





\subsection{Evaluation Metrics of FS-Mol Benchmark}
\label{ap:fs_mol_metrics}

The $\Delta\text{AUPRC}$ (Area Under the Curve for Precision-Recall) serves as a pivotal statistical measure utilized for assessing enhancements in the efficacy of classification models when confronted with imbalanced datasets due to targeted modifications, like algorithmic adjustments or alterations in data processing methodologies. By contrasting the precision-recall curve's area prior to and post adjustments, this metric adeptly elucidates the extent of enhancement in the capacity of model to identify minority classes, thereby supplying a quantitative foundation for optimizing the model and facilitating decision-making support. 

% Following~\citet{stanley2021fsmol}, we employ the $\Delta$AUPRC as an evaluation metric to compare all the baselines. The specific calculation formula is as follows:
In line with the research conducted by~\citet{stanley2021fsmol}, we employ the $\Delta$AUPRC as an evaluation metric for comparing all baseline models. The specific calculation formula is detailed below:
\begin{equation}
\triangle \operatorname{AUPRC}\left(f_{\bm{\theta},\mathbf{w}}\right)=\operatorname{AUPRC}\left(f_{\bm{\theta},\mathbf{w}}\right)-\frac{N_\tau^q(1)}{N_\tau^q},
\end{equation}
where the ${N_\tau^q}(1)$ represents the number of active molecules in query set $\mathcal{Q}_\tau$.  

\subsection{Details of Experimental Setup}
\label{ap:fs_mol_experimental_setup}


In UniMatch, the hyperparameters used by UniMatch are reported in Table~\ref{tab:UniMatch hyperparameters}. What is more, on FS-Mol benchmark \citep{stanley2021fsmol}, we set the batch task 21 and weight decay 5e-5. And we train the model for 10,000 epoches. 




\section{Details of Meta-MolNet Benchmark}
\label{ap:meta_molnet}
% \input{0D-appendix}
In this section, we first introduce the details of Meta-MolNet benchmark \citep{lv2024meta} in Section~\ref{ap:meta_molnet_benchmark}. In addition, we provide the details of the baselines in Section~\ref{ap:meta_molnet_baselines}. Finally, the details of evaluation metric is provided in Section~\ref{ap:meta_molnet_mryrics}. 

\subsection{Details of Benchmarks}
\label{ap:meta_molnet_benchmark}
Meta-MolNet is an innovative benchmarking platform designed to improve molecular machine learning models by integrating diverse datasets through multitask and transfer learning, spanning applications from drug discovery to materials science.
In this paper, we use 7 classification tasks on Meta-MolNet benchmark to evaluate our UniMatch, which include GSK3, JNK3, HIV, Tox21, ToxCast, PCBA and MUV. 
The GSK3 dataset focuses on predicting the activity of compounds against the GSK3 enzyme, which is associated with diseases like diabetes and Alzheimer's. The JNK3 dataset assesses the inhibitory activity of compounds against JNK3, a kinase implicated in neurodegenerative diseases. The HIV dataset contains data for predicting the ability of compounds to inhibit HIV replication. Tox21 evaluates the toxicity of compounds across multiple biological pathways, while ToxCast predicts the toxic effects of environmental chemicals. The PCBA dataset measures compound activity across various bioassays from the PubChem database. Lastly, the MUV dataset provides a rigorous and unbiased benchmark for validating virtual screening methods. Together, these tasks offer a comprehensive evaluation framework for molecular machine learning models. The detailed description of datasets in Table~\ref{tab:meta_molnet_benchmark}.
\begin{table}[htb]
\centering
\caption{Detailed Description of the benchmark datasets}
\label{tab:meta_molnet_benchmark}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l|ccccccccc}
        \toprule[1pt]
        %\hline
        Task type & Datasets & Category & Data type & Tasks & \makecell{No. of\\Molecules} & \makecell{No. of\\Scaffolds} & \makecell{Molecules/\\Scaffolds ratio} & Metrics & Threshold \\ 
       
        % single task classification
        \midrule
        \multirow{3}{*}{\makecell{Single Task\\Classification}} 
        & GSK3 & Biophysics & SMILES & 1 & 3,197 & 38 & 84.13 & ROC-AUC & 30 \\ 
        & JNK3 & Biophysics & SMILES & 1 & 4,873 & 62 & 78.60 & ROC-AUC & 30 \\ 
        & HIV & Biophysics & SMILES & 1 & 6,386 & 68 & 93.91 & ROC-AUC & 30 \\ 
    
        % mult task classification
        \midrule
        \multirow{4}{*}{\makecell{Multi Task\\Classification}} 
        & Tox21 & Physiology & SMILES & 12 & 2,119 & 12 & 176.58 & ROC-AUC & 30 \\ 
        & ToxCast & Physiology & SMILES & 617 & 2,372 & 14 & 169.43 & ROC-AUC & 30 \\ 
        & PCBA & Biophysics & SMILES & 128 & 21,835 & 34 & 642.21 & PRC-AUC & 200 \\ 
        & MUV & Biophysics & SMILES & 17 & 11,671 & 152 & 76.78 & PRC-AUC & 30 \\ 
        \bottomrule[1pt]
        \end{tabular}
    }
\end{table}

\subsection{Details of Baselines}
\label{ap:meta_molnet_baselines}
% Four types of baselines (classical machine learning, graph-based models, message passing neural networks, and self-supervised pre-training models) are chosen for comparative analysis on the Meta-MolNet benchmark~\citep{lv2024meta}.
Four types of baselines—classical machine learning models, graph-based models, message passing neural networks, and self-supervised pre-training models—are chosen for comparative analysis on the Meta-MolNet benchmark~\citep{lv2024meta}.

\paragraph{Classical Machine Learning Methods.}
% Support vector machines (\textbf{SVM}) \citep{bao2016lbsizecleav}, extreme gradient boosting algorithms (\textbf{XGBoost}) \citep{deng2021xgraphboost}, and Random Forests (\textbf{RF}) \citep{fabris2018new} are among the classical machine learning methods that utilize descriptors and/or fingerprints commonly found in traditional QSPR/QSAR models \citep{cherkasov2014qsar}. Notably, the Extended Connectivity Fingerprints (ECFPs) \citep{rogers2010extended, glen2006circular} and Molecular ACCess System (MACCS) keys \citep{bender2004similarity, unterthiner2014deep} are widely used as fingerprints in such models. \textbf{SVM} \citep{bao2016lbsizecleav} is a robust machine learning algorithm that aims to identify the optimal solution for classification task by determining the maximum margin hyperplane within a high-dimensional space. \textbf{XGBoost} \citep{deng2021xgraphboost} is a proficient machine learning technique that utilizes distributed gradient boosting to offer rapid, adaptable, and user-friendly solutions. \textbf{RF} \citep{fabris2018new} can be seen in Appendix~\ref{ap:fs_mol_benchmarks}.

Support Vector Machines (\textbf{SVM}) \citep{bao2016lbsizecleav}, extreme gradient boosting algorithms (\textbf{XGBoost}) \citep{deng2021xgraphboost}, and Random Forests (\textbf{RF}) \citep{fabris2018new} are among the classical machine learning methods that utilize descriptors and/or fingerprints commonly found in traditional QSPR/QSAR models \citep{cherkasov2014qsar}. Notably, the Extended Connectivity Fingerprints (ECFPs) \citep{rogers2010extended, glen2006circular} and Molecular ACCess System (MACCS) keys \citep{bender2004similarity, unterthiner2014deep} are widely used as fingerprints in such models.
\textbf{SVM} \citep{bao2016lbsizecleav} is a robust machine learning algorithm designed to identify the optimal solution for classification tasks by determining the maximum margin hyperplane within a high-dimensional space. \textbf{XGBoost} \citep{deng2021xgraphboost} is a proficient machine learning technique that utilizes distributed gradient boosting to provide rapid, adaptable, and user-friendly solutions. Information about \textbf{RF} \citep{fabris2018new} can be found in Appendix~\ref{ap:fs_mol_benchmarks}.

\paragraph{Supervised Learning Methods.}
% Graph Convolutional Network (\textbf{GCN}) \citep{duvenaud2015convolutional}, Directed Message Passing Neural Networks (\textbf{DMPNN}) \citep{yang2019analyzing}, Directed graph-based communication message transfer neural network (\textbf{CMPNN}) \citep{song2020communicative}, \textbf{Attentive FP} \citep{xiong2019pushing}, and Triplet message network (\textbf{TrimNet}) \citep{10.1093/bib/bbaa266} are among the supervised learning methods. 
Graph Convolutional Networks (\textbf{GCN}) \citep{duvenaud2015convolutional}, Directed Message Passing Neural Networks (\textbf{DMPNN}) \citep{yang2019analyzing}, Communicative Message Passing Neural Networks (\textbf{CMPNN}) \citep{song2020communicative}, \textbf{Attentive FP} \citep{xiong2019pushing}, and Triplet Message Networks (\textbf{TrimNet}) \citep{10.1093/bib/bbaa266} are among the supervised learning methods.
%
Specifically, \textbf{GCN}\footnote{\url{https://github.com/tkipf/gcn.git}} \citep{duvenaud2015convolutional} 
employs convolution operations based on the eigen decomposition of the Laplacian matrix, which allows them to aggregate information from neighboring nodes and derive node embedding representations.
% 
\textbf{DMPNN}\footnote{\url{https://github.com/chemprop/chemprop.git}} \citep{yang2019analyzing} 
use Laplacian eigen decomposition for convolution operations, aggregating information from neighboring nodes to derive node embeddings.
% 
\textbf{CMPNN}\footnote{\url{https://github.com/SY575/CMPNN.git}} \citep{song2020communicative} 
enhance modeling of molecular properties by using a node-edge interaction module to effectively integrate atom and bond features.
% 
\textbf{Attentive FP}\footnote{\url{https://github.com/OpenDrugAI/AttentiveFP.git}} \citep{xiong2019pushing}
employs atom and bond attributes to create feature vectors, preserving spatial information and capturing both local and nonlocal effects with a graph attention mechanism.
% 
\textbf{TrimNet}\footnote{\url{https://github.com/yvquanli/trimnet.git}} \citep{10.1093/bib/bbaa266} 
utilizes a triplet message mechanism to extract edge information from atom-bond-atom interactions, achieving state-of-the-art performance.


\paragraph{Self-supervised Learning Methods.}
% \textbf{CDDD} \citep{winter2019learning}, \textbf{Mol2Context-vec} \citep{10.1093/bib/bbab317}, \textbf{MolBERT}, \textbf{N-gram}, and pre-training graph neural networks (\textbf{Pre-GNN}) \citep{hu2020strategies} are self-supervised learning methods that can learn to extract meaningful molecular descriptors by pre-training on large-scale molecular datasets. These approaches do not rely on fixed feature extraction rules but instead learn data-driven methods for feature extraction, which can lead to more generalizable features that are less prone to overfitting. 
\textbf{CDDD} \citep{winter2019learning}, \textbf{Mol2Context-vec} \citep{10.1093/bib/bbab317}, \textbf{MolBERT}, \textbf{N-gram}, and \textbf{Pre-GNN} \citep{hu2020strategies} are self-supervised methods that pre-train on large molecular datasets to extract meaningful descriptors. These data-driven approaches produce generalizable features, avoiding fixed extraction rules and reducing overfitting.
% 
Specifically, \textbf{CDDD}\footnote{\url{https://github.com/jrwnter/cddd.git}} \citep{winter2019learning} 
% learns a feature representation from low-level encodings from a huge corpus of chemical structures. Inspired by neural machine translation techniques, it translates between two semantically equivalent but syntactically distinct representations of molecular structures, compressing the meaningful information both representations have in common in a low-dimensional representation vector.
learns features from a large chemical structure corpus by translating between different molecular representations, compressing shared information into a low-dimensional vector.
% 
\textbf{Mol2Context-vec}\footnote{\url{https://github.com/lol88/Mol2Context-vec.git}} \citep{10.1093/bib/bbab317} 
% is a deep contextualized Bi-LSTM architecture, which can integrate different levels of internal states to generate dynamic representations of molecular substructures. It allows the model to capture a rich understanding of the molecular context, including the interactions between various atomic groups within the molecule, particularly intramolecular hydrogen bonds and other non-covalent interactions.
uses a Bi-LSTM to create dynamic representations of molecular substructures, capturing intramolecular hydrogen bonds and other non-covalent interactions.
% 
\textbf{MolBERT}\footnote{\url{https://github.com/BenevolentAI/MolBERT.git}} \citep{fabian2020molecular} 
% is designed based on a Transformer architecture, which employs the BERT to learn flexible and high-quality molecular representations to resolve drug discovery problems.
is a Transformer-based model that uses BERT \citep{devlin2018bert} to learn high-quality molecular representations for drug discovery.
%
\textbf{N-gram}\footnote{\url{https://github.com/chao1224/n\_gram\_graph.git}} \citep{liu2019n} 
% learns a graph-level representation by capturing the co-occurrence patterns of local substructures within the graph. It extracts all possible n-grams (sequences of n-connected nodes) from the input graph, then constructs a histogram or frequency vector to represent the occurrences of these n-grams, which serves as the graph-level representation.
captures co-occurrence patterns of local substructures by extracting n-grams from the graph and creating a histogram to represent their frequencies, forming the graph-level representation.
% 
\textbf{Pre-GNN}\footnote{\url{https://github.com/snap-stanford/pretrain-gnns.git}} \citep{hu2020strategies} 
% is a novel pre-training strategy for graph neural networks (GNNs), which pre-trains an expressive GNN model by learning useful representations at both the individual node level and the entire graph level to simultaneously capture local and global structural information within the molecular graphs during the pre-training phase. 
pre-trains graph neural networks by learning representations at both node and graph levels, capturing local and global structural information in molecular graphs.

\paragraph{Meta-learning Methods.}
\textbf{Meta-GAT} \citep{lv2024meta} and \textbf{ADKF-IFT} \citep{chen2023metalearning} are two typical meta-learning methods.  
% 
Specifically, \textbf{Meta-GAT}\footnote{\url{https://github.com/lol88/Meta-MolNet.git}} \citep{lv2024meta} 
% is a graph attention network model that employs a cross-domain meta-learning approach to reliably predict molecular properties using only a few example scenarios. By analyzing and extracting meta-knowledge from similar molecules in different source domains, Meta-GAT is able to reduce the sample complexity required when transferred to new chemical spaces. This meta-knowledge then enables Meta-GAT to quickly adapt to new scaffold molecules in the target domain through internal iteration on a small number of examples.
s a graph attention network that uses cross-domain meta-learning to predict molecular properties with few examples. By extracting meta-knowledge from similar molecules across domains, it reduces sample complexity and quickly adapts to new scaffold molecules with minimal data.
% 
\textbf{ADKF-IFT}\footnote{\url{https://github.com/Wenlin-Chen/ADKF-IFT.git}} \citep{chen2023metalearning} can be seen in Appendix~\ref{ap:fs_mol_benchmarks}.

\subsection{Evaluation Metrics of Meta-MolNet}
\label{ap:meta_molnet_mryrics}

% In this paper, we use the benchmark datasets of higher ratio of molecules/scaffolds, which is significantly more challenging than random cross-validation and low ratio of molecules/scaffolds \citep{lv2024meta}, for the generalization ability evaluation. For classification tasks, we use Area Under the Receiver Operating Characteristic Curve (\textbf{AUROC}) and Area Under the Precision-Recall Curve (\textbf{PRAUC}) as evaluation metrics. Specifically, AUROC measures the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) across different classification thresholds. The AUROC ranges from 0 to 1, where 0.5 represents a random classifier and 1 represents a perfect classifier. A higher AUROC value indicates a better classification performance. It is well suited for evaluating the performance of binary classification tasks (GSK3, JNK3, HIV, Tox21, and ToxCast). Meanwhile, the PRAUC considers the trade-off between precision (positive predictive value) and recall (sensitivity). The PRAUC ranges from 0 to 1, where a higher value indicates better classification performance. It is used to evaluate the classification ability of a model in the context of imbalanced datasets. Therefore, it is more suitable for evaluating tasks (PCBA, MUV) with severely skewed distributions. 
In this paper, we use benchmark datasets with a higher ratio of molecules to scaffolds, presenting a significantly more challenging scenario compared to random cross-validation and datasets with a lower ratio \citep{lv2024meta}, for evaluating generalization ability. For classification tasks, we use Area Under the Receiver Operating Characteristic Curve (\textbf{AUROC}) and Area Under the Precision-Recall Curve (\textbf{AUPRC}) as evaluation metrics.
%
Specifically, \textbf{AUROC} measures the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) across different classification thresholds. \textbf{AUROC} ranges from 0 to 1, where 0.5 represents a random classifier and 1 represents a perfect classifier. A higher \textbf{AUROC} value indicates better classification performance, making it well-suited for evaluating binary classification tasks such as GSK3, JNK3, HIV, Tox21, and ToxCast.
%
Meanwhile, \textbf{AUPRC} considers the trade-off between precision (positive predictive value) and recall (sensitivity). Like \textbf{AUROC}, \textbf{AUPRC} ranges from 0 to 1, with higher values indicating better performance. \textbf{AUPRC} is particularly useful for evaluating models on imbalanced datasets, making it more suitable for tasks such as PCBA and MUV, which have severely skewed distributions.

\subsection{Details of Experimental Setup}
% On Meta-MolNet bechmark, we set the query set 8 and support set 2. We employ AdamW optimizer \citep{loshchilov2017decoupled} with a learning rate of 0.001 for meta-learning, while the inner learning rate is 0.001 for fine-tuning the task-specific modules within each task. The weight decay is 5e-4. We set the training epoch 100.

On the Meta-MolNet benchmark, we set the query set size to 8 and the support set size to 2. We employ the AdamW optimizer \citep{loshchilov2017decoupled} with a learning rate of 0.001 for meta-learning and an inner learning rate of 0.001 for fine-tuning the task-specific modules within each task. A weight decay of 5e-4 is applied. The model is trained for 100 epochs to ensure robust performance.



\section{Further Experiments Results on FS-Mol}
% \input{0E-appendix}
\label{ap:further_experiments_results_on_fs_mol}
\subsection{Overall Performance}
\label{ap:overall_performance}
\begin{figure*}
\centering\includegraphics[width=0.98\textwidth]{supp_fs_mol_all.pdf}
    \caption{Box plots illustrate how different methods perform in classifying 157 FS-Mol test tasks across various support set sizes.} %\textcolor{blue}{MQ: rearrange font and layout}}
    \label{fig:supp_fs_mol_all}
\end{figure*}


Figure \ref{fig:supp_fs_mol_all} (a)$\sim$(e) show the performance of different methods in classifying 157 FS-Mol \citep{stanley2021fsmol} test tasks across various support set sizes via box plots. The box plots show the distribution of classification accuracies for each method, providing insight into their overall performance and effectiveness in handling varying support set sizes. Our HieMatch demonstrates superior performance compared to the state-of-the-art (SOTA) method across all metrics. 

\subsection{Sub-benchmark Performance}
\label{ap:sub_benckmark}
FS-Mol \citep{stanley2021fsmol} divides tasks into 7 sub-benchmarks using Enzyme Commission (EC) numbers \citep{hu2012assignment}, allowing for assessment across the entire benchmark. In classification tasks with a support set size of 16, Table~\ref{table: sub_benchmark} illustrates the performance of the top methods across all sub-benchmarks. The results highlight that, while excelling in overall performance, HieMatch emerges as the top performer in half of the sub-benchmarks for classification tasks.



\begin{table}[htb]
\centering
\label{table: sub_benchmark}
\caption{The classification performance for the 16 support set size.}
% \tiny
\resizebox{\linewidth}{!}{
    % \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} c c  c ccccc@{\extracolsep{\fill}}}
    \begin{tabular}{cccccccc}
    \toprule
    \multicolumn{3}{c}{ FS-Mol sub-benchmark (EC category) } & \multicolumn{5}{c}{ Method } \\
    % \hline
    \midrule
    Class & Description & \#tasks &  RF  & GP-ST & GNN-MAML & ADKF-IFT & UniMatch \\
    \midrule
    1 & oxidoreductases & 7 &  0.081 $\pm$ 0.032  &  0.013 $\pm$ 0.019  & 0.046 $\pm$ 0.023  &  0.103 $\pm$ 0.0036 & \textbf{0.231} $\pm$ \textbf{0.075} \\
    2 & kinases & 125 &  0.082 $\pm$ 0.006  &  0.013 $\pm$ 0.004  &  0.178 $\pm$ 0.009  &  0.247 $\pm$ 0.010 & \textbf{0.256} $\pm$ \textbf{0.012} \\
    3 & hydrolases & 20 &  0.158 $\pm$ 0.026  &  0.062 $\pm$ 0.019 & 0.106 $\pm$ 0.024  &  0.213 $\pm$ 0.029  & 0.201 $\pm$ 0.028   \\
    4 & lysases & 2 &  0.218 $\pm$ 0.172  &  0.161 $\pm$ 0.112   &  0.218 $\pm$ 0.147 & 0.223 $\pm$ 0.160 & 0.211 $\pm$ 0.061 \\
    5 & isomerases & 1 & 0.119 $\pm$ 0.029  &  -0.014 $\pm$ 0.015  &  0.006 $\pm$ 0.021  & 0.121 $\pm$ 0.049 & 0.087 $\pm$ 0.025\\
    6 & ligases & 1 &  0.027 $\pm$ 0.069  &  -0.011  $\pm$ 0.003   &  0.001 $\pm$ 0.017 &  0.103 $\pm$ 0.066 & 
    \textbf{0.359} $\pm$ \textbf{0.011} \\
    \multirow[t]{2}{*}{7} & translocases & 1 &  0.102 $\pm$ 0.053  &  0.067 $\pm$ 0.050  & 0.001 $\pm$ 0.021 &  0.082 $\pm$ 0.049 & -0.009 $\pm$ 0.011 \\
    \midrule
    & all enzymes & 157 &  0.093 $\pm$ 0.007  &  0.021 $\pm$ 0.005  & 0.162 $\pm$ 0.009  &  0.230 $\pm$ 0.009 & \textbf{0.245} $\pm$ \textbf{0.011} \\
      \toprule
    \end{tabular}
}
\end{table}


\subsection{Meta-testing Costs}
\label{ap:meta_testing_costs}

In this section, we compare the inference time of our UniMatch with meta-learning approaches. Figure~\ref{fig:supp_fs_mol_clock} illustrates that UniMatch takes slightly more time compared to ProtoNet \citep{snell2017prototypical} and GNN-MAML \citep{guo2021few}. Additionally, ADKF-IFT \citep{chen2023metalearning} exhibits the longest reference time. However, it is important to note that UniMatch still maintains a relatively fast inference time, making it a viable option for meta-learning tasks. 

\begin{figure*}[htb]
\centering\includegraphics[width=0.7\textwidth]{fs_mol_clock.pdf}
    \caption{The wall-clock time, along with standard errors, is recorded during meta-testing on a predetermined set of FS-Mol classification tasks for comparison with the meta-learning approaches.} %\textcolor{blue}{MQ: rearrange font and layout}}
    \label{fig:supp_fs_mol_clock}
    % \vspace{-7pt}
\end{figure*}





% \subsection{Comparison of Model Parameter Capacity}
% \label{ap:model_parameters}



\section{Visualization Experiments}
% \input{0F-appendix}
\label{ap:visualization_exp}
The details of the ten molecules used in Section~\ref{subsection:visualization} are represented in Table~\ref{tab:visualization_mol}.

\begin{table}[thb]
    \centering
    \caption{The ten molecular sampled from the Tox21 dataset are used for the NR-AhR toxicity prediction task. ``1'' indicates active molecules, while ``0'' indicates inactive molecules.}
    \begin{tabular}{l|c}
    \toprule
        SMILES & Label \\
    \midrule
    CCOc1ccc2nc(S(N)(=O)=O)sc2c1 & 1 \\
    CC1=C(C(=O)Nc2ccccc2)S(=O)(=O)CCO1  & 0 \\ 
    CC(C)(C)C1CCC(=O)CC1  & 0 \\
    Nc1ccc(/N=N/c2ccccc2)cc1 & 1 \\
    COCC(C)O & 0 \\
    Nc1ccccc1C(=O)Oc1ccc2ccccc2c1 & 1 \\
    ONc1ccccc1 & 1 \\
    CC(O)CNCC(C)O & 0 \\
    CCCCC(CC)CCC(CC(C)C)OS(=O)(=O)[O-] & 0 \\
    O=C([O-])COc1nn(Cc2ccccc2)c2ccccc12  & 0 \\
    \bottomrule   
    \end{tabular}
 
    \label{tab:visualization_mol}
\end{table}



\section{Related Work}
% \input{0H-appendix}
\label{ap:related_work}
\subsection{Hierarchical Representation Learning on Graphs}
\label{subsection:related_work_hierarchical_representation}
Hierarchical representation learning is crucial for graphs as it captures multi-scale structures, enabling models to discern both local and global patterns more effectively \citep{grattarola2022understanding}.
Existing methods can be divided into three categories: layer-wise hierarchical methods, architecture-level methods, and supergraph-based methods. Layer-wise hierarchical methods, such as DiffPool \citep{ying2018hierarchical}, JK-Nets \citep{xu2018representation}, Top-K Pooling \citep{lee2019self}, ASAP \citep{ranjan2020asap}, HGP-SL \citep{zhang2019hierarchical}, and MixHop \citep{abu2019mixhop},  primarily capture graph structures by clustering nodes or using adjacency matrix powers, but they struggle to balance local and global structures, limiting their ability to capture multi-level features. 
Additionally, architecture-level methods, such as FraGAT \citep{zhang2021fragat}, MGSSL \citep{zhang2021motif}, and UniCorn \citep{feng2024unicorn} capture multi-scale features but struggle with task-specific structural changes and often lack generalization, particularly for new tasks. Supergraph methods, like HiMol \citep{zang2023hierarchical}, offer hierarchical representation, but they are hindered by high computational complexity and insufficient sensitivity to structural diversity.
To solve this issue, HiPM \citep{kang2024adapting} introduces a prompt tree to model relationships between tasks. However, this approach still requires a substantial amount of data to be effective.


\section{Discussion, Limitation and Future Work}
\label{ap:conclusion}
% \input{0G-appendix}
\paragraph{Conclusion.}
In this paper, we propose Universal Matching Network (UniMatch) to address the limitations of existing few-shot learning approaches in drug discovery. 
UniMatch leverages a dual matching mechanism that integrates hierarchical molecular matching and implicit task-level matching to capture multi-scale structural features and inter-task relationships effectively. By utilizing hierarchical pooling and matching techniques, UniMatch aligns representations across atomic, substructural, and molecular levels, preserving essential structural details that are often overlooked by single-scale methods.
Our experimental results demonstrated that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, with significant improvements in AUROC and $\Delta$AUPRC. Additionally, UniMatch showed exceptional generalization ability on the Meta-MolNet benchmark. However, our analysis revealed that the model's performance on regression tasks could be further improved by addressing specific issues in the fusion module.



\paragraph{Limitation: Simple Fusion Design.}

\label{ap:limitation}
The fusion mechanism in the proposed UniMatch model is relatively simplistic, which may limit its ability to effectively integrate information from different hierarchical levels. This design could result in suboptimal performance, as the model may not fully capture complex interactions and dependencies across multiple scales of molecular structures. More sophisticated fusion strategies, such as attention-based fusion or multi-scale feature aggregation, could enhance the model’s capability to combine features more effectively. Leveraging such advanced techniques would enable UniMatch to exploit the rich hierarchical information inherent in molecular structures, thereby improving prediction accuracy and generalization. Although these advanced fusion methods may increase computational complexity, the potential gains in model performance justify this investment.

\paragraph{Limitation: Underfitting on Regression Tasks.}
Our UniMatch model exhibits underfitting on regression tasks, indicating that it may not be capturing all the necessary features and complexities required for accurate predictions. Experimental results show that the underfitting issue arises from using a linear layer in the final fusion module. When different layer features are combined using a weighted average approach instead, the model achieves significantly better performance and converges properly. This suggests that the linear layer may not adequately capture the relationships between features from different layers for regression tasks. Therefore, replacing the linear fusion with a weighted average aggregation method could resolve the underfitting issue and allow the model to better capture complex feature relationships, thereby improving its performance on regression tasks.


\paragraph{Future Work.}
In the future, we will focus on enhancing the fusion mechanism within UniMatch to better capture the complex relationships between features from different hierarchical levels. Specifically, we will explore advanced fusion techniques such as attention-based fusion and multi-scale feature aggregation to replace the current simplistic linear approach. Additionally, we plan to conduct more extensive experiments on a wider range of datasets and tasks to ensure the robustness and generalizability of our model. Another promising direction is to integrate domain-specific knowledge and features into the model to further improve its predictive accuracy. 
As part of our ongoing efforts, we also plan to further investigate the interpretability of UniMatch by incorporating gradient-based methods (e.g., DeepLIFT \citep{shrikumar2017learning}) and exploring the Kolmogorov-Arnold Network (KAN) \citep{liu2024kan}  to gain deeper insights into feature importance and model decision-making.
Finally, we will work on optimizing the computational efficiency and scalability of UniMatch to facilitate its application in large-scale drug discovery projects.




\end{document}
