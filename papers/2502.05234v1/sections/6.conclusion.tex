\section{Conclusion}
In this paper, we investigated the critical role of temperature in multi-sample aggregation strategies. We observed that the optimal temperature varies significantly across models due to differences in training strategies and data distributions. By analyzing the relationship between training-testing distribution similarity and the optimal temperature range, we identify a strong correlation that provides valuable insights into model behavior. Furthermore, we proposed the first method for automatically predicting optimal temperatures across diverse tasks, achieving this without labeled data. Our findings contribute to a deeper understanding of temperature's impact on language model performance and offer a practical approach for optimizing inference settings.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.