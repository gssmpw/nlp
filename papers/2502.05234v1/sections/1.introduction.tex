\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/intro_v2.png}
    \vspace{-7mm}
    \caption{\textbf{(a) The entropy turning point (EntP)} is defined as the point where the log-scale of token-level entropy shifts from concave to convex. \textbf{(b)} The accuracy tested in EntP is highly correlated with the best accuracy from grid search.}
    \label{fig: temp_in_intro}
    \vspace{-3.5mm}
\end{figure}
\section{Introduction}
Large language models (LLMs) have demonstrated remarkable capabilities across various domains, including question answering~\cite{kamalloo2023evaluating}, intelligent agents~\cite{wang2024survey, zhang2023building}, scientific discovery~\cite{ma2024llm, romera2024mathematical}, and mathematical reasoning~\cite{ahn2024large, sun2024easy, lin2024lean, wu2024inference}. A fundamental research question in generative models is how to effectively sample solutions from a learned distribution and perform inference-time reasoning.

Recently, multi-sample aggregation strategies have gained increasing attention. These strategies involve generating multiple solutions from the underlying distribution and aggregating them into a final prediction~\cite{wei2022chain, yao2024tree}. Common aggregation techniques, such as majority voting, weighted majority voting, and best-of-N selection, have demonstrated significant performance improvements in benchmark evaluations of LLMs~\cite{welleck2024decoding, wang2024planning}.

Despite the promising success of multi-sample aggregation strategies, there remains a lack of deep understanding regarding how to optimize the sampling process to enhance LLM performance under different conditions, including variations in training datasets, task types, and model sizes. A crucial open question is how to tune temperature, a key hyperparameter that controls the smoothness of the system-learned distribution. Intuitively, increasing the temperature leads to a smoother distribution, enhancing the diversity of sampled outputs. However, excessively high temperatures can introduce many low-quality samples, making aggregation more challenging~\cite{holtzman2019curious, renze2024effect}. Conversely, lowering the temperature results in a highly concentrated distribution, reducing diversity and potentially omitting high-quality samples. Striking the right balance between over-sampling and under-sampling is therefore essential for optimizing LLM performance.

A common practice in prior evaluations is to use the same temperature across all methods despite variations in training datasets, task types, model sizes, and aggregation strategies. This is clearly suboptimal. An alternative approach is to empirically tune the temperature using labeled validation data for each task, dataset, model size, and aggregation strategy~\cite{zhang2024scaling,dhuliawala2024adaptive}. However, such a process is tedious and time-consuming and heavily dependent on the availability of labeled validation data, limiting its applicability when such data are scarce.

In this paper, we present the first systematic investigation of how temperature affects LLM performance under multi-sample aggregation strategies across various conditions. Furthermore, we propose a principled algorithmic solution for automated temperature optimization without requiring labeled validation data. Our key idea is as follows:
\begin{enumerate}
    \item We use the confidence score of each model as a self-assessment measure.
    \item If this self-assessment measure is highly correlated with model accuracy on test data, it can serve as a surrogate metric for tuning temperature in the absence of labeled validation data.
\end{enumerate}
A surprising finding from our temperature tuning experiments is the discovery of a phenomenon we term the \emph{entropy turning point (EntP)} in the self-assessed performance curve. As illustrated in Figure~\ref{fig: temp_in_intro}(a), the token-level entropy (y-axis) of an LLM varies with temperature values (x-axis), shown by the blue curve, while its log-scale representation appears as the red curve. Notably, there is a transition point (EntP) where the red curve shifts from concave to convex. Figure~\ref{fig: temp_in_intro}(b)
%reveals that EntP is highly correlated with the best test-set accuracy of an LLM in response to temperature tuning. 
shows that the accuracy scores at EntP for a set of LLMs are strongly correlated with their highest accuracy scores obtained through grid-based temperature tuning.
This finding supports our intuition that EntP can be leveraged to automatically determine the optimal temperature for each LLM using multi-sample aggregation strategies.
We introduce \textsc{TURN}, our proposed approach for automated temperature optimization. Through extensive experiments, TURN has demonstrated strong generalizability across diverse tasks (e.g., mathematical problem-solving, code generation), model sizes, and aggregation strategies (e.g., majority voting, best-of-N). It consistently outperforms baseline methods using a fixed temperature, yielding significant performance improvements. Additionally, our approach enhances the interpretability of temperatureâ€™s role in model performance by analyzing EntP.  Moreover, our analysis explores how the optimal temperature is influenced by the divergence or similarity between model training and tasks (Section~\ref{sec: 3}).

In summary, TURN provides a novel, efficient, and principled method for optimizing temperature in LLM inference with multi-sample aggregation. It eliminates the need for labeled validation data and significantly improves performance across a wide range of applications.
