\section{Evaluating \textsc{TURN}}
%\input{tables/5.accuracy}
\input{tables/1.math_hit_rate}
\input{tables/2.compared_to_fix_temperature}
We want to answer the following research questions about our approach \textsc{TURN} for selecting the optimal temperature:
\begin{itemize}
    \item \textbf{RQ1}: How is the accuracy of TURN in automatic temperature prediction?
    \item \textbf{RQ2}: How efficient is TURN regarding the number of samples (the parameter $N$ in Algo.~\ref{alg:auto find})?
\end{itemize}
Through experiments, \textsc{TURN} proves effective across models, aggregation strategies, and tasks while remaining efficient, requiring only a few samples for temperature prediction.
\subsection{Experiment Setup}
We evaluate our methods in two scenarios where sampling-based inference is widely used: \emph{Math Problem Solving with Majority Voting} and \emph{Code Generation with Best-of-N}. The datasets and models are as follows:

\textbf{Math Problem Solving:}\ We assess language models’ reasoning abilities using the MATH dataset~\cite{hendrycks2021measuring}, which consists of competition-level math problems. To accommodate multiple models, we randomly select 200 test problems (40 per difficulty level). Accuracy is measured based on majority voting. We test general-purpose models (Llama~\cite{dubey2024llama}, Mistral~\cite{jiang2023mistral}), domain-specific models (Llemma~\cite{azerbayev2023llemma}, OpenMath2~\cite{toshniwal2024openmathinstruct}, Deepseek-Math~\cite{shao2024deepseekmath}), and fine-tuned models (Math-Shepherd~\cite{wang2024math}, Easy-to-Hard~\cite{sun2024easy}).

\textbf{Code Generation:}\ For code generation, we use the MBPP dataset~\cite{austin2021program}, selecting the first 100 programming problems. Accuracy is measured using pass@K, where correctness is determined by passing provided unit tests. We regard the unit tests as the best-of-N strategy with a perfect reward model to rank answers. Besides general-purpose models, we evaluate code-specific models, including Deepseek-Coder~\cite{guo2024deepseek}, CodeLlama~\cite{roziere2023code}, Qwen2.5-Coder~\cite{hui2024qwen2}, and Yi-coder~\cite{yicoder}.


\textbf{Implement Details:}\ For both tasks, we sample 256 times per question at each temperature level and compute accuracy across different sampling sizes. For temperature prediction in \textsc{TURN}, we use an interval of \( t=0.1 \) and set \( N = 8 \times \text{dataset size} \) (an excessive sample size, see Section~\ref{sec: sample efficiency} for discussion). Additional inference configurations are detailed in Appendix~\ref{app:inference_config}.

\input{tables/4.variance}
\subsection{Evaluation Metrics}

To assess the performance of our algorithm for automatically selecting the optimal sampling temperature, we define the following key metrics (all the metrics are calculated under a large sample size of 128, refer to Section~\ref{sec: temperature varies} for discussion):

\textbf{Metrics:} We use the following metrics to evaluate the accuracy and reliability of our temperature prediction algorithm:
\begin{itemize}
\item{\textbf{Hit Rate (HR):}} The frequency with which \textsc{TURN} selects a temperature within the \emph{\(\epsilon\)-optimal range}\footnote{Defined in Section~\ref{sec: temperature varies}.}, indicating practical reliability.  
\item{\textbf{Temperature Gap (TG):}} The absolute difference between the predicted temperature and the nearest boundary of the \emph{\(\epsilon\)-optimal temperature range}.
\item{\textbf{Performance Drop (PD):}} The accuracy loss compared to the best temperature found via grid search.
\end{itemize}

\subsection{Baseline}
As no existing method automatically adjusts temperatures in multi-sample aggregation strategies, we compare against a \textbf{fixed temperature} baseline. We search over \(\{0.1, 0.3, 0.5, 0.7, 0.9, 1.1\}\) and select the temperature that maximizes overall accuracy. This mimics a common yet suboptimal practice where developers apply a single temperature across all models, disregarding variations in model behavior and task requirements.  

\subsection{Results}  
We evaluated 13 models on two tasks—MATH (with majority voting) and MBPP (with Best-of-N)—and present the results in Table~\ref{table: hit rate}. Recall Figure~\ref{fig: temp_in_intro}(b), the \emph{correlation coefficient} between the accuracy of the predicted temperature and the best accuracy from grid search is $0.9998$ for MATH (and $0.9913$ for MBPP). \textsc{TURN} achieves a Hit Rate of \(12/13\) on MATH and \(11/13\) on MBPP, indicating strong performance across most models. The Temperature Gap remains minimal even when the predicted temperature falls outside the $\epsilon$-optimal range (0.023 for MATH and 0.015 for MBPP). Compared to the best temperatures found via grid search, \textsc{TURN} incurs only a small average performance drop \((0.32\%\) and \(0.59\%\), respectively). Full per-model results and predicted turning points are provided in Appendix~\ref{app: results}.

\textbf{Comparison with Fixed Temperatures:} 
We next compare \textsc{TURN} to a fixed temperature baseline. Specifically, we sample temperatures from 0.1 to 1.1 at intervals of 0.2 and report the \emph{Temperature Gap (TG)} and \emph{Performance Drop (PD)} in Table~\ref{tab:temperature_comparison}. Our method outperforms the best of fixed temperatures by 0.5\% on MATH and 0.4\% on MBPP in average accuracy. When both tasks are combined, the margin increases to 0.75\%, highlighting the benefit of adaptive temperature selection over a uniform fixed temperature.

\textbf{Number of Samples for Temperature Estimation:}
\label{sec: sample efficiency}
Finally, we assess the efficiency of \textsc{TURN} by examining the prediction variance under different sample sizes for Llama-3.1-8B-Instruct on MATH. As shown in Table~\ref{tab: sample variance}, we report the average variance of the entropy curve across all choices of \(T\), the variance of predicted temperature, and the average performance drop. We find out that even with a moderate sample size (e.g. $40$ samples), the variance remains low and the performance drop is tiny (0.2\%), suggesting that a small sampling budget is sufficient for accurate temperature estimation and thus proves the efficiency of our algorithm.

%\weihua{Why sample size = 128, calculating the optimal temperature range overlap}

%\paragraph{Instruction Following}: Instruction following tasks measure how well language models can follow and execute given instructions. We use the InstructGPT dataset~\cite{ouyang2022training}, which includes a variety of tasks such as summarization, translation, and question-answering. The evaluation metric is the average score given by human annotators based on the correctness and relevance of the generated outputs.