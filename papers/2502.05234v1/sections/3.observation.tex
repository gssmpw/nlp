\begin{figure}[ht]
    \centering
    % Top sub-figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\textwidth]{figs/correlation_math_v2.png}
    \end{subfigure}
    \vskip -1em  % vertical space
    % Bottom sub-figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\textwidth]{figs/correlation_code_v2.png}
    \end{subfigure}
    \vskip -2em
    \caption{Plot of midpoints of optimal temperature ranges (x-axis, sample size 128) vs. distances between models and tasks (y-axis). A strong negative correlation is observed on the MATH and MBPP datasets, with correlation coefficients of -0.895 and -0.777.}
    \label{fig: correlation code generation}
    \vspace{-8mm}
\end{figure}
\section{Correlation Between Model Training and Optimal Temperature}
\label{sec: 3}

Multi-sample aggregation strategies—commonly used in problem-solving, code generation, and related domains—leverage information from multiple samples, which helps escape local minima and improve robustness. In these settings, \emph{sample diversity} becomes crucial: a diverse set of candidate samples increases the likelihood that the correct solution appears in the pool, rather than repeating the same mistake. The \emph{temperature} parameter is a primary lever for controlling this diversity.

We hypothesize that how a model is trained impacts the optimal temperature for multi-sample inference strategies. In particular, a more specialized or fine-tuned model can safely explore higher temperatures without drifting into low-quality outputs. In contrast, a general-purpose model typically benefits from a lower temperature to remain focused on relevant content.

We investigate this in two steps: In Section~\ref{sec: temperature varies}, we show that the optimal temperature varies for a base, instruction-tuned, and fine-tuned model. Then in Section~\ref{sec: temperature correlation}, we establish a general relationship between a model’s proximity to the target task and its corresponding optimal temperature. Our key insight is that token-level entropy is a proxy of distance from a task, which motivates our entropy-based method for automatic temperature selection in Section~\ref{sec: 4}.

%Our empirical studies reveal that different language models under various training stages respond differently to performance changes in temperature. In particular, a strong relationship exists between how closely a model’s training data aligns with the target task and the temperature that yields optimal performance. Intuitively, a more specialized or fine-tuned model can safely explore higher temperatures without drifting into low-quality outputs. In contrast, a general-purpose model typically benefits from a lower temperature to remain focused on relevant content. This observation underpins our hypothesis that the more closely a model’s training data aligns with a target task, the higher the temperature at which sampling-based strategies will excel.

\subsection{Optimal Temperature Range Varies}
\label{sec: temperature varies}

%First, we show that the best temperature varies based on whether a model is a base model, an instruction-tuned model, or a model fine-tuned specifically for the target task.

%To do so, we evaluate the accuracy of each model using multi-sample aggregation with a variety of temperatures. As we see in the curve in Figure .. the best temperature varies by model. For example, the best temperature for the base model is 0.5 when 128 samples are used, compared to 1.0 for the task-specific model.

%We also make two observations that we rely on in the subsequent experiments. First, as shown in the heatmap, several temperatures may be “optimal”, in that there is a very small performance gap. Hence we consider an optimal temperature window (define).

%Second, the optimal temperature varies based on the number of samples. However, we noticed that the optimal temperature stabilizes after a sufficient number of samples (here 32). Hence we will focus on the sufficient sample setting (namely 128) in the rest of the paper.

We first demonstrate that the optimal sampling temperature varies by model type. We test three \emph{Mistral-7B} variants: the \emph{pretrained base model}, the \emph{instruction-finetuned version (Mistral-7B-Instruct)}, and a \emph{task-finetuned model for MATH}\footnote{Model link: \href{https://huggingface.co/peiyi9979/mistral-7b-sft}{https://huggingface.co/peiyi9979/mistral-7b-sft}}~\cite{wang2024math}. Each model is evaluated using multi-sample aggregation across different temperatures.
Figure~\ref{fig: teaser}(a) presents the accuracy heatmap for the Mistral-7B-Instruct model on the MATH dataset. At smaller sample sizes, lower temperatures tend to produce better accuracy. However, higher temperatures can yield better results as the sample size increases. For a fixed sample size, the accuracy curve follows a single-peak pattern: it rises as temperature increases and peaks, and then gradually declines, staying relatively steady near the peak.

Since the single-peak behavior, we define the \textbf{$\epsilon$-optimal temperature range}. This range encompasses temperatures $T$ where the accuracy $A(T)$ is no less than $A(T^*) - \epsilon$, with $A(T^*)$ representing the peak accuracy. Given the curve's single-peak nature, this range forms an interval around $T^*$. For our analysis, we set $\epsilon = 0.02$, effectively capturing the temperatures close to the peak where the accuracy remains relatively high.

We then plot the midpoint of this optimal temperature range for each model variant and various sample sizes (Figure~\ref{fig: teaser}(b)). We observe that the pretrained model has the lowest midpoint, the instruction-finetuned model has a higher midpoint, and the task-finetuned model has the highest. Another observation is that optimal temperature ranges change slowly once beyond a sample size of 32. Therefore, we choose a sample size of 128 in our following experiments to ensure stable performance in the rich-sample setting.

From these observations, we hypothesize a general relationship between how closely a model is tuned to a particular task and the temperature that yields the best accuracy. We discuss this hypothesis further in the next section.

\subsection{Correlation Between Training-Task Similarity and Optimal Temperature}
\label{sec: temperature correlation}
Our goal is to establish a general relationship between a model’s learned distribution and its optimal temperature for a task. Our key intuition is that token-level entropy can serve as a surrogate for a model's `distance' from a target task and that this distance helps identify the optimal temperature.

Specifically, we define a distance metric that measures how similar a model’s training data is to a given task. Let \(\mathcal{T} = \{X_1, ..., X_k\}\) be the task with $k$ problem instances. We define this distance \(\mathcal{D}(\mathcal{M}, \mathcal{T})\) as the average of token-level entropy \(\mathcal{H}(.)\) of the language model \(\mathcal{M}\) when generating the answers \(\mathcal{A} = \{Y_1, ..., Y_k\}\) for the problems in \(\mathcal{T}\):
\begin{align}
\mathcal{D}(\mathcal{M}, \mathcal{T})
&=
\frac{1}{k}
\sum_{i=1}^{k}
\left[
  \frac{1}{|Y_i|}
  \sum_{j=1}^{|Y_i|}
  \mathcal{H}\bigl(p_\mathcal{M}\bigl(\cdot \mid X_i,\,Y_{i,<j}\bigr)\bigr)
\right],
\end{align}
\vspace{-5mm}
where
\begin{align}
\label{eq: entropy}
\mathcal{H}(p)&=
  -\sum_{v \in p} 
     p\bigl(v) 
     \log p\bigl(v\bigr).
\end{align}
To avoid bias toward ground-truth references, we use model-generated sequences \(\{Y_i\}\) instead of official gold solutions. Meanwhile, the distance is measured at a low temperature $T=0.5$ to ensure the generation stability.

We evaluated several language models on the MATH and MBPP datasets, including pretrained, instruction-finetuned, and task-finetuned models. Figure~\ref{fig: correlation code generation} plots the midpoint of the optimal temperature range against our distance metric, demonstrating a strong negative correlation. Specifically, across our model set, the correlation on MATH is \(-0.895\), while on MBPP it is \(-0.777\).

In practice, this suggests using a higher temperature (e.g., \(T = 0.9 \sim 1.1\)) for task-finetuned models and a lower temperature (e.g., \(T = 0.5 \sim 0.7\)) for more general-purpose models (pretrained or instruction-finetuned).

\iffalse
\subsection{Link to Token Probability Distribution}

Given the numerous training methods available for language models, the training data and target tasks can vary significantly. During inference, the distribution of token probabilities also differs. As shown in Figure~\ref{fig: teaser}~(a), we applied three types of language models to the MATH dataset and measured the average probability of the top 20 tokens, each of the models has a different purpose (a pretrained model, an instruction-finetuned model, and a task-finetuned model). For the pretrained model, the token probability distribution is relatively flat compared with the instruction-finetuned model, which has been more specialized for question answering. As expected, the token probability distribution becomes even more concentrated when a model is specifically fine-tuned on the task’s training set.

Differences in token probability distributions also affect performance in sample-based inference strategies. Intuitively, a flat token probability distribution model may need a lower temperature to focus on the most relevant tokens. In contrast, a model with a concentrated token probability distribution may require a higher temperature to explore more diverse tokens. As shown in Figure~\ref{fig: teaser}~(b), we evaluated the three models under various sampling sizes and temperatures and then visualized the results in a heatmap. These results indicate that the optimal temperature for the pretrained model is lower than for the instruction-finetuned model, while the task-finetuned model benefits from the highest temperature.
\fi