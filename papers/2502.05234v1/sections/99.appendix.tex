\section{Inference Configuration} \label{app:inference_config}

\subsection{Software} 
Our experiments build upon two open-source projects: \emph{Easy-to-Hard Generalization}~\cite{sun2024easy} for the MATH dataset and \emph{bigcode-evaluation-harness}~\cite{bigcode-evaluation-harness} for the MBPP dataset. We employ \emph{vLLM}~\cite{kwon2023efficient} to accelerate inference. All experiments can be reproduced on a single L40S or A6000 GPU.

\subsection{Sampling Parameters}
We use zero-shot inference for models fine-tuned specifically for each dataset. For general-purpose models, we use four in-context examples (few-shot inference) to ensure correct output formatting. The maximum output length is set to 1024 tokens for all tasks. For the MATH dataset, we use top-k sampling with $k = 20$. No additional sampling constraints are imposed for the MBPP dataset.

\subsection{Metric Calculation}
To compute majority-vote results for the MATH dataset, we consider two samples to have the same answer if they match after normalization. For the pass@K metric, we follow the definition in~\citet{chen2021evaluating}. Let $N$ be the total number of samples and $C$ be the number of correct samples. Then \(\mathrm{pass}@K\) is defined as:
\begin{align}
\mathrm{pass}@K = 1 - \frac{\binom{N - C}{K}}{\binom{N}{K}}.
\end{align}
\section{Details of the Stochastic Process Model}
\label{app: toy model}
We introduce a stochastic process model to explain that (1) the token-level entropy increases steadily at the beginning but rises rapidly when the sampling temperature reaches a certain threshold. (2) The optimal temperature is near the turning point when using multi-sample aggregation strategies.

The stochastic process model has two underlying assumptions: (1) Every token can be labeled as `proper' or `improper' at each decoding step. Generally, proper tokens have relatively higher logits than improper tokens, while the number of improper tokens is much higher than that of proper tokens. (2) When an improper token is generated, improper tokens have a higher generation probability in the next step, and vice versa.

Under these two assumptions, we can calculate the token-level entropy under different sampling temperatures, and the temperature-entropy curve fits that of real language models. Meanwhile, the percentage of improper tokens quickly increases after the turning point, implying a quick drop in sample quality in real language models.

\subsection{Model Setup}
\subsubsection{Initial Conditions}
We consider a discrete-time process \(\{x_t\}_{t=0}^{K}\) where each \(x_t \in [0,1]\) represents the model’s probability of producing an improper token at time step \(t\). We start with an initial error rate:
\[
x_0 = x_{\text{init}} \in [0,1].
\]
Conceptually, \(x_{\text{init}}\) corresponds to the model’s baseline `error propensity' at the start. This value is related to the sampling temperature \(T\) of the language model: higher \(T\) typically yields a flatter probability distribution over tokens, increasing the chance of selecting an improper token and thus increasing \(x_{\text{init}}\). (See Section \ref{sec: temp_to_init} for a heuristic link between temperature and initial error rate.)

\subsubsection{Interpreting the Error Rate}
At each step \(t\), the language model chooses a single token, and each token is classified as proper or improper. Although in practice, the correctness of a token depends on the context and is not truly binary, we approximate this by treating correctness as a Bernoulli trial:
\begin{itemize}
   \item Probability of producing an improper token: \(x_t\);
   \item Probability of producing a proper token: \(1 - x_t\).
\end{itemize}
Define a random variable \(E_t\) that indicates whether an error occurred at time \(t\):
   \[
   E_t = \begin{cases} 
   1 & \text{with probability } x_t, \text{ (improper token)},\\
   0 & \text{with probability } 1 - x_t, \text{ (proper token)}.
   \end{cases}
   \]

\subsubsection{Error Rate Update Rules}
   After each step, the error rate \(x_{t+1}\) is updated based on whether the token at time \(t\) was improper or proper:

   \paragraph{If an error occurs (\(E_t = 1\)):}
   The error rate is increased. Intuitively, making a mistake can make the model more likely to continue making errors. Formally, we update:
   \[
   x_{t+1} = 1 - (1 - x_t)^\alpha.
   \]
   For \(x_t \in [0,1]\) and $\alpha > 1$ ($\alpha$ is a hyperparameter), it can be seen that \(x_{t+1} \ge x_t\). Here $\alpha$ can be regarded as the noise tolerance rate, measuring how stable the model is when suffering from unexpected noises, and we will try different $\alpha$ in experiments.

   \paragraph{If a proper token is produced (\(E_t = 0\)):}
   The error rate is reduced, reflecting a `reinforcement' of correct behavior. We do this by:
   \[
   x_{t+1} = \max(x_t^\alpha,\ x_{\text{init}}).
   \]
   It generally makes it smaller, so this step lowers the error rate. However, we do not allow the error rate to drop below the initial baseline \(x_{\text{init}}\).

\subsubsection{Linking Initial Error Rate and Temperature}
\label{sec: temp_to_init}
   At step $t$, the model’s token probability mass is divided into:
   \begin{itemize}
       \item \textbf{Improper tokens} with total probability \(P_{1, \text{improper}} = x_t\).
       \item \textbf{Proper tokens} with total probability \(P_{0, \text{proper}} = 1 - x_t\).
   \end{itemize}
Therefore, by definition:
   \[
   x_{\text{init}} = P_{1, \text{improper}}.
   \]
Under higher temperatures \(T\), the softmax distribution flattens, increasing \(P_{1,\text{improper}}\) because the number of improper tokens is large but their logits are low. Thus, $x_\text{init}$ increases as \(T\) increases.

\subsubsection{Type of Tokens during decoding}

The probability of tokens when decoding is usually multi-peak (i.e., except for the token with the highest logit, some other tokens have reasonably high logits and are also acceptable during decoding), so it is natural to consider a scenario with three categories of tokens:
   \begin{itemize}
       \item \textbf{Proper tokens:} A small number of tokens with high logits. Let $N_0$ be the number of proper tokens. To capture the multi-peak behavior, the logits of proper tokens $l_{0,1}, ..., l_{0, N_0}$are sampled from the Gaussian distribution $\mathcal{N}(L_0, \sigma_0)$.
       \item \textbf{Low Probability Improper tokens:} Many low-logit tokens where language models seldom choose them. Let $N_1$ be the number of tokens in this type and their logits are set to $L_1$ for simplicity.
       \item \textbf{High Probability Improper tokens:} Due to insufficient training or errors in training data, some tokens may have exceptionally high logits but are logically improper in (\textit{e.g., } the token $3$ in $1 + 1 = 3$). Since different language models behave differently and this type of token will be selected regardless of temperature, we only consider decoding without high-probability improper tokens in our discussion.
   \end{itemize}
For the first two types of tokens, we have:
   \[
    L_{0} > L_{1}, \ \ N_0 \ll N_1.
   \]
\subsubsection{Token Probability During Sampling}

At each step \(t\), the probability of producing improper tokens is \(x_t\). Let $p_{t, \text{proper/improper}, j}$ be the probability of the $j$-th proper/improper tokens at step $t$. For the improper tokens, we have:
\[
p_{t,\text{improper}, j} = \frac{x_t}{N_1},\ \ \forall j \in [1, N_1].
\]
For the proper tokens, we allocate the remaining probability \(1 - x_{t}\) according to their relative logits:
\[
p_{t,\text{proper}, j} = (1 - x_{t}) \, \text{softmax}(l_{0,1}, ..., l_{0, N_0})_j, \ \ \forall j \in [1, N_0].
\]
This ensures that the relative ordering of probabilities for the proper tokens remains determined by their logits, while the total mass allocated to improper tokens is \(x_t\).

\subsubsection{Entropy Calculation}

We define the token-level entropy \(\mathcal{H}\) over a sequence of decoding steps:
\[
\mathcal{H} = \frac{1}{K} \sum_{t=1}^{K} \left( -\sum_{\text{j}}^{N_0} p_{t,\text{proper}, j} \log p_{t,\text{proper}, j} -\sum_{\text{j}}^{N_1} p_{t,\text{improper}, j} \log p_{t,\text{improper}, j} \right)\]
Here, $K$ is the total number of decoding steps considered.

\subsection{Experiment}

\subsubsection{Model Hyperparameter}

The toy model has the following hyperparameters:
\begin{itemize}
    \item The numbers of proper and improper tokens: $N_0, N_1$;
    \item The logits of proper and improper tokens: $l_{0,\{1, ..., N_0\}}, l_{1,\{1, ..., N_1\}}$, where:
\[l_{0, i} \sim \mathcal{N}(L_0, \sigma_0), \ \ l_{1, i} = L_1.\]
    \item The number of total steps $K$.
    \item The noise tolerance rate $\alpha$.
\end{itemize}
The input is temperature $(T)$ and the output is the average token-level entropy $(E)$ over 500 samples.
In our experiment, we set the hyperparameters to be:
\begin{align*}
N_0=10, \ \ N_1 = 30000, \ \ L_0 = 0,\ \  L_1 = -10,\ \ \sigma_0 = 1,\ \ K=512.
\end{align*}
We test different noise tolerance rates $\alpha \in [1.5, 2.0, 2.5, 3.0]$ to show behaviors under different noise tolerance rates.

\subsubsection{Result}

Here is the temperature-entropy curve derived from the toy model under different noise tolerance rates $\alpha$ (Figure~\ref{fig:toy_curves}):
\begin{figure}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.6\textwidth]{figs/simple_model_all.png}
        \caption{The temperature-entropy curves.}
        \label{fig:first_image}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.6\textwidth]{figs/simple_model_all_log.png}
        \caption{The temperature-log entropy curves.}
        \label{fig:second_image}
    \end{subfigure}
    \caption{The curves derived from the stochastic process model under different $\alpha$.}
    \label{fig:toy_curves}
\end{figure}
\begin{figure}[ht]
    \vspace{-3mm}
    \centering
    \includegraphics[width=0.6\textwidth]{figs/simple_model_all_correct_rate.png}
    \caption{The Temperature-Improper Token (\%) curves.}
    \label{fig:correct rate}
\end{figure}
\begin{figure}[ht]
\vspace{-3mm}
\centering
\includegraphics[width=0.6\textwidth]{figs/entropy.png}
\vspace{-3mm}
\caption{The temperature-entropy curves.}
\label{fig:curves}
\end{figure}
\begin{figure}[h!]
\vspace{-3mm}
\centering
\includegraphics[width=0.6\textwidth]{figs/log_entropy.png}
\vspace{-3mm}
\caption{The temperature-log entropy curves.}
\label{fig:real_curves}
\vspace{-3mm}
\end{figure}
The curves under different noise tolerances have a similar shape. Generally, the curve can be divided into two parts: \textbf{(sub)-linear} increase and \textbf{(super)-exponential} increase. In the first part, the increase is due to the various choices among the proper tokens, while the sharp rise in the second part is due to the loss of control (i.e., the model frequently chooses nonsense tokens).

The curve is very similar to the behavior of real language models, and some reference entropy curves and log-entropy curves are shown in Figure~\ref{fig:curves} and~\ref{fig:real_curves}.

\paragraph{Relation to Improper Token Rate} It is natural to consider proper tokens can lead to correct answers and the improper tokens will result in incorrect answers, so we measure the percentage of improper tokens. As shown in Figure~\ref{fig:correct rate}, when the temperature exceeds the turning point, the percentage of improper tokens increases quickly, implying a quality drop in samples. Interestingly, the difference in noise tolerance rates has little inference on the turning point but controls the improper token increasing speed after the turning point. Nevertheless, the percentage of improper tokens increases quickly under all tested $\alpha$.
%Fig. \ref{fig:correct rate} is the temperature-correct rate curve, and the big drop in the correct rate happens at the same point as the entropy begins to increase.

\subsection{Conclusion}
Our toy model provides a simplified yet insightful framework for understanding how temperature-dependent sampling dynamics may give rise to characteristic shifts in the model’s output distribution. The model predominantly chooses from the proper tokens in the low-temperature (or sub-linear growth) regime, resulting in relatively stable and controlled outputs. The distribution flattens and nonsense tokens gain significant probability mass as temperature increases beyond a certain threshold. This transition leads to a sudden and steep increase in entropy—mirroring observations in actual large language models—and a corresponding drop in the correct rate. Therefore, increasing the temperature can initially increase generation diversity (sampling among proper tokens) with a small correctness drop. However, its performance suffers a quick drop after reaching a certain threshold (i.e., the turning point).

\section{Aggregation Adaptation Calculation}
\label{app: bias}
The choice of aggregation function affects the optimal generation temperature. For example, in majority voting, the final answer must be selected by the majority, whereas in best-of-N, only a single correct instance out of the N samples is required.

In the case of majority voting, the turning point on the entropy curve aligns with the optimal temperature, so we set its adaptation to 0. For best-of-N, we computed an adaptation on MATH and then tested it on MBPP to confirm generality. Specifically, we averaged the difference between the midpoints of the optimal temperature ranges for best-of-N and majority voting across 13 models on MATH. This difference was $0.092$ on average. Therefore, for simplicity, we set the aggregation adaptation for best-of-N to $0.1$.

\begin{table}[ht]
\centering
\caption{\textbf{Aggregation Adaptation for Best-of-N}: we calculate midpoints of optimal temperature ranges on Majority Voting and Best-of-N for MATH. The difference between the average of midpoints is $0.092$, so we set the adaptation factor to $0.1$.}
\begin{tabular}{c|ccccccccccccc|c}
\toprule
Aggregation & \multicolumn{13}{c|}{Individual Models (Models are listed in the same order as Table~\ref{table: hit rate})} & Average \\
\hline
Best-of-N & 0.6 & 0.8 & 0.6 & 0.6 & 0.7 & 0.5 & 0.6 & 1.1 & 1.2 & 0.5 & 0.6 & 1.3 & 1.0 & 0.7769\\
Majority Voting & 0.6 & 0.9 & 0.6 & 0.5 & 0.3 & 0.6 & 0.5 & 1.1 & 0.9 & 0.5 & 0.6 & 1.0 & 0.8 & 0.6846\\
\bottomrule
\end{tabular}
\label{tab:averages}
\end{table}

\section{Results of All Tested Models}

We present the accuracy heatmaps and entropy estimations for all tested models. Figure~\ref{fig: heatmap MATH} shows the heatmaps of model accuracy for the MATH dataset, while Figure~\ref{fig: heatmap MBPP} displays the heatmaps for the MBPP dataset. Additionally, Figure~\ref{fig: curve_math} illustrates the entropy curve estimations for the MATH dataset, and Figure~\ref{fig: curve_mbpp} provides the entropy curve estimations for the MBPP dataset.

\label{app: results}
\begin{figure*}[ht]
    \center
\includegraphics[width=0.95\textwidth]{figs/merged_heatmap_MATH.png}
    \caption{The accuracy heatmap for all tested models on the MATH dataset. The green line is our predicted temperature.}
    \label{fig: heatmap MATH}
\end{figure*}
\begin{figure*}[ht]
    \center
    \includegraphics[width=0.95\textwidth]{figs/merged_heatmap_MBPP.png}
    \caption{The accuracy heatmap for all tested models on the MBPP dataset. The green line is our predicted temperature.}
    \label{fig: heatmap MBPP}
\end{figure*}
\begin{figure*}[ht]
    \center
\includegraphics[width=0.95\textwidth]{figs/curve_math.png}
    \caption{The entropy curves and turning points of language models when testing on the MATH dataset.}
    \label{fig: curve_math}
\end{figure*}
\begin{figure*}[ht]
    \center
    \includegraphics[width=0.95\textwidth]{figs/curve_code.png}
    \caption{The entropy curves and turning points of language models when testing on the MBPP dataset.}
    \label{fig: curve_mbpp}
\end{figure*}