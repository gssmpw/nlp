\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{figs/entropy_curve_v2.png}
\vspace{-6mm}
\caption{\textbf{Entropy Curve Characteristics.} 
\textbf{(a)} The token-level entropy \(\mathcal{H}\) (solid blue line) increases slowly at lower temperatures and then jumps sharply at a critical turning point. In contrast, the entropy for a fixed (greedy) generation stays low (dotted blue line). \(\log(\mathcal{H})\) (red line) reveals a transition from concavity to convexity that aligns with the sharp increase in \(\mathcal{H}\), marking the entropy turning point (EntP). \textbf{(b)} EntP hits the best temperature, and it varies between different models.}
\label{fig: entropy_curve}
\vspace{-5mm}
\end{figure}
\section{Entropy-Based Automatic Temperature Selection}
\label{sec: 4}
Determining an optimal sampling temperature is crucial in multi-sample aggregation strategies, yet existing approaches often rely on labeled data or tuning on a validation set. This reliance becomes problematic when no such data are available. 
In this section, we show how to leverage token-level entropy as an intrinsic property to pinpoint a suitable temperature without labeled data. We first demonstrate a spike on \emph{token-level entropy} as a signal of quality collapse in Section~\ref{sec: spike}. Then develop a method that automatically selects temperature using an \emph{entropy turning point (EntP)} derived from the spike in Section~\ref{sec: turn}. Finally, we applied a stochastic process model to explain the mechanism of our algorithm in Section~\ref{sec: toy model}.
\subsection{Entropy Spike as an Indicator of Quality Collapse}
\label{sec: spike}
First, we discover a surprising phenomenon that we call the entropy spike. Specifically, increasing the temperature smoothly increases the model’s entropy, until a dramatic spike where the entropy rapidly increases. We believe the spike is a good indicator of sample quality collapse.

As illustrated in Figure~\ref{fig: entropy_curve}(a), we calculate the token-level entropy at different temperature levels (solid blue line). To reduce computational overhead, we compute the entropy only over the top-$K$ tokens (with the highest probabilities) at each step, setting $K=1000$ in all subsequent experiments. The entropy curve remains stable for lower temperatures but then shows a sudden rise. One might attribute this behavior to temperature’s role in flattening the distribution (Equation~\ref{Formula 1}). However, the following analysis indicates that this spike reflects a substantial change in the model’s next-token distribution.

Specifically, we constrain the model to generate the same outputs produced by greedy decoding while evaluating entropy under a higher temperature (dotted blue
line). If temperature alone were responsible for the entropy spike, these fixed outputs would yield a similarly high entropy. However, as shown in Figure~\ref{fig: entropy_curve}(a), we observe a significant gap between these two entropy curves, indicating that the actual sampling distribution undergoes a large shift.

Thus, we infer that the sudden rise in the entropy curve implies a substantial drop in sample quality. Setting the temperature around this sudden rise can balance sufficient diversity without a large quality drop, which is suitable for multi-sample aggregation strategies.

\subsection{Turning Point Temperature Selection (\textsc{TURN})}
\label{sec: turn}
Given the token-level entropy curve of a language model on a specific task, how can we identify a suitable temperature for multi-sample aggregation strategies? Inspired by the difference in the shapes of the entropy curve: When the temperature remains low, the entropy increases \emph{flatly}. However, when the sampling temperature is near the spike, the entropy increases \emph{(super)-exponentially}, implying a quality drop in samples. Therefore, after taking the logarithm of the entropy curve (shown in Figure~\ref{fig: entropy_curve}(a), red line), the flat part becomes concave while the exponentially-increase part becomes convex. We define the \emph{entropy turning point (EntP)} as the temperature where the log entropy curve becomes convex. Figure~\ref{fig: entropy_curve}(b) tests the llemma-7b base model and its task-finetuned variant\footnote{Model link: \href{https://huggingface.co/ScalableMath/llemma-7b-sft-metamath-level-1to3-hf}{https://huggingface.co/ScalableMath/llemma-7b-sft-metamath-level-1to3-hf}}~\cite{sun2024easy}, and EntP matches the position with the highest accuracy and varies between different models. Based on EntP, we develop a new method for automatic temperature prediction in multi-sample aggregation strategies, called Turning Point Temperature Selection (\textsc{TURN}).

The optimal temperature should be around EntP to achieve both sample quality and diversity. At the same time, we found that some aggregation methods may be more tolerant of quality drops (e.g., for best-of-N, only one sample is enough to be correct). So we added a small adaptation factor $\beta$ based on the aggregation function, and it is set to $0$ and $+0.1$ for majority voting and best-of-N, respectively. The aggregation adaptation for best-of-N is calculated in the MATH dataset but can be directly applied to other tasks. Refer to Appendix~\ref{app: bias} for details.

Specifically, given a language model $\mathcal{M}$, a task $\mathcal{T}=\{X_1,\ldots,X_k\}$ with $k$ input instances, and an aggregation method $\mathcal{A}$.
To estimate the token-level entropy, we random sample $N$ times. In each time, we randomly choose an input instance $X_i$, and generate one sample by $\mathcal{M}$ under each candidate temperature $t_j = j \cdot t$ (with $t$ being the temperature interval and $j = 0,1,\ldots, J$, where $J=\lfloor t_{\max}/t\rfloor$). These entropies are then aggregated to calculate the average entropy $\mathcal{H}(t_j)$ at each temperature $t_j$. By taking the logarithm, we obtain $\ell(t_j) = \log \mathcal{H}(t_j)$.

Next, we identify EntP index $j^*$, where the second derivative of $\ell$ changes from negative to positive and select its corresponding temperature $j^*\cdot t$. Then we add the aggregation adaptation factor $\beta$ to form the final prediction.
%The corresponding temperature $j^* \cdot t$ is then adjusted with a bias $\beta_{\mathcal{A}}$ related to the aggregation method $\mathcal{A}$, resulting in the final prediction $t^*_{\text{biased}} = t^* + \beta_{\mathcal{A}}$. 
The pseudocode for our algorithm is listed in Algo. \ref{alg:auto find}.
\input{tables/3.algorithm}
%\weihua{The turning point of the entropy curve means the quality of samples begins to quickly drop ..., due to the initial difference in sampling strategies, best-of-N is better ..., we give an $+\epsilon$ bias, and we simply set it to 0.1 in our experiment.}
\begin{figure}[ht]
\includegraphics[width=0.48\textwidth]{figs/toy_model.png}
\vspace{-5mm}
\caption{\textbf{Stochastic Process Model.} We run our process model in the setting: $N_0=10$, $N_1=30000$, $L_0=0$, $\sigma_0=1$, $L_1=-10$, and $\alpha=2$. \textbf{(a)} The entropy curve is similar to that of the real language model: flat at first, and then sharply increases. \textbf{(b)} We calculate the relation between temperature and the percentage of improper tokens in simulation, and the percentage of improper tokens quickly increases after EntP.}
\vspace{-5mm}
\label{fig: toy_model}
\end{figure}

\subsection{A Stochastic Process Model}
\label{sec: toy model}
We applied a stochastic process model to explain why the entropy curve exhibits a sudden spike and what that spike signifies.

Because inference is sequential, when the language model makes an error (for example, by sampling an improper token), it increases the likelihood of further mistakes. Meanwhile, the model may occasionally recover and return to a correct trajectory.

To simulate this process, we adopt a stochastic process model with \(K\) steps in sequential, generating a token in each step. At the start, the model has an initial error rate \(p = p_{\text{init}}\), representing the probability of selecting an improper token. At each step, if the model selects an improper token, the likelihood of further errors increases to \(1 - (1 - p)^\alpha\), where \(\alpha > 1\) is called the noise tolerance rate. Conversely, if the model selects a proper token, the error probability decreases to \(p^\alpha\) (but cannot be smaller than \(p_\text{init}\)).

To build a bridge between the temperature \(T\) and the initial error rate \(p_{\text{init}}\), we propose an estimation. All tokens are labeled proper or improper irrelevant to contexts, and the number of improper tokens (\(N_1\)) is much larger than that of proper tokens (\(N_0\)). In the beginning, proper tokens have high logits \(L_0\) with a variance \(\mathcal{N}(0, \sigma_0^2)\) to reflect the nature that there may be several proper next tokens with similar logits. Improper tokens have uniformly low logits \(L_1\). Then, the initial error rate \(p_{\text{init}}\) is determined as the probability of selecting an improper token based on the logits and temperature. 
During inference, all improper tokens equally share the error rate \(p\), while proper tokens account for the remaining probability based on their logits.

Using this setup, we can estimate the token-level entropy. As shown in Figure~\ref{fig: toy_model}(a), the simulated entropy curve (blue line) aligns well with the observed entropy curves of a real language model (Figure~\ref{fig: entropy_curve}(1) solid blue line). Meanwhile, Figure~\ref{fig: toy_model}(b) shows the relationship between the temperature and the percentage of improper tokens, which rises quickly after EntP. This observation suggests that, before EntP, increasing the temperature can help explore the proper tokens. However, after EntP, the increase in the percentage of improper tokens makes the model uncertain and creates errors, implying a quick drop in sample quality. The behavior of the stochastic process model is consistent with our observations of language models, proving that token-level entropy is a good indicator of sample quality. Detailed formulas and experiments can be found in Appendix~\ref{app: toy model}.

\iffalse
\subsection{Balancing Diversity and Quality}
\begin{figure}[ht] 
\centering 
\includegraphics[width=0.48\textwidth]{figs/quality_diversity.png} 
\caption{\textbf{Diversity and Quality Dynamics}: Diversity and quality trends for four language models across varying temperatures, and the suitable temperature ranges are marked in brackets after LM names. The Answer Diversity demonstrates a (sub-)linear increase, whereas sample quality remains stable initially before experiencing a sharp decline. We also noticed that models specifically tuned on the dataset (i.e., (d)) may be tolerant to temperature, resulting in a large suitable temperature range.}
\label{fig: diversity and quality}
\end{figure}
Intuitively, increasing the sampling temperature enhances the diversity of generated answers but tends to reduce their quality. To quantify these two aspects, we define:

\begin{itemize}
    \item \textbf{Quality:} The average accuracy of a single sample.
    \item \textbf{Diversity:} The Shannon entropy of a set of answers. We notice that, at high temperatures, some generated answers become incomplete or invalid. Therefore, we ignore these samples and apply the Miller–Meadow correction~\cite{miller1955note} to mitigate the bias introduced by ignorance.
\end{itemize}
Figure~\ref{fig: diversity and quality} illustrates how sample quality and diversity change with temperature for four language models. Generally, quality remains relatively stable as the temperature increases, then experiences a sharp decline. In contrast, diversity typically grows approximately linearly or sub-linearly but may drop at very high temperatures, where the model seldom produces coherent outputs for answer parsing.

From these observations, one might choose a temperature that maintains a reasonable level of quality while maximizing diversity. However, such a selection relies on accessing quality metrics from a validation set, which is impossible when labeled data are unavailable.
\fi