\begin{figure*}[t]
\includegraphics[width=1\textwidth]{figs/teaser_v2.png}
\vspace{-6mm}
\caption{ \textbf{(a) Accuracy Heatmap.} Performance of Mistral-7B-Instruct-v0.3 under majority voting across different temperatures. The best temperature for each sampling size is highlighted in bold white, and the optimal temperature range is shaded white. The green line shows the temperature predicted by our method. \textbf{(b) Midpoint of Optimal Temperature Range vs. Number of Samples.} The optimal temperature range varies by model; those with training data more closely matching the task tend to favor higher temperatures.}
\label{fig: teaser}
\end{figure*}
\section{Preliminary \& Related Work}
Before moving to our main contributions, we first review how language models typically generate samples and an introduction to multi-sample aggregation strategies.
\paragraph{Language Model Sampling} Language models typically generate outputs for generative tasks by autoregressively sampling from the conditional probability distribution over the next token, given both the input context and previously generated tokens. Formally, for an input \(X\) and an output sequence \(Y = (y_1, y_2, \dots, y_N)\), the probability of producing \(Y\) is given by:
\begin{align}
P(Y \mid X) \;=\; \prod_{i=1}^{N} P\bigl(y_i \;\bigm|\; y_{<i},\, X\bigr).
\label{Formula 1}
\end{align}
To compute the probability distribution, the model obtains a set of logits $z_i$ and then divides them by a temperature hyperparameter \(T\) before applying the softmax function and a regularization function $\mathcal{F}$:
\begin{align}
P(y_i \mid y_{<i}, X) \;=\; \mathcal{F}\left(\operatorname{softmax}\Bigl(\frac{z_i}{T}\Bigr)\right),
\end{align}
where \(z_i\) is the logit corresponding to token \(y_i\). The temperature \(T\) controls how peaked or flat the resulting probability distribution will be. The regularization function $\mathcal{F}$ is used to reschedule the sampling process (e.g., Top-$k$~\cite{kool2019stochastic}, Top-$p$~\cite{holtzman2019curious}, Min-$p$~\cite{nguyen2024turning} and Locally Typical Sampling~\cite{meister2023locally}).

\paragraph{Multi-Sample Aggregation Strategy} Since different random seeds can produce varying outcomes, a common approach to mitigate sampling variance is to draw multiple samples and aggregate their results. In practice, it leads to substantial performance improvements and has been widely adopted to achieve state-of-the-art performance in math reasoning~\cite{sun2024easy, jaech2024openai}, code generation~\cite{wang2024planning}, and many other domains.

Specifically, a set of candidate outputs \(Y = \{Y_1, \dots, Y_N\}\) is generated and then aggregated into a final answer. Two standard aggregation methods are typically employed:
\begin{itemize}
    \item \textbf{Majority Voting}: The final answer is the output that appears most frequently among the candidates, i.e.,
\[
\hat{y} \;=\; \arg\max_{y \in \{Y_1, \dots, Y_N\}} \sum_{i=1}^{N} \mathbb{I}\bigl(Y_i = y\bigr),
\]
where \(\mathbb{I}(\cdot)\) is the indicator function, which returns 1 if its argument is true and 0 otherwise. This method is frequently used in where evaluating whether two outputs are equivalent is relatively easy. The method is also called self-consistency~\cite{wang2022self}.
    \item \textbf{Best-of-N}: Each sample is scored by a reward function \(G\), and the final answer is the one with the highest score:
\[
\hat{y} \;=\; \arg\max_{y \in \{Y_1, \dots, Y_N\}} G(y).
\]
The reward function \(G\) can be defined in various ways, such as a separate language model’s likelihood, or a trained or verified reward model.
\end{itemize}

%\paragraph{Deterministic Decoding} Deterministic methods, such as greedy decoding and beam search~\cite{freitag2017beam}, produce a single fixed sequence (or a set of top sequences) based on the model’s output logits. While these methods ensure locally optimal probabilities at each step, they may fail to achieve the global optimum and can sometimes yield repetitive content.

\paragraph{Choosing Temperature in Multi-Sample Aggregation} 
Despite the widespread use of multi-sample aggregation strategies in state-of-the-art systems, the question of choosing the important temperature parameter remains under-explored. 

Some studies have investigated selecting a temperature for a single-sample method~\cite{zhang2024edt, li2024dynamic, kumar2019calibration, xie2024calibrating, dhuliawala2024adaptive} or multi-sample aggregation with a validation set~\cite{zhang2024scaling}. Our method has two key differences: (1) we focus on state-of-the-art multisample aggregation strategies rather than single-sample inference, and (2) we find the optimal temperature without validation data.

%Existing research has examined the role of temperature across various scenarios~\cite{renze2024effect}, but multi-sample inference has often been overlooked. \citet{dhuliawala2024adaptive} introduced preference optimization approaches to identify the optimal temperature. \citet{zhang2024edt} proposed a rule-based adaptation method based on the entropy of previous tokens. \citet{kumar2019calibration} and \citet{xie2024calibrating} applied ad-hoc temperature predictions to calibrate language models. \citet{li2024dynamic} leveraged learned diversity values for temperature prediction in dialogue tasks. \citet{zhang2024scaling} regarded temperature selection as a budget allocation problem, employing a hill-climbing algorithm. To the best of our knowledge, we are the first comprehensive study of temperature for sampling-based inference strategies.
%\weihua{all of them require validation data, the difference in inference, single sample v.s. multi-sample, multi-sample is important. Entropy-based method, correlation, good system not all system. }
%\begin{figure*}[t]
%    \includegraphics[width=1\textwidth]{figs/similarity.png}  
%    \caption{\textbf{(a) Accuracy Heatmap:} The performance of three types of language models (pretrained, instruction-finetuned, and task-finetuned) with majority voting on various temperature levels. The best temperature under each sampling size is turned white bold and the optimal temperature range is turned white. \textbf{(b) Average Log-Probability Distribution:} The optimal temperature range has a strong relationship with the token probability distribution. If the distribution is relatively uniform, a low temperature is needed to sharpen the distribution, while if the distribution is concentrated, a higher temperature is more suitable to increase diversity.}
 %   \label{fig: teaser}
%\end{figure*}