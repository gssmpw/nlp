\begin{table}[ht]
\centering
\caption{Accuracy of the models when applied \textsc{TURN}, compared with the best temperature from grid search.\weihua{Maybe is a better metric compared with hit rate}}
\label{table: accuracy}
\small
\centering
\begin{tabularx}{0.48\textwidth}{@{} l c c c @{}}
\toprule
\multicolumn{4}{c}{\textbf{MATH Dataset with Majority Voting}} \\
\midrule
\textbf{Model} & \textbf{Predict} & \textbf{Best} & $\Delta$\\
\midrule
mistral-7b-sft & 0.4625 & 0.47 & \\
math-shepherd-mistral-7b-rl & 0.5275 & 0.5275 \\
Mistral-7B-v0.3 & 0.2663 & 0.27 \\
Mistral-7B-Instruct-v0.3 & 0.3325 & 0.3325 \\
deepseek-math-7b-base & 0.57 & 0.575 \\
deepseek-math-7b-instruct & 0.6525 & 0.66 \\
llemma-7b & 0.3913 & 0.3913 \\
llemma-7b-sft-metamath-hf & 0.5075 & 0.5075 \\
Llama-3.1-8B-Instruct & 0.655 & 0.6525 \\
Llama-3.1-8B & 0.4025 & 0.4075 \\
Llama-3.2-3B-Instruct & 0.645 & 0.645 \\
Llama-3.2-3B & 0.1975 & 0.2075 \\
OpenMath2-Llama3.1-8B & 0.755 & 0.7575 \\
\midrule
\multicolumn{4}{c}{\textbf{MBPP Dataset with Best-of-N}} \\
\midrule
\textbf{Model} & \textbf{Predict} & \textbf{Best} & $\Delta$ \\
\midrule
deepseek-coder-7b-base-v1.5 & 0.891 & 0.918 \\
deepseek-coder-7b-instruct-v1.5 & 0.8952 & 0.9129 \\
CodeLlama-7b-hf & 0.8297 & 0.8265 \\
CodeLlama-7b-Python-hf & 0.8471 & 0.8542 \\
CodeLlama-7b-instruct-hf & 0.8154 & 0.8154 \\
Qwen2.5-Coder-7B & 0.9610 & 0.9610 \\
Qwen2.5-Coder-7B-Instruct & 0.9778 & 0.9778 \\
Yi-coder-9B & 0.9579 & 0.9676 \\
Yi-Coder-9B-chat & 0.9501 & 0.9501 \\
Llama-3.1-8B & 0.8486 & 0.8594 \\
Llama-3.1-8B-Instruct & 0.8638 & 0.8677 \\
Mistral-7B-v0.3 & 0.8129 & 0.8109 \\
Mistral-7B-Instruct-v0.3 & 0.7990 & 0.7990 \\
\bottomrule
\end{tabularx}
\end{table}