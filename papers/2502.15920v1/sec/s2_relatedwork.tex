\section{Related Work}
\label{sec:related_work}
\paragraph{Challenges in Long Context Understanding}
LLMs struggle with long contexts despite supporting up to 2M tokens~\cite{dubey2024llama3,reid2024gemini}. 
The ``lost-in-the-middle'' effect~\cite{liu2024lost} and degraded performance on long-range tasks~\cite{li2023loogle} highlight these issues. To address this, ProLong~\cite{prolong} finetunes base models on a large, carefully curated long-context corpus. While this approach improves performance on long-range tasks, it comes at a significant cost, requiring training with an additional 40B tokens and long-input sequences.


%Recent studies have highlighted significant challenges in LLMs' processing of extended contexts. While models like Llama-3~\cite{dubey2024llama3} and Gemini~\cite{reid2024gemini} support context windows up to 128K or even 2M tokens, they struggle with effective utilization of this capacity. 
%The ``lost-in-the-middle'' phenomenon~\cite{liu2024lost} shows that models often fail to leverage information from the middle of long contexts, while \citet{li2023loogle} demonstrated that performance degrades significantly on tasks requiring long-range dependencies. 
%To address this issue, ProLong~\cite{prolong} finetunes base models on a large, carefully curated long-context corpus. While this approach improves performance on long-range tasks, it comes at a significant cost, requiring training with an additional 40B tokens and long-input sequences.

% ProLong~\cite{prolong} provides a solution through extensive continued pretraining (40B tokens) on long-context data, but this approach requires significant computational resources and may not work well for tasks require more than raw long-context abilities.
% These studies suggest that merely increasing the context window size is insufficient; enhancing true long-context understanding remains a significant challenge.

\paragraph{Inference-time Scaling for Long-Context}
The Self-Taught Reasoner (STaR) framework \citep{zelikman2022star} iteratively generates rationales to refine reasoning, with models evaluating answers and finetuning on correct reasoning paths. \citet{wang2024multi} introduced Model-induced Process Supervision (MiPS), automating verifier training by generating multiple completions and assessing accuracy, boosting PaLM 2's performance on math and coding tasks. \citet{li2024large} proposed an inference scaling pipeline for long-context tasks using Bayes Risk-based sampling and fine-tuning, though their evaluation is limited to shorter contexts (10K tokens) compared to ours (128K tokens).

%The Self-Taught Reasoner (STaR) framework, proposed by \citet{zelikman2022star}, presents a method where language models iteratively generate step-by-step rationales to improve reasoning capabilities. This approach involves the model generating rationales for questions, evaluating the correctness of the answers, and fine-tuning based on successful reasoning paths. %Building upon this, \citet{zelikman2024quiet} introduced \textsc{Quiet-STaR}, which enables models to generate internal rationales at each token to enhance predictions. These methods aim to improve inference-time reasoning without extensive human supervision. 
%Furthermore, \citet{wang2024multi} introduced Model-induced Process Supervision (MiPS), an automated data curation method that eliminates the need for human annotation in training verifiers. MiPS involves the model generating multiple completions of an intermediate solution step and calculating the accuracy based on the proportion of correct completions. Their approach significantly improved the performance of PaLM 2 on math and coding tasks. 
%Building on these ideas, \citet{li2024large} proposed an inference scaling pipeline for long-context tasks where LLM outputs are sampled and weighted using Bayes Risk, followed by fine-tuning on preferred outputs. Although their approach shares similarities with ours, their evaluation focuses on much shorter context lengths (around 10K tokens) compared to ours (up to 128K tokens).

% On this line of research, \citet{li2024large} proposed inference scaling pipeline to sample outputs from LLMs and weight them with Bayes Risk. They then finetune the model on preferred outputs. While sharing similarity with our approach, the context length of the problems considered in the paper is significantly shorter (around 10K) than ours (up to 128K).

\paragraph{Agentic Workflow for Long-Context} 
Agentic workflows~\cite{yao2022react} enable LLMs to autonomously manage tasks by generating internal plans and refining outputs iteratively. 
The LongRAG framework~\cite{zhao-etal-2024-dual} enables an LLM and an RAG module to collaborate on long-context tasks by breaking down the input into smaller segments, processing them individually, and integrating the results to form a coherent output.
Chain-of-Agents (CoA)~\cite{zhang2024chain} tackles long-context tasks through decomposition and multi-agent collaboration. In CoA, the input text is divided into segments, each handled by a worker agent that processes its assigned portion and communicates its findings to the next agent in the sequence.
Unlike these, our approach employs a single LLM that orchestrates its own reasoning and retrieval without relying on multiple components. By dynamically structuring its process and iteratively refining long-context information, our model reduces complexity while maintaining efficiency.



% \paragraph{Agentic Workflow for Long-Context}
% The concept of agentic workflows~\cite{yao2022react} in LLMs involves structuring models to autonomously manage tasks by generating and following internal plans or chains of thought. This approach allows models to handle complex tasks by decomposing them into manageable steps and iteratively refining their outputs. For instance, the LongRAG framework~\cite{zhao-etal-2024-dual} enables an LLM and an RAG module to collaborate on long-context tasks by breaking down the input into smaller segments, processing them individually, and integrating the results to form a coherent output. This method enhances the model's ability to manage and reason over extended contexts by leveraging internal planning and iterative refinement. Chain-of-Agents (CoA)~\cite{zhang2024chain} addresses the challenges of processing long-context tasks by leveraging multi-agent collaboration among LLMs. In CoA, the input text is divided into segments, each handled by a worker agent that processes its assigned portion and communicates its findings to the next agent in the sequence.
% Different from these approaches, we focus on an agentic system with a single primary LLM that autonomously orchestrates its reasoning and retrieval processes without relying on multiple interacting components. 
% Instead of distributing tasks across separate entities, our model dynamically structures its own reasoning process, iteratively retrieving, attending to, and refining long-context information within a unified framework. This enables efficient handling of extended contexts while reducing the complexity introduced by multi-agent coordination.


