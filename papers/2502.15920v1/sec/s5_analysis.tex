\section{Analyses \& Ablation Studies}
\label{sec:analysis}
In this section, we take a closer look at how each part of our approach affects long-context understanding and retrieval. Specifically, we study three main questions: (1) Can the finetuned system benefit from multi-round \coc? (2) Does adding clarifications and pointing back to the original document help the model understand and utilize the context more accurately? (3) How much additional compute overhead does \method add to the process?

\input{tab/multiround_table}
\subsection{How many rounds of \coc are needed?}
\paragraph{Setup.}
We add additional rounds of reasoning in the evaluation and see if the LLM can benefit from multi-rounds of reasoning at test-time.

\paragraph{Analysis.}
The results, presented in Table~\ref{tab:multiround}, indicate that additional rounds of agentic reasoning do provide performance improvements. 

This suggests that while significant benefits of self-clarification are achieved in the first round, additional rounds still contribute to further improvements. 
One possible explanation is the nature of our dataset: approximately 92\% of the questions are resolved within a single round of clarification. However, for the remaining cases, extended reasoning allows the model to refine its understanding, leading to measurable gains in performance with more clarification and reasoning.

% This suggests that most of the benefits of self-clarification are already realized in the first round.
% One potential explanation is the nature of our dataset: approximately 92\% of the questions are resolved within a single round of clarification. 
% As a result, the model may have adapted to favor shorter reasoning paths, making additional rounds of clarification redundant in most cases. 
% This highlights an important consideration for long-context modeling—while iterative refinement can be useful, its effectiveness is contingent on the complexity and structure of the dataset.


\input{tab/ablation_table}
\subsection{Do Self-Clarifications and Pointback Help in Long-Context Understanding?}
\paragraph{Setup.}
To evaluate the impact of each component in our agentic workflow, we compare the full \method-8B model against two variants: one without the self-clarification step and another without the contextual grounding (\emph{pointback}) step. 
We use the four RAG datasets with 128K context length as the evaluation benchmark, and compare the performance alongside the original model.

\paragraph{Analysis.}
Table~\ref{tab:ablation} shows the results on four QA benchmarks with a 128K context length. Removing self-clarification leads to an absolute performance drop of at least 10 points across most tasks (e.g., from 71.1\% to 57.8\% on HotpotQA), confirming that the model benefits from clarifying its own uncertainties when the context is long. Meanwhile, omitting pointback yields degenerate results, indicating that pinpointing relevant information at each stage is crucial for long-context QA. Overall, these findings highlight the importance of both clarifications and context-grounding to maximize retrieval accuracy and robustness in lengthy documents.

\subsection{How much additional compute cost does \method impose in generation?}
Since additional generation steps are introduced in the QA process, we assess the overhead in inference time.
Naïvely, long-context inference and multi-round conversations could significantly amplify compute costs. However, by leveraging prefix caching to store computed KV caches, the additional cost scales linearly with the number of newly generated tokens rather than exponentially.

To quantify this overhead, we conduct a runtime evaluation on 100 queries with a 128K context size. The results, summarized in~\cref{tab:runtime_overhead}, demonstrate that the additional computational overhead remains minimal when using prefix caching.
\input{tab/runtime_table}
