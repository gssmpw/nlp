\section{Evaluation}
\label{sec:evaluation}

\input{tab/lmm_eval_table_v2}

\input{img/fig_rag_result}

% \input{tab/prompting_table}
% \input{tab/rag_table}
% \input{tab/main_table}



In this section, we assess our method~\method using a suite of evaluation tasks drawn from the HELMET long-context benchmark~\cite{yen2024helmet}. Our experiments focus on testing models’ ability to retain, process, and reason over extended contexts ranging from 8K to 128K tokens.

\subsection{Tasks and Metrics}
We evaluate our models and baselines on the Helmet~\cite{yen2024helmet} long-context evaluation benchmark's retrieval-augmented generation (RAG) and long-range QA (LongQA) tasks ranging from 8K, 16K, 32K, 64K, to 128K.

We use GPT-4o as the judge for answer correctness, with the prompt template shown in~\cref{asec:prompt_template}. 
We report accuracies for all datasets.


The RAG test suite includes: 
(1) \textbf{HotpotQA}~\cite{yang-etal-2018-hotpotqa}, a multi-hop reasoning dataset over Wikipedia; 
(2) \textbf{Natural Questions}~\cite{kwiatkowski2019natural}, real user queries with Wikipedia-based short and long answers; 
(3) \textbf{TriviaQA}~\cite{JoshiTriviaQA2017}, a large-scale trivia dataset with question-answer pairs linked to evidence documents; 
(4) \textbf{PopQA}~\cite{mallen2023llm_memorization}, a dataset testing model memorization with fact-based questions from popular culture.

The LongQA test suite includes: 
(1) \textbf{NarrativeQA}~\cite{kocisky-etal-2018-narrativeqa}, a reading comprehension dataset with Wikipedia summaries and story-based Q\&A; 
(2) \textbf{InfiniteBench QA}~\cite{zhang-etal-2024-bench}, a long-range QA benchmark requiring reasoning over extended contexts; 
(3) \textbf{InfiniteBench Multiple-Choice}~\cite{zhang-etal-2024-bench}, a multiple-choice variant of the previous evaluating reading comprehension over long documents.

For the four RAG tasks, each question is put alongside a set of relevant contexts, and the overall input length is increased by appending irrelevant context. Consequently, these tasks become strictly more difficult as the context window expands. 
In contrast, for the three LongQA tasks, the relevant context may not appear in the truncated input (the first 8K, 16K, or 128K tokens). Hence, performance might improve at longer input lengths simply because the necessary information becomes available only after including more tokens.

%Given these differences in data construction, we use the four RAG datasets as our primary evaluation benchmark and report results on the three LongQA benchmarks only at the largest input size.
% \subsection{Baselines}
% We compare~\method against a diverse set of strong baselines that represent different approaches to handling long-context tasks.

\subsection{Baselines} 

We compare~\method against a diverse set of strong baselines representing different approaches for handling long-context tasks. Our comparisons include two main categories. 

Under prompting methods we consider techniques that require no additional model training. In particular, we evaluate (a) the chain-of-thought approach~\cite{kojima2022large}, which encourages models to decompose complex questions into intermediate reasoning steps; (b) fact-and-reflection prompting~\cite{zhao-etal-2024-fact}, which iteratively verifies and refines factual claims to enhance consistency; (c) plan-and-solve prompting~\cite{wang2023plan}, where the model first outlines a high-level plan before sequentially executing it to address structured reasoning tasks; and (d) LongRAG~\cite{zhao-etal-2024-longrag} where a hybrid RAG system is used to retrieve relevant context to generate global summaries and local details~\footnote{Note that LongRAG provided finetuned models as well. But the SFT-ed Llama3-8B only supports 8K context length. Thus we did not include it in our comparison.}. 

In the fine-tuning category, we focus on models that have been specifically adapted for extended context data. For a substantial comparison, we employ Prolong-8B-512K~\cite{prolong}—a model based on the Llama3 8B architecture that has been further trained on an additional 40B tokens of long-context data.
%and we also include recent long-range models such as Qwen2.5-14B-1M~\cite{yang2025qwen2}, which support context lengths up to 1M tokens with a larger and stronger base model.


\subsection{Main Results}

The performance of \method and baseline models is shown in~\cref{fig:rag_result}. 

%Our analysis highlights the strengths of \method-8B, our proposed method, which leverages a \textit{self-clarification} mechanism to enhance long-context reasoning.

\paragraph{Self-clarification significantly improves multi-hop reasoning.} \method-8B consistently surpasses other methods in HotpotQA. By iteratively refining its understanding, resolving ambiguities, and verifying intermediate steps, the model achieves higher accuracy, particularly as context length increases.

\paragraph{Robust performance across diverse datasets.} Unlike baseline models, \method-8B maintains consistently strong performance across RAG and LongQA benchmarks, demonstrating its ability to adapt effectively to different long-context tasks.

\paragraph{Reduced performance degradation with longer contexts.} While most models experience significant accuracy drops as context length increases, \method-8B remains stable. Its self-clarification and pointback mechanisms effectively filter noise from irrelevant information, allowing the model to extract and prioritize essential evidence.


\paragraph{Fine-tuning vs. prompting trade-offs.} While structured prompting techniques like \textit{plan-and-solve} improve short-context reasoning, they struggle with extreme context lengths (e.g., 128K tokens). In contrast, \method-8B, through targeted finetuning with self-clarification and pointback, maintains robust long-context reasoning without relying on complex prompting strategies. Although ProLong-8B, another finetuned model, achieves strong results, it comes with significantly higher training costs. \method-8B, by contrast, is more data-efficient and generalizes better to novel tasks, making it a more practical and effective solution for long-context reasoning.

Overall, these results underscore the effectiveness of \method-8B in tackling long-context understanding challenges. The integration of self-clarification plays a crucial role in improving grounding, reasoning, and comprehension in long-context settings.


\subsection{Performance on Short-Context Tasks}
To demonstrate that our fine-tuning process preserves the model's general capabilities while enhancing long-context understanding, we evaluated the finetuned model on a diverse set of standard benchmarks. These include elementary and advanced reasoning tasks ARC Easy and ARC Challenge~\cite{Clark2018ThinkYH}, mathematical problem-solving GSM8K~\cite{cobbe2021training}, MathQA~\cite{amini2019mathqa}, and broad knowledge assessment MMLU~\cite{hendryckstest2021, hendrycks2021ethics}, MMLU-Pro~\cite{wang2024mmlupro}.

We report the average performance across short-context tasks in~\cref{tab:performance-long-benchmarks}, and each individual task result can be found in~\cref{asec:short_context}. We find that the short-context performance is well preserved, demonstrating that \method's core reasoning and problem-solving abilities remain strong and are not compromised by the significant improvements to its long-context understanding powers.

% The results are shown in~\cref{tab:performance-benchmarks}. % Briefly explain the benchmarks here
% By performing well on these short-context tasks, the finetuned model demonstrates that its core reasoning and problem-solving abilities remain strong and are not compromised by improvements to its long-context understanding powers.


