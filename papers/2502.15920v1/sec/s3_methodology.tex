\section{\coclong Workflow}
\label{sec:methodology}
Our approach centers on enhancing long-context comprehension through an iterative, self-refining process that blends inference-time scaling with agentic reasoning. 
We coin this agentic workflow Chain-of-Clarifications (\coc).
In this section, we detail its key components, including the self-clarification process and the pointback mechanism, as illustrated in~\cref{fig:pipeline}.

Our proposed \coc framework is designed to mitigate the gap between nominal and effective context sizes in large language models. 
Rather than processing the entire long context and potentially multi-hop questions in a single pass, our methodology decomposes the task into a sequence of targeted sub-tasks. At each \coc step, the model autonomously:

\begin{itemize}
    \item \textbf{Generates clarifying questions} by identifying areas of the long input that require further elaboration or are prone to misinterpretation.
    \item \textbf{Pointbacks to relevant context} by using a pointback mechanism that highlights critical segments of the context by naming the index of relevant paragraphs. 
    In the data collection phase, this is done by iteratively querying the LLM about the relevance of each paragraph with respect to the question.
    After training, the model is finetuned to generate the related paragraph indexes directly in a single pass.  
    \item \textbf{Answers clarifying questions} by integrating highlighted context into consideration to build a more accurate and contextually grounded understanding of the long document.
    \item \textbf{Answers the original question} by combining all newly gathered clarifications, the model attempts to generate a valid answer to the original question.
\end{itemize}

It is important to note a key distinction between \coc path generation during data collection and the actual task deployment of the agentic workflow. In the data generation phase, we prompt the LLM to iteratively process each chunk of input text along with its self-generated clarifying questions, ensuring accurate retrieval of relevant context. 
% However, this approach is computationally expensive, as each contextual grounding step involves hundreds of inference calls. To mitigate this cost, we limit prompting to the data collection stage, where we prioritize obtaining high-quality training data.
During training, rather than relying on repeated inference calls, we finetune the model to directly generate the indexes of relevant paragraphs using pointback examples, effectively amortizing the computational cost into training. This enables the model to internalize the retrieval process, allowing it to dynamically synthesize relevant clarifications and contextual references at inference time without requiring extensive additional prompting.


% We generate long-context reasoning traces as learning targets using inference-time scaling. For each question, multiple traces are constructed in a tree structure, where the most effective trace is retained for later SFT and DPO. 
% Meanwhile, failed traces serve as negative examples in DPO to refine the model’s reasoning quality.

% Our framework leverages an inference-time scaling process to generate detailed reasoning traces that serve as learning targets for subsequent SFT and DPO. During inference, the model generates multiple candidate traces for a given question, structured in a tree-like formation where each branch represents a distinct sequence of clarifications, pointbacks, and reasoning refinements. This multiplicity of traces allows the model to explore diverse paths of inference, capturing a rich variety of reasoning patterns over extended contexts.

% From this tree of candidate traces, a selection mechanism identifies the most coherent and contextually grounded trace based on the correctness of the final answer. 
% Only the best-performing trace is retained for later SFT, ensuring that the fine-tuning process is guided by high-quality examples that exemplify effective long-context comprehension.

% In the data collection phase, this process is executed over a wide array of long documents and question prompts, thereby accumulating a robust dataset of refined reasoning traces. 
% These traces are subsequently used to fine-tune the model in an SFT framework, where the objective is to align the model's output with the high-quality reasoning patterns observed during inference. The training objective encourages the model to internalize the iterative process of generating clarifying questions, executing targeted pointbacks, and refining its internal state—thereby enhancing its ability to handle extended contexts in a single forward pass during deployment.

% This integration of inference-time scaling with SFT not only augments the effective context utilization but also creates a feedback loop that continuously improves the model’s performance on long-context tasks. By leveraging dynamically generated traces as learning targets, our approach offers a scalable pathway to bridge the gap between nominal context size and the effective reasoning capabilities required for complex, long-form documents.

