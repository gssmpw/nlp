\section{Introduction}


\input{img/pipeline_figure}

Large language models have achieved notable milestones in natural language processing, demonstrating exceptional performance in tasks such as mathematical reasoning, code generation, and conversational understanding~\cite{openai2023gpt4, deepseekai2025deepseekr1incentivizingreasoningcapability}. 
However, effectively comprehending and utilizing long-context inputs remains a major challenge. Complex queries often require models to retrieve multiple relevant pieces of information from extensive contexts and synthesize them coherently. While recent advancements have extended context windows to 128K and even 2M tokens~\cite{dubey2024llama3, touvron2023llama2, reid2024gemini}, these models still struggle to fully integrate and reason over large-scale contextual information. 
Recent studies~\cite{liu2024lost, prolong} highlight a fundamental challenge in long-context understanding: the disparity between a model’s nominal context size—the theoretical maximum input length—and its effective context window, the portion of the input the model actively utilizes for reasoning. This gap significantly impacts the understanding performance, limiting the model's ability to fully comprehend and integrate long-context information. 
% This disparity between nominal context size and effective long-context capabilities has been highlighted in recent studies~\cite{liu2024lost, prolong}.
% The mismatch between nominal and effective context lengths limits performance on tasks that require nuanced understanding of lengthy and complex documents, as essential details often fail to contribute meaningfully to the final output. 
% This mismatch constrains the model performance on tasks that require a nuanced understanding of long and complex documents, as crucial details often are missed in generating the final output.

We introduce a novel framework \method to enhance long-context comprehension in LLMs. 
%Inspired by recent advances in retrieval-augmented generation~\cite{zhao-etal-2024-dual} and agentic workflow~\cite{zhao2024expel}, 
% Our approach enables the model to autonomously navigate and dissect extended inputs through clarification question generation and contextual grounding, as depicted in~\cref{fig:pipeline}.
% At the core of our framework is a self-clarification mechanism which allows the model to raise and answer clarifying questions and a pointback mechanism, which allows the model to dynamically reference the original context, thereby reinforcing its comprehension and yielding more accurate, contextually grounded responses.
As illustrated in~\cref{fig:pipeline}, the core of \method is \textbf{Chain-of-Clarifications} (\coc), a process where models enhance their understanding by generating clarification questions, retrieving relevant information from the long context and answering their own clarification questions based on the gathered evidence. Rather than relying on a direct response, \coc helps models refine their reasoning iteratively, resolving uncertainties along the way. We structure the framework into the following two stages.

\paragraph{\coc Path Construction.} To collect reliable \coc~understanding path, we structure data collection as a tree search, where each \coc~step represents a node. We leverage extended inference time to determine the effective clarification questions to ask and the relevant evidence to retrieve. With a search depth of three and a branching factor of eight, \method successfully retrieves 97.8\% of the correct answers in NarrativeQA~\cite{kocisky-etal-2018-narrativeqa}, demonstrating its capability to tackle complex questions that require multi-step reasoning over long-context inputs.

\paragraph{\coc Path Distillation.} Once the dataset is collected from the tree-search process, we train the model to generate effective clarifications and contextual groundings in a single pass, eliminating the need for scaling at inference time. This is achieved by distilling these collected paths into LLMs through supervised finetuning (SFT) and direct preference optimization (DPO)~\cite{rafailov2024direct}, effectively amortizing the computational cost from inference to training.


% The main contributions of our work are:
% \begin{itemize}
% \item A novel self-clarification and pointback mechanism that allows large language models to iteratively refine their understanding and dynamically reference earlier context, significantly improving long-context reasoning.

% \item  Efficient data generation strategy, relying on self-generated training data instead of external supervision, and achieves superior performance on long-context benchmarks by narrowing the gap between nominal and effective context utilization.

% \item A structured framework that enhances autonomous navigation of extended inputs, strengthening comprehension and decision-making in long-context tasks.
% \end{itemize}

%A key aspect of our methodology lies in the synergy between iterative retrieval and agentic orchestration. 
%By combining self-clarification queries with a dynamic retrieval mechanism, the model actively refines which segments of the extended input are most relevant to the current query. 

% We train the model to directly generate effective clarifications and contextual groundings in a single pass. 
% We first collect a dataset from the diverse clarification paths and context groundings in the aforementioned tree-search process; we then distill these paths into LLMs through supervised finetuning and direct preference optimization~\cite{rafailov2024direct}, effectively amortizing the cost from inference to training.

Our method \method~significantly improves model's long-context understanding capabilities without relying on laborious human annotations or stronger teacher models for data generation. Instead, the base model's self-generated CoC paths enables it to teach itself to process long-context inputs more effectively. This approach harnesses the model’s inherent long-context capabilities—previously only accessible through an additional LLM agent—allowing it to independently refine its reasoning and retrieval processes.
Empirically, we demonstrate that \method~consistently boosts performance across a set of question-answering tasks up to 128K tokens, outperforming both prompting-based approaches and other long-context-finetuned LLMs.
By integrating self-clarification and context grounding in an agentic manner, we take a step further toward enabling LLMs to comprehend long contexts.
% distinguishes itself by relying on self-generated data rather than external supervision, embodying an agentic paradigm where the model refines its understanding through iterative inquiry. 
% This workflow not only bridges the gap between nominal and effective context sizes, but also paves the way for building more autonomous and robust long-context processing systems.

% The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work, Section~\ref{sec:methodology} details our proposed framework, Section~\ref{sec:evaluation} presents empirical evaluations demonstrating its efficacy, and Section~\ref{sec:conclusion} discusses the broader implications and potential future directions.