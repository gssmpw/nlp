\section{Conclusion}
\label{sec:conclusion}

In this work, we introduce Agentic Long-Context Understanding (\method), a framework designed to enhance large language models' ability to process and reason over long-context inputs with self-generated data. 
By incorporating an agentic workflow (\coc) that dynamically refines model reasoning through self-clarifications and contextual grounding, \method significantly improves LLM's long context understanding capabilities.

Through a combination of trace data collection and two-stage post-training, our approach enables models to autonomously explore multiple reasoning paths, distill the most effective clarification strategies, and improve their understanding of lengthy documents. 
Extensive evaluations on long-context benchmarks demonstrate that AgenticLU outperforms existing prompting techniques and finetuned baselines, maintaining strong performance across context lengths up to 128K tokens. 
Additionally, ablation studies confirm that self-clarification and pointback mechanisms play a crucial role in improving retrieval and reasoning over long-contexts.

