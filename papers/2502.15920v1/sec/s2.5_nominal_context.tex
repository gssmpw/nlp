\section{The Context Size Gap}
% Many contemporary LLMs claim to have extremely long context sizes.
% Proprietary models like GPT4o~\cite{openai2023gpt4}, Claude3.5-Sonnet~\cite{claude3} and Google Gemini-2.0-Pro~\cite{team2023gemini} claims to have 128K, 200K, and 2M tokens. 
% Open sourced models such as Llama3 series of models claims 128K context length~\cite{dubey2024llama3}, Qwen2.5 just released long context versions of 7B and 14B models supporting up to 1M tokens~\cite{yang2025qwen2}. Seems with scaling and efficient long-context attention mechanisms, the evolving context size could be able to solve most of the current long-context problems.


% Recent large language models have made strong claims about their context lengths. For example, proprietary models such as GPT4o~\cite{openai2023gpt4}, Claude3.5-Sonnet~\cite{claude3}, and Google Gemini-2.0-Pro~\cite{team2023gemini} state that they can handle up to 128K, 200K, and 2M tokens, respectively. Meanwhile, open-source models like the Llama3 series~\cite{dubey2024llama3} advertise 128K-token contexts, and Qwen2.5’s latest long-context versions (7B/14B) reportedly support contexts of up to 1M tokens~\cite{yang2025qwen2}. At first glance, these rapid developments—enabled by scaling and more efficient attention mechanisms—appear poised to solve many of the current long-context challenges.

% However, the situation is more nuanced than the numbers suggest. Recent studies~\cite{prolong,yen2024helmet,shang2024ai} have shown that the \emph{effective} context size of an LLM (the length over which it can reliably perform tasks such as information retrieval and complex reasoning) often diverges from its claimed, or \emph{nominal}, context length. To illustrate this gap, we conducted an experiment demonstrating that models’ reasoning abilities diminish as context lengths grow, reinforcing the discrepancy between nominal and effective context sizes.

State-of-the-art LLMs have made strong claims about their context lengths, supporting hundreds of thousands of input tokens. However, recent studies~\cite{prolong,yen2024helmet,shang2024ai} have shown that the \emph{effective} context size of an LLM (the length over which it can reliably perform tasks such as information retrieval and complex reasoning) often diverges from its claimed, or \emph{nominal}, context length. 

% To illustrate this gap, we conducted an experiment demonstrating that models’ reasoning abilities diminish as context lengths grow, reinforcing the discrepancy between nominal and effective context sizes.

To illustrate this gap, we evaluate Llama3.1-8B-Instruct, which supports a 128K-token context, on the HotPotQA dataset to test multi-hop QA performance at various input lengths (8K, 16K, 32K, 64K, and 128K). We artificially expand the input by adding irrelevant context and measure the accuracy of its answers  using GPT-4o as a judge. As shown in \cref{fig:hotpotQA}, The model’s performance degrades substantially as increasing context length, demonstrating the discrepancy between nominal and effective context sizes.

\input{img/fig_hotpotqa}

While expanding nominal context capacity is undoubtedly important, we argue that it is not sufficient for solving all long-context problems. By analogy with computer memory, simply having more capacity does not guarantee efficient or accurate computation; one must also manage the ``loading'' of relevant information in and out of this memory. Therefore, we propose an agentic workflow aimed at helping LLMs process and interpret extended contexts more intelligently. 

% To further streamline this approach, we apply inference-time scaling on the base model during the data construction period to automatically generate agentic reasoning traces and paragraph groundings, reducing the need for labor-intensive human annotation. With our newly generated data, we further perform SFT and DPO on the base model to distill Chain of Clarification (CoC) ability from the sampled agentic reasoning traces and paragraph groundings. In inference of our finetuned model (\method), we do not require inference-time scaling which saves the inference cost. (See details in Sec.~\ref{sec:methodology})

% However, the reality is a little bit more complex that that.
% It has been pointed out in recent research~\cite{prolong,yen2024helmet, shang2024ai} that LLM's claimed context size is often mismatched with its effective context size, i.e., the context size that it can perform non-trivial information processing tasks such as information retrieval and reasoning.

% We perform an experiment to illustrate the gap, illustrating that model's reasoning ability decays as the context grow, solidifying the existence of the gap between nominal context size and effective context size.
% experiment result will be added as a figure here

% To solve the long context problems, we believe extending the nominal context size is a key step but not the most important step.
% We foresight that the context size would behave like computer memories, to process large-scale computation challenges it is true that one needs a big enough memory, but more importantly, the algorithm to smartly load in and load out things into and out of the memory.
% Therefore we propose the agentic workflow to encourage LLMs to integetly process and comprehend long-context queries. 
% And to efficiently obtain training data without costly human labeling effort, we utilize techniques of inference-time scaling to generate traces of agentic reasoning and paragraph grounding.




