\section{Data Generation \& Model Training}

\paragraph{Dataset} 
We use the NarrativeQA~\cite{kocisky-etal-2018-narrativeqa} dataset to facilitate long-context QA and generate agentic workflow traces with 14.7K QA pairs in the training set. NarrativeQA is designed for reading comprehension over narrative texts, such as books and movie scripts, where each example includes a full story and a set of corresponding QA pairs. This dataset emphasizes deeper reasoning and long-context understanding, as many questions require synthesizing information from multiple parts of the narrative rather than focusing solely on particular local context. Its relatively long passages make NarrativeQA particularly suitable for testing and refining agentic reasoning in large language models, as the answers often depend on weaving together details spanning the entire text.

\paragraph{Base Model} Our base model is \textit{Llama3.1-8B-Instruct}~\cite{dubey2024llama3}, an 8-billion-parameter instruction-tuned Llama model. This model is built on the same transformer architecture as Llama3, but with additional fine-tuning data to improve its performance on multi-turn dialogue and instruction-following tasks.

\subsection{\coc Path Construction}
We employ a test-time scaling approach to generate \coc paths. For each question, we construct a tree of search paths where each node represents a distinct clarification question posed by the LLM.

In our experiments, we use a branching factor of 8 at each depth and select the most promising trace based on an evaluation score that combines:
\begin{itemize}
    \item \textbf{Semantic similarity}, measured by the RougeL~\cite{lin-2004-rouge} score relative to the ground truth.
    \item \textbf{Discrete correctness}, evaluated by a binary verification using GPT4o-mini.
\end{itemize}

In the data construction process, the relevant context is found by iteratively querying the LLM about the relevance of all chunked passages. Here we use 512 as the chunk size. This process is compute-intensive but only happens in data collection. 
After the training, the LLM will directly generate the paragraph numbers of the relevant context as shown in the lower right of~\cref{fig:pipeline}.

% \input{img/fig_datastat}

For most long-context tasks, a single clarification question suffices because the required reasoning is not highly complex. 92\% of the questions in our experiments are resolved correctly with just one round of clarification. More challenging tasks may require multiple rounds of clarification: two rounds resolve 53\% of the remaining 8\%, and three rounds resolve 35\% of the remaining 4\%. Because of the exponentially increasing cost—and given that 97.4\% of the training questions are already solved—we limit the maximum depth of our inference scaling to 3.

The statistics of the collected dataset are shown in~\cref{tab:narrativeqa_stats}. The total number of conditional generation tokens that the LLM trained on is 17M tokens, with input that has an average length of 67K and a max length of 128K tokens.

\input{tab/dataset_table}

\subsection{\coc Path Distillation}
We employ a two-stage finetuning recipe: Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO)~\cite{rafailov2024direct}, to convert our base model into a long-context understanding agent.
The dataset statistics is described in~\cref{tab:narrativeqa_stats}, with input length up to 128K tokens. 

\paragraph{Supervised Fine-Tuning} In the first phase, we finetune \textit{Llama3.1-8B-Instruct} using the generated \coc paths. Each training example includes (1) the full context from NarrativeQA, (2) the question, and (3) the step-by-step reasoning trace leading to the final answer. 
By exposing the model to these traces, we encourage it to internalize multi-step reasoning strategies and context grounding for the long-context inputs. 
The SFT stage uses a standard cross-entropy loss on the next-token prediction task, ensuring the model learns how to produce consistent and complete reasoning sequences.

\paragraph{Direct Preference Optimization} 
In the second phase, we apply Direct Preference Optimization to further refine the model’s output quality. 
To create preference pairs, we sample incorrect workflow traces as negative examples with using GPT4o-mini as the judge for answer correctness from the test-time scaling. 
DPO explicitly optimizes the model to generate higher-ranked responses more frequently, thus aligning the agent’s outputs with desirable characteristics, such as clarity, correctness, and coherence. This stage ensures that even among valid reasoning paths, the model learns to prioritize the most instructive reasoning.

The details for the two-phase training are listed in~\cref{asec:hyperparameters}.
