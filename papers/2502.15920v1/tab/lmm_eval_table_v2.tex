\begin{table*}[ht]
  \centering
  \caption{Performance difference of \method and its base, Llama3.1-8B-Instruct ($\delta=$\method-8B minus Llama3.1-8B), on long context (the 128K tasks) and short-context benchmarks (6 regular tasks including ARC, GSM8K, and MMLU), the details of the short-context performance can be found in~\cref{asec:short_context}. Scores represent accuracy, with \method demonstrating significantly improved performance across long-context tasks with minimal effect on regular task performance.}
  \label{tab:performance-long-benchmarks}
  \resizebox{\textwidth}{!}{
\begin{tabular}{l|r|rrrrrrrr}
  \toprule
  \textbf{Model} & \textbf{Short Avg} & HotpotQA & Natural Questions & TriviaQA & PopQA & NarrativeQA & InfiniQA & InfiniChoice & \textbf{Long Avg}  \\
  \midrule
  Llama3.1-8B  & \textbf{62.3}  & 40.0   & 56.1    & 80.6    & 56.1   & 38.0    & 48.0   & 55.0     & \textbf{53.4}  \\
 AgenticLU ($\delta$)    & \textbf{-0.6} & +31.1 & +21.7 & +7.7 & +9.4 & +18.0  & +2.0 & +13.0  & \color{red}{\textbf{+14.7}} \\
  \bottomrule
\end{tabular}
}
\end{table*}