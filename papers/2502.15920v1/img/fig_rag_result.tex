\begin{figure*}[t!]
    \begin{center}
    \includegraphics[width=\linewidth , keepaspectratio]{img/rag_result.pdf}
    \end{center}
    % \caption{\textbf{Long-context performance across context lengths and tasks.} Results showing QA accuracy across seven tasks (four RAG tasks: HotpotQA, Natural Questions, TriviaQA, PopQA; three LongQA tasks: NarrativeQA, InfbenchQA, InfbenchChoice) for varying input lengths from 8K to 128K tokens. 
    % We compare the finetuned \method-8B against the base Llama3.1-8B model and baselines including prompting methods (Step-by-Step, Plan-and-Solve, Fact-and-Reflect, LongRAG) and ProLong-8B, a long-context model continued pretrained from Llama3-8B with additional 40B tokens. 
    % \method-8B consistently maintains strong performance across most tasks and context lengths.}
    \caption{\textbf{Main results on 7 long-context tasks across context lengths from 8K to 128K.} Our \method-8B (dotted orange) achieves significant improvements on \emph{all} tasks over our base model Llama3.1-8B (solid orange). We also compare with the prompting methods (Step-by-Step, Plan-and-Solve, Fact-and-Reflect, LongRAG) and the state-of-the-art ProLong-8B model. \method-8B consistently maintains strong performance across most tasks and context lengths.}
    \label{fig:rag_result}
\end{figure*}
