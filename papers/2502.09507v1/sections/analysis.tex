\section{Why Does CLIP (Not) Generalize?}\label{sec:analysis}
In this section, we investigate which changes in the CLIP model led to domain and compositional generalization, or lack thereof, observed in the previous section.

\subsection{The Role of Visual Embeddings}
\begin{table}[t]
    \centering
    \caption{\textbf{Domain diversity increases feature sharing in the embeddings.}
    We report the percentage point increase in top-k shared \acrshort{sae} features, averaged over $c \in C_2$ and $k \in \{5, 10, 15, 20\}$, when comparing low diversity (Natural-only) to high diversity (Leave-out-domain, CG high-diversity). Note that we used the CLIP models using CC12M as base dataset, since we found that \acrshortpl{sae} extracted poor features for the ImageNet-Captions models.
    }
    \begin{tabular}{lcc}
        \toprule
         & Clipart & Sketch \\
        \midrule
        Natural-only $\rightarrow$ Leave-out-domain  & +7.1 & +4.1 \\
        Natural-only $\rightarrow$ CG high-diversity & +6.9 & +3.4 \\
        \bottomrule
    \end{tabular}
    \label{tab:feature-sharing}
\end{table}

Intuitively, we would expect CLIP to share more features in its visual embeddings across domains as generalization improves.
To test this hypothesis, we applied an unsupervised dictionary learning technique, \ie, \acrfullpl{sae} \citep{bricken2023monosemanticity,huben2024sparse}, to extract interpretable features from CLIP's visual embeddings $\mathbf{a}\in\mathbb{R}^p$: 
\begin{equation}
    \text{SAE}(\mathbf{a}):=(g\circ\phi\circ f)(\mathbf{a}),
\end{equation}
where $\phi$ is a ReLU non-linearity, and $f$ and $g$ are the linear encoder with weights $\mathbf{W}_f\in\mathbb{R}^{p\times h}$ and decoder with weights $\mathbf{W}_g\in\mathbb{R}^{h\times p}$, respectively. We trained the \acrshort{sae} with an $L_2$ reconstruction loss and $L_1$ sparsity regularization. Refer to \cref{app:sae} for further technical details.

After extracting interpretable features, we computed the percentage overlap of the $k$ most important features between pairs of domains per class.
\Cref{tab:feature-sharing} confirms that a CLIP model that generalizes better shares indeed more features in its visual embeddings.
\begin{finding}
    CLIP shares more features in its embeddings as generalization improves.
\end{finding}

\subsection{Why Does CLIP Sometimes Fail To Generalize?}\label{sec:quickdraw}
\begin{table}[t]
    \centering
    \caption{\textbf{Domain-specific captions do not explain CLIP's poor generalization performance to unseen quickdraw classes in the CG high-diversity setting.}
    As we replace all domain-specific captions (second row), we find that visual embeddings are more aligned (\cref{fig:quickdraw-alignment:aligned}) but this does not lead to improved generalization performance.
    }
    \resizebox{\columnwidth}{!}{\begin{tabular}{cccc}
        \toprule
        \multicolumn{2}{c}{Captions} & \multicolumn{2}{c}{Classes} \\
        \cmidrule(l{0pt}r{2pt}){1-2}\cmidrule(l{2pt}r{0pt}){3-4}
        domain-specific & domain-invariant & seen & unseen \\
        \midrule
        50\%   & 50\% & 50.7 & 1.7 \\
        0\%  & 100\%  & 46.2 & 0.7 \\
        \bottomrule
    \end{tabular}}
    \label{tab:quickdraw-alignment}
\end{table}
\Cref{fig:effective-robustness} shows that CLIP typically generalizes well compositionally with high domain diversity, except for quickdraw. It is tempting to attribute this solely to the unique characteristics of quickdraw images (\cref{app:shift_examples}). However, the first row of \cref{tab:quickdraw-alignment} highlights that CLIP can correctly classify the quickdraw images of seen classes, indicating that is has learned something useful for the quickdraw domain, yet CLIP performs poorly on images of unseen classes.

\paragraph{Are domain-specific captions the cause?}
We initially hypothesized that using domain-specific captions--our training included both domain-invariant and domain-specific captions (\cref{sec:exp_setup})--might have inadvertently led CLIP to prioritize uniformity over alignment in its loss function. We suspect this occurs due to the stark visual differences between quickdraw images and those from other domains, making alignment challenging. In order to minimize the total loss in spite of this, CLIP may adopt a shortcut: prioritizing uniformity, aligning quickdraw image embeddings with the text embeddings of the domain-specific captions, while sacrificing alignment with domain-invariant captions. Consequently, we would expect a clear separation between quickdraw image embeddings and those from other domains--and indeed, we observe this (\cref{fig:quickdraw-alignment:non-aligned}). However, this separation may come at a cost, limiting CLIP's ability to generalize across the quickdraw domain, particularly to unseen classes.

\begin{figure}[t]
    \centering
    \begin{subfigure}[c]{0.49\linewidth}
        \centering
        \includegraphics{figures/umap-quickdraw-alignment-1}
        \caption{Domain-invariant and specific captions.}
        \label{fig:quickdraw-alignment:non-aligned}
    \end{subfigure}
    \begin{subfigure}[c]{0.49\linewidth}
        \centering
        \includegraphics{figures/umap-quickdraw-alignment-2}
        \caption{Only domain-invariant captions.}
        \label{fig:quickdraw-alignment:aligned}
    \end{subfigure}
    \caption{\textbf{The separation of the visual embeddings between quickdraw and the images of other domains is due to domain information in captions.} However, the better alignment of the visual embeddings does not improve compositional generalization (\cref{tab:quickdraw-alignment}).
    }
    \label{fig:quickdraw-alignment}
\end{figure}
As a remedy, we trained CLIP using only domain-invariant captions. Although the visual embeddings are now better aligned (\cref{fig:quickdraw-alignment:aligned}), generalization performance remains poor (second row of \cref{tab:quickdraw-alignment}). Thus, the domain invariance or specificity of captions cannot solely explain CLIP's poor performance on quickdraw images of unseen classes.

\paragraph{The role of shared intermediate features and circuitry}
While visual embeddings appear aligned in \Cref{fig:quickdraw-alignment:aligned}, it does not necessarily imply that the computations leading up to these embeddings are also aligned. Thus, we hypothesize that \emph{insufficient sharing of intermediate representations and circuits} \citep{hohman2019summit,olah2020zoom} within CLIP's vision encoder may explain its poor generalization to the unseen classes. For example, if CLIP learns a separate circuit for quickdraw images instead of sharing sufficient functionality across the domains, such a circuit may work for seen classes but will fail on unseen classes, since the unseen classes of the test domain can only be inferred with the help of the other domains.

\paragraph{Analysis tools}
To test our hypothesis, we used tools from representational similarity analysis to evaluate intermediate representations. Additionally, we introduce a novel, related concept for circuits: \emph{mechanistic similarity}, which measures the similarity between circuits.

For the representational analysis, we measured the representational similarity of classes across domains. Specifically, we used center kernel alignment (CKA) \citep{kornblith2019similarity} with the unbiased Hilbert-Schmidt Independence Criterion (HSIC) estimator \citep{song2012feature}; refer to \cref{app:cka} for technical details.

For the mechanistic analysis, we identified the circuit for each class of each domain in the first step. Specifically, we identified the $k$ most important model components--\ie, axis-aligned neurons--by computing their indirect effect \citep{pearl2001direct} on the model's predictions, following recent work \citep{vig2020investigating,schrodi2022towards,meng2022locating,marks2024sparse}. Intuitively, the indirect effect quantifies how much a neuron contributes to the model's prediction. These identified neurons serve as the nodes of the graph representing the circuit. Next, we determined the $k'$ most influential predecessors of each neuron, forming the edges of the circuits, that intuitively represent the information flow. To do this, we computed the indirect effects of the predecessors on each neuron. Refer to \cref{app:act_patching} for further details on this procedure.

In the second step, we measured circuit similarity using graph similarity measures. Specifically, we measured the similarity of the circuits of each class across different domains, \eg, the circuit for quickdraw dog images \vs those for clipart, infograph, \etc. We computed the layer-wise Jaccard index of the $k$ most important neurons, \ie, the nodes of the circuit. Beyond this simple node overlap measure, we used the normalized Weisfeiler-Lehman subtree graph kernel \citep{shervashidze2011weisfeiler} to capture more complex structural and hierarchical similarities. Refer to \cref{app:act_patching} for details on these similarity measures.

\paragraph{Result}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics{figures/overlap_score_linear_cka_hsic1_mean_all_quickdraw_no_xticks}
        \caption{Representational similarity.}
        \label{subfig:repr-similarity}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics{figures/overlap_score_all_quickdraw_no_xticks}
        \caption{Amount of shared neurons.}
        \label{subfig:shared-neurons}
    \end{subfigure}\\\bigskip
    \begin{subfigure}[t]{\linewidth}
        \centering
        \begin{tabular}{ccc}
            \toprule
             & Quickdraw & Other \\\midrule
            Weisfeiler-Lehman similarity ($\uparrow$) & 0.218 & 0.268 \\
             \bottomrule
        \end{tabular}
        \caption{Circuit similarity.}
        \label{subfig:graph-sim}
    \end{subfigure}
    \caption{\textbf{CLIP separates quickdraw images from other domains in its intermediate representations and circuitry, even though final visual embeddings can be aligned.} We used the CLIP model trained on only domain-invariant captions from \cref{fig:quickdraw-alignment:aligned,tab:quickdraw-alignment} (second row).
    Scores are averaged over classes and higher scores mean higher representational similarity (\subref{subfig:repr-similarity}), more shared neurons (\subref{subfig:shared-neurons}), or more similar circuitry (\subref{subfig:graph-sim}).}
    \label{fig:quickdraw-analysis}
\end{figure}
\Cref{fig:quickdraw-analysis} confirms that the quickdraw domain noticeably differs from the other domains, both in terms of representational and mechanistic similarity, supporting our hypothesis that (compositional) generalization requires sufficient sharing of intermediate features and circuitry.
\begin{finding}
    Sufficient sharing of intermediate features and circuitry is crucial for generalization to succeed.
\end{finding}