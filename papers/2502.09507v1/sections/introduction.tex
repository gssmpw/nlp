\section{Introduction}
Foundation models are considered a decisive step towards more generic AI models~\citep{bommasani2021opportunities}. For example, CLIP scaled the alignment of image-text pairs via a contrastive loss to millions of samples \citep{radford2021learning,jia2021scaling,zhai2023sigmoid}.
Unlike traditional classifiers from the ImageNet era, which often experience substantial performance drops under distribution shifts, CLIP demonstrates unprecedented generalization to ``\acrfull{ood}'' data \citep{radford2021learning}. However, what drives this improved \acrshort{ood} generalization?

Recent work has converged on the conclusion that CLIP's \emph{diverse} training distribution is the primary factor driving its unprecedented generalization performance. For example, \citet{fang2022data} found that other factors such as language supervision, training data size, or the contrastive loss play only a minor role, while \citet{nguyen2022quality} showed that data quality is more important than quantity. More recently, \citet{mayilvahanan2024search} demonstrated that CLIP's ``generalization performance [...] drops to levels similar to what has been observed for ImageNet-trained models'' \citep[p.~10]{mayilvahanan2024search} by limiting the diversity of (visual) domains\footnote{We refer to a domain as a group of images sharing a common style, such as natural images, sketches, paintings, \etc.} to a minimum, \ie, by removing all non-natural samples. This shows that the mixture of various (non-natural) domains plays an important role for CLIP's generalization, yet the underlying mechanisms remain unexplored. This brings us to our core research question:
\begin{center}
    \emph{How does the mixture of diverse (visual) domains in the training data affect CLIP's generalization performance?}
\end{center}
In particular, we investigate under which circumstances CLIP can learn the object class invariances across the training domains with the aim to generalize to entirely unseen domains--a fundamental question about its \emph{domain generalization} capability \citep{blanchard2011generalizing,muandet2013domain,gulrajani2021in}. 
We also study questions about CLIP's \emph{compositional generalization} \citep{hupkes2020compositionality,wiedemer2023compositional}, which is believed to be an important factor of its generalization performance \citep{mayilvahanan2024does,udandarao2024no} and a long-standing challenge of machine learning research. Adapting \citeauthor{szabo2012case}'s (\citeyear{szabo2012case}) classical example, we ask pictorially: Can CLIP, trained on natural images of cats and dogs along with sketches of cats, generalize to sketches of dogs?

To answer such questions, we construct fully controllable experimental conditions that allow precise and systematic manipulation of the domain mixtures and exposure to object classes in the training data (see \cref{fig:dataset-setup}), while keeping all other variables, such as the CLIP model type, training process, and class distribution constant. Specifically, we augmented a base dataset consisting primarily of natural images, such as ImageNet-Captions \citep{fang2022data}, with non-natural samples from DomainNet \citep{peng2019moment}, including the domains Clipart, Infograph, Painting, Quickdraw, and Sketch. By systematically including subsets of these domains and their classes, we study questions about CLIP's domain generalization and compositional generalization capabilities. We complement these experiments with in-depth data-centric and mechanistic analyses to understand what changes in the CLIP model led to the improved generalization or failure thereof. Our experiments uncovered the following key findings:
\begin{itemize}[topsep=1pt]
    \itemsep0pt
    \item \textbf{Domain diversity improves generalization}: We reaffirm the intuition that diversity of domains in the training distribution is critical for both domain and compositional generalization. However, CLIP only \emph{weakly} generalizes, as there remains a performance gap to a model that has seen similar samples during training.
    \item \textbf{Compositional generalization is challenging}: Surprisingly, including a domain in the training data does not always improve generalization to unseen classes within that domain. However, ensuring sufficient class diversity within the test domain--ideally with no overlap with the queried classes in evaluation--along with high domain diversity, can significantly reduce the aforementioned performance gap.
    \item \textbf{Generalization requires sufficient feature and circuit sharing}: When CLIP generalizes well compositionally, it shares more embeddings and intermediate features between different domains. However, CLIP sometimes fails to generalize to certain domains. We provide a twofold explanation: (1) the inputs from these domains lack shared features and, as a result, (2) the model has limited shared intermediate features and circuitry between domains, constraining its ability to generalize effectively. We support this hypothesis through representational similarity analysis and introduce a related concept for circuits\footnote{Interconnected internal model mechanisms/components for performing a specific computation or task \citep{olah2020zoom}.}: \emph{mechanistic similarity}, which measures similarity between circuits.
\end{itemize}
This work presents a comprehensive study for systematically investigating CLIP's domain generalization and compositional generalization capabilities. This enables us to unveil the capabilities and limitations of CLIP. Our code will be published together with the conference paper.