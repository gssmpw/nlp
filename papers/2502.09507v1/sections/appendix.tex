\section{Further Details for Section~\ref{sec:exp_setup}}
\subsection{DomainNet Examples}\label{app:shift_examples}
\Cref{fig:samples} visualizes four examples from all six image domains (clipart, infograph, painting, quickdraw, real, sketch) from DomainNet \citep{peng2019moment}. Some domains are more visually similar than others. For example, sketches and quickdraw are typically gray images, while paintings often contain a similar level of image detail as real (natural) images.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth,trim=0 500 0 0,clip]{figures/domainnet-samples.png}
    \caption{\textbf{Random examples across the six domains of DomainNet.}}
    \label{fig:samples}
\end{figure}

\subsection{DomainNet Training Captions}\label{app:dn-captions}
The DomainNet dataset \citep{peng2019moment} does not include captions. Therefore, we created captions using the class names to train CLIP models on DomainNet data. For this, we used prompt templates similar to \citet{radford2021learning}. Specifically, we used the following templates: 
\begin{itemize}[topsep=0pt]
    \itemsep0em 
	\item ``\texttt{a \{domain\} of a \{class\}.}'',
	\item ``\texttt{a \{class\} \{domain\}.}'',
	\item ``\texttt{a \{domain\} depicting a \{class\}.}'',
	\item ``\texttt{a \{class\} depicted in a \{domain\}.}'',
	\item ``\texttt{a \{domain\} showing a \{class\}.}'', 
	\item ``\texttt{a \{class\} is visible in a \{domain\}.}''.
\end{itemize}
For each DomainNet image sample, we randomly sampled one of the templates and inserted the corresponding class name for the placeholder \verb|{class}|. For the \verb|{domain}| placeholder, we randomly chose with equal probability a generic, domain-invariant term, such as \textit{image} or \textit{picture}, or a domain-specific term, such as \textit{clipart} or \textit{painting}. The terms are provided in \cref{tab:experiments:keywords}.

\begin{table}[t]
    \centering
    \caption{\textbf{Generic, domain-invariant and domain-specific terms.} Generic, domain-invariant terms are shared across domains, while domain-specific terms can be either its domain name or a synonym.}
    \begin{tabular}{cc}
        \toprule
        Domain & Terms \\
        \midrule
        Generic & image, picture \\\midrule
        Clipart & clipart, illustration \\
        Infograph & infograph, informational chart \\
        Painting & painting, art \\
        Quickdraw & quickdraw, doodle \\
        Real & photo, snapshot \\
        Sketch & sketch, drawing \\
        \bottomrule
    \end{tabular}
    \label{tab:experiments:keywords}
\end{table}

\subsection{Further Details on the Training Data Construction}\label{app:data_construction}
\paragraph{Dataset Construction}
We created our training datasets based on a base dataset $D_0$ (\ie, ImageNet-Captions, CC3M, or CC12M) which provides a large collection of (mostly) natural images along with corresponding language descriptions. 
To address issues related to class shifts, we then augmented the base dataset with samples from DomainNet-Real, which consists of natural images. Finally, we created different domain mixtures (\cref{fig:dataset-setup}) by incorporating subsets of samples from various non-natural domains of DomainNet $D_r$ with $r\in\{\text{Clipart},\; \text{Infograph},\; \text{Painting},\; \text{Quickdraw},\; \text{Sketch}\}$ into the training data, as outlined in \cref{sec:problem-setup}.

\paragraph{Subsampling}
To ensure fair performance comparisons between the different domain mixtures (\cref{fig:dataset-setup}), we applied subsampling to maintain comparable final dataset sizes when adding additional domains to the training data. We designed our subsampling method to preserve the original data distribution as much as possible. Specifically, if domain $D_i$ contained twice as many samples as domain $D_j$ before subsampling, this ratio remained approximately constant afterward. Likewise, the class distribution within each domain was preserved as much as possible.

Note that the non-natural domains in DomainNet vary significantly in size, \eg, quickdraw has more than three times as many samples as clipart. Thus, we chose to only keep dataset sizes fixed within the same test domain. That is, for a given test domain, CG low-diversity and CG high-diversity datasets have the same size. However, dataset sizes may differ across test domains (\ie, CG low-diversity settings for different test domains are not necessarily of equal size). Since the Natural-only lower bound is independent of the choice of the test domain, it contains slightly fewer samples than the other mixtures.

\subsection{Choice of Classes $C_2$}\label{sub:class-choices}
We carefully chose the subset $C_2$ in a way that the classes are diverse and not biased towards any spurious features (\eg, color). We considered the 147 classes, with a one-to-one match in ImageNet (see \cref{app:in-dn-mapping}), as the possible candidates for $C_2$. We selected about 10\% of these candidates, \ie, 15 classes. For the selection process, we randomly sampled from the set of candidates. To ensure that we adequately covered the different super-categories of DomainNet (\eg., furniture, mammal, tool, \etc), we kept only the first random sample for each category, rejecting further samples from the same category. We also manually rejected some samples if we considered them to be too similar to our existing selection. Our final selection of classes is 
\begin{equation}
\begin{split}
    C_2 = \{&\text{aircraft carrier}, \ \text{axe}, \ \text{banana}, \ \text{barn}, \ \text{bed}, \ \text{candle}, \ \text{lion}, \ \text{mountain}, \\
    &\text{necklace}, \ \text{penguin}, \ \text{pizza}, \ \text{saxophone}, \ \text{television}, \ \text{tractor}, \ \text{traffic light}\}.
\end{split}
\end{equation}
and $C_1$ are all other 330 classes of DomainNet.

\subsection{DomainNet Evaluation Prompts}\label{app:dn-prompts}
We evaluated the zero-shot performance of our CLIP models using the OpenAI templates from \citet{radford2021learning}. Since painting and sketch templates are already contained, we added the templates for the missing domain names:
\begin{itemize} 
    \item ``\texttt{a clipart of the \{class\}.}'',
    \item ``\texttt{a clipart of a \{class\}.}'',
    \item ``\texttt{an infograph of the \{class\}.}'',
    \item ``\texttt{an infograph of a \{class\}.}'',
    \item ``\texttt{a quickdraw of the \{class\}.}'',
    \item ``\texttt{a quickdraw of a \{class\}.}''.
\end{itemize}
Following \citet{radford2021learning}, we created zero-shot weights for all 345 DomainNet Classes from these templates by taking the class-wise average over the text embeddings of all templates (marginalization to obtain the ``true'' object embedding) and normalizing afterwards. Formally, let $c \in \{c_1,\dots,c_n\}$ be a class, $T$ be the set of templates, $t_{c}$ a template with the name of class $c$ inserted, and $g$ be the text encoder of our CLIP model. Assuming that $g$ produces $L_2$-normalized embeddings, we computed the zero-shot weights of $c$ as:
\begin{equation}
    \mathbf{w}_c = \frac{\frac{1}{\vert T \vert} \sum_{t \in T} g(t_c)}{\left\Vert \frac{1}{\vert T \vert} \sum_{t \in T} g(t_c) \right\Vert_2}.
\end{equation}

\subsection{CLIP Training Details}\label{app:clip-training-details}
Following \citet{fang2022data}, we trained CLIP models with an embedding size of 1024 with ResNet-50 \citep{he2016deep} and a transformer text encoder \citep{vaswani2017attention} (12 layers with a width of 512, 8 attention heads, and context length of 77).
We trained the models for 32 epochs with a batch size of 1024 with AdamW (learning rate of 0.001, $\beta_1=\text{0.9}$, $\beta_2=\text{0.999}$, $\epsilon=\text{1e-8}$, weight decay of 0.2) and cosine annealing learning rate scheduling with 500 warmup steps. We used the default data augmentations of OpenCLIP. We used the code from OpenCLIP \citep{cherti2023reproducible} (\url{https://github.com/mlfoundations/open_clip}, License: custom) for our training implementation.

\subsection{Supervised Classifier Details}\label{app:supervised-training-details}
\paragraph{Training Details}
Similar to \citet{fang2022data}, we trained the supervised ResNet-50 classifiers \citep{he2016deep} for 90 epochs with a batch size of 256 using SGD with Nesterov momentum, weight decay of 1e-4, momentum of 0.9, initial learning rate of 0.01 (they used 0.1) with step-wise decay by 0.1 at epochs 30, 50, and 70. We used the same image augmentations as for our CLIP models.

\paragraph{Joining the Class Distributions of ImageNet and DomainNet}\label{app:in-dn-mapping}
Both ImageNet-Captions and DomainNet provide class labels for training supervised classifiers. However, their class distributions differ significantly in diversity and granularity. ImageNet has nearly three times as many classes as DomainNet and is more fine-grained. For example, ImageNet distinguishes over 100 different dog breeds, whereas DomainNet has only a single dog class.

To address this, we created a mapping from ImageNet classes to DomainNet classes. Each ImageNet class was either mapped to a single DomainNet class or left unmatched, and multiple ImageNet classes could be mapped to the same DomainNet class. We constructed this mapping manually based on class names, the WordNet hierarchy \citep{miller1995wordnet}, and the NAVIGU image explorer (\url{https://navigu.net/#imagenet}, \citet{barthel2023navigu}), ensuring that only semantically valid mappings were retained.

Our final mapping assigned 450 ImageNet classes to DomainNet, including 147 one-to-one mappings. Using this mapping, we merged the class distributions of ImageNet and DomainNet. Specifically, we relabeled the 450 mapped ImageNet classes with their corresponding DomainNet labels. The remaining 550 unmatched ImageNet classes were combined with the 345 DomainNet classes, resulting in a total of 895 classes. We then trained our supervised classifiers on these 895 classes.

\section{Additional Results For Section~\ref{sec:exp_when}}\label{app:additional-results}
In \cref{fig:effective-robustness}, we compared the effective robustness trends of our different training setup across different domains. \cref{tab:main-result} shows the final test performances (averaged over three runs) of our CLIP models with ImageNet-Captions as the base dataset and ResNet-50 as the vision encoder.

\begin{table*}[h]
    \centering
    \caption{\textbf{CLIP robustness results for the considered experimental conditions (\cref{fig:dataset-setup})}. We repeated CLIP trainings three times and evaluated the models on the unseen classes of the test domain. CLIP generalizes better with higher domain diversity. Intriguingly, CLIP achieves superior performance when not seeing the domain at all \vs seeing a subset of it. However, while diversity substantially improves CLIP's generalization performance, there remains a performance gap to a model that has seen similar samples to the classes $C_2$.
    \cref{fig:effective-robustness} shows the respective effective robustness plots.
    }
    \label{tab:main-result}
    \begin{tabular}{lccccc}
        \toprule
        Data composition (\cref{fig:dataset-setup}) & Clipart & Infograph & Painting & Quickdraw & Sketch \\
        \midrule
        Natural-only & $20.3 \pm 0.7$ & $11.8 \pm 0.2$ & $34.1 \pm 1.4$ & $0.8 \pm 0.1$ & $19.5 \pm 0.7$ \\
        \midrule
        Leave-out-domain           & $27.4 \pm 1.2$ & $13.6 \pm 0.6$ & $33.8 \pm 0.5$ & $4.8 \pm 1.1$ & $30.1 \pm 1.4$ \\
        \midrule
        CG low-diversity & $17.1 \pm 1.2$ & $10.8 \pm 1.0$ & $31.4 \pm 0.1$ & $2.0 \pm 0.9$ & $19.2 \pm 2.0$ \\
        \textcolor{gray}{\hspace{1em} w/ classes $C_2$ (upper bound)} & \textcolor{gray}{$37.2 \pm 0.8$} & \textcolor{gray}{$21.5 \pm 1.8$} & \textcolor{gray}{$45.5 \pm 0.8$} & \textcolor{gray}{$56.0 \pm 0.6$} & \textcolor{gray}{$50.3 \pm 1.2$} \\
        CG high-diversity   & $27.6 \pm 1.5$ & $12.7 \pm 2.0$ & $34.6 \pm 1.2$ & $1.7 \pm 0.7$ & $28.1 \pm 0.8$ \\
        \textcolor{gray}{\hspace{1em} w/ classes $C_2$ (upper bound)}   & \textcolor{gray}{$36.6 \pm 1.0$} & \textcolor{gray}{$18.8 \pm 0.3$} & \textcolor{gray}{$41.8 \pm 1.4$} & \textcolor{gray}{$51.5 \pm 1.9$} & \textcolor{gray}{$44.3  \pm 0.8$} \\
        \bottomrule
    \end{tabular}
\end{table*}

\cref{fig:domain-itp} shows which domains are the most helpful to include for generalizing to sketch images, and reveals that clipart and painting contribute strongly, while infograph and quickdraw have little benefit or even slightly decrease performance. We also conducted the same experiment with clipart as the test domain. \cref{fig:domain-itp-clipart} shows that sketches are also the most helpful domain to include for generalizing to clipart, suggesting that both domains might leverage similar visual features. The inclusion order of the remaining domains remained the same as in the sketch experiment with the added difference that after including painting, both infograph and quickdraw slightly decreased generalization performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=.5\linewidth]{figures/sns-domain-interpolation-clipart-all}
    \caption{\textbf{Domain-wise robustness gains for clipart.} \cref{fig:domain-itp} shows that clipart is the most helpful domain to include for generalizing to sketches. Similarly, sketches are also the most helpful for generalizing to clipart.}
    \label{fig:domain-itp-clipart}
\end{figure}

\subsection{Validity of the Results across Architecture, Dataset, and Loss Choices}\label{app:other_choices}
We conducted the experiments from the main text (\cref{fig:effective-robustness,tab:main-result}) using ImageNet-Captions as the base dataset and a ResNet-50 vision encoder. To ensure the consistency of our findings across different base datasets, vision encoder architectures, and contrastive loss functions, we performed additional experiments by systematically varying each of these components. Due to computational resource constraints, we repeated these experiments only for the clipart and sketch domains, where increasing domain diversity yielded the most significant robustness gains.

\paragraph{Architecture}
For the architecture experiments, we trained two CLIP configurations with different image encoder architectures. The first configuration used a Swin-T \citep{liu2021swin}, the same text encoder as in our ResNet-50 experiments, and an embedding dimension of 512. The second configuration used a ViT-S-32 \citep{touvron2021training}, a slightly smaller text encoder with a width of 384, only six attention heads, and also a smaller embedding dimension of 384. In addition, for ViT-S-32, we used slightly adjusted AdamW hyperparameters, \ie, $\beta_2 = 0.98$ and $\epsilon = 10^{-6}$. All other hyperparameters were consistent with the ResNet-50-based experiments (see \cref{app:clip-training-details}). 

\cref{tab:architecture,fig:effective-robustness:architectures} confirm that for transformer-based vision encoders, robustness also improves with increasing domain diversity, as expected. Similarly, compositional generalization performs worse than domain generalization.
\begin{table*}[h]
    \centering
    \caption{\textbf{Results when varying CLIP's vision encoder.} We find similar trends across these vision encoder choices.}
    \label{tab:architecture}
    \begin{tabular}{cccc}
        \toprule
        Vision encoder & Training data setup & Clipart & Sketch \\
        \midrule
        \multirow{4}{*}{ResNet-50} & Natural-only        & 19.4 & 19.6 \\
                                   & Leave-out-domain                  & 27.1 & 31.8 \\
                                   & CG low-diversity & 18.7 & 16.4 \\
                                   & CG high-diversity   & 28.6 & 28.6 \\
        \midrule
        \multirow{4}{*}{ViT-S-32} & Natural-only        & 12.0 & 5.9 \\
                                  & Leave-out-domain                  & 15.1 & 8.2 \\
                                  & CG low-diversity & 12.7 & 5.8 \\
                                  & CG high-diversity   & 13.8 & 8.3 \\
        \midrule
        \multirow{4}{*}{Swin-T} & Natural-only        & 17.1 & 11.3 \\
                                & Leave-out-domain                  & 20.3 & 15.2 \\
                                & CG low-diversity & 17.4 & 10.4 \\
                                & CG high-diversity   & 22.3 & 14.0 \\
        \bottomrule
    \end{tabular}
\end{table*}
\begin{figure*}[h]
    \hfill
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-clipart-ViT-S-32}
        \caption{Clipart with ViT-S-32.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-sketch-ViT-S-32}
        \caption{Sketch with ViT-S-32.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-clipart-swin_tiny_patch4_window7_224}
        \caption{Clipart with Swin-T.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-sketch-swin_tiny_patch4_window7_224}
        \caption{Sketch with Swin-T.}
    \end{subfigure}
  \caption{\textbf{Effective robustness plots for different vision encoders.} Refer to \cref{fig:effective-robustness} for the effective robustness plots for the ResNet-50 vision encoder.}
  \label{fig:effective-robustness:architectures}
\end{figure*}

\paragraph{Base Dataset}
For the base dataset experiments, we trained our ResNet-50 CLIP configuration using CC3M and CC12M as the base datasets. Following \citet{radford2021learning}, we used a maximum learning rate of 5e-4. We further adjusted the number of warmup steps to 2000 and the batch size to 2048. All other hyperparameters were consistent with the ResNet-50-based experiments (\cref{app:clip-training-details}).

\cref{tab:dataset,fig:effective-robustness:datasets} show that both CC3M and CC12M exhibit the same robustness trends as our ImageNet-Captions models. However, the robustness gains are smaller compared to ImageNet-Captions. This is most likely due to the reduced relative weighting of the DomainNet images with larger base dataset sizes, as well as the higher inherent diversity of CC3M and CC12M. For example, both CC3M and CC12M include some non-natural images, which may diminish the effect of further increasing domain diversity. Note that compositional generalization seems to be slightly better than domain generalization for the larger datasets, which we attribute to domain contamination (see \cref{app:challenge-cg} why this may benefit compositional generalization). We leave further investigation into the effect of scale and diversity of the base dataset for future work.

\begin{table*}[h]
    \centering
    \caption{\textbf{Results when varying the base dataset $D_0$.} We observe similar trends as for CC3M and CC12M as for ImageNet-Captions. The only exception is that compositional generalization now tends to always work better than domain generalization for CC12M. We attribute this to a domain contamination of CC12M, \ie, CC12M contains a lot of sketch and clipart images.}
    \label{tab:dataset}
    \begin{tabular}{cccc}
        \toprule
        Base dataset $D_0$ & Training data setup & Clipart & Sketch \\
        \midrule
        \multirow{4}{*}{ImageNet-Captions} & Natural-only        & 19.4 & 19.6 \\
                                           & Leave-out-domain                  & 27.1 & 31.8 \\
                                           & CG low-diversity & 18.7 & 16.4 \\
                                           & CG high-diversity   & 28.6 & 28.6 \\
        \midrule
        \multirow{4}{*}{CC3M} & Natural-only        & 32.2 & 31.5 \\
                              & Leave-out-domain                  & 31.6 & 35.1 \\
                              & CG low-diversity & 28.4 & 30.5 \\
                              & CG high-diversity   & 35.5 & 33.6 \\
        \midrule
        \multirow{4}{*}{CC12M} & Natural-only        & 39.6 & 48.3 \\
                               & Leave-out-domain                  & 46.1 & 51.2 \\
                               & CG low-diversity & 38.5 & 41.7 \\
                               & CG high-diversity   & 48.7 & 51.7 \\
        \bottomrule
    \end{tabular}
\end{table*}
\begin{figure*}[h]
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-clipart-cc3m}
        \caption{Clipart with CC3M.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-sketch-cc3m}
        \caption{Sketch with CC3M.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-clipart-cc12m}
        \caption{Clipart with CC12M.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-sketch-cc12m}
        \caption{Sketch with CC12M.}
    \end{subfigure}
  \caption{\textbf{Effective robustness plots for the different base datasets.} Refer to \cref{fig:effective-robustness} for the effective robustness plots for the ImageNet-Captions dataset.}
  \label{fig:effective-robustness:datasets}
\end{figure*}

\paragraph{Contrastive Loss}
For the loss experiments, we trained our ResNet-50 CLIP configuration but replaced the standard CLIP loss with the SigLIP loss \citep{zhai2023sigmoid}. 

\cref{tab:siglip,fig:effective-robustness:siglip} show that our SigLIP models are consistent with our observations from the experiments in the main text.
\begin{table*}[h]
    \centering
    \caption{\textbf{Results when using a different loss function (SigLIP \citep{zhai2023sigmoid}).} The choice of loss function does not change the trends observed for CLIP's original contrastive loss.}
    \label{tab:siglip}
    \begin{tabular}{cccc}
        \toprule
        Loss function & Training data setup & Clipart & Sketch \\
        \midrule
        \multirow{4}{*}{CLIP}   & Natural-only & 19.4 & 19.6 \\
                                & Leave-out-domain & 27.1 & 31.8 \\
                                & CG low-diversity & 18.7 & 16.4 \\
                                & CG high-diversity   & 28.6 & 28.6 \\
        \midrule
        \multirow{4}{*}{SigLIP} & Natural-only & 20.4 & 19.7 \\
                                & Leave-out-domain & 28.1 & 26.1 \\
                                & CG low-diversity & 19.2 & 16.4 \\
                                & CG high-diversity   & 25.7 & 27.5 \\
        \bottomrule
    \end{tabular}
\end{table*}
\begin{figure*}[h]
    \centering
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-clipart-siglip}
    \end{subfigure}
    \hspace{1em}
    \begin{subfigure}[c]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-sketch-siglip}
    \end{subfigure}
  \caption{\textbf{Effective robustness plots for SigLIP.} Refer to \cref{fig:effective-robustness} for effective robustness plots when using CLIP's original contrastive loss.}
  \label{fig:effective-robustness:siglip}
\end{figure*}

\subsection{Challenges of Compositional Generalization}\label{app:challenge-cg}
In \cref{sec:cg-challenges}, we found that compositional generalization settings can suffer from an exposure to a subset of classes during training that are later queried in evaluation. This can make compositional generalization fail but can be alleviated by using domain samples from classes that do not overlap with the class distribution which is queried during evaluation. \Cref{tab:cg-overfitting} shows that replacing \textit{all} DomainNet sketches with sketches that do not overlap with DomainNet's classes (see \cref{app:in-dn-mapping}), using ImageNet-Sketch, improves compositional generalization performance from 28.1\% to 36.9\%. 

In practice, however, enforcing little to no class overlap may not always be feasible; particularly in zero-shot settings where CLIP is applied to data and/or tasks that are unknown at training time. Therefore, we investigated the severity of this in greater detail. To do this, we partitioned the set of classes $C = C_1 \cup C_2 \cup C_3$ in our test domain $D_i$ into three disjoint subsets (\cref{fig:cg-severity:illustration}): 
\begin{itemize}
    \item $C_1$: Classes that are seen during training and are queried during evaluation.
    \item $C_2$: Classes that are \emph{not} seen during training and are queried during evaluation. Note that only classes from $C_2$ are contained in the test set $D_i^{C_2}$.
    \item $C_3$: Classes that are seen during training and are \emph{not} queried during evaluation.
\end{itemize}
This partitioning allowed us to systematically assess the impact of class overlap on compositional generalization by constructing training sets with varying mixtures of classes from $C_1$ and $C_3$.

For this experiment, we selected the sketch domain as our test domain.\footnote{We chose the sketch domain due to the availability of a large class distribution from ImageNet-Sketch.} The subsets $C_1$ and $C_2$ were defined as described in \cref{sec:problem-setup}, meaning that $C_1 \cup C_2$ represents the class distribution queried during evaluation (\ie, all DomainNet classes). For $C_3$, we used classes from ImageNet-Sketch \citep{wang2019learning} that do not overlap with the classes from DomainNet (see \cref{app:in-dn-mapping} for more details). Note that throughout all these experiments, the class distribution in the other domains $D_{j\neq i}$ remained unchanged--that is, classes from $C_1$ and $C_2$ were included in training, while no classes from $C_3$ were introduced.

As shown in \cref{tab:cg-overfitting,fig:cg-severity:results} (rightmost bar), training exclusively on samples from $C_3$ significantly improves compositional generalization performance compared to not seeing any test domain samples from $D_i$ (zero line) or only seeing samples from classes $C_1$ of $D_i$ (leftmost bar). To further investigate this, we trained CLIP models using samples from $C_1 \cup C_3$ of the test domain $D_i$ and gradually reduced the number of classes from $C_1$, while keeping all other factors fixed. \Cref{fig:cg-severity:results} shows that reducing class overlap (moving from left to right)--\ie, decreasing the number of $C_1$ classes seen during training--consistently improves compositional generalization performance. This reaffirms the detrimental effect of class overlap, as demonstrated in \cref{tab:cg-overfitting}, but also highlights that increasing class diversity can help mitigate its impact on compositional generalization performance.

\begin{figure}[h]
    \centering
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{figures/data-curation}
        \vspace{0.1cm}
        \caption{Classes from the test domain.}
        \label{fig:cg-severity:illustration}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.65\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-cg-class-overlap}
        \caption{Effect of different mixtures of the test domain's classes $C_1 \cup C_3$ seen during training.}
        \label{fig:cg-severity:results}
    \end{subfigure}
    \hfill
    \caption{\textbf{Severity of class overlap on compositional generalization.}
    \subref{fig:cg-severity:illustration}: We partitioned the classes $C$ in the test domain $D_i$ into three disjoint subsets: $C=C_1 \cup C_2 \cup C_3$. During evaluation, both $C_1$ and $C_2$ are included in the query set, allowing the model to predict any class from these subsets. However, the actual test set, $D_i^{C_2}$, only contains samples from $C_2$. In contrast, the classes in $C_3$ are excluded from both the query and test sets. This partitioning allows us to investigate the severity of class overlap for compositional generalization performance. In particular, we found that including classes of $C_1$ led to a significant drop in compositional generalization--even performing worse than domain generalization (\cref{fig:effective-robustness}). On the other hand, replacing the class samples from $C_1$ with the ones of $C_3$ (which are not part of the query set) resulted in a significant improvement in compositional generalization (\cref{tab:cg-overfitting}).
    %
    \subref{fig:cg-severity:results}:
    To investigate the severity, we varied the mixture of classes from $C_1$ and $C_3$. We find that reducing classes from $C_1$ (moving from left to right), while keeping all other factors fixed, steadily improves compositional generalization performance (with the exceptions for the class mixtures 25\% $C_1$ + 100\% $C_3$ and 1 class $C_1$ + 100\% $C_3$).}
    \label{fig:cg-severity}
\end{figure}

\paragraph{Supervised classifiers}
We also investigated to what extent supervised classifiers are vulnerable to this bias. \Cref{tab:cg-overfitting-sv} confirms that supervised classifiers are also susceptible to it.
\begin{table}[t]
    \centering
    \caption{\textbf{Supervised classifiers are also susceptible to the seen class bias.} Supervised classifiers' compositional generalization also deteriorates due to a partial test domain overlap and replacing them with samples from non-overlapping classes significantly improves compositional generalization.}
    \begin{tabular}{lc}
        \toprule
        Training data setup (\cref{fig:dataset-setup}) & Sketch \\
        \midrule
        Leave-out-domain  & 27.4 \\
        \midrule
        CG high-diversity & 22.2 \\
        \hspace{1em} w/ sketches of non-queried classes only & 30.1 (+7.9) \\
        \bottomrule
    \end{tabular}
    \label{tab:cg-overfitting-sv}
\end{table}

\subsection{Closing The Generalization Gap}\label{app:generalization-gap}
\cref{tab:main-result} clearly shows that there is a significant performance gap between the CG high-diversity with and without including domain-specific samples of our test classes $C_2$, even for domains like clipart and sketch where generalization seems to work reasonably well. We conducted an interpolation experiment between the two settings with and without such samples to better understand how many of these samples are actually required to close this ``generalization gap''. To further investigate how the generalization capability of a model impacts the required number of samples, we also performed the same experiment for the CG low-diversity setting.

For a given test domain $D_i$, we considered the number of samples of our test set $D_i^{C_2}$ to be 100\% and then trained additional CLIP models in which we successively added 5\%, 10\%, 15\%, 20\%, 40\%, 60\%, and 80\% of these samples to the training data. Note that we ensured that the overall dataset size remained unchanged.

\cref{fig:upper-bound-itp} shows that performance on the classes seems to follow a roughly linear relationship with the number of samples allowed for all domains, except for the quickdraw domain. For quickdraw, the relationship seems to be log-linear instead, which may be due to the fact that CLIP does not generalize at all for quickdraw. This observation also relates to the findings of \citet{udandarao2024no}, who predict that a linear increase in samples leads only to a log-linear increase in zero-shot performance. Across all domains and settings, we observed that to achieve the maximally possible performance, 100\% of the samples from $D_i^{C_2}$ are required.

\begin{figure*}
    \centering
    \hfill
    \begin{subfigure}[c]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-allowed-samples-clipart}
        \caption{Clipart.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-allowed-samples-infograph}
        \caption{Infograph.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-allowed-samples-painting}
        \caption{Painting.}
    \end{subfigure} 
    \hfill \\
    \bigskip
    \hspace*{\fill}
    \begin{subfigure}[c]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-allowed-samples-quickdraw}
        \caption{Quickdraw.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-allowed-samples-sketch}
        \caption{Sketch.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{\textbf{Interpolation between CG settings with and without domain-specific samples of the test classes $C_2$.} For each interpolation, we fitted both a linear and a log-linear regression model and visualized the fit with a lower mean squared error (MSE). Performance on the test classes appears to follow a roughly linear relationship with the number of test samples included, except for quickdraw, which shows a strong log-linear relationship.}
    \label{fig:upper-bound-itp}
\end{figure*}

\subsection{Role of Language Supervision}\label{app:language-supervision}
Previous studies comparing the robustness of CLIP models and supervised classifiers either examined models trained on different datasets or focused on low-diversity datasets consisting mostly of natural images, such as ImageNet-Captions \citep{fang2022data}. Since both ImageNet-Captions and DomainNet provide class labels, we investigated how our domain mixtures affect the robustness of supervised classifiers and compared the results to CLIP models.

\cref{tab:supervised,fig:effective-robustness-supervised-full} show the results of our experiments on supervised classifiers. We found that CLIP models consistently exhibit slightly higher robustness than their supervised counterparts, which may be due to the richness of captions \citep{xue2024understanding,wen2024makes}. Interestingly, CLIPâ€™s advantages are more pronounced in compositional generalization settings (CG low/high-diversity). However, in the domain generalization setting (Leave-out-domain), supervised classifiers can sometimes achieve generalization comparable to CLIP.

\begin{table*}[t]
    \centering
    \caption{\textbf{Performance comparison across supervised experiments.}
    The robustness of supervised models also increases with domain diversity. However, supervised classifiers generalize slightly worse than CLIP models (\cref{fig:effective-robustness-supervised,fig:effective-robustness-supervised-full}).}
    \label{tab:supervised}
    \begin{tabular}{lccccc}
        \toprule
        & Clipart & Infograph & Painting & Quickdraw & Sketch \\
        \midrule
        Natural-only      & $17.6 \pm 1.6$  & $10.9 \pm 0.8$ & $32.6 \pm 1.6$ &  $0.7 \pm 0.1$ & $15.0 \pm 1.1$ \\
        \midrule
        Leave-out-domain  & $30.8 \pm 2.1$ & $14.5 \pm 1.2$ & $35.7 \pm 1.2$ & $10.1 \pm 0.5$ & $27.4 \pm 2.1$ \\
        \midrule
        CG low-diversity  & $13.2 \pm 0.6$ &  $9.7 \pm 0.5$ & $25.1 \pm 0.9$ &  $0.0 \pm 0.0$ & $12.0 \pm 1.1$ \\
        CG high-diversity & $25.0 \pm 1.6$ & $12.2 \pm 0.7$ & $28.6 \pm 0.4$ &  $0.7 \pm 0.4$ & $22.2 \pm 0.7$ \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}
    \centering
    \hfill
    \begin{subfigure}[c]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-clipart-supervised}
        \caption{Clipart.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-infograph-supervised}
        \caption{Infograph.}
    \end{subfigure}
    \hfill \\
    \bigskip
    \hfill
    \begin{subfigure}[c]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-painting-supervised}
        \caption{Painting.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-quickdraw-supervised}
        \caption{Quickdraw.}
    \end{subfigure}
    \hfill \\
    \bigskip
    \begin{subfigure}[c]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-sketch-supervised}
        \caption{Sketch.}
    \end{subfigure}
    \caption{\textbf{Effective robustness plots for CLIP vs. supervised classifiers.} While the general trends are very similar between CLIP models and supervised classifiers, CLIP models typically generalizes better than the supervised classifiers.}
    \label{fig:effective-robustness-supervised-full}
\end{figure*}

\section{Additional Technical Details and Results for Section~\ref{sec:analysis}}
\subsection{Technical Details on Sparse Autoencoders and Additional Results}\label{app:sae}
Following \citep{bricken2023monosemanticity,huben2024sparse}, we used a \acrfull{sae} to extract interpretable features from CLIP's visual embeddings $\mathbf{a}\in\mathbb{R}^p$. The \acrshort{sae} is defined as follows:
\begin{equation}
    \text{SAE}(\mathbf{a}):=(g\circ\phi\circ f)(\mathbf{a}),
\end{equation}
where $\phi$ is a ReLU non-linearity, and $f$ and $g$ are linear encoder with weights $\mathbf{W}_f\in\mathbb{R}^{p\times h}$ or decoder with weights $\mathbf{W}_g\in\mathbb{R}^{h\times p}$, respectively. We trained the \acrshort{sae} with an $L_2$ reconstruction loss and $L_1$ sparsity regularization:
\begin{equation}
    \mathcal{L}(\mathbf{a}) = \Vert \mathbf{a} - (g\circ\phi\circ f)(\mathbf{a}) \Vert_2^2 + \lambda \Vert (\phi\circ f)(\mathbf{a}) \Vert_1,
\end{equation}
where $\lambda$ governs the sparsity regularization strength.

We trained \acrshort{sae}'s on the activations of our CC12M CLIP models\footnote{We also tried the CLIP models that were trained with ImageNet-Captions as base dataset but found that the \acrshort{sae} extracted poorly interpretable features.} (see \cref{app:other_choices}). We used CC12M and the complete DomainNet training set to train the \acrshort{sae}'s to identify interpretable features. The hidden dimension $h$ was set to 4096, \ie, 4x the embedding dimension of the CLIP model's output dimensionality. We trained the \acrshort{sae} for 200 epochs using a batch size of 4096. The regularization strength hyperparameter $\lambda$ was set to 1e-4. To alleviate the dying neuron problem, dead neurons were resampled every 500,000 training steps. Our implementation is based on the code published by \citet{rao2024discover} (\url{https://github.com/neuroexplicit-saar/discover-then-name}, License: MIT).

\subsection{Technical Details on Center Kernel Alignment and Additional Results}\label{app:cka}
Let $\mathbf{X}^{d_1} \in \mathbb{R}^{C\times p}$ and $\mathbf{Y}^{d_2} \in \mathbb{R}^{C\times p}$ contain the $C$ mean visual embeddings of each class for the domains $d_1$ or $d_2$, respectively. Then, we compute the Gram matrices/kernels $\mathbf{K}=\mathbf{X}^{d_1}(\mathbf{X}^{d_1})^T$ and $\mathbf{L}=\mathbf{Y}^{d_2}(\mathbf{Y}^{d_2})^T$ that contain the pairwise similarities of each pair of mean class embeddings. Note that we used a linear kernel here, as commonly done in the representational similarity literature. Alternatively, we also tried a non-linear kernel (\ie, RBF kernel) with similar results (see \cref{fig:non-linear-cka}).
Center kernel alignment is defined by \citet{kornblith2019similarity} as follows:
\begin{equation}
    \text{CKA}(\mathbf{K}, \mathbf{L})=\frac{\text{HSIC}(\mathbf{K}, \mathbf{L})}{\sqrt{\text{HSIC}(\mathbf{K}, \mathbf{K})\text{HSIC}(\mathbf{L}, \mathbf{L})}} ,
\end{equation}
where we used the used the unbiased Hilbert-Schmidt Independence Criterion (HSIC) estimator \citep{song2012feature} following \citet{nguyen2021do}, defined as follows:
\begin{equation}
        \text{HSIC}(\mathbf{K},\mathbf{L})=\frac{1}{C(C-3)}\Bigl( tr(\Tilde{\mathbf{K}}\Tilde{\mathbf{L}})
        +\frac{\mathbf{1}^T\Tilde{\mathbf{K}}\mathbf{1}\mathbf{1}^T\Tilde{\mathbf{L}}\mathbf{1}}{(C-1)(C-2)}-\frac{2}{C-2}\mathbf{1}^T\Tilde{\mathbf{K}}\Tilde{\mathbf{L}}\mathbf{1}\Bigr),
\end{equation}
where we set the diagonal elements of $\mathbf{K}$ and $\mathbf{L}$ to zero in
$\Tilde{\mathbf{K}}$ or $\Tilde{\mathbf{L}}$, respectively.

We computed center kernel alignment between all pairs of domains for each class. Thereby, we can assess the representational similarity for each class across the domains. For visualization, we averaged over all classes to obtain a representational similarity estimate for each domain. In the main text, we further averaged together the non-quickdraw domains (all domains are shown in \Cref{fig:linear-cka,fig:non-linear-cka}).

\paragraph{Additional results}
\Cref{fig:linear-cka,fig:non-linear-cka} show results for all classes $C$, the classes seen during training $C_1$, and the unseen classes $C_2$. Interestingly, we find that the representational gap widens for the unseen classes $C_2$.
\begin{figure}[t]
    \centering
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_linear_cka_hsic1_mean_all_quickdraw}
        \caption{All classes.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_linear_cka_hsic1_mean_id_quickdraw}
        \caption{Seen classes only.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_linear_cka_hsic1_mean_ood_quickdraw}
        \caption{Unseen classes only.}
        \label{subfig:linear-cka-unseen}
    \end{subfigure} \\
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_linear_cka_hsic1_mean_all_domains}
        \caption{All classes.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_linear_cka_hsic1_mean_id_domains}
        \caption{Seen classes only.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_linear_cka_hsic1_mean_ood_domains}
        \caption{Unseen classes only.}
        \label{subfig:linear-cka-unseen-domains}
    \end{subfigure}
    \caption{\textbf{Linear center kernel alignment (CKA) similarity.} Quickdraw has the lowest representational similarity across domains. This particularly emphasized for the unseen classes $C_2$ (\subref{subfig:linear-cka-unseen}, \subref{subfig:linear-cka-unseen-domains}).}
    \label{fig:linear-cka}
\end{figure}
\begin{figure}[t]
    \centering
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_kernel_cka_hsic1_mean_all_quickdraw}
        \caption{All classes.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_kernel_cka_hsic1_mean_id_quickdraw}
        \caption{Seen classes only.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_kernel_cka_hsic1_mean_ood_quickdraw}
        \caption{Unseen classes only.}
    \end{subfigure} \\
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_kernel_cka_hsic1_mean_all_domains}
        \caption{All classes.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_kernel_cka_hsic1_mean_id_domains}
        \caption{Seen classes only.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_kernel_cka_hsic1_mean_ood_domains}
        \caption{Unseen classes only.}
    \end{subfigure}
    \caption{\textbf{Non-linear center kernel alignment (CKA) similarity.} The results follow the same pattern as the linear CKA (\cref{subfig:repr-similarity,fig:linear-cka}): Quickdraw is the most representational dissimilar domain.}
    \label{fig:non-linear-cka}
\end{figure}

\subsection{Technical Details on the Circuit Similarity Analysis and Additional Results}\label{app:act_patching}
Attributing the causal effects of model components is a key goal of (mechanistic) interpretability research \citep{meng2022locating,marks2024sparse,mueller2024quest}. We drew inspiration from them to analyze the level of sharing of the most important CLIP's model components, \ie, the axis-aligned neurons in its vision encoder, across domains. To do this, we first must attribute their importance via the indirect effect \citep{pearl2001direct}.

Let $m_{:l}(I^d_\text{clean})=\mathbf{a}^{l,n}_\text{clean}\in\mathbf{p}$ be a $p$-dimensional neuron $n$ in the $l$-th layer of model $m$ for input image $I_\text{clean}$. Further, we define $\mathbf{a}^{l,n}_\text{patch}\in\mathbf{p}$ as a corrupted baseline. The computation of the indirect effect is defined as follows:
\begin{equation}
    \text{IE}=
    m_{L,c}(I^d_\text{clean}|\text{do}(\mathbf{a}^l=\mathbf{a}^{l,n}_\text{patch}))-m_{L,c}(\mathbf{a}^{l,n}_\text{clean}),
\end{equation}
where $m_{L,c}$ is the output logit for class $c$ of the last layer $L$ (note that we can obtain this logit through the dynamic zero-shot weights that can be generated by CLIP's text encoder, see \cref{app:dn-prompts}) and $\text{do}(\mathbf{a}^l=\mathbf{a}^{l,n}_\text{patch})$ denotes the do-operator \citep{pearl2009causality} that intervenes on the computation of the CLIP model by setting the activations $\mathbf{a}^l$ to $\mathbf{a}^{l,n}_\text{patch}$.

Since there are lot of neurons in CLIP's vision encoder, we sped up the computation through a linear approximation using integrated gradients \citep{sundararajan2017axiomatic}, following \citet{marks2024sparse}:
\begin{equation}\label{eq:indirect-effect}
    \hat{\text{IE}}_\text{ig}=\left( \sum\limits_\alpha \nabla_{\mathbf{a}^{l,n}} m_{l:}(\alpha\mathbf{a}^{l,n}_\text{clean}+(1-\alpha)\mathbf{a}^{l,n}_\text{patch}) \right)(\mathbf{a}^{l,n}_\text{patch}-\mathbf{a}^{l,n}_\text{clean}),
\end{equation}
where $\alpha\in\{0, \frac{1}{N},\cdots,\frac{N-1}{N}\}$) and we set $\mathbf{a}^{l,n}_\text{patch}$ to $\mathbf{0}$.
We adapted the codebase from \citet{marks2024sparse} (\url{https://github.com/saprmarks/feature-circuits}, License: MIT) for our implementation.

\paragraph{Discovery of circuits}
We use above computation for the indirect effect to find the $k$ most important neurons and $k'$ most important preceding neurons of each of those neurons for \emph{each} class of \emph{each} domain. This approach is outlined below in more detail:
\begin{enumerate}
    \item \textbf{Identify the $k$ most important neurons}: We directly apply \cref{eq:indirect-effect}. We used $N=10$ steps for the linear approximation and only retained the 10\% most important neurons per layer. Note that these neurons represent the nodes of the graph representing the circuit.
    \item \textbf{Identify the $k'$ most important preceding neurons of each neuron}: We adapted \cref{eq:indirect-effect} to measure the effect of preceding neurons $n$ of layer $l<l'$ on the activations of neuron $n'$ in layer $l'$. Specifically, we replaced $m_{L,c}$ by $m_{l',n'}$ and measured the $L_2$ change caused by the clean activations $\mathbf{a}^{l,n}_\text{clean}$ and intervened activations $\alpha\mathbf{a}^{l,n}_\text{clean}+(1-\alpha)\mathbf{a}^{l,n}_\text{patch}$. We also set $N$ to 10. Note that this will yield us edges for the graph representing the circuit. After computing the indirect effects of all preceding neurons, we only retained the $k'=3$ most important edges.
\end{enumerate}

\paragraph{Measuring circuit similarity}
We compared the circuit similarity of pairs of graphs resorting to graph similarity measures. For example, we computed the Jaccard index for the nodes $N_{C_{\text{domain}~d_1\text{, class c}}},\;N_{C_{\text{domain}~d_2\text{, class c}}}$, as follows:
\begin{equation}
    \frac{|N_{C_{\text{domain}~d_1\text{, class c}}}\cap N_{C_{\text{domain}~d_2\text{, class c}}}|}{|N_{C_{\text{domain}~d_1\text{, class c}}}\cup N_{C_{\text{domain}~d_2\text{, class c}}}|} \quad .
\end{equation}

However, the simple node overlap cannot capture more complex structural and hierarchical similarities between circuits. Thus, we used the normalized Weisfeiler-Lehman subtree graph kernel \citep{shervashidze2011weisfeiler}. The Weisfeiler-Lehman graph kernel is popular graph kernel choice since (1) it can handle labeled, directed graphs of different sizes, (2) it is expressive, and (3) scales well to large graphs. The main idea of the Weisfeiler-Lehman kernel is the following procedure, given two labeled graphs, $G_1$ and $G_2$:
\begin{enumerate}
    \item \textbf{Multiset labeling and sorting}: For each node $n\in G_1$, create a multiset (unordered set with duplicates allowed) consisting of the node's $n$ current label and the sorted labels of its neighbors. Repeat this step for each node $n'\in G_2$.
    \item \textbf{Label compression}: Assign a unique new label to each of these multisets using a hash function.
    \item \textbf{Counting occurences}: Count the occurrences of each compressed label to obtain the feature vectors $\phi_h(G_1),\;\phi_h(G_2)$.
    \item \textbf{Relabeling}: Replace the current node labels with the newly compressed labels.
\end{enumerate}
We can repeat this procedure for $h$ iterations and compute the similarity of graphs via:
\begin{equation}
    K(G_1,G_2)=\langle\phi(G_1),\;\phi(G_2)\rangle=\sum_h \phi_h(G_1)\cdot\phi_h(G_2) \quad .
\end{equation}
Finally, we normalize the kernel to obtain a similarity score between 0 and 1:
\begin{equation}
    \Tilde{K}(G_1,G_2)=\frac{K(G_1,G_2)}{\sqrt{K(G_1,G_1)\cdot K(G_2,G_2)}} \quad .
\end{equation}
For our analysis, we used $h=3$ iterations. Our implementation is based on the publicly available code from \url{https://github.com/emanuele/jstsp2015}, License MIT.

\paragraph{Additional results}
\Cref{fig:neuron-analysis,tab:graph-similarity} shows results for all classes $C$, the classes seen during training $C_1$, and the unseen classes $C_2$. Interestingly, we find that, similar as for the representational similarities in \Cref{fig:linear-cka,fig:non-linear-cka}, nodes are less shared and circuits are slightly less similar for the unseen classes.
\begin{figure}[t]
    \centering
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_all_quickdraw}
        \caption{All classes.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_id_quickdraw}
        \caption{Seen classes only.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_ood_quickdraw}
        \caption{Unseen classes only.}
        \label{subfig:neuron-analysis-unseen}
    \end{subfigure} \\
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_all_domains}
        \caption{All classes.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_id_domains}
        \caption{Seen classes only.}
    \end{subfigure}
    \begin{subfigure}[c]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overlap_score_ood_domains}
        \caption{Unseen classes only.}
        \label{subfig:neuron-analysis-unseen-domains}
    \end{subfigure}
    \caption{\textbf{Amount of shared neurons.} The quickdraw domains shares the least neurons. This is especially apparent for the unseen classes $C_2$ (\subref{subfig:neuron-analysis-unseen}, \subref{subfig:neuron-analysis-unseen-domains}).}
    \label{fig:neuron-analysis}
\end{figure}
\begin{table}[t]
    \centering
    \begin{tabular}{lcccccc}
        \toprule
         & Clipart & Infograph & Painting & Quickdraw & Real & Sketch \\\midrule
        All classes & 0.281 & 0.246 & 0.266 & 0.218 & 0.273 & 0.273 \\
        Seen classes only & 0.281 & 0.246 & 0.265 & 0.218 & 0.273 & 0.273 \\
        Unseen classes only & 0.278 & 0.258 & 0.274 & 0.211 & 0.272 & 0.275 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Weisfeiler-Lehman similarities.} Higher similarities mean higher circuit (graph) similarity.
    The quickdraw domain exhibits the least degree of similarity, supporting our hypothesis that sharing of the circuitry is critical for generalization.
    }
    \label{tab:graph-similarity}
\end{table}