\section{Related Work}\label{sec:related-work}
\paragraph{\acrshort{ood} generalization of CLIP}
CLIP's remarkable \acrshort{ood} generalization has sparked research in identifying the factors driving it.
\citet{fang2022data} showed that CLIP's training distribution--rather than its dataset size, language supervision, or contrastive loss--is the primary factor of its \acrshort{ood} generalization. Similarly, \citet{nguyen2022quality} found that dataset quality outweighs quantity. However, the precise characteristics of the training distribution contributing to CLIP's generlization performance remained unclear.

While high train-test similarity was initially believed to be a key factor, \citet{mayilvahanan2024does} found its impact to be smaller than expected. Instead, other factors, such as the class distribution, were shown to have a more important role \citep{wen2024makes}.
Moreover, caption richness has been found to enhance CLIP's robustness \citep{xue2024understanding,wen2024makes}, and CLIP's loss fosters the learning of disentangled representations, facilitating the generalization to unseen attribute-object combinations \citep{abbasi2024deciphering}.
At the same time, other work revealed that CLIP exhibits behaviors resembling those of supervised classifiers. For example, CLIP fails to generalize when \emph{all} non-natural images are removed from its training data \citep{mayilvahanan2024search}, and CLIP can be vulnerable to spurious correlations \citep{wang2024sober}.
While these works have significantly advanced our understanding of CLIP's \acrshort{ood} generalization, key questions remain. For example, ``Can CLIP generalize to an entirely unseen domain?'' (domain generalization), ``Can CLIP generalize to unseen class-domain combinations?'' (compositional generalization), and which factors contribute to such generalization?

\paragraph{\acrshort{ood} generalization beyond CLIP}
The study of \acrshort{ood} generalization has been a focal point in the recent machine learning literature, covering various learning setups (refer to Table~2 of \citet{gulrajani2021in} for a comprehensive overview). In this work, we focus on two specific setups: domain generalization and compositional generalization.
In domain generalization, models are trained on \emph{multiple domains} and evaluated on an \emph{entirely unseen domain} \citep{blanchard2011generalizing,muandet2013domain,gulrajani2021in}. This is known as ``learning from multiple environments'' in the causality literature \citep{peters2016causal,arjovsky2019invariant,arjovsky2019out,richens2024robust}.
Compositional generalization, on the other hand, examines whether models generalize to unseen combinations of factors which were seen separately in training.
Compositional generalization was recently studied for, \eg, (causal) generative models \citep{atzmon2020causal,okawa2023compositional,wiedemer2023compositional}, object-centric models \citep{wiedemer2024provable}, disentangled \citep{xu2022compositional}, and (general) visual representation learning \citep{misra2017red,schott2022visual,saranrittichai2022overcoming}.
