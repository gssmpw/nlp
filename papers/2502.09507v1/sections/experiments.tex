\section{When Does CLIP Exhibit Domain and Compositional Generalization?}\label{sec:exp_when}
In this section, we trained CLIP models on our systematically constructed training data setups (refer to \cref{app:clip-training-details} for training details), as described in the previous sections and illustrated in \cref{fig:dataset-setup}, to investigate \emph{when} CLIP can achieve domain and compositional generalization. \cref{fig:effective-robustness} summarizes the results for CLIP models with ResNet-50 vision encoder and trained on ImageNet-Captions as base dataset. We discuss the results and key findings below. 

To ensure the validity of our results and findings, we validated them across several alternative choices: (1) base datasets (CC3M, CC12M), (2) vision encoders (ViT-S-32, Swin-T), and (3) contrastive loss choices (SigLIP \citep{zhai2023sigmoid}). These additional results, which are consistent with those in \cref{fig:effective-robustness}, are provided in \cref{app:other_choices}.

\paragraph{The role of domain diversity}
\begin{figure*}[t]
    \begin{subfigure}[c]{\textwidth}
        \centering
        \includegraphics[trim={6.2cm 5.15cm 0 0.1cm},clip,width=0.8\textwidth]{figures/sns-effective-robustness-legend-flat}\smallskip
    \end{subfigure} \\
    \begin{subfigure}[c]{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-clipart-flat}
    \end{subfigure}
    \begin{subfigure}[c]{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-infograph-flat}
    \end{subfigure}
    \begin{subfigure}[c]{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-painting-flat}
    \end{subfigure}
    \begin{subfigure}[c]{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-quickdraw-flat}
    \end{subfigure}
    \begin{subfigure}[c]{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sns-effective-robustness-sketch-flat}
    \end{subfigure}
    \caption{\textbf{High diversity domain mixtures exhibit improved effective robustness.} Each point represents the average performance over three consecutive training epochs and three seeds, with higher opacity indicating later training epochs. High diversity domain mixtures, such as Leave-out-domain (red) and CG high diversity (purple), have consistently higher generalization performance than their low diversity counterparts. These gains are especially pronounced in the clipart and sketch domains. However, for the quickdraw domain, generalization fails even in the high diversity settings--a limitation we will further investigate in \cref{sec:quickdraw}.
  }
  \label{fig:effective-robustness}
\end{figure*}

By constructing domain mixtures and controlling for all other factors, such as dataset size or model choice, we are able to isolate the impact of domain diversity in \Cref{fig:effective-robustness}. In particular, \cref{fig:effective-robustness} reaffirms the hypothesis that domain diversity is a key factor in enhancing CLIP's generalization: CLIP achieves both significantly better domain and compositional generalization in settings with high domain diversity (Leave-out-domain, CG high-diversity) compared to settings with low domain diversity (Natural-only, CG low-diversity).

While these results demonstrate that domain diversity is critical for the (compositional) generalization of CLIP, they do not provide insights into the importance of each domain individually, since all domains are added at once. Thus, we successively added domains to CG low-diversity until arriving at CG high-diversity to assess their importance.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{figures/sns-domain-interpolation-sketch-all}
    \caption{\textbf{Certain domains contribute more strongly to generalization performance than others.} When domains are added successively from CG low-diversity (left) to CG high-diversity (right), some domains significantly improve generalization, while others provide only small gains or even slightly degrade performance.}
    \label{fig:domain-itp}
\end{figure}
\cref{fig:domain-itp} shows that certain domains contribute more strongly than others, while other domains can even slightly decrease performance on the test domain. These observations suggest that while domain diversity is generally beneficial, the relationship between the added domain(s) and the test domain is a critical factor in facilitating generalization.
\begin{finding}
    Domain diversity enhances domain generalization and compositional generalization. However, the overlap between domains matters, too.
\end{finding}

\paragraph{How close is CLIP to the maximally achievable performance?}
In our previous experiments, we observed that CLIP generalizes well given sufficient domain diversity. However, how close is CLIP to the maximally achievable performance if it had been trained on class-specific samples from the test domain?

\Cref{tab:generalization-gap} shows that, even in high diversity settings with better generalization, a gap to the upper bound performance remains. We analyzed the number of test class samples required to close this gap in \cref{app:generalization-gap}. We found that the number of required samples tends to scale linearly.
\begin{table}[t]
    \centering
    \caption{\textbf{There is a performance gap when CLIP has seen close class samples of the test domain.} With sufficient domain diversity, CLIP generalizes well to unseen classes $C_2$ of the clipart and sketch test domains $D_i$. However, even in these cases, there remains a gap compared to models also trained on domain-specific samples of the classes, \ie, $D^{C_2}_i$. \cref{app:generalization-gap} provides the results of the other domains.}
    \label{tab:generalization-gap}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        Training data setup (\cref{fig:dataset-setup}) & Clipart & Sketch \\
        \midrule
        Leave-out-domain & 27.4 & 30.1 \\
        \midrule
        CG high-diversity & 27.6 & 28.1 \\
        \hspace{1em}w/ classes $C_2$ (upper bound) & 36.6 (+9.0) & 44.3 (+16.2) \\
        \bottomrule
    \end{tabular}}
\end{table}

\paragraph{Achieving good compositional generalization is challenging}\label{sec:cg-challenges}
While high diversity improves generalization, one may expect CLIP to generalize better to unseen classes within a test domain if it has seen some other classes of that domain during training. However, \Cref{fig:effective-robustness} surprisingly reveals the opposite: CLIP models trained on a subset of classes from the test domain (CG low-/high-diversity) are often slightly outperformed by models that have not seen the test domain at all (Natural-only, Leave-out-domain).
\begin{finding} 
    Compositional generalization can be weaker than domain generalization.
\end{finding}
This finding is surprising, since the test domain is entirely unseen and domain generalization can require extrapolation. In contrast, compositional generalization is expected to perform better as it has partial exposure to that domain. However, our results suggest that this intuition may not always hold.

To better understand this, we investigated the role of the test domain's chosen classes for training and the ones that are queried during evaluation. For example, CLIP may learn the shortcut that all sketches belong to the subset of seen classes, which becomes wrong for the unseen classes queried in evaluation.
To test this hypothesis, we replaced DomainNet's sketches of the classes $C_1$ in our previous CG settings with sketches of the classes $C'_1$ that are \emph{not} queried in evaluation. For this, we used ImageNet-Sketch \citep{wang2019learning} and excluded all ImageNet classes that overlap with the classes from DomainNet; refer to \cref{app:in-dn-mapping} for details on the class overlap.

\begin{table}[t]
    \centering
    \caption{\textbf{Seeing a subset of classes of the test domain can worsen compositional generalization.} Including a subset of sketches from DomainNet (CG low/high-diversity) slightly decreases performance on the unseen sketch classes $C_2$ compared to not seeing that domain at all (Leave-out-domain). However, adding sketches of classes that do not overlap with DomainNet's classes, instead improves compositional generalization performance, suggesting that compositional generalization can be limited by (only) partial, suboptimal inclusion of the test domain.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc}
        \toprule
        Training data setup (\cref{fig:dataset-setup}) & Sketch \\
        \midrule
        Natural-only & 19.5 \\
        Leave-out-domain  & 30.1 \\
        \midrule
        CG low-diversity & 19.2 \\
        \hspace{1em} w/ sketches of non-queried classes only & 27.1 (+7.9) \\\midrule
        CG high-diversity & 28.1 \\
        \hspace{1em} w/ sketches of non-queried classes only & 36.9 (+8.8) \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:cg-overfitting}
\end{table}
\cref{tab:cg-overfitting} confirms that including sketches of non-queried classes improves compositional generalization. However, it also highlights a failure mode: while compositional generalization can work for CLIP (to some extent, \cf, \cref{tab:generalization-gap}), partial exposure to classes of the test domain that overlap with the classes queried in evaluation significantly worsens compositional generalization. We further analyzed the severity of this overlap on compositional generalization performance in \cref{app:challenge-cg}.

\paragraph{The role of language supervision}
To investigate the influence of language supervision, we replicated our previous experiments with a supervised classifier. Specifically, we trained the classifier on the combined class distribution of ImageNet and DomainNet. Refer to \cref{app:supervised-training-details} for further (training) details.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sns-effective-robustness-sketch-supervised}
    \caption{\textbf{Effective robustness of CLIP vs. supervised classifiers.} Similar to CLIP, the robustness of supervised classifiers also increases with the domain diversity of the training data. However, CLIP consistently shows superior performance. Refer to \cref{app:language-supervision} for the results on the remaining domains.}
    \label{fig:effective-robustness-supervised}
\end{figure}
\cref{fig:effective-robustness-supervised} shows that generalization performance of supervised classifiers increases with domain diversity, similar to CLIP. However, CLIP consistently exhibits higher performance compared to the classifiers, which can be attributable to caption richness \citep{xue2024understanding,wen2024makes}.