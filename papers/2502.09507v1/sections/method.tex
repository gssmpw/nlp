\section{Problem Setup}\label{sec:problem-setup}
\begin{figure*}[t]
  \includegraphics[trim={0.25cm 0.3cm 0.5cm 0.3cm},clip,width=\textwidth]{figures/dataset-setup}
  \caption{\textbf{Training data setups and CLIP's performance in these setups.}
  \textbf{A}: We systematically varied the domain mixture and object class exposure of CLIP's training data across four scenarios while controlling for other factors like dataset size and model choice. Specifically, we trained CLIP models on (1) mostly natural images (``Natural-only'') to obtain a lower performance bound, (2) a diverse set of domains excluding the test domain (``Leave-out-domain'') to assess its domain generalization, (3) natural images with a subset of test domain classes (``CG low-diversity'') to evaluate its compositional generalization, and (4) a combination of diverse domains plus a subset of test domain classes (``CG high-diversity'') for a more diverse compositional generalization setting.
  \textbf{B}: CLIP trained on diverse domains demonstrates stronger \acrshort{ood} generalization compared to training on only natural images or fewer domains. Remarkably, CLIP can perform as well or better even without exposure to some part (\ie, object classes) of the test domain during training.
  }
  \label{fig:dataset-setup}
\end{figure*}
Recent work highlighted that CLIP is trained on a substantial amount of non-natural images \citep[Table~2]{mayilvahanan2024search}, but the role of these non-natural images in enabling CLIP's \acrshort{ood} generalization remains unclear. In this work, we aim to address this question by systematically analyzing the effect of different domain mixtures in CLIP's training data (see \cref{fig:dataset-setup}), allowing us to study CLIP's ability to generalize to entirely unseen domains (\textit{domain generalization}) and to novel combinations of known domains and classes (\textit{compositional generalization}).

\paragraph{Notations}
Let $D_0$ denote the base domain mostly consisting of natural images with image-text pairs $(I^0_i, T^0_i)$ (\textcolor{fig1red}{red} in \cref{fig:dataset-setup}). Further, we consider $m$ non-natural domains $D_r$ for $r\in\{1,\dots,m\}$ with image-text pairs $(I^r_i, T^r_i)$ (\textcolor{fig1blue}{blue}, \textcolor{fig1green}{green}, \textcolor{fig1orange}{orange}).
Lastly, we consider the object classes $C = \{c_1,\dots,c_n\}$ for the images $I$ which we divide into two disjoint subsets $C_1 = \{c_1,\dots,c_k\}$ (squares) and $C_2 = \{c_{k+1},\dots,c_n\}$ (circles). We denote the subset of $D_r$ that contains only classes from $C'$ as $D_r^{C'}$.

\paragraph{Training data setups}
Below, we specify the four training data setups; see \cref{fig:dataset-setup} for a visual overview.
\begin{itemize}[topsep=0pt]
    \itemsep0em 
    \item \textbf{Natural-only (lower bound)}: We train only on our base image domain $D_0$ (\eg, ImageNet-Captions \citep{fang2022data}, see \cref{sec:exp_setup} for further details), consisting (almost) exclusively of natural images of \emph{all} classes $C$. This condition serves as our lower bound and mirrors the training distributions considered in \citet{fang2022data,mayilvahanan2024search}.
    %
    \item \textbf{Leave-out-domain (domain generalization)}: We train on a diversity of domains $\bigcup_{j \neq i} D_j \cup D_0$ but hold out the test domain $D_i$, following the classical domain generalization learning setup \citep{blanchard2011generalizing,muandet2013domain,gulrajani2021in}. Note we could also interpret this as \emph{extrapolation} in certain cases, \ie, does CLIP generalize to data \emph{outside} the domain coverage seen during training?
\end{itemize}
\begin{definition}
A model \emph{compositionally generalizes} if it can accurately classify any new combinations of seen factors (here, classes and domains) not seen together during training.
\end{definition}
\vspace{-0.5em}
Following this definition, we construct the training data setups for \acrfull{cg} as follows:
\begin{itemize}[topsep=0pt]
    \itemsep0em 
    \item \textbf{CG low-diversity}: We train on the base domain $D_0$ and a subset of classes $C_1$ of the test domain $D_i^{C_1}$: $D_0\cup D_i^{C_1}$.
    %
    \item \textbf{CG high-diversity}: We train on the base domain $D_0$, a subset of classes $C_1$ of the test domain $D_i^{C_1}$, and a diverse set of other domains $\mathbf{D}_{j\neq i}:=\bigcup_{j \neq i} D_j$ containing \emph{all} classes $C$: $D_0 \cup D_i^{C_1} \cup \mathbf{D}_{j\neq i}$.
\end{itemize}

\paragraph{Test data}
Our test data for \emph{all} training data setups consists of the \emph{same novel} combinations of the classes $C_2$ of the test domain $D_i$: $D_i^{C_2}$.
By keeping the test set fixed throughout all conditions, we ensure comparability across these setups.

\section{Experimental Setup}\label{sec:exp_setup}

\paragraph{Datasets}
We used either ImageNet-Captions \citep{fang2022data}, CC3M \citep{sharma2018conceptual}, or CC12M \citep{changpinyo2021conceptual} as our base image-text datasets $D_0$ (\textcolor{fig1red}{red} in \cref{fig:dataset-setup}).
Captions in ImageNet-Captions are constructed using the title, tag, and description (if provided) to yield maximal descriptiveness. To mitigate the influence of class distribution shift, we augmented the base datasets with natural samples from DomainNet-Real \citep{peng2019moment}.

For the domain-specific image-text pairs $D_r$ (\textcolor{fig1blue}{blue}, \textcolor{fig1green}{green}, \textcolor{fig1orange}{orange} in \cref{fig:dataset-setup}), we used DomainNet's non-natural domains: Clipart, Infograph, Painting, Quickdraw, and Sketch \citep{peng2019moment}. Since DomainNet provides no captions, we created captions by using \emph{domain-invariant} templates (\eg, \texttt{an image of a \{class\}}) or \emph{domain-specific} templates (\eg, \texttt{a \{domain\} of a \{class\}}); see \cref{app:dn-captions} for further details.
We used comparable final training dataset sizes across our different training conditions; refer to \cref{app:data_construction} for details. 
Finally, the class choices for $C_1$ and $C_2$ are provided in \cref{sub:class-choices}.

\paragraph{Evaluation of CLIP models}
We evaluated CLIP models in the classical zero-shot classification setting across all DomainNet classes $C$ using the standard OpenAI templates \citep{radford2021learning}, extended with templates for the missing domains of DomainNet; see \cref{app:dn-prompts} for further details. To mitigate the effect of class imbalance, we calculated the \emph{balanced} top-1 accuracy, which we will hereon refer to as top-1 accuracy for brevity.