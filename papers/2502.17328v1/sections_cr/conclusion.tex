%We introduced a novel approach that unifies dialogue synthesis and summarization within a large language model (LLM) to enhance few-shot dialogue summarization without relying on external data. 
%By leveraging self-alignment through data synthesis with direct preference optimization, we enabled both tasks to mutually reinforce each other.
%Furthermore, our domain-shifting training strategy effectively incorporated synthesized dialogues without introducing artifacts, improving summarization accuracy.
We introduce a novel approach that mutually reinforces dialogue synthesis and summarization capabilities of a large language model (LLM) to improve few-shot dialogue summarization without relying on external data.
By leveraging the dialogue synthesis capability enhanced by DPO, we synthesize well-formatted, coherent dialogues to augment the few-shot real dataset.
Furthermore, the two-stage training strategy effectively incorporated synthesized dialogues without introducing artifacts, improving summarization accuracy.
Empirical results demonstrated significant improvements: a 1.5\% increase in ROUGE scores, a 0.3\% improvement in BERT scores.
%, and data generation that is 30 to 60 times faster than baseline models. 
Human evaluations confirmed that our method outperforms the real-only baseline and, in certain aspects, surpasses human-annotated ground truth summaries. 
%This highlights the potential of self-aligned LLMs in advancing dialogue summarization.
Our approach offers a practical solution for real-world applications with limited data, utilizing the model's inherent capabilities without external resources. 
Future work could extend this self-alignment framework to other NLP tasks affected by data scarcity and explore its integration with larger or more diverse LLM architectures.






