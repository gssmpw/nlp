\subsection{Prompt Templates}
Table \ref{tab:prompt} shows the prompt templates we use in this work for different purposes.

\begin{table*}[]
\begin{tabular}{l}
\hline
\textbf{Topic extraction}\\ \hline
\begin{tabular}[c]{@{}l@{}}Please determine the main topic of the provided summary.\\ The topic should be a brief phrase that captures the essence of the summary.\\ Instructions:\\ - Ensure the topic is around 2 words in length.\\ - It should clearly reflect the core idea of the summary.\\ - Avoid specific details and names; the topic should be general enough to apply to various summaries.\\ - Use precise and specific language.\\ Summary:  {[}summary{]}\\ Topic:\end{tabular}\\ \hline
\textbf{Summary synthesis given topic}\\ \hline
\begin{tabular}[c]{@{}l@{}}Please write a summary for a document based on the provided topic.\\ Instructions:\\ - The summary should be approximately {[}word count{]} words in length.\\ - Ensure the summary captures the main idea related to the topic.\\ Topic: {[}topic{]}\\ Summary:\end{tabular}\\ \hline
\textbf{Dialogue synthesis given summary}\\ \hline
\begin{tabular}[c]{@{}l@{}}Please write a dialogue based on the given summary. \\ Each line should start with the anonymous speaker\'s index followed by a colon.\\ Instruction:\\ - Use the {[}num of speaker{]} speakers in the dialogue: \\ \#1, \#2 ...\\ - Ensure each line starts with the speaker's index followed by a colon.\\ - Write the dialogue clearly and naturally.\\- The dialogue should be around {[}num of turns{]} turns and {[}num of words in dialogue{]} words in length. \\ Summary: {[}summary{]}\end{tabular} \\ \hline
\textbf{Dialogue summarization}\\ \hline
\begin{tabular}[c]{@{}l@{}}Document: {[}Dialogue{]}\\ Summarize the provided document\end{tabular}\\ \hline
\textbf{Dialogue Summarization (zero shot)}\\ \hline
\begin{tabular}[c]{@{}l@{}}Dialogue: {[}Dialogue{]}\\ Summarize the provided dialogue.\end{tabular}\\ \hline
\textbf{Dialogue Summarization (in-context learning)} \\ \hline
\begin{tabular}[c]{@{}l@{}}Your task is to summarize the provided dialogue. The following are some examples: \\ Example 1:\\ Dialogue: {[}Example dialogue 1{]}\\ Summary: {[}Example summary 1{]}\\ \\ Example 2:\\ Dialogue: {[}Example dialogue 2{]}\\ Summary: {[}Example summary 2{]}\\ …\\  Now, here is the target dialogue: \\ {[}Dialogue{]}\\ Summarize the target dialogue.\end{tabular}\\ \hline
\textbf{Dialogue Summarization (zero shot + length control)}\\ \hline
\begin{tabular}[c]{@{}l@{}}Dialogue: {[}Dialogue{]}\\ Summarize the provided dialogue. \\ The summary should be around {[}average word count of summaries in training set{]} words in length.\end{tabular} \\ \hline
\end{tabular}
\caption{Prompt templates.}
\label{tab:prompt}
\end{table*}

\subsection{Examples of Summarization Result}
Table~\ref{tab:summ_example_1} presents an example dialogue along with summaries produced by various methods, including the human-labeled ground truth summary, the output from the zero-shot pre-trained model, summaries generated using in-context learning, and those fine-tuned with real data only as well as with the MRDS approach.

\begin{table*}[]
\centering
\begin{tabular}{l}
\hline
\textbf{Input dialogue}\\ \hline
%\begin{tabular}[c]{@{}l@{}}
Avery: You went to Ethan's house? \\
David: yeah I had to babysit \\
Avery: Aww, how do you babysit, just curious \\
David: I had to go through a lot :/ \\
Avery: Was his sister naughty \\
David: Tooo much \\
Avery: Lol \\
David: I will just refuse net time :/ \\
Avery: As you wish \\
David: :/ \\
Avery: I just got his text \\
David: What is he saying\\
Avery: He is asking me to say thanks to you\\
David: yeah whatever<3\\
Avery: He was saying that your phone was switched off\\
David: Yeah i have just turned it on\\
Avery: I have told him about that\\
David: k\\
Avery: Gotta go now \\
%\end{tabular}\\
\hline
\textbf{Ground Truth Summary} \\ \hline
David was looking after Ethan's sister. Ethan is grateful. David won't do it again. \\
\hline
\textbf{Generated Summary (Zero Shot)} \\ \hline
The dialogue is between Avery and David, discussing David's experience babysitting Ethan's sister. \\
David mentions that the sister was "too much" and that he will refuse to babysit again. \\
Avery shares that Ethan sent a text asking Avery to thank David, \\
and also mentions that David's phone was switched off. \\
David confirms that he just turned it on and the conversation ends with Avery saying they have to go.\\
\hline
%\textbf{Generated Summary (zero shot + length control)} \\ \hline
%David babysat Ethan's sister, had a tough time, and \textcolor{red}{received a text from Ethan} asking Avery to thank him. \\
%\hline
\textbf{Generated Summary (In-context Learning)} \\ \hline
David babysat at Ethan's house and had a difficult time. \\ Avery asks how he managed it and David says it was a lot of work. \\ Avery asks if Ethan's sister was naughty and David confirms that she was. \\ David jokes that he will refuse to babysit again. \\ Avery mentions Ethan sent a text asking Avery to thank David, and David agrees to let Avery do so.\\ Avery also mentions that Ethan's phone was switched off, which David had just turned back on. \\
\hline
\textbf{Generated Summary (Real only)} \\ \hline
David had to babysit Ethan's sister. \\
\hline
\textbf{Generated Summary (MRDS, Ours)} \\ \hline
David had to babysit Ethan's sister. Avery got a text from Ethan saying thanks to David.\\
\hline
\end{tabular}
\caption{Summarization results from our method and baselines.}
\label{tab:summ_example_1}
\end{table*}

\subsection{Synthetic Dialogue from SFT Trained Synthesizer}
To show the limited capability of dialogue synthesizer trained on few shot real data, we provide an example of low quality synthetic dialogue in Table \ref{tab:synthesis_sft_sample}.

\begin{table*}[]
\centering
%\small
%\caption{Low quality synthetic sample from SFT trained dialogue synthesizer. Please note the format and content errors highlighted in \textcolor{blue}{blue} and \textcolor{red}{red}, respectively.}
\begin{tabular}{l}
\hline
\textbf{Summary $s$:} \\
\#2 can't join \#1 and Mike for a game because \\
he is studying.\\
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Generated dialogue $\hat{d}$}:\\ 
\#1: So will you come at 2?\\ 
\#2: \textcolor{red}{sure bro,} what time you gonna be with Mike?\\ 
\#1: about 20 min.00, Mike will call you\\ 
\textcolor{blue}{2: he is going}\\ 
\textcolor{blue}{3 no way, he's still sleeping hehehe} \\ 
\textcolor{blue}{2} \textcolor{red}{he's studying}\\ 
\textcolor{blue}{1: and?}\\  
\textcolor{blue}{not coming}\end{tabular} \\ \hline 
\end{tabular}
\caption{Low quality dialogue generated by SFT-trained dialogue synthesizer. \#1 and \#2 are anonymized speaker names. The text highlighted in \textcolor{red}{red} shows content that is not aligned with the summary (\#2 can't join and \#2 is studying instead of Mike). The lines highlighted in \textcolor{blue}{blue} are format errors, since they do not start with a proper speaker name (\#1,  \#2, ...).}
\label{tab:synthesis_sft_sample}
\end{table*}


\subsection{Preference Pair Examples}
Tables~\ref{tab:preference_content_example} and~\ref{tab:preference_format_example} provide examples of content-based and format-based preference pairs, respectively. In Table~\ref{tab:preference_content_example}, the summarization model scores the content alignment of the dialogues using $\mathcal{M}_{sum}(s|\hat{d})$, selecting the best and worst ones as the preferred and rejected dialogues. In Table~\ref{tab:preference_format_example}, the preferred dialogues are synthesized using IDS, where $\mathcal{F}(\hat{d}) = 1$, while the rejected dialogues are generated through the SFT synthesizer's one-shot synthesis, where $\mathcal{F}(\hat{d}) = 0$.

% Specifically, for each summary $s$, we first sample multiple clean dialogues from the SFT-trained dialogue synthesizer $\mathcal{M}_{dlg}^{SFT}$ following the IDS method describe above, 
% and then pick the dialogues with the best and least content alignment scores to form preference pairs $\{s, \hat{d}_1, \hat{d}_2\ |\ s \in \mathcal{S},
% \mathcal{M}_{sum}(s|\hat{d}_1) >\  \mathcal{M}_{sum}(s|\hat{d}_2)\}$. \textbf{ravi: put an example content-based pair in the appendix}

%  We use this data to form preference pairs 
% $\{s, \hat{d}_1, \hat{d}_2\ |\ s \in \mathcal{S},  
%  \mathcal{F}(\hat{d}_1) = 1,\   \mathcal{F}(\hat{d}_2) = 0\}$
% , where $\mathcal{F}(d)$ denotes the format check-based binary scoring function (1 for clean dialogues, 0 for those with errors). 

\begin{table*}[]
\centering
\begin{tabular}{l}
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Summary:} \\ On Sunday \#1 and \#2 are meeting with Dominica and James who got married recently.\end{tabular}\\ 
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Preferred dialogue:}\\
\#1: Hahaha we must meet Dominica tomorrow with James. \\
\#2: yep \\
\#1: I dont think she will be very available after the marriage.\\
\#2: ye ye, she's very busy\\
\#1: I asked :) :) <file\_gifile>\\
\#2: so 8  o'clock?\\
\#1: sounds super
\end{tabular}\\
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Rejected dialogue:}\\
\#1: are u free next Sunday?\\
\#1 file\_gifiles : yeah\\
\#2: ok \\
\#1: wanna meet Domin?\\
\#2: ok\\
\textcolor{blue}{\# fileifile} \\
\textcolor{blue}{\# gileile} \\
\textcolor{blue}{\# : ok: <file :)}\\
\textcolor{blue}{file :) <file :)  : :file :) <file < :)}\\
\textcolor{blue}{\# Ok, ok :) <file :)}\\
\textcolor{blue}{ :) it sounds cool}
\end{tabular} \\ \hline
\end{tabular}
\caption{Example of a format-based preference pair. \#1 and \#2 are anonymized speaker names. The lines highlighted in \textcolor{blue}{blue} are format errors, since they do not start with a proper speaker name (\#1,  \#2).}
\label{tab:preference_format_example}
\end{table*}



\begin{table*}[]
\centering
\begin{tabular}{l}
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Summary:} \\ \#1 is outside waiting for \#2, who is late.\end{tabular}\\ 
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Preferred dialogue:}\\
\#1: Hey, it's \#1. I'm sitting outside where are you?.\\
\#2: \textless{}file\_giffile\textgreater\\
\#1: You're late! And I was waiting for you there.\end{tabular}\\
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Rejected dialogue:}\\
\#1: I can't come to the meeting\\
\#2: what did?\\
\#1: I have not enough money for cab. I must wait for \#1\\
\#2: \#1 is waiting for us!\\
\#1: how can I get there?\\
\#2: use bus, it's the fastest, come back me\end{tabular} \\ \hline
\end{tabular}
\caption{Example of a content-based preference pair. \#1 and \#2 are anonymized speaker names.}
\label{tab:preference_content_example}
\end{table*}


\subsection{Data Formatting and Anonymization}

To simplify the training of the data synthesizer, we perform anonymization preprocessing on both summaries and dialogues. Specifically, we extract the number of speakers and their names from the dialogues as metadata and replace the names in both dialogues and summaries with uniform identifiers (e.g., "\#1", "\#2").

\subsubsection{Data Formatting}
\label{sec:format}
To ensure that the synthetic data adheres to the correct format, we have established a set of formatting rules:

\begin{itemize}[label=-, topsep=0pt, itemsep=1pt, parsep=0pt, partopsep=0pt]
\item \textbf{Speaker Identity}: each sentence in the dialogues must begin with a speaker identifier followed by a colon (e.g., "\#1:", "\#2:").
\item \textbf{Consistency of Names}: Synthetic dialogues and summaries should not contain incorrect or extra names—for example, "\#4" in dialogue-summary pairs with fewer than four speakers, or a "\#" symbol without a number.
\item \textbf{Inclusion of Speaker Names}: Synthetic summaries must contain at least one anonymized speaker name.
\end{itemize}

For summary synthesis, we discard any summaries that do not comply with these formatting rules. For dialogue synthesis, we apply Iterative Dialogue Synthesis (IDS) to ensure that the synthetic dialogues conform to the correct format.

% To ensure that the synthetic data adheres to the correct format, we design a set of formatting rules. For dialogues, we first check that the beginning of each sentence starts with a speaker identifier followed by a colon (e.g., "\#1:", "\#2:"). Second, we verify that the synthetic dialogues and summaries do not contain incorrect or extra names—such as "\#4" in dialogue-summary pairs with fewer than four speakers or a "\#" symbol without a number. Third, we also check the synthetic summary should contain at least one anonymized speaker name in the summary. For summary synthesis, we discard any summaries that are in an incorrect format. For dialogue synthesis, we apply Iterative Dialogue Synthesis (IDS) to ensure that the synthetic dialogues conform to the correct format.


% and train the dialogue and summary synthesizer on the data format.



\subsubsection{Iterative Dialogue Synthesis}
Table~\ref{tab:iter} illustrates an example of iterative dialogue synthesis, showing partial inputs given to the synthesizer and the corresponding synthesized outputs. During the synthesis process, we check for formatting issues as described in Sec.~\ref{sec:format}. If an error is found, the flawed sentence, along with all subsequent sentences, is discarded, and the correctly formatted segment (e.g., "\#1:", "\#2:") is used to re-synthesize the dialogue.



\begin{table*}[]
\centering
\begin{tabular}{l}
\hline
\textbf{Input summary}\\ \hline
Both \#1 and \#2 forgot about Monica's birthday today. \\ As they do not know what present to buy for her,  \#2 will phone Monica's boyfriend and ask his advice.\\ 
\hline 
\textbf{First partial response}\\ \hline
\textcolor{orange}{\textbf{\#1:}}\\
\hline 
\textbf{First synthesis result}\\ \hline
\textbf{\#1:} 
\textcolor{blue}{I forgot about Monicas birthday today haha} \\
\textcolor{blue}{\#2: Me to, me too}\\
\textcolor{blue}{\#1: what now?}\\
\textcolor{blue}{\#2: we should make something?} \\
\textcolor{blue}{\#1: what do you propose to buy?}\\
\textcolor{blue}{\#1: I'm asking my brother's opinion, he knows her a bit more}\\
\textcolor{blue}{\#2: same here, I will phone her boyfried}\\
\textcolor{blue}{\#1: you must have more info}\\
\textcolor{blue}{\textit{\underline{2: ya, I will get some advice}}}\\
\textcolor{blue}{\textit{1:}}\\
\textcolor{blue}{\textit{I asked and we decided to buy her something}}\\
\textcolor{blue}{\textit{2.: 1 can not see.}}\\
\textcolor{blue}{\textit{5. This 2 1 3’s got 1 6.75.}} \\ 
% \textit{\underline{2:} ya, I will get some advice}\\
% \textit{\#1: maybe a giftcard?}\\
% \textit{1: or what?}\\
% \textit{2: I asked and we decided to buy her something}\\
% \textit{1: so u decided already}\\
% \textit{2: we should, I guess} \\ 

\hline 
\textbf{Second partial response}\\ \hline
\#1: I forgot about Monicas birthday today haha\\
\#2: Me to, me too\\
\#1: what now?\\
\#2: we should make something? \\
\#1: what do you propose to buy?\\
\#1: I'm asking my brother's opinion, he knows her a bit more\\
\#2: same here, I will phone her boyfried\\
\#1: you must have more info\\
\textcolor{orange}{\textbf{\#2:}}\\

\hline
\textbf{Second synthesis result}\\ \hline
\#1: I forgot about Monicas birthday today haha\\
\#2: Me to, me too\\
\#1: what now?\\
\#2: we should make something? \\
\#1: what do you propose to buy?\\
\#1: I'm asking my brother's opinion, he knows her a bit more\\
\#2: same here, I will phone her boyfried\\
\#1: you must have more info\\
\textbf{\#2:} \textcolor{blue}{sure}\\
\textcolor{blue}{\#1: I am already looking for some ideas}\\
\textcolor{blue}{\#2: it looks hopeless}\\
\textcolor{blue}{\#2: I will call and he phone}\\
\textcolor{blue}{\#1: yes }\\
\hline
\end{tabular}
\caption{An example of iterative dialogue synthesis (IDS), condensed for brevity. In each step, the correctly-formatted partial response is concatenated to the initial dialogue synthesis prompt and given as input for generation. Newly generated text appears in blue, while discarded text is italicized. In each step, we also add an anonymized speaker name (shown in yellow) at the end of the partial response before using it for generation. We always use \#1 as the first partial response, and randomly choose an anonymized speaker in the subsequent steps.}
\label{tab:iter}
\end{table*}


\subsubsection{Synthetic Dialogue Recovery}
After data synthesis, we restore the anonymized dialogues and summaries before training the summarization model by replacing placeholders with real names using metadata extracted during the anonymization process. For synthetic summaries that lack original metadata, we seed the synthesis process with metadata from the real data in the few-shot training set, then replace the placeholders accordingly. Alternatively, we can generate random names and speaker numbers to populate the placeholders.


% After data synthesis, we need to recover the anonymized dialogues before training the summarization model. We use the metadata extracted during the anonymization process to replace the identifiers with the real names in the dialogues and summaries. For synthetic summaries, we use metadata from the real data as seeds to synthesize summaries and dialogues and then replace the identifiers with actual names afterward. Alternatively, this information can be randomly generated using random names and speakers' numbers.



\subsection{Human Evaluation Details}
We hired five machine learning researchers, and received their consent to report the results from their annotation work.
We adopt the human evaluation method proposed in the previous work \citep{xie-etal-2024-shot}.
For completeness, we describe the 4 metrics (informativeness, faithfulness, fluency and redundancy) and the corresponding instructions in Table \ref{tab:human_eval_instruct}.


\begin{table*}[]
\centering
\begin{tabular}{l}
\hline
\textbf{Informativeness}\\ \hline
Whether the critical information in the dialogue is missed in the summary: \\
*0: lots of the critical information in the dialogue is missed; \\
*1: a small amount of the critical information in the dialogue is missed;\\
*2: no critical information in the dialogue is missed.\\
\hline
\textbf{Faithfulness}\\ \hline
Whether the information presented in the summary is factually incorrect or unmentioned \\
according to the dialogue: \\
*0: lots of the information presented in the summary is factually incorrect or unmentioned; \\
*1: a small amount of the information presented in the summary is factually incorrect or unmentioned;\\
*2: no information presented in the summary is factually incorrect or unmentioned.\\
\hline
\textbf{Fluency}\\ \hline
Whether the sentences in the summary are ungrammatical or ill-formed: \\
*0: lots of the sentences in the summary are ungrammatical or ill-formed; \\
*1: a small amount of the sentences in the summary are ungrammatical or ill-formed;\\
*2: no sentence in the summary is ungrammatical or ill-formed.\\
\hline
\textbf{Redundancy}\\ \hline
 Whether the expressions of the summary can be simplified: \\
*0: lots of the expressions of the summary can be simplified; \\
*1: a small amount of the expressions of the summary can be simplified;\\
*2: no expression of the summary can be simplified.\\
\hline
\end{tabular}
\caption{Human evaluation metrics and their corresponding instructions}
\label{tab:human_eval_instruct}
\end{table*}


\subsection{Training Hyperparameters}
Table~\ref{tab:hyp} presents the hyperparameters used for the summarization and dialogue synthesis models. We conducted an extensive search for hyperparameters on the summarization model trained exclusively with real data. All other SFT training follows the same set of hyperparameters. The LR threshold for the two-stage summarization approach refers to the minimum learning rate reached during the first stage of training with synthetic data. Once this threshold is met, the second stage begins using real data, applying the same hyperparameters as those used for the real-only summarization model.

The DPO synthesis model is initialized from the checkpoints of the SFT synthesis model and is trained using both DPO and SFT loss. The batch size for DPO training is four, randomly sampled from two preference sets: format-based and content-based preference pairs. Additionally, SFT training data with a batch size of one is included in the process, and the combined losses are used to update the synthesis model.


\begin{table*}[htbp]
\centering
\caption{Hyper-parameters used for training dialogue summarization, two-stage dialogue summarization, dialogue/summary synthesis, and DPO-based dialogue synthesis models.}
\label{tab:hyp}
\begin{tabular}{@{} lcccc @{}}
\toprule
 & Summarization & Two-stages Summ. & Synthesis & DPO Syn. \\
\midrule
Loss & CE & CE & CE & DPO and CE \\
Batch Size & 10 & 10 & 10 & {\small 4 DPO \& 1 SFT}  \\
Learning Rate & 2.0e-4 & 2.0e-4 & 2.0e-4 & 1.0e-5 \\ 
Optimizer & ReduceLROnPlateau  & ReduceLROnPlateau  & ReduceLROnPlateau & Fixed LR \\ 
Validation Steps & 2 & 2 & 2 & 20 \\
Patience & 5 & 5 & 5 & - \\
Factor & 0.7 & 0.7 & 0.7 & - \\
Warmup Steps &  50 & 50 & 50 & - \\ 
Early Stopping &50 & 50 & 50 & - \\ 
LR Threshold & - & 2.0e-5 & - & - \\\hline
LoRA Rank &  \multicolumn{4}{c}{16} \\  
LoRA Alpha & \multicolumn{4}{c}{32} \\ 
Dropout Rate & \multicolumn{4}{c}{0.4} \\
\bottomrule
\end{tabular}
\end{table*}

%%%% Originally it is in exp result section
\subsection{Dialogue Synthesis Quality}


\begin{table*}[htp]
\centering
\begin{tabular}{lccc}
\toprule
 \textbf{Synthesis Model} & \textbf{Synthesis Method} & \textbf{Format Corr.} & \textbf{Summ. CE} \\
\midrule
SFT  &  One-shot & 24\% & 4.74 \\
SFT & Iterative  & 100\%* & 4.69 \\
\midrule
Joint Preference Set  & One-shot &  78\% & 4.62 \\
Joint Preference Set + SFT  & One-shot &  \textbf{96\%} & \underline{4.45} \\
Separate Preference Sets + SFT (MRDS) & One-shot &  \underline{92\%} & \textbf{4.13} \\
\bottomrule
\end{tabular}
\caption{Dialouge Synthesis with different training strategy on SAMSum 300 shots experiments.}
\label{tab:DPO abl}
\end{table*}


We evaluated the performance of different dialogue synthesis methods, including direct synthesis (One-shot), iterative dialogue synthesis (Iterative), and various configurations of DPO, as presented in Table~\ref{tab:DPO abl}. In our DPO training, we constructed different sets of preference pairs based on the definitions provided earlier. For the single joint preference set, we used dialogues from the post-processing step as the preferred data (\( \hat{d}_1 \)) and the raw dialogues without iterative synthesis as the rejected data (\( \hat{d}_2 \)), forming preference pairs:
\(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{F}(\hat{d}_1) = 1, \mathcal{F}(\hat{d}_2) = 0, \mathcal{M}_{\text{sum}}(s \mid \hat{d}_1) > \mathcal{M}_{\text{sum}}(s \mid \hat{d}_2)\}\). 
For the separated preference sets, we included both formatting preference pairs:
\(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{F}(\hat{d}_1) = 1, \mathcal{F}(\hat{d}_2) = 0\}\)
and summarization preference pairs:
\(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{M}_{\text{sum}}(s \mid \hat{d}_1) > \mathcal{M}_{\text{sum}}(s \mid \hat{d}_2)\}\). 
We also incorporated supervised fine-tuning data into the DPO training, indicated as "+SFT."

From Table~\ref{tab:DPO abl}, the SFT model's one-shot synthesis had low format correctness (24\%) and a high summarization cross-entropy (CE) loss of 4.74. While Iterative synthesis ensured 100\% format correctness, the CE loss remained similar at 4.69, showing little improvement in content alignment.
Applying DPO with the joint preference set improves format correctness to 78\% and reduces the summarization CE loss to 4.62. However, training with DPO loss alone was unstable, so adding SFT loss ({Joint Preference Set + SFT}), stabilized training, achieving 96\% format correctness and a CE loss of 4.45. Yet, this approach prioritized formatting over content quality, limiting further improvement in CE loss.

By separating the preference sets and including both formatting and summarization preferences in DPO training ({Separate Preference Sets + SFT}), our MRDS approach effectively balanced format correctness (92\%) and significantly lowered the CE loss to 4.13, indicating better alignment with summaries. This demonstrates that separating preference sets allows concurrent optimization of formatting and content, leading to the best overall performance.

\subsection{Statistical Analysis of Dialogue Summarization Experiments}
To further validate the improvements introduced by the MRDS method, we conduct each experiment three times under the 100‐shot setting on both the SAMSum and DialogSum datasets. For each run, we compute the mean and sample standard deviation (using $n-1$ in the denominator) for ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore. In addition, independent two-sample t-tests (with degrees of freedom $df=4$) were performed to assess the statistical significance of the differences between the baseline (summarization adapter fine-tuned with real data only) and MRDS results.

Tables~\ref{tab:stat_samsum} and~\ref{tab:stat_dialogsum} summarize the results for SAMSum and DialogSum, respectively. For instance, on SAMSum the ROUGE-1 score improved from an average of 50.90\% (std = 0.18\%) to 52.10\% (std = 0.25\%), a gain of 1.20 percentage points (t = 6.80, p $\approx$ 0.003). Similar statistically significant improvements are observed across all metrics and on both datasets.

\begin{table*}[htbp]
\centering
\caption{Statistical analysis on SAMSum 100-shot experiments (values in percentages). The baseline method adopts summarization adapter finetuned with real data only.}
\label{tab:stat_samsum}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{Baseline} (mean $\pm$ std) & \textbf{MRDS (Ours)} (mean $\pm$ std) & \textbf{Difference} & \textbf{t-value} & \textbf{p-value} \\
\midrule
ROUGE-1    & 50.90 $\pm$ 0.18   & 52.10 $\pm$ 0.25   & +1.20    & 6.80 & 0.003 \\
ROUGE-2    & 26.54 $\pm$ 0.04   & 27.54 $\pm$ 0.31   & +1.00    & 5.52 & 0.005 \\
ROUGE-L    & 42.62 $\pm$ 0.09   & 43.42 $\pm$ 0.35   & +0.80    & 3.87 & 0.018 \\
BERTScore  & 86.59 $\pm$ 0.04   & 86.81 $\pm$ 0.07   & +0.22    & 4.84 & 0.011 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Statistical analysis on DialogSum 100-shot experiments (values in percentages). The baseline method adopts summarization adapter finetuned with real data only.}
\label{tab:stat_dialogsum}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{Baseline} (mean $\pm$ std) & \textbf{MRDS (Ours)} (mean $\pm$ std) & \textbf{Difference} & \textbf{t-value} & \textbf{p-value} \\
\midrule
ROUGE-1    & 44.04 $\pm$ 0.55   & 45.47 $\pm$ 0.15   & +1.43    & 4.35 & 0.015 \\
ROUGE-2    & 18.23 $\pm$ 0.35   & 19.26 $\pm$ 0.27   & +1.03    & 4.06 & 0.017 \\
ROUGE-L    & 35.97 $\pm$ 0.49   & 37.25 $\pm$ 0.13   & +1.28    & 4.32 & 0.016 \\
BERTScore  & 86.81 $\pm$ 0.11   & 87.19 $\pm$ 0.02   & +0.38    & 5.86 & 0.008 \\
\bottomrule
\end{tabular}
\end{table*}

The t-test results confirm that the improvements achieved by MRDS over the baseline are statistically significant (p-values < 0.05) across all metrics on both datasets.


\subsection{Comparison with State-of-the-art Methods}
In Table \ref{tab:compare_sota}, we compare the proposed MRDS with three existing state-of-the-art methods that use data augmentation and pseudo labeling to address the data scarcity problem for dialogue summarization task.
The results show that our MRDS method achieves competitive or better performance in Rouge-1, Rouge-2 and Rouge-L.
In the table, we also compare the different backbone models, the amount of few-shot paired data and the need of unlabelled data among existing methods and MRDS.

\begin{table*}[htbp]
%\centering
\caption{Comparison among the proposed MRDS and three existing state-of-the-art methods (Semi-CODA \cite{chen2021simple}, SiCF \cite{he-etal-2024-semi} and COMPO \cite{ouyang2023compositional}) on SAMSum dataset. $^*$Using unlabled data.}
\label{tab:compare_sota}
%\begin{tabular}{l|lll|lll}
\begin{tabular}{lccccc}
\toprule
Methods     & backbone       & num. of paired data & Rouge-1 & Rouge-2 & Rouge-L \\ \hline
Semi-CODA$^*$   & BART-large               & 147                 & 42.16   & 17.82   & 38.89   \\
SiCF$^*$      & DialogLM                & 147                 & 45.85   & 19.90   & 35.96   \\
COMPO       & BART-large               & 147                 & 49.78   & 24.65   & 45.41   \\
MRDS (ours) & Llama3-8B-Instruct       & 100                 & 52.1    & 27.5    & 43.4    \\ 
\bottomrule
\end{tabular}
\end{table*}