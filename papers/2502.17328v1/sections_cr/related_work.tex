\paragraph{Low-resource/Few-shot Dialogue Summarization.}
Multiple lines of methods have been proposed to address the data sparsity problem of dialogue summarization.
%
\citet{yu-etal-2021-adaptsum} and \citet{zou-etal-2021-low} employ datasets from multiple source domains to conduct pre-training.
%
\citet{he-etal-2024-semi} rely on semi-supervised learning techniques such as pseudo labeling to incorporate the additional dialogue data without summary annotation.
%
\citet{xie-etal-2024-shot} and \citet{zhao-etal-2022-domain} design sophisticated prompt tuning strategies, enabling cross-task knowledge transfer.
%
Recently, several methods have leveraged synthetic data generation using external knowledge or unlabeled datasets. For instance, GENDEX \citep{park2024gendex} generates synthetic dialogues by utilizing external knowledge bases, thereby enriching the training data and improving model performance. Similarly, compositional data augmentation proposed by \citet{ouyang2023compositional} creates new training samples by recombining existing data, enhancing diversity without additional manual annotations. Additionally, \citet{tian2024dialogue} employed a mixture-of-experts framework that integrates external knowledge to enhance summarization capabilities.
While these approaches have demonstrated performance gains, they often depend on domain-specific resources, large unlabeled datasets, or external stronger models, which may not be available or practical in few-shot settings.
%While these approaches have demonstrated performance gains, they often depend on domain-specific resources, large unlabeled datasets, or pre-trained models, which may not be available or practical in few-shot settings.
This reliance on external data limits their applicability in scenarios where access to such resources is constrained. 
%\paragraph{Data Augmentation in NLP.}


\iffalse
\paragraph{Self-Alignment.}
Instruction following
output response with DPO

Direct Preference Optimization (DPO) is a technique for aligning language model outputs with desired preferences using preference data directly in the optimization process. Unlike reinforcement learning approaches such as Reinforcement Learning from Human Feedback (RLHF) \citep{bai2022training}, which require a separate reward model and can be computationally intensive, DPO integrates preference data into the training objective without additional overhead. \citet{rafailov2024direct} introduced DPO to efficiently fine-tune language models according to human preferences, demonstrating improvements in generating coherent and contextually appropriate responses. This method reduces reliance on external feedback mechanisms and is suitable for scenarios with limited computational resources.
\fi

\paragraph{Synthetic Data from LLMs}
Many previous works have shown that LLMs are capable of synthesizing high quality training data for machine learning models. 
One line of methods primarily focus on zero-shot learning scenario \citep{ye-etal-2022-zerogen, ye-etal-2022-progen, gao2023selfguided, meng2022generating, gupta2024targen}, where they sample data from LLMs based on task related prompts, and use the synthetic data to train small, task-specific models from scratch.
Other works also demonstrate the effectiveness of synthetic data from LLM in different domains such as speech recognition \citep{corpus-synthesis}, information extraction \citep{10.1145/3477495.3531863, josifoski-etal-2023-exploiting}, text-to-SQL \citep{yang-etal-2024-synthesizing}, and dialogue state tracking \citep{kulkarni-etal-2024-synthdst, mehri-etal-2022-lad}.
Recently, several works also investigate the idea of LLM self-improvement, suggesting that the synthetic data from LLMs can improve their own instruction following abilities.
Self-instruct \citep{wang-etal-2023-self-instruct} samples from an LLM to create a synthetic prompt-response paired dataset, which can be used to finetune the original LLM.
\citet{li2024selfalignment} introduce instruction backtranslation, which obtains synthetic instruction prompts from back-translating a web scale corpus with the same LLM.
\citet{gulcehre2023reinforced}, \citet{pmlr-v235-yuan24d}, and \citet{pmlr-v235-chen24j} pay attention to the generation of responses, but utilize them in different manners.
\citet{gulcehre2023reinforced} rely on an external scoring function to obtain the reward of synthetic responses.
\citet{pmlr-v235-yuan24d} propose a self-rewarding framework, using the LLM to score the response generated from itself.
\citet{pmlr-v235-chen24j} design a self-play mechanism, finetuning the LLM to distinguish the responses generated by the itself and human responses.
%
%In comparison with the previous approaches in LLM self-improvement, we improves two capabilities, dialogue synthesis and summarization simultaneously, via the proposed mutual reinforcement mechanism.