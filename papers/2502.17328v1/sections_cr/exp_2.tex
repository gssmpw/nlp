
% Our experiments utilize two widely used dialogue summarization datasets: SAMSum and DialogSum. Each dataset presents unique characteristics that help evaluate the effectiveness of our proposed approach across different types of dialogues.

%\paragraph{SAMSum}~\citep{gliwa-etal-2019-samsum} contains over 16,000 conversations with human-written summaries. These conversations are casual with colloquial language and abbreviations mimicking everyday chats among friends and family.

%\paragraph{DialogSum} \citep{chen2021dialogsum} includes about 13,460 face-to-face spoken dialogues each paired with an abstractive summary. Most of these conversations are between friends, colleagues, and between service providers and customers. They cover a variety of daily-life topics like shopping, travel, schooling, appointments, and customer service.

\paragraph{Datasets} We experiment with two widely-used benchmark datasets, namely SAMSum~\citep{gliwa-etal-2019-samsum} and DialogSum~\citep{chen2021dialogsum}. The SAMSum dataset contains over 16,000 casual conversations mimicking everyday chats among friends and family. The DialogSum dataset includes about 13,460 face-to-face spoken dialogues between friends, colleagues, and between service providers and customers, covering various daily-life topics. These datasets also provide a human-written summary for each dialogue. Since this work focuses on few-shot settings, for each dataset, we experiment with either 100 or 300 dialogue-summary pairs as the few-shot training dataset $\mathcal{D}_p$.

%It tests models' ability to summarize everyday conversations with diverse speakers.

% \paragraph{TODSum} TODSum \citep{zhao2021todsum} is a task-oriented dialogue summarization dataset featuring dialogues between users and virtual assistants or service agents in domains like restaurant reservations, flight bookings, and technical support. Its summaries capture user intent, system responses, and outcomes, focusing on goal-oriented dialogues.

% \paragraph{Few-shot setting}
%In our experiments, we evaluate our approach in the few shot learning scenario of dialogue summarization tasks. %Specifically
%We utilize 100/300 real dialogue-summary pairs as seed training dataset for dialogue summarization, summary/dialogue synthesis LoRA adapters.
% Since the focus of this work is on few-shot settings, for each dataset, we experiment with either 100 or 300 dialogue-summary pairs as the few-shot training dataset $\mathcal{D}_p$.

%\paragraph{Model} We use Llama3-8B-Instruct \citep{dubey2024llama} as the pretrained base LLM. For brevity, we will refer to this model as Llama3 in the rest of the paper. We use a rank of 16 and an alpha of 32 for all LoRA adapters, and keep the base model parameters frozen when training the LoRA adaptors. 

%\paragraph{Prompts} Table~\ref{tab:prompt} in the Appendix shows all the prompts used for topic extraction, topic-based  summary synthesis, summary-based dialogue synthesis and dialogue summarization tasks.

\subsection{Alternative Methods}
% We compare our MRDS method with several alternative approaches for dialogue summarization. 

% \textbf{Zero-shot:} Zero-shot summarization performance of Llama3. \textbf{ICL:} Summarization performance of Llama3 using in-context learning with $k=7$ examples.~\footnote{We experimented with different number of in-context examples and $k=7$ worked best.}

%For experiments with the pre-trained model, we evaluate zero-shot results ({\textbf{Zero}}) and in-context learning \citep{brown2020language} with $k=7$ examples ({\textbf{ICL}}) using the Llama3 model. For the fine-tuned Llama3-based summarization models, we include:

We compare our MRDS method with several dialogue summarization alternative approaches, divided into two categories: methods using the pre-trained Llama3 model without fine-tuning and methods fine-tuned on real or synthetic data.
\paragraph{Pre-trained Methods}
\begin{itemize} [label=-, topsep=0pt, itemsep=1pt, parsep=0pt, partopsep=0pt]
\item \textbf{Zero-shot:} Zero-shot summarization performance of Llama3. 
\item \textbf{ICL:} Summarization performance of Llama3 using in-context learning with $k=7$ examples.~\footnote{We experimented with different number of in-context examples and $k=7$ worked best.}
\end{itemize}
\paragraph{Fine-tuned Methods}
\begin{itemize} [label=-, topsep=0pt, itemsep=1pt, parsep=0pt, partopsep=0pt]
\item \textbf{Real only}: Fine-tuning with real data only.
\item \textbf{SFT}: Two-stage training using synthetic dialogues generated by the SFT dialogue synthesizer.
\item \textbf{SFT + Post-processing}: Two-stage training using synthetic dialogues from the SFT dialogue synthesizer, enhanced with Iterative Dialogue Synthesis (IDS) and content alignment filtering.
\end{itemize}


\subsection{Implementation Details}
% LLaMA3
We use Llama3-8B-Instruct \citep{dubey2024llama} as the pretrained base LLM. We use a rank of 16 and an alpha of 32 for all LoRA adapters, and keep the base model parameters frozen while training the LoRA adaptors. Table~\ref{tab:prompt} shows all the prompts used for topic extraction, topic-based  summary synthesis, summary-based dialogue synthesis, and dialogue summarization tasks. All presented results are averaged over three runs.

\paragraph{Dialogue Summarization}
% peft settings, domain shifting training
%For the real-only dataset, we search for the best hyper-parameter for the baseline and all the following experiments.
For the baseline model trained exclusively on real data, we optimized the hyperparameters and applied the same settings to all subsequent experiments for consistency. Our training strategy includes a batch size of 10 and a maximum learning rate of $2.0 \times 10^{-4}$ with a warmup over the first 50 batches. We use the \texttt{ReduceLROnPlateau} scheduler with a patience of 5 and a reduction factor of 0.7. Training is stopped if the loss does not improve for 100 steps. We select the best checkpoint based on the validation loss obtained during the real data training phase.

% Continue training until lr reaches the minimum $2.0 \times 10^{-5}$. Real training follows the same LR schedule with warmup, max, and min learning rates. 
In synthetic data experiments, we employ a two-stage training approach using the same hyperparameters. In the first phase, we train exclusively on synthetic data until the learning rate reduces to $2.0 \times 10^{-5}$, effectively serving as a pre-training phase. In the second phase, we apply the same training strategy as in the real-only experiments to ensure a fair comparison.



\paragraph{Dialogue Synthesis}
% peft and DPO settings
For the dialogue synthesizer trained with SFT only, we use a learning rate of $2.0 \times 10^{-4}$ along with the \texttt{ReduceLROnPlateau} scheduler. The batch size and other hyperparameters are the same as those used for dialogue summarization. When training the synthesizer using both SFT and DPO, we start from the SFT checkpoint. In this combined training, we use a batch size of four for DPO and one for SFT, jointly updating the dialogue synthesizer by combining the losses from both objectives. A fixed learning rate of $1 \times 10^{-5}$ is used during this phase. We validate the synthesizer checkpoints on the official validation set of the dataset, evaluating both format correctness and summarization cross-entropy loss. We select the checkpoint with the lowest summarization CE loss, ensuring at least 85\% format correctness. Detailed training hyperparameters are provided in Table~\ref{tab:hyp}.

% what learning rate and schedule, batch size? Stopping criteria is based on standard SFT validation loss on real validation dataset dialogues. Stop if the loss doesn't improve for 50 steps. 

% When the synthesizer is trained using both SFT and DPO, we use the SFT checkpoint to begin with.

% What is the ratio of the format and content pairs? Is there a weight for SFT  loss? What is the DPO loss batch size and SFT loss batch size? Learning rate and its schedule for SFT + DPO training? Stopping criterion when training synthesizer? 
% This training uses fixed learning rate of $1e-5$.
% We validate synthesizer checkpoints on a validation dataset (official real validation set of the dataset) using format correctness and summarization loss criteria. Pick the checkpoint without lowers summarization CE loss while being at least 85\% correct for format.


%For dialogue synthesis, we use the same hyperparameters as in dialogue summarization during the initial supervised training phase. We then apply Direct Preference Optimization (DPO) training to the supervised model. The DPO preference sets are constructed with equal parts focusing on format-based preference pairs and conent-based preference pairs. To stabilize the training of the dialogue synthesis model, as explained in Section~\ref{sec:dialogue}, we incorporate the supervised fine-tuning (SFT) data into the DPO training at a ratio of one to four.


% For dialogue summarization, in addition to the proposed MRDS method, we also list some alternative methods for comparison. For the experiment with the pretrained model, we compare with Llama3 modelâ€”zero-shot results from the pre-trained model (Zero) and in-context learning ($k=7$) with the pre-trained model ({ICL}). For the fine-tuned Llama3-based summarization model, we compare with fine-tuned with real data only (Real only), two-stages training with synthetic dialogue from SFT dialogue synthesizer (SFT), and two-stages training with synthetic dialogue from SFT dialogue synthesizer with IDS and summarization loss filtering (SFT + Post-processing). 