\paragraph{Datasets} We experiment with two widely-used benchmark datasets, namely SAMSum~\citep{gliwa-etal-2019-samsum} and DialogSum~\citep{chen2021dialogsum}. The SAMSum dataset contains over 16,000 casual conversations mimicking everyday chats among friends and family. The DialogSum dataset includes about 13,460 face-to-face spoken dialogues between friends, colleagues, and between service providers and customers, covering various daily-life topics. These datasets also provide a human-written summary for each dialogue. Since this work focuses on few-shot settings, for each dataset, we experiment with either 100 or 300 dialogue-summary pairs as the few-shot training dataset $\mathcal{D}_p$.

\paragraph{Model} We use Llama3-8B-Instruct \citep{dubey2024llama} as the pretrained base LLM. For brevity, we will refer to this model as Llama3 in the rest of the paper. We use a rank of 16 and an alpha of 32 for all LoRA adapters, and keep the base model parameters frozen when training the LoRA adaptors. 

\paragraph{Alternative approaches:} \textit{Zero-shot:} Zero-shot performance of pretrained Llama3. \textit{ICL:} Performance of pretrained Llama3 with in-context learning using $k=7$ examples.~\footnote{We experimented with different number of in-context examples and $k=7$ worked best.} \textit{Real-only-SFT:} Supervised finetuning of the summarization adapter using only few-shot real data $\mathcal{D}_p$. \textit{SFT-based synthesizer:} The summarization model is trained using SFT with both few-shot real data $\mathcal{D}_p$ and synthetic dialogue-summary pairs generated using a dialogue synthesizer that has been trained only using SFT with few-shot real data $\mathcal{D}_p$.

\paragraph{Prompts} Table~\ref{tab:prompt} in the Appendix shows the prompts used for topic extraction, topic-based summary synthesis, summary-based dialogue synthesis, zero-shot summarization, summarization with ICL, and summarization with trained adapter.

\paragraph{Metrics:} We use BERTScore \citep{bert-score}, ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L (R-L) \citep{lin-2004-rouge} as evaluation metrics to compare the summarization output with the groundtruth summary.






\begin{table*}[ht]
\centering
\caption{Summarization performance of various methods in few-shot settings.}
\label{tab:summarization}
\begin{tabular}{lcccc|cccc}
\toprule
\multirow{2}{*}{Approach} & \multicolumn{4}{c}{SAMSum} & \multicolumn{4}{c}{DialogSum} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
\midrule
% Zero shots & 31.3 & 12.3 & 23.9 & 81.2 & \\
Zero shot & 31.3 & 12.3 & 23.9 & 81.2 & 28.2 & 10.0 & 21.4 & 81.6\\
ICL (k=7) & 39.5 & 17.6 & 30.9 & 83.2 & 31.4 & 11.9 & 24.5 & 83.1\\
\midrule
\textbf{100 Real shots} \\
\midrule
Real only & 50.9 & 26.5 & 42.6 & 86.6 & 44.0 & 18.2 & 36.0 & 86.8 \\ 
% \midrule
SFT & 50.9 & 26.5 & 42.6 & 86.5 & 45.1 & 19.0 & 36.9 & 86.9 \\ 
SFT + Post-processing & 51.8 & 27.3 & \textbf{43.5} & 86.7 & 44.7 & 18.8 & 36.5 & 87.1 \\
MRDS (ours) & \textbf{52.1} & \textbf{27.5} & 43.4 & \textbf{86.8} & \textbf{45.5} & \textbf{19.3} & \textbf{37.2} & \textbf{87.2} \\
\midrule
\textbf{300 Real shots} \\
\midrule
Real only & 51.1 & 26.9 & 42.8 & 86.5 & 45.2 & 19.5 & 37.3 & 87.2 \\
% \midrule
SFT & 52.1 & 27.6 & 43.7 & 86.8 & 45.9 & 19.8 & 37.7 & 87.2 \\ 
SFT + Post-processing & \textbf{52.7} & 28.1 & 44.1 & \textbf{87.0} & 46.1 & 20.0 & 37.6 & 87.3 \\
MRDS (ours) & \textbf{52.7} & \textbf{28.3} & \textbf{44.4} & \textbf{87.0} & \textbf{47.0} & \textbf{20.4} & \textbf{38.6} & \textbf{87.5} \\

\bottomrule
\end{tabular}
\end{table*}



\textbf{SFT + Post-processing}: Two-stage training using synthetic dialogues from the SFT dialogue synthesizer, enhanced with Iterative Dialogue Synthesis (IDS) and content alignment filtering.




\paragraph{Dialogue Summarization}
% peft settings, domain shifting training
%For the real-only dataset, we search for the best hyper-parameter for the baseline and all the following experiments.
For the baseline model trained exclusively on real data, we optimized the hyperparameters and applied the same settings to all subsequent experiments for consistency. Our training strategy includes a batch size of 10 and a maximum learning rate of $2.0 \times 10^{-4}$ with a warmup over the first 50 batches. We use the \texttt{ReduceLROnPlateau} scheduler with a patience of 5 and a reduction factor of 0.7. Training is stopped if the loss does not improve for 100 steps. We select the best checkpoint based on the validation loss obtained during the real data training phase.

% Continue training until lr reaches the minimum $2.0 \times 10^{-5}$. Real training follows the same LR schedule with warmup, max, and min learning rates. 
In synthetic data experiments, we employ a two-stage training approach using the same hyperparameters. In the first phase, we train exclusively on synthetic data until the learning rate reduces to $2.0 \times 10^{-5}$, effectively serving as a pre-training phase. In the second phase, we apply the same training strategy as in the real-only experiments to ensure a fair comparison.



\paragraph{Dialogue Synthesis}
% peft and DPO settings
For the dialogue synthesizer trained with SFT only, we use a learning rate of $2.0 \times 10^{-4}$ along with the \texttt{ReduceLROnPlateau} scheduler. The batch size and other hyperparameters are the same as those used for dialogue summarization. When training the synthesizer using both SFT and DPO, we start from the SFT checkpoint. In this combined training, we use a batch size of four for DPO and one for SFT, jointly updating the dialogue synthesizer by combining the losses from both objectives. A fixed learning rate of $1 \times 10^{-5}$ is used during this phase. We validate the synthesizer checkpoints on the official validation set of the dataset, evaluating both format correctness and summarization cross-entropy loss. We select the checkpoint with the lowest summarization CE loss, ensuring at least 85\% format correctness. Detailed training hyperparameters are provided in Table~\ref{tab:hyp}.

% what learning rate and schedule, batch size? Stopping criteria is based on standard SFT validation loss on real validation dataset dialogues. Stop if the loss doesn't improve for 50 steps. 

% When the synthesizer is trained using both SFT and DPO, we use the SFT checkpoint to begin with.

% What is the ratio of the format and content pairs? Is there a weight for SFT  loss? What is the DPO loss batch size and SFT loss batch size? Learning rate and its schedule for SFT + DPO training? Stopping criterion when training synthesizer? 
% This training uses fixed learning rate of $1e-5$.
% We validate synthesizer checkpoints on a validation dataset (official real validation set of the dataset) using format correctness and summarization loss criteria. Pick the checkpoint without lowers summarization CE loss while being at least 85\% correct for format.


%For dialogue synthesis, we use the same hyperparameters as in dialogue summarization during the initial supervised training phase. We then apply Direct Preference Optimization (DPO) training to the supervised model. The DPO preference sets are constructed with equal parts focusing on format-based preference pairs and conent-based preference pairs. To stabilize the training of the dialogue synthesis model, as explained in Section~\ref{sec:dialogue}, we incorporate the supervised fine-tuning (SFT) data into the DPO training at a ratio of one to four.


% For dialogue summarization, in addition to the proposed MRDS method, we also list some alternative methods for comparison. For the experiment with the pretrained model, we compare with Llama3 model—zero-shot results from the pre-trained model (Zero) and in-context learning ($k=7$) with the pre-trained model ({ICL}). For the fine-tuned Llama3-based summarization model, we compare with fine-tuned with real data only (Real only), two-stages training with synthetic dialogue from SFT dialogue synthesizer (SFT), and two-stages training with synthetic dialogue from SFT dialogue synthesizer with IDS and summarization loss filtering (SFT + Post-processing). 

















\begin{table*}[ht]
\centering
\caption{Comparison of Summarization Methods on 100 and 300 shots.}
\label{tab:summarization}
\begin{tabular}{lcccc|cccc}
\toprule
\multirow{2}{*}{Approach} & \multicolumn{4}{c}{SAMSum} & \multicolumn{4}{c}{DialogSum} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
\midrule
% Zero shots & 31.3 & 12.3 & 23.9 & 81.2 & \\
Zero shot & 31.3 & 12.3 & 23.9 & 81.2 & 28.2 & 10.0 & 21.4 & 81.6\\
ICL (k=7) & 39.5 & 17.6 & 30.9 & 83.2 & 31.4 & 11.9 & 24.5 & 83.1\\
\midrule
\textbf{100 Real shots} \\
\midrule
Real only & 50.9 & 26.5 & 42.6 & 86.6 & 44.0 & 18.2 & 36.0 & 86.8 \\ 
% \midrule
SFT & 50.9 & 26.5 & 42.6 & 86.5 & 45.1 & 19.0 & 36.9 & 86.9 \\ 
SFT + Post-processing & 51.8 & 27.3 & \textbf{43.5} & 86.7 & 44.7 & 18.8 & 36.5 & 87.1 \\
MRDS (ours) & \textbf{52.1} & \textbf{27.5} & 43.4 & \textbf{86.8} & \textbf{45.5} & \textbf{19.3} & \textbf{37.2} & \textbf{87.2} \\
\midrule
\textbf{300 Real shots} \\
\midrule
Real only & 51.1 & 26.9 & 42.8 & 86.5 & 45.2 & 19.5 & 37.3 & 87.2 \\
% \midrule
SFT & 52.1 & 27.6 & 43.7 & 86.8 & 45.9 & 19.8 & 37.7 & 87.2 \\ 
SFT + Post-processing & \textbf{52.7} & 28.1 & 44.1 & \textbf{87.0} & 46.1 & 20.0 & 37.6 & 87.3 \\
MRDS (ours) & \textbf{52.7} & \textbf{28.3} & \textbf{44.4} & \textbf{87.0} & \textbf{47.0} & \textbf{20.4} & \textbf{38.6} & \textbf{87.5} \\

\bottomrule
\end{tabular}
\end{table*}
%\midrule
%MOE (full-shot)\citep{tian2024dialogue} & 55.9 & 30.9 & 52.0 & 75.6 & 49.8 & 24.8 & 47.4 & 68.5 \\

% \begin{table*}[ht]
% \centering
% \caption{Comparison of Summarization Methods on 100 shots}
% \label{tab:100shots}
% \begin{tabular}{lcccc|cccc}
% \toprule
% \multirow{2}{*}{Approach} & \multicolumn{4}{c}{SAMSum} & \multicolumn{4}{c}{DialogSum} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% & R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
% \midrule
% % Zero shots & 31.3 & 12.3 & 23.9 & 81.2 & \\
% Zero shots + Length & 38.4 & 12.7 & 30.1 & 81.9 & 34.1 & 11.4 & 27.1 & 79.5\\
% ICL (k=7) & 39.5 & 17.6 & 30.9 & 83.2 & 31.4 & 11.9 & 24.5 & 83.1\\
% \midrule
% 100 Real shots only & 50.9 & 26.5 & 42.6 & 86.6 & 44.0 & 18.2 & 36.0 & 86.8 \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% Post-processing & 51.8 & 27.3 & 43.5 & 86.7 & 44.7 & 18.8 & 36.5 & 87.1 \\
% DPO & 52.1 & 27.5 & 43.4 & 86.8 & 45.5 & 19.3 & 37.2 & 87.2 \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% Post-processing & 52.5 & 28.1 & 44.3 & 87.0 & 45.3 & 19.7 & 37.4 & 87.0 \\
% DPO & 52.5 & 28.3 & 44.5 & 87.0 & 46.5 & 20.3 & 38.3 & 87.4 \\
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table*}[ht]
% \centering
% \caption{Comparison of Summarization Methods on 300 shots}
% \label{tab:300shots}
% \begin{tabular}{lcccc|cccc}
% \toprule
% \multirow{2}{*}{Approach} & \multicolumn{4}{c}{SAMSum} & \multicolumn{4}{c}{DialogSum} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% & R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
% \midrule
% 300 Real shots only & 51.1 & 26.9 & 42.8 & 86.5 & 45.2 & 19.5 & 37.3 & 87.2 \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% Post-processing & 52.7 & 28.1 & 44.1 & 87.0 & 46.1 & 20.0 & 37.6 & 87.3 \\
% DPO & 52.7 & 28.3 & 44.4 & 87.0 & 47.0 & 20.4 & 38.6 & 87.5 \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% Post-processing & 53.0 & 28.3 & 44.5 & 87.1 & 46.8 & 20.8 & 38.8 & 87.6 \\
% DPO & 52.8 & 28.3 & 44.4 & 87.0 & 47.0 & 20.7 & 38.7 & 87.5 \\
% \bottomrule
% \end{tabular}
% \end{table*}

\subsection{Dialogue Summarization}
We conducted experiments on the SAMSum and DialogSum datasets to evaluate the effectiveness of our proposed mutual reinforcing data synthesis (MRDS) method. The results are presented in Table~\ref{tab:summarization}, comparing various summarization approaches under 100-shot and 300-shot settings using metrics such as ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (R-L) \citep{lin-2004-rouge}, and BERTScore \citep{bert-score}.

In the zero-shot setting, the pre-trained LLM (Zero shot) achieves R-1 scores of 31.3 on SAMSum and 28.2 on DialogSum. The in-context learning approach (ICL) largely improves the R-1 score on SAMSum to 39.5 and DialogSum to 31.4, indicating that ICL provides efficient improvement in low-resource scenarios.

When fine-tuning with 100 real shots, the real-only method significantly improves performance over zero-shot methods, achieving R-1 scores of 50.9 on SAMSum and 44.0 on DialogSum. 
Incorporating dialogues from the SFT model maintains similar performance, while post-processing techniques (SFT + Post-processing) further enhance results, increasing R-1 scores to 51.8 on SAMSum and 44.7 on DialogSum, indicating the effectiveness of IDS and content alignment filtering. 
In the 300-shot setting, all methods benefit from additional training data. 
The real-only method reaches R-1 scores of 51.1 on SAMSum and 45.2 on DialogSum, with {SFT} showing incremental gains, and {SFT + Post-processing} achieving R-1 scores of 52.7 on SAMSum and 46.1 on DialogSum.

Our proposed MRDS method outperforms all approaches in both 100-shot and 300-shot settings. 
In the 100-shot setting, MRDS achieves the highest R-1 scores of 52.1 on SAMSum and 45.5 on DialogSum, along with improvements in R-2, R-L, and BERTScore metrics, highlighting its ability to leverage synthesized data effectively. 
In the 300-shot setting, MRDS continues to deliver the best performance, matching the top R-1 score of 52.7 on SAMSum and setting a new high of 47.0 on DialogSum. 
The method consistently delivers superior ROUGE and BERTScore values, highlighting its robustness and scalability with increased data. This shows that MRDS not only improves efficiency by eliminating post-processing steps but also significantly boosts summarization performance.


% Additionally, in experiments with unseen summaries, we found that incorporating additional information can further improve performance by 1.4\% to 2.3\% in ROUGE scores, achieving the best summarization scores. This indicates that combining extra summaries in the same style can enhance results, even without the need for additional dialogues.

% \paragraph{TODSum}

% Since the TODSum dataset does not include speaker names before each sentence, it does not encounter formatting issues. Therefore, we applied only summarization filtering as post-processing, and for DPO, we utilized only the summarization preference pairs. We compared experiments on 100-shot samples of TODSum, as shown in Table~\ref{tab:todsum}. In addition to the baseline, we included results from models using directly generated dialogues after filtering and models further refined with DPO to demonstrate the effectiveness of post-processing and preference fine-tuning.

% The results show that using raw dialogues generated from synthetic summaries improves the baseline by an average of 1.3\%. Further improvements of 3.7\% are achieved when applying filtering or DPO. For experiments with unseen summaries, the improvements are even more significant, with an 8\% increase using raw dialogues and an 11\% increase after filtering and DPO. These findings suggest that both data filtering and DPO bring substantial enhancements to the summarization model.



% \begin{table}[ht]
% \centering
% \caption{Comparison of Summarization Methods on TODSum 100 shots.}
% \label{tab:todsum}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Approach} & \multicolumn{4}{c}{TODSum 100 shots} \\
% \cmidrule(lr){2-5}
% & R-1 & R-2 & R-L & B-S \\
% \midrule
% Real only & 73.4 & 52.2 & 62.6 & 91.9 \\
% \midrule
% \multicolumn{5}{l}{\textbf{Synthetic Summaries}} \\

% \midrule
% w/o Filtering & 73.9 & 53.5 & 64.0 & 92.1 \\
% Filtering & 74.9 & 55.9 & 65.3 & 92.5 \\
% DPO &  75.0 & 55.9 & 65.1 & 92.5 \\
% \midrule
% \multicolumn{5}{l}{\textbf{Unseen Summaries}} \\
% \midrule
% w/o Filtering & 77.2 & 60.2 & 68.6 & 93.2 \\
% Filtering & 79.5 & 63.0 & 71.3 & 93.8 \\
% DPO & 79.6 & 63.1 & 71.5 & 93.9 \\
% \bottomrule
% \end{tabular}
% \end{table}


% \begin{table*}[ht]
% \centering
% \caption{Comparison of Summarization Methods on SAMSum Datasets}
% \label{tab:samsum}
% \begin{tabular}{lcccc|cccc}
% \toprule
% \multirow{2}{*}{Approach} & \multicolumn{4}{c}{100 shots} & \multicolumn{4}{c}{300 shots} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% & R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
% \midrule
% Real only & 50.9 & 26.5 & 42.6 & 86.6 & 51.1 & 26.9 & 42.8 & 86.5 \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% Post-processing & 51.8 & 27.3 & 43.5 & 86.7 & 52.7 & 28.1 & 44.1 & 87.0 \\
% DPO & 52.1 & 27.5 & 43.4 & 86.8 & 52.7 & 28.3 & 44.4 & 87.0  \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% Post-processing & 52.5 & 28.1 & 44.3 & 87.0 & 53.0 & 28.3 & 44.5 & 87.1 \\
% DPO & 52.5 & 28.3 & 44.5 & 87.0 & 52.8 & 28.3 & 44.4 & 87.0 \\
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table*}[ht]
% \centering
% \caption{Comparison of Summarization Methods on DialogSum Datasets}
% \label{tab:dialogsum}
% \begin{tabular}{lcccc|cccc}
% \toprule
% \multirow{2}{*}{Approach} & \multicolumn{4}{c}{100 shots} & \multicolumn{4}{c}{300 shots} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% & R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
% \midrule
% Real only & 44.0 & 18.2 & 36.0 & 86.8 & 45.2 & 19.5 & 37.3 & 87.2 \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% Post-processing & 44.7 & 18.8 & 36.5 & 87.1 & 46.1 & 20.0 & 37.6 & 87.3 \\
% DPO & 45.5 & 19.3 & 37.2 & 87.2 & 47.0 & 20.4 & 38.6 & 87.5 \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% Post-processing & 45.3 & 19.7 & 37.4 & 87.0 & 46.8 & 20.8 & 38.8  & 87.6 \\
% DPO & 46.5 & 20.3 & 38.3 & 87.4 & 47.0 & 20.7 & 38.7 & 87.5 \\
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table*}[ht]
% \centering
% \caption{Comparison of Summarization Methods on TODSum Datasets}
% \label{tab:todsum}
% \begin{tabular}{lcccc|cccc}
% \toprule
% \multirow{2}{*}{Approach} & \multicolumn{4}{c}{100 shots} & \multicolumn{4}{c}{300 shots} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% & R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
% \midrule
% Real only & 73.4 & 52.2 & 62.6 & 91.9 & 81.2 & 64.9 & 73.5 & 94.3  \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% w/o Filtering & 73.9 & 53.5 & 64.0 & 92.1 & 82.0 & 66.2 & 74.3 & 94.5 \\
% Filtering & 74.9 & 55.9 & 65.3 & 92.5 & 82.6 & 67.4 & 75.3 & 94.7 \\
% DPO &  75.0 & 55.9 & 65.1 & 92.5 & 82.4 & 66.8 & 74.8 & 94.6 \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% w/o Filtering & 77.2 & 60.2 & 68.6 & 93.2 & {81.8} & {66.3} & {74.1} & {94.5} \\
% Filtering & 79.5 & 63.0 & 71.3 & 93.8 & {83.4} & {68.8} & {76.4} & {95.0} \\
% DPO & 79.6 & 63.1 & 71.5 & 93.9 & 85.4 & 72.7 & 79.4 & {95.5} \\
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table*}[th]
% \caption{Human and GPT-4 evaluation results on SAMSum with 300-shot data, covering informativeness (\textbf{Info.}), faithfulness (\textbf{Faith.}), fluency (\textbf{Flu.}), and redundancy (\textbf{Redun.}). The \textbf{R+S} experiments utilized synthesized summaries and dialogues.}
% \label{tab:sub}
% \centering
% \begin{tabular}{cc} 
%     \begin{subtable}{0.474\linewidth}
%         \centering
%         \begin{tabular}{lcccc}
%         \toprule
%          & \textbf{Info.} & \textbf{Faith.} & \textbf{Flu.} & \textbf{Redun.} \\
%         \midrule
%         \textbf{GT}   & 1.47 & 1.71 & 1.82 & 1.88 \\
%         \textbf{Real} & 1.00 & 1.70 & \textbf{1.95} & 1.75 \\
%         \textbf{R+S (Ours)}  & 1.45 & \textbf{1.95} & \textbf{1.95} & \textbf{1.95} \\
%         \textbf{Zero} & \textbf{1.94} & 1.78 & \textbf{1.94} & 0.87 \\
%         \bottomrule
%         \end{tabular}
%         \caption{Human evaluation.}
%         \label{tab:he}
%     \end{subtable} & % Separate the tables with an '&' to put them side by side
    
%     \begin{subtable}{0.474\linewidth}
%         \centering
%         \begin{tabular}{lcccc}
%         \toprule
%          & \textbf{Info.} & \textbf{Faith.} & \textbf{Flu.} & \textbf{Redun.} \\
%         \midrule
%         \textbf{GT}   & 1.75 & 1.89 & \textbf{1.99} & 1.94 \\
%         \textbf{Real} & 1.50 & 1.81 & 1.98 & \textbf{1.97} \\
%         \textbf{R+S (Ours)}  & 1.62 & 1.83 & \textbf{1.99} & \textbf{1.97} \\
%         \textbf{Zero} & \textbf{1.99} & \textbf{1.96} & \textbf{2.00} & 1.70 \\
%         \bottomrule
%         \end{tabular}
%         \caption{GPT-4 evaluation.}
%         \label{tab:gpt}
%     \end{subtable}
% \end{tabular}
% \end{table*}

\begin{table}[th]
\caption{Human evaluation results on SAMSum with 300-shot data, covering informativeness (inf.), faithfulness (fai.), fluency (flu.), redundancy (red.), and the average (ave.) of the four scores.}
% The \textbf{R+S} experiments utilized synthesized summaries and dialogues.}
\label{tab:sub}
\centering
\begin{tabular}{lcccc|c}
\toprule
     & \textbf{Inf.} & \textbf{Fai.} & \textbf{Flu.} & \textbf{Red.} & \textbf{Ave.} \\
\midrule
% \textbf{GT}            & 1.17 & 1.71 & 1.86 & 1.57 \\
% \midrule
\textbf{Zero}          & \textbf{1.95} & 1.50 & \underline{1.94} & 0.83 & 1.56 \\
% \textbf{Zero + len}    & 1.63 & \underline{1.77} & 1.88 & \textbf{2.00} & \underline{1.82} \\
\textbf{ICL}           & \underline{1.65} & 1.69 & \textbf{2.00} & 1.23 & 1.64 \\
% \midrule
\textbf{Real}          & 1.32 & 1.61 & \textbf{2.00} & 1.94 & 1.72 \\
\textbf{MRDS}           & 1.55 & \textbf{1.88} & \underline{1.94} & \textbf{2.00} & \textbf{1.84} \\
\bottomrule
\end{tabular}
\end{table}


\begin{table*}[htp]
\centering
\begin{tabular}{lccc}
\toprule
 \textbf{Synthesis Model} & \textbf{Synthesis Method} & \textbf{Format Corr.} & \textbf{Summ. CE} \\
\midrule
SFT  &  One-shot & 24\% & 4.74 \\
SFT & Iterative  & 100\%* & 4.69 \\
\midrule
Joint Preference Set  & One-shot &  78\% & 4.62 \\
Joint Preference Set + SFT  & One-shot &  \textbf{96\%} & \underline{4.45} \\
Separate Preference Sets + SFT (MRDS) & One-shot &  \underline{92\%} & \textbf{4.13} \\
\bottomrule
\end{tabular}
\caption{Dialouge Synthesis with different training strategy on SAMSum 300 shots experiments.}
\label{tab:DPO abl}
\end{table*}

\begin{table}[htp]
\centering
\caption{Comparison of efficiency for dialogue synthesis with (Dialogues/hour).}
\label{tab:eff}
\begin{tabular}{lcc}
\toprule
% \multirow{2}{*}{Approach}  & \multicolumn{2}{c}{SAMSum}\\ 
% % & \multicolumn{2}{c}{DialogSum}  \\
% \cmidrule(lr){2-3} 
{Approach} & {100 shots} & {300 shots}  \\
\midrule
\multicolumn{3}{l}{\textbf{SAMSum}} \\
\midrule
Post-Processing & 63 & 37.5  \\
MRDS & \textbf{550} & \textbf{2900}  \\
\midrule
\multicolumn{3}{l}{\textbf{DialogSum}} \\
\midrule 
Post-Processing & 85 & 66.7  \\
MRDS & \textbf{315} & \textbf{2500} \\
% \midrule
\bottomrule
\end{tabular}
\end{table}



\subsection{Human Evaluation}
\label{sec:he}

Table~\ref{tab:sub} presents the results of human evaluations, comparing four groups of summaries: two from the pretrained instructed Llama3 model—zero-shot results (Zero) and in-context learning (ICL)—and two from the fine-tuned Llama3-based summarization model: trained on real data only (Real) and trained with the proposed MRDS approach, which combines real and synthetic summaries and dialogues. Five human evaluators assessed the summaries based on informativeness, faithfulness, fluency, and redundancy, using a scale from zero to two, following the evaluation protocol from \citep{xie-etal-2024-shot}. The average scores across the four metrics are also reported.

For the pre-trained models, while the zero-shot model (Zero) achieves the highest score in informativeness (1.95), it scores poorly in redundancy (0.83), indicating a tendency to produce overly long summaries by including too much information --undesirable in summarization tasks. 
% Providing expected length information in the prompt ({Zero+len}) resolves the length issue, improving redundancy to the maximum score of 2.00 and providing notable improvements compared to other pre-trained baselines. 
The in-context learning model (ICL) shows slight improvements in faithfulness (1.69 vs. 1.50) and redundancy (1.23 vs. 0.83) compared to zero-shot, indicating that it generates more concise and faithful summaries but still inherits some limitations of the pre-trained model.
For the fine-tuned models, the Real approach outperforms the pre-trained models in overall average score (1.72 vs. 1.56 for Zero and 1.64 for ICL), demonstrating the benefit of fine-tuning with real data in improving summary quality.
% However, despite outperforming the {Zero+len} model in ROUGE and BERT scores (as shown in Table~\ref{tab:summarization}), it shows a lower average in human evaluation (1.72 vs.\ 1.82). This suggests that the pre-trained Llama3 with length guidance aligns better with human preferences.
However, our proposed MRDS method achieves the highest average score (1.84), outperforming both the real-only and pre-trained models, particularly in faithfulness and redundancy. 
% slightly outperforming the best pre-trained model with length guidance ({Zero+len}, average 1.82)
This suggests that incorporating synthesized data helps the model produce more precise and concise summaries. The results highlight that our proposed MRDS approach significantly enhances summarization quality from the perspective of human evaluators. Example summaries from different approaches are shown in Table~\ref{tab:summ_example_1}.

% Table~\ref{tab:sub} presents the subjective results from both human and GPT-4 evaluations. We compare four groups of summaries: ground truth (\textbf{GT}) from human annotations, and three outputs from a Llama3-based summarization model: trained on real data only (\textbf{Real}), trained with a combination of real and synthesized summaries and dialogues (\textbf{R+S}), and zero-shot results from the pre-trained model (\textbf{Zero}). Five human evaluators assessed the summaries based on the dialogues across four aspects—informativeness, faithfulness, fluency, and redundancy—using a scoring scale from zero to two, following the evaluation protocol in \citep{evaluation_reference}. We also applied GPT-4 evaluation to the same set of dialogues and summaries.

% From the human evaluation results in Table~\ref{tab:he}, we observe that our proposed method (\textbf{R+S}) significantly outperforms the \textbf{Real} baseline in three of four aspects. Notably, the \textbf{R+S} model even surpasses the ground truth (\textbf{GT}) in metrics such as faithfulness and redundancy, indicating that the integration of synthesized data enhances the model's ability to generate accurate and concise summaries. This suggests that our self-aligned LLM effectively captures essential information and presents it more coherently than models trained solely on real data.

% While the zero-shot model (\textbf{Zero}) achieves the highest score in informativeness, it performs significantly worse in redundancy, scoring only 0.87 compared to 1.95 by our method. This implies that the zero-shot model tends to produce overly long summaries, attempting to include all information from the dialogues without prioritizing conciseness—a less desirable trait in summarization tasks. The superior performance of our \textbf{R+S} model in redundancy demonstrates its ability to generate summaries that are both informative and succinct.

% These human evaluation results underscore the effectiveness of our proposed method. The significant improvements across three aspects highlight the impact of combining real and synthesized data through self-alignment. By leveraging the dialogue synthesis capabilities of LLMs, our approach not only enhances the summarization performance but also surpasses the limitations of models trained on limited real data. This demonstrates the potential of self-aligned LLMs in advancing dialogue summarization, particularly in few-shot scenarios where data scarcity is a challenge.

% In the GPT-4 evaluation shown in Table~\ref{tab:gpt}, our proposed method (\textbf{R+S}) continues to outperform the \textbf{Real} baseline across all metrics. As anticipated, the zero-shot model is favored by GPT-4, consistent with previous findings that GPT-4 tends to prefer summaries generated by pre-trained LLMs \citep{gpt4_bias_reference}. However, regarding redundancy, both the \textbf{R+S} and \textbf{Real} fine-tuned models are preferred over the zero-shot model, reinforcing the observation that the zero-shot model generates excessively lengthy summaries.


% \begin{table}[ht]
% \centering
% \begin{tabular}{lcccc}
% \toprule
%  & \textbf{Info.} & \textbf{Faith.} & \textbf{Flu.} & \textbf{Redun.} \\
% \midrule
% \textbf{GT}   & 1.47 & 1.71 & 1.82 & 1.88 \\
% \textbf{Real} & 0.80 & 1.70 & \textbf{1.95} & 1.75 \\
% \textbf{R+S}  & 1.53 & \textbf{1.95} & \textbf{1.95} & \textbf{1.95} \\
% \textbf{Zero} & \textbf{1.94} & 1.78 & \textbf{1.94} & 0.87 \\
% \bottomrule
% \end{tabular}
% \caption{Results for human evaluation.}
% \end{table}

% \begin{table}[ht]
% \centering
% \begin{tabular}{lcccc}
% \toprule
%  & \textbf{Info.} & \textbf{Faith.} & \textbf{Flu.} & \textbf{Redun.} \\
% \midrule
% \textbf{GT}   & 1.75 & 1.89 & \textbf{1.99} & 1.94 \\
% \textbf{Real} & 1.50 & 1.81 & 1.98 & \textbf{1.97} \\
% \textbf{R+S}  & 1.62 & 1.83 & \textbf{1.99} & \textbf{1.97} \\
% \textbf{Zero} & \textbf{1.99} & \textbf{1.96} & \textbf{2.00} & 1.70 \\
% \bottomrule
% \end{tabular}
% \caption{Results for gpt4 evaluation.}
% \end{table}


\subsection{Dialogue Synthesis}
\label{sec:result_dia}
\paragraph{Synthesis Quality}
% Formatting and Summarization score
% Efficiency (iterative generation)

We evaluated the performance of different dialogue synthesis methods, including direct synthesis (One-shot), iterative dialogue synthesis (Iterative), and various configurations of DPO, as presented in Table~\ref{tab:DPO abl}. In our DPO training, we constructed different sets of preference pairs based on the definitions provided earlier. For the single joint preference set, we used dialogues from the post-processing step as the preferred data (\( \hat{d}_1 \)) and the raw dialogues without iterative synthesis as the rejected data (\( \hat{d}_2 \)), forming preference pairs:
\(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{F}(\hat{d}_1) = 1, \mathcal{F}(\hat{d}_2) = 0, \mathcal{M}_{\text{sum}}(s \mid \hat{d}_1) > \mathcal{M}_{\text{sum}}(s \mid \hat{d}_2)\}\). 
For the separated preference sets, we included both formatting preference pairs:
\(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{F}(\hat{d}_1) = 1, \mathcal{F}(\hat{d}_2) = 0\}\)
and summarization preference pairs:
\(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{M}_{\text{sum}}(s \mid \hat{d}_1) > \mathcal{M}_{\text{sum}}(s \mid \hat{d}_2)\}\). 
We also incorporated supervised fine-tuning data into the DPO training, indicated as "+SFT."

From Table~\ref{tab:DPO abl}, the SFT model's one-shot synthesis had low format correctness (24\%) and a high summarization cross-entropy (CE) loss of 4.74. While Iterative synthesis ensured 100\% format correctness, the CE loss remained similar at 4.69, showing little improvement in content alignment.
Applying DPO with the joint preference set improves format correctness to 78\% and reduces the summarization CE loss to 4.62. However, training with DPO loss alone was unstable, so adding SFT loss ({Joint Preference Set + SFT}), stabilized training, achieving 96\% format correctness and a CE loss of 4.45. Yet, this approach prioritized formatting over content quality, limiting further improvement in CE loss.

By separating the preference sets and including both formatting and summarization preferences in DPO training ({Separate Preference Sets + SFT}), our MRDS approach effectively balanced format correctness (92\%) and significantly lowered the CE loss to 4.13, indicating better alignment with summaries. This demonstrates that separating preference sets allows concurrent optimization of formatting and content, leading to the best overall performance.



% For dialogue synthesis, we compare the performance of direct synthesis (One-shot), iterative dialogue synthesis (Iterative), and various settings of DPO, as shown in Table~\ref{tab:DPO abl}. 
% In our DPO training, we construct different sets of preference pairs based on the definitions provided earlier. 
% For the single joint preference set, we use dialogues from the post-processing step as the preferred data (\( \hat{d}_1 \)) and the raw dialogues without iterative synthesis as the rejected data (\( \hat{d}_2 \)). 
% The preference pairs are formed as \(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{F}(\hat{d}_1) = 1, \mathcal{F}(\hat{d}_2) = 0, \mathcal{M}_{\text{sum}}(s \mid \hat{d}_1) > \mathcal{M}_{\text{sum}}(s \mid \hat{d}_2)\}\). 
% For the separated preference sets, we include both formatting preference pairs \(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{F}(\hat{d}_1) = 1, \mathcal{F}(\hat{d}_2) = 0\}\) and summarization preference pairs \(\{s, \hat{d}_1, \hat{d}_2 \mid s \in \mathcal{S}, \mathcal{M}_{\text{sum}}(s \mid \hat{d}_1) > \mathcal{M}_{\text{sum}}(s \mid \hat{d}_2)\}\). 
% We also incorporate supervised fine-tuning data into the DPO training, indicated as "+SFT".

% First, from the SFT model, we found the one-shot synthesis results are with low format correctness (24\%), and with the highest summarization score. On the other hand, after applying IDS, ensuring 100\% format correctness, but with similar summarization score. For the DPO based model, we first conduct the experiments with the DPO loss only with the joint preference set. However, we found that only by applying DPO loss without SFT loss makes the training become corrupted soon and generate corruption output, so the results are suboptimal. After adding the SFT loss, the training become more stable and be able to learn to generate data with high quality format and better summarization score (4.45 vs. 4.74). However, the joint preference set tends to train the data synthesis model to learn better formatting instead of the summarization score. Therefore, in the last experiments, we found that by separating the two preference sets, the model can better improve both the format and summarization scores, leading to the best summarization score (4.13), and with 92\% format correctness.


\paragraph{Synthesis Efficiency}
We compared the efficiency of the post-processing approach—which includes iterative synthesis and content alignment filtering—with the DPO-based MRDS approach (Table~\ref{tab:eff}). In the 100-shot scenarios, although MRDS still required iterative synthesis due to less consistent format correctness with limited data, it achieved 550 dialogues per hour on SAMSum compared to 63 dialogues per hour with post-processing—an 8.7-fold improvement. In the 300-shot scenarios, MRDS significantly increased throughput, generating 2,900 dialogues per hour on SAMSum versus 37.5 dialogues per hour with post-processing—a 77-fold improvement—by eliminating the need for IDS or content alignment filtering due to enhanced format correctness and summarization alignment.


% To evaluate the efficiency of our final dialogue generation, we compare the post-processing method—which includes iterative dialogue generation and filtering—with the DPO-based MRDS approach in Table~\ref{tab:eff}.
% Since in Table~\ref{tab:DPO abl}, the proposed MRDS synthesis approach largely improves the format correctness and the summarization scores of the data synthesis model, the MRDS approach does not require summarization filtering in 300 shots scenarios. Therefore, the throughput of the 300 shots model improves 37 to 77 times (2900 vs. 37.5 shots). For 100 shots, we still apply IDS without filtering because it does not consistently produce correctly formatted dialogues with limited amounts of real data. Therefore, the throughput improvement are in 3.7 to 8.7 times (550 vs. 63).



% In the case of the DPO method, since the 100-shot DPO models do not consistently produce correctly formatted dialogues, we continue to use iterative generation without filtering for the 100-shot experiments. For the 300-shot experiments, we generate the dialogues directly using the DPO models without any post-processing.

% Since in Table~\ref{tab:DPO abl}, the proposed MRDS synthesis approach largely improves the format correctness and the summarization scores of the data synthesis model, the MRDS approach does not need to 




\subsection{Model Analysis}

% \paragraph{Learning Rate and LoRA config.}
% % Only the baseline model
% We optimize the learning rate and Lora configuration on the real-only baselines. The experiment settings for synthesis experiments follow the parameters with real-only experiments to ensure a fair comparison. As shown in Table~\ref{}


% On synthesis/unseen summary experiments, no need to show the same summary.
% 5:5, 3:7, dynamic, domain shifting
% SAMSum 100 : 269biwf6yy, 9cnyd3s673, 9zgpaugiun
% SAMSum 100 : g982hbcf53 (Synthesis 3:7)
% % DialogSum: hg8j65tw42, s3bujcrfcv, rgz4wtufcr
% \begin{table}[ht]
% \centering
% \caption{Comparison of different training strategies with synthesis data.}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Approach}  & \multicolumn{4}{c}{SAMSum 100 shots}  \\
% \cmidrule(lr){2-5} 
% & R-1 & R-2 & R-L & BERTScore  \\
% \midrule
% Real only & 50.9 & 26.5 & 42.6 & 86.6  \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% r:s = 5:5 & 51.2 & 26.6 & 42.5 &  \\
% r:s = 3:7 & 51.8 & 27.1 & 43.3 &  \\
% Dynamic Ratio & 51.9 & 27.1 & 43.1 &  \\
% Domain Shifting &  52.5 & 28.1 & 44.3 & 87.0   \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% r:s = 3:7 & 49.8 & 24.7 & 41.1 &   \\
% Domain Shifting & 51.8 & 27.3 & 43.5 & 86.7   \\
% \bottomrule
% \end{tabular}
% \end{table}

% Unseen SAMSum 300 shots
% Fixed (5:5): 7cpih3xwek
% Dynamic: 7sgp5canvc



% \begin{table}[ht]
% \centering
% \caption{Comparison of different training strategies with synthesis data.}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Approach}  & \multicolumn{4}{c}{SAMSum 300 shots}  \\
% \cmidrule(lr){2-5} 
% & R-1 & R-2 & R-L & BERTScore  \\
% \midrule
% Real only & 51.1 & 26.9 & 42.8 & 86.5  \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% Fixed Ratio &    \\
% Domain Shifting & 52.7 & 28.1 & 44.1 & 87.0  \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% Fixed Ratio & 52.4 & 28.1 & 44.0 & \\
% Dynamic Ratio & 52.6 & 27.6 & 43.8 &  \\
% Domain Shifting &  53.0 & 28.3 & 44.5 & 87.1   \\
% % \midrule
% \bottomrule
% \end{tabular}
% \end{table}


% Synthesis Summ DialogSum 300 shots
% Fixed: ewgejd2gq8

% Unseen DialogSum 300 shots
% Fixed (5:5): tb5zwm4h97
% Dynamic: 82fadmw7i8




% Same Summ DialogSum 300 shots
% Fixed ratio: tad45wxc93

\begin{table*}[ht]
\centering
\caption{Comparison of different summaries with synthesis data on DialogSum 100 and 300 real shots. Synthesis dialogues are generated with iterative dialogue synthesis and content alignment filtering.}
\label{tab:summaries}
\begin{tabular}{lcccc|cccc}
\toprule
\multirow{2}{*}{Approach} & \multicolumn{4}{c}{100 Real shots} & \multicolumn{4}{c}{300 Real shots} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
\midrule
Real only & 44.0 & 18.2 & 36.0 & 86.8 & 45.2 & 19.5 & 37.3 & 87.2 \\
\midrule
\multicolumn{5}{l}{\textbf{Fixed Ratio Training}} \\
\midrule
Same Summ & 42.8 & 17.4 & 35.3 & 86.4 & 44.6 & 19.0 & 36.7 & 86.7 \\
Synthetic Summ & 44.1 & 18.2 & 36.1 & 86.7 & 44.0 & 18.2 & 35.5 &  86.6 \\
Unseen Summ & 44.5 & 18.8 & 36.3 & 87.0 & 46.6 & {20.5} & 38.1 & {87.5} \\
\midrule
\multicolumn{5}{l}{\textbf{Two-Stage Training}} \\
\midrule
Same Summ & 42.5 & 16.8 & 34.6 & 86.3 & 45.8 & 19.9 & 37.6 & 87.2 \\
Synthetic Summ & 44.7 & 18.8 & 36.5 & 87.1 & {46.1} & {20.0} & {37.6} & {87.3}   \\
Unseen Summ &  45.3 & 19.7 & 37.4 & 87.0 &  {46.8} & {20.8} & {38.8}  & {87.6}  \\
\bottomrule
\end{tabular}
\end{table*}


% \begin{table}[ht]
% \centering
% \caption{Comparison of different summaries with synthesis data on DialogSum 300 shots. Synthesis dialogues are generated with iterative dialogue generation and summarization score filtering.}
% \label{tab:strategy}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Approach}  & \multicolumn{4}{c}{DialogSum 300 shots}  \\
% \cmidrule(lr){2-5} 
% & R-1 & R-2 & R-L & B-S  \\
% \midrule
% Real only & 45.2 & 19.5 & 37.3 & 87.2  \\
% \midrule
% \multicolumn{5}{l}{\textbf{Fixed Ratio}} \\
% \midrule
% Same Summ & 44.6 & 19.0 & 36.7 & 86.7 \\
% Synthetic Summ & 44.0 & 18.2 & 35.5 &  86.6 \\
% Unseen Summ & 46.6 & {20.5} & 38.1 & {87.5} \\
% \midrule
% \multicolumn{5}{l}{\textbf{Domain Shifting}} \\
% \midrule
% Same Summ &  45.8 & 19.9 & 37.6 & 87.2 \\
% Synthetic Summ & {46.1} & {20.0} & {37.6} & {87.3}   \\
% Unseen Summ &  {46.8} & {20.8} & {38.8}  & {87.6}  \\
% % \midrule
% \bottomrule
% \end{tabular}
% \end{table}



% DialogSum 100 shots, Fixed ratio: 
% Unseen Summ sawqy9qra8



% \begin{table}[ht]
% \centering
% \caption{Comparison of different summaries with synthesis data on DialogSum 100 shots. Synthesis dialogues are generated with iterative dialogue generation and summarization score filtering.}
% \label{tab:strategy}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Approach}  & \multicolumn{4}{c}{DialogSum 100 shots}  \\
% \cmidrule(lr){2-5} 
% & R-1 & R-2 & R-L & B-S  \\
% \midrule
% Real only & 44.0 & 18.2 & 36.0 & 86.8 \\
% \midrule
% \multicolumn{5}{l}{\textbf{Fixed Ratio}} \\
% \midrule
% Same Summ & 42.8 & 17.4 & 35.3 & 86.4 \\
% Synthetic Summ & 44.1 & 18.2 & 36.1 & 86.7  \\
% Unseen Summ & 44.5 & 18.8 & 36.3 & 87.0 \\
% \midrule
% \multicolumn{5}{l}{\textbf{Domain Shifting}} \\
% \midrule
% Same Summ & 42.5 & 16.8 & 34.6 & 86.3   \\
% Synthetic Summ & 44.7 & 18.8 & 36.5 & 87.1  \\
% Unseen Summ &  45.3 & 19.7 & 37.4 & 87.0   \\
% % \midrule
% \bottomrule
% \end{tabular}
% \end{table}



 
 
% \begin{table}[ht]
% \centering
% \caption{Comparison of different summaries with synthesis data on SAMSum 100 shots. Synthesis dialogues are generated with iterative dialogue generation and summarization score filtering.}
% \label{tab:strategy}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Approach}  & \multicolumn{4}{c}{SAMSum 100 shots}  \\
% \cmidrule(lr){2-5} 
% & R-1 & R-2 & R-L & B-S  \\
% \midrule
% Real only & 50.9 & 26.5 & 42.6 & 86.6  \\
% \midrule
% \multicolumn{5}{l}{\textbf{Fixed Ratio}} \\
% \midrule
% Same Summ &   \\
% Synthetic Summ &  \\
% Unseen Summ &  51.8 & 27.1 & 43.3 & \\
% \midrule
% \multicolumn{5}{l}{\textbf{Domain Shifting}} \\
% \midrule
% Same Summ &    \\
% Synthetic Summ & 51.8 & 27.3 & 43.5 & 86.7   \\
% Unseen Summ &  52.5 & 28.1 & 44.3 & 87.0 \\
% % \midrule
% \bottomrule
% \end{tabular}
% \end{table}




% \begin{table}[ht]
% \centering
% \caption{Comparison of training strategies with synthesis data on DialogSum 300 shots. Synthesis dialogues are generated with iterative dialogue generation and summarization score filtering.}
% \label{tab:strategy}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Approach}  & \multicolumn{4}{c}{DialogSum 300 shots}  \\
% \cmidrule(lr){2-5} 
% & R-1 & R-2 & R-L & B-S  \\
% \midrule
% Real only & 45.2 & 19.5 & 37.3 & 87.2  \\
% \midrule
% \multicolumn{5}{l}{\textbf{Synthetic Summaries}} \\
% \midrule
% Fixed Ratio & 44.0 & 18.2 & 35.5 &  86.6 \\
% Domain Shifting & \textbf{46.1} & \textbf{20.0} & \textbf{37.6} & \textbf{87.3}   \\
% \midrule
% \multicolumn{5}{l}{\textbf{Unseen Summaries}} \\
% \midrule
% Fixed Ratio & 46.6 & \underline{20.5} & 38.1 & \underline{87.5} \\
% Dynamic Ratio & \textbf{46.9} & \underline{20.5} & \underline{38.4} & \underline{87.5} \\
% Domain Shifting &  \underline{46.8} & \textbf{20.8} & \textbf{38.8}  & \textbf{87.6}  \\
% % \midrule
% \bottomrule
% \end{tabular}
% \end{table}





% \begin{table*}[ht]
% \centering
% \caption{Comparison of Summarization Methods on 100 shots experiments.}
% \begin{tabular}{lcccc|cccc}
% \toprule
% \multirow{2}{*}{Approach} & \multicolumn{4}{c}{SAMSum} & \multicolumn{4}{c}{DialogSum} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9}
% & R-1 & R-2 & R-L & BERTScore & R-1 & R-2 & R-L & BERTScore \\
% \midrule
% Real only & 50.9 & 26.5 & 42.6 & 86.6 & 44.0 & 18.2 & 36.0 & 86.8  \\
% \midrule
% \textbf{Synthetic Summaries} \\
% \midrule
% r:s = 5:5 &   \\
% r:s = 3:7 &   \\
% Domain Shifting & 51.8 & 27.3 & 43.5 & 86.7 & 44.7 & 18.8 & 36.5 & 87.1  \\
% \midrule
% \textbf{Unseen Summaries} \\
% \midrule
% r:s = 5:5 & 51.2 & 26.6 & 42.5 & & 44.5 & 18.9 & 36.4 & \\
% r:s = 3:7 & 51.8 & 27.1 & 43.3 & & 45.0 & 19.2 & 36.8 & \\
% Dynamic Ratio & 51.9 & 27.1 & 43.1 & & 45.9 & 19.7 & 37.4 & \\
% Domain Shifting &  52.5 & 28.1 & 44.3 & 87.0 & 45.3 & 19.7 & 37.4 & 87.0  \\
% \bottomrule
% \end{tabular}
% \end{table*}



\paragraph{Summaries Effectiveness.}
To evaluate the impact of different types of summaries on dialogue synthesis, we experimented with two training strategies: (1) a fixed ratio of real to synthesized data (1:1), and (2) a two-stage training approach, where the model is first trained on synthesized data before switching entirely to real data. We tested these strategies using three types of summaries: the same summaries (identical to those in the real data), synthetic summaries generated from our synthesis process in Sec.~\ref{sec:summ_train}, and unseen summaries drawn from additional training data.

Table~\ref{tab:summaries} presents the results. Under the fixed ratio strategy, using the same summaries did not yield any improvement over the real-only approach, likely due to overfitting to the limited real summaries. Synthetic summaries provided some improvement in the 100-shot setting but introduced artifacts that degraded performance in the 300-shot setting. This suggests that mixing synthetic and real data in a fixed ratio can lead to the model learning undesirable patterns from the synthetic data as the amount of real data increases.
% we observed that using the same summaries does not yield any improvement over the real-only apporach, likely due to overfitting to the limited real summaries. Synthetic summaries boosted performance in the 100-shot setting but introduced artifacts that degraded performance in the 300-shot setting, indicating that the fixed ratio strategy amplifies the learning of undesirable patterns from synthetic data as training progresses.

% Notably, the unseen summaries performed well in both training strategies. When using unseen summaries, both the fixed ratio and two-stage training approaches showed improvements over the real-only approach. This suggests that incorporating additional real summaries—which are not part of the initial training set and do not paired with dialouges—can provide substantial benefits. The consistent performance of unseen summaries across both training methods demonstrates the value of extra real information in enhancing the model's generalization capabilities.

% In contrast, the two-stage training strategy further highlighted the drawbacks of using the same summaries, as performance was actually worse in the 100-shot setting due to overfitting. However, when using synthetic summaries in the two-stage training, the model showed improved results, as it was better able to adapt to real data after initially training on synthesized data. Overall, the two-stage training approach proved more effective when using synthetic or unseen summaries, as it minimized the risk of overfitting and avoided the artifacts introduced by synthetic summaries in the fixed ratio approach, resulting in better generalization across different datasets and training shots.

In contrast, the two-stage training approach produced better results. Training with synthetic summaries in the first stage allowed the model to learn general patterns from the synthesized data before fine-tuning on real data, which improved performance in both the 100-shot and 300-shot settings. Specifically, using synthetic summaries in two-stage training significantly outperformed using the same summaries, mitigating overfitting and enhancing generalization. This highlights the value of synthetic summaries within the more effective two-stage training framework.
Additionally, incorporating unseen summaries led to improvements in both training strategies, but the two-stage training still provided superior results. 
% This demonstrates that while extra real summaries are beneficial, synthetic summaries can be equally valuable when used within an effective training framework like two-stage training.





% For unseen summaries, it brings consistent improvements to the baseline. 


% The results reveal that using the same summaries in the two-stage training approach tends to overfit to the real summaries, as indicated by a lack of significant improvement in the metrics, particularly in the 100-shot setting. This overfitting likely occurs because the model becomes overly reliant on the real data in the second stage, limiting the generalization potential of the model when faced with unseen examples.

% In contrast, synthetic summaries in the fixed ratio approach introduce artifacts into the training process, which degrade performance by causing the model to learn undesirable patterns from the synthesized data. This is evident from the lower R-1 and BERT scores compared to the two-stage training strategy. However, when using unseen summaries, the two-stage training proves more effective, as the model benefits from the diversity of the synthetic data without inheriting artifacts. The results show that our proposed MRDS method consistently outperforms the fixed ratio approach by successfully leveraging the synthetic summaries without overfitting or learning problematic artifacts, achieving the best overall performance across both datasets and shot settings.


% Table~\ref{tab:summary} shows the performance across these strategies. The fixed ratio method performed inconsistently, with synthesized summaries degrading performance due to artifacts. In contrast, synthesized dialogues improved results, suggesting that the model is less sensitive to dialogue artifacts. The dynamic ratio approach showed some improvement with unseen summaries but led to overfitting before fully incorporating real data, failing to resolve artifact issues.

% Our domain-shifting strategy effectively mitigates these problems. By training initially on synthesized data and then switching to real data, the model learns general summarization skills before adapting to the target domain, avoiding artifacts. This method leverages pre-training benefits while maintaining the strengths of real data training.
% The superiority of the domain-shifting strategy is further validated through human evaluations (see Section~\ref{sec:he}), demonstrating improved summarization quality compared to other methods. The final model benefits from broad learning and domain-specific refinement, surpassing models trained solely on real data or relying on zero-shot LLM capabilities.


\paragraph{Dialogue Generation and Filtering.}
% The last ablation studies on the unseen summary.

\begin{table}[ht]
\centering
\caption{Ablation study for dialogue generation, filtering, and DPO on unseen summaries. The training is conducted in two stages training.}
\label{tab:filter}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Approach}  & \multicolumn{4}{c}{DialogSum 300 shots}  \\
\cmidrule(lr){2-5} 
& R-1 & R-2 & R-L & B-S   \\
\midrule
Real only & 45.2 & 19.5 & 37.3 & 87.2  \\
\midrule
\multicolumn{5}{l}{\textbf{Unseen Summaries}} \\
\midrule
w/o dialogue  & 45.8 & 19.9 & 37.3 & 87.2 \\
w/o filtering & 46.3 & 20.1 & 37.9 & 87.3 \\
w/ filtering &  \underline{46.8} & \textbf{20.8} & \textbf{38.8}  & \textbf{87.6}  \\
\midrule
MRDS & \textbf{47.0} & \underline{20.7} & \underline{38.7} & \underline{87.5}  \\
\bottomrule
\end{tabular}
\end{table}

To evaluate the benefits of generating and filtering dialogues, we conducted experiments using unseen summaries with the two-stage training strategy. In the first stage, we tested three types of synthetic data: summaries only, summaries with unfiltered dialogues, and approaches involving content alignment filtering or DPO training (MRDS), as shown in Table~\ref{tab:filter}. We found that training the model with summaries only did improve upon the real-only approach; however, adding dialogues to the training data further enhanced the results. Finally, the models utilizing filtering or DPO outperformed all other methods, demonstrating that including dialogues with filtering and DPO effectively improves the final outcomes.



% \paragraph{Dialogue Summarization}

% \paragraph{Dialogue Generation}



% Appendix
% 1. Prompts
% 2. Dialogue Examples
% 3. Summary Examples