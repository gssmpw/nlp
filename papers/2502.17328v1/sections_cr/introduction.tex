
\begin{figure}[t]
    \centering
    %\includegraphics[width=1.0\columnwidth]{figures/self-aligned.pdf}
    \includegraphics[width=1.0\columnwidth]{figures/mrds.pdf}
    \caption{Mutual Reinforcement via Data Synthesis - We first leverage the summarization capability of the pretrained LLM for scoring dialogue preference pairs that are used to improve the dialogue synthesis capability with DPO, and then use the improved synthesis capability to generate SFT data for improving the summarization capability.}
    \label{fig:align}
    \vspace{-15pt}
\end{figure}

Dialogue summarization focuses on producing concise and coherent summaries of conversations in various domains such as customer service \citep{feigenblat-etal-2021-tweetsumm-dialog, zhao2021todsum}, medical consultations \citep{chintagunta-etal-2021-medically, jain2022survey, zeng-etal-2020-meddialog}, and casual interactions \citep{chen2021dialogsum,gliwa-etal-2019-samsum}. 
While state-of-the-art large language models (LLMs) such as Llama3 \citep{dubey2024llama} %and ChatGPT \citep{achiam2023gpt} 
have been shown to work well for a wide variety of natural language processing tasks, their dialogue summarization performance is unsatisfactory in many target domains of interest. This could be because, these models, despite being trained on massive datasets, may have seen limited data related to dialogue summarization in specific target domains. 

Collecting a large-scale, real-world dialogue dataset and manually annotating it with concise summaries is not only time-consuming and expensive, but it also raises privacy concerns in many target domains, as conversational data is often sensitive in nature. 
%Machine learning models trained on limited data often perform poorly to deploy in many real-word applications. 
Consequently, there is a growing interest in developing few-shot learning approaches that improve the dialogue summarization capabilities of pretrained LLMs using limited real dialogue-summary pairs from the target domain.

%Unlike document summarization, training dialogue summarization models faces unique challenges due to the interactive, turn-based nature of conversations, making it heavily reliant on manual annotation. This dependency not only makes the process time-consuming and expensive but also raises privacy concerns when handling personal dialogues. Scarcity of annotated data poses significant hurdles for training effective machine learning models for many real-world applications where data collection is challenging. Consequently, there is a growing interest in developing few-shot learning approaches that perform well with limited data.

Existing works that address data scarcity for dialogue summarization assume access to either additional dialogue-summary pairs from other domains \citep{li-etal-2023-dionysus,park2024gendex,yu-etal-2021-adaptsum,zou-etal-2021-low, zhong2022dialoglm} or additional unlabeled dialogues from the target domain \citep{chen2021simple,he-etal-2024-semi} or 
%models that are larger and more powerful than the target LLM 
teacher models that are larger and more powerful than the target models \citep{ouyang2023compositional,pham-etal-2023-select}. While these approaches have shown some performance improvements, they need access to external resources that may not be available in all scenarios. Different from these works, we focus on improving the dialogue summarization capability of a pretrained LLM using limited real dialogue-summary pairs from the target domain without relying on any additional data sources or external models.

%Previous methods have attempted to address data scarcity for dialogue summarization by leveraging external knowledge, unannotated data, or pre-trained models to augment training data or incorporate additional information \citep{park2024gendex, ouyang2023compositional, tian2024dialogue, gunasekara-etal-2021-summary}. \textbf{ravi: Can we add references specific to each thing here (external knowledge, unannotated data, etc.) instead of a common list at the end of the sentence.)}
%Techniques such as synthetic data generation using unlabeled datasets and integrating knowledge from external models have shown performance improvements. 
%However, these approaches often rely on resources beyond the initial dataset, which may not be accessible or feasible in all scenariosâ€”particularly in few-shot settings where only minimal data is available.
% \textbf{ravi: Overall, this paragraph is vague. we need to a bit more specific about what the previous methods are doing.}

% 3. Motivation, data synthesis capabilities and internal knowledge 
LLMs possess a vast amount of implicit knowledge acquired during pre-training on large-scale text corpora, enabling them to generate contextually relevant text. 
Motivated by this, we introduce the framework of Mutual Reinforcement via Data Synthesis (MRDS) that harnesses the internal knowledge embedded within these models and their inherent data synthesis capability to address the data scarcity problem in dialogue summarization. Specifically, given a pretrained LLM, our method incorporates dialogue synthesis and summarization into a mutually reinforcing cycle to enhance both capabilities simultaneously (see Fig.~\ref{fig:align}). 

To improve the dialogue synthesis capability, we create a dataset of synthetic dialogue preference pairs scored by leveraging the summarization capability of the pretrained LLM, and train a LoRA \citep{hu2022lora} adapter for dialogue synthesis using direct preference optimization (DPO) with the synthetic dialogue preference pairs in addition to supervised finetuning (SFT) with the limited real dialogue-summary pairs. This ensures that the generated dialogues are coherent and closely aligned with their corresponding summaries. To improve the summarization capability, we utilize the trained dialogue synthesis adapter to generate synthetic dialogue-summary pairs, and train a LoRA adapter for summarization by performing SFT with limited real data and generated synthetic data. Effectively, the initial summarization capability of the pretrained LLM helps improve the dialogue synthesis capability which in turn helps further improve the summarization capability.

%Specifically, given a pretrained LLM, our method uses two LoRA adapters \citep{hu2022lora}, one for dialogue synthesis and one for dialogue summarization, and incorporates them into a mutually reinforcing cycle to enhance both summarization and synthesis capabilities simultaneously. To improve the dialogue synthesis capability, we create a dataset of synthetic preference pairs scored by the summarization adapter and conduct direct preference optimization (DPO). This ensures that the generated dialogues are coherent and closely aligned with their corresponding summaries. To improve the summarization capability, we use the dialogue synthesis adapter to generate synthetic data for supervised finetuning (SFT) of the summarization adapter.

% Motivated by the inherent data synthesis capabilities of LLMs, we explore leveraging the internal knowledge embedded within these models to address the data scarcity problem in dialogue summarization. LLMs possess a vast amount of implicit knowledge acquired during pre-training on large-scale text corpora, enabling them to generate contextually relevant text. By harnessing this internal knowledge, we mutually reinforce an LLM's dialogue synthesis and summarization capabilities via synthetic data.


%%% 4. Proposed approach: Mutual reinforcing and DPO, iterative dialogue generation, summarization score filtering.
%In this work, we introduces the concept of Mutual Reinforcing Data Synthesis (\textbf{MRDS}) within LLMs to tackle dialogue summarization in few-shot scenarios without relying on any additional knowledge or external data. 
%Given a pretrained LLM, our method incorporate dialogue generation and summarization in a mutually reinforcing cycle to enhance both capabilities simultaneously. To improve dialogue synthesis capability, we employ summarization capability for preference scoring and conduct direct preference optimization (DPO). It ensures that the generated dialogues are well-formatted, coherent, and closely aligned with their corresponding summaries. To improve summarization capability, we utilize dialogue synthesis capability to prepare more synthetic training data.
%As a result, both capabilities are enhanced based on the knowledge inherent in the pretrained LLM.

%Our method integrates dialogue generation and summarization in a mutually reinforcing cycle to enhance both capabilities simultaneously through cycle consistency.
% Our method mutual
%We employ iterative dialogue generation refined by summarization loss through direct preference optimization (DPO), ensuring that the generated dialogues are well-formatted, coherent, and closely aligned with their corresponding summaries as shown in Fig.~\ref{fig:align}. 
%This process not only improves dialogue generation but also provides additional training data for the summarization model, enhancing its generalization abilities while utilizing only the data and knowledge inherent in the model itself.


% 5. Main contribution: cycle consistency, summarization, dialogue writing.
\paragraph{Major contributions:}
\begin{itemize}
  %\item \textbf{Mutual Reinforcement Mechanism:} We develop a mutual reinforcement mechanism within a unified LLM that integrates dialogue generation and summarization, achieving cycle consistency to enhance both tasks without relying on external knowledge or large-scale datasets.
  \item \textbf{Mutual reinforcement framework:} We introduce a framework that incorporates dialogue synthesis and summarization capabilities of an LLM into a mutually reinforcing cycle, and enhances both capabilities using limited target domain data.
  %\item \textbf{DPO-Enhanced Synthetic Data:} We employ direct preference optimization to refine synthetic data, ensuring the generated dialogues are correctly formatted, coherent, and closely aligned with their summaries, thereby enhancing the quality of dialogue generation.
  \item \textbf{DPO-Enhanced dialogue synthesis:} We propose to use direct preference optimization to improve the dialogue synthesis capability of a pretrained LLM, ensuring that the generated dialogues are correctly formatted, coherent, and closely aligned with their summaries.
  \item \textbf{Experimental validation:} We demonstrate consistent improvements over various alternative approaches in terms of BERT and ROUGE scores on two widely-used benchmark datasets, namely SAMSum and DialogSum. The proposed approach also achieves the best average score in human evaluations. We also present several ablation results that demonstrate the effectiveness of individual components of the proposed approach.
  %in enhancing the model's ability to generate accurate and concise summaries.
  % Average Human Evaluation scores
\end{itemize}


  % \item \textbf{Domain Shifting Training:} We design a novel training approach to include synthesis training data for the summarization model without brining artifacts into the final summarization model.
% Experiments demonstrate that our method effectively addresses data scarcity and outperforms existing techniques in dialogue summarization tasks. 

%By eliminating dependence on external resources and additional data, our approach offers a practical solution for real-world applications where data collection is challenging and resources are limited.


% This approach demonstrates the potential of unified LLMs to streamline both generation and summarization processes in few-shot settings.




