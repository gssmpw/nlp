% 1. We propose [What] for [What Problem]
% 2. Key [Innovation/Motivation/Observation] making our paper special
% 3. 2 or 3 Striking Features of Our Work
% 4. Supplementing Details


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\columnwidth]{figures/self-aligned.pdf}
%     \caption{Self-alignment in LLMs: We leverage the model's dialogue generation capabilities to create synthetic dialogues, then apply summarization loss to perform preference fine-tuning for improving dialogue writing.}
%     \label{fig:align}
% \end{figure}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figures/DPO.pdf}
%     \caption{Post-processing and DPO in the proposed system: We utilize iterative dialogue generation to maintain the correct dialogue format, while cross-entropy filtering is applied to enhance dialogue quality. The pairs $(y_w^f,y_l^f)$ and $(y_w^s,y_l^s)$ represent format and summarization preferences, respectively, used for DPO training of the dialogue generation model.}
%     \label{fig:dpo}
% \end{figure*}


\begin{figure}[t]
    \centering
    %\includegraphics[width=1.0\columnwidth]{figures/DPO_new.pdf}
    \includegraphics[width=1.0\columnwidth]{figures/synthesis_dpo_2.pdf}
    \caption{
    Preference pairs for DPO - Given a summary set \( \mathcal{S} \), the LLM generates synthetic dialogues which are evaluated based on content alignment and format correctness. Content-based preferences leverage the LLM's own summarization capability to assess how well the synthesized dialogues align with the input summaries. Format-based preferences ensure that the dialogues follow proper formatting.
    }
    \label{fig:dpo}
\end{figure}



% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figures/DPO2.pdf}
%     \caption{Self-alignment between dialogue generation and summarization. The generated dialogues are utilized to train the summarization model, while the cross-entropy loss is leveraged to create preference data for DPO training of the dialogue generation model.}
%     \label{fig:dpo}
% \end{figure*}

\paragraph{Notations} Let $\mathcal{D}_p$ denote the limited real dataset of dialogue-summary pairs, and $\mathcal{S}$ denote the set of summaries in $\mathcal{D}_p$.

%We evolve the given LLM (Llama3-8B-Instruct~\citep{dubey2024llama} in our experiments) into two LLMs, one focusing on dialogue summarization and the other focusing on dialogue synthesis using the LoRA approach of~\citep{hu2022lora}, and improve each of them by leveraging the other.
%The summarizer is trained with synthetic data generated by the dialogue synthesis model, and the dialogue synthesizer is trained using DPO~\citep{rafailov2024direct} with synthetic preference pairs scored by the summarization model. 
%The two models help each other get better at their respective tasks without using any external teacher models or additional real dialogue-summary pairs.
Given a pretrained LLM, we characterize its dialogue synthesis and summarization capabilities using two separate LoRA \citep{hu2022lora} adapters. 
%, and improve each of these capabilities by leveraging the other. 
First, we leverage the summarization capability of the pretrained LLM to score synthetic dialogue preference pairs, and use this data to train the dialogue synthesis adapter using DPO in addition to SFT on real data $\mathcal{D}_p$. Then, we leverage the trained dialogue synthesis adapter to generate synthetic dialogue-summary pairs that are used (in addition to real data $\mathcal{D}_p$) to train the summarization adapter.
%The summarization model (pretrained LLM with summarization adapter) is trained with synthetic data generated by the dialogue synthesis model (pretrained LLM with dialogue synthesis adapter), and the dialogue synthesis model is trained using DPO~\citep{rafailov2024direct} with synthetic preference pairs scored by the summarization model.
Effectively, the synthesis and summarization capabilities help improve each other without using any external models or additional real data.

%change keep all weights of the pretrained LLM frozen when training the adapters.



%First, we start with a summarization LLM (Llama3-8B-Instruct~\citep{dubey2024llama} in our experiments) and evolve it into a dialogue synthesizer by training a low-rank adapter using DPO with synthetic preference pairs scored by the summarization model (in addition to SFT with few-shot real data $\mathcal{D}_p$). Then, we use this dialogue synthesizer to generate synthetic data that is used to re-train the summarization model (along with few-shot real dataset $\mathcal{D}_p$). This way, the summarization model improves itself without using any external teacher models or additional real dialogue-summary pairs.
 
\subsection{Dialogue Synthesizer Training}
\label{sec:dialogue}
The dialogue synthesizer (pretrained LLM with dialogue synthesis adapter) takes a summary $s$ as input and generates a dialogue $\hat{d}$. 
A straightforward way to train this model is to perform SFT with the real dialogue-summary pairs $\mathcal{D}_p$.
%%% explanation of format
%First, we standardize the format of the dialogues in $\mathcal{D}_p$
%by replacing the speaker name in each dialogue turn with a hash followed by the speaker index (e.g. Alan $\to$ \#1, James $\to$ \#2). 
%Then, we perform supervised finetuning (SFT) with the re-formatted few shot data. 
However, when the amount of such training data is limited, the resulting dialogue synthesizer $\mathcal{M}_{dlg}^{SFT}$ often generates poor quality dialogues in terms of both dialogue format and content (see Table~\ref{tab:synthesis_sft_sample}).
%%\textbf{ravi: Make a separate table with just two bad dialogues}


To encourage the dialogue synthesizer to generate higher quality dialogues, we construct a synthetic dataset of preference pairs $\{s, \hat{d}_1, \hat{d}_2\ |\ s \in \mathcal{S}, \mathcal{P}(\hat{d}_1) >\  \mathcal{P}(\hat{d}_2)\}$, where $\mathcal{P}$ denotes a dialogue quality scoring function, and train the synthesizer using DPO. Specifically, we generate two types of preference pairs using two different quality scoring functions, one focusing on the dialogue format and the other focusing on the dialogue content (see Fig.~\ref{fig:dpo}). Both the preferred and rejected dialogues in these pairs are synthetic dialogues generated using the SFT-trained dialogue synthesizer $\mathcal{M}_{dlg}^{SFT}$. 

%, i.e., $\hat{d}_1, \hat{d}_2 \sim \mathcal{M}_{dlg}^{SFT}(d|s)$.
 %using the process outlined in Fig.~\ref{fig:iterative}.
 
\paragraph{Format-based Preference Pairs}
Since the SFT-trained synthesizer $\mathcal{M}_{dlg}^{SFT}$ often generates dialogues with several formatting errors, we develop an iterative dialogue synthesis (IDS) method that generates correctly-formatted dialogues.
%%\textbf{ravi: The notion of first partial response in Table. 9 is not explained anywhere. Shoudl we create a figure explaining IDS instead of a table.} 
First, we generate a dialogue conditioned on the input summary and check it for format errors. We discard the portion of the dialogue after the first detected error,
%%\textbf{ravi: we need to explain what exactly we mean by format. The concept is not explained anywhere.}
concatenate the remaining correctly-formatted partial dialogue to the initial input prompt and give it as input to the dialogue synthesizer to complete the dialogue. This process is repeated until we get a dialogue without formatting errors. Table~\ref{tab:iter} shows an example run of this IDS process. 

For a given summary, we generate multiple dialogues with format errors by directly sampling from the SFT-trained synthesizer $\mathcal{M}_{dlg}^{SFT}$ and multiple clean dialogues by repeating the above IDS process several times. We use these samples to form preference pairs
$\{s, \hat{d}_1, \hat{d}_2\ |\ s \in \mathcal{S},  
 \mathcal{F}(\hat{d}_1) = 1,\   \mathcal{F}(\hat{d}_2) = 0\}$
, where $\mathcal{F}$ denotes the format check-based binary scoring function (1 for clean dialogues, 0 for those with errors).
%At the end of this IDS process, we get a clean dialogue and several intermediate dialogues with formatting errors. We use this data to form preference pairs 
%$\{s, \hat{d}_1, \hat{d}_2\ |\ s \in \mathcal{S},  
% \mathcal{F}(\hat{d}_1) = 1,\   \mathcal{F}(\hat{d}_2) = 0\}$
%, where $\mathcal{F}$ denotes the format check-based binary scoring function (1 for clean dialogues, 0 for those with errors). 
When trained with these preference pairs, the dialogue synthesizer learns to generate dialogues in correct format. Table~\ref{tab:preference_format_example} shows an example of format-based preference pair.%\textbf{ravi: put an example format-based pair in the appendix}
% \mathcal{F}(\hat{d}_1) = 1,\   \mathcal{F}(\hat{d}_2) = 0\}
% \mathcal{P}_F(\hat{d}_1) >\  \mathcal{P}_F(\hat{d}_2)\}$
% $\mathcal{P}_F(d) \in \{0,1\}$

% Talk about what exactly does the format check function does.

\paragraph{Content-based Preference Pairs}
A dialogue $\hat{d}$ generated by a well-trained dialogue synthesizer should have high content alignment with the summary $s$ used to generate $\hat{d}$. We leverage the summarization capability of the pretrained LLM to measure this alignment. The main motivation is that if we summarize the synthetic dialogue $\hat{d}$, the corresponding summary $s$ should have high probability as the summarization output. Based on this, we use the likelihood $\mathcal{M}_{sum}(s|\hat{d})$ of the summary $s$ conditioned on the dialogue $\hat{d}$ measured by the pretrained LLM as the alignment score.
%We leverage the summarization model to measure this alignment. The main motivation is that if we summarize the synthetic dialogue $\hat{d}$ using the summarization model, the corresponding summary $s$ should have high probability as the summarization model's output. Based on this, we use the likelihood $\mathcal{M}_{sum}(s|\hat{d})$ of the summary $s$ conditioned on the dialogue $\hat{d}$ measured by the summarization model $\mathcal{M}_{sum}$ as the alignment score. 

To encourage the dialogue synthesizer to generate dialogues that have high content alignment with the input summaries, we construct a dataset of preference pairs using the content alignment score $\mathcal{M}_{sum}(s|\hat{d})$. 
Specifically, for each summary $s$, we first generate multiple clean dialogues from the SFT-trained dialogue synthesizer $\mathcal{M}_{dlg}^{SFT}$ following the IDS process described above, and then pick the dialogues with the best and least content alignment scores to form preference pairs $\{s, \hat{d}_1, \hat{d}_2\ |\ s \in \mathcal{S},
\mathcal{M}_{sum}(s|\hat{d}_1) >\  \mathcal{M}_{sum}(s|\hat{d}_2)\}$. Table~\ref{tab:preference_content_example} shows an example of content-based preference pair.

Instead of using the pretrained LLM, we also explored training the summarization adapter on the limited real data $\mathcal{D}_p$ and using it for alignment scoring to generate DPO training data. However, we did not observe significant improvements in the final summarization results (after training the dialogue synthesizer with DPO, generating synthetic dialogues, and using them to train the final summarization adapter).
%\textbf{ravi: put an example content-based pair in the appendix}

% \mathcal{P}_C(\hat{d}_1) >\  \mathcal{P}_C(\hat{d}_2)\}$.
\paragraph{Training with DPO and SFT}
In addition to DPO with synthetic preference pairs, we also use SFT with the limited real data $\mathcal{D}_p$ to train the dialogue synthesizer. We accumulate gradients from both losses in each optimization step. While DPO with synthetic data encourages the model to generate contextually accurate dialogues in correct format, it does not explicitly encourage the model to generate dialogues that mimic the target distribution represented by real data $\mathcal{D}_p$. By combining DPO with SFT on real data, we encourage the model to generate dialogues closer to the target distribution while being contextually accurate and better-formatted.

\subsection{Summarizer Training}
\label{sec:summ_train}
The summarization model (pretrained LLM with summarization adapter) is trained using SFT with limited real data $\mathcal{D}_p$ and additional synthetic dialogue-summary pairs.

\paragraph{Synthetic Summary Generation}
To generate synthetic summaries that mimic the distribution represented by the real summary set $\mathcal{S}$, we first extract a (2-3 words) topic for each summary $s \in \mathcal{S}$ using the pretrained LLM. 
Then, for each topic, we generate multiple new synthetic summaries by using a topic-based summary synthesizer. 
This synthesizer is obtained by supervised LoRA finetuning of the pretrained LLM using the real summaries in $\mathcal{S}$ and the corresponding extracted topics. %Table~\ref{tab:prompt} shows the prompts used for topic extraction and summary synthesis.

\paragraph{Synthetic Dialogue Generation}
%For each synthetic summary, we generate a dialogue using the dialogue synthesizer that has been trained with both DPO and SFT, as explained in the previous section.
For each synthetic summary, we generate a dialogue by directly sampling from the dialogue synthesizer that has been trained with both DPO and SFT, as explained in the previous section. 
Since this improved synthesizer already generates dialogues with high quality, we do not use the costly iterative synthesis approach in this step.

\paragraph{Training with Synthetic and Real Data}
Models trained only on limited real data tend to overfit quickly. While combining real and synthetic data samples in each minibatch could address this issue to some extent, finding the perfect ratio between the two data sources is challenging and highly dependent on the quality of the synthetic data. Moreover, using a fixed ratio of real and synthetic data samples throughout the training may not be optimal. If the minibatch is dominated by synthetic data, then the model may inherit the artifacts present in the synthetic data, and if the minibatch is dominated by real data, the model may start to overfit before taking full advantage of the synthetic data. 

To address these issues, we follow a two-stage training strategy, where we train only on synthetic data in the first stage and only on real data in the second stage. The synthetic-only first stage allows the model to learn general dialogue summarization skills without the risk of quickly overfitting on limited real data. The real-only second stage allows the model to adapt to the distribution of the real data mitigating the artifacts learned from synthetic data. Following this two-stage approach, we make effective use of both synthetic and real data, resulting in a more accurate summarization model.

% \subsection{Summary Expansion}
% We perform dialogue synthesis experiments using three distinct types of summaries: same, synthesis, and unseen. For the "same" category, we utilize summaries from within the few-shot training set. For "unseen" summaries, we use additional summaries from the remaining training data that do not have corresponding dialogues. For "synthesis" summaries, we aim to ensure a distribution of summaries across various topics. To achieve this, we first extract topics from the existing summaries and then generate new summaries based on those topics. 


% For the real-only baseline, we train a LoRA adapter for summarization using only the real data. To integrate the synthesized data without introducing artifacts into the final summarization model, we develop a new training strategy called domain-shifting training.


%  To maintain the quality of the generated dialogues, we anonymize the dialogues by using uniform names (e.g., "$\#1$" or "$\#2$")  and restore the actual names after generation. 



% While the dialogue writing process described in Sec.~\ref{sec:dialogue} yields high-quality synthesized dialogue-summary pairs, it involves iterative generation and additional data preparation for filtering, leading to a slow generation process for creating effective dialogues. To address this, we implement preference fine-tuning to train the model to produce effective dialogues directly, reducing the need for post-processing. This approach integrates the post-processing steps into the dialogue writing model.


% We also experimenting with first finetuning Llama3-8B-Instruct for the summarization task using the few-shot data $\mathcal{D}_p$ and then using it as the alignment scoring model. This did not result in significant changes to the final results.