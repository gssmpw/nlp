\section{The CADE Dataset}\label{sec:corpus}
In this section we present the process that led to the creation of the Canceling Attitudes DEtection (CADE) dataset: a corpus of YouTube comments annotated for the study of canceling attitudes against controversial events. The section is organized as follows: we first present the data collection (\Cref{collection}) phase, then we describe the design of the annotation task (\Cref{annotation-task}), and present the Annotation Lab: the participatory approach that we adopted to recruit annotators (\Cref{annotation-lab}).


\subsection{Data collection}\label{collection}
As described in \Cref{sec:related}, cancel culture is characterized by the online public shaming of celebrities for their actions or behaviors. In a NLP perspective, this can be conceived as an event-centric task \cite{chen2021event}, where the characteristics of the event orient the interest of the research. Relying on this assumption we chose YouTube %\footnote{\url{https://www.youtube.com/}} 
 as source for our data collection. We leveraged the public APIs
 %\footnote{\url{https://developers.google.com/youtube/v3}}  
  to obtain communicative situations where the presence of canceling attitudes can be assessed. 

Firstly, we selected controversial events related to six celebrities that refer to various topics: J.K. Rowling (homo-transphobia), Kanye West (antisemitism), Lizzo (harassment), Halle Bailey (anti-woke culture), Ellen DeGeneres (bullying), Andrew Tate (sexual assault).
For each of them, we manually selected a video of a news broadcast reporting the event the target celebrity got canceled for. Since the annotators had to watch it before completing the annotation task, we opted for the most viewed video among those that did not exceed 4 minutes. For each video we extracted all the comments and randomly sampled 350 comments for each celebrity, resulting in a total of $2,100$ texts for the annotation. 


\subsection{Design of the annotation task}\label{annotation-task} 
The design of the annotation task takes into consideration two research needs: the identification of annotators' moral profiles and the definition of an annotation scheme that is effective in representing the complexity of social interactions canceling attitudes rely on.

\paragraph{Moral Foundation Questionnaire.} 
In social psychology, a common method for eliciting people's moral profile is the administration of questionnaires \cite{graham2013moral,hinz2005investigating}, which have been recently used in NLP to assess the moral stance of LLMs \cite{abdulhai-etal-2024-moral} and people \cite{davani2024disentangling}. Coherently with this research, we chose the 30-item Moral Foundation Questionnaire (MFQ30) \cite{graham2013moral}\footnote{\url{https://moralfoundations.org/questionnaires/}}, consisting of two blocks. In the first block, respondents have to reply to the question \textit{When you decide whether something is right or wrong, to what extent are the following considerations relevant to your thinking?}, rating 15 sentences on a scale from 0 (This consideration has nothing to do with my judgments of right and wrong) to 5 (This is one of the most important factors when I judge right and wrong). The second assignment is to read the 15 sentences and indicate their agreement or disagreement using a scale from 0 (strongly agree) to 5 (strongly disagree). %--- complete questionnaire in Appendix \ref{app-mfq}. 
By aggregating respondents' replies, it is possible to elicit which moral foundations contribute the most to their moral profile.

\paragraph{Annotation scheme.}
The annotation scheme is composed of two axes: stance \cite{aldayel2021stance} and acceptability \cite{forbes2020social}. The former is adopted to annotate the stance of the comment towards the celebrity; acceptability is adopted to evaluate whether the comment is perceived as morally unacceptable by the annotator. 

For each of the six target celebrities, we prepared a summary of the controversial event that introduced the topic to the annotators (Appendix \ref{app-event_description}) and then asked them to watch a YouTube video 
% that presents 
on the controversial event (Appendix \ref{app-annotation_materials}). 
Following this step, they could annotate
% A second step is the annotation of 
the stance and the unacceptability of comments about celebrities. First, annotators must evaluate whether the YouTube user intended to attack, defend or was neutral towards the controversial event involving the target celebrity (stance).
Annotators must then evaluate the social unacceptability of the comment, choosing on a scale that ranges from 1 (totally acceptable) to 4 (totally unacceptable). %This 

\subsection{Annotation Lab}\label{annotation-lab}
Since our research aims to identify how people with specific moral profiles perceive canceling attitudes in a realistic scenario, our annotator recruitment strategies focused on the involvement of people who have specific interests in the issue. We engaged with three stakeholders: activists against online discrimination, AI researchers, and NLP students. For each stakeholder we organized an annotation lab, which is designed following the literature on Participatory AI \cite{Delgado2023}: people are not only recruited as annotators, but are involved during the whole dataset creation process. For this reason, we involved stakeholders rather than relying on crowdsourcing annotator platforms. 

The annotation lab is structured in three main steps: \textit{i)} engagement: we presented a set of slides through a shared video in which we provided a definition of the phenomenon of cancel culture, we shared our research objective and its social impact and explain the whole annotation process; \textit{ii)} annotation: people were asked to fill out the MFT questionnaire and to annotate YouTube comments according to our annotation scheme; \textit{iii)} feedback: we asked participants to provide feedback about the annotation process and share their ideas about potential applications of a technology for the identification of canceling attitudes. To this aim, we adopted two methodologies: the administration of a checkout questionnaire, and the organization of two focus groups. 

\paragraph{Feedback questionnaire.} We administered the questionnaire to all the involved stakeholders. It included 7 questions (Appendix \ref{app-checkout_qst}) about three aspects of the annotation lab: evaluating the experience in terms of its emotional impact and difficulty; providing improvements to the annotation scheme; and suggesting downstream applications of a technology trained on such a resource.

\paragraph{Focus group.} We organized one focus group aimed at students and one aimed at activists. During these meetings, we presented the preliminary results of the annotation task, and kicked off a semi-structured discussion to collect their feedback along three topics of interest: \textit{i)} evaluation of the data creation process; \textit{ii)} the relevance of morality for the task; \textit{iii)} the co-design of NLP technologies based on the dataset. 

