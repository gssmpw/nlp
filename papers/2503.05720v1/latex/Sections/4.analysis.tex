\section{Corpus Analysis}
\label{sec:corpus-analysis}
In this section we analyse the CADE corpus.\footnote{The repository of the dataset will be made available under a CC BY license after the anonymity period.} We first describe the composition of annotators (\Cref{ss:process}), then the agreement between them (\Cref{ss:iaa}), finally we describe whether annotators moral profiles correlate with their sociodemographic traits (\Cref{ss:moral_profiles}).
% has an independent impact in their perception of canceling attitudes. 

\subsection{Annotation Process} \label{ss:process}
The annotation task involved  $57$ annotators belonging to three stakeholders: $30$ students, $17$ activists, and $10$ researchers. In addition, each annotator voluntarily shared their information about gender identity, and age, which was grouped into generations (Boomer, GenX, GenY and GenZ), nationality, ethnicity, education level and employment status.
% , and stakeholder type (student, researcher, activists). 
All the sociodemographic data are reported in \Cref{tab:ann_demographics} in Appendix \ref{app-demographics}.
%, the annotators could always write their answers instead of choosing among the provided options.
Having adopted a participatory approach to annotators recruitment (\Cref{annotation-lab}) that focuses on engaging with people who are interested in the phenomenon rather than hiring crowdworkers, the pool of annotators is not balanced along all sociodemographic axes. 

Each annotator is asked to annotate a subset of $210$ comments gathered from YouTube (\Cref{collection}): $35$ for each celebrity and the controversial event related to them. After cleaning unrelated comments, the final corpus includes $2,094$ texts and $11,935$ annotations. Each text has been annotated an average of $5.7$ times (with a median of $6$). Annotators had the option to refrain from assigning a label. We eliminated those who did not finish more than one-third of the task.


\subsection{Inter Annotator Agreement} \label{ss:iaa}
To assess the Inter Annotator Agreement (IAA) between annotators we employed Krippendorff's Alpha \cite{krippendorff2011computing}, which handles both agreement by chance (as the more common Cohen's Kappa agreement score), and incomplete annotations. We computed the IAA per sample and averaged them, resulting in a moderate agreement on stance ($0.501$), and a fair agreement on acceptability ($0.222$). The strong difference between the two tasks highlights how people tend to agree more when judging YouTube users' intentions rather than when they are asked to evaluate which messages they consider unacceptable \cite{davani2024disentangling}
%, compared to defining their boundaries in what can be considered acceptable. 
Labeling %a user's stance 
the stance expressed by a comment results less subjective than stating %what 
whether the comment is %is
to consider unacceptable. We report the scores broken down by sample in \Cref{tab:sample-iaa} in Appendix \ref{app-annotation_materials}.

\subsection{Annotators Moral Profiles} \label{ss:moral_profiles}
As a third part of the analysis of our annotation corpus, we assign annotators' moral profiles by leveraging the results of the MFT Questionnaire (\Cref{annotation-task}) and test to which extent they correlate with the following sociodemographic characteristics: gender, age, and stakeholder type.\footnote{Since the recruitment of stakeholder has been performed in European countries, we decided not to include %the other collected demographics 
nationality and ethnicity as variables of our study, as they result to be unbalanced.%because they are more unbalanced across the annotators.
 We also excluded the education level and employment status because they are strongly dependent on the type of stakeholders involved during the annotation, which is the only social condition monitored by design.} %we were interested in exploring these traits as potential stakeholders, the only variable monitored by design.}

We first computed the moral profile of annotators by obtaining their scores over the five foundations (care, fairness, loyalty, authority and purity). Each foundation is assigned a score between 0 (irrelevant) and 5 (very relevant). Together, the five scores represent the moral profile of the annotator.   

We then computed the Pearson coefficient between these profiles and gender, age, and stakeholder type. We found no statistically significant relationship between traits and moral foundations. This suggests that when annotators are grouped based on demographics, their moral profiles exhibit high variability and sparsity.

Given these preliminary results, we assigned a moral profile to each annotator through a clustering process that relies on their replies to the questionnaire. Annotators were represented with a vector with 5 dimensions, where each value in the vector represents the score obtained by the annotator for each moral foundation. 
We computed the pairwise distance with Euclidean metric and performed Agglomerative Clustering, which is preferred because it does not require setting the number of clusters as a parameter. We used Ward's linkage criterion;  to choose the best number of clusters we relied on three intrinsic evaluation metrics that do not need ground truth labels, namely Silhouette Coefficient \cite{ROUSSEEUW198753}, Calinski Harabaz Index \cite{Cali≈Ñski01011974} and Davies Bouldin Index \cite{Bouldin1979}. We obtained $2$ clusters, Cluster\_0 (CL\_0) with $41$ annotators, and Cluster\_1 (CL\_1) with $16$.

\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{latex/images/cl_foundatoins.png}
    \caption{Heatmap visualization for cluster analysis}
    \label{fig:cluster-foundations}
\end{figure}

We performed a qualitative analysis of the obtained moral clusters to understand which patterns emerge from their composition.
First, we analyzed whether the clusters were consistent with the scores that individuals obtained on each foundation. The heatmap shown in \Cref{fig:cluster-foundations} represents 
% the correlation between the two clusters and the moral foundations. 
the mean score of the five foundations for each cluster. Looking at the higher divergence, we computed the Pearson coefficient between the clusters and each of the moral foundations, reporting a correlation with loyalty ($r = 0.598$ with $p-value = 9.191 \cdot 10^{-7}$), authority ($r = 0.759$ with $p-value = 7.562 \cdot 10^{-12}$) and purity ($r = 0.792$ with $p-value = 2.063 \cdot 10^{-13}$).

These results are theoretically motivated by the MFT theory, which distinguishes between individual binding foundations (care, harm), rooted on the preservation of individual freedom, and group-binding foundations (loyalty, authority, and purity) which focus on the duties of individuals towards their social groups. In this sense, cluster\_1 is characterized by a higher moral attitude towards belonging to a group.

\begin{table}[]
    \centering\small
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{llcc}
    \hline
          \multicolumn{2}{c}{Demographics}&   CL\_0& CL\_1\\
    \hline
  \multirow{4}{*}{Gender identity} &Female& 23 (67.7\%) & 11 (32.3\%) (\\
 & Male& 16 (80\%) & 4 (20\%)\\
 & Non-binary& 2 (100\%)&-\\
 & Prefer not to say& -&1 (100\%)\\
 \hline
 \multirow{4}{*}{Generation}& Boomer& 1 (100\%)&-\\
 & GenX& 1 (50\%) &1 (50\%)\\
 & GenY& 13 (73\%) & 5 (27\%)\\
          &GenZ& 
     26 (73\%)& 10 (27\%)\\
 \hline
 \multirow{3}{*}{Stakeholder}& Activist& 15 (89\%) &2 (11\%)\\
 & Researcher& 7 (70\%)&3 (30\%)\\
 & Student& 19 (64\%) &11 (36\%)\\
 \hline
 \end{tabular}}
    \caption{Composition of the clusters (CL\_0 and CL\_1) in respect to annotators' gender identity, generation and role as stakeholder}
    \label{tab:cl-composition}
\end{table}

Once moral clusters have been validated against the MFT, we investigated again the presence of sociodemographic patterns in clusters to gain more insights from their potential correlation with moralities.
\Cref{tab:cl-composition} shows the composition of moral clusters broken down by annotators' gender, generation, and role as stakeholders. As it can be observed, the distribution of moral clusters along the gender axis is uniform: 32\% of women and 20\% of men belong to cluster\_1, showing a distribution that can be explained by annotators' gender imbalance. This is even more emphasized if generations are observed: moral clusters are perfectly distributed in Generation Y and Generation Z, despite the age of annotators being highly unbalanced towards the latter. This suggests that, in the context of this research, age is not a factor in determining the morality of annotators. Observing the distribution of moral clusters among stakeholders shows interesting patterns, instead. If on one side 36\% of students and 30\%  of researchers belong to cluster\_1, only 11\% of activists belong to this cluster. 