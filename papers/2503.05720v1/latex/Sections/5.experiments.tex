\section{Experiments} \label{sec:experiments}

In this section we leverage the CADE dataset to answer the two Research Questions outlined in \Cref{introduction}. In \Cref{RQ1} we analyze human disagreement in the evaluation of canceling attitudes with a specific focus on the impact of morality in this task. In \Cref{RQ2} we study how LLMs classify canceling attitudes, analyzing whether they follow specific patterns that align them to moral or sociodemographic characteristics of annotators.
\begin{table*}[h]
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{c c c |c c |c c |c c |c c |c c | cc}  % Adjusted column count
        & \multicolumn{2}{c|}{Rowling} & \multicolumn{2}{c|}{West} & \multicolumn{2}{c|}{Lizzo} & \multicolumn{2}{c|}{Bailey}  
        & \multicolumn{2}{c|}{DeGeneres} & \multicolumn{2}{c}{Tate} & \multicolumn{2}{c}{All Celeb.} \\
        & a-d-n & un. & a-d-n & un.& a-d-n & un. & a-d-n & un.& a-d-n & un. & a-d-n & un.& a-d-n & un.\\ 
        
        moral\_0 & \textbf{.13-.71-.16} &2.12& \textbf{.20-.55-.24}&1.76& .63-.08-.26&2.72& \textbf{.40-.34-.26}&\textbf{1.77}& .67-.13-.20&\textbf{2.05}& \textbf{.14-.71-.15}&\textbf{1.78}& .37-.42-.21&2.03\\
        moral\_1 & \textbf{.21-.59-.19}&2.06& \textbf{.27-.47-.25}&1.71& .63-.06-.30&2.59& \textbf{.35-.32-.32}&\textbf{1.90}& .66-.13-.19&\textbf{2.27}& \textbf{.15-.61-.23}&\textbf{1.66}& .38-.37-.25&2.03\\
        \midrule
        
        men & .17-.67-.15&\textbf{2.05}& .24-.52-.23&\textbf{1.67}& .63-.09-.25&2.64& .40-.34-.25&\textbf{1.66}& \textbf{.66-.15-.17}&\textbf{2.03}& \textbf{.13-.74-.13}&1.77& .38-.42-.20&1.96\\
        women & .14-.68-.17&2.15& .21-.53-.25&1.79& .63-.07-.29&2.72& .38-.32-.30&\textbf{1.90}& \textbf{.67-.11-.21}&\textbf{2.17}& \textbf{.15-.66-.19}&1.73& .36-.40-.24&2.07\\
        \midrule
        
        activist & \textbf{.12-.74-.13}&2.15& .22-.53-.23&\textbf{1.86}& \textbf{.63-.12-.22}&2.77& .38-.33-.27&\textbf{1.89}& .65-.13-.20&2.07& .13-.72-.14&\textbf{1.82}& .36-.44-.20&2.09\\
        student & \textbf{.18-.63-.19}&2.11& .21-.54-.25&\textbf{1.72}& \textbf{.63-.05-.30}&2.63& .40-.33-.27&1.81& .66-.12-.20&\textbf{2.16}& \textbf{.15-.64-.20}&\textbf{1.73}& .37-.38-.25&2.02\\
        researcher & .13-.72-.15&\textbf{2.00}& .26-.49-.24&\textbf{1.61}& .63-.09-.28&2.66& .34-.34-.32&\textbf{1.67}& .68-.14-.17&\textbf{2.04}& .13-.73-.14&1.67& .36-.42-.21&2.04\\
        \midrule
        
        all & .15-.68-.17&2.1& .23-.53-.24&1.7& .64-.08-.28&2.6& .39-.33-.28&1.8& .67-.13-.20&2.1& .14-.69-.17&1.7& &\\
        \bottomrule
    \end{tabular}}
    \caption{Percentage of Attack (a), Defend (d) and Neutral (n) labels, and the average unacceptability (un.) for each group and celebrity. In bold scores with a chi-square test below $p<0.05$. Only for stakeholders, the chi-squared is computed one vs. all.}
    \label{tab:stance-acc_all}
\end{table*}
\subsection{STUDY 1: What is the impact of individual's morality in evaluating canceling attitudes?}\label{RQ1}
The first experiment aims to analyze the human disagreement of annotators in perceiving canceling attitudes, focusing in particular on the influence of their moral profiles (\Cref{ss:moral_profiles}) in their evaluation. Given the type of dataset, which replicates online situations triggered by controversial events, we analyze to which extent human disagreement is determined by the type of events and celebrities targeted by YouTube comments.

The first step of the study is the analysis of stance and unacceptability emerging from YouTube comments, broken down by celebrity. \Cref{tab:stance-acc_all} shows the annotations aggregated by celebrities (the columns) and moral and sociodemographic groups (the rows). For each celebrity, the relative distribution of stance labels (a-d-n)  and the average unacceptability scores are reported.

The table shows that \textbf{canceling attitudes vary significantly among celebrities}. Looking at the last row, only a few comments attack J.K. Rowling ($15\%$) for her transphobic declarations and Andrew Tate ($14\%$) for the accusation against him of sexual assault. Conversely, Lizzo suffers a high number of attacks ($64\%$) for her misconduct against a member of her staff, as well as Ellen DeGeneres ($67\%$) for her bullying attitudes in her working environment. 
The perceived unacceptability of YouTube comments appears to be orthogonal to stance, since comments can be perceived as highly unacceptable regardless they attack celebrities or not. For instance, J.K. Rowling is mostly defended but the unacceptability score is high. This means that there are many comments that defend her by adopting canceling attitudes against other targets: in these cases Daniel Radcliffe and Emma Watson, who called out against her transphobic claims. Lizzo is mostly attacked and comments about her scored the highest unacceptability ($2.6$): this means that she is the actual target of canceling attitudes. Comments on Andrew Tate and Kanye West, who mainly defend them, also score the lowest unacceptability scores ($1.7$), showing that their focus is more on defending them rather than attacking other targets. 

The analysis of annotations broken down by moral cluster, gender, and stakeholder shows lower variation among moral and sociodemographic axes than one emerging from the analysis of celebrities (\Cref{tab:stance-acc_all}, last column). The percentage of comments annotated as attacking celebrities is uniform along morality, gender, and type of stakeholder, ranging from $0.36$ to $0.38$. The percentage of comments evaluated as defending celebrities shows more variation along moral clusters (cluster\_0: $42\%$ \textit{versus} cluster\_1: $37\%$) and stakeholders (activists: $44\%$ \textit{versus} students: $38\%$). Variation in the evaluation of unacceptability is even less significant. Annotators' moral profiles appear to have no impact on the perception of this phenomenon: the average unacceptability is $2.03$ for both groups. Stakeholders show some variation, with the average unacceptability provided by activists ($2.09$) that diverges by $0.07$ points from the average unacceptability provided by students ($2.02$). Differences along the gender axis are more significant, as it is possible to observe a high variation between the unacceptability score assigned by women ($2.07$) with one assigned by men ($1.96$). 

If the behaviors of different moral and sociodemographic groups are observed through the lens of specific events, very diverging patterns in the evaluation of canceling attitudes emerge. 

For each axis of interest (e.g., morality) we computed the relative distribution of stance and the average unacceptability scores and computed the Chi-squared test between the distribution of labels assigned by annotators belonging to different groups (e.g., cluster\_0 \textit{vs} cluster\_1) in order to assess if there is a significant divergence between their evaluations. All the cases where the Chi-squared test shows a statistically significant divergence ($p<0.05$) are highlighted in bold in \Cref{tab:stance-acc_all}. As it can be observed in the table, the different perception of stance among annotators with different moral profiles clearly emerges if specific celebrities are examined. The two moral clusters diverge in the evaluation of attacking comments against J.K. Rowling (cluster\_0: $13\%$ \textit{versus} cluster\_1: $21\%$) and Kanye West (cluster\_0: $20\%$ \textit{versus} cluster\_1: $27\%$) and of defending comments against Andrew Tate (cluster\_0: $71\%$ \textit{versus} cluster\_1: $61\%$). Similar variations can be observed at the level of the gender axis and stakeholder axis. Women and men diverge the most in evaluating the defending stance on comments on Andrew Tate (men: $74\%$ \textit{versus} women $66\%$); students ($18\%$) perceive more attacking comments against J.K. Rowling than activists ($12\%$) and researchers ($12\%$).

The perception of unacceptability shows interesting patterns as well. Except for comments about Lizzo, where there is no statistically significant divergence along any considered axis, each axis of moral and sociodemographic variation shows differences in the attribution of the unacceptability of comments. In 5 cases out of 6 women and activists rank YouTube comments as more unacceptable on average. In 4 cases out of 6 people belonging to the moral cluster\_0, which is the one more tied to individual bindings, are inclined to label comments as more unacceptable.

The study shows that the magnitude of canceling attitudes significantly varies depending on the type of controversial event: some celebrities are more likely to trigger violent reactions than others. Human disagreement in the evaluation of canceling attitudes in YouTube comments heavily depends on such variation. This is particularly true for the role of morality in evaluating unacceptability. When patterns of annotation towards specific celebrities are observed, \textbf{morality appears to drive the disagreement between annotators} more than their gender and stakeholder type. 




% \begin{itemize}
%     \item methodology description of agreement e chi-squared 
%     \item 3 axes are independent 
%     \item celebrity/event are relevant
%     \item researcher highest acceptability (attivisti al contrario)
%     \item say something on gender e moral clusters
%     \item repeat everything for stance
% \end{itemize}





\subsection{STUDY 2: Do Different LLMs Align with Different Moral Profiles in Evaluating Canceling Attitudes?}\label{RQ2}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{latex/images/llm_grid_v2.pdf}
    \caption{LLM variability in alignment on the unacceptability task.}
    \label{fig:llm-heatmap}
\end{figure*}

% \begin{table*}[!ht]
%     \centering
%     \small
% \begin{tabular}{l|rr|rr|rrr|r}
% \textbf{Language model} &\textbf{ CL 0} & \textbf{CL 1} & \textbf{Female} & \textbf{Male} & \textbf{Activist} & \textbf{Researcher} & \textbf{Student} & \textbf{All} \\
% \hline
% OLMo & 35.54 & 36.37 & 37.45 & 33.17 & 34.79 & 37.09 & 36.62 & 37.80 \\
% BLOOMZ & 35.99 & 35.82 & 36.38 & 37.41 & 38.29 & 34.92 & 36.48 & 35.74 \\
% DeepSeek R1 1.5B & 36.67 & 36.64 & 36.61 & 36.35 & 36.95 & 34.99 & 35.77 & 35.71 \\
% OPT-IML & 19.71 & 26.02 & 22.89 & 18.38 & 22.07 & 22.49 & 22.75 & 18.17 \\
% Llama 3.2 1B & 33.28 & 33.72 & 34.12 & 32.57 & 33.07 & 34.99 & 33.81 & 35.23 \\
% Llama 3.2 3B & 35.48 & 36.83 & 35.71 & 35.17 & 36.55 & 36.25 & 35.81 & 36.21 \\
% Ministral 8B & 35.03 & 36.41 & 36.76 & 32.97 & 34.53 & 36.68 & 36.50 & 36.77 \\
% \end{tabular}
%     \caption{Macro-averaged f-score of the LLMs in the stance category in relation to each of the considered annotator groups. The \textit{All} column indicates the f-score when considering all groups simultaneously.}
%     \label{tab:llm-fscores-stance}
% \end{table*}


% \begin{table*}[!ht]
%     \centering
%     \small
% \begin{tabular}{l|rr|rr|rrr|r}
% \textbf{Language model} &\textbf{ CL 0} & \textbf{CL 1} & \textbf{Female} & \textbf{Male} & \textbf{Activist} & \textbf{Researcher} & \textbf{Student} & \textbf{All} \\\hline
% OLMo & 42.40 & 41.16 & 40.89 & 42.07 & 37.92 & 40.48 & 43.07 & 46.31 \\
% BLOOMZ & 44.50 & 46.00 & 41.75 & 48.92 & 41.09 & 39.52 & 46.92 & 43.20 \\
% DeepSeek R1 1.5B & 42.48 & 43.61 & 39.81 & 47.32 & 39.61 & 38.92 & 44.11 & 42.17 \\
% OPT-IML & 41.63 & 43.09 & 40.90 & 44.09 & 38.39 & 39.06 & 43.84 & 40.50 \\
% Llama 3.2 1B & 33.70 & 31.34 & 34.47 & 32.82 & 31.91 & 33.32 & 32.34 & 34.08 \\
% Llama 3.2 3B & 37.11 & 35.66 & 36.48 & 36.70 & 33.08 & 37.02 & 37.09 & 39.80 \\
% Ministral 8B & 45.03 & 45.42 & 44.04 & 44.93 & 38.53 & 44.76 & 46.24 & 48.35 \\
% \end{tabular}
%     \caption{Macro-averaged f-score of the LLMs in the acceptability category in relation to each of the considered annotator groups. The \textit{All} column indicates the f-score when considering all groups simultaneously.}
%     \label{tab:llm-fscores-accept}
% \end{table*}

% \begin{table*}[!ht]
%     \centering
%     \small
% \begin{tabular}{l|rrrrrrr}
% \textbf{Language model} &\textbf{ CL 0} & \textbf{CL 1} & \textbf{Female} & \textbf{Male} & \textbf{Activist} & \textbf{Researcher} & \textbf{Student}  \\
% \hline
% OLMo & 19.13 & 18.05 & 19.26 & 17.75 & 16.43 & 16.86 & 18.74 \\
% BLOOMZ & 14.46 & 11.56 & 14.65 & 13.49 & 14.04 & 11.71 & 13.90 \\
% DeepSeek R1 1.5B & 12.66 & 10.52 & 12.54 & 12.49 & 9.18 & 11.35 & 12.72 \\
% OPT-IML & 9.09 & 8.53 & 7.79 & 8.51 & 7.52 & 7.87 & 9.16 \\
% Llama 3.2 1B & 8.28 & 7.29 & 8.26 & 8.35 & 7.67 & 8.32 & 8.00 \\
% Llama 3.2 3B & 4.54 & 4.00 & 5.13 & 5.53 & 4.97 & 5.07 & 5.21 \\
% Ministral 8B & 18.77 & 17.39 & 18.62 & 17.81 & 16.03 & 16.52 & 17.83 \\

% \end{tabular}
%     \caption{Standard deviation of LLMs when considering the studied celebrities.}
%     \label{tab:llm-fscores-std}
% \end{table*}

In this second study, we examine the attitude of LLMs towards cancel culture by observing how their performance varies in relation to human annotators.
We focus on the unacceptability task, as it is the most sensible in terms of morality, and is the target of the assessment towards cancel culture by both human annotators and language models.
To do so, we select six relevant models, similarly as done by \citet{NEURIPS2023_a2cf225b}, namely:
% The selected models are:
\texttt{OLMo 7B}, %\footnote{\url{https://huggingface.co/allenai/OLMo-2-1124-7B-Instruct}},
\texttt{BLOOMZ 3B}, %\footnote{\url{https://huggingface.co/bigscience/bloomz-3b}},
\texttt{DeepSeek R1 1.5B}, %\footnote{\url{deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B}},
\texttt{OPT-IML 1.3B}, %\footnote{\url{https://huggingface.co/facebook/opt-iml-max-1.3b}},
\texttt{Llama 3.2 3B}, and %\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}}, and
\texttt{Ministral 8B}.%\footnote{\url{https://huggingface.co/mistralai/Ministral-8B-Instruct-2410}}.

For extracting the models' annotations, we adopt a straightforward zero-shot setting, prompting the models to classify the unacceptability of each comment.
To analyze the generated annotations, we evaluate through a classification task, aggregating human annotations through majority vote, and treating them as a sort of gold standard.
Although the models are not actually performing a classification task, we framed the evaluation as such.
In this way, we use the macro-averaged F1-score to evaluate language models' performances using human annotations as a reference.

Figure~\ref{fig:llm-heatmap} shows the result of this experiment.
On an aggregate level, we observe that the models largely vary their alignment to human annotators when considering the different celebrities.
These variations show when examining both celebrities (vertical axis) and annotator groups (horizontal axes).
In light of this observation, we argue that language models show a similar behavior to that of human annotators, varying their perception towards cancel culture with the event that is being assessed.

From an in-depth observation of \Cref{fig:llm-heatmap} both common and specific patterns among models emerge. LLMs show different degrees of alignment with specific moral clusters. OLMo classifies unacceptability with a highest F-score against annotations provided by moral cluster\_0 in $5$ cases out of $6$ while OPT-IML shows an opposite alignment, obtaining a highest F-score against moral cluster\_1 annotations in $4$ cases out of $5$. The alignment of LLMs with annotators broken down by gender appears to systematically lean towards men. BLOOMZ and DeepSeek always obtain the highest F-1 scores against men's annotations, OPT-IML in $5$ cases out of $6$. Models that align the most with women are OLMo and Ministral, which both score a highest F-score against them in $3$ cases out of $6$. The analysis of stakeholders shows a general higher alignment of LLMs with students, which are the most represented group in our pool of annotators. In particular, BLOOMZ always aligns with this category of stakeholders. On the opposite, activists are the stakeholders against which models are more misaligned. Specifically, Llama and Ministral obtain the lowest F-score against annotations provided by this group in $5$ cases out of $6$.

Some general patterns seem to emerge if specific celebrities are observed. All models show the highest agreement with men and annotators belonging to the moral cluster\_1 in the evaluation of unacceptability of comments against Andrew Tate, and the lowest agreement with activists. Other general patterns are specific to the alignment of models with moral clusters. $5$ models out of $6$ align with moral cluster\_0 in the evaluation of unacceptability against Ellen DeGeneres and Halle Bailey, while the remaining models obtain the same F-1 score for both moral clusters. LLMs' classifications of unacceptability of comments against Halle Bailey and Kanye West always lean towards men's annotations while there are no celebrities against which models systematically align with women in the classification of unacceptability. 

\iffalse
Observing models' behavior against specific celebrities allows also the identification of cases where they show the highest divergence. Ministral, Llama, and OLMo are more aligned with the moral cluster\_0 in the classification of comments against J.K. Rowling and Lizzo, while DeepSeek, Bloom, and OPT-IML lean towards judgments provided by moral cluster\_1. When looking at the gender axis, Ministral, Llama, and OLMo align the most with women's annotations against Lizzo while DeepSeek, Bloom, and OPT-IML mostly align with men. 

It is interesting to notice that the Llama 3.2 model shows a low variability in comparison with the rest of the models, but it also consistently obtains lower scores in the evaluation.
In contrast, the BLOOMZ model obtains the highest scores, up to 0.66 in F-score, but shows larger variability with different celebrities.
The most noteworthy difference is found in the \textit{Lizzo} assessments, where the BLOOMZ, DeepSeek and OPT-IML show especially high variation in comparison to other models.
\fi
% Attending to the stance evaluation (Table~\ref{tab:llm-fscores-stance}), we observe that the differences between the moral clusters are generally low.
% Similarly, differences between the female and male groups are low, with the interesting exception of the OLMo and Ministral models, which show a higher alignment with female annotators.
% Besides, considering the stakeholder groups, it can be seen that each model shows variation on their alignments.
% The OLMo and BLOOMZ models show higher scores with the researcher and activist groups, respectively.
% We do not observe a clear trend of alignment to any of the stakeholder groups by all models.

In terms of comparing the annotator groups, we see that there are differences between the moral clusters, being BLOOMZ the model that shows the largest variation.
Likewise, we observe differences between the female and male groups, with several models showing greater alignment with the male group.
A relevant exception is Ministral, that shows a balance behavior in terms of gender, obtaining high scores overall.
Focusing on the stakeholders group, we also observe more variation in this evaluation.
In general, the models align with the student group to a greater extent.


The study shows that \textbf{LLMs tend to exhibit different perspectives in classifying unacceptability}. By observing their F-score on each event, it is possible to notice models that are more aligned with a specific moral cluster in comparison to models that are more aligned with the other one. Models also diverge along the gender axis: some of them always lean towards men's annotation while others show variation in their alignment with men and women. As for human annotations, the type of event influences the alignment between groups of annotators and LLMs classification, which in some cases systematically leans towards specific groups while in others it triggers divergent behaviors between models. 

% \begin{table}[!ht]
%     \centering\small
%     \begin{tabular}{c|cl}
%     \hline
%     \textbf{Language model} &  \textbf{Stance} & \textbf{Acceptability} \\
%     \hline
%     OLMo 2 & 37.80 & 46.31 \\
%     BLOOMZ & 35.74 & 43.20 \\
%     DeepSeek R1 1.5B & 35.71 & 42.17 \\
%     OPT-IML & 18.17 & 40.50 \\
%     Llama 3.2 1B & 35.23 & 34.08 \\
%     Llama 3.2 3B & 36.21 & 39.80 \\
%     Ministral 8B & 36.77 & 48.35 \\
%     OpenAI's API (?) & - & - \\
%  \hline
%  \end{tabular} 
%     \caption{XXX}
%     \label{tab:llm-fscores-agg}
% \end{table}



% - Selection of LLMs, similarly done as \cite{NEURIPS2023_a2cf225b}

% - The aim is to assess each model's attitude towards cancel culture. Thus, to extract the annotations, prompt in a zero-shot setting. 

% - Scores are low, this is not a classification task, but an evaluation framed as classification. We use macro-averaged f-score.

% - Table X shows the aggregated results, for all groups of annotators. Tables Y and Z show the separated results.

% - Patters on aggregate level
% \begin{itemize}
%     \item On an aggregate level (tab \ref{tab:llm-fscores-agg}), models tend to reach a higher score in the acceptability task, except for the Llama 1B, that shows a slightly inverse behaviour.
%     \item OPT-IML model obtains a surprisingly low score on the stance task.
%     \item OLMo and Ministral obtain the highest scores in the stance and acceptability tasks, respectively.
% \end{itemize}

% - Patterns for tables \ref{tab:llm-fscores-stance} and \ref{tab:llm-fscores-accept}.
% \begin{itemize}
    % \item Stance
    % \begin{itemize}
    %     \item OPT-IML model diverges from the rest of the models, as seen in the aggregate measure
    %     \item Differences between moral clusters are low
    %     \item Differences between the female and male groups is generally low, with the exception of OLMo and Ministral models, that show a higher alignment with female annotators
    %     \item Considering the stakeholder distinction, each model shows variation in their alignment. The OLMo and BLOOMZ models show higher metrics with the researcher and activist groups, respectively. 
    %     \item In aggregate, there is no clear trend of alignment to any of the stakeholder groups.
    % \end{itemize}
%     \item Acceptability
%     \begin{itemize}
%         \item As before OPT-IML diverges from the rest of the models.
%         \item Differences between the moral clusters tend to be higher in this task, the BLOOMZ model shows the largest variation.
%         \item Differences between female and male are larger than with the previous task. In this case many models show a greater alignment with the male group. A relevant exception is Ministral, that shows a balanced behaviour in terms of gender, with high metrics overall.
%         \item Attending to the stakeholders, we observe more differences in this task. Overall, the models align with the studing group to a greater extent.
%     \end{itemize}
% \end{itemize}





\iffalse
\begin{figure*}[h]
    \centering
    \subfigure{\includegraphics[width=0.3\textwidth]{latex/images/radar_stance_J K Rowling.png}}
    \subfigure{\includegraphics[width=0.3\textwidth]{latex/images/radar_stance_Kanye West.png}}
    \subfigure{\includegraphics[width=0.3\textwidth]{latex/images/radar_stance_Lizzo.png}}
    
    \subfigure{\includegraphics[width=0.3\textwidth]{latex/images/radar_stance_Halle Bailey.png}}
    \subfigure{\includegraphics[width=0.3\textwidth]{latex/images/radar_stance_Ellen DeGeneres.png}}
    \subfigure{\includegraphics[width=0.3\textwidth]{latex/images/radar_stance_Andrew Tate.png}}
    
    \caption{Radar plot on Stance}
    \label{fig:radar-stance}
\end{figure*}
\fi


