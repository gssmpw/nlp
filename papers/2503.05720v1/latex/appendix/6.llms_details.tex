\section{LLM implementation details}

In this work, we have used the following public models:
\begin{itemize}
    \item \texttt{OLMo 7B}\footnote{\url{https://huggingface.co/allenai/OLMo-2-1124-7B-Instruct}}
    \item \texttt{BLOOMZ 3B}\footnote{\url{https://huggingface.co/bigscience/bloomz-3b}}
    \item \texttt{DeepSeek R1 1.5B}\footnote{\url{deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B}}
    \item \texttt{OPT-IML 1.3B}\footnote{\url{https://huggingface.co/facebook/opt-iml-max-1.3b}}
    \item \texttt{Llama 3.2 3B}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}}
    \item \texttt{Ministral 8B}\footnote{\url{https://huggingface.co/mistralai/Ministral-8B-Instruct-2410}}
\end{itemize}

To generate the annotations with these models, we have followed a zero-shot approach, prompting the models to generate their annotations.
All models were executed in a NVIDIA Titan X Pascal GPU, with 12GB of memory.

To generate the annotations regarding stance, the used prompt is as follows:
\textit{Classify the text into being defensive, neutral or attacking.}
Similarly, we used the following prompt to generate the acceptability annotations:
\textit{Classify the text into a scale from 1 to 4, considering how much the text contributes to shaming or degrading a subject, being 1 the lower and 4 the higher.}

