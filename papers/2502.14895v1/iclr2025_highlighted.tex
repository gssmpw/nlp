
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage[x11names]{xcolor}
\definecolor{edit}{rgb}{0.0, 0.5, 0.7}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{booktabs}   % 美化表格线条
\usepackage{siunitx}


\title{High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning.
%
Current prediction methods are limited by training and storage efficiency, mainly focusing on 2D spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions at each timestamp remain largely unexplored.
%
To address such a challenge, we introduce a comprehensive framework for 3D radar sequence prediction in weather nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient and accurate forecasting.
%
Specifically, rather than relying on a 4D Gaussian for dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by employing a group of Gaussians while effectively capturing their movements across consecutive frames. It ensures consistent tracking of each Gaussian over time, making it particularly effective for prediction tasks.
%
With the temporally correlated Gaussian groups established, we utilize them to train GauMamba, which integrates a memory mechanism into the Mamba framework. This allows the model to learn the temporal evolution of Gaussian groups while efficiently handling a large volume of Gaussian tokens. As a result, it achieves both efficiency and accuracy in forecasting a wide range of dynamic meteorological radar signals.
%
The experimental results demonstrate that our STC-GS can efficiently represent 3D radar sequences with over $16\times$ higher spatial resolution compared with the existing 3D representation methods, while GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum of high-dynamic weather conditions.


\end{abstract}

\section{Introduction}

Weather nowcasting is a critical component of meteorological forecasting that focuses on predicting short-term weather conditions based on real-time observations. It supports many meteorological applications, such as precipitation forecasting~\cite{convgru, diffcast}, extreme weather warnings~\cite{nowcastnet}, and hurricane prediction~\cite{hurricane}, and plays a vital role in various applications, including disaster management, transportation safety, and urban planning. Accurate nowcasting relies on timely and precise predictions of rapidly changing weather patterns. 
%
One of the key pathways for weather nowcasting is radar sequence prediction, which focuses on forecasting future radar echo frames from current observations.

Recent advancements have primarily concentrated on predicting 2D radar sequences at specific altitudes~\cite{predrnn, earthformer, predrnnv2, prediff, diffcast}. However, the atmospheric system develops in three spatial dimensions, where the intensity of echoes from low-altitude clouds is closely linked to surface convective activity, while higher-altitude clouds indicate potential severe weather. Relying solely on 2D radar echoes at specific altitudes overlooks the dependencies between different heights and fails to capture the complete structure of the system. 
%
As shown in Fig.~\ref{fig:2Dto3D}, a straightforward way is to extend the 2D predictive model to 3D. However, this transition requires the model to receive higher-dimensional features, leading to increased memory usage and computational complexity, which further restricts high-resolution scaling.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.92\columnwidth]{fig_task_hist.pdf}
\end{center}
\vspace{-0.5cm}
\caption{\textbf{Comparisons between 2D and 3D radar sequence prediction frameworks.} 
%
(a) Current 2D methods mainly focus on 2D spatial predictions at specific altitudes. Limited by training and storage efficiency, it is challenging to extend these methods to  3D architectures directly.
%
(b) The size of the storage occupied by a single frame input is calculated according to the resolution of two datasets. The resolution of MOSAIC is $36 \times 384 \times 512$, 2D input is $384 \times 512$, 3D input is $36 \times 384 \times 512$, and the size of 3D Gaussians is $49,152 \times 11$. The resolution of NEXRAD is $36 \times 512 \times 512$ with 6 channels, the inputs are $6\times512\times512$, $6\times36\times512\times512$ and $49,152 \times 16$.
}
\label{fig:2Dto3D}
\vspace{-0.5cm}
\end{figure}



To address such an issue, we present a comprehensive framework for effective 3D radar sequence prediction. This framework utilizes the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and incorporates GauMamba for accurate and efficient forecasting.
%
Specifically, our STC-GS differs from traditional methods that rely on 4D Gaussians for daily dynamic scene reconstruction, where only the main subjects are dynamic and most objects are treated as locally static. In contrast, radar observations of clouds are continuously in motion and changing, without adhering to any rigid body or morphological constraints. 
%
STC-GS begins by reconstructing the initial frame utilizing a group of 3D Gaussians~\citep{3dgs}, which are then employed as anchors. By monitoring and aggregating changes to these anchors as subsequent frames are pre-reconstructed, STC-GS effectively captures the underlying motion trends of the radar sequence. The initial Gaussians from the first frame, combined with the series of motion trends extracted from subsequent frames, establish a clear temporal continuity. Such a scheme ensures the consistent tracking of each 3D Gaussian over time, thereby enhancing its efficacy in predictive tasks.
%
In practice, we develop a bidirectional reconstruction pipeline for STC-GS that accurately models the growth and dissipation of radar echoes. This pipeline incorporates dual-scale constraints to preserve the temporal coherence of Gaussian movements across both global trends and local details in the data. 


Once we establish temporally correlated Gaussian groups across all of the frames, we adopt them to train the proposed GauMamba. It integrates a memory mechanism into the Mamba framework, allowing it to model the temporal evolution of Gaussian groups while efficiently managing a large volume of Gaussian tokens. 
%
This capability is crucial for effectively forecasting a wide range of dynamic meteorological radar signals.
%
Overall, our framework not only enhances the accuracy of 3D radar sequence predictions but also improves computational efficiency, making it a robust solution for weather nowcasting applications. By leveraging the strengths of STC-GS and GauMamba, we aim to advance the state-of-the-art in meteorological forecasting, providing timely and accurate information for various practical applications.


The main contributions of this work can be summarized as follows. (1) To our best knowledge, this is the first work for 3D-based weather nowcasting by predicting high-dynamic radar sequences. Our framework introduces a novel 3D Gaussian representation termed STC-GS that adeptly captures radar data dynamics, paired with a memory-augmented network, GauMamba, which learns temporal evolution from these representations to forecast radar changes. 
%
(2) We propose a bidirectional Gaussian reconstruction pipeline, which is designed to precisely track the motion trajectory of 3D Gaussians along the sequential frames. It incorporates dual-scale constraints to ensure coherence at both global trends and local details, effectively preserving the temporal consistency of Gaussian movements.
%
(3) In addition, we collect and organize a novel high-dynamic 3D radar sequence dataset named MOSAIC and reorganize a dataset named NEXRAD. 
%MOSAIC comprises 24,542 single-channel high-resolution radar observations, while NEXRAD consists of 6,255 six-channel radar observations. 
Our experiments demonstrate that GauMamba effectively predicts future 3D radar data with $16 \times$ resolution, achieving a $19.7\%$ and $50\%$ reduction in Mean Absolute Error (MAE), and a notable improvement in predicting regions with significant radar signals.


\section{Related Work}

\subsection{3D Gaussian representation}

Recently, Gaussian Splatting-based representations offer real-time rendering with high training efficiency and garner considerable research interests\cite{tang2023dreamgaussian, diolatzis2024nd-gaussian, mallick2024taming, zhou2024feature3dgs}. Motivated by the success of 3D Gaussian Splatting, numerous studies have extended it to real-time dynamic scene reconstruction and rendering.
%
Incremental translation methods \cite{dyn-3dgs, 3dgstream} tackle this challenge by initializing each frame based on the preceding one, leveraging motion constraints to enforce temporal coherence. Other approaches extend Gaussian representations to 4D space-time \cite{4d-gaussian} or model global scene deformations with neural networks or polynomial \cite{4dgs, deformable_gaussian, spacetime-gaussian}, enabling efficient reconstructions. Despite these advancements, such methods face limitations in accurately capturing complex or discontinuous scene dynamics. Moreover, they often rely on fixed color and opacity settings to ensure consistency.

\textcolor{edit}{
Recent studies \cite{shen2024gamba, yi2024mvgamba, ziwen2024long, zhang2025gslrm} have explored integrating Mamba \cite{mamba, mamba2} or Transformer architectures with 3D Gaussian representations, focusing on reconstructing 3D Gaussians from single or multi-view images. However, these approaches are limited in their ability to model spatiotemporal dynamics, as the lack of memory mechanisms. Our method leverages sequences of 3D Gaussians to represent the temporal evolution of 3D radar echo data and employs a Memory-Augmented GauMamba model to effectively integrate information from preceding frames. 
}

\subsection{Spatio-temporal prediction}

Spatio-temporal prediction is crucial in meteorological forecasting, requiring models to capture both spatial patterns and temporal dynamics. U-Net architectures using 2D or 3D CNNs have been applied to tasks like precipitation nowcasting, Arctic Sea ice prediction, and ENSO forecasting \cite{sevir, sea-ice, enos}, though they struggle with temporal dependencies. To improve this, methods such as ConvLSTM \cite{convlstm}, ConvGRU \cite{convgru}, and PredRNN \cite{predrnn, predrnnv2} integrate memory mechanisms to better handle spatio-temporal correlations. E3D-LSTM \cite{e3d} combines 3D CNNs with LSTM for long-term forecasting, while PhyDNet \cite{phydnet} embeds physical constraints into models. SimVP \cite{simvp} simplifies prediction using convolutional encoders and decoders, while transformer-based models \cite{fourcastnet, rainformer, earthformer} capture long-range dependencies. However, deterministic models often struggle with prediction blur and fail to capture the stochastic nature of weather systems. To address this, diffusion-based models \cite{prediff, diffcast} have been introduced to estimate spatio-temporal uncertainty.

\section{Methodology}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.92\columnwidth]{fig2.pdf}
\end{center}
\vspace{-0.5cm}
\caption{\textbf{Overview of our 3D prediction framework based on STC-GS and GauMamba}. 
%
(a) STC-GS can effectively compress the size of 3D data while fully representing it.  GauMamba is a memory-augmented predictive model that leverages STC-GS for effective and accurate predictions. \textcolor{edit}{The STC-GS at Frame $t$ is input into the GauMamba to predict a set of DiffGaussians, $\Delta \mathcal{G}_{t+1}$, representing the differences between $\mathcal{G}_{t+1}$ and $\mathcal{G}_0$. This process is applied iteratively from Frame $0$ to Frame $T_{in} + T_{out} -1$.}
%
(b) In the process of radar reconstruction, dual-scale constraints are implemented to capture both the global trends and the local details present in the Gaussian motions. }
\label{fig:rec}
\end{figure}


\subsection{Preliminary} 
\textbf{3D Gaussian Splatting (3DGS).}
Gaussian Splatting~\cite{3dgs} uses a collection of 3D Gaussians to represent 3D objects or scenes. Each Gaussian is characterized by a position $\displaystyle \vp \in \mathbb{R}^3 $, a scaling factor $\displaystyle \vs \in \mathbb{R}^3 $, and a rotation quaternion $\displaystyle \vq \in \mathbb{R}^4 $. For rendering RGB images, an opacity value $ \alpha \in \mathbb{R} $ and a color feature $\displaystyle \vc \in \mathbb{R}^C $ are also included, with optional spherical harmonics for modeling view-dependent effects. In this way, a group of Gaussians can be represented as $\boldsymbol{\Theta}$, where $ \boldsymbol{\Theta}_i = \{\displaystyle \vp_i, \displaystyle \vs_i, \displaystyle \vq_i, \alpha_i, \displaystyle \vc_i\} $ denotes the parameters of the $i$-th Gaussian. 

% The rasterization stage involves calculating the Gaussian's contribution to individual pixels. Initially, the 3D position of each Gaussian is converted into the camera's coordinate system by applying the camera's intrinsic and extrinsic parameters. Following this, the renderer divides the screen into tiles and culls the Gaussians that fall outside the view frustum. This step optimizes processing efficiency by discarding Gaussians that do not contribute to the final image. Lastly, the renderer conducts alpha blending in a depth-sorted order within the view space for each pixel, amalgamating the contributions of overlapping Gaussians to generate the final rendered image.

The rasterization stage involves calculating the Gaussian's contribution to individual pixels. Initially, each Gaussian is projected into the camera's coordinate. Following this, the renderer divides the screen into tiles and culls the Gaussians that fall outside the view frustum. Lastly, the renderer conducts alpha blending in a depth-sorted order within the view space for each pixel.


\subsection{The Overall Framework}


As illustrated in Fig. \ref{fig:rec}, we introduce a comprehensive framework for the prediction of 3D radar echo sequences in weather nowcasting. The overall framework includes two main components. The first part is the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS)  for dynamic radar data representation. The second is the GauMamba, a memory-augmented Mamba network, coupled with STC-GS representations, for efficient and precise forecasting. 
%
In practice, unlike 3DGS, which is designed for rendering real-world RGB scenes, our STC-GS is specifically tailored for radar observations, whose definition is provided in Sec. \ref{sec:definition}.


Given the high-resolution 3D radar sequence, we first require to re-represent each frame $\mF_i \in \mathbb{R} ^{ H \times W \times C}$ into a group of 3D Gaussians, where $H$ and $W$ donate the vertical and horizontal spatial resolution, and $C$ denotes the number of measurement channels. 
%
Unlike the traditional 3DGS technology used for scene reconstruction, our goal is to obtain a new representation of 3D radar data based on reconstruction to further accomplish downstream prediction tasks. Therefore, the proposed STC-GS should maintain the spatiotemporal coherence based on reconstruction.
%
In this way, we introduce a bi-directional reconstruction scheme to achieve it, where a backward pre-reconstruction is first conducted to preliminarily retain the growth and dissipation of the Gaussians along the temporal dimension, and the forward reconstruction is carried out by using the pre-reconstruction results as the reference to enhance the accuracy. Please refer to Sec.~\ref{sec:bidirection} for more details.
%


Next, the prediction based on 3D radar can be transformed to predict the variation of STC-Gaussians along frames. We coupled the memory mechanism with Mamba
to achieve linear time training and inference based on Gaussian Groups and the capacity to model the temporal evolution of Gaussian groups. Our proposed GauMamba takes parameters of $t$-th frame Gaussians $\mathcal{G}_t$ as inputs and predicts the difference between the $\mathcal{G}_{t+1}$ and $\mathcal{G}_0$, see \ref{sec:gaumamba} for more details.


\subsection{SpatioTemporal Coherent Gaussian Representation}


The goal of reconstructing 3D radar data into a set of 3D Gaussians is to obtain a feature-dense representation that can nearly losslessly represent the original 3D radar data.
%
To achieve this, we propose a differentiable reconstruction pipeline motivated by 3D Gaussians Splatting (3DGS) method~\cite{3dgs}.
%
Instead of relying on Structure from Motion (SfM) in 3DGS, our STC-GS randomly samples points from the original 3D radar data to initialize the positions and radar features of the 3D Gaussians. 
%
We then design a differentiable radar profile renderer capable of rendering radar cross-sections from any angle, including horizontal, vertical, and oblique views. Readers can refer to Appx. \ref{supp:render} for more details.



\subsubsection{Definition of 3D Radar Gaussian in STC-GS}\label{sec:definition}


Given the specific location $\displaystyle \vx \in \sR^3$ of the radar frame, we use the slightly perturbated coordinates indices of the corresponding pixels as the initial positions of the 3D Gaussians, to avoid falling into saddle points in the optimization space. 
%
Unlike using RGB values to reconstruct real-world scenes, reconstructing radar observation data requires 
$N$ dimensional features $\displaystyle \vf \in \sR^N$, which indicates radar echo intensity, horizontal reflectivity factor, spectrum width, etc. 
%
The number of radar features $N$ depends on the specific radar dataset.
Additionally, defining a 3D Gaussian requires an optimizable full 3D covariance matrix $\displaystyle \mSigma$. 
% However, $\displaystyle \mSigma$ is physically meaningful only when it is positive semi-definite, which cannot always be guaranteed during optimization. To tackle this, $\displaystyle \mSigma$ is decomposed into a rotation matrix $\displaystyle \mR$ and a scaling matrix $\displaystyle \mS$ by:
It can be decomposed into a rotation matrix $\displaystyle \mR$ and a scaling matrix $\displaystyle \mS$ for by:
$$
\displaystyle \mSigma = \mR \mS \mS^T \mR^T
$$
In practice, the rotation matrix $\displaystyle \mR$ and the scaling matrix $\displaystyle \mS$ are represented as a rotation quaternion $\displaystyle \vq \in \sR^4$ and a scaling factor $\displaystyle \vs \in \sR^3$, respectively. In summary, for the $i$-th Gaussian, the optimizable attributes are given by $\displaystyle \hat{\boldsymbol{\Theta}}_i = \{\vx_i, \vf_i, \vs_i, \vq_i\}$. We then optimize each Gaussian by minimizing the error between the rendered results and the radar profiles from the origin 3D radar data. 


\subsubsection{Bidirection Reconstruction Scheme.} 
\label{sec:bidirection}

% Disabling the adaptive density control in Gaussian Splatting severely degrades reconstruction quality, causing under-reconstruction in regions that emerge in later frames which hampers the accurate capture of cloud dynamics in radar sequences. To address this, we introduce a bidirectional reconstruction strategy that achieves similar functionality to the adaptive density control strategy without the requirement of adding or removing Gaussians. To achieve a spatiotemporally coherent Gaussian representation, it is essential to accurately capture frame-by-frame motion trends from the raw data while also reconstructing the growth and dissipation of clouds from high-dynamic radar sequences.
Adding or removing Gaussians during reconstruction disrupts spatiotemporal consistency, making it impossible to track Gaussians throughout the sequence. However, simply disabling adaptive density control significantly degrades reconstruction quality, impairing the capture of cloud dynamics in radar sequences. To address these issues, we propose a bidirectional reconstruction strategy that preserves spatiotemporal consistency without altering the Gaussian set. This approach effectively captures frame-by-frame motion trends from raw data while accurately modeling the growth and dissipation of clouds in highly dynamic radar sequences.

The bidirectional reconstruction strategy that consists of two stages: backward reconstruction and forward reconstruction. In the \textbf{backward reconstruction stage}, we pre-reconstruct from timestamp $T$ to timestamp $0$. Only the position of the Gaussians are optimized using the \textbf{local detail and global trend constraints}. Components that persist or gradually dissipate over time are preserved, and each Gaussian's position is updated according to cloud motion at each timestamp. By the time backward reconstruction reaches timestamp $0$, it effectively incorporates information from future frames, bringing this information back to the initial frame.
In the \textbf{forward reconstruction stage}, we iteratively reconstruct each frame from timestamp $0$ to timestamp $T$. We uniformly sample from the Gaussians obtained during the backward reconstruction and the original 3D data at timestamp $0$ to initialize a new set of Gaussians. In the coarse reconstruction stage, we optimize the positions using the local detail and global constraints, then optimize all Gaussian parameters by additional reconstruction loss.

% To obtain a well-spatiotemporal coherent representation, it necessitates ensuring that each Gaussian maintains spatiotemporal consistency. This implies that each Gaussian must accurately follow the motion of the portion it is reconstructing. Furthermore, reconstructing high-dynamic radar sequences requires adjusting all attributes of the Gaussians. The radar sequences exhibit complex and diverse dynamics, including motion, deformation, growth, and dissipation. When multiple types of changes occur simultaneously, it becomes unclear which parameters should be adjusted to effectively minimize reconstruction error.

Achieving a spatiotemporally coherent representation requires each Gaussian to precisely track the motion of the region it reconstructs. Besides, radar sequences often exhibit complex and diverse dynamics, including motion, deformation, growth, and dissipation. Reconstructing such high-dynamic sequences necessitates simultaneous adjustments to all attributes of the Gaussians. Identifying which parameters to modify for optimal error minimization becomes a significant challenge.
%
Thus, the prediction based on 3D Gaussians introduces new challenges for the optimization process during the reconstruction phase. To address this, we propose a local detail constraint (3D flow constraint) and a global trend constraint (global energy constraint), which introduce 3D motion priors and optimal position distribution priors estimated from the original 3D radar data. 
% Moreover, the adaptive density control strategy in the original Gaussian Splatting disrupt the spatiotemporal consistency of the Gaussians and lead to a variable number of Gaussians. A simplistic approach might suggest omitting the densification and pruning processes. However, we argue that such simplifications significantly degrade reconstruction quality, as evidenced by empirical results shown in Table \ref{tab_rec}. To mitigate these issues, we propose a bidirectional reconstruction strategy.

\textbf{Local Detail Constraint.} 
To introduce 3D motion priors from original 3D radar data and achieving a spatiotemporally coherent for each Gaussian, we introduce a 3D optical flow constraint, as shown in Fig. \ref{fig:rec}(b). First, we utilize a pre-trained 2D optical flow model, RAFT \cite{raft}, to estimate the motion of radar observations across the xoy, xoz, and yoz planes. These 2D flows are then fused into a pseudo 3D flow within an xyz grid to approximate motion in 3D space. We take the 3D flow grid closest to each Gaussian as the reference flow, and constrain the distance between the displacement of the Gaussians $\displaystyle \Delta \vx \in \sR^3$ and the reference 3D flow $\Delta_{flow}$ to ensure that the Gaussians move in tandem with the corresponding cloud formations:
$$
\displaystyle \mathcal{L}_{local} = \Vert \Delta \vx - \Delta_{flow} \Vert ^2.
$$
However, since the 3D optical flow is estimated and fused from 2D images, it may not always be accurate. Moreover, employing an optical flow model pre-trained on real-world image sequences to estimate the flow of radar sequences could introduce inductive bias, which may accumulate over iterations. As a result, while the 3D flow constraint helps ensure that the Gaussians generally align with the direction of actual cloud motion, it cannot guarantee that each Gaussian is optimally positioned in every iteration.

\textbf{Global Trend Constraint.} 
From the original sparse 3D radar observation data, we can infer an optimal distribution of a group of Gaussians that accurately reconstructs the 3D data. Intuitively, the probability of Gaussians existing should be high in non-null regions of the 3D radar data, gradually decreasing to zero from the boundary areas between non-null and null regions to fully null regions, as shown in Fig. \ref{fig:rec}(b). We approximate the unnormalized optimal distribution of Gaussians using a smoothed function with threshold suppression. Specifically, we apply a Gaussian kernel-based smooth convolution to the original radar data and clamp values that exceed a predefined threshold $\tau$, the estimated unnormalized optimal distribution $\text{P}(\mF)$ can be expressed mathematically as follows:
$$
\displaystyle \text{P}(\mF) = \min (\mF \ast \mathcal{K}(\cdot), \tau).
$$
Here, $\mF$ is the original radar data, $\ast$ denotes the convolution operator and $\mathcal{K}(\cdot)$ is the Gaussian kernel.
For the current group of 3D Gaussians, we can derive their corresponding position probability densities by reusing the differentiable renderer. Specifically, each 3D Gaussian is treated as a normal distribution centered at its current location, and reuse the renderer to obtain the position probability densities of the current Gaussians. Then, utilizing the energy function in energy-based learning \cite{ebm}, we can define a simple yet effective method to measure the similarity between two distributions:
$$
\mathcal{L}_{global} = \Vert \mathcal{R}_\text{P} (\mathcal{G}) - \text{P}(\mF) \Vert ^2 .
$$
Here, $\mathcal{R}__\text{P}$ denotes the reused randerer for distribution estimation and $\mathcal{R}_\text{P} (\mathcal{G})$ is the estimated distribution based on current Gaussians $\mathcal{G}$.
By minimizing this energy function, we can ensure that each Gaussian in the current set is positioned at a more optimal initial location.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\columnwidth]{fig_mambagru.pdf}
\end{center}
\caption{\textbf{The overall architecture of the GauMamba.} 
%
(a) Main architecture of GauMamba which consists of multiple stacked MambaGRU Block.
%
(b) Detailed architecture design of MambaGRU.}
\label{fig_mamba}
\end{figure}


\subsection{Memory Augmented Mamba Predictive Model}
\label{sec:gaumamba}

We now introduce our proposed Mamba-based predicion model termed GauMamba, incorporated with the spatiotemporal coherent Gaussian representation for high-dynamic 3D radar sequence prediction. After reconstructing the raw radar data into a sequence of Gaussian groups, we aim to train the predictive model based on the 3D Gaussian representations.

Thanks to STC-GS, the prediction of 3D radar sequence can be transfromed into the prediction of variation of Gaussian group along the temporal dimension. We leverage the standard Mamba \cite{mamba, mamba2} and incorporate GRU memory mechanism to predict parameters of future Gaussians. 
%
The former can effectively handle a large number of Gaussian tokens as input, while the latter can effectively learn the spatiotemporal relationships between adjacent Gaussian Groups.
%
As shown in Figure \ref{fig_mamba}(a), the proposed GauMamba consists of multiple stacked MambaGRU blocks. The $l$-th layer MambaGRU block at timestamp $t$ takes two inputs: the output embedding from the previous layer $\mathcal{C}^{l-1}_t$ and the hidden state used for memory from the previous timestamp $\mathcal{H}^l_{t-1}$. The block then outputs the updated Gaussian embeddings $\mathcal{C}^l_t$, along with the updated memory hidden state $\mathcal{H}^l_t$ to be passed to the next timestamp.



\textbf{Training objective.}
In our 3D Gaussians prediction training, we parameterize our model $f_\theta$ to predict DiffGaussian $\Delta \mathcal{G}_t = \mathcal{G}_t - \mathcal{G}_0$ for $t = 1, 2, \cdots, T_{in} + T_{out}$, using: 
$$
\mathcal{L}_{pred} = \sum_{t=1}^{T_{in+1}}\Vert f_\theta(\mathcal{G}_{t-1}, \mathcal{H}_{t-1}) - \mathcal{G}_t \Vert ^ 2 + \sum_{t=T_{in+2}}^{T_{out}}\Vert f_\theta(\mathcal{\hat{G}}_{t-1}, \mathcal{H}_{t-1}) - \mathcal{G}_t \Vert ^ 2
$$
Here, $T_{in}$ is the length of given observation sequence, $T_{out}$ is the length of predicted sequence, and $\mathcal{\hat{G}}_{t-1} = \mathcal{G}_0 \oplus \Delta \mathcal{\hat{G}}_{t-1}$. 
Notably, since we only have access to true states of the first $T_{in}$ Gaussian Groups, there is no true value for timestamps beyond $T_{in+1}$ to be used as input for prediction. Thus, for $t \leq T_{in} + 2$, $\mathcal{\hat{G}}_{t-1}$ is estimated based on the predicted result $\Delta \mathcal{\hat{G}}_{t-1}$ from the previous timestamp and the initial state $\mathcal{G}_0$. Specifically, we tokenize $\mathcal{G}_0$ to obtain its embedding, and then combine it with the embedding of $\Delta \mathcal{\hat{G}}_{t-1}$, which is produced by the final MambaGRU block from the previous time step. This summation results in the estimated embedding for timestamp $t-1$.

\section{Experiment}
% This section presents our datasets, comparative results and ablation studies.
%
More details of datasets, implementation, metrics, and experimental setup are in Appendix \ref{append:exp}.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-0.8cm}
    \includegraphics[width=0.5\textwidth]{fig_rec.pdf}
    \caption{Qualitative results of reconstruction.}
    \label{fig_rec}
\end{wrapfigure}

\subsection{Dataset}\label{exp:data}
\textcolor{edit}{
The datasets used in this study include NEXRAD and MOSAIC. \textbf{NEXRAD}\footnote{https://huggingface.co/datasets/Ziyeeee/3D-NEXRAD} comprises radar observations of severe storms in the U.S., with 3D reflectivity data sampled at 5-minute intervals. Seven radar features, such as reflectivity, azimuthal shear, differential reflectivity, and so on, are included. \textbf{MOSAIC} records radar observations of storms in Guangdong, China, with 6-minute intervals, focusing solely on intensity data of radar echoes. Both datasets are preprocessed to ensure consistent vertical spacing and are divided into training, validation, and test sets. The prediction task involves forecasting up to 20 future frames based on 5 observed frames. For further information, please refer to the supplementary material \ref{supp:dataset}.
}

\subsection{Experiment Results}\label{exp:res}

\textbf{Resconstruction.}
First, we verify the effectiveness of our proposed re-represent method on the high-dynamic 3D radar dataset and compare it with other state-of-the-art methods designed for dynamic scene reconstruction. Table \ref{tab_rec} presents the quantitative evaluation result on reconstructing hight-dynamic 3D radar sequences. 
%
\textcolor{edit}{
The competing methods adhere to their original configurations, with certain parameters fixed during optimization. In contrast, our approach allows all parameters to be freely optimized, posing a significantly more challenging task. Despite this, our method achieves superior performance across all metrics.
%
As illustrated in Figure \ref{fig_rec}, our proposed method preserves more details and maintains consistent accuracy throughout the entire sequence. 
}

%Our approach outperforms existing methods on all metrics and achieves a $10\times$ accuracy improvement in MAE score. And more or less all these existing models encounter the convergence difficulties mentioned in the previous analysis. resulting in unusable results.



\begin{table}[t]
\vspace{-0.6cm}
\setlength\tabcolsep{5pt}
\caption{Comparision of reconstruction in NEXRAD}
\label{tab_rec}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l | c c c c c}
\toprule
% \hline
Model & MAE_{\times10}^\downarrow & PSNR(dB)^\uparrow & SSIM^\uparrow & LPIPS^\downarrow & \textcolor{edit}{LPIPS\textsubscript{Radar}}^\downarrow\\
\midrule

% 3DGStream^\dag & 0.0210 & 14.451 & 0.818 & 0.228 \\
% 4DGS^\dag & 0.0331 & 19.178 & 0.172 & 0.317 \\
% Deform-3DGS^\dag & 0.0115 & 26.218 & 0.543 & 0.194 \\

3DGStream \cite{3dgstream} & 0.019 & 38.133 & 0.954 & 0.091 & 0.902 \\
4DGS \cite{4dgs} & 0.028 & 35.731 & 0.933 & 0.135 & 0.623 \\
Deform-3DGS \cite{deformable_gaussian} & 0.029 & 35.027 & 0.931 & 0.141 & 0.578 \\

\midrule
Ours & \textbf{0.014} & \textbf{40.262} & \textbf{0.970} & \textbf{0.057} & \textbf{0.123} \\

% Model & MAE_{\times 100}^\downarrow & PSNR(dB)^\uparrow & SSIM^\uparrow & LPIPS^\downarrow \\
% \midrule

% 3DGStream \cite{3dgstream} & 0.198 & 38.133 & 0.954 & 0.091 \\
% 4D-GS \cite{4dgs} & 0.280 & 35.731 & 0.933 & 0.135 \\
% Deform 3D-GS \cite{deformable_gaussian} & 0.296 & 35.027 & 0.931 & 0.141 \\

% 3DGStream\dag & 0.363 & 31.338 & 0.911 & 0.207 \\
% 4D-GS\dag & 0.285 & 35.753 & 0.933 & 0.136 \\
% Deform 3D-GS\dag & 0.252 & 36.428 & 0.938 & 0.134 \\

% \midrule
% Ours & \textbf{0.147} & \textbf{40.262} & \textbf{0.970} & \textbf{0.057} \\

\bottomrule
\end{tabular}}
\end{center}
\end{table}

\begin{wrapfigure}{r}{0.4\textwidth}  % 图片在右侧，宽度占0.4列
    \centering
    \vspace{-0.2cm}
    \includegraphics[width=0.4\textwidth]{fig_memory.pdf}
    \caption{Memory usage of different methods with various input resolutions.}
    \label{fig_gpu_usage}
\end{wrapfigure}

\textbf{Prediction.}
The large and cumbersome 3D tensors presents a significant challenge in extending radar echo predictions from 2D to 3D, thereby limiting the scalability of models for high-resolution predictions.
%
Fig. \ref{fig_gpu_usage} illustrates the GPU memory requirements for feature extraction and prediction using methods based on 3D data, including ConvGRU, PhyDNet, SimVP, and DiffCast, alongside our proposed feature-dense 3D Gaussian prediction approach. Scaling these radar echo prediction methods, which use raw 3D data as input, to high-resolution training and deployment is both difficult and impractical, as the time and space complexity scales quadratically, \textit{i.e.}, $O(\text{N}^2)$, with respect to the horizontal resolution of $\text{N} \times \text{N}$. 
%
\textcolor{edit}{
In contrast, the memory usage of our proposed Gaussian-based method is independent of the input or output resolution, scaling linearly only with the number of Gaussian primitives. As shown in Fig. \ref{fig_gpu_usage}, the memory usage of our GauMamba method is evaluated with a fixed number of Gaussian primitives tailored for high-resolution images, balancing computational efficiency and reconstruction precision. When predict at lower resolutions, e.g., $\text{N} \le 256$, fewer Gaussian primitives are required, leading to reduced memory consumption. 
%We recommend that the number of Gaussian primitives be reduced quadratically as the horizontal resolution decreases.
}

With the available computational resources of 4 A100 80G GPUs, methods that use raw 3D data as input cannot be effectively trained at a horizontal resolution of $512 \times 512$. To facilitate a fair comparison with our proposed GauMamba, we trained ConvGRU, PhyDNet, SimVP, and DiffCast at a horizontal resolution of $128 \times 128$, and trained GauMamba at $512 \times 512$, while maintaining a consistent vertical resolution. Then, we upsampled the predictions from ConvGRU, PhyDNet, SimVP, and DiffCast to $512 \times 512$ for a fair evaluation.


The quantitative performance metrics of two 3D radar datasets are presented in Table \ref{tab_pred_gb} and \ref{tab_pred_us} respectively. In general, our proposed GauMamba with STC-GS as input outperforms all existing models based on raw 3D radar data, even if predicting in an actual higher resolution. The GauMamba achieves $12.1\%$, $69.0\%$ and $4.8\%$, $101.1\%$ improvements in CSI-20 and CSI-30 with $4\times4$ pooling in MOSAIC and NEXRAD dataset, respectively. We also present the qualitative results in Fig. \ref{fig_pred}. The prediction results of our proposed method contain more details and the high values of the significant regions are preserved. This is because the re-represented Gaussian with significant features more prioritized to be tracked and predicted in order to minimize our optimization objective. 
%
\textcolor{edit}{
In addition, the results of Mamba and our GauMamba demonstrate that the approaches and experiments developed within our proposed framework, which redefine the 3D prediction task by first re-representing the sequences with 3D Gaussians and then predicting their future evolution,  significantly outperforms traditional methods, highlighting the effectiveness and robustness of our reformulation.
}


\begin{figure}[t]
\vspace{-0.4cm}
\begin{center}
\includegraphics[width=0.95\columnwidth]{fig_pred.pdf}
\end{center}
\caption{\textbf{Qualitative results of 3D radar prediction.}
%
(a) The prediction results in MOSAIC dataset.
%
(b) The prediction results in NEXRAD dataset.
%
Detail comparisons are plotted in the last column, respectively. Our proposed framework can more effectively predict local changes in detail, which may determine specific weather change trends.
}
\label{fig_pred}
\end{figure}


\begin{table}[t]
\setlength\tabcolsep{5pt}
\caption{Experiment results in MOSAIC}
\label{tab_pred_gb}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l | c c c c c c c c}
\toprule

Model & MAE^\downarrow & SSIM^\uparrow & LPIPS^\downarrow & \textcolor{edit}{LPIPS\textsubscript{Radar}}^\downarrow & CSI-20_{\text{Pool4}}^\uparrow & CSI-30_{\text{Pool4}}^\uparrow & CSI-40_{\text{Pool4}}^\uparrow \\
\midrule
ConvGRU & 1.728 & 0.621 & 0.303 & 4.837 & - & - & - \\
PhyDNet & 0.910 & 0.810 & 0.244 & 1.451 & \underline{0.294} & 0.108 & 0.002 \\
SimVP & 0.890 & 0.835 & 0.270 & 3.516 & 0.264 & 0.075 & - \\
DiffCast & 1.878 & 0.355 & 0.433 & 2.216 & 0.305 & 0.126 & 0.006 \\
\midrule
\textcolor{edit}{Mamba} & \underline{0.750} & \underline{0.894} & \underline{0.164} & \underline{0.777} & 0.293 & \underline{0.166} & \underline{0.055} \\
GauMamba & \textbf{0.714} & \textbf{0.897} & \textbf{0.157} & \textbf{0.741} & \textbf{0.342} & \textbf{0.213} & \textbf{0.062} \\
\bottomrule

% Model & ME^{\rightarrow 0} & MAE^\downarrow & SSIM^\uparrow & LPIPS^\downarrow & LPIPS_{\text{Radar}}^\downarrow & CSI-20_{\text{Pool4}}^\uparrow & CSI-30_{\text{Pool4}}^\uparrow & CSI-40_{\text{Pool4}}^\uparrow \\
% \midrule
% ConvGRU & -0.122 & 1.728 & 0.621 & 0.303 & 4.837 & - & - & - \\
% PhyDNet & 0.151 & 0.910 & 0.810 & 0.244 & 1.451 & \underline{0.294} & 0.108 & 0.002 \\
% SimVP & \underline{0.105} & 0.890 & 0.835 & 0.270 & 3.516 & 0.264 & 0.075 & - \\
% DiffCast & 1.092 & 1.878 & 0.355 & 0.433 & 2.216 & 0.305 & 0.126 & 0.006 \\
% \midrule
% Mamba & -0.367 & \underline{0.750} & \underline{0.894} & \underline{0.164} & \underline{0.777} & 0.293 & \underline{0.166} & \underline{0.055} \\
% GauMamba & \textbf{-0.103} & \textbf{0.714} & \textbf{0.897} & \textbf{0.157} & \textbf{0.741} & \textbf{0.342} & \textbf{0.213} & \textbf{0.062} \\
% \bottomrule

\end{tabular}}
\end{center}
\end{table}


\begin{table}[t]
\vspace{-0.6cm}
\setlength\tabcolsep{5pt}
\caption{Experiment results in NEXRAD}
\label{tab_pred_us}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l | c c c c c c c c}
\toprule
Model & MAE^\downarrow & SSIM^\uparrow & LPIPS^\downarrow & \textcolor{edit}{LPIPS\textsubscript{Radar}}^\downarrow & CSI-20_{\text{Pool4}}^\uparrow & CSI-30_{\text{Pool4}}^\uparrow & CSI-40_{\text{Pool4}}^\uparrow \\
\midrule
ConvGRU & 0.006 & 0.819 & 0.205 & 1.621 & 0.306 & - & - \\
PhyDNet & 0.017 & 0.373 & 0.320 & 2.058 & \underline{0.311} & 0.089 & 0.002 \\
SimVP & 0.066 & 0.379 & 0.481 & 2.925 & 0.085 & 0.088 & 0.018 \\
DiffCast & 0.157 & 0.004 & 0.932 & 4.057 & 0.049 & 0.021 & 0.021 \\
\midrule
\textcolor{edit}{Mamba} & \underline{0.004} & \underline{0.899} & \underline{0.129} & \underline{0.699} & 0.309 & \underline{0.165} & \underline{0.074} \\
GauMamba & \textbf{0.003} & \textbf{0.900} & \textbf{0.126} & \textbf{0.665} & \textbf{0.326} & \textbf{0.179} & \textbf{0.078} \\

% Model & ME^{\rightarrow 0} & MAE^\downarrow & SSIM^\uparrow & LPIPS^\downarrow & LPIPS_{\text{Radar}}^\downarrow & CSI-20_{\text{Pool4}}^\uparrow & CSI-30_{\text{Pool4}}^\uparrow & CSI-40_{\text{Pool4}}^\uparrow \\
% \midrule
% ConvGRU & \underline{0.0008} & 0.006 & 0.819 & 0.205 & 1.621 & 0.306 & - & - \\
% PhyDNet & 0.0139 & 0.017 & 0.373 & 0.320 & 2.058 & \underline{0.311} & 0.089 & 0.002 \\
% SimVP & 0.0230 & 0.066 & 0.379 & 0.481 & 2.925 & 0.085 & 0.088 & 0.018 \\
% DiffCast & 0.1525 & 0.157 & 0.004 & 0.932 & 4.057 & 0.049 & 0.021 & 0.021 \\
% \midrule
% Mamba & -0.0016 & \underline{0.004} & \underline{0.899} & \underline{0.129} & \underline{0.699} & 0.309 & \underline{0.165} & \underline{0.074} \\
% GauMamba & \textbf{0.0006} & \textbf{0.003} & \textbf{0.900} & \textbf{0.126} & \textbf{0.665} & \textbf{0.326} & \textbf{0.179} & \textbf{0.078} \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

\subsection{Ablation Study}\label{exp:abla}

\subsubsection{Reconstruction}

\textbf{Reconstruction w/o local details.}
The flow constraint introduces a pseudo-3D motion flow that guides the reconstruction process. Its absence leads to dynamic Gaussians that struggle to accurately track the movement of the reconstructed elements. This deficiency results in a $32.6\%$ increase in the MAE score.
%
\textbf{Reconstruction w/o global trends.}
The energy-based constraint plays a crucial role during the initial stages by ensuring that the set of Gaussians achieves a more appropriate coarse-level spatial distribution. This constraint also helps mitigate error accumulation that can arise from inaccurate flow estimations. After removing the energy-based iteration phase, we keep the total number of iterations constant.
%
\textbf{Reconstruction w/o bidirectional reconstruction.}
Before the forward reconstruction phase, a backward reconstruction is conducted to optimize the spatial distribution of the Gaussians. Omitting this reverse pre-reconstruction stage results in the loss of the ability to propagate information about areas that emerge in the future back to the first frame. Consequently, the initialization of Gaussian positions becomes inadequate, leading to a significant decrease in accuracy.

\begin{table}[t]
\caption{Ablation Study in Reconstruction}
\label{exp-res}
\begin{center}
\begin{tabular}{l | c c c c}
\toprule
Method & MAE^\downarrow_{\times 100} & PSNR(dB)^\uparrow & SSIM^\uparrow & LPIPS^\downarrow \\
\midrule
Reconstruct w/o flow (local details) & 1.947 & 37.521 & 0.953 & 0.089 \\
Reconstruct w/o energy (global trends) & 1.497 & 40.049 & 0.969 & 0.059 \\
Reconstruct w/o bi-direction & 3.245 & 32,746 & 0.921 & 0.152 \\
\midrule
Full Model & \textbf{1.468} & \textbf{40.262} & \textbf{0.970} & \textbf{0.057} \\
% \textbf{-0.031} & \underline{0.716} & 0.854 & \underline{0.862} & \textbf{0.412} & \textbf{0.366} & \underline{0.338} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}[!t]
\caption{Ablation Study in Prediction}
\setlength\tabcolsep{5pt}
\label{exp-res}
\begin{center}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{l | c c c c c c c}
\toprule
GauMamba & ME^{\rightarrow 0} & MAE^\downarrow & SSIM^\uparrow & LPIPS^\downarrow & CSI-20_{\text{Pool4}}^\uparrow & CSI-30_{\text{Pool4}}^\uparrow & CSI-40_{\text{Pool4}}^\uparrow \\
\midrule
 w/o Memory & -0.445 & 0.858 & 0.883 & 0.170 & 0.224 & 0.122 & 0.035 \\
 w/o GRU & -0.374 & 0.743 & 0.895 & 0.161 & 0.289 & 0.165 & 0.052 \\
 w/o Sort & -0.532 & 0.880 & 0.883 & 0.183 & 0.179 & 0.085 & 0.025 \\
\midrule
Full Model & \textbf{-0.103} & \textbf{0.714} & \textbf{0.897} & \textbf{0.157} & \textbf{0.342} & \textbf{0.213} & \textbf{0.062} \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}



\subsubsection{Prediction}

\textbf{GauMamba w/o Mermory.} Removing the memory mechanism results in a vanilla Mamba, which is not suitable for the current prediction task. To evaluate the effectiveness of the memory mechanism, we incorporate DLinear \cite{dlinear}, a simple yet effective prediction method that does not rely on memory. Specifically, we split a vanilla Mamba into two symmetric parts: an encoder and a decoder. The encoder generates embeddings from historical observations, which are subsequently fed into DLinear to predict future embeddings. The decoder then produces the predicted parameters of Gaussian. However, Mamba with DLinear does not surpass our method with a $20.2\%$ increment of MAE score. Our memory mechanism can be updated temporally while considering global embeddings. In contrast, DLinear infers future embeddings based solely on local embeddings.
%
\textbf{GauMamba w/o GRU.} We removed the reset gate and update gate from the GRU, while retaining the memory mechanism. The hidden states, which store historical information, are directly concatenated with the current Gaussian embeddings. In this configuration, the MAE score increases by $4.1\%$, which is lower than that of GauMamba w/o Mermory, highlighting the effectiveness of the memory mechanism. However, removing GRU reduces the flexibility in updating the memory.
%
\textbf{GauMamba w/o Sort.} Morton sorting aggregates spatially adjacent points in the 1D sequence. Without Morton sorting, the Gaussians are input into GauMamba in their original unsorted order, as determined during the reconstruction stage. Although each Gaussian maintains its position consistently across frames, a significant performance degradation is observed. This suggests that spatially unstructured sequences negatively impact the feature extraction capabilities.


\section{Conclusion}
In this work, we introduced a novel framework for 3D radar sequence prediction in weather forecasting, addressing the limitations of current 2D spatial prediction methods. 
%
Our proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) and GauMamba effectively capture dynamic radar signals while maintaining high efficiency and accuracy. 
%
Experimental results demonstrate that STC-GS achieves superior reconstruction accuracy compared to existing 3D representations, while GauMamba outperforms state-of-the-art models in predicting dynamic weather conditions. 
%
\textcolor{edit}{
In general, we provide a general solution framework for 3D prediction, paving the way for future advancements in this domain. 
%
However, the current scope of our approach is specialized, focusing primarily on radar-based weather nowcasting, with limited exploration of broader applications. We are actively working to enhance its generalization and simplify the pipeline for broader applicability to dynamic 3D scenarios.
}

\bibliography{iclr2025}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix
\section{3D Radar Cross-Section Rendering}\label{supp:render}
3D Radar Cross-Section Rendering aims to project 3D Gaussians from the world coordinate system onto the virtual camera imaging plane to render specific cross-section results. First, we apply a transformation matrix to convert the 3D Gaussians from the world coordinates to the camera coordinates. Then, we discard any Gaussians that exceed a predefined threshold distance from the virtual imaging plane. Finally, the remaining Gaussians are rendered in order of their distances to the imaging plane, from nearest to farthest. The 3D Gaussian's contribution to the pixel can be calculated by:
$$
\displaystyle F = \sum_{i \in N_p} \vf_i \cdot e^{-\frac{1}{2}(\vx_p - \vx_g_i)^T \mSigma_i^{-1} (\vx_p - \vx_g_i)}
$$
where $N_p$ is the number of Gaussians overlapping with the given pixel, $\displaystyle \vf_i$ is the radar features of $i$-th Gaussian, $\displaystyle \vx_p$ and $\displaystyle \vx_g_i$ are the centered positions of the pixel and $i$-th gaussian respectively, $\displaystyle \mSigma_i^{-1}$ is the 3D covariance matrix of $i$-th Gaussian. The rasterization strategy, similar to the origin 3DGS, is fully differentiable, enabling to take full use of the GPU acceleration framework to optimize parameters of Gaussians using stochastic gradient descent. 





\section{Memory Mechanism.}
The core of the Gated Recurrent Unit (GRU) is based on two primary gates: the reset gate and the update gate, which control the flow of information. Unlike LSTM, GRU combines the forget and input gates into a single update gate, simplifying the architecture. When a new input $\mathcal{C}_t$ is received at time step $t$, the reset gate $r_t$ controls how much of the previous hidden state $\mathcal{H}_{t-1}$ should be ignored when computing the candidate hidden state $\mathcal{\hat{H}}_t$. A smaller reset gate value causes the model to "forget" parts of the previous hidden state, effectively resetting the memory. The update gate $z_t$ determines how much of the previous hidden state $\mathcal{H}_{t-1}$ should be retained, controlling the balance between preserving past information and incorporating new data. the update gate $z_t$ controls the final hidden state $\mathcal{H}_t$, which is a combination of the previous hidden state $\mathcal{H}_{t-1}$ and the candidate hidden state $\mathcal{\hat{H}}_t$.
The advantages of GRU's gating mechanism is that it allows efficient control over information flow and help to mitigate the vanishing gradient problem by maintaining long-term dependencies more effectively. 

As illustrated in Fig.~\ref{fig_mamba}, incorporating Mamba and GRU preserves the linear-time complexity of Mamba and introduces a long-range memory mechanism that allows mamba-based models to predict the next Gaussians based on the previous Gaussians. The key equations are shown in below:
$$ \displaystyle
r_t = \sigma(\mW_{cr}\mathcal{C}_t + \mW_{hr}\mathcal{H}_{t-1} + \vb_r)
$$
$$ \displaystyle
z_t = \sigma(\mW_{cz}\mathcal{C}_t + \mW_{hz}\mathcal{H}_{t-1} + \vb_z)
$$
$$ \displaystyle
\mathcal{\hat{H}}_t = \mW_{ch}\mathcal{C}_t + \mW_{hh} (r_t \circ \mathcal{H}_{t-1}) + \vb_h
$$
$$
\mathcal{\hat{C}}_t = \text{BiMamba}(\mathcal{\hat{H}}_t)
$$
$$
\mathcal{H}_t = z_t \circ \mathcal{H}_{t-1} + (1 - z) \circ \mathcal{\hat{C}}_t
$$



\section{Experiment Setting}\label{append:exp}

\subsection{Dataset}\label{supp:dataset}
\textbf{NEXRAD:} The 3D gridded radar reflectivity data used in this study were collected by the U.S. NEXRAD WSR-88D radar network. These data are sourced from the ds841.6 dataset product available through the National Center for Atmospheric Research (NCAR) Research Data Archive \cite{dsd841001}. Due to constraints on storage and GPU resources, radar observations of severe storm events in 2022 with a longitude and latitude grid size ranging from 512 to 1024 are selected from different geographical coverage.


The selected storm events are observed at 5-minute intervals with a horizontal resolution of approximately 0.021 degrees. A total of 6255 3D radar observation are considered in this study. To preprocess the raw radar data, a quality control procedure is applied, following the methodology outlined in \cite{alg-gridrad}. Subsequently, a central 512 $\times$ 512 longitude/latitude grid is cropped from each event observation. In the vertical dimension, there are 28 levels, spanning from 0.5 km to 7 km with 0.5 km intervals, and from 7 km to 22 km with 1 km intervals. To ensure consistent vertical spacing of features, we interpolate the original data to obtain 44 vertical layers with 0.5 km intervals spanning from 0.5 km to 22 km.
The 3D radar data from these storm events include seven channels: (1) $Z_H$, the horizontal reflectivity factor, which indicates the intensity of radar returns from precipitation; (2) $SW$, the spectrum width, representing the variability of Doppler velocities within the radar pulse volume; (3) $AzShr$, the azimuthal shear, a measure of wind shear in the horizontal plane often used to detect rotation within storm systems; (4) $Div$, the divergence, which reflects the horizontal divergence or convergence of wind fields; (5) $Z_{DR}$, the differential reflectivity, used to differentiate precipitation types by comparing horizontal and vertical polarizations; (6) $K_{DP}$, the specific differential phase, providing information on phase shifts of the radar signal, useful for estimating rainfall rates; and (7) $r_{HV}$, the correlation coefficient between horizontal and vertical polarizations, which assesses the uniformity of precipitation particles and identifies non-meteorological targets.

Given the objective of predicting up to 20 future frames (100 minutes) based on 5 observed frames (25 minutes), we sample 25 continuous frames with a stride of 10 from each event. These sequences are then divided into training, validation, and test sets in a 9:0.5:0.5 ratio.

\textbf{MOSAIC} The MOSAIC dataset records radar echoes over several years within Guangdong, China, with a time interval of 6 minutes between consecutive frames. To ensure efficient training while considering GPU and storage limitations, we excluded radar data that did not capture meteorological events, focusing on severe storm events in 2022. This resulted in a total of 24,542 radar observations. The raw data have a horizontal resolution of 880 $\times$ 1050 pixels, with 21 vertical layers ranging from 0.5 km to 6 km with 0.5 km intervals, from 6 km to 10 km with 1 km intervals, and additional layers at 12 km, 14 km, 15.5 km, 17 km, and 19 km. To achieve consistent vertical spacing of features, we interpolate the original data to obtain 38 vertical layers with 0.5 km intervals spanning from 0.5 km to 19 km. The central 768 $\times$ 1024 region is cropped and downsampled to 384 $\times$ 512, resulting in radar observation data of dimensions $38 \times 384 \times 512$. Unlike the NEXRAD dataset, MOSAIC includes only the intensity data of radar echoes. For the task of predicting up to 20 future frames (120 minutes) based on 5 observed frames (30 minutes), we sample 25 continuous frames with a stride of 20 from each event. These sequences are then divided into training, validation, and test sets in a 9:0.5:0.5 ratio.

\subsection{Implementation Details}
We define the prediction task as predicting 20 frames future frames based on 5 initial frames, following \cite{diffcast}. For reconstruction, we random sample $10\%$ points from non-null regions to initialize the Gaussians. To maintain a constant total number of Gaussians, additional points are randomly sampled from the null regions, bringing the final count to 49,152 Gaussians. During the reverse pre-reconstruction stage, we perform 5,000 iterations to adjust the positions of the Gaussians, applying 3D flow and energy constraints. In the forward reconstruction stage, we first optimize the positions of the Gaussians in the same manner for the first 5,000 iterations. Afterward, all Gaussian parameters are optimized, incorporating the 3D flow, energy constraints, and reconstruction loss. The initial learning rate is set to 0.002, which is gradually reduced to 0.0002 by the end of training.
For the prediction task, we train our framework for 50 epochs using the Adam optimizer with a learning rate of 0.0005. For baseline methods, their 2D operators are extended to 3D to handle 3D radar prediction tasks, with their configurations adjusted accordingly for different datasets. All experiments are conducted under the same settings using 4 A100 GPUs.

\subsection{Metrics}\label{supp:metrics}
To evaluate the accuracy of predictions, we calculate the Mean Error (ME) and Mean Absolute Error (MAE) to assess the overall numerical discrepancy between the predicted results and the ground truth. A ME value closer to zero indicates better accuracy; a ME greater than zero signifies that the predictions generally exceed the ground truth, while a ME less than zero indicates the opposite.
To evaluate the visual quality of the predictions, we employ the Structural Similarity Index (SSIM)\cite{ssim}, Peak Signal-to-Noise Ratio (PSNR) and Learned Perceptual Image Patch Similarity (LPIPS)\cite{lpips}. \textcolor{edit}{Here, we use the pretrained AlexNet as the evaluator of LPIPS. Besides, we pretrained a BiGAN model on radar data in a self-supervised manner. The LPIPS score calculated with the encoder of BiGAN is marked as LPIPS\textsubscript{Radar}.
%
The results in Table \ref{tab_pred_gb} and \ref{tab_pred_us} demonstrate that LPIPS\textsubscript{Radar} is well-aligned with the original LPIPS results. More importantly, it highlights perceptual differences that were undetected by the original LPIPS and robust to noise. In Table \ref{tab_pred_gb}, the scores for ConvGRU and DiffCast show discrepancies between LPIPS and LPIPS\textsubscript{Radar}. Referring to Figure \ref{fig_pred} left, it can be clearly observed that ConvGRU fails to predict the next few frames accurately, instead providing a smoothed average result. In contrast, DiffCast produces results that are closer to the ground truth but with some noise. The higher LPIPS score for DiffCast indicates that LPIPS lacks robustness to noise in radar data, while LPIPS\textsubscript{Radar} more accurately reflects the perceptual differences between the two methods.} 

In addition, the Critical Success Index (CSI) is used to quantify the degree of pixel-wise agreement between the prediction and the ground truth. CSI is defined as $\frac{\text{Hits}}{\text{Hits + Misses + False Alarms}}$, where Hits (truth=1, pred=1), Misses (truth=1, pred=0), and False Alarms (truth=0, pred=1) are counted after binarizing the continuous values of predictions and ground truth into 0/1 values at thresholds [20, 30, 40] dBz. Following \cite{nature21, prediff, diffcast}, we report the CSI at pooling scale $4 \times 4$, which relax the pixel-wise matching to evaluate the accuracy on neighborhood aggregations. 

% \subsection{Reconstruction}
% To accurately reconstruct highly dynamic radar sequences, all parameters of the 3D Gaussians need to be optimized. Accordingly, we modified the code of the comparison methods to ensure that each parameter’s transformation can be reconstructed while retaining the original reconstruction strategies.
% Notably, the goal of reconstruction in this task is to enable subsequent prediction based on the Gaussians, where each dynamic Gaussian must maintain complete spatiotemporal consistency throughout the entire time sequence. However, the adaptive density control strategy used in dynamic scene reconstruction disrupts temporal consistency. Therefore, in our comparative experiment, we disable the density control in these methods.

\subsection{Prediction}
The existing prediction models used for comparison are originally designed for 2D sequence prediction. Therefore, adjustments must be made to adapt them for 3D radar sequence prediction. Two feasible modifications can be considered. The first involves treating the newly introduced third dimension, which represents the vertical axis, as a channel dimension. However, this approach would limit the model’s ability to predict changes along the vertical axis as effectively as it does in the horizontal plane. Instead, we opted to extend these models from 2D to 3D, enabling them to natively handle 3D prediction. Specifically, we expanded 2D CNNs to 3D CNNs and similarly extended modules such as ConvGRU and ConvLSTM included in PhyDNet and DiffCast from 2D to 3D. Additionally, the 2D PDE constraint in PhyDNet is also adapted for 3D prediction.

\section{Extended experiments}

\begin{table}[t]
% \vspace{-0.6cm}
\setlength\tabcolsep{5pt}
\caption{Extended Experiment results in NEXRAD}
\label{supp:ext_exp}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l | c c c c c c c c}
\toprule
Model & MAE^\downarrow & SSIM^\uparrow & LPIPS^\downarrow & LPIPS\textsubscript{Radar}^\downarrow & CSI-20_{\text{Pool4}}^\uparrow & CSI-30_{\text{Pool4}}^\uparrow & CSI-40_{\text{Pool4}}^\uparrow \\
\midrule
ConvGRU & 0.006 & 0.836 & 0.194 & 1.632 & 0.326 & - & - \\
PhyDNet & 0.017 & 0.366 & 0.323 & 2.114 & \underline{0.348} & 0.097 & 0.002 \\
SimVP & 0.008 & 0.817 & 0.176 & 1.483 & 0.227 & 0.002 & 0.000 \\
DiffCast & 0.152 & 0.005 & 0.925 & 4.005 & 0.051 & 0.023 & 0.044 \\
\midrule
Mamba & \underline{0.004} & \underline{0.902} & \underline{0.125} & \underline{0.625} & 0.304 & \underline{0.158} & \underline{0.075} \\
GauMamba & \textbf{0.003} & \textbf{0.907} & \textbf{0.122} & \textbf{0.600} & \textbf{0.361} & \textbf{0.205} & \textbf{0.089} \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

\textcolor{edit}{
We conduct extended experiments using the NEXRAD dataset spanning three years (2020–2022) under the same training epochs, aiming to investigate the impact of increased dataset diversity and size on model performance. As shown in Table \ref{supp:ext_exp}, most metrics of these models exhibit slight improvements when trained on the extended dataset. This performance gain can be attributed to the enhanced data diversity and the increased number of iteration steps enabled by the larger dataset. Notably, our model continues to outperform others, demonstrating its robustness and effectiveness even under extended experimental conditions.}

\section{Qualitative Results}

\textbf{Reconstruction.}
%
\textcolor{edit}{The full reconstructed results are shown in Fig. \ref{supp:fig:rec0}, \ref{supp:fig:rec1} and \ref{supp:fig:rec2}.}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{rec_0.pdf}
\end{center}
\caption{
Qualitative results of 3D radar reconstruction from frame 1 to frame 10.
}
\label{supp:fig:rec0}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{rec_1.pdf}
\end{center}
\caption{
Qualitative results of 3D radar reconstruction from frame 11 to frame 20.
}
\label{supp:fig:rec1}
\end{figure}

\begin{figure}[h]
\hspace{0.01\columnwidth}
\begin{left}
\includegraphics[width=0.63\columnwidth]{rec_2.pdf}
\end{left}
\caption{
Qualitative results of 3D radar reconstruction from frame 21 to frame 25 and details of frame 25.
}
\label{supp:fig:rec2}
\end{figure}

% \begin{wrapfigure}{r}{width=0.63\textwidth}
% \centering
% \includegraphics[width=0.63\textwidth]{rec_2.pdf}
% \caption{Qualitative results of 3D radar reconstruction from frame 21 to frame 25 and details of frame 25.}
% \label{supp:fig:rec2}
% \end{wrapfigure}

\textbf{Prediction.}
%
The full predicted results are shown in Fig. \ref{fig_pred1}, Fig. \ref{fig_pred2}, Fig. \ref{fig_pred3}, and Fig. \ref{fig_pred4}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{mosaic_0.pdf}
\end{center}
\caption{
Qualitative results of 3D radar prediction from frame 6 to frame 15 in MOSAIC dataset.
}
\label{fig_pred1}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{mosaic_1.pdf}
\end{center}
\caption{
Qualitative results of 3D radar prediction from frame 16 to frame 25 in MOSAIC dataset.
}
\label{fig_pred2}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{nexrad_0.pdf}
\end{center}
\caption{
Qualitative results of 3D radar prediction from frame 6 to frame 15 in NEXRAD dataset.
}
\label{fig_pred3}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{nexrad_1.pdf}
\end{center}
\caption{
Qualitative results of 3D radar prediction from frame 16 to frame 25 in NEXRAD dataset.
}
\label{fig_pred4}
\end{figure}

\section{Discussion}
\textbf{Training objective of GauMamba.} 
It is possible to directly predict $\mathcal{G}_t$ based on $\mathcal{G}_{t-1}$. However, we found that this approach often leads to poor convergence. The difficulty arises because directly predicting $\mathcal{G}_t$ requires the model to estimate both the overall trend of $\mathcal{G}$ and its exact position. In contrast, predicting $\Delta \mathcal{G}_t$ simplifies the problem by focusing only on the changes from the initial timestamp $0$ to timestamp $t$, thus reducing the solution space and making it more compact. Another viable alternative is to predict the difference between consecutive timestamps, i.e., $\Delta \mathcal{\hat{G}}_t = \mathcal{G}_t - \mathcal{G}_{t-1}$. However, this method suffers from the drawback of error accumulation during inference, where the iterative predictions lead to increasingly compounded errors over time.

\textbf{Discussion of convergence issue in reconstruction.}
To accurately reconstruct highly dynamic radar sequences, all parameters of the 3D Gaussians need to be optimized. Following this setting the existing dynamic Gaussian reconstruction methods faces significant challenges to convergence. The fixed parameters serve as efficient anchors, allowing each Gaussian to lock onto a corresponding region of the object being reconstructed, thereby consistently tracking changes in the object. When all parameters become adjustable, this crucial prior constraint is lost, leading to incoherent movement of the Gaussians with the reconstructed parts, and resulting in a vague and suboptimal optimization objective. In a extreme case, 3D Gaussians exhibiting random Brownian motion can still reconstruct dynamic scenes and achieve a suboptimal outcome, as long as the deformation function or neural network used for reconstruction has sufficient capacity and enough iterations. Furthermore, each Gaussian influences only a very small region of the space, which theoretically limits the effective area for gradient descent. A small perturbation introduced by randomly initialized neural networks or polynomial functions can cause the Gaussian to jump out of its original reconstruction region, hindering the gradient flow from the corresponding ground truth area from properly affecting the disturbed Gaussian and trapping it in an unrelated region, leading to significant convergence issues.


\end{document}
