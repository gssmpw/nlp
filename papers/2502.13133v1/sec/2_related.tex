\section{Related Work}
\label{sec:related}

\noindent
\textbf{Audio-driven Talking Heads.}
Earlier approaches for audio-driven talking face generation, such as Video Rewrite~\cite{bregler1997video} and Voice Puppetry~\cite{voicepuppetry}, propose probabilistic models that map phonemes extracted from an audio signal to corresponding mouth shapes (visemes). This phoneme-to-viseme mapping can be learned by hidden Markov models (HMMs)~\cite{sako2000hmm, fu2005audio}, decision trees~\cite{kim2015decision}, or long short-term memory (LSTM) units~\cite{fan2015}. Synthesizing Obama~\cite{suwajanakorn2017synthesizing} is one of the first notable works that produces photo-realistic lip synced videos of former U.S.\ President Barack Obama. Subsequent works propose encoder-decoder architectures, most of them trained as generative adversarial networks (GANs)~\cite{goodfellow2014generative}, using large video datasets
% , in order to achieve appealing visual quality
~\cite{wav2lip,yousaid,Vougioukas_2019_CVPR_Workshops,chen2019hierarchical,zhou2019talking,pcavs,Yang:2020:MakeItTalk,thies2020neural,Lahiri_2021_CVPR}. 
% Wav2Lip~\cite{wav2lip} achieves a highly competitive lip sync accuracy.
Most operate in the 2D space, generating low-resolution videos, while some of them use intermediate representations, like landmarks or 3DMM parameters~\cite{blanz1999morphable,zhang2023sadtalker}. They mainly focus on precise lip synchronization, \eg, Wav2Lip~\cite{wav2lip}. A few enable additional control of head pose~\cite{pcavs,Yang:2020:MakeItTalk} or emotion~\cite{drobyshev2024emoportraits,xu2023high,eamm,wang2022pdfgc,Gan_2023_ICCV}.
Recent approaches produce higher-resolution images, by learning a 3D representation of the human head based on neural radiance fields (NeRFs)~\cite{guo2021ad,semantic,dfanerf,ye2023geneface,ye2023geneface++,lipnerf} or gaussian splatting~\cite{cho2024gaussiantalker,li2025talkinggaussian}. Still, the output is a 2D video and they require a pre-recorded speech signal as input.

Another line of work addresses the problem of audio-driven 4D facial animation. Several works learn to animate subject-specific face models~\cite{karras2017audio,pham2017speech,richard2021audio,cao2005expressive} or artist-designed character rigs~\cite{taylor2017deep,edwards2016jali,zhou2018visemenet} based on input speech. Works~\cite{VOCA2019,aneja2023facetalk,thambiraja2023imitator,xing2023codetalker,Peng_2023_ICCV,sun2024diffposetalk}, like VOCA~\cite{VOCA2019}, MeshTalk~\cite{richard2021meshtalk}, FaceFormer~\cite{fan2022faceformer}, and FaceTalk~\cite{aneja2023facetalk} show expressive animation of 3D meshes, accurately lip syncing to speech inputs. However, they do not learn head motion and only use untextured 3D meshes that lack detail.

% \noindent
% \textbf{Diffusion-based Talking Heads.}
Diffusion models~\cite{ho2020denoising,song2020denoising,song2020score} have recently taken over in the generative modeling domain. They have already shown improved results in talking faces~\cite{stypulkowski2024diffused,shen2023difftalk,ma2023dreamtalk,tian2024emo,xu2024hallo,wei2024aniportrait,kim2024moditalker}. Some of them operate in the image space, while more recent ones propose latent diffusion models; conditioned on audio, they generate latent face embeddings. VASA-1~\cite{xu2024vasa} achieves lifelike generation of audio-driven talking faces. However, all these can only produce 2D videos. More related to our work, Audio2Photoreal~\cite{ng2024audio2photoreal} synthesizes photo-realistic 4D humans. While it generates natural-looking gestures, it lacks in lip syncing, and again assumes existing speech signal as input. In contrast, we propose a method that can animate 4D avatars from just text. Furthermore, we use flow matching~\cite{lipman2022flow}, which compared to diffusion models, achieves faster inference and better performance.



\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{images/overview.png}
   \caption{\textbf{Overview of \MethodName}. Given any input text, our method synthesizes \textit{expressive audio-visual 4D talking avatars}, jointly generating head and facial dynamics and the corresponding speech signal. Two parallel diffusion transformers with intermediate highway connections ensure communication between the audio and visual modalities. \MethodName can be additionally conditioned on the audio-visual input of a user, in order to synthesize conversational avatars in dyadic interactions.}
   \label{fig:overview}
   \vspace{-5pt}
\end{figure*}



\noindent
\textbf{Text-driven Talking Heads.} The problem of text-driven talking faces is much less explored. Very early works, like MikeTalk~\cite{ezzat2000visual}, are based on phoneme-to-viseme mappings~\cite{taylor2012dynamic,wan2013photo}. Subsequently, neural networks learn text-to-lip positions in the image space~\cite{kumar2017obamanet,liu2022parallel,choi2024text}, but they work on low-resolution 2D videos. Some works propose cascaded approaches, \ie, text-to-speech (TTS) and then speech or latent codes to vision~\cite{zhang2022text2video,ye2023ada,mitsui2023uniflg}. However, cascaded methods usually are slower and prone to error accumulation. Other works use text for video editing~\cite{fried2019text}, or as a description~\cite{wang2024instructavatar,ling2024posetalk,diao2023ft2tf,xu2023high,ma2023talkclip}, in order to control the emotional state or the identity of the generated subject.
Most related with us are TTSF~\cite{Jang_2024_CVPR} and NEUTART~\cite{milis2023neural}, which jointly generate speech and video. However, they again work with low-resolution 2D videos. In contrast, we synthesize high-quality, photo-realistic 4D avatars. Through our fully-parallel architecture, trained end-to-end with flow matching, we achieve fast inference and natural-looking talking heads. Additionally, our method enables an \textit{always-on} avatar that actively listens and reacts, leading to empathetic interactions with a user.

% Very few works have addressed the problem text-driven talking faces~\cite{kumar2017obamanet,liu2022parallel,zhang2022text2video,choi2024text}, but they only work on low-resolution 2D videos.
% Since these are trained on 2D datasets, they might produce artifacts or blurry results, and they do not have any capability of animating the head pose. They also assume available pre-recorded speech. In contrast, our method synthesizes...

% Ada-TTA~\cite{ye2023ada} proposes a cascaded approach that first adapts a TTS systeom to a target speaker's voice and then uses GeneFace++~\cite{ye2023geneface++} for talking face generation.

% The most related approach with our work is TTSF~\cite{Jang_2024_CVPR}. However, we identify the following  main differences: (a) We address the problem of 4D talking faces, not just 2D video generation. (b) We propose a fully-parallel architecture with intermediate connections, achieving natural multi-modal synchronization, in contrast to TTSFâ€™s late fusion. (c) Our architecture is trained end-to-end with flow matching, achieving very low latency, comparing with the GAN-based TTSF. (d) We additionally address the case of dyadic conversations, where our model communicates with a participant in real-time, generating empathetic interactions.

% Text-to-audiovisual (Generating both audio and video modalities):
% Very early work (with visemes): MikeTalk~\cite{ezzat2000visual}, ~\cite{taylor2012dynamic} 
% Early work that proposes HMM-based text-to-speech synthesis active
% appearance model (AAM)-based facial animation: ~\cite{wan2013photo}


% NEUTART~\cite{milis2023neural}: 2D talking faces from video - visual quality (blurry)

% TTSF~\cite{Jang_2024_CVPR}

% Match-TTSG~\cite{mehta2024unified} audio + gestures - not photorealistic


% Compared to Faces that Speak~\cite{Jang_2024_CVPR}, we do not use any prior network (serves as initial condition for flow matching and takes as input the first motion and acoustic features), or any GAN-based or lip-sync losses. We also use disentangled representations for facial expression and head pose. 





% \noindent
% \textbf{Text-to-Speech.}

% Text-to-spectrogram: Matcha-TTS~\cite{mehta2024matcha}

% \noindent
% \textbf{Pairing with an LLM.}

% Recent works have tried to pair generative models with LLMs. For example, ChatPose~\cite{feng2024chatpose} proposes a method that combines an LLM with SMPL~\cite{SMPL:2015} model and produces 3D human poses.

% However, these methods are limited to text descriptions, lacking photo-realistic human interaction. We propose a simple approach that generates a talking head with corresponding audio, converting the LLM-user interaction to a proper photorealistic human-to-human interaction.







% \section{Formatting your paper}
% \label{sec:formatting}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



% %-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


% %-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

% \begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.pdf}
% \end{verbatim}
% }


% %-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.