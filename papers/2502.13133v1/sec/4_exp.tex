\section{Experiments}
\label{sec:experiments}

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.52\linewidth]{images/synthesis_text_birds.png}
   \includegraphics[width=0.46\linewidth]{images/synthesis_text_painting.png}
   \vspace{-5pt}
   \caption{\textbf{Qualitative Results of \MethodName}. From just raw text characters as input, \MethodName synthesizes expressive audio signal (shown as mel-spectrogram on top) and corresponding head and facial dynamics of our 4D talking avatar.}
   \label{fig:qual_text_melspec_avatar}
   % \vspace{-5pt}
\end{figure*}

\begin{table*}[t]
  \centering
  \begin{tabular}{@{}l|ccccccccc@{}}
  \toprule
    & \textit{Lip Sync} & \multicolumn{1}{c}{\textit{Realism}} & \multicolumn{2}{c}{\textit{Diversity}} & \multicolumn{2}{c}{\textit{AV-Alignment}} & \multicolumn{2}{c}{\textit{Audio Quality}} \\
    \midrule
    Method & F1$_{lips}$$\uparrow$ & FD$_{e}$$\downarrow$ & Div$_{h}$$\uparrow$ & Div$_{e}$$\uparrow$ & BC$_{h}$$\uparrow$ & BC$_{e}$$\uparrow$ & MCD$\downarrow$ & WER$\downarrow$ \\
    \midrule
    Separate Models & 0.933 & 0.981 & 0.023 & 0.614 & 0.218 & 0.184 & 1.009 & 0.179 \\
    Shared Weights & 0.910 & 0.862 & 0.024 & 0.664 & 0.208 & 0.174 & 0.986 & 0.287\\
    Cascaded & 0.848 & 1.223 & 0.026 & 0.571 & 0.222 & 0.222 & 1.009 & 0.179 \\
    \midrule
    % AV-Flow w/ Fusion (b) & 0.865 & 0.001 & \textbf{0.828} &  0.022 & \textbf{0.695} & \textbf{0.271} & 0.186 & 0.880 & \\
    AV-Flow (Ours) & \textbf{0.964} & \textbf{0.861} & \textbf{0.029} & \textbf{0.680} & \textbf{0.258} & \textbf{0.229} & \textbf{0.900} & \textbf{0.157}\\
    \bottomrule
  \end{tabular}
  \caption{\textbf{Ablation Study.} We compare with the following variants: (a) Separate Models: 2 separate DiTs (one for audio and one for visual generation), without any connections, (b) Shared Weights: 1 model for both modalities with shared weights, (c) Cascaded Method: sequence of audio DiT (tokens-to-speech) and visual DiT (speech-to-video). Our proposed \MethodName achieves the best results.}
  \label{tab:quant_ablation}
  \vspace{-10pt}
\end{table*}



\noindent
\textbf{Datasets.} We use the publicly available dataset proposed by Audio2Photoreal~\cite{ng2024audio2photoreal}. This dataset includes dyadic conversations between 4 pairs of subjects. Each session lasts about 2 hours, with a total duration of 8 hours. There are 4 individuals. In each session one is the main ``actor'' and the other one is the ``participant''.  The actors are prompted to a diversity of situations, including informal and more professional interactions. Both subjects are captured simultaneously in multi-view capture domes, enabling photo-realistic rendering. The data include the raw audio, face expression codes of the actors, and pre-trained personalized renderers.

In addition to this dataset, we capture an internal dataset of 50 hours in a similar setting. It includes 1 main actor, who is engaged in dyadic conversations with 20 different participants. Similarly with~\cite{ng2024audio2photoreal}, we extract the raw audio at 48 kHz. Applying a simple voice activity detection (VAD), we separate the audio of the actor from the audio of the participant. We extract face encodings and head poses of the full 50 hours for the actor. We also have the raw video of the participant from one camera view and a pre-trained personalized renderer for the actor.

% \noindent
% \textbf{Baselines.}

\noindent
\textbf{Evaluation Metrics.} Since our approach synthesizes both audio and vision, we evaluate both modalities. Regarding the visual part, we follow similar works~\cite{ng2024audio2photoreal,richard2021meshtalk,richard2021audio} and choose a combination of metrics that capture:
% the lip synchronization, the realism, and the diversity of the synthesized avatars:
\begin{itemize}
    \item \textit{Lip synchronization}: We first reconstruct the ground truth and generated 3D meshes per frame. We determine the lip closures when the corresponding vertices of the inner upper and lower lips match (\ie, the distance is almost zero). We compute the F1-score to emphasize the importance of both high precision and high recall~\cite{richard2021audio}.
    \item \textit{Realism}: We measure the Fr\'echet distance between ground truth and generated face expressions (FD$_{e}$), in order to estimate the distribution distance.
    \item \textit{Diversity}: We measure the diversity of the generated head poses (Div$_{h}$) and expressions (Div$_{e}$) as the standard deviation across samples in our test set. 
\end{itemize}
An important contribution of our method is the correlation between our synthesized audio and video:
\begin{itemize}
    \item \textit{Audio-visual alignment}: We calculate the Beat Align Score~\cite{zhang2023sadtalker,zhu2023taming,siyao2022bailando,li2021ai} between audio and head motion (BC$_{h}$) and between audio and facial motion (BC$_{e}$). The audio beats are estimated by detecting peaks in onset strength~\cite{ellis2007beat,mcfee2015librosa} in the generated speech, and the motion beats as the local minima of the kinetic velocity~\cite{li2021ai}.
\end{itemize}
Finally, we evaluate our synthesized speech signal:
\begin{itemize}
    \item \textit{Audio quality}: We measure the mel cepstral distortion (MCD), which estimates the difference between mel cepstra with dynamic time warping~\cite{lee2022bigvgan,Jang_2024_CVPR}, and the word error rate (WER) to evaluate the intelligibility~\cite{paszke2019pytorch,yang2021torchaudio}.
    % , using a pre-trained Wav2Vec2 model for ASR~\cite{paszke2019pytorch}.
\end{itemize}

\noindent
\textbf{Implementation Details.}
In ~\cref{eq:ourloss}, we use $\lambda_s = 3.0$, $\lambda_f = 1.0$, and $\lambda_h = 0.2$. We notice that 8 steps for the Euler solver give similar results with 16 or 32 steps, and lead to faster inference. We use rotary positional embeddings~\cite{su2024roformer} and windows of 10 frames for the DiTs. For our input data rate of 86 fps (see~\cref{sec:method_architecture}), this leads to a negligible starting latency of around 120 ms, and real-time synthesis.
%See suppl.~for more details.


% \textbf{Metrics}

% Vision generation: lip closures, FID, diversity in expressions/head poses

% Speech generation: Audio quality, WER (if text available)

% Beat alignment (Beat Align Score or Beat Consistency Score) similar with other works~\cite{zhang2023sadtalker,zhu2023taming,siyao2022bailando}, where audio beats are estimated by detecting peaks in onset strength~\cite{ellis2007beat}.
% % librosa.beat.beat\_track to calculate the beats

% Diversity: std of expressions and head poses.

\subsection{Ablation Study}\label{sec:exp_ablation}

We first conduct an ablation study, comparing our proposed audio-visual fusion with different variants: (a) Separate models: we train 2 separate transformers in parallel (one for audio and one for visual generation), without any connections. (b) Shared weights: we train a single model for both modalities by concatenating the inputs/outputs. (c) Cascaded approach: we learn the audio DiT (tokens-to-speech) followed by the visual DiT (speech-to-video). \cref{tab:quant_ablation} shows the corresponding quantitative results. \MethodName achieves the best results across all the metrics. It produces well-synchronized lips, with accurate lip closures, which is crucial for photo-realistic 4D talking avatars. In addition, the proposed audio-visual fusion leads to the best correlation between audio and motion, as measured by the BC$_{h}$ and BC$_{e}$ metrics.

\cref{fig:qual_text_melspec_avatar} shows qualitative results of our method. From just raw text characters as input (no audio available), \MethodName synthesizes expressive audio signal and corresponding facial and head dynamics of our 4D talking avatar. Notice the accuracy of the lip motion for each phoneme (written at the bottom), as well as the expressiveness of the avatar.


\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{images/lipsync1.png}
   \includegraphics[width=\linewidth]{images/lipsync4.png}
   \caption{\textbf{Qualitative Evaluation.} We compare with state-of-the-art methods for audio-driven talking faces, namely FaceTalk~\cite{aneja2023facetalk}, VASA-1~\cite{xu2024vasa}, Audio2Photoreal~\cite{ng2024audio2photoreal}, and the text-driven TTSF~\cite{Jang_2024_CVPR} (the only one that can generate speech from text like ours). We re-implement VASA-1 and TTSF (denoted with an asterisk) for our data (face encodings and renderers). FaceTalk only animates the face (not head motion). Our proposed \MethodName synthesizes the corresponding phoneme (shown on top) more accurately.}
   \label{fig:qual_lipsync}
   % \vspace{-5pt}
\end{figure*}






\begin{table*}
  \centering
  \begin{tabular}{@{}l|ccccccccc@{}}
    \toprule
    % Method & Lips & FID/FVD & Diversity - Exp / Head & Alignment with Speech - Exp/Head &  Audio Quality & Speed \\
    & \textit{Lip Sync} & \multicolumn{1}{c}{\textit{Realism}} & \multicolumn{2}{c}{\textit{Diversity}} & \multicolumn{2}{c}{\textit{AV-Alignment}} & \multicolumn{2}{c}{\textit{Audio Quality}} & {\textit{Speed}} \\
    \midrule
    Method & F1$_{lips}$$\uparrow$ & FD$_{e}$$\downarrow$ & Div$_{h}$$\uparrow$ & Div$_{e}$$\uparrow$ & BC$_{h}$$\uparrow$ & BC$_{e}$$\uparrow$ & MCD$\downarrow$ & WER$\downarrow$ & Time (s)$\downarrow$ \\
    \midrule
    % Another audio-driven &&&&& \\
    FaceTalk & 0.851 & 0.873 & N/A & 0.670 & N/A & 0.209 & N/A & N/A & 1.443 \\
    % SadTalker & \\
    VASA-1$^{*}$ & 0.846 & 0.887 & 0.032 & 0.664 & 0.210 & 0.204 & N/A & N/A & 0.965 \\
    Audio2Photoreal & 0.920 & 0.879 & 0.022 & 0.586 & 0.198 & 0.202 & N/A & N/A & 1.578 \\
    \midrule
    VASA-1$^{*}$ w/ TTS & 0.710 & 2.665 & \textbf{0.033} & 0.678 & 0.190 & 0.201 & N/A & N/A & 1.265 \\
    Audio2Photoreal w/ TTS & 0.813 & 3.289 & 0.021 & 0.538 & 0.168 & 0.175 & N/A & N/A & 1.778\\
    TTSF$^{*}$ & 0.929 & 0.962 & 0.023 & 0.630 & 0.233 & 0.211 & 1.229 & 0.285 & 0.400  \\
    \midrule
    AV-Flow (Ours) & \textbf{0.964} & \textbf{0.861} & 0.029 & \textbf{0.680} & \textbf{0.258} & \textbf{0.229} & \textbf{0.900} & \textbf{0.157} & \textbf{0.398} \\
    \bottomrule
  \end{tabular}
  \caption{\textbf{Quantitative Evaluation.} We compare with state-of-the-art methods for audio-driven talking faces, namely FaceTalk~\cite{aneja2023facetalk}, VASA-1~\cite{xu2024vasa}, and Audio2Photoreal~\cite{ng2024audio2photoreal}. We convert them to text-driven by attaching our TTS (denoted w/ TTS), and compare with TTSF~\cite{Jang_2024_CVPR} which is the only method that can generate speech from text like ours. We re-implement VASA-1 and TTSF (denoted with an asterisk) for our 3D data.  We evaluate the synthesis quality, as well as the inference speed (seconds for generating 20 sec.~offline).}
  \label{tab:quant_results}
  \vspace{-10pt}
\end{table*}


\subsection{Evaluation}

\textbf{Baselines.} Very few works for talking faces have addressed the problem of text-driven generation, and even fewer the problem of joint audio-visual generation. To the best of our knowledge, our proposed \MethodName is the first approach that can generate audio-visual 4D talking heads from only text input. A concurrent work with us is TTSF~\cite{Jang_2024_CVPR}. However, we identify the following  main differences: (a) TTSF only generates 2D videos (not 4D avatars). (b) We propose a fully-parallel architecture with intermediate highway connections, focusing on the audio-visual fusion. (c) Our architecture is trained end-to-end with flow matching, achieving fast inference, compared with the GAN-based TTSF. (d) We additionally address the case of dyadic conversations, where our model communicates with a user.  Since TTSF's code is not available, we implement their method for our data (TTSF$^{*}$), where we train a personalized model that predicts face encodings and head motion (therefore, without any GAN-based losses and identity prediction). 

Most of the state-of-the-art methods are audio-driven and focus on the generation of 2D talking faces. We choose to compare with the seminal VASA-1~\cite{xu2024vasa} that similarly with us trains a DiT (but not with flow matching) on the latent space of head and facial dynamics. We adapt VASA-1 to be able to deal with our 3D data (encodings and renderer) and name the variant VASA-1$^{*}$. We also compare with the audio-driven face generation of Audio2Photoreal~\cite{wan2013photo}, as well as FaceTalk~\cite{aneja2023facetalk}. Note that FaceTalk only generates expressions, not head motion, but focuses on 4D avatars compared with the other 2D methods. Finally, we attach our text-to-speech (TTS) model and convert VASA-1$^{*}$ and Audio2Photoreal to text-driven methods. For fair comparison with all these methods, in our evaluation we use input audio from our test set that is converted to audio features (audio-driven) or text tokens (text-driven). 

\noindent
\textbf{Quantitative Evaluation.} \cref{tab:quant_results} demonstrates the corresponding quantitative results. \MethodName produces the best audio-visual alignment, as well as the most accurate lip synchronization. It also synthesizes diverse face and head motion. VASA-1$^{*}$ with TTS seems to slightly surpass our method in the head diversity (Div$_{h}$). However, qualitatively we noticed that this is because it generates more noisy head motion (higher variance). Our input tokens are also more robust as input: in the case of our conversational data, VASA-1$^{*}$ generates noisy and random motion during pauses of the actor. Additionally, we produce better audio quality compared with TTSF~\cite{Jang_2024_CVPR}. 

\noindent
\textbf{Inference Speed.} Since our method is based on flow-matching end-to-end, it achieves faster inference than the other methods. We measure the inference speed as the time for a single pass of the model on a single A100 GPU, averaged across multiple runs. We assume audio features stored
% (otherwise Wav2Vec2 feature extraction would take another 3-4 seconds for FaceTalk and VASA-1)
and omit the renderer for this calculation. The last column of \cref{tab:quant_results} shows this time in seconds. Using only 8 steps for the Euler solver, \MethodName needs only around 200ms to synthesize around 20 seconds of audio-visual content (offline speed). With the additional text-to-tokens module, it requires less than 400ms. Our TTSF$^{*}$ does not include any identity prediction or StyleGAN-based architecture, and thus the inference becomes faster from the original TTSF~\cite{Jang_2024_CVPR}. In comparison, the text-driven VASA-1$^{*}$ that is diffusion-based needs more than 1 second for the same length of video generation. 






\noindent
\textbf{Qualitative Evaluation.} 
\cref{fig:qual_lipsync} shows qualitative comparisons with state-of-the art methods for talking face generation. 
% Here, we collect test audio from internet videos and
We use test audio from the EARS dataset~\cite{richter2024ears}, and convert it to input audio features or text tokens accordingly. Since this input comes from a different individual than the training subject, lip syncing becomes more challenging. We show a variety of phonemes (on top) and corresponding mouth positions for each method. \MethodName demonstrates significant robustness, expressively and accurately animating the 4D talking avatars, under any input text. We also encourage the readers to watch our supplemental video.



% Variants:

% - 1 model (text-to-vision)

% - 2 separate models (text-to-vision and text-to-speech): voice quality degrades, no alignment between expressivness in speech and visual result

% - 2 models with shared weights = 1 model with 2 outputs

% - 2 models with interactions (linear fusion after each transformer block)

% - cascaded model (text-to-speech --> speech-to-vision)


% Fusion variants (supplementary):

% - linear fusion, linear fusion with skip connection, beta-gamma from AdaIn


% Compare audio quality and WER with TTS methods (e.g. Matcha-TTS) - can be a small table in supplementary

% TTS + audio-driven method (cascaded) --> higher inference speech (ours fully parallel)

% 60fps offline speed (vs 45fps vasa-1) on A100 (300ms + 800ms + 8.5s = 10sec for 600 frames at 30 fps)
% with nfe steps = 8 --> 198ms, with 4 --> 82ms (for 1725 frames)
% adding 200ms for TTS

% 1.45 for logmel, 4.92 for wav2vec










\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{images/smirk.png}
   % \includegraphics[width=\linewidth]{images/smirk2.png}
   % \vspace{-5pt}
   \caption{\textbf{Audio-Visual Guidance in Dyadic Conversations.} The actor reacts (with their gaze or smile) according to the participant's expression and/or voice (\MethodName with guidance).}
   \label{fig:qual_smirk}
   \vspace{-10pt}
\end{figure}


% \begin{figure}[t]
%   \centering
%    \includegraphics[width=\linewidth]{images/headexp_spec.png}
%    \caption{Correlation between synthesized speech, head pose and facial dynamics.}
%    \label{fig:spec_head_exp}
% \end{figure}

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{images/guidance_plot.png}
   \caption{\textbf{Guidance over time}. L2 norm of expression codes for the ground truth actor, the participant, and the generated actor with or without guidance. With guidance, \MethodName produces the appropriate reaction in dyadic interactions.}
   \label{fig:plot_guidance}
   \vspace{-10pt}
\end{figure}

\subsection{Audio-Visual Guidance in Conversations}

As mentioned in \cref{sec:method_dyadic}, we propose to provide audio-visual guidance from a participant in dyadic conversations, extending \MethodName to \textit{conversational} avatars. Very few methods put avatars in this setting, actively talking and listening, although this interaction is common in everyday life. Audio2Photoreal~\cite{ng2024audio2photoreal} produces listening behavior based on audio input, but cannot react to participant's expressions.

\cref{fig:qual_smirk} shows the same actor talking with different participants. The basic \MethodName (without any guidance) is shown in the last column. With our audio-visual guidance (3rd col.), the generated avatar reacts with their gaze and/or smile, according to the participant's expression or voice.  
\cref{fig:plot_guidance} shows the L2 norm of the SMIRK~\cite{SMIRK:CVPR:2024} expression codes over time for the ground truth actor, participant, and generated actor with and without guidance. The graph shows how the actor reacts at the same time or before/after the participant, while they interact. With the proposed guidance, \MethodName produces the appropriate expression, leading to empathetic interactions. We also noticed a $2\%$ decrease in FD$_{e}$ for our test set with this guidance (see also suppl.).



% \textbf{Figures}

% - Figure with comparisons with other methods (show lip-syncing and corresponding phoneme)

% - Figure showing visual guidance from participant (expression and reaction), listening behavior, nodding, smiling/laughing, surprise

% - Figure showing diverse expressions for same phoneme 

% - Figure showing beats and corresponding head motion / expression

% - Figure showing expressions of both in time (showing correlation / reactions) ?



% \textbf{Comparisons with audio-driven methods for talking faces:}

% - VASA-1: our DiT model without flow matching is similar with VASA-1. It is identity-specific and we use our own decoder.

% - Sadtalker ? (audio to expression mapping)

% - DreamTalk: diffusion-based - similar

% - Diffused Heads

% - DiffTalk

% - Evonne's photoreal

% FaceTalk~\cite{aneja2023facetalk}: diffusion with wav2vec

% Should we add methods like FaceFormer, CodeTalker, ScanTalk?


% \textbf{Comparisons with text-driven methods for talking faces:}

% - Faces that Speak~\cite{Jang_2024_CVPR}: GAN-based + lip-sync loss + flow matching in the middle for generating motion features. They generate both speech and video from text. They don't have code ? We can compare with their fusion - simple addition ?

% - We can compare with audio-driven methods (e.g. top 2), by using as input our generated speech (cascaded approach: TTS + audio-driven method)



