\section{Conclusion}
\label{sec:conclusion}

In conclusion, we introduce a novel method for joint audio-visual generation of 4D talking avatars, given only text inputs (\eg, obtained by an LLM). Our fully-parallel diffusion-based architecture ensures cross-modal communication, leading to synchronized audio and visual modalities. Trained with flow matching, \MethodName leads to fast inference and natural-looking talking faces. It also enables dyadic conversations, animating an always-on avatar that actively listens and reacts to the audio-visual input of a user. We believe that this work gets one step closer to enabling natural interaction between a human and an AI system. 

\noindent
\textbf{Limitations and Ethical Considerations.}
While our method produces natural-looking avatars, it does not have any understanding of the semantic content of the inputs. E.g., if a user makes a joke without laughing themselves, the model would not know that it was a joke and could not react accordingly. This will be an interesting exploration for future work. In this work, we only use consenting participants. 
Since our method is identity-specific, only these can be rendered. 
This addresses ethical concerns of generating non-consenting subjects.
