\begin{abstract}
% The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
% Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
% The abstract is to be in 10-point, single-spaced type.
% Leave two blank lines after the Abstract, then begin the main text.
% Look at previous \confName abstracts to get a feel for style and length.
% We introduce AV-Flow, a fully-parallel audio-visual generative model for 4D talking faces, based on flow matching. In contrast to prior work that conditions on existing speech signal, we propose a generative model that synthesizes speech and vision jointly, conditioned on any input tokens. We demonstrate human-like speech synthesis, synchronized lip motion and lively facial expressions and head pose; all generated from just text characters. The core premise of our approach lies on the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized intonation in both the generated speech and facial expressions (e.g., eyebrow motion). Our model is trained with flow matching, with only 200ms look-ahead, leading to very low latency and diverse results. AV-Flow can be easily deployed as a plug-and-play on top of an LLM, converting text tokens to human-like audio-visual outputs. In case of dyadic conversations, we can optionally condition on information from a participant and synthesize empathetic interactions. Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars.
% We introduce AV-Flow, a fully-parallel audio-visual generative model for 4D talking avatars\MZ{Would "talking heads" be more accurate?}, based on flow matching. In contrast to prior work that assumes an existing speech signal, we synthesize speech and vision jointly from just text input. We demonstrate human-like speech synthesis, synchronized lip motion and lively facial expressions and head pose; all generated from just input text. The core premise of our approach lies in the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized speech intonation and facial dynamics (e.g., eyebrow motion). Our model is trained with flow matching, leading to expressive results and fast inference. In case of dyadic conversations,
% we can additionally condition on a participant and synthesize empathetic interactions.
% AV-Flow produces an always-on avatar, that actively listens and reacts to audio-visual input of a user. 
% \MZ{There seem to be multiple use cases, i.e., talking head from text and talking head conditioned on other user. This is unclear until the last sentance of the abstract}
% Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars.
We introduce AV-Flow, an audio-visual generative model that animates photo-realistic 4D talking avatars given only text input. In contrast to prior work that assumes an existing speech signal, we synthesize speech and vision jointly. We demonstrate human-like speech synthesis, synchronized lip motion, lively facial expressions and head pose; all generated from just text characters. The core premise of our approach lies in the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized speech intonation and facial dynamics (e.g., eyebrow motion). Our model is trained with flow matching, leading to expressive results and fast inference. In case of dyadic conversations,
% we can additionally condition on a participant and synthesize empathetic interactions.
AV-Flow produces an always-on avatar, that actively listens and reacts to the audio-visual input of a user. 
% \MZ{There seem to be multiple use cases, i.e., talking head from text and talking head conditioned on other user. This is unclear until the last sentance of the abstract}
Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars.
Project page: \small{\url{https://aggelinacha.github.io/AV-Flow/}}.
\end{abstract}