\section{Method}
\label{sec:method}

We present \MethodName, a novel method for \textit{joint audio-visual generation} of 4D talking avatars, driven by just text inputs. An overview of our approach is illustrated in \cref{fig:overview}. \MethodName consists of two inter-connected diffusion transformers, one for audio and one for visual generation. Given input text tokens, the audio transformer generates a mel-spectrogram, through a series of transformer blocks. Correspondingly, the vision transformer generates head and facial dynamics. We design intermediate highway connections that ensure communication between the audio and visual modalities. The synthesized mel-spectrogram is decoded to a speech signal via a pre-trained vocoder. Correspondingly, the predicted head and facial dynamics are rendered to the output 4D avatar with a pre-trained decoder. The overall method produces audio and visual outputs in a \textit{fully-parallel} way.
% The input tokens can be derived from just text characters or input speech using a speech-to-text model. 
In case of dyadic interactions, we additionally condition on audio-visual signals from a user, which guides the synthesis of natural-looking conversational 4D avatars.


% tokens-to-spectrogram and tokens-to-vision (with interactions) - additional guidance (visual guidance from participant in dyadic conversations)

\subsection{Representations}\label{sec:method_representations}

Our training data consist of dyadic conversations between a main subject, dubbed as ``actor'', and a ``participant'' (or user). The raw audio includes one channel for each of them. The 3D avatar representation of the actor corresponds to the latent space of a Codec Avatar~\cite{deep_appearance, wuu2022multiface}.
% , which we build following~\cite{wuu2022multiface}. 
% This representation is a 256-dimensional vector of latent expression codes for the face and a 6-DoF head rotation and translation.
For the participant, we have access to a monocular video showing their face.
%We have available annotations for the facial dynamics and the head pose of the actor, as well as the raw video of the participant.
We first present our basic \MethodName model, which is trained on the actor's data only (pairs of audio, head and face encodings). In \cref{sec:method_dyadic}, we demonstrate how we can additionally condition on participant's information to model dyadic interactions.

\noindent
\textbf{Input Tokens.}
We transcribe the raw audio using a pre-trained Wav2Vec2 model~\cite{baevski2020wav2vec} for speech recognition (ASR)~\cite{panayotov2015librispeech,yang2021torchaudio}. We extract the outputs of the last layer (logits), which essentially correspond to text character predictions. We use the logits $a_i \in \realnum ^{29}$ per frame $i$ as input tokens of our model.
% (see \cref{fig:overview}).
At inference time, we can drive our model from raw text alone, by learning a text-to-tokens module that maps raw text to character-level logits (see~\cref{sec:method_textdriven}).

% To make representations between training and inference compatible, we train a model that maps from raw text on character level to wav2vec2-style logits, see Section~\ref{?}. \ar{and maybe more detailed description in supp?}

\noindent
\textbf{Facial Dynamics.} Our data include facial expression codes $\bm{f}_i \in \realnum ^ {256}$ per frame $i$ of the actor, which correspond to the latent space of a VAE~\cite{deep_appearance}. These represent the holistic facial motion, including facial expression, lip motion, eyebrow and eyelid movements.

\noindent
\textbf{Head Dynamics.} 
% Our data also include head poses, extracted as a rotation matrix and translation vector.
Our data also include a 6-DoF head rotation and translation. We convert the rotation matrix to quarternion representation and concatenate with the translation vector, leading to a head pose $\hat{\bm{h}}_{i} \in \realnum ^ {7}$ per frame $i$. We train a small temporal VAE on the head poses, with one transformer encoder layer for the encoder and one for the decoder, in order to learn a more robust and temporally consistent representation. We use the latent space of this VAE to encode the head dynamics $\bm{h}_{i} \in \realnum ^ {8}$ per frame $i$.



% Expression codes 

% Head pose (quartenion and temporal VAE)

% Participant (FLAME expression codes and head pose from SMIRK) - Dyadic setting


\subsection{Architecture}\label{sec:method_architecture}

\noindent
\textbf{Diffusion Transformers.} We construct 2 parallel diffusion transformers, one for the audio generation and one for the visual generation. Each of them is based on the original architecture of latent diffusion transformer (DiT) models~\cite{peebles2023scalable}, and consists of $N$ blocks. We follow the variant of \textit{in-context conditioning}, concatenating input noise with the input tokens. The transformers are trained to progressively denoise the input noise, in order to restore the corresponding signal, and model the appropriate distribution. Formally, for a sequence of $n$ frames with input tokens $A = \{\bm{a}_1, \bm{a}_2, \dots, \bm{a}_n\}$, the audio transformer learns to synthesize the corresponding mel-spectrogram $S \in \realnum ^{n \times 80}$, and the visual transformer learns to synthesize the corresponding
head poses $H = \{\bm{h}_1, \bm{h}_2, \dots, \bm{h}_n\}$ and facial encodings $F = \{\bm{f}_1, \bm{f}_2, \dots, \bm{f}_n\}$. In order to facilitate the communication between the modalities and achieve exact correspondence of the number of frames and spectrogram dimension, we upsample the ground truth annotations to the rate of the spectrogram bins (at 86 fps).


\noindent
\textbf{Audio-Visual Fusion.} We design intermediate highway connections that enable communication between the audio and visual modalities. In this way, we achieve synchronized speech intonation with facial and head dynamics. Formally, for an output $\bm{x}^{a}_{l} \in \realnum ^{d_l}$ of a DiT block $l$ of the audio transformer ($l = 1, \dots, N$), and the corresponding output $\bm{x}^{v}_{l}  \in \realnum ^{d_l}$ of the vision transformer, we learn a linear fusion:
\begin{align}\label{eq:fusion}
    \bm{y}^{a}_{l} = \bm{x}^{a}_{l} + \bm{U}_{l}^\intercal [\bm{x}^{a}_{l}; \bm{x}^{v}_{l}] + \bm{b}_{l}, \\
    \bm{y}^{v}_{l} = \bm{x}^{v}_{l} + \bm{V}_{l}^\intercal [\bm{x}^{a}_{l}; \bm{x}^{v}_{l}] + \bm{c}_{l},
\end{align}
where $\bm{U}_{l}, \bm{V}_l \in \realnum ^{2d_l \times d_l}$ and $\bm{b}_{l}, \bm{c}_{l} \in \realnum ^{d_l}$ are learnable parameters.
The resulting features $ \bm{y}^{a}_{l} $ and $ \bm{y}^{v}_{l} $ are then fed as input into the next transformer block $ l+1 $ of the audio and visual transformer, respectively (see~\cref{fig:overview}).


\noindent
\textbf{Vocoder.} We decode the synthesized mel-spectrogram to a speech signal using a pre-trained BigVGAN vocoder~\cite{lee2022bigvgan}. We use the base version that is trained with speech signals sampled at 22050 Hz and spectrograms with 80 mel bands. We keep its weights frozen.

\noindent
\textbf{Avatar Decoder and Renderer.} We use the mesh-based Codec Avatar decoder and renderer released in~\cite{ng2024audio2photoreal,timur2021driving_renderer}.


% Diffusion Transformer

% Audio-to-vision module


\subsection{Flow Matching}

Flow matching is an efficient approach for generative modeling, recently introduced by Lipman \etal~\cite{lipman2022flow}. It combines ideas from continuous normalizing flows (CNF) and diffusion models. It leads to simpler paths with straight line trajectories, compared to the curved paths of diffusion models. Thus, it enables faster training and inference. In this section, we describe an overview of flow matching, that we use to train \MethodName. We refer the interested reader to~\cite{lipman2022flow}.

Let $\bm{x} \in \realnum ^{d}$ an observation in the data space (that can be a spectrogram sample or head pose or face encoding in our case), sampled from an unknown distribution $q(\bm{x})$. A probability density path is a time-dependent probability density function $p_t: [0, 1] \times \realnum ^d \rightarrow \realnum ^ {+} $. CNFs construct a probability path $p_t$ such that $p_0$ is a simple prior distribution, \ie, a standard normal distribution $p_0(\bm{x}) = \mathcal{N}(\bm{x}; \bm{0}, \bm{I})$, and $p_1$ approximates the distribution $q$. A time-dependent vector field $u_t: [0, 1] \times \realnum ^d \rightarrow \realnum ^ {d}$ generates the path $p_t$, and is used to construct a flow $\phi_t: [0, 1] \times \realnum ^d \rightarrow \realnum ^ {d}$; $\phi_t$ pushes the data from the prior towards the target distribution and is defined via the ordinary differential equation (ODE):
\begin{equation}
    \frac{d}{dt} \phi_t (\bm{x}) = v_t(\phi_t(\bm{x})) ; \quad \phi_0(\bm{x}) = \bm{x}.
    \label{eq:ode}
\end{equation}
%
The vector field $v_t$ is approximated by a neural network with parameters $\bm{\theta}$. Flow matching proposes the following objective, that allow us to flow from $p_0$ to $p_1$:
\begin{equation}\label{eq:fm_objective}
    \mathcal{L}_{\text{FM}}(\bm{\theta}) = \mathop{\mathbb{E}_{t, p_t(\bm{x})}} \| v_t(\bm{x}; \bm{\theta}) - u_t(\bm{x}) \|^2.
\end{equation}
%
As a tractable instantiation of Eq.~\eqref{eq:fm_objective}, we follow the Optimal Transport (OT) formulation from~\cite{lipman2022flow} where the flow from $p_0$ to $p_1$ is modeled by a straight line.
Then,
\begin{align}
    \phi_t(\bm{x}) = (1 - (1-\sigma_\text{min})t) \bm{x} + t\bm{x}_1
\end{align}
and the network $ v_t $ predicting the flow from a random Gaussian sample $\bm{x}_0 \sim p_0(\bm{x})$ to a data sample $\bm{x}_1 \sim q(\bm{x}_1)$ can be trained by optimizing the conditional flow matching objective:
\begin{align}\label{eq:flowloss}
    \mathcal{L}_{\text{CFM}} = \mathbb{E}_{t, \bm{x}_0, \bm{x}_1} \| v_t(\phi_t(\bm{x}_0)) - \big(\bm{x}_1 - (1 - \sigma_\text{min})\bm{x}_0\big) \|^2.
\end{align}
We empirically found that an L1 loss leads to more realistic results than an L2 loss.
Therefore, we optimize the objective:
\begin{equation}\label{eq:ourloss}
    \mathcal{L}_{\text{AV-Flow}} = \lambda _{s} \mathcal{L}_{s} + \lambda _{h} \mathcal{L}_{h} + \lambda _{f} \mathcal{L}_{f} \; ,
\end{equation}
%
where $\mathcal{L}_{s}$, $\mathcal{L}_{h}$, $\mathcal{L}_{f}$ are the objectives as in Eq.~\eqref{eq:flowloss} for the mel-spectrograms $S$, head poses $H$, and facial dynamics $F$ correspondingly, using L1 norm instead of L2.
Once the network $ v_t $ is trained, any ODE solver can be used to solve Eq.~\eqref{eq:ode}. We use the Euler solver in our work.

% %
% This is a simple and intuitive objective, but intractable in practice, since we do not have any prior knowledge  and there are many possible probability paths $p_t$. Thus, Lipman~\etal~\cite{lipman2022flow} propose to construct $p_t$ via a mixture of simpler conditional probability paths $p_t(\bm{x}|\bm{x}_1)$, given a data sample $x_1$.
% % , a conditional probability path $p_t(\bm{x}|\bm{x}_1)$ satisfies $p_0(\bm{x}|\bm{x}_1) = p(\bm{x})$ at time $t=0$ and $p_1(\bm{x} | \bm{x}_1)$ at time $t=1$.
% Conditional flow matching (CFM) learns a simpler objective that results in the same optima as \cref{eq:fm_objective}:
% \begin{equation}\label{eq:cfm_objective}
%     \mathcal{L}_{\text{CFM}}(\bm{\theta}) = \mathop{\mathbb{E}_{t, q(\bm{x}_1), p_t(\bm{x} | \bm{x}_1)}} \| v_t(\bm{x}; \bm{\theta}) - u_t(\bm{x} | \bm{x}_1) \|^2 \;,
% \end{equation}
% %
% where $t \sim \mathcal{U}[0,1]$, $\bm{x}_1 \sim q(\bm{x}_1)$, and $\bm{x} \sim p_t(\bm{x} | \bm{x}_1)$. 
% % This allows us to sample unbiased estimates and approximate $u_t$ with a neural network $v_t$. Interestingly, 
% It is proven that \cref{eq:fm_objective} and \cref{eq:cfm_objective} have identical gradients w.r.t.~the learnable parameters $\bm{\theta}$~\cite{lipman2022flow}.

% We train \MethodName with conditional flow matching, dubbed as flow matching for short. We construct $p_t$ as a Gaussian conditional probability path: $p_t(\bm{x} | \bm{x}_1) = \mathcal{N}(\bm{x} | \bm{\mu}_t(\bm{x}_1), \sigma_t(\bm{x}_1)^2 \bm{I})$. 
% We adopt the variant with optimal transport (OT)~\cite{lipman2022flow} and set $\bm{\mu}_t(\bm{x}) = t\bm{x}_1$, $\sigma_t(\bm{x}) = 1 - (1 - \sigma_{min)}) t$.
% % $\bm{\mu}_0(\bm{x}_1) = 0$, $\sigma_0(\bm{x}_1) = 1$, $\bm{\mu}_1(\bm{x}_1) = \bm{x}_1$, and $\sigma_1(\bm{x}_1) = \sigma_{min}$~\cite{lipman2022flow}.
% % \begin{equation}
% %     \bm{\mu}_t(\bm{x}) = t\bm{x}_1, \sigma_t(\bm{x}) = 1 - (1 - \sigma_{min)}) t \; ,
% % \end{equation}
% %
% The conditional vector field is defined as $u_t (\bm{x} | \bm{x}_1) = \frac{\bm{x}_1 - (1 - \sigma_{min)}) \bm{x}}{1 - (1 - \sigma_{min)})t}$, and the corresponding flow is $\phi_t(\bm{x}) = (1 - (1 - \sigma_{min)})t)\bm{x} + t\bm{x}_1$. Lipman~\etal~\cite{lipman2022flow} note that 
% diffusion models correspond to special cases of Gaussian paths and OT leads to faster training, faster generation, and better performance compared to diffusion paths.

% % Then, \cref{eq:cfm_objective} takes the form:
% % \begin{equation}
% %     \mathop{\mathbb{E}_{t, q(\bm{x}_1), p_t(\bm{x} | \bm{x}_1)}} \|u_t ( \psi_t(\bm{x}_0)) - \left(  \bm{x}_1 - (1 - \sigma_{min)})\bm{x}_0 \right)\|^2 \;.
% % \end{equation}
% % %

% In our work, we empirically noticed that L1 loss leads to more photo-realistic results. We optimize the objective:
% \begin{equation}\label{eq:ourloss}
%     \mathcal{L}_{\text{AV-Flow}} = \lambda _{s} \mathcal{L}_{s} + \lambda _{h} \mathcal{L}_{h} + \lambda _{f} \mathcal{L}_{f} \; ,
% \end{equation}
% %
% where $\mathcal{L}_{s}$, $\mathcal{L}_{h}$, $\mathcal{L}_{f}$ are the CFM objectives for the mel-spectrograms $S$, head poses $H$, and facial dynamics $F$ correspondingly, using L1 norm (see also suppl.).
% % \ar{this can probably be condensed if we need to make some space. It's probably enough to explain up the the ODE and then write a short paragraph on the loss, without all the details.}


\subsection{Text-Driven Generation}\label{sec:method_textdriven}

As mentioned in \cref{sec:method_representations}, during training, our input tokens are logits extracted from an ASR model, since we do not have available any text annotations. These input tokens are essentially predictions of text characters. Thus, our model easily generalizes to any input text at inference. To demonstrate this capability, we train a small text-to-tokens model (see \cref{fig:overview}), that maps raw text to logits, using the LJSpeech dataset~\cite{ljspeech17}. It follows a similar architecture with~\cite{mehta2024matcha}.
% , but predicts logits (not spectrograms).
It first maps the input text to phonemes and learns corresponding embeddings. It then predicts their duration and projects them to character-level logits at 86 fps (see also suppl.).
% (see more details in the suppl.~document).
% \ar{this needs more details. It's unclear how we go from raw text to 86fps logits/characters. It's even unclear \textit{that} we go from raw text to 86fps logits/characters. Maybe refer to supp for details and explain it there to save the space here?}


% Audio-Visual Generation

% tokens from ASR: tokens-to-vision + tokens-to-speech

% Interaction between Modalities (Fusion)

% Vocoder


\subsection{Dyadic Conversations}\label{sec:method_dyadic}

% dyadic setting - visual guidance from participant

An important capability of AV-Flow is that it can be easily conditioned to other input signals, that can guide the audio-visual generation accordingly. Using our conversational data (described in more detail in \cref{sec:experiments}), we demonstrate how we can guide the 4D talking avatar in a dyadic interaction, based on the audio-visual input of a user (see \cref{fig:overview}). 
In this way, we synthesize a \textit{conversational} avatar, that \textit{actively listens} and reacts (\eg, with facial expressions or head nodding), leading to empathetic interactions.
% \ar{One important point to highlight is that this allows us to model active listening, where the avatar reacts to the user even when it's not speaking.}

We propose to provide audio and visual guidance from the raw monocular video of the participant. Given each video frame $i$, we extract features $\bm{s}_i$ using SMIRK~\cite{SMIRK:CVPR:2024}. SMIRK predicts FLAME~\cite{FLAME:SiggraphAsia2017} parameters given a single image and faithfully captures a large variety of facial expressions. The features $\bm{s}_i = [\bm{e}_i; \bm{j}_i; \bm{r}_i]$ include facial expressions $\bm{e}_i \in \realnum ^{50}$, jaw pose $\bm{j}_i \in \realnum ^{3}$, and head rotation $\bm{r}_i \in \realnum ^{3}$ per frame $i$. We further extract ASR tokens $\bm{a}_i^{p}$ from the audio channel of the user, similarly with our input tokens for the actor. Both $\bm{s}_i$ and $\bm{a}_i^{p}$ are concatenated with the input tokens, giving audio-visual guidance to our method and enabling dyadic interaction with a user.

% We also evaluate the case where we do not have available the video of the participant, but only the speech signal. In this case, we can extract ASR tokens $\bm{a}_i^{p}$ from the corresponding audio channel, similarly with our input tokens for the actor.







% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.