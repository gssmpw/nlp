\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\renewcommand\thesection{\Alph{section}}

\setcounter{section}{0}

\section*{Contents}

\noindent
The supplementary document is organized as follows: 
\begin{enumerate}
    \item[A.] Additional Results
    \item[B.] Implementation Details
    \item[C.] Ethical Considerations
    % \item Limitations
    % \item Ethical Considerations
\end{enumerate}
We strongly encourage the readers to watch our supplementary video.


\section{Additional Results}



\noindent
\textbf{Audio-Visual Guidance.}
As mentioned in~\cref{sec:method_dyadic}, we propose to provide audio-visual guidance from a participant in dyadic conversations. We condition the model to visual information, by extracting features $\bm{s}_i$ using SMIRK~\cite{SMIRK:CVPR:2024} from the monocular video of the participant. For the audio, we further extract ASR tokens $\bm{a}_i^{p}$ from the audio channel of the participant. We notice that both modalities are important, in order to produce realistic and meaningful interactions. However, overall for our conversational data, the results feel realistic even if we condition on only one modality. In~\cref{fig:audioonly_visiononly_audiovisual}, we demonstrate some cases where we notice some difference when only one modality is available. In the first row, the avatar better mirrors the expression, with a wider smile, when visual information is present. In the second row, it produces a realistic  but unnecessary smile with only visual guidance, whereas it loses eye contact with only audio guidance. In the third row, the audio seems to play an important role that makes the user to smile. Overall, since we have available both audio and video, we propose to condition \MethodName to both modalities to produce our photo-realistic \emph{always-on} avatar.

% As mentioned in the Sec.~4.3, we noticed a $2\%$ decrease in FD$_{e}$ for our test set of dyadic conversations, by adding the proposed audio-visual guidance.
\cref{tab:quant_with_without_smirk} shows the quantitative results for the basic \MethodName without guidance and with audio-visual guidance. We compute the F1-score for the lip closures, as well as the F1-score for the smiles. The lip closures are detected by measuring the distance of the vertices of the inner upper and lower lips for the 3D mesh per frame. Similarly, smiling is detected when the distance of the left and right corners of the mouth is larger than a threshold. We also compute the Fr\'echet distance between ground truth and generated face expressions (FD$_{e}$) to estimate the distribution distance. With our proposed guidance, we notice an increase in F1-score for the smiles and a decrease in FD$_{e}$ for the dyadic setting, as the avatar produces more realistic reactions and facial expressions while listening to the user. Our basic \MethodName achieves a slightly more accurate lip synchronization.



\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{images/audio_visual_audiovisual.png}
   \caption{\textbf{Audio-Visual Guidance.} 
   \MethodName with audio-visual guidance produces more realistic expressions while listening. In audio-only guidance, the avatar might lose eye contact or not mirror a smile. In visual-only guidance, it might produce more smiles than needed.}
\label{fig:audioonly_visiononly_audiovisual}
\end{figure}



% \begin{table*}[t]
%   \centering
%   \begin{tabular}{@{}l|ccccccccc@{}}
%   \toprule
%     & \textit{Lip Sync} & \multicolumn{1}{c}{\textit{Realism}} & \multicolumn{2}{c}{\textit{Diversity}} & \multicolumn{2}{c}{\textit{AV-Alignment}} & \multicolumn{2}{c}{\textit{Audio Quality}} \\
%     \midrule
%     Method & F1$_{lips}$$\uparrow$ & FD$_{e}$$\downarrow$ & Div$_{h}$$\uparrow$ & Div$_{e}$$\uparrow$ & BC$_{h}$$\uparrow$ & BC$_{e}$$\uparrow$ & MCD$\downarrow$ & WER$\downarrow$ \\
%     \midrule
%     AV-Flow w/ Fusion (b) & 0.865 & \textbf{0.828} &  0.022 & \textbf{0.695} & \textbf{0.271} & 0.186 & 0.880 & \\
%     AV-Flow (Ours) & \textbf{0.964} & \textbf{0.861} & \textbf{0.029} & \textbf{0.680} & \textbf{0.258} & \textbf{0.229} & \textbf{0.900} & \textbf{0.157}\\
%     \bottomrule
%   \end{tabular}
%   \caption{\textbf{Ablation Study.} }
%   \label{tab:quant_ablation_2}
%   \vspace{-10pt}
% \end{table*}

\begin{table}[t]
  \centering
  \begin{tabular}{@{}l|ccc@{}}
  \toprule
    % & \textit{Lip Sync} & \multicolumn{1}{c}{\textit{Realism}} & \multicolumn{2}{c}{\textit{Diversity}} & \multicolumn{2}{c}{\textit{AV-Alignment}} & \multicolumn{2}{c}{\textit{Audio Quality}} \\
    % \midrule
    Method & F1$_{lips}$$\uparrow$ & F1$_{smiles}$$\uparrow$ & FD$_{e}$$\downarrow$ \\
    \midrule
    AV-Flow w/o guidance & \textbf{0.964} & 0.611 &  0.861 \\
    AV-Flow w/ guidance & 0.933 & \textbf{0.685} & \textbf{0.845} \\
    \bottomrule
  \end{tabular}
  \caption{\textbf{AV-Flow with or without Guidance.} In dyadic conversations, the proposed audio-visual guidance leads to more realistic reactions and facial expressions, while the avatar is listening to the user. Without guidance, our basic \MethodName achieves slightly more accurate lip synchronization.}
  \label{tab:quant_with_without_smirk}
  % \vspace{-10pt}
\end{table}


\noindent
\textbf{Audio-Visual Alignment.}
We design intermediate highway connections that enable communication between the audio and visual diffusion transformers.
In our ablation study in~\cref{sec:exp_ablation}, we notice that our proposed audio-visual fusion leads to the best correlation between audio and motion, as measured by the BC$_{h}$ and BC$_{e}$ metrics.
This audio-visual correlation is also shown in \cref{fig:audio_exp_correlation}. We observe patterns where the energy of the facial motion matches the energy of the corresponding synthesized audio (plotted as the normalized squared L2-norm of the generated facial dynamics and mel-spectrogram over time). We compare with the variant of the separate models, without any cross-modal connections, where the correlation is lower.

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.9\linewidth]{images/side_views.png}
   \caption{\textbf{Qualitative Results of \MethodName.} We show frontal and side views for corresponding phonemes. We use pre-trained personalized renderers~\cite{timur2021driving_renderer} that synthesize photo-realistic 3D avatars.}
   \label{fig:frontal_side_views}
\end{figure*}

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{images/audio_exp_correlation.png}
   \caption{\textbf{Audio-Visual Alignment.} Correlation between synthesized speech and facial motion by AV-Flow, compared to the variant of separate models without any connections. Energy is estimated as the normalized squared L2-norm of the generated facial dynamics and mel-spectrogram over time.}
   \label{fig:audio_exp_correlation}
\end{figure}


\noindent
\textbf{Additional Qualitative Results.}
\cref{fig:frontal_side_views} shows additional qualitative results of our method, rendered in frontal and side views. We use pre-trained personalized renderers~\cite{timur2021driving_renderer,ng2024audio2photoreal} that produce photo-realistic 3D avatars. Therefore, we can render the generated avatars from any viewpoint. In this work, we are mostly interested in the facial expression, lip synchronization and realistic head motion over time. We only provide the side views for completeness. We refer the interested reader to~\cite{timur2021driving_renderer,deep_appearance} for more details in Codec Avatars.

\noindent
\textbf{Video Results.}
We encourage the readers to watch our supplementary video.


\section{Implementation Details}

\noindent
\textbf{Text-to-Tokens.} 
Since our training dataset does not include any text annotations, we extract tokens (logits) from the raw audio using an ASR model (see~\cref{sec:method_representations}). At inference time, in order to be able to synthesize audio-visual content directly from text characters, we learn a text-to-tokens model. Inspired by the architecture proposed by Matcha-TTS~\cite{mehta2024matcha}, we first map the input text to phonemes. We learn phoneme embeddings (192-dimensional) that are passed through a text encoder of 3 1D convolutional layers. A duration predictor gives their duration. A diffusion transformer of 3 layers maps the features to logits, which can be used as input tokens to our model. We follow the rest hyperparameters, architecture and training with flow matching of Matcha-TTS~\cite{mehta2024matcha}. Our main difference is that we predict character-level logits, not mel-spectrograms. We use LJSpeech~\cite{ljspeech17} to train our text-to-tokens model.

\noindent
\textbf{Architecture.}
We use $N=8$ blocks for our audio and visual DiTs. We first project the inputs to 512-dimensional through a linear layer. Each transformer block has input and output dimensions of 512, hidden size of 1024, and 4 heads for the multi-head self-attention.
We use windows of 10 frames, looking only 2 frames in the future. With this windowing, we achieve only 120ms latency, as mentioned in~\cref{sec:experiments}. We have also tried windows of 20 frames, getting similar results, but a bit higher latency.
As described in~\cref{sec:method_architecture}, we upsample the data at 86fps to achieve exact correspondence between audio and video. We extract mel-spectrograms following the same extraction as BigVGAN~\cite{lee2022bigvgan}. In this way, we directly use the pre-trained BigVGAN as our vocoder to get the output speech signal. Our input tokens are extracted from a pre-trained Wav2Vec2 model with the base architecture, that is trained for ASR using 960 hours of unlabeled audio from the LibriSpeech dataset~\cite{paszke2019pytorch,panayotov2015librispeech,yang2021torchaudio}.

\noindent
\textbf{Training.}
During training, we use a batch size of 16 segments. Each segment corresponds to a duration of 20 seconds.
We set $\sigma_\text{min} = 10^{-6}$.
Our implementation is based on PyTorch~\cite{paszke2019pytorch}. We use AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $10^{-4}$, and hyperparameters $\beta_1 = 0.9$, $\beta_2 = 0.98$, $\epsilon = 10^{-9}$. We train \MethodName for about 36 hours (1 million iterations) on a single A100 GPU.


\section{Ethical Considerations}

We use the publicly available dataset proposed by Audio2Photoreal~\cite{ng2024audio2photoreal}. We also collected an additional dataset of 50 hours in a similar setting. Both datasets include dyadic conversations between individuals. The data include raw audio and video, as well as face expression codes and head poses for the actors, paired with pre-trained personalized renderers~\cite{timur2021driving_renderer,deep_appearance}. During collection of the data, we have followed appropriate procedures and all individuals have provided their full consent for our research work. Our model is identity-specific and thus, only those individuals can be rendered and no one else. This addresses ethical concerns of generating subjects without their consent, or generating misleading content. We have also used audio from the EARS dataset~\cite{richter2024ears} to test lip synchronization to custom input speech and the widely used LJSpeech~\cite{ljspeech17} to train our text-to-tokens module.

Although we have strictly followed all these procedures in collecting and using our data, we would like to note the potential misuse of similar technologies in generating photo-realistic human avatars. Apart from the benefits for education, virtual communication, healthcare, etc, there is still the possibility of generating misleading content. Research on fake content detection and forensics is crucial.
We intend to release our source code to help improving such research.



% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.