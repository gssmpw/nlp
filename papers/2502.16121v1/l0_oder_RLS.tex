\documentclass[lettersize,journal]{IEEEtran}
%\documentclass[landscape,onecolumn,draftclsnofoot,12pt]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[ruled]{algorithm2e}
\usepackage{amsthm}
\usepackage{amssymb}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
%\usepackage[ruled,linesnumbered]{algorithm2e}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

\begin{document}
	\title{From Target Tracking to Targeting Track — Part II: Regularized Polynomial Trajectory Optimization}
	\author{Tiancheng Li, \textit{IEEE Senior Member}, Yan Song, Guchong Li, Hao Li%Jingdong Chen, \textit{IEEE Fellow}
		\thanks{Manuscript created August 2024; \\
			This work was supported in part by the National Natural Science Foundation of China under Grants 62422117 and 62201316 %and 62071389, in part by the Natural Science Basic Research Program of Shaanxi Province under Grant 2023JC-XJ-22, 
			and in part by the Fundamental Research Funds for the Central Universities. 
			\\
			Tiancheng Li, Yan Song, Guchong Li and H. Li are with the Key Laboratory of Information Fusion Technology (Ministry of Education), School of Automation, Northwestern Polytechnical University, Xi’an 710129, China, E-mail: t.c.li@nwpu.edu.cn, syzx@mail.nwpu.edu.cn, guchong.li@nwpu.edu.cn, lihao714925@163.com. 
			Hao Li is also with Xi'an Branch of China Academy of Space Technology, Xi'an 710100, China
	}}
	
	\markboth{Journal of \LaTeX\ Class Files, August~2024}%
	{How to Use the IEEEtran \LaTeX \ Templates}
	
	\maketitle
	
	\begin{abstract}
		Target tracking entails the estimation of the evolution of the target state over time, namely the target trajectory. Different from the classical state space model, our series of studies, including this paper, model the collection of the target state as a stochastic process (SP) that is further decomposed into a deterministic part which represents the trend of the trajectory and a residual SP representing the residual fitting error. 
		%Different from the traditional point state-oriented Markov model, 
		Subsequently, 
		the tracking problem is formulated as a learning task regarding the trajectory SP %of which a core part is the mean function of time (FoT) that represents a mean estimate of the trajectory.  
		%track-oriented optimization problem that aims to establish and 
		for which a key part is to estimate a trajectory FoT (T-FoT) best fitting the measurements in time series. For this purpose, we consider the polynomial curve and address the regularized polynomial T-FoT optimization employing two distinct regularization strategies seeking trade-off between the accuracy and simplicity. 
		%optimization with two distinctive strategies of regularization, seeking trade-off between the fitting accuracy and the polynomial simplicity. % in consistent with the law of parsimony (Occam's Razor). %Two optimization approaches are proposed for solving this NP (non-deterministic polynomial)-hard problem. 
		One limits the order of the polynomial and then the best choice is determined by grid searching in a narrow, bounded range while the other adopts $\ell_0$ norm regularization for which the hybrid Newton solver is employed. 
		Simulation results obtained in both single and multiple maneuvering target scenarios demonstrate the effectiveness of our approaches. 
	\end{abstract}
	
	\begin{IEEEkeywords}
		Target tracking, trajectory function of time, regularization, polynomial fitting, recursive least squares
	\end{IEEEkeywords}
	
	
	\section{Introduction}\label{sec:Introduction}
	\IEEEPARstart{T}{he} online estimation of the trajectory of a moving target such as airplane, robot, missile, etc., called \textit{target tracking}, has been a persistent and prominent research topic, with wide-ranging implications in aerospace, traffic management and defense, among many others \cite{bar2004estimation,vo2015multitarget}. % It represents a crucial aspect of many applications, and its continuous investigation and development are of paramount importance. 
	The classic methodology, exemplified by a wide variety of Bayesian filters, is to design a state space model (SSM) \cite{Sarkka13book} consisting of a Markov-jump model to describe the dynamics of the target and a measurement model to relate the measurement with the state of the target. The SSM-based methods can be divided into two major classes, depending on whether the ground truth state is assumed to be a deterministic or random variable, which leads to two distinctive groups of estimators, as shown on the left part of Fig.\ref{fig:Taxonomy}. One is the minimum mean square error (MMSE) driven point state estimator such as the milestone Kalman filter, % \cite{kalman60}
	Gaussian mixture filter, %Student's $t$ filter, 
	and so on \cite{Li2017AGC}. The other is the Bayesian risk-driven density estimator  such as the particle filter and various random finite set filters \cite{vo2015multitarget}. 
	
	%The classic approach to target tracking since the milestone Kalman filter \cite{kalman60} is utilizing a dynamic (e.g., Markov-type) model to describe the movement of the target and a measurement model to relate the measurement of the sensor(s) with the state of the target. In addition, appropriate models are needed to characterize the false and missing data \cite{Bar-Shalom01,Sarkka13book}. All of these render the optimal state estimation in the minimum mean square error or Bayesian sense. In this case, the metric needed for estimate evaluation is applied on the point set, either regarding a single target or multiple targets as illustrated in Fig. \ref{fig:diff_track} (a) and (b), respectively.  
	
	\subsection{Relevant Work}
	The actual motion modes are typically unknown in practice and even time-variant, that is, maneuvering \cite{li2005survey}, or simply are too complicated to be sufficiently described \cite{Zhang24L-GBM}. In addition, appropriate models are needed to characterize false, missing and even irregular data \cite{bar2004estimation,Sarkka13book}, which is also intractable. % if not impossible. 
	%{These have imposed a great challenge to the performance of most filters \cite{Li17AGC} and led to fruitful research on the SSM identification; see e.g. \cite{Dunik17} for many manifold strategies for estimating the noise covariance alone.}  
	In the absence of accurate a priori knowledge, model mismatching and model identification delay will all lead to an ineluctable estimation error \cite{li2016effectiveness,fan2011impact,xiang2018impact,zhang2022fast}. In particular, various data-driven approaches have recently been proposed based on the SSM or for replacing some calculation components of the SSM for which the readers are referred to  \cite{gao2019long,Ghosh2024Dense} %Ghosh2024datadriven
	and \cite{Pinto2023DMTT,Zhang24transformer,Zhang24L-GBM} and the references therein. %, respectively. %, for non-cooperative targets, there is a great challenge to identify the target dynamics in order to properly set up a filter. 
	
	
	Instead of estimating the {discrete-time point state} of the target based on a meticulously designed SSM% which deals with ``tracking'' as a standard state estimation problem
	, we are interested in estimating the {continuous-time trajectory} which is given by a curve function of time (FoT) \cite{li2018joint,li2023target}. %That is, compared with the classic SSM-driven tracking approaches that estimate the point state of the target in time series, our approach aims to estimate the trajectory FoT (T-FoT) over a sliding time window. %, namely \textit{targeting the track}. 
	This class of data-driven methods is Markov-free and aims to estimate the trajectory directly rather than the state, as illustrated on the right part of Fig. \ref{fig:Taxonomy}. 
	The T-FoT not only provides the dynamics information of the target such as the position, velocity, and acceleration, but also the high-level information such as the movement pattern of the target and smoothness of the trajectory, making it advantageous in comparing with traditional point estimators. Even more importantly, it inherently accommodates unevenly-arrived measurements and stochastic correlation between measurement noises over time \cite{Li25TFoT-part3}.  
	Relevant attempts on continuous-time trajectory modeling can be found in %found in \cite{Rudd94,Wang94,Anderson-Sprecher96,Furgale12,Pacholska20}. In fact, the trajectory spatio-temporal feature has also been implicitly utilized in the deep learning based tracking approaches \cite{Pinto2023DMTT,Zhang24transformer,Zhang24L-GBM}. The readers are kindly referred to 
	the cutting-edge survey for continuous-time state estimation in a broader realm of robotics \cite{Talbot2024continuoustimestate}. However, most of these approaches, including our previous work \cite{li2016fitting,li2023target}, assume the deterministic real state/trajectory and do not provide uncertainty about the trajectory estimate. This identified research gap will be explored and resolved in this work and the companion papers.  
	%The position, velocity, and acceleration of the target at any particular time, and even some high-level information such as the motion model/pattern of the target, can all be inferred from the trajectory. 
	
	% Existing approaches to target trajectory estimation, modeling the trajectory whether as a discrete-time series of position points or as a continuous-time curve, can be found in the literature study given in our previous work \cite{li2016fitting,li2023target}. 
	% {%This paper will extend our previous work on the continuous time trajectory study. 
		% The latter which describes the movement of the target in the continuous-time domain and embraces data-driven approaches is the focus of this paper.}
	 
	%Basically, existing target tracking approaches can be grouped into two general classes: model-driven and data-driven. Model-driven tracking methods, exemplified by a variety of Bayesian filters, typically involve the design of a state space model (SSM) that captures the dynamics of the target through the Markov-jump model, as well as the measurement model \cite{li2016fitting}. However, except for the measurement model, these models are not available in advance and are difficult to identify accurately online. Additionally, the unpredictability of target motion patterns presents a significant challenge in maneuvering target tracking.  The multiple model approach is often considered as the primary approach to solve this mixed estimation problem \cite{li2005survey}. In particular, the interacting multiple model (IMM) algorithm \cite{blom1988interacting} and the variable structure multiple model (VSMM) algorithm [5] are the most commonly used. The model set of the former is fixed, while the latter uses a variable model set, adaptive selected from a wide range of models, and operating many models in parallel with high computational costs \cite{zhang2022fast}.
	
	%Although these multiple model algorithms have been extensively applied in maneuvering target tracking \cite{mazor1998interacting,johnston2001improvement}, traditional model-based algorithms still face several challenges: Markov assumptions can sometimes be insufficient to accurately describe  actual processes, model decision delays, and model mismatches. Various model-based methods have been explored to address these issues. To tackle the limitations of the first-order Markov assumption in describing dynamic processes, the  IMM algorithm based on a second-order Markov chain was proposed in \cite{blair1994estimation}, and an IMM algorithm based on a higher-order Markov chain was introduced in \cite{suchomski2001high}. Unfortunately, these methods remain computationally intensive, particularly when multiple dynamic models are considered. Additionally, the impact of model decision delay on state estimation error is examined in \cite{fan2011impact} and \cite{xiang2018impact,zhang2022fast}.
	

	
	
	\begin{figure}[htbp]
		\centerline{\includegraphics[width=0.9\columnwidth]{FrameworkNovelty.pdf}}
		\caption{A taxonomy of existing prominent estimators for target tracking}
		\label{fig:Taxonomy}
	\end{figure}
	
	\subsection{Introduction to Companion Papers}
	% Our series of works are built on modeling the target trajectory by a curve function of time (FoT), which was referred to as the trajectory FoT (T-FoT) \cite{li2018joint,li2023target}. %, rather than by any discrete-time Markov model. 
	% In other words, the evolution of the target state over time is modeled in the spatio-temporal space as 
	% \begin{equation} \label{eq:T-FoT}
		% {\mathbf{x}_t} = f(t), 
		% \end{equation}
	% where $t\in \mathbb{R}^+$ denotes the time, $\mathbf{x}_t\in \mathcal{X}$ represents the target state at time $t$, $f:\mathbb{R}^+ \rightarrow \mathcal{X}$ the T-FoT and $\mathcal{X}$ the state space.
	
	Our series of studies are founded on modeling the target trajectory using a trajectory function of time (T-FoT) \cite{li2018joint,li2023target}. %, rather than by any discrete-time Markov model. 
	That is, the evolution of the target state over time is modeled by T-FoT $f:\mathbb{R}^+ \rightarrow \mathcal{X}$ in the spatio-temporal space and %$\mathbf{x}_t\in \mathcal{X}$ represents 
	the target state at time $t$ is given by  
	\begin{equation} \label{eq:T-FoT}
		{\mathbf{x}_t} = f(t), 
	\end{equation}
	where $t\in \mathbb{R}^+$ denotes the time and $\mathcal{X}$ denotes the state space.
	
	% However, the basic T-FoT approach \cite{li2018joint,li2023target} does not provide uncertainty about the estimate of the T-FoT. Moreover, it fails to utilize any information about the correlation between the states at different times. To put it simply, in a smooth curve, the points corresponding to close times are statistically spatially closer to each other than those far away. 
	To take advantage of the latent state-over-time correlation and to provide an assessment of the uncertainty associated with the estimate of the T-FoT, within our series of companion papers \cite{Li25TFoT-part2,Li25TFoT-part3} including this one, we further model %the T-FoT  $f$ of 
	the collection of the target states in time series as a stochastic process (SP) $\mathcal{F}\triangleq \{\mathbf{x}_t: t\in \mathbb{R}^+ \} $.  %\left( \cdot;\varTheta \right)  
	%specified by parameters $\varTheta $. 
	That is, any T-FoT is a sample path of this SP, that is,
	\begin{equation}
		f\sim \mathcal{F} .
	\end{equation}
	%the continuous time-space series Trajectory Function of Time (T-FoT) satisfies:
	% \begin{equation}
		% f\sim \mathcal{F}. \label{eq:TSP}
		% \end{equation}
	% %where $m(\cdot)$ is the mean FoT of the TSP.  
	
	Based on the deterministic-stochastic decomposition approach rooted in Wold and Cram\'er's decomposition theorems \cite{Wold1938,Cramer1961,Box1994}, the SP can be decomposed into a deterministic FoT that captures the trend of the SP $\mathcal{F}$ and the residual SP (RSP) contains all the randomness, i.e., 
	\begin{equation}
		f(t)=F(t;\mathbf{C})+\epsilon(t) \label{eq:cramer}
	\end{equation}
	where the deterministic FoT $F(\cdot; \mathbf{C})$ is specified by parameters $\mathbf{C}$ and $\epsilon(\cdot) \sim \mathcal{E}(\cdot; \varTheta) $ denotes the RSP specified by parameters $\varTheta$. Note that $\epsilon(t)$ was interpreted as the fitting error of $F(t; \mathbf{C})$ to $f(t)$ in \cite{li2018joint,li2023target} but no SP model was built for it. % which is 
	
	The contributions of our series of companion papers including three parts are structured as follows.  
	\begin{itemize}
		\item Part I \cite{Li25TFoT-part1} proposed a metric for evaluating the quality of any T-FoT estimate $\hat{f}$, which will be used in this paper. This metric actually provides a distance between any two trajectories given in terms of FoT. 
		\item Part II, as detailed in this paper, addresses the online fitting of a specific polynomial FoT $F(\cdot;\mathbf{C})$ subject to regularized optimization. %i.e., learning the mean FoT $m(\cdot)$ which represents the trend of the TSP. % with two distinct strategies of , 
		Two distinct regularization strategies are proposed to strike an optimal balance between fitting accuracy and T-FoT simplicity. 
		\item Part III \cite{Li25TFoT-part3} offers solutions for learning the RSP $\epsilon(\cdot) \sim \mathcal{E}(\cdot; \varTheta) $ (as well as the correlation between the measurement noises) for which two specific representative SPs are considered, respectively: the Gaussain process (GP) and Student's $t$ process (StP).
		% \item Part II \cite{Li25TFoT-part2} and Part III \cite{Li25TFoT-part3} offer solutions to online learning of the SP via decomposing it into a deterministic FoT and the residual zero-mean SP based on the Cram\'er's theory \cite{Cramer1961}. The former estimates the best polynomial FoT based on regularized optimization %that employs two distinct strategies of regularization.  %, seeking trade-off between the accuracy and simplicity. 
		% %\item offers solutions for 
		% while the latter estimates %the residual zero-mean SP %hyperparameters $\varTheta $ 
		% %for 
		% two representative SPs: the Gaussian process (GP) and Student's $t$ process (StP).
	\end{itemize}
	
	
	%\subsection{Introduction to Companion Papers}
	% Formally speaking, the movement of the target is described by a spatio-temporal trajectory function of time (T-FoT) \cite{li2018joint,li2018single,li2023target} as follows
	% \begin{equation}\label{eq:T-FoT}
		% \mathbf{x}_t=f(t),
		% \end{equation}
	% where $t\in \mathbb{R}^+$ denotes the time, $\mathbf{x}_t\in \mathcal{X}$ represents the target state at time $t$, $f: \mathbb{R}^+ \rightarrow \mathcal{X}$ the T-FoT such as a polynomial and $\mathcal{X}$ the state space. %It avoids the difficulty of Markov and process noise modeling, yielding a data-driven tracking approach needing little a-priori information \cite{li2023target,li2018joint,li2018single}.
	
	% %The current research focuses on identifying the functional form of $f(t)$, with the objective of obtaining a smooth trajectory while minimizing the online fitting error of the measurement.  We employ the fundamental polynomial function to determine  $f(t)$  based on the principle of Taylor expansion. 
	% Once the T-FoT is obtained, the position (and even the velocity, and acceleration which correspond to the first and second order derivatives of the position against time) %(and its first and second order derivatives over time which indicate the velocity and acceleration, respectively) %; see, e.g., \citep{Pilte17}
	% of the target at any particular time can be easily inferred from the trajectory. %Moreover, high-level information such as the class/model/pattern/feature of the target and so on may be able to be inferred from the continuous-time trajectory but not from a series of discrete point estimates. 
	
	\subsection{Contribution and Organization of This Paper} 
	% Different from the classic \textit{target tracking} framework that estimates the point state $\mathbf{x}_k$ of the target on discrete measurement times $k=1,2,\cdots$, we are now \textit{targeting the track}, namely estimating the T-FoT $f$ %, as shown in \eqref{eq:TSP}, 
	% based on the measurements obtained in time series. 
	% More formally speaking, we further parameterize the mean function $m\left(t\right)$ 
	% %parameterize $f(t)$ by an $\gamma $-order polynomial $F\left( t;C_k \right) $ with parameters $C_k=\left\{ c_0,c_1,\cdots ,c_{\gamma} \right\} $ \cite{li2018joint}, 
	% as $F(t;\mathbf{C})$ by utilizing a set of coefficients $\mathbf{C}$, which are estimated based on the available measurements. That is,  
	% \begin{equation}
		% m(t)=F(t;\mathbf{C})+ \epsilon (t),
		% \end{equation}
	% where $\epsilon(t)$ represents the approximate error between the parameterized FoT $F(t;\mathbf{C})$ and the mean function $m(t)$. 
	% %By employing a parameterization technique for the function $f(t)$, the initial estimate \eqref{eq:T-FoT} undergoes a transformation, resulting in the estimation of the parameters $C$.
	%
	% If the estimate $F(t;\mathbf{C})$ is unbiased in the sense that $\mathbb{E}[\epsilon(t)] =\mathbf{0}$, the following residual function will be subject to a zero-mean SP
	% \begin{equation}
		% 	e\left( \cdot \right) =f\left( \cdot \right) -F\left( \cdot;C_k \right) \sim \mathcal{G}\mathcal{P}\left( \boldsymbol{O},\kappa \left( \cdot, \cdot \right) \right) .
		% 	\label{eq:e_t_modeling}
		% \end{equation}
	We redefine the classic target tracking problem as an T-FoT-oriented SP learning task based on deterministic-stochastic decomposition \eqref{eq:cramer}. By this, learning the T-FoT $F(t;\mathbf{C})$ and the fitting error $e\left( t \right)$ constitutes the core of our two contributions to SP-based tracking including this paper and the companion paper \cite{Li25TFoT-part3}, respectively. 
	Our first contribution in this paper is to apply the most known polynomial trend decomposition \cite{Hodrick1997postwar,Urbin2012time}, i.e., we assume the trend T-FoT $F(t;\mathbf{C})$ by a polynomial. Although polynomial fitting in a sliding time window has been implemented earlier \cite{Rudd94,Wang94,Anderson-Sprecher96,li2018joint,li2023target}, no SP modeling and decomposition has been recognized. However, the polynomial \cite{Fan96LocalPolynomial} is quite flexible and can be analytically optimized, but faces two notable risks, namely underfitting and overfitting. To avoid this, we propose two distinctive strategies for regularization in order to select a proper polynomial order or a sparse number of polynomial parameters. It should also be noted that the RSP will usually be zero mean if the learned trend T-FoT $F(\cdot;\mathbf{C})$ is chosen as the mean function of the RSP; see the proof given in \cite{Li25TFoT-part3}. Therefore, the learning of $F(\cdot;\mathbf{C})$ plays also an important role in determining the RSP. %This overcomes the limitation of our 
	%with regularization and  analytically optimized. 
	%However, data fitting always faces two notable risks a.k.a. underfitting and overfitting. In the case of polynomial fitting, %the former typically manifests when a polynomial of insufficient order is selected to model a complex data relationship while the latter generally transpires when a polynomial of excessively high order is chosen \cite{sapatinas2004elements}. Therefore, 
	%it is crucial to select a proper fitting order or a suitable number of fitting parameters. %for which we employ the $\ell_0$ regularization. 
	%The resulted optimization is, however, often non-deterministic polynomial (NP) hard for which we propose two solvers of different complexities. 
	
	% This is the second part of two companion papers on the T-FoT approach. The first part contributed a metric for evaluating the T-FoT estimate \cite{Li25TFoT-part1}, % in the general case of multiple targets, 
	% which has been used for performance evaluation in this paper. 
	
	The remainder of this paper is organized as follows. Preliminary work is addressed in Section \ref{sec:Preliminaries}, including the constrained T-FoT model and $\ell_0$-regularization optimality. % with constraint. %Section \ref{sec:Optimization} formulates the joint optimization problem of trajectory estimation. 
	Consequently, two distinctive approximate solvers for the optimization problem are given in Sections \ref{sec:ORLS} and \ref{sec:Newton}, respectively. % model approximate optimization algorithm designed to control the highest order. The Newton method for T-FoT $\ell_0$ optimization is proposed in Section .  
	Extension to the multiple target trajectory case is briefly discussed in Section \ref{sec:extension-MTT}. The simulation results are given in Section \ref{sec:simulation} before the paper is concluded in Section \ref{sec:conclusion}. 
	
	
	\section{Preliminaries}\label{sec:Preliminaries}
	% In this paper, we denote the state of the target at discrete measurement time $k \in \mathbb{N}^+ $ as $\mathbf{x}_k\in \mathcal{X}$, which is essentially a point on %the target trajectory that can be expressed by a function of continuous time $t\in \mathbb{R}^+$, namely 
	% the T-FoT $f: \mathbb{R}^+ \rightarrow \mathcal{X}$, where $\mathbb{N}^+$ and $\mathbb{R}^+$ denote the positive integer set and positive real number set, respectively. 
	
	
	%The following Cram\'er's decomposition theory supports our choice of the polynomials for determining $F(t;\mathbf{C})$:
	%Obviously, this significantly extends our previous work  \cite{li2018joint,li2023target} where the learned T-FoT $F(\cdot;\mathbf{C})$ is used to approximate the actual T-FoT $f(\cdot)$. 
	%In this paper and \cite{Li25TFoT-part3}, the estimated $F(\cdot;\mathbf{C})$ is an estimate of the mean function of the TSP. %, although the estimation method is the same. 
	
	%In the following, we will focus only on the learning of T-FoT $F(\cdot;\mathbf{C})$ from the measurements; the learning of the residual function will be addressed in the companion paper \cite{Li25TFoT-part3}.  
	%, which actually approximate the actual T-FoT too
	% The key role of this paper is on learning the fo
	% \begin{equation}
		%     f(t)\approx F(t;\mathbf{C})
		% \end{equation}
	
	% \begin{theorem}\cite{}
		%     any time series (stochastic process) can be decomposed into the superposition of two parts. One part is a deterministic trend component determined by a polynomial, and the other part is a stationary zero-mean error component. 
		% \end{theorem}
	
	
	\subsection{Polynomial T-FoT}
	By harnessing the principle of Taylor series expansion, it becomes feasible to employ higher-order polynomials to represent any %intricate 
	smooth FoT. 
	%In particular, a highly effective strategy for achieving precise approximation involves parameterizing the trajectory using polynomials that span multiple state dimensions. 
	Polynomial fitting has proven effective in fitting measurements \cite{Tian22PolyFit} and has demonstrated superiority and flexibility in T-FoT modeling in our earlier works \cite{li2018joint,li2018single,li2023target}. The polynomial T-FoT that spans multiple state dimensions is simply given as follows 
	\begin{equation}\label{eq:polynomial}
		F\left ( t;\mathbf{C}_\gamma \right )= \sum_{i=0}^{\gamma }\mathbf{c}_{i}t^i,
	\end{equation}
	where $\gamma $ refers to  the order of the fitting function which may be given exactly in advance or specified with a higher bound, $\mathbf{C}_\gamma=\left\{ \mathbf{c}_i \right\} _{i=0,1,\cdots ,\gamma}$ represents the polynomial trajectory coefficients, $\mathbf{c}_i= \big[c_i^{(1)}, c_i^{(2)}, \cdots, c_i^{(r)}  \big]$, $r$ indicates the dimension in the concerning state space $\mathcal{X}$, %namely the position coordinates only in this paper, 
	$\mathbf{c}_0,\mathbf{c}_1,\mathbf{c}_2$ correspond to the initial position, velocity, and acceleration of the target. %, $$ corresponds to its , and $$ corresponds to its .
	
	In many real-world applications, the trajectory is only defined in the position space. This is preferable when measurements are made on the position. % since the fitting performs the best merely in the state space that is measured directly. 
	Moreover, by computing the derivatives of the position T-FoT with respect to time $t$,  we can estimate the velocity and acceleration of the target as follows:
	\begin{equation}
		\frac{\partial f\left( t \right)}{\partial t}\Bigg|_{t=0}  =\mathbf{c}_1, ~~~
		\frac{\partial ^2f\left( t \right)}{\partial ^2t}\Bigg|_{t=0}  =\mathbf{c}_2. \label{eq2.3}
	\end{equation}
	
	As a rule of thumb, $\gamma = 1$ and $\gamma = 2$ are sufficient to model constant velocity and constant acceleration, respectively. This validates the interpretability of the T-FoT model and an advantage in comparison with some other forms of FoT such as the B-spline \cite{Hadzagic2011batchSpline,Furgale12,Tirado2022Spline} and so on \cite{Pacholska20}.  
	
	\subsection{Sliding Time-Window T-FoT Fitting}
	In order to manage the complexity of the track function, it is common to use a time window to estimate the trajectory parameters. The default time window is given in a sliding manner as $K = [k', k]$, where $k'=\max(1,k-T_\text{w})$, $k$ denotes the current time, $T_\text{w}$ represents the supposed maximum length of the time window, and the operator $\max(a,b)$ produces the maximum between $a$ and $b$. 
	
	%Mathematically, the observation function is commonly formulated in discrete time as
	%\begin{equation}\label{eq:measurement}
	%z_k=h\left( x_k,v_k \right),
	%\end{equation}
	%where $h(\cdot)$ and $v_k$ denote the measurement function and noise at time $k$, respectively.
	
	The real target T-FoT $f(t)$ %as well as the real target state $\mathbf{x}_k$ at any time $k$ 
	is measured in discrete time-series, i.e., 
	\begin{equation}\label{eq:measurement}
		\mathbf{y}_k=h_k\left( f(k),\mathbf{v}_k \right), 
	\end{equation}
	where $\mathbf{y}_k \in \mathcal{Y}$ denotes the measurement received at time $k\in \mathbb{N}^+$, $\mathcal{Y}$ denotes the measurement space, $h_k(\cdot,\cdot): \mathcal{X} \times \mathcal{V} \rightarrow \mathcal{Y}$ and $\mathbf{v}_k \in \mathcal{V}$ denote the measurement function and noise at measuring time $k$, respectively.
	
	%Combining the measurement function \eqref{eq:measurement} and the T-FoT \eqref{eq:T-FoT} yields an optimization problem with the goal of finding the best polynomial T-FoT  as an estimate of $f (t)$, by minimizing the measurement data fitting error, i.e.,
	To minimize the fitting error $\epsilon(t)$ as given in \eqref{eq:cramer}, the parameters of the fitting T-FoT should be determined as follows
	\begin{equation} \label{eq:C_k}
		\hat{\mathbf{C}}_{\gamma}=\underset{\mathbf{C}_{\gamma}}{\text{arg}\min} \mathcal{D}_K(\mathbf{C}_{\gamma}) %\sum_{t=k'}^k{\lVert z_t-h_t\left( F\left( t;C_\gamma \right) ,\bar{v}_t \right) \rVert^2_{\text{var}(z_t)}}
		,
	\end{equation}
	where the data fitting error as adopted in this paper is defined in the least squares (LS) sense  
	\begin{equation}
		\mathcal{D}_K(\mathbf{C}_{\gamma})\triangleq \sum_{t=k'}^k{\lVert \mathbf{y}_t-h_t\left( F\left( t;\mathbf{C}_\gamma \right) ,\bar{\mathbf{v}}_t \right) \rVert^2_{\text{var}(\mathbf{y}_t)}},
	\end{equation} 
	where 
	%\(\left\| {{\rm{a}} - {\rm{b}}} \right\|\) is the measure of the distance between \({\rm{a}}\) and \({\rm{b}}\), 
	\(\bar{\mathbf{v}}_t\) denotes the expectation of the measurement error, \(\text{var}(\mathbf{y}_t)\) denotes the variance of the measurement $\mathbf{y}_t$ and \( \left \|  \mathbf{z} \right \|^{2}_P \) denotes the normalized \(\ell_2\)-norm distance, also known as the Mahalanobis distance, as follows %, \(k\) is  the present time and  \(k' \) is the starting time of the sliding time window. 
	%We know
	\begin{equation}
		\|  \mathbf{z}  \|^{2}_\mathbf{P} \triangleq \mathbf{z}^\mathrm{T}\mathbf{P}^{-1}\mathbf{z}.
	\end{equation}
	It is known that the above weighted LS estimate is also the maximum likelihood estimate if $\mathbf{y}_t-h_t\left( F\left( t;\mathbf{C}_\gamma \right) ,\bar{\mathbf{v}}_t \right)$ follows a zero-mean Gaussian distribution. 
	
	% \begin{equation}\label{eq:C_k}
		% \hat{C}=\underset{C}{\text{arg}\min}\left( \sum_{t=k'}^k{\lVert z_t-h_t\left( F\left( t;C \right) ,\bar{v}_t \right) \rVert^2_2} \right) ,
		% \end{equation}
	% where  \(\left\| {{\rm{a}} - {\rm{b}}} \right\|\) is the measure of the distance between \({\rm{a}}\) and \({\rm{b}}\), \(\left \|  \cdot \right \| _{2} \) denotes the \(\ell_2\)-norm,  and \(\bar{v}_t\) is an averaging factor employed to compensate for observation errors.
	
	To be more general, in the presence of any a-priori model information or system constraint, it may be incorporated into
	the optimization cost function by adding a
	regularization factor $\varOmega _F(\mathbf{C}_{\gamma})$ as a measure of the disagreement of the fitting function with the a-priori model constraint, leading to
	\begin{equation} \label{eq:Const-T-FoT-Op}
		\mathrm{\bf Problem ~ 1}: ~~ \hat{\mathbf{C}}_{\gamma}=\underset{\mathbf{C}_{\gamma}}{\text{arg}\min}\left( \mathcal{D}_K(\mathbf{C}_{\gamma})+\lambda \varOmega _F\left( \mathbf{C}_{\gamma} \right) \right),
	\end{equation}
	where the coefficient $\lambda$ trades off between the data fitting error and the model/constraint fitting error. %, which is often referred to as a penalty coefficient. 
	
	In this paper, we will consider two regularization functions $\varOmega _F\left( \mathbf{C}_{\gamma} \right) \triangleq \gamma +1 $ and $\varOmega _F\left( \mathbf{C}_{\gamma} \right) \triangleq  \rVert \mathbf{C}_{\gamma}  \rVert _0 $, respectively, where  $\lVert \mathbf{C} \rVert _0$ denotes the $\ell_0$ norm of $\mathbf{C}$, counting the number of non-zero elements of $\mathbf{C}$. Two optimization solvers are proposed in the following Sections \ref{sec:ORLS} and \ref{sec:Newton}, respectively. One employs a grid-searching approach based on an upper bound of the optimal order of the polynomial while the other employs the hybrid Newton method. 
	
	
	
	\subsection{$\ell_0$-regularization Optimality}
	There are some useful definitions and results on the smoothness, convexity and optimality of the T-FoT. %we introduce a $\tau$-stationary point,  an optimality condition of 
	Consider the following optimization,
	\begin{equation}\label{eq:l0_form}
		\underset{\mathbf{C}\in \mathbb{R}^n}{\min}\ g\left( \mathbf{C} \right) +\lambda \lVert \mathbf{C} \rVert _0.
	\end{equation}
	
	
	\begin{definition}[L-Smooth]
		Let $L \ge 0$. A function $g$ is said to be L-smooth if there exists $\bigtriangledown g$ (differentiable) and the following inequality holds for all $\mathbf{u},\mathbf{w}\in \mathbb{R}^n$ \cite{beck2017first}, %  and satisfies  
		\begin{equation}\label{eq:smooth}
			\lVert \bigtriangledown g\left( \mathbf{u} \right) -\bigtriangledown g\left( \mathbf{w} \right) \rVert _*\le L\lVert \mathbf{u}-\mathbf{w} \rVert,
		\end{equation}
		where $\lVert \cdot \rVert$ and $\lVert \cdot \rVert _*$ are a pair of dual norms, where $\lVert \mathbf{u} \rVert _*=sup\left\{ \mathbf{u}^\text{T} \mathbf{v}|\lVert \mathbf{v} \rVert \le 1 \right\}$, the constant $L$ is called the smoothness parameter.
	\end{definition}
	
	The first useful result on L-smooth functions is the descent lemma, which states that they can be upper bounded by a certain quadratic function. 
	\begin{lemma}[{descent lemma}] Let $g$ be an L-smooth function ($L \ge 0$). Then  for any $\mathbf{u},\mathbf{w}\in \mathbb{R}^n$,
		\begin{equation}
			g\left( \mathbf{w} \right) \le g\left( \mathbf{u} \right) +\left< \bigtriangledown g\left( \mathbf{u} \right) ,\mathbf{w}-\mathbf{u} \right> +\left(L/2\right)\lVert \mathbf{w}-\mathbf{u} \rVert ^2,
		\end{equation}
		where $\left<\cdot ,\cdot \right> $ represents the inner product. 
	\end{lemma}
	The proof can be found in \cite[Lemma 5.7]{beck2017first}.
	
	\begin{definition}[$\ell$-strongly convex]
		A function $g$ is called $\ell$-strongly convex for a given $\ell>0$ if {\rm dom}$(g)$ is convex and the following inequality holds for any $\mathbf{u},\mathbf{w}\in$ {\rm dom}$(g)$ and $\varphi \in \left [ 0,1 \right ] $,
		\begin{align}
			g( \varphi \mathbf{u} +\left(1-\varphi \right)\mathbf{w} ) \le  &  \varphi g\left( \mathbf{u} \right) +\left(1-\varphi \right)g\left( \mathbf{w} \right) \nonumber \\
			& -\frac{\ell}{2}\varphi\left(1-\varphi \right)\lVert  \mathbf{u}-\mathbf{w} \rVert ^2.
		\end{align}
	\end{definition}
	
	\begin{lemma}
		If there exists $\bigtriangledown g$ (differentiable), the function $g$ is strongly convex with a constant $\ell>0$  if
		\begin{equation}\label{eq:convex}
			g\left( \mathbf{w} \right) \ge g\left( \mathbf{u} \right) +\left< \bigtriangledown g\left( \mathbf{u} \right) ,\mathbf{w}-\mathbf{u} \right> +\left(\ell/2\right)\lVert \mathbf{w}-\mathbf{u} \rVert ^2.
		\end{equation}
	\end{lemma}
	The proof can be found in \cite[Theorem 5.24]{beck2017first}.
	
	\begin{definition}[$\tau$-stationary]
		A point $\mathbf{C}\in \mathbb{R}^n$ is called a $\tau$-stationary point  \cite{zhou2021newton} of the problem \eqref{eq:l0_form} if there is a $\tau >0$ satisfying
		\begin{equation}\label{eq:tau_stati_point}
			\begin{aligned}
				\mathbf{C} &\in \text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( \mathbf{C}-\tau \bigtriangledown g\left( \mathbf{C} \right) \right) \\
				& \triangleq \underset{\mathbf{D}\in \mathbb{R}^n}{\text{arg}\min}\frac{1}{2}\lVert \mathbf{D}-\left( \mathbf{C}-\tau \bigtriangledown g\left( \mathbf{C} \right) \right) \rVert ^2+\tau \lambda \lVert \mathbf{D} \rVert _0.
			\end{aligned}
		\end{equation}
	\end{definition}
	
	Consider now the proximal operator $\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}$ \cite{attouch2013convergence} %.  The operator $\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}$ can be 
	expressed in a closed form as follows
	\begin{equation}
		\begin{aligned}
			&\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( \mathbf{C} \right)=  \\
			&\left\{\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( \mathbf{c}_0 \right),\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left(\mathbf{c}_1 \right),\cdots,\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( \mathbf{c}_\gamma \right)\right\},
		\end{aligned}
	\end{equation}
	where
	\begin{equation}\label{eq:prox}
		\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( \mathbf{c}_i \right)  = \begin{cases}
			0,&\left| \mathbf{c}_i \right|<\sqrt{2\tau \lambda}\\
			0\ or\ \mathbf{c}_i,&\left| \mathbf{c}_i \right|=\sqrt{2\tau \lambda} \\
			\mathbf{c}_i,& \left| \mathbf{c}_i \right|>\sqrt{2\tau \lambda}
		\end{cases}
	\end{equation}
	
	The stationarity is a necessary condition for the local optimality of problem \eqref{eq:l0_form}. In case of convex $g$, it is a necessary and sufficient global optimality condition \cite{beck2018proximal}.
	
%\subsection{Model and Scenario Assumptions}
%
%The measurement model of the sensor is characterized as linear, indicating that it measures the position information of the target. The position measurement $z_k=\left[ \begin{array}{c}
%	z_{k}^{\left( 1 \right)}\\
%	z_{k}^{\left( 2 \right)}\\
%\end{array} \right] $ is given by
%
%\begin{equation}\label{eq2.6}
%z_k=\left[ \begin{array}{c}
%	p_{x,k}\\
%	p_{y,k}\\
%\end{array} \right] +v_k,
%\end{equation}
%where $\left[ p_{x,k},p_{y,k} \right] ^{\text{T}}$ is the position of the target.
%
%In addition, we take into account position-relevant measurements that can be converted to positions at each scan. This implies that the measurement function is injective \cite{li2016effectiveness} or multiple sensors being used \cite{li2017clustering,li2018distributed}. For instance, the range-bearing measurement model can be mathematically represented as follows
%\begin{equation}\label{eq2.7}
%\left[ {\begin{array}{*{20}{c}}
%{{r_k}}\\
%{{\theta _k}}
%\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
%{\sqrt {p_{x,k}^2 + p_{y,k}^2} }\\
%{{{\tan }^{ - 1}}\left( {\frac{{p_{y,k}^{}}}{{p_{x,k}^{}}}} \right)}
%\end{array}} \right] + {v_k}.
%\end{equation}
%which can be converted to the position measurements by
%\begin{equation}\label{eq2.8}
%{z_k} = \left[ {\begin{array}{*{20}{c}}
	%{{r_k}\cos \left( {{\theta _k}} \right)}\\
	%{{r_k}\sin \left( {{\theta _k}} \right)}
	%\end{array}} \right].
	%\end{equation}
	
	\section{Polynomial T-FoT with Limiting Order} %\label{sec:Optimization} 
	\label{sec:ORLS}
	
	%In this section, we consider a polynomial-T-FoT. Mathematically, 
	Polynomial order $\gamma$ is a critical parameter in trajectory fitting \cite{stoica2004model}, and its value may exhibit time-varying characteristics in response to the maneuvering behavior of the target.
	The improper fitting model faces two notorious challenges, a.k.a. underfitting and overfitting due to underestimated and overestimated polynomial order, respectively. As illustrated in Figs. \ref{fig:diff_order_poly} (a) and (c), the former manifests itself when a polynomial of insufficient order is selected %to model a complex data relationship 
	while the latter typically transpires when a polynomial of excessively high order is chosen. % \cite{sapatinas2004elements}.  
	%On the one hand, a low-order polynomial model may fail to capture the complex features and relationships inherent in the data, leading to poor fitting, referred to as underfitting (as illustrated in Fig. \ref{fig:diff_order_poly} (a)). On the other hand, a high-order polynomial model can more accurately adapt to the training data and may even pass through each training sample perfectly. However, the too high order can cause the model to capture noise and random fluctuations, thereby reducing its generalization ability and resulting in overfitting (as illustrated in Fig. \ref{fig:diff_order_poly} (c)). 
	
	
	\begin{figure}[htbp]
		\centerline{\includegraphics[width=0.9\columnwidth,trim=100 2 90 20,clip]{order128.eps}}
		\caption{Fitting by polynomials of different orders.}
		\label{fig:diff_order_poly}
		\vspace{-2mm}
	\end{figure}
	
	%\section{$\ell_0$ Norm Approximation Solver} \label{sec:ORLS}
	%\subsection{$\ell_0$ Norm Approximation}
	Our first proposed approach defines the regularization item as $\varOmega _F\left( \mathbf{C}_{\gamma} \right) \triangleq \gamma +1 $. Then, Problem 1 is reduced to
	%makes use of an $\ell_0$ norm approximation technique by considering a simple $\ell_0$ norm, namely $\lVert \mathbf{C}_{\gamma} \rVert _0 \approx \gamma + 1$.  Eq. \eqref{eq:C_k_l0} can be reformulated as follows
	\begin{equation}\label{eq:Ck_order}
		\mathrm{\bf Problem ~ 2}: ~~ \hat{\mathbf{C}}_{\gamma}=\underset{\mathbf{C}_{\gamma}}{\text{arg}\min}\left( \mathcal{D}_K(\mathbf{C}_{\gamma}) +\lambda \left( {\gamma + 1} \right) \right).
	\end{equation}
	
	In Problem 2, the fitting error of the polynomial gradually decreases as the order increases, while %. For the $\ell_0$ error,  it is noteworthy that 
	the regularization term increases by \(\lambda\) with each increment in the order as illustrated in Fig. \ref{fig:order_error}. After a certain point where the lowest overall cost exists, the constant increase of the order penalty will be more significant than the decrease of the data fitting error. %This offers a helpful guideline for addressing optimization problems. 
	%By leveraging this crucial character, 
	As such, we can start %systematically explore various polynomial orders during the optimization process. Starting 
	with $\gamma =0$ and incrementally increase the order while monitoring the increase in the regularization term, until the lowest overall cost was found at $\gamma^*$. This grid search allows us to efficiently obtain the optimal order $\gamma^*$ in a narrow, bounded range. %without exhaustively evaluating potential possible orders.  Incorporating strategy,  the optimization problem can addressed in an iterative manner by progressively refining the solution until an optimal order determined.
	Therefore, we set \textit{halting condition} for the grid search as if either $\gamma$ reaches the maximum order (cf. Eq. \eqref{eq:gamma_UpBound}) or if the reduction of the fitting error is less than the increase of the order cost at each step, namely,
	\begin{equation} \label{eq:Halt_con}
		|\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma^*}) -\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma^*+1}) | \le \lambda,
	\end{equation}
	where $\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma^*+1})$ denotes the data fitting error corresponding to a polynomial order of $\gamma^*+1$.
	
	
	\begin{figure}[htbp]
		\centerline{\includegraphics[width=0.7\columnwidth]{order_error.eps}}
		\caption{Illustration of the monotonous decrease of data fitting error $\mathcal{D}_K(\mathbf{C}_{\gamma})$ and the monotonous increase of order cost with the increase of $\gamma$. %between the measurement error and the $\ell_0$ error. The measurement  error is represented by the solid curve, while the $\ell_0$ error is represented by the dashed curve.
		}
		\label{fig:order_error}
	\end{figure}
	
	
	
	\subsection{Bounds for $\gamma^ *$ and $\lambda$}
	%We delve into the following two aspects: the determination of the optimal value of \({\gamma^ * }\) and of the parameter \(\lambda\). % and its proper configuration. 
	
	\subsubsection{Bounds of \({\gamma^ * }\)}
	%It is intractable to determine the optimal value of  \({\gamma^ * }\). %, direct solution of the equation can be challenging. 
	We propose to narrow the range of the optimal choice \({\gamma^ * }\) by imposing the following upper bound. 
	
	
	\begin{proposition}
		In accordance with the principle that the error associated with the optimal order is less than that of other orders, an upper bound for the optimal order $ \gamma^{*}$ is given
		\begin{equation}\label{eq:gamma_UpBound}
			\gamma^{*} \le \frac{\mathcal{D}_K(\hat{\mathbf{C}}_1)}{\lambda } + 1.
		\end{equation}
	\end{proposition}
	
	\begin{proof}
		%Firstly, it is important to note that the optimal value   \({\gamma^ * \ge 1}\). Setting  \({\gamma^ *=1 }\) corresponds to a linear polynomial, which for a often appropriate for CV model. Simultaneously, it 
		The result is rooted in the fact that the overall fitting error in the case of the optimal value of \({\gamma^ * }\) is no greater than that of the first-order, that is 
		\begin{equation}
			\mathcal{D}_K(\hat{\mathbf{C}}_1)+2\lambda \geq  \mathcal{D}_K(\hat{\mathbf{C}}_{\gamma^{*}})+(\gamma^{*}+1)\lambda.
		\end{equation}
		This together with the fact $\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma^{*}}) \ge 0$ leads to \eqref{eq:gamma_UpBound}.
		%, then $J _{min,1}-\left( \gamma ^*-1 \right) \lambda \ge0$, so we get the upper bound of the optimal order $\gamma ^*$ as $\gamma ^*\le\frac{J_{min,1}}{\lambda}+1$.
	\end{proof}
	
	
	% By leveraging the lower limit of the optimal estimation, we can determine the range within which the value of \(\gamma^{*}\) resides, that is
	% \begin{equation}\label{eq:gamma_UpBound}
		% 1 \le \gamma^{*}< \frac{\epsilon^2_1}{\lambda } + 1.
		% \end{equation}
	
	%By considering these restrictions, we can narrow down the feasible choice of \(\gamma^{*}\) %and make informed decisions about the appropriate polynomial order for the polynomial fitting task.
	
	\subsubsection{Bounds of \( \lambda \)}
	%Now, we delve into the issue of determining the value of the regularization parameter \( \lambda \) in polynomial fitting. 
	The regularization parameter \( \lambda \), which encourages sparsity in the polynomial coefficients, plays a vital role in balancing the fitting capacity and the sparsity of the model. %It controls the intensity of the regularization term, which encourages sparsity in the polynomial coefficients. 
	In other words, a larger \( \lambda \) exerts a stronger regularization and consequently more parameters are driven toward zero, resulting in a sparser model. %On the other hand, selecting smaller values of \( \lambda \) reduces the impact of the regularization term. This reduction in regularization strength allows more non-zero polynomial coefficients to be retained, resulting in a less sparse model. 
	%In the sparse case, the model may have a lower degree of complexity, as fewer coefficients contribute to the fitting process. 
	Note that model sparsity is not equivalent to a lower polynomial degree, as these two concepts represent distinct aspects of model complexity.
	
	%The appropriate choice of \( \lambda \) is highly dependent on the specific requirements and characteristics of the problem at hand. Prior knowledge about the data and domain expertise can guide the selection of an initial estimate for \( \lambda \). However, it is often necessary to fine-tune the value of \( \lambda \) to achieve the desired balance between model complexity and sparsity.
	
	% This article highlights two key points regarding the selection of the regularization parameter \( \lambda \) in polynomial fitting. Firstly, it emphasizes that \( \lambda \) should be chosen to be greater than zero, ensuring the regularization term's effectiveness in promoting sparsity. Secondly, the selection of \( \lambda \) should satisfy the condition that it is smaller than the first-order error ($\epsilon^2_1$), that is:
	% \begin{equation} \label{eq:lambda_in_range}
		% 0 < \lambda  < \epsilon^2_1.
		% \end{equation}
	
	% Furthermore, when performing polynomial fitting with a sliding window size of $d$, it is important to consider the potential overfitting state when the polynomial order is set to $d-1$. In this case, the polynomial passes through all the observation points, which can lead to  poor generalization.
	
	% To avoid this overfitting scenario, the optimal value of $\gamma^{*}$ should be chosen to be less than
	% $d-1$. This can be expressed as:
	% \begin{equation}
		% \frac{\epsilon _{1}^{2}}{\lambda}+1<d-1\Rightarrow \lambda >\frac{\varepsilon _{1}^{2}}{d-2}.
		% \end{equation}
	
	% Combining this inequality with the condition in equation \eqref{eq:lambda_in_range}, the parameter $\lambda$ should be chosen within the following range:
	% \begin{equation}\label{eq:lambda_range}
		% \frac{\varepsilon _{1}^{2}}{d-2}<\lambda< \epsilon^2_1.
		% \end{equation}
	
	\begin{proposition}
		The upper and lower bounds for the factor $ \lambda$ are given
		\begin{equation} \label{eq:lambda-range}
			\frac{\mathcal{D}_K(\hat{\mathbf{C}}_1)}{T_\text{w}-2}<\lambda\le \mathcal{D}_K(\hat{\mathbf{C}}_1),
		\end{equation}
		where $T_w$ is the concerning data size. 
	\end{proposition}
	
	\begin{proof}
		Firstly, it emphasizes that \( \lambda \) should be chosen to be greater than zero, ensuring the effectiveness of the regularization term in promoting sparsity. Secondly, the selection of \( \lambda \) should satisfy the condition that it is smaller than the first-order error $\mathcal{D}_K(\hat{\mathbf{C}}_1)$, %corresponding  the CV model, 
		that is, $ \lambda  \le \mathcal{D}_K(\hat{\mathbf{C}}_1)$.  
		
		Furthermore, %in the case of a sliding time window of length $d$, 
		overfitting occurs when the polynomial order is set equal to or larger than $T_w-1$ when the polynomial passes through all the measurement points. Therefore, to avoid this, it is required that
		\begin{equation}
			\frac{\mathcal{D}_K(\hat{\mathbf{C}}_1)}{\lambda}+1<T_\text{w}-1.
		\end{equation}
		which results in the lower bound $\lambda >\frac{\mathcal{D}_K(\hat{\mathbf{C}}_1)}{T_\text{w}-2}$.
	\end{proof}
	
	
	%By carefully choosing the parameter $\lambda$ from the range given in \eqref{eq:lambda-range}, we may be able to get a judicious equilibrium among regularization effectiveness, promotion of sparsity, and accuracy in fitting. We next propose two approaches for determining the optimal choice in this range. 
	
	
	\subsection{Order-Recursive Least Squares (ORLS) Solver for Linear Measurement} 
	
	We write the vector form of the T-FoT with regard to all time-instants in the time-window $K=[k',k]$ as follows, 
	\begin{equation} \label{eq:linearized-measurement}
		F\left(K;\mathbf{C}_{\gamma} \right)  = \mathbf{Z}_{\gamma}\mathbf{C}_{\gamma},
	\end{equation}
	where $\mathbf{Z}_{\gamma}$ denotes a Vandermonde matrix of size $T_\text{w}\times (\gamma+1)$, % \cite{olver2006applied}, 
	structured as follows,
	\begin{equation}
		\mathbf{Z}_{\gamma} \triangleq \left[ \begin{matrix}
			1&		{k'}&		\cdots&		{k'}^{\gamma}\\
			1&		{k'+1}&		\cdots&		{(k'+1)}^{\gamma}\\
			\vdots&		\vdots&		\ddots&		\vdots\\
			1&		k&		\cdots&		{k}^{\gamma}\\
		\end{matrix} \right].
	\end{equation}
	Then, given linear measurement with additive, zero-mean noise, namely $\mathbf{y}_k= \mathbf{x}_k + \mathbf{v}_k, \bar{\mathbf{v}}_t =\mathbf{0}$, we get 
	\begin{align} \label{eq:D_k(C)-Linear}
		\mathcal{D}_K(\mathbf{C}_{\gamma})&=\lVert \mathbf{Y}_K -\mathbf{Z}_{\gamma}\mathbf{C}_{\gamma} \rVert^2_{\text{var}(\mathbf{Y})} \nonumber \\
		&=\left(\mathbf{Y}_K -\mathbf{Z}_{\gamma}\mathbf{C}_{\gamma}\right)^\text{T}{\text{var}(\mathbf{Y})}^{-1}\left(\mathbf{Y}_K -\mathbf{Z}_{\gamma}C_{\gamma}\right),
	\end{align}
	where the  position measurements is denoted as
	\begin{equation}\label{eq:PositionMeasuremSet}
		\begin{aligned}
			\mathbf{Y}_K \triangleq &\left[ \begin{matrix}
				\mathbf{y}_{k'},&		\mathbf{y}_{k'+1},&		\cdots,&		\mathbf{y}_k\\
			\end{matrix} \right] ^{\text{T}}.
		\end{aligned}
	\end{equation}
	
	
	In minimizing $\mathcal{D}_K(\mathbf{C}_{\gamma})$, if the uncertainty of the observed data is time-invariant (i.e., the noise is homogeneous), the term $\text{var}(\mathbf{y}_t)$ can be disregarded, resulting in a standard LS fitting. Otherwise, we can factorize it as ${\text{var}(\mathbf{Y})} ^{-1}=\mathbf{A}^{T}\mathbf{A}$ with positive definite $T_\text{w} \times T_\text{w} $ matrix $\mathbf{A}$ according to the Cholesky factorization method \cite{Ye21Cholesky}. % as long as it is . 
	%In order to calculate $\hat{C}_\gamma=\underset{C_{\gamma}}{\text{arg}\min} \mathcal{D}_K(C_{\gamma})$, we note that $\left({\text{var}(Z)}\right)^{-1}$ was assumed positive definite and hence can be factored as $\left({\text{var}(Z)}\right)^{-1}=A^{T}A$ with some $d \times d$ matrix $A$. 
	This yields 
	\begin{align}
		\mathcal{D}_K(\mathbf{C}_{\gamma})&=\left(\mathbf{Y}_K -\mathbf{Z}_{\gamma}\mathbf{C}_{\gamma}\right)^\text{T}\mathbf{A}^{T}\mathbf{A}\left(\mathbf{Y}_K -\mathbf{Z}_{\gamma}\mathbf{C}_{\gamma}\right) \nonumber \\
		&=\left(\tilde{\mathbf{Y}}_K -\tilde{\mathbf{Z}}_{\gamma}\mathbf{C}_{\gamma}\right)^\text{T}\left(\tilde{\mathbf{Y}}_K -\tilde{\mathbf{Z}}_{\gamma}\mathbf{C}_{\gamma}\right),
	\end{align}
	where $\tilde{\mathbf{Y}}_K \triangleq \mathbf{A}\mathbf{Y}_K$ and $\tilde{\mathbf{Z}}_{\gamma} \triangleq \mathbf{A}\mathbf{Z}_{\gamma}$. 
	
	So far, $\mathcal{D}_K(\mathbf{C}_{\gamma})$ is now formulated as conventional LS problem %(albeit with transformed data $\tilde{Z}_K $ and transformed model matrix $\tilde{Y}_{\gamma}$), 
	for which the solution is 
	\begin{equation}\label{eq:hat(C)-gamma}
		\hat{\mathbf{C}}_{\gamma}  = {\left( {{\tilde{\mathbf{Z}}_{\gamma}^\text{T}}\tilde{\mathbf{Z}}_{\gamma}} \right)^{ - 1}}{\tilde{\mathbf{Z}}_{\gamma}^\text{T}}\tilde{\mathbf{Y}}_K .
	\end{equation}
	\begin{equation}
		\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma})=\left(\tilde{\mathbf{Y}}_K -\tilde{\mathbf{Z}}_{\gamma}\hat{\mathbf{C}}_{\gamma}\right)^\text{T}\left(\tilde{\mathbf{Y}}_K -\tilde{\mathbf{Z}}_{\gamma}\hat{\mathbf{C}}_{\gamma}\right).
	\end{equation}
	
	We then employ the ORLS algorithm \cite[Sec. 8]{steven1993fundamentals} to iteratively update the parameters to the optimal one. % and improve the computational efficiency. The specific iterative formula is as follows:
	% Denote the $N\times (\gamma+1)$  time matrix as $Y_{\gamma}$, and the least squares method based on $Y_{\gamma}$ as $\hat C_{\gamma}$ or
	% \begin{equation}\label{eq:hat(C)-gamma}
		% \hat C_{\gamma}  = {\left( {{Y_{\gamma}^\text{T}}Y_{\gamma}} \right)^{ - 1}}{Y_{\gamma}^\text{T}}Z.
		% \end{equation}
	% The minimum least squares error based o $Y_{\gamma}$ is
	% \begin{equation}\label{eq:min_epsilon}
		% \epsilon _{\min\text{,}\gamma}^{2}=( Z_K -Y_{\gamma}\hat{C}_{\gamma})^\text{T} ( Z_K -Y_{\gamma}\hat{C}_{\gamma}).
		% \end{equation}
	Each time, we increase the order by one, adding a column to the time matrix, resulting in a new matrix that can be partitioned as follows
	\begin{equation}
		\tilde{\mathbf{Z}}_{\gamma +1}=\left[ \begin{matrix}
			\tilde{\mathbf{Z}}_{\gamma}&		\tilde{\mathbf{z}}_{\gamma +1}\\
		\end{matrix} \right] ,
	\end{equation}
	where $\tilde{\mathbf{z}}_{\gamma +1}=\mathbf{A}\mathbf{z}_{\gamma +1}$, $\mathbf{z}_{\gamma +1}=\left[k'^{\gamma+1}, (k'+1)^{\gamma+1}, \cdots, k^{\gamma+1} \right]^\text{T}$.
	
	To update $\hat{\mathbf{C}}_{\gamma}$ and $\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma})$ we use \cite[Eq.(8.28)]{steven1993fundamentals}
	\begin{equation}\label{eq:c_recurisive}
		\hat{\mathbf{C}}_{\gamma +1}=\begin{bmatrix}
			\hat{\mathbf{C}}_{\gamma}-\frac{\left( \tilde{\mathbf{Z}}_{\gamma}^{T}\tilde{\mathbf{Z}}_{\gamma} \right) ^{-1}\tilde{\mathbf{Z}}_{\gamma}^{T}\tilde{\mathbf{z}}_{\gamma +1}\tilde{\mathbf{z}}_{\gamma +1}^{T}\mathbf{W}_{\gamma}^{\bot}\tilde{\mathbf{Y}}_K }{\tilde{\mathbf{z}}_{\gamma +1}^{T}\mathbf{W}_{\gamma}^{\bot}\tilde{\mathbf{z}}_{\gamma +1}}\\
			\frac{\tilde{\mathbf{z}}_{\gamma +1}^{T}\mathbf{W}_{\gamma}^{\bot}\tilde{\mathbf{Y}}_K }{\tilde{\mathbf{z}}_{\gamma +1}^{T}\mathbf{W}_{\gamma}^{\bot}\tilde{\mathbf{z}}_{\gamma +1}}
		\end{bmatrix},
	\end{equation}
	where 
	\begin{equation}
		\mathbf{W}_\gamma^{\bot} \triangleq \mathbf{I} - {\tilde{\mathbf{Z}}_\gamma}\mathbf{B}_{\gamma}\tilde{\mathbf{Z}}_\gamma^\text{T}.
	\end{equation}
	where $\mathbf{I}$ denotes the identity matrix. $\mathbf{W}_\gamma^{\bot}$ is the projection matrix onto the subspace orthogonal to that spanned by the columns of $\tilde{\mathbf{Z}}_{\gamma}$ and the abbreviation
	%To avoid inverting $\tilde{Y}_{\gamma}^{T}\tilde{Y}_{\gamma}$ we let
	\begin{equation}\label{eq4.7}
		\mathbf{B}_{\gamma} \triangleq \left( \tilde{\mathbf{Z}}_{\gamma}^{T}\tilde{\mathbf{Z}}_{\gamma} \right) ^{-1},
	\end{equation}
	is updated recursively as follows \cite[Eq.(8.30)]{steven1993fundamentals}
	\begin{equation}\label{eq:D_recurisive}
		{\mathbf{B}_{\gamma + 1}} = \begin{bmatrix}
			{{\mathbf{B}_\gamma} + \frac{{{\mathbf{B}_\gamma}\tilde{\mathbf{Z}}_\gamma^\text{T}{\tilde{\mathbf{z}}_{\gamma + 1}}\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\tilde{\mathbf{Z}}_\gamma}{\mathbf{B}_\gamma}}}{{\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\mathbf{W}_\gamma^{\bot}}{\tilde{\mathbf{z}}_{\gamma + 1}}}}}&{ - \frac{{{\mathbf{B}_\gamma}\tilde{\mathbf{Z}}_\gamma^\text{T}{\tilde{\mathbf{z}}_{\gamma + 1}}}}{{\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\mathbf{W}_\gamma^{\bot}}{\tilde{\mathbf{z}}_{\gamma + 1}}}}}\\
			{ - \frac{{\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\tilde{\mathbf{Z}}_\gamma}{\mathbf{B}_\gamma}}}{{\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\mathbf{W}_\gamma^{\bot}}{\tilde{\mathbf{z}}_{\gamma + 1}}}}}&{\frac{1}{{\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\mathbf{W}_\gamma^{\bot}}{\tilde{\mathbf{z}}_{\gamma + 1}}}}}
		\end{bmatrix}.
	\end{equation}
	%where $W_\gamma^{\bot}=I - {\tilde{Y}_\gamma}B_{\gamma}\tilde{Y}_\gamma^\text{T}$. 
	The LS fitting error is reduced by \cite[Eq.(8.31)]{steven1993fundamentals}
	\begin{equation}\label{eq:MIN_recurisive}
		\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma})-\mathcal{D}_K(\hat{\mathbf{C}}_{\gamma+1}) = \frac{{{{\left( {\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\mathbf{W}_\gamma^{\bot}}\tilde{\mathbf{Y}}_K } \right)}^2}}}{{\tilde{\mathbf{z}}_{\gamma + 1}^\text{T}{\mathbf{W}_\gamma^{\bot}}{\tilde{\mathbf{z}}_{\gamma + 1}}}}.
	\end{equation}
	
	% Since the accuracy of polynomial fitting decreases  as the order increases, we can deduce that
	% \({\gamma^ * }\) is the optimal solution when
	% \begin{equation}\label{eq:lambda_gamma}
		% \frac{{{{\left( {h_{\gamma^ *  + 1}^\text{T}{W_{\gamma^ *}^{\bot} }Z} \right)}^2}}}{{h_{\gamma^{ *}  + 1}^\text{T}{W_{\gamma^ *}^{\bot} }{h_{\gamma^ *  + 1}}}} \le \lambda .
		% \end{equation}
	
	% First, we observe the first term of Eq. \eqref{eq:Ck_order}, $s_{\gamma}(C_{\gamma}) = Y_{\gamma}C_{\gamma}$, with a $N\times (\gamma+1)$ model matrix $Y_{\gamma}$ and a $(\gamma+1)$-dimensional parameter vector $C_{\gamma}$. This first term can be solved using the least squares method to obtain an analytical solution for an $\gamma$-order polynomial, that is
	
	
	
	% We term \eqref{eq:c_recurisive}, \eqref{eq:D_recurisive} and \eqref{eq:MIN_recurisive} the order recursive least square (ORLS) method. The derivation of the formula is detailed in \cite{steven1993fundamentals}. Employing the ORLS avoids the computationally expensive matrix inversions performed at each sub-iteration.  The recursion begins by determining  \(\hat C _{1}\) and \(\epsilon^2_{min,1}  \)  using Eq. \eqref{eq:hat(C)-gamma} and \eqref{eq:min_epsilon}, respectively.
	%The detailed derivation of the aforementioned ORLS recursive formula, that is \eqref{eq:c_recurisive}, \eqref{eq:D_recurisive} and \eqref{eq:MIN_recurisive} can be found in reference \cite{steven1993fundamentals}. 
	Employing ORLS avoids the computationally expensive matrix inversion at each iteration. The iteration to find the optimal $\lambda^*$ will be terminated with the monitoring of the error reduction as given in \eqref{eq:MIN_recurisive}, according to the halting condition \eqref{eq:Halt_con}. We summarize the process of the algorithm in Algorithm. \ref{alg_ORLS}.
	
	\begin{algorithm}[!ht]
		\caption{The $\gamma$-limiting ORLS algorithm}\label{alg_ORLS}
		\KwIn{Sensor measurement \(\mathbf{Y}_K\) in the time-window $K$, T-FoT parameters: \( \lambda \), $\gamma=0$.}
		\KwOut{Optimal polynomial order \(\gamma^{*}\), polynomial coefficient \(\hat {\mathbf{C}}_{\gamma^{*}}\).}
		\begin{algorithmic}[1]
			\STATE {Initialize \(\hat {\mathbf{C}}_{0}\), \(\mathcal{D}_K (\hat{\mathbf{C}}_0) \) and \(\mathbf{B}_0\);}
			\WHILE {the halting conditions are not violated}
			{
				\STATE {\(\gamma \leftarrow \gamma+1\);}
				\STATE {calculate \eqref{eq:hat(C)-gamma} and \eqref{eq:c_recurisive} for updating parameters;}
				\STATE {calculate %\eqref{eq:D_recurisive} and 
					\eqref{eq:MIN_recurisive} for checking the halting condition;}
			}
			\ENDWHILE
			\STATE {\textbf{return} \(\gamma^{*}=\gamma, \hat{\mathbf{C}}_{\gamma^{*}} = \hat{\mathbf{C}}_{\gamma}\) ;}
		\end{algorithmic}
	\end{algorithm}
	
	
	\subsection{Extension to Nonlinear Measurement} \label{sec:nonlinearMeasurement}
	Nonlinear measurements generally do not admit a closed-form LS solution for the T-FoT optimization problem. Instead, the corresponding nonlinear LS optimization must be based on computationally intensive iterative/numerical solvers. Two alternatives that can mitigate this challenge %reduce the complexity of the problem 
	are considerable. One converts the non-linear measurement to the linear position measurements as discussed in \cite[Sec. 3]{li2023target}. This is also referred to as the transformation of parameters \cite[Ch.8.9]{steven1993fundamentals}. This, however, applies only to the determined or over-determined measurement system, i.e., the dimension of the measurement is no lower than the dimension of the state space. %, or to say the position space if the T-FoT is modelled only in the position space. 
	For example, the range-bearing measurement can be converted to position measurement for position T-FoT fitting \cite{li2023target}. 
	The other choice is to linearize the nonlinear measurement, % based on the Taylor expansion, 
	similar with what has been done within the prevalent extended Kalman filter (KF) in extending the KF. Reconsider the measurement function \eqref{eq:measurement} linearized as follows: 
	\begin{equation}
		\mathbf{y}_k=\hat{\mathbf{y}}_{k-1}+\mathbf{J}_k\left(\mathbf{x}_k-\hat{\mathbf{x}}_{k-1}\right)+\mathbf{v}_k,
	\end{equation}
	where $\hat{\mathbf{y}}_{k-1}=h(\hat{\mathbf{x}}_{k-1}, \bar{\mathbf{v}_{k+1}})$, $\hat{\mathbf{x}}_{k-1}$ denotes the state estimate extracted from the T-FoT by $\hat{\mathbf{x}}_{k-1} = F(k-1; \mathbf{C}_{k-1})$ based on the parameters $\mathbf{C}_{k-1}$ obtained at time $k-1$, and $\mathbf{J}_k$ denotes the Jacobian matrix of the partial derivatives of the measurement function $h_k(\cdot)$. 
	
	Then, given $\hat{\mathbf{x}}_{t},\hat{\mathbf{y}}_{t}$ estimated at time $t=k',k'+1,...,k-1$ and all of the measurements in the time-window $\mathbf{y}_k, k\in K$, we define  %\eqref{eq:PositionMeasuremSet} can be reformulated as 
	% \begin{equation} \label{eq:ExtendedMeasuremSet}
		% \begin{aligned}
			%  \delta Z_K \triangleq &\left[ \begin{matrix}
				%  	z_{k'}-\hat{z}_{k'-1} + J_{k'}\hat{x}_{k'-1},&	\cdots,&		z_{k}-\hat{z}_{k-1} + J_{k}\hat{x}_{k-1}\\
				% \end{matrix} \right] ^{\text{T}}
			% \end{aligned}
		% \end{equation}
	$ \mathbf{\delta Y}_K \triangleq [
	\mathbf{y}_{k'}-\hat{\mathbf{y}}_{k'-1} + \mathbf{J}_{k'}\hat{\mathbf{x}}_{k'-1}, \mathbf{y}_{k'+1}-\hat{\mathbf{y}}_{k'} + \mathbf{J}_{k'+1}\hat{\mathbf{x}}_{k'}, 	\cdots,		\mathbf{y}_{k}-\hat{\mathbf{y}}_{k-1} + \mathbf{J}_{k}\hat{\mathbf{x}}_{k-1} 
	] ^{\text{T}}$
	and reformulate \eqref{eq:D_k(C)-Linear} as 
	\begin{align} \label{eq:D_k(C)-ExLinear}
		\mathcal{D}_K(\mathbf{C}_{\gamma})&=\lVert  \mathbf{\delta Y}_K - \mathbf{J}_K  \mathbf{Z}_{\gamma}\mathbf{C}_{\gamma} \rVert^2_{\text{var}( \mathbf{\delta Y})},
	\end{align}
	where $\mathbf{J}_K \triangleq \text{diag} (\mathbf{J}_{k'}, \mathbf{J}_{k'+1},\cdots, \mathbf{J}_{k})$ is a diagonal matrix with diagonal elements $\mathbf{J}_{k'}, \mathbf{J}_{k'+1},\cdots, \mathbf{J}_{k}$ in order. 
	
	Comparing \eqref{eq:D_k(C)-Linear} with \eqref{eq:D_k(C)-ExLinear}, it can be seen that $\mathbf{Y}_K$ and $\mathbf{Z}_{\gamma}$ were replaced by $\delta \mathbf{Y}_K$ and $\mathbf{J}_K  \mathbf{Z}_{\gamma}$ in the latter, respectively. The $\gamma$-limiting ORLS algorithm can be similarly used to estimate $\mathbf{C}_{\gamma}$. The detail is omitted.   
	
	\section{$\ell_0$-regularized T-FoT Fitting}\label{sec:Newton}
	
	%This section primarily discusses a direct processing algorithm for solving the $\ell_0$ optimization problem, employing the hybrid Newton method for optimization solution. Firstly, we present the optimality conditions for $\ell_0$-regularized optimization.
	%\subsection{Optimality Condition}
	% A point $x\in \mathbb{R}^n$ is called a $\tau$-stationary point of the problem is there is a $\tau >0$ satisfying
	% \begin{equation}\label{eq:tau_stati_point}
		% \begin{aligned}
			% x &\in \text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( x-\tau \bigtriangledown f\left( x \right) \right) \\
			% & :=\underset{z\in \mathbb{R}^n}{\text{arg}\min}\frac{1}{2}\lVert z-\left( x-\tau \bigtriangledown f\left( x \right) \right) \rVert ^2+\tau \lambda \lVert z \rVert _0.
			% \end{aligned}
		% \end{equation}
	% \begin{lemma}
		%   A point $x$ is a $\tau$-stationary point with $\tau>0$ of \eqref{eq:l0_form} if and only if
		%   \begin{equation}\label{eq:tau_judge}
			%  \begin{cases}
				% \bigtriangledown _if\left( x \right) =0\ and\ \left| x_i \right|\ge \sqrt{2\tau \lambda},&i\in \text{supp}\left( x \right)\\
				% \left| \bigtriangledown _if\left( x \right) \right|\le \sqrt{2\lambda/\tau},&i\notin \text{supp}\left( x \right) \\
				% \end{cases}
			% \end{equation}
		% \end{lemma}
	% The proof of this result is proved  \cite[Lemma 2]{blumensath2008iterative}.
	
	
	%\subsection{$\ell_0$-regularized T-FoT}
	%To trade off between trajectory accuracy and simplicity in determining the T-FoT model \eqref{eq:Const-T-FoT-Op}, % when selecting the order and coefficients of the fitting polynomial, 
	For a judicious equilibrium  between promotion of sparsity and fitting accuracy in the T-FoT model \eqref{eq:Const-T-FoT-Op},
	a prevalent approach is to enforce sparsity, typically characterized by the $\ell_0$ norm \cite{zhou2021newton} regularization. In other words, %the constrained T-FoT model \eqref{eq:Const-T-FoT-Op} in 
	Problem 1 is specified using the $\ell_0$-norm in accordance with the law of parsimony, leading to a new form: % (Occam's Razor):
	\begin{equation}\label{eq:C_k_l0}
		\mathrm{\bf Problem ~ 3}: ~~ \hat{\mathbf{C}}_{\gamma}=\underset{\mathbf{C}_{\gamma}}{\text{arg}\min}\left( \mathcal{D}_K(\mathbf{C}_{\gamma})+\lambda \lVert \mathbf{C}_{\gamma} \rVert _0 \right).
	\end{equation}
	%and the coefficient $\lambda>0$ trades off between the data fitting error and the model fitting error %. where $\lambda >0$ is the penalty parameter and $\lVert C \rVert _0$ is the $\ell_0$ norm of $C$, counting the number of non-zero elements of $C$.
	
	% Our primary objective is to solve \eqref{eq:C_k_l0_matrix}. Additionally, this problem can be reformulated as an equivalent general form:
	
	% \begin{equation}\label{eq:l0_form}
		% \underset{x\in \mathbb{R}^n}{\min}\ f\left( x \right) +\lambda \lVert x \rVert _0,
		% \end{equation}
	This $\ell_0$-regularized optimization problem is nonconvex, noncontinuous, and NP-hard, % \cite{natarajan1995sparse},
	no matter what $\mathcal{D}_K(\mathbf{C}_{\gamma})$. %This makes it NP-hard \cite{natarajan1995sparse}. %Conventional approaches rooted in continuous optimization theory and algorithms are ill-equipped to effectively tackle these complex challenges, thereby presenting substantial obstacles in the search for feasible solutions.
	%In recent decades, extensive research efforts have been devoted to $\ell_0$ optimization, resulting in a significant body of knowledge. 
	Existing approaches %for addressing $\ell_0$-regularized optimization problems 
	can be broadly categorized into two main groups: model transformation and direct processing \cite{some2020zhao}. %These approaches offer distinct and varied strategies to tackle the inherent challenges associated with $\ell_0$ optimization problems.
	The model transformation method focuses on converting the nonconvex and nonsmooth $\ell_0$ norm %function $\lVert C \rVert_0$
	into a more tractable form, either by transforming it into a convex function like the $\ell_1$-regularization \cite{kim2007interior} %\cite{kim2007interior,van2009probing,zou2006adaptive} %tibshirani1996regression,
	or an easy-handling nonconvex function % foucart2009sparsest,chartrand2008restricted,
	like the $\ell_{1/2}$-regularization \cite{xu2012lell} % ,zeng2014l_
	and $\ell_p$ norm relaxation, where $0 < p < 1$ \cite{fung2011equivalence}. % ,fan2018variable
	%This relaxes the constraint on the sparsity of the decision vector, enabling the development of efficient and feasible algorithms. %Convex relaxation algorithms and theories, such as the optimal convex approximation with the $\ell_1$-norm, have significantly advanced and matured. 
	These advancements have led to the emergence of various highly efficient first-order algorithms, including the iterative shrinkage-thresholding algorithm \cite{blumensath2008iterative}, % ,khoramian2012iterative
	augmented Lagrangian method, and alternating direction multiplier method (ADMM) \cite{he2020optimal}. % cui2016convergence,,han2018linear
	
	%Another notable approach to deal with non-convex term is the $\ell_p$ norm relaxation, where $0 < p < 1$ \cite{fung2011equivalence,fan2018variable}. 
	
	% This technique %has gained considerable attention and 
	% is supported by %a substantial body of 
	% well-established theoretical studies \cite{fung2011equivalence,fan2018variable}.
	
	The direct processing approach to $\ell_0$ optimization aims to bypass the need for exact recovery verification and broadens the applicability of the findings. %In recent years, a plethora of problems has arisen that necessitate the utilization of algorithmic solutions, including 
	Representative solutions include the iterative hard-thresholding algorithm \cite{blumensath2009iterativecompressed}, active set Barzilar-Borwein algorithm \cite{cheng2020active}, %support detection and root finding \cite{huang2018constructive}, 
	variational approach \cite{ito2013variational}, to name a few. % \cite{jiao2015primal}.
	In what follows, we detail a direct processing solver based on the hybrid Newton approach to obtain the optimal solution of \eqref{eq:C_k_l0}.  For simplicity, we will omit the subscript of $\gamma$ (which can be set to any positive integer).
	
	For the problem \eqref{eq:C_k_l0}, a $\tau$-stationary point is  defined when a $\tau > 0$ exists that satisfies
	\begin{equation}\label{eq:eq19_stati_point}
		\mathbf{C} \in \text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( \mathbf{C}-\tau \bigtriangledown \mathcal{D}_K\left( \mathbf{C} \right) \right).
	\end{equation}
	
	The hybrid Newton method \cite{boyd2004convex} needs the strong smoothness and convexity of $\mathcal{D}_K(\mathbf{C})$ and aims to establish the relationship between the $\tau$-stationary point and the local/global minimizer of \eqref{eq:C_k_l0}. Next, we introduce   the following theorem, which clarifies the relationship between the $\tau$-stationary point and the local/global minimizer of \eqref{eq:C_k_l0}. Theorem \ref{theorem1} and the proof are detailed in \cite[Theorem 1]{zhou2021newton}.
	
	
	\begin{theorem} %\cite[Theorem 1]{zhou2021newton} 
		\label{theorem1}
		For problem \eqref{eq:C_k_l0}, the following results hold.
		
		1) (\textbf{Necessity}) A global minimizer $\mathbf{C}^{\ast}$ is also a $\tau$-stationary point for any $0 <\tau<
		1/L$ if $\mathcal{D}_K(\mathbf{C})$ is strongly smooth with $L > 0$. Moreover,
		\begin{equation}\label{eq:necessity_global}
			\mathbf{C}^{\ast}=\text{Prox}_{\tau \lambda \lVert \cdot \rVert _0}\left( \mathbf{C}^{\ast}-\tau \bigtriangledown \mathcal{D}_K\left( \mathbf{C}^{\ast} \right) \right).
		\end{equation}
		
		2) (\textbf{Sufficiency}) A $\tau$-stationary point with $\tau>0$ is a local minimizer if $\mathcal{D}_K(\mathbf{C})$ is convex.
		Furthermore, a $\tau$-stationary point with $\tau  \ge 1/\ell$ is also a (unique) global minimizer if $\mathcal{D}_K(\mathbf{C})$ is strongly convex with $\ell>0$.
	\end{theorem}
	
	To express the solution of (\ref{eq:eq19_stati_point}) more explicitly, we define
	\begin{equation} \nonumber %\label{eq:defi_T}
		T\triangleq T_{\tau}\left( \mathbf{C},\lambda \right) :=\left\{ i\in \mathbb{N}:\left| \mathbf{c}_i-\tau \bigtriangledown _i \mathcal{D}_K\left( \mathbf{C}\right) \right|\ge \sqrt{2\tau \lambda} \right\},
	\end{equation}
	where $\mathbb{N}=\left\{0,1,\cdots,\gamma \right\}$, $\bigtriangledown _i \mathcal{D}_K\left( \mathbf{C}\right)=\left(\bigtriangledown \mathcal{D}_K\left( \mathbf{C}\right)\right)_i$.
	
	Based on this set, we introduce the following stationary equation
	\begin{equation}\label{eq:stationary}
		G_{\tau}\left( \mathbf{C};T \right) \triangleq \left[ \begin{array}{c}
			\bigtriangledown_T \mathcal{D}_K\left( \mathbf{C}\right)\\
			\mathbf{C}_{\overline{T}}\\
		\end{array} \right] =\mathbf{0},
	\end{equation}
	where $\overline{T}$ is the complementary set of $T \subseteq \mathbb{N}$.
	
	
	%\subsection{Algorithm Design}
	
	The relationship between \eqref{eq:eq19_stati_point} and \eqref{eq:stationary} is revealed in \cite{zhou2021newton}.
	To solve \eqref{eq:stationary}, we first locate the hidden index set $T$ by employing an adaptive updating rule as follows. First, calculate an approximation $T_{\kappa}$ for a computed point $\mathbf{C}^{(\kappa)}$ (the $\kappa$-th iteration). Then, by fixing this set $T_{\kappa}$, apply the Newton method to $G_{\tau} (\mathbf{C}; T_{\kappa})$ once obtaining a direction $s^{(\kappa)}$. In other words, $s^{(\kappa)}$ is a solution to the following equation. %system
	\begin{equation}
		\bigtriangledown G_{\tau}\left(\mathbf{C}^{(\kappa)};T_{\kappa} \right) s+G_{\tau}\left( \mathbf{C}^{(\kappa)};T_{\kappa} \right)=\mathbf{0} .
	\end{equation}
	The explicit formula of $G_{\tau}\left(\mathbf{C}^{(\kappa)};T_{\kappa}  \right)$ from (\ref{eq:stationary}) implies that $s^{(\kappa)}$ satisfies
	\begin{equation}\label{eq:s^k_solution}
		\mathcal{H}_{(\kappa)}s_{T_{\kappa}}^{(\kappa)}=\mathcal{G}_{(\kappa)} \mathbf{C}_{\overline{T}_{\kappa}}^{(\kappa)}-p^{(\kappa)}_{T_{\kappa}},
	\end{equation}
	where 
	\begin{equation} \nonumber
		\begin{aligned}
			\mathcal{H}_{(\kappa)} & = \bigtriangledown _{T_{\kappa}}^{2} \mathcal{D}_K\left( \mathbf{C}^{(\kappa)} \right) ,  \\
			\mathcal{G}_{(\kappa)}&=\bigtriangledown_{T_k,\overline{T}_{\kappa}}^{2}\mathcal{D}_K\left( \mathbf{C}^{(\kappa)} \right), \\
			p^{(\kappa)}&= \bigtriangledown_{T_k} \mathcal{D}_K\left( \mathbf{C}^{(\kappa)}\right).
		\end{aligned}
	\end{equation}
	And its solution satisfies
	\begin{equation}\label{eq:p^ks^k_solution}
		\left< p^{(\kappa)}_{T_{\kappa}},s_{T_{\kappa}}^{(\kappa)} \right> \le -\delta \lVert s^{(\kappa)}  \rVert ^2+ \lVert \mathbf{C}_{\overline{T}_{\kappa}}^{(\kappa)} \rVert ^2 / \left(4 \tau \right),
	\end{equation}
	where $\delta \in \left( 0,\min \left( 1, \ell\right) \right)$.
	$0<\tau  \le \frac{2\overline{\alpha }\delta \beta}{(\gamma+1)L^2}$, $\beta \in \left( 0,1 \right) $, 
	\begin{equation}
		\overline{\alpha }:=\min \left\{ \frac{1-2\sigma}{L/\delta-\sigma},\frac{2\left( 1-\sigma \right) \delta}{L},1 \right\},
	\end{equation}
	where $\sigma \in \left( 0,1/2 \right) $.
	
	If \eqref{eq:s^k_solution} is solvable and \eqref{eq:p^ks^k_solution} is satisfied, then update
	$s^{(\kappa)}$ using the Newton direction, as indicated by \eqref{eq:s^k_solution} and $s_{\overline{T}_{\kappa}}^{(\kappa)}=-\mathbf{C}_{\overline{T}_{\kappa}}^{(\kappa)}$.
	% the following formula:
	% \begin{equation}
		%   s_{\overline{T}_{(\kappa)}}^{(\kappa)}=-\mathbf{C}_{\overline{T}_{(\kappa)}}^{(\kappa)}.
		% \end{equation}
	Otherwise, we use the gradient descent method to update the direction $s^{(\kappa)}$, that is, $s_{T_{\kappa}}^{(\kappa)} = -p^{(\kappa)}_{T_{\kappa}} ,
	s_{\overline{T}_{\kappa}}^{(\kappa)} = -\mathbf{C}_{\overline{T}_{\kappa}}^{(\kappa)}$. 
	% \begin{equation}
		% \begin{aligned}
			%   s_{T_{(\kappa)}}^{(\kappa)}&=-p^{(\kappa)}_{T_{(\kappa)}} ,\\
			%   s_{\overline{T}_{(\kappa)}}^{(\kappa)}&=-\mathbf{C}_{\overline{T}_{(\kappa)}}^{(\kappa)}.
			% \end{aligned}
		% \end{equation}
	
	
	% Now, let us consider the above formulas. The second part of $s^{(\kappa)}$ can be derived directly without any difficulties. To find $s^{(\kappa)}$, it is necessary to solve a linear equation with $\left| T_{(\kappa)} \right|$ (the cardinality of $T_{(\kappa)}$) equations and $\left| T_{(\kappa)} \right|$  variables. If a full Newton direction is taken, then the next iteration is $C^{(\kappa+1)}=C^{(\kappa)}+s^{(\kappa)}=\left[ \begin{matrix}
		% 	\left( C_{T_{(\kappa)}}^{(\kappa)}+s_{T_{(\kappa)}}^{(\kappa)} \right) ^{\text{T}}&		0\\
		% \end{matrix} \right] ^{\text{T}}$. This means that the support set of $C^{(\kappa+1)}$ will be located within $T_{(\kappa)}$. Namely,
	% \begin{equation}
		% \text{supp}\left( C^{(\kappa+1)}\right) \subseteq T_{(\kappa)}.
		% \end{equation}
	
	Subsequently, we employ the line search methodology to ascertain the step size $\rho^{(\kappa)}$ for which the inexact Armijo-Goldstein criterion  \cite{Liu2020Optimization} can be adopted, $\mathbf{C}^{(\kappa+1)}=\mathbf{C}^{(\kappa)}+\rho^{(\kappa)} s^{(\kappa)}$, where
	\begin{align}\nonumber 
		\mathbf{C}^{(\kappa+1)} & := \left[ \begin{array}{c}
			\mathbf{C}_{T_{\kappa}}^{(\kappa)}+\rho^{(\kappa)} s_{T_{\kappa}}^{(\kappa)}\\
			\mathbf{C}_{\overline{T}_{\kappa}}^{(\kappa)}
			+s_{\overline{T}_{\kappa}}^{(\kappa)}\\
		\end{array} \right] \nonumber \\
		&=\left[ \begin{array}{c}
			\mathbf{C}_{T_{\kappa}}^{(\kappa)}+\rho^{(\kappa)} s_{T_{\kappa}}^{(\kappa)}\\
			0\\
		\end{array} \right] . \nonumber 
	\end{align}
	
	The parameter settings can adhere to the following conditions: 
	% $\delta \in \left( 0,\min \left( 1,l \right) \right) $, $0<\tau \le \overline{\tau} \le \frac{2\overline{\alpha }\delta \beta}{nL^2}$, where
	% \begin{equation}
		% \overline{\alpha }:=\min \left\{ \frac{1-2\sigma}{L/\delta-\sigma},\frac{2\left( 1-\sigma \right) \delta}{L},1 \right\}.
		% \end{equation}
	$0<\lambda \le \underline{\lambda}$, where
	\begin{equation}
		\underline{\lambda }\triangleq \underset{i}{\min}\left\{ \frac{\tau}{2}\left| \bigtriangledown _i \mathcal{D}_K\left( \mathbf{0} \right) \right|^2:\bigtriangledown _i \mathcal{D}_K\left( \mathbf{0}\right) \ne 0 \right\}.
	\end{equation}
	
	\textit{Halting conditions:}
	It is reasonable to terminate the algorithm at the $\kappa$-th step if $\kappa$ reaches the maximum number of iterations or $\mathbf{C}^{(\kappa)}$ satisfies $\text{supp}\left( \mathbf{C}^{(\kappa)} \right) \subseteq T_{\kappa}=T_{\kappa-1}$ and $\lVert G_{\tau_{\kappa}}\left( \mathbf{C}^{(\kappa)};T_{\kappa} \right) \rVert \le 10^{-6}$. $\text{supp}\left(\mathbf{C}^{(\kappa)} \right) $ be its support set consisting of the indices of the nonzero elements of $\mathbf{C}^{(\kappa)}$.
	
	\begin{remark}
		%When juxtaposing 
		In comparing the ORLS solver limiting the polynomial order with the hybrid Newton algorithm for $\ell_0$-regularized polynomial optimization, it can be seen that the latter %which directly tackles the $\ell_0$-regularized optimization problem 
		does not rely on linear or linearized measurement as the former does but still needs $\mathcal{D}_K(\mathbf{C})$ strongly convex and needs to set four parameters $\sigma, \beta, \delta, \tau$ properly. For nonconvex data fitting models, measurement linearization/conversion as mentioned in Section \ref{sec:nonlinearMeasurement} that will result in strongly convex $\mathcal{D}_K(\mathbf{C})$ is practically considerable.   %linearization. 
	\end{remark}
	
	
	\begin{figure}[htbp]
		\centerline{\includegraphics[width=0.9\columnwidth]{NT-FoTL0O_set.png}}
		\caption{Flowchart of the hybrid Newton algorithm for T-FoT $\ell_0$ optimization.}
	\end{figure}
	
	% For notational convenience, let
	%  \begin{equation}
		%  S_{(\kappa)}=\widetilde{T}_{(\kappa)}\setminus T_{(\kappa-1)}.
		%  \end{equation}
	% where $\widetilde{T}_{(\kappa)}=\left\{ i\in \mathbb{N}:\left| c_i^{(\kappa)}-\tau p^{(\kappa)}_i \right|\ge \sqrt{2\tau \lambda} \right\}$.
	%\widetilde{T}_k=\left\{ i\in \mathbb{N}_n:\left| x_{i}^{k}-\tau g_{i}^{k} \right|\ge \sqrt{2\tau \lambda} \right\}.
	%\end{equation}
	% \begin{equation}
		% g^k:=\bigtriangledown f\left( x^k \right) , \ H_k:=\bigtriangledown _{T_k}^{2}f\left( x^k \right) ,\ G_k:=\bigtriangledown _{T_k,J_k}^{2}f\left( x^k \right).
		% \end{equation}
	
	
	
	%We summarize the framework of the algorithm in Algorithm \ref{algorithm1}.
	%\begin{algorithm}[!ht]
	%        \caption{Newton method for the T-FoT $\ell_0$ optimization(NT-FoTL0O)}\label{algorithm1}
	%        \KwIn{Parameters $\tau>0$, $\delta>0$, $0<\lambda \le \underline{\lambda}$, $\sigma \in \left( 0,\dfrac{1}{2} \right) $, $ \beta \in (0,1)$. Set $T_{-1}= \emptyset $ and $k \Leftarrow 0$.}
	%        \KwOut{$x^k$.}
	%
	%        \begin{algorithmic}[1]
		%    \WHILE {The halting conditions are violated}
		%          \STATE  {
			%          \IF {$S_k = \emptyset$}
			%          \STATE {$T_k=T_{k-1}$}.
			%          \ELSE
			%          \STATE
			%              {$T_k= \widetilde{T}_k$, where $\widetilde{T}_k=\left\{ i\in \mathbb{N}_n:\left| x_{i}^{k}-\tau g_{i}^{k} \right|\ge \sqrt{2\tau \lambda} \right\}.$}
			%          \ENDIF
			%          \IF {(\ref{eq:c_recurisive}) is solvable and its solution satisfies
				%          $
				%            \left< g_{T_k}^{k},d_{T_k}^{k} \right> \le -\delta \lVert d^k \rVert ^2+\left( \dfrac{1}{4\tau} \right) \lVert x_{\overline{T}_k}^{k} \rVert ^2
				%          $ }
			%          \STATE {update $d^k$ by Newton direction,
				%          $
				%            H_kd_{T_k}^{k}=G_kx_{J_k}^{k}-g_{T_k}^{k},d_{\overline{T}_k}^{k}=-x_{\overline{T}_k}^{k}.
				%          $}
			%          \ELSE
			%          \STATE {update $d^k$ by Gradient direction,
				%          $
				%            d_{T_k}^{k}=-g_{T_k}^{k},d_{\overline{T}_k}^{k}=-x_{\overline{T}_k}^{k}.
				%          $}
			%          \ENDIF
			%          \\
			%         \STATE  {Find the smallest non-negative integer $m_k$ such that
				%          $
				%            f\left( x^k\left( \beta ^{m_k} \right) \right) \le f\left( x^k \right) +\sigma \beta ^{m_k}\left< g^k,d^k \right>
				%          $}
			%          \\
			%          \STATE { Set $\alpha_k=\beta ^{m_k}$, $x^{k+1}=x^k(\alpha_k)$ and $k \Leftarrow k+1$.}
			%          }
		%          \ENDWHILE
		%%
		%%          \WHILE {The halting conditions are violated}
		%%
		%%            \STATE your idea;
		%%
		%%            \ENDWHILE
		%
		%   \STATE {\textbf{return} $x^k$;}
		%\end{algorithmic}
		%\end{algorithm}
		
		
		% The parameter settings can adhere to the following conditions: 
		% % $\delta \in \left( 0,\min \left( 1,l \right) \right) $, $0<\tau \le \overline{\tau} \le \frac{2\overline{\alpha }\delta \beta}{nL^2}$, where
		% % \begin{equation}
			% % \overline{\alpha }:=\min \left\{ \frac{1-2\sigma}{L/\delta-\sigma},\frac{2\left( 1-\sigma \right) \delta}{L},1 \right\}.
			% % \end{equation}
		% $0<\lambda \le \underline{\lambda}$, where
		% \begin{equation}
			% \underline{\lambda }:=\underset{i}{\min}\left\{ \frac{\tau}{2}\left| \bigtriangledown _if\left( 0 \right) \right|^2:\bigtriangledown _if\left( 0 \right) \ne 0 \right\}
			% \end{equation}
		
		
		
		
		
		
		\section{Extension to Multiple Target Case} \label{sec:extension-MTT}
		Given that the measurement-to-track (M2T) association can be properly resolved, the multiple target version of Problems 2 and 3 can be the same addressed for each target in parallel. So, both proposed T-FoT optimization solvers can be directly extended to multiple T-FoT optimization in parallel. All the theoretical results obtained so far hold for each target.  
		
		Let us index measurement received at time $t$ by $j \in \mathcal{J}_t, t \in [k',k]$ which may be either the real measurement of the target or the clutter, and the target T-FoT set $\mathcal{I}$ specified by a group of parameters $\big\{ \mathbf{C}_{(i)}\big\}_{i\in \mathcal{I}}$, where $\mathbf{C}_{(i)}$ represents the parameters of the $i$-th estimated trajectory which can be of different orders with each other. % (we disregard the subscript order $\gamma$). 
		Here, we propose a joint M2T association and multi-trajectory fitting framework as follows.
		\begin{align}
			& \big\{ \mathbf{C}_{(i)} \big\} _{i\in \mathcal{I}} = \underset{\left\{ \mathbf{C}_{(i)} \right\} _{i\in \mathcal{I}}}{\text{argmin}} \sum_{i\in \mathcal{I}} \Big(   \lambda_i \lVert \mathbf{C}_{(i)}\rVert_0 +  \nonumber \\
			&~~~~~~ \sum_{t=k'}^k \sum_{j\in \mathcal{J}_t} \mathbf{1}_i \big( \mathbf{y}_{t}^{(j)}  \big)  \big\lVert \mathbf{y}_{t}^{(j)}-h_t\big( F( t;\mathbf{C}_{(i)} ) ,\bar{\mathbf{v}}_{t}^{(j)} \big) \big\rVert^2_{\text{var}(\mathbf{y}_t^{(j)})}  \Big)  ,\label{eq:MTT_C_k} \\
			& s.t. \ \ \mathbf{1}_i \big( \mathbf{y}_{t}^{(j)} \big) \in \left\{ 0,1 \right\}, \forall i\in \mathcal{I}, j\in \mathcal{J}_t, t\in [k',k] \nonumber \\
			& ~~~~~~ \sum_{i\in \mathcal{I}}{\mathbf{1}_i \big( \mathbf{y}_{t}^{(j)} \big)}\le 1, \forall j\in \mathcal{J}_t,  t\in [k',k] \label{eq:i_I-const} \\
			& ~~~~~~ \sum_{j\in \mathcal{J}_t} {\mathbf{1}_i \big( \mathbf{y}_{t}^{(j)} \big)}\le 1, \forall i\in \mathcal{I} , t\in [k',k]\label{eq:j_J-const} 
		\end{align}
		where $\lambda_i$ is the constraint coefficient of the $i$-th polynomial T-FoT, $\mathbf{1}_i \big( \mathbf{y}_{t}^{(j)} \big)$ represents the association of the $j$-th measurement received at time $t$ with the $i$-th trajectory, $\mathbf{1}_i \big( \mathbf{y}_{t}^{(j)} \big) =1$ for associated and $\mathbf{1}_i \big( \mathbf{y}_{t}^{(j)} \big) =0$ otherwise, \eqref{eq:i_I-const} is due to the constraint that each measurement can at most be associated to one trajectory while \eqref{eq:j_J-const} limits that each trajectory can at most be associated with one measurement at any time instant. 
		
		% To add the $\ell_0$ norm regularization, the joint optimization framework is extended to 
		
		% \begin{align}
			% & \big\{ C_k^{(i)} \big\} _{i\in I} = \underset{\left\{ C^{(i)} \right\} _{i\in I}}{\text{argmin}} \sum_{i\in I} \Big(  \lambda \lVert C^{(i)}\rVert_0 + \nonumber \\
			% & \sum_{t=k'}^k \sum_{j\in J_t} \big\lVert z_{t}^{(j)}-h_t\big( F( t;C^{(i)} ) ,\bar{v}_{t}^{(j)} \big) \big\rVert^2_{\text{var}(z_t^{(j)})} \times \mathbf{1}_i \big( z_{t}^{(j)} \big) \Big)  
			% \end{align}
		
		
		\begin{remark}
			The multi-target T-FoT model \eqref{eq:MTT_C_k} aims to solve the problem of data association and trajectory fitting jointly, which is nontrivial especially when the number of targets is unknown and false/missing data are presented. The optimization becomes more challenging when the order of each T-FoT is not given in advance but has to be taken into account in the cost function. %traded-off with the fitting accuracy in order to avoid over/under fitting. 
			We leave these implementation problems aside but focus on the fundamental problem of comparing and evaluating a given set of T-FoT estimates based on the available ground truth. 
		\end{remark}
		
		
		\begin{remark}
			The above formulation for joint M2T association and multi-target trajectory fitting is intractable. Our approach solves it by separating the M2T association and fitting problems, and thus cannot guarantee optimality. 
			In fact, the M2T association itself, especially in the presence of clutter, is intractable, which has been a long-standing challenge in the tracking community \cite{Stefano2022DA}. The question of how to efficiently solve this joint optimization problem remains open.  
		\end{remark}
		
		
		\section{Simulation}\label{sec:simulation}
		We will consider both single and multiple target tracking scenarios in our simulation. %given proper data association solution. 
		In the former, the real target trajectory is generated using the traditional SSM (and so the real trajectory is a time-series of discrete points, not a real continuous-time T-FoT) while in the latter, the real trajectories are just generated by polynomial T-FoTs. In both simulation scenarios, the proposed T-FoT approaches including the $\gamma$-limiting ORLS and $\ell_0$-hybrid Newton algorithms are carried out with a sliding time window of maximum $T_\text{w}=10$ sampling steps (corresponding to 1 second in total). The parameters needed in the hybrid Newton approach are set as given in Table \ref{tab:table1}. For comparison, the polynomial T-FoTs of fixed order $\gamma =1$ and $\gamma =2$ are also considered. 
		% is assumed as follows 
		% \begin{equation}
			% \left\{ \begin{array}{l}
				% 	p_{x,t}=c_{x,0}+c_{x,1}t\\
				% 	p_{y,t}=c_{y,0}+c_{y,1}t\\
				% \end{array} \right. .
			% \end{equation}
		Furthermore, %as addressed in section \ref{sec:Newton}, 
		comparison methods also include the $\ell_1$-ADMM algorithm which substitutes the $\ell_0$-norm by $\ell_1$-norm, namely $\varOmega _F\left( \mathbf{C}_{\gamma} \right) \triangleq  \lVert \mathbf{C}_{\gamma} \rVert _1$ in \eqref{eq:Const-T-FoT-Op}, and then adopts the ADMM solver \cite{lou2018fast} to estimate the T-FoT. In addition, the KF is also simulated in the single-target tracking case. 
		% as well as the favorable value for each parameter used in our simulation.
		
		
		\subsection{Single Maneuvering Target Tracking}
		This simulation scenario is analogous to the one presented in Section 4.1.4 of \cite{hartikainen2008optimal}, where the motion of a maneuvering object switches between Wiener process velocity (WPV) with a low process noise of power spectral density $0.1$, and Wiener process acceleration (WPA) 
		with a high process noise of power spectral density $1$. The system is simulated with $100$ sampling steps, each of $1$s. % (with the sensor revisit interval $\bigtriangleup =1s$). 
		The real target motion model was manually set to WPV during steps $1$s-$30$s, $46$s-$70$s and $86$s-$100$s and to WPA during steps $31$s-$45$s, and $71$s-$85$s. This leads to four times of maneuvering. 
		
		% The true starting state of the system is set to $\mathbf{x}_0=\left[ \begin{matrix}
			% 	0&		0&		0&		-1&		0&		0\\
			% \end{matrix} \right] ^{\text{T}}$.
		We consider the linear measurement with additive, zero-mean noise, namely 
		\begin{equation}\label{eq:LinearMeansure-sim}
			\mathbf{y}_k= \mathbf{x}_k + \mathbf{v}_k,
		\end{equation}
		where the measurement noise is white Gaussian satisfying 
		\begin{equation} 
			\mathrm{E}\left[ {\mathbf{v}_k} \right] = 0, ~ \mathrm{E}\left[ {\mathbf{v}_k\mathbf{v}_j^\text{T}} \right] = \left[ {\begin{array}{*{20}{c}}
					{100}&0\\
					0&{100}
			\end{array}} \right]{\delta _{kj}},
		\end{equation}
		where \(\delta _{kj}\) %is the Kronecker-delta function which 
		is equal to one if \(k=j\) and to zero otherwise.
		
		
		% \begin{table}[!t]
			% \caption{Setting of Parameters in hybrid Newton method\label{tab:table1}}
			% \centering
			% \begin{tabular}{|c||c|}
				% \hline
				% Parameters& Value Used \\
				% \hline
				% $\sigma$  &  $5\times 10^{-5}$\\
				% \hline
				% $\beta$ &  0.5 \\
				% \hline
				%  $\delta$ &  $10^{-10}$ \\
				% \hline
				%  $\tau$&  1 \\
				% \hline
				% \end{tabular}
			% \end{table}
		
		\begin{table}[!t]
			\caption{Setting of Parameters in hybrid Newton method\label{tab:table1}}
			\centering
			\begin{tabular}{ c | c}
				\hline\noalign{\smallskip}
				Parameters& Value Used \\ \noalign{\smallskip}\hline \noalign{\smallskip}
				$\sigma$  &  $5\times 10^{-5}$    \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\beta$ &  0.5  \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\delta$ &  $10^{-10}$     \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\tau$&  1    \\ \noalign{\smallskip}\hline
			\end{tabular}
		\end{table}
		
		The simulation is carried out for $50$ Monte Carlo runs, where each run involves generating a trajectory randomly originating from the same initial point but different process noises and observation series, according to the above statistical models. The real trajectory and T-FoT estimates in one run are given in Fig. \ref{fig1}. %As shown, all T-FoT estimators estimate the trajectory well. 
		To gain further insights, the average root mean square error (RMSE) of the position estimation over time is given in Fig. \ref{fig2} and the time-averaged RMSE (over 100 sampling steps), as well as the average computing time for each step, are given in Table \ref{tab:table2}. The simulation is conducted on MATLAB R2018b. 
		
		\begin{itemize}
			\item  On the estimation accuracy, %regardless of the optimization algorithm employed, 
			%both proposed regularized polynomial T-FoT approaches namely  and 
			the $\gamma$-limiting ORLS algorithm  performs the best, the proposed $\ell_0$-hybrid Newton approach the second, both outperforming the fitting using fixed-order (whether $\gamma=1$ or $\gamma=2$) and the $\ell_1$-ADMM approach. %The tracking accuracy achieved by the $\gamma$-limiting ORLS algorithm is inferior to that obtained through the $\ell_0$-hybrid Newton method and the $\ell_1$-ADMM. 
			% All these T-FoT approaches except that using fixed order $\gamma=1$ outperform the KF that uses a fixed model and does not suit target maneuvering. %It is obvious that the direct processing optimization method surpasses that of the algorithm based on the $\ell_0$ convex relaxation approximation.
			The results demonstrated the importance of model adaption (the order of the T-FoT) in case of target maneuvering. It also confirms that $\ell_1$ regularization which may result in a high order yet minimized sum of the polynomial coefficients is undesired in our case. 
			
			\item  On the computing speed, all the three regularized T-FoT approaches (using adaptive orders) are unsurprisingly slower than those using fixed order. In particular, the hybrid Newton approach for $\ell_0$-regularized optimization suffers from the slowest calculation speed. It is valuable yet challenging to speed up this algorithm for practical use. The ORLS solver with limiting polynomial order $\gamma$ offers much superior computational speed compared to the direct hybrid Newton algorithm. While the $\ell_1$-ADMM approach runs fast too, close to the $\gamma$-limiting ORLS approach, its accuracy is further reduced. %However, we must note that the computing efficiency is software-dependent, and the results showed here are based on MATLAB R2018b, which features a powerful fitting toolbox.
		\end{itemize}
		
		
		
		\begin{figure}[htbp]
			\vspace{-2mm}
			\centerline{\includegraphics[width=0.8\columnwidth]{single_scen.eps}}
			\caption{Real trajectory and estimates given by different  estimators in one trial.}
			\label{fig1}
			\vspace{-2mm}
		\end{figure}
		
		
		\begin{figure}[htbp]
			\vspace{-2mm}
			\centerline{\includegraphics[width=0.8\columnwidth]{rmse_add_admm.eps}}
			\caption{RMSE of different  estimators over sampling steps.}
			\label{fig2}
			\vspace{-2mm}
		\end{figure}
		
		% \begin{table}[!t]
			% \caption{Average performance of different estimators over 200 sampling steps in the single target case \label{tab:table2}}
			% \centering
			% \begin{tabular}{|c|c|c|}
				% \hline
				% Estimators & Aver. RMSE (m) & Aver. Time (s)\\
				% \hline
				% KF  & 2.0225  & \textbf{0.0057}\\
				% \hline
				% Fixed order $\gamma =1$  & 4.4357 & 0.1052\\
				% \hline
				% Fixed order $\gamma =2$  & 0.8024 & 0.0807\\
				% \hline
				% $\ell_1$-ADMM & 0.6440& 0.1660\\
				% \hline
				% $\gamma$-limiting ORLS & 0.5412 & 0.1307\\
				% \hline
				% $\ell_0$-hybrid Newton & \textbf{0.4361}& 66.3188\\
				% \hline
				% \end{tabular}
			% \end{table}
		
		\begin{table}[!t]
			\caption{Average performance of different estimators over 100 sampling steps in the single target case \label{tab:table2}}
			\centering
			\begin{tabular}{ c | c c}
				\hline\noalign{\smallskip}
				Estimators    & Aver. RMSE (m) & Aver. Time (s) \\ \noalign{\smallskip}\hline \noalign{\smallskip}
				Fixed order $\gamma =1$   &  34.1762  & 0.0409     \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				Fixed order $\gamma =2$  & 14.7818   & 0.0389 \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\ell_1$-ADMM   &  14.2232 & 0.0644    \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\gamma$-limiting ORLS  & \textbf{12.5376}   & 0.0821 \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\ell_0$-hybrid Newton  & 13.9804 & 22.5460     \\ \noalign{\smallskip}\hline 
			\end{tabular}
		\end{table}
		
		% \subsection{Nonlinear observation maneuvering target tracking}
		% %
		% This simulation is set the same as that given in in Section 4.2.2 of \cite{hartikainen2008optimal}. To simulate the deterministic target motion, two Markov models using insignificant noises are assumed with sampling step size $\bigtriangleup =0.5s$. The first is given by a single linear WPV model with insignificant process noise (zero-mean and power spectral density $0.01$). The other is given by a combination of this WPV model with a nonlinear constant turn-rate (CT) model (no position and velocity noise but zero-mean turn rate noise with covariance $0.15$).
		
		% %The measurement is made on the noisy bearing of the object, which is given by four sensors located at $\left[ s_{x,1},s_{y,1} \right] ^{\text{T}}=\left[ -0.5,3.5 \right] ^{\text{T}}$, $\left[ s_{x,2},s_{y,2} \right] ^{\text{T}}=\left[ -0.5,-3.5 \right] ^{\text{T}}$, $\left[ s_{x,3},s_{y,3} \right] ^{\text{T}}=\left[ 7,-3.5 \right] ^{\text{T}}$ and $\left[ s_{x,4},s_{y,4} \right] ^{\text{T}}=\left[ 7,3.5 \right] ^{\text{T}}$, respectively.
		
		% The range-bearing measurement model is used where $v_{r,k}$ and $v_{\theta,k}$ are, individually, independent identical distributed zero-mean Gaussian with standard deviation $\sigma_r=0.01m$ and $\sigma_{\theta}=0.01rad$, respectively.
		
		% \begin{figure}[htbp]
			% \centerline{\includegraphics[width=\columnwidth]{nonlin.eps}}
			% \caption{Nonlinear simulation scenario: Real trajectory (starting from $\triangle$ and ending at $\Box$).}
			% \label{fig6.3}
			% \end{figure}
		
		% Upon examining Table \ref{tab:table3}, it becomes evident that the track fitting utilizing $\ell_0$ optimization yields a significant enhancement in tracking accuracy, with a $70\%$ improvement compared to fixed-order fitting. This notable improvement underscores the effectiveness of $\ell_0$ optimization in refining track fitting, thereby providing more precise tracking results.
		
		% While there is a slight increase in computational cost, the $\ell_0$ optimization method with ORLS incurs only a negligible delay of a few milliseconds. This minor delay is a small trade-off for the considerable gains in accuracy and suggests that the method is well-suited for real-time applications where both precision and efficiency are critical.
		
		% Conversely, the direct processing optimization method utilizing the hybrid Newton method exhibits a considerably longer computation time. While this method may offer high accuracy, it comes with increased computational demands, making it less practical for scenarios that require rapid processing.
		
		% \begin{table}[!t]
			% \caption{Average performance of different estimatiors\label{tab:table3}}
			% \centering
			% \begin{tabular}{|c||c||c|}
				% \hline
				% Estimators & Aver.RMSE & Compt.Time(s)\\
				% \hline
				% Fixed Fitting  & 0.6027& \textbf{0.0756}\\
				% \hline
				% $\ell_0$-ORLS & \textbf{0.2063}& 0.1207\\
				% \hline
				% $\ell_0$-hybrid Newton & 0.2094& 20.7860\\
				% \hline
				% \end{tabular}
			% \end{table}
		
		
		
		\subsection{Multiple Target Tracking}
		%To avoid distracting the attention to our key contribution, we 
		We now consider a more challenging tracking scenario with two targets, each with a lifespan from time $1$s to time $100$s. One target exhibits maneuvers at times $20$s and $70$s, whereas the other target maneuvers around time $40$s. The trajectories that are generated using polynomial curves (for which a maneuver occurs when the polynomial order changes) in one run are given in Fig. \ref{fig:multi_scen}. Similar with \eqref{eq:LinearMeansure-sim}, measurements are made on the 2-dimensional position with zero-mean Gaussian noise of standard deviation $1$m. The two targets are so well separated that the position measurements and the tracks can be correctly associated by using the standard global nearest-neighbor approach. 
		The target detection probability $P_D=1$. Clutter follows a Poisson model with an average $15$ clutter-points per measuring time  
		%a uniform intensity $\kappa _k\left( z \right) =1.0417 \times 10^{-4}$ m$^{-2}$ 
		in the region $\left[-170, 150\right]$m $\times \left[-150, 300\right]$m. 
		Thanks to the unit detection probability and the perfect M2T association, the T-FoT estimators are free of the MD problem. % on the observation region. 
		%Again, a sliding time window of maximum 10 sampling steps (1s in total) and a polynomial T-FoT of fixed order $\gamma = 2$ were utilized. 
		Due to the absence of the target dynamics and the background profile, no Bayesian filters can be properly set up, and so our comparison includes only T-FoT solvers using different polynomials. 
		
		For the performance evaluation, both the OSPA (optimal sub-pattern assignment) \cite{schuhmacher2008consistent} and the Star-ID (spatio-temporal-aligned trajectory integral distance) \cite{Li25TFoT-part1} metrics are used. The former compares the real states with the point state estimates extracted from the estimated T-FoT at each measuring time while the latter calculates the integral distance between the estimated and real T-FoTs in the fitting time-window part which is basically a cumulative distance of trajectories over the concerning time-window. We set the cutoff parameter $c= 20$m for the OSPA and relatively segment/trajectory cutoff parameter $c_\text{S}=c_\text{T}= 20$m for the Star-ID, and the same metric order $p=2$ for both of them.  
		The average OSPA error and Star-ID of different T-FoT estimators over time are shown in Fig. \ref{fig:ospa} and Fig. \ref{fig:Star-ID}, respectively. 
		Furthermore, the time-averaged Star-ID (TA-Star-ID \cite{Li25TFoT-part1}) of different T-FoT estimators which divides the Star-ID by the length of the concerning time-window is shown in Fig. \ref{fig:TA-Star-ID}. 
		The averages of the TA-Star-ID, of the OSPA distances and of the computing time over 100 sampling steps are given in Table \ref{tab:table3}. 
		
		
		
		\begin{figure}[htbp]
			\centerline{\includegraphics[width=0.85\columnwidth]{multi_scen_o1_o2.eps}}
			\caption{Real and estimated T-FoTs of two targets in the cluttered environment.}
			\label{fig:multi_scen}
		\end{figure}
		
		\begin{figure}[htbp]
			\centerline{\includegraphics[width=0.8\columnwidth]{ospa_multi.eps}}
			\caption{Average OSPA error of different T-FoT estimators over time.}
			\label{fig:ospa}
		\end{figure}
		
		\begin{figure}[htbp]
			\centerline{\includegraphics[width=0.8\columnwidth]{starid_o1_o2.eps}}
			\caption{Average Star-ID error of different T-FoT estimators over time.}
			\label{fig:Star-ID}
		\end{figure}
		
		\begin{figure}[htbp]
			\centerline{\includegraphics[width=0.8\columnwidth]{ta_starid_winl.eps}}
			\caption{Average TA-Star-ID error of different T-FoT estimators over time.}
			\label{fig:TA-Star-ID}
		\end{figure}
		
		% \begin{figure*}[htbp]
			% \centerline{\includegraphics[width=2.4\columnwidth]{sum_metric.eps}}
			% \caption{Average TA-Star-ID of different T-FoT estimators over time.}
			% \label{fig:TA-Star-ID}
			% \end{figure*}
		
		% \begin{table*}[!t]
			% \caption{Average performance of different estimators over 100 simulation steps in multi-target case \label{tab:table3}}
			% \centering
			% \begin{tabular}{|c||c|c|c|c|}
				% \hline
				% T-FoT solver & OSPA (m) & TA-Star-ID (m) & Star-ID (m$\cdot$s) & Time (s)\\
				% \hline
				% Fixed order $\gamma =1$ &16.9798 & 49.7611 & 1185.4049 &0.1198\\
				% \hline
				% Fixed order $\gamma =2$ &3.4852 & 37.2427&  931.4229 &\textbf{0.0959}\\
				% \hline
				% $\ell_1$-ADMM & 1.1381 & 31.2683 & 716.8125& 0.6407\\
				% \hline
				% $\gamma$-limiting ORLS & 1.0720 & 31.1933 & 714.2782&  0.1964\\
				% \hline
				% $\ell_0$-hybrid Newton & \textbf{0.7879} & \textbf{30.9620}&\textbf{709.5508} & 47.0239\\
				% \hline
				% \end{tabular}
			% \end{table*}
		
		\begin{table*}[!t]
			\caption{Average performance of different estimators over 100 simulation steps in the multi-target case \label{tab:table3}}
			\centering
			\begin{tabular}{ c | cccc}
				\hline\noalign{\smallskip}
				T-FoT solver & OSPA (m) & TA-Star-ID (m) & Star-ID (m$\cdot$s) & Time (s) \\ \noalign{\smallskip}\hline \noalign{\smallskip}
				Fixed order $\gamma =1$ &11.6494 & 59.5925 & 580.8893 &0.0817     \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				Fixed order $\gamma =2$ &3.5599 & 58.7122 & 572.1178 &\textbf{ 0.0812} \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\ell_1$-ADMM & 2.8418 & 58.6648 &  571.6438 & 0.1254    \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\gamma$-limiting ORLS & 1.3761 & 58.6200 & 571.1960 &  0.1499 \\ \noalign{\smallskip}\hline\noalign{\smallskip}
				$\ell_0$-hybrid Newton & \textbf{1.3447} & \textbf{58.1211} & \textbf{566.9380} & 27.6312     \\ \noalign{\smallskip}\hline
			\end{tabular}
		\end{table*}
		
		
		Regarding the estimation accuracy, it is evident that in terms of both OSPA distance and Star-ID, the polynomial fitting using the $\ell_0$-hybrid Newton approach performs the best again in all, the $\gamma$-limiting ORLS the second and the $\ell_1$-ADMM the third, all outperforming significantly the fitting with a fixed order. 
		%Concurrently, the average distance achieved by the hybrid Newton method surpasses that of the ORLS and ADMM algorithm. When comparing the error graphs of Fig. \ref{fig:ospa} and Fig. \ref{fig:Star-ID} (different evaluation metrics), it is evident that, although the trends of their average errors are consistent, the error graph for Star-ID represents the cumulative error of the target from the beginning to the current moment. In contrast, OSPA calculates the error at the current moment only. Consequently, the Star-ID graph almost invariably exhibits an increasing trend. 
		However, the hybrid Newton method exhibits a considerably much higher computing cost compared to the others, making it impractical for real-time scenarios. In contrast, the slight increase of the computational cost in the proposed $\gamma$-limiting ORLS approach together with its outstanding accuracy make it well-suited for real-time applications. %, the ORLS incurs only a negligible delay of a few milliseconds. This minor delay is a small trade-off for the considerable gains in accuracy and suggests that the method is well-suited for real-time applications where both precision and efficiency are critical.
		%Conversely, %the direct processing optimization method utilizing 
		
		In comparing Fig. \ref{fig:TA-Star-ID} with Fig. \ref{fig:ospa}, the simulation also confirms that the TA-Star-ID performs consistently with the OSPA metric while the former, which accounts for the whole trajectory-segment in the time-window, varies much smoother than the latter in the time series. This precisely aligns with the remarkable distinction between the trajectory estimator and the point state estimator. 
		
		
		\section{Conclusion}\label{sec:conclusion}
		Our series of work aims to establish a data-driven SP-based learning-for-tracking framework which yields the continuous-time trajectory rather than discrete-time point estimates. This paper, as part II of the series, addresses the learning of the trajectory SP trend by polynomial, which plays a key role in the whole SP learning \cite{Li25TFoT-part3}. In particular, we address the polynomial T-FoT optimization with two distinctive strategies of regularization, balancing the fitting accuracy and the polynomial smoothness/simplicity. One greedily searches the optimal order $\gamma$ of the polynomial in a narrow, bounded range. In particular, the $\gamma$-limiting ORLS solver that fits the linear measurement model is detailed. The other employs a hybrid Newton approach to address the NP-hard $\ell_0$ regularization with convex data fitting model. 
		The simulation results demonstrate that our proposed T-FoT optimization method significantly outperforms fixed-order polynomial fitting approaches, as well as the $\ell_1$ regularization solved by the ADMM method, in terms of tracking accuracy in the case of both single and multiple maneuvering target tracking. While the computing speed of $\gamma$-limiting ORLS solver is acceptable, the hybrid Newton approach for $\ell_0$ regularization is computationally intensive and not suitable for online tracking. More effort is needed to solve the regularized optimization in real time and integrate its learning with that of the RSP. % for which the neural network can be considered. %Future work should pay attention to the computing efficiency of these optimization algorithms.  


% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
	\providecommand{\url}[1]{#1}
	\csname url@samestyle\endcsname
	\providecommand{\newblock}{\relax}
	\providecommand{\bibinfo}[2]{#2}
	\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
	\providecommand{\BIBentryALTinterwordstretchfactor}{4}
	\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
		\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
	\providecommand{\BIBforeignlanguage}[2]{{%
			\expandafter\ifx\csname l@#1\endcsname\relax
			\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
			\typeout{** loaded for the language `#1'. Using the pattern for}%
			\typeout{** the default language instead.}%
			\else
			\language=\csname l@#1\endcsname
			\fi
			#2}}
	\providecommand{\BIBdecl}{\relax}
	\BIBdecl
	
	\bibitem{bar2004estimation}
	Y.~Bar-Shalom, X.~R. Li, and T.~Kirubarajan, \emph{Estimation with applications to tracking and navigation: theory algorithms and software}.\hskip 1em plus 0.5em minus 0.4em\relax Hoboken, NJ, USA: John Wiley \& Sons, 2004.
	
	\bibitem{vo2015multitarget}
	B.~Vo, M.~Mallick, Y.~Bar-Shalom, S.~Coraluppi, R.~Osborne, R.~Mahler, and B.~Vo, ``Multitarget tracking wiley encyclopedia of electrical and electronics engineering,'' \emph{Wiley Encyclopedia of Electrical and Electronics Engineering}, pp. 1--15, 2015.
	
	\bibitem{Sarkka13book}
	S.~Sarkka, \emph{Bayesian Filtering and Smoothing}.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Cambridge University Press, 2013.
	
	\bibitem{Li2017AGC}
	T.~Li, J.~Su, W.~Liu, and C.~JM, ``Approximate Gaussian conjugacy: parametric recursive filtering under nonlinearity, multimodality, uncertainty, and constraint, and beyond,'' \emph{Frontiers Inf Technol Electronic Eng}, vol.~18, pp. 1913--1939, 2017.
	
	\bibitem{li2005survey}
	X.~R. Li and V.~P. Jilkov, ``Survey of maneuvering target tracking. Part V. multiple-model methods,'' \emph{IEEE Trans. Aerosp. Electron. Syst.}, vol.~41, no.~4, pp. 1255--1321, 2005.
	
	\bibitem{Zhang24L-GBM}
	C.~Zhang, J.~Deng, and W.~Yi, ``Data-driven online tracking filter architecture: A lightgbm implementation,'' \emph{Signal Process.}, vol. 221, p. 109477, 2024.
	
	\bibitem{li2016effectiveness}
	T.~Li, J.~M. Corchado, J.~Bajo, S.~Sun, and J.~F. De~Paz, ``Effectiveness of bayesian filters: An information fusion perspective,'' \emph{Inf. Sci.}, vol. 329, pp. 670--689, 2016.
	
	\bibitem{fan2011impact}
	H.~Fan, Y.~Zhu, and Q.~Fu, ``Impact of mode decision delay on estimation error for maneuvering target interception,'' \emph{IEEE Trans. Aerosp. Electron. Syst.}, vol.~47, no.~1, pp. 702--711, 2011.
	
	\bibitem{xiang2018impact}
	S.~Xiang, H.~Fan, and Q.~Fu, ``Impact of mode decision delay on estimation error in continuous-time controlled system,'' \emph{IEEE Access}, vol.~6, pp. 73\,265--73\,272, 2018.
	
	\bibitem{zhang2022fast}
	C.~Zhang, J.~Deng, W.~Yi, and X.~Lu, ``A fast and robust maneuvering target tracking method without markov assumption,'' in \emph{Proc. FUSION 2022}, Link{\"o}ping, Sweden, July 2022, pp. 1--8.
	
	\bibitem{gao2019long}
	C.~Gao, J.~Yan, S.~Zhou, P.~K. Varshney, and H.~Liu, ``Long short-term memory-based deep recurrent neural networks for target tracking,'' \emph{Inf. Sci.}, vol. 502, pp. 279--296, 2019.
	
	\bibitem{Ghosh2024Dense}
	A.~Ghosh, A.~Honor\'e, and S.~Chatterjee, ``Danse: Data-driven non-linear state estimation of model-free process in unsupervised learning setup,'' \emph{IEEE Trans. Signal Process.}, vol.~72, pp. 1824--1838, 2024.
	
	\bibitem{Pinto2023DMTT}
	J.~Pinto, G.~Hess, W.~Ljungbergh, Y.~Xia, H.~Wymeersch, and L.~Svensson, ``Deep learning for model-based multiobject tracking,'' \emph{IEEE Trans. Aerosp. Electron. Syst.}, vol.~59, no.~6, pp. 7363--7379, 2023.
	
	\bibitem{Zhang24transformer}
	Y.~Zhang, G.~Li, X.-P. Zhang, and Y.~He, ``A deep learning model based on transformer structure for radar tracking of maneuvering targets,'' \emph{Inf. Fusion}, vol. 103, p. 102120, 2024.
	
	\bibitem{li2018joint}
	T.~Li, H.~Chen, S.~Sun, and J.~M. Corchado, ``Joint smoothing and tracking based on continuous-time target trajectory function fitting,'' \emph{IEEE Trans. Autom. Sci. Eng.}, vol.~16, no.~3, pp. 1476--1483, 2018.
	
	\bibitem{li2023target}
	T.~Li, Y.~Song, and H.~Fan, ``From target tracking to targeting track: A data-driven yet analytical approach to joint target detection and tracking,'' \emph{Signal Process.}, vol. 205, p. 108883, 2023.
	
	\bibitem{Talbot2024continuoustimestate}
	W.~Talbot, J.~Nubert, T.~Tuna, C.~Cadena, F.~Dümbgen, J.~Tordesillas, T.~D. Barfoot, and M.~Hutter, ``Continuous-time state estimation methods in robotics: A survey,'' 2024.
	
	\bibitem{li2016fitting}
	T.~Li, J.~Prieto, and J.~M. Corchado, ``Fitting for smoothing: A methodology for continuous-time target track estimation,'' in \emph{Proc. IPIN 2016}, Alcala de Henares, Spain, October 2016, pp. 1--8.
	
	\bibitem{Li25TFoT-part2}
	T.~Li, Y.~Song, G.~Li, and H.~Li, ``From target tracking to targeting track — \cal{P}art \cal{II}: Regularized polynomial trajectory optimization,'' \emph{Companion Paper, arXiv preprint}, 2025.
	
	\bibitem{Li25TFoT-part3}
	T.~Li, J.~Wang, G.~Li, and D.~Gao, ``From target tracking to targeting track — \cal{P}art \cal{III}: Stochastic process modeling and online learning,'' \emph{Companion Paper, arXiv preprint}, 2025.
	
	\bibitem{Wold1938}
	H.~Wold, \emph{A Study in the Analysis of Stationary Time Series}.\hskip 1em plus 0.5em minus 0.4em\relax Uppsala: Almqvist \& Wiksell, 1938.
	
	\bibitem{Cramer1961}
	H.~Cram\'er, ``On some classes of nonstationary stochastic processes,'' \emph{Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability}, vol.~2, pp. 57--78, 1961.
	
	\bibitem{Box1994}
	G.~E.~P. Box and G.~M. Jenkins, \emph{Time Series Analysis: Forecasting and Control}.\hskip 1em plus 0.5em minus 0.4em\relax Upper Saddle River, NJ, United States: Prentice Hall PTR, 1994.
	
	\bibitem{Li25TFoT-part1}
	T.~Li, Y.~Song, H.~Fan, and J.~Chen, ``From target tracking to targeting track — \cal{P}art \cal{I}: A metric for spatio-temporal trajectory evaluation,'' \emph{Companion Paper, arXiv preprint}, 2025.
	
	\bibitem{Hodrick1997postwar}
	R.~J. Hodrick and E.~C. Prescott, ``Postwar u.s. business cycles: An empirical investigation,'' \emph{Journal of Money, Credit and Banking}, vol.~29, no.~1, pp. 1--16, 1997.
	
	\bibitem{Urbin2012time}
	G.~Urbin and S.~J. Koopman, \emph{Time Series Analysis by State Space Methods}.\hskip 1em plus 0.5em minus 0.4em\relax Oxford: Oxford University Press, 2012.
	
	\bibitem{Rudd94}
	J.~G. Rudd, R.~A. Marsh, and J.~A. Roecker, ``Surveillance and tracking of ballistic missile launches,'' \emph{IBM J. Res. Dev.}, vol.~38, no.~2, pp. 195--216, 1994.
	
	\bibitem{Wang94}
	Z.~Wang and H.~Zhou, ``Mathmetical processing to tracking data of range and range rate,'' \emph{Chin. Space Sci. Technol.}, vol.~3, pp. 17--24, 1994.
	
	\bibitem{Anderson-Sprecher96}
	R.~Anderson-Sprecher and R.~V. Lenth, ``Spline estimation of paths using bearings-only tracking data,'' \emph{J Am Stat Assoc.}, vol.~91, no. 433, pp. 276--283, 1996.
	
	\bibitem{Fan96LocalPolynomial}
	J.~Fan, \emph{Modelling and Its Applications: Monographs on Statistics and Applied Probability}.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Routledge, 1996.
	
	\bibitem{Tian22PolyFit}
	Z.~Tian, K.~Yang, M.~Danino, Y.~Bar-Shalom, and B.~Milgrom, ``Launch point estimation with a single passive sensor without trajectory state estimation,'' \emph{IEEE Trans. Aerosp. Electron. Syst.}, vol.~58, no.~1, pp. 318--327, 2022.
	
	\bibitem{li2018single}
	T.~Li, ``Single-road-constrained positioning based on deterministic trajectory geometry,'' \emph{IEEE Commun. Lett.}, vol.~23, no.~1, pp. 80--83, 2018.
	
	\bibitem{Hadzagic2011batchSpline}
	M.~Hadzagic and H.~Michalska, ``A bayesian inference approach for batch trajectory estimation,'' in \emph{Proc. FUSION 2011}, Chicago, IL, USA, 2011, pp. 1--8.
	
	\bibitem{Furgale12}
	P.~Furgale, T.~D. Barfoot, and G.~Sibley, ``Continuous-time batch estimation using temporal basis functions,'' in \emph{Proc. ICRA 2012}, Saint Paul, MN, USA, May 2012, pp. 2088--2095.
	
	\bibitem{Tirado2022Spline}
	J.~Tirado and J.~Civera, ``Jacobian computation for cumulative b-splines on se(3) and application to continuous-time object tracking,'' \emph{IEEE Robotics and Automation Letters}, vol.~7, no.~3, pp. 7132--7139, 2022.
	
	\bibitem{Pacholska20}
	M.~Pacholska, F.~Dümbgen, and A.~Scholefield, ``Relax and recover: Guaranteed range-only continuous localization,'' \emph{IEEE Robot. Autom. Lett.}, vol.~5, no.~2, pp. 2248--2255, 2020.
	
	\bibitem{beck2017first}
	A.~Beck, \emph{First-order methods in optimization}.\hskip 1em plus 0.5em minus 0.4em\relax Philadelphia, USA: Society for Industrial and Applied Mathematics, 2017.
	
	\bibitem{zhou2021newton}
	S.~Zhou, L.~Pan, and N.~Xiu, ``Newton method for $\ell_0$-regularized optimization,'' \emph{Numer. Algorithms}, pp. 1--30, 2021.
	
	\bibitem{attouch2013convergence}
	H.~Attouch, J.~Bolte, and B.~F. Svaiter, ``Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward--backward splitting, and regularized gauss--seidel methods,'' \emph{Math. Program.}, vol. 137, no.~1, pp. 91--129, 2013.
	
	\bibitem{beck2018proximal}
	A.~Beck and N.~Hallak, ``Proximal mapping for symmetric penalty and sparsity,'' \emph{SIAM J. Optim.}, vol.~28, no.~1, pp. 496--527, 2018.
	
	\bibitem{stoica2004model}
	P.~Stoica and Y.~Selen, ``Model-order selection: a review of information criterion rules,'' \emph{IEEE Signal Processing Magazine}, vol.~21, no.~4, pp. 36--47, 2004.
	
	\bibitem{Ye21Cholesky}
	Q.~Ye, A.~A. Amini, and Q.~Zhou, ``Optimizing regularized cholesky score for order-based learning of bayesian networks,'' \emph{IEEE Trans. Pattern Anal. Mach. Intell.}, vol.~43, no.~10, pp. 3555--3572, 2021.
	
	\bibitem{steven1993fundamentals}
	M.~K. Steven, \emph{Fundamentals of statistical signal processing}.\hskip 1em plus 0.5em minus 0.4em\relax Englewood Cliffs, NJ: PTR Prentice-Hall, 1993, vol.~10, no. 151045.
	
	\bibitem{some2020zhao}
	C.~Zhao, Z.~Luo, and N.~Xiu, ``Some advances in theory and algorithms for sparse optimization,'' \emph{Oper. Res. Trans.}, vol.~24, no.~4, pp. 1--24, 2020.
	
	\bibitem{kim2007interior}
	S.-J. Kim, K.~Koh, M.~Lustig, S.~Boyd, and D.~Gorinevsky, ``An interior-point method for large-scale $\ell_1 $-regularized least squares,'' \emph{IEEE J. Sel. Top. Signal Process.}, vol.~1, no.~4, pp. 606--617, 2007.
	
	\bibitem{xu2012lell}
	Z.~Xu, X.~Chang, F.~Xu, and H.~Zhang, ``$\ell_{1/2}$ regularization: A thresholding representation theory and a fast solver,'' \emph{IEEE Trans. Neural Netw. Learn. Syst.}, vol.~23, no.~7, pp. 1013--1027, 2012.
	
	\bibitem{fung2011equivalence}
	G.~Fung and O.~Mangasarian, ``Equivalence of minimal $\ell_0$-and $\ell_p$-norm solutions of linear equalities, inequalities and linear programs for sufficiently small $p$,'' \emph{J. Optim. Theory Appl.}, vol. 151, pp. 1--10, 2011.
	
	\bibitem{blumensath2008iterative}
	T.~Blumensath and M.~E. Davies, ``Iterative thresholding for sparse approximations,'' \emph{J. Fourier Anal. Appl.}, vol.~14, pp. 629--654, 2008.
	
	\bibitem{he2020optimal}
	B.~He, F.~Ma, and X.~Yuan, ``Optimal proximal augmented lagrangian method and its application to full jacobian splitting for multi-block separable convex minimization problems,'' \emph{IMA J. Numer. Anal.}, vol.~40, no.~2, pp. 1188--1216, 2020.
	
	\bibitem{blumensath2009iterativecompressed}
	T.~Blumensath and M.~E. Davies, ``Iterative hard thresholding for compressed sensing,'' \emph{Appl. Comput. Harmon. Anal.}, vol.~27, no.~3, pp. 265--274, 2009.
	
	\bibitem{cheng2020active}
	W.~Cheng, Z.~Chen, and Q.~Hu, ``An active set barzilar--borwein algorithm for $\ell_0$ regularized optimization,'' \emph{J. Glob. Optim.}, vol.~76, no.~4, pp. 769--791, 2020.
	
	\bibitem{ito2013variational}
	K.~Ito and K.~Kunisch, ``A variational approach to sparsity optimization based on lagrange multiplier theory,'' \emph{Inverse Probl.}, vol.~30, no.~1, p. 015001, 2013.
	
	\bibitem{boyd2004convex}
	S.~Boyd and L.~Vandenberghe, \emph{Convex optimization}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge, CB2 8RU, UK: Cambridge university press, 2004.
	
	\bibitem{Liu2020Optimization}
	H.~Liu, J.~Hu, Y.~Li, and Z.~Wen, \emph{Optimization: Modeling, Algorithm and Theory}.\hskip 1em plus 0.5em minus 0.4em\relax Beijing, China: Higher Education Press, 2020.
	
	\bibitem{Stefano2022DA}
	S.~Marano, P.~Braca, L.~M. Millefiori, P.~Willett, and W.~D. Blair, ``When the closest targets make the difference: An analysis of data association errors,'' \emph{IEEE Open J. Signal Process.}, vol.~3, pp. 372--386, 2022.
	
	\bibitem{lou2018fast}
	Y.~Lou and M.~Yan, ``Fast l1--l2 minimization via a proximal operator,'' \emph{J. Sci. Comput.}, vol.~74, no.~2, pp. 767--785, 2018.
	
	\bibitem{hartikainen2008optimal}
	J.~Hartikainen, A.~Solin, and S.~S{\"a}rkk{\"a}, ``Optimal filtering with kalman filters and smoothers--a manual for matlab toolbox ekf/ukf,'' \emph{Biomed. Eng.}, pp. 1--57, 2008.
	
	\bibitem{schuhmacher2008consistent}
	D.~Schuhmacher, B.-T. Vo, and B.-N. Vo, ``A consistent metric for performance evaluation of multi-object filters,'' \emph{IEEE Trans. Signal Process.}, vol.~56, no.~8, pp. 3447--3457, 2008.
	
\end{thebibliography}


\end{document}


