\section{Related Work}
\label{Related_Work}
% The purpose of knowledge distillation is to transfer the "dark knowledge" of a complex teacher network to a lightweight student network, thereby enhancing the performance of the student network.
Knowledge distillation transfers the "dark knowledge" of a complex teacher network to a lightweight student network, enhancing the performance of the student network.
Depending on the type of transferred knowledge, previous knowledge distillation (KD) methods can be categorized into three main groups: based on transferring logits**Hinton, "Distilling the Knowledge in a Neural Network"**, features**Romero, "FitNets: Hints for Thin Deep Neural Subnetworks"**, and attention**Mallya, " Piggyback: Adapting a Pretrained Multitask Model to an Unseen Task by Multiplexing Gradient Information Across Tasks"**.

Many transferring features methods followed FitNet**Romero, "FitNets: Hints for Thin Deep Neural Subnetworks"** by utilizing single-stage features for knowledge distillation. 
PKT**Lee, "Partial Knowledge Transfer between Overparameterized Neural Networks"** aligned the probability distributions of the teacher and student network features by minimizing their statistical divergence. 
SimKD**Mirzadeh, "Improved Knowledge Distillation via Clustering"** decoupled the classification head from the feature extractor, enabling effective knowledge transfer by directly reusing the teacher's classifier to guide the student's feature learning. 
CRD**Tian, "Contrastive Representation Distillation"** combined contrastive learning with knowledge distillation by leveraging a memory buffer to optimize contrastive objectives.

In the feature-based distillation methods, many works proposed to utilize multi-level feature distillation.
OFD**Gou, "Object-Focus Distillation for Efficient Knowledge Transfer"** enhanced student network performance by adjusting the placement of feature distillation layers, introducing a novel activation function called Margin ReLU, and employing partial L2 distance as the feature alignment metric.
ReviewKD**Zhu, "Review-based Knowledge Distillation for Deep Neural Networks"** improved knowledge distillation by introducing a review mechanism, in which the lower-layer features of the teacher guide the higher-layer features of the student.

However, previous methods primarily focused on global feature information, without addressing the decoupling of relationships between global and local features. 
Therefore, we introduce multi-scale decoupling of feature into feature distillation to decouple different levels of knowledge.
% % Therefore, we introduce multi-scale feature decoupling into feature distillation to effectively separate and transfer knowledge at different levels.
% Therefore, we introduce multi-scale feature decoupling into feature distillation to  separate global and local features.
Furthermore, we improve the previous contrastive representation distillation method.
Although also grounded in the mutual information maximization theory, our method eliminates the reliance on a large memory buffer for updating the feature information of positive and negative samples, achieving budget-saving by relying solely on single-batch samples(see Appendix~\ref{CompressionRatio} for detailed analysis).
We have also achieved state-of-the-art performance in numerous experiments, further demonstrating the excellence of our method.