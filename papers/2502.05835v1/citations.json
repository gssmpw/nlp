[
  {
    "index": 0,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey",
        "title": "Distilling the Knowledge in a Neural Network"
      },
      {
        "key": "luo2024scale",
        "author": "Luo, Shicai Wei Chunbo Luo Yang",
        "title": "Scale Decoupled Distillation"
      },
      {
        "key": "zhao2022decoupled",
        "author": "Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun",
        "title": "Decoupled knowledge distillation"
      },
      {
        "key": "sun2024logit",
        "author": "Sun, Shangquan and Ren, Wenqi and Li, Jingzhi and Wang, Rui and Cao, Xiaochun",
        "title": "Logit standardization in knowledge distillation"
      },
      {
        "key": "jin2023multi",
        "author": "Jin, Ying and Wang, Jiaqi and Lin, Dahua",
        "title": "Multi-level logit distillation"
      },
      {
        "key": "li2023curriculum",
        "author": "Li, Zheng and Li, Xiang and Yang, Lingfeng and Zhao, Borui and Song, Renjie and Luo, Lei and Li, Jun and Yang, Jian",
        "title": "Curriculum temperature for knowledge distillation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "romero2014fitnets",
        "author": "Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua",
        "title": "Fitnets: Hints for thin deep nets"
      },
      {
        "key": "tian2019contrastive",
        "author": "Tian, Yonglong and Krishnan, Dilip and Isola, Phillip",
        "title": "Contrastive representation distillation"
      },
      {
        "key": "chen2022knowledge",
        "author": "Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun",
        "title": "Knowledge distillation with the reused teacher classifier"
      },
      {
        "key": "chen2021distilling",
        "author": "Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya",
        "title": "Distilling knowledge via knowledge review"
      },
      {
        "key": "heo2019comprehensive",
        "author": "Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young",
        "title": "A comprehensive overhaul of feature distillation"
      },
      {
        "key": "park2019relational",
        "author": "Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu",
        "title": "Relational knowledge distillation"
      },
      {
        "key": "ahn2019variational",
        "author": "Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D and Dai, Zhenwen",
        "title": "Variational information distillation for knowledge transfer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zagoruyko2016paying",
        "author": "Zagoruyko, Sergey and Komodakis, Nikos",
        "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer"
      },
      {
        "key": "guo2023class",
        "author": "Guo, Ziyao and Yan, Haonan and Li, Hui and Lin, Xiaodong",
        "title": "Class attention transfer based knowledge distillation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "romero2014fitnets",
        "author": "Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua",
        "title": "Fitnets: Hints for thin deep nets"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "passalis2020probabilistic",
        "author": "Passalis, Nikolaos and Tzelepi, Maria and Tefas, Anastasios",
        "title": "Probabilistic knowledge transfer for lightweight deep representation learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2022knowledge",
        "author": "Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun",
        "title": "Knowledge distillation with the reused teacher classifier"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "tian2019contrastive",
        "author": "Tian, Yonglong and Krishnan, Dilip and Isola, Phillip",
        "title": "Contrastive representation distillation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "heo2019comprehensive",
        "author": "Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young",
        "title": "A comprehensive overhaul of feature distillation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2021distilling",
        "author": "Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya",
        "title": "Distilling knowledge via knowledge review"
      }
    ]
  }
]