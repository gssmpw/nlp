@inproceedings{ahn2019variational,
  title={Variational information distillation for knowledge transfer},
  author={Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D and Dai, Zhenwen},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9163--9171},
  year={2019}
}

@inproceedings{chen2021distilling,
  title={Distilling knowledge via knowledge review},
  author={Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5008--5017},
  year={2021}
}

@inproceedings{chen2022knowledge,
  title={Knowledge distillation with the reused teacher classifier},
  author={Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11933--11942},
  year={2022}
}

@inproceedings{guo2023class,
  title={Class attention transfer based knowledge distillation},
  author={Guo, Ziyao and Yan, Haonan and Li, Hui and Lin, Xiaodong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11868--11877},
  year={2023}
}

@inproceedings{heo2019comprehensive,
  title={A comprehensive overhaul of feature distillation},
  author={Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1921--1930},
  year={2019}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{jin2023multi,
  title={Multi-level logit distillation},
  author={Jin, Ying and Wang, Jiaqi and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24276--24285},
  year={2023}
}

@inproceedings{li2023curriculum,
  title={Curriculum temperature for knowledge distillation},
  author={Li, Zheng and Li, Xiang and Yang, Lingfeng and Zhao, Borui and Song, Renjie and Luo, Lei and Li, Jun and Yang, Jian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={2},
  pages={1504--1512},
  year={2023}
}

@article{luo2024scale,
  title={Scale Decoupled Distillation},
  author={Luo, Shicai Wei Chunbo Luo Yang},
  journal={arXiv preprint arXiv:2403.13512},
  year={2024}
}

@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3967--3976},
  year={2019}
}

@article{passalis2020probabilistic,
  title={Probabilistic knowledge transfer for lightweight deep representation learning},
  author={Passalis, Nikolaos and Tzelepi, Maria and Tefas, Anastasios},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={32},
  number={5},
  pages={2030--2039},
  year={2020},
  publisher={IEEE}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@inproceedings{sun2024logit,
  title={Logit standardization in knowledge distillation},
  author={Sun, Shangquan and Ren, Wenqi and Li, Jingzhi and Wang, Rui and Cao, Xiaochun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15731--15740},
  year={2024}
}

@article{tian2019contrastive,
  title={Contrastive representation distillation},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal={arXiv preprint arXiv:1910.10699},
  year={2019}
}

@article{zagoruyko2016paying,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1612.03928},
  year={2016}
}

@inproceedings{zhao2022decoupled,
  title={Decoupled knowledge distillation},
  author={Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
  booktitle={Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition},
  pages={11953--11962},
  year={2022}
}

