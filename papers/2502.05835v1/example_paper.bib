@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{chen2024mecd,
  title={{MECD}: Unlocking Multi-Event Causal Discovery in Video Reasoning},
  author={Chen, Tieyuan and Liu, Huabin and He, Tianyao and Chen, Yihang and Gan, Chaofan and Ma, Xiao and Zhong, Cheng and Zhang, Yang and Wang, Yingxue and Lin, Hui and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}
@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}
@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}
@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}
@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5058--5066},
  year={2017}
}
@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}
@article{courbariaux2015binaryconnect,
  title={Binaryconnect: Training deep neural networks with binary weights during propagations},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}
@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}
@inproceedings{zhang2018shufflenet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6848--6856},
  year={2018}
}
@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
@article{zagoruyko2016paying,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1612.03928},
  year={2016}
}
@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}
@article{kim2019qkd,
  title={Qkd: Quantization-aware knowledge distillation},
  author={Kim, Jangho and Bhalgat, Yash and Lee, Jinwon and Patel, Chirag and Kwak, Nojun},
  journal={arXiv preprint arXiv:1911.12491},
  year={2019}
}
@inproceedings{park2022prune,
  title={Prune your model before distill it},
  author={Park, Jinhyuk and No, Albert},
  booktitle={European Conference on Computer Vision},
  pages={120--136},
  year={2022},
  organization={Springer}
}
#logit_distillation-----------------------------------------------------------------------------
@article{luo2024scale,
  title={Scale Decoupled Distillation},
  author={Luo, Shicai Wei Chunbo Luo Yang},
  journal={arXiv preprint arXiv:2403.13512},
  year={2024}
}
@inproceedings{zhao2022decoupled,
  title={Decoupled knowledge distillation},
  author={Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
  booktitle={Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition},
  pages={11953--11962},
  year={2022}
}
@inproceedings{sun2024logit,
  title={Logit standardization in knowledge distillation},
  author={Sun, Shangquan and Ren, Wenqi and Li, Jingzhi and Wang, Rui and Cao, Xiaochun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15731--15740},
  year={2024}
}
@inproceedings{jin2023multi,
  title={Multi-level logit distillation},
  author={Jin, Ying and Wang, Jiaqi and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24276--24285},
  year={2023}
}
@inproceedings{li2023curriculum,
  title={Curriculum temperature for knowledge distillation},
  author={Li, Zheng and Li, Xiang and Yang, Lingfeng and Zhao, Borui and Song, Renjie and Luo, Lei and Li, Jun and Yang, Jian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={2},
  pages={1504--1512},
  year={2023}
}
#-------------------------------------------------------------------------------------------------
#feature_distillation-----------------------------------------------------------------------------
@article{tian2019contrastive,
  title={Contrastive representation distillation},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal={arXiv preprint arXiv:1910.10699},
  year={2019}
}
@inproceedings{chen2022knowledge,
  title={Knowledge distillation with the reused teacher classifier},
  author={Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11933--11942},
  year={2022}
}
@inproceedings{chen2021distilling,
  title={Distilling knowledge via knowledge review},
  author={Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5008--5017},
  year={2021}
}
@inproceedings{heo2019comprehensive,
  title={A comprehensive overhaul of feature distillation},
  author={Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1921--1930},
  year={2019}
}
@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3967--3976},
  year={2019}
}
@inproceedings{ahn2019variational,
  title={Variational information distillation for knowledge transfer},
  author={Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D and Dai, Zhenwen},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9163--9171},
  year={2019}
}
@article{passalis2020probabilistic,
  title={Probabilistic knowledge transfer for lightweight deep representation learning},
  author={Passalis, Nikolaos and Tzelepi, Maria and Tefas, Anastasios},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={32},
  number={5},
  pages={2030--2039},
  year={2020},
  publisher={IEEE}
}
#-------------------------------------------------------------------------------------------------
#attention_distillation-----------------------------------------------------------------------------
@inproceedings{guo2023class,
  title={Class attention transfer based knowledge distillation},
  author={Guo, Ziyao and Yan, Haonan and Li, Hui and Lin, Xiaodong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11868--11877},
  year={2023}
}
#-------------------------------------------------------------------------------------------------
#-yuan_li-------------------------------------------------------------------------------------------
@article{zhang2022quantifying,
  title={Quantifying the knowledge in a DNN to explain knowledge distillation for classification},
  author={Zhang, Quanshi and Cheng, Xu and Chen, Yilan and Rao, Zhefan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={4},
  pages={5099--5113},
  year={2022},
  publisher={IEEE}
}
@article{nagarajan2023student,
  title={On student-teacher deviations in distillation: does it pay to disobey?},
  author={Nagarajan, Vaishnavh and Menon, Aditya K and Bhojanapalli, Srinadh and Mobahi, Hossein and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={5961--6000},
  year={2023}
}
#-------------------------------------------------------------------------------------------------
#dataset------------------------------------------------------------------------------------------
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}
#-------------------------------------------------------------------------------------------------
#network_architectures------------------------------------------------------------------------------------------
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}
@inproceedings{ma2018shufflenet,
  title={Shufflenet v2: Practical guidelines for efficient cnn architecture design},
  author={Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={116--131},
  year={2018}
}
#-------------------------------------------------------------------------------------------------