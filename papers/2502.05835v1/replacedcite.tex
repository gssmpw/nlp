\section{Related Work}
\label{Related_Work}
% The purpose of knowledge distillation is to transfer the "dark knowledge" of a complex teacher network to a lightweight student network, thereby enhancing the performance of the student network.
Knowledge distillation transfers the "dark knowledge" of a complex teacher network to a lightweight student network, enhancing the performance of the student network.
Depending on the type of transferred knowledge, previous knowledge distillation (KD) methods can be categorized into three main groups: based on transferring logits____, features____, and attention____.

Many transferring features methods followed FitNet____ by utilizing single-stage features for knowledge distillation. 
PKT____ aligned the probability distributions of the teacher and student network features by minimizing their statistical divergence. 
SimKD____ decoupled the classification head from the feature extractor, enabling effective knowledge transfer by directly reusing the teacher's classifier to guide the student's feature learning. 
CRD____ combined contrastive learning with knowledge distillation by leveraging a memory buffer to optimize contrastive objectives.

In the feature-based distillation methods, many works proposed to utilize multi-level feature distillation.
OFD____ enhanced student network performance by adjusting the placement of feature distillation layers, introducing a novel activation function called Margin ReLU, and employing partial L2 distance as the feature alignment metric.
ReviewKD____ improved knowledge distillation by introducing a review mechanism, in which the lower-layer features of the teacher guide the higher-layer features of the student.

However, previous methods primarily focused on global feature information, without addressing the decoupling of relationships between global and local features. 
Therefore, we introduce multi-scale decoupling of feature into feature distillation to decouple different levels of knowledge.
% % Therefore, we introduce multi-scale feature decoupling into feature distillation to effectively separate and transfer knowledge at different levels.
% Therefore, we introduce multi-scale feature decoupling into feature distillation to  separate global and local features.
Furthermore, we improve the previous contrastive representation distillation method.
Although also grounded in the mutual information maximization theory, our method eliminates the reliance on a large memory buffer for updating the feature information of positive and negative samples, achieving budget-saving by relying solely on single-batch samples(see Appendix~\ref{CompressionRatio} for detailed analysis).
We have also achieved state-of-the-art performance in numerous experiments, further demonstrating the excellence of our method.