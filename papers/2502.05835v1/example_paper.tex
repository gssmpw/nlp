%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{xcolor}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025} % 提交前把这个解注释

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025} % 提交前把这个注释

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% 自己加的
\usepackage{multirow}
\usepackage{tabularx}
% \usepackage{graphicx}

% 提交arxiv时候注释掉了
% % The \icmltitle you define below is probably too long as a header.
% % Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}
% \newcommand{\tychen}[1]{{\color{orange}{[tychen: #1]}}}
\twocolumn[
\icmltitle{Contrastive Representation Distillation via Multi-Scale Feature Decoupling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Cuipeng Wang}{yyy}
\icmlauthor{Tieyuan Chen}{zzz}
\icmlauthor{Haipeng Wang}{yyy}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Key Laboratory for Information Science of Electromagnetic Waves, Ministry of Education Fudan University, University of Fudan, Shanghai, China}
\icmlaffiliation{zzz}{Shanghai Jiao Tong University, Shanghai, China}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Cuipeng Wang}{cpwang23@m.fudan.edu.cn}
% \icmlcorrespondingauthor{Tieyuan Chen}{tieyuanchen@sjtu.edu.cn}
\icmlcorrespondingauthor{Haipeng Wang}{hpwang@fudan.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution


\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
% Knowledge distillation is a technique designed to improve the performance of a student network (small)  without increasing its size by transferring the knowledge from a teacher network (large).
% Previous methods have primarily focused on the transfer of global feature information, often neglecting the disentanglement of diverse types of information embedded within various parts of the features.
% In this work, we introduce multi-scale decoupling in the feature transfer process for the first time, where local features are individually processed and integrated with contrastive learning.
% Our approach eliminates the reliance on a large memory buffer, which is commonly required in prior contrastive feature distillation method, enabling performance improvements for the student network using only single-batch samples.
% Finally, extensive evaluations on CIFAR-100 and ImageNet show our method's superiority, with some student networks distilled using our approach outperforming their pre-trained teacher network.
% This further underscores the effectiveness of our method in enabling student networks to comprehensively absorb the knowledge from teacher networks.
Knowledge distillation is a technique aimed at enhancing the performance of a smaller student network without increasing its parameter size by transferring knowledge from a larger, pre-trained teacher network. 
Previous approaches have predominantly focused on distilling global feature information while overlooking the importance of disentangling the diverse types of information embedded within different regions of the feature.
In this work, we introduce multi-scale decoupling in the feature transfer process for the first time, where the decoupled local features are individually processed and integrated with contrastive learning.
Moreover, compared to previous contrastive learning-based distillation methods, our approach not only reduces computational costs but also enhances efficiency, enabling performance improvements for the student network using only single-batch samples.
Extensive evaluations on CIFAR-100 and ImageNet demonstrate our method's superiority, with some student networks distilled using our method even surpassing the performance of their pre-trained teacher networks. 
These results underscore the effectiveness of our approach in enabling student networks to thoroughly absorb knowledge from teacher networks.
\end{abstract}


\section{Introduction}
\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure1_3.png}}
\caption{\textbf{Different Level Feature Heat Map Visualization.} Features of different scales and positions focus on distinct category information. }
\label{fig:intro}
\end{center}
\vskip -0.2in
\end{figure}
\begin{figure*}[ht]
\vskip 0.2in
\centering
\centerline{\includegraphics[width=\linewidth]{fig2_0129.png}}
\vspace{-15pt}
\caption{\textbf{The architectural comparison between our method and traditional distillation methods.} (a) Traditional multi-layer feature distillation framework. For single-layer feature distillation, the process involves performing feature distillation at a single stage only. (b) Our proposed method framework, where the features at each layer are decoupled by multiple scales. Different local features focus on different class information. Additionally, we use \textbf{ABP} to enhance the focused feature information, classifying the different feature samples and outputting them into \textbf{CL} (contrastive loss).}
\label{figure2}
\vskip -0.2in
\end{figure*}
\label{Introduction}
The past few decades have witnessed remarkable achievements of neural networks in the field of computer vision. 
With the proposal of the residual network architecture ~\cite{he2016deep}, the depth of neural networks has increased significantly. 
Deeper networks with more parameters have brought improved performance; however, they also come with trade-offs.
As network depth increases, both computational and storage costs escalate accordingly, posing a significant challenge to deployment on resource-constrained devices.
To address this issue, various model compression techniques have been proposed, including model pruning~\cite{frankle2018lottery,li2016pruning,liu2018rethinking,luo2017thinet}, model quantization~\cite{jacob2018quantization,courbariaux2015binaryconnect}, lightweight network design~\cite{howard2017mobilenets,sandler2018mobilenetv2,zhang2018shufflenet}, and knowledge distillation~\cite{hinton2015distilling,zagoruyko2016paying,romero2014fitnets}.

In light of the significant potential of knowledge distillation for model compression, we focus on the application and optimization of knowledge distillation techniques in this paper.
Knowledge distillation is a specialized form of transfer learning, where the "knowledge" from a larger pre-trained network (also known as the teacher) is transferred to a smaller student network (a.k.a. the student) to enhance the performance of the latter.
% Knowledge distillation is applicable to a wide range of network architectures and can be integrated with other model compression techniques~\cite{kim2019qkd,park2022prune} to enhance network performance further, making it a highly promising area for research. 
% \\
Hinton et al.~\cite{hinton2015distilling} first propose distilling a teacher's knowledge into a student by minimizing the Kullback-Leibler (KL) divergence between their predictions.
However, logit-based KD approaches fail to fully utilize the "knowledge" embedded in the teacher network.
To address this limitation, FitNet~\cite{romero2014fitnets} introduces distillation using intermediate-layer features of the network.
AT~\cite{zagoruyko2016paying} further leverages attention maps to facilitate knowledge transfer. 
CRD~\cite{tian2019contrastive} improves feature distillation by incorporating contrastive learning for knowledge transfer.
% while ReviewKD~\cite{chen2021distilling} improves feature distillation performance through the introduction of a review mechanism, where the lower-layer features of the teacher guide the higher-layer features of the student.
Reviewing existing methods based on feature distillation, these approaches essentially achieve knowledge transfer on features by applying certain transformations on the features of each layer or through elaborate design of loss functions.
Despite yielding favorable outcomes, these methods remain suboptimal as they primarily focus on transferring global features and do not adequately address the local information within each layer's features or conduct corresponding processing.
As shown in Figure~\ref{fig:intro}, the features at different stages within the network may include information from multiple categories. Consequently, local features at different positions and scales may predominantly focus on distinct category information. 
% \tychen{In detailed, the features of shallow layers represent ..., while the features of deeper layers focus on ...}
Zhang et al.~\cite{zhang2022quantifying} have shown that the process of knowledge distillation primarily emphasizes the transfer of foreground "knowledge points," while feature information from different categories, considered as background "knowledge points," is often overlooked during the distillation process.
Consequently, when only global features are considered as a whole for distillation, the student network may fail to capture the local information in the teacher network fully. This can result in the student being unable to fully assimilate the knowledge from the teacher, leading to suboptimal performance.

To address this issue, we propose a method of feature multi-scale decoupling, we propose a method of multi-scale feature decoupling that decouples the features of the student and teacher networks across multiple scales during the feature distillation process. 
Decoupling is an effective approach for addressing scenarios where factors of varying importance are sub-optimally treated as a unified whole~\cite{chen2024mecd}. 
This method ensures that the student not only learns the global feature knowledge from the teacher network but also fully captures the local knowledge.

Additionally, we propose an improved and budget-saving feature distillation method based on contrastive learning. CRD~\cite{tian2019contrastive} requires complex sample processing and a large memory buffer to store feature representations of all samples from both the teacher and student networks, along with additional parameters.
% By introducing multi-scale feature decoupling to contrastive representation distillation, not only is the number of samples enriched, enabling us to acquire abundant feature information with merely a single batch of samples, but also by processing global and local features separately.
By introducing multi-scale feature decoupling to contrastive representation distillation, we not only enrich the number of samples, enabling us to acquire abundant feature information with merely a single batch of samples, but also process global and local features separately.
This enables features from different-category samples within the same batch to be treated as negative samples, while even distinct local parts of the same sample, belonging to different categories, are also considered negative samples.
Furthermore, the feature multi-scale decoupling process is parameter-free and significantly enriches the sample information.
Eventually, the rich and decoupled feature sample information is input into our designed contrastive loss function(\textbf{CL}), leading to enhanced performance of the student network. 
We also employ the Attention-Based Projector (\textbf{ABP}, detailed in Section~\ref{Extensions}) to enhance discriminative regions through channel-wise attention.
The overall framework is shown in Figure~\ref{figure2}

In general, we summarize our contributions as follows:
% \begin{itemize}
% \item We reveal the limitations of traditional feature distillation methods that focus solely on global feature information and highlight the resulting issues. To address these, we propose the multi-scale decoupling and processing of the features of each layer, enriching the feature information and allowing the student network to acquire more knowledge.
% \item We eliminate the reliance on a large memory buffer inherent in traditional contrastive learning-based feature distillation and propose a contrastive representation distillation approach based on multi-scale decoupling. This method not only allows the extraction of rich feature representations using only batch-level inputs, significantly reducing the use of parameters but also facilitates more effective knowledge transfer from the teacher network to the student network, improving the performance of the student network.
% \item We conduct extensive experiments on several benchmark datasets and achieve state-of-the-art results across multiple teacher-student network pairs, demonstrating the effectiveness of our method.
% \end{itemize}
\begin{itemize}
\item We reveal the limitations of traditional feature distillation methods that focus solely on global feature information and highlight the resulting issues.
\item We propose the multi-scale feature decoupling module and an efficient contrastive representation distillation approach, enriching the feature information using only batch-level inputs and allowing the student network to better acquire feature knowledge from teacher.
\item We conduct extensive experiments on several benchmark datasets and achieve state-of-the-art results across multiple teacher-student network pairs, with many students even outperforming their teachers, demonstrating the effectiveness of our method.
\end{itemize}
% ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Related Work}
\label{Related_Work}
% The purpose of knowledge distillation is to transfer the "dark knowledge" of a complex teacher network to a lightweight student network, thereby enhancing the performance of the student network.
Knowledge distillation transfers the "dark knowledge" of a complex teacher network to a lightweight student network, enhancing the performance of the student network.
Depending on the type of transferred knowledge, previous knowledge distillation (KD) methods can be categorized into three main groups: based on transferring logits~\cite{hinton2015distilling,luo2024scale,zhao2022decoupled,sun2024logit,jin2023multi,li2023curriculum}, features~\cite{romero2014fitnets,tian2019contrastive,chen2022knowledge,chen2021distilling,heo2019comprehensive,park2019relational,ahn2019variational}, and attention~\cite{zagoruyko2016paying,guo2023class}.

Many transferring features methods followed FitNet~\cite{romero2014fitnets} by utilizing single-stage features for knowledge distillation. 
PKT~\cite{passalis2020probabilistic} aligned the probability distributions of the teacher and student network features by minimizing their statistical divergence. 
SimKD~\cite{chen2022knowledge} decoupled the classification head from the feature extractor, enabling effective knowledge transfer by directly reusing the teacher's classifier to guide the student's feature learning. 
CRD~\cite{tian2019contrastive} combined contrastive learning with knowledge distillation by leveraging a memory buffer to optimize contrastive objectives.

In the feature-based distillation methods, many works proposed to utilize multi-level feature distillation.
OFD~\cite{heo2019comprehensive} enhanced student network performance by adjusting the placement of feature distillation layers, introducing a novel activation function called Margin ReLU, and employing partial L2 distance as the feature alignment metric.
ReviewKD~\cite{chen2021distilling} improved knowledge distillation by introducing a review mechanism, in which the lower-layer features of the teacher guide the higher-layer features of the student.

However, previous methods primarily focused on global feature information, without addressing the decoupling of relationships between global and local features. 
Therefore, we introduce multi-scale decoupling of feature into feature distillation to decouple different levels of knowledge.
% % Therefore, we introduce multi-scale feature decoupling into feature distillation to effectively separate and transfer knowledge at different levels.
% Therefore, we introduce multi-scale feature decoupling into feature distillation to  separate global and local features.
Furthermore, we improve the previous contrastive representation distillation method.
Although also grounded in the mutual information maximization theory, our method eliminates the reliance on a large memory buffer for updating the feature information of positive and negative samples, achieving budget-saving by relying solely on single-batch samples(see Appendix~\ref{CompressionRatio} for detailed analysis).
We have also achieved state-of-the-art performance in numerous experiments, further demonstrating the excellence of our method.
\section{Method}
In Sec.~\ref{Multi-Scale Feature Decoupling}, we describe the feasibility and implementation of multi-scale feature decoupling. Then in Sec.~\ref{sec:contras}, we combine it with a contrastive loss to derive the feature distillation loss function.
Finally, the complete training objective is introduced in Sec.~\ref{sec:loss}.
% The subsequent analysis and derivation focus solely on the penultimate layer features in the feature extractor(corresponding to single-level feature distillation) in Sec.~\ref{}.
% Finally, we apply this strategy to each stage of the neural network's feature extractor (a.k.a.  multi-level feature distillation).
\\
\textbf{Notation.} Given a batch of input images $x_{ i }$, $x_{ j }$  (where $i , j = 1 , 2 , . . . , B$), $ i = j$ indicates the same input image. We use $f^{ T }$ and $f^{ S }$ to denote the feature extractors of the teacher and student networks, respectively. 
For the penultimate layer features in the feature extractor, we do not apply any processing to the teacher network's features $ f ^ { T } ( x _ { j } ) \in R ^ { c_{ T } \times h _ { T } \times w _ { T } }$, denoted as $ T ^ { j }$.
For the student network's features $f ^ { S } ( x _ { i } ) \in R ^ { c_{S} \times h_{S} \times w_{ S } }$, we project them using ABP to match the same number of channels as the teacher network, and the resulting features are denoted as $S ^ { i }$, where $c_{S}$ and $ c_{ T }$ represent the number of feature channels for the student and teacher networks, respectively.
Similarly, $h_{ S }, w_{ S }$ and $h _ { T },  w _ { T }$ are the spatial dimensions of the student and teacher network features, respectively.
The classifier of the teacher network, denoted as $ fc ^ { T }$, maps the feature $ T ^ { i }$ to its corresponding class $C^{ i } = fc ^ { T } ( T ^ { i } )$.
% The penultimate layer features (before logits) can then be represented as $ S ^ { i } = f ^ { S } ( x _ { i } ) \in R ^ { c_{S} \times h_{S} \times w_{ S } }$ and $ T ^ { j } = f ^ { T } ( x _ { j } ) \in R ^ { c_{ T } \times h _ { T } \times w _ { T } }$, where $c_{S}$ and $ c_{ T }$ represent the number of feature channels for the student and teacher networks, respectively. Similarly, $h_{ S }, w_{ S }$ and $h _ { T },  w _ { T }$ are the spatial dimensions of the student and teacher network features, respectively. The classifier of the teacher network, denoted as $ fc ^ { T }$, maps the feature $ T ^ { i }$ to its corresponding class $C^{ i } = fc ^ { T } ( T ^ { i } )$.
\subsection{Multi-Scale Feature Decoupling}
\label{Multi-Scale Feature Decoupling}
An entire image often couples the information of multiple classes, and during the feature extraction process in neural networks, different stages, scales, and spatial locations of features (referred to as local features) may focus on category-specific information that different from the global features.
Therefore, when transmitting the features of each stage, solely transmitting the fused global features of various fine-grained details may lead to ambiguous knowledge transfer to the student network, preventing it from acquiring the knowledge of other category-specific local features within this global feature and resulting in suboptimal performance.

For the multi-scale decoupling module, the main process involves first decoupling the features of each sample from both the teacher and student networks at multiple scales.
Afterward, the decoupled feature samples are classified to facilitate the construction of the subsequent contrastive loss function. 
The specific implementation details are as follows:
The feature maps $ S ^ { i }$ and $ T ^ { j }$  obtained from the feature extractors of the student and teacher networks are subjected to pooling operations at multiple scales. As a result, each feature map produces $M$ pooled feature maps with varying pooling sizes and positions, denoted as 
$ S _ { m } ^ { i } \in R ^ { c _ { S } \times 1 \times 1 }, T _ { n } ^ { j } \in R ^ { c _ { T } \times 1 \times 1 }$, where $ m, n = 1, 2, ..., M$, and when $ m = n$, it indicates that the pooling size and pooling position of the feature maps are identical. Additionally, when $ i = j$, it signifies that the input image to both the student and teacher networks is the same.
The feature map $ T ^ { j }$ obtained from the teacher network’s $j$-th input image undergoes multi-scale feature decoupling, resulting in $M$ feature maps $T _ { n } ^ { j }$, which are then fed into the teacher network's classifier to obtain $M$ categories $C_ { n }^{ j } = fc ^ { T } ( T _ { n }^ { j } )$. 
For a batch of input images of size $B$, we obtain $M \times B$ feature samples from both the student and teacher networks.
When $C_ { m }^{ i } = C_ { n }^{ j }$, where $i = j$ and $m = n$, the corresponding student and teacher feature sample pair $(S _ { m }^ { i }, T _ { n }^ { j })$ is called a \textbf{related positive sample pair}, where the input image, pooling size, and pooling position are identical;
When $C_ { m }^{ i } = C_ { n }^{ j }$, where $i \neq j$ or $m \neq n$, it indicates that the corresponding student and teacher feature samples belong to the same category but may differ in input image, pooling size, or pooling position. This pair $(S _ { m }^ { i }, T _ { n }^ { j })$ is referred to as an \textbf{other positive sample pair};
When $C_ { m }^{ i } \neq C_ { n }^{ j }$, it indicates that the corresponding student and teacher feature samples belong to different categories, and we call this pair $(S _ { m }^ { i }, T _ { n }^ { j })$ a \textbf{negative sample pair}.

\subsection{Contrastive Loss}
\label{sec:contras}
% When designing the contrastive loss function, we abandon the reliance on the large memory buffer in CRD~\cite{tian2019contrastive} for storing all sample feature information of the teacher and student networks and other parameters. 
When designing the contrastive loss function, we eliminate the reliance on the large memory buffer used in CRD~\cite{tian2019contrastive}, which stores feature information of both teacher and student networks, as well as other parameters.
Instead, we enrich the sample data by decoupling the student and teacher features at multiple scales.
For a batch of input data with size $B$, we can obtain $B$ pairs of student-teacher network feature maps $(S^ { i }, T^ { i })$. After performing multi-scale decoupling on the features, we can obtain $N = M \times B$ pairs of student-teacher network feature maps $(S _ { m }^ { i }, T _ { m }^ { i })$.We would like to push closer the representations $S _ { m }^ { i }$ and $T _ { m }^ { i }$, while pushing apart $S _ { m }^ { i }$ and $T _ { n }^ { j }$ (where $i \neq j$ or $m \neq n$).
Specifically, we consider the joint distribution $p(S _ { m }^ { i }, T _ { n }^ { j })$ between the student network feature samples and the teacher network feature samples,  as well as the product of the marginal distributions $p(S _ { m }^ { i })p(T _ { n }^ { j })$, in order to maximize the mutual information between \textbf{related positive sample pairs}:
\begin{equation}
I ( S _ { m } ^ { i } , T _ { m } ^ { i } ) = E _ { p ( S _ { m } ^ { i } , T _ { m } ^ { i } ) } ( \log ( \frac { p ( S _ { m } ^ { i } , T _ { m } ^ { i } ) } { p ( S _ { m } ^ { i } ) p ( T _ { m } ^ { i } ) } ) )
\label{eq:q1}
\end{equation}
where $E _ { p ( S _ { m } ^ { i } , T _ { m } ^ { i } ) } (\cdot)$ indicates the expectation over relevant positive sample pairs, $p ( S _ { m } ^ { i } , T _ { m } ^ { i } )$ denotes the joint distribution of relevant positive sample pairs.

To achieve this goal, we defined a distribution $q$ and introduced a latent variable $V$, which determines whether the distribution $q$ for a given pair of student-teacher feature samples $(S _ { m }^ { i }, T _ { n }^ { j })$ is derived from the joint distribution $p ( S _ { m } ^ { i } , T _ { n } ^ { j } )$ or the product of the marginal distributions $p(S _ { m }^ { i })p(T _ { n }^ { j })$:
\begin{equation}
q ( S _ { m } ^ { i } , T _ { n } ^ { j } | V = 1 ) = p ( S _ { m } ^ { i } , T _ { n } ^ { j } )
\label{eq:q2}
\end{equation}
\begin{equation}
q ( S _ { m } ^ { i } , T _ { n } ^ { j } | V = 0 ) = p ( S _ { m } ^ { i } ) p ( T _ { n } ^ { j } )
\label{eq:q3}
\end{equation}
When $i = j$ and $m = n$, the student-teacher feature pair is considered as a \textbf{related positive sample pair}. 
These features originate from the same input image, with identical pooling size and pooling position, making them highly correlated.
Thus, we set $V = 1$.
For the \textbf{other positive sample pairs} introduced in the previous section~\ref{Multi-Scale Feature Decoupling}, although these samples exhibit certain similarities, they have been effectively decoupled, making them weakly correlated.
% although these samples exhibit certain similarities, they are effectively decoupled from the target samples and are thus weakly correlated.
Consequently, we set $V = 0$, and these pairs are filtered out in subsequent operations.
% Consequently, we use a discriminator to filter out these pairs.
For the \textbf{negative sample pairs}, these feature pairs focus on entirely different categories of information. 
Therefore, we regard them as independent and set $V = 0$.
The prior probability of the latent variable $T$ can be easily derived as:
% $$ q ( T = 1 ) = \frac { 1 } { N } , q ( T = 0 ) = \frac { N - 1 } { N }$$
\begin{equation}
q ( V = 1 ) = \frac { 1 } { N } , q ( V = 0 ) = \frac { N - 1 } { N }
\label{eq:q4}
\end{equation}
According to Bayes' theorem, the posterior formula for the variable $V$ can be written as:
\begin{align}
&q ( V = 1 | S_{m}^{i}, T_{n}^{j} )\nonumber\\
&= \frac { q ( S _ { m } ^ { i } , T _ { n } ^ { j } | V = 1 ) q ( V = 1 )} { q ( S _ { m } ^ { i } , T _ { n } ^ { j } | V = 1 ) q ( V = 1 ) + q ( S _ { m } ^ { i } , T _ { n } ^ { j } | V = 0 ) q ( V = 0 )}\nonumber\\
&= \frac {p ( S _ { m } ^ { i } , T _ { n } ^ { j } )} {p ( S _ { m } ^ { i } , T _ { n } ^ { j } ) + (N-1)p ( S _ { m } ^ { i } ) p ( T _ { n } ^ { j } )}
\label{eq:q5}    
\end{align}
Taking the negative logarithm of both sides of equation~\ref{eq:q5}, and then applying the monotonicity of the logarithm function, we get:
\begin{align}
&- \log q ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } )\nonumber\\
& = \log ( 1 + \frac { ( N - 1 ) p ( S _ { m } ^ { i } ) p ( T _ { n } ^ { j } ) } { p ( S _ { m } ^ { i } , T _ { n } ^ { j } ) } )\nonumber\\
& \geq \log ( N - 1 ) - \log ( \frac { p ( S _ { m } ^ { i } , T _ { n } ^ { j } ) } { p ( S _ { m } ^ { i } ) p ( T _ { n } ^ { j } ) } )
\label{eq:q6}    
\end{align}
To maximize the mutual information between \textbf{related positive sample pairs}, we take the expectation on both sides of equation~\ref{eq:q6} w.r.t. $q ( S _ { m } ^ { i } , T _ { n } ^ { j } | V = 1 )$ (which is equivalent to $p ( S _ { m } ^ { i } , T _ { n } ^ { j } )$, where $i = j$ and $m = n$)
After rearranging, we obtain the form of equation~\ref{eq:q1}:
\begin{align}
&I ( S _ { m } ^ { i } , T _ { m } ^ { i } )\nonumber\\
&\geq \log ( N - 1 ) + E _ { p ( S _ { m } ^ { i } , T _ { m } ^ { i } )} (\log q ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } ))
\label{eq:q7}
\end{align}
After removing the constant term on the right-hand side of equation~\ref{eq:q7}, we obtain the lower bound of mutual information between \textbf{related positive sample pairs}, denoted as $MI\ bound$.
% between the student and teacher features for \textbf{related positive sample pairs}:
\begin{equation}
 MI\ bound = E _ { p ( S _ { m } ^ { i } , T _ { m } ^ { i } )} (\log q ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } ))
\label{eq:q8}
\end{equation}
Maximizing the lower bound $MI\ bound$ is equivalent to maximizing $I ( S _ { m } ^ { i } , T _ { m } ^ { i } )$, but since we cannot obtain the true distribution $q ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } )$, we construct an approximate distribution $ \tilde { q } ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } )$, the formula is as follows: 
% When we maximize the lower bound $MI\ bound$, it is equivalent to maximizing $I ( S _ { m } ^ { i } , T _ { m } ^ { i } )$, but we are unable to obtain the true distribution $q ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } )$. Therefore, we construct a distribution $ \tilde { q } ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } )$, and the specific formula is as follows:
\begin{equation}
\tilde { q } ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } ) = \frac { e x p ( sim ( S _ { m } ^ { i } , T _ { m } ^ { i } ) ) } { \sum _ { j = 1 } ^ { B } \sum _ { n = 1 } ^ { M } D ( S _ { m } ^ { i } , T _ { n } ^ { j } ) }
\label{eq:q9}
\end{equation}
where $sim(\cdot)$ is the cosine similarity function between two feature samples, and $D ( S _ { m } ^ { i }, T _ { n } ^ { j } )$ is a discriminator that determines whether the sample pair belongs to the \textbf{other positive sample pair}:
\begin{equation}
D ( S _ { m } ^ { i } , T _ { n } ^ { j } ) = 
\begin{cases}
0 , C _ { m } ^ { i } = C _ { n } ^ { j }, where\; i \neq j\;or\;m \neq n\\
exp ( sim ( S _ { m } ^ { i } , T _ { m } ^ { j } ) ) , o t h e r w i s e
\end{cases}
\label{eq:q10}
\end{equation}
Therefore, when maximizing Equation~\ref{eq:q11}, it is equivalent to maximizing the mutual information between \textbf{related positive sample pairs}:
\begin{equation}
\max E _ { p ( S _ { m } ^ { i } , T _ { m } ^ { i } ) } ( \log \tilde { q } ( V = 1 | S _ { m } ^ { i } , T _ { n } ^ { j } ) )
\label{eq:q11}
\end{equation}
% Given that the student network lacks the robust performance of the teacher network, we aim to make the feature directions of the student and teacher feature samples as similar as possible. 
Due to the capacity gap, a lightweight student struggles to match the feature magnitude of a cumbersome teacher. Thus, we align the student's feature distribution with the teacher's as similar as possible.
Therefore, before the feature samples are input into the loss function for calculation, we first perform L2 normalization. The loss function for single-layer feature distillation is as follows:
\begin{equation}
\begin{split}
\small
&L _ { singel\_kd}= \\
&- \frac { 1 } { B M } \sum _ { i = 1 } ^ { B } \sum _ { m = 1 } ^ { M } \frac { e x p ( s i m ( N o r m ( S _ { m } ^ { i } ) , N o r m ( T _ { m } ^ { i } ) ) ) } { \sum _ { j = 1 } ^ { B } \sum _ { n = 1 } ^ { M } D ( N o r m ( S _ { m } ^ { i } ) , N o r m ( T _ { n } ^ { j } ) ) } 
\label{eq:q12}
\end{split}
\end{equation}
where $B$ represents the batch size, $M$ denotes the number of distinct pooled feature maps obtained through multi-scale decoupling of a single feature sample, $Norm(\cdot)$ applies L2 normalization, while $sim(\cdot)$ computes the cosine similarity between two feature samples. $D(\cdot, \cdot)$ as defined in Equation~\ref{eq:q10}, serves as a discriminator to determine whether a student-teacher feature sample pair should be classified as \textbf{other positive sample pair}.
% refers to the discriminator defined in equation~\ref{eq:q10}, in which is used to determine whether the student-teacher feature sample pair is classified as an \textbf{other positive sample pair}.
% is the number of different pooled feature maps obtained after performing multi-scale decoupling on a single feature sample, 
\subsection{Training Objective}
\label{sec:loss}
In the process of feature extraction by neural networks, the focus of feature information varies across stages: shallow features focus on gradient information, middle-layer features capture local information of the target region, and deep-layer features emphasize the global information of the target region.
Through experimental studies, we find that \textbf{different student models have distinct requirements for information at various layers of the teacher network}(Details are in Section~\ref{Extensions}).
Therefore, when only a single layer is utilized for knowledge distillation, the student network may be unable to effectively learn the knowledge from the teacher network, leading to suboptimal performance.
Some student networks may only require knowledge from a single layer of the teacher network to learn effectively, while others may need multiple layers to achieve optimal performance.

In the design of the feature distillation loss function, we carefully consider and decide to apply multi-scale feature decoupling separately to features at each stage, followed by the computation of the contrastive loss as described in Equation~\ref{eq:q12}. Then, the losses from each individual stage are aggregated:
% In the design of the feature distillation loss function, we ultimately adopted a method where features at each stage are decoupled at multiple scales, followed by the computation of the contrastive loss as described in Equation ~\ref{eq:q12}. Finally, the losses from each individual stage are aggregated:
\begin{equation}
L _ { k d } = \sum _ { L a y e r } L _ { s i n g l e\_kd }
\label{eq:q13}
\end{equation}
where $Layer$ refers to all layers from the first stage to the penultimate stage in the feature extraction process. 
Finally, the distillation loss is combined with the cross-entropy loss $L_{ce}$ from hard labels to obtain the final loss function.
\begin{equation}
L = L_{ce} + \beta L_{kd}  
\end{equation}
where $\beta$ indicates the hyperparameter, for the trade-off of $L_{ce}$ and $L_{kd}$.
% -------------------------------------------------------------------------------------
\begin{table*}[ht]
\vskip 0.15in
\renewcommand{\arraystretch}{1.4}  % 临时增加行高
\centering
\caption{\textbf{The Top-1 Accuracy(\%) of different knowledge distillation methods on the validation set of CIFAR-100.} The teacher and student have identical architectures but different configurations. The best and second best results are emphasized in \textbf{bold} and \underline{underlined} cases. We mark the results where our method outperforms the teacher network with \textbf{*}.}
\scalebox{0.92}{
\begin{tabular}{ccccccccc}
\hline\multirow{4}{*}{Type}                   & Teacher & ResNet110 & ResNet110 &ResNet56 & ResNet32$\times$4 & WRN-40-2 & WRN-40-2 & VGG13 \\ 
                                        & Acc     & 74.31     & 74.31     &72.34    & 79.42            & 75.61   
& 75.61    &74.64\\\cline{2-9}  
                                        & Student & ResNet32  & ResNet20  &ResNet20 & ResNet8$\times$4  & WRN-16-2 
& WRN-40-1 & VGG8                                      \\
                                        & Acc     & 71.14     & 69.06     &69.06   & 72.50             & 73.26    
& 71.98    &70.36                                    \\\hline
\multirow{3}{*}{Logit} & KD & 73.08 & 70.67 & 70.66 & 73.33 & 74.92 & 73.54 & 72.98\\
 & CTKD & 73.52 & 70.99 & 71.19 & 73.39 & 75.45 & 73.93 & 73.52 \\
 & DKD & \underline{74.11}  & 71.06 & \textbf{71.97} & 76.32 & \underline{76.24} & 74.81 & 74.68 \\ \hline
\multirow{2}{*}{Attention} & AT & 72.31 & 70.65 & 70.55 & 73.44 & 74.08 & 73.93 & 71.43 \\ 
                           & CAT-KD & 73.62 & 71.37 & 71.37 & 76.91 & 75.60 & 73.93 & 74.65 \\ \hline
\multirow{5}{*}{Feature}  & FitNet & 71.06 & 68.99 & 69.21 & 73.50 & 73.58 & 73.93 & 71.02 \\
                          & RKD & 71.82 & 69.25 & 69.25 & 71.90 & 73.35 & 73.93 & 71.48  \\
                          & SimKD & 73.92 & 71.06 & 71.06 & \textbf{78.08} & 75.53 & 73.93 & 74.89 \\
                          & OFD & 73.23 & 71.29 & 71.29 & 74.95 & 75.24 & 73.93 & 73.95 \\ 
                          & ReviewKD & 73.89 & 71.34 & \underline{71.89} & 75.63 & 76.12 & 73.93 & \underline{74.84} \\\hline
\multirow{2}{*}{Feature}  & CRD & 73.48 & \underline{71.46} & 71.16 & 75.48 & 75.45 & 73.93 & 73.94 \\
                          & \textbf{Ours} & \textbf{*74.61} &\textbf{72.05} & 71.46 & \underline{77.43} & \textbf{*76.66} & \textbf{*75.75} & \textbf{*75.65}\\
                          & $\uparrow$ & +1.13 & +0.59 & +0.36 & +1.95 & +1.21 & +1.82 & +1.71\\\hline
\end{tabular}
}
\label{table11}
\vskip -0.1in
\end{table*}
% -------------------------------------------------------------------------------------
\begin{table*}[t]
\vskip 0.15in
\centering
\renewcommand{\arraystretch}{1.4}  % 临时增加行高
\caption{\textbf{The Top-1 Accuracy(\%) of different knowledge distillation methods on the validation set of CIFAR-100.} The teacher and student have distinct architectures. The best and second best results are emphasized in \textbf{bold} and \underline{underlined} cases. We mark the results where our method outperforms the teacher network with \textbf{*}.}
\scalebox{0.835}{
\begin{tabular}{ccccccccc}
\hline
\multirow{4}{*}{Type}                   & Teacher & ResNet32$\times$4  & ResNet32$\times$4 & ResNet32$\times$4 & ResNet32$\times$4 & WRN-40-2 & VGG13 & ResNet50 \\ 
                                        & Acc     & 79.42     & 79.42     &79.42    & 79.42   & 75.61  & 74.64    &79.34\\
\cline{2-9}  
                                        & Student & ShuffleNetV1  & ShuffleNetV2  & WRN-40-2  & WRN-16-2 & ResNet8$\times$4 & MobileNetV2 & MobileNetV2                                \\
                                        & Acc     & 70.50   & 71.82   & 75.61   & 73.26  & 72.50 & 64.60   & 64.60 \\  
\hline
\multirow{3}{*}{Logit} & KD & 74.07 & 74.45 & 77.70 & 74.90 & 73.97 & 67.37 & 67.35\\
                       & CTKD & 74.48 & 75.37 & 77.66 & 75.57 & 74.61 & 68.50 & 68.67 \\
                       & DKD & 76.45 & 77.07 & 78.46 & 75.70 & 75.56 & 69.71 & 70.35 \\ 
\hline
\multirow{2}{*}{Attention} & AT & 75.55 & 72.73 & 77.43 & 73.91 & 74.11 & 59.40 & 58.58 \\
                           & CAT-KD & \underline{78.26} & \underline{78.41} & 78.59 & 76.97 & \underline{75.38} & 69.13 & \underline{71.36} \\
\hline
\multirow{5}{*}{Feature} & FitNet & 73.95 & 73.54 & 77.69 & 74.70 & 74.61 & 64.16 & 63.16 \\
                         & RKD & 72.28 & 73.21 & 77.82 & 74.86 & 75.26 & 64.52 & 64.43  \\
                         & SimKD & 77.18 & 78.39 & \underline{79.29} &\underline{77.17} & 75.29 & 69.44 & 69.97 \\
                         & OFD & 75.98 & 76.82 & 79.25 & 76.17 & 74.36 & 69.48 & 69.04 \\
                         & ReviewKD & 77.45 & 77.78 & 78.96 & 76.11 & 74.34 & \underline{70.37} & 69.89 \\
\hline
\multirow{3}{*}{Feature} & CRD & 75.11 & 75.65 & 78.15 & 75.65 & 75.24 & 69.73 & 69.11 \\
                         & \textbf{Ours} & \textbf{78.65} &\textbf{78.67} & \textbf{*80.34} & \textbf{77.48} & \textbf{*76.93} & \textbf{70.47} & \textbf{71.51}\\
                         & $\uparrow$ & +3.54 & +3.02 & +2.19 & +1.83 & +1.69 & +0.74 & +2.40\\\hline
\end{tabular}
}
\label{table22}
\vskip -0.1in
\end{table*}
% -------------------------------------------------------------------------------------
\begin{table}[h]
\vskip 0.15in
\centering
\renewcommand{\arraystretch}{1.3}  % 临时增加行高
\caption{\textbf{The Top-1 and Top-5 Accuracy(\%) on the ImageNet validation set.} The best results are emphasized in \textbf{bold} cases.}
\scalebox{0.8}{
\begin{tabular}{ccccc}
\hline
Teacher/Student & \multicolumn{2}{c}{ResNet32/ResNet18} & \multicolumn{2}{c}{ResNet50/MobileNet} \\ 
\hline
Accuracy        & Top-1 & Top-5 & Top-1 & Top-5\\
\hline
Teacher        & 73.31 & 91.42 & 76.16 & 92.86\\
Student        & 69.75 & 89.07 & 68.87 & 88.76\\
\hline
KD             & 71.03 & 90.05 & 70.05 & 89.80\\
CTKD           & 71.38 & 90.27 & 71.16 & 90.11\\
DKD            & 71.70 & 90.41 & 72.05 & 91.05\\
\hline
AT             & 70.69 & 90.01 & 69.56 & 89.33\\
CAT-KD         & 71.26 & 90.45 & 72.24 & 91.13\\
\hline
SimKD          & 71.59 & 90.48 & 72.25 & 90.86\\
OFD            & 70.81 & 89.98 & 71.25 & 90.34\\
ReviewKD       & 71.61 & 90.51 & 72.56 & 91.00\\
\hline
CRD            & 71.17 & 90.13 & 71.37 & 90.41\\
\textbf{Ours}  & \textbf{71.99} & \textbf{90.52} & \textbf{73.06} & \textbf{91.15}\\
\hline
\end{tabular}
}
\vskip -0.1in
\label{table33}
\end{table}
% -------------------------------------------------------------------------------------
\begin{table}[h]
\vskip 0.15in
\centering
% \renewcommand{\arraystretch}{1.5}  % 临时增加行高
\caption{\textbf{Ablation study.} CL: our designed contrastive loss function. Multi-scale decoupling (MSD) is decomposed into two modules: multi-scale feature pooling (MSP) and sample classification (SC).}
\resizebox{0.48\textwidth}{!}{
\setlength{\tabcolsep}{8mm}
% \begin{tabular}{c{1.5cm}|c{1.5cm}c{1.5cm}|c{1.5cm}}
\begin{tabular}{c|c|c|c}
\hline
\multirow{2}{*}{CL} & \multicolumn{2}{c|}{MSD} & \multirow{2}{*}{Accuracy}\\
\cline{2-3}
                    &  MSP       &     SC     &  \\
\hline
                    &            &            &   75.48 \\
\hline                    
 \checkmark        &              &            &   76.44 \\
\hline
 \checkmark        &  \checkmark   &            &   77.10 \\ 
\hline
 \checkmark        &  \checkmark   &  \checkmark &   77.43 \\ 
\hline
\end{tabular}
}
\vskip -0.1in
\label{table44}
\end{table}
% -------------------------------------------------------------------------------------
\begin{figure*}[ht]
\vskip 0.2in
\centering
\subfigure[Vanilla KD]{
    \label{feature_visual_1}
    \includegraphics[width=0.21\linewidth]{tea_res32x4_stu_wrn40_2_kd.png}% \includegraphics[width=8cm, height=6cm]{figure2_1.pdf}
}
\subfigure[Ours]{
    \label{feature_visual_2}
    \includegraphics[width=0.21\linewidth]{tea_res32x4_stu_wrn_40_2.png}% \includegraphics[width=8cm, height=6cm]{figure2_1.pdf}width=\columnwidth width=3cm, height=3cm 
}
\subfigure[Teacher]{
    \label{feature_visual_3}
    \includegraphics[width=0.21\linewidth]{resnet32x4.png}
    }
\subfigure[Ours]{
    \label{feature_visual_4}
    \includegraphics[width=0.21\linewidth]{wrn_40_2.png}
}
\caption{\textbf{Visualization results of test images from CIFAR-100 with t-SNE.} In (a) and (b), We randomly sample 10 out of the 100 classes. The features extracted by the teacher and student models are represented in dark and light colors, respectively, and in our method, they are nearly indistinguishable. In (c) and (d), we present the visualizations of all classes for the teacher and the student trained with our method.}
\label{figure3}
\vskip -0.2in
\end{figure*}
% -------------------------------------------------------------------------------------
\begin{table}[h]
\vskip 0.15in
\centering
\caption{\textbf{Results of student networks with different projectors.} The teacher and student networks used are ResNet32$\times$4 and ResNet8$\times$4,respectively.}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{6mm}
\begin{tabular}{c|c}
\hline
Projector & Accuracy(\%)\\
\hline\
1$\times$1Conv &  76.87    \\
1$\times$1Conv-1$\times$1Conv &  77.31\\
1$\times$1Conv-3$\times$3Conv-1$\times$1Conv &  77.07\\
1$\times$1Conv-1$\times$1Conv-Attention &  77.43    \\
\hline
\end{tabular}
}
\vskip -0.1in
\label{table55}
\end{table}
% -------------------------------------------------------------------------------------
\begin{table}[h]
\vskip 0.15in
\centering
\renewcommand{\arraystretch}{1.5}  % 临时增加行高
\caption{The results of distillation using different levels of information for various student networks.} 
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2mm}
\begin{tabular}{c|cccc}
\hline
Feature layers used & ShuffleNetV2  & ResNet8x4  & WRN-40-2  & ShuffleNetV1 \\
\hline
$f3$ &   78.16 &  77.26  &  \textbf{80.81}   &  77.88 \\
\hline
$f3,f2$&  78.35  & \textbf{77.51}  &  80.57  &   78.41 \\
\hline 
$f3,f2,f1$ &   \textbf{78.67} &  77.43  &  80.34  &   \textbf{78.65}  \\
\hline                             
\end{tabular}
}
\vskip -0.1in
\label{table66}
\end{table}
% -------------------------------------------------------------------------------------
\section{Experiments}
\textbf{Datasets.} Experiments on CIFAR-100~\cite{krizhevsky2009learning} and ImageNet~\cite{russakovsky2015imagenet} are conducted.
CIFAR-100~\cite{krizhevsky2009learning} dataset is a well-established benchmark for image classification encompassing 100 categories.
It contains 50,000 training images and 10,000 validation images, with each image having a resolution of 32x32 pixels.
ImageNet~\cite{russakovsky2015imagenet} is a larger-scale image classification dataset, consisting of images from 1,000 categories. 
The training set contains 1.28 million images, while the validation set includes 50,000 images.
\\
\textbf{Implementation Details.} On CIFAR-100 dataset, we conduct experiments on various classic network architectures, including VGG~\cite{simonyan2014very}, ResNet~\cite{he2016deep}, WideResNet~\cite{zagoruyko2016wide}, MobileNet~\cite{sandler2018mobilenetv2}, and ShuffleNet~\cite{ma2018shufflenet,zhang2018shufflenet}.
We use the same training setting of ~\cite{tian2019contrastive}.
\\
Specifically, we set 240 epochs to train all models, with the learning rate decaying by a factor of 0.1 every 30 epochs after the first 150 epochs. 
we set $\beta$ to 0.8.
For each model, we set the batch size to 64. Each model is trained three times, and the average accuracy is reported.
For fairness, previous method results are either taken from their original papers (when the training settings match ours) or obtained using the authors' released code with our training setting.
\\
On ImageNet, we use the standard training procedure with a total of 100 epochs. The learning rate is decayed every 30 epochs, with an initial learning rate of 0.1, and the batch size for each training batch is set to 256.
we also set $\beta$ to 0.8.
\subsection{Main Results}
\noindent\textbf{Results on CIFAR-100.} We divide the previous methods into three groups as described in Section~\ref{Related_Work}.
In the logits-based distillation methods, we use the classic KD method as well as the more advanced CTKD and DKD methods.
In the attention-based distillation methods, we use AT and CAT-KD.
In the feature-based distillation methods, we use several popular and cutting-edge methods.
\\
Table~\ref{table11} summarizes results on CIFAR-100 with the teacher and student having identical architectures but different configurations.Table~\ref{table22} shows the results where the teacher and student have distinct architectures.
% Our method clearly achieves state-of-the-art results in most cases, significantly outperforming the previous contrastive learning-based feature distillation method, CRD.
% It is clearly shown that our method achieves state-of-the-art results in the vast majority of cases, significantly outperforming the previous contrastive learning-based feature distillation method CRD.
% It is worth noting that our method significantly outperforms the previous contrastive learning-based feature distillation method CRD.
It is clearly shown that our method achieves state-of-the-art results in the vast majority of cases, with all results outperforming the previous contrastive learning-based feature distillation method CRD significantly.
Moreover, our method enables many student networks to outperform the teachers. The results above demonstrate the superiority of our method to fully learn the knowledge from the teacher network.
\\
\noindent\textbf{Results on ImageNet.} Table~\ref{table33} compares the differences between different methods in terms of top-1 and top-5 accuracy.
We experiment with two settings of distillation from ResNet50 to MobileNet, and from ResNet34 to ResNet18 respectively.
The results indicate that our method outperforms all other methods, whether in the architecture of the same style
or different architecture.
This further demonstrates the robustness of our method's performance across various teacher-student network pairs in large-scale datasets.
\\
\textbf{More Analysis.} As shown in Table~\ref{table11} and Table~\ref{table22}, many student networks outperform their respective teacher networks after applying our distillation method. 
This finding contradicts the previous empirical belief that the student network should merely mimic the distribution of teacher networks as closely as possible.
Some studies, such as~\cite{nagarajan2023student}, indicate that student networks learn a systematic bias from the teacher network's distribution. 
This systematic bias, induced by the regularization effect of knowledge distillation, amplifies high-confidence predictions while suppressing low-confidence ones, which can lead to student networks outperforming teacher networks.
\subsection{Extension Experiments}
\label{Extensions}
\textbf{Ablation Study.} We conduct ablation experiments by incrementally adding components to measure their individual effects. The results are shown in Table~\ref{table44}, we conduct each experiment three times and report the average accuracy. We use ResNet8$\times$4 as the student network and ResNet32$\times$4 as the teacher network, with CRD serving as our baseline.

First, we introduce our designed contrastive loss function (denoted as CL), which operates using only single-batch samples, and observe a performance improvement over the baseline. To further highlight the superiority of our proposed multi-scale decoupling mechanism, we divide it into two modules: multi-scale feature pooling (MSP) and sample classification (SC). When multi-scale feature pooling (MSP) is introduced, as shown in the third line, the student network achieves better performance. Combining MSP with sample classification (SC) further improves the results, achieving the best performance.

\textbf{Feature visualizations.} As shown in Figure~\ref{feature_visual_2}, the features extracted by the teacher model (dark colors) and the student model distilled using our method (light colors) form tight clusters within the same class and are clearly separated across different classes, while closely aligning with the teacher network's features.

Figure~\ref{feature_visual_3} and Figure~\ref{feature_visual_4} visualize the feature maps of all classes from the pre-trained teacher network and the student network trained with our method, respectively. It is evident that our approach achieves performance comparable to the teacher network.(in fact, the student network trained with our method outperforms the teacher network in actual results) This demonstrates that our method effectively enables the student model to fully learn the knowledge of the teacher network.

\textbf{Selection of the Projector.} In the selection of the projector, we align with the objective of feature distillation, which is to maximally transfer the knowledge from regions of interest to the student network.
Therefore, attention maps are used to enhance the features in the regions of interest, facilitating better knowledge transfer.
To demonstrate the superiority of our projector, we conducted experimental comparisons using various projectors under the same settings. 
The results, as shown in Table~\ref{table44}, highlight the superior performance of our attention-based projector (ABP).

\textbf{Different student models have distinct requirements for information at various layers.}
During feature extraction, different stages focus on distinct information. 
Generally, deeper features capture the global characteristics of the target, while shallower features emphasize more localized feature information, with the shallowest layers focusing primarily on gradient  details.
This can be observed in Figure~\ref{fig:intro}.

We conduct the following experiments to further validate our finding.
For fairness, the teacher network is consistently set as ResNet32x4, and features are incrementally accumulated from the penultimate layer of the feature extractor (denoted as $f3$) upwards, adding $f2$ and $f1$ for comparative analysis.
% For fairness, the teacher network is consistently set as ResNet32x4, and features are incrementally accumulated from the penultimate layer in the feature extractor(denoted as $f3$) upwards ($f2$ and $f1$) for comparative analysis .
Using student networks with different architectures for validation, the results demonstrate that for some student networks, utilizing only the penultimate layer already achieves optimal performance, outperforming multi-layer feature strategies.
However, for other student networks, incorporating more layers of features leads to progressively better performance.
This result further confirms our finding.
\section{Conclusion and Future Work}
In this paper, we propose a framework that overcomes the limitation of prior feature distillation methods, which focus solely on global features.
A novel approach is proposed that decouples features at multiple scales and integrates them with a newly designed contrastive representation distillation method. 
Experiments conducted on ImageNet and CIFAR100 indicate that our framework is both efficient and effective, reaching SOTA results under most settings.

For future research, the interrelations among features at different stages could be decoupled based on the findings in this work to further enhance distillation performance.

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Compression Ratio.}
\label{CompressionRatio}
Generally, with other settings remaining the same, fewer training parameters require less computational resources and result in shorter training time.
In contrastive learning, achieving performance improvement requires as many negative samples as possible. 
To accomplish this, CRD introduces a massive memory buffer that stores feature samples from both the student and teacher networks, along with other parameters. 
However, this also has a notable drawback: it significantly increases both training time and computational resources.
\\
Our method eliminates the need for a memory buffer and enriches samples within a single batch. 
This parameter-free process significantly reduces both computational resource dependency and training time requirements.
Since CRD only distills features from a single layer, we adopt the same strategy by using the loss function from Equation~\ref{eq:q12}.
As shown in Table~\ref{table77}, our method significantly reduces the extra parameters compared to CRD(0.32\%-12.91\%), and the results from each experiment also outperform CRD.
Moreover, our method significantly reduces training time. For instance, under the same hardware configuration, it shortens the training time by approximately one hour for the ResNet32$\times$4-ResNet8$\times$4 teacher-student network pair.
\begin{table}[h]
\vskip 0.15in
\centering
\renewcommand{\arraystretch}{1.2}  % 临时增加行高
\caption{The extra training parameters and accuracy rates required by CRD and our single-level distillation method, and the final compression ratio are composed of the ratio of the two methods.} 
% \scalebox{1}{
% % \setlength{\tabcolsep}{10pt}  % 调整列间距以保持清晰的字体
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{4mm}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Distillation Mechanism} & Teacher & ResNet110 & ResNet32$\times$4 & ResNet32$\times$4 & ResNet32$\times$4 & ResNet32$\times$4 \\
\cline{2-7}
                                        & Student & ResNet32  & ResNet8$\times$4  & WRN-40-2  & ShuffleNetV1 & ShuffleNetV2\\
\hline
\multirow{2}{*}{CRD}                    & Acc     &  73.48    &  75.51    &    78.15   &    75.34   &  75.65 \\
\cline{2-7}
                                        & Params  & 12816645  & 12865797 &   12849413 & 12955909 & 12964101 \\
\hline                                       
\multirow{2}{*}{Ours\_single}            & Acc     &  74.42(\textuparrow 1.94)     &  77.22(\textuparrow 1.71)    & 80.81(\textuparrow 2.66)     & 77.88(\textuparrow 2.54)  & 78.16(\textuparrow 2.51)   \\
\cline{2-7}
                                        & Params  & 41281 & 656641 &     312321 & 1672961 & 1286049\\
\hline
\multicolumn{1}{c}{Compression Ratio}  &    &  0.32\%  & 5.10\%  & 2.43\%  &  12.91\% & 9.92\%  \\                                  
\hline
\end{tabular}
}
\vskip -0.1in
\label{table77}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
