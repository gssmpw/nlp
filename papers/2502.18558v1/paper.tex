% mnras_template.tex 
%
% LaTeX template for creating an MNRAS paper
%
% v3.3 released April 2024
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.3 April 2024
%   Updated \pubyear to print the current year automatically
% v3.2 July 2023
%	Updated guidance on use of amssymb package
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}

% Allow "Thomas van Noord" and "Simon de Laguarde" and alike to be sorted by "N" and "L" etc. in the bibliography.
% Write the name in the bibliography as "\VAN{Noord}{Van}{van} Noord, Thomas"
\DeclareRobustCommand{\VAN}[3]{#2}
\let\VANthebibliography\thebibliography
\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Avoid using amssymb if newtxmath is enabled, as these packages can cause conflicts. newtxmatch covers the same math symbols while producing a consistent Times New Roman font. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{bm}
\usepackage{float}
\usepackage{stfloats}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[Transfer Learning for Transients]{Transfer Learning for Transient Classification: From Simulations to Real Data and ZTF to LSST}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[R. Gupta et al.]{
Rithwik Gupta,$^{1,2}$
Daniel Muthukrishna$^{1}$\thanks{E-mail: \href{mailto:danmuth@mit.edu}{danmuth@mit.edu}}
\\
% List of institutions
$^{1}$Kavli Institute for Astrophysics and Space Research, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\\
$^{2}$Irvington High School, 41800 Blacow Rd, Fremont, CA 94538, USA\\
}
% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Prints the current year, for the copyright statements etc. To achieve a fixed year, replace the expression with a number. 
\pubyear{\the\year{}}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
Machine learning has become essential for automated classification of astronomical transients, but current approaches face significant limitations: classifiers trained on simulations struggle with real data, models developed for one survey cannot be easily applied to another, and new surveys require prohibitively large amounts of labelled training data. These challenges are particularly pressing as we approach the era of the Vera Rubin Observatory's Legacy Survey of Space and Time (LSST), where existing classification models will need to be retrained using LSST observations. We demonstrate that transfer learning can overcome these challenges by repurposing existing models trained on either simulations or data from other surveys. Starting with a model trained on simulated Zwicky Transient Facility (ZTF) light curves, we show that transfer learning reduces the amount of labelled real ZTF transients needed by 75\% while maintaining equivalent performance to models trained from scratch. Similarly, when adapting ZTF models for LSST simulations, transfer learning achieves 95\% of the baseline performance while requiring only 30\% of the training data. These findings have significant implications for the early operations of LSST, suggesting that reliable automated classification will be possible soon after the survey begins, rather than waiting months or years to accumulate sufficient training data. 
% The code used in this work is \href{https://github.com/Rithwik-G/transient-transfer-learning}{publicly available}.
\end{abstract}







\begin{keywords}
surveys -- supernovae: general -- software: machine learning -- techniques: photometric
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}

With the development of advanced survey telescopes, we are entering a new era for astronomical study. The Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to generate tens of millions of transient alerts per night, an order of magnitude larger than any survey before. This unprecedented volume of data has motivated the development of numerous light-curve classification methods \citep[e.g.][]{Charnock2016, Muthukrishna19RAPID, SupernnoovaMoller2019, Boone2019Avocado, Gomez_2020, Villar2019SuperRAENN, Villar2020SuperRAENN, Carrasco_Davis_2021, Boone_2021, Qu_2021, Gagliano2021, Pimentel2023, Gomez_2023, sheng2023neuralenginediscoveringluminous, desoto2024superphotrealtimefittingclassification, Rehemtulla_2024, Shah2025}.
% \citep[e.g.][]{Narayan2018MachineStream, SupernnoovaMoller2019, Muthukrishna19RAPID,FullLightCurveClass1, PELICANPasquet2019, Lochner2016, Charnock2016, PELICANPasquet2019,Boone2019Avacodo, Shah2025, DeSoto2024Superphot, Pimentel2023, Tarek2024,Perez-Carrasco_2023, FirstImpressionsGagliano2023,FLEETGomez, SuperRAENN,Carrasco-Davis2020,SCONEQu2021,ParSNIPBoone2021,NEEDLE2023,BTSBOTRehemtulla2024}. 

Past studies have been limited by three key constraints. First, classification pipelines are often trained and validated on simulated data, but struggle to generalize to real observations due to subtle differences between simulated and observed light curves \citep[e.g.][]{Muthukrishna19RAPID}. Second, models developed for specific surveys typically require complete retraining when applied to data from different telescopes, despite the underlying physics of the transients remaining unchanged. Third, achieving high classification performance requires large amounts of labelled training data - a resource-intensive process requiring considerable time and expensive spectroscopic follow-up observations.

To address these limitations, we propose using transfer learning, a method that has been successfully applied across many fields in machine learning to adapt models trained for one task to similar applications \citep{transfer}. The technique works by reusing features and relationships that a model has already learned, even when the specific data distributions differ. This is particularly effective in two scenarios that directly address the challenges in astronomical classification: when labelled data in the target domain are limited or when training new models from scratch is computationally expensive. By reusing learned features and fine-tuning only specific components of the model, transfer learning can dramatically reduce both the amount of labelled data and computational resources needed to achieve high performance on new tasks.


The potential of transfer learning is particularly relevant as we prepare for LSST. It provides a solution to the gap between simulated and real data. Deep learning models require large amounts of training data, which has led to the development of sophisticated light curve simulations \citep[e.g.][]{PlasticcSim,elasticc} and numerous classifiers trained on these simulations \citep[e.g.][]{Boone2019Avocado,Hlozek2020PLAsTiCCResults}. However, simulations do not perfectly capture real observations, making training on labelled survey data necessary \citep[e.g.][]{Rehemtulla_2024}. While previous work has treated simulated and real data independently, with models often struggling to generalize between the two, transfer learning provides a framework to leverage features learned from simulations while adapting to real observations. Similarly, while most classifiers are built for specific surveys, transfer learning enables models trained on existing facilities to be adapted for new ones, significantly reducing the data requirements for achieving high performance.

In this work, we demonstrate three key benefits of transfer learning for astronomical transient classification:

\begin{enumerate}
\item Transfer learning reduces the amount of data needed to generalize from simulated data to real observations
\item Transfer learning reduces the amount of labelled data needed to generalize from one survey to another
\item Transfer learning reduces training time for classification models, which is important if computing resources are limited
\end{enumerate}

The paper is organized as follows. We introduce the datasets used for our experiments in Section \ref{sec:data}. We outline our experimental setup and transfer learning methodology in Section \ref{sec:methods}. We then examine the benefits of using transfer learning in Section \ref{sec:results}, demonstrating both the reduction in required training data and the ability to transfer between simulated and real observations. Finally, we discuss the broader implications of this work and future directions in Section \ref{sec:conclusion}.



\section{Data}
\label{sec:data}

% \subsection{Overview}


In this work, we use three distinct datasets to evaluate transfer learning for transient classification: simulated light curves that match the observing properties of the Zwicky Transient Facility (ZTF Sims), real observations from the ZTF Bright Transient Survey (BTS Data), and simulated light curves matching the expected LSST observing properties (LSST Sims). Each dataset provides different opportunities to test transfer learning between simulated and real data, as well as between different surveys. The class distribution for each dataset is summarised in Table \ref{table:datadesc}.

\begin{table*}
\begin{center}
\caption{Class distribution for the datasets used in this work.}
\label{table:ztfdistribution} 
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Class & SNIa & SNIa-91bg & SNIax & SNIb & SNIc & SNIc-BL & SNII & SNIIn & SNIIb & TDE & SLSN-I & AGN \\
\hline
ZTF Sims & 11587 & 13000 & 13000 & 5267 & 1583 & 1423 & 13000 & 13000 & 12323 & 11354 & 12880 & 10561 \\
LSST Sims & 10424 & 12272 & 13000 & \multicolumn{3}{c|}{|------- 8849 (SNIb/c) -------|} & \multicolumn{3}{c|}{|-------- 5901 (SNII) --------|}  & 6014 & 13000 & 13000 \\
BTS Data & \multicolumn{3}{c|}{|---------- 4454 (SNIa) ----------|} & \multicolumn{3}{c|}{|-------- 259 (SNIb/c) --------|} & \multicolumn{3}{c|}{|-------- 1212 (SNII) --------|} & 0 & 0 & 0 \\
\hline
\end{tabular}
\label{table:datadesc}

\end{center}
\end{table*}


\subsection{Simulated ZTF Data (ZTF Sims)}

The simulated ZTF dataset is described in \S~2 of both \citet{gupta2024} and \citet{Muthukrishna2021}, and is based on the models developed for the Photometric LSST Astronomical Time-Series Classification (PLAsTiCC) \citep{PlasticcSim}. Each transient has flux and flux error measurements in the $g$ and $r$ passbands with a median cadence of 3 days in each band\footnote{The public MSIP ZTF survey has since changed to a 2-day median cadence}. The dataset includes light curves from 12 transient classes: SNIa, SNIa-91bg, SNIax, SNIb, SNIc, SNIc-BL, SNII, SNIIn, SNIIb, TDE, SLSN-I, and AGN.


\subsection{ELAsTiCC (LSST Sims)}

The LSST simulations are drawn from the Extended LSST Astronomical Time Series Classification Challenge (ELAsTiCC) dataset \citep{elasticc}, which builds upon the original PLAsTiCC simulations with improved models and more realistic survey properties. Light curves are simulated in all six LSST passbands ($ugrizy$) with the expected LSST cadence and depth. We use a subset of classes that overlap with our ZTF simulations: SNIa, SNIax, SNIa-91bg, SNII, SNIb/c, and TDE. Note that some classes are grouped differently than in the ZTF simulations (e.g., SNIb and SNIc are combined).


\subsection{Bright Transient Survey Data (BTS Data)}

For real observations, we use data from the ZTF Bright Transient Survey as described in \S~2 of \citet{Rehemtulla_2024}. The BTS systematically classifies every bright ($m < 18.5$ mag) extragalactic (and non-AGN) transient discovered by ZTF, providing a spectroscopically complete sample. Light curves are available in the ZTF $g$ and $r$ filters, and we use data from three broad transient classes: SNIa, SNII, and SNIb/c. We use these broader classifications due to the small number of transients classified in most subtypes.


\section{Methods}
\label{sec:methods}

\subsection{Classifiers}

We build a Recurrent Neural Network (RNN) classifier with Gated Recurrent Units \citep[GRU;][]{GRU} following the architecture used in \citet{Muthukrishna19RAPID} and updated in \citet{Gupta2024-icml,gupta2024}. As illustrated in Fig.~\ref{fig:architecture}, we include a parallel fully-connected (dense) neural network to process any available metadata, which in this work consists only of Milky Way extinction, and we do not use redshift or any host galaxy information. While we present results using this architecture, our transfer learning methodology can work with any neural network design.

The input vector, $\bm{X}_s$, for a transient $s$, is a $4 \times N_t$ matrix, where $N_t$ is the maximum number of timesteps for any light curve in our datasets. $N_t = 656$ in this work, however most light curves have far fewer observations. Each row $i$ of the input matrix $\bm{X}_s$ contains the following information:
\begin{equation}
    \bm{X}_{si} = [\lambda_p, t_{si}, f_{si}, \epsilon_{si}],
    \label{eq:input_vector}
\end{equation}
where $t_{si}$ is the time of the $i$th observation scaled between 0 and 1, $\lambda_p$ is the median wavelength of the passband, $f_{si}$ is the flux scaled by dividing by 500, and $\epsilon_{si}$ is the corresponding flux error with the same scaling. We select a constant scaling factor of 500 as it is close to the mean flux in our dataset and enables real-time classification by avoiding the need for future observations. Our model does not require redshift information, making it useful in the early observations of LSST.

Using this input method has two key advantages. First, it circumvents the need for interpolation between observations, which can be problematic for sparsely sampled light curves. Second, it passes all passband information through the same channel, which allows us to easily repurpose a trained model for a different survey with a different number of passbands.

To counteract class imbalances in our dataset, we use a weighted categorical cross-entropy loss to train our model. The weight of a training object $w_c$ is inversely proportional to the fraction of transients from the class $c$ in the training set,
\begin{equation}
    w_c = \frac{T}{T_c},
\end{equation}
where $T$ is the total size of the dataset and $T_c$ is the number of transients from class $c$ in the dataset. We train our models using the \texttt{adam} optimizer over $100$ epochs using \texttt{EarlyStopping} implemented in \texttt{keras}.


\subsection{Transfer Learning}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/NewArchitecture.pdf  }
    \caption{Schematic illustrating our neural network architecture and transfer learning methodology. The base model is first trained on ZTF simulations to output probabilities $[P_{b_1}, P_{b_2}, ..., P_{b_n}]$ for each of the $n$ classes in the base dataset. For transfer learning, we replace the output classification layer with a new one that outputs probabilities $[P_{t_1}, P_{t_2}, ..., P_{t_m}]$ for the $m$ classes in the target dataset. When transferring to BTS data, we freeze the base model and only train the new classification layer. When transferring to LSST, we also unfreeze the initial GRU layers to learn the additional passbands while keeping the dense layers frozen. The complexity of the additional Multi-Layer Perceptron (MLP) layers depends on the similarity between the base and target datasets - zero neurons for BTS (which is similar to ZTF sims) and two layers for LSST (which has different passbands).}
    \label{fig:architecture}
\end{figure*}

To assess the effectiveness of transfer learning, we define a base dataset and a target dataset. We first train a classifier on the base dataset, and then adapt it for the target dataset with the specific focus of analysing how much labelled data we need from the target dataset to achieve good performance. For all analyses in this work, ZTF Sims serves as the base dataset.

We are particularly interested in transfer learning from simulated to real data (ZTF to BTS) and, in preparation for the Vera Rubin Observatory, from ZTF to LSST data. Thus, we run two experiments, using either BTS Data or LSST Sims as the target dataset. In both cases, we intentionally limit the amount of labelled data available in the target dataset.

Fig.~\ref{fig:architecture} summarises our model architecture and experimental setup for transfer learning. We initially train a model to classify a transient into one of the $n$ classes in the ZTF Sims. After pretraining on the ZTF Sims, we replace the classification layer with a Multi-Layer Perceptron (MLP) followed by a new classification layer. The new classification layer varies in size depending on the number of classes, $m$, in the target dataset. The complexity of the MLP is determined by how different the target dataset is from the base dataset. We describe the details of each of these experiments in the following subsections.

\subsubsection{Fine-tuning for BTS}
When fine-tuning our model on the Bright Transient Survey, we replace the classification layer with a new one for the BTS classes and freeze all other layers of the base model. Since BTS and ZTF Sims share the same observing properties, we only retrain the new output layer and freeze the rest of the model weights. This prevents the network from modifying the features it has already learned about transients from its pretraining on the ZTF Sims. We use an empty MLP (as seen in Fig.~\ref{fig:architecture}), such that there are no intermediate layers connecting the base model to the classification head.


\subsubsection{Fine-tuning for LSST}

Transferring from ZTF to LSST presents an additional challenge: adapting from two passbands ($gr$) to six ($ugrizy$). While our input format (see eq. \ref{eq:input_vector}) enables the model to learn a coarse relationship between flux and wavelength, the model requires retraining to capture transient behaviour across the new passbands.

For LSST, we unfreeze both the initial GRU layers (blue in Fig.~\ref{fig:architecture}) and the final classification layers (yellow in Fig.~\ref{fig:architecture}). We found that only retraining the final layers, as we did for BTS, resulted in poor performance. The initial GRU layers, which process the time-series information, need to be retrained to effectively handle the more complex and sparser LSST light curves.

An experiment transferring from LSST to ZTF revealed that the choice of which layers to retrain depends on the relationship between the base and target datasets. When going from LSST to ZTF, freezing the entire base model worked best, likely because ZTF's passbands are effectively a subset of LSST's. While this direction of transfer is not necessarily useful in practice, it demonstrates the importance of matching the transfer learning approach to the specific datasets involved.



\section{Results}
\label{sec:results}

To evaluate the effectiveness of transfer learning, we compare classifiers trained with transfer learning against those trained from scratch on the target dataset. We focus on two key experiments: transferring from simulated to real data (ZTF Sims to BTS) and transferring between surveys (ZTF to LSST). In both cases, we analyse how much labelled data from the target dataset is needed to achieve good classification performance. As we vary the amount of labelled data, we use 80\% of the limited data to train the model, and 20\% to validate its performance and determine optimal training time. The remaining data is used in the test set to evaluate the model.


\subsection{Simulated to Real Data}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/ZTFtoBTSGranular.pdf}
    \caption{Classification performance as a function of training set size for models trained on BTS data. The orange points show models trained directly on BTS data, while the blue points show models first trained on ZTF Sims and then fine-tuned on BTS data. For transfer learning, we freeze the base model and retrain only the classification layer. Transfer learning achieves equivalent performance to full training while requiring only 25\% of the labelled data. The error bars show the standard deviation across 10 independent training runs. The large variance at small training set sizes is due to the limited validation data for determining optimal training duration with \textit{Early Stopping}. Training on simulations first significantly reduces the amount of data needed to create a good classifier on real data. We use the Area Under the Receiver Operating Characteristics Curve (AUROC) to measure classification performance.}
    \label{fig:ZTFtoBTS}
\end{figure}

To evaluate transfer learning from simulations to real data, we compare two approaches: fine-tuning a model pretrained on ZTF Sims and training a model directly on BTS data. For both approaches, we train with varying amounts of BTS data, sampling from the full dataset to maintain realistic class distributions. 

We evaluate the performance using the Area Under the macro-averaged Receiver Operating Characteristics Curve (AUROC). The Receiver Operating Characteristics (ROC) curve plots the False Positive Rate against the True Positive Rate over a range of probability thresholds. We compute the area under the averaged ROC Curve across all classes to quantify classification performance.

Fig.~\ref{fig:ZTFtoBTS} shows that transfer learning from simulated data significantly reduces the amount of real labelled data needed to build an effective classifier. A model pretrained on ZTF Sims requires only 25\% of the BTS data to match the performance of a model trained from scratch, and achieves 90\% of the baseline performance with just 5\% of the data. This reduction in required training data is particularly valuable given the time and expense of obtaining spectroscopic labels.

The large variance in performance at small training set sizes (see Fig.~\ref{fig:ZTFtoBTS}) is due to the limited validation data available for determining optimal training duration with \textit{Early Stopping}. However, even with this uncertainty, transfer learning consistently outperforms training from scratch when labelled data is limited.


\subsection{ZTF to LSST}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/ZTFtoLSSTGranular.pdf}
    \caption{Classification performance as a function of training set size for models trained on LSST simulations. The orange points show models trained directly on LSST Sims, while the blue points shows models first trained on ZTF Sims and then fine-tuned on LSST sims. For transfer learning, we unfreeze the initial GRU layers to handle the additional LSST passbands while keeping the dense layers frozen. Transfer learning achieves 95\% of baseline performance while requiring only 30\% of the training data. Error bars show the standard deviation across 10 independent training runs. The increased variance compared to Fig.~\ref{fig:ZTFtoBTS} reflects the additional complexity of adapting to LSST's six passbands. We use the Area Under the Receiver Operating Characteristics Curve (AUROC) to measure classification performance.}
    \label{fig:ZTFtoLSST}
\end{figure}

To evaluate transfer learning between surveys, we compare models pretrained on ZTF Sims with those trained directly on LSST simulations. Unlike the BTS experiment where we maintained real-world class distributions, here we sample an equal number of objects per class. This approach is possible with simulated data where we have abundant examples of even rare transient classes, and it enables us to evaluate performance across all classes including rare ones. 

As shown in Fig.~\ref{fig:ZTFtoLSST}, transfer learning significantly reduces the data requirements for developing LSST classifiers. A model pretrained on ZTF achieves 95\% of the baseline performance while requiring only 30\% of the training data. Despite the additional complexity of adapting to LSST's six passbands (compared to ZTF's two), the pretrained model effectively leverages its understanding of transient behaviour to reduce the amount of new training data needed. This suggests that existing classification models from current surveys like ZTF can be efficiently adapted for LSST, enabling rapid deployment of reliable classifiers early in the survey.


\subsection{Training Time and Computational Efficiency}
Another advantage of transfer learning beyond reducing data requirements is reduced training time, which is especially important in the era of data-driven astronomy. We find that models initialized with pretrained weights converge 25\% faster than those trained from scratch for two reasons. First, the model begins with an understanding of general transient behaviour, requiring fewer iterations to learn class-specific features. Second, with many layers frozen, there are fewer parameters to optimize during training.

The reduction in training time, while not critical on modern GPUs (training from scratch takes approximately 15 minutes on a Tesla V100), becomes significant when processing large datasets or when computational resources are limited. For example, when testing multiple model architectures or performing cross-validation with many training runs, the 25\% reduction in training time per model can substantially reduce total computation time.

\section{Conclusion}
\label{sec:conclusion}

Observations of the transient universe are entering a new era with the advent of wide-field surveys like the Vera Rubin Observatory. The unprecedented volume of data from LSST will require robust automated classification systems that can be deployed rapidly after the survey begins. Traditionally, developing reliable classifiers for a new survey requires accumulating substantial amounts of labelled data over months or years of operations. In this work, we demonstrate that transfer learning provides an effective solution to this challenge.

Our results show two significant advantages of transfer learning for astronomical transient classification:

\begin{enumerate}

\item Fine-tuning existing models requires 70\% to 95\% less labelled data than training from scratch. A model pretrained on ZTF simulations achieves equivalent performance on real data while requiring only 25\% of the labelled examples. Similarly, when adapting ZTF models for LSST, transfer learning maintains 95\% of the baseline performance with just 30\% of the training data. These results suggest that classifiers trained on existing surveys or simulations can be efficiently adapted to new data sources.

\item Transfer learning reduces computational requirements, with models converging 25\% faster than those trained from scratch. This improvement in training efficiency is particularly valuable when developing and testing multiple model architectures or when computational resources are limited.
\end{enumerate}

The success of transfer learning between both simulated and real data, as well as between different surveys, demonstrates that these models learn generalizable features of astronomical transients. This has important implications for future surveys like LSST, suggesting that reliable automated classification will be possible soon after observations begin, rather than waiting to accumulate large training sets.

Several promising directions remain for future work. First, combining data from multiple existing surveys could provide more robust base models that capture a wider range of transient behaviour. Second, systematic investigation of different transfer learning architectures (e.g. which layers to freeze, optimal MLP configurations) could further improve performance. Finally, applying these techniques to a broader range of astronomical phenomena could extend the benefits of transfer learning beyond transient classification.

As we prepare for the era of LSST, our results demonstrate that transfer learning will be crucial for developing the next generation of astronomical classification systems, enabling rapid scientific discovery through efficient adaptation of existing models.

\section*{Acknowledgements}
We thank Nabeel Rehemtulla and Ved Shah for useful discussions and for providing access to the Bright Transient Survey (BTS) and ELAsTiCC datasets, respectively.

This work used Bridges-2 at Pittsburgh Supercomputing Center through allocation PHY240105 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services \& Support (ACCESS) program \citep{NSF-ACCESS-Boerner2023}, which is supported by U.S. National Science Foundation grants \#2138259, \#2138286, \#2138307, \#2137603, and \#2138296.

This work made use of the \texttt{python} programming language and the following packages: \texttt{numpy} \citep{numpy}, \texttt{matplotlib} \citep{matplotlib}, \texttt{scikit-learn} \citep{scikit-learn}, \texttt{pandas} \citep{pandas}, \texttt{astropy} \citep{astropy}, \texttt{keras} \citep{keras}, and \texttt{tensorflow} \citep{tensorflow}.


\section*{Data Availability}
The code used in this work is publicly available at \url{https://github.com/Rithwik-G/transient-transfer-learning}.

The ZTF simulations were generated using models from PLAsTiCC \citep{KesslerPlasticcModels}, available at \url{https://zenodo.org/record/2612896\#.YYAz1NbMJhE}. The light curves were simulated to match ZTF observing properties using the \texttt{SNANA} software package \citep{SNANA} with ZTF survey observing logs. These simulations build upon those used in \citet{Muthukrishna19RAPID}, updated to resolve issues with core-collapse supernovae as described in \citet{Muthukrishna2021} and \citet{Gupta2024-icml,gupta2024}. The simulated data is available upon reasonable request to the corresponding author.

The ELAsTiCC simulations were provided to the authors by Ved Shah and are also publicly available from the LSST Dark Energy Science Collaboration (DESC) on the National Energy Research Scientific Computing Center (NERSC) at \url{https://portal.nersc.gov/cfs/lsst/DESC_TD_PUBLIC/ELASTICC/}. 

The ZTF Bright Transient Survey (BTS) dataset was provided to the authors by Nabeel Rehemtulla and is also publicly available from the LSST alert brokers as described at \url{https://sites.astro.caltech.edu/ztf/bts/bts.php}.




\bibliographystyle{mnras}

\bibliography{references}


\appendix



% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
