\section{Related Work}
% 之后删减
\subsection{3D Morphing}

Previous 3D morphing methods focus on shape-only correspondence~\cite{tam2012registration}, and realize 3D morphing through interpolation or deformation between corresponding 3D primitives (e.g., points, vertices, and faces). They can be divided into \textit{(a) Axiomatic Methods:} They tend to rely on sparse landmarks~\cite{kim2011blended,edelstein2019enigma} or use functional maps~\cite{ovsjanikov2012functional} to address under-constrained mapping spaces, such as MapTree~\cite{ren2020maptree} and SmoothShells~\cite{eisenberger2020smooth}. Energy-minimizing functions~\cite{sorkine2007rigid,sorkine2004laplacian} and skinning methods~\cite{fulton2019latent,jacobson2014skinning} enable deformation control, while optimal transportation~\cite{solomon2015convolutional,tsai2022multiview} approximates shape correspondence. \textit{(b) Learning-Based Methods:} This category of methods can be categorized into introducing other generative priors and using aligned data. SATR~\cite{abdelreheem2023satr} introduces the semantic labels from multi-view images. NSSM~\cite{morreale2024neural} uses Dinov2~\cite{oquab2023dinov2} for sparse landmarks and SRIF~\cite{sun2024srif} introduces large vision models~\cite{zhang2024diffmorpher} for semantic shape registration and morphing. A recent class of works leverages text prompts as user inputs for driving a deformation towards an arbitrary textual prompt~\cite{gao2023textdeformer,mohammad2022clip,michel2022text2mesh}, but CLIP~\cite{radford2021learning} objective lacks a full understanding of object details. For data prior, category-specific training~\cite{yumer2015semantic} on a topology-aligned dataset such as NeuroMorph~\cite{eisenberger2021neuromorph} is also prevailing in learning-based 3D shape analysis. 

These methods focus on shape-only morphing or rely on rigging annotations, requiring extra effort due to 2D-3D mismatches. In contrast, our work enables textured 3D morphing without learning correspondence or deformation, using a 3D diffusion model to regenerate interpolated representations, addressing prior challenges and offering a new direction for 3D morphing research.

\vspace{-0.4cm}
\subsection{Image Morphing}

Image morphing is a long-standing challenge in computer vision and graphics~\cite{aloraibi2023image, wolberg1998image, zope2017survey}. Traditional methods~\cite{beier2023feature, bhatt2011comparative, darabi2012image, liao2014automating, shechtman2010regenerative} use feature-based warping and blending to create smooth transitions but struggle to generate new content, often leading to artifacts. Data-driven methods~\cite{averbuch2016smooth, fish2020image} leverage large single-class datasets to achieve smoother results but are limited in cross-domain or personalized applications due to their reliance on specific data. The methods like DiffMorpher~\cite{zhang2024diffmorpher} and AID~\cite{he2024aid} address this by utilizing pre-trained diffusion models on diverse datasets, enabling flexible morphing across a wide range of object categories. \textit{Inputting multi-view images to image morphing methods enables 3D-aware morphing.}

\vspace{-0.4cm}
\subsection{3D Diffusion Model}

Generative 3D priors fall into two types: native 3D diffusion models and 3D-aware diffusion models. Due to the scarcity of 3D data, methods leveraging 2D priors to generate 3D or multi-view content have been proposed. For instance, Score Distillation Sampling~\cite{poole2022dreamfusion} distills 3D information from a 2D diffusion model. 3D-aware generation can be divided into two steps: multi-view image generation~\cite{shi2023mvdream} followed by feed-forward 3D reconstruction~\cite{xu2024instantmesh}. However, these methods inherently lack a 3D latent space. To address this, native 3D diffusion models~\cite{zhang20233dshape2vecset,vahdat2022lion,lan2025ln3diff} that encode and learn 3D representations have been introduced. These models typically consist of two steps: training a VAE to encode 3D data and training a diffusion model based on corresponding latent codes. We use Gaussian Anything~\cite{lan2024gaussiananything} as the 3D diffusion prior, encoding 3D information in a point cloud latent space.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{framework.pdf}
    \vspace{-0.6cm}
    \caption{The framework of our method. The 3D diffusion prior is a two-stage (geometry \& texture) generation model. Beyond basic interpolation, Attention Fusion  is explored to improve smoothness, while Token Reordering  and Low-Frequency Enhancement are proposed to improve plausibility.}
    \label{fig: framework}
    \vspace{-0.1cm}
\end{figure*}