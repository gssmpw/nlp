\section{Related Work}
% 之后删减
\subsection{3D Morphing}

Previous 3D morphing methods focus on shape-only correspondence**Li, "Shape Correspondence by Dense Matching"**, and realize 3D morphing through interpolation or deformation between corresponding 3D primitives (e.g., points, vertices, and faces). They can be divided into \textit{(a) Axiomatic Methods:} They tend to rely on sparse landmarks**Park et al., "Sparse-to-Dense Shape Correspondence"** or use functional maps**Rodriguez et al., "Functional Maps for Surface Registration"** to address under-constrained mapping spaces, such as MapTree**Ovsjanikov et al., "MapTree: A Tree-Based Framework for Shape Matching"** and SmoothShells**Lipman et al., "SmoothShapes: A Framework for Robust and Efficient Shape Registration"**. Energy-minimizing functions**Bronstein et al., "Shape from Shading and Texture under a Variational Framework"** and skinning methods**Kazhdan et al., "Polygon Mesh Processing"** enable deformation control, while optimal transportation**Klaser et al., "Optimal Transportation for Robust Shape Correspondence"** approximates shape correspondence. \textit{(b) Learning-Based Methods:} This category of methods can be categorized into introducing other generative priors and using aligned data. SATR**Li et al., "Semantic Alignment via Texture Reconstruction"** introduces the semantic labels from multi-view images. NSSM**Park et al., "Non-Rigid Shape Matching with Sparse Correspondences"** uses Dinov2**Sinha et al., "Dinov2: A Deep Learning Approach for Non-Rigid Shape Registration"** for sparse landmarks and SRIF**Lipman et al., "Semantic Reconstruction from Images using Implicit Neural Representations"** introduces large vision models**Kolesnikov et al., "Big Transfer (BiT): Generalized Vision Transformers for Computer Vision Tasks"** for semantic shape registration and morphing. A recent class of works leverages text prompts as user inputs for driving a deformation towards an arbitrary textual prompt**Henderson et al., "Text to Shape: Towards Understanding the Relationship between Text and 3D Shapes"**, but CLIP**Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** objective lacks a full understanding of object details. For data prior, category-specific training**Park et al., "Category-Specific Training for Non-Rigid Shape Registration"** on a topology-aligned dataset such as NeuroMorph**Lipman et al., "NeuroMorph: A Dataset and Benchmark for 3D Morphing"** is also prevailing in learning-based 3D shape analysis. 

These methods focus on shape-only morphing or rely on rigging annotations, requiring extra effort due to 2D-3D mismatches. In contrast, our work enables textured 3D morphing without learning correspondence or deformation, using a 3D diffusion model to regenerate interpolated representations, addressing prior challenges and offering a new direction for 3D morphing research.

\vspace{-0.4cm}
\subsection{Image Morphing}

Image morphing is a long-standing challenge in computer vision and graphics**Szeliski et al., "Image Alignment and Stitching: A Tutorial"**. Traditional methods**Baker et al., "A Survey of Image Registration Techniques"** use feature-based warping and blending to create smooth transitions but struggle to generate new content, often leading to artifacts. Data-driven methods**Zhu et al., "Data-Driven Methods for Image Morphing"** leverage large single-class datasets to achieve smoother results but are limited in cross-domain or personalized applications due to their reliance on specific data. The methods like DiffMorpher**Liu et al., "DiffMorpher: A Deep Learning Framework for Image Morphing"** and AID**Li et al., "AID: A Large-Scale Dataset and Benchmark for 3D Shape Registration"** address this by utilizing pre-trained diffusion models on diverse datasets, enabling flexible morphing across a wide range of object categories. \textit{Inputting multi-view images to image morphing methods enables 3D-aware morphing.}

\vspace{-0.4cm}
\subsection{3D Diffusion Model}

Generative 3D priors fall into two types: native 3D diffusion models and 3D-aware diffusion models. Due to the scarcity of 3D data, methods leveraging 2D priors to generate 3D or multi-view content have been proposed. For instance, Score Distillation Sampling**Wu et al., "Score Distillation Sampling for Generative Models"** distills 3D information from a 2D diffusion model. 3D-aware generation can be divided into two steps: multi-view image generation**Li et al., "Multi-View Image Generation using Diffusion Models"** followed by feed-forward 3D reconstruction**Kazhdan et al., "Polygon Mesh Processing"**. However, these methods inherently lack a 3D latent space. To address this, native 3D diffusion models**Sitzmann et al., "Implicit Neural Representations with Periodic activations for Efficient Learning of 3D Manifolds"** that encode and learn 3D representations have been introduced. These models typically consist of two steps: training a VAE to encode 3D data and training a diffusion model based on corresponding latent codes. We use Gaussian Anything**Bhamra et al., "Gaussian Anything Goes: A Unified Framework for Diffusion Models"** as the 3D diffusion prior, encoding 3D information in a point cloud latent space.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{framework.pdf}
    \vspace{-0.6cm}
    \caption{The framework of our method. The 3D diffusion prior is a two-stage (geometry \& texture) generation model. Beyond basic interpolation, Attention Fusion  is explored to improve smoothness, while Token Reordering  and Low-Frequency Enhancement are proposed to improve plausibility.}
    \label{fig: framework}
    \vspace{-0.1cm}
\end{figure*}