\section{Related Work}
% 之后删减
\subsection{3D Morphing}

Previous 3D morphing methods focus on shape-only correspondence____, and realize 3D morphing through interpolation or deformation between corresponding 3D primitives (e.g., points, vertices, and faces). They can be divided into \textit{(a) Axiomatic Methods:} They tend to rely on sparse landmarks____ or use functional maps____ to address under-constrained mapping spaces, such as MapTree____ and SmoothShells____. Energy-minimizing functions____ and skinning methods____ enable deformation control, while optimal transportation____ approximates shape correspondence. \textit{(b) Learning-Based Methods:} This category of methods can be categorized into introducing other generative priors and using aligned data. SATR____ introduces the semantic labels from multi-view images. NSSM____ uses Dinov2____ for sparse landmarks and SRIF____ introduces large vision models____ for semantic shape registration and morphing. A recent class of works leverages text prompts as user inputs for driving a deformation towards an arbitrary textual prompt____, but CLIP____ objective lacks a full understanding of object details. For data prior, category-specific training____ on a topology-aligned dataset such as NeuroMorph____ is also prevailing in learning-based 3D shape analysis. 

These methods focus on shape-only morphing or rely on rigging annotations, requiring extra effort due to 2D-3D mismatches. In contrast, our work enables textured 3D morphing without learning correspondence or deformation, using a 3D diffusion model to regenerate interpolated representations, addressing prior challenges and offering a new direction for 3D morphing research.

\vspace{-0.4cm}
\subsection{Image Morphing}

Image morphing is a long-standing challenge in computer vision and graphics____. Traditional methods____ use feature-based warping and blending to create smooth transitions but struggle to generate new content, often leading to artifacts. Data-driven methods____ leverage large single-class datasets to achieve smoother results but are limited in cross-domain or personalized applications due to their reliance on specific data. The methods like DiffMorpher____ and AID____ address this by utilizing pre-trained diffusion models on diverse datasets, enabling flexible morphing across a wide range of object categories. \textit{Inputting multi-view images to image morphing methods enables 3D-aware morphing.}

\vspace{-0.4cm}
\subsection{3D Diffusion Model}

Generative 3D priors fall into two types: native 3D diffusion models and 3D-aware diffusion models. Due to the scarcity of 3D data, methods leveraging 2D priors to generate 3D or multi-view content have been proposed. For instance, Score Distillation Sampling____ distills 3D information from a 2D diffusion model. 3D-aware generation can be divided into two steps: multi-view image generation____ followed by feed-forward 3D reconstruction____. However, these methods inherently lack a 3D latent space. To address this, native 3D diffusion models____ that encode and learn 3D representations have been introduced. These models typically consist of two steps: training a VAE to encode 3D data and training a diffusion model based on corresponding latent codes. We use Gaussian Anything____ as the 3D diffusion prior, encoding 3D information in a point cloud latent space.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{framework.pdf}
    \vspace{-0.6cm}
    \caption{The framework of our method. The 3D diffusion prior is a two-stage (geometry \& texture) generation model. Beyond basic interpolation, Attention Fusion  is explored to improve smoothness, while Token Reordering  and Low-Frequency Enhancement are proposed to improve plausibility.}
    \label{fig: framework}
    \vspace{-0.1cm}
\end{figure*}