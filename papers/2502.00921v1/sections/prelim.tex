\section{Technical preliminaries}\label{sec:prelims}

\paragraph{Probability notation.}
Given distributions $P,Q$ defined on $(\Omega,\mathcal{F})$ with a base measure $\mu$, the \emph{total variation distance} is defined as $\TV(P,Q) \triangleq \frac{1}{2}\int |dP-dQ| d\mu$. For random variables $X,Y$, we will also use $\TV(X,Y)$ as shorthand to denote the $\TV$ of the measures of $X,Y$. Let $\supp(P) = \{x \in \Omega|dP(x)>0\}$ denote the support.  We will also use the following well-known relationship.
\begin{restatable}{lemma}{ratioinequalitylemma}\label{lem:ratio_inequality}
For probability measures $P,Q$, $\mathbb{E}_{x \sim P} \left[\frac{\D Q}{\D P+\D Q} \right]\le \frac{1}{2}\sqrt{1-\TV^2(P,Q)}.$ 
\end{restatable} \vspace{-0.3em}
To study feature localization in diffusion and autoregressive models, we consider a \emph{forward-reverse} experiment. A forward-reverse experiment considers the amount of ``noise'' one would need to add to a generation so that running the generative model starting from the noised generation would still yield a sample with the same feature. For a diffusion model, this could mean taking an image of a cat, adding Gaussian noise, and resampling to see if the result is still a cat. For a language model, it could mean truncating a story about a cat and resampling to check if the story remains about a cat. Now, we will use the language of stochastic localization to place these analogous experiments for diffusion and language models within the same framework. \subsection{Stochastic localization samplers}\label{sec:stocloc}

We formally define the framework for stochastic localization samplers, following~\cite{montanari2023samplingdiffusionsstochasticlocalization}. Let $X \sim p$ be a random variable over $\mathbb{R}^d$.\footnote{These definitions are easily carried over to the setting where $X$ lives in a discrete space.} We consider a sequence of random variables $(Y_t)_{t \in \I}$ with a compact index set $\I \subset [0,\infty)\bigcup \{\infty\}$. As $t$ increases, $Y_t$ becomes \emph{less informative} and \emph{degrades} the original information about $X$ (Definition \ref{def:observation_process}). As in \cite{montanari2023samplingdiffusionsstochasticlocalization}, we will only consider \emph{complete} observation processes, where information about the path $(Y_t)_{t \in \I}$ uniquely identifies $X$: for any measurable set $A \subset \mathbb{R}^n$, we require $P(X \in A|(Y_t)_{t \in \I}) \in \{0,1\}$. For the sake of simplicity, we will assume  $0,\infty \in \I$ and $Y_\infty$ is totally uninformative about $X$. \footnote{ Note that our formulation of stochastic localization differs slightly in several minor ways. First, in that work the index set $\I$ is not necessarily compact; while we assume compactness of $\I$, this still encapsulates most applications of generative models, in which the sample $X$ is realized in finitely many steps. Secondly, our indexing of time is the reverse of that of in~\citep{montanari2023samplingdiffusionsstochasticlocalization}; in that work, the $Y_t$'s become \emph{more} informative about $X$ as $t$ increases. We make this choice purely for cosmetic reasons. 
}
\begin{definition}\label{def:observation_process}
$(Y_t)_{t \in \I}$ is an \emph{observation process} with respect to $X$ if 
for any positive integer $k$ and sequence $t_1<t_2<\dots<t_k \in \I$, the sequence $X \rightarrow Y_{t_1} \rightarrow Y_{t_2} \rightarrow \cdots \rightarrow Y_{t_k}$ forms a Markov chain.
\end{definition}


\noindent Because $X\to Y_{t_1}\to \cdots \to Y_{t_k}$ is a Markov chain, its reverse $Y_{t_k} \to \cdots \to Y_{t_1}\to X$ is also a Markov chain. To any such observation process one can thus associate a generative model as follows:

\begin{definition}
    Given observation process $(Y_t)_{t\in \I}$ and times $t_1 < \cdots < t_m = \infty$ in $\I$, the associated \emph{stochastic localization sampler} is the algorithm that generates a sample for $X$ by first sampling $Y_{t_m}$ and then, for $k = m-1,m-2,\ldots,0$, sampling from the posterior on $Y_{t_k}$ conditioned on $Y_{t_{k+1}}$ by taking one step in the reverse Markov chain above, and finally sampling $X$ conditioned on $Y_{t_0}$.
\end{definition}

In Appendix~\ref{app:ex_stocloc}, we formally verify that diffusion and autoregressive models are special cases of this framework. In practice, one does not have access to the true posteriors of the data distribution and must learn approximations to the posterior from data. This issue of learning the true distribution is orthogonal to our work, and thus we define $X \sim p$ to be the sampler's distribution. Furthermore, it is more natural to study the sampler's distribution for applications such as interpretability or jailbreaks. 

\paragraph{Features, mixtures, and sub-mixtures.} \enspace To capture the notion of a feature of the generation, we assume that the distribution $X \sim p$ is a \emph{mixture model}. Consider a discrete set $\Theta=\{\theta_1,\dots,\theta_K\}$ with non-negative weights $w_1,\dots,w_K$ summing to $1$. Each $\theta_i \in \Theta$ is associated with a probability density function $p^{\theta_i}:\mathbb{R}^n \to \mathbb{R}^{\geq 0}$. To generate a sample $X \sim p$, we first draw $\theta \sim \Cat(\Theta,\{w_i\}_{i=1}^K)$ and return $X \sim p^\theta$.  This yields an overall density of $p  \triangleq \sum_{\theta \in \Theta} w_\theta p^\theta$. For any non-empty $S\subset\Theta$, we also define the sub-mixture $p^S$ by $p^S \triangleq \sum_{\theta \in S}\frac{w_\theta}{\sum_{\phi \in S} w_\phi} p^\theta$. 
\begin{remark}
Note that the definition of $\Theta$ is extremely flexible and can be tailored to the particular data modality or task. For example, $\Theta$ could be $\{\textbf{cat}, \textbf{dog}\}$ for image diffusion models; $\{\textbf{right}, \textbf{wrong}\}$ for math and reasoning tasks; $\{\textbf{unsafe}, \textbf{safe}\}$ for jailbreaks. 
\end{remark}

\noindent Here we study a \emph{family of observation processes} corresponding to observation processes for different initial distributions of $X \sim p^S$ for $S \subset \Theta$. To ensure that we can meaningfully compare the observation processes within this family, we will assume that the \emph{degradation procedure is fixed}. To formalize this intuition, we borrow the language from diffusion models of a forward process, which degrades $X$, and a reverse process, which takes a degraded $Y_t$ and produces $X$. 




\subsection{Forward-reverse experiment}
Now we describe the general formalism under which we will study critical windows. Fixing some $t \in \I$ and $S \subset \Theta$, we start with some $X \sim p^S$, sample $Y_t|X$ from the observation process conditioning on $X$, and finally take $X'|Y_t$ from the stochastic localization sampler conditioning on $Y_t$. The can be understood as a generalization of the \emph{forward-reverse} experiment in diffusions, originally studied in \citep{sclocchi2024phasetransitiondiffusionmodels,sclocchi2024probinglatenthierarchicalstructure,li2024criticalwindowsnonasymptotictheory}, to arbitrary stochastic localization samplers. \\ 


\noindent \textbf{Forward process.} \enspace For any $t \in \I$, define the forward Markov transition kernel $\kernelforward_t(A|X)=P(Y_t \in A|X)$. Note the forward Markov transition kernel does not depend on the distribution of $X$. The fact that the forward process is agnostic to the specifics of the original distribution is shared by the most widely used stochastic localization samplers. For example, in diffusion and flow-matching models, the forward transition is a convolution of $X$ with a Gaussian; in autoregressive language models, it is masking of the last remaining token in the sequence. For any $t \in \I$ and $S \subset \Theta$, we let $p_t^S$ denote the law of $Y_t^S$, where we sample $X^S \sim p^S$ and then sample $Y_t^S \sim \kernelforward_t(\cdot |X^S)$. We omit the $\Theta$ in $p_t^\Theta$. \\

\noindent\textbf{Reverse process.} \enspace For any $t\in \I$ and initial distribution $X \sim p$, we define the posterior of $X$ given $Y_t$ by $\kernelreverse[](A|Y_{t})=P_{X \sim p}(X \in A|Y_{t})$, that is, the distribution of $X$ given by starting the sampling process at $t \in \I$ and $Y_t$ instead of $\infty$ and $Y_\infty$. We will also use this notation for the probability density. \\

\noindent Now, we are ready to describe the main forward-reverse experiment that we will study. 

\begin{definition}[Forward-reverse experiment \citep{sclocchi2024phasetransitiondiffusionmodels,sclocchi2024probinglatenthierarchicalstructure,li2024criticalwindowsnonasymptotictheory}]\label{sec:noisedenoise} For nonempty $S \subset \Theta$ and $\wh{T} \in \I$, let $\modrevlawX{S}{\wh{T}}$ be the distribution of $X^{S,\wh{T}}$ defined by the following procedure:\footnote{Note that this equips $2^{\Theta}$ with the structure of a poset, i.e. $A \subset B$ if and only if there exists some $t \in \I$ such that running the forward-reverse experiment up to 
$t$ from $p^A$ yields $p^B$.}
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt]
    \item Sample $Y_{\wh{T}}^S \sim p_{\wh{T}}^S$ \---- i.e. run the forward process for time $\wh{T}$ starting at the sub-mixture $p^S$.
    \item Sample $X^{S,\wh{T}} \sim \kernelreverse[]_{}(\cdot | Y_{\wh{T}}^S)$ \---- i.e. run the reverse process starting at $Y^S_{\wh{T}}$ to sample from the posterior on $X$.
\end{enumerate}
\end{definition}

We emphasize that in the second step, we run the reverse process with the prior on $X$ given by the \emph{entire distribution $p$} rather than the sub-mixture $p^S$. If we did the latter, the marginal distribution of the result would simply be $p^S$. Instead, the marginal distribution of $X^{S,\wh{T}}$ is some distribution whose relation to $p$ and sub-mixtures thereof is \emph{a priori} unclear. Intuitively, as $\wh{T} \to 0$, this distribution converges to $p^S$, and as $\wh{T}\to\infty$, this distribution converges to $p$. The essence of our work is to understand the transition between these two regimes as one varies $\wh{T}$. 
\subsubsection{Instantiation for LLMs}
For intuition about what these definitions actually mean, consider an autoregressive language model, which produces stories of cats or dogs. We have a discrete set of tokens $\mathcal{A}$ and $X \in \mathcal{A}^T$ representing length-$T$ sequences. Letting $\I=\{0,1,2\dots,T\}$, the \emph{observation process} is defined with $Y_i \in \mathcal{A}^{T-i}$, $Y_0 \triangleq X$, and $Y_i$ being the  first $T-i$ elements of $Y_{i-1}$ for $i =1,\dots,T$. It is easy to see that this is Markovian and the samples become less informative about the original $X$ as $t \to \infty$. In the associated \emph{stochastic localization sampler}, the posterior $Y_t|Y_{t+1}$ for adjacent $t,t+1 \in \I$ is the conditional distribution of $T-t$-length sequences given a prefix of length $T-(t+1)$, exactly the task of next-token prediction.  
Finally, we study the \emph{forward-reverse experiment} applied to a story of a cat. For LLMs, this means masking the last $\wh{T}$ tokens of a sample and then resampling with the same model. If $\wh{T}$ is small, the story will likely still mention a cat and resampling will yield a story about a cat. If $\wh{T}$ is large, then the first appearance of cat may be truncated, so resampling could produce a story about a dog as well.


