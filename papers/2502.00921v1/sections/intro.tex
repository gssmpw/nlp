\section{Introduction}\label{sec:intro}

In large language models (LLMs), undesirable behavior can often emerge very suddenly. For example,
\begin{itemize}
    \item Claude transitioned from coding to browsing pictures of Yellowstone while using a computer~\citep{anthropicyellowstone}.
    \item The Phi-4 team reported that the probability of correctly answering a math problem can plummet with a single token~\citep{abdin2024phi4technicalreport,lin2024criticaltokensmattertokenlevel}.
    \item Gemini abruptly threatened a student who was using it to study~\citep{geminijailbreak}.
    \item Llama models can be jailbroken by manipulating the first few generated tokens~\citep{qi2024safetyalignmentjusttokens,prefillingattack}.
\end{itemize} 
These abrupt shifts are not unique to autoregressive models. In diffusion models, it has been observed that certain properties like the presence of an object in the background or the image class emerge in narrow time intervals, sometimes called \emph{critical windows}, of the generation process~\citep{ho2020denoising,meng2022sdedit,9879163,raya2023spontaneous,georgiev2023journey,sclocchi2024probinglatenthierarchicalstructure,sclocchi2024phasetransitiondiffusionmodels,biroli2024dynamical,li2024criticalwindowsnonasymptotictheory}. 
\begin{figure}[H]
    \centering
        \includegraphics[width=0.7\linewidth]{figures/schematic.png}
    \caption{Examples of critical windows across different data modalities and samplers, including image diffusion models, discrete diffusion models, and chain of thought for reasoning and jailbreaks in autoregressive language models.}
    \label{fig:schematic-diagram}
\end{figure}
Critical windows, broadly characterizable as \emph{a few steps of the sampling procedure} during which features of the final output appear, arise in many contexts for different generative models and data modalities (Figure~\ref{fig:schematic-diagram}). They are extremely useful from an interpretability perspective as they represent the steps of the sampler responsible for a given property of the output~\citep{georgiev2023journey,qi2024safetyalignmentjusttokens}, and have also been used to provide richer stepwise rewards for preference optimization and finetuning  \citep{abdin2024phi4technicalreport,lin2024criticaltokensmattertokenlevel,qi2024safetyalignmentjusttokens}. As the applications of generative models proliferate, it is crucial from interpretability, safety, and capability perspectives to understand how and why these critical windows emerge. 

Recently, this phenomenon has received significant attention within the theoretical literature on diffusion models~\citep{raya2023spontaneous,sclocchi2024probinglatenthierarchicalstructure,sclocchi2024phasetransitiondiffusionmodels,biroli2024dynamical,li2024criticalwindowsnonasymptotictheory}. While existing works do offer predictive theory in the diffusions setting, they either (A) make strong distributional assumptions or (B) rely heavily on the particulars of diffusion, which do not straightforwardly extend to autoregressive models. Works in the former category carry out non-rigorous statistical physics calculations tailored to specific toy models of data like mixtures of Gaussians or context-free grammars with random production rules~\citep{sclocchi2024phasetransitiondiffusionmodels,sclocchi2024probinglatenthierarchicalstructure,biroli2024dynamical,raya2023spontaneous}. Works in the latter category derive rigorous bounds in settings without explicit parametric structure, e.g. mixtures of strongly log-concave distributions~\citep{li2024criticalwindowsnonasymptotictheory}, but they rely on tools like Girsanov's theorem which are specific to Gaussian diffusion. Additionally, the bounds in the latter are generally cruder, losing dimension-dependent factors. We ask:
\begin{center}
\emph{Is there a simple, general theory that can explain critical windows across all generative modeling paradigms and data modalities?}
\end{center}

\subsection{Our contributions}

In this work, we develop a simple theoretical framework that explains critical windows in both diffusion models and autoregressive models. Our bounds are fully rigorous and show that such windows arise generically when the model localizes from a larger sub-population to a smaller one, which we will formalize in the following section. Below we highlight our main contributions:
\begin{enumerate}
    \item \textbf{Generality}: In comparison to existing work, our theory (Theorem~\ref{thm:masters_theorem}) makes no distributional assumptions, requires no statistical physics or stochastic calculus machinery, and removes the dimension dependence suffered by existing rigorous bounds. \item \textbf{Diverse instantiations}: To illustrate the flexibility of our bounds, we explicitly compute the locations and widths of these windows for different generative models and data modalities (Section~\ref{sec:eg_theory}) and empirically verify our predictions on structured output examples. One such example we provide elucidates a new connection between critical windows for in-context learning and the all-or-nothing phenomenon in statistical inference.  
    \item \textbf{Insights into hierarchical data}: We instantiate our bounds for hierarchically structured models of data, significantly generalizing results of~\citep{li2024criticalwindowsnonasymptotictheory} which only applied to diffusions and Gaussian mixtures (Section~\ref{sec:hierarchy}). This allows us to show that the hierarchy for a generative model may resemble the hierarchy of the true data generating process if both come from the same kind of sampler, but in general may differ.
    \item \textbf{Experimental results}: Finally, we empirically demonstrate critical windows for generations from~\llamainstruct,~\phiinstruct, and~\qweninstruct~on $7$ different math and reasoning benchmarks. Concurrently with~\citep{abdin2024phi4technicalreport,lin2024criticaltokensmattertokenlevel}, we observe that critical windows occur during important mistakes in the reasoning patterns of LLMs.
\end{enumerate}

In fact, our theory applies more generally to any \emph{stochastic localization sampler} (see Section~\ref{sec:stocloc} for a formal description)~\cite{montanari2023samplingdiffusionsstochasticlocalization,chen2022localization}. Roughly speaking, a stochastic localization scheme is any generative model given by a time-reversal of a Markovian degradation process which takes a sample from the target distribution and generates progressively less informative ``observations'' of it. In diffusion models, the degradation is a convolution of the original sample with larger and larger amounts of Gaussian noise. In autoregressive models, the degradation is the progressive masking of entries from right to left. Importantly, our theory does not use anything about the specific structure of the sampler beyond the Markovianity of the observation process.

Finally, our theory provides valuable insights for practitioners. For instance, in Example~\ref{example:ar_jailbreak} we provide a model for critical windows in jailbreaks and the Yellowstone example~\citep{anthropicyellowstone,qi2024safetyalignmentjusttokens}, and argue that training on corrections from critical windows can enable models to recover from these `bad' modes of behavior. This provides rigorous theoretical justification for~\cite{qi2024safetyalignmentjusttokens}'s approach for deepening safety alignment through finetuning. 

\subsection{Related work}\label{sec:related}
We briefly overview some related work here and defer our discussion of other relevant literature to Appendix~\ref{sec:app:related_work_cont}. 

\paragraph{Theory of critical windows in diffusion.} Several recent works have studied critical windows in the context of diffusion models, using either statistical physics methods \citep{raya2023spontaneous,sclocchi2024probinglatenthierarchicalstructure,sclocchi2024phasetransitiondiffusionmodels,biroli2024dynamical} or Girsanov's theorem \citep{li2024criticalwindowsnonasymptotictheory}. The statistical physics papers assume an explicit functional form for the data and use accurate and non-rigorous statistical physics methods to compute critical windows. For instance, \cite{biroli2024dynamical} computes the critical time at which the reverse process specializes to one component for a mixture of two spherical Gaussians using a Landau-type perturbative calculation, and \cite{sclocchi2024phasetransitiondiffusionmodels,sclocchi2024probinglatenthierarchicalstructure} passed through a mean-field approximation to compute the critical windows for a \emph{random hierarchy model} ~\citep{petrini2023deep}, a multi-level context-free grammar with random production rules. Our work is most similar to \citep{li2024criticalwindowsnonasymptotictheory}, which derives rigorous, non-asymptotic bounds analogous to our Theorem~\ref{thm:masters_theorem} for mixtures of log-concave distributions with Girsanov's theorem \citep{DBLP:conf/iclr/ChenC0LSZ23}.

In contrast to existing work, our theory applies to all localization-based samplers, including diffusion and autoregressive language models, and imposes no functional form or log-concavity assumptions on the distribution. We also improve upon the main theorem of~\citep{li2024criticalwindowsnonasymptotictheory} by obtaining dimension-independent error bounds. Using our improved theorem, we can extend the definition of hierarchy of critical windows from \cite{li2024criticalwindowsnonasymptotictheory} to all localization-based samplers and, for continuous diffusions, to distributions beyond mixtures of Gaussians. 
\paragraph{Forward-reverse experiment.} Here we study the forward-reverse experiment, where we \emph{noise and denoise} samples with a given attribute to understand critical windows. This was also explored in \citep{li2024criticalwindowsnonasymptotictheory,sclocchi2024phasetransitiondiffusionmodels,sclocchi2024probinglatenthierarchicalstructure}. This approach is very similar to the framework in which one imagines re-running the reverse process at an intermediate point $Y_t$~\cite{georgiev2023journey,biroli2024dynamical,raya2023spontaneous}. Both perspectives provide rigorous frameworks to understand critical windows, and in the case where the forward process is deterministic, i.e. autoregressive language models, these frameworks are equivalent.
\paragraph{Stochastic localization.} 
\cite{el2022sampling,montanari2023posterior,alaoui2023sampling, montanari2023sampling, huang2024sampling} applied Eldan's stochastic localization method~\citep{eldan2013thin,eldan2020taming} to develop new sampling algorithms for distributions inspired by statistical physics. Our work applies the stochastic localization framework~\citep{montanari2023samplingdiffusionsstochasticlocalization} to understand an empirical phenomenon appearing among different localization-based samplers widely used in practice.

