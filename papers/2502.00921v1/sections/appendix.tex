\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\appendix
\onecolumn
\section{Additional related work}\label{sec:app:related_work_cont}
\paragraph{Theory of in-context learning for language models.} With respect to theory for language models, our results are most closely related to the Bayesian framework for in-context learning \citep{xie2022explanationincontextlearningimplicit,akyurek2023learningalgorithmincontextlearning, garg2023transformerslearnincontextcase,zhang2023doesincontextlearninglearn,arora2024bayesianscalinglawsincontext}. For example, \citep{xie2022explanationincontextlearningimplicit} also considered a mixture model of topics and showed that language models can learn the underlying class despite in-context learning and training distribution mismatch. We view this manuscript as connecting the Bayesian framework for in-context learning to other empirical phenomena observed in language models and diffusion models and the all-or-nothing phenomenon. 

\paragraph{Chain of thought.} \citep{lin2024criticaltokensmattertokenlevel,abdin2024phi4technicalreport} also observed that the presence of critical windows in the chain of thought of math and reasoning tasks and their significance in leading the model to incorrect outputs, concurrent with our results in Figure~\ref{fig:cw_diff_combined}. They then used them to provide rewards or data for a preference optimization algorithm to improve reasoning performance. \citep{lin2024criticaltokensmattertokenlevel} called them \emph{critical tokens} and utilized a contrastive estimation algorithm to identify critical windows and provide token-level rewards. The Phi-4 Technical report called them \emph{pivotal tokens}, developed a binary-search based algorithm to identify the location of critical windows, and used them to produce contrasting pairs for preference optimization~\citep{abdin2024phi4technicalreport}. Using our broad theoretical perspective, we provide new insight into critical windows of these kinds and view our work as corroborating and extending these empirical works.

\paragraph{Jailbreaks.} Existing work on jailbreaks has studied the appearance of critical windows in the first few generated tokens \citep{qi2024safetyalignmentjusttokens,zhang2024dissecting,he2024safedataidentifyingbenign,lin2023unlockingspellbasellms}. Our theory provides a simple explanation for when jailbreaks occur: when the unaligned component assigns a much higher probability to the current text than the aligned component, then the model is jailbroken. This generalizes the explanation from \cite{qi2024safetyalignmentjusttokens} (see Example~\ref{example:ar_jailbreak} for our particular formalism of their insights). It also explains the success of perplexity-based monitors for jailbreaks \citep{alon2023detectinglanguagemodelattacks}, which monitor for a low probability of the context and generation. We view our work as providing a rigorous mathematical framework for jailbreaks, as well as highlighting the important role off-distribution contexts play in eliciting harmful behaviors; we also develop a novel jailbreak from our framework (Section~\ref{app:jailbreak_dataset}
) similar to the adversarial contrast decoding method proposed by \citep{zhao2024adversarialcontrastivedecodingboosting}, which also uses a likelihood ratio between an unaligned and an aligned model. However, we use a jailbroken and non-jailbroken pair of models instead of two versions of the model with different prompts. 


\section{Examples of stochastic localization samplers}\label{app:ex_stocloc}
In this section, we present several kinds of generative models within the stochastic localization framework and their forward and reverse processes.  \begin{example}[Continuous Diffusion Models \citep{li2024criticalwindowsnonasymptotictheory}] For continuous diffusion models, the forward process progressively degrades samples $X \sim p$ into pure Gaussian noise through scaling and convolution with Gaussian noise. It is the Ornstein-Uhlenbeck process, a stochastic process $(\forward_t)_{t\ge 0}$ given by the following stochastic differential equation (SDE),
\begin{equation}
    \D\forward_t = -\forward_t\,\D t + \sqrt{2}\D B_t\,, \qquad \forward_0 \sim p\,, \label{eq:forward}
\end{equation}
where $(B_t)_{t\ge 0}$ is a standard Brownian motion. Let $q_t \triangleq \mathrm{law}(\forward_t)$ for $t \geq 0$, and observe that as $t\to \infty$, $q_t$ converges exponentially quickly to the standard Gaussian distribution $\mathcal{N}(0,\mathrm{Id})$. Assume we end the forward process at time $T \ge 0.$ For the reverse process $(\reverse_t)_{t\in[0,T]}$, we employ the \emph{reversal} of the Ornstein-Uhlenbeck SDE, given by  
\begin{equation}
    \D\reverse_t = \{\reverse_t + 2\nabla\ln q_{T-t}(\reverse_t)\}\,\D t + \sqrt{2}\,\D B_t\,,\qquad \reverse_T \sim q_T\, \label{eq:reverse}
\end{equation}
where here $(B_t)_{t\ge 0}$ is also a Brownian motion. Defining $(Y_t)_{t \in \I}=(X_t)_{t \in \I}$, we see that the forward process satisfies the Markov property in  Definition~\ref{def:observation_process}, and the information from the original sample $X_0$ is degraded by more steps in the SDE. Furthermore, the reverse SDE with parameterized by the score function $\nabla \ln q_{T-t}(\reverse_t)$ can be viewed as successively sampling from the posteriors via Tweedie's formula.  
\end{example}
\begin{example}[Discrete Diffusion Models \citep{lou2024discretediffusionmodelingestimating}] Consider a set $\mathcal{A}$ denoting the vocabulary and let $p \in \mathcal{A}^T$, and consider a forward process with index set $\I=[0,K]\cup \{\infty\}$, $Y_0=X$, and $Y_t \in \mathcal{A}^T$ defined in the limit as follows,
\[p(Y_{t+\Delta t} = a \mid Y_t = b) = \delta_{ab} + Q_t(b, a) \Delta t + O(\Delta t^2) \tag{2},\]
where $Q_t \in \mathbb{R}^{n \times n}$ are diffusion matrices with nonnegative non-diagonal entries and columns which sum to $0$. $(Y_t)_{t \in \I}$ is also a Markov chain and as $t \to \infty$, $Y_t$ is degraded until it is eventually uninformative about the original sample $Y_0$.  
\end{example}

\begin{example}[Autoregressive Language Models]
Consider a set $\mathcal{A}$ denoting the vocabulary and let $p \in \mathcal{A}^T$, and consider a forward process with index set $\I=\{0,1,2,\dots,T\}$, $Y_0=X$, and $Y_t \in \mathcal{A}^{T-t}$. For $t \in \I$, we let $Y_t$ equal the last first $T-t$ tokens of $X$. Clearly this is a Markov Chain, and the reverse process is equivalent to next-token prediction.
\end{example}
\section{Proof details}
\subsection{Deferred details from Section~\ref{sec:master}}\label{app:master_details}
For Theorem~\ref{thm:masters_theorem}, we employ the following two helper lemmas. 

\masterinequality*
\begin{proof}
We can rewrite  $\kernelreverse[](x| Y_{\wh{T}}), \kernelreverse[](x| Y_{\wh{T}},\Send)$ using the law of total probability and Bayes' rule. 
\begin{align*}
\kernelreverse[](x| Y_{\wh{T}}) &=\sum_{\theta \in \Theta} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta)\\
\kernelreverse[](x| Y_{\wh{T}},\Send) &=\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}},\Send)\kernelreverse[](x|Y_{\wh{T}},\theta) = \frac{\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta)}{\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})}.
\end{align*}
Note that the second equality on the second line follows from the fact that for all $\theta \in \Send$, the posteriors $\kernelreverse[]_{t \to \Theta} (\cdot|Y_t) \propto \kernelreverse[]_{t \to \Theta}(\cdot|Y_t,\Send)$  by the same normalization constant. Therefore the difference can be written as 
\begin{align*}
&\int \left|\kernelreverse[](x| Y_{\wh{T}})-\kernelreverse[](x| Y_{\wh{T}},\Send)\right| dx \\
&= \int \left|\sum_{\theta \in \Theta} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta)-\frac{\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta)}{\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})}\right| dx \\
&=  \int \Bigg|\left(1-\frac{1}{\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})}\right)\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta) + \sum_{\theta \in \Theta-\Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta)\Bigg| dx. 
\end{align*}
If $\sum_{\theta \in \Theta-\Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})=0$, then the above is equal to $0$ and we are done. If it is non-zero, we can factor out $\sum_{\theta \in \Theta-\Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})$ term, which allows us to write everything in terms of posteriors with respect to $\Theta-\Send$ and $\Send$, 
\begin{align*}
&\int \Bigg|\left(1-\frac{1}{\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})}\right)\sum_{\theta \in \Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta) + \sum_{\theta \in \Theta-\Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})\kernelreverse[](x|Y_{\wh{T}},\theta)\Bigg| dx\\
&= \sum_{\theta \in \Theta-\Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}}) \int \left|\kernelreverse[](x|Y_{\wh{T}},\Send)-\kernelreverse[](x|Y_{\wh{T}},\Theta-\Send)\right|dx 
\end{align*}
Employing the trivial observation that 
\begin{align*}
&\int \left|\kernelreverse[](x|Y_{\wh{T}},\Send)-\kernelreverse[](x|Y_{\wh{T}},\Theta-\Send)\right|dx 
\\
&\leq \int \kernelreverse[](x|Y_{\wh{T}},\Send)+\kernelreverse[](x|Y_{\wh{T}},\Theta-\Send)dx  \leq 2,
\end{align*}
we have
\begin{align*}
\int \left|\kernelreverse[](x| Y_{\wh{T}})-\kernelreverse[](x| Y_{\wh{T}},\Send)\right| dx 
 \leq 2 \sum_{\theta \in \Theta-\Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}}).
\end{align*}
\end{proof}

\masterinequalitytwo*
\begin{proof}
We obtain through Bayes' rule, 
\begin{align*}
 \sum_{\theta \in \Theta-\Send} \kernelreverse[]_{t \to \Theta}(\theta|Y_{\wh{T}})
= \frac{\sum_{\theta \in \Theta-\Send} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})}{\sum_{\theta \in \Theta} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})}. 
\end{align*}
We divide by the same normalizing constant $\sum_{\theta \in \Theta-\Send} w_\theta$ to obtain
\begin{align*}
 \frac{\sum_{\theta \in \Theta-\Send} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})}{\sum_{\theta \in \Theta} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})} &= \frac{\frac{\sum_{\theta \in \Theta-\Send} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})}{\sum_{\theta \in \Theta-\Send} w_\theta}}{\frac{\sum_{\theta \in \Theta-\Send} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})}{\sum_{\theta \in \Theta-\Send} w_\theta}+\frac{\sum_{\theta \in \Send} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})}{\sum_{\theta \in \Send} w_\theta} \cdot \frac{\sum_{\theta \in \Send} w_\theta}{\sum_{\theta \in \Theta-\Send} w_\theta}}\\
&\leq \max\left(1,\frac{\sum_{\theta \in \Theta-\Send} w_\theta}{\sum_{\theta \in \Send} w_\theta}\right)  \frac{p^{\Theta-\Send}_{\wh{T}}(Y_{\wh{T}})}{ p^{\Theta-\Send}_{\wh{T}}(Y_{\wh{T}})+p^{\Send}_{\wh{T}}(Y_{\wh{T}})}. 
\end{align*}

\end{proof}


\subsection{Proofs for Section~\ref{sec:eg_theory}}\label{app:eg_theory}
\subsubsection{Diffusions}
Here we use an alternative $f$-divergence to characterize the critical windows, the squared \emph{Hellinger distance}, defined as $\mathrm{H}^2(P,Q) \triangleq \int(\sqrt{dP}-\sqrt{dQ})^2 d\mu$, because there are explicit computations for the Hellinger distance for mixtures of Gaussians. We similarly exploit the following ratio inequality akin to Lemma~\ref{lem:ratio_inequality}, 
\begin{restatable}{lemma}{ratioinequalitylemmahellinger}\label{lem:ratio_inequality_hellinger}
For probability measures $P,Q$, 
\begin{align*}
\mathbb{E}_{x \sim P} \left[\frac{\D Q}{\D P+\D Q} \right]
\le \frac{1}{2}(1-\frac{1}{2}\mathrm{H}^2(P,Q)).
\end{align*}
\end{restatable}
We apply the following well-known formula for the Hellinger distance between two Gaussians.  
\begin{restatable}{lemma}{hellingergauss}\label{lem:hellingergauss}
We have 
$$
1-\frac{1}{2}H^2(\mathcal{N}(\mu_P,\Sigma_P),\mathcal{N}(\mu_Q,\Sigma_Q))= \frac{|\Sigma_P|^{1/4}|\Sigma_Q|^{1/4}}{\left|\frac{\Sigma_P+\Sigma_Q}{2}\right|^{1/2}} \exp \left\{-\frac{1}{8}(\mu_P-\mu_Q)^\top \left[\frac{\Sigma_P+\Sigma_Q}{2}\right]^{-1}(\mu_P-\mu_Q)\right\}.
$$
\end{restatable}
\diffusiontwogaussian*
\begin{proof}
The proof for $\Tbefore$, a simple application of Pinsker's inequality, can be found in Appendix B.1 of \citep{li2024criticalwindowsnonasymptotictheory}. Directly applying the new Master Theorem~\ref{thm:masters_theorem} to $\Tafter$, we need only show that 
\begin{align*}
\frac{\sqrt{2}}{2} \left(1-\frac{1}{2}\mathrm{H}^2(p^{+1}_{\Tafter},p^{-1}_{\Tafter})\right) \leq \frac{\sqrt{2}}{2} \exp(-\frac{1}{2}\|\mu\|^2e^{-2t})\leq \epsilon.
\end{align*}
\end{proof}

\gaussianrandommeans*
\begin{proof}
The proof for $\Tbefore$ can be found in Section 5.2 of~\citep{li2024criticalwindowsnonasymptotictheory}. We need to slightly modify the proof of Theorem~\ref{thm:masters_theorem} so that we can write the desired bound for $\Tafter$ in terms of the Hellinger distance of individual components. We use the same notation. By convexity, we can bound
\begin{align*}
\E_{Y_{\wh{T}} \sim p^{\Send}_{\wh{T}}}\left[ \frac{\sum_{\theta \in \Theta-\Send} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})}{\sum_{\theta \in \Theta} w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})} \right] &\leq \sum_{\theta \in \Theta-\Send} w_\theta  \sum_{\phi \in \Send} w_\phi  \E_{Y_{\wh{T}} \sim p^{\phi}_{\wh{T}}} \left[ \frac{p^\theta_{\wh{T}}(Y_{\wh{T}})}{w_\theta p^\theta_{\wh{T}}(Y_{\wh{T}})+w_\phi p^\phi_{\wh{T}}(Y_{\wh{T}})} \right]\\
&\leq K \max_{\theta \in \Theta-\Send,\phi \in \Send} \left(1-\frac{1}{2}\mathrm{H}^2(p^\theta_{\wh{T}},p^\phi_{\wh{T}})\right) \leq \epsilon,
\end{align*}
when $\wh{T} \leq \Tafter$. To conclude the second part of the theorem, observe that by concentration of measure (e.g., Theorem 3.1.1 from \citep{vershynin_high-dimensional_2018}) and a union bound, there exists a constant $T$ independent of $d$ such that $\|\mu_j\| \in [\sqrt{d}-T,\sqrt{d}+T]$ for all $j \in [K]$ with high probability. Furthermore, by known Gaussian Suprema inequalities, we can also assume that there exists a constant $T'$ independent of $d$ such that $|\langle \mu_i,\mu_j\rangle| \leq T'\|\mu_i\|$ (Lemma 5.1 from~\citep{Handel2016}). Thus, we can conclude that \begin{align*}
\max_{j \in [K]} \|\mu_i-\mu_j\|^2 &\leq 2d+4T\sqrt{d}+2T^2+2T'(\sqrt{d}+T) = O(d).\\
\max_{j \in [K]} \|\mu_i-\mu_j\|^2 &\geq 2d-4T\sqrt{d}+2T^2-2T'(\sqrt{d}+T) = \Omega(d).
\end{align*}
The difference in $\log$ scale is thus constant,
\begin{align*}
\frac{1}{2}\left(\ln \max_{j \in [K]} \|\mu_i-\mu_j\|^2  -\ln \min_{j \in [K],i\ne j} \|\mu_i-\mu_j\|^2  \right) = O_{K,\epsilon}(1)
\end{align*}
\end{proof}


\randommaskingdirac*
\begin{proof}
To prove $\TV(\modrevlaw{1}{\wh{T}}{},p^{1}) \lesssim \epsilon$ when $\wh{T} \leq \Tafter$, observe that when $\wh{T} \leq \Tafter$, the probability that all the differing elements between $\ell_1,\ell_{-1}$ are masked is exactly $\wh{T}^{d_H(\delta_{\ell_{1}},\delta_{\ell_{-1}})}\leq \epsilon^2$. That means that there exists a set $A$ with $p^{+1}_{\wh{T}}(A) \geq 1-\epsilon^2$ and $p^{-1}_{\wh{T}}(A)=0$, so by the definition of total variation, $\TV(p^{+1}_{\wh{T}},p^{-1}_{\wh{T}})\geq  1- \epsilon^2$. Obviously, $\TV(p^{+1}_{\wh{T}},p^{+1}_{\wh{T}})=0$ as well, so by Theorem~\ref{thm:masters_theorem}, we obtain $\TV(\modrevlaw{1}{\wh{T}}{},p^{1}) \lesssim \epsilon$. To prove that $\TV(\modrevlaw{1}{\wh{T}}{},p^{\{\pm 1\}}) \leq \epsilon$ when $\wh{T} \geq \Tbefore$, we need only show that $\TV(p^{+1}_{\wh{T}},p_{\wh{T}}) \leq \epsilon$. By Lemma 15 of \citep{li2024criticalwindowsnonasymptotictheory}, it suffices to show that $\TV(p^{+1}_{\wh{T}},p^{-1}_{\wh{T}}) \leq \epsilon$ by a simple triangle inequality argument. Consider the set $A \subset \{\pm 1, \mathrm{[MASKED]}\}^T$ such that $\TV(p^{+1}_{\wh{T}},p^{-1}_{\wh{T}})=p^{+1}_{\wh{T}}(A)-p^{-1}_{\wh{T}}(A)$. Consider the set $B =\supp(p^{+1}_{\wh{T}})\cap \supp(p^{-1}_{\wh{T}})$. For any $x \in B$, we know $p^{+1}_{\wh{T}}(x)=p^{-1}_{\wh{T}}(x)$ because the same number of tokens need to be masked from $\ell_{\pm 1}$. This means we have $p^{+1}_{\wh{T}}(B)=p^{-1}_{\wh{T}}(B)\geq \wh{T}^{d_H(\ell_{+1},\ell_{-1})} \geq 1-\epsilon$. Because $p^{+1}_{\wh{T}}(A)-p^{-1}_{\wh{T}}(A)=p^{+1}_{\wh{T}}(A-B)-p^{-1}_{\wh{T}}(A-B)$, we have $p^{+1}_{\wh{T}}(A)-p^{-1}_{\wh{T}}(A) \leq p^{+1}_{\wh{T}}(\{\pm 1, \mathrm{[MASKED]}\}^T-B) \leq \epsilon$.
\end{proof}

\subsubsection{Autoregression}\label{app:autoregression}
\arbinomialbiased*
\begin{proof}
Because only the direction of steps matter, we can model the critical window for this random walk as observing a sequence of $\pm 1$ with an autoregressive language model. Let $p \in \{\pm 1\}^T$, and consider a forward process with index set $\I=\{0,1,2,\dots,T\}$, $Y_0=X$, and $Y_t \in \{\pm 1,\mathrm{[MASKED]}\}^T$. For $t \in \I$, we let the last $t$ tokens of $Y_t$ be deterministically set to $\mathrm{[MASKED]}$. We generate data as a mixture of biased coins with separation $2\Delta < 0.01.$ For a mixture of two biased coins, with probabilities of $0.5 \pm \Delta$ ($\theta_{\pm 1}$ respectively) of yielding $1$, we can compute the critical window and show that it tightly clusters around $\Theta(1/\Delta^2)$. Let $\Theta=\{\theta_{\pm 1}\},  p^{\theta_{\pm 1}} \sim (\textrm{Bern}(\theta_{\pm 1}))^{\otimes T}, w_{\pm 1}=\frac{1}{2}$. We also assume $\epsilon^2 < 10^{-3}(0.5-\Delta)(0.5+\Delta)$. Then, on component $1$ we have the critical window $\Tbefore= T-\frac{\epsilon^2}{\Delta^2}+2$ and $\Tafter  = T-\frac{\ln (2/\epsilon^2)}{2\Delta^2}.$ When $\wh{T} \leq \Tafter$, then $\TV(\modrevlaw{1}{\wh{T}}{},p^1) \lesssim \epsilon$. When $\wh{T} \geq \Tbefore$, $\TV(\modrevlaw{1}{\wh{T}}{},p)\lesssim \epsilon$. 

Note that the number of $+1$ is sufficient for disambiguating $\theta_{\pm 1}$. To prove the bounds $\Tbefore$, we show that with only $\frac{\epsilon^2}{\Delta^2}-2$ samples the total variation between $0.5-\Delta$ and $0.5+\Delta$ is negligible. Using  \cite{doi:10.1137/S0040585X9797821X}, we find
\begin{align*}
\TV(p^{+1}_{\Tafter},p^{-1}_{\Tafter}) \leq \frac{2\Delta \sqrt{\frac{T-\Tafter+2}{2(0.5-\Delta)(0.5+\Delta)}}}{\left(1-2\Delta \sqrt{\frac{T-\Tafter+2}{2(0.5-\Delta)(0.5+\Delta)}}\right)^2} \lesssim 3\epsilon.
\end{align*}
For $\Tafter$, we compute how many samples it takes for $p^{\pm 1}$ to have only $\epsilon$ overlap in total variation using Hoeffding's inequality. If we have $n$ samples, the mean $\overline{X}$ of the $n$ samples of $\pm1$ for $p=0.5+\Delta$ satisfies the concentration inequality $P(|\overline{X} - 2\Delta|>\Delta) \leq 2\exp(-2nt^2)$ (furthermore we can ignore the stopping condition by our requirement that  $\frac{\ln (2/\epsilon^2)}{2\Delta^2} < A$). We find $P(|\overline{X} - 2\Delta|>\Delta)  \leq \epsilon^2$ for $T-\Tafter$ samples, proving that the total variation is at least $1-\epsilon^2$. 
\end{proof}

\subsubsection{Autoregressive model with a Gaussian mixture model as the underlying concept distribution }\label{app:autoregressive_gmm}
We consider a model for autoregressive data similar to the one presented in \citep{arora2019latentvariablemodelapproach}. Each word is a vector $w \in \R^d$ and  the context length is $T \in \mathbb{Z}^{>0}$. The original samples are $x \in \R^{T \times d}$. Let $\Theta = \{u,v\}$, where $u, v \in \mathcal{S}^{d-1}$. We define the distribution $p^\theta$ for $\theta \in \Theta$ as follows. We generate the path of a discourse vector $\left(C_t^\theta\right)_{t \in [0,\infty]} \in \mathbb{R}^d$ with the reverse SDE Orstein-Uhlenbeck process such that $C_\infty^\theta \sim \mathcal{N}(0,\Id)$ and $C_0^\theta \sim \mathcal{N}(0, \Id + \alpha \theta \theta ^\top)$ for some $\alpha > 0$. We let $q^\theta_t$ be the law of $C^\theta_t$ for $t \geq 0$. We let $\I=\{0,1,\dots,T\}$, and for $t \in \I$, we draw samples $w^\theta_t \in \mathbb{R}^d$ where we impose a normal Gaussian prior and have $w^\theta_t|C_t^\theta \propto \exp(\langle C_t^\theta, \cdot \rangle )$. Then we return the corpus $\{w^\theta_t\}_{t \in \I}$ as an output. 
\begin{theorem}[Autoregressive with a mixture of two Gaussians as the concept distribution ]\label{example:ar_mixture}



We assume that $\exp(-T) \sqrt{\alpha-\log(1+\alpha)} \leq \epsilon$. Let $\phi(x)=\frac{2+x}{(2+x (1+\langle u,v\rangle)/2)(2+x (1-\langle u,v\rangle)/2)}$. Then, on component $u$ we have 
\begin{align}
\Tafter = \frac{1}{2} \ln\left[\frac{\alpha}{\phi^{-1}(\epsilon^2)}\right], \Tbefore =\frac{1}{2}\ln\left[\frac{\alpha\sqrt{1+\alpha^2} \sqrt{1-\langle u,v\rangle^2}}{\epsilon}\right]. 
\end{align}
When $\wh{T} \leq \Tafter$, then $\TV(\modrevlaw{\{u\}}{\wh{T}}{},p^{\{u\}}) \lesssim \epsilon$. When $\wh{T} \geq \Tbefore$, $\TV(\modrevlaw{\{u\}}{\wh{T}}{},p)\lesssim \epsilon$. 
\end{theorem}

\noindent This proof will require Theorem~\ref{thm:girsanov} and Lemmas~\ref{lem:hellingergauss} and~\ref{lem:opnormtwovec}, which is stated below:
\begin{theorem}[Section 5.2 of~\cite{DBLP:conf/iclr/ChenC0LSZ23}]\label{thm:girsanov}
    Let $(Y_t)_{t\in[0,T]}$ and $(Y'_t)_{t\in[0,T]}$ denote the solutions to
    \begin{align*}
        \D Y_t &= b_t(Y_t) \, \D t + \sqrt{2}\D B_t\,, \qquad Y_0 \sim q \\
        \D Y'_t &= b'_t(Y'_t)\, \D t + \sqrt{2}\D B_t\,, \qquad Y'_0 \sim q\,.
    \end{align*}
    Let $q$ and $q'$ denote the laws of $Y_T$ and $Y'_T$ respectively. If $b_t, b'_t$ satisfy that $\int^T_0 \mathbb{E}_Q\,\norm{b_t(Y_t) - b'_t(Y_t)}^2\, \D t < \infty$, then $\KL(q\|q') \le \int^T_0 \mathbb{E}_Q\,\norm{b_t(Y_t) - b'_t(Y_t)}^2\,\D t$.
\end{theorem}


\begin{restatable}{lemma}{opnormtwovec}\label{lem:opnormtwovec}
Let $u,v \in \mathcal{S}^{d-1}$. Then
\begin{align*}
\|uu^\top - vv^\top\|_{\rm op} &\leq \sqrt{1-\langle u,v\rangle^2}\\ 
\lambda(uu^\top + vv^\top) &= \{1\pm \langle u,v \rangle \}.
\end{align*}
\end{restatable}
\begin{proof}
There exists $r \in \mathcal{S}^{d-1}$ such that $v = \langle u,v\rangle u + \sqrt{1-\langle u,v\rangle^2} r$ and $u \perp r$. We find that 
\begin{align*}
uu^\top - vv^\top = (1-\langle u,v\rangle^2) uu^\top - (1-\langle u,v\rangle^2) rr^\top - \langle u,v\rangle \sqrt{1-\langle u,v\rangle^2}[ur^\top+ru^\top].
\end{align*}
We can explicitly compute the eigenvalues of $uu^\top - vv^\top$ using the discriminant and find that they are equal to $\pm \sqrt{1-\langle u,v\rangle^2}.$
By a similar derivation, we can write $$uu^\top +vv^\top = (1+\langle u,v\rangle^2) uu^\top + (1-\langle u,v\rangle^2) rr^\top+\langle u,v\rangle \sqrt{1-\langle u,v\rangle^2}[ur^\top+ru^\top].$$  
which gives us eigenvalues for $uu^\top+vv^\top$ of  $1\pm \langle u,v \rangle $.
\end{proof}

\begin{proof}
To compute the $\Tafter$ bounds, we compare the difference in Hellinger distance of the distribution of words words at generated at index $\wh{T}$, $w^u_{\wh{T}}$. By the data processing inequality $1-\frac{1}{2}H^2(p^u_{\wh{T}},p^v_{\wh{T}}) \leq 1-\frac{1}{2}H^2(w^u_{\wh{T}},w^v_{\wh{T}})$, so it suffices to show $1-\frac{1}{2}H^2(w^u_{\wh{T}},w^v_{\wh{T}}) \lesssim \epsilon$.  Because the Gaussian is its own conjugate prior and $w^u_{\wh{T}}|C^\theta_t \propto \exp(-\frac{1}{2}\|w^u_{\wh{T}}-C^\theta_t\|^2)$, we can compute $w^u_{\wh{T}} |C^\theta_t  \sim \mathcal{N}(C^\theta_t,\Id)$ and $w^u_{\wh{T}} \sim \mathcal{N}(0,2\Id+\alpha e^{-2t} uu^\top)$. Applying Lemmas~\ref{lem:hellingergauss} and~\ref{lem:opnormtwovec}, we can explicitly compute 
\begin{align*}
1-\frac{1}{2}H^2(w^u_{\wh{T}},w^v_{\wh{T}})\lesssim \sqrt{\frac{2+\alpha e^{-2\wh{T}}}{(2+\alpha e^{-2\wh{T}} (1+\langle u,v\rangle)/2)(2+\alpha e^{-2\wh{T}} (1-\langle u,v\rangle)/2)}} \lesssim \sqrt{\phi(\alpha e^{-2\wh{T}})} \leq \epsilon.
\end{align*}

\noindent To compute $\Tbefore$, we first use the data processing inequality to reduce the difference in the emitted tokens to the difference in the paths of the context vectors, and then apply the approximation error bounds from Theorem~\ref{thm:girsanov} to bound the differences in path measures. When $\wh{T} \geq \Tbefore$, we can use the triangle inequality to write
$\TV(\modrevlaw{\{u\}}{\wh{T}}{},p) = \TV(p^u_{\wh{T}},p^{\{u,v\}}_{\wh{T}}) \leq \TV(p^u_{\wh{T}},p^v_{\wh{T}})$. Note that $p^\theta_{\wh{T}}$ is the distribution of the first $T-\wh{T}$ tokens generated by the model under $\theta$. Note that $p^u_{\wh{T}}$ is a function of $(C^u_t)_{t \in \I\cap[\wh{T},T]}$ and $p^v_{\wh{T}}$ is a function of $(C^v_t)_{t \in \I\cap[\wh{T},T]}$. By the data processing inequality, we can bound the difference in terms of the distributions over the tokens in terms of the law of the process of the discourse vectors, $$\TV(p^u_{\wh{T}},p^v_{\wh{T}}) \leq \TV((C^u_t)_{t \in \I\cap[\wh{T}]},(C^v_t)_{t \in \I\cap[\wh{T},T]}) \leq \TV((C^u_t)_{t \in [\wh{T},T]},(C^v_t)_{t \in [\wh{T},T]}).$$ 
Note that for $\theta \in \Theta$, $(C^\theta_t)_{t \in [\wh{T},T]}$ is generated by the following reverse time SDE, 
\begin{equation}
    \D C^\theta_t = \{C^\theta_t + 2\nabla\ln q^\theta_{t}(C^\theta_t)\}\,\D t + \sqrt{2}\,\D B_t,  \qquad t \in [\wh{T},T], C^\theta_T \sim q^\theta_T. \label{eq:reverse_discourse}
\end{equation}
Now we define $(C^{u \to v}_t)_{t \in [\wh{T},T]}$ to be the reverse SDE defined by initializing at $q^u_T$ but with the score of $q^v_t$,
\begin{equation}
    \D C^{u \to v}_t = \{C^{u \to v}_t + 2\nabla\ln q^v_{t}(C^{u \to v}_t)\}\,\D t + \sqrt{2}\,\D B_t,  \qquad t \in [\wh{T},T], C^{u \to v}_T \sim q^u_T. \label{eq:reverse_discourse_mod}
\end{equation}
By the triangle inequality, we have 
\begin{align*}
\TV((C^u_t)_{t \in [\wh{T},T]},(C^v_t)_{t \in [\wh{T},T]}) \leq \underbrace{\TV((C^u_t)_{t \in [\wh{T},T]},(C^{u \to v}_t)_{t \in [\wh{T},T]})}_{\text{(I)}}+\underbrace{\TV((C^{u \to v}_t)_{t \in [\wh{T},T]},(C^{v}_t)_{t \in [\wh{T},T]})}_{\text{(II)}}. 
\end{align*}
\noindent To bound (I), observe that the SDEs have different scores but the same initializations. We apply Theorem~\ref{thm:girsanov} to $\TV((C^u_t)_{t \in [\wh{T},T]},(C^{u \to v}_t)_{t \in [\wh{T},T]})$ and obtain 
\begin{align*}
\TV((C^u_t)_{t \in [\wh{T},T]},(C^{u \to v}_t)_{t \in [\wh{T},T]}) &\leq \sqrt{\KL((C^u_t)_{t \in [\wh{T},T]}||(C^{u \to v}_t)_{t \in [\wh{T},T]})}\\
&\leq \sqrt{\int_{\wh{T}}^T \E_{X \sim C^u_t}\|\nabla \ln p^u_t(X) - \nabla \ln p^v_t(X)\|^2 dt}
\end{align*}
We simplify the inner expectation by using the 
\begin{align*}
\|\nabla \ln p^u_t(X)- \nabla \ln p^v_t(X)\| &= \|\left[(\Id+\alpha e^{-2t}uu^\top)^{-1}-(\Id+\alpha e^{-2t}vv^\top)^{-1}\right]x\|\\
 &= \|\left[\left(\Id-\frac{\alpha e^{-2t}}{1+\alpha e^{-2t}}uu^\top\right)-\left(\Id-\frac{\alpha e^{-2t}}{1+\alpha e^{-2t}}vv^\top\right)\right]x\|\\
&\lesssim \alpha e^{-2t} \|uu^\top - vv^\top\|_{\rm op}\left\|\prod_{\textrm{span}(u,v)}x\right\|,\\
&= \alpha e^{-2t}\sqrt{1-\langle u,v\rangle^2}\left\|\prod_{\textrm{span}(u,v)}x\right\|.\quad (\text{Lemma}~\ref{lem:opnormtwovec})
\end{align*}
We can upper bound $\E_{X \sim C^u_t}\|\prod_{\textrm{span}(u,v)}X\|^2$ by considering right-triangular $L$ such that $L^\top L  = \Id+\alpha uu^\top$. and $\prod_{\textrm{span}(u,v)} X = \prod_{\textrm{span}(u,v)}L Y$, where $Y \sim \mathcal{N}(0,\Id)$. The operator norm of $\prod_{\textrm{span}(u,v)} L$ is 
\begin{align*}
\left\| \prod_{\textrm{span}(u,v)} \circ L \right\|_{\rm op}   \leq \left\| L \right\|_{\rm op}   \leq \sqrt{1+\alpha^2e^{-2t}} \leq \sqrt{1+\alpha^2}.
\end{align*}
$\prod_{\textrm{span}(u,v)} \circ L$ is also rank $2$ and $\left(\prod_{\textrm{span}(u,v)}\circ L\right)Y=\left(\prod_{\textrm{span}(u,v)}\circ L \right)\circ \prod_{L^{-1}\textrm{span}(u,v)}Y$, where $\prod_{L^{-1}\textrm{span}(u,v)}Y \sim \mathcal{N}(0,\Id_2)$. Thus we have
\begin{align*}
\E_{Y \sim \mathcal{N}(0,\Id)}\left[\left\|\left(\prod_{\textrm{span}(u,v)}\circ L\right)Y\right\|^2\right] = \E_{Y' \sim \mathcal{N}(0,\Id_2)}\left[\left\|\left(\prod_{\textrm{span}(u,v)}\circ L \right)Y'\right\|^2\right] \lesssim 1+\alpha^2
\end{align*}
Combining this information together, we are able to compute,
\begin{align*}
\TV((C^u_t)_{t \in [\wh{T},T]},(C^{u \to v}_t)_{t \in [\wh{T},T]})  &\leq \sqrt{\int_{\wh{T}}^T \E_{X \sim C^u_t}\|\nabla \ln p^u_t(X) - \nabla \ln p^v_t(X)\|^2 dt}\\
&\lesssim \alpha\sqrt{1+\alpha^2} \sqrt{1-\langle u,v\rangle^2} e^{-2\wh{T}} \\
&\lesssim \epsilon.  
\end{align*}
\noindent To bound (II), we observe that both are run with the same score so we need only bound the difference at initialization. By the data processing inequality, we again have $\TV((C^{u \to v}_t)_{t \in [\wh{T},T]},(C^{v}_t)_{t \in [\wh{T},T]}) \leq \TV(p^u_T,p^v_T)$. We can again apply the triangle inequality to get $\TV(p^u_T,p^v_T) \leq \TV(p^u_T,\gamma^d) +\TV(\gamma^d,p^v_T) \leq $. For any $\theta \in \Theta$, we have by the forward convergence of the OU process $\TV(p^u_T,\gamma^d) \leq \exp(-T)\sqrt{\KL(p_u||\gamma^d)}$. We can explicitly compute $\KL(p_u||\gamma^d)$ as 
\begin{align*}
\KL(p_u||\gamma^d) = \frac{1}{2}\left[d+\alpha -d - \log (\Id+ \alpha uu^\top)\right] = \frac{1}{2}\left[\alpha - \log(1+\alpha)\right]. 
\end{align*}
Thus, we obtain the following bound on 
$\TV((C^{u \to v}_t)_{t \in [\wh{T},T]},(C^{v}_t)_{t \in [\wh{T},T]})$ of 
\begin{align*}
\TV((C^{u \to v}_t)_{t \in [\wh{T},T]},(C^{v}_t)_{t \in [\wh{T},T]}) &\lesssim \exp(-T) \sqrt{\alpha-\log(1+\alpha)} \lesssim \epsilon.  \qedhere
\end{align*}
\end{proof}

\subsubsection{Interweaving transitions from other distributions}\label{app:interweaving}
In this section, we extend our critical windows framework to the setting where at certain steps of sampling procedure, instead of using the reverse Markov transition kernel from the original stochastic localization sampler, we use an alternative distribution which is not necessarily related to the original sampler. This includes many important applications of generative models, in which one seeks to combine the priors learned from data with some other algorithm. For example, one may want to combine the language model with  a problem generation oracle in in-context learning \citep{dong2024surveyincontextlearning}.

As \cite{xie2022explanationincontextlearningimplicit} points out, the transition from the answer to one problem to the problem statement of another example in-context learning is determined by an alternative transition kernel (which they call $p_{\textrm{prompt}}$). Although the probability of transition from one answer to the problem statement of another example is extremely low under the natural data distribution, one still hopes that with sufficiently many samples, the model selects the correct $\theta^* \in \Theta$ if these lower probability transitions are overcome by the distributional difference for $\theta \in \Theta$ with $\theta \ne \theta^*$. Similarly, under our critical windows framework, we can hope to capture the idea that we specialize to a particular $\theta^*$ given a sufficiently long context. In Section~\ref{subsec:gen_interweaving}, we first present a general framework for characterizing critical windows in this setting. Then, in section~\ref{subsec:icl}, we consider the case of in-context learning by autoregressive language models and prove convergence. 

\subsubsection{General interweaving framework}\label{subsec:gen_interweaving}
\noindent We  present this framework for the case where the index set $\I=\{0,1,\dots,m\}$ is discrete. Like before, assume we have a series of reverse Markov transition kernels $\kernelreverse[p]_{k \to k-1} (\cdot|\cdot)$, for $k \in\I$, but we also assume we have an alternative distribution $\kernelreverse[\alt]_{k \to k-1}(\cdot |\cdot)$ that we use to sample for transitions $k \in A \subsetneq \I$. For our sampling procedure, we sample $Y_m$, and for $k=m-1,m-2,\dots,0$, we take $Y_{k}\sim \kernelreverse[p]_{k+1\to k} (\cdot|Y_{k+1})$ for $k \in \I-A$ and $Y_{k}\sim \kernelreverse[\alt]_{k+1\to k} (\cdot|Y_{k+1})$ for $k \in A$. We denote the final distribution $p^{\alt}$. \\

\noindent Now, we also need to adjust our definitions of $p^S$ to this particular sampling procedure. We define $p^{S,\alt}$ for $S \subset \Theta$ to the distribution over outputs when we instead use the kernels $\kernelreverse[]_{k+1\to k} (\cdot|Y_{k+1},S)$ instead of $\kernelreverse[]_{k+1\to k} (\cdot|Y_{k+1})$. To relate $p^{\theta,\alt}$ to $p^{\theta,\alt}$ for $\theta \in \Theta$, we need to assume transitions from $\alt$ do not affect the posterior distribution over $p^{\Theta}(\theta|Y_t)$. 
\begin{assumption}\label{equality_other_transitions}
For all $y \in \mathbb{R}$ and $x \in \supp(\kernelreverse[\alt]_{k+1\to k} (\cdot|y))$, we have for all $\theta,\theta' \in \Theta$, the equality $\kernelreverse[]_{k+1\to k} (x|y,\theta)=\kernelreverse[]_{k+1\to k} (x|y,\theta')$. 
\end{assumption}
\noindent Adopting our definitions from Section~\ref{sec:master}, we let \begin{align}
T_{\mathrm{lower},\alt}(\epsilon) \in  \{&t \in  \I:  \TV(p_{t}^{\Sinit,\alt  },p_{t}^{\Send,\alt}) \leq \epsilon\} \label{eq:Tlower_alt}  \\
T_{\mathrm{upper},\alt}(\epsilon)\in  \{&t \in \I:\TV(p_{t}^{\Send,\alt}  ,p_{t}^{\Theta-\Send,\alt }) \geq 1-\epsilon^2\,\}.  \label{eq:Tupper_alt}
\end{align}
The main challenge of the below corollary is simply show that the final distribution $p^{\Theta,\alt}$ can be written as a mixture of $p^{\theta,\alt}$ with the same mixing weights as before.
\begin{restatable}{corollary}{corrollaryinterweaving}\label{corr:interweaving}
Under Assumption~\ref{equality_other_transitions}, for $\epsilon > 0$, if $\wh{T} \ge T_{\mathrm{lower},\alt}(\epsilon)$ and $\wh{T} \le T_{\mathrm{upper},\alt}(\epsilon)$, then  
\begin{equation}
\TV(\modrevlaw{\Sinit}{\wh{T}}{}, p^{\Send,\alt}) \leq \left(1+\sqrt{2}\max\left(1,\frac{\sum_{\theta \in \Theta-\Send} w_\theta}{\sum_{\theta \in \Send} w_\theta}\right)/2\right)  \epsilon.
\end{equation}
\end{restatable} 
\begin{proof}
We need only show that $p^{\Theta,\alt}\triangleq \sum_{\theta \in \Theta} w_\theta p^{\theta,\alt}$. It suffices to shows that the probability of generating a path $Y_m,Y_{m-1},\dots,Y_0$ are the same under both density functions. We need only consider  transitions for $k \in \I-A$, because for $k \in A$, the transitions are both given by the alternative distribution. For the transitions not given by $\alt$, note that we are using the original model, so 
\begin{align}
p^{\Theta,\alt}(Y_{k-1}|Y_k) = \frac{\sum_{\theta \in \Theta }w_\theta p^\theta(Y_k) \kernelreverse[]_{k\to k-1} (Y_{k-1}|Y_{k},\theta)}{\sum_{\theta \in \Theta}w_\theta p^\theta(Y_k)}.\label{eq:alt_all}
\end{align}
Furthermore, for the mixture model, this probability is 
\begin{align}
p^{\mathrm{mix}} = \frac{\sum_{\theta \in \Theta }w_\theta p^{\theta,\alt}(Y_m,Y_{m-1},\dots,Y_{k+1},Y_k) \kernelreverse[]_{k\to k-1} (Y_{k-1}|Y_{k},\theta)}{\sum_{\theta \in \Theta}w_\theta p^{\theta,\alt}(Y_m,Y_{m-1},\dots,Y_{k+1},Y_k)}.\label{eq:alt_mix_decompose}
\end{align}
The distinction between Equation~\ref{eq:alt_all} and Equation~\ref{eq:alt_mix_decompose} is that in the former we are using the likelihood of $p^\theta$ instead of   $p^{\theta,\alt}$. Thus it suffices to show that $p^\theta \propto  p^{\theta,\alt}$. We explicitly write out the probability,
\begin{align*}
p^\theta(Y_m,Y_{m-1},\dots,Y_{k+1},Y_k) = \prod_{i=k+1}^m p^{\theta}(Y_{i-1}|Y_i) \propto \prod_{i=k+1, i\notin A}^m p^{\theta}(Y_{i-1}|Y_i), 
\end{align*}
where the proportionality follows from the fact that we can ignore the probability of the transitions produced by $\alt$ under Assumption~\ref{equality_other_transitions}. By definition, this is proportional to $p^{\theta,\alt}(Y_m,Y_{m-1},\dots,Y_{k+1},Y_k)$ up to a normalization constant independent of $\theta$. 
\end{proof}

\subsubsection{In-context learning} \label{subsec:icl}
Now, we will specialize our framework to the case of in-context learning. As in \cite{xie2022explanationincontextlearningimplicit}, we assume that the language model is given inputs of the form $[x_1,y_1,o,x_2,y_2,o,\dots,x_t,y_t,o,x_{t+1}]$, where $x_1$ is the input, $y_1$ is the output, and $o$ is a delimiter token that separate different in-context samples from each other. We assume that the transitions $y_i \to o \to x_{i+1}$ are sampled by some alternative probability distribution $P_{\textrm{other}}(\cdot | \cdot)$. We require that  $P_{\textrm{other}}$ selects the $x_i$ i.i.d.
\begin{assumption}\label{assum:icl_iid}
The distribution of $P_{\textrm{other}}(x_{t+1}| x_1,y_1,o,x_2,y_2,o,\dots,x_t,y_t,o)=P_{\textrm{other}}(x_1)$.
\end{assumption}
\noindent Then we assume that the transitions $x_i \to y_i$ are generated by some $\theta^* \in \Theta$, which does not depend on any of the previous tokens before the delimiter. 
\begin{assumption}\label{assum:icl_well_specified} (Well-specification)
There exists some $\theta^* \in \Theta$ such that $y_i$ is generated from $y_i\sim P^{\leftarrow}(\cdot |[x_1,y_1,o_1,\dots,o_{i-1},x_i],\theta^*)$.
\end{assumption}
\begin{assumption}\label{assum:ind_about_to_delim}
For all $\theta \in \Theta$,  we have $ P^{\leftarrow}(\cdot |[x_1,y_1,o_1,\dots,o_{i-1},x_i],\theta)= P^{\leftarrow}(\cdot |x_i,\theta)$.  
\end{assumption}
\noindent We also assume statistical separation of $\theta^*$ from $\Theta-\{\theta^*\}$ in terms of Hellinger distance. 
\begin{assumption}\label{assum:icl_separation}
Let $p_{(x,y)}^S$ for $S \subset \Theta$ be the distribution of $(x_1,y_1,o)$ where $x_1 \sim P_{\textrm{other}}(\cdot |o)$ and $y_1 \sim p^S(\cdot |x_1)$. There exists $\delta>0$ such that $H^2(p_{(x,y)}^{\Theta-\{\theta^*\}},p_{(x,y)}^{\theta^*}) \geq \delta.$
\end{assumption}
\begin{restatable}{example}{corollaryicl}\label{ex:app:corrollary_icl}
Let $T\geq \ln\left(\frac{1-\delta/2}{\epsilon}\right)$. Under Assumptions~\ref{equality_other_transitions},~\ref{assum:icl_iid},~\ref{assum:icl_well_specified},~\ref{assum:ind_about_to_delim}, and~\ref{assum:icl_separation}, we have \[\TV([x_1,\dots,x_{T+1},y_{T+1}],[x_1,\dots,x_{T+1},\tilde{y}_{T+1}])\lesssim \epsilon/w_{\theta^*}.\]
\end{restatable}
\begin{proof}
It suffices to upper bound $1-\frac{1}{2}H^2(P^{\theta,\alt}_{3T},P^{\Theta-\{\theta\},\alt}_{3T})$ by $O(\epsilon)$. First observe that the distribution $p^{S,\alt}$ for $S \subset \Theta$ factors along the delimiters by a factor independent of $S$ using Assumptions~\ref{assum:icl_iid} and ~\ref{assum:ind_about_to_delim}, so we have
\[P^{S,\alt}_{3t}(x_1,y_1,o_1,x_2,y_2,o_2,\dots,x_t,y_t,o_t)\propto \prod_{i=1}^{t}P^{S,\alt}_3(x_i,y_i,o_i).\]
Using the tensorization property of Hellinger distance and our definition of $T,\delta$, we have
\begin{align*}
1-\frac{1}{2}H^2(P^{\theta,\alt}_{3T},P^{\Theta-\{\theta\},\alt}_{3T}) &\lesssim\left[1-\frac{1}{2}H^2(P^{\theta,\alt}_{3},P^{\Theta-\{\theta\},\alt}_{3})\right]^T \leq \epsilon. \qedhere
\end{align*}
\end{proof}

\section{Experiments}
\subsection{Jailbreak Experiments}
\subsubsection{Reproducing critical windows for jailbreaks from existing papers}

Existing work has already identified the presence of critical windows in the domain of jailbreaks. Here we present critical windows for a simplified prefill jailbreak based on the prefill attack \citep{prefillingattack} and repeating token jailbreak \citep{nasr2023scalableextractiontrainingdata} for \llamainstruct. In the first figure, we plot the probability of the model giving a harmful response, computed using the \texttt{StrongReject} Gemma 7b auditor from \citep{souly2024strongrejectjailbreaks}, as a function of the fraction of the phrase \texttt{Sure, here is how to} appended to the front of the model's generation. We can see that there is a large jump in the attack success rate after only including a few tokens in the prefix. The second figure is a reproduction of Figure 12 from \cite{nasr2023scalableextractiontrainingdata}. It shows that the probability of repeating the next token increases substantially as the first few tokens are included a few times in the context. 
\begin{figure}[H] 
    \centering
    \includegraphics[width=0.4\linewidth]{figures/prefill.png}
    \includegraphics[width=0.5\linewidth]{figures/repeating_token.png}
    \caption{Examples of critical windows in jailbreaks for ~\llamainstruct. \textbf{Left:} Critical window for a prefill jailbreak \citep{prefillingattack}. \textbf{Right:} Critical window for a repeating token jailbreak \citep{nasr2023scalableextractiontrainingdata}. }
    \label{fig:app:jailbreak_cw}
\end{figure}


\subsubsection{Experimental details from jailbreak}\label{app:jailbreak_dataset}
Now we apply our theory to develop a new jailbreak detection method, based on a likelihood ratio between an aligned and unaligned model. Intuitively, our theory states that when the unaligned component assigns a high probability to the text compared to the entire model, the model is likely to be jailbroken. We use a \llamainstruct~model jailbroken with LoRA to not refuse harmful prompts~\citep{llama_jailbroken} as a proxy for the unaligned model. We evaluate these different methods on a dataset of jailbreaks and benign prompts from \citep{bailey2024obfuscatedactivationsbypassllm}. 

\paragraph{Dataset.} We use the same dataset as~\citep{bailey2024obfuscatedactivationsbypassllm} but provide details here for completeness. The benign dataset consists of inputs from UltraChat~\citep{ding2023enhancingchatlanguagemodels}, a large dialogue dataset, and Xstest~\citep{rottger2024xstesttestsuiteidentifying}, which contains benign queries that are often incorrectly refused by language models. The benign queries are filtered to ensure that \llamainstruct~does not refuse any of them. The dataset of harmful prompts is based off of the Circuit Breakers dataset~\citep{zou2024improvingalignmentrobustnesscircuit}.  The datasets include the following jailbreaking methods from the extant literature: PAIR \citep{chao2023jailbreaking}, AutoDAN \citep{liu2024autodan}, Many-Shot Jailbreaking (MSJ) \citep{anil2024manyshot}, 
Multi-Turn Attacks \citep{li2024llmdefensesrobustmultiturn,haize2024mt}, Prefill, GCG \citep{zou2023universaltransferableadversarialattacks}, and other Misc. attakcs from \citep{wei2023jailbrokendoesllmsafety}.  For each jailbreaking method, it is applied to a prompt from the Circuit Breaker dataset and evaluated to see if the generation from \llamainstruct~is helpful and harmful, as determined by the StrongReject jailbreaking classifier~\citep{souly2024strongrejectjailbreaks}).

\paragraph{Evaluation Metric.} As is standard in the jailbreak detection literature \citep{bailey2024obfuscatedactivationsbypassllm}, we report the recall at the false positive rate at $0.01$. 


Table~\ref{tab:log_ratio_table} displays the recall and several other baselines. Crucially, the log likelihood ratio methods does obtain recall $>1$ for $5$ different categories of jailbreaks. While our methods do perform worse than existing methods, it is important to note that they still work and that their poor performance could be explained by the fact that we have to use a proxy for the unaligned mode of the model. 

\begin{table}[H]
\caption{Recall (\textrm{FPR}=0.01) for our  likelihood ratio threshold, a perplexity threshold ~\citep{alon2023detectinglanguagemodelattacks}, and a MLP-based detector trained on activations~\citep{bailey2024obfuscatedactivationsbypassllm} for predicting different jailbreaks. \textrm{prompt/gen} denote the logprobs of the prompt and generation, respectively.} 
\label{tab:log_ratio_table}
\centering
\begin{tabular}{l|ccccccc}
\toprule
& \textbf{AutoDAN}  & \textbf{GCG} & \textbf{Multi-Turn} & \textbf{Misc} & \textbf{MSJ} & \textbf{Pair} & \textbf{Prefill} \\
\midrule
$\log p^{\textrm{unaligned}}_{\textrm{prompt}}-\log p^{\textrm{aligned}}_{\textrm{prompt}}$ & 0.000  & 0.000 & 0.028 & 0.000 & 0.063 & 0.000 & 0.077 \\
$\log p^{\textrm{unaligned}}_{\textrm{gen}}-\log p^{\textrm{aligned}}_{\textrm{gen}}$ & 0.082  & 0.030 & 0.000 & 0.100 & 0.000 & 0.061 & 0.051 \\
$\log p^{\textrm{aligned}}_{\textrm{prompt}}$ & 0.000  & 0.576 & 0.056 & 0.063 & 0.013 & 0.000 & 0.077 \\
$\log p^{\textrm{aligned}}_{\textrm{gen}}$ & 0.205 & 0.150 & 0.570 & 0.200 & 0.006 & 0.015 & 0.416 \\
MLP  & 1.00 & 0.956 & 0.873 & 0.663  & 1.00 & 0.833 & 1.00 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Chain of thought experiments}\label{app:cot_experiment_details}
\subsubsection{Experimental details}
We describe the prompts we used in our evaluation of different datasets. The system prompt for all datasets is \texttt{Produce a correct solution to the following /TASK/ question.}, where \texttt{/TASK/} is the type of question of the dataset, i.e. science, math, or logic. For each question, we create a user prompt by appending \texttt{Think of the /TASK/ question thoroughly step by step. Please only respond with the answer after reasoning thoroughly.} in front of the question. Once the model completes its generation (max generation length set to $2048$ and default sampling parameters), we append the user prompt \texttt{Given all of the above, what’s the single, most likely answer? Your answer should have the format "The answer is ANSWER", where ANSWER is your answer.} for the multiple choice benchmarks and \texttt{Given all of the above, what’s the single, most likely answer? Simplify it completely. Your answer should have the format "The answer is \$ANSWER\$", where ANSWER is your answer in LaTeX.} Note that when we ask the model for the final answer, we set the temperature to $0$. \\

Across three different models (~\llamainstruct,~\phiinstruct, and~\qweninstruct), we find that conditioned on the occurrence of critical windows, the model generation is significantly less accurate compared to generations without critical windows. 

\begin{figure}[H] 
    \centering
    \includegraphics[width=0.4\linewidth]{figures/cw_llama.png}
    \includegraphics[width=0.4\linewidth]{figures/cw_phi.png}
    \includegraphics[width=0.4\linewidth]{figures/cw_qwen.png}
    \hfill
    \caption{Left-to-right: \llamainstruct,~\phiinstruct, and~\qweninstruct~barplots of original generation accuracy for generations with ($\geq0.5$ jump in probability) and without critical windows $(<0.5)$. }
    \label{fig:app:cw_bar}
\end{figure}



\begin{figure}[H] 
    \centering
    \includegraphics[width=13cm]{figures/phi_zz.png}
    \caption{Probability that the answer is the same as a function of the percentage of the generation remaining for different math and reasoning benchmarks for~\phiinstruct. Highlighted are generations with a 50\% increase and no 30\% decrease in the probability over one step. }
    \label{fig:app:phi_cw_benchmark}
\end{figure}

\begin{figure}[H] 
    \centering
    \includegraphics[width=14cm]{figures/qwen_zz.png}
    \caption{Probability that the answer is the same as a function of the percentage of the generation remaining for different math and reasoning benchmarks for~\qweninstruct. Highlighted are generations with a 50\% increase and no 30\% decrease in the probability over one step. }
    \label{fig:app:qwen_cw_benchmark}
\end{figure}





\subsubsection{Comparison with directly answering for~\llamainstruct}
It is interesting to compare our approach in Figure~\ref{fig:app:cw_bar} with that from~\cite{lanham2023measuringfaithfulnesschainofthoughtreasoning}, in which the model is directly asked for the answer (Figure~\ref{fig:app:cot_faithfulness}). We can see that directly answering has a lower consistency than resampling across different amounts of generation remaining. Interestingly, the discrepancy is much higher for the math-datasets (MATH, AQUA) compared to extremely small difference in consistency for the other reasoning datasets. This may be because it is difficult to \emph{extract} the answer from the middle of a chain of thought for math datasets than for other datasets. 

\begin{figure}[H] 
    \centering
    \includegraphics[width=0.5\linewidth]{figures/cot_faithfulness.png}
    \caption{Difference between resampling to obtain the answer versus directly asking the model for the answer (as in \citep{lanham2023measuringfaithfulnesschainofthoughtreasoning}) for \llamainstruct. }
    \label{fig:app:cot_faithfulness}
\end{figure}

\subsubsection{Examples of critical windows}\label{app:eg_critical_cot}
We provide a few more examples of critical windows (bolded). 

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=LogiQA example (\llamainstruct)]
\textbf{Q}: Recently, some countries in the world, especially the United States and Japan, have blamed their domestic economic mystery on China's fixed exchange rate system between the RMB and the US dollar. Some domestic manufacturing companies have caused pressure. The following are all questions about the above views, except? (A) Japan has been in recession for more than ten years. At that time, China's merchandise exports were very small, and the renminbi was very weak. (B) The US economic recession was mainly caused by the destruction of the myth of the Internet economy, and the "9.11" terrorist attacks exacerbated this recession. (C) China's GDP accounts for only 3.9 of the world's total, foreign trade is less than 5 of the world's, and it does not have the ability to affect the world market price and supply and demand. (D) Consumers in Western countries have benefited greatly from cheap goods in China.\\

\textbf{A}: The question is asking which of the following is NOT a view about the views that the countries have blamed their domestic economic mystery on China's fixed exchange rate system between the RMB and the US dollar. We can \textbf{analyze each choice one by one. Choices A, B, and D are all views that the countries have blamed their domestic economic mystery on China's fixed exchange rate system between} the RMB and the US dollar. Choice C is an argument that China's fixed exchange rate system between the RMB and the US dollar has no effect on the world market price and supply and demand. So, it is not a view about the views that the countries have blamed their domestic economic mystery on China's fixed exchange rate system between the RMB and the US dollar. Therefore, the correct answer is C.\\

\textbf{Correct answer}: D
\end{tcolorbox}


\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=MATH example (\llamainstruct)]
\textbf{Q}:  Let $a$ and $b$ be the solutions of the equation $2x^2-10x+5=0$. What is the value of $(2a-3)(4b-6)$?\\

\textbf{A}: To find the value of $(2a-3)(4b-6)$, we need to first find the values of $a$ and $b$ by solving the equation $2x^2-10x+5=0$. We can use the quadratic formula to solve for $a$ and $b$: $x = \frac{-b\pm\sqrt{b^2-4ac}}{2a}$. In this case, $a=2$, $b=-10$, and $c=5$. Plugging these values into the quadratic formula, we get: $x = \frac{-(-10)\pm\sqrt{(-10)^2-4(2)(5)}}{2(2)}$ $x = \frac{10\pm\sqrt{100-40}}{4}$ $x = \frac{10\pm\sqrt{60}}{4}$ $x = \frac{10\pm2\sqrt{15}}{4}$ So, the solutions are $a = \frac{10+2\sqrt{15}}{4}$ and $b = \frac{10-2\sqrt{15}}{4}$. Now that we have the values of $a$ and $b$, we can substitute them into the expression $(2a-3)(4b-6)$: $(2a-3)(4b-6) = (2(\frac{10+2\sqrt{15}}{4})-3)(4(\frac{10-2\sqrt{15}}{4})-6)$ Simplifying the expression, we get: $(2a-3)(4b-6) = (\frac{10+2\sqrt{15}}{2}-3)(\frac{10-2\sqrt{15}}{1}-6)$ $(2a-3)(4b-6) = (\frac{10+2\sqrt{15}-6}{2})(\frac{10-2\sqrt{15}-6}{1})$ $(2a-3)(4b-6) = (\frac{4+2\sqrt{15}}{2})(\frac{4-2\sqrt{15}}{1})$ \textbf{Now, we can simplify the expression further by multiplying the numerators and denominators:} $\mathbf{(2a-3)(4b-6) = \frac{(4+2\sqrt{15})(4-2\sqrt{15})}{2}}$ \textbf{Using the difference of squares formula} $\mathbf{(a+b)(a-b) = a^2-b^2}$\textbf{, we can simplify the numerator:} $\mathbf{(2a-3)(4b-6) = \frac{4^2-(2\sqrt{15})^2}{2}}$ $\mathbf{(2a-3)(4b-6) = \frac{16-60}{2}}$ Simplifying the expression further, we get: $(2a-3)(4b-6) = \frac{-44}{2}$ $(2a-3)(4b-6) = -22$ Therefore, the value of $(2a-3)(4b-6)$ is $-22$.\\

\textbf{Correct answer}: $-22$
\end{tcolorbox}


\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=ARC Easy example (\llamainstruct)]
\textbf{Q}:  Why are diseases transmitted through the bites of infected animals not considered contagious diseases? (A) The infected animals do not show symptoms. (B) The person in direct contact is the only one infected. (C) Humans do not become sick from animal diseases. (D) The diseases are treatable with antibiotics.\\

\textbf{A}: To answer this question, we need to understand the definitions of contagious and non-contagious diseases. Contagious diseases are those that are spread through direct or indirect contact with an infected person. Non-contagious diseases, on the other hand, are spread through other means, such as vectors like insects or animals. Step 1: Consider the definition of contagious diseases. Contagious diseases are spread through direct or indirect contact with an infected person. Step 2: Consider the definition of non-contagious diseases. Non-contagious diseases are spread through vectors like insects or animals. Step 3: Analyze the given options. Option A states that the infected animals do not show symptoms. This is not necessarily true, as some infected animals may show symptoms. Option B states that the person in direct\textbf{ contact is the only one infected, which is not a characteristic of contagious diseases. Option D states that the diseases are treatable with antibiotics, which is not relevant to the mode of transmission. Step 4: Based on the analysis, the correct} answer is the one that accurately describes the mode of transmission of the diseases. The final answer is: $\boxed{C}$. \\

\textbf{Correct answer}: (B)
\end{tcolorbox}


\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=AQUA-RAT example (\phiinstruct)]
\textbf{Q}: At a conference, one team is made up of 4 men and 4 women. Four presenters are chosen to present the team's findings in front of the entire conference. How many different groups of presenters can be chosen from the team if a team cannot be composed of men only or women only? (Two groups of presenters are considered different if at least one presenter is different.) A)120 B)19 C)180 D)420 E)460
\\

\textbf{A}: 1. There are 8 people in the team, 4 men and 4 women. 2. We need to choose 4 presenters for the conference, without having an all-male or all-female group. 3. We can choose 4 out of 8 using combinations: C(8,4) = 8! / (4! * (8-4)!) = 70 4. We'll subtract two groups that aren't allowed: all men and all women groups. So we'll subtract combinations of just men (4C4) and just women (4C4). So, we have: In all, we need to choose from combinations of (8C4 - 4C4 - 4C4) which equals: 70 (total combination for 4 people) - 1 (all men group) - 1 (all women group) = 68 5. However, we should also consider the mixed group of 2 men and 2 women which would be another prohibited combination. Since the two men and two women groups would be the same, we subtract: C(4,2) * C(4,2) = 6 6. So, finally, our calculation will be: 68 - \textbf{6 = 62. The correct answer among the choices we have isn't amongst those. There seems to be a mistake in the question or the given options. But close to the choices, an answer of 420 could seem interconnected if we consider combinations when men and women are mixed in all possible ways which is not the deducted understanding in our solution. Correct answer: E)}420 should be revised or further clarified in the question context. Otherwise, based on our understanding of how combinations are applied in scenarios like this, none of these options is quite correct. The answer is E)420.\\

\textbf{Correct answer}: B
\end{tcolorbox}
\end{figure}

\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=LogiQA example (\phiinstruct)]
\textbf{Q}: The person in charge of the relevant department said that pilots will be carried out in some areas this year to prepare for the system and policy preparations for the comprehensive clean-up of "small property houses", and require all localities to confirm the registration and certification of rural collective land. All small-property houses are not subject to registration and are not protected by law. Therefore, this newly built house in Hexi Village is not protected by law. Which of the following is true will most weaken the above argument. (A) This newly built house in Hexi Village has been tacitly approved by relevant departments. (B) This newly built house in Hexi Village is a small property house. (C) This newly built house in Hexi Village is built on rural collective land. (D) Some of the newly built houses in Hexi Village are not small property houses.\\

\textbf{A}: The statement implies that small property houses are not protected by law and can be demolished if necessary in the preparations for the system and policy preparations for the comprehensive clean-up. -We have to find the option that weakens this argument the most. Option A suggests that the department has tacitly approved the newly built house. This weakens the argument, \textbf{but it can still be argued that the house is subject to demolition if the comprehensive clean-up policy is implemented. Option B states that this newly built house in Hexi Village is a small property house. This directly contradicts the argument that all small-property houses are not protected by law }and not subject to registration and certification. Therefore, this house would not be demolished and would be protected by law. Option C says that the house is built on rural collective land. This doesn't disprove the argument – it might still be a small-property house without legal protection. Option D makes only some of the newly built houses in Hexi Village not small property houses. This doesn't address the argument that all small-property houses are not protected by law and not subject to registration, but only carries an exceptional circumstance. Thus, the option that most weakens the argument is (B) as it directly contradicts the statement by implying that the specific house is protected by law. The answer is B.\\

\textbf{Correct answer}: D
\end{tcolorbox}
\end{figure}

\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=MATH example (\qweninstruct)]
\textbf{Q}: Carson flips over the cards of a standard 52-card deck one at a time. What is the probability that he flips over the ace of spades before any face card (jack, queen or king)?
\\

\textbf{A}: 1. First, we need to identify the relevant cards for our problem. The ace of spades (A$\spadesuit$) and the face cards (J, Q, K) are the ones we're focusing on. There are 4 face cards in a standard deck of 52 cards (12 face cards total, but we only consider 4 for this problem). 2. There are 52 cards in total, and 5 of these (A$\spadesuit$, J$\spadesuit$, Q$\spadesuit$, K$\spadesuit$, J$\heartsuit$, Q$\heartsuit$, K$\heartsuit$, J$\diamondsuit
$, Q$\diamondsuit
$, K$\diamondsuit
$, J$\clubsuit
$, Q$\clubsuit
$, K$\clubsuit
$) are either the ace of spades or a face card. Therefore, there are 47 other cards that are neither the ace of spades nor face cards. 3. The problem can be simplified by considering the probability of the ace of spades coming before any face card in a sequence of drawing these 9 cards (the ace of spades and the \textbf{8 face cards). 4. We can think of the problem as a sequence of drawing 9 cards, where we are interested in the position of the ace of spades relative to the face cards. The ace of spades can come in any of the 9 positions, and we want to find the probability that it comes before any of the face cards. 5. For the ace of spades to come before any} face card, it must be in one of the first 5 positions (since there are 4 face cards, and the ace of spades must come before any of them). There are 9 positions in total, and 5 of these are favorable. 6. Therefore, the probability that the ace of spades comes before any face card is the ratio of the number of favorable outcomes to the total number of outcomes, which is \(\frac{5}{9}\). So, the probability that Carson flips over the ace of spades before any face card is \(\boxed{\frac{5}{9}}\).\\

\textbf{Correct answer}: $\frac{1}{13}$
\end{tcolorbox}
\end{figure}



\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=AQUA-RAT example (\qweninstruct)]
\textbf{Q}: ABCDE is a regular pentagon with F at its center. How many different quadrilaterals can be formed by joining 4 of the points A,B,C,D,E and F? A)12 B)10 C)5 D)15 E)20 \\

\textbf{A}: We need to form quadrilaterals by selecting 4 points from the 6 points (A, B, C, D, E, F) where F is the center of the regular pentagon. To form a quadrilateral, we need to select 4 points out of these 6. However, we need to ensure that the quadrilateral is valid. A key observation here is \textbf{that the center F must be included in the selection of 4 points because otherwise, we would only have points on the perimeter of the pentagon, which would not form a closed quadrilateral with a center point. So, we need to choose 3 points from the 5 points A}, B, C, D, E to form the quadrilateral along with F. The number of ways to choose 3 points from 5 is given by the combination formula \( \binom{5}{3} \): \[ \binom{5}{3} = \frac{5!}{3!(5-3)!} = \frac{5 \times 4}{2 \times 1} = 10 \] Thus, there are 10 different ways to choose 3 points from the 5 perimeter points, and each choice, along with the center F, forms a valid quadrilateral. Therefore, the correct answer is \( \boxed{10} \).\\

\textbf{Correct answer}: D
\end{tcolorbox}
\end{figure}

\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=MATH example (\phiinstruct)]
\textbf{Q}:  Square $ABCD$ has side lengths of 13 units. Point $E$ lies in the interior of the square such that $AE = 5$ units and $BE = 12$ units. What is the distance from $E$ to side $AD$?\\

\textbf{A}: 1. Since $ABCD$ is a square, we know that all sides are equal and the diagonals bisect each other at right angles. 2. Drawing a diagram, we can see that $\triangle ABE$ and $\triangle BDE$ are right triangles with right angles at point $B$. 3. We can use Pythagoras' theorem to find $DE$: $DE = \sqrt{BE^2 - AE^2} = \sqrt{12^2 - 5^2} = \sqrt{144 - 25} = \sqrt{119} \approx 10.91$ units. 4. \textbf{Since $ABCD$ is a square, $AD = 13$ units. So, the distance from $E$ to $AD$ is $13 - AE = 13 - 5 =$} $8$ units. The distance from $E$ to side $AD$ is 8 units. The answer is $8$.\\

\textbf{Correct answer}: $\frac{25}{13}$
\end{tcolorbox}
\end{figure}



\begin{figure}[H]
\centering
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=MMLU example (\qweninstruct)]
\textbf{Q}: A store owner noticed that an expensive electronic device was missing from the store. He assumed that a shoplifter had taken it. Later that day, the store owner was cleaning up the small room in the back of the store that the employees of the store used to store their personal items while they were at work. The store owner was straightening up the coat rack in the back room, when he accidentally knocked one of the employee's coats off the hanger. As the coat fell onto the floor, the missing electronic device fell out of the pocket of the coat. It was still in its original packaging, and the store owner checked the serial number on the package to confirm that it was the device missing from the store's shelves. The store owner knew that the coat belonged to a woman who worked as a cashier in the store. The store owner felt bad, but decided that he had no choice but to call the police. Two police officers arrived at the store. They gathered information from the store owner, who signed a criminal complaint against the woman. The officers then discussed the matter with the woman. Although she denied stealing anything, the officers arrested her and informed her that they would need to take her to the police station. After the police took the woman away, another employee of the store found out that the woman had been arrested. That employee immediately went to the store owner and admitted that he was actually the one who had stolen the device, and that he had hidden it in the woman's coat, intending to retrieve it later before the end of the woman's work shift. The store owner promptly called the police and told them that there had been a mistake and that the theft charge against the woman was invalid. The police released the woman, who spent a total of approximately 30 minutes in police custody. If the woman asserts a claim for false imprisonment against the arresting officers, will the woman prevail? (A) Yes, because the woman had not committed a crime at the time she was arrested. (B) Yes, because the woman was arrested and held in custody. (C) No, because the woman was detained for only 30 minutes. (D) No, because the officers had probable cause to arrest the woman based on the store owner's complaint and honest belief that the woman had stolen the device.\\

\textbf{A}: 1) The key issue here is whether the woman's arrest was lawful. 2) For an arrest to be lawful, there must be probable cause to believe the person committed a crime. 3) The store owner believed the woman had stolen the device based on the device falling out of her coat. \textbf{4) However, it was later revealed that the device was actually planted in her coat by another employee. 5) This means the store owner's belief was mistaken, and there was no probable cause for the arrest.} 6) False imprisonment occurs when a person is unlawfully detained. 7) The arrest was unlawful because it was based on a false belief. 8) Therefore, the woman's claim for false imprisonment is valid because she was unlawfully detained. Given these points, the correct answer is: (A) Yes, because the woman had not committed a crime at the time she was arrested. This answer is correct because the arrest was based on a mistaken belief and there was no probable cause, which means the arrest was unlawful, and the woman was falsely imprisoned. The answer is (A).\\

\textbf{Correct answer}: D
\end{tcolorbox}
\end{figure}


\subsubsection{Varied temperature sampling}\label{app:temp_samp_cot}

In our above experiments with critical windows, we have been generating model responses at their default sampling temperatures: \llamainstruct: $0.6$, \phiinstruct: $1.0$, and \qweninstruct: $0.7$. A natural question to ask is whether the sampling temperature affects the presence or strength of critical windows across these models. At one extreme, sampling at infinite temperature exhibits no critical windows since each token is generated uniformly at random. At the other extreme, zero temperature sampling is deterministic, so there is only one mode of behavior and critical windows again cannot exist. At intermediate temperatures, decreasing sampling temperature reduces token variance, so the sharpness of transition from general mixture to target submixture can be heightened. At the same time, reduced sampling variance also reduces the likelihood of transitioning between modes as well, leaving the overall effect on critical windows unclear. 


First, we report the frequency of critical windows across the different temperatures for the MATH dataset: $0.1, 0.4, 0.7, $ and $1.0$. Then, we report the average size of a critical window on $400$ samples from MATH, running the forward-reverse experiment on each sample $25$ times and taking the max consecutive jump as the critical window size for that problem. From Figures \ref{fig:app:temp_cw} and \ref{fig:app:temp}, each model has its own relationship between temperature and critical window frequency and size, with no clear definite relationship across all models.

\begin{figure}[H] 
    \centering
    \includegraphics[width=6cm]{figures/llama_temp_cw.png}
    \includegraphics[width=6cm]{figures/phi_temp_cw.png}
    \includegraphics[width=6cm]{figures/qwen_temp_cw.png}
    \caption{Temperature versus critical window frequency for ~\llamainstruct,~\phiinstruct, and~\qweninstruct.}
    \label{fig:app:temp_cw}
\end{figure}



\begin{figure}[H] 
    \centering
    \includegraphics[width=6cm]{figures/llama_temp.png}
    \includegraphics[width=6cm]{figures/phi_temp.png}
    \includegraphics[width=6cm]{figures/qwen_temp.png}
    \caption{Temperature versus mean critical window size for ~\llamainstruct,~\phiinstruct, and~\qweninstruct.}
    \label{fig:app:temp}
\end{figure}
 