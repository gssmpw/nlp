\section{Hierarchies in stochastic localization samplers} \label{sec:hierarchy}

Herein we propose a theory of hierarchical sampling within our critical windows framework. 
It is motivated by the observation that a single trajectory can contain multiple critical windows (Figure~\ref{fig:app:structured_llama}), each splitting a sub-population into smaller sub-populations. This hierarchy is naturally represented as a tree: the root signifies that all sub-populations are indistinguishable under enough noise, while the leafs represent distinct modes in $p$. A path from the root to a leaf captures the progressive refinement of the original distribution $p$ into increasingly specific components. To formalize this, we introduce the concept of an $\epsilon$-mixture tree, which decomposes $p$ into a hierarchical structure. 

\begin{definition}\label{def:hierarchy:tree}
For an error term $\epsilon>0$ and mixture model $p$,  an \textbf{$\epsilon$-mixture tree} is a tuple $$(T,\{\kernelforward(\cdot  |\cdot)\},\I,\Theta,\{p^\theta\}_{\theta \in \Theta},\text{\subsetfunc},\text{\noiseamount}).$$ The tree $T=(V,E)$ is associated with a function \subsetfunc$:V\to 2^{\Theta}\backslash\{\emptyset\}$, which maps vertices to sub-mixtures. We require $\subsetfunc$ satisfies the following two properties: (1) \subsetfunc$(\textrm{root})=\Theta$; (2) If $u$ is a parent of $v$, $\subsetfunc(v) \subset \subsetfunc(u)$. We consider a \noiseamount$:V \to \mathbb{R}^{ \geq 0}$, which characterizes the noise levels that result in the aggregations of mixture components described by vertices in the mixture tree. $\text{\noiseamount}(u)$ is defined such that all $p^\theta_{\text{\noiseamount}(u)}$ for $\theta \in \text{\subsetfunc}(u)$ overlap greatly and for $p^{\text{\subsetfunc}(u)}_{\text{\noiseamount}(u)},p^{\Theta-\text{\subsetfunc}(u)}_{\text{\noiseamount}(u)}$ have negligible overlap. Thus we require that $\noiseamount$ satisfy three properties: (1) For distinct $\theta_i,\theta_j \in \Theta$ with leaf nodes $w,v$ such that $\theta_i \in \subsetfunc(w), \theta_j \in \subsetfunc(v)$, if $u$ is the lowest common ancestor of $w,v$, then we require $\TV(p^{\theta_i}_{\noiseamount(u)},p^{\theta_j}_{\noiseamount(u)}) \leq \epsilon$; (2) For $u \in V$, we have statistical separation between $\subsetfunc(u)$ and $\Theta-\subsetfunc(u)$ in terms of $\TV$, $\TV(p^{\subsetfunc(u)}_{\noiseamount(u)},p^{\Theta-\subsetfunc(u)}_{\noiseamount(u)}) \geq 1-\epsilon^2;$ and (3) If $v \in V$ is a parent of $u$, we have $\noiseamount(u) < \noiseamount(v)$. Property $1$ establishes bounds on $\Tlower$, and properties $2$ and $3$ establishes bounds on $\Tupper$.
\end{definition}

We emphasize that this framework is highly general, solely defined with the \emph{initial distribution $p$} and \emph{the forward process}. It strictly expands the definition in \citep{li2024criticalwindowsnonasymptotictheory}, which focused on hierarchies of isotropic Gaussians, to all localization-based samplers and mixture models. We can also relate it to the sequences of critical windows we observe in Figure~\ref{fig:app:structured_llama}, capturing the idea that each critical window represents the refinement into smaller subpopulations of $p$.   
\begin{restatable}{corollary}{corrollaryhierarchy}\label{corr:hierarchy_example}\ Consider an $\epsilon$-mixture tree. For $\theta_i \in \Theta$, consider the path $u_1,u_2,u_3,\dots,u_{H'} \in V$ where $u_1$ is the leaf node with $\theta_i \in \text{\subsetfunc}(u_1)$ and $u_{H'}$ is the root. There is a sequence of times $T_1<T_{2}<\dots<T_{H'}$ with $\TV(\modrevlaw{\{i\}}{T_\ell}{}, p^{\textrm{\subsetfunc}(u_\ell)}) \lesssim_{w} \epsilon$.
\end{restatable} 
\begin{proof}
For $\ell \in [H']$, we let $T_\ell = \noiseamount(u_\ell)$. We apply Theorem~\ref{thm:masters_theorem} with $\Sinit=\{i\}$ and $\Send=\subsetfunc(u_\ell)$. We know $\TV(p^{\Send}_{T_\ell},p^{\Theta-\Send}_{T_\ell}) \geq 1-\epsilon^2$ by Condition $2$ in Definition~\ref{def:hierarchy:tree}. By Lemma 15 of \citep{li2024criticalwindowsnonasymptotictheory}, we know $\TV(p^{\{i\}}_{T_\ell},p^{\Sinit}_{T_\ell}) \leq \max_{j \in \Sinit} \TV(p^{\{i\}}_{T_\ell},p^{\{j\}}_{T_\ell})$. This is $\leq \epsilon$ for all $j \in \Sinit$ by Condition 3 on $\noiseamount$ and the data processing inequality. 
\end{proof}

We first observe that the hierarchy of two samplers with the same forward process are identical if the samplers agree on sub-populations. Assume we have $\{p^\theta\}_{\theta \in \Theta}$ (e.g. the true distribution) and $\{q^\theta\}_{\theta \in \Theta}$ (e.g. a generative model), where $q^\theta \approx p^\theta$ across all $\theta \in \Theta$ with the same $\{w_\theta\}_{\theta \in \Theta}$. 
\begin{restatable}{corollary}{corollarylearnsample}\label{corr:learn_sample}
Consider an $\epsilon$-mixture tree $(T,\{\kernelforward(\cdot  |\cdot)\},\I,\Theta,\{p^\theta\}_{\theta \in \Theta},\text{\subsetfunc},\text{\noiseamount})$. Suppose we have another distribution $\{q^\theta\}_{\theta \in \Theta}$ such that $\TV(p^\theta,q^\theta) \leq \delta/2$ for all $\theta \in \Theta$. Then we have $\epsilon+\sqrt{\delta}$-mixture tree given by $(T,\{\kernelforward(\cdot  |\cdot)\},\I,\Theta,\{q^\theta\}_{\theta \in \Theta},\text{\subsetfunc},\text{\noiseamount})$.
\end{restatable}
\begin{proof}
We need only check the first and second properties of $\noiseamount$ with parameter $\epsilon+\sqrt{\delta}$. To do this, it suffices to show $\TV(q^{\theta_i}_{\noiseamount(u)},q^{\theta_j}_{\noiseamount(v)}) \leq \epsilon+\delta$ and $\TV(q^{\subsetfunc(u)}_{\noiseamount(u)},q^{\Theta-\subsetfunc(u)}_{\noiseamount(u)}) \geq 1-\epsilon^2-\delta.$ By the data processing inequality, we just need to show this at $t=0$, and we prove the stronger statement that for $S_1 \subset \Theta$, $\TV(p^{S_1},q^{S_1}) \leq \delta/2$. This follows from Lemma 15 of \citep{li2024criticalwindowsnonasymptotictheory} and $\TV(p^\theta,q^\theta) \leq \delta/2$ for all $\theta \in \Theta$. 
\end{proof}

This similarity does not hold generally, if the generative model does not have the same forward process the data generating procedure. In fact, we can define arbitrary hierarchies by designing an appropriate forward process. 
\begin{restatable}{example}{examplearhierarchy}\label{ex:autoregressive_hierarchy}
Consider a set of alphabets $\{\mathcal{A}_i\}_{i=1}^d$ and define $\Theta=\{(a_i)_{i=1}^d: \forall i \in [d], a_i \in \mathcal{A}_i \}$ and $p^{\theta_i} = \delta_{\theta_i}$. 
Let $\I=[0,1,2,\dots,d]$. and for any permutation $i_1,i_2,\dots,i_d$ of $[d]$, define a forward process such that at $t \in \I$, we mask all $i_d,i_{d-1},\dots,i_{d-t}$. This constructs a hierarchy where the values for $i_1,i_2,\dots,i_d$ are decided in that order. \end{restatable} 

\begin{proof}
We construct the following $0$-mixture tree as follows. We let the leaf nodes be the set $\Theta$. We let two leaf nodes $u,v$ have the same parent if and only if they share the same values on the alphabet at $i_1,i_2,\dots,i_{d-1}$; we also define the parent as the union of all of its children. We now treat the parents we constructed as the roots, and let them have the same parent if and only if  they share the same values on the tuple $i_1,i_2,\dots,i_{d-2}$. We continue to do this until we are left with one root node. We let $\subsetfunc$ map each node to the corresponding set and $\noiseamount$ map each node to its distance from a leaf node.

By the construction of $T$, it is clear that $\subsetfunc$ satisfies the desired properties. For distinct $\theta_i,\theta_j \in \Theta$, the lowest common ancestor of $\theta_i,\theta_j$ represents the largest $k$ such that indices $i_1,\dots,i_k$ are the same for $\theta_i,\theta_j$. Because $p^{\theta_i}_{\noiseamount(u)}$ is just the tuple of the values of $\theta_i,\theta_j$ at $i_1,\dots,i_k$, we know $\TV(p^{\theta_i}_{\noiseamount(u)},p^{\theta_j}_{\noiseamount(u)}) = 0 $. For any $u \in V$ representing the values at index $(i_\ell)_{\ell=1}^k$, all $\theta \notin \subsetfunc(u)$ does not share the same values at these indices by definition, so we also know \begin{align}\TV(p^{\subsetfunc(u)}_{\noiseamount(u)},p^{\Theta-\subsetfunc(u)}_{\noiseamount(u)}) &=1\,.\qedhere\end{align}
\end{proof}
Finally, we note that hierarchies of diffusions are generally shallower than hierarchies for autoregressive models. The hierarchy for a mixture of Gaussians cannot grow linearly with the dimension $d$, e.g. it is $O(1)$ in Example~\ref{example:gaussianrandommeans} for mixtures of Gaussians with randomly selected means or $O(\sqrt{\log d})$ in the hierarchy of Gausssians in~\citep{li2024criticalwindowsnonasymptotictheory}. This is because the forward process simultaneously contracts all distances with the same dependence on $d$ together at the same time. However, in contrast, depth can scale linearly with the context length for autoregressive models, (Example~\ref{ex:autoregressive_hierarchy} or Figure~\ref{fig:app:structured_llama}). We speculate that this could mean autoregressive models can learn more complex feature hierarchies than diffusions.