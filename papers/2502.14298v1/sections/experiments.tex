\section{Experimental Results}
\label{sec:exp}

\begin{comment}
\begin{table*}[]
    \centering
    \begin{tabular}{lccccccccc}
    \toprule
    \multirow{3}{*}{Dataset}  & \multicolumn{4}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} & \phantom{}& \multicolumn{4}{c}{Prior variance $\sigma_p^2 =\frac{1}{9}$} \\
    \cmidrule{2-5} \cmidrule{7-10}
       & \multicolumn{2}{c}{Bayes posterior} & \multicolumn{2}{c}{Robust posterior} & \phantom{}& \multicolumn{2}{c}{Bayes posterior} & \multicolumn{2}{c}{Robust posterior} \\
       & NLL & adv-NLL & NLL & adv-NLL  & \phantom{}& NLL & adv-NLL & NLL & adv-NLL  \\
       \midrule
       Wine Quality & $1.1986$ & $1.2305$ & $1.1970$ & $1.2250$  & \phantom{}& $1.2789$ & $1.3181$ & $1.2749$ & $1.3082$ \\
       Energy Efficiency & $0.9736$ & $0.9902$ & $0.9734$ & $0.9896$  & \phantom{}& $0.9621$ & $0.9872$ & $0.9655$ & $0.9850$ \\
       Air Quality & $0.9684$ & $0.9817$ & $0.9687$ & $0.9804$  & \phantom{}& $0.9634$ & $0.9793$ & $0.9637$ & $0.9759$ \\
       Air Foil & $1.1631$ & $1.2106$ & $1.1662$ & $1.2099$  &\phantom{}&  $1.1304$ & $1.1798$ & $1.1347$ & $1.1787$ \\
       Auto MPG & $1.0225$ & $1.0402$ & $1.0223$ & $1.0393$  & \phantom{}& $1.0291$ & $1.0618$ & $1.0297$ & $1.0536$ \\
       Abalone & $1.1889$ & $1.2440$ & $1.2051$ & $1.2433$  & \phantom{}& $1.1566$ & $1.2597$ & $1.1627$ & $1.2101$ \\
       \bottomrule
    \end{tabular}
    \caption{Robust posterior trained with $\delta=0.1$ and the testing time adversarial perturbation used in NLL is NLL: $\deltat=0.1$.
    $\ell(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \|Y - X\theta\|^2.$ 
    adv-NLL: $ {\ell}_\delta(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \Big\| |Y - X\theta| + \delta \|\theta\| 1_n \Big\|^2.$
    }
    \label{tab:my_label}
\end{table*}
\end{comment}

\begin{table*}[t]
    \centering
    \begin{tabular}{lccccc}
    \toprule
    % \multirow{3}{*}{Dataset}  & \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} \\
    % \cmidrule{2-6}
    \multirow{2}{*}{Dataset}  %& \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} \\
       & \multicolumn{2}{c}{Standard generalization (NLL) $\ell$} & \phantom{}& \multicolumn{2}{c}{Adversarial generalization (adv-NLL) $\ell_{\deltat}$} \\
       & Bayes posterior $q$  & Robust posterior  $q_{\delta}$ & \phantom{}& Bayes posterior $q$ & Robust posterior $q_{\delta}$  \\
       \midrule
       Abalone & \textbf{1.1586} \tiny{$\pm$ 0.013} & 1.1729 \tiny{$\pm$ 0.015} && 1.2539 \tiny{$\pm$ 0.012} & \textbf{1.2178} \tiny{$\pm$ 0.015} \\
       Air Foil & \textbf{1.1656} \tiny{$\pm$ 0.008} & 1.1690 \tiny{$\pm$ 0.008} && 1.2194 \tiny{$\pm$ 0.008} & \textbf{1.2175} \tiny{$\pm$ 0.008} \\
       Air Quality & \textbf{0.9665} \tiny{$\pm$ 0.002} & 0.9670 \tiny{$\pm$ 0.002} && 0.9826 \tiny{$\pm$ 0.003} & \textbf{0.9792} \tiny{$\pm$ 0.002} \\
       Auto MPG & 1.0231 \tiny{$\pm$ 0.006} & \textbf{1.0228} \tiny{$\pm$ 0.007} && 1.0552 \tiny{$\pm$ 0.006} & \textbf{1.0469} \tiny{$\pm$ 0.008} \\
       California Housing & \textbf{1.1193} \tiny{$\pm$ 0.003} & 1.1267 \tiny{$\pm$ 0.005} && 1.1910 \tiny{$\pm$ 0.003} & \textbf{1.1769} \tiny{$\pm$ 0.005} \\
       Energy Efficiency & \textbf{0.9709} \tiny{$\pm$ 0.006} & 0.9731 \tiny{$\pm$ 0.007} && 0.9996 \tiny{$\pm$ 0.007} & \textbf{0.9945} \tiny{$\pm$ 0.008} \\
       Wine Quality & 1.2339 \tiny{$\pm$ 0.007} & \textbf{1.2322} \tiny{$\pm$ 0.006} && 1.2696 \tiny{$\pm$ 0.008} & \textbf{1.2627} \tiny{$\pm$ 0.006} \\
       \bottomrule
    \end{tabular}
    \caption{%Standard and adversarial generalization 
    Test NLL and adversarial NLL of Bayes and robust posteriors on real datasets. The prior variance is set to $\sigma_p^2 = \frac{1}{d}$. The robust posterior is trained with $\delta=0.1$, and adversarial generalization is evaluated using the same training-time perturbation ($\deltat=0.1$). The adversarial generalization results demonstrate that the robust posterior $q_\delta$ is consistently more robust than the Bayes posterior $q$. For both standard and adversarial generalization, the best-performing model is bold.
    %$\ell(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \|Y - X\theta\|^2.$  adv-NLL: $ {\ell}_\delta(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \Big\| |Y - X\theta| + \delta \|\theta\| 1_n \Big\|^2.$
       \label{tab:real_data} }
       \vspace{-0.3cm}
\end{table*}

In this section, we present $(i)$ adversarial robustness of Bayes and robust posteriors on real datasets; $(ii)$ validation of the derived generalization certificates for the posteriors, and compare it to the prior work of \citep{germain2016pac}.
%We validate our derived certificates and conduct ablation studies in Bayesian linear regression setting. 
Code to reproduce all experiments is provided~\footnote{\url{https://figshare.com/s/dc9034bb2e323a87a7a4}}, and implementation and hardware details are given in \Cref{app:exp}.

\paragraph{Datasets and hyperparameters} %To evaluate the adversarial robustness of the posteriors, 
We consider the following regression datasets with $70-30$ train-test split: \emph{Abalone} \citep{abalone}, \emph{Air Foil} \citep{airfoil}, \emph{Air Quality} \citep{air_quality}, \emph{Auto MPG} \citep{auto_mpg}, \emph{California Housing} \citep{pedregosa2011cali}, \emph{Energy Efficiency} \citep{energy_efficiency}, \emph{Wine Quality} \citep{wine_quality}. The datasets are standardized to zero mean and unit variance. 
% Cheng Soon: I'm trying to save one line. :-/
%The complete statistics of the datasets are provided in \Cref{app:exp}. 
We provide results for prior variance $\sigma_p^2 = \{\frac{1}{100}, \frac{1}{9}, \frac{1}{d}\}$ where $d$ is the data feature dimension. % in the main draft and $\sigma_p^2 = \frac{1}{9}$ in \Cref{app:exp_real}.
%
For certificate validation, we use a synthetic dataset where the data features $x \sim \mathcal{N}(0, \sigma_x^2 I_d)$ and $y = \theta^* x^\top + \epsilon$  with $d=5$ and $\epsilon \sim \mathcal{N}(0, \sigma^2)$ with $\sigma^2 = \frac{1}{9}$ and $\| \theta^* \|^2 = 0.5$. We fix prior variance $\sigma_p^2 = 0.01$ and consider a range of training samples from $10$ to $10^4$ and $10^4$ test samples. 
All results are averaged over 5 seeds and reported with standard deviation.
 %Unlike the Bayes posterior, the robust posterior (Gibbs posterior) lacks a closed-form expression due to the intractability of its normalizing constant. To address this, 
We employ Hamiltonian Monte Carlo (HMC) to efficiently sample from the posterior distribution. Specifically, we utilize the No-U-Turn Sampler (NUTS) \citep{hoffman2014no}, an adaptive variant of HMC that automatically tunes step sizes and trajectory lengths for improved sampling efficiency. 
%We measure NLL and adversarial NLL with $\deltat$ perturbation radius to evaluate the performance of Bayes and robust posteriors.


\begin{figure*}[t]
    \centering\vspace{-0.1cm}
    \includegraphics[width=0.24\linewidth]{figures/germain_d5_nsplit10_0.3.pdf}
    \includegraphics[width=0.24\linewidth]{figures/avg_std_bound_d5_nsplit10_0.3_2.pdf}
    \includegraphics[width=0.24\linewidth]{figures/avg_std_bound_d5_nsplit10_0.3_3.pdf}
    \includegraphics[width=0.24\linewidth]{figures/avg_std_bound_d5_nsplit10_0.3_4.pdf}
    \caption{Validation of the derived generalization certificates \Cref{thm:std_post_std_loss,thm:std_post_adv_loss,thm:adv_post_std_loss,thm:adv_post_adv_loss}. (left to right) Standard generalization of Bayes posterior with comparison to prior work \citet{germain2016pac}, standard generalization of the robust posterior, adversarial generalization of Bayes posterior, and adversarial generalization of the robust posterior. %The training loss of the posterior, empirical test of standard/adversarial NLL, and the corresponding PAC-Bayesian certificate are plotted in each case.
    \label{fig:pac_bounds}}
    \vspace{-0.3cm}
\end{figure*}

\subsection{Results on real data}

We present the standard and adversarial generalization results for Bayes and robust posteriors evaluated on real data  in \Cref{tab:real_data}, using a prior variance of $\sigma_p^2 = \frac{1}{d}$. Additional results for $\sigma_p^2 = \{\frac{1}{100}, \frac{1}{9}\}$ are in \Cref{app:exp_real}. 
Our findings clearly demonstrate that \emph{the robust posterior $q_\delta$ consistently outperforms the standard Bayes posterior $q$ in terms of adversarial robustness}. Moreover, for certain choices of prior, the robust posterior also achieves superior standard generalization (see \Cref{tab:real_data,tab:real_data_p9,tab:real_data_p100}). %This suggests that adversarial robustness in Bayesian models can be enhanced through probabilistic inference and may not necessarily suffer from the severe robustness-accuracy trade-off commonly observed in non-Bayesian models \citep{tsipras2019robustness}.
This suggests that adversarial robustness in Bayesian models can potentially be enhanced through probabilistic inference and may not always be subject to the severe robustness-accuracy trade-off commonly observed in non-Bayesian models \citep{tsipras2019robustness}.

\subsection{Validation of certificates} 
We validate our derived generalization certificates in \Cref{fig:pac_bounds} by plotting the PAC-Bayesian bounds as a function of the number of training samples. Since these bounds provide rigorous upper estimates on the generalization error, they are not directly comparable to each other but rather serve as theoretical guarantees. While the bounds may appear conservative, it is important to note that these are the first rigorous PAC-Bayesian bounds for adversarial robustness.
Additionally, we compare our standard generalization bound for the Bayes posterior with the prior work \citep{germain2016pac} in \Cref{fig:pac_bounds} (left). Although both approaches leverage PAC-Bayesian principles, the bound from \citep{germain2016pac} is numerically lower than ours because they approximate the expected training loss in  \Cref{th:pac_bayes_bounded_cgf} using the empirical loss, while we compute the actual expected training loss. 

\begin{comment}
\begin{figure}[h]
    \centering
    \vspace{-0.5cm}
    \includegraphics[width=0.8\linewidth]{figures/germain_d5_nsplit10_0.3.pdf}
    \caption{Comparison to \citep{germain2016pac}
    \label{fig:comp_germain}}
\end{figure}
\end{comment}