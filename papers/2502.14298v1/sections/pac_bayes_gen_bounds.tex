\section{Standard and Adversarial Generalization Certificates}
\label{sec:pac_bayes_certs}
In this section, we focus on Bayesian linear regression (i.e. a robustified squared error loss or Gaussian NLL) for the robust posterior in~\Cref{cor:robust_posterior}.
We consider labels generated using a true parameter $\theta^*$,  $y_i=x_i^\top \theta^* + \epsilon_i$ where $\mathbb{E}[x_i] = 0, \, \mathbb{E}[\|x_i\|^2] = \sigma_x^2$ and $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$. %Recall that $X \in \mathbb{R}^{n\times d}$ is the feature matrix, and $Y \in \mathbb{R}^n$ is the label vector. 

\paragraph{PAC-Bayesian generalization certificates} 
Unlike traditional generalization bounds based on uniform convergence such as VC-dimension \citep{vapnik1971uniform}, Rademacher complexity \citep{shalev2014understanding}, and information-theory \citep{zhang2006information}, PAC-Bayes \citep{mcallester1998some} focuses on Bayesian predictors %, which make predictions by drawing parameters from a learned posterior distribution 
rather than a single deterministic hypothesis class. This perspective allows PAC-Bayes to provide \emph{data-dependent} generalization guarantees, 
%PAC-Bayesian theorems provide data-driven generalization bounds 
that are computed on training samples without relying on the test data. 
As such, all certificates computed in this section depend on the data $X$, $Y$ in a non-obvious way.
%Furthermore, the guarantees are uniformly valid for all $\theta \sim \rho$, %\russ{something wrong with this notation here $\rho(\theta)$ is a nonnegative number, not a set}.
%thus, providing numerical certificates on the generalization capabilities of the model. 
%To derive PAC-Bayesian bounds, we assume the prior to be Gaussian distribution $\theta \sim \mathcal{N}(0, \sigma^2_p I)$ where $I$ is identity matrix.
While other approaches based on uniform convergence or information theory result in a worst-case guarantee, PAC-Bayesian offers fine-grained analysis by taking advantage of informed prior choice leading to a tighter certificate. 
%only in an estimate of the generalization, and not certificates. 
%\russ{Is this related to ``oracle'' versus ``empirical'' bounds, or is that something different?}
%The PAC-Bayes bounds provide certificates unlike the others which only give an estimate of the generalization. Others include Rademacher, uniform convergence, VC theory..

\paragraph{Data and constants in bounds} In addition to the data $X, Y$, each of the bounds in this section also rely on constants such as the training and testing perturbation allowances $\delta$ and $\deltat$.
We intuitively describe the role of these data and constant terms following the presentation of each of the bounds, even though the main purpose of the bounds is as computable data-dependent certificates.

\paragraph{Standard and adversarial generalization}
We derive certificates for both standard and adversarial generalization (columns in \Cref{tab:overview}). In order to formalize this, let us define the expected and empirical errors for standard loss $\ell$ as $R(\theta)=\expectation{(x,y) \sim \mathcal{P}}{\ell\big(\theta, (x,y)\big)}$ and $r(\theta)=\frac{1}{n} \mathcal{L}(\theta, \mathcal{D})$, respectively. Similarly, the expected and empirical adversarial errors under perturbation $\delta$ are defined as $ {R}_\delta(\theta)=\expectation{(x,y) \sim \mathcal{P}}{ {\ell}_\delta\big(\theta, (x,y)\big)}$ and $ {r}_\delta(\theta)=\frac{1}{n}  \mathcal{L}_\delta(\theta, \mathcal{D})$, respectively. 
In the context of Bayesian inference, the standard generalization risk certificate quantifies the expected loss of the posterior $\rho(\theta)$ on unperturbed test data $x$: $\expectation{\theta \sim \rho}{R(\theta)}$. %, i.e., how well the posterior $\rho$ performs on non-perturbed test data in the worst-case. 
Similarly, the adversarial generalization certificate quantifies the performance
%on adversarially perturbed test data:
of the test data under adversarial perturbation $\deltat$: % is obtained by bounding the error on adversarially perturbed test data, i.e., 
$\expectation{\theta \sim \rho}{ {R}_{\deltat}(\theta)}$. %, i.e., how well the posterior $\rho$ performs on adversarially perturbed test data. 
% We derive the bounds on these quantities (thereby certificates) considering both standard and adversarially robust posteriors, $q(\theta)$ and $ {q}(\theta)$, respectively.
We derive the certificates by bounding the respective quantity for both the standard Bayes posterior $q(\theta)$ and adversarially robust posterior $ {q}_\delta(\theta)$.
Note that we allow the perturbation $\delta$ used for inference (i.e. calculation of $ {q}_\delta(\theta)$) to be different to the perturbation $\deltat$ at test-time.

\subsection{Cumulant generating function}% of losses}

To derive the standard and adversarial generalization certificates, we leverage the PAC-Bayesian theorem for any loss with bounded cumulant generating function (CGF) in \citet{banerjee2021information}. 
We state the result in \Cref{th:pac_bayes_bounded_cgf}, which requires a bounded CGF.
We then show that the CGFs of the standard and adversarial losses corresponding with Gaussian NLLs are bounded in \Cref{lm:cgf_bounds_std_adv_loss}. 
%Recently, \citet{banerjee2021information} derived tighter PAC-Bayesian bounds for any loss with bounded cumulant generating function (CGF). 
\begin{theorem}[Theorem 6 in \citet{banerjee2021information}] \label{th:pac_bayes_bounded_cgf}
Consider data $\mathcal{D}$ and any loss $\ell \big(\theta, (x,y)\big)$ with its corresponding expected and empirical generalization errors $R(\theta)=\expectation{(x,y) \sim \mathcal{P}}{\ell \big(\theta, (x,y)\big)}$ and $r(\theta)=\frac{1}{n} \mathcal{L}(\theta, \mathcal{D})$, respectively.  %, and a random variable $Z=\expectation{}{\ell } - \ell $. 
Let the CGF of the loss $\psi(t) = \log \expectation{}{\exp \lp t \lp \expectation{}{\ell } - \ell  \rp \rp}$  be bounded, where for some constant $c>0$, $t \in (0,1/c)$. 
%\russ{What is $c$?}
Then, we have, with probability at least $1-\beta$, for all densities 
$\rho(\theta)$,
    $$\expectation{\theta \sim \rho}{R(\theta)}
\leq 
\expectation{\theta \sim \rho}{r(\theta)} 
+ \frac{1}{t} \left[ \frac{\mathrm{KL}({\rho} \| \pi) + \log \frac{1}{\beta}}{n} + \psi(t) \right].$$
\end{theorem}
Before stating the CGF bound for the losses, we first define a \textit{sub-gamma} random variable. A random variable with variance $s^2$ and scale parameter $c$ is said to be sub-gamma if its CGF $\psi$ satisfies the following upper bound: 
$$\psi(t) \leq \frac{s^2t^2}{2(1-ct)} \text{ for all } 0 < t < 1/c.$$ 
We state the CGF bounds for both standard and adversarial losses in \Cref{lm:cgf_bounds_std_adv_loss} and provide the proof in \Cref{app:cgf_std_adv_loss}.

% \russ{What is sub-gamma?}
\begin{lemma}%[CGF of standard and adversarial losses are bounded] 
[CGF bounds for standard and adversarial losses] \label{lm:cgf_bounds_std_adv_loss}
%The standard and adversarial losses are sub-gamma distributed and their CGFs are bounded as 
%The CGF of both standard and adversarial losses are bounded as 
%$\psi(t) \leq \frac{s^2t^2}{2(1-ct)}$ for all $0<t<1/c$.
The standard and adversarial losses are both sub-gamma with the following variance $s^2$ and scale factor $c$. 
In the case of standard loss, 
%$$s^2 = \frac{2}{t} \lp \sigma^2_p \sigma_x^2d + \sigma_x^2 \|\theta^*\|^2 + \sigma^2 \lp 1-2t\sigma^2_p \sigma_x^2\rp \rp, c=2\sigma^2_p \sigma_x^2.$$ %and $t \in (0,\frac{1}{2\sigma^2_p \sigma_x^2}).$
\begin{align}
    c&=\frac{\sigma^2_p \sigma_x^2}{\sigma^2}, \quad %\nonumber \\
    s^2 = \frac{1}{t} \lp cd - ct +1 + \frac{\sigma_x^2 \|\theta^*\|^2}{\sigma^2}\rp.
    %s^2 &= \frac{2}{t} \lp \sigma^2_p \sigma_x^2 \lp d - 2\sigma^2t \rp + \sigma_x^2 \|\theta^*\|^2 + \sigma^2 \rp. 
    \label{eq:cgf_std_loss}
\end{align}
For adversarial loss with $\deltat$ perturbation, 
%$$s^2 = \frac{4}{t} \lp \sigma_p^2d \lp \sigma_x^2 + \delta^2 \rp + \sigma_x^2 \|\theta^*\|^2 + 2\sigma^2 \lp 1-4\sigma^2_pt\lp \sigma_x^2+\delta^2 \rp \rp \rp,$$ $$c=4\sigma^2_p\lp \sigma_x^2+\delta^2 \rp.$$ % and $t\in \lp 0, \frac{1}{4\sigma^2_p\lp \sigma_x^2+\delta^2 \rp} \rp$.
\begin{align}
c&=\frac{2\sigma^2_p\lp \sigma_x^2+\deltat^2 \rp}{\sigma^2}, \nonumber \\
s^2 &= \frac{2}{t} \lp cd - ct + 1 + \frac{\sigma_x^2 \|\theta^*\|^2}{\sigma^2} \rp.
\label{eq:cgf_adv_loss}
\end{align}
\end{lemma}

Using the sub-gamma property of the losses and applying their CGF bounds in \Cref{th:pac_bayes_bounded_cgf}, we derive the standard and adversarial generalizations of Bayes posterior $q(\theta)$ in \Cref{ss:bayes_cert}, % \Cref{thm:std_post_std_loss,thm:std_post_adv_loss}, 
and robust posterior $q_\delta(\theta)$ in \Cref{ss:robust_cert}, and present the proofs in \Cref{app:pac_bounds_proof_robust_posterior}. % \Cref{thm:adv_post_std_loss,thm:adv_post_adv_loss_gen,thm:adv_post_adv_loss}. %\Cref{thm:std_post_std_loss,thm:std_post_adv_loss,thm:adv_post_std_loss,thm:adv_post_adv_loss_gen,thm:adv_post_adv_loss}. 
Each of the bounds depends on parameters $c$ and $s^2$, and intuitively larger values of either lead to worse bounds, as the losses are subject to higher variability.

\subsection{Generalization certificates for Bayes posterior}
\label{ss:bayes_cert}
Using the sub-gamma property of the standard loss, the certificate for the standard generalization of the Bayes posterior $q(\theta)$ is derived in \citet{germain2016pac} by setting the free variable $t$ in CGF to $1$. We restate this result in \Cref{thm:std_post_std_loss}, expressing it explicitly in terms of the data. This contrasts with the formulation in \citet[Corollary 5]{germain2016pac}, where the bound is expressed in terms of the posterior (which in turn depends on the data). %Nevertheless, the bounds essentially capture the same behavior 
%Fundamentally, both bounds capture the same generalization behavior, with the difference being that our formulation explicitly highlights the dependence on data. % and enables 
%which is consistent with the result in \citet[Corollary 5]{germain2016pac}.

\begin{theorem}[Standard generalization of Bayes posterior, adapted from \citet{germain2016pac}] \label{thm:std_post_std_loss}
Consider $c$ and $s$ as defined in \Cref{eq:cgf_std_loss} with $t=1$, $\sigma_p^2 < \frac{\sigma^2}{\sigma_x^2}$, $W_d= I_d + \frac{\sigma_p^2}{\sigma^2} X^\top X$ and $W_n= I_n + \frac{\sigma_p^2}{\sigma^2} XX^\top$. Then, with probability at least $1-\beta$, we have the following certificate for standard generalization of the Bayes posterior $q(\theta)$: 
\begin{align}
\expectation{\theta \sim q}{R(\theta)} &\leq \frac{1}{n} \log \sqrt{\det \lp W_d \rp} + \frac{1}{2n\sigma^2} Y^\top W_n^{-1} Y  \nonumber \\
&\qquad + \frac{1}{n} \log \frac{1}{\beta}  +\frac{s^2}{2(1-c)}.
\label{eq:pac_bayes_std_post_std_loss}
\end{align}
%holding with probability at least $1-\beta$.
%where $M= I + 2\sigma_p^2 XX^\top$.
%holds with probability at least $1-\beta$. % where $s,c$ are as defined in \Cref{eq:cgf_std_loss}. %and $\sigma_p^2 < \frac{1}{2\sigma_x^2}$.
\end{theorem}
Increasing sub-gamma variability parameters $s^2$ and $c$ increase the bound~\eqref{eq:pac_bayes_std_post_std_loss}, as expected. 
Informally, the term depending on $\log \det W_d$ is a sum of $d$ $\log$ eigenvalues of $W_d$, and if $X^\top X$ is low-rank, most of these $\log$ eigenvalues are close to $0$. 
Hence the first term decreases like $1/n$.
The other data dependent term is essentially the product of $Y^\top$ and the average training error of ridge regression, which should be small if the dataset is large and the model is well-specified.
%Subsequently, we derive the other cases in the following and establish new results, especially for adversarial loss and adversarially robust posterior. %While the standard generalization of the adversarial posterior might not be a realistic valid setting, we provide the result for completion.

Next, we derive the adversarial generalization certificate for Bayes posterior similar to standard generalization.

\begin{theorem}[Adversarial generalization of Bayes posterior]\label{thm:std_post_adv_loss}
Consider $c$ and $s$ as defined in \Cref{eq:cgf_adv_loss} with $t=1$, $\sigma_p^2 < \frac{\sigma^2}{2 \lp \sigma_x^2 + \deltat^2\rp}$, $W_d= I_d + \frac{\sigma_p^2}{\sigma^2} X^\top X$ and $W_n= I_n + \frac{\sigma_p^2}{\sigma^2} XX^\top$. Then, with probability at least $1-\beta$, we have the following certificate for adversarial generalization of the Bayes posterior ${q}(\theta)$: 
\begin{align}
&\expectation{\theta\sim q}{  {R}_{\deltat}(\theta)} \leq \frac{2}{n} \log \sqrt{\det \lp W_d \rp} + \frac{1}{n\sigma^2} Y^\top W_n^{-1} Y \nonumber \\
&\qquad \qquad + \frac{1}{n} \log \frac{1}{\beta}  +\frac{s^2}{2(1-c)} + \frac{d\deltat^2 \sigma_p^2}{\sigma^2-2n\deltat^2 \sigma_p^2}. 
\label{eq:pac_bayes_std_post_adv_loss}
\end{align}
\end{theorem}

While \Cref{thm:std_post_std_loss,thm:std_post_adv_loss} are upper bounds and are incomparable, we note that the main difference in \Cref{thm:std_post_adv_loss} is the additional constant term dependent on the perturbation radius $\deltat$ and the data-dependent terms are scaled by $2$. 
The additional constant term captures the effect of testing the model adversarially, increasing the bound. 
This term behaves linearly in $\deltat$ for small $\deltat$.
%The constants $c$ and $s$ in \Cref{thm:std_post_adv_loss} are specific to adversarial loss derived in \Cref{eq:cgf_adv_loss}.
%Proofs are presented in \Cref{app:pac_bounds_proof_robust_posterior}.

\subsection{Generalization certificates for robust posterior}
\label{ss:robust_cert}

First we derive the standard generalization of robust posterior. While this setting may not be of practical interest, we provide the result for completeness.

\begin{theorem}[Standard generalization of robust posterior] \label{thm:adv_post_std_loss}
Consider $c$ and $s$ as defined in \Cref{eq:cgf_std_loss} with $t=1$, $\sigma_p^2 < \frac{\sigma^2}{\sigma_x^2}$, 
$k_\delta = \frac{2n\delta^2\sigma_p^2}{\sigma^2} + 1$, 
% $Z_2= I \lp 4n\delta^2\sigma_p^2 + 1 \rp + 4\sigma_p^2 XX^\top$, and 
$U_d= k_\delta I_d + \frac{2\sigma_p^2}{\sigma^2} X^\top X$,
$U_n= k_\delta I_n + \frac{2\sigma_p^2}{\sigma^2} XX^\top$, 
% $Z_3= I \lp 4n\delta^2\sigma_p^2 + 1 \rp + 2\sigma_p^2 XX^\top$,
$V_d= k_\delta I_d + \frac{\sigma_p^2}{\sigma^2} X^\top X$, and $V_n= k_\delta I_n + \frac{\sigma_p^2}{\sigma^2} X X^\top$. Then, with probability at least $1-\beta$, we have the following certificate for standard generalization of the robust posterior ${q}_\delta(\theta)$: 
\begin{align}
\expectation{  \theta \sim {q}_\delta}{R(\theta)}  &\leq \frac{2}{n} \log \sqrt{\det(U_d)} + \frac{2}{n k_\delta \sigma^2}Y^\top U_n^{-1} Y \nonumber \\
&\quad - \frac{1}{n} \log \sqrt{\det \lp V_d \rp} -  \frac{k_\delta}{n \sigma^2} Y^\top V_n^{-1} Y \nonumber \\
&\quad + \frac{1}{n} \log \frac{1}{\beta}  +\frac{s^2}{2(1-c)}. 
\label{eq:pac_bayes_adv_post_std_loss}
\end{align}
\end{theorem}
The terms involving $\det U_d$ and $\det V_d$ are sums of $d$ $\log$ eigenvalues divided by $n$, so they scale like $1 / n$. 
As in the previous bounds, the remaining data dependent bounds resemble the product of $Y^\top$ and the average error of ridge regression with an effective regularization parameter.
%Using a different analysis, we derive a tighter bound 

For the robust posterior, we consider the cases $\deltat=\delta$ and $\deltat\neq\delta$ separately. We derive a tighter bound for the special case when the allowed adversarial perturbation at train and test-time are the same, i.e., $\deltat = \delta$. 
\begin{theorem}[Adversarial generalization of robust posterior with $\deltat=\delta$] \label{thm:adv_post_adv_loss}
Consider $c$ and $s$ as defined in \Cref{eq:cgf_adv_loss} with $t=1$, $\sigma_p^2 < \frac{\sigma^2}{2 \lp \sigma_x^2 + \deltat^2\rp}$,
$k_\delta = \frac{2n\delta^2\sigma_p^2}{\sigma^2} + 1$, 
% $Z_2= I \lp 4n\delta^2\sigma_p^2 + 1 \rp + 4\sigma_p^2 XX^\top$, and 
$U_d= k_\delta I_d + \frac{2\sigma_p^2}{\sigma^2} X^\top X$, and $U_n= k_\delta I_n + \frac{2\sigma_p^2}{\sigma^2} XX^\top$. 
% $Z= I \lp 4n\delta^2\sigma_p^2 + 1 \rp + 4\sigma_p^2 XX^\top$, 
Then, with probability at least $1-\beta$, we have the following certificate for adversarial generalization of the robust posterior $  {q}_\delta(\theta)$: 
\begin{align}
\expectation{  \theta \sim {q}_\delta}{  {R}_\delta(\theta)}  &\leq \frac{1}{n} \log \sqrt{\det \lp U_d \rp} + \frac{1}{n k_\delta \sigma^2} Y^\top U_n^{-1} Y  \nonumber \\
&\qquad + \frac{1}{n} \log \frac{1}{\beta}  +\frac{s^2}{2(1-c)}. \label{eq:pac_bayes_adv_post_adv_loss}
\end{align}
\end{theorem}
Compared with \Cref{thm:adv_post_std_loss}, \Cref{thm:adv_post_adv_loss} only includes $1$ times the $U_d$ and $U_n$ dependent terms, instead of $2$ times the $U_d$ and $U_n$ dependent terms minus the $V_d$ and $V_d$ dependent terms. 
Empirically, in \Cref{sec:exp}, we find that this leads to a favorable bound.
%Each of these can be understood as generalization bounds of standard Bayes regression using an effective regularisation parameter.




Finally, using a different analysis we derive the adversarial generalization focusing on a general setting where the adversarial  perturbation radius at test-time $\deltat$ is not the same as the radius used to learn the posterior $\delta$.
\begin{theorem}[Adversarial generalization of robust posterior] \label{thm:adv_post_adv_loss_gen}
Consider $c$ and $s$ as defined in \Cref{eq:cgf_adv_loss} with $t=1$, $\sigma_p^2 < \frac{1}{4 \lp \sigma_x^2 + \deltat^2\rp}$,
$k_\delta = \frac{2n\delta^2\sigma_p^2}{\sigma^2} + 1$, 
$U_d= k_\delta I_d + \frac{2\sigma_p^2}{\sigma^2} X^\top X$, and $U_n= k_\delta I_n + \frac{2\sigma_p^2}{\sigma^2} XX^\top$. Then, with probability at least $1-\beta$, we have the certificate for adversarial generalization of the robust posterior $  {q}_\delta(\theta)$: 
\begin{align}
&\expectation{  \theta \sim {q}_\delta}{  {R}_{\deltat}(\theta)} \leq \frac{2}{n} \log \sqrt{\det \lp U_d \rp} + \frac{2}{n k_\delta \sigma^2} Y^\top U_n^{-1} Y  \nonumber \\
&\,\, + \frac{1}{n} \log \frac{1}{\beta}  +\frac{s^2}{2(1-c)} + \frac{\lp \deltat^2 -\delta^2 \rp \sigma_p^2 d}{\sigma^2-2n\lp \deltat^2 -\delta^2 \rp \sigma_p^2}.
\label{eq:pac_bayes_adv_post_adv_loss_gen}
\end{align}
\end{theorem}

%Proofs are presented in \Cref{app:pac_bounds_proof_robust_posterior}.

%\subsection{Proof sketch}
%\label{ss:proof_sketch}
%\todo[inline]{add it at the end depending on the page constaint}