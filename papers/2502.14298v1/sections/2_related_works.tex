\section{Discussion and Related Work}
\label{sec:related_works}



%\textbf{Extension to non-linear settings}



\paragraph{Generalization bounds for probabilistic models} 
Estimating the average generalization performance as the availability of training data increases, commonly referred to as learning curves, has been extensively studied for probabilistic models such as Gaussian processes (GPs) and Bayesian linear models. 
In GP regression, significant strides have been made in understanding generalization performance over the past two decades. For instance, \citet{sollich2002learning, sollich1998learning} estimated learning curves by bounding the prediction variance, while \citet{opper1998general} analyzed learning curves through bounds on prediction error. Further developments include \citet{sollich2001gaussian}, who investigated learning curves under mismatched models, 
%\russ{is mismatched the same as misspecified, or something different?} 
and \citet{jin2022learning}, who provided a more realistic analysis by assuming the eigenspectrum of the prior and the eigenexpansion coefficients of the target function follow a power-law distribution.
Notably, \citet{williams2000upper} derived non-trivial upper and lower bounds for GPs, offering key insights into their generalization capabilities. More recently, \citet{savvides2024error} bounded both the variance of the predictions and the bias.
While learning curves provide valuable average-case insights, they do not offer certificates. 


%Statistical physics based analysis - replica theory

%Information theory based analysis

%While several works studied estimating the generalization performance, very limited research has been conducted in providing generalization certificates of probabilistic models.

%In the case of GP regression, \citet{sollich2002learning, sollich1998learning} estimated learning curves by bounding the variance of the prediction, providing insights into the convergence of GP predictions with increasing data. Similarly, \citet{opper1998general} derived learning curves by bounding the prediction error, offering an alternative perspective on generalization performance. Extensions of these analyses include mismatched model scenarios. For instance, \citet{sollich2001gaussian} characterized learning curves for GP regression under model mismatch, elucidating the impact of misspecified priors on the generalization performance. Furthermore, \citet{williams2000upper} derived upper and lower bounds for GPs, which serve as fundamental results to assess the theoretical limits of GP learning curves.
%
%Statistical physics-inspired approaches, such as replica theory, have also been applied to analyze learning curves in probabilistic models. These methods provide a deep theoretical understanding of model generalization by leveraging analogies with physical systems. Additionally, information-theoretic methods have been explored to derive generalization bounds, emphasizing the connection between data encoding and prediction accuracy.

%\paragraph{Generalization certificates} PAC-Bayes bounds typically hold with arbitrarily high probability and express a trade-off between the empirical risk on the training set and a measure of complexity of the predictors class. \russ{remove?}

\paragraph{Generalization certificates and their practical significance}
In the context of GPs, PAC-Bayesian bounds are derived for GP classification  \citet{seeger2002pac} and GP regression \citep{suzuki2012pac}. 
Beyond GPs, PAC-Bayesian theory has been instrumental in explaining the generalization capabilities of neural networks where the uniform convergence type of analyses fail \citep{dziugaite2017computing,lotfi2022pac}.
Practically, these bounds are useful in hyperparameter optimization \citep{cherian2020efficient} and improving model training \citep{reeb2018learning,wang2023improving} by directly minimizing the bound.
%While PAC-Bayesian bounds are known to offer meaningful guarantees that can even explain generalization of neural networks \citep{lotfi2022pac}, practically, these bounds are useful to find optimal hyperparameters and improve the training by directly optimizing the bound.

\paragraph{Robustness certificates for GPs} 
While guaranteeing robustness against adversarial perturbations remain a relatively underexplored area, it is important to note that even heuristic methods aimed at improving the robustness of probabilistic models are less developed \citep{hernandez2011robust,bradshaw2017adversarial,grosse2018limitations}. Moreover, there exists no notion of adversarially robust posteriors or GPs. In the context of GP classification, \citet{blaas2020adversarial} provided robustness guarantees for the standard Bayes posterior by computing upper and lower bounds for the maximum and minimum of GP classification probabilities under adversarial perturbations. 
Similar analysis of robustness certification for standard GP regression has been investigated in works such as \citet{patane2022adversarial} and \citet{cardelli2019robustness}. %The results highlight the interplay between model uncertainty, adversarial perturbations, and prediction reliability in probabilistic settings.

\paragraph{Other robust posteriors} 
In addition to alternative robust posteriors under the optimization-centric view of Bayes rule (as discussed in Appendix~\ref{app:adv_loss}) existing and distinct notions of robust posterior are also available.
Focusing on the categorical distribution (a special exponential families), \citet{wicker2021bayesian} define a robust likelihood by marginalizing out perturbed softmax probability distributions with respect to a distribution on the perturbation allowance. 
This is then used as a standard likelihood inside Bayesian inference to building Bayesian neural networks.


\paragraph{Adversarially robust optimization} 
Although adversarially robust optimization might appear conceptually similar to our proposed adversarially robust posterior formalism, it addresses a fundamentally different problem. In Bayesian optimization, the goal is to select $x_t$ such that it yields a high value even under adversarial perturbations, i.e., maximizing $f(\tilde{x}_t)$, where $f$ is an unknown function \citep{bogunovic2018adversarially, kirschner2020distributionally}. The fundamental distinction lies in the fact that $f$ is not explicitly known in adversarially robust optimization. %which fundamentally differentiates it from our objective and 
Moreover, the objective of sequentially choosing $x$ to learn the unknown $f$ is different from learning a posterior from a given data that is adversarially robust.