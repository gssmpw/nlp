\section{Adversarial robust loss}
\label{app:adv_loss}
\subsection{The robust posterior}
The variational form of exact Bayes inference can be obtained by minimising the KL divergence of the notional posterior $q'$ from the true posterior $q$ over all probability density functions, as follows.
\begin{align*}
    &\phantom{{}={}} q(\theta \mid \mathcal{D}) \\
    &= \argmin_{q' \in \Pi} KL(q' \Vert q) \\
    &=  \argmin_{q' \in \Pi}\expectation{q'(\theta \mid \mathcal{D})} {\log \frac{q'(\theta \mid \mathcal{D}) \int p(Y \mid X, \theta') \pi(\theta') d\theta' }{ p(Y \mid X, \theta) \pi(\theta)}} \\
    &=\argmin_{q' \in \Pi}\mathop{\mathbb{E}}\limits_{q'(\theta \mid \mathcal{D})}\Bigg[\sum_{i=1}^n  -\log \frac{p(y_i \mid x_i, \theta)}{\underbrace{\int p(y_i \mid x_i, \theta) dy_i}_{=1}}\Bigg]  +  \underbrace{\log \int \prod_{i=1}^n p(y_i \mid x_i, \theta') \pi(\theta') d\theta'}_{\text{const. w.r.t. $q'$}} + KL(q' \Vert \pi ) \numberthis \label{eq:standard_bayes_choices} \\
    &= \argmin_{q' \in \Pi}\expectation{q'(\theta \mid \mathcal{D})}{\sum_{i=1}^n  -\log p(y_i \mid x_i, \theta)} + KL(q' \Vert \pi ) .
\end{align*}
\subsection{Other notions of robust posterior}
Note that in~\eqref{eq:standard_bayes_choices}, we explicitly convey that they likelihood is a proper likelihood (i.e. integrates to $1$) and the posterior is a proper posterior, possessing a normalizing constant which is independent of the functional variable $q'$. 
Thus, from an optimisation-centric view, both terms can be ignored.
When we generalize to Gibbs Bayes posteriors by changing the negative log likelihood to an adversarial loss, we therefore obtain
\begin{align*}
    &\phantom{{}={}} q(\theta \mid \mathcal{D}) =\argmin_{q' \in \Pi}\expectation{q'(\theta \mid \mathcal{D})}{\sum_{i=1}^n  \max_{\Vert \widetilde{x}_i - x_i \Vert_2 \leq \delta }-\log p(y_i \mid x_i, \theta)} + KL(q' \Vert \pi ) = \frac{\exp\big( - \mathcal{L}( \theta, \mathcal{D}) \big) \pi(\theta) }{\int \exp\big( - \mathcal{L}( \theta', \mathcal{D}) \big) \pi(\theta') d\theta' }, \numberthis \label{eq:our_choice}
\end{align*}
with $\mathcal{L}(\theta, \mathcal{D}) = \sum_{i=1}^n  \max_{\Vert \widetilde{x}_i - x_i \Vert_2 \leq \delta }-\log p(y_i \mid x_i, \theta)$.

We note however that this is not the only natural choice of ``robustifying'' posterior inference. 
In particular, starting from~\eqref{eq:standard_bayes_choices}, there are four choices depending on whether we ignore or do not ignore the normalising constants of the likelihood and the posterior when inserting the operator $\max_{\Vert \widetilde{x}_i - x_i \Vert_2 \leq \delta }$.
These four choices all lead to the same standard Bayesian posterior (i.e. $\delta = 0$), but lead to different notions of robust posterior.
In addition to these four choices, one may also consider robustifying the likelihood before the normalizing step, leading to a true likelihood (and thus, standard Bayesian inference)
Our choice~\eqref{eq:our_choice} allows for a tractable loss term, leads to a satisfying theory, and good empirical performance. 
Other choices do not immediately lead to a tractable loss term, and we leave this and investigation of their theory and empirical performance for future work.


\iffalse
\begin{align*}
    &=\argmin_{q' \in \Pi}\mathbb{E}_{q'(\theta \mid \mathcal{D})} \Big[ \sum_{i=1}^n  -\log p(y_i \mid x_i, \theta) \Big] + D(q' \Vert \pi ) \numberthis \label{eq:variational} \\
    &= \argmin_{q' \in \Pi}\mathbb{E}_{q'(\theta \mid \mathcal{D})} \Bigg[ \sum_{i=1}^n  -\log \frac{p(y_i \mid x_i, \theta)}{\underbrace{\int p(y_i \mid x_i, \theta) dy_i}_{=1}} \Bigg] + D(q' \Vert \pi )   
     \\
     &= \argmin_{q' \in \Pi}\mathbb{E}_{q'(\theta \mid \mathcal{D})} \Bigg[ \sum_{i=1}^n  -\log \frac{p(y_i \mid x_i, \theta)}{\underbrace{\int p(y_i \mid x_i, \theta) dy_i}_{=1}} \Bigg] + D(q' \Vert \pi )   
     +  \underbrace{\log \int \prod_{i=1}^n p(y_i \mid x_i, \theta') \pi(\theta') d\theta'}_{\text{const. w.r.t. $q'$}} \\
     &= \argmin_{q' \in \Pi}\mathbb{E}_{q'(\theta \mid \mathcal{D})} \Bigg[ \sum_{i=1}^n  -\log \frac{p(y_i \mid x_i, \theta)}{\underbrace{\int \exp\Big( - \big( -\log p(y_i \mid x_i, \theta) \big) \Big) dy_i}_{=1}} \Bigg] + D(q' \Vert \pi )   
     +  \underbrace{\log \int \prod_{i=1}^n p(y_i \mid x_i, \theta') \pi(\theta') d\theta'}_{\text{const. w.r.t. $q'$}} \\ 
     &= \argmin_{q' \in \Pi}\mathbb{E}_{q'(\theta \mid \mathcal{D})} \Bigg[ \sum_{i=1}^n  -\log p(y_i \mid x_i, \theta) + \log \int \exp\Big( - \big( -\log p(y_i \mid x_i, \theta) \big) \Big) dy_i \Bigg] + D(q' \Vert \pi )   \numberthis \label{eq:variational2} \\ 
\end{align*}
\fi

\iffalse
\subsection{Proof of \lowercase{\Cref{lm:adv_loss_adv_training}}}
We show the equivalence between the robust loss and adversarial training.

\begin{align*}
    &\phantom{{}={}} \arg\min_{\theta } - \log \lp \min_{\| \widetilde{x}_i - x_i \| \leq \delta} p\lp \widetilde{X}, Y \mid \theta  \rp \rp  \\
    &= \arg\min_{\theta } \sum_{i=1}^n - \log \lp \min_{\| \widetilde{x}_i - x_i \| \leq \delta} p\lp \widetilde{x}_i, y_i \mid \theta \rp \rp\\
    &= \arg\min_{\theta } \sum_{i=1}^n  \max_{\| \widetilde{x}_i - x_i \| \leq \delta} - \log \lp p\lp \widetilde{x}_i, y_i \mid \theta \rp \rp\\
    %\arg\min_{\theta } \prod_{i=1}^n \min_{\| \widetilde{x}_i - x_i \| \leq \delta} p\lp \widetilde{x}_i, y_i \mid \theta \rp 
    &= \arg\min_{\theta } \sum_{i=1}^n \max_{\| \widetilde{x}_i - x_i \| \leq \delta} -\log \lp p \lp y_i \mid \widetilde{x}_i,  \theta \rp p\lp  \widetilde{x}_i \mid  \theta \rp \rp \quad; p\lp  \widetilde{x}_i \mid  \theta \rp = p\lp  \widetilde{x}_i \rp \text{is constant w.r.t } \theta \\
    &= \arg\min_{\theta } \sum_{i=1}^n \max_{\| \widetilde{x}_i - x_i \| \leq \delta} -\log p \lp y_i \mid \widetilde{x}_i,  \theta \rp \impliedby \text{Our adversarially robust loss \Cref{eq:adv_loss}} \\
    &= \arg\min_{\theta } \sum_{i=1}^n \max_{\| \widetilde{x}_i - x_i \| \leq \delta} \frac{1}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \lp \theta^\top \widetilde{x}_i - y_i \rp^2  \quad; p \lp y_i \mid \widetilde{x}_i,  \theta \rp \sim \mathcal{N}\lp \theta^\top \widetilde{x}_i, \sigma^2 \rp \\
    &= \arg\min_{\theta } \sum_{i=1}^n \max_{\| \widetilde{x}_i - x_i \| \leq \delta} \lp \theta^\top \widetilde{x}_i - y_i \rp^2 \quad \impliedby \text{Adversarial training objective}
\end{align*}
\fi

\subsection{Proof of \lowercase{\Cref{lm:adv_loss_closed_form_gaussian,lm:adv_loss_closed_form}}}
\begin{proof}
We begin with the Gaussian case, then consider the more general exponential family case, and then return to the Gaussian case.
Choosing a Gaussian likelihood and a linear predictor, up to some constant,
\begin{align*}
    \max_{\widetilde{x}_i: \Vert x_i - \widetilde{x}_i \Vert \leq \delta}-\log p(y_i \mid \widetilde{x}_i, \theta) &= \max_{\widetilde{x}_i: \Vert x_i - \widetilde{x}_i \Vert \leq \delta} (y_i - \widetilde{x}_i^\top \theta)^2 \\
    &= \max_{\widetilde{x}_i: \Vert x_i - \widetilde{x}_i \Vert \leq \delta} \big(y_i -x_i^\top \theta - \theta^\top (\widetilde{x}_i -x_i) \big)^2 \\
    &= \max_{\widetilde{x}_i: \Vert x_i - \widetilde{x}_i \Vert \leq \delta} \big(y_i -x_i^\top \theta - \Vert \theta \Vert_2  \delta \cos \gamma \big)^2,
\end{align*}
where $\gamma$ is the angle between $\theta$ and $\widetilde{x}_i - x_i$.
The argument of $(\cdot)^2$ is maximally positive or negative when $\cos\gamma$ is $\pm 1$ and shares the same sign as $y_i - x_i^\top \theta$. Thus
\begin{align*}
    \max_{\widetilde{x}_i: \Vert x_i - \widetilde{x}_i \Vert \leq \delta}-\log p(y_i \mid \widetilde{x}_i, \theta) &= \big( |y_i - x_i^\top \theta| + \Vert \theta \Vert \delta\big)^2 \\
    &= (y_i - x_i^\top \theta)^2 + 2 \delta \Vert \theta \Vert |y_i - x_i^\top \theta|  + \Vert \theta \Vert^2 \delta^2,
\end{align*}
That is, $\tilde{x}_i =  \delta \sign(x_i^\top \theta - y_i)\frac{\theta}{\|\theta\|} + x_i$. 
%For detailed proof check \citet[Section 6.2]{javanmard2020precise}.

\iffalse
\textcolor{red}{Exact loss with explicit constant terms and factors is
$$\frac{1}{2} \log \lp 2\pi \sigma^2\rp + \frac{1}{2\sigma^2} \lp y_i - \widetilde{x}_i^\top \theta \rp^2$$
}
\fi

Consider the Bregman divergence and apply the law of cosines (e.g. Property 1 of~\citet{nielsen2021geodesic}),
\begin{align*}
 \max_{\widetilde{x}: \Vert x - \widetilde{x} \Vert \leq \delta} d_\phi \big( \theta^\top \widetilde{x}, y^\ast \big) &= \max_{\widetilde{x}: \Vert x - \widetilde{x} \Vert \leq \delta} d_\phi \big( \theta^\top \widetilde{x}, \theta^\top x \big) + d_\phi \big( \theta^\top x, y^\ast \big) - \theta^\top (\widetilde{x} - x)\big(\nabla \phi(y^\ast) - \nabla \phi(\theta^\top x) \big) \\
 &= \max_{\widetilde{x}: \Vert x - \widetilde{x} \Vert \leq \delta} \phi(\theta^\top \widetilde{x}) - \phi(\theta^\top x) - \nabla \phi(y^\ast) \theta^\top(\widetilde{x} - x) + d_\phi(\theta^\top x, y^\ast).
\end{align*}
This is a convex objective on a convex constraint set $\Vert x - \widetilde{x} \Vert_2^2 \leq \delta^2$, so there exists a unique maximum on an extremal point on the constraint set.
The KKT conditions give that at the optimal,
\begin{align*}
    \big(\nabla(\phi(\theta^\top \widetilde{x}) - \nabla \phi(y^\ast)\big) \theta - 2 \lambda (\widetilde{x} - x) = 0,
\end{align*}
for Lagrange multiplier $\lambda \leq 0$.
Therefore, $\widetilde{x}$ satisfies the implicit equation
\begin{align*}
    \widetilde{x} = \frac{\big(\nabla(\phi(\theta^\top \widetilde{x}) - \nabla\phi(y^\ast)\big) \theta}{2 \lambda} + x. \numberthis \label{eq:implicit_xtilde}
\end{align*}
We must have the solution on the extremal, so 
\begin{align*}
    \delta = \Big| \frac{\big(\nabla(\phi(\theta^\top \widetilde{x}) - \nabla\phi(y^\ast)\big) }{2 \lambda} \Big| \Vert \theta \Vert_2 \quad \text{and so} \quad \lambda = \Big| \frac{\nabla(\phi(\theta^\top \widetilde{x}) - \nabla\phi(y^\ast) }{2 \delta} \Big| \Vert \theta \Vert_2.
\end{align*}
Plugging $\lambda$ back into~\eqref{eq:implicit_xtilde}, we find that the optimal $\widetilde{x}$ is a linear combination of $\theta$ and $x$,
\begin{align*}
    \widetilde{x} = \delta \underbrace{\sign \big(\nabla(\phi(\theta^\top \widetilde{x}) - \nabla\phi(y^\ast) \big)}_{:= s \in \{-1, 1\}} \Vert \theta \Vert_2^{-1} \theta + x.
\end{align*}
The maximum value is then
\begin{align*}
    \phi(s \delta \Vert \theta \Vert_2 + \theta^\top x) - \phi(\theta^\top x) - \nabla \phi(y^\ast) s \delta \Vert \theta \Vert_2 + d_\phi(\theta^\top x, y^\ast).
\end{align*}
Finally, note that $\nabla \phi(y^\ast) = \nabla \phi\big( (\nabla \phi)^{-1}(y)\big)=y$.
We may then compute the maximum by testing the two $s \in \{ -1, 1\}$, and choosing the value of $s$ which gives the maximum result.
In the case of Gaussian loss (i.e. squared error), the parameters are self-dual and we have
\begin{align*}
    &\phantom{{}={}} \phi(s \delta \Vert \theta \Vert_2 + \theta^\top x) - \phi(\theta^\top x) - \nabla \phi(y) s \delta \Vert \theta \Vert_2 + d_\phi(\theta^\top x, y) \\
    &=\delta^2 \Vert \theta \Vert_2^2 + 2 s \delta \Vert \theta \Vert_2 (\theta^\top x) - 2 y s \delta \Vert \theta \Vert_2 + \Vert y - \theta^\top x \Vert_2^2 \\
    &= \delta^2 \Vert \theta \Vert_2^2 + 2 s \delta \Vert \theta \Vert_2  \big( (\theta^\top x) -  y  \big) + \Vert y - \theta^\top x \Vert_2^2,
\end{align*}
the maxima being $\delta^2 \Vert \theta \Vert_2^2 + 2  \delta \Vert \theta \Vert_2  \big| (\theta^\top x) -  y  \big| + \Vert y - \theta^\top x \Vert_2^2$ with $s = \sign (\theta^\top x - y)$.
\end{proof}

\clearpage

\section{Proof of CGF bounds for standard and adversarial NLL losses in \MakeLowercase{\Cref{lm:cgf_bounds_std_adv_loss}}}
\label{app:cgf_std_adv_loss}

We first derive the following helpful \Cref{lm:adv_loss_bounds,lm:std_gaussian_int} to derive the CGF bounds and the generalization certificates.

\begin{lemma}[Upper and lower bounds for adversarial NLL loss]
\label{lm:adv_loss_bounds} 
Using the closed-form of the adversarial NLL loss, the upper and lower bounds are
\begin{align*}
     \ell_\delta(\theta, \mathcal{D}) &= \sum_{i=1}^n \lp  \frac{1}{2\sigma^2}\big(|y_i - x_i^\top \theta| + \Vert \theta \Vert \delta\big)^2 + \frac{1}{2} \log \lp 2\pi \sigma^2\rp \rp \\
    &\leq \sum_{i=1}^n \lp \frac{1}{2\sigma^2} \Big( 2(y_i - x_i^\top \theta)^2 + 2\Vert \theta \Vert^2 \delta^2 \Big) + \frac{1}{2} \log \lp 2\pi \sigma^2\rp \rp \quad ; (a-b)^2 \geq 0 \implies a^2+b^2 \geq 2ab \\
     \ell_\delta(\theta, \mathcal{D}) &\geq \sum_{i=1}^n \lp  \frac{1}{2\sigma^2} \Big( (y_i - x_i^\top \theta)^2 + \Vert \theta \Vert^2 \delta^2 \Big) + \frac{1}{2} \log \lp 2\pi \sigma^2\rp \rp
\end{align*}
\qed
\end{lemma}

\begin{lemma}[Standard Gaussian integral] \label{lm:std_gaussian_int} The integral of the form 
$\int \exp\left(-\theta^\top M \theta + 2b^\top \theta\right) d\theta$ evaluates to $\sqrt{\frac{\pi^d}{\det M}} \exp\left(b^\top M^{-1}b\right).$

\begin{proof}
\begin{align*}
    &-\theta^\top M \theta + 2b^\top \theta = -(\theta - M^{-1}b)^\top M (\theta - M^{-1}b) + b^\top M^{-1}b \quad; \text{Completing the square} \\
    &\text{Therefore, integral becomes } \int \exp\left(-(\theta - M^{-1}b)^\top M (\theta - M^{-1}b) + b^\top M^{-1}b\right) d\theta \\
    &= \exp\left(b^\top M^{-1}b\right) \int \exp\left(-(\theta - M^{-1}b)^\top M (\theta - M^{-1}b)\right) d\theta \\
    &= \exp\left(b^\top M^{-1}b\right) \sqrt{\frac{\pi^d}{\det M}} \quad; \int \exp\left(-\phi^\top M \phi\right) d\phi = \sqrt{\frac{\pi^d}{\det M}}
\end{align*}
\end{proof}
\end{lemma}

\paragraph{Proof of CGF bounds for standard NLL loss}
Now we derive the CGF bound for standard NLL loss in the following. Note that similar derivation is done in \citet{germain2016pac}.
\begin{proof}
\begin{align*}
    &\log \mathbb{E}_{\theta}\mathbb{E}_{x_i}\expectation{y_i \mid x_i}{\exp \lp t \lp \mathbb{E}_{x_i}\expectation{y_i \mid x_i}{\frac{1}{2\sigma^2}(y_i - x_i^\top \theta)^2} - \frac{1}{2\sigma^2}(y_i - x_i^\top \theta)^2 \rp \rp} \\
    &= \log \mathbb{E}_{\theta}\mathbb{E}_{x_i}\expectation{y_i \mid x_i}{\exp \lp \frac{t}{2\sigma^2} \lp \mathbb{E}_{x_i}\expectation{y_i \mid x_i}{(y_i - x_i^\top \theta)^2}\rp \exp \lp - \frac{t}{2\sigma^2}(y_i - x_i^\top \theta)^2 \rp \rp} \quad; t>0 \implies \exp(-t(.)^2) \leq 1 \\
    &\leq \log \mathbb{E}_{\theta}\mathbb{E}_{x_i}\expectation{y_i \mid x_i}{\exp \lp \frac{t}{2\sigma^2} \lp \mathbb{E}_{x_i}\expectation{y_i \mid x_i}{(y_i - x_i^\top \theta)^2}\rp \rp} \\
    &= \log \mathbb{E}_{\theta}\mathbb{E}_{x_i}\expectation{\epsilon_i}{\exp \lp \frac{t}{2\sigma^2} \lp \mathbb{E}_{x_i}\expectation{\epsilon_i}{(x_i^\top (\theta^* -\theta) + \epsilon_i)^2}\rp \rp}  \\
    &= \log \expectation{\theta}{\exp \lp \frac{t}{2\sigma^2} \lp \sigma_x^2 \| \theta^* -\theta \|^2 + \sigma^2  \rp\rp} \quad; \epsilon_i \sim \mathcal{N}(0, \sigma^2), \mathbb{E}[x_i] = 0, \mathbb{E}[\|x_i\|^2] = \sigma_x^2 \\
    &= \log \int \exp \lp \frac{t}{2\sigma^2}\lp \sigma_x^2 \theta^\top \theta - 2\sigma_x^2 \theta^{*\top} \theta +  \sigma_x^2 \|\theta^*\|^2 +  \sigma^2 \rp - \frac{1}{2\sigma_p^2}\theta^\top \theta  \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta \quad; \theta \sim \mathcal{N}(0, \sigma^2_p I) \\
    &= \log \int \exp \lp - \lp \frac{1-t\sigma_p^2 \sigma_x^2/\sigma^2}{2\sigma^2_p} \rp \theta^\top \theta - \frac{t\sigma_x^2}{\sigma^2} \theta^{*\top} \theta + \frac{t\sigma_x^2}{2\sigma^2}  \|\theta^*\|^2 + \frac{t}{2}  \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta %\\
\end{align*}
\begin{align*}
    &= \log \sqrt{\frac{\pi 2 \sigma^2_p}{1-t\sigma^2_p \sigma_x^2/\sigma^2}}^d \exp \lp \frac{t^2 \sigma_x^4 \|\theta^*\|^2\sigma^2_p/2\sigma^2}{1-t\sigma_p^2 \sigma_x^2/\sigma^2} + \frac{t \sigma_x^2}{2\sigma^2} \|\theta^*\|^2 + \frac{t}{2} \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} \quad; t< \frac{\sigma^2}{\sigma^2_p \sigma_x^2},  \text{Lemma \ref{lm:std_gaussian_int}} \\
    &= \frac{d}{2}\log \frac{1}{1-t\sigma^2_p \sigma_x^2/\sigma^2} + \frac{t \sigma_x^2 \|\theta^*\|^2/2\sigma^2}{1-t\sigma_p^2 \sigma_x^2/\sigma^2} + \frac{t}{2} \\
    &\leq \frac{t\sigma^2_p \sigma_x^2d/\sigma^2}{2\lp 1-t\sigma^2_p \sigma_x^2/\sigma^2\rp} + \frac{t \sigma_x^2 \|\theta^*\|^2/\sigma^2}{2 \lp 1-t\sigma_p^2 \sigma_x^2/\sigma^2 \rp} + \frac{t}{2} \quad ; -\log (1-x) \leq \frac{x}{1-x} \\
    &= \frac{t^2s^2}{2(1-tc)}
\end{align*}
From above we get $s^2 = \frac{1}{t} \lp \frac{\sigma^2_p \sigma_x^2d}{\sigma^2} + \frac{\sigma_x^2 \|\theta^*\|^2}{\sigma^2} + \lp 1-\frac{t\sigma^2_p \sigma_x^2}{\sigma^2}\rp \rp $, $c=\frac{\sigma^2_p \sigma_x^2}{\sigma^2}$ and $t \in (0,\frac{\sigma^2}{\sigma^2_p \sigma_x^2})$.
\end{proof}

\paragraph{Proof of CGF bounds for adversarial NLL loss}
The cumulant generating function of the adversarial loss can be bounded similar to standard loss as follows. 
\begin{proof}
\begin{align*}
    &\log \expectation{\theta}{\exp \lp \frac{t}{2\sigma^2} \mathbb{E}_{x_i}\expectation{y_i \mid x_i}{ (y_i - x_i^\top \theta)^2 + 2 \delta \Vert \theta \Vert |y_i - x_i^\top \theta|  + \Vert \theta \Vert^2 \delta^2} \rp} \\
    &\leq \log \expectation{\theta}{\exp \lp \frac{t}{2\sigma^2} \mathbb{E}_{x_i}\expectation{y_i \mid x_i}{ 2(y_i - x_i^\top \theta)^2 + 2 \Vert \theta \Vert^2 \delta^2} \rp} \quad; \text{Lemma \ref{lm:adv_loss_bounds}} \\
    &= \log \expectation{\theta}{\exp \lp \frac{t}{2\sigma^2} \mathbb{E}_{x_i}\expectation{\epsilon_i}{ 2(x_i^\top \lp \theta^* - \theta \rp + \epsilon_i)^2 + 2\Vert \theta \Vert^2 \delta^2} \rp} \\
    &= \log \expectation{\theta}{\exp \lp \frac{t}{\sigma^2} \sigma_x^2 \| \theta^* - \theta \|^2 + t + \frac{t}{\sigma^2}\Vert \theta \Vert^2 \delta^2 \rp} \\
    &= \log \int \exp\lp \lp \frac{t\sigma_x^2}{\sigma^2} + \frac{t\delta^2}{\sigma^2} - \frac{1}{2\sigma^2_p} \rp \|\theta\|^2 - \frac{2t \sigma_x^2}{\sigma^2} \theta^{*\top} \theta + \frac{t \sigma_x^2}{\sigma^2} \|\theta^*\|^2 + t  \rp \frac{1}{\sqrt{2\pi \sigma^2_p}^d} d\theta \\
    &= \log \sqrt{\frac{\pi 2 \sigma^2_p}{1-2\sigma^2_pt\lp \sigma_x^2+\delta^2 \rp/\sigma^2}}^d \exp \lp \frac{t^2\sigma_x^4\|\theta^{*}\|^2 2\sigma^2_p/\sigma^2}{1-2\sigma^2_pt\lp \sigma_x^2+\delta^2 \rp/\sigma^2}+ \frac{t \sigma_x^2}{\sigma^2} \|\theta^*\|^2 + t \rp \frac{1}{\sqrt{2\pi \sigma^2_p}^d} \\
    &=\frac{d}{2}\log \frac{1}{1-2\sigma^2_pt\lp \sigma_x^2+\delta^2 \rp/\sigma^2} + \frac{\frac{t \sigma_x^2}{\sigma^2} \|\theta^*\|^2 + t \lp 1-\frac{2\sigma^2_p t}{\sigma^2}\lp \sigma_x^2+\delta^2 \rp \rp}{1-2\sigma^2_pt\lp \sigma_x^2+\delta^2 \rp/\sigma^2} \\
    &\leq \frac{\sigma_p^2td \lp \sigma_x^2 + \delta^2 \rp/\sigma^2}{1-2\sigma^2_pt\lp \sigma_x^2+\delta^2 \rp/\sigma^2}+ \frac{\frac{t \sigma_x^2}{\sigma^2} \|\theta^*\|^2 + t \lp 1-\frac{2\sigma^2_p t}{\sigma^2}\lp \sigma_x^2+\delta^2 \rp \rp}{1-2\sigma^2_pt\lp \sigma_x^2+\delta^2 \rp/\sigma^2} \\
    &= \frac{t^2s^2}{2(1-tc)}
\end{align*}
For adversarial loss, %$t < \frac{\sigma^2}{2\sigma^2_p\lp \sigma_x^2+\delta^2 \rp}$.
$s^2 = \frac{2}{t} \lp cd + \frac{\sigma_x^2 \|\theta^*\|^2}{\sigma^2} + \lp 1-ct \rp \rp$, $c=\frac{2\sigma^2_p\lp \sigma_x^2+\delta^2 \rp}{\sigma^2}$ and $t\in \lp 0, \frac{\sigma^2}{2\sigma^2_p\lp \sigma_x^2+\delta^2 \rp} \rp$.
\end{proof}

\section{Proof of Theorems in \MakeLowercase{\Cref{ss:robust_cert}}}
\label{app:pac_bounds_proof_robust_posterior}

In this section, we derive \Cref{thm:std_post_std_loss,thm:std_post_adv_loss,thm:adv_post_std_loss,thm:adv_post_adv_loss,thm:adv_post_adv_loss_gen} using \Cref{th:pac_bayes_bounded_cgf}. From the PAC-Bayesian bounds for bounded CGF loss theorem, it is clear that we need to bound the expected training risk plus the KL divergence between the posterior and prior. In the bound derivation, we require to bound the negative log normalizing constants of the posteriors which will be presented first in \Cref{lm:neg_log_z_bayes,lm:neg_log_z_robust}. %We will first provide the bounds for the negative log normalizing constants for Bayes and robust posteriors below.

\begin{lemma}[Negative log normalizing constant of the Bayes posterior] 
The normalizing constant $z$ of Bayes posterior $q$ is 
\begin{align*}
    {z} &= \int \exp\Big( -\mathcal{L}(\theta, \mathcal{D}) \Big) \pi(\theta) d\theta \\
    &= \int \exp\Big( -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - x_i^\top \theta)^2 \Big) \pi(\theta) d\theta \\
    &= \frac{\exp(-Y^\top Y/2\sigma^2)}{\sqrt{2\pi\sigma^2_p}^d}\int \exp\Big( -\theta^\top \lp \frac{1}{2\sigma^2}X^\top X + \frac{1}{2\sigma^2_p}I \rp \theta + \frac{1}{2\sigma^2}2 Y^\top X \theta \Big) \Big)  d\theta  \\
    &= \frac{\exp \lp -\frac{1}{2\sigma^2}Y^\top Y + \frac{1}{2\sigma^2}Y^\top X \lp \frac{1}{\sigma^2}X^\top X + \frac{1}{\sigma^2_p}I \rp^{-1} X^\top Y\frac{1}{\sigma^2} \rp}{\sqrt{ \det \lp \frac{\sigma_p^2}{\sigma^2} X^\top X +  I\rp}}
\end{align*}
\begin{align*}
     \log \frac{1}{z} &= \frac{1}{2}\log \det \lp \frac{\sigma_p^2}{\sigma^2} X^\top X + I \rp + \frac{1}{2\sigma^2} \lp Y^\top Y - Y^\top X \lp \frac{1}{\sigma^2} X^\top X + I \rp^{-1} X^\top Y\frac{1}{\sigma^2} \rp  \\
    &=   \frac{1}{2}\log  \det \lp \frac{\sigma_p^2}{\sigma^2} X^\top X + I \rp + \frac{1}{2\sigma^2} Y^\top \lp I + \frac{\sigma_p^2}{\sigma^2}  XX^\top  \rp^{-1} Y \quad; \text{Woodbury Matrix Identity}
\end{align*}
\qed
\label{lm:neg_log_z_bayes}
\end{lemma}

Similar to above \Cref{lm:neg_log_z_bayes}, we derive the negative log normalizing constant of the robust posterior in the following.

\begin{lemma}[Negative log normalizing constant of the robust posterior]
The normalizing constant $z_\delta$ of robust posterior $q_\delta$ is
\begin{align*}
    z_\delta &= \int \exp\Big( -\mathcal{L}_\delta(\theta, \mathcal{D}) \Big) \pi(\theta) d\theta \\
    &\geq \int \exp\Big( -{\frac{1}{\sigma^2}} \sum_{i=1}^n \Big( (y_i - x_i^\top \theta)^2 + \Vert \theta \Vert^2 \delta^2 \Big) \Big) \pi(\theta) d\theta \\
    &= \frac{1}{\sqrt{2\pi\sigma^2_p}^d}\int \exp\Bigg( - \Big( \frac{n \delta^2}{{\sigma^2}}\Vert \theta \Vert^2  + \frac{1}{2\sigma^2_p}\Vert \theta \Vert^2 + \frac{1}{{\sigma^2}} \Vert Y - X\theta \Vert^2  \Big) \Bigg)  d\theta  \quad; \pi(\theta) \sim \mathcal{N}(0,\sigma^2_p I)\\
    &= \frac{1}{\sqrt{2\pi\sigma^2_p}^d}\int \exp\Bigg( - \Big( \frac{ n\delta^2}{{\sigma^2}} \theta^\top \theta  + \frac{1}{2\sigma^2_p}\theta^\top \theta + {\frac{1}{\sigma^2}}\lp \Vert Y \Vert^2 - 2 Y^\top X \theta + \theta^\top X^\top X \theta \rp \Big) \Bigg)  d\theta  \\
    &= \frac{1}{\sqrt{2\pi\sigma^2_p}^d} \exp(-\frac{1}{\sigma^2}\Vert Y\Vert^2) \int \exp\Bigg( - \theta^\top \big( (\frac{n\delta^2}{{\sigma^2}} + \frac{1}{2\sigma^2_p})I + \frac{X^\top X}{{\sigma^2}}  \big)\theta + \frac{2 Y^\top X\theta}{{\sigma^2}} \Bigg)  d\theta \quad; \text{Lemma \ref{lm:std_gaussian_int}}  \\
    &=  \sqrt{\frac{1}{\det \lp 2{\frac{\sigma_p^2}{\sigma^2}}X^\top X + ({\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp}} \exp(-\frac{\Vert Y\Vert^2}{{\sigma^2}}) \exp\lp \frac{2Y^\top X}{{\sigma^4}} \lp {\frac{2}{\sigma^2}}X^\top X + ({\frac{2 n\delta^2}{\sigma^2}} + \frac{1}{\sigma^2_p})I\rp^{-1}X^\top Y\rp %\\
\end{align*}
\begin{align*}
    \log \frac{1}{z_\delta}&\leq  %\sqrt{\cancel{2^d \sigma^{2d}_p} \det \lp 2\textcolor{red}{\frac{\sigma_p^2}{\sigma^2}}X^\top X + (\textcolor{red}{\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + \cancel{\frac{1}{2\sigma^2_p}})I\rp} \exp\lp \frac{2 Y^\top Y}{2\sigma^2} -  \textcolor{red}{\frac{2}{\sigma^4}}Y^\top X \lp \textcolor{red}{\frac{2}{\sigma^2}}X^\top X + (\textcolor{red}{\frac{1}{\sigma^2}}2 n\delta^2 + \frac{1}{\cancel{2}\sigma^2_p})I\rp^{-1}X^\top Y\rp \\
    %&= 
    \frac{1}{2} \log \det \lp 2{\frac{\sigma_p^2}{\sigma^2}}X^\top X + ({\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp +  {\frac{1}{\sigma^2}} Y^\top  \lp I - \frac{2}{{\sigma^2}}X \lp {\frac{2}{\sigma^2}}X^\top X + ({\frac{1}{\sigma^2}}2 n\delta^2 + \frac{1}{\sigma^2_p})I\rp^{-1} X^\top  \rp Y  \\
    &= \frac{1}{2} \log  \det \lp 2{\frac{\sigma_p^2}{\sigma^2}}X^\top X + ({\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp +  {\frac{1}{\sigma^2}} Y^\top  \lp I + \frac{2\sigma^2_p}{2n\delta^2\sigma^2_p + \sigma^2} XX^\top  \rp^{-1} Y  \quad; \text{Woodbury Matrix Identity} \\
    &= \frac{1}{2} \log \det \lp 2{\frac{\sigma_p^2}{\sigma^2}}X^\top X + ({\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp + {\frac{\sigma^2}{\sigma^2 \lp 2n\delta^2 \sigma_p^2 + \sigma^2 \rp}} Y^\top  \lp I \lp \frac{\sigma_p^2}{\sigma^2}2n\delta^2 + 1 \rp + \frac{2\sigma^2_p}{\sigma^2} XX^\top  \rp^{-1} Y
\end{align*}
\qed
\label{lm:neg_log_z_robust}
\end{lemma}

With the above bounds derived, we are now ready to derive the generalization certificate using \Cref{th:pac_bayes_bounded_cgf}. Following holds for all the cases:

$(i)$ The CGF bound of respective loss-NLL in case of standard generalization and adversarial NLL in case of adversarial generalization)-appears in the theorem. This directly implies that the constants $c$ and $s$ should follow \Cref{lm:cgf_bounds_std_adv_loss} according to the considered setting.

$(ii)$ The free parameter in the CGF bound, $t$ is choosen as $t=1$, which is also done in the prior work \citep{germain2016pac}. This means $1 \in \lp 0, 1/c \rp$, that is, $c < 1$.

Consequently, the effective \Cref{th:pac_bayes_bounded_cgf} is stated below for clarity.  

\begin{theorem}[\Cref{th:pac_bayes_bounded_cgf}] \label{th:pac_bayes_bounded_cgf_t1}
Consider data $\mathcal{D}$ and any loss $\ell \big(\theta, (x,y)\big)$ with its corresponding expected and empirical generalization errors $R(\theta)=\expectation{(x,y) \sim \mathcal{P}}{\ell \big(\theta, (x,y)\big)}$ and $r(\theta)=\frac{1}{n} \mathcal{L}(\theta, \mathcal{D})$, respectively.  %, and a random variable $Z=\expectation{}{\ell } - \ell $. 
Let the CGF of the loss $\psi(t) \leq \frac{t^2s^2}{2 \lp 1-tc \rp}$  be bounded, where for some constant $c \in \lp 0,. 1 \rp$ and $t \in (0,1/c)$. 
Then, we have, with probability at least $1-\beta$, for all densities 
$\rho(\theta)$,
    $$\expectation{\theta \sim \rho}{R(\theta)}
\leq 
\expectation{\theta \sim \rho}{r(\theta)} + \frac{1}{n} \mathrm{KL}({\rho} \| \pi) + \frac{1}{n} \log \frac{1}{\beta}
+ \frac{s^2}{2 \lp 1-c \rp}.$$
\end{theorem}

Therefore, we only need to bound the following for $\rho$ being standard Bayes $q$ and robust posterior $q_\delta$, and $\ell$ being NLL and adversarial NLL.
\begin{align}
\expectation{\theta \sim \rho}{r(\theta)} + \frac{1}{n} \mathrm{KL}({\rho} \| \pi). \label{eq:eff_pac_bound}
\end{align}

\subsection{Standard generalization of Bayes posterior (\MakeLowercase{\Cref{thm:std_post_std_loss})}}
\label{pf:std_post_std_loss}
In this case, $\rho = q$ and $\ell$ is NLL in \Cref{eq:eff_pac_bound} which reduces the expression to the following.
\begin{align*}
    \expectation{\theta \sim q}{r(\theta)} + \frac{1}{n} \mathrm{KL}({q} \| \pi) 
    &= \frac{1}{n} \int \frac{1}{{z}}\ell(\theta, \mathcal{D}) \exp(-\ell(\theta, \mathcal{D}))\pi(\theta) d\theta + \frac{1}{n} \int \frac{1}{{z}} \exp(-\ell(\theta, \mathcal{D}))\pi(\theta) \log \lp \frac{\exp(-\ell(\theta, \mathcal{D}))\pi(\theta)}{z\pi(\theta)} \rp d\theta \\
    &= \frac{1}{n} \int \frac{1}{z} \exp(-\ell(\theta, \mathcal{D}))\pi(\theta) \log \lp \frac{1}{z} \rp d\theta \\
    &= \frac{1}{n} \log \frac{1}{z} \stepcounter{equation}\tag{\theequation}\label{eq:std_post_std_loss_normz}
\end{align*}

Therefore, we obtain \Cref{thm:std_post_std_loss} by substituting \Cref{lm:neg_log_z_bayes} in \Cref{eq:std_post_std_loss_normz} and combining it with the \Cref{th:pac_bayes_bounded_cgf_t1}.
\qed

\subsection{Adversarial generalization of robust posterior $\delta=\deltat$ (\MakeLowercase{\Cref{thm:adv_post_adv_loss})}}

Notice that the above derivation in \Cref{pf:std_post_std_loss} holds for $\rho=q_\delta$ and $\ell$ is adversarial loss with $\delta$ perturbation with $z_\delta$ as the normalization constant. That is, the perturbation at train and test-time are the same $\delta=\deltat$. 

\begin{align*}
    \expectation{\theta \sim q_\delta}{r(\theta)} + \frac{1}{n} \mathrm{KL}({q_\delta} \| \pi) 
    &= \frac{1}{n} \log \frac{1}{z_\delta} \stepcounter{equation}\tag{\theequation}\label{eq:adv_post_adv_loss_normz}
\end{align*}

Therefore, we obtain \Cref{thm:adv_post_adv_loss} by substituting \Cref{lm:neg_log_z_robust} in \Cref{eq:adv_post_adv_loss_normz} and combining it with the \Cref{th:pac_bayes_bounded_cgf_t1}.
\qed

\subsection{Adversarial generalization of Bayes posterior (\MakeLowercase{\Cref{thm:std_post_adv_loss})}}

In this case, $\rho = q$ and $\ell$ is adversarial NLL with $\deltat$ perturbation in \Cref{eq:eff_pac_bound}. We upper bound it as follows.

For ease of notation, only in the following, we use short hand notations for $\ell$ to denote $\ell(\theta, \mathcal{D})$ and $\ell_{\deltat}$ to denote $\ell_{\deltat}(\theta, \mathcal{D})$.

\begin{align*}
    \expectation{\theta \sim {{q}}}{\frac{1}{n} \ell_{\deltat} (\theta, \mathcal{D})} + \frac{1}{n} \mathrm{KL}({{q}} \| \pi) 
    &= \frac{1}{n} \int \frac{1}{{z}}\ell_{\deltat}\exp(-{\ell})\pi(\theta) d\theta + \frac{1}{n} \int \frac{1}{{z}} \exp(-{\ell})\pi(\theta) \log \lp \frac{\exp(-{\ell})\pi(\theta)}{{z}\pi(\theta)} \rp d\theta \\
    &= \frac{1}{n} \int \frac{1}{{z}}\lp \ell_{\deltat} - \ell \rp\exp(-{\ell})\pi(\theta) d\theta + \frac{1}{n} \int \frac{1}{{z}} \exp(-{\ell})\pi(\theta) \log \lp \frac{1}{{z}} \rp d\theta \\
    &\leq \frac{1}{n} \int \frac{1}{{z}}\lp 2\ell +  {\frac{1}{2\sigma^2}} 2n \|\theta\|^2 \delta^2 - \ell \rp\exp(-{\ell})\pi(\theta) d\theta + \frac{1}{n} \log \frac{1}{{z}} \quad; \text{Lemma \ref{lm:adv_loss_bounds}} \\
    &\leq \frac{1}{n} \log \int \frac{1}{{z}} \exp \lp \ell +  {\frac{1}{2\sigma^2}}2n \|\theta\|^2 \delta^2 \rp \exp(-{\ell})\pi(\theta) d\theta +  \frac{1}{n} \log \frac{1}{{z}} \quad; \text{Jensen's ineq.} \\
    &= \frac{1}{n} \log \frac{1}{{z}} \int  \exp \lp \lp \frac{2n \delta^2}{ {2\sigma^2}} - \frac{1}{2\sigma_p^2} \rp \|\theta\|^2  \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta +  \frac{1}{n} \log \frac{1}{{z}} \\
    &= \frac{1}{n} \log \frac{1}{{z}} \sqrt{\frac{2\pi \sigma_p^2 {\sigma^2}}{ {\sigma^2}- {2}n\delta^2 \sigma_p^2}}^d \frac{1}{\sqrt{2\pi \sigma_p^2}^d} +   \frac{1}{n} \log \frac{1}{{z}} \\
    &=  \frac{2}{n} \log \frac{1}{{z}} + \frac{d}{2n} \log \frac{1}{1- {\frac{2}{\sigma^2}}n\delta^2 \sigma_p^2} \\
    &\leq  \frac{2}{n} \log \frac{1}{{z}} + \frac{d}{2n} \frac{ {\frac{2}{\sigma^2}}n\delta^2 \sigma_p^2}{1- {\frac{2}{\sigma^2}}n\delta^2 \sigma_p^2} \quad; -\log(1-x) \leq \frac{x}{1-x} \\
    &= \frac{2}{n} \log \frac{1}{{z}} + \frac{d\delta^2 \sigma_p^2 {/\sigma^2}}{1-2n\delta^2 \sigma_p^2 {/\sigma^2}} \stepcounter{equation}\tag{\theequation}\label{eq:std_post_adv_loss_normz}
\end{align*}

We obtain \Cref{thm:std_post_adv_loss} by substituting \Cref{lm:neg_log_z_bayes} in \Cref{eq:std_post_adv_loss_normz} and combining it with the \Cref{th:pac_bayes_bounded_cgf_t1}.
\qed

\subsection{Standard generalization of robust posterior (\MakeLowercase{\Cref{thm:adv_post_std_loss})}}

In this case, $\rho = q_\delta$ and $\ell$ is NLL in \Cref{eq:eff_pac_bound}. We upper bound it as follows.

For ease of notation, only in the following, we use short hand notations for $\ell$ to denote $\ell(\theta, \mathcal{D})$ and $\ell_{\delta}$ to denote $\ell_{\delta}(\theta, \mathcal{D})$.

\begin{align*}
    \expectation{\theta \sim { q_\delta }}{\frac{1}{n}{\ell}(\theta, \mathcal{D})} + \frac{1}{n} \mathrm{KL}({ q_\delta } \| \pi) 
    &= \frac{1}{n} \int \frac{1}{ z_\delta }{\ell}\exp(-\ell_{\delta})\pi(\theta) d\theta + \frac{1}{n} \int \frac{1}{ z_\delta } \exp(-\ell_{\delta})\pi(\theta) \log \lp \frac{\exp(-\ell_{\delta})\pi(\theta)}{ z_\delta \pi(\theta)} \rp d\theta \\
    &= \frac{1}{n} \int \frac{1}{ z_\delta }\lp -\ell_{\delta}+ \ell \rp\exp(-\ell_{\delta})\pi(\theta) d\theta + \frac{1}{n} \int \frac{1}{ z_\delta } \exp(-\ell_{\delta})\pi(\theta) \log \lp \frac{1}{ z_\delta } \rp d\theta \\
    &\leq \frac{1}{n} \int \frac{1}{ z_\delta }\lp -\ell - {\frac{1}{2\sigma^2}}n \|\theta\|^2 \delta^2 + \ell \rp\exp(-\ell_{\delta})\pi(\theta) d\theta + \frac{1}{n} \log \frac{1}{ z_\delta } \quad; \text{Lemma \ref{lm:adv_loss_bounds}} \\
    &\leq \frac{1}{n} \log \int \frac{1}{ z_\delta } \exp \lp -{\frac{1}{2\sigma^2}}n \|\theta\|^2 \delta^2 \rp \exp(-\ell_{\delta})\pi(\theta) d\theta +  \frac{1}{n} \log \frac{1}{ z_\delta } \quad; \text{Jensen's ineq.} \\
    % &= \frac{1}{n} \log \frac{\int  \exp \lp \lp -2n \delta^2 - \frac{1}{2\sigma_p^2} \rp \|\theta\|^2 - \ell \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta}{\int \exp(-\ell_{\delta})\pi(\theta) d\theta} +  \frac{1}{n} \log \frac{1}{ z_\delta } \\
    &= \frac{1}{n} \log \int  \exp \lp \lp -{\frac{1}{2\sigma^2}}2n \delta^2 - \frac{1}{2\sigma_p^2} \rp \|\theta\|^2 - \ell \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta  +  \frac{2}{n} \log \frac{1}{ z_\delta } \stepcounter{equation}\tag{\theequation}\label{eq:adv_post_std_loss_normz}
    %\\
    % &= \frac{1}{n} \log \int  \exp \lp \theta^T \lp -X^TX - \lp 2n \delta^2 +\frac{1}{2\sigma_p^2} \rp I \rp \theta - Y^TY + 2Y^TX\theta \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta \\
    % &\qquad \qquad +  \frac{2}{n} \log \frac{1}{ z_\delta } \\
    % &\leq \frac{1}{n} \log \int \frac{1}{ z_\delta } \exp(-2\ell_{\delta}+\ell)\pi(\theta) d\theta + \frac{1}{n} \log \frac{1}{ z_\delta } \quad; \text{Jensen's ineq.} \\
    % &\leq \frac{1}{n} \log \int \frac{1}{ z_\delta } \exp(\ell)\pi(\theta) d\theta + \frac{1}{n} \log \frac{1}{ z_\delta }  \quad; \ref{eq:2ladv-l}
\end{align*}

The evaluation of the above integral $\int  \exp \lp \lp -{\frac{1}{2\sigma^2}}2n \delta^2 - \frac{1}{2\sigma_p^2} \rp \|\theta\|^2 - \ell \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta  +  \frac{2}{n} \log \frac{1}{ z_\delta }$ is as follows:
\begin{align*}
    % z_\delta  &= \int \exp\Big( -\ell_{\delta}(Y \mid X, \theta) \Big) \pi(\theta) d\theta \\
    %&\geq \int \exp\Big( -2 \sum_{i=1}^n \Big( (y_i - x_i^\top \theta)^2 + \Vert \theta \Vert^2 \delta^2 \Big) \Big) \pi(\theta) d\theta \quad; \text{note this needs to be bounded in \ref{eq:chi-sq_bound}}\\
    &= \frac{1}{\sqrt{2\pi\sigma^2_p}^d}\int \exp\Bigg( -\frac{1}{2} \Big( {\frac{1}{2\sigma^2}}4 n \Vert \theta \Vert^2 \delta^2 + \frac{1}{\sigma^2_p}\Vert \theta \Vert^2 + {\frac{2}{2\sigma^2}} \Vert Y - X\theta \Vert^2  \Big) \Bigg)  d\theta  \quad; \pi(\theta) \sim \mathcal{N}(0,\sigma^2_p I)\\
    &= \frac{1}{\sqrt{2\pi\sigma^2_p}^d}\int \exp\Bigg( -\frac{1}{2} \Big( {\frac{1}{2\sigma^2}}4 n\delta^2 \theta^\top \theta  + \frac{1}{\sigma^2_p}\theta^\top \theta + {\frac{1}{2\sigma^2}} \lp 2 \Vert Y \Vert^2 - 4 Y^\top X \theta + 2 \theta^\top X^\top X \theta \rp \Big) \Bigg)  d\theta  \\
    &= \frac{1}{\sqrt{2\pi\sigma^2_p}^d} \exp(-{\frac{1}{2\sigma^2}}\Vert Y\Vert^2) \int \exp\Bigg( - \theta^\top \big( ({\frac{1}{2\sigma^2}}2 n\delta^2 + \frac{1}{2\sigma^2_p})I + {\frac{1}{2\sigma^2}} X^\top X  \big)\theta + {\frac{2}{2\sigma^2}} Y^\top X\theta \Bigg)  d\theta \quad; \text{Lemma \ref{lm:std_gaussian_int}}  \\
    &=  \sqrt{\frac{1}{\det \lp  {\frac{\sigma_p^2}{\sigma^2}}X^\top X + ( {\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp}} \exp(- {\frac{1}{2\sigma^2}}\Vert Y\Vert^2) \exp\lp  {\frac{1}{2\sigma^4}} Y^TX \lp  {\frac{1}{\sigma^2}} X^\top X + ( {\frac{1}{\sigma^2}}2 n\delta^2 + \frac{1}{\sigma^2_p})I\rp^{-1}X^TY\rp \\
    &=  \sqrt{\frac{1}{\det \lp  {\frac{\sigma_p^2}{\sigma^2}}X^\top X + ( {\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp}} \exp  {\frac{1}{2\sigma^2}} \lp - Y^T \lp I -  {\frac{1}{\sigma^2}}X \lp  {\frac{1}{\sigma^2}}X^\top X + ( {\frac{1}{\sigma^2}}2 n\delta^2 + \frac{1}{\sigma^2_p})I\rp^{-1}X^T\rp Y \rp \\
     &=  \sqrt{\frac{1}{\det \lp  {\frac{\sigma_p^2}{\sigma^2}}X^\top X + ( {\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp}} \exp \lp -  {\frac{1}{2\sigma^2}}Y^T \lp I +  {\frac{1}{\sigma^2}}XX^T \lp \frac{\sigma_p^2\sigma^2}{\sigma^2+2 n \delta^2 \sigma^2_p}\rp\rp^{-1} Y \rp \\
     %&=  \sqrt{\frac{1}{ \det \lp  {\frac{\sigma_p^2}{\sigma^2}}X^\top X + ( {\frac{\sigma_p^2}{\sigma^2}}2 n\delta^2 + 1)I\rp}} \exp \lp -  {\frac{1+2n\delta^2\sigma_p^2/\sigma^2}{2\sigma^2}}Y^T \lp I \lp \frac{\sigma_p^2}{\sigma^2}2n\delta^2 + 1 \rp + {\frac{\sigma_p^2}{\sigma^2}}XX^T \rp^{-1} Y \rp 
     \stepcounter{equation}\tag{\theequation}\label{eq:adv_post_std_loss_normz_2}
\end{align*}
Therefore, we obtain \Cref{thm:adv_post_std_loss} by substituting \Cref{eq:adv_post_std_loss_normz_2} in \Cref{eq:adv_post_std_loss_normz} and combining it with the \Cref{th:pac_bayes_bounded_cgf_t1}.
\qed

\clearpage
\subsection{Adversarial generalization of robust posterior when $\delta \ne \deltat$ (\MakeLowercase{\Cref{thm:adv_post_adv_loss_gen})}}

In this case, $\rho = q_\delta$ and $\ell$ is adversarial NLL with $\deltat$ perturbation in \Cref{eq:eff_pac_bound}. We upper bound it as follows.
%
For ease of notation, only in the following, we use short hand notations for $\ell_\delta$ to denote $\ell_\delta(\theta, \mathcal{D})$ and $\ell_{\deltat}$ to denote $\ell_{\deltat}(\theta, \mathcal{D})$.

\begin{align*}
    \expectation{\theta \sim {q_\delta}}{\frac{1}{n}{\ell}_{\deltat}(\theta, \mathcal{D})} + \frac{1}{n} \mathrm{KL}({q_\delta} \| \pi) 
    &= \frac{1}{n} \int \frac{1}{z_\delta }{\ell}_{\deltat}\exp(-{\ell}_\delta)\pi(\theta) d\theta + \frac{1}{n} \int \frac{1}{z_\delta } \exp(-{\ell}_\delta)\pi(\theta) \log \lp \frac{\exp(-{\ell}_\delta)\pi(\theta)}{z_\delta \pi(\theta)} \rp d\theta \\
    &= \frac{1}{n} \int \frac{1}{z_\delta }\lp {\ell}_{\deltat} - {\ell}_\delta \rp\exp(-{\ell}_\delta)\pi(\theta) d\theta + \frac{1}{n} \int \frac{1}{z_\delta } \exp(-{\ell}_\delta)\pi(\theta) \log \lp \frac{1}{z_\delta } \rp d\theta \\
    &\leq  \frac{1}{n} \int \frac{1}{z_\delta }\lp 2\ell + \frac{2n \| \theta \|^2 \deltat^2}{2\sigma^2} - \ell - \frac{n\|\theta \|^2\delta^2}{2\sigma^2} \rp\exp(-{\ell}_\delta)\pi(\theta) d\theta + \frac{1}{n}  \log \frac{1}{z_\delta } \quad; \text{Lemma \ref{lm:adv_loss_bounds}} \\
    &\leq \frac{1}{n} \log \int \frac{1}{z_\delta } \exp \lp \ell + \frac{n \|\theta\|^2 \lp 2\deltat^2 - \delta^2\rp}{2\sigma^2} \rp \exp(-{\ell}_\delta)\pi(\theta) d\theta +  \frac{1}{n} \log \frac{1}{z_\delta } \quad; \text{Jensen's ineq.} \\
    &= \frac{1}{n} \log \frac{1}{z_\delta } \int  \exp \lp \lp \frac{n \lp 2\deltat^2 - 2\delta^2 \rp}{2\sigma^2} - \frac{1}{2\sigma_p^2} \rp \|\theta\|^2  \rp \frac{1}{\sqrt{2\pi \sigma_p^2}^d} d\theta +  \frac{1}{n} \log \frac{1}{z_\delta } \\
    &= \frac{1}{n} \log \frac{1}{z_\delta } \sqrt{\frac{2\pi \sigma_p^2}{1-2n\lp \deltat^2 -\delta^2 \rp \sigma_p^2/\sigma^2}}^d \frac{1}{\sqrt{2\pi \sigma_p^2}^d} +   \frac{1}{n} \log \frac{1}{z_\delta } \\
    &=  \frac{2}{n} \log \frac{1}{z_\delta } + \frac{d}{2n} \log \frac{1}{1-2n\lp \deltat^2 -\delta^2 \rp \sigma_p^2/\sigma^2} \\
    &\leq  \frac{2}{n} \log \frac{1}{z_\delta } + \frac{d}{2n} \frac{2n\lp \deltat^2 -\delta^2 \rp \sigma_p^2/\sigma^2}{1-2n\lp \deltat^2 -\delta^2 \rp \sigma_p^2/\sigma^2} \quad; -\log(1-x) \leq \frac{x}{1-x} \\
    &= \frac{2}{n} \log \frac{1}{z_\delta } + \frac{\lp \deltat^2 -\delta^2 \rp \sigma_p^2 d/\sigma^2}{1-2n\lp \deltat^2 -\delta^2 \rp \sigma_p^2/\sigma^2} \stepcounter{equation}\tag{\theequation}\label{eq:adv_post_adv_loss_normz_gen}
\end{align*}

Substituting \Cref{lm:neg_log_z_robust} in \Cref{eq:adv_post_adv_loss_normz_gen} and combining it with the \Cref{th:pac_bayes_bounded_cgf_t1} proves \Cref{thm:adv_post_adv_loss_gen}.

\FloatBarrier