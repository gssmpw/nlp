\section{Preliminaries}
\label{sec:prelim}
Our work combines elements from adversarial robustness and probabilistic inference.
We briefly outline these topics here, and establish some preliminaries.
\paragraph{Notation}
%We denote expectation with $\expectation{}{\cdot}$, and use $\mathcal{N}(\mu, \Sigma)$ for Gaussian distribution with mean vector $\mu$ and positive semidefinite covariance matrix $\Sigma$. 
%We represent determinant of matrix $M$ as $\det(M)$, 
We represent the entry-wise absolute value of matrix $M$ as $|M|$, and vector Euclidean norm as $\| \cdot \|$.
We use $I_n$ for identity matrix of size $n \times n$ %whose dimension is understood from the context.
and $1_n$ for a vector of size $n$ with all ones.
We denote by $\ell\big(\theta, (x,y) \big)$ a loss evaluated on parameter $\theta$ and single $(x,y)$ data pair, and use $\mathcal{L}(\theta, \mathcal{D}) = \sum_{i=1}^n \ell\big(\theta, (x_i,y_i) \big)$ for the sum of the losses over a dataset.
The expected and empirical average errors are $R(\theta)=\expectation{(x,y) \sim \mathcal{P}}{\ell\big(\theta, (x,y)\big)}$ and $r(\theta)=\frac{1}{n} \mathcal{L}(\theta, \mathcal{D})$.

\subsection{Adversarial robustness}
We are given $n$ labeled data samples drawn i.i.d. from an unknown probability measure $\mathcal{P}$, denoted by $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^n$ with $x_i \in \mathbb{X} \subseteq \mathbb{R}^d$ representing the feature vector with the corresponding label $y_i \in \mathbb{R}$. We later use $X\in\mathbb{R}^{n\times d}$ to refer to the matrix of all feature vectors, and $Y \in \mathbb{R}^n$ for the vector of all $n$ labels.
We follow the standard setting of supervised learning~\citep{bishop2007pattern,deisenroth20mathml}, where we minimize the empirical loss $\ell$ on the training set $\mathcal{D} = (X,Y) = \{x_i, y_i\}_{i=1}^n$ with respect to the parameters $\theta$ of our model,
\begin{align*}
    \theta^\ast = \argmin_{\theta} \sum_{i=1}^n \ell\big(\theta, (x_i, y_i) \big) =  \argmin_{\theta} \mathcal{L}(\theta, \mathcal{D}).
\end{align*}
For the adversarially robust setting, we consider perturbed data $\widetilde{x}_i$ but not perturbed labels. 
Following typical adversarial constructs~\citep[for example]{szegedy2013intriguing} we consider perturbations whose $\ell_2$ distance is bounded by a user-defined constant $\delta$.
\begin{align*}
    \theta^\ast = \argmin_{\theta} \sum_{i=1}^n \max\limits_{\Vert \widetilde{x}_i - x_i \Vert \leq \delta}\ell\big(\theta, (\widetilde{x}_i, y_i) \big).
\end{align*}
Here we focus on the parametric supervised setting, where a parameter $\theta$ is mapped to a prediction $f_\theta(x_i)$ on a feature $x_i$ with some parameterized function $f_\theta:\mathbb{X} \to \mathbb{F}$.




\subsection{Probabilistic inference}
% \textbf{Generalized variational inference.} 
\paragraph{Bayesian inference}
Of central interest in Bayesian inference is the posterior $q(\theta \mid \mathcal{D})$. 
The posterior reflects a belief of an unknown quantity of interest $\theta$ updated from a prior belief $p(\theta)$ in light of the likelihood $p(\mathcal{D} \mid \theta) = \prod_{i=1}^n p(y_i \mid x_i, \theta)$ of observations under the model.
One way to compute this update is via Bayes' rule, $q(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) p(\theta)$.
%In this work, we consider an isotropic Gaussian prior of mean zero and variance $\sigma_p^2$: $\theta \sim \mathcal{N}(0, \sigma^2_p I)$ where $I$ is identity matrix. 
An alternate optimization-centric perspective of Bayesian inference, introduced by \citet{csiszar1975divergence,donsker1983asymptotic},
%Recent advancements, such as the optimization-centric perspective of Bayesian inference introduced by \citet{knoblauch2022optimization}, 
reformulates the objective of deriving the Bayesian posterior as solving an optimization problem. 
Specifically, the Bayesian posterior distribution $q(\theta \mid \mathcal{D})$ is obtained by minimizing a variational objective,
\begin{align}
	q(\theta \mid \mathcal{D}) = \arg\min_{\rho \in \Pi} \expectation{\theta \sim \rho}{\text{-}\log p(\mathcal{D} \mid \theta)} + KL(\rho \Vert \pi), \label{eq:general_vi}
\end{align} 
where $\Pi$ is the space of all probability measures, $\pi$ is the prior on $\theta$, $\text{-}\log p(\mathcal{D} \mid \theta)$ is the negative log-likelihood loss on the data, and $KL(\rho \Vert \pi) =  \expectation{\theta \sim \rho}{\log \frac{\rho(\theta)}{\pi(\theta)}}$ is the Kullback-Leibler (KL) divergence. 

\paragraph{Generalized Bayesian inference}
Unfortunately, even under the optimization-centric view, Bayesian inference suffers from some limitations.
First, the normalizing constant and/or optimization problem can be intractable.
Second, the prior is often chosen for convenience and, particularly in large models, may not be truly calibrated to the statistician's prior beliefs.
Third, the likelihood is often also chosen for convenience, very often corresponding with losses which are not robust.
The Rule of Three (ROT)~\citep{knoblauch2022optimization} generalizes standard Bayesian inference via the optimization view, addressing the limitations above. %the need of being able to perform tractable inference in the case of misspecified priors, misspecified likelihoods and finite computing resources. 
Generalizing~\eqref{eq:general_vi}, the ROT replaces the negative log likelihood (NLL) with an arbitrary loss function $\mathcal{L}$, the KL divergence $KL$ with an arbitrary divergence $D$, and  the space $\Pi$ of all probability measures with a subset of all probability measures $\Lambda$,
\begin{align}
	q(\theta \mid \mathcal{D}) = \arg\min_{\rho \in \Lambda} \expectation{\theta \sim \rho}{\mathcal{L}(\theta, \mathcal{D})} + D(\rho \Vert \pi). \label{eq:rot}
\end{align} 
The ROT has axiomatic foundations and also comes with guarantees on estimation procedures. 
%It also provides for a flexible way to derive posteriors, particularly in non-standard settings. 
The variational objective balances two competing terms: $(i)$ the expected loss term, which encourages the posterior to assign a higher probability to parameters that fit the data well, and $(ii)$ the divergence term, which regularizes the posterior by penalizing deviations from the prior.

\paragraph{Gibbs Bayesian inference}
As a special case of~\eqref{eq:rot}, the Gibbs posterior addresses the problem of mis-specified and non-robust likelihoods, and also partially addresses the problem of intractability. 
The Gibbs posterior is obtained by retaining the KL divergence and space of all probability measures $\Pi$ from~\eqref{eq:general_vi} in~\eqref{eq:rot}, but using a general loss $\mathcal{L}(\theta, \mathcal{D})$ in place of the NLL $\text{-}\log p(\mathcal{D} \mid \cdot)$.
%This formulation, known as the \emph{PAC/Gibbs Bayes inference} \citep{knoblauch2022optimization}, provides a flexible way to derive posteriors, particularly in non-standard settings. The variational objective balances two competing terms: $(1)$ the expected loss term, which encourages the posterior to assign higher probability to parameters that fit the data well, and $(2)$ the KL divergence term, which regularizes the posterior by penalizing deviations from the prior. %to remain close to the prior. 
In this case, the minimizer~\eqref{eq:rot} is called %the general loss $\mathcal{L}(\theta, \mathcal{D})$ in \Cref{eq:general_vi} is 
the \emph{Gibbs posterior}, and admits a closed-form (up to the normalizing constant)~\citep[for example]{alquier2016properties,knoblauch2022optimization},% \citep{knoblauch2022optimization},
\begin{align*}
	q(\theta \mid \mathcal{D}) &= \arg\min_{\rho \in \Pi} \expectation{\theta \sim \rho}{\mathcal{L}(\theta, \mathcal{D})} + KL(\rho \Vert \pi) \\
    &=\frac{\exp\big( - \mathcal{L}( \theta, \mathcal{D}) \big) \pi(\theta) }{\int \exp\big( - \mathcal{L}( \theta', \mathcal{D}) \big) \pi(\theta') d\theta' }. \numberthis \label{eq:gibbs_posterior}
\end{align*}

\paragraph{Gaussian linear regression}
A notable special case 
arises when the loss is chosen as the negative log-likelihood $\mathcal{L}(\theta, \mathcal{D}) = - \log p(\mathcal{D} \mid \theta) = 
\sum_{i=1}^n - \log p(y_i \mid x_i, \theta)$ with isotropic Gaussian prior $\pi \sim \mathcal{N}(0, \sigma_p^2 I_d)$, as it recovers the standard Bayes posterior 
\iffalse
$q(\theta)$, where
$$q(\theta) = \frac{p(\mathcal{D} \mid \theta) \pi(\theta)}{\int p(\mathcal{D} \mid \theta^\prime)\pi(\theta^\prime) d\theta^\prime} = \mathcal{N}\lp \hat{\theta}, \Sigma \rp$$
\fi
$q(\theta)= \mathcal{N}\lp \hat{\theta}, \Sigma \rp$, where $\Sigma = \frac{1}{\sigma^2}X^\top X + \frac{1}{\sigma_p^2}I_d$ and $\hat{\theta} = \frac{1}{\sigma^2}\Sigma^{-1}X^\top Y$ \citep{bishop2007pattern}.
In this work, we consider an isotropic Gaussian prior of mean zero and variance $\sigma_p^2$: $\theta \sim \mathcal{N}(0, \sigma^2_p I)$ 
and denote the %standard Bayes posterior as $q(\theta)$ and its corresponding 
negative log-likelihood loss on $\mathcal{D}$ as $\ell(\theta, \mathcal{D})$ which is 
$$\ell(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \|Y - X\theta\|^2.$$

\iffalse
\textbf{Adversarial loss and adversarially robust posterior.} 
We define a novel adversarial loss $ {\ell}_\delta(\theta, \mathcal{D})$ in the Bayesian framework as 
\begin{align}
     {\ell}_\delta(\theta, \mathcal{D}) = \sum_{i=1}^n \max_{\|  \widetilde{x}_i - x_i \|_2 \leq \delta} - \log p(y_i \mid  \widetilde{x}_i, \theta) \label{eq:adv_loss}
\end{align} 
where $\delta$ controls the allowable perturbation in the features. The corresponding adversarially robust posterior $ {q}_\delta(\theta)$ is the Gibbs posterior with loss $\mathcal{L}(\theta, \mathcal{D}) =  {\ell}_\delta(\theta, \mathcal{D})$. This formulation is motivated by its equivalence to the adversarial training objective as proved in the following lemma. 

\begin{lemma}[Equivalence between adversarial loss and adversarial training] 
The point estimate obtained by optimizing $\arg\min_{\theta}  {\ell}_\delta(\theta, \mathcal{D})$ is equivalent to performing adversarial training with objective $\arg \min_{\theta} \sum_{i=1}^n \max_{\| \widetilde{x}_i -x_i\|_2 \leq \delta} \lp  \widetilde{x}_i^\top \theta - y_i \rp^2$.
\label{lm:adv_loss_adv_training}
\end{lemma}
\fi

%\subsection{Bayesian linear regression with exponential families}
\subsection{Linking loss functions and probability distributions}
We consider probabilistic models $p\big(y \mid f_\theta(x) \big)$ that belong to an exponential family
and associate the NLL with the notion of empirical loss $\ell$ via Bregman divergences, and vice versa.
When such losses are later subject to adversarial perturbation, this allows us to make use of the geometrical properties of the Bregman divergence (more specifically, the law of cosines) to study adversarial extensions of probabilistic models in~\Cref{lm:adv_loss_closed_form_gaussian,lm:adv_loss_closed_form}.


\paragraph{Exponential families} Generalizing Gausssian families, exponential families provide a flexible and theoretically tractable class of probability distributions~\citep[\S~6.6.3]{deisenroth20mathml}. 
For our purposes, it suffices to consider $1$ dimensional (and therefore minimal) exponential families.
Let $t:\mathbb{F} \to \mathbb{R}$ be a measurable function called a \emph{sufficient statistic}.
Let $\mu$ be a nonnegative measure, called the \emph{base measure}, defined on some appropriate sigma algebra generated by $\mathbb{F}$.
An exponential family is the set of all probability distributions (with respect to base measure $\mu$) parameterized by natural parameter $\eta$ of the form
\begin{align*}
    p(y \mid \eta) &= \exp\big( \eta t(y) - \phi(\eta) \big),
\end{align*}
where $\phi(\eta) = \log \int_{\mathbb{F}} \exp\big( \eta t(y) \big) \, \mu(dy)$ is called the log normalizing constant, such that $\phi(\eta) \in \mathbb{R}$.
We assume an extremely mild condition on exponential families, that they are \emph{regular}.
Regular means that the set of all $\eta$ such that $\phi(\eta) \in \mathbb{R}$ is an open set.
Proposition 2 of~\citet{wainwright2008graphical} then states that $\phi$ is a strictly convex function.


\paragraph{Bregman divergence}
Generalizing the \textbf{squared} Euclidean distance, Bregman divergences allow for a natural class of loss functions for use in a wide variety of supervised and unsupervised applications.
Assume $\mathbb{F}$ is a convex set and let $\phi:\mathbb{F} \to \mathbb{R}$ be a continuously differentiable and strictly convex function (so called generator).
The \emph{Bregman divergence} $d_\phi:\mathbb{F} \times \mathbb{F} \to \mathbb{R}$ generated by $\phi$ is defined by
\begin{align*}
    d_\phi(y_1, y_2) = \phi(y_1) - \phi(y_2) - \nabla \phi(y_2)^\top (y_1 - y_2),
\end{align*}
and is strictly convex in its first argument.

\iffalse
The dual Bregman divergence is generated by the convex conjugate $\phi^\ast$ of $\phi$. 
The Bregman divergence satisfies a dual reversal property, which for our purposes may be stated as
\begin{align*}
    d_{\phi^*}(y_1, y_2) = \phi(y_2^\ast) - \phi(y_1^\ast) - y_1^\top (y_2^\ast - y_1^\ast), \numberthis \label{eq:dual_reversal}
\end{align*}
where the dual coordinates are $y_1^\ast = (\nabla \phi)^{-1}(y_1)$ and $y_2^\ast = (\nabla \phi)^{-1}(y_1)$.
\fi

A link between geometric loss functions in Bregman divergences and probabilistic loss functions in NLLs is provided through the fact that (informally speaking) every NLL of an exponential family is a Bregman divergence.
More precisely, in our current context, if $p\big(y \mid f_\theta(x) \big)$ belongs to a regular exponential family with  log normalizing function $\phi$ and natural parameter $\eta = f_\theta(x)$, then by~\citet[Theorem 4]{banerjee2005clustering},%and a dual reversal property of Bregman divergences,
\begin{align*}
    - \log p\big(y \mid f_\theta(x) \big) &= d_\phi\big(f_\theta(x), y^\ast \big) + C(y) \numberthis \label{eq:exp_bregman_link}
\end{align*}
where $C(y)$ is an additive constant independent of $\theta$ and $x$ available in closed-form, and $y^\ast = (\nabla \phi)^{-1}(y)$ is the dual coordinate of $y$.
Note that some technical care is required in ensuring that the dual coordinate $y^\ast$ lies in the effective domain of the divergence $d_\phi$, and~\eqref{eq:exp_bregman_link} is a slight abuse of notation since $y^\ast$ may be $\pm\infty$, but nevertheless the divergence $d_\phi$ itself remains well defined on an appropriate extension of its domain.
See~\citet[Example 8]{banerjee2005clustering} for an example.
The special and uniquely symmetric case of squared Euclidean distance is obtained when $\phi(y) = \Vert y \Vert_2^2$.







