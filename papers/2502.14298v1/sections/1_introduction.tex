\section{Introduction}
\label{sec:intro}
Machine learning models are vulnerable to adversarial inputs, where small, carefully crafted perturbations to the input data can significantly degrade model performance. 
These perturbations, though imperceptible to humans (e.g. in computer vision contexts), can cause models to make incorrect predictions with high confidence. 
Significant progress has been made in understanding and improving adversarial robustness of point predictors, with efforts in defense mechanisms, attack strategies, and the trade-offs between robustness and generalization \citep{szegedy2013intriguing, shhafahi2019are, li2023adersarial}. 
A key insight from this body of research is that models  susceptible to adversarial attacks often exhibit near-perfect empirical generalization --— achieving similar performance on training and test data~\citep{goodfellow2014explaining}.%under standard evaluation settings \citep{goodfellow2014explaining}. 
This observation suggests that deriving formal guarantees is critical to understanding the interplay between generalization and adversarial robustness. %  is critical. %for developing models that are both accurate and reliable.
%This observation underscores the need for formal guarantees that account for both generalization and robustness.


Probabilistic models %, particularly those based on Bayesian inference, 
offer an alternative paradigm that quantifies uncertainty, a property that can detect adversarial inputs and reject uncertain predictions. 
However, despite these advantages, probabilistic models have received significantly less attention than their non-probabilistic counterparts in adversarial settings \citep{bradshaw2017adversarial, grosse2018limitations}. 
While they have been studied for robustness against outliers \citep{kim2008outlier}, label noise \citep{hernandez2011robust}, and domain shifts \citep{ovadia2019can}, their susceptibility to adversarial attacks remains largely unexplored. 
There exists no notion of adversarially robust probabilistic inference, and hence no formal generalization guarantees,
%This gap in the literature raises the following fundamental questions:
thus raising a fundamental question:

% earlier version
\begin{comment}
Despite extensive studies on adversarial robustness in deterministic machine learning models, probabilistic models—particularly those based on Bayesian inference—remain underexplored \citep{bradshaw2017adversarial, grosse2018limitations}. Bayesian methods offer inherent uncertainty quantification, which is often seen as a desirable property for robustness as it enables detecting the adversarial inputs and rejecting uncertain predictions. However, while Bayesian models have been studied in contexts such as outlier detection \citep{kim2008outlier}, label noise \citep{hernandez2011robust}, and domain adaptation, their susceptibility to adversarial attacks remains largely unexamined. 
In particular, there exists no well-defined notion of adversarially robust Bayesian inference, therefore, no formal generalization guarantees for such a robust Bayesian model,
%This gap in the literature raises the following fundamental questions:
thus raising a fundamental question:
\end{comment}

\emph{How can we define and develop adversarially robust probabilistic inference and derive generalization certificates (formal guarantees) for such models?}

In this work, we address this question by first introducing the notion of \emph{adversarially robust posteriors}. 
We achieve this by formulating an adversarial variant of the negative log-likelihood (NLL) loss --- drawing inspiration from adversarial training, one of the most effective defense strategies against adversarial attacks in standard machine learning~\citep{madry2018towards} --- and taking an optimization-centric perspective of (generalized) Bayesian inference \citep{alquier2016properties}.
In doing so, we obtain a posterior that is robust to adversarial perturbations.
%For Bayesian linear regression, we establish generalization guarantees for the robust posterior. 
We leverage the PAC-Bayesian framework, a powerful tool for deriving data-dependent generalization bounds for Bayesian predictors %such as aggregated and randomized predictors 
\citep{mcallester1998some,catoni2004statistical}, and derive the \emph{first} rigorous certificates for the robust posterior on linear regression.
Furthermore, we also derive PAC-Bayesian based generalization certificates for Bayes posterior obtained using the standard negative log-likelihood loss.

In summary, our main \textbf{contributions} are as follows.
%by optimizing an adversarial loss that we define in the Bayesian contexts. The adversarial loss is defined by taking inspiration from one of the popular and effective defense techniques to learn an adversarially robust non-Bayesian model called adversarial training \citep{madry2018towards}. 
\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
    \toprule
    & Standard generalization & Adversarial generalization   \\
         & NLL $\ell$ & Adversarial NLL $\ell_{\deltat}$  \\
         & $R(\theta)=\expectation{\mathcal{D} \sim \mathcal{P}}{\ell(\theta, \mathcal{D})}$ & $R_{\deltat}(\theta)=\expectation{\mathcal{D} \sim \mathcal{P}}{\ell_{\deltat}(\theta, \mathcal{D})}$  \\
         \midrule
         \multirow{2}{*}{Bayes posterior $q$} & $\expectation{\theta \sim q}{R(\theta)}$ & $\expectation{\theta \sim q}{R_{\deltat}(\theta))}$   \\
         & \Cref{thm:std_post_std_loss} & \Cref{thm:std_post_adv_loss} \\
         \midrule
         \multirow{2}{*}{Robust posterior $q_\delta$} & $\expectation{\theta \sim q_\delta}{R(\theta)}$ & $\expectation{\theta \sim q_\delta}{R_{\deltat}(\theta)}$ \\
         & \Cref{thm:adv_post_std_loss} & Theorems~\ref{thm:adv_post_adv_loss} and \ref{thm:adv_post_adv_loss_gen}\\
         \bottomrule
    \end{tabular}} %
    \caption{Overview of our derived generalization certificates. The guarantees are derived for the standard \emph{Bayes posterior} $q$ and the novel \emph{robust posterior} $q_{\delta}$, where $\delta$ denotes the training adversarial allowance. We consider generalization to standard NLL loss (\emph{standard generalization}) as well as adversarial NLL (\emph{adversarial generalization}), with a potentially different adversarial allowance $\deltat$. \label{tab:overview}}
    \vspace{-0.4cm}
\end{table}
%\textbf{Contributions.} The main contributions of our work are:
%$(i)$ We take an optimization-centric perspective of Bayesian inference and introduce a novel adversarial negative log-likelihood loss in \Cref{sec:prelim}. Using this loss and minimizing the PAC/Gibbs Bayes inference objective, we obtain the adversarially robust posterior (\Cref{sec:prelim}).
\begin{enumerate}[($i$), topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item In exponential families, we review a one-to-one correspondence between the adversarial negative log-likelihood loss and the class of Bregman divergences. % in \Cref{sec:prelim}. 
    Based on this correspondence, we introduce a novel adversarial negative log-likelihood loss in \Cref{sec:adversarially-robust-posterior}. 
    This probabilistic-geometric connection allows us to solve the adversarial perturbation problem in closed-form, for all exponential families, allowing for an adversarially robust formulation of generalized linear models.
    \item We define the adversarially \emph{robust posterior} as minimizing a variational objective with this loss, thereby extending Bayesian inference to adversarially robust generalized linear models settings, in \Cref{sec:adversarially-robust-posterior}.
%By minimizing the variational inference objective with this loss (see \Cref{sec:prelim} for details), we derive the adversarially robust posterior, extending Bayesian inference to adversarial settings.
\item In \Cref{sec:pac_bayes_certs}, focusing on the case of a Gaussian family, we derive the PAC-Bayesian generalization certificates for the Bayes posterior and the robust posterior under two settings: %$a)$ generalization to standard negative log-likelihood loss which we call as \textit{standard generalization} (\Cref{thm:std_post_std_loss,thm:std_post_adv_loss}), and $b)$ generalization to adversarial negative log-likelihood loss which we call as \textit{adversarial generalization} (\Cref{thm:adv_post_std_loss,thm:adv_post_adv_loss_gen}).
$a)$ \emph{standard generalization}: guarantees for standard negative log-likelihood loss (\Cref{thm:std_post_std_loss,thm:std_post_adv_loss}),
and $b)$ \emph{adversarial generalization}: guarantees for adversarial negative log-likelihood loss (\Cref{thm:adv_post_std_loss,thm:adv_post_adv_loss,thm:adv_post_adv_loss_gen}). 
\Cref{tab:overview} gives an overview of our bounds. 
%\item 
We experimentally validate the derived certificates in \Cref{sec:exp} showing non-trivial guarantees.
\end{enumerate}
We discuss the practical significance of the bounds, several technicalities, %extension to non-linear settings 
and touch on related works in \Cref{sec:related_works};  and conclude in \Cref{sec:conclusion}.






