\section{Adversarially robust generalized linear models}
\label{sec:adversarially-robust-posterior}

In this section, we derive the adversarially robust posterior $q_\delta(\theta)$ when the likelihood is respectively a Gaussian likelihood, and more generally an exponential family likelihood.
The result in~\Cref{lm:adv_loss_closed_form} may be of independent interest for studying adversarially robust models even in the setting of point-estimation.
It allows, for example, an adversarially robust extension of  logistic regression (binary-valued data), Poisson regression (count-valued data), and exponential or gamma regression (positive-valued data).
More generally, any generalized linear model~\citep{mccullagh1989generalized} with canonical link function may be adversarialized. 

\paragraph{Adversarial negative log likelihood} 
We consider adversarial losses $ {\ell}_\delta(\theta, \mathcal{D})$ of the form
\iffalse
\begin{align*}
     {\ell}_\delta(\theta, \mathcal{D}) &= \sum_{i=1}^n \max_{\|  \widetilde{x}_i - x_i \|_2 \leq \delta} - \log p\big(y_i \mid  f_\theta(\widetilde{x}_i) \big) \numberthis \label{eq:nll_exponentialfam}\\
     &= \sum_{i=1}^n \max_{\|  \widetilde{x}_i - x_i \|_2 \leq \delta} d_\psi\big(f_\theta(x_i), y_i^\ast \big) + C(y) \numberthis \label{eq:adv_loss}
\end{align*} 
\fi
\begin{align*}
     {\ell}_\delta\big(\theta, (x,y) \big) &=  \max_{\|  \widetilde{x} - x \|_2 \leq \delta} - \log p\big(y \mid  f_\theta(\widetilde{x}) \big) \numberthis \label{eq:nll_exponentialfam}\\
     &=  \max_{\|  \widetilde{x} - x \|_2 \leq \delta} d_\psi\big(f_\theta(\widetilde{x}), y^\ast \big) + C(y), \numberthis \label{eq:adv_loss}
\end{align*} 
where $\delta$ controls the allowable perturbation in the features. 
%The corresponding adversarially robust posterior $ {q}_\delta(\theta)$ is the Gibbs posterior with loss $\mathcal{L}(\theta, \mathcal{D}) =  {\ell}_\delta(\theta, \mathcal{D})$. 
%This formulation is motivated by its equivalence to the adversarial training objective as proved in the following lemma. 
\iffalse
We motivate probabilistic adversarial losses through the link between Bregman divergences and exponential families.
\begin{lemma}[Equivalence between adversarial loss and adversarial training] 
The point estimate obtained by optimizing $\arg\min_{\theta}  {\ell}_\delta(\theta, \mathcal{D})$ is equivalent to performing adversarial training with objective $\arg \min_{\theta} \sum_{i=1}^n \max_{\| \widetilde{x}_i -x_i\|_2 \leq \delta} d_\psi\big(y_i, f_\theta(x_i)\big)$.
\label{lm:adv_loss_adv_training}
\end{lemma}
\fi
Considering a linear predictor and Gaussian likelihood (squared error Bregman divergence) allows us to derive the robust loss in closed-form. 
\begin{lemma}[Robust loss in closed-form for Gaussian likelihood] \label{lm:adv_loss_closed_form_gaussian}
Under a linear predictor $f_\theta(x)=\theta^\top x$ and in the case where the exponential family is a Gaussian family,
$$ \ell_\delta(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \Big\| |Y - X\theta| + \delta \|\theta\| 1_n \Big\|^2,$$
and the adversarial perturbation of the sample $x$ is $\widetilde{x} = \delta \sign(\theta^\top x - y) \frac{\theta}{\Vert \theta \Vert_2} + x = \delta \sign(\theta^\top \widetilde{x} - y) \frac{\theta}{\Vert \theta \Vert_2} + x$.
\end{lemma}
More generally, a linear predictor with any exponential family likelihood (Bregman divergence) allows us to derive the robust loss in closed-form.
% (\Cref{lm:adv_loss_closed_form}). %as stated in \Cref{lm:adv_loss_closed_form}. \Cref{lm:adv_loss_adv_training,lm:adv_loss_closed_form} are proved in \Cref{app:adv_loss}. 
\iffalse
\begin{lemma}[Adversarial loss in closed-form] Under a linear predictor $f_\theta(x) = \theta^\top x$ and Gaussian likelihood, the closed form for the robust loss is
$$ {\ell}_\delta(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \Big\| |Y - X\theta| + \delta \|\theta\| 1_n \Big\|^2.$$
Here the adversarial perturbation of sample $x_i$ is $\widetilde{x}_i = x_i - \delta \sign(y_i-x_i^T\theta)\frac{\theta}{\|\theta\|}$. 
\label{lm:adv_loss_closed_form}
\end{lemma} \Cref{lm:adv_loss_adv_training,lm:adv_loss_closed_form} are proved in \Cref{app:adv_loss}. 
%\todo{should we consider the constant term in $\ell$ and $ {\ell}$?}
\fi

\begin{lemma}[Robust loss in closed-form for exponential family likelihood]
\label{lm:robust-expfam-loss}
Under a linear predictor $f_\theta(x)=\theta^\top x$ and an exponential family likelihood, the robust loss is
    \begin{align*}
        &\phantom{{}={}} \ell_{\delta}\
\big(\theta, (x, y) \big) \\
&= \max_{s \in \{-1, 1\}}\psi(s \delta \Vert \theta \Vert_2 + \theta^\top x) - \psi(\theta^\top x) - y s \delta \Vert \theta \Vert_2 \\
&\phantom{{}=\max_{s \in \{-1, 1\}}}+ d_\psi(\theta^\top x, y^\ast) + C(y),
    \end{align*}
and the adversarial perturbation of the sample $x$ is $\widetilde{x} = \delta \sign \big(\nabla(\psi(\theta^\top \widetilde{x}) - y \big) \frac{\theta}{\Vert \theta \Vert_2} + x$.
\label{lm:adv_loss_closed_form}
\end{lemma}
Note that in the case of a general exponential family likelihood, a trivial maximization problem over $s \in \{-1, 1\}$ must be solved.
All other terms in~\Cref{lm:adv_loss_closed_form} are available in closed-form. 
In practice, this optimization problem can be solved by simply evaluating the objective for $s=1$ and $s=-1$, and picking the result with the highest value.
\Cref{lm:adv_loss_closed_form,lm:adv_loss_closed_form_gaussian} are proved in \Cref{app:adv_loss}. 

\iffalse
\begin{lemma}[Equivalence between adversarial loss and adversarial training] 
The point estimate obtained by optimizing $\arg\min_{\theta}  {\ell}_\delta(\theta, \mathcal{D})$ is equivalent to performing adversarial training with objective $\arg \min_{\theta} \sum_{i=1}^n \max_{\| \widetilde{x}_i -x_i\|_2 \leq \delta} \lp  \widetilde{x}_i^\top \theta - y_i \rp^2$.
\label{lm:adv_loss_adv_training}
\end{lemma}
\fi


\iffalse
\begin{lemma}[Equivalence between adversarial loss and adversarial training] 
The point estimate obtained by optimizing $\arg\min_{\theta}  {\ell}_\delta(\theta, \mathcal{D})$ is equivalent to performing adversarial training with objective $\arg \min_{\theta} \sum_{i=1}^n \max_{\| \widetilde{x}_i -x_i\|_2 \leq \delta} \lp  \widetilde{x}_i^\top \theta - y_i \rp^2$.
\label{lm:adv_loss_adv_training}
\end{lemma}
\fi

We are now ready to define our robust posterior for Bayesian generalized linear models.
\begin{corollary}[Robust posterior]    \label{cor:robust_posterior}
    The Gibbs posterior~\eqref{eq:gibbs_posterior} obtained by setting the loss $\mathcal{L}$ to be an adversarially perturbed exponential family NLL~\eqref{eq:nll_exponentialfam} (or equivalently, an adversarially perturbed Bregman divergence~\eqref{eq:adv_loss}) under a linear model $f_\theta(x) = \theta^\top x$ is given by
    \begin{align*}
        q_\delta(\theta) = \frac{\exp\Big( -\sum_{i=1}^N\ell_\delta\big( \theta, (x_i, y_i)\big) \Big) \pi(\theta) }{\int \exp\Big( - \sum_{i=1}^N\ell_\delta\big( \theta', (x_i, y_i)\big) \Big) \pi(\theta') d\theta'},
    \end{align*}
    where $\ell_\delta\big( \theta, (x_i, y_i)\big) $ is as in~\Cref{lm:adv_loss_closed_form}, or in the special case of a Gaussian (or squared loss),~\Cref{lm:adv_loss_closed_form_gaussian}.
\end{corollary}
We note that this notion of a robust posterior is not the only choice, however this choice does lead to tractable losses derived from adversarial likelihoods, and also allows us to derive generalization guarantees in \Cref{sec:pac_bayes_certs}.
See \Cref{app:adv_loss} for a discussion of other choices.