\section{Datasets and implementation details}
\label{app:exp}

%\subsection{Datasets and implementation details}
The real datasets used in the experiments are given in \Cref{tab:data_stat}, and they are available in UCI repository \citep{uci} or OpenML \citep{OpenML2013}.
We use it 70-30 train-test split to learn and test the posteriors.
%
The code is developed in Pytorch \citep{paszke2017automatic} and use Pyro package \citep{bingham2019pyro} for NUTS distribution sampler. 
All the experiments are run on CPU of Apple M1 chip with 16GB memory. The run time is between seconds upto a few minutes.

\begin{table}[h]
    \centering
    %\vspace{-0.2cm}
    \begin{tabular}{lcc}
\toprule
Dataset            & Number of samples         & Data dimension \\ \midrule
Abalone            & 4177 & 10             \\
Air Foil           & 1503                      & 5              \\ 
Air Quality        & 7355                      & 11             \\ 
Auto MPG           & 393                       & 9              \\ 
California Housing & 20640                     & 8              \\ 
Energy Efficiency  & 768                       & 8              \\ 
Wine Quality       & 1599                      & 11             \\ \bottomrule
\end{tabular}
% \vspace{-0.2cm}
    \caption{Real datasets}
    \vspace{-0.4cm}
    \label{tab:data_stat}
\end{table}

\clearpage

\section{Additional experimental results on real data}
%\label{app:exp_res}
%\subsection{Real data}
\label{app:exp_real}

We provide the additional results on the real datasets for prior variance $\sigma_p^2 = \frac{1}{100}$ in \Cref{tab:real_data_p100} and $\frac{1}{9}$ in \Cref{tab:real_data_p9}. The experimental results are consistent with the results in \Cref{tab:real_data}. We observe that informed choice of prior favors robust posterior in terms of adversarial generalization.

\begin{table*}[h]
    \centering
    \begin{tabular}{lccccc}
    \toprule
    % \multirow{3}{*}{Dataset}  & \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} \\
    % \cmidrule{2-6}
    \multirow{2}{*}{Dataset}  %& \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} \\
       & \multicolumn{2}{c}{Standard generalization (NLL) $\ell$} & \phantom{}& \multicolumn{2}{c}{Adversarial generalization (adv-NLL) $\ell_{\deltat}$} \\
       & Bayes posterior $q$  & Robust posterior  $q_{\delta}$ & \phantom{}& Bayes posterior $q$ & Robust posterior $q_{\delta}$  \\
       \midrule
       Abalone & \textbf{1.1664} \tiny{$\pm$ 0.014} & 1.1797 \tiny{$\pm$ 0.014} && 1.2221 \tiny{$\pm$ 0.014} & \textbf{1.2172} \tiny{$\pm$ 0.015} \\
       Air Foil & \textbf{1.1714} \tiny{$\pm$ 0.008} & 1.1788 \tiny{$\pm$ 0.009} && \textbf{1.2183} \tiny{$\pm$ 0.009} & {1.2219} \tiny{$\pm$ 0.009} \\
       Air Quality & \textbf{0.9668} \tiny{$\pm$ 0.002} & 0.9674 \tiny{$\pm$ 0.002} && 0.9800 \tiny{$\pm$ 0.002} & \textbf{0.9791} \tiny{$\pm$ 0.002} \\
       Auto MPG & \textbf{1.0295} \tiny{$\pm$ 0.010} & 1.0305 \tiny{$\pm$ 0.010} && \textbf{1.0471} \tiny{$\pm$ 0.011} & {1.0474} \tiny{$\pm$ 0.011} \\
       California Housing & \textbf{1.1195} \tiny{$\pm$ 0.003} & 1.1280 \tiny{$\pm$ 0.005} && 1.1862 \tiny{$\pm$ 0.003} & \textbf{1.1768} \tiny{$\pm$ 0.005} \\
       Energy Efficiency & \textbf{0.9834} \tiny{$\pm$ 0.009} & 0.9838 \tiny{$\pm$ 0.009} && 1.0007 \tiny{$\pm$ 0.010} & \textbf{1.0006} \tiny{$\pm$ 0.010} \\
       Wine Quality & \textbf{1.2323} \tiny{$\pm$ 0.006} & 1.2329 \tiny{$\pm$ 0.006} && 1.2642 \tiny{$\pm$ 0.006} & \textbf{1.2614} \tiny{$\pm$ 0.006} \\
       \bottomrule
    \end{tabular}
    \caption{Test NLL and adversarial NLL of Bayes and robust posteriors on real datasets. The prior variance is set to $\sigma_p^2 = \frac{1}{100}$. The robust posterior is trained with $\delta=0.1$ in the adversarial NLL loss, and adversarial generalization is evaluated using the same training-time perturbation ($\deltat=0.1$). The adversarial generalization results demonstrate that the robust posterior $q_\delta$ is consistently more robust than the Bayes posterior $q$. For both standard and adversarial generalization, the best-performing model for each dataset is highlighted in bold.
    %$\ell(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \|Y - X\theta\|^2.$  adv-NLL: $ {\ell}_\delta(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \Big\| |Y - X\theta| + \delta \|\theta\| 1_n \Big\|^2.$
    }
    \label{tab:real_data_p100}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{lccccc}
    \toprule
    % \multirow{3}{*}{Dataset}  & \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} \\
    % \cmidrule{2-6}
    \multirow{2}{*}{Dataset}  %& \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} \\
       & \multicolumn{2}{c}{Standard generalization (NLL) $\ell$} & \phantom{}& \multicolumn{2}{c}{Adversarial generalization (adv-NLL) $\ell_{\deltat}$} \\
       & Bayes posterior $q$  & Robust posterior  $q_{\delta}$ & \phantom{}& Bayes posterior $q$ & Robust posterior $q_{\delta}$  \\
       \midrule
       Abalone & \textbf{1.1585} \tiny{$\pm$ 0.013} & 1.1726 \tiny{$\pm$ 0.014} && 1.2554 \tiny{$\pm$ 0.011} & \textbf{1.2176} \tiny{$\pm$ 0.015} \\
       Air Foil & \textbf{1.1657} \tiny{$\pm$ 0.009} & 1.1694 \tiny{$\pm$ 0.009} && 1.2191 \tiny{$\pm$ 0.009} & \textbf{1.2176} \tiny{$\pm$ 0.009} \\
       Air Quality & \textbf{0.9665} \tiny{$\pm$ 0.002} & 0.9670 \tiny{$\pm$ 0.002} && 0.9826 \tiny{$\pm$ 0.003} & \textbf{0.9791} \tiny{$\pm$ 0.002} \\
       Auto MPG & 1.0231 \tiny{$\pm$ 0.006} & \textbf{1.0228} \tiny{$\pm$ 0.007} && 1.0552 \tiny{$\pm$ 0.006} & \textbf{1.0469} \tiny{$\pm$ 0.008} \\
       California Housing & \textbf{1.1193} \tiny{$\pm$ 0.003} & 1.1267 \tiny{$\pm$ 0.005} && 1.1909 \tiny{$\pm$ 0.003} & \textbf{1.1768} \tiny{$\pm$ 0.005} \\
       Energy Efficiency & \textbf{0.9712} \tiny{$\pm$ 0.006} & 0.9733 \tiny{$\pm$ 0.007} && 0.9990 \tiny{$\pm$ 0.007} & \textbf{0.9947} \tiny{$\pm$ 0.008} \\
       Wine Quality & 1.2338 \tiny{$\pm$ 0.006} & \textbf{1.2321} \tiny{$\pm$ 0.006} && 1.2697 \tiny{$\pm$ 0.008} & \textbf{1.2625} \tiny{$\pm$ 0.006} \\
       \bottomrule
    \end{tabular}
    \caption{Test NLL and adversarial NLL of Bayes and robust posteriors on real datasets. The prior variance is set to $\sigma_p^2 = \frac{1}{9}$. The robust posterior is trained with $\delta=0.1$ in the adversarial NLL loss, and adversarial generalization is evaluated using the same training-time perturbation ($\deltat=0.1$). The adversarial generalization results demonstrate that the robust posterior $q_\delta$ is consistently more robust than the Bayes posterior $q$. For both standard and adversarial generalization, the best-performing model for each dataset is highlighted in bold.
    %$\ell(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \|Y - X\theta\|^2.$  adv-NLL: $ {\ell}_\delta(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \Big\| |Y - X\theta| + \delta \|\theta\| 1_n \Big\|^2.$
    }
    \label{tab:real_data_p9}
\end{table*}

\begin{comment}
\begin{table*}[t]
    \centering
    \begin{tabular}{lccccccccccc}
    \toprule
    \multirow{3}{*}{Dataset}  & \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{100}$} & \phantom{}& \multicolumn{5}{c}{Prior variance $\sigma_p^2 =\frac{1}{9}$} \\
    \cmidrule{2-6} \cmidrule{8-12}
       & \multicolumn{2}{c}{Standard gen. $\ell$} & \phantom{}& \multicolumn{2}{c}{Adversarial gen. $\ell_{\deltat}$} & \phantom{}& \multicolumn{2}{c}{Standard gen. $\ell$} & \phantom{}& \multicolumn{2}{c}{Adversarial gen. $\ell_{\deltat}$} \\
       & Bayes $q$  & Robust $q_{\delta}$ & \phantom{}& Bayes $q$ & Robust $q_{\delta}$  & \phantom{}& Bayes $q$ & Robust $q_{\delta}$ & \phantom{}& Bayes $q$ & Robust $q_{\delta}$  \\
       \midrule
       Abalone & $\textbf{1.1889}$ & $1.2051$ &\phantom{}& $1.2440$ &  $\textbf{1.2433}$  & \phantom{}& $\textbf{1.1566}$ &  $1.1627$ &\phantom{}& $1.2597$ & $\textbf{1.2101}$ \\
       Air Foil & $\textbf{1.1631}$ & $1.1662$ &\phantom{}& $1.2106$ &  $\textbf{1.2099}$  &\phantom{}&  $\textbf{1.1304}$ &  $1.1347$ &\phantom{}& $1.1798$ & $\textbf{1.1787}$ \\
       Air Quality & $\textbf{0.9684}$ &  $0.9687$ &\phantom{}& $0.9817$ & $\textbf{0.9804}$  & \phantom{}& $\textbf{0.9634}$ &  $0.9637$ &\phantom{}& $0.9793$ & $\textbf{0.9759}$ \\
       Auto MPG & $1.0225$ &  $\textbf{1.0223}$ &\phantom{}& $1.0402$ & $\textbf{1.0393}$  & \phantom{}& $\textbf{1.0291}$ & $1.0297$ &\phantom{}& $1.0618$ & $\textbf{1.0536}$ \\
       California Housing & $\textbf{1.1166}$ &  $1.1245$ &\phantom{}& $1.1840$ & $\textbf{1.1734}$  & \phantom{}& $\textbf{1.1280}$ & $1.1333$ &\phantom{}& $1.1280$ & $\textbf{1.1843}$ \\
       Energy Efficiency & $0.9736$ &  $\textbf{0.9734}$ &\phantom{}& $0.9902$ & $\textbf{0.9896}$  & \phantom{}& $\textbf{0.9621}$ &  $0.9655$ &\phantom{}& $0.9872$ & $\textbf{0.9850}$ \\
       Wine Quality & $1.1986$ & $\textbf{1.1970}$  &\phantom{}& $\textbf{1.2305}$ & $1.2250$  & \phantom{}& $1.2789$ &  $\textbf{1.2749}$ &\phantom{}& $1.3181$ & $\textbf{1.3082}$ \\
       \bottomrule
    \end{tabular}
    \caption{Robust posterior trained with $\delta=0.1$ and the testing time adversarial perturbation used in NLL is NLL: $\deltat=0.1$.
    $\ell(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \|Y - X\theta\|^2.$ 
    adv-NLL: $ {\ell}_\delta(\theta, \mathcal{D}) = \frac{n}{2}\log \lp 2\pi \sigma^2 \rp + \frac{1}{2\sigma^2} \Big\| |Y - X\theta| + \delta \|\theta\| 1_n \Big\|^2.$
    }
    \label{tab:app_real_data}
\end{table*}
\end{comment}

%\subsection{Evaluation of \MakeLowercase{\Cref{thm:adv_post_adv_loss_gen}}}
%\label{app_exp:adv_post_gen}

