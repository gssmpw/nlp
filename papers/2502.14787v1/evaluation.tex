\section{Evaluation}\label{sec:evaluation}

We evaluate our prototype of \system to answer the following questions.

\begin{itemize}
    \item \emph{Correctness}: Is it an exact MWPM decoder? (\S\ref{ssec:eval-setup})
    \item \emph{Latency}: How long does it take from receiving the last round of measurement to decoding finishes? (\S\ref{ssec:eva-decoding-latency})
    \item \emph{Effective Accuracy}: What is the logical error rate of a circuit considering latency-induced idle errors? (\S\ref{ssec:eva-real-time})
    \item \emph{Scalability}: What limits the maximum code distance that achieves sub-$\mu s$ latency and how to scale up? (\S\ref{ssec:eva-resource})
\end{itemize}

\begin{figure}[tb]
    \centering
    \begin{minipage}{\linewidth}
        \newcommand{\subsubfigurewidth}{0.95\linewidth}
        \renewcommand*\thesubfigure{\alph{subfigure}}  % use 1,2,3... instead of a,b,c... to distinguish between names of vertices
        \centering
        \begin{subfigure}{.536\linewidth}
            \centering
            \includegraphics[width=\subsubfigurewidth]{figures/evaluation/decoding_latency_curves/circuit_level_software_zero_add.pdf}
            \includegraphics[width=\subsubfigurewidth]{figures/evaluation/distribution/d_9_p_0.001_software_zero_add.pdf}
            \caption{Parity Blossom (CPU).}
            \label{fig:circuit-level-software}
        \end{subfigure}
        \begin{subfigure}{.446\linewidth}
            \centering
            \includegraphics[width=\subsubfigurewidth]{figures/evaluation/decoding_latency_curves/circuit_level_fusion.pdf}
            \includegraphics[width=\subsubfigurewidth]{figures/evaluation/distribution/d_9_p_0.001_fusion.pdf}
            \caption{Micro Blossom (FPGA).}
            \label{fig:circuit-level-fusion}
        \end{subfigure}
        \caption{Decoding latency of circuit-level noise model. In the top row, we plot the average decoding latency with different code distances and physical error rates. With $p=0.1\%$, \system achieves sub-$\mu s$ latency at $d< 15$. In the bottom row, we plot the decoding latency distribution of $d=9$, $p=0.1\%$, each accumulating 1000 logical errors (\num{2.5e8} samples with a logical error rate of $p_L=\num{4.1e-6}$). The plot is in log-log scale, to show several orders of magnitude differences in latency and probability.}
        \label{fig:eva-decoding-latency}
    \end{minipage}
\end{figure}


\subsection{Setup}
\label{ssec:eval-setup}
We first test the correctness of the prototype on various types of decoding graphs and verify its optimality by comparing its results with those of a known exact MWPM decoder~\cite{wu2023qce} (see \S\ref{ssec:ae-correctness}).

For the rest of the evaluation, we focus on surface codes with circuit-level noise like the one shown in \autoref{fig:bkgd-circuit-level} with a measurement cycle of $\qty{1}{\mu s}$.
We set the maximum edge weight to 14, sufficient to distinguish $p_e$ from $0.1\%$ to $0.3\%$ assuming $\max p_e = 0.1\%$, while using only $\qty{4}{bits}$.

We use Parity Blossom~\cite{wu2023qce} running on the Apple M1 Max CPU as the baseline.
We choose Parity Blossom, instead of Sparse Blossom~\cite{higgott2025sparse}, as the baseline for three reasons. 
First, while Sparse Blossom has the lowest decoding latency reported for MWPM decoders, Parity Blossom is a very close second, with only 2x longer latency. 
Second, \system is logically equivalent to Parity Blossom, which does not implement some important optimizations used by Sparse Blossom. Although these optimizations help Sparse Blossom achieve the lowest decoding latency, we find that they are difficult to accelerate with hardware. 
Third, the authors of Sparse Blossom report its decoding latency for various $d$ using the same Apple M1 Max CPU, allowing for direct comparison when necessary. For example, \system achieves $8\times$ lower decoding latency for $d=13$ and $p=0.1\%$ compared to Sparse Blossom.
Note that we only evaluate the CPU wall time in the baseline evaluation, excluding the I/O latency to the quantum controller, which is typically more than $\qty{0.8}{\mu s}$~\cite{neugebauer2018understanding,apple-thunderbolt-latency}.
On the other hand, the evaluation of Micro Blossom includes all I/O latency between the CPU and the accelerator.

\subsection{Decoding Latency}\label{ssec:eva-decoding-latency}

We measure the end-to-end decoding latency from the time the syndrome is ready to the time a correction bit is available.
We evaluated both the average decoding latency and the distribution in \autoref{fig:eva-decoding-latency}.

Micro Blossom reduces the average decoding latency, especially with a low physical error rate, as shown in \autoref{fig:eva-decoding-latency}.
This is because it has a better asymptotic average time complexity of $O(p^2 d^2 + 1)$ compared to Parity Blossom's $O(p d^3 + 1)$.
However, \system runs at a lower clock frequency and includes I/O latency, and thus has a larger latency floor than the software baseline.
We analyze the scaling of clock frequencies in \S\ref{ssec:eva-resource}.


\begin{figure}[tb]
    \centering
    \begin{minipage}{\linewidth}
        \newcommand{\subsubfigurewidth}{0.99\linewidth}
        \renewcommand*\thesubfigure{\alph{subfigure}}  % use 1,2,3... instead of a,b,c... to distinguish between names of vertices
        \centering
        \begin{subfigure}{.495\linewidth}
            \centering
            \includegraphics[width=\subsubfigurewidth]{figures/evaluation/decoding_latency_curves/circuit_level_compare.pdf}
            \caption{Individual Improvement.}
            \label{fig:individual-techniques}
        \end{subfigure}
        \begin{subfigure}{.495\linewidth}
            \centering
            \includegraphics[width=\subsubfigurewidth]{figures/evaluation/measurement_round/measurement_round.pdf}
            \caption{Batch vs. Stream Decoding.}
            \label{fig:stream-vs-batch-decoding}
        \end{subfigure}
        \caption{With $p=0.1\%$, (a) the three key ideas in \S\ref{sec:overview} reduce the decoding latency by 17x compared to Parity Blossom; (b) stream decoding at $d=9$ with round-wise fusion achieves a constant decoding latency regardless of measurement rounds.}
        \label{fig:compare}
    \end{minipage}
\end{figure}

We further study the distribution of the decoding latency to show our $O(|V|^3)$ worst-case time complexity improvement over the software $O(|V|^4)$~\cite{higgott2025sparse}.
We define \emph{$k$-tolerant cutoff latency} $L^{k}_{\text{cutoff}}$, such that the probability $\mathbb{P}(L \ge L^{k}_{\text{cutoff}}) = k p_L$.
By cutting off at a latency of $L^{k}_{\text{cutoff}}$, the logical error rate is at most $(1+k) p_L$.
Clearly, $L^{k=0}_{\text{cutoff}}$ corresponds to the worst-case decoding time.
For practical purposes, $k$ does not have to be 0.
We show the cutoff latency for $k \in \{ 0.01, 0.1, 1 \}$ in \autoref{fig:eva-decoding-latency}, corresponding to logical error rates of at most $1.01 p_L$, $1.1p_L$ and $2p_L$, respectively.
Apart from improving average decoding latency, \system also significantly enhances $k$-cutoff latency by orders of magnitude compared to the software MWPM decoder.
We also fit a probability density function $P(L)$ which shows that higher latencies are exponentially unlikely.

We also evaluate the improvement of individual key ideas from \S\ref{sec:parallel-dual} to \S\ref{sec:fusion}, namely parallel dual phase, parallel primal phase, and round-wise fusion.
As shown in \autoref{fig:individual-techniques}, compared to the baseline of the software MWPM decoder, the key ideas improve the decoding latency by 2.0x, 4.2x and 2.0x, respectively.
Together, \system reduces the decoding latency by 17x.
In particular, round-wise fusion (\S\ref{sec:fusion}) allows the decoder to focus on a few recent rounds, as shown in \autoref{fig:stream-vs-batch-decoding}.
This opens up the possibility to build an accelerator with PUs of a constant number of measurement rounds instead of scaling with $d$.


\subsection{Effective Logical Error Rate}\label{ssec:eva-real-time}

We define \emph{effective logical error rate} $p^{\text{eff}}_L$ that includes idle errors accumulated while waiting for the decoded result.
If there is no logical feedforward, then decoding can be performed offline, and $p^{\text{eff}}_L = p_L$.
However, logical feedforward is necessary, as explained in \S\ref{sec:background}.
We use the logical circuit in \autoref{fig:real-time-decoding} as an example.
If a decoding process takes latency $L$ to finish, then $p^\text{eff}_L = p_L (1 + L/d)$.
Suppose that the latency distribution is described by a probability density function $P(L)$ where $\int P(L) dL = 1$.
The overall effective logical error rate is then $p^\text{eff}_L = \int p_L (1 + L/d) P(L) dL = p_L (1 + \bar{L}/d)$ where $\bar{L} = \int L\,P(L) dL$ is the average decoding latency.
Thus, the effective logical error rate is only dependent on the average decoding latency, not the worst-case latency.

We compare \system against Parity Blossom and Helios~\cite{liyanage2023qce,liyanage2024fpga}, the fastest Union-Find decoder to date.
As shown in \autoref{fig:effective-logical-error-rate}, Micro Blossom achieves the best $p^\text{eff}_L$ in most conditions, except that Helios is better at very high $p$ and $d$ and Parity Blossom is better at very low $p$ and $d$.

\begin{figure}[t]
    \centering
    \subfloat[Helios~\cite{liyanage2023qce}]{\includegraphics[width=0.324\linewidth]
    {figures/evaluation/heatmaps/helios.pdf}}
    \subfloat[Parity Blossom]{\includegraphics[width=0.285\linewidth]
    {figures/evaluation/heatmaps/parity_blossom_zero_add.pdf}}
    \subfloat[Micro Blossom]{\includegraphics[width=0.370\linewidth]
    {figures/evaluation/heatmaps/micro_blossom.pdf}}
    \caption{The ratio of additional logical error rate compared to $p_L$ of a zero-latency MWPM decoder ($p^\text{eff}_L / p^\text{MWPM}_L - 1$). The green region shows the best decoder (lowest ratio) under specific $p$ and $d$.}
    \label{fig:effective-logical-error-rate}
\end{figure}

\subsection{Scalability: Resource and Clock}\label{ssec:eva-resource}

We analyze the scalability of \system in supporting larger code distance $d$ while achieving sub-$\mu s$ decoding latency.
\system, especially its accelerator, can be limited by  (\textit{i}) available parallel compute resources  and (\textit{ii}) feasible clock frequency.
(\textit{i}) As shown in \autoref{tab:resource-usage}, the major resource bottleneck in our prototype is the FPGA logic, where it consumes $\qty{867}{k}$ LUTs out of $\qty{900}{k}$ available on the VMK180 board to support $d=15$.
The currently largest Xilinx SoC VP1902 features $\num{8.5e6}$ LUTs, which supports up to $d = 31$ given the $O(d^3\ \text{polylog}(d))$ resource scaling.
One potential way to support even larger $d$ is to use context switching like that in~\cite{liyanage2024fpga} to time-multiplex a \puv (\pue) between multiple vertices (edges), at the cost of slightly higher resource usage and decoding latency.
(\textit{ii}) To achieve sub-$\mu s$ decoding latency at $d=15$, the clock frequency must be at least $\qty{68}{MHz}$ to catch up with the $O(p^2 d^2 + 1)$ decoding time scaling (\S\ref{sec:fusion}).
This clock frequency is beyond the reach of commercially available FPGAs but achievable if the accelerator is implemented in ASIC~\cite{barber2023realtime,patra2017cryo,ueno2022neo,ueno2022qulatis}.

We can further push the limit using the insight from \autoref{fig:stream-vs-batch-decoding}: the decoder only needs to focus on a constant number of recent measurement rounds.
The constant is calculated based on the measurement rate, physical error rate, and accelerator clock frequency.
In this case, the resource usage only scales with $O(d^2\ \text{polylog}(d))$ instead of $O(d^3\ \text{polylog}(d))$.

\input{resource-usage-table}
