\section{System Implementation}\label{sec:implementation}

We prototype \arch with a Xilinx VMK180 evaluation board.
The heterogeneous architecture involves two parts: the software running on an ARM Cortex-A72 CPU in the processing subsystem (PS) and the hardware accelerator implemented in the programmable logic (PL).
The CPU accesses the hardware accelerator via an internal 64-bit AXI4 bus between the PL and the PS.

\textbf{Software}:~~
We implement the software with 2.7k lines of embedded Rust code. Our implementation eschews dynamic memory and is suitable for embedded CPUs and potentially for high-level synthesis (HLS) to achieve further speedup.

\textbf{Hardware}:~~
We implement the accelerator using SpinalHDL~\cite{spinalhdlv193}, an open-source hardware description language (HDL) library that offers fine-grained register-transfer level (RTL) control along with high-level language features such as type checking and abstracted bus protocols.
With SpinalHDL, one implements a hardware module in a Scala class that can be parameterized. 

Our implementation consists of 4.5k lines of Scala code.
We implement the top-level hardware accelerator module as a Scala class, which in turn uses 19 other modules such as \puv and \pue (also implemented as Scala classes). 
The top module takes as input a JSON file that describes the decoding graph. To implement the accelerator for a different decoding graph, one only needs to provide a JSON file and SpinalHDL will automatically generate synthesizable Verilog code for the accelerator using our implementation. We use Xilinx's Vivado~\cite{vivado2023} tool to generate the FPGA bitstream.

\textbf{Microarchitecture}: The accelerator implements the combinational logic described in \S\ref{sec:parallel-dual} to \S\ref{sec:fusion} with pipelining to achieve a high clock frequency.
As shown in \autoref{fig:microarchitecture-new}, the implementation used in our evaluation employs three pipeline stages:
\begin{itemize}
    \item The \emph{Pre-Match} stage detects isolated \emph{Conflicts} using \autoref{eq:regular-matching-condition} to \autoref{eq:fusion-matching-condition} and determines which vertices are pre-matched to another, i.e., distributedly handles the isolated \emph{Conflicts} on vPUs by temporarily setting $s_v = 0$ for those pre-matched vertices.
    \item The \emph{Execute} stage locally executes the instruction (\autoref{tab:instruction-set}) on each vPU, potentially leaving the vPU state invalid.
    \item The \emph{Update} stage then rectifies the vPU state by propagating the vertex states according to the definition in \autoref{tab:states}.
\end{itemize}
We highlight three important aspects of this microarchitectural design.
First, all PUs execute the same instruction in a synchronous manner. A PU only interacts with its neighbors.
Second, for an ideal clock speed, we use post-synthesis timing analysis to ensure that each stage has approximately the same logic delay.
Finally, our Scala implementation allows easy modification of the pipeline depth, supporting up to 11 stages, which could be beneficial when a higher clock speed is necessary.

While adding pipeline does not reduce the decoding latency, it improves decoding throughput by allowing the accelerator to multiplex across multiple independent decoding tasks.
In our prototype, the accelerator supports context switching with a depth of up to 1024 and its controller includes a response buffer.
Normally, the CPU must wait when reading a response from the accelerator given a context ID.
With the response buffer, the CPU can issue multiple ``\code{find Conflict}'' instructions of different contexts.
The buffer stores the responses and returns the buffered entry if no other instruction invalidates the entry.
In this setup, the CPU becomes the bottleneck due to blocking reads, which could be alleviated by using Direct Memory Access (DMA).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/slides/microarchitecture-new.pdf}
    \caption{Microarchitecture of \system Accelerator, showing two neighboring PUs (one vPU and the other ePU). The computation involves three pipeline stages: Pre-Match (PM), Execute (EX) and Update (UP), with additional clock cycles for synchronous Fetch (FE) and Write (WR).}
    \label{fig:microarchitecture-new}
\end{figure}
