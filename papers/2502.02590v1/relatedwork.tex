\section{Related Work}
\subsection{Articulated object modeling} 
One line of work focuses on the perception end, including reconstruction of articulated objects from observations, joint parameter estimation, etc. \cite{jiang2022ditto} and \cite{huang2021multibodysync} train a network to reconstruct an articulated object from input multi-frame point clouds. Some other works \citep{hsu2023ditto, gadre2021act, nie2023structure, wang2022adaafford} learn joint parameters from active interaction. \cite{liu2023paris}, \cite{weng2024neural}, \cite{wei2022self} and \cite{mandi2024real2code} reconstruct the geometry and joint parameters of an articulated object from multiview images obtained at distinct states of the articulated object. \cite{chen2024urdformer} reconstructs articulated objects from a single real-world RGB image through a network trained on large-scale synthetic data. \citep{li2020category, jiang2022opd, sun2024opdmulti, yan2020rpm, liu2022toward, liu2023self, geng2023gapartnet} estimates joint parameters and part segments given the poincloud or image of an articulated object. However, these approaches either demand extensive observation of the same object or rely on existing articulated object data to train neural networks, with limited ability to generalize across only a small range of categories.

Alongside perception works, generation works including  \cite{lei2023nap} and \cite{liu2024cage}, represents articulated objects as graphs and employs a diffusion model to fit the distribution. The shape, position, and semantic information of the parts are encoded in the vertices, while joint parameters are stored in the edges.  Similar to perception methods, these generation methods also depend on existing articulated object datasets to train their diffusion models and are unable to generalize beyond the training data. 

Additionally, there is a recent survey on articulated object modeling \cite{liu2024survey}.

Overall, none of the existing methods can automatically create articulated objects in an open-vocabulary manner. 

\subsection{Part aware 3D generation} Instead of generating an 3D object as a whole, part aware 3D generation methods generate objects with part-level information. \cite{gao2019sdm} learns two separate VAEs to model global structural information and detailed part geometries. \cite{yang2022dsg} encodes 3D objects into a representation that disentangles structure and geometry. \cite{mo2019structurenet} utilizes a graph VAE to encode shapes into hierarchical graph representations. \cite{wu2020pq} applies a Seq2Seq generative network for part assembly and generation. \cite{nakayama2023difffacto} independently models part geometry and global part transformation, and conditioned on both of them, a diffusion model synthesizes the final shape. \cite{koo2023salad}, on the other hand, generates part geometry latents with one diffusion model and synthesizes the global geometry with another diffusion model conditioned on these latents. \cite{liu2024part123}, a recent release, learns a Neus model from generated multi-view images and corresponding 2D segmentation maps provided by \cite{kirillov2023segment}, and extracts 3D segmentation masks using a clustering algorithm.

While these methods can produce shapes with plausible structures and part geometries, they frequently depend on object datasets with part-level annotations and fail to generalize beyond the datasets used for training. Furthermore, they do not generate articulation parameters for individual parts, causing the generated parts to be individually non-manipulatable in simulation, which limits their applicability in downstream tasks for robot learning.