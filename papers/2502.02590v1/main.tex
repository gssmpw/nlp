%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{listings}
\lstdefinestyle{markdownstyle}{
    basicstyle=\ttfamily\tiny,
    backgroundcolor=\color{backcolour},   
    xleftmargin=0.05\textwidth,
    xrightmargin=0.05\textwidth,
    breakindent=0\dimen0,
    columns=flexible,
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    breakatwhitespace=true,
    breakautoindent=true,
}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\model}{\textsc{Articulate AnyMesh}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}


\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\twocolumn[
\icmltitle{Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xiaowen Qiu}{equal}
\icmlauthor{Jincheng Yang}{equal}
\icmlauthor{Yian Wang}{}
\icmlauthor{Zhehuan Chen}{}
\icmlauthor{Yufei Wang}{}
\icmlauthor{Tsun-Hsuan Wang}{}
\icmlauthor{Zhou Xian}{}
%\icmlauthor{}{sch}
\icmlauthor{Chuang Gan}{}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Artciulated Objects}

\vskip 0.3in
]


\begin{abstract}
3D \textit{articulated} objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (\textit{e.g.}, cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context.
To address these limitations, we propose \model, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints.
Our experiments show that \model~can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system. Our Github website is \url{https://articulate-anymesh.github.io/}.
\end{abstract}

\section{Introduction}
\label{submission}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.85\linewidth, trim=0 160 0 120, clip]{figures/teaser.png}
    \vspace{-3mm}
    \caption{\textbf{\model} turns 3D meshes (\textit{e.g.} retrieved from Objaverse \cite{deitke2024objaverse}) into articulated objects. Our pipeline is capable of processing a wide range of objects, including daily necessities, furniture, vehicles and even fictional objects.}
    \label{fig:teaser}
\end{figure*}


The gathering of data on a large scale is an emerging research trend in embodied AI and robotics. Large foundation models built for robotics and embodied AI are extremely data-hungry, intensifying the need for large-scale data collection. Compared to collecting data in the real world, gathering data in simulations is significantly faster and more convenient, making it easier to capture various types of ground truth information such as physical states, segmentation masks, depth maps, etc. Existing works have explored many different aspects of how to carry out large-scale data collection in simulations, ranging from asset generation \citep{chen2024urdformer}, scene generation \citep{yang2024physcene,yang2024holodeck}, task design \citep{wang2023robogen}, demonstration collection \citep{dalal2023imitating,ha2023scaling}, reward design \citep{ma2023eureka}, etc.

Despite these efforts, a key challenge remains: collecting diverse and realistic articulated objects essential for everyday life, which is vital for producing diverse data that can be generalized to real-world applications. One intuitive approach to achieve this is to use generative models.
While there has been substantial progress in 3D asset generation, few available methods are capable of addressing the demand for the collection of articulated objects. Most 3D generation approaches, utilizing a forward pass of a trained network \citep{long2024wonder3d, hong2023lrm, xu2024instantmesh, shi2023zero123++, shi2023mvdream}, or SDS loss optimization \citep{poole2022dreamfusion,wang2024prolificdreamer,qiu2024richdreamer}, produce only the surface of the object. These objects can only be manipulated as a whole body. For instance, a closet produced through these 3D generation methods cannot be opened or used to store clothes. Part-aware 3D generation approaches \citep{gao2019sdm,yang2022dsg,mo2019structurenet,wu2020pq,nakayama2023difffacto,koo2023salad,liu2024part123} generate 3D objects together with their part-specific details. Although these methods are more sensitive to structure, the objects produced are still restricted to whole-object manipulation for the lack of motion parameters. Articulated object creation approaches \citep{jiang2022ditto, liu2023paris, chen2024urdformer, lei2023nap, liu2024cage, mandi2024real2code, nie2023structure, gadre2021act} are capable of producing articulated objects with several interactive parts, demonstrating functionality. However, such methods require dense observation of a to be reconstructed articulated object in multiple joint states \citep{jiang2022ditto, liu2023paris, mandi2024real2code, huang2021multibodysync}, or are restricted to the limited-scale data and object categories used to train their network \citep{chen2024urdformer, lei2023nap, liu2024cage}. As a result, current methods struggle to automatically generate a wide variety of articulated objects, particularly those from underrepresented or absent categories in existing datasets \citep{xiang2020sapien, wang2019shape2motion, geng2023gapartnet}. 

Consequently, the collection of diverse articulated objects presents extra challenges compared to non-articulated 3D assets: (1) accurate semantic structures must accompany geometry and appearance, (2) articulation parameters such as joint orientation and position are required, and (3) the relatively small-scale articulated object datasets, compared to 3D object datasets, are insufficient to support the training of a generalizable model.
To overcome these challenges, we introduce \model, an automated pipeline that converts any 3D mesh into a corresponding articulated asset. In order to go beyond existing articulated object datasets, our pipeline leverages prior knowledge from visual and language foundation models \citep{achiam2023gpt, kirillov2023segment, rombach2022high} and generalizable geometric clues, rather than relying on existing labeled articulated object data. Our pipeline starts with a 3D mesh, either generated or handcrafted, followed by the stages of \textbf{Movable Part Segmentation}, \textbf{Articulation Estimation} and \textbf{Refinement}. 

%TODO we have changed the segmentation method%
In \textbf{Movable Part Segmentation}, the goal is to identify all movable parts and determine their semantics for subsequent articulation estimation. Recent open-vocabulary 3D part segmentation methods \citep{liu2023partslip, zhou2023partslip++, umam2024partdistill, xue2023zerops, yang2024sampart3d, liu2024part123} utilize an open-vocabulary segmentation model (namely SAM) and a multi-modal foundation model on rendered 2D images, integrating the 2D information to achieve 3D part segmentation. In this work, we adopt the pipeline proposed by PartSlip++ \cite{zhou2023partslip++} for 3D part segmentation.

After obtaining the 3D segmentation of non-fixed parts and their semantics, the next stage in our proposed pipeline focuses \textbf{Articulation Estimation}. Unlike previous approaches \citep{li2020category, huang2021multibodysync, zeng2024mars}, which rely on training datasets to predict articulation parameters. 
To overcome the limitations of existing datasets and extent to open-vocabulary manner, we extract geometric clues inherent in articulated objects instead of relying on learned models.  Specifically, the joint connecting two segmented parts is inherently related to the geometry of the connected area. For example, in cases where the connection follows a straight line, such as a laptop hinge or an open door, this line effectively represents the joint since the hinge structures align with it. We provide GPT-4o with such information through visual prompting, allowing it to leverage common-sense knowledge geometric reasoning to infer joint parameters. This approach allows generalization to unseen objects with accurate segmentation, and the performance will continue to improve as the vision-language model becomes more advanced.

At this stage, a functional articulated object has already been created. However, if the input mesh is a generated or scanned surface mesh that only contains surface geometry and texture, the segmented 3D parts often suffer from occlusion, and may leave holes in other parts. For example, in the closed state, only the outer surface of the drawer will be segmented, and its internal structure will not be present at all. 

To address these issues and complete our proposed pipeline, we incorporated a final stage: \textbf{Refinement}. In this stage, we leverage 2D diffusion models \citep{rombach2022high, poole2022dreamfusion} to close holes, enhance geometry, and improve texture quality. Such models are trained on millions of images, establishing a solid prior about 3D objects. This 3D prior can be effectively distilled using SDS (Score Distillation Sampling) loss \cite{poole2022dreamfusion}. Although this optimization process is not aware of 3D part segmentation, causing parts to grow into each other, we solve this by a simple yet effective optimization strategy that incorporates random transformations of movable parts according to their joint parameters during the SDS loss optimization phase.  

We conducted experiments to compare joint parameter estimation accuracy and unconditional generation capability with various baselines. Experimental results demonstrate that while our method matches state-of-the-art articulated object modeling methods for selected categories that they are trained on within PartNet-Mobility dataset \cite{xiang2020sapien}, it also exhibits the capacity to model a broader range of articulated objects beyond these categories. 
We summarize our contributions as follows: 
\begin{itemize}   
    \item We introduce \model, a pipeline capable of creating diverse, realistic and complex articulated objects in an open-vocabulary manner.
    \item We propose a novel articulation parameter estimation method that leverages geometric clues and a tailored visual prompting method to estimate joint parameters in an open-vocabulary manner. 
    \item We propose a novel optimization strategy that enables using diffusion prior to refine articulated objects while keeping the semantics of parts from being changed or lost. 
\end{itemize}

\section{Related Work}
\subsection{Articulated object modeling} 
One line of work focuses on the perception end, including reconstruction of articulated objects from observations, joint parameter estimation, etc. \cite{jiang2022ditto} and \cite{huang2021multibodysync} train a network to reconstruct an articulated object from input multi-frame point clouds. Some other works \citep{hsu2023ditto, gadre2021act, nie2023structure, wang2022adaafford} learn joint parameters from active interaction. \cite{liu2023paris}, \cite{weng2024neural}, \cite{wei2022self} and \cite{mandi2024real2code} reconstruct the geometry and joint parameters of an articulated object from multiview images obtained at distinct states of the articulated object. \cite{chen2024urdformer} reconstructs articulated objects from a single real-world RGB image through a network trained on large-scale synthetic data. \citep{li2020category, jiang2022opd, sun2024opdmulti, yan2020rpm, liu2022toward, liu2023self, geng2023gapartnet} estimates joint parameters and part segments given the poincloud or image of an articulated object. However, these approaches either demand extensive observation of the same object or rely on existing articulated object data to train neural networks, with limited ability to generalize across only a small range of categories.

Alongside perception works, generation works including  \cite{lei2023nap} and \cite{liu2024cage}, represents articulated objects as graphs and employs a diffusion model to fit the distribution. The shape, position, and semantic information of the parts are encoded in the vertices, while joint parameters are stored in the edges.  Similar to perception methods, these generation methods also depend on existing articulated object datasets to train their diffusion models and are unable to generalize beyond the training data. 

Additionally, there is a recent survey on articulated object modeling \cite{liu2024survey}.

Overall, none of the existing methods can automatically create articulated objects in an open-vocabulary manner. 

\subsection{Part aware 3D generation} Instead of generating an 3D object as a whole, part aware 3D generation methods generate objects with part-level information. \cite{gao2019sdm} learns two separate VAEs to model global structural information and detailed part geometries. \cite{yang2022dsg} encodes 3D objects into a representation that disentangles structure and geometry. \cite{mo2019structurenet} utilizes a graph VAE to encode shapes into hierarchical graph representations. \cite{wu2020pq} applies a Seq2Seq generative network for part assembly and generation. \cite{nakayama2023difffacto} independently models part geometry and global part transformation, and conditioned on both of them, a diffusion model synthesizes the final shape. \cite{koo2023salad}, on the other hand, generates part geometry latents with one diffusion model and synthesizes the global geometry with another diffusion model conditioned on these latents. \cite{liu2024part123}, a recent release, learns a Neus model from generated multi-view images and corresponding 2D segmentation maps provided by \cite{kirillov2023segment}, and extracts 3D segmentation masks using a clustering algorithm.

While these methods can produce shapes with plausible structures and part geometries, they frequently depend on object datasets with part-level annotations and fail to generalize beyond the datasets used for training. Furthermore, they do not generate articulation parameters for individual parts, causing the generated parts to be individually non-manipulatable in simulation, which limits their applicability in downstream tasks for robot learning.

\section{Method}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_pipeline_new_new.pdf}
    \vspace{-3mm}
    \caption{\model~converts 3D meshes from various sources to high quality articulated objects via three main parts: \textbf{A.} Movable Part Segmentation, \textbf{B.} Articulation Estimation, \textbf{C.} Refinement.}
    \label{fig:pipeline}
\end{figure*}

\subsection{Overview}

Our pipeline converts any mesh into its articulated counterpart, including hand-crafted meshes with part geometry, surface meshes generated through text-to-3D or image-to-3D methods and meshes reconstructed from real-world objects. The process involves three main steps, including \textbf{Movable Part Segmentation}, \textbf{Articulation Estimation} and \textbf{Refinement}, as depicted in Figure~\ref{fig:pipeline}. Specifically, the \textbf{Movable Part Segmentation} step employs the method proposed by PartSlip++ \cite{liu2023partslip} to segment movable parts from the input mesh.  \textbf{Articulation Estimation} extracts the articulation parameters for each movable part by leveraging geometric cues and prior knowledge from visual and language foundation models. In the \textbf{Refinement} stage, the geometry and texture of the articulated object are enhanced with geometry-aware 2D diffusion model Richdreamer \citep{qiu2024richdreamer}, guided by SDS loss to distill the diffusion model’s 3D prior. During this stage, movable parts undergo random transformation to improve optimization performance.

\subsection{Movable Part Segmentation via VLM assistant}
As shown in Figure~\ref{fig:pipeline}A,  this step aims to segment all movable parts from a 3D mesh and identify their semantics in an open-vocabulary manner. We primarily use PartSlip++ \cite{zhou2023partslip++} as our segmentation method. In PartSlip++, an image of the input mesh is first fed into a VLM to recognize all movable parts. Next, an open-vocabulary grounding model \cite{ren2024dino}, and SAM are applied to 2D rendered images of the input mesh to extract bounding boxes and segmentation masks of the relevant part semantic labels. These multiview 2D detection and segmentation results are then fused to produce the 3D part segmentation.

\subsection{Articulation Estimation via Geometry-aware Visual Prompting}
Achieving an open-vocabulary scope for articulation estimation requires going beyond the limited-scale datasets of articulated objects.
To address this, we propose a \textit{Geometry-aware Visual Prompting} approach that generalizes across a diverse range of articulated objects by leveraging shared features in geometry, mechanical structure, and functionality. Specifically, we observe that joints connecting two parts are strongly relevant to the geometry in areas where the parts are in close proximity, which we will refer to as the \textit{connecting area}. 

Denote the point clouds of two neighboring parts $a$ and $b$ as $P_a$ and $P_b$ respectively. To define the connecting area, we select the points in $P_a$ whose distance to the closest point in $P_b$ is below a predefined threshold and vice versa for points in $P_b$. The selected points from $P_a$ and $P_b$ together form the connecting area between the parts. If no point is selected, the threshold is doubled, and the process is repeated until a connecting area is identified. We focus primarily on two dominant joint types: prismatic joints and revolute joints. These are estimated using distinct methods tailored to their respective mechanical structures.


\textbf{Revolute joints} 
connect two parts of an articulated object through physical hinges or other similar hinge-like mechanisms, such as the lid of a cardboard box. The rotation axis of a revolute joint aligns with the length of its hinge, meaning that identifying hinges in 3D space allows for straightforward estimation of the rotation axis. Since a hinge physically connects two parts, it is typically located within the region where the parts are joined. Driven by this observation, we propose the following visual prompting procedure. First, we divide the connecting area into multiple clusters, with the cluster centers serving as candidate points. 
These candidate points are then projected onto 2D rendered images of the input mesh and labeled with numbers, as illustrated in Figure~\ref{fig:pipeline}B.
This image is used to prompt GPT4o to select two or more points that define the hinge, thereby establishing the rotation axis. The rendering viewpoint is chosen to maximize the number of pixels of the part under consideration. K-means is run multiple times with varying cluster counts, and the final count is selected to maximize the number of candidate points while avoiding overlapping labels on the rendered image.

In some cases, a hinge may not be positioned on the object's surface, resulting in only one end of the rotation axis identifiable on the object's surface, such as with knobs. In these cases, we assume the rotation axis to be perpendicular to the plane fitted to the connected area. The axis is then determined by the normal vector of this plane and the single selected point. When the hinge mechanism is hidden beneath the surface, the connecting structure cannot be assumed to be entirely within the connecting area. Thus, in addition to candidate points from the connecting area, we sample additional points from the part surface. Specifically, we use the $l_0$-cut pursuit \cite{landrieu2017cut} algorithm to generate super points from the part's point cloud, using point normals and color as features, and derive candidate points from these super points. This ensures the new candidate points are distributed across different geometric and textured regions of the part.

\textbf{Prismatic joints} 
connect two parts of an articulated object through sliding mechanisms, enabling linear motion along a single axis. If the input mesh includes physically accurate inner geometry, sampling a collision-free sliding direction is likely to yield an accurate prismatic joint axis. However, most meshes, particularly surface meshes, lack the ideal geometry needed for this collision-based approach. Additionally, the sliding mechanism is often concealed beneath the object's surface, making it less identifiable compared to hinge structures for revolute joints and making the strategy used for revolute joints impractical for prismatic joints.

Based on their functionality, prismatic joints can be categorized into two types: 

1. \textbf{Inward \& Outward sliding joints}: These joints allow the child component to slide in and out of the object it is connected to, as seen in parts like drawers and push buttons. Although the translation direction may vary, the motion to pull the part out is predominantly outward from the object. Consequently, the sliding direction is characterized by the normal vector of the plane fitted to the connecting area.

2. \textbf{Surface sliding joints}:
These joints enable the child component to slide along the surface of the object, as seen in parts like sliding windows and toaster levers. For such joints, the potential translation direction is constrained to two dimensions along the surface. This allows us to annotate a 2D rendered image with arrows and prompt GPT-4 to select the most appropriate direction.

Given an articulated object and its segmented parts, we prompt GPT-4 to classify all prismatic joints into these two categories. Based on the classification, the translation directions of the prismatic joints are then estimated accordingly.

\subsection{Refinement via Randomized Joint State}

After completing the stages of 3D segmentation and joint estimation, we obtain an articulated object with the correct structure. However, when the input mesh is a surface mesh, the result often remains flawed. Object parts segmented from surface meshes—lacking inner structures—inevitably suffer from occlusion, leading to incomplete part meshes with holes.

To address this, we integrate a refinement step into our pipeline. This step completes occluded geometries, fills holes, and enhances texture quality by distilling 3D object geometry and texture priors from a geometry-aware 2D diffusion model into the articulated object using SDS loss \cite{poole2022dreamfusion}. 
The refinement process builds on Richdreamer \cite{qiu2024richdreamer}, which sequentially applies a normal-depth diffusion model and an albedo diffusion model to optimize the geometry and texture of 3D object, respectively. 

However, directly applying existing SDS optimization method \cite{qiu2024richdreamer} on articulated objects in fixed postures often causes parts to grow and overlap, resulting in artifacts when the object parts move. To solve this problem, we propose a novel optimization technique for articulated objects: randomizing the articulation configuration during optimization. In each optimization step, along with randomly sampling camera positions and diffusion timesteps, we also sample motion parameters (e.g., rotation angles and translation lengths) for each non-fixed part and apply these transformations.
This approach ensures that most parts are exposed in certain poses, while overlapping geometry in one pose becomes evident as artifacts in other poses, allowing gradual optimization. Since the parts are already recognizable at initialization, the diffusion model preserves their semantics effectively, ensuring that the meaning and identity of the parts remain intact throughout the refinement process.

For implementation details please refer to appendix \ref{sec_refinement_deet}.

\section{Experiments} \label{experiments_section}
\textbf{Implementation}
In movable part segmentation stage, we use Partslip++ \cite{zhou2023partslip++} for segmentation and replace the grounding model with DINO-X to achieve better performance. In addition, GPT4o is prompted to extract part labels of the input mesh and some of its joint configurations like joint type, parent link, etc.
In articulation estimation stage, we visually prompt GPT4o for the rest of the joint configurations. 
For refinement stage, we implement a multi-part DMTet \cite{shen2021deep} for articulated objects representation. Each part of the object is assigned an independent DMTet network for describing its geometry and texture features. We exploit the normal-depth diffusion model and albedo-diffusion model proposed in Richdreamer \cite{qiu2024richdreamer} for geometry and texture prior.

\textbf{Baselines} 
To the best of our knowledge, this is the first work to address the challenge of converting meshes into articulated objects in an open-vocabulary manner. Consequently, no existing baseline directly process the same input and output as \model. 
Instead, we evaluate individual components separately, comparing to corresponding baselines. 
% For the 3D segmentation step, we use PartSlip\cite{liu2023partslip} and PartDistill\cite{umam2024partdistill} as the baseline. PartSlip leverages the open-vocabulary grounding capability of GLIP and fuses the 2D detections to obtain 3D segmentation. PartDistill utilizes a cross-modal distillation framework to transfer 2D knowledge from VLM for 3D part segmentation. 
\textbf{The articulation estimation step} in our pipeline takes object parts as input and outputs their joint configurations. NAP \cite{lei2023nap} and CAGE \cite{liu2024cage}, which learn diffusion models to generate articulated objects, can also generate joint configurations conditioned on object parts, making them the closest baselines for this step. Additionally, methods such as ANCSH \cite{li2020category}, OPD \cite{jiang2022opd}, and OPDmulti \cite{sun2024opdmulti} directly estimate articulation parameters. These methods take a single observation (e.g., RGB, depth, or both) as input and simultaneously predict part segmentation and joint parameters, serving as additional baselines. We compare these methods to the combined 3D segmentation and articulation estimation steps in \model.
\textbf{For the refinement step}, since our approach is based on Richdreamer \cite{qiu2024richdreamer}, it serves as a natural baseline for this stage. 

\textbf{Metrics} 
For articulation parameter estimation, we evaluate joint directional accuracy using angular error and joint positional accuracy using the distance between lines.

For the refinement step, since no ideal metric directly evaluates the visual quality and structural correctness of 3D objects without a reference set, we instead render 2D images of articulated objects in different states and assess these images using metrics that measure image-caption similarity. The underlying assumption is that a structurally accurate and visually plausible articulated object will produce rendered images that better correspond to its category, resulting in higher image-text correspondence scores. Specifically, we use CLIPscore \citep{hessel2021clipscore} and VQAscore \citep{lin2024evaluating} as the metrics for this evaluation.

\subsection{Quantitative Experiments}

\textbf{Articulation estimation}

As outlined in the baseline section, we evaluate the performance of articulation parameter estimation under two settings:
\begin{itemize}
  \item \textbf{Object part mesh} as input. For articulation parameter estimation using object part meshes, we compare \model~against two generative diffusion models, NAP and CAGE, which represent articulated objects as graphs. In this experiment, the ground-truth shapes of all parts are provided, and the task is to estimate their articulation parameters. NAP generates joint parameters (edges) conditioned on ground-truth vertices, such as part bounding boxes, spatial locations, and shape latents. For CAGE, attributes like bounding boxes, joint types, and semantic labels are provided for each node, while joint axes and ranges are generated. We conduct this evaluation using articulated objects from PartNet-Mobility, following the train-test split used in CAGE and retraining NAP on the same split. We assess in-domain performance using objects from the test set and generalization performance on categories excluded from training. As shown in Table~\ref{tab:part_mesh}, while \model~achieves comparable performance to NAP and CAGE in-domain, it significantly outperforms them in unseen categories.
  \item \textbf{Single Observation} as input. We also compare \model~against three methods—ANCSH, OPD, and OPDmulti—for articulation parameter estimation from single observations. ANCSH uses a single-view point cloud as input, while OPD and OPDmulti take an RGB image (optionally with depth information) as input. These methods output part segmentation and joint parameters. To adapt \model~for this setting, we modify the pipeline such that only one image (the input observation) is segmented. The resulting segmentation is projected into a point cloud with labels, which is then processed in the articulation estimation step. The results, presented in Table~\ref{tab:single_obs}, show that \model~outperforms ANCSH, OPD, and OPDmulti within their respective training domains, despite being designed as an training free, open-vocabulary method.
\end{itemize}

\begin{table}[]
\begin{center}
\begin{tabular}{c|cccc}
  \Xhline{2.5\arrayrulewidth}
     & \multicolumn{2}{c|}{angle error}                   & \multicolumn{2}{c}{position error}       \\ \cline{2-5} 
     & \multicolumn{1}{c}{ID $\downarrow$} & \multicolumn{1}{c|}{OOD $\downarrow$} & \multicolumn{1}{c}{ID $\downarrow$} & {OOD $\downarrow$}           \\ \hline
NAP  & 31.89                   & 42.23                    & 0.53                    & 0.225          \\
CAGE & 8.96                    & 58.64                    & \textbf{0.136}          & 0.192          \\
\textbf{Ours} & \textbf{7.90}           & \textbf{4.81}            & 0.190                   & \textbf{0.075} \\ \Xhline{2.5\arrayrulewidth}
\end{tabular}
\label{tab:part_mesh}
\caption{The results for object part mesh joint estimation are evaluated on two sets of testing object categories: \textbf{in-domain (ID)}, consisting of categories included in CAGE's training set, and \textbf{out-of-domain (OOD)}, consisting of some categories from PartNet-Mobility that are not part of CAGE's training set.}
\end{center}
\end{table}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig_generation_hr2.pdf}
    \vspace{-4mm}
    \caption{Visualization of articulated objects output by the refinement step. (a) Diverse articulated objects produced by \model~(the input meshes are generated with InstantMesh \cite{xu2024instantmesh}); (b) Distinct textures created by the refinement step for the same input, with geometry and articulation parameters preserved (highlighted by the boxes); (c) A fictional object produced by \model}
    \vspace{-4mm}
    \label{fig:result_combined}
\end{figure*}

\begin{figure}[ht]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{figures/fig_objaverse_2.pdf}
    \vspace{-6mm}
    \caption{\model~applies to hand-crafted meshes retrieved from Objaverse}
     \vspace{-1mm}
    \label{fig:objaverse}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/real-3.pdf}
    \vspace{-4mm}
    \caption{We build the digital twins of real-world objects in the simulation using our method. Then we sample trajectories completing the tasks in the simulation and replicate those trajectories in the real-world.}
    \vspace{-4mm}
    \label{fig:real}
\end{figure*}


\begin{table}[]
\vspace{-3mm}
\begin{tabular}{ccccc}
\Xhline{2.5\arrayrulewidth}
\multicolumn{1}{c|}{\textbf{}}                & ANCSH       & OPD         & OPDmulti   & \textbf{Ours}       \\ \cline{1-5}
\multicolumn{1}{c|}{angle error $\downarrow$} & 6.74                 & 10.73                & 9.66                 & \textbf{5.37}   \\
\multicolumn{1}{c|}{position error $\downarrow$}  & 0.065                & 0.117                & 0.108                & \textbf{0.049}   \\ \Xhline{2.5\arrayrulewidth}
\multicolumn{1}{l}{}                          & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}

\end{tabular}
\label{tab:single_obs}
\vspace{-6mm}

\caption{The results of single observation joint estimation. The dataset for training and testing is the one-door dataset (a subset of OPDsynth), which is used to train ANCSH as provided in its PyTorch version Github repository.}

\vspace{-6mm}
\end{table}

\textbf{Refinement}

For the refinement step, we evaluate the visual quality and structural correctness of refined articulated objects using the metrics described in Section Metrics. We test three configurations on the intermediate output of the articulation estimation step: (1) No Refinement step applied, (2) Refinement step applied without random transformation (same as Richdreamer) and (3) Refinement step applied with random transformation.
The surface meshes used in this experiment are generated using the text-to-3D model InstantMesh \cite{xu2024instantmesh}, conditioned on text prompts corresponding to categories in PartNet-Mobility. The results, presented in table \ref{tab_refinement}, show that the highest scores are achieved with refinement incorporating random transformations, highlighting the effectiveness of our proposed refinement step.

\begin{table}[]
\begin{center}

\begin{tabular}{c|ccc}
\Xhline{2.5\arrayrulewidth}
\textbf{}  & \textbf{w/o Re} & \textbf{w/o RT} & \textbf{Re /w RT} \\ \hline
CLIP Score $\uparrow$& 0.7329                  & 0.7928                      & \textbf{0.8205}            \\
VQA Score  $\uparrow$& 0.6551                  & 0.8164                      & \textbf{0.9376}            \\ \Xhline{2.5\arrayrulewidth}
\end{tabular}
\label{tab_refinement}

\caption{Evaluation of the refinement step. Re stands for refinement and RT is the abbreviation of random transformation. }
\end{center}
\end{table}

\subsection{Applications}

\textbf{Articulated object generation}
By integrating an exisitng 3D generation model to generate surface meshes as the input to \model, our pipeline takes advantage of the open-vocabulary 3D generation capabilities of the 3D generation model and inherits its generative paradigm. As shown in Figure \ref{fig:result_combined}, our extended pipeline generates high-quality articulated objects of various categories, from everyday objects to professional tools. 
% By providing proper prompts, we can obtain customized and stylish results in the refinement step.


\textbf{Annotate 3D object datasets}
Other than generating from scratch, our work can also start from existing artist designed meshes. For example, we annotate part segmentation and joint structures on objects from Objaverse \citep{deitke2024objaverse}. Such objects usually have high-quality mesh and textures and also with inner structures. Thus, we only execute the 3D segmentation part and articulation estimation part of our pipeline to annotate these objects. The results are demonstrated in Figure~\ref{fig:objaverse}. Objects in the teaser (Figure \ref{fig:teaser}) are also produced in this way.

\textbf{Real-to-Sim-to-Real}
In this experiment, we first reconstruct the 3D surface mesh of real-world objects using 2DGS \cite{huang20242d}. We then apply \model~to convert the reconstructed mesh into an articulated object represented in URDF format, making it compatible with simulation environments. Next, we sample action targets and use motion planning \citep{sucan2012open} to avoid collisions, generating a trajectory to successfully complete the task in simulation. Finally, we replicate the trajectory in the real world and observe that the robot arm can effectively execute the task.

We focus on three tasks involving different articulated objects: \textbf{pushing a drill trigger}, \textbf{opening a microwave door}, and \textbf{rotating a steering wheel}, as shown in Figure~\ref{fig:real}. 


Our results show that the generated trajectories can be successfully transferred to the real world, demonstrating minimal error in part segmentation and joint prediction.

\textbf{Policy learning in simulation}
We follow DexArt \cite{bao2023dexart} for articulated object manipulation policy learning and evaluate two tasks: opening a laptop lid and lifting a bucket.  To augment the training set for the "bucket" and "laptop" experiments, we include additional articulated objects of the same categories generated by \model. Additionally, we diversify the testing set by incorporating some generated articulated objects. We fine-tune the checkpoints trained exclusively on the original dataset and evaluate their performance on the augmented testing set. The results, presented in Table~\ref{tab_policy}, demonstrate that the data generated by \model~effectively enhances robot learning. The articulated objects generated by \model are sourced from Objaverse. For visualizations of the data used, please refer to Appendix Section \ref{sec_dataset}.

\begin{table}[]
\begin{center}

\begin{tabular}{c|cc|cc}
\Xhline{2.5\arrayrulewidth}
            & \multicolumn{2}{c|}{Laptop} & \multicolumn{2}{c}{Bucket} \\ \cline{2-5} 
            & original  & augment       & original  & augment      \\ \hline
sucess rate $\uparrow$ & 0.408     & \textbf{0.513}  & 0.339     & \textbf{0.479} \\ \Xhline{2.5\arrayrulewidth}
\end{tabular}
\label{tab_policy}
\vspace{-3mm}
\caption{Success rates of manipulation policy learning experiments, comparing policies fine-tuned on the original training set with those fine-tuned on the augmented training set. Results are reported for evaluations on the augmented test set. }
\end{center}
\vspace{-4mm}
\end{table}

\section{Conclusion}
In this paper, we introduce $\model$, a novel framework for the critical task of converting open-vocabulary 3D meshes into their articulated counterparts. Our pipeline first segments 3D objects based on part-level semantics, estimates the articulation structure among object parts, and then refines the geometry and texture to repair flaws and enhance visual quality. By leveraging advancements in vision-language models and visual prompting techniques, we effectively segment parts and estimate articulation structures using common geometric cues. Our pipeline creates high-quality textured articulated objects from generated 3D meshes, hand-crafted 3D assets, and reconstructed meshes. The resulting assets can support the learning of articulated object manipulation skills in simulation, which can then be transferred to real-world robots.

\textbf{Limitations and Future work} 
Although our method creates articulated objects with semantically correct parts, the articulation parameters are not always accurate or physically grounded. This limitation arises from three key challenges: (1) existing and generated 3D meshes often lack correct physical structures, (2) current 3D segmentation methods occasionally produce incorrect results, and (3) GPT4o exhibits biases in certain scenarios.
Additionally, some uncommon cases are not covered by our method—for example, when a drawer is designed to be pulled out up-front. 

If future advancements lead to the development of large VLMs with enhanced 3D reasoning capabilities, our pipeline could leverage these improvements to become more robust and reliable. Addressing these challenges to create fully accurate and physically grounded articulation parameters represents an exciting direction for future research.


\nocite{wang2022adaafford}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Details of the Refinement Step}
\label{sec_refinement_deet}

The refinement step builds on Richdreamer \cite{qiu2024richdreamer} because it separately optimizes geometry and texture using two distinct diffusion models: a normal-depth diffusion model and an albedo diffusion model. Unlike computing the SDS loss purely in RGB space, which is suboptimal for geometry optimization, the normal-depth diffusion model demonstrates greater geometry awareness.

In Richdreamer, the Score Distillation Sampling process proposed by DreamFusion \cite{poole2022dreamfusion} is formulated as follows:

Given a 3D representation $\phi$, and a differentiable renderer $g$, the rendered image is $x = g(\phi)$. The SDS loss is then used to optimize the 3D representation $\phi$:

$\nabla_{\phi} \mathcal{L}_{\text{SDS}}(\phi, x=g(\phi)) = \mathbb{E}_{t, \epsilon}\left[w(t)(\epsilon_{\theta}(z_t ; y, t) - \epsilon) \frac{\partial x}{\partial \phi}\right]$, 

where $z_t$ is the noisy latent code, $\epsilon$ is the injected noise and $\epsilon_\theta$ is the noise predicted by a denoising model $\theta$, conditioned on timestep $t$ and text embedding $y$. The term $w(t)$ is a timestep dependent weighting factor.

In Richdreamer and other previous works the 3D representation $\phi$ is static, whereas in our case, it is articulated. Omitting other attributes, we denote the 3D representation of articulated objects as $\phi_q$, where $q$ is a vector representing joint positions. During optimization, the base of the articulated object remains fixed. Since non-fixed joints of interest (revolute, prismatic, and continuous) all have one degree of freedom, each element in $q$ corresponds to the position of a non-fixed joint in $\phi_q$. The articulated object in its rest configuration is denoted as $\phi_{q_0}$. A transformation function $T$ maps $\phi_{q_0}$ to $\phi_q = T(\phi_{q_0}, q)$ given the desired joint positions $q$.

The optimization process for Richdreamer \cite{qiu2024richdreamer} and other previous SDS optimization methods can be briefly summarized by the following pseudo-code:

\begin{verbatim}
FOR i IN iterations:
    render an image x (x = g(phi))
    sample timestep t
    compute SDS loss
    update optimizer
\end{verbatim}
In our approach, we extend this process by sampling joint positions and transforming object parts accordingly:
\begin{verbatim}
FOR i IN iterations:
    sample joint position q
    transform parts according to q (phi_q = T(phi_q0, q))
    render an image x (x = g(phi_q))
    sample timestep t
    compute SDS loss
    update optimizer
\end{verbatim}

We also provide visualizations of the input and output of the refinement step, as shown in Figure \ref{fig:vis_abla}. The pencil case and lighter in the second and third rows have manually set joint parameters and shape primitives as link geometries. The refinement step successfully optimizes storage space for the drawer and pencil case and generates a nozzle structure for the lighter, and produces plausible textures.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fig-ablation-new.pdf}
    \caption{Visualization of the input and output of the refinement step. }
    \label{fig:vis_abla}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Implementation of policy learning experiment}
\label{sec_dataset}

In this experiment, we evaluate two tasks proposed by DexArt \cite{bao2023dexart}. The first task is lifting a bucket, where a dexterous hand must grasp the bucket handle and lift it. In the original implementation, the bucket handle is thickened to prevent severe penetration, and we adhere to this setting. The second task is opening a laptop lid, where a dexterous hand must open the lid to a 90-degree angle.

For both tasks, we fine-tune the "no-pretrain" policy checkpoint provided by DexArt's official GitHub repository using a combined training set consisting of DexArt's original samples (sourced from PartNet-Mobility \cite{xiang2020sapien}) and additional samples (sourced from Objaverse \cite{deitke2024objaverse}) generated by \model. As a baseline, we fine-tune the policy for the same number of steps but only use the original dataset. During evaluation, the new policies are tested on both the original test set and the additional test samples generated by \model. Visualizations of the datasets used are provided in Figures \ref{fig:bk_train}, \ref{fig:lp_train}, and \ref{fig:ltest}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig_bucket_train.pdf}
    \vspace{-4mm}
    \caption{Visualization of the original training set for the bucket task from DexArt \cite{bao2023dexart} and the augmented training set collected using \model.}
    \vspace{-4mm}
    \label{fig:bk_train}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig_laptop_train.pdf}
    \vspace{-4mm}
    \caption{Visualization of the original training set for the laptop task from DexArt \cite{bao2023dexart} and the augmented training set collected using \model.}
    \vspace{-4mm}
    \label{fig:lp_train}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig_test.pdf}
    \vspace{-4mm}
    \caption{Visualization of the original test set used by DexArt \cite{bao2023dexart} and augmented test set collected using \model.}
    \vspace{-4mm}
    \label{fig:ltest}
\end{figure*}


\section{Prompting details}

The prompt for recognizing parts in an rendered image of a input mesh:

\begin{lstlisting}[style=markdownstyle]
You have a good understanding of the structure of articulated objects. Your job is to assist the user to analyze the structure of an object. Specifically, the user will give you an image of an articulated object, and your task is to recognize the main parts of that object. You should give your answer in the following format:

```part_list
(1) part_name: name of the part; description: a brief description about the part, and how it moves
(2) part_name: name of the part; description: a brief description about the part, and how it moves
...

```

Remember:
(1) Do not answer anything not asked.
(2) Your answer should be purely based on the input image, do not imagine anything.
(3) If there are multiple parts with the same semantic, just add one part to the list. For example, if there are four wheels, just add one part whose name is wheel.
\end{lstlisting}

The prompt to generate the link and joint configuration of the articulated object, and exclude non-movable parts:
\begin{lstlisting}[style=markdownstyle]
SYSTEM PROMPT:

You have a good understanding of the structure of articulated objects. You are very familiar with URDF format. Your job is to assist the user to analyze the structure of an articulated object. Specifically, the user will name an object and then give you the main parts of that object. You will have to group these parts into links and then give the joints connecting these links. You should give your answer in the following format:

```articulation tree
parts:
(1) part_name: name of the recognized part;
...

links:
(1) link_name: name of the link;
...

joints:
(1) joint_name: name of the joint; joint_type: type of the joint; parent_link: name of the parent link; child_link: name of the child link; joint_limit: [lower limit, upper limit];
...
```

For example:
```articulation tree
parts:
(1) part_name: Front windshield;
(2) part_name: Doors;
(3) part_name: Headlights;
(4) part_name: Wheels;
(5) part_name: Windows;

links:
(1) link_name: Chasis;
(2) link_name: Doors;
(3) link_name: Wheels;
(4) link_name: Windows;

joints:
(1) joint_name: wheel_chasis_joint; joint_type: continuous; parent_link: Chasis ; child_link: Wheels; joint_limit: None;
(2) joint_name: door_chasis_joint; joint_type: revolute; parent_link: Chasis ; child_link: Doors; joint_limit: [0, 90];
(3) joint_name: door_chasis_joint; joint_type: prismatic; parent_link: Chasis ; child_link: Windows; joint_limit: [0, 1];

```

Remember:
(1) Do not answer anything not asked.
(2) If a part is actually movable in any direction, its joint type is floating.
(3) Available joint types are: fixed, prismatic, revolute, continuous and floating.
(4) For joint_type, only answer one word (among the available types), do not answer anything else.
(5) For every part that is not fixed, there must be a unique link for it.
(6) For parts that are fixed, try to group as many as you can.
(7) Joint limit must be given for prismatic joints and revolute joints. The unit of a revolute joint limit is degrees. The unit of a prismatic joint limit is the size of its child link in its translation direction. The joint limit should be two numbers.

USER PROMPT:

Object: OBJECT_NAME
Parts: RECOGNIZED_PARTS
\end{lstlisting}


The following prompts are for \textbf{joint parameter estimation}:

For revolute joints, we first prompt GPT4o to determine whether both points of the joint are on the surface:

\begin{lstlisting}[style=markdownstyle]
You are an assistant with a deep understanding of the structure of objects. Your task is to help users determine the hinge position of some parts of a given object mesh using common sense. It is important to note that the object is represented by a mesh, so you only have access to the object's surface and no access to its inner structure. The term "hinge" here does not only refer to the mechanical structure of a hinge, but also has a broader meaning. For example, the connection between a cardboard box lid and the body of the box is also considered a hinge.

Specifically, the user will provide you with an object, and the part for which the hinge position needs to be predicted will be specified. You will have to decide whether (1) both ends of the hinge are positioned on the surface of the object or (2) only one end of the hinge is positioned near the surface of the object, and the other end is inside the object.

For example, the hinge of a door has both its ends positioned on the door frame, which is recognizable and falls into the first category. The hinge of a wheel has one end recognizable on the center of the wheel, and its other end hidden inside the car (normally an object mesh of a car will not have detailed mechanical structures), which falls into the second category.

Please give your answer in the following format:
```hinge_info
description: a brief description of the hinge
choice: (1) or (2)
```
\end{lstlisting}

Based on GPT4o's selection, we provide a tailored prompt. For choice (1), the specific prompt is as follows. Once GPT4o selects two or more points, we fit a line through these points, defining the rotation axis.

\begin{lstlisting}[style=markdownstyle]
You are an assistant with a deep understanding of the structure of objects. Your task is to answer some questions about the input image of an object. The input image is of a {object_name}. The image has some points marked, each with an numerical ID as a label. Please select the points that are on the rotation axis of the {part_name} of the {object_name}. Give your answer in the following format:

```hinge points
description: a brief description of the location of the rotation axis and the selected points
selected IDs: ID of selected point1, ID of selected point2, ... (for example 1,3)
```

Remember: 
(1) Do not answer anything not asked for.
(2) Select two or more points.
(3) Give your answer based on the provided image.
\end{lstlisting}

For choice (2), the prompt is as follows. The rotation axis is determined using the point selected by GPT4o and the normal of the connected area.
\begin{lstlisting}[style=markdownstyle]
You are an assistant with a deep understanding of the structure of objects. Your task is to answer some questions about the input image of an object. The input image is of a {object_name}. The image has some points marked, each with an numerical ID as a label. Please select the point that is on the rotation axis of the {part_name} of the {object_name}. Give your answer in the following format:

```hinge points
description: a brief description of the location of the rotation axis and the selected points
selected IDs: ID of the selected point
```

Remember: 
(1) Do not answer anything not asked for.
(2) Only select one point that is the most suitable.
(3) Give your answer based on the provided image.
\end{lstlisting}

For prismatic joints, we prompt GPT4o to determine whether its child link moves in and out or along the surface:
\begin{lstlisting}[style=markdownstyle]
You are an assistant with a deep understanding of the structure of objects. Your task is to help users to determine the translation direction of some parts of a given object mesh using common sense. It is important to note that the object is represented by a mesh, so you only have access to the object's surface and no access to its inner structure.

Specifically, the user will provide you with an object and the part for which the translation direction needs to be predicted will be specified. You will have to decide whether the translation direction is outwards from/inwards towards the mesh, or along the surface of the mesh.

When a part moves outwards, you will see more of that part coming out from the object. When a part moves inwards, you will see portions of that part going into the object. When a part moves along the surface of an object, you still see the exact same part. 

For example, a pressing button can be pressed inwards (when you press it, the button goes into the object), the telescopic handle of a suitcase can be pulled outwards (when you pull it, the entire handle comes out of the suitcase), and a stick shift moves along the surface of a shift pattern (when you are shifting, the shift does not go into the transmission or out of the transmission).

Please give your answer in the following format:
```translation_axis_info
description: a brief description of the translation axis
choice: outward/inward or surface
```
\end{lstlisting}
If the child link moves inward or outward, the translation direction is defined by the normal vector of the connected area. If it moves along the surface, we draw four arrows (originating from the child link's center and pointing up, down, left, and right) on the image and prompt GPT4o to select the translation direction:
\begin{lstlisting}[style=markdownstyle]
You are an assistant with a deep understanding of the structure of objects. Your task is to answer some questions about the input image of an object. The input image is of a {object_name}. The image has some arrows marked, each with a different color. Please select the arrow that indicate the translation direction of the {part_name} of the {object_name}. Give your answer in the following format:

```sliding direction
description: a brief description of the direction of the sliding axis and the selected arrow
selected arrow: color of the arrow (in red, yellow, blue, green)
```

Remember: 
(1) Do not answer anything not asked for.
(2) Select one arrow.
\end{lstlisting}

The selected arrow is then projected onto the plane fitted to the connecting area, and the translation direction is defined by the direction of the projected arrow.

4. Validate joint limits. For revolute joints, we incrementally rotate the child link based on the joint parameters and identify the maximum range where penetration remains below a predefined threshold.
For prismatic joints, we follow the joint limits provided by GPT4o.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
