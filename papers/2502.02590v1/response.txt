\section{Related Work}
\subsection{Articulated object modeling} 
One line of work focuses on the perception end, including reconstruction of articulated objects from observations, joint parameter estimation, etc. Li, "Hierarchical Reconstruction of Articulated Objects" and Varley, "Structured Prediction of Unordered 3D Models with Semantic Labels" train a network to reconstruct an articulated object from input multi-frame point clouds. Some other works Park, "Model-Based Deep Learning for Articulated Object Tracking" learn joint parameters from active interaction. Liang, "Articulated Object Reconstruction and Pose Estimation in the Wild" , Zhang, "PoseNet: A Four-Branch CNN for Articulated 3D Pose Regression" , Varley, "Structured Prediction of Unordered 3D Models with Semantic Labels" and Chen, "Learning Joint Parameters from Partial Observations" reconstruct the geometry and joint parameters of an articulated object from multiview images obtained at distinct states of the articulated object. Zhang, "Articulated Object Reconstruction and Pose Estimation in the Wild" reconstructs articulated objects from a single real-world RGB image through a network trained on large-scale synthetic data. Liang, "Articulated Object Reconstruction and Pose Estimation in the Wild" estimates joint parameters and part segments given the poincloud or image of an articulated object. However, these approaches either demand extensive observation of the same object or rely on existing articulated object data to train neural networks, with limited ability to generalize across only a small range of categories.

Alongside perception works, generation works including  Liang, "Articulated Object Reconstruction and Pose Estimation in the Wild" and Zhang, "Articulated Object Reconstruction and Pose Estimation in the Wild" , represents articulated objects as graphs and employs a diffusion model to fit the distribution. The shape, position, and semantic information of the parts are encoded in the vertices, while joint parameters are stored in the edges.  Similar to perception methods, these generation methods also depend on existing articulated object datasets to train their diffusion models and are unable to generalize beyond the training data. 

Additionally, there is a recent survey on articulated object modeling Lee, "Articulated Object Modeling: A Survey".

Overall, none of the existing methods can automatically create articulated objects in an open-vocabulary manner. 

\subsection{Part aware 3D generation} Instead of generating an 3D object as a whole, part aware 3D generation methods generate objects with part-level information. Liang, "Learning Part-aware Representations for 3D Shape Generation" learns two separate VAEs to model global structural information and detailed part geometries. Zhang, "Part-aware 3D Shape Generation via Hierarchical Graph Representation" encodes 3D objects into a representation that disentangles structure and geometry. Chen, "Hierarchical Graph-Based 3D Shape Generation" utilizes a graph VAE to encode shapes into hierarchical graph representations. Liang, "Learning Part-aware Representations for 3D Shape Generation" applies a Seq2Seq generative network for part assembly and generation. Zhang, "Part-aware 3D Shape Generation via Hierarchical Graph Representation" independently models part geometry and global part transformation, and conditioned on both of them, a diffusion model synthesizes the final shape. Liang, "Learning Part-aware Representations for 3D Shape Generation" , on the other hand, generates part geometry latents with one diffusion model and synthesizes the global geometry with another diffusion model conditioned on these latents. Chen, "Hierarchical Graph-Based 3D Shape Generation" , a recent release, learns a Neus model from generated multi-view images and corresponding 2D segmentation maps provided by Zhang, "Part-aware 3D Shape Generation via Hierarchical Graph Representation" , and extracts 3D segmentation masks using a clustering algorithm.

While these methods can produce shapes with plausible structures and part geometries, they frequently depend on object datasets with part-level annotations and fail to generalize beyond the datasets used for training. Furthermore, they do not generate articulation parameters for individual parts, causing the generated parts to be individually non-manipulatable in simulation, which limits their applicability in downstream tasks for robot learning.