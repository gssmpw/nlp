\section{Experiment}
\label{sec:exp}

We design our experiments to investigate the following questions: 
(i) What is ARSQ's performance when the offline dataset is suboptimal?
(ii) What is ARSQ's performance when online collected data is suboptimal?
(iii) How do various design factors of ARSQ affect the performance?

\textbf{Benchmarks.} We evaluate our approach on two continuous control benchmarks: D4RL \cite{D4RL} and RLBench \cite{RLBench}. Both domains provide access to online interaction data and a limited number of demonstrations, enabling us to assess the performance of ARSQ in diverse settings. We present representative results here due to limited space and leave full results in Appendix~\ref{sec:app-exp}.

\textbf{Baselines.} We use CQN \cite{CQN}, a state-of-the-art value-based RL method for continuous control, as our baseline. CQN employs a coarse-to-fine action selection strategy and independently predicts Q-values for each action dimension. Additionally, CQN trains using a combination of online training and offline demonstrations.
Besides, we also include DrQ-v2 \cite{DrQv2}, a renowned actor-critic algorithm designed for vision-based RL, along with its enhanced version, DrQ-v2+, as benchmarks. 
We also feature ACT \cite{ACT} and a CQN-style behavior cloning (BC) policy among our baselines.
Details about the baselines can be found in Appendix~\ref{sec:app-baseline}.

\subsection{Performance on D4RL}
\label{sec:exp-d4rl}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/6_exp-d4rl-full.png}
    \vspace{-1em}
    \caption{D4RL main results. \textit{mr}, \textit{m}, and \textit{me} represent \textit{medium-replay}, \textit{medium}, and \textit{medium-expert}, respectively.}
    \label{fig:exp-d4rl-full}
    \vspace{-0.5em}
\end{figure*}

\paragraph{Main results.}
To evaluate ARSQ's performance when the offline dataset is suboptimal, we consider three distinct locomotion tasks from the D4RL benchmark, each with three datasets of varying quality. 
The \textit{medium} dataset is gathered using a medium-level policy, whereas the \textit{medium-expert} dataset comprises a combination of medium-level and expert demonstrations. The \textit{medium-replay} dataset includes data ranging from completely random to medium-level.
The input to the model consists of state representations, while the output corresponds to torques applied at each hinge joint. 
A dense reward is provided to encourage completing the task, staying alive, and discourage vigorous actions that consume excessive energy.

We evaluate ARSQ, CQN \cite{CQN}, and BC in this setting.
At the beginning of online training, the replay buffer for both ARSQ and CQN is initialized with an offline dataset, and online data is added as the training progresses.
Additionally, both ARSQ and CQN incorporate the BC objective (Eq.~(\ref{eq:med-alg-bc})) towards offline dataset.
The BC baseline is trained solely offline using the offline dataset with the BC objective.
We report the converged performance of ARSQ, CQN and BC, averaged over three random seeds. 

As shown in Fig.~\ref{fig:exp-d4rl-full}, ARSQ exhibits outstanding performance across all nine datasets, demonstrating its ability to effectively identify suboptimal actions and learn more efficiently from the available offline data. 
ARSQ surpasses CQN, particularly in the \textit{medium-replay} and \textit{medium-expert} datasets, where optimal data is not predominant, highlighting that ARSQ is less biased toward frequently observed suboptimal actions. 
Notably, both ARSQ and CQN outperform BC, indicating that conducting reinforcement learning online enhances policy performance.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/6_exp-d4rl-quality-main.png}
    \vspace{-1em}
    \caption{D4RL results on different demonstration quality averaged over 3 tasks, with each task containing 3 datasets respectively. We report the normalized return provided by D4RL.}
    \label{fig:exp-d4rl-quality-main}
    \vspace{-3mm}
\end{figure}
\paragraph{Analysis on demonstration quality.}
To better investigate the influence of dataset quality, we rank trajectories by episode return for each dataset, and labeling the top $30$\%, middle $30$\%, and bottom $30$\% of the data as offline demonstrations.
The behavior cloning objective is applied only to these offline demonstrations. 
We report the converged performance ARSQ, CQN and BC.
As illustrated in Fig.~\ref{fig:exp-d4rl-quality-main}, ARSQ consistently outperforms both CQN and BC across all three levels of demonstration quality. 
Notably, when using the bottom $30$\% of data as offline demonstrations, ARSQ achieves approximately $2.2 \times$ the final performance of CQN.
In contrast, with the lowest demonstration quality, CQN performs slightly worse than BC, revealing CQN's sensitivity to demonstration quality, which negatively affects its online training. 
These results further validate the effectiveness of our method when using suboptimal offline datasets.


\subsection{Performance on RLBench}
\label{sec:exp-rlb}

To further evaluate ARSQ's performance, we focus on six tasks from RLBench \cite{RLBench}. 
The agent receives input as RGB images and proprioceptive states and outputs the change in joint angles to control the robot arm. 
Unlike D4RL, the reward is sparse, offering a binary value (0 or 1) only at the final timestamp. 
Although each task is provided with 100 expert demonstrations, the agent might gather unsuccessful trajectories during its interaction with the environment. 
This setup allows us to examine the performance when the data collected online is suboptimal.

In this domain, we evaluate the performance of ARSQ, CQN, DrQ-v2+, DrQ-v2, ACT and BC.
All reinforcement learning methods incorporate the behavior cloning objective (Eq.~(\ref{eq:med-alg-bc})) towards expert demonstrations and successful trajectories collected online.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/6_exp-rlb-task.png}
    \vspace{-1em}
    \caption{RLBench results on different tasks. Each experiment begins with 100 expert demonstrations, and all RL methods include a behavior cloning objective.}
    \label{fig:exp-rlb-task}
    \vspace{-1em}
\end{figure*}

As shown in Fig.~\ref{fig:exp-rlb-task}, ARSQ demonstrates superior performance compared to all other algorithms, highlighting its effectiveness in online learning with suboptimal collected data.
Additionally, ARSQ exceeds ACT, highlighting the importance of reinforcement learning in online training.

\vspace{-3mm}
\subsection{Ablation Studies}
\label{sec:exp-abl}

In this section, we evaluate the impact of key design factors in ARSQ: auto-regressive conditioning (Fig.\ref{fig:med-main}) and advantage prediction network (Fig.\ref{fig:med-nn}).

\paragraph{Ablation on auto-regressive conditioning}
We consider several variants of ARSQ on auto-regressive conditioning.
\vspace{-2mm}
\begin{itemize}\setlength{\itemsep}{0pt}
    \item \textit{Swap}: We reverse the conditioning order, applying dimensional conditioning first, followed by coarse-to-fine conditioning.
    \item \textit{w/o CF Cond.}: We remove the coarse-to-fine conditioning and output actions at multiple levels simultaneously.
    \item \textit{w/o Dim Cond.}: We remove the dimensional conditioning and instead output all action dimensions simultaneously at each level.
    \item \textit{w/o CF}: We replace the coarse-to-fine structure entirely by discretizing each action dimension into  $B^L$  bins and then applying dimensional conditioning.
    \item \textit{Plain}: We remove both the coarse-to-fine structure and dimensional conditioning.
\end{itemize}

\begin{figure}[ht]
    \vspace{-1em}
    \centering
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/6_exp-abl-ar-d4rl.png}
        \label{fig:exp-abl-ar-d4rl}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/6_exp-abl-ar-rlb.png}
        \label{fig:exp-abl-ar-rlb}
    \end{subfigure}
    \vspace{-1em}
    \caption{Ablation on auto-regressive conditioning in D4RL (left) and RLBench (right).}
    \label{fig:exp-abl-ar}
\end{figure}

As depicted in Fig.~\ref{fig:exp-abl-ar}, \textit{Swap} demonstrates a slight decline in performance, underscoring the effectiveness of the current conditioning order design.
Additionally, removing any of the components degrades performance to varying degrees.
When all components are removed, as in \textit{Plain}, the performance is at its lowest, emphasizing the significance of dimensional and coarse-to-fine action generation.

\paragraph{Ablation on shared backbone.}
The advantage network of ARSQ utilizes a shared backbone to reduce the number of parameters and speed up the learning process. 
To assess the impact of this choice, we introduce two variants. The network architecture of these two variants can be found in Appendix~\ref{sec:app-baseline}.
\vspace{-2mm}
\begin{itemize}\setlength{\itemsep}{0pt}
\item \textit{Separate}: We employ separate networks for each action dimension.
\item \textit{Level Shared}: We employ shared networks for each coarse-to-fine level.
\end{itemize}


\begin{figure}[h]
    \vspace{-0.5em}
    \centering
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/6_exp-abl-qc-d4rl.png}
        \label{fig:exp-abl-qc-d4rl}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/6_exp-abl-qc-rlb.png}
        \label{fig:exp-abl-qc-rlb}
    \end{subfigure}
    \vspace{-1em}
    \caption{Ablation on shared backbone in D4RL (left) and RLBench (right).}
    \label{fig:exp-abl-qc}
\end{figure}

As shown in Fig.~\ref{fig:exp-abl-qc}, the standard ARSQ consistently performs well in both environments. In contrast, using either a level-shared or separate backbone results in diminished performance.
This demonstrates the effectiveness of the shared backbone design. 
