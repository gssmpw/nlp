\section{Method}

In this section, we begin by discussing the process of discretizing multi-dimensional actions in a coarse-to-fine manner. 
Building on this, we extend the soft Q-learning theory with a focus on the dimensional soft advantage. 
Subsequently, we introduce our \arsq (ARSQ) algorithm, which is overviewed in Fig.~\ref{fig:med-main}.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/5_med-main.png}
    \vspace{-2em}
    \caption{The ARSQ algorithm. The action space is discretized using a coarse-to-fine approach. By predicting dimensional soft advantages, ARSQ generates actions in an auto-regressive manner within a single decision-making step.}
    \label{fig:med-main}
\end{figure*}

\subsection{Coarse-to-fine Action Discretization}
\label{sec:med-coarse}

To apply Q-learning \cite{NatureDQN} in a continuous domain, a straightforward approach is to discretize the action space \cite{DiscretePPO, CQN}. 
For a continuous action of $d$ dimensions $\mathbf{a}_c = (a_c^1, a_c^2, \ldots, a_c^d) \in \mathbb{R}^D$, the discretized action $\mathbf{a} = (a^1, a^2, \ldots, a^D)$ can be represented by

\begin{equation}
    a^d = \arg \max_i | a_c^d - b_i |
\end{equation}

where $\mathbf{b} = (b_1, \ldots, b_B)$ are the centers of $B$ discretization intervals, or bins, which typically provide a uniform separation of the given action space.
However, obtaining a finer separation of the action space necessitates a greater number of bins, thereby increasing the computational load when assessing the Q function for each discrete action bin.

To address this issue, we can apply a coarse-to-fine action discretization approach \cite{CQN}, similar to the method used in \cite{HD-CNN} for computer vision, as illustrated in Fig.~\ref{fig:med-main}. 
With $L$ levels and $B$ uniform separation bins at each level, the discrete action for dimension $d$ at level $l$ is expressed as:

\begin{equation}
    a^{d, l} = \lfloor \frac{a^{d} - \sum_{i=1}^{l-1} B^{L-i} a^{d, i}}{B^{L-l}} \rfloor
\end{equation}

Here, $\lfloor \cdot \rfloor$ represents the floor function.

During inference, the policy generates discrete actions progressively through each level $(\mathbf{a}^{\langle \cdot \rangle, 1}, \mathbf{a}^{\langle \cdot \rangle, 2}, \cdots, \mathbf{a}^{\langle \cdot \rangle, L})$. 
These are then combined to produce the final discrete action.

\subsection{Dimensional Soft Advantage for Policy Representation}
\label{sec:med-dsa}

Building on action discretization, we initially extend soft Q-learning to discrete spaces. 
The soft value function is expressed as
\begin{equation}
    V^*_{\text{soft}}(\mathbf{s}) = \alpha \log \sum_{\mathbf{a}' \in \mathcal{A}} \exp \left( \frac{1}{\alpha} Q^*_{\text{soft}}(\mathbf{s}, \mathbf{a}') \right)
\end{equation}
And we omit the subscript $t$ for $\mathbf{s}_t$ and $\mathbf{a}_t$
To further streamline the expression of the policy, we define the soft advantage.

\begin{definition}[Soft Advantage]
    The soft advantage of $\mathbf{a}$ at $\mathbf{s}$ is given by
    \begin{equation}
        A^*(\mathbf{s}, \mathbf{a}) = Q^*_{\text{soft}}(\mathbf{s}, \mathbf{a}) - V^*_{\text{soft}}(\mathbf{s})
    \end{equation}
\end{definition}

Similar to the advantage in policy gradient-based RL algorithms, the soft advantage assesses how much taking action $\mathbf{a}$ at state $\mathbf{s}$ is beneficial. 
Thus, the optimal policy in Eq.~(\ref{eq:pre-soft-policy}) can be expressed as
\begin{equation}
\label{eq:med-sa-pi}
    \pi^*(\mathbf{a} | \mathbf{s}) = \exp \left( \frac{1}{\alpha} A^*(\mathbf{s}, \mathbf{a}) \right)
\end{equation}
Considering the multi-dimensional action space, it still remains necessary to use a neural network to output $B^{L \times D}$ Q values in the final layer, as per the DQN \cite{NatureDQN}.

However, outputting such a large number of Q values imposes a significant computational burden on the neural network.
Inspired by auto-regression \cite{GPT3}, we address this problem by generating the Q function for a state-action pair in an auto-regressive manner. 

For clarity, we treats discrete action discussed in Sec.~\ref{sec:med-coarse} in one level. 
The multi-level coarse-to-fine discrete action can be considered as additional action dimensions, without compromising generalization.
We first define the dimensional soft advantage for policy representation.

\begin{definition}[Dimensional Soft Advantage]
    The dimensional soft advantage of the action $a^d$ at state $\mathbf{s}$, considering the previous dimensional actions $\mathbf{a}^{-d} = (a^1, \cdots, a^{d-1})$, is expressed by
    \begin{equation}
    \label{eq:med-dsa}
        \pi (a^d | \mathbf{s}, \mathbf{a}^{-d}) = 
        \frac
        {exp \left( \frac{1}{\alpha} A^d(\mathbf{s}, \mathbf{a}^{-d}, a^d)) \right)}
        {Z(\mathbf{s}, \mathbf{a}^{-d})}
    \end{equation}
\end{definition}

where $Z^d(\mathbf{s}, \mathbf{a}^{-d})$ represents the partition function.

However, the dimensional soft advantage is not related to other equations and remains intractable. 
To address this, we propose the following theorem to establish a connection between the dimensional soft advantage and the soft advantage.

\begin{theorem}
\label{the:med-sum}
    If dimensional soft advantage $m^d(\mathbf{s}, \mathbf{a}^{-d}, a^d))$ satisfies
    \begin{equation}
    \label{eq:med-dsa-cond}
        log \sum_{a^{d'}} \exp{\left( \frac{1}{\alpha} A^d(\mathbf{s}, \mathbf{a}^{-d}, a^{d'}) \right)} = 0
    \end{equation}
    then the soft advantage can then be expressed as the summation of the dimensional soft advantages
    \begin{equation}
        \sum_{d=1}^D A^d(\mathbf{s}, \mathbf{a}^{-d}, a^d) = A(\mathbf{s}, \mathbf{a})
    \end{equation}
\end{theorem}
\begin{proof}
    See Appendix~\ref{sec:app-proof}.
\end{proof}

Through Eq.~(\ref{eq:med-dsa}) and Theorem~\ref{the:med-sum}, we extend soft Q-learning to a auto-regressive policy along action dimension.

Since we do not introduce additional elements in policy optimization, the Q-iteration follows the same update rule as soft Q-learning.
Based on Eq.~(\ref{eq:pre-soft-update}), we have
\begin{equation}
\label{eq:med-dsa-update}
    V_{\text{soft}}(\mathbf{s}_t) + A(\mathbf{s}_t, \mathbf{a}_t) 
    \leftarrow r_t + \gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p(s)} \left[ V_{\text{soft}}(\mathbf{s}_{t+1}) \right]
\end{equation}
The maximum entropy policy described in Eq.~(\ref{eq:pre-soft-policy}) can be obtained by repeatedly applying Eq.~(\ref{eq:med-dsa-update}) until it converges.

\subsection{Auto-Regressive Soft Q-learning}
\label{sec:med-alg}
\input{tab/med-pseudo}

Building on the theory outlined in Sec.~\ref{sec:med-dsa}, we introduce the \arsq (ARSQ) algorithm. 
The pseudo code for the ARSQ algorithm is presented in Algorithm~\ref{pse:med-alg}. 
We will discuss the various design choices of ARSQ.

\paragraph{Behavior cloning objective.}
To leverage offline demonstration data during online training, we introduce an additional behavior cloning loss term. 
Following previous works \cite{DQfD,CQN}, we encourage actions present in the offline dataset to be preferred over other actions. 
Specifically, we define the loss as
\begin{equation}
\label{eq:med-alg-bc}
\begin{aligned}
    \mathcal{L}_{BC}^{d} = \sum_{a^{d}} \max ( 
    & A^{d, \theta_i}(\mathbf{s}, \mathbf{a}^{-d}_e, a^{d}) \\ 
    & - A^{d, \theta_i}(\mathbf{s}, \mathbf{a}^{-d}_e, a_e^{d}), C_m )
\end{aligned}
\end{equation}
where $\mathbf{a}_e$ denotes the expert action observed in the offline dataset, and $C_m$ is a hyper-parameter controlling the margin. 
This objective encourages the soft advantages of expert actions to be at least $C_m$ higher than those of other actions.

\paragraph{Policy representation.}
As discussed in Sec. \ref{sec:med-dsa}, ARSQ predicts dimensional soft advantages, which function as both components of the Q function and policy representation. 
The network architecture is illustrated in Fig. \ref{fig:med-nn}.
In practical design, the soft value $V_{\text{soft}}$ and the dimensional soft advantage $A^d$ are predicted using two separate neural networks. 
The advantage prediction network estimates the dimensional soft advantage for each action dimension, based on the partially generated action from previous dimensions, creating an auto-regressive sequence. 
In practical design, we use a globally-shared MLP in the advantage network, with separate heads to predict the dimensional soft advantages.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/5_med-nn.png}
    \vspace{-2em}
    \caption{Network architecture of ARSQ. The soft value $V_{\text{soft}}$ and the dimensional soft advantage $A^d$ are predicted by two separate networks. The advantage network utilizes a shared backbone, and advantage constraints are applied to its output.}
    \label{fig:med-nn}
\end{figure*}

Another challenge is applying the constraint of the dimensional soft advantage as per Eq.~(\ref{eq:med-dsa-cond}). 
Here, we enforce a hard constraint by normalizing each output head through log-sum-exp subtraction, ensuring consistency across outputs.
\begin{equation}
\label{eq:med-alg-cons}
\begin{aligned}
    A^d(\mathbf{s}_t, \mathbf{a}^{-d} &, a^d) 
    = u^d(\mathbf{s}_t, \mathbf{a}^{-d}, a^d) \\
    & - log \sum_{a^{d'}} \exp \left(
    \frac{1}{\alpha} u^d(\mathbf{s}_t, \mathbf{a}^{-d}, a^{d'})
    \right)
\end{aligned}
\end{equation}
where $u^d$ is the output of the $d$-th output head.

Furthermore, to stabilize training and address the over-estimation problem \cite{fujimoto2018addressing, DoubleQ}, we implemented a double Q network alongside a target network in our practical application.
Therefore, the optimization objective in Eq.~(\ref{eq:med-dsa-update}) is modified to
\begin{equation}
    \mathbf{y}_t =  \gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p(s)} \left[ \text{min} \left( V^{\overline{\phi}_1}_{\text{soft}1}(\mathbf{s}_{t+1}), V^{\overline{\phi}_2}_{\text{soft}2} (\mathbf{s}_{t+1}) \right) \right]
\end{equation}
\begin{equation}
\label{eq:med-alg-rl}
    \mathcal{L}_{RL} = \frac{1}{2} \left( V^{\phi_i}_{\text{soft}}(\mathbf{s}_t) + A^{\theta_i}(\mathbf{s}_t, \mathbf{a}_t) - \mathbf{y}_t  \right)
\end{equation}
where $V^{\overline{\phi}_i}_{\text{soft}}$ represents the soft value predicted by the target network.

\paragraph{Auto-regressive conditioning.} 
\label{sec:med-alg-ar}
In Sec.~\ref{sec:med-dsa}, we explained the process of handling discrete action in one coarse-to-fine level.
With multi-level coarse-to-fine action discretization, the auto-regressive conditioning encompasses two aspects. 
\emph{Dimensional conditioning} refers to generating actions for each dimension in an auto-regressive sequence, while \emph{coarse-to-fine conditioning} involves generating actions for each dimension from coarse to fine. 
In practice, we implement coarse-to-fine conditioning prior to dimensional conditioning. 
Specifically, dimensional conditioning serves as the inner conditioning, while coarse-to-fine conditioning acts as the outer conditioning across levels. 
We explore swapping the order of conditioning in Sec.~\ref{sec:exp-abl}, and the results indicate that the current design better captures interdependencies between action dimensions.
