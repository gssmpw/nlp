\section{Preliminaries}
\subsection{Problem Formulation}

In this paper, we consider the standard RL setting with the addition of a pre-collected dataset \(\mathcal{D}\) for continuous control. 
The problem can be represented as MDP, defined by the tuple \((\mathcal{S}, \mathcal{A}, \gamma, p, r, d_0)\). 
Here, \(\mathcal{S}\) is the continuous state space, \(\mathcal{A}\) is the continuous action space, \(\gamma \in (0,1)\) is the discount factor, \(p(s' \mid s,a)\) is the transition dynamics, \(r(s,a)\) is the reward function, and \(d_0(s)\) is the distribution of the initial state.
In addition to interacting with the environment online, we assume access to a pre-collected dataset \(\mathcal{D}=\{(s_i,a_i,r_i,s_i')\}\), which can substantially reduce sample complexity and provide broader state-action coverage.

\subsection{Soft Q Learning}

To improve policy exploration, maximum entropy RL enhances the reward by adding an entropy term \cite{MaxEntIRL, SoftQ, SAC}, so the optimal policy seeks to maximize entropy at every state it visits. The objective is defined as
\begin{equation}
    J(\pi) =  \sum_{t=0}^{T} \mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t) \sim \rho_\pi} \left[ r(\mathbf{s}_t, \mathbf{a}_t) + \alpha \mathcal{H}(\pi(\cdot | \mathbf{s}_t)) \right]
    \label{eq:pre-soft-obj}
\end{equation}
where $\mathcal{H}$ is entropy, $T$ is the episode length and $\rho_\pi$ is the trajectory distribution induced by policy $\pi$.
The temperature parameter $\alpha$ dictates how much importance is placed on the entropy term in comparison to the reward. 
Let the soft Q-function defined as:
\begin{equation}
\begin{split}
    & Q^*_{\text{soft}}(\mathbf{s}_t, \mathbf{a}_t) = r_t + \\
    & \mathbb{E}_{(\mathbf{s}_{t+1}, \dots) \sim \rho_\pi} 
    \left[ \sum_{l=1}^\infty \gamma^l \left( r_{t+l} + \alpha H\left(\pi^*(\cdot | \mathbf{s}_{t+l})\right) \right) \right]
\end{split}
\end{equation}
Then the optimal policy for Eq.~(\ref{eq:pre-soft-obj}) is given by
\begin{equation}
\label{eq:pre-soft-policy}
    \pi^*(\mathbf{a}_t | \mathbf{s}_t) = \exp \left( \frac{1}{\alpha} \left( Q^*_{\text{soft}}(\mathbf{s}_t, \mathbf{a}_t) - V^*_{\text{soft}}(\mathbf{s}_t) \right) \right)
\end{equation}
\begin{equation}
\label{eq:pre-soft-v}
    V^*_{\text{soft}}(\mathbf{s}_t) = \alpha \log \int_{\mathcal{A}} \exp \left( \frac{1}{\alpha} Q^*_{\text{soft}}(\mathbf{s}_t, \mathbf{a}') \right) d\mathbf{a}'
\end{equation}
Similar to the standard Q-function and value function, the Q-function can be connected to the value function at a future state using a soft Bellman equation.
\begin{equation}
\label{eq:pre-soft-update}
    Q^*_{\text{soft}}(\mathbf{s}_t, \mathbf{a}_t) = r_t + \gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p(\mathbf{s}_t, \mathbf{a}_t)} \left[ V^*_{\text{soft}}(\mathbf{s}_{t+1}) \right]
\end{equation}
The proof can be found in \cite{MaxEntIRL, SoftQ}.



