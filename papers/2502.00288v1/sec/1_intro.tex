\section{Introduction}
\label{sec:intro}


Deep reinforcement learning (RL) has demonstrated remarkable performance across various continuous control domains \cite{SAC, PPO}. However, these breakthroughs often come at the cost of extensive online interactions, which are required for effective convergence \cite{DotaFive, NatureDQN}. This reliance on large-scale exploration poses a major challenge in real-world applications, where data collection can be expensive, time-consuming, or even risky. To alleviate this burden, value-based RL methods, which directly approximate the Q-function rather than parameterizing a policy, have gained popularity due to their improved sample efficiency \cite{GQN, HGQN, DecQN} and have shown advances in continuous control tasks by discretizing each of the dimensions of continuous action spaces.~\cite{CQN}. Moreover, some studies integrate offline demonstration data into training to further accelerate early learning, reducing the dependence on purely online exploration \cite{RLPD}. In this paper, we adopt this training paradigm to address continuous control using value-based RL, incorporating offline data into the online training process.

For value-based RL, the discretization scheme results in an exponentially large discrete action space, making RL training and exploration challenging. To mitigate this, existing value-based methods often estimate the Q-value for each action dimension independently~\cite{SDQN, DecQN}. However, this simplification comes with a limitation—it neglects interdependencies between action dimensions, potentially leading to suboptimal decision-making. When training data exhibits multiple modes, such as a mix of optimal and suboptimal demonstrations, independently estimating Q-values can bias action selection toward the most frequent behaviors rather than the truly optimal ones.
This limitation is particularly pronounced in the early stages of learning, when the agent relies heavily on imperfect offline data and lacks sufficient online refinement.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/3_toy-env.png}
        \caption{An example dataset for a one-step decision-making environment.}
        \label{fig:intro-toy-env}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/3_toy-cqn.png}
        \caption{Q function given by independent action decomposition.}
        \label{fig:intro-toy-cqn}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/3_toy-ar.png}
        \caption{Q function given by auto-regressive action decomposition (Ours).}
        \label{fig:intro-toy-ar}
    \end{subfigure}
    \caption{A motivating example of how Q decomposition influences policy training, as detailed in Appendix~\ref{sec:app-example}.}
    \label{fig:intro-case}
\end{figure*}

Consider a simple one-step decision-making task with two-dimensional actions $(a_1,a_2) \in \mathcal{A} = [-1, 1]^2 \subset \mathbb{R}^2$, shown in Fig.~\ref{fig:intro-case}, where an agent selects an action $(a_1,a_2)$ given state $s$ and receives a reward $r$ before the episode terminates. Suppose the training dataset consists of three distinct modes: one optimal mode with $r=1$, two suboptimal modes with $r=0.1$ and $r=-1$, with the latter occurring more frequently. If the suboptimal modes are more prevalent in the dataset, conventional Q-learning approaches that estimate action dimensions independently, i.e., $Q(s,a_i)$, could undervalue the optimal mode. This bias can hinder the correct identification and reinforcement of the optimal action mode, leading to slow convergence and degraded policy performance.

To address this issue, we propose \arsq (ARSQ), a novel approach that captures cross-dimensional dependencies in discretized high-dimensional action spaces. 
Instead of treating each dimension independently, ARSQ adopts an auto-regressive structure, sequentially estimating advantages for each action dimension conditioned on the previously selected dimensions. This allows the method to better model interdependencies, ensuring that correlated action dimensions are jointly optimized rather than selected in isolation. Additionally, ARSQ adopts a coarse-to-fine hierarchical discretization strategy inspired by CQN~\cite{CQN}, further enhancing sample efficiency for fine-grained continuous control.
We theoretically show that the original Q function can be expanded into an auto-regressive formulation with dimensional advantage estimation under the framework of soft Q-learning. Our approach integrates these insights into an auto-regressive soft Q-network, which is specifically designed for continuous control tasks. 

To evaluate ARSQ, we conduct extensive experiments on the D4RL and RLBench continuous control benchmarks, challenging it against a variety of widely used reinforcement learning and imitation learning baselines. Results indicate that ARSQ consistently surpasses these baselines, achieving up to $1.62\times$ performance over existing value-based RL when trained with suboptimal demonstrations on D4RL. Ablation studies further highlight the significance of ARSQ’s key components, confirming its effectiveness in continuous control tasks.

Our contributions include:
\vspace{-0.5em}
\begin{itemize}
    \item We extend Soft Q-learning framework to value-based reinforcement learning with dimensional advantage estimation.
    \item We propose the ARSQ algorithm to capture dependencies in action dimensions and enhance learning from suboptimal data.
    \item Through extensive experiments, we demonstrate that ARSQ can learn better policies when data suboptimality arises from either offline datasets or data collected online.
\end{itemize}
