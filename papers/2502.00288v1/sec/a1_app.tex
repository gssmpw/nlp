\section{Proof of Theorem~\ref{the:med-sum}}
\label{sec:app-proof}

First, we express the policy using conditional probability, and then replace it with Eq.~(\ref{eq:med-dsa}).
\begin{equation}
\begin{aligned}
    \pi (\mathbf{a} | \mathbf{s})
    &= \prod_{d=1}^D \pi (a^d | \mathbf{s}, \mathbf{a}^{-d} ) \\
    &= \prod_{d=1}^D \frac
    {exp \left( \frac{1}{\alpha} A^d(\mathbf{s}, \mathbf{a}^{-d}, a^d) \right)}
    {Z(\mathbf{s}, \mathbf{a}^{-d})} \\
    &= \frac
    {\prod_{d=1}^D exp \left(\frac{1}{\alpha} A^d(\mathbf{s}, \mathbf{a}^{-d}, a^d) \right) }
    {\prod_{d=1}^D Z^d(\mathbf{s}, \mathbf{a}^{-d})} \\
    &= \frac
    {exp \left( \frac{1}{\alpha} \sum_{d=1}^D A^d(\mathbf{s}, \mathbf{a}^{-d}, a^d)\right)}
    {\prod_{d=1}^D Z^d(\mathbf{s}, \mathbf{a}^{-d})} \\
\end{aligned}
\end{equation}
We can then apply Eq.~(\ref{eq:med-dsa-cond}), resulting in
\begin{equation}
    \pi (\mathbf{a} | \mathbf{s})
    = exp \left( \frac{1}{\alpha} \sum_{d=1}^D A^d(\mathbf{s}, \mathbf{a}^{-d}, a^d)\right)
\end{equation}
Recall that the policy $\pi (\mathbf{a} | \mathbf{s})$ can be represented using the soft advantage as shown in Eq.~(\ref{eq:med-sa-pi}). Therefore, we have
\begin{equation}
    \sum_{d=1}^D A^d(\mathbf{s}, \mathbf{a}^{-d}, a^d)
     = A(\mathbf{s}, \mathbf{a})
\end{equation}

\section{Implementation Details}

\subsection{Action Selection}
As illustrated in Algorithm~\ref{pse:med-alg}, the action selection process receives inputs from $A_{\theta_1}$ and $A_{\theta_2}$ and produces $\mathbf{a}_t$. 
Eq.~(\ref{eq:med-sa-pi}) and Eq.~(\ref{eq:med-alg-cons}) describe the action selection process utilizing a single soft advantage network. 
To leverage the benefits of a double network, we employ two advantage networks to generate more precise actions. 
This process is detailed in Algorithm~\ref{pse:med-alg-act}.

\input{tab/med-pseudo-act}

\subsection{Variant of Behavior Cloning Objective}
As discussed in Sec.~\ref{sec:med-alg}, we incorporate an behavior cloning objective to effectively utilize offline demonstration data during online training, as defined in Eq.~(\ref{eq:med-alg-bc}). 

Following prior works \cite{CQL}, we also employ a variant of this objective, expressed as:  
\begin{equation}
    \mathcal{L}_{BC-v}^{d} = \max \left( 
    \text{log} \sum_{a^d \neq a^d_e}
    \text{exp} \left( A^{d, \theta_i}(\mathbf{s}, \mathbf{a}^{-d}_e, a^{d}) \right) - A^{d, \theta_i}(\mathbf{s}, \mathbf{a}^{-d}_e, a^{d}_e), C_m \right)
\end{equation}

where $a^d_e$ is the expert action and $C_m$ is a predefined margin constant.

We observe that this variant objective achieves better performance in scenarios where action modes are concentrated, such as in the \textit{medium} and \textit{medium-expert} series of datasets in D4RL.
Consequently, we adopt this variant objective when working with such datasets.

\subsection{Network Architecture}

In RLBench tasks, observations consist of a combination of RGB images and low-dimensional states. 
To compute the dimensional soft advantage for a given dimension, we first input the RGB images and low-dimensional states into a Convolutional Neural Network (CNN) \cite{CNN} encoder and a Multi-Layer Perceptron (MLP) \cite{MLP} encoder, respectively, to extract feature representations. 
These representations are then used to predict the soft value. 
Concurrently, the feature representations are combined with actions from previous dimensions and coarse-to-fine levels to create auto-regressive conditioning. 
An MLP-based shared backbone and output head are then utilized to determine the dimensional soft advantage for the given dimension.

In D4RL tasks, observations consist solely of low-dimensional states, and feature representations are derived directly from these states.


\subsection{Hyper-parameters}
\input{tab/med-alg-hyperparam}

The hyperparameters of ARSQ are presented in Table~\ref{tab:alg-hyperparam}. 
We provide the typical hyperparameters for ARSQ in D4RL (\textit{hopper-medium}) and RLBench (\textit{Open Oven}). 
In RLBench, ARSQ employs RandomShift \cite{DrQv2} for image augmentation. 
Additionally, ARSQ utilizes SiLU \cite{SiLU} and LayerNorm \cite{LayerNorm} as activation functions in RLBench.


\section{Experiment Setup}

\subsection{Motivating Example Setup}
\label{sec:app-example}

As introduced in Sec.~\ref{sec:intro} and illustrated in Fig.~\ref{fig:intro-case}, we consider a motivating example to demonstrate the impact of Q decomposition on policy training. 
The dataset is depicted in Fig.~\ref{fig:intro-toy-env}, with each point to be a data point in the dataset.
The color of the data points indicates the reward of the data point.
To illustrate the Q function of value-based RL algorithms, we first discretize the action space with $2$ bins in each action dimension. 

\begin{itemize}
    \item Q function given by independent action decomposition is an example of DecQN \cite{DecQN}, as well as in CQN \cite{CQN}, which features just a single coarse-to-fine level. 
    In this setting, we employ separate tabular Q functions, $Q(s, a_1)$ and $Q(s, a_2)$, for action dimension 1 and action dimension 2. 
    The Q function is learned by gradient descent.
    \item For the Q function obtained through auto-regressive action decomposition, we employ both tabular soft advantage functions, $A^1(s, a_1)$ and $A^2(s, a_1, a_2)$ for action dimension 1 and action dimension 2, and a tabular soft value function $V_{\text{soft}}(s)$. 
    The Q value reported in Fig.~\ref{fig:intro-toy-ar} is a sum of the soft value and the dimensional soft advantage of the corresponding dimensions, i.e., $Q(s, a_1, a_2) = V_{\text{soft}}(s) + A^1(s, a_1) + A^2(s, a_1, a_2)$. The soft advantage functions and the soft value function are simultaneously learned through gradient descent.
\end{itemize}



\subsection{Environment and Dataset}
\label{sec:app-env}

\paragraph{D4RL Gym Environment}
D4RL \cite{D4RL} provides datasets for various tasks to evaluate the performance of reinforcement learning. 
In this context, we use 3 Gym Locomotion tasks and datasets from D4RL to assess the performance of ARSQ and other baselines. 
These tasks are illustrated in Fig.~\ref{fig:app-env-d4rl}. The agent's observations include its states, such as the angle and velocity of each rotor. 
The agent's actions consist of torques applied between the robot's links, constrained within the range of $(-1, 1)$. 
The reward is dense, offering incentives for task completion and survival, while penalizing excessive energy-consuming actions.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/app-env-d4rl.png}
    \caption{D4RL Gym tasks used in experiment.}
    \label{fig:app-env-d4rl}
\end{figure}

\paragraph{D4RL Dataset}
In D4RL, we use the \textit{medium-replay}, \textit{medium}, and \textit{medium-expert} datasets for tasks involving \textit{half-cheetah}, \textit{hopper}, and \textit{walker2d}. 
In Section~\ref{sec:exp-d4rl}, to examine the impact of dataset quality, we rank trajectories based on episode returns within these nine datasets. 
Specifically, we compute the total reward for each data chunk within each dataset. 
We then rank these data chunks and select the top, middle, and bottom $30\%$ accordingly.
This is akin to rank trajectories but is easier to handle.

To better demonstrate the suboptimal nature of the datasets, we have plotted a histogram of the data chunk rewards, as shown in Fig.~\ref{fig:app-env-data-d4rl}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/app-env-data-d4rl.png}
    \caption{Histogram of reward in D4RL datasets.}
    \label{fig:app-env-data-d4rl}
\end{figure}


\paragraph{RLBench Environment}
RLBench \cite{RLBench} serves as a benchmark and learning environment for robot control. 
We have selected 20 tasks from RLBench and present results for 6 of them in Sec.~\ref{sec:exp}. 
An illustration of the environment can be seen in Fig.~\ref{fig:app-env-rlb}. 
The input consists of RGB images with a resolution of 84 Ã— 84, captured from four camera angles: front, wrist, left-shoulder, and right-shoulder, along with a history of the past seven observations. 
The output specifies the change in joint angles at each time step, utilizing the delta JointPosition mode provided by RLBench. 
In our experiments, we use a binary sparse reward system (0 or 1), which is awarded only at the final timestamp of an episode to indicate task success.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/app-env-rlb.png}
    \caption{Example of RLBench tasks used in experiment.}
    \label{fig:app-env-rlb}
\end{figure}



\subsection{Baselines and Evaluation Details}
\label{sec:app-baseline}

\paragraph{Main results baselines.}
As mentioned in Sec.~\ref{sec:exp-d4rl}, within D4RL, we utilize the implementation from \cite{CQN} and modify its CNN-based encoder to an MLP-based encoder as the CQN baseline. 
The BC baseline originates from CQN but operates with the RL learning objective turned off and without any online environment interaction.

In RLBench, we use the baselines and results of CQN, DrQ-v2+, DrQ-v2, ACT, and CBC as reported in the original CQN paper \cite{CQN}.

\paragraph{Ablation study baselines.}
As mentioned in Sec.~\ref{sec:exp-abl}, we utilize the \textit{Separate} and \textit{Level Shared} backbone baselines for an ablation study to explore the effectiveness of the shared backbone in the advantage network. 
The network architectures of these two baselines are illustrated in Fig.~\ref{fig:app-baseline-abl-nn-s} and Fig.~\ref{fig:app-baseline-abl-nn-ls}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{fig/5_med-nn-abl-s.png}
    \caption{Network architecture of \textit{Separate} backbone baseline in ablation study.}
    \label{fig:app-baseline-abl-nn-s}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{fig/5_med-nn-abl-ls.png}
    \caption{Network architecture of \textit{Level Shared} backbone baseline in ablation study.}
    \label{fig:app-baseline-abl-nn-ls}
\end{figure}



\section{Additional Results}
\label{sec:app-exp}

\paragraph{Sensitivity of temperature coefficient $\alpha$.}
Our methods are derived from Soft Q-learning, which aims to achieve a maximum-entropy policy.
The temperature coefficient $\alpha$ in Eq.~(\ref{eq:pre-soft-obj}) affects the balance between maximizing policy entropy and the reward from the environment. 
We conducted experiments to examine how varying $\alpha$ impacts policy learning. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/6_exp-abl-alpha-d4rl.png}
        \label{fig:exp-abl-alpha-d4rl}
    \end{subfigure}
    \hspace{0.01\textwidth}
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/6_exp-abl-alpha-rlb.png}
        \label{fig:exp-abl-alpha-rlb}
    \end{subfigure}
    \caption{Sensitivity of temperature coefficient $\alpha$ in D4RL and RLBench.}
    \label{fig:exp-abl-alpha}
\end{figure}

As shown in Fig.~\ref{fig:exp-abl-alpha}, a very high $\alpha$ results in reduced performance and unstable training, whereas a very low $\alpha$ also hampers policy improvement by restricting exploration.

\paragraph{D4RL results per task for different demonstration quality.}
In Sec.~\ref{sec:exp-d4rl}, we present the D4RL results, averaged over all 9 datasets, based on varying demonstration quality. 
The results for each task are illustrated in Fig.~\ref{fig:exp-d4rl-quality}. 
ARSQ consistently outperforms the CQN and BC baselines in nearly every task, demonstrating its ability to maintain stable performance across datasets of varying quality.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/6_exp-d4rl-quality.png}
    \caption{D4RL results per task on different demonstration quality.}
    \label{fig:exp-d4rl-quality}
\end{figure}

\paragraph{RLBench results in all 20 tasks.}
In Sec.~\ref{sec:exp-rlb}, we present results for six selected tasks from RLBench. 
The complete results for all 20 tasks are displayed in Fig.~\ref{fig:exp-rlb-task-all}. 
These results indicate that ARSQ performs comparably or better across these tasks, showcasing its ability to learn effectively even when the data collected online is not optimal.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/6_exp-rlb-full.png}
    \caption{RLBench results in all 20 tasks.}
    \label{fig:exp-rlb-task-all}
\end{figure}



\section{Computational Cost Analysis}
As discussed in Sec.~\ref{sec:med-alg}, ARSQ generates actions in each dimension in an auto-regressive manner. 
To analyze the overhead, we conducted experiments on both D4RL (\textit{hopper-medium}) and RLBench (\textit{Open Oven}) tasks. The training and inference times for ARSQ and CQN were evaluated 1,000 times and averaged. 
These experiments were conducted on a single Nvidia RTX 3090 graphics card.

The results are shown in Fig.~\ref{fig:exp-abl-compute}. 
ARSQ exhibits similar training times to CQN, due to the parallel optimization implemented and the batch training nature of the auto-regressive model. 
However, ARSQ experiences higher inference latency compared to CQN. 
We aim to address this issue by grouping the action dimensions and outputting the grouped dimensional actions auto-regressively, a solution we plan to explore in future work.

\begin{table}[h]
    \centering
    \begin{tabular}{c|cc}
        \toprule
         & D4RL & RLBench \\
        \midrule
        ARSQ Inference & 4.1 & 32.1 \\
        ARSQ Training & 12.2 & 290.5  \\
        CQN Inference & 2.6 & 6.9 \\
        CQN Training & 11.6 & 260.5 \\
        \bottomrule
    \end{tabular}
    \caption{Computational time in D4RL and RLBench (ms). }
    \label{fig:exp-abl-compute}
\end{table}
