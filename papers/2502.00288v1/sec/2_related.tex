\section{Related Works}

\paragraph{Value-based RL for Continuous Control.}
Despite their inherently straightforward critic-only framework, value-based reinforcement learning (RL) algorithms have achieved notable success \cite{NatureDQN,AlphaGo,AlphaZero,GQN,CQN}. 
Although these algorithms are primarily designed for discrete action spaces, recent efforts have sought to adapt them to continuous control by discretizing the continuous action space \cite{BDQ,DecQN}. 
However, the curse of dimensionality remains a significant challenge, as the number of discretization bins increases exponentially with the action dimension \cite{DDPG}.
To address this issue, some studies have modified the Markov Decision Process (MDP) of the environment, transforming it into a sequential decision-making problem along the action dimension \cite{SDQN,QTransformer}. 
Other approaches treat each action dimension independently, generating the Q function separately for each dimension \cite{BDQ,HGQN,DecQN,GQN}, akin to treating each action dimension as a multi-agent RL problem \cite{COMA,MAPPO}.
Recent research \cite{CQN} has employed a coarse-to-fine discretization approach to improve sample efficiency.
However, treating each action dimension independently may disrupt the correlation between different action dimensions, potentially diminishing performance in policy optimization.
Some studies \cite{CQNAS} have attempted to solve this issue through action sequence prediction.
Our approach generates actions in an auto-regressive manner, considering the correlations between dimensions and improving policy learning, which is orthogonal to \cite{CQNAS}.

\vspace{-1em}
\paragraph{Online RL with Offline Demonstration.}
Deep reinforcement learning often requires a large amount of online interactions to achieve convergence \cite{DotaFive, NatureDQN}. 
To address this challenge, many methods have been proposed that leverage offline demonstrations to guide online exploration and accelerate policy training \cite{DAPG, RLPD}. 
Some approaches involve performing offline RL pretraining before initiating online RL training \cite{Off2On, CalQL, UniO4, BOORL}. 
However, these approaches often depend on expensive offline pretraining. 
To mitigate this, some works explore incorporating offline demonstration data directly into the training process. 
One strategy initializes the replay buffer with offline data \cite{DQfD, RLPD}, while another balances sampling between online and offline data to improve training stability \cite{PEX, Modem}. 
Additionally, certain methods explicitly introduce a behavior cloning loss to leverage high-quality demonstrations for better guidance \cite{NPPAC, DAPG, HERDDPG}.  
In this work, we adopt the paradigm of integrating offline demonstrations into training to enhance sample efficiency in continuous control tasks. 
Specifically, we improve value-based RL by introducing an auto-regressive structure that sequentially estimates advantage for each action dimension. 
This design enables better handling of suboptimal data, whether from offline demonstrations or trajectories collected during training.
