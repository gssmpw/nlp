\section{Conclusion}
In this paper, we introduced \arsq (ARSQ), a novel value-based RL approach tailored for continuous control tasks with suboptimal data. 
ARSQ addresses the limitations of existing value-based methods by adopting an auto-regressive structure that sequentially estimates soft advantage for each action dimension, thereby capturing cross-dimensional dependencies. 
Through empirical evaluations, we show that ARSQ significantly surpasses existing methods, highlighting its effectiveness in learning from suboptimal data.

For future directions, an adaptive coarse-to-fine discretization can be used to balance control granularity with the overhead of additional bins. 
Another approach to explore is grouping unrelated dimensions to shorten the conditioning chain length, thereby speeding up computation. 
