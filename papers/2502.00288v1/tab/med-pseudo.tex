\begin{algorithm}[tb]
    \caption{Auto-Regressive Soft Q Algorithm (ARSQ) }
    \label{pse:med-alg}
\begin{algorithmic}
    \STATE Initialize $\theta_{1, 2}, \phi_{1, 2}$ for $A^{\theta_i}$ and $V_{\text{soft}}^{\phi_i}$
    \STATE Assign target parameters $\overline{\theta}_i,  \overline{\phi}_i \leftarrow \theta_i, \phi_i$.
    \STATE Offline dataset $\mathcal{D}$, replay buffer $\mathcal{R} \leftarrow \mathcal{D}$.
    \FOR{each epoch}
        \FOR{each environment step}
            \STATE select $\mathbf{a}_t$ with $A_{\theta_1}$ and $A_{\theta_2}$ (\ref{eq:med-sa-pi}, \ref{eq:med-alg-cons})
            \STATE $\mathbf{s}_{t+1} \sim p(\mathbf{s}_{t+1} | \mathbf{s}_t, \mathbf{a}_t)$
            \STATE $\mathcal{R} \leftarrow \mathcal{R} \cup \{ \mathbf{s}_t, \mathbf{a}_t, r_t, \mathbf{s}_{t+1} \}$
        \ENDFOR
        \FOR{each gradient step}
            \STATE Sample mini-batch $b_D$, $b_R$ from $\mathcal{D}$, $\mathcal{R}$
            \STATE Calculate $\mathcal{L}_D = \mathcal{L}_{RL} + \beta \mathcal{L}_{BC}$ with $b_D$ (\ref{eq:med-alg-bc}, \ref{eq:med-alg-rl})
            \STATE Calculate $\mathcal{L}_R = \mathcal{L}_{RL}$ with $b_R$ (\ref{eq:med-alg-rl})
            \STATE Update $m_{\theta_i}$ according to $\hat{\nabla}_{\theta_i}(\mathcal{L}_D + \mathcal{L}_R)$
            \STATE Update $V_{s, \phi_i}$ according to $\hat{\nabla}_{\phi_i}(\mathcal{L}_D + \mathcal{L}_R)$
            % \STATE Update target networks $\overline{\theta}_i \leftarrow \rho \overline{\theta}_i + (1 - \rho) \theta_i$
            % \STATE Update target networks $\overline{\phi}_i \leftarrow \rho \overline{\phi}_i + (1 - \rho) \phi_i$
            \STATE Update target networks $\overline{\theta}_i \leftarrow \rho \overline{\theta}_i + (1 - \rho) \theta_i$ and $\overline{\phi}_i \leftarrow \rho \overline{\phi}_i + (1 - \rho) \phi_i$.
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}
