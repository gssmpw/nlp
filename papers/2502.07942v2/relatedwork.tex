\section{Related work}
% LLM-based web agents have gained significant attention in recent years due to their
%     potential to automate, optimize, and enhance various tasks across the web, such
%     as information retrieval, decision making, and interaction with dynamic environments

    \textbf{Web Agents.} LLM-based web agents have gained significant attention in recent years due to their ability to automate, optimize, and enhance a wide range of web-based tasks, such as information retrieval, decision-making, and interactions within dynamic environments~\cite{zhou2023webarena,
    deng2023mind2web, yao2023webshop, pan2024webcanvas, levy2024stwebagentbench,
    dechezelles2024browsergym}.  Many existing approaches~\cite{koh2024tree,
    putta2024agentq, yu2025exact}
    utilize search-based methods like Monte Carlo Tree Search (MCTS) to obtain
    more online examples from the web environments. 
    Although these methods benefit from increased interactions, their performance does not scale with the number of interactions. In contrast, our framework introduces an iterative symbiotic improvement cycle that continually refines both data synthesis and task performance. More detailed related work is provided in Appendix~\ref{app:related}.
    % These methods typically require more interactions with the environment, yet their performance remains unchanged as the number of interactions increases. Therefore, we propose a framework that enables iterative symbiotic improvement. 

    \noindent\textbf{Knowledge Distillation.} Knowledge distillation is a pivotal technique for transferring the advanced capabilities of a large, powerful model to a smaller, more efficient one~\cite{Gou_2021, xu2024survey}. Recent work on LLM distillation~\cite{hinton2015distilling, anand2023gpt4all, hsieh2023distilling} has shifted the focus from merely replicating the teacher modelâ€™s output to capturing its underlying reasoning and decision-making processes. Additionally, several methods have been proposed to fine-tune language models for web tasks~\cite{yin2024agent,hong2024cogagent,lai2024autowebglm}, further enhancing decision-making abilities. Despite these efforts, distilled small LLMs still lag behind their larger counterparts in performance. Our work addresses this gap by introducing a novel distillation approach that leverages iterative symbiotic improvements to enhance the capabilities of small LLMs. More detailed distillation related work is provided in Appendix~\ref{app:related}.
    % Knowledge Distillation is a pivotal technique for transferring advanced capabilities from a larger, more powerful model to a smaller, more efficient one~\cite{Gou_2021, xu2024survey}. 
    % Moreover, recent work~\cite{anand2023gpt4all, hsieh2023distilling} on LLM distillation has shifted focus from replicating the output behavior of the teacher model to learning the thinking and reasoning paradigms capabilities. Also, there are methods trying to fine-tune language models for web tasks to enhance decision-making capabilities ~\cite{yin2024agent,hong2024cogagent,lai2024autowebglm}. The distilled small LLMs still exhibit a performance gap compared to the large LLMs. Our work introduces a novel distillation method to enhance their performance. More detailed related work is provided in Appendix~\ref{app:related}.