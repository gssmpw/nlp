
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{placeins} 
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}







\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}       %
\hypersetup{colorlinks,linkcolor={red}} \usepackage{xspace}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage{algpseudocode}
% \usepackage{amsmath}
% Attempt to make hyperref and algorithmic work together better:

% Use the following line for the initial blind version submitted for review:


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{url}
\usepackage[most]{tcolorbox}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{threeparttable}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{colortbl}
\usepackage{color}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{bbding}
\usepackage{tabularx}
\usepackage{mathrsfs}
% \newtheorem{theorem}{Theorem}
\usepackage{bbm}
% \usepackage[table,xcdraw,dvipsnames]{xcolor}

\usepackage{listings}

% if you use cleveref..
\usepackage[capitalize, noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\ourmethod}{{\fontfamily{lmtt}\selectfont \textbf{AgentSymbiotic}}\xspace}
\usepackage{algorithm}
\usepackage{algorithmic}


\usepackage{fix-cm}



\definecolor{customGreen}{rgb}{0.32, 0.64, 0.16}


% \newcommand{\ourmethod}{\texttt{AgentSymbiotic}\xspace}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.






\title{Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}




\author{Ruichen Zhang\thanks{\ \ Equal contribution.}$^{1}$, Mufan Qiu$^{*1}$, Zhen Tan$^{*2}$, Mohan Zhang$^{1}$, Vincent Lu$^{3}$, Jie Peng$^{1}$,\\ \textbf{Kaidi Xu}$^{4}$\textbf{,} \textbf{Leandro Z. Agudelo}$^{5}$\textbf{,} \textbf{Peter Qian}$^{3}$\textbf{,}
\textbf{Tianlong Chen}$^{1}$ \\
$^{1}$University of North Carolina at Chapel Hill, $^{2}$Arizona State University, \\ $^{3}$Ventus AI, $^{4}$Drexel University, $^{5}$Daice Labs\\
% $^*$Equal Contribution
}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
% \vspace{-3mm}
Web browsing agents powered by large language models (LLMs) have shown tremendous potential in automating complex web-based tasks. 
    Existing approaches typically rely on large LLMs (\textit{e.g.}, \texttt{GPT-4o}) to explore web environments and generate trajectory data, which is then used either for demonstration retrieval (for large LLMs) or to distill small LLMs (\textit{e.g.}, \texttt{Llama3}) in a process that remains \textit{decoupled} from the exploration.
    In this paper, we propose \ourmethod, an iterative framework that \textit{couples} data synthesis with task-performance, yielding a \underline{\textit{\textbf{“symbiotic improvement”}}} for both large and small LLMs. Our study uncovers a \textit{complementary dynamic} between LLM types: while large LLMs excel at generating high-quality trajectories for distillation, the distilled small LLMs—owing to their distinct reasoning capabilities—often choose actions that diverge from those of their larger counterparts. This divergence drives the exploration of novel trajectories, thereby enriching the synthesized data. However, we also observe that the performance of small LLMs becomes a bottleneck in this iterative enhancement process. To address this, we propose two \textit{innovations} in LLM distillation: a \underline{\textit{\textbf{speculative data synthesis}}} strategy that mitigates off-policy bias, and a \underline{\textit{\textbf{multi-task learning}}} approach designed to boost the reasoning capabilities of the student LLM. Furthermore, we introduce a \underline{\textit{\textbf{Hybrid Mode for Privacy Preservation}}} to address user privacy concerns. Evaluated on the \textsc{WebArena} benchmark, \ourmethod achieves SOTA performance with both LLM types. Our best Large LLM agent reaches $52\%$, surpassing the previous best of $45\%$, while our 8B distilled model demonstrates a competitive $49\%$, exceeding the prior best of $28\%$. Code will be released upon acceptance.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The autonomous navigation and completion of tasks on the web is a critical capability for AI~\cite{xie2023openagents,yao2023webshop,zhou2023language}. Recent advances in large language models (LLMs) have enabled impressive progress in web browsing agents, as demonstrated by benchmarks such as \textsc{WebArena}~\cite{zhou2023webarena}. Traditionally, current approaches~\cite{su2025learn} adopt a \textit{decoupled} paradigm: First, a data synthesis phase deploys a large LLM to interact with the web environment and generate trajectory data; Subsequently, a task-performing phase uses this data—either as demonstration retrieval for large LLMs or as distillation material for small LLMs.

% However, deploying such agents faces two major challenges: the infeasibility for even closed-source large LLMs to explore the environment comprehensively, and the degraded performance of existing small LLMs deployed locally.

% A common approach is to choose between these two options - either choose the closed-source large LLMs or open-source small LLMs. However, we argue that this represents a false dichotomy. 

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.50\textwidth]{fig/fig0.pdf} % 调整宽度为单栏
    \vspace{-6mm}
    \caption{\small Illustration of the symbiotic improvement between small and large LLMs, where each of them benefits the other.}
    \label{fig:0} 
    \vspace{-3mm}
\end{figure}

% 
In this work, we show that large and small LLMs can engage in a \underline{\textbf{\textit{symbiotic}}} relationship, which enhances both data synthesis and distillation in a \textit{coupled} iterative manner, as illustrated in Figure~\ref{fig:0}. Specifically, we introduce \ourmethod, a novel framework in which large and small LLMs collaborate through an iterative improvement cycle. The process is described as follows:

% each helping to improve the other's capabilities. Our key insight is that their complementary strengths and weaknesses can be leveraged to create a continuously improving system. 

% - large LLMs' superior reasoning but high cost, and small LLMs' efficiency but lower reliability -

\begin{figure*}[t] 
    \centering
    \includegraphics[width=\textwidth]{fig/fig1.pdf}
    \vspace{-5mm}
    \caption{\small Overview of the \ourmethod framework. Step \textcolor{red}{1}: The large LLM interacts with the environment to generate high-quality trajectories, which are then used to distill small LLMs. Step \textcolor{red}{2}: Multi-task learning and Speculative Data Synthesis are applied during distillation to enhance the reasoning capabilities of the small LLM  and mitigate off-policy bias between the two LLMs. Step \textcolor{red}{3}: The small LLM further explores the environment to produce diverse and valuable trajectories. Step \textcolor{red}{4}: Then the knowledge base containing high-quality trajectories and comprehensive trajectories is incorporated into the large LLM's RAG process, improving its performance. This iterative process establishes a symbiotic improvement cycle, enhancing both large and small LLMs over time.}
    \label{fig:1} 
    \vspace{-3mm}
\end{figure*}

% \vspace{-2mm}
% \vspace{-1em}
% \begin{enumerate}[itemsep=1.5pt]
    \noindent$\rhd$~\textit{Step} $1$ - \textbf{Trajectory Generation:}
    The large LLM utilizes retrieval-augmented generation (RAG)~\cite{wu2024longmemeval} to refine its performance. By learning from both successful and failed trajectories during rounds of self-interaction, it produces increasingly robust navigation paths.
    
    \noindent$\rhd$~\textit{Step} $2$ - \textbf{Trajectory Distillation:}
    A multi-LLM debate mechanism~\cite{du2023improving,liang-etal-2024-encouraging} is employed to evaluate the generated trajectories. Selected trajectories serve as critical data for distilling small LLMs.
    % \vspace{-1mm}
    
    \noindent$\rhd$~\textit{Step} $3$ - \textbf{Small LLM Exploration:}
    Small LLMs, distilled from the large LLM, are deployed to explore the environment more efficiently and extensively due to their faster inference speeds and increased stochasticity in action generation~\cite{sanh2019distilbert}. This process uncovers diverse trajectories—including edge cases or novel solutions that the large LLM might overlook.
    
    % \vspace{1mm}
    \noindent$\rhd$~\textit{Step} $4$ - \textbf{Symbiotic Improvement:} 
     This iterative cycle creates a mutually beneficial loop: the large LLM refines its generation capabilities with enriched feedback from small LLM explorations, while the small LLM continually benefits from high-quality data and distilled expertise provided by the large LLM.
% Together, this cycle exemplifies symbiotic cooperation, where the large LLM assists the small LLMs in generating higher-quality exploration, and the small LLMs, in turn, contribute back by enriching the large LLM’s knowledge base for RAG, driving consistent improvements in their respective capabilities.
% \vspace{2mm}
% \end{enumerate}


Despite these advances, a significant limitation remains: the performance of small LLMs distilled from large models often falls short of the level required to fully support and enhance large LLMs. Our analysis identifies two root causes behind this gap: (a) \textit{Off-policy bias}~\cite{caccia2024fine} arises when the training data—generated under a large LLM's policy—diverges from the small LLM’s deployment environment; and (b) the loss of critical reasoning capabilities during distillation~\cite{guo2025deepseek} further undermines the small LLM's effectiveness. Based on these insights, we introduce here two key technical innovations in distilling web browsing agents: (a) A \textbf{speculative data synthesis strategy} that mitigates off-policy bias by leveraging multiple action candidates generated by the large LLM to filter and refine the distillation trajectories; and (b) A \textbf{multi-task learning} approach that jointly predicts actions and intermediate reasoning steps, thereby preserving the critical reasoning abilities of the distilled model.

% Furthermore, a significant limitation in existing distillation techniques is observed: the performance of small LLMs distilled from large ones often falls short of the standards necessary to effectively support and enhance large LLMs. Through detailed analysis, two root causes behind this gap were identified: (a) \textit{Off-policy bias}~\cite{caccia2024fine}, which arises when the data used to train the small LLMs is generated by the large LLM under a different policy, leading to a mismatch between the training data distribution and the small LLM’s actual deployment environment. (b)~The
% % Additionally, there is a 
% \textit{lack of preservation} for critical reasoning capabilities during the distillation process. Based on these insights, two key technical innovations are therefore introduced to address fundamental challenges in distilling web browsing agents. First, the off-policy bias problem is addressed through a \textbf{speculative data synthesis strategy}, which leverages multiple action candidates generated by the large LLM to filter small LLM's action and enhance the quality of distillation trajectories. Second, the crucial reasoning capabilities of distilled LLMs are preserved using \textbf{multi-task learning}, which jointly predicts both actions and intermediate reasoning steps.

Moreover, as real-world deployments of web agents must safeguard user privacy—particularly when handling sensitive data such as passwords, credit card details, or phone numbers—we integrate a \textbf{hybrid mode for privacy preservation}. In this mode, any step that might involve private data is delegated to a local small LLM rather than a cloud-based large LLM, ensuring confidentiality.
% Beyond improving task completion rates, real-world deployments of web agents must address user privacy concerns—especially when private data (e.g., passwords, credit card information,phone number) is encountered. To this end, a hybrid mode for privacy preservation is incorporated, delegating any potentially private step to a local small LLM instead of relying on the cloud-based large LLM. 
% Specifically, DeepSeek-R1~\cite{guo2025deepseek} is employed to detect private information in observations (or actions), with the system switching to the local small LLM whenever such content is identified. This strategy prevents private data from being exposed to external APIs while still capitalizing on the large LLM’s strengths for non-private operations. 
Our contributions can be summarized as follows:
\vspace{-3mm}
\begin{itemize}[leftmargin=*]
\item[\ding{182}] \textbf{\textit{Synergistic Framework.}} We present a novel framework that establishes an iterative, symbiotic cycle between large and small LLMs, enabling them to leverage their complementary strengths for mutual enhancement.
% We present a novel framework that enables an iterative improvement cycle between large LLMs and small LLMs, leveraging their complementary strengths to achieve mutual enhancement.
\vspace{-3mm}
\item[\ding{183}] \textbf{\textit{Technical Innovations.}} We introduce two key advancements in distillation techniques: (a) a speculative data synthesis strategy to counteract off-policy bias and (b) a multi-task learning approach to maintain reasoning capabilities.
% We propose two key advancements in distillation techniques: (a) use speculative data synthesis to address off-policy bias and (b) use multi-task learning to preserve reasoning capabilities.
\vspace{-3mm}
\item[\ding{184}] \textbf{\textit{State-of-the-art Performance.}} Experiments show that on the \textsc{WebArena} benchmark, \ourmethod achieves state-of-the-art performance with both LLM types: the large LLM achieves $52\%$, surpassing the previous best open-source $45\%$, while our 8B distilled LLaMA-3 model achieves $49\%$, approaching the performance of agents based on Claude-3.5.
\vspace{-3mm}
% \item[\ding{185}] \textbf{\textit{Broader Impact.}} \ourmethod empowers developers and researchers to harness existing LLM resources effectively by combining the robust capabilities of large models with the exploratory efficiency of smaller models.

% enables developers and researchers to quickly leverage existing LLM resources by combining the powerful capabilities of large LLMs with the exploratory advantages of small LLMs together. 
% Due to its generality and ease of deployment, this method can be potentially applied to other tasks.
% \vspace{-1mm}
\item[\ding{185}] \textbf{\textit{Hybrid Mode for Privacy Preservation.}} We integrate a hybrid mode for privacy preservation that directs sensitive tasks to a local small LLM, ensuring that private user data remains secure.
% protects private user data by selectively delegating private tasks to a local small LLM, ensuring confidentiality.
\end{itemize}





    \section{Related work}
% LLM-based web agents have gained significant attention in recent years due to their
%     potential to automate, optimize, and enhance various tasks across the web, such
%     as information retrieval, decision making, and interaction with dynamic environments

    \textbf{Web Agents.} LLM-based web agents have gained significant attention in recent years due to their ability to automate, optimize, and enhance a wide range of web-based tasks, such as information retrieval, decision-making, and interactions within dynamic environments~\cite{zhou2023webarena,
    deng2023mind2web, yao2023webshop, pan2024webcanvas, levy2024stwebagentbench,
    dechezelles2024browsergym}.  Many existing approaches~\cite{koh2024tree,
    putta2024agentq, yu2025exact}
    utilize search-based methods like Monte Carlo Tree Search (MCTS) to obtain
    more online examples from the web environments. 
    Although these methods benefit from increased interactions, their performance does not scale with the number of interactions. In contrast, our framework introduces an iterative symbiotic improvement cycle that continually refines both data synthesis and task performance. More detailed related work is provided in Appendix~\ref{app:related}.
    % These methods typically require more interactions with the environment, yet their performance remains unchanged as the number of interactions increases. Therefore, we propose a framework that enables iterative symbiotic improvement. 

    \noindent\textbf{Knowledge Distillation.} Knowledge distillation is a pivotal technique for transferring the advanced capabilities of a large, powerful model to a smaller, more efficient one~\cite{Gou_2021, xu2024survey}. Recent work on LLM distillation~\cite{hinton2015distilling, anand2023gpt4all, hsieh2023distilling} has shifted the focus from merely replicating the teacher model’s output to capturing its underlying reasoning and decision-making processes. Additionally, several methods have been proposed to fine-tune language models for web tasks~\cite{yin2024agent,hong2024cogagent,lai2024autowebglm}, further enhancing decision-making abilities. Despite these efforts, distilled small LLMs still lag behind their larger counterparts in performance. Our work addresses this gap by introducing a novel distillation approach that leverages iterative symbiotic improvements to enhance the capabilities of small LLMs. More detailed distillation related work is provided in Appendix~\ref{app:related}.
    % Knowledge Distillation is a pivotal technique for transferring advanced capabilities from a larger, more powerful model to a smaller, more efficient one~\cite{Gou_2021, xu2024survey}. 
    % Moreover, recent work~\cite{anand2023gpt4all, hsieh2023distilling} on LLM distillation has shifted focus from replicating the output behavior of the teacher model to learning the thinking and reasoning paradigms capabilities. Also, there are methods trying to fine-tune language models for web tasks to enhance decision-making capabilities ~\cite{yin2024agent,hong2024cogagent,lai2024autowebglm}. The distilled small LLMs still exhibit a performance gap compared to the large LLMs. Our work introduces a novel distillation method to enhance their performance. More detailed related work is provided in Appendix~\ref{app:related}.

\section{Methodology}
\label{sec:method}

\textbf{Overview}. In this section, we introduce our framework, \ourmethod, which is designed to enhance the capabilities of both large and small LLMs. Our approach has two key components: (1) large LLMs access more comprehensive and diverse references through Speculative Data Synthesis during Retrieval-Augmented Generation (RAG)~\cite{wu2024longmemeval}, and (2) small LLMs integrate reasoning into their predictions during the distillation process using synthesized data. 
% Specifically, we illustrate the \textit{complementary nature} of large and small LLMs (§~\ref{sec:LS}), how to build a \textit{RAG-enhanced} large LLM (§~\ref{sec:rag}), how to \textit{improved distillation} for small LLM (§~\ref{sec:distill}), and how to build \textit{privacy detect hybrid mode} (§~\ref{sec:privacy}).

\textbf{Problem Formulation.} Let $o \in \mathcal{O}$ represent an observation, which consists of an accessible tree structure provided by the web environment along with a corresponding instruction. An action $a \in \mathcal{A}$ corresponds to a command that can be executed and interpreted by the web environment. The reason $r \in \mathcal{R}$ captures the rationale behind why an LLM chooses to execute a specific action in response to an observation. The state $s \in \mathcal{S}$ corresponds to the current state of the environment. At each step $i$, large LLM ($M_{\mathrm{L}}$) predicts the next action $a_{i}$ and reason $r_{i}$ based on the interaction history $\mathcal{H} = (o_{0}, a_{0}, o_{1}, a_{1}, \dots, o_{i})$. In contrast, the small LLM ($M_{\mathrm{S}}$) makes its predictions for $a_{i}$ and $r_{i}$ based solely on the current observation $o_{i}$.

    % Our overall goal is twofold. First, we aim to enable the $M_{L}$ to interact
    % with the environment iteratively, leveraging accumulated retrieval examples to
    % maximize the cumulative reward across all tasks. Second, we aim to train the
    % $M_{S}$ to mimic the behavior of the $M_{L}$ on the web-based tasks and explore
    % new trajectories to enrich the knowledge base for retrieval to help large LLMs.

    % Formally, let \( T \) denote a task, and let \( \mathcal{E}_T \) represent an interaction episode on task \( T \). The cumulative reward \( R_T \) for task \( T \) is defined as:

    % \[
    % R_T = \sum_{i=1}^{N_T} R(o_i, a_i),
    % \]

    % where \( N_T \) is the total number of steps in the interaction episode, and \( R(o_i, a_i) \) is the reward obtained by executing action \( a_i \) in observation \( o_i \). The objective is to optimize \( M_L \) such that the cumulative reward \( R_T \) is maximized across all tasks:

    % \[
    % \max_{M_L} \sum_{T=1}^{812} R_T.
    % \]

    % Second, we aim to train the \( M_S \) to mimic the behavior of the \( M_L \) on the web-based tasks. We assume that \( M_S \) has learnable parameters \( \theta_S \), while \( M_L \)'s parameters \( \theta_L \) are fixed. For each task \( T \), we are given a set of observations \( \mathcal[o] \) and a corresponding set of actions \( \mathcal[a] \).

    % To measure the divergence between the distributions of \( M_L \) and \( M_S \) for a given input-output pair \( (o, a) \), we define the following metric:

    % \[
    % D(M_L \| M_S)(y|x) = \frac{1}{L_y} \sum_{i=1}^{L_y} D(M_L(\cdot|y_{<i}, x) \| M_S(\cdot|y_{<i}, x)),
    % \]

    % where \( L_y \) is the length of the output sequence \( y \), \( i \) indicates the decoding step, \( x \) represents the input (observation \( o \)), and \( y_{<i} \) denotes the partial sequence generated up to step \( i-1 \). Here, \( D(\cdot \| \cdot) \) represents a divergence metric, such as KL divergence.

    % The training objective is to minimize the divergence \( D(M_L \| M_S)(y|x) \) such that \( M_S \) can effectively imitate \( M_L \) on task \( T \). Formally, the objective is:

    % \[
    % \min_{\theta_S} \mathbb{E}_{T \sim \mathcal{T}} \mathbb{E}_{(o, a) \sim (\mathcal{O}_T, \mathcal{A}_T)} \left[ D(M_L \| M_S)(y|x) \right],
    % \]

    % where \( \mathcal{T} \) is the set of tasks, and the expectation is taken over the task distribution and the data within each task. This ensures that the small LLM (\( M_S \)) generalizes well across tasks while closely imitating the behavior of the large LLM (\( M_L \)).

    \subsection{Large LLMs and Small LLMs Can Benefit Each Other}
    \label{sec:LS}
    

    % Large LLMs generally exhibit more advanced reasoning capabilities and higher
    % success rates on complex tasks due to their extensive pre-training and superior
    % in-context learning abilities. 
    
    
    % Despite this
    % performance gap, small LLMs typically offer reduced computational cost and
    % faster inference, making them better suited for large-scale exploration in
    % environments with high branching factors, such as web browsing.
    % Moreover, small LLMs often choose actions differing from those of the large LLMs owing to their distinct reasoning shaped by the distillation process
    
    {\textbf{Exploration-Exploitation Trade-off}}. The fundamental difference between large and small LLMs in web-based tasks can be understood through the lens of the \emph{exploration-exploitation} trade-off.
    Large LLMs excel at \emph{exploitation}—that is, they are very effective at accurately selecting actions in well-understood scenarios—while small LLMs, with their faster inference speeds, are more agile and capable of \emph{exploring} a broader range of possible actions.  
    Formally, for a given task $\mathcal{T}$,
    let $E(M, \mathcal{T})$ denote the expected performance of an LLM $M$ on
    tasks drawn from $\mathcal{T}$. Empirically, large LLMs $M_{L}$ often
    satisfy the inequality:
    \begin{equation}
        E(M_{\mathrm{L}}, \mathcal{T}) \;>\; E(M_{\mathrm{S}}, \mathcal{T}), \label{eq:llm-better-smaller}
    \end{equation}
    where $M_{\mathrm{S}}$ represents a small LLM with significantly fewer parameters. 
    
    Moreover, small LLMs tend to be more flexible in their action selection because they are more sensitive to changes in observations and less capable of capturing complex patterns~\cite{wang2024comprehensive}. This increased sensitivity leads them to exhibit more variable behavior, enabling them to explore a larger subset of state-action pairs in the environment. In other words, if we denote by $\pi_{M_L}$ and $\pi_{M_S}$ the policies of the large and small LLMs respectively, small LLMs tend to cover more of the state-action space $(\mathcal{S} \times \mathcal{A})$,
    % \begin{equation}
    %     \small \left\lvert \{(s,a) \mid \pi_{M_\mathrm{S}}(a \mid s) > 0\} \right\rvert >
    %     \left\lvert \{(s,a) \mid \pi_{M_\mathrm{L}}(a \mid s) > 0\} \right\rvert
    % \end{equation}
    under a given time or computational budget. This broader coverage can uncover diverse trajectories—including corner cases, failed attempts, or novel solutions—that might be missed by a $M_{L}$ operating on its own.

    % under a given time or computational budget. This broader coverage of the environment
    % by small LLMs can uncover trajectories (e.g., corner cases, failed attempts, or novel solutions) that the large LLM policy might overlook by itself.

   
   

\noindent{\textbf{Synergistic Gains.}} Large and small LLMs have a \emph{complementary} relationship that can be harnessed in an \emph{iterative} manner, to achieve performance beyond what either model could reach individually. We define Success Rate (SR) as the proportion of tasks in which an LLM reaches the correct goal state, as follows:
    \begin{equation}\label{eq:sr}
        \text{SR}(M) \;=\; \frac{\sum_{i=1}^{N} \mathbb{I}\Bigl(s_{T}^{i} = s_{T}^{\text{goal}}\Bigr)}{N}
        ,
    \end{equation}
    More precisely, $N$ is the total number of tasks, $s_{T}^{i}$ is the final state
    reached by the LLM $M$ after executing the sequence of actions
    $(a_{1}, a_{2}, \dots, a_{T})$ for task $i$, and $s_{T}^{\text{goal}}$ is the
    correct goal state for the task. The indicator function $\mathbb{I}(\cdot)$
    evaluates to $1$ if the final state $s_{T}^{i}$ matches the goal state
    $s_{T}^{\text{goal}}$, and $0$ otherwise, for a time horizon~$T$.
    Consider a scheme with the following components:
    \vspace{-2mm}
    \begin{enumerate}[leftmargin=*,itemsep=1.5pt]
        \item [\ding{182}] The large LLM $(M_{\mathrm{L}})$ interacts with the environment to produce \emph{high-quality
            trajectories}, which are then used to distill the small LLM $(M_{\mathrm{S}})$.
\vspace{-2mm}
        \item [\ding{183}] The distilled $M_{\mathrm{S}}$ subsequently engages in \emph{exploratory interactions}, discovering \emph{new trajectories} that the $M_{\mathrm{L}}$ may have overlooked.
\vspace{-1mm}
        \item [\ding{184}]These additional trajectories are incorporated into the knowledge base
            for the large LLM’s RAG, thereby improving $M_\mathrm{L}$ for the \emph{next iteration} of environment interaction.
    \end{enumerate}
    This process is repeated over multiple rounds, creating a compounding feedback loop where each LLM benefits from the strengths of the other. Let
    $\text{SR}^{\text{(iter)}}(M_{\mathrm{L}}, M_{\mathrm{S}})$ denote the final success rate of
    this \emph{iterative} procedure after several rounds. We then define a \emph{synergy
    metric}~$\Delta$:
    \begin{equation}\small
        \Delta \;=\; \text{SR}^{\text{(iter)}}(M_{\mathrm{L}}, M_{\mathrm{S}}) \;-\; \max\Bigl(\text{SR}
        (M_{\mathrm{L}}),\,\text{SR}(M_{\mathrm{S}})\Bigr) \label{eq:synergy_iterative}
    \end{equation}
    A strictly positive $\Delta$ indicates that the iterative scheme yields higher success rates than the best single-LLM approach, demonstrating the power of synergistic cooperation between both LLM types.
    % We expect a strictly positive value of $\Delta$, which indicates that this iterative scheme
    % yields higher success rates than the best single-LLM approach. 


    \vspace{-1mm}
    \subsection{Build RAG-Enhanced Large LLM}
    \label{sec:rag} While some agents enable LLMs to act in augmented observation-action spaces or use search algorithms for web navigation, these methods often face limitations—such as time-consuming design of action spaces, increased interaction steps, and the inability to improve performance iteratively. To overcome these challenges, we enhance the performance of large LLMs by integrating RAG within the \textsc{AgentOccam} framework. The detailed implementation steps are outlined in Algorithm~\ref{alg:rag_large_model}. 
    % While some agents enable LLMs to act within augmented observation-action spaces or use search algorithms for dynamic web navigation, these approaches face limitations. Designing effective action spaces is time-consuming and often fails to enhance performance, while search-based methods require more steps for interaction with the environment.

    % To address these challenges, in this section, we enhance the performance of large LLMs by integrating RAG based on AgentOccam framework.

    \noindent{\textbf{Data Synthesis.}}
    Our agent begins by interacting with the environment across various tasks, accumulating both successful and failed trajectories. Each trajectory (denoted as $\mathcal{H}$)
    is decomposed into all possible subsequences that start and end with an observation. For example, a trajectory like
    $(o_{0}, a_{0}, o_{1}, a_{1}, o_{2})$ is split into subsets such as
    $(o_{0}, a_{0}, o_{1})$ and $( o_{1}, a_{1}, o_{2})$, among others. Inspired by recent approaches that use LLMs as judges~\cite{li2024generation,gu2024survey,tan2024large}, we employ a \textit{multi-LLM debate}~\cite{liang-etal-2024-encouraging} mechanism to generate task instructions and summaries for these trajectories. The generated content is evaluated against predefined criteria; only the trajectories that satisfy all criteria are retained. These validated trajectories, along with their corresponding instructions and summaries, are stored in a knowledge base for later retrieval.
    % Multiple LLMs collaborate to generate and evaluate the following:
    % \begin{itemize}
    %     \item \textbf{Task Instructions:} Descriptions of the task represented by the trajectory.
    %     \item \textbf{Trajectory Summaries:} Concise explanations of the trajectory's key actions and outcomes.
    % \end{itemize}
    % The generated contents are evaluated against some criteria. Only trajectories meeting all criteria are retained, and
    % the validated trajectories, along with their corresponding instructions and
    % summaries, are stored in a \textit{knowledge base} for retrieval.
\begin{figure*}[htbp] 
    \centering
    \includegraphics[width=\textwidth]{fig/fig2.pdf}
    \vspace{-10mm}
    \caption{Overview of two key innovations in LLM distillation: (a) \textbf{Speculative Data Synthesis}, which mitigates off-policy bias by leveraging both large and small LLMs. At each step, the small LLM generates an action based on the observation, while the large LLM produces a set of top-$K$ action candidates. If the small LLM's action is within the large LLM's top-$K$ actions, it is accepted (\textcolor{customGreen}{$\checkmark$}); otherwise, the large LLM's action is chosen for subsequent interactions (\textcolor[HTML]{E51400}{\ding{55}}). (b) \textbf{Multi-task Learning}, which enhances reasoning capabilities by training small LLM to predict both actions and rationales, enabling it to handle multiple tasks and address missing reasoning capabilities during distillation. CoT indicates Chain-of-Thought~\cite{wang2024chain}.}
    \label{fig:2} 
    \vspace{-5mm}
\end{figure*}

    \noindent{\textbf{RAG Example Retrieval Strategies.}}
To retrieve relevant knowledge for the agent, we propose a mixture of three strategies:
(a)~\textit{Task-Guided Summary Retrieval}: Queries are generated from task instructions and webpage observations to retrieve relevant past experiences from the RAG knowledge base.
    (b)~\textit{Direct Observation and Instruction Matching}: The current observation and instruction are directly matched with entries in the knowledge base.
    % The system matches the current webpage observation and instruction directly with observations and instruction from previously recorded trajectory examples.
    (c)~\textit{Trajectory Similarity Search}: Similar interaction examples are retrieved by computing and comparing trajectory embeddings using cosine similarity. After retrieving $K$ trajectory examples, a {filtering step} performed by an LLM ensures their quality and relevance. The detailed prompt is provided in Appendix~\ref{app:Filter}. Since similarity alone doesn't guarantee usefulness for action prediction, an LLM employs a {chain-of-thought}~\cite{wang2024chain} reasoning process to evaluate and rank these examples. The selected high-quality examples then aid the large LLM in its decision-making during interaction with the environment.



% \FloatBarrier
    \vspace{-1mm}\subsection{Improved Distillation for Small LLMs}
    \label{sec:distill} To further enhance the performance of distillation, we introduce two key innovations for processing web-browsing trajectories (see Figure~\ref{fig:2}):
    (a) a speculative data synthesis strategy designed to correct off-policy
    bias~\cite{caccia2024fine}, and (b) a multi-task learning approach aimed at improving reasoning capabilities of the small LLM. These innovations are detailed in Algorithm~\ref{alg:improved_distillation}
    % \vspace{-1mm}
    
    \noindent{\textbf{Speculative Data Synthesis.}} 
    In complex environments such as the web, where decision-making tasks require nuanced reasoning and adaptability, it has been observed that a distilled small agent (e.g., a LLaMA-based model) often produces actions that differ significantly from those of a large LLM. Traditional knowledge distillation (KD) methods, such as supervised KD, struggle in such settings due to the large gap in reasoning and decision-making capabilities between the small and large LLMs. To address these challenges, we propose a dynamic collaboration between the large (teacher) and small (student) LLMs to iteratively and adaptively generate high-quality training data.

    Specifically, at each step $i$, the speculative synthesis process unfolds as: \ding{182} \textit{Student Proposal:} The small LLM generates a reasoning trace and an action based on the current observation $o_{i}$ and instruction $I$. \ding{183} \textit{Teacher Evaluation:} Simultaneously, the large LLM, guided by chain-of-thought prompting, generates $K$ action candidates along with their corresponding reasoning traces. \ding{184} \textit{Action Candidate Filtering:} The action proposed by the small LLM is evaluated against the set of candidate actions produced by the large LLM. If the student’s action is among the teacher’s candidates, it is deemed reliable and is executed; if not, it is rejected and replaced by the most reliable candidate selected from the teacher’s proposals.
    % The small LLM's action undergoes evaluation against the large LLM’s set of action candidates: $i$) If the student’s action is found within the large LLM’s candidates, it is deemed reliable and executed. $ii$) If the action falls outside the candidate set, it is rejected and replaced by the most reliable action selected from the candidates.

    This speculative interaction ensures that the small LLM learns to align its policy with the large LLM’s superior reasoning while retaining the ability to explore and propose its own actions. Importantly, the speculative synthesis process evolves over time as the small LLM improves:
    \vspace{-3mm}
    \begin{itemize}[leftmargin=*]
        \item \textbf{Early Training Phase:} During the initial stages, the small LLM often proposes suboptimal or low-quality actions. In this phase, the \textit{Action Candidate Filtering} process closely resembles supervised KD, where the teacher LLM corrects the student by replacing their actions with high-confidence actions from the teacher.
\vspace{-2mm}
        \item \textbf{Later Training Phase:} As the small LLM’s reasoning and action selection performance improves, the filtering process transitions to a more \textit{speculative mode}, where the teacher increasingly accepts the small LLM’s proposals. 
    \end{itemize}
\vspace{-2mm}
    By gradually shifting responsibility from the teacher LLM to the student LLM, this approach enables the small LLM to effectively bridge its knowledge gap with the large LLM.

    The trajectories generated through the speculative data synthesis process are combined with those produced by the large LLM. This combined dataset forms the foundation for further distillation, enabling the small LLM to learn from both its own exploratory actions and the large LLM’s high-quality trajectories. This iterative refinement process results in a small LLM that not only mimics the large LLM’s performance but also demonstrates robust decision-making in diverse environments.
    

    \noindent{\textbf{Multi-Task Learning.}} 
    Training an agent for complex environments often requires large amounts of manually annotated data, making it challenging to effectively associate observations with the corresponding actions. To overcome this, we leverage a large LLM to generate both action predictions and intermediate reasoning steps, which serve as rich supervisory signals for training small LLMs. In this way, the small LLM learns not only to predict actions but also to produce intermediate reasoning steps that explain those actions.
    % To address this, a large LLM is leveraged to generate both action predictions and intermediate reasoning steps, which serve as rich supervision signals for training small LLMs, where the small LLM not only predict actions but also generate intermediate reasoning steps. 
    % . 
    % This approach ensures that reasoning
    % capabilities are preserved during the distillation process, going beyond
    % simple action prediction.
    % Inspired by the chain-of-thought prompting literature, a {multi-task learning} framework is adopted to train
    
    Specifically, the large LLM is prompted to produce outputs that include \ding{182} \textit{Action Generation:} It predicts the next action based on the agent’s current state. \ding{183} \textit{Full Reasoning:} It generates the rationale explaining why the action is appropriate. 

    
% This framework provides two key benefits:
%     \vspace{-2mm}
%     \begin{itemize}[leftmargin=*]
%         \item \textbf{Preservation of Reasoning Capabilities:} The rationale
%             generation task helps prevent catastrophic forgetting by requiring
%             the LLM to explicitly learn and retain structured reasoning
%             processes, even within the computational constraints
%             of a smaller architecture.
%         \vspace{-1mm}
%         \item \textbf{Improved Generalization:} Generating rationales allows the
%             LLM to learn richer, task-specific features that improve its overall
%             understanding of the environment, resulting in better generalization
%             to unseen situations.
%     \end{itemize}

   
\vspace{-3mm}
\subsection{Hybrid Mode for Privacy Preservation}
\label{sec:privacy}
\vspace{-1mm}

While the large LLM offers superior reasoning and broader knowledge, many web tasks involve confidential or high-stakes information (e.g., passwords and payment details) that must be handled with care. To safeguard user privacy, we propose a hybrid mode that allows the system to automatically switch between a local small LLM and a cloud-based large LLM. 
% To safeguard privacy, a hybrid mode for privacy preservation is proposed. It allows automatic switches between the local small LLM and the cloud-based large LLM:

This hybrid mode operates as follows:

\noindent\textbf{Privacy Detection.}
Before processing any environment observation or action, the content is scanned by a local \texttt{DeepSeek-R1}~\cite{guo2025deepseek} model, which flags potential private information—such as personally identifiable data or security tokens.

\noindent\textbf{Local Processing.}
If the observation or action is deemed private, the decision-making step is delegated to the small LLM deployed locally.
% By avoiding external calls for Large LLM in such cases, private data is ensured to remain confined to trusted, on-premise resources.

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.48\textwidth]{fig/fig3.pdf} % 调整宽度为单栏
    \vspace{-3mm}
    \caption{The Privacy Detector analyzes each step's observation and action for private data. If detected, a local small LLM ensures confidentiality by predicting the next action and reason. Otherwise, a cloud-based large LLM handles predictions, leveraging its superior reasoning capabilities for non-sensitive tasks.}
    \label{fig:3} 
    \vspace{-3mm}
\end{figure}


\noindent\textbf{Cloud Processing.}
If no private information is detected, the agent leverages the large, cloud-based LLM to benefit from its advanced capabilities.
% This setup effectively balances user privacy with the strong performance benefits provided by the large LLM.

By combining on-device inference for sensitive steps with cloud-based reasoning for non-sensitive tasks, this hybrid mode offers a practical and robust solution for building privacy-preserving web-based agents. Detailed prompts for privacy detection are provided in the Appendix~\ref{app:private}.
% By combining on-device inference for sensitive steps with powerful cloud-based reasoning for other steps, our hybrid mode for privacy preservation offers a practical design for robust yet privacy-preserving web-based agents. The detailed private detect prompt is provided in Appendix~\ref{app:private}.







     
    % In this section, we conduct a comprehensive evaluation
    % of the \ourmethod framework.
    % introduced in Section~\ref{sec:method}
    % and motivated by our vision of large \textit{versus} small LLM synergy in Section~\ref{sec:intro}.
    
    % We aim to validate following hypotheses: (\textbf{\textit{i}})~Large LLMs and small
    % LLMs can iteratively enhance each other’s performance via complementary
    % exploration-exploitation principle, (\textbf{\textit{ii}})~Our retrieval-augmented
    % generation (RAG) design (Section~\ref{sec:rag}) improves large-LLM
    % performance by incorporating new trajectories discovered by small LLMs,  (\textbf{\textit{iii}})~Our
    % improved distillation strategies (Section~\ref{sec:distill}) consistently
    % uplift the quality of small LLMs, enabling them to serve as competent co-pilots
    % for large LLMs, and (\textbf{\textit{iv}})~Our hybrid mode for Privacy Preservation (Section~\ref{sec:privacy}) can protect privacy information when interact with environment.
\vspace{-2mm}
    \section{Experimental Setup}
    \label{sec:exp_setup} \vspace{-1mm}
Our experiment settings are as follows. Implementation details are presented in Appendix~\ref{app:exp}.

    \noindent\textbf{Environment.} \textsc{WebArena} is a benchmark simulating realistic websites across various domains such as e-commerce, collaborative software development, and social forums. Each domain poses a distinct set of tasks (\textit{e.g.}, purchasing items, creating an issue on {\fontfamily{pcr}\selectfont GitLab}, participating in a {\fontfamily{pcr}\selectfont Reddit} discussion), thereby testing the agent’s ability to plan and execute complex, multi-step actions. We report the average \emph{success rate} (SR, defined in Eq.\eqref{eq:sr}) across all 812 tasks as our primary metric, consistent with prior work~\cite{zhou2023webarena}.
   

    \noindent\textbf{Agents.}
    Unless otherwise stated, we consider two classes of LLMs: (\textit{i}) {Large, closed-source LLMs} (\texttt{Claude-3.5-sonnet}, \texttt{GPT-4-Turbo}, \texttt{GPT-4o}); (\textit{ii}) {Smaller, open-source LLMs}, which include two subcategories: \texttt{DeepSeek-R1-Distill-Qwen-32B} used for privacy detection, and  \texttt{DeepSeek-R1-\\Distill-Llama-8B}, \texttt{Llama-3.2-1B-Instruct}, and \texttt{Llama-3.1-8B-Instruct} for distillation. Large LLMs are accessed via API. All the small LLMs are deployed locally.


\noindent\textbf{Baselines.}
We compare our \ourmethod approach against several representative baselines:  {{Vanilla prompting}}: Use predefined action options to interact with the environment and generate structured responses. Existing baselines explore various approaches to enhancing LLM-based web agents, including optimization, adaptation, policy learning, planning, workflow memory, API integration, evaluation, and multi-agent strategies {{AgentOccam}}~\cite{yang2024agentoccam}, {{Learn-by-Interact}}~\cite{su2025learn}, {WebArena-replication}~\cite{zhou2023webarena}, {SteP-replication}~\cite{sodhi2024step}, LATS~\cite{zhou2023language}, AWM~\cite{wang2024agent}, API-Based Agent~\cite{song2024beyond}, AutoEval~\cite{pan2024autonomous}, WebPilot~\cite{zhang2024webpilot}. See more detailed baseline in Appendix~\ref{app:base}.

% {{AgentOccam}} ~\cite{yang2024agentoccam}: An LLM-based web agent that optimizes observation and action spaces.
% {{Learn-by-Interact}} ~\cite{su2025learn}: A data-centric framework for adapting LLM agents without human annotations.
% {WebArena-replication} ~\cite{zhou2023webarena}: A few-shot in-context learning agent using large language models.
% {SteP-replication} ~\cite{sodhi2024step}: A dynamic LLM policy framework for diverse web tasks.
% LATS~\cite{zhou2023language}: A framework integrating reasoning, acting, and planning via Language Agent Tree Search.
% AWM~\cite{wang2024agent}: A workflow memory method for guiding agent decision-making.
% API-Based Agent~\cite{song2024beyond}: A framework combining API calls and web browsing for web tasks.
% AutoEval~\cite{pan2024autonomous}: An evaluation-driven approach for improving web navigation and device control.
% WebPilot~\cite{zhang2024webpilot}: A multi-agent system enhancing MCTS for complex web tasks.



\begin{table}[t]
\centering
\caption{Comparison of final success rates (SR) among various large  LLM  and small LLM base agents on \textsc{WebArena}. Scores marked with $^{*}$ indicate cited scores from the corresponding papers' experiment scores.}
\label{tab:main_results}
\vspace{-3mm}

    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l c c}
    \toprule
    \textbf{Method} & \textbf{Model} & \textbf{SR (\%) $\uparrow$} \\ 
    \midrule
    WebArena-replication  & \texttt{GPT-4-Turbo}         & $16.5^{*}$ \\     {AutoEval}             & \texttt{GPT-4}     & $20.2^{*}$ \\
    {Reflection}             & \texttt{Claude-3.5}     & $32.4^{*}$ \\
    SteP-replication      & \texttt{GPT-4-Turbo}         & $33.3^{*}$ \\
    {LATS}             & \texttt{Claude-3.5}     & $34.2^{*}$ \\
    {AWM}             & \texttt{GPT-4}     & $35.6^{*}$ \\
    {WebPilot}             & \texttt{GPT-4o}     & $37.2^{*}$ \\
    {Learn-by-Interact}             & \texttt{Claude-3.5}     & $39.2^{*}$ \\
    {API-Based Agent}             & \texttt{GPT-4o}     & $43.9^{*}$ \\
    {AgentOccam}            & \texttt{GPT-4-Turbo}    & $45.7^{*}$ \\
    {AgentOccam}             & \texttt{Claude-3.5}     & $48.5$ \\
    \rowcolor[gray]{0.90}\textbf{\ourmethod}     & \texttt{Claude-3.5}     & $\textbf{52.1}$ \\
    
    
    \midrule
    Vanilla prompting & \texttt{LLaMA-1B}       &  $2.4$ \\
    Vanilla prompting & \texttt{LLaMA-8B}       &  $5.6$ \\
    Vanilla prompting & \texttt{DeepSeek-R1-8B}       &  $8.5$ \\
     {Learn-by-Interact}     & \texttt{Codegemma-7B}            & $17.9^{*}$ \\
    {Learn-by-Interact}     & \texttt{Codestral-22B}            & $28.0^{*}$ \\
    % \midrule
    \rowcolor[gray]{0.90}\textbf{\ourmethod}      & \texttt{LLaMA-1B}       & $24.1$ \\
    \rowcolor[gray]{0.90}\textbf{\ourmethod}      & \texttt{DeepSeek-R1-8B}       & $43.6$ \\
    \rowcolor[gray]{0.90}\textbf{\ourmethod}& \texttt{LLaMA-8B}       & $\textbf{48.5}$ \\

    
    \midrule
    \textbf{\ourmethod-{\fontfamily{lmtt}\selectfont \textbf{Hybrid}}}         & \makecell{\texttt{Claude-3.5} \\+ \texttt{LLaMA-8B}}     & $50.5$ \\

    
    \bottomrule
    \end{tabular}
    }
   
\end{table}

\vspace{-2mm}
\section{Experiment Results}
    \label{sec:experiments}
\subsection{Superior Results of \textbf{\ourmethod}}
\label{sec:main_results}
\vspace{-1mm}

Table~\ref{tab:main_results} summarizes the final success rates of each agent on the entire \textsc{WebArena} (\#$812$ tasks). We obtain the following findings:
% \vspace{-2mm}
% \begin{enumerate}
\ding{182}\textbf{\ourmethod{\fontfamily{lmtt}\selectfont \textbf{-Claude-3.5}}} achieves an success rate (SR) of \textbf{$52\%$}, significantly outperforming the previous best open-source result of (\textbf{$45\%$}). This improvement underscores the effectiveness of our iterative synergy approach (\S\ref{sec:LS}) and the use of diverse, high-quality trajectories in the RAG module (\S\ref{sec:rag}).
% \vspace{-1mm}
\ding{183}\textbf{\ourmethod{\fontfamily{lmtt}\selectfont \textbf{-LLaMA-8B}}} attains \textbf{$48.5\%$}, a substantial improvement over the original \texttt{LLaMA-8B} baseline ($5\%$) and prior small-LLM methods (up to $28\%$). 
    This validates our claim that improved \emph{distillation} (\S\ref{sec:distill}) and re-using newly explored trajectories enable even 8B-parameter models to approach large-LLM performance (\textbf{$52\%$}). 
\vspace{-1mm}
\ding{184} Our \textbf{\ourmethod} consistently maintains a clear performance gap over standard few-shot or fine-tuned approaches. This highlights that the \emph{complementary dynamic} between large and small LLMs—exploited in a multi-round loop—is critical for robust performance on complex web tasks. One of the trajectory examples is shown in Appendix~\ref{app:Trajectory}.
% \end{enumerate}

Figure~\ref{fig:6} presents our iterative experiment on a subset of \textsc{WebArena} tasks. It shows that as the iteration number increases, the synergy metric ($\Delta$), as defined in Equation~\ref{eq:synergy_iterative}, also gradually increases, indicating an improvement in our performance.
\begin{figure}[t] 
    \centering
    \includegraphics[width=0.48\textwidth]{fig/fig6.pdf} % 调整宽度为单栏
    \vspace{-6mm}
    \caption{ The synergy metric ($\Delta$), which defined in Equation~\ref{eq:synergy_iterative}, increases as the iterative time progresses.}
    \label{fig:6} 
    \vspace{-5mm}
\end{figure}

\begin{table*}[t]
\centering
\caption{Ablation study on small LLM distillation for comparison of success rate (SR in $\%$) in specific \textsc{WebArena} sub-domains. (\#Tasks) indicates the number of scenarios in each domain. ``multi-task'' denotes Multi-Task Learning for Reasoning. ``Speculative'' denotes Speculative Data Synthesis.}
\label{tab:ablation}
\vspace{-3mm}

\resizebox{\textwidth}{!}{
\begin{tabular}{l c c c c c c c}
\toprule
\textbf{Agent}  & 
\makecell{\textbf{Overall SR} \\ (\#812)} & 
\makecell{{\fontfamily{pcr}\selectfont Shopping} \\ (\#187)} & 
\makecell{{\fontfamily{pcr}\selectfont {Shopping Admin}} \\ (\#182)} & 
\makecell{{\fontfamily{pcr}\selectfont {GitLab}} \\ (\#180)} & 
\makecell{{\fontfamily{pcr}\selectfont {Map}} \\ (\#109)} & 
\makecell{{\fontfamily{pcr}\selectfont {Reddit}} \\ (\#106)} &
\makecell{{\fontfamily{pcr}\selectfont {Multisite}} \\ (\#48)} \\
\midrule
% WebArena-replication & GPT-4-Turbo & $16.5$ & $16.6$ & $15.9$ & $10.0$ & $22.9$ &$21.7$ &$16.7$ \\
% SteP-replication     & GPT-4-Turbo & $33.3$ & $33.2$ & $32.4$ & $26.7$ & $35.8$ &$52.8$ & $12.5$  \\
% AgentOccam           & GPT-4-Turbo & $43.1$ & $40.6$ & $45.6$ & $37.8$ & $46.8$ & $61.3 $ &$14.6$  \\
% % AgentOccam           & Claude-3.5 & $48.5$ & $51.5$ & $35.7$ & $44.9$ & $69.2 $\\
% AgentOccam           & Claude-3.5  & $48.5$ & $51.3$ & $35.7$ & $47.8$ & $52.3$ & $70.8 $ &$31.2$  \\
% \midrule
% \textbf{AgentSymbiotic-Privacy}            & \makecell{Claude-3.5 \\+ LLaMA-8B} & $50.5$ & $52.9$ & $43.4$ & $47.2$ & $63.3$ & $62.3$ & $25.0$ \\
\texttt{LLaMA-8B}  & $40.8$ & $50.3$ & $30.8$ & $41.1$ & $37.6$ & $51.9$ & $22.9$ \\

\emph{  + multi-task}  & $43.2$ & $46.5$ & $29.1$ & $46.1$ & $45.0$ & $61.3$ & $29.2$ \\

\emph{  + speculative}  & $46.8$ & $46.0$ & $34.6$ & $48.3$ & $56.9$ & $68.9$ & $18.8$ \\

\emph{  + speculative + multi-task}  & $48.5$ & $48.7$ & $41.2$ & $47.2$ & $57.8$ & $63.2$ & $27.1$ \\

% \textbf{AgentSymbiotic} & Claude-3.5 & $\textbf{52.1}$ & $48.7$ & $49.5$ & $51.7$ & $60.6$ & $66.1$ & $29.2$\\

\bottomrule

\end{tabular}
% \vspace{-4mm}
}

\end{table*}

\begin{figure*}[t] 
\vspace{-3mm}
    \centering
    \includegraphics[width=\textwidth]{fig/fig5.pdf}
    \vspace{-10mm}
    \caption{Comparison of success rates between our method and the baseline across different task categories.}
    \label{fig:5} 
    \vspace{-4mm}
\end{figure*}





\subsection{Ablation - Dissecting \textbf{\ourmethod}}
\label{sec:ablation}

We further dissect the gains of our framework by analyzing the two key innovations introduced in the {distillation} process (\S\ref{sec:distill}):
(\textbf{i})~\textit{speculative data synthesis} to mitigate off-policy bias,
(\textbf{ii})~\textit{multi-task learning} to preserve reasoning capabilities.

% \noindent\textbf{Small LLM Distillation.}
Table~\ref{tab:ablation} provides the {success rate} of various \texttt{LLaMA-8B} configurations. We compare vanilla supervised fine-tuning (\emph{\texttt{LLaMA-8B}}),  Multi-Task Learning for Reasoning (\emph{multi-task}), and Speculative Data Synthesis (\emph{speculative}) strategy.
Results show that:
\ding{182}~Switching from plain SFT ($40\%$) to “\texttt{LLaMA-8B} \emph{+ speculative}” ($46.8\%$) confers a large boost, validating that “teacher-filtered” data expansions help rectify off-policy mismatches.
\ding{183}~Combining \emph{speculative} with \emph{multi-task} ($49\%$) yields the best performance, reinforcing the importance of maintaining chain-of-thought reasoning while also exploring the environment for speculative data synthesis. 
\ding{184}~We observe that a straightforward fine-tuning approach, without incorporating any \emph{speculative} or \emph{multi-task}, still achieves a remarkably high success rate (SR) of $40\%$, significantly surpassing the $28\%$ SR of the previous $22$B Learn-by-Interact model. This improvement can be attributed to several key factors: (a) our high-quality trajectories generated by a large LLM serve as a distillation dataset, (b) we employ a multi-LLM debate mechanism to select execution trajectories that are valuable for distillation step, and (c) our experiments are built upon the {AgentOccam}  framework, which includes an observation compression component to improve the performance.
  



% \begin{table}[htbp]
% \centering
% \caption{Ablation study on small LLM distillation. We report success rates (SR) on WebArena. 
% ``multi-task'' denotes Multi-Task Learning for Reasoning. 
% ``speculative'' denotes Speculative Data Synthesis.}
% \label{tab:ablation}
% \begin{tabular}{l c}
% \toprule
% \textbf{Method} & \textbf{SR (\%)} \\
% \midrule
% Llama-8b(before tuning)                                 & $0.05$ \\
% Llama-8b                           & $0.40$ \\
% Llama-8b + multi-task     & $0.43$ \\
% % Llama-8b-on policy                     & 0.38 \\
% Llama-8b + speculative                   & $0.47$ \\
% Llama-8b + speculative + multi-task & $\textbf{0.49}$ \\

% \bottomrule
% \end{tabular}
% \end{table}



\subsection{Domain-Specific Analysis}

To validate that our \textbf{\ourmethod} improvements generalize across different web domains, we report performance per domain in \textsc{WebArena}. 
Figure~\ref{fig:5} shows a representative subset of tasks for {\fontfamily{pcr}\selectfont Shopping, Shopping Admin, GitLab, Map, Reddit}, and  {\fontfamily{pcr}\selectfont Multisite} forums. 
\ourmethod consistently outperforms or closely matches the best domain-specific baselines~(e.g., $7\%$ higher for {\fontfamily{pcr}\selectfont GitLab} tasks), highlighting the advantage of iterative synergy in discovering domain-specific action patterns (especially via small LLM exploration) and systematically incorporating them into the large LLM’s RAG knowledge base and help decision-making.


        




\noindent\textbf{Qualitative Observations.} 
We observe that tasks in “{\fontfamily{pcr}\selectfont Shopping Admin}”, “{\fontfamily{pcr}\selectfont Shopping}” or “{\fontfamily{pcr}\selectfont GitLab}” often require multi-step forms and error-handling logic. 
In these domains, small LLMs can occasionally stumble onto unorthodox solutions (e.g., toggling unexpected web elements or exploring deeper page links), which subsequently become valuable references in the large LLM’s RAG knowledge store.
Such synergy is precisely the mechanism described in Section~\ref{sec:LS}, wherein small LLM exploration broadens the \emph{action-state coverage}, enabling large LLMs to better \emph{exploit} newly discovered or less conventional paths.


\vspace{-1mm}
\subsection{Hybrid Mode Analysis}
We utilize a locally deployed \texttt{DeepSeek-R1} in hybrid mode for privacy preservation to analyze whether each observation and action contains privacy-related information. As shown in Figure ~\ref{fig:4}, we present the probability of encountering privacy-sensitive information across different task types. Experimental results indicate that the {\fontfamily{pcr}\selectfont Shopping Admin} category exhibits the highest occurrence of privacy-related information, primarily due to webpage observations containing sensitive details such as phone numbers, shipping addresses, and purchase histories. In contrast, categories like {\fontfamily{pcr}\selectfont Reddit} and {\fontfamily{pcr}\selectfont GitLab} rarely involve filling in or viewing personal information. 

Furthermore, personal privacy information constitutes a significant portion, reaching $44.6\%$, while the total proportion of privacy-related information sums up to $61.2\%$, far exceeding the $38.8\%$ of non-privacy-related cases. These findings highlight the critical importance of safeguarding privacy information in the domain of autonomous agents.  Privacy detection example is shown in Appendix~\ref{app:hybrid}.

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.48\textwidth]{fig/fig4.pdf} % 调整宽度为单栏
    \caption{The Privacy Detector detects and categorizes tasks containing privacy information. We analyze their distribution to understand privacy interactions better.}
    \label{fig:4} 
    \vspace{-5mm}
\end{figure}

% \vspace{-1mm}
\section{Conclusion}
\label{sec:Conclusion}
% \vspace{-1mm}
In this paper, we introduced \ourmethod, an efficient and straightforward framework that establishes an iterative cycle in which a large LLM and a small LLM continuously enhance each other’s performance. Within this framework, we proposed two novel distillation techniques—speculative data synthesis and multi-task learning—that significantly improve the effectiveness of distilling small LLMs. Additionally, we designed a hybrid mode for privacy preservation, leveraging the complementary strengths of large and small LLMs to safeguard users’ private information.

% In this paper, we introduce an efficient and simple framework, \ourmethod, which establishes an iterative cycle in which a large LLM and a small LLM continuously symbiotically improve each other's performance. Within this framework, we propose two novel distillation techniques:  speculative data synthesis and multi-task learning, which significantly improve the effectiveness of distilling the small LLM. Additionally, we design a hybrid mode for privacy preservation, leveraging the collaboration between the large LLM and the small LLM to safeguard users' private information. 
% Experimental results demonstrate that both our large LLM and small LLM achieve SOTA performance. Future work includes extending environments like OSWorld and developing more advanced methodologies for precise privacy protection detection.







\clearpage
\section*{Limitations}

\begin{itemize}[leftmargin=*]
    \item \textbf{Budget Constraints:} Due to budget constraints, models like GPT-o1 were not included in our experiments. Moreover, migrating to other benchmarks also incurs substantial API costs. Therefore, similar to many related papers~\cite{yang2024agentoccam,zhou2023webarena,sodhi2024step}, we focus solely on the \textsc{WebArena}~\cite{zhou2023webarena} framework. However, our method is simple and efficient, without any benchmark-specific optimizations, making it easily transferable to other models.
Also, we are unable to measure results after multiple iterative steps and can only evaluate the results after three iterations on a subset of tasks.
Moreover, we were unable to reproduce the results of all baselines. Instead, we referenced the results reported in their respective papers and marked them with an asterisk (*) in the table for clarification.

    \item \textbf{Privacy Assessment:} Since \textsc{WebArena} does not provide labels for private information within its tasks, we are unable to quantitatively assess privacy protection. We did not explore private methods further, evaluate DeepSeek~\cite{guo2025deepseek} on other private datasets, or investigate additional distillation methods, as our primary focus is on the symbiotic improvement of both large and small LLMs.
    % \item \textbf{Omitted Details on RAG Learning:} Due to space constraints, we did not provide a detailed description of the RAG interaction learning process. This component of our method is based on \textit{Learn-by-Interact}~\cite{su2025learn}.

    \item \textbf{Hardware and Time Constraints:} Extending distillation to more and larger models is highly challenging due to hardware and time limitations. Therefore, we selected these three models \texttt{DeepSeek-R1-Distill-Llama-8B}, \texttt{Llama-3.2-1B-Instruct}, and \texttt{Llama-3.1-\\8B-Instruct} for our distillation experiments.

    % \item \textbf{Formatting of Examples in Appendix:} The trajectory examples~\ref{app:Trajectory} and Hybrid Mode examples~\ref{app:hybrid} in the Appendix have been manually formatted with line breaks; otherwise, they would appear as an unstructured block of text composed of \texttt{\textbackslash n} and \texttt{\textbackslash t} characters. For this reason, we did not include additional examples. More unprocessed examples can be found on the \textsc{WebArena} leaderboard.

    \item \textbf{Comparison with Open-Source Methods:} In the \textsc{WebArena} leaderboard results, we compare our method only with projects that have open-source code. The highest previous result among open-source methods was \textit{AgentOccam-Judge}~\cite{yang2024agentoccam}, with a score of 45.7. In this paper, we do not consider closed-source results such as \textit{OpenAI Operator}.

    \item \textbf{Temperature Setting and Variability in Results:} Since LLM outputs often do not strictly follow our instructions to generate structured and multi-faceted responses, we set the temperature to 0.6. As a result, our experimental outcomes may vary across multiple attempts on the same task, sometimes succeeding and sometimes failing. However, due to hardware and budget constraints, we were unable to conduct multiple trials to estimate the variance in accuracy.
\end{itemize}
% Due to budget constraints, models like GPT-o1 were not included in our experiments. Moreover, migrating to other benchmarks also incurs substantial API costs. Therefore, similar to many related papers, we focus solely on the \textsc{WebArena} framework. However, our method is simple and efficient, without any benchmark-specific optimizations, making it easily transferable to other models. Moreover, we are unable to measure the results after multiple iterative steps and can only evaluate the results after three iterations on subset tasks. Additionally, we were unable to reproduce the results of all baselines. Therefore, we chose to reference the results reported in their papers and marked them with * in the table for clarification. Since \textsc{WebArena} does not provide labels for private information within its tasks, we are unable to provide a quantitative assessment of privacy protection. We did not further explore private methods, evaluate DeepSeek on other private datasets, or further explore other distillation methods, as our primary focus is on the symbiotic improvement of both large and small LLMs. Due to space constraints, we did not provide a detailed description of the RAG interaction learning process. This part of our method is based on the {Learn-by-Interact}. Additionally, due to hardware and time constraints, extending distillation to more and larger models is highly challenging. Therefore, we selected these three models for our distillation experiments. In the Appendix, the observations in the trajectory examples and Hybrid Mode examples have been manually formatted with line breaks; otherwise, they would appear as a cluttered block of text composed of \text{\textbackslash n} and \text{\textbackslash t} characters. For this reason, we did not include additional examples. More unprocessed examples can be found on the \textsc{WebArena} leaderboard. In the \textsc{WebArena} leaderboard results, we only compare our method with projects that have open-source code. The highest previous result among open-source methods was {AgentOccam-Judge} with a score of 45.7. In this paper, we do not consider closed-source results such as {OpenAI Operator}. Since LLM outputs often do not strictly follow our instructions to generate structured and multi-faceted responses, we had to set the temperature to 0.6. As a result, our experimental outcomes may vary across multiple attempts on the same task, sometimes succeeding and sometimes failing. However, due to hardware and budget constraints, we were unable to conduct multiple trials to provide an accuracy range.

\section*{Ethics Statement}
Our work on symbiotic web-browsing agents underscores a commitment to responsibly advancing AI while protecting user privacy. Specifically, we use only publicly available benchmark data from \textsc{WebArena} and incorporate a “hybrid mode” that detects and handles sensitive or personal information locally, thus limiting the exposure of private data to external services. Any system built upon our framework should similarly ensure that interactions involving potentially identifiable or private user data are confined to secure, on-device models. Although we demonstrate performance gains through iterative cooperation between large and small LLMs, measures must be taken to guard against possible misuse, such as unauthorized logging of passwords or personal data. Moreover, in adhering to standard research ethics, all code and experiments were designed with transparency in mind, prioritizing reproducibility, explainability, and the principle of “do no harm”.
% We take ethical considerations very seriously. This work specifically addresses the critical concern of privacy when users interact with autonomous agents. Our proposed framework significantly reduces the risk of private information being inadvertently exposed online. All experiments are conducted on publicly available datasets, and our findings and conclusions are reported with accuracy and objectivity. Therefore, we believe that this research does not pose ethical concerns.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

\section*{Acknowledgement}
This work is generously supported by Ventus AI with Funds and Computing.

\bibliography{custom}

\appendix

\onecolumn
    \section{Related Work}
     \label{app:related}
     \textbf{Agent.} Many agent-based methods have been proposed to tackle real-world challenges, spanning diverse domains such as software engineering, reinforcement learning, multi-agent collaboration, and web interaction.

One line of research focuses on enhancing agent decision-making and problem-solving capabilities. For instance, Monte Carlo Tree Search (MCTS) and Hindsight Feedback have been employed to improve software agents~\cite{antoniades2024swe}, while an MCTS-based approach has been designed to update foundation models for long-horizon tasks~\cite{yu2025improving}. Additionally, bi-level tree search has been explored as a mechanism for self-improving LLM decision-making~\cite{light2025strategist}.

Another key area of advancement is multi-agent collaboration and coordination. Recent work has introduced a novel framework for multi-agent motion generation~\cite{tian2025direct}, as well as an Internet of Agents (IoA) framework that enhances collaboration among autonomous agents using large language models~\cite{chen2025internet}. Similarly, a multi-agent system has been developed to solve complex queries by leveraging specialized agents for different sub-tasks~\cite{li2025agentoriented}, while efficient offline coordination has been explored through diffusion-based trajectory stitching~\cite{yuan2025efficient}.

In the realm of LLM-powered agents, researchers have investigated their application in interactive and visual environments. For example, an embodied agent has been designed to learn causal relationships in the open-world setting of Minecraft~\cite{yu2024adam}, and a new benchmark has been proposed to evaluate coding agents' performance in real-world software engineering tasks involving visual elements~\cite{yang2025swebench}. Additionally, multiple studies have focused on LLM-based web agents, such as leveraging webpage UIs for text-rich visual understanding~\cite{liu2025harnessing} and synthesizing agent trajectories using web tutorials~\cite{xu2025agenttrek}.

Further research has explored LLM alignment and adaptation. Studies have examined the moral alignment of LLM agents~\cite{tennant2025moral}, the differences between aligned LLMs and browser-based agents~\cite{kumar2025aligned}, and strategies for LLM-driven self-improvement in web-based tasks~\cite{patel2025large}. Additionally, a retrieval-augmented generalist agent has been proposed to enable in-context adaptation to new environments~\cite{sridhar2025regent}, while a large-scale benchmark has been introduced to evaluate API-based agents in real-world scenarios~\cite{shen2025shortcutsbench}.

Finally, benchmarking and evaluation frameworks have become increasingly prevalent in agent research. Efforts include benchmarking multimodal retrieval-augmented generation using dynamic VQA datasets~\cite{li2025benchmarking}, assessing long-context multimodal agents via video-based web tasks~\cite{jang2025videowebarena}, and designing an asynchronous planning benchmark for LLM-driven agents~\cite{gonzalez-pumariega2025robotouille}.

As agent research continues to evolve, these developments pave the way for more capable, adaptable, and collaborative intelligent systems across a wide range of real-world applications.

     
    \textbf{Web Agent.} Recent research has made significant strides in improving web agents, particularly by leveraging curated or automatically synthesized interaction trajectories as training datasets or in-context examples~\cite{qi2024webrl, shen2024scribeagent, hu2024agentgen, zeng2023agenttuning}. For instance, \citet{su2025learn} proposed a data-centric framework that enables LLM agents to adapt to new environments by synthesizing agent-environment interaction trajectories without requiring human annotations.

Another active area of research focuses on multi-agent collaboration for complex web tasks~\cite{hou2024coact, fourney2024magenticone, zhang2024webpilot}. Within this domain, \citet{fourney2024magenticone} and \citet{fu2025agentrefine} introduced a multi-agent architecture where a lead agent is responsible for planning, tracking progress, and dynamically re-planning to recover from errors. Additionally, \citet{yang2024agentoccam} demonstrated that refining a web agent’s observation and action space to better align with LLM capabilities can yield impressive zero-shot performance.

Several studies have incorporated Monte Carlo Tree Search (MCTS) techniques to enhance web agents' decision-making capabilities. These methods iteratively expand intermediate states (tree nodes) through multiple trials on the same task~\cite{zhou2023language, zhang2024webpilot, putta2024agentq}. \citet{koh2024tree} further refined this approach by employing a trained value function to guide the search process and backtrack within the task execution tree. Meanwhile, Auto Eval and Refine~\cite{pan2024autonomous} introduced a reflective reasoning mechanism~\cite{shinn2023reflexion}, using a dedicated evaluator to refine task execution based on insights from previous trials.

Earlier research explored prompt-based methods~\cite{yao2023react, yang2024swe, gur2023real, zhang2023you}, though these approaches are inherently constrained by the capabilities of their underlying foundation models. Other studies have focused on training LLMs using human-annotated examples~\cite{chen2023fireact, li2020mapping}, while recent advancements have introduced progressive understanding web agents for web crawler~\cite{DBLP:journals/corr/abs-2404-12753} and web scraper generation~\cite{DBLP:conf/emnlp/HuangGPLLXWC24}.

Several works have also explored environment modeling and reinforcement learning for web agents. \citet{chae2025web} proposed Web Agents with World Models, which learn and leverage environment dynamics for web navigation. \citet{qi2025webrl} introduced WebRL, a framework that trains LLM web agents using a self-evolving online curriculum reinforcement learning approach. Additionally, \citet{xu2025agenttrek} developed AgentTrek, which synthesizes agent trajectories using web tutorials as guidance.

To benchmark web agent performance, recent studies have introduced TurkingBench~\cite{DBLP:journals/corr/abs-2403-11905}, a challenging benchmark for evaluating web agents across various tasks. Furthermore, research on contextual understanding has led to methods that enhance decision-making by learning to better interpret web pages~\cite{lee2025learning}.

As web agents continue to evolve, these advancements contribute to more adaptive, autonomous, and intelligent systems capable of efficiently navigating and interacting with complex web environments. 


    \noindent\textbf{Knowledge Distillation.}  Earlier methods~\cite{hinton2015distilling, kim2020paraphrasing, mirzadeh2019improved, 10.5555/3294771.3294842, fu2023specializing, magister2023teaching, mukherjee2023orca, li2024mixed} focus on training a smaller student network based on the output of a larger teacher network. \citet{huang2022incontext} introduces in-context learning distillation to combine in-context learning objectives with language modeling objectives to distill both the in-context few-shot learning ability and task knowledge to the smaller models.
    Besides, \citet{chen2023fireact} demonstrates that fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a $77\%$ HotpotQA performance increase. \citet{xu2024speculative} shows how to solve off-policy bias. \citet{su2025learn} showcases that utilizing the synthesized trajectory data for training yields better results compared to using it as in-context examples. 



\noindent\textbf{Benchmarks.} 
Recent advancements in benchmarking have introduced diverse evaluation datasets targeting specific capabilities of large language models (LLMs). SWE-bench~\cite{jimenez2023swe} focuses on assessing LLMs' performance in software engineering tasks, including code generation, debugging, and documentation. \textsc{WebArena}~\cite{zhou2023webarena} evaluates the ability of LLMs to navigate websites, extract information, and perform web-based tasks in simulated online environments. OSWorld~\cite{xie2024osworld} provides a platform to test LLMs' reasoning and adaptability in open-ended, dynamic, and exploratory simulated worlds. Finally, Spider2-V~\cite{cao2024spider2} extends the original Spider benchmark by introducing more complex SQL queries and diverse database interactions, testing LLMs' proficiency in structured query language tasks. These benchmarks collectively push the boundaries of LLM evaluation across software engineering, web interaction, open-world reasoning, and database management.

\clearpage
\section{Implementation Details} \label{app:exp}
\subsection{Distillation Training}
We conduct the distillation training on \textbf{8 H100 GPUs}, using full-parameter fine-tuning for the models \texttt{Llama-3.2-1B-Instruct}, \texttt{Llama-3.1-8B-Instruct}, and \texttt{DeepSeek-R1-Distill-Llama-8B}. The training process spans \textbf{2 epochs}, with a learning rate of $10^{-4}$ and a context length of $10,000$. The distillation methodology follows the guidelines provided in \texttt{meta-llama/llama-cookbook}. We adopt the \texttt{alpaca\_dataset} format and enable Fully Sharded Data Parallel (FSDP) to facilitate efficient distributed training.

\subsection{Inference Pipeline}
For inference, we employ the \texttt{vLLM} framework, running on \textbf{4 H100 GPUs}. The \textsc{WebArena} framework is deployed on \textbf{8 CPU machines}, utilizing an Amazon Machine Image (AMI) pre-installed with all necessary websites. To enhance efficiency, we leverage the official task-parallel Bash script for parallel execution, rather than processing tasks sequentially by task ID.

\subsection{Chain-of-Thought Prompting}
For Chain-of-Thought (CoT) prompting, we follow the design principles outlined in \textsc{Thinking-Claude},  ensuring structured and effective reasoning in model responses. The implementation details can be found at \href{https://github.com/richards199999/Thinking-Claude}{\texttt{github.com/richards199999/Thinking-Claude}}.



\section{Baselines} \label{app:base}
We compare our \ourmethod approach against several representative baselines:  {{AgentOccam}} ~\cite{yang2024agentoccam}: An LLM-based web agent that refines its observation and action spaces, aligning them more closely with the LLM’s inherent capabilities.
{{Learn-by-Interact}} ~\cite{su2025learn}: A data-centric framework designed to adapt LLM agents to the environment without the need for human annotations.
{WebArena-replication} ~\cite{zhou2023webarena}: An agent that is implemented in a few-shot in-context learning fashion with
powerful large language models.
{SteP-replication} ~\cite{sodhi2024step}: A dynamic framework to compose LLM policies for solving diverse web tasks through adaptable control states.
LATS~\cite{zhou2023language}: A framework integrating reasoning, acting, and planning via Language Agent Tree Search.
AWM~\cite{wang2024agent}: A workflow memory method for guiding agent decision-making.
API-Based Agent~\cite{song2024beyond}: A framework combining API calls and web browsing for web tasks.
AutoEval~\cite{pan2024autonomous}: An evaluation-driven approach for improving web navigation and device control.
WebPilot~\cite{zhang2024webpilot}: A multi-agent system enhancing MCTS for complex web tasks.

    \clearpage
    \section{RAG Algorithm} \label{app:rag}

    \begin{algorithm}
        [htbp]
        \caption{RAG-Enhanced Large LLM}
        \label{alg:rag_large_model}
        \begin{algorithmic}
            [1]

            \STATE \textbf{Input:} Task instruction $I$, current observation
            $o_{i}$, RAG knowledge base $B_{RAG}$, number of retrieved examples
            $K$. \STATE \textbf{Output:} Interaction History$H$.

            \FOR{each instruction $I$ in environment} \STATE $T =interact(I)$
            \STATE $T' = subsequences(T)$ \STATE //Split $T$ into subsequences
            $T'$ \FOR{each $T'$} \STATE $I', S' = LLM(T')$ \STATE //generate task
            instructions and summaries \IF{$LLM(T', I', S')$} \STATE
            $B_{RAG}.append(T', I', S')$ \STATE //Store $(T', I', S')$ in
            $B_{RAG}$ if valid. \ENDIF \ENDFOR \ENDFOR

            $H \gets \emptyset$ \STATE // Initialize History. \FOR{each Step $i$ in interaction for $I$}
            \STATE $q \gets M_{L}(I, o_{i})$ \STATE //Generate retrieval queries
            \STATE $E \gets \emptyset$ \STATE // Initialize retrieved examples. \FOR{strategy in \{retrieval strategies\}}
            \STATE
            $E \gets E \cup \text{Retrieve}(q,{B}_{RAG},\text{strategy}, H_{i-1})$
            \ENDFOR \STATE $E_{\text{filtered}}\gets \text{Filter}(E, M_{L})$

            \STATE $[a_{i}, r_{i}] \gets M_{L}(I, o_{i}, E_{\text{filtered}})$

            \STATE $H_{i+1}\gets H_{i}.append(o_{i},[a_{i}, r_{i}])$ \ENDFOR
            \STATE \textbf{Return:} $H$.
        \end{algorithmic}
    \end{algorithm}
    
Given the WebArena environment, we can first leverage commonly accessible resources such as official documentation, tutorials, FAQs, and community forums to generate diverse task instructions using a Self-Instruct~\cite{wang2022self} approach. For each generated task, LLMs then aim to solve it, which results in a long trajectory such as $(o_{0}, a_{0}, o_{1}, a_{1}, o_{2})$. Then it is split into subsequences like $(o_{0}, a_{0}, o_{1})$, $( o_{1}, a_{1}, o_{2})$. For each subsequence, we use a Claude-3.5 to generate task instructions and summaries. Then a multi-LLM debate mechanism is employed to evaluate the generated trajectories. Selected trajectories serve as RAG knowledge base.

Later, when interacting with the environment, at each step $i$, access is provided to the current observation $o_{i}$, the task instruction $I$, and additional information, such as interaction history summaries or previously generated plans. To retrieve relevant knowledge to assist the agent, a mixture of three different retrieval strategies is proposed:

(a)~\textbf{Task-Guided Summary Retrieval}: Queries are generated from task instructions and webpage observations to retrieve relevant past experiences from the RAG knowledge base.

    (b)~\textbf{Direct Observation and Instruction Matching}: Match the current observation
    and instruction with those from the RAG knowledge base.
    The system matches the current webpage observation and instruction directly with observations and instruction from previously recorded trajectory examples.
    
    (c)~\textbf{Trajectory Similarity Search}: Retrieve similar interaction
    examples by computing and comparing trajectory embeddings via cosine
    similarity. 
    
    After retrieving $K$ trajectory examples, a \textbf{filtering step} ensures their
    quality and relevance. Since similarity alone doesn't guarantee usefulness
    for action prediction, an LLM employs a \textit{chain-of-thought} reasoning
    process to evaluate and rank examples. High-quality examples are selected to
    aid decision-making and used for the agent. 


Our RAG Algorithm is based on {{Learn-by-Interact}} ~\cite{su2025learn}.

    

\clearpage
    \section{Distillation Algorithm} \label{app:dis}
    \begin{algorithm}
        [htbp]
        \caption{Improved Distillation for Small LLMs}
        \label{alg:improved_distillation}
        \begin{algorithmic}
            [1]

            \STATE \textbf{Input:} Large LLM $M_{\mathrm{L}}$, Small LLM $M_{\mathrm{S}}$, environment
            $E$, instruction $I$, number of action candidates $K$, Judge LLM
            $M_{\mathrm{R}}$. \STATE \textbf{Output:} Distilled small LLM $M_{\mathrm{S}}$.

            \STATE Initialize training dataset $D \gets \emptyset$.

            \FOR{each instruction $I$ in environment} \STATE $\mathcal{H} \gets \emptyset$ \STATE
            // Initialize interaction history. \FOR{each Step $i$ in interaction for $I$}
            \STATE $o_{i} \gets E.\text{get\_observation}()$ \STATE
            $[a_{i}, r_{i}] \gets M_{\mathrm{S}}(I, o_{i})$ \STATE // small LLM generates
            action and reason \STATE $\{(a^{k}, r^{k})\}_{k=1}^{K} \gets M_{\mathrm{L}}(I,
            \mathcal{H}_{i}, o_{i})$ \STATE // Large LLM generates $K$ action candidates. \IF{$a_{i} \in \{a^{k}\}_{k=1}^{K}$}
            \STATE $a \gets a_{i}$ \hfill // Accept action. \ELSE \STATE
            $a \gets Best(a^{k})$ \STATE // Select best action
            from large LLM. \ENDIF

            \STATE $\mathcal{H}_{i+1}\gets \mathcal{H}_{i}.append(o_{i}, a_{i}, r_{i})$ \ENDFOR

            \IF{$M_{\mathrm{R}}(\mathcal{H})$} \STATE // Use LLM to judge the quality of trajectory
            \STATE $D \gets D \cup \mathcal{H}$. \ENDIF \ENDFOR

            \STATE // Train $M_{\mathrm{S}}$ using multi-task learning: \FOR{each batch $(o, a, r)$ in $D$}
            \STATE Compute loss for next action prediction $L_{\text{action}}$.
            \STATE Compute loss for reasoning generation $L_{\text{reasoning}}$.
            \STATE Optimize $M_{\mathrm{S}}$ with $L_{\text{action}}$ and $L_{\text{reasoning}}$.
            \ENDFOR

            \STATE \textbf{Return:} Distilled small LLM $M_{\mathrm{S}}$.
        \end{algorithmic}
    \end{algorithm}

We use Speculative Data Synthesis to leverage dynamic collaboration between the large (teacher) and small (student) LLMs to iteratively and adaptively generate high-quality training data. At each step $i$, small LLM ($M_{\mathrm{S}}$) predicts the action-reason $[a_{i}, r_{i}]$ based on the instruction and observation $(I, o_{i})$.
While large LLM ($M_{\mathrm{L}}$) predicts the action-reason candidates $\{(a^{k}, r^{k})\}_{k=1}^{K}$ based on the instruction, interaction history, and observation $(I, \mathcal{H}_{i}, o_{i})$. If the student’s action is found within the large LLM’s candidates, it is deemed reliable and executed. If the action falls outside the candidate set, it is rejected and replaced by the most reliable action selected from the candidates.

Also, training an agent typically requires a large amount of manually annotated data, and it is often challenging to effectively associate actions with observations. To address this, we first leverage the large LLM to predict the success or failure of each trajectory. Additionally, we enhance the model's performance enabling it to generate intermediate rationales and improve its reasoning capabilities. To preserve the crucial reasoning capabilities during distillation, we implement a multi-task learning approach that goes beyond simple action prediction. Drawing inspiration from the chain-of-thought prompting literature, we prompt the large model to generate not just actions, but also intermediate reasoning steps, including:

\begin{itemize}
\item Action Generation: Predicting only the next action
\item Reason: Include generating rationale; Producing analysis of the current state; Summarizing past interactions.
\end{itemize}
The diverse training objectives help the model retain structured reasoning capabilities while operating within the computational constraints of a smaller architecture. By alternating between these objectives, the training process ensures the model develops a balanced skill set. This is especially crucial for web browsing agents, where success depends not only on selecting the correct actions but also on maintaining a coherent understanding of task progress and navigation history.






    

\clearpage
\section{Filtering Step Prompt} \label{app:Filter}

\input{latex/prompt/filter}



\clearpage
\section{Private Detect Prompt} \label{app:private}


\input{latex/prompt/privateprompt}
\clearpage
\section{Hybrid Mode Example} \label{app:hybrid}

\input{latex/prompt/hybrid}
\clearpage
\section{Chain-of-thought Prompt} \label{app:cot}

\input{latex/prompt/cot}

\clearpage
\section{Filtering Step Example} \label{app:Filterexp}

\input{latex/prompt/filterexample}
\clearpage
\section{Trajectory Example} \label{app:Trajectory}

\input{latex/prompt/Trajectory}

\clearpage
\section{Potential Risks} \label{app:risk}
First, errors in small LLM exploration could propagate through iterative updates, leading to unintended biases. Second, while our hybrid mode enhances privacy, imperfect detection may still expose sensitive data. Lastly, reliance on synthesized knowledge introduces risks of hallucination or misleading information. Future work should focus on improving robustness, privacy safeguards, and knowledge verification.

\end{document}
