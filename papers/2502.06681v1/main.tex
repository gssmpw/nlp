
\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lineno}
\usepackage{tikz}
\usepackage{todonotes}
\usepackage{pgfplots}
\usepackage{array} % Ensure array package is loaded
\usepackage{svg}
\usepackage{graphicx}
\usepackage{amssymb} 
\usepackage{amsmath}

\usepackage{caption}
\usepackage{subcaption}


\usepackage[acronym,]{glossaries}
\newacronym{hmm}{HMM}{Human Muscular Manipulability}
\newacronym{kmi}{KMI}{Kinematic Manipulability Index}
\newacronym{dmi}{DMI}{Dynamic Manipulability Index}
\newacronym{lci}{LCI}{Local Conditionint Index}
\newacronym{emg}{EMG}{Electromyography}
\newacronym{mae}{MAE}{Mean Absolute Error}
\newacronym{ae}{AE}{Absolute Error}
\newacronym{imu}{IMU}{Inertial Measurement Unit}

\usepackage{array}

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}


\title{CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis}

\author[1*]{Bessie Dominguez-Dager}
\author[1]{Felix Escalona}
\author[1]{Francisco Gomez-Donoso}
\author[1]{Miguel Cazorla}


\affil[1]{Institute for Computing Research, P.O. Box 99. 03080, Alicante, Spain.}
\affil[*]{corresponding author(s): Bessie Dominguez-Dager (bessie.dominguez@ua.es)}


\begin{abstract}

Person re-identification (Re-ID) is a key challenge in computer vision, requiring the matching of individuals across different cameras, locations, and time periods. While most research focuses on short-term scenarios with minimal appearance changes, real-world applications demand robust Re-ID systems capable of handling long-term scenarios, where persons' appearances can change significantly due to variations in clothing and physical characteristics. 
In this paper, we present CHIRLA, Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis, a novel dataset specifically designed for long-term person Re-ID. CHIRLA consists of recordings from strategically placed cameras over a seven-month period, capturing significant variations in both temporal and appearance attributes, including controlled changes in participants' clothing and physical features. The dataset includes 22 individuals, four connected indoor environments, and seven cameras. We collected more than five hours of video that we semi-automatically labeled to generate around one million bounding boxes with identity annotations. By introducing this comprehensive benchmark, we aim to facilitate the development and evaluation of Re-ID algorithms that can reliably perform in challenging, long-term real-world scenarios.


\end{abstract}
\pgfplotsset{compat=1.18} 
\begin{document}

\flushbottom
\maketitle

\section*{Background \& Summary}

Person re-identification (Re-ID) is a challenging problem in computer vision that involves matching individuals across different camera views, locations and times. This task becomes even more complex when considering long-term scenarios, where persons may change their appearance due to variations in clothing or alterations in physical characteristics. 

Early Re-ID research relied on datasets that primarily focused on short-term Re-ID, involving single images or small image sequences. Examples include ETH \cite{schwartz09d}, GRID \cite{loy2010time}, VIPeR \cite{gray2008viewpoint}, and CAVIAR \cite{cheng2011custom}. These datasets typically feature low-resolution images and limited temporal information.
In later years, with the advent of deep learning, larger datasets began to appear. These datasets expanded the number of identities (IDs), provided more views of the same person from different cameras and offered longer sequences. This is the case of datasets such as CUHK03 \cite{li2014deepreid}, RAiD \cite{das2014consistent} and Market1501 \cite{zheng2015scalable}.

The most recent datasets directly provide the complete videos of the sequences, with the necessary bounding boxes and ID annotations, so that they can be used to evaluate the actual performance of video re-identification systems, and can take advantage of complementary tracking techniques. In this category we can find the datasets MARS \cite{zheng2016mars} and PRW \cite{zheng2017person}.

While many of these existing datasets focus on short-term Re-ID, there is a growing need for datasets that capture the variability of individuals' appearances over extended periods. Such datasets are crucial for developing more robust Re-ID algorithms capable of handling real-world challenges, particularly in environments where individuals frequently change their clothing or physical characteristics. Existing datasets often lack the variability in clothing and appearance changes, making them less suitable for training and evaluating models designed for long-term Re-ID scenarios.

Table \ref{tab:datasetreview} summarizes the characteristics of the datasets mentioned above. In this table, \textit{time coherence} refers to whether the cameras in the analyzed dataset recorded simultaneously and were synchronized across views, enabling the identification of the same person across different cameras or during transitions between camera viewpoints in the environment setup. This feature also includes the requirement of providing the corresponding videos from that setup, allowing their analysis for person Re-ID tasks. 
The listed resolution corresponds to the maximum resolution available in each dataset. In cases where cameras or sequences were recorded at different resolutions, only the highest resolution is reported. Some datasets provide cropped ID bounding boxes directly, while others offer full-frame images. For the CUHK03 dataset \cite{li2014deepreid}, the resolution refers to bounding box crops, which are either manually created or automatically detected, with resolutions varying around 160×60 pixels.

\begin{table}[!htb]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    Datasets & \begin{tabular}[c]{@{}c@{}}\# \\ Videos\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. Time\\ /Videos (s)\end{tabular} & \# IDs & \begin{tabular}[c]{@{}c@{}}Total \\ Frames\end{tabular} & \begin{tabular}[c]{@{}c@{}}Image\\ Resolution\end{tabular}  & \begin{tabular}[c]{@{}c@{}} \# \\ Cameras\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\ Coherence\end{tabular} & \begin{tabular}[c]{@{}c@{}}IDs\\ Coherence\end{tabular} \\ \hline
    ETH \cite{schwartz09d}          & 4  & 38  & 146   & 2,293     & 640×480   & 1 & x & x \\ \hline
    GRID \cite{loy2010time}   & -  &  - & 250   &   1,275     & 320×230   &  8 &  x &  \checkmark \\ \hline
    VIPeR  \cite{gray2008viewpoint}        & -  & -  & 632  &   -       & 128×48    & 2 & x & \checkmark  \\ \hline
    CAVIAR  \cite{cheng2011custom}       & -  & -  & 72   & 1,220     & 72×144  & 2  & x  &  \checkmark \\ \hline
    CUHK03 \cite{li2014deepreid}        & -   & -  & 1,360 &  13,164   & Variable* & 6 & x  & \checkmark \\ \hline
    RAiD \cite{das2014consistent}          &  -  & -  & 43    & 6,920     & 128×64         & 4 &  x & \checkmark \\ \hline
    Market1501 \cite{zheng2015scalable}    & -  &  - & 1,501 &  32,668        & 1080×1920 & 6 &  x & \checkmark \\ \hline
    MARS \cite{zheng2016mars}          & $\sim$20,000 & 2   & 1,261 & 1,191,003 & 1080×1920 & 6 & x & \checkmark \\ \hline
    PRW \cite{zheng2017person}           & -   & -  & 932   & 11,816    & 1080×1920 & 6 &  x & \checkmark \\ \hline
    \textbf{CHIRLA}& 70 &  284 & 22    & 596,345   & 1080×720  & 7 & \checkmark & \checkmark \\ \hline
    \end{tabular}}
    \caption{Summary of the reviewed datasets and their features. Time coherence indicates whether cameras recorded simultaneously and were synchronized, enabling person re-ID analysis across viewpoints. The listed resolution corresponds to the maximum resolution available; for datasets with varying resolutions, only the highest is reported.}
    \label{tab:datasetreview}    
\end{table}

Although many of these datasets include a large number of IDs, they are often constrained by the short durations of their recordings, limiting their utility for long-term Re-ID evaluations. In most cases, once an individual appears in one video or sequence, they are not annotated or re-identified in subsequent sequences, restricting the evaluation of Re-ID algorithms designed for long-term scenarios.

This paper introduces a novel dataset specifically designed for long-term person (Re-ID) in an indoor environment. The dataset captures a multi-room office space featuring interconnected indoor areas where movement follows realistic patterns, including hallways, shared workspaces, and entry points. Recordings were obtained from multiple cameras strategically placed around the office, covering various angles and perspectives.
Data collection was conducted over a seven-month period to capture the natural temporal variations in participants' appearances. To further enhance the complexity of the dataset, participants were instructed to change their clothing between recording sessions, introducing significant variability in appearance that challenges existing Re-ID algorithms. The primary objective of this dataset is to provide a comprehensive benchmark for evaluating Re-ID models in real-world, long-term scenarios where individuals' clothing and physical appearance may vary significantly over time. By combining diverse recordings with controlled variations in appearance, this dataset offers a valuable resource for advancing research in person Re-ID, particularly for applications requiring high reliability over extended periods.

\section*{Methods}
\label{sec:Methods}

In this Section, we describe the capture setup and environment, including the specifications and placement of the cameras, the interconnection architecture, and the software used for data capture, organization, and labeling.

This study was conducted under the supervision of the Ethical Committee of the University of Alicante (Approval No. UA-2022-11-12). All subjects were informed about the study, including how their data would be handled, and provided their consent to participate and share the collected data. Specifically, participants granted permission for their inclusion in dataset video recordings, which capture physical data (face and body features in video recordings), and consented to making these recordings publicly available.
The dataset has been made publicly available under the CC-BY license for academic use only and is not permitted for commercial purposes.


\subsection*{Capture Setup}
\label{subsec:capture-setup}

The dataset was recorded in the Robotics, Vision and Intelligent Systems Research Group headquarters at the University of Alicante, Spain, which displays a typical office environment. The capture setup involves two main items: the physical spaces and the cameras placed in them. 

Regarding the physical spaces in which the dataset has been captured, the layout of the area covered by the cameras is shown in Figure \ref{fig:camera_position}. The environment features a typical office setting with computers, desks, chairs, panels and whiteboards. Some objects eventually appear in some videos such as robots, cardboard boxes or others. As the videos are captured during a large time frame, the appearance of the environments slightly changed from one to another. There are three large working areas, one large corridor and one small office. These areas have the following settings. L1 is a big laboratory of 33.3 $m^2$ of area that is connected to the main corridor H1 and has a camera C7 placed in the opposite corner of the door, pointing at it. L2 is another laboratory of 20.64 $m^2$ that contains two cameras: C2, which is located in the opposite corner of the door, pointing at it; and C3, which is located in the south-west corner of the room, pointing at the office O1 door. This room is connected to another laboratory L3 and the corridor H1. The last laboratory is L3, which features 27.93 $m^2$ and has one camera C4 placed above the door that connects L2 and L3, and is oriented to the door. L3 is connected to the corridor H1 and the laboratory L2. There is also an individual office O1, that has 8,48 $m^2$ of area. It has one camera C1 located in the opposite corner of the door, oriented at it. The office is connected to the laboratory L2. Finally, the corridor H1 is also involved. It has an area of 44.01 $m^2$ that is covered by camera C6 and C5. H1 is connected to the three laboratories L1, L2 and L3.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.5\linewidth]{plan_labs.png}
    \caption{Position and orientation of the cameras within the environment}
    \label{fig:camera_position}
\end{figure}

The placements of the camera have been chosen, so the images depict most of the rooms and also the doors, which are of special interest for person Re-ID tasks.
Sample frames of the point of view of each camera are shown in Figure \ref{fig:camera_pov}. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.24\linewidth, height=2.5cm]{c1.PNG}
    \includegraphics[width=0.24\linewidth, height=2.5cm]{c2.PNG}
    \includegraphics[width=0.24\linewidth, height=2.5cm]{c3.PNG}
    \includegraphics[width=0.24\linewidth, height=2.5cm]{c4.PNG}
    \includegraphics[width=0.24\linewidth, height=2.5cm]{c5.PNG}
    \includegraphics[width=0.24\linewidth, height=2.5cm]{c6.PNG}
    \includegraphics[width=0.24\linewidth, height=2.5cm]{c7.PNG}
    \caption{Sample frames of the point of view of each camera: C1-C4 (top row) and C5-C7 (bottom row).}
    \label{fig:camera_pov}
\end{figure}


\subsection*{Hardware, Connection Architecture and Synchronization}
\label{subsec:synchro}

The hardware component of the system primarily consists of a network of cameras. The selected cameras are Reolink RLC-410W, which are equipped with a 1/2.7" CMOS image sensor and deliver 5.0 megapixel resolution. Each camera features a 4.0 mm lens with an aperture of F/2.0, providing an 80° horizontal and 58° vertical field of view. In addition, they include backlight compensation to enhance image quality. The video is compressed using the H.264 format. Despite the cameras' capability of recording at 2560×1920 pixels, videos were captured at 1080×720 pixels to prevent network congestion and maintain a consistent 30 fps across all cameras.

The camera network consists of seven cameras deployed in the environment. Although these cameras support WiFi connectivity, most of them were connected via Ethernet cables to ensure a faster and more stable connection. Six of the cameras were connected to a central router using Cat 6 Ethernet cables, while one camera (C7) was connected via WiFi. The selected router is the Asus RT-AC3100 Gigabit Router, which features 8 Ethernet ports and solid WiFi capabilities.

To complete the setup, a server computer was incorporated into the system. The chosen machine is powered by an Intel i9-13900K processor with 32 cores running at 5.5 GHz, paired with 32 GB of DDR4 RAM, and operates on Ubuntu 20.04. This server is responsible for running the interconnection framework, retrieving images from all cameras, synchronizing them, and storing the collected data.


To do all these tasks, we rely on ROS (Robotic Operating System) Noetic. ROS is a comprehensive collection of software libraries and tools designed to support the development of robotic applications. It offers everything from device drivers to advanced algorithms, along with a suite of powerful tools for developers. It also includes tools to connect to remote cameras, retrieve images from them, and allows the implementation of custom synchronization techniques.

To ensure synchronization among all the cameras and guarantee that they all capture the same moment in time, the process begins by waiting several seconds to establish stable connections with each camera. This delay ensures that all cameras are properly initialized and ready to operate without any interruptions. Additionally, the frame buffer in the camera software is disabled to prevent delays caused by accumulated frames, ensuring real-time visualization of the captured frames. Furthermore, a uniform capture frequency is configured across all cameras, allowing for consistent synchronization and ensuring that all cameras capture the exact same moment in time as closely as possible. This combination of techniques minimizes the risk of desynchronization and maximizes the accuracy of the system for time-sensitive applications.

Each camera is launched in parallel within a separate ROS node, utilizing OpenCV's functionality to connect to the video stream via the Real Time Streaming Protocol (RTSP). This setup ensures efficient management of multiple video streams, enabling the system to handle concurrent connections to each camera. By assigning each camera to an independent node, the architecture leverages the distributed nature of ROS, allowing for better scalability and resource management while maintaining the synchronization and real-time performance required for the application.

\subsection*{Procedure}
\label{subsec:procedure}

The procedure for recording a sequence is straightforward. The person in charge of the dataset started recording at a random moment of the day. The subjects present in the environment were instructed to move around without any specific purpose. No constraints were applied to their dynamics, interactions, or behaviors. This approach aimed to capture realistic scenarios, which involve multiple person occlusions, changes in trajectories, and even variations in clothing or appearance, posing a significant challenge to person Re-ID methods.

Nonetheless, the process of labeling the sequences involves both automatic labeling algorithms and manual annotation. 
First, individuals are extracted from each video using the YOLOv8 algorithm \cite{Jocher_Ultralytics_YOLO_2023}. YOLOv8 (You Only Look Once, Version 8) is a deep learning-based object detection architecture capable of predicting the location and class of specific objects within an image. It has different model sizes considering the depth of the architecture, the runtime and the accuracy it provides. The YOLOv8x (extra-large) version is used as it provides the best accuracy. In our case, we feed the architecture with all the frames in order to retrieve the regions occupied by persons. The actual output consists of the $X$ and $Y$ coordinates of the opposite corners of each person's bounding box. Despite YOLOv8 being a state-of-the-art object detection method, it is not flawless, so each automatically generated label was manually verified, being confirmed or corrected when necessary. Additionally, new labels were assigned to individuals who were missed by the automatic detection process.

It should be noted that individuals are labeled only if they can be sufficiently identified; otherwise, their labels are removed. In addition to the bounding box annotations, the dataset includes a unique ID for each person. These IDs were labeled following a semi-automatic approach based on Deep SORT \cite{deepsort}. Deep SORT is an advanced object tracking method that extends the SORT (Simple Online and Realtime Tracking) algorithm \cite{sort2016} by incorporating deep learning for more accurate tracking. It leverages appearance features extracted by a deep neural network to improve track association, allowing it to handle occlusions and re-identify objects more effectively across frames. The algorithm was applied to all videos in our dataset to retrieve tracklets, which were subsequently reviewed and corrected manually to ensure accuracy. Importantly, the assigned IDs remain consistent across all cameras and sequences.

To streamline and expedite the manual verification and correction of bounding boxes and ID annotations, we developed a graphical user interface (GUI). This tool significantly facilitates the labeling process (Figure \ref{fig:gui}). The GUI is publicly available at \cite{bdager_preid_labeling_gui}.

\begin{figure}[!htb]
    \centering
    \includegraphics[height=6cm]{screenshot2.png}
    % \includegraphics[height=5cm]{screenshot1.png}
    \caption{GUI designed to enhance annotations.}
    \label{fig:gui}
\end{figure}


\subsection*{Evaluation Metrics}
\label{subsec:evaluation}

This dataset can be used for evaluation from two different but complementary points of view: pure Re-ID of persons and tracking of persons, allowing both, algorithms that perform only one of the two tasks and those that incorporate both facets to be evaluated.

\subsubsection*{Person Tracking Metrics}
\label{subsec:people-tracking-metrics}

In terms of person tracking metrics, which assess the ability of algorithms for short-term recognition, i.e., to detect a person, identify them and follow them while they remain in the scene until they leave it. In this respect, we will rely on the CLEAR MOT Metrics \cite{bernardin2008evaluating, mot16, deepsort} to define the following metrics to evaluate the tracking of persons on individual cameras since the areas covered by each camera do not overlap each other, or the percentage of overlap is low.

To understand the following metrics, we must define some concepts:
\begin{itemize}
\item {\textbf{Fragmentation ($FM$)}.  The number of interruptions in a ground truth trajectory. %, where the trajectory is momentarily untracked. 
A fragmentation is recorded each time a trajectory's status changes from "tracked" to "untracked" and tracking of the same trajectory is resumed at a later point.}
\item {\textbf{Identity Switch ($IDSW$)}. The number of times the reported ID associated with a ground truth trajectory changes throughout the tracking process. This occurs when the tracking system incorrectly reassigns the ID of a tracked person to a different ID. } 
\item {\textbf{Mostly Tracked ($MT$)}. The percentage of ground truth trajectories that correctly assigned the same ID label for at least 80\% of their lifespan. } 
\item {\textbf{Mostly Lost ($ML$)}. The percentage of ground-truth trajectories that are successfully tracked for no more than 20\% of their lifespan. } 
\item {\textbf{False Positive ($fp_t$)}. The number of targets the tracker detects in frame $t$ where there is none in the ground truth.}
\item {\textbf{False Negative ($fn_t$)}. The number of true targets missed by the tracker in frame $t$.}
\end{itemize}

The most used metric to evaluate multi-object tracking performance for a single-camera is Multiple Object Tracking Accuracy (MOTA), which is defined as shown in Equation \ref{eq:mota}.

\begin{equation}
\label{eq:mota}
    MOTA = 1 - \frac{FN + FP + IDSW}{GT},
\end{equation}

where:
\begin{conditions}
FN & total number of false negatives over all frames. \\
FP & total number of false positives over all frames. \\
IDSW & total number of ID switches over all frames. \\   
GT & total number of ground truth detections over all frames.
\end{conditions}

MOTA can be seen as derived from three error ratios:
\begin{itemize}
    \item The ratio of false negatives in the sequence, computed over the total number of objects present in all frames, shown in Equation \ref{eq:misses}.
    \item The ratio of false positives, shown in Equation \ref{eq:false-positives}.
    \item The ratio of ID switches, shown in Equation \ref{eq:mismatches}.
\end{itemize}

\begin{equation}
\label{eq:misses}
    {FN}_{ratio} = \frac{FN}{GT},
\end{equation}

\begin{equation}
\label{eq:false-positives}
    {FP}_{ratio} = \frac{FP}{GT},
\end{equation}

\begin{equation}
\label{eq:mismatches}
    {IDSW}_{ratio} = \frac{IDSW}{GT}.
\end{equation}

MOTA takes into account all object configuration errors made by the tracker across all frames: false positives, false negatives and ID switches. %
It provides a very intuitive measure of the tracker's performance in detecting objects and maintaining their trajectories, regardless of the accuracy with which object locations are estimated. 

A complementary metric is IDF1, which evaluates the consistency of object IDs over time  \cite{ristani2016performance}. Specifically, IDF1 measures the F1-score of correctly matched object trajectories, using intersection over union (IoU) and a threshold, considering both the precision and recall of the identifications at the frame level. IDP (ID Precision) measures the precision of detected trajectories that are correctly matched with the ground truth trajectories while IDR (ID Recall) measures the recall of ground truth trajectories that are correctly matched with the detected trajectories. The final IDF1-Score is the harmonic mean of IDP and IDR, calculated as shown in Equation \ref{eq:idf1}.

\begin{equation}
\label{eq:idf1}
IDF1 = 2 * \frac{IDP * IDR}{IDP+IDR}.
\end{equation}

IDF1 is particularly important because it evaluates how well an algorithm maintains the consistency of object IDs across frames. Unlike MOTA, IDF1 places additional emphasis on the quality of ID matching.

\subsubsection*{Re-identification Metrics}
\label{subsec:reid-metrics}

Tracking metrics evaluate a method's ability to maintain the same ID throughout a sequence once it has been assigned. However, these metrics do not account for the system's ability to re-identify individuals across different sequences, particularly when a person leaves the scene and a significant amount of time has passed before their reappearance. In this Section, short-term tracking capability is set aside, and the emphasis is placed on the system's ability to recognize IDs across different cameras, viewpoints, and noticeable changes in appearance.

The Cumulative Matching Characteristic (CMC) is a widely used evaluation metric in person Re-ID tasks for assessing the performance of recognition algorithms \cite{gray2007evaluating}. The CMC curve represents the probability that a correct match appears within the top $k$ ranks of a ranked list of matches. This metric provides a function of different $k$-rank accuracies, as defined in Equation \ref{eq:cmc}.

\begin{equation}
\label{eq:cmc}
     Acc_k= \left\{ \begin{array}{ll} 1, & \text{if top-$k$ ranked gallery samples contain the query ID} \\ \\ 0, & \text{otherwise} \end{array} \right.
\end{equation}

In a typical person Re-ID scenario, the system is given a query image of a person, and it has to find the correct match from a gallery of images. The system ranks all images in the gallery based on similarity to the query image. The CMC curve plots the probability of the correct match being within the top $k$ ranked positions across all queries. This metric is suitable for scenarios where the task is to rank potential matches rather than to assign a binary label to them. This ranking approach is crucial because, in large datasets, the probability of a random match being correct is very low, making it an ineffective metric for recognition. The CMC curve addresses this by focusing on ranking performance rather than binary classification performance. One of the most important associated metrics is the rank-$1$ accuracy, which means the system can correctly identify an individual from the probe camera in the gallery camera as the top match. 

CMC evaluation measurement is valid only if there is only one ground truth match for a given query \cite{zheng2015scalable}. However, if multiple ground truths exist, the CMC curve can be biased because recall is not considered. Therefore, an alternative metric to evaluate the overall performance of the Re-ID system is the Mean Average Precision (mAP). For each query, the area under the Precision-Recall curve, referred to as average precision (AP), is computed. Precision can be defined as the ratio between correct ID correspondences (True Positives) and the number of retrieved correspondences, as shown in Equation \ref{eq:precision}.

\begin{equation}
\label{eq:precision}
    precision = \frac{TP}{TP+FP}.
\end{equation}

By default, precision takes all the retrieved correspondences into account, but it can also be evaluated at a given number of retrieved images, commonly known as cut-off rank, where the model is only assessed by considering its top-most queries. The measure is called precision at $k$ or P$@k$. Using the P$@k$ we can calculate the average precision (AP$@n$) for a number $n$ of matches, as shown in Equation \ref{eq:apn}. The final AP is the value of AP$@n$ when all the ground truth positives have been obtained.


\begin{equation}
\label{eq:apn}
    AP@n = \frac{1}{GTP} \sum_{k=1}^{n}P@k * rel@k.
\end{equation}

\noindent where:
\begin{conditions}
GTP & total number of ground truth positives (correct samples of the query class), \\
$n$ & total number of candidate matches ($n$ elements most similar to the query), \\
rel@k  & relevance function that equals 1 if the ID at rank $k$ is correct. \\
\end{conditions}

Subsequently, the mean of all AP values across all queries, known as mAP, is calculated, as shown in Equation \ref{eq:map}. This metric takes into account both the precision and recall of an algorithm, offering a more thorough evaluation.

\begin{equation}
\label{eq:map}
    mAP = \frac{1}{N}\sum_{i=1}^{N} AP_i.
\end{equation}

\noindent where:
\begin{conditions}
N & total number of queries, \\
i & index for the i-th query, 
\end{conditions}


\section*{Data Records}
\label{sec:data-records}

The dataset is composed of a collection of videos, as provided by the set of cameras in the environment, and the corresponding Re-ID labels. The labels comprise the bounding box of the individuals present in each frame of the video and the corresponding ID. In the following Subsections, we will delve in the components of the dataset and the organization details.


\subsection*{Dataset Components}
\label{sec:dataset_components}

The dataset consists of 10 sequences, each containing seven videos recorded by the seven cameras deployed in the environment. The videos were collected over a period of seven months, allowing the dataset to capture variations in both the environment and the appearance of individuals. The average duration of a single video is 284 seconds, with a range from 113 to 567 seconds. In total, the dataset comprises more than 5.52 hours of video and 596,345 frames, with a total of 963,554 annotated bounding boxes. The details of each sequence are presented in Table \ref{tab:seq}. The videos are stored in AVI format, encoded using DivX MPEG-4 at 30 fps with a bitrate of 4048 Kbps.

\begin{table}[!htb]
    \centering
    \begin{tabular}{|l|p{6cm}|l|l|l|}
    \hline
    Sequence ID & Subject IDs & Duration (s) & Avg. \# Frames & \# Bboxes \\ \hline
    0           &  1, 3, 5, 6, 9, 10, 18, 19                   & 298   &  8,913   &  77,510  \\ \hline
    1           &  1, 3, 5, 6, 8, 9, 10, 18, 19                & 131   &  3,894   &  50,374  \\ \hline
    2           &  1, 3, 5, 6, 8, 9, 10, 18, 19                & 113   &  3,351   &  43,058  \\ \hline
    4           &  1, 2, 3, 4, 5, 6, 7, 8                      & 286   &  8,565   &  82,430  \\ \hline
    6           &  1, 2, 3, 4, 5, 9, 24                         & 205   &  6,237   &  55,075  \\ \hline
    7           &  1, 2, 3, 4, 5, 9                            & 250   &  7,504   &  66,451  \\ \hline
    20          &  1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 15, 16 & 463   &  13,903  &  203,143 \\ \hline
    24          &  2, 5, 6, 7, 10, 11, 14, 25, 26              & 567   &  16,988  &  214,792 \\ \hline
    25          &  2, 6, 7, 9, 10, 12, 14                      & 270   &  8,084   &  69,432  \\ \hline
    26          &  1, 2, 3, 5, 6, 7, 9, 10, 11, 14             & 258   &  7,749   &  101,289 \\ \hline
    \end{tabular}
    \caption{Data from the sequences of CHIRLA Dataset.}
    \label{tab:seq}
\end{table}

The dataset includes 22 different individuals, 17 males and five females, ranging in age from 20 to 60 years. As expected, each ID has distinct physical features, and their appearance may vary across different sequences. However, it is worth noting that not all individuals appear in every sequence, nor are they necessarily visible in all cameras.

In addition to the video data, we provide Re-ID labels.  As mentioned earlier, each video is accompanied by a label file that contains, for each frame, the bounding boxes of all detected individuals along with their corresponding IDs. The labels are stored in JSON format, making them easy to parse.
Each JSON file consists of a dictionary where the keys represent the frame numbers of the video, and the corresponding values are lists. Each element in these lists is a dictionary containing two keys: \texttt{"id"}, which stores the unique ID assigned to each individual, and \texttt{"BboxP"}, which contains a list of the bounding box coordinates \([X_1, Y_1, X_2, Y_2]\) that define the region occupied by the person in that frame. Frame and ID values are 1-based, whereas bounding box coordinates are 0-based. Specifically, the top-left corner of the bounding box corresponds to $(0,0)$. Figure \ref{fig:labels_sample} presents examples of labeled individuals in randomly selected frames from the dataset.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.32\linewidth, height=3.5cm]{lab_1.png}
    \includegraphics[width=0.32\linewidth, height=3.5cm]{lab_2.png}
    \includegraphics[width=0.32\linewidth, height=3.5cm]{lab_3.png}
    %\includegraphics[width=0.24\linewidth, height=2.5cm]{lab_4.png}
    \caption{Random frames with the labels superimposed.}
    \label{fig:labels_sample}
\end{figure}

Finally, the dataset is publicly available in the Science Data Bank data repository and can be accessed following the link \url{https://doi.org/10.57760/sciencedb.20543}.


\subsection*{Organization}
 % Dataset Structure

The dataset is organized into two primary directories: \texttt{annotations} and \texttt{videos}. Each directory contains 10 subdirectories corresponding to the individual sequences listed in Table \ref{tab:seq}. These sequences are labeled as \texttt{seq\_XXX}, where \texttt{XXX} represents a three-digit, zero-padded sequence number (e.g., \texttt{seq\_026}). Within each sequence subdirectory, the data is further divided by camera views, with seven files provided per sequence, one for each camera view. Each file is named based on the camera number, as well as the date and time when the videos were recorded.

The \texttt{videos} directory contains the video files specific to each camera view and sequence. Each sequence subdirectory includes seven video files, one for each camera view, stored in AVI format (e.g., \texttt{camera\_1\_2023-12-05-12:12:13.avi}). 
The \texttt{annotations} directory contains the corresponding annotation files in JSON format. Each sequence includes seven JSON files, one per camera view. Each file provides frame-wise annotations for the corresponding camera view within the sequence, including bounding box coordinates and object IDs.


\section*{Technical Validation}
\label{sec:tech-val}

In this Section, we present well-defined benchmarks, to rigorously tested our dataset to ensure it meets the necessary requirements for challenging and realistic scenarios in person tracking and Re-ID systems. 
We present a series of experiments designed to evaluate the robustness of the algorithms in diverse settings. 

\subsection*{Benchmark}
\label{sec:benchmark}

Here, we describe the proposed tests designed to evaluate both the tracking and Re-ID systems, as well as those that integrate both functionalities. Each system will be individually assessed against the two benchmarks defined in the following Subsections.

\subsubsection*{Benchmark for Person Tracking}
\label{sec:benchmark-tracking}

These tests assess the ability of a system to detect a person in the scene, create an ID for them, and track them on a single camera until the person disappears from the scene for a few seconds. 
For this purpose, the video sequences are divided into a training set and a test set. The training set is used to optimize certain parameters of the tracking algorithm that make it applicable to this setting, and adapt them to the resolution and the distance at which persons are seen in the scene. On the other hand, the test set will be used to evaluate the performance of the tracking systems, making use of the MOTA, IDF1 and their auxiliary metrics, as well as the generalization capacity of these systems after having optimized their parameters in videos from training sequences only.

With the proposed training and test sets, we check the capabilities of the tracking system in the following scenarios:

\begin{itemize}
    \item \textbf{Tracking after brief occlusions}. The person is totally hidden by an object or another person for a short period of time and becomes visible again without having left the scene. The objective of this scenario is to assess the ability of the system to maintain the ID and tracking of a person after a brief occlusion. 
    \item \textbf{Tracking in scenarios with multiple people and occlusions}. The scene includes several people, some of whom cross each other or are hidden, creating frequent occlusions. The objective of this scenario is to evaluate the system's ability to track multiple people simultaneously, even in situations where occlusions arise from their interactions.

In the analyzed scenarios, occlusions are treated as total occlusions, meaning no detections of the analyzed person are available in the occluded frames.
\end{itemize}

\subsubsection*{Benchmark for Re-ID}
\label{sec:benchmark-reidentification}

These benchmarks evaluate the ability of a system to re-identify a person in different situations that affect the continuity of their identification on one or more cameras. The objective is to determine the system's ability to recognize a person who has left the field of view or whose appearance has changed, based on comparison with previous images or sequences.

For this purpose, the video sequences are divided into training and test sets. The training sets are used to optimize the parameters of the Re-ID algorithm, adapting it to the visual characteristics of the recorded persons, so that it learns to recognize their ID, especially focusing on the characteristics that allow them to be identified in the long term. 

On the other hand, the test sets are designed to evaluate the performance of the system in different Re-ID scenarios, using metrics such as mAP, CMC and other auxiliary metrics that allow measuring its generalization capacity after the optimization of parameters in sequences other than training ones.

With the proposed training and test sets, we check the capabilities of the Re-ID system in the following scenarios:

\begin{itemize}

    \item \textbf{Re-identification after reappearance on the scene}. The person leaves the field of view of a camera and returns to the scene after a time interval. This reappearance can also include a change in the person appearance (e.g., takes off or puts on a jacket, wears a hat or changes an accessory). %, without having changed his or her appearance. 
    The objective of this scenario is to assess the ability of the system to re-identify the person correctly after a break in visibility.
    \item \textbf{Long-term re-identification}. The person is seen in different recordings captured on different days, implying possible changes in appearance and environment. The objective of this scenario is to evaluate the system's ability to recognize a person when recordings come from different days, with potential variations in lighting, appearance and other contextual factors.
    \item \textbf{Multi-camera re-identification}. The person is captured by several cameras at different locations, with little or no overlap between the fields of view of the cameras. 
    The objective of this scenario is to assess the system's ability to re-identify the person when viewed from different angles and perspectives.
    \item \textbf{Multi-camera Long-term re-identification.} The person is captured by several cameras at different locations and days. The objective of this scenario is to evaluate the system's robustness in re-identifying the person from various angles and perspectives, ensuring reliable performance under diverse temporal and contextual conditions.
\end{itemize}


\section*{Train and Test Data}

This Section provides a comprehensive overview of the training and test datasets defined for each tracking and Re-ID benchmark scenario. It details the data format, organizational structure, and the methodology employed for data generation and annotation. 

\subsection*{Tracking Data}

For the tracking benchmark, the train set consists of two sequences, while the test set includes eight sequences. This distribution remains consistent across the two tracking scenarios defined: 


\begin{itemize}
    \item {\textbf{Tracking after brief occlusions.}}
Brief occlusions are defined as interruptions lasting between one and five seconds. For this scenario, only the frames immediately before and after the occlusion starts and ends are included for each ID, along with their corresponding bounding box coordinates.

The data is structured in a hierarchical JSON format. Each JSON file consists of a dictionary where the keys represent individual IDs in the video, and the values are lists of annotations. Each annotation is represented as a dictionary containing four keys:
\begin{itemize}
    \item \texttt{"frame\_init"}: The frame number immediately before the occlusion starts.
    \item \texttt{"frame\_end"}: The frame number immediately after the occlusion ends.
    \item \texttt{"bbox\_init"}: A list of coordinates \([X1, Y1, X2, Y2]\) defining the bounding box of the person in the frame before the occlusion starts.
    \item \texttt{"bbox\_end"}: A list of coordinates \([X1, Y1, X2, Y2]\) defining the bounding box of the person in the frame after the occlusion ends.
\end{itemize}

\item {\textbf{Tracking in scenarios with multiple people and occlusions.}}
This scenario extends the conditions of the brief occlusions scenario by requiring that the occluded ID has an intersection with another ID before the occlusion begins. An IoU threshold of 0.3 is applied to ensure sufficient overlap between bounding boxes before the ID is occluded. The data for multiple occlusions is represented using the same JSON format as in the brief occlusions scenario.
\end{itemize}

\subsection*{Re-Identification Data}
\label{subsec:data-reid}

Person Re-ID experiments focus on evaluating frames following long occlusions, which are defined as lasting more than five seconds. This distinction ensures a clear differentiation from tracking scenarios that involve shorter occlusions.

Across the four scenarios defined in the Re-ID benchmark, the training sets include the first visible frame of each ID, followed by four consecutive frames sampled at an 18-frame gap (equivalent to 2.4 seconds of visibility). In contrast, the test sets comprise the reappearance frames and the four subsequent frames for each reappearance, using the same sampling strategy as the training sets. However, a minimum of 20 frames per ID is required in the test sets. If fewer than 20 frames are available, additional frames are randomly sampled to meet the requirement.

In addition, IDs present in the dataset are assigned negative values in the test sets when they meet the long-term occlusion conditions but are not included in the training sets. This labeling explicitly defines them as unknown IDs, providing an additional challenge to the Re-ID system.

All data is organized in a hierarchical JSON structure. Each JSON file consists of a dictionary where the keys represent individual IDs within the video, and the values are lists of annotations. Each annotation is represented as a dictionary with the following two keys:
\begin{itemize}
    \item \texttt{"frame"}: The frame number where the person is present in the video.
    \item \texttt{"BboxP"}: A list of coordinates \([X1, Y1, X2, Y2]\) representing the bounding box of the person in that frame, where \([X1, Y1]\) denotes the top-left corner, and \([X2, Y2]\) denotes the bottom-right corner.
\end{itemize}

For each benchmark scenario, subsets of training and test data are provided. The final evaluation of benchmark results is computed as the average of the results from individual subsets.
The specific details of the training and test data for each Re-ID scenario are outlined below. In all cases, the videos selected for evaluation were carefully analyzed to ensure they align with the benchmark definitions for each Re-ID task, with an emphasis on presenting challenging cases. 
Figure \ref{fig:train_test} illustrates sample data for one of the Re-ID scenario benchmarks. 

\begin{itemize}
\item \textbf{Re-identification after reappearance on the scene.}
This scenario includes 10 train and test set splits. The training set consists of annotations derived from 10 videos, with each subset containing one video per sequence. However, the selection was made from nine distinct sequences, as sequences that did not meet the long occlusion conditions were excluded. To complete the set, one sequence was repeated, sampling two different videos.
If uninterrupted frames were not available within the analyzed duration (2.4 seconds), only the frames within the specified range were included. For each video analyzed in the training set, the corresponding test set includes all frames from the same video, where the individual reappears following a long occlusion.

\item \textbf{Long-term re-Identification.}
The train and test sets consist of seven subsets. The training set includes annotations from one video per camera, resulting in a total of seven videos, with one video in each subset. The corresponding test subset comprises all remaining videos from the same camera analyzed in the training set, across different sequences. 

\item \textbf{Multi-Camera re-Identification.}
In this scenario, data is distributed across 10 training and test subsets. The training set consists of annotations from one video per sequence, resulting in 10 videos in total, with one video in each subset. The corresponding test subset includes all remaining videos within each analyzed sequence.

\item \textbf{Multi-Camera Long-term re-Identification.}
In this case we also provide 10 subsets for training and testing. The training set consists of annotations from one video per sequence, resulting in 10 videos in total, with one video in each subset. Each test subset includes two videos from one or two sequences, which are always distinct from those used in the corresponding training subset.

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[height=5.2cm]{train.png}
        \caption{Train data samples}
        \label{fig:train_samples}
    \end{subfigure}
    \hspace{0.04\textwidth} % Slight overlap to minimize space
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[height=5.2cm]{test.png}
        \caption{Test data samples}
        \label{fig:test_samples}
    \end{subfigure}
    \caption{Samples of train and test data for two IDs in the long-term Re-ID benchmark. The first row represents ID "2" from the CHIRLA dataset, captured by camera 5 at different time intervals. The second row represents ID "1", captured by camera 4 over time. These samples demonstrate the variations in clothing and appearance of individuals across different time periods.}
    \label{fig:train_test}
\end{figure}
  
\end{itemize}


\subsection*{Organization}

The benchmark data is organized into two main directories: \texttt{tracking} and \texttt{reid}. Each directory contains subdirectories corresponding to the different scenarios, each further divided into training and test sets.

\begin{itemize}
    \item \textbf{Tracking Directory Structure}
    
The \texttt{tracking} directory includes two subdirectories, one for each tracking benchmark scenario: \texttt{brief\_occlusions} and \texttt{multiple\_people\_occlusions}. Each scenario is divided into \texttt{train} and \texttt{test} subdirectories, organized by sequence name (e.g., \texttt{seq\_004}). 
Within each sequence subdirectory, the data is further structured by camera views. For each sequence, seven JSON files are provided, one per camera view, labeled with the camera name and metadata (e.g., \texttt{camera\_7\_2023-06-08-12:29:08.json}). Alongside the JSON files, there is an \texttt{imgs} directory, which contains seven subdirectories, one for each camera view. These subdirectories share the same names as the corresponding JSON files.

Inside each camera view subdirectory, the data is organized into subfolders named after the IDs present in the corresponding video. Each ID subfolder contains cropped bounding box images for that ID, selected based on the defined tracking scenario. The images are stored in PNG format and are named as \texttt{frame\_n.png}, where \texttt{n} corresponds to the frame number in the original video.

    \item \textbf{Re-identification Directory Structure}

The \texttt{reid} directory includes four scenarios: \texttt{reappearance}, \texttt{long\_term}, \texttt{multi\_camera}, and \texttt{multi\_camera} \texttt{\_long\_term}. Each scenario has its own \texttt{train} and \texttt{test} subdirectories. 
Within each \texttt{train} and \texttt{test} directory, there are subset directories named as \texttt{train\_X} or \texttt{test\_X}, where \texttt{X} is the subset index (e.g., \texttt{train\_0}, \texttt{test\_1}). The \texttt{long\_term} scenario contains seven subsets in both the training and test sets, while \texttt{reappearance}, \texttt{multi\_camera}, and \texttt{multi\_camera\_long\_term} contain 10 subsets for training and testing.

Inside these directories, the structure follows the same format as the one outlined for the tracking benchmark, including sequences, JSON files, and \texttt{imgs} directories. The number of data points within each train and test set depends on the data splitting methodology described in Section Re-Identification Data.

\end{itemize}


\section*{Usage Notes}

The CHIRLA dataset provides a comprehensive resource for the development and evaluation of person Re-ID algorithms in challenging, long-term scenarios. It enables testing the capability of models to identify persons across different cameras, times, and conditions, including variations in clothing and appearance.
Although Re-ID is the primary focus, the dataset can also be used for tracking algorithms. It allows for the evaluation of an algorithm's ability to detect, identify, and maintain consistent IDs within single-camera settings. Additionally, the dataset supports scenarios such as tracking through occlusions and multi-person interactions.


The dataset is publicly available and data collection adhered to ethical guidelines, with informed consent obtained from all participants. 


\section*{Code Availability}
\label{sec:code-availability}

The information for downloading the dataset and the software to be used for the evaluation of the systems on this dataset can be found in the following GitHub repository: \url{https://github.com/bdager/CHIRLA}.

\bibliography{sample}


\section*{Acknowledgements}

This paper is part of the grant PID2022-138453OB-I00 funded by MICIU/AEI/10.13039/501100011033 and by “ERDF A way of making Europe”. Bessie Dominguez-Dager has been funded by the grant  FPU23/02247 from Ministerio de Ciencia, Innovacion y Universidades of Spain.


\section*{Author contributions statement}

B. D.: Software, Validation, Methodology, Data Curation, Writing. F. E.: Software, Formal analysis, Data Curation, Methodology. F. G.: Conceptualization, Writing - Original Draft, Supervision. M. C.: Resources, Writing - Review \& Editing, Funding acquisition. 


\section*{Competing interests} 

The authors declare no known competing financial interests.


\end{document}