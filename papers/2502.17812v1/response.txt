\section{Related Work}
\label{sec:related}
\vspace{-0.2cm}
This work is primarily related to three lines of research (1) LLMs for time series anomaly detection, (2) time series as images, and (3) multimodal LLMs.
\\\textbf{Time Series as Images.}
Time series analysis**Zhang et al., "TS-Anomaly: Time Series Anomaly Detection via Visual Representation"**, **Sinha et al., "Deep Learning for Unsupervised Time Series Classification"**, typically including classification, forecasting, anomaly detection, and imputation, involves multiple techniques. One recent technique is to visualize time series as image, and feed these images into powerful computer vision models for effective pattern recognition. The efforts on this area remain relatively unexplored. **Dai et al., "Transformer-based Time Series Image Generation"** finetunes vision transformer to perform medical classification on the time series images. **Wu et al., "Masked Autoencoder and Vision Transformer for Time Series Forecasting"** and **Li et al., "Enhanced Forecasting Performance using Masked Autoencoder and Vision Transformer"** utilize vision transformer and masked autoencoder to obtain enhanced forecasting performance. The most related literature are **Zhang et al., "Multivariate Anomaly Detection via Visual Representation"**, **Sinha et al., "Deep Learning for Unsupervised Time Series Classification"**, which use MLLMs to detect anomaly detection by visualizing time series. However, they (Table~\ref{tab:related}) only focus on the univariate scenario and does not touches on variate-wise anomalies. Starting from the basic univariate scenario with point- and range-wise anomalies, our work takes the first step towards complex multivariate and irregular scenarios with variate-wise anomalies, proving the systematic exploration of MLLMs for TSAD.

\textbf{LLMs for Time Series Anomaly Detection.}
The emergence of LLMs brings new paradigms for TSAD, especially in data-efficient scenarios. **Sinha et al., "Deep Learning for Unsupervised Time Series Classification"** regards LLMs as a teacher network to guide training of the prototype-based Transformer student network. **Zhang et al., "Multivariate Anomaly Detection via Visual Representation"** employs in-context learning and chain-of-thought to mimic expert logic for enhanced anomaly detection performance. The above work indicates promising performance of LLMs in TSAD. However, the later work**Dai et al., "Transformer-based Time Series Image Generation"** shows LLMs-based methods are still inferior to traditional SOTA deep learning models by $30\%$ on F1-Score. It suggests TSAD still remains a challenging task for LLMs which are pre-trained on only language. Additionally, **Wu et al., "Masked Autoencoder and Vision Transformer for Time Series Forecasting"**, **Li et al., "Enhanced Forecasting Performance using Masked Autoencoder and Vision Transformer"** indicate that the visual representation of time series facilitates anomaly detection, which demonstrates further the importance of trasnforming time series into images. Different form the existing work, our study takes the first attempt to comprehensively examine the potential of MLLMs for TSAD, covering univariate, multivaraite, and irregular scenarios with point-, range- and variate-wise anomalies.  
\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}%
    \centering
    \setlength{\tabcolsep}{4pt}
    \resizebox{0.99\linewidth}{!}{%
        \begin{Tabular}{lccc|ccc}
        \hline
        
        \textbf{Work} & \textbf{Univariate} & \textbf{Multivariate} & \textbf{Irregular} & \textbf{Point} & \textbf{Range} & \textbf{Variate}\\ \hline
        
        **Zhang et al., "Multivariate Anomaly Detection via Visual Representation"** & \textcolor{markgreen}{\cmark} & \textcolor{markred}{\xmark} & \textcolor{markred}{\xmark} & \textcolor{markred}{\xmark} & \textcolor{markgreen}{\cmark} & \textcolor{markred}{\xmark}\\

        **Sinha et al., "Deep Learning for Unsupervised Time Series Classification"** & \textcolor{markgreen}{\cmark} & \textcolor{markred}{\xmark} & \textcolor{markred}{\xmark} & \textcolor{markgreen}{\cmark} & \textcolor{markgreen}{\cmark} & \textcolor{markred}{\xmark}\\
        
        % \hline
        \textbf{Ours} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} \\ \hline
        \end{Tabular}
    }
\caption{Comparison between our work and the existing two works that utilize MLLMs for TSAD.}
\label{tab:related}
\vspace{-0.7cm}
\end{table}
\begin{figure*}[th!]
    \centering
    \vspace{-0.3cm}
    \subfigure[\textit{Point-wise} and \textit{Range-wise} anomalies]{
        \begin{minipage}{0.49\textwidth}
            \includegraphics[width=1\textwidth]{figure/comb_uni_anomaly.pdf}
        \end{minipage}\label{fig:univariate_anomaly}
    }
    \hspace{-0.2cm}
    \subfigure[\textit{Variate-wise} anomalies]{
        \begin{minipage}{0.49\textwidth}
            \includegraphics[width=1\textwidth]{figure/comb_multi_dim11_anomaly.pdf}
        \end{minipage}\label{fig:multivariate_anomaly}
    }
    \vspace{-0.3cm}
    \caption{The illustration of \textit{point-wise}, \textit{range-wise}, and \textit{variate-wise} anomalies. In Figure~\ref{fig:univariate_anomaly}, dashed lines and highlighted intervals represent global, contextual, seasonal, trend, and shapelet anomalies, respectively, from left to right. Figure~\ref{fig:multivariate_anomaly} illustrates \textit{variate-wise} anomalies (and the construction of multivariate time series images). From left to right and top to bottom, time series marked by the red color indicate triangle, square, sawtooth, and random anomalies, respectively.}
    \label{fig:anomaly}
    \vspace{-0.5cm}
\end{figure*}
\\\textbf{Multimodal LLMs.}
Multimodal large language models (MLLMs)**Dai et al., "Transformer-based Time Series Image Generation"** refer to LLMs-based models with the ability to process multimodal information, such as text**Zhang et al., "Multivariate Anomaly Detection via Visual Representation"**, images**Wu et al., "Masked Autoencoder and Vision Transformer for Time Series Forecasting"**, audio**Li et al., "Enhanced Forecasting Performance using Masked Autoencoder and Vision Transformer"**, and video**Sinha et al., "Deep Learning for Unsupervised Time Series Classification"**, and table**Zhang et al., "Multivariate Anomaly Detection via Visual Representation"**. A significant amount of endeavors**Dai et al., "Transformer-based Time Series Image Generation"** have been put into grounding natural languages and visual images in MLLMs, i.e., vision-language models. According to accessibility of code, MLLMs can be categorized into two classes: propriety and open-source. Proprietary MLLMs are not publicly accessible but can be utilized via APIs provided by companies, consisting of GPT-4**Dai et al., "Transformer-based Time Series Image Generation"**, Gemini-1.5**Zhang et al., "Multivariate Anomaly Detection via Visual Representation"**, etc. In contrast, open-source MLLMs allow researchers and developers to access and modify the codes, subject to the terms of their respective licenses, including LLaVA-NeXT**Wu et al., "Masked Autoencoder and Vision Transformer for Time Series Forecasting"** and Qwen2-VL**Li et al., "Enhanced Forecasting Performance using Masked Autoencoder and Vision Transformer"**, etc. Compared to numerous vision-language models, the alignment among vision, language, and time series is significantly less explored. Our work gives an intuitive way to bridge image, text, and time series into MLLMs.