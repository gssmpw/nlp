\section{Related Work}
\label{sec:related}
\vspace{-0.2cm}
This work is primarily related to three lines of research (1) LLMs for time series anomaly detection, (2) time series as images, and (3) multimodal LLMs.
\\\textbf{Time Series as Images.}
Time series analysis~\cite{lee2024z,xu2024sst,du2024tsi,huang2024graph}, typically including classification, forecasting, anomaly detection, and imputation, involves multiple techniques. One recent technique is to visualize time series as image, and feed these images into powerful computer vision models for effective pattern recognition. The efforts on this area remain relatively unexplored. \cite{li2024time} finetunes vision transformer to perform medical classification on the time series images. \cite{yang2024vitime} and \cite{chen2024visionts} utilizes vision transformer and masked autoencoder to obtain enhanced forecasting performance. The most related literature are \cite{zhuang2024see} and \cite{zhou2024can} which use MLLMs to detect anomaly detection by visualizing time series. However, they (Table~\ref{tab:related}) only focus on the univariate scenario and does not touches on variate-wise anomalies. Starting from the basic univariate scenario with point- and range-wise anomalies, our work takes the first step towards complex multivariate and irregular scenarios with variate-wise anomalies, proving the systematic exploration of MLLMs for TSAD.

\textbf{LLMs for Time Series Anomaly Detection.}
The emergence of LLMs brings new paradigms for TSAD, especially in data-efficient scenarios. \cite{liu2024largekd} regards LLMs as a teacher network to guide training of the prototype-based Transformer student network. \cite{liu2024large,dong2024can} employs in-context learning and chain-of-thought to mimic expert logic for enhanced anomaly detection performance. The above work indicates promising performance of LLMs in TSAD. However, the later work~\cite{alnegheimish2024large} shows LLMs-based methods are still inferior to traditional SOTA deep learning models by $30\%$ on F1-Score. It suggests TSAD still remains a challenging task for LLMs which are pre-trained on only language. Additionally, \cite{zhou2024can} and \cite{dong2024can} indicate that the visual representation of time series facilitates anomaly detection, which demonstrates further the importance of trasnforming time series into images. Different form the existing work, our study takes the first attempt to comprehensively examine the potential of MLLMs for TSAD, covering univariate, multivaraite, and irregular scenarios with point-, range- and variate-wise anomalies.  
\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}%
    \centering
    \setlength{\tabcolsep}{4pt}
    \resizebox{0.99\linewidth}{!}{%
        \begin{Tabular}{lccc|ccc}
        \hline
        
        \textbf{Work} & \textbf{Univariate} & \textbf{Multivariate} & \textbf{Irregular} & \textbf{Point} & \textbf{Range} & \textbf{Variate}\\ \hline
        
        \cite{zhou2024can} & \textcolor{markgreen}{\cmark} & \textcolor{markred}{\xmark} & \textcolor{markred}{\xmark} & \textcolor{markred}{\xmark} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markred}{\xmark}\\

        \cite{zhuang2024see} & \textcolor{markgreen}{\cmark} & \textcolor{markred}{\xmark} & \textcolor{markred}{\xmark} & \textcolor{markgreen}{\cmark} & \textcolor{markgreen}{\cmark} & \textcolor{markred}{\xmark}\\
        
        % \hline
        \textbf{Ours} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} & \textcolor{markgreen}{\textbf{\cmark}} \\ \hline
        \end{Tabular}
    }
\caption{Comparison between our work and the existing two works that utilize MLLMs for TSAD.}
\label{tab:related}
\vspace{-0.7cm}
\end{table}
\begin{figure*}[th!]
    \centering
    \vspace{-0.3cm}
    \subfigure[\textit{Point-wise} and \textit{Range-wise} anomalies]{
        \begin{minipage}{0.49\textwidth}
            \includegraphics[width=1\textwidth]{figure/comb_uni_anomaly.pdf}
        \end{minipage}\label{fig:univariate_anomaly}
    }
    \hspace{-0.2cm}
    \subfigure[\textit{Variate-wise} anomalies]{
        \begin{minipage}{0.49\textwidth}
            \includegraphics[width=1\textwidth]{figure/comb_multi_dim11_anomaly.pdf}
        \end{minipage}\label{fig:multivariate_anomaly}
    }
    \vspace{-0.3cm}
    \caption{The illustration of \textit{point-wise}, \textit{range-wise}, and \textit{variate-wise} anomalies. In Figure~\ref{fig:univariate_anomaly}, dashed lines and highlighted intervals represent global, contextual, seasonal, trend, and shapelet anomalies, respectively, from left to right. Figure~\ref{fig:multivariate_anomaly} illustrates \textit{variate-wise} anomalies (and the construction of multivariate time series images). From left to right and top to bottom, time series marked by the red color indicate triangle, square, sawtooth, and random anomalies, respectively.}
    \label{fig:anomaly}
    \vspace{-0.5cm}
\end{figure*}
\\\textbf{Multimodal LLMs.}
Multimodal large language models (MLLMs)~\cite{yin2023survey} refer to LLMs-based models with the ability to process multimodal information, such as text~\cite{liang2024taxonomy,huang2024can}, images~\cite{liu2024visual,hu2024bliva}, audio~\cite{deshmukh2023pengi,zhang2023speechgpt}, and video~\cite{he2024ma,fu2024video}, and table~\cite{sui2024table,wang2024piecing}. A significant amount of endeavors~\cite{hilal2022financial,black2024pi_0} have been put into grounding natural languages and visual images in MLLMs, i.e., vision-language models. According to accessibility of code, MLLMs can be categorized into two classes: propriety and open-source. Proprietary MLLMs are not publicly accessible but can be utilized via APIs provided by companies, consisting of GPT-4~\cite{achiam2023gpt}, Gemini-1.5~\cite{team2024gemini}, etc. In contrast, open-source MLLMs allow researchers and developers to access and modify the codes, subject to the terms of their respective licenses, including LLaVA-NeXT~\cite{li2024llavanext-strong} and Qwen2-VL~\cite{wang2024qwen2}, etc. Compared to numerous vision-language models, the alignment among vision, language, and time series is significantly less explored. Our work gives an intuitive way to bridge image, text, and time series into MLLMs.