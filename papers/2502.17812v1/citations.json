[
  {
    "index": 0,
    "papers": [
      {
        "key": "lee2024z",
        "author": "Lee, Zed and Lindgren, Tony and Papapetrou, Panagiotis",
        "title": "Z-Time: efficient and effective interpretable multivariate time series classification"
      },
      {
        "key": "xu2024sst",
        "author": "Xu, Xiongxiao and Chen, Canyu and Liang, Yueqing and Huang, Baixiang and Bai, Guangji and Zhao, Liang and Shu, Kai",
        "title": "SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting"
      },
      {
        "key": "du2024tsi",
        "author": "Du, Wenjie and Wang, Jun and Qian, Linglong and Yang, Yiyuan and Ibrahim, Zina and Liu, Fanxing and Wang, Zepu and Liu, Haoxin and Zhao, Zhiyuan and Zhou, Yingjie and others",
        "title": "Tsi-bench: Benchmarking time series imputation"
      },
      {
        "key": "huang2024graph",
        "author": "Huang, Xiaoyu and Chen, Weidong and Hu, Bo and Mao, Zhendong",
        "title": "Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2024time",
        "author": "Li, Zekun and Li, Shiyang and Yan, Xifeng",
        "title": "Time series as images: Vision transformer for irregularly sampled time series"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yang2024vitime",
        "author": "Yang, Luoxiao and Wang, Yun and Fan, Xinqi and Cohen, Israel and Chen, Jingdong and Zhao, Yue and Zhang, Zijun",
        "title": "Vitime: A visual intelligence-based foundation model for time series forecasting"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chen2024visionts",
        "author": "Chen, Mouxiang and Shen, Lefei and Li, Zhuo and Wang, Xiaoyun Joy and Sun, Jianling and Liu, Chenghao",
        "title": "Visionts: Visual masked autoencoders are free-lunch zero-shot time series forecasters"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhuang2024see",
        "author": "Zhuang, Jiaxin and Yan, Leon and Zhang, Zhenwei and Wang, Ruiqi and Zhang, Jiawei and Gu, Yuantao",
        "title": "See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhou2024can",
        "author": "Zhou, Zihao and Yu, Rose",
        "title": "Can LLMs Understand Time Series Anomalies?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2024largekd",
        "author": "Liu, Chen and He, Shibo and Zhou, Qihang and Li, Shizhong and Meng, Wenchao",
        "title": "Large language model guided knowledge distillation for time series anomaly detection"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024large",
        "author": "Liu, Jun and Zhang, Chaoyun and Qian, Jiaxu and Ma, Minghua and Qin, Si and Bansal, Chetan and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei",
        "title": "Large Language Models can Deliver Accurate and Interpretable Time Series Anomaly Detection"
      },
      {
        "key": "dong2024can",
        "author": "Dong, Manqing and Huang, Hao and Cao, Longbing",
        "title": "Can LLMs Serve As Time Series Anomaly Detectors?"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "alnegheimish2024large",
        "author": "Alnegheimish, Sarah and Nguyen, Linh and Berti-Equille, Laure and Veeramachaneni, Kalyan",
        "title": "Large language models can be zero-shot anomaly detectors for time series?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhou2024can",
        "author": "Zhou, Zihao and Yu, Rose",
        "title": "Can LLMs Understand Time Series Anomalies?"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "dong2024can",
        "author": "Dong, Manqing and Huang, Hao and Cao, Longbing",
        "title": "Can LLMs Serve As Time Series Anomaly Detectors?"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhou2024can",
        "author": "Zhou, Zihao and Yu, Rose",
        "title": "Can LLMs Understand Time Series Anomalies?"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhuang2024see",
        "author": "Zhuang, Jiaxin and Yan, Leon and Zhang, Zhenwei and Wang, Ruiqi and Zhang, Jiawei and Gu, Yuantao",
        "title": "See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yin2023survey",
        "author": "Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong",
        "title": "A survey on multimodal large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liang2024taxonomy",
        "author": "Liang, Yueqing and Yang, Liangwei and Wang, Chen and Xu, Xiongxiao and Yu, Philip S and Shu, Kai",
        "title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs"
      },
      {
        "key": "huang2024can",
        "author": "Huang, Baixiang and Chen, Canyu and Xu, Xiongxiao and Payani, Ali and Shu, Kai",
        "title": "Can Knowledge Editing Really Correct Hallucinations?"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "hu2024bliva",
        "author": "Hu, Wenbo and Xu, Yifan and Li, Yi and Li, Weiyue and Chen, Zeyuan and Tu, Zhuowen",
        "title": "Bliva: A simple multimodal llm for better handling of text-rich visual questions"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "deshmukh2023pengi",
        "author": "Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming",
        "title": "Pengi: An audio language model for audio tasks"
      },
      {
        "key": "zhang2023speechgpt",
        "author": "Zhang, Dong and Li, Shimin and Zhang, Xin and Zhan, Jun and Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng",
        "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "he2024ma",
        "author": "He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam",
        "title": "Ma-lmm: Memory-augmented large multimodal model for long-term video understanding"
      },
      {
        "key": "fu2024video",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "sui2024table",
        "author": "Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei",
        "title": "Table meets llm: Can large language models understand structured table data? a benchmark and empirical study"
      },
      {
        "key": "wang2024piecing",
        "author": "Wang, Haoran and Rangapur, Aman and Xu, Xiongxiao and Liang, Yueqing and Gharwi, Haroon and Yang, Carl and Shu, Kai",
        "title": "Piecing It All Together: Verifying Multi-Hop Multimodal Claims"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "hilal2022financial",
        "author": "Hilal, Waleed and Gadsden, S Andrew and Yawney, John",
        "title": "Financial fraud: a review of anomaly detection techniques and recent advances"
      },
      {
        "key": "black2024pi_0",
        "author": "Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others",
        "title": "A Vision-Language-Action Flow Model for General Robot Control"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "team2024gemini",
        "author": "Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "li2024llavanext-strong",
        "author": "Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan",
        "title": "LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  }
]