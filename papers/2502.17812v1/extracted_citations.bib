@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{alnegheimish2024large,
  title={Large language models can be zero-shot anomaly detectors for time series?},
  author={Alnegheimish, Sarah and Nguyen, Linh and Berti-Equille, Laure and Veeramachaneni, Kalyan},
  journal={arXiv preprint arXiv:2405.14755},
  year={2024}
}

@article{black2024pi_0,
  title={A Vision-Language-Action Flow Model for General Robot Control},
  author={Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others},
  journal={arXiv preprint arXiv:2410.24164},
  year={2024}
}

@article{chen2024visionts,
  title={Visionts: Visual masked autoencoders are free-lunch zero-shot time series forecasters},
  author={Chen, Mouxiang and Shen, Lefei and Li, Zhuo and Wang, Xiaoyun Joy and Sun, Jianling and Liu, Chenghao},
  journal={arXiv preprint arXiv:2408.17253},
  year={2024}
}

@article{deshmukh2023pengi,
  title={Pengi: An audio language model for audio tasks},
  author={Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={18090--18108},
  year={2023}
}

@article{dong2024can,
  title={Can LLMs Serve As Time Series Anomaly Detectors?},
  author={Dong, Manqing and Huang, Hao and Cao, Longbing},
  journal={arXiv preprint arXiv:2408.03475},
  year={2024}
}

@article{du2024tsi,
  title={Tsi-bench: Benchmarking time series imputation},
  author={Du, Wenjie and Wang, Jun and Qian, Linglong and Yang, Yiyuan and Ibrahim, Zina and Liu, Fanxing and Wang, Zepu and Liu, Haoxin and Zhao, Zhiyuan and Zhou, Yingjie and others},
  journal={arXiv preprint arXiv:2406.12747},
  year={2024}
}

@article{fu2024video,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@inproceedings{he2024ma,
  title={Ma-lmm: Memory-augmented large multimodal model for long-term video understanding},
  author={He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13504--13514},
  year={2024}
}

@article{hilal2022financial,
  title={Financial fraud: a review of anomaly detection techniques and recent advances},
  author={Hilal, Waleed and Gadsden, S Andrew and Yawney, John},
  journal={Expert systems With applications},
  volume={193},
  pages={116429},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{hu2024bliva,
  title={Bliva: A simple multimodal llm for better handling of text-rich visual questions},
  author={Hu, Wenbo and Xu, Yifan and Li, Yi and Li, Weiyue and Chen, Zeyuan and Tu, Zhuowen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={3},
  pages={2256--2264},
  year={2024}
}

@article{huang2024can,
  title={Can Knowledge Editing Really Correct Hallucinations?},
  author={Huang, Baixiang and Chen, Canyu and Xu, Xiongxiao and Payani, Ali and Shu, Kai},
  journal={arXiv preprint arXiv:2410.16251},
  year={2024}
}

@article{huang2024graph,
  title={Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection},
  author={Huang, Xiaoyu and Chen, Weidong and Hu, Bo and Mao, Zhendong},
  journal={arXiv preprint arXiv:2412.19108},
  year={2024}
}

@article{lee2024z,
  title={Z-Time: efficient and effective interpretable multivariate time series classification},
  author={Lee, Zed and Lindgren, Tony and Papapetrou, Panagiotis},
  journal={Data mining and knowledge discovery},
  volume={38},
  number={1},
  pages={206--236},
  year={2024},
  publisher={Springer}
}

@misc{li2024llavanext-strong,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}

@article{li2024time,
  title={Time series as images: Vision transformer for irregularly sampled time series},
  author={Li, Zekun and Li, Shiyang and Yan, Xifeng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liang2024taxonomy,
  title={Taxonomy-Guided Zero-Shot Recommendations with LLMs},
  author={Liang, Yueqing and Yang, Liangwei and Wang, Chen and Xu, Xiongxiao and Yu, Philip S and Shu, Kai},
  journal={arXiv preprint arXiv:2406.14043},
  year={2024}
}

@article{liu2024large,
  title={Large Language Models can Deliver Accurate and Interpretable Time Series Anomaly Detection},
  author={Liu, Jun and Zhang, Chaoyun and Qian, Jiaxu and Ma, Minghua and Qin, Si and Bansal, Chetan and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2405.15370},
  year={2024}
}

@article{liu2024largekd,
  title={Large language model guided knowledge distillation for time series anomaly detection},
  author={Liu, Chen and He, Shibo and Zhou, Qihang and Li, Shizhong and Meng, Wenchao},
  journal={arXiv preprint arXiv:2401.15123},
  year={2024}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{sui2024table,
  title={Table meets llm: Can large language models understand structured table data? a benchmark and empirical study},
  author={Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
  booktitle={Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  pages={645--654},
  year={2024}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{wang2024piecing,
  title={Piecing It All Together: Verifying Multi-Hop Multimodal Claims},
  author={Wang, Haoran and Rangapur, Aman and Xu, Xiongxiao and Liang, Yueqing and Gharwi, Haroon and Yang, Carl and Shu, Kai},
  journal={arXiv preprint arXiv:2411.09547},
  year={2024}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{xu2024sst,
  title={SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting},
  author={Xu, Xiongxiao and Chen, Canyu and Liang, Yueqing and Huang, Baixiang and Bai, Guangji and Zhao, Liang and Shu, Kai},
  journal={arXiv preprint arXiv:2404.14757},
  year={2024}
}

@article{yang2024vitime,
  title={Vitime: A visual intelligence-based foundation model for time series forecasting},
  author={Yang, Luoxiao and Wang, Yun and Fan, Xinqi and Cohen, Israel and Chen, Jingdong and Zhao, Yue and Zhang, Zijun},
  journal={arXiv preprint arXiv:2407.07311},
  year={2024}
}

@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}

@article{zhang2023speechgpt,
  title={Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities},
  author={Zhang, Dong and Li, Shimin and Zhang, Xin and Zhan, Jun and Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2305.11000},
  year={2023}
}

@article{zhou2024can,
  title={Can LLMs Understand Time Series Anomalies?},
  author={Zhou, Zihao and Yu, Rose},
  journal={arXiv preprint arXiv:2410.05440},
  year={2024}
}

@article{zhuang2024see,
  title={See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers},
  author={Zhuang, Jiaxin and Yan, Leon and Zhang, Zhenwei and Wang, Ruiqi and Zhang, Jiawei and Gu, Yuantao},
  journal={arXiv preprint arXiv:2411.02465},
  year={2024}
}

