\section{Observation}
\subsection{Attention Temporal Patterns}
Our research is motivated by the observation that attention exhibits three distinct temporal patterns: re-access, sequential, and seasonal. 
As shown in \autoref{fig:attention_heatmap}, the re-access pattern occurs as vertical lines, which shows repeated attention to specific tokens. 
The sequential pattern is marked by diagonal lines, which show the attention moves sequentially to the next tokens.
Meanwhile, the seasonal pattern is characterized by the periodic recurrence of critical tokens, which appears in \autoref{fig:attention_heatmap} as alternating regions of high and uniform attention scores. 
These patterns are consistently observed across different models and datasets. 

\subsection{Predict Attention by Time Series Methods}
Capturing attention patterns enables accurate prediction of subsequent attention, suggesting that attention is inherently predictable.
Given that LLMs are autoregressive, the process of token generation can be naturally modeled as a time series. 
Consequently, we propose to model attention scores as a spatiotemporal sequence, where they form a time series along the inference dimension and a spatial sequence along the token dimension. 
This frames attention prediction as a time series forecasting task, leveraging temporal prediction techniques to forecast the next-step attention. 
In contrast, existing approaches rely on static attention modeling, limiting their ability to adapt to dynamic temporal variations.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth] %, height=0.5\linewidth
        {figures/observation/larger.png}
    \caption{Visualization of three temporal attention patterns. \textbf{Re-access}  shows repeated attention to specific tokens. \textbf{Sequential} shows attention progresses toward the next tokens. \textbf{Seasonal} exhibits periodic recurrence as alternating bands of high and uniform attention scores.}
    \vspace{-0.4cm}
    \label{fig:attention_heatmap}
\end{figure}


\subsection{Association Between Attention Patterns and Query}
To better understand why attention follows temporal patterns, we investigate the underlying causes. We find that the high continuity between queries plays a central role in shaping attention behavior. In particular, the query sequence exhibits a cosine autocorrelation of 87\% at a one-step lag (in App.~\ref{appendix:q_similarity}), which highlights its strong similarity. High query autocorrelation, as observed in our work, is also reported in \citet{lee2024infinigen}.
Based on this, we further analyze the correlation between attention in step $i$ and step $i+1$.
Focusing on the logits before $\text{Softmax}$, the attention computation at step $i$ is given as $A_i = \frac{1}{\sqrt{D}} \mathbf{Q_i} \mathbf{K_i}^\top = \frac{1}{\sqrt{D}} q_i k_{1:i}^\top$,
where $q_i, k_i \in \mathbb{R}^{D} $ represent the query and key vectors of the token $i$, respectively.
Assuming the query at step $i+1$ satisfies the relationship 
$q_{i+1} = q_i + \Delta q$ ,
the attention computation at step $i+1$ can then be expressed as:\vspace{-5pt}
\begin{equation}
\begin{aligned}
A_{i+1} &= \frac{1}{\sqrt{D}} \mathbf{Q_{i+1}} \mathbf{K_{i+1}}^\top  \\
&= \frac{1}{\sqrt{D}} q_{i+1} k_{1:i+1}^\top   \\
&=\frac{1}{\sqrt{D}} (q_{i}+\Delta q) k_{1:i+1}^\top   \\
&=\frac{1}{\sqrt{D}} (q_{i} k_{1:i+1}^\top + \Delta q k_{1:i+1}^\top) \\
\end{aligned}
\end{equation}
Focusing on the values of $A_{i+1}$ in the first $i$ positions,
\begin{equation}
\begin{aligned}
A_{i+1}[1:i] &=\frac{1}{\sqrt{D}} q_{i} k_{1:i}^\top + \frac{1}{\sqrt{D}}\Delta q k_{1:i}^\top \\
&= A_i + \Delta A \\
\end{aligned}
\end{equation}

Therefore the difference between $A_i$ and $A_{i+1}$ is dominated by $\Delta q$. Since $q_i$ and $q_{i+1}$ have high similarity, $\Delta q$ is relatively small. Consequently, $\Delta A$ is small and $A_i \approx A_{i+1}$. Therefore, the critical tokens of adjacent steps are similar, which is suitable for cross-token prefetching.



