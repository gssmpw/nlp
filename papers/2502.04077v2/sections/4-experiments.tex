\section{Experimental Results}
% 


\subsection{Settings}

\textbf{Tasks.}
We use the LongBench \citep{bai2024longbench} dataset for long context evaluation.
LongBench is a widely used benchmark for long-context LLM inference. It consists of 16 datasets covering 6 tasks, including single- and multi-document QA (SQA and MQA), summarization, few-shot learning, synthetic tasks, and code completion. LongBench uses different metrics for each dataset and calculates an average score to measure overall performance. See details in App. \ref{sec:app_longbench}.

We employ the GSM8K \citep{cobbe2021gsm8k} dataset, a mathematics dataset annotated with CoT (Chain-of-Thought) reasoning processes, for evaluating CoT tasks. To evaluate the performance of our method with varying prompt lengths, we adjust the few-shot number. The evaluation metric is the accuracy of the final computational results.

\textbf{Baselines.}
We select four sparse attention approaches as our baselines. These include three cache eviction methods, StreamingLLM \citep{xiao2024streamingllm}, H2O \citep{zhang2023h2o}, SnapKV \citep{li2024snapkv} and one cache retrieval method Quest \citep{tang2024quest}.
For a fair comparison, SnapKV's window size is set to 64, consistent with our setting.
Aside from this modification, we use the official implementations and default parameters of the methods for reproducing the experiments.
More details in App. \ref{sec:app_baselines}.

We choose two widely used long-context models for our evaluation: LongChat-v1.5-7b-32k \citep{kwon2023longchat} with 32K context length and LLaMA-3.1-8B-Instruct \citep{meta2024llama3.1} with 128K context length. 

\textbf{Implementation details.}
\emph{Predictor Preparation.}
\ours predictor consists of three convolutional layers, with \(3 \times 3\) kernels in the first two layers and a \(1 \times 1\) kernel in the last layer. For each LLM, the \ours models are trained on a small subset of the datasets, where five samples (around 3\%) were selected from each task in the LongBench dataset. During training, only decoding-stage attention was utilized.
The trained \ours models are evaluated on all tasks, including LongBench and GSM8K. The impressive results on GSM8K highlight the generalization capabilities and effectiveness of our approach.

\textit{Hyper-parameters.}
We set the history step H to 64, the block size b to 16 and the calibration step M to 5. Performance analysis of these hyperparameters is discussed in \autoref{sec:exp_ablation}. 
Follow Quest \citep{tang2024quest}, we did not apply our method or any other algorithms to the first two layers of the LLM. 
Following the settings of H2O and StreamingLLM, 
We allocated the budget equally to the prefix and local tokens, assigning 64 tokens each.
The remaining KV budget is allocated to intermediate tokens, determined by the prediction model.
We conducted experiments on NVIDIA A800 (80GB) GPUs.


\subsection{Attention Recovery Rate}
\input{tables/recovery_rate}

We first evaluate the attention recovery rate $R_{rec}$ of \ours, defined as the proportion of attention scores over critical tokens identified by the method relative to the total scores as defined in \autoref{eq:attention_recovery_score}, which reflects the intermediate information loss caused by KV cache compression methods. 
On the three representative tasks—QA, summary, and mathematical reasoning—with different KV cache budgets, \ours consistently achieves a higher average recovery rate compared to H2O\citep{zhang2023h2o} and Quest \citep{tang2024quest} as shown in \autoref{table:attention_recovery}.
This demonstrates that our method accurately identifies the positions of critical tokens, thereby minimizing information loss. 
Notably, with the KV budget of 512, \ours shows a significant advantage over Quest, achieving a 7\% higher average recovery rate, emphasizing our robustness under the extremely high (20$\times$) compression ratio. Furthermore, in the mathematical reasoning task, block-retrieval method Quest shows substantial recovery loss, suggesting potential limitations in this complex task. In contrast, our block-prediction method alleviates this issue, further underscoring its reliability.

\subsection{Final Accuracy Evaluation}
\label{sec:exp_main}


\textbf{Results on LongBench.} We evaluate our method on various long-context tasks in the LongBench benchmark, with the KV cache budgets ranging from 512 to 4096.
We restrict H2O’s attention to the past 64 steps to ensure fairness and denote this variant as H2O+. 
As shown in \autoref{table:main_longbench}, \ours surpasses the performance of all SOTA KV cache eviction and retrieval methods across various KV budgets and LLMs. Notably, the average performance loss compared to the full cache is less than 0.5\% across all cache budgets. This demonstrates the ability of our method to effectively model attention patterns and precisely predict the locations of critical tokens.
Furthermore, under an extremely sparse budget setting of 4\% (512/13K), our approach results in only a 0.44\% decrease in performance compared to the full cache, indicating a 76\% improvement over the state-of-the-art Quest method, which suffers a 1.84\% reduction.
\input{tables/main_longbench}



\textbf{Results on CoT reasoning.}
As shown in \autoref{table:gsm8k}, we evaluate our method on the mathematical reasoning dataset GSM8K with LLaMA-3.1. By adjusting the number of few-shot examples to simulate the long-context reasoning process with short and long CoTs, we generate input lengths of 4K, 8K, and 16K.
Unlike long-context tasks, CoT mathematical reasoning tasks present distinct challenges. For example, the retrieval-based SOTA method, Quest, excels on long-context benchmarks 
but performs poorly on CoT tasks, showing a 16.91\% accuracy drop at a sequence length of 16K. 
In contrast, \ours achieves a significantly smaller accuracy loss of just 2.05\%. Moreover, across all sequence lengths, our method outperforms baselines in most cases. Under a 25\% cache budget, it even exceeds the accuracy of the full cache, achieving lossless performance. 
H2O+ also performs well in this task, likely due to half of the cache budget being on local tokens, which remain unaffected by variations of the few-shot. Additionally, H2O+ includes only 64 steps of historical information, limiting the influence of irrelevant attention scores. 


\input{tables/main_gsm8k}




\subsection{Ablation Study}
\label{sec:exp_ablation}

The effects of varying hyperparameters on the performance of \ours with Longchat are studied. Three key hyperparameters, including history step, calibration step, and block size, are evaluated across six tasks from the LongBench dataset. Detailed task-specific results are available in App.~\ref{sec:app_ablation}. Additionally, the prediction model implementation is also evaluated.


\textbf{Block size $b$.}
As shown in \autoref{fig:block_size}, \ours maintains superior average performance across block sizes from 8 to 64.
While performance degradation occurs with increasing block size due to coarser token positioning, our method exhibits a milder decline compared to Quest's block-granularity KV cache retrieval. 
The performance drop in Quest may be attributed to the loss of total attention score information when the min-max based attention score upper bound estimation of each block is used for retrieved, leading to inaccuracies in block identification. 
In contrast, our approach accurately captures attention patterns, thereby improving critical block identification and mitigating the drawbacks of block-wise retrieval.


\textbf{Calibration step $M$.}
\autoref{fig:calibration_step} shows the average performance of \ours with calibration steps ranging from 1 to 20. Performance improves with shorter calibration intervals, indicating that the calibration scheme reduces cumulative errors caused by differences between sparse and original attention distributions. However, higher calibration frequencies increase computational costs, creating a trade-off between accuracy and efficiency.

\textbf{History step $H$.} As depicted in \autoref{fig:history_step}, we evaluate the average accuracy of \ours{} over a range of history steps. Overall, the performance gap compared to the full cache remains consistently small.
Notably, 
the middle value of $H$ maximizes performance by providing the necessary information for pattern recognition, while mitigating redundancy caused by the decaying self-correlation of attention scores.
\begin{figure*}[t]
    \centering
    
    \begin{subfigure}[b]{0.32\textwidth}
        % \centering
        \includegraphics[width=\columnwidth]{figures/exp/block_size.pdf}  
        \caption{Block Size}
        \label{fig:block_size}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        % \centering
        \includegraphics[width=\columnwidth]{figures/exp/calibration_step.pdf} 
        \caption{Calibration Step}
        \label{fig:calibration_step}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        % \centering
        \includegraphics[width=\columnwidth]{figures/exp/history_step.pdf} 
         \caption{History Step}
        \label{fig:history_step}
    \end{subfigure}
    % \hfill
    
    \caption{Hyper-parameter ablation on Longchat with LongBench.}
    \label{fig:ablation}
\end{figure*}



\textbf{Prediction model.} 
We evaluate the performance of various prediction model implementations. MLP is applied to the sequence dimension to handle variations in attention length. For LSTM, attention is divided into 16 width blocks and predictions are independent for each block, referred to as LSTM-block in \autoref{table:ablation_model}. Similarly, CNN-block utilizes a block-wise prediction approach, employing a fully connected layer for fixed block lengths. Finally, the CNN model, as the primary setting in our experiments, predicts all attention scores simultaneously.
As reported in \autoref{table:ablation_model}, CNN outperforms other models, achieving the highest attention recovery rate by effectively capturing attention patterns. MLP failed to account for neighboring token interactions, while LSTM-block and CNN-block were restricted to block-level information without global context. Notably, CNN also required the fewest parameters and was the most memory-efficient during inference.


\subsection{Efficiency}
\input{tables/ablation_model}
We evaluate the final LLM inference acceleration of our method in \autoref{table:efficiency}, using the widely adopted cross-layer prefetching system with cache offloading as the baseline, which is implemented in \citet{wolf-etal-2020-transformers}.
By reducing cache transfers, \ours achieves a 1.2$\times$ speedup in per-token decoding with a 1K budget, demonstrating more efficient inference. 
The added token evaluation time is negligible, leading to a substantial overall speed improvement.
Furthermore, we apply \ours to our proposed cross-token prefetching framework, which enables asynchronous cache retrieval and I/O transferring between layers. This integration yields an additional 1.4$\times$ speedup in the decoding stage, further highlighting the effectiveness of our approach.

\begin{table}[h]
\centering
\caption{The decode latency of per token of LLaMA-3.1-8B with 32K context length.}
\label{table:efficiency}
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Method} & \textbf{Decode latency(s)} & \textbf{Speed} \\
\midrule
cross-layer pref. & 0.364 & 1$\times$ \\
cross-layer pref.+\textbf{AttentionPredictor} & 0.295 & 1.2$\times$ \\
\textbf{cross-token pref.+AttentionPredictor} & \textbf{0.262} &\textbf{ 1.4$\times$} \\
\bottomrule
\end{tabular}%
}
\end{table}

