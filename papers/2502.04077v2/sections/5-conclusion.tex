\section{Conclusion}

We present \ours, the first learning-based critical token identification approach for KV cache compression. 
With a lightweight convolution model, \ours captures spatiotemporal patterns of attention score and predicts the next-token attention score accurately. 
Additionally, we propose the first cross-token prefetching framework, which effectively mitigates prediction and transfer delays in the LLM decoding stage. 
Experiments on the long-context datasets demonstrate that \ours achieves comparable accuracy of LLM with 16Ã— KV cache compression.

