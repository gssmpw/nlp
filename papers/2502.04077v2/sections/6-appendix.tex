%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Query Similarity}
\label{appendix:q_similarity}
We evaluate the similarity of consecutive queries. Specifically, we extract query data from LLaMA-3.1-8B using LongBench and compute the cosine autocorrelation of adjacent queries with a lag of 1. As shown in \autoref{fig:app_q}, the heatmap illustrates the query similarity across each layer and head of the LLM. The average similarity is 86\% for Longchat and 87\% for LLaMA-3.1, indicating strong continuity. Furthermore, the similarity in LLaMA is more consistent across layers, while Longchat shows higher similarity in the shallower layers.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\linewidth] %, height=0.5\linewidth
        {figures/observation/q_longchat.pdf}
    \includegraphics[width=0.45\linewidth] %, height=0.5\linewidth
        {figures/observation/q_similarity_llama.pdf}
    \caption{Query similarity of Longchat and LLaMA-3.1.}
    \label{fig:app_q}
\end{figure}




\section{Implementaion of \ours}
\label{sec:app_ourimplementaion}

\textbf{Training Details.} 
We capture the attention scores during full-cache inference as raw data. 
We package the attention scores of $H$ steps, together with the $(H+1)$-th stepâ€™s score, as input-output pairs. The training data is block-compressed to align with the usage during LLM model inference. 
Specifically, the input history attention is $A_H \in \mathbb{R}^{H \times \frac{t}{b}}$ and the output attention score is $S \in \mathbb{R}^{1 \times \frac{t}{b}}$. 
Attention scores with insufficient lengths are padded with zeros at the end to ensure that all data within a package have the same length. 
For the attention at step $t+1$, we only predict the first $t$ positions because the last position is derived from the newly generated key, which does not need to be compressed or transferred. 
Notably, only the attention scores from the decoding phase are used as output data. 
As a result, the training dataset includes the final $H$ steps of the pre-filling attention and all attention scores from the decoding stage. To train the model, we calculate the discrepancy between the predicted and real attention scores using mean squared error (MSE) loss. The training epoch is set to 30, and we select the model with the best attention recovery rate as shown in \autoref{eq:attention_recovery_score}.

\textbf{Cross-token Prefetching.}
We leverage GPU parallel streams and CPU multi-threading to parallelize the token retrieval and cache transfer across different layers.  Consistent with the main experiment, we maintain uncompressed caches for the first two layers to ensure higher inference accuracy.

\section{Experiments}
\subsection{Implementation of Baselines}
\label{sec:app_baselines}

Our experiments are based on LLMs using 16-bit floating points, and we utilize FlashAttention2~\citep{dao2024flashattention} during the prefilling stage to reduce GPU memory consumption. Since FlashAttention2 does not output intermediate attention scores, for methods that rely on historical attention, we additionally compute partial prefill attention to obtain the scores. Our batch size is fixed at 1. 
For all baselines, we use the official code implementations to ensure the effectiveness of the methods. Additionally, for all methods, we skip compress the first two layers of the LLM.
\begin{itemize}
\item StreamingLLM~\citep{xiao2024streamingllm} finds that the sink token is very important, so it retains the initial and the most recent few tokens. For StreamingLLM, we evenly distribute the budget between sink tokens and local tokens.

\item
H2O~\citep{zhang2023h2o} accumulates all historical attention scores as an estimation. We evenly distribute the budget between the heavy budget and the recent budget. Since computing the full historical data can cause OOM with longer inputs, we use the last 64 steps of historical information to calculate the heavy hitters, and denote this as H2O+. Note that accumulating all prefill attention scores causes H2O to focus on initial tokens, so this is an improvement to the method's effectiveness.
\item SnapKV~\citep{li2024snapkv} accumulates attention scores within a recent window as an estimate and performs one-time filtering during the prefill stage. We set the time window to a size of 64 to align with our method. Since SnapKV only performs cache compression once during the prefill stage, the number of KV tokens it uses continues to grow during decoding, resulting in a relatively lower compression ratio compared to other methods.
\item Quest~\citep{tang2024quest} uses the current query and paged key to approximate the attention score. We use a chunk size of 16, corresponding to the block size in our method, which is also the parameter value used in the original paper.
\end{itemize}




\subsection{Details of LongBench Dataset}
\label{sec:app_longbench}


LongBench~\citep{bai2024longbench} is a carefully crafted benchmark suite designed to evaluate the capabilities of language models in processing extended documents and complex information sequences. It was developed for multi-task assessment of long-context inputs. The details of the metrics, number of words, language, and data used in LongBench are presented in \autoref{table:appendix_longbench}.

\begin{table}[htbp]
\centering  
\caption{An overview of the dataset statistics in LongBench. 'Source' denotes the origin of the context. 'Avg len' refers to the average number of words, which is shorter than the token length after tokenization. 'Accuracy (CLS)' refers to classification accuracy, while 'Accuracy (EM)' refers to exact match accuracy.}
\resizebox{0.65\textwidth}{!}{
\label{table:appendix_longbench}
\begin{tabular}{llrccc}
\toprule
\textbf{Dataset} & \textbf{Source} & \textbf{Avg len} & \textbf{Metric} & \textbf{Language} & \textbf{\#data} \\
\midrule
\emph{Single-Document QA} \\
NarrativeQA & Literature, Film & 18,409 & F1 & English & 200 \\
Qasper & Science & 3,619 & F1 & English & 200 \\
MultiFieldQA-en & Multi-field & 4,559 & F1 & English & 150 \\
\midrule
\emph{Multi-Document QA} \\
HotpotQA & Wikipedia & 9,151 & F1 & English & 200 \\
2WikiMultihopQA & Wikipedia & 4,887 & F1 & English & 200 \\
MuSiQue & Wikipedia & 11,214 & F1 & English & 200 \\
\midrule
\emph{Summarization} \\
GovReport & Government report & 8,734 & Rouge-L & English & 200 \\
QMSum & Meeting & 10,614 & Rouge-L & English & 200 \\
MultiNews & News & 2,113 & Rouge-L & English & 200 \\
\midrule
\emph{Few-shot Learning} \\
TREC & Web question & 5,177 & Accuracy (CLS) & English & 200 \\
TriviaQA & Wikipedia, Web & 8,209 & F1 & English & 200 \\
SAMSum & Dialogue & 6,258 & Rouge-L & English & 200 \\
\midrule
\emph{Synthetic Task} \\
PassageCount & Wikipedia & 11,141 & Accuracy (EM) & English & 200 \\
PassageRetrieval-en & Wikipedia & 9,289 & Accuracy (EM) & English & 200 \\
\midrule
\emph{Code Completion} \\
LCC & Github & 1,235 & Edit Sim & Python/C\#/Java & 500 \\
RepoBench-P & Github repository & 4,206 & Edit Sim & Python/Java & 500 \\
\bottomrule
\end{tabular}
}

\end{table}


\subsection{Construction of Long COT Dataset}
\label{appendix:gsm8k_example}
To simulate Chain-of-Thought (CoT) tasks within long-context, we increased the number of few-shot examples. Specifically, we randomly selected a fixed number of questions and standard CoT answer pairs as prompts, along with the questions to be tested. We chose 25, 47, and 97 few-shot examples, resulting in input lengths of approximately 4K, 8K, and 16K tokens respectively. The few-shot data were sourced from the GSM8K training set. Since the test set does not overlap with the training set, the answers remain undisclosed to the LLM.
% \todoo{Figure of few-shot gsm8k.}

\subsection{Longbench Results of All Tasks}
\label{sec:app_longbench_table}
Due to space constraints, we presented the test results after aggregating task types in Section \ref{sec:exp_main}. Here, we present the individual results for all tasks. The \autoref{table:app_longbench} shows that our method surpasses the majority of the SOTAs on most tasks, and the average performance under all budgets and LLMs exceeds all compared methods, demonstrating the effectiveness of our approach.

\input{tables/appendix_longbench}

\subsection{Ablation Study Results}
\label{sec:app_ablation}
\textbf{Hyperparameter results.}
We evaluate the impact of hyperparameters on our method. Specifically, we train the predictor using the default parameters of the main experiment, i.e., $b=16$, $H=64$. We employ full attention without sparse computation during training, which is equivalent to performing distribution error calibration at each step, i.e., $M=1$. Subsequently, we test the performance under different hyperparameters using the trained model. Other parameters during testing remain consistent with the main experiment, with a KV budget of 1K. We evaluate six representative tasks in Longbench. The results are shown in \autoref{tab:app_ablation}.

When the block size is 8, we achieve comparable performance to the non-compressed model, demonstrating the effectiveness of our method. Additionally, we significantly mitigate the accuracy drop caused by the increase in retrieval granularity. When $b=64$, \ours outperforms Quest by 10\%. For the summary long output task, increasing the frequency of error calibration effectively improves model performance, underscoring the effectiveness of our error calibration method.

\begin{table}[htb]
\centering
\caption{Results of \ours with different hyperparameters.}
\label{tab:app_ablation}
\resizebox{0.65\textwidth}{!}{%
\begin{tabular}{@{}ccccccccc@{}}
\toprule
\multicolumn{9}{c}{\textbf{LongBench+Longchat}} \\ \midrule
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Hyperprameter}}} & \textbf{SQA} & \textbf{MQA} & \textbf{Summary} & \textbf{Few-shot} & \textbf{Synthetic} & \textbf{Code} & \multirow{2}{*}{\textbf{Average}} \\ \cmidrule(lr){3-8}
\multicolumn{2}{c}{} & \textbf{MF-en} & \textbf{HotpotQA} & \textbf{QMSum} & \textbf{TriviaQA} & \textbf{Pre} & \textbf{Lcc} &  \\ \midrule
\multicolumn{2}{c}{\textbf{Full Cache}} & 43.09 & 33.05 & 22.79 & 83.99 & 30.50 & 52.94 & \textbf{44.39} \\ \midrule
\multirow{4}{*}{\makecell{\textbf{History} \\ \textbf{Step}}} & \textbf{16} & 41.83 & 34.00 & 22.22 & 84.45 & 28.00 & 51.87 & \textbf{43.73} \\
 & \textbf{32} & 41.60 & 33.90 & 22.32 & 84.81 & 30.00 & 52.53 & \textbf{44.19} \\
 & \textbf{64} & 41.67 & 33.64 & 22.30 & 84.85 & 28.00 & 52.22 & \textbf{43.78} \\
 & \textbf{128} & 41.51 & 34.39 & 22.45 & 84.35 & 26.00 & 52.96 & \textbf{43.61} \\ \midrule
\multirow{5}{*}{\makecell{\textbf{Calibration} \\ \textbf{Step}}} & \textbf{1} & 42.89 & 33.27 & 22.35 & 84.70 & 28.50 & 52.53 & \textbf{44.04} \\
 & \textbf{2} & 42.47 & 33.61 & 22.43 & 84.90 & 28.00 & 52.13 & \textbf{43.92} \\
 & \textbf{5} & 41.67 & 33.64 & 22.30 & 84.85 & 28.00 & 52.22 & \textbf{43.78} \\
 & \textbf{10} & 41.10 & 33.87 & 22.37 & 84.85 & 28.00 & 52.36 & \textbf{43.76} \\
 & \textbf{20} & 41.54 & 33.99 & 22.12 & 84.90 & 28.00 & 52.50 & \textbf{43.84} \\ \midrule
\multirow{4}{*}{\makecell{\textbf{Block} \\ \textbf{Size}}} & \textbf{8} & 43.09 & 34.61 & 22.38 & 85.29 & 28.00 & 52.76 & \textbf{44.36} \\
 & \textbf{16} & 41.67 & 33.64 & 22.30 & 84.85 & 28.00 & 52.22 & \textbf{43.78} \\
 & \textbf{32} & 40.85 & 34.47 & 22.46 & 84.87 & 25.00 & 53.02 & \textbf{43.45} \\
 & \textbf{64} & 42.60 & 34.04 & 22.01 & 84.85 & 19.00 & 52.86 & \textbf{42.56} \\ \bottomrule
\end{tabular}%
}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%