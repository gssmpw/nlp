\section{Introduction}

Large language models (LLM) like OpenAI o1~\citep{openai2024o1} have shown impressive scalability and effectiveness in tackling complex tasks through long chain-of-thought (CoT) reasoning and multi-turn conversations~\citep{minaee2024large, huang2024understanding}. 
However, these long-context tasks require LLMs to handle extremely lengthy contexts, presenting computational and memory challenges for LLMs~\citep{zhou2024survey}. 
Specifically, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, consumes huge GPU memory~\citep{li2024survey}. 
For example, for a model with 7 billion parameters, the parameters consume only 14 GB of memory whereas the KV cache requires around 72 GB with the 128K prompt length~\citep{yang2024pyramidinfer}. 
As the decoding latency and memory footprint scale with the KV cache, it is important to compress the KV cache as the prompt expands. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/intro.pdf}
    \caption{A comparison of H2O, Quest, and \ours for identifying critical tokens in the next step with history attention score. Our learning-based spatiotemporal predictor captures the dynamic attention patterns and accurately predicts next-step attention scores.}
    \vspace{-0.7cm}
    
    \label{fig:intro_comparison}
\end{figure}

Many attention sparsity-based methods are proposed to compress the KV cache in the sequence dimension. 
Previous works have shown that a small portion of tokens dominate the attention distribution,  substantially determining token generation precision \citep{liu2023dejavu,zhang2023h2o, ge2024fastgen}.
Therefore, we can dramatically reduce the cache size by computing attention sparsely with only the critical tokens, while maintaining LLM performance. 
Cache eviction methods \citep{zhang2023h2o, xiao2024streamingllm,li2024snapkv} use heuristic ranking with attention scores to identify critical keys and values and evict the less relevant ones. However, heuristic scoring methods can only model static patterns and struggle to identify critical tokens accurately, causing these methods to suffer from a degradation in LLM performance.
Cache retrieval methods \citep{tang2024quest, zhang2024pqcache} identify critical tokens by approximating attention using compressed keys and the current query.
Although effective,
these methods demand substantial computational resources, and model accuracy tends to degrade as the compression ratio and retrieval granularity increase. 
In particular, \citet{tang2024quest} experiences a sharp 11\% accuracy drop when the retrieving page size increases from 16 to 64.
Additionally, retrieval-based methods rely on the query token in the current step, limiting their ability to overlap the estimation time overhead with actual LLM inference computation in cache prefetching systems. 
Thus, it is essential to accurately and rapidly identify critical tokens of the KV cache.



In our paper, we explore the importance of temporal patterns in attention scores for identifying critical KV cache. Previous research has indicated that attention scores exhibit repetitive patterns \citep{jiang2024minference, ge2024fastgen} and are intrinsic to the LLM. 
These findings suggest that identifying attention patterns enables the prediction of critical tokens, making cache compression possible while preserving most information as in multi-tier cache management systems \citep{hashemi2018learning, li2021block}.
We further observe that attention patterns have temporal characteristics, such as re-access, sequential, and seasonal, illustrated in \autoref{fig:attention_heatmap}.
To capture the attention patterns, we introduce \ours, a time-series prediction approach to predict attention scores for critical token identification, as shown in \autoref{fig:intro_comparison}. 
Since temporal attention patterns are dynamic and difficult to capture with existing heuristic methods, our approach is the first learning-based method to address this challenge.
We frame the prediction of attention scores as a spatio-temporal forecasting problem, treating token positions as spatial patterns. 
We then train a convolutional model to predict next-step attention scores based on historical attention time series, enabling us to accurately identify the most critical tokens during LLM decoding.
Notably, the lightweight \ours allows for accurate predictions with negligible memory consumption.
Additionally, we employ distribution error calibration by periodic computing dense attention,
and we apply block-wise attention compression to further improve the efficiency of prediction. 
By accurately identifying critical tokens, \ours retains most of the attention information after the KV cache compression.

To further accelerate LLM decoding, we apply \ours to our proposed KV cache management framework, a cross-token KV cache prefetching system.
Current LLM serving systems offload the KV cache to the CPU to reduce the GPU memory usage of long-context generation, but CPU-GPU transfer latency of KV cache becomes a significant bottleneck.  
KV cache prefetching presents a solution to hide cache loading time by asynchronously loading the required KV cache in advance.
In contrast to the existing cross-layer approach \citep{lee2024infinigen}, our cross-token method can capitalize on longer transfer times, and adapt to more sophisticated and accurate methods for identifying critical tokens. 
Additionally, the transfer data for each token is better organized, leading to improved I/O utilization.


We evaluated our method on several representative LLMs with varying KV cache budgets. On the widely used LongBench dataset, our approach achieves 
% the LLM accuracy drop of no more than 0.5\% 
comparable accuracy
with 16× KV cache compression, outperforming the state-of-the-art by 41\%.  With 32K context, our prefetching framework achieves a 1.4$\times$ speedup of per token decoding latency.
% Additionally, on the CoT task with 16K prompt length, we achieved a 14.86\% improvement in accuracy over other methods at the same compression ratio.
In summary, we make the following contribution:
\begin{itemize}
\item 
Based on our observation of the temporal pattern in the attention score, we propose \ours, the first learning-based critical token identification approach.
\item We propose the first cross-token prefetching framework, which effectively mitigates prediction and transfer delays in the LLM decoding stage. 
\item Experiments on the long-context datasets demonstrate that our approach achieves comparable accuracy of LLM with 16× KV cache compression.

\end{itemize}
