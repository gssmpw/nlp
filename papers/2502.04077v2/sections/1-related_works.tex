\section{Related works}
\subsection{Efficient LLM Inference}
Several efforts have optimized pre-trained model inference efficiency from different perspectives.
Speculative decoding methods~\citep{cai2024medusa, li2024eagle, sun2024triforce, liu2024deepseekv3} use smaller draft models to accelerate the auto-regressive decoding process of LLMs. 
Specifically, these methods employ a draft model to efficiently predict several subsequent tokens as generation results, which are then validated in parallel using the target LLM.
In contrast, our approach focuses on predicting the attention scores of the next token, serving as an estimation for KV cache compression.
Other methods include model compression~\citep{frantar2023gptq, lin2024awq} and inference systems~\citep{kwon2023vllm, sheng2023flexgen, song2024tackling, 2023lmdeploy, zheng2024sglang, ye2025flashinfer}.


\subsection{KV Cache Compression}
Many methods are dedicated to compressing the KV cache while retaining as much information of attention as possible.
Many works have found that attention scores are sparse, so sparse computation can be performed on high-score positions by estimating attention scores.

\textbf{Cache eviction.} These methods use heuristic approaches to identify key-value pairs of high importance and evict the less relevant ones.
StreamingLLM~\citep{xiao2024streamingllm} observes that the earliest tokens have high attention scores during inference, so it only retains the initial and the recent few tokens.
H2O~\citep{zhang2023h2o} accumulates all historical attention scores as an estimation, but suffers from the accumulation error of scores from the first few tokens being too frequent. 
SnapKV~\citep{li2024snapkv} accumulates attention scores within a recent window as an estimate and performs one-time filtering during the prefill stage.
MInference~\citep{jiang2024minference} inductively defines three attention patterns and pre-assigns each head a fixed compression method, determining hyperparameters by attention scores within a recent window and staying static during the decoding stage.
All these methods are heuristic-based and statically model attention patterns. 
They struggle to capture the dynamic temporal patterns within attention scores accurately.

\textbf{Cache retrieval.} These methods aim to retrieve the most critical tokens with approximate attention scores.
Quest~\citep{tang2024quest} achieves this by using the current query and paged key to approximate the attention score. However, Quest is sensitive to the page size, and the accuracy significantly drops with large page sizes and small budgets. 
Similarly, PQCache~\citep{zhang2024pqcache} applies key quantization during the prefilling stage and reuses these quantized keys in decoding to approximate attention.
Since these methods require information from the current step, they cannot use asynchronous computation to cover long estimation durations. 
Other methods involving KV cache quantization~\citep{liu2024kivi, hooper2024kvquant, zhang2024qhitter} and KV cache budget allocation~\citep{cai2024pyramidkv, yang2024pyramidinfer, feng2024adakv} are orthogonal to our token scoring approach and can be combined with our \ours.

\textbf{KV cache cross-layer prefetching.} These methods hide part of the cache transfer time based on offloading the cache to the CPU. However, as the sequence length increases, the transfer time is too long to be hidden.
InfiniGen~\citep{lee2024infinigen} combines cache retrieval and prefetching by approximating the attention score for the next layer to load the critical cache.
However, the estimation time increases significantly as the sequence grows, and the inference time for a single layer is insufficient to cover this. 
In contrast, our cross-token prefetching framework can hide longer estimation and transfer time within the per-token inference time.
