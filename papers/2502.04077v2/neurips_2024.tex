\documentclass{article}
\pdfoutput=1
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{colortbl}
\setlength{\tabcolsep}{0.25em}
\usepackage{wrapfig}
\usepackage{natbib}

\title{AttentionPredictor: Temporal Pattern Matters \\ for Efficient LLM Inference}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.
\input{macros}

\author{
    Qingyue Yang$^1$\thanks{This work was done when Qingyue Yang was an intern at Huawei.}
  \And
   Jie Wang$^1$\thanks{Corresponding author. Email: jiewangx@ustc.edu.cn.}
   \And
   Xing Li$^2$
   \And
   Zhihai Wang$^1$
   \And
   Chen Chen$^2$
   \And
   Lei Chen$^2$ 
   \And 
   Xianzhi Yu$^2$
   \And
   Wulong Liu$^2$
   \And
   Jianye Hao$^{2,3}$
   \And
   Mingxuan Yuan$^2$
   \And
   Bin Li$^1$
   \AND
   $^1$MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition,\\
   University of Science and Technology of China\\
   $^2$Noah's Ark Lab, Huawei Technologies\\
   $^3$College of Intelligence and Computing, Tianjin University
}


\begin{document}


\maketitle


\begin{abstract}
With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation.
To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores.
However, these methods often struggle to accurately determine critical tokens as they neglect the \textit{temporal patterns} in attention scores, resulting in a noticeable
degradation in LLM performance. 
To address this challenge, we propose \ours, which is the first learning-based critical token identification approach.
Specifically, \ours learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score.
An appealing feature of \ours is that it accurately predicts the attention score while consuming negligible memory.
Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage.
By retaining most of the attention information, \ours achieves 16$\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art. The code is available at \href{https://github.com/MIRALab-USTC/LLM-AttentionPredictor}{https://github.com/MIRALab-USTC/LLM-AttentionPredictor}.
\end{abstract}


\input{sections/0-introduction}
\input{sections/1-related_works}
\input{sections/2-observation}
\input{sections/3-methods}
\input{sections/4-experiments}
\input{sections/5-conclusion}

\newpage

\bibliography{neurips_2024}
\bibliographystyle{neurips_2024}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\input{sections/6-appendix}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% END INSTRUCTIONS %%%




\end{document}