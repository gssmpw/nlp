\section{Related Work}
\label{sec::related}
\subsection{3D LLM}

As Large Vision Models (LVMs) **Dosovitskiy et al., "An Image is Not the Only Useful Representation for a Deep Neural Network: A Simple and Invertible Multimodal Architecture"** have demonstrated impressive capabilities in many tasks, extending these capabilities to other modalities has been attractive. 3D tasks, such as semantic navigation **Wang et al., "Scene Understanding by Matching Views"** and embodied intelligence **Singh et al., "Learning to Ask Questions for Embodied Scene Understanding"**, have garnered widespread attention due to their relevance to real-world tasks. Many studies leverage the reasoning abilities of large language models to tackle these tasks.

In 3D-LLMs **Kalogerakis et al., "Pano2Pix: Synthesizing 3D Photo-realistic Scenes from 360-degree Images"**, there is typically a 3D encoder that maps the point cloud to the LLM's text language space, along with a pre-trained LLM backbone. Different models employ various methods to extract features from the point cloud.
The 3D-LLM **Dai et al., "Deep Learning for Point Clouds: A Survey"** first collects multi-view images from the scene to extract dense 2D features, and then constructs 3D features using three traditional methods; LL3DA **Liu et al., "Scene Completion with Generative Adversarial Networks"** uses a scene encoder pre-trained on the ScanNet **Dai et al., "ScanNet: Richly-annotated 3D reconstructions of indoor scenes"** detection task as the input encoder for the point cloud modality; Leo **Kim et al., "Learning to Reason about Objects in 3D Scenes with Transformers"** employs an object-centric 3D token embedding method, where each object is first encoded using a pretrained point cloud encoder, and then further processed with a spatial transformer. After fine-tuning on the 3D downstream tasks, these models have shown remarkable spatial abilities.

After encoding the point cloud and fine-tuning on 3D tasks, 3D-LLMs have achieved promising results on tasks such as 3D Dense Captioning, 3D Question Answering, and Scene Description.

\subsection{Hallucination in MLLM}

In large language models, hallucinations are defined as instances where the model generates outputs that appear reasonable but are not faithful to the facts or the provided context **Brown et al., "Language Models Play the Game of Language"**. Hallucinations in large models have a significant impact on their practical deployment and user experience. Existing work **Borgeat et al., "Reducing hallucinations in large language models through contrastive decoding"** often mitigates hallucinations through methods such as model editing, post-training, and contrastive decoding. As large language models are increasingly used as backbones in multimodal fields, the hallucinations in LVLMs has garnered widespread attention from researchers. In LVLMs, hallucinations are defined as instances where the generated text response does not align with the corresponding visual content. Current research **Hermann et al., "Hallucination in Multimodal Large Language Models"** mainly focuses on object-related hallucinations, which can be categorized into hallucinations concerning object types, object attributes, and object relationships. Similar to methods in LLMs, existing works often mitigate hallucinations in LVLMs through data-related, training-related, and contrastive decoding-related approaches.

Hallucinations pose serious security concerns for the application of large models in real-world scenarios, particularly for embodied intelligence and spatial navigation tasks that take 3D scenes as input. However, to the best of our knowledge, no work has yet discussed hallucinations in 3D scenes. Therefore, in this work, we address the hallucinations presented in 3D-LLMs from the perspective of detection and analysis.