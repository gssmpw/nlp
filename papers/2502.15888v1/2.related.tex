\section{Related Work}
\label{sec::related}
\subsection{3D LLM}

As Large Vision Models (LVMs)~\cite{shen2024aligning, zhang2022dino, kirillov2023segment, oquab2023dinov2} have demonstrated impressive capabilities in many tasks, extending these capabilities to other modalities has been attractive. 3D tasks, such as semantic navigation~\cite{zheng2024towards, huang2023embodied} and embodied intelligence~\cite{jatavallabhula2023conceptfusion, hong2024multiply}, have garnered widespread attention due to their relevance to real-world tasks. Many studies leverage the reasoning abilities of large language models to tackle these tasks.

In 3D-LLMs~\cite{hong20233d-3dllm}, there is typically a 3D encoder that maps the point cloud to the LLM's text language space, along with a pre-trained LLM backbone. Different models employ various methods to extract features from the point cloud.
The 3D-LLM~\cite{hong20233d-3dllm} first collects multi-view images from the scene to extract dense 2D features, and then constructs 3D features using three traditional methods; LL3DA~\cite{chen2024ll3da} uses a scene encoder pre-trained on the ScanNet~\cite{dai2017scannet} detection task as the input encoder for the point cloud modality; Leo~\cite{huang2023embodied} employs an object-centric 3D token embedding method, where each object is first encoded using a pretrained point cloud encoder, and then further processed with a spatial transformer. After fine-tuning on the 3D downstream tasks, these models have shown remarkable spatial abilities.

After encoding the point cloud and fine-tuning on 3D tasks, 3D-LLMs have achieved promising results on tasks such as 3D Dense Captioning, 3D Question Answering, and Scene Description.

\subsection{Hallucination in MLLM}

In large language models, hallucinations are defined as instances where the model generates outputs that appear reasonable but are not faithful to the facts or the provided context~\cite{filippova-2020-controlled}. Hallucinations in large models have a significant impact on their practical deployment and user experience. Existing work~\cite{leng2024mitigating, liu2023mitigating, yu2024hallucidoctor, zhai2023halle} often mitigates hallucinations through methods such as model editing, post-training, and contrastive decoding. As large language models are increasingly used as backbones in multimodal fields, the hallucinations in LVLMs has garnered widespread attention from researchers. In LVLMs, hallucinations are defined as instances where the generated text response does not align with the corresponding visual content. Current research ~\cite{rohrbach2018object, li2023evaluating, hu2023ciem} mainly focuses on object-related hallucinations, which can be categorized into hallucinations concerning object types, object attributes, and object relationships. Similar to methods in LLMs, existing works often mitigate hallucinations in LVLMs through data-related, training-related, and contrastive decoding-related approaches.

Hallucinations pose serious security concerns for the application of large models in real-world scenarios, particularly for embodied intelligence and spatial navigation tasks that take 3D scenes as input. However, to the best of our knowledge, no work has yet discussed hallucinations in 3D scenes. Therefore, in this work, we address the hallucinations presented in 3D-LLMs from the perspective of detection and analysis.
