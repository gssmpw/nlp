\section{Introduction}

% ### Revised and Translated Version of the Introduction Plan 
% Intro第一段引入3D LLM
Large Language Models (LLMs) have demonstrated remarkable capabilities in solving complex tasks such as code completion~\cite{kanade2020learning,wang2021codet5}, mathematical reasoning~\cite{jiang2024forward,guo2024exploring}, and dialogue generation~\cite{li2024dialogue, le2020uniconv}. The success of LLMs has inspired researchers to extend this approach to more modalities, aiming to enhance models’ understanding of the real world. Large Vision-language Models, which process both images and text ~\cite{wang2024qwen2, deitke2024molmo} enabling models to interpret visual content. However, 2D information typically provides single-perspective view, which limits models' ability to accurately understand spatial relationships within a scene. Recently, researchers have explored the integration of 3D point cloud data as an input modality to improve large models’ spatial understanding. Previous works such as 3D-LLM~\cite{hong20233d-3dllm}, PointLLM~\cite{xu2024pointllm}, 3D-VLA~\cite{zhen2024-3d-vla} employed an encoder to extract features from point clouds, followed by a projector that maps these features into the tokenized space of the LLM. After fine-tuning on various 3D scene, 3D LLMs demonstrate promising performance on spatial reasoning tasks.  

% such as visual navigation~\cite{zheng2024towards, huang2023embodied}, 3D comprehension~\cite{jatavallabhula2023conceptfusion, hong2024multiply}, and 3D question answering.

% This limitation poses challenges for tasks requiring spatial reasoning , such as visual navigation ~\cite{zhang2024vla, kirsanov2019discoman} and 3D captioning~\cite{chen2021scan2cap, luo2023scalable, yuan2022x, li2023uni3dl}. 
%第二段简要介绍3D-LLM的做法 

Despite the success achieved by multi-modal LLMs, \textbf{hallucination}~\cite{rohrbach2018object, li2023evaluating, hu2023ciem, guan2024hallusionbench} has been identified in both Large Language Models and Large Vision-Language models. Hallucination refers to the generation of content by large models that appears correct in form but contains errors or misleading information. This poses significant challenges to the application of large models in high-risk domains such as Healthcare, Law, or Finance. Numerous methods for evaluating and detecting hallucinations have been proposed to enhance the faithfulness of large models. TruthfulQA and HalluQA have been introduced to identify hallucinations in Large Language Models (LLMs), while CHAIR and POPE have been developed to detect object-centric hallucinations in Large Vision-Language Models (LVLMs).
% These hallucinations manifest as outputs that conflict with factual information or descriptions that do not align with the visual content. 
However, there is a lack of systematic studies on the existence of hallucinations in 3D-LLMs. In this work, we conduct a comprehensive investigation into the hallucination issue in 3D-LLMs and analyze the underlying causes.

%第三段需要介绍难点和贡献
With the introduction of the point cloud modality and depth information, the challenges faced by 3D-LLMs have shifted from image description to the accurate understanding of spatial relationships. Defining hallucinations in 3D scenes precisely has become a significant challenge. Therefore, we first define 3D hallucinations from a broad to a specific perspective, comparing them with text and visual hallucinations. Then, we use traditional evaluation method to evaluate 3D hallucinations in different representative 3D-LLMs and find they all suffer from severe hallucinations. To find the causes of severe hallucinations, we approach this analysis from the perspectives of modality co-occurrence in the training datasets and data distribution. We emphasize excessively high co-occurrence frequencies between objects is the main reason. Finally, previous 2D hallucination detection methods have predominantly focused on object-centric hallucinations, as the relationships between objects are relatively straightforward. However, in 3D scenes, there exists a multitude of complex spatial relationships. To address this, we propose a novel method and dataset capable of efficiently and accurately detecting spatial relationship hallucinations in models.
% To the best of our knowledge, we are the first to conduct a systematic investigation and analysis of hallucinations Therefore, we first define 3D hallucinations from a broad to a specific perspective, comparing them with text and visual hallucinations identified in previous works. As shown in Table 1, hallucinations in 3D-LLMs not only include those observed in previous modalities but also feature unique hallucinations related to spatial scale and accurate spatial relationships. Then, we evaluate the extent of 3D hallucinations in different representative 3D-LLMs. We categorize the questions from the 3D datasets into different types based on the hallucination classifications in Table 1 and perform detection for each category separately. As shown in Table 2, most 3D-LLMs exhibit object hallucinations, with spatial relationship hallucinations being even more pronounced. Furthermore, we analyze the causes of spatial hallucinations. We approach this analysis from the perspectives of modality co-occurrence in the training datasets and data distribution. Our findings confirm that the occurrence of hallucinations is primarily driven by excessively high co-occurrence frequencies between objects in the point cloud scenes, providing insights for the construction of future point cloud datasets.

The contributions of our paper can be summarized as follows: (1) To the best of our knowledge, we are the first to investigate and define 3D hallucinations. 
(2) We evaluate several state-of-the-art 3D-LLMs and provide a detailed analysis of the causes of hallucinations.
(3) We construct a new dataset and established a detection benchmark to efficiently and accurately identify 3D hallucinations.

\begin{figure*}[t] % [t] 表示图片放置在页面顶部，跨双栏
\centering
\includegraphics[width=\textwidth]{figs_evaluation/case_study_lky.png} % 使用 \textwidth 适应双栏宽度
\caption{In 3D scenes, the relationships between objects are significantly more complex than those in text or images. The left side of the figure illustrates hallucinations related to relative positional relationships and absolute positional relationships, while the right side demonstrates attribute hallucinations such as color, size, and shape.}
\label{fig:example}
\end{figure*}
% \begin{table*}[h]
% \centering
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{@{}lcccccccccccc@{}}
% \toprule
% \multirow{3}{*}{Hallucination Type} & 
% \multicolumn{3}{c}{Input Modality} &
% \multicolumn{3}{c}{Modality Conflict} & 
% \multicolumn{3}{c}{Object Hallucination} &
% \multicolumn{3}{c}{Relation Hallucination}  \\ \cmidrule(lr){2-13} 
%                                & Text    & Vision   & Depth   & Knowledge Conflict & Text-Image Conflict & Scene Conflict &Color & Shape & Size & Abstract& Relative & Accurate \\ \midrule
% \multirow{1}{*}{Text Hallucination} & \checkmark & \xmark  & \xmark  & \checkmark & \xmark & \xmark & \checkmark & \xmark &  \xmark & \checkmark & \xmark & \xmark \\
% \multirow{1}{*}{Image Hallucination} & \checkmark & \checkmark & \xmark & \checkmark & \checkmark & \xmark & \checkmark & \checkmark & \xmark & \checkmark & \checkmark & \xmark \\
% \multirow{1}{*}{3D Hallucination} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
% \bottomrule
% \end{tabular}
% }
% \caption{Classification of Hallucinations and their Relationship with Input Modalities and Features}
% \end{table*}

% \begin{table}[H]
%     \centering
%    \resizebox{1.0\linewidth}{!}{ \begin{tabular}{cccccc}
%     \toprule
%      & Precision & Recall & F1Score & Rouge & Meteor \\
%     \midrule
%     ll3da & 36.36 & 16.67 & 22.86 & 25.87 & 14.98 \\
%     3dllm & 22.97 & 8.20  & 10.92 & 9.94  & 4.37  \\
%     \bottomrule
%     \end{tabular}}
%     \label{table:num_agents}
%     \caption{Evaluate Result of Sota 3D-LLM}
% \end{table}

% \begin{table}[h]
% \centering
% \normalsize
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{ccccccc}
% \toprule
% \multirow{3}{*}{Model Type} & 
% \multicolumn{3}{c}{Input Modality} &
% \multicolumn{3}{c}{Modality Conflict} \\ \cmidrule(lr){2-7}
% & Text    & Vision   & Depth   & Knowledge Conflict & Text-Image Conflict & Scene Conflict \\ \midrule
% \multirow{1}{*}{LLM} & \checkmark & \xmark  & \xmark  & \checkmark & \xmark & \xmark \\
% \multirow{1}{*}{LVLM} & \checkmark & \checkmark & \xmark & \checkmark & \checkmark & \xmark  \\
% \multirow{1}{*}{3D-LLM} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  \\
% \bottomrule
% \end{tabular}}
% \caption{Classification of Hallucinations and their Relationship with Input Modalities and Features}
% \end{table}
% Large Language Models (LLMs) and Their Applications in Multimodal Tasks
% - Large Language Models (LLMs) have demonstrated remarkable capabilities in solving complex tasks by following human instructions, even in zero-shot scenarios.  
% - Building on the success of LLMs, researchers have begun exploring the integration of LLMs with Vision-Language Pretraining Models (VLPMs) to enhance semantic understanding of visual information. This fusion has given rise to Large Vision-Language Models (LVLMs), with GPT-4's performance in multimodal tasks being a notable example.  
% - The success of multimodal LLMs in domains like images and videos has inspired researchers in the 3D point cloud field to investigate the feasibility of developing large-scale models for 3D point cloud tasks.  

% Architecture and Training of Large-Scale 3D Point Cloud Models  
% - Large-scale 3D point cloud models typically employ an encoder to extract features from 3D point clouds, followed by a projector that maps these features into the tokenized space of the LLM. Examples include 3D-LLM, PointLLM, 3D-VLA, LiDAR-LLM, and Agent3D-Zero. The LLM is then used for reasoning.  
% - The training process generally involves three key stages: independent pretraining of vision and language encoders, pretraining for point cloud-text alignment, and visual instruction fine-tuning.  

% Object Hallucination in 3D Point Clouds
% - While large-scale 3D point cloud models outperform smaller models on 3D point cloud tasks, they inherit the hallucination problems commonly found in LLMs and multimodal LLMs. These models tend to generate content such as objects, attributes, or relationships that are not present in the point cloud.  
% - A prevalent issue is "faithfulness hallucination," where the content generated by the LLM does not align with the contextual instructions or the visual information.  

% Research Motivation and Objectives
% - The goal of this study is to systematically evaluate the hallucination issues in existing large-scale 3D point cloud models and quantify the severity of these hallucinations.  
% - Based on evaluation metrics, the study aims to analyze the underlying causes of hallucinations in 3D point clouds.  

% Preliminary Findings and Contributions
% **[Anticipated Results]** 
% - Initial experimental results indicate that most large-scale 3D point cloud models suffer from severe object hallucination issues, even more frequently than smaller-scale vision-language models.  
% - The study further explores the influence of visual instructions on hallucinations, revealing that objects, relationships, and attributes frequently mentioned in instructions or co-occurring with visual objects are more prone to hallucination.  
% - The research also experiments with adapting 2D hallucination mitigation methods to the 3D domain.  

% Significance of the Study 
% - By evaluating the hallucination issues in large-scale 3D point cloud models, this study aims to provide new insights and methods for developing more reliable 3D point cloud models that better meet human needs.  
% % The remainder of this paper is as follows.
% % We first review the related works in Section~\ref{sec::related}.
% % We then present an overview of the simulation system in Section~\ref{sec::system} and elaborate on it in Section~\ref{sec::method}.
% % We conduct experiments in Section~\ref{sec::exp} and conclude our paper in Section~\ref{sec::conclusion}.