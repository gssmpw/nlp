\makeatletter
\renewcommand{\subsubsection}[1]{%
  \par\noindent\textbf{#1}\par % 不换行且加粗
}
\makeatother

\section{Evaluation and Detection}\label{sec::exp}

% \begin{figure*}[t] % [t] 表示图片放置在页面顶部，跨双栏
% \centering
% \includegraphics[width=\textwidth]{figs_evaluation/evaluationProcess.png} % 使用 \textwidth 适应双栏宽度
%      \caption{In the evaluation process, we generate new QA pairs by changing the scene while keeping the questions fixed: different scenes are randomly selected to form new QA pairs. Additionally, we modify the questions while keeping the scene fixed: spatial relationship-related questions are selected, and all QA pairs are transformed such that the object A is the focus. Then, the spatial relationship in the questions is inverted, generating new QA pairs.}
%      \label{fig:evaluationProcess}
% \end{figure*}



\subsection{ Inadequacy of Existing Evaluation Frameworks}
Existing evaluation frameworks for 2D multimodal models, such as POPE~\cite{li2023evaluating}, are insufficient for addressing the challenges in 3D point cloud large language models (LLMs). Since the POPE view uses yes/no questions to evaluate model object hallucinations, which cannot accurately assess the model's understanding of spatial relationships or visual details such as attributes.In Section 3, we assess hallucinations in 3D point cloud models by evaluating object hallucination in description tasks. However, this method has two main limitations: 1) It only detects hallucinations in description tasks, as not all responses involve objects. 2) It doesn't analyze other types of hallucinations, such as attribute or relational errors.

 Therefore, we aim to propose a more stable, fair, and flexible evaluation framework for evaluating hallucinations in 3D point clouds.
% 随机场景实验的结果
\begin{table*}[h]
\centering
\small
\begin{tabular}{llccc|ccc}
\hline
\multicolumn{2}{c} {\multirow{3}{*}{Type}} & \multicolumn{3}{c|}{ll3da} & \multicolumn{3}{c}{3dllm} \\ \cline{3-8}
 
     &   & \multicolumn{2}{c}{Accuracy} & \multirow{2}{*}{\textbf{$HR_{random}$\%}}  & \multicolumn{2}{c}{Accuracy} & \multirow{2}{*}{\textbf{$HR_{random}$\%}}  \\
     
     &   & Rouge-L & Meteor&  & Rouge-L & Meteor&  \\ \hline


\multirow{4}{*}{Relation} & Direction  & 30.62  & 19.53 &\textbf{33.21} & 30.32  & 19.77 &\textbf{30.43}\\
                              & Containment  & 43.28  & 35.27&\textbf{36.89}  & 42.51  & 31.98&\textbf{43.69} \\ 
                              & Contact & 35.08  & 23.55 &\textbf{34.72}  & 35.58  & 24.2 &\textbf{36.79}\\ 
                              & Distance    & 32.02  & 22.71 &\textbf{31.49} & 32.5  & 21.36&\textbf{28.94} \\ \hline
                             
\multirow{3}{*}{Property}  & Color                    & 47.38  & 41.9 &\textbf{62.69} & 51.72   & 47.38&\textbf{61.77} \\ 
                                      & Shape                    & 42.74  & 31.9  &\textbf{49.48}& 44.56  & 32.94 &\textbf{46.39}\\ 
                                      & Size                     & 43.74  & 39.01 &\textbf{74.29} & 47.48  & 37.57&\textbf{51.43} \\ \hline
                                     
\multicolumn{2}{c}{Comparison}                 & 24.75  & 17.65 &\textbf{63.16} & 29.43  & 21.82&\textbf{42.11} \\ 
\multicolumn{2}{c}{Quantity} & 50.18  & 41.84&\textbf{63.93}  & 49.85  & 42.68 &\textbf{53.88} \\ 
\multicolumn{2}{c}{Usage} & 32.22  & 21.62 &\textbf{34.78} & 30.88  & 22.03&\textbf{26.09} \\ 
\multicolumn{2}{c}{Other}  & 37.22  & 31.72&\textbf{38.89}  & 39.8  & 32.82&\textbf{25.93} \\ 
\hline
\end{tabular}
\caption{Model Performance and Hallucination Rate in Random Scenarios.Accuracy refers to the evaluation result between the model's response and the ground truth. $HR_{random}$ is the hallucination rate calculated based on random scenes as defined in Section 5.}
\label{table:randomSceneResult}
\end{table*}
% 随机场景实验的分析图
\begin{figure*}[h]
\begin{center}
\centering
    \includegraphics[width=0.8\linewidth]{figs_evaluation/propertydistribution.png}
     \caption{Impact of Attribute Simplicity on Accuracy.ROUGE represents the average quality of question-answer pairs for a specific item, while the Top 3 Ratio is the proportion of the three most common attributes of the item.}
     \label{fig:topkRatio}
\end{center}
\end{figure*}
\subsection{Proposed Evaluation Framework}
We propose two strategies for detecting hallucinations in 3D point cloud models.\\
\textbf{\emph{Random Point Cloud Pair Evaluation}} ~We select a random point cloud and ask the model the same question on both the original and new point clouds. If the answers are identical, it's considered a hallucination, indicating the model doesn't integrate visual context and just maps the question to a fixed answer.\\
\textbf{\emph{Opposite Question Evaluation}} ~For a fixed point cloud, we ask two Opposite questions (e.g., "What is on the right of the table?" and "What is on the left?"). If the model gives the same answer, it's a hallucination, suggesting the model isn't using the spatial information from the point cloud.

 By employing these two strategies, we aim to identify cases where the model fails to distinguish between spatially different scenarios or produces inconsistent responses to questions.
\subsection{ Inadequacy of Existing Evaluation Frameworks}
The entire pipeline is illustrated in Figure \ref{fig:evaluationProcess}. \\
\textbf{Data Generation}: In the \textbf{change scene} experiment, for each $(Q_i,A_i,S_i)$ pair, we randomly select a different $S_j$ from the scene set to create a new $(Q_i,A_i,\{S_i, S_j\})$ pair dataset.In the \textbf{change question} experiment, we first select questions related to spatial relationships and use GPT-4 to convert each QA pair into a dataset where the answer is an object, resulting in the \textit{scanqa-SR} dataset. For each spatial relationship question $Q_i$ in \textit{scanqa-SR}, we generate its opposite $Q_j$ to form $(\{Q_i, Q_j\},A_i,S_i)$ pairs, creating the \textit{scanqa-SR-Opposite} dataset.\\
\textbf{Experiment}: We then conduct tests using the aforementioned data on different models. 
In Experiment 1, for a given question \( q_i \), we generate two answers, \( a_{ij} \) and \( a_{ik} \), corresponding to two different scenes, \( s_{j} \) and \( s_{k} \), respectively.
We use BLEU-4~\cite{papineni2002bleu}, ROUGE~\cite{lin-2004-rouge}, and METEOR~\cite{banerjee2005meteor} metrics to measure the similarity between two answers. The hallucination rate($HR_{random}$) is calculated as follows:
\begin{equation}
HR_{\text{random}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}(\text{ROUGE}(a_{ij}, a_{ik}) > 0.66)
\end{equation}
In Experiment 2, for a fixed scene \( s_i \), we generate two answers, \( a_{ji} \) and \( a_{jk} \), for two semantically opposite questions, \( q_j \) and \( q_k \).The hallucination rate($HR_{opposite}$) is calculated as follows:
\begin{equation}
HR_{\text{opposite}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}(\text{ROUGE}(a_{ji}, a_{jk}) > 0.66)
\end{equation}
% \textbf{Hallucination Rate}: We use BLEU-4\cite{papineni2002bleu}, ROUGE\cite{lin-2004-rouge}, and METEOR\cite{banerjee2005meteor} metrics to measure the similarity between two answers. We define hallucination as occurring when the ROUGE score between two answers is greater than 0.66, indicating that the answers are semantically identical. The hallucination rate(\(HR\)) is computed as the ratio of hallucinations to the total number of experiments (\(N\)).
% \begin{equation}
% HR_{\text{random}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}(\text{ROUGE}(a_{ij}, a_{ik}) > 0.66)
% \end{equation}
% \begin{equation}
% HR_{\text{opposite}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}(\text{ROUGE}(a_{ji}, a_{jk}) > 0.66)
% \end{equation}

\begin{table*}[h]
\centering
\small
\begin{tabular}{lllcccc}
\hline
Dataset & Task &Model & Bleu-4  & Rouge-L & Meteor   &\textbf{$HR_{opposite}$\%}  \\ \hline
\multirow{6}{*}{scannet}&\multirow{2}{*}{scanqa}&ll3da&7.64&36.56&26.95&/\\ 
& &3dllm&0.80&37.46&28.18&/ \\  \cline{2-7}
&\multirow{2}{*}{scanqa-SR}&ll3da&0.02&13.34&9.68&/ \\ 
&&3dllm&0.0&15.55&10.28&/ \\   \cline{2-7}
&\multirow{2}{*}{scanqa-SR-Opposite}&ll3da&/&/&/&\textbf{56.27} \\ 
&&3dllm&/&/&/&\textbf{52.25} \\ \hline
\end{tabular}
\caption{Model Performance and Hallucination Rate on Semantically Opposite Questions.BLEU-4, ROUGE, and METEOR are evaluation metrics for model response quality based on ground truth, while $HR_{opposite}$ represents the hallucination rate in the opposite-question experiment.}
\label{table:oppositeQuestionResult}
\end{table*}
\section{Evaluation on 3dllm and ll3da}
\subsection{Hallucinations in Random Scene Queries}
We evaluate two models using the approach above. Table \ref{table:randomSceneResult} presents the results for random scenes. ROUGE and METEOR measure performance on ScanQA, while $HR_{random}$ is defined in Section 5.2. The table shows a positive correlation between accuracy and hallucination rate. LL3DA and 3DLLM both exhibit low accuracy and hallucination rates for spatial questions but higher rates for object attributes.\\
This suggests that the model exhibits significant hallucination issues, where it answers questions without considering the visual context, yet its responses appear 'better' or closer to the ground truth. Upon examining the training set, we find that object attributes often align with typical characteristics—for example, tables are usually black, white, or brown, and televisions are typically rectangular. This indicates that the model learns attribute associations due to the homogeneous nature of indoor scenes and the limited diversity of attributes.
% 模型准确率与物品之间的关系
\subsection{Relationship Between Attribute Uniformity and Answer Accuracy}
We plotted Figure \ref{fig:topkRatio} to illustrate the relationship between the uniformity of an object's properties and the accuracy of the answers. For instance, chair color is queried 346($N$) times, with black ($T_1$ times), brown ($T_2$ times), and gray ($T_3$ times) as the most frequent colors. To quantify attribute uniformity, we introduce the "Top-K Ratio," where the Top-3 Ratio for the chair can be calculated as: 
\begin{equation}
\text{Top-3 Ratio} = \frac{T_1 + T_2 + T_3}{N}.
\end{equation} The x-axis of the figure represents the average ROUGE score for questions related to a specific object, with higher ROUGE scores indicating that the questions regarding the object's properties are more easily answered correctly. The three plots from left to right show the relationship between the accuracy of answers and the uniformity of the object's properties, specifically color, shape, and size.
In the plots for color and shape, the distribution of points is approximately linear, confirming a strong positive correlation between the accuracy of the answers and the uniformity of the object's properties. Additionally, we observed that many points clustered near a Top-3 Ratio of 1, suggesting that the dataset contains objects with highly uniform attributes. Such objects tend to exhibit a strong correlation between the object and a specific attribute, which makes it easier for the model to hallucinate the correct attribute. 
\subsection{Hallucinations in Opposite-Question Queries}The results for testing with 
opposite questions within the same scene are presented in Table \ref{table:oppositeQuestionResult}.The ScanQA dataset includes a wide range of QA pairs involving various attributes, spatial relationships, and other data types. In contrast, ScanQA-SR focuses solely on spatial relationships and transforms all QA pairs into those where the answer is the object itself.\\
By comparing the results from these two datasets, we observe that the ROUGE scores for ScanQA-SR are significantly lower than those for ScanQA. This indicates that the model is more prone to errors when dealing with spatial relationship tasks. To investigate whether the model truly understands the meaning of spatial relationships, we created a dataset of opposite questions specifically for spatial relationships. The goal was to assess the model’s ability to handle questions about opposing spatial positions.\\
However, we found that the hallucination rate for both models exceeded 50\%. This suggests that when posed with opposite questions about the same scene, the model has a 50\% chance of giving the same answer. This further demonstrates that the model is prone to errors and hallucinations when handling spatial relationship queries. The results imply that the model may lack a proper visual-semantic understanding of spatial relationships, leading it to answer incorrectly without considering point cloud data.

% \begin{table*}[t]
% \centering
% \begin{tabular}{lllccccc}
% \hline
% Dataset & Task &Model & Bleu-4  & Rouge-L & Meteor & Acc & Hallucination Rate  \\ \hline
% \multirow{6}{*}{scannet}&\multirow{2}{*}{scanqa}&ll3da&7.64&36.56&26.95&26.46&/\\ 
% & &3dllm&0.80&37.46&28.18&26.57&/ \\  \cline{2-8}
% &\multirow{2}{*}{scanqa-object-spatial}&ll3da&0.02&13.34&9.68&7.85&/ \\ 
% &&3dllm&0.0&15.55&10.28&8.34&/ \\   \cline{2-8}
% &\multirow{2}{*}{scanqa-spatial-hallucination}&ll3da&1.32&57.02&56.21&/&56.27 \\ 
% &&3dllm&1.08&55.65&52.60&/&52.25 \\ \hline
% \end{tabular}
% \caption{detection}
% \label{table:performance}
% \end{table*}




% We have merged several spatial relationships into broader categories to simplify classification. The relationships are grouped as follows:

% \begin{itemize}
%     \item \textbf{Directional Relationships}: These include terms such as \textit{opposite}, \textit{under}, \textit{above}, \textit{outside}, \textit{right}, \textit{left}, \textit{front}, \textit{behind}, \textit{in front}, \textit{diagonal}, and \textit{facing}.
%     \item \textbf{Contact/Support Relationships}: This category includes relationships like \textit{resting on}, \textit{lying on}, \textit{standing on}, \textit{on}, \textit{on top of}, \textit{hanging on}, \textit{attached to}, and \textit{against}.
%     \item \textbf{Containment/Surrounding Relationships}: Terms like \textit{inside}, \textit{outside}, \textit{surrounding}, and \textit{enclosed by} fall under this group.
%     \item \textbf{Distance Relationships}: This group includes \textit{far away}, \textit{away}, \textit{near}, \textit{close to}, and \textit{detached from}.
%     \item \textbf{Adjacency/Relative Location Relationships}: This category involves relationships such as \textit{next to}, \textit{adjacent to}, \textit{on each side of the couch}, \textit{between}, and \textit{between two walls}.
%     \item \textbf{Other Descriptive Relationships}: This group includes \textit{centered on}, \textit{off the wall}, and \textit{described as}.
% \end{itemize}
















% \begin{figure*}[ht]
% \begin{center}
% \centering
%     \includegraphics[width=0.93\linewidth]{figs_evaluation/question_reverse_line.jpg}
%      \caption{contract question}
% \end{center}
% \end{figure*}

