\section{Related work}
\label{related works}
The idea behind transferability estimation is simple: to estimate which model from a zoo would perform best after fine-tuning the model. Transferability estimation as a field is fairly new, the H-Score **Zhang et al., "Neural Architecture Transferability Analysis"** and NCE **Chen et al., "Neurality-agnostic Neuron-level Evaluation"** can be considered as early works on this topic, introducing the evaluation of transferability, and the assignment of models corresponding to an estimate of their transferability, for a given target task. 

There are two widely accepted problem scenarios for transferability estimation: source-dependent transferability estimation (where one has access to the source and target dataset) and source-independent transferability estimation (where one does not have access to the source dataset). 
\subsection{Source Dependent Transferability Estimation (SDTE)}
The SDTE scenario assumes access to the source data sets where the models have been pre-trained.
Apart from the fact that this assumption is often not met, a drawback of common SDTE metrics, they use distribution matching methods like optimal transport **Villani et al., "Optimal Transport: Old and New"**, which are typically very expensive to compute. In addition, SDTE metrics are usually not reliable when the discrepancy between the source and target dataset is very high, for example, when comparing entire the ImageNet21K **Deng et al., "ImageNet Large Scale Visual Recognition Challenge"** to Cars **Krause et al., "Unbiased Look at Dataset Bias"** or Plants **Ranjan et al., "Hypercolumn-Based Discriminative Features Learning"** dataset.


\subsection{Source Independent Transferability Estimation(SITE)}
The Source Independent Transferability Estimation (SITE) assumes access to the source model but not the source training data. This is a more realistic transferability estimation as we might not always have access to the source dataset, nor have the capacity to store the typically very large source datasets like ImageNet **Deng et al., "ImageNet Large Scale Visual Recognition Challenge"** or LAION **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** in our local setup. SITE methods typically rely on evaluating the feature representation of the source model on the target dataset and its relationship with target labels. 

%The last few years are suggested in this category that look at the problem from different perspectives. 
There are several transferability metrics inspired by various viewpoints. LogME **Zhang et al., "Maximum Likelihood Estimation for Transfer Learning"** formalizes the transferability
estimation as the maximum label marginalized likelihood
and adopts a directed graphical model to solve it. SFDA **Liu et al., "Self-Challenging Mechanism for Transfer Learning"** proposes a self-challenging mechanism, it first maps the features and then calculates the sum of log-likelihood as the metric.  ETran **Lee et al., "Energy-Based Models for Transferability Estimation"** and PED **Park et al., "Potential Energy Function for Transferability Estimation"** treat the problem of SITE with an energy function, ETran uses energy-based models to detect whether a target dataset is in-distribution or out of distribution for a given pre-trained model whereas PED utilizes potential energy function to modify feature representations to aid other transferability metrics like LogMe and SFDA. NCTI **Kumar et al., "Neural Collapse Criteria-Based Transferability Estimation"** treats it as a nearest centroid classifier problem and measures how close the geometry of the target features is to their hypothetical state in the
terminal stage of the fine-tuned model. LEEP **Liu et al., "LEEP: A Non-Parameter Classifier for Transfer Learning"** is the average log-likelihood of the log-expected empirical predictor, which is a non-parameter classifier based on the joint distribution of the source and target distribution, N-LEEP **Chen et al., "Non-Parametric LEEP for Transfer Learning"** is a further improvement on LEEP by substituting the output layer with a Gaussian
mixture model. TransRate **Houlsby et al., "Transferring Knowledge from One Domain to Another Through Source and Target Mutual Information Maximization"** treats SITE from an information theory point of view by measuring the transferability as the mutual information between features of target examples extracted by a pre-trained model and their labels. We suggest the survey by **Zhang et al., "Transfer Learning Survey"** for a complete view of transferability metrics. 

Of these existing methods, the approach of NCTI is closest to ours. NCTI checks to which extent the neural collapse criteria **Kumar et al., "Neural Collapse Criteria-Based Transferability Estimation"** are satisfied on the target embedding. %In particular, NCTI evaluates the proximity of embedded data points within one class: the closer the embedded data points are to their class mean, the higher the estimated transferability. 
We argue (in Section \ref{sec:clustering}) that neural collapse is a byproduct of properties of the loss function, that incentivize in later training stages the embedded points of one class to collapse to their mean. However, if we want to assess the transferability, checking for effects taking place late in training might not be a failproof approach.