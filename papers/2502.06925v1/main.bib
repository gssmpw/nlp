@inproceedings{conceptvar,
author = {P\'{e}rez, Eduardo and Rendell, Larry A.},
title = {Learning despite concept variation by finding structure in attribute-based data},
year = {1996},
isbn = {1558604197},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Thirteenth International Conference on International Conference on Machine Learning},
pages = {391–399},
numpages = {9},
location = {Bari, Italy},
series = {ICML'96}
}

@article{hess2020softmax,
  title={Softmax-based classification is k-means clustering: Formal proof, consequences for adversarial attacks, and improvement through centroid based tailoring},
  author={Hess, Sibylle and Duivesteijn, Wouter and Mocanu, Decebal},
  journal={arXiv preprint arXiv:2001.01987},
  year={2020}
}

@article{neuralcollapse,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@ARTICLE{clustering,
  author={Bezdek, J.C. and Pal, N.R.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
  title={Some new indexes of cluster validity}, 
  year={1998},
  volume={28},
  number={3},
  pages={301-315},
  keywords={Clustering algorithms;Couplings;Statistics;Volume measurement;Partitioning algorithms;Measurement standards;Clouds;Computer science;Fuzzy logic;Machine intelligence},
  doi={10.1109/3477.678624}}
@article{conceptlearning,
author = {Rendell, Larry and Cho, Howard},
title = {Empirical Learning as a Function of Concept Character},
year = {1990},
issue_date = {Aug. 1990},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {5},
number = {3},
issn = {0885-6125},
url = {https://doi-org.dianus.libr.tue.nl/10.1023/A:1022651406695},
doi = {10.1023/A:1022651406695},
abstract = {Concept learning depends on data character. To discover how, some researchers have used theoretical analysis to relate the behavior of idealized learning algorithms to classes of concepts. Others have developed pragmatic measures that relate the behavior of empirical systems such as ID3 and PLS1 to the kinds of concepts encountered in practice. But before learning behavior can be predicted, concepts and data must be characterized. Data characteristics include their number, error, “size,” and so forth. Although potential characteristics are numerous, they are constrained by the way one views concepts. Viewing concepts as functions over instance space leads to geometric characteristics such as concept size (the proportion of positive instances) and concentration (not too many “peaks”). Experiments show that some of these characteristics drastically affect the accuracy of concept learning. Sometimes data characteristics interact in non-intuitive ways; for example, noisy data may degrade accuracy differently depending on the size of the concept. Compared with effects of some data characteristics, the choice of learning algorithm appears less important: performance accuracy is degraded only slightly when the splitting criterion is replaced with random selection. Analyzing such observations suggests directions for concept learning research.},
journal = {Mach. Learn.},
month = sep,
pages = {267–298},
numpages = {32},
keywords = {Empirical concept learning, concepts as functions, experimental studies}
}

# datasets

@inproceedings{
schuhmann2022laionb,
title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade W Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa R Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=M3Y74vmsMcY}
}
@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}
@article{caltech101,
title = {Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories},
journal = {Computer Vision and Image Understanding},
year = {2007},
note = {Special issue on Generative Model Based Vision},
doi = {https://doi.org/10.1016/j.cviu.2005.09.012},
author = {Li Fei-Fei and Rob Fergus and Pietro Perona},
}
@InProceedings{flowers,
  author       = "Maria-Elena Nilsback and Andrew Zisserman",
  title        = "Automated Flower Classification over a Large Number of Classes",
  booktitle    = "Indian Conference on Computer Vision, Graphics and Image Processing",
  month        = "Dec",
  year         = "2008",
}
@article{plantvillage,
    title = {Identification of plant leaf diseases using a nine-layer deep convolutional neural network},
    journal = {Computers and Electrical Engineering},
    volume = {76},
    pages = {323-338},
    year = {2019},
    issn = {0045-7906},
    doi = {https://doi.org/10.1016/j.compeleceng.2019.04.011},
    url = {https://www.sciencedirect.com/science/article/pii/S0045790619300023},
    author = {Geetharamani G. and Arun Pandian J.},
    keywords = {Artificial intelligence, Deep convolutional neural networks, Deep learning, Dropout, Image augmentation, Leaf diseases identification, Machine learning, Mini batch, Training epoch, Transfer learning},
}
@article{DIBAS,
    doi = {10.1371/journal.pone.0184554},
    author = {Zielinski, Bartosz AND Plichta, Anna AND Misztal, Krzysztof AND Spurek, Przemyslaw AND Brzychczy-Wloch, Monika AND Ochonska, Dorota},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Deep learning approach to bacterial colony classification},
    year = {2017},
    month = {09},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0184554},
    pages = {1-14},
    number = {9}
}

@article{RESISC,
  author    = {Gong Cheng and Junwei Han and Xiaoqiang Lu},
  title     = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},
  journal   = {CoRR},
  volume    = {abs/1703.00121},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.00121},
  eprinttype = {arXiv},
  eprint    = {1703.00121},
  timestamp = {Mon, 02 Dec 2019 09:32:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/ChengHL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cars,
  title = {3D Object Representations for Fine-Grained Categorization},
  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},
  year = {2013},
  address = {Sydney, Australia},
  author = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}
@inproceedings{textures,
  title={THE KTH-TIPS database},
  author={Mario Fritz and E. Hayman and B. Caputo and J. Eklundh},
  year={2004},
  url = {https://www.csc.kth.se/cvap/databases/kth-tips/index.html}
}
@article{100-sports,
    title={100 Sports Image Classification},
    author={Gerald Piosenka},
    url={https://www.kaggle.com/datasets/gpiosenka/sports-classification},
    publisher= {Kaggle}
}

# Transformer models
@inproceedings{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv'e J'egou},
  booktitle={International Conference on Machine Learning},
  year={2020},
}
@article{coat,
  title={Co-Scale Conv-Attentional Image Transformers},
  author={Weijian Xu and Yifan Xu and Tyler A. Chang and Zhuowen Tu},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={9961-9970},
  url={https://api.semanticscholar.org/CorpusID:233219797}
}
@inproceedings{maxvit,
  title={MaxViT: Multi-Axis Vision Transformer},
  author={Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li},
  booktitle={European Conference on Computer Vision},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247939839}
}
@inproceedings{
mobilevit,
title={MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},
author={Sachin Mehta and Mohammad Rastegari},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vh-0sUt8HlG}
}
@article{mvit,
  title={Multiscale Vision Transformers},
  author={Haoqi Fan and Bo Xiong and Karttikeya Mangalam and Yanghao Li and Zhicheng Yan and Jitendra Malik and Christoph Feichtenhofer},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={6804-6815},
  url={https://api.semanticscholar.org/CorpusID:233346705}
}

@inproceedings{XCit,
  title={XCiT: Cross-Covariance Image Transformers},
  author={Alaaeldin El-Nouby and Hugo Touvron and Mathilde Caron and Piotr Bojanowski and Matthijs Douze and Armand Joulin and Ivan Laptev and Natalia Neverova and Gabriel Synnaeve and Jakob Verbeek and Herv{\'e} J{\'e}gou},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235458262}
}

@article{otce,
  title={OTCE: A Transferability Metric for Cross-Domain Cross-Task Representations},
  author={Yang Tan and Yang Li and Shao-Lun Huang},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
}

@inproceedings{szegedy_rethinking_2016,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	booktitle = {{CVPR}},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	year = {2016}
}

@INPROCEEDINGS{LEAD,
  author={Hu, Zixuan and Li, Xiaotong and Tang, Shixiang and Liu, Jun and Hu, Yichun and Duan, Ling-Yu},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={LEAD: Exploring Logit Space Evolution for Model Selection}, 
  year={2024},
  doi={10.1109/CVPR52733.2024.02708}}

@INPROCEEDINGS{PED,
  author={Li, Xiaotong and Hu, Zixuan and Ge, Yixiao and Shan, Ying and Duan, Ling-Yu},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Exploring Model Transferability through the Lens of Potential Energy}, 
  year={2023},
  doi={10.1109/ICCV51070.2023.00500}}

  @inproceedings{meta-album-2022,
    title={Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification},
    author={Ullah, Ihsan and Carrion, Dustin and Escalera, Sergio and Guyon, Isabelle M and Huisman, Mike and Mohr, Felix and van Rijn, Jan N and Sun, Haozhe and Vanschoren, Joaquin and Vu, Phan Anh},
    booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    url = {https://meta-album.github.io/},
    year = {2022}
  }
@INPROCEEDINGS{BCE,
  author={Pándy, Michal and Agostinelli, Andrea and Uijlings, Jasper and Ferrari, Vittorio and Mensink, Thomas},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Transferability Estimation using Bhattacharyya Class Separability}, 
  year={2022},
  doi={10.1109/CVPR52688.2022.00896}}
@INPROCEEDINGS{NCTI,
  author={Wang, Zijian and Luo, Yadan and Zheng, Liang and Huang, Zi and Baktashmotlagh, Mahsa},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability}, 
  year={2023},
  doi={10.1109/ICCV51070.2023.00511}}
@INPROCEEDINGS{NCE,
  author={Tran, Anh and Nguyen, Cuong and Hassner, Tal},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Transferability and Hardness of Supervised Classification Tasks}, 
  year={2019},
  volume={},
  number={},
  pages={1395-1405},
  keywords={Task analysis;Training;Entropy;Data models;Random variables;Machine learning;Computational modeling},
  doi={10.1109/ICCV.2019.00148}}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@INPROCEEDINGS{Hscore,
  author={Bao, Yajie and Li, Yang and Huang, Shao-Lun and Zhang, Lin and Zheng, Lizhong and Zamir, Amir and Guibas, Leonidas},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, 
  title={An Information-Theoretic Approach to Transferability in Task Transfer Learning}, 
  year={2019},
  volume={},
  number={},
  pages={2309-2313},
  keywords={Task analysis;Measurement;Three-dimensional displays;Training;Testing;Correlation;Image recognition;Task transfer learning;Transferability;H-Score;Image recognition & classification},
  doi={10.1109/ICIP.2019.8803726}}


@INPROCEEDINGS{NLEEP,
  author={Li, Yandong and Jia, Xuhui and Sang, Ruoxin and Zhu, Yukun and Green, Bradley and Wang, Liqiang and Gong, Boqing},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Ranking Neural Checkpoints}, 
  year={2021},
  doi={10.1109/CVPR46437.2021.00269}}

@inproceedings{leep,
author = {Nguyen, Cuong V. and Hassner, Tal and Seeger, Matthias and Archambeau, Cedric},
title = {LEEP: a new measure to evaluate transferability of learned representations},
year = {2020},
publisher = {JMLR.org},
abstract = {We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30\% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {676},
numpages = {12},
series = {ICML'20}
}


@inproceedings{logme,
  title={LogME: Practical Assessment of Pre-trained Models for Transfer Learning},
  author={Kaichao You and Yong Liu and Mingsheng Long and Jianmin Wang},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231985863}
}



@inproceedings{li_rethinking_2020,
	title = {Rethinking the {Hyperparameters} for {Fine}-tuning},
	booktitle = {{ICLR}},
	author = {Li, Hao and Chaudhari, Pratik and Yang, Hao and Lam, Michael and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
	year = {2020}
}

@inproceedings{analysis-transferability,
author = {Agostinelli, Andrea and P\'{a}ndy, Michal and Uijlings, Jasper and Mensink, Thomas and Ferrari, Vittorio},
title = {How Stable Are Transferability Metrics Evaluations?},
year = {2022},
doi = {10.1007/978-3-031-19830-4_18},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIV},

}

@inproceedings{transrate,
  title={Frustratingly easy transferability estimation},
  author={Huang, Long-Kai and Huang, Junzhou and Rong, Yu and Yang, Qiang and Wei, Ying},
  booktitle={International Conference on Machine Learning},
  pages={9201--9225},
  year={2022},
  organization={PMLR}
}
@inproceedings{pactran,
author = {Ding, Nan and Chen, Xi and Levinboim, Tomer and Changpinyo, Soravit and Soricut, Radu},
title = {PACTran: PAC-Bayesian Metrics for Estimating the Transferability of Pretrained Models to Classification Tasks},
year = {2022},
isbn = {978-3-031-19829-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi-org.dianus.libr.tue.nl/10.1007/978-3-031-19830-4_15},
doi = {10.1007/978-3-031-19830-4_15},
abstract = {With the increasing abundance of pretrained models in recent years, the problem of selecting the best pretrained checkpoint for a particular downstream classification task has been gaining increased attention. Although several methods have recently been proposed to tackle the selection problem (e.g. LEEP, H-score), these methods resort to applying heuristics that are not well motivated by learning theory. In this paper we present PACTran, a theoretically grounded family of metrics for pretrained model selection and transferability measurement. We first show how to derive PACTran metrics from the optimal PAC-Bayesian bound under the transfer learning setting. We then empirically evaluate three metric instantiations of PACTran on a number of vision tasks (VTAB) as well as a language-and-vision (OKVQA) task. An analysis of the results shows PACTran is a more consistent and effective transferability measure compared to existing selection methods.},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIV},
pages = {252–268},
numpages = {17},
location = {Tel Aviv, Israel}
}

@inproceedings{SFDA,
author = {Shao, Wenqi and Zhao, Xun and Ge, Yixiao and Zhang, Zhaoyang and Yang, Lei and Wang, Xiaogang and Shan, Ying and Luo, Ping},
title = {Not All Models Are Equal: Predicting Model Transferability in a Self-challenging Fisher Space},
year = {2022},
isbn = {978-3-031-19829-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi-org.dianus.libr.tue.nl/10.1007/978-3-031-19830-4_17},
doi = {10.1007/978-3-031-19830-4_17},
abstract = {This paper addresses an important problem of ranking the pre-trained deep neural networks and screening the most transferable ones for downstream tasks. It is challenging because the ground-truth model ranking for each task can only be generated by fine-tuning the pre-trained models on the target dataset, which is brute-force and computationally expensive. Recent advanced methods proposed several lightweight transferability metrics to predict the fine-tuning results. However, these approaches only capture static representations but neglect the fine-tuning dynamics. To this end, this paper proposes a new transferability metric, called Self-challenging Fisher Discriminant Analysis (SFDA), which has many appealing benefits that existing works do not have. First, SFDA can embed the static features into a Fisher space and refine them for better separability between classes. Second, SFDA uses a self-challenging mechanism to encourage different pre-trained models to differentiate on hard examples. Third, SFDA can easily select multiple pre-trained models for the model ensemble. Extensive experiments on 33 pre-trained models of 11 downstream tasks show that SFDA is efficient, effective, and robust when measuring the transferability of pre-trained models. For instance, compared with the state-of-the-art method NLEEP, SFDA demonstrates an average of 59.1\% gain while bringing 22.5x speedup in wall-clock time. The code will be available at .},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIV},
pages = {286–302},
numpages = {17},
keywords = {Image classification, Model ranking, Transfer learning},
location = {Tel Aviv, Israel}
}

@inproceedings{etran,
  title={ETran: Energy-Based Transferability Estimation},
  author={Gholami, Mohsen and Akbari, Mohammad and Wang, Xinglu and Kamranian, Behnam and Zhang, Yong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={18613--18622},
  year={2023}
}

@article{cifar10and100,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
@INPROCEEDINGS{PDE,
  author={Li, Xiaotong and Hu, Zixuan and Ge, Yixiao and Shan, Ying and Duan, Ling-Yu},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Exploring Model Transferability through the Lens of Potential Energy}, 
  year={2023},
  volume={},
  number={},
  pages={5406-5415},
  keywords={Potential energy;Deep learning;Computer vision;Correlation;Codes;Computational modeling;Dynamics},
  doi={10.1109/ICCV51070.2023.00500}}

@misc{ding2024modeltransfersurveytransferability,
      title={Which Model to Transfer? A Survey on Transferability Estimation}, 
      author={Yuhe Ding and Bo Jiang and Aijing Yu and Aihua Zheng and Jian Liang},
      year={2024},
      eprint={2402.15231},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15231}, 
}
# SSL Methods
@INPROCEEDINGS{InDis,
  author={Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination}, 
  year={2018},
  doi={10.1109/CVPR.2018.00393}}

@INPROCEEDINGS{MoCo,
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Momentum Contrast for Unsupervised Visual Representation Learning}, 
  year={2020},
  doi={10.1109/CVPR42600.2020.00975}}
@inproceedings{BYOL,
 author = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
 volume = {33},
 year = {2020}
}
@inproceedings{InfoMin,
 author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {What Makes for Good Views for Contrastive Learning?},
 year = {2020}
}
@inproceedings{pclv1v2,
title={Prototypical Contrastive Learning of Unsupervised Representations},
author={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{selav1v2,
title={Self-labelling via simultaneous clustering and representation learning},
author={Asano YM. and Rupprecht C. and Vedaldi A.},
booktitle={International Conference on Learning Representations},
year={2020},
}

@InProceedings{deepclsuter,
  title={Deep Clustering for Unsupervised Learning of Visual Features},
  author={Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  booktitle={European Conference on Computer Vision},
  year={2018},
}
@inproceedings{swav,
 author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
 year = {2020}
}

@inproceedings{ssltransfer,
    title = {{How Well Do Self-Supervised Models Transfer?}},
    year = {2021},
    booktitle = {CVPR},
    author = {Ericsson, Linus and Gouk, Henry and Hospedales, Timothy M.},
    url = {http://arxiv.org/abs/2011.13377},
    arxivId = {2011.13377}
}
#Others
@inproceedings{10.1145/2736277.2741088,
author = {Vigna, Sebastiano},
title = {A Weighted Correlation Index for Rankings with Ties},
year = {2015},

publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
doi = {10.1145/2736277.2741088},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
series = {WWW '15}
}

@article{complexitysurvey,
author = {Lorena, Ana C. and Garcia, Lu\'{\i}s P. F. and Lehmann, Jens and Souto, Marcilio C. P. and Ho, Tin Kam},
title = {How Complex Is Your Classification Problem? A Survey on Measuring Classification Complexity},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi-org.dianus.libr.tue.nl/10.1145/3347711},
doi = {10.1145/3347711},
abstract = {Characteristics extracted from the training datasets of classification problems have proven to be effective predictors in a number of meta-analyses. Among them, measures of classification complexity can be used to estimate the difficulty in separating the data points into their expected classes. Descriptors of the spatial distribution of the data and estimates of the shape and size of the decision boundary are among the known measures for this characterization. This information can support the formulation of new data-driven pre-processing and pattern recognition techniques, which can in turn be focused on challenges highlighted by such characteristics of the problems. This article surveys and analyzes measures that can be extracted from the training datasets to characterize the complexity of the respective classification problems. Their use in recent literature is also reviewed and discussed, allowing to prospect opportunities for future work in the area. Finally, descriptions are given on an R package named Extended Complexity Library (ECoL) that implements a set of complexity measures and is made publicly available.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {107},
numpages = {34},
keywords = {complexity measures, classification, Supervised machine learning}
}

@ARTICLE{hobasu,
  author={Tin Kam Ho and Basu, M.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Complexity measures of supervised classification problems}, 
  year={2002},
  volume={24},
  number={3},
  pages={289-300},
  keywords={Labeling;Extraterrestrial measurements},
  doi={10.1109/34.990132}}

@book{klogmorov,
author = {Li, Ming and Vitanyi, Paul},
title = {An Introduction to Kolmogorov Complexity and Its Applications},
year = {2019},
isbn = {3030112977},
publisher = {Springer Publishing Company, Incorporated},
edition = {4th},
abstract = {This must-read textbook presents an essential introduction to Kolmogorov complexity (KC), a central theory and powerful tool in information science that deals with the quantity of information in individual objects. The text covers both the fundamental concepts and the most important practical applications, supported by a wealth of didactic features. This thoroughly revised and enhanced fourth edition includes new and updated material on, amongst other topics, the Miller-Yu theorem, the Gacs-Kucera theorem, the Day-Gacs theorem, increasing randomness, short lists computable from an input string containing the incomputable Kolmogorov complexity of the input, the Lovasz local lemma, sorting, the algorithmic full Slepian-Wolf theorem for individual strings, multiset normalized information distance and normalized web distance, and conditional universal distribution. Topics and features: describes the mathematical theory of KC, including the theories of algorithmic complexity and algorithmic probability; presents a general theory of inductive reasoning and its applications, and reviews the utility of the incompressibility method; covers the practical application of KC in great detail, including the normalized information distance (the similarity metric) and information diameter of multisets in phylogeny, language trees, music, heterogeneous files, and clustering; discusses the many applications of resource-bounded KC, and examines different physical theories from a KC point of view; includes numerous examples that elaborate the theory, and a range of exercises of varying difficulty (with solutions); offers explanatory asides on technical issues, and extensive historical sections; suggests structures for several one-semester courses in the preface. As the definitive textbook on Kolmogorov complexity, this comprehensive and self-contained work is an invaluable resource for advanced undergraduate students, graduate students, and researchers in all fields of science.}
}

@INPROCEEDINGS {repcomplexity,
author = { Kam Ho, Tin },
booktitle = { 2022 26th International Conference on Pattern Recognition (ICPR) },
title = {{ Complexity of Representations in Deep Learning }},
year = {2022},
volume = {},
ISSN = {},
pages = {2657-2663},
abstract = { Deep neural networks use multiple layers of functions to map an object represented by an input vector progressively to different representations, and with sufficient training, eventually to a single score for each class that is the output of the final decision function. Ideally, in this output space, the objects of different classes achieve maximum separation. Motivated by the need to better understand the inner working of a deep neural network, we analyze the effectiveness of the learned representations in separating the classes from a data complexity perspective. Using a simple complexity measure, a popular benchmarking task, and a well-known architecture design, we show how the data complexity evolves through the network, how it changes during training, and how it is impacted by the network design and the availability of training samples. We discuss the implications of the observations and the potentials for further studies. },
keywords = {Training;Deep learning;Neural networks;Training data;Robustness;Complexity theory;Behavioral sciences},
doi = {10.1109/ICPR56361.2022.9956594},
url = {https://doi.ieeecomputersociety.org/10.1109/ICPR56361.2022.9956594},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Aug}

@inproceedings{cupy_learningsys2017,
  author       = "Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman",
  title        = "CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations",
  booktitle    = "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)",
  year         = "2017",
  url          = "http://learningsys.org/nips17/assets/papers/paper_16.pdf"
}

@article{cuml,
  title={Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence},
  author={Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
  journal={arXiv preprint arXiv:2002.04803},
  year={2020}
}
@unpublished{unpubot,
  author = "",
  title  = "",
  note = "Unpublished work"
}