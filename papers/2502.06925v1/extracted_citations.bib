@INPROCEEDINGS{Hscore,
  author={Bao, Yajie and Li, Yang and Huang, Shao-Lun and Zhang, Lin and Zheng, Lizhong and Zamir, Amir and Guibas, Leonidas},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, 
  title={An Information-Theoretic Approach to Transferability in Task Transfer Learning}, 
  year={2019},
  volume={},
  number={},
  pages={2309-2313},
  keywords={Task analysis;Measurement;Three-dimensional displays;Training;Testing;Correlation;Image recognition;Task transfer learning;Transferability;H-Score;Image recognition & classification},
  doi={10.1109/ICIP.2019.8803726}}

@INPROCEEDINGS{NCE,
  author={Tran, Anh and Nguyen, Cuong and Hassner, Tal},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Transferability and Hardness of Supervised Classification Tasks}, 
  year={2019},
  volume={},
  number={},
  pages={1395-1405},
  keywords={Task analysis;Training;Entropy;Data models;Random variables;Machine learning;Computational modeling},
  doi={10.1109/ICCV.2019.00148}}

@INPROCEEDINGS{NCTI,
  author={Wang, Zijian and Luo, Yadan and Zheng, Liang and Huang, Zi and Baktashmotlagh, Mahsa},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability}, 
  year={2023},
  doi={10.1109/ICCV51070.2023.00511}}

@INPROCEEDINGS{NLEEP,
  author={Li, Yandong and Jia, Xuhui and Sang, Ruoxin and Zhu, Yukun and Green, Bradley and Wang, Liqiang and Gong, Boqing},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Ranking Neural Checkpoints}, 
  year={2021},
  doi={10.1109/CVPR46437.2021.00269}}

@INPROCEEDINGS{PED,
  author={Li, Xiaotong and Hu, Zixuan and Ge, Yixiao and Shan, Ying and Duan, Ling-Yu},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Exploring Model Transferability through the Lens of Potential Energy}, 
  year={2023},
  doi={10.1109/ICCV51070.2023.00500}}

@inproceedings{SFDA,
author = {Shao, Wenqi and Zhao, Xun and Ge, Yixiao and Zhang, Zhaoyang and Yang, Lei and Wang, Xiaogang and Shan, Ying and Luo, Ping},
title = {Not All Models Are Equal: Predicting Model Transferability in a Self-challenging Fisher Space},
year = {2022},
isbn = {978-3-031-19829-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi-org.dianus.libr.tue.nl/10.1007/978-3-031-19830-4_17},
doi = {10.1007/978-3-031-19830-4_17},
abstract = {This paper addresses an important problem of ranking the pre-trained deep neural networks and screening the most transferable ones for downstream tasks. It is challenging because the ground-truth model ranking for each task can only be generated by fine-tuning the pre-trained models on the target dataset, which is brute-force and computationally expensive. Recent advanced methods proposed several lightweight transferability metrics to predict the fine-tuning results. However, these approaches only capture static representations but neglect the fine-tuning dynamics. To this end, this paper proposes a new transferability metric, called Self-challenging Fisher Discriminant Analysis (SFDA), which has many appealing benefits that existing works do not have. First, SFDA can embed the static features into a Fisher space and refine them for better separability between classes. Second, SFDA uses a self-challenging mechanism to encourage different pre-trained models to differentiate on hard examples. Third, SFDA can easily select multiple pre-trained models for the model ensemble. Extensive experiments on 33 pre-trained models of 11 downstream tasks show that SFDA is efficient, effective, and robust when measuring the transferability of pre-trained models. For instance, compared with the state-of-the-art method NLEEP, SFDA demonstrates an average of 59.1\% gain while bringing 22.5x speedup in wall-clock time. The code will be available at .},
booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIV},
pages = {286–302},
numpages = {17},
keywords = {Image classification, Model ranking, Transfer learning},
location = {Tel Aviv, Israel}
}

@inproceedings{cars,
  title = {3D Object Representations for Fine-Grained Categorization},
  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},
  year = {2013},
  address = {Sydney, Australia},
  author = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}

@misc{ding2024modeltransfersurveytransferability,
      title={Which Model to Transfer? A Survey on Transferability Estimation}, 
      author={Yuhe Ding and Bo Jiang and Aijing Yu and Aihua Zheng and Jian Liang},
      year={2024},
      eprint={2402.15231},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15231}, 
}

@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@inproceedings{leep,
author = {Nguyen, Cuong V. and Hassner, Tal and Seeger, Matthias and Archambeau, Cedric},
title = {LEEP: a new measure to evaluate transferability of learned representations},
year = {2020},
publisher = {JMLR.org},
abstract = {We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30\% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {676},
numpages = {12},
series = {ICML'20}
}

@inproceedings{logme,
  title={LogME: Practical Assessment of Pre-trained Models for Transfer Learning},
  author={Kaichao You and Yong Liu and Mingsheng Long and Jianmin Wang},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231985863}
}

@article{neuralcollapse,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@article{otce,
  title={OTCE: A Transferability Metric for Cross-Domain Cross-Task Representations},
  author={Yang Tan and Yang Li and Shao-Lun Huang},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
}

@article{plantvillage,
    title = {Identification of plant leaf diseases using a nine-layer deep convolutional neural network},
    journal = {Computers and Electrical Engineering},
    volume = {76},
    pages = {323-338},
    year = {2019},
    issn = {0045-7906},
    doi = {https://doi.org/10.1016/j.compeleceng.2019.04.011},
    url = {https://www.sciencedirect.com/science/article/pii/S0045790619300023},
    author = {Geetharamani G. and Arun Pandian J.},
    keywords = {Artificial intelligence, Deep convolutional neural networks, Deep learning, Dropout, Image augmentation, Leaf diseases identification, Machine learning, Mini batch, Training epoch, Transfer learning},
}

