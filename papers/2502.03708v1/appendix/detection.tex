\section{Additional (quantitative) detection and steering results}
\label{app: additional results}
\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}l|c|c|c|c|c|c|c|c@{}|}
\cline{2-9}
& \multicolumn{8}{c|}{\textbf{Dataset (train$/$test set sizes)}}\\
\cline{2-9}
& \multicolumn{2}{c|}{HE} & \multicolumn{2}{c|}{HE(Gen.)} & \multicolumn{2}{c|}{FAVA} & \multicolumn{2}{c|}{HE-Wild} \\
& \multicolumn{2}{c|}{(12051$/$3997)} & \multicolumn{2}{c|}{(10307$/$2254)} & \multicolumn{2}{c|}{(7360$/$100)} & \multicolumn{2}{c|}{(550$/$50)} \\
\hline
\textbf{Classifier} & Acc. (\%) & F1 & Acc. (\%) & F1 & Acc. (\%) & F1 & Acc. (\%) & F1 \\
\hline
\multicolumn{9}{@{}l|}{\textit{Aggregated layers}} \\
RFM/RFM & 91.9$\pm$0.2 & .920$\pm$.003 & 82.8$\pm$0.6 & .712$\pm$.009 & 66.4$\pm$4.0 & .633$\pm$.050 & 58.5$\pm$7.3 & .559$\pm$.067 \\
RFM/Lin.Reg. & 90.2$\pm$0.3 & .904$\pm$.002 & \textbf{85.0$\pm$0.2} & \textbf{.721$\pm$.004} & 62.6$\pm$3.6 & .597$\pm$.045 & 65.9$\pm$6.9 & .636$\pm$.061 \\
Lin.Reg./RFM & 88.5$\pm$0.5 & .887$\pm$.005 & 81.8$\pm$0.4 & .692$\pm$.006 & 64.3$\pm$2.6 & .594$\pm$.050 & 64.0$\pm$6.0 & .620$\pm$.059 \\
Lin.Reg. & 89.3$\pm$0.4 & .896$\pm$.004 & 83.4$\pm$0.5 & .695$\pm$.012 & 60.5$\pm$3.9 & .580$\pm$.040 & 55.8$\pm$6.1 & .545$\pm$.058 \\
Logistic & \textbf{92.4$\pm$0.2} & \textbf{.922$\pm$.002} & 80.2$\pm$0.3 & .602$\pm$.009 & 57.7$\pm$5.3 & .437$\pm$.065 & 58.9$\pm$6.0 & .573$\pm$.058 \\
\hline
\multicolumn{9}{@{}l|}{\textit{Best single layer}} \\
RFM & 89.4$\pm$0.3 & .895$\pm$.003 & 83.0$\pm$0.2 & .679$\pm$.006 & 58.3$\pm$5.9 & .550$\pm$.073 & 66.4$\pm$6.7 & .625$\pm$.064 \\
Lin.Reg. & 89.1$\pm$0.3 & .894$\pm$.003 & 79.1$\pm$0.1 & .651$\pm$.007 & 54.3$\pm$5.4 & .508$\pm$.056 & 65.4$\pm$6.2 & .612$\pm$.062 \\
Logistic & 90.9$\pm$0.3 & .907$\pm$.003 & 72.8$\pm$0.6 & .342$\pm$.018 & 53.3$\pm$3.7 & .053$\pm$.053 & \textbf{66.5$\pm$6.5} & \textbf{.648$\pm$.063} \\
LAT \citep{representation_engineering} & 76.2$\pm$0.8 & .767$\pm$.012 & 83.2$\pm$0.3 & .687$\pm$.008 & 56.7$\pm$4.0 & .568$\pm$.099 & NA & NA \\
LLM-Check \citep{llmselfcorrect} & 51.7$\pm$0.7 & .248$\pm$.104 & 67.4$\pm$0.4 & .038$\pm$.019 & \textbf{68.0$\pm$3.7} & .611$\pm$.078 & NA & NA \\
\hline
\multicolumn{9}{@{}l|}{\textit{Judge models}} \\
Base model & 52.7$\pm$0.6 & .120$\pm$.003 & 65.6$\pm$0.3 & .127$\pm$.017 & 64.5$\pm$4.6 & .450$\pm$.072 & 43.5$\pm$6.6 & .403$\pm$.062 \\
GPT-4o \citep{gpt4o} & 65.2$\pm$0.7 & .661$\pm$.006 & 65.2$\pm$0.7 & .661$\pm$.006 & 66.9$\pm$3.1 & \textbf{.660$\pm$.039} & 46.2$\pm$4.9 & .404$\pm$.044 \\
\hline
\end{tabular}
\caption{\textbf{Accuracy and F1 scores for hallucination detection on instruction-tuned Gemma-2-9B}. The larger values are better and the top score per dataset is in bold. The datasets included are HaluEval \citep{halueval}, HaluEval (the `general' detection subset), FAVA \citep{fava}, and HaluEval-Wild \citep{haluevalwild}.}
\label{fig: f1+accuracies, halluc detection, gemma}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}l|ccc|ccc|ccc@{}|}
\cline{2-10}
& \multicolumn{9}{c|}{\textbf{Dataset and Model}} \\
\cline{2-10}
& \multicolumn{3}{c|}{\textbf{AgentHarm}} & \multicolumn{3}{c|}{\textbf{ToxicChat}} & \multicolumn{3}{c@{}|}{\textbf{TruthGen}} \\
\cline{2-10}\\[-8pt]
\textbf{Classifier} & \shortstack{Llama\\8B} & \shortstack{Llama\\70B} & \shortstack{Gemma\\9B} & \shortstack{Llama\\8B} & \shortstack{Llama\\70B} & \shortstack{Gemma\\9B} & \shortstack{Llama\\8B} & \shortstack{Llama\\70B} & \shortstack{Gemma\\9B} \\
\cline{2-10}
\hline
\multicolumn{10}{@{}l|}{\textit{Aggregated layers}} \\
RFM/RFM & 73.30 & 83.81 & 73.30 & 96.12 & 96.51 & 95.82 & 87.22 & 90.04 & 87.85 \\
RFM/Lin.Reg. & \textbf{82.10} & 84.09 & \textbf{79.83} & 95.86 & 96.56 & 96.31 & 88.90 & 90.63 & 88.95 \\
Lin.Reg./RFM & 74.72 & 77.56 & 76.70 & 96.23 & 96.38 & 95.76 & 86.79 & 90.08 & 86.08 \\
Lin.Reg./Lin.Reg. & 80.11 & 82.95 & 73.86 & 96.19 & 96.79 & 96.35 & 88.44 & 90.68 & 88.69 \\
Logistic/Logistic & 76.14 & 78.13 & 76.42 & 96.20 & 96.63 & 96.29 & 87.97 & \textbf{91.05} & 86.62 \\
\hline
\multicolumn{10}{@{}l|}{\textit{Best single layer}} \\
RFM & \textbf{82.10} & \textbf{85.23} & 73.30 & 95.31 & 96.21 & 95.75 & 87.89 & 90.84 & 87.89 \\
Lin.Reg. & 78.41 & 68.18 & 68.75 & 95.87 & 96.59 & 96.05 & 88.65 & 90.76 & 87.22 \\
Logistic & 65.63 & 76.99 & 71.31 & 95.65 & 96.21 & 96.09 & 87.81 & \textbf{91.05} & 85.19 \\
LAT & 51.99 & 51.70 & 50.85 & 92.58 & 92.60 & 92.58 & 85.87 & 88.69 & 82.28 \\
\hline
\multicolumn{10}{@{}l|}{\textit{Judge models}} \\
Base Model & 44.89 & 57.39 & 50.28 & 92.01 & 94.85 & 92.80 & 85.65 & 89.70 & 51.90 \\
GPT-4o & \textemdash & 53.13 & \textemdash & \textemdash & 94.98 & \textemdash & \textemdash & 90.38 & \textemdash \\
ToxicChat-T5-Large & NA & NA & NA & \textemdash & \textbf{96.83} & \textemdash & NA & NA & NA \\
\hline
\end{tabular}
\caption{\textbf{Accuracies for harmful, toxic, and dishonest content detection across models}. Models are Llama-3.1-8B-it, Llama-3.3-70B-it, and Gemma-2-9B-it. The datasets are AgentHarm, ToxicChat, and TruthGen. \textbf{$\,^*$}Many of the truthful/untruthful responses in TruthGen were themselves generated by GPT-3.5 and GPT-4. \textbf{$^{\,**}$}ToxicChat-T5-Large was designed and optimized specifically for the ToxicChat benchmark.}
\label{fig: accuracies, non-halluc detection combined}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}l|cc|cc|cc|cc|cc|cc|cc|cc|cc@{}|}
\cline{2-19}
& \multicolumn{18}{c|}{\textbf{Standard Errors by Dataset and Model}} \\
\cline{2-19}
& \multicolumn{6}{c|}{\textbf{AgentHarm}} & \multicolumn{6}{c|}{\textbf{ToxicChat}} & \multicolumn{6}{c@{}|}{\textbf{TruthGen}} \\
\cline{2-19}\\[-8pt]
& \multicolumn{2}{c|}{\shortstack{Llama\\ 8B}} & \multicolumn{2}{c|}{\shortstack{Llama\\ 70B}} & \multicolumn{2}{c|}{\shortstack{Gemma\\ 9B}} & \multicolumn{2}{c|}{\shortstack{Llama\\ 8B}} & \multicolumn{2}{c|}{\shortstack{Llama\\ 70B}} & \multicolumn{2}{c|}{\shortstack{Gemma\\ 9B}} & \multicolumn{2}{c|}{\shortstack{Llama\\ 8B}} & \multicolumn{2}{c|}{\shortstack{Llama\\ 70B}} & \multicolumn{2}{c@{}|}{\shortstack{Gemma\\ 9B}} \\
\textbf{Classifier} & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\
\cline{2-19}
\hline
\multicolumn{19}{@{}l|}{\textit{Aggregated layers}} \\
RFM/RFM & - & - & - & - & - & - & .17 & .010 & .26 & .015 & .12 & .012 & .74 & .009 & .77 & .011 & 1.05 & .015 \\
RFM/Lin.Reg. & - & - & - & - & - & - & .28 & .033 & .16 & .011 & .10 & .009 & .78 & .005 & .99 & .011 & 0.56 & .011 \\
Lin.Reg./RFM & - & - & - & - & - & - & .13 & .016 & .14 & .007 & .24 & .017 & 1.14 & .010 & .91 & .011 & 1.31 & .019 \\
Lin.Reg./Lin.Reg. & - & - & - & - & - & - & .10 & .012 & .07 & .007 & .05 & .007 & .21 & .006 & .75 & .010 & 1.22 & .016 \\
Logistic/Logistic & - & - & - & - & - & - & .10 & .012 & .15 & .015 & .05 & .007 & .72 & .007 & .91 & .008 & 0.51 & .013 \\
\hline
\multicolumn{19}{@{}l|}{\textit{Best single layer}} \\
RFM & - & - & - & - & - & - & .33 & .054 & .14 & .014 & .10 & .015 & .51 & .010 & .97 & .010 & 1.44 & .014 \\
Lin.Reg. & - & - & - & - & - & - & .14 & .015 & .14 & .014 & .21 & .021 & .65 & .005 & 1.03 & .010 & 0.98 & .014 \\
Logistic & - & - & - & - & - & - & .09 & .018 & .17 & .018 & .20 & .027 & .43 & .004 & .95 & .008 & 0.93 & .010 \\
LAT & - & - & - & - & - & - & .04 & .005 & .21 & .066 & .04 & .005 & 1.21 & .018 & .85 & .014 & 1.68 & .022 \\
\hline
\multicolumn{19}{@{}l|}{\textit{Judge models}} \\
Base Model & - & - & - & - & - & - & - & - & - & - & - & - & 1.16 & .008 & .96 & .012 & 0.96 & .012 \\
GPT-4o & - & - & - & - & - & - & - & - & - & - & - & - & .91 & .011 & .91 & .011 & 0.91 & .011 \\
ToxicChat-T5-Large & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
\hline
\end{tabular}
\caption{\textbf{Standard errors for accuracies and F1 scores across models and datasets}. Dashes (-) indicate no standard error was reported in the original data. }
\label{tab: standard errors}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Mandarin} & \textbf{German} & \textbf{Spanish} \\ \midrule
\textbf{RFM} & 3.92$\pm$0.33 & 3.76$\pm$0.55 & 3.87$\pm$0.39 \\
\textbf{Linear} & \textbf{3.95$\pm$0.21} & 3.75$\pm$0.57 & 3.81$\pm$0.48 \\
\textbf{Logistic} & \textbf{3.95$\pm$0.26} & 3.75$\pm$0.55 & 3.86$\pm$0.42 \\
\textbf{Difference-in-means} & 3.94$\pm$0.27 & 3.76$\pm$0.53 & 3.86$\pm$0.44 \\
\textbf{PCA} & 3.91$\pm$0.34 & \textbf{3.79$\pm$0.57} & \textbf{3.89$\pm$0.34} \\
\bottomrule
\end{tabular}
\caption{Comparison of average scores given by GPT-4o judge model for language translation by steering Llama-3.1-8B-it. Maximum score of 4 for a perfect translation. Steering direction is extracted using RFM, Linear or Logistic regression, Difference-in-means, and PCA.}
\label{fig: language steering results, llama}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method}         & \textbf{Mandarin}          & \textbf{German}           & \textbf{Spanish}          \\ \midrule
\textbf{RFM}            & 3.80$\pm$0.58          & 3.92$\pm$0.30           & \textbf{3.93$\pm$0.29}           \\ 
\textbf{Linear}         & 3.76$\pm$0.66          & 3.89$\pm$0.46           & 3.82$\pm$0.71           \\
\textbf{Logistic}       & 3.83$\pm$0.49          & 3.93$\pm$0.29           & 3.92$\pm$0.30           \\
\textbf{Difference-in-means}& 3.82$\pm$0.55          & \textbf{3.94$\pm$0.27}           & \textbf{3.93$\pm$0.25}           \\
\textbf{PCA}            & \textbf{3.89$\pm$0.34}          & 3.86$\pm$0.48           & 3.85$\pm$0.51           \\
\bottomrule
\end{tabular}
\caption{Comparison of average scores given by GPT-4o judge model for language translation by steering Gemma-2-9B-it. Maximum score of 4 for a perfect translation. Steering direction is extracted using RFM, Linear or Logistic regression, Difference-in-means, and PCA.}
\label{fig: language steering results, gemmma}
\end{table}