\section{Background on linear and nonlinear methods}
\label{app: rfms}

\subsection{Kernel machines}
A kernel machine is a supervised non-parametric machine learning model. Specifically, we apply Kernel Ridge Regression (KRR) to learning a mapping between inputs $X \in \Real^{n \times d}$ and labels $y \in \Real^n$. KRR is parametrized by a kernel function $K : \Real^{d} \times \Real^{d} \rightarrow \Real$ that produces a kernel evaluation between two samples of dimension $d$. An example of such a function might be the commonly-used Gaussian kernel, which evaluates to $K(x,x') = \exp\round{-\|x-x'\|^2}$ for two samples $x,x'$. Abusing notation, we write $K(X,Z) \in \Real^{n \times m}$ for datasets $X \in \Real^{n \times d}$ and $Z \in \Real^{m \times d}$ to indicate the matrix of kernel evaluations between all pairs of points in $X$ and $Z$. The kernel machine is trained by generating a set of coefficients $\alpha = K(X,X)^{-1} y$. Once trained, the kernel machine on test points $X_{\mathrm{test}} \in \Real^{m \times d}$ admits the following closed form:
\begin{align*}
    f_{\mathrm{KRR}}(X_{\mathrm{test}} ) = K(X_{\mathrm{test}},X)\, \alpha~. 
\end{align*}

\subsection{Recursive Feature Machines (RFMs)}

RFMs are an algorithm for enabling feature learning in general machine learning models by using the Average Gradient Outer Product (AGOP). We use RFMs in conjunction with kernel machines and learn concept vectors through eigenvectors of the Average Gradient Outer Product (AGOP). RFMs have been shown to achieve state-of-the-art performance on tabular dataset benchmarks and close the performance gaps with neural networks on certain vision tasks \cite{rfm_science}. Moreover, RFMs have achieved significant successes for prediction in scientific applications \citep{wu2024inverse, aristoff2024fast, shen2024interpretableqsprmodelingusing}. RFMs combine recursive estimation of the AGOP with exact kernel regression to enable feature learning in non-neural models. As an intuitive motivation, when the target function depends on only a low-dimensional subspace of the input data, the AGOP generated by RFM will capture this low-rank structure \citep{rfm_science}. Hence, RFM can be thought of as a nonlinear prediction method, which, as a byproduct, performs ``supervised PCA'' through its AGOP matrix. We define the AGOP operator here:
\begin{definition}[Average Gradient Outer Product (AGOP)] Given any differentiable predictor, $f: \Real^d \rightarrow \Real$, trained on $n$ data points $x^{(1)}, \ldots, x^{(n)} \in \Real^d$, the Average Gradient Outer Product operator of the predictor on the data, written $G: \round{\Real^d \rightarrow \Real} \rightarrow \Real^{d \times d}$ is written,
\begin{align}
G(f) = \frac{1}{n}\sum_{i=1}^n \nabla_x f(x^{(i)}) \nabla_x f(x^{(i)})\tran \in \Real^{d \times d}~.
\end{align}
\end{definition}
As RFM implements feature learning through the AGOP, it is an especially useful probe when low-rank structure exists in the data, yet either (1) the target function is a non-linear function of the projection of the data onto this low-dimensional space, or (2) the rank of this subspace is strictly greater than $1$ (and therefore a linear model cannot learn the target). The RFM algorithm as used with kernel machines is written formally in Algorithm~\ref{alg: RFM}. Note this formulation generalizes that in Section~\ref{sec: techniques} by equivalently treating the modification of inner products in the kernel function with the $M$ matrix as transforming the data by $M^{1/2}$.

\begin{algorithm}[h]
\caption{Recursive Feature Machine (RFM) \citep{rfm_science}}\label{alg: RFM}
\begin{algorithmic}
\Require $(X, y), K, \tau$ \Comment{Train data: $x^{(1)},\ldots, x^{(n)} \in \Real^{d}, y \in \Real^{n}$, kernel: $K$, iters.: $\tau$, ridge: $\mu \in \Real$}
\State $M_0 = I_{d}$
\State $X_0 = [x^{(1)},\ldots, x^{(n)}]\tran \in \Real^{n \times d}$
\For{$t = 0,\ldots,\tau-1$}
    \State Solve $\alpha \leftarrow \round{K(X_t,X_t) + \mu I}^{-1}y$ \Comment{Define predictor $f^{(t)}(x) = K(M^{1/2}_tx,X_t)\alpha$}
    \State $M_{t+1} \leftarrow \frac{1}{n}\sum_{i=1}^n \nabla_x f^{(t)}(x^{(i)}) \nabla_x f^{(t)}(x^{(i)})\tran \in \Real^{d \times d}$ 
    \Comment{Feature matrix (AGOP) computation}
    \State $X_{t+1} \leftarrow X_0 M^{1/2}_{t+1}$
\EndFor
\\\Return $\alpha, M_{\tau-1}$ \Comment{Kernel regression coefficients: $\alpha$, feature matrix: $M_{\tau-1}$}
\end{algorithmic}
\end{algorithm}

RFM accepts training data and a kernel function $k : \Real^{d} \times \Real^{d} \rightarrow \Real$. For each iteration of RFM, the algorithm (1) performs kernel ridge regression to obtain coefficients $\alpha \in \Real^n$, (2) computes the feature matrix $M$ from the AGOP of the kernel predictor, (3) transforms the data with the square root of the kernel matrix. When the labels consist of multiple output dimensions, we additionally average the AGOP over output classes. The algorithm then outputs the coefficients and the feature matrix of the predictor in the final iteration.

The concept vectors we extract for detection and steering are contained in the top eigenvectors of the AGOP. As the AGOP is the covariance of the gradients of the predictor on the training data, the top eigenvectors of the AGOP matrix reflect which directions heavily influence the outputs of the predictor. When the predictor is trained on enough data to predict the strength of a certain concept in activation space, the top eigendirections of the AGOP correspond to the directions in activation space distinguish the strength of that concept. Therefore, we expect the top eigenvectors of the AGOP are directions corresponding to the concepts being measured.

\subsection{Unsupervised probes: PCA and Difference-in-means} 
\label{app: pca}
Principal components analysis (PCA) is applied to extract concept vectors in this context in the following way. This method requires a set of $n$ pairs of activations from positive and negative examples of a concept, $\{(a^{(+)}_1,a^{(-)}_1),\ldots, (a^{(+)}_n,a^{(-)}_n)\}$. From these pairs, we construct $n$ difference vectors $v_i = b_i \cdot \round{a^{(+)}_i - a^{(-)}_i}$ where $b_i$ are i.i.d. Bernoulli random variables samples uniformly from $\{-1,+1\}$. Then we extract the top principal components of the difference vectors $v_1, \ldots, v_n$.

For Difference-in-means (DM), we do not require pairs of prompts. Instead given a set of positive examples $\{a^{(+)}_i\}_{i=1}^n$ and negative examples $\{a^{(-)}_i\}_{i=1}^m$. we estimate the positive and negative means $\mu^{(+)} = \frac{1}{n} \sum_{i=1}^n a^{(+)}_i$, $\mu^{(-)} = \frac{1}{m} \sum_{i=1}^n a^{(-)}_i$. The single concept direction we extract is $v = \mu^{(+)} - \mu^{(-)}$.\\

\noindent Note both of these approaches are limited to binary labeled concepts and do not apply to labels that are gradated.

\subsection{Classification metrics}
\label{app: metrics}
For classification tasks where the number of samples belonging to each class drastically differs (called an imbalanced task), standard accuracy can be a misleading metric for the performance of our learning algorithm. Hence, for our detection tasks we additionally report the \textit{F1 score}, which is the harmonic mean of the precision and recall scores. Namely, 
$$F_1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}},$$ 
where precision is defined as $$\text{precision} = \frac{\text{true positives}}{\text{true positives} + \text{false positives}}$$ and recall is defined as 
$$\text{recall} = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}.$$ 
Precision measures the proportion of true positive predictions among all positive predictions, and recall measures the proportion of true positives that were correctly identified. The F1 score ranges from 0 to 1, with 1 indicating perfect precision and recall.

\section{Method Details}
\label{app: method details}

We provide pseudo-code for our detection algorithm (Algorithm~\ref{alg: aggregation}) and our steering algorithm (Algorithm~\ref{alg: steering}).

\subsection{LLM Concept Detection}
\subsubsection{Function Specifications}
\begin{description}
\item[\texttt{Blocks($f_\theta$)}] \hfill \\
\textbf{Description:} Returns an iterator over the transformer blocks in the LLM. \\
\textbf{Parameters:}
\begin{itemize}
\item $f_\theta$: The LLM model with parameters $\theta$
\end{itemize}
\textbf{Returns:} Iterator yielding indices of transformer blocks from 1 to $L$ where $L$ is the total number of layers

\item[\texttt{GetActivations($f_\theta$, $X$, $\ell$)}] \hfill \\
\textbf{Description:} Extracts intermediate activations from a specific layer in the model. \\
\textbf{Parameters:}
\begin{itemize}
\item $f_\theta$: The LLM model with parameters $\theta$
\item $X$: Input data to process
\item $\ell$: Layer number to extract activations from
\end{itemize}
\textbf{Returns:} Activation tensors $A_\ell$ from the specified layer
\item[\texttt{TrainProbe($A_\ell$, $Y$)}] \hfill \\
\textbf{Description:} Trains a (non)linear probe to detect concepts from layer activations. \\
\textbf{Parameters:}
\begin{itemize}
\item $A_\ell$: Layer activations from the model
\item $Y$: Ground truth concept labels
\end{itemize}
\textbf{Returns:} Trained probe model $P_\ell$ for concept detection
\item[\texttt{Concatenate($[T_1,\ldots,T_n]$)}] \hfill \\
\textbf{Description:} Combines multiple tensors into a single tensor along a specified dimension. \\
\textbf{Parameters:}
\begin{itemize}
\item $[T_1,\ldots,T_n]$: List of tensors to concatenate
\end{itemize}
\textbf{Returns:} Single concatenated tensor $F \in \mathbb{R}^{m \times d}$
\item[\texttt{TrainAggregator($F$, $Y$)}] \hfill \\
\textbf{Description:} Trains a model to aggregate concept predictions across layers. \\
\textbf{Parameters:}
\begin{itemize}
\item $F$: Concatenated concept scores from all layers
\item $Y$: Ground truth labels
\end{itemize}
\textbf{Returns:} Trained aggregator model for combining layer-wise predictions
\end{description}

\begin{algorithm}[h]
\caption{LLM Concept Detection by Aggregation}
\label{alg: aggregation}
\begin{algorithmic}[1]
\Require LLM $f_\theta$, training data $(X_{train}, Y_{train})$, validation data $(X_{val}, Y_{val})$
\Procedure{Fit}{}
    \For{block $l$ in Blocks($f_\theta$)}
        \State $A_l \gets \text{GetActivations}(f_\theta, X_{train}, l)$
        \State $P_l \gets \text{TrainProbe}(A_l, Y_{train})$ \Comment{Train probe for layer l}
    \EndFor
    \State $F_{val} \gets \text{Concatenate}([P_l(\text{GetActivations}(f_\theta, X_{val}, l)) \text{ for } l \text{ in Blocks}(f_\theta)])$
    \State $\text{aggregator} \gets \text{TrainAggregator}(F_{val}, Y_{val})$\\
    \Return probes $\{P_l\}$, aggregator
\EndProcedure\\
\Procedure{Predict}{$x$, probes $\{P_l\}$, aggregator}
    \State $F_x \gets \text{Concatenate}([P_l(\text{GetActivations}(f_\theta, x, l)) \text{ for } l \text{ in Blocks}(f_\theta)]))$\\
    \Return $\text{aggregator}(F_x)$
\EndProcedure
\end{algorithmic}
\end{algorithm}






\subsection{LLM Steering}
\subsubsection{Function Specifications}
\begin{description}
\item[\texttt{Blocks($f_\theta$)}] \hfill \\
\textbf{Description:} Returns an iterator over the transformer blocks in the LLM. \\
\textbf{Parameters:}
\begin{itemize}
\item $f_\theta$: The LLM model with parameters $\theta$
\end{itemize}
\textbf{Returns:} Iterator yielding indices of transformer blocks from 1 to $L$ where $L$ is the total number of layers

\item[\texttt{Embed($x$)}] \hfill \\
\textbf{Description:} Converts input tokens into their corresponding embedding vectors in the LLM's embedding space. \\
\textbf{Parameters:}
\begin{itemize}
\item $x$: Input text/tokens (sequence of discrete tokens)
\end{itemize}
\textbf{Returns:} Dense vector representation $A_1 \in \mathbb{R}^{n \times d}$ where $n$ is sequence length and $d$ is embedding dimension
\item[\texttt{BlockForward($A_{\ell-1}$, $\ell$)}] \hfill \\
\textbf{Description:} Performs the standard forward pass through a transformer block at the specified layer. \\
\textbf{Parameters:}
\begin{itemize}
\item $A_{\ell-1}$: Input activations from previous layer
\item $\ell$: Current layer number
\end{itemize}
\textbf{Returns:} Output activations $A_\ell$ after applying attention and feed-forward operations
\item[\texttt{GetLogits($h_L$)}] \hfill \\
\textbf{Description:} Converts the final hidden state into logits for token prediction. \\
\textbf{Parameters:}
\begin{itemize}
\item $h_L$: Final layer hidden state
\end{itemize}
\textbf{Returns:} Vector $p \in \mathbb{R}^{|V|}$ of logits over vocabulary $V$
\end{description}

\begin{algorithm}[h]
\caption{LLM Steering}
\label{alg: steering}
\begin{algorithmic}[1]
\Require LLM $f_\theta$, steering vectors ${c_\ell}$, control coefficient $\eps$
\Procedure{Steer}{$x$}
\State $A_1 \gets \text{Embed}(x)$ \Comment{Initial token embedding}
\For{block $\ell$ in Blocks($f_\theta$)}
\State $A_\ell \gets \text{BlockForward}(A_{\ell-1}, \ell)$ \Comment{Standard forward pass}
\State $A_\ell \gets A_\ell + \eps c_\ell$
\EndFor
\State $p \gets \text{GetLogits}(hL)$ \Comment{Final layer logits}
\Return $p$
\EndProcedure\\
\end{algorithmic}
\end{algorithm}


\section{Experimental details}
\label{app: experimental details}
All experiments on 8B and 9B parameter models were performed on an A40 or A100 GPU with 38-42 GB of RAM. We used two to three GH200 nodes with $\approx$100GB of RAM for experiments with Llama-3.3-70B. For RFM and linear regression, we extract a validation set from the training set (80/20 split), with a maximum of 256 validation points, for tuning hyperparameters - bandwidth and the ridge. For RFM, we tune bandwidths among [0.2, 1, 5, 10, 100, 1000] and ridges among [1e-3, 1e-2, 1e-1, 1e0, 1e1]. For linear regression, we tune ridge among [0., 1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1, 10, 100, 1000, 10000]. For RFM, we use 8 iterations in the layer-wise predictors and 4 iterations for the aggregation method, except for the multiclass task (HE-Wild), where we use 3 iterations for the layerwise predictors. For logistic regression we use the default sklearn settings, except for aggregation we increase the \texttt{max\_iters} parameter to 1000 ensure convergence of their internal parameter fitting procedure. We do not use a bias term for concept extraction with any of our methods as we found these worsened the quality/interpretability of the features measured by steering performance. Following the extraction of concepts, when generating layer-wise detectors, we learn an additional bias term. For detection with AgentHarm, HE, TruthGen, we extract concept scores for the top two eigendirections of the AGOP. For FAVA, we use the top three. For ToxicChat, we use the top single direction. For the multitask HE-Wild task, there are six total classes, so we extract the top six eigenvectors of RFM AGOP as concept vectors.

For TruthGen, we randomly sample 5 train/val/test splits of sizes 2000/1500/474 points. We report the average and standard deviation over the test sets. For ToxicChat, a test set is provided, which we fix, then we sample five train/val sets in a 60/40 split among all of the original training samples. For HaluEval, using the QA dataset, we split the entire dataset evenly into a training and test set - i.e. the first 3997 of the total 7994 samples are used for training, while the second half are used for testing. We then form 5 random val/test splits from the second half by divide the test set evenly into a val and a new test set. For HaluEval (Gen.), we use the same training set as the HaluEval with QA prompts, but now sample five val/test splits (with even splits) among all the `general' subset. For HaluEval-Wild, we randomly sample 20 train/val/test splits of sizes 300/250/50. We use these same splits across all binary tasks. For FAVA, we randomly sample 20 val/test splits of sizes 360/100, and use the 7000 random samples from the provided train set for training (we ensure all methods used the same train split). For AgentHarm, we use the provided validation set for both training and validation, then the provided test set for testing. 

For LAT \citep{representation_engineering}, which requires equal numbers of positive and negative prompts, we use the same splits as the other supervised methods then extend the training set so that it has equal number of positive/negative examples by randomly sampling from the existing examples. E.g. given 10 negative and 1 positive example in the training set, we would repeat the 1 positive example 9 more times to generate 10 total pairs of pos./neg. examples. If this were two positive examples, we would extend the positive set by sampling one of the original two examples 8 more times.

As noted in \citet{representation_engineering}, the sign of the vector for PCA corresponding to a particular concept can be arbitrary as returned by the eigenvalue decomposition algorithm. In our work, we choose the sign of AGOP eigenvectors to be $+1$ if the projection of the activations onto this vector is positively correlated with the labels, and $-1$ otherwise. Note the sign of the concept vectors is only relevant for steering.

For methods that aggregated RFM as the layer-wise predictor with linear or logistic regression as the aggregation method (e.g. RFM/Lin.Reg.), we only take the single top eigenvector as the concept vector per layer.


