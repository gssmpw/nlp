\section{Introduction}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/main_figure.pdf}
    \caption{Methodology for (A) detecting and (B) steering concepts in language models by aggregating layer-wise predictors. (C) Examples of steered generations with instruction-tuned Llama-3.1-8B toward one or multiple concepts using the top eigenvector of the AGOP from RFM. These concepts include harmfulness, Shakespearean/Poetic English, and dishonesty. The produced SSNs in the first box are valid according to online verification services. }
    \label{fig: main figure}
\end{figure}


Large Language Models (LLMs) show expert level performance on a broad range of scientific, mathematical, language, and factual assessment tasks \citep{claude, gpt4o}.  Yet LLMs know more than they express in their initial responses.
% \cite{carlini2023scalableVideo, wei2022chain, representation_engineering, gottesman2024estimating}
Consider the example from~\citet{carlini2023scalableVideo}, where the authors asked GPT-3.5 for the number of staff at the British Broadcasting Company (BBC). When asked directly, the model claimed that it did not have the exact number. However, as the authors showed, this information along with  other specific details could be elicited indirectly.  Furthermore, methods such as  Chain-of-Thought prompting~\cite{wei2022chain} can elicit correct outputs 
from models despite initially incorrect responses, by having the model review and correct its own outputs. Uncovering LLM knowledge and mapping it to human interpretable concepts is a fundamental open problem. 

A prominent approach to this challenge has focused on unsupervised methods, such as sparse autoencoders, to extract interpretable patterns, or \textit{features}, from internal activations in LLMs~\cite{yun2021transformer,bricken2023towards,cunningham2023sparse,lindsey2024sparse}.  This work has led to the discovery of a number of features corresponding to concepts and entities (a notable example is the Golden Gate Bridge~\cite{bricken2023towards}).  Moreover,  by adjusting the magnitude of internal activations corresponding to these features, it was possible to \textit{steer} the LLM to focus on one of these identified concepts.  For instance, researchers at Anthropic demonstrated that modifying activations related to the Golden Gate Bridge led Claude's responses focusing on this concept~\cite{bricken2023towards}. 
Another approach~\cite{chen2024selfie} has turned to LLMs themselves to summarize concepts corresponding to internal activation patterns.  

The primary challenge of  unsupervised approaches is isolating specific concepts of interest such as harmfulness or hallucinations, from the sets of features.  To this end, a large volume of work has focused instead on being able to extract specific concepts from LLMs by contrastive or supervised learning approaches~\citep{othello_probe, azaria2023internal, turner2023activation, burns2022discovering, li2024inference, panickssery2023steering, belinkov2022probing, stolfo2024improving}.  For example, the works of \citep{gottesman2024estimating, othello_probe} used linear and logistic regression on internal activations (a method known as \textit{probing}) to query specific knowledge in LLMs such as historical facts or states of an Othello game board.  The work of~\cite{representation_engineering} applied a variety of probing methods on internal activations of LLMs to detect and steer LLMs on human alignment concepts including honesty, morality, gender bias, memorization, and power seeking. In a related line of work~\cite{LLMcheck} the authors developed approaches for detecting whether an LLM is hallucinating by analyzing statistics from the model's activations.

In this work, we introduce a general methodology for detection and steering with respect to arbitrary pre-specified concepts.  The two key innovations in our work are (1) the use of nonlinear methods for learning features useful for both steering and detection, and (2) aggregation of features from multiple layers for improving detection. Namely, we first train nonlinear, feature learning predictors known as Recursive Feature Machines (RFMs)~\cite{rfm_science} on internal activations at each layer of LLMs to detect a pre-specified concept of interest.  In addition to building predictors, RFMs automatically identify feature vectors most relevant for the task. These features are the top eigenvectors of the Average Gradient Outer Product (AGOP) matrices described in Section~\ref{sec: techniques}. For each layer, we train a separate RFM model on the activations at that layer. We refer to these directions as the RFM features at each layer. We then aggregate the RFM features across all layers to both improve predictive performance for detecting concepts and to steer LLMs toward these concepts. Our approach is computationally efficient, as our RFMs utilize kernel ridge regression as the base predictive algorithm (see Section~\ref{sec: techniques} for details). It is also highly data efficient, using as few as $64$ labeled training prompts per concept.  An overview of our approach is shown in Figure~\ref{fig: main figure}.   

We detect the presence and strength of concepts in input text to an LLM by training RFM probes on the activations at every layer and \textit{aggregating}, building a predictor on top of features extracted from layer-wise predictors.  We proceed to demonstrate the effectiveness of our detection approach on seven benchmarks for detecting hallucinations, harmfulness, toxicity, and untruthful content.  We find that our method outperforms previous detection approaches utilizing network activations. Further, in most of the considered tasks, our approach utilizing relatively small, publicly available language models (e.g., Llama-3.1 with 8 billion parameters) outperforms state-of-the-art LLMs, such as GPT-4o, that are prompted to perform detection. 

With regard to steering, we find that RFM features can be used to steer LLMs based on a wide range of pre-specified concepts. To steer a model, we use RFM to extract concept-specific feature vectors at every layer of an LLM and add these vectors to activations at each layer to steer LLM output to the specified concept. Notably, the set of ``steerable'' concepts is much broader than what has been  considered previously~\citep{othello_probe, turner2023activation, representation_engineering}. Beyond the standard concepts studied in prior work (e.g., harmfulness), our approach enables steering based on semantic disambiguation, toward different human languages, programming languages, hallucinated responses, science subjects, and poetic/Shakespearean English. Furthermore, as we will show, we find that previously proposed contrastive approaches were unable to steer some of these concepts.

Our method extends to steering concepts with numerical attributes like product reviews and enables steering multiple concepts simultaneously. For example, we can steer toward combined concepts of Shakespearean English with harmfulness to elicit Social Security Numbers (SSNs) in Shakespearean English or we can steer toward dishonesty combined with Shakespearean English or poetic verse to produce dishonest output in these styles (see Figure~\ref{fig: main figure}C). To the best of our knowledge, our approach is the only method capable of steering multiple concepts simultaneously by adding a linear combination of concepts at every layer. The closest prior works steer multiple concepts to induce format/length/word-specific instructions \citep{stolfo2024improving} and alter performance on binary question/answer tasks \citep{van2024extending}, but do not steer the content of open-ended generations. Notably, these works report that their methods failed to steer linear combinations of concepts.

Our lightweight detectors for any desired concepts can be easily integrated into standard LLM inference pipelines without the need for separate concept-specific fine-tuned models. Furthermore, we envision that our work on steering will open new avenues for better understanding the structure in representations learned by neural networks and for developing effective, cost-efficient alternatives to standard back-propagation based fine-tuning methods of deep neural networks. 
