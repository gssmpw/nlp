\begin{table}
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}l|c|c|c|c|c|c|c|c@{}|}
\cline{2-9}
& \multicolumn{8}{c|}{\textbf{Dataset (train$/$test set sizes)}}\\
\cline{2-9}
& \multicolumn{2}{c|}{HE} & \multicolumn{2}{c|}{HE(Gen.)} & \multicolumn{2}{c|}{FAVA} & \multicolumn{2}{c|}{HE-Wild} \\
& \multicolumn{2}{c|}{(12051$/$3997)} & \multicolumn{2}{c|}{(10307$/$2254)} & \multicolumn{2}{c|}{(7360$/$100)} & \multicolumn{2}{c|}{(550$/$50)} \\
\hline
\textbf{Classifier} & Acc. (\%) & F1 & Acc. (\%) & F1 & Acc. (\%) & F1 & Acc. (\%) & F1 \\
\hline
\multicolumn{9}{@{}l|}{\textit{Aggregated layers}} \\
RFM/RFM & \textbf{93.8$\pm$0.2} & \textbf{.939$\pm$.002} & 83.7$\pm$0.2 & .720$\pm$.004 & \textbf{68.4$\pm$4.0} & \textbf{.652$\pm$.049} & 64.5$\pm$6.1 & .628$\pm$.061 \\
RFM/Lin.Reg. & 93.7$\pm$0.3 & .938$\pm$.002 & \textbf{84.6$\pm$0.5} & \textbf{.721$\pm$.012} & \textbf{68.4$\pm$4.0} & \textbf{.652$\pm$.049} & 63.7$\pm$8.6 & .619$\pm$.077 \\
Lin.Reg./RFM & 92.9$\pm$0.3 & .930$\pm$.003 & 83.5$\pm$0.3 & .713$\pm$.008 & 66.2$\pm$4.7 & .617$\pm$.063 & 66.7$\pm$5.9 & .650$\pm$.062 \\
Lin.Reg./Lin.Reg. & 92.9$\pm$0.1 & .931$\pm$.001 & 84.0$\pm$0.4 & .711$\pm$.008 & 66.1$\pm$3.8 & .620$\pm$.052 & 63.7$\pm$8.6 & .619$\pm$.077 \\
Logistic/Logistic & 93.5$\pm$0.2 & .934$\pm$.002 & 81.8$\pm$0.5 & .638$\pm$.009 & 61.4$\pm$4.2 & .355$\pm$.058 & 66.7$\pm$5.9 & .650$\pm$.062 \\
\hline
\multicolumn{9}{@{}l|}{\textit{Best single layer}} \\
RFM & 91.6$\pm$0.5 & .917$\pm$.006 & 82.0$\pm$0.3 & .669$\pm$.007 & 63.9$\pm$3.1 & .614$\pm$.033 & \textbf{70.2$\pm$5.3} & \textbf{.674$\pm$.051} \\
Lin.Reg. & 92.0$\pm$0.2 & .922$\pm$.002 & 78.6$\pm$0.2 & .617$\pm$.008 & 62.6$\pm$4.6 & .603$\pm$.053 & 68.6$\pm$5.7 & .651$\pm$.060 \\
Logistic & 92.8$\pm$0.2 & .927$\pm$.002 & 71.0$\pm$0.5 & .244$\pm$.014 & 57.8$\pm$4.2 & .235$\pm$.064 & 67.9$\pm$4.9 & .658$\pm$.046 \\
LAT \citep{representation_engineering} & 77.3$\pm$0.5 & .808$\pm$.004 & 80.1$\pm$0.5 & .629$\pm$.011 & 54.5$\pm$4.7 & .320$\pm$.087 & NA & NA \\
LLM-Check \citep{LLMcheck} & 51.6$\pm$0.4 & .105$\pm$.040 & 67.3$\pm$0.4 & .025$\pm$.025 & 65.0$\pm$3.1 & .608$\pm$.046 & NA & NA \\
\hline
\multicolumn{9}{@{}l|}{\textit{Judge models}} \\
Base model & 60.0$\pm$0.7 & .525$\pm$.008 & 76.8$\pm$0.2 & .542$\pm$.011 & 54.3$\pm$4.3 & .000$\pm$.000 & 39.9$\pm$6.1 & .373$\pm$.067 \\
GPT-4o \citep{gpt4o} & 65.2$\pm$0.7 & .661$\pm$.006 & 65.2$\pm$0.7 & .661$\pm$.006 & 66.9$\pm$3.1 & .660$\pm$.039 & 46.2$\pm$4.9 & .404$\pm$.044 \\
\hline
\end{tabular}
\caption{\textbf{Accuracy and F1 scores for hallucination detection with instruction-tuned Llama-3.1-8B as the base model}. The larger values are better and the top score per dataset is in bold. The datasets included are HaluEval (HE) \citep{halueval}, the general' detection subset of HaluEval - HE(Gen.), FAVA \citep{fava}, and HaluEval-Wild (HE-Wild) \citep{haluevalwild}. HE-Wild is a multiclass classification task, in which case F1 score is averaged over the output classes.}
\label{fig: f1+accuracies, halluc detection, llama}
\end{table}

\begin{table}
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}l|ccc|ccc|ccc@{}|}
\cline{2-10}
& \multicolumn{9}{c|}{\textbf{Dataset and Model}} \\
\cline{2-10}
& \multicolumn{3}{c|}{\textbf{AgentHarm}} & \multicolumn{3}{c|}{\textbf{ToxicChat}} & \multicolumn{3}{c@{}|}{\textbf{TruthGen}} \\
\cline{2-10}\\[-8pt]
\textbf{Classifier} & \shortstack{Llama\\8B} & \shortstack{Llama\\70B} & \shortstack{Gemma\\9B} & \shortstack{Llama\\8B} & \shortstack{Llama\\70B} & \shortstack{Gemma\\9B} & \shortstack{Llama\\8B} & \shortstack{Llama\\70B} & \shortstack{Gemma\\9B} \\
\cline{2-10}
\hline
\multicolumn{10}{@{}l|}{\textit{Aggregated layers}} \\
RFM/RFM & .689 & .833 & .720 & .722 & .751 & .705 & .870 & .898 & .875 \\
RFM/Lin.Reg. & \textbf{.815} & .847 & \textbf{.788} & .645 & .733 & .705 & .890 & .905 & .888 \\
Lin.Reg./RFM & .747 & .738 & .768 & .729 & .749 & .701 & .866 & .899 & .857 \\
Lin.Reg./Lin.Reg. & .798 & .825 & .725 & .687 & .754 & .710 & .884 & .906 & .885 \\
Logistic/Logistic & .720 & .746 & .759 & .682 & .729 & .705 & .874 & .907 & .858 \\
\hline
\multicolumn{10}{@{}l|}{\textit{Best single layer}} \\
RFM & .814 & \textbf{.850} & .736 & .566 & .686 & .627 & .881 & \textbf{.909} & .881 \\
Lin.Reg. & .783 & .669 & .684 & .648 & .734 & .672 & .887 & .907 & .871 \\
Logistic & .564 & .731 & .736 & .609 & .683 & .676 & .872 & .907 & .844 \\
LAT \citep{representation_engineering} & .667 & .595 & .034 & .160 & .324 & .160 & .850 & .884 & .821 \\
\hline
\multicolumn{10}{@{}l|}{\textit{Judge models}} \\
Base Model & .575 & .426 & .023 & .426 & .588 & .588 & .845 & .892 & .887 \\
GPT-4o & \textemdash & .127 & \textemdash & \textemdash & .544 & \textemdash & \textemdash & .899$^*$ & \textemdash \\
ToxicChat-T5-Large \citep{toxicchat} & NA & NA & NA & \textemdash & \textbf{.772}$^{**}$ & \textemdash & NA & NA & NA \\
\hline
\end{tabular}
\caption{\textbf{F1 scores for harmful, toxic, and dishonest content detection across models}. Bold indicates the top score per dataset. The models from which we extracted activations are instruction-tuned Llama-3.1-8B, Llama-3.3-70B, and Gemma-2-9B. The datasets are AgentHarm \citep{agentharm}, ToxicChat \citep{toxicchat}, and TruthGen \citep{truthgen}. \textbf{$\,^*$}Many of the truthful/untruthful responses in TruthGen were themselves generated by GPT-3.5 and GPT-4. \textbf{$^{\,**}$}ToxicChat-T5-Large was designed and optimized specifically for the ToxicChat benchmark.}
\label{fig: f1 scores, combined non-halluc detection}
\end{table}