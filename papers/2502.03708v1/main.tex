\documentclass{article}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[numbers,sort]{natbib}
\usepackage{hyperref}
\usepackage{csvsimple,booktabs,adjustbox}
\usepackage{aux/macros}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{algpseudocode,algorithm,algorithmicx,soul}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{needspace}
\usepackage{tabularx}
\usepackage[normalem]{ulem}

% \usepackage{authblk}
\newcommand\contributionNote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{\kern-5pt \textcolor{white}{\rule{5pt}{2ex}}#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\usepackage{bbm}

\newcommand\misha[1]{\textcolor{blue}{(MB: #1)}}
\newcommand\daniel[1]{\textcolor{purple}{(DB: #1)}}
\newcommand{\eb}[1]{\textcolor{red}{(\textbf{EB: #1})}}

\title{Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers}

\author{
\large
  \begin{tabular}{c@{\hspace{2cm}}c}  % Adds 2cm space between columns
    Daniel Beaglehole & Adityanarayanan Radhakrishnan \\
    Computer Science and Engineering & Broad Institute of MIT and Harvard \\ 
    UC San Diego & Harvard SEAS \\
    \texttt{dbeaglehole@ucsd.edu} & \texttt{aradha@mit.edu} \\[1.5ex]
    Enric Boix-Adser√† & Mikhail Belkin \\
    MIT Mathematics & Hal\i c\i o\u glu Data Science Institute \\
    Harvard CMSA & UC San Diego \\
    \texttt{eboix@mit.edu} & \texttt{mbelkin@ucsd.edu} \\
  \end{tabular}
}

\date{} 

\begin{document}

\maketitle

\begin{abstract} 

A trained Large Language Model (LLM) contains much of human knowledge. Yet, it is difficult to gauge the extent or accuracy of that knowledge, as LLMs do not always ``know what they know''
and may even be actively misleading.  In this work, we give a general method for detecting semantic concepts in the internal activations of LLMs.  Furthermore, we show that our methodology can be easily adapted to steer LLMs toward  desirable outputs.  Our  innovations are the following: (1) we use a nonlinear feature learning method to identify important linear directions for predicting concepts from each layer; (2) we aggregate features across layers to build powerful concept detectors and steering mechanisms.  We showcase the power of our approach by attaining state-of-the-art results for detecting hallucinations, harmfulness, toxicity, and untruthful content on seven benchmarks. We highlight the generality of our approach by steering LLMs towards new concepts that, to the best of our knowledge, have not been previously considered in the literature, including: semantic disambiguation, human languages, programming languages, hallucinated responses, science subjects, poetic/Shakespearean English, and even multiple concepts simultaneously. Moreover, our method can steer concepts with numerical attributes such as product reviews. We provide our code (including a simple API for our methods) at \url{https://github.com/dmbeaglehole/neural_controllers.git}.
\end{abstract}

\input{intro}
\input{techniques}
\input{detection}
\input{steering}
\input{discussion}
\input{acknowledgements}
\clearpage
\bibliographystyle{abbrvnat}
\bibliography{aux/references}

\clearpage
\appendix

\input{appendix/prompts}
\input{appendix/details}
\input{appendix/generations}
\clearpage
\input{appendix/detection}
\clearpage
\input{appendix/judge_details}

\end{document}
