\section{Detection results}
We show that combining (1) aggregation of layer-wise predictors and (2) RFM probes at each layer and (3) using RFM as an aggregation method gives state-of-the-art results for detecting concepts from LLM activations.  We outline results below and note that additional experimental details are provided in  Appendix~\ref{app: experimental details}.



\paragraph{Benchmark datasets.} To evaluate concept predictors, we consider seven commonly-used benchmarks in which the goal is to evaluate LLM prompts and responses for hallucinations, toxicity, harmful content, and truthfulness.  To detect hallucinations, we use HaluEval (HE) \citep{halueval}, FAVA \citep{fava}, and HaluEval-Wild (HE-Wild) \citep{haluevalwild}. To detect harmful content and prompts, we use AgentHarm \citep{agentharm} and ToxicChat \citep{toxicchat}. For truthfulness, we use the TruthGen benchmark \citep{truthgen}.  Labels for concepts in each benchmark were generated according to the following procedures.  The TruthGen, ToxicChat, HaluEval, and AgentHarm datasets contain labels for the prompts that we use directly. HE-Wild contains text queries a user might make to an LLM that are likely to induce hallucinated replies. For this benchmark, we perform multiclass classification to detect which of six categories each query belongs to. LAT \citep{representation_engineering} does not apply to multiclass classification and LLM-Check \citep{LLMcheck} detects hallucinations directly, not queries that may induce hallucinations. Hence, we mark these methods with `NA' for this task. For FAVA, we follow the binarization used by \citet{LLMcheck} on this dataset, detecting simply the presence of at least one hallucination in the text. For HE, we consider an additional detection task in which we transfer the directions learned for the QA task to the general hallucination detection (denoted HE(Gen.) in Figure~\ref{fig: f1+accuracies, halluc detection, llama}). 



\paragraph{Evaluation metrics.} On these benchmarks, we compare various predictors based on their ability to detect whether or not a given prompt contains the concept of interest.  We evaluate the performance of detectors based on their test accuracy as well as their test F1 score, a measure that is used in cases where the data is imbalanced across classes (see Appendix~\ref{app: metrics}).  For benchmarks without a specific train/validation/testing split of the data, we randomly sampled multiple splits and reported the average F1 scores and (standard) accuracies of the detector fit to the given train/validation set on the test set on instruction-tuned Llama-3.1-8B, Llama-3.3-70B, and Gemma-2-9B. 



\input{tables}



\paragraph{Results.} Among all detection methods based on activations for hallucination detection, RFM was a component of the winning model across all datasets. These include (1) RFM from the single best layer, or (2) aggregating layers with RFM as the layer-wise predictor and either linear regression or RFM as the aggregation model (Table~\ref{fig: f1+accuracies, halluc detection, llama}). For Gemma-2-9B, the best performing method, among those that learn from activations, used aggregation or RFM on the single best layer on three of the four datasets (Table~\ref{fig: f1+accuracies, halluc detection, gemma}). The detection performance for the best performing detector on Llama-3.1-8B was better than that of Gemma-2-9B across all hallucination datasets, hence the best performing detectors taken across both models utilized both aggregation and RFM. The results are similar for detecting harmful, toxic, and dishonest content (Table~\ref{fig: f1 scores, combined non-halluc detection}). Among all models tested, the best performing model utilized aggregation and/or RFM as one of its components. Further, among the smaller LLMs (Llama-3.1-8B and Gemma-2-9B), the overall best performing model for each dataset utilized RFM. 

Moreover, aggregation with RFM as a component (either as the layer-wise predictor or aggregation method) performed better than the state-of-the-art judge model, GPT-4o \citep{gpt4o}. For Truthgen, we found that our method out-performed GPT-4o on Llama-3.3-70B but not with Llama-3.1-8B. We note that GPT-4o may have an advantage on this task as the truthful and dishonest responses were generated from earlier version of this model (GPT-3.5 and 4).  

Our aggregation method also performs favorably compared to methods designed for specific tasks. For example, our aggregation method outperformed a recent hallucination detection method based on the self-consistency of the model (LLM-Check \citep{LLMcheck}), which was argued to be the prior state-of-the-art in their work on the FAVA dataset. Further, despite the generality of our method, we perform only slightly worse than the fine-tuned model (ToxicChat-T5-Large) for toxicity detection on the ToxicChat dataset in F1 score and accuracy (Tables~\ref{fig: f1 scores, combined non-halluc detection} and \ref{fig: accuracies, non-halluc detection combined}). 