\section{Steering results}

Beyond detecting concepts, in many cases it is also useful to steer models towards certain behaviors. For example, steering based on activations has been applied to reducing bias \citep{representation_engineering}, thwarting jailbreaks \citep{circuit_breakers}, and reducing toxic content generation \citep{turner2023activation}.  In this section, we demonstrate that a  broad range of concepts can be steered by adding concept vectors to layer-wise activations. These concept vectors are taken to be the top eigenvector from RFM AGOP, $M_{\tau}$, computed separately at each layer. 

We begin by discussing steering for code translation and word disambiguation. We then demonstrate that the top eigenvector of the AGOP can be used to steer a range of novel concepts, including inducing hallucinations, extracting personally identifiable information, disambiguating semantic meanings, inducing scientific subjects, inducing Shakespearean/prose/poetic English, and translating programming / human languages. We conclude by demonstrating the RFM can be used to learn a sentiment vector from gradated reviews (ordinal data as opposed to binary outputs) and steer generated ratings across a range of values.

\paragraph{Human/programming languages.} We first show that the output of the LLMs can be steered toward different human and programming languages using our methodology. We learn the steering vectors for language by providing the model with queries to complete translation from a source language to a target language or back to the original source language (for details see Appendix~\ref{app: prompts}). In Figures~\ref{fig: full language steering results}, \ref{fig: english_spanish, llama-3.1-8B}, \ref{fig: english_chinese, llama-3.1-8B}, we prompt the model with a question in a source language, then apply the steering vector to generate a response in a different target language, such as steering from English to Mandarin. For programming languages, we prompt the model to re-state the original program (given in Python) then apply the Javascript vectors to steer the LLM to generate the program in the Javascript language.

We quantitatively compare RFM, linear / logistic regression, PCA, and Difference-in-means (DM) on programming and human language translation. In particular, we first steer Llama-3.1-8B-it and Gemma-2-9B-it models to translate sentences into target languages: Javascript for programming and Mandarin Chinese, German, and Spanish for human languages. We then evaluate the translations using GPT-4o as a judge model: we prompt GPT-4o to give a rating (from 1 to 5 for programming and from 1 to 4 for human languages). We report the ratings obtained by each steering method on programming in Figure~\ref{fig: full language steering results}. 
To further validate the GPT-4o judge scores for programming translations, we manually tested $15$ randomly selected programs with scores of $5/5$. Out of those $15$ programs, $14$ ran  successfully ran and passed at least five test cases. One program executed and passed the test cases  a single extra curly brace was removed. We find that regression methods (RFM, linear, logistic) outperform unsupervised methods (PCA, DM), and that RFM was the best performing steering method overall. Note that PCA fails to produce a single valid translation for programming. All methods succeed on human language translation with similar average ratings (Tables~\ref{fig: language steering results, llama} and \ref{fig: language steering results, gemmma}).

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/steered_language_fig.pdf}    
    \caption{\textbf{Steering programming and human languages.} (A) Visualization of language steering capabilities. (B) Comparison of translation from Python to Javascript by steering on two models, Gemma-2-9B-it and Llama-3.1-8B-it.  For each model and method, we report the average rating (1-5 scale) assessed by a GPT-4o judge model, where 5 is the best score and 1 is the minimum score. The five methods are RFM, Linear / Logistic Regression, Difference-in-means (DM), and Principal Components Analysis (PCA). (C) Distribution of steering performance across different methods.}
    \label{fig: full language steering results}
\end{figure}

\paragraph{Word meaning disambiguation.} We give another example where steering with RFM out-performs linear regression and PCA that has not been studied in prior work - word disambiguation. Consider, for example, disambiguating between two public figures with the last name Newton: Cam Newton, the former professional American football player, and Isaac Newton, the physicist/mathematician. We prompt Llama-3.1 with questions where the Newton being discussed is either ambiguous or where there is a likely choice of disambiguation. We show that one can steer the model with RFM to respond to queries with a desired Newton, even when the prompt clearly refers to a particular disambiguation (Figure~\ref{fig: rfm/pca newton, llama-3.1-8B}). For all steering algorithms in this figure, we tuned the control coefficients in increments of $0.1$ between $0.3$ and $0.8$ and tuned the choices of layer to steer among two options: (1) steer all blocks, (2) steer all but the final five blocks. PCA and linear regression interventions induced the model to give incorrect and lower quality responses to the questions than RFM steering. For example, when steering to explain what Cam Newton is known for, steering the model with PCA causes the model to claim Newton was a basketball player for the Lakers, which is incorrect. Steering with linear regression causes the model to respond that Cam Newton invented baseball (which is incorrect) and gives hybrid outputs discussing Isaac Newton. These responses indicate that PCA and linear regression have learned a less specific vector than RFM for distinguishing Cam and Isaac Newton.  We present additional examples of word disambiguation in Appendix~\ref{app: generations}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/newton_rfm_pca_linear_comparison.pdf}
    \caption{\textbf{Steering instruction-tuned Llama-3.1-8B to interpret names as different identities.} We present the example of steering the LLM toward interpreting Newton as Isaac Newton (the scientist) and Cam Newton (the American football player). We compare steering with the top eigenvector of AGOP  from RFM, the top principal component of difference vectors (PCA), and linear regression (Lin. Reg.).}
    \label{fig: rfm/pca newton, llama-3.1-8B}
\end{figure}

\paragraph{Steering multiple concepts simultaneously.} We have already shown in Figure~\ref{fig: main figure} that one can steer for combinations of harmful and dishonest content with poetry/Shakespeare style. We steer these generations by adding linear combinations of the concept vectors for distinct concepts to every layer. Our finding shows the advantage of our methodology, as prior methods were unsuccessful in steering with linear combinations of concept vectors in the same layer \citep{van2024extending, stolfo2024improving}, as claimed in those works.

\paragraph{English style.} We additionally show we can steer the model generate a variety of responses in poetic style (Figure~\ref{fig: steered poetry style}, Appendix~\ref{app: generations}), Shakespearean style (Figure~\ref{fig: shakespeare, llama-3.1-8B}). Even when prompted with modern queries, the model generates valid and informative responses in Shakespearean and poetic English. The poetic response even maintains short stanzas and occasional rhymes (pain/strain, sleep/creep at the end of the first and third stanzas). We also show that directions extracted from the eigenvectors of RFM give better steering for poetry style than logistic regression (Figure~\ref{fig: steered poetry style}).

\paragraph{Private information, harmful content, hallucinations.} We further steer models to give instructions for harmful or illicit behaviors (Figure~\ref{fig: harmful, llama-3.1-8B}) and respond with personally identifiable information (PII) such as social media accounts and emails (Figure~\ref{fig: PII, llama-3.1-8B}). On ten consecutive samples of SSNs from the model, all numbers were determined to be valid by third-party verification services. The social media links and emails were also valid, so we again redact them from the figure. We further steer the model to generate hallucinated responses to questions (Figure~\ref{fig: hallucination, llama-3.1-8B}). For example, in response to a query about whether forest fires cause earthquakes, the model generates a pseudo-mathematical argument to estimate the percentage of forest fires that cause earthquakes (third row of Figure~\ref{fig: hallucination, llama-3.1-8B}). \\

\noindent We provide examples of additional steerable concepts in Appendix~\ref{app: generations} including \textbf{political leanings} and \textbf{science subjects}. 

\subsection{Concept extraction and steering from gradated values}

We conclude the section by demonstrating an additional ability of RFM over methods that rely on binary classes such as logistic regression, PCA, and DM -- learning from gradated signals. While prior work has shown that sentiment can be steered using a binary set of positive and negative prompts \citep{subramani2022extracting, turner2023activation}, we use RFM to learn a direction corresponding to sentiment from reviews with numeric (non-binary) ratings. We then steer LLMs to output ratings across a gradation of values between the minimum and maximum score. 

We prompt Llama-3.1-8B and Gemma-2-9B with many sample Amazon reviews for appliances \citep{hou2024bridging} as inputs. The prompts take the following form:
\begin{center}
\fbox{
\parbox[c][0.5cm]{0.5\textwidth}{
\begin{center}
{\sffamily\small Give a rating out of 5 for the following review: \{REVIEW\}.}
\end{center}
}
}
\end{center}
For each prompt of this form, we predict the true rating (which is a numeric value from 1 to 5 given by the Amazon product user) using RFM applied to the activations.  RFM learns a concept vector corresponding to sentiment, which we show can steer generated ratings between the minimum and maximum rating by modulating the control coefficient. To demonstrate our ability to steer ratings, we use OpenAI's o1 model to synthetically generate a list of 100 items that might receive reviews. We then prompt the model to generate a review for an ``average'' version of that item. We then steer the model with the sentiment vector to generate reviews of the desired sentiment. In Figure~\ref{fig: steered ratings}A, we show the average item rating over the 100 items as a function of the steering coefficient. In particular, we can induce not just highly negative and highly positive reviews, but also moderately positive and negative sentiment, indicating that review sentiment is (approximately) represented on a continuum in activation space. In Figures~\ref{fig: steered ratings}B-C, we show that the steered reviews are detailed and remain specific to the item/entity being reviewed.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/steered_ratings.pdf}
    \caption{\textbf{Steering reviews toward different ratings for instruction-tuned Llama-3.1-8B and Gemma-2-9B LLM on various items and entities.} (A) When prompted to review 100 `average' items, we steer Llama and Gemma toward different ratings and report the average ratings across these items as a function of the control coefficient (normalized to be between -1 and 1). (B) A specific example of steering Llama to give a negative review for a Harry Potter movie. (C) A specific example of steering Llama to give a positive review for a student's poor homework assignment. }
    \label{fig: steered ratings}
\end{figure}

