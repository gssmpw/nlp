\section{Discussion and conclusion}

In this work we have shown how complex concepts can be detected through identifying one or few directions in the activation space of LLMs using nonlinear predictors. Remarkably, such detection is invariably better than simply posing the question to the base model and is mostly better than the judgment of  far more powerful models, such as GPT-4o. Our paper adds further evidence to the body of work (including~\citep{carlini2023scalableVideo, wei2022chain, representation_engineering, gottesman2024estimating})  indicating that the models know more than they tell and, possibly, know more than they know, so that the information can only be extracted through indirect means, e.g.  through external predictors from their activations. It also  suggests that analyzing the internal states of models may be a generally better way of eliciting information than simply conversing with the model using human language. Our results point to a path forward for detecting concepts in text without the need for auxiliary fine-tuned models.

They also provide further evidence for the {\it linear representation hypothesis}~\citep{mikolov2013linguistic, pennington2014glove} suggesting that many concepts are represented as vectors (or low-dimensional subspaces) in the activations of LLMs. In fact, we show that even seemingly very complex transformations, such as rewording text poetically or translating from English to Mandarin, can be steered with a single fixed direction at each layer. 

Perhaps the most surprising aspect of these representations is that directions useful for detecting a concept or distinguishing between concepts can also be used for steering concepts. Consider for example the English to Chinese translation. It is clear that ``the manifold'' of English sentences has an invertible ``translation map'' to the manifold of Chinese sentences, as reasonably accurate translations between languages exist. 
But why should this map be implementable through a single linear vector addition at each layer? Perhaps even more strikingly, why can this vector be discovered through classification?  Note that simply distinguishing a text in English from a text in Chinese is a trivial problem as they occupy distinct regions in the token spaces due to very different character sets (for a human a cursory look at the page suffices). There is absolutely no {\it a priori} reason to believe that directions useful for  telling languages apart would be useful for translation. 

\begin{wrapfigure}{r}{0.4\textwidth}
   \centering
   \includegraphics[width=0.35\textwidth]{figures/discussion_fig.pdf}
   \caption{Even when both detection and steering can be implemented by a linear operation, the corresponding detection and steering vectors can be very different.}
   \label{fig: discussion}
\end{wrapfigure}

A simple illustration is given in Figure ~\ref{fig: discussion}.  The ellipses represent different classes, such as English and Chinese.  Even if Class 1 and Class 2 can be matched through a linear transformation (why would that be true for languages?), we see that the steering direction cannot be recovered from the detection direction. While the classes are linearly separable, they are not separable along the steering direction. Understanding what binds together detection and steerability is a fundamental open question in understanding neural representations.

Furthermore, despite large amounts of work on mechanistic interpretability and other aspects of LLMs, it remains mysterious how these linear structures arise in training and what aspects of the training data makes them possible. Additionally, while most  concepts  are seemingly represented by linear subspaces, it is not always the case~\citep{li2022emergent, engels2024not}.

Finally, we note that our methods open a pathway toward adjusting models without the usual backpropagation-based fine-tuning,  purely through modifying activations. One day  models may  even be trained that way.  
