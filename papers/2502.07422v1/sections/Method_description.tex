\subsection{State-of-the-art Analysis: Observations, Challenges, and Goals}

Our methodology is a strategic response to solve the challenges derived from our evaluation of SOTA methods presented in figure~\ref{fig:sota}, The challenges are summarized as follows:
%
%\vspace{-8pt}
\begin{enumerate}[leftmargin=*]
    \item Model designs and architectures directly affect fairness, robustness, and generalization. Therefore, unlike current techniques that concentrate on either enhancing the data quality or tweaking the training algorithms, we aim to enhance these metrics by modifying the model architecture.
%\vspace{-5pt}
    \item Increasing depth (feature size) and width (number of features) of the model has been shown to improve accuracy. but, at the cost of exacerbating fairness disparities. Therefore, We propose an MoE-based scaling approach that can improve performance without compromising fairness.
%\vspace{-5pt}
    \item Existing design techniques for Edge DNNs neglect fairness, robustness, and generalization resulting in biased, non-robust, and non-generalizable models. Therefore, in our work, We embed fairness, robustness, and generalization as objectives or constraints in the Edge DNNs design process.
\end{enumerate}
%\vspace{-5pt}
%
\subsection{Methodology Overview}

In response to the aforementioned challenges, we introduce MoENAS, a method that utilizes NAS to create edge DNNs that are accurate, fair, robust, and capable of generalization, all while adhering to constraints on model size and efficiency. The MoENAS methodology is structured around three core steps as shown in Figure~\ref{fig:MoENAS_details}: (1) integrating a Switch FNN layer (S-FFN) into an attention-based architecture, (2) search process within the expert mixing space, and (3) expert pruning. 




\begin{figure}[ht]
    \centering \includegraphics[width=\linewidth]{figures/BMoENAS_overview_9.pdf}
    \caption{Overview of MoENAS Methodology. This figure illustrates the three core steps of the MoENAS approach: (1) Replace the standard FFN layer (grey) with a Switch FFN layer (colored rectangle refers to the experts) (2) Execution of a search process within the expert mixing space to identify optimal expert combinations for accuracy, fairness, and robustness; (3) Prune the least used experts to ensure model compactness and efficiency while maintaining high performance.}
    \label{fig:MoENAS_details}
\end{figure}

%Further elaboration on each of these components are provided in sections~\ref{method:switch-fnn},~\ref{method:search-process} and~\ref{method:expert-pruning} of our methodology.

\subsubsection{Switch FFN layer (S-FFN)}
\label{method:switch-fnn}
%
This step involves replacing the standard FFN layer in the attention block with an S-FFN layer. This modified layer, inspired by \cite{fedus2022switch}, utilizes a MoE technique, enabling dynamic routing of features to different specialized experts. 

Traditionally, the structure of the attention block comprises two primary components as shown in Figure~\ref{fig:switch_fnn}.a: an attention layer and a feed-forward network~\cite{vaswani2017attention}. The attention layer's dynamic weight adaptation based on input is a key contributor to its SOTA performance~\cite{khan2022transformers}. However, the static nature of the standard FFN layer, which processes every feature in the same manner, represents a missed opportunity for dynamic adaptation.

Addressing this limitation, we replace the traditional FFN layer with an S-FFN layer. This later selects experts dynamically based on the features using a trained layer called the router. Such an approach ensures that the processing of features is contextually optimized based on their characteristics, thereby extending the dynamic adaptability characteristic of the attention mechanism throughout the model.

\textit{Anatomy of the Switch FFN Layer (S-FFN):} 
The S-FFN layer, as shown in Figure~\ref{fig:switch_fnn}.b, is architecturally composed of two primary components: a router, which directs each feature to a specific expert, and the expert set, where each expert is tailored to process a subset of the feature map based on the router's assignment. This structure allows for an adaptive and efficient processing of features, aligning with our objective to improve fairness, robustness, and generalization in neural network models.
\newline

\begin{figure}[t]
    \centering
\includegraphics[width=\linewidth]{figures/BMoENAS_SFNN.pdf}
    \caption{Detailed view of the Attention Block with Switch FFN Layer. This layer is based on MoE, each feature vector is processed by an FFN layer (expert) among the FFN set (experts set) based on the router selection.}
    \label{fig:switch_fnn}
    %\vspace{-8pt}
\end{figure}


\textit{Why an MoE-based Layer?}\\
In our NAS strategy, we chose an MoE approach because it allows the model to function like a skilled team of experts, where each expert specializes in analyzing certain features. Just as in a team where each member brings their expertise to focus on the area they know best, MoE models dynamically route inputs to the most appropriate expert. This ensures that each part of the data is processed by the expert most capable of handling it, leading to better overall performance. This adaptability of MoE not only enhances accuracy but also addresses fairness by ensuring that the model's decisions are more equitable across different subgroups.

\subsubsection{Search Process}~\\ \label{method:search-process}
%The second step involves a search over the space of the switching architecture where the number of experts in each block is varied, this search is guided using performance predictors that are trained to predict the metrics (accuracy, skin fairness, robustness) based on the model encoding shown in figure~\ref{fig:search_space} \cite{}.
The integration of the S-FFN layer, as previously outlined, presents clear benefits, yet the optimal placement and method of incorporation within the architecture require further exploration. In response, we utilize a NAS strategy to determine the most effective way to include these layers. This approach seeks architectures that achieve a balance between objectives, maximizing accuracy, fairness, and robustness, reducing overfitting, and maintaining a manageable model size.

The proposed search process begins with the selection of an attention-based architecture, for which we chose MobileNetVitV2, based on experimental analyses(see Figure~\ref{fig:sota}). Within this model, all FFN layers are substituted with Switch FFN layers as demonstrated in Figure~\ref{fig:switch_fnn}. The variation in the number of experts within each Switch FFN layer constitutes the core of our search space diversity. Specifically, in the case of MobileNetVitV2, which comprises nine layers, each layer can have from 1 to 8 experts. Consequently, each architecture configuration within this search space is encoded as a vector, succinctly representing the number of experts in each layer as shown in Figure~\ref{fig:search_space}.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.85\linewidth]{figures/BMoENAS_Search_space.pdf}
    \caption{
Search space of MoENAS: Each candidate architecture is built on the MobileViTv2 macro-architecture, varying only in the number of experts per S-FFN layer, represented as a vector encoding the expert count for each S-FFN layer.}
    \label{fig:search_space}
\end{figure*}

To explore this space, we employed a modified Bayesian Optimization (BO) strategy \cite{white2021bananas}, optimized to concurrently address four objectives: Test Accuracy, Skin Fairness, Robustness, and Generalization, while treating model size as a constraint. This approach utilized a population-based BO mechanism\cite{pelikan2002scalability}. To guide the search, we train surrogate models capable of estimating each objective metric from the vector representation of an architecture as shown in Figure~\ref{fig:search_space}. Here, XGBoost was selected as the surrogate, known for its effectiveness in handling such input vectors \cite{benmeziane2021comprehensive}. 

\subsubsection{Expert Pruning}~\\ \label{method:expert-pruning}
%The third step involves pruning the least utilized expert. After finding one or more suitable architectures, this step focuses on making these architectures more compact. We do this by removing the least important experts gradually until the model reaches a certain level of performance, ensuring it remains efficient.
%
While incorporating the Switch FFN Layer in place of the standard FFN does not impact the computational cost (measured in FLOPs), the model's size increases due to the presence of multiple FFNs within the switch layer. To manage this increase, we have carefully designed our search space to ensure all models remain within a manageable size range of 1M to 3M parameters. To further optimize the model size, we introduce a strategy called expert pruning inspired by \cite{chen2022task}.

Expert pruning (Figure~\ref{fig:MoENAS_overview}), is a method aimed at reducing the model's size by removing the least utilized experts. First, we run the model across all validation images to track the frequency of each expert's use. Then, we rank the experts based on their usage and identify those that are least utilized and therefore are candidates for pruning. 

Once an expert is pruned, the features that would have been routed to it are redirected to their next best option, as determined by the router's probabilities. Following this, the model undergoes evaluation to assess its performance. If the performance remains above a predetermined threshold, the process can be repeated to further refine the model's size and efficiency. This iterative approach ensures that we maintain a balance between model size, efficiency, and performance, making our models efficient for deployment in resource-constrained edge computing environments.

\textit{A question that may arise is why we use post-search pruning when pruned models are already in the search space.} The key reason is that post-search pruning occurs after the model has been fully trained, similar to how knowledge distillation works. Just as a smaller model distilled from a larger one outperforms the same model trained from scratch, training a larger MoE model and then pruning it can yield better results than directly training a smaller model. This post-training adjustment allows us to optimize the model, preserving and enhancing the most effective pathways identified during training, ultimately leading to improved efficiency.


\label{sec:metrics}
\subsection{Evaluation Metrics}
To assess the performance of our models, we focus on several key metrics: 
\begin{itemize}[leftmargin=*]
%\vspace{-8pt}
    \item \textbf{Validation and Test Accuracy:} This metric evaluates the model's accuracy on person classification using COCO~\cite{lin2014microsoft} for the validation and FACET~\cite{lin2014microsoft} for the test.
%\vspace{-4pt}
    \item \textbf{Skin Fairness:} examine the consistency of model accuracy across different skin tones. To calculate it, we adopt a formula that adjusts the Statistical Parity Difference (SPD)\cite{sheng2022larger}, used to calculate unfairness, with a constant $\beta$, to quantify fairness. The formula is given by:
%
     \begin{equation}
    \text{Fairness} = \frac{\beta - \text{SPD}}{\beta}
    \end{equation}
%
    \begin{equation}
    \text{SPD} = \sum_{i=1}^{N} \left| \text{Acc}_{G_i} -
    \text{Acc}_{mino} 
    \right|
    \label{eq:spd}
\end{equation}
%
%     \noindent
% \begin{minipage}{.4\linewidth}
%  \begin{equation}
%     \text{Fairness} = \frac{\beta - \text{SPD}}{\beta}
%     \end{equation}
% \end{minipage}%
% \begin{minipage}{.5\linewidth}
% \begin{equation}
%     \text{SPD} = \sum_{i=1}^{N} \left| \text{Acc}_{G_i} -
%     \text{Acc}_{mino} 
%     \right|
%     \label{eq:spd}
% \end{equation}
% \end{minipage}
%    
    $\beta$ is set to 0.2. $N$ is the number of groups (10 skin tones), and $\text{Acc}_{G_i}$ is the accuracy for group $i$, with $\text{Acc}_{\text{mino}}$ being the accuracy of the minority group (the group with the least number of images).%\vspace{-4pt}
    \item \textbf{Robustness to Light:} Measures the model's performance consistency under poor lighting conditions by calculating the accuracy for the subset of poorly lit images in the test dataset.%\vspace{-4pt}
\item \textbf{Overfitting:} Measured by the difference between validation and test accuracy, it indicates the model's generalization ability. A smaller gap suggests better generalization to unseen data since test and training/validation data come from two different distributions.
\end{itemize}
%\vspace{-5pt}