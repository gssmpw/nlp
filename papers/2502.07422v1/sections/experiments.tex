In the experimental section, we evaluate our method by benchmarking it against state-of-the-art edge DNNs, selected from the Hugging Face list of the fastest and most accurate edge models~\cite{huggingface_timm_fastest}. Before the comparative analysis, we outline the key elements of our experiments: the datasets used and the training schemes of the models under consideration.

\subsection{Datasets}
\label{sec:dataset}
%In this research, our target task is image classification, specifically determining whether an image contains a person. 
To train and test the models, we strategically employ two datasets, COCO and FACET, each chosen for their suitability to different aspects of our study. The COCO~\cite{lin2014microsoft} dataset is utilized for both training and validation, And the FACET dataset~\cite{gustafson2023facet} is used for testing. 

Facet includes exhaustively labeled, manually annotated attributes covering demographic and physical traits of individuals across 32k images, allowing for a fine-grained analysis of model performance across diverse demographic groups. Unlike many benchmarks that simplify attributes such as skin tone into binary categories like white or black, FACET employs a nuanced approach by using a spectrum from 1 to 10 to represent varying skin tones, capturing subtle differences in skin color. This detailed annotation system, combined with the comprehensive scope of the dataset, makes FACET the best choice for assessing and improving the fairness and robustness of vision models across multiple demographic axes~\cite{gustafson2023facet}.

\begin{table*}[ht]
\footnotesize
\centering
\caption{Comparison of model performance: MoENAS vs SOTA edge DNNS}
\label{tab:results}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{|p{2.7cm}|p{5.6cm}|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.3cm}|}
\hline
\textbf{Category} & {\makecell{\\ \\ \textbf{Model}}} 
& \multicolumn{1}{c|}{\multirow{3}{*}{\makecell{\textbf{Test} \\ \textbf{Accuracy} \\ (\%)}}}  
& \multicolumn{3}{c|}{\makecell{ \textbf{Fairness}}} 
& \multicolumn{1}{c|}{\multirow{2}{*}{\makecell{\textbf{Robust-} \\ \textbf{ness} (\%)}}}   
& \multicolumn{1}{c|}{\multirow{2}{*}{\makecell{\textbf{Overfit-} \\ \textbf{ting}(\%)}}} 
&  \multicolumn{1}{c|}{\multirow{3}{*}{ \makecell{\textbf{Model size} \\ (Million \\ Params)}}}
\\ \cline{4-6}
& & & \makecell{\textbf{Fairness} \\ \textbf{Score} \\ (\%)}  & \makecell{\textbf{Accuracy} \\ \textbf{Lightest} \\ \textbf{Skin}(\%)} & \makecell{\textbf{Accuracy} \\ \textbf{Darkest} \\ \textbf{Skin}(\%)} &  & & \\ 
\hline 

 & EfficientVit(m0)~\cite{cai2023efficientvit} & 81.92 & 83.08 & 89.14 & 78.68 & 76.31 & 8.78 & 2.16 \\

& EfficientVit(m1)~\cite{cai2023efficientvit} & 85.69 & 85.79 & 92.84 & 84.16 & 80.29 & 4.05 & 2.79 \\

 & RepGhostNet~\cite{chen2022repghost} & 84.55 & 79.49 & 91.88 & 80.21 & 79.76 & 7.48 & 1.04 \\

 & MobileVit(xxs)~\cite{mehta2021mobilevit} & 85.11 & 82.13 & 91.82 & 82.70 & 76.31 & 7.34 & 0.95 \\

\textbf{Edge SOTA models}& LcNet~\cite{cui2021pp} & 85.44 & 84.85 & 92.4 & 83.58 & 82.14 & 6.60 & 1.08 \\

& MobileVit(s)~\cite{mehta2021mobilevit} & 86.48 & 79.80 & 92.78 & 83.58 & 78.29 & 5.78 & 4.94 \\

& MobileVitV2~\cite{mehta2022separable} & 86.99 & 85.29 & 93.23 & 85.04 & 82.72 & 6.50 & 1.11 \\

& MobileNetV2~\cite{sandler2018mobilenetv2} & 87.70 & 79.51 & 95.19 & 84.17 & 83.54 & 4.62 & 2.23 \\

& Resnet18~\cite{he2016deep} & 88.05 & 77.02 & 94.12 & 83.87 & 83.93 & 7.21 & 11.17 \\

\hline

& MnasNet(s)~\cite{tan2019mnasnet} & \centering 81.81 & \centering 77.00 & 88.37 &  78.00  
& 71.33 & 6.88 & \colorbox{lime}{\textbf{0.75}} \\

& MobileNetV3(s)~\cite{howard2019searching} & 82.85 & 83.33 & 91.25 & 80.65 & 77.34 & 7.71 & 1.02 \\

\textbf{HW-NAS models} & Tinynet(e)~\cite{han2020model} & 85.38 & 80.44 & 92.84 & 82.4 & 79.81 & 6.99 & 0.76 \\

& TinyNet(d)~\cite{han2020model} & 86.05 & 86.63 & 92.52 & 85.04 & 81.56 & 6.63 & 1.06 \\

 & ProxylessNas~\cite{cai2018proxylessnas} & 87.39 & 76.21 & 92.65 & 83.28 & 82.44 & 5.82 & 5.39 \\

\hline

 & \textbf{MoENAS-S} & 90.33 & \colorbox{lime}{\textbf{93.13}} & 95.59 & \colorbox{lime}{\textbf{90.62}} & 84.51 & \colorbox{lime}{\textbf{0.21}} & 2.71 \\
\textbf{Proposed Models} & \textbf{MoENAS-XS} & 89.4 & 87.51 & 94.38 & 87.68 &  85.54 & 0.74 & 2.43 \\
& \textbf{MoENAS-XXS} & \colorbox{lime}{\textbf{91.72}} & 88.27 & \colorbox{lime}{\textbf{96.55}} & 88.27 & \colorbox{lime}{\textbf{90.32}} & 3.79 & 1.90 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table*}

\subsection{Training Setup}

To ensure a fair and comprehensive comparison, each SOTA model is initialized with its pre-trained backbone on the ImageNet dataset~\cite{deng2009imagenet}. To accommodate the diverse optimization strategies inherent to each model, we employ five distinct training schemes. These schemes are derived from the best practices recommended in the respective papers of the SOTA models~\cite{sandler2018mobilenetv2, mehta2021mobilevit, tan2019efficientnet}. For each model, we conduct training sessions under each of the five schemes and evaluate their performance based on test accuracy. The scheme that yields the highest test accuracy for a model is then selected as the optimal training approach for that particular model.

The models from our search space are trained only on one chosen scheme among the five, which is the most effective for MobileVitV2~\cite{mehta2022separable} since it is our base architecture. The training duration for each model, including our own, spans 150 epochs, allowing sufficient time for the models to converge.

%More details about the two datasets and the reasons for choosing them, the details about the training setup and configuration for both the SOTA models and the models from our search space, and details about the used hardware are discussed in the supplementary material - Appendix~A.

%It is widely used in computer vision\cite{liu2020deep}. The portion of the data set designated to identify whether or not people are present consists of approximately 115k images for training, and 8k images for validation~\cite{chowdhery2019visual}.

%For the test set, the fairness and robustness of computer vision models were assessed on the newly created FACET~\cite{gustafson2023facet} dataset. FACET provides detailed annotations that cover demographic and physical traits of individuals, such as gender and skin tone, alongside environmental attributes such as lighting and visibility conditions, which allow us to thoroughly evaluate our model's performance in terms of fairness and robustness. FACET comprises 32k images, out of which 22k were selected based on the criterion of featuring one primary person (other people may still appear in the background). This selection criterion allows for categorizing images based on the skin tone of the primary person.

%\subsection{Hardware}

%\subsection{Training Setup}
%To ensure a fair and comprehensive comparison, each SOTA model is initialized with its pre-trained backbone on the ImageNet dataset~\cite{deng2009imagenet}. To accommodate the diverse optimization strategies inherent to each model, we employ five distinct training schemes. These schemes are derived from the best practices recommended in the respective papers of the SOTA models \cite{sandler2018mobilenetv2, mehta2021mobilevit, tan2019efficientnet}. For each model, we conduct training sessions under each of the five schemes and evaluate their performance based on test accuracy. The scheme that yields the highest test accuracy for a model is then selected as the optimal training approach for that particular model.

%The models from our search space are trained only on one chosen scheme among the five, which is the most effective for MobileVitV2~\cite{mehta2022separable} since it is our base architecture. The training duration for each model, including our own, spans 150 epochs, allowing sufficient time for the models to converge.

\subsection{Hardware and Computational Cost}
\label{app:hw}
Training was conducted on an internal machine server equipped with 64 CPU cores, 4 A6000 GPUs, and 256 GB of RAM. The search cost is primarily dominated by the time required to train models from the search space in order to train the surrogate models. A total of 127 models were trained, each taking an average time of 2 hours and 21 minutes per model. The total GPU hours required for training all models amount to approximately 299 hours (2.35 hours per model * 127 models).

However, due to the availability of 4 GPUs and the capability to train 4 models simultaneously, the actual search time was significantly reduced. The real search time was about 5 days (114 hours), allowing us to efficiently complete the training process.

\subsection{Results and Discussion}
\subsubsection{Performance Across Metrics}
The results of our experimental analysis are reported in Table~\ref{tab:results}. Notably, our MoENAS models exhibit superior performance across all key metrics (Test Accuracy, Skin Fairness, Robustness, Overfitting) compared to the SOTA edge DNNs. Specifically, \mbox{MoENAS-S} achieves the highest performance on Skin Fairness (93.13\%). MoENAS-XXS not only excels in Test Accuracy (91.72\%), but also showcases good robustness (87.34\%) and low overfitting (3.79\%). %This observation that our models consistently outperform the previous SOTA Pareto front across different metric pairings can also be visually observed in Appendix~B. %Figure~\ref{fig:results} in the appendix \ref{app:pareto}. 
These results illustrate the model's ability to generalize across diverse conditions without compromising overall performance.
%These results indicate that MoENAS models achieve an efficient balance between these metrics, making DNNs accurate, fair, robust, and compact for edge computing applications.
% Insert figure showing the Pareto front here. Ensure you have the figure file in your project if using \includegraphics.
%In summary, the MoENAS models not only set new benchmarks but also showed effectiveness in designing edge DNNs that are powerful, efficient and fair for real-world applications.






\subsubsection{Performance Across Skin Tones}



%
% \setlength{\columnsep}{6pt}%
% \begin{wrapfigure}{r}{0.5\linewidth}
% \centering
% \includegraphics[width=0.5\textwidth]{figures/SkinF_plot_avg.pdf}
%         \label{fig:graph1}
%     \caption{Average Accuracy across Skin Tones of MoENAS vs Average SOTA models. MoENAS achieve better accuracy for all individual skin tones while also reducing the gap the accuracy gap between the lightest and darkest skin tones to $\Delta$ 5.6\%.}
%     \label{fig:skin_tone_comparison}
%     \vspace{-20pt}
% \end{wrapfigure}
%
To further demonstrate the performance of our MoENAS-S model, we compare its accuracy across 10 different skin tones against the average accuracy of SOTA as reported in Figure~\ref{fig:Accuracy_Skin_Tone_Models} (a). Our findings reveal that the MoENAS-S model not only enhances the accuracy for each skin tone but also significantly narrows the accuracy gap between the lightest and darkest skin tones ($\Delta$ of 5.6\%), in contrast to the observed average of the SOTA models ($\Delta$ of 14.1\%).

% \begin{figure}[!ht]
% \centering
% \includegraphics[width=0.9\linewidth]{figures/SkinF_plot_avg.pdf}
%         %\label{fig:graph1}
%     \caption{Average Accuracy across Skin Tones of MoENAS vs Average SOTA models. MoENAS achieve better accuracy for all individual skin tones while also reducing the gap the accuracy gap between the lightest and darkest skin tones to $\Delta$ 5.6\%.}
%     \label{fig:skin_tone_comparison}
%     %\vspace{-20pt}
% \end{figure}

%, as well as the top three SOTA models (MobileViTv2: $\Delta$13.6\%, MobileNetv2: $\Delta$ 9.2\%, and EfficientViTm0: $\Delta$ 11.2\%). 
These results demonstrate our model's capability to uplift overall performance while concurrently minimizing disparities across skin tones, thus successfully addressing the gaps identified in the motivation section of our study. %To support these findings, A more detailed one-on-one comparison with SOTA models is presented in Appendix~B. %Appendix \ref{app:one-one}.

\subsubsection{Detailed Comparative Analysis with State-of-the-Art Models}
\label{app:one-one}
We also provide a detailed one-on-one comparison of the MoENAS-S model against 10 state-of-the-art (SOTA) models as shown in Figure~\ref{fig:Accuracy_Skin_Tone_Models} (b-l). The MoENAS-S model consistently outperformed each of the 10 SOTA models in terms of accuracy for all 10 skin tones. Also, MoENAS-S model achieved a gap of only 5.6\%, while the lowest gap achieved by the SOTA models is 9.2\%. This substantial reduction highlights our model's effectiveness in addressing disparities across different skin tones and therefore reducing bias. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/Accuracy_Skin_Tone_Models.pdf}
    \caption{Accuracy across Skin Tones. (a) MoENAS vs. Average SOTA models. (b-l) MoENAS vs. Individual SOTA models. MoENAS achieves better accuracy for all individual skin tones while also reducing the gap the accuracy gap between the lightest and darkest skin tones to $\Delta$ = 5.6\%.}
    \label{fig:Accuracy_Skin_Tone_Models}
\end{figure*}

\subsubsection{Performance and Pareto Front Analysis}
\label{app:pareto}
The results from Table \ref{tab:results} demonstrate that our MoENAS models consistently outperform the previous SOTA Pareto front across various metrics. This superiority is visually evident in Figure \ref{fig:results}, where our models are positioned on the Pareto front for all three graphs. These findings indicate that MoENAS models achieve an efficient balance between accuracy, fairness, robustness, and compactness, making them ideal for edge computing applications. In summary, MoENAS models not only set new benchmarks but also excel in designing edge DNNs that are powerful, efficient, and fair for real-world applications.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.9\linewidth]{figures/Res_plot_genrale_MoENAS.pdf}
    \caption{Performance comparison of MoENAS vs SOTA based on Test Accuracy, Overfitting, Robustness, skin fairness, and model size. The graphs show that our models (in green) dominate SOTA models on all metrics.}
    \label{fig:results}
\end{figure*}

\subsubsection{Discussion}
In summary, our proposed MoENAS method demonstrates an effective approach to bridge the gaps in the performance of edge DNNs with respect to fairness, robustness, and generalization. 
 Our methodology effectively creates more equitable, robust, and efficient edge DNNs, advancing the state of edge DNN design. However, MoENAS slightly increases the model size by 0.4M compared to the average edge DNNs' sizes. This trade-off opens new avenues for further optimization. Future work could focus on minimizing this increase in size while also expanding the model's objectives to include factors like latency or explainability, thereby broadening the scope and application of our approach in edge computing environments.

% \begin{figure}
% \centering
%     \includegraphics[width=0.6\textwidth]{figures/SkinF_plot_avg.pdf}
%         \label{fig:graph1}
%     \caption{Average Accuracy across Skin Tones of MoENAS vs Average SOTA models. MoENAS achieve better accuracy for all individual skin tones while also reducing the gap the accuracy gap between the lightest and darkest skin tones to $\Delta$ 5.6\%.}
%     \label{fig:skin_tone_comparison}
% \end{figure}