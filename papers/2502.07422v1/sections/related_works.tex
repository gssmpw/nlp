\subsection{Fairness and Robustness in DNNs}
Significant strides have been made in DNNs, particularly in enhancing fairness, robustness, and generalization. \cite{narayanan2024fairness, ali2024assessing, zhang2021understanding}. Research efforts have predominantly centered around data-centric and algorithmic-centric approaches. Data-centric methods focus on manipulating the input or training data to achieve more balanced representations and outcomes \cite{wu2022fair, celis2020data}, while algorithmic-centric strategies modify the learning algorithms themselves to ensure fairer, more robust model behavior \cite{li2021ditto}. However, very few works delve into the intricate relationship between fairness and robustness and the underlying model design \cite{sheng2022larger}. Moreover, much of the existing literature focuses on large, resource-intensive models which makes their findings inapplicable at the edge \cite{parraga2023fairness, gustafson2023facet}.

\subsection{Hardware Aware Neural Architecture Search}
HW-NAS is a technique that focuses on the automatic design of DNNs. This automation can be presented as a search problem over a set of decisions defining the various components of a DNN aiming to balance and optimize for competing goals such as performance and computational cost\cite{benmeziane2021comprehensive}. HW-NAS has dramatically transformed edge computing, introducing highly efficient models~\cite{terven2023comprehensive,koonce2021mobilenetv3,mehta2021mobilevit}. However, a notable limitation of current HW-NAS efforts, especially in edge computing, is their primary focus on accuracy and efficiency metrics while overlooking other crucial aspects such as fairness, robustness, and generalization, creating a critical gap as demonstrated in Figure~\ref{fig:sota}. \textit{This underscores the need for future NAS methodologies to incorporate these aspects to enhance the design and deployment of equitable and robust edge DNNs.}

\subsection{Mixture of Experts}

The MoE concept is an ensemble learning technique that has been increasingly used in DNNs~\cite{rincy2020ensemble}. It has been successfully applied to scale Natural Language Processing and Computer Vision models ~\cite{du2022glam, riquelme2021scaling}. The Switch Transformer stands out as a prominent example \cite{fedus2022switch}, showcasing an architecture that scales efficiently to a trillion parameters through its use of sparse MoE layers. These layers allows to selectively activate a subset of parameters (or ``experts'') based on the input, significantly enhancing model capacity and performance without a linear increase in computational demand. 

%Inspired by this, Vision MoE (V-MoE) has been introduced as a sparse version of the Vision Transformer \cite{riquelme2021scaling}, showing promising results in image recognition and achieving competitive performance with SOTA networks while achieving less computing requirements at inference time. 

The benefits of MoE extend beyond model scaling, as their use of dynamic routing based on input features indicates potential for improving other metrics such as fairness, robustness, and generalization. The ability of MoE models to adapt based on the inputs makes them versatile and effective in handling complex and varied data, thus enhancing their ability to generalize and maintain good performance across different scenarios (robustness) and various 
subgroups (fairness). \textit{The potential use of MoE (Mixture of Experts) for improving these metrics is still a relatively under-explored area~\cite{cui2022synergy, aimar2023balanced}. However, it represents an intriguing research direction, as demonstrated in this paper.}



