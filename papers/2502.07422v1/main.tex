%File: anonymous-submission-latex-2025.tex
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% DO NOT CHANGE THIS
%\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{cite}
%\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
%\usepackage{graphicx}
\usepackage{textcomp}
%\usepackage{xcolor}
%\usepackage{times}  % DO NOT CHANGE THIS
%\usepackage{helvet}  % DO NOT CHANGE THIS
%\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{subcaption}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
%\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
%\frenchspacing  % DO NOT CHANGE THIS
%\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
%\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{array}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{xcolor}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
%\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
% \lstset{%
% 	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
% 	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
% 	aboveskip=0pt,belowskip=0pt,%
% 	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
% %
% % Keep the \pdfinfo as shown here. There's no need
% % for you to add the /Title and /Author tags.
% \pdfinfo{
% /TemplateVersion (2025.1)
% }

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

%\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{\textsc{\textit{MoENAS}}: Mixture-of-Expert based Neural Architecture Search for jointly Accurate, Fair, and Robust Edge Deep Neural Networks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Lotfi Abdelkrim Mecharbat\textsuperscript{1}, Alberto Marchisio\textsuperscript{1,2}, Muhammad Shafique\textsuperscript{1,2}, Mohammad M. Ghassemi\textsuperscript{3}, \\
  Tuka Alhanai\textsuperscript{1} \\
  \textsuperscript{1}Center for Quantum and Topological Systems (CQTS), NYUAD Research Institute, NYUAD, Abu Dhabi, UAE\\
  \textsuperscript{2}eBRAIN Lab, Division of Engineering, New York University Abu Dhabi (NYUAD), Abu Dhabi, UAE\\
  \textsuperscript{3}Department of Computer Science, Michigan State University, MI USA\\
  \texttt{lam9297@nyu.edu, alberto.marchisio@nyu.edu, muhammad.shafique@nyu.edu,}\\
  \texttt{ghassem3@msu.edu, tuka.alhanai@nyu.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}

There has been a surge in optimizing edge Deep Neural Networks (DNNs) for accuracy and efficiency using traditional optimization techniques such as pruning, and more recently, employing automatic design methodologies. However, the focus of these design techniques has often overlooked critical metrics such as fairness, robustness, and generalization. As a result, when evaluating SOTA edge DNNs' performance in image classification using the FACET dataset, we found that they exhibit significant accuracy disparities (14.09\%) across 10 different skin tones, alongside issues of non-robustness and poor generalizability. In response to these observations, we introduce \textit{Mixture-of-Experts-based Neural Architecture Search (MoENAS)}, an automatic design technique that navigates through a space of mixture of experts to discover accurate, fair, robust, and general edge DNNs. \textit{MoENAS} improves the accuracy by 4.02\% compared to SOTA edge DNNs and reduces the skin tone accuracy disparities from 14.09\% to 5.60\%, while enhancing robustness by 3.80\% and minimizing overfitting to 0.21\%, all while keeping model size close to state-of-the-art models average size (+0.4M). With these improvements, \textit{MoENAS} establishes a new benchmark for edge DNN design, paving the way for the development of more inclusive and robust edge DNNs.

%There has been a surge in optimizing edge Deep Neural Networks (DNNs) for accuracy and efficiency using traditional optimization techniques such as pruning, and more recently, employing automatic design methodologies. However, the focus of this design techniques has often overlooked critical metrics such as fairness, robustness, and generalization. As a result, when evaluating SOTA edge DNNs performance in image classification using FACET dataset, We found that they exhibit significant accuracy disparities (14.09\%) across 10 different skin tones, alongside issues of non-robustness and poor generalizability. In response to these observations, we introduce \textit{Mixture-of-Experts-based Neural Architecture Search (MoENAS)}, an automatic design technique that navigates through a space of mixture of experts to discover accurate, fair, robust, and general edge DNNs. \textit{MoENAS} improves upon SOTA edge DNNs  accuracy by 4.02\% and reduces the skin tone accuracy disparities from 14.09\% to 5.60\%, while enhancing robustness by 3.80\% and minimizing overfitting to 0.21\% all while keeping model size close the state-of-the-art models average size (+0.4M). With these improvements, \textit{MoENAS} establishes a new benchmark for edge DNN design, paving the way for the development of more inclusive and robust edge DNNs.
\

\end{abstract}


\section{Introduction}
\label{sec:introduction}
\input{sections/new_introduction}


\section{Related Works}
\label{sec:related_works}
\input{sections/related_works}


%\vspace{-0.3cm}

%\section{State of the Art Study}
%\label{sec:sota}
%\input{sections/sota}

\section{\textit{MoENAS} Methodology}
\label{sec:approach}
\input{sections/Method_description}
%\vspace{-0.25cm}

\section{Evaluation of our \textit{MoENAS} Methodology}
\label{sec:eval}
\input{sections/experiments}
%\vspace{-0.25cm}
\section{Ablation Studies}
\label{sec:study}
\input{sections/ablation}

\section{Limitations}
\label{sec:Limitations}

Despite the clear advantages in terms of accuracy, fairness, and robustness of our MoENAS framework, it has some limitations. the main one is regarding the model size, due to the nature of MoEs, the model size cannot be reduced as much as standalone edge DNNs, which limits the applicability of MoENAS to moderately low resources, while it is unsuitable for extremely tiny ML use cases. On the other hand, this paper has obvious positive societal impacts that improve fairness in DNNs.

\section{Conclusion}
Our paper introduces Mixture-of-Experts-based Neural Architecture Search (\textit{MoENAS}), a novel approach that goes beyond enhancing accuracy and efficiency of edge DNNs by also addressing other critical issues such as fairness, robustness, and generalization. By utilizing a mixture of experts, \textit{MoENAS} has significantly improved accuracy by 4.02\%, skin fairness by 4.61\%, robustness by 3.80\%, and minimized overfitting to 0.21\%, all while maintaining model size close to the average SOTA size. Future research directions include applying our method to other attention-based models and extending its capabilities to convolutional networks, aiming for broader applicability. Another promising research direction is investigating the explainability of \textit{MoENAS} by analyzing the choice of experts within the architecture. This approach could provide deeper insights into the decision-making process of the method and the network, enabling the development of more transparent and interpretable NAS methods and edge DNNs.

\section*{Acknowledgment}
This work was supported in part by the NYUAD Center for Quantum and Topological Systems (CQTS), funded by Tamkeen under the NYUAD Research Institute grant CG008.

%\bibliographystyle{unsrt}
%\bibliographystyle{splncs04}
\bibliographystyle{ieeetr}
\bibliography{main.bib}

% \newpage
% \label{appendix}
% \input{sections/appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}