\appendices

%%%%%%%%%%%%%%%%%%%%
% PROOFS
%%%%%%%%%%%%%%%%%%%%
\section{Proofs}
\label{app:proofs}


\subsection{Proof of Lem.~\ref{lem:linear-convex-incentive-conjugate-grad}}
\label{subapp:proof-linear-convex-incentive-conjugate-grad}

Since $\langle \lambda, \phi(w)\rangle$ is linear in $\lambda$ for each $w$ and $\bar{g}^*$ is the pointwise minimum of a family of affine functions in $\lambda$, $-\bar{g}^*$ is convex~\cite[Sec.~3.2.3]{boyd2004convex}.
Since, for each $\lambda \succeq_{\mathset{K}^*} 0$, $g + \langle \lambda, \phi(\cdot)\rangle$ is $\constsc$-strongly convex (by Assumption~\ref{assum:properties-of-gi}) with a closed convex domain, \eqref{eq:linear-convex-dual} has a unique minimum and $-\bar{g}^*(\lambda) < \infty$ for each $\lambda \in \mathset{K}^*$.
So, $\dom{\bar{g}^*} = \mathset{K}^*$.

By Danskin's Theorem \cite[Prop.~B.25]{bertsekas1997nonlinear}%
% (also see \cite[Prop.~A.22]{bertsekas1971control})
, $-\bar{g}^*$ is subdifferentiable on $\setint(\mathset{K}^*)$ with subdifferential given by
% Since $-\bar{g}^*$ is a proper convex function, it is subdifferentiable on the interior of its domain, $\setint(\mathset{K}^*)$ (see Sec.~\ref{subsubsec:subdifferential-of-a-convex-function}).
% The subdifferential of $-\bar{g}^*$ is given by \cite[Prop.~A.22]{bertsekas1971control} as,
\begin{equation*}
    \partial (-\bar{g}^*)(\lambda) = \bigl\{ -\phi(\bar{w}): \bar{w} \in \textstyle{\argmin}_w \{g(w) + \langle \lambda, \phi(w)\rangle\} \bigr\}.
\end{equation*}
Using the fact that \eqref{eq:linear-convex-dual} has a unique minimum for all $\lambda \in \mathset{K}^*$, we get that $\bar{g}^*$ is differentiable on $\setint(\mathset{K}^*)$ and that its gradient is given by \eqref{eq:linear-convex-incentive-conjugate-grad}.
If $\lambda \in \setbd(\mathset{K}^*)$ (the boundary of $\mathset{K}^*$), $-\phi(w^*(\lambda))$ is a subgradient of $-\bar{g}^*$.
For ease of notation, we assign $\nabla \bar{g}^*(\lambda) = \phi(w^*(\lambda))$ when $\lambda \in \setbd(\mathset{K}^*)$.

Next, we prove the continuity of $w^*(\cdot)$.
Let $\{\lambda^{(k)} \} \rightarrow \bar{\lambda}$, $\lambda^{(k)} \in \mathset{K}^*$, $\lVert \lambda^{(k)} \rVert \leq M$, and $w^{(k)} = w^*(\lambda^{(k)})$.
First, we show that $\{w^{(k)}\}$ is bounded.
Choose any $w' \in \dom{g}$ and $v \in \partial g(w')$.
Then, by $\constsc$-strong convexity of $g$ and $\mathset{K}$-convexity of $\phi$ (see Sec.~\ref{subsubsec:strong-convexity-and-lipschitz-smoothness}),
\begin{align*}
    g(w) + \langle \lambda^{(k)}, \phi(w)\rangle & \geq g(w') + \langle v, w - w'\rangle + \frac{m}{2} \lVert w - w'\rVert^2 \\
    & \text{\phantom{$\geq$}} + \langle \lambda^{(k)}, \phi(w') + D\phi(w') [w - w']\rangle, \\
    & \geq g(w') - M \lVert \phi(w')\rVert + \frac{m}{2} \lVert w - w'\rVert^2 \\
    & \text{\phantom{$\geq$}} - (\lVert v\rVert + M \lVert D \phi(w') \rVert) \lVert w - w'\rVert.
\end{align*}
Since $g(w') + \langle \lambda^{(k)}, \phi(w')\rangle \leq g(w') + M \lVert \phi(w')\rVert$, we have
% \begin{equation*}
%     \lVert w^k - w' \rVert \leq M' := \frac{ \lVert v\rVert + M \lVert D \phi(w') \rVert + \sqrt{\constsc M \lVert \phi(w')\rVert}}{(\constsc/2)}.
% \end{equation*}
% % So, $g(w) + \langle \lambda^k, \phi(w)\rangle$ is uniformly lower bounded by a radially unbounded function.
% Thus $\{w^k \}$ is bounded, meaning we can rewrite \eqref{eq:linear-convex-incentive-conjugate-grad-w} as,
% \begin{equation*}
%     w^k = w^*(\lambda^k) = \argmin_{w: \lVert w - w'\rVert \leq M'} \{g(w) + \langle \lambda^k, \phi(w)\rangle\}.
% \end{equation*}
% Applying the Maximum Theorem~\cite[Sec.~E.3]{ok2007real} to the above equation, it follows that $\lim_{k \rightarrow \infty} w^*(\lambda^k) = w^*(\bar{\lambda})$, meaning $w^*(\cdot)$ is continuous.
\begin{equation*}
    \lVert w^{(k)} - w' \rVert \leq \frac{ \lVert v\rVert + M \lVert D \phi(w') \rVert + \sqrt{\constsc M \lVert \phi(w')\rVert}}{(\constsc/2)}.
\end{equation*}
Thus $\{w^{(k)} \}$ is bounded.
Let $\bar{w}$ be a limit point of $\{w^{(k)}\}$ and $\{w^{(l)}\}$ be the corresponding subsequence.
Then,
\begin{align*}
    \bar{g}^*(\bar{\lambda}) & \leq g(\bar{w}) + \langle \bar{\lambda}, \phi(\bar{w})\rangle = \lim_{l \rightarrow \infty} \bigl(g(w^{(l)}) + \langle \lambda^{(l)}, \phi(w^{(l)})\rangle \bigr), \\
    & = \lim_{l \rightarrow \infty} \bar{g}^*(\lambda^{(l)}) \leq \lim_{l \rightarrow \infty} \bigl(g(w^*(\bar{\lambda})) + \langle \lambda^{(l)}, \phi(w^*(\bar{\lambda}))\rangle\bigr), \\
    & = g(w^*(\bar{\lambda})) + \langle \bar{\lambda}, \phi(w^*(\bar{\lambda}))\rangle = \bar{g}^*(\bar{\lambda}).
\end{align*}
So, any limit point $\bar{w}$ of $\{w^{(k)}\}$ minimizes $g(w) + \langle \bar{\lambda}, \phi(w)\rangle$.
Therefore, there is a unique limit point of $\{w^{(k)}\}$ and $\{w^{(k)}\} \rightarrow w^*(\bar{\lambda})$, i.e. $w^*(\cdot)$ is continuous.
\hidetext{Alternatively, we can use the Maximum Theorem~\cite[Sec.~E.3]{ok2007real}.}


\subsection{Proof of Lem.~\ref{lem:linear-convex-lipschitz-smooth-dual}}
\label{subapp:proof-linear-convex-lipschitz-smooth-dual}

From Lem.~\ref{lem:linear-convex-incentive-conjugate-grad}, $-\bar{g}^*$ is a convex function.
Since $\bar{w} = w^*(\bar{\lambda})$, the following optimality condition must hold~\cite[Thm.~23.8]{rockafellar1997convex}:
\begin{equation*}
    0 \in \partial g(\bar{w}) + \{D \phi(\bar{w})^\top \bar{\lambda}\}.
\end{equation*}
So, $\exists v \in \partial g(\bar{w})$ such that $v = -D \phi(\bar{w})^\top \bar{\lambda}$.
$g$ is $\constsc$-strongly convex under Assumption~\ref{assum:properties-of-gi}, and so for all $w \in \dom{g}$ we have that (see Sec.~\ref{subsubsec:strong-convexity-and-lipschitz-smoothness}% and \cite[Cor.~3.2.1]{nesterov2018lectures}
)
\begin{equation*}
    g(w) \geq g(\bar{w}) + \langle v, w - \bar{w}\rangle + (\constsc/2) \lVert w - \bar{w}\rVert^2.
\end{equation*}
Under Def.~\ref{def:linear-convex-incentive}, for any $\lambda \in \mathset{K}^*$, $\langle \lambda, \phi(\cdot) \rangle$ is convex and differentiable.
By \eqref{eq:subdifferential-definition}, we have that for all $w \in \dom{g}$,
\begin{equation*}
\begin{split}
    \langle \lambda, \phi(w)\rangle & \geq \langle \lambda, \phi(\bar{w})\rangle + \langle \lambda, D \phi(\bar{w})[w - \bar{w}]\rangle, \\
    & = \langle \lambda, \phi(\bar{w})\rangle + \langle D \phi(\bar{w})^\top \lambda, w - \bar{w}\rangle.
\end{split}
\end{equation*}
Adding the above two inequalities and using the fact that $v = -D \phi(\bar{w})^\top \bar{\lambda}$, we get that for all $w \in \dom{g}$ and $\lambda \in \mathset{K}^*$,
\begin{equation*}
\begin{split}
    g(w) + \langle \lambda, \phi(w)\rangle & \geq g(\bar{w}) + \langle \lambda, \phi(\bar{w})\rangle + (\constsc/2) \lVert w - \bar{w}\rVert^2\\
    & \text{\phantom{$\geq$}} + \langle D \phi(\bar{w})^\top (\lambda - \bar{\lambda}), w - \bar{w}\rangle.
\end{split}
\end{equation*}
Note that for a fixed $\lambda$, the RHS of the above inequality is quadratic in $w$.
Minimizing both the LHS and the RHS over $w \in \real^{\Nw\horizon}$ and using \eqref{eq:linear-convex-dual}, we get that for all $\lambda \in \mathset{K}^*$, % (after minimizing the quadratic function in the RHS),
\begin{equation*}
\begin{split}
    \bar{g}^*(\lambda) & \geq g(\bar{w}) + \langle \lambda, \phi(\bar{w})\rangle - \lVert D \phi(\bar{w})^\top (\lambda - \bar{\lambda}) \rVert^2 / (2\constsc), \\
    & = \bar{g}^*(\bar{\lambda}) + \langle \lambda - \bar{\lambda}, \phi(\bar{w})\rangle - \lVert D \phi(\bar{w})^\top (\lambda - \bar{\lambda}) \rVert^2 / (2\constsc), \!\!
\end{split}
\end{equation*}
which is the same as \eqref{eq:linear-convex-lipschitz-smooth}.

Let $\Lambda \subset \mathset{K}^*$ be any compact set.
We show that the restriction $-\bar{g}^*|_{\Lambda}$ is Lipschitz smooth.
By continuity of the function $w^*(\cdot)$ defined in \eqref{eq:linear-convex-incentive-conjugate-grad-w} (Lem.~\ref{lem:linear-convex-incentive-conjugate-grad}), $w^*(\Lambda)$ is a compact set.
By continuous differentiability of $\phi$ (Def.~\ref{def:linear-convex-incentive}), $\lVert D\phi(w)\rVert$ is bounded on $w^*(\Lambda)$.
Then, Lipschitz smoothness of $-\bar{g}^*|_{\Lambda}$ follows from \eqref{eq:linear-convex-lipschitz-smooth} and \cite[Thm.~2.1.5]{nesterov2018lectures}.


\subsection{Proof of Thm.~\ref{thm:linear-convex-incentives-iterative-method}}
\label{subapp:proof-linear-convex-incentives-iterative-method}

% Consider the function $\tilde{g}^*(\lambda) = \bar{g}^*(\lambda) - \langle \lambda, \phi(w^*)\rangle$.
From Lem.~\ref{lem:linear-convex-incentive-conjugate-grad}, $-\tilde{g}^*$ is a differentiable convex function on $\mathset{K}^*$.
The gradient of $\tilde{g}^*$ can be computed using \eqref{eq:linear-convex-incentive-conjugate-grad} as follows:
\begin{equation*}
    \nabla (-\tilde{g}^*)(\lambda) = -\phi(w^*(\lambda)) + \phi(w^*).
\end{equation*}
By the incentive controllability assumption, Assum.~\ref{assum:linear-convex-incentive-controllability}, $\exists \lambda^* \in \mathset{K}^*$ such that $w^*(\lambda^*) = w^*$.
So, $\nabla (-\tilde{g}^*)(\lambda^*) = 0$, i.e., $\lambda^*$ is a minimizer of $-\tilde{g}^*$, and $-\tilde{g}^*$ is lower bounded.
Next, using \eqref{eq:linear-convex-lipschitz-smooth} with $\bar{\lambda} = \lambda^{(k)}$ and $\bar{w} = w^{(k)}$, we get that
\begin{align}
\label{eq:linear-convex-incentive-iterative-method-proof-majorization}
    % -\tilde{g}^*(\lambda) & \leq -\tilde{g}^*(\lambda^k) + \langle \lambda - \lambda^k, \phi(w^*) - \phi(w^k)\rangle \\
    % & \text{\phantom{$\leq$}} + \frac{1}{2\constsc} \lVert D \phi(w^k)^\top (\lambda - \lambda^k) \rVert^2 + \epsilon_k \lVert \lambda - \lambda^k \rVert^2, \\
    -\tilde{g}^*(\lambda) & \leq -\tilde{g}^*(\lambda; \lambda^{(k)}), \\
    & \leq -\hat{g}^*(\lambda; \lambda^{(k)}) := -\tilde{g}^*(\lambda; \lambda^{(k)}) + \epsilon^{(k)} \lVert \lambda - \lambda^{(k)} \rVert^2, \nonumber
\end{align}
% where $\epsilon_k \lVert \lambda - \lambda^k \rVert^2$ is added for regularization.
Thus, $-\hat{g}^*(\cdot \,; \lambda^{(k)})$ is a differentiable convex function that majorizes $-\tilde{g}^*(\lambda)$, with $-\hat{g}^*(\lambda^{(k)}; \lambda^{(k)}) = -\tilde{g}^*(\lambda^{(k)})$.
% So, \eqref{eq:linear-convex-incentives-iterative-method-const-step} is a majorization-minimization scheme where we first majorize $-\tilde{g}^*(\lambda)$ at iterate $\lambda^k$ using $-\tilde{g}(\lambda; \lambda^k)$, and then minimize $-\tilde{g}^*(\lambda; \lambda^k)$ to obtain $\lambda^{k+1}$.
Using Lem.~\ref{lem:linear-convex-lipschitz-smooth-dual}, and the assumption that $\{\epsilon^{(k)}\}$ and $\{\lambda^{(k)} \}$ are bounded, $-\hat{g}^*(\cdot \,; \lambda^{(k)})$ is Lipschitz smooth, with a constant independent of $k$.
Thus, by the convergence of gradient descent for Lipschitz smooth functions, any limit point $\lambda^*$ of $\{\lambda^{(k)} \}$ is a stationary point of $-\tilde{g}^*$ \cite[Prop.~2.3.2]{bertsekas1997nonlinear}\hidetext{$\ $(also see \cite[Sec.~5]{pilanci2018ee364b})}.
Because $\dom(-\tilde{g}^*) = \mathset{K}^*$, by the necessary and sufficient condition for optimality of convex problems \cite[Prop.~2.1.2(a)]{bertsekas1997nonlinear},
\begin{align*}
    & \nabla (-\tilde{g}^*)(\lambda^*) = \phi(w^*) - \phi(w^*(\lambda^*)) \in \mathset{K}, \\
    & \Rightarrow \phi(w^*) \succeq_{\mathset{K}} \phi(w^*(\lambda^*)).
\end{align*}
By Assumption~\ref{assum:linear-convex-incentive-controllability}, this implies that $w^* = w^*(\lambda^*)$ for all limit points $\lambda^*$ of $\{ \lambda^{(k)}\}$.
By continuity of $w^*(\cdot)$ (Lem.~\ref{lem:linear-convex-incentive-conjugate-grad}), $\lim_{k \rightarrow \infty} w^{(k)} = \lim_{k \rightarrow \infty} w^*(\lambda^{(k)}) = w^*(\lambda^*) = w^*$.

The dual cost decrease guarantee \eqref{eq:linear-convex-incentives-iterative-method-dual-cost-decrease} follows from \eqref{eq:linear-convex-incentive-iterative-method-proof-majorization} applied to $\lambda^{(k+1)}$ and the equality $-\tilde{g}^*(\lambda^{(k)}) = -\hat{g}^*(\lambda^{(k)}; \lambda^{(k)})$.

Let $\lambda^* \in \mathset{K}^*$ be an optimal incentive for $w^*$.
Then, by Lem.~\ref{lem:linear-convex-incentive-conjugate-grad}, $\nabla (-\tilde{g}^*)(\lambda^*) = 0$.
Using \eqref{eq:subdifferential-definition}, Lem.~\ref{lem:linear-convex-incentive-conjugate-grad}, and Lem.~\ref{lem:linear-convex-lipschitz-smooth-dual}, we have that for any $\lambda \in \mathset{K}^*$,
\begin{equation*}
-\tilde{g}^*(\lambda^*) \leq -\tilde{g}^*(\lambda) \leq -\tilde{g}^*(\lambda^*) + \frac{1}{2\constsc} \lVert D\phi(w^*)^\top (\lambda - \lambda^*) \rVert^2.
\end{equation*}
Thus, if $D\phi(w^*)^\top (\bar{\lambda} - \lambda^*) = 0$ for some $\bar{\lambda} \in \mathset{K}^*$, then $-\tilde{g}^*(\lambda^*) = -\tilde{g}^*(\bar{\lambda})$, i.e., $\bar{\lambda}$ minimizes $-\tilde{g}^*(\lambda)$.
Thus, $\bar{\lambda}$ is an optimal incentive.


% \iftrue
\iffalse
%%%%%%%%%%%%%%%%%%%%
% INCENTIVE CONTROLLABILITY COUNTEREXAMPLE
%%%%%%%%%%%%%%%%%%%%
\section{Incentive Controllability for General Incentives - A Counterexample}
\label{app:incentive-controllability-for-general-incentives-a-counterexample}

In this subsection, we motivate the use of linear incentives for the hierarchical MPC problem.
The incentive controllability problem for the incentive LoMPC cost function $g(w; \xi_0, u, \lambda)$ is as follows:
For any $w^* \in \dom g$, find $\lambda^*$ such that $w^* = w^*(\lambda^*) = \argmin_w g(w; \xi_0, u, \lambda^*)$.
Let $\Lambda$ be the set of valid incentives (see Sec.~\ref{sec:linear-convex-incentives}).
The set 
\begin{equation}
\label{eq:cost-weight-tuning-example-w-star}
\mathset{W}^* = \{\argmin_w g(w; \xi_0, u, \lambda): \lambda \in \Lambda\},
\end{equation}
contains all the values of $w$ such that an incentive can achieve $w$.
If $\mathset{W}^* = \dom g$, then incentive controllability holds.
In Thm.~\ref{thm:bimpc-equivalence}, we proved that incentive controllability holds for a linear incentive structure $\langle \lambda, w\rangle$.
However, this need not be the case for general incentives of the form $\langle \lambda, \phi(w)\rangle$, where each component of $\phi$ is a convex function and $\lambda \geq 0$ (see Rem.~\ref{rem:linear-convex-incentive-componentwise}).
We provide a counterexample to show that $\mathset{W}^*$ can be an arbitrarily small set.
For the counterexample below, we suppress the dependence on $(\xi_0, u)$.

Let $g(w; \lambda): \real^2 \times \real^3 \rightarrow \real$ be as follows,
\begin{equation}
\label{eq:cost-weight-tuning-example}
    g(w; \lambda) = \lambda^1 g_1(w) + \lambda^2 g_2(w) + \lambda^3 g_3(w),
\end{equation}
where $\lambda_1, \lambda_2, \lambda_3 \geq 0$ and
\begin{align}
\label{eq:cost-weight-tuning-example-costs}
    g_1(w) & = \lVert \begin{bmatrix}w_1 & w_2-1\end{bmatrix} \rVert^2_2, \nonumber \\
    g_2(w) & = \lVert \begin{bmatrix}w_1-1 & w_2-1\end{bmatrix} \rVert^2_2, \\
    g_3(w) & = \lVert \begin{bmatrix}w_1/\epsilon & w_2\end{bmatrix} \rVert_\infty, \nonumber
\end{align}
where $\epsilon > 0$ is a hyperparameter.
The minima of $g_1$, $g_2$, and $g_3$ are $(0,1)$, $(1,1)$, and $(0,0)$ respectively.
The set $\mathset{W}^*$ as a function of $\epsilon$ is plotted in Fig.~\ref{subfig:cost-weight-tuning-example-inf-norm}.
We can see that for small values of $\epsilon$, the set of inputs $w$ attainable using an incentive is small, so incentive controllability does not hold.
This problem persists even if we consider a smooth approximation of $g_3$, as shown in Fig.~\ref{subfig:cost-weight-tuning-example-soft-inf-norm}.
\begin{equation}
\label{eq:cost-weight-tuning-example-soft-cost}
    \tilde{g}_3(w) = (1/2) \ln(e^{2w_2} + e^{-2w_2} + e^{2w_1/\epsilon} + e^{-2w_1/\epsilon}).
\end{equation}

\begin{figure}%[!htp]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/cost_tuning_l1.png}
        \caption{$\infty$-norm, $g_3$}
        \label{subfig:cost-weight-tuning-example-inf-norm}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/cost_tuning_soft_l1.png}
        \caption{Soft $\infty$-norm, $\tilde{g}_3$}
        \label{subfig:cost-weight-tuning-example-soft-inf-norm}
    \end{subfigure}
    \caption{The set of optimal solutions of the incentive cost function $g(w; \lambda)$ given in \eqref{eq:cost-weight-tuning-example}.
    The incentive is given by $\lambda$, and the colored region is the set $\mathset{W}^* = \{\argmin_w g(w; \lambda): \lambda \geq 0\}$.
    The figure on the left corresponds to the costs in \eqref{eq:cost-weight-tuning-example-costs}, while the figure on the right corresponds to \eqref{eq:cost-weight-tuning-example-soft-cost}.
    These plots show that the set of optimal solutions for the incentive cost function can be arbitrarily small.
    }
    \vspace{-5pt}
    \label{fig:cost-weight-tuning-example}
\end{figure}

Note that $g$ consists of $2$-norms and $\infty$-norms along with weights $\lambda$, which is a common design choice for cost functions.
However, this simple choice is insufficient for incentive controllability because the set of LoMPC optimal solutions is still restricted.
Using a linear incentive structure ensures incentive controllability. %, and thus we only consider such incentives in this paper.
In Sec.~\ref{sec:linear-convex-incentives}, we discuss assumptions on general linear-convex incentive structure $\langle \lambda, \phi(w) \rangle$ such that incentive controllability still holds (see Def.~\ref{def:linear-convex-incentive}).

\fi

% \iftrue
\iffalse
%%%%%%%%%%%%%%%%%%%%
% ITERATIVE METHOD USING ADMM
%%%%%%%%%%%%%%%%%%%%
\section{Iterative Method using Consensus ADMM}
\label{app:iterative-method-consensus-admm}

\begin{lemma}
\label{lem:iterative-method-consensus-admm}
Let Assumption~\ref{assum:properties-of-g} hold, and $w^* \in \dom{g}$.
Let $\lambda^k, w^k$ be iterates satisfying,
\begin{subequations}
\label{eq:iterative-method-consensus-admm}
\begin{align}
    w^{k+1} & = w^*(\lambda^k) = \argmin_{w} ( g(w) + \langle\lambda^k, w \rangle ), \\
    \lambda^{k+1} & = \lambda^k + \frac{\bar{\constsc}}{2} (w^{k} - w^*),
\end{align}
\end{subequations}
where $\bar{\constsc} \in (0, \constsc)$.
Then, $w^k \rightarrow \ w^*$.
\end{lemma}

\begin{proof}
Consider the following cost function,
\begin{equation*}
\begin{split}
    \bar{g}(w) & = \bigl(g(w) - (\bar{\constsc}/2) \lVert w\rVert_2^2 \bigr) + \bigl( \indicator{ \{ w^*\}} + (\bar{\constsc}/2) \lVert w \rVert^2_2 \bigr), \\
    & = g_1(w) + g_2(w).
\end{split}
\end{equation*}
where $\bar{\constsc} \in (0, \constsc)$.
Since $g$ is $\constsc$-strongly convex, $g_1$ is a convex function (see Sec.~\ref{subsubsec:strong-convexity-and-lipschitz-smoothness}).
Moreover, the minimum of $\bar{g}$ occurs at $w^*$ because $\dom{\bar{g}} = \{w^*\}$.

The minimum of $\bar{g}$ can be computed iteratively using consensus ADMM~\cite[Sec.~7.1]{boyd2011distributed}.
Let $w_1^k, w^k_2$ be the primal iterates and $\mu_1^k, \mu^k_2$ be the dual iterates for $g_1$ and $g_2$ respectively.
Consensus ADMM iterations with step size $\bar{\constsc}$ are,
\begin{align*}
    w_1^{k+1} & = \argmin_{w_1} \bigl( g_1(w_1) + \langle \mu_1^k, w_1 {-} \bar{w}^k \rangle + \frac{\bar{\constsc}}{2} \lVert w_1 {-} \bar{w}^k\rVert_2^2\bigr), \\
    & = \argmin_{w_1} ( g(w_1) + \langle \mu^k_1 - \bar{\constsc} \bar{w}^k, w_1 \rangle ), \\
    w_2^{k+1} & = \argmin_{w_2} \bigl( g_2(w_2) + \langle \mu_2^k, w_2 {-} \bar{w}^k\rangle + \frac{\bar{\constsc}}{2} \lVert w_2 {-} \bar{w}^k\rVert_2^2\bigr), \\
    & = w^*, \\
    \mu_1^{k+1} & = \mu_1^k + \bar{\constsc} (w_1^{k+1} - \bar{w}^{k+1}), \\
    \mu_2^{k+1} & = \mu_2^k + \bar{\constsc} (w_2^{k+1} - \bar{w}^{k+1}),
\end{align*}
where $\bar{w} = (w_1 + w_2)/2$.
This can be reduced to
\begin{align*}
    w_1^{k+1} & = \argmin_{w_1} \bigl( g(w_1) + \langle \mu^k_1 - (\bar{\constsc}/2) (w_1^k + w^*), w_1\rangle \bigr), \\
    \mu_1^{k+1} & = \mu_1^k + \frac{\bar{\constsc}}{2} (w^{k+1} - w^*).
\end{align*}
Substituting $\lambda^k = \mu_1^k - \bar{\constsc} \bar{w}^k$ and $w^k = w_1^k$, we get,
\begin{align*}
    w^{k+1} & = \argmin_{w} \bigl( g(w) + \langle\lambda^k, w\rangle \bigr), \\
    \lambda^{k+1} & = \lambda^k + \frac{\bar{\constsc}}{2} (w^{k} - w^*).
\end{align*}
The convergence of $w^k$ to $w^*$ in \eqref{eq:iterative-method-consensus-admm} follows from the convergence of ADMM~\cite[Sec.~3.2.1]{boyd2011distributed} and strong convexity of $g_1$.
\end{proof}

\fi

%%%%%%%%%%%%%%%%%%%%
% ROBUST BIMPC FORMULATION
%%%%%%%%%%%%%%%%%%%%
\section{Robust BiMPC Formulation}
\label{app:robust-bimpc-formulation}

To write the robust BiMPC formulation \eqref{eq:robust-bimpc} for the problem structure defined in \eqref{eq:bimpc}, we can explicitly write the constraints \eqref{subeq:robust-bimpc-w-g-feasibility} and \eqref{subeq:robust-bimpc-w-f-feasibility}.
We assume, for now, that the state constraint sets $\mathset{X}$ and $\mathset{X}_\Omega$ in \eqref{eq:bimpc} are polytopic.
% In other words, we assume that the state constraints $x_k \in \mathset{X}$ for all $k \in \langle \horizon \rangle$ and $x_\horizon \in \mathset{X}_\Omega$ of the BiMPC \eqref{eq:bimpc} are encoded by
% \begin{equation*}
%     \{x \in \real^{\Nx(\horizon+1)}: C x \leq d\}.
% \end{equation*}
In other words, we assume that the state constraints $x_k \in \mathset{X}$ for all $k \in \langle \horizon \rangle$ and $x_\horizon \in \mathset{X}_\Omega$ of the BiMPC \eqref{eq:bimpc} are encoded by $\{x \in \real^{\Nx(\horizon+1)}: C x \leq d\}$.
%
% Given $(\xi_0, u, \hat{w})$, the constraint \eqref{subeq:robust-bimpc-w-g-feasibility} can be expressed using the LoMPC problem \eqref{eq:ith-lompc} as follows (recall from Rem.~\ref{rem:multiple-lompcs-no-state-constraints} that the LoMPCs cannot have any state constraints):
% \begin{equation*}
%     \hat{w}_k \in \mathset{W}, \quad k \in \langle \horizon \rangle.
% \end{equation*}
Given $(\xi_0, u, \hat{w})$, the constraint \eqref{subeq:robust-bimpc-w-g-feasibility} can be expressed using the LoMPC problem \eqref{eq:ith-lompc} as $\hat{w}_k \in \mathset{W}, \ k \in \langle \horizon \rangle$ (recall from Rem.~\ref{rem:multiple-lompcs-no-state-constraints} that the LoMPCs cannot have any state constraints).
%
The domain of the BiMPC cost function, $\dom{f}$, in the BiMPC constraint \eqref{subeq:robust-bimpc-w-f-feasibility} consists of dynamics constraints \eqref{subeq:bimpc-dynamics} and state and input constraints \eqref{subeq:bimpc-stage-constraints}, \eqref{subeq:bimpc-terminal-constraints}.
% The input constraint in \eqref{subeq:bimpc-stage-constraints} can be directly written as follows:
% \begin{equation*}
%     u_k \in \mathset{U}, \quad k \in \langle \horizon \rangle.
% \end{equation*}
The input constraint in \eqref{subeq:bimpc-stage-constraints} can be directly written as $u_k \in \mathset{U}, \ k \in \langle \horizon \rangle$.

Given $(x_0, u, \hat{w})$, the dynamics constraint \eqref{subeq:bimpc-dynamics} can be expressed in batch form as follows \cite[Sec.~8.2]{borrelli2017predictive}:
\begin{equation*}
    x = \bar{A}^u x_0 + \bar{B}^u_1 u + \bar{B}^u_2 \hat{w},
\end{equation*}
where $\bar{A}^u \in \real^{\Nx(\horizon+1) \times \Nx}$, $\bar{B}^u_1 \in \real^{\Nx(\horizon+1) \times \Nu\horizon}$, and $\bar{B}^u_2 \in \real^{\Nx(\horizon+1) \times \Nw\horizon}$ are the batch matrices constructed from $A^u$, $B^u_1$, and $B^u_2$.
The robustness constraint \eqref{subeq:robust-bimpc-w-f-feasibility} requires that $C x \leq d$ is satisfied for all $x$ of the form
\begin{equation*}
    x = \bar{A}^u x_0 + \bar{B}^u_1 u + \bar{B}^u_2 \hat{w} + \bar{B}^u_2 \tilde{w},
\end{equation*}
where $\lVert \tilde{w}\rVert \leq \bar{\theta}/\constsc$.
This is equivalent to
\begin{equation*}
    \sup_{\tilde{w}: \lVert \tilde{w}\rVert \leq \bar{\theta}/\constsc} C_j (\bar{A}^u x_0 + \bar{B}^u_1 u + \bar{B}^u_2 \hat{w} + \bar{B}^u_2 \tilde{w}) \leq d_j, \quad \forall j
\end{equation*}
where $C_j, d_j$ are the rows of $C$ and $d$ (see \cite[Sec.~6.4]{boyd2004convex}).
Thus, the robustness constraint can be written as
\begin{equation*}
    C_j (\bar{A}^u x_0 + \bar{B}^u_1 u + \bar{B}^u_2 \hat{w}) + (\bar{\theta}/\constsc)\lVert C_j \bar{B}^u_2\rVert_* \leq d_j, \quad \forall j,
\end{equation*}
where $\lVert \cdot \rVert_*$ is the dual norm~\cite[App.~A.1.6]{boyd2004convex}.
Combining all the above constraints, the constraints of the robust MPC \eqref{eq:robust-bimpc} can be written as
\begin{subequations}
\begin{align}
    & \hat{w}_k \in \mathset{W}, \ u_k \in \mathset{U}, \quad k \in \langle \horizon \rangle, \\
    & \hat{x} = \bar{A}^u x_0 + \bar{B}^u_1 u + \bar{B}^u_2 \hat{w}, \\
    & C_j \hat{x} + (\bar{\theta}/\constsc)\lVert C_j \bar{B}^u_2\rVert_* \leq d_j, \quad \forall j.
\end{align}
\end{subequations}
If some state constraint sets $\mathset{X}$ and $\mathset{X}_\Omega$ in \eqref{eq:bimpc} are given as quadratic constraints, the S-procedure can be used to reformulate the robustness constraint \eqref{subeq:robust-bimpc-w-f-feasibility} \cite[App.~B]{boyd2004convex}.


% \iftrue
\iffalse
%%%%%%%%%%%%%%%%%%%%
% EV CHARGING EXAMPLE - PARAMETERS
%%%%%%%%%%%%%%%%%%%%
\section{Parameters for EV Charging Example}
\label{app:ev-parameter-values}

\begin{table}[H]
\footnotesize
\setlength\extrarowheight{1pt}
\caption{Parameters for EV Charging Example}\label{tab:ev-parameters}
\begin{tabularx}{\columnwidth}{|c|c|L|}
\hline
Parameter & Value & Definition \\ \hline
$M$ & $1000$ & EV population size \\
$(\Theta^s, \Theta^l)$ & $(10, 50)$ & EV battery capacity (in \SI{}{kWh}), Sec.~\ref{subsec:ev-lompc-formulation} \\
$(w^s_\text{max}, w^l_\text{max})$ & $(0.25, 0.15)$ & Maximum normalized charging rate, \eqref{eq:ev-lompc-dynamics} \\
$(y^s_\text{max}, y^l_\text{max})$ & $(0.9, 0.9)$ & Maximum normalized SoC, \eqref{eq:ev-lompc-dynamics} \\
$(y_{\text{min}, 1}, y_{\text{min}, 2})$ & $(0.3, 0.5)$ & Range of initial normalized SoC, Sec.~\ref{subsec:ev-lompc-formulation} \\
$\horizon$ & $12$ & LoMPC horizon length, \eqref{eq:ev-lompc-dynamics} \\
$\delta$ & $0.05$ & LoMPC cost parameter, \eqref{eq:ev-lompc-cost} \\
$B$ & $30$ & Total EV battery capacity (in \SI{}{MWh}), \eqref{eq:ev-lompc-b} \\
\hline
$x_\text{max}/B$ & 0.3 & Normalized storage battery capacity, \eqref{eq:ev-bimpc-dynamics} \\
$u^g_\text{max}/B$ & 1 & Normalized maximum electricity generation, \eqref{eq:ev-bimpc-dynamics} \\
$u^b_\text{max}/B$ & 0.3 & Normalized maximum charge/discharge amount, \eqref{eq:ev-bimpc-dynamics} \\
$c^gB^{1.7}$ & $10^{-3}$ & Normalized BiMPC cost parameter, \eqref{eq:ev-bimpc-generation-cost} \\
$\gamma$ & $5$ & BiMPC cost parameter, \eqref{eq:ev-bimpc-tracking-cost} \\
\hline
$\epsilon_\text{tol}$ & $0.01$ & Incentive solver tolerance, Alg.~\ref{alg:iterative-method} \\
$\epsilon_k$ & $0.01$ & Regularization parameter, \eqref{subeq:linear-convex-incentives-iterative-method-const-step-flow} \\
\hline
$\horizon_r$ & $1$ & Robustness horizon, Rem.~\ref{rem:robustness-horizon} \\
$P$ & $12$ & Number of partitions for differential pricing, Rem.~\ref{rem:ev-differential-pricing} \\
\hline
\end{tabularx}
\end{table}

\fi

% \section*{Acknowledgment}

% Add acknowledgments if needed.
