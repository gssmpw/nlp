\section{Multiple Lower-level MPCs}
\label{sec:multiple-lompcs}

\subsection{Robust BiMPC Formulation}
\label{subsec:robust-bimpc-formulation}

In this section, we consider the hierarchical MPC problem \eqref{eq:reduced-ith-lompc} and \eqref{eq:reduced-bimpc} for the case of multiple LoMPCs.
Recall that the LoMPC problems are indexed by the set $\mathset{M}$ (with $M = |\mathset{M}|$).
First, we define the incentive BiMPC problem as an extension to \eqref{eq:reduced-bimpc} as follows (see Tab.~\ref{tab:hierarchical-mpc-variable-definitions} for variable definitions):
{
% \small
\begin{subequations}
\label{eq:multiple-lompcs-incentivized-bimpc}
\begin{alignat}{6}
    (\nu^*, \lambda^*)(\xi_0) \in \ && \argmin_{\substack{\nu=(u, \bm{w}), \\ \lambda \in \mathset{K}^*}} \ & f(u, w; \xi_0), \span\span\span\span\span \label{subeq:multiple-lompcs-incentivized-bimpc-cost} \\
    && \ \text{s.t} \quad &
    w^i &&= w^{i*}(\xi_0, u, \lambda), \ i \in \mathset{M}, \label{subeq:multiple-lompcs-incentivized-bimpc-w-optimality} \\
    &&& w &&= \frac{1}{M} \sum_{i \in \mathset{M}} w^i,
\end{alignat}
\end{subequations}
}%
where $w^{i*}(\xi_0, u, \lambda) := \argmin_{w^i} g^i(w^i; \xi_0, u, \lambda)$.
From \eqref{subeq:multiple-lompcs-incentivized-bimpc-w-optimality}, we note that all LoMPCs are provided the same incentive $\lambda \in \mathset{K}^*$.
We assume an affine parametric form for the incentive LoMPC cost functions $g^i$:
\begin{equation}
\label{eq:parametric-form-gi}
\begin{split}
    g^i(w^i; \xi_0, u, \lambda) := & \ g(w^i; \xi_0, u) + \langle \lambda, \phi(w^i)\rangle \\
    & + \langle \theta^i(\xi_0, u), w^i\rangle,
\end{split}
\end{equation}
where $\theta^i(\xi_0, u)$ is a parameter for the $i$-th LoMPC satisfying $\lVert \theta^i(\xi_0, u) \rVert \leq \bar{\theta}(\xi_0) \ \forall i \in \mathset{M}$, and $\langle \lambda, \phi(w)\rangle$ is a linear-convex incentive (see Def.~\ref{def:linear-convex-incentive}) with a $\mathset{K}$-convex function $\phi$ and $\lambda \in \mathset{K}^*$.
It can be shown that all LQR-like LoMPC problems have a parametric cost as given in \eqref{eq:parametric-form-gi} (see Rem.~\ref{rem:multiple-lompcs-lompc-lqr}).
In particular, the EV charging costs generally considered in literature can be written in the form \eqref{eq:parametric-form-gi} (see Sec.~\ref{subsec:ev-lompc-formulation}).
Note that for the single LoMPC case considered in Secs.~\ref{sec:linear-incentives-for-hierarchical-mpc} and \ref{sec:linear-convex-incentives}, we can choose $\theta^i \equiv 0$.

\begin{assumption}
\label{assum:multiple-lompcs-properties-of-gi}
For all $i \in \mathset{M}$, $g^i$ has the parametric form given by \eqref{eq:parametric-form-gi}, where the cost function $g$ satisfies Assumption~\ref{assum:properties-of-gi} and $\lVert \theta^i(\xi_0, u) \rVert \leq \bar{\theta}(\xi_0) \ \forall i \in \mathset{M}$.
\end{assumption}

\begin{remark}
\label{rem:multiple-lompcs-no-state-constraints}
From Sec.~\ref{subsec:hierarchical-formulation}, the cost function $g^i$ includes the state and input constraints for the $i$-th LoMPC.
If Assumption~\ref{assum:multiple-lompcs-properties-of-gi} holds, then, from \eqref{eq:parametric-form-gi}, $\dom{g^i}(\cdot \, ; \xi_0, u) = \dom{g^j}(\cdot \, ; \xi_0, u)$ for all $i, j \in \mathset{M}$ and $(\xi_0, u)$.
Therefore, to satisfy Assumption~\ref{assum:multiple-lompcs-properties-of-gi}, the LoMPCs in $\mathset{M}$ must not have any state constraints.
\end{remark}

Given Rem.~\ref{rem:multiple-lompcs-no-state-constraints}, the following remark identifies a class of LoMPCs for which Assumption~\ref{assum:multiple-lompcs-properties-of-gi} holds.

\begin{remark}
\label{rem:multiple-lompcs-lompc-lqr}
One class of LoMPC problems that always satisfy Assumption~\ref{assum:multiple-lompcs-properties-of-gi} is input-constrained LQRs with quadratic state cost and strongly convex input cost.
Any such LQR problem can be expressed in batch form as \cite[Sec.~8.2]{borrelli2017predictive}
\begin{equation}
\label{eq:input-constrained-lqr}
    \textstyle{\min}_{w^i \in \mathset{W}^\horizon} \bigl( (w^i)^\top Q w^i + \textstyle{\sum}_k \; g_w(w^i_k) + (y^i_0)^\top R w^i \bigr),
\end{equation}
where $Q \succeq 0$, $\mathset{W} \subset \real^\Nw$ is a closed convex set \changes{denoting the input constraints}, and $g_w: \real^\Nw \rightarrow \real$ is a strongly convex function.
In this case, $\theta^i(\xi_0, u) = R^\top y^i_0$.
The EV LoMPC considered in Sec.~\ref{sec:numerical-examples} is one such example.
\end{remark}

For the rest of the section, we suppress the dependence of $\theta$ and $\bar{\theta}$ on $(\xi_0, u)$.
Since all LoMPCs are provided the same incentive $\lambda \in \real^\Nlambda$ (from \eqref{subeq:multiple-lompcs-incentivized-bimpc-w-optimality}), incentive controllability is not guaranteed for the BiMPC problem \eqref{eq:multiple-lompcs-incentivized-bimpc} with multiple LoMPCs.
However, we can show that under Assumption~\ref{assum:multiple-lompcs-properties-of-gi}, incentive controllability holds with bounded error.

\begin{lemma} (Bounded incentive controllability)
\label{lem:bounded-incentive-controllability}
Let Assumptions~\ref{assum:linear-convex-incentive-controllability} and \ref{assum:multiple-lompcs-properties-of-gi} hold.
Given any $\xi_0$, let $(u^*, \hat{w}^*) \in \dom f$ with $\hat{w}^* \in \dom g$,
% Let $(u^*, \hat{w}^*)$ be an optimal solution of the following optimization,
% \begin{subequations}
% \label{eq:bounded-incentive-controllability-equivalence}
% \begin{alignat}{6}
%     (u^*, \hat{w}^*)(\xi_0) \in \ && \argmin_{\nu=(u, \hat{w})} \ & f(\nu; \xi_0), \label{subeq:bounded-incentive-controllability-equivalence-cost} \\
%     && \ \text{s.t} \quad &
%     \hat{w} \in \dom g(\cdot \; ; \xi_0, u), \label{subeq:bounded-incentive-controllability-equivalence-w-feasibility}
% \end{alignat}
% \end{subequations}
and $\lambda^* \in \mathset{K}^*$ be such that $-D\phi(\hat{w}^*)^\top\lambda^* \in \partial_w g(\hat{w}^*; \xi_0, u^*)$ (by Assumption~\ref{assum:linear-convex-incentive-controllability}).
% $\lambda^*$ can be computed using the iterative method, Thm~\ref{thm:linear-convex-incentives-iterative-method}.
Let, for $i \in \mathset{M}$, $w^{i*}$ be the optimal solution of the $i$-th LoMPC with the incentive $\lambda^*$:
\begin{equation}
\label{eq:bounded-incentive-controllability-wi}
    \hspace{-5pt} w^{i*} = \textstyle{\argmin}_{w^i} ( g(w^i; \xi_0, u^*) + \langle \lambda^*, \phi(w^i)\rangle + \langle \theta^i, w^i\rangle ). \hspace{-4pt}
\end{equation}
Then, $w^* = (1/M) \sum_{i \in \mathset{M}} w^{i*} \in \dom{g}$ and
\begin{equation}
\label{eq:bounded-incentive-controllability-error-bound}
    \bigl\lVert \hat{w}^* - w^* \bigr\rVert \leq \bar{\theta}/\constsc.
\end{equation}
\end{lemma}

\begin{proof}
Corresponding to the incentive LoMPC cost function $g^i$ in \eqref{eq:parametric-form-gi}, we define the dual function, $\bar{g}^*$, as follows:
\begin{equation}
\label{eq:bounded-incentive-controllability-conjugate-g}
    \bar{g}^*(\theta) := \textstyle{\min}_w \ ( g(w; \xi_0, u^*) + \langle \lambda^*, \phi(w) \rangle + \langle \theta, w\rangle ).
\end{equation}
Then, $\bar{g}^*(\theta) = -g^*(-\theta)$, where $g^*$ is the conjugate function (see Sec.~\ref{subsubsec:conjugate-of-a-convex-function}) of the $\constsc$-strongly convex function $g(\cdot \,; \xi_0, u^*) + \langle \lambda^*, \phi(\cdot) \rangle$.
So, by Prop.~\ref{prop:duality}, we have that $\bar{g}^*$ is a $(1/\constsc)$-smooth concave function.
By \eqref{eq:conv-analysis-conjugate-subgrad},
\begin{equation}
\label{eq:bounded-incentive-controllability-grad-g}
    \hspace{-4pt} \nabla \bar{g}^*(\theta) = \textstyle{\argmin}_w \, ( g(w; \xi_0, u^*) + \langle \lambda^*, \phi(w) \rangle + \langle \theta, w\rangle ).
\end{equation}
Since $-D\phi(\hat{w}^*)^\top\lambda^* \in \partial_w g(\hat{w}^*; \xi_0, u^*)$, we have from the optimality condition for $g(w; \xi_0, u^*) + \langle \lambda^*, \phi(w)\rangle$ that~\cite[Thm.~23.8]{rockafellar1997convex}
\begin{equation}
\label{eq:bounded-incentive-controllability-w-hat}
    \hat{w}^* = \textstyle{\argmin}_w \ ( g(w; \xi_0, u^*) + \langle \lambda^*, \phi(w) \rangle ).
\end{equation}
Using \eqref{eq:bounded-incentive-controllability-grad-g}, \eqref{eq:bounded-incentive-controllability-wi}, \eqref{eq:bounded-incentive-controllability-w-hat}, and the fact that $\bar{g}^*$ is $(1/\constsc)$-smooth, we get that
\begin{align*}
    \lVert \hat{w}^* - w^{i*}\rVert = \lVert \nabla \bar{g}^*(0) - \nabla \bar{g}^*(\theta^i) \rVert \leq \lVert \theta^i \rVert/\constsc \leq \bar{\theta}/\constsc.
\end{align*}
Finally, \eqref{eq:bounded-incentive-controllability-error-bound} follows from the triangle inequality.
Since $w^{i*} \in \dom{g} \ \forall i$ and $\dom{g}$ is convex, $w^* \in \dom{g}$.
\end{proof}

\hidetext{
\begin{remark}
TODO: In the absence of additional information about the LoMPC cost function $g$, the bound provided by \eqref{eq:bounded-incentive-controllability-error-bound} is tight.
To see this, we note that the error bound $\bar{\theta}/\constsc$ arises from the $(1/\constsc)$-Lipschitz smoothness of the dual function $\bar{g}^*$ in \eqref{eq:bounded-incentive-controllability-conjugate-g}.
We consider a $1$-dimensional example as follows:
Let $\theta^1 = -1$ and $\theta^i = 1$ for $2 \leq i \leq M$, with $\bar{\theta} = 1$ and $\lambda^* = 0$.
Let $g(w) = w^2 + \indicator{\{w \geq 0\}}$, where $\indicator{(\cdot)}$ is the indicator function (see Sec.~\ref{subsec:notations}), with $\constsc = 2$.
Then from \eqref{eq:bounded-incentive-controllability-w-hat}, $\hat{w}^* = 0$, and from \eqref{eq:bounded-incentive-controllability-wi}, $w^{1*} = 1/2$, $w$.
(See notebook for example).
\end{remark}
}

\hidetext{TODO: Remark about more complex parametric forms.}

% Replace w with \hat{w}^* if needed.
For any $w \in \dom{g}$, Lem.~\ref{lem:bounded-incentive-controllability} proves the existence of an incentive $\lambda^*$ such that the average LoMPC solution $w^*$ with incentive $\lambda^*$ has bounded error compared to $w$.
Moreover, from \eqref{eq:bounded-incentive-controllability-w-hat}, $\lambda^*$ is an optimal incentive corresponding to $w$, when $\theta = 0$.
Therefore, using Thm.~\ref{thm:linear-convex-incentives-iterative-method}, we can iteratively compute $\lambda^*$ given $(u^*, w)$.
We also note that the error bound given by \eqref{eq:bounded-incentive-controllability-error-bound} is independent of the number of LoMPCs $M$.

% The statement of Thm.~\ref{thm:bounded-incentive-controllability} can be interpreted in two steps.
% First, \eqref{eq:bounded-incentive-controllability-equivalence} defines the notion of a team-optimal solution $(u^*, \hat{w}^*)$ for the case of multiple LoMPCs, similar to Thm.~\ref{thm:bimpc-equivalence}.
% Next, Thm.~\ref{thm:bounded-incentive-controllability} proves the existence of an optimal incentive $\lambda^*$ that results in bounded error compared to $\hat{w}^*$.
% Moreover, from \eqref{eq:bounded-incentive-controllability-w-hat}, $\lambda^*$ is an optimal incentive corresponding to $\hat{w}^*$, when $\theta = 0$.
% Therefore, using Thm.~\ref{thm:linear-convex-incentives-iterative-method}, we can iteratively compute $\lambda^*$ given $(u^*, \hat{w}^*)$.

% To summarize Thm.~\ref{thm:bounded-incentive-controllability}, we first solve the optimization problem \eqref{eq:bounded-incentive-controllability-equivalence} to obtain $(u^*, \hat{w}^*)$.
% Then, using Thm.~\ref{thm:linear-convex-incentives-iterative-method} and the LoMPCs as black-box solvers, we get an optimal incentive $\lambda^*$.
% Thm.~\ref{thm:bounded-incentive-controllability} guarantees that the solution obtained from the LoMPCs with incentive $\lambda^*$ has bounded error compared to $\hat{w}^*$.
% Additionally, the error bound, given by \eqref{eq:bounded-incentive-controllability-error-bound}, is independent of the total number of LoMPCs $M$.

In the case of a single LoMPC, as considered in Sec.~\ref{sec:linear-incentives-for-hierarchical-mpc}, the incentive controllability property for linear incentives leads to the equivalent formulation \eqref{eq:equivalent-bimpc} of the incentive BiMPC \eqref{eq:incentivized-bimpc}.
Similarly, we can show that the bounded incentive controllability property, Lem.~\ref{lem:bounded-incentive-controllability}, leads to a robust formulation of the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc}.
By Lem.~\ref{lem:bounded-incentive-controllability}, for any $w \in \dom{g}$, we can find an incentive such that the average LoMPC solution $w^* \in \dom{g}$ has bounded error compared to $w$.
We must account for this error to ensure that $(u^*, w^*)$ is also feasible for the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc}.

% Given any $(u^*, \hat{w}^*)$ with $\hat{w}^* \in \dom{g}$, and $w^{i*}$ from \eqref{eq:bounded-incentive-controllability-wi}, we can obtain an error bound $\beta$ for the control input at time $t = 0$ from Thm.~\ref{thm:bounded-incentive-controllability} as,
% \begin{equation}
% \label{eq:w0-error-bound-beta}
% \begin{split}
%     \bigl| \hat{w}^*_0 - (1/M) \textstyle{\sum}_i \, w^{i*}_0\bigr| & = \bigl| (\hat{w}^* - (1/M) \textstyle{\sum}_i \, w^{i*})^\top e_1\bigr|, \\
%     & \leq \bigl\lVert \hat{w}^* - (1/M) \textstyle{\sum}_i \, w^{i*} \bigr\rVert \lVert e_1\rVert_*, \\
%     & \leq \frac{\bar{\theta}}{\constsc} \lVert e_1\rVert_* =: \beta,
% \end{split}
% \end{equation}
% where $e_1$ is the unit vector $[1, 0, ..., 0]^\top$, and $\lVert \cdot \rVert_*$ is the dual norm (see Sec.~\ref{subsubsec:conjugate-of-a-convex-function}).
Next, we describe a robust formulation of \eqref{eq:multiple-lompcs-incentivized-bimpc} using Lem.~\ref{lem:bounded-incentive-controllability}.
The following main result is a generalization of Thm.~\ref{thm:bimpc-equivalence} for the case of multiple LoMPCs (see Tab.~\ref{tab:properties-of-the-dual-functions-and-main-results}).

\begin{theorem} (Robust BiMPC formulation)
\label{thm:robust-bimpc-formulation}
Let Assumptions~\ref{assum:linear-convex-incentive-controllability} and \ref{assum:multiple-lompcs-properties-of-gi} hold.
For any $\xi_0$, let $(u^*, \hat{w}^*)$ be an optimal solution of the following optimization problem:
\begin{subequations}
\label{eq:robust-bimpc}
\begin{alignat}{6}
    (u^*, \hat{w}^*) \in \ && \argmin_{\nu=(u, \hat{w})} \ & f(\nu; \xi_0), \label{subeq:robust-bimpc-cost} \\
    && \ \text{s.t} \quad &
    \hat{w} \in \dom g(\cdot \; ; \xi_0, u), \label{subeq:robust-bimpc-w-g-feasibility} \\
    &&& \{(u, \hat{w})\} + \mathset{E} \subset \dom f(\cdot \; ; \xi_0), \label{subeq:robust-bimpc-w-f-feasibility}
\end{alignat}
\end{subequations}
where the addition between sets in \eqref{subeq:robust-bimpc-w-f-feasibility} is the Minkowski sum (see Sec.~\ref{subsec:notations}), and
\begin{equation}
\label{eq:robust-bimpc-error-set}
    \mathset{E} := \{(\mathbb{0}, w) \in \real^{\Nu\horizon} \times \real^{\Nw\horizon}: \lVert w \rVert \leq \bar{\theta}/\constsc\}.
\end{equation}
Then, for $\lambda^* \in \! \mathset{K}^*$ such that $-D\phi(\hat{w}^*)^\top\lambda^* \in \partial_w g(\hat{w}^*; \xi_0, u^*)$,
$(u^*, \bm{w}^*, \lambda^*)$ is feasible for the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc} and
\begin{equation*}
    \bigl\lVert \hat{w}^* - w^* \bigr\rVert \leq \bar{\theta}/\constsc,
\end{equation*}
where $\bm{w}^* = (w^{1*}, ..., w^{M*})$, $w^* = (1/M) \sum_{i \in \mathset{M}} w^{i*}$, and
\begin{equation*}
    w^{i*} = \textstyle{\argmin}_{w^i} ( g(w^i; \xi_0, u^*) + \langle \lambda^*, \phi(w^i) \rangle + \langle \theta^i, w^i \rangle ).
\end{equation*}
\end{theorem}

\begin{proof}
Since $\mathbb{0}_{(\Nu+\Nw)\horizon} \in \mathset{E}$ and $(u^*, \hat{w}^*)$ satisfies the constraint \eqref{subeq:robust-bimpc-w-f-feasibility}, $(u^*, \hat{w}^*) \in \dom f(\cdot \; ; \xi_0)$.
The assumptions of Lem.~\ref{lem:bounded-incentive-controllability} are satisfied, and so \eqref{eq:bounded-incentive-controllability-error-bound} holds.
Thus, by \eqref{eq:bounded-incentive-controllability-error-bound} and \eqref{subeq:robust-bimpc-w-f-feasibility},
\begin{equation*}
    (u^*, w^*) \in (u^*, \hat{w}^*) + \mathset{E} \subset \dom f(\cdot \; ; \xi_0).
\end{equation*}
Therefore, $(u^*, \bm{w}^*, \lambda^*)$ is feasible for the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc}.
\end{proof}

The robust BiMPC \eqref{eq:robust-bimpc} defines a team-optimal solution $(u^*, \hat{w}^*)$ for the incentive hierarchical problem given by \eqref{eq:multiple-lompcs-incentivized-bimpc} and \eqref{eq:linear-convex-incentivized-lompc}.
Since the same incentive is provided to all LoMPCs, it might not be possible to obtain the team-optimal solution $(u^*, \hat{w}^*)$ for any incentive.
However, Thm.~\ref{thm:robust-bimpc-formulation} shows that we can achieve a solution $(u^*, w^*)$ that has bounded error compared to the team-optimal solution.
The robust formulation \eqref{eq:robust-bimpc} shrinks the domain of the BiMPC problem to account for this error and thus guarantees that the obtained solution $(u^*, w^*)$ satisfies the BiMPC constraints.

\hidetext{
\begin{remark} (Bounded optimality of the Robust BiMPC)
The error bound $\lVert \hat{w}^* - w^* \rVert \leq \bar{\theta}/\constsc$ from Thm.~\ref{thm:robust-bimpc-formulation}, together with the optimal value of \eqref{eq:robust-bimpc}, also provides an upper bound on the optimal value of the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc}.
This optimality bound can be used to guarantee stability (in the sense of Lyapunov) for the system.
\end{remark}
}

\begin{remark} (Explicit robust BiMPC formulation)
\label{rem:explicit-robust-bimpc-formulation}
Since the state and input constraints of the BiMPC are convex (see Sec.~\ref{subsec:hierarchical-formulation}), $\dom{f}$ is convex.
Thus, the robustness constraint \eqref{subeq:robust-bimpc-w-f-feasibility} is a convex constraint.
When the state constraints of the BiMPC problem \eqref{eq:bimpc} are polytopic or quadratic, we can explicitly write the robustness constraint \eqref{subeq:robust-bimpc-w-f-feasibility} using duality theory.
We show this explicit formulation in App.~\ref{app:robust-bimpc-formulation}.
\end{remark}

\begin{remark} (Robustness horizon)
\label{rem:robustness-horizon}
The robust BiMPC \eqref{eq:robust-bimpc} accounts for the error in the optimization variable $\hat{w}$ for the entire horizon $\horizon$.
The robustness horizon $\horizon_r > 0$ can be defined as the number of time steps for which the error in $\hat{w}$ is considered.
For $\horizon_r \leq \horizon$, we modify the robust BiMPC \eqref{eq:robust-bimpc} by redefining $\mathset{E}$ in \eqref{subeq:robust-bimpc-w-f-feasibility} as follows:
\begin{equation*}
% \label{eq:robust-bimpc-error-set-robust-horizon}
    \mathset{E} := \{(\mathbb{0}, w_r, \mathbb{0}) \in \real^{\Nu\horizon} \times \real^{\Nw\horizon_r} \times \real^{\Nw(\horizon-\horizon_r)}: \lVert w_r \rVert \leq k \bar{\theta}/\constsc\},
\end{equation*}
where $k = \max_w \{\lVert w_{0:\horizon_r-1} \rVert: \lVert w\rVert \leq 1\}$.
% Using $\mathset{E}$ (as defined above) for $\horizon_r < \horizon$ takes away the feasibility guarantee of $(u^*, \bm{w}^*, \lambda^*)$ for \eqref{eq:multiple-lompcs-incentivized-bimpc}.
Using $\horizon_r < \horizon$ expands the feasible set of \eqref{eq:robust-bimpc} but only guarantees the satisfaction of the BiMPC constraints for $\horizon_r$ time steps.
In other words, the state $x^*_k$ obtained from $(u^*, \bm{w}^*)$ might violate the BiMPC state constraints for $k > \horizon_r$.
However, since the hierarchical MPC problem is solved with state feedback at each time step (see Fig.~\ref{fig:hierarchical-mpc-formulation}), we can choose $N_r < N$, as long as \eqref{eq:robust-bimpc} is feasible.
% There is a tradeoff for different values of the robustness horizon $N_r$:
% For large values of $N_r$, the robust BiMPC \eqref{eq:robust-bimpc} can be overly conservative, while smaller values of $N_r$ can affect the feasibility of \eqref{eq:robust-bimpc}.
% For the example considered in Sec.~\todo{(ref)}, we choose $N_r < N$ and discuss the tradeoff between different values of $N_r$.
\end{remark}

\begin{remark}
\label{rem:partition-of-lompcs}
    We can extend the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc} to account for a partition of the set of LoMPCs $\mathset{M}$ as $\sqcup_j \, \mathset{M}_j$.
    Here, the BiMPC assigns a different incentive $\lambda^j$ to each set of LoMPCs $\mathset{M}_j$.
    We can formulate a robust BiMPC similar to \eqref{eq:robust-bimpc} by considering the error bounds corresponding to each partition.
\end{remark}

To summarize the results in this section, we first specify the assumptions on the incentive hierarchical MPC problem in Assumption~\ref{assum:multiple-lompcs-properties-of-gi}.
Then, we show that although incentive controllability does not hold for the case of multiple LoMPCs, we can still guarantee bounded incentive controllability, Lem.~\ref{lem:bounded-incentive-controllability}.
Finally, we use the bounded incentive controllability property to formulate a robust BiMPC, Thm.~\ref{thm:robust-bimpc-formulation}, which defines a team-optimal solution.
In the next subsection, we outline a procedure for solving the hierarchical MPC problem given by \eqref{eq:multiple-lompcs-incentivized-bimpc} and \eqref{eq:linear-convex-incentivized-lompc} using the results from Secs.~\ref{sec:linear-convex-incentives} and \ref{sec:multiple-lompcs}.

\subsection{Computation of the Hierarchical MPC Solution}
\label{subsec:computation-of-the-hierarchical-mpc-solution}

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/incentive-coordinator-design.png}
    \caption{\changes{A framework for solving the hierarchical MPC problem given by \eqref{eq:multiple-lompcs-incentivized-bimpc} and \eqref{eq:linear-convex-incentivized-lompc} (this figure corresponds to the controller subsystem in Fig.~\ref{fig:hierarchical-mpc-formulation})}.
    At each time step, given the current state $\xi(t)$, we first solve the robust BiMPC \eqref{eq:robust-bimpc} to get the team-optimal solution $(u^*, \hat{w}^*)$.
    By the bounded incentive controllability property, Lem.~\ref{lem:bounded-incentive-controllability}, we can find an incentive $\lambda^*$ such that the average LoMPC solution $w^*$ has bounded error compared to $\hat{w}^*$.
    The \emph{incentive solver} uses the iterative method in Thm.~\ref{thm:linear-convex-incentives-iterative-method} to compute $\lambda^*$.
    The output $(u^*, \bm{w}^*)$ is guaranteed to be feasible for the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc}.
    In the flowchart, the solid lines indicate information flows that happen multiple times per time step, while the dashed lines indicate those that happen once.
    }
    \label{fig:hierarchical-mpc-solution-procedure}
\end{figure}

We propose a two-step procedure to solve the hierarchical MPC problem given by \eqref{eq:multiple-lompcs-incentivized-bimpc} and \eqref{eq:linear-convex-incentivized-lompc}.
The method is depicted in Fig.~\ref{fig:hierarchical-mpc-solution-procedure}, and Algs.~\ref{alg:iterative-method} and \ref{alg:incentivized-bimpc-solution} provide an algorithm for the solution computation.

We first describe the overall method, as shown in Fig.~\ref{fig:hierarchical-mpc-solution-procedure} and Alg.~\ref{alg:incentivized-bimpc-solution}.
At each time step, given the current state $\xi(t)$, we first solve the robust BiMPC problem \eqref{eq:robust-bimpc} to obtain a team-optimal solution $(u^*, \hat{w}^*)$ (line~\ref{alg-line:solve-robust-bimpc}, Alg.~\ref{alg:incentivized-bimpc-solution}).
The team-optimal solution is provided to an \emph{incentive solver} that iteratively computes an optimal incentive $\lambda^*$ corresponding to $\hat{w}^*$ using Thm.~\ref{thm:linear-convex-incentives-iterative-method} (line~\ref{alg-line:solve-optimal-incentive}, Alg.~\ref{alg:incentivized-bimpc-solution}).
We note that, from Thm.~\ref{thm:robust-bimpc-formulation}, the LoMPC solution $\bm{w}^*$ obtained using the incentive $\lambda^*$ is feasible for the incentive BiMPC \eqref{eq:multiple-lompcs-incentivized-bimpc}.
The optimal solution $(u^*, \bm{w}^*)$ is then used as an input to the dynamical system \eqref{eq:coupled-dynamics} to obtain the next state $\xi(t+1)$ (see Fig.~\ref{fig:hierarchical-mpc-formulation}).

Next, we describe the \emph{incentive solver}, which is used to compute an optimal incentive $\lambda^*$ given the team-optimal solution $(u^*, \hat{w}^*)$, as shown in Alg.~\ref{alg:iterative-method}.
We follow the iterative method described in Thm.~\ref{thm:linear-convex-incentives-iterative-method}.
At each iteration, we update the $w$-iterate using \eqref{subeq:linear-convex-incentives-iterative-method-const-step-gradient} to the average LoMPC solution obtained from the current incentive $\lambda$ (line~\ref{alg-line:w-update}, Alg.~\ref{alg:iterative-method}).
The $\lambda$-iterate is updated using the majorization-minimization step in \eqref{subeq:linear-convex-incentives-iterative-method-const-step-flow} (line~\ref{alg-line:lambda-update}, Alg.~\ref{alg:iterative-method}).
We iterate these update steps until the error $\lVert \hat{w}^* - w\rVert$ is less than the $\bar{\theta}/\constsc$, which is the error bound guaranteed by Lem.~\ref{lem:bounded-incentive-controllability}, up to a tolerance $\epsilon_\text{tol} > 0$.
We emphasize that the iterative method described by Alg.~\ref{alg:iterative-method} does not require the knowledge of the LoMPC functions $g^i$.
Instead, the optimal solutions from the LoMPCs are repeatedly queried to obtain an optimal incentive (see Assumption~\ref{assum:privacy-of-lompc}).


\begin{algorithm}[tp]
\small
\DontPrintSemicolon
\caption{Incentive BiMPC Solver}\label{alg:incentivized-bimpc-solution}
\Input{$\xi(t)$ \tcp*[r]{Current state}}
\Output{$(u^*, \bm{w}^*)$, $\lambda^*$ \tcp*[r]{Incentive BiMPC solution}}
\Function{
\solveIncentiveBiMPC{$\xi(t)$} \\
\tcp*[h]{Solves the incentive BiMPC problem \eqref{eq:multiple-lompcs-incentivized-bimpc} using \eqref{eq:robust-bimpc}}
}{
    $\xi_0 \gets \xi(t)$\;
    \tcp*[h]{Solve robust BiMPC \eqref{eq:robust-bimpc}}\;
    $(u^*, \hat{w}^*) \gets$ \solveRobustBiMPC{$\xi_0$}\label{alg-line:solve-robust-bimpc}\;
    \tcp*[h]{Optimal incentive from Alg.~\ref{alg:iterative-method}}\;
    $\lambda^* \gets$ \optimalIncentive{$\xi_0, u^*, \hat{w}^*$}\label{alg-line:solve-optimal-incentive}\;
    \For{$i \gets 1$ \KwTo $M$}{
    $w^{i*} \gets w^{i*}(\xi_0, u^*, \lambda^*)$ \tcp*[r]{see \eqref{subeq:multiple-lompcs-incentivized-bimpc-w-optimality}}
    }
    $\bm{w}^* \gets (w^{1*}, ..., w^{M*})$\;
    \KwRet $u^*$, $\bm{w}^*$, $\lambda^*$\;
}
\end{algorithm}


\subsection{Advantages and Disadvantages of Proposed Method}
\label{subsec:advantages-and-disadvantages-of-proposed-method}

Finally, we discuss the advantages and disadvantages of our method compared to an exact bilevel optimization solver, such as the one proposed in \cite{mintz2018control}.
The advantages of our method are as follows:
1) Our method scales well to a large number of LoMPCs.
In particular, neither the robust BiMPC computation time nor the optimal incentive computation time grows with the number of LoMPCs (the optimal incentive computation relies on the LoMPC solution for any incentive, which is performed in a distributed manner).
Also, the error bound provided by Lem.~\ref{lem:bounded-incentive-controllability} is independent of the number of LoMPCs.
In contrast, the BiMPC reformulation in \cite{mintz2018control} grows in size with the number of LoMPCs.
2) Our method only requires the knowledge of the LoMPC domain, but not the LoMPC cost function.
The disadvantages of our method are as follows:
1) Our method only guarantees the optimality of the incentive BiMPC problem \eqref{eq:multiple-lompcs-incentivized-bimpc} up to an error bound.
2) Our method places constraints on the LoMPC structure (see Assumption~\ref{assum:multiple-lompcs-properties-of-gi}) and does not provide any guarantees when the incentive $\lambda$ is constrained.
On the other hand, the BiMPC reformulation in \cite{mintz2018control} allows for constraints on the incentive $\lambda$ and computes the optimal solution of the BiMPC.


\begin{algorithm}[tp]
\small
\DontPrintSemicolon
\caption{Iterative Method for Optimal Incentives}\label{alg:iterative-method}
\Input{$\xi_0$, $(u^*, \hat{w}^*)$ \tcp*[r]{Initial state, team-optimal solution}}
\Parameter{$\epsilon_{\text{tol}}$, $\bar{\theta}$, $\constsc$}
\Output{$\lambda^*$ \tcp*[r]{$\lambda^*$ is such that \eqref{eq:bounded-incentive-controllability-wi}, \eqref{eq:bounded-incentive-controllability-error-bound} hold}}
% \Function{\avgLoMPCSolution{$\xi_0$, $u^*$, $\lambda$}}{
%     $w \gets$ $(1/M)\sum_{i \in \mathset{M}} w^{i*}(\xi_0, u^*, \lambda)$ \tcp*[r]{see \eqref{subeq:multiple-lompcs-incentivized-bimpc-w-optimality}}
%     \KwRet $w$
% }
% \Function{\errorLoMPCSolution{$\xi_0$, $u^*$, $\lambda$, $\hat{w}^*$} \\
% \tcp*[h]{Convergence error, see Lem.~\ref{lem:bounded-incentive-controllability}}}{
%     $\text{err} \gets$ $\max_{i \in \mathset{M}} \lVert \hat{w}^* - w^{i*}(\xi_0, u^*, \lambda)\rVert$\;
%     \KwRet $\text{err}$
% }
\Function{\optimalIncentive{$\xi_0$, $u^*$, $\hat{w}^*$} \\
\tcp*[h]{Solves for an optimal incentive using Thm.~\ref{thm:linear-convex-incentives-iterative-method}}}{
    $\lambda \gets \lambda_{\text{ws}}$\tcp*[r]{Warm-start solution from previous call}
    $(w, \text{err}) \gets$ \LoMPCSolution{$\xi_0$, $u^*$, $\lambda$, $\hat{w}^*$}\;
    % \While{$\lVert w - \hat{w}^*\rVert \geq \bar{\theta}/\constsc + \epsilon_{\text{tol}}$}{
    \While{$\text{err} > \bar{\theta}/\constsc + \epsilon_{\text{tol}}$}{
    \tcp*[h]{Solve \eqref{subeq:linear-convex-incentives-iterative-method-const-step-flow} with $(w^{(k)}, \lambda^{(k)}, w^*) = (w, \lambda, \hat{w}^*)$}\;
    $\lambda \gets$ \incentiveUpdate{$w$, $\lambda$, $\hat{w}^*$}\label{alg-line:lambda-update}\;
    \tcp*[h]{Update $w$, see \eqref{subeq:linear-convex-incentives-iterative-method-const-step-gradient}}\;
    $(w, \text{err}) \gets$ \LoMPCSolution{$\xi_0$, $u^*$, $\lambda$, $\hat{w}^*$}\label{alg-line:w-update}
    }
    \tcp*[h]{Regularize incentive using \eqref{eq:optimal-incentive-regularization}, Rem.~\ref{rem:optimal-incentive-regularization}}\;
    $\lambda \gets$ \incentiveRegularization{$\lambda$, $w$}\;
    \KwRet $\lambda$
}
\Function{\LoMPCSolution{$\xi_0$, $u^*$, $\lambda$, $\hat{w}^*$}}{
    \tcp*[h]{Average LoMPC solution, see \eqref{eq:linear-convex-incentivized-lompc} and \eqref{eq:parametric-form-gi}}\;
    $w \gets$ $(1/M)\sum_{i \in \mathset{M}} w^{i*}(\xi_0, u^*, \lambda)$\;
    \tcp*[h]{Convergence error, see Lem.~\ref{lem:bounded-incentive-controllability}}\;
    % $\text{err} \gets$ $\max_{i \in \mathset{M}} \lVert \hat{w}^* - w^{i*}(\xi_0, u^*, \lambda)\rVert$\;
    $\text{err} \gets$ $\lVert \hat{w}^* - w\rVert$\;
    \KwRet $(w, \text{err})$
}
\end{algorithm}
