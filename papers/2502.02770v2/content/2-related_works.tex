\section{Related Works}

% \mingyu{Citations!!! Do not wait until the last minutes to add citations. They will greatly affect the length of the paper so we need to get them in place NOW.}

\textbf{Sparse Attention.} Sparse attention techniques aim to reduce the memory and computational burden of self-attention in long-context LLMs. H2O~\cite{zhang2023h2o}, StreamingLLM~\cite{xiao2023streamingllm} and SnapKV ~\cite{li2024snapkv} evict non-critical tokens in a static, query- agnostic manner, which are often referred to as KV cache compression. In contract, SparQ~\cite{ribar2023sparq}, Double Sparsity~\cite{yang2024post} and Quest~\cite{tang2024quest} retain all tokens in GPU and select critical tokens to save loading. Recent works like RetrievalAttention~\cite{liu2024retrievalattention} adopts advanced algorithm to better estimate the token criticality. However, these methods are all top-$k$ based, requiring configuring an appropriate budget beforehead, leading to over-selection or under-selection. In this paper, we focus on solving this problem via a  more essential approach.

% SparQ and Double Sparsity filter important tokens using online and offline heavy channels of the incoming query and key cache. 
% Quest segments tokens into pages and dynamically selects relevant ones through page-based estimation.
% These methods can be categorized based on three criteria: (1) whether they retain all tokens (selection-based) or evict tokens (eviction-based), (2) whether they target prefilling or decoding, and (3) whether their sparse patterns are query-aware or query-agnostic.

% and LM-Infinite~\cite{han2024lminfinitezeroshotextremelength} are eviction-based, query-agnostic methods designed for decoding, retaining only the initial and local tokens.
%  and Scissorhands~\cite{liu2023scissorhandsexploitingpersistenceimportance} use query-aware patterns for token eviction.
% Both identify and preserve "heavy hitter" tokens based on accumulated attention scores while evicting less important ones.
% SparQ~\cite{ribar2023sparq}, Double Sparsity~\cite{yang2024post} and Quest~\cite{tang2024quest} are selection-based, query-aware methods for decoding. 
% SparQ and Double Sparsity filter important tokens using online and offline heavy channels of the incoming query and key cache. 
% Quest segments tokens into pages and dynamically selects relevant ones through page-based estimation.
% TidalDecode~\cite{yang2024tidaldecodefastaccuratellm} is selection-based and query-aware, leveraging earlier attention layers to determine which tokens to select in later layers during decoding.
% RetrievalAttention~\cite{liu2024retrievalattention} is selection-based and query-aware, utilizing approximate nearest neighbor search (ANNS) to identify and retrieve the most relevant KV vectors during decoding.
% FlexPrefill~\cite{anonymous2025flexprefill} and MInference~\cite{jiang2024minference10acceleratingprefilling} are sparse attention methods designed for prefilling. 
% FlexPrefill dynamically adjusts sparse patterns and computational budgets in real-time to adapt to query-specific demands. 
% MInference applies pre-identified sparse patterns optimized for GPU computation to significantly reduce prefilling latency.
% MagicPIG~\cite{chen2024magicpiglshsamplingefficient} introduces local sensitive hashing (LSH) as a selection-based, query-aware sampling strategy to replace top-$k$ attention. 
\textbf{Dynamic Budget.} Recent studies have extensively demonstrated that the optimal budgets vary significantly across different layers \cite{cai2024pyramidkv, yang2024pyramidinfer}, attention heads \cite{feng2025adakvoptimizingkvcache, xiao2024duo, tang2024razorattention}, and prompts (tasks) \cite{zhou2024dynamickv}.
% This dynamic nature is also confirmed by our subsequent experiments (\autoref{fig:dynamism}). 
These works tend to focus on only one aspect of the dynamism in attention mechanisms. However, in this paper, we point out that it is the different distributions of attention weights that are the root cause of this dynamism

% This variability poses significant challenges for deploying these algorithms in serving systems, where the budget must be configured statically before deployment and remains fixed for extended periods until the next modification.

% DuoAttention~\cite{xiao2024duoattentionefficientlongcontextllm} combines both eviction and selection strategies. 
% It applies a full KV cache to retrieval heads and a constant-length lightweight KV cache to streaming heads.
% Ada-KV~\cite{feng2025adakvoptimizingkvcache} is an eviction-based, query-aware method that introduces head-wise adaptive budget allocation.


% However, these methods largely rely on top-$k$ or fixed budgets, which are unable to adapt dynamically to the varying accuracy-efficiency trade-offs required in real-world scenarios. 
% This limitation motivates the development of approaches like \textit{Twilight}, which introduces adaptive budgeting to sparse attention.

% \subsection{KV Cache Quantization}


\textbf{Other KV Cache Optimizations} Several alternative approaches focus on optimizing the KV cache beyond sparsification, including quantization~\cite{hooper2024kvquant10millioncontext, kivi,kang2024gearefficientkvcache,nawrot2024dynamicmemorycompressionretrofitting}, linear attention~\cite{wang2020linformerselfattentionlinearcomplexity, katharopoulos2020transformersrnnsfastautoregressive}, and memory-efficient attention mechanisms such as FlashAttention~\cite{dao2023flashattention2} and SageAttention~\cite{zhang2025sageattention,zhang2024sageattention2}.
% LinearAttention~\cite{} and Linformer~\cite{} approximate self-attention with linear complexity, significantly reducing KV cache size. 
% However, these methods alter the attention mechanism itself, making them complementary to our approach, which preserves standard attention structures.
% Memory-efficient attention methods, including  optimize the computation and memory usage of attention by improving data movement and low bit computation. 
% These techniques are independent of sparse attention and can be integrated with our method for further efficiency gains.
Our approach is orthogonal to these methods, as it focuses on adaptive KV cache pruning rather than compression, approximation, or memory-efficient computation, and can be combined with them for enhanced performance.