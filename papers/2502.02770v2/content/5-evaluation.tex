\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/qbit.pdf}}
\caption{Sum of normalized attention weights for selected tokens under different quantization bits, with $p=0.85$.}
\label{fig:qbit}
\end{center}
\end{figure}

\section{Evaluation}

In this section, we will perform several experiments to demonstrate that equipping current SOTA sparse attention algorithms with Twilight improves their efficiency while preserving the accuracy.

\subsection{Accuracy Evaluation}

\subsubsection{Setup}
\textbf{Benchmarks and Models.} We evaluate Twilight on two types of benchmarks: Longbench \cite{bai2024longbench} for long context and three separate tests for short context ($500\sim2$k tokens), GSM8K \cite{cobbe2021training}, COQA \cite{reddy-etal-2019-coqa} and perplexity on PG19 dataset \cite{rae2019compressive}. We select three widely used models, Longchat-7B-v1.5-32k \cite{longchat2023}, LLaMA2-7b-chat \cite{touvron2023llama} and LLaMA-3.1-8B-Instruct \cite{dubey2024llama} (128k context length), with two of them have the long context ability $\ge$ 32k, covering two mainstream attention implementations \textit{multi-head attention} (MHA) and \textit{group
query attention} (GQA).

\textbf{Baselines.} We use two SOTA top-$k$ sparse attention methods \textbf{Quest} \cite{tang2024quest} and \textbf{DS} \cite{yang2024post}. The hyperparameter $p$ of Twilight is set to $0.95$ for LLaMA 2/3 and $0.85$ for Longchat. We do not apply any sparse methods to the first two layers as it is illustrated in baselines' paper to ensure a fair comparison.

\subsubsection{Results on Longbench}

We comprehensively evaluate Twilight's long context ability on 12 different tasks chosen from Longbench, covering all task types, on two long context models. For each baseline, we choose four budget settings: 256, 1024, 4096, 8192 to observe the impact on budget, and a specially setting named "Twilight", means using Twilight pruner in this algorithm to dynamically determine the budget. In this case, the base algorithm uses a very conservative budget, 8192, in our case. We also equip Full with Twilight (i.e. The token selector is a trivial one which selects all tokens each time) for better comparison.

The full results are shown in \autoref{table:longbench} in \autoref{appendix}. In Longchat, the Twilight series outperforms its original version by up to 5.7\% in score, while successfully pruning up to \textbf{98\%} of the redundant tokens over-selected by the base algorithm. In LLaMA-3.1-8b-Instruct, Twilight achieves almost zero accuracy loss ($<$1\%) with a slight increase in budget usage. We hypothesize that this slight increase is due to the knowledge being more compressed in LLaMA-3.1.

\subsubsection{Results on Short Tasks}
\begin{table}[ht]
\caption{Evaluation on three short-context benchmarks.}
\label{table:midbench}
\begin{center}
\begin{small}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Methods} &  GSM8K(flexible/strict)$\uparrow$ & COQA(em/f1)$\uparrow$ & PG-19 Perplexity$\downarrow$ \\
\midrule
\multicolumn{4}{c}{\textsc{Llama-2-7B-Chat}} \\
\midrule
Full Cache & 0.2290/0.2282 & 0.5935/0.7511  & 7.503 \\
Quest & 0.0114/0.0061 & 0.5150/0.6991  & 14.15 \\
DS & 0.1820/0.1812 & 0.6043/0.7632  & 7.622 \\
Twilight & \textbf{0.2153/0.2115}& \textbf{0.6088/0.7642} & \textbf{7.600} \\
\midrule
\multicolumn{4}{c}{\textsc{Llama-3.1-8B-Instruct}} \\
\midrule
Full Cache & 0.7726/0.7475 & 0.6363/0.7882  & 7.490 \\
Quest & 0.0773/0.0652 & 0.5310/0.7033  & 19.00 \\
DS & 0.3806/0.3609 & \textbf{0.6490/0.8003}  & 10.23 \\
Twilight & \textbf{0.7771/0.7604} & 0.6325/0.7869 & \textbf{7.529}\\
\bottomrule
\end{tabular}
}
\end{small}
\end{center}
\end{table}
We then demonstrate that the Twilight pruner itself does not negatively impact performance on two zero-shot generation tasks: GSM8K and COQA using lm-harness framework \cite{gao2021framework}, as well as one perplexity test in the PG-19 data set. Since we are specifically evaluating the pruner, we do not integrate Twilight into the baseline models. All baselines use a budget of 64, which is comparable to the budget after Twilight's pruning. The results in \autoref{table:midbench} show that Twilight outperforms Quest and DS by a significant margin, with nearly zero loss compared to full attention.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/breakdown.pdf}}
\caption{Time breakdown of self-attention. At batch size $64$, Quest-Twi (Quest with Twilight) outperforms Quest without Twilight by approximate $2 \times$.}
\label{fig:breakdown}
\end{center}
\end{figure}

\subsection{Efficiency Evaluation}

\subsubsection{Setup}
\textbf{Datasets.} We evaluate the efficiency of Twilight on both the self-attention operator and the end-to-end decoding latency on a single A100 GPU. Since the budget is prompt (task)-related, our experiments are conducted on Longbench, from which we select three different types of tasks: Qasper \cite{dasigi2021dataset} for QA, GovReport \cite{huang2021efficient} for summarization, and LCC \cite{guo2023longcoder} for coding. We use prompts ranging from 10k to 30k tokens for evaluation. Given that Twilight is designed for deploying sparse attention in serving systems, we use batch inference in our experiments.

\textbf{Baselines and Implementation Details.} We compare our methods with the following baselines:
PyTorch's scaled-dot-product-attention (SDPA), with \textbf{FlashAttention2} \cite{dao2023flashattention2} and Memory-Efficient Attention \cite{xFormers2022} as backends.
\textbf{FlashInfer} \cite{ye2025flashinfer}, a high-performance kernel library for LLM serving.
\textbf{Quest}, which achieves state-of-the-art latency performance among sparse attention methods.
We integrate Twilight with both FlashInfer and Quest, resulting in \textbf{FlashInfer-Twi} and \textbf{Quest-Twi}. We modified the Quest kernels to support batch inference. We implemented Twilight using both CUDA and OpenAI Triton \cite{tillet2019triton}, with the technical details described in Section \ref{sec:kernel}.

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth*2]{figures/kernels.pdf}}
\caption{Latency and speedup of self-attention at different sequence length and batch size.}
\label{fig:kernel}
\end{center}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth*2]{figures/e2e.pdf}}
\caption{Time-Per-Output-Token (TPOT) speedup in end-to-end serving scenario.}
\label{fig:e2e}
\end{center}
\end{figure*}

\subsubsection{Speedup on Self-Attention}

We first evaluate the speedup on self-attention operator across different batch sizes and sequence length. As \autoref{fig:kernel} shows, FlashInfer-Twi and Quest-Twi achieve a speedup up to $6.5\times$ and $15.4\times$ separately comparing with FlashAttention2. Moreoever, they accelerates the base algorithm (FlashInfer and Quest) by $2.2\times$ and $1.4\times$, separately.

\subsubsection{End-to-End Decoding Speedup}
We conduct end-to-end inference evaluation with the similar settings, with the difference that the batch size ranges from 32 to 256, for a better simulation of serving scenario. \autoref{fig:e2e} tells that Quest-Twi achieves up to $3.9\times$ decoding acceleration comparing with FlashInfer, with a $1.35\times$ extra speedup comparing to Quest without Twilight.

\subsection{Ablation Study}
\textbf{Selection of Quantization Bits.} We previously argued that 4-bit quantization is suitable for estimating attention weights in Section \ref{sec:kernel}. To empirically verify this, we examined the sum of real attention weights for 2-bit, 4-bit, and 8-bit quantizations. As shown in \autoref{fig:qbit}, the sum of attention weights for 2-bit quantization drops significantly, while 4-bit and 8-bit quantizations maintain stability. Although 8-bit quantization can be used for attention computation \cite{zhang2025sageattention}, it is overly precise for the purpose of top-$p$ sampling. Considering the balance between estimation accuracy and computational efficiency, 4-bit quantization emerges as a good choice.

\textbf{Time Breakdown for Twilight.} Given Twilight's hierarchical architecture, which comprises three distinct components, it is crucial to analyze the time breakdown. \autoref{fig:breakdown} illustrates the time breakdown for different batch sizes in a 32k retrieval task. In this scenario, Quest employs a budget of 8192 (1/4 sparsity), while Twilight further prunes this budget down to 256. The breakdown aligns closely with the theoretical cost model we presented earlier, demonstrating that Twilight significantly reduces the time required for the sparse attention kernel while introducing some additional overheads.