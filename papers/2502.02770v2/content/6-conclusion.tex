\section{Conclusion}

In this paper, we first highlight that top-$k$ attention struggles to find an optimal budget due to the dynamic nature of attention weight distributions. We then introduce Twilight, a framework with hierarchical selector-optimizer architecture that leverages top-$p$ filtering to address this challenge. Empirical results demonstrate that Twilight can adaptively prune up to 98\% of redundant tokens, resulting in a $15.4\times$ acceleration in self-attention operations and a $3.9\times$ reduction in end-to-end per-token latency. Comparing to the base algorithm it is applied to, Twilight offers an additional $2\times$ speedup. Our work underscores the importance of adaptive attention sparsity, paving the way for future research on efficient sparse attention.