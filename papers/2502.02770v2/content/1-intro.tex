\section{Introduction}
\label{submission}

\begin{figure*}
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{figures/teaser.pdf}}
\caption{\textbf{Comparison of Top-$k$ and Top-$p$ Sparsity in Approximate Attention.} Approximate attention typically employs techniques such as pooling, channel pruning, and quantization to approximate the query($\tilde{Q}$) and key($\tilde{K}$), enabling the estimation of attention weights. These weights are then used to select important tokens for sparse attention. (a) Top-$k$ Sparsity, utilized by methods like H2O and Quest, relies on a fixed $k$-token budget, which often results in \textbf{over-selection} ($\sum \tilde{p_i}>$ 0.9) or \textbf{under-selection} ($\sum \tilde{p_i}<$ 0.9). (b) Our proposed Top-$p$ Sparsity \textbf{dynamically adjusts the budget} to accumulate sufficient attention weights ($\sum \tilde{p_i} = $ 0.9), enabling more efficient and adaptive sparse attention.}
\label{fig:teaser}
\end{center}
\end{figure*}

% \mingyu{We need citations here and there in the intro, when talking about background trends (i.e., the first paragraph) and previous work.}

Large language models (LLMs) with long-context capabilities have revolutionized a wide array of natural language processing tasks, such as retrieval-based applications, document summarization~\cite{bai2024longbench}, and code generation~\cite{jain2024livecodebenchholisticcontaminationfree}.
The increasing availability of models supporting context windows up to 1M to 10M tokens~\cite{yang2025qwen251mtechnicalreport,geminiteam2024gemini15unlockingmultimodal} highlights the growing potential of these advancements. 
For instance, video language models (VLMs)~\cite{wang2024qwen2vlenhancingvisionlanguagemodels} that process video often require tens of thousands of tokens. 
Similarly, large reasoning models~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability,kimiteam2025kimik15scalingreinforcement}, which are rapidly growing in popularity, frequently demand substantial token lengths to enable chain-of-thought (CoT) reasoning. 
Consequently, the importance of long-context LLMs is increasing rapidly to meet the needs of these sophisticated applications.

Despite the transformative potential of long-context LLMs, they come with substantial computational and memory costs~\cite{zhang2023h2o,tang2024quest,xiao2024duo}, primarily driven by the attention mechanism. 
% During the prefilling stage, the attention mechanism exhibits quadratic computational complexity relative to the context length. 
% This inherent property of scaled dot-product attention makes it highly computational intensive. 
% Consequently, when processing long contexts, attention computations dominate the prefilling latency, leading to a bottleneck in the overall system performance. 
% Unlike the prefilling stage, where multiple queries are processed simultaneously, decoding involves generating only one token at a time. 
% This process relies heavily on loading a large key-value (KV) cache to calculate the next tokenâ€™s probability distribution. 
In the decoding stage, the latency is primarily attributed to memory bandwidth limitations, as the KV cache must be repeatedly loaded. 
This memory-bound nature of attention leads to increased latency since the KV cache sizes grow as the sequence becomes longer.
% While the latency of linear layers remains constant during generation, the latency of attention increases progressively, consuming an ever-greater proportion of the end-to-end latency in long-context generation tasks. 
Furthermore, the substantial size of the KV cache significantly increases GPU memory consumption, compounding the challenges of scaling long-context LLMs.
% \mingyu{This paragraph is a bit too verbose; it can be shrinked if space is needed.}

\cf{Previous research has extensively investigated the use of attention sparsity (KV cache sparsity) to accelerate long-context inference, both during the prefilling and decoding stages. The core idea is to compute approximate attention on a subset of tokens, often referred to as ``critical tokens" or ``heavy hitters"~\cite{zhang2023h2o}. 
% Since attention mechanisms are typically memory-bound, the volume of I/O operations is closely related to the speed of computation. 
In practical deployments of these algorithms, it is necessary to specify the number of selected tokens, denoted as $B$, commonly referred to as the KV cache budget. A top-$k$ operation is required to identify the indices of the critical tokens that correspond to the top-$B$ highest estimated scores. As previously mentioned, a smaller $B$ significantly reduces the I/O operations, while a larger $B$ retains more contextual information, thereby minimizing accuracy loss. 
% Therefore, selecting an appropriate value for $B$ can achieve the best of both worlds. 
}

% \jiaming{However, the distribution of attention weights varies across different attention heads. Previous works~\cite{retrievalhead, xiao2024duo} have demonstrated that some heads, referred to as retrieval heads, are trained to extract important information from long contexts, while others focus only on local information. As shown in Fig.~\ref{fig:teaser}, some attention distributions concentrate on a small subset of tokens, which we refer to as focused attention. In contrast, some attention distributions are flatter, where many tokens have similar attention weights; we define this as diffuse attention. For focused attention, using a fixed token budget for Top-$k$ attention often leads to over-selection, as only a few tokens are sufficient to accumulate sufficient attention weights. Similarly, for diffuse attention, a fixed token budget can result in under-selection, as a larger number of tokens are necessary to ensure accurate attention modeling.}
%As shown in \autoref{fig:ppl}, top-$k$ algorithms reach a saturation point where both accuracy and efficiency are optimized.}

% \mingyu{What is $K$ (and similarly, seq\_len)? Undefined notation. (Actually worse! This is conflicting notation; $K$ is used as the key tensor later in the paper, not a length, while $B$ as the number of token is a length) 
% Either define the notations here, or use natural language to explain the flow rather than using math notations}

% Serving long context inference with an efficient algorithm and a suitable $B$ seems like a promising approach to mitigating challenges in attention operations.
However, identifying the optimal values for $B$ where both accuracy and efficiency are optimized is inherently challenging due to two major reasons:
\textbf{(a) Saturation points are runtime dynamic.} 
Previous works~\cite{wu2024retrievalheadmechanisticallyexplains, xiao2024duo} have demonstrated that some heads, referred to as retrieval heads, are trained to extract important information from long contexts, while others focus only on local information. 
As shown in Figure~\ref{fig:teaser}, the distribution of attention weights may vary across different attention heads. 
Some attention distributions concentrate on a small subset of tokens, which we refer to as focused attention. 
In contrast, some attention distributions are flatter, where many tokens have similar attention weights; we define this as diffuse attention.
For focused attention, using a fixed token budget for Top-$k$ attention often leads to over-selection, as only a few tokens are sufficient to accumulate sufficient attention weights. 
Similarly, for diffuse attention, a fixed token budget can result in under-selection, as a larger number of tokens is necessary to ensure accurate attention modeling.
\textbf{(b) Current algorithms suffer from varying degrees of inefficiency.} As shown in Figure \ref{fig:ppl}, we observe that the saturation point is highly dependent on the specific algorithm, necessitating offline calibration to determine the appropriate budget for each algorithm individually. The actual algorithms, like Quest \cite{tang2024quest} or DS \cite{yang2024post}, have to over-select some tokens as the inevitable inaccuracy in estimating the importance of tokens comparing with oracle.

% \cf{{\textbf{(a) Current algorithms suffer from varying degrees of inefficiency.}} As shown in Figure \ref{fig:ppl}, we observe that the saturation point is highly dependent on the specific algorithm, necessitating offline calibration to determine the appropriate budget for each algorithm individually.
% {\textbf{(b) Saturation points are runtime dynamic across different dimensions.}} Recent studies have extensively demonstrated that the optimal budgets vary significantly across different layers \cite{cai2024pyramidkv, yang2024pyramidinfer}, attention heads \cite{feng2024adakvoptimizingkvcache, xiao2024duo, tang2024razorattention}, and prompts (tasks) \cite{zhou2024dynamickv}. This dynamic nature is also confirmed by our subsequent experiments (\autoref{fig:dynamism}). This variability poses significant challenges for deploying these algorithms in serving systems, where the budget must be configured statically before deployment and remains fixed for extended periods until the next modification.}

\cf{In this work, we reveal that top-$k$ methods exhibit issues similar to those previously encountered in LLM sampling. Drawing on this analogy, we introduce top-$p$ sampling into sparse attention to address the budget estimation problem. Our study demonstrates that top-$p$ can determine the KV cache budget in a more intrinsic and dynamic way compared to top-$k$. Based on these observations, we built Twilight, a hierarchical KV cache pruning framework that enhances existing sparse attention algorithms with adaptive budgeting capabilities. Specifically, Twilight first lets the base algorithm to select a large subset of tokens using a conservative budget, and then further refines this subset by retaining only the top-$p$ tokens.}

Evaluations are conducted in two aspects: accuracy and efficiency. First, we demonstrate that Twilight optimizes the base algorithms with nearly no accuracy loss on both mid-context benchmarks (GSM8K \cite{cobbe2021training}, COQA \cite{reddy-etal-2019-coqa}, PG19 dataset \cite{rae2019compressive}) and a comprehensive long-context benchmark, Longbench \cite{bai2024longbench}. Next, we show that Twilight accelerates full attention by up to \jiaming{$15.4\times$} times and existing sparse attention methods, by up to \jiaming{$2.2\times$} times, leading to a \jiaming{$3.9\times$} end-to-end speedup. Our contributions are summarized as follows:
\begin{itemize}
\item We conduct an in-depth investigation into a significant issue in top-$k$ sparse attention: the difficulty in identifying the optimal budget (the saturation point). We propose using top-$p$ sampling to dynamically determine this point at runtime.
\item We introduce Twilight, a framework that can endow any existing sparse attention method with adaptive budgeting capabilities, thereby improving their efficiency and facilitating their deployment.
\item We evaluate Twilight in terms of both accuracy and efficiency, demonstrating an \jiaming{$15.4\times$} speedup on self-attention.
\end{itemize}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/ppl.pdf}}
\caption{The relationship of budgets to perplexity on PG-19 dataset in different top-$k$ sparse attention methods.
% \mingyu{Use (export as) pdf files for the figures. Pdf files allow the texts in the figure to be selectable and searchable, while png cannot.\\
% This figure is current unexplained in the text.\\
% Also, there are multiple-defined labels, fig:ppl and fig:time. Check the latex compilation warnings and fix.}
}
\label{fig:ppl}
\end{center}
\vskip -0.3in
\end{figure}