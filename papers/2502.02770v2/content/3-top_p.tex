\section{Bringing Top-$p$ Sampling to Sparse Attention\label{sec:top_p}}

In this section, we formulate the current sparse attention methods and re-examine the root causes of the problems. We argue that to mathematically approximate the attention output, the goal is to select a minimal set of indices such that the sum of their attention scores meets a certain threshold. Therefore, we propose that top-$p$ sampling should be used instead of top-$k$ to filter out the critical tokens.

\subsection{Problem Formulation\label{sec:formulate}}

We start by formulating the sparse attention. Consider the attention computation during the decoding phase, where we have the query vector $Q \in \mathbb{R}^{1 \times d}$, and the key-value cache $K, V \in \mathbb{R}^{n \times d}$. Here, $d$ denotes the head dimension, and $n$ represents the context length. 
% The standard decoding attention can be formulated as
% \begin{equation}
%     O = \text{softmax} \bigg( \frac{Q \cdot K^T}{\sqrt{d}} \bigg) V = WV
% \end{equation}
% where $W \in \mathbb{R}^{1\times n}$ (sometimes denoted as $P$) represents the (normalized) attention weights. 
% Sparse attention, on the other hand, only loads a subset of tokens from the KV cache and computes the partial attention, which can be represented using a mask matrix.
\begin{definition}[Sparse Attention]
Let $\mathcal{I}$ be the set of selected indices, the output of the sparse attention equals to
\begin{equation}
    \hat{O} = \text{softmax} \bigg( \frac{Q \cdot K^T}{\sqrt{d}} \bigg) \Lambda_{\mathcal{I}} V = W \Lambda_{\mathcal{I}} V
\end{equation}
where $\Lambda_{\mathcal{I}} \in \mathbb{R}^{n \times n},
    \Lambda_{\mathcal{I}}[i, j] = 
    \begin{cases}
    1 & \text{if } i=j \text{ and } i \in \mathcal{I} \\
    0 & \text{otherwise}
    \end{cases}
$.
\end{definition}

% As mentioned above, there are two major types of top-$k$-based methods: static (query-agnostic) and dynamic (query-aware) methods. In our formulation, the difference lies on whether $\mathcal{I}$ depends on the query vector each time.

% \mingyu{Citation needed. Again, you need to add citations when you write the draft, not later. Because it is difficult for you to know where a citation is needed afterwards.}

\cf{
To minimize the output error $\Vert O-\hat{O}\Vert$, we need to carefully select the subset of tokens that are used in the sparse attention computation. However, directly optimizing this objective function without loading the full KV cache is challenging. Earlier research has shown that the distribution of V is relatively smooth \cite{atom}, which implies that the bound is relatively tight.
\begin{equation}
\label{eq:error}
\begin{aligned}
\mathcal{L} = \Vert O - \hat{O}\Vert &= \Vert W(\Lambda_\mathcal{I} - \mathbf{1}^{n \times n}) V\Vert \\
&\le \Vert W(\Lambda_\mathcal{I} - \mathbf{1}^{n \times n}) \Vert \cdot \Vert V \Vert
\end{aligned}
\end{equation}
Therefore, the objective becomes minimizing $\Vert W(\Lambda_{\mathcal{I}} - \mathbf{1}_{n\times n})\Vert = 1 - \sum_{i \in \mathcal{I}} W[i]$, which means selecting a subset of tokens that maximizes the sum of attention weights. If we fix the number of the subset, i.e. $|\mathcal{I}|$, then we have the oracle top-$k$ attention:

\begin{definition}[Oracle Top-$k$ Sparse Attention] Given the budget $B$,
\label{def:topk}
\begin{equation}
    \mathcal{I} = \arg\max_{\mathcal{I}} \sum_{i=1}^n W \Lambda_{\mathcal{I}}\ \ \ \text{s.t.} \ |\mathcal{I}| = B
\end{equation}
\end{definition}
}

The oracle top-$k$ attention serves as a theoretical upperbound of current sparse methods.

% \begin{figure*}[ht]
% \begin{center}
% \centerline{\includegraphics[width=2\columnwidth]{figures/distrib1.pdf}}
% \caption{Diverse distributions observed in attention weights. \textbf{The leftmost image} illustrates a "Flat" distribution \textbf{(Diffuse Attention)}, where the weights are uniformly distributed. \textbf{The middle image} depicts a "Peaked" distribution \textbf{(Focused Attention)}, where the weights are concentrated on the head and tail tokens. When overlaid, the differences between these distributions become readily apparent.}
% \label{fig:distrib}
% \end{center}
% \end{figure*}

\begin{figure*}[t]
  \centering
    \includegraphics[width=0.666\columnwidth]{figures/diffuse.pdf}
    \includegraphics[width=0.666\columnwidth]{figures/focus.pdf}
    \includegraphics[width=0.666\columnwidth]{figures/overlap.pdf}
  \caption{Diverse distributions observed in attention weights. \textbf{The leftmost image} illustrates a "Flat" distribution \textbf{(Diffuse Attention)}, where the weights are uniformly distributed. \textbf{The middle image} depicts a "Peaked" distribution \textbf{(Focused Attention)}, where the weights are concentrated on the head and tail tokens. When overlaid, the differences between these distributions become readily apparent.}
  \label{fig:distrib}
\end{figure*}

\subsection{Rethink the Problem of Top-$k$}
\cf{The Achilles’ heel of top-$k$ attention, as we described earlier, is the dilemma in determining a uniform budget $B$. A larger $B$ leads to inefficiency, while a smaller $B$ results in accuracy loss. We find that this predicament is quite similar to the one encountered in the sampling phase of large language models (LLMs). During the sampling phase, the model samples the final output token from a predicted probability distribution. Nucleus sampling \cite{holtzman2019curious}, or top-$p$ sampling, was proposed to address the problem that top-$k$ sampling cannot adapt to different next-word distributions.}

\cf{Motivated by this insight, we examine the distributions of attention weights more closely. \autoref{fig:distrib} displays two different types of attention weight distributions in real LLMs mentioned in \autoref{fig:teaser}. In \autoref{eq:error}, we demonstrated that the output error can be related to the sum of attention weights. It is straightforward to observe that, when comparing a flat distribution to a peaked one, a greater number of tokens must be selected in the flat distribution to reach the same cumulative threshold. Therefore, we argue that \textbf{the core reason for budget dynamism is the dynamic nature of attention weight distributions at runtime.} Drawing inspiration from top-$p$ sampling, we introduce top-$p$ sparse attention by directly apply threshold to the sum of attention weights.}

% We believe that \textbf{the core reason for the dynamic budgets is the dynamic distributions of attention weights at runtime}, which motivates us to replace top-$k$ in current sparse attention algorithm with top-$p$.


% However, we found that \texttt{top-$k$} \mingyu{why such style set?} using a fixed budget $B$, making it impossible for existing algorithms to leverage adaptive sparsity. 
% \mingyu{Grammar issue; this sentence misses the predicate (the verb).}
% As Figure \mingyu{?} shows, the distributions of attention weights vary across different attention heads, layers and queries. Using a uniformed $B$ leads to either inefficiency due to a larger $B$ or accuracy lose due to a smaller $B$. This brings challenges when we deploy these algorithms in serving systems, where


% \begin{figure}[ht]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figures/distrib.png}}
% \caption{Different distributions which not only appear in next word probability distribution of LLM sampling, but also in attention weights distribution. For a “Flat” distribution, the weights are more uniform and top-$k$ should use a larger budget. For a “Peaked” distribution, the weights is more skewed and top-$k$ can achieve the same probability sum with a less budget.}
% \label{fig:distr}
% \end{center}
% \end{figure}

\begin{definition}[Oracle Top-$p$ Sparse Attention] Given the threshold $p$,
\label{def:topp}
\begin{equation}
    \mathcal{I} = \arg\min_{\mathcal{I}} |\mathcal{I}|\ \ \ \ \text{s.t.} \ \sum_{i=1}^n W \Lambda_{\mathcal{I}} \ge p
\end{equation}
\end{definition}

% Top-$p$ is inherently adaptive to different distributions since it directly goes to our ultimate goal, i.e. the sum of normalized attention weights.

Comparing to top-$k$, top-$p$ is advantageous because it provides a theoretical upperbound of error in \autoref{eq:error} by $(1-p) \cdot \Vert V \Vert$. Under this circumstance, top-$p$ reduces the budget as low as possible, making it both efficient and adaptive to different distributions.

% We evaluated the oracle top-$p$ in two respects: efficiency and its adaptive budget capability. For the former, we use cosine similarity as the metrics with a 10k retrieval prompt, as \autoref{fig:cos} shows. The relative error for top-$p$ with $p=0.95$ was found to be around the theoretical bound, proving its effective control of error. For top-$k$, the results were also good with $B=128$, but degradation was observed with $B=16$, indicating under-selection. We successfully observe the budget dyxnamism in \autoref{fig:dynamism}, which demonstrates that top-$p$ sparse attention can adaptively adjust attention sparsity at runtime. Given this capability, we will focus on using top-$p$ to address the budget problem in top-$k$ in the next section.

% Additionally, we found that real-practice top-$k$ methods, such as Quest \cite{tang2024quest}, performed worse than the oracle top-$k$ with the same budget, highlighting the limitations of fixed-budget approaches

% \mingyu{You have these many formal definitions and notations, but at the end you do not have a formal proof to show Def 3.3 is better than 3.2 to match the error minimization of Eq (4). While intuitively this is true, showing it explicitly would be better.}

% \begin{figure}[h]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figures/cos.pdf}}
% \caption{Relative output error measured by cosine similarity in each layer.}
% \label{fig:cos}
% \end{center}
% \vskip -0.3in
% \end{figure}

% \begin{figure}[ht]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figures/dynamism.pdf}}
% \caption{Dynamic budgets observed in oracle top-$p$ attention. We observe the dynamism across four dimensions: different \textbf{prompts (tasks)}, different \textbf{queries} with the same prompt, different \textbf{layers} in the same query, and different \textbf{heads} in the same layer.}
% \label{fig:dynamism}
% \end{center}
% \end{figure}