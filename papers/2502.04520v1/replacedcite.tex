\section{Related Works}
\subsection{Language Model Interpretation}
Language models (LMs) ____ are gaining widespread attention across various fields due to their strong performance on a variety of tasks, like reasoning and knowledge retrieval. However, the black-box nature of (neural) LMs hinders human's understanding of their working mechanism. Various methods have been developed to interpret LM behavior by analyzing its parameters and intermediate representations. Several works suggest that LMs store knowledge inside the feedforward layers____, which are used in a key-value matching manner to map inputs into related knowledge____. Some parameters are also found to perform certain relational transformations for the LM____, known as the task representations____.
For certain subsets of relations, LMs have been unexpectedly found to encode knowledge in a linear manner____, suggesting a potential role of linearity in their understanding of relational structures. However, it remains unknown how the LM understands the transformation between relations. Our work shows the linearity between the output from several relation pairs given the same input.

\subsection{Model Generalization}


The power of modern deep neural networks lies in their remarkable ability to generalize effectively to unseen inputs. However, the exact mechanisms through which these models achieve generalization remain poorly understood. For instance, in the context of knowledge editing, numerous research studies have observed that standard fine-tuning methods for updating knowledge often struggle to meet critical objectives simultaneously ____. On one hand, they fail to prevent unintended modifications to unrelated knowledge. On the other hand, they frequently fall short of ensuring that logical deductions based on the updated knowledge are properly incorporated ____. 
Previous research has proposed various metrics and methods to measure and predict generalization in deep neural networks. However, these approaches don't cover the perspective of correlation in model generalization proposed in our work ____.

\subsection{Hallucination Detection}

Hallucination remains one of the most significant challenges in the deployment of language models (LMs) ____. Numerous studies have explored approaches to predict and mitigate this issue. For instance, some prior works utilize trained classifiers to identify hallucinations ____. Another method involves detecting hallucinations by clustering semantically similar responses and calculating entropy across these clusters ____. Additionally, the MIND framework has been proposed to exploit the internal states of LMs during inference, enabling real-time hallucination detection ____. Moreover, formal methods guided by iterative prompting have been employed to dehallucinate LM outputs ____. RAG has also been used to detect and correct hallucinations in LM ____. Our study presents an innovative approach to predicting hallucinations, different from existing methodologies, by leveraging the correlation.