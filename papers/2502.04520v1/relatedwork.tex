\section{Related Works}
\subsection{Language Model Interpretation}
Language models (LMs) ~\citep{achiam2023gpt4,team2024gemma,olmo,dubey2024llama3} are gaining widespread attention across various fields due to their strong performance on a variety of tasks, like reasoning and knowledge retrieval. However, the black-box nature of (neural) LMs hinders human's understanding of their working mechanism. Various methods have been developed to interpret LM behavior by analyzing its parameters and intermediate representations. Several works suggest that LMs store knowledge inside the feedforward layers~\citep{key_value,knowledge_neuron,locate_edit_fact}, which are used in a key-value matching manner to map inputs into related knowledge~\citep{ffn_vocab}. Some parameters are also found to perform certain relational transformations for the LM~\cite{fv,linguistic_region}, known as the task representations~\citep{task_representation}.
For certain subsets of relations, LMs have been unexpectedly found to encode knowledge in a linear manner~\citep{linear_relation}, suggesting a potential role of linearity in their understanding of relational structures. However, it remains unknown how the LM understands the transformation between relations. Our work shows the linearity between the output from several relation pairs given the same input.

\subsection{Model Generalization}


The power of modern deep neural networks lies in their remarkable ability to generalize effectively to unseen inputs. However, the exact mechanisms through which these models achieve generalization remain poorly understood. For instance, in the context of knowledge editing, numerous research studies have observed that standard fine-tuning methods for updating knowledge often struggle to meet critical objectives simultaneously \cite{onoe2023can, hoelscher2023detecting, meng2022locating, gupta2023editing}. On one hand, they fail to prevent unintended modifications to unrelated knowledge. On the other hand, they frequently fall short of ensuring that logical deductions based on the updated knowledge are properly incorporated \cite{cohen2024evaluating, zhong2023mquake}. 
Previous research has proposed various metrics and methods to measure and predict generalization in deep neural networks. However, these approaches don't cover the perspective of correlation in model generalization proposed in our work \cite{yu2022predictingoutofdistributionerrorprojection, garg2022leveragingunlabeleddatapredict, kang2024learningdynamicsrevealgeneralization}.

\subsection{Hallucination Detection}

Hallucination remains one of the most significant challenges in the deployment of language models (LMs) \cite{zhang2023sirenssongaiocean, Huang_2024}. Numerous studies have explored approaches to predict and mitigate this issue. For instance, some prior works utilize trained classifiers to identify hallucinations \cite{jiang2024largelanguagemodelshallucination, quevedo2024detectinghallucinationslargelanguage, chen2024hallucinationdetectionrobustlydiscerning}. Another method involves detecting hallucinations by clustering semantically similar responses and calculating entropy across these clusters \cite{farquhar2024detecting}. Additionally, the MIND framework has been proposed to exploit the internal states of LMs during inference, enabling real-time hallucination detection \cite{su2024unsupervisedrealtimehallucinationdetection}. Moreover, formal methods guided by iterative prompting have been employed to dehallucinate LM outputs \cite{jha2023dehallucinating}. RAG has also been used to detect and correct hallucinations in LM \cite{mishra2024finegrainedhallucinationdetectionediting}. Our study presents an innovative approach to predicting hallucinations, different from existing methodologies, by leveraging the correlation.