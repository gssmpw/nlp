The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse).
This paper uncovers the phenomenon of \emph{linear correlations} in LMs during knowledge composition. 
For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., ``\textit{X lives in the city of}''$\rightarrow$ ``\textit{X lives in the country of}'' for every given \textit{X}. 
This mirrors the linearity in human knowledge composition, such as \textit{Paris}$\rightarrow$ \textit{France}. 
Our findings indicate that the linear transformation is 1) resilient to large-scale fine-tuning, 2) generalizing updated knowledge when aligned with real-world relationships, 3) but causing hallucinations when it deviates. 
Empirical results suggest that linear correlation can serve as a potential identifier of LM's generalization. 
Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter. \footnote{Code: \href{https://github.com/KomeijiForce/LinCorr}{https://github.com/KomeijiForce/LinCorr}}

% This paper reveals that language models (LMs) can perform if-then generalization between \emph{two related relations} (e.g., if ``X's \emph{residential city} is \underline{Tokyo}'', then ``X's \emph{residential country} is \underline{Japan}.'') in a linear manner.
% Given two relations, one can prompt an LM with various Xs for both relations to obtain paired logit distributions of next-token prediction.
% We empirically show that it is possible to fit these paired distributions properly using a linear transformation. 
% We find that when such linearity holds, this particular linear transformation remains robust even after fine-tuning the LM on large-scale data, suggesting that fine-tuning on one relation (e.g., \emph{residential city}) can implicitly teach the LM about related relations (e.g., \emph{residential country}).
% We also investigate how the linearity is built inside the LM, which shows the linearity is maintained by each layer.
% Furthermore, if this linearity is faulty, it introduces a new type of hallucination that cannot be easily corrected through fine-tuning, as it arises from a fundamental misalignment in the model's generalization process.
% These findings deepen our understanding of how LMs encode and generalize relational knowledge, with implications for improving reasoning reliability and mitigating hallucinations.


% This paper unveils an important role of linearity in LM's if-then generalization across relations $(s, r, o)$, e.g., $(\textrm{John}, \textrm{Residential City}, \textrm{Tokyo}) \rightarrow (\textrm{John}, \textrm{Residential Country}, \textrm{Japan})$. When implication $(s, r_1, o_1) \rightarrow (s, r_2, o_2)$ holds, LMs are found capable of (but not always) learning a linear transformation from $o_1$ to $o_2$ in the logit of next token prediction by prompting $(s, r_1)$ and $(s, r_2)$. More interestingly, the linear transformation is stable enough to map $o_1$ to $o_2$ with the model after large-scale fine-tuning. 
% Consequently, learning new $(s, r_1, o_1)$ automatically generalizes to $(s, r_1, o_2)$ through the linear transformation. In cases where linearity is not learned, LMs show weaker if-then generalization across relations. By detecting layer-wise outputs from LM, we find each layer inside the LM maintains the linearity. Finally, we identify a new type of hallucination caused by fault linear transformation in generalization, which is hard to eliminate as it is also stable to fine-tuning.

% This paper unveils a linear correlation across LM decoding with the same entity and a certain set of relations. 
% \jingbo{the example here is not clear}
% For instance, we find $(W, b)$ transforms \jingbo{I assume this is a $Wx+b$ transformation. Then what is $x$ here? The embedding of this sentence? Or the distribution of the next token prediction? The predicted logit of the first prompt -> the predicted logit of the second prompt; Please revise the draft and reflect this. } the \emph{``X lives in the city of''} prediction to \emph{``X lives in the country of''}, even when \emph{``X''} \jingbo{Instead of ``X'', I would rather use ``$<$entity$>$'' since in the beginning, you mentioned ``same entity''} is an arbitrary \jingbo{how arbitrary it is? needs to be person? or some entity? or even random tokens? Can we have this ``X'' in different places? Not at the beginning of the prompt? A: Random token+any position} phrase unable to identify the output. 
% More interestingly, $(W, b)$ is found stable after large scale of fine-tuning \jingbo{do you mean $(W, b)$ is the same between base LM vs. instruction-tuned LM? A: Yes. Is there any requirement/condition? For example, the other parts in the prompt are the same relation? We mentioned ``a certain set of relations'' in the first sentence. What is this certain set? A: Currently, this is post-verified as in the linearity relation paper. ``the other parts in the prompt are the same'' is not a requirement, e.g., ``X speaks the language of'' and ``X originates from the country of''}. 
% \jingbo{this seems a finding just related to the correlation between those words? Like city --$>$ country, tokyo --$>$ japan? A: This is the underlying reason for model transitive generalization?}
% Following this attribute \jingbo{property?}, we find LM able to compose knowledge with linearity transformation, e.g., \emph{``X lives in the city of \textbf{Tokyo}''} $\rightarrow$ \emph{``X lives in the country of \textbf{Japan}''}. 
% In contrast, relations without linearity transformation are much harder to transfer knowledge to each other. 
% Our work reveals how linearity \jingbo{what is this linearity? Can we define it by the LM itself? For example, can we add some constraint/regularization during the LM pre-training/SFT to help LM preserve such property? Or this is for granted from the word embedding space? Yes, I think this property is tied with the distribution of word embeddings. Then what's the implication here to LM? Or shall we shout it out that some generalization in LM is not by LM? I feel the current abstract is not clear at the conclusion and implication. Because this structure should be inside the encoding parameters instead of word embeddings. But I } helps LMs generalize learned knowledge to other relations.