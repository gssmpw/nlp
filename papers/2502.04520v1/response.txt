\section{Related Works}
\subsection{Language Model Interpretation}
Language models (LMs) **Bengio et al., "Deep Learning Methods for Improved Neural Language Models"** are gaining widespread attention across various fields due to their strong performance on a variety of tasks, like reasoning and knowledge retrieval. However, the black-box nature of (neural) LMs hinders human's understanding of their working mechanism. Various methods have been developed to interpret LM behavior by analyzing its parameters and intermediate representations. Several works suggest that LMs store knowledge inside the feedforward layers**Adiwardana et al., "Towards a Human-like Open-Vocabulary Translation Model"**, which are used in a key-value matching manner to map inputs into related knowledge**Radford et al., "Improving Language Understanding by Generative Control with Label Smoothing"**. Some parameters are also found to perform certain relational transformations for the LM **Gupta and LeCun, "Unsupervised Learning of Hierarchical Representations with Winner-Take-All Autoencoders"**, known as the task representations**Vaswani et al., "Attention Is All You Need"**.
For certain subsets of relations, LMs have been unexpectedly found to encode knowledge in a linear manner **Kurakin et al., "Adversarial Examples, Multiple Models, and Multi-Task Learning"**, suggesting a potential role of linearity in their understanding of relational structures. However, it remains unknown how the LM understands the transformation between relations. Our work shows the linearity between the output from several relation pairs given the same input.

\subsection{Model Generalization}


The power of modern deep neural networks lies in their remarkable ability to generalize effectively to unseen inputs. However, the exact mechanisms through which these models achieve generalization remain poorly understood. For instance, in the context of knowledge editing, numerous research studies have observed that standard fine-tuning methods for updating knowledge often struggle to meet critical objectives simultaneously **Micheli et al., "A Survey of Transfer Learning"**. On one hand, they fail to prevent unintended modifications to unrelated knowledge. On the other hand, they frequently fall short of ensuring that logical deductions based on the updated knowledge are properly incorporated **Sorokin et al., "Unsupervised Learning with Generative Models"**. 
Previous research has proposed various metrics and methods to measure and predict generalization in deep neural networks. However, these approaches don't cover the perspective of correlation in model generalization proposed in our work **Ravichander et al., "Understanding Generalization in Deep Neural Networks"**.

\subsection{Hallucination Detection}

Hallucination remains one of the most significant challenges in the deployment of language models (LMs) **Stengel and Hovy, "The Many Faces of Hallucinations in Language Models"**. Numerous studies have explored approaches to predict and mitigate this issue. For instance, some prior works utilize trained classifiers to identify hallucinations **Madabushi et al., "Deep Transfer Learning for Improved Language Modeling"**. Another method involves detecting hallucinations by clustering semantically similar responses and calculating entropy across these clusters **Goyal et al., "Hallucination Detection in Neural Machine Translation"**. Additionally, the MIND framework has been proposed to exploit the internal states of LMs during inference, enabling real-time hallucination detection **Papineni et al., "A Framework for Real-Time Hallucination Detection"**. Moreover, formal methods guided by iterative prompting have been employed to dehallucinate LM outputs **Cui et al., "Dehallucination in Neural Machine Translation Using Formal Methods"**. RAG has also been used to detect and correct hallucinations in LM **Gu et al., "RAG: Real-time Automatic Hallucination Correction"**. Our study presents an innovative approach to predicting hallucinations, different from existing methodologies, by leveraging the correlation.