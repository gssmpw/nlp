\begin{table*}[h]
\small
\centering
\setlength{\tabcolsep}{4pt}
\begin{threeparttable}
  \caption{Evaluation of \lumyn on SRE scenarios}
  \label{tab:sreagent-eval}
  \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{Models}}
      & \multicolumn{4}{c}{\textbf{Diagnosis}}
      & \multicolumn{2}{c}{\textbf{Mitigation}} \\
    \cmidrule(lr){2-5}\cmidrule(lr){6-7}
    & \textbf{pass@1 (\%)$\uparrow$}
    & \textbf{FL (NTAM)$\uparrow$}
    & \textbf{FPC (NTAM)$\uparrow$}
    & \textbf{MTTD (s)$\downarrow$}
    & \textbf{pass@1 (\%)$\uparrow$}
    & \textbf{MTTR (s)$\downarrow$}\\
    \midrule
    \textbf{granite-3.1-8B-instruct} &
    \cellcolor[gray]{0.97} $3.57 \pm 0.94$ &
    \cellcolor[gray]{0.96} $0.16 \pm 0.02$ &
    \cellcolor[gray]{0.94} $0.19 \pm 0.02$ &
    $259.92 \pm 65.01$ &
    $0.24 \pm 0.25$ &
    $845.50 \pm \text{---}$ \\
    \textbf{llama-3.1-8B-instruct} &
    $0.99 \pm 0.51$ &
    $0.07 \pm 0.01$ &
    $0.08 \pm 0.01$ &
    \cellcolor[gray]{0.85} $\textbf{57.50} \pm 2.05$ &
    \cellcolor[gray]{0.98} $1.98 \pm 0.68$ &
    \cellcolor[gray]{0.85} $\textbf{245.13} \pm 40.66$ \\
    \textbf{llama-3.3-70B-instruct} &
    \cellcolor[gray]{0.98} $3.10 \pm 0.84$ &
    \cellcolor[gray]{0.96} $0.16 \pm 0.02$ &
    \cellcolor[gray]{0.95} $0.16 \pm 0.02$ &
    \cellcolor[gray]{0.95} $191.85 \pm 31.34$ &
    \cellcolor[gray]{0.96} $3.33 \pm 0.90$ &
    \cellcolor[gray]{0.98} $776.27 \pm 252.87$ \\
    \textbf{gpt-4o} &
    \cellcolor[gray]{0.85} $\textbf{13.81} \pm 1.67$ &
    \cellcolor[gray]{0.85} $\textbf{0.39} \pm 0.05$ &
    \cellcolor[gray]{0.85} $\textbf{0.34} \pm 0.03$ &
    \cellcolor[gray]{0.86} $72.44 \pm 4.71$ &
    \cellcolor[gray]{0.85} $\textbf{11.43} \pm 1.52$ &
    \cellcolor[gray]{0.86} $282.47 \pm 30.04$ \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \scriptsize
    \item[1] 42 scenarios (21 scenarios with traces and 21 without traces).
    \item[2] 10 runs per scenario per model.
    \item[3] pass@1 values are shown as percentages. `\text{---}' indicates missing data. 
    \item[4] std error for each metric is listed.
    \item[5] \textbf{FL (NTAM)} = Normalized topology-aware metric for root cause, 
          \textbf{FPC (NTAM)} = Normalized topology-aware metric for fault propagation chain (value between 0 and 1.0), 
          \textbf{MTTD} = Mean time to diagnosis (seconds), 
          \textbf{MTTR} = Mean time to repair (seconds). \textbf{Bold}: the best performance.
    \item[6] Details of NTAM are available in \Cref{appx:ntam}
  \end{tablenotes}
\end{threeparttable}
\end{table*}

\section{Results}
\label{sec:results}

\subsection{Evaluation Setup}
 

To understand the impact of reasoning and planning capabilities of LLMs on \bench scenarios, we instantiate our agents using different LLM models, both for natural language reasoning and code generation. 
Specifically, we employ GPT-4o\footnote{Checkpoint version 2024-11-20}, Llama-3.3-70B-instruct, Llama-3.1-8B-instruct, and Granite-3.1-8B-instruct for tasks that rely on natural language understanding and reasoning. For code-focused use cases, we utilize GPT-4o-mini, Llama-3.1-405b-instruct, and Mixtral-8x7b-instruct. 
All models use a context window of 128K tokens, enabling them to process more extensive input sequences.

We conduct our experiments primarily on AWS EC2 instances (m4.xlarge), although \bench can also be readily deployed on a consumer-grade laptop using a pseudo-cluster, thus making it easier to develop AI agents (Appendix \ref{appx:sre:exp_setup})

Below, we provide an overview of our baseline agentsâ€™ performance across \bench scenarios for SRE, CISO, and FinOps. Our findings indicate that both open-source and proprietary models often struggle with real-world tasks, underscoring the importance of benchmarks that push the limits of reasoning and planning in foundation models. For more comprehensive results and detailed scenario-level discussions, please refer to Appendix~\ref{appx:sre} (SRE), Appendix~\ref{appx:ciso} (CISO), and Appendix~\ref{appx:finops} (FinOps).

\begin{table*}[h]
\small
\centering
\setlength{\tabcolsep}{4pt}
\begin{threeparttable}
  \caption{Evaluation of CISO Compliance Assessment Agent on CISO scenarios}
  \label{tab:cisoagent-eval}
  \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{Models}}
      & \multicolumn{4}{c}{\textbf{Scenario pass@1 (\%) $\uparrow$}}
      & \multirow{2}{*}{\textbf{O/A pass@1 (\%) $\uparrow$}} 
      & \multirow{2}{*}{\textbf{TTP (s) $\downarrow$}} \\
    \cmidrule(lr){2-5}
    & \textbf{kyverno}
    & \textbf{k8s-opa}
    & \textbf{rhel-opa}
    & \textbf{kyverno-update} \\
    \midrule
    \textbf{granite-3.1-8B-instruct} &
    \cellcolor[gray]{0.99} $7.84 \pm 3.84$ &
    \cellcolor[gray]{1.00} $0.00 \pm 0.00$ &
    \cellcolor[gray]{1.00} $0.00 \pm 0.00$ &
    \cellcolor[gray]{1.00} $1.59 \pm 1.58$ &
    \cellcolor[gray]{1.00} $1.71 \pm 0.76$ &
    \cellcolor[gray]{1.00} $197.03 \pm 2.52$ \\
    \textbf{mixtral-8x7B-instruct} &
    \cellcolor[gray]{1.00} $7.35 \pm 3.19$ &
    \cellcolor[gray]{1.00} $1.43 \pm 1.42$ &
    \cellcolor[gray]{1.00} $0.00 \pm 0.00$ &
    \cellcolor[gray]{1.00} $1.29 \pm 4.34$ &
    \cellcolor[gray]{0.99} $3.94 \pm 1.03$ &
    \cellcolor[gray]{0.88} $120.63 \pm 3.77$ \\
    \textbf{llama-3.1-8B-instruct} &
    \cellcolor[gray]{0.99} $8.57 \pm 3.37$ &
    \cellcolor[gray]{1.00} $0.00 \pm 0.00$ &
    \cellcolor[gray]{1.00} $0.00 \pm 0.00$ &
    \cellcolor[gray]{0.94} $7.46 \pm 3.23$ &
    \cellcolor[gray]{0.99} $3.59 \pm 1.07$ &
    \cellcolor[gray]{0.88} $121.49 \pm 3.00$ \\
    \textbf{llama-3.3-70B-instruct} &
    \cellcolor[gray]{0.95} $18.46 \pm 4.94$ &
    \cellcolor[gray]{1.00} $0.00 \pm 0.00$ &
    \cellcolor[gray]{0.99} $1.43 \pm 2.88$ &
    \cellcolor[gray]{0.94} $8.06 \pm 3.50$ &
    \cellcolor[gray]{0.95} $9.32 \pm 1.67$ &
    \cellcolor[gray]{0.99} $189.61 \pm 2.71$ \\
    \textbf{mistral-large-2} &
    \cellcolor[gray]{1.00} $6.56 \pm 3.20$ &
    \cellcolor[gray]{0.92} $22.73 \pm 5.32$ &
    \cellcolor[gray]{0.96} $7.23 \pm 2.88$ &
    \cellcolor[gray]{0.92} $10.45 \pm 3.77$ &
    \cellcolor[gray]{0.94} $11.55 \pm 1.95$ &
    \cellcolor[gray]{0.95} $167.98 \pm 3.42$ \\
    \textbf{llama-3.1-405B-instruct} &
    \cellcolor[gray]{0.96} $16.22 \pm 4.32$ &
    \cellcolor[gray]{0.93} $20.83 \pm 4.86$ &
    \cellcolor[gray]{0.96} $8.75 \pm 3.26$ &
    \cellcolor[gray]{0.98} $3.17 \pm 2.22$ &
    \cellcolor[gray]{0.93} $12.46 \pm 1.98$ &
    \cellcolor[gray]{0.97} $178.89 \pm 3.37$ \\
    \textbf{gpt-4o-mini} &
    \cellcolor[gray]{0.96} $16.18 \pm 4.54$ &
    \cellcolor[gray]{0.85} $\textbf{43.10} \pm 6.99$ &
    \cellcolor[gray]{0.85} $\textbf{30.38} \pm 5.43$ &
    \cellcolor[gray]{0.93} $9.43 \pm 4.08$ &
    \cellcolor[gray]{0.85} $\textbf{25.19} \pm 2.80$ &
    \cellcolor[gray]{0.85} $102.40 \pm 3.70$ \\
    \textbf{gpt-4o} &
    \cellcolor[gray]{0.85} $\textbf{40.28} \pm 5.99$ &
    \cellcolor[gray]{0.86} $39.34 \pm 6.55$ &
    \cellcolor[gray]{0.96} $7.61 \pm 2.81$ &
    \cellcolor[gray]{0.85} $\textbf{17.74} \pm 4.92$ &
    \cellcolor[gray]{0.85} $24.74 \pm 2.64$ &
    \cellcolor[gray]{0.85} $\textbf{101.29} \pm 3.81$ \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \scriptsize
    \item[1] 50 scenarios.
    \item[2] 8 runs per scenario per model.
    \item[3] pass@1 values are shown as percentages.
    \item[4] TTP Time to process (seconds).\\
    \item[5] \textbf{kyverno} = New K8s CIS-benchmarks on Kyverno, easy scenario class; 
          \textbf{k8s-opa} = New K8s CIS-benchmarks on OPA, medium scenario class;
          \textbf{rhel-opa} = New RHEL9 CIS-benchmarks on Ansible-OPA, medium scenario class;
          \textbf{kyverno-update} = Update K8s CIS-benchmarks on Kyverno, hard scenario class.
  \end{tablenotes}
  \vspace{-10pt}
\end{threeparttable}
\end{table*}


\begin{table*}[h]
\small
\centering
\begin{threeparttable}
  \caption{Evaluation of FinOpsAgent on FinOps scenarios.}
  \label{tab:finopsagent-eval}
  \begin{tabular}{@{}lccp{1.85cm}p{1.85cm}p{1.85cm}p{1.85cm}@{}}
    \toprule
    \multirow{2}{*}{\textbf{Models}} 
      & \multicolumn{1}{c}{\textbf{Diagnosis}}
      & \multicolumn{5}{c}{\textbf{Mitigation}} \\
    \cmidrule(lr){2-2}\cmidrule(lr){3-7}
     & \textbf{pass@1 (\%) $\uparrow$} 
     & \textbf{pass@1 (\%) $\uparrow$} 
     & \textbf{Proximity to Optimal CPU Cost $\uparrow$} 
     & \textbf{Proximity to Optimal Memory Cost $\uparrow$} 
     & \textbf{Proximity to Optimal CPU Efficiency $\uparrow$} 
     & \textbf{Proximity to Optimal Memory Efficiency $\uparrow$} \\
    \midrule
    \textbf{granite-3.1-8B-instruct} 
      & 0 
      & 0 
      & $0.47 \pm 0.01$ 
      & \cellcolor[gray]{0.94} $0.48 \pm 0.06$ 
      & $0.53 \pm 0.04$ 
      & \cellcolor[gray]{0.93} $0.94 \pm 0.01$ \\
    \textbf{llama-3.1-8B-instruct} 
      & 0 
      & 0 
      & \cellcolor[gray]{0.85} $\textbf{0.49} \pm 0.01$ 
      & $0.46 \pm 0.07$ 
      & \cellcolor[gray]{0.95} $0.56 \pm 0.08$ 
      & \cellcolor[gray]{0.85} $0.96 \pm 0.02$ \\
    \textbf{llama-3.3-70B-instruct} 
      & \cellcolor[gray]{0.92} 16.6
      & 0 
      & $0.47 \pm 0.01$ 
      & \cellcolor[gray]{0.91} $0.49 \pm 0.05$ 
      & $0.53 \pm 0.03$ 
      & \cellcolor[gray]{0.85} $0.96 \pm 0.02$ \\
    \textbf{gpt-4o} 
      & \cellcolor[gray]{0.85} \textbf{33} 
      & 0 
      & \cellcolor[gray]{0.93} $0.48 \pm 0.01$ 
      & \cellcolor[gray]{0.85} $0.51 \pm 0.02$ 
      & \cellcolor[gray]{0.85} $\textbf{0.63} \pm 0.07$ 
      & $0.92 \pm 0.08$ \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \scriptsize
    \item pass@1 values are shown as percentages. 
    \item Proximity values shows how close the observed values to optimal values. 
    One represents achieving optimal and any deviations from 1 represents sub-optimal performance.
  \end{tablenotes}
\end{threeparttable}
\end{table*}

\subsection{Overall Results}
\Cref{tab:sreagent-eval}, \Cref{tab:cisoagent-eval} and \Cref{tab:finopsagent-eval} show the performance of SRE-agent, CISO-agent, and FinOps-agent respectively. 

\textbf{SRE.} 
We measure the efficiency of \lumyn on its ability to diagnose and mitigate production incidents (e.g., ``a high error rate on frontend service'').


Diagnosis efficiency is measured using pass@1\cite{chen2021evaluating} (i.e., identifying the cause as mentioned in ground truth), NTAM (Normalized Topology-Aware Metric) for root cause and fault propagation chain, and time to diagnosis\footnote{NTAM is Normalized topology-aware metric that measures the quality of the predicted root cause and fault propagation chains using a system and application topology. Refer to \Cref{appx:ntam}.}.
Mitigation efficiency is measured in terms of pass@1 (i.e., whether the alert was cleared) and mean time to repair.

As shown in \Cref{tab:sreagent-eval}, across all SRE scenarios, GPT-4o consistently outperforms the other models, achieving the highest pass@1 scores for diagnosis (13.81\%) and mitigation (11.43\%), as well as the highest score on NTAM (FL and FPC) metrics. 
Llama-3.3-70B ranks second overall, trailing GPT-4o on most metrics.
The 8B models have lower mitigation success rate. 
Surprisingly, Granite-3.1-8B (without any specialized finetuning) achieves higher accuracy than Llama-3.1-70B on the diagnosis task. 

Removing trace data can drastically reduce success rates (see \Cref{tab:appx:sre:traces} and \Cref{tab:appx:sre:disabled} in Appendix). For instance, GPT-4o's pass@1 in diagnosis falls from 13.81\% with traces to 9.52\% without them, and mitigation plummets to 2.86\%. This highlights the critical role of system observability in SRE, which \bench can evaluate under varying conditions. As there is no perfect observability in practice, how to guide SRE-agents to collect new observability data and to help SRE-agents reason about failures with incomplete observability is an important but open problem.

\textbf{CISO.}
We measure the efficacy of our agents across the four scenario classes introduced in \Cref{tab:bench_scenarios}. Each \textit{scenario\_class} imposes a distinct set of CIS-benchmarks requirements (e.g., ``minimize the admission of containers wishing to share the host network namespace''), each class has a specific level of complexity (e.g., Easy, Medium, or Hard), and generates scenario-specific code artifacts. 

The efficacy of CISO-agents is measured based on the ability to detect artifact misconfigurations (aka non-compliance, e.g., no minimum count of containers sharing namespace, or the count is above the threshold), or confirm proper configurations (aka compliance), within the varied environments of the scenario classes randomly injected with misconfigurations. 
Notably, GPT-based models dominate on both pass@1 and Time to Process metrics. The pass@1 is nearly 2x better than second-best models (alternating between llama-3.1-405b-instruct and mistral-large-2), while the TTP shows a handling of the scenarios in the minimal time across our scenario classes.

\textbf{FinOps.}
We measure the effectiveness of FinOps-agent on its ability to diagnose and mitigate the origin of cost alert (e.g., `increase in cost by 20\%'). 
Diagnosis effectiveness is measured using pass@1 (i.e., identifying the cause).
Mitigation effectiveness is measured in terms of proportional proximity to optimal cost of running, and efficiency that can be achieved for that workload.

GPT-4o consistently outperforms all other models, achieving a 33\% pass rate for diagnosing the origin of the cost increase alert. 
Performance on additional metrics related to cost and workload efficiency remains comparable across all models, with none attaining optimal CPU and memory cost or delivering high CPU efficiency. 



\subsection{Impact of Scenario Complexity}
\textbf{SRE.}
    We categorize scenarios as Easy, Medium, or Hard based on factors such as fault propagation chain length, number of resolution steps, and the diversity of technologies involved, as described in \Cref{ss:bench-sre-eq-task-complexity}. 
    Our results show that success rates (pass@1) clearly decline as the \textit{scenario\_complexity} increases.
    For example, GPT-4o (the best performing model) diagnosed only 36\%, 7.73\% and 5.0\% of the Easy, Medium, and Hard scenarios, respectively (refer to \Cref{tab:sre:diag_pass1}).
    Similarly, GPT-4o (the best performing model) successfully mitigated only 21\%, 12.27\% and 0.0\% of Easy, Medium, and Hard scenarios, respectively 
    (refer to \Cref{tab:sre:repair_pass1}). 
    
    None of the models could mitigate the hard scenarios in any of the runs, whereas over half of the Easy scenarios see successful mitigation. 
    Notably, GPT-4o is the only model that successfully diagnosed multiple ``Hard'' scenarios. 
      

\textbf{CISO.}
The complexity of the CISO scenarios is directly mapped to scenario classes. For example, \textit{scenario\_complexity} of Kyverno scenarios is Easy, \textit{scenario\_complexity} of k8s-opa and rhel-opa is Medium, while \textit{scenario\_complexity} of Kyverno-update scenarios is Hard. 
All models struggle, as expected, as the difficulty of the scenarios increases from the Easy \textit{kyverno} class to the Hard \textit{kyverno-upadate} class. 

\textbf{FinOps.}
Currently, \bench only has two FinOps scenarios, \textit{scenario\_complexity} of one is Easy and the other is Hard. None of the models, could diagnose (except for GPT-4o) or mitigate the hard scenario. 

This spectrum of complexity in \bench ensures that evaluations capture both straightforward and highly intricate problems across personas.

\subsection{Inherent Non-determinism in the Environment} 
GPT-4o remains the top performer across all evaluated personas (SRE, CISO, and FinOps), yet it still exhibits notable variability in scenario outcomes. 
For example, the SRE-agent with GPT-4o struggles to maintain deterministic behavior despite hyperparameter tuning aimed at ensuring consistency. 
SRE-agent with GPT-4o diagnosed the problem only in 6 out of 10 runs for scenario 13, 1 out of 10 runs for scenario 8, and 8 out of 10 runs for scenario 21, respectively (refer to \Cref{fig:sre:trace_on_diagnosis_pass1} for details on all scenarios).
Similarly, it mitigated 6 out of 10 runs for scenario 16, 2 out of 10 runs for scenario 8, and 5 out of 10 runs for scenario 21, respectively (refer to \Cref{fig:sre:trace_on_repair_pass1} for details on all scenarios).
This inherent non-determinism was observed with FinOps and CISO scenarios as well. 

These fluctuations arise from minor real-time telemetry changes, which can alter the large language modelâ€™s token generation. By tracking such dynamic behavior over multiple runs, \bench provides crucial insights into each agentâ€™s robustness and reliability.




