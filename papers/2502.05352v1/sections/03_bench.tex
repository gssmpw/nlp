\begin{figure*}
    \centering    
    \includegraphics[width=0.77\linewidth]{figures/main/bench-2.pdf}
    \vspace{-8pt}
    \caption{\bench automation framework.}
\label{fig:bench_design1}
\vspace{-10pt}
\end{figure*}


\section{\bench}
\label{sec:bench}


ITBench is a systematic benchmarking framework and runtime environment designed to evaluate AI agents tasked with automating IT operations, incorporating a robust architecture (see \Cref{fig:bench_design1}) comprising the AI Agent, Scenario Specification and Environment, Evaluator, and Leaderboard to facilitate comprehensive performance assessment.

Here, we present a brief overview of the key components: 1) Scenario Specification and Environment, 2) AI Agents, and 3) Leaderboard. More details are in Appendix \ref{appx:framework}.


\subsection{Scenario Specification and Environment}
\label{sec:scenarioenv}

The bench incorporates a collection of problems that we call \textit{scenarios}. For example, one of the problems in \bench is to resolve a ``High error rate on service order-management'' in a Kubernetes environment. Another example that is relevant for CISO persona involves assessing the compliance posture for a ``new control rule detected for RHEL 9.'' A fundamental challenge is to emulate such problems in a manageable testbed environment. A scenario environment is an operational testbed in which a specific problem(s) occurs. 

A scenario $p$ generally corresponds to a problem to be solved in \bench. We formalize $p$ as a tuple $<M, E, T, D>$, where the variables are as follows:

\textbf{Scenario Specification.} $M$ represents metadata and deployment descriptors, for each scenario, which is stored in the Scenario Specs database in \bench (see \Cref{fig:bench_design1}). Exemplar metadata elements per scenario include \textit{scenario\_name}, \textit{scenario\_description}, \textit{scenario\_domain}, \textit{scenario\_class}, \textit{scenario\_complexity}, and \textit{scenario\_groundtruth} (see \Cref{tab:bench_scenarios}), which are defined below:
\begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
    \item \textit{scenario\_name} is name given to a scenario. For example, a scenario in \bench has name "Recommendation Service Cache."
    \item \textit{scenario\_description} describes the scenario. Example description of the scenario is "Recommendation Service in Astronomy Shop has a cache failure."
    \item \textit{scenario\_domain} represents different personas namely "SRE", "CISO", "FinOps" within IT automation. 
    \item \textit{scenario\_class} is used to group similar scenarios, such as "Kyverno-opa", "Kyverno-update", "CacheFailure", "HighCPU", and "CorruptImage".
    \item \textit{scenario\_complexity} captures the difficulty of a problem and is defined using domain knowledge. \Cref{fig:role-task-breakdown}, shows the breakdown of SRE, CISO, and FinOps scenarios in the bench. \Cref{fig:sre-task-difficulty}, \ref{fig:ciso-task-difficulty}, and \ref{fig:finops-task-difficulty} shows \textit{scenario\_complexity} distribution for SRE, CISO, and FinOps, respectively. SRE scenarios are developed based on real-world incidents observed in our own SaaS products. CISO scenarios are based on CIS benchmark \cite{CISBenchmarks}. FinOps scenarios are sparsely represented in \bench due to the lack of standard benchmarks. We based our scenario using ``Domains'' and ``Capabilities'' identified by the FinOps Foundation \cite{finopsbench} to describe key business outcomes. 
    \item \textit{scenario\_groundtruth} records task-specific outcomes that the Evaluator uses to compare against the agent's expected output. For instance, in incident resolution for SREs, the ground truth for the Diagnosis task includes a list of entities involved in the fault propagation chain, the actual fault propagation chain(s), and fault conditions, while for the Mitigation task, it captures plausible mitigation actions.
\end{itemize}


\textbf{Environment.} $E$ represents an an operational testbed where the problem occurs. Components within the environment expose APIs to observe and control the environment. When the Agent Builder registers the agent for benchmarking, the Benchmark Runner (see \Cref{fig:bench_design1}) randomly selects a set of scenarios, which may be optionally filtered based on the \textit{agent\_type} and \textit{agent\_level}. Next, the Benchmark Runner iterates through the set of scenarios and for each scenario it instantiates a testbed. An example of an environment includes Kubernetes cluster installed with OpenTelemetry Astronomy Shop Demo application \cite{otelastronomy}, observability stack including Grafana \cite{grafana}, Loki \cite{loki}, Jaeger \cite{jaeger}, and Prometheus \cite{prometheus}, along with mechanisms that induce problem(s) in the environment.

\textbf{Triggering Events.} $T$ is a set of triggering events that occur due to manifestation of a specific problem in the environment. Tools are configured to observe the environment and raise triggering events on problematic conditions. An example of a triggering event is "High Error Rate on adservice", which may be triggered in the environment due to cache failure problem.

\textbf{Desired Outcome.} $D$ defines the automation objective and represents the ultimate goal. For instanace, in case of SRE incident resolution, the ultimate goal is to clear $T$ in the $E$.














\begin{table*}[h]
    \centering
    \footnotesize
    \begin{threeparttable}
        \caption{Exemplar scenario classes and their complexity per scenario domain in \bench across 94 scenarios.}
        \label{tab:bench_scenarios}
        \begin{tabular}{m{0.08\textwidth}m{0.47\textwidth}m{0.15\textwidth}m{0.2\textwidth}}
            \toprule
            \textbf{Scenario Domain} & \textbf{Scenario Class} & \textbf{Scenario \mbox{Complexity}} & \textbf{Technologies} \\
            \midrule
             SRE & CacheFailure - Create a memory leak due to an exponentially growing cache            & Medium              & K8s, Redis, MongoDB \\
            & HighCPU - Trigger high CPU load in target service          & Medium                 & K8, Host, Pods \\
            & CorruptImage - Deployment uses wrong Docker image & Easy              & K8s, Image registry\\
            & HTTPRequestBodyTamperFault - Modify HTTP Post request between services     & Medium                & K8s, ingress/egress  \\
            & HTTPRequestAbortFault - Interrupt HTTP connection between services     & Medium                & K8ss, ingress/egress  \\
            & MemoryResourceLimit - Reduce memory limit on target service    & Easy                & K8s, Host, Pod  \\\midrule
            CISO & New K8s CIS-benchmarks on Kyverno & Easy            & K8, Kyverno  \\
            & New K8s CIS-benchmarks on OPA           & Medium            & K8s, OPA, Kubectl  \\
            & New RHEL9 CIS-benchmarks on Ansible-OPA         & Medium            & RHEL9, OPA, Ansible \\
            & Update K8s CIS-benchmarks on Kyverno               & Hard              & K8s, Kyverno   \\ \midrule
            FinOps & CostAlertMisconfiguration - Alert threshold is too low causing false alerts         &          Easy     & K8s, HPA  \\
            & AutoscalerMisconfiguration - Horizontal pod autoscaler thresholds are misconfigured creating excess pods           &    Hard          & K8s, HPA \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
        \item[1] Scenario complexity  depends on the characteristics of the scenario, and is independent from agent capability. See appendices for details.
        \item[2] K8s refers to Kubernetes~\cite{kubernetes}.
        \item[3] Here, `technologies' refers to the set of tools and systems that a domain expert must understand to handle the task.
        \end{tablenotes}
    \end{threeparttable}
\end{table*}

\subsection{AI Agents}
\label{sec:baseline-agent}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/main/agent_env.pdf}
    \caption{Agent and environment as a POMDP. Agents interact with the environment via the APIs exposed by \bench's toolbox.}
    \vspace{-10pt}
    \label{fig:bench-agent-POMDP}
\end{figure}












\begin{table}[h]
    \centering
    \small
    \begin{threeparttable}
        \caption{Personas, tasks, and metrics in \bench.}
        \label{tab:bench_tasks_metrics}
        \begin{tabular}{llp{2.7cm}}
            \toprule
            \textbf{Personas} &  
            \textbf{Tasks} &
            \textbf{Metrics} \\
            \midrule
             SRE & Diagnosis & pass@1, Fault Localization, Fault Propagation Chain, Mean Time to Diagnosis\\
              &    Mitigation & pass@1, Mean Time to Repair  \\
            \midrule
              CISO & Collect evidence & pass@1   \\
                   & Scan assessment posture & pass@1, Time to Process  \\
            \midrule
             FinOps & Identify inefficiency & pass@1 \\
             &    Mitigate inefficiency & pass@1, Hourly infra cost, Efficiency \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table}








\begin{figure*}[t]
    \begin{subfigure}{0.23\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/main/task_breakdown.pdf}
        \caption{Scenarios by persona.}
        \label{fig:role-task-breakdown}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\linewidth}
        \centering
        \includegraphics[width=\linewidth]{appx/usecases/SRE/figures/bench_sre_task_complexity.pdf}
        \caption{SRE scenario complexity.}
        \label{fig:sre-task-difficulty}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\linewidth}
        \centering
        \includegraphics[width=\linewidth]{appx/usecases/compliance/figures/caa_scenarios_diff_level.pdf}
        \caption{CISO scenario complexity.}
        \label{fig:ciso-task-difficulty}
    \end{subfigure}
      \hfill
    \begin{subfigure}{0.23\linewidth}
        \centering
        \includegraphics[width=0.78\linewidth]{appx/usecases/finops/figures/bench_finops_task_complexity.pdf}
        \caption{FinOps scenario complexity.}
        \label{fig:finops-task-difficulty}
    \end{subfigure}
    \caption{Characterization of \bench scenarios.}
    \label{fig:task-analysis}
\end{figure*}











