\section{Introduction}
\label{sec:intro}
Modern IT systems are driving many facets of our economy. They have grown significantly in complexity with the adoption of cloud computing and agile development practices \cite{ITComplexity2022hbr, finops2024}. Effective management of these systems is becoming extremely challenging as corporations struggle to keep up with this growing complexity. Various IT personas ranging from Chief Information Officers to Site Reliability Engineers, Security and Compliance officers and IT engineers in general are struggling to ensure resiliency, reliability, security and cost effective operations of IT Systems.


The recent CrowdStrike outage highlighted these challenges as it brought down our society's most critical systems from hospital services to air travel and was estimated to cost US Fortune 500 companies a staggering \$5.4 billion \cite{csincident2024}. This incident underlined the critical need for intelligent IT incident resolution, compliance and risk management capabilities, a topic also addressed in the Digital Operational Resiliency Act (DORA) in Europe \cite{dora}. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/main/background-motivation.pdf}
    \caption{Sample personas and IT tasks. Bell icon represents event-triggered tasks. Information icon represents other tasks such as data analysis, preventive maintenance tasks, or continuous optimization.}
    \label{fig:motivation}
\end{figure*}

The rising popularity of AI agents and their projected ability to handle intricate tasks have increased the demand for AI agents managing IT systems \cite{WIRE19-2024, IDC2024, pujar2023automatedcodegenerationinformation}. Given the complexity of IT tasks, a major hurdle for this research is establishing systematic methods to assess the effectiveness of AI agents prior to their production deployment \cite{Bogin2024SUPEREA, kapoor2024aiagentsmatter}. Consequently, there is an urgency to develop methods for evaluation of AI agents based on real IT tasks and their corresponding environments. %


This paper addresses this critical need and presents \bench, a first of its kind framework that is both comprehensive and visionary for benchmarking real-life IT automation tasks. The goal of \bench is to measure the performance of AI agents across a wide variety of complex and real-life IT tasks across personas including, \textit{Site Reliability Engineering (SRE)} focusing on availability and resiliency, \textit{Compliance and Security Operations (CISO)} ensuring compliance and security of IT implementations, and \textit{Financial Operations (FinOps)} enforcing cost efficiencies and optimizing return on investment, among others (as shown in \cref{fig:motivation}).

\bench aims to advance innovation and establish new standards in the field. Our contributions can be summarized along the following three axes:

\begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
\item \textbf{Reflecting the real world:} \bench  addresses the IT automation requirements that are relevant and prevalent in production settings. SRE scenarios are based on real-world incidents observed in our own SaaS products, CISO’s are on CIS benchmark (for Internet Security, CIS). FinOps scenarios are identified by the FinOps Foundation \cite{finopsbench} through key business outcomes.

\item \textbf{Being open and extensible with comprehensive IT coverage:} 
We view \bench as a central hub for benchmarking AI-driven solutions across diverse IT automation use cases. To support this, we provide IT benchmark suites and a framework for vertical (i.e., adding more scenarios) and horizontal expansion (i.e., adding more personas), ensuring extensive coverage of IT tasks. 
\bench is an open-source framework built with open-source technologies, while allowing organizations with proprietary technologies to use it for developing and benchmarking their solutions.  


\item \textbf{Enabling automated evaluation with partial scoring:} 
\bench is designed to provide constructive feedback to drive improvements in the design of agentic solutions for IT problems. It includes a comprehensive evaluation framework and leaderboard that provide feedback to users at various stages of their agents' reasoning process. %

\end{itemize}

\bench provides push-button 
deployment and tooling for setting up environment, runtime agent, guardrail engine, as well as authorization and authentication. It allows developers and researchers to build novel solutions for managing complex IT systems. 
Currently, \bench addresses reactive problems including incidents diagnosis and resolution, 
compliance assessments in regulated environments for new controls, and cost management events. 
In future, we plan to expand on benchmark evaluation capabilities and include new benchmarks for additional IT processes. 
Currently, \bench comprises of an initial set of 94 scenarios spanning across SRE (42), CISO (50), and FinOps (2), with respective successful scenario handling rate of 13.8\%, 25.2\%, and 0\% (refer to \Cref{sec:results}).

We believe that, similar to the highly influential SWEBench \cite{jimenez2024swebench}, our new \bench framework—which encapsulates and measures the ability of AI agents to automate complex, real-world IT tasks—will spur a comparable acceleration in the performance of real-world IT AI agents.


