\section{Statements}
\subsection{Ethics \& Broader Impacts}
\label{appx:statement:impacts}
This research presents a novel benchmarking framework to measure the performance of AI agents across a wide variety of complex and real-life IT tasks, which has the potential to be a key enabler for AI-driven IT automation that is correct, safe and fast. While the primary focus is on advancing the field of machine learning, as this effort is an open-source framework built with open-source technologies, it allows organizations with proprietary technologies to use it
for developing and benchmarking their solutions more effectively. 
It also encourages mindsharing in the community and lowers the barrier to innovate in IT domain.

Agents that interact with the system pose several risks.
We identify three main risks that could arise when building and using a \bench and associated agents, then discuss how we incorporates measures that mitigate such problems.

First is the security risks that come with executing LM-generated code/commands on the system. Examples include executing commands like \texttt{kubectl delete node} and  \texttt{rm -rf asset/}.
To defend against this, we containerize the agents, and also provide a self-contained Kubernetes environment to create various scenarios.  

Second, if the wider community develops interest for \bench and associated agents and builds upon it, it is also possible that illegitimate evaluation datasets or infrastructure can be used to inject testing devices with malicious code or instructions to generate malicious code.
For instance, an unofficial repository claiming to host an inference/evaluation harness for \bench and associated agents could include a task instance with an issue description that tells the LM agent to build key logging functionality and store it in a hidden folder.
To eliminate confusion and reduce the possibility of such an event, we provide clear guidelines listed on our GitHub repositories, data stores, and websites indicating the official repositories and channels that we actively maintain.
We also encourage third parties to incorporate any improvements into our codebase and help with integrating such contributions.

Lastly are the consequences of \bench agents being deployed in the real world.
Prior works have conceptualized and put forth prototypes of agents that can carry out offensive security measures.
It is also not difficult to imagine that a system like \lumyn can be incorporated into pipelines resulting in the production of malicious code and libraries.
The strong performance of agents on \bench implies that future AI systems will likely be increasingly adept in the aforementioned use cases.
Releasing \bench agents as open source agents can support research towards designing sound, effective constraints for what software engineering agents are permitted to do.
It can also serve as a system that legal experts and policy-making entities can experiment with to shape the future of what AI-driven end to end software engineering could look like.

\subsection{Reproducibility}
\label{appx:statement:reproducibility}
To help the greater community reproduce the results presented in this paper and build on the \bench, we open source all of our resources that were created for this project.
The source code for the interactive pipeline, context management logic, command implementations, interface design, and everything else is entirely available in a GitHub repository.
We provide extensive text and video documentation describing how to run and modify different parts of the codebase.
Practitioners should be able to easily recover our findings by running the agent with simple scripts.
The results presented in the main and supplementary parts of this paper can be fully obtained by following instructions in the repositories.
Finally, we also maintain an active online help forum to assist with any reproduction problems or questions about how to build on \bench.

