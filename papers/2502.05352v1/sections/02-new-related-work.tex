\section{Related Work}
\label{sec:background-rl}


\begin{table*}
    \footnotesize
    \centering
    \begin{threeparttable}
        \caption{Comparison of \bench\ with related benchmarks }
        \label{tab:aiops_benchmarks}
        \begin{tabular}{%
            l                 %
            c                 %
            m{0.21\textwidth} %
            c                 %
            m{0.11\textwidth} %
            l                 %
            l                 %
        }
            \toprule
            \textbf{Benchmark} & 
            \textbf{\#Scenarios} & 
            \textbf{Personas and Tasks} & 
            \textbf{Resolvable} & 
            \textbf{Automated Evaluation} & 
            \textbf{Environment} & 
            \textbf{Leaderboard} \\
            \midrule

            \bench (ours)
            & 94 
            & \begin{tabular}[l]{@{}l@{}}
            SRE -- Incident Resolution, \\
              CISO -- Compliance Assessment, \\
              FinOps -- Cost Management \\
                \end{tabular}
            & \cmark   
            & \cmark
            & Real Env.
            & \cmark\, (verified)\\
            
            TrainTicket%
            & 22
            & SRE -- Incident Diagnosis
            & \xmark
            & \xmark
            & Real Env.
            & \xmark \\
            
            AIOpsLab%
            & 10 
            & SRE -- Incident Resolution
            & \cmark
            & \xmark
            & Real Env.
            & \cmark\,(unverified) \\
            
            InsightBench%
            & 100
            & Ticket Data Analysis
            & \xmark
            & \xmark
            & Synthetic
            & \xmark \\
            
            TSB-AD%
            & 40
            & Anomaly Detection
            & \xmark
            & \cmark
            & Synthetic
            & \xmark \\
            
            CIS%
            & 1000+
            & Compliance/Security Focal
            & \cmark
            & \xmark
            & n/a (info. only)
            & \xmark \\
            
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
        \item[1] \textbf{Note:} We are not aware of related benchmarks in the FinOps domain that go beyond scorecards.
        \end{tablenotes}
    \end{threeparttable}
\end{table*}

\bench targets a comprehensive set of tasks for a wide range of personas within IT automation. The initial release of \bench focuses on evaluating scenarios within IT Operations (ITOps). \cref{fig:motivation} illustrates currently targeted personas and exemplar tasks that they are routinely facing.
There is clearly a rising trend and interest in developing benchmarks to evaluate AI and ML techniques in ITOps with specific focus on SRE, CISO and FinOps.

TrainTicket~\cite{zhou2018fault} provides 22 scenarios
collected through an industrial survey of real-world incidents, using hardcoded faults in the TrainTicket application to focus on fault localization. AIOpsLab~\cite{aiopslab} provides 10 SRE-focused scenarios  
(referred to as problems) utilizing a real environment (system) integration that allows interactive access to text, time series, and tabular data. InsightBench \cite{sahu2024insightbench} provides 100 scenarios to analyze ticket data using static tabular data and synthetic scenarios. TSB-AD \cite{TSBench} focuses on anomaly detection with 40 synthetic scenarios. %

CIS-Benchmark~\cite{cis-b} provides best practices for securing IT infrastructure. Despite the name of ``benchmark'', it offers only 
recommendation policies; it provides no experimental platform. %
Recently, Cloud Native Compute Foundation (CNCF) Sandbox project
\cite{oscal-compass} released an SDK to support the translation of the CIS human readable formats into \cite{oscal} compliance as code standard of the National Institute of Standards and Technology for programmatic usage in compliance automation. \bench CISO automation leverages this technology to assess policy requirements. 

FinOps Foundation \cite{finopsbench}, provides benchmarks that compare cloud financial performance across organizations and departments, focusing on KPIs such as resource utilization efficiency, contract coverage, and cost apportionment. These benchmarks help assess cloud efficiency by evaluating internal and external metrics, fostering structured, collaborative approaches to cloud optimization. 

While existing benchmarks are valuable resources for specific tasks and use cases, and highlight the critical need for systematic benchmarking,  they are limited in reflecting real-world IT problems, covering broad IT landscape, and automating evaluation. These limitations are addressed in \bench, as shown in Table~\ref{tab:aiops_benchmarks}.











