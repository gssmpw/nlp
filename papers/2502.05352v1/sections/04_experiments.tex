
In IT automation, the different personas are focused on specific desired outcome, which defines their automation goals. 
For SREs, incident resolution is the primary objective. Achieving this can involve multiple steps, such as diagnosing an incident, or a single step, like generating a diagnosis report. CISO persona focuses on the regulatory controls posture assessment process, including \textit{Collect evidence} and \textit{Scan assessment posture} tasks. FinOps persona focuses on the cost management, where sample tasks include \textit{Identify inefficiency} and \textit{Mitigate inefficiency}. During evaluation, each step (task) is assessed independently and is measured using well defined metrics, see \Cref{tab:bench_tasks_metrics}.

The goal of \bench is to evaluate AI agents on a broad range of real-world IT automation tasks that are otherwise performed by SREs, FinOps, CISO personas.

In this paper, an AI \textit{agent} is defined as an autonomous or semi-autonomous software program that uses an LLM to plan, make decisions, interact with the target environment, and execute actions to achieve goals.  An AI agent is expected to successfully handle any of the scenarios in the \bench, by interacting with the environment. 

As shown in \Cref{fig:bench-agent-POMDP}, agent and environment form a Partially Observed Markov Decision Process (POMDP), where the state is the snapshot of the environment. The state transitions are determined by the environment, which are then (partially) observed by the agent. %

Given a scenario $p$ instantiated in an environment $E$, an agent probes the environment via one of the tools and receives an observation $o_t \in \mathcal{O}$, based on which, it decides the next action: 
\begin{equation}
    a_t  = f(o_t|\bar{o}_{t-1};\bar{a}_{t-1})
\end{equation}
Here $f$ is the agent's decision function. 
$\bar{o}_{t-1}$ is the sequence of observations up to time $t-1$ and $\bar{a}_{t-1}$ is the sequence of actions taken up to $t-1$.  

At the beginning, $o_0$ may be a triggering event showing a problematic state $s_0$ of the environment. Given state $s_{t-1}$ and action $a_{t-1}$, the environment transitions to the next state: 
\begin{equation}
    s_{t} = g(s_{t-1},a_{t-1})
\end{equation} 
The observation $o_t$ is determined as a function of the state and is in general a proxy for the environment state $s_t$, hence the formulation can be thought of as a POMDP: 
\begin{equation}
    o_t = h(s_t)
\end{equation}

The set $\mathcal{A}$ of actions is defined as $\mathcal{Q} \bigcup \{\bot\}$, where $\mathcal{Q}$ is the set of tools and $\bot$ represents the `stop action' by the agent. 
We define $t^*$ as the time when agent stops: 
\begin{equation}
    t^*=\min \{t | a_t=\bot \} 
\end{equation}

An agent reflects on the result to guide its next action, continuing until the final goal is achieved. Given a set of scenarios that the agent works on, it targets to maximize the success defined as follows: 
\begin{equation}
    \mathbb{E}_{p \sim \pi_p}(\mathbb{I}(g(s^{p}_{t^*}, f(o_{t^*}|\bar{o}_{t^*-1},\bar{a}_{t^*-1}))=s^p_G))
\end{equation}
where $\mathbb{I}$ is an indicator function comparing the terminating state with goal state, $\pi$ is the distribution of scenarios.  



\subsection{Baseline AI Agents}
We developed baseline agents: \lumyn for SRE, Compliance Assessment Agent for CISO, and FinOps-agent for FinOps.
Each of these agents uses state-of-the-art agentic techniques such as ReAct-based planning~\cite{yao2023react}, reflection~\cite{shinn2023reflexion}, and disaggregation~\cite{xu2023rewoodecouplingreasoningobservations}.
Reflection techniques vary from syntax checking/linting, semantic validation~\cite{xie2024travelplannerbenchmarkrealworldplanning}, and llm-as-a-judge~\cite{zheng2023judgingllmasajudgemtbenchchatbot}.

We open source two baseline agents (SRE\footnote{\url{https://github.com/IBM/itbench-sre-agent}} and CISO\footnote{\url{https://github.com/IBM/itbench-ciso-caa-agent}}) along with \bench. We use the open-source CrewAI framework \cite{crewai} to create and manage agents. The agents can be configured to use various LLMs either through watsonx, Azure, or vLLM. Each agent is initialized with a prompt that describes its goal, the context, the tasks, and the expected output format. In-context learning examples are included to guide the agent and demonstrate tool usage. Agents use natural language to access tools to interact with the environment for information gathering. 

Logs, traces, and metrics collected during the diagnosis process would overwhelm the context window of any LLM currently available due to large volume of data. Therefore, agent targeting the SRE or FinOps persona are equipped with specialized tools to interact with the environment (refer to \Cref{fig:bench-agent-POMDP}): 1) NL2Traces to extract trace data in a structured format, 2) NL2Metrics to analyze key system metrics, 3) NL2Logs to parse log data effectively, 4) NL2Kubectl to perform Kubernetes-specific operations, and a summarization tool to condense extensive data into actionable insights. For example, agent may use the NL2Kubectl tool to ``list all of the pods in the default namespace.'' In turn, NL2Kubectl tool uses an LLM to transform the utterance into an executable command, i.e.``kubectl get pods -n default''. 

Similarly, the compliance assessment required for new regulations and technologies, with the evidence and diverse policy languages would be overwhelming if submitted directly to LLMs. 
The compliance agents designed for CISO compliance assessment automation are equipped with specialized tools. 
These tools include capabilities to 1) generate policies such as Kyverno or OPA Rego Policy as Code starting from natural language specifications, 2) generate scripts for the collection of evidence, 3) access code repositories such as git to facilitate GitOps workflows for code management, and 4) deploy and execute the generated policies to accomplish the assessment task. %


 































    
    




\subsection{Leaderboard} 
\bench includes a leaderboard to promote reproducibility and comparative analysis, following the AI common task framework  \cite{Donoho2019,VarshneyKS2019}. %
The leaderboard offers a predefined, extensible set of performance metrics designed to provide clear insights into agent performance relative to the evaluation criteria.


\bench devises \textit{scoring methods for partially correct solutions} to provide meaningful feedback for summative assessments. %
This comprehensive approach establishes a new standard for evaluating and advancing AI-driven solutions in IT automation. For each scenario that an agent works on, upon task completion, the \bench
records the final system state, which is then used at the end of all scenario runs along with the pre-defined ground truth data to validate 
how well the agent performed across all the scenarios. For each scenario that an agent works on, upon task completion, the \bench
records the final system state, which is then used at the end of all scenario runs along with the pre-defined ground truth data to validate 
how well the agent performed across all the scenarios.

We are open-sourcing a small subset (11 out of 94) of scenarios \bench\footnote{\url{https://github.com/IBM/itbench-sample-scenarios}} along with the baseline agents to help the community familiarize with \bench through practical examples. We reserve the remaining scenarios in \bench to benchmark and evaluate the submitted agentic solutions.












