\section{\bench}
\label{appx:framework}

\bench framework, as shown in \Cref{fig:IT Agent Bench Leaderboard workflow}, supports two main phases corresponding to two personas as follows: (i) \textbf{benchmark registration} phase, where the target is the Benchmark Submitter persona, and (ii) \textbf{agent registration} phase, focusing on the Agent Submitter persona and the actual runtime benchmarking execution and evaluation.

\subsection{Benchmark Registration} 
This phase comprises two main steps: (i) scenario development and registration, and (ii) tasks and evaluation metrics registration.

\textbf{Scenario Development and Registration}

Our scenarios are designed to instantiate real-world IT problems in realistic and manageable environments. Each scenario comprises of two core components: (i) an environment specification, and (ii) a scenario specification metadata. 
The Benchmark Submitter persona then registers these scenarios with \bench, which stores them in its database. 
Each scenario is described using the metadata shown in \cref{ss:bench:tab:scenario_fields}.

\begin{table}[htbp]
\centering
\begin{threeparttable}
\centering
\caption{Scenario Metadata and Examples.}
\label{ss:bench:tab:scenario_fields}
\begin{tabular}{@{}lp{5cm}}
\toprule
\textbf{Field} & \textbf{Example} \\
\midrule
Type & CISO, SRE, FinOps \\
Name & For CISO: k8s CIS-b Minimize containers w/ shared net namespace \\
Description & For CISO: Minimize the admission of containers wishing to share the host network namespace \\
Complexity & Easy, Medium, Hard \\
Class &  
                  For CISO, this is defined based on the technology (e.g., k8s w/ Kyverno; k8s w/ OPA; Rhel9 w/ OPA). 
                  \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}


\textbf{Tasks and Evaluation Metrics Registration} For each scenario type, the Benchmark Submitter registers a well-defined set of tasks that form the basis for the Agent performance evaluation. \Cref{tab:bench_scenarios} summarizes the \bench currently supported IT automation tasks. Moving forward, we plan to extend \bench to incorporate additional tasks (e.g., threat analysis and resource optimization) and to broaden its applicability to other domains (e.g., DevOps).


\subsection{Agent Registration}
During this phase, the Agent Submitter first registers as a user on the platform, then follows with the Agent Registration. 

\subsubsection{Agent Registration}
During Agent Registration, the Agent Submitter specifies the agent metadata as shown in \Cref{ss:bench:tab:agent_fields}.

\begin{table}[htbp]
\begin{threeparttable}
\centering
\caption{Agent Metadata and Examples.}
\label{ss:bench:tab:agent_fields}
\begin{tabular}{@{}lp{4cm}}
\toprule
\textbf{Field} & \textbf{Example} \\
\midrule
Agent Name & -- \\
Agent Type (predefined) & CISO, SRE, FinOps \ldots \\
Agent Level & Beginner, Intermediate, Expert \\
            & (maps to scenario complexity: Easy, Medium, Hard) \\
Scenario Class    %
                  & For CISO: rhel9 w/ OPA; Kubernetes w/ Kyverno; Kubernetes w/ OPA, Kyverno update \\
\end{tabular}
\end{threeparttable}
\end{table}

Once the agent has been registered, the Agent Submitter selects the agent, and the corresponding benchmarks are retrieved from the database using the \textit{agent\_type, agent\_level}, and \textit{scenario\_class} specified during registration for the Agent. 
The Agent Submitter subsequently receives the tasks that the agent must complete to meet the designated objective, each of which has pre-defined evaluation metrics.

\subsection{Leaderboard}

Effective benchmarking of IT automation tasks, especially when selecting LLMs tailored to an organization's specific needs, requires consistent tracking and comparison of agent performance. The Leaderboard facilitates this need by offering a predefined, extensible set of performance metrics that provide clear insights into agent performance relative to the evaluation criteria. 

The Leaderboard supports both API and UI interfaces, enabling a streamlined benchmarking workflow. Users must register the agent endpoint via the Leaderboard’s UI or API. The agent can then query the Leaderboard to retrieve and deploy benchmark scenarios before reporting their operational status. The scenarios can be deployed either automatically by the \bench, as described above, in its hosted environment, or manually outside the Leaderboard, in the user's hosted environment, in which case both agent and environment can still leverage the same Leaderboard API endpoint to publish status updates. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/appendix/bench_details/bench-runner.drawio.png}
    \caption{\bench leaderboard workflow.}
    \label{fig:IT Agent Bench Leaderboard workflow}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{appx/ciso-leaderboard.png}
    \caption{Example \bench leaderboard.}
    \label{fig:ciso-leaderboard}
\end{figure*}

The end-to-end workflow for the agent benchmarking process, after its registration by the Agent Submitter, is illustrated in \Cref{fig:IT Agent Bench Leaderboard workflow}, and summarized in the following.

\begin{enumerate}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
    \item New benchmark jobs are stored in the Benchmark Queue for processing.
    \item The Benchmark Runner fetches a benchmark scenario for a particular agent from the Benchmark Queue.
    \item The Benchmark Runner provisions the environment as per the benchmark scenario specification.
    The scenario's environment is the set of systems required for the execution of a specific IT task.%
    The Agent interacts with (and can potentially modify) the environment  to solve the given IT automation tasks. A benchmark evaluation measures the Agent's performance based on whether it successfully completes the tasks in the given environment. 
    The environment could be, for instance, a Kubernetes cluster running a target application or a RHEL 9 host with a specific configuration to be validated. 
    The environment is under the direct control of the Agent and therefore may be subject to destructive actions (in case of faulty performance), thus functioning as a sort of ``playground.''
    \item For each scenario included in the benchmark run, the Benchmark Runner and the Agent execute the following steps:
        \begin{enumerate}
            \item The Agent continuously polls the \textbf{get\_manifest} API to monitor when a new manifest enters the \textbf{Ready} state.
            \item Benchmark Runner deploys the scenario's environment by executing the \textbf{deploy\_scenario} function. Each environment reports its status to the Agent API Server using the \textbf{post\_bstatus} API.
            \item The Benchmark Runner monitors the environment's status via the Agent API Server's \textbf{get\_bstatus} API. Once the status becomes \textbf{Deployed}, it injects a fault into the environment by executing the \textbf{inject\_fault} function.
            \item The Benchmark Runner continues to monitor the environment's status using the \textbf{get\_bstatus} API. Once the status reaches \textbf{FaultInjected}, it updates the manifest's status in the Benchmark DB to \textbf{Ready}, including key details such as Benchmark ID, Scenario ID, cluster credentials, and URLs in the manifest. This allows the Agent to access and retrieve this manifest for working with the environment.
            \item Once the manifest status is \textbf{Ready}, the Agent retrieves it. The manifest contains URLs and credentials required to launch the Agent. Before starting the Agent, the Agent calls the \textbf{post\_status} API of the Agent API Server to report its status as \textbf{STARTED}.
            \item After the Agent completes its execution, the \textbf{post\_status} API is called again to report the Agent’s completion its status as \textbf{FINISH}.
            \item Benchmark Runner starts the evaluation and executes the \textbf{delete\_scenario} function.
        \end{enumerate}
    \item Once the evaluation results for all the scenarios in the benchmark are ready, Benchmark Runner aggregates them and publishes the results to the Leaderboard.
\end{enumerate}




We instantiated the Leaderboard evaluation metrics for a few IT automation tasks as detailed in \Cref{sec:scenarioenv}, \Cref{tab:bench_scenarios}. 
In \Cref{fig:ciso-leaderboard} shows the Leaderboard landing page displaying the benchmarking metrics and results for the CISO compliance assessment agent.







