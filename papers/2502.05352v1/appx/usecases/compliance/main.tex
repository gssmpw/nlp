\section{Chief Information Security Officer (CISO) and Benchmarking the Compliance Assessment Agent}
\label{appx:ciso}
\subsection{Background}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{appx/usecases/compliance/figures/caa-background.pdf}
    \caption{Compliance Authoring and Administration vs. Policy Validation Point Engines}
    \label{fig:CAA-background}
\end{figure*}
Advances in technology are increasing application and infrastructure complexity. As a result, traditional approaches that depend on a dedicated security and compliance team to identify vulnerabilities in production systems and mitigate them based on threat that they pose to the organization, are no longer working. Modern organizations rely on a Development, Security, and Operations (DevSecOps) practice, a process in which application security is verified before deployment, where security and regulatory controls are put in place at software development time. Then, post deployment, runtime checks take over. With these multiple layers of security and compliance checks owned by different teams, some of them with limited cybersecurity knowledge, not only it is no longer feasible to have manual compliance processes for the technical security controls, but the automation of those processes also needs an unprecedented acceleration to keep up the go-to-market pace and scale.

The overall process starts with CISOs, security administrators, or regulators establishing and authoring the relevant body of compliance recommendations, typically in natural language, for specific mission critical environments. Then they rely on dev teams or security focals to collect status as evidence across those environments, and to validate it against the recommendations in view of obtaining the authorization to operate or other certifications. The sought after benefit from automating the \textbf{evidence collection} and its \textbf{validation} is to enable scalability, both in handling complex environments and supporting frequent scans -daily or on demand per system fix or update-  for posture measurement and reporting. Examples of validation automation tools are policy engines such as Kyverno \cite{Intro:Kyverno}, OPA Gatekeeper \cite{Intro:GateKeeper} for Kubernetes , Ansible \cite{Intro:Ansible} for PaaS, or Cloud Security Posture Management (CSPM) solutions for the cloud. 
\Cref{fig:CAA-background} illustrates the dichotomy of compliance authoring on the left versus compliance validation via policy scripts and their diverse programmatic languages on the right. 

Automating the translation of natural-language recommendations into policy scripts requires an unprecedented level of trust and synchronization across domains and experts typically in different business units. Additionally, it also demands an unprecedented level of technical knowledge for the compliance teams typically focused on legal and IP matters. 

The rising popularity of AI agents and their projected ability to handle intricate tasks have increased the demand for AI agents managing IT systems (John, 2024; Miguel Carreon, 2024). Given the complexity of the compliance tasks, a major hurdle for this research is establishing systematic methods to assess the effectiveness of our AI agents prior to their production deployment. Consequently, there is an urgency to develop
methods for evaluation of AI agents based on real IT tasks and their corresponding environments.

We detailed our CISO compliance assessment agent deployment and execution in its git repo documentation available in open-source \cite{CISO-CAAagentrepo}.
We present below our CISO agent benchmarking performance using a well defined benchmarking methodology with real-world scenarios and environments. A sample of those scenarios with their environment executable automation packages is also available in open-source \cite{CISO-CAAscenariorepo}.

\subsection{Real-World Benchmarking }

Our CISO compliance assessment agent (CAA) and corresponding bench bring together the latest technology on compliance as code to enable the programmatic expression of regulatory controls and their posture assessment, using Gen AI generation of code to fulfill these tasks. Our agent aim is to empower a compliance team in accelerating the adoption and operation of new regulatory programs by automating the generation of code for the evidence collection and for its posture validation against the requirements, based on compliance requirements described in natural language. Our benchmarking experiments cover the end-to-end agentic workflow from the discovery of the policy assessment engine in the benchmark scenario, the generation of assessment policy as code and its real-time git PR management, deployment,  execution, and posture generation. Finally, the results are evaluated, rated, and reported in our \bench solution leaderboard.

\subsubsection{Terms and Notations}
 We define the following key terms used hereafter to describe the main aspects of the agent framework and benchmarking methodology:

\begin{enumerate}
    \item \emph{Agent:} An agent is an AI driven software that autonomously acts on behalf of a persona to solve a given task. We group the agents by Agent Types that reflect the IT operations personas, for example CISO, FInOps, or SRE type.   
    
    \item \emph{Task:} A task is a specific job corresponding to the role of a persona that the agents aim to automate. Typical tasks for CISO are to collect evidence and assess compliance controls posture.

    \item \emph{Scenario:} A scenario is a real-life occurrence of a task in a given setting. For CISO, for instance, \textit{each} Kubernetes CIS-benchmark requirement instantiated on OPA is a unique scenario. The scenarios can be grouped in classes. 
 
    \item \emph{Scenario Class:} A scenario class is a class of real-life scenarios that are grouped together expecting the same behavior and outcome from the corresponding persona. Examples of scenarios classes are the \textit{set} Kubernetes CIS-benchmarks on OPA engine, the \textit{set} of RHEL9 CIS-benchmarks on Ansible engine, or a \textit{set} of Kubernetes CIS-benchmarks updates on Kyverno engine.

    \item \emph{Scenario Environment:} A scenario environment is the part of the scenario that specifies the deployment settings. 
    

    \item \emph{Environment State:} An environment has a countable set of states that we consider to mark a particular condition at a specific time. Example of states are an environment initial deployment state, an environment failure state after a fault or non-compliant configuration injection, or an environment remediated or compliant state after mitigation. 

    \item \emph{Goal:} A goal is the desired state for the environment known as the goal state.
\end{enumerate}
    
Agents are tasked to transition environments from their initial state to their goal state in the most efficient manner. At their disposal are environment actions, including requests for observations or actuation attempts to affect the state of the system. The agents first step towards moving the environment into the goal state is by reasoning over the outcomes of several observational actions to determine the next optimal step. With a strong hypothesis for what that is, agents seek to find and execute strategies to move towards the goal state in the most efficient way. 

Our CISO compliance assessment tasks are coupled with pre-defined scenarios for assessing the effectiveness of the agents delivering the automation in a standard manner. We detail in the next section these pre-defined CISO tasks before detailing our \bench and CISO scenarios. 

\subsubsection{CISO Compliance Assessment Tasks}

The compliance assessment tasks include various activities aimed at comparing the actual state of the systems with the desired state described in English in the policy. Based on this comparison, the system provides a "pass" or "fail" posture with respect to the policy.



\begin{itemize}
    \item \textbf{Identify Evidence Collector (IEC):} Acquiring evidence requires selecting collection mechanisms appropriate to the target system's characteristics. For instance, evidence about the state of an application in a Kubernetes cluster necessitates access to the Kubernetes API, often through tools like "kubectl" command. For host configuration evidence, tools like Ansible Playbooks are suitable. This IEC task and associated agent or tools identifies the collector used in the environment in view of generating the script for its corresponding language and interface.    
    
    \item \textbf{Identify Policy Assessment Tool (IPA):} Evaluating evidence against policies requires selecting a suitable policy engine. For general scenarios, the industry is using the open source policy engine Open Policy Agent (OPA)~\cite{Intro:OPA} with its specific programmatic language Rego. Alternatively, for Kubernetes-specific configurations, Kyverno policies prevalently used along OPA. This IPA task and associated agent or tools identifies the appropriate policy engine for the scenario's policy at hand, in view of generating policies code according to the policy engine's  programmatic language and interface.    
    
    \item \textbf{Collect Evidence (CE):} This CE task and associated agent or tools is responsible for the actual evidence collection, including the generation of code, management of code, deployment and execution of the evidence collection code to acquire the actual evidence from the environment. Proper placement and execution of the code are necessary to achieve this in a reliable and scalable manner.
    
    \item \textbf{Scan Assessment Posture (SAP):} This SAP task and associated agent or tools is responsible for generating the posture whether the evidence does or does not satisfy the scenario CIS-benchmark requirement. It includes the generation of validation code, management of code, deployment and execution of code on the policy engine to assess the evidence and produce the compliance posture.  
\end{itemize}

Table \ref{tab:bench_scenarios} summarizes the CISO tasks initially supported in our \bench, namely CE and SAP. The other will be covered in subsequent releases. 
These tasks are executed in \bench against predefined, standard scenarios and compared to the ground truth expected assessment posture "pass"/"fail" stored in the \bench under each scenario environment specification. 

\subsection{\bench Architecture for handling CISO Tasks}

\bench uses open source technologies to create repeatable and reproducible scenarios and environments for the CISO tasks, on a Kubernetes cluster as shown in \Cref{fig:bench_design_CISO}.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{appx/usecases/compliance/figures/it-bench-ciso-arch.png}
    \caption{Architecture of \bench responsible for orchestrating CISO scenarios.}
    \label{fig:bench_design_CISO}
\end{figure*}

\subsubsection{Principles}

Following the bench principles indicated in the introduction, our \bench uses open-source technologies to construct completely repeatable and reproducible  scenarios that simulate real-world incidents. 
\begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
    \item \textbf{Mimic CISO best practices.} \bench follows the guidelines outlined by the National Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF)~\cite{NIST-CSF} for CISOs and security teams to improve their organization's cybersecurity as follows: (1) \textit{Identify} critical data, systems, assets, and capabilities to protect; (2) \textit{Protect} via security measures that limit the impact of incidents; (3) Develop a strategy to \textit{detect} non-compliance with clear procedures and tools; (4) \textit{Respond} via plans to quickly eliminate threats and mitigate damage; (5) Design a \textit{recovery} policy to support timely recovery to normal operations. Our first \bench release covers the CISO's CSF first three activities by evaluating the following compliance tasks: (1) Identify and collect evidence from the systems in the selected scenarios; (2) Implement the policies recommended in the scenarios; (3) Assess the policies posture to detect failure into non-compliance. The remaining two CISO's CSF activities of \textit{respond} and \textit{recover} will make the topic of a future \bench release that will support agents collaboration, namely the leverage of the SRE agent for mitigation. 
   
    \item \textbf{Mimic CISO real-world problems.} 
    We studied and used in \bench the real-world Cloud Internet Security (CIS) Benchmarks~\cite{cis-b} which are a set of best practices for securing the IT cloud infrastructure. They are recognized worldwide as the cloud security standards.
    We used those CIS-benchmarks to create our CISO compliance policies scenarios of various complexity levels: 25\% Easy, 50\% Medium, and 25\% Hard policies (see Figure \ref{fig:ciso-task-difficulty2}). 

    \item \textbf{Provide observability.} 
    Cloud Native Compute Foundation (CNCF) recent Sandbox project (OSCAL-compass, 2024) released a compliance as code SDK to support the machine readable compliance as code standard (OSCAL, 2024) of the
National Institute of Standards and Technology for programmatic usage in compliance automation. \bench CISO automation leverages this methodology to represent the CIS-benchmarks requirements, detect the events of creation or update of requirements, and trigger the creation or update of evidence collection and validation code.

    \item \textbf{{Ensure} Determinism.} \bench enforces the scenarios and their environments are generated as per the specification, while the environment cleanup after each scenario ensures a clean slate for the next run. 
\end{itemize}

\subsubsection{\bench Architecture}

The environments for the benchmarking scenarios comprise a Kubernetes Cluster and virtual machines (VMs). Each CISO scenario pre-defined in the \bench as described in the section hearafter, is managed by a deployable stack, a software component responsible for handling the benchmarking process and environment in a real run-time environment. 

The deployable stack manages various tasks, including preparation ("deploy\_environment"), fault injection ("inject\_fault"), agent performance evaluation ("evaluate"), and environment cleanup ("delete\_environment") for each benchmark scenario run. Each deployable stack is specifically designed for a particular CISO compliance assessment scenario, ensuring the necessary tools, configurations, and policies are in place. Environment administrators define these scenarios deployable stacks and configure the required software, which can involve setting up policy engines, tools, or creating conditions that simulate violations of specific compliance requirements.

A deployable stack may for instance intentionally exhibit a misconfiguration settings to mimic a violation of a particular compliance standard.  In the execution of a scenario, the agent is presented initially with the natural language description of that scenario compliance requirement. Based on the description, the agent generates the necessary artifacts, including scripts for evidence collection and policies for evidence evaluation. The run-time scenario environment is deployed and made accessible to the agent. The agent accesses the environment to retrieve evidence, deploy policies, and work toward achieving the specified automation goal of assessing the compliance posture, in this case as "fail" or "not-satisfied".

During the scenario run process, the agent notifies the \bench of the start and completion of its tasks. Upon receiving the completion notification, the \bench accesses the environment to measure the benchmarking metrics. Once the metrics for all predefined scenarios are collected, they are aggregated and displayed on the \bench Leaderboard. Fig.~\ref{fig:CISO_CAA_ench} illustrates the end-to-end benchmarking process for the CISO scenarios.


 \begin{figure}[t!]
        \centering
        \includegraphics[width=0.45\linewidth]{appx/usecases/compliance/figures/caa_scenarios_diff_level.pdf}
        \caption{CISO scenario complexity.}
        \label{fig:ciso-task-difficulty2}
    \end{figure}
    
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{appx/usecases/compliance/figures/ciso_caa_bench.pdf}
    \caption{CISO Compliance Assessment Agent end-to-end Benchmarking Process.}
    \label{fig:CISO_CAA_ench}
\end{figure*}

In the context of our CISO compliance assessment benchmarking, the scenario environment, including Kubernetes Clusters and VMs, is prepared by the Agent Submitter. The setup and metric measurements are automated using a tool called Mini-Bench. Benchmarking results, including Task Metrics, are registered with a central Bench Server via API. These results are displayed on the Leaderboard on the Bench Server. This setup allows benchmarking to proceed uniformly, whether using the Agent Submitter's local environment or a remote environment, as the same API interactions are employed in both cases.

\subsection{\bench Real-World CISO Scenarios}
We used in \bench the real-world Cloud Internet Security (CIS) Benchmarks~\cite{cis-b} standard to create our CISO compliance assessment scenarios. The technologies that we have considered as playground for benchmarking our CISO agent are Kubernetes and RHEL9, however, any other technology available in CIS can be leveraged with their CIS-benchmark infusing the policies scenario in \bench.

\Cref{tab:Kube-CIS-b-bench} and \Cref{tab:CRHEL9-CIS-b} illustrate examples of the typical CIS-benchmarks recommendations. Each scenario rendered on \bench is designed to mirror the complexity of the recommendation. This ensures that \bench replicates real-world compliance requirements thus allowing for a realistic evaluation of agents.  


\begin{table*}[htbp]
\centering
\small
\begin{threeparttable}
\caption{Kubernetes - Center for Internet Security Benchmarks (sample)}
\label{tab:Kube-CIS-b-bench}
\centering
\begin{tabular}{@{}llp{3cm}p{1.3cm}p{7.3cm}@{}}
\toprule
\textbf{Section \#} &
\textbf{Recommendation \#} &
\textbf{Title} &
\textbf{Assessment Status} &
\textbf{Description} \\
\midrule
1.1.1 & 1.1.1.1 & Ensure cramfs kernel module is not available & Automated &
The `cramfs` filesystem type is a compressed read-only Linux filesystem embedded in small footprint systems. A `cramfs` image can be used without having to first decompress the image.\\

1.1.1 & 1.1.1.2 & Ensure freevxfs kernel module is not available & Automated &
The `freevxfs` filesystem type is a free version of the Veritas type filesystem. This is the primary filesystem type for HP-UX operating systems.\\

1.1.1 & 1.1.1.3 & Ensure hfs kernel module is not available & Automated &
The `hfs` filesystem type is a hierarchical filesystem that allows you to mount Mac OS filesystems.\\

1.1.1 & 1.1.1.4 & Ensure hfsplus kernel module is not available & Automated &
The `hfsplus` filesystem type is a hierarchical filesystem designed to replace `hfs` that allows you to mount Mac OS filesystems.\\

1.1.1 & 1.1.1.5 & Ensure jffs2 kernel module is not available & Automated &
The `jffs2` (journaling flash filesystem 2) filesystem type is a log-structured filesystem used in flash memory devices.\\

1.1.1 & 1.1.1.8 & Ensure usb-storage kernel module is not available & Automated &
USB storage provides a means to transfer and store files ensuring persistence and availability of the files independent of network connection status. Its popularity and utility has led to USB-based malware being a simple and common means for network infiltration and a first step to establishing a persistent threat within a networked environment.\\

1.1.1 & 1.1.1.9 & Ensure unused filesystems kernel modules are not available & Manual &
Filesystem kernel modules are pieces of code that can be dynamically loaded into the Linux kernel to extend its filesystem capability, or so-called base kernel, of an operating system. Filesystem kernel modules are typically used to add support for new hardware (as device drivers), or for adding system calls.\\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}



\begin{table*}[htbp]
\small
\begin{threeparttable}
\caption{Red Hat Enterprise Linux - Center for Internet Security Benchmarks (sample).}
\label{tab:CRHEL9-CIS-b}
\centering
\begin{tabular}{@{}llp{3cm}p{1.3cm}p{7.45cm}@{}}
\toprule
\textbf{Section \#} & 
\textbf{Recommendation \#} & 
\textbf{Title} & 
\textbf{Assessment Status} & 
\textbf{Description} \\
\midrule
1.1.2 & 1.1.2.3 & Ensure noexec option set on \texttt{/tmp} partition & Automated &
The `noexec` mount option specifies that the filesystem cannot contain executable binaries. \\

1.1.2 & 1.1.2.4 & Ensure nosuid option set on \texttt{/tmp} partition & Automated &
The `nosuid` mount option specifies that the filesystem cannot contain `setuid` files. \\

1.1.3 & -- & Configure \texttt{/var} & -- &
The `/var` directory is used by daemons and other system services to temporarily store dynamic data. Some directories created by these processes may be world-writable. \\

1.1.3 & 1.1.3.2 & Ensure nodev option set on \texttt{/var} partition & Automated &
The `nodev` mount option specifies that the filesystem cannot contain special devices. \\

1.1.3 & 1.1.3.3 & Ensure nosuid option set on \texttt{/var} partition & Automated &
The `nosuid` mount option specifies that the filesystem cannot contain `setuid` files. \\

1.1.4 & -- & Configure \texttt{/var/tmp} & -- &
The `/var/tmp` directory is a world-writable directory used for temporary storage by all users and some applications. Temporary files residing in `/var/tmp` are to be preserved between reboots. \\

1.1.4 & 1.1.4.2 & Ensure noexec option set on \texttt{/var/tmp} partition & Automated &
The `noexec` mount option specifies that the filesystem cannot contain executable binaries. \\

1.1.4 & 1.1.4.3 & Ensure nosuid option set on \texttt{/var/tmp} partition & Automated &
The `nosuid` mount option specifies that the filesystem cannot contain `setuid` files. \\

1.1.4 & 1.1.4.4 & Ensure nodev option set on \texttt{/var/tmp} partition & Automated &
The `nodev` mount option specifies that the filesystem cannot contain special devices. \\

1.1.5 & -- & Configure \texttt{/var/log} & -- &
The `/var/log` directory is used by system services to store log data. \\

1.1.5 & 1.1.5.2 & Ensure nodev option set on \texttt{/var/log} partition & Automated &
The `nodev` mount option specifies that the filesystem cannot contain special devices. \\

1.1.5 & 1.1.5.3 & Ensure noexec option set on \texttt{/var/log} partition & Automated &
The `noexec` mount option specifies that the filesystem cannot contain executable binaries. \\

1.1.5 & 1.1.5.4 & Ensure nosuid option set on \texttt{/var/log} partition & Automated &
The `nosuid` mount option specifies that the filesystem cannot contain `setuid` files. \\

1.1.6 & -- & Configure \texttt{/var/log/audit} & -- &
The auditing daemon, `auditd`, stores log data in the `/var/log/audit` directory. \\

1.1.6 & 1.1.6.2 & Ensure noexec option set on \texttt{/var/log/audit} partition & Automated &
The `noexec` mount option specifies that the filesystem cannot contain executable binaries. \\

1.1.6 & 1.1.6.3 & Ensure nodev option set on \texttt{/var/log/audit} partition & Automated &
The `nodev` mount option specifies that the filesystem cannot contain special devices. \\

1.1.6 & 1.1.6.4 & Ensure nosuid option set on \texttt{/var/log/audit} partition & Automated &
The `nosuid` mount option specifies that the filesystem cannot contain `setuid` files. \\

1.1.7 & -- & Configure \texttt{/home} & -- &
Please note that home directories could be mounted anywhere and are not necessarily restricted to `/home`, nor restricted to a single location, nor is the name restricted in any way. Checks can be made by looking in `/etc/passwd`, looking over the mounted file systems with `mount` or querying the relevant database with `getent`. \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}


\subsection{CISO Scenario Classes and their Complexity}
\subsubsection{New-K8s-CIS-b-Kyverno}

New-K8s-CIS-b-Kyverno represents the Easy scenario class in the \bench. The 10 scenarios in this class are prepared based on the CIS Benchmark for Kubernetes, specifically focusing on the Pod Security Policy. This scenario assumes a Kubernetes cluster with a pre-configured Kyverno policy engine. Within the cluster, certain misconfigurations related to Pod Security Policy are present, but the Agent is unaware of their exact locations.

The requirements for the misconfigurations that need to be addressed are communicated to the Agent. Based on these requirements, the Agent generates a Kyverno policy and deploys it to the cluster. Subsequently, the Agent collects the report from the cluster. The accuracy of this report is verified by checking whether it successfully identifies the misconfigurations. If the policy is correctly generated and deployed, the report should indicate the appropriate posture. Conversely, if errors occur, an incorrect posture will be reported.

In this scenario, the four Compliance Assessment Task are evaluated as follows:

\begin{itemize}
\item IEC: Assessed by verifying whether the correct configuration is reflected in the Kyverno policy.
\item IPA: Evaluated by confirming that the policy is successfully generated for Kyverno.
\item CE: Measured by verifying whether a Kyverno report is generated in the predefined location.
\item SAP: Determined by whether the posture reported in the Kyverno report matches the expected value.
\end{itemize}

\subsubsection{New-K8s-CIS-b-OPARego}

New-K8s-CIS-b-Kubectl-OPARego is categorized under Medium complexity scenarios. This benchmark comprises 10 scenarios derived from the CIS Benchmark for Kubernetes, specifically focusing on Pod Security Policies.

The foundational background for this scenario closely resembles that of New-K8s-CIS-b-Kyverno, sharing the assumption of a Kubernetes cluster as the operating environment and CIS Benchmark for Kubernetes. The key difference lies in the dual output for Open Policy Agent (OPA) policy engine: the generation of both an evidence fetcher script and a policy checker code. 

\begin{itemize}
\item A fetcher script is designed to gather the required evidence from the target cluster by executing kubectl commands.
\item A policy checker  verifies the collected evidence for compliance based on predefined rules. This is implemented using the Open Policy Agent (OPA) and defined through OPA Rego policies.
\end{itemize}

The goal of the agent in this scenario is to generate two outputs: 1) a script executing kubectl commands (fetcher), 2) an OPA Rego policy for compliance verification (checker).

The verification process evaluates the outputs generated by the agent as follows: The fetcher script, which consists of kubectl commands, is executed against a real Kubernetes cluster to collect evidence. The collected evidence is then assessed using the generated OPA Rego policy and the OPA policy engine to verify whether the results align with expected compliance outcomes.

The scenario is assessed on the following four Compliance Assessment Tasks:
\begin{itemize}
\item IEC: Determine whether a fetcher script, incorporating kubectl commands, is successfully generated.
\item IPA: Verify if the checker, implemented as an OPA Rego policy, is correctly generated.
\item CE: Check whether evidence can be successfully collected by executing the fetcher script against the cluster.
\item SAP: Verify that the OPA Rego policy evaluates the collected evidence as expected, producing the correct compliance assessment.
\end{itemize}

This approach provides a structured evaluation of the agent's capability to generate effective scripts and policies for Kubernetes cluster compliance assessment.

\subsubsection{New-RHEL9-CIS-b-Ansible-OPA}

New-RHEL9-CIS-b-Ansible-OPA belongs to the Medium complexity scenario class and comprises 20 scenarios based on the CIS benchmark for RHEL9 OS. This scenario shares common characteristics with New-K8s-CIS-b-Kubectl-OPARego, including the generation of two codes (a fetcher script and a checker policy), and the use of OPA Rego Policies for compliance verification. The primary distinction lies in the target system: unlike New-K8s-CIS-b-Kubectl-OPARego, which focuses on Kubernetes clusters, this scenario targets RHEL9 hosts. Consequently, instead of using kubectl as the fetcher script, New-RHEL9-CIS-b-Ansible-OPA employs Ansible playbooks. The objective of this scenario is to generate Ansible playbooks as fetcher scripts and OPA Rego Policies as checkers.

The verification process evaluates the outputs generated by the agent as follows: the fetcher script (Ansible playbook) is executed against a real RHEL9 host to collect evidence. The collected evidence is subsequently analyzed using the generated OPA Rego policy on the OPA policy engine, assessing whether the results align with the expected compliance outcomes.

In this scenario, the agent's performance is evaluated on the following four Compliance Assessment Task:

\begin{itemize}
\item IEC: Does the agent generate a fetcher script in the form of an Ansible playbook?
\item IPA: Does the agent generate an OPA Rego policy for the checker?
\item CE: Can the Ansible playbook successfully execute against an RHEL9 host and collect relevant evidence?
\item SAP: Can the generated OPA Rego policy evaluate the collected evidence and produce the expected compliance results?
\end{itemize}

This scenario is designed to assess the agent's performance in compliance evaluation tasks for host management environments other than Kubernetes, specifically focusing on RHEL9 systems.

\subsubsection{Update-K8s-CIS-b-Kyverno}

Update-K8s-CIS-b-Kyverno falls under the scenario class with a complexity level classified as Hard and currently includes 10 scenarios. Unlike New-K8s-CIS-b-Kyverno, which generates new Kyverno policies based on new requirements specified for that environment in the goal, this scenario involves a different objective. Specifically, it takes an existing Kyverno policy as input, along with instructions detailing modifications to the original requirements, and generates an \textit{updated} policy to meets the revised requirements. The updated policy is then deployed (or updated) as the final output.

The validation process for this scenario is consistent with the methodology used in New-K8s-CIS-b-Kyverno.

\subsection{CISO \bench Evaluation}
 
We conduct our experiments primarily on AWS EC2 instances (m4.xlarge), although \bench can also be readily deployed on a consumer-grade laptop using a pseudo-cluster, thus making it easier to develop AI agents.

We measure the efficacy of our CISO compliance assessment agent on a set of 50 scenarios across the four scenario classes introduced in \Cref{tab:bench_scenarios}. Each scenario class imposes a distinct set of CIS-benchmarks requirements (e.g., ``minimize the admission of containers wishing to share the host network namespace''), each class has a specific level of complexity (e.g., Easy, Medium, Hard), and generates scenario-specific code artifacts. 

In our evaluation we considered a variety of LLMs, such as GPT-4o, Llama-3.3-70B-instruct, Llama-3.1-8B-instruct, and Granite-3.1-8B-instruct for tasks that rely on natural language understanding and reasoning. For code-focused use cases, we additionally utilize GPT-4o-mini, Llama-3.1-405b-instruct, and Mixtral-8x7b-instruct. 
All models use a context window of 128K tokens, enabling them to process more extensive input sequences.


\subsubsection{Evaluation Metrics}

The efficacy of our CISO agents is measured based on the ability to detect artifact misconfigurations (aka non-compliance, e.g., no minimum count of containers sharing namespace, or the count is above the threshold), or confirm proper configurations (aka compliance), within the varied environments of the scenario classes randomly injected with misconfigurations. 

We evaluate how effectively the agent detects the (non)compliance using the following metrics:
\begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
    \item Success rate is quantified using pass@1.
    \item Efficiency is captured through Time to Process (TTP).
\end{itemize}

\subsubsection{Metric definitions}

\textbf{pass@1.}
We evaluate the agent proper assessment of the posture "pass"/"fail" using the pass@1 metric~\cite{chen2021evaluating}, which is defined as follows:
$$\text{pass@}k := \mathbb{E}_{\text{Scenarios}}\left[1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\right] $$
It is an unbiased estimator of correctness in \textit{k}=1 trials across all scenarios. 

\textbf{Time to Process.}
We identify the scenarios where an agent identifies the posture  successfully (i.e., misconfiguration results in a "fail" posture, while a compliant configuraiton results in a "pass" posture). For these task scenarios, we calculate \textit{TTP}, which measures how soon an agent performed the assessment. Otherwise, \textit{TTP} is set to infinite. 

\subsubsection{Evaluation Results}

\textbf{Overall agent results.}

Overall our results in \Cref{tab:cisoagent-eval} and \Cref{fig:pass-percent-ciso} show the GPT-based models dominate on both pass@1 and Time to Process metrics. The pass@1 is nearly 2x better than second-best models (alternating between llama-3.1-405b-instruct and mistral-large-2), while the TTP shows a handling of the scenarios in the minimal time across our scenario classes.

\textbf{Impact of Scenario Complexity}

The complexity of the CISO scenarios is directly mapped to scenario classes. For example, Kyverno scenarios are of Easy complexity, k8s-opa and rhel-opa are of Medium complexity, while Kyverno-update scenarios are of Hard complexity. 
As expected, all the models struggle, as expected, as the difficulty of the scenarios increases from the Easy \textit{kyverno} class to the Hard \textit{kyverno-upadate} class. 

\subsubsection{CISO Compliance Assessment Agent Trajectories}  

We examine two sample trajectories from our experiments to highlight the agent's thought patterns and actions. These include two scenarios, one successful and one unsuccessful, derived from the NEW-CIS-B-RHEL9-ANSIBLE-OPA experiments with LLaMa 405B, focusing on the CIS Benchmark for RHEL9 (control section 5.1.8 and 5.2.19).

In the successful scenario, the playbook is successfully generated, tested independently, leading to successful data collection, followed by the successful creation of an OPA, and ultimately detecting issues through evaluation in the bench.

In the unsuccessful scenario, the playbook is successfully generated and tested independently, leading to successful data collection, but the OPA cannot be properly created. This results in repeated syntax error corrections and retries, eventually reaching the maximum retry limit.

Following this, we present the trajectories for both cases. In the successful scenario the agent is tasked with performing a compliance assessment based on the following request:

\begin{mdframed}
Ensure cron is restricted to authorized users.
The system must:
  Allow only authorized users to schedule jobs using cron.
Specific requirements:

- The file /etc/cron.allow must exist and specify the list of authorized users.

- The file /etc/cron.deny must not exist.

- Unauthorized users (e.g., user1, user2) must not be listed in /etc/cron.allow.

To check the condition, do the following steps. 
- collect configuration from the RHEL host and save it locally. you can use ansible-playbook to do that. 
- chcek if the condition is met by using rego policy with the input given by the step above. 

for those steps, you need to create ansible playbook `playbook.yml` and OPA rego policy `policy.rego`.
If you can fix the generated code, do it and run the fixed code again.
Once you get a final answer, you can quit the work.
\end{mdframed}

The agent runs this task, provided with the target environment credentials and a working directory for saving the generated files to be submitted.
The Figure \ref{fig:rhelsuccess-1} shows the trajectory that the agent takes to achieve this request.  

\def\rhelsuccess{Success Scenario}
\def\rhelfailure{Failure Scenario}

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=0.8\linewidth]{appx/usecases/compliance/figures/ibm-ciso-success-traject.png}
    \caption{Sample Trajectory of LLaMa 405B in \rhelsuccess} 
    \label{fig:rhelsuccess-1}
\end{figure} 

\vspace{1em}

In the next scenario (CIS Benchmark control section 5.2.19), the agent is given a different compliance requirement.  
This time, the focus is on ensuring that SSH login sessions adhere to security best practices.
However, unlike the previous scenario, the agent encounters a critical issue: it fails to correctly write a syntactically valid Rego policy, causing repeated OPA errors during execution.  

The compliance requirement for Scenario 5.2.19 is as follows:

\begin{mdframed}
Ensure SSH LoginGraceTime is set to one minute or less 

To check the condition, do the following steps. 
- collect configuration from the RHEL host and save it locally. you can use ansible-playbook to do that. 
- chcek if the condition is met by using rego policy with the input given by the step above. 

for those steps, you need to create ansible playbook `playbook.yml` and OPA rego policy `policy.rego`.
If you can fix the generated code, do it and run the fixed code again.
Once you get a final answer, you can quit the work.

\end{mdframed}

The agent run this task, provided with the target environment credentials and a working directory for saving the generated files to be submitted.
The Figure \ref{fig:rhelfailure-1} show the trajectory that the agent takes to achieve this request.  

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=0.8\linewidth]{appx/usecases/compliance/figures/ibm-ciso-bad-traject.png}
    \caption{Sample Trajectory of LLaMa 405B in \rhelfailure} 
    \label{fig:rhelfailure-1}
\end{figure} 

\vspace{1em}

After repeating this process three times, the maximum retry limit was reached, and only the currently saved playbook.yml and policy.rego were used at the evaluation. At the evaluation, this policy.rego is syntactically wrong so the evaluation did not pass.


\begin{table*}[htp]
\small
\centering
\begin{threeparttable}
\caption{Experimental details}
\label{tab:exp-setup-ciso}
\begin{tabular}{@{}l c c c c c@{}}
\toprule
\multirow{2}{*}{\textbf{CISO/Agent: Models}} 
 & \multirow{2}{*}{\textbf{Scenarios}}
 & \multicolumn{4}{c}{\textbf{Experiment Setup}} \\
\cmidrule(lr){3-6}
 & 
 & \textbf{\#Repeats} 
 & \textbf{\#Total} 
 & \textbf{\%Exp. Completion}
 & \textbf{\%Agent Submission}\\
\midrule
\textbf{granite-3.1-8B-instruct}           & 50 & 8 & 400 & 91.41\% & 88.48\% \\
\textbf{mixtral-8x7B-instruct}           & 50 & 8 & 400 & 91.02\% & 94.67\% \\
\textbf{llama-3.1-8B-instruct}   & 50 & 8 & 400 & 93.21\% & 85.71\% \\
\textbf{llama-3.3-70B-instruct} & 50 & 8 & 400 & 92.82\% & 89.11\% \\
\textbf{mistral-large-2}           & 50 & 8 & 400 & 94.48\% & 85.23\% \\
\textbf{llama-3.1-405B-instruct}                & 50 & 8 & 400 & 92.43\% & 84.50\% \\
\textbf{gpt-4o-mini}                       & 50 & 8 & 400 & 95.25\% & 85.71\% \\
\textbf{gpt-4o}                       & 50 & 8 & 400 & 90.83\% & 90.54\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item \textit{Note:} “\%Exp. Completion” is the percentage of experiments that were started and finished by the bench runner correctly. \\
\item \textit{Note:} “Trials Agent Submitted” is the percentage of all trials completed in which the agent returned results.\\
\end{tablenotes}
\end{threeparttable}
\end{table*}



\begin{table*}[h]
\small
\centering
\begin{threeparttable}
  \caption{CISO/Agent: Assessed Results}
  \label{tab:agent101}
  \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{Models}}
      & \multicolumn{4}{c}{\textbf{Scenario pass@1 (\%) $\uparrow$}}
      & \multirow{2}{*}{\textbf{Avg. pass@1 (\%) $\uparrow$ }} 
      & \multirow{2}{*}{\textbf{MPR (s) $\downarrow$}} \\
    \cmidrule(lr){2-5}
    & \textbf{kyverno}
    & \textbf{k8s-opa}
    & \textbf{rhel-opa}
    & \textbf{kyverno-upadate} \\
    \midrule
    \textbf{granite-3.1-8B-instruct} &
    $7.84 \pm 3.84$ &
    $0.00 \pm 0.00$ &
    $0.00 \pm 0.00$ &
    $1.59 \pm 1.58$ &
    $1.71 \pm 0.76$ &
    $197.03 \pm 2.52$ \\
    \textbf{mixtral-8x7B-instruct} &
    $7.35 \pm 3.19$ &
    $1.43 \pm 1.42$ &
    $0.00 \pm 0.00$ &
    $1.29 \pm 4.34$ &
    $3.94 \pm 1.03$ &
    $120.63 \pm 3.77$ \\
    \textbf{llama-3.1-8B-instruct} &
    $8.57 \pm 3.37$ &
    $0.00 \pm 0.00$ &
    $0.00 \pm 0.00$ &
    $7.46 \pm 3.23$ &
    $3.59 \pm 1.07$ &
    $121.49 \pm 3.00$ \\
    \textbf{llama-3.3-70B-instruct} &
    $18.46 \pm 4.94$ &
    $0.00 \pm 0.00$ &
    $1.43 \pm 2.88$ &
    $8.06 \pm 3.50$ &
    $9.32 \pm 1.67$ &
    $189.61 \pm 2.71$ \\
    \textbf{mistral-large-2} &
    $6.56 \pm 3.20$ &
    $22.73 \pm 5.32$ &
    $7.23 \pm 2.88$ &
    $10.45 \pm 3.77$ &
    $11.55 \pm 1.95$ &
    $167.98 \pm 3.42$ \\
    \textbf{llama-3.1-405B-instruct} &
    $16.22 \pm 4.32$ &
    $20.83 \pm 4.86$ &
    $8.75 \pm 3.26$ &
    $3.17 \pm 2.22$ &
    $12.46 \pm 1.98$ &
    $178.89 \pm 3.37$ \\
    \textbf{gpt-4o-mini} &
    $16.18 \pm 4.54$ &
    $43.10 \pm 6.99$ &
    $30.38 \pm 5.43$ &
    $9.43 \pm 4.08$ &
    $25.19 \pm 2.80$ &
    $102.40 \pm 3.70$ \\
    \textbf{gpt-4o} &
    $40.28 \pm 5.99$ &
    $39.34 \pm 6.55$ &
    $7.61 \pm 2.81$ &
    $17.74 \pm 4.92$ &
    $24.74 \pm 2.64$ &
    $101.29 \pm 3.81$ \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \footnotesize
    \item ``pass@1'' values are in percent. pass@1 is calculated as defined in Codex~\cite{chen2021evaluating}\\
    \item ``MPR'' mean processing time\\
    \item \textbf{kyverno} = New K8s CIS-benchmarks on Kyverno, 
          \textbf{k8s-opa} = New K8s CIS-benchmarks on OPA, 
          \textbf{rhel-opa} = New RHEL9 CIS-benchmarks on Ansible-OPA,
          \textbf{kyverno-update} = Update K8s CIS-benchmarks on Kyverno.
  \end{tablenotes}
\end{threeparttable}
\end{table*}


\begin{figure*}
    \includegraphics[width=0.95\textwidth]{appx/usecases/compliance/figures/percent_pass_scenario.pdf}
    \caption{\label{fig:result:percent_repaired}Percent pass@1 for each scenario.}
    \label{fig:pass-percent-ciso}
\end{figure*}
