\section{Site Reliability Engineering}
\label{appx:sre}



\subsection{Background}
\label{ss:sre-background}
With the unprecedented growth in scale and complexity of modern IT systems and infrastructures, failures are the norm instead of exceptions~\cite{patterson:02,gunawi:16,kendrick:12,dimartinoDSN2014,veeraraghavan:osdi:18,Liu:2019,Ghosh:22}. First, {\it hardware failures} are frequent in large-scale IT infrastructures. For example, a new cluster at Google undergoes about a thousand individual machine failures and thousands of disk failures every year~\cite{Dean:2009}. Many of these failures further trigger correlated failures~\cite{ford:10}. New hardware fault models such as silent data corruptions in compute units~\cite{Hochschild:2021} and fail-slow storage~\cite{Gunawi:2018} further increase the challenges of detection and mitigation. In fact, in geo-distributed hyperscalar infrastructures, datacenter-level disasters are no longer rare events~\cite{veeraraghavan:osdi:18}.

Moreover, high velocity of software changes, {\it software failures} caused by code bugs~\cite{Gunawi:14} and misconfigurations~\cite{xu:13} have also become a major cause of IT system failures and service outages, significantly outnumbering hardware failures in recent years~\cite{maurer:15,Barroso:2018}.
For example, IT systems undertake hundreds to thousands of configuration changes daily, which introduces misconfigurations and triggers latent bugs~\cite{sun:osdi:20,tang:15}.
Recent trends in software architectures such as microservices and serverless computing~\cite{Jonas:19} are further enlarging IT reliability challenges by magnifying system complexity and dynamics with sophisticated interactions~\cite{tang:eurosys:23} and emergent behavior~\cite{Huang:2022}. 

The goal of Site Reliability Engineering (SRE) is to achieve high availability and serviceability of IT systems, in the presence of the aforementioned failures~\cite{srehandbook}. The essential job of SRE is failure management\footnote{\scriptsize We follow the classic Fault-Error-Failure model~\cite{aviz:04}, where a {\it fault} is a root cause such as a software bug, a hardware malfunction, or a misconfiguration. A fault can produce abnormal behaviors referred to as {\it errors}. However, some of these errors are transient and have no system-level effect. Only errors that propagate and become observable manifest as {\it failures}, such as crash, hang, incorrect result, or incomplete functionality, etc.}---detecting, diagnosing, and mitigating failures in production systems to prevent production {\it incidents} (the failures that cause user-perceived impacts) or to minimize the impacts and damages of incidents when incident {\it alerts} are triggered. Specifically:

\vspace{-10pt}
\begin{packed_itemize}
    \item {\bf Detection.} SRE must promptly detect production failures via logs, traces, and other telemetry data; detecting failures is the first step to prevent incidents or at least minimize their blast radius and impacts.  
    \item {\bf Diagnosis.} SRE must analyze the root causes of detected failures and localize the faults (e.g., the faulty component and the condition that triggers the fault). 
    \item {\bf Mitigation.} SRE must mitigate the failures to prevent propagation that leads to larger failures or incidents. Mitigation typically follows a resolution plan outlining a sequence of actions to restore the system to its expected state~\cite{chen:eurosys:24}.
\end{packed_itemize}

\bench currently focuses on diagnosis and mitigation tasks with plans to include more tasks 
    such as incident detection, prevention of similar failures/incidents (e.g., by regression testing).

Detection is simplified with golden-signal-based alerts, which observability tools provide natively. Though, the challenge intensifies during an event storm, requiring SREs to distinguish actionable alerts by suppressing false positives and prioritizing those that demand immediate attention — a daily struggle in incident resolution.
Both of these tasks are included in \bench by injecting multiple faults within certain scenarios, causing a flood of alerts. 
The agent must then determine which alerts to prioritize and in what order.

{\bf Urgent need of SRE automation.}
Currently, SRE is largely a human-based practice---SRE engineers are at the forefront of detecting, diagnosing, and mitigating failures and incidents daily~\cite{Beyer:2018,srehandbook}. 
However, IT systems are growing in scale and demand beyond what
human-based practice can reliably, continuously, and efficiently manage, and the cost of human resources and the limit of human reasoning has already become the bottleneck of failure and incident resolution. Today, SRE for IT systems has already become the major TCO (Total Cost of Ownership) of any cloud and software companies~\cite{Boulton2019, IDCStormClouds2024}.
\textit{Hence, SRE automation is no longer an optional enhancement, but an operational imperative.}

In fact, today's IT systems are already increasingly managed by operation programs that automate labor-intensive, human-based operations, known as {\it IT automation}. For example, 
modern cloud management platforms like Kubernetes~\cite{Burns:2016}, 
    Twine~\cite{Tang:2020}, and ECS~\cite{ecs} implement {\it operator} programs to automate a wide variety of operations such as software upgrades, configuration, autoscaling, etc.
However, so far, SRE has not yet become a common part of IT automation due to fundamental challenges of failure managements.



































\subsection{Real-world Incident Example}
\label{ss:sre-tasks}
\Cref{tab:incident_details} shows a real-world incident report based on SREs' raw work notes. 
In this incident, SREs were notified of several alerts of type~\textemdash~high error rate (>1\% in last 10 minutes) on a service \textemdash~by Slack. 
The \textit{fault} occurred due to a ``node failure'' due to the accidental deletion of resources during a decommissioning process aimed at cutting IT costs.
The fault caused shard unavailability, leading to an Elasticsearch failure and an SLO violation due to the error rate SLI.
The fault propagated to cause unavailability of shards which in turn led to elasticsearch failure. 
The unavailability of elasticsearch caused SLO violation of error rate SLI.

The \textit{Ops resolution plan} included trial and error to finally arrive at a state which allowed SRE personnel to execute existing mitigation playbook\footnote{Playbook is a structured set of predefined procedures or automated scripts that outline the steps required to perform specific operational tasks or respond to incidents. Playbooks standardize responses, reduce errors, and enable automation of repetitive tasks, enhancing efficiency and reliability in IT operations.}.

As shown, such incidents provide valuable information in terms of:
\begin{inparaenum}[(i)]
    \item time to detection, diagnosis (post detection) and mitigation (post diagnosis), 
    \item symptoms and customer impact, 
    \item faulty condition, fault propagation path and depth,
    \item operation resolution plan, and 
    \item long term fix and improvements. 
\end{inparaenum}
Such real world insights into fault occurrence, propagation, and resolution are invaluable for fault prevention, and automating incident handling. 

\begin{table*}

    \centering
    \begin{threeparttable}
        \caption{An incident that occurred on a SaaS data platform. This incident shows the complex relationship between SRE and FinOps persona, as FinOps ensures that IT environment is cost optimized to meet the financial efficiency goals, while SREs focus is on minimizing service impact and resolving the issue.}
        \label{tab:incident_details}
        \begin{tabular}{m{0.3\textwidth}m{0.65\textwidth}}
            \toprule
            \textbf{Incident} & \textbf{Details} \\
            \midrule
            Triggering alert & Seven alerts of type - ``High error rate on service."\\
            Summary & Error was encountered due to unexpected node failures and EBS volume issues during the downscaling of the Elasticsearch (ES) cluster because of a human error. Downscaling of ES was initiated to save AWS costs associated with running the service.\\
            Incident duration & 180 minutes \\
            Time to detection & 60 minutes \\
            Time to diagnosis & 60 minutes \\
            Time to mitigate & 120 minutes \\
            Symptoms &  [\cmark] Traffic: $\downarrow$, [\cmark] Error: $\uparrow$, [\xmark] Saturation, [\xmark] Latency \\
            Customer impact & Yes.\\ %
            Fault propagation depth & six \\
            Fault propagation & \begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
                \item [$\downarrow$] Human error: accidental removal of healthy nodes during decommissioning process (maintenance window)
                \item [$\downarrow$] Primary failed while replica initializing (human extrapolation based on the context and manual validation)
                \item [$\downarrow$] Shard assignments failed (ES event: shard unassigned)
                \item [$\downarrow$] Elasticsearch became unhealthy (ES event: RED status)
                \item [$\downarrow$] Services unable to get data from ES (trace)
                \item [$\downarrow$] Increase in error rate on 7 services (events)
            \end{itemize} \\
            Faults & human error, failure during recovery \\
            Resolution plan & \includegraphics[width=0.3\textwidth]{appx/usecases/SRE/figures/remediation-plan.pdf}\\
            Resol. plan size & 5 (4 human + 1 automation via playbooks) \\
            Long term improvements & \begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
                \item [$\checkmark$] Maintain 24-hour gap between instance deletion and EBS deletion
                \item [$\checkmark$] Runbooks updated accordingly
            \end{itemize} \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table*}



\begin{table*}[h]
    \centering
    \begin{threeparttable}
        \caption{SRE tasks}
        \label{tab:bench_sre_tasks}
        \begin{tabular}{p{0.39\textwidth}p{0.55\textwidth}}
            \toprule
            \textbf{Task} & \textbf{Task Description} \\
            \midrule
            \textbf{Fault localization} & Identify the faulty entity (root cause) and fault condition. \\
            \textbf{Fault propagation analysis \mbox{(aka root cause analysis)}} & Identifying the causal chain from the root cause entity to the alert, including the identification of fault condition at each step of the chain. \\
            \textbf{Recommend mitigation actions} & Identifying corrective actions to resolve the incident (excluding the execution). \\
            \textbf{Mitigate incident} & Executing corrective actions to clear the alert. \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table*}



\subsection{\bench Architecture}
\label{ss:sre-bench}
\bench uses open source technologies to create completely repeatable and reproducible incidents (scenarios) on a Kubernetes platform as shown in \Cref{fig:bench_design}. 
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{appx/usecases/SRE/figures/sre_bench-2.pdf}
    \caption{Architecture of \bench responsible for orchestrating SRE  scenarios.}
    \label{fig:bench_design}
\end{figure*}

\textbf{Orchestration.}
The core workflow involves a sequence of interactions between the \lumyn\footnote{Henceforth, we will refer to the agents handling SRE tasks as \lumyn} and various components of \bench. 
Initially, \lumyn (\circled{1}) enrolls in the benchmark leaderboard by sending the \texttt{enroll} command, which prompts the \bench to create a session (\circled{2}) and provide necessary credentials and details (e.g., Kubernetes access, time limits). 
Once ready, the agent sends the \texttt{ready} signal (\circled{3}), triggering the scenario executor to install a selected scenario from the scenarios database. 
This specification is used to set up the environment and  inject the fault, including installation of the observability tools (\circled{4}).
During the active phase (\circled{5}), the agent interacts with the environment using tools like \textit{NL2Alerts}, \textit{NL2Logs}, \textit{NL2Metrics}, and \textit{NL2Traces} to complete the task.
Upon task completion or time expiration (\circled{5}), \lumyn sends the \texttt{finish} command (\circled{6}), signaling \bench to evaluate the provided outputs and clean up the environment. 
The scenario executor validates the work of \lumyn (\circled{7}) restores the system to its baseline state (\circled{8}).
The interaction \circled{3} \textemdash \circled{8} continues until scenario manager sends session finish signal (\circled{9}). 

\subsubsection{Principles}
Following the bench principles indicated in the introduction, our \bench uses open-source technologies to construct completely repeatable and reproducible scenarios to simulate real-world incidents. 
\begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
    \item \textbf{Mimic SRE Best Practices.} \bench follows the guidelines outlined in SRE handbook~\cite{srehandbook} such as alerting on golden signals per application and enabling monitoring and observability. 
    Hence, in our current version, the detection is provided out-of-the-box using the approach outlined in ~\cite{srehandbook}.
    \item \textbf{Mimic Real-world Incidents.} 
We systematically examined 105 real-world incidents from our SaaS products to derive relevant incident patterns. 
Although we integrated several of these patterns into our \bench scenarios, not all were included due to the complexities of accurately reproducing these incidents and mirroring production-level characteristics. Nevertheless, our \bench will continuously evolve through the ongoing incorporation of additional incident patterns.
At the time of writing this paper, \bench supports
    24\% Easy, 24\% Medium, and 52\% Hard incidents, as shown in \Cref{fig:ss-bench-sre-task-complexity}. 

    
    
    \item \textbf{Provide Observability.} 
    In real-world scenarios, SREs use observability tools alongside command-line access to monitor systems. These tools provide multiple data modalities such as traces, logs, metrics, and events—and support alerting for efficient anomaly detection, trend analysis, and automated troubleshooting. 
    \bench defaults to Grafana~\cite{grafana} but can support other tools including IBM Instana~\cite{instana}, Dynatrace~\cite{dynatrace}, and Datadog~\cite{datadog}. 
    \item \textbf{Model Data Variability.} 
    Depending on system criticality and budget, some data modalities may be missing; for instance, only about 20\% of applications have tracing enabled, complicating incident diagnosis. 
    \bench allows flexible control to enable, disable, or partially enable data modalities as needed.
    \item \textbf{{Manage} Scalability} Scenario hyperparameters consists of (i) environment specification and (ii) scenario specification. 
    Environment specification allows (i) application selection and their related infrastructure selection (e.g., replica count), and data censoring parameters.
    Scenario specification allows selection of hyper parameters (e.g., service name on which to inject fault on). 
    \bench creates a database of scenarios offline using the aforementioned hyper parameters.
    \item \textbf{{Ensure} Determinism.} \bench ensures that alerts are generated according to the scenario specifications before making the scenarios available in \bench. Moreover, \bench ensures that all the assertions (e.g., application is running correctly, alerts are fired correctly) are passed before sending the `READY' state signal to the agent.  
\end{itemize}




\subsubsection{Recreating Incidents in \bench using Real-world Scenarios}
By leveraging detailed incident reports from real-world outages, such as the one summarized in \Cref{tab:incident_details}, we systematically reconstruct similar failure scenarios in \bench. 
As outlined in \Cref{tab:testbed_setup}, this involves configuring a multi-node Elasticsearch cluster with EBS volumes and introducing targeted disruptions—ranging from altering network configurations (e.g., changing ports or IPs) to simulating node and volume deletions, or disabling write operations on specific shards. 
Each recreated scenario is designed to mirror the complexity of the observed production failures with small variations, including similar failure propagation paths, impact on metrics (such as error rates and latency), and the associated operational mitigation steps. 
This ensures that \bench incidents (scenarios) in \bench accurately replicate real-world technical details while also capturing the associated decision-making challenges, allowing for a realistic and representative evaluation of agents.




\begin{table*}[t!]
    \centering
    \begin{threeparttable}
        \caption{Recreated failure scenarios using the incident description described in \cref{tab:incident_details}.}
        \label{tab:testbed_setup}
        \begin{tabularx}{\linewidth}{lXXX}
            \toprule
            \multicolumn{4}{c}{\textbf{Testbed Setup}} \\
            \midrule
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
                \item Develop an application that uses Elasticsearch for data storage and retrieval.
                \item A minimum of 3 nodes in the Elasticsearch cluster must be configured.
                \item Attach EBS volumes to each node to simulate the volume usage conditions as in the incident.
                \item Create an index with a sufficient number of documents to stress the system.
            \end{itemize}
            } \\
            \midrule
             & \textbf{Incident Scenario 1} & \textbf{Incident Scenario 2} & \textbf{Incident Scenario 3} \\
            \midrule
            Description &
            Make ES unavailable by changing port, IP address, etc. &
            (i) Identify a victim node: choose one of the nodes within the cluster and delete it, and (ii) delete the attached EBS volume. &
            Identify a victim shard and make it read-only (i.e., disable writes). \\
          
            Fault propagation &
            IP/Port changed $\rightarrow$ ES unavailable $\rightarrow$ Increased error rate in app &
            Similar to incident described in \cref{tab:incident_details} &
            Similar to incident described in \cref{tab:incident_details}, except caused by hardware failure \\
      
            Ops mitigation plan &
            Change the IP address/port to the correct value &
            Similar to incident described in \cref{tab:incident_details} &
            (i) Enable writes on the victim shard, or (ii) follow the procedure similar to incident described in \cref{tab:incident_details} \\
            \bottomrule
        \end{tabularx}
    \end{threeparttable}
\end{table*}

\subsection{Characterizing \bench incidents}
\label{ss:sre:scenario_complexity}

\Cref{tab:bench_sre_task_scenarios} summarizes the scenarios that are currently available in \bench. Beyond these 21 scenarios, \bench can easily produce a far larger range of fault patterns by parameterizing key dimensions such as the target application, the precise location of fault injection, and the number and types of concurrent faults. For instance, if the target application is HotelReservation, the fault of the PodFailure scenario alone can be applied to any of the 18 pods, effectively extending to another 18 scenarios. In this way, \bench can be used to systematically generate hundreds or even thousands of variations. In our evaluation, we focus on representative scenarios, while still enabling users to customize and scale their tests. 
\begin{table*}[htbp!]
\centering
    \centering
    \begin{threeparttable}
        \caption{Unique Scenarios available in \bench.}
        \label{tab:bench_sre_task_scenarios}
        \begin{tabular}{llcc}
            \toprule
             \textbf{Scenario Pattern} & \textbf{Technologies Impacted} & \textbf{\# Fault Propagation} & \textbf{\# Resolution Steps} \\
            \midrule
             CacheFailure               & nodejs                & 3 & 2 \\
             HighCPU &  Java, nodejs & 3 & 2 \\
             ServiceFailure & Java, nodejs & 4 & 3 \\
             ManualGarbageCollection & Java, nodejs & 3 & 2 \\
             MemoryLeak & python, Node.js, Go & 8 & 6 \\
             CorruptDeployment & Go, Java, nodejs & 8 & 6 \\
             CorruptDeployment & Java, Go, nodejs & 7 & 5 \\
             CorruptDeployment & Go, nodejs & 2 & 1 \\
             NetworkDelay &  Go, python, nodejs & 4 & 1 \\
             PodFault &  Go, nodejs & 2 & 2 \\
             NetworkPartition &  Tonic, Rust, Go, nodejs & 4 & 1 \\
             CorruptImage & Go, nodejs & 3 & 1 \\
             CorruptImage &  nodejs & 2 & 1 \\
             CPUStress &  python, nodejs & 2 & 2 \\
             HTTPRequestBodyTamperFault &  Ruby, Go & 3 & 1 \\
             HTTPRequestAbortFault &  PHP, Go, Tonic, Rust, nodejs & 4 & 1 \\
             HTTPRequestBodyTamperFault &  Ruby, Go, nodejs & 3 & 2 \\
             JVMCodeReturnFault &  Java, nodejs & 3 & 1 \\
             PodFailure &  Java, nodejs & 1 & 1 \\
             IncorrectAuthentication & .NET, Go, nodejs & 2 & 1 \\
             MemoryResourceLimit & Go,  nodejs & 1 & 2 \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table*}



\begin{figure*}[t!]
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{appx/usecases/SRE/figures/bench_sre_task_causal_path_length.pdf}
        \caption{Fault propagation chain length. }
        \label{fig:bench-sre-task-causal-path}
    \end{subfigure}\hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{appx/usecases/SRE/figures/bench_sre_task_resolution_steps.pdf}
        \caption{Mitigation plan size.}
        \label{fig:bench-sre-task-resolution-plan-size}
    \end{subfigure}\hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{appx/usecases/SRE/figures/bench_sre_tasks_num_technology.pdf}
        \caption{\# Technologies involved.}
        \label{fig:bench-sre-task-num-technologies}
    \end{subfigure}
    \caption{Characterizing \bench scenarios.}
    \label{fig:bench-sre-task-characterization}
\end{figure*}

\Cref{fig:bench-sre-task-characterization} illustrates key incident characteristics observed in our dataset, including the \textit{fault propagation chain length} (\Cref{fig:bench-sre-task-causal-path}), the \textit{resolution plan size} (\Cref{fig:bench-sre-task-resolution-plan-size}), and the \textit{number of distinct technologies} involved (\Cref{fig:bench-sre-task-num-technologies}). Intuitively, as the length of the fault propagation chain grows, the incident becomes more challenging to diagnose. Similarly, a longer resolution plan suggests that restoring service health requires multiple steps and interventions. The involvement of various technologies introduces additional complexity due to the diversity of tools, data sources, and failure modes.

Since \textit{fault propagation length}, \textit{resolution plan size}, and \textit{technology heterogeneity} all influence the difficulty of incident resolution, we define overall task complexity as their geometric mean. \Cref{ss:bench-sre-eq-task-complexity} captures this relationship:


\begin{equation}
\resizebox{\linewidth}{!}{
    $\text{Complexity} = {\sqrt[3]{(\text{propagation path length}\times\text{\# resolution steps}\times\text{\# technologies})}}
    $}
    \label{ss:bench-sre-eq-task-complexity}
\end{equation}



This formulation offers a balanced complexity measure, where the geometric mean ensures that all three factors contribute proportionally, rather than allowing one dominant factor to skew the assessment. 
While factors like required skill sets or the number and type of diagnostic interactions (e.g., tool invocations or queries) could further refine our complexity measure, these factors are often highly dependent on the observability platform, domain expertise, and team-specific processes. 
As discussed, LMs can potentially mitigate skill gaps through targeted fine-tuning and knowledge integration, thereby reducing the variability introduced by differences in human expertise and diagnostic strategies. 
Thus, we focus on the three core factors that are more consistent and inherent to the complexity of the incident itself.

\Cref{fig:ss-bench-sre-task-complexity} presents the distribution of task complexity values across our incident dataset using the above geometric mean formulation. 
The results show a diverse range of scenarios, with varying degrees of difficulty reflected in the natural interplay among propagation depth, resolution steps, and multi-technology integration. 
This complexity quantification provides a foundation for future analyses, including evaluating how automated reasoning tools, enriched observability stacks, or improved operator training might shift the distribution toward easier, more manageable tasks.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\linewidth]{appx/usecases/SRE/figures/bench_sre_task_complexity.pdf}
    \caption{SRE scenario complexity.} 
    \label{fig:ss-bench-sre-task-complexity}
\end{figure}

\subsubsection{Experimental Setup}
\label{appx:sre:exp_setup}
These tasks are implemented as Ansible playbooks to benefit from automation pipelines such as Ansible AWX. Below, we present one of our fault injection implementations, which utilizes Kubernetes network policies to simulate port blocking for a target service. We use roles to define different actions related to both fault injection and fault removal respectively. Our fault injections can be reconfigured using the variables to target different services to create additional scenarios. Each scenario has been validated to produce a relevant alert in Grafana, which provides important context to an agent working on a scenario.

\begin{lstlisting}[language=YAML]
---
- name: Define Network Policy to block port 8080
  set_fact:
    network_policy_spec: |
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: "deny-{{ target_service }}-{{ target_port }}" 
        namespace: "{{ target_namespace_project_name }}"
      spec:
        podSelector:
          matchLabels:
            app.kubernetes.io/name: "{{ target_service }}"
        policyTypes:
        - Ingress
        ingress:
        - ports:
          - protocol: TCP
            port: {{ target_port }}
          from: []
  when:
    - is_custom
    - is_fault_injection or is_fault_removal
    - is_network_policy_service_block

- name: Apply Network Policy
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    definition: "{{ network_policy_spec }}"
  register: network_policy_apply_result
  when:
    - is_custom
    - is_fault_injection
    - is_network_policy_service_block

- name: Remove Network Policy
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: absent
    api_version: v1
    kind: NetworkPolicy
    name: "deny-{{ target_service }}-{{ target_port }}" 
    namespace: "{{ target_namespace_project_name }}"
  register: network_policy_removal_result
  when:
    - is_custom
    - is_fault_removal
    - is_network_policy_service_block
\end{lstlisting}

For our experiments, we utilized an AWS m4 xlarge cluster configured with 1 control-plane node and 3 worker nodes. The worker nodes had 12 cores and 48 GiB of RAM, with 16 cores and 64 GiB of RAM being used in total. To gain insights into the resource demands imposed by our scenarios, we analyzed the cluster’s performance during a one-hour test period. The key metrics include Persistent Volume Claim (PVC) usage, CPU consumption, and memory utilization, as summarized in \cref{tab:resource-usage}.

\begin{table}[h]
    \centering
    \small
    \begin{threeparttable}
        \caption{Cluster resource usage during fault injection.}
        \begin{tabular}{lccc}
            \toprule
           \textbf{Resources} & \textbf{Usage}  & \textbf{Requests}  & \textbf{Limits} \\ 
 \midrule
CPU       & 2.06571 cores         & 8.19 cores        & 6.16 cores      \\ \hline
Memory    & 13.84 GiB             & 12.89 GiB         & 16.93 GiB       \\ \hline
PVC       & 62.21 GiB             & -                 & 160 GiB \\
\bottomrule 
\end{tabular}
\label{tab:resource-usage}
\end{threeparttable}
\end{table}



\bench also supports experiments on Kind clusters, offering a lightweight and portable option for local testing. We validated this capability on a machine with the following configuration: 1 control-plane node, Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 12 CPU cores, and 16 GB RAM, running Red Hat Enterprise Linux. This setup allows researchers to efficiently simulate fault scenarios, such as observability stack deployment, OpenTelemetry application deployment, and fault injection tasks, with minimal infrastructure overhead. For example, Incident 22 demonstrated an average CPU usage of 361.71\% and memory consumption of 93.53\%, confirming the feasibility of Kind clusters for reproducible testing.


\subsection{SRE-Agent}
\label{ss:sre-agent}
As described in Section~\ref{sec:baseline-agent}, agents interact with the target environment, collect observability data, and execute action to accomplish its goals. For SRE, the goal is to diagnose and mitigate incidents. Below, we describe the observability data collected by the SRE-Agent and our LM-based, multi-agent system implementation. 

\subsubsection{Observability Data}
\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\linewidth]{appx/usecases/SRE/figures/multi-modal.pdf}
    \caption{Multi-modality data for SRE task.} 
    \label{fig:ss-bench-sre-multi-modality}
\end{figure} 
As shown in ~\Cref{fig:ss-bench-sre-multi-modality}, SRE tasks involve analyzing multi-modal observability data: logs, traces, and metrics. 

\textbf{Logs.} Logs are semi-structured text records that capture hardware and software events. They are often categorized by severity levels, such as INFO, WARN, and ERROR, to reflect the system's runtime status and the seriousness of its behavior.

\textbf{Traces.} Request traces describe the execution flow of user requests as they traverse through various service instances in a distributed system. They provide a hierarchical representation of service invocations, where each operation is referred to as a span. A span records information about a single service invocation, such as its start time, duration, and associated metadata, including tags and logs. Spans are linked together to form a trace, capturing the complete execution path of the request. Additionally, program exception traces capture program crashes, providing valuable insights for developers during debugging.

\textbf{Metrics.} Metrics provide time-series data monitoring system performance and user-perceived indicators, such as latency, error rates, and resource utilization.



\begin{table*}[t!]
   
    \centering
    \begin{threeparttable}
        \caption{\label{tab:tools}List of the tools used by \lumyn}
        \begin{tabular}{m{0.2\textwidth}m{0.45\textwidth}m{0.10\textwidth}}
            \toprule
            \textbf{Name} & \textbf{Description} & \textbf{Supports Reflection}\\
            \midrule
            NL2Kubectl & Interacts directly with Kubernetes & yes \\
            NL2Traces & Interacts with Grafana API for traces & yes \\
            NL2Metrics & Interacts with Grafana API for fetching metrics stored in Prometheus & yes \\
            NL2Logs & Interacts with Grafana API for fetching logs stored in Loki & yes \\
            NL2Alerts & Interacts with Grafana API for fetching alerts & yes \\
            Mitigation & Generates mitigation plans & no \\
            Wait & Pauses execution for the specified seconds & no \\
            Summarization & Summarizes the input content & no \\
            DiagnosisJsonReport & Generates JSON Report of the diagnosis & no \\
            MitigationJsonReport & Generates JSON Report of the mitigation plan & no \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table*}


\subsubsection{\lumyn Architecture and Implementation}
The \lumyn architecture consists of two LM-based agents, a Diagnosis Agent and a Resolution Agent as shown in~\Cref{fig:agent_arch}. We first define the following basic components used in our implementation:
\begin{itemize}
    \item \textit{Agent.} An agent is an autonomous or semi-autonomous software program that uses a LM to plan, make decisions, interact with the target environment, and execute actions to accomplish goals.
    \item \textit{Task.} A task is a specific goal that the agent must accomplish before its execution terminates. In our implementation, a task is a complex multi-step process (e.g. diagnosing the cause of an incident). Tasks also have tools associated with them that the agent can use to achieve the goal.
    \item \textit{Tool.} A tool is a function or API call that the agent can use to perform a specific sub-task, such as, interact with the target environment to collect observability data. 
\end{itemize}

We now describe our implementation of each of the above components.


\textbf{Tools.} Table~\ref{tab:tools} lists all the tools available to \lumyn. All our tools are also LM-based, where the LM is prompted with an utterance from the agent instructing it to perform the required sub-task. The tools are of two types based on whether they generate natural language (e.g., Mitigation) or function calls (e.g., NL2Kubectl). Further, to potentially improve the accuracy and usability of our tools, we equip them with the following features.

\begin{itemize}
\item \textit{Reflection.} To enable automatic correction of wrong LM responses, they are provided with external feedback~\cite{pan2023automatically, huang2023large} from \textit{linters}. Specifically, for tools that generate function calls, linters are developed to validate the syntax and semantics of the output. If the linter finds a problem with the generated function call, the LM is re-prompted with the linter's feedback so that it can attempt to fix the problem. Similarly, if the generated function call passes linting, but causes an error upon execution, the error message and the failing function call are used to re-prompt the LM for a fixed function call. 

\item \textit{Summarizer.} For some tools, such as NL2Logs and NL2Traces, the output is not directly returned to the agent because it is very long and contains extraneous information. These tools utilize an additional step that prompts a LM with the output and asks it to provide a detailed summary with only relevant information.
\end{itemize}

\textbf{Tasks:} We define the following two tasks to be completed by \lumyn. Each task includes a description of its completion process and the expected output upon completion.  Each task also has tools associated with it that the agent can use to execute sub-tasks, gather information or interact with the environment.

\begin{itemize}
\item  \textit{Diagnosis Task.} For diagnosis, the goal is to identify the entire fault propagation chain, i.e., \textit{fault propagation chain} (FPC) analysis, and identify the exact cause of the problem within the chain, i.e., \textit{fault localization} (FL).

\item \textit{Mitigation Task.} For mitigation, the goal is to provide natural-language mitigation plans, and execute them to successfully clear the triggering alert. The mitigation plans increase agent explanability and help SREs in understanding why the agent executed certain commands.
\end{itemize}

\textbf{Agents.}  Overall, \lumyn consists of two agents, namely, diagnosis and mitigation agents. Each agent is assigned tasks that it must complete. In general, multi-agent systems can be \textit{hierarchical} or \textit{sequential}. Sequential execution allows tasks to be completed in a fixed, linear order. In hierarchical execution, a ``manager'' agent determines the task execution order and co-ordinates with the other agents. We adopt sequential execution because it is well suited for the SRE use case, where an incident must be diagnosed before it can be resolved. Although, the order of task execution is fixed, the sub-tasks or steps within each task may be completed in any order as determined by the agent itself. We describe the overall workflow of both our agents below.
\begin{itemize}
\item \textit{Diagnosis Agent.} First, the Diagnosis Agent uses the NL2Alerts tool to retrieve the active alerts in the environment. The agent then flexibly and iteratively uses observability tools to gather traces, logs, and metrics from the affected entity mentioned in the alert, and entities associated with the affected entity. It may also use NL2Kubectl commands to investigate the environment. Once the agent determines that it has sufficient information to provide a diagnosis, it proceeds to generate a structured diagnosis report in JSON format with its findings to facilitate evaluation. After the report is generated, the Mitigation Agent takes over.  

\item \textit{Mitigation Agent.} The Mitigation Agent ingests the diagnosis report to create mitigation plans and then utilizes the available tools to implement the plan. This involves using NL2Kubectl commands. To ensure that the executed commands mitigated the incident, it can also use the NL2Alerts tools to check whether the alerts in environment have been cleared. Further, since alerts could sometimes temporarily appear to get cleared due to fluctuations in a live environment, the agent can use the Wait tool to check whether the alerts \textit{stay} cleared even after some time. Finally, upon completion of the execution, the agent generates a JSON explaining the mitigation steps that it took.
\end{itemize}
  


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{appx/usecases/SRE/figures/agent_arch.pdf}
    \caption{\lumyn architecture}
    \label{fig:agent_arch}
\end{figure}





\subsection{\bench Evaluation}
\label{ss:sre-evaluation}

\subsubsection{Experimental details}

We evaluate the \lumyn agent on a set of 42 SRE scenarios in the \bench. For the agent’s LM-based planning component, we consider four distinct models: gpt-4o, granite-3.1-8b-instruct, llama-3.3-70b-instruct, and llama-3.1-8b-instruct. None of these models are fine-tuned.

\Cref{tab:sre-model-param} shows the main hyper-parameter values used in our experiments. These values were chosen to ensure as deterministic results as possible. $decoding\_method$ is applicable for all models except gpt-4o. 


\begin{table}[h]
    \centering
    \small
    \begin{threeparttable}
        \caption{Model hyper-parameters.}
        \begin{tabular}{p{3.5cm}l}
            \toprule
           \textbf{Hyper-parameter} &  \textbf{Value} \\ 
 \midrule
$temperature$ & 0 \\
$top\_p$ & 1e-7\\
$seed$ & 42 \\
$decoding\_method$ & greedy \\ 
\bottomrule
\end{tabular}
\label{tab:sre-model-param}
\end{threeparttable}
\end{table}



\begin{table*}[htp]
\centering
\begin{threeparttable}
\caption{Experimental details}
\label{tab:exp-setup-sre}
\begin{tabular}{@{}l c c c c@{}}
\toprule
\multirow{2}{*}{\textbf{Models}} 
 & \multirow{2}{*}{\textbf{Scenarios}}
 & \multicolumn{3}{c}{\textbf{Experiment Setup}} \\
\cmidrule(lr){3-5}
 & 
 & \textbf{\#Repeats} 
 & \textbf{\#Total} 
 & \textbf{\%Agent Submission} \\
\midrule
\textbf{granite-3.1-8B-instruct}         & 42 & 10 & 420 & 98.76\% \\
\textbf{llama-3.1-8B-instruct} & 42 & 10 & 420  & 100.0\% \\
\textbf{llama-3.3-70B-instruct} & 42 & 10 & 420 & 100\% \\
\textbf{gpt-4o}                       & 42 & 10 & 420 & 99.75\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item \textit{Note:} “\%Agent submission” is the percentage of all trials completed in which the agent returned results. 
\end{tablenotes}
\end{threeparttable}
\end{table*}

\subsubsection{Evaluation Metrics}
\label{sss:eval-metrics}

We evaluate each LM-based agents on two primary tasks: (i) Diagnosis and (ii) Mitigation.

\textbf{Diagnosis.}
The agent is evaluated for diagnosis based on its ability to provide accurate \textit{fault localization} and \textit{fault propagation chains}.
Fault localization allows SREs to identify the exact resource \textit{causing} the problem, whereas fault propagation chain allows SREs to understand how the fault is cascading across the application stack and impacting the application. 
Fault propagation chain can be further used for other important tasks such as blast radius analysis.
\begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
    \item 	\textit{Fault localization} performance is measured using pass@1 and Normalized Topology-Aware Match (NTAM).
	\item \textit{Fault propagation chain} is assessed with NTAM.
	Additionally, we track Mean Time to Diagnosis (MTTD) to gauge overall diagnostic efficiency.
\end{itemize}

\textbf{Mitigation.}
For mitigation, we evaluate how effectively the agent resolves incidents (i.e., clears alerts).
\begin{itemize}[left=0pt, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
    \item Success rate is quantified using pass@1.
    \item Efficiency is captured through Mean Time to Resolution (MTTR).
\end{itemize}
At the time of writing of this paper, \bench lacks the ability to automatically measure the natural language-based unstructured outputs fault condition (i.e., what is wrong with the identified resource) but have plans to extend to this task using LM-as-a-judge~\cite{zheng2023judgingllmasajudgemtbenchchatbot}.

\subsubsection{Metric definitions}
\label{appx:ntam}
\textbf{pass@1.}
We evaluate both fault localization and mitigation using the pass@1 metric~\cite{chen2021evaluating}, which is defined as follows:
$$\text{pass@}k := \mathbb{E}_{\text{Scenarios}}\left[1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\right] $$
It is an unbiased estimator of correctness in \textit{k}=1 trials across all scenarios. For \textit{fault localization}, correctness means whether the predicted root cause exactly matches the ground truth root cause. For \textit{mitigation}, correctness means whether the alerts are cleared. 


\textbf{Normalized Topology-aware Matching.}
Existing approaches for evaluating \textit{fault propagation chains} and \textit{fault localization} focus on exact matches with  the ground truth ~\cite{ahmed2023recommending, zhu2024hemirca, chen2024automatic}, which overlooks topology and finer-grained analysis of propagation chains. For example, existing approaches cannot effectively differentiate agents and models when predicted propagation chains or root causes do not exactly match the ground truth, as they fail to measure how close the predictions are to the actual faults. %
Hence, we propose a new metric \textit{Normalized Topology-Aware Match} (NTAM), which measures agent performance compared to ground truth via topology-aware distance calculation.

NTAM requires a topology graph, where the nodes are the entities of the system, and edges indicate various types of connections between them (e.g., Deployment owns ReplicaSet). Given such a topology, it can be used to evaluate both the set of entities in the fault propagation chains, and the set of root cause entities for fault localization. NTAM is inspired by topology-based distance metrics and information retrieval concepts, such as BM25~\cite{fang2011diagnostic}, that down-weight less discriminative features. It is a flexible, general function with configurable components for fine-grained evaluation of predicted output quality.%

Specifically, it consists of the following main components:
\begin{itemize}
    \item \textit{Topology-based distance scoring} functions, which consider both the edge-type and sub-tree size, rewarding predicted entities closer to the ground truth. %
    Further, nodes with fewer connections (smaller sub-trees) receive higher scores, as they are more discriminative for fault localization.%
    
    \item A \textit{node importance factor} based on the position of the ground truth entity in the propagation chain. This captures the intuition that predicting the ground truth root-cause entity correctly should be rewarded more than getting another entity on the chain correct.
    \item \textit{Penalization terms for length mismatch} between the predicted and ground-truth entities. This is to ensure that predictions having too many or too few entities get lower scores.
\end{itemize}

All the components have corresponding hyper-parameters that can be tuned to adjust their contributions to the overall score. The final score is normalized to be between 0 and 1, where 1 indicates a perfect match. For fault localization, instead of evaluating the set of all entities, only the ground-truth and predicted root-cause entities are considered.




\textbf{Mean Time to Diagnosis.}
For the scenarios where an agent finishes diagnosis successfully (i.e., root cause entities are found), we calculate \textit{MTTD}, which measures how soon (in seconds) an agent performs diagnosis. Otherwise, \textit{MTTD} is set to infinite. 

\textbf{Mean Time to Repair.}
Similarly, for mitigation, we identify the scenarios where an agent executes an automated action to resolve the faults successfully (i.e., alerts are cleared). For these scenarios, we calculate \textit{MTTR} (in seconds), which measures how soon an agent performs mitigation. Otherwise, \textit{MTTR} is set to infinite. 

\subsubsection{Evaluation Results}

We present evaluation results for four LM-based agents across 42 SRE scenarios in the \bench framework.


\textbf{Overall agent results.}
gpt-4o shows the strongest performance, achieving a 13.81\% pass@1 in diagnosis and 11.43\% pass@1 in mitigation (\Cref{tab:sreagent-eval}), significantly higher than any other agent. Moreover, it also attains the best scores on the NTAM metrics (FL and FPC). 
Notably, in hard scenarios (\Cref{tab:appx:sre:traces}), gpt-4o is the only agent capable of performing multiple accurate diagnosis (granite only succeeded once), and \textit{none of the agents can repair the hard scenarios}.
Meanwhile, llama-3.1-8B, despite having fewer parameters, offers the fastest detection (lowest MTTD of 57.50s) and repair times (lowest MTTR of 245.13seconds) among successful attempts. 
Although granite-3.1-8B shares the same parameter size as llama-3.1-8B, it demonstrates slightly better diagnostic capabilities yet weaker mitigation ability. 
llama-3.3-70B performs second best overall, trailing behind gpt-4o on all the metrics we compute. 






\begin{table*}[!ht]
\small
\centering
\caption{Diagnosis pass@1 (in \%).}
\label{tab:sre:diag_pass1}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
\midrule
\textbf{gpt-4o}                             & 36.00 $\pm$ 4.73 & 7.73 $\pm$ 1.74 & 5.00 $\pm$ 2.24 \\
\textbf{granite-3.1-8B-instruct}    & 8.00 $\pm$ 2.68  & 2.73 $\pm$ 1.05 & 1.00 $\pm$ 1.03 \\
\textbf{llama-3.1-8B-instruct}      & 1.18 $\pm$ 1.20  & 1.36 $\pm$ 0.79 & 0.00 $\pm$ 0.00 \\
\textbf{llama-3.3-70B-instruct}      & 10.00 $\pm$ 2.93 & 1.36 $\pm$ 0.78 & 0.00 $\pm$ 0.00 \\
\bottomrule
\end{tabular}

\end{table*}

\begin{table*}[!ht]
\small
\centering
\caption{Repair pass@1 (in \%)}
\label{tab:sre:repair_pass1}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\
\midrule
\textbf{gpt-4o}                             & 21.00 $\pm$ 4.06 & 12.27 $\pm$ 2.19 & 0.00 $\pm$ 0.00 \\
\textbf{granite-3.1-8B-instruct}     & 1.00 $\pm$ 1.01  & 0.00 $\pm$ 0.00  & 0.00 $\pm$ 0.00 \\
\textbf{llama-3.1-8B-instruct}       & 5.88 $\pm$ 2.48  & 1.36 $\pm$ 0.80  & 0.00 $\pm$ 0.00 \\
\textbf{llama-3.3-70B-instruct}      & 7.00 $\pm$ 2.50  & 3.18 $\pm$ 1.16  & 0.00 $\pm$ 0.00 \\
\bottomrule
\end{tabular}

\end{table*}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{appx/usecases/SRE/figures/scenario-15-traj.pdf}
    \caption{Sample Trajectory of llama-3.3-70b-instruct in Scenario 15} 
    \label{fig:incident-15}
\end{figure} 

\textbf{Result analysis by scenario complexity.} 
As we categorize the benchmark scenarios into Easy, Medium, and Hard levels based on the complexity described in \Cref{ss:bench-sre-eq-task-complexity}, a clear performance gap emerges as \textit{scneario\_complexity} increases. In particular, \Cref{tab:sre:diag_pass1} shows lower diagnosis accuracy (pass@1) in more complex scenarios, and \Cref{tab:sre:repair_pass1} reveals a corresponding drop in mitigation success (pass@1). Among the five hard scenarios, none can be resolved by any agent in any run. By contrast, for easy scenarios, over half (five out of eight) scenarios were successfully repaired by at least one agent, and six were diagnosed correctly. 
We use difference‐of‐proportions z‐test to compare success rates across different task levels (evaluating two levels each time). %
\textit{The agent performance consistently declines from Easy to Hard scenarios, validating our complexity model based on propagation chain length, resolution steps, and technology diversity.}

\textbf{Interdependence between diagnosis and mitigation.} Interestingly, diagnosis and mitigation are often intuitively assumed to be interdependent, with accurate diagnosis serving as a prerequisite for effective mitigation. However, \textit{our findings reveal that: in some scenarios, agents can successfully mitigate an incident despite misidentifying the root cause.} For example, in scenario 15 (\Cref{fig:incident-15}), an agent using the llama-3.3-70b model incorrectly identified the root cause as ``memory limit'' in the service, while the real root cause was HTTP request corruption fault; yet it still managed to resolve the issue by scaling up the email service pods to make it functional, essentially bypassing the handling of actual HTTP fault. Such cases illustrate how generic mitigation actions, such as restarting services or scaling replicas, can sometimes fix the system symptoms even without a fully accurate diagnosis. We observed similar behavior in real-world SRE incident analysis, where, despite the root cause remaining unidentified, SREs were able to mitigate the incident.

Conversely, some scenarios highlight the opposite issue: scenario 13, though labeled as ``easy'', cannot be fixed by any of the tested agents, even though they achieved high scores in diagnosing the root cause. Notably, gpt-4o attained roughly 80\% pass@1 and over 0.7 on both FPC and RC (NTAM) metrics. This implies that, \textit{although the agent cannot fully resolve certain issues, it can still offer near-accurate diagnostic insights, potentially assisting human operators in debugging.} 

Some scenarios yield even worse outcomes. For example, in scenario 19 (\Cref{fig:incident-19}), the agent fails to identify the root cause and cannot repair the system, offering only a mitigation plan at the end that is entirely ineffective.



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{appx/usecases/SRE/figures/ibm-agent-bad-traj.pdf}
    \caption{Sample Trajectory of gpt-4o in Scenario 19} 
    \label{fig:incident-19}
\end{figure} 





\textbf{Inconsistency between runs.} Another crucial observation is the agents' inconsistency across repeated runs. For instance, llama-3.1-8b-instruct and mistral-large-2 in scenario 11 occasionally succeed in only a single run out of 10. Though gpt-4o can reliably repair the scenario 8, its running time can fluctuate between 100 and 800 seconds. \textit{These inconsistencies stem from real-time telemetry fluctuations, where minor changes (e.g., CPU utilization reported at 58\% in one run vs.\ 71\% in another) affect LM outputs, leading to varied diagnostic and mitigation results.}












\textbf{Impact of tracing on accuracy.}
Many benchmarks provide raw telemetry data, a key differentiator of \bench is its alert-driven workflow, which mirrors how SREs are notified of faults through golden-signal-alerts triggered from collected telemetry data. To further assess the importance of different telemetry sources, \textit{\bench also supports automated telemetry data masking.} 
As shown in \Cref{tab:appx:sre:traces} and \Cref{tab:appx:sre:disabled}, gpt-4o sees its diagnosis pass@1 drop from 18.10\% (with traces) to 9.52\% (without traces), and its mitigation pass@1 plummet to 2.86\%. Similarly, llama-3.3-70B experiences its diagnosis rate decline from 5.24\% to 0.95\%.
In fact, only three  scenarios were successfully resolved by gpt-4o once trace data was masked.
Take Scenario 13 (easy level) as an example. The agent is able to achieve an 80\% diagnosis rate in its all runs; however, when masking the traces, the rate drops to 0. Note that all of these telemetry masking and agent evaluation steps are integrated into \bench’s automated pipeline. Agents can be evaluated with different observability configurations in \bench.



\begin{table*}[h]
\small
\centering
\begin{threeparttable}
  \caption{Evaluation of SRE-agent only on scenarios with tracing enabled.}
  \label{tab:appx:sre:traces}
  \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{Models}}
      & \multicolumn{4}{c}{\textbf{Diagnosis}}
      & \multicolumn{2}{c}{\textbf{Mitigation}} \\
    \cmidrule(lr){2-5}\cmidrule(lr){6-7}
    & \textbf{pass@1 (\%)$\uparrow$}
    & \textbf{FL (NTAM)$\uparrow$}
    & \textbf{FPC (NTAM)$\uparrow$}
    & \textbf{MTTD (s)$\downarrow$}
    & \textbf{pass@1 (\%)$\uparrow$}
    & \textbf{MTTR (s)$\downarrow$}\\
    \midrule
    \textbf{granite-3.1-8B-instruct} &
    $3.33 \pm 1.20$ &
    $0.15 \pm 0.02$ &
    $0.16 \pm 0.01$ &
    $341.58 \pm 81.71$ &
    $0.48 \pm 0.50$ &
    $845.50 \pm $ \textemdash \\
    \textbf{llama-3.1-8B-instruct} &
    $0.50 \pm 0.51$ &
    $0.07 \pm 0.01$ &
    $0.08 \pm 0.01$ &
    $58.24 \pm $ \textemdash&
    $2.50 \pm 1.09$ &
    $245.39 \pm 49.45$ \\
    \textbf{llama-3.3-70B-instruct} &
    $5.24 \pm 1.59$ &
    $0.21 \pm 0.02$ &
    $0.22 \pm 0.02$ &
    $155.78 \pm 19.91$ &
    $5.71 \pm 1.60$ &
    $449.50 \pm 46.59$ \\
    \textbf{gpt-4o} &
    $18.10 \pm 2.58$ &
    $0.45 \pm 0.05$ &
    $0.37 \pm 0.03$ &
    $67.53 \pm 3.84$ &
    $20.00 \pm 2.75$ &
    $266.97 \pm 32.95$ \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
      \scriptsize 
      \item 21 scenarios, 10 runs per scenario. 
  \end{tablenotes}
\end{threeparttable}
\end{table*}

\begin{table*}[h]
\small
\centering
\begin{threeparttable}
  \caption{Evaluation of SRE-agent only on scenarios in which tracing is disabled.}
  \label{tab:appx:sre:disabled}
  \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{Models}}
      & \multicolumn{4}{c}{\textbf{Diagnosis}}
      & \multicolumn{2}{c}{\textbf{Mitigation}} \\
    \cmidrule(lr){2-5}\cmidrule(lr){6-7}
    & \textbf{pass@1 (\%)$\uparrow$}
    & \textbf{FL (NTAM)$\uparrow$}
    & \textbf{FPC (NTAM)$\uparrow$}
    & \textbf{MTTD (s)$\downarrow$}
    & \textbf{pass@1 (\%)$\uparrow$}
    & \textbf{MTTR (s)$\downarrow$}\\
    \midrule
    \textbf{granite-3.1-8B-instruct} &
    $3.81 \pm 1.30$ &
    $0.18 \pm 0.02$ &
    $0.21 \pm 0.02$ &
    $160.97 \pm 51.06$ &
    $0.00 \pm 0.00$ &
    \textemdash \\
    \textbf{llama-3.1-8B-instruct} &
    $1.46 \pm 0.84$ &
    $0.06 \pm 0.01$ &
    $0.07 \pm 0.01$ &
    $57.26 \pm 2.88$ &
    $1.46 \pm 0.83$ &
    $244.96 \pm 68.53$ \\
    \textbf{llama-3.3-70B-instruct} &
    $0.95 \pm 0.68$ &
    $0.11 \pm 0.02$ &
    $0.10 \pm 0.02$ &
    $430.86 \pm $ \textemdash &
    $0.95 \pm 0.67$ &
    $1429.80 \pm 552.71$ \\
    \textbf{gpt-4o} &
    $9.52 \pm 2.15$ &
    $0.32 \pm 0.05$ &
    $0.31 \pm 0.04$ &
    $85.19 \pm 12.84$ &
    $2.86 \pm 1.12$ &
    $385.87 \pm 15.292$ \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \footnotesize
    \item 21 scenarios, 10 runs per scenario.
  \end{tablenotes}
\end{threeparttable}
\end{table*}

\textbf{Flexibility and extensibility.} Beyond scenario design, \textit{\bench is designed with flexibility and extensibility as guiding principles, allowing for both the addition of new scenarios within existing tasks and the support of new tasks like resource management.} We have already integrated four distinct applications, including both microservice applications (OpenTelemetry-Demo and Hotel-Reservation from DeathStarBench), and non-microservice (TiDB application and Elasticsearch application). \bench makes it straightforward to incorporate custom applications by simply adding an Ansible playbook. It took around four human hours for the external collaborators to add an application to \bench. Moreover, by \bench introduces realistic faults at multiple system layers (e.g., application, virtualization), ensuring a comprehensive evaluation of agent performance across a wide range of failures. 



\begin{figure*}
    \includegraphics[width=\textwidth]{appx/usecases/SRE/figures/trace_on_all_complexity_bar_diagnosed.pdf}
    \caption{\label{fig:sre:trace_on_diagnosis_pass1} Percent diagnosed for each scenario with tracing enabled.}
\end{figure*}

\begin{figure*}
    \includegraphics[width=\textwidth]{appx/usecases/SRE/figures/trace_on_all_complexity_bar_ntam_root_cause.pdf}
    \caption{\label{fig:sre:trace_on_ntam_rc} Normalized topology-aware metric (NTAM) for root cause for scenarios with tracing enabled.}
\end{figure*}

\begin{figure*}
    \includegraphics[width=\textwidth]{appx/usecases/SRE/figures/trace_on_all_complexity_bar_ntam_fault_propagation.pdf}
    \caption{\label{fig:sre:trace_on_ntam_fpc} Normalized topology-aware metric (NTAM) for fault propagation chain (FPC) for each scenario with tracing enabled.}
\end{figure*}

\begin{figure*}
    \includegraphics[width=\textwidth]{appx/usecases/SRE/figures/trace_on_all_complexity_bar_repaired.pdf}
    \caption{\label{fig:sre:trace_on_repair_pass1}Percent repaired for each scenario with tracing enabled.}
\end{figure*}




\input{appx/usecases/SRE/agent_reports}




