\section{Method for Synthetic Data Generation}
\label{app:synthetic_data}

\subsection{Multi-hop Generation of Attribution Data}
To generate synthetic data with the use of Wikipedia, we use the WikiNLP dataset \cite{gashteovski2019opiec}. It contains the text from all Wikipedia articles along with annotations for links within the text that link to other Wikipedia articles. The main idea is to use the links in order to imitate reasoning hops across different (related) articles. Therefore, we filter out all articles that either do not contain links or that contain links to articles that do not contain links. Finally, for each article, we use only the first paragraph, because this is considered to be the paragraph that contains the most ``definitional information'' \cite{bovi2015}; i.e., information that precisely describes the target concept of the article and contains the most important information about it.

%\textcolor{red}{TODO: Write here (at length?) about how our procedure differs from hotpotqa, even though it is similar.
%We are not recreating hotpot qa, the main differences are}

%\textcolor{red}{- Their process it more currated, with human involvement in multiple steps (selecting articles, writing questions + answers + attributions).}
%\textcolor{red}{- We also create dialog data.}
%\textcolor{red}{- We do not enforce the multiple hops, but rather, allow the LLM to decide whether such QA pair is possible.}

Then, for each article, we randomly select a sentence that contains at least one link to another Wikipedia article.\footnote{To make sure we have multi-hop scenario, we also check if the other Wikipedia article also contains at least one valid link to another Wikipedia article.} Each of the sentences that we sample serve as ground truths for the context attribution. With these sentences, we then prompt an LLM to generate a question-answer pair.

\subsection{Question-Answer Pairs Generation}
Using the multi-hop chain of sentences, we prompt an LLM (see \S\ref{app:prompts}) by providing it \textit{only} the formed chain as evidence. The LLM generates a question-answer pair that must be answered using the information in these supporting sentences, ensuring the pairs are grounded in the provided evidence.

\subsection{Distractors Mining}
In realistic scenarios, whether the context is user-provided or retrieved through RAG, the system typically encounters multiple context documents that are highly similar to those containing the evidence sentences. To bridge this gap between our synthetically generated training data using \synqa and the data models encounter ``in the wild'', we augment each training sample with hard negative distractor articles. We obtain embeddings using E5 \cite{wang2022text} for each Wikipedia article in our collection. Then, for each article containing a supporting sentence for the question-answer pair, we randomly sample up to three distractor articles that share semantic similarity with the ground truth article. This process increases the difficulty of the training data, producing models better equipped to handle diverse testing scenarios.

\subsection{Comparison to HotpotQA}

Although our method is inspired by HotpotQA \cite{Yang2018HotpotQAAD}, note that we do not aim to recreate the HotpotQA dataset. Our method has significant differences, which result in both much higher amount of data and in higher domain variability. 

Particularly, their method is more curated and involves humans in multiple steps. First, the authors manually select the target entities (and, with that, the target articles from which the annotators create the question and answer pairs). The reason for this is because many highly specialized articles---e.g., the article for IPv4 protocol---will not be suitable for crowd-workers to both identify meaningful questions and provide answers for those questions. Our approach does not have this constraint and, therefore, produces data that has much higher domain variability. 

Second, their method uses mechanical Turk workers to annotate the questions, answers and attribution sentences. In our case, we automatically select the hopped sentences (which serve as gold context attribution data), and then we use these sentences to generate question-answer pairs with an LLM.

Third, while HotpotQA always enforces multi-hop QA pairs, we do not instruct the LLM to do that. Rather, we first allow the LLM to decide whether generating such multihop QA pair is actually possible for the incoming context attribution sentences. If so, then the LLM generates multihop QA pairs. Otherwise, it generates direct QA pairs that do not need hops; i.e., QA pairs like in SQuAD \cite{Rajpurkar2016SQuAD1Q}.

Fourth, the HotpotQA annotation method does not allow for dialogue QA. In our method, we also create dialogue multi-hop data.

With these differences in mind, we showed that, compared to HotpotQA, our data generation method exhibits the following advantages: (1) our method generates data with higher domain variability; (2) our method goes beyond multi-hop QA and also generates direct QA pairs (like SQuAD) as well as dialogue QA data; (3) we generate the data in completely automatic manner without the involvement of humans. 


%\textcolor{red}{TODO: The procedure with the hops creation needs to be added in the appendix.}


%\begin{enumerate}
%    \item Read the data
%    \item For each data point, do:
%    \item check if the link is hopable. If not, skip. Otherwise, continue.
%    \item sample a linked sentence. This sentence needs to be hopable and to be able to do a second hop from it as well.
%    \item sample a link from the linked sentence. The link should be linkable too.
%\end{enumerate}