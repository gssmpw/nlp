\section{Related Work}
%\textcolor{red}{TODO: Go over the section (ContextCite in particular); Position more precisely w.r.t. related work.}

%We split the related work papers into three parts: (1) context attribution task; (2) in-line citation generation; and (3) post-hoc context attribution.

%\subsection{Context Attribution Task}

%\paragraph{AIS Task.} \citet{rashkin2023measuring} formally proposed the general task of \emph{Attributable to Identified Sources (AIS)}, which aims at answering the following query: given a generated text $t_g$ and a context text $t_c$, is $t_g$ attributable to $t_c$? Broadly speaking, the AIS task can be applied on every NLP task that entails generated text from a GenAI model, including text summarization \cite{ernst2021summary}, table-to-text generation \cite{Parikh2020ToTToAC} and QA \cite{Nakano2021WebGPTBQ}. In our work, we focus on the case of context attribution for QA.

%\paragraph{Granularity.} Many prior methods focus on context attribution on either a document or paragraph level: when an attributive text from the context $t_c$ is assigned to a generated text $t_g$,  then $t_c$ is typically consisted of either whole paragraphs \cite{Menick2022TeachingLM,Yue2023AutomaticEO,Li2024AttributionBenchHH} or whole documents that are either previously retrieved \cite{rashkin2023measuring,Huang2024AdvancingLL} or retrieving the relevant documents from a large document collection is part of the task \cite{Gao2023RARRRA,Buchmann2024AttributeOA}. Such coarse-grained attributions pose difficulties for end-users, because it is harder for them to use the coarse-grained text for further manual fact verification \cite{slobodkin2024attribute}. Other line of work have focused on more fine-grained granularity such as phrases \cite{CohenWang2024ContextCiteAM} or tokens \cite{Phukan2024PeeringIT}. This is also problematic, because these methods are typically too slow for real applications. For these reasons, we focus on sentence-level granularity, as this allows both the context attribution methods to be fast enough and usable by humans for manual fact verification of the generated text. %Instead, users prefer the granularity level to be on sentence level \cite{slobodkin2024attribute}. 


%\paragraph{Context Attribution Categories.} There are two major categories of the context attribution task for QA: (1) in-line citation generation: the LLM is instructed to generate citations along with the generated answers \cite{bohnet2022attributed,gao2023enabling,Huang2024AdvancingLL,slobodkin2024attribute}; (2) post-hoc attribution: the models classify whether a given piece of text is attributable to the answer of the question \cite{Yang2018HotpotQAAD,CohenWang2024ContextCiteAM,Nakano2021WebGPTBQ}. In this work, we focus on post-hoc context attribution for QA.


%In contrast to the original AIS work, Our work differentiates in three important aspects: (1) we focus on a broader QA setup (i.e., single-question QA and conversational QA), which makes our work a subset of the broader AIS task; (2) we focus on more fine-grained level: our attributions are not on the entire text level, but rather on a sentence level, which has been shown in user studies to be more useful to end-users \cite{slobodkin2024attribute}; (3) the AIS task entails a manual evaluation framework, while our work provides automatic evaluation with golden data.

We split the related work on context attribution for QA into two categories: (1) in-line citation generation: LLMs are instructed to generate citations along with the generated answer; (2) post-hoc context attribution: perform the attribution \emph{after} the LLM generates the answer. In this section, we outline these works and their differences from our work. For more comprehensive discussion on related work, see Appendix~\ref{app:rel_work}.

\subsection{In-line Citation Generation}

In this setup, researchers use LLMs to produce in-line citations along with the generated text \cite{bohnet2022attributed,gao2023enabling,Huang2024AdvancingLL}. This typically works on paragraph or document level. One line of work focuses on fine-tuning methods for tackling the problem \cite{gao2023enabling,Schimanski2024TowardsFA,Berchansky2024CoTARCA,patel2024towards}, while another line of work proposes synthetic data generation methods for fine-tuning such models \cite{Huang2024LearningFG,Huang2024AdvancingLL}. 
\citet{slobodkin2024attribute} propose a fine-grained task, where the attributions are on sentence level, because such granularity is more useful to human end users. Since generating such in-line citations can result in producing completely made up citations, \citet{Yue2023AutomaticEO} propose a task that checks whether in-line generated citations from LLMs are actually attributable or not. %Instead of using binary attributable/non-attributable labels (like with AIS), they propose more fine-grained labels for this problem: attributable, extrapolatory, contradictory and non-attributable. 
Unlike such approaches, we focus on post-hoc context attributions, because this directly predicts a link to a factual source, and therefore avoiding the risk of making up the source.%: given an answer to a question, find the sentences in the context that support the factuality of the answer.

\subsection{Post-hoc Context Attribution}

In post-hoc context attribution, the aim is to determine which parts of the context are attributable to an already answered question \cite{Yang2018HotpotQAAD}. There has been a significant amount of work on training models for the context attribution problem on sentence level for multi-hop QA \cite{zhang2024end, ho2023analyzing,yin2023rethinking,fu2021decomposing,tu2020select,fang-etal-2020-hierarchical}. However, they do not investigate this problem in the context of LLMs. Moreover, the methods are constrained \emph{only} to multi-hop QA, and are not tested on broader QA context, such as on dialogue QA. In our work, we propose methods that use LLMs as data generators. This allows us to better generalize and cover multiple QA settings simultaneously, therefore better matching real-world needs. %generalize across multiple QA settings, such as multi-hop QA and conversational QA.

Another line of work focuses on coarse-level granularity and provide attributions either on paragraph level \cite{rashkin2023measuring,Menick2022TeachingLM} or document level \cite{Nakano2021WebGPTBQ,Gao2023RARRRA,Buchmann2024AttributeOA}. However, in a user study \citet{slobodkin2024attribute} observe that such granularity level is not optimal for humans when manually fact-checking LLM-generated content. Their experiments suggest that sentence-level granularity is ideal for humans. This is why we adopt sentence-level granularity in our work, despite this being a harder task. On the other hand, there has been work that focuses on the other extreme: assigning context attributions on sub-sentence level \cite{CohenWang2024ContextCiteAM,Phukan2024PeeringIT}. Such methods are computationally expensive and this hinders their practical usability. Our work ensures that models can be run in real-time to make them practical for end users.
%\footnote{For more detailed discussion on related work, see App.~\ref{app:rel_work}.} 

%ContextCite \cite{CohenWang2024ContextCiteAM} focus on fine-grained post-hoc attribution method that is on a phrase level.

%\subsection{Context Attribution Tasks} 

%\paragraph{Attributable to Identified Sources (AIS):} given a generative text $t_g$ and a context text $t_c$, is $t_g$ attributable to $t_c$? \citet{rashkin2023measuring} propose a manual framework that defines the AIS task and evaluates the AIS scores across several NLP tasks, namely conversational QA \cite{Anantha2020OpenDomainQA,Dinan2018WizardOW}, text summarization \cite{Nallapati2016AbstractiveTS} and table-to-text \cite{Parikh2020ToTToAC}. Our work differentiates in three important aspects: (1) we focus on a broader QA setup (i.e., single-question QA and conversational QA), which makes our work a subset of the broader AIS task; (2) we focus on more fine-grained level: our attributions are not on the entire text level, but rather on a sentence level, which has been shown in user studies to be more useful to end-users \cite{slobodkin2024attribute}; (3) the AIS task entails a manual evaluation framework, while our work provides automatic evaluation with golden data.

%This can be done either post-hoc \cite{Ramu2024EnhancingPA} or in contributive (i.e., causal) manner \cite{CohenWang2024ContextCiteAM}. ClaimVer \cite{Dammu2024ClaimVerEC} does attribution posthoc, but the mapping is done between the generated text and KG triples. The system outputs rationale explanations, attribution scores, relevant KG triples and generates rationales.

%\paragraph{In-line Citation Generation:} %Self-citation methods (or attributed generation, or in-line citation generation) uses LLMs to produce in-line citations along with the generated text \cite{bohnet2022attributed,gao2023enabling,Huang2024AdvancingLL}. This typically works on paragraph or document level. %\citet{gao2023enabling} proposed a fine-tuning method for tackling the problem, while \citet{Huang2024AdvancingLL} propose a synthetic data generation for fine-tuning such models. \citet{slobodkin2024attribute} propose a fine-grained task, where the attributions are on sentence level, because such granularity is more useful to human end users. Because generating such in-line citations can result in producing completely made up citations, \citet{Yue2023AutomaticEO} propose a task that checks whether the in-line generated citations from LLMs are actually attributable or not. Instead of using binary attributable/non-attributable labels (like with AIS), they propose more fine-grained labels for this problem: attributable, extrapolatory, contradictory and non-attributable. Contrary to such approaches, our work focuses on post-hoc context attributions: given an answer to a question, find the sentences in the context that support the factuality of the answer.

%\citet{Yue2023AutomaticEO} propose a task that checks whether in-line generated citations from LLMs are attributable or not. 

%\paragraph{Post-hoc Attribution:} determines which parts of the context are attributable to an already answered question \cite{Yang2018HotpotQAAD}. Within the post-hoc context, there are two other subcategorizations of the task: \textit{contributive} and \textit{corroborative post-hoc attribution} \cite{CohenWang2024ContextCiteAM}.

%\paragraph{Post-hoc Attribution (Contributive):} ContextCite \cite{CohenWang2024ContextCiteAM} and Mirage \cite{Qi2024ModelIA} define a post-hoc task that aims at discovering which parts of the context \textit{caused} the LLM to generate the particular response. Their evaluation methods, however, are based on  proxy metrics that do not rely on golden annotations, while in our work we rely on automatic annotations that rely on golden data. 

%\paragraph{Post-hoc Attribution (Corroborative):} this task is similar to contributive post-hoc attribution. The difference is that the constraint for causality is not necessarily enforced, but should support the factuality of the statement \cite{CohenWang2024ContextCiteAM}. Many works are based on coarse-grained level and provide attributions on either paragraph level \cite{Menick2022TeachingLM}, document level \cite{Nakano2021WebGPTBQ} or on multi-document level, where they have a RAG component that retrieves the documents that are potentially attributable \cite{Gao2023RARRRA,Buchmann2024AttributeOA}.

%ContextCite \cite{CohenWang2024ContextCiteAM} define the task of \textit{contributive attribution}, which aims at discovering what pieces of text \textit{caused} the LLM to generate the particular response. This is different from our approach, where we try to find the attributions that support or imply a statement (i.e., post-hoc attribution). In Mirage \cite{Qi2024ModelIA}, the authors evaluate the answer attribution w.r.t. the RAG. They too, claim to be solving the causal part of the problem.

%\citet{Buchmann2024AttributeOA} focus on long-form attribution, which covers multiple tasks that use long-form context (e.g., QA and text summarization). They do not tackle fine-grained attribution (e.g., sentence level). They also tackle the issue of non-attributable cases. This paper focuses on retrieval case, where the evidence needs to be automatically retrieved. 

%\paragraph{Context Attributions to other Modalities.} Other line of work maps the attributions to other modalities, such as knowledge graphs \citet{Dammu2024ClaimVerEC}. Similarly, \citet{Maheshwari2024PresentationsAN} take multi document collection as input, construct a graph of narratives, and then generate a presentation (i.e., slides) for the topic, along with attributions from the generated content of the slides with the original documents. We do not investigate such cases, and focus on attributing answers to sentences within the user-provided context.

%\paragraph{Post-hoc Attribution for Text Summarization.} \citet{ernst2021summary} proposed a task, dataset and baseline model (dubbed SuperPAL) for detecting attributions for text summarization. In a followup work, \citet{ernst2022proposition} an extension of the task, this time for clustering propositions for text summarization and \citet{ernst2024power} extend this to multi-document summarization. \citet{krishna2023longeval} investigate whether such text summarization alignments are helpful for humans. In our work, we focus on the question answering (QA) task.


%\subsection{Datasets}

%\paragraph{AIS.} \citet{rashkin2023measuring} proposed the AIS dataset, which contains three tasks: question answering, table-to-text and text summarization. Here, for each data point, there is a query and an LLM-generated response, along with label by humans whether it is fully attributable or not. This data is on paragraph and document level, and lacks the granularity of a sentence level. Therefore, we do not use it in our work.

%\paragraph{HotpotQA.} With HotpotQA \cite{Yang2018HotpotQAAD}, the authors propose an explainable multi-hop QA dataset. The dataset also contains attribution links (i.e., explanations) for the answers: spans of text that belong to the input context, which are supporting the statement in the answer. The authors set up baselines for measuring the ability of attributions of models on sentence level, which is in line with what we do. In our work, we integrated HotpotQA as part of our setup for both training and testing.

%\paragraph{AttributionBench.} This is a benchmark for attribution evaluation of LLM generated content \cite{Li2024AttributionBenchHH}. In particular, the benchmark assesses whether the assigned attribution on a generated text is actually attributable. In particular, given a query, response set $\mathcal{R}$ (containing claims) and evidence set $E$, the task is to label as "attributable" or "not attributable" every claim against $E$. This work operates on a coarse-grained level (paragraphs or whole documents). Similarly, \citet{Yue2023AutomaticEO} proposed another dataset for evaluating attribution of LLM-generated text, same on paragraph level. In our work, we focus on sentence-level context attribution. Therefore, we did not include this dataset in our work.

%\paragraph{Conversational QA.} CoQA \cite{Reddy2018CoQAAC} is a conversational QA dataset, which contains a context, questions-answer pairs between two people (teacher and student) and sentence-level supporting evidence for the context. We use this dataset in our evaluation to test the out-of-domain capabilities of LLMs for context attribution. We also use QuAC \cite{Choi2018QuACQA} and ORConvQA \cite{qu2020open}, which are conversational QA datasets similar to CoQA. 

%\paragraph{QASPER.} This dataset is from the scientific domain \cite{dasigi2021dataset}. The dataset contains title and an abstract of a paper, question and answer about the content. This data has information on paragraph level, not on sentence level. Therefore, we do not use it in our experiments.

%\paragraph{WikiQA.} \citet{Dammu2024ClaimVerEC} use WikiQA \cite{yang2015wikiqa}, because it's Wikipedia-based dataset, which can be linked to Wikipedia-derived KG like Wikidata \cite{vrandevcic2014wikidata}. In our work, we focus only on text modality, which is why we do not include this dataset into our evaluation.


% IMPORTANT: we didn't include this data due to the lack of time. Could be interesting to see. \citet{Liu2023EvaluatingVI} evaluate several generative search engines on their ability to correctly attribute sources to the generated content. This is a manual evaluation and is, therefore, not scalable. \textbf{They released a dataset with source attribution on sentence level, which we can reuse.} Unlike question answering, this dataset contains queries with generative search engine responses. This dataset is also used by \citet{Ramu2024EnhancingPA} and \citet{slobodkin2024attribute}.





%\subsection{Metrics and Evaluation} 

%\paragraph{AIS.} The AIS framework \cite{rashkin2023measuring} is human annotation framework. Given a generated text chunk and a context chunk (this can be sentence, paragraph or document), a human evaluated whether the generated text chunk is fully attributible or not. It is basically a binary classification problem. In their data, the authors focus on document level granularity, which is not useful for humans. In our setup, we check for each sentence in the context if it supports the answer.

%\paragraph{AutoAIS.} To evaluate the attributed information, \citet{slobodkin2024attribute} use \textbf{AutoAIS metric}: an NLI-based scoring. Prior studies have shown that this metric highly correlated with human annotations \cite{bohnet2022attributed,gao2023enabling}. This is an extension to the AIS metric. We do not use proxy metrics, but rely on golden annotations by humans.

%\paragraph{AttrScore.} With AttrScore, \citet{Yue2023AutomaticEO} consider LLM generated content on the one hand and citation documents on the other hand. Then, the score evaluates whether a provided citation is attributable, extrapolatory, contradictory or non-attributable. Essentially, it is an extension of AIS, such that it provides more fine grained labels for the provided citations. The AttrScore is basically a fine-tuned LLM that provides these scores.

%\paragraph{Unsupervised Metrics.} ContextCite proposed the Top-k-drop and LDS metric to evaluate the causal post-hoc attribution. These metrics do not require labeled data. \citet{Berchansky2024CoTARCA} uses ROUGE and BERTScore to evaluate their results. We do not use unsupervised metrics and rely on automated evaluation with golden annotations by humans.

%\subsection{Methods} 

%\paragraph{Multihop QA.} There has been significant amount of work on tackling the context attribution problem on sentence level for multi-hop QA \cite{zhang2024end, ho2023analyzing,yin2023rethinking,fu2021decomposing,tu2020select,fang-etal-2020-hierarchical}. While we also investigate this problem, in contrast to our work, these works focus \emph{only} on the multihop QA task. In our work, we also explore other QA setups, including conversational QA with different domains. Moreover, these papers do not investigate the capabilities of LLMs about the context attribution problem, but rather are proposing specific methods that are tailor made for the multihop QA problem, which involves both answering the questions and providing supporting sentences to the answers. %\citet{zhang2024end} proposed an end-to-end beam retrieval method that solves the problem of both answering the questions and assigning attribution.  \citet{ho2023analyzing} built a model based on BigBird \cite{zaheer2020big} 

%\paragraph{In-line Citation Generation.} Another line of work focuses on guiding LLMs to generate in-line citations along with the generated text \cite{Li2023ASO}. \citet{slobodkin2024attribute} tackle this problem on a sentence level, but do not investigate the post-hoc context attribution case. Moreover, they rely on proxy metrics such as AutoAIS \cite{Gao2023RARRRA} and BERTScore \cite{zhang2020bertscore}. Similarly, \citet{bohnet2022attributed} proposes methods for in-line citation generation, but this work is more coarce grained and focuses on paragraph and document level. They also report their findings on proxy metrics. Their method is based on retrieval and they do not investigate the LLMs capabilities thoroughly. \citet{gao2023enabling} assign citations to LLM generated content, where they retrieve the information from a large collection of documents (also, it's on paragraph and document level, not on sentence level). START \cite{Huang2024AdvancingLL} propose a data synthetic generation method for in-line citation generation on document level, where each citation refers to an entire document. FRONT \cite{Huang2024LearningFG} also investigates synthetic data generation of in-line citation generation, where the citations assigned to the sentences in the output are entire documents. Similarly, \citet{Schimanski2024TowardsFA} propose a synthetic data generation pipeline for fine tuning models that solve the same problem. \citet{Berchansky2024CoTARCA} use Chain-of-Thought approaches and fine-tuning smaller LLMs in order to solve this problem. \citet{patel2024towards} also fine-tune a model specifically for this task, and the attributions are on paragraph level.

%\paragraph{Post-hoc Context Attribution.} \citet{Ramu2024EnhancingPA} propose template-based in-context learning method for post-hoc context attribution. In particular, they use standard retrievers as a first step to pre-rank the text (e.g., BM25 and dual encoders \cite{ni2022large}) and then they use LLMs to calssify (i.e., rerank) the relevant sentences. ContextCite \cite{CohenWang2024ContextCiteAM} uses ablation-based methods to infer the attributions of post-hoc generated text. 


%\citet{slobodkin2024attribute} focus on locally attributed text generation: a task that aims at attributing information to the LLM text generation on the fly. In particular, their method is the following: the model first focuses on the right parts of the context (attribution) and then it generates the answer to the query.

%Prior work \cite{bohnet2022attributed,gao2023enabling} was attributing information to whole documents or paragraphs, which makes the task not useful for humans. In contrast, \citet{slobodkin2024attribute} focuses on sentence-level granularity (similar to our approach) and subsentence-level granularity. They focus on two tasks: Multi Document Summarization (MDS) and Long Form Question Answering (LFQA). In contrast, in our approach, we focus on (1) post-hoc attributions of the generated text; (2) multiple variants of question answering (i.e., multi-hop QA and dialogue Q\&A). For data, they used a modified version of \cite{Liu2023EvaluatingVI}. For the in-line citation generation, \citet{Huang2024AdvancingLL} proposed a synthetic data generation method, which helps LLMs generate better training data for this task. 

%\citet{Ramu2024EnhancingPA} use template-based in-context learning with LLMs to tackle the post-hoc attribution problem. 



%which seems like what we are doing (also, on Q\&A). 

%\subsection{Miscellaneous}

 %\citet{si2024large} is more human-centric paper; check here if there is evidence for why we should do this from human side point of view. 
 %Evidence-based Q\&A \cite{Schimanski2024TowardsFA}. 
 
%\cite{Phukan2024PeeringIT} attribution in contextual Q\&A. 


% Other papers suggested by Goran: a survey \cite{Li2023ASO}

%This work is about sub-sentence encoder; probably not relevant unless we want to argue about sub-sentence propositions \cite{Chen2024SubSentenceEC}.

%This paper shows that post-hoc explanations of LLMs can improve them; maybe argument for explaining why we do this? \cite{Krishna2023PostHE}

%Papers suggested by Shahbaz: %\cite{Alnuhait2024FactCheckmatePD,Tao2024YourWL,Orgad2024LLMsKM,Chen2024XplainLLMAK}
% COMMENT: these papers are mostly on the topic of hallucination and are not directly linked to our work.

% Leakge detection with n-grams \cite{xu2024benchmarking}

% TruthReader \cite{Li2024TruthReaderTT} is a UI that enables users to upload documents, query an LLM w.r.t.~the documents, and show attributions of the LLM outputs w.r.t.~the input documents. The attributions are scored with ROUGE scores. 


