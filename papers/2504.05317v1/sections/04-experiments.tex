\section{Experimental Study}\label{sec:experiments}
We conduct a comprehensive evaluation across multiple aspects: zero-shot performance, comparison with training on gold attribution data, and generalization to dialogue settings.
% . Our experiments span both in-\textit{isolation} question-answering datasets and in-\textit{dialogue} scenarios,
With our experiments, we shed light on the performance and practical utility of our approach.

% \textcolor{red}{TODO: Should we introduce the two settings separately: in-isolation QA and in-dialogue QA?}

% \textcolor{red}{TODO: We need an introduction sentence for this section.}

% \textcolor{red}{TODO: We mention isolated context attribution in some parts of the paper, while it is not clear how it differs from dialog-based context attribution.}

\subsection{Experimental Setting}
We evaluate model performance using precision (P), recall (R), and F1 score. For each sentence in the LLM's output, the context-attribution models identify the set of context sentences that support that output sentence. Precision measures the proportion of predicted attributions that are correct, while recall measures the proportion of ground truth attributions that are successfully identified.
%F1 is the harmonic mean of precision and recall.

For a fair and comprehensive evaluation, we train all models with a single pass over the training data unless stated otherwise, referring to this setup as \textbf{1P} when needed. For a more controlled comparison, some experiments limit the number of training samples each model encounters. Since the synthetic dataset contains approximately 1.0M samples, we allow models to \textit{observe} an equivalent number of samples from the gold training set, ensuring comparable exposure to models trained on data from \synqa. We refer to this setting as \textbf{1M} when necessary. For all models, we fine-tune only the LoRA parameters (alpha=64, rank=32) using a fixed learning rate of 1e-5 and a weight decay of 1e-3. 

\textbf{In-domain datasets:} We use \squadcolor{SQuAD} \cite{Rajpurkar2016SQuAD1Q} and \hotpotcolor{HotpotQA} \cite{Yang2018HotpotQAAD} as our primary in-domain benchmarks.\footnote{For some experiments (e.g., in Table~\ref{table:zero-shot-models}), these datasets are also \textit{out-of-domain} w.r.t. data generated by \synqa.} SQuAD provides clear sentence-level evidence for answering questions, serving as a strong baseline for direct attribution. HotpotQA introduces multi-hop reasoning, requiring models to link information across multiple sentences (sometimes from different articles) to identify the correct evidence chain. Additionally, HotpotQA includes distractor documents—closely related yet incorrect sources—posing a more challenging but realistic setting for evaluating attribution performance.

%\textcolor{red}{TODO (Kiril): Explain what you do to OR-QUAC, you combine the background with the context?}
\textbf{Out-of-domain datasets:} To assess generalization beyond the training distribution, we evaluate models on \quaccolor{QuAC} \cite{Choi2018QuACQA}, \coqacolor{CoQA} \cite{Reddy2018CoQAAC}, \orquaccolor{OR-QuAC} \cite{qu2020open}, and \doqacolor{DoQA} \cite{campos-etal-2020-doqa}. %\footnote{We consider these datasets as \textit{out-of-domain}, as none of the models we train are exposed to the training data of these datasets.}. 
These datasets present conversational QA scenarios that differ from SQuAD and HotpotQA. Specifically, QuAC and CoQA introduce multi-turn dialogue structures with coreferences, challenging models to track context across multiple turns. This conversational nature creates a methodological challenge: while these datasets are valuable for evaluating dialogue-based attribution, their reliance on conversation history makes direct comparison with models trained on single-turn QA datasets impossible.

To enable comprehensive evaluation across dialogue QA and single-turn QA, we create two versions of each dataset:
\begin{inparaenum}[(i)]
    \item a rephrased version using Llama 70B \cite{Dubey2024TheL3} that converts questions into standalone format for fair comparison with models trained on single-turn context attribution (suffixed by ``-ST''), and
    \item the original version for assessing dialogue-based attribution.
\end{inparaenum}

% To adapt these datasets for isolated context attribution (e.g., such as SQuAD and HotpotQA, where the question-answer pair is standalone),
% \footnote{We refer to isolated context attribution the scenario where the question-answer pair are standalone: i.e., do not contain coreferences.},
% we rephrase question-answer pairs (using Llama 70B), so that coreferencing is unnecessary. However, in dialogue-based settings, we evaluate models on the original, unmodified versions of these datasets.

DoQA extends this challenge further by incorporating domain-specific dialogues (cooking, travel and movies)%\footnote{The domains covered in DoQA are: cooking, travel and movies \cite{campos-etal-2020-doqa}.}
, thus testing the models' adaptability to specialized contexts. OR-QuAC includes %open-retrieval dialogue settings, assesses models' ability to attribute context in less structured environments, adding another layer of complexity to generalization evaluation. \textcolor{red}{TODO (Kiril): Check the papers for these datasets in case something is overlooked here.}
context-independent rewrites of the dialogue questions, such that they can be posed in isolation of prior context (i.e., single-turn QA). This enables us to test the models on their capabilities in both single-turn QA and dialogue QA settings.

\subsection{Methods}
We compare our method (\synqa) against several baselines, including sentence-encoder-based models, zero-shot instruction-tuned LLMs, and models trained on synthetic and gold context-attribution data. Specifically, we experiment with the following methods:

\paragraph{Sentence-Encoders:} We embed each sentence in the context along with the question-answer pair, and select attribution sentences based on cosine similarity with a fixed threshold, tuned on a small validation set.

\paragraph{Zero-shot (L)LMs:} We evaluate various instruction-tuned (L)LMs in a zero-shot manner, as such models have been shown to perform well across a range of NLP tasks \cite{shu2023exploitability,zhang2023instruction}. During inference, we provide an instruction template describing the task to the LLM (see Appendix~\ref{app:prompts} for details).
%as such models have been shown to perform well across a range of NLP tasks.

\paragraph{Ensembles of LLMs:} We aggregate the predictions of multiple LLMs through majority voting, selecting attribution sentences that receive consensus from at least 50\% of the ensemble. In our experiments, we use Llama8B \cite{Dubey2024TheL3}, Mistral7B, and Mistral-Nemo12B \cite{Jiang2023Mistral7} as the ensemble constituents.


\paragraph{Models trained on in-domain gold data:} Fine-tuning on gold-labeled attribution data provides an upper bound on in-domain performance, helping us assess how well synthetic training data generalizes.

\paragraph{\synatt:} \synatt generates synthetic training data by prompting multiple LLMs to perform context attribution in a discriminative manner, aggregating their outputs via majority voting, and training a smaller model on the resulting dataset. To make it a stronger baseline against \synqa, we give the training data of SQuAD and HotpotQA (the context, questions, and answers) to the LLMs and ask them to perform context attribution (note that we do not use the gold attribution). Finally, we train a model on the generated synthetic data.

\paragraph{\synqa:} We train models using synthetic data generated by our proposed method \synqa. Note that even though we train models using \synqa attribution data, we ensure they are not exposed to \textit{any} parts of the evaluation data.\footnote{We identify data leakage by representing each Wikipedia article as a MinHash signature. Then, for each training Wikipedia article, we retrieve candidates from the testing datasets via Locality Sensitivity Hashing and compute their Jaccard similarity \cite{dasgupta2011fast}. We flag as potential leaks pairs exceeding a threshold empirically set to 0.8.}

\subsection{Results and Discussion}
Evaluating our context attribution models requires a multifaceted approach, as performance is influenced by both the quality of training data and the model’s ability to generalize beyond in-domain distributions. Therefore, we design our experiments to address five core questions:
\begin{inparaenum}[(i)]
    \item How well do zero-shot LLMs perform on context-attribution QA tasks (\S\ref{sec:experiments-zero-shot})?
    \item Can models trained on synthetic data generated by \synqa exceed the performance of models trained on gold context-attribution data (\S\ref{sec:experiments-gold})?
    \item To what extent do models generalize to dialogue settings where in-domain training data is unavailable (\S\ref{sec:experiments-dialog})?
    \item How well do models scale in terms of synthetic data quantity generated by \synqa (\S\ref{sec:scalling-trends})?
    \item How do improved context attributions impact the end users' speed and ability to verify questions answering outputs (\S\ref{sec:user-study})?
\end{inparaenum}
% (i) How well do zero-shot LLMs perform context-attribution? (ii) Can synthetic attribution data serve as a viable alternative to gold supervision, particularly in out-of-domain settings? (iii) How do scaling trends affect generalization performance across diverse datasets?

%By systematically comparing models trained on synthetic data to both zero-shot and gold-supervised baselines, we aim to uncover the trade-offs between scalability, performance, and generalization. 
%Collectively, our findings provide a deeper understanding of how synthetic data can be leveraged for context attribution, potentially mitigating the reliance on costly human-annotated datasets.

\subsubsection{Comparison to Zero-Shot Models}\label{sec:experiments-zero-shot}

% Zero-shot v.s. SynQA-trained Models

\begin{table*}[t]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lccccccccccccccc} \toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Training data} & \multicolumn{3}{c}{\squadcolor{Squad}} & \multicolumn{3}{c}{\hotpotcolor{Hotpot}} & \multicolumn{3}{c}{\quaccolor{Quac-ST}} & \multicolumn{3}{c}{\coqacolor{CoQA-ST}} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
& & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\ \midrule
\textbf{\textit{Baselines}} \\
Random & -- & 19.8 & 15.4 & 17.3 & 4.8 & 15.2 & 7.3 & 5.2 & 15.1 & 7.7 & 7.3 & 15.1 & 9.9 \\
E5 | 561M & Zero-shot & 38.1 & 76.5 & 50.9 & 12.4 & 41.4 & 19.1 & 65.0 & 73.8 & 69.1 & 61.1 & 15.2 & 24.4 \\
HF-SmolLM2 | 365M & Zero-shot & 28.1 & 46.4 & 35.0 & 5.1 & 7.3 & 6.0 & 10.6 & 22.6 & 14.4 & 10.6 & 21.5 & 14.2 \\
Llama | 1B & Zero-shot & 37.5 & 62.0 & 46.7 & 5.3 & 28.1 & 8.9 & 8.8 & 65.4 & 15.4 & 11.9 & 52.8 & 19.4 \\
Mistral | 7B & Zero-shot & 71.5 & 94.4 & 81.4 & 42.9 & 42.7 & 42.8 & 63.2 & 88.6 & 73.8 & 59.0 & 72.2 & 64.9 \\
Llama | 8B & Zero-shot & 71.9 & 96.9 & 82.6 & 49.2 & 52.9 & 51.0 & 64.1 & 92.1 & 75.6 & 55.7 & 76.4 & 64.4 \\
Mistral-NeMo | 12B & Zero-shot & 89.5 & 94.5 & 91.8 & 46.4 & 47.3 & 46.8 & 81.8 & 85.3 & 83.5 & 79.0 & 67.2 & 72.6 \\
Ensemble | 27B & Zero-shot & 83.1 & 96.3 & 89.2 & 48.1 & 59.6 & 53.2 & 74.8 & 90.3 & 81.8 & 69.5 & 73.6 & 71.5 \\
Llama | 70B & Zero-shot & 95.3 & 95.6 & 95.5 & 87.6 & 37.5 & 52.5 & 89.7 & 87.8 & 88.7 & \textbf{87.5} & \textbf{73.3} & \textbf{79.8} \\
\midrule
\textbf{\textit{Baselines}} \\
%Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA}; \synatt (1P) & 89.8 & 96.5 & 93.0 & 50.6 & 58.6 & 54.3 & 64.9 & 91.5 & 75.9 & 53.1 & 75.5 & 62.3 \\
%Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA}; \synatt (1M) & 84.3 & \textbf{96.9} & 90.2 & 54.4 & 58.0 & 56.1 & 63.4 & 92.4 & 75.2 & 52.5 & 77.5 & 62.6 \\ \midrule
Llama | 1B & \synatt (1P) & 89.8 & 96.5 & 93.0 & 50.6 & 58.6 & 54.3 & 64.9 & 91.5 & 75.9 & 53.1 & 75.5 & 62.3 \\
Llama | 1B & \synatt (1M) & 84.3 & \textbf{96.9} & 90.2 & 54.4 & 58.0 & 56.1 & 63.4 & 92.4 & 75.2 & 52.5 & 77.5 & 62.6 \\ \midrule
\textbf{\textit{Ours}} \\
Llama | 1B & \syntheticcolor{\synqa} & \textbf{96.0} & 96.2 & \textbf{96.1} & \textbf{89.6} & \textbf{69.4} & \textbf{78.2} & \textbf{93.3} & \textbf{89.1} & \textbf{91.1} & \underline{82.3} & 68.5 & \underline{74.8} \\
\bottomrule
\end{tabular}
}
\caption{Comparison of zero-shot models and those trained with synthetic data. Larger zero-shot LMs excel, but our \synqa model outperforms all but one for one dataset while being smaller. \textbf{Bold} denotes best method, \underline{underline} if our method is second best. 1P: models trained with a single pass over the training data. 1M: models trained with 1M samples to match the size of the \synqa data.}
\label{table:zero-shot-models}
\end{table*}

In Table~\ref{table:zero-shot-models}, we present the performance of zero-shot models, and models trained without gold context-attribution data. 
%\footnote{Note that the \synatt baseline models are trained using question-answer pairs from SQuaAD and HotpotQA, however, the context-attribution annotations are obtained using an ensemble of LLMs.}. 
State-of-the-art sentence-encoder models (e.g., E5) perform relatively poorly, consistent with prior findings \cite{CohenWang2024ContextCiteAM}. In contrast, LLMs exhibit strong performance, with improvements correlating with model size. Ensembling multiple zero-shot LLMs further enhances performance, leveraging complementary strengths across models, but making the attribution more expensive. We also tested models trained with the discriminative method \synatt. These models significantly outperform their non-fine-tuned counterparts of the same size. However, as postulated, our generative approach \synqa outperforms \synatt significantly in all but one case. Additionally, \synqa surpasses zero-shot LLMs that are orders of magnitude larger, showing that we can train a model that is both more accurate and efficient.

\subsubsection{Comparison to Models Trained on Gold Attribution Data}\label{sec:experiments-gold}

\begin{table*}[t]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lccccccccccccccc} \toprule
\multirow{3}{*}{Model} & \multirow{3}{*}{Training data} 

& \multicolumn{6}{c}{\textbf{In-Domain}} 
& \multicolumn{6}{c}{\textbf{Out-of-Domain}} \\ \cmidrule(lr){3-8} \cmidrule(lr){9-14}

& & \multicolumn{3}{c}{\squadcolor{SQuAD}} & \multicolumn{3}{c}{\hotpotcolor{HotpotQA}} 
& \multicolumn{3}{c}{\quaccolor{QuAC-ST}} & \multicolumn{3}{c}{\coqacolor{CoQA-ST}} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}

& & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\ \midrule
\textbf{\textit{Baselines}} \\
Llama | 1B & Zero-shot & 37.5 & 62.0 & 46.7 & 5.3 & 28.1 & 8.9 & 8.8 & 65.4 & 15.4 & 11.9 & 52.8 & 19.4 \\
Llama | 1B & \squadcolor{SQuAD} (1P) & 98.4 & 98.4 & 98.4 & 48.7 & 20.0 & 28.4 & 92.6 & 85.8 & 89.0 & 79.9 & 64.3 & 71.2 \\
Llama | 1B & \hotpotcolor{HotpotQA} (1P) & 41.3 & 87.3 & 56.0 & 87.5 & 79.9 & 83.5 & 45.2 & 89.9 & 60.1 & 41.0 & 70.9 & 52.0 \\
Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} (1P) & 98.3 & 98.3 & 98.3 & \textbf{89.7} & 78.9 & 84.0 & 90.4 & 90.0 & 90.2 & 83.1 & 68.0 & 74.8 \\
Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} (1M) & \textbf{98.3} & \textbf{98.4} & \textbf{98.3} & 87.0 & \textbf{85.2} & \textbf{86.1} & 84.0 & 89.2 & 86.6 & 79.2 & 66.4 & 72.2 \\ \midrule
\textbf{\textit{Ours}} \\
Llama | 1B & \syntheticcolor{\synqa} & 96.0 & 96.2 & 96.1 & \underline{89.6} & 69.4 & 78.2 & \underline{93.3} & 89.1 & \underline{91.1} & 82.3 & 68.5 & \underline{74.8} \\
Llama | 1B & \syntheticcolor{\synqa} \& \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} & \underline{98.2} & \underline{98.3} & \underline{98.2} & 89.3 & \underline{82.4} & \underline{85.8} & \textbf{94.5} & \textbf{92.7} & \textbf{93.6} & \textbf{85.5} & \textbf{71.0} & \textbf{77.6} \\
\bottomrule
\end{tabular}
}
\caption{Comparison of models fine-tuned on synthetic vs.~gold in-domain data. Our \synqa approach generalizes better while remaining competitive in-domain. \textbf{Bold} denotes best method, \underline{underline} our method when second best. 1P: models trained with a single pass over the training data. 1M: models trained with 1M samples to match the size of the \synqa data.}
\label{table:fine-tuned-models}
\end{table*}

In Table~\ref{table:fine-tuned-models}, we compare models trained on synthetic and gold in-domain context-attribution datasets. As expected, fine-tuning on in-domain gold datasets (SQuAD and HotpotQA) yields highly specialized models that perform well on in-domain data.
% The performance on the out-of-domain datasets is comparable to Llama 70B, the best zero-shot LLM.
% In contrast, \synqa models outperform Llama 70B on out-of-domain datasets while also achieving near identical scores on the in-domain datasets.
However, models trained on data obtained by \synqa exhibit competitive performance on in-domain tasks and consistently surpass in-domain-trained models on out-of-domain datasets. 
This strong out-of-domain generalization is crucial for practical deployments, where models must handle diverse, previously unseen contexts that often differ substantially from their training data.

\subsubsection{Comparison to Zero-Shot and Fine-Tuned Models in Dialogue Contexts}\label{sec:experiments-dialog}
% \begin{table}[t]
% \centering
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{lccccccc} 
% \toprule

% \multirow{2}{*}{Model} & \multirow{2}{*}{Training data} & \multicolumn{3}{c}{\quaccolor{QuAC}} & \multicolumn{3}{c}{\coqacolor{CoQA}} \\ 
% \cmidrule(lr){3-5} \cmidrule(lr){6-8}

%  &  & P & R & F1 & P & R & F1 \\ 
% \midrule
% \textbf{\textit{Baselines}} \\
% Llama | 1B & Zero-shot & 20.9 & 47.9 & 29.1 & 35.6 & 40.2 & 37.8 \\
% Mistral | 7B & Zero-shot & 64.9 & 83.9 & 73.2 & 54.4 & 64.9 & 59.2 \\
% Llama | 8B & Zero-shot & 81.4 & 89.0 & 85.0 & 77.8 & 72.1 & 74.8 \\
% Mistral NeMo | 12B & Zero-shot & 84.8 & 85.4 & 85.1 & 81.7 & 68.4 & 74.5 \\
% \midrule
% Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} (1P) & 72.9 & 68.0 & 70.3 & 79.3 & 64.4 & 71.0 \\
% Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} (1M) & 56.0 & 49.0 & 52.3 & 63.0 & 51.2 & 56.5 \\ 
% \midrule
% \textbf{\textit{Ours}} \\
% Llama | 1B & \syntheticcolor{\synqa} & \textbf{91.3} & \underline{91.4} & \underline{91.3} & \underline{81.7} & \underline{71.4} & \underline{76.2} \\
% Llama | 1B & \syntheticcolor{\synqa} \& \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} & \underline{91.1} & \textbf{92.3} & \textbf{91.7} & \textbf{82.3} & \textbf{73.2} & \textbf{77.5} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Context attribution on QuAC and CoQA (dialog data); both datasets are out-of-domain. Despite the size advantage of zero-shot LLMs, our \synqa models outperform fine-tuned and larger zero-shot models. \textbf{Bold} denotes best method, \underline{underline} our method when second best.}
% \label{table:dialog-datasets}
% \end{table}

\begin{table*}[t]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lcccccccccccccc} 
\toprule

\multirow{3}{*}{Model} & \multirow{3}{*}{Training data} 

& \multicolumn{12}{c}{\textbf{Out-of-Domain}} \\ \cmidrule(lr){3-14}

& & \multicolumn{3}{c}{\quaccolor{QuAC}} & \multicolumn{3}{c}{\coqacolor{CoQA}} 
& \multicolumn{3}{c}{\orquaccolor{OR-QuAC}} & \multicolumn{3}{c}{\doqacolor{DoQA}} \\ 

\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}

 &  & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\ 
\midrule
\textbf{\textit{Baselines}} \\
Llama | 1B & Zero-shot & 30.8 & 45.5 & 36.8 & 39.4 & 37.9 & 38.6 & 33.0 & 46.6 & 38.6 & 12.2 & 22.6 & 15.9 \\
Mistral | 7B & Zero-shot & 76.6 & 81.8 & 79.1 & 67.6 & 61.3 & 64.3 & 82.5 & 85.1 & 83.8 & 74.9 & 77.9 & 76.4 \\
Llama | 8B & Zero-shot & 84.7 & 88.8 & 86.7 & 79.3 & 72.0 & 75.5 & 88.0 & 91.3 & 89.6 & 77.9 & 91.4 & 84.1 \\
Mistral-NeMo | 12B & Zero-shot & 85.7 & 85.4 & 85.5 & 81.9 & 68.4 & 74.5 & 88.9 & 88.8 & 88.8 & 86.0 & 84.2 & 85.1 \\
Llama | 70B & Zero-shot & 88.5 & 87.7 & 88.1 & \textbf{88.3} & \textbf{74.9} & \textbf{81.1} & 81.7 & 86.3 & 83.9 & 85.2 & 82.0 & 83.5 \\
\midrule
\textbf{\textit{Baselines}} \\
Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} (1P) & 71.3 & 66.8 & 69.0 & 79.0 & 64.2 & 70.8 & 61.6 & 57.5 & 59.5 & 67.4 & 57.8 & 62.2 \\
Llama | 1B & \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} (1M) & 52.6 & 49.3 & 50.9 & 61.2 & 50.2 & 55.2 & 48.5 & 44.6 & 46.5 & 53.2 & 49.1 & 51.1 \\ \midrule
\textbf{\textit{Ours}} \\
Llama | 1B & \syntheticcolor{\synqa} & \textbf{91.3} & \underline{91.4} & \underline{91.3} & 81.7 & 71.4 & 76.2 & \textbf{92.6} & \underline{95.3} & \textbf{94.0} & \textbf{86.3} & \underline{94.5} & \textbf{90.2} \\
Llama | 1B & \syntheticcolor{\synqa} \& \squadcolor{SQuAD} \& \hotpotcolor{HotpotQA} & \underline{91.1} & \textbf{92.2} & \textbf{91.7} & \underline{82.3} & \underline{73.2} & \underline{77.5} & \underline{90.3} & \textbf{96.4} & \underline{93.2} & \underline{85.1} & \textbf{96.0} & \textbf{90.2} \\
\bottomrule
\end{tabular}
}
\caption{Context attribution on QuAC, CoQA, OR-Quac, and DoQA (dialogue data); all datasets are out-of-domain. Despite the size advantage of zero-shot LLMs, our \synqa models outperform fine-tuned and larger zero-shot models. \textbf{Bold} denotes best method, \underline{underline} our method when second best. 1P: models trained with a single pass over the training data. 1M: models trained with 1M samples to match the size of the \synqa data.}
\label{table:dialog-datasets}
\end{table*}

We evaluate dialogue context attribution, for which we do not use any gold in-domain training data (Tab.~\ref{table:dialog-datasets}).
% exists.
Here, models must handle follow-up questions that rely on previous turns, often involving coreferences and other dialogue-specific complexities. As expected, zero-shot LLMs exhibit a strong size-performance correlation, with larger models consistently outperforming smaller ones—even those fine-tuned on single-turn question-answer attribution (trained on gold SQuAD and HotpotQA data). However, fine-tuning smaller models with our synthetic data generation strategy leads to superior performance, surpassing both their fine-tuned counterparts and much larger zero-shot LMs. This demonstrates the effectiveness of \synqa in enhancing context attribution in a dialogue setting and without requiring in-domain supervision.

\subsubsection{Scaling Trends and Generalization Performance}\label{sec:scalling-trends}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/size_performance.pdf}
        \caption{Model performance vs.~size.}
        \label{fig:size_performance}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/data_quantity.pdf}
        \caption{F1 score vs.~training data size.}
        \label{fig:data_quantity}
    \end{subfigure}
    \caption{Comparison of model performance and scalability. (a) Larger zero-shot models achieve good F1 scores, but our method \synqa (based on Llama 1B) outperforms them while being orders of magnitude smaller. (b) Performance improves consistently with more \synqa training data, highlighting its scalability.}
    \label{fig:combined}
\end{figure*}

Fig.~\ref{fig:size_performance} shows F1 scores averaged across datasets, with model size on the x-axis and performance on the y-axis. Models trained on \synqa-generated data significantly outperform their baseline zero-shot counterparts, while also achieving superior performance compared to zero-shot LLMs that are orders of magnitude larger. This shows our method is highly data-efficient, enabling small models to close the gap with much larger counterparts.



In Figure~\ref{fig:data_quantity}, we analyze model performance as the quantity of synthetic training data increases, reporting F1 scores separately for in-domain and out-of-domain datasets. As we scale data quantity, performance improves consistently across datasets for isolated context attribution. This trend highlights the scalability of our approach, indicating that further gains can be achieved by increasing synthetic data availability.
%Notably, despite the lack of direct supervision on in-domain datasets, more data results in improved performance.%, reinforcing the robustness of our method.

\subsubsection{User Study: \synqa increases efficiency and accuracy assessment}\label{sec:user-study}
We conducted a user study to evaluate the efficiency and accuracy of verifying the correctness of LLM-generated answers using context attribution. Our hypothesis is that higher-quality context attributions, visualized to guide users, facilitate faster and more accurate verification of LLM outputs. Specifically, in each trial, we presented users with a question, a generated answer, and relevant context, along with
% context
attributions visualized as highlights. Their task was to leverage these attributions to judge if the answer was correct w.r.t.~a provided context. See Figure~\ref{fig:user_interface} in Appendix~\ref{app:user_study}.
% for an example. %Appendix~\ref{app:user_study} for an example.

The study compares three scenarios:
\begin{inparaenum}[(i)]
\item 
\textbf{No Alignment:} a baseline condition without context attributions, requiring users to manually read and verify the answer against the entire context;
\item 
\textbf{Llama 1B (Zero-shot):} context attributions generated by the Llama 1B model were visualized;
\item 
\textbf{\synqa}: context attributions generated by our approach were visualized.
\end{inparaenum}

We employed a within-subjects experimental design for our human evaluation (with 12 participants), ensuring that the same participants evaluate all the aforementioned alignment scenarios, thus requiring fewer participants for reliable results \cite{greenwald:1976}. However, this can be susceptible to learning effects where participants perform better in later scenarios, because they learned the task from previous examples. To mitigate this, we counterbalanced the scenario order using a Latin Square design \cite{belz:2010,bradley:1958}, where each alignment scenario appears in each position an equal number of times across all participants. Finally, we randomized the example order within each scenario per participant. For each example, we measured: \textbf{verification time} (seconds from display to judgment submission) and \textbf{verification accuracy} (binary correct/incorrect judgment).

\begin{figure}[hb!]
    \centering
    \includegraphics[width=1.0\linewidth]{img/user_study_big_font.png}
    \caption{Relationship between Evaluation Time (seconds) and Accuracy (\%) for three answer verification settings:  \emph{Llama 1B (Zero-shot)}, \emph{No Alignment} and \synqa. \synqa demonstrates the lowest evaluation time and highest accuracy, indicating its superior performance in facilitating efficient and accurate answer verification.}
    \label{fig:user_study}
\end{figure}

\noindent \textbf{Results.} We observed a clear trend in verification performance across the different attribution settings, with \synqa demonstrating superior effectiveness (Fig.~\ref{fig:user_interface}). \synqa has the lowest average verification time per example (\textbf{148.6} seconds), significantly faster than \emph{No Alignment} (171.8 seconds) and attributions from \emph{Llama 1B} (163.4 seconds). Concurrently, in terms of verification accuracy, \synqa achieved the highest average accuracy (\textbf{86.4\%}). While \emph{No Alignment} (84.1\%) and \emph{Llama 1B (77.3\%)} also yielded reasonable accuracy, attributions from \synqa are clearly of higher quality helping users be more accurate.


% \begin{table*}[t]
% \centering
% \resizebox{1.0\textwidth}{!}{
% \begin{tabular}{lccccccccccccccc} \toprule
% \multirow{2}{*}{Model} & \multirow{2}{*}{Training data} & \multicolumn{3}{c}{\squadcolor{Squad}} & \multicolumn{3}{c}{\hotpotcolor{HotPot QA}} & \multicolumn{3}{c}{\quaccolor{Quac}} & \multicolumn{3}{c}{\coqacolor{CoQA}} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
% & & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\ \midrule
% Random & -- & 19.8 & 15.4 & 17.3 & 4.8 & 15.2 & 7.3 & 5.2 & 15.1 & 7.7 & 7.3 & 15.1 & 9.9 \\
% E5 | 561M & Zero-shot & 38.1 & 76.5 & 50.9 & 12.4 & 41.4 & 19.1 & 65.0 & 73.8 & 69.1 & 61.1 & 15.2 & 24.4 \\
% HF-SmolLM2 | 135M & Zero-shot & X & X & X & X & X & X & X & X & X & X & X & X \\
% HF-SmolLM2 | 365M & Zero-shot & 28.1 & 46.4 & 35.0 & 5.1 & 7.3 & 6.0 & 10.6 & 22.6 & 14.4 & 10.6 & 21.5 & 14.2 \\
% Llama | 1B & Zero-shot & 37.5 & 62.0 & 46.7 & 5.3 & 28.1 & 8.9 & 8.8 & 65.4 & 15.4 & 11.9 & 52.8 & 19.4 \\ %\midrule
% Mistral | 7B & Zero-shot & 71.5 & 94.4 & 81.4 & 42.9 & 42.7 & 42.8 & 63.2 & 88.6 & 73.8 & 59.0 & 72.2 & 64.9 \\
% Llama | 8B & Zero-shot & 71.9 & 96.9 & 82.6 & 49.2 & 52.9 & 51.0 & 64.1 & 92.1 & 75.6 & 55.7 & 76.4 & 64.4 \\
% Mistral NeMo | 12B & Zero-shot & 89.5 & 94.5 & 91.8 & 46.4 & 47.3 & 46.8 & 81.8 & 85.3 & 83.5 & 79.0 & 67.2 & 72.6 \\
% Ensemble | 27B & Zero-shot & 83.1 & 96.3 & 89.2 & 48.1 & 59.6 & 53.2 & 74.8 & 90.3 & 81.8 & 69.5 & 73.6 & 71.5 \\
% Llama | 70B & Zero-shot & 95.3 & 95.6 & 95.5 & 87.6 & 37.5 & 52.5 & 89.7 & 87.8 & 88.7 & 87.5 & 73.3 & 79.8 \\
% \midrule
% Llama | 1B & \squadcolor{SQ} \& \hotpotcolor{HP}; Disc. synthetic (1 pass) & 89.8 & 96.5 & 93.0 & 50.6 & 58.6 & 54.3 & 64.9 & 91.5 & 75.9 & 53.1 & 75.5 & 62.3 \\
% Llama | 1B & \squadcolor{SQ} \& \hotpotcolor{HP}; Disc. synthetic (1.0M) & 84.3 & 96.9 & 90.2 & 54.4 & 58.0 & 56.1 & 63.4 & 92.4 & 75.2 & 52.5 & 77.5 & 62.6 \\ \midrule
% Llama | 1B & \synqa (130K no dist.) & 87.1 & 88.2 & 87.6 & 67.9 & 44.8 & 54.0 & 85.3 & 82.5 & 83.9 & 72.7 & 63.8 & 68.0 \\
% Llama | 1B & \syntheticcolor{\synqa (130K)} & 89.9 & 90.7 & 90.3 & 87.9 & 63.5 & 73.7 & 88.7 & 85.4 & 87.0 & 77.2 & 65.5 & 70.9 \\
% Llama | 1B & \syntheticcolor{\synqa (550K)} & 93.6 & 94.5 & 94.0 & 88.9 & 67.3 & 76.6 & 89.8 & 87.9 & 88.9 & 77.1 & 67.1 & 71.8 \\
% Llama | 1B & \syntheticcolor{\synqa (700K)} & 95.1 & 95.5 & 95.3 & 87.8 & 69.6 & 77.6 & 93.6 & 89.1 & 91.3 & 82.0 & 68.9 & 74.9 \\
% Llama | 1B & \syntheticcolor{\synqa (1.0M)} & 96.0 & 96.2 & 96.1 & 89.6 & 69.4 & 78.2 & 93.3 & 89.1 & 91.1 & 82.3 & 68.5 & 74.8 \\
% New & \syntheticcolor{\synqa (1.0M)} & 95.7 & 96.9 & 96.3 & 89.6 & 66.4 & 76.3 & 91.8 & 91.5 & 91.6 & 80.8 & 71.3 & 75.8 \\
% Llama | 1B & \syntheticcolor{\synqa (1.0M with dialog data)} & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & \\
% \midrule
% HF-SmolLM2 | 135M & \synqa (690K) & X & X & X & X & X & X & X & X & X & X & X & X \\
% HF-SmolLM2 | 365M & \synqa (700K) & 73.9 & 74.2 & 74.1 & 85.2 & 68.7 & 76.0 & 79.6 & 76.5 & 78.0 & 68.8 & 59.8 & 64.0 \\ \midrule
% Llama | 1B & \squadcolor{SQ}; Gold (1 pass) & 98.4 & 98.4 & 98.4 & 48.7 & 20.0 & 28.4 & 92.6 & 85.8 & 89.0 & 79.9 & 64.3 & 71.2 \\
% Llama | 1B & HQ; Gold (1 pass) & 41.3 & 87.3 & 56.0 & 87.5 & 79.9 & 83.5 & 45.2 & 89.9 & 60.1 & 41.0 & 70.9 & 52.0 \\
% Llama | 1B & \squadcolor{SQ} \& \hotpotcolor{HQ}; Gold (1 pass) & 98.3 & 98.3 & 98.3 & 89.7 & 78.9 & 84.0 & 90.4 & 90.0 & 90.2 & 83.1 & 68.0 & 74.8 \\
% Llama | 1B & \squadcolor{SQ} \& \hotpotcolor{HQ}; Gold (1.0M) & 98.3 & 98.4 & 98.3 & 87.0 & 85.2 & 86.1 & 84.0 & 89.2 & 86.6 & 79.2 & 66.4 & 72.2 \\
% Llama | 1B & \syntheticcolor{\synqa (1.0M)} \& \squadcolor{SQ} \& \hotpotcolor{HQ}; Gold (1 pass) & 98.3 & 98.3 & 98.3 & 86.4 & 84.1 & 85.2 & 96.6 & 90.2 & 93.3 & 86.0 & 69.4 & 76.8 \\
% Llama | 1B & \syntheticcolor{\synqa (1.0M)} \& \squadcolor{SQ} \& \hotpotcolor{HQ}; Gold (1 pass) & 98.2 & 98.3 & 98.2 & 89.3 & 82.4 & 85.8 & 94.5 & 92.7 & 93.6 & 85.5 & 71.0 & 77.6 \\
% \midrule
% Llama | 1B & \synqa (1.0M) \& \squadcolor{SQ} \& \hotpotcolor{HQ} \& \quaccolor{Q} \& \coqacolor{CQ}; Gold (1 pass) & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & \\ \midrule
% HF-Smol | 365M & \syntheticcolor{\synqa (1.0M)} \& \squadcolor{SQ} \& \hotpotcolor{HQ}; Gold (1 pass) & 98.1 & 98.2 & 98.2 & 83.4 & 83.2 & 83.3 & 80.0 & 90.5 & 84.9 & 71.7 & 67.4 & 69.5 \\
% HF-Smol | 365M & \syntheticcolor{\synqa (1.0M)} \& \squadcolor{SQ} \& \hotpotcolor{HQ} \& \quaccolor{Q} \& \coqacolor{CQ}; Gold (1 pass) & 98.0 & 98.1 & 98.1 & 86.8 & 81.7 & 84.2 & 96.6 & 92.9 & 94.7 & 85.6 & 77.0 & 81.1 \\
% HF-Smol | 135M & \syntheticcolor{\synqa (1.0M)} \& \squadcolor{SQ} \& \hotpotcolor{HQ} \& \quaccolor{Q} \& \coqacolor{CQ}; Gold (1 pass) & 98.0 & 98.0 & 98.0 & 77.8 & 76.3 & 77.0 & 95.0 & 91.6 & 93.3 & 81.2 & 71.8 & 76.2 \\
% \bottomrule
% Llama | 1B & All; Gold & 1B   & 96.8 & 96.8 & 96.8 & 88.1 & 83.8 & 85.9 & 94.7 & 89.3 & 91.9 & 88.7 & 76.8 & 82.3 \\
% HF-SmolLM2 & All; Gold & 360M & 98.3 & 98.4 & 98.3 & 85.1 & 78.2 & 81.5 & 96.6 & 92.4 & 94.5 & 88.3 & 74.6 & 80.8 \\ \bottomrule
% \end{tabular}
% }
% \caption{Corroborative context-attribution of LMs on Squad QA, HotPot QA, Quac, and CoQA.}
% \label{table:all-datasets}
% \end{table*}