\section{Introduction}




%\begin{figure*}[h]
%\centering
%\includegraphics[width=1.0\textwidth]{img/LLM_Explainer_for_paper.pdf}

%\caption{Left: LLMs might hallucinate. This means in high-risk domain, the user would still have to read all relevant source information to judge if the LLM output is reliable. This is tedious, error prone and time consuming. Right: To better support the user, we introduce LLM Explainer, which automatically generate source attribution links. These links show for each LLM generated sentence, which source sentences are relevant. This significantly speeds up the verification process and empowers the user to retain control over workflows that include LLMs.}
%\label{fig:explainer}
%\end{figure*}

%LLMs are statistical models trained to predict the next likely word. They have no concept of correct and incorrect information. Of course, often they produce impressive results - because they have seen an impressive amount of training data and processed it well. In contrast, it is not because they can \textit{understand} the text they processed \cite{BenderETAL:21}. 

%%% GG: Too broad of an initial intro

%\textcolor{red}{TODO: add in intro or early somewhere that we follow \citet{slobodkin2024attribute}'s insight that sentence level granularity is best.}

Large Language Models (LLMs) have become ubiquitous, with Question Answering (QA) as their most common use case \cite{trippas2024users}. 
%LLMs can answer many user questions based on their stored knowledge from pre-training and instruction- and preference-tuning. However, they are often tasked to provide answers to questions based on a provided reference context that contains information presumed to be beyond the LLMs' existing knowledge \cite{rashkin2023measuring}; e.g., answering \textit{``which territories does Donald Trump want to make part of the USA?''} by using a collection of recent news articles. 
However, LLMs have a tendency to hallucinate: they generate content that is factually incorrect w.r.t.~a previously provided reference text. %To help users identify such issues, 
This poses the need for \textit{context attribution} methods that create links between the answer and different relevant parts of the (potentially large) reference text; for an illustration of the task, see Figure~\ref{fig:fig1}. 

For these reasons, reliable and efficient context attribution is instrumental in manually verifying the factuality of LLM-generated content. By conducting a user study, \citet{slobodkin2024attribute} report two important findings in this respect: (1) attribution models reduce the human workload in a fact-checking task by as much as 50\%; and (2) sentence level granularity---i.e., grounding the answers in one or more relevant sentences in the reference context---is the most efficient granularity level for manual fact-checking of LLM-generated answers.

%By conducting a user study, \citet{slobodkin2024attribute} observed that sentence level granularity---i.e., grounding the answers in one or more sentences in the reference context---is the most efficient granularity level for manual fact checking of LLM-generated answers.

%\citet{slobodkin2024attribute} observed that grounding such generated answers
%Such reference text is, for example, automatically retrieved when applying Retrieval Augmented Generation (RAG) \cite{lewis2020retrieval}, but could also come from other sources. 

%creates the need for \textit{context attribution}: grounding of generated answers in the concrete parts of the reference text, often referred to as \textit{evidence}. 
%Reliable context attribution is instrumental in reducing the manual effort of verifying the factuality of LLM generations. \citet{slobodkin2024attribute}, for example, report that attribution models reduce the human workload in fact-checking task by as much as 50\%. Therefore, we systematically explore how to provide reliable and fast attribution models.
%
%Millions of people use Large Language Models (LLMs) in their work today. Most people interact with the LLMs by asking them questions and receiving answers in return. With the LLMs' ability to generate fluent texts and solve a wide range of tasks, they have transformed both research and industry. Yet, their outputs are not always factually correct. Therefore, it is crucial for users to trace the origin of the generated text, in order to assess the reliability of the LLM's outputs. 


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{img/task_figure.pdf}
    \caption{Post-hoc context attribution: Given a question, an LLM-generated answer, and context (from human input or retrieval), the model identifies supporting sentences within the context. Our user study (\S\ref{sec:user-study}) shows that presenting these supporting sentences helps users verify LLM answers more quickly and accurately.}
    % Context attribution for question-answering: given a question, answer and context, find attributive information from the context that supports the answer. The question is provided by the user, the context can be retrieved or manually provided, and the answer is generated by an LLM.}
    \label{fig:fig1}
\end{figure}

% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{img/user_view_dialog.pdf}
%     \caption{Context attribution for multi-turn dialog question-answering.}
%     \label{fig:fig_dialog}
% \end{figure*}

% CL Original long text of related work, keeping here for reference and shortened below
%Given the task importance, several lines of work on context attribution have recently emerged, focusing on text summarization, i.e., attributing the summary sentences to the evidence in long input document(s) \cite{krishna2023longeval,ernst2024power}, attributing citations (e.g., produced as part of scientific text generation) to evidence within the cited articles \cite{gao2023enabling,Huang2024AdvancingLL} and in question answering, attributing the answers to segments of context of various granularity \cite{Phukan2024PeeringIT,CohenWang2024ContextCiteAM}. Much of the attribution work, however, operates on document or paragraph level---i.e., the task is to identify the correct documents/paragraphs from a larger collection of pre-retrieved documents/paragraphs provided in the context \cite{Buchmann2024AttributeOA,Nakano2021WebGPTBQ}---and/or produce attributions as in-line citations as part of the generated output itself, where each reference document is assigned a reference number \cite{gao2023enabling}. Such a formulation comes with prominent limitations: (1) the user still has to read the (potentially long) document(s) to verify the generation and (2) the LLM needs to learn to generate outputs in a new format, with (correct) citation numbers interleaved into the answer. In contrast, the post-hoc attribution methods perform the attribution after the LLM generates the answer. These models, however, either also attribute to coarse-grained units of text \cite{Nakano2021WebGPTBQ,Menick2022TeachingLM,Buchmann2024AttributeOA}, or, provide fine-grained attributions but their inference is computationally expensive \cite{CohenWang2024ContextCiteAM,Phukan2024PeeringIT}, hindering their adoption in practice.   

Given the task importance, recent context-attribution research spans text summarization \cite{krishna2023longeval,ernst2024power}, citation attribution \cite{gao2023enabling,Huang2024AdvancingLL}, and question answering \cite{Phukan2024PeeringIT,CohenWang2024ContextCiteAM}. However, the solutions rely on document- or paragraph-level evidence, which comes with the following limitations: (1) the user still has to read the (potentially long) document(s) to verify the generated text; and (2) the LLM needs to correctly generate the reference output alongside answering the question correctly. 
%In contrast, post-hoc attribution methods avoid limitation (2) but still suffer from limitation (1) \cite{Nakano2021WebGPTBQ,Menick2022TeachingLM,Buchmann2024AttributeOA} or are computationally expensive \cite{CohenWang2024ContextCiteAM,Phukan2024PeeringIT}. These challenges hinder the adoption of context attribution in practice. 
In contrast, the post-hoc attribution methods perform the attribution \emph{after} the LLM generates the answer. These models, however, either attribute to coarse-grained units of text \cite{Nakano2021WebGPTBQ,Menick2022TeachingLM,Buchmann2024AttributeOA}, or provide fine-grained attributions but their inference is computationally expensive \cite{CohenWang2024ContextCiteAM}, hindering their adoption in practice. 


%\gog{GG stopped here, will add the remaining part on how our work differs and what the main contributions are -- basically expansion of that part of the abstract.}


  %To address this issue, researchers have proposed context attribution methods, mostly in the context of Question Answering (QA) \cite{Ramu2024EnhancingPA}. The goal is to find supporting evidence for a generated text within a pre-defined context. %In this paper, we study the case of context attribution for QA, because people most naturally use LLMs in the QA setup: people pose queries and the LLMs answer them. 
%The QA context attribution task is to ground the answers from an LLM to a pre-defined context 
%In particular, a QA context attribution method should find alignments from the generated answer to specific chunks of text (e.g., sentences) within the provided context; see Fig.~\ref{fig:fig1} for an illustration of the task.

%Researchers view the QA context attribution problem from different perspectives. One line of work focuses on in-line context attribution, whereas LLMs are instructed to generate in-line citations in addition to the generated text \cite{gao2023enabling, Huang2024AdvancingLL}. However, such approaches sometimes provoke the LLMs to generate completely non-existing citations (i.e., they hallucinate) \cite{Liu2023EvaluatingVI}, thus making the task for verification even harder for humans, because now they need to verify the factfulness of both the generated answer and the generated citations. To address this issue, \citet{Yue2023AutomaticEO} proposed a method for post-hoc verification of in-line generated citations. 

%Because many of the currently available LLMs are opaque, researchers have proposed post-hoc attributions to the LLM generated content. \citet{CohenWang2024ContextCiteAM} proposed a method that aim to discover causality links between the generated text and the attributions. Such work, however, is not practical, because it is too slow to be used in realistic scenarios. On the other hand, methods that aim to post-hoc align answers to pre-defined context in search of supporting evidence are faster and more realistic. In this paper, we focus on such post-hoc context attribution for QA.

%While such post-hoc context attribution for QA has been extensively studied in the past \cite{Yang2018HotpotQAAD,zhangyue2023rethinking,zhang2024end}, there has been no thorough study on the use of current LLMs on solving this task. 

% GORJAN: We do not explore the balance between accuracy and granularity; but rather, we pick a level of granularity (e.g., sentence), and roll with that one.
% To make context attribution practical, we explore the balance between accuracy, granularity, and practical performance. 

%In contrast to vast majority of existing work, in this paper we focus on \textit{post-hoc} and \textit{fine-grained} (sentence-level) context attribution with LLMs. 
In this paper, we explore how LLMs can generate synthetic data for attribution fine-tuning, enabling accurate, sentence-level, and real-time efficient models. For data generation, we compare two approaches: (1) in the fairly straightforward \textit{attribution synthesis} (\synatt), we start with question-answer pairs from a reference text and prompt the LLM to identify the supporting sentences; (2) in our novel \textit{question-answer synthesis} (\synqa), we use Wikipedia sentences and prompt the LLM to generate a question-answer pair that is fully supported by these sentences. Given the generated data, we fine-tune smaller, more efficient context attribution models and compare their performance.
%(1) the fairly straightfoward \textit{attribution synthesis} given question-answer pairs (\synatt), where given some reference text, we (i) first obtain a question-answer pair and then (ii) prompt an LLM to attribute the answer to one or more sentence(s) from the reference text\footnote{In practice, we use the question-answer pairs from the in-domain training datasets; and use an ensemble of LLMs to perform the context attributions (see~\S\ref{sec:experiments-zero-shot}).};
% \gog{CHECK the COMMENT (1) the fairly straightfoward \textit{attribution synthesis} given question-answer pairs (\synatt), where given some reference text, we i) first prompt the LLM to generate a question-answer pair and then ii) prompt it to attribute the answer to one or more sentence(s) from the reference text;}
%against our novel (2) \textit{question-answer synthesis} (\synqa) where we (i) start from a set of sentences from Wikipedia, mutually linked via entities, which are meant to be the attribution sentences; we then (ii) prompt the LLMs to generate a question-answer pair for which the provided sentences represent a complete and correct attribution. In both cases, we fine-tune a context attribution model on the generated data.
% %The evaluation encompasses six context attribution datasets and considers two different real-world scenarios:

Through extensive evaluation encompassing six datasets and two real-world scenarios (attribution for single-turn questions: i.e., a single question and single answer, and for dialogue questions: i.e., as part of a conversation), we demonstrate that models trained on synthetic data generated by \synqa: \circled{1} Outperform zero-shot LLMs that are orders of magnitude larger, while maintaining real-time inference capabilities (\S\ref{sec:experiments-zero-shot}); \circled{2} Achieve competitive performance on in-domain tasks and superior generalization to out-of-domain datasets compared to models trained on gold data (\S\ref{sec:experiments-gold}); \circled{3} Successfully handle dialogue-based attribution without requiring in-domain training data (\S\ref{sec:experiments-dialog}); \circled{4} Show consistent performance improvements as synthetic training data increases (\S\ref{sec:scalling-trends}); \circled{5} Significantly improve users' speed and accuracy in verifying LLM-generated answers (\S\ref{sec:user-study}). %These results highlight the viability of scalable, data-efficient context attribution techniques, thus paving the way for more interpretable and trustworthy AI systems. 

These findings suggest that \synqa reduces dependence on large human-labeled datasets while improving context attribution robustness. Our user study further validates the practical utility of fine-tuned small models in real-world question-answering applications, demonstrating their effectiveness in diverse settings. Overall, these results highlight the viability of scalable, data-efficient context attribution techniques, paving the way for more interpretable and trustworthy AI systems.


%These findings suggest that \synqa reduces dependence on large human-labeled datasets, while improving the context attribution robustness. Our user study validates the practical utility of fine-tuned small models in real-world question-answering applications. 

%The evaluation encompasses six context attribution datasets and considers two different real-world scenarios: (a) attribution for questions asked in-\textit{isolation} (i.e., a single question and single answer, see Figure~\ref{fig:fig_dialog})) and (b) for questions asked in-\textit{dialog} (i.e., as part of a conversation, see Figure~\ref{fig:fig_dialog}). We compare the two data generation methods fine-tuned on smaller models (1B parameters) against baseline based on sentence encoders and zero-shot attribution inference with big LLMs (up to 70B parameters; individual and ensembled). We show that \synqa trained models outperform the other models, including \synatt models, across datasets, types of data (in-domain or out-of-domain), and attribution settings (in-\textit{isolation} and in-\textit{dialogue}).

%against fine-tuning small LLMs (1B parameters) on data generated with \synqa and \synatt. Our results render attribution data synthesis with our novel \synqa approach particularly effective: fine-tuned on \synqa data, the small LM outperforms not only zero-shot inference with two orders of magnitude larger LLMs, but also its counterparts trained on \synatt data as well as fairly large gold context attribution data. Moreover, we show that merging gold and \synqa attribution data leads to further gains. 
% Finally, our
% small 
% user study shows that humans are faster and can more accurately assess the LLM-generated answer with our \synqa model. \synqa provides highly accurate, sentence-level context attribution in real-time. It is therefore a practical tool that helps users evaluate and trust LLM-generated content.


% \textcolor{red}{TODO: When doing contributions, forward reference to the experimental sections}

% \textcolor{red}{TODO: Make contributions stand out: \circled{1}; \circled{2}}

%we carry out a small user study which confirms that humans are faster and better at the findings of the benchmark evaluation, validating the usefulness of LMs fine-tuned on synthetic attribution data.            

%To address this gap, we investigate the capabilities of current LLMs to perform the QA context attribution task. In particular, we study the problem from multiple aspects: (1) zero-shot with existing LLMs; (2) fine-tuning smaller LLMs on gold data; (3) fine-tuning smaller LLMs with discriminative and generative synthetic generation strategies; (4) fine-tuning on different data sizes; (5) a user study that verifies that our trained models are helpful for users. 

%We conducted experimental study on a data compiled from several different QA datasets. We found that using zero-shot with small LMs are not performing well, though they are fast and cheap to use. On the other hand, large LMs have decent performance, but their use is unrealistic in many scenarios due to their low speed. Regarding the use of gold data, we found that fine-tuning small LMs on gold data achieves good performance and realistic speed, but they do not perform well on out-of-domain samples. Therefore, using gold-data is unrealistic, because one cannot have control over what type of samples the model will encounter. To extend the usability of small LMs to out-of-domain samples, we turned to synthetic data generation. We examined two strategies: discriminative and generative synthetic data generation. We found that these strategies indeed help with the performance for out-of-domain samples. Moreover, these small models were able to outperform even the large LMs. Finally, we performed a user study to see if such models would be helpful to humans. We found that our models help humans in verifying the answers from questions and improves the speed of their performance.



%Essentially, the task is defined as follows: given a generated text $T_g = \lbrace s_1^g, s_2^g, \ldots , s_n^g \rbrace $---where $s_i$ is a sentence---and attribution context $T_c = \lbrace s_1^c, s_2^c, \ldots, s_n^c \rbrace$, find alignments $s_i^c \rightarrow A$, where $A$ is a subset of $T_g$. In other words, each sentence from the set of generated sentences ($T_g$) should have at least one alignment sentence that indicates origin (see Figure 1).



%There are different variants of the context attribution task. 




%it is important for people to be able to quickly see where the generated information is coming from, so that they can verify if the LLMs output is correct. 

%There is no easy way to prevent hallucinations produced by LLMs -- it is an inherent shortcoming of the transformer architecture that powers LLMs. Therefore, a crucial question is: \textit{What can we do to mitigate the issues of hallucinations and unlock the value of LLMs for use cases where hallucinations are especially detrimental?}

%To address this question, we introduce \textit{LLM Explainer}, which provides users with the power to efficiently verify whether LLM output is correct or not. Figure \ref{fig:explainer} provides an example of the value that LLM Explainer unlocks: Assume we would like to use a LLM in a high-risk domain to generate summaries, such as a financial advisor who might want to speed up report writing. She cannot simply ask a LLM to write a summary for her because any hallucinations in the report might lead to financial losses or fines. To verify the reports correctness, the advisor would still have to spend a lot of time to read all source document to verify the correctness of the LLM-generated summary. LLM Explainer can support the advisor and enables a faster verification process by automatically creating links between a sentence in the summary and the original source sentences that are relevant. This empowers the user to safely use a LLM in a high-risk domain and gives the user a tool to have final control over the output.

%To provide the links that the LLM Explainer generates, we study source attribution. \cl{TODO@Kiril: can you add a paragraph here that is based on the related work and highlights the shortcomings? }


%First, we setup a series of datasets where we can measure how well LLMs can align text to a set of source references. Second, we create a series of LLM models to perform source attribution. Here crucially we focus on distilled models to create smaller LLMs that are both cheaper and faster to run, which is important to have our LLM Explainer deliver insights in real time. Third, we provide an evaluation framework and test the different models on the various datasets with a detailed ablation study. Fourth, we describe a human evaluation experiment that showcases how LLM Explainer supports users in real world settings.




