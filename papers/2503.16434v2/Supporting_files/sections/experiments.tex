\section{Experiments}

\subsection{Research Questions}
We aim to answer the following research questions through our experiments.
\begin{itemize}[noitemsep,topsep=0pt, parsep=0pt,partopsep=0pt, leftmargin=14pt]
    \item \textbf{RQ0:} How accurate is \name\ at solving the problems of interest?
    \item \textbf{RQ1:} How effective is the proposed interactive system in improving users' understanding of concepts compared to text-only language model systems? 
    \item \textbf{RQ2:} Does providing step-by-step guidance with hints improve learning outcomes compared to systems that provide final answers directly?
\end{itemize}

\textbf{RQ0} will confirm that \name\ successfully solves the prompted questions with its interaction loops, and how it compares to existing language-only and non-interactive systems~\citep{hu2024visual,hurst2024gpt}. \textbf{RQ1} enables us to assess whether our proposed interactive system improves users' conceptual understanding and overcomes the limitations of traditional text-based systems. For \textbf{RQ2}, systems that deliver the final answer directly can negatively impact the cognitive processes of the user by failing to motivate critical thinking and reasoning. Providing step-by-step guidance encourages active participation to support users' long-term knowledge retention and skill development.  


\begin{table*}[htb!]
\centering

\begin{tabular}{p{0.2\textwidth}p{0.7\textwidth}}
\hline
\textbf{Topic} & \textbf{Feedback} \\
\hline
Visualization quality & \textit{``The graphs are good sanity-checks for my workings.''} \newline \textit{``The visual illustrations help a lot. The intuitive drawing makes the interaction feel more natural.''} \newline \textit{``The visualizations were also very helpful in gaining a more conceptual understanding outside of just equations.''} \\
\hline
Interactive experience & \textit{``It was nice that it didn't give me the final answer right away, and instead gave hints/prompts to try.''} \newline \textit{``It showed me how to approach the problem step by step.''} \newline \textit{``I like that it guides you through the problem-solving approach without jumping straight to the answer, like ChatGPT.''} \\
\hline
Learning experience & \textit{``I think the graph was particularly helpful for solving the integral, especially when the integral was one without an antiderivative. The visualization made the math feel more intuitive/meaningful.''} \newline \textit{``The diagrams provided were very nice, despite I didn't ask for them.''} \newline \textit{``The visual illustrations help a lot. The intuitive drawing makes the interaction feel more natural''} \\
\hline
\end{tabular}
\vspace{1mm}
\caption{Qualitative feedback from users based on three aspects: visualization quality, interactive experience, and learning experience. Users noted that visualizations helped in understanding concepts, interactivity guided problem-solving effectively, and the learning experience felt more intuitive due to the graphical and step-by-step approach provided.}
\vspace{-4mm}
\label{table:qualitative_feedback}
\end{table*}



\subsection{Performance Comparisons}



We first run experiments evaluating the performance of \name\ on directly solving the problem without giving hints to evaluate whether the system can solve the problem itself. We observe in Table \ref{table:experiment_comparison} that \name\ consistently outperforms Visual Sketchpad on IsoBench. Visual Sketchpad's prompts encourage the LMM to use its own visual reasoning to solve problems rather than use code execution to help solve the problems. \name\ on the other hand does not specify the same constraint to not use code execution, instead allowing its use to improve problem-solving accuracy. This highlights the effectiveness of combining both code execution for calculations and visual reasoning in solving complex problems. We remove the constraint of using visual reasoning rather than code execution as we want to make the system achieve as high accuracy as possible to reduce errors that may confuse the student.
We use the original IsoBench prompts \cite{fu2024isobench}, appending a prompt footer (see \ref{prompt_footer}) that instructs the model to explain its answer and format the output accordingly. Additionally, we employ a system prompt (see \ref{subsec:prompt_solve_problem}) directing \name\ to solve the question directly.


\subsection{User Studies}



\begin{table}[htb!]
\centering
\vspace{2mm}
\begin{tabular}{lr}
\toprule
Metric &  Average Score (out of 5) \\
\midrule
Interface Intuitiveness &           4.71 \\
System Responsiveness   &           4.43 \\
Clarity of Visuals      &           4.57 \\
Accuracy of Visuals     &           4.71 \\
Effectiveness of Hints  &           4.57 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\caption{Average user ratings for key aspects of the interactive system, including metrics on usability, sufficiency, and quality. The table highlights the system's strong performance in providing a pleasant interactive and visual learning experience.}
\vspace{-4mm}
\label{table:quantitative_feedback}
\end{table}

\subsubsection{Participant Information.}
For this user study, we recruited university underclassmen undergraduate students participants, who have basic knowledge in certain mathematics topics such as algebra or geometry but were not experts in all domains. We ran the initial study on 7 users with no monetary compensation. The study data was stored in a secure cloud-operated space with limited access. For further user studies that are to be conducted, the approval request has been submitted to the Institutional Review Board.

Participants were introduced to the system with a brief tutorial. This included an explanation of system features, interaction techniques, and a demonstration. Next, participants interacted with both \name\ and ChatGPT to solve a set of assigned mathematical problems, alternating between the systems. After completing the tasks, participants were asked to complete a questionnaire, reflecting on their experience using our system. This questionnaire gathered participants' background knowledge, ratings on various aspects of their experience, and qualitative feedback.

\subsubsection{Dataset and Metrics.}
The primary goal of the user study was to assess the system's ability to enhance the learning experience by fostering active engagement and improving conceptual understanding with visual artifacts. \cite{visualizationcrucial2017} To ensure that our findings could translate to practical use cases, we tested with mathematical problems that could efficiently benefit from visual artifacts. Questions added to the users' question bank were sourced from previous exam questions of the Scholastic Aptitude Test (SAT), the IsoBench dataset \cite{fu2024isobench}, a previous study on visualization for math \cite{rosken2006picture} and Geometry3k \cite{lu2021inter}. Visualization plays an important role when learning mathematics, and a good example is solving an integral problem. Visual interpretations of the function and shading parts of the graph can significantly benefit the students' learning experience, which has been demonstrated in previous studies \cite{rosken2006picture}. By selecting both practical mathematical topics that range across various difficult levels, we ensured the system catered to a broad range of learners. The system's potential for versatile use across subjects and learning environments allows our results to demonstrate foundational interaction between AI and humans for education.

Since one of the core features of \name\ is its integration of visual and textual feedback, a key consideration for our user study was to analyze the quality of the system's multimodal interaction. To do so, we measured how students engaged with these modalities, focusing on the effectiveness of visual explanations in complementing or enhancing textual guidance. Another key consideration was the evaluation of our systemâ€™s iterative learning process. To evaluate this, students were encouraged to iterate through problems to explore how the feedback influenced their learning experience. 


\subsubsection{Qualitative Results.}

We summarize the key qualitative feedback we received from our user studies in Table~\ref{table:qualitative_feedback}. Participants appreciated the tool's personalized visualizations, frequently mentioning how they easily grasped complex concepts with the visualization feature. The tool's feature of using a sketchpad to directly input handwritten work resonated well with the participants, and this visual, interactive approach allowed the students to develop problem-solving strategies while gaining the required insights into the underlying concepts for future applications. A participant noted that ``the visualizations were also very helpful in gaining a more conceptual understanding outside of just equations''. Participants also specifically noted the system's effectiveness in guiding them through the problem-solving process without immediately providing the final answer, helping them truly understand the materials covered in the problem, one noting that ``I like that it guides you through the problem-solving approach without jumping straight to the answer, like ChatGPT''. Overall, the feedback we received highlighted the system's ability to enhance the learning experience by fostering human-AI collaboration in learning environments. 

\subsubsection{Quantitative Results.}

The quantitative evaluation of \name\ was conducted through participant feedback on various aspects of the tool. Table \ref{table:quantitative_feedback} summarizes the average scores across various key metrics collected from user responses. The metric includes interface intuitiveness, system responsiveness, and accuracy of visuals. These results indicate that the interface was overall well received. Specifically, the system's interface intuitiveness and the accuracy of visual aids received the highest average scores of 4.71/5, indicating that participants found the tool easy to navigate and appreciated the precision of its generated visuals. Similarly, the clarity of visuals and the effectiveness of hints were rated highly, both scoring 4.57/5. This highlights that the visuals were designed in a manner that helps the problem-solving thought process and teaches the true concepts behind the problems. The system's responsiveness scored slightly lower at 4.43/5, while still favorable, suggesting that the tool could have efficiency improvements to reduce the wait time for hints. Overall, these quantitative insights provide a strong foundation for understanding user perceptions and areas that may benefit from enhancements. The consistently high ratings across most metrics reinforce the tool's potential for user testing with a broader range of users and various educational applications. 