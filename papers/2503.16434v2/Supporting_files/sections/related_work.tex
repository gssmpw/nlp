
\section{Related Work}
\begin{figure*}[htb!]
\centering
\vspace{-0mm}
\includegraphics[width=0.9\textwidth]{Supporting_files/figures/overalldiagram.png}
\vspace{-4mm}
\caption{This flowchart demonstrates the interaction between user inputs, large multimodal models, and visualization components. The system processes user inputs, generates textual hints, interprets and executes code, and provides visualizations to guide problem-solving. The user can also annotate visualizations using the interactive whiteboard component.}
\label{fig:architecture_diagram}
\vspace{-0mm}
\Description{This image shows a flowchart of the system flow, along with a screenshot example of a whiteboard interaction.}
\end{figure*}

\textbf{Benefits of visualizations in learning.}
Visualizations play a crucial role in mathematics education, helping students understand abstract concepts, solve problems, and improve memory retention \cite{arcavi2003role, schoenherr2024characterizing}, especially in visually oriented topics such as geometry and calculus \cite{zhang2023dynamic}. Cognitive science research highlights that visualization can also reduce cognitive load by presenting data in a more accessible format \cite{sweller1994cognitive} such as diagrams and images. Tools like graphing calculators and interactive whiteboards \cite{mata2016interactive} have further demonstrated the benefits of incorporating visual elements into learning and have worked towards bridging the gap between theoretical constructs and practical understanding.

\textbf{Multimodal interaction and visualizations.} With the advancement of LMMs, researchers have explored the use of code generation to solve visual reasoning tasks. VisProg \cite{gupta2023visual} generates pseudocode interpreted as a `visual program', enabling modular, compositional visual reasoning using in-context learning. ViperGPT \cite{suris2023vipergpt} uses a similar approach but generates Python code instead of a domain-specific language, making it more flexible.
Visual Sketchpad \cite{hu2024visual} creates programs that generate intermediate diagrams like drawing auxiliary lines in geometry which are used in a visual chain of thought to enhance performance on a range of mathematical and computer vision tasks. Recent work has also focused on improving models' reasoning capabilities. Chain-of-Thought (CoT) \cite{wei2022chain}, enhances large language models' reasoning abilities by guiding them to generate intermediate reasoning steps before arriving at a final answer. This concept has been generalized to incorporate vision with visual chain-of-thought reasoning, which involves step-by-step reasoning across modalities \cite{rose2024visual, shao2024visual}. However, these approaches are designed to facilitate LMM visual reasoning rather than interacting with and helping humans in education. 
%Researchers have explored the use of LMMs to make multimodal interactions between AI and humans more natural \cite{fu2024vita}. Recent work has also focused on improving models' reasoning capabilities. Chain-of-Thought (CoT) \cite{wei2022chain}, enhances large language models' reasoning abilities by guiding them to generate intermediate reasoning steps before arriving at a final answer. This concept has been generalized to incorporate vision with visual chain-of-thought reasoning, which involves step-by-step reasoning across modalities \cite{rose2024visual, shao2024visual}. Visual Sketchpad \cite{hu2024visual} incorporates a visual chain of thought to enhance multimodal reasoning with diagram creation via code generation. However, Visual Sketchpad cannot interact or collaborate with students and instead directly solves problems, which can negatively impact long-term understanding. Tangible user interfaces that allow interaction between humans and computers can enhance learning experiences \cite{Chettaoui2020tangible}, and \name\ further explores interactivity as part of multimodal reasoning. 


%Our framework focuses explicitly on creating interpretative and multi-step visual hints as well as facilitating text and visual interaction to enable multimodal human-AI collaboration.

\textbf{AI for education.} Recent advances in NLP and LMMs have demonstrated significant potential in automating education \cite{graesser2004autotutor, khan2023harnessing}, with the ability to help students in a wide range of subjects including math \cite{shridhar2022automatic, prihar2023comparing}, programming \cite{zhang2024pydex, balse2023evaluating, kazemitabaar2023novices} and language \cite{yancey2023rating, xiao2024automation}, and in assisting humans with tutoring \cite{wang2024tutor}. However, while LMMs can serve as effective educational tools, excessive reliance on them for direct answer generation has been shown to negatively impact student learning outcomes \cite{bastani2024generative, nie2024gpt}. This issue can be mitigated by moderating LMM responses to provide hints rather than complete answers \cite{krupp_moderating}, thereby encouraging active learning. Furthermore, incorporating both textual and visual representations of student work, rather than relying on text alone, has been found to enable more effective feedback mechanisms \cite{li2024automated}.

