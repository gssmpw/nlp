\section{\name}


\name\ enhances problem-solving and tutoring by incorporating a multimodal interaction loop, allowing humans to interact with LMMs using both text and images.
%Unlike traditional intelligent tutoring systems that rely solely on text-based explanations, \name\ enables LMMs to generate step-by-step visual hints which the human can then annotate and then send back to the LMM in an iterative manner, fostering a more intuitive and collaborative learning experience.
%Given a multimodal query that consists of textual and visual elements, \name\ engages in an iterative interaction loop to guide students through problem-solving.
We show the overall interactive usage in Figure~\ref{fig:geometry_with_code} and its detailed architecture in Figure~\ref{fig:architecture_diagram}. The key components of \name\ include:
\begin{enumerate}[noitemsep,topsep=0pt, parsep=0pt,partopsep=0pt, leftmargin=14pt]
    \item \textbf{Problem analysis}: The system evaluates the problem to determine whether a visualization would be beneficial.
    \item \textbf{Visual generation}: If a visualization is deemed helpful, the system generates a Python program to create an appropriate visualization.
    \item \textbf{Hint generation}: Then a textual hint is generated which references the visualization if one was created.
    \item \textbf{Interactive whiteboard}: The hint and visualization appear in the chatbot interface as usual. However, if a visualization is generated, it is also displayed on the userâ€™s interactive whiteboard, allowing direct annotation and interaction. The annotated visualization along with additional text or images can be sent directly from the whiteboard back to the LMM which processes the new input and continues the iterative reasoning cycle.
\end{enumerate}

Together, this loop fosters continuous human-AI collaboration, where the model provides interactive assistance in both language and vision to enhance concept understanding. We now explain each of these four steps in detail.


\begin{table*}[htb!]
\vspace{2mm}
\centering
\begin{tabular}{lccccccc}
\toprule
Model & Maxflow & Isomorphism & Connectivity & Convexity & Parity \\
\midrule
GPT-4o~\citep{hurst2024gpt} & 25.0 & 50.8 & 96.1 & 87.2 & 84.4 \\
Visual Sketchpad~\citep{hu2024visual} & 66.3 & 65.3 & 98.4 & 94.9 & 94.7 \\
\name\ (ours) & 100.0 & 75.0 & 99.2 & 96.5 & 95.6 \\
Improvement & \textbf{+33.7} & \textbf{+9.7} & \textbf{+0.8} & \textbf{+1.6} & \textbf{+0.9} \\
\bottomrule
\end{tabular}
\vspace{1mm}
\caption{Accuracy scores on graph algorithms and mathematical functions. \name\ outperforms Visual Sketchpad and other large multimodal model baselines by using code execution for calculations to solve tasks.}
\vspace{-4mm}
\label{table:experiment_comparison}
\end{table*}

\textbf{1) Problem Analysis.} We prompt a pre-trained GPT-4o model~\citep{hurst2024gpt} regarding when visualizations may be helpful based on the problem and user query (see examples in Appendix \ref{subsec:system_prompt_hint_viz}).

\textbf{2) Visualization Generation.} To draw accurate diagrams, we generate Python programs using the LMM which, when executed, render the visualizations, similar to recent works such as Visual Sketchpad \cite{hu2024visual}, VisProg \cite{gupta2023visual} and ViperGPT \cite{suris2023vipergpt}. For the LMM, we use GPT-4o \cite{hurst2024gpt} for multimodal reasoning and use the OpenAI Code Interpreter tool to run and execute the code. To draw the visualizations, we generate Python code that uses common Python libraries, primarily \texttt{matplotlib}, for plotting. The Code Interpreter then executes the code, and if it fails to run, the LMM will iterate on the code by attempting to run different code until it successfully generates a runnable program. Once the program is successfully executed, the resulting image file is displayed to the user.

\textbf{3) Hint Generation.} We prompt a GPT-4o model to give subtle hints rather than revealing the answer and to generate visualizations for the hint using code. To enable good hints and appropriate visual hints, we collected human demonstrations of good hints interleaved with images in an example conversation between a student and a tutor, using several samples of math and coding problems. We then fine-tuned the model with this data (prompt found in Appendix \ref{subsec:system_prompt_hint_viz}). In subsequent iterations, the model uses the conversation history, a screenshot of the user's interactive whiteboard, and the user's query to generate hints. Finally, we also provide the model access to the OpenAI Code Interpreter to perform mathematical calculations by executing code to reduce calculation errors, similar to \citet{chen2023program}. For example, the model will perform calculations such as numerical integration for calculus questions, or arithmetic for systems of equations to provide more accurate hints and check student answers (see examples in Appendix \ref{sec:code_exec}).

\textbf{4) Interactive Whiteboard.} The interactive whiteboard component lets users intuitively collaborate with the LMM using textual and visual information. Users can take snapshots of their whiteboard to ask questions with their sketches, these snapshots are then sent to the chatbot server. The LMM would automatically be prompted with the screenshot and additional text or images provided by the user. Once a new visualization is generated, it is sent to the interactive whiteboard server, which automatically presents it to the user to continue drawing and interacting with the system.


\subsection{User Interface}

% \begin{figure}[hb!]
%     \centering
%     \fbox{\includegraphics[width=0.4\textwidth]{Supporting_files/figures/chatbot_screenshot.png}}
%     \caption{Screenshot of the chatbot interface for \name\ . The user can view the generated visual hints and interact with \name\ by typing messages and uploading images.}
%     \label{fig:chatbot_screenshot}
% \end{figure}\hspace{5mm}
% \begin{figure}[hb!]
%     \centering
%     \fbox{\includegraphics[width=0.4\textwidth]{Supporting_files/figures/interactive_whiteboard_screenshot.png}}
%     \caption{Screenshot of the interactive whiteboard component of \name. The user can annotate on the visualization generated by \name\ which was done using an iPad and Apple Pencil during user studies.}
%     \label{fig:interactive_whiteboard_screenshot}
% \end{figure}

\begin{figure}[hb!]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{Supporting_files/figures/chatbot_screenshot.png}}
        \caption{Screenshot of the chatbot interface for \name. The user can view the generated visual hints and interact with \name\ by typing messages and uploading images.}
        \label{fig:chatbot_screenshot}
        \Description{This image is a screenshot of an example usage of the chatbot portion of \name\.}
    \end{minipage}
    \hspace{5mm}
    \begin{minipage}{0.4\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{Supporting_files/figures/interactive_whiteboard_screenshot.png}}
        \caption{Screenshot of the interactive whiteboard component of \name. The user can annotate on the visualization generated by \name\ which was done using an iPad and Apple Pencil during user studies.}
        \label{fig:interactive_whiteboard_screenshot}
        \Description{This image is a screenshot of an example usage of the whiteboard portion of \name\.}
    \end{minipage}
\end{figure}


Users primarily interact with \name\ through the chatbot interface shown in Figure \ref{fig:chatbot_screenshot} and the interactive whiteboard interface shown in Figure \ref{fig:interactive_whiteboard_screenshot}.

\textbf{Chatbot Interface:} When the user wants to type a message or upload an image they can do so through the chatbot interface. \name\ will then respond with a textual message along with an optional visualization. The user can also see the code used to generate visualizations or for mathematical calculations in the dropdown menu of the `Used code\_interpreter` message sent by \name\ after the visualization/calculation.

\textbf{Interactive Whiteboard Interface:} When the user wants to draw, write math or annotate on the diagram given by \name\ , they can do so on the interactive whiteboard. A natural way to do this is by using an iPad and Apple Pencil. They can then share the working on the interactive whiteboard with \name\ using the `Send Screenshot` button. Visualizations generated by \name\ will automatically appear as a new page on the interactive whiteboard and users can toggle between pages to access their working at various stages of solving different problems.