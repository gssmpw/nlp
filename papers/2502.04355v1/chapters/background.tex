\section{Background and Motivation}

\subsection{Motivation}
Despite their remarkable capabilities, the effectiveness of LLMs in tackling complex, real-world coding challenges remains inadequately explored. Traditional benchmarks often fall short in assessing the nuanced reasoning, adaptability, and efficiency required for high-stakes programming tasks.

The International Collegiate Programming Contest (ICPC), the oldest, largest, and most prestigious programming contest in the world, presents a unique and rigorous environment for evaluating LLM performance\cite{Chen2021Evaluating}. ICPC-style problems are characterized by their intricate constraints, diverse problem categories, and emphasis on algorithmic efficiency and edge-case handling. These attributes make them ideal for benchmarking the reasoning capabilities, accuracy, and resource utilization of LLMs in a controlled yet challenging setting \cite{b1,b2}.

Moreover, the increasing reliance on LLMs for automated code generation in software engineering necessitates a deeper understanding of their strengths and limitations. Evaluating LLMs against competition-level programming problems not only highlights their problem-solving prowess but also identifies critical areas for improvement, such as generalization to unseen problems and efficient resource usage \cite{b3,b4}. This study aims to bridge the existing research gaps by providing a comprehensive assessment of state-of-the-art LLMs using a curated set of ICPC World Finals problems.

\subsection{Background}
To thoroughly evaluate the performance of LLMs on ICPC-style problems, we selected a diverse range of models representing various architectures, training methodologies, and optimization strategies. The models under consideration include GPT-4o, Mistral Large, Llama-3.1-405B, o1-mini, and o1-preview. Each model was chosen for its distinct characteristics and potential applicability to structured problem-solving tasks.

\subsubsection{GPT-4o}
GPT-4o \cite{b27,b11,b26} is a general-purpose language model renowned for its high accuracy in code generation and reasoning tasks. Leveraging its extensive training on diverse datasets, GPT-4o\cite{Extending} demonstrates strong capabilities in understanding complex problem statements and generating syntactically correct code. However, its performance may be less optimized for structured problem-solving compared to models specifically fine-tuned for reasoning and iterative refinement.

\subsubsection{Mistral Large}
Mistral Large specializes in handling domain-specific tasks with a focus on efficient resource utilization. Designed to balance performance with computational efficiency, Mistral Large offers insights into the trade-offs between model complexity and practical deployment in resource-constrained environments. Its performance on ICPC problems provides valuable data on how domain specialization impacts problem-solving efficacy \cite{b13}.

\subsubsection{Llama-3.1-405B}
Llama-3.1-405B \cite{b27,b12} is a computationally efficient model tailored for general-purpose use. Serving as a benchmark for lightweight LLMs in competitive programming, Llama-3.1-405B facilitates comparisons regarding scalability and efficiency without significant compromises in accuracy. Its performance metrics shed light on the feasibility of deploying smaller models in high-stakes programming scenarios. It is also particularly interesting as the best-performing open source model.

\subsubsection{OpenAI o1 Family}
The o1-mini and o1-preview models \cite{b4,b10} from the OpenAI o1 family are specifically fine-tuned for chain-of-thought (CoT) reasoning and iterative refinement processes. These models are engineered to excel in structured, multi-step problem-solving tasks, making them highly suitable for ICPC-style evaluations. Their design emphasizes enhanced reasoning pathways and reduced hallucination rates, enabling more consistent and accurate code generation under complex constraints.

By systematically applying each model to this diverse set of problems, the study evaluates not only the accuracy and robustness of the solutions but also the computational efficiency and adaptability of the models in handling unseen and complex programming tasks. This rigorous evaluation framework provides a holistic view of the current capabilities of LLMs in competitive programming contexts, informing future developments in model design and training methodologies.
