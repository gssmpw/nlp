\section{Data Analysis}
The performance of large language models (LLMs) across various ICPC-style problems was analyzed using key metrics such as accuracy, verdict distribution, and resource utilization. This analysis highlights the strengths and limitations of each model, providing actionable recommendations for enhancing LLMs in technical problem-solving.

\subsection{Accuracy Trends Across Years}
Accuracy trends across different years demonstrate the consistency of LLMs in solving ICPC problems, encompassing both past (seen data) and 2024 (unseen data) problems. As depicted in Figure~\ref{fig:accuracy_trends}, the o1-mini and o1-preview models consistently outperform other models, particularly on unseen 2024 problems. The heatmap in Figure~\ref{fig:accuracy_heatmap} further illustrates this trend, showing that o1-mini maintains a significant advantage in generalization, achieving a 15.4\% accuracy on unseen data compared to 8.5\% on seen data.

These trends align with findings in \cite{b1}, where ICPC problems are identified as ideal benchmarks for evaluating LLMs' reasoning and adaptability. Models such as GPT-4o, Mistral Large, and Llama-3.1-405B exhibited challenges with unseen data, reflecting their reliance on training data overlap \cite{b27}.

\subsection{Verdict Distribution}
Analyzing the types of errors made by LLMs reveals stark contrasts between models. As shown in Figures~\ref{fig:verdict_distribution_o1} to \ref{fig:verdict_distribution_llama}, the o1-mini and o1-preview models had the highest proportion of "Accepted" (AC) verdicts, with "Wrong Answer" (WA) being the most common non-AC verdict. In contrast, other models displayed a higher prevalence of error states like "Time Limit Exceeded" (TLE) and "Compile Error" (CE), emphasizing their struggles with problem constraints (Figure~\ref{fig:verdict_distribution_other}).

\subsection{Performance on Seen vs. Unseen Problems}
Figure~\ref{fig:performance_seen_unseen} compares the performance of LLMs on seen (past years) and unseen (2024) problems. The o1-mini model demonstrates the least drop in accuracy when transitioning to unseen data, highlighting its robust reasoning capabilities and effective training methodology \cite{b3,b4}. In contrast, other models exhibit significant performance degradation, consistent with findings in \cite{b2}.

This supports the argument that ICPC problems are effective for testing the generalization abilities of LLMs, as their structure closely mirrors real-world programming challenges \cite{b1,b27}.

\subsection{Resource Utilization}
Resource efficiency is equally important in evaluating LLMs. Table~\ref{tab:resource_utilization} summarizes the average runtime and memory usage for each model. The o1-mini and o1-preview models demonstrated superior computational efficiency, aligning with \cite{b4}'s emphasis on optimized reasoning paths. Other models, such as GPT-4o, showed higher variability in runtime and memory usage, reflecting a lack of optimization for resource-constrained environments.

\begin{table}[htbp]
    \caption{Resource Utilization of LLMs}
    \centering
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Model} & \textbf{Average Runtime (ms)} & \textbf{Average Memory Usage (MB)} \\ \midrule
        o1-mini        & 150                          & 512                                \\
        o1-preview     & 175                          & 530                                \\
        GPT-4o         & 300                          & 1024                               \\
        Mistral Large  & 250                          & 800                                \\
        Llama-3.1-405B  & 280                          & 950                                \\ \bottomrule
    \end{tabular}
    \label{tab:resource_utilization}
\end{table}

\subsection{Implications for Future Research}
The analysis underscores several key areas for future research:
\begin{enumerate}
    \item \textbf{Robust Training Methods}: The exceptional performance of o1-mini validates the efficacy of chain-of-thought reasoning and fine-tuned iterative refinement \cite{b4}. Future work should explore further enhancements to these training methodologies to bolster model robustness and generalization.
    \item \textbf{Contamination-Free Benchmarks}: The significant performance gap between seen and unseen data highlights the necessity for uncontaminated benchmarks to fairly assess generalization \cite{b2}. Developing and curating such datasets will be crucial for accurate model evaluation.
    \item \textbf{Comprehensive Error Analysis}: A deeper understanding of verdict distributions can guide the development of models better suited for handling algorithmic complexity and edge cases. Incorporating comprehensive error analysis into the evaluation process can lead to more reliable and efficient LLMs.
\end{enumerate}

This comprehensive analysis demonstrates that o1 models consistently outperform others in accuracy, robustness, and resource efficiency. By leveraging ICPC problems as benchmarks, this study provides a foundation for enhancing LLMs for complex technical problem-solving tasks.
