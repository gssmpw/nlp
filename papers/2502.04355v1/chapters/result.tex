\section{Experimental Evaluation}
We use Selenium\cite{b21} for the automated downloading of PDFs from the website. For data pre-processing, we utilized PyPDF2\cite{b22} to extract text and re (regular expressions)\cite{b23} for cleaning and formatting the extracted problem statements. We evaluate five large language models (LLMs): GPT-4o \cite{b27}, Mistral Large \cite{b11}, Llama-3.1-405B \cite{b10}, and both o1-mini and o1-preview \cite{b4}. Solutions\cite{b25} generated using their respective APIs \cite{b9} are submitted to Codeforces Gym ICPC contests \cite{b8}. To evaluate \Name{}, we consider four research questions:\\
\textbf{RQ1:} How do LLMs perform on new ICPC problems
compared to those potentially seen during training?\\
\textbf{RQ2:} What are the trends in accuracy across different
categories?\\
\textbf{RQ3:} What factors contribute to variability in LLM performance?\\
\textbf{RQ4:} How are verdicts distributed across various LLMs
for the given problem set?

\subsection{Evaluation Metrics}
The performance of each model was assessed using the following metrics:\\
\textbf{Accuracy (pass@1)}: The percentage of problems solved correctly on the first attempt served as the primary metric of success \cite{b1,b2}.\\
\textbf{Error Analysis}: Errors were categorized based on verdict types to identify systematic failures in each model's reasoning or implementation \cite{b27}.\\
\textbf{Resource Utilization}: Average runtime and memory usage were measured by Codeforces to assess computational efficiency \cite{b4}.\\
\textbf{Temporal Comparison}: Models were tested on two subsets of data:
    \begin{itemize}
        \item \textbf{Past ICPC Problems}: Problems likely included in the models’ training data.
        \item \textbf{Unseen 2024 Problems}: Problems released after the models' training cutoff dates, ensuring evaluation on novel data \cite{b2,b27}.
    \end{itemize}

\subsection{Results \& Analysis}

Evaluating large language models (LLMs) on ICPC problems reveals notable differences in accuracy, verdict distribution, and resource utilization across models. These findings offer detailed insights into our research questions, highlighting each model's strengths and limitations.\\
\textbf{RQ1: LLMs Performance on Unseen ICPC Problems:}
Figure~\ref{fig:accuracy_heatmap} presents a heatmap showing the accuracy trends of various LLMs across ICPC World Finals problems from 2011 to 2023 and the unseen 2024 dataset. The results highlight the stark contrast in performance between models optimized for reasoning and general-purpose models when handling challenging unseen problems.
\begin{figure}[htbp]
    \centering
    \includegraphics[scale= 0.38]{figures/Accuracy_Trends_Across_Years__Heatmap.png}
    \caption{Accuracy Trends Across Years (Heatmap)}
    \label{fig:accuracy_heatmap}
\end{figure}

The heatmap reveals that the o1-mini and o1-preview models demonstrate varied accuracy across the years, with both achieving their peak accuracy of 25.0\% in 2017. On the unseen 2024 dataset, o1-mini and o1-preview achieved accuracies of 15.4\% and 7.7\%, respectively, indicating their ability to generalize to new problem sets effectively.

In contrast, GPT-4o, Mistral Large, and Llama-3.1 consistently show 0\% accuracy across all years, including the unseen 2024 problems. These results emphasize their limitations in solving complex ICPC World Finals problems. This trend aligns with the hypothesis that general-purpose models lack the structured reasoning pathways necessary to excel in competition-level problem-solving without specific optimization.

Overall, the results confirm that models like o1-mini and o1-preview exhibit superior robustness and adaptability. Their consistent performance across years highlights the effectiveness of their fine-tuning and chain-of-thought reasoning capabilities. Conversely, the failure of other models to achieve any meaningful accuracy underscores the challenges faced by general-purpose LLMs in high-stakes competitive programming tasks.
\\
\textbf{RQ2: Accuracy Trends For Different Categories:}
The accuracy trends across problem categories provide insights into the strengths and limitations of the evaluated models. Categories like Implementation (5 problems), Graph Theory (3 problems) and Math (4 problems)\cite{Cobbe2021Training} saw higher overall success rates, particularly for the o1-mini and o1-preview models. These categories test the models' ability to handle structured and logical problem-solving tasks.

In contrast, categories such as Geometry and Greedy, each with only 1 problem, presented challenges across all models, indicating potential difficulties in handling specialized or less frequent problem types. This disparity highlights the need for targeted optimization to improve performance in these areas.

The following chart (Figure~\ref{fig:category_trends}) illustrates the number of solved problems across different categories:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Number_of_solved_problems_across_categories.png}
    \caption{Number of solved problems across categories.}
    \label{fig:category_trends}
\end{figure}

The findings emphasize that problem category distribution plays a significant role in shaping model performance. The o1 models consistently demonstrated better adaptability across diverse categories compared to GPT-4o, Mistral Large, and Llama-3.1-405B, which struggled to generalize effectively. These trends align with the hypothesis that specialized training, such as chain-of-thought reasoning, enhances model capabilities in handling category-specific complexities.
\\
\textbf{RQ3: Variabilities in Model Performance:} Three key factors—dataset contamination, training methodologies, and reasoning strategies—contribute to variations in model performance.
\subsubsection{Dataset Contamination}
One major variability in LLM performance stems from dataset contamination. As shown in Figure~\ref{fig:accuracy_heatmap}, the accuracy of o1-mini and o1-preview on non-contaminated data (2024) is significantly lower than their highest accuracy on potentially contaminated data from earlier years. For example, o1-preview achieves 25.0\% accuracy in 2017, whereas its accuracy on the unseen 2024 problems drops to 7.7\%. Similarly, o1-mini's accuracy drops from 25.0\% in 2017 to 15.4\% in 2024. This disparity suggests that data contamination likely contributes to inflated performance metrics, as models may rely on memorized patterns rather than genuine reasoning for problems included in their training data. These results highlight the need for contamination-free datasets to accurately evaluate reasoning and generalization capabilities \cite{b1, b27}.

\subsubsection{Training Methodologies}
The training methodologies adopted by different models significantly influence their performance variability. As highlighted in Figure~\ref{fig:accuracy_heatmap} and Figure~\ref{fig:verdict_distribution}, o1-mini and o1-preview consistently outperform general-purpose models such as GPT-4o and Llama-3.1-405B. This success is attributed to their specialized fine-tuning, which emphasizes chain-of-thought (CoT) reasoning and iterative refinement.

Unlike general-purpose models, the o1-mini and o1-preview models are trained to simulate step-by-step problem-solving, enabling them to break down complex ICPC problems into manageable sub-tasks\cite{Patel2021Are}. For example, the verdict distribution analysis in Table~\ref{tab:verdict_distribution_counts} shows that o1-mini achieves 16 Accepted (AC) solutions and 10 Compile Errors (CE), compared to GPT-4o, which recorded 0 ACs and 41 CEs. These results highlight how tailored training methodologies enhance the reasoning pathways of o1 models, directly translating to higher success rates on unseen, complex tasks \cite{b4, b13}.

\subsubsection{Reasoning Strategies}
The reasoning strategies employed by LLMs also play a crucial role in their performance variability. Models in the o1 family leverage advanced CoT prompting, enabling logical, step-by-step reasoning that aligns with the structured problem-solving nature of ICPC challenges. This is evident in their ability to handle edge cases and adhere to strict computational constraints, as shown in Figure~\ref{fig:verdict_distribution}, where o1-mini achieves the highest percentage of ``Accepted" (AC) verdicts at 9.64\%.

In contrast, general-purpose models like GPT-4o and Llama-3.1-405B often lack CoT-specific training, resulting in a reliance on pattern recognition and memorized data. This limitation leads to higher error rates on unseen problems and significantly reduced adaptability, as reflected in their 0\% accuracy across all years \cite{b27}.

These findings reinforce the importance of specialized training methodologies and advanced reasoning strategies in achieving consistent and robust performance on competitive programming tasks. The combination of contamination-free datasets, tailored training, and effective reasoning strategies enables models like o1-mini and o1-preview to outperform general-purpose alternatives.
\\
\textbf{RQ4: Verdict Distribution Analysis:}
Verdict distributions highlight differences in error frequencies and success rates across models. As illustrated in Figure~\ref{fig:verdict_distribution}, o1-preview and o1-mini had the highest proportions of ``Accepted" (AC) verdicts, with 9.64\% for o1-mini and 9.04\% for o1-preview. In contrast, models like GPT-4o showed predominantly error states such as ``Compile Error" (CE) and ``Wrong Answer" (WA). Table~\ref{tab:verdict_distribution_counts} shows that o1-mini and o1-preview achieved 16 and 15 Accepted (AC) solutions, respectively, while GPT-4o, Mistral Large, and Llama-3.1-405B recorded 0 ACs. 

For error states, GPT-4o had the highest percentage of Compile Errors (CE) at 24.7\%, followed by Llama-3.1 at 19.88\% and Mistral Large at 18.07\%. ``Wrong Answer" (WA) verdicts were the most frequent error state across all models, with o1-mini recording 124 WA (74.70\%) and GPT-4o recording 55 WA (33.25\%).

These results emphasize the superiority of the o1 models, particularly their ability to achieve higher success rates and fewer Compile Errors compared to general-purpose models. However, the dominance of ``Wrong Answer" verdicts across all models highlights the challenges faced in generating accurate solutions for competitive programming tasks. Figure~\ref{fig:verdict_distribution} and Table~\ref{tab:verdict_distribution_counts} summarize these findings.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.35]{figures/Verdict_Distribution_Across_Models.png}
    \caption{Verdict Distribution Across Models}
    \label{fig:verdict_distribution}
\end{figure}

\begin{table}[htbp]
    \caption{Verdict Distribution Across Models (Counts)}
    \centering
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        \textbf{Model} & \textbf{AC} & \textbf{CE} & \textbf{RE} & \textbf{TLE} & \textbf{WA} & \textbf{MLE} \\
        \midrule
        o1-mini        & 16          & 10          & 0           & 13           & 124         & 1           \\
        o1-preview     & 15          & 19          & 2           & 10           & 120         & 0           \\
        GPT-4o         & 0           & 41          & 39          & 31           & 55          & 0           \\
        Mistral-Large  & 0           & 30          & 48          & 23           & 65          & 0           \\
        Llama-3.1      & 0           & 33          & 36          & 25           & 70          & 0           \\
        \bottomrule
    \end{tabular}
    \label{tab:verdict_distribution_counts}
\end{table}

These findings are consistent with \cite{b4}, which highlights the superior chain-of-thought reasoning capabilities of o1 models, enabling them to handle complex problem constraints more effectively.

\subsection{Key Insights}
The comprehensive evaluation highlights several key takeaways:
\begin{enumerate}
    \item \textbf{o1 Models Superior Performance}: o1-mini and o1-preview consistently outperform other LLMs in accuracy, verdict distribution, and resource efficiency, demonstrating their advanced reasoning and calibration.
    \item \textbf{Impact of Training Methodologies}: Models with specialized training for chain-of-thought reasoning exhibit greater robustness and adaptability to unseen problems.
    \item \textbf{Necessity for Contamination-Free Benchmarks}: The significant performance drop in general-purpose models on unseen data underscores the importance of uncontaminated benchmarks to accurately assess model generalization \cite{b2,b13}.
    \item \textbf{Resource Efficiency}: o1 models not only achieve higher accuracy but also demonstrate superior computational efficiency, making them more suitable for resource-constrained environments.
\end{enumerate}

These insights set the stage for future research in training methodologies and evaluation frameworks for LLMs in technical problem-solving.

