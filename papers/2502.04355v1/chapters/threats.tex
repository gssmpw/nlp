\section{Threats to Validity}
In this section, we discuss four types of threats, similar to prior research \cite{pyreddy2025emoxpt}.
\subsection{Threats to Internal Validity}
Data contamination poses a major threat to our study’s internal validity. Some of the ICPC problems we used might already exist in the evaluated LLMs’ training data. We select
problems from years (2011-2024) that likely do not overlap with the training cutoffs of popular models, but ensuring no overlap is challenging. If any problem appears in the training data, they could artificially inflate performance scores, especially for models exposed to similar problem statements\cite{Xu2023Lemur}. Therefore, we evaluated the models on ICPC 2024's problems within 24 hours of the contest to ensure none of the models were trained on that data. 

\subsection{Threats to External Validity}
The study focuses on ICPC World Finals problems, which may limit the generalizability of the findings. While ICPC problems are diverse and complex, they represent only a subset of the programming challenges faced in broader software engineering and real-world application development. As a result, the performance of LLMs on ICPC problems may not directly translate to other programming tasks, potentially limiting the applicability of our conclusions to different domains \cite{b1,b2,b12}. To mitigate this bias, we use a diverse set of 166 ICPC World Finals problems, covering various categories and difficulty levels, to strengthen the robustness of the evaluation.

\subsection{Threats to Construct Validity}
We test the models in a zero-shot setting without additional fine-tuning or iterative interactions. This approach may not fully leverage the models’ capabilities, as fine-tuning or interactive prompting could improve performance on specific tasks. Additionally, differences in how each model interprets prompts or handles specific problem components may introduce variability not fully accounted for in our study. These constraints could limit the ability to generalize the findings to scenarios where models are fine-tuned or interactively guided during problem-solving \cite{b3,b4,b11}. To mitigate this bias, we document all experimental procedures and provide access to the scripts used for data preprocessing and solution evaluation to ensure reproducibility and transparency.
\subsection{Threats to Conclusion Validity}
Our evaluation relies exclusively on the Codeforces Gym platform for submitting solutions and determining verdicts. This creates a dependency on the platform’s specific execution environment and judging criteria. Variations in compiler versions, runtime environments, or hidden test cases used by the platform can affect the consistency and reliability of the verdicts. Additionally, platform-specific nuances in error reporting or resource measurement may influence the evaluation of resource utilization metrics \cite{b27,b13}. To minimize this bias, we ensure that all submissions are made under the same conditions within Codeforces Gym, maintaining consistency in verdict determination and resource measurement.
\ignore{
\subsection{Mitigation Strategies}
To address these threats, we employed several mitigation strategies:
\begin{itemize}
    \item \textbf{Data Selection}: Carefully selected problems from years less likely to overlap with the training data of major LLMs to minimize the risk of data contamination.
    \item \textbf{Diverse Problem Set}: Utilized a diverse set of 83 ICPC World Finals problems encompassing various categories and difficulty levels to enhance the robustness of the evaluation.
    \item \textbf{Consistent Evaluation Environment}: Ensured that all submissions were made under the same conditions within Codeforces Gym to maintain consistency in verdict determination and resource measurement.
    \item \textbf{Transparent Reporting}: Documented all experimental procedures and provided access to scripts used for data preprocessing and solution evaluation to facilitate reproducibility and transparency.
\end{itemize}

By acknowledging and addressing these threats, we aim to provide a balanced and comprehensive assessment of LLM performance on competitive programming benchmarks.
}