\section{Related Work}
Advancements in large language models (LLMs) have spurred significant interest in evaluating their capabilities across various domains. In competitive programming, several studies emphasize the importance of rigorous benchmarks and highlight challenges such as reasoning ability, data contamination, and evaluation methodologies.

\subsection{Competition-Level Problems as LLM Evaluators}
Programming challenges from platforms like Codeforces and the International Collegiate Programming Contest (ICPC) offer unique evaluation benchmarks for LLMs due to their complexity and diversity. These problems require a deep understanding of algorithms, mathematics, and reasoning, making them ideal for assessing LLM capabilities. Performance on unseen problems often drops significantly, indicating limitations in reasoning and generalization \cite{b1,b12}. These challenges underscore the need for reliable benchmarks and techniques to enhance reasoning in LLMs.

\subsection{Evaluation Methodologies}
Traditional methods for evaluating code generation have faced criticism due to issues like ``context leakage" and ``evolving-ignored" problems. Benchmarks such as those discussed in \cite{b2} better simulate real-world scenarios by considering the evolving nature of software development. These approaches reduce inflated performance metrics caused by unrealistic evaluation settings, providing a more accurate reflection of an LLM's problem-solving capabilities.

\subsection{Code Refinement and Interactivity}
Interactive code refinement and test-driven workflows have been shown to improve the quality of LLM-generated solutions. Iterative problem-solving techniques, combined with clear feedback, enhance LLM performance, particularly on complex programming challenges \cite{b3,b13}.

\subsection{Model-Specific Insights}
Studies focusing on specific models, such as the o1 family, emphasize their advanced chain-of-thought reasoning and robust handling of competitive programming problems \cite{b4}. These models outperform others in minimizing hallucinations and achieving consistent performance across diverse tasks, further validating their efficacy for high-stakes evaluations. Similarly, Mistral 7B demonstrates optimized performance for efficiency, highlighting the importance of model design for resource-constrained environments \cite{b11}.

\subsection{Challenges in Code Evaluation}
Issues like data contamination, overfitting, and reliance on pretraining data limit the generalization of LLMs. Research on the calibration and correctness of LLM-generated code highlights the importance of confidence scores and error analysis in ensuring reliable outputs, particularly in scenarios requiring high precision \cite{b6}.

\subsection{Performance Comparisons}
Comparative studies of models like GPT-4o, Llama-3.1, and o1 systems reveal stark differences in accuracy and resource efficiency \cite{b27,b10}. These comparisons underscore the importance of both training data and architectural design in achieving superior results on competitive programming problems.

By synthesizing these findings, our work contributes to the growing body of research by evaluating multiple LLMs on ICPC-style problems. This expands on previous studies by combining problem-solving performance with detailed error and resource utilization analysis.
