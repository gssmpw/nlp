\section{Introduction}
Large language models (LLMs)\cite{b14} have revolutionized natural language processing (NLP)\cite{b15} and their applications in various technical domains, including algorithmic problem-solving and code generation. Trained on extensive and diverse datasets, these models exhibit remarkable capabilities in understanding and generating code\cite{b19}. However, their performance on complex, real-world coding challenges remains an area of active investigation. Competitive programming problems, such as those from the International Collegiate Programming Contest (ICPC)\cite{b8}, offer a unique opportunity to rigorously evaluate LLMs due to their intricate constraints, computational demands, and emphasis on algorithmic efficiency.

ICPC problems are particularly well-suited for evaluating LLM performance\cite{Austin2021Program} for several reasons. First, these problems require a combination of logical reasoning, algorithmic thinking, and precise implementation, reflecting real-world software engineering challenges. Second, their diverse categories, including graph theory, dynamic programming, and computational geometry, offer a broad spectrum of difficulty levels, testing an LLM's\cite{Carlini2022Quantifying} ability to generalize across various problem types. Third, the focus on correctness, efficiency, and edge-case handling aligns with critical evaluation metrics such as runtime performance and memory usage \cite{b1,b2,b10}.

Existing works use LLMs at different stages of code generation and testing. For example, Fakhoury et al. \cite{b3} improve solutions through test feedback and refine them step-by-step. Liu et al. \cite{b1} identify problems like data contamination and over-reliance on memorized patterns, which reduce the reliability of results. A recent study \cite{b13} explores how adding syntax and grammar rules during training can enhance the accuracy and robustness of LLMs. Similarly, Coignion et al. \cite{b27} analyze the efficiency of code generated by LLMs on Leetcode, revealing performance trends and benchmarks. However, these studies primarily focus on specific aspects of code generation, such as correctness or efficiency, without comprehensively evaluating LLM performance in solving complex, real-world competitive programming problems. To address this gap, we propose an approach, \Name{}, to assess the capabilities of advanced LLMs, including OpenAI’s o1 family\cite{b18}, GPT-4o\cite{b16}, Mistral Large\cite{b7}, and Llama-3.1-405B\cite{b17}, in tackling ICPC problems. 

To develop \Name{}, we collect a curated dataset of 166 ICPC World Finals problems from the years 2011 to 2024, providing a comprehensive benchmark for assessing the reasoning, accuracy, and efficiency of LLMs. The study assessed five state-of-the-art models—GPT-4o, Mistral Large, Llama-3.1-405B, o1-preview and o1-mini-selected for their diverse architectures and training methodologies. Comprehensive data preprocessing involved extracting problem components, standardizing prompts, cleaning text, customizing templates for each model, and validating the preprocessed data. Solutions generated by the LLMs were submitted to Codeforces Gym ICPC contests, receiving automated feedback on correctness and efficiency. All experiments were conducted in a controlled environment, with reproducible submissions and publicly available scripts to ensure consistency and transparency. In summary we make the following contributions:
\begin{itemize}
    \item We propose a performance analyzer, \Name{}, to assess the performance of five different LLMs on new ICPC problems and examine accuracy trends over time.
    \item We present an experimental evaluation that identifies factors contributing to variability in LLM performance and highlights the distribution of verdicts across various LLMs for the given problem set.
\end{itemize}

Our findings reveal that o1 models, due to their enhanced Chain of Thought reasoning and calibration, significantly outperform others in terms of accuracy, robustness, and efficiency. Additionally, we identify critical limitations, such as overestimation of confidence by some models and their struggle with complex, high-difficulty problems. Recent studies, such as \emph{The Llama 3 Herd of Models} \cite{b11}, further highlight the challenges in ensuring consistent performance across diverse technical domains, emphasizing the need for specialized benchmarks. By addressing these issues and leveraging insights from ICPC problem evaluations, this paper lays the foundation for improving LLM design and training methodologies to meet the challenges of technical problem-solving.
