\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{tikz}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs} % For better tables
\newcommand{\commenttsz}[1]{{\color{green} \sf (TSZ: #1)}}
\newcommand{\ignore}[1]{}
\newcommand{\Name}{LLM-ProS}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{LLM-ProS: Analyzing Large Language Models' Performance in Competitive Problem Solving}
\author{\IEEEauthorblockN{Md Sifat Hossain, Anika Tabassum, Md. Fahim Arefin}
\IEEEauthorblockA{
Department of Computer Science and Engineering,\\
University of Dhaka, Dhaka, Bangladesh \\
Email: $\lbrace$\textit{mdsifat-2019217800@cs.du.ac.bd,
}\\
\textit{anika-2019417844@cs.du.ac.bd,}\\
\textit{fahim@cse.du.ac.bd}$\rbrace$}\\
\and
\IEEEauthorblockN{Tarannum Shaila Zaman}
\IEEEauthorblockA{
Department of Information Systems, \\
University of Maryland, Baltimore County, Maryland, USA\\
Email: $\lbrace$\textit{zamant@umbc.edu}$\rbrace$}
}

\newcommand\copyrighttext{%
  \footnotesize \textcopyright 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or promotional
  purposes, creating new collective works, for resale or redistribution to servers or
  lists, or reuse of any copyrighted component of this work in other works.}
  
\newcommand\copyrightnotice{%
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south,yshift=10pt] at (current page.south) {\fbox{\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{\copyrighttext}}};
\end{tikzpicture}%
}

\maketitle
\copyrightnotice

\begin{abstract}

The rapid advancement of large language models has opened new avenues for automating complex problem-solving tasks such as algorithmic coding and competitive programming. This paper introduces a novel evaluation technique, LLM-ProS, to assess the performance of state-of-the-art LLMs on International Collegiate Programming Contest (ICPC) problems. Using a curated dataset of 166 World Finals problems from 2011 to 2024, we benchmark the models' reasoning, accuracy, and efficiency. We evaluate the five models-GPT-4o, Mistral Large, Llama-3.1-405B, and the o1 family, consisting of o1-mini and o1-preview, across critical metrics like correctness, resource utilization, and response calibration. Our results reveal significant differences in the models' abilities to generalize, adapt, and solve novel problems. We also investigated the impact of training methodologies, dataset contamination, and chain-of-thought reasoning on model performance. The findings provide new insights into optimizing LLMs for algorithmic tasks, highlighting both strengths and limitations of current models.

\end{abstract}

\begin{IEEEkeywords}
Large Language Models, Competitive Programming, ICPC, Performance Evaluation, Chain-of-Thought Reasoning
\end{IEEEkeywords}

\input{chapters/intro}
\input{chapters/background}
\input{chapters/method}
% \input{chapters/experiment}
\input{chapters/result}
\input{chapters/threats}
\input{chapters/relatedw}
\section{Conclusion}
This study underscores the efficacy of competition-level programming problems, specifically those from the International Collegiate Programming Contest (ICPC), as robust benchmarks for evaluating large language models (LLMs). Utilizing a curated dataset of 166 ICPC World Finals problems from 2011 to 2024, we systematically assessed state-of-the-art LLMs, including OpenAI's o1 family, GPT-4o, Mistral Large, and Llama-3.1-405B. Our findings reveal that the o1 models, particularly o1-mini and o1-preview, significantly outperform others in terms of accuracy, robustness, and computational efficiency, owing to their advanced chain-of-thought (CoT) reasoning and diverse training methodologies. In contrast, models such as GPT-4o, Mistral Large, and Llama-3.1-405B demonstrate limited generalization abilities and higher error rates on unseen 2024 problems, highlighting their reliance on pretraining data and the need for improved generalization skills \cite{b1,b6}. Additionally, the analysis of verdict distributions indicates that o1 models consistently achieve higher proportions of ``Accepted" (AC) verdicts while minimizing errors like ``Compile Error" (CE) and ``Time Limit Exceeded" (TLE), in contrast to other models which show higher frequencies of these errors. These insights emphasize the importance of developing contamination-free benchmarks and enhancing reasoning capabilities through advanced training methodologies like CoT and iterative refinement \cite{b3,b4}. Overall, \Name{} contributes to a deeper understanding of LLM performance in real-world problem-solving scenarios and paves the way for further advancements in LLM design and evaluation, ensuring that they can meet the complex demands of technical problem-solving tasks.

\bibliographystyle{IEEEtran}
\bibliography{main}

\end{document}
