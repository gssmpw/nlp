\section{Introduction}
\label{sec:intro}



Image generative models have progressed explosively in recent years, with the prevalence of Generative Adversarial Networks (GANs) and diffusion models. Text-to-image generation methods such as Stable Diffusion~\cite{LDM,SDXL}, DALL-E 3~\cite{Dalle3}, and Imagen~\cite{ImgGen} are capable of synthesizing high-quality images by taking textual descriptions (prompts) as the input. One key step of these models is to understand the user intention and semantic meanings from the text prompts and encode them to text features for further driving image content generation with diffusion models. To this end, most of the existing methods adopt an encoder-based language model structure (\emph{e.g.}, CLIP~\cite{CLIP} or T5~\cite{T5}), which were pre-trained on a limited number of image-caption pairs or texts pairs due to the expensive data annotation cost, resulted in the unsatisfying performance for the image generation quality and reliability. Thus, obtaining a user-desired image with these methods is not easy. In particular, to generate a complex and detail-rich image, repetitive trials of manipulating the text prompts are very common. For example, as shown in \cref{fig:head}, the state-of-the-art DALL-E 3 fails to comprehend the entities and their relationships described in the complex prompts, resulting in numerous omissions.


On the other hand, we have also witnessed a very fast development of the Large Language Models (LLMs), \emph{e.g.}, GPT-4~\cite{GPT4}, PaLM~\cite{PaLM} and Llama2~\cite{Llama2}, which have shown very incredible power on semantic understanding, reasoning and naturally interacting with human. These LLMs mainly employ the decoder-only structure that can be trained on a massive scale of unlabeled textual data. Unfortunately, bridging the ability of LLMs with the current diffusion-based text-to-image generation framework is unexplored due to the incompatibility of these two model architectures. Some recent attempts have made to borrow the ability of LLMs for enhancing the text-to-image generation performance with the diffusion models~\cite{LLM_Grounded, LLM_Blueprint}. Their approaches proposed to enrich or rewrite the user text prompt through LLMs and still rely on the vanilla text encoders to guide the image generation process within the diffusion models, leading to sub-optimal performance. 

%Specifically, encoder-based language models have inherent weaknesses in language comprehension and logical reasoning, often leading to significant deviations from user prompts when generating images. For instance, CLIP frequently ignores key attributes and relationships described in the given input text. 
%Encoder models also lack capabilities like in-context learning and recursive feedback that could improve fidelity to prompts. 

%

%In the last column of \cref{fig:head}, existing text-encoder-based models also exhibit poor logical reasoning capabilities, failing to accurately understand that the user's intent is to describe the functionalities of the cat rather than its states. 

%Moreover, their inherent design makes it challenging to perform in-context learning and learning with interactive conversations and human feedback, which are common for state-of-the-art LLMs.
% Additionally, encoder models struggle to handle long and complex textual descriptions.
% They often fail to capture the precise semantics and logical entailments within complex textual descriptions, leading to significant deviations between the generated images and user intentions. % prompts.
%Additionally, encoder models struggle to capture the precise semantics and logical entailments within complex textual descriptions, leading to significant deviations between the generated images and user intentions.







% %While some recent methods utilize decoder-only LLMs to aid Diffusion models in generating better images, these approaches simply rewrite descriptions or provide supplementary information. They do not address the underlying deficiencies of encoder-based language models.
% While there have been attempts to enhance text-to-image generation in diffusion models by leveraging generative Decoder-only LLMs~\cite{LLM_Grounded, LLM_Blueprint}, existing methods primarily focus on rewriting descriptions or providing auxiliary information, such as using LLMs to rewrite or expand the initial prompts, or to predict the image layout.
% %Recent approaches have sought to improve text-to-image diffusion models by incorporating large decoder-only LLMs, such as using LLMs to rewrite or expand the initial prompts, or to predict layout descriptions. 
% However, these approaches still take the text representations from those encoder-based models as the conditions for the diffusion models, without alleviating their most critical bottlenecks.
% Specifically, the rewritten prompts still need to pass through the same text encoder, which remains a bottleneck due to its inadequate comprehension and reasoning capabilities. 
% % Therefore, the core weakness of existing diffusion models remains the encoder-based language model, rather than those simple prompts. 
% Therefore, there is an urgent need for approaches to extracting text representations from decoder-only LLMs.

%我的方法
%为了应对这一挑战，我们提出了新的方法和理论，能够将强力的decoder-only LLMs中的知识直接导入到diffusion模型中。具体来说，我们根据diffusion模型的相关理论，将Transformer结构的生成式LLM看作一种diffusion模型。并从encoder-decoder结构的LLM进行类推，得到了通过score function来从decoder-only LLMs中估计相应的text encoding的方法。
To tackle this challenge, we propose a novel and general approach to upgrading various text-to-image diffusion models by borrowing the strength of semantic understanding from large language models (LLMs). In particular, we reveal that a Transformer-based language model (\emph{e.g.} ChatGPT~\cite{GPT4}) can be rephrased as the denoising steps in Denoising Diffusion Probabilistic Models (DDPMs)~\cite{DDPM}. 
Viewing LLMs as diffusion models, we have further derived theoretical underpinnings for extracting text encodings from the blocks of LLMs.
%This finding drives us to derive a simple yet effective network module that is attached to the cross-attention part of the denoising U-Net,
These findings drive us to attach a simple yet effective network module to the cross-attention part of the denoising U-Net.
as shown in \cref{fig:LLM}. This module enables us to effectively integrate block-wise representations within the language model for generating the text encoding of the input text prompt, which can accurately capture the semantic meanings and contextual dependency among words due to the power of pre-trained LLMs. We name this module as LLMDiff-Adapter as it can be a plug-and-play component for connecting LLMs with various text-to-image diffusion models and gaining conspicuous improvement. As some examples shown in \cref{fig:head}, the results generated by our model can better preserve the semantic meanings and user intent from the input prompts, \emph{e.g.} well representing the entities and their relationships for image generation. 

% To tackle these challenges, we propose a novel approach that effectively integrates the knowledge from powerful LLMs into text-to-image diffusion models.
% % Theoretically, we extract text encodings that can be used for text understanding from generative Decoder-only LLMs. 
% Specifically, we argue that the decoder of a Transformer-based LLM can be treated as a diffusion model, where each Transformer block corresponds to a diffusion time step, as shown in \cref{fig:LLM}.
% In order to estimate the implicit text encoding for Decoder-only LLMs, we derive a score function for the distribution of text encoding conditioned on the predicted text, and perform Langevin dynamics sampling to obtain the encoding given the input text prompt.
%This extracted knowledge serves as guidance for the Diffusion model during the image generation process. Subsequently, we introduce the LLMDiff-Adapter, a tailored mechanism that facilitates the seamless integration of large generative language models into existing text-to-image diffusion models.
% Through those text features, we introduce the LLMDiff-Adapter, a tailored framework that facilitates the seamless integration of large generative language models into existing text-to-image diffusion models.
%Based on the sampled text encoding, we introduce the LLMDiff-Adapter, a tailored framework that facilitates the seamless integration of large generative language models into existing text-to-image diffusion models.
% This adaptation enhances the models' controllability and language understanding, leading to improved detail fidelity and image precision.
%This adapter is attached to the cross-attention modules of the denoising U-Net, and it is able to bridge various pre-trained diffusion models and LLMs.
%This adapter operates on the cross-attention modules of the denoising U-Net, and can be efficiently trained with the pre-trained parameters of the diffusion model and the LLM frozen.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{imgs/llm_struct3.pdf}
    \vspace{-10pt}
    \caption{Comparison with other neural network structures employed for computing text encoding in diffusion models. Our proposed LLMDiff, which leverages a decoder-only structure by casting the transformer-based language model as a diffusion model, can predict the text encodings for text-to-image generation by integrating layer-wise representations in the language model. Intuitively, compared with other structures (\emph{e.g.} encoder-decoder) our LLMDiff is more powerful in exploring the semantic meanings and dependency among words from the input text prompt. More details and theoretical derivations are provided in \cref{sec:decoder_encodings}.}
    %\vspace{-10pt}
    \label{fig:LLM}
\end{figure*}

%实验
In the evaluation, we conduct a comparative analysis of different text-to-image models on the same benchmarks. We compared the performance of using our proposed LLMDiff-Adapter against other architectures, \emph{e.g.} simply connecting the output of decoder-only LLMs or adopting encoder structure such as CLIP~\cite{CLIP} and T5~\cite{T5} through linear layers to the diffusion models. The experimental results show that our model achieves superior performance among the competitions in several aspects, including the quality of generated image details, logical coherence, and comprehensive understanding of the text descriptions. The relevant quantitative results are also presented to underscore the effectiveness of our approach in solving the limitations of current diffusion-based text-to-image generation methods.

