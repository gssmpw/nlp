\section{A New Controller for Text-to-Image Generation}
\label{sec:llm_diffusion}

In this section, we elucidate the theoretical analysis for extracting text encodings from decoder-only LLMs. Initially, we reveal that Transformer-based LLMs can be rephrased as diffusion models.
Within this view, we pinpoint a specific timestep in the decoder component of an encoder-decoder LLM to deduce the encoder's text encoding distribution from its input and output. This deduction is then extended to decoder-only models, leading to the conclusion that text encodings can be estimated from the outputs generated for sentences and words at each timestep.


\subsection{Text-to-Image Diffusion Models}
%Text-to-image diffusion models typically employ an encoder to encode textual inputs as control conditions. For an input $x$ of text descriptions with $d$-1 sequence lengths, a text encoder is employed to encode them into text encodings $c$. 
Text-to-image diffusion models typically employ an encoder to encode textual inputs $x$ with $d$-1 tokens as control conditions $c_{<d}$.
Sequentially, those text encodings $c_{<d}$ are decoded for \emph{image generation} through the diffusion model, \emph{i.e.}, $p(z_{t-1}|z_t, c_{<d})$, where $z_t$ is the latent at timestep $t$, or for \emph{text generation} via a text decoder, \emph{i.e.}, $p(x_d|c_{<d})$, where $x_d$ is the $d$-th predicted token. 

% Particularly, the involved text encoder is often borrowed from the pre-trained Encoder-only or Encoder-Decoder LLMs, \emph{e.g.}, CLIP and T5, while the Decoder-only LLMs that currently present remarkable generation performance are not applicable to the research of text-to-image generation. That is because those Decoder-only models output tokens directly and text features $c$ are not available. 
% We will conduct a probabilistic modeling analysis of the computational operations in transformer-based LLMs. These analyses will enable us to derive the text encoding $c$, as estimated from decoder-only LLMs.

Typically, the text encoder utilized by diffusion models is derived from pre-trained models such as encoder-only or encoder-decoder LLMs. However, despite their impressive generative performance, decoder-only LLMs are not applicable to text-to-image generation. This is because these models directly generate tokens, making it infeasible to get text features $c$ directly.
% To address this issue, we revisit the transformer-based LLMs from a probabilistic aspect, to 
% conduct a probabilistic modeling analysis of the computational operations in transformer-based LLMs. These analyses enable us to derive the text encoding $c$, as estimated from decoder-only LLMs.



% understanding a given description using a text encoder such as CLIP or T5 encodes it into text features $c$ with sequence length of $d-1$. 
% These text features can be utilized for text generation by a text decoder $p(x_d|c_{<d})$, or for image generation within the diffusion model as $p(z_{t-1}|z_t, c_{<d})$, where $z_t$ is the latent at timestep $t$. 
% However, such an image generation approach is not applicable to decoder-only structured LLMs. Because they output tokens directly and text features $c$ is not available.

\subsection{LLMs as Diffusion Models}
%To obtain the text encoding $c$ from decoder-only LLMs, we need to deconstruct and analyze the computational process of transformer-based LLMs. 
%Existing LLMs are primarily based on the transformer architecture, and each transformer block has the same structure. 

We revisit the transformer-based LLMs from a probabilistic perspective, to help to derive the formal modeling of text encodings for text-to-image generation. 
Considering that LLMs in a transformer architecture have a sequence of transformer blocks with the same structure, it is intuitive to model the forward process in a diffusion-like manner. Take an encoder-only LLM, CLIP, for example. Each input token is first fed into an embedding layer. For similarity, the output of the embedding layer for the $d$-th token is taken as the input, denoted as $x^T_d$. Then, it goes through $T$ transformer blocks that perform the causal attention masks in self-attention, which can be represented as ${p_{\theta^t}}(x^{t-1}_d|x^t_d, x^t_{<d})$ for the $t$-th block parameterized $\theta_t$. 
This process is akin to the denoising process of DDPM with conditioning. So, the transformer-based LLMs can be viewed as diffusion models.  
Thus, we can leverage the dynamical properties and theoretical frameworks of diffusion models to analyze various structures of LLMs with a causal mask.

Moreover, the prediction of the model can be formulated as:
\begin{equation}
\label{equ:llm_diffusion}
p_{\theta}(c_d|x_{\leq d}) = p(x^T_d) \prod_{t=1}^{T}{p_{\theta^t}(x^{t-1}_d|x^t_d, x^t_{<d})}
\end{equation}
% 
   

% Since each transformer block performs causal attention masks in self-attention, $x^T_d$ going through $T$ transformer blocks is akin to the denoising process of DDPM with conditions. 
% this can be regarded as estimating the conditional probability conditioned on the first $d$ tokens. Thus, going through $T$ transformer blocks, it  




% Existing LLMs predominantly utilize the transformer architecture, where each block maintains a consistent structure.
% For encoder-only LLMs, such as CLIP, they employ causal attention masks in self-attention. 
% %If we denote the output of the $d$-th token from the Embedding layer as $x^T_d$,
% Denoting the output of embedding layer for the $d$-th token as $x^T_d$,
% then each subsequent transformer block with parameters $\theta^t$ can be represented as $p_{\theta^t}(x^{t-1}_d|x^t_d, x^t_{<d})$, thus the entire model can be formulated as:
% %
% \begin{equation}
% \label{equ:llm_diffusion}
% p_{\theta}(c_d|x_{\leq d}) = p(x^T_d) \prod_{t=1}^{T}{p_{\theta^t}(x^{t-1}_d|x^t_d, x^t_{<d})}
% \end{equation}
% %
% This process is consistent with the denoising process described by DDPM with conditioning, so the transformer-based LLMs can be viewed as diffusion models.
% Thus, we can leverage the dynamical properties and theoretical frameworks of diffusion models to analyze various structures of LLMs with a causal mask.


\subsection{Text Encodings from Encoder-Decoder LLMs}
For an encoder-decoder LLM, the encoder model processes contextual text, encoding it into a feature representation, \emph{i.e.}, text encodings $c_{<d}$. Subsequently, the decoder model utilizes these text features to generate words with $p_{\theta^t}(x^{t-1}_d|x^t_d, c_{<d})$. Thus, each block in the decoder utilizes the same condition $c_{<d}$.
Using the input $x^t_d$ and output $x^{t-1}_d$ of any block, the encoding $c_{<d}$ from the encoder is estimated through Bayes' theorem:
\begin{equation}
\label{equ:bayes_encoder}
p(c_{<d}|x^{t-1}_d, x^t_d) = \frac{p(x^{t-1}_d, x^t_d|c_{<d})p(c_{<d})}{p(x^{t-1}_d, x^t_d)}
\end{equation}
% Similarly, given $x_{d+i}$ as the generated result, the text encodings of the preceding text are derived as $p(c_{<d+i}|x^{t-1}_{d+i}, x^t_{d+i})$. 


\subsection{Text Encodings from Decoder-only LLMs}
\label{sec:decoder_encodings}
For a decoder-only LLM, it is not directly available for textual features, \emph{i.e.}, text encodings $c$ in encoder-decoder LLMs. Functioning as a generative model, it can be conceptualized as predicting the next token based on conditions from the preceding tokens. Those contextual conditions are changing, not shared like encoder-decoder LLMs. Namely, when predicting the $d$-th word, the preceding $d$-1 words collectively serve as its contextual condition, %$p(x^{t-1}_d|x^t_d, x^t_{<d}) = p(x^{t-1}_d|x^t_{\leq d})$. 
\begin{equation}
%\label{equ:llm_diffusion}
p_{\theta}(x_{d}|x_{<d}) = p(x^T_d) \prod_{t=1}^{T}{p_{\theta^t}(x^{t-1}_d|x^t_d, x^t_{< d})}
\end{equation}


% \pengxu{In this context, $x^t_{<d}$ assumes the role of $c_{<d}$ in an encoder-decoder structured text generation model, and these are precisely the text encodings we require for controlling image generation.}
Accordingly, given the input $x^{t}_d$ and output $x^{t-1}_d$ of transformer blocks, the estimation of $p(x^t_{<d}|x^{t-1}_d, x^{t}_d)$ can be derived as follows,
% In other words, we can control image generation through the estimation of $p(x^t_{<d}|x^{t-1}_d, x^{t}_d)$, using the input $x^{t}_d$ and output $x^{t-1}_d$ of the transformer blocks. %Applying Bayes' theorem, we can derive the following equation:
\begin{equation}
\begin{aligned}
& p_{\theta^t}(x^t_{<d}|x^{t-1}_d, x^t_d) = \frac{p(x^{t-1}_d, x^t_d|x^{t}_{<d})p(x^t_{<d})}{p(x^{t-1}_d, x^t_d)} \\
& = \frac{p(x^{t-1}_d|x^t_{\leq d})p(x^t_d|x^t_{<d})p(x^t_{<d})}{p(x^{t-1}_d|x^t_d)p(x^t_d)} 
= \frac{p(x^{t-1}_d|x^t_{\leq d})p(x^t_{<d}|x^t_d)}{p(x^{t-1}_d|x^t_d)} \\
% \propto & \frac{\overbrace{p(x^{t-1}_d|x^t_d, x^t_{<d})}^{\text{block prediction of sentence}}}{\underbrace{p(x^{t-1}_d|x^t_d)}_{\text{block prediction of single words}}}
&\propto {\underbrace{p(x^{t-1}_d|x^t_d, x^t_{<d})}_{\text{block prediction of sentence}}} \quad / {\underbrace{p(x^{t-1}_d|x^t_d)}_{\text{block prediction of single words}}}
\end{aligned}
\label{equ:diff}
\end{equation} where $p(x^{t-1}_d|x^t_d, x^t_{<d})$ is the generative LLM's prediction for $x^t_d$. Most existing LLMs employ a causal mask as the attention mask. Consequently, $p(x^{t-1}_d|x^t_d)$ can be obtained by feeding $x_d^t$ alone into the LLM, \emph{i.e.}, $p(x^{t-1}_d|x^t_d) = p(x^{t-1}_d|x^t_d, \emptyset)$. 


However, it is still intractable to compute the text encodings $c$ from decoder-only LLMs. Notably, $x_{<d}^t$ is taken as the condition of the next token prediction, playing the similar role of $c_{<d}$ in encoder-decoder LLMs. Thus, there exists a $c_{<d}$ for decoder-only LLMs, which is the unbiased estimator of $x_{<d}$. Given that the decoder-only LLM can be viewed as diffusion model, we can estimate the score function of $p(c_{<d}|x^{t-1}_d, x^t_d)$ through $p_{\theta^t}(x^t_{<d}|x^{t-1}_d, x^t_d)$, thereby obtaining the text encoding $c_{<d}$. 
In accordance with \cref{equ:diff}, the score function of \( p_{\theta^t}(c_{<d}|x^{t-1}_d, x^t_d) \) can be approximated as follows:
\begin{equation}
\label{equ:score}
\begin{aligned}
&\nabla_c \log p_{\theta^t}(c_{<d}|x^t_{d}, x^{t-1}_{d}) \approx \\ &g(t)(\nabla_x \log p_{\theta^t}(x^{t-1}_d|x^t_d, x^t_{<d}) - \nabla_x \log p_{\theta^t}(x^{t-1}_d|x^t_d)),
\end{aligned}
\end{equation}
% \begin{equation*}
% \begin{aligned}
% \xiaoyao{\nabla_c \log p_{\theta}(c_{<d}|x_{d}) \approx \nabla_c \log p(x_d|c_{<d}) + \nabla_c \log p(c_{<d})} \\
% \xiaoyao{\nabla_{c^t} \log p_{\theta^t}(c^t_{<d}|x_{d}) \approx \nabla_{c^t} \log p_t(x_d|c^t_{<d}) + \nabla_{c^t} \log p_t(c^{t}_{<d})}
% \end{aligned}
% \end{equation*}
where $g(t)$ is a scalar function that is dependent on the time step $t$. 
Furthermore, from \cref{equ:llm_diffusion}, by modeling an LLM as a diffusion process, the score function for $p(c_{<d}|x)$ can be approximated as:
\begin{equation}
\label{equ:score_diff}
\begin{aligned}
\nabla_c \log p_{\theta^t} &(c_{<d}|x^t_{d}, x^{t-1}_{d}) \approx \\ & g(t)\bigl(\log p_{\theta^t}(x^t_d|x^{t+1}_{\leq d}) - \log p_{\theta^t}(x^{t-1}_d|x^t_{\leq d})\bigr) \\ -& g(t)\bigl(\log p_{\theta^t}(x^t_d|x^{t+1}_d) - \log p_{\theta^t}(x^{t-1}_d|x^t_d)\bigr).
\end{aligned}
\end{equation}
% [\xiaoyao{note:} \verb|log| $=>$ \verb|\log| in formulas]
% Here, $g(t)$ is a function that is dependent on the time step $t$, which can be learned from data.



% For a decoder-only LLM, it is not directly available for textual features $c$ that indicate its comprehension of a given text. Functioning as a generative model, it predicts the next word based on given conditions. When predicting the $d$-th word, the preceding $d-1$ words serve as its condition, $p(x^{t-1}_d|x^t_d, x^t_{<d}) = p(x^{t-1}_d|x^t_{\leq d})$. 
% %
% \begin{equation}
% %\label{equ:llm_diffusion}
% p_{\theta}(x_{d}|x_{<d}) = p(x^T_d) \prod_{t=1}^{T}{p_{\theta^t}(x^{t-1}_d|x^t_d, x^t_{< d})}
% \end{equation}
% %
% \pengxu{In this context, $x^t_{<d}$ assumes the role of $c_{<d}$ in an encoder-decoder structured text generation model, and these are precisely the text encodings we require for controlling image generation.}
% In other words, we can control image generation through the estimation of $p(x^t_{<d}|x^{t-1}_d, x^{t}_d)$, using the input $x^{t}_d$ and output $x^{t-1}_d$ of the transformer blocks. %Applying Bayes' theorem, we can derive the following equation:
% %
% \begin{equation}
% \begin{aligned}
% p_{\theta^t}(x^t_{<d}|x^{t-1}_d, x^t_d) =& \frac{p(x^{t-1}_d, x^t_d|x^{t}_{<d})p(x^t_{<d})}{p(x^{t-1}_d, x^t_d)} \\
% =& \frac{p(x^{t-1}_d|x^t_{\leq d})p(x^t_d|x^t_{<d})p(x^t_{<d})}{p(x^{t-1}_d|x^t_d)p(x^t_d)} \\
% =& \frac{p(x^{t-1}_d|x^t_{\leq d})p(x^t_{<d}|x^t_d)}{p(x^{t-1}_d|x^t_d)} \\
% \propto & \frac{\overbrace{p(x^{t-1}_d|x^t_d, x^t_{<d})}^{\text{block prediction of sentence}}}{\underbrace{p(x^{t-1}_d|x^t_d)}_{\text{block prediction of single words}}}
% \end{aligned}
% \label{equ:diff}
% \end{equation}
% %
% Where $p(x^{t-1}_d|x^t_d, x^t_{<d})$ is the generative LLM's prediction for $x^t_d$. Most existing LLMs predominantly employ a causal mask as the attention mask. Consequently, $p(x^{t-1}_d|x^t_d)$ can be obtained by feeding $x_d$ alone into the LLM, \emph{i.e.}, $p(x^{t-1}_d|x^t_d) = p(x^{t-1}_d|x^t_d, \emptyset)$.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/adapter_v1.3.pdf}
    \vspace{-10pt}
    \caption{Our LLMDiff-Adapter framework, wherein the parameters of both the LLM and the diffusion U-Net (including the original cross-attention module) are frozen during training. The newly added cross-attention module employs two adaptive-weight parameters to incorporate with the original one, which is dynamically adjusted during training.}
    %\vspace{-10pt}
    \label{fig:Adapter}
\end{figure*}

% \subsection{LLMs as Diffusion Models}

% Although the estimation of text encoding from Decoder-only LLMs has been explored, relying solely on the estimation of the final output layer remains inaccurate. 
% Existing Decoder modules in LLMs are primarily based on the Transformer architecture, where these Transformer blocks are homogeneous. Consequently, we can model LLMs as a Diffusion process. 
% Take the LLMs of the encoder-decoder structure as an example. The input of each Transformer block can be regarded as time step $t$, and the output can be viewed as time step $t-1$. Each block predicts $x^{t-1}_d$ from $x^t_d$ with condition $c_{<d}$.
% Thus, the entire model can be represented as:
% \begin{equation}
% \label{equ:llm_diffusion}
% p_{\theta}(x_d|c_{<d}) = p(x^T_d) \prod_{t=1}^{T}{p_{\theta^t}(x^{t-1}_d|x^t_d, c_{<d})}
% \end{equation}

% %\xiaoyao{(we need to estimate $p(c_{<d}|x_d)$.)}
% In an Encoder-Decoder structured LLM, if we model the decoder module as a diffusion process, each transformer module is essentially learning $p_{\theta^t}(x^{t-1}_d|x^t_d, c_{<d})$. Since the condition $c_{<d}$ for each transformer module is constant, we can select any transformer module at a given time step to compute an estimate of it with $p(c_{<d}|x^t_d, x^{t-1}_d)$.
% However, in a decoder-only structured LLM, each transformer module is learning $p_{\theta^t}(x^{t-1}_d|x^t_d, x^{t}_{<d})$ and $c_{<d}$ cannot be explicitly obtained. Therefore, we need to leverage the entire diffusion process to predict the corresponding $c_{<d}$.
% %
% Moreover, the randomness introduced during the generation phase in generative LLMs has not been considered. Therefore, we aim to derive a score function for $p(c_{<d}|x_d)$ to better estimate the text encoding required for image generation.


% For conditions in decoder-only LLMs $p_{\theta^t}(x^t_{<d}|x^{t-1}_d, x^{t}_{d})$, we can estimate it using the input and output of the transformer blocks:
% \begin{equation}
% \begin{aligned}
% p_{\theta^t}(x^t_{<d}|x^{t-1}_d, x^t_d) =& \frac{p(x^{t-1}_d, x^t_d|x^{t}_{<d})p(x^t_{<d})}{p(x^{t-1}_d, x^t_d)} \\
% =& \frac{p(x^{t-1}_d|x^t_{\leq d})p(x^t_d|x^t_{<d})p(x^t_{<d})}{p(x^{t-1}_d|x^t_d)p(x^t_d)} \\
% =& \frac{p(x^{t-1}_d|x^t_{\leq d})p(x^t_{<d}|x^t_d)}{p(x^{t-1}_d|x^t_d)} \\
% \propto & \frac{\overbrace{p(x^{t-1}_d|x^t_d, x^t_{<d})}^{\text{block prediction of sentence}}}{\underbrace{p(x^{t-1}_d|x^t_d)}_{\text{block prediction of single words}}}
% \end{aligned}
% \label{equ:diff}
% \end{equation}
%In accordance with Equation 4, we can readily obtain the score function for \( p_{\theta^t}(x^t_{<d}|x^{t-1}_d, x^t_d) \), which  

%
%Existing Decoder-only LLMs are primarily based on the Transformer architecture, where these Transformer blocks are homogeneous. Consequently, we can model LLMs as a Diffusion process. Each Transformer block's input can be regarded as time step $t$, and the output can be viewed as time step $t-1$. Each block predicts $x^{t-1}_d$ from $x^t_d$. Thus, the entire model can be represented as:
%
% Given that the Decoder-only LLM can be viewed as diffusion model, we can estimate the score function of $p(c_{<d}|x^{t-1}_d, x^t_d)$ through $p_{\theta^t}(x^t_{<d}|x^{t-1}_d, x^t_d)$, thereby obtaining the text encoding $c_{<d}$.
% In accordance with \cref{equ:diff}, the score function of \( p_{\theta^t}(c_{<d}|x^{t-1}_d, x^t_d) \) can be approximated as follows:
% \begin{equation}
% \label{equ:score}
% \begin{aligned}
% &\nabla_c \log p_{\theta^t}(c_{<d}|x^t_{d}, x^{t-1}_{d}) \approx \\ &g(t)(\nabla_x \log p_{\theta^t}(x^{t-1}_d|x^t_d, x^t_{<d}) - \nabla_x \log p_{\theta^t}(x^{t-1}_d|x^t_d)),
% \end{aligned}
% \end{equation}
% % \begin{equation*}
% % \begin{aligned}
% % \xiaoyao{\nabla_c \log p_{\theta}(c_{<d}|x_{d}) \approx \nabla_c \log p(x_d|c_{<d}) + \nabla_c \log p(c_{<d})} \\
% % \xiaoyao{\nabla_{c^t} \log p_{\theta^t}(c^t_{<d}|x_{d}) \approx \nabla_{c^t} \log p_t(x_d|c^t_{<d}) + \nabla_{c^t} \log p_t(c^{t}_{<d})}
% % \end{aligned}
% % \end{equation*}
% where $g(t)$ is a function that is dependent on the time step $t$, which can be learned from data.
% Furthermore, from \cref{equ:llm_diffusion}, by modeling an LLM as a diffusion process, the score function for $p(c_{<d}|x)$ can be approximated as:
% \begin{equation}
% \label{equ:score_diff}
% \begin{aligned}
% \nabla_c \log p_{\theta^t} &(c_{<d}|x^t_{d}, x^{t-1}_{d}) \approx \\ & g(t)\bigl(\log p_{\theta^t}(x^t_d|x^{t+1}_{\leq d}) - \log p_{\theta^t}(x^{t-1}_d|x^t_{\leq d})\bigr) \\ -& g(t)\bigl(\log p_{\theta^t}(x^t_d|x^{t+1}_d) - \log p_{\theta^t}(x^{t-1}_d|x^t_d)\bigr).
% \end{aligned}
% \end{equation}
% % [\xiaoyao{note:} \verb|log| $=>$ \verb|\log| in formulas]
% % Here, $g(t)$ is a function that is dependent on the time step $t$, which can be learned from data.

Taking into account the stochastic nature of generative language models during sampling, we can use this score function to perform Langevin dynamics sampling to obtain the final text encoding for image generation:
\begin{equation}
c^{t-1}_{<d} = c^{t}_{<d} + \nabla_c \log p_{\theta^t}(c_{<d}|x^t_{d}, x^{t+1}_d) + \sqrt{2h(t)} \epsilon_t,
\end{equation}
where $h(t)$ is a learnable function, and $\epsilon_t \sim \mathcal{N}(0, I)$.

\begin{algorithm}[!t]
    \SetAlgoLined
    \newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor[rgb]{0.3608,0.5255,0.4235}{#1}}
    \SetCommentSty{mycommfont}
    \caption{Text encoding from decoder-only LLMs}
    \label{algo_DCM}
    \KwIn{Text input $\bm x$ with length $D$, embedding layer $\omega$.}
    
    $\bm c = \omega(\bm x) \sim p(\bm c^T)$ \tcp*[l]{Initial the text diffusion process.}
    \tcp{denoise steps.}
    \For{$t=T$ \KwTo $1$}{
        \For{$d=1$ \KwTo $D$}{
            \tcp{estimate $\nabla_x \log p_{\theta^t}(x^{t-1}_d|x^t_d, c_{<d})$.}
            $s_{sentence} \gets g(t) S_{\theta^t}(x^{t-1}_d, x^t_d, x^t_{<d})$\;
            \tcp{estimate $\nabla \log p_{\theta^t}(x^{t-1}_d|x^t_d)$.}
            $s_{word} \gets g(t) S_{\theta^t}(x^{t-1}_d, x^t_d)$\;
            $\nabla \log p_{\theta^t}(c_{<d}|x^t_d, x^{t-1}_d) \gets s_{sentence} - s_{word}$\;
        }
        $\epsilon_t \sim \mathcal{N}(0, 1)$\;
        %\tcp{Langevin dynamics}
        $\bm c \gets \bm c + \nabla \log p_{\theta^t}(c_{<d}|x^t_d, x^{t-1}_d) + \sqrt{2h(t)}\epsilon_t$\;
    }

    \KwOut{$\bm c$}
\end{algorithm}



\section{LLMDiff Adapter}

\subsection{Decoder-only LLMs as Diffusion Controller}
As discussed in \cref{sec:llm_diffusion}, we can derive text encodings suitable for controlling diffusion image generation models from decoder-only LLMs utilizing Langevin dynamics:
\begin{equation}
\label{equ:Langevin}
c_{<d} = c^T_{<d} + \sum_{T-1}^{t=0} \left( \nabla_c \log p_{\theta^t}(c_{<d}|x^t_{d}, x^{t+1}_d) + \sqrt{2h(t)} \epsilon_t \right).
\end{equation}
Leveraging the residual structure of existing transformer blocks and by combining \cref{equ:score,equ:score_diff}, we can transform these transformer blocks to derive the model for predicting scores: $S_{\theta^t}(x^{t-1}_d, x^{t}_{\leq d}) \approx \nabla_x \log p(x^{t-1}_d|x^t_d, x^t_{<d})$, $S_{\theta^t}(x^{t-1}_d, x^{t}_{d}) \approx \nabla_x \log p(x^{t-1}_d|x^t_d)$.
Accordingly, the estimation of $c$ is implemented by \cref{algo_DCM}.
Based on this text encoding, we can construct an adapter to integrate decoder-only LLMs into existing diffusion models. %, enhancing their language comprehension, knowledge base, and logical reasoning capabilities. 
In contrast to the primary practice of merely employing LLMs for text optimization and encoding optimized texts via a text encoder with inherent performance limitations, text encodings derived from LLMs to control the generation of diffusion models can be a superior alternative for diffusion model training from scratch or adaption in a pre-trained diffusion model. In the following, we will elaborate an effective adaptor in a pre-trained diffusion model for image generation.

\subsection{LLMDiff Adapter: Bridging Decoder-Only LLMs and Pre-trained Diffusion Models}
%In order to leverage the pre-training knowledge embedded in existing diffusion models more effectively, we introduce the LLMDiff Adapter.
To leverage the pre-trained knowledge of existing diffusion models more effectively, we propose an LLMDiff Adapter incorporating text encoding from generative decoder-only LLMs into a pre-trained text-to-image diffusion model, as illustrated in \cref{fig:Adapter}. The original cross-attention module is aligned with the preceding text encoder, and it is what actually imposes a bottleneck on the comprehension of user prompts. 
%However, there still many beneficial insights for text-to-image generation are learned during the pre-training phase. 
However, it still holds a wealth of knowledge and insights for text-to-image generation, learned during the pre-training phase.
Therefore, we keep the original cross-attention module intact and align it with the encoding derived from LLMs through linear layers. 
%This alignment allows us to harness the extensive pre-training knowledge of the original model, preserving ample generative capacity.
This enables effective utilization of the knowledge of large-scale pre-trained models, preserving basic generation capabilities.

Simultaneously, an additional cross-attention module is introduced to learn how to better generate images based on the text encoding derived from LLMs. The outputs of these two modules are combined through a set of learnable weight factors: $a_1$, $a_2$, $b_1$, and $b_2$, and the overall computation can be formulated as follows:
\begin{equation}
\begin{aligned}
f = attn\bigl(\hat{\tau_q}(q), \hat{\tau_k}(\phi(\bm c)), \hat{\tau_v}(\phi(\bm c))\bigr) a_1e^{b_1} + \\ attn\bigl(\tau_q(q), \tau_k(\bm c), \tau_v(\bm c)\bigr) a_2e^{b_2},
\end{aligned}
\end{equation}
where $\hat{\tau}$ is the linear layer of the original cross-attention module, $\tau$ is that in additional cross-attention module, and $\phi$ is the linear layer to align the LLMs with the original cross-attention module.
For training stability, the initial values of $a_1$ and $b_1$ are set to 1 and 0, respectively, while $a_2$ and $b_2$ start at 0. 

During the model learning, the newly added cross-attention module gradually refines the outputs, effectively adapting the knowledge of generative LLMs to the diffusion model.
% This integration enhances the quality of generated images by providing improved details, stronger logical coherence, and fidelity.
Our Adapter is trained with the MSE loss for diffusion models:
\begin{equation}
\begin{aligned}
\mathcal{L} = \| \epsilon_{\theta}(z_t, \bm c) - \epsilon \|^2,
\end{aligned}
\end{equation}
where $\epsilon_{\theta}$ is the diffusion U-Net, $z_t$ is the latent feature map at timestep $t$, and $\epsilon \sim \mathcal{N}(0, I)$.
