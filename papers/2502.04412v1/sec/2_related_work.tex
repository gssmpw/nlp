\section{Related Works}
\label{sec:related}

\subsection{Text-to-Image Diffusion Models}
Recently, diffusion-based image generation models have achieved remarkable success. These models learn to iteratively denoise a noisy image and generate the image progressively~\cite{DDPM}.
% The first approach, known as the DDPM~\cite{DDPM}, iteratively denoises a noisy image and generates the image progressively.
Compared to GAN-based methods, diffusion models are more stable in training and able to generate more diverse images.
% However, the generation process of DDPM requires thousands of iterations, making it impractical to reality scenarios. To address this limitation, several algorithms such as DDIM~\cite{DDIM}, K-Diffusion~\cite{?}, and DPM solver~\cite{?} have been developed to accelerate the sampling and denoising process of DDPM.
% 
With the advent of diffusion models incorporating guidance mechanisms~\cite{guid_cls, guid_cls_free}, there has been a notable advancement in the performance of diffusion models. For the first time, diffusion models beat GANs in conditional generation tasks. 
Ever since, the focus of research on text-to-image synthesis has gradually shifted from GAN to Diffusion~\cite{GLIDE, RiFeGAN, ZeroShotGen, ctrlGAN}. Some large-scale text-to-image models~\cite{Dalle3, CogView, Parti, LDM} have achieved highly accurate and fine-grained controllable semantic generation. The recently proposed latent diffusion model (LDM)~\cite{LDM} unprecedentedly makes high-resolution and high-quality text-to-image models become a reality. Based on LDM, DALL-E 3~\cite{Dalle3} has ushered the text-to-image models into unprecedented levels, leveraging powerful text encoders and high-quality data.
% \xiaoyao{However, existing text-to-image models ...}

\subsection{Large Language Models}
In recent years, large-scale language models with billions of parameters have demonstrated remarkable performance across various natural language understanding and generation tasks. 
The dominant form of language models shifted from BERT-like models~\cite{BERT, RoBERTa} that focus on language understanding to the currently prevalent generative language models with decoder-only architectures~\cite{Llama2, GPT4, phi1.5, PaLM}. These decoder-only models have successfully unified a wide spectrum of tasks, showing commendable proficiency in dialogue interactions. 
Even in language comprehension tasks, \cite{CLIP-BERT, MMLU_paper} also shows that CLIP and BERT style text encoders perform worse than decoder-only LLMs.
Moreover, recent models exhibit the ability of in-context learning~\cite{LLMfewshot}, enabling them to adaptively leverage contextual information to accomplish downstream tasks. 

\subsection{LLMs for Text-to-Image Generation}
Existing text-to-image diffusion models are primarily based on encoder-structured text models like CLIP and T5. However, there are ongoing efforts of works that seek to explore the potential for transposing the wealth of knowledge inherent in Large Language Models (LLMs) into existing diffusion frameworks. Certain research endeavors, such as \cite{LLM_Grounded, LLM_Blueprint}, have attempted to utilize LLMs to predict the layout of objects, thereby enhancing the logical coherence and overall quality of the images produced by diffusion models. This is achieved by employing LLMs to rewrite the prompts, ensuring a better alignment between the generated images and the input text. Another approach~\cite{PromptCrafter} attempts to use LLMs to help users construct better prompts, leveraging the capabilities of LLMs to generate superior images.
%to directly align the feature spaces of the text encoder and the LLM. It employs the LLM to expand simple descriptions into more complex ones, subsequently aligning the features of simple descriptions with those of complex ones.

While these pioneering efforts are indeed instrumental in integrating the knowledge of LLMs into diffusion models, they predominantly employ indirect methods to bridge the gap between them, and thus, are inherently constrained by the limitations of the inefficient text encoder. In contrast, we propose a novel method that directly integrates the output of the LLM into the existing diffusion model. By completely discarding the text encoder, we aim to liberate the text-to-image diffusion models from the bottleneck of language comprehensibility, which may significantly enhance their performance in controllable image generation.