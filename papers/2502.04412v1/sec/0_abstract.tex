

\begin{abstract}
%Recently, diffusion models have rapidly evolved, becoming a dominant force in the realm of image generation. Especially for the text-to-image generation task, substantial progress has been achieved, enabling the generation of high-quality images based on textual descriptions. 

Groundbreaking advancements in text-to-image generation have recently been achieved with the emergence of diffusion models. These models exhibit a remarkable ability to generate highly artistic and intricately detailed images based on textual prompts. However, obtaining desired generation outcomes often necessitates repetitive trials of manipulating text prompts just like casting spells on a magic mirror, and the reason behind that is the limited capability of semantic understanding inherent in current image generation models. Specifically, existing diffusion models encode the text prompt input with a pre-trained encoder structure, which is usually trained on a limited number of image-caption pairs. The state-of-the-art large language models (LLMs) based on the decoder-only structure have shown a powerful semantic understanding capability as their architectures are more suitable for training on very large-scale unlabeled data. 
In this work, we propose to enhance text-to-image diffusion models by borrowing the strength of semantic understanding from large language models, and devise a simple yet effective adapter to allow the diffusion models to be compatible with the decoder-only structure. Meanwhile, we also provide a supporting theoretical analysis with various architectures (e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive empirical evaluations to verify its effectiveness. 
%In the evaluation, we conduct not only extensive empirical results but also the supporting theoretical analysis with various architectures (e.g., encoder-only, encoder-decoder, and decoder-only). 
The experimental results show that the enhanced models with our adapter module are superior to the stat-of-the-art models in terms of text-to-image generation quality and reliability.





% In the rapidly evolving landscape of image generation, diffusion models have emerged as a dominant force, particularly in the text-to-image generation domain, facilitating the production of high-quality images from textual descriptions. 
% %
% However, existing models rely on relatively simple language models with encoder structures like the text encoders of CLIP or T5. In contrast, the state-of-the-art Large Language Models (LLMs) primarily employ decoder-only structures, offering considerable generative prowess.
% %Regrettably, existing text-to-image diffusion models have yet to effectively harness the immense comprehension capabilities and logical reasoning inherent in these advanced LLMs.
% Regrettably, the potential of these advanced LLMs, particularly their superior comprehension and logical reasoning capabilities, remains underexploited in current text-to-image diffusion models.
% To bridge this gap, we propose the LLMDiff-Adapter approach, 
% which demonstrates the potential of decoder-only generative LLMs within text-to-image diffusion models, 
% enabling these models to effectively leverage the extensive prior knowledge and logical reasoning abilities inherent in LLMs.

%In the rapidly evolving landscape of image generation, diffusion models have emerged as a dominant force, particularly in the text-to-image generation domain, facilitating the production of high-quality images from textual descriptions. However, existing models predominantly utilize relatively simplistic language models with encoder structures, akin to the text encoders of CLIP or T5. This stands in contrast to the state-of-the-art Large Language Models (LLMs), which primarily adopt decoder-only structures, thus providing considerable generative prowess.

%Unfortunately, the potential of these advanced LLMs, particularly their superior comprehension and logical reasoning capabilities, remains underexploited in current text-to-image diffusion models. To address this deficiency, we introduce the LLMDiff-Adapter methodology. This novel approach illustrates the untapped potential of decoder-only generative LLMs within the text-to-image diffusion paradigm, thereby enabling these models to effectively harness the extensive prior knowledge and logical reasoning capacities inherent in LLMs.

\end{abstract}