\section{Experiments}

\subsection{Experimental Settings}
\noindent
\textbf{Dataset.}
We utilized a subset of data collected from GRIT~\cite{GRIT} and midjourney-v5-202304-clean~\cite{mjv5-data}. Simple filtering was applied to the image resolution and texts,  with the total number of data used for training approximating 1 million. To ensure a fair comparison, our model was trained alongside existing text-encoder-based models (including SD1.5~\cite{LDM}, SDXL~\cite{SDXL}, and T5-based SD1.5 models) using the same dataset and similar Adapters.

\noindent
\textbf{Base models.}
Our experiments are conducted based on pre-trained Stable Diffusion (SD) 1.5 model~\cite{LDM}, utilizing two Large Language Models (LLMs), Phi1.5~\cite{phi1.5} and Vicuna1.5-7B~\cite{vicuna2023}. The number of parameters of Phi1.5 is close to that of the text encoders of CLIP and T5, thereby ensuring a fair comparison of the performance.

\noindent
\textbf{Implementation details.}
Our LLMDiff Adapter has approximately 45M parameters. We utilize AdamW optimizer with a learning rate of 1e-5 for the Adapter training. The size of input images is 512x512, in conjunction with the Aspect Ratio Bucket, which automatically groups images of different aspect ratios into different batches and seeks to avoid image cropping as much as possible. The weighted coefficients of the two cross attentions are initialized as follows: $a_1=1$, $b_1=0$, $a_2=0.1$, $b_2=0$. LLM itself does not require fine-tuning  %its output can be cached in advance, thereby not consuming training resources, 
and we use a batch size of 256 for training on 8 NVIDIA A100 GPUs with 40GB VRAM.

\begin{table}
\vspace{-2pt}
% \vspace{0mm}
% \setlength{\belowcaptionskip}{-1cm}
\caption{Quantitative analysis of our LLMDiff Adapter compared with existing methods.}
\label{tab:exp}
\vspace{-5pt}
\centering
% \renewcommand\arraystretch{0.9}
% \setlength\tabcolsep{5pt}
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{2.6pt}    
% \setlength{\abovecaptionskip}{-2cm}    \setlength{\belowcaptionskip}{-5cm} 
\resizebox{0.47\textwidth}{!}{%
% \small
\begin{tabular}{c|cccc} 
    \toprule
    Method & SigLIP Score $\uparrow$ & Quality$\uparrow$ & Complexity$\uparrow$ & Beauty$\uparrow$ \\ \midrule
    SD1.5  & 4.6          & 74.1    & 23.2       & 88.9      \\ \hline
    SDXL   & 6.2          & 76.5    & 23.9       & 90.6      \\ \hline
    SD1.5 +(T5-XL)     & 7.4          & 74.9    & 22.5       & 90.9  \\ \hline
    Ours (phi1.5) & 5.8          & 76.3    & \textbf{24.9}       & 91.0    \\ \hline
    Ours (Vicuna-7B) & \textbf{8.5}          & \textbf{78.6}    & 24.7       & \textbf{92.9}      \\
    \bottomrule
\end{tabular}
}
\vspace{-5pt}
\end{table}

\noindent
\textbf{Metrics.}
We assess the models from three dimensions.
(1) For \textit{controllability}, we evaluate the degree of matching between the generated images and the given text via the CLIP Score. However, since the SD model itself is based on CLIP, for fairness, we employ the SigLIP-L-384~\cite{SigLIP} model to calculate the SigLIP Score:
\begin{equation}
Score(I, L) = 100 \times sigmoid(\alpha cos(f_{img}(I), f_{text}(L)) + \beta),
\end{equation}
where $I$ is the input image, $L$ is the input text, $f_{img}$ is the image encoder, and $f_{text}$ is the text encoder.  $\alpha$ and $\beta$ are the learned parameters from SigLIP model.
(2) For \textit{image quality}, we utilize CLIP-IQA~\cite{CLIP-IQA} to evaluate the quality of the images from the aspects of image details and overall image quality. 
(3) As for the \textit{logicality} of images, we employ the user study. 
% Originating from various logical reasoning capabilities, we have provided a set of specialized prompts. These prompts serve as a tool to evaluate whether the generated images manifest the implicit logic embedded within the prompts.
For each model, we construct 15 prompts from multiple perspectives, including action logic, color matching, and the number of objects, etc. Each prompt generates 10 images, and human evaluators judge whether the core logic of these prompts is reflected in the images.


\subsection{Quantitative Analysis}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{imgs/exp.pdf}
    \vspace{-10pt}
    \caption{In comparison with existing approaches, LLMDiff exhibits superior capabilities in both language comprehension and action understanding. Furthermore, it is proficient in generating images with high-quality details. %The prompt of the last column is the same as the right column in \cref{fig:head}. 
    }

    \label{fig:exp}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{imgs/exp_reason.pdf}
    \vspace{-10pt}
    \caption{Model evaluation on the capability of causal and logical reasoning for text-to-image generation.}

    \label{fig:exp_reason}
    \vspace{-10pt}
\end{figure*}

For a quantitative analysis of our method, we use the SigLIP Score to evaluate how well the generated images match the given text. Furthermore, we use CLIP-IQA to analyze the image's Quality, Complexity, and Beauty, thereby assessing whether the overall quality of the generated image is better. 

In \cref{tab:exp}, our proposed method based on Vicuna-7B achieved a SigLIP Score of 8.5, which is 31\% higher than the best existing SDXL model of 6.2. Meanwhile, the model based on phi-1.5 has a 26\% improvement compared to the SD1.5 used as our baseline, approaching the level of SDXL. These results suggest that our LLMDiff Adapter can effectively combine the existing LLM and Diffusion models. Allowing the powerful text comprehension capabilities of the LLM to be utilized in the text-to-image diffusion model, thereby generating images with sufficient controllability. The more powerful the LLM, the stronger the controllability it brings.

Regarding the quality of the generated images in \cref{tab:exp}, our method surpasses existing methods in multiple aspects such as overall image quality, complexity of details, and aesthetic appeal of the image. The Quality score reached 78.6, improving by 2.7\% compared to the currently best SDXL model. In terms of the complexity of the generated image features, the Complexity score reached 24.7, with an improvement of 3.3\% compared to SDXL. The aesthetic appeal score of the image also reached 92.9, surpassing the existing SDXL by 2.5\%. By controlling the generation process of the Diffusion model through the LLM model, we can not only improve the alignment between the image and the text, but also enhance the quality, detail, and aesthetic appeal of the generated images.

\subsection{Qualitative Evaluation}
%\pengxuc{change the order of sec.5.2 and sec.5.3}


% Our model is qualitatively evaluated from multiple aspects, including the ability to understand action descriptions, entity relationships, spatial structures, and complex descriptions. As can be seen from \cref{fig:exp}, leveraging the potent semantic comprehension capabilities of the Large Language Models, our approach produces a more precise and controllable result when describing multiple entities and their interrelationships. For instance, in the first column, our method can accurately generate an image of a white rabbit on a wooden bench and two blue cats next to it, demonstrating a more precise understanding of entity quantity and color correspondence. Moreover, it can accurately comprehend the action where only one cat is looking at the white rabbit.
% In contrast, existing methods struggle to understand inter-entity relationships, like actions.
%Our model is evaluated qualitatively from several aspects, encompassing its capacity to comprehend actions, entity relationships, spatial structures, and complex descriptions. As depicted in \cref{fig:exp}, our approach, powered by the robust semantic comprehension capabilities of LLMs, yields a more precise and controllable outcome in the portrayal of multiple entities and their inter-relationships. For instance, in the first column of \cref{fig:exp}, our method accurately generates an image of a white rabbit seated on a wooden bench with two blue cats next to it, demonstrating a refined understanding of entity quantity and color correspondence. Furthermore, it exhibits a precise comprehension of the action wherein only one cat is looking at that white rabbit. In contrast, existing methods struggle to understand inter-entity relationships, like actions.
Our model is evaluated qualitatively on its ability to understand actions, entity relationships, spatial structures, and complex descriptions. As shown in \cref{fig:exp}, our approach,  powered by the robust semantic comprehension of LLMs, delivers more precise and controllable outcomes in depicting multiple entities and their inter-relationships. For instance, in the first column of \cref{fig:exp}, our method accurately generates an image of a white rabbit seated on a wooden bench with two blue cats next to it, showcasing a refined understanding of entity quantity and color correspondence. It also correctly comprehend the action where only one cat looks at the rabbit, unlike existing methods that struggle with such inter-entity actions.

%The results in the third column further illustrate that our method can accurately generate the three different entities specified in the description, without any feature confusion. In contrast, other models tend to focus on keyword comprehension, struggling to understand the relationship between different parts of the sentence as a whole. We described three entities: a robot, a robot with a rat head, and a rat. If one only focuses on keywords, it is easy to overlook some entities, as there is overlap between the keywords. Existing models based on text encoders like those of CLIP or T5 fail to accurately understand these relationships between entities and cannot generate images precisely based on the entity descriptions with keyword overlaps.
%The third column further underscores our method's ability to accurately generate distinct entities as specified in the description, without any feature confusion. Instead, existing models tend to prioritize keyword comprehension and struggle to understand the holistic context of the sentence. We described three entities: a robot, a robot with a rat head, and a rat. A keyword-based approach risks overlooking some entities due to keyword overlap. Existing models leveraging text encoders like those of CLIP or T5 fail to accurately understand these inter-entity relationships and fail to generate images precisely when entity descriptions have keyword overlaps.
The third column highlights our method's ability in accurately generating distinct entities from descriptions without feature confusion. Existing models often focus on keywords and miss the holistic context, failing with overlapping keywords. For instance, with entities like a robot, a robot with a rat head, and a rat, keyword-based approach risks overlooking some entities due to overlaps. Models leveraging text encoders like CLIP or T5 cannot accurately interpret these relationships, resulting in imprecise image generation.

%In terms of generating complex scenes from long text descriptions, existing methods struggle to accurately comprehend the provided long text, and the features of the generated scenes often do not match the description, missing a lot of features. Our method, instead, releases the power of LLMs into the diffusion model, and leverages the powerful long-text comprehension capabilities of LLMs to precisely depict each part of the scene, as well as the relationships between entities. For example, in the last column of \cref{fig:exp}, our method can precisely depict the complex indoor scene from a long text, including the pink rose, three lemons, a sofa and pillows on it, and the interior decoration. In contrast, existing methods fail to accurately generate these complex features. Most existing methods only generate relatively simple room scenes, with a paucity of detailed features.
%When it comes to generating complex scenes from extensive text descriptions, prevailing methods struggle to accurately comprehend the provided long text, resulting in generated scenes that often diverge from the description, omitting numerous features. Our method, instead, integrates the power of LLMs into the diffusion model, exploiting LLMs' powerful long-text comprehension capabilities to precisely delineate each part of the scene, as well as the inter-entity relationships. For instance, as indicated in the last column of \cref{fig:exp}, our method can accurately render the complex indoor scene described in the extensive text, encompassing the pink rose, three lemons, a sofa with pillows, and the interior decoration. In contrast, existing methods fall short in accurately generating these complex features, typically producing simpler room scenes with a dearth of detailed features.
For complex scenes derived from extensive text descriptions, prevailing methods often misinterpret long texts, leading to incomplete scene generation. Our method integrates LLMs into the diffusion model, exploiting LLMs' powerful long-text comprehension capabilities to precisely delineate each scene part and inter-entity relationships. In the last column of \cref{fig:exp}, our method precisely renders a complex indoor scene described in extensive text, including the pink rose, three lemons, a sofa with pillows, and interior decoration. In contrast, existing methods typically produce simpler, less detailed scenes.

%Simultaneously, our method is capable of generating a richer array of meaningful detail features. Our approach is based on the SD1.5 model, but with the powerful comprehension capabilities of the LLM, the LLMDiff Adapter can significantly enhance the overall texture and detail quality of the generated images. The features generated by SD1.5 are generally fragmented, comprising indistinguishable local features that are difficult to understand. In contrast, our method can generate features with stronger coherence and more meaningful local features, especially in complex scenes described by long text.
%Furthermore, our method exhibits promising abilities in generating various meaningful detailed features. Our approach is based on the SD1.5 model, but when combined with the remarkable comprehension capabilities of LLM, our LLMDiff Adapter can significantly enhance the overall texture and detail quality of the generated images. The features generated by SD1.5 are typically fragmented, encompassing indistinguishable local features that are challenging to understand. In contrast, our method is capable of generating features with better coherence and more meaningful local features, particularly in complex scenes described by long texts.
Moreover, our method excels in generating meaningful detailed features. Based on the SD1.5 model, our LLMDiff Adapter, combined with LLMs' powerful comprehension capabilities, significantly enhances the texture and detail quality of generated images. While SD1.5 often produces fragmented and indistinct features, our approach generates coherent and meaningful local features, especially in complex scenes described by long texts.



\subsection{Analysis of Reasoning Ability}
% Large-scale pre-trained generative LLMs have better commonsense reasoning and logical reasoning capabilities compared to other language models with encoder structures. LLMs can infer the information implied in the known conditions and infer unknown information that is not explicitly given. These capabilities can also be brought to the diffusion model.
Existing text-to-image generation models tend to produce visually highly similar image details with the given texts. 
%Instead, it is rather blind whether they can generate image details that texts are not included and need commonsense or reasoning. 
They struggle to generate image details that texts do not explicitly indicate but are necessary for commonsense or reasoning.
In our experiments, this is also taken into consideration for model evaluation, in \cref{fig:exp_reason}. 


In the first two columns of \cref{fig:exp_reason}, our model can accurately understand the number of entities in the description, which is a great challenge for current diffusion models based on text encoders. With the help of LLM's understanding of quantifiers and entity relationships, we can enable the diffusion model to accurately generate the number of entities given in the description text.

%In the third column, it is expected that the model generates a picture of what a cat would see whether it falls from a great height. It is unfavoured to observe a cat in the image, and it would be better to present what this cat sees. This requires the model to have sufficient logical reasoning ability. If the model simply understands the sentence in a keyword manner, it is easy to generate a picture of a cat falling from a great height, which is not what we want. Existing models based on text encoders do not have this logical reasoning ability. Our method based on LLM, instead, can understand this well, generating the picture that the cat sees, not the picture of the cat falling, even though there is the keyword ``cat" in our description.
% In the third column of \cref{fig:exp_reason}, the model is tasked with generating an image from the perspective of what a cat would see when it falls from a great height. The subject is not the cat itself, but rather the scene as viewed by the cat. This requires the model have enough capacity of logical reasoning. 
% %A simplistic keyword-based understanding of the sentence can easily lead to generating an image featuring a cat falling from a great height, which deviates from the intention of the prompt.  
% A model that primarily relies on keywords to interpret sentences can easily produce an image of a cat falling from a great height.
% % Existing models based on text encoders lack this logical reasoning capacity. 
% Conversely, our method, grounded in LLMs, comprehends user intent well and generates the scene as perceived by the cat, not an image of the falling cat, despite the presence of the keyword ``cat" in our description.
The third column of \cref{fig:exp_reason} illustrates our modelâ€™s advanced reasoning abilities. Tasked with generating an image from the perspective of a falling cat, our model accurately depicts the scene as seen by the cat, rather than mistakenly showing a falling cat, as would be typical of models focusing on keywords only.

%From the fourth column, we can see that our model can be well utilized for LLM's ability to infer physical laws. We hope to generate a picture of a glass ball falling from a high place. According to physical laws, this glass ball will definitely shatter, there will definitely be many cracks inside, and the cracks should spread from the bottom up, and there should be some fragments produced by the breaking of the glass ball around. Existing methods do not have this reasoning ability, the glass balls they generate do not shatter, and they are still a complete glass ball, which does not conform to objective physical laws. Our method can infer these conclusions well, can accurately generate the features implied in the description, and can generate a broken glass ball well, and a glass ball that starts to shatter from the bottom due to impact. These are things that diffusion models based on text encoders cannot understand.
%The fourth column illustrates how our model effectively leverages the LLM's capability to infer the physical rules. The task is to generate an image of a glass ball falling from a great height. In accordance with physical rules, the glass ball will inevitably shatter upon impact, making a lot of internal cracks that should propagate upwards from the point of impact, surrounded by fragments produced by the shattering of the glass ball. Existing methods lack this reasoning ability: the glass balls they generate remain intact, contradicting objective physical rules. Our method, instead, is able to draw these inferences accurately, generating the features implied in the description, including a shattered glass ball and a glass ball that begins to fracture from the bottom due to the impact. These are what text-encoder-based diffusion models fail to comprehend.

The fourth column illustrates how our model effectively leverages the LLM's capability to infer the physical laws. The task is to generate an image of a glass ball falling from a great height. Our model accurately generates an image of a glass ball shattering upon impact, consistent with real-world physics, unlike existing models that depict an intact glass ball. These are what text-encoder-based diffusion models fail to comprehend, as they lack this reasoning ability.

% From the last column, we can see that our model can also make good use of LLM's own imagination ability of functions. We hope that the model will generate a pig that can fly. This is a description of the pig's own abilities, not a description of the pig's state or actions. So we hope that the model generates an animal whose body structure can fly, and it has the primary features of a pig. The existing models all generate a pig flying in the sky, and even the SD model simply draws a pig in the sky, without any imagination about the function of flying. 
% Our model, instead, can understand the user's intention in the text description well. Our model does not draw a pig in the sky, but imagines it based on LLM's understanding of the function of flying in its knowledge base. Common flying animals are birds, so our model gives some features of birds to the pig. The pig's ears become larger and can perform functions similar to wings. It has just two feet and is smaller in size, which are common features of birds. Our model can imagine that animals with these features have the function of flying. And our model imagines based on some features of the pig itself, rather than creating a pair of wings that do not conform to the real world logic and 
% directly adding them to the pig.

The last column reveals our model's capacity to utilize LLM's inherent imagination ability and understanding of functions. The goal is to create a pig that can fly, focusing on its inherent ability to fly rather than its state or actions. It is expected that the generated image should depict an animal with a pig's primary features but a body structure adapted for flight. 
%Contrary to existing models that depict a pig merely flying or floating in the sky, 
Existing models all generate a pig flying in the sky, and particularly, SD models simply draw a pig floating in the sky, without any imagination about the function of flying. 
Our model, instead, infers the user's intention from the text and conceptualizes the ability to fly based on LLM's knowledge base. It borrows structure characteristics from common flying animals, like birds, and integrates them into the pig. The pig's ears evolve into larger structures resembling wings, and it reduces to two feet and a smaller size, which are typical bird traits. Our model thus imagines a pig with flying capabilities rooted in real-world logic and pig features, rather than forcibly attaching wings.

\subsection{Analysis of Scaling Factors}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.40\textwidth]{imgs/scale.pdf}
    \vspace{-3ex}
    \caption{The scale factor of newly added attentions and the original attentions in each cross-attention module of U-Net.}

    \label{fig:scale}
    %\vspace{-16pt}
\end{figure}

According to the results in \cref{fig:scale}, which shows the weight distribution across various layers in the new and original cross-attentions, a substantial portion of the original knowledge within the model is preserved. The preservation is less in layers with a higher number of parameters, while layers at both ends, which have fewer parameters, retain more. The newly added rectification module primarily operates in the decoder part of the U-Net.

\section{Conclusion and Limitations}
% We have view the generative LLMs with a Transformer-based Decoder-only structure as a diffusion model, from which we sample to obtain implicit text encodings that can be used for image generation. Based on these text encodings, we have designed the LLMDiff Adapter to integrate them into a text-to-image diffusion model. This significant enhancement of the model's controllability has also improved its reasoning capabilities pertaining to common sense, logic, and physical laws. The generate images is more realistic, with improved detail and quality. Furthermore, our approach surpasses existing methods that based on text encoders in various quantitative evaluation metrics. 
In this paper, we have viewed generative LLMs with a transformer-based decoder-only structure as a diffusion model, and thus we can sample implicit text encodings for image generation. We propose an LLMDiff Adapter to incorporate these encodings into a text-to-image diffusion model, enhancing the model's controllability and reasoning abilities, logic, and physics. The generated images are more realistic, with improved detail and quality. Moreover, our method outperforms existing text-encoder-based methods in various quantitative metrics.

\noindent
\textbf{Limitations.} Our method requires the output from each Transformer block of LLM, and thus it is incompatible with closed-source models like GPT-4 and Claude~3. 