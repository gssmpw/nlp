\section{Experiments}
\label{sec:experiments}
We validate BEVDiffuser on 3D object detection task, the most common task used to evaluate the effectiveness of the learned BEV feature maps \cite{bevformer,liu2023bevfusion,le2024diffuser, zou2024diffbev}. 3D object detection is critical in autonomous driving that requires both semantic and geometric understanding of the environment to identify and locate objects in 3D space. In this section, we first introduce our experimental setting in Sec.~\ref{sec:exp_setting}. In Sec.~\ref{sec:capacity}, we showcase the capacity of BEVDiffuser in denoising and generating BEV feature maps. We further demonstrate plug-and-play performance of BEVDiffuser in Sec.~\ref{sec:pp_performance} by comparing BEVDiffuser enhanced BEV models with their baseline counterparts.



\subsection{Experimental Settings}
\label{sec:exp_setting}

\textbf{Dataset.} We conduct experiments on large-scale nuScenes \cite{nuscenes} dataset. nuScenes is a well-established benchmark for autonomous driving tasks that contains 1,000 20-second driving videos, with keyframes annotated at 2 Hz. Specifically, for 3D object detection task, each keyframe provides six RGB images and a LiDAR scan covering a 360-degree field of view, as well as annotated 3D bounding boxes for objects of interest,  which are categorized by one of 10 predefined object classes. In total, the dataset contains 1.4 million annotated bounding boxes, making it well-suited for object detection task. 

\noindent\textbf{Metrics.} We adopt the official evaluation metrics provided by nuScenes detection benchmark \cite{nuscenes} to evaluate the 3D object detection performance. Specifically, mean average precision (mAP) calculates average precision by defining a true positive based on the 2D center distance between predictions and ground truth. The five true positive metrics, namely ATE, ASE, AOE, AVE, and AAE measure average translation, scale, orientation, velocity, and attribute errors,  respectively. nuScenes detection score (NDS) consolidates all the metrics into a weighted sum.

\noindent\textbf{BEV Models.} We apply BEVDiffuser to four representative and widely adopted BEV models, namely BEVFormer-tiny \cite{bevformer}, BEVFormer-base \cite{bevformer}, BEVFormerV2 \cite{bevformerv2}, and BEVFusion \cite{liu2023bevfusion}. BEVFormer and BEVFormerV2 are transformer-based methods that detect objects from only cameras, while BEVFusion adopts LSS-based method for camera inputs and then fuses camera and LiDAR features for object detection. Comparing to BEVFormer-base, BEVFormer-tiny shortens temporal dependencies and produces much smaller BEV feature maps, thereby requiring less computational cost and enabling fast development. BEVFormerV2 is a two-stage detector where a perspective head is introduced to train the image backbones and generate object proposals for the detection head. To save the computational cost, we adopt its simplest version which involves no temporal information and employs Deformable DETR \cite{zhudeformable} as the detection head.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/bevdiffuser_plot.png}
    \caption{3D object detection performance of various BEV models on nuScenes \texttt{val} dataset (denoising steps $=0$). The performance ramps up when adopting BEVDiffuser to denoise their BEV feature maps with increasing denoising steps, indicating the powerful denoising capability of our BEVDiffuser. }
    \label{fig:bevdiffuser_plot}
\end{figure}

\subsection{Capacity of BEVDiffuser}
\label{sec:capacity}

To validate the capacity of BEVDiffuser, we train BEVDiffuser on BEV feature maps produced by each pretrained BEV model, i.e. BEVFormer-tiny, BEVFormer-base, BEVFormerV2, and BEVFusion, and we denote the trained BEVDiffuser as $\mathrm{BD}^{tiny}$,  $\mathrm{BD}^{base}$, $\mathrm{BD}^{V2}$, and $\mathrm{BD}^{fu}$, respectively. In particular, since the size of the BEV produced by  BEVFormer-base, BEVFormerV2, and BEVFusion is too large that hinders the efficient training of the diffusion models, we attach downsample and upsample layers before and after the diffusion models to reduce and restore the BEV size accordingly. Given that BEVFormer-base and BEVFormerV2 share a similar BEV feature space with BEVFormer-tiny,  we employ the trained $\mathrm{BD}^{tiny}$ as their diffusion models and only train the downsample and upsample layers to get $\mathrm{BD}^{base}$ and  $\mathrm{BD}^{V2}$. 

\begin{table*}[ht!]
\begin{center}
\begin{tabular}{l|cc|cc|ccccc}
\specialrule{0.12em}{0pt}{2pt}
Method & Mod. & BEV Size & NDS$\uparrow$ & mAP$\uparrow$ & mATE$\downarrow$ & mASE$\downarrow$ & mAOE$\downarrow$ &mAVE$\downarrow$ & mAAE$\downarrow$\\
\specialrule{0.12em}{1pt}{1.5pt}
BEVFormer-tiny \cite{bevformer} &C &$50\times50$ &35.5 &25.2 &0.898 &0.293 &0.650 &0.656 &0.216 \\
\textbf{+ BEVDiffuser} &C &$50\times50$ &\textbf{39.1} &\textbf{28.3} &\textbf{0.859} &\textbf{0.285} &\textbf{0.558} &\textbf{0.592} &\textbf{0.212} \\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFormer-base \cite{bevformer} &C &$200\times200$ &51.8 &41.7 &0.673 &0.273 &0.371 &0.393 &0.198 \\
\textbf{+ BEVDiffuser} &C &$200\times200$ &\textbf{53.7} &\textbf{43.0} &\textbf{0.638} &0.274 &\textbf{0.333} &\textbf{0.355} &\textbf{0.179} \\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFormerV2-base$^*$ \cite{bevformerv2} &C &$200\times200$ &41.1 &32.7 &0.768 &0.285 &0.499 &0.780 &0.195 \\
\textbf{+ BEVDiffuser} &C &$200\times200$ &\textbf{44.7} &\textbf{37.1} &\textbf{0.718} &0.286 &\textbf{0.448} &\textbf{0.740} &0.197 \\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFusion$^*$ \cite{liu2023bevfusion} &LC &$180\times180$ &70.9 &67.6 &0.278 &0.253 &0.305 &0.267 &0.188 \\
\textbf{+ BEVDiffuser} &LC &$180\times180$ &\textbf{71.9} &\textbf{69.2} &\textbf{0.276} &\textbf{0.252} &\textbf{0.294} &\textbf{0.266} &\textbf{0.184} \\
\specialrule{0.12em}{1.5pt}{0pt} 
\end{tabular}
\caption{Comparison of 3D object detection performance on nuScenes \texttt{val} dataset. Our BEVDiffuser brings consistent performance improvement to existing BEV models, with notable gains in NDS and mAP.  ``Mod.'' abbreviates modality, where ``L'' and ``C'' denote LiDAR and camera, respectively. ($^*$ : model retrained under the same code base and GPU resources as its counterpart for fair comparison.) }
\label{tbl:val_performance}
\end{center}
\end{table*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/gen_vis.png}
    \caption{3D object detection visualizations of two BEV feature maps generated by our BEVDiffuser ($\mathrm{BD}^{fu}$) from random noise. The alignment between predictions and ground truth demonstrates that BEVDiffuser has strong controllable generation capability.  }
    \label{fig:gen_vis}
\end{figure}

\noindent\textbf{BEV Denoising Capability.} We use the trained BEVDiffuser to denoise the BEV feature maps from each BEV model and assess their 3D object detection performance using the denoised features. Fig.~\ref{fig:bevdiffuser_plot} reports the mAP and NDS achieved on the nuScenes \texttt{val} dataset. Noticeably, the detection performance of all BEV models has been significantly improved after the BEV feature maps are denoised. The performance grows sharply when the number of denoising steps gradually increases to $5$, demonstrating the powerful denoising capability of BEVDiffuser. After denoising the BEV feaure maps for $5$ steps, the performance growth slows down, which is expected since less noise remains. This observation further confirms BEVDiffuser's efficiency in denoising BEV feature maps.


\noindent\textbf{BEV Generation Capability.} BEVDiffuser as a conditional diffusion model is also able to generate a BEV feature map from a conditioning layout. To evaluate its BEV generation capability, we use the trained BEVDiffuser ($\mathrm{BD}^{fu}$) to generate BEV feature maps from random noise $\mathcal{N}(\bm{0}, \bm{I})$, conditioned on the ground-truth layout built from nuScenes \texttt{mini-val} dataset. To speed up the generation process, we adopt DDIM scheduler \cite{songdenoising} to skip steps in denoising process. In practice, we run $50$ denoising steps to generate the BEV feature maps. We further decode the generated BEV feature maps using the pretrained detection head from BEVFusion and achieve $41.1\%$ NDS and $36.7\%$ mAP for detection on nuScenes \texttt{mini-val} dataset. We visualize the detection results from the LiDAR top view in Fig.~\ref{fig:gen_vis}. As shown in the figure, the predictions using the generated BEV feature maps align well with the ground truth, showing the strong controllable generation capability of BEVDiffuser. This capability makes BEVDiffuser even promising in augmenting data for corner cases and developing driving world model \cite{wang2024driving, yang2024generalized, gao2024vista} in the BEV feature space, which we leave for future research.

\begin{table}[b!]
    \centering
    \resizebox{1.0\linewidth}{!}{
     \begin{tabular}{l|cccc}
        \toprule
        Model {\footnotesize(\textbf{+ BEVDiffuser}) } & Mod. & BEV Size & \# Params & FPS \\
        \midrule
        BEVFormer-tiny &  C & $50\times50$ &  33.6 M & 6.0 \\
        \midrule
        BEVFormer-base & C  &   $200\times200$ & 69.1 M  & 2.7 \\
        \midrule
        BEVFormerV2-base & C  & $200\times200$& 56.3 M  & 3.2 \\
        \midrule     
        BEVFusion & LC  & $180\times180$ & 40.8 M & 2.9$^\dag$ \\
        \bottomrule
    \end{tabular}
    }
    \caption{Computational efficiency tested on 1 A100 GPU. Plugging in BEVDiffuser doesn't change the network architecture and therefore maintain the same computational efficiency as the baselines. ($\dag$: tested on official MMCV implementation) } 
    \label{tab:comp_efficiency}
\end{table}

\subsection{Plug-and-Play Performance of BEVDiffuser}
\label{sec:pp_performance}

\begin{table*}[ht!]
\begin{center}
\begin{tabular}{l|cccccccccc}
\specialrule{0.12em}{0pt}{2pt}
Method &\rot{\makecell[l]{Constr. Veh.\\{\small(1.0\%)}}} &\rot{\makecell[l]{Bus\\{\small(1.0\%)}}} &\rot{\makecell[l]{Motorcycle\\{\small(1.2\%)}}} &\rot{\makecell[l]{Bicycle\\{\small(1.3\%)}}} &\rot{\makecell[l]{Trailer\\{\small(1.7\%)}}} &\rot{\makecell[l]{Truck\\{\small(6.5\%)}}} &\rot{\makecell[l]{Traf. Cone\\{\small(10.2\%)}}} &\rot{\makecell[l]{Barrier\\{\small(15.9\%)}}} &\rot{\makecell[l]{Pedestrian\\{\small(18.0\%)}}} &\rot{\makecell[l]{Car\\{\small(43.1\%)}}} \\
\specialrule{0.12em}{1pt}{1.5pt}
BEVFormer-tiny \cite{bevformer} &5.8 &23.4 &21.4 &20.3 &6.6 &19.2 &38.4 &37.9 &33.2 &45.7 \\
\textbf{+ BEVDiffuser} &\textbf{7.2} &\textbf{30.3} &\textbf{26.9} &\textbf{24.0} &\textbf{8.2} &\textbf{22.8} &\textbf{40.7} &\textbf{40.0} &\textbf{34.8} &\textbf{48.1} \\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFormer-base \cite{bevformer}  &12.9 &44.5 &43.0 &39.8 &17.2 &37.0 &58.5 &52.6 &49.4 &61.9 \\
\textbf{+ BEVDiffuser} &\textbf{13.5} &\textbf{47.1} &\textbf{44.8} &\textbf{41.7} &\textbf{18.0} &\textbf{37.2} &\textbf{59.6} &\textbf{55.6} &\textbf{50.3} &61.8\\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFormerV2-base$^*$ \cite{bevformerv2}  &3.4 &33.7 &29.8 &25.6 &7.5 &26.5 &52.4 &50.1 &42.8 &55.5 \\
\textbf{+ BEVDiffuser} &\textbf{6.4} &\textbf{41.8} &\textbf{35.1} &\textbf{30.1} &\textbf{11.8} &\textbf{32.0} &\textbf{55.5} &\textbf{54.5} &\textbf{45.0} &\textbf{58.8}\\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFusion$^*$ \cite{liu2023bevfusion}  &29.9 &74.9 &75.3 &60.4 &46.7 &62.4 &79.3 &70.2 &88.1 &89.3\\
\textbf{+ BEVDiffuser} &\textbf{30.9} &\textbf{76.6} &\textbf{76.9} &\textbf{63.3} &\textbf{48.4} &\textbf{65.2} &\textbf{79.9} &\textbf{72.9} &\textbf{88.3} &\textbf{89.5} \\
\specialrule{0.12em}{1.5pt}{0pt} 
\end{tabular}
\caption{Per-class object detection results (mAP) on nuScenes \texttt{val} dataset. Note that object classes are sorted based on the percentage of their occurrences in the dataset (shown under the class names). BEVDiffuser exhibits overall improvements across all classes, with more significant gains on long-tail objects that appears only 1-2$\%$ in the dataset, such as \textit{construction vehicle} and \textit{bus}. }
\label{tbl:class_performance}
\end{center}
\end{table*}

\begin{table*}[ht!]
\begin{center}
\begin{tabular}{lcccccccccccc}
\specialrule{0.12em}{0pt}{2pt}
& &\multicolumn{2}{c}{Sunny}&& \multicolumn{2}{c}{Rainy}&& \multicolumn{2}{c}{Day}&& \multicolumn{2}{c}{Night}\\
\cline{3-4}\cline{6-7}\cline{9-10}\cline{12-13}
Method & Mod. & NDS$\uparrow$ & mAP$\uparrow$ && NDS$\uparrow$ & mAP$\uparrow$ &&NDS$\uparrow$ & mAP$\uparrow$ &&NDS$\uparrow$ & mAP$\uparrow$\\
\specialrule{0.12em}{1pt}{1.5pt}
BEVFormer-tiny \cite{bevformer} &C &34.9 &25.0 &&37.7 &26.9 &&35.8 &25.6 &&18.1 &9.5 \\
\textbf{+ BEVDiffuser} &C &\textbf{38.4} &\textbf{28.0} &&\textbf{42.2} &\textbf{30.1} &&\textbf{39.4} &\textbf{28.7} &&\textbf{19.5} &\textbf{11.4} \\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFormer-base \cite{bevformer} &C &50.9 &41.1 &&55.2 &43.8 &&52.0 &41.9 &&28.4 &21.1 \\
\textbf{+ BEVDiffuser} &C &\textbf{52.9} &\textbf{42.4} &&\textbf{56.5} &\textbf{45.2} &&\textbf{54.0} &\textbf{43.3} &&\textbf{30.4} &\textbf{22.6} \\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFormerV2-base$^*$ \cite{bevformerv2} &C &40.2 &32.7 &&44.8 &31.7 &&41.5 &33.3 &&18.6 &11.4 \\
\textbf{+ BEVDiffuser} &C &\textbf{43.4} &\textbf{36.4} &&\textbf{49.4} &\textbf{38.8} &&\textbf{45.1} &\textbf{37.7} &&\textbf{21.2} &\textbf{14.7} \\
\specialrule{0.01em}{1pt}{1.5pt}
BEVFusion$^*$ \cite{liu2023bevfusion} &LC &70.5 &67.0 &&72.8 &69.4 &&71.1 &67.7 &&44.0 &39.9 \\
\textbf{+ BEVDiffuser} &LC &\textbf{71.5} &\textbf{68.9} &&\textbf{72.9} &\textbf{69.6} &&\textbf{72.0} &\textbf{69.3} &&\textbf{45.2} &\textbf{41.3} \\
\specialrule{0.12em}{1.5pt}{0pt} 
\end{tabular}
\caption{Object detection performance on nuScenes \texttt{val} dataset under different weather and lighting conditions. BEVDiffuser consistently improves upon its baseline counterparts in all scenarios across all metrics. In particular, we observe significant improvements at night scenarios, when poor lighting conditions pose a significant challenge for camera-based perception.  }
\vspace{-15pt}
\label{tbl:weather_performance}
\end{center}
\end{table*}

BEVDiffuser can be a plug-and-play module for state-of-the-art BEV models without any bells and whistles. Here, we plug the trained BEVDiffuser into the training process of BEVFormer-tiny, BEVFormer-base, BEVFormerV2, and BEVFusion, respectively. We use BEVDiffuser to denoise the existing BEV feature maps for $5$ steps and train new BEV models from scratch under the supervision of the denoised feature maps to get the BEVDiffuser enhanced models. We compare the BEVDiffuser enhanced models with their baseline counterparts to assess the plug-and-play performance of the BEVDiffuser.

\noindent\textbf{3D Object Detection Comparison.} We report the 3D object detection performance of all models achieved on nuScenes \texttt{val} dataset in Tab.~\ref{tbl:val_performance}.  As shown in the table, our BEVDiffuser enhanced models consistently outperform their baseline counterparts across almost all the metrics, especially in NDS and mAP. Notably, our BEVDiffuser enhanced BEVFormer-tiny raises NDS and mAP by $10.1\%$ and $12.3\%$ respectively. Similarly, BEVDiffuser boosts BEVFormerV2 by achieving $8.8\%$  and $13.5\%$ improvement in NDS and mAP. For more complex BEV models, i.e. BEVFormer-base and BEVFusion, where their BEV feature maps have been well learned as shown by their outstanding object detection performance, our BEVDiffuser continues to effectively denoise their BEV feature maps, guide their training process, and consistently improve the performance.

It is worth highlighting that BEVDiffuser brings performance enhancement to BEV models at no cost of any additional adaptation efforts or computational overhead. As a training-only plug-in, BEVDiffuser is removed at deployment, leaving an enhanced BEV model with the architecture unchanged, which is then used for testing. As a result, our BEVDiffuser enhanced models share the same network size and latency as their baseline counterparts which are summarized in Tab.~\ref{tab:comp_efficiency}. Unlike previous work \cite{le2024diffuser, zou2024diffbev} that need to pass their integrated diffusion models multiple times to denoise the BEV feature maps on-the-fly, our method is more flexible and superior in latency-critical applications like autonomous driving. 


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/visualization.png}
    \caption{Visualization results of BEVDiffuser enhanced BEVFormer-tiny on nuScenes \texttt{val} dataset. Compared to the baseline BEVFormer-tiny, BEVDiffuser helps to reduce hallucinations (first three columns) and detect safety-critical objects (last two columns).  }
    \label{fig:vis}
\end{figure*}

\noindent\textbf{Performance on Long-tail Objects.} BEV feature maps, optimized only for downstream task performance, tend to misclassify and overlook underrepresented objects. As illustrated in Tab.~\ref{tbl:class_performance} where per-class object detection results are presented, all baseline models are more effective at detecting the predominant object \textit{car}, compared to long-tail objects like \textit{construction vehicle} and \textit{bus}, which appear only 1-2\% of the time. In contrast, BEVDiffuser denoises BEV feature maps using ground-truth layout as guidance that captures the joint distributions of all objects. As a result, BEVDiffuser exhibits overall improvements across all classes as demonstrated in  Tab.~\ref{tbl:class_performance}. Notably, it achieves more substantial gains for long-tail objects. For example, BEVDiffuser improves BEVFormer-tiny's detection of the long-tail objects, \textit{construction vehicle} and \textit{bus}, with mAP enhancement of $24.1\%$ and $29.5\%$, respectively. BEVDiffuser enhanced BEVFormerV2 also increases mAP by $88.2\%$ and $23.4\%$ for detecting \textit{construction vehicle} and \textit{bus}. The remarkable improvements in long-tail object detection emphasize the enhanced BEV feature maps learned by BEVDiffuser, showing its effectiveness in BEV denoising process.



\noindent\textbf{Robustness Analysis.} We analyze the robustness of the BEVDiffuser under different weather and lighting conditions. From Tab.~\ref{tbl:weather_performance}, BEVDiffuser consistently improves its baseline counterparts for both sunny and rainy, day and night scenarios. Specifically, while poor lighting condition at night poses significant challenge for camera-based perception, BEVDiffuser achieves $20.0\%$ and $28.9\%$ mAP improvements over the baseline BEVFormer-tiny and BEVFormerV2, respectively. In addition, on sunny days, BEVDiffuser also compensates for camera noise caused by overexposure, leading to improved detection performance. BEVFusion, which enhances robustness by using multi-modal sensors, i.e camera and LiDAR, still benefits from BEVDiffuser in challenging weather and lighting conditions. The notable improvements across all scenarios highlight the enhanced robustness delivered by BEVDiffuser. 


\noindent\textbf{Qualitative Results.} Fig.~\ref{fig:vis} depicts how BEVDiffuser improves the 3D object detection performance. We show the ground-truth and the predicted 3D bounding boxes on camera images for comparison. As shown in the first three columns, BEVDiffuser reduces hallucinations generated by the baseline model, BEVFormer-tiny. Taking the second column as an example, BEVFormer-tiny mistakenly detects pedestrians nearby, as indicated by the blue bounding boxes. In comparison, our BEVDiffuser enhanced model effectively resolves such false positive detections. Moreover, BEVDiffuser also helps to minimize false negative detections. As the last two columns demonstrate, our BEVDiffuser enhanced model successfully detects the pedestrian in front of the autonomous vehicle and the car crossing the road, both of which are overlooked by the baseline model but are crucial for ensuring the autonomous vehicle's safe operation. Overall, BEVDiffuser aligns the detections more closely with the ground truth, highlighting its effectiveness in enhancing the quality of the BEV feature maps. We present more qualitative results in supplementary materials.

