\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Model Architecture}
We follow Latent Diffusion Models (LDMs) \cite{rombach2022high} to build a conditional diffusion model as our BEVDiffuser by augmenting the U-Net with cross-attention layers. The cross-attention operation is defined in Equation~\ref{eq:attn}, where $W_*$ represents learnable projection matrices unless otherwise specified, $\varphi_i(\bm{x}_t)$ denotes the intermediate embedding of $\bm{x}_t$ from the $i$-th layer of the U-Net, and $\tau_\theta(y)$ indicates the embedding of the condition $y$. 
\begin{equation}
\label{eq:attn}
\begin{gathered} 
cross\mbox{-}attn(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d}}) \cdot V  \\
Q = \varphi_i(\bm{x}_t)W_Q^i,\, K= \tau_\theta(y)W_K^i,\, V=\tau_\theta(y)W_V^i 
\end{gathered}
\end{equation}

To better fuse the BEV feature map $\bm{x}_t$ and the layout condition $y=l$ and have more control over all the objects specified in the layout, we adopt the global conditioning and the object-aware local conditioning mechanism proposed by \cite{zheng2023layoutdiffusion}. Specifically, we first use a transformer-based layout fusion module $LFM$ as $\tau_\theta$ to get a self-attended embedding $o'_i$ for each object $o_i$ as shown in Equation~\ref{eq:lfm}. In this way, $o'_0$ contains the information of the entire layout and is then added to $\bm{x}_t$ for global conditioning, i.e., $\bm{x}_t' = \bm{x}_t + o'_0W_o $. Meanwhile, the embedding of all the objects $l'=\{o'_i\}_{i=0}^{n}$ is used to construct the key $K_l$ and the value $V_l$ of the layout for object-aware local conditioning. We adopt convolutional operations for the construction as shown by Equation~\ref{eq:qkv_l}. Similarly, we construct the query, key and value of the BEV feature as Equation~\ref{eq:qkv_x} shows. To align the BEV feature with the layout, we divide the BEV feature map $\bm{x}_t$ equally into $k\times k$ bounding boxes, denoted by $\{b_x\}_{1}^{k\times k}$. We encode the bounding boxes from both BEV feature and layout, i.e., $b_x$ and $b_l$, into the same embedding space using the shared weights $W_b$ and $W_p$, and get the positional embedding $P_x$ and $P_l$ for the BEV feature and the layout, respectively (see Equation~\ref{eq:pos}). $P_x$ and $P_l$ are utilized to generate the fused query, key and value by combining the BEV feature and the layout for the cross-attention operation, as formulated in Equation~\ref{eq:qkv}. $[\ \cdot\ ] $ represents the concatenation operation.

\begin{equation}
\label{eq:lfm}
\begin{aligned}
 l' = \{o'_i\}_{i=0}^{n} &= LFM(\{o_i\}_{i=0}^{n}) \\
 &= self\mbox{-}attn(\{c_iW_c + b_iW_b\}_{i=0}^{n})
\end{aligned}
\end{equation}

\begin{equation}
\label{eq:qkv_l}
K_l,\, V_l = conv_{w_l}(l')
\end{equation}

\begin{equation}
\label{eq:qkv_x}
Q_x,\, K_x,\, V_x = conv_{w_x}(\varphi_i(\bm{x}_t'))
\end{equation}

\begin{equation}
\label{eq:pos}
P_x = b_xW_bW_p, \quad P_l = b_lW_bW_p
\end{equation}

\begin{equation}
\label{eq:qkv}
Q = 
\begin{bmatrix}
Q_x \\
P_x
\end{bmatrix},
\,K =
\begin{bmatrix}
K_x& K_l \\
P_x& P_l
\end{bmatrix},
\,V =
\begin{bmatrix}
V_x& V_l 
\end{bmatrix}
\end{equation}

\begin{figure}[t!]
    \centering
     \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gen_original.png}
         \caption{Existing layout.}
         \label{fig:ctrl_gen_a}
     \end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gen_remove.png}
         \caption{Objects removed.}
         \label{fig:ctrl_gen_b}
     \end{subfigure} 
     \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gen_add.png}
         \caption{Objects added.}
         \label{fig:ctrl_gen_c}
     \end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gen_move.png}
         \caption{Objects repositioned.}
         \label{fig:ctrl_gen_d}
     \end{subfigure} 
    \caption{BEV feature maps generated by our BEVDiffuser ($\mathrm{BD}^{fu}$) from pure noise, conditioned on user-defined layouts. We modify an existing layout (a) from nuScenes \texttt{mini-val} dataset by randomly removing (b), adding (c), and repositioning (d) some objects, as highlighted by the red boxes. BEVDiffuser generates accurate BEV feature maps, enabling the detection head to produce predictions that closely align with the ground truth. }
    \label{fig:ctrl_gen}
\end{figure}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/vis_tiny.png}
    \caption{Visualization results of our BEVDiffuser enhanced BEVFormer-tiny on nuScenes \texttt{val} dataset. As shown in \texttt{CAM\_FRONT} and \texttt{CAM\_FRONT\_RIGHT}, BEVDiffuser helps BEVFormer-tiny to detect the car intending to cross the road under the challenging lighting condition. Moreover, BEVDiffuser also helps to reduce hallucinations generated by BEVFormer-tiny, especially on \texttt{CAM\_FRONT\_LEFT}. }
    \label{fig:vis_tiny}
\end{figure*}


\section{Implementation Details}
Our implementation is built upon the official BEVFormer implementation \footnote{\url{https://github.com/fundamentalvision/BEVFormer}} and the MMCV implementation of the BEVFusion \footnote{\url{https://github.com/open-mmlab/mmdetection3d/tree/main/projects/BEVFusion}}. The hyperparameter $\lambda$ and $\lambda_{BEV}$ are empirically tuned based on the scale of the loss. 
Specifically, we configure $\lambda$ and $\lambda_{BEV}$ as follows: for BEVFormer-tiny and BEVFormer-base, $\lambda = 0.1$ and $\lambda_{BEV} = 100$; for BEVFormerV2, $\lambda = 0.05$ and $\lambda_{BEV} = 100$; and for BEVFusion, $\lambda = 0.2$ and $\lambda_{BEV} = 20$.

\section{Ablation Study}

We conduct an ablation study on BEVDiffuser ($\mathrm{BD}^{tiny}$) to validate our design choices of layout conditioning and optimization objective, i.e. optimizing towards $x_{t_0}$ with the task loss. Note that to optimize towards $\hat{\epsilon}_{t}$, we are not able to attach the task head or use the task loss. As shown in Tab.~\ref{tbl:ablation}, without the task loss, whether we optimize towards $x_{t_0}$ or $\hat{\epsilon}_{t}$, the denoising capability we obtained is quite limited, demonstrating that the task loss is critical to guarantee the denoising performance. Similarly, our layout conditioning also contributes to the superior denoising capability of BEVDiffuser, as evidenced by the inferior performance of the unconditional model.

\begin{table}[ht!]
\setlength{\tabcolsep}{4.8pt}
\small
\begin{center}
\begin{tabular}{lccccc}
&& \multicolumn{4}{c}{\# denoising steps}\\
\cline{3-6}
% \specialrule{0.12em}{0pt}{0pt}
 Method & obj. & 1 & 3 & 5 & 10\\
\specialrule{0.12em}{0pt}{1pt}
\textbf{Ours} &$x_{t_0}$ & \textbf{35.8/47.7} &\textbf{40.4/52.3} &\textbf{40.8/52.7} & \textbf{40.3/52.3}\\
\hline
\multirow{2}{*}{$-$task}& $x_{t_0}$ &24.5/34.7 &23.1/32.8 &21.7/31.0 &17.4/26.1 \\
&$\hat{\epsilon}_{t}$ &25.2/35.5 &25.2/35.5 &25.2/35.5 &25.2/35.5 \\
\hline
$-$cond. &$x_{t_0}$ & 25.4/35.4 & 25.3/35.3 & 25.1/35.0 & 24.7/34.6\\
\specialrule{0.12em}{0pt}{0pt}
\end{tabular}
\caption{Ablation study. mAP/NDS achieved by the variants of BEVDiffuser ($\mathrm{BD}^{tiny}$) with increasing denoising steps (1$\rightarrow$10). Results validate that both the task loss and the layout conditioning contribute to the superior denoising capability of BEVDiffuser. }
\label{tbl:ablation}
\end{center}
\end{table}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/vis_base.png}
    \caption{Visualization results of our BEVDiffuser enhanced BEVFormer-base on nuScenes \texttt{val} dataset. While BEVFormer-base shows good performance in the crowded environment, BEVDiffuser enhances its performance further, such as by detecting a human riding a bicycle in front of the autonomous vehicle, as indicated by the red bounding box in \texttt{CAM\_FRONT} and \texttt{CAM\_FRONT\_LEFT}.}
    \label{fig:vis_base}
\end{figure*}

\section{Additional Qualitative Results}
\subsection{Controllable BEV Generation}


We present user-defined layout-conditioned BEV generation in Fig.~\ref{fig:ctrl_gen}. We modify an existing layout by randomly removing, adding, or repositioning some objects, and then condition the BEVDiffuser on the modified layouts to generate BEV feature maps. As shown in Fig.~\ref{fig:ctrl_gen},  BEVDiffuser is able to produce BEV feature maps that enable accurate object detection in alignment with the specified layouts, demonstrating its strong controllable generation capability. This capability facilitates easy adjustments to object presence and positioning in the BEV feature space, paving the way for large-scale data collection and driving world model development to advance autonomous driving.


\subsection{3D Object Detection}




\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/vis_v2.png}
    \caption{Visualization results of our BEVDiffuser enhanced BEVFormerV2 on nuScenes \texttt{val} dataset. In this representative example, despite the rain causing blurriness in the camera images, BEVDiffuser still enables BEVFormerV2 to reliably detect the object in front of the autonomous vehicle, as captured by the LiDAR top view.   }
    \label{fig:vis_v2}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/vis_fusion.png}
    \caption{Visualization results of our BEVDiffuser enhanced BEVFusion on nuScenes \texttt{val} dataset. BEVFusion, which integrates both camera and LiDAR data, delivers robust performance in low-light conditions at night. BEVDiffuser further enhances BEVFusion by effectively reducing false negatives, as demonstrated in the LiDAR top view.}
    \label{fig:vis_fusion}
\end{figure*}

We visualize the 3D object detection results achieved by our BEVDiffuser enhanced BEVFormer-tiny, BEVFormer-base, BEVFormerV2 and BEVFusion in Fig.~\ref{fig:vis_tiny}, Fig.~\ref{fig:vis_base}, Fig.~\ref{fig:vis_v2} and Fig.~\ref{fig:vis_fusion}, respectively. We present the ground-truth and predicted 3D bounding boxes in both multi-camera images and the LiDAR top view to offer a comprehensive overview of the models' performance. As illustrated in the figures, BEVDiffuser consistently enhances the existing BEV models for object detection in complex environments and under challenging conditions by minimizing both false positives and false negatives, demonstrating its ability to improve the quality of the BEV representations.