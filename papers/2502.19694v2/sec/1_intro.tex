\section{Introduction}
\label{sec:intro}

\begin{figure}[ht!]
    \small
    \centering
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/bev/bev_avg.png}
         \caption{BEVFormer \cite{bevformer}}
         \label{fig:bev_tiny}
     \end{subfigure}
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/bev/bev_avg_0_5_5.png}
         \caption{BEVDiffuser}
         \label{fig:bev_ours}
     \end{subfigure} 
     \begin{subfigure}[b]{0.15\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/bev/gt_lidar.png}
         \caption{LiDAR Top View }
         \label{fig:gt_view}
     \end{subfigure}
    \caption{Comparisons of BEV feature maps: (a) generated by BEVFormer (tiny) \cite{bevformer}, (b) denoised by BEVDiffuser in $5$ steps. Channel-wise features are averaged for visualization. BEVDiffuser denoises and substantially enhances the BEV feature maps.}
    \vspace{-5pt}
    \label{fig:bev}
\end{figure}

Bird's-eye-view (BEV) representations have become crucial in advancing autonomous driving tasks, including perception, prediction, and planning, by providing a comprehensive top-down understanding of the surrounding environment \cite{bevformer,liu2023bevfusion,Uniad,VAD}. By integrating data from various sensors, such as multi-view cameras and LiDAR, BEV generates a unified scene representation that empowers autonomous systems to make more accurate and informed decisions. The effectiveness of the BEV representations has sparked considerable interest, resulting in a diverse set of approaches for learning BEV representations from single-modal \cite{yin2021center, bevformer} or multi-modal \cite{liu2023bevfusion, lin2024rcbevdet} sensors, using geometry-based \cite{philion2020lift} or transformer-based \cite{bevformer} methods. These advanced BEV generation techniques have emerged as state-of-the-art solutions for a variety of benchmark tasks, including 3D object detection \cite{liu2023bevfusion, huang2022bevdet4d}, map segmentation \cite{philion2020lift, pan2020cross} and autonomous planning \cite{Uniad,VAD}.


Despite recent advancements in BEV generation, the issue of noise in these BEV representations remains largely unresolved. Generated BEV representations are inherently noisy (see Fig.~\ref{fig:bev_tiny}) due to the imperfections of acquisition sensors such as camera and LiDAR, as well as the limitations in the learning process \cite{zou2024diffbev,le2024diffuser}. The noise from acquisition sensors introduces inaccuracies, including imprecise localization of object boundaries in BEV feature maps, which degrades performance in downstream tasks. Additionally, in the absence of direct supervision, BEV representations are typically optimized only for downstream task performance, leading to potential biases within the BEV feature maps. Generative models, particularly diffusion models, are well-suited to address this challenge due to their powerful denoising capabilities \cite{sohl2015deep, songdenoising, rombach2022high}. Diffusion models have demonstrated remarkable success in image and video generation \cite{ramesh2022hierarchical,rombach2022high,blattmann2023align}, and recent studies have extended their applicability to tasks such as image classification and object detection \cite{li2023your,chen2023diffusiondet, nachkov2023diffusion}. Leveraging diffusion models to denoise and enhance BEV representations holds significant potential for improving the robustness and accuracy of BEV-based downstream tasks.


In this work, we introduce BEVDiffuser, a novel diffusion model that denoises BEV representations with ground-truth guidance. BEVDiffuser is trained on BEV feature maps generated by existing BEV models, such as BEVFormer and BEVFusion~\cite{bevformer,liu2023bevfusion}. We add varying levels of noise to these BEV feature maps and train BEVDiffuser to predict the clean BEV, conditioned on the ground-truth object layout to effectively guide the denoising process. Once trained, BEVDiffuser operates in a plug-and-play manner, enhancing current BEV models by providing denoised BEV feature maps as additional supervision during training. BEVDiffuser is used only in training time and removed at deployment, leaving the enhanced BEV models without any architectural modifications for inference. Consequently, BEVDiffuser improves the performance of existing BEV models without requiring any adaptation efforts or introducing any computational latency at inference time.


BEVDiffuser, as a flexible plug-and-play module, can be seamlessly incorporated into any BEV model. In this study, we conduct an extensive evaluation of BEVDiffuser on four widely adopted state-of-the-art BEV models using the challenging nuScenes~\cite{nuscenes} dataset. The experimental results demonstrate BEVDiffuser's exceptional denoising capabilities (see Fig.~\ref{fig:bev_ours}), which enable significant enhancements to existing BEV models, demonstrated by notable improvements of $12.3\%$ in mAP and $10.1\%$ in NDS for 3D object detection. Additionally, our experiments show that BEVDiffuser substantially improves performance in long-tail object detection and under challenging weather and lighting conditions, highlighting its ability to produce more accurate and robust BEV representations. Furthermore, BEVDiffuser also shows high-quality BEV generation capabilities from pure noise with layout conditioning, which can pave the way for large-scale data collection to advance autonomous driving. Qualitative visualizations further validate the observed quantitative improvements. 

We summarize our main contributions as follows:
\begin{itemize}
    \item We propose BEVDiffuser, a novel diffusion model that effectively denoises BEV feature maps using the ground-truth object layout as guidance. 
    \item BEVDiffuser can be operated in a plug-and-play manner during training time to enhance existing BEV models without modifying their architectures or introducing additional computational overhead during inference.
    \item Extensive experiments on the nuScenes dataset demonstrate that BEVDiffuser possesses strong BEV denoising and generation capabilities, significantly enhances BEV models both quantitatively and qualitatively, and exhibits improved robustness in long-tail cases and adverse weather and lighting conditions.   
\end{itemize}
