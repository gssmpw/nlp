\section{Related Work}
\label{sec:related_work}
\subsection{BEV Feature Map}
\label{sec:bev}
Camera-only BEV feature generation works can be broadly categorized into two main approaches: geometry-based methods, represented by Lift-Splat-Shoot (LSS) \cite{philion2020lift}, and transformer-based methods, exemplified by BEVFormer \cite{bevformer}. LSS \cite{philion2020lift} generates BEV feature maps from multi-view images by leveraging the estimated depth distribution, followed by \cite{li2023bevdepth, huang2021bevdet, huang2022bevdet4d}. In contrast, transformer-based methods utilize powerful attention mechanism to extract attended image features for BEV generation. BEVFormer \cite{bevformer} and its follow-up BEVFormerV2 \cite{bevformerv2} have gained significant interest as they capture both spatial and temporal information through spatial cross-attention and temporal self-attention mechanisms, respectively. Another line of work presents strategies to fuse multi-modal sensor inputs for more robust BEV feature generation \cite{liang2022bevfusion,liu2023bevfusion,lin2024rcbevdet}. BEVFusion \cite{liu2023bevfusion} is a representative work that introduces a unified framework for camera and LiDAR sensors by combining multi-modal features in BEV space. In contrast to these works, we propose a plug-and-play diffusion model designed to enhance the BEV feature maps by denoising the intrinsic noise from both the acquisition sensors and the learning process. 


\subsection{Diffusion Model Enhanced BEV} 
Diffusion models are a class of generative models that have demonstrated impressive performance and stability \cite{sohl2015deep,song2019generative, ho2020denoising}. While diffusion models have been primarily used for generative tasks, such as image generation \cite{rombach2022high, zhang2023adding, ramesh2022hierarchical,zheng2023layoutdiffusion} and video generation \cite{singer2023makeavideo, blattmann2023align, wu2024lamp, wang2024videocomposer}, their applications to downstream tasks such as image classification \cite{li2023your}, object detection \cite{chen2023diffusiondet}, semantic segmentation \cite{li2023open}, and motion prediction \cite{jiang2023motiondiffuser} have recently been investigated. 

Only a few approaches have been proposed to use diffusion models for enhancing the BEV feature maps \cite{le2024diffuser, zou2024diffbev}, which is the focus of this study. Specifically, DiffBEV \cite{zou2024diffbev} applies a conditional diffusion model to progressively refine the noisy BEV feature maps, using the learned features as conditions. The denoised BEV is then fused with the original BEV to perform downstream tasks. Similarly, DifFUSER \cite{le2024diffuser} leverages a diffusion model for better sensor fusion. It enhances the fused features obtained from camera and LiDAR sensors by denoising them conditioned on partial camera and LiDAR features during run time. Both approaches demonstrate the potential of diffusion models for denoising and enhancing BEV feature maps.  However, unlike our BEVDiffuser, these approaches rely on noisy information as conditions to guide the denoising process which is less effective. Moreover, they require multiple passes through their integrated diffusion model during inference, making them computationally expensive for latency-critical real-world applications like autonomous driving.
 