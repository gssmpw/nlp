
\documentclass[conference]{IEEEtran}

%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amssymb,mathtools}
\usepackage{algorithm,algpseudocode}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{url}
\usepackage{hyperref}
\usepackage{subcaption}
% \usepackage{subfigure}
\usepackage[export]{adjustbox}
\usepackage{flushend}
\usepackage[T1]{fontenc}
%\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% \usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%

\newcommand{\aagent}{\texttt{active agent}\xspace}
\newcommand{\pagent}{\texttt{passive agent}\xspace}
\newcommand{\approach}{\texttt{cMarlTest}\xspace}
\newcommand{\tool}{\texttt{RLbT}\xspace}
\newcommand{\curiosityrl}{\texttt{curiosity-based RL}\xspace}
\newcommand{\sparserl}{\texttt{sparse-reward RL}\xspace}
\newcommand{\encov}{\texttt{Entity Coverage}\xspace}
\newcommand{\enconcov}{\texttt{Entity Connection Coverage}\xspace}
\newcommand{\fourrooms}{\texttt{4-room}\xspace}
\newcommand{\eightrooms}{\texttt{8-room}\xspace}
\newcommand{\randomlarge}{\texttt{LargeMaze\_1}\xspace}
\newcommand{\randommedium}{\texttt{LargeMaze\_2}\xspace}
\newcommand{\rl}{reinforcement learning\xspace}
\newcommand{\RL}{Reinforcement Learning\xspace}
\newcommand{\MARL}{Multi-Agent Reinforcement Learning\xspace}
\newcommand{\marlacronym}{MARL\xspace}
\newcommand{\rlacronym}{RL\xspace}
\newcommand{\SARL}{\texttt{Single-Agent Reinforcement Learning}\xspace}
\newcommand{\sut}{\texttt{Lab Recruits}\xspace}
\newcommand{\lr}{\texttt{Lab Recruits}\xspace}

\newcommand{\buttonsdoors}{\texttt{L1}\xspace}
\newcommand{\med}{\texttt{Small}\xspace}
\newcommand{\lrg}{\texttt{Medium}\xspace}
\newcommand{\ext}{\texttt{Large}\xspace}

\newcommand{\rqa}{\textbf{RQ1}\xspace}
\newcommand{\rqaa}{\textbf{RQ1.1}\xspace}
\newcommand{\rqb}{\textbf{RQ2}\xspace}
\newcommand{\rqbb}{\textbf{RQ2.1}\xspace}


\newcommand*{\MyIndent}{\hspace*{0.5cm}}%
\newcommand\FK[1]{\textcolor{blue}{#1}}
\newcommand\RF[1]{\textcolor{blue}{#1}}
\newcommand\EXAMPLE[1]{\textcolor{brown}{#1}}


%%
%% end of the preamble, start of the body of the document source.
\setlength {\marginparwidth }{2cm}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Curiosity Driven Multi-agent Reinforcement Learning for 3D Game Testing}


\author{\IEEEauthorblockN{Raihana Ferdous}
\IEEEauthorblockA{\textit{Consiglio Nazionale delle Ricerche} \\
Pisa, Italy \\
raihana.ferdous@isti.cnr.it}
\and
\IEEEauthorblockN{Fitsum Kifetew}
\IEEEauthorblockA{\textit{Fondazione Bruno Kessler} \\
Trento, Italy \\
kifetew@fbk.eu}
\and
\IEEEauthorblockN{Davide Prandi}
\IEEEauthorblockA{\textit{Fondazione Bruno Kessler} \\
Trento, Italy \\
prandi@fbk.eu}
\and
\IEEEauthorblockN{Angelo Susi}
\IEEEauthorblockA{\textit{Fondazione Bruno Kessler} \\
Trento, Italy \\
susi@fbk.eu}
}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle



\begin{abstract} 
Recently testing of games via autonomous agents has shown great promise in tackling challenges faced by the game industry, which mainly relied on either manual testing or record/replay. In particular \RL (\rlacronym) solutions have shown potential by learning directly from playing the game without the need for human intervention.

In this paper, we present \approach, an approach for testing 3D games through curiosity driven \MARL (\marlacronym). \approach deploys multiple agents that work collaboratively to achieve the testing objective.
%and exercise the various entities/functionalities of the game. 
The use of multiple agents helps resolve issues faced by a single agent approach.

We carried out experiments on different levels of a 3D game comparing the performance of \approach with a single agent \rlacronym variant. Results are promising where, considering three different types of coverage criteria, \approach achieved higher coverage. \approach was also more efficient in terms of the time taken, with respect to the single agent based variant.

\end{abstract}


\begin{IEEEkeywords}
Curiosity driven Reinforcement learning, game testing, coverage based testing
\end{IEEEkeywords}



\section{Introduction} \label{sec:introduction}
\input{introduction}

\section{Running Example} \label{sec:runningexample}
\input{runningexample}

\section{\approach: Curiosity Driven \marlacronym for Game Testing} \label{sec:approach}
\input{approach}

\section{Evaluation Setup} \label{sec:experiment}
\input{experiment}

\section{Results and Discussion} \label{sec:results}
\input{results}

\section{Related Works} \label{sec:relatedworks}
\input{relatedworks}

\section{Conclusion} \label{sec:conclusion}
\input{conclusion}

\section*{Acknowledgment}
We acknowledge the support  of the PNRR project FAIR -  Future AI Research (PE00000013),  under the NRRP MUR program funded by the NextGenerationEU.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,reference}



\end{document}
