Software testing has long been recognized as a critical aspect of the development process and accounts for a significant portion of the overall budget. Over the years researchers have developed numerous techniques and tools to automate as much as possible the testing process, hence reducing the associated cost. Despite research efforts, testing remains a primarily manual task for certain software systems, such as computer games~\cite{DBLP:conf/ast/PolitowskiPG21}. Computer games, and particularly 3D games, bring several challenges to the table when considering their automated testing. They are highly interactive, involving continuous input spaces, as well as visual elements that are not easy to deal with in an automated manner. Consequently, automated testing of games has been mostly limited to capture-replay based solutions, which tend to produce brittle tests. Recent research using Reinforcement Learning (\rlacronym) has shown promising results~\cite{silver2017masteringGo,mnih2013playing,openAI-dota2,DBLP:journals/tciaig/AlbaghajatiA23,politowski2022towards, DBLP:journals/tciaig/AspertiCPPS20},
%, in particular with board games~\cite{DBLP:journals/tciaig/AlbaghajatiA23}
where \rlacronym is used to train agents that can play the game and win it.  There have been recent efforts in the automated testing of the wider scope of software applications that fall under the category of Extended Reality (XR) systems~\cite{DBLP:conf/icst/PradaPKDVLDKDF20}, which include 3D games as well.

 In a previous work, we proposed using single agent \rlacronym for testing games where we obtained encouraging results~\cite{DBLP:conf/kbse/FerdousKPS22}. We also observed that while \rlacronym works reasonably well and serves as a good starting point, it faces a number of challenges that are difficult to tackle with a single agent. One such challenge is the fact that modern 3D games are only partially observable. Hence, a single agent faces difficulty in properly assessing the effect(s) of its actions. For instance, at a certain point in the game, if the agent flips a light switch that controls several light bulbs distributed in different rooms/corridors (some of which could be behind closed doors), it is extremely difficult for the agent to observe the effect of the switch (i.e., which bulbs are alight and which not). Similarly, the size of the game is also another challenge where the agent would need to cover a lot of ground to fully explore the game level~\cite{DBLP:conf/kbse/FerdousKPS22}.


To address the aforementioned challenges, in this paper, we propose \approach, a testing approach for 3D games  exploiting \marlacronym where two (or more) agents work collaboratively to explore the game world and achieve the desired testing objectives. \approach uses a curiosity driven reward mechanism in such a way that the agents are driven towards unexplored areas of the game world, maximizing coverage of the game. This work builds on our previous work~\cite{DBLP:conf/kbse/FerdousKPS22} using a similar setup and environment but instead, it applies a multi-agent \rlacronym approach to address some of the limitations of the aforementioned work. The use of a similar setup and environment enables a direct comparison of the results of the single agent and multi agent \rlacronym in the same context.


This paper presents details about \approach and the experiment we carried out to assess its performance with respect to a single agent \rlacronym based approach. %tries to assess the suitability of \marlacronym for automated testing of 3D games as compared to a single agent \rlacronym approach. 
Specifically, we aim to answer the following research questions:

\noindent 
\textbf{\rqa (Effectiveness)} What is  the effectiveness of \approach for testing 3D games with respect to the single agent \rlacronym approach? Effectiveness is measured by calculating the coverage of the 3D game achieved by the compared approaches.

\noindent
\textbf{\rqb (Efficiency)} What is  the efficiency of \approach for testing 3D games with respect to the single agent \rlacronym approach? Efficiency is measured by calculating the amount of time required by the compared approaches to achieve their final coverage. %Efficiency is compared when the compared approaches achieve the same level of coverage.

The rest of the paper is organized as follows: in Section~\ref{sec:runningexample} we introduce a running example we use in the rest of the paper. In Section~\ref{sec:approach} we present \approach, followed by the experimental setup in Section~\ref{sec:experiment}. The results of the experiment are presented in  Section~\ref{sec:results}. Section~\ref{sec:relatedworks} discusses related works, and finally Section~\ref{sec:conclusion} concludes the paper and outlines future work.