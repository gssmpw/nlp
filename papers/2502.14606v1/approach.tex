Our proposed approach is to employ \marlacronym where multiple agents work collaboratively to achieve better exploration of the game world. In the literature, \marlacronym based approaches have been applied in the context of game play~\cite{OpenAIHideandSeekbaker2019,vinyals2019grandmasterStarCraft,park2019multi,zhou2021hierarchical}, where the main objective is typically to train agents that can play and eventually win the game. 
%The most successful cases being predominantly board or similar games. 
In our case the objective is not to train agents capable of winning a game, but rather agents that can effectively explore the game world from the perspective of \emph{software testing} where the ultimate goal is to achieve full coverage of the desired coverage criterion. Different notions of coverage could be considered, depending on the game and desired testing level~\cite{10.1145/3643658.3643920}. In our experiments, we consider three coverage criteria, i.e., \emph{entity coverage}, \emph{entity connection coverage}, and \emph{spatial coverage} (as detailed in Section~\ref{sec:metrics}). However, the approach can easily be extended to include other coverage criteria~\cite{10.1145/3643658.3643920}.

Deployment of the \marlacronym architecture for testing of 3D games is  difficult~\cite{canese2021multi}, 
one of the prominent challenges being the non-stationarity of the environment. 
Though \marlacronym agents are autonomous entities with individual goals and independent decision making capabilities, they are influenced by each other's decisions while simultaneously learning by interacting with a shared environment. 
%As a result, agents need to take into account and interact with not only the environment but also other learning agents. 
This indicates that the environment dynamics become non-stationary making learning an optimal policy difficult. 
%Moreover, the complexity increases drastically with the number of agents and their action space. 
%Since multiple agents concurrently improve their policies according to their own goals, the environmental dynamics become non-stationary making learning an optimal policy incredibly difficult.  
%Thus 
Non-stationarity of the environment violates the Markov property which is the foundation of the basic \rlacronym approach stating that the environment where the agent is learning is stationary, and, the state transition and reward depend only on the current state of an agent~\cite{sutton2018reinforcement}. Hence, a Markov Decision Process \cite{bellman1957markovian} is no longer feasible to describe \marlacronym. Thus, \marlacronym which considers the decision making process involving multiple agents is modelled through a stochastic game, also known as a Markov game \cite{littman1994markov}. 

Various approaches and frameworks are proposed in recent research to solve the Markov game \cite{DBLP:journals/corr/abs-2011-00583, zhang2021multi} and to handle non-stationarity issues \cite{DBLP:journals/corr/abs-1906-04737}. These approaches range from using modifications of standard \rlacronym training methods to computing and sharing additional information. 
Although these policies can moderate the non-stationarity of the environment, learning algorithms still suffer from instability, 
especially in games where agents only partially observe the environment.
For example, the problem of testing and providing coverage of 3D games which we formulate as a cooperative multi-agent \rl problem becomes a particularly challenging class of problem due to the presence of partial observability. Here, agents must learn to coordinate in a non-stationary environment, while relying only on their partial information/observations. The agents may be oblivious to the actions of other agents even though these actions have a direct impact on the environment, and hence on their reward. 

One possible solution to address the partial observability issue in the cooperative \marlacronym is the possibility of sharing experience between the agents \cite{gerstgrasser2024selectively}. %For instance, the agents can exchange information using communication protocol, skilled agents may serve as teachers for the other learners \todo{add citation}. 
Here, we have proposed \approach, a cooperative \marlacronym solution where we define the agents to be fully or partially independent with different roles, meaning they are assigned to achieve different but related sub-goals. Performing different sub-tasks ensures that they are independent, hence, they can be benefited by sharing experience to achieve the main goal that is, to maximize game coverage. 
%
The rest of this section describes the details of our proposed approach \approach for \marlacronym-based testing of 3D games.   

\subsection{States and actions}
\approach relies on the underlying agent-based testing platform~\cite{prasetya2020aplib} for interacting with the game under test (GUT). The underlying test agent can perform low-level activities in the game world with complete autonomy. For instance, the test agent can navigate from point A to point B in the game on its own, where points A and B are identified by entities in the game (e.g., 
 doors) rather than pixel level coordinates as is typical in game testing contexts. This enables \approach to remain at a higher level of abstraction in terms of the states and actions available to the \rlacronym agent. In other words, when formulating the \rlacronym problem, we can define high-level states and actions rather than low-level ones. An example of a high-level action could be \emph{go to door A}, while a state could be \emph{door A is open, door B is closed, and button C is pressed}. Such state/action specifications are high-level with respect to low level actions such as \emph{move left for 10 pixels}. \approach uses the agent library of the iv4XR framework which provides a high-level view of the agent's observations of the environment~\cite{prasetya2020aplib}.

The states in \approach are defined in terms of game entities and their attributes. Let $E$ be the set of entity types in a game $G$ and 
$Attr_{E_i}$ the set of attributes of entity type $E_i$,
%each entity type $E_i$ has a set of attributes $Attr_{E_i}$, 
where each attribute $Attr$ can assume a set of values $Val$. The set of possible states is defined as: $$E \times Attr \times Val$$ %In the case of \sut, $E = \{Door, Button\}$, $Attr_{Door} = \{ID, isOpen\}$, $Val_{isOpen} = \{true, false\}$, $Attr_{Button} = \{ID, isPressed\}$, $Val_{isPressed} = \{true, false\}$. 
In the case of \sut, we have
\begin{itemize}
    \item $E = \{Door, Button\}$
    \item $Attr_{Door} = \{ID, isOpen\}$ \\ $Attr_{Button} = \{ID, isPressed\}$
    \item $Val_{isOpen} = Val_{isPressed} = \{true, false\}$
\end{itemize}
Hence, for a given level of \sut, the state space is defined by the combination of the values of the doors and buttons in the level. For instance, the level \buttonsdoors in our running example (see Figure~\ref{fig:runningexample} in Section~\ref{sec:runningexample}) includes three doors and four buttons and  the possible states are \{(door1, isOpen, true), (door1, isOpen, false), (door2, isOpen, true), (door2, isOpen, false), (door3, isOpen, true), (door3, isOpen, false), (bttn1, isPressed, true),(bttn1, isPressed, false), (bttn2, isPressed, true), (bttn2, isPressed, false), (bttn3, isPressed, true), (bttn3, isPressed, false), (bttn4, isPressed, true), (bttn4, isPressed, false)\}.

At a given state $S$ of the game, the possible set of actions are those allowed by the game for the entities available in $S$. The specific set of actions depends on the GUT. For instance, in the case of \sut, typical possible actions include pressing a button, going through a door, etc. Going back to our running example, given a state $S = (bttn2, isPressed, true), (door3, isOpen, false)$ (this corresponds to a situation where the player is near the button \emph{bttn2}), the possible actions are either to interact with button \emph{bttn2} or to interact with (go through) door \emph{door3}. Note that depending on the position of the player in the game, the observed state is typically partial, and hence the set of possible actions is also limited to those entities available in the current state.

\subsection{Agent roles}
\approach uses a \marlacronym setup in which multiple agents work collaboratively to achieve full \emph{coverage} of the desired adequacy criterion. 
In general, \marlacronym agents could be cooperative, competitive, or a mix~\cite{DBLP:journals/tsmc/BusoniuBS08}. While in this work we adopt cooperative agents, other types of multi agent configurations, e.g., competitive agents, could be further explored as part of future studies. 

In the context of this paper, the testing objective is to explore the game world until some adequacy criterion (coverage) is satisfied. To this end, we adopted a fully cooperative \marlacronym scheme in which agents with distinct roles work together to achieve \emph{full coverage}.  Specifically, \approach defines two agent roles: \aagent and \pagent. The \aagent is responsible for actively interacting with the environment by performing \emph{actions} to learn an optimal policy, while the \pagent is limited to only observing the environment and sharing the observation with the \aagent, without performing any actions. The \aagent behaves as a typical \rlacronym agent that observes the environment and performs actions, with the only addition that its observation is further \emph{enriched} with the observations it receives from one or more {\pagent}s. On the other hand, the \pagent performs only observations of the environment and sends them to the \aagent. 
%It is also possible to assign a group of {\pagent}s to work in parallel  to increase the environment exploration.

\subsection{Reward function} \label{subsec:rewardfunction}
\approach employs a reward function that promotes the exploration of new areas in the game, hence the agents are encouraged to follow their curiosity \cite{pathak2017curiosity}, and discouraged from revisiting previously seen areas and from being stationary.
We formulate the reward proportional to the transition's novelty, providing a low and decreasing reward (e.g., penalty) for revisiting previously explored states, while a high reward is given for reaching new areas or triggering new actions.
The reward is computed based on how distant the current observation is from the most similar observation in memory using the Jaccard coefficient \cite{jaccard1912distribution}. The reward mechanism is similar to the one employed in our previous work with single agent \rlacronym~\cite{DBLP:conf/kbse/FerdousKPS22}.

\subsection{Methodology}
The two types of agents defined in \approach are activated simultaneously. 
After executing an action, the \aagent observes the current state of the environment according to its observation range. The \pagent aids the \aagent to improve this knowledge by sharing the information it gathers about the current state of the environment from its current location in the game world. Note that the observations of the two agents could be identical (if they are close to one another), completely different (if they are too far apart in different areas of the game world), or partially overlapping depending on their locations.

The \aagent uses Q-learning, a model-free \rlacronym algorithm that does not require prior knowledge of the environment and is applicable in different environments. It is a value-based, off-policy algorithm that tries to find the best series of actions based on the agent's current state. %It learns an optimal policy by training a action-value function containing the value of each state-action pair. 
The curiosity-based reward mechanism (described in \ref{subsec:rewardfunction}) is used to calculate the reward corresponding to an action chosen by the agent. A tabular Q-learning solution is used to keep the design simple. To ensure convergence and reduce the Q-table size, a state similarity measure is adopted when updating the Q-table. Instead of making a new entry for every state-action pair, a similarity calculation is performed to identify the most similar state, if it exists, in the Q-table. 

\emph{Decayed Epsilon-Greedy} is used as the initial policy to balance between  exploration and exploitation by allowing the \aagent to explore more when it does not have enough information about the environment, and to do exploitation once it has gathered enough information by interacting with the environment. 
The exploration-exploitation  process is controlled by the $\epsilon$ parameter, which \emph{is decayed} by a factor in each episode. The decaying factor is calculated based on the number of learning episodes.