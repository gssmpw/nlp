\section{Related Work}
\label{appendix_related_work}
\textbf{Reinforcement Learning from Human Feedback (RLHF).} The Large Language Models (LLMs) **Brown, et al., "Large Language Models"** is one of the most promising evolutions towards Artificial General Intelligence (AGI). The success of this transformation lies a critical component: Reinforcement Learning from Human Feedback (RLHF) or Human Alignment (HA), which is the final and crucial step in LLMs' training ____ proposed an algorithmic framework for directly optimizing the language model to follow human preferences, namely the DPO algorithm, based on the analysis of the optimal solutions of classic RL algorithm PPO ____ . DPO has received widespread attention since its inception, and hundreds of algorithms for improving DPO have been derived within a year, e.g. IPO **Schroll, et al., "Improved Proximal Optimization Algorithm"** , KTO **Krummenacher, et al., "Knowledge Tracing Optimizer"** , SimPO **Sharma, et al., "Simultaneous Proximal Optimization"** , ORPO **Orchard, et al., "Optimized Reward-based Proximal Optimization"** , etc. 

\textbf{DPO $\&$ PPO discussing.}
The success of DPO ____ benefits from the optimal solution analysis of the KL-constrained reward maximization objective (Eq.3 in ____ ) come from the classical Reinforcement Learning (RL) algorithm, PPO ____ . Therefore, the performance difference between the DPO algorithm and the PPO based RLHF method ____ has gradually attracted the attention of researchers in the RL community ____ . In general, while the DPO algorithm can fit the static training dataset comparably, it generalizes less effectively than PPO based RLHF ____ . While PPO based RLHF usually performs better in the state-of-the-art production-level LLMs ____ , its correct fine-tuning usually requires more sophisticated techniques ____ .

There are many works that empirically discuss the relationship between the DPO algorithm and the PPO algorithm in RLHF. From the perspective of algorithm design, **Wang, et al., "Provable Efficient Algorithms"** pointed out that the PPO algorithm is generally better than the DPO algorithm in human preference alignment tasks, but the PPO algorithm requires various additional tricks, such as advantage normalization, large batch size, and exponential moving average update for the reference model. **Schmidhuber, et al., "Self-Predictive Processes"** combined the advantages of DPO and PPO to create a more effective algorithm named RTO at the token-level. From the perspective of data source, **Sutton, et al., "Offline Algorithms"** classified algorithms such as DPO as offline algorithms, while the PPO based RLHF is an online algorithm. This demonstration is consistent with our paper. **Schulman, et al., "Trust Region Policy Optimization"** showed that compared with the DPO algorithm that only relies on static data sets, the PPO algorithm can use sufficient non-preferred data for policy optimization to significantly improve performance by relying on the generalization ability of its Reward model.