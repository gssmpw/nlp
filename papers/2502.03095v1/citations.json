[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhao2023survey",
        "author": "Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others",
        "title": "A survey of large language models"
      },
      {
        "key": "chang2024survey",
        "author": "Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others",
        "title": "A survey on evaluation of large language models"
      },
      {
        "key": "hadi2024large",
        "author": "Hadi, Muhammad Usman and Al Tashi, Qasem and Shah, Abbas and Qureshi, Rizwan and Muneer, Amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and others",
        "title": "Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects"
      },
      {
        "key": "minaee2024large",
        "author": "Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng",
        "title": "Large language models: A survey"
      },
      {
        "key": "hadi2023survey",
        "author": "Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others",
        "title": "A survey on large language models: Applications, challenges, limitations, and practical usage"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "bubeck2023sparks",
        "author": "Bubeck, S{\\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others",
        "title": "Sparks of artificial general intelligence: Early experiments with gpt-4"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "arumugam2019deep",
        "author": "Arumugam, Dilip and Lee, Jun Ki and Saskin, Sophie and Littman, Michael L",
        "title": "Deep reinforcement learning from policy-dependent human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "singh2022flava",
        "author": "Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe",
        "title": "Flava: A foundational language and vision alignment model"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "dai2023safe",
        "author": "Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong",
        "title": "Safe rlhf: Safe reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "DPO",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      },
      {
        "key": "engstrom2019implementation",
        "author": "Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander",
        "title": "Implementation matters in deep rl: A case study on ppo and trpo"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "IPO",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "KTO",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "SimPO",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ORPO",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Reference-free monolithic preference optimization with odds ratio"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "DPO",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "DPO",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "PPO",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "PPO-basedRLHF",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ivison2024unpacking",
        "author": "Ivison, Hamish and Wang, Yizhong and Liu, Jiacheng and Wu, Zeqiu and Pyatkin, Valentina and Lambert, Nathan and Smith, Noah A and Choi, Yejin and Hajishirzi, Hannaneh",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lin2024limited",
        "author": "Lin, Yong and Seto, Skyler and ter Hoeve, Maartje and Metcalf, Katherine and Theobald, Barry-John and Wang, Xuan and Zhang, Yizhe and Huang, Chen and Zhang, Tong",
        "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yan20243d",
        "author": "Yan, Yuzi and Miao, Yibo and Li, Jialian and Zhang, Yipin and Xie, Jian and Deng, Zhijie and Yan, Dong",
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "xu2024dpo",
        "author": "Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi",
        "title": "Is dpo superior to ppo for llm alignment? a comprehensive study"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xu2024dpo",
        "author": "Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi",
        "title": "Is dpo superior to ppo for llm alignment? a comprehensive study"
      },
      {
        "key": "ivison2024unpacking",
        "author": "Ivison, Hamish and Wang, Yizhong and Liu, Jiacheng and Wu, Zeqiu and Pyatkin, Valentina and Lambert, Nathan and Smith, Noah A and Choi, Yejin and Hajishirzi, Hannaneh",
        "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback"
      },
      {
        "key": "yan20243d",
        "author": "Yan, Yuzi and Miao, Yibo and Li, Jialian and Zhang, Yipin and Xie, Jian and Deng, Zhijie and Yan, Dong",
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhong2024dpo",
        "author": "Zhong, Han and Feng, Guhao and Xiong, Wei and Zhao, Li and He, Di and Bian, Jiang and Wang, Liwei",
        "title": "Dpo meets ppo: Reinforced token optimization for rlhf"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "tang2024understanding",
        "author": "Tang, Yunhao and Guo, Daniel Zhaohan and Zheng, Zeyu and Calandriello, Daniele and Cao, Yuan and Tarassov, Eugene and Munos, R{\\'e}mi and Pires, Bernardo {\\'A}vila and Valko, Michal and Cheng, Yong and others",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "li2023policy",
        "author": "Li, Ziniu and Xu, Tian and Yu, Yang",
        "title": "Policy optimization in rlhf: The impact of out-of-preference data"
      },
      {
        "key": "lin2024limited",
        "author": "Lin, Yong and Seto, Skyler and ter Hoeve, Maartje and Metcalf, Katherine and Theobald, Barry-John and Wang, Xuan and Zhang, Yizhe and Huang, Chen and Zhang, Tong",
        "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization"
      }
    ]
  }
]