\section{Related Work}
\label{appendix_related_work}
\textbf{Reinforcement Learning from Human Feedback (RLHF).} The Large Language Models (LLMs) \cite{zhao2023survey,chang2024survey,hadi2024large,minaee2024large,hadi2023survey,achiam2023gpt,bubeck2023sparks} is one of the most promising evolutions towards Artificial General Intelligence (AGI). The success of this transformation lies a critical component: Reinforcement Learning from Human Feedback (RLHF) or Human Alignment (HA), which is the final and crucial step in LLMs' training \cite{arumugam2019deep,ouyang2022training,singh2022flava,bai2022training,dai2023safe}. Motivated by the instability, complexity, and incurring significant computational costs of the RLHF process, \cite{DPO} proposed an algorithmic framework for directly optimizing the language model to follow human preferences, namely the DPO algorithm, based on the analysis of the optimal solutions of classic RL algorithm PPO \cite{schulman2017proximal,engstrom2019implementation}. DPO has received widespread attention since its inception, and hundreds of algorithms for improving DPO have been derived within a year, e.g. IPO\cite{IPO}, KTO\cite{KTO}, SimPO\cite{SimPO}, ORPO\cite{ORPO}, etc. 

\textbf{DPO $\&$ PPO discussing.}
The success of DPO \cite{DPO} benefits from the optimal solution analysis of the KL-constrained reward maximization objective (Eq.3 in \cite{DPO}) come from the classical Reinforcement Learning (RL) algorithm, PPO \cite{PPO}. Therefore, the performance difference between the DPO algorithm and the PPO based RLHF method \cite{PPO-basedRLHF} has gradually attracted the attention of researchers in the RL community \cite{ivison2024unpacking}. In general, while the DPO algorithm can fit the static training dataset comparably, it generalizes less effectively than PPO based RLHF \cite{lin2024limited}. While PPO based RLHF usually performs better in the state-of-the-art production-level LLMs \cite{yan20243d}, its correct fine-tuning usually requires more sophisticated techniques \cite{xu2024dpo}.

There are many works that empirically discuss the relationship between the DPO algorithm and the PPO algorithm in RLHF. From the perspective of algorithm design, \cite{xu2024dpo,ivison2024unpacking,yan20243d} pointed out that the PPO algorithm is generally better than the DPO algorithm in human preference alignment tasks, but the PPO algorithm requires various additional tricks, such as advantage normalization, large batch size, and exponential moving average update for the reference model. \cite{zhong2024dpo} combined the advantages of DPO and PPO to create a more effective algorithm named RTO at the token-level. From the perspective of data source, \cite{tang2024understanding} classified algorithms such as DPO as offline algorithms, while the PPO based RLHF is an online algorithm. This demonstration is consistent with our paper. \cite{li2023policy,lin2024limited} showed that compared with the DPO algorithm that only relies on static data sets, the PPO algorithm can use sufficient non-preferred data for policy optimization to significantly improve performance by relying on the generalization ability of its Reward model.