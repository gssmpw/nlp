\newpage
\onecolumn
\appendix
\section*{\LARGE Appendix}

% In the Appendix, we provide additional experiment details and spell out the proofs omitted in the text.

\section{Theoretical Analysis}
\label{appen:proof}
Here, we present complete proofs of our theoretical study, starting with notations.
We assume all the samples (time series) are absolutely summable, and finite.
% while considering they are sampled from a finite-dimensional vector space $V$ over a field $\mathbb{R}^d$. 



\subsection{Representations and Notations}
\label{appen:notations}
\subsubsection{Frequency domain}
Fourier transform of a real-valued sample with a finite duration is obtained as in Equation~\ref{eq:appendix_fourier}.

\begin{equation}\label{eq:appendix_fourier}
     \mathcal{F}(\boldsymbol{\mathrm{x}}) = |X(e^{j \omega})| e^{j \phi(\omega)} = \int_{-\infty}^{\infty} \mathbf{x}(t) e^{-j \omega t},
\end{equation}

where $\omega = \frac{2 \pi}{\mathrm{T}}$, and $\omega$ and $\mathrm{T}$ are the frequency in radian and period for all sinusoidals~\footnote{Harmonics and sinusoids are used interchangeably throughout the paper.} in the range of Nyquist rate. 
$|X(e^{j \omega})|$ and $\phi(\omega)$ denote the amplitude and phase for all frequencies, respectively. 
Thus, the amplitude and phase angle of a particular sinusoidal are represented as $|X(e^{j \omega_0})|$ and $\phi(\omega_0)$.
Similarly, the period for this sinusoidal is $\mathrm{T}_0 = 2\pi/\omega_0$.
The phase difference between a sinusoidal and an angle $\phi$ is shown in Equation~\ref{eq:appendix_phase_dif}.

\begin{equation}\label{eq:appendix_phase_dif}
    \Delta \phi_{\mathbf{x}(t)} = 
    \begin{cases}
      \frac{(\theta - 2\pi) * T_0}{2 \pi}, & \text{if } \theta > \pi \\    
      \frac{\theta * T_0}{2 \pi}, & \text{else}
    \end{cases},
    \hspace{2mm} \text{and} \hspace{2mm} \theta = [\phi(\omega_0) -\phi]\ \%\ 2\pi,
\end{equation}

where $\mathrm{T}_0/2\pi = 1/\omega_0$.
The calculated phase difference between the sample and given angle normalizes the phase change for the sinusoidal to exactly match the angle $\phi$ when shifted $\Delta \phi_{\mathbf{x}(t)}$ in the complex domain as in equations below.
\begin{gather}\label{eq:appendix_complex_shift}
    \mathcal{F}(\boldsymbol{\mathrm{x}}) = |X(e^{j \omega})| e^{j \phi(\omega)}  e^{-j \omega \Delta \phi} \hspace{3mm} \text{after shifting} \hspace{3mm} |X(e^{j \omega})| e^{j \phi(\omega_0)}  e^{-j \omega_0 (\theta/\omega_0 )}  \\
    |X(e^{j \omega_0})| e^{j \phi(\omega_0)}  e^{-j \omega_0 (\phi(\omega_0) - \phi)/\omega_0 )} \\
    |X(e^{j \omega_0})| e^{j \phi} \hspace{3mm} \text{for the sinusoidal with frequency $\omega_0$} 
\end{gather}
\subsubsection{Transformation and Diffeomorphisms}
\label{sec:Diffeomorphisms}
The observed samples $\mathbf{x}(t)$ from the sets with the (shift) variants are considered elements of manifolds $M^{\phi}$ according to the phase angle $\phi(\omega_0)$ of the harmonic with a period equal to or longer than the length of the segment $t$, i.e., the specific harmonic with period $\mathrm{T}_0$ and frequency $\omega_0$. In other words, if the phase angle of the harmonic $\omega_0$ is $\phi_a$ for a sample $\mathbf{x}(t)$, the sample lies in manifold $\mathcal{M}^{\phi_a}$.
% Add T_o here as well
The mapping function $f:\mathcal{M}^{\phi_a} \rightarrow \mathcal{M}^{\phi_b}$ between manifolds is defined as $\mathcal{T}(\mathbf{x}(t), \phi_b) = (\Tilde{\mathbf{x}}(t), \Delta \phi_b)$ where $\mathbf{x}(t)$ lies in $\mathcal{M}^{\phi_a}$, and $\Tilde{\mathbf{x}}(t)$ lies in $\mathcal{M}^{\phi_b}$.
Similarly, the inverse mapping $f^{-1}:\mathcal{M}^{\phi_b} \rightarrow \mathcal{M}^{\phi_a}$ is represented as $\mathcal{T}(\Tilde{\mathbf{x}}(t),\phi_a) = (\mathbf{x}(t), \Delta \phi_a)$.


$\Tilde{\mathbf{x}}(t)$ and $\mathbf{x}(t)$ only differ by a random time shift $t^{\prime}$ where this random time shift can be calculated using the phase angles of harmonics with frequency $\omega_0$, which makes the presented transformation a bijective function between time series manifolds.
The differentiation of the mapping function at sample $\mathbf{x}(t)$ is shown as $D f_x:T_x \mathcal{M}^{\phi_a} \rightarrow T_{f_x} \mathcal{M}^{\phi_b}$.

\clearpage

\subsection{Notation List}
\label{appen:notation_list}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.7}
    \begin{tabular}{ll}
        \textbf{Notation} & \textbf{Description} \\
        \hline
        $\boldsymbol{\mathrm{x}}$ & Time series represented as a bold lowercase symbol \\
        $\mathcal{F}(\mathrm{x})$ & Discrete Fourier transformation of time series $\mathrm{x}$ \\
        $\angle$ & The complex argument for obtaining phase values \\
        $\omega$ & Variable that represents the frequencies in radian \\
        $|X(e^{j \omega})|$ & Magnitude components of the Fourier transformation of a time series \\
        $\phi(\omega)$ & Phase components of the Fourier transformation of a time series \\
        $\omega_0$ & Frequency of the specific harmonic whose period is equal or longer than the sample $\mathbf{x}$ \\
        $\mathrm{T}_0 = \frac{2\pi}{\omega_0}$ & Period of the specific harmonic with frequency $\omega_0$ \\
        $\Phi$ & Random variable for phase angles, i.e., $\phi \sim \Phi$ \\
        $\phi(\omega_0)$ & Phase angle of a specific harmonic with the frequency $\omega_0$ \\
        $\phi_{\mathbf{x}(t)}$ & Variable that represents phase angles of all harmonics for sample $\mathbf{x}(t)$ \\ 
        $\phi_{\mathbf{x}(t-t^{\prime})}$ & Variable that represents phase angles for sample $\mathbf{x}(t-t^{\prime})$ \\ 
        $\phi_{\mathbf{x}(t)} (\omega_0)$ & Phase angle of the specific harmonic with the frequency $\omega_0$ for sample $\mathbf{x}(t)$ \\
        $\mathcal{T}(\mathbf{x}, \phi)$ & The proposed transformation function \\
        $\Delta \phi$ & Phase difference between two angles \\ 
        $\Delta \phi_{\mathbf{x}(t)}$ &\multicolumn{1}{p{12cm}}{Phase difference between the given angle $\phi$ in the transformation and the harmonic with frequency $\omega_0$ when sample $\mathbf{x}(t)$ is decomposed using Fourier transformation} \\
        $\Delta \phi_{\mathbf{x}(t - t^{\prime})}$ &\multicolumn{1}{p{12cm}}{Phase difference between the given angle $\phi$ in the transformation and the harmonic with frequency $\omega_0$ when sample $\mathbf{x}(t - t^{\prime})$ is decomposed using Fourier transformation} \\
        $\mathcal{M}^{\phi}$ & Manifold notation with an angle $\phi$ \\
        $\mathcal{M}^{\phi_a}$ & A specific manifold with the angle $\phi_a$ \\
        $\mathcal{T}(\mathbf{x}, \phi_a)$ & The output of the transformation, i.e., a time series that lies on the manifold $\mathcal{M}^{\phi_a}$ \\
        $\mathcal{F}(\mathcal{T}(\mathbf{x}, \phi))$ &\multicolumn{1}{p{12cm}}{
        Fourier transformation of the output from the proposed transformation: Since the transformation function produces a tuple, we specifically apply the Fourier transformation to the first output, which corresponds to the time series. 
        } \\        
        $f_{\theta}(.)$ & Parametric mapping with parameter $\theta$ \\
        $f_{G_{\theta}}: \mathbb{R}^d \rightarrow \Phi$ & The guidance network that outputs an angle to map the sample to a specific manifold \\
        $f_{C_{\theta}}: X \rightarrow Y$ & The classifier neural network \\
        $\text{Var} (.)$ & Variance function \\
        $\%$ & Modulo operation \\
        \bottomrule
    \end{tabular}
    \caption{Detailed list of notations used in this work}
    \label{tab:notations}
\end{table}


\clearpage

\subsection{Proofs}
\begin{lemma}[Circular Shift]\label{append_lemma:upper_bound_shift_space}
Given a sample $\mathbf{x}$ in the interval $[0,t_{int}]$ and its shifted version $\mathbf{x}(t-t^{\prime})$, where the shift is a random value from the finite real numbers, i.e., $t^{\prime} \in (-\infty, \infty)$.
The shift is periodic with the signal length $t_{int}$, leading to the same vector representation when the shift is an integer multiple of the signal length.
\begin{gather*}
\infty > t > | t^{\prime} | > - \infty, \hspace{2mm} 0 = t^{\prime} \Mod{t_{int}} \implies \mathbf{x}(t) = \mathbf{x}(t-t^{\prime}) 
\end{gather*}
\end{lemma}
\begin{proof}
From circular shift, we know
\begin{gather}
    \mathbf{x}(t) = \mathbf{x}(t \Mod{t_{int}}) \\
    \mathbf{x}(t-t^{\prime}) = \mathbf{x}( (t-t^{\prime}) \Mod{t_{int}})
\end{gather}
Therefore, 0 = $t^{\prime} \Mod{t_{int}} \implies \mathbf{x}(t) = \mathbf{x}(t-t^{\prime}) $
\end{proof}
Throughout the proofs, the length of the samples and their intervals are denoted by the same variable $t$. In other words, the samples are assumed to start at $t=0$ and finish at $t_{int}=t$.


\subsection{Proof for Proposition~\ref{prop:shift_change}}
\begin{proposition}[Time shift as a Group Operation]\label{appendix_prop:shift_change}
Shift operation in time domain defines an Abelian Group of phase angles in the frequency domain for each harmonic with frequency $\omega_k$.
\begin{gather}\label{appendix_eq:prop_shift_change}
        (\Phi_k, + \hspace{-2mm} \mod 2\pi), \hspace{2mm} \text{where} \hspace{2mm} \Phi_k = { \phi \mid \phi = (\phi(\omega_k) + \omega_k t^{\prime}) \text{ mod } 2\pi,  t^{\prime} \in \mathbb{R} }
\end{gather}
\end{proposition}
\begin{proof}
Let $\mathbf{x}(t + t^{\prime})$ be randomly shifted variant of sample $\mathbf{x}(t)$ where $t^{\prime} \in \mathbb{R}$.
We can decompose these two sequences as in below using Fourier transformation. 
\begin{gather}
\mathbf{x}(t) \xrightarrow[]{\mathcal{F(.)}} |X(e^{j \omega})| e^{j \phi(\omega)} \hspace{2mm} \text{and} \hspace{2mm} \mathbf{x}(t+t^{\prime}) \xrightarrow[]{\mathcal{F(.)}} |X(e^{j \omega})|  e^{j ( \phi(\omega) + \omega t^{\prime})} 
\end{gather}
The phase values of these two time series for a harmonic at the given frequency can be expressed as follows.
\begin{gather}
\phi_{\mathbf{x}(t)}(\omega_k) = \phi(\omega_k) \hspace{2mm} \text{and} \hspace{2mm} \phi_{\mathbf{x}(t-t^{\prime})} (\omega_k) = \phi(\omega_k) + \omega_k t^{\prime} 
\end{gather}
Then, we can define a set of phase angles $\Phi_k$ with shift values $t^{\prime}$.
\begin{gather}
\Phi_k = \{ \phi \mid \phi = (\phi(\omega_k) + \omega_k t^{\prime}) \text{ mod } 2\pi,  t^{\prime} \in \mathbb{R} \}
\end{gather}

This set of phase angles with time shift operation defines the circle group $\mathbb{T}$.
The circle group is Abelian~\citep{fuchs1960abelian}, with time shifts corresponding to multiplication in the complex plane. 
For completeness, we have shown all the group axioms.
% Next, we show that this set defines a group under $2\pi$ modular addition.

\subsection*{Group Axioms}
The set of phase angles with time shift, \( \Phi_k = \{ \phi \mid \phi = (\phi(\omega_k) + \omega_k t') \mod 2\pi, t' \in \mathbb{R} \} \), satisfies the five axioms of an Abelian group under modular addition.

\textit{Axiom 1: Closure}
\\[2pt]
For any two phase angles \( \phi_1, \phi_2 \in \Phi_k \), their sum is also in \( \Phi_k \).

Let \( \phi_1 = (\phi(\omega_k) + \omega_k t'_1) \mod 2\pi \) and \( \phi_2 = (\phi(\omega_k) + \omega_k t'_2) \mod 2\pi \). Then their sum is:
\[
\phi_1 + \phi_2 = (\phi(\omega_k) + \omega_k t'_1 + \phi(\omega_k) + \omega_k t'_2) \mod 2\pi
\]
\[
\phi_1 + \phi_2 = (\phi(\omega_k) + \omega_k (t'_1 + t'_2)) \mod 2\pi
\]
Since \( t'_1 + t'_2 \in \mathbb{R} \), the sum is also in \( \Phi_k \), so closure holds.
\\[7pt] 
\textit{Axiom 2: Associativity}
\\[3pt] 
For any three phase angles \( \phi_1, \phi_2, \phi_3 \in \Phi_k \), their sum is associative under addition modulo \( 2\pi \).

\[
((\phi_1 + \phi_2) \mod 2\pi) + \phi_3 ) \mod 2\pi = (\phi_1 + (\phi_2 + \phi_3) \mod 2\pi ) \mod 2\pi
\]

Let \( \phi_1 = (\phi(\omega_k) + \omega_k t'_1) \mod 2\pi \), \( \phi_2 = (\phi(\omega_k) + \omega_k t'_2) \mod 2\pi \), and \( \phi_3 = (\phi(\omega_k) + \omega_k t'_3) \mod 2\pi \). 
Then:

\[
((\phi_1 + \phi_2) \mod 2\pi) + \phi_3 = (\phi(\omega_k) + \omega_k t'_1 + \omega_k t'_2) \mod 2\pi + \phi_3
\]
\\[2pt]
\[
= (\phi(\omega_k) + \omega_k (t'_1 + t'_2) \mod 2\pi + \omega_k t'_3) \mod 2\pi
\]

Similarly, for the right-hand side:

\[
\phi_1 + ((\phi_2 + \phi_3) \mod 2\pi) = \phi_1 + (\phi(\omega_k) + \omega_k (t'_2 + t'_3) \mod 2\pi)
\]
\[
= (\phi(\omega_k) + \omega_k t'_1 + \omega_k (t'_2 + t'_3)) \mod 2\pi
\]

Using $(a + b) \mod 2\pi \hspace{2mm} = (( a \mod 2\pi ) + ( b \mod 2\pi )) \mod 2\pi$, both sides simplify to \( (\phi(\omega_k) + \omega_k (t'_1 + t'_2 + t'_3)) \mod 2\pi \).
Thus, associativity holds under addition modulo \( 2\pi \).
\\[7pt] 
\textit{Axiom 3: Identity Element}
\\[2pt]  
The identity element in this group is the phase angle when no time shift has occurred, i.e., \( t' = 0 \).
\[
\phi_0 = (\phi(\omega_k) + \omega_k \cdot 0) \mod 2\pi = \phi(\omega_k)
\]
For any \( \phi_1 \in \Phi_k \), we have:
\[
\phi_1 + \phi_0 = \phi_1
\]
Thus, \( \phi_0 \) is the identity element.
\\[7pt] 
\textit{Axiom 4: Inverse Element}
\\[2pt]  
For any phase angle \( \phi_1 = (\phi(\omega_k) + \omega_k t') \mod 2\pi \), its inverse is:
\[
\phi^{-1}_1 = (-\omega_k t') \mod 2\pi
\]
Then:
\[
\phi_1 + \phi^{-1}_1 = (\phi(\omega_k) + \omega_k t' - \omega_k t') \mod 2\pi = \phi(\omega_k)
\]
Thus, each element has an inverse.
\\[7pt] 
\textit{Axiom 5: Commutativity}
\\[2pt]  
For any two phase angles \( \phi_1, \phi_2 \in \Phi_k \), the sum is commutative:
\[
\phi_1 + \phi_2 = \phi_2 + \phi_1
\]
This follows from the commutativity of modular addition, so the group is Abelian.
\end{proof}

Proposition~\ref{prop:shift_change} states that shift operation in time domain defines an Abelian group of phase angles for each harmonic.
Proposition~\ref{prop:shift_change} holds a key role in our algorithm, as it establishes an abstract connection between the time-domain shift operation and its effects on samples in the frequency domain.


% the labelling criteria of time series samples are shift-invariant.
% Although we use the shift-consistency to prove this, a more rigorous approach can use group representations as follows.

% Given the time series data $\mathbf{x}$ and corresponding labels $\mathbf{y}$.
% We define the operation of time shift $t'$ as a group operation, where $t'$ represents the amount of shift. 
% The set of all possible time shifts forms a group under addition, denoted by $(\mathbb{T}, +)$, where $\mathbb{T}$ is the set of shifts bounded by $[0, 2\pi]$ (Using Lemma~\ref{append_lemma:upper_bound_shift_space}), taken periodically to cover every value within this interval.
% We represent the effect of time shift $t'$ on the time series data $\mathbf{x}$ as a linear transformation. 
% In other words, the transformation is represented by $T_{t'}: \mathbf{x} \rightarrow \mathbf{x}(t-t^{\prime})$, where $\mathbf{x}(t-t^{\prime})$ represents the time series data after the shift.
% Since time shift is a linear transformation, it forms a group homomorphism from the group of time shifts $(\mathbb{T}, +)$ to the group of linear transformations on the space of time series data. 
% In other words, for any two time shifts $t_1'$ and $t_2'$, the corresponding transformation matrices satisfy (withing the same modulo of Lemma~\ref{append_lemma:upper_bound_shift_space}):
% \[M_{t_1' + t_2'} = M_{t_1'} \cdot M_{t_2'}\]
% Since time shift is a bijective linear transformations of a vector space to itself, the relevant information that raw time series data conveys about the label space  (i.e., $\mathcal{I}(\mathbf{y}; \mathbf{x})$) remains constant despite a random time shift $t^{\prime}$.
% The logical reasoning for this proposition is also quite straightforward. 


\clearpage
\subsubsection{Proof for Theorem~\ref{theorem:guarentees_shifting}}
\label{sec:proof_for_theorem_bijectivity}
\begin{theorem}[Covering the Entire Time Space Injectively]\label{append_theorem:guarentees_shifting}
Given a sample $\mathbf{x}$, the defined function $\mathcal{T}(\mathbf{x}, \phi): \Phi \times \mathbb{R}^d \rightarrow \mathbb{R}^d \times \Delta \Phi$ is bijective such that all shift variants of a sample can be covered with the unique phase angle of a harmonic whose period is longer or equal to the length of $\mathbf{x}$.  
% \begin{gather*}
% \forall \phi_a, \phi_b \in \Phi,\  \mathcal{T}(\mathbf{x}, \phi_a) = \mathcal{T}(\mathbf{x}, \phi_b) \implies \phi_a = \phi_b \\
% \forall t^{\prime} \in \mathbb{R},\ \exists \phi \in \Phi,\ \mathcal{T}(\mathbf{x},\phi) = \mathbf{x}(t-t^{\prime}),
% \end{gather*}
\begin{gather*}
\forall \phi_a, \phi_b \in \Phi,\  \mathcal{T}(\mathbf{x}, \phi_a) = \mathcal{T}(\mathbf{x}, \phi_b) \implies \phi_a = \phi_b \\
\forall t^{\prime} \in \mathbb{R},\ \exists \phi \in \Phi,\ \mathcal{T}(\mathbf{x},\phi) = \left(\mathbf{x}(t - t^{\prime}), \Delta \phi\right),
% \\
% \mathcal{T}(\mathbf{x}, \phi) = (\mathbf{x}(t - t^{\prime}), \Delta \phi)
\end{gather*}
\end{theorem}

\begin{proof}
\begin{gather}
\mathbf{x}(t) \xrightarrow[]{\mathcal{F(.)}} |X(e^{j \omega})| e^{j \phi(\omega)} \\
\mathbf{x}(t-t^{\prime}) \xrightarrow[]{\mathcal{F(.)}} |X(e^{j \omega})|  e^{j ( \phi(\omega) - \omega t^{\prime})} \\
\phi_{\mathbf{x}(t)} = \phi(\omega),\hspace{2mm} \phi_{\mathbf{x}(t-t^{\prime})} = \phi(\omega) - \omega t^{\prime} \\
\phi_{\mathbf{x}(t)} - \phi_{\mathbf{x}(t-t^{\prime})} = \omega t^{\prime},
\end{gather}
Using Euler's formula, 
\begin{gather}
\phi_{\mathbf{x}(t)}  - \phi_{\mathbf{x}(t-t^{\prime})} = \left(\omega t^{\prime}\right)\Mod{2\pi} \\
\phi_{\mathbf{x}(t)}  - \phi_{\mathbf{x}(t-t^{\prime})} = \left(\frac{2\pi}{T} t^{\prime}\right)\Mod{2\pi} \\ 
\forall T_0 \in T,\ \infty > T_0 \geq t \implies \phi_{\mathbf{x}(t)} (\omega_0)  - \phi_{\mathbf{x}(t-t^{\prime})}(\omega_0) = \frac{2\pi}{T_0} t^{\prime}
\end{gather}
Thus,
\begin{gather}
\phi_{\mathbf{x}(t)} (\omega_0)  - \phi_{\mathbf{x}(t-t^{\prime})}(\omega_0) = \phi_{\mathbf{x}(t)} (\omega_0)  - \phi_{\mathbf{x}(t-t^{\prime})}(\omega_0) \implies t^{\prime} = t^{\prime}
\end{gather}
Also,
\begin{gather}
\forall T_0 \in T,\ \infty > t > T_0 \implies \phi_{\mathbf{x}(t)} (\omega_0)  - \phi_{\mathbf{x}(t-t^{\prime})}(\omega_0) = \left(\frac{2\pi}{T_0} t^{\prime}\right)\mod{2\pi}, \\
\because  \exists t^{\prime} \in \mathbb{R}, \left(\frac{2\pi}{T_0} t^{\prime}\right) > 2 \pi \\
\therefore \hspace{2mm} \exists t^{\prime}, t^{\prime} \in \mathbb{R},\hspace{2mm} \neg (\phi_{\mathbf{x}(t)} (\omega_0)  - \phi_{\mathbf{x}(t-t^{\prime})}(\omega_0) = \phi_{\mathbf{x}(t)} (\omega_0)  - \phi_{\mathbf{x}(t-t^{\prime})}(\omega_0) \implies t^{\prime} = t^{\prime}),
\end{gather}
which proves injection. Similarly, for surjection using Lemma~\ref{append_lemma:upper_bound_shift_space},
\begin{gather}
    \forall t^{\prime} \in \mathbb{R},\hspace{2mm} \infty > T_0 \geq t > |t^{\prime}| \implies \phi_{\mathbf{x}(t)} (\omega_0)  - \phi_{\mathbf{x}(t-t^{\prime})}(\omega_0) = \frac{2\pi}{T_0} t^{\prime}
\end{gather}
\end{proof}
The final equation completes the proof by showing that the phase difference between the sample and its shifted version can get unique values for any shift, i.e., covering the whole time space.

\clearpage

\subsubsection{Proof for Theorem~\ref{theorem:guarentees_transformation}}
\setlength{\jot}{3mm}
\begin{theorem}[Guarantees for Shift-Invariancy]
Given a sample $\mathbf{x}$ and a randomly shifted variant of it $\mathbf{x}(t-t^{\prime})$, if the transformation function $\mathcal{T}(\mathbf{x}, \phi)$ is applied to both samples with the same angle $\phi_a$, the resulting time series will be the same while carrying the same information.
\begin{gather*}\label{appendix_eq:guarentees_transformation}
% \mathcal{T}(\mathbf{x}(t), \phi_a) = \mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a) 
\mathcal{T}(\mathbf{x}(t), \phi_a) = \left( \Tilde{\mathbf{x}}(t),\ \Delta \phi_{\mathbf{x}(t)} \right), \hspace{3mm} \mathcal{T}(\mathbf{x}(t - t^{\prime}), \phi_a) = \left( \Tilde{\mathbf{x}}(t),\ \Delta \phi_{\mathbf{x}(t - t^{\prime})} \right)
\end{gather*}
\end{theorem}
\begin{proof}
    \begin{gather}
    \mathbf{x}(t) \xrightarrow[]{\mathcal{F(.)}} |X(e^{j \omega})| e^{j \phi(\omega)}, \hspace{2mm} \mathbf{x}(t-t^{\prime}) \xrightarrow[]{\mathcal{F(.)}} |X(e^{j \omega})| e^{j \phi(\omega)} e^{\omega t^{\prime}} \\
        \phi_{\mathbf{x}(t)} = \phi(\omega), \hspace{2mm} \phi_{\mathbf{x}(t-t^{\prime})} = \phi(\omega) - \omega t^{\prime}  \\
         \phi_{\mathbf{x}(t-t^{\prime})} - \phi_{\mathbf{x}(t)} = - \omega t^{\prime}
         \end{gather}
         Using Equation~\ref{eq:appendix_phase_dif}, the phase difference between samples $(\mathbf{x}(t), \mathbf{x}(t-t^{\prime}))$ and $\phi_a$ can be obtained as,
         \begin{gather}
         \Delta \phi_{\mathbf{x}(t)} = \frac{[\phi(\omega_0) - \phi_a]}{2\pi} \cdot T_0,\ \hspace{3mm} \Delta \phi_{\mathbf{x}(t-t^{\prime})} = \frac{[\phi(\omega_0) - \omega_0 t^{\prime} - \phi_a]}{2\pi} \cdot T_0 \\
         \Delta \phi_{\mathbf{x}(t-t^{\prime})} - \Delta \phi_{\mathbf{x}(t)} = -\omega_0 t^{\prime} \frac{T_0}{2\pi} 
         \end{gather}
         Using Fourier transform as in Equation~\ref{eq:phase_shift2},
         \begin{gather}
         \mathcal{F}\left\{ \mathcal{T}(\mathbf{x}(t),\phi_a) \right\} = |X(e^{j \omega})| e^{j \phi(\omega)} e^{-j \omega \Delta \phi_{\mathbf{x}}(t)} \\
         \mathcal{F}\left\{ \mathcal{T}(\mathbf{x}(t-t^{\prime}),\phi_a) \right\} = |X(e^{j \omega})| e^{j \phi(\omega)} e^{-j\omega t^{\prime}} e^{-j \omega \Delta \phi_{\mathbf{x}_{(t-t^{\prime})}}} 
         \end{gather}
         Given that the amplitudes are identical, demonstrating equality in phase is sufficient, as shown below,
         \begin{gather}
         \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)} = \phi(\omega) - \omega \Delta \phi_{\mathbf{x}(t)},  \hspace{3mm}
         \phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)} = \phi(\omega) - \omega t^{\prime} - \omega \Delta \phi_{\mathbf{x}_{(t-t^{\prime})}} \\
         \phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)} - \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)} = -\omega t^{\prime} -\omega \Delta \phi_{\mathbf{x}_{(t-t^{\prime})}} + \omega \Delta \phi_{\mathbf{x}(t)} \\       
         \phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)} - \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)} = -\omega t^{\prime} -\omega \left[\Delta \phi_{\mathbf{x}_{(t-t^{\prime})}} - \Delta \phi_{\mathbf{x}(t)}\right] \\     
         \phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)} - \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)} = -\omega t^{\prime} -\omega \left[-\omega_0 t^{\prime} \frac{T_0}{2\pi}\right] \\          
         \phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)} - \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)} = -\omega t^{\prime} +\omega \left[\frac{2 \pi}{T_0} t^{\prime} \frac{T_0}{2\pi}\right] \\         
         \phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)} - \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)} = -\omega t^{\prime} + \omega  t^{\prime} \\  
         \phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)} = \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)}  \\
        \mathcal{F}\left\{ \mathcal{T}(\mathbf{x}(t),\phi_a) \right\} = |X(e^{j \omega})| e^{j \phi_{\mathcal{T}(\mathbf{x}(t), \phi_a)}} \\
         \mathcal{F}\left\{ \mathcal{T}(\mathbf{x}(t-t^{\prime}),\phi_a) \right\} = |X(e^{j \omega})| e^{j\phi_{\mathcal{T}(\mathbf{x}(t-t^{\prime}), \phi_a)}}      
    \end{gather}
    Therefore, the output time series samples will be the same after applying the transformation.
\end{proof}
We complete the proof by showing the phase and magnitude of Fourier transformation of both samples are the same after transformation even though a random unknown shift is applied to the sample.
Thus, the proposed transformation guarantees shift-invariancy without limiting the range of shifts.
% This proof shows that we can map any sample to a desired manifold $M^{\phi}$, regardless of its original manifold.
\clearpage
% \subsubsection{Differentiability of the Transformation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm}
\label{appen:Algorithm}
In this section, we present the pseudocode and the PyTorch~\citep{Torch} implementation for the proposed transformation function. 
Algorithm~\ref{alg:prop_transformation} details each step of the transformation, which takes a sample, $\mathbf{x}$, and an angle, $\phi$ as inputs and outputs the transformed sample.

\begin{algorithm}
\caption{\label{alg:prop_transformation} Algorithm for the proposed diffeomorphism.}
\begin{algorithmic}[1]
\setlength{\itemsep}{1.4ex} % Adjust line spacing here
\State \textbf{Input:} $\mathbf{x}$, $\phi_a$ 
\State \textbf{Output:} $\mathcal{T}(\mathbf{x}, \phi_a)$
\State $|X(e^{j \omega})| e^{j \phi(\omega)} = \int_{-\infty}^{\infty} \mathbf{x}(t) e^{-j \omega t}$
\Comment{\small Calculate the Fourier transformation to obtain harmonics}
\State $\phi(\omega_0) = \angle \left( |X(e^{j \omega_0})| e^{j \phi(\omega_0)} \right)$
\Comment{\small Obtain the angle for the harmonic with period $\mathrm{T}_0$}
\State $\theta = [\phi(\omega_0) -\phi_a]\ \%\ 2\pi$
\State $       \Delta \phi = 
    \begin{cases}
      \frac{(\theta - 2\pi) * T_0}{2 \pi}, & \text{if } \theta > \pi \\    
      \frac{\theta * T_0}{2 \pi}, & \text{else}
    \end{cases}$
\Comment{\small Calculate the phase difference between the harmonic and the angle $\phi$}
\State $|X(e^{j \omega})| e^{j (\phi(\omega) - \omega \Delta \phi) } = |X(e^{j \omega})| e^{j \phi(\omega)} * e^{-j \omega \Delta\phi}$
\Comment{\small Apply a linear phase shift to each harmonic}
\State \textbf{Return:} $\mathcal{F}^{-1}(|X(e^{j \omega})| e^{j (\phi(\omega) - \omega \Delta \phi) })$
\end{algorithmic}
\end{algorithm}


% Define custom colors
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}
\definecolor{commentgreen}{RGB}{50,120,50}
\definecolor{eminence}{RGB}{108,48,130}
\definecolor{weborange}{RGB}{255,165,0}
\definecolor{frenchplum}{RGB}{129,20,83}
% Define custom colors
\definecolor{deepblue}{RGB}{0, 0, 139}
\definecolor{darkgreen}{RGB}{0, 100, 0}
\definecolor{darkpurple}{RGB}{128, 0, 128}
\definecolor{goldenrod}{RGB}{218,165,32}
\definecolor{lightgray}{RGB}{211,211,211}

% Set up the listing environment
\lstset{
  language=Python,
  backgroundcolor=\color{white},
  % commentstyle=\color{green!60!black},
  commentstyle=\color{eminence}\bfseries,
  keywordstyle=\color{purple},              % Keywords (like def) in blue
  numberstyle=\tiny\color{gray},
  stringstyle=\color{purple},
  basicstyle=\ttfamily\footnotesize,
  identifierstyle=\color{black},          % Normal text (identifiers) in black
  breaklines=true,
  frame=single,
  captionpos=b,
  emph={def},
  emphstyle={\color{blue}},               % 'def' keyword in blue
    % Define emphasis for built-in functions like abs (if needed)
  emph={[3]abs},                          % Built-in function 'abs'
  emphstyle={[3]\color{black}},           % 'abs' in black
  escapechar=\&,                          % Escape character for inline LaTeX
  % Custom functions highlighting (orange)
  classoffset=1,                          % Switch to a new class for custom functions
  morekeywords={diffeomorphism,distanceCalculate},  % Custom functions
  keywordstyle=\color{deepblue}\textbf,            % Style for custom functions
  classoffset=0,                          % Return to default class (for built-in keywords)
}


Below, we provide the PyTorch implementation of our proposed transformation function, which includes two functions. 
The first function, \texttt{distanceCalculate}, computes the phase difference between the harmonic with frequency $\omega_0$ and the desired input angles.
The second function, \texttt{diffeomorphism}, performs the main transformation: it takes as inputs the batch of samples $\mathbf{x}$ and the angles $\phi$, and outputs the transformed samples in the time domain.

% Redefine the listing name
\renewcommand{\lstlistingname}{Implementation}

\begin{lstlisting}[caption={PyTorch implementation of the proposed transformation}]
def distanceCalculate(angleDiff):
    theta = angleDiff % (2 * torch.pi)
    # Calculate the angular distance on the unit circle
    theta[theta > torch.pi] -= 2 * torch.pi
    return theta

def diffeomorphism(sample, desiredAngles):
    B, L, D = sample.shape
    samplesFFT = torch.fft.rfft(sample, dim=1)
    freq = torch.fft.rfftfreq(n=L)
    phAngle = torch.angle(samplesFFT)
    # Get the phase angle of the harmonic with frequency T_0
    angles = phAngle[torch.arange(phAngle.size(0)), 1, 0].squeeze()
    # Calculate the angle difference 
    theta = distanceCalculate(angles-desiredAngles)
    # Normalize it to the sepecific harmonic with frequency w_0
    dtheta = theta / (2*torch.pi*freq[1])  
    # Create complex exponentials with specific phase values
    linShift = torch.exp(-1j*2*torch.pi*freq[None,:]*dtheta[:,None])
    linShift = linShift.unsqueeze(dim=2).expand(-1, -1, D)
    # Apply a linear phase shift to all harmonics
    shiftedFFT = linShift*(samplesFFT)
    # Return to the time domain
    transformedSamples = torch.fft.irfft(shiftedFFT, n=L, dim=1)
    return transformedSamples
\end{lstlisting}




\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{appen:experiments}
Here, we give a detailed description of datasets, architectures, metrics, and training details for our experiments. 
We performed our experiments on NVIDIA GeForce RTX 4090 GPUs, involving training with three random seeds for all datasets, totaling approximately 480 GPU hours including ablation.
We reported the mean of three runs with the standard deviation.

\subsection{Datasets}
\label{appendix:datasets}
In this section, we give details about the datasets that are used during our experiments.
Overall, we have used eight datasets with six different time series tasks including \textit{heart rate prediction}, \textit{cardiovascular disease classification}, \textit{activity recognition}, \textit{step counting}, \textit{sleep stage classification}, and \textit{lung sound classification} from six sensor modalities \textit{photoplethysmography}, \textit{electrocardiogram}, \textit{inertial measurement units}, \textit{electroencephalography}, and \textit{audio}.
When selecting datasets for our experiments, we prioritized signals that provide meaningful insights into individuals' mental and physical health, where robust inference is particularly critical.
Therefore, we specifically choose signals generated by humans.
Additional to main experiments, in appendix, we also included \textit{lung audio classification} from \textit{audio} signals.

\subsubsection{Heart Rate Prediction}

\paragraph{IEEE SPC}  
The IEEE SPC dataset overall has 22 recordings of 22 subjects, ages ranging from 18 to 58 performing three different activities~\citep{Binary_CorNet}. 
Each recording has sampled data from three accelerometer signals and two PPG signals along with the ECG data with a sampling frequency of 125\,Hz. 
All these recordings were recorded from the wearable device placed on the wrist of each individual. 
All recordings were captured with two 2-channel PPGs with green LEDs, a tri-axial accelerometer, and a chest ECG for the ground-truth HR estimation. 
We averaged the two channels of PPG for prediction.
We choose the last five subjects of SPC22 to be used for source domains.
Throughout our experiments, we used PPG channels without integrating any inertial measurements. 

\paragraph{Dalia} 
The PPG dataset for motion compensation and heart rate estimation in Daily Life Activities (DaLiA) was recorded from 15 subjects (8 females, 7 males, mean age of $30.6$), where each recording was approximately two hours long. 
PPG signals were recorded while subjects went through different daily life activities, for instance sitting, walking, driving, cycling, working, and so on. 
PPG signals were recorded at a sampling rate of 64\,Hz. 
The first five subjects are used as source domains, similar to~\citet{demirel2023chaos}.


We standardize all PPG datasets as follows, same as the previous works~\citep{CorNET}.
Initially, a fourth-order Butterworth bandpass filter with a frequency range of 0.5--4\,Hz is applied to PPG signals. 
Subsequently, a sliding window of 8 seconds with 2-second shifts is employed for segmentation, followed by z-score normalization of each segment. 
Lastly, the signal is resampled to a frequency of 25\,Hz for each segment.
We used an 8-block ResNet model with a stride of 2, a learning rate of \(5e{-4}\), and a batch size of 32.
Additional results with further experiments can be found in Appendix~\ref{appendix:Additional_Results}.


\subsubsection{Human Activity Recognition}

\paragraph{UCIHAR}
Human activity recognition using a smartphone's dataset (UCIHAR)~\citep{UCIHAR} is collected by 30 subjects within the age range of 16 to 48 performing six daily living activities with a waist-mounted smartphone. 
Six activities include walking, sitting, lying, standing, walking upstairs, and walking downstairs. 
Data is captured by 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50\,Hz.
We used the pre-processing technique the same as in~\citep{GILE} such that the input contains nine channels with 128 features (it is sampled in a sliding window of 2.56 seconds and 50\% overlap,
resulting in 128 features for each window).
Windows are normalized to a mean of zero and unit standard deviation before feeding it to the models.
The experiments are conducted with a leave-one-domain-out strategy with the first five subjects, where one of the domains is chosen to be the unseen target~\citep{KDD_paper}.
We used an 8-block ResNet model with a stride of 2, a learning rate of \(3e{-3}\), and a batch size of 32.


\paragraph{HHAR}
Heterogeneity Dataset for Human Activity Recognition (HHAR) is collected by nine subjects within an age range of 25 to 30 performing six daily living activities with eight different smartphones---Although HHAR includes data from smartwatches as well, we use data from smartphones---that were kept in a tight pouch and carried by the users around their waists~\citep{hhar}.
Subjects then perform six activities including cycling, sitting, descending stairs, ascending stairs, standing, and walking.
Considering the variable sampling frequencies of smart devices in the HHAR dataset, we downsampled the readings to 50\,Hz. We employed sliding windows with lengths of 100 (two seconds) and 50, using a specified step size.
These windows were then normalized to a mean of zero with unit standard deviation. In our experiments, we utilized the data from the first four subjects (i.e., a, b, c, d) as source domains, following a similar approach to previous papers~\citep{KDD_paper, demirel2023chaos}.
We used an 8-block ResNet model with a stride of 2, a learning rate of \(1e{-3}\), and a batch size of 64.
The learning rate \(1e{-3}\) for the guidance network, was the same for the activity recognition task. 




\subsubsection{Cardiovascular disease (CVD) classification}

\paragraph{Chapman}
Chapman University, Shaoxing People’s Hospital (Chapman) ECG dataset which provides 12-lead ECG with a 10-second sampling rate of 500\,Hz. 
The recordings are downsampled to 100\,Hz, resulting in each ECG frame consisting of 1000 samples.
The labeling setup follows the same approach as in~\citet{chapman} with four classes: atrial fibrillation, GSVT, sudden bradycardia, and sinus rhythm. 
The ECG frames are normalized to have a mean of 0 and scaled to have a standard deviation of 1.
We split the dataset to 80--20\% for training and testing as suggested in~\citet{chapman}.
We chose leads I, II, III, and V2 during our experiments for both ECG datasets.

\paragraph{PhyioNet 2017}
The 2017 PhysioNet/CinC Challenge aims to classify, from 8,528 single-lead ECG recordings (between 30 s and 60 s in length), whether the recording shows normal sinus rhythm, atrial fibrillation (AF), an alternative rhythm, or is too noisy to be classified, i.e., four classes.
We normalize the signals to have zero mean and unit standard deviation.
Additionally, we zero-pad the shorter recordings to ensure they have the same length.
We split the dataset into training, validation, and test sets according to the patients using a 60, 20, 20 configuration.

For both datasets in CVD task, we used an 8-block ResNet model with a stride of 2, a learning rate of \(5e{-4}\), and a batch size of 32.
The learning rate for the guidance network was set same as the main classifier architecture.


\subsubsection{Step counting}
The Clemson dataset has 30 participants (15 males, 15 females), Each participant wore three Shimmer3 sensors.
We used the IMU sensor readings from non-dominant wrists to predict step count where each sensor recorded accelerometer and gyroscope data at 15 Hz. 
We calculated the total magnitude of the accelerometer and fed it to the model as a pre-processing without any filtering.
We used window lengths of 32 seconds without an overlap in the regular walking setting.
We conducted 10-fold cross-validation, with each fold consisting of 3 subjects for testing and validation.
And, six randomly selected subjects were used for training in each fold.
We used a 3 layer of FCN architecture, a learning rate of \(5e{-4}\), and a batch size of 64.
The learning rate for the guidance network was set same as the main classifier architecture. 


\subsubsection{Sleep stage classification}
We used the Sleep-EDF dataset which has five classes: wake (W), three different non-rapid eye movements (N1, N2, N3), and rapid eye movement (REM).
The dataset includes whole-night PSG sleep recordings, where we used a single EEG channel (i.e., Fpz-Cz) with a sampling rate of 100\,Hz.
We employed the identical data split as presented in the paper~\citep{tstcc}, accessible online, without applying any additional pre-processing steps. 
we used a 16-block ResNet model with a stride of 2, a learning rate of \(1e{-3}\), and a batch size of 64.
We ran three distinct seeds using the same split and reported the mean and standard deviation on the test set.


\subsubsection{Lung sound classification}
We used Respiratory@TR which contains lung sounds recorded from left and right sides of
posterior and anterior chest wall and back using two digital stethoscopes from 42 subjects collected in Antakya State Hospital~\citep{Altan2017MultimediaRD}.
The 12 channels of lung sounds are focused on upper lung, middle lung, lower lung and costophrenic angle areas of posterior and anterior sides of the chest.
The recordings are validated and labeled by two pulmonologists evaluating the collected chest X-ray, PFT and
auscultation sounds of the subjects.
Labels fall into 5 COPD severities (COPD0, COPD1, COPD2, COPD3, COPD4).
The patients aged 38 to 68 are selected from different occupational groups, socio-economic status and genders.
We performed 10-fold cross-validation on data
from 42 subjects, with one fold reserved for validation in each iteration.
All 12 channels, combining left and right chest wall recordings from six channels each, were used.


The audio was segmented into 8-second windows with a 2-second overlap to capture temporal patterns.
% As the signal modality differs for audio application, we have optimized certain hyperparameters accordingly.
We used an 8-block ResNet model with a stride of 3, a learning rate of \(1e{-5}\), and a batch size of 15.
Although audio models typically use Mel-frequency spectrograms with different networks, we did not observe a significant performance difference between these models and our initial model.
Therefore, we used the original model with temporal data, consistent with our main experiments.


\subsection{Metrics}
We used the common evaluation metrics in the literature for each task.
Specifically, we used mean absolute error (MAE), root mean square error (RMSE), and Pearson correlation coefficient ($\rho$) for \textit{heart rate prediction}.
We used accuracy (Acc), macro-F1 score (F1) for activity recognition, and an additional area under the receiver operating characteristic curve (AUC) for cardiovascular disease classification~\citep{CLOCS}.
We used the mean absolute percentage error (MAPE) for step counting~\citep{step_counting, FEMIANO2022206}.
For lung audio classification, we evaluated performance using accuracy, macro F1, and weighted F1 scores (F1, W-F1).
For sleep stage classification, we used the same metrics—accuracy, macro F1, and weighted F1 scores (F1, W-F1)—along with Cohen’s Kappa coefficient ($\kappa$)~\citep{Cohen1960ACO}.



\subsection{Baselines}
\label{appendix:baselines}
\subsubsection{LPF (Blurring)}
In convolutional neural networks, pooling layers or convolutions with strides greater than 2 would conduct downsampling on the feature maps to reduce their size. 
However, since features are averaged or discarded in the downsampling, information may be lost, i.e., aliasing. 
Traditional pooling aggregates all values within the window to a single value.
In contrast, \citet{zhang2019shiftinvar} aims to minimize information loss caused by pooling via replacing the traditional kernel with a Gaussian kernel as an low-pass filtering (LPF). 
Specifically, LPF applies a Gaussian-weighted function to the neighborhood values around each feature for convolution, and obtains a weighted average result.


Compared to the pooling operations of simply selecting the maximum/average value within the window, LPF uses Gaussian-weighted averages to preserve the relative positions and spatial relationships between features. 
In our implementations, we used 1D version of LPF with a length of 5. 
We evaluated filter lengths of ${3, 5, 7}$ to determine the optimum size for each task.
During our experiments, we observed the best performance with a filter length of 5, except for HR prediction, where a length of 3 was optimal.


\subsubsection{APS}
To make the sampling layer invariant to shifts, \citet{aps} proposed to subsample by partitioning feature maps into polyphase components and select the component with the highest norm.
This approach has a significant limitation when applied to time series, especially with the nonlinear activation functions. 
It tends to overlook variations in boundaries arising from the translation of samples, thereby imposing additional shift constraints~\citep{learnable_polyphase}.
Consequently, the evaluation of these methods is restricted to a narrow range of shifts, covering only a limited subset of the shift space. 
Moreover, as this approach selects the component with the highest norm, it requires feature maps to have unique values.


\subsubsection{Wavelet Networks}
Wavelet networks~\citep{Wavelet_Networks} consist of several stacked layers that respect scale and translation. 
At the beginning, the network consists of a lifting group convolution layer that lifts input time-series to the scale-translation group, followed by arbitrarily many group convolutional layers.
At the end of the network, a global pooling layer is used to produce scale-translation invariant representations.
Wavelet Networks are proposed for equivariant mappings between input and output. 
However, to turn an equivariant network into an invariant network, an extra layer that is equivariant in this degenerate sense (in practice, this often means either averaging or creating a histogram of the activations of the last layer) should be applied~\citep{kondor18a_generalization_equivarance}.
For example, the well-known wavelet scattering network achieves invariance by stacking equivariant layers followed by a final invariant one in that of scattering networks~\citep{Mallat_Group_scattering}.
However, our proposed method does not require an additional layer as it operates on data manifolds directly.


We used the original GitHub (\texttt{https://github.com/dwromero/wavelet\_networks}) implementation, leaving the dropout and base parameters unchanged.
We searched over the learning rate (e.g., $\{10^{-3}, 10^{-4}, 10^{-5}\}$) using the validation set for each time series task.


\subsubsection{Canonicalization}
We also compared our method with a canonicalization approach which is based on learning mappings to canonical samples.
Specifically, we used the equiadapt library (\texttt{https://github.com/arnab39/equiadapt})~\citep{mondal2023equivariant,canonical} while adding translation equivarant architectures for shift operations.
For translation equivariant canonicalization architecture, we implemented a convolutional neural network with a kernel size of five and three layers.
The network avoids pooling layers to prevent aliasing and instead employs global average pooling at the final stage to aggregate information across the signal length.
The output is reshaped to separate the channels corresponding to each discrete translation, followed by aggregation over the fiber channels to produce translation-equivariant activations.

We used a discrete Group representation with number of translations set to 16.
The canonicalization network and classifier were trained jointly, incorporating a prior regularization loss to guide the learning process.
For optimization, we employed the Adam optimizer with a learning rate of \(1 \times 10^{-3}\) and no weight decay. 
We also performed a grid search to identify the optimal learning rate for the canonicalization network; however, no significant performance improvements were observed across different learning rates.


% Performance of our algorithms was initially assessed with
% the F1 score (Dice, 1945; Sørensen, 1948). But afterwards we
% switched to Cohen’s kappa (Cohen, 1960) because F1 scores are a
% biased measure of classification quality (Powers, 2014), which is a
% problem when comparing recordings with a different prevalence
% of the classes 


% Here, we explain how to calculate each metric for different time-series tasks.
% The MAE, RMSE, and correlation coefficient ($\rho$) are calculated using the following equations:

% \begin{equation}\label{eq:MAE}
% \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |\hat{y}_i - y_i|
% \end{equation}

% \begin{equation}\label{eq:RMSE}
% \text{RMSE} =  \sqrt{\frac{\sum_{n=1}^{n} (\hat{y}_i - y_i)^2}{K}}
% \end{equation}

% \begin{equation}\label{eq:corr}
% \rho = \frac{\sum_{i=1}^n (\hat{y}_i - \mu_{\hat{y}})(y_i - \mu_y)}{\sqrt{\sum_{i=1}^n (\hat{y}_i - \mu_{\hat{y}})^2}\sqrt{\sum_{i=1}^n (y_i - \mu_y)^2}}
% \end{equation}

% \begin{equation}\label{eq:MAPE}
%     \text{MAPE} = \frac{100}{n} \sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right|\%
% \end{equation}

% where n represents the total number of segments. 
% The variables $\hat{y}_i$ and $y_i$ denote the output of the model and ground truth values in beats (respiration)-per-minute or number of steps for the $n^{th}$ segment, respectively. 
% We reported the percentage for the Pearson correlation coefficient to save precision in the tables.


% -------------------------------------------------- %
% -------------------------------------------------- %
% -------------------------------------------------- %
% -------------------------------------------------- %
% -------------------------------------------------- %
\subsection{Implementation Details}
\label{appendix:Implementation_Details}
Here, we have provided the details of the architectures, and hyperparameters.
Primarily, we used the 1D ResNet~\citep{resnet1d} implementation in the supervised settings.
While some alternative deep learning models can perform better in time series such as the combination of convolutional and LSTM layers, similar to previous works in shift consistency~\citep{zhang2019shiftinvar}, we focused on deep learning models which are mainly composed of convolutional layers.  

\subsubsection{Architectures}
Here, we present the details of architectures that are investigated for the performance of shift-invariant techniques.
Some details that are not given in the tables are as follows.
Batch normalization~\citep{batch} is applied after each convolutional block.
ReLU activation is employed following batch normalization, in line with~\citep{ResNet}.
We also applied a Dropout~\citep{Dropout} with 0.5 after each activation and before the convolutions.
Finally, a global average pooling is implemented before the linear layers.


\begin{table}[h]
    \setlength{\tabcolsep}{3pt} % Adjust this value to decrease/increase column spacing
    \centering
    \caption{ResNet architecture}\label{appendix_resnet}
    \renewcommand{\arraystretch}{0.9}
    \begin{tabular}{lllll}
       \toprule
        Repetition & Layer & Kernel Size & Output Size & Stride \\
        \midrule
        \multirow{1}{*}{1} & Input (C,T) & - & (C, T) & - \\
        \multirow{1}{*}{1} & Conv & (5, 1) & (64, T/2) & 1 \\
        \midrule
        \multirow{3}{*}{R} & Residual Block & & & \\
        &  Conv & (5, 1) & (128, T/4) & S \\
        &  Conv & (5, 1) & (128, T/4) & 1 \\
        \midrule
        1 & Linear & - & (n\_classes,) & - \\
        \midrule
        \multicolumn{3}{l}{\# Parameters for \textit{dataset} (C,T, R, S)}  \\
        \multicolumn{2}{l}{\textit{IEEE SPC} (C=1, T=200, R=8, S=2)} & &  $\approx$210k \\
        \multicolumn{2}{l}{\textit{DaLiA} (C=1, T=200, R=8, S=2)} & & $\approx$210k \\
        \multicolumn{2}{l}{\textit{Chapman} (C=4, T=1000, R=8, S=2)} & &  $\approx$197k \\
        \multicolumn{2}{l}{\textit{PhysioNet} (C=1, T=6000, R=8, S=2)} & &  $\approx$197k \\
        \multicolumn{2}{l}{\textit{UCIHAR} (C=9, T=65, R=8, S=2)} & &  $\approx$200k \\
        \multicolumn{2}{l}{\textit{HHAR} (C=6, T=51, R=8, S=2)} & & $\approx$200k \\
        \multicolumn{2}{l}{\textit{Respiratory} (C=12, T=6000, R=8, S=3)} & &  $\approx$200k \\
        \multicolumn{2}{l}{\textit{Sleep} (C=1, T=3000, R=16, S=2)} & &  $\approx$3.2M \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{table}[h]
    \setlength{\tabcolsep}{3pt} % Adjust this value to decrease/increase column spacing
    \caption{The model topologies of the classifier $f_C$ and guidance network $f_G$}
    \centering
    \begin{subtable}[t]{0.48\textwidth}
        \centering
        \caption{FCN architecture}\label{appendix_tab_fcn_v1}
        \begin{tabular}{lll}
            \toprule
            Layer & \multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}Kernel\\Size\end{tabular}} & \multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}Output\\Size\end{tabular}} \\
            & & \\
            \midrule
            Input (C,T) & - & (C, T) \\
            Conv (32 kernels) & (8, 1) & (32, T-4) \\
            Max Pooling & (2,1) & (32, (T-4)/2) \\
            Conv (64 kernels) & (8, 1) & (64, (T-4)/2-4) \\
            Max Pooling & (2,1) & (64, (T-4)/4-2) \\
            Conv (128 kernels) & (8, 1) & (128, (T-4)/4-6) \\
            Max Pooling & (2,1) & (128, (T-4)/8-3) \\
            Linear  & - & (n\_classes,) \\
            \midrule
            \multicolumn{3}{l}{\# Parameters for \textit{dataset} (C,T)}  \\
            \multicolumn{2}{l}{\textit{Clemson} (C=1, T=240)} & $\approx$432k \\
            \bottomrule
        \end{tabular}
    \end{subtable}\hfill
    \begin{subtable}[t]{0.48\textwidth}
        \centering
        \caption{Guidance architecture}\label{appendix_tab_fcn_controller}
        \begin{tabular}{lll}
            \toprule
            Layer & \multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}Kernel\\Size\end{tabular}} & \multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}Output\\Size\end{tabular}}  \\
            & \\
            \midrule
            Input (C,T) & --- & (C, T)  \\
            Conv (4 kernels) & (8, 1) & (4, T-5)  \\
            Max Pooling & (2,1) & (4, (T-5)/2+1) \\
            Conv (16 kernels) & (5, 1) & (16, (T-5)/2-1)  \\
            Max Pooling & (2,1) & (16, (T-5)/4+1)   \\
            Conv (32 kernels) & (3, 1) & (32, (T-5)/4+1)  \\
            Max Pooling & (2,1) & (32, (T-5)/8+1)  \\
            Linear  & - & (n\_classes,) \\
            \midrule
            \multicolumn{3}{l}{\# Parameters for \textit{dataset} (C,T)}  \\
            \multicolumn{2}{l}{\textit{UCIHAR} (C=9, T=65)} &  $\approx$2.5k \\
            \multicolumn{2}{l}{\textit{HHAR} (C=6, T=51)} & $\approx$2k \\
            \multicolumn{2}{l}{\textit{Clemson} (C=1, T=240)} & $\approx$3k \\
            \midrule 
            \multicolumn{2}{l}{\textit{IEEE SPC} (C=1, T=200)} &  $\approx$2.4k \\  
            \multicolumn{2}{l}{\textit{DaLiA} (C=1, T=200)} &  $\approx$2.4k \\ 
            \multicolumn{2}{l}{\textit{Chapman} (C=4, T=500)} &  $\approx$6k \\ 
            \multicolumn{2}{l}{\textit{PhysioNet} (C=1, T=3000)} &  $\approx$13k \\ 
            \multicolumn{2}{l}{\textit{Respiratory} (C=1, T=1500)} &  $\approx$66k \\ 
            \multicolumn{2}{l}{\textit{Sleep} (C=12, T=6000)} &  $\approx$8k \\ 
            \bottomrule
        \end{tabular}
    \end{subtable}
\end{table}

Tables~\ref{appendix_resnet},~\ref{appendix_tab_fcn_v1}, and~\ref{appendix_tab_fcn_controller} give an overall for the architectures with the number of parameters for each dataset.
From these tables, it can be observed that the number of parameters for the guidance network is much less than the main classifier, where the ratio is close to $\approx$2--4\%.


The parameter count of the guidance network could be further reduced by selectively inputting only the important frequencies or the frequency band for each time series task, which can be determined using prior knowledge, rather than the entire spectrum.
Nevertheless, for the sake of consistency, we perform the Fourier transform with the number of harmonics same as the length of time series and provide the entire spectrum to the guidance network as the input.


One advantage of this input modeling is that the ratio between the number of parameters for the guidance network and the main classifier decreases more when the main classifier has more blocks as they are independent.
For example, the guidance network has 1000$\times$ fewer parameters than the main classifier for the sleep stage classification task where we have used 16 ResNet blocks for the classifier model for all techniques.
At the same time, this addition increases the performance metrics up to 3\%.
Furthermore, increasing the number of parameters can decrease the performance of the models as it can cause overfitting of the training data~\citep{cao2022benign, wen2023benign}, which is observed in our case as well (see Table~\ref{tab:appendix_sleep_2} in Appendix~\ref{appendix:Additional_Results} for additional results).







% -------------------------------------------------- %
% -------------------------------------------------- %
% -------------------------------------------------- %
% -------------------------------------------------- %
% -------------------------------------------------- %
% -------------------------------------------------- %
\clearpage
\section{Additonal Results}
\label{appendix:Additional_Results}

\subsection{Sleep stage classification}
\label{appendix:BIDMC}
Here, we present extended results for the sleep stage classification in Table~\ref{tab:appendix_sleep_2}. 
Specifically, we include the F1 score as an additional metric and employ a larger network to observe its impact.
\begin{table*}[h]
\caption{Performance comparison of ours with other methods in \textit{EEG} for sleep stage classification}
\begin{adjustbox}{width=0.8\columnwidth,center}
\label{tab:appendix_sleep_2}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{@{}llllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Sleep-EDF}   \\ 
\cmidrule(r{15pt}){2-6}  
& S-Cons $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & W-F1 $\uparrow$ & $\kappa$ $\uparrow$ \\
\midrule
Baseline & 95.06\small$\pm$0.61 & 75.41\small$\pm$2.01 & 65.40\small$\pm$1.33 & 74.87\small$\pm$1.92 & 67.12\small$\pm$2.96  \\
Baseline (2$\times$) & 91.09\small$\pm$1.26 & 73.88\small$\pm$2.10 & 65.84\small$\pm$3.29 & 74.32\small$\pm$2.86 & 65.14\small$\pm$2.94  \\
Aug. & 99.00\small$\pm$0.17 & 74.89\small$\pm$1.11 & 64.71\small$\pm$1.55 & 74.03\small$\pm$1.46 & 65.89\small$\pm$1.81  \\
LPF & 92.43\small$\pm$1.24 & 73.56\small$\pm$2.93  & \textbf{68.08\small$\pm$1.97} & 76.01\small$\pm$1.98 & 65.68\small$\pm$3.46  \\
APS & --- & --- & --- & --- & ---  \\
\midrule
Ours & \textbf{100\small$\pm$0.00} & \textbf{77.90\small$\pm$1.92} & 67.01\small$\pm$2.65 & \textbf{76.77\small$\pm$2.58} & \textbf{70.01\small$\pm$1.10} \\
Ours\small+\small LPF & 100\small$\pm$0.00 & 73.12\small$\pm$1.89 & 67.42\small$\pm$1.99 & 75.34\small$\pm$1.61 & 64.98\small$\pm$2.27 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}
Although our method ranked second in F1 metric, it is important to highlight that F1 scores are a biased measure of classification quality~\citep{bad_f1_1,Powers2015WhatTF}, which is a problem when comparing recordings with a different prevalence of the classes as in sleep staging~\citep{sleep_kappa_better}.
Therefore, we reported additional metrics for measuring the performance. 
In particular, we included the kappa score, a metric widely used for evaluating algorithms in this task~\citep{sleep_kappa_better, expert_level_sleep}.
Additionally, sleep stage classification and certain other tasks have empty APS values as the authors' original implementation encountered overflow issues with matrix repetition, particularly for longer arrays.


Overall, our proposed method demonstrates a significant performance improvement in three of the metrics, with high kappa and accuracy---both of which are commonly used in the medical domain~\citep{expert_level_sleep} while ranking second in the F1 metric, following the LPF approach.
We also performed the same ablation experiments for investigating the behavior of the loss function on the performance of the model for the sleep stage classification and reported the results in Table~\ref{tab:appendix_sleep_ablation} while excluding the consistency metric as the model that include the proposed transformation are always completely shift-invariant.


\begin{table*}[h]
\caption{Ablation experiments for sleep stage classification}
\begin{adjustbox}{width=0.6\columnwidth,center}
\label{tab:appendix_sleep_ablation}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{Sleep-EDF}   \\ 
\cmidrule(r{15pt}){2-5}  
& Acc $\uparrow$ & F1 $\uparrow$ & W-F1 $\uparrow$ & $\kappa$ $\uparrow$ \\
\midrule
$\mathcal{T}(\mathbf{x}, \phi)$ & 75.54\small$\pm$2.39 & 66.96\small$\pm$1.78 & 75.53\small$\pm$2.29 & 67.08\small$\pm$0.03  \\
$\mathcal{L}^{\prime}_G$ & 77.21\small$\pm$1.51 & 67.67\small$\pm$1.67 & 76.89\small$\pm$1.71 & 69.39\small$\pm$0.02  \\
$\hat{\mathcal{L}}_G$ & 77.75\small$\pm$1.23 & \textbf{68.04}\small$\pm$1.16 & \textbf{77.01}\small$\pm$1.07 & 69.94\small$\pm$0.01  \\
Ours & \textbf{77.80}\small$\pm$1.95 & 67.01\small$\pm$2.65 & 76.77\small$\pm$2.58 & \textbf{70.01}\small$\pm$2.50 \\
\midrule
Change &  \textcolor{Green}{+2.26}  &  \textcolor{Green}{+0.05}  & \textcolor{Green}{+1.24}  & \textcolor{Green}{+2.93}  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

Table~\ref{tab:appendix_sleep_ablation} also supports the previous experiments and claims regarding the advantages of guiding the proposed diffeomorphism with a neural network.
When the models are trained by guiding the mapping function, the performance of the models increases up to 3\% in Kappa ($\kappa$) score.

\subsection{Heart rate prediction}
Here, we conducted additional experiments to evaluate the models' performance using varied amounts of training and testing data.
Initially, we reduced the training data while increasing the testing data by dividing the datasets in half based on subjects to investigate the performance with less training data.
Table~\ref{appendix_tab:performance_ppg1} presents the results, where our proposed method also increases performance by 3–4\% compared to the baseline architecture while reducing the variance between different runs and improving shift consistency by 40–60\%, even in the low data regime.

\begin{table*}[h]
\caption{Performance comparison of ours and other methods in \textit{PPG} datasets for HR estimation}
\begin{adjustbox}{width=1\columnwidth,center}
\label{appendix_tab:performance_ppg1}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{IEEE SPC22} & \multicolumn{4}{l}{DaLiA}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ & S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ \\
\midrule
Baseline & 55.70\small$\pm$2.61 & 25.93\small$\pm$0.07 & 11.55\small$\pm$0.08 & 48.85\small$\pm$0.85 & 31.48\small$\pm$0.41 & 12.51\small$\pm$0.05 & 5.59\small$\pm$0.05 & 85.37\small$\pm$0.20 \\
Aug. & 70.35\small$\pm$0.47 & 26.21\small$\pm$0.48 & 12.06\small$\pm$0.33 & 48.50\small$\pm$1.22 & 53.38\small$\pm$0.29 & 12.31\small$\pm$0.09 & 5.60\small$\pm$0.05 & 85.90\small$\pm$0.13 & \\
LPF & 67.43\small$\pm$0.56 & 25.87\small$\pm$1.02  & 14.03\small$\pm$0.69 & 47.76\small$\pm$2.49 & 39.82\small$\pm$1.33 & 12.65\small$\pm$0.09 & 5.81\small$\pm$0.01 & 84.87\small$\pm$0.23 & \\
APS & 60.43\small$\pm$1.08 & 24.83\small$\pm$1.26 & 11.14\small$\pm$0.83 & 52.10\small$\pm$2.90 & 38.99\small$\pm$0.85 & 12.49\small$\pm$0.11 & 5.61\small$\pm$0.04 & 85.68\small$\pm$0.17 & \\
\midrule
Ours & 100\small$\pm$0.00 & \textbf{24.67\small$\pm$0.06} & \textbf{11.10\small$\pm$0.16} & \textbf{52.26\small$\pm$0.27}& 100\small$\pm$0.00 & \textbf{12.30\small$\pm$0.11} & \textbf{5.57}\small$\pm$0.03 & \textbf{85.95}\small$\pm$0.25 \\
Ours\small+\scriptsize LPF & 100\small$\pm$0.00 & 26.01\small$\pm$0.27 & 14.04\small$\pm$0.24 & 47.20\small$\pm$0.86 & 100\small$\pm$0.00 & 12.78\small$\pm$0.09 & 6.18\small$\pm$0.01  & 84.48\small$\pm$0.26 \\
Ours\small+\scriptsize APS & 100\small$\pm$0.00 & 24.67\small$\pm$0.32 & 11.38\small$\pm$0.13 & 51.64\small$\pm$0.73 & 100\small$\pm$0.00 & 12.40\small$\pm$0.08 & 5.67\small$\pm$0.02 & 85.63\small$\pm$0.30 & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



We also performed the same ablation studies for heart rate prediction task in the low data regime and presented the results in Table~\ref{tab_appen:performance_hr_ablation}.


\begin{table}[h]
\centering
\caption{\label{tab_appen:performance_hr_ablation} Ablation experiments for \textit{HR} task with less training data}
\begin{adjustbox}{width=0.6\linewidth,center}
\begin{tabular}{@{}lllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{IEEE SPC22} & \multicolumn{3}{l}{DaLiA$_{PPG}$} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  
&  MAE $\downarrow$ & RMSE $\downarrow$ & $\rho$ $\uparrow$ & MAE $\downarrow$ & RMSE $\downarrow$ & $\rho$ $\uparrow$ \\
\midrule
$\mathcal{T}(\mathbf{x}, \phi)$  & 12.04 & 25.49  & 50.72 & 6.10 & 12.94 & 85.03 \\
$\mathcal{L}^{\prime}_G$ & 11.29 & 25.10 & 51.10 & 5.63 & 12.80 & 85.10  \\
$\hat{\mathcal{L}}_G$ & 11.32 & 25.08 & 51.18 & 5.60 & 12.71 & 85.12  \\
Ours  &\textbf{11.10} & \textbf{24.67} & \textbf{52.26} & \textbf{5.57} & \textbf{12.30} & \textbf{85.95} \\
\midrule
Change  & \textcolor{Green}{+0.94} & \textcolor{Green}{+0.82} & \textcolor{Green}{+1.54} & \textcolor{Green}{+0.53} & \textcolor{Green}{+0.64} & \textcolor{Green}{+0.92} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
The ablation study results with a smaller training set align with our main findings, where reducing variations uniformly using the same angle negatively impacts performance.
Likewise, expanding the potential solution space with $\hat{\mathcal{L}}_G$ leads to a performance decline compared to our method.


% \subsection{Lung audio classification}
% \label{appendix:lung}
% We reported the ablation study of 
% % The audio was segmented into 8-second windows with a 2-second overlap to capture temporal patterns.
% % We used an 8-block ResNet model with a stride of 3, a learning rate of \(1e{-5}\), and a batch size of 15.
% % Although audio models typically use Mel-frequency spectrograms with different networks, we did not observe a significant performance difference between these models and our initial model.
% % Therefore, we used the original model with temporal data, consistent with our main experiments.
% \begin{table*}[h]
% \caption{Performance comparison of ours with other methods in \textit{Audio} for COPD}
% \begin{adjustbox}{width=0.4\columnwidth,center}
% \begin{tabular}{@{}llllll@{}}
% \toprule
% \multirow{2}{*}{Method} & \multicolumn{3}{l}{Respiratory}   \\ 
% \cmidrule(r{15pt}){2-4}  
% & Acc $\uparrow$ & F1 $\uparrow$ & W-F1 $\uparrow$  \\
% \midrule
% $\mathcal{T}(\mathbf{x}, \phi)$ & 21.28\small$\pm$7.43 & 55.03\small$\pm$2.89 & 18.14\small$\pm$6.39   \\
% $\mathcal{L}^{\prime}_G$ & 27.17\small$\pm$6.71 & 55.58\small$\pm$9.18 & 21.46\small$\pm$4.07  \\
% $\hat{\mathcal{L}}_G$ & 28.57\small$\pm$8.31 & 54.28\small$\pm$6.56 & 23.73\small$\pm$4.65   \\
% Ours & \textbf{33.10\small$\pm$5.12} & \textbf{60.13\small$\pm$4.67} & \textbf{28.33\small$\pm$6.55}  \\
% \midrule
% Change &  \textcolor{Green}{+11.82}  &  \textcolor{Green}{+5.10}  & \textcolor{Green}{+10.19}  \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table*}


\clearpage


% -----------------------------------------------------------------
% -----------------------------------------------------------------
% -----------------------------------------------------------------
\subsection{Diffeomorphisms in Deep Learning}
\label{sec:appendix_diffeo_in_deep_learning}
In this section, we review previous transformation functions and diffeomorphisms used in deep learning models.
Invariant classification of input samples with neural networks has a long-standing history, with the Spatial Transformer Network (STN) being introduced to learn transformation functions for invariant image classification~\citep{Spatial_transformer_nets}.
Similarly, Temporal Transformer Networks (TTN), an adaptation of STNs for time series applications, were introduced to predict the parameter of warp functions and align time series~\citep{Lohit2019TemporalTN, ICML_diffemor}.
Recent methods have focused on optimizing a known family of diffeomorphism, known as diffeomorphic warping functions~\citep{closed_form_diffeomorphic} for time series alignment, through deep learning~\citep{Deep_Diffeomorphic, Diffeomorphic_TAN}.
Thus, a significant distinction between our approach and previous techniques lies in the fact that we introduce a novel tailored diffeomorphism that is capable of mapping samples subjected to shifts to the same point in the high-dimensional data manifold, to ensure shift-invariancy.
% Subsequently, we guide the presented transformation with deep learning models. 


Although previous techniques were designed for different purposes, such as time-warping~\citep{Lohit2019TemporalTN}, we also evaluated and compared the performance and shift consistency of their transformation functions.
Specifically, we implemented sequence temporal transformations (STN) for clinical time series from~\citet{exploit_invariance_bs} and TTN~\citep{Lohit2019TemporalTN} where the transformation and the classifier are trained together to maximize classification performance by minimizing the cross-entropy loss for both methods.
In our implementation of STN, we followed the original design, using a neural network with two convolutional layers and two pooling layers, where pooling is applied between and after the convolutions.
After the pooling operations, two fully connected layers are applied to the resulting feature maps to obtain the transformation parameter.
The overall results in time series tasks are presented in the tables below.


\begin{table*}[h]
\caption{Performance comparison of our method with different transformations in HR estimation}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ppg_other_diffeo}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{IEEE SPC22} & \multicolumn{4}{l}{DaLiA}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ & S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ \\
\midrule
Baseline & 61.99\small$\pm$1.19 & 18.39\small$\pm$2.96 & 10.28\small$\pm$1.41 & 62.64\small$\pm$5.74 & 32.08\small$\pm$0.22 & 9.86\small$\pm$0.23 & 4.40\small$\pm$0.03 & 86.01\small$\pm$0.51 \\
Aug. & 76.48\small$\pm$1.77 & 18.73\small$\pm$1.15 & 10.42\small$\pm$0.40 & 64.06\small$\pm$3.70 & 52.77\small$\pm$0.39 & 9.85\small$\pm$0.21 & 4.47\small$\pm$0.06 & 85.99\small$\pm$0.49 & \\
\midrule
Baseline+STN & 67.13\small$\pm$1.53 & 18.45\small$\pm$2.73 & 10.35\small$\pm$0.73 & 63.49\small$\pm$4.10 & 44.81\small$\pm$0.25 & 9.90\small$\pm$0.17 & 4.43\small$\pm$0.04 & 85.96\small$\pm$0.47 \\
Baseline+TTN & 60.12\small$\pm$1.10 & 20.57\small$\pm$1.23 & 11.55\small$\pm$1.87 & 60.17\small$\pm$3.64 & 39.13\small$\pm$0.30 & 10.23\small$\pm$0.30 & 4.45\small$\pm$0.05 & 84.31\small$\pm$0.67 \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{16.25\small$\pm$0.72} & \textbf{9.45\small$\pm$0.03} & \textbf{70.12\small$\pm$2.10}& \textbf{100\small$\pm$0.00} & \textbf{9.75\small$\pm$0.15} & \textbf{4.39\small$\pm$0.03} & \textbf{86.06\small$\pm$0.19} & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



\begin{table*}[h]
\caption{Performance comparison of our method with different transformations in \textit{ECG} datasets}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:performance_ecg_other_diffeo}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Chapman} & \multicolumn{4}{l}{PhysioNet 2017}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC (\%)$\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC $\uparrow$ \\
\midrule
Baseline & 98.53\small$\pm$0.17 & 91.32\small$\pm$0.23 & 91.22\small$\pm$0.24 & 98.34\small$\pm$0.16 & 98.37\small$\pm$0.15 & \textbf{83.22\small$\pm$0.72} & 73.50\small$\pm$1.99 & 93.21\small$\pm$0.30 \\
Aug. & 99.00\small$\pm$0.16 & 91.96\small$\pm$0.19 & 91.89\small$\pm$0.22 & 98.45\small$\pm$0.18 & 98.96\small$\pm$0.17 & 82.28\small$\pm$1.18 & 72.32\small$\pm$2.20 & 93.20\small$\pm$0.42  \\
\midrule
Baseline+STN & 98.31\small$\pm$0.13 & 91.45\small$\pm$0.20 & 91.33\small$\pm$0.19 & 98.31\small$\pm$0.14 & 98.55\small$\pm$0.14 & 83.12\small$\pm$0.50 & 73.27\small$\pm$1.54 & 93.23\small$\pm$0.28 \\
Baseline+TTN & 97.69\small$\pm$0.15 & 91.27\small$\pm$0.17 & 90.54\small$\pm$0.38 & 98.23\small$\pm$0.21 & 97.12\small$\pm$0.23 & 82.51\small$\pm$0.63 & 71.43\small$\pm$1.43 & 93.07\small$\pm$0.35 \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{92.10\small$\pm$0.25} & \textbf{91.93\small$\pm$0.85} & \textbf{98.47\small$\pm$0.15} & \textbf{100\small$\pm$0.00} & 83.15\small$\pm$0.65 & \textbf{74.12\small$\pm$1.80} & \textbf{93.28\small$\pm$0.31} & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}




\begin{table*}[h]
\centering
\caption{Performance comparison of our method with different transformations in \textit{IMU} datasets}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:performance_imu_other_diffeo}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{UCIHAR} & \multicolumn{3}{l}{HHAR} & \multicolumn{3}{l}{Clemson} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  \cmidrule(r{15pt}){8-10} \\ 
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ 
& S-Cons (\%) $\uparrow$ & MAPE $\downarrow$ & MAE $\downarrow$ \\
\midrule
Baseline & 94.07\small$\pm$1.38 & 85.39\small$\pm$2.30 & 83.20\small$\pm$2.94 & 98.27\small$\pm$0.33 &  91.87\small$\pm$1.36 & 91.16\small$\pm$1.38 & 54.31\small$\pm$4.40 & 4.76\small$\pm$0.11 & 2.74\small$\pm$0.08 \\
Aug. & 96.55\small$\pm$0.80 & 85.42\small$\pm$4.50 & 83.69\small$\pm$6.74 & 98.38\small$\pm$0.28 & \textbf{91.97\small$\pm$0.44} & \textbf{91.31\small$\pm$0.49} & 61.01\small$\pm$4.88 & 4.08\small$\pm$0.14 & 2.29\small$\pm$0.07  \\
\midrule
Baseline+STN & 93.96\small$\pm$1.22 & 83.22\small$\pm$1.23 & 83.57\small$\pm$2.14 & 98.30\small$\pm$0.24 &  88.92\small$\pm$1.10 & 89.10\small$\pm$1.20 & 58.56\small$\pm$4.78 & 4.94\small$\pm$0.13 & 2.53\small$\pm$0.10 \\
Baseline+TTN & 93.32\small$\pm$1.95 & 83.27\small$\pm$1.57 & 82.78\small$\pm$3.12 & 97.10\small$\pm$0.78 &  90.03\small$\pm$1.74 & 90.18\small$\pm$1.10 & 45.89\small$\pm$3.02 & 5.43\small$\pm$0.20 & 2.89\small$\pm$0.18 \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{87.71\small$\pm$1.98} & \textbf{85.67\small$\pm$2.47} & \textbf{100\small$\pm$0.00} & 91.93\small$\pm$1.14 & 91.12\small$\pm$1.03 & \textbf{100\small$\pm$0.00} & \textbf{4.28\small$\pm$0.34} & \textbf{2.43\small$\pm$0.21} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


As shown in the tables, other transformation functions fail to achieve true shift-invariance.
While the STN method shows some improvements on certain datasets, it still lacks full shift-invariance.
This limitation arises because STN relies on a neural network to estimate transformation parameters from time series data.
However, this temporal transformation has no information about the position of the time series.
Thus, when the input is shifted, the output changes.



\clearpage


% -----------------------------------------------------------------
% -----------------------------------------------------------------
% -----------------------------------------------------------------
\subsection{Expanded Comparison}
Here, we compared the performance of techniques when random data augmentation (Aug.) is applied during training. 
In other words, the samples are randomly shifted and fed to the models during training.
During inference, the performance of methods with original samples is evaluated without applying any shifts.
We present the results in the tables below.


\begin{table*}[h]
\caption{Performance comparison of our method and other techniques with data augmentation for HR estimation}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ppg_all_aug}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{IEEE SPC22} & \multicolumn{4}{l}{DaLiA}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ & S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ \\
\midrule
Baseline & 61.99\small$\pm$1.19 & 18.39\small$\pm$2.96 & 10.28\small$\pm$1.41 & 62.64\small$\pm$5.74 & 32.08\small$\pm$0.22 & 9.86\small$\pm$0.23 & 4.40\small$\pm$0.03 & 86.01\small$\pm$0.51 \\
Aug. & 76.48\small$\pm$1.77 & 18.73\small$\pm$1.15 & 10.42\small$\pm$0.40 & 64.06\small$\pm$3.70 & 52.77\small$\pm$0.39 & 9.85\small$\pm$0.21 & 4.47\small$\pm$0.06 & 85.99\small$\pm$0.49 & \\
LPF & 76.88\small$\pm$0.73 & 20.20\small$\pm$1.54  & 13.44\small$\pm$0.82 & 65.40\small$\pm$1.92 & 38.67\small$\pm$0.30 & 10.01\small$\pm$0.30 & 4.67\small$\pm$0.12 & 85.68\small$\pm$0.51 & \\
APS & 73.99\small$\pm$1.06 & 19.42\small$\pm$0.60 & 12.98\small$\pm$0.29 & 65.27\small$\pm$1.32 & 44.33\small$\pm$0.16 & 10.45\small$\pm$0.40 & 5.01\small$\pm$0.17 & 84.69\small$\pm$0.85 & \\
WaveletNet & 51.71\small$\pm$1.95 & 21.56\small$\pm$1.01 & 14.61\small$\pm$0.34 & 60.74\small$\pm$4.37 & 36.71\small$\pm$3.04 & 15.46\small$\pm$0.64 & 7.67\small$\pm$0.23 & 76.13\small$\pm$1.86 & \\

\midrule

LPF + Aug. & 76.17\small$\pm$1.15 & 20.39\small$\pm$1.15  & 13.22\small$\pm$0.36 & 61.72\small$\pm$2.87 & 55.62\small$\pm$0.21 & 12.57\small$\pm$0.19 & 6.02\small$\pm$0.11 & 84.94\small$\pm$0.40 & \\

APS + Aug. & 74.88\small$\pm$0.61 & 18.01\small$\pm$0.15 & 10.57\small$\pm$0.15 & 66.40\small$\pm$2.21 & 53.70\small$\pm$0.08 & 12.84\small$\pm$0.10 & 6.08\small$\pm$0.02 & 85.62\small$\pm$0.65 & \\

WaveletNet + Aug. & 50.14\small$\pm$0.14 & 20.10\small$\pm$1.15 & 13.41\small$\pm$0.57 & 61.90\small$\pm$3.50 & 36.71\small$\pm$3.04 & 15.46\small$\pm$0.64 & 7.67\small$\pm$0.23 & 76.13\small$\pm$1.86 & \\

\midrule

Ours & \textbf{100\small$\pm$0.00} & \textbf{16.25\small$\pm$0.72} & \textbf{9.45\small$\pm$0.03} & \textbf{70.12\small$\pm$2.10}& \textbf{100\small$\pm$0.00} & \textbf{9.75\small$\pm$0.15} & \textbf{4.39\small$\pm$0.03} & \textbf{86.06\small$\pm$0.19} \\
Ours\small+\scriptsize LPF & 100\small$\pm$0.00 & 20.34\small$\pm$1.62 & 13.77\small$\pm$0.84 & 65.60\small$\pm$2.31 & 100\small$\pm$0.00 & 10.72\small$\pm$0.11 & 5.30\small$\pm$0.03  & 84.12\small$\pm$0.23 \\
Ours\small+\scriptsize APS & 100\small$\pm$0.00 & 18.81\small$\pm$1.59 & 12.32\small$\pm$0.84 & 67.01\small$\pm$3.79 & 100\small$\pm$0.00 & 10.47\small$\pm$0.09 & 5.10\small$\pm$0.03 & 84.62\small$\pm$0.31 & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[h]
\caption{Performance comparison of ours and other techniques with data augmentation in \textit{ECG} datasets for CVD classification}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ppg_all_ecg}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Chapman} & \multicolumn{4}{l}{PhysioNet 2017}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC (\%)$\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC $\uparrow$ \\
\midrule
Baseline & 98.53\small$\pm$0.17 & 91.32\small$\pm$0.23 & 91.22\small$\pm$0.24 & 98.34\small$\pm$0.16 & 98.37\small$\pm$0.15 & 83.22\small$\pm$0.72 & 73.50\small$\pm$1.99 & 93.21\small$\pm$0.30 \\
Aug. & 99.00\small$\pm$0.16 & 91.96\small$\pm$0.19 & 91.89\small$\pm$0.22 & 98.45\small$\pm$0.18 & 98.96\small$\pm$0.17 & 82.28\small$\pm$1.18 & 72.32\small$\pm$2.20 & 93.20\small$\pm$0.42  \\
LPF & 98.69\small$\pm$0.14 & 92.01\small$\pm$0.23  & 91.94\small$\pm$0.58 & 98.50\small$\pm$0.24 & 98.94\small$\pm$0.39 & 84.40\small$\pm$0.16 & 75.68\small$\pm$0.76 & 93.80\small$\pm$0.32 & \\
APS & 98.60\small$\pm$0.17 & 90.69\small$\pm$0.89 & 89.44\small$\pm$1.00 & 98.31\small$\pm$0.24 & --- & --- & --- & --- \\

WaveletNet & 91.02\small$\pm$1.14 & 90.87\small$\pm$1.02 & 90.02\small$\pm$1.00 & 97.94\small$\pm$0.21 & 65.03\small$\pm$0.71 & 76.06\small$\pm$0.64 & 63.35\small$\pm$3.40 & 87.02\small$\pm$0.29\\

\midrule
LPF + Aug. & 98.85\small$\pm$0.20 & 92.05\small$\pm$0.13 & 91.94\small$\pm$0.40 & \textbf{98.53\small$\pm$0.20} & 99.00\small$\pm$0.50 & 83.27\small$\pm$0.61 & 74.03\small$\pm$1.56 & 93.20\small$\pm$0.31 & \\
APS + Aug. & 98.60\small$\pm$0.17 & 90.69\small$\pm$0.89 & 89.44\small$\pm$1.00 & 98.31\small$\pm$0.24 & --- & --- & --- & --- \\
WaveletNet + Aug. & 91.02\small$\pm$1.14 & 90.87\small$\pm$1.02 & 90.02\small$\pm$1.00 & 97.94\small$\pm$0.21 & 65.03\small$\pm$0.71 & 78.90\small$\pm$0.57 & 65.88\small$\pm$1.44 & 88.67\small$\pm$0.22/\\
\midrule

Ours & \textbf{100\small$\pm$0.00} & \textbf{92.10\small$\pm$0.25} & 91.93\small$\pm$0.85 & 98.47\small$\pm$0.15 & \textbf{100\small$\pm$0.00} & 83.15\small$\pm$0.65 & 74.12\small$\pm$1.80 & 93.28\small$\pm$0.31 \\
Ours\small+\small LPF & 100\small$\pm$0.00 & 92.05\small$\pm$0.52 & \textbf{91.96\small$\pm$0.54} & 98.51\small$\pm$0.10 & 100\small$\pm$0.00 & \textbf{85.20\small$\pm$0.40} & \textbf{77.50\small$\pm$1.21} & \textbf{94.20\small$\pm$0.19} \\
Ours\small+\small APS & 100\small$\pm$0.00 & 91.61\small$\pm$1.11 & 91.10\small$\pm$0.56 & 98.36\small$\pm$0.20 & --- & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[h]
\centering
\caption{Performance comparison of our method and others with data augmentation in \textit{IMU} datasets for Activity and Step}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:performance_all_imu}
\renewcommand{\arraystretch}{0.7}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{UCIHAR} & \multicolumn{3}{l}{HHAR} & \multicolumn{3}{l}{Clemson} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  \cmidrule(r{15pt}){8-10} \\ 
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ 
& S-Cons (\%) $\uparrow$ & MAPE $\downarrow$ & MAE $\downarrow$ \\
\midrule
Baseline & 94.07\small$\pm$1.38 & 85.39\small$\pm$2.30 & 83.20\small$\pm$2.94 & 98.27\small$\pm$0.33 &  91.87\small$\pm$1.36 & 91.16\small$\pm$1.38 & 54.31\small$\pm$4.40 & 4.76\small$\pm$0.11 & 2.74\small$\pm$0.08 \\
Aug. & 96.55\small$\pm$0.80 & 85.42\small$\pm$4.50 & 83.69\small$\pm$6.74 & 98.38\small$\pm$0.28 & 91.97\small$\pm$0.44 &91.31\small$\pm$0.49& 61.01\small$\pm$4.88 & 4.08\small$\pm$0.14 & 2.29\small$\pm$0.07  \\
LPF & 95.05\small$\pm$0.21 & 83.96\small$\pm$3.44 & 81.08\small$\pm$4.21 & 98.10\small$\pm$0.10 & 92.10\small$\pm$0.80 &91.43\small$\pm$0.94& 59.77\small$\pm$4.40 & 4.16\small$\pm$0.16 & 2.35\small$\pm$0.11  \\
APS & 96.40\small$\pm$0.03 & 81.75\small$\pm$4.11 & 79.01\small$\pm$5.33 & 98.30\small$\pm$0.24 & 91.83\small$\pm$1.35 &91.01\small$\pm$1.47 & 45.50\small$\pm$2.69 & 4.74\small$\pm$0.16 & 2.69\small$\pm$0.07 \\
WaveletNet & 94.56\small$\pm$1.31 & 82.78\small$\pm$4.62 & 80.73\small$\pm$5.59 & 96.76\small$\pm$0.15 & 90.72\small$\pm$0.38 & 90.71\small$\pm$0.39 & 59.14\small$\pm$3.10 & 5.20\small$\pm$0.66 & 2.95\small$\pm$0.41 \\

\midrule
LPF + Aug. & 97.65\small$\pm$1.30 & 84.67\small$\pm$3.45 & 83.32\small$\pm$3.50 & 98.65\small$\pm$0.12 & 92.45\small$\pm$0.78 & 91.70\small$\pm$0.62 & 59.77\small$\pm$4.40 & 3.81\small$\pm$0.13 & 2.23\small$\pm$0.10  \\
APS + Aug. & 96.40\small$\pm$0.03 & 78.40\small$\pm$3.75 & 75.43\small$\pm$4.33 & 98.87\small$\pm$0.34 & 92.40\small$\pm$0.40 & 91.49\small$\pm$0.73 & 45.50\small$\pm$2.69 & 3.94\small$\pm$0.10 & 2.50\small$\pm$0.07 \\
WaveletNet + Aug. & 94.56\small$\pm$1.31 & 82.78\small$\pm$4.62 & 80.73\small$\pm$5.59 & 96.76\small$\pm$0.15 & 90.72\small$\pm$0.38 & 91.71\small$\pm$0.39 & 60.23\small$\pm$2.54 & 5.18\small$\pm$1.02 & 3.02\small$\pm$0.48 \\

\midrule

Ours & \textbf{100\small$\pm$0.00} & \textbf{87.71\small$\pm$1.98} & \textbf{85.67\small$\pm$2.47} & \textbf{100\small$\pm$0.00} & 91.93\small$\pm$1.14 & 91.12\small$\pm$1.03 & \textbf{100\small$\pm$0.00} & 4.28\small$\pm$0.34 & 2.43\small$\pm$0.21 \\
Ours\small+\small LPF & 100\small$\pm$0.00 & 84.78\small$\pm$2.46 & 82.58\small$\pm$2.62 & 100\small$\pm$0.00 & \textbf{92.51\small$\pm$0.55} &\textbf{91.80\small$\pm$0.62} & 100\small$\pm$0.00 & \textbf{3.75\small$\pm$0.33} & \textbf{2.12\small$\pm$0.18}  \\
Ours\small+\small APS & 100\small$\pm$0.00 & 82.96\small$\pm$1.79 & 81.10\small$\pm$1.73 & 100\small$\pm$0.00 & 91.38\small$\pm$0.32 & 90.64\small$\pm$0.32 & 100\small$\pm$0.00 & 3.87\small$\pm$0.19 & 2.19\small$\pm$0.11  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


From the results, we can see that applying shift data augmentation during training did not consistently improve performance, and in some cases, it led to a decrease.
For instance, in the HR prediction task, training the low-pass filtering method with randomly shifted samples resulted in lower performance.
We believe that random shifts may reduce the inter-class separation between samples, causing them to overlap in the feature space~\citep{chaos}.
As a result, even though the number of training samples increases with augmentation, this reduced separation can lead to a decline in model performance.



\clearpage

\subsection{Performance in Different Hyperparameters}
In this section, we vary the training batch size to evaluate the performance of the proposed method.
Given that the guidance network includes an additional loss function to reduce angle variance within a batch, we examined its performance under different batch sizes. 
The results are presented in Tables~\ref{tab:appendix_performance_ppg_all_batch}, \ref{tab:appendix_performance_ppg_all_ecg_batch}, and \ref{tab:performance_all_imu_batch}.


\begin{table*}[h]
\caption{Performance comparison of our method and other techniques with a different batch size for HR estimation}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ppg_all_batch}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{IEEE SPC22} & \multicolumn{4}{l}{DaLiA}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ & S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ \\
\midrule
Baseline & 64.85\small$\pm$1.87 & 19.28\small$\pm$0.41 & 11.51\small$\pm$0.21 & 65.15\small$\pm$1.43 & 36.34\small$\pm$0.39 & 12.47\small$\pm$0.17 & 5.53\small$\pm$0.06 & 85.65\small$\pm$0.20 \\
Aug. & 78.90\small$\pm$1.12 & 19.51\small$\pm$0.93 & 11.82\small$\pm$0.49 & 63.62\small$\pm$1.55 & 52.77\small$\pm$0.39 & \textbf{12.36}\small$\pm$0.09 & 5.61\small$\pm$0.06 & 85.84\small$\pm$0.16 & \\
LPF & 72.16\small$\pm$1.13 & 20.66\small$\pm$1.26 & 13.86\small$\pm$1.06 & 65.22\small$\pm$3.06 & 42.08\small$\pm$0.18 & 12.82\small$\pm$0.26 & 5.88\small$\pm$0.07 & 84.50\small$\pm$0.47 & \\
APS & 71.25\small$\pm$1.19 & 19.79\small$\pm$0.92 & 12.59\small$\pm$0.68 & 67.05\small$\pm$0.85 & 42.01\small$\pm$0.26 & 12.37\small$\pm$0.18 & \textbf{5.50}\small$\pm$0.04 & \textbf{85.75}\small$\pm$0.39 & \\

\midrule

Ours & \textbf{100\small$\pm$0.00} & \textbf{18.29\small$\pm$0.71} & \textbf{11.30\small$\pm$0.57} & \textbf{69.10\small$\pm$1.20} & \textbf{100\small$\pm$0.00} & 12.49\small$\pm$0.16 & 5.55\small$\pm$0.07 & 85.60\small$\pm$0.23 \\
Ours\small+\scriptsize LPF & 100\small$\pm$0.00 & 20.49\small$\pm$0.77 & 14.18\small$\pm$0.42 & 67.28\small$\pm$3.20 & 100\small$\pm$0.00 & 13.17\small$\pm$0.23 & 6.43\small$\pm$0.11  & 83.63\small$\pm$0.57 \\
Ours\small+\scriptsize APS & 100\small$\pm$0.00 & 20.03\small$\pm$0.95 & 13.23\small$\pm$0.69 & 66.40\small$\pm$4.03 & 100\small$\pm$0.00 & 12.59\small$\pm$0.17 & 5.76\small$\pm$0.05 & 85.22\small$\pm$0.27 & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[h]
\caption{Performance comparison of ours and other techniques with a different batch size in \textit{ECG} datasets for CVD classification}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ppg_all_ecg_batch}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Chapman} & \multicolumn{4}{l}{PhysioNet 2017}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC (\%)$\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC $\uparrow$ \\
\midrule
Baseline & 99.00\small$\pm$0.13 & 92.56\small$\pm$0.25 & \textbf{91.58}\small$\pm$0.26 & 98.59\small$\pm$0.12 & 98.41\small$\pm$0.26 & 82.71\small$\pm$1.55 & 73.16\small$\pm$3.39 & 93.00\small$\pm$0.30  \\
Aug. & 99.08\small$\pm$0.16 & 92.46\small$\pm$0.18 & 91.45\small$\pm$0.16 & 98.57\small$\pm$0.20 & 98.73\small$\pm$0.07 & 82.03\small$\pm$1.60 & 72.61\small$\pm$4.10 & 92.63\small$\pm$0.51  \\
LPF & 98.88\small$\pm$0.17 & 92.32\small$\pm$0.15  & 91.31\small$\pm$0.17 & 98.58\small$\pm$0.14 & 98.92\small$\pm$0.40 & 83.07\small$\pm$1.30 & 73.98\small$\pm$3.85 & 93.60\small$\pm$0.70 & \\
\midrule
Ours & \textbf{100\small$\pm$0.00} & \textbf{92.58\small$\pm$0.26} & 91.50\small$\pm$0.30 & \textbf{98.59}\small$\pm$0.10 & \textbf{100\small$\pm$0.00} & \textbf{83.14}\small$\pm$0.82 & \textbf{74.40}\small$\pm$2.28 & \textbf{93.60}\small$\pm$0.20 \\
Ours\small+\small LPF & 100\small$\pm$0.00 & 92.28\small$\pm$0.25 & 91.34\small$\pm$0.27 & 98.54\small$\pm$0.20 & 100\small$\pm$0.00 & 83.01\small$\pm$1.01 & 72.94\small$\pm$2.74 & 93.05\small$\pm$0.50 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[h]
\centering
\caption{Performance comparison of our method and others with a different batch size in \textit{IMU} datasets for Activity and Step}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:performance_all_imu_batch}
\renewcommand{\arraystretch}{0.7}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{UCIHAR} & \multicolumn{3}{l}{HHAR} & \multicolumn{3}{l}{Clemson} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  \cmidrule(r{15pt}){8-10} \\ 
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ 
& S-Cons (\%) $\uparrow$ & MAPE $\downarrow$ & MAE $\downarrow$ \\
\midrule
Baseline & 95.13\small$\pm$1.21 & 89.89\small$\pm$1.76 & 89.37\small$\pm$1.82 & 98.25\small$\pm$0.14 & \textbf{91.99}\small$\pm$0.86 & \textbf{91.20}\small$\pm$0.91 & 43.61\small$\pm$1.53 & 4.27\small$\pm$0.26 & 2.41\small$\pm$0.14 \\
Aug. & 96.55\small$\pm$0.80 & 91.46\small$\pm$1.10 & 90.69\small$\pm$1.58 & 98.13\small$\pm$0.27 & 91.70\small$\pm$1.55 & 90.94\small$\pm$1.61 & 63.74\small$\pm$4.83 & \textbf{3.81}\small$\pm$0.21 & \textbf{2.13}\small$\pm$0.11  \\
LPF & 96.88\small$\pm$0.76 & 92.13\small$\pm$0.56 & 92.23\small$\pm$0.76 & 98.14\small$\pm$0.09 & 90.29\small$\pm$0.86 & 89.45\small$\pm$0.94 & 52.60\small$\pm$3.71 & 4.24\small$\pm$0.24 & 2.38\small$\pm$0.15  \\
APS & 97.12\small$\pm$0.75 & 92.43\small$\pm$1.40 & 92.07\small$\pm$1.21 & 98.30\small$\pm$0.24 & 91.83\small$\pm$1.35 &91.01\small$\pm$1.47 & 42.16\small$\pm$0.98 & 5.00\small$\pm$0.03 & 2.84\small$\pm$0.02 \\

\midrule

Ours & \textbf{100\small$\pm$0.00} & \textbf{92.78\small$\pm$0.51} & \textbf{92.94\small$\pm$0.33} & \textbf{100\small$\pm$0.00} & 91.77\small$\pm$0.56 & 91.10\small$\pm$0.60 & \textbf{100\small$\pm$0.00} & 4.06\small$\pm$0.16 & 2.24\small$\pm$0.10 \\
Ours\small+\small LPF & 100\small$\pm$0.00 & 89.78\small$\pm$1.25 & 90.04\small$\pm$1.11 & 100\small$\pm$0.00 & 91.40\small$\pm$1.20 & 90.60\small$\pm$1.32 & 100\small$\pm$0.00 & 4.14\small$\pm$0.09 & 2.32\small$\pm$0.06  \\
Ours\small+\small APS & 100\small$\pm$0.00 & 90.64\small$\pm$1.46 & 90.40\small$\pm$1.62 & 100\small$\pm$0.00 & 91.19\small$\pm$0.60 & 90.36\small$\pm$0.55 & 100\small$\pm$0.00 & 4.41\small$\pm$0.16 & 2.48\small$\pm$0.10  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}
When we doubled the batch size during training, we observed a performance decrease in some datasets for our method, such as Clemson and DaLiA.
We believe this could be due to the need to adjust the guidance network’s learning rate when changing the batch size.
Additionally, since the introduced loss function aims to reduce overall angle variance for a batch of samples, larger batches may pose optimization challenges for the guidance network.
It is important to note that despite performance decreases in some cases, our method consistently outperformed baseline models with the original batch size used in the main experiments.


\clearpage
% -----------------------------------------------------------------
% -----------------------------------------------------------------
% -----------------------------------------------------------------
\section{Improvements in Performance Across Different Networks}
\label{appendix:other_networks}
In this section, we conduct experiments to observe the performance of our proposed method when it is integrated into different network architectures.
First, we employed the same 1D ResNet architecture for IMU related tasks as we used the fully convolutional network without residual connections in the main results because of better performance.
Second, we applied a transformer with positional encoding, which is designed for time series tasks~\citep{KDD_paper}, to all tasks.
Specifically, we used linear layers with a stack of four identical blocks.
The linear layer converts the input data to embedding vectors of 128. 
Each block is made up of a multi-head self-attention layer and a fully connected feed-forward layer. 
We use residual connections around each layer. 


We have not included blurring (LPF) and adaptive sampling in the transformer network analysis, as these methods are primarily tailored for convolutional architectures.
Additionally, due to lack of convergence in the heart rate prediction task, we have omitted reporting results from the transformers.
We reported the results in the tables below.


\begin{table*}[h]
\centering
\caption{Performance comparison of our method with others in \textit{IMU} with ResNet}
\begin{adjustbox}{width=1\columnwidth,center}
\label{appendix_tab:performance_imu}
\renewcommand{\arraystretch}{0.7}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{UCIHAR} & \multicolumn{3}{l}{HHAR} & \multicolumn{3}{l}{Clemson} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  \cmidrule(r{15pt}){8-10} \\ 
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ 
& S-Cons (\%) $\uparrow$ & MAPE $\downarrow$ & MAE $\downarrow$ \\
\midrule
Baseline & 97.40\small$\pm$0.86 & 85.02\small$\pm$3.92 & 83.35\small$\pm$3.90 & 99.29\small$\pm$0.02 &  91.78\small$\pm$1.07 & 91.70\small$\pm$1.08 & 84.80\small$\pm$4.15 & 6.83\small$\pm$1.60 & 3.97\small$\pm$0.96 \\
Aug. & 98.40\small$\pm$0.37 & 86.66\small$\pm$1.26 & 85.12\small$\pm$1.53 & 99.31\small$\pm$0.04 & 92.82\small$\pm$0.35 &\textbf{92.84}\small$\pm$0.35& 95.88\small$\pm$1.10 & 6.62\small$\pm$1.10 & 3.83\small$\pm$0.66  \\
LPF & 97.91\small$\pm$0.60 & 84.03\small$\pm$2.67 & 82.49\small$\pm$2.93 & 93.01\small$\pm$1.92 & 91.33\small$\pm$1.43 &91.38\small$\pm$1.40 & 94.59\small$\pm$0.71 & 4.46\small$\pm$0.04 & 2.50\small$\pm$0.26  \\
APS & 98.02\small$\pm$0.46 & 81.98\small$\pm$3.36 & 79.23\small$\pm$4.20 & 93.01\small$\pm$1.92 & 92.13\small$\pm$0.22 &92.14\small$\pm$0.22 & 93.01\small$\pm$1.92 & 6.61\small$\pm$1.44 & 3.84\small$\pm$0.84 \\
\midrule
Ours & \textbf{100\small$\pm$0.00} & \textbf{87.12\small$\pm$2.21} & \textbf{85.21\small$\pm$3.10} & \textbf{100\small$\pm$0.00} & 91.90\small$\pm$0.10 & 92.02\small$\pm$0.07 & \textbf{100\small$\pm$0.00} & 6.55\small$\pm$0.75 & 3.93\small$\pm$0.61 \\
Ours\small+\small LPF & 100\small$\pm$0.00 & 83.05\small$\pm$3.86 & 80.14\small$\pm$3.62 & 100\small$\pm$0.00 & \textbf{92.45\small$\pm$0.45} & 92.50\small$\pm$0.44 & 100\small$\pm$0.00 & \textbf{4.45\small$\pm$0.22} & \textbf{2.45\small$\pm$0.13}  \\
Ours\small+\small APS & 100\small$\pm$0.00 & 84.33\small$\pm$2.93 & 83.01\small$\pm$3.13 & 100\small$\pm$0.00 & 92.25\small$\pm$0.17 & 92.30\small$\pm$0.16 & 100\small$\pm$0.00 & 6.07\small$\pm$0.47 & 3.50\small$\pm$0.28  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[h]
\centering
\caption{Performance comparison of our method with others in \textit{IMU} with Transformer}
\begin{adjustbox}{width=1\columnwidth,center}
\label{appendix_tab:performance_imu_transformer}
\renewcommand{\arraystretch}{0.7}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{UCIHAR} & \multicolumn{3}{l}{HHAR} & \multicolumn{3}{l}{Clemson} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  \cmidrule(r{15pt}){8-10} \\ 
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ 
& S-Cons (\%) $\uparrow$ & MAPE $\downarrow$ & MAE $\downarrow$ \\
\midrule
Baseline & 90.44\small$\pm$0.48 & 69.15\small$\pm$4.79 & 65.64\small$\pm$4.59 & 96.98\small$\pm$0.20 &  91.10\small$\pm$1.83 & 91.04\small$\pm$1.92 & 93.87\small$\pm$2.12 & 6.55\small$\pm$0.83 & 3.77\small$\pm$0.48 \\
Aug. & 93.68\small$\pm$0.64 & 73.23\small$\pm$2.75 & 69.79\small$\pm$3.63 & 98.35\small$\pm$0.06 & 89.21\small$\pm$0.07 &89.16\small$\pm$0.11 & 95.46\small$\pm$2.65 & 6.54\small$\pm$0.34 & \textbf{3.76}\small$\pm$0.17  \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{74.02\small$\pm$3.01} & \textbf{70.42\small$\pm$3.47} & \textbf{100\small$\pm$0.00} & \textbf{91.55\small$\pm$1.20} & \textbf{91.19\small$\pm$1.19} & \textbf{100\small$\pm$0.00} & \textbf{6.50\small$\pm$0.55} & 3.77\small$\pm$0.31 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[h]
\caption{Performance comparison of ours and others in \textit{ECG} with Transformer}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ecg_transformer}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Chapman} & \multicolumn{4}{l}{PhysioNet 2017}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC (\%)$\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC $\uparrow$ \\
\midrule
Baseline & 98.53\small$\pm$0.17 & 91.32\small$\pm$0.23 & 91.22\small$\pm$0.24 & 98.34\small$\pm$0.16 & 98.37\small$\pm$0.15 & 83.22\small$\pm$0.72 & 73.50\small$\pm$1.99 & 93.21\small$\pm$0.30 \\
Aug. & 99.00\small$\pm$0.16 & 91.96\small$\pm$0.19 & 91.89\small$\pm$0.22 & \textbf{98.45\small$\pm$0.18} & 98.96\small$\pm$0.17 & 82.28\small$\pm$1.18 & 72.32\small$\pm$2.20 & 93.20\small$\pm$0.42  \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{92.10\small$\pm$0.25} & \textbf{91.93\small$\pm$0.85} & 98.40\small$\pm$0.15 & \textbf{100\small$\pm$0.00} & \textbf{83.35\small$\pm$0.65} & \textbf{74.12\small$\pm$1.80} & \textbf{93.28\small$\pm$0.31} & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\clearpage

We also integrated our proposed transformation into some recent neural networks and investigated the performance.
Mainly, we employed ModernTCN~\citep{modernTCN} and T-WaveNet~\citep{twavenet} architectures.
When we implemented ModernTCN, we follow the original implementation from
We set the patch size and stride to 5 and 2, respectively, while keeping the backbone and dropout rate the same as in the original implementation. 
The stem, downsampling, and FFN ratios were set to 1. 


\begin{table*}[h]
\caption{Performance comparison of our method for HR estimation using ModernTCN}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ppg_modernTCN}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{IEEE SPC22} & \multicolumn{4}{l}{DaLiA}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ & S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ \\
\midrule
Baseline & 38.62\small$\pm$0.23 & 34.67\small$\pm$0.45 & 29.26\small$\pm$0.41 & 3.45\small$\pm$0.54 & 10.24\small$\pm$0.46 & 20.52\small$\pm$0.46 & 11.90\small$\pm$0.24 & 60.16\small$\pm$1.27 \\
Aug. & 50.15\small$\pm$0.07 & 34.36\small$\pm$2.14 & 28.29\small$\pm$3.04 & 01.95\small$\pm$2.23 & 39.10\small$\pm$1.27 & 15.36\small$\pm$1.16 & 7.25\small$\pm$0.64 & 79.97\small$\pm$2.53 & \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{33.45\small$\pm$1.07} & \textbf{29.33\small$\pm$1.09} & \textbf{08.96\small$\pm$3.73} & \textbf{100\small$\pm$0.00} & \textbf{15.20\small$\pm$1.22} & \textbf{7.13\small$\pm$0.10} & \textbf{80.05\small$\pm$1.08} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



\begin{table*}[h]
\caption{Performance comparison of our method for ECG datasets using ModernTCN}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ecg_modernTCN}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}llllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Chapman} & \multicolumn{4}{l}{PhysioNet 2017}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC (\%)$\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC $\uparrow$ \\
\midrule
Baseline & 98.53\small$\pm$0.17 & 55.48\small$\pm$1.34 & 46.07\small$\pm$1.03 & 73.35\small$\pm$0.34 & 87.77\small$\pm$1.20 & 51.84\small$\pm$4.80 & 22.41\small$\pm$2.30 & 60.34\small$\pm$1.64 \\
Aug. & 95.51\small$\pm$3.38 & 81.93\small$\pm$3.60 & 79.15\small$\pm$4.63 & 94.88\small$\pm$1.11 & 98.96\small$\pm$0.17 & 60.13\small$\pm$2.57 & 28.86\small$\pm$5.23 & 70.57\small$\pm$6.10  \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{83.80\small$\pm$2.06} & \textbf{80.73\small$\pm$2.70} & \textbf{95.41\small$\pm$0.33} & \textbf{100\small$\pm$0.00} & \textbf{60.58\small$\pm$1.50} & \textbf{30.73\small$\pm$1.08} & \textbf{72.58\small$\pm$3.76} & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



\begin{table*}[h!]
\centering
\caption{Performance comparison of our method for IMU datasets using ModernTCN}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_imu_modernTCN}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{UCIHAR} & \multicolumn{3}{l}{HHAR} & \multicolumn{3}{l}{Clemson} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  \cmidrule(r{15pt}){8-10} \\ 
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ 
& S-Cons (\%) $\uparrow$ & MAPE $\downarrow$ & MAE $\downarrow$ \\
\midrule
Baseline & 95.89\small$\pm$2.10 & 86.68\small$\pm$1.53 & 85.45\small$\pm$2.35 & 97.23\small$\pm$0.18 &  92.21\small$\pm$0.76 & 92.09\small$\pm$1.10 & 73.96\small$\pm$2.18 & 4.03\small$\pm$0.11 & 2.32\small$\pm$0.09 \\
Aug. & 96.55\small$\pm$0.80 & 85.42\small$\pm$4.50 & 83.69\small$\pm$6.74 & 98.38\small$\pm$0.28 & 91.97\small$\pm$0.44 &91.31\small$\pm$0.49& 61.01\small$\pm$4.88 & 4.08\small$\pm$0.14 & 2.29\small$\pm$0.07  \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{88.73\small$\pm$1.47} & \textbf{87.19\small$\pm$1.94} & \textbf{100\small$\pm$0.00} & \textbf{94.12\small$\pm$0.87} & \textbf{93.43\small$\pm$1.10} & \textbf{100\small$\pm$0.00} & \textbf{3.88\small$\pm$0.26} & \textbf{2.15\small$\pm$0.16} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[h!]
\centering
\caption{Performance comparison of our method using ModernTCN for sleep stage classification}
\begin{adjustbox}{width=0.7\columnwidth,center}
\label{tab:appendix_sleep_2_modernTCN}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{@{}llllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Sleep-EDF}   \\ 
\cmidrule(r{15pt}){2-6}  
& S-Cons $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & W-F1 $\uparrow$ & $\kappa$ $\uparrow$ \\
\midrule
Baseline & 71.84\small$\pm$1.78 & 69.50\small$\pm$1.81 & 62.84\small$\pm$1.39 & 71.21\small$\pm$1.66 & 60.39\small$\pm$2.20  \\
Aug. & 95.47\small$\pm$5.75 & 72.96\small$\pm$4.72 & 64.50\small$\pm$4.14 & 73.61\small$\pm$5.12 & 64.90\small$\pm$6.13  \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{73.36\small$\pm$3.10} & \textbf{65.37\small$\pm$2.88} & \textbf{74.10\small$\pm$1.97} & \textbf{65.42\small$\pm$3.51} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


From these results, we can see that ModernTCN architecture performs relatively poor compared to 1D ResNet architecture in most tasks.
However, for the IMU related tasks, ModernTCN outperforms other architectures.
Similarly, when we integrate our method into the ModernTCN, the performance increases by 5--10\% while decreasing the variation between runs.


\clearpage


We also performed experiments with T-WaveNet, a tree-structured wavelet deep neural network.
The model decomposes input signals into multiple subbands and builds a tree structure with data-driven wavelet transforms the bases of which are learned using invertible neural networks. 
We use the original implementation from \texttt{https://openreview.net/forum?id=U4uFaLyg7PV}.
Following the original implementation, the wavelet functions are learned together with the neural network instead of using stationary wavelet transforms like Haar.



\begin{table*}[h]
\caption{Performance comparison of our method for HR estimation using T-WaveNet}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ppg_twavenet}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{IEEE SPC22} & \multicolumn{4}{l}{DaLiA}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ & S-Cons (\%) $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & $\rho$ (\%) $\uparrow$ \\
\midrule
Baseline & 49.73\small$\pm$1.61 & 21.78\small$\pm$1.87 & 15.77\small$\pm$1.59 & 60.30\small$\pm$4.16 & 39.69\small$\pm$0.17 & 13.38\small$\pm$0.19 & 6.15\small$\pm$0.08 & 83.10\small$\pm$0.47 \\
Aug. & 71.11\small$\pm$1.24 & 19.29\small$\pm$2.31 & 12.16\small$\pm$1.51 & 66.50\small$\pm$6.30 & 53.95\small$\pm$0.28 & 12.82\small$\pm$0.30 & 5.89\small$\pm$0.11 & 83.63\small$\pm$0.76 & \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{19.03\small$\pm$2.41} & \textbf{12.10\small$\pm$2.10} & \textbf{67.76\small$\pm$6.55}& \textbf{100\small$\pm$0.00} & \textbf{12.65\small$\pm$0.25} & \textbf{5.59}\small$\pm$0.10 & \textbf{84.05\small$\pm$0.57} \\
% Ours\small+\scriptsize LPF & 100\small$\pm$0.00 & 20.34\small$\pm$1.62 & 13.77\small$\pm$0.84 & 65.60\small$\pm$2.31 & 100\small$\pm$0.00 & 10.72\small$\pm$0.11 & 5.30\small$\pm$0.03  & 84.12\small$\pm$0.23 \\
% Ours\small+\scriptsize APS & 100\small$\pm$0.00 & 18.81\small$\pm$1.59 & 12.32\small$\pm$0.84 & 67.01\small$\pm$3.79 & 100\small$\pm$0.00 & 10.47\small$\pm$0.09 & 5.10\small$\pm$0.03 & 84.62\small$\pm$0.31 & \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\begin{table*}[h]
\caption{Performance comparison of our method for ECG datasets using T-WaveNet}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_ecg_twavenet}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Chapman} & \multicolumn{4}{l}{PhysioNet 2017}  \\ 
\cmidrule(r{15pt}){2-5}  \cmidrule(r{15pt}){6-10}  
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC (\%)$\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & AUC $\uparrow$ \\
\midrule
Baseline & 97.23\small$\pm$0.19 & 93.17\small$\pm$0.80 & 92.40\small$\pm$0.78 & 98.87\small$\pm$0.18 & 94.72\small$\pm$1.91 & 78.94\small$\pm$1.60 & 71.43\small$\pm$2.24 & 92.44\small$\pm$1.03 \\
Aug. & 98.19\small$\pm$0.68 & \textbf{93.63\small$\pm$0.36} & 92.89\small$\pm$0.32 & 98.96\small$\pm$0.10 & 95.43\small$\pm$0.89 & 79.77\small$\pm$0.99 & 70.72\small$\pm$1.42 & 92.33\small$\pm$0.63 & \\
Ours & \textbf{100\small$\pm$0.00} & 93.45\small$\pm$0.40 & \textbf{92.92\small$\pm$0.50} & \textbf{99.00\small$\pm$0.15} & \textbf{100\small$\pm$0.00} & \textbf{79.89\small$\pm$1.10} & \textbf{71.61\small$\pm$2.10} & \textbf{92.50\small$\pm$0.89} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[h]
\centering
\caption{Performance comparison of our method for IMU datasets using T-WaveNet}
\begin{adjustbox}{width=1\columnwidth,center}
\label{tab:appendix_performance_imu_twavenet}
\renewcommand{\arraystretch}{0.7}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{l}{UCIHAR} & \multicolumn{3}{l}{HHAR} & \multicolumn{3}{l}{Clemson} \\ 
\cmidrule(r{15pt}){2-4}  \cmidrule(r{15pt}){5-7}  \cmidrule(r{15pt}){8-10} \\ 
& S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & S-Cons (\%) $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ 
& S-Cons (\%) $\uparrow$ & MAPE $\downarrow$ & MAE $\downarrow$ \\
\midrule
Baseline & 97.63\small$\pm$1.45 & 72.77\small$\pm$2.36 & 70.48\small$\pm$4.16 & 98.37\small$\pm$0.96 & 92.37\small$\pm$0.89 & 91.53\small$\pm$1.03 & 89.50\small$\pm$0.50 & 6.69\small$\pm$0.54 & 3.90\small$\pm$0.31 \\
Aug. & 98.30\small$\pm$2.43 & 72.82\small$\pm$3.34 & 68.78\small$\pm$3.53 & 98.68\small$\pm$0.65 & 92.88\small$\pm$1.15 & \textbf{92.05\small$\pm$1.28} & 89.29\small$\pm$0.72 & 6.62\small$\pm$0.59 & 3.83\small$\pm$0.31  \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{74.04\small$\pm$2.10} & \textbf{71.04\small$\pm$3.25} & \textbf{100\small$\pm$0.00} & \textbf{92.95\small$\pm$1.14} & 91.60\small$\pm$0.93 & \textbf{100\small$\pm$0.00} & \textbf{6.03\small$\pm$0.50} & \textbf{3.54\small$\pm$0.35} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[h!]
\centering
\caption{Performance comparison of our method using T-WaveNet for sleep stage classification}
\begin{adjustbox}{width=0.7\columnwidth,center}
\label{tab:appendix_sleep_2_T_WaveNet}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{@{}llllll@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{l}{Sleep-EDF}   \\ 
\cmidrule(r{15pt}){2-6}  
& S-Cons $\uparrow$ & Acc $\uparrow$ & F1 $\uparrow$ & W-F1 $\uparrow$ & $\kappa$ $\uparrow$ \\
\midrule
Baseline & 71.84\small$\pm$1.78 & 69.50\small$\pm$1.81 & 62.84\small$\pm$1.39 & 71.21\small$\pm$1.66 & 60.39\small$\pm$2.20  \\
Aug. & 95.47\small$\pm$5.75 & 72.96\small$\pm$4.72 & 64.10\small$\pm$1.23 & 73.61\small$\pm$5.12 & 64.90\small$\pm$6.13  \\
Ours & \textbf{100\small$\pm$0.00} & \textbf{73.36\small$\pm$5.10} & \textbf{65.90\small$\pm$1.07} & \textbf{74.10\small$\pm$3.97} & \textbf{65.42\small$\pm$3.51} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}


As shown in tables, the proposed transformation also increases the performance of the different neural networks.
One important result from the comparison of these tables is that there is a correlation between the model's ability to remain invariant to shifts and its performance, up to a certain threshold where the model performs adequately. 
However, beyond that point, as the model's performance declines, the consistency in shift increases, resulting in the model consistently outputting the wrong class.
For instance, in the case of step counting, the transformer architecture performs quite worse and fails to distinguish between samples. 
As a result, the consistency of the transformer is higher compared to ResNet and fully convolutional networks while the performance is lower.
We believe that investigating the invariance of different neural architectures alongside their performance on time series tasks can shed light on model networks and the fundamental reasons behind abrupt output changes with small changes in the input signal, i.e., $\approx$ 10--15\,ms shift.


\clearpage

\section{Visual Examples for the Guidance Network}
\label{appendix:visual_examples}
In this section, we provide some visual examples to show how the proposed transformation function works. 
First, we show the t-SNE~\citep{t_sne} representations of the embeddings obtained from a trained model with and without applying our transformation in Figure~\ref{fig:t_SNE}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/t_sne_of_mapping_3.pdf}
    \caption{\textbf{(a)} Comparison of pairwise Euclidian distances of randomly shifted embeddings with and without applying our method.
    \textbf{(b)} t-SNE visualizations of embeddings without our method show some shifted samples clustering with opposite class embeddings.  
    \textbf{(c)} With our transformation, all shifted variants of the same signal cluster correctly within their true class label. }
    \label{fig:t_SNE}
\end{figure}

For visualization, we selected 50 different ECG (healthy) signals from the test set. 
We then took a single arrhythmia ECG sample from the test set, applied 49 shifts to it (50 samples with the original), and created variants shown in blue.
Finally, we compared the embeddings with and without applying our proposed transformation function.
As seen in Figure~\ref{fig:t_SNE}, applying our transformation function maps the shifted samples to a single point in the embedding space, with the maximum Euclidean distance between embeddings being close to \(10^{-6}\).


Second, we conducted a simple experiment to investigate how the guidance network works with the proposed transformation.
Specifically, we created a two-label classification task where the model classifies sinusoids by frequency.
The dataset includes two waveforms: \( x_1(t) = \cos{(\omega_1 t + \phi_1)} + \cos{(\omega_2 t + \phi_2)} \) and \( x_2(t) = \cos{(\omega_1 t + \phi_1)} + \cos{(\omega_3 t + \phi_3)} \), with the model identifying whether the input contains frequency \( \omega_2 \) or \( \omega_3 \). Frequencies were set at 5, 24, and 25 Hz for \( \omega_1 \), \( \omega_2 \), and \( \omega_3 \), respectively, with \( \omega_1 \) included in both waveforms to increase task difficulty.
We set the sampling rate of the signals to 300\,Hz.



% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/output_angles.pdf}
%     \caption{Input waveforms to the classifier in the third epoch
%     \textbf{(a)} without applying our transformation function. 
%     \textbf{(b)} with our guidance network ($f_{\theta_G}$).
%     }
%     \label{fig:output_angle}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/output_angles_2.pdf}
    \caption{Input waveforms to the classifier in the third epoch
    \textbf{(a)} without applying our transformation function. 
    \textbf{(b)} with our guidance network ($f_{\theta_G}$).
    Another experiment with a different seed. Input waveforms
    \textbf{(c)} without applying our transformation function.
    \textbf{(d)} with our guidance network ($f_{\theta_G}$).
    }
    \label{fig:output_angle}
\end{figure}

To add diversity, we randomly shifted \( x_1(t) \) and \( x_2(t) \) by angles sampled from \( [0, \pi) \) and added Gaussian noise (\( \epsilon \)) with variance of 0.1. 
We used the FCN similar to that specified in Appendix~\ref{appendix:Implementation_Details} as the architecture. 
This experimental setup is inspired by similar experiments exploring neural network behaviors~\citep{spectral_bias}.


Figure~\ref{fig:output_angle} illustrates an interesting result: after a few weight updates, the guidance network assigns angles \(\phi \) that maximize the Euclidean distance between inter-class samples.
For instance, before applying the guidance network, the distance between a pair of samples is $9.12$ (Figure~\ref{fig:output_angle} \textbf{(a)}).
After applying the guidance network, this distance increases by four to $43.6$ (Figure~\ref{fig:output_angle} \textbf{(b)}).
Interestingly, running the same experiment with a different seed (i.e., a new random initialization of the guidance network) shows that the assigned angles differ, but the Euclidean distances between the samples remain almost unchanged, shifting only slightly from $43.6$ to $42.8$ (See Figure~\ref{fig:output_angle} \textbf{(d)}).


This experiment can also explain the occasional performance increase when the angle variance increases with a loss term as there is no single solution for minimizing the distance, but there can be infinitely many depending on the frequency distribution of the dataset and classes. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/different_seeds.pdf}
    \caption{
    \textbf{(a)} Angle assignments across different seeds. 
    \textbf{(b)} Euclidean distances between intra-class samples, compared with and without the proposed transformation (W/o $\mathcal{T}(\mathbf{x},\phi)$).
    The results show that the Euclidean distances are highly consistent across different seeds, with the curves nearly overlapping.
    Additionally, when the transformation is not applied, the distances between intra-class samples are noticeably higher.
    }
    \label{fig:different_seeds}
\end{figure}


We conducted an additional experiment to analyze the angle assignments across runs using the IEEE SPC22 dataset. The experiment was performed with three random seeds, and the results are presented in Figure~\ref{fig:different_seeds}.
In Figure~\ref{fig:different_seeds} \textbf{(a)}, we reported the assigned angles in $\cos{(\phi)}$ as the angle $-\pi$ and $\pi$ are the same due to the circular property of the angles. 
While the assigned angles vary slightly between runs, the Euclidean distances between samples consistently converge to similar values.
Specifically, the intra-class samples are closer in the transformed space compared to the case when the proposed transformation is not applied.


\clearpage


\section{Expanded Review of Related Work}
\label{appendix:extensive_related_work}


\paragraph{Transformation}
Applying phase shifts to time series is commonly used in signal processing~\citep{simon_haykin, Oppenheim}.
Recently, shifting phase values of harmonics have also been applied in the machine learning community for data augmentation of time series~\citep{demirel2023chaos, KDD_paper}.
However, our proposed transformation differs from these in two key aspects.
First, our work is the first to represent every point in the shift space uniquely with the phase angle of a harmonic whose period is equal to or longer than the length of the sample, i.e., $\mathrm{T}_0 \leq t$.
This observation enables us to design a bijective transformation that ensures shift invariance.
Additionally, we integrated this observation into deep learning frameworks using a novel loss function, demonstrating that our proposed method enhances model performance while ensuring shift invariance.


Second, we apply linear phase shifts to keep waveform features intact.
Since if an input signal is subjected to a phase shift that is a nonlinear function of $\omega$ similar to~\citet{KDD_paper,demirel2023chaos}, then the complex exponential components of the input at different frequencies will be shifted in a manner that results in a change in their relative phases.
Superimposing these exponentials can result in a signal that significantly differs from the input if special precautions are not taken.
Thus, this alteration in the waveform~\citep{Oppenheim} can potentially affect downstream labels or generate unrealistic signals.


However, our transformation applies the tailored shift linearly to all harmonics while keeping the information content unchanged~\citep{Mallat_Group_scattering} as the transformation operates as a group action.





\paragraph{Shift-invariant Kernels}
Learning shift invariant representations from data has a long history in machine learning~\citep{shift_invariant_sparse_coding, random_features}.
The initial effort focused on designing shift-invariant kernels for feature extraction~\citep{random_features}, which were applied to support vector machines.
A different approach introduced shift-invariant sparse coding technique, which reconstructs an input using all basis functions across all possible shifts~\citep{shift_invariant_sparse_coding}.
However, the classification performance of these techniques were significantly outperformed by the modern networks.
Thus, recent approaches have focused on integrating shift-invariant kernels into modern convolutional neural networks in a stacked manner while using Gaussian low-pass filters to prevent aliasing~\citep{NIPS2014_81ca0262}.


However, applying low-pass filters to prevent anti-aliasing completely is not possible~\citep{Oppenheim}.
Thus, high-frequency components will always (partially) alias.
This is more problematic for time series as the interaction of high and low frequencies are more common~\citep{demirel2023chaos, Robert_knight_1}.
Therefore, applying a low-pass filter can reduce the performance of neural networks in certain time series tasks, as shown by our experiments. The filtering may inadvertently attenuate important high-frequency components, which are essential for distinguishing patterns, leading to suboptimal model outcomes.


\paragraph{Learning based Transformations}
Methods to standardize inputs have been around for a long time~\citep{YUCEER1993687}.
An important recent work along this direction is the Spatial Transformer Network (STN) being introduced to learn transformation functions for invariant image classification~\citep{Spatial_transformer_nets}.
Similarly, Temporal Transformer Networks (TTN), an adaptation of STNs for time series applications, were introduced to predict the parameter of warp functions and align time series~\citep{Lohit2019TemporalTN, ICML_diffemor}.
Recent studies have utilized canonical equivarant networks to obtain mapping points for inputs~\citep{canonical}.
However, these methods face significant limitations as the operation order increases. Specifically, higher-order transformations in group equivarant networks require additional filter copies in the lifting layer and an increased number of parameters in the subsequent group convolution layers.
While this can improve performance, it comes at the cost of significantly higher computational and model complexity.
Furthermore, prior works restrict mappings to a finite number of group elements defined by the canonicalization network.
In contrast, our proposed transformation eliminates this limitation entirely, enabling each sample to map to any point in the input space—infinitely many—without relying on a neural network. This is achieved by uniquely representing each point in the shift space using a specific harmonic.




\clearpage



\section{Discussion, Limitations and Future Work}
\label{appendix:limitations}
In this work, we propose a new diffeomorphism to achieve shift invariant deep learning models for time series in real-world tasks. 
While existing techniques show promise for images, they fall short in time series, where the interaction of low and high frequencies are an important part of the data generation.
The proposed transformation offers a novel solution, ensuring that samples will map the same point in the high dimensional data manifold despite a random shift.
Theoretical and empirical analysis demonstrates its effectiveness across several time series tasks, enhancing model robustness while improving the performance.


While our approach consistently improves the performance of deep learning models for time series data, it is worth noting the potential areas for future investigation and improvement.
First, we conducted our experiments on health-related time series tasks from humans since the robustness of models is crucial in those domains.
Therefore, extending the proposed transformation to images for shift or rotation invariancy presents an intriguing direction for future investigations.
Thus, we believe that future research could benefit on adapting our approach to diverse domains, including images, to explore shift or rotation invariance further.
Second, our approach requires samples to be expanded into the sum of periodic sinusoidals with Fourier expansion followed by using the phase angle of the one whose period equals or exceeds the length of the signal.
Input samples should be decomposed the sinusoidals while considering this requirement.
Therefore, we believe future work can benefit by detecting the phase of a specific sinusoidal which satisfies the condition and apply a linear phase all-pass filter without performing the operation in the frequency domain. 
Lastly, we observed notable performance improvements from the additional guidance network when it is used with the proposed diffeomorphism while applying a proper loss function.
Thus, we believe that further performance improvements can be achieved through a refined design incorporating alternative inputs.


