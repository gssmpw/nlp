\section{Introduction}
\label{sec:introduction}
Inference on time series is essential for several important applications, such as heart rate (HR) estimation~\citep{KOSHY2018124}, activity recognition~\citep{JAMA_step}, and cardiovascular health monitoring~\citep{hannun_cardiologist_level_2019}, which are generally performed using signals that are encoded as a sequence of discrete values over time.
Most of these signals contain features that characterize the signal independently of their position in time~\citep{TDNN, demirel2023chaos}.
In other words, the information content of signals generally remains unchanged under the action of ﬁnite groups such as translations~\citep{Mallat_Group_scattering}.
% For example, a jump in activity recognition should be identified as a jump regardless of whether it occurs now or two seconds later.
Therefore, ensuring the ability to accurately capture these inherent patterns is crucial for the reliability of the deep learning models in such critical human-involved health-related tasks~\citep{safety_mobile_health, trustworthiness_ML}.
% Since most of these signals contain features that  remain unchanged with shifts~\citep{TDNN, demirel2023chaos}, ensuring the ability to accurately capture these inherent patterns is crucial for the reliability and effectiveness of the deep learning models in such critical human-involved health-related tasks~\citep{safety_mobile_health, trustworthiness_ML}.



Deep learning networks perform downsampling by using strided-convolution and pooling~\citep{ResNet, ImageNet_Classify_Hinton}, which cause loss of information due to high-frequency components of the input alias into lower frequencies, i.e., aliasing~\citep{Oppenheim}.
Previous works have proposed to employ a low-pass filter to prevent the aliasing and mitigate information loss during downsampling~\citep{zhang2019shiftinvar, NIPS2014_81ca0262}.
While this additional filtering improved the robustness, the effect of employed low-pass filters is quite poor compared to the ideal implementation (see Figure~\ref{fig:motivation} \textbf{a} and \textbf{b}), which still causes high-frequency components to alias into lower ones.


Despite the potential benefits of emphasizing low-frequency components for image recognition, as it aligns with human perception~\citep{human_perception}, the imperfect preservation of frequency components with each subsampling layer contributes to information loss, leads to performance degradation, especially in tasks where the significance lies in both low and high-frequency components with their interactions.
A more recent approach to achieve shift-invariant neural networks involves the use of adaptive subsampling grids~\citep{aps}. 
However, these methods still fail to guarantee shift-invariancy due to the change in content at the boundary~\citep{learnable_polyphase} and impose constraints on the shift range to maintain invariance.


\begin{figure}[t]
\centering
    \includegraphics{Figures/motivation_ICLR.pdf}
    \caption{\textbf{(a)}~The magnitude response of the ideal low-pass filter and binomial filter that is employed in~\citep{zhang2019shiftinvar} for preventing aliasing. \textbf{(b)} Time domain representations of the ideal and binomial filters with interpolation for smoother waveforms. \textbf{(c)} An 8-second signal for blood volume changes and its $t^{\prime} $ shifted version, obtained through photoplethysmogram--—a widely utilized signal for heart rate monitoring~\citep{apple_study}.
    \textbf{(d)} The heart rate prediction of a trained ResNet with binomial filters to prevent aliasing.
    Different amounts of shifts $(t^{\prime} \in [-4,4])$ change the trained model output drastically from 140 to 60 beats per minute (bpm).
    \textbf{(e)} A 10-second electrocardiogram (ECG) signal from a patient with atrial fibrillation (AFIB). \textbf{(f)} The model misclassifies the abnormal AFIB pattern as a healthy sinus rhythm (SR), with shifts causing a complete change in output probability.}
    \label{fig:motivation}
\end{figure}

% is impossible to realize practically as the filter's impulse response extends to infinity in time.

Consequently, the evaluation of these methods is confined to a limited range of shifts while covering a small subset of the space.
Additionally, their reliance on a grid scheme introduces a dependence on sampling rates, resulting in performance gaps across the entire shift space~\citep{Fractional_CVPR}.


In this work, we propose a differentiable bijective function that maps samples from their high-dimensional data manifold to another manifold of the same dimension, without any dimensional reduction.
Our method ensures that randomly shifted samples---representing variations of the same signal---are mapped to the same point in the space, preserving all task-relevant information.

% In this work, we propose a method that maps samples from their high-dimensional data manifold to another manifold of the same dimension, without any dimensional reduction. This ensures that shifted samples—representing variations of the same signal—are mapped to the same point in space, preserving all task-relevant information.


Since our method modifies the data space, it can be integrated into any deep learning architecture, offering an adaptable and complementary solution for achieving shift-invariancy in time series.
Summarizing our contributions in this paper:
\begin{itemize}
\item We introduce a novel diffeomorphism to ensure shift-invariancy in neural networks.
Additionally, we incorporate the proposed diffeomorphism into the network architecture using a novel, tailored loss term to further enhance performance while ensuring invariance.

\item We demonstrate both theoretically and empirically that the proposed transformation guarantees shift-invariancy in models without imposing any limits to the range of shifts or changing model topology, which enable previous methods to be used in conjuction.

\item We conduct extensive experiments on six time series tasks with nine datasets.
Our experiments show that the proposed approach consistently improves the performance while decreasing the variance and enabling models to achieve complete shift-invariance.
\end{itemize}

