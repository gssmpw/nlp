
\part{Appendix} % Start the appendix part
\parttoc % Insert the appendix TOC

\section{Proofs of Results in \Cref{sec:technical_work}}

\noindent \textbf{Notation.} For every $x \in \mathbb{R}$ and every function $f : \mathbb{R} \to \mathbb{R}$, we denote the left-limit of $f$ at $x$ by $f(x-) = \lim_{y \uparrow x} f(y)$.





\subsection{Proof of \Cref{prop:Myerson_and_Monteiro}}
\begin{proof}[\textbf{Proof of \Cref{prop:Myerson_and_Monteiro}}]
The proof follows from section 4.1 in \cite{monteiro2010optimal} by reapplying the same argument to the truncated ironed virtual value function.
\end{proof}

\subsection{Proofs of Lemmas in \Cref{sec:outline}}


\begin{proof}[\textbf{Proof of \Cref{lem:continuity}}]

\textit{Step 1:} We first show that for every $z \in I$, we have that $| \partial \Psi_{G}(z)| =1.$

    We note that for every $z,z' \in I$ we have that,
    \begin{equation*}
     \frac{H_{G}(z) - H_{G}(z')}{G(z)-G(z')} = \frac{\int_{z'}^z t dG(t)}{G(z) - G(z')} + \frac{\int_{z'}^z  (1-G(t))dt}{G(z) - G(z')}.
    \end{equation*}
    Given that $G$ admits a positive density on $I$, we have that $\lim_{z' \to z} \frac{H_{G}(z) - H_{G}(z')}{G(z)-G(z')} = z - \frac{1-G(z)}{g(z)}$ exists and is finite.

    Next, assume for the sake of contradiction that there exists $z \in I$ such that $| \partial \Psi_{G}(z)| > 1$, i.e. $\ell_{G}(z) < s_{G}(z)$. 

We argue that $H_{G}(z) = \Psi_{G}(z)$. 
Note that $G$ and $H_{G}$ are continuous at $z$, and $(vi)$ in \Cref{prop:monteiro} implies that $\Psi_{G}$ is also continuous at $z$.
Hence if, $H_{G}(z) > \Psi_{G}(z)$ we also have that $H_{G}(z-) > \Psi_{G}(z-)$, which implies by \Cref{prop:monteiro} property $(iv)$ and $(v)$ that there exists $z^* > z$ such that for every $x \in (z,z^*)$, we have that $s_{G}(z-) = s_{G}(z) = \ell_{G}(x)$  and $\ell_{G}(x) = \ell_{G}(z)$. Hence, $s_{G}(z) = \ell_{G}(z)$ which contradicts our initial assumption. Therefore,  
\begin{equation}
\label{eq:H_equal_Psi}
 H_{G}(z) = \Psi_{G}(z).
\end{equation}

Furthermore, for every $x \in [a,b]$, let
\begin{equation*}
    \rho(x) = \begin{cases}
        \Psi_{G}(z) + \ell_{G}(z) \cdot(G(x)-G(z)) \quad \text{for $z \in [a,z)$,}\\
        \Psi_{G}(z) + s_{G}(z) \cdot(G(x)-G(z)) \quad \text{for $z \in [z,b)$}.
    \end{cases}
\end{equation*}
By definition of the generalized sub-gradients in \eqref{eq:subgradient}, we have that $\rho(x) \leq \Psi_{G}(x)$. Furthermore, the definition of $\Psi_{G}(x)$ implies that $\Psi_{G}(x) \leq H_{G}(x)$ for every $x \in [a,b]$. 
Therefore, by using \eqref{eq:H_equal_Psi} we have established that, for every $x \in [a,b)$,
\begin{equation*}
    H_{G}(x) \geq \rho(x) = \begin{cases}
         H_{G}(z) + \ell_{G}(z) \cdot(G(x)-G(z)) \quad \text{for $x \in [a,z)$,}\\
         H_{G}(z) + s_{G}(z) \cdot(G(x)-G(z)) \quad \text{for $x \in [z,b)$}.
    \end{cases}
\end{equation*}
This implies that, $\frac{H_{G}(x) - H_{G}(z)}{G(x)-G(z)} \geq s_{G}(z)$ for every $x \in[z,b)$ and $\frac{H_{G}(x) - H_{G}(z)}{G(x)-G(z)} \leq \ell_{G}(z)$ for every $x \in [a,z)$. Then, by taking a limit over $x$ towards $z$, we obtain that,
\begin{equation*}
    \ell_{G}(z) \geq \lim_{x \to z} \frac{H_{G}(x) - H_{G}(z)}{G(x)-G(z)} \geq s_{G}(z).
\end{equation*}
Given that $\ell_{G}(z) <s_{G}(z)$ this leads to a contradiction. Therefore, for every $z \in I$, we have that $| \partial \Psi_{G}(z)| = 1$.


\textit{Step 2:} We next establish that $\ell_{G}$ is continuous on $I$.

We note that $g$ is positive and continuous on $I$. This implies that $G$ is increasing and continuous on $I$ and its inverse function $G^{-1}$ is well-defined in $G(I)$, increasing and continuous. 

Consider $\hat{\Psi}$ defined for every $z' \in G(I)$ as $\hat{\Psi}(z') = \Psi_{G}(G^{-1}(z'))$. We next show that $\hat{\Psi}$ is differentiable on $G(I)$. Let $z \in I$ and recall that $| \partial \Psi_{G}(z)| = 1$. For every $x \in G(I)$, we have that
    \begin{equation*}
         \frac{\hat{\Psi}(G(z)) - \hat{\Psi}(x)}{G(z)-x} = \frac{\Psi_{G}(z) - \Psi_{G}(G^{-1}(x))}{G(z)-G\left( G^{-1}(x) \right)}.
    \end{equation*}
By taking a limit as $x$ tends to $z$ and by noting that $G^{-1}$ is continuous at $z$ we obtain that,
    \begin{equation*}
        \lim_{x \to z} \frac{\hat{\Psi}(G(z)) - \hat{\Psi}(x)}{G(z)-x} = \lim_{x \to z} \frac{\Psi_{G}(z) - \Psi_{G}(G^{-1}(x))}{G(z)-G\left( G^{-1}(x) \right)} = \ell_{G}(z),
    \end{equation*}
where the equality follows from the fact that $\partial \Psi_{G}(z) = \{ \ell_{G}(z) \}$.  Therefore, $\hat{\Psi}$ is differentiable at $G(z)$ and its derivative is $\ell_{G}(G^{-1}(z))$. 

We have just established that $\hat{\Psi}$ is a differentiable function on $G(I)$ and its derivative is equal to $\ell_{G} \circ G^{-1}$. This implies that $\hat{\Psi}$ is convex on $G(I)$ as $\ell_{G}$ and $G$ are non-decreasing. Hence, $\hat{\Psi}$ is a uni-variate differentiable function  which is convex. Therefore, it is continuously differentiable on $G(I)$ \cite[Corollary 25.5.1]{rockafellar2007convex}. We conclude that $\ell_{G} \circ G^{-1}$ is continuous on $G(I)$ and the continuity of $G$ on $I$ allows us to conclude by composition that $\ell_{G}$ is continuous on $I$.
\end{proof}

\begin{proof}[\textbf{Proof of \Cref{lem:inclusion_to_eq}}]
We first note that for every $x \in I$, the assumption that $\ell_{F}(x) \in \partial \Psi_{G}(x)$ implies that $\ell_{F}(x) \geq \ell_{G}(x)$. We next show the reverse inequality.

If $|\partial \Psi_{G}(x)| = 1$, we have that $\ell_{F}(x) \in \partial \Psi_{G}(x) =  \{ \ell_{G}(x) \} $ which implies that $\ell_{G}(x) = \ell_{F}(x)$.

Next, assume that $|\partial \Psi_{G}(x)| > 1$.
The item $(i)$ in \Cref{prop:monteiro} implies that there exist at most countably many points that satisfy this. Hence, there exists a sequence $(x_n)_{n \in \mathbb{N}} \in I^{\mathbb{N}}$ such that for every $n \in \mathbb{N}$, we have that $|\partial \Psi_{G}(x_n)| = 1$, and $\lim_{n \to \infty} x_n = x$.
We then note that,
\begin{equation*}
\ell_{G}(x) \stackrel{(a)}{=} \lim_{n \to \infty} \ell_{G}(x_n) \stackrel{(b)}{=} \lim_{n \to \infty} \ell_{F}(x_n) \stackrel{(a)}{=} \ell_{F}(x), 
\end{equation*}
where the equalities $(a)$ hold because $\ell_{F}$ and $\ell_{G}$ are continuous on $I$ and $(b)$ holds because $|\partial \Psi_{G}(x_n)| = 1$ and $x_n \in I$ for all $n \in \mathbb{N}$. 

We conclude that $\ell_{G}(x) = \ell_{F}(x)$ for every $x \in I$.
\end{proof}


\begin{proof}[\textbf{Proof of \Cref{lem:prop_mu}}]

\,

\noindent \textit{(i)} Let $y \leq s$, we first prove that $\mu_y$ is not increasing.

By replacing  $\aFs,\bFs$ with their expressions as a function of $\aF,\bF$, we can rewrite for every $y \leq s$  and $x \geq s$ that,
\begin{equation*}
\mu_{y}(x) = \gamma \cdot \aF + \gamma \cdot \bF \cdot F(y) - (1-\gamma) \cdot \bF - \gamma \cdot H_{F}(y) + (1-\gamma) \cdot y.
\end{equation*}
Next, we will differentiate this expression with respect to $x$. 
First note that,
\begin{equation*}
\frac{d \aF}{dx} \stackrel{(a)}{=} \frac{dH_{F}(x)}{dx} - \frac{d \bF \cdot F(x)}{dx} \stackrel{(b)}{=} - F(x) \frac{d \bF}{dx},
\end{equation*}
where $(a)$ follows from \Cref{lem:charac_PsiF} and $(b)$ holds because $\frac{dH_{F}(x)}{dx} = \bF \cdot f(x)$. In fact, since $F$ is regular and admits a positive density on its support, we have $\bF \cdot f(x) = \ell_F(x) \cdot f(x) = x \cdot f(x) - (1 - F(x))$. The statement then follows by noting that $H_F(x) = -x\cdot (1-F(x))$.

Hence, we obtain that
\begin{equation*}
\frac{d \mu_{y}(x)}{dx} =  \frac{d \bF}{dx} \cdot \Big ( \gamma F(y) - \gamma F(x) - (1-\gamma) \Big) \leq 0,
\end{equation*}
where the last inequality holds because $F(y) \leq F(x)$ as $y \leq s \leq x$ and $\frac{d \bF}{dx} \geq 0$ because $\bF = \ell_{F}(x)$ which is non-decreasing by item $(ii)$ in \Cref{prop:monteiro}.

\noindent\textit{(ii):} Let $s \geq y > y'$ and let $x \geq s$. We have that,
\begin{align*}
\mu_{y}(x) - \mu_{y'}(x) &= \gamma \cdot \bF \cdot (F(y) - F(y')) - \gamma \cdot ( H_{F}(y) - H_{F}(y')) + (1-\gamma) \cdot (y-y')\\
&\stackrel{(a)}{=}  \gamma \cdot (F(y) - F(y')) \cdot \left( \bF - \frac{H_{F}(y) - H_{F}(y')}{F(y) - F(y')}  \right) + (1-\gamma) \cdot (y-y'),
\end{align*}
where $(a)$ holds because $F(y) > F(y')$ as $F$ has a positive density. Note that $(1-\gamma) \cdot (y-y') > 0$. Hence, to conclude, it is sufficient to show that
\begin{equation}
\label{eq:accroissement}
\bF - \frac{H_{F}(y) - H_{F}(y')}{F(y) - F(y')} \geq 0.
\end{equation}
Let $z = F(y)$ and $z' = F(y')$.
We note that $F$ is regular. Hence, $H_{F}(F^{-1}(\cdot))$ is convex. This implies that the mapping
$q \mapsto \frac{H_{F}(F^{-1}(z)) - H_{F}(F^{-1}(q))}{z-q}$
is non-decreasing. This implies that, 
\begin{equation*}
\frac{H_{F}(F^{-1}(z)) - H_{F}(F^{-1}(z'))}{z-z'} \leq \lim_{q \to z} \frac{H_{F}(F^{-1}(z)) - H_{F}(F^{-1}(q))}{z-q} = \frac{dH_{F}(F^{-1}(z))}{dz} = \bF[y]
\end{equation*}
Hence, we have that,
\begin{equation*}
\frac{H_{F}(y) - H_{F}(y')}{F(y) - F(y')}  \leq \bF[y] \stackrel{(a)}{\leq} \bF,
\end{equation*}
where $(a)$ holds because $\bF[\cdot]$ is non-decreasing by regularity of $F$ and $y \leq x$. Hence, we have proved that \eqref{eq:accroissement} holds, which concludes the proof of $(ii)$.

\noindent\textit{(iii):}
We first note that
\begin{align*}
    \mu_s(s) &= \gamma \cdot \aF[s] + \gamma \cdot \bF[s] \cdot F(s) - (1-\gamma) \cdot \bF[s] - \gamma \cdot H_{F}(s) + (1-\gamma) \cdot s \stackrel{(a)}{=} (1-\gamma) \cdot (s- \bF[s]) \stackrel{(b)}{>} 0, 
\end{align*}
where  $(a)$ follows from \Cref{lem:charac_PsiF} and $(b)$ holds because $\bF[s] = \ell_F(s) =  s - \frac{1-F(s)}{f(s)} < s$.

Furthermore,
\begin{align*}
    \mu_s(b) &= \gamma \cdot \aF[b] + \gamma \cdot \bF[b] \cdot F(s) - (1-\gamma) \cdot \bF[b] - \gamma \cdot H_{F}(s) + (1-\gamma) \cdot s\\
    &= \gamma \cdot \left( H_{F}(b) -  H_{F}(s) + \bF[b] \cdot F(s) - \bF[b] \cdot F(b) \right) + (1-\gamma) \cdot (s - \bF[b]) \\
    &= \gamma \cdot (F(b) - F(s)) \cdot \left( \frac{H_{F}(b) -  H_{F}(s)}{F(b)-F(s)} - \bF[b]  \right) + (1-\gamma) \cdot (s - \bF[b])\\
    &\stackrel{(a)}{\leq} (1-\gamma) \cdot (s - \bF[b]) \stackrel{(b)}{=} (1-\gamma) (s - b) < 0,
\end{align*}
where $(a)$ follows from the convexity of $H_F(F^{-1}(\cdot))$  and $(b)$ holds because $\bF[b] = \ell_F(b) =  b - \frac{1-F(b)}{f(b)} = b$ as $F(b) = 1$.

\end{proof}


\begin{proof}[\textbf{Proof of \Cref{prop:from_F_to_feasible_Fs}}]
The existence of $T$ follows from the intermediate value theorem applied to the function $\mu_s$ which is continuous, non-increasing (by property $(i)$ in \Cref{lem:prop_mu}) and satisfies $\mu_s(s) > 0$ and $\mu_s(M) \leq 0$ (by property $(iii)$ in \Cref{lem:prop_mu}).

Let $x \geq T$ and let $\bF = \ell_F(x)$. By \eqref{eq:subgrad_are_solutions}, there exists $\aF$ such that $(\aF,\bF)$ is optimal for Problem~\eqref{eq:gen_virtual_value} at the point $x$. We next show that the candidate solution $(\aFs,\bFs)$ defined in \eqref{eq:candidate} is feasible for Problem~\eqref{eq:F_gamma_after_s}.

The feasibility of $(\aFs,\bFs)$ for Problem~\eqref{eq:F_gamma_after_s} follows from \Cref{lem:feasible_post_T}.
We next show that $(\aFs,\bFs)$ is an optimal solution at $x$.

We note that the constraint \eqref{eq:constraint_post_s} evaluated at $x$ implies that any feasible $(\alpha,\beta)$ should satisfy
\begin{equation*}
\alpha + \beta \cdot (1-\gamma) + \beta \cdot \gamma \cdot F(x) \leq \gamma \cdot H_{F}(x).
\end{equation*}
Given that the LHS is equal to the objective of the problem, the value $\Psi_{F_{\gamma,s}}(x)$ of the problem is lower or equal to $\gamma \cdot H_{F}(x)$. We next show that $(\aFs,\bFs)$ achieves that value. Indeed, we remark that
\begin{align*}
\aFs + \bFs \cdot (1-\gamma) + \bFs \cdot \gamma \cdot F(x) = \gamma \cdot \left( \aF + \bF \cdot F(x) \right) = \gamma \cdot H_{F}(x),
\end{align*}
where the last equality follows from \Cref{lem:charac_PsiF}. 
This shows that $(\aFs,\bFs)$ is optimal for Problem \eqref{eq:F_gamma_after_s} at $x$. By \eqref{eq:subgrad_are_solutions}, this implies that $\ell_{F}(x) = \bF = \bFs   \in \partial \Psi_{F_{s,\gamma}}(x)$. Hence, $\ell_{F}(x) \geq \ell_{F_{s,\gamma}}(x)$.
\end{proof}

\section{Proof of \Cref{thm:main}}
\subsection{Proof of \Cref{thm:main}} \label{sec:apx_main_proof}

\begin{proof}[\textbf{Proof of \Cref{thm:main}}]~

\noindent  \textit{Step 1:}  We first characterize $\ell_{F_{\gamma,s}}(x)$ for $x \in [s,b]$.

Let $T$ be as defined in \Cref{prop:from_F_to_feasible_Fs}. 
\Cref{prop:from_F_to_feasible_Fs} implies that for every $x \in [T,b]$, we have that $\ell_{F}(x) \in \partial \Psi_{F_{\gamma,s}}(x)$. Furthermore, \Cref{lem:continuity} implies that $\ell_F$ and $\ell_{F_{\gamma,s}}$ are continuous on $[T,b]$ because they both admit a positive density on $[T,b]$ (since $T > s$). We conclude from \Cref{lem:inclusion_to_eq} that $\ell_{F_{\gamma,s}}(x) = \ell_{F}(x)$ for every $x \in [T,b]$.


We next prove that $\ell_{F_{\gamma,s}}(x) = \ell_{F}(T)$ for every $x \in [s,T]$. 

Note that, $F_{\gamma,s}(s) - F_{\gamma,s}(s-) = 1- \gamma > 0$. Hence, property $(iii)$ in \Cref{prop:monteiro} implies that
\begin{equation}
\label{eq:l_s_monteiro}
\ell_{F_{\gamma,s}}(s) = \frac{\Psi_{F_{\gamma,s}}(s)-\Psi_{F_{\gamma,s}}(s-)}{F_{\gamma,s}(s)-F_{\gamma,s}(s-)} = \frac{\Psi_{F_{\gamma,s}}(s)-\Psi_{F_{\gamma,s}}(s-)}{1-\gamma}.
\end{equation}
We next show that
\begin{equation}
\label{eq:bounds_psi_s}
    \Psi_{F_{\gamma,s}}(s-) \leq \gamma \cdot H_{F}(s) - (1-\gamma) \cdot s \quad \mbox{and} \quad  \Psi_{F_{\gamma,s}}(s) \geq \gamma \cdot H_{F}(s) - (1-\gamma) \cdot s + (1-\gamma) \cdot \ell_{F}(T).
\end{equation}

On the one hand, we note that for every $x < s$, \eqref{eq:constraint_pre_s} evaluated at $y = x$ implies that $\Psi_{F_{\gamma,s}}(x) \leq \gamma \cdot H_{F}(x) - (1-\gamma) \cdot x$. By taking the  left-limit to $s$ on both sides of the inequality and using the continuity of $H_{F}$ we obtain that $\Psi_{F_{\gamma,s}}(s-) \leq \gamma \cdot H_{F}(s) - (1-\gamma) \cdot s.$

On the other hand, let  $(\aFs[T],\bFs[T])$ be as defined in \eqref{eq:candidate}. \Cref{lem:feasible_post_T} implies that this vector is a feasible solution for Problem \eqref{eq:F_gamma_after_s}.  Therefore, 
\begin{align*}
\Psi_{F_{\gamma,s}}(s) &\stackrel{(a)}{\geq} \aFs[T] + \gamma \cdot \bFs[T] \cdot F(s) + (1-\gamma) \cdot \bFs[T]\\
&\stackrel{(b)}{=} \gamma \cdot H_{F}(s) - (1-\gamma) \cdot s + (1-\gamma) \cdot \bFs[T] \stackrel{(c)}{=}  \gamma \cdot H_{F}(s) - (1-\gamma) \cdot s + (1-\gamma) \cdot \ell_{F}(T),
\end{align*}
where $(a)$ holds by feasibility of $(\aFs[T],\bFs[T])$, $(b)$ holds because $\mu_s(T) = 0$ which implies that $\aFs[T] + \gamma \cdot \bFs[T] \cdot F(s) = \gamma \cdot H_{F}(s) - (1-\gamma)\cdot s$ and $(c)$ follows from the fact that by definition of $\bFs[T]$, we have that $\bFs[T] = \bF[T] = \ell_{F}(T)$.

By replacing in \eqref{eq:l_s_monteiro} the bounds derived for $\Psi_{F_{\gamma,s}}(s-) $ and $\Psi_{F_{\gamma,s}}(s)$ in \eqref{eq:bounds_psi_s}, we obtain that, $\ell_{F_{\gamma,s}}(s) \geq \ell_{F}(T).$ We then have that,
\begin{equation*}
    \ell_{F}(T) \stackrel{(a)}{=} \ell_{F_{\gamma,s}}(T) \stackrel{(b)}{\geq} \ell_{F_{\gamma,s}}(s) \geq \ell_{F}(T),
\end{equation*}
where $(a)$ has been established at the beginning of the proof, and $(b)$ follows from the monotonicity of $\ell_{F_{\gamma,s}}$ and because $T > s$. Hence, $\ell_{F_{\gamma,s}}$ is constant on $[s,T]$ equal to $\ell_{F_{\gamma,s}}(T)$.



\textit{Step 2:} 
Consider the following threshold:
\begin{equation}
\label{eq:def_T1}
    T_1 = \inf \{x \leq s \text{ s.t. } \ell_{F_{\gamma,s}}(x) = \ell_{F_{\gamma,s}}(s) \}. 
\end{equation}
Let us prove that for every $x < T_1$, we have that $\ell_{F_{\gamma,s}}(x) = \ell_{\gamma F}(x)$, where $\ell_{\gamma F}(x) = \inf \partial \Psi_{\gamma F}(x)$ and, for every $x$, we define
\begin{subequations}\label{eq:gammaF}
\begin{alignat}{2}
\Psi_{\gamma F}(x) = \; &\!\sup_{\alpha,\beta \in \mathbb{R}} &\;& \alpha +  \beta \cdot \gamma \cdot F(x) \\
&\text{s.t.} &      &  \alpha + \beta \cdot \gamma \cdot F(y) \leq \gamma \cdot H_{F}(y) - (1-\gamma) \cdot y \quad \forall y \leq s 
\end{alignat}
\end{subequations}
We  show in  \Cref{lem:relaxing_Psi_F_s} that for every $x \in [a,T_1)$, we have that $\Psi_{\gamma F}(x) = \Psi_{F_{\gamma,s}}(x)$. Furthermore, the feasible set of problem \eqref{eq:F_gamma_after_s} is included in the one of problem \eqref{eq:gammaF}, and both problems share the same objective function. Therefore, any optimal solution of \eqref{eq:F_gamma_after_s} is optimal for \eqref{eq:gammaF} which implies that $\partial \Psi_{F_{\gamma,s}}(x) \subset \partial \Psi_{\gamma F}(x)$ for all $x \in [a,T_1)$. In particular, we have $\ell_{F_{\gamma,s}}(x) \in \Psi_{\gamma F}(x)$ for all $x \in [a,T_1)$. 

Moreover, for every $x \in [a,T_1)$, $F_{\gamma,s}(x) = \gamma F(x)$, and the distribution $\gamma F$ has a positive density $\gamma f$. Hence, \Cref{lem:continuity} implies that $\ell_{F_{\gamma,s}}$ and $\ell_{\gamma F}$ are continuous on $[a,T_1)$. We conclude from \Cref{lem:inclusion_to_eq} that for every $x \in [a,T_1)$, we have that $\ell_{F_{\gamma,s}}(x) = \ell_{\gamma F}(x)$, and \Cref{prop:Myerson_and_Monteiro} implies that, $\ell_{F_{\gamma,s}}(x) = \mathrm{IRON}_{[a,s]}[\gamma F](x)$ for every $x \in [a,T_1)$.
 
\textit{Step 3:} To complete our characterization we show that the threshold defined in \eqref{eq:def_T1} satisfies $T_1 = s$. 

\Cref{lem:continuity}  implies that $\ell_{F_{\gamma,s}}$ is continuous on $[a,s)$, as $F_{\gamma,s}$ has a positive density on $[a,s)$.
Next, let us prove that $\lim_{x \uparrow T_1} \ell_{F_{\gamma,s}}(x) < \lim_{x \downarrow T_1} \ell_{F_{\gamma,s}}(x)$.

For every $x < T_1$, we showed in step 2 that $\ell_{F_{\gamma,s}}(x) = \mathrm{IRON}_{[a,s]}[\gamma F](x)$.  \Cref{lem:iron_lower_virtual} implies that for every $x \in [a,T_1)$, we have that $\mathrm{IRON}_{[a,s]}[\gamma F](x) \leq \sup_{v \in [a,s]} \varphi_{\gamma F}(v)$. Furthermore, we note that $\varphi_{\gamma F}(v) = v - \frac{1/\gamma-F(v)}{f(v)}$. Hence, for every $v \in [a,s)$,
\begin{equation*}
    \varphi_{\gamma F}(v) = v - \frac{1/\gamma-F(v)}{f(v)} = v - \frac{1-F(v)}{f(v)} - \frac{\frac{1}{\gamma}-1}{f(v)}\stackrel{(a)}{=} \ell_{F}(v) -  \frac{\frac{1}{\gamma}-1}{f(v)} < \ell_{F}(v),
\end{equation*}
where $(a)$ follows from the regularity of $F$. Therefore,
\begin{equation*}
\lim_{x \uparrow T_1} \ell_{F_{\gamma,s}}(x) \leq \sup_{v \in [0,s]} \varphi_{\gamma F}(v) <  \sup_{v \in [0,s]} \ell_{F}(v) = \ell_{F}(s),
\end{equation*}
were the last equality holds because $\ell_F$ is non-decreasing.

On the other hand, we have by definition of $T_1$ (see \eqref{eq:def_T1}) that $\lim_{x \downarrow T_1} \ell_{F_{\gamma,s}}(T_1) = \ell_{F_{\gamma,s}}(s) = \ell_{F}(T)$. Given that $s \leq T$ and $\ell_F$ is non-decreasing, we conclude that $\lim_{x \uparrow T_1} \ell_{F_{\gamma,s}}(x) < \ell_{F}(s) \leq \ell_{F}(T) = \lim_{x \downarrow T_1} \ell_{F_{\gamma,s}}(T_1)$. This implies that $\ell_{F_{\gamma,s}}$ is not continuous at $T_1$ and given that $\ell_{F_{\gamma,s}}$ is continuous on $[0,s)$ and $T_1 \leq s$, we conclude that $T_1 =s$.

Finally, we note that the structure of the revenue-maximizing auction follows from property $(vii)$ in \Cref{prop:monteiro}.
\end{proof}


\subsection{Auxiliary Results and Proofs}

\begin{lemma}
\label{lem:relaxing_Psi_F_s}
Let $T_1$ be as defined in \eqref{eq:def_T1}. Then, for every $x < T_1$, $\Psi_{\gamma F}(x) = \Psi_{F_{\gamma,s}}(x)$.   
\end{lemma}

\begin{proof}[\textbf{Proof of \Cref{lem:relaxing_Psi_F_s}}]
First, remark that $\Psi_{\gamma F}(x) \geq \Psi_{F_{\gamma,s}}(x)$, as \eqref{eq:gammaF} is a relaxation of \eqref{eq:F_gamma_after_s}, in which we removed the constraints \eqref{eq:constraint_post_s}.

Let us assume for the sake of contradiction that $\Psi_{\gamma F}(x) > \Psi_{F_{\gamma,s}}(x)$. 

Let $(\hat{\alpha}(x),\hat{\beta}(x))$ (resp. $(\alpha'_{\gamma F}(x),\beta'_{\gamma F}(x))$) be an optimal solution for Problem \eqref{eq:F_gamma_after_s} (resp.  \eqref{eq:gammaF}). We note that an optimal solution is achieved for each problem as we are optimizing linear functions and the value of the problem is finite.

\textit{Step 1:} We will show that there exists $\tilde{y} \geq s$ such that,
\begin{equation}
\label{eq:constraint_achieved}
    \hat{\alpha}(x) + \hat{\beta}(x) \cdot \gamma \cdot F(\tilde{y}) + (1-\gamma) \cdot  \hat{\beta}(x) = \gamma \cdot H_{F}(\tilde{y}).
\end{equation}
Assume for the sake of contradiction that there does not exist any $\tilde{y} \geq s$ such that \eqref{eq:constraint_achieved} holds and consider,
\begin{align*}
        S_1 =& \sup_{y \in [s,b]} \hat{\alpha}(x) + \hat{\beta}(x) \cdot \gamma \cdot F(y) + (1-\gamma) \cdot  \hat{\beta}(x) - \gamma \cdot H_{F}(y),\\
        S_2 =& \sup_{y \in [s,b]} \alpha'_{\gamma F}(x) + \beta'_{\gamma F}(x) \cdot \gamma \cdot F(y) + (1-\gamma) \cdot  \beta'_{\gamma F}(x) - \gamma \cdot H_{F}(y).
\end{align*}
Note that both $S_1$ and $S_2$ are finite and achieved as the functions are continuous on a compact segment. Furthermore, $S_1 < 0$, otherwise \eqref{eq:constraint_achieved} would hold.

For every $\lambda \in [0,1]$, let $(\alpha_\lambda(x),\beta_\lambda(x)) = \lambda \cdot (\hat{\alpha}(x),\hat{\beta}(x)) + (1-\lambda) \cdot (\alpha'_{\gamma F}(x),\beta'_{\gamma F}(x)).$ We will choose $\lambda \in (0,1)$ such that $(\alpha_\lambda(x),\beta_\lambda(x))$ is feasible for Problem \eqref{eq:F_gamma_after_s} and achieves an objective strictly greater than the one achieved by $(\hat{\alpha}(x),\hat{\beta}(x))$. 

Note that for every $\lambda \in [0,1]$ we have that the constraints \eqref{eq:constraint_pre_s} are satisfied for all $y < s$ as both $(\hat{\alpha}(x),\hat{\beta}(x))$ and $(\alpha'_{\gamma F}(x),\beta'_{\gamma F}(x))$ satisfy these constraints and any convex combination of feasible solution is still feasible for these constraints.
Furthermore, $(\alpha_\lambda(x),\beta_\lambda(x))$ satisfies the constraints \eqref{eq:constraint_post_s} for all $y \geq s$ if and only if,
\begin{equation*}
    \sup_{y \in [s,b]} \alpha_\lambda(x) + \beta_\lambda(x) \cdot \gamma \cdot F(y) + (1-\gamma) \cdot  \beta_\lambda(x) - \gamma \cdot H_{F}(y) \leq 0.
\end{equation*}
By construction of $(\alpha_\lambda(x),\beta_\lambda(x))$, we have that
\begin{equation*}
    \sup_{y \in [s,b]} \alpha_\lambda(x) + \beta_\lambda(x) \cdot \gamma \cdot F(y) + (1-\gamma) \cdot  \beta_\lambda(x) - \gamma \cdot H_{F}(y) \leq \lambda \cdot S_1 + (1-\lambda) \cdot S_2.
\end{equation*}
Given that $S_1 < 0$, there exists $\lambda > 0$ such that $\lambda \cdot S_1 + (1-\lambda) \cdot S_2 \leq 0.$ In what follows we fix such $\lambda$. We obtain that $(\alpha_\lambda(x),\beta_\lambda(x))$ is feasible for the constraints \eqref{eq:constraint_post_s} for all $y \geq s$ which implies that $(\alpha_\lambda(x),\beta_\lambda(x))$ is feasible for \eqref{eq:F_gamma_after_s}. 

Finally, we note that the objective obtained with the solution $(\alpha_\lambda(x),\beta_\lambda(x))$ satisfies,
\begin{align*}
    \alpha_\lambda(x) + \beta_\lambda(x) \cdot \gamma \cdot F(x) 
    = \lambda \Psi_{F_{\gamma,s}}(x) + (1-\lambda) \Psi_{\gamma F}(x)
    \stackrel{(a)}{>} \Psi_{F_{\gamma,s}}(x) = \hat{\alpha}(x) + \hat{\beta}(x) \cdot \gamma \cdot F(x),
\end{align*}
where $(a)$ holds because $\Psi_{F_{\gamma,s}}(x) < \Psi_{\gamma F}(x)$. This contradicts the optimality of $(\hat{\alpha}(x),\hat{\beta}(x))$ for Problem \eqref{eq:F_gamma_after_s}. 

Therefore, $(\hat{\alpha}(x),\hat{\beta}(x))$ must satisfy \eqref{eq:constraint_achieved} for some $\tilde{y} \geq s$. 

\textit{Step 2:} We next show that for every $z \in (x,s]$, we have that  $\ell_{F_{\gamma,s}}(z) = \ell_{F_{\gamma,s}}(s)$. 
Let $\tilde{y} \geq s$ be such that $(\hat{\alpha}(x),\hat{\beta}(x))$ satisfies \eqref{eq:constraint_achieved}. We next show that this implies that $\hat{\beta} (x) \in \partial \Psi_{F_{\gamma,s}}(\tilde{y})$. 
Indeed, $(\hat{\alpha}(x),\hat{\beta}(x))$ is feasible for Problem \eqref{eq:F_gamma_after_s} at $\tilde{y}$ and it is optimal, because $\Psi_{F_{\gamma,s}}(\tilde{y}) \leq \gamma H_{F}(\tilde{y})$ and \eqref{eq:constraint_achieved} implies that $(\hat{\alpha}(x),\hat{\beta}(x))$ achieves this value. Hence, $\beta^*(x) \in \partial \Psi_{F_{\gamma,s}}(\tilde{y})$. Consequently, we have that $\partial \Psi_{F_{\gamma,s}}(\tilde{y}) \cap \partial \Psi_{F_{\gamma,s}}(x) \neq \emptyset$.

We conclude that for every $x' \in (x,\tilde{y}]$ we have that
\begin{equation*}
    \ell_{F_{\gamma,s}}(s) \stackrel{(a)}{\leq}  \ell_{F_{\gamma,s}}(\tilde{y}) \stackrel{(b)}{=} \ell_{F_{\gamma,s}}(x') \stackrel{(a)}{\leq} \ell_{F_{\gamma,s}}(s)
\end{equation*}
where $(a)$ follows from the monotonicity of the $\ell_{F_{\gamma,s}}$, and the fact that $x \leq s \leq \tilde{y}.$ Furthermore $(b)$ holds by \Cref{lem:disjoint_subg}.

Therefore, for every $x' \in (x,s]$, we have that $\ell_{F_{\gamma,s}}(x') = \ell_{F_{\gamma,s}}(s)$.

In particular, there exists $z \in (x,T_1)$ such that $\ell_{F_{\gamma,s}}(z) = \ell_{F_{\gamma,s}}(s)$. This contradicts the definition of $T_1$. Therefore, $\Psi_{\gamma F}(x) = \Psi_{F_{\gamma,s}}(x)$ for every $x \in [0,T_1)$.

\end{proof}


\begin{lemma}
\label{lem:F_and_H}
For every distribution $F$, any $\gamma \in (0,1)$ and any $s$ in the support of $F$, we have for every $x$ that,
\begin{equation*}
F_{\gamma,s}(x) = \begin{cases}
\gamma \cdot F(x) \quad \text{if $x < s$}\\
\gamma \cdot F(x) + (1-\gamma) \quad \text{if $x \geq s$}
\end{cases} 
\quad 
\mbox{and} 
\quad 
H_{F_{\gamma,s}}(x) = \begin{cases}
\gamma \cdot H_{F}(x) - (1-\gamma) \cdot x  \quad \text{if $x < s$}\\
\gamma \cdot H_{F}(x) \quad \text{if $x \geq s$}.
\end{cases}
\end{equation*}
\end{lemma}

\begin{proof}[\textbf{Proof of \Cref{lem:F_and_H}}]
We characterized $F_{\gamma,s}$ in \eqref{eq:cumulative-F}. For $x < s$, we have that,
\begin{equation*}
    H_{F_{\gamma,s}}(x) = \int_0^x t d F_{\gamma,s}(t) - \int_0^x (1-F_{\gamma,s}(t))dt = \gamma \int_0^x t d F(t) - \int_0^x (1-\gamma F(t))dt = \gamma H_{F}(x) - (1-\gamma) \cdot x. 
\end{equation*}
Moreover, for $x \geq s$, we have that,
\begin{align*}
    H_{F_{\gamma,s}}(x) &= \int_0^x t d F_{\gamma,s}(t) - \int_0^x (1-F_{\gamma,s}(t))dt\\
    &= (1-\gamma) \cdot s + \gamma \int_0^s t d F(t) - \int_0^s (1-\gamma F(t)) dt - (1-\gamma) \cdot (x-s) \\
    &= \gamma \cdot H_{F}(x).
\end{align*}

\end{proof}


\begin{lemma}
\label{lem:charac_PsiF}
For every $x$, let $(\aF,\bF)$ be an optimal solution of \eqref{eq:gen_virtual_value}, then
\begin{equation*}	
\aF + \bF \cdot F(x) = H_{F}(x).
\end{equation*}
\end{lemma}
\begin{proof}[Proof of \Cref{lem:charac_PsiF}]
We note that by optimality of $(\aF,\bF)$, we have that $\Psi(x) = \aF(x) + \bF(x) \cdot F(x).$ Furthermore as $F$ has a positive density it is strictly increasing, and its inverse function $F^{-1}$ is well-defined. The function defined for every $z \in [0,1]$ as $z \mapsto \Psi(F^{-1}(z))$ then corresponds to the convex envelope of $z \mapsto H_{F}(F^{-1}(z))$. The latter is convex because $F$ is regular. Therefore, $\Psi(F^{-1}(z)) = H_{F}(F^{-1}(z))$ for every $z \in [0,1]$. By evaluating this equality for $z = F(x)$, we obtain that $\aF(x) + \bF(x) \cdot F(x) = \Psi(x) = H_{F}(x)$.
\end{proof}

\begin{lemma}	
\label{lem:feasible_post_T}
Let $T$ such that, $\mu_s(T) = 0$. Then, for every $x \geq T$, the vector $(\aFs,\bFs)$ is feasible for Problem \eqref{eq:F_gamma_after_s}.
\end{lemma}
\begin{proof}[\textbf{Proof of \Cref{lem:feasible_post_T}}]

Let $x \geq T$ and let $\bF = \ell_F(x)$. By \eqref{eq:subgrad_are_solutions}, there exists $\aF$ such that $(\aF,\bF)$ is optimal for Problem~\eqref{eq:gen_virtual_value} at the point $x$. We next show that the candidate solution $(\aFs,\bFs)$ defined in \eqref{eq:candidate} is feasible for Problem~\eqref{eq:F_gamma_after_s}.

Let $y \geq s$. We note that,
\begin{align*}
\aFs + (1-\gamma) \cdot \bFs + \bFs \cdot \gamma \cdot F(y) \leq \gamma \cdot H_{F}(y) &\iff \gamma \cdot \left(  \aF + \bF \cdot F(y) \right) \leq \gamma H_{F}(y) \\
&\iff \aF + \bF \cdot F(y)  \leq H_{F}(y).
\end{align*}
The last inequality holds because $(\aF,\bF)$ is feasible for \eqref{eq:gen_virtual_value}. Hence, the first inequality holds. Which implies that $(\aFs,\bFs)$ satisfies the constraint \eqref{eq:constraint_post_s}.

Furthermore, fix $y \leq s$. By definition of $\mu_y$, we have that $(\aFs,\bFs)$ satisfies \eqref{eq:constraint_pre_s} if and only if, $\mu_y(x) \leq 0.$ This inequality holds because,
\begin{equation*}
\mu_y(x) \stackrel{(a)}{\leq} \mu_s(x) \stackrel{(b)}{\leq} \mu_s(T_2) = 0, 
\end{equation*}
where $(a)$ follows from property $(ii)$ in \Cref{lem:prop_mu} and $(b)$ from property $(i)$ in \Cref{lem:prop_mu}.
Therefore, $(\aFs,\bFs)$ is feasible for Problem \eqref{eq:F_gamma_after_s}.

\end{proof}

\begin{lemma}
\label{lem:disjoint_subg}
Let $x < y$, assume that $\partial \Psi_{F}(x) \cap \partial \Psi_{F}(y) \neq \emptyset$, then for every $x' \in (x,y]$, we have that $\ell_{F}(x') = \ell_{F}(y)$.
\end{lemma}
\begin{proof}[\textbf{Proof of \Cref{lem:disjoint_subg}}]
Let $u \in \partial \Psi_{F}(x) \cap \partial \Psi_{F}(y).$ Let $x' \in (x,y)$ and assume for sake of contradiction that $\ell_{F}(x') < \ell_{F}(y)$. 
We have that, $s_{F}(x') \geq s_{F}(x) \geq u \geq \ell_{F}(y) > \ell_{F}(x') \geq \ell_{F}(x)$.
Hence, $(\ell_{F}(x), s_{F}(x)) \cap (\ell_{F}(x'), s_{F}(x')) \neq \emptyset$. This contradicts item $(ii)$ \Cref{prop:monteiro}. Therefore, for every $x' \in (x,y]$, we have that $\ell_{F}(x') = \ell_{F}(y)$.
\end{proof}


\begin{lemma}
    \label{lem:iron_lower_virtual}
    Let $F$ be a distribution with positive and continuous density and let $\varphi_F$ be the virtual function. Then, for every $x,t \in [a,b]$ such that $x \leq t$, we have that
    \begin{equation*}
    \mathrm{IRON}_{[a,t]}[F](x) \leq \sup_{v \in [a,t]} \varphi_{F}(v). 
    \end{equation*}
\end{lemma}

\begin{proof}[\textbf{Proof of \Cref{lem:iron_lower_virtual}}]
    Let $t \in [a,b]$.
    Note that by construction $\mathrm{IRON}_{[a,t]}[F]$ is a non-decreasing function. Therefore, it is sufficient to prove that, 
    \begin{equation}
    \label{eq:ineq_to_prove}
    \mathrm{IRON}_{[a,t]}[F](t) \leq \sup_{v \in [a,t]} \varphi_{F}(v). 
    \end{equation}
    Assume for the sake of contradiction that this inequality does not hold.

    Recall the definition of $J$ (see \eqref{eq:J}) and of $G_{F(t)}$, the convex hull of the restriction of $J$ on $[0,F(t)]$.
    By definition, we have that
    \begin{equation*}
        G_{F(t)}(F(t)) = \min_{ \substack{(\lambda,r_1,r_2) \in [0,1]\times[0,F(t)]^2\\ \text{s.t. } \lambda \cdot r_1 + (1-\lambda) \cdot r_2 = F(t)} } \lambda \cdot J(r_1) + (1-\lambda) \cdot J(r_2) = J(F(t)),
    \end{equation*}
    where the last equality holds because $F(t)$ is an extreme point of $[0,F(t)]$, hence $r_1$ must equal $r_2$. Similarly we can show that $G_{F(t)}(0) = J(0).$ Let $u = \sup \{ x < t \text{ s.t. } G_{F(t)}(F(x)) = J(F(x)) \}$ Note that $u$ exists and is finite as the set is non-empty (it includes $0$) and bounded. We reason by disjunction of cases on the value of $u$.

    \textit{Case 1: $u = t$.} \Cref{prop:Myerson_and_Monteiro} and \Cref{lem:continuity} implies that $\mathrm{IRON}_{[a,t]}[F]$ is continuous as $F$ has a positive and continuous density. By assumption we have that $\mathrm{IRON}_{[a,t]}[F](t) > \sup_{v \in [a,t]} \varphi_{F}(v)$. The continuity of $\mathrm{IRON}_{[a,t]}[F]$  implies that there exists $\epsilon$ such that for every $t' \in [t-\epsilon,t]$ we have that, $\mathrm{IRON}_{[a,t]}[F](t') > \sup_{v \in [a,t]} \varphi_{F}(v)$. Moreover, as $u = t$, for $\epsilon$ small enough we also have that $G_{F(t)}(F(t-\epsilon)) = J(t-\epsilon)$. Fix such $\epsilon$ and observe that,
    \begin{align*}
        \int_{F(t-\epsilon)}^{F(t)} \varphi_F(F^{-1}(r)) dr &\stackrel{(a)}{=} J(F(t)) -  J(F(t - \epsilon))\\
        &= G_{F(t)}(F(t)) - G_{F(t)} (F(t-\epsilon)) \stackrel{(b)}{=} \int_{F(t-\epsilon)}^{F(t)}  \mathrm{IRON}_{[a,t]}[F](F^{-1}(r)) dr,
    \end{align*}
    where $(a)$ follows form \eqref{eq:J} and $(b)$ holds because $F$ is a continuous and increasing (as it admits a positive density everywhere) and hence it is invertible with inverse $F^{-1}.$
    
    Hence, we have established that 
    \begin{equation*}
        \int_{F(t-\epsilon)}^{F(t)} (\varphi_F(F^{-1}(r)) -\mathrm{IRON}_{[a,t]}[F](F^{-1}(r)))  dr =0,
    \end{equation*}
    which contradicts the fact that $\mathrm{IRON}_{[a,t]}[F](t') > \sup_{v \in [a,t]} \varphi_{F}(v)$ for every $t' \in [t-\epsilon,t]$.

    \textit{Case 2: $u < t$.}
    In that case $G_{F(t)}$ has a constant differential on $(u,t]$. Hence we obtain that,
    \begin{align*}
       \int_{F(u)}^{F(t)} \varphi_F(F^{-1}(r)) dr &= J(F(t)) -  J(F(u))\\
       &=  G_{F(t)}(F(t)) - G_{F(t)}(F(u))\\
       &=  \int_{F(u)}^{F(t)}  \mathrm{IRON}_{[a,t]}[F](F^{-1}(r)) dr > (F(t)-F(u)) \sup_{v \in [a,t]} \varphi_{F}(v).
    \end{align*}
    This leads to a contradiction.


    

    
\end{proof}




\begin{proposition}[\cite{monteiro2010optimal}]
\label{prop:monteiro}
Let $F$ be a distribution. We have that:
\begin{enumerate}
\item[i.] $ \{ x  \, \text{s.t.} \,  |\partial \Psi_{F}(x)| > 1 \}$ is at most countable.   
\item[ii.] $\ell_F(\cdot)$ and $s_F(\cdot)$ are non-decreasing and $(\ell(x),s(x)) \cap (\ell(x'),s(x')) = \emptyset$ for all $x \neq x'$.
\item [iii.] If $F(x) - F(x-) >0$, we have that $\ell_{F}(x) = \frac{\Psi_{F}(x)-\Psi_{F}(x-)}{F(x)-F(x-)}$.
\item [iv.] If $\Psi_F(x-) < H_F(x-)$, then there exists an interval $[x,z^*)$ such that $s(z) = \ell(z) = s(x-)$ for every $z \in (x,z^*)$.
\item [v.] If $\Psi_F(x) < H_F(x)$, then there exists an interval $[x,z^*)$ such that $\ell(z) = \ell(x)$ for every $z \in (x,z^*)$.
\item [vi.] For every $x$, $\Psi_{F}$ is continuous at $x$ if $F$ is continuous at $x$.
\item [vii.] Let $(F_i)_{i \in \{1,\ldots,n\}}$ be the value distributions of the buyers, and $(\hat{v}_i)_{i \in \{1,\ldots,n\}}$ their reported values. The following auction is revenue-maximizing. Allocate to the buyer with the highest non-negative value of $\ell_{F_i}(\hat{v}_i)$, and make them pay  $\ell_{F_i}^{-1} \left( \max \{0, \max_{j \neq i} \ell_{F_j}(\hat{v}_j)\} \right)$
\end{enumerate}
\end{proposition}

\begin{proof}[\textbf{Proof of \Cref{prop:monteiro}}]
We next point to the results in \cite{monteiro2010optimal} implying each of the points in the proposition.
    $(i)$ follows from Remark 3, $(ii)$ follows from Proposition 1.f and Remark 3, $(iii)$ follows from Proposition 4, $(iv)$ and $(v)$ are established in Proposition 5, $vi$ follows from Proposition 3 and $(vii)$ is established in Theorem 5.
\end{proof}

\section{Proof of \Cref{cor:optimal_price}}

\begin{proof}[\textbf{Proof of \Cref{cor:optimal_price}}]
   \Cref{thm:main} applied to the single buyer case implies that the optimal mechanism is a posted price mechanism using the price $p^*$, where $p^*$ satisfies $\bar{\varphi}_{F_{\gamma,s}}(p^{*}-) \leq 0$ and $\bar{\varphi}_{F_{\gamma,s}}(p^{*}) \geq 0$. Note that when $\bar{\varphi}_{F_{\gamma,s}}$ is continuous at $p^*$, this condition becomes $\bar{\varphi}_{F_{\gamma,s}}(p^*) = 0$.
   
   Let $p_F = \inf \varphi_{F}^{-1}(\{0\})$. We note that $p_F$ exists as $\varphi_{F}(0) < 0$. Next, denote by $T_{s}$ the threshold defined in \Cref{thm:main}, where we explicitly mark the dependence in $s$. \Cref{lem:prop_mu} implies that $T_s$ is non-decreasing in $s$.

   Let $L_{\gamma} = \inf \{ s \geq 0 \mid T_s \geq p_F\}$, where we set $L_{\gamma} = 0$ if this set is empty. Furthermore, let $U_\gamma = \sup\{s \leq 1 \mid \mathrm{IRON}_{[0, s]}[\gamma F](s) \leq 0 \}$.
   
   
   \textit{Step 1:} For every $s < L_{\gamma}$, we show that $\varphi_{F}(p^*) = 0$. We remark that $\bar{\varphi}_{F_{\gamma,s}}(T_s) < 0$ because $T_s < p_F$ (by definition of $L_{\gamma}$). Since $\bar{\varphi}_{F_{\gamma,s}}(\cdot)$ is non-decreasing, we have that $\bar{\varphi}_{F_{\gamma,s}}(v) < 0$ for all $v < T_s$. Therefore, $p^* \geq T_s$, which implies by \Cref{thm:main} that $\bar{\varphi}_{F_{\gamma,s}}(p^*) = \varphi_{F}(p^*)$. Given that $\varphi_{F}$ is continuous as $F$ and $f$ are continuous, we must have that $\varphi_{F}(p^*) = 0$.

   \textit{Step 2:} For every $s \in [L_{\gamma}, U_{\gamma})$, we show that $p^* = s$. We remark that for every $v < s$, 
   \begin{equation*}
       \bar{\varphi}_{F_{\gamma,s}}(v) = \mathrm{IRON}_{[0, s]}[\gamma F](v) \leq \mathrm{IRON}_{[0, s]}[\gamma F](s) \leq 0,
   \end{equation*}
   where the last inequality holds by definition of $U_{\gamma}$. Therefore, $\bar{\varphi}_{F_{\gamma,s}}(s-) \leq 0$, furthermore, given that $s \geq L_{\gamma}$, we have that
   $\bar{\varphi}_{F_{\gamma,s}}(s) = \bar{\varphi}_{F_{\gamma,s}}(T_s) \geq \bar{\varphi}_{F_{\gamma,s}}(p_F) = 0$.  This implies that $p^* = s.$

   \textit{Step 3:} For every $s \geq U_{\gamma}$, we show that $\varphi_{\gamma F}(p^*) = 0$. We note that in that case, $p^* < s$ as such $p^*$ must satisfy the first order condition for $p \cdot (1-\gamma F(p))$, which implies is equivalent to $\varphi_{\gamma F}(p^*) = 0$. 
\end{proof}


