\section{Characterization of the Optimal Auction}\label{sec:optimal_auction}

In this section, we first introduce a slight generalization of Myerson's ironing operation, which we will need to state our results. We then present our main theorem, and demonstrate what it implies for some simple distributions. We also show that our main theorem fails if we remove the regularity assumption. 

\subsection{Truncated Myerson Ironing}\label{sec:ironing}

Consider a distribution $F$ supported on $[a,b]$ and which admits a positive density on its support.  In that case $F$ is strictly increasing on $[a,b]$ and therefore it admits an inverse function $F^{-1}$ strictly increasing on $[0,1]$. When the virtual value function of $F$ defined for every $x \in [a,b]$ as $\varphi_{F}(x)$  
is not monotonic non-decreasing, \citet{myerson1981optimal} proposes a general procedure called ironing to characterize the optimal auction. In what follows we introduce our slight generalization of Myerson's ironing operator. The only difference between the operator we introduce below and the one presented in \citet{myerson1981optimal} is that we also allow for the operation to be performed only in an interval of the quantile space rather than over the entire quantile space. Hence, we call this operation the truncated Myerson ironing. If we restrict $x$ to be equal to 1 in what follows, we would be mimic the definition of the original Myerson ironing operator.



For every quantile $q \in [0,1]$, let
\begin{equation}
\label{eq:J}
    J(q) = \int_0^q \varphi_{F}(F^{-1}(r)) dr.
\end{equation}
Furthermore, for every $x \in [0,1]$, let $G_x:[0,x] \to \mathbb{R}$ be the convex hull of the restriction of the function $J$ on $[0,x]$, formally defined for every $q \in [0,x]$ as,
\begin{equation*}
    G_x(q) = \min_{ \substack{(\lambda,r_1,r_2) \in [0,1]\times[0,x]^2\\ \text{s.t. } \lambda \cdot r_1 + (1-\lambda) \cdot r_2 = q} } \lambda \cdot J(r_1) + (1-\lambda) \cdot J(r_2) 
\end{equation*}
By definition, $G_x$ is convex on $[0,x]$. Therefore, it is continuously differentiable on $[0,x]$ except at countably many points. For every $q \in [0,x]$, we define the function $g$ as,
\begin{equation*}
    g_x(q) = \begin{cases}
        G'_x(q) \quad \text{if $G$ is differentiable at $q$}\\
        \lim_{\tilde{q} \downarrow q} G'_x(\tilde{q}) \quad \text{otherwise.}
    \end{cases}
\end{equation*}
The convexity of $G_x$ implies that $g_x$ is monotone non-decreasing. For any $t \in [a,b]$ we define the truncated ironed virtual of $F$ on $[a,t]$ as the mapping,
\begin{equation*}
    \mathrm{IRON}_{[a,t]}[F] : \begin{cases}
        [a,t] \to \mathbb{R}\\
        v \mapsto g_{F^{-1}(t)}(F(v)).
    \end{cases}
\end{equation*}


We note that $\mathrm{IRON}_{[a,b]}[F]$ corresponds to the classical notion of ironing introduced in \citet{myerson1981optimal}. We emphasize that when $t < b$, the mapping $\mathrm{IRON}_{[a,t]}[F]$ is in general different from the restriction of $\mathrm{IRON}_{[a,b]}[F]$ on $[a,t]$ (see \Cref{fig:ironing_operator}). 

 \begin{figure}[h!]
    \centering
    \subfigure[Convexification in quantile space]{
    \begin{tikzpicture}[scale=.65]
    \begin{axis}[
        width=10cm,
        height=10cm,
        xmin=-0.,xmax=1.0,
        ymin=-0.12,ymax=0.01,
        scaled y ticks={base 10:2},
        table/col sep=comma,
        xlabel={$q$},
        ylabel={$H(q)$},
        grid=both,
        legend pos=south west
    ]


    \addplot [blue, dashed, line width=.7mm] table[x=F,y=H] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{Before ironing}
    


    \addplot [red, very thick] table[x=F,y=psi] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{$\mathrm{IRON}_{[0,2]}$}
    

    \addplot [teal, very thick] table[x=F,y=psi_cut05] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{$\mathrm{IRON}_{[0,0.5]}$}
    


    \addplot [black, very thick] table[x=F,y=psi_cut02] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{$\mathrm{IRON}_{[0,0.2]}$}
    


    \end{axis}
    \end{tikzpicture}
    }
    \subfigure[Virtual value]{
    \begin{tikzpicture}[scale=.65]
    \begin{axis}[
        width=10cm,
        height=10cm,
        xmin=0,xmax=2.0,
        ymin=-2.5,ymax=2,
        table/col sep=comma,
        xlabel={$v$},
        ylabel={virtual value},
        grid=both,
        legend pos=south east
    ]
    
    \addplot [blue,  dashed, line width=.7mm] table[x=x,y={virtual_value_preiron}] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{Before ironing}

    \addplot [red, very thick] table[x=x,y=virtual_value] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{$\mathrm{IRON}_{[0,2]}$}

    \addplot [teal, very thick] table[x=x,y=virtual_value_cut05] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{$\mathrm{IRON}_{[0,0.5]}$}

    \addplot [black, very thick] table[x=x,y=virtual_value_cut02] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{$\mathrm{IRON}_{[0,0.2]}$}

    \end{axis}
    \end{tikzpicture}
    }
    \caption{ 
    The figure illustrates the truncated ironing procedure. The distribution $F$ used is a mixture of two truncated normals on $[0,2]$ with parameters $(0.1,0.04)$ and $(1.9,1.8)$ and respective weights $0.8$ and $0.2$.  (a) The figure shows the initial $J$ function (in blue) and the convex envelopes of this function on different intervals: $F^{-1}(0.2)$, $F^{-1}(0.5)$ and $F^{-1}(2)$.  (b) The figure shows the induced virtual value function before ironing and by ironing on three subintervals: $0.2$, $0.5$ and $2$.} 
    \label{fig:ironing_operator}
    \end{figure}


\subsection{Main Result}\label{sec:main}

If the distribution $F$ does not admit a density that is positive everywhere in the support, the classical Myerson ironing procedure is not applicable since it relies on the existence of the inverse $F^{-1}$. In this case, there exists a more general virtual value characterization developed by \citet{monteiro2010optimal} that is still applicable. That characterization is difficult to work with because it involves generalized convex hulls, rather than the standard convexification used by Myerson. We defer the presentation and discussion of how to use this complex machinery until Section \ref{sec:technical_work}. We are now ready to state the main result of the paper, which states that if the value distributions are regular, then an ironing procedure that has the same complexity as Myerson does apply. 


\begin{theorem}\label{thm:main}
Let $F_i$ be distributions satisfying Assumption \ref{ass:regular}. Then, there exists a direct mechanism that is revenue-maximizing. In this mechanism, given reported values $\hat{v}_i$, the seller allocates the good to the buyer with the highest non-negative value of $\bar{\varphi}^i_{\gamma_i, s_i}(\hat{v}_i)$, where the function $\bar{\varphi}^i_{\gamma_i, s_i}(\hat{v}_i)$ is defined as:
\begin{equation}
\label{eq:ironed-vv} 
\bar{\varphi}^i_{\gamma_i, s_i}(v) = 
\begin{cases}
    \mathrm{IRON}_{[0, s_i]}[\gamma_i F_i](v), & \text{if } a \leq v < s_i, \\
    \varphi_{F_i}(T_i), & \text{if } s_i \leq v < T_i, \\
    \varphi_{F_i}(v), & \text{if } T_i \leq v \leq b.
\end{cases}
\end{equation}
for every $v \in [a_i, b_i]$. Furthermore, the winning bidder pays  the minimum amount they would need to bid to still win.
The constants $(T_i)_{i \in \{1, \ldots, n\}}$ are defined in \Cref{prop:from_F_to_feasible_Fs}, and the operator $\mathrm{IRON}$ is as specified in Section \ref{sec:ironing}.
\end{theorem}
We present the key technical arguments required to proof \Cref{thm:main} in \Cref{sec:technical_work}.


\Cref{thm:main} above states that $\bar\varphi^i_{\gamma_i,s_i}$ is the correct notion of ironed virtual value function given posterior beliefs $F_{\gamma_i,s_i}^i$. Before the signal $s_i$, the correct pre-ironing virtual value is given by $\mathrm{IRON}_{[0, s_i]}[\gamma_i F_i]$, which might require ironing, but where ironing can be done using Myerson's classical approach but with the domain truncated to $[0,s_i]$. Immediately after the signal, we need to iron out a segment $[s_i,T_i]$ of the virtual value to account for the mass at $s_i$. After $T_i$, the original virtual value function $\varphi_{F_i}$ applies. 

The theorem can be interpreted as a near-decomposition result. Ironing the section strictly before the signal yields $\mathrm{IRON}_{[0, s_i]}[\gamma_i F_i]$ while ironing the virtual value from $s_i$ (inclusive) onward yields the second and third clauses of Eq. \eqref{eq:ironed-vv}.
We call this a near-decomposition, not a full decomposition, because $T_i$ creates a link between the two sides, as the value of $T_i$ depends on the distribution before the signal.

The key assumption that enables this near-decomposition is the regularity of $F_i$. The next example shows that if $F_i$ is irregular, then Theorem \ref{thm:main} may fail.

\begin{example}
    Consider the distribution $F$ putting a $0.8$ weight on a truncated normal on $[0.5,0.52]$ with mean $0.51$ and std $0.05$, and a $0.2$ weight on the Uniform over $[0,1]$. We note that this distribution is not regular. In \Cref{fig:counter_example}, we compare the value of $\mathrm{IRON}_{[0,s]}[\gamma F]$ and the actual generalized ironed virtual value of  $F_{\gamma,s}$  computed using the method described in \Cref{sec:technical_work}, for $s = 0.53$ and $\gamma = 0.9$.
\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[scale = 0.65]
    \begin{axis}[
        width=10cm,
        height=10cm,
        xmin=0.5,xmax=0.55,
        ymin=0.4,ymax=0.55,
        table/col sep=comma,
        xlabel={$v$},
        ylabel={virtual value},
        grid=both,
        legend pos=north west
    ]
    
    \addplot [black,  line width = 0.7 mm,unbounded coords=jump] table[x=x,y={virtual_value}] {Data/counter_example.csv};
    \addlegendentry{Ironed virtual value}

    \addplot [red,  line width = 0.7 mm,unbounded coords=jump] table[x=x,y={virtual_value_pre_s},restrict expr to domain={\thisrow{x}}{0:0.531}] {Data/counter_example.csv};
    \addlegendentry{$\mathrm{IRON}_{[0,s]}(\gamma F)$}
    
    \draw[blue, dashed, thick] (axis cs:0.53, 0.4) -- (axis cs:0.53, 0.5);
    \filldraw[blue] (axis cs:0.53,0.4) circle (2pt) node[anchor=south west]{\footnotesize $s=0.53$};

    \end{axis}
    \end{tikzpicture}
    \caption{\textbf{Numerical counter-example to the near-decomposition property without regularity.}}
    \label{fig:counter_example}
\end{figure}

\Cref{thm:main} claims that the generalized ironed virtual value of  $F_{\gamma,s}$ should be equal to $\mathrm{IRON}_{[0,s]}[\gamma F]$ for every $v < s$. However, \Cref{fig:counter_example} demonstrates that this statement does not hold in our example. This figure shows that when $F$ is not regular, the ironing procedure cannot independently be executed on the intervals $[0,s]$ and $[s,1]$ as described in \Cref{thm:main}. Intuitively, when $F$ is not regular, $s$ may lie in a region that already required ironing under the prior distribution $F$. Consequently, when considering the posterior distribution $F_{\gamma,s}$ the values before and after $s$  be taken into account to properly compute the ironed virtual value around $s$. 
\end{example}


It is useful to see what Theorem \ref{thm:main} implies for some simple distributions. If $F$ is a uniform [0,1] distribution, then the virtual value is given by:
  \begin{equation*} \bar \varphi_{F_{\gamma,s}}(v) =   \begin{cases}
     2v - 1/\gamma, &\hbox{ for } v < s,\\
     2T - 1, &\hbox{ for } s \leq v < T,\\
     2v - 1, &\hbox{ for } v \leq  T.\\
 \end{cases}\end{equation*}
 If $F$ is an exponential distribution, then ironing might be required to the left of the signal. Note that the exponential distribution is not only a regular distribution, but satisfies the even stronger condition of monotone hazard rate. Despite this, the pre-signal distribution still sometimes requires ironing (see \Cref{fig:illustration_theorem1}).

    \begin{figure}[h]
    \centering
    \subfigure[Exponential prior $(\lambda = 1), \gamma = 0.95$]{
    \begin{tikzpicture}[scale = 0.65]
    \begin{axis}[
        width=10cm,
        height=10cm,
        xmin=0,xmax=6.5,
        ymin=-4,ymax=6,
        table/col sep=comma,
        xlabel={$v$},
        ylabel={virtual value},
        grid=both,
        legend pos=north west
    ]

    \addplot [blue, dashed,  thick,unbounded coords=jump] table[x=x,y={preiron_s=5}] {Data/virtual_value_gamma=095_exponential.csv};
    \addlegendentry{Unironed (s=5)}
    
    \addplot [red,  thick,unbounded coords=jump] table[x=x,y={s=5}] {Data/virtual_value_gamma=095_exponential.csv};
    \addlegendentry{Ironed (s=5)}
    \addplot[only marks, red, mark=*,forget plot] coordinates {(5, 4.952221754226520)};
    \end{axis}
    \end{tikzpicture}
    }
    \subfigure[Uniform prior, $\gamma = 0.75$]{
    \begin{tikzpicture}[scale = 0.65]
    \begin{axis}[
        width=10cm,
        height=10cm,
        xmin=0,xmax=1,
        ymin=-1.5,ymax=1.5,
        table/col sep=comma,
        xlabel={$v$},
        ylabel={virtual value},
        grid=both,
        legend pos=north west
    ]
    


    \addplot[domain=0:0.4,samples=50,thick,dashed,blue] {2*x - 1/0.75};  % For x < s
    \addplot[domain=0.4:1,samples=50,thick,dashed,blue,forget plot] {2*x - 1};        % For x > s
    \addlegendentry{Unironed (s=0.4)}

    \addplot [red,  thick,unbounded coords=jump] table[x=x,y={s=0.4}] {Data/virtual_value_gamma=075_uniform.csv};
    \addlegendentry{Ironed (s=0.4)}
    \addplot[only marks, red, mark=*,forget plot] coordinates {(0.41, 0.25170764)};
   

    \end{axis}
    \end{tikzpicture}
    }
    \caption{\textbf{Ironed virtual value for different priors.} In each plot the unironed virtual value corresponds to the naive evaluation $\varphi_{F_{\gamma,s}}$, wherever it is well defined (i.e., everywhere but at $s$). The ironed virtual value corresponds to the virtual value characterized in \Cref{thm:main}.}
     \label{fig:illustration_theorem1}
    \end{figure}