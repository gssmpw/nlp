\section{Key Technical Arguments}
\label{sec:technical_work}
In this section we present the key technical arguments needed to prove \Cref{thm:main}. We first describe the family of semi-infinite dimensional problems developed in \citet{monteiro2010optimal} to characterize the ironed virtual value for arbitrary distributions. We then solve this family of problems to obtain our closed-form solution.

\subsection{Ironing for Arbitrary Distributions}
\label{sec:gen_ironing}
Let $F$ be a regular distribution which admits a positive density $f$ on its support. 
For any $\gamma \in (0,1)$ and any $s$ in the support of $F$, recall the definition of the post-signal distribution $F_{\gamma,s}$ defined in Eq.~\eqref{eq:cumulative-F}.
We note that the post-signal distribution does not admit a density at $v = s$. In this setting, the virtual value function used to iron in the Myerson sense (see Section \ref{sec:ironing}), and which is defined for every distribution $F$ with positive density on its support
is not well-defined. In what follows, we present the formalism developed in \citet{monteiro2010optimal} to characterize the optimal auction for general distributions. This formalism generalizes Myerson's characterization.

For every distribution $F$ (which does not need to have a density), we define for every $x \in [a,b]$ the function
\begin{equation*}
H_{F}(x) = \int_{a}^x t  dF(t) - \int_a^x (1-F(t))dt.
\end{equation*}
Fix $t \in [a,b]$. For every $x \in [a,t]$, we define the generalized convex hull of $H_F$ as,
\begin{subequations}
\label{eq:gen_virtual_value}
\begin{alignat}{2}
\Psi_{F}^t(x) = \; &\!\sup_{\alpha,\beta \in \mathbb{R}} &\;& \alpha + \beta \cdot F(x) \\
&\text{s.t.} &      &  \alpha + \beta \cdot F(y) \leq H_{F}(y) \quad \forall y \in [a,t]. 
\end{alignat}
\end{subequations}
Let $\partial \Psi_{F}^t(x)$ be the generalized sub-differential of $\Psi_{F}^t$ at $x$ defined as the set of $\beta \in \mathbb{R}$ such that
\begin{equation}
\label{eq:subgradient}
\Psi_{F}^t(z) \geq \Psi^t_{F}(x) + \beta \cdot (F(z) - F(x)) \quad \text{for every $z \in [a,t]$}.  
\end{equation}
Equivalently (see Section 2 of \citet{monteiro2010optimal}), one has that
\begin{equation}
    \label{eq:subgrad_are_solutions}
    \partial \Psi_{F}^t(z) = \{ \beta \in \mathbb{R} \text{ s.t. there exists $\alpha \in \mathbb{R}$ such that $(\alpha,\beta)$ is optimal for \eqref{eq:gen_virtual_value}} \}.
\end{equation}
Furthermore, let $\ell^t_{F}(x) = \inf \partial \Psi^t_{F}(x)$ and $s^t_{F}(x) = \sup \partial \Psi^t_{F}(x)$\footnote{Note that we will drop dependencies in $t$ when $t = b$, as $\Psi_F^b$ corresponds to the generalized convex hull of $H_F$ on the whole domain $[a,b]$.
}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[every text node part/.style={align=center},transform shape,]
    \begin{axis}[
        width=10cm,
        height=8cm,
        xmin=-0.,xmax=2.0,
        ymin=-0.3,ymax=0.01,
        scaled y ticks={base 10:2},
        table/col sep=comma,
        xlabel={$v$},
        ylabel={Negative Revenue},
        grid=both,
        legend pos=south east
    ]

    \addplot [blue, dashed, line width=.7mm] table[x=x,y=H] {Data/ironing_example_mix_truncated_normals.csv};
    \addlegendentry{Before ironing}


    \addplot [black, line width=.5mm] table[x=x,y=psi] {Data/ironing_example_mix_truncated_normals.csv} ;
    \addlegendentry{Ironed curve}


 \addplot [violet, line width=.3mm] table[x=x,y expr={-1.22+1.2*\thisrow{F}}] {Data/ironing_example_mix_truncated_normals.csv} node[pos = 0.7,below right] {\footnotesize $y = -1+1.2  F(v)$};



    \addplot [red, line width=.3mm] table[x=x,y expr={-0.03-0.2*\thisrow{F}}] {Data/ironing_example_mix_truncated_normals.csv} node[pos = 0.7,below left] {\footnotesize $y = -0.03-0.2  F(v)$};


   

    \addplot [teal, line width=.3mm] table[x=x,y expr={-0.044-0.08*\thisrow{F}}] {Data/ironing_example_mix_truncated_normals.csv} node[pos = 0.3,below] { \footnotesize $y = -0.044-0.08  F(v)$};

    \end{axis}
    \end{tikzpicture}
    \caption{The figure illustrates the ironing procedure defined by \citet{monteiro2010optimal}. The distribution $F$ used is mixture of two truncated normals on $[0,2]$ with parameters $(0.1,0.04)$ and $(1.9,1.8)$ and respective weights $0.8$ and $0.2$. Instead of the standard convexification in quantile space, \cite{monteiro2010optimal} perform a generalized convexification in the value space where affine functions of $F$ are used to iron the revenue curve.}
    \label{fig:monteiro}
    \end{figure}


\citet{monteiro2010optimal} show that the mapping $\ell_F$ generalizes the notion of ironed virtual value functions for distributions which do not necessarily have a positive density. Figure \ref{fig:monteiro} shows an example of this kind of ironing works via generalized convexification in value space. In particular, they show that when $F$ admits a positive density on its support, $\ell_F$ is equal to the usual Myerson ironing operator $\mathrm{IRON}_{[a,b]}[F].$ Our next result extends this result to the truncated ironing operator.

\begin{proposition}
    \label{prop:Myerson_and_Monteiro}
    Let $F$ be a distribution with positive density on $[a,b]$. Then, for every $t \in [a,b]$, $\ell_F^t = \mathrm{IRON}_{[a,t]}[F]$. 
\end{proposition}



\if false
     \begin{figure}
    \centering
    \begin{tikzpicture}[every text node part/.style={align=center},transform shape,]
    \begin{axis}[
        width=10cm,
        height=8cm,
        xmin=-0.,xmax=1.0,
        ymin=-0.7,ymax=0.01,
        table/col sep=comma,
        xlabel={$v$},
        ylabel={Negative Revenue},
        grid=both,
        legend pos=south east,
        legend style ={font ={\tiny}}
    ]

    \addplot [blue, dashed, line width=.7mm,unbounded coords=jump] table[x=x,y=H] {Data/iron_uniform_with_hal_s=04_gamma=075.csv};
    \addlegendentry{Before ironing}

    \addplot [red, line width=.6mm, unbounded coords=jump]  table[x=x, y=psi, restrict expr to domain={\thisrow{x}}{0.625:1}]{Data/iron_uniform_with_hal_s=04_gamma=075.csv};
    \addlegendentry{Ironed}
    


    \addplot [red, line width=.6mm, unbounded coords=jump]  table[x=x, y=psi, restrict expr to domain={\thisrow{x}}{0.4:1}]{Data/iron_uniform_with_hal_s=04_gamma=075.csv};



    \addplot [violet, line width=.3mm,unbounded coords=jump] table[x=x,y expr={-0.36+0.25*\thisrow{F}}] {Data/iron_uniform_with_hal_s=04_gamma=075.csv};
    
    
    \filldraw[black] (axis cs:0.625,-0.7) circle (2pt) node[anchor=south east]{$T$};
    
    \end{axis}
    \end{tikzpicture}
    \end{figure}


        \begin{figure}[h]
    \centering
    \begin{tikzpicture}[every text node part/.style={align=center},transform shape,]
    \begin{axis}[
        width=10cm,
        height=8cm,
        xmin=-0.,xmax=7.0,
        ymin=-0.7,ymax=0.01,
        table/col sep=comma,
        xlabel={$v$},
        ylabel={Negative Revenue},
        grid=both,
        legend pos=south east,
        legend style ={font ={\tiny}}
    ]

    \addplot [blue, dashed, line width=.7mm,unbounded coords=jump] table[x=x,y=H] {Data/iron_exp_hal_s=5_gamma=095.csv};
    \addlegendentry{Before ironing}


    

     \addplot [red, line width=.6mm, unbounded coords=jump]  table[x=x, y=psi]{Data/iron_exp_hal_s=5_gamma=095.csv};%\addlegendentry{Ironed}}
     \addlegendentry{Ironed}




    \addplot [teal, line width=.3mm,unbounded coords=jump] table[x=x,y expr={-0.87+0.62*\thisrow{F}}] {Data/iron_exp_hal_s=5_gamma=095.csv};



    
    \end{axis}
    \end{tikzpicture}
    \caption{Revenue curve before and after ironing for the exponential distribution with $\lambda = 1$, signal $s=5$ and $\gamma = 0.5$. The figure also shows the affine function of $F$ that is used to ``convexify'' the value segment just before the signal $s$.}
    \label{fig:convexified-revenue}
    \end{figure}
%\end{frame}
\fi


We note that while \citet{monteiro2010optimal} provide a structural result about the general ironed virtual value function, one still needs to solve in general infinitely many semi-infinite optimization problems to be able to implement the optimal auction. In what follows, we characterize we solve Problem \eqref{eq:gen_virtual_value} for our model.

\subsection{Outline of the proof of \Cref{thm:main}}
\label{sec:outline}
Fix a regular distribution $F$ with positive continuous density $f$ on its support $[a,b]$.
The generalized convex hull of $H_{F_{\gamma,s}}$ is defined as,
\begin{subequations}
\begin{alignat}{2}
\Psi_{F_{\gamma,s}}(x) = \; &\!\sup_{\alpha,\beta \in \mathbb{R}} &\;& \alpha + \beta \cdot F_{\gamma,s}(x) \nonumber \\
&\text{s.t.} &      &  \alpha + \beta \cdot F_{\gamma,s}(y) \leq H_{F_{\gamma,s}}(y) \quad \forall y \in [a,b]. \nonumber
\end{alignat}
\end{subequations}

By expressing $F_{\gamma,s}$ and $H_{F_{\gamma,s}}$ as a function of $F$, $H_F$, $\gamma$ and $s$ (see \Cref{lem:F_and_H}), we obtain the following equivalent expression for $\Psi_{F_{\gamma,s}}$. For every $x$ we have that,
\begin{subequations}\label{eq:F_gamma_after_s}
\begin{alignat}{2}
\Psi_{F_{\gamma,s}}(x) = \; &\!\sup_{\alpha,\beta \in \mathbb{R}} &\;& \alpha  + \beta \cdot \gamma \cdot F(x) + \mathbbm{1} \{ x \geq s \} \cdot \beta \cdot (1-\gamma) \\
&\text{s.t.} &      &  \alpha + \beta \cdot \gamma \cdot F(y) \leq \gamma \cdot H_{F}(y) - (1-\gamma) \cdot y \quad \forall y < s. \label{eq:constraint_pre_s} \\ 
&  &      &  \alpha + \beta \cdot (1-\gamma) + \beta \cdot \gamma \cdot F(y) \leq \gamma \cdot H_{F}(y) \quad \forall y \geq s. \label{eq:constraint_post_s}
\end{alignat}
\end{subequations}
%To prove our result, we first characterize the solutions of Problem \eqref{eq:F_gamma_after_s} by constructing a threshold $T$ such that for every $x \geq T$, solutions of  Problem  \eqref{eq:gen_virtual_value} can be related to the ones of  Problem \eqref{eq:F_gamma_after_s}.

To prove \Cref{thm:main}, we aim to relate $\ell_{F_{\gamma,s}}$ to $\ell_{\gamma F}^s$ on the interval $[a,s)$ and $\ell_{F_{\gamma,s}}$ to $\ell_{F}$ on the interval $[s,b]$. Then, by applying \Cref{prop:Myerson_and_Monteiro}, we obtain the desired expression.\\

\noindent \textbf{Key proof technique.} To establish this result, we first prove that the generalized virtual value functions we consider are well-behaved on every interval which does not include $s$. We prove more generally the following result on the regularity of the generalized virtual value function.
\begin{lemma}
    \label{lem:continuity}
    Let $I$ be an interval included in $[a,b]$. Assume that $G$ admits a density $g$ that is positive and continuous on $I$. Then, $\ell_{G}$ is continuous on $I$.
\end{lemma}
Given a distribution $G$, recall that $\ell_{G}$ is the lowest generalized sub-gradient of the function $\Psi_G$ which is itself the generalized convex hull of the function $H_{G}$. Therefore, \Cref{lem:continuity} extends the statement that ``the convex hull of a differentiable function of one variable is continuously differentiable'' to our generalized notions of convexity and differentials. 

% From the expression of ${F_{\gamma,s}}$ derived in \Cref{lem:F_and_H}, and by the assumption that $F$ admits a positive and continuous density on $[a,b]$, we have that the distribution ${F_{\gamma,s}}$ admits a positive and continuous density on any interval which does not admit $s$. Hence, \Cref{lem:continuity} implies that the generalized virtual value $\ell_{F_{\gamma,s}}$


In turn, the key argument to prove that two distributions of interest $F$ and $G$ have the same virtual value function on some interval consists in first establishing the continuity of $\ell_F$ and $\ell_G$ by using \Cref{lem:continuity}. We then prove that $\ell_F$ is a generalized sub-gradient of $\Psi_G$ on the whole interval and conclude applying the following lemma.
\begin{lemma}
\label{lem:inclusion_to_eq}
Let $F$ and $G$ be two distributions on $[a,b]$, and let $I$ be an interval included in $[a,b]$. If $\ell_F(x) \in \partial \Psi_{G}(x)$ for all $x \in I$, and if $\ell_F$ and $\ell_G$ are continuous on $I$, then $\ell_F = \ell_G$ on $I$.
\end{lemma}
We next show how we relate the generalized virtual value functions of the distributions of interest on the intervals $[a,s)$ and $[s,b]$.\\

\noindent \textbf{Analysis on the interval $[s,b]$.}
We first prove that for some $T$ (defined in \Cref{prop:from_F_to_feasible_Fs}) we have that $\ell_F = \ell_{F_{\gamma,s}}$ over the interval $[T,b]$. As discussed previously, we establish this result by leveraging \Cref{lem:inclusion_to_eq}. Hence, it is sufficient to prove that $\ell_{F}(x) \in \partial \Psi_{F_{\gamma,s}}$ for every $x \in [T,b]$. We note that the definition of the generalized differential presented in \eqref{eq:subgrad_are_solutions} implies that $\ell_{F}(x) \in \partial \Psi_{F_{\gamma,s}}$ if and only if there exists an optimal solution for Problem \eqref{eq:F_gamma_after_s} where $\beta = \ell_F(x)$. In what follows, we construct such a solution.


Let $x \in [s,b]$ and remark that \eqref{eq:subgrad_are_solutions} implies that there there exists $(\aF,\bF)$ such that $\bF = \ell_{F}(x)$ which is optimal for Problem \eqref{eq:gen_virtual_value}. We define our related candidate solution for Problem \eqref{eq:F_gamma_after_s} as,
\begin{equation}
\label{eq:candidate}
(\aFs,\bFs) = (\gamma \cdot \aF - (1-\gamma) \cdot \bF, \bF).
\end{equation}
A critical aspect of the construction in \eqref{eq:candidate} is that $\bFs = \bF = \ell_{F}(x)$. Therefore, proving optimality of $(\aFs,\bFs)$ for Problem \eqref{eq:F_gamma_after_s} implies that $\ell_F(x) \in \partial \Psi_{F_{\gamma,s}}$.

A straightforward algebraic manipulation allows us to show that for every $x \in [s,b]$ the couple $(\aFs,\bFs)$ satisfies the constraint \eqref{eq:constraint_post_s} for every $y \geq s$.
However, the constraints \eqref{eq:constraint_pre_s} are not necessarily satisfied for all $x \in [s,b]$. We define the threshold $T$ such that $(\aFs,\bFs)$ satisfies the constraint \eqref{eq:constraint_pre_s} for all $y < s$.  
To that end, we define the following auxiliary mapping. For every $y \leq s$, let $\mu_y$ be defined as,
\begin{equation*}
\mu_y(x) = \aFs + \gamma \cdot \bFs \cdot F(y) - \gamma \cdot H_{F}(y) + (1-\gamma) \cdot y \quad \text{for every $x \in [s,b]$.}
\end{equation*}
This definition, implies that $(\aFs,\bFs)$ satisfies the constraint \eqref{eq:constraint_pre_s} at a given $y$ if and only if, $\mu_{y}(x) \leq 0$.  Consequently, the feasibility of $(\aFs,\bFs)$ for Problem \eqref{eq:F_gamma_after_s} reduces to the analysis of the sign of $\mu_y$. Our next result provides structural properties about $\mu_y$.
\begin{lemma}\label{lem:prop_mu}
\,
\begin{enumerate}
\item[(i)] $\mu_y$ is non-increasing for every $y \leq s$.
\item[(ii)] If $y > y'$, then for every $x \in [s,b]$, $\mu_{y}(x) > \mu_{y'}(x)$.
\item[(iii)] $\mu_s(s) > 0$ and $\mu_s(b) \leq 0$.
\end{enumerate}
\end{lemma}
\Cref{lem:prop_mu} implies, by property $(ii)$, that for every $(\aFs,\bFs)$ the most stringent constraint \eqref{eq:constraint_pre_s}  is for $y =s$. Furthermore, property $(i)$ implies that if $(\aFs,\bFs)$ satisfies \eqref{eq:constraint_pre_s} for a given $y$ and a given $x$ then for all $x' \geq x$, $(\aFs[x'],\bFs[x'])$ also satisfies \eqref{eq:constraint_pre_s} at $y$. By using these results, we construct a threshold $T$ such that the $(\aFs,\bFs)$ is feasible for all $x \geq T$. More generally, we prove the optimality of $(\aFs,\bFs)$ for Problem \eqref{eq:F_gamma_after_s} and establish the following result.
\begin{lemma}
\label{prop:from_F_to_feasible_Fs}
There exists $T \in (s,b]$ such that $\mu_s(T) = 0$. Furthermore, for every $x \in [T,b]$, we have that $\ell_{F}(x) \in \partial \Psi_{F_{\gamma,s}}(x)$.
\end{lemma}
Combining \Cref{lem:continuity}, \Cref{lem:inclusion_to_eq} and \Cref{prop:from_F_to_feasible_Fs} we conclude that $\ell_F = \ell_{F_{\gamma,s}}$ on $[T,b]$. We complete the proof on the interval $[s,b]$ by showing that $\ell_{F_{\gamma,s}}$ is constant on $[s,T]$.\\

\noindent \textbf{Analysis on the interval $[a,s)$.}
On this interval, we show that $\ell_F=\ell_{\gamma F}^s$, where $\ell_{\gamma F}^s$ is defined as the smallest generalized sub-gradient of the function, defined for every $x \in [a,s)$ as
\begin{subequations}\label{eq:main-gammaF}
\begin{alignat}{2}
\Psi^s_{\gamma F}(x) = \; &\!\sup_{\alpha,\beta \in \mathbb{R}} &\;& \alpha +  \beta \cdot \gamma \cdot F(x) \\
&\text{s.t.} &      &  \alpha + \beta \cdot \gamma \cdot F(y) \leq \gamma \cdot H_{F}(y) - (1-\gamma) \cdot y \quad \forall y \in [a,s]. %\label{eq:constraint_gF}
\end{alignat}
\end{subequations}
We note that for every $x \in [a,s)$, Problem \eqref{eq:main-gammaF} is a relaxation of Problem \eqref{eq:F_gamma_after_s} in which we removed the constraint \eqref{eq:constraint_post_s}. The main argument consists in proving that the relaxation is tight in the sense that the value of both problems is the same. 
In particular, we establish that for every $x \in [a,s)$, either $\ell_{F_{\gamma,s}}(x) = \ell_{F_{\gamma,s}}(s)$ or $\ell_{F_{\gamma,s}}(x) \in \partial \Psi_{\gamma F}^s(x)$. By \Cref{lem:inclusion_to_eq} we then conclude that $\ell_{F_{\gamma,s}}(x) \in \{\ell_{F_{\gamma,s}}(s), \ell_{\gamma F}^s(x)\}.$
Using a continuity argument, we conclude that $\ell_{F_{\gamma,s}}$ must equal $\ell_{\gamma F}^s$ on the whole interval $[a,s)$.





The complete proof of \Cref{thm:main} is presented in \Cref{sec:apx_main_proof}.

    