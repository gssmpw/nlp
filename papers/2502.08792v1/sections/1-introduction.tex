\section{Introduction}\label{sec:intro}

In this work, we study the Bayesian auction design problem where a seller aims to design a revenue-maximizing mechanism to sell an indivisible good to $n$ buyers. In the classical version of the problem \citep{myerson1981optimal}, each buyer's private value is independently drawn from a prior distribution, which is common knowledge to all agents. A classical feature of this problem is information asymmetry: while buyers know their own private values, the seller has no direct access to this information beyond the prior. However, in many practical applications of mechanism design, such as in advertising auctions, the seller often does possess additional information about buyers' private values. In particular, sellers can train a machine learning models to predict buyers' valuations. To do so, they can often rely on a wealth of data: past interactions with the same buyer, contextual information, and even bids by similar buyers.

However, sellers face significant challenges when using ML predictions to design auctions. One particular challenge is that some of the most advanced prediction systems, such as Large Language Models (LLMs) and deep neural networks more generally, often hallucinate. By hallucinate, we mean that they sometimes generate output that appears to be of high quality but that is in fact uncorrelated to the true quantity of interest. Even worse, such systems typically lack any sort of uncertainty quantification, making it difficult to decide whether the ML output should be used or discarded. They can appear confident in their predictions even when such predictions are completely erroneous. 


Motivated by the increasing importance of such hallucination-prone models in practice, we propose a novel Bayesian framework to explore the design of mechanisms that incorporate ML predictions taking into account the risk the predictions could be hallucinations. Specifically, in our framework, each buyer's private value is independently drawn from a known prior distribution, and the seller observes a signal for each buyer. This signal either equals the buyer's private value or, with some probability, is independently sampled and uncorrelated with the buyer's value. We call such uncorrelated signals  \textit{hallucinations}. 
Our framework stands out due to its ability to model signals generated by machine learning models that do not quantify uncertainty in their predictions and differs from more classical models of signals with statistical error (as discussed in \Cref{sec:noise}). Our paper is inspired by the recent literature on learning-augmented algorithms (see \Cref{sec:lit_review}) but our approach is quite different from prior work. Instead of the two-objective approach common in the computer science literature, we propose using a classical Bayesian framework to analyze our problem. 


For the one buyer case, the optimal auction is easy to interpret. The signal space is broken into three segments, and the seller should post a price according to the realized segment. For intermediate signal values, the seller should \textit{follow} the signal. That is, they should set the price of the item according to the signal. For low signals, the seller should \textit{ignore} the signal. If the prediction is that the buyer's value is low, the seller is better off betting that the signal is a hallucination since pricing low does not help the seller. For high signals, the seller should \textit{cap} the signal. Capping the signal means pricing at a value below the signal, but above the signal-ignoring monopoly price. Capping is a way of benefiting from the high signal while hedging against a hallucination. 

Our main technical contribution consists in characterizing the structure of the optimal signal-revealing direct mechanism when the seller observes signals about the buyers' private values. Signal-revealing refers to the fact that the seller does not try to obfuscate the signals from the agents.  In our setting, the seminal characterization developed by \citet{myerson1981optimal} does not hold because the posterior distribution induced by the signal does not admit a continuous density. Instead, we use the more complex formalism developed in \citet{monteiro2010optimal} to characterize optimal auctions for arbitrary distributions. Their method shows that the ironed virtual value of each buyer can be expressed by solving infinitely many semi-infinite linear optimization problems. In \cref{thm:main} we leverage this formalism to develop a \textit{closed-form} expression of the virtual value of each buyer under the posterior distribution given the signal. The posterior distribution always requires ironing, even if the prior was a regular distribution. In a sense, our main technical result is essentially a near-decomposition of the ironing pre- and post-signal. That is, the ironing for values below and above the signal can be performed nearly independently of each other.




\subsection{Literature Review}\label{sec:lit_review}
  Mechanism design and auction theory have been very active areas of research since at least the 1960s, including the celebrated Vickrey-Clarke-Groves framework for welfare maximization \citep{vickrey1961counterspeculation,clarke1971multipart,groves1973incentives}. 
  \citet{myerson1981optimal} laid the foundation for the literature on revenue maximization, proving many of the results that we build on: revelation principle, the role of the virtual value and the ironing procedure. We also build closely on \citet{monteiro2010optimal}, who developed techniques for ironing virtual values in settings where the priors do not have densities. 
  For general distributions, the complexity of the revenue-maximizing auction derived in \citet{myerson1981optimal} has motivated extensive research into simple and more practical mechanisms that are easier to implement while remaining near-optimal \citep{hartline2009simple,roughgarden2019approximately}. 
  Our work contributes to this literature by modeling a practical setting in which the seller relies on machine learning algorithms that provide hallucination-prone predictions and by studying the design of optimal mechanisms that are robust to such predictive errors.
  

Our work relates to the literature on learning-augmented algorithms, also known as algorithms with predictions/advice in which a decision-maker has access to some prediction with unknown accuracy. The standard goal in this literature is to design algorithms achieving a good trade-off between two performance metrics: consistency, which is the performance if the predictions are perfect, and robustness, which corresponds to the performance when the predictions are adversarial \citep{purohit2018improving,lykouris2021competitive}. This framework has been applied to study several problems across fields. More recently, \citep{agrawal2022learning,balkanski2022strategyproof,gkatzelis2022improved,banerjee2022online,xu2022mechanism} studied learning-augmented algorithms in the context of problems with strategic agents, including mechanism design.  In particular, \cite{xu2022mechanism,balkanski2023online,caragiannis2024randomized} and \citet{lu2024competitive} consider the auction design problem in which the private value of the agents are adversarially chosen and in which the seller observes a prediction with unknown accuracy. The goal of these works is to design a mechanism which performs well both good consistency and robustness. In a similar vein, \citet{balcan2023bicriteria} propose a welfare-efficient mechanism for settings with ML signals which offers a minimum revenue guarantee. Our work is conceptually related to this literature as we also assume that the seller observes a prediction which can be used to infer the values of the buyers. A key modeling distinction is that we consider a fully Bayesian setting in which the values of the buyers are sampled from a known distribution and in which our model for prediction errors assume that the ML algorithm is ``randomly'' wrong, as opposed to adversarially wrong.

Our work also relates to data-driven mechanism design, which leverages a finite set of samples, independently drawn from the buyers' value distribution, to design mechanisms \citep{cole2014sample,gonczarowski2017efficient,guo2019settling}. Closely related is \citet{devanur2016sample}, who assume that the decision-maker observes a signal to infer additional information about the buyers' values. The typical focus in this literature is on sample complexity. Unlike this work, we consider settings where the signal may be hallucination-prone. While prior studies often assume that samples are drawn from true value distributions, recent work has addressed mechanisms that account for potentially corrupted samples \citep{cai2017learning,brustle2020multi,guo2021robust,besbes2022beyond}. Similarly, we study mechanisms robust to prediction errors, including corruption. However, our approach diverges by assuming the seller knows the buyers' value distributions and uses side information to infer realized values, focusing on the behavior of machine learning predictions rather than the data-generation process.


Our work focuses on direct mechanisms where the seller does not attempt to conceal the signals from the agents. One important question that we leave open is whether this assumption is without loss of optimality. \citet{maskin&tirole1990informedprincipal} develop a non-cooperative framework for the principal-agent relationship, focusing on scenarios where the principal has private information. They model this interaction as a three-stage game in which both parties first learn their private types before the principal proposes a contract. These contracts specify actions and transfers while ensuring verifiability to mitigate moral hazard. Unlike \citet{myerson1983informedprincipal}'s cooperative game approach, which examined core nonemptiness under incomplete information, Maskin and Tirole emphasize strategic interaction and the sorting effects of contract proposals. 
\citet{maskin&tirole1990informedprincipal} argue that the principal generally does not incur a loss by withholding information from the agent. 
Therefore, in the case of a single buyer, assuming the signal is publicly observable is without loss of optimality. 
In the case of multiple buyers, it is not immediately clear that the public disclosure of signals is without loss of optimality. However, we consider the benchmark scenario in which signals are publicly released as a necessary first step in isolating the effects of hallucination while controlling for the auctioneer’s potential information manipulation. Furthermore, analyzing the public signal case provides a lower bound on the auctioneer’s achievable payoff. The analysis of privately observed signals falls beyond the scope of this paper and is left for future research.




