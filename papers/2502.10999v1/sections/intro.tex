\section{Introduction}
Diffusion models have become the dominant paradigm in image generation~\cite{ho2020denoising, saharia2022photorealistic, zhang2023adding, rombach2022high, betker2023improving, ramesh2022hierarchical, esser2024scaling}, because of their iterative denoising process that allows fine-grained image synthesis. While these models effectively capture data distributions of photorealistic or artistic images, they still fall short in generating high-fidelity text. Rendering text in images is inherently more challenging as it requires precise knowledge of the geometric alignment among strokes, the arrangement of letters as words, the legibility across varying fonts, sizes, and styles, and the integration of text into visual backgrounds. At the same time, humans are more sensitive to minor errors in text, such as a missing character or an incorrectly shaped letter, compared to natural elements in a visual scene that allow for a much higher degree of variation.

Increasing attention has been paid to visual text rendering~\cite{bai2024intelligent, han2024ace, li2024hfh} due to its high user demands. Instead of relying solely on diffusion models to remember exactly how to render text, recent research is starting to embed the visual attributes of texts, such as glyphs~\cite{tuo2023anytext, liu2024glyph, ma2024glyphdraw2, yang2024glyphcontrol}, as input conditions to diffusion models. However, it is still difficult for users to specify the desired font in the open world, and there remain open challenges that burden the development of font-controllable text rendering:
% \vspace{-1mm}
\begin{itemize}
\item No ground-truth font label annotation is available in the massive training dataset, while synthetic images often fail to accurately mimic subtle details that appear in reality.
% \vspace{-1mm}
\item There are numerous fonts available in the open world, but many fonts with different names are very similar which confounds evaluation.
% \vspace{-2mm}
\item Users like visual designers may want to explore different fonts during their design process, even creating novel fonts of their own. 
% In addition, the user-selected layout locations may not be exactly what is intended.
% \vspace{-1mm}
\item State-of-the-art image generation models are either closed-sourced~\cite{betker2023improving, midjourney} or users still cannot edit texts or fonts without altering other parts of the image.
\end{itemize}

This work aims to address the above challenges. To summarize our contributions, we introduce the simplest and, to our best knowledge, one of the few~\cite{ma2024glyphdraw2, liu2024glyph} open-source methods for rendering visual text with user-controllable fonts. 
We provide code in the hope that others can draw inspiration from the underlying \textbf{data-driven algorithm} and benefit from the \textbf{simplicity in the self-supervised training}.


We also provide the community a comprehensive dataset for font-aware glyph controls collected from diverse real-world images. We further propose a \textbf{quantitative evaluation metrics for handling fuzzy fonts in the open world.} Experimental results demonstrate that our method, ControlText, as a text and font editing model, facilitates a human-in-the-loop process to generate multilingual text with user-controllable fonts in a zero-shot manner.
