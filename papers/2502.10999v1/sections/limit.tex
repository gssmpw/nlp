\section{Limitations}
Our model is based on ControlNet~\cite{zhang2023adding} with a CLIP text embedding model~\cite{radford2021learning}, although modified by AnyText~\cite{tuo2023anytext} to incorporate glyph line information. However, the CLIP-text encoder has relatively limited language understanding capabilities compared to state-of-the-art foundation models. Unlike text itself, this limitation affects the model's ability to accurately render complex artistic visual features or backgrounds, which users might specify in their input prompts, such as asking the text to appear like clouds or flames, that go beyond merely the font information.

Additionally, due to limited training resources, our experiments were conducted using a smaller diffusion model as a proof-of-concept compared to commercial ones. Each epoch requires approximately 380 GPU hours on NVIDIA V100 GPUs with 32 GB of memory, but we anticipate significantly improved efficiency on newer hardware and with a larger memory. This constraint may result in suboptimal inpainting of background regions within the text area, as well as instability in the quality of rendered text. The users also have limited controls of background pixels behind the text. 

Some sacrifice in text quality is observed for non-Latin languages on the AnyText-Benchmark in exchange for improved font controllability.

The embedding layers of the glyph controls can also lead to reduced text quality, especially when the text in a font is very small, thin, or excessively long. In such cases, fine details of the font information in the glyphs may be lost. 

As with all other text-to-image algorithms that rely on diffusion models, our approach requires a certain number of denoising steps to generate a single image at inference. End-to-end transformer-based models~\cite{xie2024show} may improve the time efficiency of the generation process.


% The entire pipeline also requires a user-friendly front-end interface to allow users to create glyph control images efficiently, especially working with more complex text layouts or curved typography effects. However, as long as users specify the input glyph controls, our ControlNet is already prepared to follow the pixel-level information to render the image with the required text and font. 


\section{Ethical Impact}
This work is intended solely for academic research purposes. While our algorithm allows users to generate images with customized text, there is a potential risk of misuse for producing harmful or hateful content or misinformation. However, we do not identify any additional ethical concerns compared to existing research on visual text rendering.