%%%%%%%%% ABSTRACT
\begin{abstract}
   This work demonstrates that diffusion models can achieve \textit{font-controllable} multilingual text rendering using just raw images without font label annotations.
   Visual text rendering remains a significant challenge. While recent methods condition diffusion on glyphs, it is impossible to retrieve exact font annotations from large-scale, real-world datasets, which prevents user-specified font control.
   To address this, we propose a data-driven solution that integrates the conditional diffusion model with a text segmentation model, utilizing segmentation masks to capture and represent fonts \emph{in pixel space} in a \textit{self-supervised} manner, thereby eliminating the need for any ground-truth labels and enabling users to customize text rendering with any multilingual font of their choice. The experiment provides a proof of concept of our algorithm in zero-shot text and font editing across diverse fonts and languages, providing valuable insights for the community and industry toward achieving generalized visual text rendering. 
   
   % This work demonstrates that diffusion models can achieve font-controllable multilingual text rendering using just raw images in the open world without font annotations. Visual text rendering remains a significant challenge for diffusion models due to the intricate structure of textual strokes they must learn from data. While recent works use diffusion models conditioned on glyphs to guide the generation process, it is impossible to retrieve exact font annotations from large-scale, real-world datasets, which prevents these algorithms from allowing users to specify fonts for the rendered texts. To address this, we propose a data-driven solution that integrates the conditional diffusion model with a text segmentation model, utilizing segmentation masks to capture and represent fonts \emph{in pixel space}  in an unsupervised manner, thereby eliminating the need for any ground-truth labels. A text recognition model also helps validate the fidelity of these conditional maps, ensuring accurate text representation. We present this algorithm as part of a user-in-the-loop copilot tool for real-world design tasks, supporting existing and user-designed fonts across multiple languages. The experiment provides a proof of concept of our algorithm in zero shot visual text generation and editing across diverse fonts and languages, providing valuable insights for the community and industry toward achieving generalized visual text rendering in images. Code will be made available at \href{https://github.com/bowen-upenn/ControlText}{github.com/bowen-upenn/ControlText}.
\end{abstract}