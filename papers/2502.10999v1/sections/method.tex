\section{Technical Approach}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/flows.pdf}
    \caption{System overview. It consists of two parts (1) Training pipeline: text segmentation masks are extracted as glyph controls from a large image dataset without ground-truth font annotations. Low-quality masks are filtered out using an OCR model, and random perturbations are applied to prevent the model from overfitting to exact pixel locations of the glyphs. (2) Inference pipeline: users upload images, specify text regions, and provide any desired font file through the user front-end. The model generates an image patch with the rendered text, which is then seamlessly blended into the original image. Throughout this figure, models marked with a fire icon indicate trainable weights, while those marked with a snowflake icon are frozen.}
    \label{fig:flow}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/eval.pdf}
    \caption{Evaluation pipeline: the cropped regions of the generated text and the input glyph are processed by a pretrained font classification model, which may not have seen the user-specified font. The proposed $l_2@k$ and $\cos@k$ metrics for fuzzy fonts assume that similar fonts have similar output probability vectors, while we retain only top-$k$ values while zeroing out the rest.}
    \label{fig:eval}
\end{figure}


We envision this method being used as a modular plug-in for existing text-to-image generation frameworks. It works with images generated by any base models or actual photos. For instance, when incorrect text is generated, or the user wants to replace some text or modify its font, our algorithm can be specifically targeted to these localized regions without altering remaining parts in images. By leveraging a human-in-the-loop approach, the model aims to render controllable visual text within the user-specified region, perform background inpainting, and blend the modified region back into the original image, regardless of its original size.


\subsection{Data-Driven Insights}

ControlText employs a data-driven approach to unlock visual text rendering with user-controllable fonts without relying on complex architectural designs, aiming to avoid pitfalls associated with the bitter lesson~\cite{sutton2019bitter} in foundation models. We feed the model a massive and diverse amount of unsupervised glyphs, each containing detailed font features in pixel space, and train the diffusion process to reconstruct target images following the explicit pixel-level hints in these input glyphs.

The key insight is that during training, the model learns to leverage pixel-level controls provided by the glyphs as a shortcut for generating text within images. Importantly, the glyphs no longer need to use a standard font, but can mimic any font appear in the target images. Unlike traditional methods, no annotations of font labels are required; the only guidance comes from how the text appears in pixel space as depicted by the input glyphs.

In inference, \textbf{the model should have seen a diverse set of glyphs during training, including intricate font features represented by pixel details near the textual edges in the glyphs.} With this information, it can render unseen languages or unfamiliar text without requiring prior knowledge of how to write the text from scratch, how to arrange individual letters or characters, or understanding their semantic meaning. \textbf{The model just treats text as a collection of pixels rather than linguistic entities.} This self-supervised data-driven approach not only enhances the model's generalizability to open-world scenarios, but also ensures scalability when more image data, computation, and larger base models become available.


\subsection{Training Pipeline}
\subsubsection{Collection of Font-Aware Glyphs} \label{sec:prepare_glyph}
Our training pipeline begins with the collection of glyph controls by performing text segmentation on images. We use TexRNet~\cite{xu2021rethinking} as our text segmentation algorithm to identify text regions and provide fine-grained masks, preserving intricate features of different fonts in pixel space, It also provides bounding boxes that will serve as position controls~\cite{tuo2023anytext}. 
% \cite{xu2021rethinking} provides masks for words, as well as word effects like shadows or stereo effects over background objects. We on-purposely exclude masks for word effects, as we want to avoid requiring users to manually sketch these effects, but rely on the diffusion models to infer and render them automatically. 
The segmentation algorithm is a pre-trained deep-learning-based model, so it may occasionally miss masks for certain letters or parts of letters. As a result, we introduce an OCR model, specifically PaddleOCR-v3~\cite{du2020pp}, into the pipeline following the segmentation process. The OCR model validates the detected text by filtering out masks that fail to meet our quality criteria, regardless of the font: (1) an OCR confidence score no lower than 0.8. (2) an edit distance no greater than 20\% of the string length. This step ensures that only high-quality segmentation masks are retained as glyph controls.

% The glyph preparation process can be expressed as
% \begin{equation}
%     \mathcal{M} (\boldsymbol{I}) = \tilde{\boldsymbol{c}}_{g} \in \mathbb{R}^{n \times n} \xrightarrow{OCR} \boldsymbol{c}_{g} \in \mathbb{R}^{n \times n},
% \end{equation}
% where $\boldsymbol{I} \in \mathbb{R}^{n \times n \times 3}$ is the target image, $\mathcal{M}$ is the text segmentation model, $\operatorname{OCR}$ means removing texts with low OCR scores in the pixel space, and $\tilde{\boldsymbol{c}}_{g}$ and $\boldsymbol{c}_{g}$ are the glyph control before and after the OCR-based filtering process.

\subsubsection{Perspective Distortion}
Segmentation masks, even after quality filtering, are not directly usable as glyph controls. In real-world scenarios, \textbf{users are unlikely to specify the exact locations of text or precisely align the text with the background}. To address this issue, we apply random perspective transformations to the collected glyph images, introducing slight translations and distortions to the text, without affecting the fonts. Specifically, we add random perturbations to the four corner points of the text's bounding box, with the perturbation upper-bounded by $\epsilon$ pixels.

\begin{comment}
Using the original corner points of the bounding box
\begin{equation}
\left\{ (x_i, y_i)\ \text{for} \ i \ \in\{1,2,3,4\} \right\}
\end{equation}
and the randomly perturbed ones 
\begin{align}
& \left\{ (x_i + \delta x_i, y_i + \delta y_i)\ \text{for} \ i \ \in\{1,2,3,4\} \right\}, \\
& \text{ s.t. }\left|\delta x_i\right| \leq \epsilon \text{ and }\left|\delta y_i\right| \leq \epsilon,
\end{align}
\end{comment}

We then compute a homography matrix $\boldsymbol{M} \in \mathbb{R}^{3 \times 3}$ that maps the original text region to a slightly distorted view. \textbf{This design ensures that the diffusion model does not rigidly replicate the exact pixel locations of the glyphs} but instead learns to adaptively position the text in a way that best integrates with the output image.

\subsubsection{Main Training Process} \label{sec:train_input}
The diffusion process builds upon AnyText~\cite{tuo2023anytext}, leveraging ControlNet~\cite{zhang2023adding} as the base model. As shown in Figure~\ref{fig:flow}, the model takes the following five inputs during training. We expect the training dataset to consist of images containing text, captions, and polygons for the text region. The text and polygons can be automatically extracted using an OCR algorithm.
\begin{itemize}  
    \item Font-aware glyph control \(\boldsymbol{c}_{g} \in \mathbb{R}^{n \times n}\): A binary mask representing the text and its font features in pixel space.
    % where textual pixels are assigned a value of 1 and background pixels are 0.  
    \item Position control \(\boldsymbol{c}_{p} \in \mathbb{R}^{n \times n}\): A binary mask for the bounding box of the text region.
    % with the bounding box pixels set to 1 and the background pixels set to 0.  
    We restrict ourselves to square local image regions. % for simplicity.
    \item Masked image \(\boldsymbol{c}_{m} \in \mathbb{R}^{n \times n \times 3}\): An RGB image normalized to the range \([-1, 1]\), where the region within the box position \(\boldsymbol{c}_{p}\) is masked to 0. Every other pixel is identical to the target image $\boldsymbol{I} \in \mathbb{R}^{n \times n \times 3}$.
    \item Image caption \(\boldsymbol{c}_{l}\): We adopt the same handling approach in~\citet{tuo2023anytext}, except for using our own $\boldsymbol{c}_{g}$. We empirically observe that image captions are not crucial for this work.
    \item Random noise input \(\boldsymbol{x}_{T} \in \mathbb{R}^{m \times m \times d}\) in the embedding space to initialize the reverse denoising process~\cite{ho2020denoising}.
\end{itemize}  
The model outputs a denoised tensor $\boldsymbol{x}_{T} \in \mathbb{R}^{m \times m \times d}$ after $T$ timesteps, which can be reconstructed back to an image $\boldsymbol{\hat{I}} \in \mathbb{R}^{n \times n \times 3}$.
% trained to approximate the ground-truth target image $\boldsymbol{I} \in \mathbb{R}^{n \times n \times 3}$.
In the expressions above, $n$ denotes the edge length of the image, $m$ represents the spatial resolution of the hidden features in the latent diffusion~\cite{ho2020denoising}, and $d$ represents the number of channels. 

We concatenate all input conditions as $\boldsymbol{c}$ and perform the following reverse denoising process:
\begin{align}
    & \boldsymbol{c} = \varphi(\operatorname{cat}[\xi_g(\boldsymbol{c}_{g}), \xi_p(\boldsymbol{c}_{p}), \xi_m(\boldsymbol{c}_{m})]) \label{eqn:condition} \\
    & p_\theta\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{c}\right)=\mathcal{N}\left(\boldsymbol{x}_{t-1} ; \mu_\theta\left(\boldsymbol{x}_t, t, \boldsymbol{c}\right), \Sigma_\theta(t)\right)\label{eqn:denoise}\vspace{-5mm}
\end{align}
where each $\xi$ is some convolutional layers that transform the input to $\mathbb{R}^{m \times m \times d}$, $\varphi$ is another fusion layer,
% using convolutions that results in $\mathbb{R}^{m \times m \times d}$
$p_\theta$ is the probabilistic model that predicts the distribution of a less noisy image $\boldsymbol{x}_{t-1}$ from $\boldsymbol{x}_{t}$ with $t \in [0, T]$,
% is the timestamp
and $\mu_\theta$ and $\Sigma_\theta$ are the mean and variance of the normal distribution $\mathcal{N}$. We follow the same training losses in AnyText~\cite{tuo2023anytext} to train this diffusion model.

% NEED TO SAY SOMETHING ABOUT THE LOSSES USED HERE EVEN IF WE ARE JUST USING THE AnyNet training scheme.


\subsection{Inference Pipeline}
Our philosophy is to design a more streamlined training pipeline that is easily scalable to larger open-world datasets, while shifting additional steps to inference time to provide users with greater control and flexibility as needed.

\subsubsection{Main Generation Process}
The reversed denoising process takes the same set of inputs outlined in Section~\ref{sec:train_input}. However, unlike training where the glyph control $\boldsymbol{c}_g$ is extracted using the text segmentation model $\mathcal{M}$, it is now provided directly by the user. 

On the user front-end, the required inputs include the original image $\boldsymbol{I}$ with a short caption $\boldsymbol{c}_{l}$, the desired text $t$, the font $f$ (which can be uploaded as a font file), and the polygon points $\boldsymbol{p}$ selected on $\boldsymbol{I}$ to define the region where the text will be rendered.
To streamline the process, the pipeline automatically converts polygon points $\boldsymbol{p}$ into the position control $\boldsymbol{c}_{p}$, generates the masked image $\boldsymbol{c}_{m}$, and converts text $t$ into the font-aware glyph control $\boldsymbol{c}_{g}$. 
% the following inputs are then automatically prepared in the backend for the user:
% \begin{itemize}  
%     \item The position control $\boldsymbol{c}_{p}$ drawn from the user-specified polygon points $\boldsymbol{p}$. 
%     \item The glyph control generated by rendering the text $t$ in the selected font $f$ on a black canvas of size $n \times n$, with the text in white. A perspective transformation is then applied to automatically fit the rendered glyph onto the position $\boldsymbol{c}_{p}$, same as the ones used during training.
%     \item The masked image $\boldsymbol{c}_{m}$ drawn by masking out regions in $\boldsymbol{I}$ within $\boldsymbol{p}$.
%     % , i.e., $\boldsymbol{c}_{m} \in [0, 255]\ \text{where } \boldsymbol{c}_{m} = \boldsymbol{I} \ \text{but with } \boldsymbol{I} \mid_{\text{pixels within } \boldsymbol{p}} = 127.5.$.
% \end{itemize}
Users are allowed to type multiple lines of text, possibly in different languages, fonts, or orientations, in a single $\boldsymbol{c}_{p}$, $\boldsymbol{c}_{g}$, and $\boldsymbol{c}_{m}$.

Finally, the reverse denoising process is run over $t$ timesteps following the same Equations~\ref{eqn:condition}-\ref{eqn:denoise} to generate the output image $\boldsymbol{x}_{0}$, whose region within the polygon mask $\boldsymbol{c}_{p}$ is will be blended into the original image $\boldsymbol{I}$ using normal seamless cloning or other blending algorithms. This completes the generation process, and the next two subsections illustrate optional steps that can be applied as needed.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/controltext_figures.2.png}
    \caption{Continuation of Figure~\ref{fig:top}. Examples of real-world and AI-generated images with text generated by ControlText in various fonts and languages. Each row presents both the rendered images and the textual part of their glyph controls. We also try the most complex Chinese character, ``biang", in the bottom row, accompanied by a zoomed-in view of the rendered character. ControlText effectively renders text with realistic integration into backgrounds while maintaining correct letters and characters in their user specified fonts.}
    \label{fig:main}
    % \vspace{-1cm}
\end{figure*}



\subsubsection{Inpainting Before Editing}
When editing text in an image, the mask $\boldsymbol{c}_m$ must encompass all the old text in the background. However, this mask could be larger than the size of the new text $t$ in the new font $f$, particularly when a narrower font is selected. Larger masks may introduce additional text rendered in the output image not specified in the glyph control $\boldsymbol{c}_g$.
% Meanwhile, because the training process described in Section~\ref{sec:prepare_glyph} utilizes an OCR-based filtering, $\boldsymbol{I}$ may contain additional text elements not included in the final glyph control $\boldsymbol{c}_g$ during training, which the text segmentation model $\mathcal{M}$ does not segment well. As a result, masks larger than the new text in new font at inference, especially with out-of-domain user images $\boldsymbol{I}$, can introduce additional text in the output image $\boldsymbol{x}_0$ within the mask region but not specified in the glyph control $\boldsymbol{c}_g$.
To address this challenge, we minimize the mask size to be just large enough to fit the text $t$ in the new font $f$. Following recommendations in \cite{li2024first}, we utilize an off-the-shelf inpainting model~\cite{razzhigaev2023kandinsky} to erase the original text. After inpainting, a new polygon $\hat{\boldsymbol{p}}$ is automatically tightened
from $\boldsymbol{p}$ to match the new text.
% , ensuring it is just large enough to contain the new text $t$ in the new font $f$. Following the recommendations in \cite{li2024first}, we utilize an off-the-shelf inpainting model~\cite{razzhigaev2023kandinsky} to erase the original text from the user-defined polygon region $\boldsymbol{p}$. Once inpainting is complete, a new polygon $\hat{\boldsymbol{p}}$ is automatically tightened to match the size of the new text $t$ in the font $f$. The tighter polygon $\hat{\boldsymbol{p}}$ and the corresponding mask $\hat{\boldsymbol{c}}_m$ will be sent to the diffusion model, enabling cleaner output images that avoid unwanted text not mentioned by the user.


\subsubsection{Small Textual Regions}
Handling smaller text remains a challenge~\cite{liu2024glyph, paliwal2024customtext}, as the diffusion process operates in the embedding space with potential information loss. To address this, we simply zoom into the text region specified by the user and interpolate it to the input size of the diffusion model. Finally, we blend the generated region with the original image $\boldsymbol{I}$. Figure~\ref{fig:main} includes some examples of small text rendered with high quality, demonstrating effective performance without the need for more complex algorithms or datasets.

\subsection{Evaluation Metrics}
\subsubsection{Evaluating Text}
We adopt the same evaluation metrics from AnyText~\cite{tuo2023anytext} to ensure that the generated text remains recognizable regardless of the font. Specifically, we utilize Sentence Accuracy (SenACC) and Node similarity with Edit Distance (NED) derived from OCR to assess text recognizability. We also employ Fréchet Inception Distance (FID) to evaluate the overall image quality.

\subsubsection{Evaluating Fuzzy Fonts}
Evaluating the accuracy of fonts in visual text remains an open question, as ground-truth font labels are typically unavailable in large-scale, real-world datasets. It is also the case that many fonts appear visually similar, making distinctions among them practically meaningless.\textbf{ These challenges highlight the need for a new evaluation metric that can handle fuzzy fonts in an open-world scenario.}
To address this, we introduce a novel evaluation framework leveraging a pre-trained font classification model $\mathcal{F}$. Specifically, we use the Google Font Classifier by Storia-AI~\cite{fontclassify2025}, an open-source model trained on $c = 3474$ fonts on both real and AI-generated images. Due to the large value of $c$, the classifier's embedding space is expected to provide meaningful representations, \textit{even if the model may have never encountered the evaluated font before.} For example, \textbf{two fonts that look similar should have similar embeddings in this pretrained font classification model}, and vice versa. Therefore, we propose two metrics $\boldsymbol{l_2@k}$ and $\boldsymbol{\cos@k}$ to evaluate font fidelity in any generated images with text of any fonts.

\begin{itemize}
    \item \textbf{Step 1 Embedding Extraction:} Both the input glyph $\boldsymbol{c}_g$ and the output image $\boldsymbol{x}_0$ are forwarded through the font classification model $\mathcal{F}$ to obtain their last-layer probabilities $\boldsymbol{p}_g, \boldsymbol{p}_x \in \mathbb{R}^{c}$, respectively, where $c$ is the number of labels $\mathcal{F}$ is pretrained on. Optionally, text regions in $\boldsymbol{x}_0$ can be first isolated using a text segmentation model $\mathcal{M}$, eliminating the influence of color and background.
    \item \textbf{Step 2 Distance Calculation:} We retain only the top $k$ largest values in $\boldsymbol{p}_g$ and $\boldsymbol{p}_x$, zeroing out the others, to ensure that the distance calculation focuses on the most likely $k$ labels. It helps reduce disturbances from the accumulation of remaining insignificant values. The metric $l_2@k$ and $\cos@k$ then compute the $l_2$-distance and $\cos$-distance between them.

 
\end{itemize}