\section{Related Work}
% \subsection{Visual Text Rendering}
\paragraph{Generation from Prompts or Text Embeddings} Text-to-image generation~\cite{zhang2023text, bie2023renaissance} has advanced significantly in recent years, leveraging conditional latent diffusion models~\cite{ho2020denoising, rombach2022high, zhang2023adding}. Foundational image generation models~\cite{ramesh2021zero, betker2023improving, midjourney, saharia2022photorealistic, flux1_ai, esser2024scaling, yang2024cross, zhao2023unleashing, hoe2024interactdiffusion, sun2025anycontrol, chang2022maskgit} have achieved remarkable progress in creating high-quality photo-realistic and artistic images. 

Despite these advancements, visual text rendering~\cite{bai2024intelligent, han2024ace, li2024hfh} continues to pose significant challenges.
% within the domain of text-to-image generation.
Several algorithms rely on text embeddings from user prompts or captions to control the diffusion process, such as TextDiffuser~\cite{chen2024textdiffuser}, TextDiffuser2~\cite{chen2025textdiffuser}, and DeepFloyd's IF~\cite{deepfloyd_if}. \citet{li2024empowering} utilizes intermediate features from OCR~\cite{du2020pp} as text embeddings, \citet{liu2022character, wang2024high, choi2024towards} take one step deeper into the character level,
% \cite{hu2024amo} replaces the Euler sampler in rectified flow~\cite{liu2022rectified} models with an overshooting sampler, 
and TextHarmony~\cite{zhao2024harmonizing} queries a fine-tuned vision-language model to generate embeddings from images and captions. 

% Research has also expanded text-to-image generation to encompass logos~\cite{zhang2024anylogo, zhu2025logosticker}, scientific figures and diagrams~\cite{rodriguez2023ocr}, and videos~\cite{liu2024text}. Our training methods focus exclusively on pixel-level controls independent of semantic meanings, making it possible to generalize to other visual elements, such as Kaomoji at the top-right of Figure~\ref{fig:top}.

\paragraph{Generation from Glyphs} The majority of algorithms rely on visual glyphs - pixel-level representations that describe the appearance of texts - to guide the generation process.
% ~\cite{tuo2023anytext, tuo2024anytext2, liu2024glyph, liu2025glyph, ma2023glyphdraw, yang2024glyphcontrol, chen2024diffute, zhang2024control, wang2024textmaster, lakhanpal2024refining, choi2024towards, zhang2024brush, li2024joytype, ma2024glyphdraw2}. 
However, massive real-world images in the training dataset often lack ground-truth font annotations, while it is challenging to extract exact font labels from any real-world photos. \textbf{Consequently, these approaches utilize a fixed standard font to render the texts on their glyph controls.} For instance, GlyphControl~\cite{yang2024glyphcontrol} and GlyphDraw~\cite{ma2023glyphdraw} render text detected by OCR models, with the former introducing an additional Glyph ControlNet to regulate the decoder of the original ControlNet model~\cite{zhang2023adding}. TextMaster~\cite{wang2024textmaster}, DiffUTE~\cite{chen2024diffute}, and AnyTrans~\cite{qian2024anytrans} aim
to maintain uniform fonts within the same image. \citet{choi2024towards} explores design translation for artistic characters.
% , and \cite{zhang2024control} studies control mechanisms at different stages of the diffusion process. 
Similarly,
% \cite{zhang2024choose} disentangles textual styles and content, whereas 
\citet{zhang2024brush} employs randomly chosen fonts and a Canny edge detector. Several approaches also incorporate automatic textual layout generation~\cite{tuo2023anytext, zhu2024visual, chen2024diffute, li2024first, seol2025posterllama, lakhanpal2024refining, paliwal2024orientext, zhao2024ltos}. For example, \cite{zhu2024visual} uses a vision-language model to identify suitable layouts, DiffUTE~\cite{chen2024diffute} leverages language models and OCR detectors to propose bounding boxes, and \cite{seol2025posterllama} employs HTML codes. 
% Other techniques include bounding box control for layouts~\cite{zhao2024ltos}, layout optimization via simulated annealing~\cite{lakhanpal2024refining}, layout alignment with surface orientations~\cite{paliwal2024orientext}, and integration of synthesis, erasure, and rendering pipelines to achieve precise control over both text and layout~\cite{li2024first}. 
In constrast, we concentrate on human-in-the-loop local text editing  rather than automatic global layout generation. Our work also eliminates the need for any large language or multimodal models, streamlining the pipeline.

Our work builds upon the codebase of AnyText~\cite{tuo2023anytext}, a glyph-based algorithm that trains a base ControlNet~\cite{zhang2023adding} model to render visual texts.
% based on a pixel-level mask which controls of how the text should appear.
Since ground-truth font annotations are not accessible from real-world images, glyphs are generated in a fixed standard font, leaving the model to infer an appropriate font.
% along with their user-specified positional masks. 
% Our approach eliminates the reliance on a fixed font across all glyphs, instead leveraging text segmentation masks~\cite{xu2021rethinking} to generate font-aware glyph controls in pixel space. 


\paragraph{Font-Controllable Generation} Fewer recent works are more closely related to ours in enabling controllable fonts~\cite{tuo2024anytext2, ma2024glyphdraw2, li2024joytype, shi2024fonts, paliwal2024customtext, liu2025glyph}. \textbf{However, none of these works provide a quantitative evaluation metric to assess the generated fonts in open-world settings.} AnyText2~\cite{tuo2024anytext2} is a concurrent work developed by the authors of AnyText~\cite{tuo2023anytext}. We share a similar architecture, but unlike \cite{tuo2024anytext2}, we eliminate its use of lengthy language prompts in the inputs, separate models to support different languages, and the trainable OCR model to encode the font features. Instead, we use OCR solely to filter out low-quality glyph controls. \citet{tuo2024anytext2} is also not yet open-sourced at the time of our submission.
% In addition, we introduce a novel quantitative evaluation metric for fuzzy fonts that look visually similar, demonstrating zero-shot performance on font control across other languages in the open world.






Glyph-ByT5~\cite{liu2025glyph} and its v2~\cite{liu2024glyph} also support font control. However, unlike \cite{liu2025glyph}, which \textit{pre-specifies font labels for each language} and focuses on improving language understanding, we assume that text rendering is independent of the semantic meanings; in our approach, only pixel-level controls matter.
GlyphDraw2~\cite{ma2024glyphdraw2} learns font features through cross-attention between glyph features and hidden variables within the image while fine-tuning a language model to generate font layouts. Unfortunately, it lacks quantitative assessment on fonts, relying on human evaluations. 
JoyType~\cite{li2024joytype} focuses on e-commerce product images in non-Latin languages using 10 \textit{pre-specified fonts with synthetic images} and additional vision-language models. It employs an OCR model for font perceptual losses, while we assume OCR to be font-agnostic.
FonTS~\cite{shi2024fonts} uses additional reference images as font style controls and \textit{requires users to select fonts from pre-specified integer font labels}. Similarly, CustomText~\cite{paliwal2024customtext} relies on specific font names provided in extended user prompts and an additional training datasets tailored for smaller fonts.
While \citet{liu2024glyph} emphasizes visual aesthetics and highlights challenges with small fonts, our work centers on font editing and share the same perspective that smaller fonts pose greater difficulties. However, we show that zooming into localized regions for text editing can already improve the quality of smaller text.
\textbf{Different from those works, we eliminate the need for pre-specified fonts with unique font names or special font tokens}~\cite{liu2024glyph, shi2024fonts, paliwal2024customtext} in user prompts, making our method \textbf{generalizable to unseen languages and fonts}. 

% While \citet{liu2024glyph} emphasizes visual aesthetics and highlights challenges with small fonts, our work centers on font editing and share the same perspective that smaller fonts pose greater difficulties. However, we show that zooming into localized regions for text editing can already improve the quality of smaller text.
% and our approach as a plug-in-module can also be applied to base models like \cite{liu2024glyph}. 

% GlyphDraw2~\cite{ma2024glyphdraw2} learns font features through cross-attention between glyph features and hidden variables within the image while fine-tuning a language model to generate font layouts. Unfortunately, it lacks quantitative assessment on fonts, relying on human evaluations. 
% JoyType~\cite{li2024joytype} focuses on non-Latin languages using 10 pre-specified fonts with synthetic images and additional vision-language models. It employs an OCR model for font perceptual losses, while we assume OCR to be font-agnostic. 
% % and employs OCR for font perceptual losses. In contrast, we assume OCR to be font-agnostic in text recognition and eliminate the need for an additional vision-language model in the pipeline. 
% FonTS~\cite{shi2024fonts} uses additional reference images as font style controls and requires users to select fonts from pre-specified integer font labels. Similarly, CustomText~\cite{paliwal2024customtext} relies on specific font names provided in extended user prompts and an additional training datasets tailored for smaller fonts. Our approach removes the dependence on specific font names, recognizing that many fonts can appear ambiguously similar and extending control to unseen fonts.

% Additionally, while we address small font sizes as in \cite{paliwal2024customtext}, we do so only during the inference stage, avoiding the need to create additional training datasets tailored for smaller fonts~\cite{paliwal2024customtext}.


% Research has also expanded text-to-image generation to encompass logos~\cite{zhang2024anylogo, zhu2025logosticker}, scientific figures and diagrams~\cite{rodriguez2023ocr}, and videos~\cite{liu2024text}. Our training methods focus exclusively on pixel-level controls independent of semantic meanings of languages, making it possible to generalize to diverse visual elements such as characters in other languages, logos, and emojis in a zero-shot setting.



% \cite{tuo2023anytext, chen2025textdiffuser, chen2024textdiffuser, zhu2024visual, liu2022character, ma2023glyphdraw, yang2024glyphcontrol, balaji2022ediff, deepfloyd_if, chen2024diffute, rodriguez2023ocr, qian2024anytrans, zhang2024choose, liu2024glyph, zhao2024harmonizing, li2024first, wang2024high, zhu2025logosticker, li2024empowering, liu2024text, seol2025posterllama, zhang2024control, wang2024textmaster, lakhanpal2024refining, choi2024towards, hu2024amo, paliwal2024orientext, zhang2024brush, zhang2024anylogo, zhao2024ltos, liu2025glyph}
% Survey~\cite{bai2024intelligent, han2024ace, li2024hfh}
% Font controllable~\cite{tuo2024anytext2, ma2024glyphdraw2, li2024joytype, shi2024fonts, paliwal2024customtext, liu2025glyph}
% Concurrent work~\cite{tuo2024anytext2}


% \cite{tuo2023anytext} we build on it. AnyText AnyWord-3M
% \cite{wang2024high} DreamText. Allow diverse font styles by jointly training text encoder and generator to enrich the character representataion space. split into character. use text embedding with additional guidance on attention.
% \cite{hu2024amo} training-free. replace euler aampler in rectified flow models with overshooting sampler.

% \cite{li2024first} Layout positions. synthesis -> erasing -> blend texts. general font for the glyph image.
% \cite{zhang2024brush} sketch image with a random chosen font + edge image from canny edge detector
% \cite{zhang2024choose} disentangle style (font, background) and content (character, texture) as separate features.
% \cite{lakhanpal2024refining} layout. standard fonts. 
% \cite{paliwal2024orientext} layout align with surface orientation
% \cite{seol2025posterllama} use HTML codes for layout formatting and text rendering
% \cite{chen2024diffute} DiffUTE. uses LLM and OCR to propose box to edit. uniform font based on surrounding texts. 
% \cite{zhu2024visual} uses VLM to identify suitable layouts and recommend contents. local diffusion model, same as ours. 
% \cite{choi2024towards} character-level. design transfer on characters across languages. standard font glyph + style images. 

% \cite{yang2024glyphcontrol} GlyphControl. use glyphs, but uniform fonts after OCR detection.
% \cite{ma2023glyphdraw} GlyphDraw. finetuned on chinese.
% \cite{wang2024textmaster} TextMaster. glyphs. standard fonts. layout. 
% \cite{zhang2024control} glyph image. standard font. explore the control at different time steps and outputs.
% \cite{qian2024anytrans} OCR. translation. 

% \cite{chen2024textdiffuser} TextDiffuser. transformer proposes character masks, based on layouts and text prompts.
% \cite{chen2025textdiffuser} TextDiffuser2. uses two LLM. special tokens to encode text prompts, layouts, and keywords. 
% \cite{deepfloyd_if} text embedding. hierarchical rendering.
% \cite{li2024empowering} intermediate features from OCR as text embedding
% \cite{liu2022character} DrawText benchmark. character-aware text encoder
% \cite{zhao2024harmonizing} TextHarmony. focus on multimodal understanding and generation. multiple lora experts. vlm outputs image tokens from captions. no clear usage of glyphs.

% \cite{liu2024text} extends Anytext\cite{tuo2023anytext} to videos
% \cite{zhang2024anylogo} logo insertion
% \cite{rodriguez2023ocr} figure and diagram generation. OCR for text perceptual loss.
% \cite{zhu2025logosticker} introduce the task of logo insertion. our method generalizes to texts, emojis, and logos.

% Differences with~\cite{tuo2024anytext2} AnyText2
% (1) Simpler architecture.
% (2) No lengthy prompts, and the prompts also didn't mention the name of the fonts.
% (3) Didn't use trainable OCR model for font encoder, but a text segmentation model in zero-shot. The OCR model only serves to filter low-quality segmentation masks.
% (4) No separate encoders for different languages.
% (5) Quantitative evaluation for fuzzy fonts that look similar. We show other languages too, and even move beyond human languages to emojis for example.


% Differences with~\cite{liu2024glyph} Glyph-ByT5-v2 text encoder and Glyph-SDXL-v2 model. 
% (1) They train the image on their Multilingual Glyph-Text Dataset of 10 different languages. We show zero-shot performance on languages not chinese or english. 
% (2) long prompts. Special tokens in encode fonts in the prompts, and we alleviate this requirement. 
% (3) We are small-scale plug-in modules for text editing in local regions, light weighted, on top of sota foundation models such as this one.
% (4) They address multilingual and visual aesthetics, not specifically font editing. 

% Differences with~\cite{liu2025glyph} Glyph-byt5
% (1) We state that visual text rendering is independent to language understanding, while their paper thinks the bottleneck is the text encoders and focus on improving language understanding.
% (2) They have a series of trained text encoders
% (3) They pre-specify fonts using unique font names, and these uniqe font names are not interpretable to a new user. The user needs to check what they mean, and can not use other new fonts. 

% Differences with \cite{ma2024glyphdraw2} GlyphDraw2
% (1) cross attention between glyph features and hidden variables within the image to learn the font features
% (2) finetune llm to generate font layouts
% (1) and (2) lead to additional adaptive text encoders
% (3) focus on 2d poster, we also extend to texts on 3d surfaces
% (4) no quantitative evaluation on fuzzy fonts

% Differences with~\cite{li2024joytype} JoyType
% (1) They focus on non-Latin languages in their paper with synthetic data. Our algorithm is more general to any real-data.
% (2) They use OCR perceptual loss, but OCR is font agnostic.
% (3) They explore only 10 fonts with only qualitative results. Our algorithm is open-world and provide quantitative evaluations.
% (4) No need for VLM.

% Differences with \cite{shi2024fonts} FonTS
% (1) they use additional reference image as the style control
% (2) requires the user to choose a predefined integer font index, and these uniqe font indices are not interpretable to a new user. The user needs to check what they mean, and can not use other new fonts. 

% Differences with \cite{paliwal2024customtext} CustomText
% (1) use specific font names in the detailed prompt, i.e., formatting functions to specify input control parameters
% (2) We tackle the small font size only in the inference stage by zoom in the local region, no need to prepare additional dataset specific for smaller fonts


% \subsection{Text Recognition and Segmentation}
% \cite{xu2021rethinking, duan2024odm}

% \subsection{Font Classification and Generation}
% \cite{kong2022look, yang2024fontdiffuser}