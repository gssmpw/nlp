\section{Discussion}
This work presents a simple and scalable proof-of-concept for multilingual visual text rendering with user-controllable fonts in the open world. We summarize our key findings as follows:

\paragraph{Font controls require no font label annotations} 
A text segmentation model can capture nuanced font information in pixel space without requiring font label annotations in the dataset, enabling zero-shot generation on unseen languages and fonts, as well as scalable training on web-scale image datasets as long as they contain text.

\paragraph{Evaluating ambiguous fonts in the open world} 
Fuzzy font accuracy can be measured in the embedding space of a pretrained font classification model, utilizing our proposed metrics $l_2@k$ and $\cos@k$.

\paragraph{Supporting user-driven design flexibility} 
Random perturbations can be applied to segmented glyphs. While this wonâ€™t affect the rendered text quality, it accounts for users not precisely aligning text to best locations and prevents models from rigidly replicating the pixel locations in glyphs.

\paragraph{Working with foundation models} 
With limited computational resources, we can still copilot foundational image generation models to perform localized text and font editing.
\\

Future work will focus on enhancing data efficiency in the training pipeline, especially fonts in low-resource languages, as well as enabling more complicated artistic style control of text from user prompts beyond font information, including its interaction with diverse underlying background. 