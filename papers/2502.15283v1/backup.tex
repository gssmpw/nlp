\begin{theorem}[Locally-Extending Mutual Compatibility]\label{thm:2_bidder_comp} For the two-bidder case, $\vv$ = ($\vv_i$, $\vv_{\shortn i}$) is a grid point used in the transformation, then $\forall \vv'\in\mathcal{N}_{\frac{\epsilon}{2}}=\{\vv' | \|\vv'-\vv\|\le \epsilon / 2\}$ is compatible.
\end{theorem}
\begin{proof}
(I) We first consider a fixed grid point of bidder $\shortn i$'s value, to which a transformation procedure is applied. Here the menu $B_i$ of bidder $i$ is unchanged. Suppose that bidder $i$ selects the $k_i$-th element $(\bm\alpha_i^{(k_i)}, \beta_i^{(k_i)})$ in this menu when its type is $\vv_i$, and bidder $\shortn i$ selects the $k_{\shortn i}$-th in its menu $B_{\shortn i}(\vv_i)$ when bidder $i$ is of the type $\vv_i$. The bidders' menu selections $k_i$ and $k_{\shortn i}$ are compatible due to our transformation process. We now prove that the mutual compatibility of allocation holds for all values in $\mathcal{N}_{i,\frac{\epsilon}{2}}=\{\vv_i' | \|\vv_i'-\vv_i\|_\infty\le \epsilon / 2\}$.

(I.A) Case 1: Both bidders do not change their menu element selection in \neighbor. In this case, the only possibility of incompatibility comes from the change in the allocation of bidder $\shortn i$, $\bm\alpha_{\shortn i}^{(k_{\shortn i})}(\vv_i)$, due to the varying $\vv_i$ value (recall that $\bm\alpha_{\shortn i}^{(k_{\shortn i})}$ is generated by the allocation network that is conditioned on $\vv_i$). With the Lipschitz constant $L_a$ of our allocation network, this change can be bounded by:
\begin{align}
    \|\Delta \bm\alpha_{\shortn i}^{(k_{\shortn i})}\|_\infty \le L_a \|\Delta \vv_i\|_\infty \le \frac{\epsilon}{2}L_a.
\end{align}
Here our safety margin $s_f= \frac{\epsilon}{2}L_a $ comes into play. With $s_f$ in the incompatibility loss (Eq.~\ref{equ:f_loss}) and the transformation process (Eq.~\ref{equ:mip_feas_standard}), we have
\begin{align}
    \bm\alpha_i^{(k_i)}+\bm\alpha_{\shortn i}^{(k_{\shortn i})} \le 1 - s_f \le 1 - \Delta \bm\alpha_{\shortn i}^{(k_{\shortn i})}.
\end{align}
It follows that $\bm\alpha_i^{(k_i)}+\bm\alpha_{\shortn i}^{(k_{\shortn i})}+\Delta \bm\alpha_{\shortn i}^{(k_{\shortn i})}\le 1$, which means the joint allocation is always feasible in \neighbor.

(I.B) For the other cases, we need the condition that bidder $\shortn i$ does not change its selection in \neighbor. We first prove that if, at $\vv_i$, the utility of the best element in bidder $\shortn i$'s menu is at least $s_m=2(v_{\max}L_a + L_p)\epsilon$ higher than that of the second best element, then bidder $\shortn i$ does not change its selection in \neighbor. Here, $v_{\max}$ is the maximum possible value and $L_p$ is the Lipschitz constant of our payment network. 

Assume that bidder $\shortn i$ changes from $k_{\shortn i}$-th element to a second menu element $(\bm\alpha_{\shortn i}^{(k_{\shortn i}')}, \beta_{\shortn i}^{(k_{\shortn i}')})$ at $\vv_i'$. At $\vv_i$, we have
\begin{align}
    &\vv_{\shortn i}^\Tau \bm\alpha_{\shortn i}^{(k_{\shortn i})}(\vv_i) - \beta_{\shortn i}^{(k_{\shortn i})}(\vv_i) \nonumber\\
    \ge &\vv_{\shortn i}^\Tau \bm\alpha_{\shortn i}^{(k_{\shortn i}')}(\vv_i) - \beta_{\shortn i}^{(k_{\shortn i}')}(\vv_i) + s_m \\
    \ge &\vv_{\shortn i}^\Tau \bm\alpha_{\shortn i}^{(k_{\shortn i}')}(\vv_i) - \beta_{\shortn i}^{(k_{\shortn i}')}(\vv_i) + 2(v_{\max}L_a + L_p)\|\vv_i'-\vv_i\|_\infty.\label{equ:case22-1}\nonumber
\end{align}
Then, at $\vv_i'$, it follows that:
\begin{align}
    & \vv_{\shortn i}^\Tau \bm\alpha_{\shortn i}^{(k_{\shortn i})}(\vv_i') - \beta_{\shortn i}^{(k_{\shortn i})}(\vv_i') \\
    \ge & \vv_{\shortn i}^\Tau \bm\alpha_{\shortn i}^{(k_{\shortn i})}(\vv_i) - \beta_{\shortn i}^{(k_{\shortn i})}(\vv_i) - (v_{\max}L_a + L_p)\|\vv_i'-\vv_i\|_\infty\nonumber \\
    \ge & \vv_{\shortn i}^\Tau \bm\alpha_{\shortn i}^{(k_{\shortn i}')}(\vv_i) - \beta_{\shortn i}^{(k_{\shortn i}')}(\vv_i) + (v_{\max}L_a + L_p)\|\vv_i'-\vv_i\|_\infty\nonumber\\
    \ge & \vv_{\shortn i}^\Tau \bm\alpha_{\shortn i}^{(k_{\shortn i}')}(\vv_i') - \beta_{\shortn i}^{(k_{\shortn i}')}(\vv_i')\label{equ:case22-2},
\end{align}
which means that bidder $\shortn i$ would still select $k_{\shortn i}$ at $\vv_i'$. 

The question then becomes how to guarantee a safety margin of $s_m=2(v_{\max}L_a + L_p)\epsilon$. Our findings indicate that for the network-generated menus of most $\vv_i$, this criterion is already met. Therefore, in practice, after sampling and prior to transforming, we verify whether this condition is satisfied for each $\vv_i\in\mathcal{V}_i$. If it is not, we add a new grid point to reduce $\epsilon$ to $\epsilon/2$. This procedure can be repeated until the condition is met.
% , or until $\epsilon/2$ is sufficiently small. In such cases, we can freeze bidder $\shortn i$'s selection in \neighbor~without significantly impacting revenue. \twadd{+ A justification for not violating IC with this freezing.}

% \twadd{Check this} The agent can always choose to select the IR option if $k_{\shortn i}$ gives him less than $0$ in utility. We note, however, that the compatibility constraint is further relaxed whenever IR is selected since the allocation probability is $0$ under the IR option, therefore we can focus on the case that bidder $\shortn i$ would not change its selection in \neighbor~without loss of generality.

Now we can discuss the case where bidder $i$ changes its menu element selection in \neighbor. Suppose that bidder $i$ changes from $k_i$-th element to another menu element $(\bm\alpha_i^{(k_i')}, \beta_i^{(k_i')})$ at $\vv_i'$. If $\bm\alpha_i^{(k_i')}$ is incompatible with $\bm\alpha_{\shortn i}^{(k_{\shortn i})}$, our construction of bidder $i$'s menu transformation process (Eq.~\ref{eq:trans_infeas}) requires that, at $\vv_i$: (Here $s_t=\epsilon/2$)
\begin{align}
    &\vv_i^\Tau \bm\alpha_i^{(k_i)} - \beta_i^{(k_i)} \ge \vv_i^\Tau \bm\alpha_i^{(k_i')} - \beta_i^{(k_i')} + s_t\nonumber \\
    \Rightarrow\ \ & \left(\vv_i^{\Tau} \bm\alpha_i^{(k_i)} - \beta_i^{(k_i)}\right) - \left(\vv_i^{\Tau} \bm\alpha_i^{(k_i')} - \beta_i^{(k_i')}\right) \ge \|\vv_i'-\vv_i\|_1\label{equ:case32-1}
\end{align}


At $\vv_i'$, it follows that:
\begin{align}
    & \left(\vv_i^{'\Tau} \bm\alpha_i^{(k_i)} - \beta_i^{(k_i)}\right) - \left(\vv_i^{'\Tau} \bm\alpha_i^{(k_i')} - \beta_i^{(k_i')}\right) \nonumber\\
    = & \left[\vv_i^{\Tau} \bm\alpha_i^{(k_i)} - \beta_i^{(k_i)} + \left(\vv_i^{'\Tau}-\vv_i^{\Tau} \right)\bm\alpha_i^{(k_i)}\right] - \left[\vv_i^{\Tau} \bm\alpha_i^{(k_i')} - \beta_i^{(k_i')} + \left(\vv_i^{'\Tau}-\vv_i^{\Tau} \right)\bm\alpha_i^{(k_i')}\right]\nonumber\\
    = & \left[\left(\vv_i^{\Tau} \bm\alpha_i^{(k_i)} - \beta_i^{(k_i)}\right) - \left(\vv_i^{\Tau} \bm\alpha_i^{(k_i')} - \beta_i^{(k_i')}\right)\right] + \left(\vv_i^{'\Tau}-\vv_i^{\Tau} \right)\left(\bm\alpha_i^{(k_i)}-\bm\alpha_i^{(k_i')}\right) \nonumber\\
    \ge & \|\vv_i'-\vv_i\|_1 -  \|\vv_i'-\vv_i\|_1 = 0, \label{equ:case32-2}
\end{align}
where the first inequality is because $\bm\alpha_i^{(k_i)}-\bm\alpha_i^{(k_i')}\ge -1$. Eq.~\ref{equ:case32-2} means that at $\vv_i'$, only an allocation compatible with $\bm\alpha_{\shortn i}^{(k_{\shortn i})}$ would be selected. Then the only possibility of incompatibility comes from the changes in $\bm\alpha_{\shortn i}^{(k_{\shortn i})}$ due to varying $\vv_i$ as network input, which reduces to Case (I.A).

(II) In case (I), we show that for any given $\vv_{\shortn i}$, as along as we transform the bidder $i$'s menu at $\vv_{\shortn i}$, the allocation is guaranteed to be compatible. During training time, we apply the transformation to a grid of $\vv_{\shortn i}$. To close the proof, we need to prove that for $\forall \vv'_{\shortn i}$ that is not a grid point, the allocation is still compatible. To be specific, we show this for $\forall \vv'_{\shortn i}\in \mathcal{N}_{\shortn i,\frac{\epsilon}{2}}=\{\vv_{\shortn i}' | \|\vv_{\shortn i}'-\vv_{\shortn i}\|_\infty\le \epsilon / 2\}$.

For $\vv'_{\shortn i}$, we require that bidder $i$ receives the same post-transformation menu as in the case of $\vv_{\shortn i}$. This menu-level discretization will not violate DSIC as the menu is still self-bid independent and the menu selection is still agent-optimizing. To see this, a special case of this menu discretization is a global menu which is used in RochetNet.

In this way, if bidder $\shortn i$ does not change its selection in $\mathcal{N}_{\shortn i,\frac{\epsilon}{2}}$, then case (II) reduces to case (I) because bidder $i$'s selection is the same as in case (I). So the question is how to ensure that bidder $\shortn i$ does not change its selection.

Suppose that bidder $\shortn i$ selects $(\bm\alpha_{\shortn i}^{(k_{\shortn i})}, \beta_{\shortn i}^{(k_{\shortn i})})$ at $\vv_{\shortn i}$. Then at $\vv_{\shortn i}'$, for any other elements than $k_{\shortn i}$, we have:
\begin{align}
    & \left(\vv_{\shortn i}^{'\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i})} - \beta_{\shortn i}^{(k_{\shortn i})}\right) - \left(\vv_{\shortn i}^{'\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i}')} - \beta_{\shortn i}^{(k_{\shortn i}')}\right) \nonumber\\
    = & \left[\vv_{\shortn i}^{\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i})} - \beta_{\shortn i}^{(k_{\shortn i})} + \left(\vv_{\shortn i}^{'\Tau}-\vv_{\shortn i}^{\Tau} \right)\bm\alpha_{\shortn i}^{(k_{\shortn i})}\right] - \left[\vv_{\shortn i}^{\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i}')} - \beta_{\shortn i}^{(k_{\shortn i}')} + \left(\vv_{\shortn i}^{'\Tau}-\vv_{\shortn i}^{\Tau} \right)\bm\alpha_{\shortn i}^{(k_{\shortn i}')}\right]\nonumber\\
    = & \left[\left(\vv_{\shortn i}^{\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i})} - \beta_{\shortn i}^{(k_{\shortn i})}\right) - \left(\vv_{\shortn i}^{\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i}')} - \beta_{\shortn i}^{(k_{\shortn i}')}\right)\right] + \left(\vv_{\shortn i}^{'\Tau}-\vv_{\shortn i}^{\Tau} \right)\left(\bm\alpha_{\shortn i}^{(k_{\shortn i})}-\bm\alpha_{\shortn i}^{(k_{\shortn i}')}\right) \nonumber\\
    \ge &\left[\left(\vv_{\shortn i}^{\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i})} - \beta_{\shortn i}^{(k_{\shortn i})}\right) - \left(\vv_{\shortn i}^{\Tau} \bm\alpha_{\shortn i}^{(k_{\shortn i}')} - \beta_{\shortn i}^{(k_{\shortn i}')}\right)\right]  -  \epsilon/2, \label{equ:case2}
\end{align}
The first term in~\ref{equ:case2} is the utility difference between two menu elements. Similar to case (I.B), after sampling and prior to transforming, we verify whether the utility of second best menu element is smaller by at least $\epsilon/2$ than that of the best element. If it is not, we add a new grid point to reduce $\epsilon$ to $\epsilon/2$. This procedure can be repeated until the condition is met. Once the condition is satisfied, bidder $\shortn i$ would note change its selection in $\mathcal{N}_{\shortn i,\frac{\epsilon}{2}}$.
% We also note that the condition of adding a new grid point in case (I.B) and (II) can be relaxed by replacing the \emph{second best menu element} by the \emph{best incompatible menu element}, as bidder $\shortn i$ can always choose a compatible element with in
\end{proof}

% (II) Case 2: Bidder $i$ changes its menu element selection in \neighbor. In this case, we prove that bidder $i$ would only choose menu elements that are compatible with bidder $\shortn i$'s choice.

% For $\forall \vv_i'\in$ \neighbor, it is easy to find a subset of samples $\mathcal{V}_i'=\{\vv_i^{\ell_1}, \vv_i^{\ell_2},\cdots,\vv_i^{\ell_m}\}$ used in the transform, so that $\vv_i'$ can be expressed as a linear combination of samples in $\mathcal{V}_i'$:
% \begin{align}
%     \vv_i' = \sum_{s=1}^{m} a_s \vv_i^{\ell_s}; \ \ \sum_{s=1}^{m} a_s=1;\ \  \forall s,\ a_s\in[0,1].
% \end{align}
% We prove by contradiction and assume that at $\vv_i'$ bidder $i$ selects menu element $(\bm\alpha_i^{(k)}, \beta_i^{(k)})$ that is 


% \begin{table} [t]
%     \caption{Yao.}\label{tab:yao_scale_up_bidder}
%     \centering
%     \begin{tabular}{CRCRCRCRCRCRCR}
%         \toprule
%         \multicolumn{2}{c}{\multirow{2}{*}{}} &
%         \multicolumn{2}{l}{\multirow{2}{*}{Alg.}} &
%         \multicolumn{2}{l}{\multirow{2}{*}{Perf.}} &
%         \multicolumn{8}{c}{Setting}\\
        
%         \cmidrule(lr){7-14}
%         \multicolumn{2}{c}{} &
%         \multicolumn{2}{l}{} &
%         \multicolumn{2}{l}{} &
%         \multicolumn{2}{c}{(3, 3, 4, 0.3)} & 
%         \multicolumn{2}{c}{(5, 3, 4, 0.3)} & 
%         \multicolumn{2}{c}{(7, 3, 4, 0.3)} & 
%         \multicolumn{2}{c}{(10, 3, 4, 0.3)}\\
%         \cmidrule(lr){1-2}
%         \cmidrule(lr){3-4}
%         \cmidrule(lr){5-6}
%         \cmidrule(lr){7-8}
%         \cmidrule(lr){9-10}
%         \cmidrule(lr){11-12}
%         \cmidrule(lr){13-14}
        
%         \multicolumn{2}{c}{\multirow{4}{*}{Ours}} & \multicolumn{2}{l}{\multirow{2}{*}{Before Transf.}}  & \multicolumn{2}{l}{Feas.} & \multicolumn{2}{l}{100\%} & \multicolumn{2}{l}{100\%} & \multicolumn{2}{l}{100\%}  & \multicolumn{2}{l}{100\%} \\
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{}  & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{7.82202u..} & \multicolumn{2}{l}{9.1435} & \multicolumn{2}{l}{7.4735} & \multicolumn{2}{l}{6.7186} \\
%         \cmidrule(lr){3-4}
%         \cmidrule(lr){5-6}
%         \cmidrule(lr){7-8}
%         \cmidrule(lr){9-10}
%         \cmidrule(lr){11-12}
%         \cmidrule(lr){13-14}
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{\multirow{2}{*}{After Transf.}}  & \multicolumn{2}{l}{Feas.} & \multicolumn{2}{l}{100\%} & \multicolumn{2}{l}{100\%} & \multicolumn{2}{l}{100\%} & \multicolumn{2}{l}{100\%}  \\
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{}  & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{\textbf{7.8309}} & \multicolumn{2}{l}{\textbf{9.1504}} & \multicolumn{2}{l}{\textbf{7.4774}} & \multicolumn{2}{l}{\textbf{6.72205}} \\
%         \midrule
%         \multicolumn{2}{c}{\multirow{5}{*}{Baselines}} & \multicolumn{2}{l}{VCG} &  \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{9.9221} & \multicolumn{2}{l}{7.9668} & \multicolumn{2}{l}{6.9815} & \multicolumn{2}{l}{6.4908}\\
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{Item-Myerson} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{10.9220} & \multicolumn{2}{l}{7.2829} & \multicolumn{2}{l}{5.4607} & \multicolumn{2}{l}{4.5490}\\
%         % \multicolumn{2}{c}{} & \multicolumn{2}{l}{Lottery AMA} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{} & \multicolumn{2}{l}{\textbf{}} & \multicolumn{2}{l}{\textbf{}} & \multicolumn{2}{l}{} & \multicolumn{2}{l}{\textbf{}}\\
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{AMenuNet} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{10.2852} & \multicolumn{2}{l}{8.1416} & \multicolumn{2}{l}{7.0697} & \multicolumn{2}{l}{6.5349}\\
%         \cmidrule(lr){3-4}
%         \cmidrule(lr){5-6}
%         \cmidrule(lr){7-8}
%         \cmidrule(lr){9-10}
%         \cmidrule(lr){11-12}
%         \cmidrule(lr){13-14}
%         \multicolumn{2}{c}{\multirow{2}{*}{}} & \multicolumn{2}{l}{\multirow{2}{*}{RegretNet}} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{12.8052} & \multicolumn{2}{l}{9.17377} & \multicolumn{2}{l}{7.50169} & \multicolumn{2}{l}{6.6799}\\
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{} & \multicolumn{2}{l}{IC Viol.} & \multicolumn{2}{l}{0.00951} & \multicolumn{2}{l}{0.00846} & \multicolumn{2}{l}{0.00548} & \multicolumn{2}{l}{0.00943} \\
%         \cmidrule(lr){1-2}
%         \cmidrule(lr){3-4}
%         \cmidrule(lr){5-6}
%         \cmidrule(lr){7-8}
%         \cmidrule(lr){9-10}
%         \cmidrule(lr){11-12}
%         \cmidrule(lr){13-14}
%         \multicolumn{2}{c}{
%         \multirow{3}{*}{Ablations}} &  \multicolumn{2}{l}{Ours} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{} \\
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{Ours} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{} & \multicolumn{2}{l}{\textbf{}} & \multicolumn{2}{l}{\textbf{}} & \multicolumn{2}{l}{}  \\
%         \multicolumn{2}{c}{} & \multicolumn{2}{l}{Ours} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{} & \multicolumn{2}{l}{\textbf{}} & \multicolumn{2}{l}{\textbf{}} & \multicolumn{2}{l}{}  \\
%         \toprule
%         \multicolumn{2}{c}{Optimal} & \multicolumn{2}{l}{\citet{yao2017dominant}} & \multicolumn{2}{l}{Rev.} & \multicolumn{2}{l}{\textbf{7.8309}} & \multicolumn{2}{l}{\textbf{9.1504}} & \multicolumn{2}{l}{\textbf{7.4774}} & \multicolumn{2}{l}{\textbf{6.72205}} \\
%         \toprule
%     \end{tabular}
% \end{table}


Introduction paragraph on why not LP.
Our MILP model for price adjustment presents several advantages over automatic mechanism design approaches using linear programming for designing auctions. (1) Deep learning offers efficient initial solutions, allowing us to significantly reducing the complexity of the MILP models.  \dcp{i don't find this prev sentence compelling---if we need a massive number of vars, then having `good initialization` doesn't obviously help}
%
We identify two particularly useful features that typically reduce >80\% of binary variables in MILP models. \dcp{`reduce`?}
 Firstly, within the network-generated menus, we observe that it is typical for only
 two or three elements to be preferred by a bidder over the IR choice. \dcp{`IR choice`?} 
We can restrict price changes to be positive \dcp{i.e., increasing prices? also, use `price` not `payment` for the menu choices ... it only becomes payment when it is actually collected from a bidder} so that the elements with negative utilities cannot become better than the IR choice after transformation, which means binary variables associated with these elements can be eliminated. Additionally, given the already low incompatibility rate at the end of training,
 the ``big-M method" is not necessary for every compatible sample. \dcp{`every compatible sample` not clear, and also before saying it's not necessary you need to say what it is used for...}
%
Rather, a simple linear constraint ensuring bidders retain their choices is sufficient. \dcp{make this more clear} (2) The MILP approach leaves undisturbed the allocation elements of a bidder's menu and just adjusts the prices associated with each element, which  further decreases computational complexity. \dcp{as per our conversations, we'll prob rewrite this to note that LPs are not DSIC. I do think we should  consider stating in intro the main scaling analysis in terms of number of agents, number of items, and Lipschitz constant for our method.}



'






\dcp{following discussion is unclear. what is this mention of supervised learning, freezing, etc? i made an earlier comment on this in prev section: we need to explain how the transforms are generalized across the entire value domain and then stick with this method in this proof} 
If we do not train the network by supervised learning to remember the price adjustments, we can choose to freeze the menu in \neighborjoint. Note that this would not violate the DSIC guarantee of our method, because the menu is still self-bid independent. The extreme of this discretized menu scheme is RochetNet that uses a constant menu. A locally constant menu in \neighborjoint~would affect the discussion of case (I). Now the utility change of any menu element (Eq.~\ref{eq:change_gap}) is upper bounded by $\epsilon/2$, which is the maximum changes in $\vv_i$. Therefore, according to Eq.~\ref{eq:no_vio_1}, we can have a safety margin $s_m=\epsilon$ to ensure that the bidders would not change their menu selection in \neighborjoint.








(Sketch) We first prove 
 by backward induction
 that the aggregate allocation of bidders at any grid point
is at most $1-n\cdot s_f$.

(Base Step) For bidder ($n-1$), the criterion for identifying  a feasible menu elements (Eq.~\ref{equ:multi_mip_feas_standard}) is the same as in the two-bidder case except for a larger safety margin ($n\cdot s_f$). Since the rest of the MILP is unchanged,
 we know that the aggregate allocation of bidder ($n-1$) and $n$ is at most $1-n\cdot s_f$.

(Inductive Step) For bidder $i<n-2$, from induction, we know that $\min\left(1, \sum_{j=i+1}^n\bm\alpha^*_{j}(\vv_{\shortn j}) \right)$ in Eq.~\ref{equ:multi_mip_feas_standard} is $\sum_{j=i+1}^n\bm\alpha^*_{j}(\vv_{\shortn j})$, and thus the transformation ensures that bidder $i$ will not select elements that make the aggregate allocation (including bidder $i$) larger than the right hand side of Eq.~\ref{equ:multi_mip_feas_standard}, which is $1-n\cdot s_f$. This finishes the proof.