\section{Introduction}

%dcp cut Standard auctions rarely allow bidders to express such non-additive preferences. Emerging to address this shortcoming, %so that more intricate valuation structures could be accommodated. 

When selling multiple items simultaneously, bidders may have complex valuations that exhibit synergies among items. For instance, some items may act as complements, making their collective value to a bidder exceed the sum of their individual values. \emph{Combinatorial auctions} (CAs) support these kinds of valuations by allowing bids on bundles of items. The need for such auctions was recognized as early as 1922~\cite{uscongress1925e}, and their formal definition dates to 1982~\cite{rassenti1982combinatorial}, where they are exemplified in the allocation of congested airport runways. Since then, CAs have proven pivotal in addressing a wide range of real-world challenges, most notably in the auctioning of spectrum licenses~\cite{cramton1997fcc,palacios-huerta24}--efforts whose far-reaching impact contributed to the awarding of the 2020 Nobel Prize in Economic Sciences~\cite{NobelPrizeEcon2020}.

Despite their  prominence, designing optimal CAs remains fundamentally challenging. As with auctions for additive or unit-demand valuations, it is typical to seek mechanisms that are (1) dominant-strategy incentive compatible (DSIC, also strategy-proof), ensuring that bidders benefit most by reporting their true values, and (2) revenue-maximizing from the auctioneer's perspective. However, even the seemingly simpler single-bidder combinatorial setting---a foundational building block for multi-bidder scenarios---still lacks a comprehensive theoretical characterization. This gap highlights the broader difficulty in developing and analyzing optimal mechanisms for general CAs.

In response to similar theoretical obstacles in additive and unit-demand valuations, researchers have explored deep learning techniques, commonly referred to as \emph{differentiable economics}~\cite{dutting2024optimal}. In particular, deep menu-based methods show promise. These methods learn a menu of options for a bidder, guaranteeing strategy-proofness, provided the menu remains self-bid independent and agent-optimizing~\citep{hammond1979straightforward}. 
For a CA, each option in a menu will correspond to a bundle of items (or a distribution on bundles) and a price. Following {\em RochetNet}~\cite{dutting2024optimal}, various methods have been developed for the single-bidder but non-combinatorial setting~\cite{curry2022differentiable,duan2023scalable,shen2019automated,dutting2024optimal}, demonstrating
the ability to rediscover auctions that are provably optimal. {\em GemNet}~\cite{wang2024gemnet} extends menu-based methods to multi-bidder scenarios and pushes the frontier for the design of multi-item auctions with additive or unit-demand
valuations. 

Unfortunately, differentiable economics has made only limited headway on the problem of optimal CA design and the fundamental challenge of handling the exponential number of bundles remains largely untouched. \citet{dutting2024optimal} deal with two items, and with a learned mechanism that does not guarantee exact DSIC. \citet{duan2024scalable} scale to 10 items, but restrict their attention to  the virtual valuation combinatorial auction (VVCA), which is not a fully general design space. \citet{ravindranath2024deep} consider a sequential CA setting. None of these studies provide a path towards DSIC and expressive, \ie, fully general, mechanisms for tens or hundreds of items, 
which presents a formidable obstacle and necessitates the development of novel methodology.
%

Focusing on the single-bidder setting, we make progress by developing a menu-based, deep learning method for DSIC and expressive CAs that scales to as many as 
150 items.   Single-bidder CAs, although less general than multi-bidder CAs, are applicable to  real-world problems. Consider  a digital content provider offering a collection of movies to a viewer, a cloud computing vendor offering different features (clock speed, number of compute units,  background execution, etc.), a monopolist seller offering  highly complementary patents to a  pharmaceutical firm, or a utility  supplier bundling electricity, gas, and renewable energy for an industrial customer. In these examples, the buyer's valuation may be influenced by strong complementarities or substitutes across items.
Moreover, and as discussed in Sec.~\ref{sec:multi-bidder}, 
this single-bidder deep mechanism design algorithm, which extends generative models to solve
the bundle scalability issue, provides a 
direction towards the automated design of multi-bidder CAs at scale.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/idea.pdf}
    \caption{(a) A menu for additive or unit-demand valuations only needs to specify allocation probabilities for each item. However, item-wise allocation probabilities are  too inflexible
   % ambiguous
    for CAs, as bidder values are specified for bundles. (b) The space complexity for representing an explicit distribution on bundles (a bundle-wise allocation) grows exponentially with the number of items. (c) We represent a bundle distribution
    through a tractable initial distribution and an ordinary differential equation (ODE). 
    % \dcp{don't capitalize Ord Diff Eq in subtitle (c)}
    \label{fig:idea}}
\end{figure}
To understand the challenge of optimal CA design, even with one bidder, 
Fig.~\ref{fig:idea} compares  menus for additive and unit-demand valuations with menus for combinatorial valuations.
For an additive bidder, it suffices to specify in a menu element (or option) the \emph{item-wise allocation probabilities} such as $[0.3, 0.4, 0.6]$ along with a price. A bidder's  expected value for the corresponding menu element can be calculated as the weighted sum of item values. A similar, item-wise  approach also works for a unit-demand bidder.
However, such item-wise allocations are ambiguous in CAs.
For example, $[0.3, 0.4, 0.6]$ could be $0.3[1,0,0]+0.4[0,1,0]+0.6[0,0,1]$ or
$0.3[1,1,1]+0.1[0,1,1]+0.2[0,0,1]$, with the same item-wise allocation  decomposed as different \emph{bundle-wise allocations}, leading to conflicting valuations. One could predefine how to interpret a marginal item-wise allocation to avoid ambiguity, for example adopting the product distribution semantics. However, a product distribution requires computation that is exponential in the number of items to evaluate the value for a bidder with a general, combinatorial valuation function. More importantly, the use of a product distribution lacks flexibility; e.g.,  $0.5[1,1,0]+0.5[0,0,1]$ cannot be represented as a product distribution. Indeed, any fixed mapping from the $m$-dimensional space ($m$ is the number of items) of item-wise allocations to the $2^m$-dimensional space of bundle-wise allocations means that many bundle-wise allocations are left uncovered. This lack of expressiveness is reflected in our experiments--learning product distributions (\eg, \bundle~in Table~\ref{tab:exp_results}) lags behind some fixed-allocation menus.


One way to achieve expressiveness would be to explicitly specify, for each menu element, an allocation probability for each possible bundle, as shown in Fig.~\ref{fig:idea} (b). However, the number of bundles grows exponentially with the number of items, making it intractable to learn these bundle-wise allocations. Aside from menu-based approaches, other state-of-the-art deep learning methods for mechanism design face a similar challenge: they do not suggest a way to represent bundle-wise allocation distributions efficiently and flexibly.
%such as RochetNet \twadd{and RegretNet}~\cite{dutting2024optimal}%with an explicit, enumerative representation. 
%\dcp{not following the intent of this prev sentence. is it getting at AMAnet, RegretNet, etc?} \tw{Yes. In the figure, we discuss menu-based methods. I want to say that other methods like RegretNet also face similar questions.}
%

%
% Bundle distributions matter when we calculate the utility of menu elements, which is the sum of bundle values weighted by bundle allocation probabilities.
%\dcp{I think the connection between $\vs_0$ and $\vs_t$ is conceptually confusing. Do we think about applying this transform to any $\vs_0$ in $\mathbb{R}^m$, with $\vs_t$ here really being the $\vs_t$ that ``is generated from applying the ODE to $\vs_0$"; that is, is this really $\vs_t(\vs_0)$, where we don't write it this way because it would be too cumbersome?} 

In this paper, we solve this problem by avoiding the need to directly represent and learn an exponentially high-dimensional specification of a distribution on bundles. Instead, we represent a distribution on bundles by a tractable and low-dimensional \emph{initial distribution}, $\alpha_0(\vs_0)$, on {\em initial bundle variables}, $\vs_0\in \mathbb{R}^m$,
and an ordinary differential equation (ODE): $d\vs_t = \varphi(t, \vs_t) dt$. The ODE operates on \emph{bundle variables}, $\vs_t\in\mathbb{R}^m$, through the {\em vector field} $\varphi(t, \cdot)$. Here $m$ is the number of items and $t\in[0,T]$ is the {\em ODE time}. A feasible bundle corresponds to a bundle variable where the entries are all 0s or 1s. Formally, $\vs_t$ is generated from a sample $\vs_0$ by applying the ODE: $\vs_t(\vs_0)=\vs_0+\int_0^t \varphi(\tau,\vs_\tau) d\tau$. We omit the dependence on $\vs_0$ and write $\vs_t$ for simplicity.
In this way, the ODE transforms the initial distribution $\alpha_0(\vs_0)$ to a final distribution $\alpha_T(\vs_T)$ at time $T$. The idea is to (1) train the vector field so that the support of the final distribution $\alpha_T(\vs_T)$ corresponds to feasible bundles; (2) fix this trained vector field $\varphi(t, \cdot)$, and learn a different initial distribution $\alpha_0(\vs_0)$ (therefore a different final distribution over bundles) and price for each menu
element, so that the menu maximizes expected revenue. 
\if 0
\dcp{clarify: I think it operates on the distribution by transforming the random variable $\vs_t$. is this the right
way to think about it?}
\dcp{add something like `A feasible bundle corresponds to a bundle variable where the entries are all 0s or 1s.`}.
\dcp{should we say something like $\vs_T$ is the random variable corresponding to 
the bundle variable at time $T$?}
\tw{This para has been reworked. Hope it can resolve the concerns.}
The generation of the final distribution is governed by the ODE. 
\fi


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/flow_init.png}
    \caption{Evolution of the vector field $\varphi$ (represented by blue curves) during the first stage of menu training: \emph{Flow Initialization}. The x- and y-axes represent the bundle variables for two of items. $x=1$ means item A is in the bundle, and $y=1$ means item B is in the bundle. We employ an ODE $d\vs_t = \varphi(t, \vs_t) dt$ to generate the final distribution $\alpha_T(\vs_T)$ (a distribution over bundles), represented by blue dots, from a simple initial distribution $\alpha_0(\vs_0)$, represented by green dots. During the first stage, $\alpha_0(\vs_0)$ is fixed as a mixture-of-Gaussian distribution. Dot opacity represents probability density. The aim of this first stage is to train the vector field so that the final distribution has all feasible bundles as its support (see Sec.~\ref{sec:viz}).
    \label{fig:flow_init}}
\end{figure}
This method draws inspiration from generative AI models, such as {\em diffusion models}~\cite{ho2020denoising, kadkhodaie2023generalization,song2021score,song2019generative,yang2025policy} and, in particular, {\em continuous normalizing flow}~\cite{chen2018neural,liu2022flow,lipman2022flow}.  We thus call the  method \name~to emphasize the core idea of using continuous normalizing flow to model bundle distributions. Prior work has successfully shown how to transform simple distributions such as the Gaussian distribution to complex \emph{target distributions} such as natural images~\cite{rombach2022high,esser2024scaling}, language~\cite{lou2024discrete}, and videos~\cite{stabilityAI2023}. In these generative AI tasks, the target distribution is known and observed as the data distribution in large-scale pre-training datasets. Our technical novelty is to extend generative models to solve optimization problems, seeking a bundle distribution (and price) for
each menu element that optimizes expected revenue. 
There is no known target distribution in our work.
%\dcp{somewhere in paper explain how novel these uses of 
%diffusion nets for combinatorial objects such as bundles are -- what is the technical innovation?}\tw{Add such an explanation here.}

% Unlike these kinds of generation models,  we face an optimization problem and with no known final distribution as a prior. 
% \dcp{`with no known final distribution as a prior` unclear}
% Nevertheless, as is standard in this literature, we say the ODE induces a \emph{flow} $\phi_t(\vs_0)$ that gives  solution to the ODE at time $t$.  \dcp{connection between `flow`, vector field, and distribution $\alpha_T$ unclear, also why `Nevertheless`}


Central to our method is that the distribution's evolution under the ODE
is governed by the {\em Liouville equation}~\cite{liouville1838note}:
%
\begin{align}
    \log \alpha_t(\vs_t) = \log \alpha_0(\vs_0) -\int_0^t \nabla\cdot\varphi(\tau, \vs_\tau) d\tau.\label{equ:demo_liouville}
\end{align}
We design the functional form of the vector field $\varphi(t, \cdot)$ so that the integral of its divergence $\nabla\cdot\varphi(t, \cdot)$ is easy to compute, thereby allowing efficient menu optimization. The initial distribution $\alpha_0(\vs_0)$ is chosen to be a simple distribution. As the first stage of menu learning, we fix $\alpha_0(\vs_0)$ to a mixture-of-Gaussian distribution 
and train a single vector field $\varphi(t, \cdot)$ to transport any initial variable $\vs_0$ to a feasible bundle at final time $T$. An illustration of how the vector field evolves during this first training stage is shown in Fig.~\ref{fig:flow_init}.
%\tw{I add this para back with more details. I think it is important as it describes the first step of menu training and some novelties.}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/flow_train.png}
    \caption{Visualization of the second stage of training: \emph{Menu Optimization}. The figure presents snapshots of four menu elements (organized in columns) at different training iterations (organized in rows), showing the bundle distribution and price for each element, along with the test-time auctioneer revenue from the entire menu at the corresponding iteration. The x- and y-axes represent the bundle variables for two of items. $x=1$ means item $A$ is in the bundle, and $y=1$ means item $B$ is in the bundle. We fix the vector field (blue curves) and update initial distributions of elements to manipulate distributions over bundles (refer to Sec.~\ref{sec:viz}).% \dcp{typo, revenue in bottom row? its the same for every element.} \tw{they are revenue of the menu :)} \dcp{but then why are the rev numbers different in each column for other rows? i thought you were showing the per-element revenue.} \tw{Because train iterations are different for other rows. E.g., we have iteration 450, 270, 150, 210 in the 3rd row (I selected critical changes during training, and they happen at different times). Only the last row has the same iteration.}
    \label{fig:flow_train}}
\end{figure}

%\dcp{[this gives a menu element]} 
With this vector field fixed, as a second stage we then train each element in a menu so that the menu maximizes expected auction revenue.  The advantage of our flow-based method is that, during both training and testing, we can efficiently calculate the utility of each menu element $k$ by substituting the bundle distribution $\alpha_T^{(k)}(\vs_T)$ with $\alpha_0^{(k)}(\vs_0)$ as described in Eq.~\ref{equ:demo_liouville}.
We re-parametrize the initial distribution $\alpha_0^{(k)}(\vs_0)$ so that the resulting bundle distribution, $\alpha_T^{(k)}(\vs_T)$, becomes trainable.
To ensure DSIC, we design the initial distribution $\alpha^{(k)}_0(\vs_0)$ to have finite support. Specifically, we use mixture-of-Dirac distributions.
This enables precise reconstruction of the bundle distribution $\alpha_T^{(k)}(\vs_T)$ and thus the utility of a menu element by enumerating the support of $\alpha_0^{(k)}(\vs_0)$.

For optimization, each menu element shares a common vector field and has separate trainable parameters for its price and  initial distribution (mixture weights and support points of the mixture-of-Dirac distribution).  
The gradients of the revenue-maximizing loss backpropagate through the ODE back to parameters of $\alpha_0^{(k)}(\vs_0)$, guiding updates that increase expected revenue. 
As a demonstration of  this second stage, we illustrate changes of the initial distribution and the corresponding bundle  distribution for each of four different menu elements, as well as corresponding prices and auction revenue in Fig.~\ref{fig:flow_train}. 
%\dcp{do we actually use mixture-of-Dirac in second stage? Is it clearer if explained this way?} \dcp{suggest using $^{(k)}$ through this paragraph to distinguish from the initial distribution used in the first stage} \tw{I changed the sentence order and add some transitions to make this part clearer.}

% we set an almost-zero variance in the components of the mixture-of-Gaussian distributions, effectively making them behave like
% \tw{or mixture weights and means of the narrow mixture-of-Gaussian distribution}
% (we will make use of the , but other distributions like a mixture-of-Dirac also work).  , %\dcp{will be a bit confusing, given we talk about mix-of-gaussian. worth a footnote to clarify the basic idea for how it becomes, in effect, finite?}%

%In principle, our method enjoys full expressiveness when the support size of the initial distribution is comparable to the number of bundles.
Although the number of bundles in the support of the bundle distribution $\alpha^{(k)}_T(\vs_T)$ for menu element $k$ is 
at most the size of the support of the initial distribution $\alpha^{(k)}_0(\vs_0)$, what this representation achieves
is that the bundle distribution $\alpha^{(k)}_T(\vs_T)$ can be flexibly learned.
In practice, we observe that even a small support size for bundle distribution $\alpha^{(k)}_T(\vs_T)$ yields strong performance across various settings. Moreover, the formulation of our method does not rely on any  assumptions regarding the bidder's valuation function, for example that the valuation function can be represented on a small number 
of bundles, and thus exhibits flexibility, applicable to a large range of CA problems.
%We then conduct comprehensive experiments to thoroughly validate these attributes.
%
%\dcp{add a couple more sentences to emphasize flexibility/expressiveness}
%

We evaluate \name~using single-bidder instantiations of CATS \citep{leyton2000towards}, a widely recognized CA testbed. Given the absence of established deep methods capable of extending to the settings considered in this paper, we benchmark against the following DSIC baselines. (1) \bundle: An adaptation of RochetNet~\cite{dutting2024optimal} with a menu of item-wise allocations (and prices) that interprets item-wise allocations as product distributions. (2) \bigbundle: This baseline fixes allocations in all menu elements, favoring bundles with the highest item counts. Prices are learned using the same gradient-based method as in RochetNet. (3) \smallbundle: Similar to \bigbundle\ but in addition to the grand bundle, which includes all items, it also includes bundles with minimal item allocations. (4) \grandbundle: Employs a grid search to determine a price for the grand bundle. In all cases, the menu element with the largest utility, if non-negative, is assigned to the bidder. 
These baselines represent progressively restrictive constraints on the flexibility of menus.


% The training time is also considerably reduced: our method converges 1.3-21.4$\times$ faster than the deep learning baselines.
Experimental results demonstrate that our method consistently and significantly outperforms all baselines across all benchmark settings and scales effectively to auctions involving up to 150 items. Specially, for auctions with 50 to 150 items, \name~achieves 1.11$-$2.23$\times$ higher revenue. This enhanced revenue performance does not compromise training efficiency. On the contrary, compared to the baseline \bundle~that also learns allocations in menu elements, our method typically requires 3.6$-$9.5$\times$ fewer training iterations and can reduce training time by about 80\% in settings with 50 or 100 items.

% --up to a halving of revenue
A critical observation is noted when we vary the support size of initial distributions, $D$, from 2 to 1. This change results in a considerable revenue decline in \name, causing its performance to become comparable to that of the baselines, with a reduction of up to 52.7\% at its most severe. Since when $D=1$ each menu element deterministically assigns a single bundle, these results underscore the importance of allowing randomized distributions over bundles in differentiable economics for CA settings, a capability uniquely enabled by our method.
% \dcp{excepting \grandbundle\ presumably! also, this is a bit susprising to me for \bigbundle\ and \smallbundle. \bundle\ doesn't surprise me.}

% Since its release in 2000, CATS has served as the standard benchmark in CA research, including five tasks inspired by economically motivated, real-world scenarios. In the two decades following CATS, spectrum auctions have emerged as a pivotal application of CAs, and SATS models settings tailored to the specific challenges of spectrum auctions. 

% These results show the flexibility, effectiveness, and scalability of \name.


% While our work primarily focuses on the traditional combinatorial auction setting rather than spectrum auctions, we conducted experiments using both CATS and SATS to comprehensively evaluate our method.
% specific applications