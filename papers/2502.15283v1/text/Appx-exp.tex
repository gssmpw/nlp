% \section{Normalizing Flow}

% A normalizing flow model provides a transportation from a distribution to another by applying a sequence of functions that are not necessarily the same. As shown in Fig.~\ref{fig:normalizing_flow}, for round $i\in\{1,\cdots, T-1\}$, the input to the function is $z_i\in\mathbb{E}^m$ \dcp{$\mathbb{R}^m$?}
% sampled from the distribution $p_i(z_i)$. After applying the function, the distribution will be transformed to $p_{i+1}(z_{i+1})$. \dcp{$t$ not $i$ in main body}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/flow.pdf}
%     \caption{A normalizing flow model that transport a simple distribution to a complex distribution.\label{fig:normalizing_flow}}
%     \Description[Normalizing flow.]{A normalizing flow model that transport a simple distribution to a complex distribution.}
% \end{figure}

% We are interested in the closed-form expression of $p_{i+1}(z_{i+1})$. According to the change of variable theorem and assuming  the neural network is invertible, we have
% \begin{align}
%     p_{i+1}(z_{i+1}) = p_i\left(f^{\shortn 1}(z_{i+1})\right)\left|\text{det}\frac{df^{\shortn 1}}{dz_{i+1}}\right|= p_i\left(z_i\right)\left|\text{det}\frac{df^{\shortn 1}}{dz_{i+1}}\right|,
% \end{align}
% where $\text{det}\frac{df}{dz}$ is the Jacobian determinant of function $f$. The inverse function theorem tells us
% \begin{align}
%     \frac{df^{\shortn 1}}{dz_{i+1}} = \frac{dz_i}{dz_{i+1}} = \left(\frac{dz_{i+1}}{dz_i}\right)^{\shortn 1} = \left(\frac{df}{dz_i}\right)^{\shortn 1}.
% \end{align}
% It follows that $p_{i+1}(z_{i+1})=p_i\left(z_i\right)\left|\text{det}\left(\frac{df}{dz_i}\right)^{\shortn 1}\right|$. For an invertible function $A=\frac{df}{dz_i}$, we have $1=det(I)=det(AA^{\shortn i})=det(A)det(A^{\shortn 1})$, so $det(A^{\shortn 1})=det(A)^{\shortn 1}$. Therefore, we have $p_{i+1}(z_{i+1}) = p_i\left(z_i\right)\left|\text{det}\frac{df}{dz_{i+1}}\right|^{\shortn 1}$, and
% \begin{align}
%     \log p_{i+1}(z_{i+1}) = \log p_i\left(z_i\right) -\log \left|\text{det}\frac{df}{dz_{i+1}}\right|.
% \end{align}
% We can apply this repeatedly and obtain
% \begin{align}
%     \log p(z) = \log p_T(z_T) = \log p_0\left(z_0\right) -\sum_{i=0}^{T-1}\log \left|\text{det}\frac{df}{dz_{i+1}}\right|.
% \end{align}


% \section{Likelihood derivation}

% \subsection{Linear}

% When $\varphi(t, s_t) = \sigma(t) Q s(t)$, we have

% \begin{align}
%     \nabla\cdot \varphi(t, s(t)) = \sum_{i=1}^d \frac{\partial \varphi_i}{\partial s_i(t)}
%     = \sum_{i=1}^d \frac{\partial}{\partial s_i(t)} \sigma(t) Q_i s(t) = \sum_{i=1}^d  \sigma(t) Q_{ii} = \sigma(t)\Tr(Q)
% \end{align}