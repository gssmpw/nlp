\section{The Flow-Based Combinatorial Auction Menu Network}

As discussed in the introduction, the major challenge in learning menus for CAs is 
to provide an expressive representation of distributions over bundles
to associate with each menu element while retaining efficiency, so that
the exponential number of possible bundles does not become a bottleneck.
%dcp cut Given that the number of bundles grows exponentially in the number of items, this requirement greatly increases the complexity of the menu representation. 
Moreover, training these representations adds another layer of difficulty: the menu must be not only concise but also easily differentiable to support training. 

\subsection{Menu representation}

Our key idea, following from score-based diffusion models and continuous normalizing flow, is to construct a concise and differentiable representation of a bundle distribution by modeling it through the solution of an ordinary differential equation (ODE). Specifically, the $k$th menu element generates its bundle distribution by the ODE,
%
\begin{equation}
    d\vs^{(k)}_t = \varphi^{(k)}(t,\vs^{(k)}_t) dt,\label{equ:de}
\end{equation}
%
for a suitable choice of vector field $\varphi^{(k)}$.
%
Here, we refer to $\vs^{(k)}_t\in \mathbb{R}^m$ as the \emph{bundle variable at time $t$}, where $m$ is the number of items. At time $T$, we require that a bundle variable $\vs^{(k)}_T$ represents a meaningful bundle, so that all entries are 0s or 1s,
and we adopt $\alpha^{(k)}_T(\vs^{(k)}_T)$ to denote the corresponding allocation probability. 
For simplicity, we omit the superscript $(k)$ when this is clear from the context.

By the  Liouville equation~\cite{liouville1838note}, the probability density at $T$ derived from Eq.~\ref{equ:de} satisfies:
\begin{align}
    \log \alpha_T(\vs_T) = \log \alpha_0(\vs_0) - \int_0^T \nabla\cdot \varphi(t, \vs_t) dt,\label{equ:liouville}
\end{align}
where $\alpha_0(\vs_0)$ denotes the initial distribution at time $0$, on initial bundle variables $\vs_0$, and $\nabla\cdot \varphi(t, \vs_t)$ is the divergence of $\varphi$.   Eq.~\ref{equ:liouville} is applicable to any $\vs_0$, and a bundle variable $\vs_t$ is generated from $\vs_0$ by $\vs_t(\vs_0)=\vs_0+\int_0^t \varphi(\tau,\vs_\tau) d\tau$. For clarity, we omit the explicit dependence on $\vs_0$ and simply write $\vs_t$.
%\dcp{as per my comment in the intro, it may be worth being a bit more pedantic here, and explaining that this can be applied to any $s_0$, and that $s_T$ is the rv at time $T$ that corresponds to this particular $s_0$.} 

\textbf{Training scheme}. Both the vector field $\varphi$ and the initial distribution $\alpha_0$ can influence the final distribution $\alpha_T$. 
Our method proceeds in two stages, involving the training of  each of
these two components in turn: 

$\quad$ \emph{(1) Flow Initialization.} We  fix  the initial distribution $\alpha_0$ and train the vector field $\varphi(t, \cdot)$ so that the final distribution, $\alpha_T$,
provides a reasonable coverage over bundles. 

$\quad$ \emph{(2) Menu Optimization.} We  fix the vector field from Stage 1, and backpropagate the revenue-maximizing loss through the flow to update the initial distribution $\alpha_0^{(k)}$ 
for each menu element $k$.

% by learning the weights $w_d^{(k)}$ and means $\bm\mu_d^{(k)}$ associated with the menu element. 

$\varphi$ and $\alpha_0(\vs_0)$ play a crucial role in maintaining a concise and easily differentiable representation and ensuring efficient training. We next propose specific functional forms for these two components that meet these criteria.

\textbf{Vector field}. We adopt the following functional form for the vector field,
%
\begin{align}
    \varphi(t, \vs_t;\xi,\theta) = \eta(t;\xi) Q(\vs_0;\theta) \vs_t,\label{equ:varphi}
\end{align}
where $Q: \mathbb{R}^{m}\rightarrow\mathbb{R}^{m\times m}$, written as a function of $\vs_0$, and the scalar factor $\eta:\mathbb{R}\rightarrow \mathbb{R}$, written as a function of  the ODE time $t\in[0,T]$, are neural networks with learnable parameters $\theta$ and $\xi$, respectively. 
%
%
We  omit dependence on $\theta$ and $\xi$ when the context is clear. This formulation's advantage becomes apparent when we consider its divergence:
%
\begin{align}
    \nabla\cdot \varphi(t,\vs_t) &= \sum_{i=1}^m \frac{\partial \varphi_i}{\partial s_{t,i}}
    = \sum_{i=1}^m \frac{\partial}{\partial s_{t,i}} \eta(t) Q_i(\vs_0) \vs_t = \sum_{i=1}^m  \eta(t) Q_{ii}(\vs_0) \\
    & = \eta(t)\Tr[Q(\vs_0)].
\end{align}

Here, $\varphi_i$ and $s_{t,i}$ are the $i$th element of $\varphi$ and $\vs_t$, respectively, $Q_i$ is the $i$th row of $Q$, and $Q_{ii}$ is the $i$th diagonal element of $Q$. Thus, the probability density at $T$ becomes
\begin{align}
    \log \alpha_T(\vs_T) = \log \alpha_0(\vs_0) - \Tr[Q(\vs_0)]\int_0^T \eta(t) dt.\label{equ:likelihood}
\end{align}

The integral in Eq.~\ref{equ:likelihood} 
is tractable as it only involves a scalar function, instead of bundle variables.
We can efficiently estimate this integral by time discretization. 

\textbf{Initial distribution}. In Stage 1, we use a mixture-of-Gaussian distribution for
the initial distribution $\alpha_0(\vs_0)$ on bundle variables $\vs_0$, with
%
\begin{align}
    \vs_0 \sim\sum_{d=1}^D w_d \mathcal{N}(\bm\mu_d, \sigma_d^2\mI_m),\label{equ:init_dist}
\end{align}
where, for $D$ components,
$\bm\mu_d\in\mathbb{R}^m$, $\sigma_d\in\mathbb{R}_{>0}$, $\mI_m$ is the $m\times m$ identity matrix, and $w_d\geq 0$ are weights satisfying $\sum_{d=1}^D w_d=1$. In Stage 2, as discussed later, we ensure DSIC by adopting a mixture-of-Dirac distribution, which is practically implemented by setting a very small variance $\sigma_d$ in a mixture-of-Gaussian distribution.

% (1,1,\cdots,1)^\Tau We expect the flow to transport a sample $z_0$ to a bundle $S\in\{0,1\}^m$.

\subsection{Stage 1: Flow initialization}

The aim of the first stage is to guarantee that the flow can transport any initial bundle variable $\vs_0$ to a feasible bundle $S\in 2^M$.  We use $\vs=(\mathbb{I}{\{i\in S\}})$ to denote the vectorization of set $S$, i.e., the $i$-th component of $\vs$ is 1 if item $i$ is in $S$ and 0 otherwise.

In practice, numerical issues make it challenging to exactly obtain an feasible bundle $\vs$; i.e., a bundle variable
with only 0s and 1s. To account for this, we allow a small region around $\vs$ to be approximated as $\vs$ by modeling the bundle as a Gaussian variable,
%
\begin{align}
    S_{\sigma_z} = \mathcal{N}(\vs, \sigma_z^2\mI_m).
\end{align}
% the random variable representing the bundle variables at $t=0$.  where $Z_0$ is 

We train the vector field networks using rectified flow (\citet{liu2022flow}, Eq.~\ref{equ:rf}). For this  stage, we fix the initial distribution $\alpha_0(\vs_0)$ to a mixture-of-Gaussian model $\alpha_0(\vs_0)=\sum_{d=1}^D w_d \mathcal{N}(\bm\mu_d, \sigma_d^2\mI_m)$ with $D$ components.
We  define
$\alpha_T(\vs_T)$ as a uniform mixture-of-Gaussian model, with  components centered around each feasible bundle, and
$\alpha_T(\vs_T)=\frac{1}{2^m}\sum_{S\in 2^M} \mathcal{N}(\vs, \sigma_z^2\mI_m)=\frac{1}{2^m}\sum_{S\in 2^M}S_{\sigma_z}$.
This target distribution only applies in Stage 1, where it serves to encourage a balanced coverage of the final distribution over feasible bundles. In Stage 2, we have an optimization problem, and there is no longer a fixed target distribution.
%

We  follow the idea of  rectified flow, and define the {\em flow training loss} as
%
\begin{align}
    \mathcal{L}_{\textsc{Flow}}(\theta,\xi) =& \mathbb{E}_{(\vs_0,\vs_T)\sim (\alpha_0,\alpha_T), t\sim [0,T]} \left[\|(\vs_T-\vs_0)-\varphi(t, \vs_t; \theta,\xi)\|^2\right], \label{equ:flow_loss}\ \ \mbox{where}\\
    & \vs_t = t\cdot \vs_T + (1-t)\cdot \vs_0,\\
    & \varphi(t, \vs_t; \theta,\xi)=\eta(t;\xi)Q(\vs_0;\theta)\vs_t.
\end{align}

% \dcp{i think you want $\vs_T$ not $\vs_1$ in eqn 16}
This loss is used to update the neural networks $Q$ and $\eta$ to encourage the vector field at interpolated points $\vs_t$ to point from $\vs_0$ to $\vs_T$.
%\dcp{I think you can say a bit more here---you've defined $Z_1$ to be a uniform distr over all bundles, so this is trying to get coverage of bundles.}
%
The expectation in the flow training loss is taken over $(\alpha_0,\alpha_T)$, 
but directly sampling from $\alpha_T$ is intractable as it involves $2^m$ bundles.

Crucially, using a flow-based representation provides a workaround. We first draw $\vs_0\sim \alpha_0$, which is straightforward given that $\alpha_0$ comprises a manageable number of components ($D$). We then round $\vs_0$ to
the nearest feasible bundle, $\vs=\mathbb{I}(\vs_0\ge 0.5)\in\{0,1\}^m$,
and sample $\vs_T\sim \mathcal{N}(\vs, \sigma_z^2\mI_m)$. This approach underscores an advantage of deep learning. Although we cannot enumerate all possible bundles, the generalization ability of neural networks allows for learning the mapping from $\alpha_0$ to $\alpha_T$ given enough training samples.


\subsection{Stage 2: Menu optimization}\label{sec:method:opt}

In the second stage, we train the menu to seek to maximize the expected revenue for the auctioneer. For each menu element $k$,  the 
trainable parameters comprise the price $\beta^{(k)}$, as well as the parameters $w_d^{(k)}$ and $\bm\mu_d^{(k)}$ that define the initial distribution $\alpha^{(k)}_0$ on the bundle variable. 
The vector field $\varphi$ is  fixed in this stage and shared
among all menu elements.

Given a bidder with a value function $v$, the payment to the auctioneer is the price associated with the menu element that provides the highest utility to the bidder. 
Thus, computing the utility of each menu element is central to evaluating the revenue objective.
We always maintain a null menu element (zero allocation, zero price), which ensures 
  individual rationality (IR), so that the bidder has  non-negative expected
utility. 

Computing the expected 
utility corresponding to a menu element with bundle distribution $\alpha^{(k)}$ 
is intractable when done with a direct calculation,
because
%
\begin{equation}
    u^{(k)}(v) = \sum_{S\in 2^M} \alpha^{(k)}(S)v(S)
\end{equation}
%
requires enumerating $2^m$ bundles for a general valuation function.
However, with our flow-based representation, we can get the bundle allocation probabilities by applying the flow to the initial distribution. Specifically, we have
\begin{align}
    u^{(k)}(v) = \mathbb{E}_{\vs_0\sim \alpha^{(k)}_0, \vs=\mathbb{I}(\phi(T,\vs_0)\ge 0.5)} \left[v(\vs) \alpha^{(k)}_0(\vs_0)\exp\left(-\Tr[Q(\vs_0)]\int_0^T \eta(t) dt]\right)\right],\label{equ:u}
\end{align}
by applying the exponential operation to both sides of Eq.~\ref{equ:likelihood}. Here, $\phi(T,\vs_0)$ is the solution of the ODE solved by forward Euler,
%
\begin{align}
    \phi(T,\vs_0) = \vs_0 + Q(\vs_0)\int_0^T \eta(t)\vs_t dt,\label{equ:phi_T_s_0}
\end{align}
%
and $\vs=\mathbb{I}(\phi(T,\vs_0)\ge 0.5)$ is the rounded final bundle. Due to its simple form, a modern ODE solver can efficiently solve the ODE (Eq.~\ref{equ:phi_T_s_0}) in just a few steps. Therefore, the calculation of $u^{(k)}(v)$ becomes tractable when we make the initial distribution simple.

% Here we see another advantage of our choice for the functional form of the vector field $\phi$ (Eq.~\ref{equ:varphi}): $u^{(k)}(v)$ can be determined by the initial distribution, without calculating bundle variables at $t>0$.  Moreover, 
% \tw{TODO: double check the reference.}  \dcp{XX STILL TO DO! XX}
% designed for diffusion models 
%

To ensure DSIC, we need to accurately calculate the expectation in Eq.~\ref{equ:u} to get the exact
utility to the bidder. We accomplish this by employing a mixture-of-Dirac distribution as the initial distribution, which has finite support. To implement this in practice, we set, for Stage 2 only, a very small variance to the Gaussian components in Eq.~\ref{equ:init_dist}, with $\sigma_d=1e\shortn 20$ for every component $d$. In this way, the utility can be obtained by enumerating over the finite support of the initial distribution:
% In this way, each Gaussian in the mixture is effectively a Dirac delta, 
\begin{align}
    u^{(k)}(v) = \sum_{d=1}^D \left[v(\vs(\bm\mu^{(k)}_d)) \alpha^{(k)}_0(\bm\mu^{(k)}_d)\exp\left(-\Tr[Q(\bm\mu^{(k)}_d)]\int_0^T \eta(t) dt]\right)\right],\label{equ:u_finite}
\end{align}
where $\vs(\bm\mu^{(k)}_d)=\mathbb{I}(\phi(T,\bm\mu^{(k)}_d)\ge 0.5)$. 
That is, the support of $\alpha^{(k)}_0$ 
consists, in effect, of the set of means, one for each component. %\dcp{this is where I think we could just say this is a mixture-of-Dirac and avoid this small variance mix-of-Gaussian discussion}. \tw{In Sec. 3.1, before we introduce Stage 1 and 2, we said the initial distribution is mixture-of-Gaussian. I suggest a minimal modification (by setting different variance in the two stages) here, but am willing to rework both Sec. 3.1 and 3.3.} 
It is worth noting that $D$ in Eq.~\ref{equ:u_finite} does not need to be the same $D$ as in Stage 1, and it could even vary across menu elements.
% \dcp{also, is it worth commenting that this $D$ does not need to be the same $D$ as in Stage 1? I suppose in principle it could even vary across elements...}
%\dcp{a bit confusing, at least to me --- I don't see where you sample different weighted combinations of the means?} \tw{$\alpha_0(\bm\mu^{(k)}_d)$ in Eq.~\label{equ:u_finite}?}

In this Stage 2, we fix the vector field $\varphi$ ($Q$ and $\eta$ networks) in Eq.~\ref{equ:u_finite} and update trainable parameters associated with the price and 
initial distribution $\alpha_0^{(k)}$ for each menu element $k$ 
during menu optimization, \ie, $\beta^{(k)}$, $\{w^{(k)}_d\}_{d=1}^{D}$, and $\{\bm\mu^{(k)}_d\}_{d=1}^{D}$. Therefore, given a set of bidder valuations $\mathcal{V}$,
the {\em revenue-maximization loss} is defined as
%
\begin{align}
    \mathcal{L}_{\textsc{Rev}}\left(\{\beta^{(k)}\}_{k=1}^K, \bigl\{ w_d^{(k)} \bigr\}_{\substack{d\in[D] \\ k\in[K]}},\bigl\{\bm\mu_d^{(k)} \bigr\}_{\substack{d\in[D] \\ k\in[K]}}\right) = -\frac{1}{|\mathcal{V}|}\sum_{v\in \mathcal{V}}\left[\sum_{k\in[K]}z^{(k)}(v)\beta^{(k)} \right],
\end{align}
%
where $z^{(k)}(v)$ is obtained by applying the differentiable SoftMax function to the utility of the bidder being allocated the $k$-th menu choice, i.e.,
%
\begin{align}
    z^{(k)}(v) = \mathsf{SoftMax}_k\left(\lambda_{\textsc{SoftMax}}\cdot u^{(1)}(v),\ldots,\lambda_{\textsc{SoftMax}}\cdot u^{(K)}(v)\right),\label{equ:softmax_in_loss}
\end{align}
%
where $\lambda_{\textsc{SoftMax}}$ is a scaling factor, and $u^{(k)}(v)$ is calculated by Eq.~\ref{equ:u_finite}.
%\dcp{do you want $\mathsf{SoftMax}_K$ not $\mathsf{SoftMax}_k$?} \tw{I used the notation of the JACM paper. $k$ acts like an index here?}
When optimizing $\mathcal{L}_{\textsc{Rev}}$, the gradients with respect to $\beta^{(k)}$ are straightforward to compute. Moreover, although $Q$ remains fixed, gradients can still backpropagate through this network to update its input,
which is $\bm\mu^{(k)}_d$. Gradients also flow through $z^{(k)}$ back into $\alpha_0$, enabling updates to the mixture weights $w^{(k)}_d$. All these gradients are automatically handled by standard deep learning frameworks.



% The probability of
% \begin{align}
%     \bm\alpha^{(k)}_{ij} = \sum_{z_1=f(z_0)=S_j} D_0(z_0)
% \end{align}

\subsection{Discussion}\label{sec:multi-bidder}

%\dcp{mention somewhere, perhaps here, that softmax becomes hardmax at test time to give DSIC}

\textbf{DSIC}. The seminal work by \citet{hammond1979straightforward} establishes necessary and sufficient conditions for a strategyproof menu-based auction: (1) Self-bid independent: the menu is independent of the bidder's bid; (2) Agent-optimizing: the bidder is assigned the menu element that maximizes their utility. As we analyze here, our method satisfies these two properties.

In \name, all element prices, as well as bundle allocations, which depend on initial distributions and the vector field, are trained on values sampled from the distribution $F$, without using any information about the bidder's specific valuation. Therefore, menus learned by \name~are self-bid independent. 
%
As discussed in Sec.~\ref{sec:method:opt}, we require the initial distribution for each menu
element to have finite support, which means that the bundle distribution for each menu element 
can be reconstructed without any approximation error.
This guarantees exact utility calculation for every menu element. Moreover, unlike the SoftMax in Eq.~\ref{equ:softmax_in_loss}, we use hard argmax at test time, thereby selecting the menu element with the highest utility to the bidder. In this way, \name\ is strictly agent-optimizing.

\textbf{Expressiveness}. In Stage 1, we initialize the vector field $\varphi$. After this stage, given appropriate initial distributions, the final distribution can in principle cover all $2^m$ bundles and is trained to seek to achieve this.
In Stage 2, since the initial distribution for a menu element has finite support of size $D$, the bundle distribution for a menu element is also limited to finite support of size $D$. 
What is crucial, though, is that we can learn which (up to) $D$ bundles are represented in the distribution
that corresponds to a menu element. In practice, we find that a bounded  $D$
that is much smaller than $2^m$ still gives very high expected revenue.


%\dcp{mention somewhere that whereas the realized distributions per menu element are limited to $D$ bundles, with $D$ bounded or at least small compared to $2^m$, the final distribution achieved
%from the vector field in Stage 1 can, in principle, have support on all $2^m$ bundles.}

\textbf{Extension to multi-bidder settings}. By providing an expressive and concise %\dcp{this also said `concise and flexible` but I think concise is the same as efficient and flexible the same as expressive}
representation of single-bidder menus for the CA setting, our method opens up the possibilities of developing a general DSIC multi-bidder CA mechanism. A principled approach is to 
adapt the idea of GemNet~\cite{wang2024gemnet}. 
%
First, we can learn a separate \name~menu for each bidder. The modification in the network architecture is that these menus should now also depend on other bidders' bids $\vb_{\shortn i}$. To achieve this, we can condition the vector field, specifically the $Q$ and $\sigma$ networks, on $\vb_{\shortn i}$ by concatenating them to the inputs. For the price of each menu element, we can model them as the output of a neural network whose input is $\vb_{\shortn i}$. During training, we can also introduce a compatibility loss in the same way as that used in GemNet. This loss penalizes any over-allocation of items in the selected agent-optimizing elements from individual menus.

The major challenge in adapting GemNet to the CA setting  
arises during the post-training stage of GemNet, which adjusts prices of menu elements so that there is provably never any over-allocation of items. For this, GemNet constructs a grid over the space of bidder values. On each grid point, GemNet formulates a mixed-integer linear program (MILP) to adjust prices to ensure that, the utility of the best  element that is compatible with the choices of others in the sense of not over-allocating items is larger than that of all other elements by a safety margin. These safety margins prevent an incompatible menu element from being selected in the regions between grid points. Although the concise \name~menu representation, in principle, enables this MILP to 
be directly adapted to the combinatorial setting and used to adjust \name~menus to obtain a DSIC CA, the main issue is that the space of bidder values exhibits exponential dimensionality in the CA setting, resulting in an excessively large grid. Reducing this complexity represents the crucial 
remaining step in future work to enable 
a general, DSIC, and multi-bidder CA mechanism.