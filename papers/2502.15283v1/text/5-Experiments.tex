\section{Visualization on a didactic example}\label{sec:viz}

We present an example to visualize the training process of our method. For this, we adopt the \texttt{Regions} environment from the CATS testbed and consider uniform distributions with five items. 

Fig.~\ref{fig:flow_init}, already presented in the paper, shows the learning effect of Stage 1. Specifically, we fix the initial distribution and update the vector field. For the flow network architecture, the $Q$ and $\sigma$ networks have three and two 128-dimensional, tanh-activated, fully-connection hidden layers, respectively. We use the Adam optimizer with a learning rate of $5e\shortn 3$ to train these networks with $20$K samples for $60$K iterations.

The x- and y-axes of Fig.~\ref{fig:flow_init} are the bundle variables for the 3rd and 4th items, respectively. $x=1$ means that the 3rd item is in the bundle, and $y=1$ means the 4th item is in the bundle. The blue lines represent the vector field, and the blue dots represent the final distribution. We can see that the vector field gradually learns to cover all possible bundles in this projected, two-item subspace, 
as indicated by points $(0,0)$, $(0,1)$, $(1,0)$, and $(1,1)$ in the plots.

Fig.~\ref{fig:flow_train} illustrates the dynamics of Stage 2. In this demonstrative example, the support size of initial distributions is set to 512, the menu size is 8, and $\lambda_{\textsc{SoftMax}}$ is 1. The figure is organized into four columns, each displaying changes in the bundle allocation (depicted by blue dots) and  price of a distinct menu element. Additionally, the total revenue at the corresponding training iteration (during test time) is  presented. Changes in the bundle distribution for a menu element result from updates to the initial distribution for the menu element. For the initial distributions, the positions of the green dots represent the means ($\bm\mu_d^{(k)}$) of the mixture-of-Dirac components, while their opacities indicate the weights ($w_d^{(k)}$) of these components. We observe that each menu element learns to allocate different bundles. For example, Menu Element 3 manages bundles $(0,1)$ and $(1,0)$, and sets a price of 88.245 after Train Iteration 1020, at which point the revenue reaches 64.31.

\section{Experiments}

% \tw{} Use \verb|\bundle| to refer Bundle-RochetNet, \verb|\smallbundle| for Small-Bundle, and \verb|\bigbundle| for Near-Grand-Bundle, and \verb|\grandbundle| for Grand-Bundle
% At a high level \citep{catshardness}, \texttt{Regions} models an auction for real estate, or more broadly, for goods where two-dimensional adjacency determines complementarities. 
% \texttt{Regions} extends the concept of \texttt{Regions} by removing the planarity constraint, capturing arbitrary complementarities between discrete goods such as electronic components or collectibles. \textit{paths} represents an auction for transportation links between cities, or more generally, for edges in a nearly planar graph. \textit{matching} represents auctions for airline takeoff and landing rights, akin to those considered by the FAA. Finally, \textit{scheduling} models a distributed job-shop scheduling environment.
% and do not involve different types of bidders (e.g., National versus Regional bidders) - a distinction not applicable in our single-bidder setting.
% marginal allocations using the , which is commonly used to re-parameterize categorical distributions. Specifically, for an item-wise allocation $\alpha\in\mathbb{R}^m$ of a menu element, this trick will return a vectorized bundle in $[0,1]^m$, where the item $i$ is in this bundle with probability $\alpha_i$, the $i$th dimension of $\alpha$.  to $\alpha$. 
% (2) \textbf{\bigbundle}: Fixed allocations include the grand bundle and bundles close to it, prioritizing those with the most items allocated. When the menu size is insufficient to include all bundles with a certain number of items, bundles are selected randomly. For instance, with 3 items and a menu size of 3, the menu would include $[1,1,1]$ (grand bundle) and a random subset of bundles containing $n-1$ items, such as $[1,1,0]$ and $[0,1,1]$. Although we randomly select this subset, one can predefine it.
% to \bigbundle, but includes the grand bundle and bundles with minimal item allocations, starting from single-item allocations. When the menu size exceeds the number of available single-item bundles, additional bundles are included by incrementally increasing the number of allocated items. For example, with 3 items and a menu size of 5, the selected bundles would include $[1,1,1]$ and single-item bundles such as $[1,0,0]$, $[0,1,0]$, $[0,0,1]$, and $[1,0,1]$.

\begin{table}[t]
    \caption{Revenue comparison on CATS across different environments, value distributions, and numbers of items. Note that \emph{when $m=10$, the menu size is large enough to accommodate all possible bundles in a menu.} \label{tab:exp_results}}
    \vspace{-0.5em}
    \centering
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Environment} & \textbf{Baseline} & $m=10$ & $m=50$ & $m=75$ & $m=100$ & $m=150$ \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Regions \\ \footnotesize Uniform Private Valuations}} 
        & \grandbundle & 162.57 & 316.27 & 321.10 & 317.00 & 314.93 \\
        & \bigbundle & \textbf{202.26} & 399.85 & 354.48 & 322.68 & 329.17 \\
        & \smallbundle & 202.16 & 322.85 & 318.33 & 326.76 & 334.20 \\
        & \bundle & 189.17 & 288.41 & 290.14 & 292.11 & 312.65  \\
        & \name & 196.19 & \textbf{555.05} & \textbf{454.96} & \textbf{417.83} & \textbf{385.68} \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Regions \\ \footnotesize Normal Private Valuations}} 
        & \grandbundle & 142.54 & 319.06 & 328.89 & 305.06 & 309.73 \\
        & \bigbundle & \textbf{181.93} & 459.97 & 405.36 & 306.60 & 312.21 \\
        & \smallbundle & 181.75 & 342.31 & 339.82 & 303.31 & 315.02 \\
        & \bundle & 167.16 & 270.23 & 300.55 & 270.23 & 291.83 \\
        & \name & 173.93 & \textbf{603.70} & \textbf{448.35} & \textbf{389.51} & \textbf{394.82} \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Arbitrary \\ \footnotesize Uniform Private Valuations}} 
        & \grandbundle & 175.09 & 329.62 & 340.40 & 343.66 & 345.98 \\
        & \bigbundle &\textbf{233.26} & 396.78 & 351.70 & 357.44 & 351.07 \\
        & \smallbundle & 222.74 & 353.20 & 355.73 & 364.33 & 360.41 \\
        & \bundle & 205.02 & 316.79 & 312.41 & 313.76 & 334.51 \\
        & \name & 211.55 & \textbf{560.71} & \textbf{467.79} & \textbf{434.77} & \textbf{420.75} \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Arbitrary \\ \footnotesize Normal Private Valuations}} 
        & \grandbundle & 186.59 & 354.86 & 345.88 & 329.99 & 336.45 \\
        & \bigbundle & \textbf{248.16} & 478.67 & 349.34 & 335.75 & 339.10 \\
        & \smallbundle & 248.13 & 376.31 & 352.80 & 343.58 & 348.39 \\
        & \bundle & 221.49 & 348.41 & 312.79 & 304.00 & 316.81 \\
        & \name & 235.80 & \textbf{646.37} & \textbf{490.03} & \textbf{428.54} & \textbf{394.37} \\
        \bottomrule
    \end{tabular}
    \vspace{-0.5em}
\end{table}
% \begin{table}[t]
%     \caption{Revenue comparison on SATS \texttt{Multi-Band Value Model}. \dcpadd{In addition to \name, the \grandbundle\ method is also competitive in this environment.} With $m=24$, we vary the maximum number of XOR atoms per valuation (\ie, valuation function size): $a = 10, 20, 30, 40, 50$. \jf{updated numbers, need to think about what to do with this table} \label{tab:sats}} 
%     \centering
%     \vspace{-0.5em}
%     \begin{tabular}{ccccccc}
%         \toprule
%         \textbf{Environment} & \textbf{Baseline} & $a=10$ & $a=20$ & $a=30$ & $a=40$ & $a=50$ \\
%         \midrule
%         \multirow{5}{*}{\makecell{SATS \\ \footnotesize Multi-Band Value Model}} 
%         & \grandbundle & 2580.2 & 2766.1 & 2848.4 & 2878.6 & 2917.5 \\
%         & \bigbundle   & \textbf{2731.6} &  \textbf{2839.3}& 2865.1 & \textbf{2962.8 } & \textbf{3013.1}\\
%         & \smallbundle & 2601.0 &  2809.9 & \textbf{2872.7} & 2941.0 & 2977.6\\
%         & \bundle      & 2413.4 & 1968.8 & 2174.9 & 2234.6 & 2211.5 \\
%         & \name        & 2600.3 & 2767.4 & 2860.3 & 2902.8 & 2926.5 \\
%         \bottomrule
%     \end{tabular}
%     \vspace{-0.5em}
% \end{table}

% According to the benchmark setting, \emph{there are 2400 possible bundles in total, which can be accommodated within the menus of baselines}. \dcp{but wouldn't they have to be small- or big-bundles for this to be true?}

\subsection{Testbeds}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/lc_m.pdf}
    \caption{Learning curves of \name~and baselines on \texttt{CATS Arbitrary} with normal valuation distributions with different numbers of items. The three rows are results for 50, 100, and 150 items, respectively. The three columns show the changes of test revenue as a function of the number of training iterations, wall time in seconds, and the number of training samples, respectively. 
    % \dcp{WOW! can we also say this is representative of SATS?} \tw{I am afraid we can not. I am considering downplaying the emphasis on our advantage in training speed, and Jeff is double checking whether baselines can converge quicker.} \dcp{ok, I talked with Jeff briefly about this yesterday and understand we're taking another look at the choice of learning rate for the RochetNet baselines.}
    \label{fig:lcm}}
\end{figure}

We evaluate our method on CATS \citep{leyton2000towards}, a standard benchmark that has been used in CA research since 2000. CATS has five different environments: \texttt{Regions}, \texttt{Arbitrary}, \texttt{Paths}, \texttt{Matching}, and \texttt{Scheduling}. These provide stylized representations of diverse real-world problems ranging from real estate and electronic components to transportation links and airline scheduling rights. Analysis of CA winner determination problem (WDP) complexity \citep{catshardness} revealed that \texttt{Matching}, \texttt{Scheduling}, and \texttt{Paths} are considerably easier to solve than \texttt{Regions} and \texttt{Arbitrary}. The WDP complexity was measured by CPLEX runs on problems with 256 goods and 1,000 %non-dominated  dcp, dropping since we don't talk about this for our own experiments
bids, reflecting the percentage of instances where the WDP is solved within a given time frame. Therefore, as in numerous previous works that have employed CATS as a benchmark \citep{gasse2019exact, hutter2009paramils, hutter2014algorithm, balcan2018learning, scavuzzo2022learning, wu2021learning, huang2023searching, song2020general, gupta2022lookback, balcan2021sample, zhang2022deep}, we focus our experiments on the challenging \texttt{Arbitrary} and  \texttt{Regions} environments. Each environment has two options for the value distribution: uniform and normal. We test both distributions with the default parameters provided by CATS.

In CATS, the valuation functions of bidders are recorded as bundle-bid pairs in an output file, with bundles from the same bidder identified by appending a dummy item tagged with a unique identifier. In effect, the bundle-bid pairs in an output file involving the same dummy item form an XOR representation of the bidder's valuation function. 
To obtain single-bidder valuations, we generate 100,000 such files and  extract valuation functions identified by a consistent dummy item. Of these, 95\% are used for training, with the remaining 5\% reserved for testing.

% \textbf{SATS} is motivated by the spectrum auction domain and includes several valuation models. We focus on the Multi-Band Value Model \citep{bichler2013core}, %\tw{Maybe this is a safer way to explain:} \twadd{
% as other models 
%dcp either oversimplify or 
% introduce bidder types that do not align with our single-bidder framework. In the Multi-Band Value Model, 24 items are grouped into four bands. Bidder values depend on the number of allocated items in each band, with some interchangeability between items, exhibiting complements within a band while being additive across bands.
%, while bidders still bid specific bundles, not item counts. 
% This setup challenges deep learning methods, as gradients may be uninformative---some changes in the bundle allocation may not affect values. As such, the Multi-Band Value Model serves to further assess the robustness of our method.
%under less-than-optimal conditions. 
% (e.g., the base value model contains only 48 bundles)
% (e.g., National vs. Regional bidders)
% In SATS, the number of bidders is specified as an argument, so we directly set it to 1 to generate single-bidder valuations.

We evaluate our method with different numbers of items: 10, 50, 75, 100, and 150, across all environments and value distributions on CATS. When varying the number of items, we set the maximum XOR atoms per bid to 5 (the default value in CATS). We also experiment with a fixed number of items (50), increasing the maximum number of XOR atoms per valuation function in each environments by configuring the  \emph{maximum substitutable bids} argument.
% of the CATS and SATS 
% we use the  setting \dcp{what does this do?} as specified in CATS, with each valuation function comprising up to five XOR atoms.

\subsection{Baselines}

Work on DSIC deep auction learning can be broadly divided into two categories. (1) Methods like AMA~\cite{curry2022differentiable,duan2023scalable} and VVCA~\cite{duan2024scalable} explore a
restricted family of affine mechanisms. Consequently, they exhibit restricted expressiveness and achieve suboptimal revenue compared to methods in the second category~\cite{dutting2024optimal}.
%\dcp{isn't solving WDPs unavoidable?}\dcp{for something like VVCA wouldn't one just have a restricted number of bundles similar to what we do? how would it be different from what we do?}\tw{I think VVCA can have these good properties, but previous work didn't realize them. Discussing these improvements might distract the readers, so I prefer to keep the discussion concise.} 
(2) Menu-based methods like RochetNet~\cite{dutting2024optimal} learn an item-wise allocation for each menu element, and, as we have discussed, this requires an interpretation such as a product distribution for combinatorial valuations.
To test the flexibility of product distributions, we establish the first baseline as follows:

\begin{table}[t]
    \caption{Effect of the support size of the initial distribution ($D$) on \name~in different CATS environments. Revenue drops dramatically when $D$ decreases from 2 to 1. \label{tab:D}}
    \centering
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Environment} & \textbf{\# Items ($m$)} & $D=1$ & $D=2$ & $D=4$ & $D=8$ & $D=16$ \\
        \midrule
        \multirow{3}{*}{\makecell{CATS-Regions \\ \footnotesize Normal Private Valuations}} 
        & 50  & 278.82 & 589.50 & 596.67 & 603.70 & 595.14\\
        & 100 & 258.18 & 364.29 & 372.49 & 389.51 & 388.06 \\
        & 150 & 291.69 & 338.52 & 368.00 & 394.28 & 383.24 \\
        \midrule
        \multirow{3}{*}{\makecell{CATS-Arbitrary \\ \footnotesize Uniform Private Valuations}}
        & 50  & 287.98 & 557.06 & 563.70 & 560.71 & 561.51\\
        & 100 & 279.75 & 425.94 & 426.80 & 434.77 & 428.53  \\
        & 150 & 318.97 & 383.44 & 396.71 & 420.75 & 411.86\\
        \bottomrule
    \end{tabular}
\end{table}
%
\begin{table} [t]
    \caption{The Effect of increasing $D$ (the support size of initial distributions) under varying menu sizes $K$ (the number of elements in a menu). The default value of $K$ is 5000 when $m\le100$, and 20,000 otherwise.
    \label{tab:DJ} }
    \centering
    \begin{tabular}{CRCRCRCRCRCRCRCRCRCRCR}
        \toprule
        % \multicolumn{2}{c}{\multirow{2}{*}{}} &
        % \multicolumn{2}{l}{\multirow{2}{*}{Alg.}} &
        % \multicolumn{10}{c}{Setting}\\
        

        \multicolumn{4}{c}{Menu Size} &
        \multicolumn{4}{c}{$K/4$} & 
        \multicolumn{4}{c}{$K/2$} &
        \multicolumn{4}{c}{$K$} & 
        \multicolumn{4}{c}{$K*2$}\\

        \cmidrule(lr){1-4}
        \cmidrule(lr){5-8}
        \cmidrule(lr){9-12}
        \cmidrule(lr){13-16}
        \cmidrule(lr){17-20}
        \multicolumn{2}{c}{\textbf{Environments}} &
        \multicolumn{2}{l}{$m$} &
        \multicolumn{2}{c}{$D$=1} & 
        \multicolumn{2}{c}{$D$=2} &
        \multicolumn{2}{c}{$D$=1} & 
        \multicolumn{2}{c}{$D$=2} &
        \multicolumn{2}{c}{$D$=1} & 
        \multicolumn{2}{c}{$D$=2} &
        \multicolumn{2}{c}{$D$=1} & 
        \multicolumn{2}{c}{$D$=2} \\

        \cmidrule(lr){1-4}
        \cmidrule(lr){5-6}
        \cmidrule(lr){7-8}
        \cmidrule(lr){9-10}
        \cmidrule(lr){11-12}
        \cmidrule(lr){13-14}
        \cmidrule(lr){15-16}
        \cmidrule(lr){17-18}
        \cmidrule(lr){19-20}
        
        \multicolumn{2}{c}{\multirow{3}{*}{\makecell{CATS-Regions \\ \footnotesize Normal Private Valuations}}} & 
                               \multicolumn{2}{l}{50}  & \multicolumn{2}{l}{228.34} & \multicolumn{2}{l}{575.52} & \multicolumn{2}{l}{259.95} & \multicolumn{2}{l}{568.94} & \multicolumn{2}{l}{278.82} & \multicolumn{2}{l}{589.50} & \multicolumn{2}{l}{315.88}  & \multicolumn{2}{l}{602.98}\\
        \multicolumn{2}{c}{} & \multicolumn{2}{l}{100} & \multicolumn{2}{l}{213.26} & \multicolumn{2}{l}{324.04} & \multicolumn{2}{l}{226.11} & \multicolumn{2}{l}{345.51} & \multicolumn{2}{l}{258.18} & \multicolumn{2}{l}{364.29} & \multicolumn{2}{l}{270.65}  & \multicolumn{2}{l}{419.12}\\
        \multicolumn{2}{c}{} & \multicolumn{2}{l}{150} & \multicolumn{2}{l}{245.22} & \multicolumn{2}{l}{312.82} & \multicolumn{2}{l}{268.78} & \multicolumn{2}{l}{318.21} & \multicolumn{2}{l}{291.69} & \multicolumn{2}{l}{338.52} & \multicolumn{2}{l}{302.04}  & \multicolumn{2}{l}{385.08}\\
        
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-6}
        \cmidrule(lr){7-8}
        \cmidrule(lr){9-10}
        \cmidrule(lr){11-12}
        \cmidrule(lr){13-14}
        \cmidrule(lr){15-16}
        \cmidrule(lr){17-18}
        \cmidrule(lr){19-20}
        
        \multicolumn{2}{c}{\multirow{3}{*}{\makecell{CATS-Arbitrary \\ \footnotesize Uniform Private Valuations}}} & 
                               \multicolumn{2}{l}{50}  & \multicolumn{2}{l}{238.96} & \multicolumn{2}{l}{546.50} & \multicolumn{2}{l}{261.61} & \multicolumn{2}{l}{551.78} & \multicolumn{2}{l}{287.98} & \multicolumn{2}{l}{557.06} & \multicolumn{2}{l}{303.53}  & \multicolumn{2}{l}{561.28}\\
        \multicolumn{2}{c}{} & \multicolumn{2}{l}{100} & \multicolumn{2}{l}{230.84} & \multicolumn{2}{l}{357.99} & \multicolumn{2}{l}{254.20} & \multicolumn{2}{l}{408.45} & \multicolumn{2}{l}{279.75} & \multicolumn{2}{l}{425.94} & \multicolumn{2}{l}{305.11}  & \multicolumn{2}{l}{438.27}\\
        \multicolumn{2}{c}{} & \multicolumn{2}{l}{150} & \multicolumn{2}{l}{297.22} & \multicolumn{2}{l}{355.07} & \multicolumn{2}{l}{303.08} & \multicolumn{2}{l}{361.01} & \multicolumn{2}{l}{318.97} & \multicolumn{2}{l}{383.44} & \multicolumn{2}{l}{343.19}  & \multicolumn{2}{l}{404.30}\\
        \bottomrule
    \end{tabular}
\end{table}

(1) \textbf{\bundle}: An adaption of RochetNet, interpreting its item-wise allocations as product distributions. Calculating bidder values remains intractable due to the need to enumerate all bundles. We address this by employing the \emph{Gumbel-SoftMax} technique~\cite{jang2017categorical}, which enables sampling from product distributions and backpropagation through the samples to update item-wise allocations. During training and testing, we use these samples to estimate bidder values. Empirically, item-wise allocations usually converge to binary vectors (0s or 1s) after training. In such scenarios, estimations of bidder values at test time are accurate, thereby ensuring that the mechanism is DSIC. If convergence to 0/1 vectors does not occur, DSIC is not exactly achieved in this baseline.

% making the calculation of bidder values tractable and ensuring DSIC. \dcp{rather than rest of sentence can we explain what we do at test time when this is not the case?} \tw{If that’s not the case, we estimate by sampling. Although we observed only binary vectors for the checkpoints we examined, we cannot guarantee DSIC. But it seems that better solutions are unlikely because enumeration is indeed intractable in scenarios with 100+ items. Should we emphasize that these results might be non-DSIC?} \dcp{I think 1-2 sentences saying what we do, and noting it can make it non-DSIC}.


To better understand the performance of \name~and \bundle, we also fix the allocations in RochetNet menus and only learn the prices using the same gradient-based optimization method as in RochetNet. Specifically, we have the following additional baselines:

(2) \textbf{\bigbundle}: This baseline focuses on large bundles, including the grand bundle (that includes all items) and those nearest in size to the grand bundle. When the menu size prevents the inclusion of all bundles of a certain size, selection is random. %(or can be pre-defined, in principle), 
% favoring bundles with higher item counts. 
For instance, with 3 items and a menu size of 3, the menu would include $[1,1,1]$ (the grand bundle) and a random subset of bundles containing $2$ items, such as $[1,1,0]$ and $[0,1,1]$.

(3) \textbf{\smallbundle}: Similar to \bigbundle, but it prioritizes minimal item allocation, along with the grand bundle; i.e., it begins with single-item bundles and expands as the menu size permits.

(4) \textbf{\grandbundle}: A simple baseline that sets a single price for the grand bundle. The price is determined through a grid search based on maximum training set revenue;  performance, as with all methods, is reported on the test set.
%
\begin{table}[t]
    \caption{Revenue comparison against baselines across different CATS environments and value distributions. The number of items is fixed at $m=50$, and we increase the valuation function size: $a = 10, 20, 30, 40$, and $50$ as maximum XOR atoms per valuation, corresponding to the {\em maxbid} parameter in CATS. \label{tab:exp_results_xor}}
    \centering
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Environment} & \textbf{Baseline} & $a=10$ & $a=20$ & $a=30$ & $a=40$ & $a=50$ \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Regions \\ \footnotesize Uniform Private Valuations}} 
        & \grandbundle & 314.46 & 309.11 & 316.04 & 320.61 & 314.35 \\
        & \bigbundle & 374.28 & 369.29 & 357.91 & 372.39 & 363.13 \\
        & \smallbundle & 328.95 & 328.92 & 323.44 & 333.00 & 326.58 \\
        & \bundle & 308.81 & 313.24 & 317.14 & 318.15 & 310.60 \\
        & \name & \textbf{533.42} & \textbf{528.46} & \textbf{528.26} & \textbf{539.92} & \textbf{537.36} \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Regions \\ \footnotesize Normal Private Valuations}} 
        & \grandbundle & 322.74 & 301.54 & 310.38 & 304.38 & 334.95 \\
        & \bigbundle & 434.32 & 375.51 & 404.74 & 381.05 & 430.24 \\
        & \smallbundle & 326.98 & 307.81 & 332.39 & 313.76 & 342.23 \\
        & \bundle & 301.47 & 297.34 & 317.10 & 302.04 & 327.32 \\
        & \name & \textbf{564.13} & \textbf{524.24} & \textbf{535.35} & \textbf{525.68} & \textbf{566.51} \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Arbitrary \\ \footnotesize Uniform Private Valuations}} 
        & \grandbundle & 331.21 & 334.43 & 330.33 & 351.01 & 336.68 \\
        & \bigbundle & 355.92 & 351.02 & 341.76 & 347.26 & 348.92 \\
        & \smallbundle & 352.34 & 354.58 & 346.36 & 354.87  & 353.58 \\
        & \bundle & 329.61 & 345.06 & 338.44 & 344.76 & 350.75 \\
        & \name & \textbf{565.54} & \textbf{583.60} & \textbf{568.01} & \textbf{579.01} & \textbf{581.39} \\
        \midrule
        \multirow{5}{*}{\makecell{CATS-Arbitrary \\ \footnotesize Normal Private Valuations}} 
        & \grandbundle & 381.51 & 335.44 & 323.52 & 320.28 & 358.83 \\
        & \bigbundle & 470.63 & 349.70 & 346.39 & 340.59 & 381.13 \\
        & \smallbundle & 402.23 & 341.79 & 337.44 & 332.90 & 368.26 \\
        & \bundle & 367.44 & 329.28 & 333.88 & 327.57 & 365.40 \\
        & \name & \textbf{664.94} & \textbf{564.90} & \textbf{552.77} & \textbf{548.22} & \textbf{615.93} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Setup}


We did not extensively fine-tune the flow model architecture, as the initial trial already yielded satisfactory results. This suggests that our formulation of the ODE, including its functional form (Eq.~\ref{equ:varphi}) and initial conditions (Eq.~\ref{equ:init_dist}), is well-suited to the needs of the CA settings, making the optimization of the flow model relatively straightforward.
% allowing flexible choices of model architectures.
Specifically, the $Q$ network comprises three 128-dimensional tanh-activated fully connected layers. When $m>100$, we increase the width of the last layer to 256. The $\sigma$ network is simpler and has two 128-dimensional tanh-activated fully connected layers. 
% We did not extensively fine-tune this architecture, as the initial trial already yielded satisfactory results. This suggests that our formulation of the ODE, including its functional form (Eq.~\ref{equ:varphi}) and initial conditions (Eq.~\ref{equ:init_dist}), is well-suited to CA settings, 
%\dcp{comment, e.g. footnote: did we need a lot of hyperparameter tuning?}

Two important hyper-parameters are $D$, the support size of the initial distribution, and $K$, the menu size. By default, $D$ is set to 8, a relatively small number. $K$ is 5000 when $m\le 100$ and is 20000 otherwise. This menu size setting is the same for our method, \smallbundle, \bigbundle, and \bundle.
Notably, the selected menu sizes are adequate to encompass all possible bundles for smaller numbers of items, such as $m=5$ or $10$. We will show the impact of different values of $D$ and $K$ in ablation studies. 

Menu optimization for \name~is conducted using the Adam optimizer with a learning rate of 0.3. $\lambda_{\textsc{SoftMax}}$ is increased from 0.001 to 0.2 over the course of training. For comprehensive details on our hyper-parameter settings, please refer to the codebase  provided with our submission. For the baselines, we fine-tuned their hyper-parameters so that they perform significantly better than the default RochetNet setting. The modifications are achieved by performing a grid search to obtain the optimum combination of $\lambda_{\textsc{SoftMax}}$ and learning rate that yields the best revenue and also guarantees convergence. Both \smallbundle~and \bigbundle~use a learning rate of $0.3$ and $\lambda_{\textsc{SoftMax}}$ of 2, while \bundle~uses a learning rate of $0.05$ and $\lambda_{\textsc{SoftMax}}$ of 20.

% We also fine-tune   and  For example, we gradually decrease the learning rate from $1e\shortn 2$ to $1e\shortn 4$ for training stability.
% \dcp{note will be available in future?}


\subsection{Results}

% \begin{table}[t]
%     \centering
%     \caption{
%     \label{tab:menu_support}}
%     \begin{tabular}{lccccccc}
%         \toprule
%         Menu Support Size & 1 & 2 & 4 & 8 & 32 & 128 \\
%         \midrule
%         CATS-Regions, normal, m=10 & 176.13, 176.10 & 174.13, 173.85 & 174.28 & 173.96\\
%         CATS-Regions, normal, m=50 & 278.56, 278.82 & 589.50 &  \\
%         CATS-Regions, normal, m=100 & 258.18 & 364.29, 353.71 & \\
%         CATS-Regions, normal, m=150 & 291.69 & 338.52\\
%         CATS-Arbitrary, m=10 & \\
%         CATS-Arbitrary, uniform, m=50 & 287.98 & 557.06\\
%         CATS-Arbitrary, uniform, m=100 & 279.75 & 425.94 \\
%         CATS-Arbitrary, uniform, m=150 & 318.97,316.84 & 383.44\\
%         \bottomrule
%     \end{tabular}
% \end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/lc_a.pdf}
    \caption{Learning curves of \name~and baselines on \texttt{CATS Regions} with uniform valuation distributions with different valuation function sizes. The two rows are results for $a=10$ and $50$ XOR atoms per valuation, respectively. The three columns show the changes of test revenue as a function of the number of training iterations, wall time in seconds, and the number of training samples, respectively.
    \label{fig:lca}}
\end{figure}

\textbf{Performance on CATS}. Table~\ref{tab:exp_results} shows the revenue of our method compared against baselines. Our method performs consistently and significantly better when the number of items is large ($m>10$), achieving revenues gains of 1.11$-$2.23$\times$. The learning curves in Fig.~\ref{fig:lcm} further illustrate this advantage. \bundle~is the baseline that also learns allocations in a menu. In CATS \texttt{Arbitrary} with normal distributions, \bundle~requires 200K iterations to converge for $m=50$ and $100$, and 250K iterations for $m=150$. In contrast, our method converges in 17K$-$21K iterations for $m=50$ and $100$ and 68K iterations for $m=150$. This translates to a 3.6$-$9.5$\times$ improvement in the number of training iterations required. Moreover, our method can also reduce training time in some settings. For example, when $m=50$, \name~reduces the training duration from about 700 seconds to 140 seconds, achieving a 5$\times$  improvement in training speed.


% The best performing baseline consistently requires 3000 seconds to converge, whereas \name~converges within approximately 140-180 seconds when $m=50$ and $100$, and within 2200 seconds when $m=150$. This translates to a 1.3-21.4$\times$ improvement in training speed.
% Similarly, in the SATS environment, where the number of possible bundles is $2400$, a menu size of $5000$ is sufficient.

When the number of items is small ($m=10$), baselines with a menu size of 5000 can effectively cover all possible bundles. Although these settings are not the primary focus of our method, \name~can still achieve comparable revenue relative to the baselines, as shown in the first column of Table~\ref{tab:exp_results}.

% \grandbundle~has relative better performance on CATS. Moreover,
Among the baselines, fixed allocation strategies (\smallbundle~and \bigbundle) outperform the full-fledged \bundle. A possible reason is that menu sizes of 5K or 20K remain negligible compared to the vast number of possible bundles like $2^{50}$ or $2^{150}$, but can pose a challenge for deep learning optimization. Therefore, the potential benefit of increased flexibility are over-weighed by the difficulty in optimization. This highlights an advantage of our method: while allowing better flexibility, it formulates an optimization problem that is more tractable, thereby enabling significantly improved performance.

\textbf{The support size of bundle distributions}. Table~\ref{tab:D} provides insights into why our method has superior performance. When we reduce the support size of initial distributions ($D$) to 1, each menu element deterministically assigns a single bundle to the bidder. This eliminates a key advantage of our method as it can no longer represent a distribution over bundles. Correspondingly, we observe a dramatic drop in revenue when $D$ is decreased from 2 to 1. For example, in the CATS \texttt{Regions} environment with normal distributions and 50 items, revenue declines sharply from 589.50 to 278.82. This trend remains consistent when we vary the menu size, as shown in Table~\ref{tab:DJ}. This pronounced performance gap between $D=1$ and $D=2$ highlights the importance of maintaining a randomized distribution over bundles in differentiable economics for CA settings---a capability uniquely enabled by our method.

Despite this loss of expressiveness when $D=1$, we find that our method can still outperform \bundle~in some settings. This is because, even when optimizing a deterministic bundle for each menu element, we are ``searching" directly within the bundle space and, in principle, can reach any possible bundle. By contrast, as we have discussed, product distributions can represent only a limited subset of possible bundles. Therefore, our method retains greater flexibility than product distributions even when $D=1$.

\textbf{Menu size and Valuation function size}. Table~\ref{tab:DJ} presents the performance of our method under varying menu sizes ($K$), specifically when they are halved or quartered, and when they are doubled. As discussed, a randomized distribution over bundles is crucial in each of these settings, as revenue drops sharply when $D$ decreases to 1. Furthermore, increasing the menu size tends to improve performance, although the gains are modest when the item count is relatively low. For example, in CATS \texttt{Regions} with normal distributions, when $m=100$, increasing the menu size from $K/4$ to $K*2$ leads to a $29.34\%$ performance boost (rising from 324.04 to 419.12), in contrast to a merely $4.77\%$ enhancement when $m=50$ (rising from 575.52 to 602.98). Increasing the maximum number of XOR atoms per valuation from $10$ to $50$ has minimal effects on the performance of both our method and the baselines, as evidenced in Table~\ref{tab:exp_results_xor} and Fig.~\ref{fig:lca}.

% Results are reported for four \dcpadd{environments}: CATS-Regions with Uniform Private Valuations and  Normal Private Valuations, CATS-Arbitrary with Uniform Private Valuations and  Normal Private Valuations. with varying XOR \dcpadd{valuation function} sizes.

% To investigate the impact of menu support size on \name's performance, we evaluate revenue across different menu sizes in the network architecture.  \dcp{menu size, not $D$ for distribution support?  if yes, then `support` prob not a good word} Table~\ref{tab:menu_support} presents the results for menu sizes ranging from 2 to 128, using the Uniform Private Valuations model in CATS-Regions, with number of items of 50.
%550.89 & 555.05 & 552.79 & 553.67

% \begin{table}[t]
%     \centering
%     \caption{ The Effect of increasing $D$ ()
%     \label{tab:menu_support}}
%     \begin{tabular}{lccccccc}
%         \toprule
%         Menu Support Size & 1 & 2 & 4 & 8 & 32 & 128 \\
%         \midrule
%         CATS-Regions, normal, m=10 & 176.13, 176.10 & 174.13, 173.85 & 174.28 & 173.96\\
%         CATS-Regions, normal, m=50  & 315.88 & 602.98 &  \\
%         CATS-Regions, normal, m=100 & 270.65 & 419.12 & \\
%         CATS-Regions, normal, m=150 & 302.04 & 385.08\\
%         CATS-Arbitrary, m=10 & \\
%         CATS-Arbitrary, uniform, m=50  & 303.53 & 561.28\\
%         CATS-Arbitrary, uniform, m=100 & 305.11 & 438.27 \\
%         CATS-Arbitrary, uniform, m=150 & 343.19 & 404.30\\
%         \bottomrule
%     \end{tabular}
% \end{table}

%
% \begin{enumerate}
% \item 
% In particular we adopt a bundle-variation on {\em RochetNet}, which we call \dcp{BundleRochetNet?} 
%
%This parameterizes a menu of \dcpadd{item-wise allocation probabilities} and their associated payments, updating them using gradient descent to minimize negated expected revenue~\cite{dutting2024optimal}. 
%
% For this, we replace the softmax function of traditional RochetNet~\cite{dutting2024optimal}
% with a {\em Gumbel-softmax}, ensuring differentiability in this setting. \dcp{need more here---explain how it in effect rounds to get a single 0/1 bundle per menu element}.
% In this way, BundleRochetNet \dcp{?}
% has a representation
% that \dcpadd{is moderately 
% expressive, limited by deterministic menu elements and the size of the menu.}

% \end{enumerate}

% A more expressive class of baselines involves offering the buyer a menu of choices. While this approach remains tractable by limiting the menu size, it accommodates a \dcpadd{more flexible allocation and payment rule}. 
%dcp cut and in principle capable of recovering other menu-based baselines  such as AMA-based approaches with a limited .
%

% Despite its potential, we find that BundleRochetNet \dcp{?}
% struggles with scalability. Gradient-based updates become unstable and suboptimal when searching for a few thousand menu items in an exponentially-large bundle space. In fact, 
% baseline.
% Results are reported for four \dcpadd{environments:} CATS-Regions with Uniform Private Valuations and  Normal Private Valuations, CATS-Arbitrary with Uniform Private Valuations and  Normal Private Valuations.