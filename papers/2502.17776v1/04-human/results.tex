\subsection{Results and Analyses}
\input{04-human/tab-human-collection-stat}

Seven expert contracted participants interacted with the designed interface to generate human-elicited TOT queries.


Table \ref{tab:human-collection-stat} shows the number of instances recorded at each phase for the Movie, Landmark, and Person domains, along with their totals.
"Phase 1 - Yes" and "Phase 1 - No" indicate whether participants recognized the presented entity. "Phase 2 - Yes" represents cases where participants recalled the entity's name, with "Correct" and "Incorrect" indicating the accuracy of their recall. "Phase 2 - No" corresponds to participants who could not retrieve the entity's name, leading to TOT query collection in Phase 3. "Phase 4 - Yes" and "Phase 4 - No" denote whether the retrieved entity was correctly confirmed, while "Phase 4 - N/A" represents cases where confirmation was not applicable.\tablefootnote{Due to session restarts or participants logging off, the numbers in "Phase 2 - No" and "Phase 3" do not always match exactly. Similarly, the sum of "Phase 2 - Yes" and "Phase 2 - No" does not always correspond to the number of "Phase 1 - Yes".}

From this human TOT query collection process, we obtained 178 Movie queries, 57 Landmark queries, and 349 Person queries, resulting in a total of 584 human-written TOT queries across three domains. 
The following is an example of a Landmark query:\\

\noindent\fbox{
\parbox{0.95\columnwidth}{
\textbf{Human-Elicited Landmark Query}:\\
i've seen a picture of this landmark but I can't quite remember where it is. it might have a garden area inside of it. you can walk from the bottom around and around to the top if i remember correctly. i don't think it's for housing. it's a sculpture. possibly located in new york or at least in the US.\\
\textbf{Correct Answer: Vessel}
}    
}\\

\noindent
As shown in the example, we qualitatively identified common linguistic patterns, such as expressions of uncertainty and distorted memories, within the collected queries.


\input{04-human/tab-psychology-connection}

Table \ref{tab:psychology-connection} presents the recognizability and retrievability of visual stimuli in our human experiments. Recognizability is measured as the ratio of participants recognizing an entity (Phase 1 - Yes) to the total instances in Phase 1. Retrievability is measured as the proportion of correctly recalled entity names (Phase 2 - Yes Correct) relative to all instances in Phase 1. These values provide insights into how well different domains induce TOT states in participants with our visual stimuli set.


\begin{figure} 
\centering
\includegraphics[width=0.9\columnwidth]{04-human/graphics/movie_cqa_human.pdf}
\caption{
Proportions of labeled codes in CQA and human-elicited queries in the Movie domain, showing a low difference in frequency distribution (EMD).
}
\label{fig:cqa-human-ling}
\end{figure}

To validate with system rank correlation, we needed overlapping entities between CQA data and our human-elicited TOT dataset. Since MS-TOT and manually collected Reddit posts had negligible overlap in the Landmark and Person domains, we turned to TOMT-KIS for matching entities.

As TOMT-KIS lacks explicit entity names, we matched entities by checking if the target name appeared as a substring in the answer posts. Only one valid match ("Fort Ticonderoga") was found in the Landmark domain, and nearly all matched queries in the Person domain were about movies or songs, as TOMT-KIS did not collect person-specific queries. Thus, we had to exclud both domains from validation.
%
For the Movie domain, we filtered TOMT-KIS queries for relevance by requiring the word "movie" while excluding "song", removing answers with multiple guesses, eliminating queries based solely on video links, and selecting the longest text for duplicate identifiers. This process yielded 303 Movie-domain queries, which we used for system rank correlation validation.



\textbf{RQ 2: Can we elicit TOT queries from humans for effective simulated evaluation of TOT retrieval systems?}
\input{04-human/human-correlation-table}

Our results indicate that human-elicited TOT queries can serve as an effective resource for simulated evaluation of TOT retrieval systems. The system rank correlation between human-elicited queries and CQA-collected queries (TOMT-KIS) in the Movie domain shows high agreement (Table \ref{tab:human-movie-correlation}).
%
Additionally, linguistic similarity analysis using Earth Moverâ€™s Distance (EMD) resulted in a value of 0.0291 (Figure \ref{fig:cqa-human-ling}), which is comparable to the lowest EMD achieved during the LLM-elicitation experiments. 

The higher proportion of opinion-related expressions in human-elicited queries may be influenced by differences in query intent. In CQA settings, users often structure their queries to maximize answerability, prioritizing factual details over personal reflections. In contrast, our elicitation method placed participants in a TOT state without the expectation of receiving an answer from community, which may have encouraged them to articulate their thoughts more freely. This introspective process could have naturally led to a greater inclusion of subjective impressions and emotions in their queries.




\textbf{RQ 3-2: Can the human-elicitation-based evaluation methods be used to other domains underrepresented in CQA-collected test collections?}

Since validation was restricted to the Movie domain due to the lack of sufficient CQA-collected queries in other domains, additional validation methods may be necessary to fully address the research question. However, the low EMD score (0.0291) in the Movie domain suggests that human-elicitation methods effectively capture authentic TOT queries, at least within this domain. Given that the elicitation process was designed to be domain-agnostic, follows established psychological principles of TOT states, and does not rely on movie-specific heuristics, these methods are likely to generalize to underrepresented domains, making them a promising direction for future TOT retrieval evaluations.
