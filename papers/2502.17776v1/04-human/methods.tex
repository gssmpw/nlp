\subsection{Visual Stimuli Collection}

To obtain images in Movie, Landmark, and Person domains, we collected entities from Wikipedia, selecting pages based on their infobox namespace\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes}.} and the presence of images. 
For the Person and Landmark domains, we directly collected images from Wikipedia pages. However, for the Movie domain, we identified linked IMDb or TMDB IDs on the Wikipedia pages and retrieved high-quality backdrop or still-cut images from TMDB. This approach was necessary because Wikipediaâ€™s movie images were predominantly posters, which often contain the movie title and cannot be used to elicit a TOT state. TMDB backdrops, on the other hand, provide spoiler-free, high-quality visuals suitable for this purpose.\footnote{\url{https://www.themoviedb.org/bible/image/}}
For the Landmark domain, we selected ``Place" as a top-level category, and for the Person domain, we chose ``Person".

Selecting appropriate infobox namespaces within these top-level categories presented challenges, as the collected images needed to be \textit{unambiguous} and \textit{popular} enough for human participants to recognize the entities. For example, in Landmark domain, images of random parks often depicted generic green spaces, making them ambiguous, whereas iconic parks were more likely to be recognizable. 
%
For the Person domain, ambiguity was defined as the presence of multiple people in the image, which could confuse participants.
To ensure the quality of the stimuli, two authors rated infobox namespaces under the ``Place" and ``Person" categories on a three-point scale:
\begin{itemize}
    \item 0: Most images would be ambiguous or unrecognizable.
    \item 1: Some images might be ambiguous.
    \item 2: Most images would be unambiguous and recognizable.
\end{itemize}
Given that our participants were based in the United States, this factor was also considered during the labeling process. 

After completing the rating and ranking process, we selected one infobox namespace (`film') for Movie, 94 for Landmark (e.g., `amusement park', `bridge', `ancient site'), and 123 for Person (e.g., `comedian', `sportsperson', `academic').

Next, we collected Wikipedia pages associated with the selected infobox namespaces and sorted them by page view counts, using the Wikipedia database dump from October 1, 2023.\footnote{\url{https://dumps.wikimedia.org/enwiki/}} We selected the top 20\% of pages by view count, as less popular entities were deemed unlikely to be recognized by participants.

Finally, we manually filtered the collected images to remove unsuitable content. For the Movie domain, we excluded inappropriate images, such as those featuring nudity or excessive gore, and posters containing the movie title. For the Landmark domain, we removed images where the entity name was visible, such as station names in train station pictures. For the Person domain, we excluded images with multiple individuals.\footnote{Detailed image deselection criteria can be found at \url{https://github.com/kimdanny/human-tot-query-elicitation-mturk/blob/main/image_deselection_criteria.md}.} After filtering, we finalized a visual stimuli collection of 1,687 Movie entities, 1,946 Person entities, and 330 Landmark entities, all associated with Wikipedia pages, ensuring a diverse and high-quality set of stimuli for eliciting TOT queries.






\subsection{Interface Design}
\input{04-human/flowchart}

We designed the flow of an interface as illustrated in Figure \ref{fig:interface-flowchart}.

\subsubsection{\textbf{Phase 1 (Recognizability)}}
In this phase, the system presents an image drawn from the pool of visual stimuli, asking participants whether they recognize the movie, landmark, or person depicted in the image, depending on the domain. The process repeats, drawing new images until the participant confirms recognition of an entity.

To ensure a balanced selection process, images in each domain were grouped into 20 buckets based on their popularity, measured by Wikipedia page view counts. When selecting an image, the system draws one random image per bucket in sequential order. The three domains---Movie, Landmark, and Person---are covered in a round-robin fashion, ensuring that images from different domains are presented in a balanced manner.

This approach minimizes the likelihood of a participant encountering the same image multiple times within a session and prevents from disproportionately drawing easily recognizable entities, leading to a more effective and controlled TOT induction process.



\subsubsection{\textbf{Phase 2 (Retrievability)}}
In this phase, we assess whether the participant can recall the name or title of the entity they recognized in Phase 1. If they cannot recall the name, it indicates that they are in a TOT state, and the interface transitions to Phase 3.

If they can recall the name, they are prompted to submit it, and the interface moves directly to Phase 4.


\subsubsection{\textbf{Phase 3 (TOT Query Collection)}}
If a participant reaches Phase 3, they are prompted to write a TOT query describing the entity they recognized but could not retrieve.

To ensure high-quality queries, we analyzed word count distributions in the MS-TOT dataset and determined that a minimum of 300 characters is necessary to effectively capture the complexity of TOT queries. To encourage longer responses, we followed the approach of \citet{agapie-13-longerqueries} and implemented a color-coded progress bar that visually indicates the current query length. The bar starts as red when the query is below 200 characters, turns yellow at 200 characters, and finally turns green once it reaches 300+ characters, signaling that the recommended length has been met.

Once the participant submits their TOT query, the interface transitions to Phase 4.


\subsubsection{\textbf{Phase 4 (Entity Confirmation)}}

In Phase 4, the system displays the Wikipedia page of the entity, allowing the participant to verify whether the entity they wrote a TOT query about or successfully retrieved is correct. This confirmation step is crucial for determining whether the collected TOT query is valid. 

After completing Phase 4, the interface loops back to Phase 1, where a new image is presented to continue the process.





