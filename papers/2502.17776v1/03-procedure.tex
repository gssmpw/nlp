\section{Method}
Overall, we elicit TOT queries from both LLMs and humans and validate them using two methods: system rank correlation (\S\ref{subsubsec:sys-rank-correlation}) and linguistic similarity (\S\ref{subsubsec:ling-sim}).

\subsection{Query Elicitation}
For LLM-elicited queries, we generate synthetic queries by exploring various prompting strategies. We experiment with different prompting conditions and model hyperparameters to identify the most effective prompt that yields the best validation results.  
This procedure is detailed in Section \S\ref{sec:llm-elicitation}.  

For human-elicited queries, we designed an interface that places participants in a TOT state using visual stimuli, such as movie stills, landmarks, and celebrity images. Participants then compose TOT queries as they would when posting on a CQA website.  
This procedure is described in Section \S\ref{sec:human-elicitation}.


\subsection{Query Validation}

\subsubsection{\textbf{System Rank Correlation}}\label{subsubsec:sys-rank-correlation}
\begin{figure} 
\centering
\includegraphics[trim=140 130 90 120, clip, width=\columnwidth]{03-synthetic/graphics/tau-validation.pdf}
\caption{
Validation of elicited queries using system rank correlation. We evaluate 40 different retrieval models using both CQA-based and elicited queries, ranking them based on search performance measured by MRR and NDCG. We then compute Kendall’s Tau and Pearson correlation to assess the agreement between the rankings derived from the two query sets.
}
\label{fig:validation-sys-rank}
\end{figure}

To validate the effectiveness of elicited TOT queries, we measure the correlation between its rankings of retrieval systems and rankings based on CQA queries for the same entities. This evaluation assesses whether retrieval models maintain consistent performance across different query sources but on the same entities. If the rankings derived from elicited queries strongly correlate with those from CQA-based queries, it indicates that the elicited queries capture similar retrieval challenges and retrieval effectiveness. A high correlation suggests that our synthetic and human-elicited queries can serve as reliable substitutes for traditionally collected CQA-based queries in evaluating retrieval systems.

To compute these correlations, we run the queries on 40 different retrieval models, comprising lexical and dense retrievers. The lexical retrievers include BM25 \cite{Robertson1995OkapiBM25} and language models with Dirichlet priors \cite{zhai2001DirichletSmoothing} using varying parameters. The dense retrievers include models of different sizes and performance levels, such as MiniLM-L6 and MiniLM-L12 \cite{miniLM}. To further introduce variation in systems, we include retrieval models with intentionally degraded performance by reinitializing the weights of certain layers in dense retrievers. Additionally, we incorporate an API-based closed-source LLM as one of the retrieval systems, specifically GPT-3.5-Turbo-Instruct, following the prompting format used in the baseline of the TREC 2024 TOT track to function as a ranker.
Furthermore, we include the top-ranked retrieval system from the TREC 2023 TOT track \cite{luis24totDPR, arguello2023overview} to provide a strong performance reference point.

Figure \ref{fig:validation-sys-rank} illustrates our validation strategy, showing how retrieval system rankings across different query sets provide insight into the reliability and effectiveness of our elicited queries.



%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\textbf{Linguistic Similarity}}\label{subsubsec:ling-sim}
\begin{figure} 
\centering
\includegraphics[width=0.9\columnwidth]{04-human/graphics/ms-tot-gold-pred.pdf}
\caption{
Validation of automatic annotation using the MS-TOT dataset.
Results show that GPT-4o-mini performs well in annotating TOT queries, closely aligning with human annotators. It achieves high accuracy and exhibits low Earth Mover’s Distance (EMD), indicating strong agreement with expert annotations.}
\label{fig:automatic-annotation-validation}
\end{figure}

Elicited TOT queries can exhibit substantial variation in writing style, word choice, experiences, and the presence of distorted memories \cite{Meier21-complex-reddit, arguello-movie-identification}. This diversity is inherent to the nature of TOT queries and is a valid characteristic of real-world TOT retrieval scenarios. Consequently, evaluating linguistic similarity between CQA-based and elicited queries using traditional methods, such as vector-based semantic similarity or lexicon-based similarity, may be ineffective.



To address this, we utilize a predefined set of TOT-specific linguistic codes introduced by \citet{arguello-movie-identification}. These handcrafted codes provide sentence-level annotations of TOT queries in the Movie domain, categorizing linguistic phenomena into eight top-level groups: `movie', `context', `previous-search', `social', `uncertainty', `opinion', `emotion', and `relative-comparison'. By leveraging this framework, we compare the linguistic distribution of codes between CQA-based and elicited queries rather than relying on direct semantic or lexical overlap.


To conduct this analysis, we annotate our elicited TOT queries in the Movie domain using these linguistic codes and compare the percentage distributions of codes found in CQA-based and elicited queries. To automate this process, we develop a language model-based automatic code annotator, prompting GPT-4o-mini\footnote{Temperature is set to 0 for reproducibility and consistency.} to produce JSON-formatted sentence-level annotations.



Before applying this annotator to LLM- and human-elicited queries, we first validate its performance on the MS-TOT dataset, where sentence-level gold annotations are available. We evaluate the annotator’s performance by computing precision and recall as prediction accuracy measures. Additionally, to assess annotation quality at a broader level, we compute query-level precision and recall, which measure the accuracy of identifying unique codes that appear within a multi-sentence query.

To further validate the annotator, we compare the distribution of annotated codes against the gold annotations using Earth Mover’s Distance (EMD), which quantifies how different the predicted code distribution is from the reference distribution. Figure \ref{fig:automatic-annotation-validation} presents the validation results of our automatic annotator on the MS-TOT dataset. Our evaluation shows that the annotator achieves sufficiently high accuracy and low EMD, confirming its suitability as an automatic labeler.



With this validation, we apply the automatic annotator to both LLM- and human-elicited queries in the Movie domain and analyze how their linguistic code distributions compare to those in CQA-based queries. However, we conduct this linguistic similarity validation only in the Movie domain, as there are no existing comprehensive handcrafted linguistic codes available for the Landmark and Person domains.
