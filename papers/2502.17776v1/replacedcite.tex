\section{Related Work}
%%%%%%%%%%%%%%
% Know-Item Retrieval and Query Simulation
%%%%%%%%%%%%%%
\subsection{Query Simulation and Know-Item Retrieval}

Query simulation methods have been used for various purposes, including document expansion ____ and synthetic test collection generation ____. In the context of known-item retrieval, these methods have been explored to improve retrieval strategies ____ and evaluation frameworks ____.



%% Query Simulation
\textit{Simulating} the known-item queries has long been an active research area ____.
Early work ____ generated synthetic queries using term-based likelihood models, selecting query terms based on their likelihood within a randomly chosen document. Later studies adapted this approach for desktop search ____ and email re-finding ____, demonstrating its effectiveness for simulated evaluations of know-item retrieval models.
%
The \textit{validation} of these query simulators has also been a key focus.
System ranking correlation ____, retrieval score distribution comparisons ____, and synthetic versus human query resemblance ____ have been used to assess their reliability.


While valuable, known-item search queries differ significantly from TOT queries, which are longer and more complex. Despite progress in simulating known-item queries, TOT retrieval remains unexplored. This paper bridges that gap by introducing novel TOT query elicitation methods and adapting established validation techniques ____ to ensure alignment with real-world queries, enabling scalable and accurate simulated evaluations.






%%%%%%%%%%%%%%
% TOT Datasets
%%%%%%%%%%%%%%
\subsection{TOT Datasets}
Several datasets have been developed to support research on TOT retrieval, primarily collected from online CQA platforms and focused on specific domains. MS-TOT ____ was constructed from the \textit{IRememberThisMovie} website and human-annotated with tags in the Movie domain. It also includes qualitative coding of TOT queries and demonstrates significant room for improvement in current retrieval technologies for such information needs. Similarly, ____ collected TOT queries from Reddit's \textit{/r/tipofmyjoystick} subreddit in the Game domain, providing coded tag information. Other datasets include Reddit-TOMT ____, focused on movies and books from Reddit's \textit{/r/tipofmytongue} subreddit; TOT-Music ____, targeting the Music domain from the same subreddit; and Whatsthatbook ____, sourced from \textit{GoodReads}, focused on the Book domain.



In response to the domain specificity of these datasets, recent efforts have aimed to expand TOT datasets across multiple areas. ____ expanded to general casual leisure domains using data from six Reddit subreddits, including games, books, and music, although other identified domains, such as videos and people, remain underrepresented. Similarly, TOMT-KIS ____ extended the collection from \textit{/r/tipofmytongue} by adapting ____'s approach with fewer filtering restrictions, resulting in 1.28 million TOT queries. However, only 47\% of these queries have identified answers, and the dataset continues to exhibit severe domain skewness toward a few topics. 


In this work, we develop and validate TOT query elicitation methods using the Movie domain for robust evaluation, then expand to Landmark and Person to assess applicability across underrepresented domains.