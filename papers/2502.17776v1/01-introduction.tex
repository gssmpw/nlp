\section{Introduction}
\label{sec:introduction}


% 0-1. What is TOT
Tip-of-the-Tongue (TOT) retrieval is a known-item search task where the searcher aims to re-find a previously encountered item but struggles to formalize the query due to an inability to recall specific identifiers, such as a document title or a personâ€™s name \cite{arguello-movie-identification, Bhargav-2022-wsdm}. 
Unlike other known-item retrieval tasks, 
TOT information requests are typically verbose and complex, often blending semantic details about the context in which the searcher previously engaged with the item \cite{Meier21-complex-reddit, lee2006known}. 
These queries frequently include linguistic phenomena such as uncertainty, exclusion criteria, relative comparisons, and even false memories \cite{arguello-movie-identification, Meier21-complex-reddit}, making them especially challenging for traditional retrieval systems \cite{arguello-movie-identification, Bhargav-2022-wsdm, lin-etal-2023-whatsthatbook, arguello2023overview}.

% 0-3. WHY IS IT IMPORTANT
Addressing the challenges of TOT retrieval is critical because searchers in the TOT state experience higher level of frustration compared to other types of memory-related search failures \cite{elsweiler2007towards}.
TOT retrieval spans a wide range of domains, from casual leisure search \cite{elsweiler2011casualleisure}, such as finding games \cite{gameTOT}, music \cite{Bhargav23MusicTOT}, and books \cite{Bhargav-2022-wsdm}, to critical applications like email retrieval \cite{Elsweiler08emailrefinding, Elsweiler2011Seeding, Kim09desktop} and enterprise content search \cite{Dumais03enterprise}. In professional settings, ineffective TOT support can lead to significant economic costs, as employees spend excessive time retrieving essential information, reducing productivity and efficiency \cite{white2015enterprisesearch}.


% New version that covers point 1, 2, 3 %%%%%%%%%%%%
The evaluation of TOT retrieval systems faces significant challenges due to limitations in test collection resources and current dataset construction methods. Most existing TOT queries in current datasets are sourced from community question answering (CQA) websites, which fail to fully capture the diversity of real-world TOT information needs \cite{Meier21-complex-reddit}. This reliance on CQA platforms introduces domain skewness, as existing datasets are heavily biased toward casual leisure topics like movies and books \cite{elsweiler2011casualleisure}, while underrepresenting areas such as people and landmarks. Even attempts to expand domain coverage \cite{Meier21-complex-reddit} have had limited success, leaving many domains underrepresented.
Additionally, the labor-intensive nature of dataset construction---which relies on manual annotation, coding, and refinement \cite{Bhargav-2022-wsdm, Bhargav23MusicTOT, arguello-movie-identification, Meier21-complex-reddit}---slows progress and restricts the ability to conduct large-scale and multi-domain evaluations. 


Advancing TOT retrieval research requires overcoming these challenges, as the lack of diverse and representative datasets leaves retrieval systems poorly optimized for real-world scenarios, reducing their ability to handle complex and ambiguous queries effectively.



However, current approaches fail to address these issues due to data scarcity, limited scalability, and restricted scope. Search engine query logs, which could provide real-world insights, remain largely inaccessible due to corporate privacy \cite{barbaro2006face} and confidentiality \cite{Poblete10confidentiality} concerns. 
As a result, prior efforts have relied on CQA platforms and manual labeling, but data collection restrictions from sites like Reddit, along with the scalability and domain bias issues, further limit dataset diversity and representativeness. These challenges underscore the need for more scalable and comprehensive approaches to TOT data collection and evaluation.
%%%%%%%%%%%%%



% 4. RESEARCH QUESTIONS
In response, this paper presents a novel evaluation framework for TOT retrieval systems, overcoming the limitations of CQA-based datasets by eliciting TOT queries from both large language models (LLMs) and humans. We develop an LLM-based TOT user simulator and a human query collection interface to address data scarcity, domain skewness, and scalability challenges, enabling more comprehensive evaluations.

Our research questions are as follows:\\
\textbf{RQ 1: Can we elicit TOT queries from LLMs for effective simulated evaluation of TOT retrieval systems?}
Unlike prior work, we explore LLM-elicited queries for TOT evaluation, validating our simulator by measuring system rank correlation and linguistic similarity with CQA-collected queries. By testing various prompt configurations and temperatures, we demonstrate that LLM-elicited queries can effectively support TOT retrieval evaluation with high validity.



\textbf{RQ 2: Can we elicit TOT queries from humans for effective simulated evaluation of TOT retrieval systems?}
To our knowledge, human-elicited TOT queries have not been previously used for evaluation. Similar to RQ1, we validate their effectiveness by comparing system rankings derived from human-elicited and CQA-based queries, demonstrating high alignment. Additionally, linguistic similarity analysis confirms that human-elicited queries share key characteristics with CQA-based queries, supporting their viability for TOT retrieval evaluation.




\textbf{RQ 3: Can the elicitation-based evaluation methods be used in other domains underrepresented in CQA-collected test collections?}
While the development and validation of our methods focus on the Movie domain---where ample CQA-collected data exists---we test their applicability to Landmark and Person domains, which are less represented in existing datasets. Validation results show that elicited queries can achieve high system rank correlations in these new domains, suggesting the potential research direction on the general method of collecting TOT queries across multiple domains.



In addition to the research contributions, the core outcomes of this work lie in its resource contributions:
\begin{itemize}
    \item A dataset of 450 synthetically generated TOT queries across three domains: Movie, Landmark, and Person.
    
    \item A method and prompt design for synthetically generating effective TOT queries from LLMs for simulated evaluation.\footnote{\url{https://github.com/kimdanny/llm-tot-query-elicitation}}
    
    \item A human TOT query elicitation interface designed to collect effective TOT queries for simulated evaluation along with a carefully curated visual stimuli collection, covering the Movie, Landmark, and Person domains.\footnote{\url{https://github.com/kimdanny/human-tot-query-elicitation-mturk}}
\end{itemize}
