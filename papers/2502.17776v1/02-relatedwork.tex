\section{Related Work}
%%%%%%%%%%%%%%
% Know-Item Retrieval and Query Simulation
%%%%%%%%%%%%%%
\subsection{Query Simulation and Know-Item Retrieval}

Query simulation methods have been used for various purposes, including document expansion \cite{nogueira2019docT5query} and synthetic test collection generation \cite{Rahmani24synthetic}. In the context of known-item retrieval, these methods have been explored to improve retrieval strategies \cite{OgilvieCallan03combining} and evaluation frameworks \cite{Azzopardi06testbeds, hagen2015corpus}.



%% Query Simulation
\textit{Simulating} the known-item queries has long been an active research area \cite{balog2006overviewWebclef, Azzopardi07SimulatedQueries, Kim09desktop, Elsweiler2011Seeding}.
Early work \cite{Azzopardi07SimulatedQueries} generated synthetic queries using term-based likelihood models, selecting query terms based on their likelihood within a randomly chosen document. Later studies adapted this approach for desktop search \cite{Kim09desktop} and email re-finding \cite{Elsweiler2011Seeding}, demonstrating its effectiveness for simulated evaluations of know-item retrieval models.
%
The \textit{validation} of these query simulators has also been a key focus.
System ranking correlation \cite{balog2006overviewWebclef}, retrieval score distribution comparisons \cite{Azzopardi07SimulatedQueries}, and synthetic versus human query resemblance \cite{Kim09desktop} have been used to assess their reliability.


While valuable, known-item search queries differ significantly from TOT queries, which are longer and more complex. Despite progress in simulating known-item queries, TOT retrieval remains unexplored. This paper bridges that gap by introducing novel TOT query elicitation methods and adapting established validation techniques \cite{zeigler2000theory} to ensure alignment with real-world queries, enabling scalable and accurate simulated evaluations.






%%%%%%%%%%%%%%
% TOT Datasets
%%%%%%%%%%%%%%
\subsection{TOT Datasets}
Several datasets have been developed to support research on TOT retrieval, primarily collected from online CQA platforms and focused on specific domains. MS-TOT \cite{arguello-movie-identification} was constructed from the \textit{IRememberThisMovie} website and human-annotated with tags in the Movie domain. It also includes qualitative coding of TOT queries and demonstrates significant room for improvement in current retrieval technologies for such information needs. Similarly, \citet{gameTOT} collected TOT queries from Reddit's \textit{/r/tipofmyjoystick} subreddit in the Game domain, providing coded tag information. Other datasets include Reddit-TOMT \cite{Bhargav-2022-wsdm}, focused on movies and books from Reddit's \textit{/r/tipofmytongue} subreddit; TOT-Music \cite{Bhargav23MusicTOT}, targeting the Music domain from the same subreddit; and Whatsthatbook \cite{lin-etal-2023-whatsthatbook}, sourced from \textit{GoodReads}, focused on the Book domain.



In response to the domain specificity of these datasets, recent efforts have aimed to expand TOT datasets across multiple areas. \citet{Meier21-complex-reddit} expanded to general casual leisure domains using data from six Reddit subreddits, including games, books, and music, although other identified domains, such as videos and people, remain underrepresented. Similarly, TOMT-KIS \cite{frobe2023-performance-pred} extended the collection from \textit{/r/tipofmytongue} by adapting \citet{Bhargav-2022-wsdm}'s approach with fewer filtering restrictions, resulting in 1.28 million TOT queries. However, only 47\% of these queries have identified answers, and the dataset continues to exhibit severe domain skewness toward a few topics. 


In this work, we develop and validate TOT query elicitation methods using the Movie domain for robust evaluation, then expand to Landmark and Person to assess applicability across underrepresented domains.


