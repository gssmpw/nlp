%%%%%%%%%%
% Results
%%%%%%%%%%
\subsection{Results and Analyses}
\textbf{RQ 1: Can we elicit TOT queries from LLMs for effective simulated evaluation of TOT retrieval systems?}

We tested various configurations to identify the most effective TOT query generation approach. The final prompt structure was determined through the extensive testing of different prompt templates, system roles, instruction types, and temperature settings, as well as evaluating the inclusion of Wikipedia summaries (Table \ref{tab:prompt-versions}). 
For each configuration, queries were generated using entities from the test set of the TREC TOT Track 2023\footnote{\url{https://trec-tot.github.io/guidelines-2023}}, derived from the MS-TOT dataset \cite{arguello-movie-identification}, and evaluated based on system rank correlation and linguistic similarity.


The top-performing strategies (Experiment ID 12 and 13) utilized a role-playing scenario, where the LLM assumed the role of a TOT searcher. The prompt began with:
\begin{tcolorbox}
Let's do a role play. You are now a person who watched a movie \{TOT Entity\} a long time ago and forgot the movie's name. 
\end{tcolorbox}
\noindent
This setup encouraged the LLM to generate queries that better captured real TOT search behavior, leading to improved retrieval rankings.

Specifically, the prompt included a summarized Wikipedia entry followed by 14 guideline rules, ensuring the searcher role-play aligned with real user behavior. 
For example, rules such as \textit{"Reflect the imperfect nature of memory with phrases that express doubt or mixed recollections"} and \textit{"Introduce a few incorrect or mixed-up details to make the recollection seem more realistic and challenging to pinpoint"} helped simulate the uncertainty and distorted memory typical of TOT searchers.
Similarly, \textit{"Share a personal anecdote related to when or with whom you watched the movie"} and \textit{"Focus on sensory details such as the overall mood, sounds, or emotional impact of the movie, using vivid descriptions"} aimed to incorporate the personal and emotional elements often found in real TOT queries.

While Experiment 12 achieved the highest Pearson correlation, Experiment 13 
resulted in the highest Kendall’s Tau correlation and the lowest linguistic distance from CQA-based queries.
%
A notable comparison is between Experiment IDs 11 and 13, which share the same temperature and 14 rules. The key difference lies in the rule structuring: while ID 11 presented all 14 rules equally, ID 13 divided them into 7 ``must" rules and 7 ``could" rules. This slight difference led to a significant increase in Kendall’s Tau and linguistic similarity, despite a minor drop in Pearson correlation. 


Through the experiments of different setups, we derived several key insights. Role-playing as a TOT searcher with explicit behavioral guidelines led to more realistically structured TOT queries. Providing generation rules proved more effective than few-shot prompting, as seen when transitioning from Prompt Version 2 (few-shot) to Prompt Version 4 (rule-based), which significantly improved validation scores.

Including a Wikipedia summary in the prompt substantially increased system rank correlation, indicating that structured background knowledge helps the LLM generate queries more aligned with real TOT queries. Additionally, lower temperature settings (0.3, 0.5) outperformed higher ones (0.7), contrary to our initial hypothesis that increased randomness would better simulate the stochastic nature of memory retrieval. The improved performance at lower temperatures likely stems from the model following the defined generation rules more closely.

Based on findings across three validation methods, we empirically conclude that LLM-generated TOT queries are an effective approach for designing simulated evaluations of TOT retrieval systems.







\begin{figure} 
\centering
\includegraphics[width=0.9\columnwidth]{03-synthetic/graphics/movie_cqa_llm_v6.pdf}
\caption{
Proportions of labeled codes in CQA and LLM-elicited queries in the Movie domain using Prompt Version V6 (Experiment ID 13). This setup achieved the lowest frequency distribution difference (EMD) among all experiments.
}
\label{fig:cqa-llm-v6-ling}
\end{figure}




\textbf{RQ 3-1: Can the LLM-elicitation-based evaluation methods be used to other domains underrepresented in CQA-collected test collections?}
\input{03-synthetic/person-landmark-correlations-table}

Based on our findings, we adopted the Experiment ID 13 configuration for the final dataset generation, extending its application to the underrepresented Landmark and Person domains. To adapt the Prompt V6 setup for these domains, we adjusted entity descriptions while preserving the core prompt structure.
For instance, the rule \textit{"Share a personal anecdote related to when or with whom you watched the movie"} was modified to \textit{"Share a personal anecdote related to the person"} to align with the context of the Person domain.

To validate the generated queries, we compared them against CQA-collected queries. Since existing TOT test collections are largely focused on the Movie domain, no established test collection exists for the Landmark and Person domains. To address this, we hand-curated a test collection, resulting in 22 Landmark-domain queries and 70 Person-domain queries from the Reddit \textit{/r/tipofmytongue} subreddit.

Table \ref{tab:person-landmark-correlation} presents the system rank correlation validation results for both domains. While the validation scores are not as strong as those observed in the Movie domain, they still demonstrate reasonably high values. This suggests that LLM-elicited query generation methods can be extended to underrepresented domains and could potentially be developed into generalizable LLM-elicitation techniques across multiple domains.


