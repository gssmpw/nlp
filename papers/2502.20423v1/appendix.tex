\section{EntRM supplements}
\label{app:entrm}

In this section, we give more insights about the Entropic Risk Measure. We first recall the definition of the EntRM.

Let $X$ be a random variable. The Entropic Risk $\mathrm{EntRM}_\beta[X]$ of parameter $\beta \in \mathbb{R}$ is defined as 
\begin{align*}
    \mathrm{EntRM}_\beta[X] =
    \begin{cases}
        \frac{1}{\beta}\,\log\!\bigl(\mathbb{E}[e^{\beta X}]\bigr), & \text{if } \beta \neq 0, \\
        \mathbb{E}[X], & \text{if } \beta = 0.
    \end{cases}
\end{align*}

\paragraph{Risk parameter interpretation.}
The parameter $\beta$ of EntRM measures risk tolerance. Large (positive) values of $\beta$ encourage a riskier behavior, as only the largest values taken by $X$ contribute significantly to the expectation. Conversely, negative values of $\beta$ promote a conservative behavior that minimizes potential losses, irrespective of the maximum reward. This behavior is illustrated by the values of the EntRM when the risk parameter approaches $\pm \infty$.

If the support of $X$ is $(R_{\min}, R_{\max})$, then 
\[
\lim_{\beta \rightarrow -\infty} \mathrm{EntRM}_\beta[X] = R_{\min} \text{ and }\lim_{\beta \rightarrow +\infty} \mathrm{EntRM}_\beta[X] = R_{\max}.
\]

When $\beta$ approaches zero, the EntRM converges to the expected value, leading to risk-neutral behavior: 
\[
\mathrm{EntRM}_\beta (X) \underset{\beta \to 0}{=} \mathbb E[X] + \frac{\beta}{2} \mathbb V[X] + o(\beta).
\]  
The risk neutral behavior obtained for $\beta = 0$ is just the expectation, and the variance appears as the sensitivity of EntRM. For a Gaussian random variable $X \sim \mathcal{N}(\mu, \sigma^2)$, it holds exactly that $\mathrm{EntRM}_\beta(X) = \mu + \frac{\beta}{2}\sigma^2$.

\paragraph{Dynamic programming.} Q-value function in the case of Entropic Risk Measure is defined as follows:
\begin{align}
\label{eq:entrm_bellman}
    Q^{\pi}_{h,\beta}(x,a) = \frac{1}{\beta}\log\left(\mathbb E\left[\exp\left(\beta R_h^\pi(x,a) \right) \right]\right).  
\end{align}
As shown in \citet{howard1972risk}, this Q-value also satisfies a Bellman equation and thus it can be optimized efficiently by value iteration, and can result in a deterministic optimal policy.

The Q-value function for the entropic risk of parameter $\beta$ satisfies the following optimal Bellman equations:
\begin{align}
    Q^{\pi}_{h,\beta}(x,a) &= \mathrm{EntRM}_\beta[r_h(x,a)] + \mathrm{EntRM}_\beta \left[Q_{h+1, \beta}^{\pi}(X',\pi(X'))\right]\\
    Q^{ * }_{h,\beta}(x,a) &= \mathrm{EntRM}_\beta[r_h(x,a)] + \mathrm{EntRM}_\beta \left[\max_{a'} Q_{h+1, \beta}^{*}(X',a')\right]
\end{align} 
Where $X' \sim p(\cdot | x,a)$.

\paragraph{Principle of Optimality.} Another implication of the Bellman equation is that the Entropic Risk Measure verifies the principle of Optimality, meaning there exists an optimal policy that is optimal for any subproblem: 
\begin{align}
    \exists \pi^*,\ \forall x,h, \quad \pi^*_{h:H} \in \argmax_\pi \mathrm{EntRM}_\beta[R^{\pi}(x)]
\end{align}
with $\pi_{h:H} = (\pi_h, \dots, \pi_H)$. This is in general not true for other risk measures, where a policy may only be optimal for a specific timestep and state, and can be suboptimal at intermediate timestep.

\paragraph{Exponential form}
While the EntRM is often defined as in \Cref{eq:EntRM-def}, the $\frac{1}{\beta} \ln$  serves solely as a renormalization factor. It does not influence the preference ordering between returns, and thus has no impact on the optimal policy in an MDP. For this reason, we also consider the exponential form $\text{sign}(\beta)\mathbb{E}_\pi[\exp(\beta X)]$ (also called \emph{exponential utility}), which can appear in some applications, like for the Threshold Probability objective. If a policy $\pi$ is optimal for $\mathrm{EntRM}_\beta$, it is also optimal for $E_\beta$, and vice versa. In general:
\begin{align*} 
    \mathrm{EntRM}_\beta(X_1) > \mathrm{EntRM}_\beta(X_2) 
    \Longleftrightarrow 
    \text{sign}(\beta)\cdot \mathbb{E}[\exp(\beta X_1)] > \text{sign}(\beta)\cdot \mathbb{E}[\exp(\beta X_2)] 
\end{align*}
and thus,
\begin{align*} 
    \argmax_{\pi} \mathrm{EntRM}_\beta[R^\pi] = \argmax_{\pi} \text{sign}\mathbb{E}_\pi[\exp(\beta R^\pi)].
\end{align*}
This equivalence is used at several occasions below.




% ------------------------------------------------------------
% ------------------------------------------------------------
% ------------------------------------------------------------




\section{More on the Value at Risk family of risk measures}
\label{app:var_family}

In this section, we provide additional details on the \emph{Value at Risk} (VaR), \emph{Conditional Value at Risk} (CVaR), and \emph{Entropic Value at Risk} (EVaR) measures discussed in the main text. Three main points are covered: (1) a clarification of conventions and notation, (2) key properties of VaR/CVaR, and (3) the derivation and theoretical advantages of EVaR.

\subsection{Conventions and Notation}

In the finance literature, VaR is generally introduced as the $\bigl(1 - \alpha\bigr)$-quantile of a \emph{loss} distribution, referring to a worst-case tail. It can also be defined via an $\alpha$-quantile for \emph{gains} (or returns), leading to slightly different formulas or sign conventions. For instance, one would usually write
\[
  \Pr\bigl(X \ge \mathrm{VaR}_{1-\alpha}[X]\bigr) = \alpha.
  \quad\text{while others use}\quad
  \Pr\bigl(X \le \mathrm{VaR}_\alpha[X]\bigr) = \alpha,
\]
Both definitions capture the idea of a critical quantile but from opposite sides of the distribution (gains vs.\ losses).

Throughout this work, we consistently adopt the low-tail perspective, defining
\[
  \mathrm{VaR}_\alpha[X] 
  \;=\;
  \inf\bigl\{\;x \,\big|\; \Pr(X \le x) \;\ge\;\alpha \bigr\},
\]
so that VaR at level $\alpha$ is simply the $\alpha$-quantile of $X$ from below. We focus on this definition to remain consistent with our usage of “negative outcomes” or “low returns” as the main source of risk. Should the convention change, one can substitute $X \leftarrow -X$ to convert between definitions without altering the mathematical properties.

To follow the change of convention, we also define the \emph{Conditional Value at Risk} (CVaR) as
\[
    \mathrm{CVaR}_\alpha[X]
    \;=\;
    \frac{1}{\alpha} \int_0^\alpha \mathrm{VaR}_\gamma[X] \, d\gamma.
    \quad\text{instead of}\quad
    \mathrm{CVaR}_{1-\alpha}[X]
    \;=\;
    \frac{1}{\alpha} \int_0^\alpha \mathrm{VaR}_{1-\gamma}[X] \, d\gamma.
\]
and the \emph{Entropic Value at Risk} (EVaR) as
\[
    \mathrm{EVaR}_\alpha[X]
    \;=\;
    \sup_{\beta < 0} \left\{ \frac{1}{\beta}\ln \mathbb E[e^{\beta X}] - \frac{1}{\beta}\ln(\alpha) \right\}
    \quad\text{instead of}\quad
    \inf_{\beta > 0}\left\{ \frac{1}{\beta}\ln \mathbb E[e^{\beta X}] - \frac{1}{\beta}\ln(1-\alpha) \right\}.
\]

\subsection{Entropic Value at Risk (EVaR)}

\emph{Entropic Value at Risk} (EVaR) was proposed as a tighter exponential-based bound on VaR and CVaR (\citealp{ahmadi-javid_entropic_2012}). Its derivation stems from the fact that exponential moment bounding can yield a close approximation to quantile-based measures. 

\begin{proof}
    By Chernoff's inequality, for any $\beta < 0$ we have:
    \[
      \Pr\bigl(X \leq \ell\bigr) \;\le\; \mathbb{E}\bigl[e^{\beta X}\bigr]\exp\bigl(-\beta\,\ell\bigr).
    \]
    Solving the equation
    \(
      \mathbb{E}\bigl[e^{\beta X}\bigr]\exp\bigl(-\beta\,\ell\bigr) \;=\; \alpha
    \)
    for $\ell$, we obtain
    \[
      \ell \;=\; a_X(\beta,\alpha)
      \;=\;
      \frac{1}{\beta}\,\ln\!\bigl(\mathbb{E}[e^{\beta X}]\bigr)
      \;-\;
      \frac{1}{\beta}\ln(\alpha).
    \]
    Hence, for any $\beta<0$ and $\alpha\in(0,1]$, the inequality
    \[
      \Pr\bigl(X \leq a_X(\beta, \alpha)\bigr)
      \;\le\;
      \alpha
    \]
    implies
    \[
      a_X(\beta, \alpha)
      \;\le\;
      \mathrm{VaR}_\alpha(X).
    \]
    Hence,
    \[
      \mathrm{EVaR}_\alpha(X)
      \;=\;
      \sup_{\beta<0}
      a_X\bigl(\beta, \alpha\bigr)
      \;\le\;
      \mathrm{VaR}_\alpha(X).
    \]
\end{proof}

% ------------------------------------------------------------
% ------------------------------------------------------------
% ------------------------------------------------------------




\section{Details on \Cref{sec:methods}}

\subsection{Derivation of Proxy Problems}
\label{app:proxy}

We here detail how the proxy problems are derived from the original problem.

\paragraph{Probability Threshold Problem.}

The inital problem is 
\[
    \min_\pi \;\Pr \!\bigl(R^\pi \;\le\; T\bigr),
\]

Using the Chernoff bound, and a few manipulations.
\begin{align*}
    \min_\pi \;\Pr\!\bigl(R^\pi \leq T\bigr) 
    &\leq \min_\pi \;\min_{\beta < 0} \mathbb{E}\bigl[e^{\beta(R^\pi - T)}\bigr] \\
    &= \min_{\beta < 0} \;\min_\pi \mathbb{E}\bigl[e^{\beta(R^\pi - T)}\bigr] \\
    &= \min_{\beta < 0} \mathbb{E}\bigl[e^{\beta\left(R^{\pi^*_\beta} - T\right)}\bigr].
\end{align*}
Where the inversion between the minimum on the policy and the minimum on $\beta$ is justified by the fact that there is only a finite number of policies.


Using results from \Cref{sec:structure}, we can further simplify the problem. Consider the EntRM optimal policies $\pi_1, \ldots, \pi_K$ and the corresponding optimality intervals $I_1, \ldots, I_K$. We have
\begin{align*}
    \min_\pi \;\Pr\!\bigl(R^\pi \leq T\bigr)
    &\leq \ldots \\
    &= \min_{\beta < 0} \mathbb{E}\bigl[e^{\beta(R^{\pi^*_\beta} - T)}\bigr] \\
    &= \min_k \;\min_{\beta \in I_k} \mathbb{E}\bigl[e^{\beta(R^{\pi_k} - T)}\bigr].
\end{align*}

\paragraph{Value at Risk Problem.} The initial problem is
\[
    \max_\pi \;\mathrm{(C)VaR}_\alpha\bigl[R^\pi\bigr].
\]

The similar manipulations lead to
\begin{align*}
    \max_\pi \;\mathrm{(C)VaR}_\alpha\bigl[R^\pi\bigr]
    &\geq \max_\pi \;\mathrm{EVaR}_\alpha\bigl[R^\pi\bigr] \\
    &= \max_\pi \sup_{\beta < 0} \mathrm{EntRM}_{\beta}\bigl[R^\pi\bigr] + \frac{1}{\beta}\ln(\alpha) \\
    &= \sup_{\beta < 0} \max_\pi \mathrm{EntRM}_{\beta}\bigl[R^\pi\bigr] + \frac{1}{\beta}\ln(\alpha) \\
    &= \sup_{\beta < 0} \mathrm{EntRM}_{\beta}\bigl[R^{\pi^*_\beta}\bigr] + \frac{1}{\beta}\ln(\alpha) \\
    &= \max_k \sup_{\beta \in I_k} \mathrm{EntRM}_{\beta}\bigl[R^{\pi_k}\bigr] + \frac{1}{\beta}\ln(\alpha).
\end{align*}
\subsection{Proof of \Cref{prop:approx_chernoff}}

We write $M_R(\beta)=\mathbb{E}[\exp(\beta R)]$ the moment generating function of the random variable $R$.

\begin{lemma}
\label{lemma:local_bound}
 Assume that $R$ is bounded between $-H$ and $H$, then for a fixed stepsize $\epsilon>0$, for $k \in \mathbb{N}$ and $\beta \in [\epsilon k, \epsilon (k+1)]$, noting $b_k = M_R(-\epsilon k)$, we have
 $$M_R(-\beta) \geq \exp(-\epsilon H)b_k \quad \text{and} \quad M_R(-\beta) \geq \exp(\epsilon H)b_{k+1},$$
 and,
 $$\frac{M_R(-\beta)}{\min \{b_k,b_{k+1}\}} \geq \exp\bigg(\epsilon \frac{-H}{2}\bigg) \bigg(\frac{b_k}{b_{k+1}}\bigg)^{\frac{1}{2H}} \geq \exp\bigg(\epsilon \frac{-H}{2}\bigg)\;,$$
 otherwise the ratio is greater or equal to $1$.
\end{lemma}

\begin{proof}
 First we have
 \begin{align*}
  M_R(-\beta)&=\mathbb{E}[\exp(-(\beta - \epsilon k)R)\exp(-\epsilon kR)] \\
  &\geq \exp(-(\beta - \epsilon k)H)\mathbb{E}[\exp(-\epsilon kR)] \geq \exp(-\epsilon H)b_k, \\
  M_R(-\beta)&=\mathbb{E}[\exp((-\beta + \epsilon (k+1))R)\exp(-\epsilon (k+1)R)] \\
  &\geq \exp(-(\epsilon (k+1)-\beta)H)\mathbb{E}[\exp(-\epsilon (k+1)R)] \geq \exp(-\epsilon H)b_{k+1}\;.
 \end{align*}
To obtain the following inequality it is enough to consider the intersection of the functions $t \mapsto \exp(-t H)b_k$ and $t \mapsto \exp((-\epsilon -t)H)b_{k+1}$ on $t \in [0,\epsilon]$: as $M_R(-\beta)$ follows both left decreasing and right increasing constraint, the minimal value possible is on the intersection.
\end{proof}

We can now give a proof of Proposition \ref{prop:approx_chernoff}.
\begin{proof}
Let $\pi_c$ and $\beta_c$ be the policy and the $\beta$ optimizing the Chernoff bound $B$. First, we can consider that $\beta_c < \ln(1/p)/a\}$. Indeed if not we can choose $\beta_c = 0$ instead:

\begin{align*}
M_{R}(\beta_c) &= \mathbb{E}[\exp(-\beta R)\mathds{1}\{R\leq a\}]+\mathbb{E}[\exp(-\beta R)\mathds{1}\{R>a\}] \\
&\geq \exp(-\beta_c a)\mathbb{P}[R\leq a] +\exp(-\beta M)\mathbb{P}[R \geq a]\\
&\geq \exp(-\beta_c a)p \geq 1 = M_{R}(0) \quad \text{using the hypothesis on }\beta_c\;. \\
\end{align*}

Let then $k\in\mathbb{N}$ be such that $\epsilon k\leq \beta_c < \epsilon(k+1)$, and suppose  $M_{R^{\pi_c}}(-\epsilon k)  \leq M_{R^{\pi_c}}(-\epsilon (k+1))$ without loss of generality. We have
$$\tilde{B} \leq \min_\pi M_{R^\pi}(-\epsilon k) \leq M_{R^{\pi_c}}(-\epsilon k) \leq \exp \bigg( \epsilon \frac{H}{2}\bigg)M_{R^{\pi_c}}(\beta_c) \leq \exp \bigg(\epsilon \frac{H}{2}\bigg)B,$$
using \Cref{lemma:local_bound}.

For $\epsilon \gets 2\log(1\varepsilon)/H$, we have that $\tilde{B} \leq (1+\varepsilon)B$.
\end{proof}


% ------------------------------------------------------------
% ------------------------------------------------------------
% ------------------------------------------------------------



\section{Proof of \Cref{sec:structure}}
\label{app:proof_structure}

\subsection{Proof of \Cref{pro:finite_action_change}.}


\begin{proof}[Proof of \Cref{pro:finite_action_change}]
    
    By \Cref{thm:beta_min}, there exist values $\beta_{\min}$ and $\beta_{\max}$ such that for all $\beta < \beta_{\min}$ (respectively, $\beta > \beta_{\max}$), a single action remains optimal. Consequently, any ``breakpoint'' (a value of $\beta$ at which the optimal action changes) must lie in the finite interval $[\beta_{\min}, \beta_{\max}]$.  
      
    Let $a_i$ and $a_j$ be two different actions. Suppose at some $\beta$, both $a_i$ and $a_j$ are optimal for $\mathrm{EntRM}_\beta(R(\cdot))$. That implies:
    \[
        \sum_i \mu_i e^{\,\beta\,x_i}
        \;=\;
        \sum_j \mu_j e^{\,\beta\,x_j},
    \]
    where $\bigl\{\mu_i, x_i\bigr\}$ and $\bigl\{\mu_j, x_j\bigr\}$ refer to the reward distributions (or points in the returns) associated with actions $a_i$ and $a_j$. The map $\beta \mapsto \sum_k \mu_k e^{\,\beta\,x_k}$ is holomorphic (analytic) as it is a finite sum of exponentials. Two distinct holomorphic functions on a finite interval can only coincide at a finite number of points, unless they coincide everywhere.  
    
    Therefore, there are only finitely many $\beta$ values where
    \[
        \mathrm{EntRM}_\beta\bigl(R(a_i)\bigr)
        \;=\;
        \mathrm{EntRM}_\beta\bigl(R(a_j)\bigr).
    \]
    Because the optimal policy can only change when two action-value curves intersect (thus altering the $\argmax$ set), a finite number of pairwise intersections implies a finite number of breakpoints in $[\beta_{\min}, \beta_{\max}]$.  
    
    Assume an action $a$ is the unique optimum at some $\beta_0$. Formally,
    \[
        \mathrm{EntRM}_{\beta_0}\bigl(R(a)\bigr)
        \;>\;
        \mathrm{EntRM}_{\beta_0}\bigl(R(a')\bigr)
        \quad
        \text{for all } a' \neq a.
    \]
    By continuity of $\beta \mapsto \mathrm{EntRM}\bigl(R(a)\bigr)$, there exists $\varepsilon > 0$ such that for all $\beta' \in [\beta_0 - \varepsilon,\;\beta_0 + \varepsilon]$, the same strict inequality holds. Hence, $a$ remains the unique optimal action throughout this interval around $\beta_0$.  
\end{proof}

\subsection{Proof of \Cref{thm:interval_policy_change}.}

We first start by deriving bounds on the growth of the EntRM when the risk parameter is changed slightly.

\begin{proposition}
    \label{pro:framing}
    Let $X$ be a random variable, $\beta \in \mathbb{R}$, and $0 < \varepsilon < |\beta|$. Then:

    If $\beta > 0$:
    \begin{align}
        \frac{\beta}{\beta+\varepsilon} \mathrm{EntRM}_\beta[X] + \frac{\varepsilon}{\beta+\varepsilon} r_{\min} 
        &\leq \mathrm{EntRM}_{\beta+\varepsilon}[X] 
        \leq \frac{\beta}{\beta+\varepsilon} \mathrm{EntRM}_\beta[X] + \frac{\varepsilon}{\beta+\varepsilon} r_{\max}, \\
        \frac{\beta}{\beta-\varepsilon} \mathrm{EntRM}_\beta[X] - \frac{\varepsilon}{\beta-\varepsilon} r_{\max} 
        &\leq \mathrm{EntRM}_{\beta-\varepsilon}[X] 
        \leq \frac{\beta}{\beta-\varepsilon} \mathrm{EntRM}_\beta[X] - \frac{\varepsilon}{\beta-\varepsilon} r_{\min}.
    \end{align}

    If $\beta < 0$:
    \begin{align}
        \frac{\beta}{\beta+\varepsilon} \mathrm{EntRM}_\beta[X] + \frac{\varepsilon}{\beta+\varepsilon} r_{\max} 
        &\leq \mathrm{EntRM}_{\beta+\varepsilon}[X] 
        \leq \frac{\beta}{\beta+\varepsilon} \mathrm{EntRM}_\beta[X] + \frac{\varepsilon}{\beta+\varepsilon} r_{\min}, \\
        \frac{\beta}{\beta-\varepsilon} \mathrm{EntRM}_\beta[X] - \frac{\varepsilon}{\beta-\varepsilon} r_{\min} 
        &\leq \mathrm{EntRM}_{\beta-\varepsilon}[X] 
        \leq \frac{\beta}{\beta-\varepsilon} \mathrm{EntRM}_\beta[X] - \frac{\varepsilon}{\beta-\varepsilon} r_{\max}.
    \end{align}
\end{proposition}


\begin{proof}
    In the following, $0 < \varepsilon < |\beta|$. We write $X = \sum \mu_i \delta_{x_i}$, with $r_{\min} = \min_i x_i$ and $r_{\max} = \max_i x_i$.

    For $\beta > 0$:
    \begin{align*}
        \mathrm{EntRM}_{\beta+\varepsilon}[X] &= \frac{1}{\beta+\varepsilon} \ln\left(\sum \mu_i e^{(\beta+\varepsilon) x_i}\right)\\ 
        &= \frac{1}{\beta+\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i} e^{\varepsilon x_i}\right) \\
        &\leq \frac{1}{\beta+\varepsilon} \ln\left(e^{\varepsilon r_{\max}} \sum \mu_i e^{\beta x_i}\right) \quad \text{(since } \frac{1}{\beta+\varepsilon} > 0) \\
        &= \frac{1}{\beta+\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i}\right) + \frac{\varepsilon}{\beta+\varepsilon} r_{\max} \\
        &= \frac{\beta}{\beta+\varepsilon} \mathrm{EntRM}_\beta[X] + \frac{\varepsilon}{\beta+\varepsilon} r_{\max},
    \end{align*}
    and
    \begin{align*}
        \mathrm{EntRM}_{\beta-\varepsilon}[X] &= \frac{1}{\beta-\varepsilon} \ln\left(\sum \mu_i e^{(\beta-\varepsilon) x_i}\right)\\ 
        &= \frac{1}{\beta-\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i} e^{-\varepsilon x_i}\right) \\
        &\geq \frac{1}{\beta-\varepsilon} \ln\left(e^{-\varepsilon r_{\max}} \sum \mu_i e^{\beta x_i}\right) \quad \text{(since } \frac{1}{\beta-\varepsilon} > 0) \\
        &= \frac{1}{\beta-\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i}\right) - \frac{\varepsilon}{\beta-\varepsilon} r_{\max} \\
        &= \frac{\beta}{\beta-\varepsilon} \mathrm{EntRM}_\beta[X] - \frac{\varepsilon}{\beta-\varepsilon} r_{\max}.
    \end{align*}
    
    For $\beta < 0$:
    \begin{align*}
        \mathrm{EntRM}_{\beta+\varepsilon}[X] &= \frac{1}{\beta+\varepsilon} \ln\left(\sum \mu_i e^{(\beta+\varepsilon) x_i}\right)\\ 
        &= \frac{1}{\beta+\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i} e^{\varepsilon x_i}\right) \\
        &\leq \frac{1}{\beta+\varepsilon} \ln\left(e^{\varepsilon r_{\min}} \sum \mu_i e^{\beta x_i}\right) \quad \text{(since } \frac{1}{\beta+\varepsilon} < 0) \\
        &= \frac{1}{\beta+\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i}\right) + \frac{\varepsilon}{\beta+\varepsilon} r_{\min} \\
        &= \frac{\beta}{\beta+\varepsilon} \mathrm{EntRM}_\beta[X] + \frac{\varepsilon}{\beta+\varepsilon} r_{\min},
    \end{align*}
    and
    \begin{align*}
        \mathrm{EntRM}_{\beta-\varepsilon}[X] &= \frac{1}{\beta-\varepsilon} \ln\left(\sum \mu_i e^{(\beta-\varepsilon) x_i}\right)\\ 
        &= \frac{1}{\beta-\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i} e^{-\varepsilon x_i}\right) \\
        &\geq \frac{1}{\beta-\varepsilon} \ln\left(e^{-\varepsilon r_{\min}} \sum \mu_i e^{\beta x_i}\right) \quad \text{(since } \frac{1}{\beta-\varepsilon} < 0) \\
        &= \frac{1}{\beta-\varepsilon} \ln\left(\sum \mu_i e^{\beta x_i}\right) - \frac{\varepsilon}{\beta-\varepsilon} r_{\min} \\
        &= \frac{\beta}{\beta-\varepsilon} \mathrm{EntRM}_\beta[X] - \frac{\varepsilon}{\beta-\varepsilon} r_{\min}.
    \end{align*}
    The other sides of the inequalities are obtained in a similar manner.
\end{proof}

We then consider the easier case of a single state MDP, which is what will be used in practice for the algorithm. There is $n$ actions $a_1, \ldots, a_n$ with reward distribution $R(a_1), \ldots, R(a_n)$.

\begin{theorem}[Interval of Action Optimality]
    \label{thm:interval_action_change}
    Let $r_{\min}$ (resp.\ $r_{\max}$) be the minimum (resp.\ maximum) achievable reward, and $\Delta R = r_{\max} - r_{\min}$. Suppose we have actions $(a_{(i)})_i$ ordered so that
    \[
        \mathrm{EntRM}_\beta\bigl[R(a_{(1)})\bigr] 
        \;>\;
        \mathrm{EntRM}_\beta\bigl[R(a_{(2)})\bigr] 
        \;\ge\; \dots 
        \;\ge\;
        \mathrm{EntRM}_\beta\bigl[R(a_{(n)})\bigr].
    \]
    In particular, \(a_{(1)}\) is the unique optimal action and \(a_{(2)}\) is the second-best. Define
    \[
        \Delta U 
        \;=\; 
        \mathrm{EntRM}_\beta\bigl[R(a_{(1)})\bigr]
        \;-\;
        \mathrm{EntRM}_\beta\bigl[R(a_{(2)})\bigr].
    \]
    Then:
    \begin{itemize}
        \item If $\beta \neq 0$, for all 
        \(\beta' \in \bigl[\beta\bigl(1 - \tfrac{\Delta U}{\Delta R}\bigr),\; \beta\bigl(1 + \tfrac{\Delta U}{\Delta R}\bigr)\bigr]\)
        and for all $i \geq 2$, we have
        \[
            \mathrm{EntRM}_{\beta'}\bigl[R(a_{(1)})\bigr] 
            \;>\; 
            \mathrm{EntRM}_{\beta'}\bigl[R(a_{(i)})\bigr].
        \]
        \item If $\beta = 0$, then for all 
        \(\beta' \in \Bigl[-\tfrac{8\,\Delta U}{\Delta R^2},\;\tfrac{8\,\Delta U}{\Delta R^2}\Bigr]\)
        and all $i \geq 2$, we have
        \[
            \mathrm{EntRM}_{\beta'}\bigl[R(a_{(1)})\bigr]
            \;>\;
            \mathrm{EntRM}_{\beta'}\bigl[R(a_{(i)})\bigr].
        \]
    \end{itemize}
\end{theorem}
    

\begin{proof}[Proof of \Cref{thm:interval_action_change} for the case $\beta \neq 0$]

    Assume $\beta \neq 0$. Let
    \[
        U_\beta^1 \;=\; \mathrm{EntRM}_\beta\bigl[R(a_{(1)})\bigr],
        \quad
        U_\beta^2 \;=\; \mathrm{EntRM}_\beta\bigl[R(a_{(2)})\bigr].
    \]
    By hypothesis, $U_\beta^1 > U_\beta^2$. Define $\Delta U = U_\beta^1 - U_\beta^2$ and $\Delta R = r_{\max} - r_{\min}$. We aim to show that if $\beta'$ remains within the specified range around $\beta$, action $a_{(1)}$ remains strictly optimal for $\mathrm{EntRM}_{\beta'}$.
    
    \textbf{Case $\beta' > \beta$ and $\beta > 0$.}

    Write $\beta' = \beta + \varepsilon$ with $\varepsilon > 0$. Assume
    \[
        \varepsilon < \beta \cdot \frac{\Delta U}{\Delta R}.
    \]
    From the previously established bounds (see \Cref{pro:framing}), we know
    \[
        \frac{\beta}{\beta + \varepsilon} \,U_\beta^1 
        \;+\; 
        \frac{\varepsilon}{\beta + \varepsilon}\,r_{\min}
        \;\le\;
        \mathrm{EntRM}_{\beta + \varepsilon}\bigl[R(a_{(1)})\bigr]
        \;=\;
        U_{\beta'}^1,
    \]
    and
    \[
        \mathrm{EntRM}_{\beta + \varepsilon}\bigl[R(a_{(2)})\bigr]
        \;=\;
        U_{\beta'}^2
        \;\le\;
        \frac{\beta}{\beta + \varepsilon}\,U_\beta^2 
        \;+\; 
        \frac{\varepsilon}{\beta + \varepsilon}\,r_{\max}.
    \]
    Thus, showing 
    \[
        \frac{\beta}{\beta + \varepsilon}\,U_\beta^1
        \;+\;
        \frac{\varepsilon}{\beta + \varepsilon}\,r_{\min}
        \;>\;
        \frac{\beta}{\beta + \varepsilon}\,U_\beta^2
        \;+\;
        \frac{\varepsilon}{\beta + \varepsilon}\,r_{\max}
    \]
    implies $U_{\beta'}^1 > U_{\beta'}^2$. Rewriting, we get
    \[
        \frac{\beta}{\beta + \varepsilon}\bigl(U_\beta^1 - U_\beta^2\bigr)
        \;>\;
        \frac{\varepsilon}{\beta + \varepsilon}\,\bigl(r_{\max} - r_{\min}\bigr),
    \]
    i.e.
    \[
        \beta \,\frac{\Delta U}{\Delta R}
        \;>\;
        \varepsilon.
    \]
    But this is exactly our assumption on $\varepsilon$. Hence $a_{(1)}$ remains strictly better than $a_{(2)}$ at $\beta' = \beta + \varepsilon$, and by extension, better than all other actions.
    
    \textbf{Case $\beta' < \beta$ and $\beta > 0$.}
    
    Now let $\beta' = \beta - \varepsilon$ with $\varepsilon > 0$. We assume
    \[
        \varepsilon < \beta \,\frac{\Delta U}{\Delta R}.
    \]
    An analogous argument shows
    \[
        \frac{\beta}{\beta - \varepsilon}\,\bigl(U_\beta^1 - U_\beta^2\bigr)
        \;>\;
        \frac{\varepsilon}{\beta - \varepsilon}\,\bigl(r_{\max} - r_{\min}\bigr).
    \]
    This inequality, combined with the upper and lower bounds for $U_{\beta - \varepsilon}^1$ and $U_{\beta - \varepsilon}^2$, yields $U_{\beta'}^1 > U_{\beta'}^2$. Consequently, $a_{(1)}$ remains the strictly optimal action at $\beta' = \beta - \varepsilon$.
    
    The case $\beta < 0$ is similar, using the associated inequalities from \Cref{pro:framing}.
    
    Hence, for all parameter shifts $\beta' \in [\beta(1 - \Delta U/\Delta R),\;\beta(1 + \Delta U/\Delta R)]$, the action $a_{(1)}$ remains strictly optimal. This completes the proof for $\beta \neq 0$.
    \end{proof}
    

The proof of the second part of \Cref{thm:interval_action_change}, when $\beta = 0$, uses \emph{Hoeffding's lemma} (see e.g. \citet{massart2007concentration}):
\begin{equation}
    \forall \lambda \in \RR, \quad \mathbb E[\exp(\lambda X)] \leq \exp\left(\lambda \E[X] + \frac{\lambda^2 \Delta R^2}{8}\right)
\end{equation}

\begin{proof}{(second part of \Cref{thm:interval_action_change})}
    Hoeffding's lemma gives
    \begin{equation}
        \left\{\begin{array}{lr}
        \mathrm{EntRM}_\beta[X] \leq \E[X] + \beta \frac{\Delta R^2}{8} & \text{if } \beta > 0\hbox{ , and}\\
        \mathrm{EntRM}_\beta[X] \geq \E[X] - \beta \frac{\Delta R^2}{8} & \text{if } \beta < 0\;.
        \end{array}\right.
    \end{equation}
    We can then proceed similarly as the previous proof. Consider $0 < \beta < \frac{8\Delta\mu}{\Delta R^2}$. Using the monotonicity of $\beta\mapsto U_\beta$, $U_\beta^1 \geq E[R(a_1)]$ and
\[
        U_\beta^1 - U_\beta^2 \geq \E[R(a_1)] - \E[R(a_2)] - \beta \frac{\Delta R^2}{8} 
        \geq \Delta\mu -  \frac{8\Delta\mu}{\Delta R^2}\frac{\Delta R^2}{8} = 0\;,
    \]
    and $U_\beta^1 > U_\beta^2$. The case $\beta < 0$ is similar.
\end{proof}    

Better bounds can also be derived, yet they are not symmetrical. These better bounds will be those used in practice in \Cref{alg:state}.

\begin{proposition}{Better bounds}
    \label{prop:better_interval_action_change}
    Let $\beta \in \RR \setminus \{0\}$. Let $a_1, \dots, a_n \in \actions$ and $R(a_i)$ be the associated rewards. Without loss of generality, assume that $U_\beta(R(a_1)) > U_\beta(R(a_2)) \geq \dots \geq U_\beta(R(a_n))$ (i.e., $a_1$ is the unique optimal action). Define $\Delta U = U_\beta(R(a_1)) - U_\beta(R(a_2))$.
    
    If $\beta > 0$, for all $\beta' \in \left[\beta\left(1 - \frac{\Delta U}{r_{\max} - U_2}\right), \beta\left(1 + \frac{\Delta U}{r_{\max} - U_1}\right)\right]$, the optimal action for $U_{\beta'}$ is $a_1$.

    If $\beta < 0$, for all $\beta' \in \left[\beta\left(1 - \frac{\Delta U}{U_2 - r_{\min}}\right), \beta\left(1 + \frac{\Delta U}{U_1 - r_{\min}}\right)\right]$, the optimal action for $U_{\beta'}$ is $a_1$.
\end{proposition}

We can verify that these intervals are better than those given by \Cref{thm:interval_action_change} by noting that $r_{\min} \leq U_\beta(R) \leq r_{\max}$. Therefore, $\frac{\Delta U}{r_{\max} - U} > \frac{\Delta U}{\Delta R}$ and $\frac{\Delta U}{U - r_{\min}} > \frac{\Delta U}{\Delta R}$.

\begin{proof}
    The proof is the same as in \Cref{pro:finite_action_change}, but using the monotonicity bounds $U_\beta(\mu) \leq U_{\beta + \varepsilon}(\mu)$ and $U_\beta(\mu) \geq U_{\beta - \varepsilon}(\mu)$ .
\end{proof}

Finally, we can prove \Cref{thm:interval_policy_change}. We use the same principle as in the proof of \Cref{thm:interval_action_change} and the principle of optimality (i.e. the optimal policy is always greedy with respect to itself) to show that the optimal policy remains the same. The inequalities from \Cref{pro:framing} are adapted by replacing $r_{\max}$ by $h\cdot r_{\max}$ and $r_{\min}$ by $h\cdot r_{\min}$ (for timestep $h$, the return is in $[h\cdot r_{\min}, h\cdot r_{\max}]$).

\begin{proof}{(of \Cref{thm:interval_policy_change})}
    Let us address the case where $\beta > 0$.  
    By the principle of optimality, we have:
    \[
        \forall h, x, \quad 
        \pi_{h,\beta}^{*}(x) = \arg \max_a \mathrm{EntRM}_\beta[R^{\pi^*}_h(x,a)]\;.
    \]
    Let $\varepsilon < \Delta$ (with $\varepsilon < \beta$).  
    We then obtain the inequalities:
    \[
        \forall h, x, a, \quad 
        \frac{\beta}{\beta + \varepsilon} \mathrm{EntRM}_\beta[R^{\pi^*}_h(x,a)] 
        + \frac{\varepsilon}{\beta + \varepsilon} h r_{\min} 
        \;\leq\; \mathrm{EntRM}_{\beta+\varepsilon}[R^{\pi^*}_h(x,a)] 
        \;\leq\; \frac{\beta}{\beta + \varepsilon} \mathrm{EntRM}_\beta[R^{\pi^*}_h(x,a)] 
        + \frac{\varepsilon}{\beta+\varepsilon} h r_{\max}\;.
    \]
    By the definition of $\varepsilon$, we get:
    \[
        \forall h, x, \forall a\neq\pi^*_h(x), \quad 
        \frac{\beta}{\beta + \varepsilon} \mathrm{EntRM}_\beta[R^{\pi^*}_h(x,a)] 
        + \frac{\varepsilon}{\beta + \varepsilon} h r_{\max} 
        \;\leq\; \frac{\beta}{\beta + \varepsilon} \mathrm{EntRM}_{\beta}[R^{\pi^*}_h(x,\pi^*_h(x))] 
        + \frac{\varepsilon}{\beta+\varepsilon} h r_{\min}.
    \]
    Therefore, 
    \begin{align*}
        \mathrm{EntRM}_{\beta'}[R^{\pi^*}_h(x,a)] 
        &\leq \frac{\beta}{\beta + \varepsilon} \mathrm{EntRM}_{\beta}[R^{\pi^*}_h(x,a)] 
        + \frac{\varepsilon}{\beta + \varepsilon} h r_{\max} \\
        &\leq \frac{\beta}{\beta + \varepsilon} \mathrm{EntRM}_{\beta}[R^{\pi^*}_h(x,\pi^*_h(x))] 
        + \frac{\varepsilon}{\beta+\varepsilon} h r_{\min} \\
        & \leq \mathrm{EntRM}_{\beta'}[R^{\pi^*}_h(x,\pi^*_h(x))] \;.
    \end{align*}
    Thus, $\pi^*$ remains greedy with respect to itself and continues to be the optimal policy.

    The cases $\beta = 0$ and $\beta < 0$ follow similarly, using the associated inequalities.
\end{proof}



\subsection{About \Cref{pro:policy_change}.}
\label{app:policy_change}
We first start with a lemma.

\begin{lemma}
    \label{lem:conditional}
    Let $Y$ be a random variable with continuous law.
    Let $X_1, \ldots, X_n$ be random variables with $Y$ independant from $(X_1, \ldots, X_n)$. Then,
    \[\Pr\bigl( Y = f(X_1, \dots, X_n) \mid X_1, \dots, X_n \bigr) = 0\]
\end{lemma}
\begin{proof}
    This is a special case of Exercise 2.1.5 in \citet{durrett2019probability}. It comes down to writing the definition of conditional probabilities and computing the integrals with Fubini's theorem.
\end{proof}


\begin{proof}(of \Cref{pro:policy_change})

    Let $\pi$ be a policy. Assume the probability transitions are fixed and only the rewards are random. We consider, for $x,h,a,a'$ the set $\breakpoints^{x,h}_{a,a'}$ of parameters such that $\mathrm{EntRM}_{\beta}[R^{\pi}_h(x,a)] = \mathrm{EntRM}_{\beta}[R^{\pi}_h(x,a')]$. We aim to show that $(\breakpoints^{x,h}_{a,a'})_{x,h,a,a'}$ have no element in common pairwise, with probability one.

    Consider orders on both $\states$ and $\actions$, and consider the associated lexigocraphic order of $[ H, 1 ] \times \states \times \actions$. We proceed by induction on this order.

    \textbf{Case $t = H$.}

    Let $x_1, x_2 \in \states$, $a_1,a'_1, a_2, a'_2 \in \actions$, with $(x_1, a_1, a_1') \neq (x_2, a_2, a_2')$ (the triplets are different, but some elements of the triplets can be equal). Consider known $\breakpoints^{x_1,H}_{a_1,a'_1}$. 

    \begin{align*}
        \forall \beta \in \mathbb{R}, \quad 
        \Pr\Big( \mathrm{EntRM}_{\beta}&[R^{\pi}_H(x_2,a_2)] = \mathrm{EntRM}_{\beta}[R^{\pi}_H(x_2,a'_2)] 
        \;\Big|\; r_H(x_1, a_1), r_H(x_1, a_1') \Big) \\
        &= \Pr\Big( r_H(x_2,a_2) = r_H(x_2,a'_2) 
        \;\Big|\; r_H(x_1, a_1), r_H(x_1, a_1') \Big) \\
        &= 0.
    \end{align*}    
 
    Because $r_H(x_2,a_2)$ and $r_H(x_2,a'_2)$ are continuous random variables and at least one of the two is not conditioned on, the probability that they are equal is zero according to \Cref{lem:conditional}.

    It is in particular true for all $\beta \in \breakpoints^{x_1,H}_{a_1,a'_1}$. Using the union bound, we get that $\breakpoints^{x_1,H}_{a_1,a'_1}$ and $\breakpoints^{x_2,H}_{a_2,a'_2}$ have no element in common with probability one.

    By considering elements in order and conditioning on the previously \emph{observed} rewards, the induction is verified for $t = H$.
    
    \textbf{Case $t < H$.}

    Let $u = h_0, x_0, a_0$. Consider all breakpoints observed before 
    \[\breakpoints = \bigcup_{\substack{(h,x, a) < u\\ (h,x, a') \leq u}}\breakpoints^{x,h}_{a,a'}. \]
    By induction, all of them are disjoint pairwise with probability one.

    \begin{align*}
        \forall \beta &\in \breakpoints, \quad 
        \Pr\Big( \mathrm{EntRM}_{\beta}[R^{\pi}_{h_0}(x_0,a_0)] = 
        \mathrm{EntRM}_{\beta}[R^{\pi}_{h_0}(x_0,a'_0)] 
        \;\Big| \big(r_h(x, a)\big)_{(h,x,a) \leq u} \Big) \\
        &= \Pr\Big( r_{h_0}(x_0,a_0) + \mathrm{EntRM}_{\beta}[R^{\pi}_{h_0+1}(X_0)] = r_{h_0}(x_0,a'_0) + \mathrm{EntRM}_{\beta}[R^{\pi}_{h_0+1}(X_0')] 
        \;\Big| \; \big(r_h(x, a)\big)_{(h,x,a) \leq u} \Big) \\
        &= P\Big( r_{h_0}(x_0,a_0) = f\left(\big(r_h(x, a)\big)_{(h,x,a) \leq u}\right)
        \;\Big| \; \big(r_h(x, a)\big)_{(h,x,a) \leq u} \Big) \\
        &= 0.
    \end{align*}
    
    Where $f\left(\big(r_h(x, a)\big)_{(h,x,a) \leq u}\right) = r_{h_0}(x_0,a'_0) + \mathrm{EntRM}_{\beta}[R^{\pi}_{h_0+1}(X_0')] - \mathrm{EntRM}_{\beta}[R^{\pi}_{h_0+1}(X_0)]$ and using \Cref{lem:conditional}.

    The induction is then proved.

    As there is a finite number of policy, this result remains when considering the set of all policies. 

    To conclude, notice that a breakpoint $\beta_0$ is a value of risk parameter for which two different action $a_1, a_2$ have the same expected return for some state $x$ and timestep $t$. Hence, $\beta_0 \in \breakpoints^{x,h}_{a_1,a_2}$. With probability one, those set are pairwise disjoint, meaning a single action changes in $\beta_0$.

    As this proof works for any value of the probability transitions, the result also remains for $p$ random.
\end{proof}

The result is what we observe for all the MDPs used in practice. Yet it is simple to construct an MDP where several actions change at the same time, it is enough to have two states $x_1, x_2$ have the same transitions function and rewards : $\forall a, x', p(x' | x_1, a) = p(x' | x_2, a)$ and $r(x_1, a) = r(x_2, a)$.

\subsection{Proof of \Cref{pro:eq_breakpoint}.}
\begin{proof}(of \Cref{pro:eq_breakpoint})
    It all relies on the continuity of the function $\beta \rightarrow \mathrm{EntRM}_\beta[R^\pi]$.  

    \begin{align*}
        \forall h,x, \quad 
        &\mathrm{EntRM}_{\beta_1}[R^{\pi^1}_h(x)] \geq \mathrm{EntRM}_{\beta_1}[R^{\pi^2}_h(x)]\\
        &\mathrm{EntRM}_{\beta_3}[R^{\pi^1}_h(x)] \leq \mathrm{EntRM}_{\beta_3}[R^{\pi^2}_h(x)]
    \end{align*}

    By continuity, there exists $\beta_2$ such that there is an equality, and, since we assume that there are no other optimal policy in $[\beta_1, \beta_3]$, the equality is verified for all $x,h$.
\end{proof}

\subsection{On the lowest breakpoint.}
\begin{theorem}
    \label{thm:beta_min}
    Consider $\pi_{\inf} = \lim_{\beta \rightarrow -\infty} \pi^*_\beta$. Consider the return $R$ taking value in $x_1 \leq \ldots \leq x_n$. We write $a_i(\pi) = P(R^{\pi_{\inf}} = x_i) - \Pr(R^{\pi}  = x_i)$. Then, the lowest breakpoint $\beta_{\inf}$ verifies
    \[
        \beta_{\inf} \geq \frac{-\log\left(1 + \max_{\pi \neq \pi_{\inf}} \max_{i > 0} \frac{|a_i(\pi)|}{|a_0(\pi)|}\right)}{\min_{i\neq j} |x_i - x_j|}\ .
    \]
\end{theorem}
\begin{proof}(of \Cref{thm:beta_min})
    
    We write $\Delta R = R_{\max} - R_{\min}$. We assume here for simplicity that all the rewards are even spaced on a grid. This mean that the values of the return are of the form $R_i = R_{\min} + i \frac{\Delta R}{n}$ for $i \in \{0, \ldots, n\}$.

With this natural assumption, the problem of finding $\beta$ such that,
\[\mathrm{EntRM}_\beta[R^\pi] = \mathrm{EntRM}_\beta[R^{\pi'}]\]
can be transformed into a problem of finding the roots of a polynomial.

\begin{align*}
    \mathrm{EntRM}_\beta\bigl[R^\pi\bigr] \;=\; \mathrm{EntRM}_\beta\bigl[R^{\pi'}\bigr]
    &\;\Longrightarrow\;
    \mathbb{E}\bigl[\exp(\beta R^\pi)\bigr] 
    \;=\;
    \mathbb{E}\bigl[\exp(\beta R^{\pi'})\bigr]
    \\
    &\;\Longleftrightarrow\;
    \sum_{i=0}^n \mu_i \,\exp\!\bigl(\beta R_i\bigr)
    \;=\;
    \sum_{i=0}^n \mu'_i \,\exp\!\bigl(\beta R_i'\bigr)
    \\
    &\;\Longleftrightarrow\;
    \sum_{i=0}^n a_i \,\exp\!\bigl(\beta R_i\bigr)
    \;=\;
    0
    \\
    &\;\Longleftrightarrow\;
    \sum_{i=0}^n a_i \,\exp\!\Bigl(\beta\,i\,\frac{\Delta R}{n}\Bigr)
    \;=\;
    0
    \\
    &\;\Longleftrightarrow\;
    \sum_{i=0}^n a_i \,X^{-i}
    \;=\;
    0
    \,.
    \\
    &\;\Longleftrightarrow\;
    \sum_{i=0}^n a_{n-i} \,X^{i}
    \;=\;
    0
    \,.
\end{align*}
Where $X = \exp\!\Bigl(-\beta\,\frac{\Delta R}{n}\Bigr)$. The first implication is not an equivalence because of the case $\beta = 0$, where the exponential form (rhs) is always equal to $1$ and thus the equality is always verified. The last implication is verified by multiplying by $X^n$ because of that same fact that $0$ is already a root of the equation.

Hence, if $X$ is a solution, $\beta$ verifies $\beta = -\frac{\log(X)}{\frac{\Delta R}{n}}$.

Cauchy's bound on the size of the largest polynomial root \citep{cauchy1828exercices} claims that the largest root verifies
\[1 + \max_{i > 0} \frac{|a_i|}{|a_0|}.\]

The lowest breakpoints corresponds to the largest breakpoint solution between the $\pi_{\inf}$ and any other policy. Hence, the lowest breakpoint verifies

\[\beta_{\inf} \geq \frac{-\log\left(1 + \max_{\pi \neq \pi_{\inf}} \max_i \frac{|a_i(\pi)|}{|a_0(\pi)|}\right)}{\frac{\Delta R}{n}}\ .\]

\end{proof}

This proof uses Cauchy's bound on the size of the largest polynomial root as it is simple to write and an efficient bound. Tighter bound have been developed in the litterature that could also be used here. See \citet{akritas2008improving} for example.

Using this polynomial formulation, it is also possible to derive a theoretical bound on the smallest distance between breakpoints. Mignotte's separation bound \citep{collins2001polynomial} gives a lower bound on the distance between two roots of the polynomial, as a function of the coefficient of such polynomial. This bound can then be transfered to a bound on the distance between breakpoints. This kind of bound could be useful to choose the necessary precision for the computation of the breakpoints, but are intractable to compute and the obtained values are too small to be relevant in practice.

\subsection{About \Cref{pro:recursive_breakpoint}.}
\label{app:recursive_breakpoint}

\begin{proposition}[Formal]
    Let $\breakpoints^h$ and $\Gamma^h$ respectively be the set of breakpoints and the optimality front when the MDP starts at timestep $h$. Let $\breakpoints\left((X_i)_i, I\right)$ the set of breakpoints for the MDP with a single state and reward distributions $(X_i)_i$ (not assumed deterministic).
    \[
    \breakpoints^h = \breakpoints^{h+1} \cup \left( \bigcup_{x\in\states, (\pi_k^h, I_k^h) \in \Gamma^h} \breakpoints\left( [R^{\pi_k^h}_h(x,a)]_a, I_k^h\right) \right)
    \]
\end{proposition}

\begin{proof}
    Consider $\beta_0$ a breakpoint. There exists $h$ such that $\beta_0 \in \breakpoints^h \setminus \breakpoints^{h+1}$. This $h$ corresponds to the last timestep where the optimal policy changes when the risk parameter crosses $\beta_0$. 
    
    In particular, let $(\pi^I, I) \in \Gamma^{h+1}$ such that $\beta_0 \in I$. By definition, 
    \[\forall t \geq h+1, \forall\beta \in I,\quad [\pi^*_{\beta}]_t(x) = [\pi^I]_t(x).\]
    
    Also, since $\beta_0$ is a breakpoint, there exists $x, a_1 \neq a_2, \pi_1 \neq \pi_2$ such that 
    \[\mathrm{EntRM}_{\beta_0}[R^{\pi^1}_h(x,a_1)] = \mathrm{EntRM}_{\beta_0}[R^{\pi^2}_h(x,a_2)],\] 
    
    By definition of $\pi^I$, it is also equal to
    \[\mathrm{EntRM}_{\beta_0}[R^{\pi^I}_h(x,a_1)] = \mathrm{EntRM}_{\beta_0}[R^{\pi^I}_h(x,a_2)],\]

    Hence, $\beta_0 \in \breakpoints\left( [R^{\pi^I}_h(x,a)]_a, I\right)$.

\end{proof}

% ------------------------------------------------------------
% ------------------------------------------------------------
% ------------------------------------------------------------

\section{Solving the exact breakpoints}
\label{app:exact_breakpoints}

Several issues rises when trying to computing the Optimality front with this method. The first one is that there is no easy way to compute the function $\beta \mapsto Q^\pi_{h,\beta}(x,\pi(x))$ without having access to the exact distribution of return for this policy, or to perform a Policy Evaluation step for each value of $\beta$, which quickly becomes computationally inefficient. 

A second issue is that this system of equation is only valid if there is a single breakpoint (i.e., no other optimal policy) between those two policies. Assume that we know $\pi_1$ is optimal for $\beta_1$ and $\pi_2$ is optimal for $\pi_2$, solving the equations gives value of the risk parameter for which one policy becomes better than the other. However, there could also be a third (or more) policy $\pi_3$ which is optimal for values of $\beta$ between $\beta_1$ and $\beta_2$. Computing the optimality front would require computing the breakpoints between $\pi_1$ and $\pi_3$, and between $\pi_3$ and $\pi_2$. As there are no simple conditions to verify the existence of another optimal policy between two known one, solving the exact breakpoint for a policy $\pi_1$ optimal for $\beta_1$ would require to solve the system of equation for all possible policy $\pi_2$, and retrieving the lowest breakpoint obtained among them all. Selecting this lowest breakpoint $\beta_2$ ensures that $\pi_1$ is only optimal for the a risk parameter up to $\beta_2$. Because of the exponential number of possible policies, such method is intractable in practice, and some approximation will be required.

Lastly, the equations themselves are non-trivial. Indeed, knowing the distribution of rewards, such an equation takes the form $\sum_{i=1}^{k} a_i \, e^{\alpha_i \beta} \;=\;0$, where the coefficient $a_i$ may be non-positive.
The function in question can be expressed as the difference of two convex functions, but it lacks many of the regularity properties needed for simple optimization methods. To the best of our knowledge, the best algorithms for solving such problems reduce it to finding roots of polynomial and using efficient solvers. Yet, the non-linear transformation required (see proof of \Cref{thm:beta_min}) makes the approximation error become significant when transposed back to our initial problem.



% ------------------------------------------------------------
% ------------------------------------------------------------
% ------------------------------------------------------------




\section{Analysis of the number of breakpoints}
\label{app:breakpoints_number}

\subsection{Theoretical bound on the number of breakpoints}

\begin{proposition}
    % Nombre théorique de breakpoints.
    \label{pro:max_breakpoints}
    Let $n$ the number of possible values of $R^{\pi}$. The number of breakpoints $B$ verifies
    \[ B \leq n\cdot |\actions|^{2|X|H}\]
\end{proposition}

\begin{proof}(of \Cref{pro:max_breakpoints})

    We first consider this lemma, on the number of roots of an exponential sum.
    \begin{lemma}[\citep{tossavainen2007lost}]
        \label{lem:exp_root}
        Let $f_n(x) = \sum^n_0 b_i k_i^x$, $b_i \in \RR, k_i > 0$. Then $f_n$ has at most $n-1$ roots.
    \end{lemma}

    This lemma allows for bounding the number of values of the risk parameter $\beta$ for which two different policies have the same entropic risk.
    \begin{proposition}
        \label{pro:nbr_crossing_deux}
            Consider two distributions $\mu_1 \neq \mu_2$ with support on $X = \{x_1, \dots, x_n\} \subset \RR$ of size $n$. Consider $X_1 \sim \mu_1, X_2 \sim \mu_2$ random variables following those distributions. Consider $\breakpoints = \{\beta \in \RR \ | \ \mathrm{EntRM}_\beta(X_1) = \mathrm{EntRM}_\beta(X_2)\}$. 
            
            Then:  $|\breakpoints| \leq n-1$
    \end{proposition}
    The proposition is straightforward by considering the exponential form of the EntRM, which is a sum of exponentials.

    Finally we conclude by considering that there are $|\actions|^{|X|H}$ deterministic markovian policies for a specific MDP, and thus $|\actions|^{2|X|H}$ pairs of policies. The breakpoints are values for which the entropic risk of two policies is equal, and thus are included in the union of the "breakpoints" of all pairs of policies. By \Cref{pro:nbr_crossing_deux}, the number of breakpoints is at most $(n-1)|\actions|^{2|X|H}$.
\end{proof}

This proposition is not in general. In the conclusion we consider the number of point of equality between any two policies, but those points cannot all be breakpoints. Only the points where one of the two policy is optimal count as breakpoints. This problem is reduced to finding the number of components of the function $h(\beta) = \max \{ f_\pi(\beta) \}_{\pi \in \Pi}$, where $f_\pi(\beta) = \mathrm{EntRM}_\beta[R^\pi]$. This combinatorial problem has been studied before (see Lemma 2.4 in \citet{atallah1985some}), but to the best of our knowledge, no better explicit formula has been found.

\subsection{Evolution with the number of actions and atoms in the distributions}
\label{app:breakpoints_number_state}


We conducted experiments in the single-state setting (the \emph{FindBreaks} setting) to determine whether the theoretical bound on the number of breakpoints is reached in practice. We generated numerous random reward distributions and used our algorithm to find the number of breakpoints. We performed two experiments: one evaluating how the number of breakpoints evolves with respect to the number of actions, and the other with respect to the number of atoms in the reward distributions.

All the distributions considered have values in $[0,1]$ with atoms evenly spaced over this interval. To generate these distributions, we treat each as an element of $[0,1]^n$, where the sum of the elements equals 1, representing a point on the $n$-dimensional simplex. The distributions are thus generated by uniformly sampling a point on this simplex.

For the first experiment, we fixed the number of atoms to 10 and varied the number of actions from 5 to 50. For the second experiment, we fixed the number of actions to 10 and varied the number of atoms from 5 to 50 as well. For each action, the reward distribution was generated randomly as previously described. In the solving algorithm, we searched for breakpoints for $\beta$ values in the range $[-15, 15]$ with a precision of 0.01. For each plot, we generated 100 independent problems and displayed a histogram of the number of breakpoints found across these 100 problems.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{hist_breakpoints_actions.pdf}
    \includegraphics[width=\textwidth]{hist_breakpoints_atoms.pdf}
    \vspace{-0.6cm}
    \caption{Illustration of the evolution of the number of breakpoints when the number of actions (Up) and atoms in the distributions (Down) is increasing. While the number of actions and atoms is multiplied by 10, the average number of breakpoints increases by less than a factor 2.}
    \label{fig:hist_breakpoints}
    %\vspace{-0.5cm}
\end{figure}

The results are shown in \Cref{fig:hist_breakpoints}. We observe that the number of breakpoints increases according to the studied parameters, with an average number of crossings of 1.25, 1.9, 2.19, and 2.36 for the first experiment, and 1.42, 1.66, 1.93, and 2.05 for the second. However, this increase is far from the theoretical bound established in \Cref{pro:max_breakpoints}. Indeed, the number of breakpoints is much lower than the theoretical bound, showing sublinear growth. This experiment confirms the efficiency of our algorithm, whose complexity is strongly tied to the number of breakpoints.


\section{FindBreaks Algorithm}
\label{app:single_state}

Here \emph{FindBreaks} is designed to be able to handle values of the risk parameter both positive and negative. In practice, using DOLFIN, all the values will be non-positive. The values of the increments $\Delta$ come from \Cref{thm:interval_action_change}.

\begin{algorithm}
    \caption{FindBreaks: Computing all optimal actions}\label{alg:state}
    \begin{algorithmic}[1]
    \REQUIRE Precision $\varepsilon \in (0,1)$, Random rewards $(R(a_i))_i$, interval $I$.
    \STATE Compute $a^* = \arg\max_a \mathrm{EntRM}_0(R(a))$ \COMMENT{Initial optimial action}
    \STATE Compute $A = \min_{a_2 \neq a^*} \mathrm{EntRM}_0(R(a^*)) - \mathrm{EntRM}_0(R(a_2))$, $\Delta R = r_{\max} - r_{\min}$ \COMMENT{Advantage function and range of rewards}
    \STATE Initialize $\beta = \frac{8\Delta\mu}{\Delta R^2}$, $\beta_{\text{old}} = 0$ \COMMENT{Initial parameter values}
    \STATE Initialize $b_\ell = 0$, $a_{\beta_{\text{old}}} = a^*$
    \WHILE{$\beta_{\min} < \beta < \beta_{\max}$}
        \STATE Compute the $\beta$ risks $\mathrm{EntRM}_\beta(R(a))$ for each action $a$ \COMMENT{Evaluate risks for all actions}
        \STATE $a_\beta = \arg \max_a \mathrm{EntRM}_\beta(R(a))$ \COMMENT{Select the action with the highest utility}
        \IF{$a_\beta \neq a_{\beta_{\text{old}}}$}
            \STATE Add $[b_\ell, \beta]$ to the interval set $\mathcal{I}$ and $a_\beta$ to the optimality front $\Pi^*$ \COMMENT{Update intervals and front}
            \STATE $b_\ell \gets \beta$ \COMMENT{Update the lower bound of the next interval}
        \ENDIF
        \STATE $a_{\beta_{\text{old}}} \gets a_\beta$ \COMMENT{Update the last optimal action}
        \STATE $\Delta U = \min_{a \neq a_\beta} \bigl(\mathrm{EntRM}_\beta(R(a_\beta)) - \mathrm{EntRM}_\beta(R(a))\bigr)$ \COMMENT{Smallest optimality gap for non-optimal actions}
        \IF{$\beta < 0$}
            \STATE $\beta \gets \beta - \max\{\beta \frac{\Delta U}{\Delta R}, \varepsilon\}$ \COMMENT{Decrease $\beta$ for negative values}
        \ENDIF
        \IF{$\beta > 0$}
            \STATE $\beta \gets \beta + \max\{\beta \frac{\Delta U}{\Delta R}, \varepsilon\}$ \COMMENT{Increase $\beta$ for positive values}
        \ENDIF
    \ENDWHILE
    \STATE Add $[b_\ell, \beta_{\max}]$ to $\mathcal{I}$ \COMMENT{Add the last interval to the set}
    \OUTPUT Optimality Front $\Gamma^*$
    \end{algorithmic}
\end{algorithm}
    
\paragraph{Empirical Evaluation and Performance Analysis}

A simple simulation illustrates the behavior or \Cref{alg:state}. We consider two actions, $a_1$ and $a_2$, with reward distributions $\varrho(a_1) = \frac{1}{2}\delta_0 + \frac{1}{2}\delta_1$ and $\varrho(a_2) = \frac{99}{100}\delta_0 + \frac{1}{100}\delta_2$. Action $a_1$ is better in expectation (lower risk) but action $a_2$ can achieve a higher reward with small probability and should only outperform action $a_1$ for large risk parameters.  
\Cref{fig:intervals_illustration} illustrates this: the functions $\mathrm{EntRM}(R(a))$ are plotted for\footnote{This experiment is design to test the transition to a risky action, so it is only relevant to observe the optimality front for $\beta>0$.} $\beta>0$. As soon as the risk parameter $\beta$ is large enough (specifically, $\beta > 3.9$), action $a_2$ becomes optimal.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{intervals_ex.pdf} 
    \vspace{-0.9cm}
    \caption{Utility functions on a single-state problem: a conservative (blue) and risky (red) actions, with respective ranges of optimality intersecting at $\beta_\text{bp}=3.9\pm 10^{-2}$. In red, we show the 800 values of $\beta$ tested with a naive grid; in green the 22 values tested by \Cref{alg:state} to identify the breakpoint. Right-hand figure zooms around $\beta_\text{bp}$.
    }
    \label{fig:intervals_illustration}
\end{figure}

\Cref{alg:state} was executed on this example for $\beta \in [0,8]$, with a precision of $\varepsilon = 10^{-2}$, but our theoretical upper bound\footnote{We show a larger upper bound for visualisation purposes. Our algorithm only evaluates 3 values past this limit so it does not hurt performance significantly.} is $\beta_{\max}=\ln(100)=4.6$. The green markers correspond to the values of $\beta$ where the algorithm computed the EntRM, while the red markers represent the values that would be computed using a plain grid search with precision $\varepsilon$. As expected, we observe that the intervals shrink 
near the breakpoint, but grow significantly larger as we move away from these regions. \Cref{fig:intervals_illustration} (Right) zooms in on the interval $\beta \in [3.6, 4.2]$ to better visualize the concentration of intervals. Around $\beta = 3.9$, we observe that a few intervals are indeed capped by the maximal precision.

In this simple example, the naive grid uses $800$ evaluations while \Cref{alg:state} only requires $22$, with an \emph{efficiency ratio} of $800/22=36$. To better quantify this gain, we run another experiment on random problems with 8 actions and reward function supported on 20 atoms in $[0,1]$ (hence with possibly much more that $1$ breakpoint). On \Cref{fig:performance_algo_state}, for each level of precision $\varepsilon \in [10^{-3},10^{-1}]$, we compare the number of evaluations required by \Cref{alg:state} with the naive $1/\varepsilon$ obtained with a plain grid search. We report the gain on average over 20 random problem and notice efficiency ratios up to 35 for high-precision $\varepsilon$ values. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{intervals_vs_naive.pdf}
    \caption{Performance gained using \Cref{alg:state}. (Left) Number of evaluations, (Right) efficiency ratio (red over blue).}
    \label{fig:performance_algo_state}
\end{figure}



The performance of this algorithm is also closely tied to the number of breakpoints. The more breakpoints there are, the smaller the intervals will tend to be, which in turn reduces the algorithm's efficiency compared to a plain grid search. Therefore, the number of breakpoints is a critical factor. \Cref{app:breakpoints_number} show that the number of breakpoints grows much slower than the theoretical bound. This allows our algorithm to be more efficient in practice than predicted by theory. 




% ------------------------------------------------------------
% ------------------------------------------------------------
% ------------------------------------------------------------




\section{More Numerical Experiments}
\label{app:more_experiments}

\subsection{Inventory Management}

We first consider some more values for the Inventory Management environments.

\begin{table}[htbp]
    \centering
    \caption{Evaluation of $P(R^\pi \leq T)$ for Inventory Management.}
    \label{tab:tau_results_bis}
    \begin{tabular}{lccccc}
    \toprule
    $T/\mu^*$    & 0.25  & 0.33  & 0.5  & 0.66   & 0.75      \\
    \midrule
    \textbf{Optimality Front} & $\mathbf{1.26e^{-5}}$  & $\mathbf{8.40e^{-5}}$ & $\mathbf{3.26e^{-3}}$ & $\mathbf{3.91e^{-2}}$   & $\mathbf{8.78e^{-2}}$   \\
    Proxy Optimization     & $2.33e^{-5}$  & $1.18e^{-4}$ & $3.28e^{-3}$ & $\mathbf{3.91e^{-2}}$   & $\mathbf{8.78e^{-2}}$   \\
    Risk neutral optimal & $1.11e^{-4} $  & $4.24e^{-4}$  & $5.73e^{-3}$   &$4.62e^{-2}$   & $9.77e^{-2}$        \\
    Nested Prob. Thresh. & $1.54e^{-3}$  & $8.37e^{-3}$  & $1$ & $1$  & $1$   \\
    \midrule
    Optimal value      & $6.29e^{-7}$  & $8.22e^{-6}$  & $7.85e^{-4}$  &  $1.65e^{-2}$   & $4.47e^{-2}$   \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Evaluation of $\mathrm{(C)VaR}_\alpha[R^\pi]$ for Inventory Management.}
    \label{tab:var_results_bis}
    \begin{tabular}{l|cccc|cccc}
    \toprule
    Risk Measure &  \multicolumn{4}{c|}{VaR} & \multicolumn{4}{|c}{CVaR} \\
    %\midrule
    Risk parameter $\alpha$   & 0.05  & 0.1 & 0.2 & 0.5 & 0.05 & 0.1 & 0.2 & 0.5\\
    \midrule
    \textbf{Optimality Front}  & $\mathbf{37}$  & $\mathbf{40}$ & $\mathbf{44}$ & $\mathbf{53}$ & $\mathbf{32.54}$  & $\mathbf{35.56}$ & $\mathbf{39.03}$ & $\mathbf{45.03}$ \\
    Proxy Optimization & $36$ & $\mathbf{40}$ & $\mathbf{44}$ & $\mathbf{53}$   & $32.28$ & $35.46$ & $39.00$ & $\mathbf{45.03}$ \\
    Risk neutral optimal & $36$ & $\mathbf{40}$ & $\mathbf{44}$ & $\mathbf{53}$ & $31.45$ & $34.68$ & $38.44$ & $44.74$ \\
    Nested Risk Measure & $35$ & $38$ & $42$ & $51$ & $30.00$ & $33.80$ & $37.98$ & $44.89$ \\
    \bottomrule
    \end{tabular}
\end{table}


The values in \Cref{tab:tau_results_bis} and \Cref{tab:var_results_bis} reveal that the improvements of our \emph{Optimality Front} method over the \emph{Proxy Optimization} becomes less significant as the level of risks decreases.

We also plot in \Cref{fig:performance_algo_mdp_ratio_inv} the efficiency ratio of using the Findbreaks, similar to \Cref{fig:performance_algo_state}. This figure highlights the polynomial gain in performances of using DOLFIN and Findbreaks when one wants to compute all the optimal policies up to a certain accuracy.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{cliff_intervals_vs_naive_ratio_inv.pdf}
    \caption{Performance gained using \Cref{alg:mdp} on Inventory Management.}
    \label{fig:performance_algo_mdp_ratio_inv}
\end{figure}

\subsection{Cliff Grid World}
\label{app:cliff}

Here we consider the Cliff grid world \citep{sutton2018reinforcement} illustrated in \Cref{fig:cliff_policies}.  The agent starts in the blue state. At each step, she has a small probability ($0.1$ here) of moving to another random direction. Due to these random transitions, it is risky to walk too close to the cliff (bottom, in red, negative reward $-\frac{1}{2}$), and conservative policies will prefer to walk further away to reach the goal (in green). The horizon is fixed at $H = 15$, so in principle, the agent has enough time to reach the end using the safe path. The reward when the goal is reached at step $h$ is $1 - \frac{h}{2H}$, which encourages the agent to reach it as fast as possible.

The agent thus faces a dilemma: she could either walk along the cliff, risking to fall but reaching the goal faster and consequently receiving a higher reward; or she could go all the way around, taking more time but with a lower probability to fall down. This can be observed in \Cref{fig:cliff_policies}, where the optimal policies for different values of $\beta$ are shown. For high values of beta (e.g., $\beta = 10$), the agent takes the risky path, while for low values of beta (e.g., $\beta = -5$), the agent takes the safe path.  One can even observe that for extremely negative values of $\beta$ values (e.g. $\beta =-10$) the agent prefers to stay away from the cliff, not even trying to reach the goal (see purple arrow in top right corner pointing up).

The Cliff environment is very different from Inventory Management in its nature. The agent only receives rewards when a certain state is reached, making the reward scarce and the return distribution simpler. This implies that the optimal policy for different measures of risks, such as Probability Threshold, VaR and CVaR are markovian for this MDP.

For the Threshold Probability problem, we only consider 2 values of the threshold, $-0.5$ corresponding to falling into the cliff, and $0$ corresponding to not reaching the goal. For the first threshold, the objective is to find the policy that is least likely to fall,  while for the second it is to find the policy with the most chances of reaching the goal. 

\Cref{tab:tau_results} confirms that our \emph{Optimality Front} method performs better than other methods for the Threshold probability. An important remark is that, here, the real optimal value is reached by the optimality front. Similar performances are observed for the CVaR in \Cref{tab:var_results_cliff}. For the VaR, the gain in performance is limited, which is explained by the scarcity of rewards in the environment (the small changes in the return distribution does not change the value of the VaR).

Compared to the Inventory Management, the gain performance for computing the optimality front is much better, with a ratio up to a factor $100$, as seen in \Cref{fig:performance_algo_mdp_cliff}.

\begin{table}[htbp]
    \centering
    \caption{Evaluation of $P(R^\pi \leq T)$ for Cliff.}
    \label{tab:tau_results_cliff}
    \begin{tabular}{lcc}
    \toprule
    $T$    & -0.5  & 0        \\
    \midrule
    \textbf{Optimality Front} & $\mathbf{3.72e^{-2}}$  & $\mathbf{4.65e^{-2}}$   \\
    Proxy Optimization     & $3.38e^{-2}$  & $4.67e^{-2}$  \\
    Risk neutral optimal & $4.50e^{-2}$  & $4.84e^{-2}$   \\
    Nested Prob. Thresh. & $1$  & $1$   \\
    \midrule
    Optimal value      & $\mathbf{3.72e^{-2}}$  & $\mathbf{4.65e^{-2}}$  \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Evaluation of $\mathrm{(C)VaR}_\alpha[R^\pi]$ for Cliff.}
    \label{tab:var_results_cliff}
    \begin{tabular}{l|cccc|cccc}
    \toprule
    Risk Measure &  \multicolumn{4}{c|}{VaR} & \multicolumn{4}{|c}{CVaR} \\
    %\midrule
    Risk parameter $\alpha$ & 0.05  & 0.1 & 0.2 & 0.5 & 0.05 & 0.1 & 0.2 & 0.5\\
    \midrule
    \textbf{Optimality Front}  & $\mathbf{0.53}$  & $\mathbf{0.63}$ & $\mathbf{0.70}$ & $\mathbf{0.76}$ & $\mathbf{-0.37}$ & $\mathbf{0.11}$ & $\mathbf{0.38}$ & $\mathbf{0.58}$ \\
    Proxy Optimization & ${0.00}$ & ${0.6}$ & $\mathbf{0.70}$  & $\mathbf{0.76}$ & $-0.39$ & $-0.08$ & $0.38$ & $0.58$ \\
    Risk neutral optimal & $\mathbf{0.53}$ & $\mathbf{0.63}$ & $\mathbf{0.70}$ & $\mathbf{0.76}$ & $-0.43$ & $0.08$ & $0.37$ & $0.58$ \\
    Nested Risk Measure  & $-0.5$ & $-0.5$ & $-0.5$  & $-0.5$ & $-0.5$ & $-0.5$ & $-0.5$ & $-0.5$\\
    \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{intervals_vs_naive_cliff.pdf}
    \caption{Performance gained using \Cref{alg:mdp} on Cliff. (Left) Number of evaluations, (Right) efficiency ratio (red over blue).}
    \label{fig:performance_algo_mdp_cliff}
\end{figure}

