

\begin{abstract}
    Risk-sensitive planning aims to identify policies maximizing some tail-focused metrics in Markov Decision Processes (MDPs). Such an optimization task can be very costly for the most widely used and interpretable metrics such as threshold probabilities or (Conditional) Values at Risk. Indeed, previous work showed that only Entropic Risk Measures (EntRM) can be efficiently optimized  through dynamic programming, leaving a hard-to-interpret parameter to choose. 
    
    We show that the computation of the \emph{full set of optimal policies} for EntRM across parameter values leads to tight approximations for the metrics of interest. We prove that this \emph{optimality front} can be computed effectively thanks to a novel structural analysis and smoothness properties of entropic risks. 
    Empirical results demonstrate that our approach achieves strong performance in a variety of decision-making scenarios.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Markov Decision Processes (MDPs) \citep{puterman2014markov}, the central models of modern Reinforcement Learning (RL), capture sequential decision making in domains as diverse as robotics, finance, healthcare, and operations research \citep{sutton2018reinforcement,silver2017mastering, charpentier2021reinforcement,polydoros2017survey}. At their core, MDPs allow agents to optimize actions so as to maximize the \emph{expected} cumulative reward via dynamic programming or other policy optimization methods. However, in many high-stakes applications, such as healthcare, average performance alone is not sufficient. In fact, it may be critical to limit the probability of catastrophic outcomes or to ensure that returns remain above certain thresholds with high confidence. This need has driven research on \emph{risk-sensitive} RL, which incorporates measures of uncertainty and tail behavior into the control objective \citep{bauerle_more_2014,tamar2015policy}.

One of the central challenges in risk-sensitive RL is to identify risk criteria that are both (1) \emph{meaningful} for real-world decision making and (2) \emph{tractable} in an MDP context. Popular approaches often revolve around the quantile-based Value at Risk (VaR) and Conditional Value at Risk (CVaR) \citep{artzner1999coherent,rockafellar2000optimization}, which are widely used in finance and operations research for bounding tail risk. 
Another common objective relies on controlling a Threshold Probability \citep{white1993minimizing}, that is the probability that total returns fall below a specified level.

Despite their practical interest and interpretability properties, these common risk metrics cannot be directly and efficiently optimized in MDPs \citep{marthe2024beyond, rowland2019statistics}.
Our work addresses precisely this gap by connecting the Threshold Probability and the (C)VaR metrics to the moment-generating function. In fact, in the context of MDPs, we show that these two optimization problems can be carefully approximated by the Entropic (Exponential) Risk Measure (EntRM) \citep{howard1972risk}.

The work of \citep{follmer2011stochastic,marthe2024beyond} shows that EntRM is unique among non-linear transformations of the return in admitting a dynamic programming decomposition, making it arguably the ``best possible'' extension of the risk-neutral Bellman recursion to a risk-aware MDP setting. Yet, despite its computational appeal, practical usage of EntRM can be hindered by interpretability concerns, especially around selecting  the risk tolerance parameter $\beta$.

We propose a unifying framework for risk-sensitive planning in MDPs through the study of the EntRM. We prove that optimal policies evolve in a structured manner as the risk parameter changes, which allows us to derive an efficient algorithm to compute all the optimal policies for EntRM, a set that we call \emph{the optimality front}. We demonstrate the consequence of this new method on the optimization of the Threshold Probability, the VaR and CVaR via dynamic programming. We provide both theoretical guarantees on the computational complexity and on the approximation error, and an empirical study on an inventory management problem.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Risk Sensitive Planning}
\label{sec:prelims}

\subsection{Risk-sensitivity in MDPs}

We consider a finite-horizon MDP $\mathcal{M} \;=\; \bigl(\states, \actions, p, r, H, p_1\bigr)$,
where $\states$ is a finite set of states, $\actions$ is a finite set of actions, $p(x'\mid x, a)$ is the probability of transitioning to state $x'$ from state $x$ when action $a$ is chosen, $r(x,a)$ is the reward function specifying the immediate reward for taking action $a$ in state $x$ (bounded in $[-1,1]$), $H$ is the finite horizon, i.e., the number of decision steps, and $p_1$ is the distribution of initial states $x_1 \sim p_1(\cdot)$.
To simplify notation, we assume all rewards and transitions are stationary, i.e. $p_t=p$ and $r_t=r$ $\forall t$, and the rewards are deterministic, but our results naturally extend to the non-stationary setting.

A \emph{policy} $\pi = (\pi_1, \ldots, \pi_H)$ is a sequence of decision rules $\pi_t : \states \rightarrow \actions$ that, at each step $t \in \{1,2,\dots,H\}$, selects an action based on the current state. For simplicity, we assume policies to be deterministic, but not necessarily stationary (a stationary policy verifies $\forall t, \pi_1 = \pi_t$). We denote $\tau = (x_1,a_1,\ldots,x_H,a_H)$ a trajectory induced by $\pi$. The corresponding \emph{cumulative reward} (or \emph{return}) is 

\[\displaystyle{ R^{\pi} \;=\; \sum_{t=1}^H r\bigl(x_t,a_t\bigr)}\]
where $x_0 \sim p_1(\cdot)$, $a_t = \pi_t(x_t)$ and $x_{t+1} \sim p(\cdot \mid x_t, a_t)$.
We also denote $R^{\pi}_h(x,a) \;=\; \sum_{t=h}^H r\bigl(x_t,a_t\bigr)$ with $x_h = x, a_h = a$, and $R^{\pi}_h(x) \;=\; R^{\pi}_h(x,\pi(x))$.

The classical objective in MDPs is to find a policy $\pi^*$ that maximizes the expected return, $\mathbb{E}_\pi[R^{\pi}]$, which is inadequate for risk-sensitive applications. In turn, risk-sensitive RL studies generic optimization problems
$\displaystyle{\max_{\pi} \rho (R^{\pi})}$
that extend the standard MDP framework by incorporating a functional $\rho$ that captures \emph{tail events} or \emph{uncertainty aversion}. 


\subsection{Risk metrics and computational limitations in MDPs}
\label{sec:expected_vs_risk}

The theory of risk measures is too rich for us to provide more than a few insights below on the most widely used examples. 
However, it has been shown \citep{kupper_representation_2009,follmer2011stochastic,rowland2019statistics,marthe2024beyond} that only the \emph{Entropic Risk Measure} satisfies a recursive Dynamic Programming equation akin to the Bellman Equation \citep{sutton2018reinforcement}, which is the cornerstone of RL algorithms and what makes the optimization problem tractable.

\paragraph{Entropic Risk Measure (EntRM).}
Originally introduced by \citet{howard1972risk} for MDPs, the EntRM for a random variable $X$ and parameter $\beta \in \mathbb{R}$ is defined as
\begin{align}
\label{eq:EntRM-def}
    \mathrm{EntRM}_\beta[X] =
    \begin{cases}
        \frac{1}{\beta}\,\log\!\bigl(\mathbb{E}[e^{\beta X}]\bigr), & \text{if } \beta \neq 0, \\
        \mathbb{E}[X], & \text{if } \beta = 0.
    \end{cases}
\end{align}
In the MDP context, maximizing $\mathrm{EntRM}_\beta[R^{\pi}]$ is also equivalent to maximizing $\mathrm{sign}(\beta)\,\mathbb{E}[\,e^{\beta R^{\pi}}]$, by composition with a monotonous function. We refer to this as the \emph{exponential form} of EntRM, and will use both forms throughout the paper, noting that the core optimization problem remains the same.

The parameter $\beta$ encodes risk tolerance: large positive $\beta$ values emphasize a risk-seeking attitude (amplifying high returns), while negative values induce conservativeness, diminishing the impact of large potential losses. 
Despite its interpretive appeal in terms of tail emphasis, $\beta$ can still be difficult to set in practice, and the measure is not \emph{positively homogeneous}, i.e. $\mathrm{EntRM}_\beta[cX] \neq c\,\mathrm{EntRM}_\beta[X]$ for $c \in \mathbb{R}$. In finance, this is problematic when scaling outcomes by different currencies or units. However, MDPs typically normalize rewards, mitigating some of these issues. Practitioners nonetheless often prefer quantile-based risk measures like VaR or CVaR. See \Cref{app:entrm} for more details on the EntRM. In the following, the risk parameter is always assumed to be non-positive.

\paragraph{Value at Risk (VaR).} 
VaR represents the quantile of the distribution. At level $\alpha \in (0,1)$:
\begin{equation}
\label{eq:VaR-def}
    \mathrm{VaR}_\alpha[R^{\pi}] 
    \;=\;
    \inf \Bigl\{\, x \;\big|\; \Pr\bigl(R^{\pi} \le x\bigr) \;\ge\; \alpha \Bigr\}.
\end{equation}

Although VaR is widely used, it suffers from a lack of \emph{sub-additivity}: it can be manipulated by adding or removing redundant risks. In the context of MDPs, VaR is particularly challenging to optimize due to its non-linearity and non-monotonicity \citep{chow2018risk}. 

\paragraph{Conditional Value at Risk (CVaR).}
To address some limitations of VaR, the \emph{Conditional Value at Risk} \citep{rockafellar2000optimization,follmer2011stochastic,bauerle2011markov,chow_algorithms_2014} at level $\alpha$ is given by
\begin{equation}
 \label{eq:CVaR-def}\mathrm{CVaR}_\alpha[R^{\pi}]
    \;=\;
    \frac{1}{\alpha} \int_0^\alpha \mathrm{VaR}_\gamma [R^{\pi}] d\gamma.  
\end{equation}
CVaR represents the expected return in the $\alpha$-worst fraction of outcomes, providing a more comprehensive view of the risk than VaR. What makes it particularly interesting is that it is a \emph{coherent risk measure}, meaning that it satisfies both sub-additivity and positive homogeneity, compared to the risk measure mentioned previously.

In this article, we chose the convention of VaR and CVaR representing the low tail of the distributions. In the literature, it is often defined using the high tail. This has no impact on the theory as a change of variable $X \leftarrow -X$ will transpose the results. More details about the different notations can be found in \Cref{app:var_family}.

\paragraph{Threshold Probability.}
Certainly the most intuitive measure of risk is the probability of falling below a user-specified threshold level $T$. Solving
$\displaystyle{
    \min_\pi \;\Pr \bigl(R^{\pi} \;\le\; T\bigr),
}$
means seeking a policy whose probability of yielding a return below $T$ is minimal. The VaR and Threshold optimization are \textbf{dual} problems. A farmer who worries about the possibility of a very poor harvest in the coming year will either ask, “What is the chance that my yield will be below 2 tons?”  (Threshold Probability viewpoint), or “With 90\% confidence, how large will my yield  be?” (VaR perspective).  Lowering the probability of dropping below a certain threshold (Threshold Probability) is directly tied to choosing a quantile-based cutoff for outcomes, as $\mathrm{VaR}_\alpha[X] = T$ just means that $P(X \leq T) = \alpha$.

\paragraph{Computational limitations.}
The EntRM verifies a Bellman Equation similar to the one of the expected return, making it computationally efficient (See \Cref{app:entrm} for details). Yet, optimization is much more complex for the other risk measures. 
Optimization of VaR and especially CVaR in MDPs have been studied extensively \citep{chow_algorithms_2014, chow2015risk,chow2018risk,achab2021robustness,bauerle2011markov}, but remains a challenging task. Optimal policies are usually non-Markovian, meaning they depend on the accumulated reward and not only the current state. This makes it harder to compute and implement in practice. Common Dynamic Programming schemes require augmenting the state space with a continuous variable, which is computationally expensive and makes the problem intractable except for very simple MDPs. 

Approximation schemes have been proposed, such as \emph{Dynamic Risk Measures} \citep{bauerle2022markov} (also called \emph{Nested} or \emph{Recursive Risk Measures}). Where the goal is to optimize recursively $V_h(x) = \max_a\rho[r(x,a) + V_{h+1}(X')]$, where $\rho$ is the risk measure, such as Var or CVaR. This problem has a natural Dynamic Programming formula that make computationnaly simple, but it does not optimize precisely the risk measure of choice. Also, the approximation is usually quite poor and the optimized objective is not law invariant, which makes it harder to interpret. Discretizing the augmented-state is another common approach, but the results are not always satisfying \citep{hau2024dynamic}. 
Optimizing the Threshold Probability raises the same challenges as VaR and CVaR \citep{white1993minimizing, wu1999minimizing, kira2012threshold} but seems to have been less studied and the literature lacks approximation schemes.

In short, while the EntRM is the only risk measure that can be efficiently optimized in MDPs, it is not what people use in practice. Practitioners prefer using more interpretable measures of risk, which are usually not tractable in MDPs. How can we leverage the computational properties of EntRM to optimize those more preferred measures of risk ?
In the sequel, we show that optimizing the EntRM over a range of risk parameters can lead to good approximation of optimal policies for several problems.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{A unified framework for Threshold Probability and VaR/CVaR optimization.}
\label{sec:methods}

The Threshold Probability and VaR/CVaR objectives are all related to the tail probabilities of the return distribution. These tail probabilities can be approximated with the help of exponential moments of the distribution by Chernoff's bound:
\begin{align}\label{eq:chernoff}
    \Pr\!\bigl(X \le T\bigr) 
    \;\;\le\;\;
    \inf_{\beta \leq 0} \;\exp\bigl(-\,\beta\,T\bigr)\,\mathbb{E}\!\bigl[e^{\,\beta\,X}\bigr].
\end{align}
Exponential moments are the core of Entropic Risk Measure and we explain in this section how Inequality~\eqref{eq:chernoff} leads to proxies for the risk metrics introduced above.

\paragraph{A proxy for the Threshold Probability.}
In the case of the Threshold Probability, the related proxy is pretty direct:
\begin{align}
    \min_\pi \;\Pr \bigl(R^{\pi} \;\le\; T\bigr) \leq \min_{\beta < 0}\min_\pi e^{-\,\beta\,T}\,\mathbb{E}\bigl[e^{\beta R^{\pi}} \bigr].
\end{align}
Some insights are given in \Cref{app:proxy}.

\paragraph{From (C)VaR to EVaR.}
Solving for the rhs of \eqref{eq:chernoff} to equal $\alpha$, \citet{ahmadi-javid_entropic_2012} introduced the \emph{Entropic Value at Risk (EVaR)} as a proxy for the VaR defined by
\[
\mathrm{EVaR}_\alpha[X] = \sup_{\beta < 0} \mathrm{EntRM}_\beta\bigl[X\bigr] - \frac{1}{\beta}\log(\alpha).
\]
It is the best approximation of the VaR based on exponential moments, and one proves that $\mathrm{VaR}_\alpha[X] \geq \mathrm{CVaR}_\alpha[X] \geq \mathrm{EVaR}_\alpha[X]$. Hence, the EVaR indeed appears as an even better approximation for the CVaR.
EVaR is a \emph{coherent} risk measure on its own, and its use for approximating VaR and CVaR has been of growing interest recently in MDPs \citep{ni_evar_2022,hau_entropic_2023,su2024evar}. The related Proxy for VaR and CVaR is 
\begin{align}
     \max_\pi \mathrm{VaR}_\alpha \left[R^{\pi}\right] \geq \sup_{\beta < 0}\max_\pi \mathrm{EntRM}_\beta\bigl[R^{\pi}\bigr] - \frac{\log(\alpha)}{\beta}.  
\end{align}

\textbf{Relaxed optimization problems. }
These proxies directly result in new optimization problems for the respective risk metrics. We note $\pi^*_\beta$ the optimal policy for the EntRM with parameter $\beta$. For the Threshold Probability, the optimal policy is $\pi^*_\beta$ where the risk parameter $\beta$ corresponds to 
\begin{align}
\label{eq:probaTreshld_proxy}
     \argmin_{\beta < 0} e^{-\,\beta\,T}\,\mathbb{E}\bigl[e^{\beta R^{\pi^*_\beta}} \bigr],
\end{align}
Similarly, for the Var/CVaR, we have
\begin{align}
    \label{eq:var_proxy}
    \arg \sup_{\beta < 0} \mathrm{EntRM}_\beta\bigl[R^{\pi^*_\beta}\bigr] - \frac{1}{\beta}\log(\alpha)
\end{align}
(which is effectively an
\emph{EVaR optimization problem}).

Those two problems are reduced to optimizing over the EntRM. There are some direct implications of this transformation in terms of the properties of the optimal policies.

\begin{proposition}[\citet{hau_entropic_2023}]
    \label{pro:proxy_policies}
    The optimal policies for the previous problems \eqref{eq:probaTreshld_proxy}-\eqref{eq:var_proxy} are deterministic and Markovian. For each problem, there exists $\beta <0$ such that the optimal policy is also optimal for the EntRM with parameter $\beta$.
\end{proposition}

Compared to the initial problems where optimal policies are usually not markovian, the transformation to the EntRM allows finding optimal policies that are easier to compute and implement.

Yet, in those new optimization problems, optimizing the EntRM is not done only for a specific parameter, but for a whole range of values. Current methods for optimizing the EntRM in MDPs are not adapted to this problem, and works for a single parameter at a time. A natural way of getting around this issue is by discretizing over the range of values of $\beta$.

\textbf{Quality of the approximations. }
The bounds above depend on the tail of the distributions which are known to be more accurate for distributions
with light tails \citep{vershynin2018high}.
In MDPs, it means that they are tighter for rich reward signals.

On the other hand, existing methods to optimize VaR, CVaR and Threshold Probability rely on dynamic programming on extended MDPs, where the state space is augmented with a continuous variable that correspond to the achievable values of the return \citep{chow_algorithms_2014, white1993minimizing}. Thus, even for small MDPs, rich reward signals may quickly increase the dimension to an extent that renders the optimization intractable. Conveniently, this is when our approximation method is relevant.

Moreover, while there is work on CVaR/VaR, this approximation scheme for the Threshold Probability problem remains unstudied so far in the literature.

\subsection{Grid-Based Optimization of the Risk Parameter}

A natural first idea to optimize the EVaR in MDPs is to discretize $\beta$ on a grid. By considering well-chosen values of $\beta$, \citet{hau_entropic_2023} show that they can get an $\varepsilon$-approximation of the optimal policy for the EVaR problem with a complexity of $O\bigl(|\mathcal{S}|^2\,|\mathcal{A}|\,H\frac{\log(1/\varepsilon)}{\varepsilon^2}\bigr)$.

We prove a similar result for the Chernoff approximation of the Threshold Probability problem.

\begin{proposition}
    \label{prop:approx_chernoff}
    Let $R$ be the return of the MDP such that there exists $a<0$ and $p>0$ with the property that for any policy $\pi$, $\Pr(R \leq a)\geq p$. Then solving \Cref{eq:probaTreshld_proxy}
    with accuracy $2\log(1+\varepsilon)/H$ on $\beta$ and $\beta_{\min} = \ln(p)/a$, finds a policy $\pi$ that satisfies
    \[
    \mathbb{P}\bigl(R^{\pi}\leq 0\bigr) \;\leq\; \tilde{B}
    \quad \text{and} \quad
    B \;\leq\; \tilde{B} \;\leq\; (1+\varepsilon)\,B,
    \]
    where $B$ is the true optimal value.
    
\end{proposition}

Hence, one needs to compute
\[
\frac{H\,\beta_{\min}}{2\,\log(1+\varepsilon)}
\]
policies to obtain a value within a factor $(1+\varepsilon)$ of the optimum. Given that computing an optimal policy for EntRM involves a complexity of $O\bigl(|\mathcal{S}|^2\,|\mathcal{A}|\,H\bigr)$, the overall complexity to achieve an approximation ratio of $(1+\varepsilon)$ is 
\[
O\!\Bigl(\frac{H^2\,|\mathcal{S}|^2\,|\mathcal{A}|\,\beta_{\min}}{\log(1+\varepsilon)}\Bigr).
\]

As expected, those bounds show that the complexity explodes as the grid is refined to obtain more accurate approximations. However, intuition suggests that computing very close values of risk parameters should lead to the same optimal policy via similar computations. Can there be an efficient method to optimize over a continuous range of risk parameters for the EntRM problem, avoiding redundant computation? Besides, the grid-based optimization computes several EntRM-optimal policies in order to optimize the proxy. Yet, the obtained result might not be the best EntRM-optimal policy for the initial problem (Threshold Probability or (C)VaR). The next section provides answers to these points by a careful study of the structure of optimal policies for the entropic risk over all values of the risk parameter.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Structural Insights into Entropic Risk Measures}
\label{sec:structure}

Optimizing EntRM for an entire range of risk parameters requires understanding the structure of EntRM optimal policies. We now show that exploiting the regularity of the $\mathrm{EntRM}_\beta$ function leads to an efficient algorithm that computes \emph{all the optimal policies} along $\beta \in \mathbb{R}$ much more efficiently than using a grid. We also show that this proves useful for surprisingly many risk-sensitive objectives.
In passing, understanding  how a small perturbation of the risk parameter can influence the optimal policy is also helpful from the point of view of interpretability  and robustness \citep{bauerle2024blackwell}. All the proofs are given in the appendix. 




\subsection{Structure Analysis}
\begin{definition}[Optimality Front]
    \label{def:optimality_front}
    For any MDP, the \emph{optimality front} is defined as $\Gamma = (\pi_k, I_k)_k$, where $(I_k)_k$ is a partition of the risk tolerance set and where $\pi_k$ is the optimal policy of EntRM for all risk tolerance parameters $\beta\in I_k$.
\end{definition}
This definition is justified by the following property, formalizing the intuition that a small perturbation of the risk parameter  typically does not change the optimal policy: the mapping $\beta \mapsto \pi^*_\beta$ is piecewise constant. 

\begin{proposition}
    \label{pro:finite_action_change}
    The Optimality Front $\Pi^*$ is a finite set of policies. Each policy in $\Pi^*$ is optimal on a finite union of closed intervals.
\end{proposition}
Crucially, we are able to prove lower bounds on the length of these intervals for a specific risk parameter $\beta$, knowing the Advantage function at this specific point.
\begin{theorem}
    \label{thm:interval_policy_change}
    Let $\beta \in \mathbb{R}$ be such that there is a unique deterministic policy $\pi^*_\beta$ optimizing $\mathrm{EntRM}_\beta$. Define the Generalized Advantage function:
    \[
       A^\pi_{h,\beta}(x,a) 
       \;=\; 
       \mathrm{EntRM}_{\beta}[R^{\pi}_h(x)] 
       \;-\; 
       \mathrm{EntRM}_{\beta}[R^{\pi}_h(x,a)].
    \]
    Then, define optimality gaps  as the smallest differences over the entire MDP:
    \begin{itemize}
        \item  $\displaystyle \Delta = \frac{|\beta|}{2}\,\min_{h,x} \min_{a \neq \pi^*_{\beta,h}(x)} \frac{1}{h}\, A^{\pi_\beta^*}_{h,\beta}(x,a)$ if $\beta \neq 0$,
        \item  $\displaystyle \Delta =  2\,\min_{h,x} \min_{a \neq \pi^*_{\beta,h}(x)} \frac{1}{h^2}\, A^{\pi_\beta^*}_{h,\beta}(x,a)$ if $\beta = 0$.
    \end{itemize}
    Then, for all $\beta' \in [\beta-\Delta, \beta + \Delta],$ the optimal policy for $\mathrm{EntRM}_{\beta'}$ remains $\pi^*_\beta$.
\end{theorem}
The first bound is usually quite good when the risk parameter is not too small, as it scales with $\beta$. For large values of $\beta$, it balances the small optimality gap (remember that $\mathrm{EntRM}_\beta{[R^{\pi}]} \rightarrow \mathrm{ess} \inf R^{\pi}$ when $\beta \rightarrow -\infty$, so the gaps tend to $0$). The degeneracy at $\beta=0$ is circumvented by the second bound.

Knowing this structure of intervals, the only information we need to determine the optimality front is the location of these subinterval boundaries. We call \emph{breakpoints} these risk-parameter values at which the optimal policy changes, and we show that the resulting change is generally only local.
\begin{proposition}
    \label{pro:policy_change}
    Consider a random MDP with reward functions and transitions $\left(r_t(x,a)\right)_{t,x,a}$ and $\left(p_t(x)\right)_{t,x}$ generated from, say, independent uniform distributions. With probability $1$, for each breakpoint $\beta \in \breakpoints$ there is a single state-horizon pair for which the optimal action changes: if $\pi^1$ is optimal for $\beta \in [\beta_1, \beta_2]$ and $\pi^2$ is optimal for $\beta \in [\beta_2, \beta_3]$, then there exists a unique state $x$ and time step $t$ such that $\pi^1_t(x) \neq \pi^2_t(x)$. 
\end{proposition}
See \Cref{app:policy_change} for more details.


\subsection{Application to Risk-Sensitive Objectives}

The structure of the optimality front allows to rewrite the proxy optimization problems as minimization over a finite set of optimal policies.

\begin{proposition}
    Consider $\Gamma = \left( \pi_k, I_k \right)_k$ the optimality front.
   \begin{align}
        \text{Eq.~(\ref{eq:probaTreshld_proxy})} &= \min_k \inf_{\beta \in I_k} \mathbb{E}\bigl[e^{\beta (R^{\pi_k} - T)} \bigr] \\
        \text{Eq.~(\ref{eq:var_proxy})} &= \min_k \inf_{\beta \in I_k} \mathrm{EntRM}_\beta\bigl[R^{\pi_k}\bigr] - \frac{1}{\beta}\log(\alpha)
   \end{align}
\end{proposition}

Both these problems are reduced to more simple optimization problems on small intervals that can be solved using gradient methods. Indeed, the first one, for the Threshold Probability, is concave \citep{boyd_convex_2004}, and the second one is quasiconcave\footnote{It becomes a concave problem after using the change of variable $\beta \leftarrow \frac{1}{\beta}$ and thus can still be optimized efficiently} \citep{hau_entropic_2023}.

While this method can be better if one wishes to truly optimize the EVaR, remember that the initial problem was to find "good" policies for the Threshold Probability, the VaR and the CVaR. Instead of finding the best EntRM optimal policy for the proxy, one can directly evaluate the metric of choice for each policy in the optimality front.

The previous optimization problem is thus reduced to the single and more general:
\begin{align}
    \label{eq:true_prolem}
    \max_k \rho \big( R^{\pi_k} \big)
\end{align}
where $\rho$ is the metric of choice and $(\pi_k)_k$ the policies of the optimality front. 




\subsection{Computing the Optimality Front}

The first step towards computing the Optimality Front is to \emph{find the breakpoints}. We first show that a direct approach is not feasible but instead we can exploit \Cref{thm:interval_policy_change}. Then, we present our algorithm, \emph{Distributional Optimality Front Iteration} (DOLFIN), based on efficient (distributional) value iteration \citep{bellemare2023distributional}. 

\paragraph{Finding the Breakpoints. }

Breakpoints mark the transition between two different intervals of optimality. According to \Cref{pro:policy_change} there must be at least two optimal policies for those particular parameter values.
This yields a system of equations characterizing the breakpoints.

\begin{proposition}
\label{pro:eq_breakpoint}
    Assume $\pi^1$ and $\pi^2$ are such that $\pi^1$ is optimal for $\beta \in [\beta_1, \beta_b]$ and $\pi^2$ is optimal for $\beta \in [\beta_b, \beta_2]$. Then $\beta_b$ satisfies:
    \[
        \forall h,x, \quad 
        \mathrm{EntRM}_{\beta_2}[R^{\pi^1}_h(x)] 
        \;=\;
        \mathrm{EntRM}_{\beta_2}[R^{\pi^2}_h(x)]
    \]
\end{proposition}
Note that since the policies only differ in one state-horizon pair (Prop.~\ref{pro:policy_change}), most of these equations are trivial. Nevertheless, one could attempt to use them to directly compute the breakpoints by resolving a system of equations. Unfortunately, we argue in \Cref{app:exact_breakpoints} that this is unfeasible due to the lack of regularity of the EntRM functions and to the computational complexity of the problem. 
In general, we show that the lack of “regularity” in these functions makes it impossible in practice to know in advance how many breakpoints, or optimal policies, might appear between two given points.

However, the advantage of using the EntRM in MDPs is that the optimal policies can be computed recursively by Dynamic Programming and we show that this implies a structure on the breakpoints. 

\begin{proposition}
    \label{pro:recursive_breakpoint}
    (Informal) Let $\breakpoints^h$
    be the set of breakpoints at time $h$ and 
    $\breakpoints^h(x)$ 
    the corresponding set of breakpoints at state $x$. 
    Then, the sets verify the recursive equation
    \[
    \breakpoints^h = \breakpoints^{h+1} \cup \left( \bigcup_{x\in\states} \breakpoints^h(x) \right)
    \]
\end{proposition}
A formal statement and the proof can be found in \Cref{app:recursive_breakpoint}.
This result shows that the set of breakpoints is simply a union over the per-state breakpoints, and they can be computed via a backward recursion. 

We now have all the tools to build an incremental approach that we call \emph{FindBreaks}. 
\Cref{thm:interval_policy_change} tells us that knowing the Entropic risk at a given point allows us to identify an interval of $\beta$ values over which the optimal policy does not change.
This fact can be utilized to `jump' over $\beta$ values.
The process is the following. At a given state, assume the distribution of the return for each action is known $(\eta(x,a))_{a\in\actions}$. Start with $\beta=0$ and iterate the following steps:
\begin{enumerate}
    \item Evaluate the Generalized Advantage function and the optimality gaps (see \Cref{thm:interval_policy_change}),
    \item Use the optimality gaps to get a lower bound $\beta-\Delta$ on the next breakpoint, and `jump': $\beta\gets \beta-\Delta$
\end{enumerate}

At some point, when getting close to a breakpoint, the increments $\Delta$ will get closer to 0. Then use a minimal increment $\epsilon$ until the optimal action changes. This can be done in parallel over states thanks to \Cref{pro:recursive_breakpoint}. Details and pseudo-code are in \Cref{app:single_state}.

This may not be the optimal way to compute the breakpoints but it exploits all the structure of the problem: both the regularity of the exponential functions and the recursive properties of the MDP optimization allow to speed up the process. The general question of characterizing optimality for this problem is a challenging open problem we leave for future work. Our final risk-sensitive optimization algorithm below is fully modular and could integrate any other breakpoint-search algorithm. 

\paragraph{Distributional Optimality Front Iteration. }
Combining both insights from Dynamic Programming and the approximation of Optimality Intervals, we derive an algorithm to compute the Optimality Front up to a desired accuracy.

This algorithm keeps in memory the distribution of the return recursively. While not compulsory, it accelerates the computation of several values of the EntRM with same reward distribution. For more details on the Dynamic Programming computation of return distributions, see \citet{bellemare2023distributional}.

\begin{algorithm}
\caption{DOLFIN - \textbf{D}istributional \textbf{O}ptima\textbf{l}ity \textbf{F}ront \textbf{I}teratio\textbf{n}}
\label{alg:mdp}
\begin{algorithmic}[1]
\REQUIRE Precision $\varepsilon \in (0,1)$; MDP $\mdp(\states, \actions, \transitions, \rewards, \horizon)$ parameters.
\STATE Select lower bound $\beta_{\min}$ \COMMENT{Computed or handpicked}
\STATE $\mathcal{I}_H \gets [\beta_{\min}, 0]$ \COMMENT{Starting interval}
\STATE $\nu_H(x) \gets \delta_0$ \COMMENT{\small Optimal return distribution at timestep $H$}

\FOR{$h = H$ \textbf{to} $1$}
    \FOR{$x \in \states$}
        \FOR{$I \in \mathcal{I}_h$}
            \STATE $\eta^I_h(x,a) \gets \varrho(x,a) * \sum_{x'} \transitions(x'\mid x,a)\,\nu^I_{h+1}(x')$
            \COMMENT{\small Return distributions}
            
            \STATE $\{\mathcal{J}, (a^*_j)_{j\in \mathcal{J}}\} \gets \textsc{FindBreaks}\bigl(\epsilon,\;(\eta^I_h(x,a))_{a}, I\bigr)$
            \COMMENT{\small Apply \Cref{alg:state} on $(\eta^I_h(x,a))_{a}$ as the reward distribution for each action}
            
            \FOR{$j \in \mathcal{J}$}
                \STATE Add $j$ to $\mathcal{I}_{h-1}$ \COMMENT{\small Update intervals for next timestep}
                \STATE $\nu^{j}_{h}(x) \gets \eta^I_h\bigl(x,a^*_j\bigr)$ 
                \COMMENT{\small Store optimal return distribution}
            \ENDFOR
        \ENDFOR
    \ENDFOR
\ENDFOR

\OUTPUT $\Gamma = \left( \pi_k, I_k \right)_k, (\eta_0^{k})^k$ \COMMENT{\small Optimality Front, distributions}

\end{algorithmic}
\end{algorithm}

DOLFIN returns the Optimality Front $\Gamma$ and it remains to solve \eqref{eq:true_prolem}: $\min_k \rho(\pi^k)$. In the experimental section below, we simply call this the \emph{Optimality Front} method. 


\paragraph{About the computational cost.}
Calling $B$ the number of breakpoints in the optimality front of the MDP, the number of calls to FindBreaks is bounded by $\Theta(|\states|HB)$ and thus heavily depend on the number of optimal policies. For a low number, only a few calls will be made and only a few Q-value evaluations will have to be computed. Using the empirical observations on the number of breakpoints, the number of calls to can be estimated to be in the order of $(|\states|H)^2$. The total complexity ultimately depends on the complexity of FindBreaks. An empirical study of our implementation can be found in \Cref{app:single_state}. In practice we observe that FindBreaks runs in $O(|A|f(1/\varepsilon)$ time, with some sublinear $f$.
When the support of the accumulated reward is too large, the implementation of the  distributional induction can benefit from the approximation schemes described in \cite{bellemare2023distributional}. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Numerical Experiments}
\label{sec:experiments}

This section evaluates the effectiveness and efficiency of the Optimality Front approach. As this is the first study to explore policies for Threshold Probability optimization in MDPs, we aim to assess how well these policies perform, compared to naive approximation methods. We then compare the performance of the Optimality Front on the VaR and CVaR metrics against the method introduced by \citet{hau_entropic_2023}. Lastly, we analyze the efficiency of our algorithm, focusing on the proposed approximation method. We quantify the improvements due to \emph{FindBreaks} compared to a naive grid-based approach, highlighting its practical advantages in reducing redundant computations.

\paragraph{Environment}

We implemented \emph{Optimality Front} on the Inventory Management MDP \citep{bellman1955optimal,scarf1960optimality}. In this example, the goal is to maximize the profit of a store selling one extensive good. The store has a strict maximal capacity of $M=10$. At each time step, the state of the store is its number of available goods, $x_t \in [M]$, and it can buy (action) a quantity $a_t\in [M]$ of new goods. The reward obtained is $r_t = [f(D_t,x_t,a_t) - C_m(x_t) - C_c(a_t)]/4M$ with $D_t$ is the random demand modeled by a binomial $D_t \sim B(0.5,M)$ and $f(D_t,x_t,a_t) =4 \min(D_t,x_t+a_t)$ is the sales profit, $C_m(x_t)=1x_t$ is the maintenance cost, and $C_c(a_t)=3+2a_t$ is the order cost. We considered a horizon $H=10$ with $s_0=0$.
Optimal policies in the Inventory Management MDP can be expressed in the form $(s_t, S_t)$: at time $t$, if the stock is less than $s_t$, then agent should buy so that they have a stock of exactly $S_t$. More precisely: if $x_t \leq s_t$, then $a^*_t = S_t - x_t$, else $a^*_t = 0$.

Another environment, Cliff, can be found in \Cref{app:more_experiments}. 

\paragraph{Optimality Front} 
For our method, \emph{Optimality Front}, we executed DOLFIN only once, with accuracy $\varepsilon = 10^{-3}$ on the chosen environment. We then computed all the metrics following \Cref{eq:true_prolem} by applying the functional of choice on all returned optimal distributions and selecting the optimal value. The Optimality Front contained $20$ different optimal policies.

\subsection{Threshold Probability}

We compare the \emph{Optimality Front} against several methods to compute a policy: \emph{Proxy Optimization} is the value obtained by optimizing \Cref{eq:probaTreshld_proxy}. The \emph{Risk neutral optimal} policy (i.e. the policy optimizing $\mathbb E[R^\pi]$) serves as a baseline. \emph{Nested Probability Threshold} corresponds to the metric of the policy obtained following the Nested Risk Measure method \citep{bauerle2022markov}. We also add, for reference, the optimal value obtained using augmented MDPs dynamic programming, resulting in non-markovian policies.

The value of the thresholds $T$ are selected as ratios of the optimal mean $\mu^*$.
\begin{table}[htbp]
    \centering
    \caption{Evaluation of $P(R^\pi \leq T)$.}
    \label{tab:tau_results}
    \begin{tabular}{lccc}
    \toprule
    $T/\mu^*$    & 0.25  & 0.33  & 0.5  \\%  & 0.66   & 0.75      \\
    \midrule
    \textbf{Optimality Front} & $\mathbf{1.26e^{-5}}$  & $\mathbf{8.40e^{-5}}$ & $\mathbf{3.26e^{-3}}$ \\% & $3.91e^{-2}$   & $8.78e^{-2}$   \\
    Proxy Optimization     & $2.33e^{-5}$  & $1.18e^{-4}$ & $3.28e^{-3}$   \\%& $3.91e^{-2}$   & $8.78e^{-2}$   \\
    Risk neutral optimal & $1.11e^{-4} $  & $4.24e^{-4}$  & $5.73e^{-3}$  \\% &$4.62e^{-2}$   & $9.77e^{-2}$        \\
    Nested Prob. Thresh. & $1.54e^{-3}$  & $8.37e^{-3}$  & $1$  \\%& $1$  & $1$   \\
    \midrule
    Optimal value      & $6.29e^{-7}$  & $8.22e^{-6}$  & $7.85e^{-4}$  \\%&  $1.65e^{-2}$   & $4.47e^{2}$   \\
    \bottomrule
    \end{tabular}
\end{table}
We observe in \Cref{tab:tau_results} that using EntRM optimal policies outperforms the risk-neutral optimal policy by up to a factor $10$ for a threshold ratio of $0.25$. On the opposite, the Nested Probability Threshold fails to find a good policy and performs worse than the risk-neutral one. The experiment also highlights the benefit of choosing the best policy from the Optimality Front instead of simply optimizing on the proxy. Not only is the optimization problem simpler, but the performances are also better. 

The true optimal value here is significantly better than what any markovian policy can achieve, especially for low thresholds. This gap is intensified by the nature of the MDP: in Inventory Management, the agent receives some reward with high variance at every timestep. For a specific state and timestep, the accumulated reward (and so, the distance to the threshold) can vary a lot, and so does the action. In some more goal-oriented MDPs, with very scarce reward, this gap can become insignificant (ex. see Cliff in \Cref{app:more_experiments}).

\subsection{Value at Risk family}

\citet{hau_entropic_2023} already showed that optimizing over the EVaR performed better than previous methods when it comes to optimizing both VaR and CVaR, including using augmented MDPs. We compare our method to theirs (here, \emph{Proxy Optimization}) and two other baselines for better illustration: the \emph{risk-neutral optimal} policy and the \emph{Nested Risk Measure} method. We compute both the VaR and CVaR with two different risk levels : 0.05 and 0.1.

\begin{table}[htbp]
    \centering
    \caption{Evaluation of $\mathrm{(C)VaR}_\alpha[R^\pi]$.}
    \label{tab:var_results}
    \begin{tabular}{l|cc|cc}
    \toprule
    Risk Measure &  \multicolumn{2}{c|}{VaR} & \multicolumn{2}{|c}{CVaR} \\
    %\midrule
    Risk parameter $\alpha$    & 0.05  & 0.1  & 0.05 & 0.1 \\
    \midrule
    \textbf{Optimality Front}  & $\mathbf{37}$  & $\mathbf{40}$ & $\mathbf{32.54}$  & $\mathbf{35.56}$  \\
    Proxy Optimization & $36$ & $\mathbf{40}$  & $32.28$ & $35.46$  \\
    Risk neutral optimal & $36$ & $\mathbf{40}$ & $31.45$ & $34.68$ \\
    Nested Risk Measure & $35$ & $38$ & $30.00$ & $33.80$ \\
    \bottomrule
    \end{tabular}
\end{table}

We observe in \Cref{tab:var_results} that for VaR and CVaR also, our algorithm outperforms pre-existing methods. 

\subsection{Optimality Front}
We first give a simple illustration of the optimality front on the Cliff environment. The agent starts in the blue cell and is encouraged to reach the green cell as fast as possible, while absolutely avoiding the cliff depicted in red. She may choose her moves, but with probability $0.3$ each choice is replaced by a random direction. Her risk tolerance is easily interpreted as her acceptance to walk close to the edge. Figure~\ref{fig:cliff_policies} shows the variety of optimal policies as the risk parameter $\beta$ varies from $-10$ (very risk averse) to $+10$ (risk-prone). Non-violet arrows show the action that is preferred as soon as $\beta$ is larger than a breakpoint whose value is given by the arrow's color.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{cliff_policies.pdf}
    \caption{Illustration of all cliffs $\beta$-optimal policies. Arrows are of the color of the smallest $\beta$ value corresponding to the optimal action $[\pi^*_{\beta}]$.}
    \label{fig:cliff_policies}
\end{figure}

We now illustrate the computational efficiency of our algorithms. DOLFIN computes intervals on which the current policy stays optimal, in order to avoid computing unnecessary values. But as the values get close to the breakpoint, using a regular grid becomes necessary as the computed values become irrelevant. We show however in \Cref{fig:dolfin_accuracy_complexity} that the number of evaluations grows sublinearly with the increasing accuracy, highlighting the gain of using such method.

\begin{figure}[htb]
    \centering
    \includegraphics{cliff_intervals_vs_naive_comp.pdf}
    \caption{Number of evaluation of EntRM required to obtain an accuracy of $\varepsilon$ on the breakpoints using our method (blue) com a naive grid (red). The efficiency ratio goes up to 15 in this example.}
    \label{fig:dolfin_accuracy_complexity}
\end{figure}

In practice, this algorithm already performs better than the grid-based methods when comparing the number of evaluations. Yet, this algorithm can also be used as a one-shot preprocessing step to consider several risk-sensitive problems (such as different quantile levels), each of which can then be solved efficiently. For instance, DOLFIN was only executed once for the evaluation of all previous metrics, while all other methods have to be executed separately for each value.

\section{Conclusion}
\label{sec:conclusion}

We propose a unified framework for optimizing risk-sensitive objectives in Markov Decision Processes. Leveraging the computational advantages of the Entropic Risk Measure (EntRM), we provide an efficient algorithm for computing the optimality front, a family of policies which are optimal on a range of risk tolerance values. This also allows us to approximate key metrics such as Threshold Probabilities, Values at Risk and Conditional Values at Risk. Our algorithm demonstrates significant practical benefits in both efficiency and policy quality. This approach not only enhances risk-sensitive planning but also provides a versatile tool for tackling a variety of decision-making problems under uncertainty. It remains to be extended beyond planning in the context of learning unknown transition probabilities and reward distributions.

