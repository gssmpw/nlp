@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{chang2024unraveling,
  title={Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures},
  author={Chang, Fu-Chieh and Wu, Pei-Yuan},
  journal={arXiv preprint arXiv:2411.16260},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{hazra2024safety,
  title={Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations},
  author={Hazra, Rima and Layek, Sayan and Banerjee, Somnath and Poria, Soujanya},
  journal={arXiv preprint arXiv:2406.11801},
  year={2024}
}

@article{imani2024exploring,
  title={Exploring Group and Symmetry Principles in Large Language Models},
  author={Imani, Shima and Palangi, Hamid},
  journal={arXiv preprint arXiv:2402.06120},
  year={2024}
}

@article{li2024safety,
  title={Safety Layers in Aligned Large Language Models: The Key to LLM Security},
  author={Li, Shen and Yao, Liuyi and Zhang, Lan and Li, Yaliang},
  journal={arXiv preprint arXiv:2408.17003},
  year={2024}
}

@article{ouyang_training_2022,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@inproceedings{yi2024vulnerability,
  title={On the vulnerability of safety alignment in open-access llms},
  author={Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={9236--9260},
  year={2024}
}

@article{zekri2024large,
  title={Large language models as markov chains},
  author={Zekri, Oussama and Odonnat, Ambroise and Benechehab, Abdelhakim and Bleistein, Linus and Boull{\'e}, Nicolas and Redko, Ievgen},
  journal={arXiv preprint arXiv:2410.02724},
  year={2024}
}

@article{zhou2024emulated,
  title={Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!},
  author={Zhou, Zhanhui and Liu, Jie and Dong, Zhichen and Liu, Jiaheng and Yang, Chao and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.12343},
  year={2024}
}

