
@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\kaoke\\Zotero\\storage\\REZ2S9YR\\Touvron …等 - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kaoke\\Zotero\\storage\\2BI688B6\\2307.html:text/html},
}

@misc{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2304.08485},
	doi = {10.48550/arXiv.2304.08485},
	abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = dec,
	year = {2023},
	note = {arXiv:2304.08485 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\kaoke\\Zotero\\storage\\YXEPQERX\\Liu …等 - 2023 - Visual Instruction Tuning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kaoke\\Zotero\\storage\\3J4EITTP\\2304.html:text/html},
}

@misc{dai_instructblip_2023,
	title = {{InstructBLIP}: {Towards} {General}-purpose {Vision}-{Language} {Models} with {Instruction} {Tuning}},
	shorttitle = {{InstructBLIP}},
	url = {http://arxiv.org/abs/2305.06500},
	doi = {10.48550/arXiv.2305.06500},
	abstract = {Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7\% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
	month = jun,
	year = {2023},
	note = {arXiv:2305.06500 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\kaoke\\Zotero\\storage\\TJPR9RHM\\Dai …等 - 2023 - InstructBLIP Towards General-purpose Vision-Language Models with Instruction Tuning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kaoke\\Zotero\\storage\\UBS8JHC4\\2305.html:text/html},
}

@misc{zhu_minigpt-4_2023,
	title = {{MiniGPT}-4: {Enhancing} {Vision}-{Language} {Understanding} with {Advanced} {Large} {Language} {Models}},
	shorttitle = {{MiniGPT}-4},
	url = {http://arxiv.org/abs/2304.10592},
	doi = {10.48550/arXiv.2304.10592},
	abstract = {The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
	month = oct,
	year = {2023},
	note = {arXiv:2304.10592 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\kaoke\\Zotero\\storage\\A66YV6L3\\Zhu …等 - 2023 - MiniGPT-4 Enhancing Vision-Language Understanding with Advanced Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kaoke\\Zotero\\storage\\5N6AR5QV\\2304.html:text/html},
}

@article{chung_scaling_nodate,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PaLM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks (at time of release), such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	language = {en},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tai, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Chi, Ed H and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V and Wei, Jason and Salakhutdinov, Ruslan},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\VEYRPWE7\\Chung …等 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf},
}

@article{ouyang_training_2022,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

% The following are jailbreak papers

@misc{gong_figstep_2023,
	title = {{FigStep}: {Jailbreaking} {Large} {Vision}-language {Models} via {Typographic} {Visual} {Prompts}},
	shorttitle = {{FigStep}},
	url = {http://arxiv.org/abs/2311.05608},
	doi = {10.48550/arXiv.2311.05608},
	abstract = {Ensuring the safety of artificial intelligence-generated content (AIGC) is a longstanding topic in the artificial intelligence (AI) community, and the safety concerns associated with Large Language Models (LLMs) have been widely investigated. Recently, large vision-language models (VLMs) represent an unprecedented revolution, as they are built upon LLMs but can incorporate additional modalities (e.g., images). However, the safety of VLMs lacks systematic evaluation, and there may be an overconfidence in the safety guarantees provided by their underlying LLMs. In this paper, to demonstrate that introducing additional modality modules leads to unforeseen AI safety issues, we propose FigStep, a straightforward yet effective jailbreaking algorithm against VLMs. Instead of feeding textual harmful instructions directly, FigStep converts the harmful content into images through typography to bypass the safety alignment within the textual module of the VLMs, inducing VLMs to output unsafe responses that violate common AI safety policies. In our evaluation, we manually review 46,500 model responses generated by 3 families of the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a total of 6 VLMs). The experimental results show that FigStep can achieve an average attack success rate of 82.50\% on 500 harmful queries in 10 topics. Moreover, we demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which already leverages an OCR detector to filter harmful queries. Above all, our work reveals that VLMs are vulnerable to jailbreaking attacks, which highlights the necessity of novel safety alignments between visual and textual modalities.},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun},
	month = dec,
	year = {2023},
	note = {arXiv:2311.05608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\kaoke\\Zotero\\storage\\UVZFZQW4\\Gong …等 - 2023 - FigStep Jailbreaking Large Vision-language Models via Typographic Visual Prompts.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kaoke\\Zotero\\storage\\3TRR6PSM\\2311.html:text/html},
}

@misc{luo_jailbreakv-28k_2024,
	title = {{JailBreakV}-{28K}: {A} {Benchmark} for {Assessing} the {Robustness} of {MultiModal} {Large} {Language} {Models} against {Jailbreak} {Attacks}},
	shorttitle = {{JailBreakV}-{28K}},
	url = {http://arxiv.org/abs/2404.03027},
	abstract = {With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K 1 , a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 opensource MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.},
	language = {en},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei},
	month = jul,
	year = {2024},
	note = {arXiv:2404.03027 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\BAG2LA4K\\Luo …等 - 2024 - JailBreakV-28K A Benchmark for Assessing the Robustness of MultiModal Large Language Models against.pdf:application/pdf},
}

@misc{hu_gradient_2024,
	title = {Gradient {Cuff}: {Detecting} {Jailbreak} {Attacks} on {Large} {Language} {Models} by {Exploring} {Refusal} {Loss} {Landscapes}},
	shorttitle = {Gradient {Cuff}},
	url = {http://arxiv.org/abs/2403.00867},
	abstract = {Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM’s rejection capability for malicious jailbreak queries, while maintaining the model’s performance for benign user queries by adjusting the detection threshold.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00867 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning}
}

@article{huang_catastrophic_2024,
	title = {Catastrophic jailbreak of open-source llms via exploiting generation.},
	language = {en},
	author = {Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
	year = {2024},
    journal = {ICLR}
}

@article{shayegani_jailbreak_2024,
	title = {Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models},
	author = {Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
	year = {2024},
    journal = {ICLR}
}

@misc{noauthor_defending_nodate,
	title = {Defending {ChatGPT} against jailbreak attack via self-reminders {\textbar} {Nature} {Machine} {Intelligence}},
	url = {https://www.nature.com/articles/s42256-023-00765-8},
	urldate = {2024-09-03},
	file = {Defending ChatGPT against jailbreak attack via self-reminders | Nature Machine Intelligence:C\:\\Users\\kaoke\\Zotero\\storage\\4F62UEQT\\s42256-023-00765-8.html:text/html},
}

@article{xie_defending_2023,
	title = {Defending {ChatGPT} against jailbreak attack via self-reminders},
	volume = {5},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00765-8},
	doi = {10.1038/s42256-023-00765-8},
	abstract = {ChatGPT is a societally impactful artificial intelligence tool with millions of users and integration into products such as Bing. However, the emergence of jailbreak attacks notably threatens its responsible and secure use. Jailbreak attacks use adversarial prompts to bypass ChatGPT’s ethics safeguards and engender harmful responses. This paper investigates the severe yet under-explored problems created by jailbreaks as well as potential defensive techniques. We introduce a jailbreak dataset with various types of jailbreak prompts and malicious instructions. We draw inspiration from the psychological concept of self-reminders and further propose a simple yet effective defence technique called system-mode self-reminder. This technique encapsulates the user’s query in a system prompt that reminds ChatGPT to respond responsibly. Experimental results demonstrate that self-reminders significantly reduce the success rate of jailbreak attacks against ChatGPT from 67.21\% to 19.34\%. Our work systematically documents the threats posed by jailbreak attacks, introduces and analyses a dataset for evaluating defensive interventions and proposes the psychologically inspired self-reminder technique that can efficiently and effectively mitigate against jailbreaks without further training.},
	language = {en},
	number = {12},
	urldate = {2024-09-03},
	journal = {Nature Machine Intelligence},
	author = {Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao},
	month = dec,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Interdisciplinary studies, Science, technology and society},
	pages = {1486--1496},
}

@misc{yu_gptfuzzer_2024,
	title = {{GPTFUZZER}: {Red} {Teaming} {Large} {Language} {Models} with {Auto}-{Generated} {Jailbreak} {Prompts}},
	shorttitle = {{GPTFUZZER}},
	url = {http://arxiv.org/abs/2309.10253},
	abstract = {Content warning: This paper contains unfiltered content generated by LLMs that may be offensive to readers. Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial “jailbreak” attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFUZZER, a novel blackbox jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFUZZER automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFUZZER starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFUZZER: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
	month = jun,
	year = {2024},
	note = {arXiv:2309.10253 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\SL6CMJIY\\Yu …等 - 2024 - GPTFUZZER Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts.pdf:application/pdf},
}

@misc{robey_smoothllm_2024,
	title = {{SmoothLLM}: {Defending} {Large} {Language} {Models} {Against} {Jailbreaking} {Attacks}},
	shorttitle = {{SmoothLLM}},
	url = {http://arxiv.org/abs/2310.03684},
	abstract = {Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SMOOTHLLM, the first algorithm designed to mitigate jailbreaking attacks. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. Across a range of popular LLMs, SMOOTHLLM sets the state-of-the-art for robustness against the GCG, PAIR, RANDOMSEARCH, and AMPLEGCG jailbreaks. SMOOTHLLM is also resistant against adaptive GCG attacks, exhibits a small, though non-negligible trade-off between robustness and nominal performance, and is compatible with any LLM. Our code is publicly available at https://github.com/arobey1/smooth-llm.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J.},
	month = jun,
	year = {2024},
	note = {arXiv:2310.03684 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\EFNDWRMK\\Robey …等 - 2024 - SmoothLLM Defending Large Language Models Against Jailbreaking Attacks.pdf:application/pdf},
}

@misc{pisano_bergeron_2024,
	title = {Bergeron: {Combating} {Adversarial} {Attacks} through a {Conscience}-{Based} {Alignment} {Framework}},
	shorttitle = {Bergeron},
	url = {http://arxiv.org/abs/2312.00029},
	abstract = {Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. Such vulnerabilities can lead to LLMs being manipulated into generating hazardous content: from instructions for creating dangerous materials to inciting violence or endorsing unethical behaviors. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM acting as a guardian to the primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis reviews that by using Bergeron to complement models with existing alignment training, we can significantly improve the robustness and safety of multiple, commonly used commercial and open-source LLMs. Specifically, we found that models integrated with Bergeron are, on average, nearly seven times more resistant to attacks compared to models without such support.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Pisano, Matthew and Ly, Peter and Sanders, Abraham and Yao, Bingsheng and Wang, Dakuo and Strzalkowski, Tomek and Si, Mei},
	month = aug,
	year = {2024},
	note = {arXiv:2312.00029 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\GAPPVT2L\\Pisano …等 - 2024 - Bergeron Combating Adversarial Attacks through a Conscience-Based Alignment Framework.pdf:application/pdf},
}

@misc{yong_low-resource_2024,
	title = {Low-{Resource} {Languages} {Jailbreak} {GPT}-4},
	url = {http://arxiv.org/abs/2310.02446},
	abstract = {AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4’s safeguard through translating unsafe English inputs into low-resource languages. On the AdvBench benchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79\% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rates, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affected speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs’ safety vulnerabilities. Therefore, our work calls for more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Yong, Zheng-Xin and Menghini, Cristina and Bach, Stephen H.},
	month = jan,
	year = {2024},
	note = {arXiv:2310.02446 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, jailbreak},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\J8GRDPFM\\Yong …等 - 2024 - Low-Resource Languages Jailbreak GPT-4.pdf:application/pdf},
}

@misc{mehrotra_tree_2024,
	title = {Tree of {Attacks}: {Jailbreaking} {Black}-{Box} {LLMs} {Automatically}},
	shorttitle = {Tree of {Attacks}},
	url = {http://arxiv.org/abs/2312.02119},
	abstract = {While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of humandesigned jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80\% of the prompts using only a small number of queries. Interestingly, TAP is also capable of jailbreaking LLMs protected by state-of-the-art guardrails, e.g., LlamaGuard. This significantly improves upon the previous state-of-the-art black-box method for generating jailbreaks.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
	month = feb,
	year = {2024},
	note = {arXiv:2312.02119 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, jailbreak},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\QVSEKE2A\\Mehrotra …等 - 2024 - Tree of Attacks Jailbreaking Black-Box LLMs Automatically.pdf:application/pdf},
}

@misc{zou_universal_2023,
	title = {Universal and {Transferable} {Adversarial} {Attacks} on {Aligned} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.15043},
	doi = {10.48550/arXiv.2307.15043},
	abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
	month = dec,
	year = {2023},
	note = {arXiv:2307.15043 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, jailbreak},
	file = {arXiv Fulltext PDF:C\:\\Users\\kaoke\\Zotero\\storage\\CBGWLGZX\\Zou …等 - 2023 - Universal and Transferable Adversarial Attacks on Aligned Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kaoke\\Zotero\\storage\\PXRK3VAH\\2307.html:text/html},
}

@misc{wei_jailbroken_2023,
	title = {Jailbroken: {How} {Does} {LLM} {Safety} {Training} {Fail}?},
	shorttitle = {Jailbroken},
	url = {http://arxiv.org/abs/2307.02483},
	abstract = {Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of “jailbreak” attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model’s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI’s GPT-4 and Anthropic’s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models’ red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity—that safety mechanisms should be as sophisticated as the underlying model—and argues against the idea that scaling alone can resolve these safety failure modes.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
	month = jul,
	year = {2023},
	note = {arXiv:2307.02483 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\ND2J4PUK\\Wei …等 - 2023 - Jailbroken How Does LLM Safety Training Fail.pdf:application/pdf},
}

@article{chao_jailbreaking_2023,
	title = {Jailbreaking {Black} {Box} {Large} {Language} {Models} in {Twenty} {Queries}},
	url = {https://openreview.net/forum?id=hkjcdmz8Ro},
	abstract = {There is growing research interest in ensuring that large language models align with human safety and ethical guidelines. Adversarial attacks known as 'jailbreaks' pose a significant threat as they coax models into overriding alignment safeguards. Identifying these vulnerabilities through attacking a language model (red teaming) is instrumental in understanding inherent weaknesses and preventing misuse. We present Prompt Automatic Iterative Refinement (PAIR), which generates semantic jailbreaks with only black-box access to a language model. Empirically, PAIR often requires fewer than 20 queries, orders of magnitude fewer than prior jailbreak attacks. PAIR draws inspiration from the human process of social engineering, and employs an attacker language model to automatically generate adversarial prompts in place of a human. The attacker model uses the target model's response as additional context to iteratively refine the adversarial prompt. PAIR achieves competitive jailbreaking success rates and transferability on open and closed-source language models, including GPT-3.5/4, Vicuna, and PaLM.},
	language = {en},
	urldate = {2024-09-03},
	author = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J. and Wong, Eric},
	month = oct,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\kaoke\\Zotero\\storage\\XJUSHBY6\\Chao …等 - 2023 - Jailbreaking Black Box Large Language Models in Twenty Queries.pdf:application/pdf},
}

@misc{liu_autodan_2024,
	title = {{AutoDAN}: {Generating} {Stealthy} {Jailbreak} {Prompts} on {Aligned} {Large} {Language} {Models}},
	shorttitle = {{AutoDAN}},
	url = {http://arxiv.org/abs/2310.04451},
	abstract = {Warning: This paper contains potentially offensive and harmful text. The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available at https://github.com/SheltonLiu-N/AutoDAN.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
	month = mar,
	year = {2024},
	note = {arXiv:2310.04451 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\kaoke\\Zotero\\storage\\WIH93AKE\\Liu …等 - 2024 - AutoDAN Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.pdf:application/pdf},
}

@inproceedings{Qi2023VisualAE,
  title={Visual Adversarial Examples Jailbreak Aligned Large Language Models},
  author={Xiangyu Qi and Kaixuan Huang and Ashwinee Panda and Mengdi Wang and Prateek Mittal},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259244034}
}

@misc{ying2024jailbreakvisionlanguagemodels,
      title={Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt}, 
      author={Zonghao Ying and Aishan Liu and Tianyuan Zhang and Zhengmin Yu and Siyuan Liang and Xianglong Liu and Dacheng Tao},
      year={2024},
      eprint={2406.04031},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.04031}, 
}

@misc{liu2024mmsafetybenchbenchmarksafetyevaluation,
      title={MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models}, 
      author={Xin Liu and Yichen Zhu and Jindong Gu and Yunshi Lan and Chao Yang and Yu Qiao},
      year={2024},
      eprint={2311.17600},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17600}, 
}

@article{Li-HADES-2024,
  author       = {Yifan Li and Hangyu Guo and Kun Zhou and Wayne Xin Zhao and Ji{-}Rong Wen},
  title        = {Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models},
  journal      = {ECCV},
  year         = {2024}
}

@inproceedings{greshake2023not,
  title={Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  booktitle={Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
  pages={79--90},
  year={2023}
}

@article{jain2023baseline,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{vibert,
 author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
 volume = {32},
 year = {2019}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019"
}

@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021}
}

@inproceedings{VL-BERT:,
title={VL-BERT: Pre-training of Generic Visual-Linguistic Representations},
author={Weijie Su and Xizhou Zhu and Yue Cao and Bin Li and Lewei Lu and Furu Wei and Jifeng Dai},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{rlhf,
author = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
title = {Deep reinforcement learning from human preferences},
year = {2017},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4302–4310}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005"
}

@misc{cajueiro2023comprehensivereviewautomatictext,
      title={A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding}, 
      author={Daniel O. Cajueiro and Arthur G. Nery and Igor Tavares and Maísa K. De Melo and Silvia A. dos Reis and Li Weigang and Victor R. R. Celestino},
      year={2023},
      eprint={2301.03403},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.03403}, 
}

@inproceedings{
xiao2023evaluating,
title={Evaluating Evaluation Metrics: A Framework for Analyzing {NLG} Evaluation Metrics using Measurement Theory},
author={Ziang Xiao and Susu Zhang and Vivian Lai and Q.Vera Liao},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=KfJffhdWO1}
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics"
}

@inproceedings{mathur-etal-2020-tangled,
    title = "Tangled up in {BLEU}: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics",
    author = "Mathur, Nitika  and
      Baldwin, Timothy  and
      Cohn, Trevor",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    pages = "4984--4997"
}

@InProceedings{BLIP,
  title = 	 {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author =       {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {12888--12900},
  year = 	 {2022}
}

@article{10.1145/3374217,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
title = {Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {24},
numpages = {41}
}

@inproceedings{wallace-etal-2019-universal,
    title = "Universal Adversarial Triggers for Attacking and Analyzing {NLP}",
    author = "Wallace, Eric  and
      Feng, Shi  and
      Kandpal, Nikhil  and
      Gardner, Matt  and
      Singh, Sameer",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019"
}

@InProceedings{oscar,
author="Li, Xiujun
and Yin, Xi
and Li, Chunyuan
and Zhang, Pengchuan
and Hu, Xiaowei
and Zhang, Lei
and Wang, Lijuan
and Hu, Houdong
and Dong, Li
and Wei, Furu
and Choi, Yejin
and Gao, Jianfeng",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
booktitle="Computer Vision -- ECCV 2020",
year="2020"
}

@inproceedings{vlmo,
 author = {Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit  and Piao, Songhao and Wei, Furu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {32897--32912},
 title = {VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts},
 year = {2022}
}

@inproceedings{10.1007/978-3-030-58577-8_7,
author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
title = {UNITER: UNiversal Image-TExt Representation Learning},
year = {2020},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
}

@InProceedings{Zhang_2021_CVPR,
    author    = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
    title     = {VinVL: Revisiting Visual Representations in Vision-Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {5579-5588}
}

@misc{Claude2024,
  author = {{Anthropic}},
  title = {Claude},
  year = {2024},
  note = {Large language model},
  howpublished = {\url{https://www.anthropic.com}},
}

@article{Chao2023JailbreakingBB,
  title={Jailbreaking Black Box Large Language Models in Twenty Queries},
  author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.08419},
  url={https://api.semanticscholar.org/CorpusID:263908890}
}

@article{rose2010automatic,
  title={Automatic keyword extraction from individual documents},
  author={Rose, Stuart and Engel, Dave and Cramer, Nick and Cowley, Wendy},
  journal={Text mining: applications and theory},
  pages={1--20},
  year={2010},
  publisher={Wiley Online Library}
}

@inproceedings{guocold,
  title={COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability},
  author={Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@misc{Detoxify,
  title={Detoxify},
  author={Hanu, Laura and {Unitary team}},
  howpublished={Github. https://github.com/unitaryai/detoxify},
  year={2020}
}

@article{cheng2024unveiling,
  title={Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model},
  author={Cheng, Hao and Xiao, Erjia and Gu, Jindong and Yang, Le and Duan, Jinhao and Zhang, Jize and Cao, Jiahang and Xu, Kaidi and Xu, Renjing},
  journal={ECCV},
  year={2024}
}

@book{thomas2006elements,
  title={Elements of information theory},
  author={Cover, Thomas and Thomas, Joy},
  year={2006},
  publisher={Wiley-Interscience}
}

@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@book{gonzalez2009digital,
  title={Digital image processing},
  author={Gonzalez, Rafael C},
  year={2009},
  publisher={Pearson education india}
}

@book{jain1989fundamentals,
  title={Fundamentals of digital image processing},
  author={Jain, Anil K},
  year={1989},
  publisher={Prentice-Hall, Inc.}
}

@article{haralick1985image,
  title={Image segmentation techniques},
  author={Haralick, Robert M and Shapiro, Linda G},
  journal={Computer vision, graphics, and image processing},
  volume={29},
  number={1},
  pages={100--132},
  year={1985},
  publisher={Elsevier}
}

@misc{tessellations1992concepts,
  title={Concepts and Applications of Voronoi Diagrams},
  author={Tessellations, Spatial},
  year={1992},
  publisher={New York: JOHN WILEY \& SONS.--2000.--стр}
}

@inproceedings{Duan2023ShiftingAT,
  title={Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models},
  author={Jinhao Duan and Hao Cheng and Shiqi Wang and Alex Zavalny and Chenan Wang and Renjing Xu and Bhavya Kailkhura and Kaidi Xu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:270095084}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@inproceedings{avrahami2022blended,
  title={Blended diffusion for text-driven editing of natural images},
  author={Avrahami, Omri and Lischinski, Dani and Fried, Ohad},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18208--18218},
  year={2022}
}

@inproceedings{azuma2023defense,
  title={Defense-Prefix for Preventing Typographic Attacks on CLIP},
  author={Azuma, Hiroki and Matsui, Yusuke},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3644--3653},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@inproceedings{tramer2022detecting,
  title={Detecting adversarial examples is (nearly) as hard as classifying them},
  author={Tramer, Florian},
  booktitle={International Conference on Machine Learning},
  pages={21692--21702},
  year={2022},
  organization={PMLR}
}

@article{hendrycks2016early,
  title={Early methods for detecting adversarial images},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={ICLR Workshop},
  year={2017}
}

@article{xu2017feature,
  title={Feature squeezing: Detecting adversarial exa mples in deep neural networks},
  author={Xu, W},
  journal={Proceedings of the 26th network and distributed system security symposium (NDSS 2018)},
  year={2018}
}

@article{feinman2017detecting,
  title={Detecting adversarial samples from artifacts},
  author={Feinman, Reuben and Curtin, Ryan R and Shintre, Saurabh and Gardner, Andrew B},
  journal={arXiv preprint arXiv:1703.00410},
  year={2017}
}

@inproceedings{carlini2017adversarial,
  title={Adversarial examples are not easily detected: Bypassing ten detection methods},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={Proceedings of the 10th ACM workshop on artificial intelligence and security},
  pages={3--14},
  year={2017}
}

@inproceedings{ma2019nic,
  title={Nic: Detecting adversarial samples with neural network invariant checking},
  author={Ma, Shiqing and Liu, Yingqi},
  booktitle={Proceedings of the 26th network and distributed system security symposium (NDSS 2019)},
  year={2019}
}

@article{lee2018simple,
  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{yin2020gat,
  title={Gat: Generative adversarial training for adversarial example detection and robust classification},
  author={Yin, Xuwang and Kolouri, Soheil and Rohde, Gustavo K},
  journal={ICLR},
  year={2020}
}

@inproceedings{sheikholeslami2021provably,
  title={Provably robust classification of adversarial examples with detection},
  author={Sheikholeslami, Fatemeh and Lotfi, Ali and Kolter, J Zico},
  booktitle={ICLR},
  year={2021}
}

@article{tramer2020adaptive,
  title={On adaptive attacks to adversarial example defenses},
  author={Tramer, Florian and Carlini, Nicholas and Brendel, Wieland and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1633--1645},
  year={2020}
}

@inproceedings{he2022your,
  title={Be Your Own Neighborhood: Detecting Adversarial Examples by the Neighborhood Relations Built on Self-Supervised Learning},
  author={He, Zhiyuan and Yang, Yijun and Chen, Pin-Yu and Xu, Qiang and Ho, Tsung-Yi},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2022}
}

@inproceedings{zhang2022towards,
  title={Towards adversarial attack on vision-language pre-training models},
  author={Zhang, Jiaming and Yi, Qi and Sang, Jitao},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={5005--5013},
  year={2022}
}

@inproceedings{lu2023set,
  title={Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models},
  author={Lu, Dong and Wang, Zhiqiang and Wang, Teng and Guan, Weili and Gao, Hongchang and Zheng, Feng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={102--111},
  year={2023}
}

@article{han2023ot,
  title={Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization},
  author={Han, Dongchen and Jia, Xiaojun and Bai, Yang and Gu, Jindong and Liu, Yang and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2312.04403},
  year={2023}
}

@article{he2023sa,
  title={Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation},
  author={He, Bangyan and Jia, Xiaojun and Liang, Siyuan and Lou, Tianrui and Liu, Yang and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2312.04913},
  year={2023}
}

@inproceedings{xu2024highly,
  title={Highly transferable diffusion-based unrestricted adversarial attack on pre-trained vision-language models},
  author={Xu, Wenzhuo and Chen, Kai and Gao, Ziyi and Wei, Zhipeng and Chen, Jingjing and Jiang, Yu-Gang},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={748--757},
  year={2024}
}

@article{pan2024sca,
  title={SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial Attack},
  author={Pan, Zihao and Wu, Weibin and Cao, Yuhang and Zheng, Zibin},
  journal={arXiv preprint arXiv:2410.02240},
  year={2024}
}

@article{zhang2024anyattack,
  title={AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models},
  author={Zhang, Jiaming and Ye, Junhong and Ma, Xingjun and Li, Yige and Yang, Yunfan and Sang, Jitao and Yeung, Dit-Yan},
  journal={arXiv preprint arXiv:2410.05346},
  year={2024}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

% DPO
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={ICML},
  year={2024}
}

@article{carlini2024aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{andriushchenko2024jailbreaking,
  title={Jailbreaking leading safety-aligned llms with simple adaptive attacks},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2404.02151},
  year={2024}
}

@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@article{zhan2023removing,
  title={Removing rlhf protections in gpt-4 via fine-tuning},
  author={Zhan, Qiusi and Fang, Richard and Bindu, Rohan and Gupta, Akul and Hashimoto, Tatsunori and Kang, Daniel},
  journal={arXiv preprint arXiv:2311.05553},
  year={2023}
}

@misc{qi2024safetyalignmentjusttokens,
      title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep}, 
      author={Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson},
      year={2024},
      eprint={2406.05946},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.05946}, 
}

@article{gerken2024emergent,
  title={Emergent Equivariance in Deep Ensembles},
  author={Gerken, Jan E and Kessel, Pan},
  journal={ICML},
  year={2024}
}

@article{li2024safety,
  title={Safety Layers in Aligned Large Language Models: The Key to LLM Security},
  author={Li, Shen and Yao, Liuyi and Zhang, Lan and Li, Yaliang},
  journal={arXiv preprint arXiv:2408.17003},
  year={2024}
}

@inproceedings{yi2024vulnerability,
  title={On the vulnerability of safety alignment in open-access llms},
  author={Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={9236--9260},
  year={2024}
}

@article{hazra2024safety,
  title={Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations},
  author={Hazra, Rima and Layek, Sayan and Banerjee, Somnath and Poria, Soujanya},
  journal={arXiv preprint arXiv:2406.11801},
  year={2024}
}

@misc{
zhao2024towards,
title={Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching},
author={Weixiang Zhao and Yulin Hu and Zhuojun Li and Yang Deng and Yanyan Zhao and Bing Qin and Tat-Seng Chua and Ting Liu},
year={2024},
url={https://openreview.net/forum?id=09JVxsEZPf}
}

@article{zhou2024emulated,
  title={Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!},
  author={Zhou, Zhanhui and Liu, Jie and Dong, Zhichen and Liu, Jiaheng and Yang, Chao and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.12343},
  year={2024}
}

@article{zekri2024large,
  title={Large language models as markov chains},
  author={Zekri, Oussama and Odonnat, Ambroise and Benechehab, Abdelhakim and Bleistein, Linus and Boull{\'e}, Nicolas and Redko, Ievgen},
  journal={arXiv preprint arXiv:2410.02724},
  year={2024}
}

@article{imani2024exploring,
  title={Exploring Group and Symmetry Principles in Large Language Models},
  author={Imani, Shima and Palangi, Hamid},
  journal={arXiv preprint arXiv:2402.06120},
  year={2024}
}

@article{chang2024unraveling,
  title={Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures},
  author={Chang, Fu-Chieh and Wu, Pei-Yuan},
  journal={arXiv preprint arXiv:2411.16260},
  year={2024}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{malladi2023kernel,
  title={A kernel-based view of language model fine-tuning},
  author={Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi and Arora, Sanjeev},
  booktitle={International Conference on Machine Learning},
  pages={23610--23641},
  year={2023},
  organization={PMLR}
}

@article{tomihari2024understanding,
  title={Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective},
  author={Tomihari, Akiyoshi and Sato, Issei},
  journal={arXiv preprint arXiv:2405.16747},
  year={2024}
}

@article{jang2024lora,
  title={LoRA Training in the NTK Regime has No Spurious Local Minima},
  author={Jang, Uijeong and Lee, Jason D and Ryu, Ernest K},
  journal={arXiv preprint arXiv:2402.11867},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{huang2023catastrophic,
  title={Catastrophic jailbreak of open-source llms via exploiting generation},
  author={Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
  journal={ICLR},
  year={2024}
}

@inproceedings{
kim2024decoupling,
title={Decoupling Noise and Toxic Parameters for Language Model Detoxification by Task Vector Merging},
author={Yongmin Kim and Takeshi Kojima and Yusuke Iwasawa and Yutaka Matsuo},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=TBNYjdOazs}
}

@article{javaheripi2023phi,
  title={Phi-2: The surprising power of small language models},
  author={Javaheripi, Mojan and Bubeck, S{\'e}bastien and Abdin, Marah and Aneja, Jyoti and Bubeck, Sebastien and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Del Giorno, Allie and Eldan, Ronen and Gopi, Sivakanth and others},
  journal={Microsoft Research Blog},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@incollection{boucheron2003concentration,
  title={Concentration inequalities},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Bousquet, Olivier},
  booktitle={Summer school on machine learning},
  pages={208--240},
  year={2003},
  publisher={Springer}
}

@book{serre1977linear,
  title={Linear representations of finite groups},
  author={Serre, Jean-Pierre and others},
  volume={42},
  year={1977},
  publisher={Springer}
}

@book{dummit2004abstract,
  title={Abstract algebra},
  author={Dummit, David Steven and Foote, Richard M},
  volume={3},
  year={2004},
  publisher={Wiley Hoboken}
}