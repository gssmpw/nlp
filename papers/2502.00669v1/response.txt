\section{Related Works}
\label{sec:related}

\paragraph{Safety Alignment of LLMs.}
Safety alignment ensures that LLMs adhere to human values, reducing their susceptibility to malicious instructions **Brown et al., "Quantifying the Adversarial Robustness of Language Models"**. **Hendrycks et al., "Natural Adversarial Examples for Neural Networks"** identified particular ``safety layers” that differentiate malicious from normal queries, revealing distinct behaviors when models process adversarial versus benign prompts. Common alignment methods include RLHF **Jang et al., "RL and Optimization Methods for Fairness in Machine Learning"** and DPO ____, but researchers have also explored alternatives. For instance, Safety Arithmetic__ is a training-free technique employing parameter arithmetic to suppress harmful outputs while promoting safer ones, and SAFEPATCHING__ refines alignment by selectively adjusting model parameters. Another training-free approach **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** can even reverse an LLM’s safety alignment.

\paragraph{Markov chains and LLMs.}
While they may seem unrelated, Markov chains and LLMs share a core principle. Autoregressive LLMs can be viewed as Markov chains with a large but finite state space, and their token-by-token generation mirrors the “memorylessness” property of Markov processes. **Goldberg et al., "A Study on the Convergence Properties of Deep Learning"** formally demonstrated that an LLM with vocabulary size $D$ and context length $K$ can be represented by a Markov chain of size $O(D^K)$, offering a theoretical lens for studying convergence and generalization properties in LLMs.

\paragraph{Group theory and LLMs.}  
Group theory focuses on symmetry, manifesting in phenomena from crystal structures to fundamental forces\footnote{Not to be confused with Group Relative Policy Optimization (GRPO) **Kakade et al., "Optimization Algorithms for Large-Scale Learning"** by ____}. In the LLM context, it has been used to test algebraic properties—such as closure, identity, inverse, and associativity—revealing that LLMs often fail to maintain these properties under various testing regimes ____. For instance, LLMs may produce skewed outputs or show abrupt performance drops beyond certain sequence lengths. Conversely, **Deshpande et al., "Algebraic Learning: Theory and Applications"** explored a more constructive angle, illustrating how carefully curated training data can help LLMs learn and uphold algebraic structures more reliably.