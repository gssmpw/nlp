% \setlength{\tabcolsep}{2pt}
% \begin{table*}[b]
% \centeringÂ·
% \small 
% \caption{Ablation Study Results for Mistral-7B Base and Llama3-8B Base Models Across AlpacaEval 2, MT-Bench, and Open LLM Leaderboard: Evaluating the Effects of Removing Length Normalization (\texttt{w/o LN}) and Incorporating a Reference Model (\texttt{w/ Ref.}).}
% \label{tab:MT-bench}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{c l ccc ccc ccc cc}
% % \begin{tabular}{lrrrrrrrrrr}
% \toprule
% &\multirow{2}{*}{\textbf{Method}} 
% & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} 
% & \multicolumn{1}{c}{\textbf{MT-Bench}} 
% & \multicolumn{8}{c}{\textbf{Open LLM Leaderboard}} \\
% \cmidrule(lr){3-4}\cmidrule(lr){5-5} \cmidrule(lr){6-13}
% && {\scriptsize \bf LC (\%)} 
% & {\scriptsize \bf WR (\%)} 
% & {\scriptsize \bf GPT-4} 
% & {\scriptsize \bf BBH}  
% & {\scriptsize \bf GPQA} 
% & {\scriptsize \bf MUSR} 
% & {\scriptsize \bf MATH}  
% & {\scriptsize \bf GSM8K} 
% & {\scriptsize \bf ARC} 
% & {\scriptsize \bf TruthfulQA}  
% & {\scriptsize \bf Winograd} \\
% \midrule
% \multirow{3}{*}{\parbox[t]{1.6cm}{\centering Mistral-7B \\ Base}} 
% &\method & 22.4  &  21.3 & 7.5 & 43.99 & 30.12  & 43.95 & 2.57 & 31.92 & 63.50 & 53.64 & 76.25\\
% \cmidrule(lr){2-13}
% &- \texttt{w/o LN} & 5.71   & 3.0   & 4.8 & 41.76 & 29.87 & 44.84 & 3.25 & 28.73 & 59.81 & 40.57 & 77.03\\
% &- \texttt{w/ Ref.} & 15.35 & 11.69 & 6.5 & 43.99 & 30.54 & 40.74 & 2.11 & 26.91 & 61.86 & 43.62 & 76.72\\
% \midrule
% \multirow{3}{*}{\parbox[t]{1.6cm}{\centering Llama3-8B \\ Base}} 
% &\method & 25.2 & 22.9  & 7.7 & 48.62 & 33.80 & 46.03 & 4.61 & 51.02 & 67.06 & 62.59 & 76.24\\
% \cmidrule(lr){2-13}
% &- \texttt{w/o LN} & 7.05  &    3.06   & 5.9 &&&&& 44.28 & 60.58 & 46.87 & 76.64\\
% &- \texttt{w/ Ref.}&  14.97   &  10.69 & 7.3 &&&&& 45.19 & 63.40 & 50.84 & 75.77\\
% \bottomrule
% \end{tabular}
% }
% \label{tab:ablation}
% \vspace{-.5em}
% \end{table*}


\setlength{\tabcolsep}{3pt}
\begin{table*}[t]
\centering
\footnotesize 
\caption{Ablation Study Results on Mistral-7B-Base and Llama3-8B-Base: Evaluating the Effects of Removing Length Normalization (\texttt{w/o LN}) and Incorporating a Reference Model (\texttt{w/ Ref.}).}
\label{tab:ablation}
\resizebox{\textwidth}{!}{
\begin{tabular}{c l ccc ccc ccc cc}
% \begin{tabular}{lrrrrrrrrrr}
\toprule
&\multirow{2}{*}{\textbf{Method}} 
& \multicolumn{6}{c}{\textbf{Open LLM Leaderboard v2}} 
& \multicolumn{4}{c}{\textbf{Open LLM Leaderboard v1}} 
\\
\cmidrule(lr){3-8}\cmidrule(lr){9-12}
&
& {\scriptsize \bf MMLU Pro}  
& {\scriptsize \bf IFEval} 
& {\scriptsize \bf BBH}  
& {\scriptsize \bf GPQA} 
& {\scriptsize \bf MUSR} 
& {\scriptsize \bf MATH}  
& {\scriptsize \bf GSM8K} 
& {\scriptsize \bf ARC} 
& {\scriptsize \bf TruthfulQA}  
& {\scriptsize \bf Winograd} \\
\midrule
\multirow{3}{*}{\parbox[t]{1.6cm}{\centering Mistral-7B \\ Base}} 
&\method & \textbf{27.84} &  15.83  & \textbf{43.99} & 30.12  & \textbf{43.95} & 2.57 & \textbf{31.92} & \textbf{63.50} & \textbf{53.64} & 76.25\\
\cmidrule(lr){2-13}
&- \texttt{w/o LN} & 25.37 & 1.66 & 41.76 & 29.87 & 44.84 & \textbf{3.25} & 28.73 & 59.81 & 40.57 & \textbf{77.03}\\
&- \texttt{w/ Ref.} & 27.31 & \textbf{16.32} & \textbf{43.99} & \textbf{30.54} & 40.74 & 2.11 & 26.91 & 61.86 & 43.62 & 76.72\\
\midrule
\multirow{3}{*}{\parbox[t]{1.6cm}{\centering Llama3-8B \\ Base}} 
&\method & 31.99  &  \textbf{41.78}  & 48.62 & \textbf{33.80} & \textbf{46.03} & \textbf{4.61} & \textbf{51.02} & \textbf{67.06} & \textbf{62.59} & 76.24\\
\cmidrule(lr){2-13}
&- \texttt{w/o LN} & 30.08 & 36.31 &  48.62 & 32.05 & 45.37 & 3.17 & 44.28 & 60.58 & 46.87 & \textbf{76.64} \\
&- \texttt{w/ Ref.}& \textbf{32.06} & 36.71 & \textbf{49.30} & 31.71 & 41.53 & \textbf{4.61} & 45.19 & 63.40 & 50.84 & 75.77\\
\bottomrule
\end{tabular}
}
\label{tab:ablation}
\vspace{-1em}
\end{table*}