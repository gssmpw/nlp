
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\iclrfinalcopy

\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}
\usepackage{bbm}%
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{fleqn, tabularx}
\usepackage{multirow}
\usepackage[export]{adjustbox}
\usepackage{amsfonts}
\usepackage{amsmath, amsthm, bbm, mathtools, commath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{quoting}
% \usepackage[T1]{fontenc}
\usepackage{bold-extra}
% \usepackage{lmodern}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\definecolor{mark}{RGB}{208,64,56}
\newcommand{\cmark}{{\color{mark}\ding{51}}}
\newcommand{\xmark}{\ding{55}}

\usepackage[algo2e,linesnumbered,ruled,lined]{algorithm2e}
\usepackage{datetime}
\usepackage{verbatim}




\usepackage[font=small]{subfig}
\usepackage{colortbl}
\usepackage{arydshln} %
\newcommand{\facc}[2]{{#1}{\scriptsize±{#2}}}
\newcommand{\bfacc}[2]{\textbf{{#1}{\scriptsize±{#2}}}}
% \newcommand{\bestcell}{\cellcolor{}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\\usepackage{bbm}{#1}}\usepackage{bbm}
\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\mbf}{\mathbf}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\bestcell}{\cellcolor{blue!25}}
\newcommand{\myparagraph}[1]{\noindent \textbf{#1}}
\newcommand{\grayrow}{\rowcolor[gray]{.9}}
\newcommand{\indd}{\textit{ind}\xspace}
\newcommand{\tran}{\textit{tran}\xspace}
\newcommand{\production}{\textit{prod}\xspace}
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\usepackage{paralist}
\makeatother
\newcommand\eqc{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily c}}}{=}}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\A}{\mbf{A}}
\newcommand{\X}{\mbf{X}}
\newcommand{\W}{\mbf{W}}
\newcommand{\Y}{\mbf{Y}}
\newcommand{\hA}{\mbf{h}_\A}
\newcommand{\hX}{\mbf{h}_\X}
% \usepackage{subfigure}
\setlength\dashlinegap{3pt}
\setlength\arrayrulewidth{.5pt}
\setlength\dashlinedash{4pt}
\newcommand{\std}[1]{\pm #1}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage{lineno} 
% color related
\definecolor{Gray}{gray}{0.9}
\usepackage{xcolor}
\colorlet{darkgreen}{green!65!black}
\colorlet{darkblue}{blue!75!black}
\colorlet{darkred}{red!80!black}
\definecolor{lightblue}{HTML}{0071bc}
\definecolor{lightgreen}{HTML}{39b54a}
\definecolor{manyshot}{HTML}{6969ff}
\definecolor{medshot}{HTML}{f7c600}
\definecolor{fewshot}{HTML}{ff6969}
\definecolor{mypurple}{HTML}{412F8A}
\definecolor{myorange}{HTML}{fc8e62}
\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}
\definecolor{Blue9}{rgb}{0.098,0.3,0.9}
% \definecolor{citecolor}{HTML}{3333A6}
\definecolor{urlcolor}{HTML}{3333A6}
\usepackage[pagebackref=false, breaklinks=true, colorlinks,
            citecolor=darkblue, urlcolor=urlcolor,linkcolor=linkcolor, bookmarks=false]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{tabularx}
\usepackage{url}
\newcommand{\teng}[1]{\textcolor{blue}{#1}}
\usepackage{longtable,tabularx,booktabs,wrapfig}
\renewcommand\ttdefault{cmtt}
\newcommand{\method}{\texttt{SimPER}\xspace}
\newcommand{\MLP}{\texttt{MLP}\xspace}
\newcommand{\GNN}{\texttt{GNN}\xspace}


\definecolor{customgray}{rgb}{0.25,0.25,0.25}
\definecolor{customred}{rgb}{0.8,0.05,0.05}
\definecolor{urlcolors}{rgb}{0.872,0.2,0.552}
\newcommand{\sbest}[1]{\textcolor{customgray}{\textbf{#1}}}
\newcommand{\best}[1]{\textcolor{customred}{\textbf{#1}}}
\newcommand{\urlcolor}[1]{\textcolor{urlcolors}{\textbf{#1}}}


% \title{\method:  Simple Perplexity Optimization for Hyperparameter-free Preference Alignment}
\title{SimPER: A Minimalist Approach to Preference \\ Alignment without Hyperparameters}
% B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning
% a minimalist approach to offline reinforcement learning

% \title{Hyperparameter-Free Preference Optimization}

% \author{Teng Xiao$^{\heartsuit}$\thanks{Equal contribution.}\hspace{1.5mm}, Yige Yuan$^{\spadesuit}$\footnotemark[1]\hspace{1.5mm}, Zhengyu Chen$^{\clubsuit}$ , Mingxiao Li$^\blacktriangle$, \\ \textbf{Shangsong Liang}$^\blacklozenge$, \textbf{Zhaochun Ren}$^\diamondsuit$, \textbf{Vasant G Honavar}$^{\heartsuit}$  \\
% $^\heartsuit$Pennsylvania State University
% $^\spadesuit$University of Chinese Academy of Sciences \\  $^\clubsuit$Meituan Inc$^\blacktriangle$Tencent AI Lab $^\blacklozenge$Sun Yat-Sen University $^\diamondsuit$Leiden University\\
% \texttt{tengxiao@psu.edu}, 
% \texttt{yuanyige923@gmail.com},
% \texttt{vhonavar@psu.edu} 
% }
\author{Teng Xiao$^{1}$\thanks{Equal contribution.}\hspace{1.5mm}, Yige Yuan$^{2}$\footnotemark[1]\hspace{1.5mm}, Zhengyu Chen$^{3}$, Mingxiao Li$^4$, \\ \textbf{Shangsong Liang}$^5$\textbf{,} \textbf{Zhaochun Ren}$^6$\textbf{,} \textbf{Vasant G Honavar}$^{1}$  \\
$^1$Pennsylvania State University
$^2$University of Chinese Academy of Sciences \\  $^3$Meituan Inc $^4$Tencent AI Lab $^5$Sun Yat-Sen University $^6$Leiden University\\
\texttt{tengxiao@psu.edu}, 
\texttt{yuanyige923@gmail.com},
\texttt{vhonavar@psu.edu} 
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. 
In this paper, we propose a simple yet effective hyperparameter-free preference optimization algorithm for alignment. We observe that promising performance can be achieved simply by optimizing inverse perplexity, which is calculated as the inverse of the exponentiated average log-likelihood of the chosen and rejected responses in the preference dataset.
The resulting simple learning objective, \method (\textbf{{Sim}}ple alignment with \textbf{Per}plexity optimization), is easy to implement and eliminates the need for expensive hyperparameter tuning and a reference model, making it both computationally and memory efficient. Extensive experiments on widely used real-world benchmarks, including MT-Bench, AlpacaEval 2, and \textbf{10} key benchmarks of the Open LLM Leaderboard with \textbf{5} base models, demonstrate that \method consistently and significantly outperforms existing approaches—even without any hyperparameters or a reference model. For example, despite its simplicity, \method outperforms state-of-the-art methods by up to \textbf{5.7} points on AlpacaEval 2 and achieves the highest average ranking across \textbf{10} benchmarks on the Open LLM Leaderboard. The source code for \method is publicly available at the Github: \url{https://github.com/tengxiao1/SimPER}.
\end{abstract}



\input{sections/introduction}
\input{sections/related}
\input{sections/method}
\input{sections/experiments}


\section{Conclusion}
In this paper, we propose a simple yet effective alignment method for large language models, named \method. \method eliminates the need for both hyperparameters and a reference model while achieving strong performance. The key idea of \method is to directly optimize the reverse perplexity of both the chosen and rejected responses in the preference dataset. Specifically, we minimize the perplexity over the chosen response while maximizing the perplexity over the rejected response, ensuring that the model produces responses with high preference scores. We also provide a theoretical understanding of \method, demonstrating that it can mitigate the problem of gradient imbalance between the chosen and rejected responses during optimization. Furthermore, we show that \method implicitly exhibits mode-seeking behavior, leading to strong alignment performance. Extensive experiments on widely used benchmarks demonstrate that \method significantly outperforms state-of-the-art methods.

\section*{Acknowledgment}
The work of Vasant G Honavar and Teng Xiao was supported in part by grants from the National Science Foundation
(2226025, 2225824), the National Center for Advancing Translational Sciences, and the National
Institutes of Health (UL1 TR002014).
% \clearpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\input{sections/appendix-proof}


\end{document}
