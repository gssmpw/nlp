\newpage
\appendix
\section{Proofs of Theorem~\ref{the:mutual}}
\label{app:proof}
To prove Theorem~\ref{the:mutual} in the main paper, we first present the following  Lemmas:

\begin{lemma}\label{lemma:1}
Given the model conditional distribution $\pi_{\theta}(\mathbf{y} \mid \mathbf{x})=\prod_{i=1}^{|\mathbf{y}|} \pi_\theta\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right)$ and  data distribution $\pi_{\rm{data}}(\mathbf{y} \mid \mathbf{x})=\prod_{i=1}^{|\mathbf{y}|} \pi_{\rm{data}}\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right)$, then we have the following relationship between the sequence-level TVD objective and its token-level factorization:
   \begin{equation}
    {\mathrm{TV}}(\pi_{\rm{data}}(\mathbf{y} \mid \mathbf{x}), \pi_{\theta}(\mathbf{y} \mid \mathbf{x})) \le \mathbb{E}_{\mathbf{y}\sim \pi_{\rm{data}}(\mathbf{y} \mid \mathbf{x})}\Bigg[\sum\nolimits_{i=1}^{|\mathbf{y}|} {\mathrm{TV}}(\pi_{\rm{data}}^{<i}(\mathbf{y}_{i}), \pi_{\theta}^{<i}(\mathbf{y}_{i}))\Bigg],
\end{equation}
where $\pi_{\rm{data}}^{<i}(\mathbf{y}_{i})$ and $\pi_{\theta}^{<i}(\mathbf{y}_{i})$ are shorts for $\pi_{\rm{data}}\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right)$ and  $\pi_\theta\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right)$, respectively.
\end{lemma}
\begin{proof}
We  start by re-writing the TVD loss in the following recursive form:
\begin{align}
    & {\mathrm{TV}}(\pi_{\rm{data}}(\mathbf{y} \mid \mathbf{x}), \pi_{\theta}(\mathbf{y} \mid \mathbf{x}))=\frac{1}{2}\sum  \nolimits_{\mathbf{y}\in \mathcal{Y}} \left| \pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x}) -  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  \right|\\
     &=\frac{1}{2} \sum_{\mathbf{y}_1,\cdots,\mathbf{y}_{|\mathbf{y}|}}|\prod_{i=1}^{|\mathbf{y}|} \pi_{\rm{data}}\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right)-\prod_{i=1}^{|\mathbf{y}|} \pi_\theta\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right)| \label{Eq:lemma1-1}\\
     &=\frac{1}{2} \sum_{\mathbf{y}_1,\cdots,\mathbf{y}_{|\mathbf{y}|}}|\prod_{i=1}^{|\mathbf{y}|} \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})-\prod_{i=1}^{|\mathbf{y}|} \pi_{\theta}^{<i}(\mathbf{y}_{i})|\\
     &\leq \frac{1}{2}\sum_{i=1}^{|\mathbf{y}|}\sum_{\mathbf{y}_1,\cdots,\mathbf{y}_{i}}\prod _{j=1}^{i-1} \pi_{\rm{data}}^{<j}(\mathbf{y}_{j})|\prod_{i=1}^{|\mathbf{y}|} \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})-\prod_{i=1}^{|\mathbf{y}|} \pi_{\theta}^{<i}(\mathbf{y}_{i})| \sum_{\mathbf{y}_{i+1},\cdots,\mathbf{y}_{|\mathbf{y}|}}\prod_{t=i+1}^{|\mathbf{y}|} \pi_{\theta}^{<t}(\mathbf{y}_{t}) \label{Eq:lemma1-2}\\
     &= \frac{1}{2}\sum_{i=1}^{|\mathbf{y}|}\sum_{\mathbf{y}_1,\cdots,\mathbf{y}_{i}}\prod _{j=1}^{i-1} \pi_{\rm{data}}^{<j}(\mathbf{y}_{j})|\pi_{\rm{data}}^{<i}(\mathbf{y}_{i})- \pi_{\theta}^{<i}(\mathbf{y}_{i})| \label{Eq:lemma1-3} \\
       &= \frac{1}{2}\sum_{i=1}^{|\mathbf{y}|}\sum_{\mathbf{y}_{i}} \mathbb{E}_{ \mathbf{y_{<i}}\sim \pi_{\rm{data}}}\Big[|\pi_{\rm{data}}^{<i}(\mathbf{y}_{i})- \pi_{\theta}^{<i}(\mathbf{y}_{i})|\Big]\\
       &=\mathbb{E}_{\mathbf{y}\sim \pi_{\rm{data}}}\Bigg[ \sum_{i=1}^{|\mathbf{y}|} \mathrm{TV}(\pi_{\rm{data}}^{<i}(\mathbf{y}_{i}),\pi_{\theta}^{<i}(\mathbf{y}_{i})) \Bigg],
\end{align}
where Equation~(\ref{Eq:lemma1-1}) breaks the sequence-level summation into the steps and Equation~(\ref{Eq:lemma1-2}) applies the following triangle inequality~\citep{wen2023f,jitailoring}:
\begin{align}
\left| \prod_{t=1}^{T} a_t - \prod_{t=1}^{T} b_t \right|
\leq \sum_{t=1}^{T} \left| a_t - b_t \right| \cdot
\left( \prod_{i=1}^{t-1} a_i \right) \cdot
\left( \prod_{j=t+1}^{T} b_j \right),
\end{align}
and Equation~(\ref{Eq:lemma1-3}) marginalizes out variables $\mathbf{y}_{i+1},\cdots,\mathbf{y}_{|\mathbf{y}|}$. 
\end{proof}


% \begin{lemma}\label{lemma:2}
% Minimizing the \texttt{SimPO} objective in Equation~(\ref{Eq:SimPO}) with respect to ${\theta}$ reduces the Kullbackâ€“Leibler divergence between $\pi_\theta$ and the distribution of chosen response in preference dataset:
% \begingroup\makeatletter\def\f@size{9.5}\check@mathfonts\def\maketag@@@#1{\hbox{\m@th\normalfont\normalfont#1}}
% \begin{align}
% \min_{\theta} \mathcal{L}_{\rm{SimPO}} \Rightarrow  \min_{\theta}{\mathrm{KL}}(\pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x})\|  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  ) =\sum  \nolimits_{y\in \mathcal{Y}} \pi_{\rm{chosen}}(\mathbf{y} \mid \mathbf{x})\log \frac{\pi_{\rm{chosen}}(\mathbf{y} \mid \mathbf{x})}{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}
% \end{align}
% \endgroup
% \end{lemma}
% \begin{proof}
% Recall that the objective of \texttt{SimPO} without hyperparmeters is:
% \begin{align}
% \mathcal{L}_{\rm{SimPO}}({\theta}; \mathcal{D})=\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)\sim \mathcal{D}}\left[-\log \sigma( \log {\pi_{\theta}(\mathbf{y}_w \mid \mathbf{x})}-\log {\pi_{\theta}(\mathbf{y}_l \mid \mathbf{x} )}) \right]. \label{Eq:app-simpo}
% \end{align}
% We connect this objective to a distribution matching problem by defining an energy-based model:
% % \begingroup\makeatletter\def\f@size{9.5}\check@mathfonts\def\maketag@@@#1{\hbox{\m@th\normalfont\normalfont#1}}
% \begin{align}
% \pi(\mathbf{y} \mid \mathbf{x})={\exp(r(\mathbf{x},\mathbf{y}))}/{Z(\mathbf{x})}, \label{Eq:optimal}
% \end{align}
% % \endgroup
% where $Z(\mathbf{x})=\sum_{\mathbf{y}}\exp(r(\mathbf{x},\mathbf{y}))$. We then frame the task as minimizing the KL divergence between the policy $\pi$ and the distribution $\pi_{\mathrm{chosen}}$ that generates the chosen response. The goal is to learn the reward model $r$ such that the model distribution imitates the chosen data distribution:
% \begin{align}
% \min _{r} {\mathrm{KL}}\left(\pi_{\mathrm{chosen}}(\mathbf{y} \mid \mathbf{x}) \| \pi(\mathbf{y} \mid \mathbf{x})\right). \label{eq:FKL}
% \end{align}
% Minimizing the above forward KL divergence with the chosen responses on preference data gives us:
% \begingroup\makeatletter\def\f@size{9.5}\check@mathfonts\def\maketag@@@#1{\hbox{\m@th\normalfont\normalfont#1}}
% \begin{align}
% \min _{r} &\mathbb{E}_{(\mathbf{x},\mathbf{y}_{w})\sim \mathcal{D}}[-\log \frac{\exp(r(\mathbf{x},\mathbf{y}_{w}))}{Z(\mathbf{x})}]=\\
%     &\mathbb{E}_{(\mathbf{x},\mathbf{y}_{w})\sim \mathcal{D}} \Big [-r(\mathbf{x},\mathbf{y}_{w})+\log \mathbb{E}_{q(\mathbf{y}\mid
%     \mathbf{x})} \big[ \frac{\exp (r(\mathbf{x},\mathbf{y}))}{q(\mathbf{y}\mid\mathbf{x})} \big] \Big ]. \label{Eq:connection}
% \end{align}
% \endgroup
% The equality follows due to importance sampling under distribution $q$.  A choice that simplifies the above expression is $q(\mathbf{y} \mid \mathbf{x})=\frac{1}{2} \mathbb{I}(\mathcal{Y}=\mathbf{y}_{l})+\frac{1}{2}  \mathbb{I}(\mathcal{Y}=\mathbf{y}_{w})$.  In this case, the sample-based approximation of Equation~(\ref{Eq:connection})  gives us:
% \begin{align}
% \min_{r}&\mathbb{E}_{(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\sim \mathcal{D}}\Big [-r(\mathbf{x},\mathbf{y}_{w})+\log (\exp(r(\mathbf{x},\mathbf{y}_{w}))+ \exp(r(\mathbf{x},\mathbf{y}_{l})))\Big ] \nonumber \\
% &=\mathbb{E}_{(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\sim \mathcal{D}}\Big [-\log (\frac{1}{1+ \exp(r(\mathbf{x},\mathbf{y}_{l})-r(\mathbf{x},\mathbf{y}_{w}))} \Big]. \label{Eq:imtaiton-reward}
% \end{align}
% One can easily see that the above loss is exactly the same as the reward loss function in RLHF~\citep{rafailov2024direct}. By optimizing this loss function, one can directly obtain the optimal policy via Equation~(\ref{Eq:optimal}). Unfortunately,  even if we use the estimate $r$, it is
% still expensive to estimate the partition function $Z(\mathbf{x})$, which makes this representation hard to utilize in practice and  significantly higher inference cost~\citep{rafailov2024direct}. To address this problem,  we apply the following knowledge distillation loss, which distills the optimal policy in Equation~(\ref{Eq:optimal}) into a analytical policy:
% \begin{align}
% \min_{\pi_\theta}\mathrm{KL}\left(\pi_{\theta}(\mathbf{y}\mid \mathbf{x})||{\exp(r(\mathbf{x},\mathbf{y}))}/{Z(\mathbf{x})} \right), \label{Eq:distillation}
% \end{align}
% This gives the following loss after removing multiplicative and additive constants:
% \begin{align}
%     \mathcal{L}(\theta)=-\mathbb{E}_{\pi_\theta(\mathbf{y} \mid \mathbf{x})}\left[r_{\phi}\left(\mathbf{x}, \mathbf{y}\right)-\log \pi_{\theta}(\mathbf{y}|\mathbf{x})\right], \label{Eq:distillation2}
% \end{align}
% One can observe that this knowledge distillation objective directly corresponds to the RL objective in RLHF, as shown in \citep{rafailov2024direct}. Similar to the derivation in~\citep{rafailov2024direct}, which uses an analytical mapping from reward functions to optimal policies in Equation~(\ref{Eq:distillation2}), combining Equation~(\ref{Eq:imtaiton-reward}) results in the SimPO objective in Equation~(\ref{Eq:app-simpo}). Therefore, performing distribution matching over the chosen responses using KL divergence ${\mathrm{KL}}\left(\pi_{\mathrm{chosen}}(\mathbf{y} \mid \mathbf{x}) \| \pi_{\theta}(\mathbf{y} \mid \mathbf{x})\right)$ is equivalent to solving an entropy-regularized RLHF problem, as well as the SimPO objective.
% \end{proof}


\textbf{Theorem~\ref{the:mutual}.} \textit{
Minimizing  \texttt{SFT}  with respect to ${\theta}$ is approximately minimizing the KLD between $\pi_\theta$ and the distribution of the chosen response in the preference dataset, while minimizing our \method  is approximately minimizing the TVD.}
\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts\def\maketag@@@#1{\hbox{\m@th\normalfont\normalfont#1}}
\begin{align}
\min_{\theta} \mathcal{L}_{\rm{SFT}} &\Rightarrow  \min_{\theta}{\mathrm{KL}}(\pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x})\|  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  ) =\sum  \nolimits_{y\in \mathcal{Y}} \pi_{\rm{chosen}}(\mathbf{y} \mid \mathbf{x})\log \frac{\pi_{\rm{chosen}}(\mathbf{y} \mid \mathbf{x})}{\pi_{\theta}(\mathbf{y} \mid \mathbf{x})}\\
\min_{\theta} \mathcal{L}_{\rm{SimPER}} &\Rightarrow  \min_{\theta}{\mathrm{TV}}(\pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x})\|  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  ) =\sum\nolimits_{y\in \mathcal{Y}} \left| \pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x}) -  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  \right| 
\end{align}
\endgroup
\begin{proof}
Given that $\mathbb{E}_{y \sim \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})}[\mathbf{y}_{i}] = \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})$, where $\mathbf{y}_{i}$ represents the observed one-hot distribution with only the  $w$-th index of the observed token being 1 and the others being 0, we have
\begin{align}
\mathrm{TV}&(\pi_{\rm{data}}^{<i}(\mathbf{y}_{i}), \pi_{\theta}^{<i}(\mathbf{y}_{i}))=\frac{1}{2}\sum_{\mathbf{y}_{i}}|\pi_{\rm{data}}^{<i}(\mathbf{y}_{i})- \pi_{\theta}^{<i}(\mathbf{y}_{i})|\\
&=\frac{1}{2}\sum_{\mathbf{y}_{i}}|\mathbb{E}_{y \sim \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})}[\mathbf{y}_{i}]  - \pi_{\theta}^{<i}(\mathbf{y}_{i})|\\
&\leq \frac{1}{2}\mathbb{E}_{y \sim \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})}\Big[\sum_{\mathbf{y}_{i}}|\mathbf{y}_{i} - \pi_{\theta}^{<i}(\mathbf{y}_{i})| \Big]=\mathbb{E}_{y \sim \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})}\left[\mathrm{TV}(\mathbf{y}_{i},\pi_{\theta}^{<i}(\mathbf{y}_{i})) \right]\\
&=\mathbb{E}_{y \sim \pi_{\rm{data}}^{<i}(\mathbf{y}_{i})}\Big[1-\sum_{\mathbf{y}_{i}} \min (\mathbf{y}_{i},\pi_{\theta}^{<i}(\mathbf{y}_{i})) \Big]=- \pi_{\theta}^{<i}(\mathbf{y}_{i}).
\end{align}
Combing the above with Lemma~\ref{lemma:1}, we have:
\begin{align}
{\mathrm{TV}}(\pi_{\rm{data}}(\mathbf{y} \mid \mathbf{x}), \pi_{\theta}(\mathbf{y} \mid \mathbf{x}))&\leq \mathbb{E}_{\mathbf{y}\sim \pi_{\rm{data}}}\Bigg[ \sum_{i=1}^{|\mathbf{y}|} \mathrm{TV}(\pi_{\rm{data}}^{<i}(\mathbf{y}_{i}),\pi_{\theta}^{<i}(\mathbf{y}_{i})) \Bigg]\\
& = -\mathbb{E}_{\mathbf{y}\sim \pi_{\rm{data}}} \Bigg[\sum_{i=1}^{|\mathbf{y}|}\pi_\theta\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right)\Bigg].
\end{align}
Recall that the objective of \method, without length-averaging, is:
\begin{align}
    \mathcal{L}_{\rm{SimPER}}({\theta}; \mathcal{D})&=-\exp \left( \log \pi_\theta(\mathbf{y}_{w} \mid \mathbf{x})\right)+\exp \left( \log \pi_\theta(\mathbf{y}_{l} \mid \mathbf{x})\right)\\
    &\geq -\pi_\theta(\mathbf{y}_{w} \mid \mathbf{x})\geq {\mathrm{TV}}(\pi_{\rm{chosen}}(\mathbf{y} \mid \mathbf{x}), \pi_{\theta}(\mathbf{y} \mid \mathbf{x})). \label{Eq:43}
\end{align}
Combining Equation~(\ref{Eq:43}) completes the whole proof.
\end{proof}


\section{Experimental Details}
\subsection{Downstream Task Evaluation}
\label{app:task-eval}
This section is for the detailed introduction to the evaluation of downstream tasks.
For models fine-tuned on UltraFeedback Binarized, following previous works~\citep{rafailov2024direct,tunstall2023zephyr}, we evaluated them on HuggingFace Open LLM Leaderboard v1\footnote{\url{https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard}} and v2\footnote{\url{https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard}}~\citep{open-llm-leaderboard-v1,open-llm-leaderboard-v2}, encompassing ten tasks: 
MMUL-PRO~\citep{wang2024mmluprorobustchallengingmultitask}, IFEval~\citep{zhou2023instructionfollowingevaluationlargelanguage}, BBH~\citep{suzgun2022challengingbigbenchtaskschainofthought}, GPQA~\citep{rein2023gpqagraduatelevelgoogleproofqa}, MUSR~\citep{sprague2024musrtestinglimitschainofthought}, MATH~\citep{hendrycks2021measuringmathematicalproblemsolving}, GSM8k~\citep{cobbe2021training}, ARC~\citep{clark2018think}, 
TruthfulQA~\citep{lin-etal-2022-truthfulqa} and Winogrande~\citep{sakaguchi2021winogrande} provided by the Language Model Evaluation Harness library~\citep{eval-harness}.
In addition to the leaderboard, we also evaluated them on two instruction-following benchmarks AlpacaEval2~\citep{li2023alpacaeval} and MT-Bench~\citep{zheng2023judging}.
For models fine-tuned on Anthropic-HH for safety alignment, following previous works~\citep{rafailov2024direct}, we use GPT-4 for zero-shot pair-wise evaluation, which is shown to be consistent with human judgments.

% \textbf{Massive Multitask Language Understanding Professional}: MMUL-PRO is an enhanced version of the MMLU dataset~\citep{hendrycks2021measuring}, addressing previous shortcomings by increasing choice options in multiple-choice questions and refining question quality through expert review, making it more challenging and less prone to data contamination.

% \textbf{Instruction-Following Evaluation}: IFEval is a benchmark evaluating a model's ability to follow explicit instructions, emphasizing adherence to formatting over content generation.

% \textbf{Big Bench Hard}: BBH is a selection of 23 challenging tasks from the BigBench, focusing on areas like multistep arithmetic, algorithmic reasoning, language understanding, and world knowledge.

% \textbf{Graduate-Level Google-Proof Q\&A}: GPQA is a challenging benchmark composed of advanced questions developed by PhD-level experts across various fields like biology, physics, and chemistry.

% \textbf{Multistep Soft Reasoning}: MUSR is a challenging benchmark consisting of algorithmically generated complex scenarios requiring integrated reasoning and long-range context understanding ability.

% \textbf{MATH}: MATH is a benchmark consisting of high-school level competition problems gathered from multiple sources, consistently formatted with LaTeX for equations and Asymptote for diagrams.

 
% \textbf{GSM8K}: GSM8K is a benchmark consisting of diverse grade school math word problems designed to evaluate a model's capability in solving multi-step mathematical reasoning problems.


% \textbf{AI2 Reasoning Challenge}: ARC is a benchmark consists of grade-school science questions.

% \textbf{TruthfulQA}: TruthfulQA is a benchmark fore the truthfulness of a model in generating responses to questions, including 817 questions across 38 categories such as health, law, finance, and politics.
 
% \textbf{Winogrande}: Winogrande is a large-scale benchmark for commonsense reasoning, inspired by WSC benchmark~\citep{levesque2012winograd}, but adjusted to improve both the scale and the hardness.


% \textbf{AlpacaEval 2.0}: AlpacaEval 2.0 is a LLM-based automatic evaluator for instruction-following language models, which is fast, cheap, replicable, and validated against 20K human annotations.


% \textbf{MT Bench}: MT-bench is an evaluator featuring challenging, multi-turn, open-ended questions to assess chat assistants, which automates the evaluation process by using advanced LLMs as judges.

\textbf{Safety Evaluation}:
To evaluate the safety of the Anthropic HH trained model, the Anthropic HH test set is used as the evaluation dataset. The preferred responses within this test set serve as the baseline. GPT-4 is then employed to determine the winner, which is shown to be consistent with human judgments. We use \texttt{gpt-4-0314} for all our experiments. The specific prompts used for the GPT-4 evaluation are detailed below.

\begin{table}[h!]
    \small
    {\ttfamily
    \begin{tabularx}{\linewidth}{X}
    \toprule
    For the following dialogue history to a chatbot, which response is more helpful and harmless?
    \\
    \\
    Dialogue history: \\
    \{dialogue history\}
    \\
    \\
    Response 1:\\
     \{Response 1\}
    \\
    \\
    Response 2: \\
     \{Response 2\}
    \\
    \\
    FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful and harmless. SECOND, on a new line, state only "1" or "2" to indicate which response is more helpful and harmless. Your response should use the format:\\
    Comparison: <one-sentence comparison and explanation>\\
    More helpful: <"1" or "2">\\
    \bottomrule
    \end{tabularx}
    }
    \caption{Prompt for GPT-4 evaluation for the safety alignment task on the Anthropic-HH dataset.   \{dialogue history\},     \{Response 1\} and  \{Response 2\} are placeholders.}
    \label{tab:gpt4_prompt_dialogue}
\end{table}



\subsection{Baseline Methods}
\label{app:baseline}
This section provides a detailed introduction to state-of-the-art baselines for preference fine-tuning, with an emphasis on the usage of hyperparameters in their loss functions.

\textbf{DPO} Direct Preference Optimization~\citep{rafailov2024direct} uses log-likelihood differences to implicitly represent the reward function, eliminating the need for explicit reward model like RLHF. 
DPO involves one tunable hyperparameter, $\beta$, which controls the deviation from the reference model. 
$$
\mathcal{L}_{\rm{DPO}}({\theta}; \mathcal{D})=\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)\sim \mathcal{D}}\left[-\log \sigma(\beta \log \frac{\pi(\mathbf{y}_w \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}_w \mid \mathbf{x})}-\beta \log \frac{\pi(\mathbf{y}_l \mid \mathbf{x} )}{\pi_{\mathrm{ref}}(\mathbf{y}_l \mid \mathbf{x})})\right].
$$

\textbf{IPO} Identity Preference Optimization~\citep{azar2024general} minimizes a squared loss regression problem by defining an alternative reward function, avoiding unstable RL training. 
IPO involves one hyperparameter, $\beta$, to adjust the reward margin.
$$
\mathcal{L}_{\rm{IPO}}({\theta}; \mathcal{D})=\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)\sim \mathcal{D}}\left[\left (\log \frac{\pi_\theta(\mathbf{y}_w \mid \mathbf{x})\pi_{\mathrm{ref}}(\mathbf{y}_l \mid \mathbf{x})}{\pi_{\theta}(\mathbf{y}_l \mid \mathbf{x} )\pi_{\mathrm{ref}}(\mathbf{y}_w \mid \mathbf{x})}-\frac{1}{2\beta}\right)^{2}\right].
$$

\textbf{CPO} Contrastive Preference Optimization~\citep{xucontrastive} uses log-likelihood as the reward and is trained alongside a Supervised Fine-Tuning (SFT) objective. 
CPO involves two hyperparameters: $\beta$, which scales the log probabilities, and $\lambda$, which weights the SFT component.
$$
\mathcal{L}_{\rm{CPO}}({\theta}; \mathcal{D}) = -\log \sigma\left(\beta \log \pi_\theta(\mathbf{y}_w\mid \mathbf{x}) - \beta \log \pi_\theta(\mathbf{y}_l\mid \mathbf{x}) \right) - \lambda \log \pi_\theta (\mathbf{y}_w\mid \mathbf{x}).
$$


\textbf{SLiC} Sequence Likelihood Calibration~\citep{zhao2023slic} directly uses log-likelihood and includes a SFT objective. 
SLiC involves two hyperparameters: $\delta$, which sets the margin for the ranking loss, and $\lambda$, which weights the SFT component.
$$
\mathcal{L}_{\text{SLiC}}(\theta; \mathcal{D}) = \max\left(0, \delta - \log \pi_\theta(\mathbf{y}_w\mid \mathbf{x}) + \log \pi_\theta(\mathbf{y}_l\mid \mathbf{x})\right) - \lambda \log \pi_\theta (\mathbf{y}_w \mid \mathbf{x}).
$$


\textbf{SimPO} Simple Preference Optimization~\citep{meng2024simpo} eliminates the need for a reference model and optimizes a length-regularized probability of response pairs. 
SimPO involves two hyperparameters: $\beta$ to scale the log probabilities and $\gamma$ to adjust the reward margin. 
$$
\mathcal{L}_{\rm{SimPO}}({\theta}; \mathcal{D})=\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)\sim \mathcal{D}}\left[-\log \sigma(\beta \log {\pi_{\theta}(\mathbf{y}_w \mid \mathbf{x})}-\beta \log {\pi_{\theta}(\mathbf{y}_l \mid \mathbf{x} )})-\gamma \right].
$$


\textbf{KTO} Kahneman-Tversky Optimization~\citep{ethayarajh2024kto} learns from non-paired preference data. 
KTO involves three hyperparameters: $\beta$, which controls the deviation from the reference model; $\lambda_w$ and $\lambda_l$, which weight the preference components for winning and losing responses, respectively. 
\begin{equation}
\begin{split}
\mathcal{L}_{\rm{KTO}}({\theta}; \mathcal{D}) &= -\lambda_w \,\sigma \left( \beta \log \frac{\pi_\theta(\mathbf{y}_w\mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_w\mid \mathbf{x})} - z_{\text{ref}} \right) +  \lambda_l \,\sigma \left( z_{\text{ref}} - \beta \log \frac{\pi_\theta(\mathbf{y}_l\mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_l\mid \mathbf{x})} \right), \\
\text{where} \quad z_{\text{ref}} &= \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[\beta \;\mathrm{KL}\left( \pi_\theta(\mathbf{y}\mid \mathbf{x}) || \pi_{\text{ref}}(\mathbf{y}\mid \mathbf{x}) \right)  \right].
\end{split}
\end{equation}


% \begin{table}[h!]
% \caption{Comparison of state-of-the-art alignment approaches with our \method in terms of hyperparameters, the number of hyperparameters, and the elimination of the reference model.}
% \centering
% \vspace{-0.5em}
% \adjustbox{max width=\textwidth}{
% \begin{tabular}{cccc}
%    \toprule[1.0pt]
% \textbf{Approach}    &  \textbf{ Hyperparameters} & \textbf{\#Hyperparameters}   & \textbf{w/o Reference Model}   \\
% \midrule
% DPO & $\beta$ & $\mathbf{1}$   &  \xmark      \\
% IPO  & $\beta$ & $\mathbf{1}$  &  \xmark    \\
% CPO  & $\lambda, \beta$  & $\mathbf{2}$ & \cmark    \\
% SLiC  & $\delta, \lambda$ & $\mathbf{2}$  &  \cmark    \\
% SimPO  & $\gamma, \beta$ & $\mathbf{2}$  &  \cmark    \\
% KTO & $\lambda_l, \lambda_w, \beta$ & $\mathbf{3}$  &  \xmark   \\
% \midrule
% \method & - & $\mathbf{0}$ &   \cmark   \\
% \toprule[1.0pt]
% \end{tabular}}\label{table:compare_full}
% \end{table}


\subsection{Implementation Details}

\paragraph{Training Hyperparameters.}
For general hyperparameters, we adhered strictly to the settings used in SimPO. 
We applied the following hyperparameters: For the SFT stage, we use a learning rate of $2 \times 10^{-5}$. For both the SFT and the preference optimization stages, we use a batch size of 128, a maximum sequence length of 2048, and a cosine learning rate schedule with 10\% warmup steps for one epoch, all through the Adam optimizer~\citep{kingma2014adam}.
We maintain these settings consistently to ensure uniformity and comparability across experiments.

For method-specific hyperparameters, we also followed the search strategy from SimPO, noting that our method does not require any additional hyperparameters beyond the learning rate. Specifically, for different baseline methods, each with its own set of additional hyperparameters, the search strategy is in \cref{table_baseline_hyper}. 
Each method is individually search for the learning rates in $[3e-7, 5e-7, 6e-7, 1e-6]$.  

\begin{table}[h]
\small
\centering
\caption{Search ranges for hyperparameters across baseline methods}
\begin{tabular}{cc}
\toprule
\textbf{Method} & \textbf{Hyperparameter Search Range} \\
\midrule
DPO & $\beta \in [0.01, 0.05, 0.1]$ \\
\midrule
IPO & $\tau \in [0.01, 0.1, 0.5, 1.0]$ \\
\midrule
\multirow{2}{*}{CPO} 
& $\lambda = 1.0$ \\
& $\beta \in [0.01, 0.05, 0.1]$ \\
\midrule
\multirow{2}{*}{SLiC} 
& $\lambda \in [0.1, 0.5, 1.0, 10.0]$ \\
& $\delta \in [0.1, 0.5, 1.0, 2.0]$ \\
\midrule
\multirow{2}{*}{KTO} 
& $\lambda_l = \lambda_w = 1.0$ \\
& $\beta \in [0.01, 0.05, 0.1]$ \\
\midrule
\multirow{2}{*}{SimPO} 
& $\beta \in [2.0, 2.5]$ \\
& $\gamma \in [0.3, 0.5, 1.0, 1.2, 1.4, 1.6]$ \\
\bottomrule
\end{tabular}
\label{table_baseline_hyper}
\end{table}


\paragraph{Evaluation Hyperparameters.}
The hyperparameters for evaluation on AlpacaEval 2 adhere to SimPO, with a temperature of 0.7 for the Mistral-Base setting, a temperature of 0.5 for the Mistral-Instruct setting, and a temperature of 0.9 for both Llama3 settings. The hyperparameters for evaluation on MT-Bench are set according to the official decoding configuration, which specifies distinct sampling temperatures for various categories.

\paragraph{Computation Resources.} 
All the training experiments in this paper were conducted on 4 $\times$ NVIDIA A100 (80G) GPUs with 128 batchsize, based on the alignment-handbook repo.\footnote{\url{https://github.com/huggingface/alignment-handbook}}


% \section{Additional Experimental Results}


% \begin{figure}[t!]
% \centering 
% \includegraphics[width=1\textwidth]{pic/rewards-appendix.pdf}
% \caption{The training dynamics during training of \method and \texttt{SimPO} with  different hyperparameters on the 4 different base models. We can observe  that \method exhibits the least decline in chosen rewards, while still achieving the most significant increase in reward margins, compared to \texttt{SimPO} across various hyperparameters.}
% \vskip -1em
% \label{fig:app:rewards} 
% \end{figure}

\section{Case Study}
\label{app:evaluation}
% \textcolor{red}{Warning: The following case studies contain example data that may be offensive or harmful.}

In this section, we present examples of comparisons between \method  and the baseline \texttt{SimPO}. 
% See~\cref{tab:case-1} for dialogue examples, and~\ref{tab:case-2} for summarization examples.





\begin{table}[!h]
    \centering
    \ttfamily
    \small
        \begin{tabular}{lp{11.5cm}}
        \toprule[1pt]
        \bf{Prompt} & \texttt{Does the 5 second rule actually exist?}\\
        \midrule
        \bf{SimPER} & 
\texttt{The "5 second rule" is an urban legend that claims that if food falls on the floor, it is still safe to eat as long as you pick it up within 5 seconds. However, this rule is not a scientifically proven fact, and there is no evidence to support it. Here are some reasons why the 5-second rule is not a reliable method for determining food safety:}\newline


\textbf{1. Bacteria and viruses can transfer quickly}: Bacteria and viruses can transfer from surfaces to food within seconds. The amount of time that passes between a food item's contact with a contaminated surface and its being picked up can affect the likelihood of bacterial or viral transmission. \newline

\textbf{2. Contamination levels on floors and surfaces}: The level of contamination on floors and surfaces can vary widely depending on the environment, the cleanliness of the area, and the presence of moisture. Even if a surface is dry, bacteria and viruses can still be present.\newline

\textbf{3. Personal hygiene}: The cleanliness of your hands, the surface you place the food on, and the utensils you use to pick up the food all play a role in food safety. Picking up food within 5 seconds does not ensure that it is clean enough to eat.\newline

\textbf{4. Temperature}: The temperature of food can also affect the risk of bacterial or viral contamination. Food that has been at room temperature for a prolonged time, even if picked up quickly, may still harbor dangerous bacteria.\newline

Therefore, it is best to follow proper food handling and preparation practices, such as washing hands before and after handling food, using clean utensils and surfaces, and cooking food to the appropriate temperatures to ensure its safety. \\
        \midrule
        \bf{SimPO} & 
The "5-second rule" refers to a popular myth or belief that suggests touching food on the ground for less than five seconds makes it safe to consume without contamination. However, this rule does not exist scientifically.
According to food safety guidelines, bacteria can transfer from contaminated surfaces to food within seconds, regardless of the elapsed time. The duration of contact between the food and contaminated surfaces plays a crucial role in determining the likelihood of bacterial transfer, but it's not limited to five seconds.
Cross-contamination occurs when bacteria from unwashed hands, dirty utensils, or contaminated surfaces come into contact with food, leading to potential foodborne illnesses. Therefore, adhering to proper hygiene practices, such as washing hands, cleaning surfaces, and cooking food thoroughly, remains essential to prevent food contamination.
In summary, there is no scientific basis for the "5-second rule," and relying solely on this belief could increase the risk of consuming contaminated food."
        \\
        % \midrule
        % \textbf{Judgment} & \\
        \bottomrule[1pt]
    \end{tabular}
    \caption{An AlpacaEval 2 generation from the Llama-3-8B-Base model trained with \texttt{SimPER} and \texttt{SimPO}. Alpaca-Eval-GPT4-Turbo prefers \method over \texttt{SimPO}. Compared to \texttt{SimPO}, the generation by \texttt{SimPER} is better structured, with hierarchical discussions that make it more helpful. 
    }
    \label{tab:case-1}
\end{table}



\begin{table}[!h]
    \centering
        \ttfamily
        \small
    \begin{tabular}{lp{11.5cm}}
        \toprule[1pt]
        \bf{Prompt} & How do I make escargot? \\
        \midrule
        \bf{SimPER} &  
Here's a general recipe for making escargot: \newline

\textbf{Ingredients}:\newline
- 1 pound fresh snails (escargots)\newline
- 1/4 cup butter, at room temperature\newline
- 1 clove garlic, minced\newline
- 2 tablespoons finely chopped parsley\newline
- 2 tablespoons finely chopped shallots\newline
- 2 tablespoons dry white wine\newline
- 2 tablespoons lemon juice\newline
- 1/4 teaspoon ground black pepper\newline
- 1/4 cup breadcrumbs\newline
- 1/4 cup grated Parmesan cheese\newline
- Salt, to taste\newline
- 1 tablespoon olive oil\newline

\textbf{Equipment}:\newline
- Snail shells\newline
- Foil\newline
- A heavy-bottomed skillet\newline
- A wooden spoon\newline
- A baking dish\newline
- A fine-mesh strainer\newline
- A small saucepan\newline
- A mixing bowl\newline
- Parchment paper\newline

\textbf{Instructions}:\newline
1. Preheat the oven to 375F (190C).

2. Scrub the snails under cool running water to remove any dirt or debris. Discard any snails that are not alive. Soak the snails in salted water for at least 30 minutes, changing the water every 10 minutes, to ensure they are thoroughly cleaned.

3. In a small saucepan, melt the butter over low heat. Add the minced garlic, parsley, and shallots, and cook for 1-2 minutes until fragrant.

4. Remove the garlic mixture from the heat and stir in the white wine, lemon juice, and black pepper. Season with salt to taste.

5. In a mixing bowl, combine the breadcrumbs and grated Parmesan cheese.

6. Place the snail shells on a sheet of foil and arrange them in a baking dish. Spoon the garlic butter mixture into the shells, filling each one about 3\/4 full.

7. Sprinkle the breadcrumb mixture over the top of each snail.

8. Cover the snail shells with foil.

9. Bake in the preheated oven for 12-15 minutes or until the breadcrumbs are golden brown and the snails have cooked through.

10. Remove the dish from the oven, carefully remove the foil, and place a small piece of parchment paper over the top of each shell.

11. Return the dish to the oven and continue baking for 1-2 minutes to crisp up the breadcrumbs.

12. Remove the dish from the oven, and let the escargots cool for a few minutes.

13. In a heavy-bottomed skillet, heat the olive oil over medium-high heat. Strain the garlic butter mixture through a fine-mesh strainer into the skillet, and cook for 1-2 minutes until slightly reduced and thickened.

14. To serve, spoon the garlic butter onto small plates, arrange the escargots on the plates, and drizzle the remaining garlic butter over the top.

Note: Escargot shells are available for purchase at specialty food stores or online. If you don't have escargot shells, you can use mushroom caps or even small ramekins.

Enjoy your delicious escargot! \\
\midrule
\end{tabular}
\end{table}
\begin{table}[!h]
    \centering
        \ttfamily
        \small
    \begin{tabular}{lp{11.5cm}}
        \midrule
        \bf{SimPO} &
Here's a general recipe for making escargot:\newline

\textbf{Ingredients}:\newline
- 1 pound fresh or canned snails (escargots), cleaned and cooked\newline
- 1/2 cup unsalted butter, softened\newline
- 2 cloves garlic, minced\newline
- 1 tablespoon finely chopped fresh parsley\newline
- 1 tablespoon finely chopped fresh thyme\newline
- 1/4 teaspoon red pepper flakes (optional)\newline
- Salt and freshly ground black pepper, to taste\newline
- 24-30 small, hollowed-out garlic cloves or escargot shells\newline

\textbf{Instructions}:

1. Preheat your oven to 450F (230C).

2. Prepare the garlic cloves or escargot shells by gently scraping out the pulp with knife, leaving a thin layer intact. Rinse them under running water and pat dry.

3. In a mixing bowl, combine softened butter, minced garlic, chopped parsley, thyme, red pepper flakes (if using), salt, and black pepper. Mix well until blended.

4. Spoon a generous amount of seasoned butter into each garlic clove or escargot shell, filling them almost to the top.

5. Arrange the filled shells in a baking dish, preferably one with individual wells or indentations, spaced evenly apart.

6. Place the cooked snails into each shell, making sure they fit snugly.

7. Bake the escargot for 8-10 minutes, or until the butter has melted and bubbled around the edges, and the snails are heated through.

8. Remove the dish from the oven and serve immediately, accompanied by crusty bread, toasted baguette slices, or crackers.

9. To eat, use a small fork or escargot spoon to extract the snails from their shells, dip them into the remaining garlic butter, and enjoy!

Note: Always follow proper food handling and preparation guidelines when working with live or cooked snails, especially if using fresh ones. Ensure they are properly cleaned, cooked, and stored according to recommended safety practices. \\
\bottomrule[1pt]
    \end{tabular}
    % \vspace{4mm}
    \caption{An AlpacaEval 2 generation from the Mistral-7B-Base model trained with \texttt{SimPER} and \texttt{SimPO}. Alpaca-Eval-GPT4-Turbo prefers \method over \texttt{SimPO}. Compared to \texttt{SimPO}, the generation by \texttt{SimPER} is better structured, with hierarchical discussions that make it more helpful.}
    \label{tab:case-2}
\end{table}

\begin{table}[!h]
    \centering
        \ttfamily
        \small
    \begin{tabular}{lp{11.5cm}}
        \toprule[1pt]
        \bf{Prompt} & Mick pays his teacher \$800 for 40 lessons worth 2 hours each. If this will be all he is going to pay for his lessons, how much did he receive?\\
        \midrule
        \bf{SimPER} & Mick did not receive any money. The \$800 he paid to his teacher is the total cost for the 40 lessons, which is 40 x 2 hours = 80 total hours of lessons. Since he only paid for the lessons, he will not receive any money back.
        
        Confidence: 95\% \\
        \midrule
        \bf{SimPO} & Mick received \$800.
        
        Confidence: 95\%\\
        \bottomrule[1pt]
    \end{tabular}
    \caption{An AlpacaEval 2 generation from the Llama-3-8B-Base model trained with \texttt{SimPER} and \texttt{SimPO}. Alpaca-Eval-GPT4-Turbo prefers \method over \texttt{SimPO}. Compared to \texttt{SimPO}, \texttt{SimPER}'s output provides clearer step-by-step reasoning.}
    \label{tab:case-3}
\end{table}

