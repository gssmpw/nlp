\section{The Proposed Method}
\label{sec:method}
\subsection{Background}
\textbf{Notations}. We consider the problem of preference fine-tuning: 
Let the text sequences $\mathbf{x} =[ x_1, x_2, \ldots ]$ denote the prompt, and $\mathbf{y}_{w}=[ y_1, y_2, \ldots ]$ and $\mathbf{y}_{l}=[ y_1, y_2, \ldots ]$ denote two responses,  sampled from the reference policy $\pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x})$. The response pairs are then presented to an oracle who express preferences for responses given the prompt, denoted as $\mathbf{y}_{w} \succ \mathbf{y}_{l} \mid \mathbf{x}$, where $\mathbf{y}_{w}$ and $\mathbf{y}_{l}$ denote chosen and rejected responses, respectively. Given dataset $\mathcal{D}$, containing preference $(\mathbf{x}, \mathbf{y}_{w}, \mathbf{y}_{l})$, the goal is to learn a language model policy $\pi_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})$ parameterized by $\boldsymbol{\theta}$ for aligning human preference. 

\textbf{DPO}. DPO~\citep{rafailov2024direct} is one of the most popular offline preference optimization methods. Instead of learning an explicit reward model like RLHF, DPO uses the log-likelihood of the policy to implicitly represent the reward function via a closed-form expression with the optimal policy:
\begin{align}
r(\mathbf{x}, \mathbf{y})=\beta\left(\log \pi_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})-\log \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x})\right)+\beta \log Z(\mathbf{x}).
\end{align}
DPO aims to optimize $\pi_{\boldsymbol{\theta}}$ based on the Bradley-Terry (BT) preference model~\citep{bradley1952rank}, $p\left(\mathbf{y}_w \succ \mathbf{y}_l \mid x\right)=\sigma\left(r(\mathbf{x},\mathbf{y}_w)-r\left(\mathbf{x},\mathbf{y}_l\right)\right)$, and with the following maximum likelihood objective: 
\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts\def\maketag@@@#1{\hbox{\m@th\normalfont\normalfont#1}}
\begin{align}
\mathcal{L}_{\rm{DPO}}({\boldsymbol{\theta}}; \mathcal{D})=\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)\sim \mathcal{D}}\left[-\log \sigma(\beta \log \frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}_w \mid \mathbf{x})}-\beta \log \frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x} )}{\pi_{\mathrm{ref}}(\mathbf{y}_l \mid \mathbf{x})})\right], \label{Eq:DPO}
\end{align}
\endgroup
where $\beta$ is a tunable hyperparameter controlling the deviation from the reference model.  

\textbf{IPO}. The Identity Preference Optimization (IPO)~\citep{azar2024general} also avoids a reward learning process and potentially unstable RL training. Specifically, IPO
chooses to directly minimize the following squared loss regression problems by defining an alternative reward function:
\begin{align}
    \mathcal{L}_{\rm{IPO}}({\boldsymbol{\theta}}; \mathcal{D})=\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)\sim \mathcal{D}}\left[\left (\log \frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid \mathbf{x})\pi_{\mathrm{ref}}(\mathbf{y}_l \mid \mathbf{x})}{\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x} )\pi_{\mathrm{ref}}(\mathbf{y}_w \mid \mathbf{x})}-\frac{1}{2\beta}\right)^{2}\right], \label{Eq:IPO}
\end{align}
where $\beta$ is also a hyperparameter. A potential advantage of IPO over DPO is that these methods don’t assume a specific preference model, like BT, and can work with general preference probabilities.

\textbf{SimPO}. Simple Preference Optimization (SimPO)~\citep{meng2024simpo} has recently been proposed to eliminate the need for a reference model in DPO while achieving promising performance. SimPO optimizes the length-regularized probability of response pairs with a margin based on the BT model:  
\begin{align}
    \mathcal{L}_{\rm{SimPO}}({\boldsymbol{\theta}}; \mathcal{D})=\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)\sim \mathcal{D}}\left[-\log \sigma(\frac{\beta}{|\mathbf{y}_{w}|} \log {\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid \mathbf{x})}-\frac{\beta}{|\mathbf{y}_{l}|} \log {\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x} )}-\gamma) \right], \label{Eq:SimPO}
\end{align}
where $\gamma$ is an additional hyperparameter indicating a target reward margin. In practice, minimizing the above preference fine-tuning objective, or any other contrastive objectives such as those of KTO~\citep{ethayarajh2024kto} and SLiC~\citep{zhao2023slic} (see Appendix~\ref{app:baseline}), requires extensive hyperparameter tuning. These hyperparameters (e.g., $\beta$ and $\gamma$) play a critical role in alignment performance, as shown by~\citep{bai2022training, liudecoding, meng2024simpo} (also see Figure~\ref{fig:motivation}), and need to be manually adjusted to achieve optimal performance, significantly increasing complexity and time cost. In this paper, we address this limitation by proposing a simple yet effective alignment objective, \method, which eliminates the need for a reference model and any tunable hyperparameters required by previous work, making the alignment process on large language model more efficient.


\subsection{The Learning Objective of \method}

In this section, we elaborate on \method. The key idea behind \method is to encourage the model to minimize the perplexity of the chosen response while simultaneously maximizing the perplexity of the rejected response within the preference dataset. Specifically, we optimize the inverse perplexity, which is calculated as the inverse of the exponentiated average negative log-likelihood of the response. The average log-likelihood of the response under the policy model $\pi_{\theta}$ is defined as follows:
\begin{align}
r_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{y})=\log p_{\boldsymbol{\theta}} (\mathbf{y} \mid \mathbf{x})=\frac{1}{|\mathbf{y}|} \log \pi_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})=\frac{1}{|\mathbf{y}|} \sum\nolimits_{i=1}^{|\mathbf{y}|}  \log \pi_{\boldsymbol{\theta}}\left(\mathbf{y}_i \mid \mathbf{x}, \mathbf{y}_{<i}\right), \label{Eq:length}
\end{align}
where $p_{\boldsymbol{\theta}} (\mathbf{y} \mid \mathbf{x})=\pi_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})^{\frac{1}{|\mathbf{y}|}}$ is the defined geometric mean over the sequence of token probabilities. The perplexity is defined as the exponentiated its average negative log-likelihood as:
\begin{align}
 \text{Perplexity}(\mathbf{y} \mid \mathbf{x})=\exp \left(-r_{\boldsymbol{\theta}} \left(\mathbf{y} \mid \mathbf{x}\right)\right)=\exp \left(-\frac{1}{|\mathbf{y}|} \log \pi_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})\right),
\end{align}
and serves as a metric closely tied to the causal language modeling objective, allowing us to assess whether a new input aligns with the model’s knowledge and how it corresponds to the pretraining data~\citep{jelinek1977perplexity,marion2023less,gonen2023demystifying}. Leveraging perplexity as a measurement of how well a language model predicts the response given the prompt,  we formulate the alignment with preference data as an optimization problem that does not require any hyperparameters and a reference model during training. Formally, our \method learning objective is given as follows:
\begin{align}
    \mathcal{L}_{\rm{SimPER}}(\boldsymbol{\theta}; \mathcal{D})&=- \text{Perplexity}^{-1}(\mathbf{y}_{w} \mid \mathbf{x})+\text{Perplexity}^{-1}(\mathbf{y}_{l} \mid \mathbf{x}) \\
    &=-\exp \left(\frac{1}{|\mathbf{y}_{w}|} \log \pi_{\boldsymbol{\theta}}(\mathbf{y}_{w} \mid \mathbf{x})\right)+\exp \left(\frac{1}{|\mathbf{y}_{l}|} \log \pi_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x})\right), \label{Eq:SimPER}
\end{align}
where we directly optimize the reverse perplexity of the chosen and rejected response as the 
reverse perplexity, i.e., the geometric mean over the sequence of token probabilities, effectively quantifies the model confidence as shown in~\citep{valentin2024cost,liu2024litcab}. Intuitively, \method increases the likelihood of the chosen response and decreases the likelihood of rejected response by optimizing the reverse perplexity, effectively aligning the language model with the preference data.


In summary, \method employs a simple yet effective formulation that directly aligns with the perplexity generation metric, eliminating the need for a reference model. We empirically find that \method still achieves a strong performance without requiring any hyperparameters, unlike previous methods.





\subsection{Analysis and Discussion}
In this section, we provide a gradient and divergence analysis to further understand our \method, 

% \textbf{Gradient Analysis.}
% We examine the gradients of \method and the state-of-the-art method \texttt{DPO}~\citep{rafailov2024direct} and \texttt{SimPO}~\citep{meng2024simpo} and to glean some insight into the optimization process. Note that our analysis also holds for other methods such as IPO~\citep{azar2024general} and SLiC~\citep{zhao2023slic}. One advantage of the \method framework is that the gradients of both chosen and rejected responses are more balanced, thus, we can prevent the model from overfitting the rejected responses. We first analyze the following gradients of \texttt{DPO} and \texttt{SimPO}:
% \begin{align}
% \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\rm{DPO}}({\boldsymbol{\theta}}; \mathcal{D}) &= -\beta \mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l) \sim \mathcal{D}} \left[ w_{\boldsymbol{\theta}} \cdot \left( \frac{\nabla_{\boldsymbol{\theta}} \pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})}{\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})} - \frac{\nabla_{\boldsymbol{\theta}} \pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}{\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}  \right) \right] \\
% \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\rm{SimPO}}({\boldsymbol{\theta}}; \mathcal{D}) &= -\beta \mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l) \sim \mathcal{D}} \left[ d_{\boldsymbol{\theta}} \cdot \left( \frac{\nabla_{\boldsymbol{\theta}} p_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})} - \frac{\nabla_{\boldsymbol{\theta}} p_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}  \right) \right], \label{Eq:SimPO-gradient}
% \end{align}
% where  $w_{\boldsymbol{\theta}} = \sigma ( \beta \log \frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}{\pi_{\rm{ref}}(\mathbf{y}_l \mid \mathbf{x})} - \beta \log \frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid \mathbf{x})}{\pi_{\rm{ref}}(\mathbf{y}_w \mid \mathbf{x})})$ and $d_{\boldsymbol{\theta}} = \sigma ( \beta \log {\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})} - \beta \log {\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid \mathbf{x})}+\gamma)$ represent the gradient weight in \texttt{DPO} and \texttt{SimPO}, respectively. $p_{\boldsymbol{\theta}}$ is the geometric mean over the sequence of token probabilities defined in Equation~(\ref{Eq:length}). It can be seen that the gradient of the model probability weighted by the reciprocal of the model probability on this rejected response. Intuitively, both gradients of \texttt{DPO} and \texttt{SimPO} also increases the likelihood of the chosen response $\mathbf{y}_{w}$ and decreases the likelihood of rejected response $\mathbf{y}_{l}$. If the rejection likelihood $\pi_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x}) \rightarrow 0 $ or $p_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x}) \rightarrow 0 $, the norm of the gradient on rejected respones will become very large, which leads to a huge step of parameter update towards decreasing the likelihood of rejected response. 
% % In other words, a lower gradient ratio between chosen and rejected responses
% % results in a swifter alteration in the probability of a rejected response compared to that of a chosen one.  
% In this case, the gradient associated with the rejected response becomes exceedingly large, which explains why \texttt{DPO} and \texttt{SimPO} tend to push the model to decrease the likelihood of chosen responses during training, as shown in Figure~\ref{fig:rewards} and other works~\citep{meng2024simpo,pal2024smaug,pang2024iterative,chen2024noise}. This occurs because rejected and chosen responses often share some tokens, leading to a decline in performance on reasoning-heavy tasks, such as math and coding, as demonstrated in several recent studies~\citep{meng2024simpo,pal2024smaug,pang2024iterative}. 

% For comparison, we calculate the gradient of our \method with respect to $\theta$ using Equation~(\ref{Eq:SimPER}):
% \begin{align}
%     \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\rm{SimPER}}({\theta};\mathcal{D})=-\mathbb{E}_{(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\sim \mathcal{D}}\left[ {\nabla_{\boldsymbol{\theta}} p_{\theta}(\mathbf{y}_w \mid \mathbf{x})} - {\nabla_{\boldsymbol{\theta}} p_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}  \right],
% \end{align}

\textbf{Gradient Analysis.}
We examine the gradients of \method and the state-of-the-art method \texttt{DPO}~\citep{rafailov2024direct} and \texttt{SimPO}~\citep{meng2024simpo} and to glean some insight into the optimization process. Note that our analysis also holds for other methods such as IPO~\citep{azar2024general} and SLiC~\citep{zhao2023slic}. One advantage of the \method framework is that the gradients of both chosen and rejected responses are more balanced, thus, we can prevent the model from overfitting the rejected responses. We first analyze the following gradients of \texttt{DPO} and \texttt{SimPO}:
\begin{align}
\nabla_{\boldsymbol{\theta}} \mathcal{L}_{\rm{DPO}}({\boldsymbol{\theta}}; \mathcal{D}) &= -\beta \mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l) \sim \mathcal{D}} \left[ w_{\boldsymbol{\theta}} \cdot \left( \frac{\nabla_{\boldsymbol{\theta}} \pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})}{\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})} - \frac{\nabla_{\boldsymbol{\theta}} \pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}{\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}  \right) \right] \\
\nabla_{\boldsymbol{\theta}} \mathcal{L}_{\rm{SimPO}}({\boldsymbol{\theta}}; \mathcal{D}) &= -\beta \mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l) \sim \mathcal{D}} \left[ d_{\boldsymbol{\theta}} \cdot \left( \frac{\nabla_{\boldsymbol{\theta}} p_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{y}_w \mid  \mathbf{x})} - \frac{\nabla_{\boldsymbol{\theta}} p_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}  \right) \right], \label{Eq:SimPO-gradient}
\end{align}
where the weights $w_{\boldsymbol{\theta}} = \sigma ( \beta \log \frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}{\pi_{\rm{ref}}(\mathbf{y}_l \mid \mathbf{x})} - \beta \log \frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_w \mid \mathbf{x})}{\pi_{\rm{ref}}(\mathbf{y}_w \mid \mathbf{x})})$ and $d_{\boldsymbol{\theta}} = \sigma ( \frac{\beta}{|\mathbf{y}_{l}|} \log {\pi_{\boldsymbol{\theta}}(\mathbf{y}_l|\mathbf{x})} -\frac{\beta}{|\mathbf{y}_{w}|} \log {\pi_{\boldsymbol{\theta}}(\mathbf{y}_w | \mathbf{x})}+\gamma)$ represent the gradient weight in \texttt{DPO} and \texttt{SimPO}, respectively. $p_{\boldsymbol{\theta}}$ is the geometric mean over the sequence of token probabilities defined in Equation~(\ref{Eq:length}). It can be seen that the gradient of the model probability weighted by the reciprocal of the model probability on this rejected response. 
% Intuitively, both gradients of \texttt{DPO} and \texttt{SimPO} also increases the likelihood of the chosen response $\mathbf{y}_{w}$ and decreases the likelihood of rejected response $\mathbf{y}_{l}$. 
If the rejection likelihood $\pi_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x}) \rightarrow 0 $ or $p_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x}) \rightarrow 0 $, the norm of the gradient on rejected response will be large, which leads to a huge step of parameter update towards decreasing the likelihood of rejected response compared to the increasing of the chosen response. 

For instance, in DPO, the gradient ratio between the decrease in the probability of rejected responses and the increase in the probability of chosen responses is as follows:
$
\frac{\pi_{\boldsymbol{\theta}}(\mathbf{y}_{w} \mid \mathbf{x})}{\pi_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x})} \cdot \frac{\nabla_{\boldsymbol{\theta}} \pi_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x})}{\nabla_{\boldsymbol{\theta}} \pi_{\boldsymbol{\theta}}(\mathbf{y}_{w} \mid \mathbf{x})},
$
which becomes infinite when $\pi_{\boldsymbol{\theta}}(\mathbf{y}_{l} \mid \mathbf{x}) \rightarrow 0$. A larger gradient ratio leads to a faster reduction in the probability of a rejected response compared to that of a chosen response, resulting in a more pronounced decrease for rejected responses,
which explains why \texttt{DPO} and \texttt{SimPO} tend to push the model to decrease the likelihood of both chosen and rejected responses during training, as shown in Figure~\ref{fig:rewards}. This occurs because rejected and chosen responses often share some tokens, leading to a decline in performance on reasoning-heavy tasks, such as math and coding, as demonstrated in several recent studies~\citep{meng2024simpo,pal2024smaug,pang2024iterative,chen2024noise}. For comparison, we calculate the gradient of our \method with respect to $\theta$ using Equation~(\ref{Eq:SimPER}):
\begin{align}
    \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\rm{SimPER}}(\boldsymbol{\theta};\mathcal{D})=-\mathbb{E}_{(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\sim \mathcal{D}}\left[ {\nabla_{\boldsymbol{\theta}} p_{\boldsymbol{\theta}}(\mathbf{y}_w \mid \mathbf{x})} - {\nabla_{\boldsymbol{\theta}} p_{\boldsymbol{\theta}}(\mathbf{y}_l \mid \mathbf{x})}  \right],
\end{align}
Where the gradient ratio between rejected and chosen responses is constant, it has a smaller norm than SimPO (Equation~\ref{Eq:SimPO-gradient}). This means that the unlearning of rejected responses is more conservative, and \method reduces the gradient imbalance issue between chosen and rejected responses. As shown in Figure~\ref{fig:rewards}, \method effectively prevents the likelihood of chosen responses from decreasing significantly compared to \texttt{SimPO}, while still achieving substantial margins between the likelihood of chosen and rejected responses. Our experiments also show that \method significantly outperforms \texttt{SimPO}.


\begin{wrapfigure}[15]{!RT}{.36\linewidth}
\vskip -1em
\includegraphics[width=0.35\textwidth]{pic/tvd.pdf}
\vskip -0.5em
\caption{Illustration of the characteristics of KLD and TVD. While SFT exhibits mass-covering behavior by minimizing forward KL, \method exhibits mode-seeking behavior, similar to RLHF~\citep{tajwar2024preference}, by minimizing TVD.}
\label{fig:tvd} 
\end{wrapfigure}

\begin{figure}[t!]
\centering 
\includegraphics[width=0.92\textwidth]{pic/rewards-mistral.pdf}
\vskip -1em
\caption{The training dynamics during training of \method and \texttt{SimPO} with  different hyperparameters on the Mistral-7B (Results on Llama3-8B can be found in Section~\ref{sec:abl}). We can observe  that \method exhibits the least decline in chosen likelihoods, while still achieving the most significant increase in likelihood margins of rejected and chosen, compared to \texttt{SimPO} across various hyperparameters.}
\vskip -1em
\label{fig:rewards} 
\end{figure}

\textbf{Divergence Analysis.}
We next present a theoretical analysis of \method, demonstrating its key properties that are advantageous for fine-tuning LLMs with preferences. Recent works~\citep{tajwar2024preference,xiao2024Cal,ji2024towards} identify mode-seeking behavior as a crucial property for preference alignment, as it reduces the likelihood of rejected responses. In what follows, we show that  \method also theoretically promotes mode-seeking behavior by optimizing the Total Variation distance (TVD) between the model distribution $\pi_\theta$ and the distribution of chosen response.
% whereas \texttt{SFT} promotes mass-covering behavior by optimizing the Kullback–Leibler divergence (KLD).
For simplicity, we remove the length averaging from \method for the  analysis.

% We  present a theoretical analysis, which shows the benefits of optimizing perplexity and \method has interesting theoretical connections with Total Variation Distance (TVD).  
% % In what follows, we show that  \method ~with calibration loss also theoretically encourages mode-seeking behavior by minimizing an upper bound of the RL objective in Equation~\eqref{Eq:reverse}:
% Next, we proceed to present a theoretical analysis of \texttt{InfoPO}. Our analyses shows that \texttt{InfoPO}  enjoys important properties that are desirable  for fine-tuning LLMs with preferences. In what follows, we show that \texttt{InfoPO} loss  theoretically encourages mode-seeking behavior by minimizing the reverse KL divergence between model distribution $\pi_\theta$ and unknown distribution of chosen response $\pi_{\rm{chosen}}$. 
\begin{theorem}\label{the:mutual}
Minimizing  \texttt{SFT}  with respect to ${\theta}$ is approximately minimizing the KLD between $\pi_\theta$ and the distribution of the chosen response in the preference dataset, while minimizing our \method  is approximately minimizing the TVD.
% \begingroup\makeatletter\def\f@size{9.5}\check@mathfonts\def\maketag@@@#1{\hbox{\m@th\normalfont\normalfont#1}}
% \begin{align}
% \min_{\theta} \mathcal{L}_{\rm{SimPER}}({\theta};\mathcal{D}) &\Rightarrow  \min_{\theta}{D}_{\mathrm{TV}}(\pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x})\|  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  )-{D}_{\mathrm{TV}}(\pi_{\rm{rejected}}(\mathbf{y}\mid\mathbf{x})\|  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x}))   \\ 
% &=\sum\nolimits_{y\in \mathcal{Y}} \left| \pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x}) -  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  \right|- \sum\nolimits_{y\in \mathcal{Y}} \left| \pi_{\rm{rejected}}(\mathbf{y}\mid\mathbf{x}) -  \pi_\mathbf{\theta}(\mathbf{y} \mid \mathbf{x})  \right| \nonumber
% \end{align}
% \endgroup
\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts\def\maketag@@@#1{\hbox{\m@th\normalfont\normalfont#1}}
\begin{align}
\min_{\boldsymbol{\theta}} \mathcal{L}_{\rm{SFT}}&\Rightarrow  \min_{\boldsymbol{\theta}}{\mathrm{KL}}(\pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x})\|  \pi_\mathbf{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})  ) =\sum  \nolimits_{\mathbf{y}\in \mathcal{Y}} \pi_{\rm{chosen}}(\mathbf{y} \mid \mathbf{x})\log \frac{\pi_{\rm{chosen}}(\mathbf{y} \mid \mathbf{x})}{\pi_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})}\\
\min_{\boldsymbol{\theta}} \mathcal{L}_{\rm{SimPER}} &\Rightarrow  \min_{\mathbf{\boldsymbol{\theta}}}{\mathrm{TV}}(\pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x})\|  \pi_\mathbf{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})  ) =\frac{1}{2}\sum\nolimits_{\mathbf{y}\in \mathcal{Y}} \left| \pi_{\rm{chosen}}(\mathbf{y}\mid\mathbf{x}) -  \pi_{\boldsymbol{\theta}}(\mathbf{y} \mid \mathbf{x})  \right| 
\end{align}
\endgroup
\end{theorem}

The proof is provided in Appendix~\ref{app:proof}. This theorem demonstrates that \method asymptotically optimizes the TVD~\citep{van2014probability,jitailoring} between the chosen data distributions and the model distribution. In theory, while \method and \texttt{SFT} aim to discover identical optimal policies, achieving this in practice would require full data coverage and infinite computation. These requirements are not met in practice, and hence, the choice of divergences and the optimization procedure affects
performance.  TVD measures the average absolute difference between $\pi_{\rm{chosen}}$ and $\pi_{\boldsymbol{\theta}}$ in all possible text sequences, and \method learns to properly allocate its probability mass to best represent the main portion of the distribution of the chosen response, while ignoring outliers. This promotes mode-seeking  behavior, which concentrates the probability mass on certain high-reward regions.  In contrast, the KLD in \texttt{SFT} encourages assigning equal probability to all responses in the dataset, leading to an overestimation of the long tail of the target distribution, as illustrated in Figure~\ref{fig:tvd}. 

% This phenomenon also matches the gradient analysis above, where the KLD in \texttt{SimPO} pushes the model to cover all training samples, resulting in an unfocused and rejected distribution. 

In summary, forward KLD encourages all chosen responses in datasets to have equal probability, leading to an overestimation of the long tail of the target distribution,  whereas reverse TVD sharpens the probability mass on certain high-reward regions of chosen response. Thus, alignment commits to generating a certain subset of high-reward responses, which is more effectively realized
by promotes mode-seeking behavior as shown in recent works~\citep{tajwar2024preference,xiao2024Cal}.


% Furthermore, we refer to this theorem to explain the effect of the second term on our objective, which leads to a negative gradient~\citep{tajwar2024preference}. \method explicitly minimizes an objective function by “pushing down" the likelihood of rejected responses, achieved by multiplying the gradient of their likelihood with a negative coefficient. This implies that \method, similar to \texttt{DPO} as shown in~\citep{tajwar2024preference}, explicitly incorporates a form of the “negative gradient" during the learning process. 






% Theorem~\ref{the:mutual} also implies that DPO, RLHF, and  \method ~ asymptotically converge to the same global optimal policy in the limit given a sufficiently large dataset, and a sufficiently large model capacity. Thus, our proposed calibration objective in Equation~\eqref{Eq:cal-obj} is in some respects similar to that of RLHF.  Table~\ref{table:compare} presents a comparison of the different methods in terms of their strengths and weaknesses compared to \method.


% Theorem~\ref{the:MLE} shows that the first contrastive term in \method ~minimizes forward KL divergence similar to MLE but with negative gradients. In practice, minimizing either KL divergence results in learned policies with different properties due to limited data coverage~\cite{murphy2012machine, nachum2016improving}. Specifically, forward KL $\mathbb{D}_{\mathrm{KL}}[\pi^* | \pi_\theta]$ promotes mode-covering behavior, whereas reverse KL $\mathbb{D}_{\mathrm{KL}}[\pi_\theta | \pi^*]$ encourages mode-seeking behavior~\cite{tajwar2024preference, nachum2016improving, agarwal2019learning}. In other words, forward KL encourages all responses in datasets to have equal probability, leading to an overestimation of the long tail of the target distribution, whereas reverse KL sharpens the probability mass on certain high-reward regions. Thus, alignment commits to generating a certain subset of high-reward responses, which is more effectively realized by minimizing the reverse KL, as the RL objective in Equation~\eqref{Eq:reverse} does. This also explains why MLE, which optimizes forward KL, performs worse than RLHF with reverse KL  as shown in~\cite{tajwar2024preference}.
% Specifically, forward KL $\mathcal{D}_{\mathrm{KL}}(\pi_{\rm{chosen}} \| \pi_\mathbf{\theta})$ promotes mass-covering behavior, whereas reverse KL $\mathcal{D}_{\mathrm{KL}}(\pi_\mathbf{\theta} \| \pi_{\rm{chosen}})$ encourages mode-seeking behavior~\citep{tajwar2024preference, nachum2016improving, agarwal2019learning}. In other words, forward KL encourages all responses in datasets to have equal probability, leading to an overestimation of the long tail of the target distribution, whereas reverse KL sharpens the probability mass on certain high-quality regions. Thus, alignment commits to generating a certain subset of high-quality responses, which is achieved more effectively by minimizing the reverse KL~\citep{tajwar2024preference}. In Section~\ref{sec:exp}, we empirically demonstrate the results of optimizing these two divergences in practice, and shows that the
% superiority of the reverse KL divergence, especially on challenging  tasks such as math problem-solving, code generation, and logical reasoning.


% \begin{theorem}\label{the:mutual}
% Minimizing the \textnormal{\method} objective in Equation~\eqref{Eq:gener} with respect to $\pi_{\theta}$ will encourage mode-seeking behavior by minimizing an upper bound of the reverse KL divergence, as RLHF does.
% \begin{linenomath}
% \begin{align}
% \mathcal{L}_{\rm{RL}}(\theta)&=\beta \mathbb{D}_{\mathrm{KL}}\left[ \pi_\theta(\mathbf{y} \mid \mathbf{x}) \| \pi^*(\mathbf{y} \mid \mathbf{x})\right]-\beta \log Z(\mathbf{x}) \leq \beta \mathcal{L}_{\rm{Cal-DPO}}(\theta)-\beta \log Z(\mathbf{x}).
% \end{align}
% \end{linenomath}
% \end{theorem}
% This theorem shows that, unlike DPO and MLE,  \method ~with calibration asymptotically minimizes a reverse KL divergence with respect to the optimal policy and is hence mode-seeking, as RLHF does. The first term, i.e., preference loss in \method ~ corresponds to optimizing the forward KL as shown in Theorem~\ref{the:MLE}. Hence, our proposed calibration loss can be understood as minimizing the gap between forward KL and reverse KL. Theorem~\ref{the:mutual} also implies that DPO, RLHF, and  \method ~ asymptotically converge to the same global optimal policy in the limit given a sufficiently large dataset, and a sufficiently large model capacity. Thus, our proposed calibration objective in Equation~\eqref{Eq:cal-obj} is in some respects similar to that of RLHF.  Table~\ref{table:compare} presents a comparison of the different methods in terms of their strengths and weaknesses compared to \method.




% \begin{align}
% \mathcal{L}=\max_{\theta} \mathbb{E}_{\pi_{\theta}(\mathbf{y}|\mathbf{x})}[\log \pi_{\text{chosen}}(\mathbf{y}|\mathbf{x})-\log \pi_{\text{rejected}}(\mathbf{y}|\mathbf{x})+\log \pi_{\theta}(\mathbf{y}|\mathbf{x})]
% \end{align}

% \begin{align}
% \mathcal{L}=\max_{\theta}\mathbb{E}_{\pi_{\theta}(\mathbf{y}|\mathbf{x})}[\log \pi_{\text{chosen}}(\mathbf{y}|\mathbf{x})-\log \pi_{\text{rejected}}(\mathbf{y}|\mathbf{x})-\log \pi_{\theta}(\mathbf{y}|\mathbf{x})+\log \pi_{\rm{ref}}(\mathbf{y}|\mathbf{x})]
% \end{align}


% \begin{align}
% \mathcal{L}=\max_{\theta}\mathbb{E}_{\pi_{\theta}(\mathbf{y}|\mathbf{x})}[\log \pi_{\text{chosen}}(\mathbf{y}|\mathbf{x})-\log \pi_{\text{ref}}(\mathbf{y}|\mathbf{x})-\log \pi_{\theta}(\mathbf{y}|\mathbf{x})+\log \pi_{\rm{ref}}(\mathbf{y}|\mathbf{x})]
% \end{align}


% \begin{align}
% \mathcal{L}=-\max_{\theta}\mathbb{E}_{\pi_{\theta}(\mathbf{y}|\mathbf{x})}[\log \pi_{\text{rejected}}(\mathbf{y}|\mathbf{x})-\log \pi_{\text{ref}}(\mathbf{y}|\mathbf{x})-\log \pi_{\theta}(\mathbf{y}|\mathbf{x})+\log \pi_{\rm{ref}}(\mathbf{y}|\mathbf{x})]
% \end{align}