\section{Related Work}
Reinforcement Learning from Human Feedback (RLHF) is an effective technique designed to align LLMs with human preferences~\citep{christiano2017deep}. 
The training process for RLHF includes three stages: initial supervised fine-tuning~\citep{zhou2024lima,xialess}, training of the reward model from human preference data~\citep{gao2023scaling,xiao2021general}, and policy optimization using reinforcement learning, notably Proximal Policy Optimization (PPO)~\citep{schulman2017proximal}. 
While RLHF provides significant benefits in various domains, such as instruction-following~\citep{ouyang2022training}, safety alignment~\citep{bai2022training}, and truthfulness enhancement~\citep{tian2023fine}, it requires a more complex training pipeline compared to traditional supervised learning methods. 
% Moreover, recent research~\citep{casperopen} has highlighted several challenges within the RLHF framework, ranging from the preference data collection, the model training to the biased outcomes~\citep{singhal2023long} issues.

% \textcolor{red}{Reinforcement Learning from Human Feedback (RLHF) is highly effective in aligning Large Language Models (LLMs) with human preferences~\cite{ouyang2022training,christiano2017deep}. In RLHF, a reward model is trained from human preference data to map responses to a scalar reward, which aligns a policy using RL algorithms like PPO~\cite{schulman2017proximal}. Although RLHF excels in instruction-following~\cite{ouyang2022training}, safety alignment~\cite{bai2022training}, and summarization~\cite{stiennon2020learning}, it requires a more complex training pipeline than supervised learning.}

% \textcolor{red}{RLHF is a technique that aligns large language
% models with human preferences and values [16 , 97 , 60 , 7]. The classical RLHF pipeline typically
% comprises three phases: supervised fine-tuning [96 , 74 , 31 , 19 , 46 , 23 , 79 , 14 , 82 ], reward model
% training [30 , 58 , 15 , 54, 35, 48], and policy optimization [ 68 , 4]. Proximal Policy Optimization (PPO)
% [ 68 ] is a widely used algorithm in the third stage of RLHF. The RLHF framework is also widely
% applied to various applications, such as mitigating toxicity [ 47 , 3 ], ensuring safety [22 ], enhancing
% helpfulness [ 75 ], serching and navigating the web [59], and improving model reasoning abilities [ 34].
% Recently, [13 ] has highlighted challenges across the whole RLHF pipeline from preference data
% collection to model training. Further research has also demonstrated that RLHF can lead to biased
% outcomes, such as verbose outputs from the model [26, 69, 81]}

Recent literature highlights the inherent complexity of online preference optimization algorithms, driving the exploration of more efficient offline alternatives. A notable advancement is Direct Preference Optimization (DPO)~\citep{rafailov2024direct}, which 
eliminate the need for explicit reward modeling by directly using the likelihood of policy to define an implicit reward fitted  to the preference data. Inspired by DPO, various methods such as IPO~\citep{azar2024general}, KTO~\citep{ethayarajh2024kto}, and others~\citep{yuan2024rrhf,xucontrastive,hong2024orpo,xiao2024Cal,xiao2024leverage} have been proposed. While these approaches are effective, they typically necessitate an extensive search for one or more hyperparameters, as well as the use of a reference model. Recently, SimPO~\citep{meng2024simpo} removed the need for a reference model in DPO, yet introduced two additional hyperparameters: the reward scaling factor and the target reward margin, which require significant manual tuning. Tuning hyperparameters often entails an iterative trial-and-error process, resulting in substantial computational overhead, particularly for large-scale language models. In this paper, we introduce \method, a simple yet effective objective that eliminates the need for costly hyperparameter tuning and a reference model, thus enhancing both learning and memory efficiency in practice. 

% where practitioners sweep over a range of regularization strengths. However, this method is computationally demanding, particularly for large-scale models.

% Recent literature underscores the complexity of online preference optimization algorithms, prompting 更 efficient offline alternatives. 
% A notable advancement in this domain is Direct Preference Optimization (DPO), which implicitly modeling reward rather than building explicit reward model.
% Beyond DPO, the field has diversified to include ranking objectives that 
% including IPO, KTO, ORPO, and SimPO.xx

% While each of these methods works with different loss functions, the idea is to increase the gap between the likelihoods of preferred and dispreferred responses~\cite{tajwar2024preference}.

% To find the optimal balance, practitioners typically use a trial-and-error approach, by sweeping over varying regular- ization strengths. However, this approach is computationally demanding, especially for large models.

Also worth mentioning is a body of work on perplexity~\citep{jelinek1977perplexity} in language modeling. Researchers use perplexity, an evaluation metric aligned with the causal language modeling objective of LLMs, to assess whether a test input falls within the LLM’s expertise and how it relates to the LLM’s pretraining data~\citep{chenlonglora,marion2023less,gonen2023demystifying}. A lower perplexity indicates that the model’s predictions are generally more accurate, while a higher perplexity suggests that the model finds the content more unpredictable. Recent work~\citep{ankner2024perplexed,muennighoff2024scaling} also uses perplexity to identify high-quality subsets of large-scale text datasets that improve performance. Perplexity has also been used to fuse knowledge from multiple models~\citep{mavromatis2024pack}. As perplexity provides a sequence-length normalized expression of the model’s confidence, recent works have utilized the inverse perplexity score to detect hallucinations~\citep{valentin2024cost} and for confidence estimation~\citep{liu2024litcab}. In contrast to these works, we propose a simple yet effective alignment objective based on perplexity, demonstrating that perplexity is also a surprisingly effective indicator for achieving alignment on preference data. 


% Recently, calibration has also been introduced into language models [37, 38]. In these works, calibration refers to the alignment of the model’s assessed confidence scores with the likelihood of its responses being correct. In contrast, our focus is on calibrating the learned implicit rewards in contrastive preference learning so as to match ground-truth rewards in aligning language models with human preferences.

 

% \textcolor{red}{Given that online preference optimization algo-
% rithms are complex and difficult to optimize [95 , 67 ], researchers have been exploring more efficient
% and simpler alternative offline algorithms. Direct Preference Optimization (DPO) [64] is a notable
% example. 
% A variety of preference optimization objectives have been
% proposed besides DPO. Ranking objectives allow for comparisons among more than two instances [ 24,
% 56 , 70, 87 ]. Another line of work explores simpler preference optimization objectives that do not rely
% on a reference model [40 , 85 ], similar to SimPO. [ 8] proposes a method to jointly optimize instructions
% and responses, finding it effectively improves DPO. [ 93 ] focuses on post-training extrapolation
% between the SFT and the aligned model to further enhance model performance. In this work, we
% compare SimPO to a series of offline algorithms, including RRHF [ 87], SLiC-HF [ 92 ], DPO [ 64 ],
% IPO [ 6], CPO [ 84], KTO [ 27 ], ORPO [40], and R-DPO [ 62], and find that SimPO can outperform
% these methods in both efficiency and performance. Recently, [ 73] proposed a generalized preference
% optimization framework that unifies different offline algorithms, and SimPO can be seen as a special
% case of this framework.}
