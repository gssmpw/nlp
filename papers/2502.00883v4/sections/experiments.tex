

\section{Experiments}
\label{sec:exp}
\textbf{Models.} Following~\citep{meng2024simpo}, we perform alignment with several families of open-source models, Llama3-8B (Base and Instruct.)~\citep{dubey2024llama} and Mistral-7B (Base and Instruct.)~\citep{jiang2023mistral}. We also use  Pythia-2.8B~\citep{biderman2023pythia,rafailov2024direct}.

\textbf{Datasets.} For the Llama3-8B-Base and Mistral-7B-Base setups, we follow the same training pipeline as Zephyr~\citep{tunstall2023zephyr} and evaluate \method on the widely used benchmark dataset for preference fine-tuning: the UltraFeedback Binarized dataset~\citep{cui2023ultrafeedback,tunstall2023zephyr}. For the Llama3-8B-Instruct and Mistral-7B-Instruct setups, we evaluate using the on-policy datasets generated by SimPO~\citep{meng2024simpo}. Specifically, we use prompts from  UltraFeedback and regenerates the chosen and rejected response pairs with the SFT models. For each prompt, it generates five responses using the SFT model with sampling. We then use llm-blender/PairRM~\citep{jiang2023llm} to score the five responses, selecting the highest-scoring one as the chosen response and the lowest-scoring one as the rejected response. For Pythia-2.8B, Anthropic-HH dataset~\citep{bai2022training} is used for dialogue generation to produce helpful and harmless responses~\citep{rafailov2024direct}. 


\textbf{Evaluation benchmarks.} Following previous work~\citep{rafailov2024direct,tunstall2023zephyr}, we evaluate methods fine-tuned on the  benchmarks on HuggingFace Open LLM Leaderboard v1 and v2~\citep{eval-harness} and instruction-following benchmarks (AlpacaEval2, MT-Bench). For evaluation on Anthropic-HH, we use GPT-4 for zero-shot pair-wise evaluation,
consistent with human judgments (see prompts in Appendix~\ref{app:task-eval}).  The task and evaluation details are given in Appendix~\ref{app:task-eval}. 

\input{tabs/mtbench}
\input{tabs/leadboard}




\textbf{Baselines.} 
We compare \method with the following preference optimization methods: DPO~\citep{rafailov2024direct}, SLiC~\citep{zhao2023slic},  IPO~\citep{azar2024general}, KTO~\citep{ethayarajh2024kto}, CPO~\citep{xucontrastive} and SimPO~\citep{meng2024simpo}.  As shown in~\citep{meng2024simpo},  hyperparameter tuning is crucial for achieving optimal performance of preference optimization methods. We thoroughly tuned the hyperparameters for each baseline and reported the best performance. More details of baselines and the hyperparameter search space can be found in Appendix~\ref{app:baseline}.  


\begin{wrapfigure}[16]{!RT}{.42\linewidth}
\includegraphics[width=0.42\textwidth]{pic/HH.pdf}
\vskip -1em
\caption{The win rates, computed by GPT-4, in comparison to the chosen responses of test prompts in the Anthropic-HH dataset.}
\label{fig:hh} 
\end{wrapfigure}


\begin{figure}[t!]
\centering 
\includegraphics[width=0.95\textwidth]{pic/rewards-llama.pdf}
\vspace{-1em}
\caption{The training dynamics during training of \method and \texttt{SimPO} with  different hyperparameters on the Llama3-8B-Base. We can observe  that \method exhibits the least decline in chosen likelihoods, while still achieving the most significant increase in likelihood margins of rejected and chosen, compared to \texttt{SimPO} across various hyperparameters, and better performance as shown in Table~\ref{tab:main_res}.}
\vskip -2.em
\label{fig:rewards_llama} 
\end{figure}





\subsection{Main Results on  Benchmarks}
\textbf{Results on Instruction-following Benchmarks}. In Table~\ref{tab:MT-bench}, we report the performance on the widely-used instruction-following benchmarks: MT-Bench and AlpacaEval 2. A notable improvement in the performance of \method on MT-Bench, as well as on AlpacaEval 2, is observed. Notably, the Llama3 fine-tuned by \method surpassed the performance of best baseline by 4.9 to 5.2 points on the AlpacaEval 2 win rate across base and instruct settings, demonstrating that \method, while eliminating the need for hyperparameters and a reference model, still achieves strong performance. Moreover, \method consistently achieves superior performance on LC win rate, demonstrating that it can generate high-quality response without substantially increasing response length. We find that MT-Bench exhibits poor separability across different methods, likely due to the limited scale of its evaluation data and its single-instance scoring protocol. This finding aligns with observations reported in \citep{meng2024simpo,li2024live}. Nevertheless, on MT-Bench, \method can still consistently achieves superior or comparable performance across various models. Additionally, we provide examples generated by both \texttt{SimPO} and \method in Appendix~\ref{app:evaluation}. These examples demonstrate that \method shows strong promise in aligning language models, ensuring that the generated responses are not only high-quality but also better structured and more reasonable.


\textbf{Results on Downstream Tasks}. In Table~\ref{table:metric_comparison}, we present a comparative analysis of performance gains in downstream tasks on the HuggingFace Leadboard, along with a comparative ranking across all tasks. As shown in the table, \method, despite its simplicity, achieves remarkable improvements over SimPO and DPO, particularly on challenging reasoning benchmarks such as IFEval, MMLU-PRO and Math. Our results demonstrate that \method is highly effective in improving performance across various models. Notably, \method outperforms the best baseline by 1.48 to 4.2 points on IFEval across various models. The average improvements over \texttt{SimPO} are especially notable in Mistral-Base and Llama3-Base. \method, despite its simplicity, achieves the best overall performance. These consistent and significant improvements highlight the robustness and effectiveness of \method. In particular, \method outperforms the recent \texttt{SimPO} by 19.48 points in GSM8K and 4.23 points in IFEval on Llama3-Base.  On GSM8K, \method consistently achieves superior performance, though it is occasionally surpassed by CPO on Mistral-Base. We hypothesize that these improvements over SimPO can be attributed to the reduced decrease in the likelihood of chosen responses; As the likelihood of a chosen response decreases, it results in suboptimal performance, particularly in mathematical reasoning and coding tasks, where the chosen responses are very likely ground-truth answers, as also shown in~\citep{pal2024smaug,yuan2024advancing}. These observations suggest the strong potential of \method in real-world applications due to the elimination of hyperparameters and the reference model in the loss function, making it more computationally and memory efficient.


\textbf{Results on Safety Alignment}.The benchmarks used above focus mainly on helpfulness. To further evaluate the effectiveness of \method in safety alignment on Pythia-2.8B following~\citep{rafailov2024direct}, we use the Anthropic-HH dataset. Figure~\ref{fig:hh} shows the win rates computed by GPT-4 over the chosen responses in the test set of Anthropic-HH. Remarkably, \method aligns better with human preferences than SimPO, DPO, and IPO, achieving win rates of approximately 60\% against the chosen responses, indicating that \method shows strong promise in terms of aligning language models and can ensure that the generated responses are not only high-quality but also safe and harmless.

\begin{figure}[t!]
\centering 
\includegraphics[width=1\textwidth]{pic/ppl.pdf}
\vskip -1em
\caption{Analyzing perplexity density and response length correlation on Mistral-7B-Base and Llama-3-8B-Base. \method not only achieves lower perplexity but also does not exploit length bias.}
\vskip -1.5em
\label{fig:logps} 
\end{figure}

\input{tabs/ablation}

\subsection{Ablations and Further Analysis}
\label{sec:abl}
\textbf{Results on Ablation Study}.
In \cref{tab:ablation}, we evaluated the effects of removing length normalization (\texttt{w/o LN}) and incorporating a reference model (\texttt{w/ Ref.}) in \method on the Open LLM Leaderboard. 
Removing length normalization generally leads to a decline in performance across most tasks, except for a slight improvement in the Winograd. 
Incorporating a reference model typically results in a performance decrease. However, it does lead to slight improvements in specific tasks such as IFEval, GPQA, and BBH for the Mistral-7B-Base model, and in MMLU Pro, BBH, and MATH for the Llama3-8B-Base model.
Based on this ablation study, we can conclude that maintaining length normalization and removing the reference model generally enhances the performance of our method.



\textbf{Analysis on Perplexity Density}.
As shown in \cref{fig:logps}, the analysis of perplexity density on the held-out test set indicates that \method consistently achieves a lower perplexity compared to \texttt{SimPO}.  Specifically, on  Mistral-7B-Base, the density peak of \method is reduced by approximately 1 relative to \texttt{SimPO}, and on the Llama-3-8B-Base, it is reduced by approximately 2. These suggest that \method is capable of generating more predictable and coherent responses by directly optimizing the perplexity.

\textbf{Analysis on Response Length Correlation}.
Spearman correlation between response length and log probabilities in \cref{fig:logps} shows that \method exhibits a significantly lower correlation compared to DPO, and is comparable to \texttt{SimPO}, suggesting that \method does not  exploit length bias and generate
longer sequences. In contrast, \method results in a spearman correlation coefficient
similar to the \texttt{SimPO}~\citep{meng2024simpo}. Our results demonstrate that \method, despite its simplicity,  consistently and significantly outperforms existing approaches
without substantially increasing response length.



% \begin{table}[t]
%     \caption{Downstream task evaluation results of tasks on the huggingface open leaderboard. \label{tab:downstream}}
%     \resizebox{\textwidth}{!}{\begin{tabular}{@{}lccccccc@{}}
%     \toprule
%                    & \textbf{MMLU (5)} & \textbf{ARC (25)} & \textbf{HellaSwag (10)} & \textbf{TruthfulQA (0)} & \textbf{Winograd (5)} & \textbf{GSM8K (5)} & \textbf{Average} \\ \midrule
                  
%                    \multicolumn{8}{c}{{\color[HTML]{222222} \textbf{Mistral-Base}}}                                                                            \\ \midrule
%     \textbf{SFT}   & 60.10            & 58.28            & 80.76                  & 40.35                  & 76.40                & 28.13             & 57.34           \\
%     \textbf{RRHF} & 57.41 & 52.13 & 80.16 & 43.73 & 76.64 & 4.78 & 52.48 \\
%     \textbf{SLiC-HF} &  59.24 & 55.38 & 81.15 & 48.36 & 77.35 & 33.74 & 59.20 \\

%     \textbf{DPO}   & 58.48            & 61.26            & 83.59                  & 53.06                  & 76.80                & 21.76             & 59.16           \\
%     \textbf{IPO}   & 60.23            & 60.84            & 83.30                  & 45.44                  & 77.58                & 27.14             & 59.09           \\
%     \textbf{CPO} & 59.39 & 57.00 & 80.75 & 47.07 & 76.48 & 33.06 & 58.96 \\
%     \textbf{KTO}   & 60.90            & 62.37            & 84.88                  & 56.60                  & 77.27                & 38.51             & 63.42           \\
%     \textbf{ORPO}  & 63.20            & 61.01            & 84.09                  & 47.91                  & 78.61                & 42.15             & 62.83           \\
%     \textbf{R-DPO} & 59.58            & 61.35            & 84.29                  & 46.12                  & 76.56                & 18.12             & 57.67           \\
%     \textbf{SimPO} $\gamma=0.3$ $\beta=2.0$ &    & 62.12           &             83.81     & 49.91                  & 77.90                & 23.28             &            \\ 
%     \textbf{SimPO} $\gamma=0.5$ $\beta=2.0$ &             & 62.12            &      83.83             & 49.45                & 77.74               & 23.73             &            \\ 
%     \textbf{SimPO} $\gamma=1.0$ $\beta=2.0$ &             & 62.80            &      83.92             & 48.90                 & 77.43                & 21.99             &            \\ 
%     \textbf{SimPO} $\gamma=1.2$ $\beta=2.0$ &            &    62.12         &      83.74             &       47.34            &     77.11            &     21.76        &            \\ 
%     \textbf{SimPO} $\gamma=1.4$ $\beta=2.0$ &            &    62.20   &              83.78    &       49.60        &   77.11      &     21.15      &       \\ 
%     \textbf{SimPO} $\gamma=1.6$ $\beta=2.0$ & 59.21            & 62.63            & 83.60                  & 50.68                  & 77.27                & 22.21             & 59.27           \\ 
    
%     \midrule
%     \multicolumn{8}{c}{{\color[HTML]{222222} \textbf{Mistral-Instruct}}}                                                                                      \\ \midrule
% \textbf{SFT}   & 60.40            & 63.57            & 84.79                  & 66.81                  & 76.64                & 40.49             & 65.45           \\
% \textbf{RRHF} & 59.75 & 64.42 & 85.54 & 67.98 & 76.64 & 37.76 & 65.35 \\
% \textbf{SLiC-HF} & 60.59 & 59.90 & 84.05 & 65.30 & 76.32 & 39.65 & 64.30 \\
% \textbf{DPO}   & 60.53            & 65.36            & 85.86                  & 66.71                  & 76.80                & 40.33             & 65.93           \\
% \textbf{IPO}   & 60.20            & 63.31            & 84.88                  & 67.36                  & 75.85                & 39.42             & 65.17           \\
% \textbf{CPO} & 60.36 & 63.23 & 84.47 & 67.38 & 76.80 & 38.74 & 65.16 \\
% \textbf{KTO}   & 60.52            & 65.78            & 85.49                  & 68.45                  & 75.93                & 38.82             & 65.83           \\
% \textbf{ORPO}  & 60.43            & 61.43            & 84.32                  & 66.33                  & 76.80                & 36.85             & 64.36           \\
% \textbf{R-DPO} & 60.71            & 66.30            & 86.01                  & 68.22                  & 76.72                & 37.00             & 65.82           \\
% \textbf{SimPO} & 60.53            & 66.89            & 85.95                  & 68.40                  & 76.32                & 35.25             & 65.56           \\  
% % \textbf{SimPO} $\gamma=0.3$ $\beta=2.5$ &   & 0.6485 & 0.8602 & 0.6891 & 0.7648 &      0.3685   &           \\   
% % \textbf{SimPO} $\gamma=0.5$ $\beta=2.5$ &   & 0.6451 &        & 0.6918 & 0.7648 &        0.3457       &           \\   
% % \textbf{SimPO} $\gamma=1.0$ $\beta=2.5$ &   & 0.6502 & 0.8598 & 0.6921 & 0.7632 &        0.3093       &           \\    
% % \textbf{SimPO} $\gamma=1.2$ $\beta=2.5$ &   & 0.6493 & 0.8607 & 0.6929 & 0.7672 &        0.2638       &           \\    
% % \textbf{SimPO} $\gamma=1.4$ $\beta=2.5$ &   & 0.6570 & 0.8622 & 0.6972 & 0.7680 &        0.2161       &           \\    
% % \textbf{SimPO} $\gamma=1.6$ $\beta=2.5$ &   & 0.6553 & 0.8626 & 0.6945 & 0.7703 &        0.1638       &           \\   \\
% \textbf{SimPO} $\gamma=0.3$ $\beta=2.5$ &   & 0.6451  & 0.8594 & 0.6915 & 0.7664 &   0.3662   &           \\   
% \textbf{SimPO} $\gamma=0.5$ $\beta=2.5$ &   & 0.6459 & 0.8595 & 0.6906 & 0.7648 &   0.3563   &           \\  
% \textbf{SimPO} $\gamma=1.0$ $\beta=2.5$ &   & 0.6519 & 0.8599 & 0.6922 & 0.7624 &  0.3245    &           \\  
% \textbf{SimPO} $\gamma=1.2$ $\beta=2.5$ &   & 0.6544 & 0.8602 & 0.6937  & 0.7640 & 0.3093 &           \\  
% \textbf{SimPO} $\gamma=1.4$ $\beta=2.5$ &   & 0.6510 & 0.8606 & 0.6955 & 0.7664 &  0.2760    &           \\  
% \textbf{SimPO} $\gamma=1.6$ $\beta=2.5$ &   & 0.6476 & 0.8612 & 0.6907  & 0.7664 &   0.2365   &           \\  

% \midrule
% \multicolumn{8}{c}{{\color[HTML]{222222} \textbf{Llama3-Base}}}                                                                                           \\ 
% \midrule
% \textbf{SFT}   & 64.88            & 60.15            & 81.37                  & 45.33                  & 75.77                & 46.32             & 62.30           \\
% \textbf{RRHF} & 64.71 & 62.12 & 82.03 & 55.01 & 77.51 & 44.28 & 64.27 \\
% \textbf{SLiC-HF} & 64.36 & 61.43 & 81.88 & 54.95 & 77.27 & 48.82 & 64.79 \\
% \textbf{DPO}   & 64.31            & 64.42            & 83.87                  & 53.48                  & 76.32                & 38.67             & 63.51           \\
% \textbf{IPO}   & 64.40            & 62.88            & 80.46                  & 54.20                  & 72.22                & 22.67             & 59.47           \\
% \textbf{CPO} & 64.98 & 61.69 & 82.03 & 54.29 & 76.16 & 46.93 & 64.35 \\
% \textbf{KTO}   & 64.42            & 63.14            & 83.55                  & 55.76                  & 76.09                & 38.97             & 63.65           \\
% \textbf{ORPO}  & 64.44            & 61.69            & 82.24                  & 56.11                  & 77.51                & 50.04             & 65.34           \\
% \textbf{R-DPO} & 64.19            & 64.59            & 83.90                  & 53.41                  & 75.93                & 39.27             & 63.55           \\
% \textbf{SimPO} $\gamma=0.3$ $\beta=2.0$ &          &      0.6425       &            82.90       &        59.27           &       77.58         &      36.85      &           \\   
% \textbf{SimPO} $\gamma=0.5$ $\beta=2.0$ &            &     64.76       &          82.86       &          59.83         &     77.82       &  36.62          &          \\   
% \textbf{SimPO} $\gamma=1.0$ $\beta=2.0$ & 64.00            & 65.19            & 83.09                  & 59.46                  & 77.19                & 31.54             & 63.41           \\   
% \textbf{SimPO} $\gamma=1.2$ $\beta=2.0$ &             &    64.59       &           83.10        &         59.20          &      76.64          &     33.66        &            \\   
% \textbf{SimPO} $\gamma=1.4$ $\beta=2.0$ &             &     64.51        &    83.11              &       59.46         &     77.03          &      32.90      &          \\   
% \textbf{SimPO} $\gamma=1.6$ $\beta=2.0$ &             &     65.19        &            83.08     &       59.76         &       76.48        &    29.26        &          \\  
% \midrule 
% \multicolumn{8}{c}{{\color[HTML]{222222} \textbf{Llama3-Instruct}}}                                                                                       \\ \midrule
% \textbf{SFT}   & 67.06            & 61.01            & 78.57                  & 51.66                  & 74.35                & 68.69             & 66.89           \\
% \textbf{RRHF} & 67.20 & 61.52 & 79.54 & 53.76 & 74.19 & 66.11 & 67.05 \\
% \textbf{SLiC-HF} & 66.41 & 61.26 & 78.80 & 53.23 & 76.16 & 66.57 & 67.07 \\
% \textbf{DPO}   & 66.88            & 63.99            & 80.78                  & 59.01                  & 74.66                & 49.81             & 65.86           \\
% \textbf{IPO}   & 66.52            & 61.95            & 77.90                  & 54.64                  & 73.09                & 58.23             & 65.39           \\
% \textbf{CPO} & 67.05 & 62.29 & 78.73 & 54.01 & 73.72 & 67.40 & 67.20 \\
% \textbf{KTO}   & 66.38            & 63.57            & 79.51                  & 58.15                  & 73.40                & 57.01             & 66.34           \\
% \textbf{ORPO}  & 66.41            & 61.01            & 79.38                  & 54.37                  & 75.77                & 64.59             & 66.92           \\
% \textbf{R-DPO} & 66.74            & 64.33            & 80.97                  & 60.32                  & 74.82                & 43.90             & 65.18           \\
% \textbf{SimPO} $\gamma=1.375$ $\beta=2.5 *$ & 65.63            & 62.80            & 78.33                  & 60.70                  & 73.32                & 50.72             & 65.25           
% \\ 
% \textbf{SimPO} $\gamma=0.3$ $\beta=2.5$ &   &  &  & &  &        &       \\   
% \textbf{SimPO} $\gamma=0.6$ $\beta=2.5$ &   &  &  & &  &        &       \\  
% \textbf{SimPO} $\gamma=1.0$ $\beta=2.5$ &   &  &  & &  &        &       \\  
% \textbf{SimPO} $\gamma=1.2$ $\beta=2.5$ &   &  &  & &  &        &       \\  
% \textbf{SimPO} $\gamma=1.4$ $\beta=2.5$ &   &  &  & &  &        &       \\  
% \textbf{SimPO} $\gamma=1.6$ $\beta=2.5$ &   &  &  & &  &        &       \\  


% \bottomrule
%     \end{tabular}}
% \end{table}




