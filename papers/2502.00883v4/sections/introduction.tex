\section{Introduction}
Learning from preference data plays a crucial role in fine-tuning large language models to ensure that pretrained LLMs are aligned with human or societal values and preferences~\citep{bai2022training,ouyang2022training,stiennon2020learning}. In recent years, reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training,christiano2017deep} has been proposed for fine-tuning language models based on human preferences. In the RLHF pipeline~\citep{ouyang2022training}, a reward model is first fit to a dataset of human preferences in the form of a classifier between chosen and rejected responses. Next, an LLM policy is trained using RL algorithms such as proximal policy optimization (PPO)~\citep{schulman2017proximal} to generate responses given the input prompts with high reward.

While RLHF produces models with impressive capabilities across diverse tasks, ranging from programming to creative writing, it introduces notable complexities into the training process~\citep{engstrom2020implementation,rafailov2024direct}, involving inefficient and unstable optimization, as well as training on separate reward and policy models. This potentially worsens the sample complexity and compromises efficient convergence. To address these issues, offline  preference fine-tuning~\citep{tajwar2024preference} methods, such as DPO~\citep{rafailov2024direct}, IPO~\citep{azar2024general}, and KTO~\citep{DBLP:conf/icml/EthayarajhXMJK24}, have been proposed to replace RLHF with supervised learning on human preference data. More recently, SimPO~\citep{meng2024simpo} eliminates the need for a reference model, making DPO more compute and memory efficient. These methods eliminate the need for explicit reward modeling by directly using the \textit{likelihood} of language model policy to define a \textit{implicit reward} fitted to the preference data, while achieving notable competitive performance~\citep{tajwar2024preference}.


\begin{wraptable}[12]{RT}{.53\linewidth}
\caption{State-of-the-art preference fine-tuning losses are compared in terms of properties: hyperparameters, the number of hyperparameters, and the elimination of the reference model (see Appendix~\ref{app:baseline} for details of these methods and hyperparameters).}
\centering
\vspace{-0.5em}
\adjustbox{max width=0.52\textwidth}{
\begin{tabular}{cccc}
   \toprule[1.0pt]
\textbf{Method}    &  \textbf{ Hyperparameters} & \textbf{\#Hyperparameters}   & \textbf{w/o Reference Model}   \\
\midrule
DPO & $\boldsymbol{\beta}$ & $\boldsymbol{1}$   &  \xmark      \\
IPO  & $\boldsymbol{\beta}$ & $\boldsymbol{1}$  &  \xmark    \\
KTO & $\boldsymbol{\lambda}_l, \boldsymbol{\lambda}_w, \boldsymbol{\beta}$ & $\boldsymbol{3}$  &  \xmark   \\
CPO  & $\boldsymbol{\lambda}, \boldsymbol{\beta}$  & $\boldsymbol{2}$ & \cmark    \\
SLiC  & $\boldsymbol{\delta}, \boldsymbol{\lambda}$ & $\boldsymbol{2}$  &  \cmark    \\
SimPO  & $\boldsymbol{\gamma}, \boldsymbol{\beta}$ & $\boldsymbol{2}$  &  \cmark    \\
\midrule
\method & - & $\boldsymbol{0}$ &   \cmark   \\
\toprule[1.0pt]
\end{tabular}}\label{table:compare}
\end{wraptable}
However, these methods require additional hyperparameters that must be carefully tuned as shown in Table~\ref{table:compare}, and the performance of current preference optimization methods, such as DPO, KTO, IPO, and others, is highly sensitive to these hyperparameters across different LLMs, as already shown by~\citep{huggingface2023preftuning,liudecoding,meng2024simpo,liu2024understanding,wu2024beta}. In Figure~\ref{fig:motivation}, we also show that tuning hyperparameters is also crucial to achieve optimal performance with the recent state-of-the-art algorithm \texttt{SimPO}, which eliminates the need for a reference model. This challenge largely prevents us from aligning large language models in  real-world applications, given that a single post-training  process for alignment is usually very expensive and takes a long time~\citep{dubey2024llama}.
To this end, we ask an important research question for large language model alignment: \textit{Can we design an efficient and effective hyperparameter-free preference optimization method for alignment?}


\begin{figure}[t!]
% \small
% \vspace{-mm}
\centering 
\includegraphics[width=1\textwidth]{pic/gamma.pdf}
\vskip -1em
\caption{Evaluation on the MT-Bench Score (1-10) of \texttt{SimPO} and our \method across different large language models reveals the high sensitivity and instability of \texttt{SimPO} with respect to its hyperparameter $\gamma$ across models. In contrast, our \method, which operates without any hyperparameters in the objective function, consistently and significantly outperforms \texttt{SimPO} across a wide range of models. Additional experimental evidence on other widely used benchmarks is provided in Section~\ref{sec:exp}.}
\vspace{-8mm}
\label{fig:motivation} 
\end{figure}
In this paper, we answer this question affirmatively. We propose \method, a simple yet effective offline preference optimization objective that eliminates the need for a reference model and any tunable hyperparameters. The key to \method is directly optimizing the reverse perplexity of chosen and rejected responses within the preference dataset. Perplexity~\citep{jelinek1977perplexity} is a well-known evaluation metric for language modeling, commonly used to assess a model’s ability to process long text. It is calculated as the inverse of the exponentiated average log-likelihood of the responses. \method achieves alignment by solving a reverse perplexity optimization problem, minimizing perplexity over the chosen response while maximizing perplexity over the rejected response, enabling the model to better align with human preferences. Our simple \method validates that perplexity is also an effective optimization indicator for LLM alignment.

% In this paper, we answer this question affirmatively for the first time. We propose \method, a simple yet effective offline preference optimization objective, which eliminates the need for reference model and any tunable hyperparameters required by previous works. The key of \method is directly optimizing the reverse perplexity of the chosen and rejected responses. Perplexity~\citep{jelinek1977perplexity} is a well-known evaluation metric for language modeling, used to demonstrate a model’s ability to process long text. It is calculated as the inverse of the exponentiated average log-likelihood of the responses. \method performs alignment by solving a reverse perplexity optimization problem, minimizing the perplexity over the chosen response while also maximizing the perplexity over the rejected response within the preference dataset, so that the model learns how to align the user preference. 
% Our simple \method, for the first time, validates that perplexity is also a good optimizaiton indicator for achieving large langug model alignment with preference data. 

% Specifically, we minimize the reverse perplexity of the chosen responses while maximize that of the rejected ones within the preference dataset, leading to an ideal situation where the post-training of preference aligns closely with the pretraining.
Moreover, our further analysis proves that, unlike optimizing Kullback-Leibler divergence (KLD) in \texttt{SimPO}, our algorithm effectively minimizes the Total Variation distance (TVD). From a gradient perspective, the robust nature of TVD balances gradients from positive and negative responses, which ensures that the contribution from negative samples does not overshadow those from its positive counterpart, thereby mitigating the issue of decreasing the likelihood of chosen response during preference optimization~ as noticed by recent works~\citep{meng2024simpo,pal2024smaug}.

We empirically demonstrate that \method enjoys promising performance on extensive benchmarks such as the Open LLM Leaderboard~\citep{beeching2023open}, MT-Bench~\citep{zheng2023judging}, and AlpacaEval 2~\citep{li2023alpacaeval}. Despite its simplicity, our results show that \method consistently and significantly outperforms existing approaches across various large language models, without the need for any hyperparameters and a reference model in the objective function for alignment.

% The surprising effectiveness of \method shows that in the context of alignment, simpler methods have been underexplored.