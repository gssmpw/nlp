\begin{table}[]
    \centering
    \begin{tabular}{p{7.5cm}}
    \toprule
    \textbf{Document:}
    \small ... In 2020, the wafer manufacturing materials market was \$34.9 billion. The cost of polishing materials accounted for 7\%. Over the past 20 years, the global semiconductor materials market has surged between 2001 and 2020, with a relatively even distribution among the major global economies. .... \\
    \textbf{Question:} \small Given that in 2020, the proportion of wafer manufacturing materials in the overall scale of semiconductor materials increased by 5\% … Can we
calculate the proportion of the polishing materials market in the semiconductor materials market in City X in 2019? ...\\
   \textbf{Reasoning Chain:} 
   \small \textcolor{red}{[Step1]~}\textcolor{cyan}{In 2020, the wafer manufacturing materials market was \$34.9 billion. The cost of polishing materials accounted for 7\%.}
   The market size was \colorbox{cyan}{34.9} × \colorbox{cyan}{0.07} = \$2.443 billion.
   \textcolor{red}{[Step2]~}\textcolor{cyan}{In 2020, the market size of wafer manufacturing materials ..., an increase of 6.5\% compared to 2019. The cost of polishing materials accounted for 4.41\% of the total cost of semiconductor materials.}
   The overall semiconductor materials market was \colorbox{yellow}{2.443} ÷ \colorbox{cyan}{0.0441} = \$55.3968 billion.
\textcolor{red}{[Step3]~}\textcolor{green}{The polishing materials market remained unchanged}, the polishing materials market in 2019 was also \$2.443 billion, and it was all produced in City X.
\textcolor{red}{[Step4]~}The wafer manufacturing market in 2019 was \colorbox{cyan}{34.9} ÷ \colorbox{cyan}{1.065} = \$32.77 billion.
\textcolor{red}{[Step5]~}The proportion of wafer manufacturing in semiconductor materials in 2020 was \colorbox{cyan}{34.9} ÷ \colorbox{yellow}{55.3968} = 63\%, \textcolor{red}{[Step6]~}so this proportion was \colorbox{yellow}{63\%} - \colorbox{green}{5\%} = 58\% in 2019.
\textcolor{red}{[Step7]~}\textcolor{cyan}{City X had the largest share, reaching 22\% in 2019.}
\textcolor{red}{[Step8]~}The semiconductor market was \colorbox{yellow}{32.77} ÷ \colorbox{yellow}{0.58} = \$56.5 billion, of which City X‘s semiconductor market is \colorbox{yellow}{56.5} * \colorbox{cyan}{22\%} = \$12.43 billion.
\textcolor{red}{[Step9]~}In City X in 2019, the polishing materials market was \$2.443 billion, and the semiconductor market was \$12.43 billion, with a ratio of \colorbox{yellow}{2.443} / \colorbox{yellow}{12.43} = 19.65\%.
\\
\textbf{Answer:} \small The ratio is 19.65\%.\\
\textbf{Checklist:} \small  Calculation error is allowed, so an answer between 18\% and 21\% is considered correct. 
The response must include that the polishing market in 2019 was (about) 2.4 billion, and the semiconductor market in 2019 was (about) 56.5 billion. \\
 \bottomrule
    \end{tabular}
    \caption{An example of question, answer, checklist, reasoning chain pairs, translated to English. The reasoning chain is utilized for human validation.
    Related paragraphs in documents are in \textcolor{cyan}{cyan}, and related information in question is in \textcolor{green}{green}.
    Statistics highlighted in \colorbox{yellow}{yellow} are inferred from existing steps, those in \colorbox{cyan}{cyan} are retrieved from documents, and those in \colorbox{green}{green} are retrieved from the question.}
    \label{tab:sample}
\end{table}

The rapid evolution of large language models (LLMs) \cite{openai_o1preview, claude35, deepseekai2025deepseekr1incentivizingreasoningcapability} has demonstrated unprecedented capabilities in long-context processing and complex reasoning. 
These advancements extend the boundaries of machine intelligence and reignite discussions about achieving Artificial General Intelligence (AGI). 
However, current technical reports of these LLMs predominantly focus on structured mathematical problem-solving \cite{MATH,lightman2023lets,AIME} and coding tasks \cite{LiveCodeBench}, creating a significant disconnect between benchmark performance and real-world reasoning requirements. 
A critical gap in assessing models' capacity for extended logical chaining with implicit connections remains understudied.


We focus on the design of long-context reasoning benchmarks based on the following issues in real-world scenarios. 
The parametric knowledge encapsulated during training inevitably suffers from temporal limitations. 
However, practical scenarios frequently demand analysis of emerging concepts or private documents that never existed in the training corpus. 
Long-context reasoning capability enables LLMs to assimilate such through contextual understanding while performing coherent reasoning.
Moreover, this capability exhibits essential domain adaptability as documents can come from various domains.

Existing long-context benchmarks mostly emphasize retrieval capabilities over genuine reasoning \cite{BAMBOO, RULER}. 
Typical evaluation paradigms exhibit three fundamental limitations: 1) Oversimplified reasoning where solutions require single-step evidence retrieval \cite{Vodrahalli2024MichelangeloLC, DOCBENCH} or shallow logical operations \cite{Vodrahalli2024MichelangeloLC, DOCBENCH, INFINITEBENCH, HELMET, DocFinQA}; 
2) Format-driven evaluation that prioritizes evaluation convenience, restricting response formats to multiple-choice or true-false questions, introducing significant guessing biases\cite{LongBench2, NoCha}; 
and 3) Domain monotonicity focusing on narrow verticals like literature analysis \cite{NoCha, DetectiveQA} while neglecting the generalization ability. 
These limitations undermine the discriminative power of existing evaluations, particularly for cutting-edge models approaching human-level performance on traditional benchmarks.



To address these gaps, we present DocPuzzle, a Chinese long-context reasoning benchmark consisting of 100 QA tasks over long real-world documents.
DocPuzzle is characterized by the following features: 
(1)~\textbf{Multiple realistic domains}. Documents in DocPuzzle are sourced from five diverse domains including academic papers, financial reports, etc. 
(2)~\textbf{Challenging reasoning}. Questions in DocPuzzle necessitate multi-step reasoning operations including arithmetic reasoning, temporal reasoning, etc. We also employ human-AI collaborative validation to ensure quality and challenge level for state-of-the-art LLMs.
(3)~\textbf{Process-aware evaluation}. A sample in DocPuzzle consists of a document, question, answer, and checklist. The checklist will be used during evaluation to check if the reasoning process is correct, thus mitigating the guessing bias of LLMs. An example is shown in Table \ref{tab:sample}).






Our contributions can be summarized as follows:
\begin{itemize}
    \item  \textbf{DocPuzzle Benchmark}:  A human-annotated Chinese dataset featuring 100 multi-domain cases with verification mechanisms. 
    % \footnote{The dataset will be publicly available soon}
    Each case integrates knowledge grounding, evidence chaining, and reasoning requirements.
    \item \textbf{Process-Aware Evaluation Framework}: A novel checklist-based assessment system that decouples reasoning validity from final answer correctness. 
    To the best of our knowledge, we are the first to offer robust, systematic model-based evaluations for the reasoning process without constraining response formats.
    \item \textbf{Competency Gap Analysis}: Systematic profiling of reasoning capabilities of LLMs.
    Our analysis reveals the reasoning capability of the models widely used in the research community.
\end{itemize}


