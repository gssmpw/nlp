\begin{table*}[]
    \centering
    \begin{tabular}{clccc}
   \toprule  
 Model Type  & Model  & Score & Total & Accuracy(\%) \\
\hline
\multirow{3}{*}{Reasoning Model}   & o1-preview   & 69.7 & 100 & \underline{69.7} \\
 & DeepSeek-R1 & 66.3 & 100 & 66.3 \\
  & DeepSeek-R1-Distill-Qwen-32B & 39.7 & 100 & 39.7 \\
 & QwQ-32B-Preview  & 41.3 & 100 & 41.3 \\
 \hline
 \multirow{15}{*}{Instruct Model}  & GPT-4o & 41 & 99 & 41.3 \\
  &  \ \ \  $_{+\  COT\ prompt}$ & 43.7 & 100 & 43.7(+2.4) \\
   & Claude 3.5 Sonnet & 48.3 & 100 & \underline{48.3} \\
  & \ \ \  $_{+\  COT\ prompt}$ & 57.7 & 100 & \underline{57.7(+9.4)} \\
   &  DeepSeek-V3 & 41.7 & 100 & 41.7 \\
 & \ \ \ $_{+\  COT\ prompt}$ & 46 & 100 & 46(+4.3) \\
&  Qwen2.5-72B-Instruct & 39.7 & 100 & 39.7 \\
 & \ \ \ $_{+\  COT\ prompt}$ & 45 & 100 & 45(+5.3) \\
 & Qwen2.5-32B-Instruct & 32.7& 100&32.7 \\
& \ \ \ $_{+\  COT\ prompt}$ & 36.3&100&36.3(+3.6) \\
& Qwen2.5-14B-Instruct & 27 & 100 & 27\\
& \ \ \ $_{+\  COT\ prompt}$ &27.3 &100&27.3(+0.3) \\
&  Qwen2.5-7B-Instruct & 20.3 &100 &20.3 \\
& \ \ \ $_{+\  COT\ prompt}$ &17.3&100 & 17.3(-3) \\
 & moonshot-v1-128k & 28 & 97 & 28.9 \\ 
   & \ \ \ $_{+\  COT\ prompt}$ & 25 & 97 & 25.7(-3.2)\\
 \bottomrule
    \end{tabular}
    \caption{Main results of different LLMs on DocPuzzle. Best scores are \underline{underlined}}
    \label{tab:main_results}
\end{table*}


\subsection{Baselines}
We conduct comprehensive evaluations across two model categories:

\paragraph{Slow-Thinking Reasoning Models} We evaluate the following four slow-thinking reasoning models: 
o1-preview \cite{openai_o1preview}, DeepSeek-R1\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, DeepSeek-R1-Distill-Qwen-32B\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, QwQ-32B-Preview\cite{qwq-32b-preview}.
For DeepSeek-R1, the response consists of reasoning content and the response content, so we take both into account\footnote{For more details, see \url{https://api-docs.deepseek.com/guides/reasoning_model}}.
For DeepSeek-R1-Distill-Qwen-32B, we set temperature to 0.3, top\_p to 0.2, and top\_k to 0.

\paragraph{Instruct Models} We evaluate the following instruct models: GPT-4o\cite{openai_gpt4o}, Claude 3.5 Sonnet \cite{claude35}, DeepSeek-V3\cite{deepseekai2024deepseekv3technicalreport}, Qwen2.5-72B-Instruct\cite{qwen2.5} and moonshot-v1-128k \textsuperscript{\ref{footnote:kimi}}.
We do not evaluate the performance of the LLaMA series, as questions in our benchmark are mostly Chinese.
For instruct models, we also evaluate the performance when employing the powerful zero-shot COT prompt, "Let's think step-by-step" \cite{kojima2023largelanguagemodelszeroshot}.
\paragraph{Setups} We use GPT-4o\textsuperscript{\ref{footnote:4o}} as the judge model.
As demonstrated in Section \ref{subsec:eval_method}, we utilize majority voting across three prompt variants.
Furthermore, to mitigate biases, we run the evaluation scripts three times, and the score is the average of the three results.


\subsection{Main Results}
Due to factors like risk control, not all requests may get valid responses.
We present the main results in Table \ref{tab:main_results}.
\paragraph{Reasoning Capability Ranking}
Slow-thinking reasoning models like o1-preview and DeepSeek-R1 significantly outperform other models on the benchmark.
DocPuzzle shows consistent rankings for the advancing models with other reasoning benchmarks, demonstrating the reasoning nature of DocPuzzle.
While most instruction models underperform specialized slow-thinking reasoning models, Claude 3.5 Sonnet and DeepSeek-V3 demonstrate exceptional reasoning capability, surpassing the Qwen-based models.


\paragraph{Scaling Law Manifestation} 
Model capability exhibits a strong correlation with reasoning ability and domain knowledge. 
Qwen2.5 series shows consistent accuracy gains from 7B (20.3\%) to 72B (39.7\%) variants.

\paragraph{Contrastive Prompt Effects}
While zero-shot CoT prompting \cite{kojima2023largelanguagemodelszeroshot} generally improves performance, effectiveness diminishes for models with limited reasoning performance. 
CoT prompting yields maximum gains for Claude 3.5 Sonnet and Qwen2.5-72B-Instruct.
Performance degradation is observed in Qwen2.5-7B-Instruct and moonshot-v1.
CoT effectiveness emerges when instruct models achieve a score over 32.7\%, indicating minimum model capacity requirements for reasoning path utilization.


