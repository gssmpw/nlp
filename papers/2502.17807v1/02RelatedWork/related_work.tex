\setlength{\tabcolsep}{3pt}
\begin{table*}[]
    \centering
\begin{tabular}{lcccccc}
\toprule  
 & \centering \makecell{\small Long-Context \\ \small Based} & \makecell{\small Realistic Tasks\\ \small Included}  & \makecell{\small Challenging\\ \small Reasoning} & \makecell{\small Free-form\\ \small Answer} & \makecell{\small Controversy\\ \small Review } & \makecell{\small Process-aware\\\small Evaluation} \\
\midrule
AIME\textsuperscript{\ref{footnote:AIME}}&  \xmark & -- & \cmark & \cmark & -- & \xmark\\
HLE\cite{HLE} & \xmark & -- & \cmark & \cmark & \cmark & \xmark\\
Ruler\cite{RULER} & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark\\
% LongBench & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark\\
InfiniteBench\cite{INFINITEBENCH} & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark\\
HELMET\cite{HELMET} & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark\\
BABILong\cite{BABILong} & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark\\
LongBench v2\cite{LongBench2} & \cmark & \cmark & \cmark & \xmark & \cmark & \xmark\\
\midrule
DocPuzzle(Ours)& \cmark & \cmark & \cmark & \cmark & \cmark & \cmark\\
\bottomrule
\end{tabular}
    \caption{Comparison of long-context benchmarks and reasoning benchmarks.  The columns are defined as follows:
(1)~Long-Context Based: Includes a lengthy context to answer upon; 
(2)~Realistic Tasks: Simulates real-world applications (e.g., document QA), excluding synthetic tasks; 
(3)~Challenging Reasoning: Requires deliberate multi-step reasoning that tests the slow-thinking capabilities of LLMs; 
(4)~Free-form Answer: Includes tasks whose answers are not predefined choices; (5)~Controversy Review: Incorporate independent reviewers in the annotation pipeline to identify controversial answers; (6)~Process-aware Evaluation: Assesses intermediate steps rather than solely final outputs to prevent random guesses.  }
    \label{tab:related_works}
\end{table*}




\subsection{Context-free Reasoning Benchmarks}
Recently, with the release of OpenAIâ€™s o1 \cite{o1report}, there has been growing attention on slow thinking and long-form Chain of Thought (CoT) as a means of improving reasoning capabilities \cite{deepseekai2025deepseekr1incentivizingreasoningcapability,qwq-32b-preview,Qin2024O1RJ}. These efforts primarily focus on domains that demand rigorous logical reasoning:
(1) Competitive math problems: AIME\footnote{https://huggingface.co/datasets/Maxwell-Jia/AIME\_2024\label{footnote:AIME}}, CNMO\footnote{https://www.cms.org.cn/Home/comp/comp/cid/12.html} and MATH\cite{MATH}; (2) Coding challenges: SWE-Bench \cite{SWE-bench}, LiveCodeBench \cite{LiveCodeBench} and Codeforces\footnote{https://codeforces.com}; (3) Scientific academic problems: GPQA \cite{GPQA} and Humanity's Last Exam  \cite{HLE}.

While these tasks address reasoning difficulties at the forefront of human intelligence, 
we do not regard these domain-specific reasoning benchmarks as adequate measures of the generalization ability of LLMs. With the intuition that diverse domains can naturally emerge from a wide range of documents, we pay attention to long-context documents to present problems that span an extensive range of fields.

\subsection{Long-context Reasoning Benchmarks}
\paragraph{Elementary Long-context Benchmarks} In the early stages of the development of long-context LLMs, synthetic tasks \cite{NIAH, RULER} dominated the evaluation landscape as a basic measure of long-context understanding. Recognizing the potential divergence between the results of synthetic tasks and real-world applications, LongBench \cite{LongBench} and InfiniteBench \cite{INFINITEBENCH} incorporate realistic tasks, such as document-based question answering and summarization. To achieve better discrimination, HELMET \cite{HELMET} aggregates contemporary long-context benchmarks. The aforementioned studies focus primarily on evaluating the fundamental capacity for long-context comprehension. Reasoning questions are scarcely incorporated, and when present, they tend to be rudimentary, requiring only one or two steps.

Although several studies explicitly emphasize evaluating long-context reasoning, their ability to discriminate is often limited. BABILong \cite{BABILong} assesses reasoning abilities through synthetically generated extended contexts. DocBench \cite{DOCBENCH} and Loong \cite{Loong} incorporate reasoning tasks based on single or multiple documents. These benchmarks face two key limitations: (1) compromised validity due to semantically inconsistent synthetic contexts, or (2) excessively simplified questions that fail to pose genuine challenges to LLMs proficient in slow and deliberate reasoning.

\paragraph{Difficult Long-context Reasoning Benchmarks} Recent works have introduced more challenging tasks that increase the complexity of evidence retrieval or require longer chains of logical reasoning. Nocha \cite{NoCha}, RuleArena \cite{RuleArena}, and DetectiveQA \cite{DetectiveQA} include difficult reasoning tasks that focus on specific domains such as fantasy novels, policies, and orthodox detective stories, respectively. LongBench v2 \cite{LongBench2} introduces difficult human-curated long-context reasoning tasks across various domains. However, their discriminative validity is undermined by three key factors: (1) limited domain coverage \cite{NoCha,RuleArena,DetectiveQA}; (2) reduced reasoning difficulty due to the exclusion of deliberately designed challenging puzzles and the reliance on domain-specific expertise \cite{LongBench2}; (3) the use of a unified multiple-choice question format, which allows the model to guess the correct answer randomly \cite{LongBench2}.
We compare our DocPuzzle benchmark and others in Table \ref{tab:related_works}.

