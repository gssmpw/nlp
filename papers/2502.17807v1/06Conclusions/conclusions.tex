We introduce DocPuzzle, a challenging yet realistic long-context reasoning benchmark. By a carefully designed construction pipeline, DocPuzzle ensures both reliability and difficulty and shows consistent rankings for advanced LLM models.
Instead of only considering the final answer, a process-aware evaluation method is also proposed to better evaluate the reasoning process of LLMs.
Our study through DocPuzzle reveals the strong long-context reasoning ability of slow-thinking models.
Another insight is that Knowledge distillation may be insufficient for transferring complex reasoning patterns.

\section*{Ethics Statement}
DocPuzzle seeks to establish an instructive benchmark for advancing research in complex reasoning with long context.
Certain data incorporated within DocPuzzle are sourced from open-source datasets and publicly available data on the Internet.
The content presented does NOT reflect the viewpoints of the authors.
As the definition of commonsense knowledge may vary among individuals, the current evaluation criteria are derived from the consensus of the annotators.




