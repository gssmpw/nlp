
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{03Methods/annotation_pipeline_steps.pdf}
    \caption{Overview of DocPuzzle construction pipeline. There are five stages before the samples are included in the benchmark.}
    \label{fig:annotation_pipeline}
\end{figure*}
DocPuzzle is a benchmark crafted to evaluate the ability of LLMs to handle highly intricate reasoning tasks based on long contexts. 
Figure \ref{fig:annotation_pipeline} illustrates the carefully designed data annotation process, which emphasizes ensuring the difficulty of the puzzles and incorporates a meticulous verification of answer controversy. 
Section \ref{sub:pipeline} introduces three stages in detail.


\subsection{Data Construction Pipeline}\label{sub:pipeline}

\paragraph{Data Collection}
We curate contexts from five domains: literature, news articles, policy documents, financial reports, and scientific papers.
The domain distribution of collected contexts is visualized in Figure \ref{fig:context_distribution}.

We recruit domain experts with at least a master's degree as annotators. To ensure sufficient reasoning depth, these annotators upload reasoning-intensive documents that they are highly familiar with. 
For policy documents and news reports requiring extended context, annotators perform article aggregation by retrieving related documents and concatenating them into coherent narratives. 
Contexts undergo semantic integrity verification through manual review, with necessary modifications to ensure answer determinism. 
The final dataset contains contexts with median and average token counts of 10,641 and 10,215 in respective \footnote{Tokenized using Qwen2.5-72B-Instruct tokenizer}.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{03Methods/category_pie_chart.png}
    \caption{Distribution of document categories in DocPuzzle benchmark.}
    \label{fig:context_distribution}
\end{figure}

\paragraph{Data Annotation}
Annotators follow rigorous guidelines that adhere to the following principles.
1)Context-Dependent Reasoning: 
In all cases,  the information required to answer the questions has to be inferred from the relevant content of the original documents rather than solely inferred from common sense or pre-trained knowledge patterns.
This design ensures the evaluation of models' contextual reasoning abilities rather than memorization capacity, as LLMs have been trained with numerous corpora, including reasoning-intensive ones.
2) Reasoning Complexity: Each question must involve at least two reasoning operations from the following categories: temporal reasoning, arithmetic reasoning, bridging, comparative analysis, or causal inference. 
We explicitly prohibit answerable questions through single-fragment retrieval.
Following \cite{geva2021didaristotleuselaptop}, we prioritize implicit reasoning paths without direct solution shortcuts. 
While allowing the incorporation of common sense knowledge, we intentionally include cognitive traps to assess model robustness under realistic confusion scenarios.
3)Answer Objectivity: To minimize evaluation bias, we restrict questions to objective ones (multiple-choice, true/false, direct Q\&A) with deterministic answers. 
Subjective questions requiring interpretative responses are excluded due to potential annotation variance.

\paragraph{Data Verification and Revision}
To address the natural language ambiguity in complex reasoning tasks, we implement a validation protocol in which the final answer gradually converges through multiple iterations of independent responses, debate, and revision. 
In the first stage, problems are answered using state-of-the-art LLMs (o1-preview\footnote{o1-preview-2024-09-12}, GPT-4o\footnote{gpt-4o-2024-08-06 \label{footnote:4o}}, QwQ-32B-Preview \cite{qwq-32b-preview}, moonshot-v1-128k \footnote{https://platform.moonshot.cn \label{footnote:kimi}}).
The responses are evaluated following the methodology in Section \ref{subsec:eval_method}. 
Annotators analyze cases especially when all models fail and revise samples with controversial answers and checklists.
Besides, we implement difficulty filtering. 
Samples solved by all baseline models are removed to maintain challenge levels. 

In the second stage, each sample undergoes independent human review. 
In cases of inconsistency, a debate between the reviewer and the annotator ensues to establish a rigorous, objective answer. 
Inconsistencies are typically due to missing information, subjective deductions, or ambiguity in the content of a document or problem. 
Contested items require consensus through debate. 
Samples with persistent disagreements are discarded.
The final dataset retains only samples demonstrating stable expert consensus.


\subsection{Evaluation Method}\label{subsec:eval_method} 
Existing reasoning benchmarks predominantly focus on structured domains like mathematics and coding \cite{lightman2023lets, jain2024livecodebenchholisticcontaminationfree} or utilize constrained formats \cite{LongBench2}, limiting their applicability to free-form textual reasoning.
As we focus on the capacity of reasoning, we allow imperfect textual organizing such as language mixing and redundant content.
Under this circumstance, employing a parser for the response may introduce bias.
On the other hand, even an erroneous reasoning chain may lead to a correct answer.
Thus, a model good at guessing will get an inflated score, especially for multiple-choice and true-or-false questions.
Our evaluation framework addresses these limitations.
To mitigate these biases, we take not only the final answer but also the reasoning chain into account by utilizing a checklist for each sample.
We allow a calculation error within an acceptable range in the checklist.
This accommodates acceptable calculation deviations while penalizing fundamental logical errors.
We prompt a judge model to judge whether the response is correct. 
To mitigate randomness, we design three similar evaluation prompts, and the evaluation result is decided by majority voting.
The prompt templates we use in the evaluation are shown in Appendix \ref{sec:appendix}.
