% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{xcolor}

\usepackage{makecell} % 单元格换行
\usepackage{tabularx} % 表格自动调整列宽
\usepackage{amssymb}
% 定义快捷命令
\newcommand{\cmark}{\textcolor{green!70!black}{\checkmark}}  % 深绿色对勾
\newcommand{\xmark}{\textcolor{red!50!black}{\texttimes}}    % 深红色叉号
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   \texttt{email@domain} \\ \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   \texttt{email@domain} \\}




\author{
 \textbf{Tianyi Zhuang\textsuperscript{1}},
 \textbf{Chuqiao Kuang\textsuperscript{1}},
 \textbf{Xiaoguang Li\textsuperscript{1}},
\\
 \textbf{Yihua Teng\textsuperscript{2}},
 \textbf{Jihao Wu\textsuperscript{2}},
 \textbf{Yasheng Wang\textsuperscript{1}},
 \textbf{Lifeng Shang\textsuperscript{1}}
\\
\\
 \textsuperscript{1}Huawei Noah’s Ark Lab
 \\
 \textsuperscript{2}Huawei Inc.
\\
 \small{
   {\{zhuangtianyi, kuangchuqiao, lixiaoguang11, tengyihua, wujihao, wangyasheng, Shang.Lifeng\}@huawei.com}
 }
}

\begin{document}
\maketitle
\begin{abstract}
We present DocPuzzle, a rigorously constructed benchmark for evaluating long-context reasoning capabilities in large language models (LLMs).
This benchmark comprises 100 expert-level QA problems requiring multi-step reasoning over long real-world documents.
To ensure the task quality and complexity, we implement a human-AI collaborative annotation-validation pipeline.
DocPuzzle introduces an innovative evaluation framework that mitigates guessing bias through checklist-guided process analysis, establishing new standards for assessing reasoning capacities in LLMs.
Our evaluation results show that: 1)~Advanced slow-thinking reasoning models like o1-preview~(69.7\%) and DeepSeek-R1~(66.3\%) significantly outperform best general instruct models like Claude 3.5 Sonnet~(57.7\%); 2)~Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B~(41.3\%) falls far behind the teacher model, suggesting challenges to maintain the generalization of reasoning capabilities relying solely on distillation.
\end{abstract}

\section{Introduction}
\input{01Introduction/introduction}

\section{Related Works}
\input{02RelatedWork/related_work}

\section{DocPuzzle}
\input{03Methods/methods}

\section{Experiments}
\input{04Experiments/experiments}

\section{Analysis}
\input{05Analysis/analysis}

\section{Conclusions}
\input{06Conclusions/conclusions}


\bibliography{acl_latex}

\appendix

\section{Evaluation Prompt}
\label{sec:appendix}
\paragraph{Evaluation Prompt 1}
You are an experienced Q\&A expert tasked with evaluating the quality of a response based on four components: the user's question, the standard answer, the answer checklist, and the provided response.

User Question: \{query\}

Standard Answer: \{answer\}

Answer Checklist: \{checklist\}

Response: \{model\_response\}


Evaluation Criteria:

Accuracy Check: Compare the response with the standard answer. If the meaning aligns with the standard answer, mark it as correct. If inconsistent, mark it as incorrect. Note: For numerical answers, responses within the allowable margin of error specified in the checklist are considered correct.

Checklist Compliance: Assess whether the response meets the mandatory requirements and flexible criteria outlined in the answer checklist. Compliance results in "correct"; non-compliance results in "incorrect".

Final Judgment: The response is deemed correct only if both criteria 1 and 2 are satisfied. Otherwise, it is marked as incorrect.

Output Requirements:
Step 1: Analysis
Begin with "Step 1: Analysis:" and conduct thorough, logical reasoning until you reach a conclusive evaluation. Stop once your reasoning is sufficient to determine the final judgment.

Step 2: Final Judgment
Begin with "Step 2: Final Judgment:" and output the result strictly in the following dictionary format. Do not include additional text like "json" or unrelated content.
Format: \{\{"Response Correctness": "Correct"/"Incorrect"\}\}

\paragraph{Evaluation Prompt 2}
As an experienced Q\&A expert, you need to evaluate the accuracy of responses based on the given user question, standard answer, answer checklist, and model response, while considering two core dimensions.

User Question: \{query\}

Standard Answer: \{answer\}

Answer Checklist: \{checklist\}

Response: \{model\_response\}

Evaluation Criteria:

Accuracy Alignment: Assess whether the response matches the standard answer in meaning. If consistent, mark as correct; otherwise, mark as incorrect. Note: For numerical answers, responses within the allowable error range specified in the checklist are deemed correct.

Checklist Adherence: Evaluate whether the response fulfills all mandatory and flexible requirements outlined in the answer checklist. Compliance results in "correct"; non-compliance results in "incorrect".

Final Verdict: The response is considered correct only if both criteria 1 and 2 are satisfied. Otherwise, it is marked as incorrect.

Output Requirements:
Step 1: Analysis
Begin with "Step 1: Analysis:" and perform detailed, rigorous reasoning and comparisons. Continue until your logical analysis is sufficient to reach a definitive conclusion.

Step 2: Evaluation Result
Begin with "Step 2: Evaluation Result:" and strictly output the final assessment in the specified dictionary format. Do not include terms like "json" or unrelated content.
Final output format: \{\{"Response Correctness": "Correct" or "Incorrect"\}\}

\paragraph{Evaluation Prompt 3}
As a seasoned expert in Q\&A systems, you will comprehensively evaluate responses to questions based on the provided user query, standard answer, answer checklist, model response, and the following two evaluation dimensions.


User Question: \{query\}

Standard Answer: \{answer\}

Answer Checklist: \{checklist\}

Response: \{model\_response\}

Evaluation Criteria:

Content Accuracy: Compare the response with the standard answer. If the response accurately reflects the content and intent of the standard answer, mark it as correct. Otherwise, mark it as incorrect. Note: For numerical calculations, minor deviations in results are deemed acceptable and marked as correct.

Checklist Compliance: Verify whether the response satisfies all requirements specified in the answer checklist. Full compliance results in "correct"; any violation leads to "incorrect".

Final Judgment: The response is considered correct only if both criteria 1 and 2 are satisfied. If either fails, the final result is "incorrect".

Output Requirements:
Step 1: Reasoning
Begin with "Step 1: Reasoning:" and conduct meticulous, logical analysis and comparisons. Continue until your reasoning process is thorough and conclusive enough to determine the final judgment.

Step 2: Evaluation Outcome
Begin with "Step 2: Evaluation Outcome:" and output the result strictly in the specified dictionary format. Avoid including terms like "json" or unrelated content.
Final output format:
\{\{"Response Correctness": "Correct" or "Incorrect"\}\}



\end{document}
