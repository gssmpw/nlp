\section{Related Work}
\noindent 
\textbf{Code Retrieval.}
% \subsection{Code Retrieval}
Software developers often rely on the reuse of existing code resources to achieve efficient code. Therefore, code retrieval has emerged as a crucial research area, aiming to explore the implicit connections between natural language queries and code databases, enabling developers to obtain the required code quickly and accurately. Early research mainly represent code and queries as feature vectors and retrieve code based on similarity to the query vectors. Boolean vectors~\citep{salton1983extended} characterize the features by indicating the presence or absence of specific features, such as specific types of AST nodes~\citep{luan2019aroma}. Another commonly used method is to map a set of tokens into a Term Frequency-Inverse Document Frequency (TF-IDF) vector, which can not only indicate whether a feature exists but also reflect the importance of this feature~\citep{diamantopoulos2018codecatch,takuya2011spontaneous}.  

With the rapid development of deep learning technology, an increasing number of studies have focused on the use of neural networks to achieve efficient code retrieval. Most of the related work adopts end-to-end neural learning methods. The query and the code are embedded into a joint vector space through the model, and the code search problem is then transformed into finding the nearest-neighbor code for a given query in this space~\citep{gu2018deep,sun2022code}. 
The core of code retrieval lies in code encoding, which is reviewed below.
% Subsequent research has been further dedicated to improving the vector representation of code learning. 

\noindent 
\textbf{Code Encoding With Transformer.}
% \subsection{Code Encoding With Transformer}
Following the significant success of the transformer architecture and pre-training in NLP, researchers have started to explore the potential of the Transformer model in code representation learning. Single encoder models mainly follow the BERT framework. CodeBERT~\citep{feng2020codebert} utilizes the masked language objective to pre-train on NL-PL pairs, and adds the substitute token detection task~\citep{clark2020electra}. GraphCodeBERT~\citep{guo2020graphcodebert} enhances the pre-training process by incorporating the data flow derived from the AST. SynCoBERT~\citep{wang2021syncobert} utilizes identifier prediction, AST edge prediction, and multimodal contrastive learning to make full use of AST. In addition, some works adopt the encoder-decoder architecture. 
% PLBART~\citep{ahmad2021unified} adjusts the BART~\citep{lewis2019bart} architecture and uses the denoising objective to pre-train on NL and PL corpora. 
CodeT5~\citep{wang2021codet5} extends the T5~\citep{raffel2020exploring} model to the code domain and additionally combines the information of the identifier nodes in the AST. TreeBERT~\citep{jiang2021treebert} utilizes the structural information of the AST by representing it as a set of paths from the root node to the terminal nodes. 

The aforementioned methods utilize Transformer models for code encoding. While some integrate AST information, they still treat it as a linear sequence. This sequence-based representation hampers the accurate capture of hierarchical and complex relationships inherent in code structures. Consequently, these approaches fail to fully leverage the syntactic and semantic richness of programming languages, which are defined by intricate nested structures that cannot be effectively represented in a flat sequential format.

% When perceiving the topological information of complex graph structures like AST, the Transformer performs relatively weakly. Although recent studies have employed means such as additional positional encoding to represent structural information, it still cannot capture the local and global structural dependency relationships between nodes as accurately as GNN. 


\noindent 
\textbf{Code Encoding With GNN.}
% \subsection{Code Encoding With GNN}
\citet{allamanis2017learning} first use GNN to represent code. To convert the code into a graph, they design complex edges based on the AST. Devign~\citep{zhou2019devign} and Reveal~\citep{chakraborty2021deep} adopt the Code Property Graph (CPG), which is composed of the AST, the control flow graph (CFG) and the data flow graph (DFG), and used the Gated Graph Neural Network (GGNN) to encode the graph. 
% IVDetect~\citep{li2021vulnerability} further enriched the content of nodes by adding more information, such as AST subtrees, data dependency contexts, and control dependency contexts.
Graphsearchnet~\citep{liu2023graphsearchnet} introduces Bidirectional GGNN (BiGGNN) to create graphs for code and text, capture local structural details, and uses a multi-head attention module to enhance BiGGNN.

These methods mainly use GNN as a tool for encoding code for vulnerability identification. In our research work, we focus on the multimodal retrieval task and integrate Transformer with GNN.
% use contrastive learning to align the embeddings of code and text. 
To ensure that our model is applicable to various PLs, we use only the basic AST without adding complex edges or nodes. 

% The above-mentioned methods either use Transformer or GNN alone, failing to fully integrate the advantages of both. In our research, we combine the two and propose an innovative framework, focusing on improving semantic code retrieval ability.