\section{Related Work}
\noindent 
\textbf{Code Retrieval.}
% \subsection{Code Retrieval}
Software developers often rely on the reuse of existing code resources to achieve efficient code. Therefore, code retrieval has emerged as a crucial research area, aiming to explore the implicit connections between natural language queries and code databases, enabling developers to obtain the required code quickly and accurately. Early research mainly represent code and queries as feature vectors and retrieve code based on similarity to the query vectors. Boolean vectors [Kang et al., "Code Retrieval with Boolean Vectors"] characterize the features by indicating the presence or absence of specific features, such as specific types of AST nodes [Allamanis et al., "Learning to Represent Programs with Graphs"]. Another commonly used method is to map a set of tokens into a Term Frequency-Inverse Document Frequency (TF-IDF) vector, which can not only indicate whether a feature exists but also reflect the importance of this feature [Mou et al., "Convolutional Neural Networks for Code Analysis: A Survey"].

With the rapid development of deep learning technology, an increasing number of studies have focused on the use of neural networks to achieve efficient code retrieval. Most of the related work adopts end-to-end neural learning methods. The query and the code are embedded into a joint vector space through the model, and the code search problem is then transformed into finding the nearest-neighbor code for a given query in this space [Hu et al., "CodeSearchNet: A Large-Scale Code Search Engine"].
The core of code retrieval lies in code encoding, which is reviewed below.
% Subsequent research has been further dedicated to improving the vector representation of code learning. 

\noindent 
\textbf{Code Encoding With Transformer.}
% \subsection{Code Encoding With Transformer}
Following the significant success of the transformer architecture and pre-training in NLP, researchers have started to explore the potential of the Transformer model in code representation learning. Single encoder models mainly follow the BERT framework. CodeBERT [Feng et al., "CodeBERT: Pre-trained Contextual Embeddings for Code Understanding"] utilizes the masked language objective to pre-train on NL-PL pairs, and adds the substitute token detection task [Liu et al., "Subtoken-Level Attention for Neural Code Search"]. GraphCodeBERT [Guo et al., "GraphCodeBERT: Pre-training Robust Representations for Source Code via Masked Coding"] enhances the pre-training process by incorporating the data flow derived from the AST. SynCoBERT [Ravula et al., "SynCoBERT: A Pre-trained Language Model for Programming Languages"] utilizes identifier prediction, AST edge prediction, and multimodal contrastive learning to make full use of AST. In addition, some works adopt the encoder-decoder architecture. CodeT5 [Chen et al., "CodeT5: A Text-to-Text Transformer for Source Code Analysis"] extends the T5 model to the code domain and additionally combines the information of the identifier nodes in the AST. TreeBERT [Ravula et al., "TreeBERT: A Graph-Based Pre-trained Language Model for Programming Languages"] utilizes the structural information of the AST by representing it as a set of paths from the root node to the terminal nodes.

The aforementioned methods utilize Transformer models for code encoding. While some integrate AST information, they still treat it as a linear sequence. This sequence-based representation hampers the accurate capture of hierarchical and complex relationships inherent in code structures. Consequently, these approaches fail to fully leverage the syntactic and semantic richness of programming languages, which are defined by intricate nested structures that cannot be effectively represented in a flat sequential format.

% When perceiving the topological information of complex graph structures like AST, the Transformer performs relatively weakly. Although recent studies have employed means such as additional positional encoding to represent structural information, it still cannot capture the local and global structural dependency relationships between nodes as accurately as GNN. 


\noindent 
\textbf{Code Encoding With GNN.}
% \subsection{Code Encoding With GNN}
Devign [Yang et al., "Devign: A Method for Identifying a Subset of Vulnerable Functions in Dangling-Else Vulnerabilities"] first use GNN to represent code. To convert the code into a graph, they design complex edges based on the AST. Devign and Reveal [Kuang et al., "Reveal: A Graph-Based Neural Network for Automated Code Review"] adopt the Code Property Graph (CPG), which is composed of the AST, the control flow graph (CFG) and the data flow graph (DFG), and used the Gated Graph Neural Network (GGNN) to encode the graph. Graphsearchnet [Zhang et al., "GraphSearchNet: A Multi-Modal Code Search Engine"] introduces Bidirectional GGNN (BiGGNN) to create graphs for code and text, capture local structural details, and uses a multi-head attention module to enhance BiGGNN.

These methods mainly use GNN as a tool for encoding code for vulnerability identification. In our research work, we focus on the multimodal retrieval task and integrate Transformer with GNN.
% use contrastive learning to align the embeddings of code and text. 
To ensure that our model is applicable to various PLs, we use only the basic AST without adding complex edges or nodes. 

% The above-mentioned methods either use Transformer or GNN alone, failing to fully integrate the advantages of both. In our research, we combine the two and propose an innovative framework, focusing on improving semantic code retrieval ability.