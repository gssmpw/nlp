\section{Experiment}

\subsection{Experimental Setup}

\subsubsection{Evaluation Datasets}

\vpara{Spider.} Spider is a cross-domain Text-to-SQL benchmark including 10,181 natural language questions and 5,693 unique SQL queries across more than 200 databases in 138 different domains~\citep{yu2018spider}. The dataset is split into the training set (7,000 examples), the development set (1,034 examples), and the test set (2,147 examples).

\vpara{BIRD.} BIRD is a large-scale cross-domain benchmark~\citep{li2024can}, which contains over 12,751 unique question-SQL pairs, and 95 large databases, covering 37 domains with a total size of 33.4GB. 
Compared to Spider, BIRD emphasizes SQL efficiency and knowledge reasoning in large databases, offering configurations with and without external knowledge. 
The training set has 9,428 examples and the development set has 1,534 examples.

\subsubsection{Metrics}

We evaluate model performance using the official metrics for each dataset.
% Spider uses Exact Match Accuracy (\textbf{EM}) and Execution Accuracy (\textbf{EX}). 
% EM checks if the predicted SQL structurally matches the correct SQL without specific values, while EX compares the execution results of the predicted SQL with the actual results, offering a more precise performance measure.
Spider uses Execution Accuracy (\textbf{EX}), which compares the execution results of the predicted SQL with the actual results, offering a more precise performance measure.
BIRD uses Valid Efficiency Score (\textbf{VES}) and EX. VES evaluates both execution accuracy and run-time efficiency of the generated SQL, ensuring the results match the reference query and accounting for execution time efficiency.
We compare our approach with existing works, including DIN-SQL~\citep{pourreza2024din}, DAIL-SQL~\citep{gao2024text}, MAC-SQL~\citep{wang2024mac},TA-SQL~\citep{qu2024before}, SuperSQL~\citep{li2024dawn}. 
% Our primary focus was on comparing prompt-based methods. 


\subsubsection{Implement Details}

In our experiments, we use GLM-4~\citep{glm2024chatglm} to process the training sets to obtain AQP and CSM demonstrations. 
% After determining the optimal settings in GLM-4, we used GPT-4o to achieve state-of-the-art (SOTA) performance on the Spider and BIRD datasets. 
In the prompt, we use three few-shot examples for both datasets because our experiments indicate that using three yields better results, as depicted in Figure \ref{fig:fewshot_number}.
For retrieving similar few-shot examples, we utilize the BGE model~\citep{zhang2023retrieve} and conduct embedding similarity search with the FAISS library~\citep{douze2024faiss}.
To minimize the randomness in the outputs of the large language models (LLMs), we set the temperature to 0. 

\input{tables/bird_dev}

\subsection{Overall Performance}

\subsubsection{BIRD Results}

As shown in Table \ref{tab:bird_dev_results}, on the BIRD Dev set, our method \method+ GPT-4o achieves a significant improvement, with an execution accuracy of 64.67\%, nearly 10\% higher than DAIL-SQL+ GPT-4. 
\method+ GPT-4o also demonstrates a notable enhancement over the GPT-4o baseline, reaching a new SOTA. 
For the Valid Efficiency Score (VES) metric, \method+ GPT-4o achieves 65.04\%, surpassing other methods, indicating the high efficiency of SQL generated by our method.
These results indicate that our method can find more valuable few-shot examples for test questions by abstracting training samples. This approach allows for more effective utilization of the knowledge contained within the training data, leading to the generation of more accurate and efficient SQL queries.
In addition, while GLM-4 is only 2.47\% behind GPT-4o, the introduction of \method increases this gap to 5.54\%. 
Although we obtain AQP and CSM few-shot examples using GLM-4 and apply these few-shot examples to GPT-4o, our method significantly enhances GPT-4o's performance. 
These performance improvements across different language models demonstrate the model-agnostic nature and strong adaptability of \method.

\input{tables/spider_results}

\subsubsection{Spider Results}

As shown in Figure~\ref{tab:spider_results}, our method significantly outperforms other prompt-based methods.
On the Spider Dev set, our method achieves an execution accuracy of 87.9\%, outperforming GPT-4o by 12.9\% and the current state-of-the-art DAIL-SQL + GPT-4 by 3.5\%, setting a new SOTA. We also achieve SOTA on the Spider Test set. 
Additionally, \method+ GLM-4 delivers competitive results.
These results further demonstrate our method's effectiveness and generalization. 
Interestingly, we observe that the performance of GLM-4 is very close to GPT-4o on Spider, even slightly outperforming GPT-4o. 
% \Wei{add several sentences}
% Moreover, the COT version of \method+ GPT-4o achieved an execution accuracy of 85.4\% on the Spider development set. 
% In subsequent experiments, we will analyze its cost-effectiveness.

\input{tables/bird_different_level}



\subsection{Ablation Study}
\subsubsection{Major Components}

We conduct ablation studies to assess the impact of the AQP and CSM components of our proposed method. The results are illustrated in Figure~\ref{tab:bird_different_level}. 
Removing the CSM component leads to a 2.94\% drop in performance, highlighting that the absence of question and database schema mapping makes it more challenging for the model to generate SQL, thus lowering accuracy.
CSM is equivalent to performing schema-linking based on few-shot examples, which can effectively reduce the number of candidate database schemas in the next step.
Without CSM, when generating SQL, the model needs to handle a large number of redundant and unrelated database schemas, making it more difficult.

Removing AQP along with CSM causes a further performance drop of 4.57\%, indicating the importance of AQP. 
AQP not only facilitates the generation of CSM but also plays a key role in retrieving similar few-shot examples. 
By masking database-related information in the original question, AQP standardizes the question, making it easier for the model to find structurally similar few-shot examples from a vast array of training samples. 
Without AQP, the model struggles to obtain relevant few-shot examples, leading to a marked drop in SQL execution accuracy.

\input{tables/bird_ablation}

\subsubsection{Minor Components}
We conduct ablation studies on several specific aspects of \method. Initially, we investigate the impact of the organization of few-shot examples on performance. In our approach, the few-shot examples retain only the tables and columns used in the example SQL. 
As illustrated in Figure~\ref{tab:bird_ablation}, when we use Full Schema, the performance of \method on BIRD decreases and results in more token consumption. Therefore, we conclude that redundant schema information in few-shot examples is unnecessary.
Similarly, we conduct ablation studies on the foreign key and cell value information of the test questions. Given that CSM emphasizes schema information, we hypothesize that foreign keys and column values that are not emphasized in the tables could be redundant. 
Experimental results indicate that including all cell values and foreign keys does indeed lead to performance degradation. Notably, including full foreign keys causes a performance drop of nearly 1\% and also increases token consumption.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{image/fewshot_new2.pdf}
    \caption{GLM-4' results on various few-shot numbers.}
    \label{fig:fewshot_number}
    \vspace{-4mm}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{image/fewshot_contrast.pdf}
    \caption{Results of few-shot example retrieval.}
    \label{fig:aqp_case}
    \vspace{-4mm}
\end{figure*}

\subsection{Further Analysis}

\subsubsection{Impacts of Few-shot Numbers}

In the in-context learning of large models, using too few examples may not provide enough reference information for generating accurate SQL queries. Conversely, using multiple examples can increase computational complexity and introduce noise, ultimately reducing performance.
As shown in Figure~\ref{fig:fewshot_number}, we conduct experiments with different numbers of few-shot examples on both BIRD and Spider datasets. 
EX performance improves as the number of few-shot examples increases from 0 to 3 across both datasets. 
However, as the number of examples is larger than 3, performance begins to decline. Thus, three examples are the best choice.


\subsubsection{Case Study}

Our method, by abstracting the samples, can find more valuable few-shot examples for test questions. 
We conduct a detailed case study to illustrate this. 
Figure~\ref{fig:aqp_case} illustrates the top-1 few-shot examples retrieved based on question similarity. The result indicates that \textbf{the SQL structure of the few-shot examples significantly differs from the gold SQL of the test question. }
This discrepancy arises because the examples retrieved based on question similarity often contain abundant domain-specific information, making the retrieval results more domain-focused rather than addressing the specific intent of the question. 
In addition, even if the examples and the test question belong to the same domain, the examples provide little help for SQL generation for the test question due to different databases.


In contrast, \textbf{the top-1 few-shot example retrieved using our proposed AQP shows a high similarity in SQL structure with the test question}, differing only in the ``WHERE'' clause. 
This is because AQP representations provides a database-agnostic representation of the questions, focusing more on the structure of the questions. 
We claim that similar question structures will lead to similar SQL structures. 
Therefore, compared to retrieval based on question similarity, examples obtained through AQP-based retrieval are more helpful in generating the correct SQL for test questions. Ablation results of AQP in Table~\ref{tab:bird_different_level} aslo demonstrates the critical role of AQP.
% Additionally, as evidenced by the table, relying solely on question similarity for retrieval results in a 1.63\% performance drop in \method+ GLM-4. This clearly underscores the critical role of AQP in selecting appropriate few-shot examples.


\subsubsection{CoT version of \method}

\method requires calling the LLM multiple times, which is costly. Therefore, we propose a Chain-of-Thought (CoT) version of \method, which generates AQP, CSM, and initial SQL in a single LLM call.
We evaluate the CoT of \method on the Spider Dev set.
As shown in Table~\ref{tab:cot}, the CoT version's execution accuracy (EX) is only 2.5 percentage points lower than \method+ GPT-4o, but it reduces the average token usage per question by 1,310 tokens. We introduced an efficiency rate to quantify the ratio of model performance to economic cost. 
The results show that the CoT version is more economical. Its efficiency rate is significantly higher than \method, reflecting better optimization in terms of computational resource consumption and performance.



\begin{table}[htbp] 
\centering 
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{lcccc} 
\toprule \textbf{Method} & \textbf{Avg. Prompt Tokens} & \textbf{EX} & \textbf{Efficiency Rate} \\ \midrule \method + GPT-4o & 3,614 & 87.9 & 0.0243 \\ CoT of \method + GPT-4o & 2,304 & 85.4 & 0.0370 \\ \bottomrule 
\end{tabular}
}
    \caption{Performance comparison of different models on execution accuracy and efficiency. Efficiency Rate is calculated by EX/Avg. Prompt Tokens)}
    \label{tab:cot}
    % \vspace{-4mm}
\end{table}