\section{Related Work}
Text-to-SQL aims to convert users' natural language queries into appropriate SQL queries, enabling non-experts to access databases and retrieve information with reduced effort and cost. Early research mainly applies rule-based methods and template matching techniques to translate natural language questions into SQL queries____. However, these methods lack scalability and adaptability.

\vpara{Sequence-to-Sequence based methods.} With the advent of deep learning, Text-to-SQL has evolved into utilizing sequence-to-sequence models____ where both the database schema and user questions are encoded as sequences, aiming to generate the corresponding SQL query. Transformer-based models, such as T5 and BERT, have enhanced their performance by incorporating relation-aware self-attention mechanisms.
% RASAT____ efficiently integrates various relational structures while leveraging the pre-trained T5 model. 
% Similarly, RESDSQL____ introduces a framework with order-enhanced encoding and skeleton-aware decoding to separate schema linking from skeleton parsing. 
% Despite their strong performance, these models require substantial resources for training and obtaining annotated question-SQL pairs.

Recently, the advent of LLMs has led to significant advancements in Text-to-SQL task____. 
Methods based on LLMs can be roughly classified into \textbf{fine-tuning based methods} and \textbf{prompt-based methods}.
Fine-tuning based methods mainly involve further training open-source language models using Text-to-SQL data____. 
Prompt-based methods utilize the in-context learning ability of closed-source language models to accomplish text-to-SQL tasks____.
%方法的不足
Fine-tuning-based methods usually require a certain amount of labeled data and computational resources.
% With the continuous improvement of closed-source models, even after training on large amounts of data, some fine-tuning based methods on open-source language models are still weaker than prompt-based methods. Therefore, we focus on prompt-based methods.

\vpara{Prompt-based methods.}
% Recently, LLMs have shown remarkable capabilities in reasoning and in-context learning across diverse tasks, including Text-to-SQL. The concept of in-context learning refers to the ability of pre-trained models to rapidly adapt and generate correct outputs for new tasks by utilizing a small set of examples or task descriptions, without modifying the model parameters. Compared to traditional approaches predicated on pre-training and fine-tuning, prompt-based methods offer several advantages. Firstly, they can execute predictive tasks without requiring extensive task-specific training data. Training models from scratch or fine-tuning existing models is resource-intensive, often necessitating a large volume of training samples and substantial computational resources, which might not always be available. Secondly, these large models, generally encompassing billions or even trillions of parameters, possess a superior ability to comprehend and generate complex query logic, thereby exhibiting enhanced generalization capabilities. 
% In the Text-to-SQL context, providing a limited number of question-SQL pairs can yield significant performance improvements. 
Some early works explore strategies for effectively representing databases in in-context learning
____. 
% QDecomp____ analyzes the impact of prompt organization from Chain-of-Thought and Least-to-Most perspectives. 
% In addition, DAIL-SQL____ encodes structural knowledge into SQL queries, chooses examples based on skeleton similarity and removed cross-domain knowledge to enhance token efficiency.
In addition, DAIL-SQL____ conducts a comprehensive and systematic evaluation of prompt-based methods, including different forms of information organization and various few-shot retrieval methods.
Subsequent work breaks down this task into multiple stages for solving.
DIN-SQL____ breaks down the Text-to-SQL task into smaller subtasks with specific prompts, then guides GPT-4 to complete each subtask, and eventually form a complete SQL query. 
TA-SQL____ introduces Task Alignment, a strategy that enhances large language models' performance and reliability in text-to-SQL tasks by mitigating hallucination through schema linking and logical synthesis.
SuperSQL____ presents NL2SQL360, a multi-faceted framework for evaluating and comparing natural language to SQL methods to help researchers and practitioners identify optimal solutions for specific needs. 

However, these studies often focus on prompt organization or task decomposition and do not consider the gap between few-shot examples and test questions, which leads to inefficient in-context learning. 
% Furthermore, the direct generation of SQL from user queries without intermediate steps impedes the model’s learning process. 
To mitigate these challenges, we propose \model, a SQL generation workflow tailored for real-world and complex database environments.