\section{Method}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{image/methodv2.pdf}
    \caption{Overall framework of proposed \model.}
    \label{fig:method}
    \vspace{-4mm}
\end{figure*}


As the complexity of questions increases, the complexity of the corresponding SQLs also increases, and the gap between questions and SQLs becomes larger and larger. To bridge the gap, we propose \model as shown in Figure~\ref{fig:method}, which consists of four components: Abstract Query Pattern, Contextual Schema Markup, Constructing Demonstrations, Generating and Correcting SQL.
Additionally, we introduce a Chain-of-Thought (CoT) version of \model to reduce token usage.


\subsection{Abstract Query Pattern}
\label{method_AQP}

%问题和提出AQP
Specifically, there are two gaps: the mapping from natural language grammar structure to SQL grammar structure, and the mapping from natural language text to database schemas. 
To alleviate the first gap, we propose an Abstract Query Pattern (AQP) module. As illustrated in Figure~\ref{fig:method}, AQP aims to obtain the structural pattern of the question by removing database-related information. Next, we introduce how to get AQP representations of natural language questions. 

%AQP介绍
We conduct AQP by prompting the LLM. 
First, we give task instructions. Specifically, the model needs to identify the database-related information within the question and then replace it with three placeholders: \textit{[TABLE], [COLUMN], and [VALUE]}. 
Second, we provide pre-processed demonstrations that already include the AQP representations of questions to help the model better conduct AQP tasks. 
Finally, we concatenate task instructions, few-shot demonstrations, database schemas, and the question to obtain the AQP prompt.
The LLM process the AQP prompt to generate the AQP representation for the question.

%AQP作用
% The AQP representation of the question significantly removes domain-specific information and is a database-agnostic representation. 
The AQP representation of the question provides a database-agnostic representation that focuses on structure.
We claim that questions with similar AQP representations have similar SQL structures and we confirm this in the experiment. 
Therefore, we can retrieve structurally similar few-shot examples based on AQP, which can effectively help generate SQL for the test question.

% Additionally, the AQP representation of the question will serve as an intermediate step in later stages, placed below the original question, and will become the basis for subsequent Contextual Schema Markup (CSM).

\subsection{Contextual Schema Markup}\label{method_CSM}

%问题和提出CSM
To alleviate the second gap, we propose a Contextual Schema Markup (CSM) module. As illustrated in Figure~\ref{fig:method}, CSM aims to associate database-related text span in the question with specific tables or columns in the database. 
For the second gap, previous research has primarily concentrated on schema-linking, extracting necessary database schemas for SQL generation, thereby reducing the impact of a large number of unrelated schemas.
However, most of these works perform schema-linking at the question level, while we perform schema-linking at the span level. 
% Moreover, our CSM representation is closer to natural language.
% Inspired by SQL-PaLM, we find that emphasizing the selection of relevant tables and columns is more effective than simply removing unselected schemas. 
% Therefore, we propose the CSM. 

%CSM介绍
We conduct AQP by prompting the LLM. 
First, we give task instructions. The model needs to determine the database schema corresponding to each placeholder in the AQP representation of the question and replace the placeholder with the original text and the corresponding database schema.
Second, we provide pre-processed demonstrations that already include the CSM representation of questions to help the model better conduct CSM tasks.
Finally, we concatenate database schemas, the question with task instructions, and few-shot examples to obtain the CSM prompt. The LLM then processes this prompt to generate the CSM representation for the question.

%CSM作用
By matching database-related text span with the tables and columns in the schema, CSM provides an effective method for integrating relevant schema in the question. 
% \Wei{other methods can achieve this.} The key advantage of this approach is its resilience to errors in table and column selection. Even if the wrong tables or columns are selected, the model can still access the full database schema via the prompt, minimizing the impact of such errors on the Text-to-SQL task. 
We believe the CSM module can provide the necessary database schemas to alleviate the semantic gap, while AQP can provide relevant structures to alleviate the structural gap. With the combined effect of the two modules, SQL generation for complex questions can be better addressed.
% We believe this stage aims to provide database information needed for generating SQL queries and the SQL query components provided by AQP. Together, these constitute a complete SQL query.


\subsection{Constructing Demonstrations}

%现有问题
Both the Spider and BIRD datasets have a large number of training samples, and existing works directly use training samples as demonstrations. 
We claim that this approach does not make full use of these valuable natural language questions and SQL pairs.
We construct AQP and CSM demonstrations from the training samples to better utilize them.

%训练集处理逻辑
We first select five representative samples from each training set and manually annotate them to obtain their AQP and CSM representations. Thus we get five AQP demonstrations and five CSM demonstrations for each dataset.
We combine database schemas, the question to be processed, SQL, and AQP demonstrations to obtain the AQP prompt, which is processed by LLM to obtain the AQP representation as described in Section~\ref{method_AQP}.
Next, we use the AQP representation of the question and CSM demonstrations to generate the CSM representation as described in Section~\ref{method_CSM}.
In these two steps, we use GLM-4 because the SQL corresponding to the question is already in the prompt, both tasks are relatively simple.
% and in theory, even smaller models should suffice for this purpose.

\subsection{Generating and Correcting SQL}

To generate the final SQL for the test question, we follow four steps. First, we obtain the AQP representation of the question. Next, we derive the CSM representation. Then, we generate the initial SQL based on the AQP and CSM representations. Finally, we validate and correct it to obtain the final SQL. Below, we introduce each step in detail.


% %test question处理方式和训练集略有不同
% For test questions, similar to the processing logic used for the training set, the first step is to obtain the AQP representation of the test questions. However, unlike the examples in the training set, real questions do not include the SQL part since the SQL needs to be generated. 

%AQP
\vpara{Generating AQP representation.} 
For the given test question, we start by retrieving the top-k most similar AQP demonstrations from all available AQP demonstrations as few-shot examples, using question similarity as a criterion for selection. Existing works indicate that the order of examples significantly affects the output. Therefore, we arrange the examples in reverse order, placing the most similar example closest to the test question.
% %fewshot 的组织形式 现有的问题
% Additionally, the way few-shot examples are organized in the prompt is essential for effective in-context learning. Traditionally, in the Text-to-SQL domain, few-shot examples have been organized in two primary ways. The first approach, as shown in Figure 1A, incorporates complete schema information alongside question-SQL pairs, providing a comprehensive context that aligns well with downstream tasks but introduces significant redundant information. The second approach, illustrated in Figure 1B, includes only question-SQL pairs. This method is simpler and uses fewer tokens but fails to structurally align well with downstream tasks.
% %提出的新形式
% Recognizing the limitations of both approaches, we propose a novel method of organizing few-shot examples, as shown in Figure 1C. In this method, we retain only essential schema information, specifically the tables and columns used in the SQL, thereby significantly reducing token usage while better aligning with downstream task requirements. Subsequent ablation studies demonstrate the effectiveness of this method, showing enhanced performance. 
By concatenating the task instructions, few-shot examples, database schemas, and the test question, and then processing them through an LLM, we obtain the AQP representation of the question as described in~\ref{method_AQP}. 

%CSM
\vpara{Generating CSM representation.} Given the AQP representation of the question, we retrieve the most similar CSM demonstrations as few-shot examples based on AQP representation similarity. 
We then integrate the task instructions, few-shot examples, database schemas, and the test question to construct the CSM prompt. Subsequently, we generate the CSM representation of the question as outlined in Section~\ref{method_CSM}.



%生成SQL阶段
\vpara{Generating SQL.} 
We obtain the AQP representation and CSM representation of the question through the above two steps.
We then retrieve values from the database, ensuring that the relevant columns are present and emphasized in the CSM representation. 
% Removing foreign key information helps reduce redundancies that could affect the model's SQL query generation. 
We select three demonstrations as few-shot examples from the results of the previous CSM step. 
We combine the task instructions, few-shot examples, database schemas, the question, the AQP representation, and the CSM representation to obtain the SQL generation prompt.
This prompt is fed into the LLM to generate the SQL.


\vpara{Correcting SQL.} 
We further validate the generated SQL to ensure its executability.
We execute the generated SQL, and if it executes successfully, the generation process ends. Otherwise, we initiate the correction process.
The initial LLMs input, incorrect SQL, and detected errors are re-input into the LLMs to obtain a new SQL. 
This process repeats until the generated SQL can be successfully executed or a maximum number of correction attempts is reached.


\subsection{Chain-of-Thought}
%CoT版本
Inspired by the Chain of Thought (CoT) approach and aware of the high token consumption from multiple calls of the language model, we propose the CoT version of \model. 
We first retrieve similar examples using the test question and then integrate the examples with the question to guide the model in sequentially generating AQP, CSM, and SQL. The results show that this approach significantly reduces token usage while maintaining minimal performance loss.
%现有COT专注于分解问题步骤
Previous research has applied Chain-of-Thought to the Text-to-SQL domain by decomposing problems into multiple steps for step-by-step resolution. 
However, as far as we know, there is no method to apply a structured chain of thought structure like ours.
% However, to our knowledge, no studies have decomposed the SQL generation process itself into steps. 
