\section{Related Work}

Text-to-SQL aims to convert users' natural language queries into appropriate SQL queries, enabling non-experts to access databases and retrieve information with reduced effort and cost. Early research mainly applies rule-based methods and template matching techniques to translate natural language questions into SQL queries~\citep{baik2020duoquest,stone2020athena++}. However, these methods lack scalability and adaptability.

\vpara{Sequence-to-Sequence based methods.} With the advent of deep learning, Text-to-SQL has evolved into utilizing sequence-to-sequence models~\citep{sutskever2014sequence} where both the database schema and user questions are encoded as sequences, aiming to generate the corresponding SQL query. Transformer-based models, such as T5 and BERT, have enhanced their performance by incorporating relation-aware self-attention mechanisms.
% RASAT~\citep{qi2022rasat} efficiently integrates various relational structures while leveraging the pre-trained T5 model. 
% Similarly, RESDSQL~\citep{li2023resdsql} introduces a framework with order-enhanced encoding and skeleton-aware decoding to separate schema linking from skeleton parsing. 
% Despite their strong performance, these models require substantial resources for training and obtaining annotated question-SQL pairs.

Recently, the advent of LLMs has led to significant advancements in Text-to-SQL task~\citep{rajkumar2022evaluating,ni2023lever,gao2024text}. 
Methods based on LLMs can be roughly classified into \textbf{fine-tuning based methods} and \textbf{prompt-based methods}.
Fine-tuning based methods mainly involve further training open-source language models using Text-to-SQL data~\citep{li2024codes,pourreza2024dts}. 
Prompt-based methods utilize the in-context learning ability of closed-source language models to accomplish text-to-SQL tasks~\citep{pourreza2024din,gao2024text,li2024dawn,qu2024before}.
%方法的不足
Fine-tuning-based methods usually require a certain amount of labeled data and computational resources.
% With the continuous improvement of closed-source models, even after training on large amounts of data, some fine-tuning based methods on open-source language models are still weaker than prompt-based methods. Therefore, we focus on prompt-based methods.

\vpara{Prompt-based methods.}
% Recently, LLMs have shown remarkable capabilities in reasoning and in-context learning across diverse tasks, including Text-to-SQL. The concept of in-context learning refers to the ability of pre-trained models to rapidly adapt and generate correct outputs for new tasks by utilizing a small set of examples or task descriptions, without modifying the model parameters. Compared to traditional approaches predicated on pre-training and fine-tuning, prompt-based methods offer several advantages. Firstly, they can execute predictive tasks without requiring extensive task-specific training data. Training models from scratch or fine-tuning existing models is resource-intensive, often necessitating a large volume of training samples and substantial computational resources, which might not always be available. Secondly, these large models, generally encompassing billions or even trillions of parameters, possess a superior ability to comprehend and generate complex query logic, thereby exhibiting enhanced generalization capabilities. 
% In the Text-to-SQL context, providing a limited number of question-SQL pairs can yield significant performance improvements. 
Some early works explore strategies for effectively representing databases in in-context learning
~\cite{rajkumar2022evaluating,chang2023prompt,tai2023exploring,tai2023exploring}. 
% QDecomp\cite{tai2023exploring} analyzes the impact of prompt organization from Chain-of-Thought and Least-to-Most perspectives. 
% In addition, DAIL-SQL~\citep{gao2024text} encodes structural knowledge into SQL queries, chooses examples based on skeleton similarity and removed cross-domain knowledge to enhance token efficiency.
In addition, DAIL-SQL~\citep{gao2024text} conducts a comprehensive and systematic evaluation of prompt-based methods, including different forms of information organization and various few-shot retrieval methods.
Subsequent work breaks down this task into multiple stages for solving.
DIN-SQL~\citep{pourreza2024din} breaks down the Text-to-SQL task into smaller subtasks with specific prompts, then guides GPT-4 to complete each subtask, and eventually form a complete SQL query. 
TA-SQL~\citep{qu2024before} introduces Task Alignment, a strategy that enhances large language models' performance and reliability in text-to-SQL tasks by mitigating hallucination through schema linking and logical synthesis.
SuperSQL~\citep{li2024dawn} presents NL2SQL360, a multi-faceted framework for evaluating and comparing natural language to SQL methods to help researchers and practitioners identify optimal solutions for specific needs. 

However, these studies often focus on prompt organization or task decomposition and do not consider the gap between few-shot examples and test questions, which leads to inefficient in-context learning. 
% Furthermore, the direct generation of SQL from user queries without intermediate steps impedes the model’s learning process. 
To mitigate these challenges, we propose \model, a SQL generation workflow tailored for real-world and complex database environments.