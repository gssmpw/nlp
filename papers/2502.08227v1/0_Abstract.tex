\begin{abstract}
Sample selection is a prevalent approach in learning with noisy labels, aiming to identify confident samples for training. Although existing sample selection methods have achieved decent results by reducing the noise rate of the selected subset, they often overlook that not all mislabeled examples harm the model's performance equally. In this paper, we demonstrate that mislabeled examples correctly predicted by the model early in the training process are particularly harmful to model performance. We refer to these examples as \emph{Mislabeled Easy Examples} (MEEs). To address this, we propose \emph{Early Cutting}, which introduces a recalibration step that employs the model's later training state to re-select the confident subset identified early in training, thereby avoiding misleading confidence from early learning and effectively filtering out MEEs. Experiments on the \emph{CIFAR}, \emph{WebVision}, and full \emph{ImageNet-1k} datasets demonstrate that our method effectively improves sample selection and model performance by reducing MEEs.
\iffalse
Our implementation can be found at \url{https://www.google.com/?client=safari}.
\fi
\end{abstract}