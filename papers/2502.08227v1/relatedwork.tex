\section{Related Work}
In this subsection, we briefly review the related work. Detailed review of the literature is given in Appendix \ref{appendix:A}.

\emph{Sample Selection} has been widely used in learning with noisy labels to improve the robustness of model training by prioritizing confident samples. An in-depth understanding of deep learning models, particularly their learning dynamics, has facilitated research in this area. Extensive studies on the \emph{Learning Dynamics} of DNNs have revealed that difficult clean examples are typically learned in the later stages of training \cite{arpit2017closer, toneva2018empirical, yuan2024early}. This insight has led to training-time metrics that quantify sample ``hardness'', such as forgetting \cite{toneva2018empirical}, example consistency \cite{pleiss2020identifying}, and learning speed \cite{jiang2021characterizing}. These metrics inspire methods that leverage learning dynamics to select clean samples \cite{zhou2021robust, maini2022characterizing}.

Various forms of \emph{Hard Label Noise} have been studied, including asymmetric noise~\citep{scott2013classification}, instance-dependent noise~\citep{xia2020part}, natural noise~\citep{wei2021learning}, adversarially crafted labels~\citep{zhang2024badlabel}, open-set noise~\citep{wei2021open}, and subclass-dominant noise~\cite{bai2023subclassdominant}. These noise are designed from the perspective of the labels, aiming to simulate challenging real-world scenarios or malicious attacks. 
In contrast to prior studies that mainly focus on different types of label noise, our work offers a fresh perspective by re-examining sample selection methods that rely on a model's early learning stages. We demonstrate that some samples hidden among those considered ``confident'' are, in fact, the most harmful. This contributes new insights into effectively identifying and handling mislabeled data.