\section{Introduction}
Deep Neural Networks (DNNs) have achieved remarkable success, while heavily relies on the availability of high-quality, accurately annotated data~\cite{han2020survey}. In practice, collecting large-scale datasets with precise labels is challenging due to the high costs involved and the inherent subjectivity of manual annotation processes. Consequently, datasets often contain noisy labels, which can degrade the generalization performance of DNNs—a problem known as learning with noisy labels (LNL) \cite{natarajan2013learning}.
One prevalent approach to address LNL is sample selection, which aims to identify confident samples for training while discarding potentially mislabeled ones. 

Sample selection methods can be categorized into two main types: loss-based and dynamics-based. Loss-based methods rely on the assumption that clean samples tend to have smaller loss values than mislabeled samples \cite{han2018co, liu2020early,xia2021sample, li2024regroup}. In contrast, dynamics-based methods exploit the memorization effect of DNNs, which suggests that DNNs learn simple patterns first and then gradually fit the assigned label for each particular minority instance, including mislabeled samples \cite{liu2020early, zhang2021understanding, yuan2024early}. By analyzing the learning dynamics of DNNs, these methods aim to identify clean samples that are learned early and consistently throughout the training process \cite{yu2019does, xia2020robust, bai2021me, wei2022self}, considering them as confident samples for training.
In recent years, dynamics-based methods have gained attention due to their ability to select Clean Hard Examples (CHEs)—challenging clean samples that are difficult to identify but crucial for achieving near-optimal generalization performance \cite{feldman2020neural, bai2021me, wei2022self, yuan2023late}.


\begin{figure*}[h]
\centering
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[scale=0.222]{earlycutting_fig1a.png}
    \vskip -0.1em
  \caption{}
  \label{fig1:sub1}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth} 
  \centering
  \includegraphics[scale=0.22]{earlycutting_fig1b.png}
  \vskip -0.1em
  \caption{}
  \label{fig1:sub2}
\end{subfigure}
  \vskip -1.1em
\caption{(a) Test accuracy curves when the originally clean training subset is augmented with 4000 \emph{Mislabeled Easy Examples} versus 4000 \emph{Mislabeled Hard Examples}. Adding Mislabeled Easy Examples leads to a larger decrease in the model’s generalization performance.
(b) Histogram illustrating the distribution of ImageNet-1k examples with 40\% symmetric label noise, showing the epoch at which each example is first correctly predicted by the model during training. The horizontal axis represents the epoch when examples are first correctly predicted, and the vertical axis represents the number of examples predicted correctly at each epoch.}
\label{fig1}
 \vskip -1.0em
\end{figure*}

Although these sample selection methods have achieved decent performance by relying on early training stages to minimize noise in the selected subset and adopting advanced strategies to retain CHEs, they often overlook that not all mislabeled examples harm the model's performance equally. Specifically, even with a low noise rate in the selected subset, the presence of certain mislabeled samples can still significantly impair the model's generalization performance. As shown in Figure~\ref{fig1:sub1}, we demonstrate that mislabeled samples which are correctly predicted by the model early in the training process disproportionately degrade performance. We refer to these easily learned and particularly harmful mislabeled samples as \emph{Mislabeled Easy Examples} (MEEs). In our analysis (see Section \ref{sec2.2}), we find that MEEs are often closer to the centers of their mislabeled classes in the feature space of classifiers trained in the early stages. This causes them to be easily and ``reasonably'' classified into the wrong classes during early training, thereby disrupting the model's early learning of simple patterns \cite{arpit2017closer}. Consequently, these examples are learned earlier and harm generalization more.


To address this issue, we propose a novel sample selection strategy called \emph{Early Cutting}, which introduces a recalibration step using the model's state at a later epoch to re-select the confident subset of samples identified during early learning. In this recalibration step, we identify samples that exhibit high loss yet are predicted with high confidence and demonstrate low sensitivity to input perturbations—characteristics indicative of MEEs. By further excluding these deceptive samples from the confident subset, we reduce MEEs negative impact on the model's generalization performance. Although this re-selection might result in the inadvertent removal of some clean samples, the impact is mitigated due to the nature of early-learned samples, which are abundant and often redundant representations of simple patterns. Removing a portion of these samples has a smaller detrimental effect compared to the significant harm caused by retaining MEEs.

We conduct extensive experiments on CIFAR \cite{krizhevsky2009learning}, WebVision \cite{li2017webvision}, and full ImageNet-1k \cite{deng2009imagenet} datasets with different types and levels of label noise. The results demonstrate that our proposed method consistently outperforms state-of-the-art sample selection methods across various settings. 


\textbf{Contributions.}
(1). We discover that mislabeled samples correctly predicted by the model early in training disproportionately harm model's performance; we define these samples as \emph{Mislabeled Easy Examples} (MEEs).

(2). We find that MEEs are closer to the centers of their mislabeled classes in the feature space of models in early training stages, causing the model to easily learn incorrect patterns, which explains their especially harmful effect.

(3). We propose a simple yet effective \emph{Early Cutting} method that recalibrates the confident subset selected during early training by leveraging the model from later stages.



\subsection{Related Work}
In this subsection, we briefly review the related work. Detailed review of the literature is given in Appendix \ref{appendix:A}.

\emph{Sample Selection} has been widely used in learning with noisy labels to improve the robustness of model training by prioritizing confident samples. An in-depth understanding of deep learning models, particularly their learning dynamics, has facilitated research in this area. Extensive studies on the \emph{Learning Dynamics} of DNNs have revealed that difficult clean examples are typically learned in the later stages of training \cite{arpit2017closer, toneva2018empirical, yuan2024early}. This insight has led to training-time metrics that quantify sample ``hardness'', such as forgetting \cite{toneva2018empirical}, example consistency \cite{pleiss2020identifying}, and learning speed \cite{jiang2021characterizing}. These metrics inspire methods that leverage learning dynamics to select clean samples \cite{zhou2021robust, maini2022characterizing}.

Various forms of \emph{Hard Label Noise} have been studied, including asymmetric noise~\citep{scott2013classification}, instance-dependent noise~\citep{xia2020part}, natural noise~\citep{wei2021learning}, adversarially crafted labels~\citep{zhang2024badlabel}, open-set noise~\citep{wei2021open}, and subclass-dominant noise~\cite{bai2023subclassdominant}. These noise are designed from the perspective of the labels, aiming to simulate challenging real-world scenarios or malicious attacks. 
In contrast to prior studies that mainly focus on different types of label noise, our work offers a fresh perspective by re-examining sample selection methods that rely on a model's early learning stages. We demonstrate that some samples hidden among those considered ``confident'' are, in fact, the most harmful. This contributes new insights into effectively identifying and handling mislabeled data.