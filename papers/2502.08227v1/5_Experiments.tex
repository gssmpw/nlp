\section{Experiments}
\subsection{Preliminary Presentation of Effectiveness}
\label{section:4_0}
We first provide empirical evidence to verify the effectiveness of \emph{Early Cutting}. Using CIFAR-10 with 40\% various synthetic label noise and ResNet-18 as the backbone, we compared our proposed \emph{Early Cutting} with \emph{loss-based} and \emph{dynamic-based} sample selection methods. Table \ref{tab1} shows that \emph{Early Cutting} consistently achieved the highest test accuracy across all noise types. Although \emph{Early Cutting} and the \emph{dynamic-based} method selected training subsets with similar noise rates, our approach's better performance indicates that focusing on filtering specific harmful mislabeled examples improves selection quality.
The last row shows the number of additional samples filtered by \emph{Early Cutting} and the high percentage of mislabeled samples among them, proving its effectiveness at identifying and removing noisy labels. As intuition, more challenging noise types result in more mislabeled samples being removed, leading to larger performance gains. Detailed settings in Appendix \ref{appendix:C}.

\subsection{Comparison with the Competitors}
\label{section:4_1}
\textbf{Competitors.}
We compare our approach with several state-of-the-art methods: robust loss functions including \textit{GCE} \cite{zhang2018generalized} and \textit{Student Loss} \cite{10412669}; robust training methods, including \textit{Co-teaching} \cite{han2018co} and \textit{CSGN} \cite{linlearning}; and sample selection methods, including \textit{Me-Momentum} \cite{bai2021me}, \textit{Self-Filtering} \cite{wei2022self}, \textit{Late Stopping} \cite{yuan2023late}, and \textit{RLM} \cite{li2024regroup}. 




\begin{table*}[!t]
\renewcommand{\arraystretch}{0.92}
\centering
	\caption{Test performance (mean$\pm$std) of each approach using ResNet-18 on CIFAR-10.}
	\label{tab2}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{5.0mm}{
\begin{tabular}{c|cccc}
\toprule

                      & \emph{Symmetric 20\%} & \emph{Symmetric 40\%} & \emph{Instance. 20\%} & \emph{Instance. 40\%} \\
                      \midrule
Cross-Entropy  &86.64 $\pm$ 0.18\% & 82.64 $\pm$ 0.29\% & 87.62 $\pm$ 0.09\% & 82.82 $\pm$ 0.37\% \\
GCE \cite{zhang2018generalized} & 91.50 $\pm$ 0.33\% & 87.02 $\pm$ 0.16\% & 89.42 $\pm$ 0.31\% & 83.10 $\pm$ 0.29\% \\
Co-teaching \cite{han2018co} & 89.13 $\pm$ 0.38\% & 82.29 $\pm$ 0.21\% & 89.42 $\pm$ 0.22\% & 81.91 $\pm$ 0.20\% \\
Me-Momentum \cite{bai2021me} & 92.76 $\pm$ 0.15\% & 90.75 $\pm$ 0.49\% & 91.87 $\pm$ 0.22\% & 88.80 $\pm$ 0.29\% \\
Self-Filtering \cite{wei2022self}  & 92.88 $\pm$ 0.22\% & 90.46 $\pm$ 0.28\% & 92.35 $\pm$ 0.13\% & 86.93 $\pm$ 0.14\% \\
Late Stopping \cite{yuan2023late}& 92.02 $\pm$ 0.17\% & 88.25 $\pm$ 1.01\% & 91.65 $\pm$ 0.26\% & 88.28 $\pm$ 0.24\% \\
RLM \cite{li2024regroup}   & 93.11 $\pm$ 0.29\% & 91.06 $\pm$ 0.17\% & 93.13 $\pm$ 0.05\% & 89.73 $\pm$ 0.32\% \\
Student Loss \cite{10412669} & 91.90 $\pm$ 0.37\% & 89.03 $\pm$ 0.32\% & 89.99 $\pm$ 0.50\% & 81.95 $\pm$ 0.51\% \\
CSGN \cite{linlearning}  & 90.09 $\pm$ 0.32\% & 87.71 $\pm$ 0.46\% & 89.45 $\pm$ 0.07\% & 88.50 $\pm$ 0.49\% \\
\midrule
\rowcolor{LightYellow} Early Cutting (Ours)  & \textbf{93.79 $\pm$ 0.14\%} & \textbf{91.80 $\pm$ 0.18\%} & \textbf{93.40 $\pm$ 0.22\%} & \textbf{90.78 $\pm$ 0.31\%} \\
\bottomrule  
\end{tabular}
}
 }
\end{table*}

\begin{table*}[!t]
\renewcommand{\arraystretch}{0.92}
\centering
\vskip -0.20in
	\caption{Test performance (mean$\pm$std) of each approach using ResNet-34 on CIFAR-100.}
	\label{tab3}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{5.0mm}{
\begin{tabular}{c|cccc}
\toprule

                      & \emph{Symmetric 20\%} & \emph{Symmetric 40\%} & \emph{Instance. 20\%} & \emph{Instance. 40\%} \\
                      \midrule
Cross-Entropy  &63.04 $\pm$ 0.41\% & 51.81 $\pm$ 0.33\% & 63.36 $\pm$ 0.22\% & 51.58 $\pm$ 0.96\% \\
GCE \cite{zhang2018generalized}  & 66.68 $\pm$ 0.35\% & 59.42 $\pm$ 0.19\% & 64.71 $\pm$ 0.15\% & 55.49 $\pm$ 0.34\% \\
Co-teaching \cite{han2018co} & 66.72 $\pm$ 0.26\% & 58.72 $\pm$ 0.43\% & 66.45 $\pm$ 0.28\% & 59.52 $\pm$ 0.32\% \\
Me-Momentum \cite{bai2021me}  & 71.94 $\pm$ 0.27\% & 67.36 $\pm$ 0.30\% & 72.47 $\pm$ 0.39\% & 63.99 $\pm$ 0.56\% \\
Self-Filtering \cite{wei2022self}  & 70.18 $\pm$ 0.39\% & 66.92 $\pm$ 0.18\% & 69.52 $\pm$ 0.38\% & 66.76 $\pm$ 0.42\% \\
Late Stopping \cite{yuan2023late} & 71.09 $\pm$ 0.71\% & 65.43 $\pm$ 0.50\% & 70.32 $\pm$ 0.06\% & 61.71 $\pm$ 0.25\% \\
RLM \cite{li2024regroup}   & 71.68 $\pm$ 0.32\% & 67.68 $\pm$ 0.36\% & 68.26 $\pm$ 0.37\% & 67.31 $\pm$ 0.64\% \\
Student Loss \cite{10412669}  & 69.04 $\pm$ 0.19\% & 64.21 $\pm$ 0.49\% & 67.62 $\pm$ 0.67\% & 56.24 $\pm$ 0.24\% \\
CSGN \cite{linlearning}  & 69.89 $\pm$ 0.22\% & 56.18 $\pm$ 0.36\% & 71.97 $\pm$ 0.10\% & 65.43 $\pm$ 0.52\% \\
\midrule
\rowcolor{LightYellow} Early Cutting (Ours)  & \textbf{76.20 $\pm$ 0.27\%} & \textbf{72.77 $\pm$ 0.17\%} & \textbf{75.03 $\pm$ 0.23\%} & \textbf{69.94 $\pm$ 0.30\%} \\
\bottomrule  
\end{tabular}
}
}
\end{table*}

\begin{table*}[!t]
\renewcommand{\arraystretch}{0.95}
\centering
\vskip -0.20in
	\caption{Test performance (mean$\pm$std) of each approach using ResNet-18 and ResNet-34 on CIFAR-N.}
	\label{tab4}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{2.6mm}{
\begin{tabular}{c|ccccc}
\toprule

                      & \emph{10N Random 1} & \emph{10N Random 2} & \emph{10N Random 3} & \emph{10N Worst} & \emph{100N Fine} \\
                      \midrule
Cross-Entropy   &86.16 $\pm$ 0.14\% & 85.74 $\pm$ 0.28\% & 85.91 $\pm$ 0.14\% & 80.00 $\pm$ 0.42\% & 54.53 $\pm$ 0.13\%  \\
Late Stopping \cite{yuan2023late} & 89.71 $\pm$ 0.73\% & 90.23 $\pm$ 0.37\% & 90.49 $\pm$ 0.31\% & 86.10 $\pm$ 0.41\% & 57.32 $\pm$ 0.19\%  \\
RLM \cite{li2024regroup}  & 92.21 $\pm$ 0.37\% & 92.27 $\pm$ 0.31\% & 92.07 $\pm$ 0.72\% & 86.25 $\pm$ 0.24\% & 57.90 $\pm$ 0.33\%  \\
Student Loss \cite{10412669}
& 90.60 $\pm$ 0.07\% & 90.44 $\pm$ 0.28\% & 90.44 $\pm$ 0.35\% & 86.16 $\pm$ 0.31\% & 58.55 $\pm$ 0.53\%  \\
CSGN \cite{linlearning}  & 89.14 $\pm$ 0.23\% & 89.49 $\pm$ 0.25\% & 89.25 $\pm$ 0.31\% & 82.88 $\pm$ 0.51\% & 58.13 $\pm$ 0.49\%  \\
\midrule
\rowcolor{LightYellow} Early Cutting (Ours)  & \textbf{92.50 $\pm$ 0.14\%} & \textbf{92.65 $\pm$ 0.11\%} & \textbf{92.36 $\pm$ 0.43\%} & \textbf{87.43 $\pm$ 0.13\%} & \textbf{66.52 $\pm$ 0.22\%}  \\
\bottomrule  
\end{tabular}
}
 }
\end{table*}


\begin{table*}[!t]
\renewcommand{\arraystretch}{0.9}
\centering
\vskip -0.20in
	\caption{Test performance of each approach using ResNet-50 on large-scale naturalistic datasets.}
	\label{tab5}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{3.95mm}{
\begin{tabular}{c|cc|c}
\toprule

                        & \emph{WebVision Validation} & \emph{ILSVRC12 Validation}
                       & \emph{ Full ImageNet-1k (Sym. 40\%)}\\
                      \midrule
Cross-Entropy   & 67.32\% & 63.84\% &   67.99\%   \\
Late Stopping \cite{yuan2023late}  & 71.56\% & 68.32\% &  71.42\%   \\
RLM \cite{li2024regroup}   & 72.28\% & 69.86\% &  68.95\%  \\
Student Loss \cite{10412669}  & 69.80\% & 67.62\% &  69.44\%   \\
CSGN \cite{linlearning}    & 72.32\% & 69.52\% &  40.68\%   \\
\midrule
\rowcolor{LightYellow} Early Cutting (Ours) & \textbf{73.81\%} & \textbf{71.20\%} &  \textbf{73.28\%}   \\
\bottomrule  
\end{tabular}
}
 }
 \vskip -0.20in
\end{table*}

\textbf{Datasets and implementation.}
We conducted experiments on several benchmark datasets to evaluate our proposed method compare with above competitors. For synthetic noise experiments, we used \emph{CIFAR-10} and \emph{CIFAR-100} \cite{krizhevsky2009learning}, adding \emph{symmetric} and \emph{instance-dependent} label noise at rates of 20\% and 40\% following standard protocols \cite{bai2021me, yuan2023late}. We split 10\% noisy trianing data for validation. For real-world noisy labels, we utilized \emph{CIFAR-N} \cite{wei2021learning}, as well as the large-scale \emph{WebVision} dataset \cite{li2017webvision}. Following previous work \cite{linlearning, li2024regroup}, we used the first 50 classes of the \emph{WebVision} dataset and validated on both the \emph{WebVision} validation set and the \emph{ILSVRC12} \cite{ILSVRC15} validation set. We further confirmed the scalability of our proposed method on the full \emph{ImageNet-1K} \cite{deng2009imagenet} with 40\% synthetic symmetric label noise. Training was performed using \emph{SGD} with a momentum of 0.9 and a weight decay of $5 \times 10^{-4}$. The initial learning rate was set to 0.1 and decayed using a cosine annealing schedule. Models were trained (for the final iteration) for 300 epochs on the \emph{CIFAR} datasets, for 200 epochs on \emph{WebVision}, and for 150 epochs on full \emph{ImageNet-1k}. We re-implemented all competitor methods with consistent settings (unless otherwise specified). Detailed settings are provided in Appendix \ref{appendix:C}.

\begin{table*}[!t]
\renewcommand{\arraystretch}{0.95}
\centering
	\caption{Test accuracy comparison of different approaches using semi-supervised learning.}
	\label{tab6}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{3.8mm}{
\begin{tabular}{cc|cc|cc}
\toprule
&&\multicolumn{2}{c|}{\emph{CIFAR-10}} & \multicolumn{2}{c}{\emph{CIFAR-100}}\\
\cmidrule(lr){3-4}\cmidrule(lr){5-6}

                      Methods & SSL & \emph{Symmetric 50\%} & \emph{Instance. 40\%}  & \emph{Symmetric 50\%} & \emph{Instance. 40\%} \\
                      \midrule
Early Cutting (Ours) & - & 90.3\% & 90.7\% & 69.6\% & 69.9\%   \\
CORES$^{2*}$ \cite{cheng2020learning} & UDA & 93.1\% & 92.2\% & 73.1\% & 71.9\%   \\
Divide-Mix \cite{li2020dividemix} & MixMatch  & 94.6\% & 93.0\% & 74.6\% & 71.7\%   \\
ELR+  \cite{liu2020early} & MixMatch  & 93.8\% & 92.2\% & 72.4\% & 72.6\%   \\
SFT+ \cite{wei2022self}  & MixMatch & 94.9\% & 94.1\% & 75.2\% & 74.6\%   \\
RLM+ \cite{li2024regroup}  & MixMatch & 95.1\% & 94.8\% & 72.9\% & 72.8\%   \\
\midrule
\rowcolor{LightYellow} Early Cutting+ (Ours)  & MixMatch & \textbf{95.8\%} & \textbf{95.5\%} & \textbf{75.6\%} & \textbf{75.4\%} \\
\bottomrule  
\end{tabular}
}
}
 \vskip -0.3em
\end{table*}

\begin{figure*}[h]
\centering
\begin{subfigure}[b]{0.49\textwidth} 
  \centering
  \includegraphics[scale=0.502]{ec_cifar10.png}
    \vskip -0.1em
  \caption{\ CIFAR-10 with 40\% symmetric label noise}
  \label{fig5:sub1}
\end{subfigure}%
\begin{subfigure}[b]{0.49\textwidth} 
  \centering
  \includegraphics[scale=0.502]{ec_cifar100.png}
  \vskip -0.1em
  \caption{\ CIFAR-100 with 40\% symmetric label noise}
  \label{fig5:sub2}
\end{subfigure}
  \vskip -1.1em
\caption{Sensitivity analysis of hyperparameters on CIFAR-10 and CIFAR-100 with 40\% symmetric label noise. In each subfigure, the left plot shows test accuracy versus thresholds for the large loss, high confidence, and low gradient norm criteria, scaled by factors of \( \frac{1}{4} \), \( \frac{1}{2} \), \(1\), \(2\), and \(4\). The right plot shows test accuracy versus Early Cutting rate \(\gamma\) set to \( n^{1/3} \), \( n^{1/2} \), \( n \), \( n^{2} \), and \( n^{3} \), where \( \gamma \geq 1 \).}
\label{fig5}
 \vskip -0.8em
\end{figure*}

\textbf{Discussions on experimental results.}
As shown in Tables~\ref{tab2}, \ref{tab3}, \ref{tab4}, and \ref{tab5}, our proposed \emph{Early Cutting} method consistently achieves outstanding performance across various datasets and noise conditions. On standard benchmarks like \emph{CIFAR}, it attains the highest test accuracy regardless of the type of label noise—symmetric, instance-dependent, or real-world—and across different noise rates. Notably, it performs exceptionally well on \emph{CIFAR-100}, which has a larger number of classes, indicating strong robustness in handling label noise in fine-grained classification tasks.
Without further fine-tuning of hyperparameters, \emph{Early Cutting} also achieves significant performance improvements on large-scale datasets such as \emph{WebVision} and full \emph{ImageNet-1k}. This demonstrates the practicality and scalability of our method in handling challenging scenarios.
By iteratively selecting confident samples and removing harmful mislabeled easy examples, our method helps the model learn from reliable data while avoiding overconfidence in early-learned samples. 
Training time evaluation is provided in Appendix~\ref{appb7}, showing that our method achieves better performance with similar time overhead to the iterative sample selection methods.


\subsection{Further Analysis}
\label{section:4_3}
\textbf{Semi-supervised learning.}
To further evaluate the effectiveness of our \emph{Early Cutting} method, we integrated it with the \emph{MixMatch} semi-supervised learning framework \cite{berthelot2019mixmatch}, resulting in \emph{Early Cutting+}. We treat the confident samples obtained from sample selection as labeled data, and the samples removed from training in the fully supervised setting as unlabeled data. We compared \emph{Early Cutting+} with advanced SSL-based LNL methods, including CORES$^{2*}$ \cite{cheng2020learning}, DivideMix \cite{li2020dividemix}, and ELR+ \cite{liu2020early}; additionally, we compared with the latest sample selection methods integrating SSL, SFT+ \cite{wei2022self} and RLM+ \cite{li2024regroup}. Some baseline results are taken from \citet{wei2022self}.
As shown in Table~\ref{tab6}, our \emph{Early Cutting+} achieves the highest test accuracy on both \emph{CIFAR-10} and \emph{CIFAR-100} under 50\% symmetric and 40\% instance-dependent label noise, surpassing previous methods. These results underscore the capability of \emph{Early Cutting} in selecting high-quality subsets of training samples, demonstrating the scalability of the \emph{Early Cutting} method.

\textbf{Sensitivity analysis.}
We conducted sensitivity analyses to evaluate the robustness of our \emph{Early Cutting} method. As shown in Figure~\ref{fig5}, our method achieves optimal test accuracy on both \emph{CIFAR-10} and \emph{CIFAR-100} with 40\% symmetric label noise when using the same default thresholds (also used for \emph{WebVision} and full \emph{ImageNet-1k}) for identifying samples with large loss, high confidence, low gradient norm, and early cutting rate. Notably, even when the hyperparameters vary over a wide range, our method exhibits minimal sensitivity, with only slight ($<1\%$) performance degradation. Results indicate that our method is robust and effective across different datasets without requiring extensive hyperparameter tuning.

