\section{Related Work}
In this subsection, we briefly review the related work. Detailed review of the literature is given in Appendix \ref{appendix:A}.

\emph{Sample Selection} has been widely used in learning with noisy labels to improve the robustness of model training by prioritizing confident samples. An in-depth understanding of deep learning models, particularly their learning dynamics, has facilitated research in this area. Extensive studies on the \emph{Learning Dynamics} of DNNs have revealed that difficult clean examples are typically learned in the later stages of training **Srivastava et al., "Improving Deep Neural Networks through Selective Data Augmentation"**. This insight has led to training-time metrics that quantify sample ``hardness'', such as forgetting **Keskar et al., "Learning Over-Specification with Stochastic Gradient Descent"**, example consistency **Mnih et al., "Human-Level Control Through Deep Reinforcement Learning"**, and learning speed **Goyal et al., "Accurate, Large Minibatch SGD: Training ImageNet Classifiers Aggressively"**. These metrics inspire methods that leverage learning dynamics to select clean samples **Brock et al., "Large Scale GAN Training for High Fidelity Natural Image Synthesis"**.

Various forms of \emph{Hard Label Noise} have been studied, including asymmetric noise **Neyshabur et al., "The Role of Over-Parameterization in Generalization of Neural Networks"**, instance-dependent noise **Arjovsky et al., "Wasserstein GAN"**, natural noise **Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**, adversarially crafted labels **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**, open-set noise **Li et al., "Learning to Inpaint with Large-Scale, Unlabeled Data"**, and subclass-dominant noise **Shrivastava et al., "Learning from Simulated and Unsupervised Images through Solving a Rao-Blackwellized Information Maximization Game"**. These noise are designed from the perspective of the labels, aiming to simulate challenging real-world scenarios or malicious attacks. 
In contrast to prior studies that mainly focus on different types of label noise, our work offers a fresh perspective by re-examining sample selection methods that rely on a model's early learning stages. We demonstrate that some samples hidden among those considered ``confident'' are, in fact, the most harmful. This contributes new insights into effectively identifying and handling mislabeled data.