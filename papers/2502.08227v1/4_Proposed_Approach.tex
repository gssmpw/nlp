\begin{table*}[h]
\renewcommand{\arraystretch}{0.92}
\centering
\caption{Training on 60\% noisy training samples selected by each method. Test accuracy (\emph{noise rates in selected training samples}).}
\label{tab1}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{3.0mm}{
\begin{tabular}{c|cccc}
\toprule

                       & \emph{Symmetric 40\%} & \emph{Asymmetric 40\%}  & \emph{Pairflip 40\%} & \emph{Instance. 40\%} \\
                      \midrule
                      
\rowcolor{LightYellow} \emph{Loss-based} Selection &83.01\% (\emph{10.44\%}) & 83.79\% (\emph{4.84\%}) & 84.16\% (\emph{10.88\%}) & 82.87\% (\emph{11.11\%})  \\                  
\rowcolor{LightYellow} \emph{Dynamic-based} Selection &89.39\% (\emph{4.57\%}) & 84.28\% (\emph{3.37\%}) & 84.71\% (\emph{10.19\%}) & 83.12\% (\emph{12.52\%})  \\ 
\rowcolor{LightYellow} + \emph{Early Cutting} Selection &\textbf{89.66\% (\emph{4.94\%})} & \textbf{84.85\% (\emph{3.33\%})} &\textbf{85.88\% (\emph{9.52\%})} &\textbf{84.31\% (\emph{12.06\%})}  \\ 
\midrule
\midrule
\rowcolor{LightGreen} Additional Samples Filtered by \emph{Early Cutting} &98 (\emph{56.12\%}) &191 (\emph{95.29\%})  & 161 (\emph{45.96\%}) & 300 (\emph{91.33\%}) \\
\bottomrule  
\end{tabular}
}
 }
 \vskip -1.2em
\end{table*}

\section{Methodology}

Based on the analysis above, MEEs—mislabeled samples that the model learns easily during early training stages—can have a disproportionately negative impact on model generalization performance. Previous methods \citep{liu2020early, bai2021me} often rely on trusting the model's early learning stages or focusing on samples with small loss, are ineffective at filtering out MEEs due to their deceptive nature.
To mitigate the influence of MEEs, we propose a novel sample selection strategy called \emph{Early Cutting}. This method leverages insights from the model at a later training stage—specifically, at the early stopping point \( t \) where overfitting begins to affect performance—to recalibrate the subset of samples selected during early learning. To select the confident subset \( \mathcal{D}^s \) at first, we build upon the concept of \emph{learning time} \( L_i \) as defined in Eq.~(\ref{eq1}). By selecting samples that are learned early by the model (i.e., those with small \( L_i \)), the method aim to capture the clean and easy-to-learn examples while include clean hard examples, which is another key challenge in sample selection.

Formally, consider \( \mathcal{D}^s = \{(\mathbf{x}_i, y_i) \}_{i=1}^N \), where \( \mathbf{x}_i \in \mathbb{R}^d \) represents input features, and \( y_i \in \{1, 2, \dots, K \} \) denotes the corresponding labels from \( K \) classes. Let \( f_{\theta^t}(\mathbf{x}) \) be a model parameterized by \( \theta^t \) at the early stopping epoch \( t \), generating class probabilities via the softmax function:
\begin{equation}
\mathbf{p}_i = f_{\theta^t}(\mathbf{x}_i) = \left[ p_i^{(1)},\ p_i^{(2)},\ \dots,\ p_i^{(K)} \right],
\end{equation}
where \( p_i^{(k)} = \Pr(y = k \,\vert\, \mathbf{x}_i; \theta^t) \) and \( \sum_{k=1}^{K} p_i^{(k)} = 1 \).

The predicted label \( \hat{y}_i \) and the prediction confidence \( c_i \) are given by: \( \hat{y}_i = \arg\max_{k} \, p_i^{(k)} \), and \( c_i = p_i^{(\hat{y}_i)} \).
The cross-entropy loss for sample \( (\mathbf{x}_i, y_i) \) is: \( L_i = -\log p_i^{(y_i)} \).


While the selected subset \( \mathcal{D}^s \) tends to retain a high-quality set of clean samples, it may still include MEEs due to their deceptive nature. To address this issue, we leverage the model's parameters \( \theta^t \) at a later training stage \( t \) to identify and remove suspicious samples from \( \mathcal{D}^s \). Specifically, we define a set of suspicious samples \( \mathcal{S} \) within \( \mathcal{D}^s \) based on the criteria of high loss and high confidence:
\begin{equation}
\mathcal{S} = \left\{ i \in \mathcal{D}^s \ \big| \ L_i > \delta,\quad c_i > \tau \right\},
\end{equation}
where \( \delta \) and \( \tau \) are thresholds for the loss and confidence, respectively. The rationale is that a high loss \( L_i \) indicates that the model's prediction at epoch \( t \) disagrees with the given label \( y_i \), and a high confidence \( c_i \) implies that the model is very certain about its (contradictory) prediction \( \hat{y}_i \ne y_i \). Therefore, samples satisfying both conditions are likely to be mislabeled, even if they were learned early.

Relying solely on loss and confidence may not be sufficient, as some hard-to-learn samples may also exhibit high loss and high confidence due to their intrinsic difficulty. To further refine our selection, we introduce the concept of gradient stability. We compute the Euclidean norm of the gradient of the loss \( L_i \) with respect to the input \( \mathbf{x}_i \):
\begin{equation}
\nabla_{\mathbf{x}_i} L_i = \frac{\partial L_i}{\partial \mathbf{x}_i}, \quad g_i = \left\| \nabla_{\mathbf{x}_i} L_i \right\|_2.
\end{equation}
A small gradient norm \( g_i \) indicates that the loss \( L_i \) is insensitive to small perturbations in \( \mathbf{x}_i \), suggesting a strong (but potentially incorrect) association between the input features and the predicted label. MEEs tend to have low gradient norms because the model has confidently mislearned them, making the loss stable even under input perturbations.
We refine \( \mathcal{S} \) by selecting samples with high gradient stability:
\begin{equation}
\mathcal{S}' = \left\{ i \in \mathcal{S} \,\Big|\, g_i < \epsilon \right\},
\end{equation}
where \( \epsilon \) is a small threshold. By removing \( \mathcal{S}' \) from \( \mathcal{D}^s \), we obtain the final subset of samples with reduced MEEs.

In practice, to avoid hyperparameter searching, we employ a method of sorting and selecting a fixed proportion of samples based on \( L_i \), \( c_i \), and \( g_i \), which circumvents the need to choose specific threshold values. Notably, the above selection is based on the samples \( \mathcal{D}^s \) chosen during the model's early learning stages. These samples are often considered to contain a large number of redundant, similar examples representing dominant patterns \cite{toneva2018empirical, feldman2020does, feldman2020neural,yuan2024early}. Therefore, this screening is considered robust to clean samples and insensitive to the scale of selection.
A sensitivity analysis is presented in Section~\ref{section:4_3}, and the detailed method algorithm is provided in Appendix~\ref{appendix:B}.

\label{sec3}
