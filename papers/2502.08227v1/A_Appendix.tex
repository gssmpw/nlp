\section{Related Work}
\label{appendix:A}
\textbf{Learning with Noisy Labels (LNL)} has been an active research area in recent years \cite{li2024transferring, wang2024tackling, xia2023regularly, xia2022extended, 9784878, 10689264, baek2024sam, englesson2024robust}, with numerous methods proposed to mitigate the impact of label noise on deep neural networks (DNNs). Formally, let \( X \) denote the input space, and let \( Y = \{1, 2, \dots, C\} \) be the set of clean labels. Consider the clean data distribution \( P(X, Y) \), from which clean samples \( (\mathbf{x}, y) \) are drawn. In practice, we often have access only to a training dataset with potentially noisy labels:
\begin{equation}
\tilde{D} = \{(\mathbf{x}_i, \tilde{y}_i)\}_{i=1}^n,
\end{equation}
where \( \mathbf{x}_i \in X \) and \( \tilde{y}_i \in Y \) are observed noisy labels. The aim is to learn a robust classifier \( f: X \rightarrow Y \) parameterized by \( \theta \), which performs well on clean test data drawn from the distribution \( P(X, Y) \).
The noise process is typically modeled using a noise transition matrix \( T \in \mathbb{R}^{C \times C} \), defined as:
\begin{equation}
T_{ij} = P(\tilde{y} = j \mid y = i), \quad \text{for } i, j \in \{1, 2, \dots, C\},
\end{equation}
which represents the probability that a clean label \( y = i \) is flipped to a noisy label \( \tilde{y} = j \). The relationship between the clean and noisy label distributions can be expressed as:
\begin{equation}
P(\tilde{y} \mid \mathbf{x}) = \sum_{k=1}^C T_{k\tilde{y}} \, P(y = k \mid \mathbf{x}).
\end{equation}
In the context of deep learning, the classifier \( f_\theta(\mathbf{x}) \) is often trained by minimizing the empirical risk over the noisy dataset:
\begin{equation}
\min_{\theta} \frac{1}{n} \sum_{i=1}^n \ell\left(f_\theta(\mathbf{x}_i), \tilde{y}_i\right),
\end{equation}
where \( \ell(\cdot, \cdot) \) is a loss function, such as the cross-entropy loss:
\begin{equation}
\ell\left(f_\theta(\mathbf{x}), \tilde{y}\right) = -\log \left( f_\theta^{(\tilde{y})}(\mathbf{x}) \right),
\end{equation}
and \( f_\theta^{(\tilde{y})}(\mathbf{x}) \) denotes the predicted probability for class \( \tilde{y} \). However, due to label noise, directly minimizing this loss can lead to the model overfitting to noisy labels, degrading its performance on clean data. To address this issue, various strategies have been proposed.
In the following discussion, we focus on heuristic methods, specifically sample selection techniques, which do not rely on the explicit estimation of \( T \) but instead incorporate strategies to mitigate the impact of noisy labels.

\textbf{Sample selection strategies.}
Sample selection has been widely used in learning with noisy labels to improve the robustness of model training by prioritizing confident samples. An in-depth understanding of deep learning models, particularly their learning dynamics, has facilitated research in this area. Extensive studies on the \textit{learning dynamics} of DNNs have revealed that difficult clean examples are typically learned in the later stages of training \cite{arpit2017closer, toneva2018empirical, lin2024on}. 

In general, sample selection methods assign a statistical characteristic to each sample and select a subset of samples that fall below a certain threshold \cite{han2018co}. The selection indicator function \( s_i \) is defined as:
\begin{equation}
s_i = \begin{cases}
1, & \text{if } \ell\left(f_\theta(\mathbf{x}_i), \tilde{y}_i\right) \leq \tau, \\
0, & \text{otherwise},
\label{eq11}
\end{cases}
\end{equation}
where \( \tau \) is a dynamically adjusted threshold. The training objective becomes:
\begin{equation}
\min_{\theta} \frac{1}{\sum_{i=1}^n s_i} \sum_{i=1}^n s_i \ell\left(f_\theta(\mathbf{x}_i), \tilde{y}_i\right).
\end{equation}
A common approach is the small-loss trick, by focusing on low-loss samples, the model is less influenced by potentially mislabeled data. Methods like Co-teaching \cite{han2018co}, Co-teaching+ \cite{yu2019does}, JoCoR \cite{wei2020combating}, and Co-learning \cite{tan2021co} utilize two networks trained in parallel that teach each other using reliable samples. SELF \cite{nguyen2019self} identifies clean samples by checking the consistency between network predictions and given labels, while DivideMix \cite{li2020dividemix} employs a two-component mixture model to separate the training data into clean and noisy groups.
Moreover, ELR \cite{liu2020early} avoid overfitting to noisy labels by relying on early-learning.


\textbf{Learning dynamics reaearch for sample selection.}
The intriguing generalization ability of modern DNNs has motivated extensive studies on their learning dynamics, which in turn has inspired a series of sample selection criteria using in Eq.(\ref{eq11}) based on these dynamics. Studies have revealed that hard and mislabeled examples are typically learned during the later stages of training \cite{arpit2017closer, toneva2018empirical, song2019does, song2021robust, maini2022characterizing, bai2021understanding, lin2024on}. This empirical observation has led to the development of various training-time metrics to quantify the ``hardness'' of examples, such as forgetting events \cite{toneva2018empirical}, example consistency \cite{pleiss2020identifying}, and learning speed \cite{maini2022characterizing, jiang2021characterizing}. These metrics have inspired LNL approaches that leverage learning dynamics to select clean samples.
Methods like Self-Filtering \cite{wei2022self}, FSLT \& SSFT \cite{maini2022characterizing}, SELFIE \cite{song2019selfie}, and RoCL \cite{zhou2021robust} adopt criteria to identify clean samples based on their learning dynamics. The success of learning dynamics-based sample selection criteria in identifying high-confidence clean samples has driven researchers to further refine these strategies.
By identifying a larger subset of clean samples for model training, the generalization performance of the trained model can be improved. \cite{xia2021sample} discovered that using loss alone to select CHEs is suboptimal. RLM \cite{li2024regroup} obtain robust loss estimation for noisy samples.

An advanced paradigm for sample selection involves a positive feedback loop: iteratively optimizing the classifier and updating the training set. Under this loop, the model's performance gradually improves, leading to better sample selection capabilities and, consequently, an enhanced ability to select clean hard examples. Me-Momentum \cite{bai2021me} and Late Stopping \cite{yuan2023late} employ similar positive feedback loops to iteratively update the model parameters and the training set, gradually improving the model's performance on noisy data.

\textbf{Hard label noise.}
Various forms of \textit{hard label noise} have been studied, including asymmetric noise~\cite{scott2013classification}, instance-dependent noise~\cite{xia2020part}, natural noise~\cite{wei2021learning}, adversarially crafted labels~\cite{zhang2024badlabel}, open-set noise~\cite{wei2021open}, and subclass-dominant noise~\cite{bai2023subclassdominant}. These noise types are designed from the perspective of the labels, aiming to simulate challenging real-world scenarios or malicious attacks. 
Recent work has also explored the impact of label noise in specific data distributions. For instance, H2E \cite{yi2022identifying} and TABASCO \cite{lu2023label} focus on the challenges posed by label noise in long-tailed distributions, where minority classes are more susceptible to mislabeling. NoiseCluster \cite{bai2023subclassdominant} introduces the concept of subclass-dominant label noise, where mislabeled examples dominate at least one subclass, leading to suboptimal classifier performance.

\textbf{Our contributions.}
In contrast to prior studies that mainly focus on different types of label noise or sample selection based on learning dynamics, our work offers a fresh perspective by re-examining sample selection methods that rely on a model's early learning stages. We demonstrate that some samples hidden among those considered ``confident'' are, in fact, the most harmful when mislabeled. Specifically, we systematically investigate the detrimental impact of \textit{Mislabeled Easy Examples (MEEs)}â€”mislabeled samples that are correctly predicted by the model early in the training process.
This insight challenges the conventional assumptions of existing methods, which often prioritize samples learned early in training as being clean. Our findings highlight the need for a more cautious approach when selecting samples based on early learning confidence. By adopting a refined sample selection criterion that accounts for the potential harm of MEEs, we can seamlessly integrate this approach with existing sample selection method \cite{yuan2023late} to further boost it performance.

\clearpage

\section{Detailed Settings}
\label{appendix:C}

\subsection{Datasets}

\emph{CIFAR-10} and \emph{CIFAR-100} \cite{krizhevsky2009learning} are standard image classification datasets consisting of $32 \times 32$ color images. Both datasets were divided into 50,000 training images and 10,000 test images.
\emph{CIFAR-N} \cite{wei2021learning} is a version of CIFAR-10 and CIFAR-100 with real-world noisy labels collected from Amazon Mechanical Turk. These datasets simulate real-world scenarios where labels may be noisy due to human error.
We used a consistent 90\%-10\% data splits for training and validation across runs in all competitors.


\emph{WebVision} \cite{li2017webvision} is a large-scale dataset containing over 2.4 million web images crawled from the internet. It covers the same 1,000 classes as the \emph{ILSVRC12} \emph{ImageNet-1K} dataset \cite{deng2009imagenet} but includes noisy labels due to the automatic collection process.
\emph{ILSVRC12} \emph{ImageNet-1K} \cite{deng2009imagenet} is a large-scale dataset of natural images with 1,000 classes. We used it to assess the scalability of our method on real-world data with synthetic noise.

\subsection{Noise Settings}

In preliminary presentation of our proposed method's effectiveness (Table \ref{tab1}), we tested four types of synthetic label noise. For \emph{Symmetric Noise}, each label has a fixed probability $r$ of being uniformly flipped to any other class.
\emph{Asymmetric Noise} flips labels to similar but incorrect classes, mimicking mistakes that might occur in real-world classification tasks. 
\emph{Pairflip Noise} involves flipping labels to a specific incorrect class in a pairwise manner.
\emph{Instance-Dependent Noise} \cite{xia2020part} is a more challenging setting where the probability of label corruption depends on the instance features. It reflects more realistic scenarios where difficult or ambiguous examples are more likely to be mislabeled.

Following prior practices \cite{bai2021me, yuan2023late}, we primarily focused on \emph{Symmetric} and \emph{Instance-Dependent} noise types in our baseline comparisons (Table \ref{tab2} and \ref{tab3}), as they are the most common and challenging synthetic noise settings used to evaluate robustness methods. We experimented with noise rates of 20\% and 40\% to assess our method's performance under varying noise intensities. 
For the \emph{CIFAR-N} task, we utilized the provided noisy labels.

\subsection{Model Architectures}

We employed variants of the ResNet architecture \cite{he2016deep} in all our experiments, training each model from scratch. Specifically, we used \emph{ResNet-18} for \emph{CIFAR-10}, \emph{ResNet-34} for \emph{CIFAR-100}, and \emph{ResNet-50} for \emph{WebVision} and \emph{ImageNet-1K} datasets. This selection aligns with previous works and provides appropriate model capacity relative to each dataset.

\subsection{Training Procedures and Hyperparameters}

Training was performed using \emph{Stochastic Gradient Descent (SGD)} with a momentum of 0.9 and a weight decay of $5 \times 10^{-4}$. The initial learning rate was set to 0.1 and decayed using a cosine annealing schedule without restarts, decreasing to $1 \times 10^{-5}$ over the course of training. 
The number of training epochs was set to 300 for \emph{CIFAR}, 200 for \emph{WebVision}, and 150 for full \emph{ImageNet-1K} experiments. Batch sizes were set to 32 for \emph{CIFAR} datasets and \emph{WebVision}, and 256 for \emph{ImageNet-1K}.

To enhance the robustness of our sample selection model, we also incorporated certain strategies from prior works \cite{linlearning, li2024regroup}, training two networks and each network learn from the other's soft predictions and utilizing exponential moving averages to stabilize training. Weak data augmentation techniques were applied during training to improve generalization. These included random cropping with a padding of 4 pixels, random horizontal flipping, and normalization using the dataset-specific mean and standard deviation. 


\subsection{Sample Selection Mechanism}

Building upon the \emph{Late Stopping} strategy \cite{yuan2023late}, we iteratively select a confident subset $\mathcal{D}^s$ of training samples, progressively reducing mislabeled data and enhancing the model's focus on clean samples.
We identify early-learned samples based on their \emph{learning times}. For each sample $(\mathbf{x}_i, y_i)$, we define its learning time $L_i$ as the earliest epoch when the model's prediction stabilizes:
\begin{equation}
L_i = \min \left\{ T_i \ \big| \ \hat{y}_i^{T_i-2} = \hat{y}_i^{T_i-1} = \hat{y}_i^{T_i} = y_i \right\},
\label{eq16}
\end{equation}
where $\hat{y}_i^t$ denotes the model's predicted label at epoch $t$.

To further address the issue of \emph{Mislabeled Easy Examples} (MEEs), we introduce an \emph{Early Cutting} step in the training loop. We first select candidates using an \emph{Early Cutting Rate} $\gamma$ of 1.5, which corresponds to selecting the earliest $\approx \frac{2}{3}$ of samples learned. Within these candidates, we remove samples that meet all three of the following criteria (detailed in Section \ref{sec3}). First, we consider samples with high loss, specifically those within the top 10\% of loss values $L_i$. Second, we look at samples with high prediction confidence, namely those within the top 20\% of confidence scores $c_i$. Third, we identify samples with low gradient norms, that is, those within the bottom 20\% of gradient norms $g_i$. By removing samples that satisfy all three conditions, we aim to eliminate MEEs that the model has confidently mislearned early on.

The refined subset $\mathcal{D}'^s$ is then used for subsequent training. We repeat the sample selection process for a total of $I_{\text{rate}}$ rounds (set to 3), progressively improving data quality and model performance. The proportion of $\mathcal{D}^s$ retained in each round is calculated to achieve an overall retention rate equal to the complement of the noise rate after $I_{\text{rate}}$ rounds. For example, with a noise rate of 40\% (aiming to retain 60\% of the data), the retention rate per round is $(60\%)^{1/3} \approx 84\%$.

\subsection{Algorithm}
\label{appendix:B}

\begin{algorithm}[h]
\caption{Iterative Sample Selection with Early Cutting}\label{alg:proposed_method}
\begin{algorithmic}[1]
\REQUIRE Training data $\mathcal{D}^0$; Number of iterations $I_{\text{rate}}$; Early cutting rate $\gamma$
\ENSURE Trained model parameters $\theta$
\FOR{$i = 1$ to $I_{\text{rate}}$}
    \STATE \emph{1. Base Sample Selection:}
    \FORALL{samples $(\mathbf{x}_j, y_j) \in \mathcal{D}^{i-1}$}
        \STATE Compute learning time $L_j$ using Eq.~(\ref{eq16})
        \STATE Select subset $\mathcal{D}^s$ using learning time $L_j$
    \ENDFOR
    \STATE \emph{2. Early Cutting:}
    \FORALL{samples $(\mathbf{x}_j, y_j) \in \mathcal{D}^s$}
        \STATE Select subset $\mathcal{D'}^s$ with earliest $\frac{1}{\gamma}$ proportion of samples in $\mathcal{D}^s$ based on $L_j$
        \STATE Compute confidence score $c_j$ and gradient norm $g_j$
        \STATE Remove samples from $\mathcal{D'}^s$ satisfying all the following conditions:
        \STATE \quad (a) Large loss; (b) High confidence; (c) Low gradient norm.
        \STATE Update training set $\mathcal{D}^{i}$
    \ENDFOR
\ENDFOR
\STATE \textbf{Final Training Phase:} Train model on refined $\mathcal{D}^{I_{\text{rate}}}$ until convergence
\RETURN Trained model parameters $\theta^*$
\end{algorithmic}
\end{algorithm}

\subsection{Training Time}
\label{appb7}
\begin{table*}[h]
\renewcommand{\arraystretch}{1.0}
\centering
	\caption{A comparison of training hours for ResNet-34 on CIFAR-100 (Tested on servers with a single RTX 4090 GPU)}
	\label{tab7}
\resizebox{1.0\textwidth}{!}{
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{ccccccc}
\toprule
 Me-Momentum \cite{bai2021me} & Late Stopping \cite{yuan2023late} & RLM \cite{li2024regroup}  & CSGN \cite{linlearning} & Early Cutting (Ours)\\
\midrule
 $15$ hours & $17$ hours & $4$ hours & $9$ hours & $15$ hours  \\
\bottomrule  
\end{tabular}
}
}
\end{table*}

\subsection{Baselines and Competitors}

We re-implemented these methods under the same experimental settings as our proposed method. When re-implementing CSGN \cite{linlearning}, we used the \texttt{AdamW} optimizer and a stepped decay learning rate schedule, as specified in the original code. Notably, CSGN \cite{linlearning} cannot handle tasks with too many classes such as ImageNet-1k well.

\clearpage

