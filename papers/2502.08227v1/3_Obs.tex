


\begin{figure*}[t]
\centering
\begin{subfigure}{0.9\textwidth}
  \centering
    \caption{Impact on model performance from mislabeled examples learned at different learning stages, using CIFAR-10.}
      \vskip -0.1em
  \includegraphics[width=\textwidth]{Unknown-28.png}
  \label{fig6a}
\end{subfigure}
\vskip -1.2em
\begin{subfigure}{\textwidth}
  \centering
  \caption{Impact on model performance from mislabeled examples learned at different learning stages, using CIFAR-100.}
  \vskip -0.1em
  \includegraphics[width=0.9\textwidth]{Unknown-29.png}
  \vskip -1em
  \label{fig6b}
\end{subfigure}
\caption{Impact of mislabeled samples learned at different stages on model generalization performance.
Subfigure \ref{fig6a} shows the scenario in the CIFAR-10 dataset, which contains 20,000 mislabeled samples (40\% instance-dependent label noise) and 30,000 clean samples. We divided the 20,000 mislabeled samples into five groups based on the order in which an initial model learned them—from earliest to latest (ranging from $(0:20,000]$). Each group was combined with the 30,000 clean samples, creating datasets with approximately 12\% label noise ($4,000/34,000$). New models were then trained on these datasets. As shown by the decreasing test accuracy, models trained on datasets containing earlier-learned mislabeled samples (e.g., ``Clean $+ (0:4000]$ Mislabeled'') exhibited lower generalization performance.
Subfigure \ref{fig6b} shows a parallel study using CIFAR-100, yielding similar conclusions.}
\label{fig6}
\vskip -1em
\end{figure*}

\section{Our Observations}
\label{sec:impact_mislabeled_examples}

In this section, we investigate the varying effects that different mislabeled examples have on model's generalization. In Section \ref{sec2.1}, we provide empirical evidence demonstrating that different mislabeled examples have varying impacts on the performance of model, with the mislabeled examples learned earlier by the model bring greater harm. In Section \ref{sec2.2}, we analyze the reasons why these examples are easily learned by the model and bring about greater harm.

\subsection{Differential Effects on Generalization from Mislabeled Examples Learned at Different Stages}
\label{sec2.1}
Previous studies have shown that DNNs typically exhibit a specific learning pattern: they tend to learn simple and clean patterns first and gradually memorize more complex or mislabeled examples later \cite{arpit2017closer, toneva2018empirical}. Based on this, some sample selection methods \cite{liu2020early, bai2021me} trust the samples learned early by the model, treating them as high-probability clean samples.
In our study, to distinguish the order in which the model learns different mislabeled examples, we refer to the definition in \citet{yuan2023late}. Specifically, we consider that the model has learned a sample $(\mathbf{x}_i, y_i)$ at time $T_i$ if it consistently predicts the given label $y_i$ for both epoch $T_i-1$ and $T_i$, regardless of whether the label is correct. Formally, we define the \emph{learning time} $L_i$ of a sample $(\mathbf{x}_i, y_i)$ as:
\begin{equation}
L_i = \min \{ T_i \mid \hat{y}_i^{T_i-1} = \hat{y}_i^{T_i} = y_i \},
\label{eq1}
\end{equation}
where $\hat{y}_i^t$ denotes the model's predicted label for instance $\mathbf{x}_i$ at epoch $t$. By tracking each sample's learning time $L_i$, we can analyze the order in which the model learns different samples and evaluate their impact on performance.

\begin{figure*}[t]
\centering
\begin{subfigure}{\textwidth}
  \centering
    \caption{The speed at which pretrained models on CIFAR-10 learn mislabeled examples from different learning stages.}
  \includegraphics[width=0.9\textwidth]{pami-latestop-fig5cifar10v3.pdf}
  \label{fig7a}
\end{subfigure}
\vskip -0.3em
\begin{subfigure}{0.9\textwidth}
  \centering
  \caption{The speed at which pretrained models on CIFAR-100 learn mislabeled examples from different learning stages.}
  \includegraphics[width=\textwidth]{pami-latestop-fig5cifar100v3.pdf}
  \vskip -1em
  \label{fig7b}
\end{subfigure}
\caption{Comparison of how pretrained models learn mislabeled examples from different learning stages. Subfigure~\ref{fig7a} shows results on CIFAR-10 with 40\% label noise. We divided the mislabeled examples into five groups based on the order the initial model learned them, mixing each group with 30,000 clean examples to form datasets with approximately \(12\%\) label noise (\(4000/34000\)). A model was pretrained on the 30,000 clean examples and then training on these new noisy datasets. Reference lines indicate the number of epochs required for the pretrained model to learn different sets of 2,000 mislabeled examples. The results reveal that earlier-learned mislabeled examples are also learned more quickly by the clean data pretrained model. Subfigure~\ref{fig7b} shows similar findings on CIFAR-100.}
\label{fig7}
\vskip -1em
\end{figure*}

To investigate how the learning order of mislabeled examples affects their impact on the model’s generalization performance, we conduct experiments on the CIFAR-10 and CIFAR-100 datasets \cite{krizhevsky2009learning} with 40\% instance-dependent label noise \cite{xia2020part}, resulting in 20,000 mislabeled eamples. We train an initial model \( M_i \) on this noisy dataset and record the learning time \( L_i \) for each sample.
We rank the mislabeled examples in ascending order of their learning times \( L_i \) and divide them into five groups of 4,000 mislabeled samples each, corresponding to learning stages from early to late: \( (0\,:\,4000] \), \( (4000\,:\,8000] \), \( (8000\,:\,12000] \), \( (12000\,:\,16000] \), and \( (16000\,:\,20000] \). For each group, we combine the 4,000 mislabeled examples with 30,000 clean examples to create new training datasets. This yields five datasets containing the same clean examples but different subsets of mislabeled examples grouped by their learning times \( L_i \).

We then train new models from scratch on these datasets using the same architecture and hyperparameters, and evaluate their generalization performance on the clean test set.
As shown in Figure~\ref{fig6}, models trained on datasets with earlier-learned mislabeled examples (e.g., combining the earliest \( (0\,:\,4000] \) mislabeled examples with 30,000 clean examples) exhibit significant lower generalization performance than those trained on datasets with later-learned mislabeled examples (e.g., combining the latest \( (16000\,:\,20000] \) mislabeled examples with 30,000 clean examples). This indicates that mislabeled examples learned earlier by the model cause greater harm to generalization.


\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\textwidth} 
  \centering
  \includegraphics[scale=0.56]{latestopfigure88a.pdf}
  \vskip -0.4em
  \caption{}
  \label{fig4:sub1}
\end{subfigure}%
\begin{subfigure}[b]{0.43\textwidth} 
  \centering
  \includegraphics[scale=0.56]{earlycutting_fig4_b.pdf}
  \vskip -0.4em
  \caption{}
  \label{fig4:sub2}
\end{subfigure}
\vskip -1.1em
    \caption{\ref{fig4:sub1} Visualization of \emph{Mislabeled Easy Examples} (MEEs) in the feature space.
    Top row: t-SNE embeddings of CIFAR-10 training samples (20\% instance-dependent label noise), colored by their \emph{given} labels (left) and their \emph{true} labels (middle).
    Bottom left: a closer look at MEEs (red points) connected to their mislabeled class centers (black stars), demonstrating how these examples cluster in ambiguous regions that overlap with the mislabeled class.
    Bottom middle and right: comparisons of the distance ratio $r=d_{\text{mislabeled}}/d_{\text{true}}$ for MEEs and other mislabeled samples, confirming that they are indeed closer to incorrect wrong labels than their true labels in the learned feature space.
    \ref{fig4:sub2} Representative MEEs.
    Each image is shown with its \emph{true} label (blue) and the \emph{mislabeled} label (red).}
\label{fig1}
\vskip -1em
\end{figure*}

To validate this observation and minimize the impact of training stage differences on the learning order of mislabeled examples, we conducted additional experiments using a model pretrained on the clean CIFAR dataset, excluding the 20,000 mislabeled examples. As shown in Figure~\ref{fig7}, whether starting from random initialization or using a pretrained model, the earlier-learned mislabeled examples are consistently learned faster by the model. This result further confirms our observation: mislabeled samples which are correctly predicted by
the model early in the training process disproportionately degrade model performance.

\subsection{Mislabeled Easy Examples}
\label{sec2.2}

In this subsection, we focus specifically on the mislabeled examples that the model learns during the early stages of training. Drawing inspiration from the concept of \emph{Clean Hard Examples}, we formally define these particularly harmful mislabeled examples learned early by the model as \emph{Mislabeled Easy Examples (MEEs)}. This term indicates that although these samples are incorrectly labeled, they are easily learned by the model.

Notably, MEEs are non-trivial because the early stages of model training are typically characterized by learning simple and correct patterns from clean samples \cite{arpit2017closer, toneva2018empirical}, while the later stages are when the model starts to memorize mislabeled samples \cite{zhang2021understanding, yuan2024early}. Therefore, it is worthwhile to conduct an in-depth exploration of the counterintuitive way in which the model learns these mislabeled samples early in training to enhance our understanding of its learning process.
To better understand the characteristics of MEEs and their impact on model generalization, we examine their positions in the model's feature space and present some representative examples.

As shown in the Figure~\ref{fig4:sub1}, we visualize the mislabeled examples that are correctly predicted by the early-stage model using t-SNE \citep{van2008visualizing} in the feature space. 
Further, to quantify the model's representations of mislabeled samples during early training, we compute the Euclidean distances from each mislabeled example learned by the early-stage model to the center of its \emph{true} class and the center of its \emph{mislabeled} class in the embedded feature space. We denote these two distances as $d_{\text{true}}$ and $d_{\text{mislabeled}}$, respectively. We then define the \emph{distance ratio} $r = d_{\text{mislabeled}} / d_{\text{true}}$. If $r < 1$, the example is closer to the \emph{mislabeled} class center than to its \emph{true} class center.
As shown in the bottom row, MEEs exhibit a notably smaller median distance ratio ($0.830$), with more than half ($53.8\%$) of them having $r < 1$. In contrast, the remaining mislabeled samples (non-MEEs) have a median ratio of $3.923$, and only $5.4\%$ are closer to the incorrect class. 

\textbf{Why MEEs are learned earlier and harm generalization more?}
Our analysis suggests that MEEs occupy regions in the feature space where their incorrect labels seem more reasonable to the model. During the early stages of training, instances of MEEs closely resemble their mislabeled classes in the feature space, the model learned them as if they were representative samples with simple patterns.  Figure~\ref{fig4:sub2} presents representative MEEs. For instance, a CIFAR-10 image of an airplane with a dominant sea background is mislabeled as a \emph{ship}, and a CIFAR-100 image featuring a predominantly orange background is mislabeled as an \emph{orange}. These examples illustrate how strong visual cues matching their given (incorrect) label classes—such as color, texture, or prominent features—can pull these samples closer to the incorrect class in the feature space.

This phenomenon explains why MEEs are learned earlier: their misleading features align with the simple patterns of their given (incorrect) label classes that the model is tend to learn during the initial training stages. Thus, the early learning of MEEs has a disproportionately negative impact on the model's generalization performance: since the model incorporates incorrect patterns associated with MEEs from the beginning, it disrupts the initial formation of simple and accurate feature representations. The erroneous features learned from MEEs become intertwined with the representations of clean data, making it challenging for the model to disentangle the clean patterns.
