\section{Design of \sysname}
In this section, we provide the detailed design of \sysname.  We first introduce the middle-tier concurrency control algorithm that guarantees serializability when the underlying RDBMS is configured to a low isolation level (Section \ref{sec:validation-based-cc}). Then, we present a self-adaptive isolation level selection approach (Section \ref{design-2}), which dynamically predicts the optimal future isolation level based on transaction dependency information. Lastly, we introduce the cross-isolation validation mechanism that ensures serializability during the self-adaptive isolation level switching (Section \ref{design-3}). 

\subsection{Middle-tier Concurrency Control \label{design-1}}
\label{sec:validation-based-cc}
% Before processing transactions, \sysname must conduct a static workload analysis to identify which transactions require keeping commit order at weak isolation levels. Transactions not introducing data anomalies under weak isolation levels can execute following the original process. While the lifecycle of transactions that requires keeping commit order encompasses three stages: execution, validation, and commit.
% require maintaining commit order under weaker isolation levels. 
% Transactions not introducing data anomalies under weaker isolation levels can proceed as usual. 
% For those that do require maintaining commit order, their lifecycle includes three phases: execution, validation, and commit.


% \textbf{Static analysis.} \sysname employs theoretical analysis to identify potential data anomalies when transactions are executed at weaker isolation levels based on transaction templates. \sysname then ensures that the commit order and dependency order of two transactions that could form vulnerable dependency edges are identical. Specifically, if there exists $T_i\rightarrow T_j$, \sysname guarantees that $T_i$ can not commit after $T_j$. Therefore, in SI, even though the \textit{dangerous structure} consists of two vulnerable dependency edges, we only need to keep the commit order of the second dependency edge. For RC, we need to handle all the identified vulnerable dependency edges. All transactions that require order preservation need to be validated before committing. As depicted in Figure~\ref{fig: SmallBank}, in the case of SmallBank, \sysname needs to validate WC and TS transactions when the database is set to SI, while \sysname needs to validate all transactions when the database is set to RC. 

% Before introducing the transaction's lifecycle, we propose a hot-cold validate lock table (VLT) to avoid the concurrent execution of read and write operations during the validation and commit phases. 
% % , and the structure of the validate lock table (VLT) is shown in Figure~\ref{fig:locktable}.
% The VLT is structured as a hash table, with the hash key typically being an invariant attribute of the table, such as the primary key. For each record $k$, the lock entry of $k$ is associated with five variables: $type$, $version$, $min\_tid\_wl$, $lease$, $count$. 
% The value of $k.type$ is either None, Write (WR), or Read (RD), meaning there is either no lock, one/more locks for write operations, or one/more locks for read operations. And $k.count$ represents how many locks on $k$ are granted. 
% The $k.version$ signifies the latest version of $k$, with its initial value set to -1. It acts as a version cache to minimize the overhead of accessing the database during the validation phase. Due to memory space limitation, \sysname can not retain all lock entries in VLT. Besides the lock entries being accessed, we aim to cache hotspot lock entries to prevent repetitive creation and release. The $k.lease$ indicates the expiration time of the lock entry, which updates once a transaction visits the lock entry, and the lock entry can be released when its lease expires. 
% % Additionally, $k.max\_tid$ and $k.max\_tid\_wl$ represent the maximum transaction ID in the locked and waiting list respectively.
% Next, it detects runtime dependencies for transactions derived from these templates and prevents them from forming anomalies. 

After receiving the transaction templates, \sysname initially analyzes to identify all static vulnerable dependencies at each low isolation level and the corresponding transaction templates involved based on Definition~\ref{def:static_vul}. 
To prevent the non-serializable scheduling, \sysname proposes the middle-tier concurrency control algorithm, which introduces a validation phase into the lifecycle of transactions derived from these templates. 
In the validation phase, \sysname detects vulnerable dependencies between transactions derived from identified templates and schedules the commit order to make it consistent with the dependency order. 
% \ls{The last sentence is confusing, what types of runtime dependencies we detects is not clear.}


% \begin{itemize}[leftmargin=*]
%     \item \textbf{Rule-I.} If the RDBMS is under SI level, the static dangerous structure is $\mathcal{T}_i \xrightarrow{rw} \mathcal{T}_j \xrightarrow{rw} \mathcal{T}_k$. We trace the read operations of $T_j$ and the write operation of $T_k$. \ls{Why we do not need to concentrate on the first \textit{rw}?}
%     \item \textbf{Rule-II.} If the RDMBS is under RC level, the static dangerous structure is like $\mathcal{T}_i \xrightarrow{rw} \mathcal{T}_j$. We trace the read operations of $T_i$ and the write operation of $T_j$.
% \end{itemize}



% \ls{This section should be formally and concisely describing our method. We can use examples to help illustrate our idea, but we should not use only examples, which makes our method informal and inaccurate.}
\subsubsection{Transaction lifecycle} 
% \begin{example}
% % Both $T_{wc}$ and $T_{ts}$ would undergo a validation phase before they enter into the commit phase because $\mathcal{T}_{wc}$ and $\mathcal{T}_{ts}$ participate in a static dangerous structure. 

% % \sysname places the runtime data item $x_0$ in the \textit{vread\_set} of $T_{wc}$ and the modified data item $x_1$ in the $vwrite\_set$ of $T_{ts}$. 
%     As shown in Figure \ref{fig:SmallBank}, there exists a static dangerous structure involving $\mathcal{T}_{wc}$ and $\mathcal{T}_{ts}$ when the RDBMS is set to SI. Thus, it is necessary to detect the RW dependency from $\mathcal{T}_{wc}$ to $\mathcal{T}_{ts}$. Both of them should undergo the validation phase before they enter to commit. 
%     In the execution phase of $T_{wc}$, we store the read set $x$ (\whiteding{1} and \whiteding{2}). In this example, $T_{wc}$ and $T_{ts}$ access the same record $x$, thus forming a runtime vulnerable dependency between the instances $T_{wc}$ and $T_{ts}$. In the validation phase, we acquire the validation lock of $x$ (\whiteding{3}) and check if $x$ has been modified via the version cache in VLT or the physical record in RDBMS (\whiteding{4}). Finally, we commit $T_{ts}$ and release the validation lock (\whiteding{5}). Moreover, 
%     \qed
% \end{example}

% \ls{I tried to seperate our method with the illustration example. However, there are some overlaps in the descriptions, please check. We SHOULD: make it clear what modifications \sysname have done to the normal transaction execution process so as to achieve our goal in our method description. The example is mainly used to illustrate the difficult-to-understand steps. }

A transaction life is divided into three phases, i.e., the execution phase, the validation phase, and the commit phase.
(1) In the execution phase, \sysname establishes a database connection with a specific isolation level, which is not adjusted until the transaction is committed or aborted. Then, after the normal transaction execution, \sysname stores the read/write data items in the thread-local buffer that may induce the vulnerable dependencies according to Definition \ref{def:static_vul};
% For example, we store the record $r$ including its version to $T_{wc}.vread\_set$ and store the updated record $r$ to $T_{ts}.vwrite\_set$. 
(2) In the validation phase, \sysname acquires validation locks for data items stored in the buffer. Then, it detects the dependencies between them and intends to schedule the commit order consistent with the dependency order.
A more detailed description of the validation phase will be given in Section~\ref{sec:design:cc:validation}; 
% For example, in the validation phase of $T_{ts}$, \sysname detect a runtime RW dependency that 
(3) In the commit phase, \sysname applies the modification into the database and releases the validation locks obtained in the validation phase. 

{
\color{blue}
\subsubsection{Data structure.} 
For validation, each data item is assigned a hash value, calculated by the collision-resistant hash function $\mathcal{H}$ and cached in the Validation Lock Table (VLT). 
Data items with the same hash value are stored in the same bucket and organized as a data item list. 
To avoid high memory overhead, only frequently accessed data items are cached using a garbage collection algorithm. Each data item, identified by its key $x$, comprises five fields: (1) $x.Type$, the type of locks acquired on the data item; (2) $x.LockNum$, the number of currently held locks on the data item; (3) $x.WaitList$, a list of transactions waiting to acquire locks on the data item; (4) $x.LatestVersion$, the most recent committed version of the data item; and (5) $x.Lease$, the timestamp indicating the last time the data item was accessed. 
}

\subsubsection{Validation\label{sec:design:cc:validation}} \sysname performs two key tasks in the validation phase: (1) detecting vulnerable dependencies; (2) scheduling the commit order consistent with dependency order. To achieve this, we explicitly add a \textit{version} column to the schema, which is incremented after every update. We trace the operation order and detect the dependencies by comparing the version of read/write data items. 
 Algorithm~\ref{alg.transaction} shows the detailed algorithm. 
 % \ls{Merge the algorithm description in the validation section. }
% \textcolor{flatgreen}{FINISHED}

% validation phase，验证已经静态分析出现的 struct 是否出现。验证是通过判断读是否被已提交的 VLT。什么是通过验证，什么是没有验证。怎么保证 commit order。validation 先检测，在解决。
% 判断读集是否被改，具体怎么验证，需要设计一个 VLT 的结构。
% Specifically, The detection process adheres to the following two rules:
% detect runtime

% \textit{ReleaseValidateLock} releases the locks obtained in the validation phase. Meanwhile, expired lock entries in the same bucket should be released. 
\begin{algorithm}[t]
    \caption{Middle-tier concurrency control algorithm}
    \small
    \label{alg.transaction}
    % \setstretch{0.85}
    \SetKwInOut{KwIn}{Input}
    % \SetKwInOut{KwOut}{Output}
    \SetKwFunction{Validate}{Validate}
    \SetKwProg{Fn}{Function}{:}{}

    \Fn{\Validate{T, conn}} {
        \KwIn{T, transaction requiring validation; \\ conn, a connection under RC}
        \tcp{Acquire the validation locks on data items}
        \For {r \textbf{in} T.vread\_set $\cup$ T.vwrite\_set} {
            res := TryValidationLock(r.key, T.tid, r.type) \\
            \While {res \textbf{is} WAIT} {
                res := TryValidationLock(r.key, T.tid, r.type) \\
            }
            \If {res \textbf{is} ERROR} {
                \Return {ERROR} 
            }
        }
        \tcp{Check the version of data items in the read set}
        \For{r \textbf{in} T.vread\_set} {
            version := 0\\
            entry := HVC.get\_lock\_entry(r.key) \\
            \If {entry.version > 0} { 
                \tcp{get the latest version from version cache}
                version := entry.version \\
            }
            \Else { \tcp{fetch the lastest version from DBMS}
                version := conn.get\_version(r.key) \\
                entry.version := version\\
            }
            \If {version \textbf{is not} r.version} {
                \Return {ERROR} \\
            }
        }
        \Return{SUCCESS}\\
    }

    \SetKwFunction{Commit}{Commit}
    \SetKwProg{Fn}{Function}{:}{}

    \Fn{\Commit{T, sess}} {
        \KwIn{sess, session for transaction execution;}
        sess.commit(T)\\
        \For{r \textbf{in} T.vwrite\_set} {
            entry := HVC.get\_lock\_entry(r.key) \\
            entry.version = r.version \\
        }
        \For {r \textbf{in} T.vread\_set $\cup$ T.vwrite\_set} {
            ReleaseValidateLock(r.key) \\
        }
    }
\end{algorithm}

\noindent\textbf{Detecting Vulnerable Dependencies.} 
% \ls{Instead of two steps, we should present this as two scenarios, i.e., (1) break the dangerous dependency on RC and (2) that on SI. }
% \ls{Come back later.}
% \ls{@QY, Please check on this paragraph. } \qy{DONE} 
We detect vulnerable dependencies based on those defined in Definition~\ref{def:vul} and Theorem~\ref{the:vulnerable}. For both the RC and SI isolation levels, we should detect the vulnerable dependency $T_i \xrightarrow{rw} T_j$. 
{
\color{blue}
We achieve this detection by introducing the VLT. 
}
Before entering the validation, the transaction first requests \textit{Shared} locks for items in the read set and \textit{Exclusive} locks for items in the write set (lines 2-7). 
There are two steps to validate each transaction $T_i$. 
(1) The first step is to check $T_i$'s read set, \sysname detects whether its read set is modified by a committed transaction $T_j$, which results in an RW dependency $T_i \xrightarrow{rw} T_j$. (2) The second step is to check transaction $T_i$'s write set, \sysname detects whether older versions of its write set have been read by some other transaction $T_j$, in which case an RW dependency $T_j \xrightarrow{rw} T_i$ occurs. 

During a transaction $T_i$'s execution phase, it stores the version of the corresponding read/write data item in its thread-local buffer. When detecting vulnerable dependencies, $T_i$ first traverses all read items and compares the version of each read item in the thread-local buffer with the item's latest version in the RDBMS (line 9-15 in Algorithm~\ref{alg.transaction}). If a version mismatch is found, indicating an RW dependency from the current transaction $T_i$ to a committed transaction, say $T_j$, then $T_i$ is aborted to 
% break the vulnerable dependency of $T_i \xrightarrow{rw} T_j$ in RC and 
ensure the consistency of commit and dependency orders (lines 16-17). 
{
\color{blue}
Notably, to mitigate the performance overhead associated with frequent comparisons between data item versions in the transaction's local buffer and the latest versions maintained in the RDBMS, arising from the interactions between the middle-tier and the database, \sysname leverages the $LatestVersion$ attribute to cache the latest versions of data items. This caching mechanism facilitates rapid retrieval of the latest version during the validation process, thereby optimizing the overall performance.
}
% \ls{Explain what the latest version represents and what the version in the thread-local buffer represents, otherwise it is not clear why it derives an RW dependency. FINISHED }

After traversing the read set, $T_i$ checks each data item in its write set to determine if there is any concurrent transaction $T_j$ reading the same data item and is undergoing validation. 
% The \textit{Exclusive} lock request is exclusive to either an \textit{Exclusive} or \textit{Shared} lock. \textit{Shared} locks can be compatible. 
We achieve this with the validation locks. 
If any validation lock request fails, indicating such a transaction $T_j$ exists, an RW dependency is detected. In this condition, the failed lock request should be appended in the corresponding item's \textit{WaitList}, making $T_i$ wait until $T_j$ commits, ensuring consistency between dependency and commit orders. If no concurrent transactions in the validation phase are reading the same item, $T_i$ proceeds to commit and create a new data version. 

\noindent\textbf{Scheduling the Commit Order.} We conduct a two-step assurance process to ensure that the commit order of transactions aligns with their dependency order. Firstly, we ensure the commit order in the middle tier is consistent with the dependency order.  Secondly, we ensure the actual commit order in the RDBMS is consistent with the commit order in the middle tier. The middle tier consistency is achieved by aborting or blocking transactions once a vulnerable dependency is detected. We ensure the RDBMS layer consistency is achieved by releasing validation locks after the transaction is committed in the RDBMS (lines 24-25). Based on this, if two concurrent transactions access the same data item $x$ (with one writing and the other reading or writing), they cannot both enter the validation phase simultaneously. One must complete validation and commit before the other can proceed to validation, ensuring the correct commit order in RDBMS. 



% We achieve the write item's validation by validation locks. Before entering the validation phase, the transaction first acquires the validation locks for all data items in the read/write set, denoted as \textit{vread\_set} and \textit{vwrite\_set}, respectively. Specifically, the transaction acquires shared locks for data items in the read set and exclusive locks for data items in the write set (line 2-7). In \sysname, validation locks are managed in the validation lock table (VLT), which is structured as a hash table, and the hash key is typically a unique column of the table, such as the primary key. 
% Each lock entry \textit{k} in the VLT has three variables: \textit{type}, \textit{lockNum}, and \textit{waitList}. The value of \textit{k.type} can be \textit{None}, \textit{Exclusive (EX)}, or \textit{Shared (SH)}, indicating no lock, one exclusive lock, or one or more shared locks, respectively. \textit{k.lockNum} represents how many shared locks on \textit{k} are granted. \textit{k.waitList} is a priority queue sorted by the begin timestamp of transactions, used to handle starvation of write operations. Moreover, with the validation locks, \sysname ensures the commit order in the RDBMS aligns with the order determined in the middle tier. 

% thread-local buffer 代表的是transaction在执行过程中访问的tuple的version记录在thread local buffer， current latest version当前已经提交的事务的最新的写的版本，通过比较版本之间的大小来判断是否有RW依赖，如果thread中的版本小于最新版本，xxx

% attempt to acquire the validation lock before committing. If successful, the transaction proceeds; otherwise, it is blocked by a read transaction, signifying an RW dependency from the read transaction to the current one. The current transaction then waits until the read transaction commits and releases its validation lock, ensuring consistency between dependency and commit orders. 

% \noindent\textbf{Scheduling the Commit Order.} To maintain the commit order in the database consistent with the dependency order we detect in the middle tier, the transaction should acquire the validation locks for all data items in the read/write set, denoted as \textit{vread\_set} and \textit{vwrite\_set}, respectively. 
% \qy{Specifically, the transaction acquires shared locks for data items in the read set and exclusive locks for data items in the write set (line 2-7).}
% \ls{It is still not clear how this is achieved even with the validation lock structure. Are read and write operations exclusive when acquiring the validation lock on them?} \textcolor{flatgreen}{FINISHED} 
% In \sysname, validation locks are managed in the validation lock table (VLT), which is structured as a hash table, and the hash key is typically a unique column of the table, such as the primary key. 
% Each lock entry \textit{k} in the VLT has three variables: \textit{type}, \textit{lockNum}, and \textit{waitList}. The value of \textit{k.type} can be \textit{None}, \textit{Exclusive (EX)}, or \textit{Shared (SH)}, indicating no lock, one exclusive lock, or one or more shared locks, respectively. \textit{k.lockNum} represents how many shared locks on \textit{k} are granted. \textit{k.waitList} is a priority queue sorted by the begin timestamp of transactions, used to handle starvation of write operations. This ensures that write transactions are not indefinitely blocked by read locks.

% represents the transactions that wait for validation; it consists of a priority queue sorted by the begin timestamp. The waitlist is used to handle the starvation of write operations; otherwise, in an extreme scenario, the $k.type$ is \textit{SH}, and the write operation has been unable to get a lock when a read operation accesses the same data item, thus affecting the commit of the write transaction.

% \noindent\textbf{Optimizations.} In the validation phase, \sysname should compare the data items in the local buffer with the latest one, which introduces additional overhead due to interactions between the middle tier and the RDBMSs. 
% % To minimize the overhead, \sysname is equipped with a hot version cache (HVC) that stores the latest version of frequently accessed data items. 
% {
% \color{blue}
% To minimize the overhead, \sysname incorporates a hot version cache (HVC) that stores the latest version of frequently accessed data items into the VLT. 
% }
% Each cache entry \textit{k} in the HVC has two attributes, i.e., \textit{lastestVersion} and \textit{lease}. The \textit{k.lastestVersion} attribute, initially set to -1, facilitates a fast path to obtain the latest version during validation. 
% {
% \color{blue}
% To ensure mutually exclusive updates of cache entries, transactions release \textit{Exclusive} locks in VLT for items in the write set after completing updates of the HVC.
% Furthermore, we implement an efficient garbage collection mechanism to constrain the HVC's memory, detailed in Section~\ref{implementation}.
% \sysname periodically removes entries in HVC and VLT whose \textit{lease} attributes fall behind the latest \textit{lease}.
% latest lease是什么，VLT和HVC是放在同一个hash table中的，
% 如果要对write set进行验证，那么一定需要
% }
% To manage memory efficiently, HVC only keeps versions for hot data items. The $k.lease$ attribute denotes the expiration time of the cache entry, updated upon access, allowing \sysname to release lock entries from the VLT when their leases expire. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/transaction_lf.pdf}
    \vspace{-4mm}
    \caption{Transaction processing in \sysname
    % \textnormal{$\mathcal{T}_{wc}$ and $\mathcal{T}_{ts}$} participate the dangerous structure in 
    }
    \label{fig:transaction_lifecycle}
    \vspace{-4mm}
\end{figure}

\begin{example}
    Take Figure~\ref{fig:transaction_lifecycle}, which provides a concise depiction of transaction processing, as an example. Recall that there exists a static vulnerable dependency $\mathcal{T}_{wc} \xrightarrow{rw} \mathcal{T}_{ts}$ in Smallbank when the RDBMS is set to SI (Figure \ref{fig:SmallBank}). Thus, it is necessary to detect the read operation of $T_{wc}$ and the write operation of $T_{ts}$. 
    In the execution phase, after the normal transaction execution (\whiteding{1}), $T_{wc}$ stores the data item \textit{x} in its \textit{vread\_set} and $T_{ts}$ stores \textit{x} in its \textit{vwrite\_set} (\whiteding{2}). 
    In the validation phase of $T_{wc}$, it acquires the shared validation lock on \textit{x} in the VLT (\whiteding{3}) and retrieves the latest version of \textit{x} from either HVC or the RDBMS (\whiteding{4}). 
    While in the validation phase of $T_{ts}$, it requests the exclusive validation lock on \textit{x} and is blocked until $T_{wc}$ releases the lock. 
    Finally, in the commit phase, $T_{wc}$ releases the validation lock on \textit{x}. This ensures that the commit order of the two transactions is consistent with the dependency order, thereby guaranteeing SER when they operate under SI.
    % \ls{The following sentences should be in step \whiteding{3}?} \qy{DONE}
    % Moreover, during the validation of $T_{wc}$, it requests the same validation lock on \textit{x} and is blocked until $T_{ts}$ has committed. This ensures that the commit order of the two transactions is consistent with the dependency order, thereby guaranteeing serializability when the RDBMS is configured to SI.
    \qed
\end{example}
% Therefore, besides storing accessed lock entries, \sysname also caches frequently accessed lock entries (hotspots) to avoid the overhead of repeatedly creating and releasing them. 

% To handle the VLT, we design two key functions: \textit{TryValidationLock()} and \textit{ReleaseValidationLock()}. Both functions need to update the expiration time of the lock entries they obtain or release. A more detailed description can be as follows: 

% \textit{TryValidationLock} attempts to obtain a lock for a specific key during the validation phase. We denote the original type of the lock entry as $type_o$ and the request type of the lock entry as $type_r$. If $type_o$ is None, the lock entry can successfully be granted regardless of $type_r$. 
% When $type_o$ is not \textit{None}, the behavior is classified into the following four categories:
% \begin{itemize}[leftmargin=*]
%     \item $type_o$ is \textit{RD} and $type_r$ is \textit{RD}. The lock should be granted because read operations do not cause any data anomalies. (return \textit{SUCCESS})
%     \item $type_o$ is \textit{RD} and $type_r$ is \textit{WR}. Since some transactions have read the record, there is an RW dependency. The write operations are blocked until the read operations are committed. (return \textit{WAIT})
%     \item $type_o$ is \textit{WR} and $type_r$ is \textit{RD}. A write operation is in progress during the validation phase. To ensure that read and write dependencies are submitted in the same order, the read request is rejected. (return \textit{ERROR})
%     \item $type_o$ is \textit{WR} and $type_r$ is \textit{WR}. There is no need to maintain the dependency or commit order for ww dependencies. This operation is not blocked and is handled by the RDBMS. (return \textit{SUCCESS})
% \end{itemize}



% \ls{It seems we are using runtime dependency and vulnerable dependency interleavingly. Check and be consistent. }
% \subsubsection{Algorithm explaination} To provide a clearer description, we present key functions in Algorithm~\ref{alg.transaction}. The \textit{Validate} function is used to detect the runtime dependency of $T$ and schedule the commit order consistent with the dependency order. We then validate $T$ by examining its \textit{vread\_set} and \textit{vwrite\_set} in two steps. First, we obtain the validation lock for those data items (lines 2-7). Then, we validate the \textit{vread\_set} to detect the existence of an RW dependency from $T$ to a committed transaction. There are two methods to get the latest version of the data. The fast path is visiting the HVC if the version is greater than 0 (lines 9-12); otherwise, we request the latest version from the database via another connection with the RC level (lines 13-15). If the version in the local buffer is earlier than the latest version, \qy{an RW dependency from $T$ to a committed transaction is detected}, the validation phase of $T$ fails (lines 16-17). If all data items in \textit{vread\_set} are the latest, the validation phase of $T$ is successful.
% The \textit{Commit} function is called after \textit{Validate}. \sysname sends a \textit{commit} request to the database and releases the validation locks acquired during the validation phase (lines 20-23) if the transaction is successfully committed, \sysname updates the HVC with the new versions in the \textit{vwrite\_set} (lines 24-25).

% dealing with the runtime transaction instances, we detect the dependency that potentially leads to anomalies. Specifically, for those transactions involved in the potential anomalies, we validate the read set before it can be committed. 


% Figure~\ref{fig:transaction_lifecycle} illustrates the transaction processing in \sysname. Before exploring the transaction lifecycle, we provide a detailed overview of the \textit{hot-cached validation lock table}, which is the key data structure ensuring atomic operations during the validation phase. Specifically, we prevent concurrent reads and writes from entering the validation phase simultaneously. Allowing this would make it impossible to determine their commit order, as the RDBMS operates as a ``black box" for us.



% \item \textit{TryValidateLock}: This function obtains the lock for a \textit{key} during the validation phase. It locates the lock entry using the key and the number of buckets in the VLT and updates its lease (lines 1-3). If \textit{entry.type} is None, indicating no transaction is currently accessing the entry during the validation phase, the transaction can immediately obtain the lock (lines 4-7). When both \textit{entry.type} and \textit{type} are RD, if \textit{tid} is less than the minimum \textit{tid} in the waitlist, meaning that the current transaction is earlier and can successfully obtain the lock. Otherwise, the current transaction is aborted to prevent starvation of write operations (lines 9-14). If \textit{entry.type} and \textit{type} are inconsistent, read and write operations cannot enter the validation phase simultaneously. Therefore, the current transaction is added to the waitlist, and \textit{entry.min\_tid\_wl} is updated (lines 16-17). Lastly, when both \textit{entry.type} and \textit{type} are WR, if \textit{tid} is less than the minimum \textit{tid} in the waitlist and the database isolation level is RC, the database resolves the \textit{ww} conflict. Otherwise, similar to RD, the current transaction is aborted to prevent starvation of read operations.

% \item \textit{ReleaseValidateLock}: This function releases the validation lock entry for a \textit{key}. If \textit{entry.count} decreases to zero, \textit{entry.type} is set to None (lines 18-33). Subsequently, expired lock entries in the VLT are removed (lines 34-36).

% hot lock entries to avoid repeated creation and release. $k.lease$ indicates the expiration time of the lock entry, which is updated upon access, allowing the lock entry to be released when its lease expires. 

% \ls{If we have implemented the existing algorithms in \sysname, then we only need to directly introduce how we can prevent phantom. If we do not implement those lock algorithms, we'd better discuss it as the extensibility of \sysname. }

{
\color{blue}
\subsubsection{Garbage collection.} 
To limit memory usage, the VLT in \sysname is implemented as a hash table with a fixed number of buckets. A garbage collection algorithm ensures that only frequently accessed data items remain cached. 
When a data item with $x$ is accessed, \sysname first calculates the corresponding bucket using the hash function $\mathcal{H}(x)$. It then searches the bucket for data item $x$ and evicts any outdated data items in this bucket. Specifically, \sysname updates the $x.Lease$ with the current time. Subsequently, any data items $x$ where the $x.Lease$ value falls behind the current time by more than a predefined threshold $\theta$. 
To prevent the data items in infrequently accessed buckets remain unremoved indefinitely, a background thread periodically scans buckets that have not been accessed for a long time and evicts any outdated data items.
}

\noindent\textbf{Discussion.} 
% \ls{As we are discussing the generality of our approach on anomilies like phantom read, we need to first conclude that \sysname is applicable to detecting phantom read, by exetending XX and XX. Except for the locking policy, we need also mention the vulnerable dependency. } \qy{DONE}
We note that range queries with predicates may potentially introduce phantom reads and violate SER. 
% \ls{@QY check the marked sentences. } \qy{DONE}
For phantom reads, the definition of vulnerable dependency remains applicable, which enables \sysname to detect and prevent this anomaly. 
The only difference is that we need to implement a larger granule validation lock, such as interval or table locks, to enable detecting the dependencies between predicates. 
% In addition to the locking policy, we also need to mention that the definition of vulnerable dependency remains applicable in this scenario. We can achieve SER by maintaining consistency between the dependency and commit order. 
As various coarse-grained locking techniques, such as SIREAD locks in PostgreSQL and gap locks~\cite{DBLP:conf/vldb/Lomet93}, already exist, we opt to implement the coarse-grained validation locks using these methods and exclude locking optimization from our paper to focus on efficient isolation level adaptation. 

% However, relational database engines must also handle predicate-based operations. Addressing ``dangerous structures'' in the context of predicate operations would require extending the middle-tier concurrency control to take validation locks on larger granules, such as intervals or tables. Granule locking techniques, such as SIREAD locks in PostgreSQL, have been extensively studied and are orthogonal to the contributions of this paper. Therefore, we do not focus on predicate-related issues in this work.

% mean that a concurrency control algorithm must also consider phantoms, where an item created or deleted in one
% transaction would change the result of a predicate operation
% in a concurrent transaction if the two transactions executed
% serially.
% \textbf{In the execution phase}, before any read or write operations, \sysname establishes a database connection with a specific isolation level. Then, after the normal execution (\whiteding{1} and \whiteding{2}), \sysname stores the read/write set in a thread-local buffer. For example, we store the record $r$ including its version to $T_{wc}.vread\_set$ and store the updated record $r$ to $T_{ts}.vwrite\_set$. 
% It then executes the queries in the database and checks if any query might introduce a vulnerable dependency. If so, \sysname stores the read/write (rw) set, including the keys and versions of the records, in a thread-local buffer.

% \textbf{In the validation phase}, \sysname checks the rw set stored in the thread-local buffer. If the buffer is empty, which implies the transaction can not participate in a vulnerable dependency, \sysname will skip the validation phase. Otherwise, \sysname needs to check the rw set whether has been modified by other concurrent transactions. \sysname retrieves the latest version of those records in the read set. \sysname prefers to find a cache of versions from the VLT. Otherwise, the executor should conduct extra access to the database. If the version in the buffer has been changed, it implies a write operation dependent on this transaction has already been committed. In this case, \sysname aborts the read transaction to keep the commit order of this rw dependency. Regarding write operations, since the isolation level of the database is set to at least RC, the database will maintain an exclusive lock on the record until the transaction is committed, thereby ensuring that the write set can not be modified. It's important to note that there's a gap between the validation phase and the commitment of the transaction. To prevent any transaction from violating the commit order during this gap, \sysname acquires a validation lock to the corresponding record in the VLT before the validation phase, which can avoid simultaneous entry of read and write operations into the validation phase. 

% \textbf{In the validation phase}, \sysname detects the runtime dependencies between transactions that are about to commit and intends to keep the dependency order consistent with the commit order. 

% checks the read/write set if the record buffer is not empty. \sysname needs to check if the rw set has been modified by other concurrent transactions. \sysname retrieves the latest versions of the records in the read set, preferably from the VLT version cache; otherwise, the executor must access the database. 
% If any version in the buffer has changed, a dependent write operation has been committed. In this case, \sysname aborts the read transaction to maintain the commit order of the \textit{rw} dependency. For write operations, since the isolation level is at least RC, the database will hold exclusive locks on the records until the transaction is committed, ensuring that the write set cannot be modified. 
% It is important to note that there is a gap between the validation phase and the transaction commit. To prevent any transaction from violating the commit order during this gap, \sysname acquires validation locks on the corresponding records in the VLT before the validation phase, avoiding concurrent read and write operations entering the validation phase simultaneously.

% \begin{multicols*}{2}
% \begin{algorithm}[t]
%     \caption{The algorithm for validation lock}
%     \small
%     \label{alg.transaction}
%     \SetKwFunction{TryLock}{TryValidationLock}
%     \SetKwProg{Fn}{Function}{:}{}
%     \setstretch{0.85}

%     \Fn{\TryLock{key, tid, isolation}} {
%         entry := TLV.get\_lock\_entry(key)\;
%         \If {entry.type \textbf{is} None} {
%             entry.type := type, entry.count := entry.count + 1 \\
%             entry.min\_tid\_wl := \textit{INT\_MAX}\\
%             \Return{SUCCESS}\\
%         }
%         \ElseIf {entry.type \textbf{is} SH} {
%             \If {type \textbf{is} SH} {
%                 \If {tid < entry.min\_tid\_wl} {
%                     entry.count := entry.count + 1\\
%                     \Return{SUCCESS}\\
%                 }
%                 \Else {
%                     \Return{ERROR}\\
%                 }
%             }
%             \Else {
%                 entry.min\_tid\_wl = \textit{min}(tid, entry.min\_tid\_wl)\;
%                 \Return{WAIT}\\
%             }
%         }
%         \Else{
%             \If {type \textbf{is} EX} {
%                 \If {isolation \textbf{is} RC \textbf{and} tid < entry.min\_tid\_wl} {
%                     entry.count := entry.count + 1\\
%                     \Return {SUCCESS}\\
%                 }
%                 \Else {
%                     \Return {ERROR}\\
%                 }
%             }
%             \Else {
%                 entry.min\_tid\_wl := \textit{min}(tid, entry.min\_tid\_wl)\\
%                 \Return {ERROR}\\
%             }
%         }

%         \SetKwFunction{ReleaseValidateLock}{ReleaseValidateLock}
%         \SetKwProg{Fn}{Function}{:}{}

%         \Fn{\ReleaseValidateLock{key}} {
%             entry := TLV.get\_lock\_entry(key)\\
%             update\_lease(entry)\\
%             entry.count := entry.count - 1\\
%             \If {entry.count \textbf{is} 0} {
%                 entry.type := None \\
%             }

%             TLV.lease\_entries(key \% bucket\_size) \\
%         }    
%     }
% \end{algorithm}
% \end{multicols*}

% \textbf{In the commit phase}, \sysname sents \textit{commit} request to the database and relinquishes the locks obtained in the preceding phase after receiving the commit results. Additionally, if the write set in the record buffer is not empty and the transaction has been committed, hence a newer version has been created, then \sysname updates the version cache in the VLT.

% \noindent \textbf{Algorithm explanation.} We list some key functions in Figure~\ref{fig:algo1} to better clarify the transaction processing in \sysname. 
% % Firstly, we introduce two crucial functions of VLT: 
% % TryValidateLock and ReleaseValidateLock should be executed atomically to avoid concurrent errors. 
% The function \textit{TryValidateLock} is used to acquire the lock of \textit{key} for the validation phase. It locates the lock entry from the key and the number of buckets in the VLT and updates its lease (lines 1-3). If the \textit{entry.type} is None, no transaction in the validation phase is accessing the entry, and the transaction obtains the lock immediately (lines 4-7).
% When both \textit{entry.type} and \textit{type} are RD, if the \textit{tid} is less than the \textit{tid} of all the transactions in the waiting list, which signifies that the current transaction is older than those in the waiting list, the lock can be successfully acquired. Otherwise, we abort the current transaction to avoid the ``starvatio'' phenomenon of the write operation (lines 9-14). 
% If \textit{entry.type} and \textit{type} are inconsistent, read and write operations cannot enter the validation phase simultaneously, thus we add the current transaction to the waiting list and update the \textit{entry.min\_tid\_wl} (lines 16-17). 
% Lastly, when both \textit{entry.type} and \textit{type} are WR.
% % and concurrent transactions are allowed to have write-write conflicts in RC, \sysname  the database to resolve these conflicts 
% If the \textit{tid} is less than the \textit{tid} of the transactions in the waiting list and the isolation level of the database is RC, we let the database resolve the ww conflicts. Otherwise, similar to the RD, we abort the current transaction to avoid ``starvation'' of read operations.
% The function \textit{ReleaseValidateLock} is used to release the validate lock entry of \textit{key}, if \textit{entry.count} is decreased to zero, we set its \textit{type} to \textit{None} (lines 18-33). Then, we remove the expired lock entries for the VLT (lines 34-36).
% The function \textit{Validate} is invoked before transaction \textit{T} can commit. Firstly, we acquire all validation locks for records in the record buffer (lines 37-42). We only regard the records in the read set because the database can guarantee that concurrent transactions cannot modify records in the write set until \textit{T} is committed. There are two methods to get the latest version of the data. For hotspot records, the version cache in the VLT can be directly read; otherwise, it's necessary to request the latest version from the database via a separate connection set to RC (lines 43-52).
% The function \textit{Commit} would commit \textit{T} in the database and then update the version cache for records in the write set. Lastly, we release the validation locks (lines 55-59).
% % \textbf{Case study.} 

% \textbf{Algorithm Explanation.} To elucidate transaction processing, we present key functions in Algorithm~\ref{alg.transaction} and describe as follows:
% \begin{itemize}
    

%     \item \textit{Validate}: This function is invoked before a transaction \textit{T} commits. Initially, all validation locks for records in the buffer are acquired (lines 37-42). Only records in the read set are considered, as the database guarantees that concurrent transactions cannot modify records in the write set before \textit{T} commits. There are two methods to access the latest version of the data. For hot records, the version cache in the VLT is directly read; otherwise, a separate connection with the RC level is used to request the latest version from the database (lines 43-52).

\subsection{Self-adaptive Isolation Level Selection \label{design-2}}\label{sec:self-adaptive_section}
% \ls{Selection or Prediction?}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.99\linewidth}
        \centering
        \begin{subfigure}{1.0\linewidth}
            \includegraphics[width=\linewidth]{figures/graph_training.pdf}
            % \caption{Achitecture of adapter}
            % \label{fig:graph_learning:adapter}
            % \vspace{-4mm}
        \end{subfigure}
        % \begin{subfigure}{0.31\linewidth}
        %     \includegraphics[width=\linewidth]{figures/tsne.pdf}
        %     \caption{Tsne}
        %     \label{fig:graph_learning:tsne}
        %     \vspace{-4mm}
        % \end{subfigure}
    \end{minipage}
    \vspace{-4mm}
    \caption{Graph-based isolation level selection model}
    \label{fig:graph_learning}
    \vspace{-4mm}
\end{figure}

% In practice, the workload often varies over longer periods of time due to changes in aspects as diverse as 
% % In the actual operation of the application, the uncertainty of the user's behavior causes the workload with dynamic inputs, where the dynamics are reflected in the change of 
% the transaction template ratio and the distribution of hotspots. There is not always an optimal isolation for all workloads because the lower the isolation level in RDMBS, the more data anomalies we need to detect and handle in \sysname. To address this, \sysname introduces a graph construct model to achieve self-adaptive isolation level selection.
% \sysname can adaptively adjust the isolation strategy and optimize performance in the dynamic load. In this section, we study how to characterize the workload and how to predict the optimal isolation strategy for special workloads. 

% In practice, the workload often varies over longer periods due to variations in factors such as the transaction template ratio and key distribution. 

Selecting optimal isolation levels for all transactions in a workload while maintaining SER is challenging as we need to balance the overhead and performance gain in different isolation levels.  
% \ls{We should be consistent with the namings throughout the paper. I did not see anomaly prevention is the transaction processing section. Yet it seems we mentioned this term in the overview?}. 
Inspired by existing approaches that conduct workload prediction~\cite{DBLP:conf/icde/ZhengZLYCPD24, DBLP:conf/sigmod/MaAHMPG18, DBLP:journals/pvldb/YuZSY24} for preemptive database tuning or index creation, which predicts future workloads based on the current workload features,  
% Therefore, it is possible to predict the optimal isolation level for future workloads based on the current workload characteristics,  
we propose a neural-network-based isolation level prediction approach, which predicts the future optimal isolation level based on the current workload features. 
The main challenges are effective workload feature selection and representation. 
Towards this end, \sysname adopts transaction dependency graphs to capture the workload features and adopts a {\color{blue}graph classification model} to facilitate self-adaptive isolation level prediction.  
% \sysname introduces a graph model designed to facilitate self-adaptive isolation level selection and predicts the isolation level through three procedures: \textit{Graph construction}, \textit{Graph Embedding}, and \textit{Prediction}. 
% We sample runtime transactions based on Mente Carlo sampling and construct dependency graphs as input to this model. The model outputs the optimal isolation level for the current workload. 
% \qy {
% In practice, the workload often fluctuates over extended periods due to variations. Fortunately, research has identified that workloads tend to follow certain patterns~\cite{DBLP:conf/icde/ZhengZLYCPD24, DBLP:conf/sigmod/MaAHMPG18, DBLP:journals/pvldb/YuZSY24}, enabling the prediction of future workloads and allowing for preemptive database tuning or index creation. Additionally, performance is closely related to the dependency graph constructed from sampled transactions. Therefore, it is possible to predict the optimal isolation level for future workloads based on the current workload characteristics.
% }


\subsubsection{Graph construction.} 
% There is a main challenge in workload modeling: concurrent transactions in the workload may have different types of queries and various read/write sets. Thus, it is not trivial to model workloads using traditional fix-length encoding methods. 
% To address this problem, we propose a graph-structured workload model comprising three matrices: a vertice matrix, an edge index matrix, and an edge attribute matrix. 

{
\color{blue}
To extract the complex features of concurrent transactions, \sysname proposes a graph-structured workload model, which is composed of three matrices, a vertex matrix, an edge index matrix, and an edge attribute matrix. Formally, a workload graph is defined as $G=(V, E, A)$, where $V$ is the vertex matrix with each row representing the feature vector of an operator, $E$ is the edge index matrix where each entry $e_{ij}$ signifies the relationship between $v_i$ and $v_j$, and $A$ is the edge attribute index with each row representing the feature vector of an edge. 

\sysname dynamically constructs the runtime workload graph by continuously using a batch of transactions which are sampled using Monte Carlo sampling. Each transaction in the batch is mapped to a vertex $v_i$, and its feature vector $V_i$ is generated by extracting the number of data items in its read set and write set.
For each vertex pair $(v_i, v_j)$, if a data dependency exists between them, i.e., their read and write sets intersect, \sysname adds an edge entry $e_{ij}$ into the edge index matrix $E$. For each edge $e_{ij}$, \sysname extracts the data dependency type and the involved relations to generate its attribute $A_{e_{ij}}$. The data dependency type for $e_{ij}$ can be either RR, RW/WR, or WW. For example, if two transactions' read sets intersect, there is an RR dependency between them. Given that the number of relations and dependency types are fixed, one-hot encoding is employed to represent these two features within the attribute matrix.
}


% One of the main challenges in workload modeling is the complexity of characterizing concurrent transactions, which can involve diverse transaction features and varying read/write sets. 
% Traditional fixed-length encoding methods are often inadequate for this task~\cite{DBLP:journals/pvldb/ZhouSLF20}. 
% To overcome this deficiency, 
% To address this, we propose a graph-based workload model that utilizes three matrices: a vertice matrix, an edge index matrix, and an edge attribute matrix. 
% \sysname utilizes three matrices, i.e., a vertice matrix, an edge index matrix, and an edge attribute matrix, as features to characterize concurrent transactions. 
% \ls{The last sentence does not pin to the key point. Why using the three matrices solves the problem of fixed length encoding?} \qy{DONE}
% \ls{We'd better provide an examle graph for the concurrent transactions we've used in the paper, say Figure 5.}

% Formally, the vertex matrix, $V \in \mathbb{N}^{N \times 2}$, captures the features of $N$ transactions. Without losing generality, we use the number of data items in the read set $r\_cnt$ and write set $w\_cnt$ as the transaction feature, which can be represented as a two-dimensional vector $\langle r\_cnt, w\_cnt \rangle$. 


% is the set of transaction nodes and $E(i, j, r_{ij}, t_{ij})$ is set of the edge between $v_i$ and $v_j$, which have relationship $r_{ij}$ in table $t_{ij}$.

 
% Each transaction is derived from one of the pre-defined transaction templates, which indicate the database's behavior and access patterns. 
 
% We encode these transaction types using one-hot encoding. Given the limited number of transaction templates, we can predefine the size of the transaction type encoding. 
% Consider transaction $T_{ts}$ and $T_{wc}$ in \ls{Figure~\ref{} } as an example. The read set and write set of $T_{ts}$ each contain one data item. Meanwhile, $T_{wc}$ has a read set containing three data items and a write set containing one data item. As shown in Figure \ref{fig:graph_learning}, the node feature matrix $V$ includes two vectors: $\langle 1, 1 \rangle$ and $\langle 3, 1 \rangle$. \ls{Which transactions is Figure \ref{fig:graph_learning} modeling? WHat are the two dimensions in $\langle 1, 1 \rangle$ each represents?}

% The edge matrix, $E \in \mathbb{N}^{2 \times M}$, represents transaction dependencies. Different transactions may have such dependencies if they access the same data items. In \sysname, we characterize workload features by using the intersection of read/write sets to define these dependencies. The edge matrix records the indices of the dependency's source and destination transactions.

% The attribute matrix, $A\in \mathbb{N}^{M \times 2}$, encapsulates the dependency type and the involved relations of $M$ edges. The dependency type can be either RR, RW/WR, or WW. 
% For example, if two transactions' read sets intersect, there is an RR dependency between them. Given fixed relations and dependency types, we use one-hot encoding to represent these dependencies $d\_type$ and the involved relations $d\_rel$. Thus, edge attributes in matric $A$ are two-dimensional vectors: $\langle d\_type, d\_rel \rangle$. 
% % one dimension for the dependency type and the other for the table.
% % For instance, consider transactions $T_{ts}$ and $T_{wc}$ in Figure \ref{fig:transaction_lifecycle} with an RW dependency on the \textit{Savings} table. If $T_{ts}$ and $T_{wc}$ are indexed as 1 and 3 in matrix V, matrix $E$ contains the vector $\langle 1, 3 \rangle$. Encoding \textit{Savings} as \textit{100} and RW dependency as \textit{010}, matrix $A$ includes $\langle 010, 100 \rangle$.

% In summary, the matrix $V$ allows us to characterize the size of the read/write set of each transaction in the workload. Meanwhile, the matrices $A$ and $E$ capture the key distribution and identify dependencies that need to be handled either in the database, in the middle tier, or both. 
% By recording the intersection of transaction read/write sets, we can character the key distribution and identify dependencies. 
% \qy{
% The performance of the workload can be influenced by two factors: (1) The detection and scheduling cost in the middle tier, which is highly related to the transaction dependencies derived from the static vulnerable dependencies; (2) The isolation guarantee cost in the RDBMS, which is also highly related to the read/write operations and the dependencies between them. 
% Therefore, it is reasonable to use the transaction dependency graph as a feature to predict the optimal isolation levels. 
% }
% \ls{We should mention that the transaction dependency is highly related to some anomaly, which should be avoided by certain isolation levels. Therefore, it is reasonable to use the transaction dependency graph as features to predict the optimal isolation levels. }

\subsubsection{Graph embedding and isolation prediction} %To predict the optimal strategy for specific workloads, we use the vertice matrix ($V$) and edge matrix ($E, A$). 
% One main challenge here is that our graph has attributes not only on nodes but also on edges. Hence, we need to pass the information onto edges to learn the features on the graph, and traditional models such as CNNs are difficult to apply in this scenario. Inspired by MPNN~\cite{DBLP:conf/icml/GilmerSRVD17} and Zhou et. al~\cite{DBLP:journals/pvldb/ZhouSLF20}, we split the prediction process into two parts: embedding network and prediction network, as shown in Figure~\ref{fig: graph}. 

{
\color{blue}
Predicting the optimal isolation strategy for the future workload using the constructed graph-structure model $G=(V, E, A)$ is challenging due to its complex structures, and dynamic and high-dimensional features, which require capturing both local and global dependencies. Heuristic methods rely on manually crafted rules that lack generalizability, while traditional machine learning models fail to leverage relational information encoded in the vertexes and edges, losing critical structural context. To address these challenges, we use a graph classification model that learns graph-level representations by aggregating node features through multiple layers of neural network-based convolutions.

As shown in Figure~\ref{fig:graph_learning}. Our graph model comprises two parts. First, we use a \textit{Graph Embedding Network} to learn and aggregate both vertex and edge features, producing node-level embedded matrix $H$ that encodes the local structure and attribute information of the graph. Second, to predict the optimal isolation strategy for the workload, we use a \textit{Graph Classification Network} that learns the mapping from the embedded matrix $H$ to perform the end-to-end graph classification to predict the optimal isolation strategy.

The \textit{Graph Embedding Network} is constructed with a three-layer architecture, where each layer applies an NNConv operation to update node representations. This process integrates node and edge features through a dynamic aggregation mechanism~\cite{DBLP:journals/pvldb/ZhouSLF20,DBLP:conf/pkdd/FurutaniSAHA19}. At each layer, an edge network maps the input edge features into higher-dimensional convolution kernels via a multi-layer perception (MLP), as shown in Eq.~\ref{eq: gnn}. This mapping dynamically transforms edge attributes into weights, which are then used during the node aggregation step. The NNConv operation produces updated node embeddings for each node $v$ using Eq.~\ref{eq: gnn}, where $\mathcal{N}(v)$ represents the neighbors of node $v$, $W^{(l)}_{e_{u,v}}$ is the edge-specific weight, and $\sigma$ denotes the active function (ReLU). Through this layer-wise propagation, the embedding module produces $H$, a set of node-level embeddings that encode the graph information.

\begin{equation}
    % H^{l+1} = \sigma^{l}(h^{l}_v + \sum_{u \in \mathcal{N}(v)} h^{l}_u W^{l})
    % \small
    \setlength\abovedisplayskip{3pt}
    \setlength\belowdisplayskip{3pt}
    \begin{cases}
    % W^{(l)}(e) = f^{(l)}(A_e) = \sigma \left( w_2^{(l)} \cdot \sigma \left( w_1^{(l)}A_e+b_1^{(l)} \right)+b_2^{(l)} \right) \\
    W^{(l)} = f^{(l)}(A) = MLP(A) \\
    H^{(l)}_v = \sigma \left( \underset{u \in \mathcal{N}(v)}{\max} \left( W^{(l)}_{e_{u,v}} \cdot H_u^{(l-1)} \right) \right)
    \end{cases}
    % \begin{cases}
    % D_i = [A_i^TA_{:,i}] H^{(l-1)} \\ 
    % H^{l+1} = \sigma^{l}(D^{-\frac{1}{2}} \tilde{A} D^{-\frac{1}{2}}W^{l}H^{l}) 
    % \end{cases}
    % H^{l+1} = \sigma^{l}(D^{-\frac{1}{2}} \tilde{E} D^{-\frac{1}{2}} H^{l} W^{l}) 
    \label{eq: gnn}
\end{equation} 

The \textit{Graph Classification Network} takes the node embeddings $H$ as inputs and passes them through two fully connected layers. The first layer applies a ReLU activation function to enhance nonlinearity. The second layer implements a softmax activation function and outputs a three-dimensional vector, with each field representing the probability of the isolation level being optimal.
}



% We use the vertice matrix ($V$), edge matrix ($E$), and attribute matrix ($A$) to predict the optimal strategy for specific workloads. 
% One primary challenge is the presence of features with both vertices and edges, requiring the propagation of information across edges to effectively learn the graph's features. Traditional models such as CNNs~\cite{DBLP:journals/corr/BrunaZSL13} are not readily applicable due to the complex, non-Euclidean structure of our inputs. Inspired by MPNN~\cite{DBLP:conf/icml/GilmerSRVD17} and Zhou et. al~\cite{DBLP:journals/pvldb/ZhouSLF20}, we divide the isolation level prediction process into two distinct phases: the embedding and prediction, as shown in Figure~\ref{fig:graph_learning}.

% {
% \color{blue}
% Features within concurrent transactions are complex, including both data dependencies and possible hidden features, such as data access distributions. Traditional ML models rely on manual feature specification, making it difficult to extract all related features and predict optimal isolation levels. Therefore, we use the graph embedding approach, which implements a GNN network to extract features automatically to provide accurate predictions.
% }

% The embedding network is designed to transform each vertice into a vertice vector that captures both verticees and edge features. This involves capturing the local graph structures around each vertice, conducting nonlinear mapping, and learning the embedding graph features by training network weights. Our training process follows a layer-wise propagation rule~\cite{DBLP:journals/corr/BrunaZSL13}. The model includes three convolutional layers, each utilizing an edge network to generate convolutional kernels based on edge attributes. These edge networks are lightweight neural networks that take edge attributes as input and output transformation matrices. Then, they aggregate information from neighboring nodes using these transformation matrices, effectively capturing the local structure and attributes of the graph. By iteratively applying message passing, information can propagate through the graph, enabling nodes to capture more global context. 
% \ls{This paragraph is messy. We should state two points about the network. First is the network structure, e.g., number of layers, input/output format and connections between layers. Second is the active function, objective fuction and training method. Similar structure applies to the prediction network. }

% The embedding network is designed to embed the workload graph into a high-dimension vector. It adheres to a layer-wise propagation rule~\cite{DBLP:journals/corr/BrunaZSL13}, incorporating three embedding layers. Each layer takes the embedding vector produced by the previous layer and the edge matrices as input and outputs the updated embedding vector. 
% Specifically, each layer employs an edge network to generate the aggregate information for vertices from the adjacent edges. 
% \ls{@QY Please check the following two sentences. } \qy{DONE}
% enabling the output vector to integrate more global contextual data. 

% The operation of one embedding layer with symmetric normalization can be mathematically described as follows 

% Here, $\sigma(*)$ represents the activation function~\cite{DBLP:journals/ijon/DubeySC22} used to facilitate diverse metric learning. 
% $A_i^T$ represents the transposed edge features associated with node $v_i$, $A_{:, i}$ represents all edge features related to node $v_i$ according to the index matric $E$; $H^{l}$ represents the node feature matrix in layer $l$; In this way, this model has the learning capability on graph-structured data.
% The matric $A$ represent the neighbour information computed by the edge index matrix $E$, and $W^{l}$ is the weight matric.  
% represents the weight matrix calculated by the lightweight edge network that generates a transformation matrix based on $A_{uv}$, thereby integrating the edge features into vertex features. 
 % We also integrate self-loops and employ symmetric normalization within the edge attributes matrix to enhance numerical stability and optimize the learning process. The modified edge attributes matrix $\tilde{A}$ with self-loops is defined as $\tilde{A} = A + I$, where $I$ is the adjacency matrix and $I_{ij} = 1$ if there is an edge between vertices $i$ and $j$ and $0$ otherwise. The degree matrix $D$ is then computed as: $D_{ii} = \sum_j \tilde{A}_{ij}$ and $D^{-\frac{1}{2}} \tilde{A} D^{-\frac{1}{2}}$ represent the The symmetrically normalized adjacency matrix of $A$ \cite{DBLP:conf/pkdd/FurutaniSAHA19}. 

% To facilitate diverse metric learning, we apply a specific activation function~\cite{DBLP:journals/ijon/DubeySC22} $\sigma^{(l)}(*)$ after each graph layer $l$. 
% , enabling non-linear transformations (e.g., ReLU~\cite{Relu}) suitable for complex learning tasks. 

% \ls{Give a reference and simplify the symmetrically normalized adjacency matrix computing process if it is a standard process. } \qy{DONE}

% The prediction network takes the high-dimensional vector produced by the embedding network as input. It outputs a three-dimensional vector, with each field representing the probability of the isolation level being optimal. We finally chose the isolation level with the highest probability. In the prediction network, we adopt a two-layer perceptron with one hidden layer and an output layer, which is particularly effective at extracting performance-related features. The hidden layer conducts data abstraction on $H^{t_0}$ and outputs an abstracted matrix $H^{t_1}$. Then, the output layer employs a log-softmax function to obtain the log probabilities for each class based on $H^{t_1}$. 

% is designed to predict the performance metrics based on the high-dimension vector generated by the embedding network. We adopt a two-layer perceptron, including a hidden layer and an output layer, which is particularly effective at extracting performance-related features from the embedded matrix $H^{(t_0)}$ provided by the embedding network. The hidden layer conducts data abstraction on $H^{(t_0)}$ and outputs an abstracted matrix $H^{(t_1)}$. Then, the output layer predicts the class scores based on $H^{(t_1)}$, passed through a log-softmax function to obtain the log probabilities for each class. 
% Each field represents the in the current workload. 

% transform each vertice into a vertice vector that captures both vertices and edge features. This transformation is achieved by capturing the local graph structures surrounding each vertice, performing nonlinear mappings, and learning the embedded graph features through the training of network weights. Our approach adheres to a layer-wise propagation rule~\cite{DBLP:journals/corr/BrunaZSL13}, incorporating three embedding layers. Each layer employs an edge network that processes edge attributes to produce convolutional kernels. These edge networks, which are lightweight neural architectures, accept edge attributes as inputs and generate transformation matrices as outputs. These matrices are then used to aggregate information from adjacent vertices, effectively capturing both the local structure and attributes of the graph. Through iterative message passing, information is propagated throughout the graph, enabling vertices to integrate more global contextual data. 
% To facilitate diverse metric learning, we apply a specific activation function~\cite{DBLP:journals/ijon/DubeySC22} $\sigma^{(l)}(*)$ after each graph layer $l$, enabling non-linear transformations (e.g., ReLU~\cite{Relu}) suitable for complex learning tasks. We also integrate self-loops and employ symmetric normalization within the edge attributes matrix to enhance numerical stability and optimize the learning process. The modified edge attributes matrix $\tilde{A}$ with self-loops is defined as $\tilde{A} = A + I$, where $I$ is the adjacency matrix and $I_{ij} = 1$ if there is an edge between verticees $i$ and $j$ and $0$ otherwise. The degree matrix $D$ is then computed as: $D_{ii} = \sum_j \tilde{A}_{ij}$. The symmetrically normalized adjacency matrix $\hat{A}$ is then denoted as $\hat{A} = D^{-\frac{1}{2}} \tilde{A} D^{-\frac{1}{2}}$. The operation of one convolution layer with symmetric normalization can be mathematically described as equation~\ref{eq: gnn}, where $\mathcal{N}(i)$ is computed by edge index matrix $E$ and the edge network $f$ is a lightweight neural network that generates a transformation matrix based on $A_{ij}$, thereby integrating the edge features into vertice features.
% \begin{equation}
%     H^{l+1}_i = \sum_{j \in \mathcal{N}(i)} \hat{A}_{ij} f(A_{ij}) \cdot H^{l}_j
%     \label{eq: gnn}
% \end{equation}

% First, we initialize the node feature $H^{(0)} = V$ and the edge feature matrix $\mathbf{A}$, along with the parameters of the three-layer perceptron used to generate the weight matrix $\mathbf{W}_l(\mathbf{A})$ for each layer $l$. The adjacency matrix $\mathbf{I}$ is also defined, where $I{ij} = 1$ if there is an edge between nodes $i$ and $j$ and $0$ otherwise. During forward propagation, for each layer, the degree matrix $\mathbf{D}$ is computed, where $D_{ii}$ represents the degree of node $i$. The weight matrix is generated using the MLP based on the edge features $\mathbf{A}$. The convolution operation is then applied with symmetric normalization:

% \begin{equation}
%     \mathbf{H}^{(l)} = \sigma \left( \mathbf{D}^{-1/2} \mathbf{A} \mathbf{D}^{-1/2} \mathbf{W}_l(\mathbf{A}) \mathbf{H}^{(l-1)} \right)
% \end{equation}

% The purpose of this normalization technique is to stabilize the training process and improve model performance by accounting for the varying degrees of nodes. The loss $\mathcal{L}$ is then computed based on the final node feature matrix $\mathbf{H}^{(L)}$ and the ground truth labels, typically using cross-entropy loss for classification tasks or mean squared error for regression tasks. Backpropagation is performed to compute the gradients of the loss with respect to the model parameters, which are subsequently updated using an optimization algorithm such as Stochastic Gradient Descent (SGD) or Adam. This iterative process continues for a predefined number of epochs or until convergence, aiming to minimize the loss and optimize the model parameters.

% \ls{We should explicitely mention the input and output of our model and explain why we can use the dependency graph of time t1 to predict the isolation level of time t2. We should also explicitly state whether the two models are trained together or seperately. }



% The loss function is vital in graph embedding, which measures the accuracy of graph embedding algorithm between predicted performance and real performance. There are two challenges to design good loss functions. 


\subsubsection{Data collection and labeling. } Our modeling approach is somewhat general and not designed specifically for specific workloads. However, in practice, we train the model separately for each type of benchmark for efficiency considerations. Taking YCSB+T as an example, we generate 600 random workloads with varying read/write ratios and key distributions. 
Each workload is executed under each isolation level for 10 seconds, with sampling intervals of 1 second, and the optimal isolation level is labeled based on throughput. 
We follow the same process for data collection and labeling in Smallbank. 
% To eliminate the effect of randomness during transition execution, we repeat the  workload execution, sampling and labelling process three times, and adopt the . 


% After this execution, we label the optimal isolation strategy for each workload based on their performance. 

\subsubsection{Model training.} In \sysname, we train the embedding and prediction network together and use cross-entropy loss for multi-class classification. 
% Let $Y$ denote the ground truth labels and $\hat{Y}$ denote the predicted probability distribution over the classes, then the cross-entropy loss can be calculated by $\mathcal{L} = - \Sigma Y \log(\hat{Y})$. 
Backpropagation involves calculating the gradients of the loss function concerning the parameters of the graph model. First, the gradient is computed for the output layer. Then, using the chain rule, these gradients are propagated backwards through the whole network, updating the parameters of each layer. For embedding layers, this process includes computing gradients for both vertex features and transformation matrices derived from edge attributes.


% Additionally, the loss function and backpropagation are critical components that govern the training process and optimization objective. 


\subsection{Cross-isolation Validation \label{design-3}} \label{sec:switch_mechanism}
If the predicted optimal isolation level changes, \sysname will adapt from the previous isolation level $I_{old}$ to the predicted isolation level $I_{new}$. Non-serializable scheduling may occur during the isolation level transition process. 
We provide a formal proof of this in Section \ref{sec:proof.switch}. 
% when concurrent transactions operate under different isolation levels. A discussion of the detailed proofs for the theorems presented in this section will be provided in Section \ref{sec:proof.switch}.
% The concurrency control mechanisms designed for individual isolation levels are inadequate to detect or handle these situations.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/cross-isolation.pdf}
    \vspace{-2mm}
    \caption{Cross-isolation validation}
    \label{fig:cross-isolation}
    \vspace{-4mm}
\end{figure}


\begin{example}
    \label{exa:cross-isolation}
    Figure~\ref{fig:cross-isolation} illustrates non-serializable scheduling during the transition from SER to RC after $T_2$ commits, making $T_1$ and $T_2$ operate under SER while $T_3$ operates under RC. In this scenario, $T_1$ is expected to be aborted to ensure SER. However, existing RDBMSs do not handle dependencies between transactions under different isolation levels, allowing $T_1$ to commit successfully, leading to non-serializable scheduling.
    Note that when transactions $T_1$, $T_2$, and $T_3$ are all executed under SER, the concurrency control in RDBMS prevents such non-serializable scheduling. 
    % Transactions $T_1$ and $T_2$ operate under SER, and the isolation level transition occurrs after the commitment of $T_2$. Transaction $T_3$ starts after the transition and operates under RC. A dependency cycle, $T_3 \xrightarrow{rw} T_1\xrightarrow{rw} T_2 \xrightarrow{ww} T_3$, emerges that involves transactions across different isolation levels, resulting in non-serializale scheduling. \ls{It seems this example is non-serializable regardless of whether it occurs during isolation level transition or not. }
    \qed
\end{example}


We need to explicitly consider the situations of cross-isolation transitions to ensure the correct transaction execution during the process. 
A straightforward approach is to wait for all transactions to complete under the previous isolation level before making the transition. In the example above, this would mean blocking $T_3$ until $T_1$ commits. 
However, it can result in prolonged system downtime, especially when there are long-running uncommitted transactions. Another possible approach is to abort these uncommitted transactions and retry them after the transition, which leads to a high abort rate. 
To mitigate these negative impacts, \sysname employs a cross-isolation validation (CIV) mechanism that ensures serializability and allows for non-blocking transaction execution without a significant increase in aborts. 
Specifically, we extend the vulnerable dependency under the single isolation level in Definition~\ref{def:vul} to the cross-isolation vulnerable dependency, defined as follows:
\begin{definition}[Cross-isolation vulnerable dependency]
    \label{def:transition_vul}
    The cross-isolation vulnerable dependency is defined as $T_j \xrightarrow{rw} T_k $ in chain $T_i \xrightarrow{rw} T_j \xrightarrow{rw} T_k$ where three transactions {\color{blue} can execute} under two different isolation levels.
    \qed
\end{definition}

We generalize Theorem \ref{the:vulnerable} to obtain corollary~\ref{cor:transition_ser} and prove it in Section~\ref{sec:proof.switch}.  
\begin{corollary}
    \label{cor:transition_ser}
    For any cross-isolation vulnerable dependency $T_i \xrightarrow{rw} T_j$, if $T_i$ commits before $T_j$, then the transaction scheduling during the isolation transition is serializable.
% Transitions between SI and RC are safe because transactions under either SI or RC would undergo the responding middle-tier concurrency control, ensuring consistency between the commit order and dependency order of $T_j$ and $T_k$. It violates the structure of the dependency cycle given in Theorem \ref{the:cross-isolation}. 
\qed
\end{corollary}

Based on Corollary \ref{cor:transition_ser}, we implement our CIV mechanism by detecting all cross-isolation vulnerable dependencies during the isolation-level transition and ensuring the consistency of the commit and dependency orders. The CIV mechanism includes three steps. (1) When the system transitions from the current isolation $I_{old}$ to the optimal isolation level $I_{new}$, the middle tier blocks new transactions from entering the validation phase until all transactions that have entered the validation phase before the transition commit or abort. Importantly, we only block transactions to enter the validation phase. Transactions can execute normally without blocking. (2) After that, the transaction that has completed the execution phase enters the cross-isolation validation phase. During the cross-isolation validation phase, transactions request validation locks according to the stricter locking method of either $I_{old}$ or $I_{new}$ to ensure that all cross-isolation vulnerable dependencies can be detected. For example, when transitioning from SI to RC, the transaction in the cross-isolation validation phase requests validation locks following RC's validation locking method, regardless of whether it is executed under SI or RC. (3) After acquiring validation locks, transaction $T_i$ first detects vulnerable dependencies of its original isolation level. Then, it detects cross-isolation vulnerable dependencies by checking whether a committed transaction modifies its read set (using the same detection method as that in Section~\ref{sec:design:cc:validation}). If such modifications are detected, $T_i$ is aborted to ensure the consistency of the commit and dependency orders. 

Once all transactions executed under $I_{old}$ are committed/aborted, the transition process ends, and new transactions will operate under $I_{new}$, which are validated in the process described in Section \ref{sec:design:cc:validation}. 
% CIV is no longer required until the next isolation level transition starts. 

{
\color{blue}
Specifically, we find that transitions between SI and RC are safe, which is proved in Section~\ref{sec:proof.switch}. 
}

% \begin{algorithm}[t]
%     \caption{The algorithm for isolation level switch}
%     \small
%     \label{alg.switch}
%     \SetKwFunction{ExtraValidation}{ExtraValidation}
%     \SetKwProg{Fn}{Function}{:}{}
%     % \setstretch{0.85}

%     \Fn{\ExtraValidation{T}} {
%         \For{r \textbf{in} T.write\_set} {
%             \textit{entry := find\_entry(r.key)}, \textit{v := r.version}\\
%             % v := r.version \\
%             \If {T $\in$ $S_{old}^{(2)}$} {
%                 \If {v $\ge$ entry.rv\_n \textbf{or} v $\ge$ entry.wv\_n} {
%                     \Return {ERROR} \\
%                 }
%             }
%             \Else {
%                 \If {v $\le$ entry.rv\_o \textbf{or} v $\le$ entry.wv\_o} {
%                     \Return {ERROR} \\
%                     % entry.version\_n := \textit{min}(entry.version\_n, r.version) \\
%                 }
%             }
%         }
%         \For{r \textbf{in} T.read\_set} {
%             \textit{entry := find\_entry(r.key)}, \textit{v := r.version} \\
%             % v := r.version \\
%             \If {T $\in$ $S_{old}^{(2)}$} {
%                 \If {v $\ge$ entry.wv\_n} {
%                     \Return {ERROR} \\
%                     % entry.version\_o := \textit{max}(entry.version\_o, r.version) \\
%                 }
%             }
%             \Else {
%                 \If {v $\le$ entry.wv\_o} {
%                     \Return {ERROR} \\
%                     % entry.version\_n := \textit{min}(entry.version\_n, r.version) \\
%                 }
%             }
%         }
%         \Return {SUCCESS} \\
%     }

%     \SetKwFunction{ExtraAfterCommit}{ExtraAfterCommit}
%     \SetKwProg{Fn}{Function}{:}{}

%     \Fn{\ExtraAfterCommit{T}} {
%         \For{r \textbf{in} T.write\_set} {
%             \textit{entry := find\_entry(r.key)}, \textit{v := r.version} \\
%              % \\
%             \If {T $\in$ $S_{old}^{(2)}$} {
%                 \textit{entry.wv\_o := max(entry.wv\_o, v)} \\
%             }
%             \Else {
%                 \textit{entry.wv\_n := min(entry.wv\_n, v)} \\
%             }
%         }
%         \For{r \textbf{in} T.read\_set} {
%             \textit{entry := find\_entry(r.key)}, \textit{v := r.version} \\
%             % \textit{v := r.version} \\
%              % \\
%             \If {T $\in$ $S_{old}^{(2)}$} {
%                 \textit{entry.rv\_o := max(entry.rv\_o, v)} \\
%             }
%             \Else {
%                 \textit{entry.wv\_n := min(entry.wv\_n, v)} \\
%             }
%         }
%     }
% \end{algorithm}

% \subsubsection{Algorithm explaination}
% The validation process is encapsulated in the \texttt{ExtraValidation} within Algorithm~\ref{alg.switch}. 
% For a transaction $T_{old}^{'}$ running under the old isolation level, its dependency relationship with a new transaction $T_{new}^{'}$ can be determined by comparing version and $version\_n$. If version is greater than or equal to $version\_n$, it indicates that $T_{old}^{'}$ depends on $T_{new}^{'}$, which is not permitted by \sysname (lines 4-8). Similarly, for $T_{new}^{''}$, if it detects a larger version committed by an old isolation level transaction $T_{old}^{''}$, indicting $T_{old}^{''}$ depends on $T_{new}^{''}$, then $T_{new}^{''}$ cannot be committed. Each entry is locked and released after a commit, similar to ValidationLock, to ensure consistency. 
% However, it is important to note that this approach can result in false positives, as it does not differentiate between write and read operations. Despite this limitation, this lightweight approach is effective due to the limited number of transactions running under the old isolation level during the switching phase. 
