\section{Introduction}
Serializable isolation level (SER) is regarded as the gold standard for transaction processing due to its ability to prevent all forms of anomalies.
SER is essential in mission-critical applications, such as banking systems in finance and air traffic control systems in transportation, which require their data to be 100\% correct \cite{DBLP:journals/pvldb/ChenPLYHTLCZD24_TDSQL}.
However, it incurs expensive coordination overhead by configuring the RDBMS to SER \cite{DBLP:journals/pvldb/VandevoortK0N21}.
% ensuring SER in databases requires scheduling all transactions equivalent to a serial order, which typically incurs expensive coordination overhead.
Despite significant efforts to alleviate this overhead~\cite {DBLP:conf/icde/LometFWW12,DBLP:conf/sosp/TuZKLM13,DBLP:conf/sigmod/YuPSD16}, maintaining a serial order of transactions to be scheduled remains a fundamental performance bottleneck.

% Relational database management systems (RDBMSs) often offer multiple isolation levels to support diverse applications.
% Thus far, there are two categories of approaches to achieving SER, namely built-in approaches and plug-in approaches.
% The first category of approaches focuses on designing new concurrency control algorithms that replace the existing two-phase locking approach (2PL) or serializable snapshot isolation (SSI) inside RDBMSs. 
% Recent works include TCM~\cite{DBLP:conf/icde/LometFWW12}, Silo~\cite{DBLP:conf/sosp/TuZKLM13}, TicToc \cite{DBLP:conf/sigmod/YuPSD16}, Sundial~\cite{DBLP:journals/pvldb/YuXPSRD18}, and RedT \cite{DBLP:journals/pvldb/ZhangLZXLXHYD23}.
% Relational database management systems (RDBMSs) often offer multiple isolation levels

In recent years, many studies have explored achieving SER by modifying applications while configuring the RDBMS to a low isolation level \cite{DBLP:journals/tods/KetsmanKNV22}. This approach is driven by two key reasons. First, some RDBMSs, such as Oracle 21c, cannot strictly guarantee SER and do not support the in-RDBMS modification of concurrency control, requiring application logic modifications to enforce it\extended{.
A more comprehensive list of RDBMSs and their supported isolation levels can be found in} \cite{DBLP:journals/pvldb/BailisDFGHS13}.
Second, RDBMSs typically offer better performance at lower isolation levels, such as read committed (RC) and snapshot isolation (SI), due to their more relaxed ordering requirements. Modifying applications to achieve SER while using a lower isolation level sometimes results in better performance compared to directly setting the RDBMS to SER~\cite{DBLP:conf/aiccsa/AlomariF15, DBLP:conf/icde/AlomariCFR08, DBLP:journals/pvldb/VandevoortK0N21}.

\begin{figure}[t]
    \centering
    \vspace{3mm}
    \begin{minipage}{0.95\linewidth}
        \centering
        % \begin{subfigure}{0.47\linewidth}
        %     \includegraphics[width=\linewidth]{figures/smallbank-1.pdf}
        %     % \caption{Original dependency graph}
        %     \label{fig:Smallbank:before}
        %     \vspace{-4mm}
        % \end{subfigure}
        \begin{subfigure}{\linewidth}
            \includegraphics[width=\linewidth]{figures/smallbank.pdf}
            % \caption{Modified dependency graph}
        \end{subfigure}
    \end{minipage}
    \vspace{-2mm}
    \caption{Static dependency graphs of Smallbank}
    \label{fig:SmallBank}
    \vspace{-4mm}
\end{figure}

The main idea of existing works that can achieve SER under low isolation levels follows three steps: 
\blackding{1} Build a {\em static dependency graph} by analyzing the transaction templates, which are the abstraction of transaction logics in real-world applications~\cite{DBLP:journals/pvldb/VandevoortK0N21, DBLP:conf/icdt/VandevoortK0N22}.  
% clients submit transactions through forms or programs, these transactions can be abstracted into templates. 
In this graph, each template is represented by a vertex, and the dependencies between templates, such as write-write (WW), write-read (WR), or read-write (RW), are depicted as edges.
% They model the transaction templates as a static dependency graph, where each template is represented by a vertex. The dependencies between templates, which can be write-write (WW), write-read (WR), or read-write (RW), are represented as edges.
\blackding{2} Configure the RDBMS to a low isolation level and identify {\em dangerous structures} that are permissible under the low isolation level but prohibited by SER according to the {\it static dependency graph}.
% Dangerous structures vary depending on the isolation level.
For example, under SI, a dangerous structure is characterized by two consecutive RW dependencies \cite{DBLP:conf/pods/Fekete05, DBLP:journals/sigmod/FeketeOO04}, while under RC, a single RW dependency constitutes the dangerous structure \cite{DBLP:conf/aiccsa/AlomariF15, DBLP:journals/pvldb/VandevoortK0N21}.
\blackding{3} Eliminate dangerous structures by modifying application logic, e.g., promoting reads to writes for certain SQL statements so that the RW dependencies are eliminated, and thus guarantees SER.

\begin{example}
\label{exa:oaofsb}
Consider the SmallBank benchmark \cite{DBLP:conf/icde/AlomariCFR08}, which consists of five transaction templates: Amalgamate (Amg), Balance (Bal), DepositChecking (DC), TransactSavings (TS), and WriteCheck (WC). Suppose the RDBMS is configured to RC.
As outlined in step \blackding{1}, the benchmark is modeled into a static dependency graph (shown in Figure \ref{fig:SmallBank}(a)). 
% The graph contains 16 WW dependencies, 5 WR dependencies, and 5 RW dependencies.
At step \blackding{2}, five dangerous structures (highlighted by red dashed arrows) are identified. These include the dependency from WC to TS and 4 dependencies from Bal to the other four templates. 
At step \blackding{3}, extra writes are introduced to convert RW dependencies to WW dependencies, thus eliminating the dangerous structures.
To achieve this, certain ``\textit{SELECT}'' statements are modified to ``\textit{SELECT ... FOR UPDATE}'' statements. 
\extended{Figure \ref{fig:sfu} illustrates the modification of Bal.}
\maintext{A more comprehensive illustration can be found in our supplementary material~\cite{TxnSails}.} 
For reference, the complete modified dependency graph, \extended{based on the original in Figure \ref{fig:SmallBank}(a), }is shown in Figure \ref{fig:SmallBank}(b).
\qed
\end{example}

\extended{
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/sfu.pdf}
    \vspace{-4mm}
    \caption{Modification of Bal transaction template}
    \label{fig:sfu}
    \vspace{-4mm}
\end{figure}
}

% the static dependency graph, shown in Figure \ref{fig:SmallBank}(a), is built. 
% Let us consider the financial application modeled in the SmallBank \cite{DBLP:conf/icde/AlomariCFR08}, with origin program dependencies shown in Figure \ref{fig:SmallBank}(a).

% We would like to note that Bal is involved in a cycle that progresses from Bal to DC, then to WC, and finally back to Bal. The dependency from Bal to DC is identified as a ``dangerous structure" under RC. 
% % while these templates have a WR dependency with Bal. When the RDBMS is configured under RC, 
% Employing either conflict materialization or promotion approaches requires extra writes within Bal transactions to maintain serializability under RC. The modified dependency graph is depicted in Figure \ref{fig:SmallBank}(b).
% This introduces false positives from two perspectives: (1) unnecessary write-write conflicts between Bal transactions accessing the same record and (2) excessive elimination of RW dependencies, which could otherwise be permissible with concurrent execution using the snapshot technique.

% identify the cyclic dependencies that conform to specific patterns, referred to as ``dangerous structures'' that are permissible under $I$ but can result in non-serializable scheduling.

% (2) the ``dangerous structures'' with dependency cycles that cannot be prevented at a given isolation level should be avoided.
% (3) select the isolation level that provides the best performance. 

% \qy {
% Existing approaches that achieve SS by modifying application logic typically follow three steps. First, the program developer selects an appropriate isolation level $I$ and constructs the static dependency graph based on the potential dependencies, including write-write (WW), write-read (WR), and read-write (RW) dependencies. Second, they analyze the graph to identify the cyclic dependencies that conform to specific patterns, referred to as ``dangerous structures'' that are permissible under $I$ but can result in non-serializable scheduling. Specifically, under SI, a ``dangerous structure'' is characterized by two consecutive RW dependencies \cite{DBLP:conf/pods/Fekete05, DBLP:journals/sigmod/FeketeOO04}, while under RC, a single RW dependency constitutes the ``dangerous structure'' \cite{DBLP:conf/aiccsa/AlomariF15, DBLP:journals/pvldb/VandevoortK0N21}. Finally, these ``dangerous structures'' under $I$ should be eliminated. A common strategy is to modify transaction programs and convert the RW dependencies into WW dependencies, typically through one of two methods: {\it materialization}, which introduces additional writes into the transaction programs that participate in the "dangerous structure" \cite{DBLP:conf/aiccsa/AlomariF15}; or {\it promotion}, which upgrades a read in the ``dangerous structure" to a write without compromising the semantics \cite{DBLP:conf/icde/AlomariCFR08, DBLP:journals/pvldb/VandevoortK0N21}. Such techniques enable critical applications to operate safely at a low isolation level while gaining performance benefits, especially in scenarios with low contention and minimal transactions requiring modification. 
% }

% Following this paradigm, various approaches that serve as a middleware system between applications and RDBMSs emerge to prevent additional anomalies outside RDBMSs.
% Such mechanisms allow critical applications to operate safely at lower isolation levels while gaining performance benefits, which may perform well with low contention and few modified transactions with extra writes.

% \textcolor{red}{Though running RMDBS at RC for the modified template in Figure \ref{fig:SmallBank}(b) can guarantee serializability,} 
% simply converting reads to writes can significantly degrade system performance for two reasons.
% First, when the workload of modified transactions becomes intensive, promoting reads to writes can block the execution of a large number of transactions.
% This is because RDBMSs often adopt MVCC, where read and write operations on the same data item do not cause conflicts, but two write operations on the same data item do. 
% Second, determining the optimal isolation level for dynamic workloads is non-trivial. Consider Example \ref{exa:oaofsb}.
% When the workload mainly consists of DC, Amg, and TS transactions, configuring the RDBMS to RC is preferable as it allows greater concurrency with low overhead for dangerous structure prevention.
% However, if Bal and WC transactions become intensive, the additional writes may lead to significant overhead, making SER a more efficient option.
% Thus, constantly configuring the RDBMS to a low isolation level does not guarantee optimal performance.

Thus far, existing studies~\cite{DBLP:journals/pvldb/BailisFFGHS14, DBLP:conf/pods/Fekete19, DBLP:conf/sigmod/BailisFHGS14, DBLP:journals/pvldb/FeketeGA09, DBLP:journals/tods/CahillRF09}  
have proposed promising solutions, enabling RDBMSs to achieve SER by operating at lower isolation levels while modifying specific query patterns within a workload. However, these studies exhibit two major shortcomings. 
First, the static modification of query patterns is inefficient.
These studies alter static SQL statements at the application level, converting certain read operations into write operations. This may result in unnecessary transaction conflicts. For instance, changing read operations to write operations may turn concurrent read-write operations into write-write conflicts in MVCC systems, thus significantly degrading transaction performance.
Second, these studies fail to address the key trade-off between the performance gains of lower isolation levels and the overhead needed to maintain SER, making it difficult to choose the optimal isolation level.
As shown in Figure \ref{fig:evaluation.dynamic} of \S\ref{sec:evaluation}, simply configuring the RDBMS to SER can sometimes outperform other methods. Additionally, as workloads evolve, the ideal isolation level may also shift, but existing studies lack the ability to adapt dynamically. 
% For example, when the workload is dominated by DC, Amg, and TS transactions, using RC allows for higher concurrency with minimal extra overhead. However, if Bal and WC transactions become frequent, the added write conflicts may cause significant overhead, making SER the better option.



% {
% \color{blue}
% Existing research has significantly refined the theory of isolation level \textit{robustness}~\cite{DBLP:journals/pvldb/BailisFFGHS14, DBLP:conf/pods/Fekete19, DBLP:conf/sigmod/BailisFHGS14, DBLP:journals/pvldb/FeketeGA09, DBLP:journals/tods/CahillRF09}, providing a theoretical foundation for configuring RDBMS to low isolation levels while ensuring SER. However, current solutions present two major shortcomings. (1) These approaches alter static SQL statements at the application level, converting certain read operations into write operations. This may result in unnecessary transaction conflicts. For instance, changing read operations to write operations may turn concurrent read-write operations into write-write conflicts in MVCC systems, significantly degrading transaction performance.
% % in MVCC-based RDBMSs, concurrent reads and writes on the same data item typically do not cause blocking; however, these approaches convert concurrent reads and writes into write-write scenarios, leading to blocking or rollbacks, which significantly degrades system performance. 
% (2) Configuring the RDBMS to lower isolation levels does not always outperform configuring RDBMS to SER, due to the trade-off between the overhead incurred to achieve SER and the performance gains from low isolation levels. Consider Example \ref{exa:oaofsb}.
% When the workload mainly consists of DC, Amg, and TS transactions, configuring the RDBMS to RC is preferable as it allows greater concurrency with little overhead for additional write operations. 
% However, if Bal and WC transactions become intensive, the additional write conflicts may lead to significant overhead, making SER a more efficient option.
% Unfortunately, the existing solutions ignore this trade-off. 
% These solutions primarily focus on lowering isolation levels to enhance efficiency. 
% sometimes requiring higher isolation levels to prevent hotspot conflicts that can lead to performance degradation. 
% Consequently, determining the optimal isolation level for RDBMSs under dynamic workloads is a complex challenge. 
%}

% \qy{
% Though existing works have significantly refined the isolation level theory~\cite{DBLP:journals/pvldb/BailisFFGHS14, DBLP:conf/pods/Fekete19, DBLP:conf/sigmod/BailisFHGS14, DBLP:journals/pvldb/FeketeGA09, DBLP:journals/tods/CahillRF09}, the solution to guarantee workload SER when RDBMS is set to low isolation levels, i.e., simply converting specific reads to writes, can significantly degrade system performance. When the workload of modified transactions promoting reads to writes can block the execution of a large number of transactions.
% This is because RDBMSs often adopt MVCC, where read and write operations on the same data item do not cause conflicts, but two write operations on the same data item do. 
% More importantly, in real-world applications~\cite{DBLP:conf/icde/ZhengZLYCPD24, DBLP:conf/sigmod/MaAHMPG18}, such as shopping~\cite{Double-Eleven, Black-Firday} and booking tickets, the workload is usually dynamic and periodic. Determining the optimal isolation of RDBMS for dynamic workloads is non-trivial. 
% Second, determining the optimal isolation level for dynamic workloads is non-trivial. Consider Example \ref{exa:oaofsb}.
% Consider Example \ref{exa:oaofsb}.
% When the workload mainly consists of DC, Amg, and TS transactions, configuring the RDBMS to RC is preferable as it allows greater concurrency with low overhead for dangerous structure prevention.
% However, if Bal and WC transactions become intensive, the additional writes may lead to significant overhead, making SER a more efficient option.
% Unfortunately, the existing solutions ignore the trade-off between the overhead incurred to prevent anomalies and the performance gains achieved by operating at lower isolation levels.
% }

% \qy{
% Moreover, in dynamic real-world workloads, the overhead of these modified statements with extra writes can evolve and potentially become a bottleneck. 
% In skewed workloads where the number of modified transactions increases, the heightened contention from concurrent writes may raise transaction abort rates, rendering the approach ineffective. 
% Furthermore, real-world workloads often exhibit dynamic data access patterns and fluctuating transaction program ratios, and no single isolation level consistently outperforms the others. Unfortunately, the above-mentioned solutions ignore the trade-off between the overhead incurred to prevent anomalies and the performance gains achieved by operating at lower isolation levels.
% }

% Let us consider the financial application modeled in the SmallBank \cite{DBLP:conf/icde/AlomariCFR08}, with origin program dependencies shown in Figure \ref{fig:SmallBank}(a).
% There are five transaction programs: amalgamate (Amg), balance (Bal), depositChecking (DC), transactSavings (TS), and writeCheck (WC), forming 16 WW dependencies, 5 WR dependencies, and 5 RW dependencies. 

% We would like to note that Bal is involved in a cycle that progresses from Bal to DC, then to WC, and finally back to Bal. The dependency from Bal to DC is identified as a ``dangerous structure" under RC. 
% % while these templates have a WR dependency with Bal. When the RDBMS is configured under RC, 
% Employing either conflict materialization or promotion approaches requires extra writes within Bal transactions to maintain serializability under RC. The modified dependency graph is depicted in Figure \ref{fig:SmallBank}(b).
% This introduces false positives from two perspectives: (1) unnecessary write-write conflicts between Bal transactions accessing the same record and (2) excessive elimination of RW dependencies, which could otherwise be permissible with concurrent execution using the snapshot technique.
% \qed
% a Bal transaction would obstruct the execution of other Bal transactions from accessing the same record. 
% Notably, no ``dangerous structure" occurs between Bal transactions, resulting in false positives in current approaches and ultimately rendering them ineffective.
% As the number of Bal transactions increases and the contention over accessed records intensifies, these extra writes may obstruct the execution of other Bal transactions, ultimately rendering these approaches ineffective.


In this paper, we present \sysname to address the aforementioned shortcomings with three key objectives:
\blackding{1} \sysname efficiently achieves SER under various low isolation levels.
\blackding{2} \sysname dynamically adjusts the optimal isolation level to maximize performance as the workload evolves.
\blackding{3} \sysname is designed to be general and adaptable across various RDBMSs, requiring no modifications to database kernels. To achieve this, \sysname is implemented as a middle-tier solution to enhance generalizability. 
However, implementing \sysname presents three major challenges. First, designing an approach that elevates various isolation levels to SER without introducing additional writes is a complex task. Second, determining the optimal isolation level requires accurately modeling the trade-offs between the performance benefits and serializability overhead associated with lower isolation levels, which is particularly challenging in dynamic workloads. Third, as workloads evolve, the optimal isolation level may need to adapt over time, making it essential to design an efficient and reliable mechanism for transitioning between isolation levels.
To address these challenges, we propose the following key techniques.


% % We aim to enable applications to adaptively operate at an isolation level and meet two requirements: 1) no anomalies and 2) optimal performance.
% % However, achieving these incurs three major challenges:
% % \blackding{1} Minimizing the overhead associated with anomaly prevention for various isolation levels is complex. 
% % \blackding{2} Accurately determining the optimal isolation level for dynamic workloads is not trivial.
% % \blackding{3} Enabling the isolation level can efficiently and correctly switch as workloads change is not straightforward.
% % \textcolor{red}{motivation example why need adaptive-selection, and description why current research does not work. }
% In this paper, we present \sysname to solve the above shortcomings with the following three key objectives: 
% \blackding{1} \sysname is efficient to achieve SER under various low isolation levels. 
% % It must be efficient to handle non-serializable scheduling under various low isolation levels and ensure SER.
% \blackding{2} \sysname is able to adjust the optimal isolation levels to maximize the performance when the workload evolves.
% \blackding{3} \sysname is general enough to be adapted for various RDMBSs, i.e., without any modifications to database kernels.
% % It requires minimal modifications to client applications and no modifications to database kernels. 
% % , ensuring high reusability and low implementation overhead.
% % The first and second requirements focus on reducing the overhead of achieving SER and considering the trade-off between the performance gains of lower isolation levels and the overhead needed to maintain SER, while the third requirement ensures broad applicability across various databases. 
% % The second requirement reduces the overhead caused by RW-to-WW conversions, while the third addresses the challenge of selecting the optimal isolation level.
% For this purpose, we choose to design \sysname as a middle-tier solution for generalizability. 
% Implementing \sysname still poses three key challenges. First, developing an approach that promotes various isolation levels to SER without additional writes and minimal overhead is not trivial. 
% {
% \color{blue}
% Second, modeling the trade-offs between the performance gains for low isolation levels and overheads to maintain SER is complex. Consequently, determining the optimal isolation level to achieve peak performance remains a complex task, especially for dynamic and periodic workloads, which are common in real-world applications~\cite{DBLP:conf/icde/ZhengZLYCPD24, DBLP:conf/sigmod/MaAHMPG18}, such as shopping~\cite{Double-Eleven, Black-Firday} and online payment \cite{DBLP:journals/pvldb/ChenPLYHTLCZD24_TDSQL}. Third, as workloads fluctuate, the optimal isolation level may need to change. Designing an efficient and correct transition between isolation levels is crucial.
% }
% To address these challenges, we propose the following key techniques.

% \todo{
% highlight the necessity of using     a middle-tier
% }

% designed to address two key requirements: \blackding{1} light overhead for preventing anomalies under lower isolation levels while ensuring serializable scheduling, and \blackding{2} adaptively selecting the optimal isolation level to maximize performance in response to dynamic workloads. To achieve these, we propose three core techniques, with the first addressing the requirement \blackding{1}, and the second and third techniques focusing on the requirement \blackding{2}.
% The key techniques and contributions are summarized as follows.
% ensures serializable scheduling for various isolation levels and allows for self-adaptive selection under dynamic workloads. We propose three key techniques to solve the abovementioned challenges. 
% eliminate anomalies and determine the optimal isolation level for various workloads. 
% \todo{Trace a global dependency graph of all transactions is costly...}

\textbf{(1) Efficient middle-tier concurrency control algorithm ensuring SER for each low isolation level (\S\ref{design-1}).} 
% Existing works rely on a static, coarse-grained approach based on transaction templates to analyze static dependency graphs and eliminate dangerous structures. However, this approach is overly conservative, as not all dangerous structures lead to violations of SS in actual transaction executions.
% To address this issue, 
We propose a runtime, fine-grained approach that operates on individual transactions rather than transaction templates, ensuring that the execution of transactions meets the requirements of SER. This approach is inspired by the theorem that a schedule is serializable if it does not contain two transactions, $T_i$ and $T_j$, where $T_j$ commits before $T_i$, but there is a dependency from $T_i$ to $T_j$ \cite{DBLP:conf/aiccsa/AlomariF15}.
Building on this theorem, we introduce a unified concurrency control algorithm to ensure SER. 
The algorithm tracks transactions with their templates involved in specific RW dependency within a static dependency graph. 
It detects the runtime dependencies and schedules the commit order to align with their dependency order. If necessary, it will abort a transaction to guarantee SER.
% Upon committing any transaction, it checks if the commit order aligns with the dependency order. If they align, the transaction is committed; otherwise, it is aborted or blocked.
% Additionally, several optimizations are proposed to reduce the overhead of monitoring dependencies between transactions.
% {
% \color{blue}
% Rather than implementing a new concurrency control mechanism within the database, the middle-tier implementation can achieve lightweight integration with various databases, provided it adheres to the isolation level definition specified in the previous theorem~\cite{DBLP:conf/icde/AdyaLO00, DBLP:journals/pvldb/PortsG12}.
% }
% As opposed to existing works that adopt static, coarse-grained approach based on transaction templates, our allow 
% Our algorithm is efficient as it avoids introducing additional write conflicts between transactions, thereby minimizing the impact on workload concurrency.
% \qy{
% Inspired by the theorem that a schedule is guaranteed to be serializable if it does not contain two transactions $T_i$ and $T_j$, where $T_j$ commits before $T_i$ despite having the RW dependency from $T_i$ to $T_j$ \cite{}. 
% % given any two transactions, $T_i$ and $T_j$, even though there is an RW dependency from $T_i$ to $T_j$, the schedule is non-serializable if $T_j$ commits before $T_i$. 
% Based on this theorem, our mechanism orchestrates the commit order of transactions, e.g., $T_i$ and $T_j$, in the middle tier to ensure serializability. This approach first identifies a portion of specific transactions that require scheduling in the middle tier based on the features of transaction templates and the concurrency control capabilities provided by the underlying database with different isolation levels. It then detects runtime dependencies between them and maintains the commit order consistent with the dependency order. 
% % we introduce a middle-tier concurrency control mechanism for a portion of specific transactions, e.g., $T_i$ and $T_j$, before they can commit. 
% % This mechanism schedules transactions that may cause non-serializability to keep the commit order consistent with the dependency order, 
% Moreover, \sysname is equipped with a contention-aware cache to minimize database interactions involving the mechanism, further reducing overhead.
% }
% \lw{Based on this theorem, we introduce a mid-tier concurrency control mechanism for a portion of specific transactions, e.g., $T_i$ and $T_j$, before they can commit. 
% This mechanism detects runtime dependencies and ensures the commit order is consistent with the dependency order. Otherwise, one of the transactions is aborted to prevent non-serializable scheduling.}
% \qy{
% However, this paradigm is overly conservative because not all dangerous structures lead to runtime violations of SS. To address this issue, we propose a dynamic, fine-grained approach based on runtime transactions to detect the dependencies and prevent them from non-serializable scheduling. 
% Our approach is motivated by the fact that dangerous structures leading to non-serializable schedules always involve two transactions, $T_i$ and $T_j$, where $T_j$ commits before $T_i$ despite having the RW dependency from $T_i$ to $T_j$. 
% We introduce a mid-tier concurrency control mechanism for a portion of specific transactions, e.g. $T_i$ and $T_j$, before they can commit. 
% This mechanism detects runtime dependencies and ensures the commit order is consistent with the dependency order. Otherwise, one of the transactions is aborted to prevent non-serializable scheduling. 
% Additionally, \sysname is equipped with a contention-aware cache to minimize database interactions involving concurrency control, further reducing overhead.
% }
% We prove the correctness of the unified concurrency control algorithm at each low isolation level in Section~\ref{sec:proof.isolation}. 

% Instead, we propose a dynamic, fine-grained approach that is equipped with a unified concurrency control to detect, and prevent dangerous structures.
% Our approach is motived by the fact that, given two transactions $T_1, T_2$, if there exists a RW depdency from $T_1$ to $T_2$, $T_1$ must commit before $T_2$.

% \qy{
% We observe that not all dangerous structures induce anomalies; only specific inconsistencies between commit and dependency orders in the dangerous structure do. Therefore, instead of eliminating static dangerous structures by modifying the workload, we focus on fine-grained runtime dependency detection to prevent anomalies, thereby reducing unnecessary transaction blocking.
% Initially, we perform offline analysis on transaction templates to identify dangerous structures under each low isolation level. During transaction processing, we introduce additional concurrency control in the middle tier for transactions that could potentially result in anomalies before they commit. 
% In this concurrency control, we detect runtime dependencies between the current transaction and others, ensuring the commit order consistent with the dependency order. Otherwise, if they are inconsistent, we abort the current transaction to prevent the anomaly.
% Furthermore, we only track a portion of specific dependencies within dangerous structures to reduce the concurrency control overhead. Additionally, \sysname features a contention-aware cache for metadata of hotspot data items, reducing database interaction during the concurrency control processing. }
% As a result, compared to existing approaches, \sysname operates at a fine granularity,  and efficiently achieving serializable scheduling (SS) under various isolation levels in a lightweight manner.

% Under SI, the ``dangerous structure'' is characterized by two consecutive RW dependencies~\cite{DBLP:conf/pods/Fekete05, DBLP:journals/sigmod/FeketeOO04}. Conversely, under RC, the ``dangerous structure'' is defined by a RW dependency~\cite{DBLP:conf/aiccsa/AlomariF15, DBLP:journals/pvldb/VandevoortK0N21}.
% the validation phase introduces additional interaction with RDBMS, which minimizes the negative impact on transaction latency; we equip \sysname with a contention-aware cache to further reduce the overhead of validation.
% As we know, the presence of dangerous structures is a necessary condition for forming dependency cycles under lower isolation levels. 
% Furthermore, we observe that there is another necessary condition that implies two transactions, $T_i$ and $T_j$, $T_i$ depends on $T_j$ while $T_j$ commits before $T_i$. Based on this, we design a lightweight validation mechanism for transactions that are potentially involved in dangerous structures. 
% The validation allows us to detect runtime dependencies and check whether the commit order can be consistent with the dependency order. Otherwise, we abort one transaction to prevent the existence of a dependency cycle. However, the validation would introduce additional interaction with the database. To minimize the negative impact on transaction latency, we enhance \sysname with a contention-aware cache. We identify hotspots using a \textit{lease} mechanism and cache the validation information metadata for these hotspots, further reducing the overhead. 

% The objective of conducting offline dependency analysis across transaction templates is to pinpoint the ``dangerous structures'' acceptable at lower isolation but might form dependency cycles that violate SER constraints. In SI, the ``dangerous structure'' is characterized by two consecutive RW dependencies~\cite{DBLP:conf/pods/Fekete05, DBLP:journals/sigmod/FeketeOO04}. Conversely, under RC, the ``dangerous structure'' is defined by a RW dependency~\cite{DBLP:conf/aiccsa/AlomariF15, DBLP:journals/pvldb/VandevoortK0N21}.
% The existence of ``dangerous structures'' is a necessary condition for forming dependency cycles under lower isolation levels. 
% two transactions are in the cycle, and their dependency order is inconsistent with the commit order. Formally, 
% Moreover, there is always a dependency from $T_i$ to $T_j$ such that $T_j$ commits before $T_i$. Instead of statically introducing extra writes into the transaction templates, we add a validation phase before transactions can potentially induce ``dangerous structures''. During this phase, we detect runtime dependencies and ensure the commit order is consistent with the dependency order. 
% The correctness of our validation-based concurrency control algorithm is proven in section~\ref{sec:correctness}. 

% Instead of statically introducing extra writes into the workload, \sysname promotes a validation-based concurrency control algorithm that dynamically detects dependencies and prevents ``dangerous structures'' 
% \sysname works in the middleware, and 
% designed to achieve efficient serializable transaction scheduling through self-adaptive isolation level selection. To address the challenges inherent in this task, we have developed three key techniques:

\textbf{(2) Self-adaptive isolation level selection mechanism (\S\ref{design-2}).} 
% To capture the dynamic characteristics of the workload, we propose an isolation-level prediction module using graph embedding. \sysname first asynchronous samples a batch of transactions based on Monte Carlo sample approach~\cite{} to represent the current workload. \sysname then uses a graph model to represent the workload characteristics, in which the vertices represent transaction features and edges between two transactions denote the data correlations. Then, \sysname feeds the workload graph into the prediction model equipped with graph neural network and message-passing techniques. Finally, the workload can be translated into three possible labels (RC, SI, SER), with the label corresponding to the largest value indicating the optimal isolation level for the current workload, as learned by our model.
% 
Rather than directly quantifying a cost model to balance the trade-off between the performance benefits of lower isolation levels and the overhead required to maintain serializability, we employ a learned model leveraging graph neural networks~\cite{DBLP:journals/corr/BrunaZSL13} and message-passing techniques~\cite{DBLP:conf/icml/GilmerSRVD17} to predict the optimal isolation level for a given workload.
Our observations reveal that the performance of various isolation levels and the overhead of achieving SER are closely influenced by two critical factors: the data access dependencies between transactions and the data access distribution within transactions. To capture these relationships, we model workload features as a graph, where vertices represent individual transaction features and edges denote data access dependencies between transactions.
Building on this insight, we propose a graph-based model for dynamic workloads that predicts the optimal isolation level using real-time workload features.
To the best of our knowledge, \textit{\sysname is the first work to enable self-adaptive isolation level selection for dynamic workloads.}

% {\color{blue}
% We propose an isolation-level selection mechanism that considers the trade-off between the performance gain of low isolation levels and the overhead of achieving SER and can always select the optimal isolation level to achieve peak performance under dynamic workloads.
% We observe that the performance of different isolation levels and the overhead of achieving SER are closely tied to two key characteristics: the data access dependencies between transactions and the data access distribution within transactions.
% }
% We observe that the optimal isolation level for different workloads is closely tied to two key characteristics: the data access dependencies between transactions and the data access distribution within transactions. 
% Based on this insight, we propose a graph model for dynamic workloads, which predicts the optimal isolation level using real-time workload features.
% Specifically, we model the workload features as a graph, where vertices represent individual transaction features and edges represent data access dependencies between transactions. Using this graph model as inputs, \sysname employs a learned model equipped with graph neural networks~\cite{DBLP:journals/corr/BrunaZSL13} and message-passing~\cite{DBLP:conf/icml/GilmerSRVD17} techniques to predict the optimal isolation level for the workload. To the best of our knowledge, \textit{\sysname is the first work to enable self-adaptive isolation level selection for dynamic workloads.}

% we propose a graph model to represent workload characteristics and implement a graph embedding 

% choose the optimal isolation level proactively according to the expected workload characteristics in the future (for future or for current workload?, according to the sampled workload characteristics in the past.).

% In practice, the workload often varies over long periods due to variations in factors such as the transaction template ratio and key distribution. We observe that the overhead of the concurrency control in both middle tier and RDBMS is highly related to the transaction dependencies. Therefore, we construct a graph to capture workload characteristics, where vertices represent transaction features and edges between two transactions denote their dependency. Furthermore, we employ a learned model equipped with graph neural networks~\cite{DBLP:journals/corr/BrunaZSL13} and message-passing~\cite{DBLP:conf/icml/GilmerSRVD17} techniques to predict the optimal isolation level for the current workload. 
% We propose an isolation-level-aware prediction method that utilizes the graph embedding technique to capture the characteristics of dynamic workloads. To do this, \sysname asynchronously collects transactions using the Monte Carlo sampling~\cite{DBLP:journals/entropy/ZhouJLWLG24}, and builds a graph to characterize the workload, where vertices represent transaction features and edges between two transactions denote the operation dependencies. This graph is then fed into a prediction model equipped with graph neural networks~\cite{DBLP:journals/corr/BrunaZSL13} and message-passing techniques~\cite{DBLP:conf/icml/GilmerSRVD17}. 
% The prediction model translates the graph into one of three possible labels: RC, SI, or SER. The label with the highest value, as determined by our model, indicates the optimal isolation level for the current workload. 
% The selection process is outlined as follows. 
% \whiteding{1} We sample runtime transactions along with the accessed data items, using this collection of samples to characterize the workload. \whiteding{2} The workload is then transformed into an undirected graph, where each vertex corresponds to a transaction, and each edge denotes the correlations between pairwise transactions. \whiteding{3} From this workload graph, we extract performance-related features via graph neural network (GNN) and message-passing (MP) techniques. \whiteding{4} We apply a perception model to map these high-dimensional features into three possible labels (RC, SI, SER), with the label corresponding to the largest value indicating the optimal isolation level for the current workload, as learned by our model. It is important to note that these procedures are not tailored to specific workloads, making our approach broadly generalizable. 


% (3) Cross-Isolation Level Validation Mechanism for Efficient Transitions and Serializable Scheduling (Section~\ref{design-3})

% The optimal isolation level for an application adapts based on workload characteristics, necessitating transitions that can introduce complex anomalies due to transactions operating under different isolation levels. A straightforward approach would be to block until all transactions under the previous isolation level have completed, but this results in system downtime, especially problematic with high transaction execution latencies.

% To enhance efficiency, we design a cross-isolation validation mechanism that introduces an additional validation step for uncommitted transactions during the transition phase. While our concurrency control ensures Serializable Scheduling (SS) under a single isolation level, the additional validation identifies dependencies between transactions under the previous and current optimal isolation levels, ensuring the overall dependency graph remains acyclic.

% 单隔离级别 -> 跨隔离级别的场景
% The correctness of this cross-isolation validation mechanism is proven in Section~\ref{sec:proof.switch}.
\textbf{(3) Cross-isolation validation mechanism that enables efficient transitions and serializable scheduling (\S\ref{design-3}).} 
%\qy{
% Although RDBMSs support different transactions to be executed under different isolation levels, they do not ensure that the scheduling of these transactions achieves SER.
% Although there are numerous studies that can achieve SS when the RDMBS is configured to a low isolation level, including the middle-tier concurrency control we mentioned before, they can not ensure the SS during the transition, where transactions under different isolation levels coexist in the system.
The optimal isolation level should adapt as the workload evolves. When the RDBMS decides to change the isolation level, new transactions must be executed under this updated isolation level. Although existing approaches can achieve SER when all transactions use a unified low isolation level, they fail to ensure SER when transactions operate under different isolation levels. This is because varying isolation levels can introduce new dangerous structures that may violate the requirements of SER.
% Although existing works, including the middle-tier concurrency control in this paper, achieve SS when transactions are executed under low isolation levels, they can not ensure the SS during the transition, where transactions under different isolation levels coexist in the system. 
% We first prove that it is safe to transition between SI and RC isolation levels if the transaction undergoes the corresponding middle-tier concurrency control for its isolation level. In other scenarios, transactions under different isolation levels could induce non-serializability that the middle-tier concurrency control fail to handle. 
% We first demonstrate that the transition between SI and RC isolation levels is safe if each transaction undergoes the appropriate middle-tier concurrency control for its isolation level. In other transition scenarios, transactions under different isolation levels may induce dependencies that the middle-tier concurrency control cannot handle, violating the SS. 
% This is because any two transactions that are executed under different isolation levels could include dangerous structures that violate the requirements of SER.
% However, during this transition, the interleaved execution of transactions under different isolation levels can potentially induce dependencies, denoted as cross-isolation level dependencies. 
% \textcolor{flatgreen}{For example, transaction $T_1$ is executed under RC. During $T_1$ processing, the RDBMS changes the isolation to SER. The new transaction $T_2$ and $T_3$ are executed with the new isolation SER. $T_1$ first modifies the data item $x$ and $T_2$ follows to read $x$, thus forming a cross-isolation RW dependency from $T_1$ to $T_2$. Likewise, a cross-isolation WR dependency from $T_3$ to $T_1$ is introduced. At the same time, there is an RW dependency from $T_2$ to $T_3$, making the three transactions a non-serializable scheduling. Given that the cross-isolation dependencies cannot be tracked by existing static approaches or concurrency control algorithms designed for a single isolation level, serializability cannot be guaranteed during isolation switching.}
% \qy{
% Non-serializable scheduling during this transition may include these cross-isolation level dependencies, which cannot be detected or managed by existing static approaches or concurrency control algorithms designed for a single isolation level. 
% }
% \lw{These dependencies may not be adequately addressed by static approaches or concurrency control algorithms designed for a single isolation level, thus violating serializability. NOT Clearly discussed.}
To address this issue, we identify dangerous structures across different isolation levels and propose a cross-isolation validation mechanism that can prevent the occurrence of these structures during transitions without causing significant system downtime.
%}
% This mechanism detects so-called cross-isolation dependencies and prevents the dependency from transactions under the previous isolation level to those under the new one. It efficiently ensures SS during the transition without obvious system downtime.
We prove the correctness of the cross-isolation validation mechanism in \S\ref{sec:proof.switch}. 

% While the concurrency control mentioned before ensures SS under a single isolation level, t
% For better illustration, we denote the transaction under the previous optimal (now suboptimal) isolation level as $T_{old}$ and those under the current optimal isolation level as $T_{new}$. Moreover, transactions under various isolation levels can induce more complex anomalies that evade the detection and prevention mentioned before. To address this, \sysname introduces an extra validation phase to the lifecycle of transactions during isolation switching, which prevents dependencies from $T_{new}$ to $T_{old}$ and ensures the acyclic nature of the overall dependency graph. 
% If the current isolation level is deemed suboptimal, \sysname switches to the optimal isolation. During this transition, transactions operating under different isolation levels can jeopardize serializability. To counter this, we have designed a non-blocking switching mechanism to achieve serializability. 

We have conducted extensive evaluations on SmallBank \cite{DBLP:conf/icde/AlomariCFR08}, TPC-C \cite{TPCC}, and YCSB+T \cite{DBLP:conf/cloud/CooperSTRS10} benchmarks. The results show that \sysname can adaptively select the optimal isolation level for dynamic workloads, achieving up to a 26.7$\times$ performance improvement over other state-of-the-art methods and up to a 4.8$\times$ performance boost compared to SER provided by PostgreSQL.

% In summary, we make the following contributions:
% \begin{itemize}[leftmargin=*]
%     \item We propose \sysname, a middleware system that dynamically detects and prevents the ``dangerous structure'' without modifying the workload or the RDBMS kernel.  
%     \item We propose a validation-based concurrency control approach, which reduces the false positives with a runtime detection of ``dangerous structures''.
%     % \item We propose a graph-based prediction model to predict the optimal isolation strategy by \textit{(i)} using a graph model to represent the workload characteristic; \textit{(ii)} learning a high-dimensional mapping from embedded features to isolation strategy using deep learning (details in \S~\ref{design-2}).
%     \item We propose a graph-based prediction model that enables RDBMSs to select the optimal isolation level as the workload varies adaptively. To the best of our knowledge, \textit{\sysname is the first work to support self-adaptive isolation level selection for dynamic workloads.}
%     \item We design a rigorous isolation level switching mechanism to ensure serializability during the transition. 
%     \item We have conducted extensive evaluations on SmallBank, TPCC, and YCSB~\cite{DBLP:conf/cloud/CooperSTRS10} workloads. The results demonstrate that \sysname can adaptively select the optimal isolation level for dynamic workloads, achieving up to a 26.7x performance improvement over other state-of-the-art methods and up to a 4.8x performance boost compared to the serialization provided by PostgreSQL.
% \end{itemize}

% \newpage

% The objective of conducting offline dependency analysis across transaction categories is to pinpoint the ``dangerous structure'' that contravenes the requirements set by SER but is permissible at a lower isolation. 



% In SI, there always exists a \textit{dangerous structure}, which consists of two consecutive read-write dependencies~\cite{DBLP:conf/pods/Fekete05}. In RC, a read-write (rw) dependency always exists~\cite{DBLP:conf/aiccsa/AlomariF15, DBLP:journals/pvldb/VandevoortK0N21}. 
% Consider the SmallBank workload, which includes 5 transaction templates: \textit{Amalgamate} (Amg), \textit{Balance} (Bal), \textit{DepositChecking} (DS), \textit{TransactSavings} (TS), and \textit{WriteCheck} (WC). Figure~\ref{fig:SmallBank} illustrates the static dependency graph and the anomalies under weak transaction isolations. When the SmallBank workload is executed in SI, transactions such as Bal, WC, and TS can form a cyclic dependency. This cycle contains a vulnerable structure, thereby violating serializability.

% introduce additional write-write conflicts, which can restrict concurrency. Additionally, the weaker the isolation level, the more data anomalies must be managed outside the database. For dynamic workloads, including changing proportions of transaction templates and varying data access patterns, trade-offs must be made between performance gains and anomaly management. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these diverse conditions. 
% It received quite a bit of attention in the literature. 

% In application workloads, transaction templates (business code) are usually not updated frequently, so static or offline analysis can be introduced to determine whether all possible transaction execution sequences satisfy serialisable scheduling.

% When the RDMBSs are set to a lower isolation level, 
% Despite years of development, transaction processing remains a critical research area due to its fundamental role in ensuring ACID properties and enhancing system performance.
% Research in this domain can be typically divided into two categories.
% The first aims to maximize transaction performance while maintaining serializability.
% However, these approaches are constrained by the bottlenecks caused by strict ordering requirements inherent in serializability.
% Consequently, another line of research opts for lower isolation levels, such as \textit{Read Committed} (RC) and \textit{Snapshot Isolation} (SI).
% These isolation levels sacrifice the strict ordering of serializability in favor of improved performance, though they may introduce data anomalies that compromise data consistency.


% It is widely recognized that under specific workloads, certain anomalies may not occur, allowing transactions to be correctly processed at weaker isolation levels. 
% A notable example is that TPC-C transactions~\cite{TPCC} can be safely scheduled under SI instead of serializability.
% However, not all workloads can be safely scheduled at weak isolation levels. For instance, consider the SmallBank benchmark. Figure~\ref{fig:SmallBank} illustrates the static dependency graph and the resulting anomalies under weak transaction isolations. A straightforward solution might be to set the database isolation level to serializable, but this approach often results in poor performance. To leverage the performance benefits of weaker isolation levels while avoiding data anomalies, some researchers have modified query statements to eliminate anomalies without altering transaction semantics. The theory has established that data anomalies, whether arising from SI or RC, typically share a common characteristic: there are two transactions $T_i$ and $T_j$, with $T_j$'s write operation depending on $T_i$'s read operation and $T_j$ commits before $T_i$ commits. In other words, eliminating these structures can guarantee serializability. There are generally two approaches to handle this: (1) \textbf{Conflict Matrialization}. This method introduces an external conflict table to materialize conflicts by adding extra write operations to both $T_i$ and $T_j$~\cite{DBLP:conf/aiccsa/AlomariF15}, which avoids concurrent execution of transactions that may have the read-write dependency; (2) \textbf{promotion}. This technique promotes the read operation in $T_i$ to a write operation~\cite{DBLP:conf/icde/AlomariCFR08}, upgrading dependencies to write-write dependencies to avoid data anomalies.
% However, these approaches have limitations. They introduce additional write-write conflicts, which can restrict concurrency. Additionally, the weaker the isolation level, the more data anomalies must be managed outside the database. For dynamic workloads, including changing proportions of transaction templates and varying data access patterns, trade-offs must be made between performance gains and anomaly management. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these diverse conditions.
% For example, as depicted in the lower right of Figure~\ref{fig: SmallBank}, achieving serializable scheduling in RC for the SmallBank workload requires promoting the read operations in Balance transactions, changing them from read-only transactions to read-write transactions. Originally, Balance transactions do not block each other, but post-promotion, they will. 
% Secondly, the weaker the isolation level, the more data anomalies must be handled outside the database. For dynamic workloads, including dynamic proportions of transaction templates and dynamic data access distribution, trade-offs must be made between the gains of both. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these situations. 

% However, these approaches generally require an analysis of the workload to determine the appropriate isolation level, [which can be inefficient and impractical for managing dynamic real-world workloads.]...
% Further, not all workloads can be safely scheduled at weak isolation levels.
% ...
% For this reason, numerous studies focus on characterizing data anomalies at weak isolation levels to determine if transaction workloads can still meet the requirements of serializability.
%, i.e., maintaining an acyclic conflict dependency graph. 

% \newpage
% Ideally, the best plug-in approach to achieve serializability should satisfy the following requirements. \blackding{1} The ``dangerous structure'' defined at a given isolation level should be prevented. \blackding{2} The cost of preventing ``dangerous structure'' should be as minimal as possible.
% \blackding{3} The isolation level of the RDBMS may be adaptively adjusted as the workload varies so that the overall performance can be maximized.
% Nevertheless, finding such an optimal approach poses two challenges.
% \textcolor{blue}{
% First, many workloads cannot run on weak isolation levels due to data anomalies, and it is not trivial to guarantee the serializability of workloads on weak isolation levels without modifying the workload and concurrency control in the database.
% Second, selecting the most appropriate isolation level based on the dynamic characteristics in dynamic workload scenarios is challenging. 
% }





% (1) we propose a plug-in middleware system that integrates the concurrency control within and outside the RDBMS kernel. It detects runtime ``dangerous structures" without modifying the workload and RDBMS kernel. This allows the second layer, the RDBMS itself, to safely operate under a low isolation level.
% To the best of our knowledge, \textit{\sysname is the first work to propose a two-layer concurrency control framework.}
% (1) we propose a plug-in middleware system that detects runtime ``dangerous structures" without modifying the workload and RDBMS kernel. This allows the second layer, the RDBMS itself, to safely operate under a low isolation level.


% dynamically modifying the isolation to suit changing workloads without compromising the system's correctness and performance.

% while improving transaction processing performance remains a critical topic in database research.
% To achieve this objective, various concurrency control (CC) algorithms have been proposed~\cite{} to schedule transactions in a specific order, such as locking order, timestamp order, etc.
% Unfortunately, due to the strict ordering requirements of serializable scheduling, even the state-of-the-art serializable CC algorithms cannot match the performance of CC algorithms supporting lower isolation levels, such as \textit{Read Committed} (RC) and \textit{Repeatable Read} (RR).

% Serializability is widely accepted as the gold standard for transaction processing in database systems.
% To achieve serializability, modern databases often adopt 2PL and its variants. For example,  PostgreSQL employs serialize snapshot isolation (SSI)~\cite{DBLP:conf/sigmod/CahillRF08, DBLP:journals/pvldb/PortsG12}, which detects and prevents consecutive rw dependencies -- known as \textit{dangerous structure}. 
% Similarly, MySQL uses strict two-phase locking (S2PL)~\cite{DBLP:conf/vldb/BayerEHR80} remains a fundamental technique, enforcing serializability by requiring transactions to acquire all necessary locks before releasing any and holding these locks until the transaction commits. 

% In addition to serializable isolation, modern database systems often provide multiple isolation levels, including \textit{Read Committed} (RC), \textit{Read Repeatable} (RR) or \textit{Snapshot Isolation} (SI). In PostgreSQL, RR is implemented as SI and in this paper, we only consider the two weak isolation levels: RC and SI. 
% For this reason, numerous studies focus on characterizing data anomalies at weak isolation levels to determine if transaction workloads can still meet the requirements of serializability, i.e., maintaining an acyclic conflict dependency graph. 
% A notable example is the TPC-C benchmark~\cite{TPCC}, which can be safely scheduled under SI.

% However, not all workloads can be safely scheduled at weak isolation levels. The SmallBank benchmark is a typical example~\cite{DBLP:conf/icde/AlomariCFR08}. Theoretical work indicates that non-serializable schedules generate a dependency graph with a cycle specific to the isolation level: \textit{dangerous structure} for SI~\cite{DBLP:conf/sigmod/CahillRF08} and a "counterflow edge" for RC~\cite{DBLP:journals/tods/KetsmanKNV22, DBLP:conf/aiccsa/AlomariF15}. This concept extends to workloads through a static dependency graph, where each program is represented by a node. A conflict edge exists between nodes if a schedule can induce that conflict. The absence of a cycle specific to the isolation level represents the workload can be scheduled at this isolation level without data anomalies. For better illustration, we plot the static dependency graph and list the vulnerable dependency edges for SmallBank in Figure~\ref{fig: SmallBank}, indicating that SmallBank cannot be safely executed at isolation levels weaker than \textit{serializable}. For example, when we run at SI, the transaction instances inherited by Bal, WC and TS can construct a cycle with two consecutive rw dependencies. 

% Fortunately, the transaction types and queries in business workload are not changed frequently, so are numerous of work has been done to identify potential data anomalies in the workload at weak isolation levels with the help of static dependent graphs (SDG), where the vertices of the graph are the transaction templates, and the edges represent the static dependencies. 
% Based on these theories, Fekete~\cite{} et al. find that TPCC~\cite{} does not suffer from data anomalies in snapshot isolation(SI), and therefore TPCC does not need to run at a stronger isolation level. However, not all workloads are robust at weak isolation levels, e.g. Alomari~\cite{} et al. found the dangerous structure~\cite{} in SmallBank, which means SmallBank can not safely execute at isolations weaker than \textit{serializable}. 

% However， ***方法 does not work properly in real applications \cite{}. 
% 举反例。

% For decades, with the popularity of the Internet, application techniques have been the hotspot of academia and industry~\cite{DBLP:journals/sigmod/TangWZYZG023}. Applications typically utilize databases to store and manage large amounts of data and acquire the database to ensure the serializable execution of concurrent transactions to ensure the correctness of business code~\cite{}.

% use the ACID characteristics of transactions to ensure the correctness of business code~\cite{}. 

% Serializability is a key property for OLTP scenarios.

% In modern databases, achieving serializability is a key property for ensuring the correctness of concurrent transactions, is accomplished through various advanced techniques. Serializable snapshot isolation (SSI)~\cite{DBLP:conf/sigmod/CahillRF08, DBLP:journals/pvldb/PortsG12} is a widely adopted method that ensures serializability by detecting and resolving conflicts between concurrent transactions operating under snapshot isolation. Another approach achieves serializable with the help of serial safety net (SSN)~\cite{DBLP:journals/vldb/WangJFP18}, which tracks the resulting dependencies and determines whether it can commit safely or must abort to avoid
% a potential dependency cycle. Additionally, Strict Two-Phase Locking (S2PL)~\cite{DBLP:conf/vldb/BayerEHR80} remains a fundamental technique, enforcing serializability by requiring transactions to acquire all necessary locks before releasing any and holding these locks until the transaction commits. 

% Besides the serializability, modern databases support a variety of isolation levels for clients to choose from. 
% Among them, isolation can avoid data anomalies when multiple clients concurrently operate the database, including dirty reads, dirty writes, and so on. To cope with different business requirements, the database has a variety of isolation levels for program developers to choose from. Generally speaking, a weaker isolation level means higher concurrency with more potential data anomalies. 
% A workload is said to be {\it robustness} at an isolation level if all possible execution sequences are serializable at that isolation level~\cite{}. 
% Choosing a reasonable isolation level scheme becomes challenging for programmers when developing applications.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.46\textwidth]{figures/intro.pdf}
%     \vspace{-3mm}
%     \caption{Approaches for preventing data anomalies in weak isolation levels}
%     \label{fig: intro}
%     \vspace{-4mm}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \vspace{3mm}
%     \includegraphics[width=0.46\textwidth]{figures/SmallBank.pdf}
%     \vspace{-3mm}
%     \caption{Static dependency graph of SmallBank. 
%     % \textnormal{double-arrow represents ww dependency, dash arro}
%     }
%     \label{fig: SmallBank}
%     \vspace{-4mm}
% \end{figure}


% Previous work modifies transaction templates in the workloads without affecting the semantics to address the problems mentioned and enable workloads that do not satisfy serializability at weak isolation levels to still benefit from the performance advantages of weak isolation levels. In previous theories, whether dealing with \textit{dangerous structures} or \textit{counterflow edges}, a common structure is observed: two transactions $T_i$ and $T_j$, with $T_j$'s write operation depending on $T_i$'s read operation and $T_j$ commits before $T_i$ commits. To eliminate this structure, there are typically two approaches to modify the workload: (1) \textbf{Conflict Matrialization}. It adds an external conflict table to materialize conflicts, introducing extra write operations into both $T_i$ and $T_j$~\cite{DBLP:conf/aiccsa/AlomariF15}, which concurrent execution of transactions that may have the rw dependency; (2) \textbf{promotion}. It promotes the read operation in $T_i$ to a write operation~\cite{DBLP:conf/icde/AlomariCFR08}, which upgrades rw dependencies to ww dependencies to avoid data anomalies. 
% It is worth noting that only a small portion of operations trigger data anomalies in some workloads. If these data anomalies can be avoided at a low cost, the workload can still be run at a weak isolation level and reap higher performance benefits. 
% However, these approaches still have limitations. First, they introduce extra write-write conflicts into transaction templates, which can limit concurrency. For example, as depicted in the lower right of Figure~\ref{fig: SmallBank}, achieving serializability in RC for the SmallBank workload requires promoting the read operations in Balance transactions, changing them from read-only transactions to read-write transactions. Originally, Balance transactions do not block each other, but post-promotion, they will. Secondly, in general, the weaker the isolation level, the more data anomalies need to be handled outside the database. For various workloads or different proportions of transaction templates within the same workload, trade-offs must be made between the gains of both. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these situations. 
% No one set of solutions is optimal in all scenarios, and none of the existing work provides solutions in this scenarios. 

% Previous theory show that when a data anomaly occurs in either RC or SI, there are always two transactions $T_i$ and $T_j$, with $T_j$'s write operation depending on $T_i$'s read operation. Meanwhile, there is also a necessary condition that \textit{$T_j$ commits before $T_i$ commits}.

% In order to handle the above limitations, we need to solve two challenges: 
% {\color{blue}
% (1) we want the choice of isolation level scheme to be transparent to the program developers so that they can pay more attention to the business logic and improve the development efficiency; 
% }
% (1) the existing work by transforming the load to make it run in a weak isolation level is not efficient; 
% (2) in dynamic scenarios, which is reflected in the conflict rate, transaction ratio, and other changes, as mentioned earlier, there is no one set of programs that can be in all scenarios to achieve optimal performance, we need to trade-off between the isolation level of the database and the data outside the database to avoid anomalies.




%, the \textit{Bal} transaction is originally read-only but becomes a read-write transaction after the workload modification. While read-only transactions do not block each other, read-write transactions do. 
% We propose a plug-in middleware system that detects runtime ``dangerous structures" without modifying the workload and RDBMS kernel. This allows the second layer, the RDBMS itself, to safely operate under a low isolation level.
 
% (1) Elimination of ``dangerous structures" is a sufficient but not necessary condition to achieve serializable scheduling.
% Thus, preventing the occurrence of dangerous structures could cause many false positives.
% To alleviate this problem, we propose a validation-based concurrency control approach.
% In this approach, we allow the occurrence of dangerous structures but introduce an extra requirement: no two transactions exist, and the dependency order and the commit order between them are inconsistent.
% That is, upon the commit of a transaction $T_i$, if there is a transaction $T_j$ that commits before $T_i$. Still, a dependency exists from $T_i$ to $T_j$, indicating that the dependency and commit order of $T_i$ and $T_j$ are inconsistent, then $T_i$ will abort. Otherwise, $T_i$ will commit.
% \sysname employs a validation-based concurrency control algorithm that dynamically detects and prevents ``dangerous structures" by verifying the consensus between dependency and the commit order of transactions. 
% The main idea of the validation is as follows: we first collect the dependencies of transactions during their read and write operations. Upon the commit of a transaction $T_i$, if there is a transaction $T_j$ that commits before $T_i$ but $T_j$ depends on $T_i$, indicating that the dependency and commit order of $T_i$ and $T_j$ are inconsistent, then $T_i$ will abort. Otherwise, $T_i$ will commit.
% We provide theoretical proof that both ``dangerous structures" and the above consistent order are the necessary conditions of serializable scheduling.
% This approach hinges on the recognition that a key necessary condition for anomalies is the presence of at least one RW dependency where the dependency and commit order are inconsistent~\cite{DBLP:conf/pods/Fekete05, DBLP:conf/aiccsa/AlomariF15}. 
% As illustrated in Figure~\ref{fig:SmallBank}(d), besides the two consecutive RW dependencies, $T_{ts}$'s write operation depends on $T_{wc}$'s read operation, while $T_{ts}$ commits before $T_{wc}$ does. \sysname would detect the runtime RW dependency between $T_{ts}$ and $T_{wc}$ and ensure $T_{ts}$ commits after $T_{wc}$ or otherwise abort $T_{wc}$. 
% For example, in Figure~\ref{fig:SmallBank}(d), because there is a dependency from $T_{ts}$ to $T_{wc}$, and $T_{ts}$ commits before $T_{wc}$. \sysname would detect the dependency in the validation phase of $T_{wc}$ and abort it due to the inconsistency of the dependency order and commit order. 


% anomalies at low isolation levels while minimally impacting concurrency. 
% Instead of statically introducing WW dependency, 
% we identify that another non-trivial necessary condition is the presence of at least an RW dependency where the dependency and commit order are inconsistent~\cite{DBLP:conf/pods/Fekete05, DBLP:conf/aiccsa/AlomariF15}. As illustrated in Figure~\ref{fig:SmallBank}(d), besides the two consecutive RW dependencies, $T_{ts}$'s write operation depends on $T_{wc}$'s read operation, while $T_{ts}$ commits before $T_{wc}$ does. Therefore, to prevent the ``dangerous structure", \sysname aims to preserve the consistency of the dependency and commit order. In the given example, we detect the runtime RW dependencies between $T_{ts}$ and $T_{wc}$, and ensure $T_{ts}$ commits after $T_{wc}$ or otherwise abort $T_{wc}$. 

% (2) \sysname utilizes a graph model to continuously predict the optimal isolation level for dynamic workloads. 
% Initially, the workload is modelled as a graph, where each vertex corresponds to a transaction, and each edge denotes the RW, WR, WW, and RR dependencies between pairwise transactions. We then use a graph-based prediction model, which includes an embedding model and a prediction model, to forecast the optimal isolation level.

% (3) If the current isolation level is deemed suboptimal, \sysname switches to the optimal isolation. During this transition, transactions operating under different isolation levels can jeopardize serializability. To counter this, we have designed a non-blocking switching mechanism to achieve serializability. 
% Additionally, we design a rigorous isolation level switching mechanism to ensure serializability during the transition phase. 
% We then simulate various random workloads offline using different isolation level strategies and identify the optimal isolation level strategy for various workloads. 
% These characterized workloads and labels are utilized offline to train the model, thereby allowing us to adapt the isolation strategy for dynamic workloads in real time. 
% efficiently and adaptively tailors transaction isolation levels for applications. To tackle the stated limitations, we introduce two main techniques: 
% (1) a \textbf{two-layer concurrency control framework} that integrates the concurrency control within and outside the kernel, and a \textbf{validation-based concurrency control algorithm} devised to achieve serializable isolation across the entire system efficiently. Therefore, there is no need to convert the rw dependency into the ww dependency. The validation-based concurrency control algorithm can guarantee a consistent commit order. In the example above, we detect the rw dependency and guarantee $T_j$ commits after $T_i$ or otherwise aborts $T_i$. In high contention scenarios, frequent write operations can modify the read set of a transaction before it is committed. To mitigate this risk, we have designed external admission mechanisms that limit concurrent transactions, reducing the likelihood of read-set modifications.

% \textbf{(2) \sysname avoids data anomalies by guaranteeing the commit order of transactions after a read-write dependency occurs.} We observe that when a data anomaly occurs in either RC or SI, there are always two transactions $T_i$ and $T_j$, where $T_j$'s write operation is dependent by $T_i$'s read operation and $T_j$ commits before $T_i$. Instead of violently transforming rw-dependency into ww dependency, we design a validation-based concurrency control algorithm based on the two-layer concurrency control architecture to guarantee the commit order. In the example above, we detect the read-write dependency and guarantee $T_j$ commits after $T_i$ or otherwise aborts $T_i$.
% \textbf{(2) \sysname proposes a graph-based prediction model to predict the optimal isolation strategy for various workloads.} We use a graph-structured model to capture the workload characteristics, in which the vertices represent transaction features and edges between two transactions denote the data dependency between them. Then, we simulate various random workloads offline on different isolation level strategies and get the optimal isolation level scheme for them. These characterized workloads and labels are used offline to train the model. Therefore, we can adaptively run the workload in the optimal isolation strategy online. In addition, when switching schemes, \sysname will follow the scheme with a weaker database isolation level to preserve the order until all the transactions running in the previous scheme have been executed, ensuring no data anomalies in the transition phase. 
% (3) \sysname also introduces a \textbf{graph model} to predict the optimal isolation strategy for various workloads. 

% \textbf{(3) \sysname propose a graph-structured workload model to capture features and a prediction model based on graph convolutional neural network (GCN).} 
% The models learn the performance of different methods on dynamic loads offline, and choose an optimal scheme online based on the statistical modules in \sysname that characterize the workload. In addition, when switching schemes, \sysname will follow the scheme with a weaker database isolation level to preserve the order until all the transactions running in the previous scheme have been executed, ensuring no data anomalies in the transition phase. 

% Relational database systems provide transactions with several isolation levels, where a weaker isolation level means higher concurrency with more potential data anomalies. 
% Web applications are increasingly built out of microservices. Building application out of microservices allow teams in large organizations to independently develop code with fewer concerns about programming language choice and code dependencies. This reduces friction in development.
