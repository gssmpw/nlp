\section{Overview of \sysname}
\label{sec:overview}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/overview.pdf}
    \vspace{-2mm}
    \caption{An overview of \sysname}
    \Description{A concise description of the figure for screen readers.}
    \label{fig:overview}
    \vspace{-4mm}
\end{figure}

\sysname works in the middle tier between the application tier and the database tier, designed to \blackding{1} ensure SER when transactions operate under a low isolation level without introducing additional writes; \blackding{2} select the optimal isolation level for dynamic workloads adaptively; \blackding{3} constantly keep SER during the isolation level transition. 
An overview of \sysname is depicted in Figure~\ref{fig:overview}. It comprises three main components: \textit{Analyzer}, \textit{Executor}, and \textit{Adapter}. 


% {
% \color{blue}
% \textbf{Transaction Templates.}  \sysname provides four interfaces for clients to register transaction templates and execute transactions.
% \begin{itemize}
%     \item \textit{Register Begin(name)}. Before executing applications, clients first register transaction templates into \sysname. Each transaction template registration is initialized by the \textit{Register Begin()} interface with a \textit{name} parameter to identify the transaction template name.
%     \item \textit{Register Txn(name, op, sql) -> (idx, status)}. Each transaction template is composed of multiple SQL statements. Clients register the SQL statement within each template using the \textit{Register Txn()} interface. This interface accepts \textit{name}, \textit{op}, and \textit{sql} as parameters, representing the template name, SQL statement type (read or write operation), and content of the SQL statement. After registration, \sysname returns a unique identifier \textit{id} for this SQL statement and a \textit{status} parameter denoting the registration result (success or failure).
%     \item \textit{Register End(name)}. After registering all SQL statements, clients complete the template registration using the \textit{Register End()} interface.
%     \item \textit{Execute Txn(name, idx, args) -> status}. Clients execute transactions by invoking the \textit{Execute Txn()} interface. It receives parameters including the template name \textit{name}, the SQL statement identifier \textit{idx}, and the arguments for SQL statements \textit{args}.
% \end{itemize}
% }

% \sysname aims to achieve optimal serializable scheduling with self-adaptive isolation level selections.
% To do this, it is equipped with (1) a validation-based concurrency control approach, which prevents data anomalies for the applications and achieves serializability even if the RDMBS is set to a low isolation level,
% (2) a graph model that characterizes the workload and predicts the optimal isolation level,
% and (3) an isolation level switching mechanism that allows long-running transactions to work under an old isolation and new transactions to work under a new isolation.
% There are four key components in \sysname: \textbf{Analyzer}, \textbf{Executor}, \textbf{Collector} and \textbf{Adapter}. 

\noindent\textbf{Analyzer.} 
{
% \color{blue}
% The application first registers transaction templates into \sysname through \textit{template interfaces} provided by the \textit{Analyzer}.
% }
\textit{Analyzer} provides \textit{template interfaces} for template registration and analysis. 
Before \sysname executes any transaction from the application, \textit{Analyzer} builds the static dependency graph for the transaction templates and identifies all the static vulnerable dependencies for each low isolation level according to Definition \ref{def:static_vul}. It then sends the static vulnerable dependencies to \textit{Executor}.

% takes the template set $\Sigma$ as the input and builds the static dependency graph for $\Sigma$ based on Definition~\ref{def:sdg}.
% It is particularly designed to identify dangerous structures that are defined Definition \ref{def:rc} for SI, and Definition \ref{def:si} for RC, respectively.

% is able to construct the $SDG$ for the workload according to Definition~\ref{def:sdg} and identify dangerous structures under low isolation levels: two consecutive RW dependencies for the SI level and a single RW dependency for the RC level. If these structures are found within a dependency cycle, \sysname must handle them carefully to ensure serializability. 

% To achieve SER under low isolation levels:
% 1. **ILM** stores static vulnerable dependencies. Before a transaction $T$ starts, ILM checks if $T$ involves these dependencies to determine if middle-tier concurrency control is needed.
% 2. If $T$ does not involve static vulnerable dependencies, the \textit{Executor} sends it directly to the RDBMS for execution. Otherwise, **DD** tracks the read/write sets of $T$ for potential RW dependencies.
% 3. **DD** detects runtime vulnerable dependencies based on these read/write sets.
% 4. **TS** then schedules commit operations to maintain consistency with the dependency order. For instance, if DD detects an RW dependency from $T_i$ to $T_j$, TS ensures $T_i$ commits before $T_j$. If $T_j$ has already committed, $T_i$ is aborted to prevent non-serializable scheduling.

\noindent\textbf{Executor.} 
\textit{Executor} provides \textit{execution interfaces} for applications to execute transactions. It ensures SER when transactions operate either at a single low isolation level or during the isolation level transition. 
There are four core modules: \textit{Isolation Level Manager (ILM)}, \textit{Dependency Detector (DD)}, \textit{Transaction Scheduler (TS)}, and \textit{Transition Governor (TG)}. 
% To achieve the SER when transactions operate under low isolation levels \blackding{1}. 
(1) ILM stores the static vulnerable dependencies, and before any transaction $T$ starts, it identifies whether $T$ involves any static vulnerable dependencies. If the template of $T$ does not involve static vulnerable dependencies, \textit{Executor} sends $T$ directly to the RDBMS for execution; otherwise, ILM triggers DD that identifies vulnerable dependencies of $T$.
% whether middle-tier concurrency control is required. If the template of $T$ does not involve static vulnerable dependencies, \textit{Executor} sends $T$ directly to the RDBMS for execution;
% The \textit{Dependency Detection} and \textit{Transaction Scheduling} implement the middle-tier concurrency control designed for those transactions. During the transaction execution, the \textit{Dependency Detection} stores the transaction's read/write set that potentially induces the RW dependencies. 
(2) DD monitors the reads and writes of $T$, detecting any runtime vulnerable dependencies between $T$ and other transactions. If $T$ is involved in any vulnerable dependencies, TS is triggered. 
% data items of transactions that are derived from static vulnerable dependencies. Then, it detects runtime vulnerable dependencies between transactions according to these data items before the transaction commitment. 
% for transactions involving static vulnerable dependencies.
(3) TS attempts to ensure that the commit and vulnerable dependency orders remain consistent between $T$ and other transactions. If the consistency cannot be guaranteed, $T$ is blocked or aborted; otherwise, $T$ proceeds to commit.
% TS tries to schedule the commit operation to maintain the commit order consistent with the dependency order. For example, given two transactions, $T_i$ and $T_j$, if DD detects an RW dependency from $T_i$ to $T_j$, and TS tries to schedule $T_i$ commit before $T_j$ commits. Otherwise, if $T_j$ has been committed, $T_i$ should be aborted to prevent non-serializable scheduling. 
% Specifically, except for the normal execution, they should store the read/write set that potentially induces the RW dependencies. Then, they would undergo a validation phase before they can be committed. In this phase, the dependency detection module detects runtime vulnerable dependencies between transactions according to their read/write set. Then, the transaction scheduling module tries to schedule the commit operation to maintain the commit order consistent with the dependency order. For example, given two transactions, $T_i$ and $T_j$, if the dependency detection module detects an RW dependency from $T_i$ to $T_j$, and the transaction scheduling module tries to schedule $T_i$ commit before $T_j$ commits. Otherwise, if $T_j$ has been committed, $T_i$ should be aborted to prevent non-serializable scheduling. 
% For other transactions, the \textit{Executor} sends them to RDMBS directly. 
% the corresponding RW dependencies required to detect and schedule their commit order. The runtime transactions derived from the transaction templates involving these RW dependencies would apply the middle-tier concurrency control algorithm.
% For transactions not inherited from the transaction templates identified by ILM in the current isolation level, 
(4) TG ensures SER during the transition between two isolation levels. It follows a new corollary, which extends Theorem \ref{the:vulnerable} 
to any two transactions, $T_i$ and $T_j$, executing under different isolation levels. 
% If $T_i \xrightarrow{rw} T_j$ and $T_i$ commit before $T_j$, the transaction scheduling during the transition achieves SER. 
% , which introduces an additional validation phase for certain transactions under SER. This mechanism ensures a consistent dependency and commit order, thereby maintaining an acyclic dependency graph.
% We first propose the serializable theorem during the transition and prove that the transition between RC and SI is safe if they handle the vulnerable dependency according to the above procedures. ILT employs a cross-isolation validation mechanism, which introduces an additional validation phase for certain transactions under SER. This mechanism ensures a consistent dependency and commit order, thereby maintaining an acyclic dependency graph.
% In other transition scenarios, ILT employs a cross-isolation validation mechanism, which introduces an additional validation phase for certain transactions under SER. This mechanism ensures a consistent dependency and commit order, thereby maintaining an acyclic dependency graph.
% The proof to ensure SS during the isolation level transition is detailed in Section \ref{sec:proof.switch}.
The proof of correctness during the isolation level transition is detailed in \S\ref{sec:proof.switch}.

% In other transition scenarios, ILT implements a cross-isolation validation mechanism, which introduces an additional validation phase for a portion of transactions under SER and ensures the consistent dependency order and commit order, thereby maintaining an acyclic dependency graph. We demonstrate the SER during the isolation level transition in Section \ref{sec:proof.switch}.

% The workload often varies over long periods, and the optimal isolation level is dynamic according to the workload's characteristics. If the optimal isolation level changes, \textit{Executor} will apply the new isolation level to incoming transactions. In this transition, transactions under two different isolation levels coexist within the system. We propose the serializable theorem during the transition and prove that the transition between RC and SI is safe if they employ the middle-tier concurrency control for the corresponding isolation level. However, in other transition scenarios involving the SER, cross-isolation level dependencies can induce non-serializable scheduling. 
% To achieve the SER when transactions operate during the transition \blackding{2}, the ILT implements a cross-isolation validation mechanism, which introduces an additional validation phase for specific SER transactions and ensures the consistent dependency order and commit order,
% escape the management of middle-tier concurrency control, hence inducing non-serializability. To handle these dependencies, the ILT implements a cross-isolation validation mechanism, 
% which prevents the dependencies from transactions under the new isolation level to transactions under the old isolation levels, 


% These components efficiently ensure serializable scheduling at each low isolation level without modifying either application logic or database kernel.


% \textbf{Executor} receives transactions from the applications and schedules their execution under SER isolation.
% It consists of two main sub-components: dependency detector and isolation switcher. 
% The dependency detector is equipped with a validation-based concurrency control approach (details in Section~\ref{sec:design:cc:validation}).
% In each approach, for each read or write of a transaction $T$, the dependency detector records the dependency from $T$ to other transactions.
% By doing this, dangerous structures over dynamic conflict graphs can be detected. 
% Upon the commit of a transaction $T_i$, if there is a transaction $T_j$ that commits before $T_i$ but there exists a dependency from $T_i$ to $T_j$, indicating that the dependency and commit order of $T_i$ and $T_j$ are inconsistent, then $T_i$ will abort. Otherwise, $T_i$ will commit.

% The isolation switcher detects the dependencies in the switch phase and prevents the dependency cycle with transactions in different isolation levels. We demonstrate the serializability later in Section \ref{sec:proof.isolation}.

% According to \cite{DBLP:conf/sigmod/CahillRF08, DBLP:conf/icdt/VandevoortK0N22}, we identify a critical point: for a dependency cycle in $DCG$ under RC or SI levels, there is always an RC dependency in the dangerous structure where the dependency order is inconsistent with the commit order (e.g., $T_i\rightarrow T_j$, but $T_j$ commits before $T_i$). 
% The dependency detector detects the runtime dependencies and maintains the consistency of dependency and commit order. The isolation switcher detects the dependencies in the switch phase and prevents the dependency cycle with transactions in different isolation levels. We demonstrate the serializability later in \S~\ref{sec:proof.isolation}.

\noindent\textbf{Adapter.} \textit{Adapter} models the trade-off between performance benefits of low isolation levels and additional serializability overhead outside the database kernel. It predicts the optimal isolation level when the workloads evolve. Initially, a dedicated thread is introduced to continuously sample aborted/committed transactions using Monte Carlo sampling \cite{DBLP:journals/entropy/ZhouJLWLG24}, capturing the read/write data items. 
After collecting a batch of transaction samples, \textit{Adapter} predicts the optimal isolation level for future workloads based on the characteristics of the batch. The prediction process consists of two steps: \textit{Workload Modeling (WM)} and \textit{Isolation Level Prediction (ILP)}. 
% There are three main modules in \textit{Adapter}: \textit{Graph Construction}, \textit{Graph Embedding}, and \textit{Prediction}. 
WM extracts performance-related features and models the workload as a graph.
In this graph, each vertex represents a runtime transaction, with its features capturing the transaction context, such as the number of data items in the read and write sets.
Each edge represents an RW or WW operation dependency between transactions.
% including the RW and WW dependencies. 
% For example, if two transactions read the same data item, there is an RR edge between them.
% \textbf{Adapter} predicts the optimal isolation level for dynamic workloads adaptively, which varies over long periods of time due to the changes in aspects as diverse as the template proportion and key distribution. First, \textbf{Workload2Graph} extracts features from the workloads that may affect the performance and characterizes the workload as a graph, where the vertices are types of transactions in the workloads, and the edges are the extended dependencies between two transactions. 
Following WM, ILP embeds the workload graph into a high-dimension vector using graph neural network \cite{DBLP:journals/corr/BrunaZSL13} and message passing techniques \cite{DBLP:conf/icml/GilmerSRVD17}, and then translates the vector into three possible labels: RC, SI, or SER. 
The label with the highest value, as determined by our model, indicates the most efficient isolation level.
% \sysname embeds the workload graph into a fixed-length vector by graph neural network \cite{DBLP:journals/corr/BrunaZSL13} and message passing techniques \cite{DBLP:conf/icml/GilmerSRVD17}. Lastly, the \textit{Predicition} module translates the high-dimension vector into three possible labels: RC, SI, or SER. The label with the highest value, as determined by our model, indicates the optimal isolation level for the current workload. The \textit{Adapter} sends the isolation level to ILT in the \textit{Executor}. 
% Then, the \textbf{Isolation Predictor} adopts a graph-based learning model to embed the graph into vectors and predicts the optimal isolation level using a deep learning model.

For reference, the detailed implementation of the interfaces and the three core components are provided in \S\ref{implementation}.

% In \sysname, the inputs are static transaction templates and dynamic transaction instances. 
% After receiving the transaction templates of the workload, we utilize the analyzer to identify dangerous structures under low isolation levels. Then, transaction instances will be handled by the executor. In addition to sending the queries to the RDMBS, we keep the read/write sets for transactions involved in potentially dangerous structures and add a validation phase in the transaction lifecycle. In this phase, the dependency detector detects the runtime dependency and maintains the consistency of dependency and commit order. This allows us to achieve serializability even if the RDMBS is set to a low isolation level.

% In the meantime, the collector samples runtime transaction instances and sends the transaction information to the adapter, which captures the feature of the current workload and represents it as a graph. Then, it forecasts the optimal workload for the workload with the help of a learning model. 
% predicts the optimal isolation level for the current workload. 
% , denoted as $T_{old}$ and $T_{new}$ i.e., the dependency between $T_{old}$ and $T_{new}$,

% The optimal isolation level for dynamic workloads varies according to their characteristics. If the optimal isolation level changes, the \textit{Executor} will apply the new isolation level to incoming transactions. In this transition, transactions under two different isolation levels coexist within the system. We found that the transition between RC and SI is safe if they employ the middle-tier concurrency control for the corresponding isolation level. However, in other scenarios, cross-isolation level dependencies can escape the management of middle-tier concurrency control, hence inducing non-serializability. 
% % and maintain serializability during the switch phase. In this phase, transactions under two different isolation levels coexist within the system, denoted as $T_{old}$ and $T_{new}$, respectively, which makes the serializability more difficult. 
% % The straightforward approach blocks $T_{new}$ until all $T_{old}$. However, this method can lead to prolonged system unavailability, particularly with long-running transactions. 
% To handle these dependencies, the ILT implements a stringent cross-isolation dependency validation mechanism, which prevents the dependencies from transactions under the new isolation level to transactions under the old isolation levels, thereby maintaining an acyclic dependency graph. 
% We demonstrate the serializability during the isolation level transition in Section \ref{sec:proof.switch}.

% We achieve the serializability of the workload through the following steps:
% \begin{itemize}[leftmargin=*]
% \item \textit{Static analysis}: 
% After receiving the transaction templates of the workload, we utilize \textbf{Analyzer} to identify potential data anomalies under low isolation levels. The Analyzer first constructs the SDG for the workload. In SDG, nodes represent transaction templates $\mathcal{T}$, and edges represent dependencies between these templates, indicating possible dependencies between transaction instances $T$.
% % The directed edges in SDG represent the possible dependencies of the transaction instances.
% If there exists $\mathcal{T}_i \xrightarrow{ww} \mathcal{T}_j$, the intersection of the write set of $\mathcal{T}_i$ and $\mathcal{T}_j$ indicates that instances $T_i$ and $T_j$ may exhibit WW dependencies at runtime. Similarly, for an edge $\mathcal{T}_i \xrightarrow{wr} \mathcal{T}_j$, a non-empty intersection of the write set of $\mathcal{T}_i$ and the read set of $\mathcal{T}_j$ indicates a potential WR dependency. Conversely, if there exists an edge $\mathcal{T}_i \xrightarrow{wr} \mathcal{T}_j$, a non-empty intersection of the read set of $\mathcal{T}_i$ and the write set of $\mathcal{T}_j$ indicates a potential RW dependency. Notably, if $T_i$ and $T_j$ are guaranteed to have a WW dependency when an RW or WR dependency occurs, then these RW or WR dependencies need not be explicitly labeled. 
% % must have a WW dependency when an RW or WR dependency occurs, then this RW or WR dependency does not need to be labeled. 
% Next, the analyzer identifies anomaly structures: two consecutive RW dependencies for the SI level and a single RW dependency for the RC level. If these structures are found within a dependency cycle, \sysname must handle them carefully to ensure serializability.
% If there are two transaction templates $\mathcal{T}_i$ and $\mathcal{T}_j$, $\mathcal{T}_i$'s write set . Meanwhile, if two transaction templates 
% As Figure~\ref{fig:SmallBank}, the analyzer identify the anomaly 

% 动态避免
% \item \textit{Runtime detection}:
% \sysname detects runtime dependencies and prevents anomalies the RDBMS can not handle under low isolation levels. 
% By Theorem \ref{def:si} and Theorem \ref{def:rc}, we identify a critical point: under RC or SI levels, there exists a vulnerable dependency where the order of dependencies is inconsistent with the order of commits (e.g., $T_i\rightarrow T_j$, but $T_j$ commits before $T_i$). 
% The core principle of concurrency control in \sysname is to align the commit order with the dependency order, thereby obviating the need to alter the workload or introduce WW dependencies, which preserves operational efficiency.
% To achieve this, we have equipped \textbf{Executor} with a \textbf{Dependency Detector} and integrated an extra validation phase into the transaction lifecycle (details in \S~\ref{design-1}). If the detector identifies the read operation of $T_i$ first, it blocks the commitment of $T_j$ util $T_i$ commits. Otherwise, if $T_j$ commits first, $T_i$ will be aborted. 
% We demonstrate the serializability of this concurrency control algorithms later in \S~\ref{sec:proof.isolation}.
% The serializability of these concurrency control algorithms is further substantiated in \S~\ref{sec:proof.isolation}.
% \end{itemize}

% \begin{example}
%     For SmallBank, the Analyzer first constructs the SDG for the transaction templates and identifies the anomalies under low isolation levels, as illustrated in Figure~\ref{fig:SmallBank}(b) and (c). Assuming the RDBMS is set to SI, the dependency detector monitors the read set of $T_{wc}$ and the write set of $T_{ts}$. Before $T_{wc}$ commit, the detector validates whether there is a previously committed $T_{ts}$ that modifies the read set of $T_{wc}$. If such a $T_{ts}$ exists, $T_{wc}$ is aborted due to dependency and commit order inconsistency. Otherwise, $T_{wc}$ can commit successfully.
%     \qed
% \end{example}

% to adapt efficiently to the optimal isolation level. 
% The green arrow represents the offline workflow and the red arrow indicates the online workflow. 
% There are two key techniques in \sysname, (1) an efficient concurrency control strategy based on validation making workload executing in weaker isolation without data anomalies; (2) an adaptive approach based on a learning model choosing the optimal isolation level and corresponding concurrency control strategy. Firstly, \sysname adds a column \textit{vid} to the table schema that requires validation, which increments once updated.

% Besides the ability to keep serializability under low isolation levels, \sysname wants to select the optimal isolation level for the real-time workload adaptively, which varies over long periods of time due to the changes in aspects as diverse as the template proportion and key distribution. To address this, we introduce \textbf{Adaper}, which mainly includes two modules (details in \S~\ref{design-2}). First, \textbf{Workload2Graph} extracts features from the workloads that may affect the performance and characterizes the workload in the form of a graph, where the vertices are types of the transactions in the workloads, and the edges are the data relationship between two transactions. 
% Then, the \textbf{Isolation Predictor} adopts a graph-based learning model to embed the graph into vectors and predicts the optimal isolation level using a deep learning model. 
% A key component here is the \textbf{Adapter}. 

% The \textbf{Collector} samples runtime transaction instances and sends the transaction information, including transaction type and read/write set, to the adapter.
% The adapter then predicts the optimal isolation level for the current workload.
% If the optimal isolation level changes, the \textbf{Isolation Switcher} will apply the new isolation level to incoming transactions. During the switchover phase, transactions under two different isolation levels coexist within the system, denoted as $T_{old}$ and $T_{new}$, respectively. While the dependency detector ensures serializability for transactions at a single isolation level, the switcher implements a more stringent dependency detection mechanism. This prevents old transactions from depending on new ones (i.e., the switcher ensures the non-existence of $T_{old} \nrightarrow T_{new}$), thereby maintaining serializability.
% We demonstrate the serializability during the isolation level switch phase in \S~\ref{sec:proof.switch}.

% \noindent \textbf{Offline workflow.} 
% Before \sysname works, application developers need to register transaction templates into \sysname. Following this, the \textbf{Static Analyzer} constructs the static dependency graph (SDG) of the workload and analyzes the workload according to the theories of weak isolation levels~\cite{}. This analysis identifies potential data anomalies under each weak isolation level, i.e., the vulnerable edges that require handling. Take \textit{SmallBank} as an example, which includes five transaction templates: Balance (Bal), Depositchecking (DC), Transactsaving (TS), Amalgamate (Amg), and Writecheck (WC). The SDG for these is illustrated in Figure~\ref{fig: SmallBank}. When the database's isolation is SI, the transaction instances of Bal, WC, and TS are denoted as $B_i$, $W_j$, and $T_k$, respectively. These can exhibit a \textit{dangerous structure}, where $B_i$ is concurrent with $W_j$, $W_j$ is concurrent with $T_k$, and $T_k$ is the first to commit. Under RC, all vulnerable edges must be considered as they could potentially cause \textit{dirty read} anomaly. Subsequently, the \textbf{Workload Simulator} generates various randomized workloads, including different ratios of transaction templates, varying data skews, and so on. \sysname then executes each workload using different isolation level strategies and labels the optimal one for that workload. The Adapter comprises two key components: Workload2Graph and GraphPredictor. Workload2Graph characterizes the transaction behaviors using transaction type and read/write set, forming a graph where vertices represent transaction features and edges indicate data dependency between transactions. Then PerformancePredictor adopts a graph-based learning model to embed the graph and predicts the query performance using a deep learning model (details in \S~\ref{design-2}). 
% The \textbf{Adapter} then tests the performance of each isolation strategy offline, utilizing a learning model to train the optimal isolation strategy for various workload scenarios 

% \noindent \textbf{Online workflow.}
% online，应用通过用户的实时请求，通过对 template 添加参数生成事务，事务首先进入 Executor 模块，Executor 主要由三个模块组成：。他们分别的功能是什么，details in 
% 在 Executor 执行结束，已知读写集，将事务特征化，送入 Statistic Collector 中，该模块通过xxx，并将时间窗口中的负载特征化，由 Adapter 生成出适合当前负载的最优方案，然后对负载进行调整。为了保证调整过程的正确性，通常在过渡阶段中，将上下层均设置为较为严格的。例如，
% The application generates online transactions based on user inputs and transaction templates, which are handed off to the \textbf{Executor} for scheduling and concurrency control. Different from existing work, we try to keep the commit order of transactions involved in a vulnerable dependency instead of altering the dependency into ww dependency (details in\S~\ref{design-1}). The \textbf{Executor} is made up of three main components: \textbf{Record Buffer}, \textbf{Flow Controller}, and \textbf{Validate Lock Table (VLT)}. The record buffer is comprised of a thread-local memory, used for storing the read/write set according to the validation strategy. The VLT is a hash table, requiring transactions to acquire the appropriate validation lock before committing. The guard's role is to maintain the ratio of read and write operations to improve the efficiency of validation (details in \S~\ref{design-3}). In the previously mentioned SmallBank example, we need to prevent all dangerous structures if the isolation level is set to SI. \sysname stores the read/write set of $W_j$ and $T_k$ during the execution. Then, both $W_j$ and $T_k$ are required to acquire locks in the VLT before committing and release them post-commitment. If $W_j$ obtains the lock before $T_k$, this guarantees the commit order for that dependency edge. Otherwise, if $T_k$ secures the lock before $W_j$, then $W_j$ will discover its read set modified and $W_j$ will eventually be rolled back. 

% Different from existing work, there exists a vulnerable dependency between transactions $W_j$ and $T_k$, and we try to keep their commit order. 

% The \textbf{Statistic Collector} samples meta information from executed transactions, including their types and read/write sets. After collecting a batch of transactions, it sends meta-information about the batch to the adapter. The adapter first characterizes the batch using a graph structure model and then employs an offline-trained model to predict the optimal isolation level strategy for the current workload. To prevent data anomalies during isolation level adjustments, \sysname validates transactions at the weaker of the two strategies until all transactions running under the previous isolation level are completed. For instance, when switching from RC to SI, Tristar will validate transactions according to the RC scheme until all SI transactions are fully executed (details in \S~\ref{design-2}).
% When adjusting the isolation level scheme, we need to ensure that there are no data anomalies, so Tristar will validate at the weaker of the two schemes until the transactions running at the previous isolation level have completed. For example, if you want to switch a database from RC to SI, Tristar will validate the currently running transactions according to the scenario where the database is set to RC until all the transactions running in SI have finished executing (details in \S~\ref{design-2}).
% It then characterizes the workload within a specific time window. Based on this characterized workload, the adapter determines the optimal isolation level and concurrency control scheme according to the model for the current workload. To ensure serialization during the adjustment processing, it's typical to set both the concurrency control strategy in \sysname and the isolation level in the database stricter during the transition phase (details in \S~\ref{design-2}).
% For instance, when \sysname needs to shift from SI to RC, the transactions in progress need to be managed for concurrency in Tristar according to the validation requirements of RC until all transactions under SI have been executed.


% \noindent \textbf{Discussion.}
% % ai4db 的 limitation
% Previous works observe that the workload typically obeys some patterns and tends to remain relatively stable for a period of time after workloads change~\cite{DBLP:conf/icde/ZhengZLYCPD24, DBLP:conf/sigmod/MaAHMPG18, DBLP:journals/pvldb/YuZSY24}. 
% Our evaluation shows that the \textit{Adapter} is highly efficient in predicting the optimal isolation level, requiring only approximately 50 ms. 
% Therefore, we will leverage the characteristics of the current workload.
% % Moreover, our evaluation shows the \textit{Adapter} uses about 50 ms to predict the isolation level.
% % Therefore, we rely on the characteristics of the current workload to predict the optimal isolation level. 
% However, when the workload is entirely random, \sysname struggles to accurately predict the optimal level. % due to the unpredictability of future workload characteristics. 
% In such cases, we can mitigate the performance overhead by simply controlling the frequency of isolation level adjustments until the workload stabilizes. 

% % There is an assumption that the time gap between dynamic workload changes should be significantly longer than the time required for the collector to detect the workload change and for the Adapter to calculate the result. When Tristar detects workload is unstable in the window, it generally disregards the changes until the workload achieves relative stability.

