\section{Introduction}
Relational database management systems (RDBMSs) often offer multiple isolation levels to support diverse applications. These isolation levels, ranging from lower to higher consistency, include \textit{read committed} ({\bf RC}), \textit{snapshot isolation} ({\bf SI}), and \textit{serializable} ({\bf SER}).
Typically, setting RDBMSs to a lower isolation level can lead to higher performance. Consequently, RDBMSs are often configured to use a lower isolation level that will not cause application anomalies.
For example, in an online retail system, setting the underlying RDBMS to RC allows product catalogs and inventory levels to be read consistently while transactions update inventory levels. This configuration ensures that customers only see committed transactions, reducing the risk of reading uncommitted changes while maintaining higher throughput.

SER is widely accepted as the gold standard for transaction processing in RDBMSs, and in many mission-critical applications, such as those in finance and stock trading, it is essential to operate at SER isolation to ensure that there does not exist any anomalies in the applications.
Thus far, there are two categories of approaches to achieving SER, namely built-in approaches and plug-in approaches.
The first category of approaches focuses on designing new concurrency control algorithms that replace the existing two-phase locking approach (2PL) or serializable snapshot isolation (SSI) inside RDBMSs. 
Recent works include TCM~\cite{DBLP:conf/icde/LometFWW12}, Silo~\cite{DBLP:conf/sosp/TuZKLM13}, TicToc \cite{DBLP:conf/sigmod/YuPSD16}, Sundial~\cite{DBLP:journals/pvldb/YuXPSRD18}, and RedT \cite{DBLP:journals/pvldb/ZhangLZXLXHYD23}.
The second category of approaches does not modify the concurrency control approaches of the RDBMS kernel. Instead, they set the RDBMSs to a lower isolation level and 
%implement concurrency control approaches
implement anomaly prevention approaches that ensure serializable scheduling at a higher level, such as in the middleware layer.
As plug-in approaches are orthogonal to built-in approaches, in this paper, we focus on plug-in approaches. 

In real-world applications, users are often requested to submit transactions by completing templates or forms provided by the application.
These transactions are structured by templates, meaning that they are the instances of the transaction templates.
This configuration helps the transactions be categorized in advance. 
% For instance, in SmallBank benchmark (shown in Figure \ref{fig:SmallBank}), transactions are classified into five categories: amalgamate (Amg), balance (Bal), depositChecking (DS), transactSavings
% (TS), and writeCheck (WC).
By organizing transactions into categories, static or offline analysis can be utilized to identify dependencies such as write/read (WR), read/write (RW), and write/write (WW) relationships among them.
\begin{figure}[t]
    \centering
    \vspace{3mm}
    \includegraphics[width=0.48\textwidth]{figures/SmallBank-v2.pdf}
    \vspace{-4mm}
    \caption{Static dependency analysis and runtime ``dangerous structure'' detection in SmallBank benchmark
    }
    \label{fig:SmallBank}
    \vspace{-4mm}
\end{figure}

\begin{example}
\label{exa:oaofsb}
Figure \ref{fig:SmallBank}(a) shows a static analysis of the SmallBank benchmark.
First, transactions are classified into five categories: amalgamate (Amg), balance (Bal), depositChecking (DS), transactSavings (TS), and writeCheck (WC).
Second, there are a total of 26 pairwise dependencies between transactions, including 16 WW dependencies, 5 WR dependencies, and 5 RW dependencies. It is important to note that Bal is a read-only transaction and has an RW dependency with the other four transaction categories, which conversely have a WR dependency with Bal.
%Additionally, there is an RW dependency between WC and TS. The WR dependency is the inverse of the RW dependency. Furthermore, WW dependencies occur between Amg and DC, TS, and WC, as well as between DC and WC.
\qed
\end{example}


The objective of conducting offline dependency analysis across transaction categories is to pinpoint the ``dangerous structure'' that contravenes the requirements set by SER but is permissible at a lower isolation. In simpler terms, these ``dangerous structures'' are acceptable at lower isolation but might form cycles that violate SER constraints. In SI, the ``dangerous structure'' is characterized by two consecutive RW dependencies~\cite{DBLP:conf/pods/Fekete05, DBLP:journals/sigmod/FeketeOO04}. Conversely, under RC, the ``dangerous structure'' is defined by a RW dependency~\cite{DBLP:conf/aiccsa/AlomariF15, DBLP:journals/pvldb/VandevoortK0N21}.

\begin{example}
\label{exa:dsofsb}
% WHAT are ``dangerous structure'' and why.
Figure \ref{fig:SmallBank}(b)(c) illustrate the ``dangerous structure" of SmallBank under SI and RC, respectively. Take SI as an example. The ``dangerous structure'' comprises one RW from Bal to WC and the other RW from WC to TS. For illustration purposes, we denote transactions generated from templates of Bal, WC, and TS as $T_{bal}$, $T_{wc}$, and $T_{ts}$, respectively.
Figure~\ref{fig:SmallBank}(d) shows an example of scheduling with two consecutive RW dependencies, i.e., from $T_{bal}$ to $T_{wc}$, and from $T_{wc}$ to $T_{ts}$.
As transaction $T_{bal}$ starts after $T_{ts}$ commits, it is possible to have a 
WR dependency from $T_{ts}$ to $T_{bal}$. This forms a dependency cycle, leading to non-serializable scheduling of $T_{bal}$, $T_{wc}$, and $T_{ts}$.
% the write operation of $T_{ts}$ is dependent on the read operation of $T_{wc}$, while $T_{ts}$ commits before $T_{wc}$, resulting in a dependency cycle.
\qed
\end{example}

% \textcolor{blue}{DESCRIBE the MAIN IDEA and Give an example}. 
% In real application workloads, transaction templates (code) are usually not updated frequently, meaning that static or offline analysis can be introduced to determine whether all possible transaction (which are instantiations of transaction templates) execution sequences satisfy serializability when the RDMBSs are set to a weak isolation level~\cite{DBLP:journals/pvldb/VandevoortK0N21}. If a cycle exists in the dependency graph generated by these transactions, then execution is not serializable. The characters of the cycle have received quite a bit of attention in the literature.  
% In SI, there always exists a \textit{dangerous structure}, which consists of two consecutive read-write dependencies~\cite{DBLP:conf/pods/Fekete05}. In RC, a read-write (rw) dependency always exists~\cite{DBLP:conf/aiccsa/AlomariF15, DBLP:journals/pvldb/VandevoortK0N21}. 
% Consider the SmallBank workload, which includes 5 transaction templates: \textit{Amalgamate} (Amg), \textit{Balance} (Bal), \textit{DepositChecking} (DS), \textit{TransactSavings} (TS), and \textit{WriteCheck} (WC). Figure~\ref{fig:SmallBank} illustrates the static dependency graph and the anomalies under weak transaction isolations. When the SmallBank workload is executed in SI, transactions such as Bal, WC, and TS can form a cyclic dependency. This cycle contains a vulnerable structure, thereby violating serializability.


% A straightforward solution might be to set the database isolation level to serializable, but this approach often results in poor performance. To leverage the performance benefits of weaker isolation levels while avoiding data anomalies, some researchers have modified query statements to eliminate anomalies without altering transaction semantics. 

Preventing the occurrence of dangerous structures is recognized as a sufficient condition to ensure that an RDBMS operating under a lower isolation level can achieve SER isolation. Consequently, existing approaches focus on eliminating dangerous structures {\it by modifying query statements without altering transaction semantics.}
In general, there are two approaches to handle this.
(1) {\bf Conflict materialization~\cite{DBLP:conf/aiccsa/AlomariF15}} introduces an external lock manager that explicitly adds an extra write operation for a read operation of each transaction with an RW dependency.
% , thereby preventing the concurrent execution of transactions that may have a RW dependency.
(2) {\bf Promotion~\cite{DBLP:conf/icde/AlomariCFR08}} promotes read operations to write operations, converting RW dependencies to WW dependencies to prevent the occurrence of dangerous structures.
% Preventing the occurence of the dangerous structures is a necessary condition to make sure that a RDBMS under a lower isolation can achieve SER isolation.
% For this reason, existing approaches focus on the elimination of the dangerous structures by by modifying the query statements but without altering transaction semantics.
% There are generally two approaches to handle this: (1) \textbf{Conflict Matrialization}. This method introduces an external conflict table to materialize conflicts by adding extra write operations to both $T_i$ and $T_j$~\cite{DBLP:conf/aiccsa/AlomariF15}, which avoids concurrent execution of transactions that may have the read-write dependency; (2) \textbf{promotion}. This technique promotes the read operation in $T_i$ to a write operation~\cite{DBLP:conf/icde/AlomariCFR08}, upgrading dependencies to write-write dependencies to avoid data anomalies.
Regarding these approaches, because they either introduce extra write operations or promote read operations to write operations, the overall concurrency can decrease. In certain cases, when the benefits of setting the RDBMS to a lower isolation level are comparable to or smaller than the costs incurred by the decrease in concurrency, these approaches may become ineffective.

% introduce additional write-write conflicts, which can restrict concurrency. Additionally, the weaker the isolation level, the more data anomalies must be managed outside the database. For dynamic workloads, including changing proportions of transaction templates and varying data access patterns, trade-offs must be made between performance gains and anomaly management. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these diverse conditions. 
% It received quite a bit of attention in the literature. 

% In application workloads, transaction templates (business code) are usually not updated frequently, so static or offline analysis can be introduced to determine whether all possible transaction execution sequences satisfy serialisable scheduling.

% When the RDMBSs are set to a lower isolation level, 
% Despite years of development, transaction processing remains a critical research area due to its fundamental role in ensuring ACID properties and enhancing system performance.
% Research in this domain can be typically divided into two categories.
% The first aims to maximize transaction performance while maintaining serializability.
% However, these approaches are constrained by the bottlenecks caused by strict ordering requirements inherent in serializability.
% Consequently, another line of research opts for lower isolation levels, such as \textit{Read Committed} (RC) and \textit{Snapshot Isolation} (SI).
% These isolation levels sacrifice the strict ordering of serializability in favor of improved performance, though they may introduce data anomalies that compromise data consistency.


% It is widely recognized that under specific workloads, certain anomalies may not occur, allowing transactions to be correctly processed at weaker isolation levels. 
% A notable example is that TPC-C transactions~\cite{TPCC} can be safely scheduled under SI instead of serializability.
% However, not all workloads can be safely scheduled at weak isolation levels. For instance, consider the SmallBank benchmark. Figure~\ref{fig:SmallBank} illustrates the static dependency graph and the resulting anomalies under weak transaction isolations. A straightforward solution might be to set the database isolation level to serializable, but this approach often results in poor performance. To leverage the performance benefits of weaker isolation levels while avoiding data anomalies, some researchers have modified query statements to eliminate anomalies without altering transaction semantics. The theory has established that data anomalies, whether arising from SI or RC, typically share a common characteristic: there are two transactions $T_i$ and $T_j$, with $T_j$'s write operation depending on $T_i$'s read operation and $T_j$ commits before $T_i$ commits. In other words, eliminating these structures can guarantee serializability. There are generally two approaches to handle this: (1) \textbf{Conflict Matrialization}. This method introduces an external conflict table to materialize conflicts by adding extra write operations to both $T_i$ and $T_j$~\cite{DBLP:conf/aiccsa/AlomariF15}, which avoids concurrent execution of transactions that may have the read-write dependency; (2) \textbf{promotion}. This technique promotes the read operation in $T_i$ to a write operation~\cite{DBLP:conf/icde/AlomariCFR08}, upgrading dependencies to write-write dependencies to avoid data anomalies.
% However, these approaches have limitations. They introduce additional write-write conflicts, which can restrict concurrency. Additionally, the weaker the isolation level, the more data anomalies must be managed outside the database. For dynamic workloads, including changing proportions of transaction templates and varying data access patterns, trade-offs must be made between performance gains and anomaly management. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these diverse conditions.
% For example, as depicted in the lower right of Figure~\ref{fig: SmallBank}, achieving serializable scheduling in RC for the SmallBank workload requires promoting the read operations in Balance transactions, changing them from read-only transactions to read-write transactions. Originally, Balance transactions do not block each other, but post-promotion, they will. 
% Secondly, the weaker the isolation level, the more data anomalies must be handled outside the database. For dynamic workloads, including dynamic proportions of transaction templates and dynamic data access distribution, trade-offs must be made between the gains of both. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these situations. 

% However, these approaches generally require an analysis of the workload to determine the appropriate isolation level, [which can be inefficient and impractical for managing dynamic real-world workloads.]...
% Further, not all workloads can be safely scheduled at weak isolation levels.
% ...
% For this reason, numerous studies focus on characterizing data anomalies at weak isolation levels to determine if transaction workloads can still meet the requirements of serializability.
%, i.e., maintaining an acyclic conflict dependency graph. 
Ideally, the best plug-in approach to achieve serializability should satisfy the following requirements. \blackding{1} The ``dangerous structure'' defined at a given isolation level should be prevented. \blackding{2} The cost of preventing ``dangerous structure'' should be as minimal as possible.
\blackding{3} The isolation level of the RDBMS may be adaptively adjusted as the workload varies so that the overall performance can be maximized.
% Nevertheless, finding such an optimal approach poses two challenges.
% \textcolor{blue}{
% First, many workloads cannot run on weak isolation levels due to data anomalies, and it is not trivial to guarantee the serializability of workloads on weak isolation levels without modifying the workload and concurrency control in the database.
% Second, selecting the most appropriate isolation level based on the dynamic characteristics in dynamic workload scenarios is challenging. 
% }
In this paper, we introduce \sysname, a middleware system equipped with three key techniques that fulfill all of the aforementioned requirements.
% \sysname works in the middleware, and 
% designed to achieve efficient serializable transaction scheduling through self-adaptive isolation level selection. To address the challenges inherent in this task, we have developed three key techniques:

% (1) we propose a plug-in middleware system that integrates the concurrency control within and outside the RDBMS kernel. It detects runtime ``dangerous structures" without modifying the workload and RDBMS kernel. This allows the second layer, the RDBMS itself, to safely operate under a low isolation level.
% To the best of our knowledge, \textit{\sysname is the first work to propose a two-layer concurrency control framework.}
% (1) we propose a plug-in middleware system that detects runtime ``dangerous structures" without modifying the workload and RDBMS kernel. This allows the second layer, the RDBMS itself, to safely operate under a low isolation level.


% dynamically modifying the isolation to suit changing workloads without compromising the system's correctness and performance.

% while improving transaction processing performance remains a critical topic in database research.
% To achieve this objective, various concurrency control (CC) algorithms have been proposed~\cite{} to schedule transactions in a specific order, such as locking order, timestamp order, etc.
% Unfortunately, due to the strict ordering requirements of serializable scheduling, even the state-of-the-art serializable CC algorithms cannot match the performance of CC algorithms supporting lower isolation levels, such as \textit{Read Committed} (RC) and \textit{Repeatable Read} (RR).

% Serializability is widely accepted as the gold standard for transaction processing in database systems.
% To achieve serializability, modern databases often adopt 2PL and its variants. For example,  PostgreSQL employs serialize snapshot isolation (SSI)~\cite{DBLP:conf/sigmod/CahillRF08, DBLP:journals/pvldb/PortsG12}, which detects and prevents consecutive rw dependencies -- known as \textit{dangerous structure}. 
% Similarly, MySQL uses strict two-phase locking (S2PL)~\cite{DBLP:conf/vldb/BayerEHR80} remains a fundamental technique, enforcing serializability by requiring transactions to acquire all necessary locks before releasing any and holding these locks until the transaction commits. 

% In addition to serializable isolation, modern database systems often provide multiple isolation levels, including \textit{Read Committed} (RC), \textit{Read Repeatable} (RR) or \textit{Snapshot Isolation} (SI). In PostgreSQL, RR is implemented as SI and in this paper, we only consider the two weak isolation levels: RC and SI. 
% For this reason, numerous studies focus on characterizing data anomalies at weak isolation levels to determine if transaction workloads can still meet the requirements of serializability, i.e., maintaining an acyclic conflict dependency graph. 
% A notable example is the TPC-C benchmark~\cite{TPCC}, which can be safely scheduled under SI.

% However, not all workloads can be safely scheduled at weak isolation levels. The SmallBank benchmark is a typical example~\cite{DBLP:conf/icde/AlomariCFR08}. Theoretical work indicates that non-serializable schedules generate a dependency graph with a cycle specific to the isolation level: \textit{dangerous structure} for SI~\cite{DBLP:conf/sigmod/CahillRF08} and a "counterflow edge" for RC~\cite{DBLP:journals/tods/KetsmanKNV22, DBLP:conf/aiccsa/AlomariF15}. This concept extends to workloads through a static dependency graph, where each program is represented by a node. A conflict edge exists between nodes if a schedule can induce that conflict. The absence of a cycle specific to the isolation level represents the workload can be scheduled at this isolation level without data anomalies. For better illustration, we plot the static dependency graph and list the vulnerable dependency edges for SmallBank in Figure~\ref{fig: SmallBank}, indicating that SmallBank cannot be safely executed at isolation levels weaker than \textit{serializable}. For example, when we run at SI, the transaction instances inherited by Bal, WC and TS can construct a cycle with two consecutive rw dependencies. 

% Fortunately, the transaction types and queries in business workload are not changed frequently, so are numerous of work has been done to identify potential data anomalies in the workload at weak isolation levels with the help of static dependent graphs (SDG), where the vertices of the graph are the transaction templates, and the edges represent the static dependencies. 
% Based on these theories, Fekete~\cite{} et al. find that TPCC~\cite{} does not suffer from data anomalies in snapshot isolation(SI), and therefore TPCC does not need to run at a stronger isolation level. However, not all workloads are robust at weak isolation levels, e.g. Alomari~\cite{} et al. found the dangerous structure~\cite{} in SmallBank, which means SmallBank can not safely execute at isolations weaker than \textit{serializable}. 

% However， ***方法 does not work properly in real applications \cite{}. 
% 举反例。

% For decades, with the popularity of the Internet, application techniques have been the hotspot of academia and industry~\cite{DBLP:journals/sigmod/TangWZYZG023}. Applications typically utilize databases to store and manage large amounts of data and acquire the database to ensure the serializable execution of concurrent transactions to ensure the correctness of business code~\cite{}.

% use the ACID characteristics of transactions to ensure the correctness of business code~\cite{}. 

% Serializability is a key property for OLTP scenarios.

% In modern databases, achieving serializability is a key property for ensuring the correctness of concurrent transactions, is accomplished through various advanced techniques. Serializable snapshot isolation (SSI)~\cite{DBLP:conf/sigmod/CahillRF08, DBLP:journals/pvldb/PortsG12} is a widely adopted method that ensures serializability by detecting and resolving conflicts between concurrent transactions operating under snapshot isolation. Another approach achieves serializable with the help of serial safety net (SSN)~\cite{DBLP:journals/vldb/WangJFP18}, which tracks the resulting dependencies and determines whether it can commit safely or must abort to avoid
% a potential dependency cycle. Additionally, Strict Two-Phase Locking (S2PL)~\cite{DBLP:conf/vldb/BayerEHR80} remains a fundamental technique, enforcing serializability by requiring transactions to acquire all necessary locks before releasing any and holding these locks until the transaction commits. 

% Besides the serializability, modern databases support a variety of isolation levels for clients to choose from. 
% Among them, isolation can avoid data anomalies when multiple clients concurrently operate the database, including dirty reads, dirty writes, and so on. To cope with different business requirements, the database has a variety of isolation levels for program developers to choose from. Generally speaking, a weaker isolation level means higher concurrency with more potential data anomalies. 
% A workload is said to be {\it robustness} at an isolation level if all possible execution sequences are serializable at that isolation level~\cite{}. 
% Choosing a reasonable isolation level scheme becomes challenging for programmers when developing applications.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.46\textwidth]{figures/intro.pdf}
%     \vspace{-3mm}
%     \caption{Approaches for preventing data anomalies in weak isolation levels}
%     \label{fig: intro}
%     \vspace{-4mm}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \vspace{3mm}
%     \includegraphics[width=0.46\textwidth]{figures/SmallBank.pdf}
%     \vspace{-3mm}
%     \caption{Static dependency graph of SmallBank. 
%     % \textnormal{double-arrow represents ww dependency, dash arro}
%     }
%     \label{fig: SmallBank}
%     \vspace{-4mm}
% \end{figure}


% Previous work modifies transaction templates in the workloads without affecting the semantics to address the problems mentioned and enable workloads that do not satisfy serializability at weak isolation levels to still benefit from the performance advantages of weak isolation levels. In previous theories, whether dealing with \textit{dangerous structures} or \textit{counterflow edges}, a common structure is observed: two transactions $T_i$ and $T_j$, with $T_j$'s write operation depending on $T_i$'s read operation and $T_j$ commits before $T_i$ commits. To eliminate this structure, there are typically two approaches to modify the workload: (1) \textbf{Conflict Matrialization}. It adds an external conflict table to materialize conflicts, introducing extra write operations into both $T_i$ and $T_j$~\cite{DBLP:conf/aiccsa/AlomariF15}, which concurrent execution of transactions that may have the rw dependency; (2) \textbf{promotion}. It promotes the read operation in $T_i$ to a write operation~\cite{DBLP:conf/icde/AlomariCFR08}, which upgrades rw dependencies to ww dependencies to avoid data anomalies. 
% It is worth noting that only a small portion of operations trigger data anomalies in some workloads. If these data anomalies can be avoided at a low cost, the workload can still be run at a weak isolation level and reap higher performance benefits. 
% However, these approaches still have limitations. First, they introduce extra write-write conflicts into transaction templates, which can limit concurrency. For example, as depicted in the lower right of Figure~\ref{fig: SmallBank}, achieving serializability in RC for the SmallBank workload requires promoting the read operations in Balance transactions, changing them from read-only transactions to read-write transactions. Originally, Balance transactions do not block each other, but post-promotion, they will. Secondly, in general, the weaker the isolation level, the more data anomalies need to be handled outside the database. For various workloads or different proportions of transaction templates within the same workload, trade-offs must be made between the gains of both. No single solution is optimal for all scenarios, and existing work does not provide comprehensive solutions for these situations. 
% No one set of solutions is optimal in all scenarios, and none of the existing work provides solutions in this scenarios. 

% 如何解决这个问题。
% 1）方法一：把读改成写
% 2）方法二：****；
% 这两类方法都存在严重的性能问题。
% 引出我们的工作。

% 挑战：
% 1）不管数据库设置成哪种隔离级别，上层应用仍然能够确保可串行化，对用户是透明的。
% 2）如何选择最佳的隔离级别，使得性能最优。

% 针对每一个挑战，提出我们的方法。
% 1）
% 2）
% 3）

% Previous theory show that when a data anomaly occurs in either RC or SI, there are always two transactions $T_i$ and $T_j$, with $T_j$'s write operation depending on $T_i$'s read operation. Meanwhile, there is also a necessary condition that \textit{$T_j$ commits before $T_i$ commits}.



% \todo{scope: multi-version, widely adopted in open-source and commercial database systems}

% In order to handle the above limitations, we need to solve two challenges: 
% {\color{blue}
% (1) we want the choice of isolation level scheme to be transparent to the program developers so that they can pay more attention to the business logic and improve the development efficiency; 
% }
% (1) the existing work by transforming the load to make it run in a weak isolation level is not efficient; 
% (2) in dynamic scenarios, which is reflected in the conflict rate, transaction ratio, and other changes, as mentioned earlier, there is no one set of programs that can be in all scenarios to achieve optimal performance, we need to trade-off between the isolation level of the database and the data outside the database to avoid anomalies.

(1) \sysname works without modifying either the workload or the RDBMS kernel. Instead of statically introducing WW dependencies into the workload, \sysname dynamically detects and prevents "dangerous structures" that the RDBMS fails to handle at lower isolation levels, significantly reducing unnecessary blocking. Consider the read-only transaction category of \textit{Bal} in SmallBank, with the RDBMS configured under RC isolation.
Because \textit{Bal} transactions involve RW dependencies, any read by \textit{Bal} transactions would be promoted to a write, potentially blocking the execution of other \textit{Bal} transactions. In contrast, \sysname does not alter read operations throughout the execution, resulting in significantly improved concurrency.


%, the \textit{Bal} transaction is originally read-only but becomes a read-write transaction after the workload modification. While read-only transactions do not block each other, read-write transactions do. 
% We propose a plug-in middleware system that detects runtime ``dangerous structures" without modifying the workload and RDBMS kernel. This allows the second layer, the RDBMS itself, to safely operate under a low isolation level.
 
(2) Elimination of ``dangerous structures" is a sufficient but not necessary condition to achieve serializable scheduling.
Thus, preventing the occurrence of dangerous structures could cause many false positives. %where transactions that can be committed are aborted by mistake.
To alleviate this problem, we propose a validation-based concurrency control approach.
In this approach, we allow the occurrence of dangerous structures but introduce an extra requirement: no two transactions exist, and the dependency order and the commit order between them are inconsistent.
That is, upon the commit of a transaction $T_i$, if there is a transaction $T_j$ that commits before $T_i$ but there exists a dependency from $T_i$ to $T_j$, indicating that the dependency and commit order of $T_i$ and $T_j$ are inconsistent, then $T_i$ will abort. Otherwise, $T_i$ will commit.
% \sysname employs a validation-based concurrency control algorithm that dynamically detects and prevents ``dangerous structures" by verifying the consensus between dependency and the commit order of transactions. 
% The main idea of the validation is as follows: we first collect the dependencies of transactions during their read and write operations. Upon the commit of a transaction $T_i$, if there is a transaction $T_j$ that commits before $T_i$ but $T_j$ depends on $T_i$, indicating that the dependency and commit order of $T_i$ and $T_j$ are inconsistent, then $T_i$ will abort. Otherwise, $T_i$ will commit.
We provide theoretical proof that both ``dangerous structures" and the above consistent order are the necessary conditions of serializable scheduling.
% This approach hinges on the recognition that a key necessary condition for anomalies is the presence of at least one RW dependency where the dependency and commit order are inconsistent~\cite{DBLP:conf/pods/Fekete05, DBLP:conf/aiccsa/AlomariF15}. 
% As illustrated in Figure~\ref{fig:SmallBank}(d), besides the two consecutive RW dependencies, $T_{ts}$'s write operation depends on $T_{wc}$'s read operation, while $T_{ts}$ commits before $T_{wc}$ does. \sysname would detect the runtime RW dependency between $T_{ts}$ and $T_{wc}$ and ensure $T_{ts}$ commits after $T_{wc}$ or otherwise abort $T_{wc}$. 
For example, in Figure~\ref{fig:SmallBank}(d), because there is a dependency from $T_{ts}$ to $T_{wc}$, and $T_{ts}$ commits before $T_{wc}$. \sysname would detect the the dependency in the validation phase of $T_{wc}$ and abort it due to the inconsistency of the dependency order and commit order. 


% anomalies at low isolation levels while minimally impacting concurrency. 
% Instead of statically introducing WW dependency, 
% we identify that another non-trivial necessary condition is the presence of at least an RW dependency where the dependency and commit order are inconsistent~\cite{DBLP:conf/pods/Fekete05, DBLP:conf/aiccsa/AlomariF15}. As illustrated in Figure~\ref{fig:SmallBank}(d), besides the two consecutive RW dependencies, $T_{ts}$'s write operation depends on $T_{wc}$'s read operation, while $T_{ts}$ commits before $T_{wc}$ does. Therefore, to prevent the ``dangerous structure", \sysname aims to preserve the consistency of the dependency and commit order. In the given example, we detect the runtime RW dependencies between $T_{ts}$ and $T_{wc}$, and ensure $T_{ts}$ commits after $T_{wc}$ or otherwise abort $T_{wc}$. 

(3) \sysname utilizes a graph model to continuously predict the optimal isolation level for dynamic workloads. 
Initially, the workload is modeled as a graph, where each vertex corresponds to a transaction, and each edge denotes the RW, WR, WW, and RR dependencies between pairwise transactions. We then use a graph-based prediction model, which includes an embedding model and a prediction model, to forecast the optimal isolation level.
If the current isolation level is deemed suboptimal, \sysname switches to the optimal isolation. During this transition, transactions operating under different isolation levels can jeopardize serializability. To counter this, we have designed a non-blocking switching mechanism to achieve serializability. 
% Additionally, we design a rigorous isolation level switching mechanism to ensure serializability during the transition phase. 
% We then simulate various random workloads offline using different isolation level strategies and identify the optimal isolation level strategy for various workloads. 
% These characterized workloads and labels are utilized offline to train the model, thereby allowing us to adapt the isolation strategy for dynamic workloads in real time. 
% efficiently and adaptively tailors transaction isolation levels for applications. To tackle the stated limitations, we introduce two main techniques: 
% (1) a \textbf{two-layer concurrency control framework} that integrates the concurrency control within and outside the kernel, and a \textbf{validation-based concurrency control algorithm} devised to achieve serializable isolation across the entire system efficiently. Therefore, there is no need to convert the rw dependency into the ww dependency. The validation-based concurrency control algorithm can guarantee a consistent commit order. In the example above, we detect the rw dependency and guarantee $T_j$ commits after $T_i$ or otherwise aborts $T_i$. In high contention scenarios, frequent write operations can modify the read set of a transaction before it is committed. To mitigate this risk, we have designed external admission mechanisms that limit concurrent transactions, reducing the likelihood of read-set modifications.

% \textbf{(2) \sysname avoids data anomalies by guaranteeing the commit order of transactions after a read-write dependency occurs.} We observe that when a data anomaly occurs in either RC or SI, there are always two transactions $T_i$ and $T_j$, where $T_j$'s write operation is dependent by $T_i$'s read operation and $T_j$ commits before $T_i$. Instead of violently transforming rw-dependency into ww dependency, we design a validation-based concurrency control algorithm based on the two-layer concurrency control architecture to guarantee the commit order. In the example above, we detect the read-write dependency and guarantee $T_j$ commits after $T_i$ or otherwise aborts $T_i$.
% \textbf{(2) \sysname proposes a graph-based prediction model to predict the optimal isolation strategy for various workloads.} We use a graph-structured model to capture the workload characteristics, in which the vertices represent transaction features and edges between two transactions denote the data dependency between them. Then, we simulate various random workloads offline on different isolation level strategies and get the optimal isolation level scheme for them. These characterized workloads and labels are used offline to train the model. Therefore, we can adaptively run the workload in the optimal isolation strategy online. In addition, when switching schemes, \sysname will follow the scheme with a weaker database isolation level to preserve the order until all the transactions running in the previous scheme have been executed, ensuring no data anomalies in the transition phase. 
% (3) \sysname also introduces a \textbf{graph model} to predict the optimal isolation strategy for various workloads. 

% \textbf{(3) \sysname propose a graph-structured workload model to capture features and a prediction model based on graph convolutional neural network (GCN).} 
% The models learn the performance of different methods on dynamic loads offline, and choose an optimal scheme online based on the statistical modules in \sysname that characterize the workload. In addition, when switching schemes, \sysname will follow the scheme with a weaker database isolation level to preserve the order until all the transactions running in the previous scheme have been executed, ensuring no data anomalies in the transition phase. 

In summary, we make the following contributions:
\begin{itemize}[leftmargin=*]
    \item We propose a middleware system, \sysname, which dynamically detects and prevents the ``dangerous structure" without modifications of either the workload or the RDBMS kernel.  
    \item We propose a validation-based concurrency control approach, which reduces the false positives with a relax detection of ``dangerous structures"
    %in \sysname, 
    % which verifies whether the dependency order consistent with the commit order,  efficiently achieving serializability 
    (details in \S~\ref{design-1}). 
    % \item We propose a graph-based prediction model to predict the optimal isolation strategy by \textit{(i)} using a graph model to represent the workload characteristic; \textit{(ii)} learning a high-dimensional mapping from embedded features to isolation strategy using deep learning (details in \S~\ref{design-2}).
    \item We propose a graph-based prediction model that enables RDBMSs to adaptively select the optimal isolation level as the workload varies (details in \S~\ref{design-2}). To the best of our knowledge, \textit{\sysname is the first work to support self-adaptive isolation level selection for dynamic workloads.}
    \item We design a rigorous isolation level switching mechanism to ensure serializability during the transition. (details in \S~\ref{design-3}). 
    \item We have conducted extensive evaluations
    %on SmallBank, TPCC, and YCSB~\cite{DBLP:conf/cloud/CooperSTRS10} workloads with both PostgreSQL~\cite{PostgreSQL} 
    and the results show that \sysname achieves up to a 26.7x performance improvement over other state-of-the-art methods and up to 4.8x performance over the serialization provided by PostgreSQL.
\end{itemize}


% Relational database systems provide transactions with several isolation levels, where a weaker isolation level means higher concurrency with more potential data anomalies. 
% Web applications are increasingly built out of microservices. Building application out of microservices allow teams in large organizations to independently develop code with fewer concerns about programming language choice and code dependencies. This reduces friction in development.
