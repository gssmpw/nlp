@inproceedings{CusText,
   title={A Customized Text Sanitization Mechanism with Differential Privacy},
   url={http://dx.doi.org/10.18653/v1/2023.findings-acl.355},
   DOI={10.18653/v1/2023.findings-acl.355},
   booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
   publisher={Association for Computational Linguistics},
   author={Chen, Sai and Mo, Fengran and Wang, Yanhao and Chen, Cen and Nie, Jian-Yun and Wang, Chengyu and Cui, Jamie},
   year={2023},
   pages={5747–5758} }

@article{HAS,
  title={Hide and seek (has): A lightweight framework for prompt privacy protection},
  author={Chen, Yu and Li, Tingxin and Liu, Huiming and Yu, Yang},
  journal={arXiv preprint arXiv:2309.03057},
  year={2023}
}

@article{InferDPT,
  title={Privinfer: Privacy-preserving inference for black-box large language model},
  author={Tong, Meng and Chen, Kejiang and Qi, Yuang and Zhang, Jie and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2310.12214},
  year={2023}
}

@article{SanText,
  title={Differential privacy for text analytics via natural text sanitization},
  author={Yue, Xiang and Du, Minxin and Wang, Tianhao and Li, Yaliang and Sun, Huan and Chow, Sherman SM},
  journal={arXiv preprint arXiv:2106.01221},
  year={2021}
}

@misc{chen2022thexprivacypreservingtransformerinference,
      title={THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption}, 
      author={Tianyu Chen and Hangbo Bao and Shaohan Huang and Li Dong and Binxing Jiao and Daxin Jiang and Haoyi Zhou and Jianxin Li and Furu Wei},
      year={2022},
      eprint={2206.00216},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2206.00216}, 
}

@misc{chen2023federatedlargelanguagemodel,
      title={Federated Large Language Model: A Position Paper}, 
      author={Chaochao Chen and Xiaohua Feng and Jun Zhou and Jianwei Yin and Xiaolin Zheng},
      year={2023},
      eprint={2307.08925},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08925}, 
}

@misc{dong2023pumasecureinferencellama7b,
      title={PUMA: Secure Inference of LLaMA-7B in Five Minutes}, 
      author={Ye Dong and Wen-jie Lu and Yancheng Zheng and Haoqi Wu and Derun Zhao and Jin Tan and Zhicong Huang and Cheng Hong and Tao Wei and Wenguang Chen},
      year={2023},
      eprint={2307.12533},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2307.12533}, 
}

@misc{duan2023flocksstochasticparrotsdifferentially,
      title={Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models}, 
      author={Haonan Duan and Adam Dziedzic and Nicolas Papernot and Franziska Boenisch},
      year={2023},
      eprint={2305.15594},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.15594}, 
}

@misc{edemacu2024privacypreservingpromptengineering,
      title={Privacy Preserving Prompt Engineering: A Survey}, 
      author={Kennedy Edemacu and Xintao Wu},
      year={2024},
      eprint={2404.06001},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06001}, 
}

@article{goldreich1998secure,
  title={Secure multi-party computation},
  author={Goldreich, Oded},
  journal={Manuscript. Preliminary version},
  volume={78},
  number={110},
  pages={1--108},
  year={1998},
  publisher={Citeseer}
}

@article{hao2022iron,
  title={Iron: Private inference on transformers},
  author={Hao, Meng and Li, Hongwei and Chen, Hanxiao and Xing, Pengzhi and Xu, Guowen and Zhang, Tianwei},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={15718--15731},
  year={2022}
}

@article{kan2023protecting,
  title={Protecting user privacy in remote conversational systems: A privacy-preserving framework based on text sanitization},
  author={Kan, Zhigang and Qiao, Linbo and Yu, Hao and Peng, Liwen and Gao, Yifu and Li, Dongsheng},
  journal={arXiv preprint arXiv:2306.08223},
  year={2023}
}

@misc{lin2024emojicryptpromptencryptionsecure,
      title={EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models}, 
      author={Guo Lin and Wenyue Hua and Yongfeng Zhang},
      year={2024},
      eprint={2402.05868},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.05868}, 
}

@misc{tang2024privacypreservingincontextlearningdifferentially,
      title={Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation}, 
      author={Xinyu Tang and Richard Shin and Huseyin A. Inan and Andre Manoel and Fatemehsadat Mireshghallah and Zinan Lin and Sivakanth Gopi and Janardhan Kulkarni and Robert Sim},
      year={2024},
      eprint={2309.11765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.11765}, 
}

@inproceedings{utpala-etal-2023-locally,
    title = "Locally Differentially Private Document Generation Using Zero Shot Prompting",
    author = "Utpala, Saiteja  and
      Hooker, Sara  and
      Chen, Pin-Yu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.566",
    doi = "10.18653/v1/2023.findings-emnlp.566",
    pages = "8442--8457",
    abstract = "Numerous studies have highlighted the privacy risks associated with large language models. Our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46{\%} reduction in author identification F1 score against static attackers and a 26{\%} reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.",
}

@misc{yan2024protectingdataprivacylarge,
      title={On Protecting the Data Privacy of Large Language Models (LLMs): A Survey}, 
      author={Biwei Yan and Kun Li and Minghui Xu and Yueyan Dong and Yue Zhang and Zhaochun Ren and Xiuzhen Cheng},
      year={2024},
      eprint={2403.05156},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.05156}, 
}

@misc{yu2024federatedfoundationmodelsprivacypreserving,
      title={Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models}, 
      author={Sixing Yu and J. Pablo Muñoz and Ali Jannesari},
      year={2024},
      eprint={2305.11414},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.11414}, 
}

@misc{zhang2024buildingfederatedgptfederated,
      title={Towards Building the Federated GPT: Federated Instruction Tuning}, 
      author={Jianyi Zhang and Saeed Vahidian and Martin Kuo and Chunyuan Li and Ruiyi Zhang and Tong Yu and Yufan Zhou and Guoyin Wang and Yiran Chen},
      year={2024},
      eprint={2305.05644},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.05644}, 
}

