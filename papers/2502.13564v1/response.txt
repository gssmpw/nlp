\section{Related Work}
\label{sec:related work}
\subsection{Privacy Preserving Techniques}
Privacy concerns have become a significant issue for the broad adoption of LLMs, and a variety of works have explored the privacy-preserving techniques for LLMs **McMahan, "Federated Learning"**.
Differential Privacy (DP) **Dwork, "Differential Privacy"** is widely adopted by adding random noise into the dataset.
Federated Learning **Konečnỳ, "Federated Learning of Neural Networks: Architecture, Representations, and High-Level Theory"** offers a local-cloud collaboration paradigm without the need to send personal data to the central server.
Other approaches like Homomorphic Encryption **Gentry, "A Fully Homomorphic Encryption Scheme"** and Multi-Party Computation (MPC) **Yao, "Protocols for Secure Computations"** is time-consuming and can hardly be applied in real-world scenarios.

\subsection{Text Sanitization Approaches}

For cloud models with only API provided, the above approaches can be infeasible to implement.
Therefore, text sanitization techniques rise as an effective method, which aims to identify and eliminate sensitive information from the text.
**Huang, "Mask-Private Information"** replace tokens selective in the text, however, their methods are not focused on privacy attributes and still lead to privacy leakage risks.
**Xu, "Text Sanitization via Adversarial Learning"** hide the private entities for anonymization and seek private entities for de-anonymization. However, they only focus on classification and translation tasks, which can not be generalized to other tasks.
**Zhang, "Private Emoji-based Text Encryption"** uses Emoji to encrypt the user inputs before sending them to LLM.
**Liu, "Privacy Filter Module for Text Sanitization"** utilizes a privacy filter module to identify and replace the privacy information in the text without obfuscating non-privacy entities. In addition, they use the open-sourced model in the filter module and can fail for long documents without training.
On the contrary, our method is a multi-stage privacy-protecting framework for general QA scenarios and our hide and recover modules have been trained on corresponding tasks to better align with the requirements.