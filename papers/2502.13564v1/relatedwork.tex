\section{Related Work}
\label{sec:related work}
\subsection{Privacy Preserving Techniques}
Privacy concerns have become a significant issue for the broad adoption of LLMs, and a variety of works have explored the privacy-preserving techniques for LLMs \cite{yan2024protectingdataprivacylarge, edemacu2024privacypreservingpromptengineering}.
Differential Privacy (DP) \cite{InferDPT, utpala-etal-2023-locally, tang2024privacypreservingincontextlearningdifferentially, duan2023flocksstochasticparrotsdifferentially} is widely adopted by adding random noise into the dataset.
Federated Learning \cite{chen2023federatedlargelanguagemodel, yu2024federatedfoundationmodelsprivacypreserving, zhang2024buildingfederatedgptfederated} offers a local-cloud collaboration paradigm without the need to send personal data to the central server.
Other approaches like Homomorphic Encryption \cite{chen2022thexprivacypreservingtransformerinference, hao2022iron} and Multi-Party Computation (MPC) \cite{goldreich1998secure, dong2023pumasecureinferencellama7b} is time-consuming and can hardly be applied in real-world scenarios.

\subsection{Text Sanitization Approaches}

For cloud models with only API provided, the above approaches can be infeasible to implement.
Therefore, text sanitization techniques rise as an effective method, which aims to identify and eliminate sensitive information from the text.
\cite{CusText, SanText, InferDPT} replace tokens selective in the text, however, their methods are not focused on privacy attributes and still lead to privacy leakage risks.
\cite{HAS} hide the private entities for anonymization and seek private entities for de-anonymization. However, they only focus on classification and translation tasks, which can not be generalized to other tasks.
\cite{lin2024emojicryptpromptencryptionsecure} uses Emoji to encrypt the user
inputs before sending them to LLM.
\cite{kan2023protecting} utilizes a privacy filter module to identify and replace the privacy information in the text without obfuscating non-privacy entities. In addition, they use the open-sourced model in the filter module and can fail for long documents without training.
On the contrary, our method is a multi-stage privacy-protecting framework for general QA scenarios and our hide and recover modules have been trained on corresponding tasks to better align with the requirements.