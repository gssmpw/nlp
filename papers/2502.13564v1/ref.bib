% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@article{SanText,
  title={Differential privacy for text analytics via natural text sanitization},
  author={Yue, Xiang and Du, Minxin and Wang, Tianhao and Li, Yaliang and Sun, Huan and Chow, Sherman SM},
  journal={arXiv preprint arXiv:2106.01221},
  year={2021}
}


@inproceedings{CusText,
   title={A Customized Text Sanitization Mechanism with Differential Privacy},
   url={http://dx.doi.org/10.18653/v1/2023.findings-acl.355},
   DOI={10.18653/v1/2023.findings-acl.355},
   booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
   publisher={Association for Computational Linguistics},
   author={Chen, Sai and Mo, Fengran and Wang, Yanhao and Chen, Cen and Nie, Jian-Yun and Wang, Chengyu and Cui, Jamie},
   year={2023},
   pages={5747â€“5758} }


@article{InferDPT,
  title={Privinfer: Privacy-preserving inference for black-box large language model},
  author={Tong, Meng and Chen, Kejiang and Qi, Yuang and Zhang, Jie and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2310.12214},
  year={2023}
}

@article{kan2023protecting,
  title={Protecting user privacy in remote conversational systems: A privacy-preserving framework based on text sanitization},
  author={Kan, Zhigang and Qiao, Linbo and Yu, Hao and Peng, Liwen and Gao, Yifu and Li, Dongsheng},
  journal={arXiv preprint arXiv:2306.08223},
  year={2023}
}

@article{shen2024fire,
  title={The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts},
  author={Shen, Zhili and Xi, Zihang and He, Ying and Tong, Wei and Hua, Jingyu and Zhong, Sheng},
  journal={arXiv preprint arXiv:2406.14318},
  year={2024}
}

@article{HAS,
  title={Hide and seek (has): A lightweight framework for prompt privacy protection},
  author={Chen, Yu and Li, Tingxin and Liu, Huiming and Yu, Yang},
  journal={arXiv preprint arXiv:2309.03057},
  year={2023}
}

@misc{GPT-4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@misc{Claude,
    title = {Model card and evaluations for Claude Models},
    author = {Claude Models},
    howpublished = {\url{https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf}},
    year={2023},
    note = {(Accessed on: [insert date here])}
}

@misc{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}

@misc{llama,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{llm_survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2024},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.18223}, 
}

@misc{yan2024protectingdataprivacylarge,
      title={On Protecting the Data Privacy of Large Language Models (LLMs): A Survey}, 
      author={Biwei Yan and Kun Li and Minghui Xu and Yueyan Dong and Yue Zhang and Zhaochun Ren and Xiuzhen Cheng},
      year={2024},
      eprint={2403.05156},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.05156}, 
}

@misc{edemacu2024privacypreservingpromptengineering,
      title={Privacy Preserving Prompt Engineering: A Survey}, 
      author={Kennedy Edemacu and Xintao Wu},
      year={2024},
      eprint={2404.06001},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06001}, 
}

@article{Yao_2024,
   title={A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly},
   volume={4},
   ISSN={2667-2952},
   url={http://dx.doi.org/10.1016/j.hcc.2024.100211},
   DOI={10.1016/j.hcc.2024.100211},
   number={2},
   journal={High-Confidence Computing},
   publisher={Elsevier BV},
   author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
   year={2024},
   month=jun, pages={100211} }

@misc{das2024securityprivacychallengeslarge,
      title={Security and Privacy Challenges of Large Language Models: A Survey}, 
      author={Badhan Chandra Das and M. Hadi Amini and Yanzhao Wu},
      year={2024},
      eprint={2402.00888},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00888}, 
}

@inproceedings{utpala-etal-2023-locally,
    title = "Locally Differentially Private Document Generation Using Zero Shot Prompting",
    author = "Utpala, Saiteja  and
      Hooker, Sara  and
      Chen, Pin-Yu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.566",
    doi = "10.18653/v1/2023.findings-emnlp.566",
    pages = "8442--8457",
    abstract = "Numerous studies have highlighted the privacy risks associated with large language models. Our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46{\%} reduction in author identification F1 score against static attackers and a 26{\%} reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.",
}

@misc{tang2024privacypreservingincontextlearningdifferentially,
      title={Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation}, 
      author={Xinyu Tang and Richard Shin and Huseyin A. Inan and Andre Manoel and Fatemehsadat Mireshghallah and Zinan Lin and Sivakanth Gopi and Janardhan Kulkarni and Robert Sim},
      year={2024},
      eprint={2309.11765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.11765}, 
}

@misc{duan2023flocksstochasticparrotsdifferentially,
      title={Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models}, 
      author={Haonan Duan and Adam Dziedzic and Nicolas Papernot and Franziska Boenisch},
      year={2023},
      eprint={2305.15594},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.15594}, 
}

@misc{chen2023federatedlargelanguagemodel,
      title={Federated Large Language Model: A Position Paper}, 
      author={Chaochao Chen and Xiaohua Feng and Jun Zhou and Jianwei Yin and Xiaolin Zheng},
      year={2023},
      eprint={2307.08925},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08925}, 
}

@misc{yu2024federatedfoundationmodelsprivacypreserving,
      title={Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models}, 
      author={Sixing Yu and J. Pablo MuÃ±oz and Ali Jannesari},
      year={2024},
      eprint={2305.11414},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.11414}, 
}

@misc{zhang2024buildingfederatedgptfederated,
      title={Towards Building the Federated GPT: Federated Instruction Tuning}, 
      author={Jianyi Zhang and Saeed Vahidian and Martin Kuo and Chunyuan Li and Ruiyi Zhang and Tong Yu and Yufan Zhou and Guoyin Wang and Yiran Chen},
      year={2024},
      eprint={2305.05644},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.05644}, 
}


@misc{chen2022thexprivacypreservingtransformerinference,
      title={THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption}, 
      author={Tianyu Chen and Hangbo Bao and Shaohan Huang and Li Dong and Binxing Jiao and Daxin Jiang and Haoyi Zhou and Jianxin Li and Furu Wei},
      year={2022},
      eprint={2206.00216},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2206.00216}, 
}

@article{hao2022iron,
  title={Iron: Private inference on transformers},
  author={Hao, Meng and Li, Hongwei and Chen, Hanxiao and Xing, Pengzhi and Xu, Guowen and Zhang, Tianwei},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={15718--15731},
  year={2022}
}

@article{goldreich1998secure,
  title={Secure multi-party computation},
  author={Goldreich, Oded},
  journal={Manuscript. Preliminary version},
  volume={78},
  number={110},
  pages={1--108},
  year={1998},
  publisher={Citeseer}
}

@misc{dong2023pumasecureinferencellama7b,
      title={PUMA: Secure Inference of LLaMA-7B in Five Minutes}, 
      author={Ye Dong and Wen-jie Lu and Yancheng Zheng and Haoqi Wu and Derun Zhao and Jin Tan and Zhicong Huang and Cheng Hong and Tao Wei and Wenguang Chen},
      year={2023},
      eprint={2307.12533},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2307.12533}, 
}

@inproceedings{liu2022clts+,
  title={CLTS+: A New Chinese Long Text Summarization Dataset with Abstractive Summaries},
  author={Liu, Xiaojun and Zang, Shunan and Zhang, Chuang and Chen, Xiaojun and Ding, Yangyang},
  booktitle={International Conference on Artificial Neural Networks},
  pages={73--84},
  year={2022}
}

@misc{wikidump,
  author = {Wikimedia Foundation},
  url = {https://dumps.wikimedia.org},
  year = {2025},
  title = {Wikimedia Downloads}
}

@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    editor = "Ng, Hwee Tou  and
      Kwong, Olivia O.Y.",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@inproceedings{see-etal-2017-get,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}


@misc{lin2024emojicryptpromptencryptionsecure,
      title={EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models}, 
      author={Guo Lin and Wenyue Hua and Yongfeng Zhang},
      year={2024},
      eprint={2402.05868},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.05868}, 
}

@misc{zheng2023judgingllmasajudgemtbenchchatbot,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}