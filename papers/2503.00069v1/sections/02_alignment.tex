\section{Contemporary Approach to LLM Alignment}
\label{sec:alignment}

Aligning LLMs with human values is commonly understood as training them to act in accordance with user intentions \citep{leike2018scalableagentalignmentreward}.
The objective of LLM alignment is often conceptualized as fulfilling three core qualities, often referred to as the ``3H'' framework: honesty (regarding their capabilities, internal states, and knowledge), helpfulness (in performing requested tasks or answering questions within safe bounds), and harmlessness (encompassing both the refusal to fulfill harmful requests and the avoidance of generating harmful content) \citep{askell2021generallanguageassistantlaboratory,Bai2022TrainingAH}.\looseness=-1 

A prominent approach to achieve this alignment is through a preference-based approach like RLHF. The RLHF pipeline usually includes three stages: supervised fine-tuning (SFT), preference sampling and reward model training \citep{NIPS2017_d5e2c0ad,steinnon2020learning}, and reinforcement learning fine-tuning either using proximal policy optimization (PPO; \citealt{schulman2017proximalpolicyoptimizationalgorithms}), or directly through policy optimization (DPO; \citealt{rafailov2023direct}). The process usually starts with a generic pre-trained language model, which undergoes supervised learning on a high-quality dataset for specific downstream tasks. In this paper, we focus on the implications of the reward modeling stage due to its connection to an incomplete contract, which we will lay out in \Cref{sec:contract}.\looseness=-1

\subsection{Reward modeling from human preference.}
In the reward modeling stage, for a given input prompt $x$, the SFT model generates paired outputs, ${y_0, y_1} \in \mathcal{Y} \times \mathcal{Y}$, where $\mathcal{Y}$ denotes the set of all possible outputs that the model can generate in response to a given input. Human evaluators then select their preferred response, $y \in {y_0, y_1}$, providing data that guides the alignment process \citep{NIPS2017_d5e2c0ad,steinnon2020learning}. 
Human preferences are modeled probabilistically using frameworks like the Bradley-Terry model \citep{19ff28b9-64f9-3656-ba40-08326a05748e}. The preference probability for one response over another is expressed as%\looseness=-1
\begin{equation}
p(y_1 \succ y_2 \mid x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))},
\end{equation}
where $r(x, y)$ is a latent reward function approximated by a parametric reward model, $r_\phi(x, y)$. Using a dataset of comparisons $\mathcal{D}$, the reward model is trained by minimizing the negative log-likelihood
\begin{align}
\mathcal{L}_R(r_\phi, \mathcal{D}) &= \\ \nonumber
&-\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \Big[\log \sigma \big( r_\phi(x, y_w) - r_\phi(x, y_l) \big)\Big],
\end{align}
where $\sigma$ is the logistic function and $y_w$ and $y_l$ denote the preferred and dispreferred completions among $(y_1, y_2)$.
