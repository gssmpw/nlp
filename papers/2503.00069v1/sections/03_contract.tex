\section{LLM Alignment as a Contract}
\label{sec:contract}

In the following, we formalize LLM alignment through the lens of contract theory \citep{RePEc:mtp:titles:0262025760,echenique2023online}, a subfield of economics that studies how agreements are designed under conditions of incomplete information.
%In addition to the technological approach, the challenge of aligning artificial intelligence with human values can be effectively analyzed through the lens of contract theory \citep{hadfieldmanell2019}. 
We describe human-LLM interactions as a \textit{principal-agent} relationship, where a \textit{principal} (e.g., the user, system designer, or a company) seeks to incentivize an \textit{agent} (an LLM) to act in a desired manner \citep{garen1994} (see \Cref{fig:intro}). 
This framework provides a way to conceptualize how the principal tries to align the agent's behavior with their objectives, using the agent's action and its reward function as a \textit{contract}. In this section, we explore the contract formalization (\Cref{sec:contract-formal}) and how the incompleteness of this contract (\Cref{sec:contract-incomplete}) directly leads to misalignment (\Cref{sec:contract-misalignment}) in the context of LLM alignment.\looseness=-1

\subsection{Contract Formalization}
\label{sec:contract-formal}
Following \citet{echenique2023online}, we define a contract as a pair $(a, r)$, where $a \in \mathcal{A}$  represents an action of an agent and $r:(\mathcal{X} \times \mathcal{Y}) \to \mathbb{R}$ is a reward function.\footnote{Here we loosely refer $a$ to mean one action or a series of actions that lead to an LLM output.}
The function $r$ determines the agent's reward based on the observed input-output pair $(x, y)$. In the context of a user-LLM interaction, an input $x \in \mathcal{X}$ corresponds to a user prompt, and output $y \in \mathcal{Y}$ is the LLM-generated response. A contract might be, for instance, a positive reward if the model avoids hate speech in the output. Here, the reward function would be trained on prompt-response pairs, awarding higher scores to responses that do not contain hate speech.\looseness=-1 

The framework is initiated, for instance, when a user, acting as the principal, initiates the interaction by prompting an LLM, thus implicitly proposing a contract. 
The LLM, acting as an agent, then implicitly either accepts or rejects this contract. Rejection of the contract manifests in the LLM not converging towards the desired output, which is a generated response without hate speech.
Upon implicitly accepting the contract, the LLM conducts an action $a$, which can be viewed as a probability distribution over all possible model outputs that satisfy the contract. We note that the user does not directly observe the LLM's internal decision of its action but only the output $y$. Consequently, the agent is rewarded according to the agreed-upon reward function, $r(x,y)$, implemented as a reward signal during the training phase. The principal experiences the utility derived from the output $y$; that is, the user benefits from the generated response but also suffers if the model behaves adversarially. This is illustrated when, despite a contract penalizing hate speech, the LLM generates responses that subtly convey harmful biases.\looseness=-1

\subsection{The Challenge of Incomplete Contracting in AI}
\label{sec:contract-incomplete}

Although the specific implications of incomplete contracting for LLM alignment remain underexplored, the concept has been studied in the broader context of AI alignment \citep{hadfieldmanell2019}.
In theory, alignment between the principal and the agent theoretically requires a \textit{complete contract} \citep{williamson1975markets,hadfieldmanell2019}. A complete contract would perfectly align the principal's objectives with the agent's behavior in all possible states of the world. This requires that action $a$ and reward function $r(x,y)$ be optimally defined for all input-output pairs. However, achieving complete contracts is practically infeasible for AI systems, rendering incomplete contracting unavoidable \citep{hadfieldmanell2019}. This is primarily due to the fact that machine learning systems inherently operate with underspecified objectives \citep{10.5555/3586589.3586815}, which stems from the practical difficulty in defining a reward function $r(x,y)$ that fully captures the complexities of the desired behavior.


The difficulty in specifying such a complete reward function arises from several issues.
First, a key challenge for AI alignment generally, real-world applications are too complex to generate all possibilities, hindering the specification of every possible $(a,r)$ pair \citep{openai_faulty_reward_functions}. The space of possible outcomes, denoted by $\mathcal{Y}$ in the formalization is not tractable. This mirrors the challenge of LLM in generating outputs for new input it might receive during inference. The challenge extends beyond the practical limitations of fully specifying objectives. 
Second, and particularly relevant for LLMs, even beyond these practical limitations, the challenge of translating complex human values into reward functions remains. Ambiguities and gaps in defining the desired action contribute to unintended and often undesirable outcomes.\looseness=-1 


\subsection{Misalignment due to an Incomplete Contract} 
\label{sec:contract-misalignment}

We frame LLM alignment as a challenge of incomplete contracting, which leads to misalignment. In the context of LLMs, this misalignment occurs when the reward function, $r(x,y)$ is underspecified, and thus might incentivize outputs that diverge from the users's true objectives.\looseness=-1

A common outcome of reward misspecification is \textit{reward hacking}, where an agent optimizes for the reward itself rather than the intended behavior. 
For example, LLMs may exploit gaps in the specifications, such as in the ``jailbreaking'' phenomenon. Here, carefully crafted prompts elicit harmful responses by bypassing weak guardrails because their reward function is not specific enough, allowing the model to optimize without complying with safety requirements \citep{chao2024jailbreakingblackboxlarge,zou2023universaltransferableadversarialattacks}. 
Another example of reward hacking is an LLM trained to generate ``helpful'' responses might learn to produce lengthy and verbose answers to prompts, as this might result in a higher score from the reward function even if it is not actually helpful to the user \citep{saito2023verbosity}. 
A related issue, ``fake alignment,'' occurs where the agents superficially comply with the training objective without adopting the intended internal goals \citep{greenblatt2024alignmentfakinglargelanguage}.
Another challenge is the \textit{inherent context dependence} of reward functions, which need to adapt appropriately to evolving contexts. A contract might specify desired behavior in a narrow scenario, but leave ambiguities for broader applications \citep{NIPS2017_32fdab65}. For example, a contract that stipulates ``no harmful bias'' in a model is inherently underspecified since the definitions of ``harmful'' and ``bias'' are context-dependent. 