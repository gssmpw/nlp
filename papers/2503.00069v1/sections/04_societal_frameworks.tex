\section{Societal Alignment Frameworks}
\label{sec:societal-frameworks}

We present societal alignment frameworks that can provide guidelines for LLM alignment in an incomplete contracting environment. 
In the following, we discuss the alignment mechanisms of social theory (\Cref{sec:social}), economic theory (\Cref{sec:economic}), and contractual theory (\Cref{sec:legal}), and explore potential solutions for improving current LLM alignment approaches.\looseness=-1

\subsection{Social Alignment}
\label{sec:social}

Human communication relies on a complex, largely implicit set of norms, values, and cues that guide individuals in interpreting each other's intentions and the world around them \citep{10.1093/acprof:oso/9780190622046.001.0001}. 
However, this process is inherently ambiguous, as much of the meaning is conveyed implicitly rather than explicitly stated. Nonetheless, humans possess a unique ability called \textit{normative competence}, which allows them to understand and judge whether certain behaviors are appropriate or inappropriate in a given context \citep{Schutz1976}. This capability is often ingrained in cultures across the world \citep{hershcovich-etal-2022-challenges, doi:10.1177/0956797615586188,10.3389/fpsyg.2018.00849}, shaping shared understanding that facilitates communication and fosters mutual understanding \citep{mercier2018enigma}. 
A similar challenge arises in user-LLM interactions, where the absence of shared norms and values can result in misaligned outputs.
For example, an LLM providing evening activity recommendations without accounting for cultural context might suggest visiting a bar or consuming alcohol in a region where such activities are prohibited or socially unacceptable, leading to responses that fail to align with local norms. 
Incorporating societal norms and values into LLMs could equip them with mechanisms to interpret and dynamically adapt to human normative systems \citep{10.5555/2447556.2447672}, much like these aid alignment within human interactions \citep{10.1093/acprof:oso/9780190622046.001.0001}.

% Normative systems
\subsubsection{Instilling norms and values} 

While norms constitute context-dependent behavioral rules that individuals follow, values represent broader ideals representing overarching goals and aspirations, shaping what individuals strive for \citep{matsumoto}. As a fundamental tool of cooperative intelligence, language plays a crucial role in expressing and reinforcing both norms and values. 
These can be instilled during LLM alignment in several ways.\looseness=-1 

LLMs, trained on vast datasets, absorb a multitude of signals about norms and values during training. However, while some attention has been given to broad ethical principles like helpfulness and harmlessness, an important aspect remains underexplored: ``contextual rules''\,---\,human norms related to cultural conventions. 
These contextual rules, while not directly influencing primary optimization objectives, are often followed due to tradition, or social norms. 
Despite their indirect nature, such rules can provide valuable signals about broader societal dynamics, thereby guiding the alignment of LLMs, as discussed by \citet{hadfieldmenell2019silly} and~\citet{koster2020silly} within the broader context of AI alignment. Although efforts such as \citet{ziems-etal-2022-moral}, \citet{zhan-etal-2024-renovi}, and \citet{chiu2024culturalteaming} introduce datasets with collections of social norms, the influence of the collected norms on improving alignment in LLMs remains underexplored \citep{aakanksha-etal-2024-multilingual}.
Contextual rules could guide the style of language to align with cultural expectations. For instance, when interacting with users from diverse cultural backgrounds, LLM could account for cultural preferences by avoiding humor that might not translate well across cultures. 
However, existing models have been shown to predominantly reflect Western values, as they have been primarily trained on Western-centric data, which limits their ability to represent multi-cultural values \citep{durmus2024towards,nayak-etal-2024-benchmarking}.\looseness=-1

Human social norms and values are continuously shaped and evaluated through daily interactions with others. These interactions involve the exchange of multimodal signals, such as language, facial expressions, and gestures \citep{doi:10.1098/rstb.2013.0302}. However, when interacting with LLMs, these cues are inherently absent, creating a normative gap in communication.
Exploring multimodality for alignment\,---\,integrating non-verbal forms of communication such as visual, or auditory signals\,---\,can serve as a promising line of research to address this normative void. By incorporating multimodal interactions, models could better align with the implicit social expectations typically conveyed through non-verbal cues.\looseness=-1

% Dynamic environment
\subsubsection{Allowing for dynamic norms and values} 
Norms and values are not static objects but dynamic equilibria that evolve through ongoing social interactions \citep{doi:10.1073/pnas.1817095116}. They are continuously re-articulated and negotiated within social contexts, evolving to address new challenges and cultural shifts \citep{annurev-psych-033020-013319}. Stereotypes, as a form of social norm, are accordingly fluid, emerging and transforming over time. An example is the shifting perception of remote work. Once seen as unprofessional or less productive, it is now widely accepted in many industries. If an LLM were trained primarily on pre-COVID data, it could reinforce outdated assumptions.\looseness=-1

While model editing and continual learning have been extensively explored for updating factual knowledge in LLMs \citep{pmlr-v162-mitchell22a,pmlr-v199-prado22a}, their application for adapting to evolving societal values and norms remains underexplored. Developing approaches to enable LLMs to dynamically identify, adapt to, and mitigate emerging biases dynamically is a crucial area for future research. Notably, even factual updates pose significant challenges, as highlighted by recent work on knowledge editing \citep{cooper2024machine, hase2024fundamental}.

\subsection{Economic Alignment}
\label{sec:economic}

Economic systems rely on specialization and the division of labor, requiring coordination among groups of people to ensure efficient allocation of resources \citep{arrow1951extension}. A central challenge in modern economic theory is aligning individual actors' interests with collective objectives \citep{hadfieldmanell2019}. Welfare economics provides a complementary perspective by formalizing optimization functions for resource allocation to maximize overall system objectives under given constraints. 
Similarly, aligning LLMs with diverse human values involves navigating trade-offs between individual and collective goals. Additionally, a coherent social welfare objective function for LLMs cannot rely solely on subjective values. Instead, real-world implementations demand collective decisions about which values to prioritize \citep{arrow1951extension,18108}. Building from this, we explore strategies for integrating economic alignment frameworks to coordinate individual preferences to achieve collective, fair objectives, and facilitating group-level aggregation, offering an alternative view to imposing monolithic objective functions across diverse user groups.\looseness=-1

\subsubsection{Economic Mechanisms for Fair Alignment} 


% Pareto-efficiency
In theoretical economics, perfect markets are often posited as achieving a Pareto-efficient distribution of welfare under a utilitarian framework \citep{arrow1951extension}. Pareto efficiency refers to a state, where no individual can be made better off without making someone worse off, and is a benchmark for efficient resource allocation \citep{black2017dictionary}. As shown in \citet{boldi2024paretooptimallearningpreferenceshidden}, Pareto efficiency offers a valuable lens for balancing competing human preferences and can serve as a foundation for techniques optimizing specific notions of group fairness, ensuring inclusive and equitable LLM alignment. Achieving such efficiency would mean tailoring the model's behavior to address diverse needs equitably, ensuring no group is disproportionately advantaged or disadvantaged without justification. 
This problem has been investigated in the field of social welfare economics, where the aggregation of diverse preferences must be balanced to ensure the collective well-being of multiple groups \citep{DASPREMONT2002459}.
For LLM alignment, these objective functions can guide the development of reward systems. As shown in general RLHF, developing welfare-centric objectives can improve fairness objectives \citep{pardeshi2024learning,cousins2024welfare}.

\subsubsection{Economic Mechanisms for Pluralistic Alignment} 

% Pluralistic alignment
Decision-making often involves multiple actors with diverse and sometimes conflicting preferences. In the context of LLMs, this necessitates approaches that account for a broad range of values. Pluralistic alignment addresses this challenge by designing models that can represent and respect diverse perspectives \citep{SorensenMFGMRYJ24,tanmay2023probingmoraldevelopmentlarge}. Unlike monolithic approaches, which attempt to impose a singular objective function, pluralistic alignment embraces the complexity of modern societies.

% Social welfare functions (ordinal and cardinal)
A critical aspect of LLM alignment involves determining how to elicit and aggregate preferences when multiple humans are affected by the behavior of an artificial agent \citep{rossi2011preferences,rao-etal-2023-ethical,pmlr-v235-conitzer24a}. This challenge extends beyond individual alignment to group alignment, where many societal issues arise from collective behavior rather than isolated actions and can be addressed by incorporating multiple objectives into the alignment process, leveraging methods such as few-shot learning to capture diverse perspectives \citep{zhou-etal-2024-beyond,zhao2024group}. 

Another critical issue in enabling pluralistic values is the trade-off between developing general-purpose models and specialized models. While specialized models tailored to specific domains, such as healthcare or justice, can better align with local norms and regulatory frameworks, they risk fragmenting values. Conversely, general-purpose models may provide broader applicability but struggle to adapt to ethically complex, domain-specific requirements. Cooperative game theory offers a framework to navigate these tensions by promoting fair resource allocation, fostering collaboration among stakeholders, and ensuring equitable outcomes \citep{10.5555/2132771}.



\subsection{Contractual Alignment}
\label{sec:legal}

% Add external and internal distinction

Law-making and legal interpretation serve as mechanisms to translate opaque human goals and values into explicit, actionable directives. Legal scholars have long recognized the inherent impossibility of drafting complete contracts \citep{macneil1977contracts,williamson1975markets,shavell1980damage,maskin1999unforseen,tirole1999incomplete,aghion2011incomplete}. This limitation stems from several key challenges. First, certain states of the world are either unobservable or unverifiable, e.g., hiding assets in complex financial arrangements can be difficult for tax authorities to identify \citep{69c1e19e-b0f8-307a-abda-571627b432cb}. Second, the limited rationality of humans restricts their ability to anticipate and optimize across the entire, combinatorially large space of potential scenarios \citep{williamson1975markets}. Consequently, precisely computing optimal outcomes becomes intractable. Furthermore, the very description of all possible contingencies is often beyond human foresight, leading to loopholes in the design of rules \citep{183a5147-673b-36d2-8ddd-a7835149772a}. Even if feasible, the costs associated with drafting and enforcing fully specified contracts would likely be prohibitive. 
Given that these challenges are analogous to those encountered in aligning LLMs, where developers aim to ensure that models produce safe and correct outputs even for inputs not directly represented in training or alignment data, we investigate insights from contract theory as potential solutions for improving LLM alignment.


% Formalization of contracts for clarification of the anticipated behaviour
\subsubsection{External Contractual Alignment}
The formalization of contracts offers a framework for anticipating and specifying desired behaviors in human-LLM interactions \citep{jacovi2021formalizing}. 
In this context, standardized documentation plays a crucial role in defining and communicating the LLMs' performance characteristics. Initiatives such as datasheets \citep{10.1145/3458723}, data statements \citep{10.1162/tacl_a_00041}, model cards \citep{10.1145/3287560.3287596}, reproducibility checklists \citep{pineau2020checklist}, fairness checklists \citep{10.1145/3313831.3376445}, and factsheets \citep{8843893} exemplify efforts to create clear, standardized guidelines that could inform the development of future regulations and legal frameworks for LLM alignment and data governance.

The rules that guide LLM alignment are currently largely constructed in consultation with domain and legal experts, by adapting documents such as the UN Declaration of Human Rights \citep{anthropic2023claude}, through public input \citep{anthropic2023collective}, or in some cases, relying on designer instincts \citep{anthropic2023claude,10.5555/3540261.3540709}. 
Importantly, the European Commission has developed detailed guidelines for trustworthy AI, which provide a structured approach to ensuring that AI systems, including LLMs, adhere to ethical principles and societal norms.\footnote{The guidelines are available at \url{https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai/}.} These documents serve as critical tools for defining the terms of human-LLM contracts and offer a principled way to ensure that the view not only reflects the developer's personal views.


\subsubsection{Internal Contractual Alignment}
While the above discussion focused on aligning LLMs through external rules, another approach takes inspiration from how parties in a contract, laws, and democratic institutions enforce principles. Instead of relying solely on external oversight, this approach embeds normative principles directly within the model's internal mechanisms. Known as \emph{constitutional AI}, this method enables LLMs to develop an internalized set of ``principles'' that guide the model to self-critique and re-write the response to ensure alignment with predefined norms. By integrating desired rules into the training objectives, constitutional AI aims to instill structural governance within models, much like how legal frameworks encode societal values into enforceable policies.
These methods provide scalable oversight precisely because they move beyond the need for direct, case-by-case human intervention. Traditional preference-based training methods, such as collecting annotations on preferred and rejected outputs, aggregate multiple annotators' judgments into a shared standard, but they still require extensive human effort at scale 
\citep{shen2023largelanguagemodelalignment,amodei2016concreteproblemsaisafety}. 
In contrast, scalable oversight techniques generalize beyond individual preferences by structuring decision-making mechanisms, similar to how democratic systems use institutionalized processes to apply laws across diverse contexts \citep{shen2023largelanguagemodelalignment}. \looseness=-1

One such method, debate \citep{irving2018aisafetydebate,irving2019ai}, mirrors adversarial legal reasoning: agents (i.e., LLMs) propose answers, engage in structured argumentation, and refine their positions, with a human judge selecting the best-supported response \citep{HAFNER20018675}. 
Similarly, constitutional AI guides LLMs using a concise constitution of high-level principles (e.g., promoting fairness or avoiding harm) \citep{bai2022constitutionalaiharmlessnessai,10.5555/3666122.3666237}. This constitution provides the basis for generating synthetic comparison examples, which are then used to fine-tune the LLM's policy. While primarily developed for integrating human values, these methods have the potential to enforce norms and regulations in a structured manner, drawing parallels to how societal governance mechanisms uphold laws and ethical standards.\looseness=-1




