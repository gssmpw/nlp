\section{Societal Alignment Frameworks and their View on Uncertainty}
\label{sec:uncertainty}


% Uncertainty coming from incomplete contracting (already above)
%The incomplete contracts in LLM alignment present a source of uncertainty in these models. 
%However, u
By framing LLM alignment as a problem of contractual incompleteness and analyzing it through the lens of societal alignment frameworks, we observe that these frameworks recognize establishing contracts, much like alignment, as inherently uncertain \citep{seita1984uncertainty}. 
In the following, we examine uncertainty in the specific case of LLM alignment through the lens of societal alignment frameworks. First, we analyze the sources of unwanted uncertainty in LLM alignment (\Cref{sec:uncertainty-alignment}). Next, we explore types of uncertainty that are essential for ethical alignment (\Cref{sec:uncertainty-values}). Finally, we highlight the need for reliable uncertainty communication in LLM alignment (\Cref{sec:uncertainty-communication}).


% Uncertainty of LLMs 
\subsection{Unwanted Uncertainty in LLM Alignment} 
\label{sec:uncertainty-alignment}

Prior research has identified epistemic uncertainty as one of the main challenges in LLM development \citep{shorinwa2024surveyuncertaintyquantificationlarge}. This form of uncertainty arises from gaps in the model's knowledge, leading to uncertainty about factual information \citep{shorinwa2024surveyuncertaintyquantificationlarge,jiang-etal-2021-know,yadkori2024believebelievellm}. Even aligned models remain susceptible to epistemic uncertainty, often failing to recognize their own knowledge limitations. For example, \citet{shorinwa2024surveyuncertaintyquantificationlarge} illustrate how an LLM confidently responded to the question, ``What is the lowest-ever temperature recorded in Antarctica?'' with incorrect information ($-\text{128.6}^\circ$F/$-\text{89.2}^\circ$C instead of the actual record of $-\text{135.8}^\circ$F/$-\text{94.7}^\circ$C), claiming 100\% confidence despite factual inaccuracy. This inability to calibrate confidence scores to actual knowledge reflects a fundamental limitation of current LLM architectures.\looseness=-1 
 

However, uncertainty in aligned LLMs presents additional complexity. The conversational nature of LLMs often creates an illusion of omniscience, making it difficult for users to discern the model's uncertainty \citep{delacroix2024lost}. 
Furthermore, human interaction with models, combined with their in-context learning capabilities \citep{NEURIPS2020_1457c0d6}, allows users to provide task-specific context that can inadvertently bypass safety guardrails and mitigations implemented during training. As highlighted by \citet{glukhov2024breachthousandleaksunsafe}, this can lead to models leaking unsafe information or performing harmful actions despite their intended safeguards.


% Uncertainty and Bias

\subsection{Uncertainty Needed in LLM Alignment} 
\label{sec:uncertainty-values}

While the unwanted epistemic uncertainty can undermine the reliability of language models, certain types of uncertainty are not only unavoidable but essential for their ethical deployment \citep{delacroix2024lost}. In the context of LLMs, this essential uncertainty can arise from evolving human values, conflicting societal norms, and the difficulty of translating abstract principles into model behavior. \looseness=-1

Aligning models to navigate trade-offs, such as between helpfulness and harmlessness or accuracy and fairness, requires addressing conflicting and often underspecified priorities, which introduces another source of uncertainty \citep{zollo2024prompt, yaghini2023learning}. For instance, when deploying an LLM, we often want to maximize performance subject to some constraints or guardrails on behavior, e.g., a chatbot should give users their desired output, as long as it is not too toxic. The effectiveness of balancing these conflicting priorities and the unintended consequences are often difficult to predict. However, this balancing act is also essential because it allows models to operate within complex, context-dependent environments where rigid adherence to a single objective could lead to harmful outcomes. 


% Communicating Uncertainty
\subsection{Uncertainty Communication} 
\label{sec:uncertainty-communication}

Building on the above, the inherent uncertainty in LLM alignment is not a weakness but often a valuable feature that enables models to handle complex scenarios ethically \citep{delacroix2024lost}. In fact, as highlighted by \citet{10.1145/3461702.3462571}, uncertainty communication can be useful for obtaining fairer models by revealing data biases, improving decision-making by guiding reliance on predictions, and building trust in automated systems.
Therefore, it is essential to develop methods for communicating uncertainty to users. Unlike humans, however, LLMs lack the non-verbal and contextual cues that naturally support communication \citep{Bisconti2021}. Existing research has shown that LLMs struggle to convey their uncertainty to users, both implicitly (e.g., hedging language) and explicitly (e.g., confidence scores), a skill that humans possess intuitively \citep{Alkaissi2023ArtificialHI, liu2024trustworthyllmssurveyguideline,shorinwa2024surveyuncertaintyquantificationlarge}. 
On the other hand, humans themselves have varying levels of understanding regarding probability and statistics, which are needed to interpret model uncertainty estimates \citep{10.1145/3461702.3462571,10.1001/archinternmed.2009.481}. Furthermore, human cognition is subject to biases that can impede accurate interpretation of uncertainty \citep{Kahneman,REYNA200889}. These challenges can be partially addressed by choosing the appropriate communication methods, a key consideration for the design of effective user interfaces \citep{8457476}, and by designing collaborative interaction environments, as discussed by \citet{Montemayor2021}.

