\section{Alternative View: The Democratic Opportunity Inherent in the Under-specified Nature of LLMs' Objectives}
\label{sec:alternative}

The challenge of aligning LLMs is often framed as a technical problem, one that can be solved through better reward modeling, training objectives, or oversight mechanisms. However, alignment is not merely a technological issue. It is fundamentally a societal one.

To understand the significance of this alternative view, one needs to take a step back and start from the following: we humans are constantly in the process of finding our way around the world. Part of that process involves imagining better ways of living together. We may find some of our practices to be inadequate, for instance, but may not always be able to articulate why. In such cases, we often resort to conversations, to refine our intuitions and distill their underlying structures. These evolving dialogues shape and refine our moral and social expectations, which, in turn, influence the values that guide our decision-making. The fact that these values change and often clash, is a good sign---a sign of ongoing critical engagement and willingness to question existing norms. 

Now consider a team of engineers considering how to design AI tools that will be deployed within contexts such as education, healthcare, or justice practices. Some of these tools, like LLMs, can be used as conversational partners. The feedback given as a context can be leveraged to refine LLMs' behavior. Given the inherently dynamic nature of the values that inform education, healthcare, or justice practices, as we previously discussed, the key problem is to establish how to structure this feedback process. Different groups of users will evolve different values over time. Are there ways of incentivizing collective, critical engagement with LLMs? Can bottom-up, iterative refinements be configured to support users' in defining the very values that preside over their practices \citep{delacroix2024lost}?

The contrast between the above and a characterization of the AI alignment problem as `fundamentally a challenge of incomplete contracting' is significant. The contract metaphor, as discussed by \citet{goldoni2018}, oversimplifies complex systems by framing alignment as a straightforward agreement between stakeholders, neglecting the broader socio-political forces, conflicting norms, and inherent tensions that shape such systems. This technology-centric framing risks oversimplifying the dynamic and pluralistic nature of alignment challenges. While societal alignment frameworks aim to address these issues, they too often rely on oversimplified assumptions. Beyond the issues with the contract metaphor, the focus on incompleteness (i.e., information asymmetries between the principal/agent) frames alignment as an epistemic designer-centric issue, rather than recognizing it first and foremost as a political question \citep{terzis2024}.
%Aside from the problematic contract metaphor \citep{goldoni2018}, the focus on information asymmetries gives a designer-centric, epistemic spin to what is first and foremost a political question \citep{terzis2024}. 
Given LLMs' unavoidable, normative effect on the practices within which they are deployed, the under-specified nature of LLMs' objectives presents an opportunity\,---\,not to perfect our specification methods, but to democratize the very process of determining what LLMs should optimize for.

The implications of this reframing extend to both research and practice. It suggests that alongside technical work such as reward modeling, we need equally sophisticated work on participatory interface designs. This dual focus acknowledges that effective participation requires not just theoretical frameworks for inclusion, but also concrete mechanisms through which diverse stakeholders can meaningfully shape LLM development \citep{kirk2024}. This might include developing new methodologies for collective value articulation \citep{Bergman2024}, creating institutional structures for meaningful public participation in LLM development, and establishing mechanisms for ongoing societal oversight and input into LLMs' objectives and constraints.

%The view of incomplete contracting as an inherent challenge of LLM alignment that we have so far presented in this paper takes inspiration from economic theory. In this perspective, the model developer is the responsible agent navigating the constraints of an incomplete contract and uncertainty. The developer's task is to anticipate diverse moral contexts and enable systems to adapt to them while demonstrating accountability for the outputs generated by the model. 


