\section{Introduction}

% LLM improvements due to alignment
As large language models (LLMs) advance to unprecedented levels of proficiency in generating human-like language, aligning their behavior with human values has become a critical challenge to ensuring their usability in real-world applications \citep{leike2018scalableagentalignmentreward,gabriel2020artificial,NEURIPS2022_b1efde53,shen2023largelanguagemodelalignment}.
% Briefly how alignment is done for LLMs
This alignment encompasses both \textit{explicit} values, such as following instructions and being helpful, and \textit{implicit} values, such as remaining truthful and avoiding biased or otherwise harmful outputs \citep{askell2021generallanguageassistantlaboratory}.
In fact, the rise of LLM-based chat assistants has largely been driven by their ability to follow instructions and engage in open-ended dialogue, demonstrating the importance of alignment, enabled by algorithms such as reinforcement learning from human feedback (RLHF; \citealt{NEURIPS2022_b1efde53,ziegler2020finetuninglanguagemodelshuman}).

% Why alignment is hard
Despite these advancements, aligning LLMs with human values remains a formidable challenge \citep{wei2023jailbroken,williams2024targetedmanipulationdeceptionoptimizing,greenblatt2024alignmentfakinglargelanguage}. 
This difficulty primarily stems from the fundamental gap between the intricacies of human values and the often narrow technological solutions \citep{hadfieldmanell2019}. 
Current LLM alignment methods, such as RLHF, often result in misspecified alignment objectives, where reward functions reflect human values only within designer (or annotators) provided scenarios, a finite set among an infinite set of values, failing to generalize in unforeseen contexts \citep{amodei2016concreteproblemsaisafety,hadfieldmanell2019,10.1145/3375627.3375851,NEURIPS2021_c26820b8,10.5555/3600270.3600957}. While developers acknowledge the problem of misspecification \citep{leike2018scalableagentalignmentreward, shen2023largelanguagemodelalignment,NEURIPS2022_b1efde53}, the root causes of this issue have been largely overlooked.


\begin{figure*}[ht]
    \centering
    \includegraphics[trim={0 12.5cm 0 0},clip,width=\textwidth]{figures/ICML_Societal_Frameworks_Fig}
    \caption{We view human-LLM interactions as a \textit{principal}-\textit{agent} framework, where a \textit{principal} (a system designer) incentivizes an \textit{agent} (an LLM) to take an action $a$ by offering a reward $r$. This framework assumes that the agent's action is driven by its reward function, forming a pair $(a,r)$ that serves as a \emph{contract} between the agent and the principal. However, this contract is incomplete. To address this incompleteness, we explore societal alignment mechanisms of social, economic, and contractual alignment as guiding principles for LLM alignment in the incomplete contracting environment.}
    \label{fig:intro}
\end{figure*}


% Alignment as an (incomplete contract)
To better understand this misalignment, we frame LLM alignment within a \emph{principal-agent}\footnote{We use `agent' in the contract theory sense, referring to an entity acting on behalf of a principal, rather than the broader AI notion of autonomous systems.} framework \citep{f376b2cf-a8b4-3622-927b-3396b646b772}, a well-established paradigm in economic theory. As shown in \Cref{fig:intro}, in this framework, the LLM acts as the agent and the model developer (or user) serves as the principal. We define a \emph{contract} as a pair: an action taken by the agent and the corresponding reward assigned by the principal. For example, a contract in LLM training could reward the model for generating responses that follow factual accuracy constraints while penalizing hallucinated outputs. The principal is able to steer the agent's behavior toward intended objectives with an appropriate reward. In an ideal scenario, a complete contract would perfectly align the agent's actions with the principal's objectives in all possible states of the world.\looseness=-1

However, designing a fully specified contract that anticipates every possible scenario in model training is infeasible \citep{hadfieldmanell2019,zhuang2020consequences}. In LLM alignment, this challenge is reflected in the reward function, which is derived from explicitly elicited values or implicitly implied values in the form of human preferences.
Yet, quantifying complex and often diverging human values is difficult \citep{leike2018scalableagentalignmentreward,feffer2023moralmachinetyrannymajority}, and capturing them effectively incurs high annotation costs \citep{klingefjord2024humanvaluesalignai}. Aggregating these values into a unified reward signal is nontrivial \citep{Kemmer_Yoo_Escobedo_Maciejewski_2020,ilvento:LIPIcs.FORC.2020.2}.

% Turning to societal frameworks
 
These alignment challenges are not unique to LLMs. In fact, they echo broader alignment problems that humans encounter daily due to incomplete contracts.
Institutions such as society, economy, and law enable us to thrive despite incompleteness.
In this position piece, \textbf{we advocate for leveraging insights from societal alignment frameworks to guide the development of LLM alignment within incomplete contracting environments}. 
Drawing on principles from social alignment (\Cref{sec:social}), economic alignment (\Cref{sec:economic}), and contractual alignment (\Cref{sec:legal}), we propose solutions to guide behavior in incomplete contracting environments, much like they have for human societies (see \Cref{fig:intro}). 
However, even within these frameworks, uncertainty remains an inherent factor in incomplete contracting environments \citep{seita1984uncertainty}. In \Cref{sec:uncertainty}, we examine how uncertainty manifests in LLM alignment. For instance, an LLM analyzing a patient's symptoms to suggest a diagnosis might lack access to the patient's full medical history or contextual background. 
In such cases, the model must navigate uncertainty to avoid overwhelming the user with complex, unfiltered medical information, which could lead to confusion or misinterpretation.
Finally, we offer an alternative view on LLM alignment (\Cref{sec:alternative}), framing its under-specified objectives as an opportunity rather than a flaw that can be resolved solely through technological solutions. Accordingly, we discuss the need for participatory alignment interface designs that actively engage diverse stakeholders in LLM alignment.



%