\section{Performance Guarantees}
We first focus on \Cref{alg:general-exp-kl-ms} with $L(x) = x-1$, which we abbreviate as \expklms.
For ease of presentation, we assume in the following that arm 1 is the optimal arm.
Based on the main conclusion in \Cref{thm:expected-regret-total}, we can conclude that \Cref{alg:general-exp-kl-ms}, \expklms satisfies a logarithmic minimax ratio (\Cref{corol:exp-kl-ms-mo}), an Asymptotic Optimality (\Cref{corol:exp-kl-ms-ao}), Sub-UCB criterion (\Cref{corol:exp-kl-ms-sub-ucb}) and adaptive variance ratio (\Cref{corol:exp-kl-ms-adaptive-variance}).

\begin{restatable}{theorem}{mainregret} \label{thm:expected-regret-total}
    For any $K$-arm bandit problem with \Cref{assum:oped,assum:reward-dist}, \expklms (\Cref{alg:general-exp-kl-ms}) has regret bounded as follows. 
    For any $\Delta > 0$ and $c \in (0, \frac{1}{4}]$: 
    \begin{align}
        &\Regret(T) \leq T\Delta + \textstyle \sum_{a\in[K]:\Delta_a > \Delta} \Delta_a \cdot \del{\frac{\ln\del{T\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2} \vee e}}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}}} +
            \nonumber
        \\
        &
        \textstyle \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\GoodEventBound + \ExpFTwo} +
            \nonumber
        \\
        &
        \textstyle \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\BadEventThreeBoundForAO} \wedge
        \del{\BadEventThreeBoundForMO}
        \label{eqn:exp-kl-ms-main-regret}
    \end{align}
\end{restatable}

The upper bound of the regret consists of four terms. The first term, $T\Delta$, accounts for cases where arms $a$ with $\Delta_a$ smaller than $\Delta$ are pulled. The remaining terms bound arms with larger $\Delta_a$. The second term is the most significant, as it plays a key role in asymptotic and finite-time analyses. 
As $T \to \infty$ and $\Delta \to 0$, dividing both sides of the regret bound by $\ln(T)$ reveals that only the second term contributes to the regret upper bound. The third and fourth terms are lower-order compared to the second term.
Notably, the last term takes a minimum of two terms. The first one is used to prove A.O. because it is independent of $T$. In the minimax optimality analysis, we use the second part since it has a lower order of $\Delta_a$ in the denominator.

From \Cref{thm:expected-regret-total} with \Cref{lemma:lip-exp-KL-lower-bound}, a KL-lower bound lemma, the following two corollaries can be derived immediately.
\begin{corollary}[Logarithmic Minimax Ratio]\label{corol:exp-kl-ms-mo}
    For any $K$-arm bandit problem with \Cref{assum:oped,assum:max-variance,assum:reward-dist}, \expklms has:
    $
        \Regret(T) \leq \iupbound{  
        \sqrt{ V_{\max} KT\ln(K)} + K \ln(T)}
    $.
\end{corollary}
\begin{corollary}[Asymptotic Optimality] \label{corol:exp-kl-ms-ao}
    For any $K$-arm bandit problem with \Cref{assum:oped,assum:max-variance,assum:reward-dist}, \expklms satisfies that:
    $
        \limsup_{T \rightarrow \infty} \fr{ \Regret(T) }{\ln(T)}
        =
        \sum_{a\in[K]:\Delta_a>0}
        \frac{\Delta_a}{\KL{\mu_a}{\mu_1}}
    $
\end{corollary}
\begin{corollary}[Sub-UCB] \label{corol:exp-kl-ms-sub-ucb}
    For any $K$-arm bandit problem with \Cref{assum:oped,assum:max-variance,assum:reward-dist}, \expklms satisfies that  $\Regret(T) \leq \iupbound{\sum_{a: \Delta_a > 0} \frac{V_{\max}\ln(T)}{\Delta_a} + \Delta_a}$.
\end{corollary}

\Cref{corol:exp-kl-ms-mo} achieves a logarithmic factor in the minimax ratio.
\Cref{corol:exp-kl-ms-ao} shows that \expklms satisfies the A.O. thus the long-term performance of \expklms is guaranteed to be optimal.
\Cref{corol:exp-kl-ms-sub-ucb} demonstrates that \expklms satisfies the Sub-UCB criterion and will never perform worse than the logarithmic regret bound achieved by UCB algorithms in the finite-time regime.
The choice that $L(k) = k-1$ in \expklms is crucial in that this ensures that the upper bound on the RHS of \Cref{eqn:exp-kl-ms-main-regret} does not deteriorate, resulting in violating A.O. or achieving a larger minimax ratio.
We also present results for other choices of $L(k)$ in \Cref{sec:extensions}.

To prove the above three Corollaries, we need a lemma to lower bound KL divergence and then split the arm with suboptimal gap $\Delta_a$ by $\Delta:= \sqrt{\tfrac{V_{\max}K}{T}}$.
Then, we can bound each term by $\iupbound{\sqrt{KT \ln(K)}}$. 
The proofs of \Cref{corol:exp-kl-ms-ao,corol:exp-kl-ms-sub-ucb} follow the same way that applies the lower bound lemma of KL divergence, but we set $\Delta$ to $0$ this time. 
Since we are primarily concerned with the asymptotic behavior, we show that as $T \to \infty$, the second term becomes $\sum_{a\in[K]: \Delta_a > 0} \frac{\Delta_a}{\KL{\mu_a}{\mu_1}}$, while all other terms vanish.
For Sub-UCB, we only need to rearrange terms after applying the lower bound lemma (\Cref{lemma:lip-exp-KL-lower-bound}) and we will get the result.

We now introduce a key assumption that allows us to derive the adaptive variance ratio property of \expklms.
\begin{assum} \label{assum:lip}
    For any bandit instance from a know $\Fcal_m$, $\mu_{\min} := \min_{a \in [K]} \mu_a$ is the minimum mean value of the bandit instance and $\mu_{\max}:= \max_{a \in [K]} \mu_a$ is the maximum. Variance function $V(\cdot)$ satisfies the Lipschitzness property over $[\mu_{\min}, \mu_{\max}]$. In other words, there exists $C_L > 0$ such that $\forall \mu, \mu' \in [\mu_{\min}, \mu_{\max}]$, $\abs{V(\mu) - V(\mu')} \leq C_L\abs{ \mu - \mu' }$.
\end{assum}


\Cref{assum:lip} covers a large set of OPED families as we mentioned in \tableref{tab:variance-function}, including Bernoulli, Poisson, Normal with fixed variance, Gamma with fixed shape parameter $k$, and Inverse Gaussian distributions. 
For instance, Gamma distribution with fixed shape parameter $k$ has mean $k\theta$ and variance $k\theta^2$, so its variance function is $V(\mu) = \mu^2/k$. Within the interval $[\mu_{\min}, \mu_{\max}]$, $V(x)$ satisfies \Cref{assum:lip} with Lipschitz constant $2\mu_{\max}/k$.
In the refined version of \Cref{thm:expected-regret-total}, we can replace $V_{\max}$ by $V(\mu_1)$ and show \expklms has adaptive variance ratio in \Cref{corol:exp-kl-ms-mo}.

\begin{corollary}[Adaptive Variance Ratio]\label{corol:exp-kl-ms-adaptive-variance}
    For any $K$-arm bandit problem with \Cref{assum:oped,assum:reward-dist,assum:lip},
    \expklms has:
    $
        \Regret(T) \leq \iupbound{\sqrt{V(\mu_1) KT\ln(K)} + K \ln(T)}.
    $
\end{corollary}





A key contribution of our work is showing that \expklms satisfies both a logarithmic minimax ratio and an adaptive variance ratio. 
As demonstrated in \Cref{corol:exp-kl-ms-adaptive-variance}, the leading term contains only a factor of $\sqrt{V(\mu_1)}$ and $\sqrt{\ln(K)}$ compared to the $\Omega(\sqrt{KT})$ lower bound.
