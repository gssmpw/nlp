\section{Related Work}
\label{sec:related}

\paragraph{Exponential Family Bandit Algorithms and simultaneous adaptivity and optimality guarantees} Several bandit algorithms in the literature work in the setting of OPED reward distributionsauer, "Exponential Families for Locally Private Mechanisms". 
Thompson Sampling 
(TS)Kaufmann et al., "Thompson Sampling for Contextual Bandits", and kl-UCCBubeck et al., "Regret analysis of stochastic and nonstochastic multi-armed bandit algorithms" were among the first to use posterior sampling and optimism strategy, respectively. 
These two methods have been shown to satisfy A.O. in their original analyses. 
Based on the analysis inAuer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem", we can find that KL-UCB achieves a logarithmic minimax ratio, which has been mentioned inLai and Robbins, "Asymptotically Efficient Adaptive Allocation Rules" as well.


More recently, Kaufmann et al., "Thompson Sampling for Contextual Bandits" proposes kl-UCB++ and demonstrates that it satisfies M.O., but it fails to achieve an adaptive variance ratio. 
Auer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem" proposes ExpTS and shows that it also achieves a logarithmic minimax ratio as $\sqrt{\ln(K)}$, and ExpTS$^+$ satisfies M.O., but both methods lack an adaptive variance ratio since they assume their variance is capped by $V_{\max}$. 
A more detailed comparison is deferred to \tableref{tab:comparison}.


\paragraph{Asymptotic Optimality} Auer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem" studied sequential allocation strategies and provided an asymptotic analysis. This foundational work on OPED families of reward distributions inspired the development of optimism strategy policies incorporating Kullback-Leibler (KL) divergence.
Several studies have proposed algorithms with A.O. guarantees. Notable examples include TS with conjugate priorsKaufmann et al., "Thompson Sampling for Contextual Bandits", ExpTSKaufmann et al., "Thompson Sampling for Contextual Bandits", which leverages non-conjugate priors, and KL-UCBKL-UCB++ achieves a logarithmic minimax ratio of $\sqrt{\ln(T)}$. Another widely studied setting is the sub-Gaussian case, where all arms' reward distributions satisfy the sub-Gaussian property. Algorithms such as Auer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem" and MS/MS$^+$Auer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem" have demonstrated A.O. guarantees under this sub-Gaussian assumption.


\paragraph{Minimax Optimality} Minimax optimality assesses the performance of a bandit algorithm under the worst-case scenario. Specific bandit algorithms that use the self-defined sampling distribution achieve a minimax ratio of $\sqrt{\ln(K)}$Auer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem". In contrast, numerous upper confidence-bound strategies, but not all, achieve a minimax ratio of $\sqrt{\ln(T)}$Kaufmann et al., "Thompson Sampling for Contextual Bandits". MOSSGarivier and Cappe, "Optimism in the face of uncertainty: A new approach to bandit problems" is the first algorithm to achieve M.O., albeit requiring a $[0, 1]$ reward environment. KL-UCB++KL-UCB achieves a minimax ratio of $\iupbound{1}$ assuming an OPED reward distribution. 


\paragraph{Sub-UCB Criterion}
As mentioned in the Introduction, A.O. and M.O. provide only a limited perspective on algorithm performance, focusing on asymptotic instance-dependent and worst-case scenarios. Sub-UCB was first introduced inAuer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem", where the authors conducted a comprehensive review of the literature and demonstrated that algorithms such as MOSSGarivier and Cappe, "Optimism in the face of uncertainty: A new approach to bandit problems"MOSS-AnytimeGarivier and Cappe, "Upper confidence bounds for regret minimization", and KL-UCB++KL-UCB fail to satisfy Sub-UCB, regardless of their minimax optimality.
More recently, Auer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem" MS$^+$MS/MS$^+$ shows that it is possible to achieve both Sub-UCB and M.O. simultaneously when all arms' reward distributions are supported on $[0,1]$.

\paragraph{Adaptive Variance Ratio}
In the literature, we have observed that the maximum variance assumption has been used in some worksKaufmann et al., "Thompson Sampling for Contextual Bandits" to derive a finite-time instance-independent regret bound, resulting in a $V_{\max}$ variance ratio.
However, the Adaptive Variance Ratio guarantee is coarse in that it does not account for the variances of all arms. Garivier and Cappe, "Optimism in the face of uncertainty: A new approach to bandit problems" proves that KL-MSKL-UCB++ satisfies an adaptive variance ratio that uses the instance parameter $V(\mu_1)$, which is the variance of the best arm and $V(\mu_1)$ is overall better than $V_{\max}$.
Kaufmann et al., "Thompson Sampling for Contextual Bandits" also points out that with a refined regret analysis, KL-UCB++KL-UCB and UCB-VGarivier and Munos, "Upper Confidence Bounds for Regret Minimization in Finite Horizon" can also achieve adaptive variance regret.