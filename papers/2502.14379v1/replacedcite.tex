\section{Related Work}
\label{sec:related}

\paragraph{Exponential Family Bandit Algorithms and simultaneous adaptivity and optimality guarantees} Several bandit algorithms in the literature work in the setting of OPED reward distributions____. 
Thompson Sampling 
(TS)____ and kl-UCB____ were among the first to use posterior sampling and optimism strategy, respectively. 
These two methods have been shown to satisfy A.O. in their original analyses. 
Based on the analysis in____, we can find that KL-UCB achieves a logarithmic minimax ratio, which has been mentioned in____ as well.


More recently, ____ proposes kl-UCB++ and demonstrates that it satisfies M.O., but it fails to achieve an adaptive variance ratio. 
____ proposes ExpTS and shows that it also achieves a logarithmic minimax ratio as $\sqrt{\ln(K)}$, and ExpTS$^+$ satisfies M.O., but both methods lack an adaptive variance ratio since they assume their variance is capped by $V_{\max}$. 
A more detailed comparison is deferred to \tableref{tab:comparison}.


\paragraph{Asymptotic Optimality} ____ studied sequential allocation strategies and provided an asymptotic analysis. This foundational work on OPED families of reward distributions inspired the development of optimism strategy policies incorporating Kullback-Leibler (KL) divergence.
Several studies have proposed algorithms with A.O. guarantees. Notable examples include TS with conjugate priors____, ExpTS____, which leverages non-conjugate priors, and KL-UCB____. Another widely studied setting is the sub-Gaussian case, where all arms' reward distributions satisfy the sub-Gaussian property. Algorithms such as AOUCB____ and MS/MS$^+$____ have demonstrated A.O. guarantees under this sub-Gaussian assumption.


\paragraph{Minimax Optimality} Minimax optimality assesses the performance of a bandit algorithm under the worst-case scenario. Specific bandit algorithms that use the self-defined sampling distribution achieve a minimax ratio of $\sqrt{\ln(K)}$____. In contrast, numerous upper confidence-bound strategies, but not all, achieve a minimax ratio of $\sqrt{\ln(T)}$____. 
____ shows that TS with a Beta prior can reach a minimax ratio of $\sqrt{\ln(T)}$, and when the reward distributions are Gaussian, the minimax ratio becomes $\sqrt{\ln(K)}$. MOSS____ is the first algorithm to achieve M.O., albeit requiring a $[0, 1]$ reward environment. KL-UCB++____ achieves a minimax ratio of $\iupbound{1}$ assuming an OPED reward distribution. 


\paragraph{Sub-UCB Criterion}
As mentioned in the Introduction, A.O. and M.O. provide only a limited perspective on algorithm performance, focusing on asymptotic instance-dependent and worst-case scenarios. Sub-UCB was first introduced in____, where the authors conducted a comprehensive review of the literature and demonstrated that algorithms such as MOSS____, MOSS-Anytime____, and KL-UCB++____ fail to satisfy Sub-UCB, regardless of their minimax optimality.
More recently, MS$^+$____ and KL-MS____ have shown that it is possible to achieve both Sub-UCB and M.O. simultaneously when all arms' reward distributions are supported on $[0,1]$.

\paragraph{Adaptive Variance Ratio}
In the literature, we have observed that the maximum variance assumption has been used in some works____ to derive a finite-time instance-independent regret bound, resulting in a $V_{\max}$ variance ratio.
However, the Adaptive Variance Ratio guarantee is coarse in that it does not account for the variances of all arms. ____ proves that KL-MS satisfies an adaptive variance ratio that uses the instance parameter $V(\mu_1)$, which is the variance of the best arm and $V(\mu_1)$ is overall better than $V_{\max}$.
____ also points out that with a refined regret analysis, KL-UCB++____ and UCB-V____ can also achieve adaptive variance regret.