\section{Introduction}

The Multi-Armed Bandit (MAB) problem models sequential decision making in which an agent repeatedly takes action, receives a reward from the environment, and would like to learn to maximize its cumulative reward.
It has attracted significant attention within the research community due to its foundational nature. Moreover, it has found practical applications in various domains, including online advertising~\citep{geng2021comparison} and clinical trials~\citep{villar2015multi}.

Formally, an MAB environment consists of $K$ arms (actions), each denoted by an integer $a \in [K]:= \cbr{1, \dots, K}$. 
Each arm is associated with a reward distribution $\nu_a$ with mean $\mu_a$. The learning agent selects an arm $I_t \in [K]$ at each time step $t$, which is the index of the arm being pulled at time $t$, and receives a reward $r_t \sim \nu_{I_t}$ from the environment.

In many environments, the reward from all arms comes from an One-Parameter Exponential Distribution (OPED) family, for example, Bernoulli distribution reward~\citep{bouneffouf2017bandit,shen2015portfolio} if it is a binary outcome or Gaussian distribution reward~\citep{jin2021mots}. OPED families are classes of probability distributions characterized by their capacity to express the likelihood of a set of outcomes in terms of a natural parameter. OPED families setting allows for flexible modeling of various types of data.
An OPED family with identity sufficient statistic induced by base measure $m$ is defined as:
\begin{align} \label{eqn:oped}
    \Fcal_m = \cbr{ p_\theta(dx) = m( dx ) \expto{ x \theta - b(\theta) }: \theta \in \Theta  },
\end{align}
where $\theta$ is the natural parameter, $b(\theta) := \ln\del{\int_\RR \expto{ x \theta } m(dx)}$ is the log-partition function, $\Theta \subset \RR$ is the space of canonical parameters. Throughout the paper, we assume that the reward distributions of all arms belong to a common $\Fcal_m$.

At each time step, the learning agent makes a decision based on the information collected in history and faces the exploration-exploitation trade-off.
It can do more exploration to pull the arms that do not perform as well as expected to gain a better estimation or do more exploitation on the good-performed arm but take a risk on a bad estimation.

The agent's primary goal is to maximize its cumulative reward over time, which is equivalent to minimizing its regret against regret to choose the optimal arm.
We denote the maximum expected reward for the environment by $\mu_{\max}:= \max_{a\in[K]} \mu_a$ define the pseudo-regret as
\begin{align*}
    \Regret(T):= \textstyle\sum_{t=1}^T \mu_{\max} - \EE[r_t] = \sum_{t=1}^T \mu_{\max} - \mu_{I_t}.
\end{align*}

Despite extensive research in MAB with OPED rewards, designing algorithms that simultaneously achieve optimality and adaptivity remains challenging.
Specifically, we would like to design algorithms with the following guarantees:

\paragraph{Fully instance-independent regret guarantee.} 
    Fully instance-independent regret guarantees provide uniform performance bounds across all bandit instances. 
    Minimax regret is an essential metric for this. 
    According to~\citet{auer03nonstochastic}, in the Bernoulli reward setting, for any bandit algorithm, there exists a bandit instance that incurs a regret of at least $\ilowbound{\sqrt{KT}}$.
    Thus, the minimax regret is expected to be at least $\ilowbound{\sqrt{KT}}$ for many OPED settings.
    Conversely, \citet{audibert09minimax} shows that for reward distributions supported on $[0,1]$, the MOSS algorithm can achieve a regret of at most $\iupbound{\sqrt{KT}}$.
    Therefore, an algorithm is said to be \textbf{minimax optimal (M.O.)} if its regret complies with $\Regret(T) = \ieqorder{\sqrt{KT}}$, and it has a minimax ratio (M.R.) if there exists a function $f(K, T)$ such that $\Regret(T) = \iupbound{\sqrt{KT}f(K, T)}$.
\paragraph{Fully Instance-dependent regret guarantee.} Instance-dependent regret guarantees provide bounds that adapt to the specific difficulty of each problem instance.
It encompasses two primary criteria: \textbf{asymptotic optimality (A.O.)}~\citep{lai85asymptotically} and \textbf{Sub-UCB}~\citep{lattimore18refining}. 
\citet{lai85asymptotically} shows that for any consistent bandit algorithm, there exists a bandit instance where the regret is lower-bounded, as shown in the following equation.
    \begin{align*}
        \limsup_{T\to\infty} \frac{\Regret(T)}{\ln(T)} \geq \textstyle\sum_{a\in[K]:\Delta_a > 0} \frac{\Delta_a}{\Dcal(\nu_a, \nu_{\max})}
    \end{align*}
    where $\Delta_a := \mu_{\max} - \mu_a$ is the suboptimality gap of arm $a$. $\nu_a, \nu_{\max}$ are reward distributions associated with arm $a$ and the best arm.
    $\Dcal(\nu, \nu')$ is the Kullback-Leibler (KL) divergence between two distributions $\nu$ and $\nu'$.
    Therefore, an algorithm is said to satisfy asymptotic optimality if its regret is upper bounded by $ \sum_{a\in[K]: \Delta_a > 0} \idel{\frac{1}{\Dcal(\nu_a, \nu_{\max})}+o(1)} \ln(T)$.
    Such an algorithm effectively learns the bandit environment and minimizes regret as the time steps $T$ progress towards infinity, aligning its regret with the theoretical minimum.

    Before \citet{lattimore18refining}, the literature primarily focused on A.O. and M.O. since they describe algorithm performance in instance-independent and instance-dependent scenarios.
    An algorithm that satisfies these metrics is generally considered superior in performance.
    However, while MOSS~\citep{audibert2009minimax} is optimal concerning both A.O. and M.O., \citet{lattimore2018refining} demonstrated that, in certain bandit instances, MOSS falls short in comparison with the simpler UCB algorithm in finite time regime. This suggests that traditional measures of optimality (A.O. and M.O.) do not fully capture the complete performance spectrum of a bandit algorithm.
    To address this gap, \citet{lattimore2018refining} introduced the Sub-UCB criterion as a complement to A.O. and M.O., aiming to evaluate whether an algorithm can match the performance of the UCB algorithm in finite-time regimes.
    An algorithm is said to satisfy the Sub-UCB criterion if there exist two constants $C_1$ and $C_2$ such that
    \begin{align*}
        \Regret(T) \leq C_1 \textstyle\sum_{a\in[K]} \Delta_a + C_2  \textstyle\sum_{a\in[K]:\Delta_a > 0} \frac{\ln(T)}{\Delta_a}
    \end{align*}
    This criterion is called "Sub-UCB" because this is a standard form of gap-dependent regret guarantee of UCB~\citep{auer02finite,lattimore18refining}.

\paragraph{Partially instance-independent guarantee.} Partially instance-independent guarantee is a middle ground between fully instance-independent and instance-dependent regret guarantees.
    One such guarantee studied in prior works is the adaptive variance ratio~\citep{qin2023kullback}.
    An algorithm $\Acal$ is said to achieve an adaptive variance ratio if the regret of the algorithm can be bounded by
    \begin{align*}
        \Regret(T) \leq \upboundlog{\sqrt{V(\mu_{\max}) KT}}
    \end{align*}
    where $V(\mu)$ is the variance of the reward distribution in $\Fcal_m$ with mean parameter $\mu$; i.e., 
    $V(\mu_{\max}) = \Var_{r \sim \nu_{\max}}[r]$.
    \footnote{$\iupboundlog{\cdot}$ is a variation of the standard Big-O notation $\iupbound{\cdot}$ that hides logarithmic factors.}
    
    
    
    Algorithms that can achieve adaptive minimax incorporate environment-specific parameters will achieve tighter regret bounds tailored to different instances. 
    Unlike traditional M.O., algorithms that can achieve M.O. will provide uniform guarantees across all possible environments. 
    Algorithms that achieve adaptive variance ratios enjoy performance bounds that adapt to the ease of the bandit instances. When the optimal arm's reward distribution is more concentrated, its regret is smaller.
    
    For instance, given a Bernoulli environment, a regret with upper bound $\sqrt{V(\mu_1) KT}$ would be much smaller for MAB instances with favorable $\mu_1$ values, since $V(\mu_{\max}) = \mu_{\max}(1-\mu_{\max})$ can be $\ll1$ when $\mu_{\max}$ is close to $0$ or $1$; in this case, this regret bound can be significantly better than the usual $\iupbound{\sqrt{KT}}$ regret bound.

\begin{table*}[t] \label{tab:comparison}
    \caption{Comparison of different MAB algorithms for reward distributions belong to an OPED family in the form of \Cref{eqn:oped}.
    }
    \centering
    \begin{tabular}{ccccc}
    \hline\hline
    \textbf{Algorithm} & \multicolumn{2}{c}{\textbf{Instance-Dependent}} & \textbf{Instance-Adaptive}      & \textbf{Instance-Independent}  \\
    \textbf{\&} & Asymptotic & Sub- & Variance  & Minimax \\
    \textbf{Analysis}  & Optimality & UCB & Ratio & Ratio \\
    \hline
    TS \citeyearp{thompson1933likelihood, korda2013thompson} 
        & Yes & N/A & N/A & N/A             \\
    ExpTS \citeyearp{jin2022finite}
        & Yes & Yes & No  & $\sqrt{\ln(K)}$ \\
    ExpTS$^+$ \citeyearp{jin2022finite}
        & Yes & No  & No  & $1$             \\
    $\varepsilon$-Exploring TS$^\star$ \citeyearp{jin2023thompson} 
        & Yes & No  & No  & $\sqrt{\ln(K)}$ \\
    kl-UCB \citeyearp{cappe2013kullback, qin2023kullback}
        & Yes & Yes & N/A & $\sqrt{\ln(T)}$ \\
    kl-UCB++ \citeyearp{menard17minimax, qin2023kullback}
        & Yes & N/A & Yes & $1$             \\
    \hline
    \text {Exp-KL-MS}
        & Yes & Yes & Yes & $\sqrt{\ln(K)}$ \\
    \hline\hline
    \end{tabular}
\end{table*} 
    
    To date, no algorithms have been identified that can simultaneously satisfy the A.O., M.O., Sub-UCB criterion in the setting of OPED reward distributions except ADA-UCB~\citep{lattimore18refining} for the special case of Gaussian rewards. The most recent example is ExpTS~\citep{jin2022finite}, which achieves a logarithmic minimax ratio, A.O. and Sub-UCB. However, several studies~\citep{menard17minimax, jin2022finite, jin2023thompson} introduce a maximum variance assumption (See \Cref{assum:max-variance})
    , leading to their regret bounds being unable to enjoy an adaptive variance ratio.

    \paragraph{Our contributions.} In recent years, the research community has gained interest on Maillard Sampling style algorithms~\citep{honda2011asymptotically,maillard2011apprentissage},
    a family of randomized MAB algorithms that have several nice features including 
    various instance-dependent and -independent guarantees, and a closed-form exploration probability amenable for offline evaluation. But to date, such analysis has only been carried out under restricted reward distribution assumptions, such as finite-sized supports~\citep{honda2011asymptotically}, sub-Gaussian~\citep{bian2022maillard}, and Bernoulli~\citep{qin2023kullback}.
    
    In this paper, we propose a Maillard Sampling-style algorithm for a host of OPED reward distributions, termed Exponential-Kullback-Leibler Maillard Sampling (\expklms), and prove that it has multiple features: asymptotic optimality, logarithmic minimax ratio, Sub-UCB, and adaptive variance ratio, by performing both asymptotic and finite-time analysis. 
    Exp-KL-MS chooses an arm according to the following distribution:
    \begin{equation}
        p_t(I_t=a) \propto \expto{ -L(N_{t-1,a}) \Dcal(\hat{\nu}_{t, a}, \hat{\nu}_{t, \max})},
        \label{eqn:exp-kl-ms}
    \end{equation}
    where $N_{t-1, a}$ is the number of arm $a$ has been pulled up to time $t-1$, 
    $\hat{\nu}_{t, a}$ is the maximum likelihood estimate (MLE) of reward distribution $\nu_a$ using rewards from arm $a$ up to time step $t-1$.
    $\hnu_{t, \max}$ is the MLE of the distribution with the highest empirical mean at time step $t$.
    $L(\cdot)$ is an inverse temperature function that satisfies $0 < L(x) \leq x$ and is monotonically increasing with $x$. 
    Large $N_{t-1, a}$ means that arm $a$ has been pulled frequently in the past, resulting in $p_t(I_t=a)$ smaller and arm $a$ being less likely to be selected.
    Large $\Dcal(\hnu_{t, a}, \hnu_{t, \max})$ represents that the estimated reward distribution of arm $a$ deviates from that of the empirical best arm, resulting in $p_t(I_t=a)$ smaller. 
    Additionally, we can also view \expklms as applying the principle of Minimum Empirical Divergence (MED)~\citep{honda2011asymptotically} to OPEDs. 
    See \Cref{sec:related} for a detailed comparison.

    We show that Exp-KL-MS achieves a finite-time regret guarantee (\Cref{thm:expected-regret-total}) which can be simultaneously converted to:
    \begin{itemize}
        \item Minimax optimality guarantee (\Cref{corol:exp-kl-ms-mo}), up to a logarithmic factor. Exp-KL-MS's regret is at most a suboptimal logarithmic factor $\sqrt{\ln(K)}$ compared to the minimax optimal regret of $\Theta(\sqrt{KT})$~\citep{audibert09minimax,auer03nonstochastic}.
        \item An asymptotic regret upper bound (\Cref{corol:exp-kl-ms-ao}) that matches the lower bound established by~\citet{lai85asymptotically}, thus showing Exp-KL-MS satisfies A.O.
        \item A Sub-UCB regret guarantee (\Cref{corol:exp-kl-ms-sub-ucb}) that ensures that the algorithm's performance is at least as good as the UCB algorithm in the finite-time regime.
        \item Adaptive variance ratio (\Cref{corol:exp-kl-ms-adaptive-variance}). Exp-KL-MS achieves a regret bound of $\iupbound{\sqrt{V(\mu_{\max}) KT}}$ and it has an instance-specific parameter $V(\mu_{\max})$ adapts to the variance in the optimal arm reward.    
        Before our work, such results were only found in the $[0,1]$-bounded reward setting~\citep{qin2023kullback}.
    \end{itemize}


\paragraph{Our Techniques}
A natural way to apply Maillard Sampling (MS)~\citep{maillard2011apprentissage} to the OPED reward setting is to analyze the sampling rule (\Cref{eqn:exp-kl-ms}) with $L(k) = k$, which was analyzed in prior works in finite-sized support~\citep{honda10asymptotically} or specific reward reward distribution (e.g. Gaussian~\citep{bian2022maillard}, Bernoulli~\citep{qin2023kullback}) settings. However, in the regret analysis of this generalized algorithm, a naive generalization of the analysis in prior works~\citep{honda10asymptotically,bian2021maillard,qin2023kullback} can no longer bound the number of suboptimal arm pulls in time steps when the optimal arm performs poorly.

Since OPED covers many distribution families, we cannot leverage properties specific to a particular family, as was done in previous works, such as Bernoulli distributions~\citep{qin2023kullback} or the sub-Gaussian property for Gaussian distributions~\citep{bian2022maillard}.

To overcome this issue, we take inspiration from~\citet{jin2022finite}'s Thompson sampling-style algorithm and modify our algorithm to $L(k)=k-1$.
As we prove in our analysis, this slight change helps the proposed algorithm to have good properties shared by many MS-based algorithms and allows us to show new variance-adaptive regret guarantees.
