\section{Related Work}
\label{sec:related}

\paragraph{Exponential Family Bandit Algorithms and simultaneous adaptivity and optimality guarantees} Several bandit algorithms in the literature work in the setting of OPED reward distributions~\citep{korda2013thompson, cappe2013kullback, menard17minimax}. 
Thompson Sampling 
(TS)~\citep{thompson1933likelihood} and kl-UCB~\citep{cappe2013kullback} were among the first to use posterior sampling and optimism strategy, respectively. 
These two methods have been shown to satisfy A.O. in their original analyses. 
Based on the analysis in~\citet{lattimore20bandit}, we can find that KL-UCB achieves a logarithmic minimax ratio, which has been mentioned in~\citet{qin2023kullback} as well.


More recently, \citet{menard17minimax} proposes kl-UCB++ and demonstrates that it satisfies M.O., but it fails to achieve an adaptive variance ratio. 
\citet{jin2022finite} proposes ExpTS and shows that it also achieves a logarithmic minimax ratio as $\sqrt{\ln(K)}$, and ExpTS$^+$ satisfies M.O., but both methods lack an adaptive variance ratio since they assume their variance is capped by $V_{\max}$. 
A more detailed comparison is deferred to \tableref{tab:comparison}.


\paragraph{Asymptotic Optimality} \citet{lai85asymptotically} studied sequential allocation strategies and provided an asymptotic analysis. This foundational work on OPED families of reward distributions inspired the development of optimism strategy policies incorporating Kullback-Leibler (KL) divergence.
Several studies have proposed algorithms with A.O. guarantees. Notable examples include TS with conjugate priors~\citep{korda2013thompson}, ExpTS~\citep{jin2022finite}, which leverages non-conjugate priors, and KL-UCB~\citep{cappe2013kullback}. Another widely studied setting is the sub-Gaussian case, where all arms' reward distributions satisfy the sub-Gaussian property. Algorithms such as AOUCB~\citep{lattimore20bandit} and MS/MS$^+$~\citep{bian2022maillard} have demonstrated A.O. guarantees under this sub-Gaussian assumption.


\paragraph{Minimax Optimality} Minimax optimality assesses the performance of a bandit algorithm under the worst-case scenario. Specific bandit algorithms that use the self-defined sampling distribution achieve a minimax ratio of $\sqrt{\ln(K)}$~\citep{jin2023thompson, jin2022finite}. In contrast, numerous upper confidence-bound strategies, but not all, achieve a minimax ratio of $\sqrt{\ln(T)}$~\citep{cappe2013kullback, auer2002finite}. 
\citet{agrawal2017near} shows that TS with a Beta prior can reach a minimax ratio of $\sqrt{\ln(T)}$, and when the reward distributions are Gaussian, the minimax ratio becomes $\sqrt{\ln(K)}$. MOSS~\citep{audibert2009minimax} is the first algorithm to achieve M.O., albeit requiring a $[0, 1]$ reward environment. KL-UCB++~\citep{menard2017minimax} achieves a minimax ratio of $\iupbound{1}$ assuming an OPED reward distribution. 


\paragraph{Sub-UCB Criterion}
As mentioned in the Introduction, A.O. and M.O. provide only a limited perspective on algorithm performance, focusing on asymptotic instance-dependent and worst-case scenarios. Sub-UCB was first introduced in~\citet{lattimore18refining}, where the authors conducted a comprehensive review of the literature and demonstrated that algorithms such as MOSS~\citep{audibert2009minimax}, MOSS-Anytime~\citep{degenne2016anytime}, and KL-UCB++~\citep{menard2017minimax} fail to satisfy Sub-UCB, regardless of their minimax optimality.
More recently, MS$^+$~\citep{bian2022maillard} and KL-MS~\citep{qin2023kullback} have shown that it is possible to achieve both Sub-UCB and M.O. simultaneously when all arms' reward distributions are supported on $[0,1]$.

\paragraph{Adaptive Variance Ratio}
In the literature, we have observed that the maximum variance assumption has been used in some works~\citep{jin2022finite, jin2023thompson, menard2017minimax} to derive a finite-time instance-independent regret bound, resulting in a $V_{\max}$ variance ratio.
However, the Adaptive Variance Ratio guarantee is coarse in that it does not account for the variances of all arms. \citet{qin2023kullback} proves that KL-MS satisfies an adaptive variance ratio that uses the instance parameter $V(\mu_1)$, which is the variance of the best arm and $V(\mu_1)$ is overall better than $V_{\max}$.
\citet{qin2023kullback} also points out that with a refined regret analysis, KL-UCB++~\citep{menard17minimax} and UCB-V~\citep{audibert09exploration} can also achieve adaptive variance regret.