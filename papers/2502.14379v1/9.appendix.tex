\newpage
\appendix

\section{Proof of main conclusion} \label{sec:main-theorem}

Before presenting the details, we outline our proof roadmap in \Cref{fig:proof-flow}. We divide the proof into three phases, moving from left to right. 
\Cref{sec:main-theorem} consists of our main conclusion when $L(k) = k - 1$, \Cref{thm:expected-regret-total} and its direct consequence, \Cref{corol:expected-regret-total-max,corol:expected-regret-total-lip} with \Cref{assum:lip}.
\Cref{sec:extensions} contains all results from other choices of inverse temperature function $L(k)$.
\Cref{sec:proof-of-propositions} includes all propositions which are used to prove \Cref{thm:expected-regret-total} when $L(k) = k - 1$, \Cref{thm:expected-regret-total-version-half} when $L(k) = k/d$, and \Cref{thm:expected-regret-total-version-identity} when $L(k) = k$. All proofs of the proposition are also provided in \Cref{sec:proof-of-propositions}.
\Cref{sec:supporting-lemma} includes all auxiliary lemmas used to prove propositions in our analysis, as well as a KL-lower bound lemma (\Cref{lemma:lip-exp-KL-lower-bound}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{pic/exp-kl-ms-proof-diagram.png}
    \caption{Roadmap of proof to the \Cref{thm:expected-regret-total}}
    \label{fig:proof-flow}
\end{figure}

\subsection{Proof of \Cref{thm:expected-regret-total}}
In this section,  we focus on the left half of the proof and show the proof of \Cref{thm:expected-regret-total}.
Remind us that we have decomposed the regret into four terms according to \Cref{eqn:regret-decomposition}:
\[
    \Regret \leq \sum_{a\in[K]:\Delta_a > 0} \Delta_a \del{\Acal_a + \Gcal_a + \Bcal_a^1 + \Bcal_a^2}
\]
The proof of \Cref{thm:expected-regret-total} follows straightforwardly from applying \Cref{pro:good,pro:bad-1,pro:bad-2-ao,pro:bad-2-mo}.
$\Acal_a$ is bounded by the threshold through a trivial analysis. $\Gcal_a$ is bounded by using \Cref{pro:good}. $\Bcal^1_a$ is bounded by \Cref{pro:bad-1} and $\Bcal^2_a$ is bounded by the minimum among results from \Cref{pro:bad-2-ao,pro:bad-2-mo}.

Here, to remind us, we restate our main conclusion (\Cref{thm:expected-regret-total}):
\mainregret*




\begin{proof}[Proof of \Cref{thm:expected-regret-total}]

Recall the proof sketch we mentioned in \Cref{sec:proof-sketch}, for each arm $a$ such that $\Delta_a > \Delta$, we divide the event of pulling suboptimal arm into four subevents at each time step. Recall the definition of terms $\Acal_{t,a}, \Gcal_{t,a}, \Bcal^1_{t,a}$ and $\Bcal^2_{t,a}$:
\begin{align*}
\Acal_a =& \sum_{t=1}^T \Acal_{t, a} = \sum_{t=1}^T I(A_{t,a} \cap U_{t,a}) \\
    \Gcal_a =& \sum_{t=1}^T \Gcal_{t, a} = \sum_{t=1}^T I(A_{t,a} \cap U_{t,a}^c \cap E_{t,a} \cap F_{t,a}) \\
    \Bcal_a^1 =& \sum_{t=1}^T \Bcal_{t, a}^1 = \sum_{t=1}^T I(A_{t,a} \cap E_{t,a}^c) \\
    \Bcal_a^2 =& \sum_{t=1}^T \Bcal_{t, a}^2 = \sum_{t=1}^T I(A_{t,a} \cap F_{t,a}^c)
\end{align*}




Based on above split cases, we can decompose the regret as follows:

\begin{align}
    \Regret(T) 
    =& \sum_{a\in[K]:\Delta_a \leq \Delta} \Delta_a \EE[N_{T,a}] + \sum_{a\in[K]:\Delta_a > \Delta} \Delta_a \EE[N_{T,a}]
    \leq T\Delta + \sum_{a\in[K]:\Delta_a > \Delta} \Delta_a \EE[N_{T,a}]
        \nonumber
    \\
    \leq&
        T \Delta + \sum_{a\in[K]:\Delta_a > \Delta}  \Delta_a \onec{\Acal_{a} + \Gcal_{a} + \Bcal^1_{a} + \Bcal^2_{a}} 
        \nonumber
    \\
    \leq&
        T \Delta + \sum_{a\in[K]:\Delta_a > \Delta}  \Delta_a \onec{u + \Gcal_{a} + \Bcal^1_{a} + \Bcal^2_{a}}
\end{align}
    In the first inequality, we bound the regret incurred by steps where an arm with $\Delta_a$ smaller than $\Delta$ is pulled by $\Delta$, noting that there are at most $T$ steps in total. 
    In the second inequality, we decompose the regret from pulling arm $a$ into $\mathcal{A}_{a}, \mathcal{G}_{a}, \mathcal{B}^1_{a}$, and $\mathcal{B}^2_{a}$. 
    In the final inequality, we bound $\mathcal{A}_{a}$ by $u$, since the event $U_{t, a}$ restricts the number of times arm $a$ can be pulled to at most $u$ times.

    Therefore, for each arm $a$, we need to apply \Cref{pro:good,pro:bad-1,pro:bad-2-ao,pro:bad-2-mo} to bound $\Acal_{t,a}, \Gcal_{t,a}, \Bcal^1_{t,a}$ and $\Bcal^2_{t,a}$, respectively. 
    Recall the inverse temperature function $L(x) = x - 1$, from propositions we have
    \begin{itemize}
        \item 
        \[
            \Gcal_a 
            \leq \ExpFOne \leq \GoodEventBound 
                \tag{\Cref{pro:good}}
        \]
        \item
        \[
            \Bcal_a^1 
            \leq \ExpFTwo
                \tag{\Cref{pro:bad-1}}
        \]
        \item
        \begin{align*}
            \Bcal_a^2
            \leq& \ExpFThreeAO 
                \tag{\Cref{pro:bad-2-ao}}
            \\
            =& \frac{1}{\KL{\mu_1-\varepsilon_2}{\mu_1}} + \sum_{k=1}^T (k-1) \expto{-k \KL{\mu_1-\varepsilon_2}{\mu_1}}
                \tag{$L(k) = k-1$}
            \\
            \leq& \frac{1}{\KL{\mu_1-\varepsilon_2}{\mu_1}} + \frac{1}{ \del{\expto{\KL{\mu_1-\varepsilon_2}{\mu_1}} - 1}^2 }
                \tag{Sum the second term and let $T \to \infty$}
            \\
            \leq& \BadEventThreeBoundForAO
        \end{align*}
        \item
        \begin{align*}
            \Bcal_a^2
            \leq& \ExpFThreeMO 
                \tag{\Cref{pro:bad-2-mo}}
            \\
            \leq& \frac{6}{\KL{\mu_1-\varepsilon_2}{\mu_1}} + 2 \sum_{k=1}^T \expto{-k \KL{\mu_1-\varepsilon_2}{\mu_1}} \cdot \ln(T/k) 
                \tag{$L(k) = k-1$}
            \\
            \leq& \frac{6}{\KL{\mu_1-\varepsilon_2}{\mu_1}} + \frac{6\ln(T\KL{\mu_1-\varepsilon_2}{\mu_1} \vee e)}{\KL{\mu_1-\varepsilon_2}{\mu_1}} 
                \tag{\Cref{lemma:geo-log-sum}} 
            \\
            \leq& \BadEventThreeBoundForMO
        \end{align*}
    \end{itemize}
 
    Combining these inequalities we will obtain the upper bound shown in the \Cref{eqn:exp-kl-ms-main-regret}.
\end{proof}


\subsection{Proof of intermediate corollaries}


Starting from \Cref{thm:expected-regret-total}, we utilize a lower bound lemma of KL divergence (\Cref{lemma:lip-exp-KL-lower-bound}) to derive two intermediate results (\Cref{corol:expected-regret-total-max,corol:expected-regret-total-lip}) with different assumptions. \Cref{corol:expected-regret-total-max} relays on the maximum variance (\Cref{assum:max-variance}) and \Cref{corol:expected-regret-total-lip} relays on the Lipschitz continues variance function (\Cref{assum:lip}). From \Cref{corol:expected-regret-total-max} by choosing appropriate $\Delta$ and $c$ we can derive all our results such as logarithmic minimax ratio (\Cref{corol:exp-kl-ms-mo}), Asymptotic Optimality (\Cref{corol:exp-kl-ms-ao}) and Sub-UCB criterion (\Cref{corol:exp-kl-ms-sub-ucb}). From intermediate result \Cref{corol:expected-regret-total-lip}, we can derive adaptive variance ratio (\Cref{corol:exp-kl-ms-adaptive-variance}).


\Cref{corol:expected-regret-total-max,corol:expected-regret-total-lip} are auxiliary corollaries introduced to simplify the analysis and provide additional intermediate results. 
For simplicity, we prove only the stronger version of the auxiliary corollary among those two, \Cref{corol:expected-regret-total-lip}, under the assumption that \Cref{assum:lip} holds. 
\Cref{corol:expected-regret-total-max} can be proven using the same procedure as \Cref{corol:expected-regret-total-lip}, but by substituting a different result from \Cref{lemma:lip-exp-KL-lower-bound}.


\begin{corollary}[Regret upper bound corollary] \label{corol:expected-regret-total-max}
    For any $K$-arm bandit problem with \Cref{assum:oped,assum:max-variance,assum:reward-dist}, \expklms (\Cref{alg:general-exp-kl-ms}) has regret bounded as follows. 
    For any $\Delta > 0$ and $c \in (0, \frac{1}{4}]$: 
    \begin{align}
            &\Regret(T)
            \leq
            T\Delta
            +\sum_{a\in[K]:\Delta_a > \Delta} \Delta_a \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
                \nonumber
            \\
            &+
            \sum_{a\in[K]:\Delta_a > \Delta} \del{\frac{2}{(1-c)^2} + \frac{2}{c^2}} \frac{V_{\max}}{\Delta_a}
            +
            \del{\frac{V_{\max}}{c^2 \Delta_a} + \frac{V_{\max}^2}{c^4 \Delta_a^3}} 
            \wedge \del{ \frac{24 V_{\max}}{c^2 \Delta_a} \ln\del{\frac{T\Delta_a^2}{V_{\max}} \vee e}}
        \label{eqn:exp-kl-ms-main-regret-corol-max}
    \end{align}
\end{corollary} 
\begin{corollary}[Regret upper bound corollary] \label{corol:expected-regret-total-lip}
    For any $K$-arm bandit problem with \Cref{assum:oped,assum:reward-dist,assum:lip}, \expklms (\Cref{alg:general-exp-kl-ms}) has regret bounded as follows. 
    For any $\Delta > 0$ and $c \in (0, \frac{1}{4}]$: 
    \begin{align}
            &\Regret(T)
            \leq
            T\Delta
            +\sum_{a\in[K]:\Delta_a > \Delta} \Delta_a \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
                \nonumber
            \\
            &+
            \sum_{a\in[K]:\Delta_a > \Delta}  \del{\frac{2}{(1-c)^2} + \frac{2}{c^2}} \del{\frac{V(\mu_1)}{\Delta_a} + C_L}
                \nonumber
            \\
            &+
            \sum_{a\in[K]:\Delta_a > \Delta}  \frac{4}{c^4} \del{\frac{V^2(\mu_1)}{\Delta_a^3} + \frac{C_L^2}{\Delta_a}} 
            \wedge \del{ \frac{24}{c^2} \del{\frac{V(\mu_1)}{\Delta_a} + C_L} \ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e} + \upbound{\Delta_a}}
        \label{eqn:exp-kl-ms-main-regret-corol-lip}
    \end{align}
\end{corollary}    

\begin{proof}[Proof of \Cref{corol:expected-regret-total-lip}]
    Based on \Cref{thm:expected-regret-total}, it suffices to show that the summation term except $u$ on the right side of the \Cref{eqn:exp-kl-ms-main-regret} is bounded by $\CommonFactorInRegretBound$ when the order of KL term in the denominator is $1$ and $\CommonFactorInRegretBoundSquared$ when order is $2$ ignoring constant factor.
    With \Cref{assum:lip}, KL divergences between two distributions in $\Fcal$ can be lower bounded using \Cref{lemma:lip-exp-KL-lower-bound} and we will apply such lower bound to upper bound the RHS of \Cref{eqn:exp-kl-ms-main-regret}.
    % We use $\upbound{\cdot}$ to hide the constant which is not dependent on any bandit instance.
    Let $\varepsilon_1 = \varepsilon_2 = c\Delta_a, c\in(0, \fr14]$, each term in the RHS of \Cref{eqn:exp-kl-ms-main-regret} can be upper bounded as follows:
    
    \begin{itemize}
    
    \item
    \begin{align}
        \GoodEventBound
        \leq&
        2 \cdot\del{
            \frac{V(\mu_1-\varepsilon_2)+C_L(\Delta_a-\varepsilon_1-\varepsilon_2)}
            {(\Delta_a - \varepsilon_1 - \varepsilon_2)^2}
            }
                \tag{\Cref{lemma:lip-exp-KL-lower-bound}}
        \\
        \leq&
        2 \cdot\del{
            \frac{V(\mu_1)+C_L(\Delta_a-\varepsilon_1)}
            {(\Delta_a - \varepsilon_1 - \varepsilon_1)^2}
        }
                \tag{Lipchitz property of $V(\cdot)$}
        \\
        \leq&
        \frac{2}{(1-2c)^2}\CommonFactorInRegretBound
                \tag{$\varepsilon_1 = \varepsilon_2 = c\Delta_a$}
    \end{align} 
    \item 
    \begin{align*}
        \ExpFTwo
        \leq&
        2 \cdot\del{\fr {V(\mu_a+\varepsilon_1)+C_L\varepsilon_1}{\varepsilon_1^2}}
                \tag{\Cref{lemma:lip-exp-KL-lower-bound}}
        \\
        \leq&
        2 \cdot\del{\fr {V(\mu_1)+C_L\Delta_a}{c^2 \Delta_a^2}}
                \tag{$\varepsilon_1 =  c\Delta_a$}
        \\
        =&
        \frac{2}{c^2} \CommonFactorInRegretBound
    \end{align*}
    \item 
    \begin{align*}
        \frac{1}{\del{\KL{\mu_1-\varepsilon_2}{\mu_1}}^2}
        \leq&
        4 \cdot\del{ \fr {V(\mu_1) + C_L \varepsilon_2}{\varepsilon_2^2} }^2
                \tag{\Cref{lemma:lip-exp-KL-lower-bound}}
        \\
        \leq&
        \frac{4}{c^4} \del{\frac{V^2(\mu_1)}{\Delta_a^4} + \frac{C_L^2}{\Delta_a^2}}
                \tag{$\varepsilon_2 =  c\Delta_a$}
    \end{align*}
    \item
    \begin{align*}
        \frac{\ln(T\KL{\mu_1-\varepsilon_2}{\mu_1} \vee e)}{\KL{\mu_1-\varepsilon_2}{\mu_1}}
        \leq&
        \frac{2(V(\mu_1) + C_L\varepsilon_2)}{\varepsilon_2^2} \ln\del{\frac{T\varepsilon_2^2}{2(V(\mu_1)+C_L\varepsilon_2)} \vee e}
                \tag{Monotonicity of $\ln(x \vee e)/x$ and \Cref{lemma:lip-exp-KL-lower-bound}}
        \\
        \leq&
        \frac{2}{c^2} \CommonFactorInRegretBound \LogFactorInRegretBound
                \tag{$\varepsilon_2 =  c\Delta_a$}
    \end{align*}
    \end{itemize}
    Using the above inequalities, we begin with the \Cref{eqn:exp-kl-ms-main-regret} of \Cref{thm:expected-regret-total} and derive the following equations:

    \begin{align*}
        \Regret(T) 
        % \leq&
        % T\Delta + \sum_{a\in[K]:\Delta_a > \Delta} \Delta_a \cdot \del{\frac{\ln\del{T\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2} \vee e}}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}}}
        % \\
        % &+
        % \sum_{a: \Delta_a > \Delta} \Delta_a \del{\GoodEventBound + \ExpFTwo}
        % \\
        % &+
        % \sum_{a: \Delta_a > \Delta} \Delta_a \del{\BadEventThreeBoundForAO \wedge \del{\BadEventThreeBoundForMO} + \upbound{1}}
        % \\
        \leq&
        T\Delta + \sum_{a\in[K]:\Delta_a > \Delta} \Delta_a \cdot \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
        \\
        &+
        \sum_{a\in[K]:\Delta_a > \Delta} \del{\frac{2}{(1-2c)^2} + \frac{2}{c^2}} \del{\frac{V(\mu_1)}{\Delta_a} + C_L}
        \\
        &+
        \sum_{a\in[K]:\Delta_a > \Delta}  \frac{4}{c^4} \del{\frac{V^2(\mu_1)}{\Delta_a^3} + \frac{C_L^2}{\Delta_a}}  \wedge \del{ \frac{24}{c^2} \del{\frac{V(\mu_1)}{\Delta_a} + C_L} \ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e} + \upbound{\Delta_a}}
    \end{align*}
\end{proof}

\subsection{Showing \expklms satisfies multiple criterion}
Notice that logarithmic minimax ratio (\Cref{corol:exp-kl-ms-mo}), Asymptotic Optimality (\Cref{corol:exp-kl-ms-ao}) and Sub-UCB criterion (\Cref{corol:exp-kl-ms-sub-ucb}) relay on the maximum variance assumption (\Cref{assum:max-variance}) instead of Lipschitzness variance assumption (\Cref{assum:lip}). Therefore, we use the intermediate result \Cref{corol:expected-regret-total-max} to prove the above three corollaries and use \Cref{corol:expected-regret-total-lip} to prove \Cref{corol:exp-kl-ms-adaptive-variance}.
\subsubsection{Proof of the logarithmic minimax ratio}

\begin{proof}[Proof of \Cref{corol:exp-kl-ms-mo}]
    We start from \Cref{corol:expected-regret-total-max}. First, we can upper bound the first summation term in \Cref{eqn:exp-kl-ms-main-regret-corol-max}. Based on the monotonicity of the function $\tfrac{\ln(ax \vee e)}{x}$, and using the result from \Cref{lemma:lip-exp-KL-lower-bound}, which states that 
    \[
    \KL{\mu_a + \varepsilon_1}{\mu_1 - \varepsilon_2} \geq \frac{(1-2c)^2 \Delta_a^2}{2V_{\max}},
    \]
    we can show that the second term on the RHS of \Cref{eqn:exp-kl-ms-main-regret} is bounded by 


    
    \begin{align}
        &
        \frac{\Delta_a\ln\del{T\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2} \vee e}}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}}
            \nonumber
        \\
        \leq&
        \frac{\Delta_a\ln\del{ \frac{T(1-2c)^2\Delta_a^2}{2V_{\max}} \vee e}}{\frac{(1-2c)^2\Delta_a^2}{2V_{\max}}}
        \leq
            \frac{2V_{\max}}{(1-2c)^2 \Delta_a}\ln\del{\frac{T\Delta_a^2}{V_{\max}} \vee e}
            \nonumber
        \\
        =&
        \upbound{\del{\frac{V_{\max}}{\Delta_a} + C_L}\ln\del{\frac{T\Delta_a^2}{V_{\max}} \vee e}}
            \label{eqn:bound-of-leading-term}
    \end{align}

    Then we will apply the above inequality to the main \Cref{corol:expected-regret-total-max}. By letting $c = \fr14$ and $\Delta = \sqrt{\tfrac{V_{\max}K \ln(K)}{T}}$, we can upper bound the regret by

    \begin{align*}
        \Regret(T)
        \leq&
        T\Delta + \sum_{a\in[K]:\Delta_a > \Delta} \upbound{\del{\frac{V_{\max}}{\Delta_a} + C_L} \LogFactorInRegretBound + \Delta_a}
        \\
        \leq&
        \sqrt{V_{\max} K T \ln(K)} +
        \upbound{\sum_{a:\Delta_a>\Delta} C_L \ln\del{\frac{T\Delta_a^2}{V_{\max}} \vee e^2} + \Delta_a}
    \end{align*}
    The last term $\iupbound{ \sum_{a:\Delta>0} C_L \ln\idel{\frac{T\Delta_a^2}{V_{\max}} \vee e^2}}$ has lower order than $\sqrt{KT}$ when $T$ is sufficient large and we can conclude that \expklms enjoys an adaptive minimax ratio as $\sqrt{\ln(K)}$.
\end{proof}
Notice that if the variance function $V(x)$ is also always a constant, such as when reward follows a Gaussian distribution with fixed variance $\sigma^2$, $C_L = 0$ and the suboptimal term will only include $\iupbound{\Delta_a}$. 


\subsubsection{Proof of the Asymptotic Optimality}
\begin{proof}[Proof of \Cref{corol:exp-kl-ms-ao}]
    Consider the KL-divergence has the following property according to \Cref{eqn:KL-eqn},
    \[
        \KL{\mu_i}{\mu_j} = b(\theta_j) - b(\theta_i) - b'(\theta_j)(\theta_j-\theta_i),
    \]
    and we assume that $b''(\cdot)$ is continuous and always positive in the parameter space $\Theta$ in \Cref{assum:oped}, $b(\cdot)$ is convex and always increasing in $\Theta$.
    Therefore, $\KL{\mu_i}{\mu_j}$ will be continuous in terms of $\mu_i$ and $\mu_j$.
    From \Cref{thm:expected-regret-total}, we only need to find two series $\icbr{\varepsilon_{1,t}}_{t=1}^T$ and $\icbr{\varepsilon_{2,t}}_{t=1}^T$ such that they satisfy the following equations
    \begin{align*}
        & T \to \infty, \varepsilon_{1, T} \to 0, \quad
        \varepsilon_{2, T} \to 0
        \\
        & \KL{\mu_1-\varepsilon_{2,T}}{\mu_1} \to \del{\ln(T)}^{1/3},
        \quad
        \KL{\mu_a+\varepsilon_{1,T}}{\mu_a} \to \del{\ln(T)}^{1/2} \\
    \end{align*}
    Since $\varepsilon_{1, T} \to 0, \varepsilon_{2, T} \to 0$ when $T \to 0$, by the continuity of $\KL{}{}$, we can also conclude that $\KL{\mu_a+\varepsilon_{1, T}}{\mu_1-\varepsilon_{2, T}}$.
    Based on the above observations, starting from \Cref{thm:expected-regret-total} and letting $\Delta = 0$ we have the following equations
    \begin{align*}
        & \lim_{T \to \infty} \frac{\Regret(T)}{\ln(T)} \\
        \leq&
        \lim_{T \to \infty}
        \sum_{a\in[K]:\Delta_a > 0} \frac{\Delta_a}{\ln(T)} \del{\frac{\ln\del{T\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2} \vee e}}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}} }
        \\
        &+
        \sum_{a\in[K]:\Delta_a > 0} \frac{\Delta_a}{\ln(T)} \del{ \GoodEventBound + \ExpFTwo }
        \\
        &+
        \sum_{a\in[K]: \Delta_a > 0} \frac{\Delta_a}{\ln(T)} \del{\BadEventThreeBoundForAO} \wedge
        \del{\BadEventThreeBoundForMO}
        \\
        \leq& 
        \lim_{T \to \infty}
        \sum_{a\in[K]:\Delta_a > 0} \frac{\Delta_a}{\ln(T)} \del{\frac{\ln\del{T}}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}} +
        \frac{2\ln(\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2} \vee e)}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}} }
        \\
        &+
        \sum_{a\in[K]:\Delta_a > 0} \frac{\Delta_a}{\ln(T)} \del{ \ExpFTwo }
        \\
        &+
        \sum_{a\in[K]: \Delta_a > 0} \frac{\Delta_a}{\ln(T)} \del{\BadEventThreeBoundForAO}
        \\
        =&
        \sum_{a\in[K]:\Delta_a > 0}\frac{\Delta_a}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}} +
        \lim_{T \to \infty}
        \sum_{a\in[K]:\Delta_a > 0}
        \frac{\Delta_a}{\ln(T)} \del{ \frac{2\ln(\KL{\mu_a}{\mu_1} \vee e)}{\KL{\mu_a}{\mu_1}}}
        \\
        &+ \sum_{a\in[K]:\Delta_a > 0}
        \frac{\Delta_a}{\ln(T)}(\ln(T))^{-1/2}
        \\
        &+ \sum_{a\in[K]: \Delta_a > 0} \frac{\Delta_a}{\ln(T)}
        \del{(\ln(T))^{-1/3} + (\ln(T))^{-2/3}}
        \\
        =&
        \sum_{a\in[K]:\Delta_a > 0}\frac{\Delta_a}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}}
    \end{align*}

    % Starting from \Cref{corol:expected-regret-total}, we let $\Delta = 0$, $c = (\ln(T))^{-1/5}$ and it suffices to show that all terms are $\ensuremath{o}(\ln(T))$ except for the first summation term.
    
    % \begin{itemize}
    %     \item The first summation term becomes,
    %     \[
    %     \lim_{T \to \infty} \sum_{a\in[K]:\Delta_a > \Delta} \frac{\Delta_a}{\ln(T)} \cdot \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
    %     =
    %     \sum_{a\in[K]:\Delta_a > 0} \frac{\Delta_a}{\KL{\mu_a}{\mu_1}}
    %     \]
    %     \item The second term, when divided by $\ln T$ is negligible when $T \to \infty$,
        
    %     \begin{align*}
    %     & \lim_{T \to \infty} \sum_{a\in[K]:\Delta_a > \Delta} \frac{\Delta_a}{\ln(T)} \del{\frac{2}{(1-c)^2} + \frac{2}{c^2}} \CommonFactorInRegretBound
    %     \\
    %     =&
    %     \lim_{T \to \infty} \sum_{a\in[K]:\Delta_a > \Delta} \frac{\Delta_a}{\ln(T)} \del{2 + 2 (\ln(T))^{2/5}} \CommonFactorInRegretBound
    %     =
    %     0
    %     \end{align*}
    %     \item The third term divided by $\ln T$ is negligible when $T \to \infty$,
    %     \begin{align*}
    %     &
    %     \lim_{T \to \infty} \sum_{a\in[K]:\Delta_a > \Delta} \frac{\Delta_a}{\ln(T)} \cdot \del{ \frac{4}{c^4} \del{\frac{V^2(\mu_1)}{\Delta_a^4} + \frac{C_L^2}{\Delta_a^2}} } \wedge \del{ \frac{2}{c^2} \CommonFactorInRegretBound \LogFactorInRegretBound + \upbound{1}}
    %     \\
    %     =&
    %     \lim_{T \to \infty} \sum_{a\in[K]:\Delta_a > 0} \frac{\Delta_a}{\ln(T)} \cdot \del{ \frac{4}{c^4} \del{\frac{V^2(\mu_1)}{\Delta_a^4} + \frac{C_L^2}{\Delta_a^2}} }
    %     =
    %     \lim_{T \to \infty} \sum_{a\in[K]:\Delta_a > 0} \frac{4\Delta_a (\ln(T))^{4/5}}{\ln(T)} \del{\frac{V^2(\mu_1)}{\Delta_a^4} + \frac{C_L^2}{\Delta_a^2}}
    %     =0
    %     \end{align*}
    % \end{itemize}
    % Therefore, we can conclude that the following inequality holds, demonstrating that \expklms satisfies asymptotic optimality.
    % \[
    %     \lim_{T \to \infty} \frac{\Regret(T)}{\ln(T)}
    %     \leq \sum_{a\in[K]: \Delta_a > 0} \frac{\Delta_a}{\KL{\mu_a}{\mu_1}}
    % \]
\end{proof}

\subsubsection{Proof of Sub-UCB criterion}
\begin{proof}[Proof of \Cref{corol:exp-kl-ms-sub-ucb}]
    
    We start from \Cref{corol:expected-regret-total-max} and set $\Delta = 0$. For the second term on the RHS of \Cref{eqn:exp-kl-ms-main-regret-corol-max}, we know it can be upper bounded by 
    \[
    \upbound{\del{\frac{V(\mu_1)}{\Delta_a} + C_L}\ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e}}
    \leq
    \upbound{\del{\frac{V_{\max}}{\Delta_a} + C_L}\ln\del{\frac{T\Delta_a^2}{V_{\max}} \vee e}},
    \]
    as shown in \Cref{eqn:bound-of-leading-term} in the proof of \Cref{corol:exp-kl-ms-mo}. 
    For the other terms on the RHS of \Cref{eqn:exp-kl-ms-main-regret-corol-max}, they can all be upper bounded by
    $\iupbound{\idel{\frac{V(\mu_1)}{\Delta_a} + C_L}\ln\idel{\frac{T\Delta_a^2}{V(\mu_1)} \vee e}} \leq \iupbound{}$ and we can conclude that
    \[
        \Regret(T) \leq \upbound{ \sum_{a\in[K]: \Delta_a > 0 }\del{\frac{V(\mu_1)}{\Delta_a} + C_L}\ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e}}
        =
        \upbound{ \sum_{a\in[K]: \Delta_a > 0} \frac{V_{\max}\ln(T)}{\Delta_a} + \Delta_a }
    \]
\end{proof}
\subsubsection{Proof of adaptive variance ratio}
\begin{proof}[Proof of \Cref{corol:exp-kl-ms-adaptive-variance}]
    Notice that adaptive variance ratio requires \Cref{assum:reward-dist}, so we start from \Cref{corol:expected-regret-total-lip}. First, we can upper bound the first summation term in \Cref{eqn:exp-kl-ms-main-regret-corol-lip}. Based on the monotonicity of the function $\tfrac{\ln(ax \vee e)}{x}$, and using the result from \Cref{lemma:lip-exp-KL-lower-bound}, which states that 
    \[
    \KL{\mu_a + \varepsilon_1}{\mu_1 - \varepsilon_2} \geq \fr12 \frac{(1-2c)^2 \Delta_a^2}{V(\mu_1) + C_L(1-c)\Delta_a},
    \]
    we can show that the second term on the RHS of \Cref{eqn:exp-kl-ms-main-regret} is bounded by 
 
    \begin{align}
        &
        \frac{\Delta_a\ln\del{T\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2} \vee e}}{\KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}}
            \nonumber
        \\
        \leq&
        \frac{\Delta_a\ln\del{ \fr12\del{\frac{T(1-2c)^2\Delta_a^2}{V(\mu_1) + C_L(1-c)\Delta_a}} \vee e}}{\frac{(1-2c)^2\Delta_a^2}{V(\mu_1) + C_L(1-c)\Delta_a}}
        \leq
            \frac{V(\mu_1) + C_L(1-c)\Delta_a}{(1-2c)^2 \Delta_a}\ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e}
            \nonumber
        \\
        =&
        \upbound{\del{\frac{V(\mu_1)}{\Delta_a} + C_L}\ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e}}
    \end{align}

    Then we will apply the above inequality to the main \Cref{thm:expected-regret-total}. By letting $c = \fr14$ and $\Delta = \sqrt{\tfrac{V(\mu_1)K \ln(K)}{T}}$, we can upper bound the regret by
    \begin{align*}
        \Regret(T)
        \leq&
        T\Delta + \sum_{a\in[K]:\Delta_a > \Delta} \upbound{\del{\frac{V(\mu_1)}{\Delta_a} + C_L} \LogFactorInRegretBound + \Delta_a}
        \\
        \leq&
        \sqrt{V(\mu_1) K T \ln(K)} +
        \upbound{\sum_{a:\Delta_a>\Delta} C_L \ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e^2} + \Delta_a}
    \end{align*}
    The last term $\iupbound{ \sum_{a:\Delta>0} C_L \ln\idel{\frac{T\Delta_a^2}{V(\mu_1)} \vee e^2}}$ has lower order than $\sqrt{KT}$ when $T$ is sufficient large and we can conclude that \expklms enjoys an adaptive minimax ratio as $\sqrt{\ln(K)}$.
\end{proof}

\section{Proof of extensions} \label{sec:extensions}
In this section, we will present results from different choices of inverse temperature function $L(\cdot)$.
We want to use $L(x) = x/d$ where $d > 1$ and $L(x) = x$ as two examples to show that \expklms can satisfy some good properties but not all of them. 
\subsection{$L(x) = x/d$}
In this subsection, we present a theorem statement with its proof that serves a similar role to \Cref{thm:expected-regret-total}, providing a regret upper bound for $\gexpklms$ with $L(k) = x/d$. Additionally, $\gexpklms$ with $L(k) = x/d$ can achieve a logarithmic minimax ratio and satisfy the Sub-UCB criterion. We summarize these results in \Cref{corol:exp-kl-ms-half-mo} and \Cref{corol:exp-kl-ms-half-sub-ucb}, with their proofs provided following the statements.

\begin{theorem} \label{thm:expected-regret-total-version-half}
    For any $K$-arm bandit problem with \Cref{assum:oped} \Cref{assum:reward-dist} \Cref{assum:lip} and $L(k) = k/d$, $d > 1$ 
    \gexpklms (\Cref{alg:general-exp-kl-ms}) has regret bounded as follows. 
    For any $\Delta \geq 0$ and $c \in (0, \frac{1}{4}]$: 
    \begin{align}
        \Regret(T)
        \leq&
        T \Delta 
        + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{d\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
            \nonumber
        \\
        &+ 
        \del{\frac{2}{(1-2c)^2} + \frac{2(2d-1)}{c^2(d-1)}} \del{\frac{V(\mu_1)}{\Delta_a} + C_L} 
            \label{eqn:exp-kl-ms-half-regret}
    \end{align}
\end{theorem}
Notice that the RHS of \Cref{eqn:exp-kl-ms-half-regret} cannot guarantee that \expklms with $L(x) = x/d$ achieves A.O. due to the presence of an additional constant of $d$ in the second term. However, since the RHS does not include $\ln(T)$ beyond the leading term (the second term), we can demonstrate that \expklms achieves M.O. as well as an adaptive variance ratio of $\sqrt{V(\mu_1)}$. Furthermore, we can prove that \expklms with $L(x) = x/d$ satisfies the Sub-UCB criterion.



\begin{proof}[Proof of \Cref{thm:expected-regret-total-version-half}]
    We follow the proof procedure used in proving \Cref{thm:expected-regret-total} and \Cref{corol:expected-regret-total-lip} but change the definition of $u$ from $\expDefU + 1$ to $\tfrac{d\ln(T\KL{\mu_1+\varepsilon_1}{\mu_a-\varepsilon_2} \vee e)}{\KL{\mu_1+\varepsilon_1}{\mu_a-\varepsilon_2}} + 1$.
    The reason we make such a change is that we need the upper bound of $\Gcal_a$ to be controlled by $1 / \KL{\mu_1+\varepsilon_1}{\mu_a-\varepsilon_2}$.
    Following the same case splitting we did in the proof of \Cref{thm:expected-regret-total}, we have
    \[
        \Regret(T) \leq T\Delta + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a u + \Gcal_a + \Bcal^1_a + \Bcal^2_a
    \]
    and for each term on the RHS are bounded by
    \begin{itemize}
        \item 
        \begin{align*}
            \Gcal_a 
            \leq& \ExpFOne \leq \GoodEventBound 
                    \tag{\Cref{pro:good}}
            \\
            \leq& \frac{2}{(1-2c)^2}\CommonFactorInRegretBound
                    \tag{\Cref{lemma:lip-exp-KL-lower-bound}}
        \end{align*}
        \item
        \begin{align*}
            \Bcal_a^1 
            \leq& \ExpFTwo
                    \tag{\Cref{pro:bad-1}}
            \\
            \leq& 2 \cdot \del{\fr {V(\mu_1)+C_L\Delta_a}{\varepsilon_1^2}}
            \leq
            \frac{2}{c^2} \CommonFactorInRegretBound
                    \tag{\Cref{lemma:lip-exp-KL-lower-bound}}
        \end{align*}
        \item 
        \begin{align*}
            \Bcal_a^2
            \leq& \ExpFThreeAO
                \tag{\Cref{pro:bad-2-ao}}
            \\
            =&
                \frac{1}{\KL{\mu_1 - \varepsilon_2}{\mu_1}} 
                + \frac{1}{d-1}\sum_{k=1}^T \expto{-k \KL{\mu_1-\varepsilon_2}{\mu_1}}
            \\
            \leq&
                \frac{1}{\KL{\mu_1 - \varepsilon_2}{\mu_1}} 
                + \frac{1}{d-1} \cdot \frac{\expto{-\KL{\mu_1-\varepsilon_2}{\mu_1}}}{1 - \expto{-\KL{\mu_1-\varepsilon_2}{\mu_1}}}
                \tag{Sum of a geometric series}
            \\
            \leq&
                \frac{d}{(d-1)\KL{\mu_1 - \varepsilon_2}{\mu_1}}
            \\
            \leq&
                \frac{2d}{c^2 (d-1)} \CommonFactorInRegretBound
        \end{align*}
        \item
        \begin{align*}
            \Bcal_a^2
            \leq& \ExpFThreeMO
                \tag{\Cref{pro:bad-2-mo}}
            \\
            =&
            \frac{6}{\KL{\mu_1 - \varepsilon_2}{\mu_1}} 
            + \frac{2}{d-1} \sum_{k=1}^T \frac{\ln(T/k)}{k} \expto{-k \KL{\mu_1 - \varepsilon_2}{\mu_1}}
            \\
            \leq&
            \frac{6}{\KL{\mu_1 - \varepsilon_2}{\mu_1}} 
            + \frac{2}{d-1} \cdot \frac{3\ln(T\KL{\mu_1-\varepsilon_2}{\mu_1} \vee e)}{\KL{\mu_1-\varepsilon_2}{\mu_1}} 
                    \tag{\Cref{lemma:geo-log-sum}} \\
            \\
            \leq&
                \frac{6d\ln(T\KL{\mu_1-\varepsilon_2}{\mu_1} \vee e)}{(d-1)\KL{\mu_1-\varepsilon_2}{\mu_1}}
            \\
            \leq&
                \frac{12d}{c^2(d-1)} \CommonFactorInRegretBound \LogFactorInRegretBound
        \end{align*}
    \end{itemize}

    
        
    Therefore, we can bound the regret by
    \begin{align*}
        &\Regret(T) \\
        \leq&
        T \Delta 
        + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{d\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}} +
        \frac{2\Delta_a}{(1-2c)^2}\CommonFactorInRegretBound
        \\
        &+
        \frac{2\Delta_a}{c^2} \CommonFactorInRegretBound
        \\
        &+
        \del{\frac{2d\Delta_a}{c^2(d-1)} \CommonFactorInRegretBound} \wedge \del{\frac{12d\Delta_a}{c^2(d-1)} \CommonFactorInRegretBound \LogFactorInRegretBound}
        \\
        \leq&
        T \Delta 
        + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{d\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
        + 
        \del{\frac{2}{(1-2c)^2} + \frac{2(2d-1)}{c^2(d-1)}} \del{\frac{V(\mu_1)}{\Delta_a} + C_L} 
    \end{align*}
\end{proof}

Based on the above \Cref{thm:expected-regret-total-version-half}, we can derive the following two corollaries for \expklms with $L(x) = x/2$:
\constantminimax*
\constantsubucb*


\begin{proof}[Proof of \Cref{corol:exp-kl-ms-half-mo}]
    We follow the proof of \Cref{corol:exp-kl-ms-mo} and choose $\Delta = \sqrt{V(\mu_1)K/T}$. Then it suffices to show each term in the RHS of \Cref{eqn:exp-kl-ms-half-regret} is upper bounded by $\sqrt{KT}$ or $\sum_{a\in[K]} \Delta_a$.
    The first term $T \Delta = \iupbound{\sqrt{KT \ln(T)}}$ because the value of $\Delta$.
    In the second term, we can utilize result of \Cref{eqn:bound-of-leading-term} from the proof of \Cref{corol:exp-kl-ms-mo}
    \begin{align*}
        &\sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{d\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
                \nonumber
        \\
        \leq&
        \upbound{ \sum_{a\in[K]: \Delta_a > \Delta} \del{\frac{V(\mu_1)}{\Delta_a} + C_L}\ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e}}
                \tag{\Cref{eqn:bound-of-leading-term}}
        \\
        \leq&
        \upbound{ \sum_{a\in[K]: \Delta_a > \Delta} \del{\frac{V(\mu_1)}{\Delta} + C_L}\ln\del{\frac{T\Delta^2}{V(\mu_1)} \vee e}}
        \\
        \leq&
        \upbound{ \sqrt{KT \ln(K)}} + \upbound{K\ln(T)}
    \end{align*}
    For the third term, we have the following equations ignoring constant factor,
    \[
        \sum_{a\in[K]: \Delta_a > 0}  \frac{V(\mu_1)}{\Delta_a} + C_L
        \leq
        \sum_{a\in[K]: \Delta_a > 0} \frac{V(\mu_1)}{\Delta} + C_L \Delta_a
        =
        \sqrt{V(\mu_1)KT \ln(K)} + C_L K
    \]
    Overall, by combining the above analysis, it suffices to show that
    \[
        \Regret(T) = \upbound{\sqrt{KT \ln(K)}} + \upbound{K \ln(T)}
    \]
\end{proof}

\begin{proof}[Proof of \Cref{corol:exp-kl-ms-half-sub-ucb}]
     We follow the proof of \Cref{corol:exp-kl-ms-sub-ucb} and choose $\Delta = 0$. Then for each term in the RHS of \Cref{eqn:exp-kl-ms-half-regret}, we can show that,
     \begin{itemize}
         \item For the second term,
         \begin{align*}
            & \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{d\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
            \\
            \leq&
            \upbound{ \sum_{a\in[K]: \Delta_a > \Delta} \del{\frac{V(\mu_1)}{\Delta_a} + C_L}\ln\del{\frac{T\Delta_a^2}{V(\mu_1)} \vee e}}
                \tag{\Cref{eqn:bound-of-leading-term}}
            \\
            =&
            \upbound{ \sum_{a\in[K]: \Delta_a > \Delta} \frac{V_{\max}\ln(T)}{\Delta_a} + \Delta_a}
         \end{align*}
         \item For the third term,
         \begin{align*}
             \sum_{a\in[K]: \Delta_a > \Delta} \frac{V(\mu_1)}{\Delta_a} + C_L
             =
             \upbound{ \sum_{a\in[K]: \Delta_a > \Delta}\frac{V_{\max}\ln(T)}{\Delta_a} + \Delta_a }
         \end{align*}
     \end{itemize}
     Combining the above analysis, we can conclude that
     \[
        \Regret(T) = \upbound{ \sum_{a\in[K]: \Delta_a > \Delta}\frac{V_{\max}\ln(T)}{\Delta_a} + \Delta_a }
     \]
\end{proof}

\subsection{$L(x) = x$}
In this subsection, we present \Cref{thm:expected-regret-total-version-identity} that provides a regret upper bound to \gexpklms with $L(k) = k$ and shows that it has an adaptive variance ratio in \Cref{corol:exp-kl-ms-one-mo}.
\begin{theorem}  \label{thm:expected-regret-total-version-identity}
    For any $K$-arm bandit problem with \Cref{assum:oped} \Cref{assum:reward-dist} \Cref{assum:lip} and $L(k) = k$, 
    \gexpklms (\Cref{alg:general-exp-kl-ms}) has regret bounded as follows. 
    For any $\Delta \geq 0$ and $c \in (0, \frac{1}{4}]$: 
    \begin{align*}
        \Regret(T)
        \leq&
        T \Delta + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
        \\
        &+ 
        \del{\frac{2}{(1-2c)^2} + \frac{14}{c^2} + \frac{4\del{(\ln(T))^2+\ln(T)}}{c^2}} \del{\frac{V(\mu_1)}{\Delta_a^2} + C_L} 
            \label{eqn:exp-kl-ms-identity-regret}
    \end{align*}
\end{theorem}


\begin{remark}
The reason we obtain a $\ln(T)$ term for the third one (as opposed to $\iupbound{1} \wedge \iupbound{\ln(T)}$) for $L(x) = x-1$ and $L(x) = x / d$ is that when $L(x) = x$, \Cref{pro:bad-2-ao} gives a vacuous bound on $\Bcal_a^2$.
\end{remark}


\begin{proof}[Proof of \Cref{thm:expected-regret-total-version-identity}]
We follow the same proof procedure as in \Cref{thm:expected-regret-total} and \Cref{thm:expected-regret-total-version-half}. This time, we define $u$ as $\expDefU + 1$ and decompose the regret as follows:
    \[
        \Regret(T) \leq T\Delta + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a u + \Gcal_a + \Bcal^1_a + \Bcal^2_a
    \]
and for each term on the RHS are bounded by
    \begin{align*}
        \Gcal_a 
        \leq& \ExpFOne \leq \GoodEventBound 
                \tag{\Cref{pro:good}}
        \\
        \leq& \frac{2}{(1-2c)^2}\CommonFactorInRegretBound
        \\
        \Bcal_a^1 
        \leq& \ExpFTwo
                \tag{\Cref{pro:bad-1}}
        \\
        \leq& 2 \cdot \del{\fr {V(\mu_1)+C_L\Delta_a}{\varepsilon_1^2}}
        \leq
        \frac{2}{c^2} \CommonFactorInRegretBound
        \\
        \Bcal_a^2
        \leq& \IdentityLBadCaseTwoMO
            \tag{\Cref{pro:bad-2-mo}}
        \\
        \leq&
        \frac{1}{\KL{\mu_1-\varepsilon_2}{\mu_1}} \del{6 + (\ln(T))^2 + 2\ln(T)}
            \tag{\Cref{lemma:integral-inequality}}
        \\
        \leq&
            \frac{2}{c^2} \CommonFactorInRegretBound \del{6 + (\ln(T))^2 + 2\ln(T)}
    \end{align*}
Therefore, we can bound the regret by
    \begin{align*}
        \Regret(T) 
        \leq& 
        T \Delta + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}} +
        \frac{2\Delta_a}{(1-2c)^2}\CommonFactorInRegretBound
        \\
        &+
        \frac{2\Delta_a}{c^2} \CommonFactorInRegretBound +
        \del{\frac{4}{c^2} \CommonFactorInRegretBound \del{6 + (\ln(T))^2 + 2\ln(T)}}
        \\
        =&
        T \Delta + \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
        \\
        &+ 
        \del{\frac{2}{(1-2c)^2} + \frac{14}{c^2} + \frac{2\del{(\ln(T))^2+\ln(T)}}{c^2}} \del{\frac{V(\mu_1)}{\Delta_a^2} + C_L} 
    \end{align*}

\end{proof}

\identityadaptive*

\begin{proof}[Proof of \Cref{corol:exp-kl-ms-one-mo}]
    We follow the proof of \Cref{corol:exp-kl-ms-mo} and choose $\Delta = \sqrt{V(\mu_1)K/T}$. Then it suffices to show each term in the RHS of \Cref{eqn:exp-kl-ms-half-regret} is upper bounded by $\sqrt{V(\mu_1)KT} \ln(T)$ or $K\ln(T)$ ignoring constant.
    The first term $T \Delta = \iupbound{\sqrt{KT}\ln(T)}$ because the value of $\Delta$.
    In the second term, we can utilize result of \Cref{eqn:bound-of-leading-term} from the proof of \Cref{corol:exp-kl-ms-mo}
    \begin{align*}
        \sum_{a\in[K]: \Delta_a > \Delta} \Delta_a \del{\frac{\ln\del{T\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a} \vee e}}{\KL{\mu_a+c\Delta_a}{\mu_1-c\Delta_a}}}
        \leq
        \upbound{ \sqrt{KT }\ln(T)} + \upbound{K\ln(T)}
    \end{align*}
    For the third term, we have the following equations ignoring constant factor,
    \begin{align*}
        & \sum_{a\in[K]: \Delta_a > 0} \del{\frac{2}{(1-2c)^2} + \frac{14}{c^2} + \frac{4\del{(\ln(T))^2+\ln(T)}}{c^2}}  \del{ \frac{V(\mu_1)}{\Delta_a} + C_L }
        \\
        \leq&
        \upbound{ \sum_{a\in[K]: \Delta_a > 0} (\ln(T))^2\del{ \frac{V(\mu_1)}{\Delta} + C_L \Delta_a } }
        =
        \upbound{ \sqrt{V(\mu_1)KT}(\ln(T))^2 } + \upbound{ K (\ln(T))^2 }
    \end{align*}
    Overall, by combining the above analysis, it suffices to show that
    \[
        \Regret(T) = \upbound{\sqrt{V(\mu_1)KT}\ln(T)} + \upbound{K (\ln(T))^2 }
    \]
\end{proof}


\section{Proof of propositions} 
\label{sec:proof-of-propositions}
In this section, we focus on the proof of the propositions, which are in the middle of \Cref{fig:proof-flow}. All propositions hold for general choices of the inverse temperature function $L$ that satisfies $0 < L(x) \leq x$ and increases monotonically with $x$. 
\Cref{pro:good} derives the conclusion directly from its definition. 
\Cref{pro:bad-1} uses its definition and Chernoff's tail bound (\Cref{lemma:maximal-inequality}) to prove the result. 
For \Cref{pro:bad-2-ao,pro:bad-2-mo}, as mentioned in \Cref{sec:proof-sketch}, the proof involves constructing a series of clean events to form intervals that lower bound the random variance of $\hat{\mu}_{t, 1}$. 
\Cref{lemma:bad-2-bounded} addresses the case where all estimates are well bounded, and \Cref{lemma:seq-estimator-deviation} handles the case where at least one $\hat{\mu}_{t, 1}$, from $t = 1$ to $t = T$, falls outside the interval.

\paragraph{Favorable term $\Gcal_{a}$}

Recall that the definition of the good estimation event $\Gcal_{a}$ is
\[
    \Gcal_{a} := \EE\sbr{ \sum_{t=K+1}^T
        \one\cbr{A_{t,a} \cap 
        U_{t-1, a}^c \cap 
        E_{t-1, a}
        }},
\]
which is the expected number of times arm $a$ is pulled in the case where the agent has collected enough samples, and the estimation of $\mu_1$ is well bounded from below, and $\mu_a$ is well bounded from above.
According to the design of the algorithm, the probability of pulling a suboptimal arm $a$ should be small and decrease as $N_{t,a}$ increasing.

    \begin{proposition} \label{pro:good}
    When $0 < L(x) \leq x$,
        \begin{align}
            \Gcal_{a} \leq \ExpFOne
                \label{eqn:good}
        \end{align}
    \end{proposition}
    \begin{proof}[Proof of \Cref{pro:good}]
    Recall the notations that $A_{t,a} = \cbr{I_t = a}$, $U_{t-1,a}^c = \cbr{N_{t-1,a} \geq u}$, $E_{t-1} = \cbr{\hmu_{t-1,\max} \geq \mu_1-{\varepsilon_2}}$, 
    $F_{t-1} = \cbr{ \hmu_{t-1,a}\leq\mu_a+{\varepsilon_1} }$.

        \begin{align}
        \Gcal_{a}
        =&
            \EE\sbr{ \sum_{t=K+1}^T
                \onec{ {A_{t,a} \cap 
                    U_{t-1,a}^c \cap 
                    E_{t-1,a} \cap 
                    F_{t-1,a}
                    }
                } \mid \Hcal_{t-1}
            }
        \nonumber \\
        =&
            \EE\sbr{ \sum_{t=K+1}^T
                \onec{U_{t-1,a}^c \cap E_{t-1,a} \cap F_{t-1,a}} 
                \EE\sbr{\onec{A_{t,a}} \mid \Hcal_{t-1}}
            }
                \tag{Law of the total expectation}
        \\
        \leq&
            \sum_{t=K+1}^T
            \EE\sbr{ 
                \one\cbr{U_{t-1,a}^c \cap E_{t-1,a} \cap F_{t-1,a}}
                \expto{-L(N_{t-1,a}) \KL{\hmu_{t-1,a}}{\hmu_{t-1,\max}}}
            }
                \tag{By \Cref{alg:general-exp-kl-ms} and $M_t \geq 1, \forall t \in [T]$}
        \nonumber \\
        \leq&
            \sum_{t=1}^T
            \expto{-L(u) \cdot
                \KL{\mu_a+\varepsilon_1}{\mu_1-\varepsilon_2}}
                    \tag{Letting indication function to be true}
        \\
        \leq&
            \ExpFOne
                \label{eqn:F_1_upper_bound}
    \end{align}
    \end{proof}

    \paragraph{Unfavorable term of arm $a$, $\Bcal^1_{a}$}
    The definition of $\Bcal^1_{a}$ is
    \[
        \Bcal^1_{a} := \EE\sbr{ \sum_{t=K+1}^T
        \one\cbr{ A_{t,a} \cap 
        U_{t-1,a}^c \cap
        E_{t-1,a}^c \cap
        F_{t-1,a}
        }},
    \]
    $\Bcal^1_{a}$ is the expected number of arm $a$ is pulled in the case where the agent has collected enough samples, but the estimation of arm $a$ deviates from true mean $\mu_a$ by at least $\varepsilon_1$.
    The probability of pulling suboptimal arm $a$ is negligible as the number of samples of arm $a$ increases, thus this unfavorable term is relatively small.

    \begin{proposition} \label{pro:bad-1}
    When $0 < L(x) \leq x$,
        \begin{align}
            \Bcal^1_{a} \leq \ExpFTwo
                \label{eqn:f2}
        \end{align}
    \end{proposition}

    \begin{proof}[Proof of \Cref{pro:bad-1}]
    Recall the notations that $A_{t,a} = \cbr{I_t = a}$, $E_{t-1,a}^c = \cbr{ \hmu_{t-1,a}>\mu_a+{\varepsilon_1} }$. We have:
    \begin{align}
        \Bcal^1_{a} =
        & 
        \EE\sbr{\sum_{t=K+1}^T 
            \one\cbr{
                A_{t,a} \cap
                E_{t-1,a}^c
                }} 
        \leq
            \EE\sbr{
            \sum_{k=2}^\infty 
            \one\cbr{
                E_{\tau_a(k)-1,a}^c
                }}
                \tag{
                when $t =\tau_a(k)$ for some $k \geq 2$ the inner indicator is non-zero
                }
        \\
        \leq & 
            \EE\sbr{
            \sum_{k=2}^\infty 
            \one\cbr{ E_{\tau_a(k)-1,a}^c }}
                \tag{Dropping unnecessary conditions
                }
        = 
            \EE\sbr{
            \sum_{k=1}^\infty 
            \one\cbr{ E_{\tau_a(k)}^c }}
        \\
        \leq &
            \sum_{k=1}^\infty
            \expto{- k\cdot \KL{\mu_a + \varepsilon_1}{\mu_a}}
                \tag{Applying \Cref{lemma:maximal-inequality}}
        \\
        \leq &
            \frac{\expto{- \KL{\mu_a + \varepsilon_1}{\mu_a}}}
                 {1 - \expto{- \KL{\mu_a + \varepsilon_1}{\mu_a} }}
                \tag{Geometric sum}
        \\
        \leq &
            \ExpFTwo
                \label{eqn:F_2_upper_bound}
        \end{align}
    \end{proof}

    \paragraph{Unfavorable term of the optimal arm, $\Bcal^2_{a}$}
    Now we need to bound the last subcase $\Bcal^2_{a}$.
    The definition of $\Bcal^2_{a}$ is
    \[
        \Bcal^2_{a} := \EE\sbr{ \sum_{t=K+1}^T
        \one\cbr{ A_{t,a} \cap
        U_{t-1,a}^c \cap
        F_{t-1,a}^c
        }
    },
    \]
    which represents the expected number of times arm $a$ is pulled when the agent has collected enough samples, and the empirical best mean is underestimated.
    To achieve asymptotic optimality and minimax optimality with a logarithmic factor, we present two propositions, each of which will be used to establish the respective property.
    Specifically, \Cref{pro:bad-2-ao} is used to prove asymptotic optimality \expklms, while \Cref{pro:bad-2-mo} is used to prove minimax optimality with a logarithmic term.
    
    \begin{proposition} \label{pro:bad-2-ao}
        If $0 < L(k) < k$,
        \begin{align}
            \Bcal^2_{a} \leq \ExpFThreeAO
                \label{eqn:bad-2-ao}
        \end{align}
        
    \end{proposition}

    \begin{proposition} \label{pro:bad-2-mo}
    If $ 0 < L(k) < k$,
        \begin{align}
            \Bcal^2_{a} \leq \ExpFThreeMO
                \label{eqn:bad-2-mo}
        \end{align}
    If $L(k) = k$,
        \begin{align}
            \Bcal^2_a \leq \IdentityLBadCaseTwoMO
                \label{eqn:bad-2-ao-corner}
        \end{align}
    \end{proposition}

    Before delving into the details of the proof, we first recall the key idea outlined in \Cref{sec:proof-sketch}.
    Consider the definition of $\Bcal^2_a$, which represents the case when the optimal arm's empirical performance is underestimated. In this context, $F^c_{t, a}$ provides an upper bound on $\hmu_{t, a}$ for $t = 1$ to $T$. 
    Next, we construct a series of events $\cbr{\Ecal_k(\boldsymbol{\alpha})}_{k=1}^T$ such that in an event $\Ecal_k(\boldsymbol{\alpha})$ we have $\cbr{\alpha_k \leq \hmu_{(k) , 1} \leq \mu_1-\varepsilon_2, \KL{\hmu_{(k), 1}}{\mu_1-\varepsilon_2} \leq g(k)}$, where $\boldsymbol{\alpha} = \cbr{\alpha_1, \alpha_2, \dots, \alpha_T}$ are lower bounds of $\hmu_{(k), 1}$, ensuring that the value of $\hmu_{(k), 1}$ remains within a reasonable range as measured in terms of KL distance. $g(\cdot)$ is a function we can choose later.
    Recall that $\hmu_{(k),1}$ is the empirical mean from the first $k$ times arm pulls of the optimal arm. Specifically, $\hmu_{(k), 1} = \tfrac{1}{k} \sum_{t=1}^T r_t \onec{N_{t, 1} < k, I_t = 1}$.

    
    We use \Cref{lemma:bad-2-bounded} to handle the case where all $\hmu_{(k), 1}$ are restricted to this reasonable range ($\Ecal(\boldsymbol{\alpha})$ is true), and \Cref{lemma:seq-estimator-deviation} to address the other case ($\Ecal(\boldsymbol{\alpha})$ is false). 
    Thus, by selecting different $g(\cdot)$ we can use \Cref{lemma:bad-2-bounded} to prove \Cref{pro:bad-2-ao,pro:bad-2-mo}.
    
    \begin{proof}[Proof of \Cref{pro:bad-2-ao}]
        Recall the definition of $\Bcal^2_{a}$. We let $g(k) \equiv +\infty $, then all lower bound $\alpha_k = R_{\min}, 1 \leq k \leq T$, $\Ecal(\boldsymbol{\alpha})$ will not impose any additional constraints on $\hmu_{(k),1}$ except $F^c_{t-1,a}$. Therefore, we only need to apply \Cref{lemma:bad-2-bounded} directly.
        When $0 < L(k) < k$,
        \begin{align*}
            \Bcal^2_{a}
            =& 
            \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap F_{t-1,a}^c }
            }
            =
            \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap F_{t-1,a}^c \cap \Ecal(\boldsymbol{\alpha}) }
            }
            \\
            \leq&
            \ExpFThreeOne
            \\
            \leq&
            \ExpFThreeAO
        \end{align*}
        When $L(k) = k$, we also let $\alpha_k = R_{\min}, 1 \leq k \leq T$ and utilize the conclusion of \Cref{lemma:bad-2-bounded} directly,
        \[
            \Bcal^2_{a}
            =
            \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap F_{t-1,a}^c }
            }
            \leq
            \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{\KL{\alpha_k}{\mu_1 - \varepsilon_2}}{\KL{\mu_1 - \varepsilon_2}{\mu_1}}
            \leq
            \infty
        \]
    \end{proof}
    
    \begin{proof}[Proof of \Cref{pro:bad-2-mo}]
    We let $g(k) = \tfrac{2\ln(T/k)}{k}$ and the definition of $\Ecal_k(\boldsymbol{\alpha}):= \icbr{ \alpha_k \leq \hmu_{(k),1} \leq \mu_1-\varepsilon_2: \KL{\hmu_{(k),1}}{\mu_1-\varepsilon_2} \leq \tfrac{2\ln(T/k)}{k} }$,
    and $\Ecal(\boldsymbol{\alpha})$ becomes,
        \[
            \Ecal(\boldsymbol{\alpha}) = \bigcap_{1\leq k\leq T} \Ecal_k(\boldsymbol{\alpha}) = \bigcap_{1\leq k\leq T} \cbr{\KL{\hmu_{(k),1}}{\mu_1-\varepsilon_2} \leq \frac{2\ln(T/k)}{k} }.
        \]

    Based on $\Ecal(\boldsymbol{\alpha})$ true or not we can split $\Bcal^2_{a}$ into two terms, $\Bcal^{2,1}_{a}$ and $\Bcal^{2,2}_{a}$, and bound them by \Cref{lemma:bad-2-bounded} and \Cref{lemma:seq-estimator-deviation}, respectively,
    \begin{align*}
        \Bcal^2_{a}
        &= 
        \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap F_{t-1,a}^c }
            }
        \leq
        \underbrace{
            \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap \Ecal(\boldsymbol{\alpha}) }
            }
        }_{\Bcal^{2,1}_{a}}
        +
        \underbrace{
            \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ \Ecal(\boldsymbol{\alpha})^c }
            }
        }_{\Bcal^{2,2}_{a}}
    \end{align*}
    When $0 < L(k) < k$, to acquire an ideal upper bound, it suffices for us to accomplish the following two aims:
    \begin{itemize}
        \item When $\Ecal(\boldsymbol{\alpha})$ is true,
        \[
            \Bcal^{2,1}_a \leq \ExpFThreeOneMO
        \]
        \item When $\Ecal(\boldsymbol{\alpha})$ does not occur, we apply \Cref{lemma:seq-estimator-deviation}
        \[
            \Bcal^{2,2}_a \leq \frac{5}{\KL{\mu_1-\varepsilon_2}{\mu_1}}
        \]
    \end{itemize}
    
    Then we apply \Cref{lemma:bad-2-bounded} to bound $\Bcal^{2,1}_a$.
    When $0 < L(k) < k$, we have the following equations,
    \begin{align*}
        \Bcal^{2,1}_{a}
        \leq& 
            \ExpFThreeOneA +
            \ExpFThreeOneB
        \\
        \leq&
            \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{L(k)}{k-L(k)}\exp(-k \KL{\mu_1-\varepsilon_2}{\mu_1})  
            \cdot
            \KL{\mu_a-\alpha_k}{\mu_1-\varepsilon_2}
                \tag{$1-e^{-x} \leq x$ when $x \geq 0$}
        \\
        \leq&
            \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{L(k)}{k-L(k)}\exp(-k \KL{\mu_1-\varepsilon_2}{\mu_1})  
            \cdot
            \frac{2\ln(T/k)}{k}
                \tag{Recall the definition of $\alpha_k$}
        \\
        \leq&
            \ExpFThreeOneMO
    \end{align*}
    Then we combine the upper bounds of $\Bcal^{2,1}_a$ and $\Bcal^{2,2}_a$, we have the following bound on $\Bcal_a^2$
    \begin{align*}
        & \Bcal^2_a \leq \Bcal^{2,1}_a + \Bcal^{2,2}_a
        \\
        \leq& \ExpFThreeOneMO + \ExpFThreeTwo
        \\
        \leq& \ExpFThreeMO
    \end{align*}
    
    When $L(k) = k$, we still do the same splitting and we adjust the upper bound of $\Bcal^{2,1}_a$ as
    \begin{align*}
        \Bcal^{2,1}_{a}
        \leq& \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{\KL{\alpha_k}{\mu_1 - \varepsilon_2}}{\KL{\mu_1 - \varepsilon_2}{\mu_1}}
        \leq
            \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{2\ln(T/k)}{k\KL{\mu_1 - \varepsilon_2}{\mu_1}}
    \end{align*}
    The we can derive the final result by
    \begin{align*}
        & \Bcal^2_a \leq \Bcal^{2,1}_a + \Bcal^{2,2}_a
        \\
        \leq& \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{2\ln(T/k)}{k\KL{\mu_1 - \varepsilon_2}{\mu_1}} + \ExpFThreeTwo
        \\
        =&
        \IdentityLBadCaseTwoMO
    \end{align*}
    
    \end{proof}
    
    
\section{Supporting lemmas} \label{sec:supporting-lemma}
    In this section, we include lemmas that are used to prove the propositions in \Cref{sec:proof-of-propositions}. For example, \Cref{lemma:bad-2-bounded} is used to bound the probability of the case where all estimates of an arm, from $t=1$ to $t=T$, are well bounded. \Cref{lemma:seq-estimator-deviation} is used to bound the probability of the case where at least one estimate of an arm, from $t=1$ to $t=T$, falls outside a restricted interval. \Cref{lemma:lip-exp-KL-lower-bound} provides a lower bound for the KL divergence. The others are folklore lemmas, and their proofs will be explained as needed.

    \subsection{All Estimates of the optimal arm are restricted to limited intervals}
    \begin{lemma} \label{lemma:bad-2-bounded}
        Suppose we have a series of real values $\boldsymbol{\alpha} = \icbr{\alpha_k}_{k=1}^T$ and $\alpha_k \leq \mu_1 - \varepsilon_2, \forall 1 \leq k \leq T$. 
        Under \Cref{assum:oped} and \Cref{assum:reward-dist}, when $0 < L(x) < x$ we have:
        \begin{align}
            & \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ \Bcal^2_{t, a} \cap \Ecal(\boldsymbol{\alpha}) }
            }
                \nonumber
            \\
            \leq&
            \ExpFThreeOne
                \label{eqn:bad-2-bounded}
        \end{align}
        and when $L(x) = x$, we have:
        \begin{align}
            & \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ \Bcal^2_{t, a} \cap \Ecal(\boldsymbol{\alpha}) }
            }
            \leq
            \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{\KL{\alpha_k}{\mu_1 - \varepsilon_2}}{\KL{\mu_1 - \varepsilon_2}{\mu_1}}
        \end{align}
    \end{lemma}
    

    \begin{proof}
    Recall the notations $A_{t,a} = \cbr{I_t = a}$ and $F_{t-1}^c = \cbr{\hmu_{t-1,\max}<\mu_1-{\varepsilon_2}}$. 
    
    % To show that, it suffices that we can prove that for each case where $N_{T,1} = k, k\in \NN^+$, the following equation is true,
    %     \[
    %         \F{3} \leq (1 + \frac{L(k)}{k-L(k)}) \expto{-k \KL{\mu_1-\varepsilon_2}{\mu_1}}
    %     \]
    \paragraph{Step 1: Applying probability transferring lemma}
    
    Starting from the LHS of \Cref{eqn:bad-2-bounded},
        \begin{align}
        & \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap F_{t-1,a}^c \cap \Ecal(\boldsymbol{\alpha}) }
            }
        =  \EE\sbr{
            \sum_{t=K+1}^T 
            \one\cbr{ F_{t-1,a}^c \cap \Ecal(\boldsymbol{\alpha}) }
                    \cdot
                    \EE\sbr{
                        \one\cbr{A_{t,a}} \mid \mathcal{H}_{t-1}
                        }
                    }
                \tag{Law of total expectation}
        \\
        &\leq \EE\sbr{
            \sum_{t=K+1}^T 
            \one\cbr{ F_{t-1,a}^c \cap \Ecal(\boldsymbol{\alpha}) }
                    \cdot
                    \expto{L(N_{t-1,1}) \cdot \KL{\hmu_{t-1,1}}{\hmu_{t-1,\max})}}
                    \EE\sbr{ \one\cbr{A_{t,1}} \mid \mathcal{H}_{t-1} }
                }
                \tag{By the probability transferring \Cref{lemma:prob-transfer}}
        \\
        &\leq \EE\sbr{
            \sum_{t=2}^T 
            \one\cbr{ A_{t,1} \cap F_{t-1,a}^c \cap \Ecal(\boldsymbol{\alpha}) }
                    \cdot
                    \expto{ L(N_{t-1,1}) \cdot \KL{\hmu_{t-1,1}}{\hmu_{t-1,\max}}}
                }
                \tag{Law of total expectation }
        \\
        &\leq \EE\sbr{
            \sum_{k=2}^T 
            \one\cbr{ \Ecal_{k-1}(\alpha) }
                    \cdot
                    \expto{ L(k-1) \cdot \KL{\hmu_{\tau_1(k)-1,1}}{\hmu_{\tau_1(k)-1,\max}}}
                }
                \tag{Only when the first arm has been pulled ($t=\tau_1(k)$) the indication function is non-zero.}
        \\
        &\leq \EE\sbr{
            \sum_{k=2}^T 
            \one\cbr{ \Ecal_{k-1}(\alpha) }
                    \cdot
                    \expto{ L(k-1) \cdot \KL{\hmu_{\tau_1(k)-1,1}}{\hmu_{\tau_1(k)-1,\max}}}
                }
            \nonumber
        \\
        &\leq \EE\sbr{
            \sum_{k=1}^T
                    \one\cbr{ \alpha_k \leq \hmu_{(k), 1} \leq \mu_1 - \varepsilon_2 } \cdot
                    \expto{L(k) \cdot \KL{\hmu_{(k),1}}{\mu_1 - \varepsilon_2}}
                }
                \tag{shift index $k$ by $1$ }
        \\
                \label{eqn:bad-2-after-prob-trans}
    \end{align}
    \paragraph{Step 2: Doubling integration trick}
    Continuing from \Cref{eqn:bad-2-after-prob-trans} we can do an integral calculation by using a doubling integration trick to simply the integral.
    Let $f_k(x) = \expto{ L(k)\cdot\KL{x}{\mu_1-\varepsilon_2}}$ and $p_k(\cdot)$ to be the PDF of $\hmu_{(k), 1}$, the \Cref{eqn:bad-2-after-prob-trans} becomes

    \begin{align*}
    	 \Bcal^{2,1}_a
    	 & \leq
    	    \EE\sbr{ 
    	        \sum_{k=1}^{T}
    	        \one\cbr{\alpha_k \leq \hmu_{(k), 1} < \mu_1 - \varepsilon_2} 
    	        \cdot 
    	        f_k(\hmu_{(k), 1})
    	        } 
    	= 
    	    \sum_{k=1}^{T}
    	    \int_{\alpha_k}^{\mu_1 - \varepsilon_2} f_k(x) p_k(x) \ddx
    	 \\
    	 & = 
    	    \sum_{k=1}^{T}
    	        \int_{\alpha_k}^{\mu_1 - \varepsilon_2} p_k(x) 
                    \del{ f_k(\mu_1 - \varepsilon_2) -
    	               \int_x^{\mu_1 - \varepsilon_2} f_k'(y) \ddy } \ddx
    	            \tag{$f_k(x) = f_k(\mu_1-\varepsilon_2) - \int_x^{\mu_1-\varepsilon_2} f_k'(y) \ddy)$}
    	 \\
    	 & = 
    	    \sum_{k=1}^{T}
    	        \int_{\alpha_k}^{\mu_1 - \varepsilon_2} 
    	        p_k(x) 
    	        f_k(\mu_1 - \varepsilon_2) \ddx +
    	    \sum_{k=1}^{T}         
    	        \int_{\alpha_k}^{\mu_1 - \varepsilon_2} 
    	        \int_x^{\mu_1 - \varepsilon_2} 
    	            p_k(x) \del{-f_k'(y)} \ddy \ddx
    	  \\
    	 & = 
    	    \underbrace{
    	    \sum_{k=1}^{T}
    	        \int_{\alpha_k}^{\mu_1 - \varepsilon_2}  
    	        p_k(x)\ddx
    	        }_{A} 
                +
    	    \underbrace{
    	    \sum_{k=1}^{T}
    	        \int_{\alpha_k}^{\mu_1 - \varepsilon_2}\int^{y}_{\mu_1 - \alpha_k} 
    	        p_k(x) \del{-f_k'(y)} \ddx \ddy
    	        }_{B}
    	            \tag{Exchange the order of integral}
    \end{align*}

    \paragraph{For $A$:}

    \begin{align}
        A
        &=
            \sum_{k=1}^{T}
	            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}  
    	        p_k(x) \ddx
	    \leq
            \sum_{k=1}^{\infty}
	            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}  
    	        p_k(x) \ddx
	    \leq
	        \sum_{k=1}^\infty
	            \expto{ -k \cdot \KL{\mu_1 - \varepsilon_2}{\mu_1} }
	                \tag{By \Cref{lemma:maximal-inequality}}
	    \\
	    &=
	        \frac{\expto{ - \KL{\mu_1 - \varepsilon_2}{\mu_1} }}
	             {1 - \expto{ - \KL{\mu_1 - \varepsilon_2}{\mu_1} }}
	                \tag{Geometric sum}
	    \\
	    &\leq
	        \ExpFThreeOneA
	                \tag{$e^x \geq x + 1$ when $x \geq 0$}
	    \\
	                \label{eqn:A}
    \end{align}

    \paragraph{For $B$}
    Notice that the derivative $\frac{\diff \KL{y}{\mu_1-\varepsilon_2}}{\ddy}$ derived from $f'_k(y)$ is negative when $y \leq \mu-\varepsilon_2$, the term $B$ is still positive.
    When $0 < L(k) < k$,
    \begin{align}
        B
        =& 
            \sum_{k=1}^{T}
            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}\int^{y}_{\mu_1 - \alpha_k} 
            p_k(x) \del{-f_k'(y)} \ddx \ddy
                \nonumber
        \\
        =&  
            \sum_{k=1}^{T}
            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}
            \PP(\alpha_k \leq \hmu_{(k), 1} \leq y) \cdot
            \del{-f_k(y)} L(k) \frac{\diff \KL{y}{\mu_1 - \varepsilon_2}}{\ddy}
            \ddy
                \tag{Calculate the derivative and inner integral}
        \\
        \leq&
            \sum_{k=1}^{T}
            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}
            \exp(-k \KL{y}{\mu_1}) \cdot
            \del{-f_k(y)} L(k) \frac{\diff \KL{y}{\mu_1 - \varepsilon_2}}{\ddy} 
            \ddy
                \tag{Apply \Cref{lemma:maximal-inequality}}
        \\
        =&
            \sum_{k=1}^{T}
            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}
                \expto{
                    -k \KL{y}{\mu_1}
                    +L(k)(\KL{y}{\mu_1 - \varepsilon_2}
                    } \cdot 
                (-L(k)) \frac{\diff \KL{y}{\mu_1 - \varepsilon_2}}{\ddy} \ddy
                \nonumber
        \\
        \leq& 
            \sum_{k=1}^{T}
            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}
                \expto{
                    -k \KL{\mu_1-\varepsilon_2}{\mu_1}
                -(k - L(k) \KL{y}{\mu_1-\varepsilon_2}
                    } \cdot 
                (-L(k)) \frac{\diff \KL{y}{\mu_1 - \varepsilon_2}}{\ddy} \ddy
                \nonumber
        \\
        =&
            \sum_{k=1}^{T}
            \frac{L(k)\exp(-k \KL{\mu_1-\varepsilon_2}{\mu_1})}{k-L(k)}
                \expto{
                -(k - L(k) \KL{y}{\mu_1-\varepsilon_2}
                    } \mid_{\alpha_k}^{\mu_1 - \varepsilon_2}
                \nonumber
        \\
        =&
            \ExpFThreeOneB
                \tag{Recall $L(k) = k-1$}
        \\
                \label{eqn:B}
        \end{align}
    When $ L(k) = k$, we can reuse the above analysis until the last inequality,
    \begin{align}
        B
        \leq& 
            \sum_{k=1}^{T}
            \int_{\alpha_k}^{\mu_1 - \varepsilon_2}
                \expto{
                    -k \KL{\mu_1-\varepsilon_2}{\mu_1}
                    } \cdot 
                (-k) \frac{\diff \KL{y}{\mu_1 - \varepsilon_2}}{\ddy} \ddy
                \nonumber
        \\
        =&
            \sum_{k=1}^{T}
            k \expto{-k \KL{\mu_1-\varepsilon_2}{\mu_1}} \KL{y}{\mu_1 - \varepsilon_2} \mid_{\mu_1-\varepsilon_2}^{\alpha_k}
                \nonumber
        \\
        =&
            \sum_{k=1}^{T}
            k \expto{-k \KL{\mu_1-\varepsilon_2}{\mu_1}} \KL{\alpha_k}{\mu_1 - \varepsilon_2}
        \\
        \leq&
            \sum_{k=1}^{T}
            \frac{\KL{\alpha_k}{\mu_1 - \varepsilon_2}}{\KL{\mu_1 - \varepsilon_2}{\mu_1}}
                \label{eqn:B-corner-case}
    \end{align}
        Based on \Cref{eqn:A,eqn:B,eqn:B-corner-case}, we obtain the final conclusion that when $0 < L(k) < k$,
        \begin{align*}
            & \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap F_{t-1,a}^c \cap \Ecal(\boldsymbol{\alpha}) }
            }
            \\
            \leq&
            \ExpFThreeOne
        \end{align*}
        and when $L(k) = k$,
        \begin{align*}
            & \EE \sbr{ 
            \sum_{t=K+1}^T
            \one\cbr{ A_{t,a} \cap F_{t-1,a}^c \cap \Ecal(\boldsymbol{\alpha}) }
            }
            \leq
            \ExpFThreeOneA +
            \sum_{k=1}^{T}
            \frac{\KL{\alpha_k}{\mu_1 - \varepsilon_2}}{\KL{\mu_1 - \varepsilon_2}{\mu_1}}
        \end{align*}
    \end{proof}
        
\subsection{Bounding the deviation of mean estimation exceeding the threshold}
    We borrow the following lemma from Lemma 3.2 in \citet{jin2023thompson} and slightly change the statement to accompany our requirement. We present the full proof since the proof in \citet{jin2023thompson} requires the maximum variance assumption, while in our setting, it might not hold.
    
    \begin{lemma} \label{lemma:seq-estimator-deviation} 
        Suppose we have a random variable $X$ following distribution $\nu$ with mean $\mu$ from an OPED family $\Fcal_m$. Assume that \Cref{assum:oped,assum:reward-dist} hold.
        We have collected a sequence of sample $\cbr{X_i}_{i=1}^k$ draw i.i.d. from $\nu$. Denote $\sum_{i=1}^s X_{i}/s$ as $\hmu_{s}$ and $\KL{x}{y}_+ := \KL{x}{y}\onec{x \leq y}$. we have the equation,
        \begin{align*}
            \mathbb{P}\del{
                \exists 1 \leq s \leq T: 
                    \KL{\hmu_s}{\mu-\varepsilon}_+ \geq \frac{e\ln(T/s)}{s}
                } 
            \leq 
            \frac{5}{T\KL{\mu-\varepsilon}{\mu} }
        \end{align*}
    \end{lemma}
    \begin{proof}
        Based on \Cref{lemma:Bregman-equation} and the monotonicity of the natural parameter, under the condition $\hmu_s \leq \mu-\varepsilon$ and $\varepsilon \geq 0$, we have $\KL{\hmu_s}{\mu-\varepsilon} \leq \KL{\hmu_s}{\mu} - \KL{\mu-\varepsilon}{\mu}$
        \begin{align*}
            & \PP\del{\exists s: 1 \leq s \leq T,
                \KL{\hmu_s}{\mu-\varepsilon}_+ \geq \frac{e\ln(T/s)}{s}
            }
            \\
            \leq&
            \PP\del{\exists s: 1 \leq s \leq T,
                \del{\KL{\hmu_s}{\mu} - \KL{\mu-\varepsilon}{\mu}}\onec{\hmu_s \leq \mu-\varepsilon} \geq \frac{e\ln(T/s)}{s}
            }
            \\
            \leq&
            \PP\del{\exists s: 1 \leq s \leq T,
                \KL{\hmu_s}{\mu}_+ - \KL{\mu-\varepsilon}{\mu} \geq \frac{e\ln(T/s)}{s}
            }
        \end{align*}

        Then we apply the peeling device $\frac{T}{e^{n+1}} < \hmu_s \leq \frac{T}{e^n}$ to give an upper bound to the above equation
        \begin{align}
            & \mathbb{P}\del{
                \exists s: 1 \leq s \leq T,
                    \KL{\hmu_s}{\mu}_+ - \KL{\mu-\varepsilon}{\mu}  \geq \frac{e \ln( T/s )}{s}
                }
                    \nonumber
            \\
            \leq& \sum_{n=0}^\infty
                \mathbb{P}\del{
                \exists s:
                    s \in \NN^+ \bigcap (\frac{T}{e^{n+1}}, \frac{T}{e^n}],
                        \KL{\hmu_s}{\mu}_+ - \KL{\mu-\varepsilon}{\mu} \geq \frac{e \ln( T/s )}{s}
                }
                    \nonumber
            \\
            \leq& \sum_{n=0}^\infty
                \mathbb{P}\del{
                \exists s: 
                    s \in \NN^+ \bigcap (\frac{T}{e^{n+1}}, \frac{T}{e^n}],
                        \KL{\hmu_s}{\mu}_+ - \KL{\mu-\varepsilon}{\mu} \geq \frac{n e^{n+1}}{T}
                }
                        \tag{Relax $s$ to the maximum in each subcase}
            \\
                    \label{eqn:before-subcases-new}
            \end{align}
            
            Here we need to discuss several subcases: 

            
            \begin{enumerate}
            \item $n > \lfloor \ln(T) \rfloor$.

            In this case $n > \ln(T) \implies \frac{T}{e^n} < 1 \implies \NN^+ \bigcap (\frac{T}{e^{n+1}}, \frac{T}{e^n}] = \emptyset$.
            Then we can bound the probability of this event happening by 0 since there is no valid choice of $s$.

            \item $\lfloor \ln(T) \rfloor - 1 < n \leq \lfloor \ln(T) \rfloor$.
            
            In this case, $n = \floor{\ln(T)}$ which implies that $( \frac{T}{e^{n+1}}, \frac{T}{e^n}]$ only contains one integer $1$.

            \item $n \leq \lfloor \ln(T) \rfloor - 1$

            The above inequality implies that $\frac{T}{e^{n+1}} \geq 1$.
            \end{enumerate}


            Then the summation of $n$ from $0$ to $+\infty$ is equivalent to the sum from $0$ to $\floor{\ln(T)}-1$.        

            \begin{align*}
            \eqref{eqn:before-subcases-new} 
            =&
            \sum_{n=0}^{\lfloor \ln(T)\rfloor - 1}
                \mathbb{P}\del{
                \exists s: 
                    s \in \NN^+ \bigcap (\frac{T}{e^{n+1}}, \frac{T}{e^n}],
                        \KL{\hmu_s}{\mu} - \KL{\mu-\varepsilon}{\mu} \geq \frac{n e^{n+1}}{T}
                }
            +
            \sum_{n=\lfloor \ln(T)\rfloor}^\infty
                0
            \\
            \leq&
            \sum_{n=0}^{\lfloor \ln(T)\rfloor - 1}
                \mathbb{P}\del{
                    \exists s \geq \lceil \fr{T}{e^{n+1}} \rceil,
                        \KL{\hmu_s}{\mu} - \KL{\mu-\varepsilon}{\mu} \geq \frac{n e^{n+1}}{T}
                }
            \\
            \leq& \sum_{n=0}^{\lfloor \ln(T)\rfloor - 1}
                \expto{ 
                    - \lceil \frac{T}{e^{n+1}} \rceil
                    \cdot \del{
                        \fr{n e^{n+1}}{T} +
                        \KL{\mu-\varepsilon}{\mu}
                    }
                }
                    \tag{(Maximal Inequality) \Cref{lemma:maximal-inequality}}
            \\
            \leq& \sum_{n=0}^\infty
                \expto{ -n -  \frac{T \KL{\mu-\varepsilon}{\mu}}{e^{n+1}}}
            = \sum_{n=0}^\infty
                \frac{1}{e^{n}}
                \expto{ - \frac{T \KL{\mu-\varepsilon}{\mu}}{e^{n+1}}}
            \\
            \leq &
                \int_{0}^\infty
                \frac{1}{e^{x}}
                \expto{ - \frac{T \KL{\mu-\varepsilon}{\mu}}{e^{x+1}}} \ddx
                +
                \frac{1}{T \KL{\mu-\varepsilon}{\mu}}
            \\
            \leq &
                \frac{e}{T \KL{\mu-\varepsilon}{\mu}} \expto{-\frac{T\KL{\mu-\varepsilon}{\mu}}{e^{x+1}}} \mid_{x=0}^{x=\infty}
                +
                \frac{1}{T \KL{\mu-\varepsilon}{\mu}}
                    \tag{Integral and $e^x \geq x$ when $x > 0$}
            \\
            =& 
                \frac{e}{T \KL{\mu-\varepsilon}{\mu} } \del{1 - \expto{-\frac{T \KL{\mu-\varepsilon}{\mu}}{e}}} + \frac{1}{T \KL{\mu-\varepsilon}{\mu} }
                    \tag{Algebra}
            \\
            \leq&
                \frac{5}{T \KL{\mu-\varepsilon}{\mu} }
        \end{align*}
                
        The first inequality relaxes the choice of $s$ from $(\frac{k}{e^{n+1}}, \frac{k}{e^n}]$ to $(\frac{k}{e^{n+1}}, \infty]$.
        The second inequality uses \Cref{lemma:maximal-inequality} where for each choice of $n$, we apply the Lemma once by setting $N = \lceil \frac{k}{e^{n+1}} \rceil$ and $y$ to be $\frac{e^{n+1}\ln\del{ e^{n+1} T/k }}{k}$.
        In the third inequality, we remove the ceiling function.
        The forth inequality uses $\sum_{x=a}^{b} f(x) \leq \int_a^b f(x)\ddx + \max_{x\in\sbr{a,b}} f(x)$ when $f(x)$ is unimodal and we let $f(x) = \frac{k}{e^x T}\expto{-\frac{k}{e^{x+1}}\KL{\mu-\varepsilon}{\mu}}$.
        For last inequality, we let $f(T) = T$ and we relax $\del{1 - \expto{-\frac{T \KL{\mu-\varepsilon}{\mu}}{e}}}$ to $1$.
    \end{proof}

\subsection{Other auxiliary lemmas} 
\subsubsection{Probability transferring}
\begin{lemma} \label{lemma:prob-transfer}
    Suppose \Cref{alg:general-exp-kl-ms} is run.
    Let $\Hcal_{t-1}$ denote the $\sigma$-field derived from the historical path up to and including time $t-1$, which is represented as $\sigma\del{ \cbr{I_i, r_i}_{i=1}^{t-1} }$ (where $I_i$ indicates the arm pulled at time round $i$ and $r_i$ is the corresponding reward). 
    Then, 
    \begin{align}
        \PP(I_t = a|\Hcal_{t-1}) 
        \leq 
        \expto{L(N_{t-1,a}) \KL{\hmu_{t-1,1}}{\hmu_{t-1,\max}}} \PP(I_t = 1 \mid \Hcal_{t-1})
            \label{eqn:prob-transfer}
    \end{align}
    \end{lemma}
    \begin{proof}
        To prove \Cref{eqn:prob-transfer}, recall the algorithm setting, we have the following relationship
        \begin{align*}
            & \PP(I_t = a|\Hcal_{t-1})
            =
                \frac{\expto{-L(N_{t-1,a}) \KL{\hmu_{t-1,a}}{\hmu_{t-1,\max}} }}
                     {\expto{-L(N_{t-1,1}) \KL{\hmu_{t-1,1}}{\hmu_{t-1,\max}} }}
                \cdot
                \PP(I_t = 1 \mid \Hcal_{t-1})
            \\
            \leq&
                \frac{ \PP(I_t = 1 \mid \Hcal_{t-1}) }
                     {\expto{-L(N_{t-1,1}) \KL{\hmu_{t-1,1}}{\hmu_{t-1,\max}} }}                
            =
                \expto{L(N_{t-1,1}) \KL{\hmu_{t-1,1}}{\hmu_{t-1,\max}} }
                \PP(I_t = 1 \mid \Hcal_{t-1})
        \end{align*}
        The inequality is due to $ \KL{\hmu_{t-1,a}}{\hmu_{t-1,\max}} \geq 0 $. 
    \end{proof}
\subsubsection{Properties of KL divergence in OPED family}
\begin{lemma}\citep{Harremo_s_2017} \label{lemma:exp-KL-eq}
    Let $\mu$ and $\mu'$ be the mean values of two distributions in $\Fcal$. The Kullback-Leibler divergence between them satisfies:
        \[
            \KL{\mu}{\mu'} = \int_{\mu}^{\mu'} \frac{x-\mu}{V(x)} \ddx
            ,
        \]
    where $V(x)$ is the variance of the distribution in $\Fcal$ with mean parameter $x$.
\end{lemma}

    \begin{lemma}[Bregman Divergence Identity] \label{lemma:Bregman-equation}
        Suppose we have three distributions
        in $\Fcal_m$
        with model parameter $\theta_a, \theta_b$ and $\theta_c$, and their means are $\mu_a, \mu_b$ and $\mu_c$, respectively. Then we have the following relationship

        \[
            \KL{\mu_a}{\mu_b} - \KL{\mu_a}{\mu_c} = 
            - \KL{\mu_b}{\mu_c} - \del{ \mu_b - \mu_a } \del{ \theta_c - \theta_b }
        \]
    \end{lemma}
    \begin{proof}
        According to \Cref{eqn:KL-eqn}, there are
        \[
            \KL{\mu_a}{\mu_b} = b(\theta_b) - b(\theta_a) - \mu_a \del{ \theta_b - \theta_a }
        \]
        \[
            \KL{\mu_a}{\mu_c} = b(\theta_c) - b(\theta_a) - \mu_a \del{ \theta_c - \theta_a }
        \]
        therefore, 
        \begin{align*}
            &
                \KL{\mu_a}{\mu_b} - \KL{\mu_a}{\mu_c}
            \\
            =&
                b(\theta_b) - b(\theta_a) - \mu_a \del{ \theta_b - \theta_a } - b(\theta_c) + b(\theta_a) + \mu_a \del{ \theta_c - \theta_a }
            \\
            =&
                b(\theta_b) - b(\theta_c)  - \mu_a \del{ \theta_b - \theta_c }
            \\
            =&
                - \del{ b(\theta_c) - b(\theta_b) - \mu_b \del{ \theta_c - \theta_b } } - \del{ \mu_b - \mu_a } \del{ \theta_c - \theta_b }
            \\
            =&
                - \KL{\mu_b}{\mu_c} - \del{ \mu_b - \mu_a } \del{ \theta_c - \theta_b }
        \end{align*}
    \end{proof}
    

    
\begin{lemma}[Lower Bound of KL] \label{lemma:lip-exp-KL-lower-bound}
    Denote the reward distributions as $(\nu_i)_{i=1}^K$. 
    Suppose all reward distributions come from an OPED family $\Fcal$ with $V_{\max}$ as the maximum variance.
    Denote two distributions $\nu_i$ and $\nu_j$ from $\Fcal$ with mean $\mu_i, \mu_j$ and variance $V(\mu_i), V(\mu_j)$, respectively.
    If $\Fcal$ satisfies \Cref{assum:lip} with Lipschitzness constance $C_L$.
    $\Delta:= \abs{\mu_j - \mu_i}$, we have a lower bound to the KL divergence between $\nu_i$ and $\nu_j$, 
    \[
        \KL{\mu_i}{\mu_j}
        \geq
        \fr{1}{2} \del{
            \fr{\Delta^2}{V(\mu_i) + C_L \Delta} \vee
            \fr{\Delta^2}{V(\mu_j) + C_L \Delta}}
    \]
    otherwise
    \[
        \KL{\mu_i}{\mu_j}
        \geq
        \fr{\Delta^2}{2V_{\max}}
    \]
    \end{lemma}
    
    \begin{proof}
        Based on the variance form of $\KL{\mu_i}{\mu_j}$ if \Cref{assum:lip} is true we have
        \begin{align*}
            \KL{\nu_m}{\nu_n}
            =&
            \int_{\mu_i}^{\mu_j} \frac{x-\mu_i}{V(x)} \ddx
            \\
            \ge&
            \int_{\mu_i}^{\mu_j} \frac{x-\mu_i}{V(\mu_i) + C_L\Delta} \ddx \vee
            \int_{\mu_i}^{\mu_j} \frac{x-\mu_i}{V(\mu_j) + C_L\Delta} \ddx 
            =
            \frac{1}{2} \del{ 
            \frac{\Delta^2}{V(\mu_i) + C_L\Delta} \vee \frac{\Delta^2}{V(\mu_j) + C_L\Delta}}
        \end{align*}
        otherwise
        \begin{align*}
            \KL{\nu_m}{\nu_n}
            =
            \int_{\mu_i}^{\mu_j} \frac{x-\mu_i}{V(x)} \ddx
            \ge
            \int_{\mu_i}^{\mu_j} \frac{x-\mu_i}{V_{\max}} \ddx
            =
            \fr{\Delta^2}{2V_{\max}}
        \end{align*}
    \end{proof}

\subsubsection{Tail bound}
\begin{lemma}\citep{menard17minimax} \label{lemma:maximal-inequality}
     Given a natural number $N$ in $\NN^+$, and a sequence of R.V.s $\cbr{X_i}_{i=1}^N$ is drawn from a one parameter exponential distribution $\nu$ with model parameter $\theta$ and mean $\mu$. Let $\hmu_n = \frac{1}{n}\sum_{i=1}^n X_i, n \in \NN$, which is the empirical mean of the first $n$ samples.
     
     Then, for $y \geq 0$ 
    \begin{align}
        \PP(\exists n \geq N, \KL{\hmu_n}{\mu} \geq y, \hmu_n < \mu ) 
        \leq& 
        \exp(-N y) \label{eqn:maximal-inequality-lower}
        \\
        \PP(\exists n \geq N, \KL{\hmu_n}{\mu} \geq y, \hmu_n > \mu ) 
        \leq& 
        \expto{-N y} \label{eqn:maximal-inequality-upper}
    \end{align}
    Consequently, the following inequalities are also true:
    \begin{align}
        \PP(\hmu_N < \mu - \varepsilon ) 
        \leq 
        \expto{-N\cdot \KL{\mu - \varepsilon}{\mu}}
        \label{eqn:chernoff-lower-tail-bound}
        \\
        \PP(\hmu_N > \mu + \varepsilon ) 
        \leq 
        \expto{-N\cdot \KL{\mu + \varepsilon}{\mu}}
        \label{eqn:chernoff-upper-tail-bound}
    \end{align}
\end{lemma}
\subsubsection{Bounding the Sum of a Series of Geometric-log }
    \begin{lemma} \label{lemma:geo-log-sum}
        Suppose that $a, T$ are positive constant and $a > 1/T, T \in \NN^+$, we have the following
        \begin{align*}
            \sum_{k=1}^T \expto{-k a } \ln(T/k)
            \leq
            \frac{3\ln(Ta \vee e)}{a}
        \end{align*}
    \end{lemma}

    \begin{proof}
        Here we consider two subcases:
        \begin{itemize}
            \item $a \geq 1$
            \item $1/T < a < 1$
        \end{itemize}
        \paragraph{Case 1: $a \geq 1$} In this case, we note that $\ln(T/k) \leq \ln(T) \leq \ln(Ta)$ for all $k \geq 1$ and bound the sum using a geometric series.
        \begin{align*}
            & 
                \sum_{k=1}^T \expto{-k a } \ln(T/k)
            \leq
                \sum_{k=1}^T \expto{-k a } \ln(Ta)
            \leq
                \ln(Ta) \sum_{k=1}^\infty \expto{-k a}
            \\
            =&
                \ln(Ta) \frac{\expto{-a}}{1-\expto{-a}}
            \leq
                \frac{\ln(Ta)}{a}
            \leq
                \frac{3\ln(Ta \vee e)}{a}
        \end{align*}


        \paragraph{Case 2: $1/T < a < 1$}
        In the second case, depending on the exact value of $a$, we split the sum of $k$ into two ranges, one is $k \leq \ceil{\frac{1}{a}}$ and another is $k > \ceil{\frac{1}{a}}$. 
        In the first range a, we can bound it by,
        \begin{align*}
            \sum_{k=1}^{\ceil{\frac{1}{a}}} \expto{-k a } \ln(T/k)
            \leq
                \ceil{\frac{1}{a}}\ln(T/\ceil{\frac{1}{a}})
            \leq
                \ceil{\frac{1}{a}} \ln(Ta)
            \leq
                \frac{2\ln\del{Ta}}{a} 
        \end{align*}

        In the second range $k > \ceil{\frac{1}{a}}$, we can relax the log term to $\ln(Ta)$ and sum them together.
        \[
            \sum_{k=\ceil{\frac{1}{a}} + 1}^{T} \expto{-k a } \ln(T/k)
            \leq
                \sum_{k=\ceil{\frac{1}{a}} + 1}^{T} \expto{-ka} \ln(T/\ceil{\frac{1}{a}})
            \leq
                \sum_{k=\ceil{\frac{1}{a}} + 1}^{T} \expto{-ka} \ln(Ta)
            \leq
                \frac{\ln\del{Ta}}{a} 
        \]

        Overall, we can bound the sum by combining the above two ranges,
        \begin{align*}
            \sum_{k=1}^{T} \expto{-k a } \ln(T/k)
            \leq
            \frac{2\ln(Ta \vee e)}{a} + \frac{\ln(Ta)}{a}
            \leq
            \frac{3\ln(Ta \vee e)}{a}
        \end{align*}

    \end{proof}

\subsubsection{Integral Inequality}
Below, we include the proof of a folklore lemma used in \citet{jin2022finite}; we include its proof here for completeness, as we cannot find proof in the literature.

\begin{lemma} \label{lemma:integral-inequality}
    Given a nonnegative integrable function $f(x)$ which is unimodal in the range $[a, b]$, $a < b$ and $a, b \in \NN^+$.
    For the sum of the series, $\sum_{x=a}^b f(x)$, we have the following inequality
    \begin{align*}
        \sum_{i=a}^{b} f(i)
        \leq 
        \int_{a}^{b} f(x) \ddx +
        \max_{x\in[a, b]} f(x)
    \end{align*}
\end{lemma}
\begin{proof}
    For an integral $[c, c+1]$, if $f(x)$ is increasing on this interval, we have the equation $f(c) \leq \int_c^{c+1} f(x) \ddx$.
    
    If $f(x)$ is decreasing on this interval, we have the equation $f(c+1) \leq \int_c^{c+1} f(x) \ddx$.
    Since the function $f(x)$ is unimodal in the range $[a, b]$, we can consider there are four subcases.
    \begin{itemize}
        \item $f(x)$ is always increasing on $(a, b)$. This case is trivial, and we use the above conclusion.
        \item $f(x)$ is always decreasing on $(a, b)$.
        \begin{align*}
            \sum_{i=a}^{b} f(i) \leq f(a) + \int_a^{b-1} f(x) \ddx \leq \int_a^b f(x) \ddx + \max_{x\in[a, b]} f(x)
        \end{align*}
        \item There exists a $c \in (a, b)$, $f(x)$ is increasing on $[a, c]$ and is decreasing on $[c, b]$.
        \begin{align*}
            \sum_{i=a}^{b} f(i) 
            =& \sum_{i=a}^{i < c} f(i) + \sum_{i \geq c}^{b} f(i) 
            \leq \int_a^c f(x) \ddx + \int_c^{b} f(x) \ddx + \max_{x\in [c, c+1]} f(x)
            \\
            \leq& \int_a^c f(x) \ddx + \int_c^{b} f(x) \ddx + \max_{x\in [a, b]} f(x) 
        \end{align*}
        \item There exists a $c \in (a, b)$, $f(x)$ is decreasing on $[a, c]$ and is increasing on $[c, b]$.
        \begin{align*}
            \sum_{i=a}^{b} f(i) 
            =& \sum_{i=a}^{i \leq c} f(i) + \sum_{i > c}^{b} f(i) 
            \leq \int_a^c f(x) \ddx + \int_c^{b} f(x) \ddx + \max_{x\in [a, a+1]} f(x)
            \\
            \leq& \int_a^c f(x) \ddx + \int_c^{b} f(x) \ddx + \max_{x\in [a, b]} f(x) 
        \end{align*}
    \end{itemize}


\end{proof}
