\section{Algorithms} 

We present our main algorithm, \gexpklms, in \Cref{alg:general-exp-kl-ms}. For the first $K$ steps, the agent pulls each arm once. After the first $K$ steps, the agent pulls an arm $a$ according to the arm sampling distribution $\idel{p_{t, a}}_{a\in[K]}$, which is proportional to $\expto{-L(N_{t-1,a}) \cdot \KL{\hmu_{t-1,a}}{\hmu_{t-1, \max}}}$. Here, $L(\cdot)$ is an inverse temperature function that satisfies $0 < L(k) \leq k$ and is monotonically increasing with $k$.
Every time the agent receives a reward $r_t$ from arm $a$, it updates the corresponding number of arm pulls $N_{t, a}$ and its empirical mean estimation $\hat{\mu}_{t, a}$.

    \begin{algorithm}[ht]
    \begin{algorithmic} 
    \STATE \textbf{Input:} $K\geq 2$
    \FOR{$t=1,2,\cdots,T$}
        \IF{$t\leq K$} 
            \STATE Pull the arm $I_t=t$ and observe reward $r_t \sim \nu_{I_t}$.
        \ELSE 
            \STATE $p_{t,a} = \expto{-L(N_{t-1,a}) \KL{\hmu_{t-1,a}}{\hmu_{t-1,\max} } } / M_t$.
            \STATE $M_t$ is a normalization factor, $M_t := \sum_{a=1}^K \expto{-L(N_{t-1,a})\KL{\hmu_{t-1,a}}{\hmu_{t-1,\max} }}$          
            \STATE Pull arm $I_t \sim p_t$ and observe reward $r_t \sim \nu_{I_t}$.
        \ENDIF
    \ENDFOR
    \end{algorithmic}
    \caption{\gexpklms: a general KL-Maillard Sampling Algorithm for OPED Rewards} 
    \label{alg:general-exp-kl-ms}
    \end{algorithm}

The choice of the inverse temperature function $L(\cdot)$ determines the balance between exploration and exploitation.
Our main algorithm, \expklms, chooses $L(x) = x - 1$.
We also analyze variants of the algorithm where $L(x) = x$ and $L(x) = x/d$ for $d$ is a constant and $d > 1$.
In an idealized setting where the estimation of the KL-divergence is accurate, the probability of pulling an arm, $p_{t, a}$, is proportional to $\exp\{- N_{t-1, a} \Dcal(\nu_a, \nu_{\max})\}$. Consequently, we expect the number of times arm $a$ is pulled to be approximately $\frac{\ln(T)}{\Dcal(\nu_a, \nu_{\max})}$ over time $T$. This aligns with the asymptotically optimal number of pulls of arm $a$ for any consistent algorithm, established in~\citet{lai85asymptotically}.

\Cref{alg:general-exp-kl-ms} generalizes algorithms from prior works in the following way: if the reward distributions are Bernoulli, we set $L(k) = k$, and \gexpklms becomes KL-MS~\citep{qin2023kullback}; if the reward distributions are Gaussian with fixed variance, we also set $L(k) = k$, and \gexpklms becomes MS~\citep{bian2022maillard}. Compared to the MED algorithm~\citep{honda2011asymptotically}, \gexpklms works with the OPED reward setting, while MED does not come with that and their distribution set is built in the setting of a finite support set, which includes all historical records.
