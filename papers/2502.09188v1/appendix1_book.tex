\section{Details of Book and Paper Processing Pipeline}
For data extraction and OCR conversion, we utilized a range of Python libraries, including Selenium\footnote{\url{https://selenium-python.readthedocs.io/}}, BeautifulSoup\footnote{\url{https://beautiful-soup-4.readthedocs.io/en/latest/}}, and Pytesseract\footnote{\url{https://github.com/madmaze/pytesseract}}. Text-based PDFs were converted using lightweight tools such as pdf2image\footnote{\url{https://github.com/Belval/pdf2image}}, while image-based PDFs required more advanced processing with Pytesseract and Fitz\footnote{\url{https://github.com/pymupdf/PyMuPDF}}. To improve accuracy, we employed an iterative approach, applying multiple tools to the same documents and manually inspecting those with errors before refining the extraction process.

\label{sec:appendixB}
\subsection{Text-based PDFs: Detailed Processing}
After removing corrupted or non-Persian documents, we apply a 3-stage processing pipeline involving document-level, character-level and line-level processing. Unlike documents from web, we first apply the document-level processing to avoid redundant processing. 

\subsubsection{Document-level Processing}
In the first stage, we applied document-level processing, where a document was viewed holistically. If it met any of the following criteria, it was eliminated: 
\begin{itemize} 
    \item Documents with fewer than 150 space-separated words. 
    \item Documents containing less than 50\% Persian characters. 
    \item Documents with an average word length of fewer than 3 characters or greater than 10 characters. 
    \item Documents with a numeric or symbolic character ratio exceeding 0.8. 
    \item Documents where over 80\% of the lines were considered short, defined as containing fewer than four space-separated words. 
    \item Documents where fewer than 10\% of the words were Persian or Arabic stopwords. 
\end{itemize}

\subsubsection{Character-level Processing}
Given that many of the books contained long Arabic text, which needed to be preserved, we only normalized non-Arabic, non-English, and non-Persian characters and symbols to their Persian format. We did not remove I'rab (diacritics). Standard procedures, such as replacing consecutive repeated characters, normalizing newlines, and removing non-standard Unicode characters, were applied as in previous section, though with additional Unicode characters added to the filtering set. Furthermore, nonsensical patterns detected in the text, which added no value and increased noise, were removed. These patterns included: 
\begin{itemize} 
    \item Website links to the source of the document. 
    \item Repeated occurrences of the book's title at the top or bottom of pages. 
    \item Page numbers in various forms, such as \FR{صفحه۱}, \FR{صفحه ۱}, \FR{صفحه۱ از ۲۰۰}, \FR{ص۱}, \FR{صفحه ۱ از کتاب ...}, etc. 
    \item Tags related to cover pages. \item Errors or tags related to multimedia, such as \texttt{'Your browser does not support the audio tag.'} 
    \item Images or tables converted to 'UNKNOWN' strings. 
    \item Personal information, such as phone numbers, email addresses, account numbers, and credit card numbers (e.g., Shaba numbers), which were found at the end of some books and at the beginning of papers. 
\end{itemize}

\subsubsection{Line-level Processing}
Following character-level processing, we performed line-level processing to remove lines that contained formulas or tables that were corrupted during the conversion from PDF to text. As part of this stage, the following types of lines or paragraphs were removed: 
\begin{itemize} 
    \item Lines with a numeric character ratio exceeding 0.8. 
    \item Lines with a symbolic character ratio exceeding 0.8. 
    \item Lines that were repeated multiple times within the document, which often included hidden watermarks or the repeated mention of the book's title. 
\end{itemize}

\subsubsection{Deduplication}
To avoid redundancy, a deduplication process was applied using MinHash and Locality-Sensitive Hashing (LSH). We deduplicated documents within each source, ensuring that only unique documents were retained.

\subsection{Image-based PDFs (OCR): Detailed Processing}
For OCR-processed documents, the primary issue was the introduction of errors during text extraction. To mitigate this, we employed the following steps:
\begin{enumerate}
    \item Removed content preceding the keywords section, which was often corrupted, using regex patterns to detect specific document structures.
    \item Removed documents with more than 5\% out-of-vocabulary tokens.
    \item REmoved papers containing more than 10 words exceeding 15 characters, indicative of merged words.
\end{enumerate}
Although some OCR-generated text still contains minor issues, such as occasional word merging, these are manageable with model tokenizers and do not significantly affect overall context and understanding.

The boxplot in Figure~\ref{fig:baxplot2} shows the token count distribution across different document sources. Books have a notably higher median token count and broader range compared to papers. Both image-based and text-based papers display lower token counts with numerous outliers, indicating diverse token lengths. Text-based papers have a lower median as they contain paper summaries as well as internal papers. Image-based papers also contain high-quality and longer scientific documents. 