\subsection{Web-based Crawled Data}
Web crawling is a common and efficient method for collecting data in any language. Websites offer a vast range of valuable information and, given their structured nature and wide availability, can largely be crawled automatically. As a result, web data is frequently used as the primary source for constructing large-scale text datasets. However, while the bulk collection of web data is straightforward, extracting meaningful content from irrelevant elements such as metadata, advertisements, and embedded links remains challenging. Web pages often contain spam-like elements, which complicates the cleaning process and increases the likelihood of errors.

Most web-based datasets begin with basic steps such as text extraction and language detection, often followed by optional URL filtering to exclude content deemed inappropriate or irrelevant. Further preprocessing steps are applied, followed by deduplication to ensure data quality and minimize redundancy. We adopt a similar approach in preprocessing the web data collected for the Matina corpus.

Matina's web-based data is divided into two parts: data crawled by our team and data taken from two public databases using the Common Crawl \citep{commoncrawl} dataset. This dual-source strategy uses both proprietary and publically available data to increase the corpus's breadth and diversity.

In any language, certain domains are recognized for their reliability and high-quality information. We identified such domains in Persian and crawled them to extract relevant textual content. This step helped minimize the inclusion of irrelevant elements such as advertisements, tags, or comments. Text extracted from headings and paragraphs was merged to form unified documents, with additional informative fields (e.g., summaries or subheadings) incorporated as metadata, if available. Because these domains were manually selected, language detection and URL filtering were unnecessary. We also ensured that the selected URLs did not contain harmful, sensitive, or adult content.

For the public datasets, Madlad-400 \citep{kudugunta2024madlad} and CulturaX \citep{nguyen2023culturax}, the initial preprocessing steps—such as language detection, text extraction, and URL filtering—had already been completed by the dataset providers. These datasets also included filters for toxic or harmful content, which allowed us to directly proceed to the next stages of preprocessing. While both datasets applied generic filters—such as language mismatch detection, character ratio checks, and word/sentence length thresholds, these filters were not language-specific. Therefore, we processed data from these sources similarly to the web data we crawled ourselves. After applying the processing on data sourced from web and the public datasets, there remained 64.3B tokens with an average document length of 1,141.8 tokens. 

After inspecting samples from various domains, we defined heuristic functions to modify documents and remove those deemed irrelevant. These heuristics were inspired by preprocessing pipelines adopted in BLOOM \citep{le2023bloom}, MassiveText \citep{muennighoff2022massiveText}, and RefineWeb \citep{penedo2023refinedweb}, but we tailored them to the specific characteristics of our data and added multiple other processing functions. 

Our preprocessing pipeline for web-based data encompasses three primary stages: character-level processing, line and paragraph-level processing, and document-level processing. Each stage employs a series of targeted operations to enhance data quality, ensure linguistic consistency, and eliminate redundancies.  Appendix~\ref{sec:appendixA} provides a full explanation of each step in the preprocessing and deduplication procedures.  

\textbf{Character-level processing} involves normalizing Persian characters, mapping symbols and numbers to their Persian equivalents, limiting the occurrence of repeated characters, standardizing newline characters, and removing non-standard Unicode symbols. This stage ensures that the text adheres to consistent encoding standards and minimizes the presence of corrupted or irrelevant characters.

\textbf{Line and paragraph-level processing} focuses on the structural integrity of the text by removing HTML and JavaScript tags, handling custom structures specific to certain domains, filtering out lines with excessive special characters, and eliminating short or incomplete lines that do not contribute meaningful content.

\textbf{Document-level processing} entails a comprehensive evaluation of each document's relevance and quality. Documents are discarded based on criteria such as insufficient length, predominance of non-Persian content, excessive repetition of words, high proportion of short lines, and the presence of out-of-vocabulary (OOV) words. These filters ensure that only high-quality, relevant, and linguistically coherent documents are retained in the corpus.

After cleaning the documents, we apply a deduplication step to mitigate data redundancy, a crucial aspect of the preprocessing pipeline highlighted in several studies \citep{gao2020pile, penedo2023refinedweb, le2023bloom}.  Utilizing the MinHash algorithm \citep{broder1997minhash}, we efficiently identify and eliminate both exact and near-duplicate documents, thereby enhancing the corpus's uniqueness.

For two manually inspected domains, \href{https://virgool.io/}{Virgool}\footnote{\href{https://virgool.io/}{https://virgool.io/}} and \href{https://en.wikishia.net/}{WikiShia}\footnote{\href{https://en.wikishia.net/}{https://en.wikishia.net/}}, we adopted a tailored processing approach to account for domain-specific characteristics. Virgool's diverse blog posts required relaxed filtering criteria to preserve technical content, while WikiShia's recursive linking and bilingual content deemed for  specialized deduplication and language handling techniques to maintain content integrity and cultural relevance.