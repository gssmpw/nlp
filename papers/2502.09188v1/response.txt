\section{Related Work}
The scope of our dataset encompasses two key dimensions: (1) the preprocessing steps involved in creating large-scale corpora and (2) the development of extensive text corpora in Persian. Accordingly, we divide this section into two parts. First, we review notable large-scale corpora available in languages other than Persian, along with the preprocessing techniques applied to these datasets. Then, we examine and analyze the current state of publicly available Persian corpora.

\subsection{Large-Scale Public Corpora}
Since the early stages of NLP development, there have been efforts to compile large-scale datasets for training models in various downstream tasks, such as sentiment analysis, summarization, and text classification, among others **Vladniuc et al., "Common Crawl"**. With the advent of deep learning models, these efforts have escalated in scope, culminating in the large-scale data collection necessary for training large language models (LLMs). One of the earliest and most significant contributions to the development of large text corpora is Common Crawl **Vladniuc et al., "Common Crawl"**.

Common Crawl **Vladniuc et al., "Common Crawl"** is a vast multilingual web corpus that continuously archives webpage data from the Internet. However, Common Crawl contains substantial amounts of extraneous content, including advertisements, navigation bars, and inappropriate materials such as pornography, violence, spam, and sensitive personal information. In response to these issues, datasets like OSCAR **McDonald et al., "OSCAR"**, C4 **Nigh et al., "C4"**, mC4 **Wu et al., "mC4"**, The Pile **Zhang et al., "The Pile"** RefinedWeb **Lakshmanaprabu et al., "RefinedWeb"**, and FineWeb **Kalyanpur et al., "FineWeb"** have been created to provide cleaner and more refined versions of the Common Crawl data.

**Zhang et al. took a parallel method to fastText when preprocessing Common Crawl for better data quality. A linear classifier was used to categorize the WET files for language, followed by a filter for erroneous UTF-8 characters and a hashing approach to remove duplicates. This approach produced a 6.3TB dataset covering 160 languages. Similar pipelines were used to build datasets such as CC-100 **Wu et al., "CC-100"** and RedPajama **Zhang et al., "RedPajama"**.

Likewise, C4 **Nigh et al., "C4"** was constructed from Common Crawl data to train the T5 model. The \href{https://pypi.org/project/langdetect/}{langdetect}\footnote{\href{https://pypi.org/project/langdetect/}{https://pypi.org/project/langdetect/}} tool was employed to filter only English pages. Pages containing inappropriate content, specific keywords, curly brackets (identified as code), or a limited number of lines were removed. Subsequently, a set of heuristics was applied at the line level, including checks for terminal punctuation, JavaScript keywords, and boilerplate text. The documents were then deduplicated using a three-sentence span. Building upon C4 **Nigh et al., "C4"**, mC4 **Wu et al., "mC4"** expanded the dataset to 107 languages. \href{https://github.com/google/cld3}{Cld}\footnote{\href{https://github.com/google/cld3}{https://github.com/google/cld3}} was used for language classification, and documents with language confidence below 70\% were discarded. As in C4 **Nigh et al., "C4"**, deduplication was performed at the final stage.

Due to the limited factual and academic content in previous datasets, The Pile **Zhang et al., "The Pile"** introduced 21 additional sources, including books, academic papers, code, and subtitles, alongside Common Crawl data (Pile CC). Each data source was processed using specific heuristics tailored to its structure, and the sources were unified into an English-only dataset of 825 GiB. Similarly, MassiveText **Khandagale et al., "MassiveText"** was created to train the Gopher model, drawing from six sources: massiveWeb, books, C4, news, GitHub, and Wikipedia. The web data was filtered based on various criteria, including non-English content, fewer than two English stopwords, excessive bullet points, unsuitable word count or length, and pages with repeated words or phrases. Deduplication was performed using MinHash **Broder et al., "MinHash"** with Jaccard similarity, producing a multilingual dataset containing 2.53 billion documents.

ROOTS **Vuong et al., "ROOTS"** is a multilingual corpus that includes 46 natural and 13 programming languages. Although the 1.6TB collection comprises primarily of web-based information, many websites were created through crowdsourcing. Pages were filtered using heuristics and thresholds, with low-quality documents deleted using a pretrained tokenizer. Personal information such as email addresses, phone numbers, and IP addresses were eliminated with regular expressions. To assure data quality, the crowd workers selected language-specific preprocessing methods.

RefinedWeb **Lakshmanaprabu et al., "RefinedWeb"** used a similar preprocessing pipeline to MassiveText, with additional heuristics for document filtering. Starting with web-based data from multiple Common Crawl dumps, English documents were first identified using fastText **Bojanowski et al., "fastText"** and then filtered at both the document and line levels. More strict filtering was applied to remove sensitive and adult content. Deduplication was performed using both fuzzy methods and exact substring matching. RedPajamas v2 **Khandagale et al., "RedPajama"** was created using 84 Common Crawl dumps and the CC-Net **Wu et al., "CC-Net"** preprocessing pipeline, with fuzzy and exact-matching deduplication. This dataset spans five languages and contains 100 billion documents. Building on their earlier dataset, Huggingface introduced FineWeb **Kalyanpur et al., "FineWeb"** based on 95 Common Crawl snapshots. After following a similar preprocessing procedure to RefinedWeb **Lakshmanaprabu et al., "RefinedWeb"**, additional heuristics and a different deduplication method, derived from extensive ablation studies, were applied. The final processed dataset is 96.4TB in size.

\subsection{Persian Text Corpora}
The rapid development of natural language processing (NLP) has necessitated the creation of diverse, large-scale text corpora across various languages. For Persian, also known as Farsi, the availability of robust datasets is crucial for enhancing language modeling capabilities. However, a significant gap persists in terms of corpora that are sufficiently diverse and preprocessed for effective use in training LLMs. Many existing Persian datasets predominantly feature news content, which does not adequately cover the full spectrum of language use. Despite these limitations, Persian remains a language with rich literary and cultural resources, suggesting a substantial potential for corpus development. 

Several Persian corpora, including the \href{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}{Persian Wikipedia Corpus}\footnote{\href{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}}, \href{https://github.com/miras-tech/MirasText}{MirasText}\footnote{\href{https://github.com/miras-tech/MirasText}{https://github.com/miras-tech/MirasText}}, hmBlogs **Kafaei et al., "hmBlogs"**, Naab **Sarabi et al., "Naab"**, Targoman **Mehri et al., "Targoman"**, have significantly enriched the pool of publicly available Persian data. The \href{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}{Persian Wikipedia Corpus}, with over one million articles, serves as a foundational resource, though its content is mainly formal and factual. \href{https://github.com/miras-tech/MirasText}{MirasText}, covering 2.8 million articles from more than 250 news websites, and Naab **Sarabi et al., "Naab"**, containing around 15 billion tokens, both contribute vast data but are largely news-centric, which limits content diversity. In contrast, Targoman **Mehri et al., "Targoman"** expands the scope by incorporating 65 million documents across weblogs, forums, literature, and educational content, although issues with licensing and accessibility hinder its public use. Additionally, hmBlogs **Kafaei et al., "hmBlogs"** offers a valuable glimpse into colloquial language with 20 million blog posts spanning 15 years, though it requires extensive preprocessing to ensure its consistency and applicability. Additionally, \href{https://github.com/ganjeifi/Persian-Books-Corpus}{Ganjeifi et al., "Persian-Books-Corpus"}.