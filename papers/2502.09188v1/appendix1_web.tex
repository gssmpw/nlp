\section{Details of Web-Based Document Processing Pipeline}
\label{sec:appendixA}
\subsection{Character-level Processing}
Character-level processing is the initial step in our preprocessing pipeline, aimed at standardizing and cleaning the text at the most granular level. This stage involves several key operations:

\begin{enumerate}
    \item Unicode Normalization: We convert all characters to their Persian equivalents, and remove Arabic I'rab marks. We then normalize space and tab characters to the standard keyboard space, with exceptions made for the half-space character used in specific Persian words.
    
    \item Symbol and Number Mapping: We map symbols and numbers not belonging to the English, Arabic, or Persian character sets to their Persian equivalents using the \href{https://github.com/arushadev/piraye}{Piraye} library. This is to ensure language consistency in the dataset.
    
    \item Repeated Characters: We identify any character repeated more than three times in sequence, typically used for emphasis, and truncate it to three occurrences to maintain readability and consistency.
    
    \item Newline Normalization: We merge consecutive newlines, including those with spaces or tabs, to standardize line breaks across documents.
    
    \item Non-standard Unicode Removal: By taking multiple samples from the data we found that there are chracters within the text that are not standard. We then detect and remove these non-standard Unicode characters, such as special emojis or corrupted symbols (e.g., bordered question marks) based on our predefined criteria. 
\end{enumerate}

\subsection{Line and Paragraph-level Processing}
Once character-level normalization is complete, we focus on the structural elements of the text. This stage involves:
\begin{enumerate}
    \item HTML and JavaScript Tag Removal: We identify lines containing HTML or JavaScript tags and functions using regular expressions and replace them with newlines.

    \item Custom Structures Handling: We inspected that some domains include unique tag structures that do not follow the format of standard tags (JavaScript and HTML) which are not captured by regular expressions. We identify and remove these using structures. 

    \item Special Character Ratio Filtering: We calculate the ratio of special characters (e.g., emojis, symbols, numbers) to total characters in each line. Lines exceeding a 0.85 ratio are removed, particularly targeting lines corrupted during text extraction, such as tables or formulas.

    \item Short Line Removal: We inspected that certain sources contain incomplete or irrelevant information in the few short lines at the start of the content. We therefore remove lines shorter these specific sources.
\end{enumerate}

\subsection{Document-level Processing}
The final stage involves document-level processing. We treat documents as a whole and remove those that meet any of the following criteria: (we refer to words as space-separated text sequences that are neither a number nor a symbol)
\begin{itemize}
    \item Short Length Filtering: Documents shorter than 30 words are removed, as they are either corrupted or devoid of useful information.

    \item Non-Persian Content Removal: Documents where over 50\% of characters are non-Persian are eliminated to maintain linguistic consistency and relevance.

    \item Repeated Words Elimination: Documents where more than 50\% of the words are identical are eliminated, targeting pages that use SEO techniques or lack informative content.

    \item Short Lines Proportion Filtering: Documents with over 50\% of lines shorter than 15 words are discarded, as they typically consist of lists or content tables.

    \item Out-of-Vocabulary (OOV) Words Filtering: Specifically for the CulturaX \citep{nguyen2023culturax} dataset, documents containing more than 2.5\% OOV words are removed to exclude irrelevant content such as code fragments or corrupted text.
\end{itemize} 

Finally, we eliminate any repeated empty newlines resulted from the removal of lines or paragraphs to maintain the document's structural integrity.

\subsection{Deduplication Process}
To address data redundancy, we leverage the MinHash algorithm \citep{broder1997minhash}, a well-established technique for efficient similarity detection in large collections of text. The deduplication pipeline consists of the following steps:\citep{broder1997minhash}. The process involves several steps:
\begin{enumerate}
    \item Text Normalization: We normalize Text within all documents by unifying recurring elements like days of the week and removing numbers and symbols. This normalization step is particularly crucial for content from websites that repost similar material daily. By handling these elements, we aim to reduce semantic duplicates.

    \item Tokenization and Hashing: We tokenize each document into 13-grams, and hash values are computed using 128 distinct hashing functions to capture text patterns.

    \item LeanMeanHash Compression: We then segment the hash values into eight sliding windows and processed using the LeanMeanHash algorithm, which compresses the hash signatures for efficient storage and comparison.

    \item Graph-based Similarity Detection: Finally, we construct a graph in which each node represents a document, and edges connect nodes based on hash similarity. By identifying connected components within this graph, only one representative document per component is retained, effectively removing duplicates and near-duplicates.
\end{enumerate}

This deduplication strategy ensures a significant reduction in redundant data, enhancing the corpus's quality and uniqueness, and facilitating better model generalization by preventing overfitting on repeated content.

\subsection{Domain-specific Processing}
Since \href{https://virgool.io/}{Virgool} and \href{https://en.wikishia.net/}{WikiShia} domains contain highly relevant content related to Persian culture and religion, it is  necessary to modify our standard preprocessing pipeline to avoid information loss. We perform the following specialized preprocessings.

For Virgool, which primarily features blog posts on diverse topics, including programming languages and mathematical content, applying the standard preprocessing thresholds resulted in the removal of valuable content. To address this, we relaxed certain filtering criteria: 
\begin{itemize}
    \item By pass the removal of numbers and symbols to preserve technical content.
    \item Incorporate more complex regular expressions to accurately detect and remove residual HTML tags or functions that were not filtered out by the standard pipeline.
    \item Adjust the ratio of Persian stopwords to lower values, and the threshold for the proportion of short lines (in relation to the total number of lines) was increased, ensuring the retention of concise but informative posts.
    \item Employed a privacy-preserving step to remove any personal data found in public blogs, even though the blogs are publicly accessible. This aspect of our pipeline will be discussed in detail in the subsequent section.
\end{itemize}

Another unique challenge with WikiShia was the significant presence of Arabic text, particularly due to references to the Quran and Arabic scholarly sources. To address this, we adjusted our processing thresholds: we increased the tolerance for Arabic stopwords while simultaneously lowering the threshold for Persian stopwords. This adjustment allowed us to better capture the bilingual nature of the content.


For WikiShia which includes bilingual content and presents challenges related to content duplication, we performe the following:

\begin{itemize}
    \item Content Duplication: our recursive crawling process exposed a significant issue of content duplication. Multiple URLs often corresponded to the same page, differing only by a minor subheading. Additionally, the site includes detailed descriptions of events associated with specific dates, resulting in multiple unique URLs hosting nearly identical content tied to calendar events.
    To address this, we employed an exact-match deduplication strategy using MinHashLSH \citep{leskovec2020lsh}. Unlike our standard deduplication pipeline, we opted not to normalize or remove dates, numbers, or references to specific days of the week, as these elements are critical for preserving the chronological and cultural relevance of the content. By applying this approach, we were able to eliminate documents with a similarity threshold of 98\% or higher.

    \item Bilingual Content Handling: Another unique challenge with WikiShia was the significant presence of Arabic text, particularly due to references to the Quran and Arabic scholarly sources. To address this, we adjusted our processing thresholds. The tolerance for Arabic stopwords was increased, while the threshold for Persian stopwords was lowered, effectively capturing the bilingual nature of the content.
\end{itemize}

The boxplot in Figure~\ref{fig:baxplot3} illustrates the token count distribution across three document sources we had for web-based data. The results are in tokens by the Llama3.1 \citep{dubey2024llama3.1} and after the application of our comprehensive preprocessing pipeline and deduplication. Data crawled by the team, named Web-crawled, show the widest range, with a median around 1000 tokens and some documents extending beyond \(10^5\) tokens. Madlad exhibits a slightly narrower distribution but still maintains substantial variation. CulturaX demonstrates the most compact distribution, with a lower median and maximum token count. These distributions highlight the success of our preprocessing in maintaining diversity while standardizing document lengths. The presence of outliers, particularly in the Web-crawled and Madlad sources, indicates that our pipeline preserves some longer, potentially information-rich documents. This final data composition ensures a balance between consistency and variety, crucial for robust model training and generalization.
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./boxplot_web.png}
  \caption{Document Length Distribution For Web-based Crawled Data}
  \label{fig:baxplot3}
\end{figure}
