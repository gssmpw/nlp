\section{Related Work}
The scope of our dataset encompasses two key dimensions: (1) the preprocessing steps involved in creating large-scale corpora and (2) the development of extensive text corpora in Persian. Accordingly, we divide this section into two parts. First, we review notable large-scale corpora available in languages other than Persian, along with the preprocessing techniques applied to these datasets. Then, we examine and analyze the current state of publicly available Persian corpora.

\subsection{Large-Scale Public Corpora}
Since the early stages of NLP development, there have been efforts to compile large-scale datasets for training models in various downstream tasks, such as sentiment analysis, summarization, and text classification, among others \citep{glockner2018snli, narayan2018xsum, wang2019glue}. With the advent of deep learning models, these efforts have escalated in scope, culminating in the large-scale data collection necessary for training large language models (LLMs). One of the earliest and most significant contributions to the development of large text corpora is Common Crawl \citep{commoncrawl}.

Common Crawl \citep{commoncrawl} is a vast multilingual web corpus that continuously archives webpage data from the Internet. However, Common Crawl contains substantial amounts of extraneous content, including advertisements, navigation bars, and inappropriate materials such as pornography, violence, spam, and sensitive personal information. In response to these issues, datasets like OSCAR \citep{oscar}, C4 \citep{raffel2020c4}, mC4 \citep{xue2020mt5}, The Pile \citep{gao2020pile} RefinedWeb \citep{penedo2023refinedweb}, and FineWeb \citep{penedo2024fineweb}  have been created to provide cleaner and more refined versions of the Common Crawl data.

\citet{oscar} took a parallel method to fastText \citep{athiwaratkun2018fasttext} when preprocessing Common Crawl for better data quality. A linear classifier was used to categorize the WET files for language, followed by a filter for erroneous UTF-8 characters and a hashing approach to remove duplicates. This approach produced a 6.3TB dataset covering 160 languages. Similar pipelines were used to build datasets such as CC-100 \citep{conneau2019cc100} and RedPajama \citep{together2023redpajama}.

Likewise, C4 \citep{raffel2020c4} was constructed from Common Crawl data to train the T5 model. The \href{https://pypi.org/project/langdetect/}{langdetect}\footnote{\href{https://pypi.org/project/langdetect/}{https://pypi.org/project/langdetect/}} tool was employed to filter only English pages. Pages containing inappropriate content, specific keywords, curly brackets (identified as code), or a limited number of lines were removed. Subsequently, a set of heuristics was applied at the line level, including checks for terminal punctuation, JavaScript keywords, and boilerplate text. The documents were then deduplicated using a three-sentence span. Building upon C4 \citep{raffel2020c4}, mC4 \citep{xue2020mt5} expanded the dataset to 107 languages. \href{https://github.com/google/cld3}{Cld}\footnote{\href{https://github.com/google/cld3}{https://github.com/google/cld3}} was used for language classification, and documents with language confidence below 70\% were discarded. As in C4 \citep{raffel2020c4}, deduplication was performed at the final stage.

Due to the limited factual and academic content in previous datasets, The Pile \citep{gao2020pile} introduced 21 additional sources, including books, academic papers, code, and subtitles, alongside Common Crawl data (Pile CC). Each data source was processed using specific heuristics tailored to its structure, and the sources were unified into an English-only dataset of 825 GiB. Similarly, MassiveText \citep{muennighoff2022massiveText} was created to train the Gopher model, drawing from six sources: massiveWeb, books, C4, news, GitHub, and Wikipedia. The web data was filtered based on various criteria, including non-English content, fewer than two English stopwords, excessive bullet points, unsuitable word count or length, and pages with repeated words or phrases. Deduplication was performed using MinHash \citep{broder1997minhash} with Jaccard similarity, producing a multilingual dataset containing 2.53 billion documents.

ROOTS \citep{laurenccon2022roots} is a multilingual corpus that includes 46 natural and 13 programming languages. Although the 1.6TB collection comprises primarily of web-based information, many websites were created through crowdsourcing. Pages were filtered using heuristics and thresholds, with low-quality documents deleted using a pretrained tokenizer. Personal information such as email addresses, phone numbers, and IP addresses were eliminated with regular expressions. To assure data quality, the crowd workers selected language-specific preprocessing methods.

RefinedWeb \citep{penedo2023refinedweb} used a similar preprocessing pipeline to MassiveText, with additional heuristics for document filtering. Starting with web-based data from multiple Common Crawl dumps, English documents were first identified using fastText \citep{athiwaratkun2018fasttext} and then filtered at both the document and line levels. More strict filtering was applied to remove sensitive and adult content. Deduplication was performed using both fuzzy methods and exact substring matching. RedPajamas v2 \citep{together2023redpajama} was created using 84 Common Crawl dumps and the CC-Net \citep{wenzek2019ccnet} preprocessing pipeline, with fuzzy and exact-matching deduplication. This dataset spans five languages and contains 100 billion documents. Building on their earlier dataset, Huggingface introduced FineWeb \citep{penedo2024fineweb} based on 95 Common Crawl snapshots. After following a similar preprocessing procedure to RefinedWeb \citep{penedo2023refinedweb}, additional heuristics and a different deduplication method, derived from extensive ablation studies, were applied. The final processed dataset is 96.4TB in size.

\subsection{Persian Text Corpora}
The rapid development of natural language processing (NLP) has necessitated the creation of diverse, large-scale text corpora across various languages. For Persian, also known as Farsi, the availability of robust datasets is crucial for enhancing language modeling capabilities. However, a significant gap persists in terms of corpora that are sufficiently diverse and preprocessed for effective use in training LLMs. Many existing Persian datasets predominantly feature news content, which does not adequately cover the full spectrum of language use. Despite these limitations, Persian remains a language with rich literary and cultural resources, suggesting a substantial potential for corpus development. 
%By merging and preprocessing available resources, we can construct a more comprehensive dataset for LLM applications.

Several Persian corpora, including the \href{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}{Persian Wikipedia Corpus}\footnote{\href{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}}, \href{https://github.com/miras-tech/MirasText}{MirasText}\footnote{\href{https://github.com/miras-tech/MirasText}{https://github.com/miras-tech/MirasText}}, hmBlogs \citep{khansari2021hmblogsbiggeneralpersian}, Naab \citep{naab}, Targoman \citep{targoman}, have significantly enriched the pool of publicly available Persian data. The \href{https://github.com/Text-Mining/Persian-Wikipedia-Corpus}{Persian Wikipedia Corpus}, with over one million articles, serves as a foundational resource, though its content is mainly formal and factual. \href{https://github.com/miras-tech/MirasText}{MirasText}, covering 2.8 million articles from more than 250 news websites, and Naab \citep{naab}, containing around 15 billion tokens, both contribute vast data but are largely news-centric, which limits content diversity. In contrast, Targoman \citep{targoman} expands the scope by incorporating 65 million documents across weblogs, forums, literature, and educational content, although issues with licensing and accessibility hinder its public use. Additionally, hmBlogs \citep{khansari2021hmblogsbiggeneralpersian} offers a valuable glimpse into colloquial language with 20 million blog posts spanning 15 years, though it requires extensive preprocessing to ensure its consistency and applicability. Additionally, \href{https://github.com/ganjoor}{Ganjoor}\footnote{\href{https://github.com/ganjoor}{https://github.com/ganjoor}} introduces classical Persian poetry from 12 poets, enhancing the stylistic and lexical range of the corpus and providing unique linguistic depth.

Parallel corpora, including TEP: Tehran English-Persian parallel corpus \citep{tiedemann-2012-parallel}, MIZAN \citep{kashefi2020mizanlargepersianenglishparallel}, and the \href{https://github.com/christos-c/bible-corpus}{Bible Corpus}\footnote{\href{https://github.com/christos-c/bible-corpus}{https://github.com/christos-c/bible-corpus}}, further extend the utility of Persian datasets by enabling translation tasks and bilingual language modeling. MIZAN \citep{kashefi2020mizanlargepersianenglishparallel}, containing one million sentence pairings between Persian and English, allows cross-linguistic studies and machine translation. However, the breadth of such corpora is frequently limited.

Standardized preprocessing techniques are required to improve Persian language modeling by filtering non-Farsi words, unifying Arabic and Farsi characters, and removing unnecessary content. These steps are crucial for creating a high-quality, clean corpus, as data quality directly impacts model performance in large language models (LLMs). While some datasets, such as Naab \citep{naab} and hmBlogs \citep{khansari2021hmblogsbiggeneralpersian}, offer preprocessed versions, this is still the exception rather than the norm in Persian corpus development.

% A concerted effort to merge and rigorously preprocess existing corpora could yield a more comprehensive dataset, capturing the full linguistic diversity of Persian across genres, ultimately benefiting LLM development and the computational representation of the Persian language.
