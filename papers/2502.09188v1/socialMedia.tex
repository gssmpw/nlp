\subsection{Social Media}

Although some books and blogs may include informal Persian text or dialogues, the overall proportion of such data is minimal. The data collected from web-based sources and books generally lacks unstructured or colloquial language. Social media, however, provides a rich source of unstructured and informal linguistic data. To capture this, we gathered Persian-language data from Twitter, as well as public channels and groups from Telegram and Eitaa (an Iranian chat application). After identifying relevant channels and groups, we crawled all associated messages and processed them using the pipeline described for web-based data, with thresholds tailored to social media content. Additional processing steps we applied are outlined below.

Upon examination, we found that shorter messages were mostly replies, often lacking substantive content or containing inappropriate language. These messages were filtered out. We also identified hashtags embedded within the text and at the end of messages. Hashtags within the text were retained to preserve context, while those at the end, frequently related to political or social topics and often irrelevant to the main content, were removed. We employed regular expressions (regex) to remove channel IDs and URLs, ensuring that irrelevant content was minimized.

A notable difference in processing social media data was the deduplication strategy. We observed that many messages from different sources differed only in date or pricingâ€”typically for goods, gold, silver, or cryptocurrencies. To address this, we removed all numeric values and dates before deduplication. After identifying and eliminating duplicate entries, we restored the original content, including numbers and dates, for the final dataset. This method ensured that informative variations were preserved while content containing no new knowledge was removed. 