\section{Introduction}
Since the introduction of the transformer architecture \citep{vaswani2017attention}, natural language processing (NLP) has advanced rapidly, transforming many language-related tasks. Transformer-based models, like BERT \citep{devlin2018bert} and GPT-2 \citep{radford2018improving}, initially focused on tasks like sentiment analysis, translation, and summarization. However, with the development of large-scale language models (LLMs), such as GPT-3 \citep{brown2020language} and later models \citep{touvron2023llama, le2023bloom, bai2023qwen, yang2024qwen2}, the research shifted towards more complex tasks, including generalization, creative problem-solving, and critical thinking.

The performance of these models, in both basic and advanced tasks, isn't just about model size or computational power—it's also heavily influenced by the quality and amount of training data. As a result, a lot of effort has gone into large-scale data collection and preprocessing \citep{gao2020pile, laurenccon2022roots, penedo2023refinedweb} to improve model capabilities and generalization.

While English dominates NLP research, there has been a growing effort to curate multilingual datasets \citep{wenzek2019ccnet, laurenccon2022roots, nguyen2023culturax, kudugunta2024madlad} and develop models capable of understanding multiple languages \citep{le2023bloom, touvron2023llama, yang2024qwen2}.

Despite Persian being widely spoken, it remains underrepresented in NLP research. Although both conventional models and LLMs can process Persian, their performance is often suboptimal, mainly because of the limited availability and poor quality of existing data. Persian text data is predominantly sourced from news websites and blogs, which often lack formal or factual content. Moreover, no standardized preprocessing pipeline exists to ensure the high quality of Persian datasets at the same level as those available for other languages.

To address this gap, we introduce the Matina Corpus, a 72.9 billion token Persian dataset designed for training language models. Unlike other Persian datasets \citep{targoman, sabeti2018mirastext}, the Matina Corpus has undergone a rigorous and well-designed preprocessing pipeline and a comprehensive deduplication process to ensure its high quality. The dataset includes not only publicly available Persian datasets but also introduces newly collected sources to ensure greater diversity and the inclusion of factual information. The diverse sources in the dataset make it suitable both for training large language models and for a variety of downstream tasks that require clean, high-quality Persian data.

The Matina Corpus includes Persian sections from Madlad \citep{kudugunta2024madlad}, CulturaX \citep{nguyen2023culturax}, and the most recent Persian Wikipedia update.  Each data source was processed differently, based on heuristics derived from careful evaluation and observation of the content. To ensure quality and avoid redundancy, deduplication was applied to related chunks of documents rather than across the entire dataset at once. The final corpus comprises a total of 72.9 B tokens, with an average document length of 1,106.5 across different sources (as summarized in Table~\ref{table:corpus-size}), illustrating both the breadth and depth of the dataset.

% The Matina Corpus is designed to enhance Persian NLP by supporting both the pretraining of large language models (LLMs) and the development of smaller models based on transformers and other architectures. The dataset enables models to perform a range of NLP tasks, including text classification, machine translation, and sentiment analysis. Incorporating this high-quality dataset into multilingual models' pretraining can improve their Persian language comprehension, addressing the resource gap and enhancing overall performance when processing Persian text.

The Matina Corpus is designed to enhance Persian NLP by supporting both the pretraining of large language models (LLMs) and the development of smaller models based on transformers and other architectures. It enables various NLP tasks, including text classification, machine translation, and sentiment analysis. To evaluate its impact, we continued the pretraining of XML-RoBERTa \citep{conneau2019unsupervised} on Matina and assessed its performance on sentiment analysis, text emotion detection, and named entity recognition, observing notable improvements over models trained on existing Persian datasets.

Furthermore, integrating this high-quality dataset into multilingual models enhances their Persian language comprehension, helping bridge the resource gap. To measure this effect, we used portions of the corpus to continue pretraining LLaMA 3.1 8B, achieving significant gains in Persian text understanding.

The rest of this paper is structured as follows: We begin by providing an overview of existing large corpora, along with the preprocessing pipelines applied to them, covering English, multilingual, and Persian datasets. Afterward, we introduce our corpus, dividing it into three distinct sections based on content, and offer details on the preprocessing steps we applied. We then assess the dataset’s effectiveness through model training and evaluation. Finally, we analyze the dataset, discuss its limitations, and conclude with a summary of our dataset.
% The rest of this paper is structured as follows: We begin by providing an overview of existing large corpora, along with the preprocessing pipelines applied to them, covering English, multilingual, and Persian datasets. Afterward, we introduce our corpus, dividing it into three distinct sections based on content, and offer details on the preprocessing steps we applied. Finally, we analyze the dataset, discuss its limitations, and conclude with a summary of our dataset.