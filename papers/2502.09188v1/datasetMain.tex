\section{Matina Corpus}
% The Matina corpus is built from a variety of data sources, each of which is processed based on its specific content characteristics. While these sources are divided into three main categories, the general preprocessing steps remain consistent, as shown in Figure~\ref{fig:pipeline}, with variations mainly in hyperparameters. However, certain sources require extra cleaning steps, which we describe in detail in their respective sections. In this section, we outline the data collection process, the applied preprocessing techniques, and the rationale behind the decisions made during these steps.

The Matina corpus is built from a variety of data sources, each of which is processed based on its specific content characteristics. Although these sources are grouped into three main categories, the overall preprocessing pipeline remains consistent, as depicted in Figure~\ref{fig:pipeline}, with variations primarily in hyperparameters. Certain sources, however, demand additional cleaning steps, which are detailed in their respective sections.

Figure~\ref{fig:boxplot1} visualizes the distribution of token counts across documents from each source, using a box plot to illustrate the variance in document length. These three categories—web-based crawled data, crawled books and papers, and social media—form the core of our dataset, each with distinct preprocessing requirements. In this section, we describe the data collection process, the preprocessing techniques applied, and the rationale behind the decisions made throughout these steps.

\begin{figure}[t]
  \includegraphics[trim=2.5cm 4cm 2.2cm 1.8cm , clip,width=\columnwidth]{./processingPipeline.pdf}
  \caption{The overall stages of processing pipeline of Matina Corpus.}
  \label{fig:pipeline}

\end{figure} 

\input{./webBased}
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./boxplot_all.png}
  \caption{Distribution of document length by source in the Matina Corpus. Length is determined by the log of the number of tokens using Llama3.1 \cite{dubey2024llama3.1} tokenizer.}
  \label{fig:boxplot1}
\end{figure}
\input{./booksAndPapers}

\input{./socialMedia}

\subsection{Final Dataset}
Applying the outlined preprocessing steps, including deduplication, resulted in a significant reduction in the number of documents. As illustrated in Figure~\ref{fig:barplot}, the overall document count decreased by an average of \textbf{24\%} after preprocessing, with a further reduction of \textbf{18.83\%} following deduplication.

The largest reduction occurred in social media content, particularly from Twitter and Telegram. Many Twitter posts were short and lacked meaningful content, while Telegram messages were often redundant, brief, and became even less informative after hashtags and links were removed. The special deduplication method we applied also identified many of these messages as duplicates. Although only 1.6\% of social media documents remained after processing, these retained documents were significantly longer, accounting for around 10\% of the total token count from the initial data.

Image-based academic papers also experienced a considerable loss during processing. In this category, the number of documents was nearly halved, as we applied multiple criteria to remove poor-quality documents. In contrast, text-based papers saw minimal loss, with only 2\% of documents eliminated during preprocessing. However, papers in this category contained more duplicates, which contributed to the reduction.

Books had the lowest proportion of document elimination during both preprocessing and deduplication. This reflects the higher quality of book content and the effectiveness of the methods used to extract data from PDF files.

For the web-crawled data, deduplication had a bigger impact than the initial preprocessing, with more documents being removed in this step. Even though we carefully tried to avoid duplicates during crawling, the nature of web crawling—often involving nested links—led to the inclusion of duplicates. Additionally, many news websites repost the same content across different agencies, which shows just how important thorough deduplication is for web-sourced data.

An interesting observation from the bar plot is that, although CulturaX FA \citep{nguyen2023culturax} and Madlad-400 FA \citep{kudugunta2024madlad} claim to have already undergone processing and deduplication, our language-specific preprocessing steps and content-specific deduplication further reduced their size. In Madlad-400 FA, only 7\% of documents were discarded, whereas nearly 70\% of CulturaX FA documents did not meet the qualifications for proper Persian data. This emphasizes the importance of language-specific processing and careful evaluation by native speakers to ensure data quality.

\begin{figure*}[t]
  \includegraphics[width=15.5cm]
  {./barplot.pdf}
  \centering
  \caption{Data reduction during preprocessing and deduplication varies significantly across sources. Social media shows the most drastic drop, with just 1.6\% of documents remaining after deduplication, while other sources retain between 56.1\% and 93.3\%. The three bars for each source represent the percentage of documents left after each stage. Overall, about 14\% of the initial documents remain.}
  \label{fig:barplot}
\end{figure*}