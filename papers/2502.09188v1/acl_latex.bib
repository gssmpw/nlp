@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, A},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{dubey2024llama3.1,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{le2023bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{team2024gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{wenzek2019ccnet,
  title={CCNet: Extracting high quality monolingual datasets from web crawl data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:1911.00359},
  year={2019}
}

@article{raffel2020c4,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{laurenccon2022roots,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}

@article{nguyen2023culturax,
  title={Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages},
  author={Nguyen, Thuat and Van Nguyen, Chien and Lai, Viet Dac and Man, Hieu and Ngo, Nghia Trung and Dernoncourt, Franck and Rossi, Ryan A and Nguyen, Thien Huu},
  journal={arXiv preprint arXiv:2309.09400},
  year={2023}
}

@article{kudugunta2024madlad,
  title={Madlad-400: A multilingual and document-level large audited dataset},
  author={Kudugunta, Sneha and Caswell, Isaac and Zhang, Biao and Garcia, Xavier and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.07461}, 
}

@misc{glockner2018snli,
      title={Breaking NLI Systems with Sentences that Require Simple Lexical Inferences}, 
      author={Max Glockner and Vered Shwartz and Yoav Goldberg},
      year={2018},
      eprint={1805.02266},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1805.02266}, 
}

@misc{narayan2018xsum,
      title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization}, 
      author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
      year={2018},
      eprint={1808.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1808.08745}, 
}

@inproceedings{sabeti2018mirastext,
  title={Mirastext: An automatically generated text corpus for persian},
  author={Sabeti, Behnam and Firouzjaee, Hossein Abedi and Choobbasti, Ali Janalizadeh and Najafabadi, SHE Mortazavi and Vaheb, Amir},
  booktitle={Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018)},
  year={2018}
}

@misc{targoman,
    author = {Targoman},
    title = {Targoman Dataset},
    year = {2022},
    url={https://oss.targoman.ir/}
}

@misc{commoncrawl,
  author       = {Common Crawl},
  title        = {Common Crawl Corpus},
  url = {https://commoncrawl.org},
  note         = {Accessed: 2024-09-28},
    year = {2008}
}

@misc{naab,
      title={naab: A ready-to-use plug-and-play corpus for Farsi}, 
      author={Sadra Sabouri and Elnaz Rahmati and Soroush Gooran and Hossein Sameti},
      year={2022},
      eprint={2208.13486},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.13486}, 
}

@misc{persianwiki,
    author = {Persian Wikipedia},
    title = {Persian-Wikipedia-Corpus Dataset},
    year = {2018},
    url={https://github.com/Text-Mining/Persian-Wikipedia-Corpus}
}

@misc{mirastext,
  author       = {miras tech},
  title        = {MirasText},
  year = {2020},
  url = {https://github.com/miras-tech/MirasText}
}
@inproceedings{tiedemann-2012-parallel,
    title = "Parallel Data, Tools and Interfaces in {OPUS}",
    author = {Tiedemann, J{\"o}rg},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf",
    pages = "2214--2218",
    abstract = "This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.",
}

@misc{ganjoor,
  author       = {ganjoor},
  title        = {Ganjoor},
  year = {2020},
  url = {https://github.com/ganjoor}
}


@misc{bible,
  author       = {bible},
  title        = {bible corpus},
  url = {https://github.com/christos-c/bible-corpus}
}
@misc{kashefi2020mizanlargepersianenglishparallel,
      title={MIZAN: A Large Persian-English Parallel Corpus}, 
      author={Omid Kashefi},
      year={2020},
      eprint={1801.02107},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1801.02107}, 
}


@misc{khansari2021hmblogsbiggeneralpersian,
      title={HmBlogs: A big general Persian corpus}, 
      author={Hamzeh Motahari Khansari and Mehrnoush Shamsfard},
      year={2021},
      eprint={2111.02362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.02362}, 
}

@inproceedings{oscar,
  title={Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures},
  author={Su{\'a}rez, Pedro Javier Ortiz and Sagot, Beno{\^\i}t and Romary, Laurent},
  booktitle={7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)},
  year={2019},
  organization={Leibniz-Institut f{\"u}r Deutsche Sprache}
}

@article{xue2020mt5,
  title={mt5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, L},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{penedo2024fineweb,
  title={The fineweb datasets: Decanting the web for the finest text data at scale},
  author={Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2406.17557},
  year={2024}
}

@article{athiwaratkun2018fasttext,
  title={Probabilistic fasttext for multi-sense word embeddings},
  author={Athiwaratkun, Ben and Wilson, Andrew Gordon and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1806.02901},
  year={2018}
}

@article{conneau2019cc100,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, A},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}

@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = {October},
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@article{muennighoff2022massiveText,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@inproceedings{broder1997minhash,
  title={On the resemblance and containment of documents},
  author={Broder, Andrei Z},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)},
  pages={21--29},
  year={1997},
  organization={IEEE}
}

@book{leskovec2020lsh,
  title={Mining of massive data sets},
  author={Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
  year={2020},
  publisher={Cambridge university press}
}

@article{sadraeijavaheri2024tookabert,
  title={TookaBERT: A Step Forward for Persian NLU},
  author={SadraeiJavaheri, MohammadAli and Moghaddaszadeh, Ali and Molazadeh, Milad and Naeiji, Fariba and Aghababaloo, Farnaz and Rafiee, Hamideh and Amirmahani, Zahra and Abedini, Tohid and Sheikhi, Fatemeh Zahra and Salehoof, Amirmohammad},
  journal={arXiv preprint arXiv:2407.16382},
  year={2024}
}


@article{ghafouri2023ariabert,
  title={AriaBERT: A Pre-trained Persian BERT Model for Natural Language Understanding},
  author={Ghafouri, Arash and Abbasi, Mohammad Amin and Naderi, Hassan},
  year={2023}
}

@misc{xlmro,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzm√°n and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.02116}, 
}

@article{alkhamissi2024investigating,
  title={Investigating cultural alignment of large language models},
  author={AlKhamissi, Badr and ElNokrashy, Muhammad and AlKhamissi, Mai and Diab, Mona},
  journal={arXiv preprint arXiv:2402.13231},
  year={2024}}

%3
@article{cao2023assessing,
  title={Assessing cross-cultural alignment between ChatGPT and human societies: An empirical study. arXiv},
  author={Cao, Y and Zhou, L and Lee, S and Cabello, L and Chen, M and Hershcovich, D},
  journal={Preprint posted online on March},
  volume={31},
  year={2023}}

@inproceedings{lu2023instag,
  title={\# instag: Instruction tagging for analyzing supervised fine-tuning of large language models},
  author={Lu, Keming and Yuan, Hongyi and Yuan, Zheng and Lin, Runji and Lin, Junyang and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}