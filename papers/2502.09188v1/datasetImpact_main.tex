\section{Assessing the Impact of the Matina Corpus}

A large-scale Persian corpus has numerous applications in NLP, including training transformer-based models for tasks such as summarization, sentiment analysis, emotion detection, question answering, sentence embeddings, and text retrieval. Additionally, such corpora play a crucial role in pretraining large language models (LLMs) and generating instructions for LLM post-training. To assess the effectiveness of the Matina Corpus, we conducted experiments on transformer-based model training and continued pretraining of LLMs. This section provides a detailed discussion of these experiments and their outcomes.

\subsection{Masked Language Model Training and Evaluation}
While LLMs have excelled in various NLP tasks such as sentiment analysis and named entity recognition (NER), there remains a need for lightweight models that can be easily fine-tuned for specific tasks and datasets. These models are typically built on transformer-based architectures, particularly masked language models trained on large-scale datasets. 

To address this need, we conducted continual pretraining of masked language models (MLMs), specifically XLM-RoBERTa Large \citep{xlmro}, on 54.69 billion tokens of our dataset. This extensive corpus facilitates the development of high-quality sentence embeddings, further refined by adapting the model into a Sentence-BERT architecture without Next Sentence Prediction (NSP). These enhancements yield more precise semantic representations, significantly improving Persian NLP tasks. By leveraging a well-curated dataset with rigorous preprocessing, our model effectively captures Persian linguistic nuances. 

To evaluate the effectiveness of Matina corpus in training transformer models, we benchmarked out Roberta-based model against existing models using datasets such as \textbf{Arman Emo}, \textbf{Pars-ABSA}, \textbf{PQUAD}, and \textbf{PEYMA}. As shown in Table \ref{tabelMLM}, our model demonstrates substantial performance gains, achieving \textbf{56.54} on \textbf{Arman Emo}, surpassing TookaBERT and AriaBERT, and \textbf{74.92} on \textbf{Pars-ABSA}, highlighting its robustness in aspect-based sentiment analysis. These results validate the impact of our dataset on enhancing Persian NLP performance, particularly within transformer-based architectures.

The success of our MLM underscores the crucial role of high-quality data in pretraining. By capturing Persian linguistic and cultural nuances, our model not only enhances task-specific performance but also advances the goal of developing inclusive and representative language technologies. This approach ensures that underrepresented languages like Persian receive the attention they deserve, fostering more equitable advancements in NLP.


\begin{table*}[t]
    \centering
    \caption{Results of Masked Language Models Evaluation.}
    \begin{tabular}{p{6.4cm}p{2cm}p{2cm}p{1.6cm}p{1.2cm}}
    \hline
        \textbf{Model} 
        & \rotatebox{0}{\textbf{Arman Emo}} 
        & \rotatebox{0}{\textbf{Pars-ABSA}} 
        % & \rotatebox{90}{\textbf{Taghche}} 
        % & \rotatebox{90}{\textbf{Snapp Food}} 
        % & \rotatebox{90}{\textbf{Digikala}} 
        % & \rotatebox{90}{\textbf{Digimag}} 
        % & \rotatebox{90}{\textbf{Persian News}} 
        & \rotatebox{0}{\textbf{PQUAD}} 
        % & \rotatebox{90}{\textbf{Reading Comp.}} 
        % & \rotatebox{90}{\textbf{DeepSentPars}} 
        & \rotatebox{0}{\textbf{PEYMA}} \\ \hline
        \textbf{XLM-RoBERTa (ours)} & \textbf{56.54}  & \textbf{74.92}  & 86.82 & \textbf{85.65} \\ 
        \textbf{TookaBERT} \citep{sadraeijavaheri2024tookabert} & 52.87 & 74.65  & 86.73 & 86.09 \\ 
        \textbf{AriaBERT} \citep{ghafouri2023ariabert} & 38.23 & 74.59  & 83.14 & 35.78 \\ 
        \textbf{XLM-RoBERTa} \citep{xlmro} & 32.48 & 74.18 & \textbf{87.6}  & 87.94 \\ 
        \textbf{mBERT} & 6.74 & 68.15 & 85.94 & 65.32 \\ \hline
    \end{tabular}
    \label{tabelMLM}
\end{table*}

\subsection{Large Language Model Pretraining and Evaluation}
\begin{table}[t]
  \centering
  \begin{tabular}{ll}
    \hline
    \textbf{Dataset}           & \textbf{Number of tokens} \\
    \hline
    Social and Politics           &    1.1 B   \\
    Cooking          &    15 M   \\
    \hline
  \end{tabular}
  \caption{\label{table:pretrainData}
    Number of tokens used for LLM continual pretraining. Tokens are counted by the Llama 3.1 \citep{dubey2024llama3.1} tokenizer. 
  }
\end{table}

\begin{figure}[b]
  \includegraphics[trim=0.4cm 0.9cm 0.8cm 0.3cm, width=\columnwidth]{./win_lose_tie_plot.pdf}
  \caption{Win rate of pretrained models over models without pretraining.}
  \label{fig:winLose}
\end{figure}

Pretraining is essential for transferring knowledge to LLMs, shaping their linguistic and factual understanding. However, multilingual LLMs often struggle with underrepresented languages like Persian and exhibit cultural biases favoring Western perspectives \cite{cao2023assessing, alkhamissi2024investigating} due to the dominance of English in their training data. This leads to diminished performance in other languages and cultures. Incorporating language-specific data during pretraining can help address this issue.

To evaluate the impact of our dataset on LLM training, we conducted the following experiment. We first tagged our dataset in an unsupervised manner using a procedure similar to InsTag \cite{lu2023instag}, categorizing it into multiple domains. From these, we selected two—social and politics and cooking—and extracted a subset of data from each domain. These domain-specific subsets were then used to train models. The token count for each domain is presented in Table \ref{table:pretrainData}. We then constructed large instruction datasets for these domains and fine-tuned LLaMA 3.2-Instruct 8B using two different approaches: (1) continued pretraining on the domain-specific data followed by instruction tuning, and (2) direct instruction tuning without additional pretraining. To evaluate model performance, we conducted a human evaluation, where annotators ranked model outputs in a win-lose format, indicating which model provided better responses to a held-out evaluation set derived from the instruction dataset.

The evaluation results, shown in Figure \ref{fig:winLose}, indicate that models benefit significantly from pretraining on even a relatively small dataset before instruction tuning. This effect is particularly noticeable in the cooking domain, where the pretrained model was preferred nearly twice as often as the model without pretraining. These findings highlight the effectiveness of the Matina Corpus in improving language models by providing high-quality, domain-specific data. Pretraining on a small, well-curated dataset not only enriches the model’s knowledge but also enhances its alignment with the target language and cultural context.


