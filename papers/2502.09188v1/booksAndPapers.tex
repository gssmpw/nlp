\subsection{Crawled Books and Papers}
Data collected from the web alone does not provide sufficient factual or literary content. To enrich our dataset, we also sourced publicly accessible books and academic papers from websites and social media channels. As demonstrated in Figure~\ref{fig:boxplot1}, the box plot of document length distribution clearly shows that books and papers contain significantly longer texts compared to web and social media content, making them more informative and comprehensive. This length, along with the depth of the content, further justifies the inclusion of these sources in our corpus. 

Since most of these sources provide data in PDF format, additional steps were required to convert PDFs into usable text. However, the limited accuracy of Persian OCR systems introduces challenges, particularly when processing PDFs that contain scanned images.

We divided the data from books and papers into two groups, each requiring different processing steps based on the nature of the data: Text-based PDFs and Image-based PDFs (OCR). Just like the data from web, the processing of books and papers involved a combination of document-level, character-level, and line-level operations to ensure data quality, as outlined below.

\subsubsection{Text-based PDFs}
Text-based PDFs primarily include books and academic papers sourced from Telegram channels and Persian websites. The PDFs were converted into text using several Python libraries. To ensure quality, we tested various tools on sample documents and applied low-level heuristic filters to remove corrupted or irrelevant content.

The filtering process involves removing documents with insufficient Persian content, short text lengths, or an excessive use of symbols. This stage ensures that only relevant and high-quality documents are retained. Following this initial filtering, we apply a preprocessing pipeline to address document, character, and line-level inconsistencies, ensuring the text is properly structured. Additional technical details on these steps, including character normalization, watermark removal, and deduplication, are provided in the Appendix~\ref{sec:appendixB}.

\subsubsection{Image-based PDFs (OCR)}
Many papers in our dataset were converted to text using image-based OCR due to the unavailability of text-based PDFs. Given the limitations of Persian OCR, errors were introduced during text extraction. To address this, we filtered out low-quality documents, focusing on those with a high percentage of nonsensical tokens or merged words. As a result, the dataset was refined to include 321,244 documents. The documents were then processed using steps  similar to those applied to web-based crawled data, with additional procedures. Additional information on the OCR-specific filtering methods is provided in the Appendix.