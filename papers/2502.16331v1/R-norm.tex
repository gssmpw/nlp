\section{$\mathcal{R}\mathrm{TV}^2$ of a kernel machine}\label{sec: rnorm}
%\rahul{idk why you still keep calling everything a classifier when there is no learning in this paper}
% The formula for $k_M$ for any two samples $x,x_0$ is
% \begin{align*}
%     k_M(\bx,\bx_0) = \exp\paren{-\frac{||\bx - \bx_0||_M^2}{2\sigma}}
% \end{align*}
% where $\sigma = 1$. Now wrt to the kernel $k_M$ for a given Mahalanbis matrix $M \succeq 0$, a kernel classifier $f$ (RFM in this case) has the form:
% \begin{align*}
%     f(x) = \sum_{i=1}^n \alpha_i k_M(\bx_i, \bx)
% \end{align*}
% where $\beta \in \reals^n$.
In this section, we study the $\mathcal{R}\mathrm{TV}^2$ of kernel machines in the RKHS $\cH$. We show that one can write an explicit computable form for the case when the dimension is $d$ is odd. 

Consider the underlying matrix $\bM \succ 0$ for the Gaussian kernel $k_{\bM}$ has the following Cholesky decomposition 
$$\bM = \bL\bL^{\mathsf{T}}.$$
Since $\bM$ is full rank and is in $\symmp$ this decomposition is unique. With this we state the following result on $\rtv{f}$ of a kernel machine $f \in \cH(\reals^d)$ with the proof in \iftoggle{longversion}{\appref{app: explicit}}{the supplemental materials}.

%\rahul{Some really inconsistent use of $\rtv{\cdot}$ vs. R-norm. Pick one name and stick to it. At some points below you even use both in the same sentence. A reader will have no idea what's going on}
%In the following, we provide the proof for one sample setting and defer the  setting to .

%\rahul{this theorem is only true for $d$ odd. we have no clue what happens when $d$ is even}
\begin{theorem}\label{thm: rtv}
    Assume the dimension of the center space is $d$ is odd.
    For a kernel machine $f \in \cH(\reals^d)$ in the space of Gaussian RKHS. If $f$ has the following representation
    \begin{align*}
        f(\cdot) = \sum_{i =1}^k \alpha_i k_{\mathbf{M}}(\bm{x}_i,\cdot),
    \end{align*}
    then the $\mathcal{R}\mathrm{TV}^2$ of $f$ has the following form:
    \[
\mathcal{R}\mathrm{TV}^2(f) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
    \int_\mathbb{R}
    \abs{\sum_{i=1}^k \alpha_i \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_i^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta},
\]
where we have used the decomposition $\mathbf{M} = \mathbf{L}^\mathsf{T}\mathbf{L}$.
Furthermore, this can be extended to the case when $f$ has a representation with infinite kernel functions.
\end{theorem}
% \begin{proof}{\textbf{outline}}
%     We begin by factorizing the metric $\mathbf{M}$ as $\mathbf{M}=\mathbf{L}^\mathsf{T}\mathbf{L}$, which enables us to express the Gaussian kernel wrt one sample $\bm{x}_0$ in the form
% \[
% g(\bm{x})=\frac{1}{(2\pi)^{d/2}}\exp\Bigl(-\frac{\|\mathbf{L}(\bm{x}-\bm{x}_0)\|^2}{2}\Bigr).
% \]
% This representation facilitates the computation of its Fourier transform. Using the change-of-variables formula, one obtains
% \[
% \hat{g}(\bm{\omega})=\exp\Bigl(-\mathrm{i}\bm{x}_0^\mathsf{T}\bm{\omega}\Bigr)\frac{1}{|\det \mathbf{L}|}\exp\Bigl(-\frac{\|\mathbf{L}^{-\mathsf{T}}\bm{\omega}\|^2}{2}\Bigr).
% \]
% The Fourier slice theorem~\citep{Blackledge} is then employed to relate the one-dimensional Fourier transform of the Radon transform of $g$, namely $\mathcal{R}\{g\}(\bm{\beta},t)$, to $\hat{g}$ evaluated at $\bm{\omega}=\omega\bm{\beta}$. By inverting the one-dimensional Fourier transform, an explicit expression for $\mathcal{R}\{g\}(\bm{\beta},t)$ is derived.

% For odd dimensions $d$, the $\sf R$-total variation norm is defined as the $L^1$-norm of the $(d+1)$st derivative (with respect to $t$) of $\mathcal{R}\{g\}(\bm{\beta},t)$. The linearity of the Fourier transform and the Radon transform then allows the result to extend from the single kernel $g$ to any kernel classifier
% \[
% f(\cdot)=\sum_{i=1}^k \alpha_i k_\mathbf{M}(\bm{x}_i,\cdot),
% \]
% as well as to the case of an infinite representation. This completes the derivation of the stated expression for $\mathcal{R}\mathrm{TV}^2(f)$.
%\end{proof}
\begin{proof}{\textbf{outline}}
The proof proceeds in three main steps:
First, we leverage the metric factorization $\mathbf{M}=\mathbf{L}^\mathsf{T}\mathbf{L}$ to express the Gaussian kernel for a single center $\bm{x}_0$ as
\[
g(\bm{x})=\frac{1}{(2\pi)^{d/2}}\exp\Bigl(-\frac{|\mathbf{L}(\bm{x}-\bm{x}_0)|^2}{2}\Bigr).
\]
Next, we compute its Fourier transform using the change-of-variables formula to obtain
\[
\hat{g}(\bm{\omega})=\exp\Bigl(-\mathrm{i}\bm{x}_0^\mathsf{T}\bm{\omega}\Bigr)\frac{1}{|\det \mathbf{L}|}\exp\Bigl(-\frac{|\mathbf{L}^{-\mathsf{T}}\bm{\omega}|^2}{2}\Bigr).
\]
Finally, we apply the Fourier slice theorem~\citep{kak_slaney} 
to connect the one-dimensional Fourier transform of $\mathcal{R}\{g\}(\bm{\beta},t)$ with $\hat{g}(\omega\bm{\beta})$. By inverting this transform, we derive the explicit expression for $\mathcal{R}\{g\}(\bm{\beta},t)$.
For odd dimensions $d$, the second-order Radon total variation of smooth functions is characterized by the $L^1$-norm of the $(d+1)$st $t$-derivative of $\mathcal{R}\{g\}(\bm{\beta},t)$. The result extends naturally to any finite kernel machine
\[
f(\cdot)=\sum_{i=1}^k \alpha_i k_\mathbf{M}(\bm{x}_i,\cdot)
\]
through the linearity of both the Fourier and Radon transforms. This extension principle also applies to infinite representations, completing the proof.
\end{proof}

\subsection{$\mathcal{R}\mathrm{TV}^2$ as an expression of Hermite polynomials}
In \secref{sec: setup}, we discussed Hermite polynomails (probabilist's). In the following, we show how \thmref{thm: rtv} can be rewritten in terms of Hermite polynomials. In the next section, we study certain useful property of this expression to show the construction of a diverging $\mathcal{R}\mathrm{TV}^2$ sequence of kernel machines.

%RKHS of the infinite-width NNGP and NTK on the sphere Sd is typically a Sobolev space (Bietti and Bach, 2021, Chen and Xu, 2021),

%\rahul{inconsistency of transpose notation ${}^\mathsf{T}$ vs. ${}^\top$ in the paper. pick one and stick to it}

First, consider the the case of a $g \in \cH$ defined on one center $\bm{x}_0 \in \reals^d$.
%In \thmref{thm: rtv}, we computed the $\cR$-norm of a Gaussian kernel classifier.
Using \thmref{thm: rtv} we can write the $\mathcal{R}\mathrm{TV}^2$-norm of the kernel machine $g$ for one center $\bm{x}_0$ as
\begin{equation}
\mathcal{R}\mathrm{TV}^2(g) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
    \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta} \label{eq: tv}
\end{equation}

% In the rest of this section, we will simplify this computation and show a lower bound on $\mathcal{R}\mathrm{TV}^2(g)$ that tends to $\infty$ in a limiting case.

First, consider the inner integral in $\mathcal{R}\mathrm{TV}^2(g)$ and denote it as
\[
I(\bm{\beta}) := \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\sigma^2}}}} \mathrm{d}t
\]
where we use $\sigma = \norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}$.

Now, let $\mu = \bm{x}_0\bm{\beta}$, $d+1$-th derivative of $\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}}$ is related to the $d+1$-th Hermite polynomial $H_{d+1}$ as follows:
\[
\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}} = (-1)^{d+1} \sigma^{-(d+1)} H_{d+1}\paren{\frac{t-\mu}{\sigma}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}}
\]
\newline
Now, let $u = \frac{t - \mu}{\sigma}$. Thus, $\mathrm{d}u = \frac{1}{\sigma} \mathrm{d}t$. Substituting this transformation to $I(\bm{\beta})$ gives
\begin{align}
I(\bm{\beta}) = \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}}}} \mathrm{d}t &= \int_\mathbb{R} \abs{ (-1)^{d+1} \sigma^{-(d+1)} H_{d+1}(u) e^{-\frac{u^2}{2}}} \sigma \mathrm{d}u \label{eq: singleint1}\\
    &= \sigma^{-d} \int_\mathbb{R} \abs{H_{d+1}(u) e^{-\frac{u^2}{2}}} \mathrm{d}u \label{eq: singleint2}
\end{align}
We can rewrite $I(\bm{\beta})$ as
\[
I(\bm{\beta}) = \sigma^{-d} \int_\mathbb{R} \abs{H_{d+1}(u) e^{-\frac{u^2}{2}}} \mathrm{d}u = \sigma^{-d} C_d
\]
where $C_d :=  \int_\mathbb{R} \abs{H_{d+1}(u) e^{-\frac{u^2}{2}}} \mathrm{d}u$. In \secref{sec: diverge}, we bound this quantity to achieve certain decay of an infinite sum.
%We will show that $C_d$ is bounded value that depends on $d$.

Replacing the computation in \eqnref{eq: singleint2} to \eqnref{eq: tv} gives
\begin{align}
    \mathcal{R}\mathrm{TV}^2(g) = \frac{C_d}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^{d+1}}} \,\mathrm{d}\bm{\beta} \label{eq: simprtv}
\end{align}
Thus, this shows that the expression of $\rtv{g}$ in \thmref{thm: rtv} can be simplified in terms of Hermite polynomials. 

In the following we extend this for $k > 1$ with the proof deferred to \iftoggle{longversion}{\appref{app: cov}}{the supplemental materials}.
\begin{lemma}\label{lem: cov}
 For a kernel machine $f \in \cH$ in the space of Gaussian RKHS. If $f$ has the following representation%\vspace{-3mm}
    \begin{align*}
        f(\cdot) = \sum_{i =1}^k \alpha_i k_{\mathbf{M}}(\bm{x}_i,\cdot)
    \end{align*}
    for a center set $\curlybracket{\x_1, \x_2,\ldots, \x_k}$. Then, we can write%\vspace{-1mm}
    \begin{gather}
\rtv{f} = \frac{1}{|\det \mathbf{L}| \sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{I_{\sf{inner}}(\bm{\beta})}{\sigma^{d+1}} \, d\bm{\beta} \label{eq:transformed_I}\\
I_{\sf{inner}}(\bm{\beta}) := \int_{\mathbb{R}} \left| \sum_{i=1}^k \alpha_i H_{d+1}\left( y + \Delta_i \right) e^{ -\frac{(y + \Delta_i)^2}{2} } \right| dy \label{eq:transformedinner}
\end{gather}
where $\sigma = \|\mathbf{L}^{-\mathsf{T}}\bm{\beta}\|$ and 
\[
\Delta_i = \frac{\bm{x}_1^\mathsf{T}\bm{\beta} - \bm{x}_i^\mathsf{T}\bm{\beta}}{\|\mathbf{L}^{-\mathsf{T}}\bm{\beta}\|}
\quad \text{for } i = 2, 3, \ldots, k
\]
and \( \Delta_1 = 0 \).
\end{lemma}

% \begin{corollary}
%     Assume a kernel function $f \in \cH(\reals^d)$ is represented with linear combination $\alpha$ such that $\ell_1(\alpha) < \infty$. Then, $\rtv{f}$ is bounded.
% \end{corollary}
% Using \lemref{lem: cov}, it is straightforward to bound the inner integral $I_{\sf{inner}}(\bm{\beta})$ for any $\bm{\beta}$ using triangle inequality in terms of $\ell_1(\alpha)$ and noting $\int_{\mathbb{R}} \lvert  H_{d+1}\left( y + \Delta_i \right) e^{ -\frac{(y + \Delta_i)^2}{2} } \rvert dy$ is bounded for each $i$. Rest follows easily.
% %\subsection{Extension of $\mathcal{R}\mathrm{TV}^2$ for $k$ > 1}

