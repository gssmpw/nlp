% \section{Gap in Gaussian RKHS and neural networks in odd
% dimensions: 
% Supplementary Materials}

% \begin{itemize}
% \item \appref{app: explicit} presents the complete proof of \thmref{thm: rtv}.\vspace{2mm}

% \item \appref{app: cov} establishes the change of variable result from \lemref{lem: cov} in the multi-sample setting.\vspace{2mm}

% \item \appref{app: useful} develops key properties of Hermite polynomials, proving the crucial relationship stated in \lemref{lem: inter}.\vspace{2mm}

% \item \appref{app: diverging} demonstrates how certain coefficient combinations can converge in RKHS norm while diverging in $\ell_1$ norm, as claimed in \exmref{exam: divergence}.\vspace{2mm}

% %\item \appref{app: properties} establishes fundamental properties of Gaussian RKHS when defined over unbounded domains.
% \end{itemize}
% \akash{toc}

\section{$\cR$-norm for centers size $k > 1$}\label{app: explicit}
In this Appendix, we provide the proof of \thmref{thm: rtv}.

First, we provide the proof for one center and then extend it to multi-centers settings.
\begin{proof}

Let
\[
    g(\bm{x}) = \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\bm{x} - \bm{x}_0}_\mathbf{M}^2}{2}},
\]

Also, define the $\bm{0}$ mean identity covariance gaussian
\[
    g_0(\bm{x}) = \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\bm{x}}^2}{2}}
\]
If we can write $\mathbf{M} = \mathbf{L}^\mathsf{T}\mathbf{L}$, then we have that
\[
\norm{\bm{x} - \bm{x}_0}_\mathbf{M}^2 = \norm{\mathbf{L}(\bm{x} - \bm{x}_0)}^2,
\]
in which case
\[
g(\bm{x}) = \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_0}^2}{2}}
\]
We have the Fourier transform
\[
    \hat{g}_0(\bm{\omega}) = \exp\paren{-\frac{\norm{\bm{\omega}}^2}{2}}.
\]
We have the equality $g(\bm{x}) = g_0(\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_0)$. Using the change of variables formula for the Fourier transform, we have
\begin{align}
    \hat{g}(\bm{\omega})
    &= \exp\paren{-\mathrm{i}(\mathbf{L}\bm{x}_0)^\mathsf{T}\mathbf{L}^{-\mathsf{T}}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \hat{g}_0(\mathbf{L}^{-\mathsf{T}} \bm{\omega}) \nonumber\\
    &= \exp\paren{-\mathrm{i}\bm{x}_0^\mathsf{T}\mathbf{L}^\mathsf{T}\mathbf{L}^{-\mathsf{T}}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} \bm{\omega}}^2}{2}} \nonumber\\
    &= \exp\paren{-\mathrm{i}\bm{x}_0^\mathsf{T}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} \bm{\omega}}^2}{2}} \label{eq: firstfour}
\end{align}

The Fourier slice theorem~\citep{kak_slaney} says that
\[
    \mathcal{F}_1\{\mathcal{R}\{f\}(\bm{\beta}, \cdot)\}(\omega) = \hat{f}(\omega\bm{\beta}).
\]
If we evaluate $ \hat{g}$ at $\bm{\omega} = \omega\bm{\beta}$ in the \eqnref{eq: firstfour}, we find
\begin{align*}
\hat{g}(\omega\bm{\beta})
&= \exp\paren{-\mathrm{i}\bm{x}_0^\mathsf{T}(\omega\bm{\beta})} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} (\omega\bm{\beta})}^2}{2}} \\
&= \exp\paren{-\mathrm{i}(\bm{x}_0^\mathsf{T}\bm{\beta})\omega} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\abs{\omega}^2\norm{\mathbf{L}^{-\mathsf{T}} \bm{\beta}}^2}{2}} \\
\end{align*}
The 1D inverse Fourier transform of this is the Radon transform of $g$, i.e.,
\[
    \mathcal{R}\{g\}(\bm{\beta}, t) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}} \exp\paren{-\frac{(t- \bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}
\]
If $d$ is odd, then the second-order Radon domain total variation is the $L^1$-norm of $(d+1)$ derivatives in $t$ of this quantity (see Equation.(28) in \cite{Parhi2020BanachSR}). That is
\begin{align}
    \mathcal{R}\mathrm{TV}^2(g)
    &= \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \int_\mathbb{R}\abs{\frac{1}{\sqrt{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}} \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta} \label{eq: rtv1}\\
    &= \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
    \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta} \label{eq: rtv2}
\end{align}
This gives the stated expression on $\rtv{f}$ for one center.
\end{proof}

\subsection{Multi-centers computation}

For the training set $\cD$ with number of centers greater than 1, i.e. $k > 1$, we can rewrite $g$ as follows:
\[
g(\bm{x}) = \sum_{i =1}^k \frac{1}{(2\pi)^{d/2}}  \alpha_i \cdot \exp\paren{-\frac{\norm{\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_i}^2}{2}}
\]
Denote by $g_i(\bm{x}) := \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_i}^2}{2}}$ for each center $\bm{x}_i \in \cD$.

Now, the Fourier transform of $g$ can be written for the extended case, noting the linearity of the transform,
\[
 \hat{g}(\bm{\omega}) = \sum_{i=1}^k \hat{g}_i(\bm{\omega})
\]
This implies that 
\[
 \hat{g}(\bm{\omega}) = \sum_{i =1}^k \exp\paren{-\mathrm{i}\bm{x}_i^\mathsf{T}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} \bm{\omega}}^2}{2}}
\]
Now, computing the inverse Fourier transform of $\hat{g}$ wrt $\bm{\omega}$ gives 
\[
\mathcal{R}\{g\}(\bm{\beta}, t) = \frac{1}{\abs{\det \mathbf{L}}} \sum_{i=1}^k \alpha_i \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}} \exp\paren{-\frac{(t- \bm{x}_i^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}
\]
As before the \sf{R}-norm of $g$, i.e. the second-order Radon domain total variation for odd values of $d$ is the $L^1$-norm of $(d+1)$ derivatives in $t$ of this quantity. Thus,
\[
\mathcal{R}\mathrm{TV}^2(g) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
    \int_\mathbb{R}
    \abs{\sum_{i=1}^k \alpha_i \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_i^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta}
\]

Since a Gaussian kernel is a Schwartz function, and using exponential decay and infinite smoothness of $k_{\textbf{M}}$ we can extend the computation of $\rtv{f}$ to the case of kernel machines with infinite centers representation as well.