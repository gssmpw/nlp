\section{Related Work}

\paragraph{Approximability with Kernel Methods}
% \begin{itemize}
%     \item equivalent expressivity: 
% \citep{arora2019harnessing,li2019enhanced,lee2019wide,novak2018bayesian,lee2017deep,matthews2018gaussian,garriga2018deep,shankar2020neural}
%     \item lazy regime: 
% \citep{mean_field_songmei,sirignano2020mean,rotskoff2022trainability,chizat2018global}

%     \item Not in RKHS: 
% \citep{yehudai2019power,ghorbani2021linearized,ghorbani2019limitations,bach_curse,allen2023backward,allen2019can}
%     \item dlogd: \cite{mean_field_songmei}
%     \item d: \citep{yehudai2019power,ghorbani2019limitations}
% \end{itemize}
\cite{bach_curse} studied various classes of single-/multi-index models with low intrinsic dimension and bounded $\rbv{\reals^d}$-norm. In contrast, \cite{ghorbani2019limitations} showed that if the covariates have the same dimension as the low intrinsic dimension of the target function, kernel and neural network approximations can be competitive. Empirically, some works show that the curse of dimensionality with kernel methods can be handled with an appropriate choice of dataset-specific kernels~\citep{arora2019harnessing, novak2018bayesian, shankar2020neural} or mirroring neural network training dynamics closely to kernel methods~\citep{mean_field_songmei, sirignano2020mean, rotskoff2022trainability, chizat2018global}. 
%curse of dimensionality is less severe if the intrinsic dimensionality of 
But a wide body of work has also shown a gap in approximation with neural networks capturing richer and more nuanced class of functions compared to kernel methods (see ~\citep{allen2019can,mean_field_songmei, yehudai2019power, ghorbani2019limitations}). In our work, we show that while Gaussian RKHS is embedded within neural networks in bounded domains, in the unbounded regime there exists a non-trivial gap between $\cH^{\sf{Gauss}}(\reals^d)$ and $\rbv{\reals^d}$.%, demonstrated by their intersection having a significant complement.
%In our work, we concretely show that Gaussian RKHS in an unbounded regime provides a contrasting gap with non-trivial complement to the intersection of $\cH^{\sf{Gauss}}(\reals^d)$ and $\rbv{\reals^d}$. \rahul{this sentence is hard to follow}

% and kernel machines 
% For instance, when the target function is a single neuron, neural networks can learn efficiently using approximately $d\log d$ samples~\citep{mean_field_songmei}, while the corresponding RKHS exhibits test error bounded away from zero even with sample size polynomial in dimension $d$~\citep{yehudai2019power, ghorbani2019limitations}. 
% These separation results extend beyond simple examples, with several studies identifying target functions that can be efficiently learned by neural networks but lie outside the corresponding RKHS \citep{yehudai2019power, ghorbani2021linearized, ghorbani2019limitations, bach_curse, allen2023backward, allen2019can}.
% Furthermore, even in the infinite-width limit, two-layer neural networks demonstrate the ability to capture a richer class of functions compared to their associated RKHS, particularly when stochastic gradient descent is appropriately scaled beyond the lazy regime \citep{mean_field_songmei, sirignano2020mean, rotskoff2022trainability, chizat2018global}.

% Some works have shown empirically that the performance of neural networks on several datasets can be almost replicated matched with suitable choice of kernels \citep{arora2019harnessing, li2019enhanced, lee2019wide, novak2018bayesian, lee2017deep, matthews2018gaussian, garriga2018deep, shankar2020neural}.
% %Empirical studies across various datasets have demonstrated that neural networks can often be replaced by suitable kernel methods with only limited performance degradation \citep{arora2019harnessing, li2019enhanced, lee2019wide, novak2018bayesian, lee2017deep, matthews2018gaussian, garriga2018deep, shankar2020neural}. 
% This is more pronounced in "lazy" learning regime, where neural network training dynamics closely mirror kernel methods \citep{mean_field_songmei, sirignano2020mean, rotskoff2022trainability, chizat2018global}.
% \akash{add ghorbani's work here} However, theoretical analyses have revealed fundamental limitations of kernel methods. 

\paragraph{The Function Spaces $\rbv{\reals^d}$}\looseness-3 The function space \(\rbv{\Omega}\) (bounded variation in the Radon domain) naturally characterizes the function approximation capabilities of shallow ReLU neural networks. \citet{Parhi2020BanachSR} established a \textit{representer theorem}, showing that solutions to variational problems over \(\rbv{\Omega}\) correspond to single-hidden layer ReLU networks with weight decay regularization. Unlike RKHS, which suffers from the curse of dimensionality, \(\rbv{\Omega}\) enables efficient function representation by capturing low-dimensional structure. %\citet{mei_rate} showed that if the target neural networks 
Moreover, there are function classes for which neural networks achieve near-minimax optimal approximation rates as shown in \citet{near_mini}, while kernel methods cannot. This suggests that, while RKHS embeddings may appear restrictive, \(\rbv{\Omega}\) provides a more expressive framework, positioning shallow networks as solutions to infinite-dimensional variational problems with superior generalization properties. %\rahul{who is talking about generalization} 
For further details see \citep{Parhi2020BanachSR,Parhi2021WhatKO,Parhi2023FunctionSpaceOO}

\paragraph{Embeddings of RKHSs and $\rbv{\Omega}$} 
For a Lipshitz open domain $\Omega \subseteq \reals^d$, it is well-known that the Sobolev space \( H^s(\Omega) \) is (equivalent to) an RKHS if and only if \( s > d/2 \), e.g. the Laplace and Matérn kernels are associated with Sobolev RKHSs (see, e.g., \cite{kanagawa2018gaussianprocesseskernelmethods}, Example 2.6). In contrast, the Gasussian RKHS \( \mathcal{H}^{\text{Gauss}}(\Omega) \) is contained in every Sobolev space, i.e., \( \mathcal{H}^{\text{Gauss}}(\Omega) \subset H^s(\Omega) \) for all \( s \geq 0 \) (cf., \cite{steinwart_svm}, Corollary 4.36). %\rahul{only on bounded domains so you should write the spaces with $\Omega$} 
Recent work has further demonstrated that the RKHSs of typical neural tangent kernel (NTK) and neural network Gaussian process (NNGP) kernels for the ReLU activation function are equivalent to the Sobolev spaces \( H^{(d+1)/2}(\mathbb{S}^d) \) and \( H^{(d+3)/2}(\mathbb{S}^d) \), respectively~\citep{bietti2021deep,chen2021deep}. \cite{Steinwart2009OptimalRF} has shown that an optimal learning rates in Sobolev RKHSs can be achieved by cross-validating the regularization parameter. On another front, embedding properties relating Sobolev spaces and the Radon bounded variation (RBV) spaces have been explored; for example, \cite{Ongie2020A} showed that \( W^{d+1}(L_1(\mathbb{R}^d)) \) embeds in \( \rbv{\mathbb{R}^d} \). More recently, \cite{mao2024approximationratesshallowreluk} established a sharp bound by proving that \( W^s(L_p(\Omega)) \) with \( s \geq 2 + (d+1)/2 \) for $p \ge 2$ embeds in \( \rbv{\Omega} \) for bounded domains \( \Omega \subset \mathbb{R}^d \). %\rahul{why do you use two different notations for Sobolev spaces in the same paragraph? $W^{d+1, 1}$ and $W^s(L_p)$???}
% For general \( s > 0 \), it is well-known that \( H^s(\Omega) \) is (equivalent to) a reproducing kernel Hilbert space (RKHS) if and only if \( s > d/2 \). For instance, the Laplace and Matérn kernels are associated with Sobolev RKHSs (see, e.g., \cite{kanagawa2018gaussianprocesseskernelmethods}, Example 2.6). In contrast, the RKHS of the Gaussian kernel, denoted by \( \mathcal{H}^{\text{Gauss}} \), is contained in every Sobolev space, i.e., \( \mathcal{H}^{\text{Gauss}} \subset H^s \) for all \( s \geq 0 \) (cf. \cite{steinwart_svm}, Corollary 4.36). Recent work has further demonstrated that the RKHSs of typical neural tangent kernel (NTK) and neural network Gaussian process (NNGP) kernels for the ReLU activation function are equivalent to the Sobolev spaces \( H^{(d+1)/2}(\mathbb{S}^d) \) and \( H^{(d+3)/2}(\mathbb{S}^d) \), respectively~\citep{bietti2021deep,chen2021deep}. Optimal learning rates in Sobolev RKHSs can also be achieved by cross-validating the regularization parameter \cite{Steinwart2009OptimalRF}. On another front, embedding properties relating Sobolev spaces and the Radon bounded variation (RTV) spaces have been explored; for example, \cite{Ongie2020A} showed that \( W^{d+1,1}(\mathbb{R}^d) \) embeds in \( \rbv{\mathbb{R}^d} \). More recently, \cite{mao2024approximationratesshallowreluk} established a sharp bound by proving that \( W^s(L_p(\Omega)) \) with \( s \geq 2 + (d+1)/2 \) embeds in \( \rbv{\Omega} \) for bounded domains \( \Omega \subset \mathbb{R}^d \).


%\cite{haas2023mind}
%where \( D^\alpha \) denotes partial derivatives in multi-index notation for \(\alpha\). It measures the magnitude of derivatives up to some order \( s \). 
% For general \( s > 0 \), \( H^s(\Omega) \) is (equivalent to) an RKHS if and only if \( s > d/2 \). For example, Laplace and Matérn kernels~\citep{kanagawa2018gaussianprocesseskernelmethods} (Example 2.6) have Sobolev RKHSs. The RKHS of the Gaussian kernel \( \mathcal{H}^{\text{Gauss}} \) is contained in every Sobolev space, \( \mathcal{H}^{\text{Gauss}} \subset H^s \) for all \( s \geq 0 \)~\citep{steinwart_svm} (Corollary 4.36).  %We make the following assumption on the kernel:
% \cite{bietti2021deep,chen2021deep} showed that RKHSs of typical NTK and NNGP
% kernels for the ReLU activation function are equivalent to the Sobolev spaces H(d+1)/2(Sd) and
% H(d+3)/2(Sd), respectively,
% Optimal rates in Sobolev RKHS can also be achieved using cross-validation of the regularization
% $\rho$ \cite{Steinwart2009OptimalRF}
% \cite{Ongie2020A} showed that the Sobolev space of $W^{d+1,1}(\reals^d)$ embeds in $\rbv{\reals^d}$ 
% \cite{mao2024approximationratesshallowreluk} has the sharpest bound where they showed that $W^{s}(L_p(\Omega))$ where $s \ge 2 + (d+1)/2$ embeds in in $\rbv{\Omega}$ for bounded $\Omega \subset \reals^d$


