\section{Preliminaries}

\begin{algorithm}
\caption{Recursive Feature Machine (RFM)}
\KwData{$X, y, K_M, T$} \tcp{\textcolor{blue}{Training data: $(X, y)$, kernel function: $K_M$, and number of iterations: $T$}}
\KwResult{$\beta, M$} \tcp{\textcolor{blue}{Solution to kernel regression: $\beta$, and feature matrix: $M$}}
\nl Initialize $M = I_{d \times d}$\; \tcp{\textcolor{blue}{Identity matrix of dimension $d \times d$}}

\For{$t \in T$}{
    \nl $K_{\text{train}} = K_M(X, X)$\; \tcp{\textcolor{blue}{$(K_M(X, X))_{i,j} := K_M(x_i, x_j)$}}
    \nl $\beta = y K_{\text{train}}^{-1}$\;
    \nl $M = \frac{1}{n} \sum_{x \in X} (\nabla f(x))(\nabla f(x))^T$\;\tcp{\textcolor{blue}{$f(x) = \beta K_M(X, x)$ with $(K_M(X, x))_i := K_M(x_i, x)$}}
    }
\end{algorithm}


\section{R-norm in one dimension}


\section{R-norm in high dimension (odd)}


The formula for $K_M$ for any two samples $x,x_0$ is
\begin{align*}
    K_M(\bx,\bx_0) = \exp\paren{-\frac{||\bx - \bx_0||_M^2}{2\sigma}}
\end{align*}
where $\sigma = 1$. Now wrt to the kernel $K_M$ for a given Mahalanbis matrix $M \succeq 0$, a kernel classifier $f$ (RFM in this case) has the form:
\begin{align*}
    f(x) = \sum_{i=1}^n \alpha_i K_M(\bx_i, \bx)
\end{align*}
where $\beta \in \reals^n$.



Let
\[
    g(\bm{x}) = \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\bm{x} - \bm{x}_0}_\mathbf{M}^2}{2}},
\]
where the (squared) Mahalanobis distance is
\[
    \norm{\bm{x} - \bm{x}_0}_\mathbf{M}^2 = (\bm{x} - \bm{x}_0)^\mathsf{T}\mathbf{M}(\bm{x} - \bm{x}_0)
\]
Also, define the $\bm{0}$ mean identity covariance gaussian
\[
    g_0(\bm{x}) = \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\bm{x}}^2}{2}}
\]
If we can write $\mathbf{M} = \mathbf{L}^\mathsf{T}\mathbf{L}$, then we have that
\[
\norm{\bm{x} - \bm{x}_0}_\mathbf{M}^2 = \norm{\mathbf{L}(\bm{x} - \bm{x}_0)}^2,
\]
in which case
\[
g(\bm{x}) = \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_0}^2}{2}}
\]
We have the Fourier transform
\[
    \hat{g}_0(\bm{\omega}) = \exp\paren{-\frac{\norm{\bm{\omega}}^2}{2}}.
\]
We have the equality $g(\bm{x}) = g_0(\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_0)$. Using the change of variables formula for the Fourier transform, we have
\begin{align*}
    \hat{g}(\bm{\omega})
    &= \exp\paren{-\mathrm{i}(\mathbf{L}\bm{x}_0)^\mathsf{T}\mathbf{L}^{-\mathsf{T}}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \hat{g}_0(\mathbf{L}^{-\mathsf{T}} \bm{\omega}) \\
    &= \exp\paren{-\mathrm{i}\bm{x}_0^\mathsf{T}\mathbf{L}^\mathsf{T}\mathbf{L}^{-\mathsf{T}}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} \bm{\omega}}^2}{2}} \\
    &= \exp\paren{-\mathrm{i}\bm{x}_0^\mathsf{T}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} \bm{\omega}}^2}{2}}
\end{align*}

The Fourier slice theorem says that
\[
    \mathcal{F}_1\{\mathcal{R}\{f\}(\bm{\beta}, \cdot)\}(\omega) = \hat{f}(\omega\bm{\beta}).
\]
If we evaluate the previous thing at $\bm{\omega} = \omega\bm{\beta}$, we find
\begin{align*}
\hat{g}(\omega\bm{\beta})
&= \exp\paren{-\mathrm{i}\bm{x}_0^\mathsf{T}(\omega\bm{\beta})} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} (\omega\bm{\beta})}^2}{2}} \\
&= \exp\paren{-\mathrm{i}(\bm{x}_0^\mathsf{T}\bm{\beta})\omega} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\abs{\omega}^2\norm{\mathbf{L}^{-\mathsf{T}} \bm{\beta}}^2}{2}} \\
\end{align*}
The 1D inverse Fourier transform of this is the Radon transform of $g$, i.e.,
\[
    \mathcal{R}\{g\}(\bm{\beta}, t) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}} \exp\paren{-\frac{(t- \bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}
\]
If $d$ is odd, then the second-order Radon domain total variation is the $L^1$-norm of $(d+1)$ derivatives in $t$ of this quantity. That is
\begin{align}
    \mathcal{R}\mathrm{TV}^2(g)
    &= \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \int_\mathbb{R}\abs{\frac{1}{\sqrt{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}} \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta} \label{eq: rtv1}\\
    &= \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
    \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta} \label{eq: rtv2}
\end{align}

\subsection{Extension of $\mathcal{R}\mathrm{TV}^2$ for $k$ > 1}
For the training set $\cD$ with number of samples greater than 1, i.e. $k > 1$, we can rewrite $g$ as follows:
\[
g(\bm{x}) = \sum_{i =1}^k \frac{1}{(2\pi)^{d/2}}  \alpha_i \cdot \exp\paren{-\frac{\norm{\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_i}^2}{2}}
\]
Denote by $g_i(\bm{x}) := \frac{1}{(2\pi)^{d/2}} \exp\paren{-\frac{\norm{\mathbf{L}\bm{x} - \mathbf{L}\bm{x}_i}^2}{2}}$ for each sample $\bm{x}_i \in \cD$.

Now, the Fourier transform of $g$ can be written for the extended case, noting the linearity of the transform,
\[
 \hat{g}(\bm{\omega}) = \sum_{i=1}^k \hat{g}_i(\bm{\omega})
\]
This implies that 
\[
 \hat{g}(\bm{\omega}) = \sum_{i =1}^k \exp\paren{-\mathrm{i}\bm{x}_i^\mathsf{T}\bm{\omega}} \frac{1}{\abs{\det \mathbf{L}}} \exp\paren{-\frac{\norm{\mathbf{L}^{-\mathsf{T}} \bm{\omega}}^2}{2}}
\]
Now, computing the inverse Fourier transform of $\hat{g}$ wrt $\bm{\omega}$ gives 
\[
\mathcal{R}\{g\}(\bm{\beta}, t) = \frac{1}{\abs{\det \mathbf{L}}} \sum_{i=1}^k \alpha_i \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}} \exp\paren{-\frac{(t- \bm{x}_i^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}
\]
As before the \sf{R}-norm of $g$, i.e. the second-order Radon domain total variation for odd values of $d$ is the $L^1$-norm of $(d+1)$ derivatives in $t$ of this quantity. Thus,
\[
\mathcal{R}\mathrm{TV}^2(g) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
    \int_\mathbb{R}
    \abs{\sum_{i=1}^k \alpha_i \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_i^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta}
\]

% Given a kernel machine
% \[
%     h(\bm{x}) = \sum_{i=1}^n \alpha_i k_M(\bm{x}, \bm{x}_i),
% \]
% we have that
% \[
%     \mathcal{R}\mathrm{TV}^2(h) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \sum_{i=1}^n \alpha_i  \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
%     \int_\mathbb{R}
%     \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_i^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta}
% \]
\iffalse
\section{Estimating \sf{R}-norm for one sample}

In \eqnref{eq: rtv2}, we computed the \sf{R}-norm of the kernel machine $g$ for one sample as
\begin{equation}
\mathcal{R}\mathrm{TV}^2(g) = \frac{1}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}}} 
    \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^2}}}} \,\mathrm{d}t\,\mathrm{d}\bm{\beta} \label{eq: tv}
\end{equation}

In the rest of this section, we will simplify this computation and show a lower bound on $\mathcal{R}\mathrm{TV}^2(g)$ that tends to $\infty$ in a limiting case.

First, consider the inner integral in $\mathcal{R}\mathrm{TV}^2(g)$ and denote it as
\[
I(\bm{\beta}) := \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\bm{x}_0^\mathsf{T}\bm{\beta})^2}{2\sigma^2}}}} \mathrm{d}t
\]
where we use $\sigma = \norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}$.

Now, let $\mu = \bm{x}_0\bm{\beta}$, $d+1$-th derivative of $\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}}$ is related to the $d+1$-th Hermite polynomial $H_{d+1}$ as follows:
\[
\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}} = (-1)^{d+1} \sigma^{-(d+1)} H_{d+1}\paren{\frac{t-\mu}{\sigma}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}}
\]
Now, let $u = \frac{t - \mu}{\sigma}$. Thus, $\mathrm{d}u = \frac{1}{\sigma} \mathrm{d}t$. Substituting this transformation to $I(\bm{\beta})$ gives
\begin{align}
I(\bm{\beta}) = \int_\mathbb{R}
    \abs{ \paren{\frac{\partial^{d+1}}{\partial t^{d+1}} \exp\paren{-\frac{(t-\mu)^2}{2\sigma^2}}}} \mathrm{d}t &= \int_\mathbb{R} \abs{ (-1)^{d+1} \sigma^{-(d+1)} H_{d+1}(u) e^{-\frac{u^2}{2}}} \sigma \mathrm{d}u \label{eq: singleint1}\\
    &= \sigma^{-d} \int_\mathbb{R} \abs{H_{d+1}(u) e^{-\frac{u^2}{2}}} \mathrm{d}u \label{eq: singleint2}
\end{align}

We can rewrite $I(\bm{\beta})$ as
\[
I(\bm{\beta}) = \sigma^{-d} \int_\mathbb{R} \abs{H_{d+1}(u) e^{-\frac{u^2}{2}}} \mathrm{d}u = \sigma^{-d} C_d
\]
We will show that $C_d$ is bounded value that depends on $d$.

Replacing the computation in \eqnref{eq: singleint2} to \eqnref{eq: tv} gives
\begin{align}
    \mathcal{R}\mathrm{TV}^2(g) = \frac{C_d}{\abs{\det \mathbf{L}}} \frac{1}{\sqrt{2\pi}} \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^{d+1}}} \,\mathrm{d}\bm{\beta} \label{eq: simprtv}
\end{align}

Now, we will show that $C_d$ and $\int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^{d+1}}} \,\mathrm{d}\bm{\beta}$ are bounded, implying $\mathcal{R}\mathrm{TV}^2(g)$ is unbounded for $\abs{\det \mathbf{L}}$ is arbitrarily close to zero.

\begin{lemma}
    The integral over the unit sphere $\mathbb{S}^{d-1}$ in \eqnref{eq: simprtv} is bounded above by a constant that depends on the largest eigenvalue of $\mathbf{L}$, i.e. there exists a constant $B$ such that
    \begin{align*}
        \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^{d+1}}} \,\mathrm{d}\bm{\beta} < B.
    \end{align*}
\end{lemma}
\begin{proof}
Denote the integral by $\cA:= \int_{\mathbb{S}^{d-1}} \frac{1}{{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^{d+1}}} \,\mathrm{d}\bm{\beta}$.
    The proof of the claim is based on bounding the norm $\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}^{d+1}$ in terms of the largest eigenvalue of $\mathbf{L}$.

    Note that $\mathbf{M} = \mathbf{L}^\mathsf{T}\mathbf{L}$. We assume that the $\mathbf{M}$ is symmetric and PSD, implying that singular values of $\mathbf{L}$ are \tt{exactly} the square root of the eigenvalues of $\mathbf{M}$, i.e
    \[ \sigma_i(\mathbf{L}) = \sqrt{|\lambda_i(\mathbf{M})|}\]
    But since $\mathbf{L}$ is invertible implying the singular values if $\mathbf{L^{-1}}$ are inverses to singular values of $\mathbf{L}$, i.e.
    \[
    \sigma_i(\mathbf{L}^{-1}) = \frac{1}{\sigma_i(\mathbf{L})}
    \]
    Now, consider the following bound
    \begin{align*}
        \frac{1}{\norm{\mathbf{L}^{-\mathsf{T}}\bm{\beta}}} \le \frac{1}{\sigma_{\min}(\mathbf{L}^{-1})} = \sigma_{\max}(\mathbf{L}) = \sqrt{\lambda_{\max}(\mathbf{M})}
    \end{align*}
    Thus, we can write 
    \[
    \cA \le \lambda_{\max}(\mathbf{M})^{\frac{d+1}{2}} \cdot \int_{\mathbb{S}^{d-1}} \,\mathrm{d}\bm{\beta} = \lambda_{\max}(\mathbf{M})^{\frac{d+1}{2}} \cdot\frac{2\pi^{\frac{d}{2}}}{\Gamma\paren{\frac{d}{2}}}
    \]
    where $\Gamma$ is a \textbf{Gamma} function.
    Since $\lambda_{\max}(\mathbf{M})$ is a bounded value, we note that $\cA$ is bounded above by a constant that depends on the largest eigenvalue of $\mathbf{M}$ and the dimension $d$.
    \end{proof}

    \section{Implicit bias for kernel machine}

    \begin{gather*}
        \min_{\alpha, M} ||f = \sum_{i,j} \alpha_i \alpha_j K_M(x_i,x_j)||_{\mathcal{F}}\\
        s.t.\forall i\,  f(x_i) = y_i\\
    \end{gather*}
    where $\mathcal{F}$ is an RKHS with Gaussian kernel with $M = \mathbb{I}$.

    We can write this down as 
    \begin{gather*}
        \sum_{i,j} \alpha_i \alpha_j K_M(x_i,x_j) = \sum_{i,j} \alpha_i \alpha_j K_{\cG}(L^{\top}x_i, L^{\top}x_j)\\
        ||f||_{\cF} = 
    \end{gather*}
    \fi

  
\section{Infinite samples asymptotics}

