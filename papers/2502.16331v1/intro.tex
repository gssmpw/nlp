\section{Introduction}

%\rahul{make sure to have punctuation with equations. For example, if a sentence ends with an equation, then there is a period in the equation environment. If there should be a comma after the equation, the comma goes inside the equation environment}
In supervised learning, we observe samples with corresponding labels, which may represent classes or continuous values. Our primary objective is to construct a function \(f: \mathbb{R}^d \to \mathbb{R}\) based on these observations that can accurately predict labels for new, unseen data points. Traditionally, reproducing kernel Hilbert spaces (RKHS) have provided a principled framework for this task, offering both theoretical guarantees and practical algorithms. Their power stems from the representer theorem, which ensures that optimal solutions can be expressed as combinations of kernel functions centered at the training points.

%\rahul{don't use $\mu$ for the activation function since it looks like a measure. Use $\sigma$ or $\rho$. The vector notation is also inconsistent with $\bm{x}$ and $\mathbf{x}$. Be consistent.}
However, the landscape of machine learning has evolved significantly with the emergence of neural networks, which have demonstrated remarkable success across diverse applications over kernel methods. The simplest neural architecture—the single-hidden layer network—builds upon the concept of ridge functions, which map \(\mathbb{R}^d \to \mathbb{R}\) via the form $\bm{x} \mapsto \sigma(\bm{w}^\top \bm{x})$, where \(\sigma: \mathbb{R} \to \mathbb{R}\) is a univariate function and \(\bm{w} \in \mathbb{R}^d \setminus \{0\}\). In practice, these networks combine multiple ridge functions:\vspace{-1mm}
\[
\bm{x} \mapsto \sum_{k=1}^K v_k\, \sigma(\bm{w}_k^\top \bm{x} - b_k),\vspace{-1mm}
\]
where \(K\) represents the network width, \(v_k \in \mathbb{R}\) and \(\bm{w}_k \in \mathbb{R}^d \setminus \{0\}\) are weights, and \(b_k \in \mathbb{R}\) are biases. While RKHS methods suffer from the curse of dimensionality, neural networks can overcome it by learning effective low-dimensional representations~\citep{ghorbani_curse, luxberg_curse}. %\rahul{Why? Ref something or explain in what sense.}

A fundamental question is to compare the approximation capabilities of neural networks with those of RKHS corresponding to different kernels. For example, \cite{mei_rate} showed that if the target function is a single neuron, neural networks can learn efficiently using roughly \(d \log d\) samples, whereas the corresponding RKHS requires a sample size that grows polynomially in the dimension \(d\) (see also~\citep{yehudai2019power, ghorbani2019limitations}).

% \rahul{please ref the right papers and fix the years and bib entries for these papers. Idk why they are wrong, but if you get them from google scholar they are right... you also ref the wrong papers for the wrong stuff}
Recent work~\citep{Parhi2020BanachSR,near_mini} has studied the Banach-space optimality of single-hidden-layer (shallow ReLU) networks over both bounded and unbounded domains \(\Omega \subseteq \reals^d\).  There, the authors established a representer theorem which demonstrates that solutions to data-fitting problems in these networks naturally reside in a kind of bounded variation space, referred to as the second-order Radon bounded variation space \(\rbv{\Omega}\). These spaces, in turn, contain several classical multivariate function spaces, including certain Sobolev spaces as well as certain spectral Barron spaces~\citep{barron_universal}. For instance, \citet{near_mini} have shown that the Sobolev space \(H^{d+1}(\Omega)\) embeds into \(\rbv{\Omega}\) for any bounded Lipschitz domain $\Omega \subset \reals^d$. Moreover, on any bounded Lipschitz domain \(\Omega \subset \reals^d\), the Gaussian reproducing kernel Hilbert space \(\cH^{\sf{Gauss}}(\Omega)\) is known to embed into the Sobolev space \(H^{s}(\Omega)\) for all \(s> 0\) (see Corollary 4.36 of~\citet{steinwart_svm}). This observation appears to highlight the strict approximation limitations of the Gaussian RKHS compared to neural networks on bounded domains. Consequently, a natural question arises: \tt{Are Gaussian kernel machines, among others, highly restrictive in approximating general functions}? Conversely, one may ask: \tt{To what extent can we approximate functions using shallow neural networks}?


To show a closure, \cite{ghorbani2019limitations} demonstrated that the gap between neural network approximations and kernel methods can be narrowed when the intrinsic dimensionality of the target function is well captured by the covariates. In this work, we take a different perspective. While Gaussian RKHS may seem rather limited in a bounded domain, we show that when considering an unbounded domain with fixed dimension \(d\), there exist functions in \(\cH^{\sf{Gauss}}(\reals^d)\) with unbounded $\rbv{\reals^d}$-norm. This observation suggests that the intersection between \(\cH^{\sf{Gauss}}(\reals^d)\) and \(\rbv{\reals^d}\) has a nontrivial complement, indicating that the Gaussian RKHS need not be inherently restrictive in its approximation capabilities if the centers for a kernel machine can be picked with large Euclidean norms. %with respect to the linear combinations. 
%\rahul{don't understand this sentence}
%in the unbounded domain where

One can argue that in the regime of kernel machines with infinite centers, an RKHS over an unbounded domain could allow functions with a large discrepancy in the linear combinations, i.e., $f = \sum_{i=1}^\infty \alpha_i k(\bm{x}_i,\cdot)$ could have bounded RKHS norm  $\norm{\cdot}_{\cH}$ while the $\ell_1$ norm of $\alpha$ is \tt{unbounded} (see \exmref{exam: divergence}). This discrepancy can be exploited to design a sequence of functions $\curlybracket{f_n}$ whose $\rbv{\reals^d}$-norm is diverging (see \thmref{thm: unbounded} in \secref{sec: diverge}).
More specifically, the key idea is as follows:
\begin{itemize}
    \item[I] Compute $\rtv{f}$ of a function which crucially depends on the spacing of the kernel centers in the space, (see \secref{sec: rnorm})\vspace{-.5mm}
    \item[II] Using the unbounded domain choose an infinite sequence of kernel centers \tt{reasonbly} far-apart once projected onto a cone to allow for unbounded $\ell_1$ norm. (see \secref{sec: diverge})\vspace{-.5mm}
\end{itemize}
%More specifically, the key idea is to explicitly compute $\rtv{f}$ of a function which crucially depends on the spacing of the samples in the space. An unbounded domain allows a choice of an infinite sequence of samples \tt{reasonbly} far-apart once projected onto a cone so that the combinations are fairly large enough for unbounded $\ell_1$.
 
%In modern machine learning, neural networks achieve the state-of-the-art on several supervised learning tasks with few steps of gradient based optimization. A typical supervised learning settings
% For a given set of supervised labels $\curlybracket{(\bm{x}_i,y_i)}_{i=1}^n$
%  a regularized empirical learning setting has the objective
% \begin{gather}
%     \min_{f \in \cF} \sum_{i=1}^n \ell(y_i, f(x_i)) + \lambda\cdot \Psi(f),
%     \textnormal{where } \Psi: \cF \to \reals_{\ge 0}, \lambda > 0, \label{eq: regular}
% \end{gather}
% where $\ell: \cY \times \cY \to \reals$ is a loss function and $\cF$ is a function class over $\reals^d$. A classical choice of model class is a reproducing kernel Hilbert space (RKHS). The accompanying regularization functional is the squared Hilbert norm of the RKHS.  In this scenario, the RKHS
% representer theorem establishes that there exists a solution to \eqnref{eq: regular} that takes the form of a linear combination of reproducing kernels centered at the data sites\\



% But last decade has shown that deep neural networks often outperform kernel
% methods in a wide variety of tasks, ranging from speech recognition [17] to image classification [22] to solving inverse problems in imaging [19]. Thus, there is great interest in
% understanding the properties of functions learned from data by neural networks, particularly
% with the rectified linear unit (ReLU) activation function, which is widey used in practice [25].



% The simplest form of a neural network are single-hidden layer neural networks which are basically superpositions of \emph{ridge functions}.
% A ridge function is any multivariate function mapping $\mathbb{R}^d \to \mathbb{R}$ of the form
% \[
% x \;\mapsto\; \rho\bigl(\mathbf{w}^\top x\bigr),
% \]
% where $\rho : \mathbb{R} \to \mathbb{R}$ is a univariate real-valued function and
% $\mathbf{w} \in \mathbb{R}^d \setminus \{0\}$.
% Single-hidden layer neural networks, in particular, are superpositions of the form
% \[
% x \;\mapsto\; \sum_{k=1}^K v_k \,\rho\!\bigl(\mathbf{w}_k^\top x \;-\; b_k\bigr),
% \]
% where $\rho : \mathbb{R} \to \mathbb{R}$ is the \emph{activation function}, $K$ is the width of the network, and for $k = 1,\dots,K$, $v_k \in \mathbb{R}$ and $\mathbf{w}_k \in \mathbb{R}^d \setminus \{0\}$ are the weights of the neural network and $b_k \in \mathbb{R}$ are the biases or offsets.

% A simple question is to \tt{what extent can neural networks approximate and the underlying gaps with RKHSs}?

% In supervised learning, we often encounter optimization problems that balance empirical performance with regularization. Given a labeled dataset $\curlybracket{(\bm{x}i,y_i)}{i=1}^n$, this takes the form:
% \begin{gather}
% \min_{f \in \cF} \sum_{i=1}^n \ell(y_i, f(\bm{x}i)) + \lambda\cdot \Psi(f),
% \textnormal{ where } \Psi: \cF \to \reals{\ge 0}, \lambda > 0,
% \end{gather}
% where $\ell: \cY \times \cY \to \reals$ represents the loss function and $\cF$ denotes a function class over $\reals^d$. Traditionally, reproducing kernel Hilbert spaces (RKHS) have served as a popular choice for the model class $\cF$, with regularization typically implemented through the squared Hilbert norm. The RKHS representer theorem guarantees that optimal solutions take the form of kernel combinations centered at training points.
% However, the past decade has witnessed the emergence of deep neural networks as powerful alternatives, consistently outperforming kernel methods across diverse applications including speech recognition, image classification, and inverse problems in imaging. This success has sparked interest in understanding neural network function approximation, particularly with the widely-adopted ReLU activation function.
% The simplest neural architecture—the single-hidden layer network—builds upon the concept of ridge functions, which map $\mathbb{R}^d \to \mathbb{R}$ through the form $\bm{x} \mapsto \rho(\mathbf{w}^\top \bm{x})$, where $\rho: \mathbb{R} \to \mathbb{R}$ is univariate and $\mathbf{w} \in \mathbb{R}^d \setminus {0}$. These networks combine multiple ridge functions:
% \[
% \bm{x} \mapsto \sum_{k=1}^K v_k \rho(\mathbf{w}_k^\top \bm{x} - b_k)
% \]
% where $K$ represents the network width, $v_k \in \mathbb{R}$ and $\mathbf{w}_k \in \mathbb{R}^d \setminus {0}$ are weights, and $b_k \in \mathbb{R}$ are biases. This formulation raises a fundamental question about the approximation capabilities of neural networks and their relationship to RKHS spaces.
 
% \akash{where does kernel ridge/ridgeless regression fit in this setting}\cite{Liang_2020}

% \akash{prior work on infinite sample asymptotics for kernels, specifically Gaussians}
% %we will study functions that map $\Omega \to \mathbb{R}$ within the Banach space of functions having second-order bounded variation in the Radon domain, denoted by $\rbv{\Omega}$. The study of these functions provides key insights into the approximation capabilities and theoretical properties of neural networks.
% Neural network approximation vs kernel methods~\cite{neural_kernel} \akash{neural network estimators can be immune to the curse of dimensionality.}
%Embedding into $\rbv{\Omega}$ space~\cite{haas2023mind,steinwart_svm} \cite{near_mini}
%\akash{a line on function spaces of these networks \citep{Parhi2019TheRO,Parhi2020BanachSR}}
