@book{xu2019generalized,
  title={Generalized Mercer kernels and reproducing kernel Banach spaces},
  author={Xu, Yuesheng and Ye, Qi},
  volume={258},
  number={1243},
  year={2019},
  publisher={American Mathematical Society}
}

@article{azizan2021stochastic,
  title={Stochastic mirror descent on overparameterized nonlinear models},
  author={Azizan, Navid and Lale, Sahin and Hassibi, Babak},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={33},
  number={12},
  pages={7717--7727},
  year={2021},
  publisher={IEEE}
}

@misc{kakade2010regularization,
      title={Regularization Techniques for Learning with Matrices}, 
      author={Sham M. Kakade and Shai Shalev-Shwartz and Ambuj Tewari},
      year={2010},
      eprint={0910.0610},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Zlinescu2002ConvexAI,
  title={Convex analysis in general vector spaces},
  author={Constantin Zălinescu},
  year={2002}
}

@book{luen,
author = {Luenberger, David G.},
title = {Optimization by Vector Space Methods},
year = {1997},
isbn = {047155359X},
publisher = {John Wiley \& Sons, Inc.},
address = {USA},
edition = {1st},
abstract = {From the Publisher:Engineers must make decisions regarding the distribution of expensive resources in a manner that will be economically beneficial. This problem can be realistically formulated and logically analyzed with optimization theory. This book shows engineers how to use optimization theory to solve complex problems. Unifies the large field of optimization with a few geometric principles. Covers functional analysis with a minimum of mathematics. Contains problems that relate to the applications in the book.}
}

@article{Zlinescu1983OnUC,
  title={On uniformly convex functions},
  author={Constantin Zălinescu},
  journal={Journal of Mathematical Analysis and Applications},
  year={1983},
  volume={95},
  pages={344-374}
}

@book{soloman,
    author = {T. Buehler and D.A. Salamon},
    title ={Functional Analysis},
    publisher ={AMS Graduate Studies in Mathematics},
    year ={2018},
    volume={191}
}

@misc{bubeck2015convex,
      title={Convex Optimization: Algorithms and Complexity}, 
      author={Sébastien Bubeck},
      year={2015},
      eprint={1405.4980},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{Lin2022,
  author = "Lin, Rong Rong and Zhang, Hai Zhang and Zhang, Jun",
  title = "On Reproducing Kernel Banach Spaces: Generic Definitions and Unified Framework of Constructions",
  journal = "Acta Mathematica Sinica, English Series",
  volume = "38",
  number = "8",
  pages = "1459-1483",
  year = "2022",
  month = "August",
  doi = "10.1007/s10114-022-1397-7",
  url = "https://doi.org/10.1007/s10114-022-1397-7",
  issn = "1439-7617",
}

@article{zhang09b,
  author  = {Haizhang Zhang and Yuesheng Xu and Jun Zhang},
  title   = {Reproducing Kernel Banach Spaces for Machine Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  number  = {95},
  pages   = {2741--2775},
  url     = {http://jmlr.org/papers/v10/zhang09b.html}
}

@book{Megginson1998,
  author    = {Megginson, R. E.},
  title     = {An Introduction to Banach Space Theory},
  publisher = {Springer-Verlag},
  address   = {New York},
  year      = {1998}
}

@book{Boyd2004,
  author    = {Boyd, Stephen and Vandenberghe, Lieven},
  title     = {Convex Optimization},
  publisher = {Cambridge University Press},
  year      = {2004},
  address   = {Cambridge},
}

@book{nemirovski1983problem,
  title={Problem Complexity and Method Efficiency in Optimization},
  author={Nemirovski, Arkadii and Yudin, David Borisovich},
  year={1983},
}

@article{beck2003mirror,
  title={Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003}
}

@inproceedings{BianchiMD,
author = {Cesa-Bianchi, Nicol\`{o} and Gaillard, Pierre and Lugosi, G\'{a}bor and Stoltz, Gilles},
title = {Mirror Descent Meets Fixed Share (and Feels No Regret)},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {980–988},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{bennett2000duality,
  title={Duality and Geometry in SVM Classifiers},
  author={Bennett, K. P. and Bredensteiner, E. J.},
  booktitle={Proceedings of the Seventeenth International Conference on Machine Learning},
  pages={57--64},
  year={2000},
  publisher={Morgan Kaufmann},
  address={San Francisco}
}

@InProceedings{micchelli2004,
author="Micchelli, Charles A.
and Pontil, Massimiliano",
editor="Shawe-Taylor, John
and Singer, Yoram",
title="A Function Representation for Learning in Banach Spaces",
booktitle="Learning Theory",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="255--269",
abstract="Kernel--based methods are powerful for high dimensional function representation. The theory of such methods rests upon their attractive mathematical properties whose setting is in Hilbert spaces of functions. It is natural to consider what the corresponding circumstances would be in Banach spaces. Led by this question we provide theoretical justifications to enhance kernel--based methods with function composition. We explore regularization in Banach spaces and show how this function representation naturally arises in that problem. Furthermore, we provide circumstances in which these representations are dense relative to the uniform norm and discuss how the parameters in such representations may be used to fit data.",
isbn="978-3-540-27819-1"
}

@article{micchelli2007,
author = {Micchelli, Charles A. and Pontil, Massimiliano},
title = {Feature Space Perspectives for Learning the Kernel},
year = {2007},
issue_date = {March 2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {66},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-006-0679-0},
doi = {10.1007/s10994-006-0679-0},
abstract = {In this paper, we continue our study of learning an optimal kernel in a prescribed convex set of kernels (Micchelli \& Pontil, 2005) . We present a reformulation of this problem within a feature space environment. This leads us to study regularization in the dual space of all continuous functions on a compact domain with values in a Hilbert space with a mix norm. We also relate this problem in a special case to $${cal L}^p$$ regularization.},
journal = {Mach. Learn.},
month = {mar},
pages = {297–319},
numpages = {23},
keywords = {Banach space regularization, Sparsity, Learning the kernels, Convex optimization, Kernel methods}
}

@inproceedings{micchelli2003cucker,
  title={Cucker Smale Learning Theory in Besov Spaces},
  author={Micchelli, C. A. and Xu, Y. and Ye, P.},
  booktitle={Advances in Learning Theory: Methods, Models and Applications},
  pages={47--68},
  publisher={IOS Press},
  address={Amsterdam, The Netherlands},
  year={2003}
}

@article{Zhang2002OnTD,
  title={On the Dual Formulation of Regularized Linear Systems with Convex Risks},
  author={Tong Zhang},
  journal={Machine Learning},
  year={2002},
  volume={46},
  pages={91-129}
}

@article{BARTOLUCCI2023194,
title = {Understanding neural networks with reproducing kernel Banach spaces},
journal = {Applied and Computational Harmonic Analysis},
volume = {62},
pages = {194-236},
year = {2023},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2022.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1063520322000768},
author = {Francesca Bartolucci and Ernesto {De Vito} and Lorenzo Rosasco and Stefano Vigogna},
keywords = {Neural networks, Reproducing kernel Banach spaces, Representer theorem, Radon transform},
abstract = {Characterizing the function spaces corresponding to neural networks can provide a way to understand their properties. In this paper we discuss how the theory of reproducing kernel Banach spaces can be used to tackle this challenge. In particular, we prove a representer theorem for a wide class of reproducing kernel Banach spaces that admit a suitable integral representation and include one hidden layer neural networks of possibly infinite width. Further, we show that, for a suitable class of ReLU activation functions, the norm in the corresponding reproducing kernel Banach space can be characterized in terms of the inverse Radon transform of a bounded real measure, with norm given by the total variation norm of the measure. Our analysis simplifies and extends recent results in [45], [36], [37].}
}

@inproceedings{venkatesh2023,
author = {Shilton, Alistair and Gupta, Sunil and Rana, Santu and Venkatesh, Svetha},
title = {Gradient Descent in Neural Networks as Sequential Learning in Reproducing Kernel Banach Space},
year = {2023},
publisher = {JMLR.org},
abstract = {The study of Neural Tangent Kernels (NTKs) has provided much needed insight into convergence and generalization properties of neural networks in the over-parametrized (wide) limit by approximating the network using a first-order Taylor expansion with respect to its weights in the neighborhood of their initialization values. This allows neural network training to be analyzed from the perspective of reproducing kernel Hilbert spaces (RKHS), which is informative in the overparametrized regime, but a poor approximation for narrower networks as the weights change more during training. Our goal is to extend beyond the limits of NTK toward a more general theory. We construct an exact power-series representation of the neural network in a finite neighborhood of the initial weights as an inner product of two feature maps, respectively from data and weight-step space, to feature space, allowing neural network training to be analyzed from the perspective of reproducing kernel Banach space (RKBS). We prove that, regardless of width, the training sequence produced by gradient descent can be exactly replicated by regularized sequential learning in RKBS. Using this, we present novel bound on uniform convergence where the iterations count and learning rate play a central role, giving new theoretical insight into neural network training.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1302},
numpages = {54},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{E2018APE,
  title={A priori estimates of the population risk for two-layer neural networks},
  author={Weinan E and Chao Ma and Lei Wu},
  journal={Communications in Mathematical Sciences},
  year={2018}
}

@article{Spek2022DualityFN,
  title={Duality for Neural Networks through Reproducing Kernel Banach Spaces},
  author={Len Spek and Tjeerd Jan Heeringa and Christoph Brune},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.05020}
}

@article{Parhi2023FunctionSpaceOO,
  title = {Function-Space Optimality of Neural Architectures with Multivariate Nonlinearities},
  author = {Parhi, Rahul and Unser, Michael},
  year = {2025},
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {7},
  number = {1},
  pages = {110--135},
  doi = {10.1137/23M1620971},
}

@misc{chung2023barron,
      title={Barron Space for Graph Convolution Neural Networks}, 
      author={Seok-Young Chung and Qiyu Sun},
      year={2023},
      eprint={2311.02838},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Sun2023AUA,
  title={A Unified Approach to Controlling Implicit Regularization via Mirror Descent},
  author={Haoyuan Sun and Khashayar Gatmiry and Kwangjun Ahn and Navid Azizan},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.13853}
}

@inproceedings{mdnavid,
 author = {Sun, Haoyuan and Ahn, Kwangjun and Thrampoulidis, Christos and Azizan, Navid},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {31089--31101},
 publisher = {Curran Associates, Inc.},
 title = {Mirror Descent Maximizes Generalized Margin and Can Be Implemented Efficiently},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c9694bf4f9bf3626f7d21158bab74f8e-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{tradeoffML,
 author = {Bottou, L\'{e}on and Bousquet, Olivier},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Tradeoffs of Large Scale Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf},
 volume = {20},
 year = {2007}
}

@article{BenTal2001TheOS,
  title={The Ordered Subsets Mirror Descent Optimization Method with Applications to Tomography},
  author={Aharon Ben-Tal and Tamar Margalit and Arkadi Nemirovski},
  journal={SIAM J. Optim.},
  year={2001},
  volume={12},
  pages={79-108}
}

@article{Censor1992ProximalMA,
  title={Proximal minimization algorithm withD-functions},
  author={Yair Censor and Stavros A. Zenios},
  journal={Journal of Optimization Theory and Applications},
  year={1992},
  volume={73},
  pages={451-464}
}

@article{eckstein,
author = {Eckstein, Jonathan},
title = {Nonlinear Proximal Point Algorithms Using Bregman Functions, with Applications to Convex Programming},
year = {1993},
issue_date = {February 1993},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {18},
number = {1},
issn = {0364-765X},
abstract = {A Bregman function is a strictly convex, differentiable function that induces a well-behaved distance measure or D-function on Euclidean space. This paper shows that, for every Bregman function, there exists a "nonlinear" version of the proximal point algorithm, and presents an accompanying convergence theory. Applying this generalization of the proximal point algorithm to convex programming, one obtains the D-function proximal minimization algorithm of Censor and Zenios, and a wide variety of new multiplier methods. These multiplier methods are different from those studied by Kort and Bertsekas, and include nonquadratic variations on the proximal method of multipliers.},
journal = {Math. Oper. Res.},
month = {feb},
pages = {202–226},
numpages = {25},
keywords = {convex programming, proximal point algorithms, method of multipliers, monotone operators}
}

@incollection{zhang2018categorization,
  title={Categorization Based on Similarity and Features: The Reproducing Kernel Banach Space Approach},
  author={Zhang, J. and Zhang, H.},
  booktitle={New Handbook of Mathematical Psychology, Volume 2},
  editor={Batchelder, W. and Colonius, H. and Dzhafarov, E. N. and Myung, J.},
  publisher={Springer},
  year={2018}
}

@article{ZHANG20111,
title = {Frames, Riesz bases, and sampling expansions in Banach spaces via semi-inner products},
journal = {Applied and Computational Harmonic Analysis},
volume = {31},
number = {1},
pages = {1-25},
year = {2011},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2010.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1063520310001120},
author = {Haizhang Zhang and Jun Zhang},
keywords = {Frames, Riesz bases, Bessel sequences, Riesz–Fischer sequences, Banach spaces, Semi-inner products, Duality mappings, Shannonʼs sampling expansions, Reproducing kernel Banach spaces, Reproducing kernel Hilbert spaces, Gaussian kernels},
abstract = {Frames in a Banach space B were defined as a sequence in its dual space B⁎ in some recent references. We propose to define them as a collection of elements in B by making use of semi-inner products. Classical theory on frames and Riesz bases is generalized under this new perspective. We then aim at establishing the Shannon sampling theorem in Banach spaces. The existence of such expansions in translation invariant reproducing kernel Hilbert and Banach spaces is discussed.}
}

@article{song201396,
title = {Reproducing kernel Banach spaces with the $\ell^1$ norm},
journal = {Applied and Computational Harmonic Analysis},
volume = {34},
number = {1},
pages = {96-116},
year = {2013},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2012.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1063520312000486},
author = {Guohui Song and Haizhang Zhang and Fred J. Hickernell},
keywords = {Reproducing kernel Banach spaces, Sparse learning, Lasso, Basis pursuit, Regularization, The representer theorem, The Brownian bridge kernel, The exponential kernel},
abstract = {Targeting at sparse learning, we construct Banach spaces B of functions on an input space X with the following properties: (1) B possesses an ℓ1 norm in the sense that B is isometrically isomorphic to the Banach space of integrable functions on X with respect to the counting measure; (2) point evaluations are continuous linear functionals on B and are representable through a bilinear form with a kernel function; and (3) regularized learning schemes on B satisfy the linear representer theorem. Examples of kernel functions admissible for the construction of such spaces are given.}
}

@inproceedings{bharath2011,
 author = {Fukumizu, Kenji and Lanckriet, Gert and Sriperumbudur, Bharath K.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},
 volume = {24},
 year = {2011}
}

@article{argyriou10a,
  author  = {Andreas Argyriou and Charles A. Micchelli and Massimiliano Pontil},
  title   = {On Spectral Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {31},
  pages   = {935--953},
  url     = {http://jmlr.org/papers/v11/argyriou10a.html}
}


@InProceedings{shilton23a,
  title = 	 {Gradient Descent in Neural Networks as Sequential Learning in Reproducing Kernel Banach Space},
  author =       {Shilton, Alistair and Gupta, Sunil and Rana, Santu and Venkatesh, Svetha},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {31435--31488},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/shilton23a/shilton23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/shilton23a.html},
  abstract = 	 {The study of Neural Tangent Kernels (NTKs) has provided much needed insight into convergence and generalization properties of neural networks in the over-parametrized (wide) limit by approximating the network using a first-order Taylor expansion with respect to its weights in the neighborhood of their initialization values. This allows neural network training to be analyzed from the perspective of reproducing kernel Hilbert spaces (RKHS), which is informative in the over-parametrized regime, but a poor approximation for narrower networks as the weights change more during training. Our goal is to extend beyond the limits of NTK toward a more general theory. We construct an exact power-series representation of the neural network in a finite neighborhood of the initial weights as an inner product of two feature maps, respectively from data and weight-step space, to feature space, allowing neural network training to be analyzed from the perspective of reproducing kernel Banach space (RKBS). We prove that, regardless of width, the training sequence produced by gradient descent can be exactly replicated by regularized sequential learning in RKBS. Using this, we present novel bound on uniform convergence where the iterations count and learning rate play a central role, giving new theoretical insight into neural network training.}
}

@ARTICLE{unser16,
  author={Unser, Michael and Fageot, Julien and Gupta, Harshit},
  journal={IEEE Transactions on Information Theory}, 
  title={Representer Theorems for Sparsity-Promoting  $\ell _{1}$  Regularization}, 
  year={2016},
  volume={62},
  number={9},
  pages={5167-5180},
  doi={10.1109/TIT.2016.2590421}
}

@article{SHI2011286,
title = {Concentration estimates for learning with $\ell_1$-regularizer and data dependent hypothesis spaces},
journal = {Applied and Computational Harmonic Analysis},
volume = {31},
number = {2},
pages = {286-302},
year = {2011},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2011.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1063520311000157},
author = {Lei Shi and Yun-Long Feng and Ding-Xuan Zhou},
keywords = {Learning theory, Data dependent hypothesis space, -regularizer and sparsity, Concentration estimate for error analysis, -empirical covering number},
abstract = {We consider the regression problem by learning with a regularization scheme in a data dependent hypothesis space and ℓ1-regularizer. The data dependence nature of the kernel-based hypothesis space provides flexibility for the learning algorithm. The regularization scheme is essentially different from the standard one in a reproducing kernel Hilbert space: the kernel is not necessarily symmetric or positive semi-definite and the regularizer is the ℓ1-norm of a function expansion involving samples. The differences lead to additional difficulty in the error analysis. In this paper we apply concentration techniques with ℓ2-empirical covering numbers to improve the learning rates for the algorithm. Sparsity of the algorithm is studied based on our error analysis. We also show that a function space involved in the error analysis induced by the ℓ1-regularizer and non-symmetric kernel has nice behaviors in terms of the ℓ2-empirical covering numbers of its unit ball.}
}

@ARTICLE{tong10,
  author={Tong, Hongzhi and Chen, Di-Rong and Yang, Fenghong},
  journal={Neural Computation}, 
  title={Least Square Regression with lp-Coefficient Regularization}, 
  year={2010},
  volume={22},
  number={12},
  pages={3221-3235},
  doi={10.1162/NECO_a_00044}
}


@InProceedings{der07a,
  title = 	 {Large-Margin Classification in Banach Spaces},
  author = 	 {Der, Ricky and Lee, Daniel},
  booktitle = 	 {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
  pages = 	 {91--98},
  year = 	 {2007},
  editor = 	 {Meila, Marina and Shen, Xiaotong},
  volume = 	 {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Juan, Puerto Rico},
  month = 	 {21--24 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v2/der07a/der07a.pdf},
  url = 	 {https://proceedings.mlr.press/v2/der07a.html},
  abstract = 	 {We propose a framework for dealing with binary hard-margin classification in Banach spaces, centering on the use of a supporting semi-inner-product (s.i.p.) taking the place of an inner-product in Hilbert spaces. The theory of semi-inner-product spaces allows for a geometric, Hilbert-like formulation of the problems, and we show that a surprising number of results from the Euclidean case can be appropriately generalised. These include the Representer theorem, convexity of the associated optimization programs, and even, for a particular class of Banach spaces, a “kernel trick” for non-linear classification.}
}

@article{fasshauer2015115,
title = {Solving support vector machines in reproducing kernel Banach spaces with positive definite functions},
journal = {Applied and Computational Harmonic Analysis},
volume = {38},
number = {1},
pages = {115-139},
year = {2015},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2014.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S1063520314000475},
author = {Gregory E. Fasshauer and Fred J. Hickernell and Qi Ye},
keywords = {Support vector machine, Regularized empirical risk, Reproducing kernel, Reproducing kernel Banach space, Positive definite function, Matérn function, Sobolev spline},
abstract = {In this paper we solve support vector machines in reproducing kernel Banach spaces (RKBSs) instead of the traditional methods in reproducing kernel Hilbert spaces (RKHSs). Using the orthogonality of semi-inner-products of RKBSs, we can obtain the finite-dimensional representations of the dual (normalized-duality-mapping) elements of support vector machine solutions. In addition, we can use Fourier transform techniques to introduce the concept of reproduction in a generalized native space such that it becomes a reproducing kernel Banach space, which can even be embedded into Sobolev spaces. Moreover, its reproducing kernel is associated with a positive definite function. The representations of the optimal solutions of support vector machines (regularized empirical risks) in these reproducing kernel Banach spaces are formulated explicitly and finite-dimensionally in terms of the positive definite functions, and their finite numbers of suitable parameters can be computed by the fixed point iteration. We also give some typical examples of reproducing kernel Banach spaces induced by Matérn functions (Sobolev splines) such that their support vector machine solutions even are computable efficiently. Moreover, each of their reproducing bases includes information from multiple training data points. These kernel-based algorithms give a fresh numerical tool for support vector classifiers.}
}

@article{tibshirani,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2024-01-14},
 volume = {58},
 year = {1996}
}

@ARTICLE{candes,
  author={Candes, E.J. and Romberg, J. and Tao, T.},
  journal={IEEE Transactions on Information Theory}, 
  title={Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information}, 
  year={2006},
  volume={52},
  number={2},
  pages={489-509},
  doi={10.1109/TIT.2005.862083}}

@article{Parhi2019TheRO,
  title = {The Role of Neural Network Activation Functions},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2020},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  pages = {1779--1783},
  doi = {10.1109/LSP.2020.3027517},
}

@misc{srinivasan2022contracting,
      title={Contracting dynamical systems in Banach spaces}, 
      author={Anand Srinivasan and Jean-Jacques Slotine},
      year={2022},
      eprint={2112.13541},
      archivePrefix={arXiv},
      primaryClass={math.DS}
}

@article{Parhi2021WhatKO,
  title = {What Kinds of Functions Do Deep Neural Networks Learn? Insights from Variational Spline Theory},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2022},
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {4},
  number = {2},
  pages = {464--489},
  doi = {10.1137/21M1418642},
}

@article{He2022RandomFF,
  title={Random Fourier Features for Asymmetric Kernels},
  author={Ming-qian He and Fan He and Fanghui Liu and Xiaolin Huang},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.08461}
}

@article{Bauschke2017ADL,
  title={A Descent Lemma Beyond Lipschitz Gradient Continuity: First-Order Methods Revisited and Applications},
  author={Heinz H. Bauschke and J{\'e}r{\^o}me Bolte and Marc Teboulle},
  journal={Math. Oper. Res.},
  year={2017},
  volume={42},
  pages={330-348}
}

@article{Azizan2019ASI,
  title={A Stochastic Interpretation of Stochastic Mirror Descent: Risk-Sensitive Optimality},
  author={Navid Azizan and Babak Hassibi},
  journal={2019 IEEE 58th Conference on Decision and Control (CDC)},
  year={2019},
  pages={3960-3965}
}

@article{Parhi2020BanachSR,
  title = {Banach Space Representer Theorems for Neural Networks and Ridge Splines},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {43},
  pages = {1--40},
  url = {https://jmlr.org/papers/v22/20-583.html},
}

@article{Xu2023SparseML,
  title={Sparse Machine Learning in Banach Spaces.},
  author={Yuesheng Xu},
  journal={Applied numerical mathematics : transactions of IMACS},
  year={2023},
  volume={187},
  pages={
          138-157
        }
}

@article{Bach2014BreakingTC,
  title={Breaking the Curse of Dimensionality with Convex Neural Networks},
  author={Francis R. Bach},
  journal={ArXiv},
  year={2014},
  volume={abs/1412.8690}
}

@article{Gribonval2019ApproximationSO,
  title={Approximation Spaces of Deep Neural Networks},
  author={R{\'e}mi Gribonval and Gitta Kutyniok and Morten Nielsen and Felix Voigtl{\"a}nder},
  journal={Constructive Approximation},
  year={2019},
  volume={55},
  pages={259-367}
}

@article{Pinkus1999ApproximationTO,
  title={Approximation theory of the MLP model in neural networks},
  author={Allan Pinkus},
  journal={Acta Numerica},
  year={1999},
  volume={8},
  pages={143 - 195}
}

@inproceedings{Savarese2019HowDI,
  title={How do infinite width bounded norm networks look in function space?},
  author={Pedro H. P. Savarese and Itay Evron and Daniel Soudry and Nathan Srebro},
  booktitle={Annual Conference Computational Learning Theory},
  year={2019}
}

@article{Wang2023SparseRT,
  title={Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces},
  author={Rui Wang and Yuesheng Xu and Mingsong Yan},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.12584}
}

@article{Wright2021TransformersAD,
  title={Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines},
  author={Matthew A. Wright and Joseph Gonzalez},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.01506}
}

@article{Chung2023BarronSF,
  title={Barron Space for Graph Convolution Neural Networks},
  author={Seok-Young Chung and Qiyu Sun},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.02838}
}

@article{Chen2023PrimalAttentionST,
  title={Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation},
  author={Yingyi Chen and Qinghua Tao and Francesco Tonin and Johan A. K. Suykens},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.19798}
}

@InProceedings{Ye13,
author="Ye, Qi",
editor="Fasshauer, Gregory E.
and Schumaker, Larry L.",
title="Support Vector Machines in Reproducing Kernel Hilbert Spaces Versus Banach Spaces",
booktitle="Approximation Theory XIV: San Antonio 2013",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="377--395",
abstract="In this article, we compare the support vector classifiers in Hilbert spaces versus those in Banach spaces. Recently, we developed a new concept of reproducing kernel Banach spaces (RKBSs). These spaces are a natural generalization of reproducing kernel Hilbert spaces (RKHSs) by extending the reproduction property from inner products to dual bilinear products. Based on the techniques of Fourier transforms, we can construct RKBSs by many well-known positive definite functions, e.g., Mat{\'e}rn functions and Gaussian functions. In addition, we can obtain finite-dimensional solutions of support vector machines defined in infinite-dimensional RKBSs. Finally, the numerical examples provided in this paper show that the solution of support vector machines in a RKBS can be computed and easily coded just as the classical algorithms given in RKHSs.",
isbn="978-3-319-06404-8"
}

@article{Zhang2012RegularizedLI,
  title={Regularized learning in Banach spaces as an optimization problem: representer theorems},
  author={Haizhang Zhang and Jun Zhang},
  journal={Journal of Global Optimization},
  year={2012},
  volume={54},
  pages={235-250}
}

@misc{wang2023sparse,
      title={Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces}, 
      author={Rui Wang and Yuesheng Xu and Mingsong Yan},
      year={2023},
      eprint={2305.12584},
      archivePrefix={arXiv},
      primaryClass={math.FA}
}

@inproceedings{Lee2016GradientDO,
  title={Gradient Descent Only Converges to Minimizers},
  author={J. Lee and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  booktitle={Annual Conference Computational Learning Theory},
  year={2016}
}

@book{yuri,
author = {Nesterov, Yurii},
title = {Introductory Lectures on Convex Optimization: A Basic Course},
year = {2014},
isbn = {1461346916},
publisher = {Springer Publishing Company, Incorporated},
edition = {1},
abstract = {It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].}
}

@article{POLYAK1963864,
title = {Gradient methods for the minimisation of functionals},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {3},
number = {4},
pages = {864-878},
year = {1963},
issn = {0041-5553},
doi = {https://doi.org/10.1016/0041-5553(63)90382-3},
url = {https://www.sciencedirect.com/science/article/pii/0041555363903823},
author = {B.T. Polyak},
abstract = {Let tf(t) be a functional defined in the (real) Hubert space H. The problem consists in finding its minimum value tff∗ = inf tf(x) and some minimum point x∗ (if such exists).}
}


@article{lojasiewicz1963topological,
  title={A topological property of real analytic subsets},
  author={Łojasiewicz, S.},
  journal={Coll. du CNRS, Les equations aux derivees partielles},
  year={1963},
  pages={87--89}
}

@article{Bubeck2014ConvexOA,
  title={Convex Optimization: Algorithms and Complexity},
  author={S{\'e}bastien Bubeck},
  journal={arXiv: Optimization and Control},
  year={2014}
}

@InProceedings{PL_linear,
author="Karimi, Hamed
and Nutini, Julie
and Schmidt, Mark",
editor="Frasconi, Paolo
and Landwehr, Niels
and Manco, Giuseppe
and Vreeken, Jilles",
title="Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-{\L}ojasiewicz Condition",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="795--811",
abstract="In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the {\L}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-{\L}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of coordinate descent and stochastic gradient for many non-strongly-convex (and some non-convex) functions. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence for support vector machines and L1-regularized least squares without additional assumptions.",
isbn="978-3-319-46128-1"
}

@book{nesterov,
author = {Nesterov, Yurii},
title = {Lectures on Convex Optimization},
year = {2018},
isbn = {3319915770},
publisher = {Springer Publishing Company, Incorporated},
edition = {2nd},
abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the authors lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics.}
}

@book{Wendland_2004, place={Cambridge}, series={Cambridge Monographs on Applied and Computational Mathematics}, title={Scattered Data Approximation}, publisher={Cambridge University Press}, author={Wendland, Holger}, year={2004}, collection={Cambridge Monographs on Applied and Computational Mathematics}}


@misc{he2024learninganalysiskernelridgeless,
      title={Learning Analysis of Kernel Ridgeless Regression with Asymmetric Kernel Learning}, 
      author={Fan He and Mingzhen He and Lei Shi and Xiaolin Huang and Johan A. K. Suykens},
      year={2024},
      eprint={2406.01435},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.01435}, 
}


@article{Borwein1994SecondOD,
  title={Second order differentiability of convex functions in Banach spaces},
  author={Jonathan Michael Borwein and Dominikus Noll},
  journal={Transactions of the American Mathematical Society},
  year={1994},
  volume={342},
  pages={43-81},
  url={https://api.semanticscholar.org/CorpusID:51851560}
}

@misc{wachsmuth2022simpleproofbaillonhaddadtheorem,
      title={A simple proof of the Baillon-Haddad theorem on open subsets of Hilbert spaces}, 
      author={Daniel Wachsmuth and Gerd Wachsmuth},
      year={2022},
      eprint={2204.00282},
      archivePrefix={arXiv},
      primaryClass={math.FA},
      url={https://arxiv.org/abs/2204.00282}, 
}

@article{Kwapień1972,
author = {Kwapień, S.},
journal = {Studia Mathematica},
language = {eng},
number = {6},
pages = {583-595},
title = {Isomorphic characterizations of inner product spaces by orthogonal series with vector valued coefficients},
url = {http://eudml.org/doc/217719},
volume = {44},
year = {1972},
}

@book{Fabian,
author = {Fabian, Marián and Habala, Petr and Hájek, Petr and Montesinos, Vicente and Zizler, Václav},
year = {2011},
month = {01},
pages = {},
title = {Banach Space Theory: The Basis for Linear and Nonlinear Analysis},
isbn = {978-1-4419-7514-0},
doi = {10.1007/978-1-4419-7515-7}
}

@book{vaclav,
author = {Guirao, Antonio and Montesinos, Vicente and Zizler, Václav},
year = {2022},
month = {01},
pages = {},
title = {Renormings in Banach Spaces: A Toolbox},
isbn = {978-3-031-08654-0},
doi = {10.1007/978-3-031-08655-7}
}

@article{JMLR:v13:kakade12a,
  author  = {Sham M. Kakade and Shai Shalev-Shwartz and Ambuj Tewari},
  title   = {Regularization Techniques for Learning with Matrices},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {59},
  pages   = {1865--1890},
  url     = {http://jmlr.org/papers/v13/kakade12a.html}
}

@inproceedings{Scholkopf2001LearningWK,
  title={Learning with Kernels: support vector machines, regularization, optimization, and beyond},
  author={Bernhard Scholkopf and Alex Smola},
  booktitle={Adaptive computation and machine learning series},
  year={2001},
  url={https://api.semanticscholar.org/CorpusID:52872213}
}
@inproceedings{Landkof1972FoundationsOM,
  title={Foundations of Modern Potential Theory},
  author={N. S. Landkof},
  year={1972},
  url={https://api.semanticscholar.org/CorpusID:117170694}
}


@InProceedings{savarese19a,
  title = 	 {How do infinite width bounded norm networks look in function space?},
  author =       {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {2667--2690},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/savarese19a/savarese19a.pdf},
  url = 	 {https://proceedings.mlr.press/v99/savarese19a.html},
  abstract = 	 {We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded; or equivalently what is the minimal norm required to approximate a given function. For functions $f:\mathbb R \rightarrow\mathbb R$ and a single hidden layer, we show that the minimal network norm for representing $f$ is $\max(\int \lvert f”(x) \rvert \mathrm{d} x, \lvert  f’(-\infty) + f’(+\infty) \rvert)$, and hence the minimal norm fit for a sample is given by a linear spline interpolation.  }
}


@inproceedings{
Ongie2020A,
title={A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case},
author={Greg Ongie and Rebecca Willett and Daniel Soudry and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1lNPxHKDH}
}

@book{Blackledge,
author = {Blackledge, J.M.},
year = {2005},
month = {11},
pages = {1-797},
title = {Digital Image Processing: Mathematical and Computational Methods}
}

@Inbook{Szabados2006,
author="Szabados, J{\'o}zsef",
editor="Horv{\'a}th, J{\'a}nos",
title="Orthogonal Polynomials",
bookTitle="A Panorama of Hungarian Mathematics in the Twentieth Century I",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="55--70",
abstract="The theory of orthogonal polynomials plays an important role in many branches of mathematics, such as approximation theory (best approximation, interpolation, quadrature), special functions, continued fractions, differential and integral equations. The notion of orthogonality originated from the theory of continued fractions, but later became an independent (and possibly more important) discipline. Among the contributors to the theory of orthogonal polynomials, we can find such outstanding mathematicians as Abel, Chebyshev, Fourier, Hermite, Laguerre, Laplace, Legendre, Markov, and Stieltjes, just to name a few. Beginning with G{\'a}bor Szeg{\H{o}}, Hungarian mathematicians like P{\'a}l Erd{\"o}s, P{\'a}l Tur{\'a}n, G{\'e}za Freud, Ervin Feldheim and others have made essential contributions to the flourishing theory of orthogonal polynomials in the last century. At this point I would like to mention two names who have made considerable efforts to propagate the work of the above mentioned Hungarian mathematicians: Richard Askey and Doron Lubinsky.",
isbn="978-3-540-30721-1",
doi="10.1007/978-3-540-30721-1_3",
url="https://doi.org/10.1007/978-3-540-30721-1_3"
}


@book{szegő1975orthogonal,
  title={Orthogonal Polynomials},
  author={Szeg{\H{o}}, G.},
  isbn={9780821810231},
  lccn={77476087},
  series={American Math. Soc: Colloquium publ},
  url={https://books.google.com/books?id=ZOhmnsXlcY0C},
  year={1975},
  publisher={American Mathematical Society}
}


@ARTICLE{barron_universal,
  author={Barron, A.R.},
  journal={IEEE Transactions on Information Theory}, 
  title={Universal approximation bounds for superpositions of a sigmoidal function}, 
  year={1993},
  volume={39},
  number={3},
  pages={930-945},
  keywords={Artificial neural networks;Fourier transforms;Approximation error;Feeds;Linear approximation;Neural networks;Feedforward neural networks;Information theory;Statistics;Statistical distributions},
  doi={10.1109/18.256500}}

@article{near_mini,
  title = {Near-Minimax Optimal Estimation With Shallow {ReLU} Neural Networks},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2023},
  journal = {IEEE Transactions on Information Theory},
  volume = {69},
  number = {2},
  pages = {1125--1140},
  doi = {10.1109/TIT.2022.3208653},
}

@inproceedings{neural_kernel,
author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
title = {When do neural networks outperform kernel methods?},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For a certain scaling of the initialization of stochastic gradient descent (SGD), wide neural networks (NN) have been shown to be well approximated by reproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed that, for some classification tasks, RKHS methods can replace NNs without a large loss in performance. On the other hand, two-layers NNs are known to encode richer smoothness classes than RKHS and we know of special examples for which SGD-trained NN provably outperform RKHS. This is true even in the wide network limit, for a different scaling of the initialization.How can we reconcile the above claims? For which tasks do NNs outperform RKHS? If covariates are nearly isotropic, RKHS methods suffer from the curse of dimensionality, while NNs can overcome it by learning the best low-dimensional representation. Here we show that this curse of dimensionality becomes milder if the covariates display the same low-dimensional structure as the target function, and we precisely characterize this tradeoff. Building on these results, we present the spiked covariates model that can capture in a unified framework both behaviors observed in earlier works.We hypothesize that such a latent low-dimensional structure is present in image classification. We numerically test this hypothesis by showing that specific perturbations of the training distribution degrade the performances of RKHS methods much more significantly than NNs.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1242},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{
haas2023mind,
title={Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension},
author={Moritz Haas and David Holzm{\"u}ller and Ulrike von Luxburg and Ingo Steinwart},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=yjYwbZBJyl}
}

@book{steinwart_svm,
author = {Steinwart, Ingo and Christmann, Andreas},
title = {Support Vector Machines},
year = {2008},
isbn = {0387772413},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology.}
}

@article{
mean_field_songmei,
author = {Song Mei  and Andrea Montanari  and Phan-Minh Nguyen },
title = {A mean field view of the landscape of two-layer neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {33},
pages = {E7665-E7671},
year = {2018},
doi = {10.1073/pnas.1806579115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1806579115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1806579115},
abstract = {Multilayer neural networks have proven extremely successful in a variety of tasks, from image classification to robotics. However, the reasons for this practical success and its precise domain of applicability are unknown. Learning a neural network from data requires solving a complex optimization problem with millions of variables. This is done by stochastic gradient descent (SGD) algorithms. We study the case of two-layer networks and derive a compact description of the SGD dynamics in terms of a limiting partial differential equation. Among other consequences, this shows that SGD dynamics does not become more complex when the network size increases. Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that—in a suitable scaling limit—SGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for “averaging out” some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.}}

@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{rotskoff2022trainability,
  title={Trainability and accuracy of artificial neural networks: An interacting particle system approach},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={9},
  pages={1889--1935},
  year={2022},
  publisher={Wiley Online Library}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A law of large numbers},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={SIAM Journal on Applied Mathematics},
  volume={80},
  number={2},
  pages={725--752},
  year={2020},
  publisher={SIAM}
}


@article{arora2019harnessing,
  title={Harnessing the power of infinitely wide deep nets on small-data tasks},
  author={Arora, Sanjeev and Du, Simon S and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong and Yu, Dingli},
  journal={arXiv preprint arXiv:1910.01663},
  year={2019}
}
@article{li2019enhanced,
  title={Enhanced convolutional neural tangent kernels},
  author={Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1911.00809},
  year={2019}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{novak2018bayesian,
  title={Bayesian deep convolutional networks with many channels are gaussian processes},
  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.05148},
  year={2018}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}
@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@article{garriga2018deep,
  title={Deep convolutional networks as shallow gaussian processes},
  author={Garriga-Alonso, Adri{\`a} and Rasmussen, Carl Edward and Aitchison, Laurence},
  journal={arXiv preprint arXiv:1808.05587},
  year={2018}
}

@inproceedings{shankar2020neural,
  title={Neural kernels without tangents},
  author={Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and Fridovich-Keil, Sara and Ragan-Kelley, Jonathan and Schmidt, Ludwig and Recht, Benjamin},
  booktitle={International conference on machine learning},
  pages={8614--8623},
  year={2020},
  organization={PMLR}
}

@article{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{ghorbani2021linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  year={2021}
}

@article{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{bach_curse,
author = {Bach, Francis},
title = {Breaking the curse of dimensionality with convex neural networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the nonconvex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomialtime algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {629–681},
numpages = {53},
keywords = {non-parametric estimation, neural networks, convex relaxation, convex optimization}
}

@inproceedings{allen2023backward,
  title={Backward feature correction: How deep learning performs deep (hierarchical) learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={4598--4598},
  year={2023},
  organization={PMLR}
}

@article{allen2019can,
  title={What can resnet learn efficiently, going beyond kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{Liang_2020,
   title={Just interpolate: Kernel “Ridgeless” regression can generalize},
   volume={48},
   ISSN={0090-5364},
   url={http://dx.doi.org/10.1214/19-AOS1849},
   DOI={10.1214/19-aos1849},
   number={3},
   journal={The Annals of Statistics},
   publisher={Institute of Mathematical Statistics},
   author={Liang, Tengyuan and Rakhlin, Alexander},
   year={2020},
   month=jun }

@misc{kanagawa2018gaussianprocesseskernelmethods,
      title={Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences}, 
      author={Motonobu Kanagawa and Philipp Hennig and Dino Sejdinovic and Bharath K Sriperumbudur},
      year={2018},
      eprint={1807.02582},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1807.02582}, 
}

@inproceedings{
chen2021deep,
title={Deep Neural Tangent Kernel and Laplace Kernel Have the Same {\{}RKHS{\}}},
author={Lin Chen and Sheng Xu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=vK9WrZ0QYQ}
}

@inproceedings{
bietti2021deep,
title={Deep Equals Shallow for Re{LU} Networks in Kernel Regimes},
author={Alberto Bietti and Francis Bach},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=aDjoksTpXOP}
}

@inproceedings{Steinwart2009OptimalRF,
  title={Optimal Rates for Regularized Least Squares Regression},
  author={Ingo Steinwart and Don R. Hush and Clint Scovel},
  booktitle={Annual Conference Computational Learning Theory},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:7741716}
}

@misc{mao2024approximationratesshallowreluk,
      title={Approximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev Spaces via the Radon Transform}, 
      author={Tong Mao and Jonathan W. Siegel and Jinchao Xu},
      year={2024},
      eprint={2408.10996},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2408.10996}, 
}

@book{kak_slaney,
  author       = {Kak, A C and Slaney, M},
  title        = {Principles of computerized tomographic imaging},
  annote       = {Tomography refers to the cross-sectional imaging of an object from either transmission or reflection data collected by illuminating the object from many different directions. The impact of tomography in diagnostic medicine has been revolutionary, since it has enabled doctors to view internal organs with unprecedented precision and safety to the patient. There are also numerous nonmedical imaging applications which lend themselves to methods of computerized tomography, such as mapping of underground resources...cross-sectional imaging of for nondestructive testing...the determination of the brightness distribution over a celestial sphere...three-dimensional imaging with electron microscopy. Principles of Computerized Tomographic Imaging provides a tutorial overview of topics in tomographic imaging covering mathematical principles and theory...how to apply the theory to problems in medical imaging and other fields...several variations of tomography that are currently being researched.},
  url          = {https://www.osti.gov/biblio/5813672},
  place        = {United States},
  publisher    = {IEEE Service Center,Piscataway, NJ},
  year         = {1988},
  month        = {01}}


@article{mei_rate,
author = {Mei, Song and Bai, Yu and Montanari, Andrea},
year = {2016},
month = {07},
pages = {},
title = {The Landscape of Empirical Risk for Non-convex Losses},
volume = {46},
journal = {Annals of Statistics},
doi = {10.1214/17-AOS1637}
}

@article{luxberg_curse,
author = {Luxburg, Ulrike von and Bousquet, Olivier},
title = {Distance--Based Classification with Lipschitz Functions},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {The goal of this article is to develop a framework for large margin classification in metric spaces. We want to find a generalization of linear decision functions for metric spaces and define a corresponding notion of margin such that the decision function separates the training points with a large margin. It will turn out that using Lipschitz functions as decision functions, the inverse of the Lipschitz constant can be interpreted as the size of a margin. In order to construct a clean mathematical setup we isometrically embed the given metric space into a Banach space and the space of Lipschitz functions into its dual space. To analyze the resulting algorithm, we prove several representer theorems. They state that there always exist solutions of the Lipschitz classifier which can be expressed in terms of distance functions to training points. We provide generalization bounds for Lipschitz classifiers in terms of the Rademacher complexities of some Lipschitz function classes. The generality of our approach can be seen from the fact that several well-known algorithms are special cases of the Lipschitz classifier, among them the support vector machine, the linear programming machine, and the 1-nearest neighbor classifier.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {669–695},
numpages = {27}
}

@article{ghorbani_curse,
author = {Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
title = {{Linearized two-layers neural networks in high dimension}},
volume = {49},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {1029 -- 1054},
keywords = {approximation bounds, kernel ridge regression, neural tangent kernel, random features, Two-layers neural networks},
year = {2021},
doi = {10.1214/20-AOS1990},
URL = {https://doi.org/10.1214/20-AOS1990}
}
