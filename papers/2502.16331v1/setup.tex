\section{Problem Setup and Preliminaries}\label{sec: setup}

\subsection{Notation and Basic Definitions}
Let $\mathcal{X} \subseteq \mathbb{R}^d$ denote the centers (aka sample) space, where $d \in \mathbb{N}$ represents the odd dimension of the input space. Throughout this work, we focus on the case where $\mathcal{X} = \mathbb{R}^d$, considering the full Euclidean space.

\subsection{Gaussian Reproducing Kernel Hilbert Space}
We begin by defining a reproducing kernel Hilbert space (RKHS) associated with a Gaussian kernel on an infinite domain. For a given positive definite Mahalanobis matrix $\mathbf{M} \in \text{Sym}_{+}(\mathbb{R}^{d \times d})$, we define the Gaussian kernel $k_{\mathbf{M}}: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ as:
\begin{equation}
    k_{\mathbf{M}}(\bm{x},\bm{y}) = \exp\left(-\frac{\|\bm{x}-\bm{y}\|_\mathbf{M}^2}{2\sigma^2}\right),
\end{equation}
where $\sigma > 0$ is a fixed scale parameter and the Mahalanobis distance is defined as:
\begin{equation}
    \|\bm{x} - \bm{y}\|_\mathbf{M}^2 = (\bm{x} - \bm{y})^{\mathsf{T}}\mathbf{M}(\bm{x} - \bm{y}).
\end{equation}

The corresponding RKHS $\mathcal{H}$ is defined as the closure of the linear span of kernel functions:
\begin{align}
    \cH := \sf{cl}\paren{\curlybracket{f: \cX \to \reals\,\bigg\lvert\, n \in \mathbb{N}, f(\cdot) = \sum_{i = 1}^n \alpha_i \cdot k_{\mathbf{M}}(\bm{x}_i,\cdot),\, \bm{x}_i \in \cX}}, \label{eq: infinitegauss}
\end{align}
where the (squared) RKHS norm $\|\cdot\|_{\cH}^2$ of a kernel machine $f \in \cH$ is defined as $\|f\|_{\cH}^2 = \sum_{i,j} \alpha_i \alpha_j k_{\bm{M}}(\bm{x}_i,\bm{x}_j)$. Alternately, we can write $\|f\|_{\cH}^2 = \alpha^{\mathsf{T}}\mathbf{K}\alpha$ where $\mathbf{K} = (k_{\mathbf{M}}(\bm{x}_i,\bm{x}_j))_{i,j}$ is an $n \times n$ matrix. 
%\akash{define the norm}
%\akash{$\sum_{i,j = 1}^{\infty} \alpha_i\alpha_j k_{\textbf{M}}(\bm{x}_i,\bm{x}_j) < \infty$}
%\rahul{closre with respect to what topology? Also, the sum being $< \infty$ is trivally true since this space is only finite sums. Please correct this so that it is introduced as RKHSs are in books/etc.}
%To ensure correctness, we show all the necessary properties of the reproducing kernel Hilbert space $\cH(\reals^d)$ in \iftoggle{longversion}{\appref{app: properties}}{the supplemental materials}. \rahul{There is no reason to include that stuff since, by construction (once the definition above is fixed), is an RKHS (see any machine learning textbook)}

\subsection{Separated Sets and Function Spaces}
For our analysis, we introduce two key definitions of separated sets that play a crucial role in our theoretical development.
\begin{definition}[$(\bm{\beta}, \delta)$-separated set]
For any given scalar $\delta > 0$ and a vector $\bm{\beta} \in \reals^d$, a $(\bm{\beta}, \delta)$-separated subset of size $n \in \mathbb{N}$ is defined as  
$$\cC_n(\bm{\beta}, \delta) := \curlybracket{\{\bm{x}_1,\ldots, \bm{x}_n\} \,|\, \, \forall \bm{x}_i,\bm{x}_j, |\bm{\beta}^{\mathsf{T}} \bm{x}_i - \bm{\beta}^{\mathsf{T}} \bm{x}_j| \ge \delta }.$$    
\end{definition}
This could be further generalized to the notion
\begin{definition}[$(\bm{\beta}, \delta, \eta)$-separated set] For any given scalars $\delta, \eta > 0$ and a vector $\bm{\beta} \in \reals^d$ a $(\bm{\beta}, \delta, \eta)$-separated subset of size $n \in \mathbb{N}$ is defined as 
    $$\cC_n(\bm{\beta}, \delta, \eta) := \curlybracket{\{\bm{x}_1,\ldots, \bm{x}_n\} \,|\, \, \forall \bm{x}_i,\bm{x}_j, \bm{\beta'} \in \reals^d, \text{ s.t. } \bm{\beta}^{\mathsf{T}}\bm{\beta}' \ge \eta \norm{\bm{\beta}}\norm{\bm{\beta}'}, |\bm{\beta}'^{\mathsf{T}} \bm{x}_i - \bm{\beta}'^{\mathsf{T}} \bm{x}_j| \ge \delta}.$$    
\end{definition}

\begin{example}\label{exam: set}
Let $\bm{\beta} = (1,0,\dots,0)$. For all $\eta_0 \ge \eta$, pick $\bm{\beta}' 
  \;=\; 
  \bigl(\eta_0,\sqrt{1-\eta_0^2},0,\dots,0\bigr)$ 
  so that
  $\|\bm{\beta}'\|=1 
  \;\;\text{and}\;\; 
  \bm{\beta}^{\mathsf{T}} \bm{\beta}' \;=\; \eta_0 \ge \eta$. Now define 
\[
  \bm{x}_i := (i-1)\,\delta\,\bm{\beta}', 
  \quad i=1,\dots,n.
\]
For $i\neq j$, we have 
\[
  \bigl|\bm{\beta}'^{\mathsf{T}} \bm{x}_i 
        - \bm{\beta}'^{\mathsf{T}} \bm{x}_j\bigr|
  = |(i-j)|\,\delta\,\|\bm{\beta}'\|^2 
  = |i-j|\,\delta 
  \,\ge\, \delta.
\]
Hence, $\{\bm{x}_1,\dots,\bm{x}_n\}$ is in $(\bm{\beta},\delta,\eta)$-separated subset of size $n$.
\end{example}

\begin{definition}[Unbounded Combinations]
For a kernel machine $f \in \mathcal{H}$ with representation $f = \sum_{i=1}^\infty \alpha_i k(\bm{x}_i,\cdot)$, we say the coefficient vector $\alpha = (\alpha_i)_{i=1}^\infty$ is unbounded with respect to $f$ if $\|\alpha\|_1 = \sum_{i=1}^\infty |\alpha_i| = \infty$.
\end{definition}

\begin{example}\label{exam: divergence}
Consider a kernel machine $f \in \cH$ corresponding to the combination $\alpha = (a_n)$ defined by $a_n = \frac{1}{n}$ for each $n \in \mathbb{N}$ and a sequence of centers $(\bm{x}_n) \subset \reals^d$ such that for all $i,j$, $\norm{\bm{x}_i - \bm{x}_j} \ge |i-j| \delta$ for some fixed scalar $\delta > 0$. For this construction, Gaussian RKHS norm $\alpha^{\mathsf{T}} \mathbf{K} \alpha = \sum_{i = 1}^\infty\sum_{j = 1}^\infty \alpha_i\alpha_j k(\bm{x}_i,\bm{x}_j) < \infty$, but $\|\alpha\|_{\ell_1}$ is unbounded. 
\end{example}
We provide the proof of the statement in the example above in \iftoggle{longversion}{\appref{app: diverging}}{the supplemental materials}.

\subsection{Probabilist's Hermite Polynomials}
Probabilist's Hermite polynomials~\citep{szegÅ‘1975orthogonal}, denoted by \( \text{He}_d(z) : \reals \to \reals \), are defined by the generating function
\[
\exp\Bigl(zt-\frac{t^2}{2}\Bigr)=\sum_{d=0}^\infty \text{He}_d(z)\frac{t^d}{d!},
\]
and they are orthogonal with respect to the standard normal density:
\[
\int_{-\infty}^{\infty} \text{He}_d(z)\, \text{He}_{d'}(z)\, \frac{1}{\sqrt{2\pi}} \exp\!\Bigl(-\frac{z^2}{2}\Bigr)\,dz = d!\,\delta_{dd'},
\]
where \(\delta_{dd'}\) is the Kronecker delta. We use the notation $H_d$ to denote the polynomial unless stated otherwise.

\subsection{Radon Transform and Bounded Variation Space}

\paragraph{Radon Transform} For a function \(f : \mathbb{R}^d \to \mathbb{R}\), its Radon transform 
\(\mathcal{R}\{f\}\) is defined by
\[
  \mathcal{R}\{f\}(\gamma, t)
  = \int_{\{ \bm{x} \in \mathbb{R}^d : \gamma^{\mathsf{T}} \bm{x} = t \}} 
    f(\bm{x})\,ds(\bm{x}),
\]
where \(\gamma \in \mathbb{S}^{d-1}\), \(t \in \mathbb{R}\), and \(ds(\bm{x})\) is the 
\((d-1)\)-dimensional Lebesgue measure on the hyperplane 
% \(\{\bm{x} : \gamma^{\mathsf{T}} \bm{x} = t\}\). Equivalently,
% \[
%   \mathcal{R}\{f\}(\gamma, t)
%   = \int_{\mathbb{R}^d} f(\bm{x})\,\delta\bigl(\gamma^{\mathsf{T}} \bm{x} - t\bigr)\,d\bm{x},
% \]
% where \(\delta\) is the Dirac delta distribution.

\paragraph{Radon Bounded Variation space} We define the second-order Radon bounded variation space $\rbv{\reals^d}$ as:
\begin{equation}
    \rbv{\reals^d} = \left\{f \in L^{\infty,1}(\mathbb{R}^d) : \rtv{f} < \infty\right\},
\end{equation}
where $L^{\infty,1}(\mathbb{R}^d)$ is the Banach space of functions with at most linear growth on $\mathbb{R}^d$. The second-order Radon total variation norm $\rtv{f}$ is defined as:
\begin{equation}
    \rtv{f} = c_d \|\partial_t^2\Lambda^{d-1}\mathcal{R}f\|_{\mathcal{M}(\mathbb{S}^{d-1}\times\mathbb{R})}. \label{eq: rnorm}
\end{equation}
Here, $\Lambda^{d-1} = (-\partial_t^2)^{\frac{d-1}{2}}$ is the ramp filter operator, $c_d^{-1} = 2(2\pi)^{d-1}$ is a dimension-dependent constant, $\|\cdot\|_{\mathcal{M}(\mathbb{S}^{d-1}\times\mathbb{R})}$ denotes the total variation norm on the measure space $\mathcal{M}(\mathbb{S}^{d-1}\times\mathbb{R})$, and all operations (including fractional derivatives) are understood 
in the distributional sense (see \cite{Parhi2020BanachSR} for more details). The Radon Total Variation semi-norm in \eqnref{eq: rnorm} is essentially the representational cost of a function as a single hidden layer infinite-width network aka $\cR$-norm 
introduced in \citet{Ongie2020A}.
% \subsection{Notation and Basic Definitions}
% Let $\mathcal{X} \subseteq \mathbb{R}^d$ denote the sample space, where $d \in \mathbb{N}$ represents the odd dimension of the input space. Throughout this work, we focus on the case where $\mathcal{X} = \mathbb{R}^d$, considering the full Euclidean space.
% \akash{fix the samples bold font}
% \subsection{Gaussian Reproducing Kernel Hilbert Space}
% We begin by defining a reproducing kernel Hilbert space (RKHS) associated with a Gaussian kernel on an infinite domain. For a given positive definite Mahalanobis matrix $\mathbf{M} \in \text{Sym}_{+}(\mathbb{R}^{d \times d})$, we define the Gaussian kernel $k_{\mathbf{M}}: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ as:
% \begin{equation}
%     k_{\mathbf{M}}(x,y) = \exp\left(-\frac{\|x-y\|_\mathbf{M}^2}{2\sigma^2}\right)
% \end{equation}
% where $\sigma > 0$ is a fixed scale parameter and the Mahalanobis distance is defined as:
% \begin{equation}
%     \|x - y\|_\mathbf{M}^2 = (x - y)^{\mathsf{T}}\mathbf{M}(x - y)
% \end{equation}

% The corresponding RKHS $\mathcal{H}$ is defined as the closure of the linear span of kernel functions:
% \begin{align}
%     \cH := \sf{cl}\paren{\curlybracket{f: \cX \to \reals\,\bigg\lvert\, n \in \mathbb{N}, f(\cdot) = \sum_{i = i}^n \alpha_i \cdot k_{\textbf{M}}(x_i,\cdot),\, x_i \in \cX,\, \sum_{i,j}^{n} \alpha_i\alpha_j k_{\textbf{M}}(x_i,x_j) < \infty}} \label{eq: infinitegauss}
% \end{align}
% To ensure correctness, we show all the necessary properties of the reproducing kernel Hilbert space $\cH(\reals^d)$ in \appref{app: properties}.

% % \paragraph{Unbounded-domain Gaussian RKHS} In this work, we consider a reproducing kernel Hilbert space of a Gaussian kernel defined on an infinite domain. Define the Gaussian RKHS $\cH$ corresponding to a Mahalanobis matrix $\textbf{M} \succ 0$ such that $\textbf{M} \in \sf{Sym}_{+}(\reals^{d \times d})$ as follows:
% % \begin{align}
% %     \cH := \sf{cl}\paren{\curlybracket{f: \cX \to \reals\,\bigg\lvert\, n \in \mathbb{N}, f(\cdot) = \sum_{i = i}^n \alpha_i \cdot k_{\textbf{M}}(x_i,\cdot),\, x_i \in \cX,\, \sum_{i,j}^{n} \alpha_i\alpha_j k_{\textbf{M}}(x_i,x_j) < \infty}} \label{eq: infinitegauss}
% % \end{align}
% % where the sample space $\cX \subseteq \reals^d$ is an open unbounded set. In this work, we consider the setting where $\cX = \reals^d$. To ensure correctness, we show all the necessary properties of the reproducing kernel Hilbert space $\cH(\reals^d)$ in \appref{app: properties}.


% % Let $k_{\textbf{M}}: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ be the Gaussian kernel for a Mahalanbis matrix $\textbf{M}$ defined by:

% % $$k_{\textbf{M}}(x,y) = \exp\left(-\frac{\|x-y\|_\textbf{M}^2}{2\sigma^2}\right)$$

% % where $\sigma > 0$ is fixed and the (squared) Mahalanobis distance is
% % \[
% %     \norm{\bm{x} - \bm{x}_0}_\mathbf{M}^2 = (\bm{x} - \bm{x}_0)^\mathsf{T}\mathbf{M}(\bm{x} - \bm{x}_0).
% % \]
% % %Let $\mathcal{H}$ be the RKHS associated with this kernel.
% % %In order to show the existence of a sequence of kernel classifiers with with  
% \subsection{Separated Sets and Function Spaces}
% For our analysis, we introduce two key definitions of separated sets that play a crucial role in our theoretical development.
% \begin{definition}[$(\bm{\beta}, \delta)$-separated set]
% For any given scalar $\delta > 0$ and a vector $\bm{\beta} \in \reals^d$, a $(\bm{\beta}, \delta)$-separated subset of size $n \in \mathbb{N}$ is defined as  
% $$\cC_n(\bm{\beta}, \delta) := \curlybracket{\{x_1,\ldots, x_n\} \,|\, \, \forall x_i,x_j, |\bm{\beta}^{\mathsf{T}} x_i - \bm{\beta}^{\mathsf{T}} x_j| \ge \delta }.$$    
% \end{definition}
% This could be further generalized to the notion
% \begin{definition}[$(\bm{\beta}, \delta, \eta)$-separated set] For any given scalars $\delta, \eta > 0$ and a vector $\bm{\beta} \in \reals^d$ a $(\bm{\beta}, \delta, \eta)$-separated subset of size $n \in \mathbb{N}$ is defined as 
%     $$\cC_n(\bm{\beta}, \delta, \eta) := \curlybracket{\{x_1,\ldots, x_n\} \,|\, \, \forall x_i,x_j, \bm{\beta'} \in \reals^d, \text{ s.t. } \bm{\beta}^{\mathsf{T}}\bm{\beta}' \ge \eta \norm{\bm{\beta}}\norm{\bm{\beta}'}, |\bm{\beta}'^{\mathsf{T}} x_i - \bm{\beta}'^{\mathsf{T}} x_j| \ge \delta}.$$    
% \end{definition}
% % \begin{example}
% % Let $\|\beta\|=1$ and choose $\beta'=\beta$; then 
% % \[
% %   \frac{\beta^{\mathsf{T}} \beta'}{\|\beta\|\|\beta'\|} = 1 \,\ge\, \eta.
% % \]
% % Define $x_i := (i-1)\delta\,\beta$ for $i=1,\dots,n$. For any $i\neq j$,
% % \[
% %   |\beta'^{\mathsf{T}} x_i - \beta'^{\mathsf{T}} x_j| 
% %   = \bigl|\,(i-j)\,\delta\,(\beta^{\mathsf{T}} \beta)\bigr| 
% %   = |i-j|\delta \,\ge\, \delta,
% % \]
% % so $\{x_1,\dots,x_n\}$ is $(\bm{\beta},\delta,\eta)$-separated.
% % \end{example}
% \begin{example}\label{exam: set}
% Let $\beta = (1,0,\dots,0)$. For all $\eta_0 \ge \eta$, pick $\beta' 
%   \;=\; 
%   \bigl(\eta_0,\sqrt{1-\eta_0^2},0,\dots,0\bigr) 
%   \quad\text{so that}\quad
%   \|\beta'\|=1 
%   \;\;\text{and}\;\; 
%   \beta^{\mathsf{T}} \beta' \;=\; \eta_0 \ge \eta$. Now define 
% \[
%   x_i := (i-1)\,\delta\,\beta', 
%   \quad i=1,\dots,n.
% \]
% For $i\neq j$, we have 
% \[
%   \bigl|\beta'^{\mathsf{T}} x_i 
%         - \beta'^{\mathsf{T}} x_j\bigr|
%   = |(i-j)|\,\delta\,\|\beta'\|^2 
%   = |i-j|\,\delta 
%   \,\ge\, \delta.
% \]
% Hence, $\{x_1,\dots,x_n\}$ is in $(\bm{\beta},\delta,\eta)$-separated subset of size $n$.
% \end{example}
% \begin{definition}[Unbounded Combinations]
% For a function $f \in \mathcal{H}$ with representation $f = \sum_{i=1}^\infty \alpha_i k(x_i,\cdot)$, we say the coefficient vector $\alpha = (\alpha_i)_{i=1}^\infty$ is unbounded with respect to $f$ if $\|\alpha\|_1 = \sum_{i=1}^\infty |\alpha_i| = \infty$.
% \end{definition}
% \begin{example}\label{exam: divergence}
% A kernel classifier $f$ with an unbounded combinations representations is for the sequence $\alpha = (a_n)$ defined by $a_n = \frac{1}{n}$ for each $n \in \mathbb{N}$ wrt to a sequence of samples $(x_n) \subset \reals^d$ such that for all $i,j$, $\norm{x_i - x_j} \ge |i-j| \delta$ for some fixed scalar $\delta > 0$. For this construction, Gaussian RKHS norm $\alpha^{\mathsf{T}} K \alpha = \sum_{i = 1}^\infty\sum_{j = 1}^\infty \alpha_i\alpha_j k(x_i,x_j) < \infty$, but $\ell_1(\alpha)$ is unbounded. 
% \end{example}
% We provide the proof of the statement in the example above in \appref{app: diverging}.
% \subsection{Probabilist's Hermite Polynomials}
% Probabilist's Hermite polynomials, denoted by \( \text{He}_d(x) \), are defined by the generating function
% \[
% \exp\Bigl(xt-\frac{t^2}{2}\Bigr)=\sum_{d=0}^\infty \text{He}_d(x)\frac{t^d}{d!},
% \]
% and they are orthogonal with respect to the standard normal density:
% \[
% \int_{-\infty}^{\infty} \text{He}_d(x)\, \text{He}_{d'}(x)\, \frac{1}{\sqrt{2\pi}} \exp\!\Bigl(-\frac{x^2}{2}\Bigr)\,dx = d!\,\delta_{dd'},
% \]
% where \(\delta_{dd'}\) is the Kronecker delta. For further details, see \cite{szegÅ‘1975orthogonal}. In the rest of the section, we use the notation $H_d$ or $H_{d+1}$ to denote the polynomial unless stated otherwise.

% \subsection{Radon Transform and Bounded Variation Space}

% \paragraph{Radon Transform} For a function \(f : \mathbb{R}^d \to \mathbb{R}\), its Radon transform 
% \(\mathcal{R}\{f\}\) is defined by
% \[
%   \mathcal{R}\{f\}(\gamma, t)
%   = \int_{\{ x \in \mathbb{R}^d : \gamma^{\mathsf{T}} x = t \}} 
%     f(x)\,ds(x),
% \]
% where \(\gamma \in S^{d-1}\), \(t \in \mathbb{R}\), and \(ds(x)\) is the 
% \((d-1)\)-dimensional surface measure on the hyperplane 
% \(\{x : \gamma^{\mathsf{T}} x = t\}\). Equivalently,
% \[
%   \mathcal{R}\{f\}(\gamma, t)
%   = \int_{\mathbb{R}^d} f(x)\,\delta\bigl(\gamma^{\mathsf{T}} x - t\bigr)\,dx,
% \]
% where \(\delta\) is the Dirac delta distribution.

% \paragraph{Radon Bounded Variation space} We define the Radon bounded variation space $\rtv{\reals^d}$ as:
% \begin{equation}
%     \rtv{\reals^d} = \left\{f \in L^{\infty,1}(\mathbb{R}^d) : \|f\|_{\text{RTV}} < \infty\right\}
% \end{equation}
% where $L^{\infty,1}(\mathbb{R}^d)$ is the Banach space of functions with at most linear growth on $\mathbb{R}^d$. The Radon total variation norm $\rtv{f}$ is defined as:
% \begin{equation}
%     \rtv{f} = c_d \|\partial_t^2\Lambda^{d-1}\mathcal{R}\{f\}\|_{\mathcal{M}(S^{d-1}\times\mathbb{R})} \label{eq: rnorm}
% \end{equation}
% Here, $\Lambda^{d-1} = (-\partial_t^2)^{\frac{d-1}{2}}$ is the ramp filter operator, $c_d^{-1} = 2(2\pi)^{d-1}$ is a dimension-dependent constant, $\|\cdot\|_{\mathcal{M}(S^{d-1}\times\mathbb{R})}$ denotes the total variation norm on the measure space $\mathcal{M}(S^{d-1}\times\mathbb{R})$, and all operations (including fractional derivatives) are understood 
% in the distributional sense (see \cite{Parhi2020BanachSR} for more details). The Radon Total Variation semi-norm in \eqnref{eq: rnorm} is essentially the representational cost of a function as a single hidden layer infinite-width network aka $\cR$-norm introduced in \citet{Ongie2020A,savarese19a}. %\akash{need to connect to the idea of the work}

% % .

% % When working with the Radon
% % transform of functions defined on $\reals^d$, the following ramp filter arises in the Radon inversion
% % formula
% % % Ramp operator (fractional derivative wrt t):
% % \[
% %   \Lambda^{d-1} 
% %   \;=\; 
% %   \bigl(-\partial_t^2\bigr)^{\frac{d-1}{2}},
% % \]
% % where \(\partial_t\) denotes the partial derivative with respect to the 
% % offset variable \(t\).  Fractional powers of \(-\partial_t^2\) are 
% % defined in terms of Riesz potentials (see, e.g., \cite{Landkof1972FoundationsOM}).
% % % Second-order BV space in the Radon domain:
% % \[
% %   \rbv{\reals^d}
% %   \;=\;
% %   \bigl\{
% %      f \in L^{\infty,1}(\mathbb{R}^d)
% %      \;\colon\;
% %      \rtv{f} < \infty
% %   \bigr\},
% % \]
% % where \(L^{\infty,1}(\mathbb{R}^d)\) is the Banach space of functions 
% % with at most linear growth on \(\mathbb{R}^d\). With this we define the $\rtv{f}$ of a classifier as
% % % Definition of the second-order total variation via the Radon transform:
% % \begin{align}
% %   \rtv{f}
% %   \;=\;
% %   c_d \,
% %   \bigl\|\,
% %     \partial_t^2\,\Lambda^{d-1}\,\mathcal{R}\{f\}
% %   \bigr\|_{\mathcal{M}(S^{d-1}\times\mathbb{R})},  \label{eq: rnorm}
% % \end{align}
% % where the norm $\norm{\cdot}_{\mathcal{M}(S^{d-1}\times\mathbb{R})}$ is the total variation norm on the measure space $\mathcal{M}(S^{d-1}\times\mathbb{R})$ and \(c_d^{-1} = 2(2\pi)^{d-1}\) is a dimension-dependent constant 
% % and all operations (including fractional derivatives) are understood 
% % in the distributional sense (see \cite{Parhi2020BanachSR} for more details). The Radon Total Variation semi-norm in \eqnref{eq: rnorm} is essentially the representational cost of a function as a single hidden layer infinite-width network that is also known as $\cR$-norm as introduced in \citet{Ongie2020A,savarese19a}. \akash{need to connect to the idea of the work}


% % ----------------



% % \subsection{Notation and Basic Definitions}
% % Let $\mathcal{X} \subseteq \mathbb{R}^d$ denote the sample space, where $d \in \mathbb{N}$ represents the dimension of the input space. Throughout this work, we focus on the case where $\mathcal{X} = \mathbb{R}^d$, considering the full Euclidean space.



% % \subsection{Separated Sets and Function Spaces}
% % For our analysis, we introduce two key definitions of separated sets that play a crucial role in our theoretical development.

% % \begin{definition}[$(\boldsymbol{\beta}, \delta)$-separated set]
% % For any $\delta > 0$ and vector $\boldsymbol{\beta} \in \mathbb{R}^d$, a $(\boldsymbol{\beta}, \delta)$-separated subset of size $n \in \mathbb{N}$ is defined as:
% % \begin{equation}
% %     \mathcal{C}_n(\boldsymbol{\beta}, \delta) := \left\{\{x_1,\ldots, x_n\} \,|\, \forall i,j \in \{1,\ldots,n\}, |\boldsymbol{\beta}^{\mathsf{T}} x_i - \boldsymbol{\beta}^{\mathsf{T}} x_j| \geq \delta \right\}
% % \end{equation}
% % \end{definition}

% % \begin{definition}[$(\boldsymbol{\beta}, \delta, \eta)$-separated set]
% % For scalars $\delta, \eta > 0$ and vector $\boldsymbol{\beta} \in \mathbb{R}^d$, a $(\boldsymbol{\beta}, \delta, \eta)$-separated subset of size $n \in \mathbb{N}$ is defined as:
% % \begin{equation}
% %     \mathcal{C}_n(\boldsymbol{\beta}, \delta, \eta) := \left\{\{x_1,\ldots, x_n\} \,|\, \forall i,j, \exists \boldsymbol{\beta}' \in \mathbb{R}^d: \boldsymbol{\beta}^{\mathsf{T}}\boldsymbol{\beta}' \geq \eta \|\boldsymbol{\beta}\|\|\boldsymbol{\beta}'\|, |\boldsymbol{\beta}'^{\mathsf{T}} x_i - \boldsymbol{\beta}'^{\mathsf{T}} x_j| \geq \delta\right\}
% % \end{equation}
% % \end{definition}

% % \begin{definition}[Unbounded Combinations]
% % For a function $f \in \mathcal{H}$ with representation $f = \sum_{i=1}^\infty \alpha_i k(x_i,\cdot)$, we say the coefficient vector $\alpha = (\alpha_i)_{i=1}^\infty$ is unbounded with respect to $f$ if $\|\alpha\|_1 = \sum_{i=1}^\infty |\alpha_i| = \infty$.
% % \end{definition}

% % \subsection{Radon Transform and Bounded Variation Space}

% % \paragraph{Radon Transform}
% % For a function $f: \mathbb{R}^d \to \mathbb{R}$, its Radon transform $\mathcal{R}\{f\}$ is defined as:
% % \begin{equation}
% %     \mathcal{R}\{f\}(\gamma, t) = \int_{\{x \in \mathbb{R}^d : \gamma^{\mathsf{T}} x = t\}} f(x)\,ds(x)
% % \end{equation}
% % where $\gamma \in S^{d-1}$, $t \in \mathbb{R}$, and $ds(x)$ is the $(d-1)$-dimensional surface measure on the hyperplane $\{x : \gamma^{\mathsf{T}} x = t\}$. Equivalently:
% % \begin{equation}
% %     \mathcal{R}\{f\}(\gamma, t) = \int_{\mathbb{R}^d} f(x)\,\delta(\gamma^{\mathsf{T}} x - t)\,dx
% % \end{equation}
% % where $\delta$ is the Dirac delta distribution.

% % \paragraph{Radon Bounded Variation Space}
% % We define the Radon bounded variation space $\text{RBV}(\mathbb{R}^d)$ as:
% % \begin{equation}
% %     \text{RBV}(\mathbb{R}^d) = \left\{f \in L^{\infty,1}(\mathbb{R}^d) : \|f\|_{\text{RTV}} < \infty\right\}
% % \end{equation}
% % where $L^{\infty,1}(\mathbb{R}^d)$ is the Banach space of functions with at most linear growth on $\mathbb{R}^d$. The Radon total variation norm $\|f\|_{\text{RTV}}$ is defined as:
% % \begin{equation}
% %     \|f\|_{\text{RTV}} = c_d \|\partial_t^2\Lambda^{d-1}\mathcal{R}\{f\}\|_{\mathcal{M}(S^{d-1}\times\mathbb{R})}
% % \end{equation}
% % Here, $\Lambda^{d-1} = (-\partial_t^2)^{\frac{d-1}{2}}$ is the ramp filter operator, $c_d^{-1} = 2(2\pi)^{d-1}$ is a dimension-dependent constant, and $\|\cdot\|_{\mathcal{M}(S^{d-1}\times\mathbb{R})}$ denotes the total variation norm on the measure space $\mathcal{M}(S^{d-1}\times\mathbb{R})$.