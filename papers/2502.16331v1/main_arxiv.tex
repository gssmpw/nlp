\documentclass[final,12pt]{colt2025} % Anonymized submission
%\documentclass[final,12pt]{colt2025} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\input{macros}
\usepackage{minitoc}
\def\eps{\varepsilon}
\newcommand{\curly}[1]{\left\{#1\right\}}
%\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\round}[1]{\left(#1\right)}
\newcommand{\inner}[1]{\left<#1\right>}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\def\Banach{\mathcal{B}}
\def\Hilbert{\mathcal{H}}
\def\Real{\mathbb{R}}
\def\tran{^\top}
\def\alphavec{\bm{\alpha}}
\def\betavec{\bm{\beta}}
\def\x{\bm{x}}
\def\y{\bm{y}}
\def\r{\bm{r}}
\def\Kmat{\mathbf{K}}
%\setlength{\parskip}{2ex}
\def\prox{\textsf{prox}}
\def\wb{\overline}
%\usepackage{booktabs}
\newtoggle{longversion}
\settoggle{longversion}{true}
%\title{Gap in Gaussian RKHS and Neural Networks: An infinite sample asymptotic}
%\author{}
%\date{}
%\usepackage{natbib}
\hypersetup{hidelinks,colorlinks,citecolor=orange}

\title[A Gap Between the Gaussian RKHS and Neural Networks]{A Gap Between the Gaussian RKHS and Neural Networks: \\ An Infinite-Center Asymptotic Analysis}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}
%\newcommand{\norm}[1]{\left\|#1\right\|}
% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\coltauthor{%
 \Name{Akash Kumar} \Email{akk002@ucsd.edu}\\
 \addr Department of Computer Science and Engineering \\
 \addr University of California, San Diego
 \AND
 \Name{Rahul Parhi} \Email{rahul@ucsd.edu}\\
 \addr Department of Electrical and Computer Engineering \\
 \addr University of California, San Diego
 \AND
 \Name{Mikhail Belkin} \Email{mbelkin@ucsd.edu}\\
 \addr Department of Computer Science and Engineering \\
 \addr Halıcıoğlu Data Science Institute \\
 \addr University of California, San Diego
}

\begin{document}
%\rahul{title does not make sense grammatically. An infinite-sample asymptotic what? You cannot end a sentence with an adjective. Also, after reading through the main results, it is not about infinite samples since it is not about data-fitting/learning. The title does not capture what is going on in the paper. A better title would be ``A Gap Between the Gaussian RKHS and Neural Networks on Unbounded Domains'' or something similar}

\maketitle
\begin{abstract}
% \rahul{abstracts should not be multiple paragraphs and should not be too heavy on math notation (I personally prefer no math notation in abstracts)}
%Recent work has studied the space of two-layered infinite-width neural networks with a characterization as bounded variation space $\rbv{\Omega}$ over the domain $\Omega \subset \reals^d$. These spaces contain several classical
% multivariate function spaces including the $L_1$- and $L_2$-Sobolev
% spaces of order $d+ 1$, where $d$ is the ambient dimension of
% the domain $\Omega \subset \reals^d$. It is classically known that this sort
% of Sobolev-regularity is sufficient to overcome the curse of
% dimensionality. On the other hand, $\rbv{\Omega}$ also contains
% functions that are much less regular. In particular, functions
% with significant variation and irregularity, but only in a few
% directions, also belong to $\rbv{\Omega}$. 
% Recent work has characterized the space of two-layered infinite-width neural networks as a bounded variation space $\rbv{\Omega}$ over domains $\Omega \subset \reals^d$. These spaces encompass several classical multivariate function spaces, including the $L_1$- and $L_2$-Sobolev spaces of order $d+1$, where $d$ represents the ambient dimension of the domain. This Sobolev regularity provides sufficient structure to overcome the curse of dimensionality in approximation theory. Notably, $\rbv{\Omega}$ also contains functions with less classical regularity, particularly those exhibiting significant variations in only a few directions.

% For bounded domains, it is well-established that Gaussian reproducing kernel Hilbert spaces (RKHS) strictly continuously embed within $\rbv{\Omega}$, demonstrating a clear gap between Gaussian RKHS with $\rbv{\Omega}$. However, this relationship becomes more nuanced in unbounded domains. In this work, we investigate the setting where $\Omega = \reals^d$ and establish a fundamental result: certain Gaussian kernel functions cannot be represented within $\rbv{\reals^d}$, providing a contrasting non-trivial gap in the complement of the intersection of these two spaces.
%In this work, we consider the problem of showing a gap between the approximability of kernel classifiers and two-layered infinite width neural networks.

Recent works have characterized the function-space inductive bias of infinite-width bounded-norm single-hidden-layer neural networks as a kind of bounded-variation-type space. This novel neural network Banach space encompasses many classical multivariate function spaces including certain Sobolev spaces and the spectral Barron spaces. Notably, this Banach space also includes functions that exhibit less classical regularity such as those that only vary in a few directions. On bounded domains, it is well-established that the Gaussian reproducing kernel Hilbert space (RKHS) strictly embeds into this Banach space, demonstrating a clear gap between the Gaussian RKHS and the neural network Banach space. It turns out that when investigating these spaces on unbounded domains, e.g., all of $\mathbb{R}^d$, the story is fundamentally different. We establish the following fundamental result: Certain functions that lie in the Gaussian RKHS have infinite norm in the neural network Banach space. This provides a nontrivial gap between kernel methods and neural networks by the exhibition of functions in which kernel methods can do strictly better than neural networks.
\end{abstract}

% \begin{keywords}%
%   List of keywords%
% \end{keywords}
\input{intro}
\input{related_work}
\input{setup}
\input{R-norm}
\input{infinite_RKHS}
%\input{results}
%\input{rkbs}


%\nocite*
%\bibliographystyle{abbrvnat}
\bibliography{ref}
 \iftoggle{longversion}{
\newpage


% Clear the current TOC
\setcounter{tocdepth}{-5}  % Temporarily hide everything
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}  % Reset depth for appendix

% Create appendix TOC
\renewcommand{\contentsname}{A Gap Between the Gaussian RKHS and Neural Networks:
Supplementary Materials}
\tableofcontents


\appendix
\input{app_explicit}
\input{app_change_of_variable}
\input{app_usefulproperty}
\input{app_example}
%\input{app_RKHS}
}
{}
%\input{app_separability}
\end{document}
