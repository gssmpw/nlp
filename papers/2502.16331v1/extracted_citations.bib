@article{Parhi2020BanachSR,
  title = {Banach Space Representer Theorems for Neural Networks and Ridge Splines},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {43},
  pages = {1--40},
  url = {https://jmlr.org/papers/v22/20-583.html},
}

@article{Parhi2021WhatKO,
  title = {What Kinds of Functions Do Deep Neural Networks Learn? Insights from Variational Spline Theory},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2022},
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {4},
  number = {2},
  pages = {464--489},
  doi = {10.1137/21M1418642},
}

@article{Parhi2023FunctionSpaceOO,
  title = {Function-Space Optimality of Neural Architectures with Multivariate Nonlinearities},
  author = {Parhi, Rahul and Unser, Michael},
  year = {2025},
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {7},
  number = {1},
  pages = {110--135},
  doi = {10.1137/23M1620971},
}

@inproceedings{Steinwart2009OptimalRF,
  title={Optimal Rates for Regularized Least Squares Regression},
  author={Ingo Steinwart and Don R. Hush and Clint Scovel},
  booktitle={Annual Conference Computational Learning Theory},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:7741716}
}

@article{allen2019can,
  title={What can resnet learn efficiently, going beyond kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{allen2023backward,
  title={Backward feature correction: How deep learning performs deep (hierarchical) learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={4598--4598},
  year={2023},
  organization={PMLR}
}

@article{arora2019harnessing,
  title={Harnessing the power of infinitely wide deep nets on small-data tasks},
  author={Arora, Sanjeev and Du, Simon S and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong and Yu, Dingli},
  journal={arXiv preprint arXiv:1910.01663},
  year={2019}
}

@article{bach_curse,
author = {Bach, Francis},
title = {Breaking the curse of dimensionality with convex neural networks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the nonconvex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomialtime algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {629â€“681},
numpages = {53},
keywords = {non-parametric estimation, neural networks, convex relaxation, convex optimization}
}

@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{garriga2018deep,
  title={Deep convolutional networks as shallow gaussian processes},
  author={Garriga-Alonso, Adri{\`a} and Rasmussen, Carl Edward and Aitchison, Laurence},
  journal={arXiv preprint arXiv:1808.05587},
  year={2018}
}

@article{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{ghorbani2021linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  year={2021}
}

@misc{kanagawa2018gaussianprocesseskernelmethods,
      title={Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences}, 
      author={Motonobu Kanagawa and Philipp Hennig and Dino Sejdinovic and Bharath K Sriperumbudur},
      year={2018},
      eprint={1807.02582},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1807.02582}, 
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{li2019enhanced,
  title={Enhanced convolutional neural tangent kernels},
  author={Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1911.00809},
  year={2019}
}

@misc{mao2024approximationratesshallowreluk,
      title={Approximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev Spaces via the Radon Transform}, 
      author={Tong Mao and Jonathan W. Siegel and Jinchao Xu},
      year={2024},
      eprint={2408.10996},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2408.10996}, 
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@article{mei_rate,
author = {Mei, Song and Bai, Yu and Montanari, Andrea},
year = {2016},
month = {07},
pages = {},
title = {The Landscape of Empirical Risk for Non-convex Losses},
volume = {46},
journal = {Annals of Statistics},
doi = {10.1214/17-AOS1637}
}

@article{near_mini,
  title = {Near-Minimax Optimal Estimation With Shallow {ReLU} Neural Networks},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2023},
  journal = {IEEE Transactions on Information Theory},
  volume = {69},
  number = {2},
  pages = {1125--1140},
  doi = {10.1109/TIT.2022.3208653},
}

@article{novak2018bayesian,
  title={Bayesian deep convolutional networks with many channels are gaussian processes},
  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.05148},
  year={2018}
}

@article{rotskoff2022trainability,
  title={Trainability and accuracy of artificial neural networks: An interacting particle system approach},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={9},
  pages={1889--1935},
  year={2022},
  publisher={Wiley Online Library}
}

@inproceedings{shankar2020neural,
  title={Neural kernels without tangents},
  author={Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and Fridovich-Keil, Sara and Ragan-Kelley, Jonathan and Schmidt, Ludwig and Recht, Benjamin},
  booktitle={International conference on machine learning},
  pages={8614--8623},
  year={2020},
  organization={PMLR}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A law of large numbers},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={SIAM Journal on Applied Mathematics},
  volume={80},
  number={2},
  pages={725--752},
  year={2020},
  publisher={SIAM}
}

@book{steinwart_svm,
author = {Steinwart, Ingo and Christmann, Andreas},
title = {Support Vector Machines},
year = {2008},
isbn = {0387772413},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology.}
}

@article{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

