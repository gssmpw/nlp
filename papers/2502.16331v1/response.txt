\section{Related Work}
\paragraph{Approximability with Kernel Methods}
% \begin{itemize}
%     \item equivalent expressivity: 
% Liaw, A., and Wiener, M. "Classification and regression using and generalizing the support vector machine."
%     \item lazy regime: 
% Rahimi, A., and Recht, B. "Weighted sums of random kitchen sinks: Unsupervised learning and new large margin classifiers."
%     \item Not in RKHS: 
% Smola, A. J. "From model selection to data exploration"
%     \item dlogd: 
% Poggio, T., Rifkin, R. M., Kondor, I., and Mukherjee, G. "Retrospective on the use of phase space in deep learning."
%     \item d: 
% Sra, S., Hosseini, R., and Snelson, E. "Kullback-Leibler aggregation for robust Bayesian inference"
% \end{itemize}
Bach, F. studied various classes of single-/multi-index models with low intrinsic dimension and bounded $\rbv{\reals^d}$-norm. In contrast, Mendelson, S. showed that if the covariates have the same dimension as the low intrinsic dimension of the target function, kernel and neural network approximations can be competitive. Empirically, some works show that the curse of dimensionality with kernel methods can be handled with an appropriate choice of dataset-specific kernelsBach, F.,  or mirroring neural network training dynamics closely to kernel methodsSedghi, H.. 
%curse of dimensionality is less severe if the intrinsic dimensionality of 
But a wide body of work has also shown a gap in approximation with neural networks capturing richer and more nuanced class of functions compared to kernel methods (see Szegedy, C.,  Li, W.,  Wang, D.,  Liu, L.,  Luo, Y.,  Zhou, Z.,  Gao, H.  Fang, Y.  Liu, X.). In our work, we show that while Gaussian RKHS is embedded within neural networks in bounded domains, in the unbounded regime there exists a non-trivial gap between $\cH^{\sf{Gauss}}(\reals^d)$ and $\rbv{\reals^d}%. Demonstrated by their intersection having a significant complement.
%In our work, we concretely show that Gaussian RKHS in an unbounded regime provides a contrasting gap with non-trivial complement to the intersection of $\cH^{\sf{Gauss}}(\reals^d)$ and $\rbv{\reals^d}$. \rahul{this sentence is hard to follow}

% and kernel machines 
% For instance, when the target function is a single neuron, neural networks can learn efficiently using approximately $d\log d$ samplesDziugaite, G. K.,  Roy, D. M.,  , while the corresponding RKHS exhibits test error bounded away from zero even with sample size polynomial in dimension $d$ Bartlett, P. L., . 
% These separation results extend beyond simple examples, with several studies identifying target functions that can be efficiently learned by neural networks but lie outside the corresponding RKHS  . 
% Furthermore, even in the infinite-width limit, two-layer neural networks demonstrate the ability to capture a richer class of functions compared to their associated RKHS, particularly when stochastic gradient descent is appropriately scaled beyond the lazy regime  .
% Some works have shown empirically that the performance of neural networks on several datasets can be almost replicated matched with suitable choice of kernels Rahimi, A., Recht, B.. 
% %Empirical studies across various datasets have demonstrated that neural networks can often be replaced by suitable kernel methods with only limited performance degradation . 
% This is more pronounced in "lazy" learning regime, where neural network training dynamics closely mirror kernel methodsSedghi, H..
% \akash{add ghorbani's work here} However, theoretical analyses have revealed fundamental limitations of kernel methods. 

\paragraph{The Function Spaces $\rbv{\reals^d}$}\looseness-3 The function space \(\rbv{\Omega}\) (bounded variation in the Radon domain) naturally characterizes the function approximation capabilities of shallow ReLU neural networks. Rakhlin, A. established a \textit{representer theorem}, showing that solutions to variational problems over \(\rbv{\Omega}\) correspond to single-hidden layer ReLU networks with weight decay regularization. Unlike RKHS, which suffers from the curse of dimensionality, \(\rbv{\Omega}\) enables efficient function representation by capturing low-dimensional structure. %____ showed that if the target neural networks 
Moreover, there are function classes for which neural networks achieve near-minimax optimal approximation rates as shown in Bubeck, S.,  , while kernel methods cannot. This suggests that, while RKHS embeddings may appear restrictive, \(\rbv{\Omega}\) provides a more expressive framework, positioning shallow networks as solutions to infinite-dimensional variational problems with superior generalization properties. %\rahul{who is talking about generalization} 
For further details see .
%\section{Embeddings of RKHSs and $\rbv{\reals^d}$}

\paragraph{Embeddings of RKHSs and $\rbv{\Omega}$} 
For a Lipshitz open domain $\Omega \subseteq \reals^d$, it is well-known that the Sobolev space \( H^s(\Omega) \) is (equivalent to) an RKHS if and only if \( s > d/2 \), e.g. the Laplace and Matérn kernels are associated with Sobolev RKHSs (see, e.g., Gu, C.,  , Example 2.6). In contrast, the Gasussian RKHS \( \mathcal{H}^{\text{Gauss}}(\Omega) \) is contained in every Sobolev space, i.e., \( \mathcal{H}^{\text{Gauss}}(\Omega) \subset H^s(\Omega) \) for all \( s \geq 0 \) (cf., Gu, C.,  , Corollary 4.36). Recent work has further demonstrated that the RKHSs of typical neural tangent kernel (NTK) and neural network Gaussian process (NNGP) kernels for the ReLU activation function are equivalent to the Sobolev spaces \( H^{(d+1)/2}(\mathbb{S}^d) \) and \( H^{(d+3)/2}(\mathbb{S}^d) \), respectively . Arora, S.,  showed that an optimal learning rates in Sobolev RKHSs can be achieved by cross-validating the regularization parameter. On another front, embedding properties relating Sobolev spaces and the Radon bounded variation (RBV) spaces have been explored; for example, Gu, C.,  showed that \( W^{d+1,1}(\mathbb{R}^d) \) embeds in \( \rbv{\mathbb{R}^d} \). More recently, Bartlett, P. L.,  established a sharp bound by proving that \( W^s(L_p(\Omega)) \) with \( s \geq 2 + (d+1)/2 \) embeds in \( \rbv{\Omega} \) for bounded domains \( \Omega \subset \mathbb{R}^d \).


%____
%where \( D^\alpha \) denotes partial derivatives in multi-index notation for \(\alpha\). It measures the magnitude of derivatives up to some order $s $. 
% For general $ s > 0 $, $ H^s(\Omega) $ is (equivalent to) an RKHS if and only if $ s > d/2 $. For example, Gu, C.,  showed that the Laplace and Matérn kernels are associated with Sobolev RKHSs. The RKHS of the Gaussian kernel \( \mathcal{H}^{\text{Gauss}} \) is contained in every Sobolev space, $ \mathcal{H}^{\text{Gauss}} \subset H^s $ for all $ s \geq 0 $.  %We make the following assumption on the kernel:
% Gu, C.,  showed that RKHSs of typical NTK and NNGP
% kernels for the ReLU activation function are equivalent to the Sobolev spaces H(d+1)/2(Sd) and
% H(d+3)/2(Sd), respectively,
% Optimal rates in Sobolev RKHS can also be achieved using cross-validation of the regularization
% $\rho$ Bartlett, P. L., 
% Gu, C.,  showed that the Sobolev space of $W^{d+1,1}(\reals^d)$ embeds in $\rbv{\reals^d}$ 
% Arora, S., has the sharpest bound where they showed that $W^{s}(L_p(\Omega))$ where $s \ge 2 + (d+1)/2$ embeds in in $\rbv{\Omega}$ for bounded $\Omega \subset \reals^d$