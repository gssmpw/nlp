%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{float}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\usepackage{changepage}
% \usepackage{caption}
\usepackage{wrapfig}
\begin{document}

\twocolumn[
\icmltitle{Non-Linear Flow Matching for Full-Atom Peptide Design}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dengdeng Huang}{sjtu}
% \icmlauthor{Firstname2 Lastname2}{yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Shikui Tu}{sjtu}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sjtu}{Shanghai Jiaotong University, Shanghai, China}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
% In this paper, \textit{\textbf{NLFlow}}, a novel non-linear multi-manifold approach based on non-linear flow matching,  is proposed to address this challenge.

\begin{abstract}
Peptide design plays a pivotal role in therapeutic applications, yet existing AI-assisted methods often struggle to generate stable peptides with high affinity due to their inability to accurately simulate the dynamic docking process. 
To address this challenge, we propose \textit{\textbf{NLFlow}}, a novel multi-manifold approach based on non-linear flow matching. 
Specifically, we design a polynomial-based conditional vector field to accelerate the convergence of the peptide's position towards the target pocket, effectively capturing the temporal inconsistencies across position, rotation, torsion, and amino acid type manifolds.
This enables the model to better align with the true conformational changes observed in biological docking processes.
Additionally, we incorporate interaction-related information, such as polarity, to enhance the understanding of peptide-protein binding. 
Extensive experiments demonstrate that NLFlow outperforms existing methods in generating peptides with superior stability, affinity, and diversity, offering a fast and efficient solution for peptide design and advancing the peptide-based therapeutic development.
\end{abstract}

\section{Introduction}
\label{intro}
% Therapeutic peptide introduction
% Therapeutic peptides are short, single-chain proteins composed of a series of amino acids. 
% Full-Atom Peptide Design enables the creation of peptides with precise and stable interactions with target receptor proteins, showing great potential in advancing therapeutic peptide development and offering a robust solution for the treatment of protein-related diseases.
Therapeutic peptides are short, single-chain proteins composed of amino acids, and their design is essential for achieving precise and stable docking with target receptor proteins~\cite{craik2013future, fosgerau2015peptide}. Full-atom peptide design enables the creation of such peptides, offering a robust solution for advancing therapeutic development and treating protein-related diseases~\cite{muttenthaler2021trends, petsalaki2008peptide, lee2019comprehensive}.
To tackle the vast and complex chemical space, recent generative models integrate protein sequence and structure data to extract rich feature representations, facilitating the exploration of sequence-structure relationships~\cite{charoenkwan2021bert4bitter, lin2023evolutionary}.
% Particularly, diffusion and flow matching models simulate peptide movement through time-dependent random pathways or vector fields~\cite{CIEMNY20181530, dauparas2022robust, watson2023novo}, where they model modalities such as displacement, rotation, and other dynamics, imposing constraints on the peptide's conformational degrees of freedom, leading to efficient exploration of conformational space and accurate simulation of docking dynamics. 
Particularly, diffusion and flow matching models simulate peptide movement through time-dependent random pathways or vector fields~\cite{CIEMNY20181530, dauparas2022robust, watson2023novo}. These models account for modalities such as displacement, rotation, and other dynamics. By imposing constraints on the peptide's conformational degrees of freedom, they enable efficient exploration of conformational space and provide an accurate simulation of docking dynamics.
Thus, they overcome the limitations of traditional experimental methods, such as high costs and long timelines~\cite{henninot2018current, bhardwaj2016accurate}.
% Deep learning, as a powerful computational approach, has made significant progress in protein design and docking tasks~\cite{CIEMNY20181530, dauparas2022robust}.
% Generative models, trained on large-scale datasets, can automatically capture the complex interactions between amino acids, thereby enhancing its understanding of biological structures. 
% In particular, diffusion probabilistic models have demonstrated great potential in protein modeling due to their flexibility~\cite{trippe2022diffusion, anand2022protein}. 
% DiffDock~\cite{corso2022diffdock} frames molecular docking as a diffusion generative model based on non-Euclidean manifolds and achieves a high top-1 success rate, significantly outperforming traditional docking methods. 
% RFDiffusion~\cite{watson2023novo} fine-tunes structural prediction networks, showcasing exceptional performance in protein design tasks. 
% PepFlow~\cite{li2024full} is a cutting-edge peptide design method that learns joint distributions through multi-modal flow matching, using vector fields to simulate torsion angles and internal geometries.

However, existing diffusion models neglect temporal dependencies across different modalities in the docking process, leading to generated peptides that cannot stably bind to target proteins~\cite{bennett2023improving}. 
Although these methods model multi modalities to simulate peptide movement, 
% including peptide displacement, rotation, and torsion, 
they treat the evolution of all these modalities as fully synchronized in time~\cite{corso2022diffdock, li2024full, lin2024ppflow}.
In reality, peptides must first move towards the target pocket before specific residue interactions can trigger subsequent conformational changes, such as backbone rotation and side-chain torsion~\cite{london2013peptide}. 
This temporal inconsistency during inference results in inaccurate peptide movement predictions and impairs the biologically realistic simulation of peptide dynamics.

\begin{figure*}[ht]
% \vspace{-3mm}
    \begin{center}
    \includegraphics[width=0.9\linewidth]{dockprocess.pdf}
    \vspace{-1mm}
    \caption{\textbf{Modal Temporal Sequence in Docking}: Position transformations occur prior to rotational and torsional changes due to the requirement that the peptide first moves into proximity with the target pocket, allowing for residue interactions with the surrounding environment, which then drive backcone rotations and side-chain torsion adjustments.}
    \label{icml-historical}
    \end{center}
\vspace{-2mm}
\end{figure*}

% To address the above challenges, we propose \textit{\textbf{NLFlow}}, a novel generative model based on Conditional Flow Matching (CFM)~\cite{lipman2022flow}. NLFlow models four manifolds: coordinates, backbone rotation, side-chain torsion, and amino acid types.
% It uses neural networks with attention to fit gradient fields from a random distribution to the peptide sequence-structure distribution, aligning the sequences and structures with the data distribution through iterative ordinary differential equations.
% Specifically, we introduce a new gradient field assumption that differs from existing methods. For the coordinate manifold, we propose the assumption that the conditional vector field is derived by taking the time derivative of a non-linear flow $\mathbf{x}_t = \mathbf{x}_0 + (1 - (1 - t)^k) \cdot (\mathbf{x}_1 - \mathbf{x}_0)$
% which leads to the vector field $\mathbf{u}_t = k(1 - t) \cdot (\mathbf{x}_1 - \mathbf{x}_0)$. 
% Intuitively, the peptide's coordinates, while maintaining the straight-line assumption, reach closer to the target at earlier time steps compared to the other manifolds, thus fulfilling our expectation of simulating the sequence in real docking, where the peptide first approaches and then adjusts its angles. 
% our method
% To address the above challenges, we propose \textit{\textbf{NLFlow}}, a novel multi-manifold based flow matching that resolves the issue of temporal inconsistency across different manifolds, which contradicts biological principles. 
% To realize this, the core idea introduces a time-varying gradient vector field approach by modeling the position manifold with a non-linear flow.
% Specifically, NLFlow employs a polynomial-based interpolation to ensure rapid initial movement towards the target pocket position, addressing the temporal inconsistency by decoupling the evolution of the position manifold from the rotation and torsion manifolds.
% Specifically, NLFlow employs a polynomial-based interpolation to model a time-varying gradient vector field, ensuring that the gradient magnitude of the position manifold decreases over time, accelerating the initial phase of evolution while decoupling the evolution of the position manifold from the rotation and torsion manifolds, thereby resolving the temporal inconsistency issue.
To resolve the temporal inconsistency across different manifolds, we propose \textit{\textbf{NLFlow}}, a novel \textbf{N}on-\textbf{L}inear multi-manifold based \textbf{Flow} matching method.
% It accelerates the peptide’s coordinate convergence towards the target in a non-uniform manner, ahead of adjustments in rotation and torsion, closely mirroring the actual docking process, where the peptide first moves towards the protein pocket and then adjusts its posture.
It accelerates the peptide’s coordinate convergence towards the target in a non-uniform manner, prioritizing positional alignment over rotational and torsional adjustments. 
This closely mirrors the actual docking process, where the peptide first moves towards the protein pocket and then fine-tunes its posture.
To realize this, the core idea is to leverage polynomial interpolation to define a time-varying gradient vector, decoupling the evolution of the position manifold from the other manifolds.
Specifically, the gradient magnitude of the position manifold decreases over time, accelerating the initial phase of convergence, thereby reflecting the temporal inconsistency across different manifolds.
Additionally, to prevent the model from focusing solely on conformational alignment while neglecting docking interactions, interaction-related information such as polarity, charge, and hydrophilicity is incorporated, ensuring stronger binding affinity to the target protein.

The NLFlow framework consists of two key components: 
(i) In the pre-training phase, a neural network is used to fit the gradient vector field of four manifolds: peptide position, rotation, torsion and amino acid types, learning distribution evolution of docking process;
(ii) In the sampling phase, based on ordinary differential equations (ODEs)~\cite{teschl2021ordinary}, the gradient vector is predicted with the pre-trained model at each time step, and the peptide conformation is updated gradually over the integration of time.
% (ii) Using an ODE-based sampling approach, the pre-trained model iteratively calculates gradients at each time step and updates the peptide conformation by integrating the velocity field with respect to the time step.
% In addition, we encode amino acid features such as polarity, hydrophilicity, and charge to better align the designed conformation with the pocket’s interaction environment, optimizing both stability and affinity of the docking results.

% testtest

NLFlow provides two key advantages: 
(i) Introduces a non-linear flow algorithm to address temporal inconsistency between different modalities. This leads to a more accurate simulation of peptide docking, generating more than \textbf{twice as many stable peptide-protein complexes} compared to the baseline;
(ii) The rapid initial position convergence of the non-linear flow reduces the ineffective time spent on conformational exploration, allowing the model to gain a deeper understanding of the relationship between fine conformational changes and interaction forces, which leads to \textbf{7.13\% more peptides with lower binding energies}.

In summary, our key contributions include: 
\begin{itemize}
    \item We first identify the \textbf{temporal inconsistencies} across modalities as a critical issue in multi-modal peptide design models, which violates biological principles and hinders the generation of high-affinity peptides.
    \item We design a novel non-linear flow matching approach \textbf{NLFlow} that decouples the evolution of different manifolds, providing a more accurate simulation of dynamic conformational changes.
    \item Extensive experiments demonstrate NLFlow’s superior performance in both co-design and re-docking, significantly improving generated peptides' \textbf{stability} and \textbf{affinity}, while providing an efficient method for advancing peptide-based therapeutic development.
\end{itemize}
% (i) We first identify the \textbf{temporal inconsistencies} across modalities as a critical issue in multi-modal peptide design models, which violates biological principles and hinders the generation of high-affinity peptides.
% (ii) We design a novel non-linear flow matching approach \textbf{NLFlow} that decouples the evolution of different manifolds, providing a more accurate simulation of dynamic conformational changes.
%     % by maintaining the correct temporal information.
% (iii) Extensive experiments demonstrate NLFlow’s superior performance in both co-design and re-docking, significantly improving generated peptides' \textbf{stability} and \textbf{affinity}, while providing an efficient method for advancing peptide-based therapeutic development.


\section{Related Work}
\subsection{Peptide generation}
% \paragraph{Peptide generation}
% \textbf{Peptide generation}
Peptide generation is a complex task in drug discovery, aiming to design peptides with specific biological functions, such as binding to target proteins or modulating enzymatic activity. 
Generative models have significantly advanced peptide design by exploring large sequence and structural spaces with greater flexibility. For example, RFDiffusion~\cite{watson2023novo}, designed for protein design, inspired the application of similar diffusion techniques for peptide generation. 
AMP-diffusion~\cite{chen2024amp} utilizes the capabilities of protein large language model ESM-2~\cite{beal2015esm} to regenerate functional antimicrobial peptides (AMPs). 
MMCD~\cite{wang2024multi} employs multi-modal contrastive learning in diffusion, exploiting the integration of both sequence and structural information to produce peptides with high functional relevance.

Peptide design is a subtask of peptide generation, involving several key components. 
% Backbone design methods~\cite{yim2023fast, bose2023se}, such as PepFlow~\cite{li2024full} and PPFlow~\cite{lin2024ppflow}, use flow matching to simulate dynamic conformational changes and optimize peptide properties.
One important aspect is backbone design, where methods like PepFlow~\cite{li2024full} and PPFlow~\cite{lin2024ppflow} use flow matching to simulate dynamic conformational changes and optimize peptide properties. 
% For side-chain packing, methods like RED-PPI~\cite{luo2023rotamer} focus on protein-protein complexes, while DiffPack~\cite{zhang2024diffpack} targets peptide-protein interactions.
Another aspect is side-chain packing, with methods such as RED-PPI~\cite{luo2023rotamer} focusing on protein-protein complexes and DiffPack~\cite{zhang2024diffpack} targeting peptide-protein interactions.

\subsection{Protein–ligand docking}
% \paragraph{Protein–ligand docking}
Protein–ligand docking aims to predict the binding pose and affinity of a ligand to a target protein. 
Traditional docking methods often rely on rigid-body simulations and predefined scoring functions, which can struggle to handle flexible ligands and complex interactions~\cite{kramer1999evaluation, totrov1997flexible}.
Recent years, deep learning (DL) approaches have significantly improved docking accuracy~\cite{yang2022protein}. Models like AtomNet~\cite{stafford2022atomnet} and OnionNet~\cite{wang2021onionnet} use convolutional neural networks (CNNs) to capture complex molecular features, significantly enhancing binding affinity predictions.
Further developments have incorporated graph-based models, such as GraphSite~\cite{shi2022graphsite} and DGraphDTA~\cite{yang2022mgraphdta}, which represent protein-ligand interactions as graphs to better model flexible docking. 
Additionally, advances in protein structure characterization, notably via AlphaFold~\cite{evans2021protein}, have enhanced understanding of protein flexibility, fueling the development of structure-based docking prediction methods~\cite{wong2022benchmarking, johansson2022improving}.
However, challenges still remain in fully capturing the dynamic and flexible nature of ligands, highlighting the need for further improvements in docking models.


\section{Method}
\subsection{Preliminary}
\paragraph{Conditional Flow Matching.}
Conditional Flow Matching (CFM)~\cite{lipman2022flow} models the transformation between a prior distribution \( p = p_0 \) and a target data distribution \( q = p_1 \) on a manifold \( \mathcal{M} \) using ODEs. The transformation is described as:
\begin{equation}
    \frac{d}{dt} \varphi_t(x) = u_t(x),
\end{equation}
where \( \varphi_t(x) \) represents the data distribution at time \( t \), and \( u_t(x) \) is the gradient vector field driving the flow. 
Since the true gradient flow \( u_t(x) \) is unknown, it is assumed based on a hypothesis form and a known target distribution \( x_1 \).
Usually a neural network is used to approximate this flow by learning a vector field \( v_t(x) \). 
The loss is defined to minimize the difference between \( v_t(x) \) and \( u_t(x|x_1) \):
\begin{equation}
    L_{\text{CFM}}(\theta) = \mathbb{E}_{t, p_t(x|x_1)} \| v_t(x) - u_t(x|x_1) \|^2,
\end{equation}

In most current methods, the distribution \( \varphi_t(x) \) is typically assumed to follow a simple linear interpolation between the initial distribution \( x_0 \) and target distribution \( x_1 \): \( \varphi_t(x) = t x_1 + (1 - t) x_0 \), where \( t \in [0, 1] \).
The corresponding gradient vector field is \( u_t(x) = \frac{d}{dt} \varphi_t(x) = x_1 - x_0 \).
This form has been proven to correspond to the Optimal Transport (OT) solution, which provides an efficient and fast means of transforming distributions with a fixed direction and magnitude.


\paragraph{Problem Statement.}
% \begin{wrapfigure}{r}{5.5cm}%靠文字内容的右侧
% \centering
%     \includegraphics[width=0.99\linewidth]{bb.png}
%     % \caption{Caption}
%     \label{fig:enter-label}
% \end{wrapfigure}
Peptide is a short sequence of amino acids, denoted by \( a \), representing its type. 
Each amino acid is characterized by its backbone and side chain.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{bb.pdf}
    \vspace{-2mm}
    \caption{Rigid peptide backbone and flexible side chains.}
    \vspace{-2mm}
    \label{fig:pep}
\end{figure}

The backbone consists of four heavy atoms \( N \), \( C_{\alpha} \), \( C \), and \( O \), with the position of \( C_{\alpha} \) serving as the reference, denoted as \( \mathbf{X}_i \) at the origin.
Peptide's orientation is modeled in two parts: the overall rigid rotation, represented by the rotation matrix \( \mathbf{R} \in \operatorname{SO}(3) \), and the side-chain torsion, captures four torsion angles \(\boldsymbol{\tau} = (\chi_1, \chi_2, \chi_3, \chi_4)\) of the rotatable bonds.

In the docking process, the peptide acts as the ligand, represented as \( Lig = \{ ( \mathbf{X}_i, \mathbf{R}_i, \boldsymbol{\tau}_i, \boldsymbol{a}_i) \}_{i=1}^n \), where $n$ is the length of peptide, while the protein serves as the receptor, denoted as \( Rec \). 
The task of designing the peptide \( Lig \) based on a spesific \( Rec \) can be formulated as learning the conditional distribution \( p(Lig | Rec) \). 
Expanding the problem to the four modalities we have modeled, the problem can be empirically decomposed into the product of probabilities of the four components: 
\begin{align}
    p(Lig | Rec) \propto &p(\{\mathbf{X}_i\}_{i=1}^n | Rec) \cdot p(\{\mathbf{R}_i\}_{i=1}^n | Rec) \nonumber\\
    \cdot &p(\{\boldsymbol{\tau}_i\}_{i=1}^n | Rec) \cdot p(\{\boldsymbol{a}_i\}_{i=1}^n | Rec).
\end{align}


\begin{figure*}
    \centering
    \vspace{-1mm}
    \includegraphics[width=0.98\linewidth]{overview.png}
    \vspace{-2mm}
    \caption{Overview of the NLFlow framework}
    \label{fig:overview}
    \vspace{-2mm}
\end{figure*}

\subsection{Non-Linear Flow for Position}
\label{non-linear flow}
Position manifold refers to the coordinates of the C-alpha atoms of the peptide backbone in Euclidean space. 
We initialize the system using a standard Gaussian distribution \( \mathbf{X}^0 \sim \mathcal{N}(0, I) \) as the random initialization, with \( \mathbf{X}^1 \) representing the target distribution. 
The task is to model the trajectory from \(  \mathbf{X}^0 \) to \( \mathbf{X}^1 \). 

Despite being efficient and fast, the OT assumption applied across all modalities leads to synchronized evolution in time.
To resolve this temporal inconsistency issue, we define a polynomial interpolation for position mainfold to explicitly decouples its evolution from other mainfolds. 
The interpolation is defined as:
\begin{equation}
    \mathbf{X}_i^t = \mathbf{X}_i^0 + (1 - (1 - t)^k) \cdot \mathbf{X}_i^1, \quad t \in [0, 1],
\end{equation}
where $k$ is a hyperparameters.

Taking the derivative of this equation with respect to time $t$ gives the corresponding gradient vector field: 
\begin{equation}
    u_t^{\mathrm{pos}}(X_i^t|X_i^0, X_i^1) = \frac{d\mathbf{X}_i^t}{dt} = k(1 - t)^{k-1} (\mathbf{X}_i^1 - \mathbf{X}_i^0),
\end{equation}
From this formulation, we can see that the gradient vector field $u_t^{\mathrm{pos}}$ is time-varying and decreases over time, while the direction remains consistent, always aligned with \( \mathbf{X}_i^1 - \mathbf{X}_i^0 \). 
In a broader view, the position manifold evolves quickly towards \( \mathbf{X}^1 \) near the initial state, reflecting the initial stage of docking where the peptide rapidly moves towards the pocket. 
As the docking progresses, the evolution slows down, mirroring the peptide's fine-tuning of its conformation near the pocket, including rotation and torsion adjustments.
The polynomial flow resolves the temporal consistency limitation of synchronized OT assumptions, enabling both efficient exploration of conformational space and precise optimization of flexible docking.
% \vspace{-5mm}
\paragraph{Non-Linear Flow and Diffusion Formulation.}
The non-linear flow model is not introduced arbitrarily but is inspired by both flow matching and diffusion processes. 
In diffusion models, the distribution evolves over time via stochastic differential equations (SDEs), with the transformation given by \( \mu_t(x) = \alpha_{1-t} x_1 \) and the noise term \( \sigma_t(x) = \sqrt{1 - \alpha_{1-t}^2} \), where \( \mu_t(x) \) represents the mean of the data distribution at time \( t \) and \( x_1 \) is the target distribution.

It has also been shown that flow matching, as defined by the vector field, can be derived into the form of \( \mu_t(x) \). 
The optimal transport (OT) assumption corresponds to a linear form: \( \mu_t(x) = t x_1 \), representing uniform evolution from the initial to the target distribution.

Building on this, our proposed non-linear flow corresponds to a time-varying interpolation: \( \mu_t(x) = (1 - (1 - t)^k) x_1 \), which reduces to the linear flow when \( k = 1 \). 
For \( k > 1 \), the non-linear flow provides increased flexibility to simulate the varying convergence speed of the position manifold in docking tasks, accurately capturing the complex dynamics of peptide conformation changes.

\paragraph{Loss Function for Position Flow.}
We use a trainable neural network \( v^\mathrm{pos} \) to approximate the time-varying gradient \( u_t^\mathrm{pos} \). 
The loss function is designed to minimize the squared error between the predicted and target gradient vectors over all timesteps, ensuring that the model learns the correct temporal dynamics. 
It can be expressed as:
\begin{equation}
\small
    \mathcal{L}_i^{\mathrm{pos}} = \mathbb{E}_{\substack{p(X_i^0), p(X_i^1)\\ p(X_i^t | X_i^0, X_i^1)}} \left\| v_t^\mathrm{pos}(X_i^t) - k(1 - t)^{k-1}(X_i^1 - X_i^0) \right\|^2.
\end{equation}

\paragraph{Non-Uniform Time Sampling Strategy.}  
% % We employ a non-uniform distribution for the time variable by defining \( t = z^k \), where \( z \sim U(0,1) \), to addresses the inherent issue in our non-linear interpolation method, where under a uniform time distribution, the resulting interpolated positions \( \mathbf{x}_t \) are more likely to be closer to \( \mathbf{x}_1 \), leading to fewer samples near \( \mathbf{x}_0 \). 
% In our non-linear setup, an inherent issue arises where \( \mathbf{x}_t \) tends to be sampled closer to \( \mathbf{x}_1 \), resulting in fewer learning opportunities near \( \mathbf{x}_0 \). 
% To address this, we employ a non-uniform time sampling strategy, defining \( t = z^k \), where \( z \sim U(0,1) \).
% % By using \( t = z^k \), we place more focus on the initial stage of docking while ensuring that the interpolated positions \( \mathbf{x}_t \) are evenly distributed between \( \mathbf{x}_0 \) and \( \mathbf{x}_1 \), ensuring better coverage of the evolution near \( \mathbf{x}_0 \).
% Through this strategy, we ensure evenly distributed interpolated positions \( \mathbf{x}_t \), with more attention given to the relatively information-sparse initial phase. 
To address the bias of standard uniform sampling—where interpolated positions \( \mathbf{X}_t \) cluster near \( \mathbf{X}^1 \) under polynomial-based interpolation—we redefine the time variable as \( t = z^k \) (\( z \sim \mathcal{U}(0,1) \)). 
This non-uniform sampling adjusts the temporal density, prioritizing the initial phase (\( t \to 0 \)) where rapid positional convergence occurs. 
By reshaping the probability distribution of \( t \), the strategy ensures balanced coverage of the entire trajectory, preventing under-sampling of critical early dynamics. 
Simultaneously, it allocates more training samples to the information-sparse initial phase, enabling the model to robustly learn fast positional alignment governed by large initial gradients, while stabilizing gradient updates across all stages. 
% This harmonizes sampling with the peptide's hierarchical docking process: rapid approach first, refinement later.

\subsection{Linear Flow for Orientation and Amino Acid Type}
In this section, we follow recent advances in flow-based methods~\cite{lin2024ppflow, li2024full}, modeling the flows of rotation, torsion, and amino acid types using the optimal transport assumption, which enables efficient transitions between states.
% As the peptide approaches the pocket, these manifolds adjust according to the pocket environment, which ensures temporal consistency and allows us to assume synchronous convergence.
As the peptide approaches the pocket, these manifolds adjust according to the pocket environment, ensuring temporal consistency. 
This adjustment allows us to treat these three modalities as time-synchronized linear flows. 
Together with the non-linear flow model discussed in the previous section, our approach decouples the evolution of the position manifold from the other manifolds, effectively capturing cross-modal temporal inconsistencies.

\paragraph{Rotation Matrices.}
A rotation matrix, \( \mathbf{R}_i \), is an element of the special orthogonal group \( SO(3) \), which describes rigid body rotations. As a Lie group, \( SO(3) \) enables its elements to be locally represented by its tangent space. It has been proven that the exponential map facilitates smooth interpolation in Lie groups. Therefore, we use the rotation matrices under the exponential map for linear interpolation, which can be expressed as:
\begin{equation}
    R_i^t = \exp_{R_0^i} \left( t \log_{R_0^i}  \left( R_1^i \right) \right),
\end{equation}
\begin{equation}
    u_t^\mathrm{rot}(R_i^t|R_i^0,R_i^1)=\frac{\log_{R_i^t}R_i^1}{1-t},
\end{equation}
The loss function \( \mathcal{L}_i^{\mathrm{rot}} \) is defined as:
\begin{equation}
    \mathcal{L}_i^{\mathrm{rot}} = \mathbb{E}_{\substack{p(R_i^0), p(R_i^1) \\ p(R_i^t | R_i^0, R_i^1)}} \left\| v_t^\mathrm{rot}(R_i^t) - \frac{\log_{R_i^t} R_i^1}{1-t} \right\|_{\mathrm{SO}(3)}^2.
\end{equation}


\paragraph{Torsion Angles.}  
The torsion angle, \( \boldsymbol{\tau}_i \in [0, 2\pi)\), refers to the angle of rotation between two planes formed by four consecutive atoms in the side-chain of a peptide. 
Unlike the rotation of a rigid body, torsion alters the internal structure without changing the overall properties. 
The torsion angle has a periodicity of \( 2\pi \), and its space is topologically a torus, with the angle smoothly wrapping around itself as it changes continuously. 
Therefore, we define \( \boldsymbol{\tau}_i^t \):
\begin{equation}
    \boldsymbol{\tau}_i^t = (1-t) \boldsymbol{\tau}_i^0 + t \boldsymbol{\tau}_i^1 \mod 2\pi,
\end{equation}
\begin{equation}
    u_t^\mathrm{toru}(\boldsymbol{\tau}_i^t|\boldsymbol{\tau}_i^0,\boldsymbol{\tau}_i^1) = \boldsymbol{\tau}_i^1 - \boldsymbol{\tau}_i^0 \mod 2\pi,
\end{equation}
The loss function \( \mathcal{L}_i^{\mathrm{toru}} \) is defined as:
\begin{equation}
    \mathcal{L}_i^{\mathrm{toru}} = \mathbb{E}_{\substack{p(\boldsymbol{\tau}_i^0), p(\boldsymbol{\tau}_i^1) \\ p(\boldsymbol{\tau}_i^t | \boldsymbol{\tau}_i^0, \boldsymbol{\tau}_i^1)}} \left\| v_t^\mathrm{toru}(\boldsymbol{\tau}_i^t) - (\boldsymbol{\tau}_i^1 - \boldsymbol{\tau}_i^0 \right)\|^2.
\end{equation}


\paragraph{Amino Acid Types.}  
% The residue type of peptide, \( a_i \in {1,2...,20}\), is a discrete variable representing the identity of an amino acid in the peptide sequence. To smooth this representation, we treat the residue types as continuous logits in a 20-dimensional space. After applying soft one-hot encoding to the residue types, the logits are linearly interpolated between the initial type \( s_i^0 \) and the target type \( s_i^1 \). 
A peptide is composed of a sequence of amino acids, where the amino acid type at position \( i \), \( a_i \in \{1, 2, ..., 20\} \), has 20 possible distinct values. Since \( a_i \) is a discrete variable, we define the soft label \( s_i \) as continuous logits in a 20-dimensional space to facilitate smooth linear interpolation of this manifold.
The interpolation for soft label \( s_i \) is defined as:
\begin{equation}
    s_i^t = t s_i^1 + (1 - t) s_i^0,
\end{equation}
\begin{equation}
    u_t^\mathrm{type}(s_i^t|s_i^0, s_i^1) = s_i^1 - s_i^0,
\end{equation}
The loss function \( \mathcal{L}_i^{\mathrm{type}} \) is defined as:
\begin{equation}
    \mathcal{L}_i^{\mathrm{type}} = \mathbb{E}_{\substack{p(s_i^0) , p(s_i^1)\\ p(s_i^t | s_i^0, s_i^1)}} \left\| v_t^\mathrm{type}(s_i^t) - (s_i^1 - s_i^0) \right\|^2.
\end{equation}


\subsection{Loss Balancing for Sequence-Structure Co-design}
To balance sequence and structural alignment, we define the spatial loss $\mathcal{L}^{\mathrm{spa}}$ as the weighted sum of the position, rotation and torsion losses:
\begin{equation}
\mathcal{L}^\mathrm{spa} = \sum_{l \in \{\text{pos}, \text{rot}, \text{tor}\}} \alpha_l \cdot \mathcal{L}^l,
\end{equation}
where $\alpha_l$ are the corresponding hyperparameters.

The sequence loss weight, $\alpha_{\mathrm{type}}$, is dynamically adjusted based on the spatial loss:
\begin{equation}
\alpha_{\text{type}} = \min\left( \max\left( \frac{20}{\mathcal{L}^\mathrm{spa}}, 1 \right), \alpha_{\text{type}}^{max} \right),
\end{equation}
where $\alpha_{\mathrm{type}}^{\text{max}}$ is a hyperparameter that controls the upper bound. 
This ensures that when spatial loss is small, indicating structural alignment with the reference, the sequence is encouraged to match it; otherwise, the alignment requirement is relaxed.
The total loss is then computed as:
\begin{equation}
\mathcal{L}^{total} = \sum_{l \in \{\text{pos}, \text{rot}, \text{tor}, \text{type}\}} \alpha_l \cdot \mathcal{L}^l.
\end{equation}

\subsection{Sampling with ODE}
% \paragraph{}
We perform sampling with the pre-trained flow model by formulating peptide generation as an ODE, where the peptide ${Lig}$ evolves according to $\frac{d}{dt} Lig = v_t({Lig})$, with derivatives computed for each of the four modalities. The equation is discretized into $N$ steps, yielding the final peptide $\overline{}{{Lig}_1}$.

Each modality's update depends not only on its own state but also on the states of other modalities. 
This interdependence highlights the importance of accurately capturing the temporal relationships across modalities. 
By iteratively updating these states, our model effectively simulates the dynamic nature of peptide docking, while enabling flexible sampling strategies for different design tasks (fix sequence for re-docking and fix backbone for side-chain packing).
\begin{algorithm}[!htb]
    \caption{Sampling with ODE}
    \begin{algorithmic}
        \STATE {\bfseries Input:} $\text{Enc}(Rec)$
        \STATE {\bfseries Init:} Initial state $Lig_0 = \{(X_i^0, R_i^0, \boldsymbol{\tau}_i^0, a_i^0)\}_{i=1}^{n}$
        \FOR{$t = 1$ {\bfseries to} $N$}
            \STATE Predict gradient vecto $v_t = \text{Dec}(Lig_{\frac{t-1}{N}}, Rec, t)$
            \STATE EulerStep ${Lig_{\frac{t}{N}}} = Lig_{\frac{t-1}{N}} + \Delta t \cdot v$
        \ENDFOR
        \STATE {\bfseries Output:} $\overline{{Lig}_1}$
    \end{algorithmic}
\end{algorithm}
\vspace{-3mm}

\begin{table*}[!h]
\caption{Comparison for Sequence-Structure Co-design: \textbf{Highest Affinity and Stability}.}
\label{co-design}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccr}
\toprule
Method   & Stability $\uparrow$& Affinity $\uparrow$&Novelty $\uparrow$& Diversity&Seq sim\\
\midrule
PepFlow      & 4.16\%  & 13.19\% &50.45\% & 0.461 &  62.94\%\\
PPFlow      & 3.05\%     & 10.68\%      & 85.07\% & \textbf{0.705}    & \textbf{25.40\%} \\
DiffPP      & 5.34\%     & 12.21\%      & 79.15\%  & 0.688     & 25.83\% \\
\midrule
NLFlow  & \textbf{12.33\%} & \textbf{17.81\%} & 84.94\% & 0.651 & 44.61\% \\
NLFlow \textit{w/o} II & 10.42\% & 11.11\% &\textbf{85.34\%} & 0.648 &  48.26\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Network Parametrization}
\paragraph{Encode with Interaction-related Information.}
We use two multi-layer perceptrons (MLPs) to encode the features of amino acids and the relationships between residue pairs.
The first MLP encodes the features of individual residues, processing the amino acid type, backbone dihedral angles, and local atomic coordinates. 
Position embeddings are integrated to identify the context of  sequences.
Additionally, properties such as polarity, charge, hydrophilicity, and sulfur presence (which is critical for stability) are encoded using one-hot vectors, serving as auxiliary interaction-related information. 
This allows the model to not only focus on structural alignment but also on the alignment of interaction forces, ultimately enhancing the affinity between the peptide and protein.
The second MLP encodes the relationships between residue pairs, capturing their relative positions, distances between atoms, and dihedral angles.
\vspace{-3mm}
\paragraph{Learning Gradient Vector with IPA and Transformers.}  
We use invariant point attention (IPA)~\cite{lee2019set} and transformer to learn the gradient vector field \( v_t \). 
The model consists of multiple IPA blocks for spatial feature learning, followed by transformer encoders for sequence modeling. 
Each block includes layer normalization, transition layers, and backbone updates, iteratively refining peptide conformations. 
Residue identity, angular encoding, and time-step embedding are integrated to ensure smooth temporal updates.


\section{Experiment}
\subsection{Set up}
In this section, we evaluate NLFlow on four tasks: (i) sequence-structure co-design, (ii) re-docking, (iii) side-chain packing and (iv) one-step generation.
% Through these evaluations, we measure NLFlow’s performance in generating stable, high-affinity peptides with accurate structural alignment and functional relevance.
Through these evaluations, we assess whether the correct temporal characterization leads to the generation of more stable, high-affinity peptides with accurate structural alignment and functional relevance.

% \subsection{Dataset}
The dataset, derived from the work of PepFlow~\cite{li2024full}, was obtained from PepBDB~\cite{wen2019pepbdb} and Q-BioLip~\cite{wei2024q}.
To ensure high-quality data, duplicates were removed, a resolution threshold of less than 4 \AA was applied, and peptide lengths were restricted to between 3 and 25 residues.
This preprocessing resulted in a final dataset consisting of 10,348 complexes, with 166 complexes reserved for the test set, and the remaining data split for training and validation.

% \subsection{Set up}
During pre-training, we simply selected the hyperparameter \( k = 2 \) for the non-linear flow model discussed in section~\ref{non-linear flow}. We trained three variants of our model to evaluate different configurations: 
(i) \textbf{NLFlow}, the full model with all components.
(ii) \textbf{NLFlow \textit{w/o} II}, which excludes the interaction-force related information.
(iii) \textbf{NLFlow \textit{w/o} II+LW}, which further removes the sequence-structure balance weight.
% Each model variant was trained for a total of 70,000 iterations to ensure sufficient training and convergence. The experiments were conducted on a Tesla P40 GPU with a batch size of 12, providing consistent conditions for model evaluation.
Each model variant was trained for a total of 65,000 iterations to ensure sufficient training and convergence. The experiments were conducted on a Tesla P40 GPU with a batch size of 12, using the Adam optimizer with a learning rate of \(5 \times 10^{-4}\) and a plateau learning rate scheduler with a factor of 0.8, patience of 10, and a minimum learning rate of \(5 \times 10^{-6}\).



% \begin{table*}[h]
% \caption{Comparison for Peptide Re-docking.}
% \label{re-docking}
% % \vskip 0.15in
% \begin{center}
% % \begin{small}
% \begin{sc}
% \begin{tabular}{lcccccr}
% \toprule
%  Method      & RMSD $\downarrow$ & Success $\uparrow$ & SSR $\uparrow$ &BSR $\uparrow$ &Diversity\\
% \midrule
% HDOCK       & 24.16     & 59.32\%    & 14.02\%  & -    &  - \\
% PepFlow      & 3.92 &62.84\%   & 83.63\%  & \textbf{91.45\%} & 0.481\\
% \midrule
% NLFlow  & \textbf{3.74} & 54.05\% & 84.04\% & 86.32\% & 0.638 \\
% NLFlow \textit{w/o} II  & 4.06 & 61.86\% & \textbf{84.18\%} & 88.86\% & \textbf{0.647} \\
% NLFlow \textit{w/o} II+LW  & 3.94 & \textbf{63.56\%} & 83.78\% & 88.14\% & 0.624 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% % \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}


\begin{table*}[h]
\begin{center}
\begin{minipage}{0.59\textwidth}
\caption{Comparison for Peptide Re-docking. \textit{Suc.} $=$Success, \textit{Div.} $=$ Diversity}
\label{tab:re-docking}
\centering
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Method      & RMSD $\downarrow$ & SSR $\uparrow$ & BSR $\uparrow$ & Suc. $\uparrow$ & Div. \\
\midrule
HDOCK       & 24.16        & 14.02\%  & -  & 12.71\%  &  - \\
PepFlow     & 4.19         & 79.18\%  & \textbf{90.02\%} & 58.89\% & 0.481 \\
\midrule
NLFlow      & \textbf{4.09}   & 80.01\%  & 85.48\% & 52.15\% & 0.638 \\
NLF \textit{w/o} II  & 4.56  & 80.64\% & 85.67\% & \textbf{60.12\%} & \textbf{0.647} \\
NLF \textit{w/o} II+LW  & 4.41  & \textbf{80.91\%} & 85.63\% & 58.89\% & 0.624 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{minipage}
\hfill
% \hspace{1mm}
\begin{minipage}{0.40\textwidth}
\centering
\caption{Comparison for Side-chain Packing}
\label{side-chain}
\begin{small}
\begin{sc}
% \begin{tabular}{lp{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}}
\begin{tabular}{lcccc}
\toprule
Method      & $\chi_1$ & $\chi_2$ & $\chi_3$ &$\chi_4$ \\
\midrule
Rosetta     & 38.31    & 43.23    & 53.61   & 71.67 \\
SCWRL4      & 30.06    & 40.40    & 49.71   & \textbf{53.79} \\
RDE-PP      & 37.24    & 47.67    & 66.88   & 62.86 \\
PepFlow     & 17.38    & 24.71    & \textbf{33.63}   & 58.49 \\
\midrule
NLFlow      & \textbf{11.69} & \textbf{19.66} & 54.87 & 55.74\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{minipage}
\end{center}
% \vskip -0.1in
\vspace{-3mm}
\end{table*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{combine.png}
     \vspace{-2mm}
    \caption{\textbf{\textit{Left:}} Box plots of metrics for  the re-docking task, including \textbf{lowest RMSD} and \textbf{best binding site similarity}. \textbf{\textit{Right:}} Reference and generated peptide for the re-docking task, highlighting the accurate \textbf{restoration of the peptide structure}.}
    \label{fig:re-dock-result}
    \vspace{-3mm}
\end{figure*}

\subsection{Sequence-Structure Co-design}
Sequence-structure co-design involves jointly generating the peptide sequence and conformation, resulting in a full-atom peptide docked onto the target protein.
% \paragraph{Baseline Model}
We evaluate three baseline models: \begin{sc}DiffPP\end{sc}, \begin{sc}PPFlow\end{sc}~\cite{lin2024ppflow} and \begin{sc}PepFlow\end{sc}~\cite{li2024full}. 
\begin{sc}DiffPP\end{sc} is a diffusion model for protein backbone parametrization, using DDPM~\cite{yang2023diffusion} and SO(3)-DPM~\cite{leach2022denoising} to model translation and rotation, along with multinomial diffusion for amino acid types. 
\begin{sc}PepFlow\end{sc} and \begin{sc}PPFlow\end{sc}, which were proposed simultaneously, are the latest models based on the flow matching framework for peptide design. They represent peptide structures by modeling backbone frames on the SE(3) manifold and side-chain dynamics on high-dimensional tori, enabling the generation of full-atom peptides with a focus on structural accuracy and torsion angle optimization.
\vspace{-2mm}
\paragraph{Metrics.}
The evaluation of the generated peptides is based on five metrics. \textbf{Energy} is calculated using Rosetta~\cite{rohl2004protein}, with two primary measures: \textbf{Affinity}, which quantifies the binding energy, where lower values indicate stronger binding potential, and \textbf{Stability}, representing the overall energy of the peptide-protein complex, with lower values indicating a more stable complex. We report the percentages of designed peptides with higher affinity and stability than the reference ones. \textbf{Diversity} is evaluated using the TM-score~\cite{zhang2005tm}, defined as the average of 1 minus the pairwise TM-scores of generated peptides, with \textbf{Novelty} calculated as the percentage of TM-score less than 0.5. \textbf{Seq Sim} is quantified by the longest common subsequence ratio between peptide pairs, reflecting the consistency between sequence and conformation.

\paragraph{Results: Achieves Superior Affinity and Stability.}
From the comparison in Table~\ref{co-design}, it can be concluded that 
(i) NLFlow exhibits a significant advantage in energy metrics, achieving 6.99\% higher stability and 4.62\% higher affinity compared to baseline models.
% This improvement is largely due to the effective handling of temporal inconsistencies across different modalities, a key distinction from the flow-based methods. 
A key distinction from flow-based methods explains that this improvement is largely due to the effective handling of temporal inconsistencies.
(ii) The inclusion of interaction-related information (II) results in the generation of 6.7\% more peptides with lower binding affinity compared to the version \textit{w/o} II, highlighting its contribution to improved peptide-protein interactions.
(iii) Our method generates diverse and novel peptide conformations. In contrast, the stronger randomness of diffusion models enables broader exploration of the conformational space, while NLFlow focuses on finer adjustments near the pocket, leading to slightly lower diversity.
% , highlighting its capacity for exploring the conformational space. 
(iv) The designed sequences also show corresponding differences, reflecting the consistency between sequence and structural in the co-design process.

% \begin{figure*}
% \begin{minipage}[b]{.5\linewidth}
% \centering
% \includegraphics[width=0.3\linewidth]{energy.png}
% \caption{pic}
% \end{minipage}
% %
% \begin{minipage}[b]{.5\linewidth}
% \centering
% \begin{tabular}{|c|c|}
% \hline
% aa 
% & bb \\ \hline
% cc 
% & dd \\ \hline
% \end{tabular}
% \captionof{table}{Table}
% \end{minipage}
% \end{figure*}

\subsection{Peptide Re-docking}
The re-docking task evaluates the model's ability to reconstruct the conformation by fixing the reference peptide sequence during the sampling stage, without retraining the model, and generating a full-atom peptide-protein complex in the docked state.
% The re-docking task evaluates the model's ability to reconstruct the conformation.
% This is done by directly using the reference peptide sequence during the sampling stage, without retraining the model, and generating a full-atom peptide-protein complex in the docked state.
% By directly using the reference peptide sequence during the inference stage without retraining the model, and generating a full-atom peptide-protein complex in the docked state.
% \paragraph{Baseline Model}
We evaluate two baseline models in re-docking task: \begin{sc}PepFlow\end{sc} and \begin{sc}HDOCK\end{sc}~\cite{yan2020hdock}.
\begin{sc}HDOCK\end{sc} is a traditional docking method, which uses a combination of rigid-body docking followed by energy minimization to predict the binding mode of two interacting proteins. 

\paragraph{Metrics.}
The evaluation of the Peptide Re-docking task is based on five metrics: 
\textbf{RMSD} measures the structural deviation between the generated peptide's $C_{\alpha}$ atoms and the reference peptide, indicating conformation accuracy;
\textbf{SSR} quantifies the similarity in secondary structure, assessing the preservation of secondary structural features;
\textbf{BSR} calculates the overlap between the docking site of the generated and the reference peptide-protein complex, reflecting binding site accuracy; 
\textbf{Success} is defined as achieving a top-1 \text{RMSD} $<$ 4 \AA, with BSR and SSR both greater than 0.8. The result reports the percentage of successful cases;
and \textbf{Diversity}, same as the co-design task, calculated as 1 minus the average TM-score, measuring the variation in peptide conformations.

\paragraph{Results: Lower RMSD in Reconstructing.}
Table~\ref{tab:re-docking} shows that 
(i) NLFlow achieves the lowest RMSD, performing superiorly in structural accuracy. This demonstrates that by addressing temporal inconsistencies between modalities, the correct docking process simulation provides the model with a strong ability to reconstruct conformations.
As illustrated in Figure~\ref{fig:re-dock-result}, NLFlow not only focuses on fine-grained atomic coordinates but also effectively restores the overall macrostructural features, 
reflecting the model's deep understanding of the docking task under correct temporal alignment.
% preserving key secondary structures essential for stable peptide-protein interactions. 
While PepFlow excels in binding site recognition (BSR), NLFlow achieves a better balance between structural alignment and conformational diversity.
(ii) Compared to flow-based models, the traditional method HDOCK, which does not redesign the peptide structure, does not provide SSR or diversity results, limiting its ability to model peptide-protein interactions. 
(iii) Among our model variants, removing interaction force information (\textit{w/o} II) increases the success rate, as it shifts the model's focus from balancing structural alignment and docking interactions to solely optimizing structural alignment.

\subsection{Side-chain Packing}
This task evaluates the model's ability to predict the correct torsional angles for the side chains, which are crucial for accurate protein-ligand docking and stability. Specifically, we calculate the mean squared error (MSE) of the four predicted side-chain torsional angles. We use energy-based methods: \begin{sc}RosettaPacker\end{sc}~\cite{leman2020macromolecular}, \begin{sc}SCWRL4\end{sc}~\cite{krivov2009improved}, and Rotamer Density Estimator (RDE)~\cite{luo2023rotamer} with Conditional Flow on TNrt: \begin{sc}RDE-PP\end{sc}~\cite{lin2024ppflow} as baselines.
% \vspace{-2mm}
\paragraph{Results: Strong Performance in Lower-Order Torsions.}
As shown in Table \ref{side-chain}, NLFlow demonstrates a clear advantage in predicting the \( \chi_1 \) and \( \chi_2 \) torsional angles, outperforming all other methods. 
This highlights the importance of adjusting torsion angles according to the pocket environment, reflecting NLFlow's ability to finely tune the conformation near the pocket.
However, for the more challenging \( \chi_3 \) and \( \chi_4 \) angles, the performance of all models is comparable, with showing worse results compared to \( \chi_1 \) and \( \chi_2 \). This highlights that higher-order torsions remain a complex task, with no model consistently outperforming others across all angles.

\subsection{One-step Generation}
Most flow matching methods rely on the assumption that the direction of the gradient vector field remaining constant, which theoretically enables one-step generation. 
However, achieving this in practice is challenging in non-linear flow due to two key issues: first, path conflicts as identified by Rectified Flow~\cite{liu2022flow} exist across all flow-based models;
second, our non-linear assumptions hinder the ability of the initial time step to capture temporal inconsistencies across different modalities, limiting the effectiveness of one-step generation.

In this task, we set varying number of inference steps (N) in sampling stage for sequence-structure co-design task to evaluate how it impacts the model’s ability and determine the minimum steps required to capture the temporal inconsistencies in multimodal data.

\begin{figure}[htbp]
\label{fig:step}
    \centering
    \begin{minipage}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{stab_affinity_plot.pdf}
        % \caption{energy versus number of steps}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{diversity_seqsim_plot.pdf}
        % \caption{Image 2}
    \end{minipage}
\vspace{-2mm}
\caption{Metrics in different inference steps. \textbf{\textit{Left:}} Energy metrics. \textbf{\textit{Right:}} Geography Metrics (Normalized to Range $[0, 1]$).}
\vspace{-3mm}
\end{figure}

\paragraph{Results: Rapid Improvement with Minimal Steps.}
As shown in Fig~\ref{fig:step}, at $step = 1$, the lack of temporal consistency across modalities leads to poor performance, particularly with stability (Stab = 0). However, by $step = 2$, the model rapidly improves, indicating that the difficulty of one-step generation primarily arises from the absence of temporal information, highlighting the importance of the non-linear assumption. After $step > 10$, further increasing the number of steps has minimal impact on the model's performance, suggesting that our model largely follows the straight-line assumption of flow, making it a fast and efficient generation model.

% \section{Conclusion}

% In this work, we proposed NLFlow, a non-linear flow matching approach for full-atom peptide design, to address the challenges of accurately restoring the peptide and protein docking process in a biologically plausible manner. 
% By designing a non-linear vector field, NLFlow resolves the temporal inconsistencies across modalities. Incorporating interaction force information, NLFlow enhances both the structural and functional properties of peptides, significantly improving their stability, affinity, and diversity compared to existing models.
% Experimental results demonstrate that NLFlow excels in both sequence-structure co-design and structural reconstruction, generating peptides with high structural accuracy and functional relevance, while also showing rapid improvement with minimal inference steps, highlighting its efficiency.

% Despite these advances, NLFlow still faces challenges in improving prediction accuracy for higher-order torsions in complex conditions. The reliance on predefined interaction force features may also limit its ability to capture the full complexity of peptide-protein interactions. Future work will focus on refining higher-order conformation predictions and integrating dynamic, data-driven interaction force learning to design better peptides.

\section{Conclusion}  
In this work, we present \textit{NLFlow}, a non-linear flow matching framework designed to address the critical challenge of temporal inconsistency across modalities in full-atom peptide design. 
By introducing a polynomial-based interpolation scheme and its associated time-varying gradient vector field, NLFlow explicitly decouples the evolution of the positional manifold from rotational, torsional, and residue-type manifolds. 
This approach captures the biologically hierarchical nature of peptide docking, resulting in peptides with enhanced binding affinity and stability. 
The integration of interaction-related features further enables the model to balance structural alignment with functional optimization, providing a nuanced simulation of peptide-protein interactions. 

Despite these advancements, NLFlow remains limited by suboptimal accuracy in predicting higher-order torsions under complex conformational constraints, 
and its predefined polynomial interpolation, though effective for temporal decoupling, restricts flexibility in modeling dynamic temporal hierarchies. 
Future directions include developing adaptive flow architectures to learn temporal dynamics directly from data, and incorporating dynamic interaction force predictors for context-aware refinement. 



\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Examples of peptides}
We present some other peptide results generated in the re-docking task, showing the recovery of the structure.
\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{redock.png}
    \caption{Examples of peptides generated in the re-docking task}
    \label{fig:enter-label}
\end{figure*}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
