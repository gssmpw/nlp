\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\setlength{\parskip}{-1.8pt}  % Removes extra space between paragraphs

% \setlength{\parskip}{0pt}
% \setlength{\parsep}{0pt}


\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{balance}

\begin{document}

\title{Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach}

% \thanks{This work was sponsored by the Junior Faculty Development program under the UM6P-EPFL Excellence in Africa Initiative.}
% }

\author{
  Abdullahi Isa Ahmed\textsuperscript{1}, 
  Jamal Bentahar\textsuperscript{2,3}, 
  El Mehdi Amhoud\textsuperscript{1} \\
  \textsuperscript{1}College of Computing, Mohammed VI Polytechnic University (UM6P), Benguerir, Morocco. \\
  \textsuperscript{2}Department of Computer Science, Khalifa University, Abu Dhabi, UAE. \\
  \textsuperscript{3}Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada. \\
  Emails: {\{abdullahi.isaahmed, elmehdi.amhoud\}@um6p.ma, jamal.bentahar@ku.ac.ae}
}

\maketitle

\begin{abstract}
As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes.
\end{abstract}

\begin{IEEEkeywords}
Internet of Things (IoT), Long rang (LoRa), Energy efficiency, UAV communication, resource allocation.
\end{IEEEkeywords}


\section{Introduction} \label{Section_1}
The next-generation Internet of Things (NG-IoT) technologies for 5G and 6G applications are revolutionizing communication by enabling seamless data exchange between devices and networks. This evolution drives intelligent applications in areas such as healthcare, smart cities, agriculture, and \hbox{autonomous} vehicles \cite{jouhari2023survey}. With IoT device connections expected to reach 125 billion by 2030 \cite{fakhruldeen2024enhancing}, energy consumption presents a significant challenge, particularly in maintaining low-power and long-range communication networks. Enhancing energy efficiency (EE) is therefore essential to align with global sustainability objectives, including the United Nations Sustainable Development Goal 7: Affordable and Clean Energy \cite{orazi2018first}. 

Low-power wide area networks (LPWANs), particularly long-range (LoRa) technology, have emerged as a cost-effective solution for long-distance communication in IoT applications. However, existing terrestrial LoRa networks depend on fixed ground-based gateways (GWs), which struggle with non-line-of-sight (NLoS) propagation, especially in dense urban or remote environments. While deploying additional terrestrial LoRa GWs is affordable, it does not necessarily resolve NLoS issues. Conversely, satellite-based IoT solutions, such as the FOSSA system\footnote{FOSSA systems is a low-Earth orbit (LEO) satellite network providing global IoT connectivity for remote areas. More details at: https://fossa.systems/}, aim at connecting IoT devices to non-terrestrial networks. However, this approach significantly increases transmission power requirements and introduces higher latency, making it impractical for many energy-constrained IoT applications.

Beyond infrastructure deployment, effective resource allocation plays a crucial role in optimizing LoRa network performance. Existing studies primarily rely on alternative optimization techniques, where complex optimization problems are decomposed into sub-problems and solved iteratively \cite{hamdi2020dynamic}. Although alternative optimization methods have proven effective in certain network environments, they often struggle to adapt dynamically to varying IoT traffic patterns and environmental conditions, leading to suboptimal resource utilization. For example, the authors in \cite{xiong2023flyinglora} employ an alternative optimization-based method for a single-flying LoRa GW, which lacks adaptability in dynamic environments. In contrast, reinforcement learning (RL)-based approaches, such as those in \cite{yu2020multi, aihara2019q}, utilize the Q-learning approach for resource allocation. However, these methods depend on static Q-tables, making them impractical for managing complex and dynamic IoT environments. To improve the EE system of LoRa, the work in \cite{10279198} proposes a framework based on deep reinforcement learning (DRL) proximal policy optimization (PPO), but it is limited to a single GW for data collection, which restricts scalability and flexibility.  

To address these limitations, we propose a UAV-assisted multi-flying LoRa GW deployment. This novel approach can dynamically reposition GWs to enhance network coverage, mitigate NLoS issues, optimize end-devices (EDs) association, minimize transmission power, and maximize energy-efficient communication. Furthermore, we utilize a multi-agent reinforcement learning (MARL) framework with multi-agent PPO (MAPPO) to optimize resource allocation in multi-flying LoRa GWs. Although single-agent RL approaches have shown promise in simpler scenarios \cite{10279198}, extending them to multi-UAV LoRa networks introduces significant challenges, including partial observability, decentralized control, and the need to optimize multiple interdependent parameters in dynamic environments. Our MAPPO approach overcomes these challenges by allowing UAV-mounted GWs to autonomously manage key network parameters, including spreading factor (SF) allocation, transmission power (TP) control, bandwidth (W) selection, and ED association. By incorporating air-to-ground (A2G) link characteristics, our framework ensures efficient resource distribution while maximizing system-wide EE. Unlike traditional methods, MAPPO continuously updates policies in real time, making it more adaptable to dynamic network conditions. To the best of the authors' knowledge, this is the first work to propose a MARL-based approach for optimizing global system EE in a multi-flying LoRa GW deployment. The main contributions of this paper are summarized as follows:
\begin{itemize}
\item We formulate the resource allocation and user association problem as a single optimization problem aimed at maximizing EE in LoRa networks, considering both device positions and A2G propagation.
\item We develop a partially observable Markov decision process (POMDP) model tailored to the LoRa system, with states, action spaces, and reward functions designed for stable MAPPO agent learning.
\item We leverage the MARL-based MAPPO approach that enables flying LoRa GWs to learn a centralized value function while optimizing their policies to maximize global system EE.
\item Our proposed framework leverages one-to-many matching algorithms for an efficient device-to-gateway association, demonstrating faster and more stable convergence compared to state-of-the-art MARL algorithms.
\end{itemize}
The rest of the paper is organized as follows. We describe the system model in Section~\ref{Section_2}. Section~\ref{Section_3} provides a mathematical formulation of the problem. In Section~\ref{Section_4}, we describe the proposed MARL configuration in detail. We evaluate the effectiveness of the proposed approach in Section~\ref{Section_5}. Finally, the paper concludes with a comprehensive summary and sets forth some perspectives in Section~\ref{Section_6}

\section{System Model} \label{Section_2} 
We consider a resource allocation framework for uplink transmission in a LoRa network consisting of $\mathcal{V}$ LoRa EDs, $\mathcal{U}$ flying GWs, and a single network server, as shown in Fig.~\ref{fig1}. In this setup, UAVs equipped with LoRa GWs are deployed over a square target area $S$. Each flying LoRa GW has a limited communication range and can simultaneously connect to multiple EDs within its association quota $\Lambda_{max}$. Specifically, each GW collects and decodes packets from all eligible EDs in range and relays these packets to the network server. The sets of EDs and GWs are denoted by $\mathcal{V} = \{1, \ldots, V\}$ and $\mathcal{U} = \{1, \ldots, U\}$, respectively.

\subsection{LoRa End Devices Mobility}
To accurately model the mobility behavior of our LoRa ground EDs, we initially deployed the EDs at random positions within an area of interest. For a given ED $v$, a unique speed vector is assigned $\mathbf{s}_{v}(t) = [s_v^{x}(t), s_v^{y}(t)]$, where $s_v^{x}(t)$ and $s_v^{y}(t)$ are respectively the speed along $x$ and $y$ axes. Furthermore, the random assignment ensures that each ED moves in an independent direction with a distinct speed. At each time step $t \in \mathcal{T} =  \{1, \ldots, T\}$, the EDs update their positions based on their assigned speed. If an ED reaches the area's boundary, it bounces back by reversing its speed direction while ensuring it remains within the limits. Additionally, there is a $p_0$ probability per ED per time step that its speed will be randomly reassigned, introducing variability in movement over time. 
\begin{figure}[t]
  \centering % Centers the figure horizontally
  \includegraphics[width=0.72\linewidth]{Figures/System_model.png} 
  \caption{The studied system model.}
  \label{fig1}
  \vspace{-14pt}
\end{figure}

\subsection{Air-to-Ground Channel Model}
In UAV communication systems, A2G link propagation plays a crucial role and can be leveraged for flying GWs communication \cite{zhou2022joint}. In this work, a probabilistic path loss model was used to model A2G communication links, whereby the line-of-sight (LoS) and NLoS links were considered separately with different probabilities of occurrence. Consequently, the likelihood of having a LoS connection between GW $u$, and an ED $v$ is given by:
\begin{equation}
 P_{LoS}(\hat{\phi})=\frac{1}{1+\vartheta e^{-\lambda(\hat{\phi}-\vartheta)}},
\label{eqt_p_los}
\end{equation}
where $\hat{\phi}$ = $sin^{-1}(\frac{h_u}{d_{u,v}})$ is the angle of elevation from GWs $u$, $u\in \mathcal{U}$ to EDs $v$, $v\in \mathcal{V}$. $h_u$ is the altitude of the UAV $u$. The Euclidean distance between $u$ and $v$ is denoted as $d_{u,v}$. $\vartheta$ and $\lambda$ are constants that depend on the environment. Note, from Eq. \eqref{eqt_p_los}, the probability values for $P_{NLoS}$ can be expressed as $P_{NLoS}(\hat{\phi}) = 1 - P_{LoS}(\hat{\phi})$. Furthermore, the path loss model from UAV $u$ to the ground LoRa ED $v$ can be expressed as: 
\begin{subequations}
\begin{align}
L^{LoS}_{u,v} &= L^{FS}(d_{r}) + 10\delta_{LoS}d_{u,v} + \mathcal{X}\sigma_{LoS}, \label{eqt_los} \\
L^{NLoS}_{u,v} &= L^{FS}(d_{r}) + 10\delta_{NLoS}d_{u,v} + \mathcal{X}\sigma_{NLoS}, \label{eqt_nlos}
\end{align}
\end{subequations}
where $L^{FS}$ denotes the free-space path-loss with \mbox{$L^{FS}(d_r) = 20 \log_{10} \left( \frac{4\pi d_r f}{c} \right)$}. $\delta$ is the path loss exponent.  $f$ is the carrier frequency in Hz, $c$ in m/s is the speed of light, and $d_{r}$ is the reference distance. $\mathcal{X}\sigma_{LoS}$ and $\mathcal{X}\sigma_{NLoS}$ are the shadowing random variables which are characterized as Gaussian random variables with zero mean and $\sigma_{LoS}$ and $\sigma_{NLoS}$ standard deviations. Consequently, the overall A2G path loss between GW $u$ and LoRa ED $v$ is characterized as:
\begin{equation}
l_{u,v}^{a2g}= P_{LoS}(\hat{\phi})L^{LoS}_{u,v} + P_{NLoS}(\hat{\phi})L^{NLoS}_{u,v}.
\label{eqt_a2g_los}
\end{equation}
\subsection{Energy Efficiency Model}
To model the EE of our proposed system, we begin by linking the network topology to the LoRa PHY parameters. This is achieved by first modeling the signal-to-noise ratio (SNR) between UAV $u$ and ED $v$ at time slot $t$, expressed as
\begin{equation}
    \rho_{u,v}(t) = \frac{P_{v}(t) \cdot G_{u,v}}{\sigma^{2}},
\end{equation}
where $G_{u,v} = 10^{-{l_{u,v}^{\text{a2g}}} / {10}}$ represents the channel gain, $P_{v}(t)$ is the TP of an ED at time $t$, and $\sigma^{2}$ is the noise power. Building on this, we calculate the signal-to-interference-plus-noise ratio (SINR) $\mho_{u,v}^{n}(t)$ between UAV $u$ and ED $v$ using the $n$-th SF at time slot $t$ as follows:
\begin{equation}
    \mho_{u,v}^{n}(t) = \frac{\rho_{u,v}(t)}{\sum_{{v}' \in \mathcal{V} \setminus \{ v \}} \psi_{{v}',n}(t) \cdot \rho_{a_{{v}'}(t),{v}'}(t) + 1},
    \label{sinr}
\end{equation}
where $\rho_{a_{{v}'}(t),{v}'}(t)$ represents the SNR between ED $v'$ and its associated UAV $a_{{v}'}(t)$, and $\psi_{{v}',n}(t)$ is the binary association parameter that indicates whether user $v'$ selected SF $n$.

The achievable data rate for the link between UAV $u$ and ED $v$ at time slot $t$ is derived using the Shannon-Hartley theorem \cite{aczel1974shannon} and is given by:
\begin{equation}
\Re_{u,v}(t) = W_{v}(t) \cdot \log_2\left(1 + \mho_{u,v}^{n}(t)\right),
\label{eqt_shannon}
\end{equation}
where $W_{v}(t)$ is the bandwidth allocated to the communication link. Furthermore, the EE of each UAV is calculated by dividing the sum of all uplink data rates from all EDs connected to the UAV by the total consumed power. Hence, we define the EE $\zeta_{u}(t)$ of UAV $u$ at time $t$ as:
\begin{equation}
\zeta_{u}(t) = \frac {\sum_{v \in \mathcal{V}} \Re_{u,v}(t) a_{u,v}(t)}{P_{T} + P_{c}},
\label{eqt_EE}
\end{equation}
where $\Re$ in bits/second is the data rate given in Eq.\eqref{eqt_shannon}. $P_{c}$ is the circuit power consumption, and $P_{T} = \sum_{v=1}^{V} P_{v}(t) a_{u,v}(t)$ is the transmission power, where $a_{u,v}(t)$ is the ED binary association.

\subsection{EDs Association and Resource Allocation Scheme} \label{signal_model}
In the UAV-mounted LoRa GWs system, implementing a dynamic EDs association is a crucial element in efficiently managing connections between ground EDs and flying GWs. Given that each UAV serves multiple EDs, it is important to ensure the association is properly established to ensure load balance in the network. Therefore, the binary association between a UAV \( u \) and an ED \( v \) at time slot \( t \) is denoted by $a_{u,v}(t) \in \{0, 1\}$, \( a_{u,v}(t) = 1 \) if ED \( v \) is being served by UAV \( u \) at time \( t \), and \( a_{u,v}(t) = 0 \) otherwise. The index of the UAV selected by ED \( v \) at time \( t \) can be expressed as $a_{v}(t) = \sum_{u \in \mathcal{U}} a_{u,v}(t) \cdot u$. 
% $\footnote{This scalar value represents the UAV associated with ED \( v \) and is essential for calculating interference in the SINR as given in Eq. \eqref{sinr}.}

In this work, SF is selected from a vector $\mathbf{\Psi} = \{\psi_{1}, \cdots, \psi_{N}\} $. For each ED $v$, the allocation of SF follows a binary association that is expressed as $\psi_{v,n}(t)$, $n \in \mathcal{N} = \{1, \cdots, N\}$. Therefore, $\psi_{v,n}(t) = 1$ if ED $v$ communicates at SF $\psi_{n}$ during time slot $t$; otherwise, $\psi_{v,n}(t) = 0$. The selected index can be expressed as $\Psi_{v}(t) = \sum_{n \in \mathcal{N}} \psi_{v,n}(t) \cdot \psi_{n}.$ We assume that each ED can transmit data using only one SF at any given time step, leading to the following constraint
% \vspace{-15pt}
\begin{equation}
    \sum_{n=1}^{N} \psi_{v,n}(t) \leq 1, \forall v \in \mathcal{V}, t \in \mathcal{T}.
    \label{sf_binary}
\end{equation}
% \vspace{-10pt}

Similarly, the transmission power level is selected from a vector $\mathbf{P} = \{p_{1}, \cdots, p_{J}\}$ in dBm. For each ED $v$, we define a binary allocation variable $p_{v,j}(t)$, $j \in \mathcal{J} = \{1, \cdots, J\}$. Therefore, $p_{v,j}(t) = 1$ if ED $v$ transmits with power level $p_{j}$ at time $t$; and $p_{v,j}(t) = 0$ otherwise. Note that the selected index is $P_{v}(t) = \sum_{j \in \mathcal{J}} p_{v,j}(t) \cdot p_{j}.$ We also assume that each ED can transmit data using only one TP at each time step, imposing the following constraints:
% \vspace{-8pt}
\begin{equation}
    \sum_{j=1}^{\mathcal{J}} p_{v,j}(t) \leq 1, \forall v \in \mathcal{V}, t \in \mathcal{T}.
    \label{tp_binary}
\end{equation}

In addition, the bandwidth $W$ for communication is selected from a vector  $\mathbf{W} = \{w_{1}, \cdots, w_{M}\}$ in kHz. We assume that each deployed flying GW contains a distinct LoRa module with specific bandwidth requirements. Hence, the bandwidth binary allocation is $w_{v,m}(t)$, $m \in \mathcal{M} = \{1,\cdots, M\}$ such that $w_{v,m}(t) = 1$ if ED $v$ transmits using bandwidth $w_{m}$ at time $t$; otherwise, $w_{v,m}(t) = 0$. Also, the selected index used in Eq.\eqref{eqt_shannon} is expressed as $W_{v}(t) = \sum_{m=1}^{M} w_{v,m}(t) \cdot w_{m}.$

Consequently, we define finite sets for all possible SF selections $\bar{\bf \Psi}$, TP allocations $\bar{\textbf{P}}$, user associations a, and bandwidth selections $\bar{\textbf{W}}$, which can be expressed as: 
% \vspace{-15pt}
\begin{subequations}
\begin{align}\bar{\mathbf{\Psi}}
 = \{ \Psi_{v}(t) \in {\bf \Psi} \mid \sum_{n=1}^{\mathcal{N}} \psi_{v,n}(t) \leq 1 , \quad \forall v \in \mathcal{V}\}, \label{sf} \\
 \bar{\mathbf{P}} = \{ P_{v}(t) \in \textbf{P} |\sum_{j=1}^{\mathcal{J}} p_{v,j}(t) \leq 1 , \quad \forall v \in \mathcal{V}\},  \label{tp} \\
\mathbf{a} = \{ a_{v}(t) \in \mathcal{U} |\sum_{u=1}^{\mathcal{U}} a_{u,v}(t) \leq 1 , \quad \forall v \in \mathcal{V}\}, \label{nodes} \\
\bar{\mathbf{W}} = \{ W_{v}(t) \in \textbf{W} |\sum_{m=1}^{\mathcal{M}} w_{v,m}(t) \leq 1 , \quad \forall v \in \mathcal{V}\}. \label{w}
\end{align}
\end{subequations}
\section{Problem Formulation} \label{Section_3}
This section provides a detailed description of the optimization problem based on our proposed system model. The objective of this paper is to maximize the global system EE of flying LoRa GWs, as defined in Eq.~\eqref{constr:a}. Consequently, we formulate our optimization problem as follows:
\begin{subequations}\label{formulated_algo1}
\begin{align}
\max_{\mathbf{\bar{\Psi}, \bar{P}, a, \bar{W}}} 
&  \quad  \sum_{t=1}^T \sum_{u=1}^U \zeta_{u}(t), \label{constr:a} \\
\text{s.t.} \quad \;\;
& \;a_{u,v}(t) \in \{0, 1\}, \forall u, v, t \in \mathcal{T}, \label{constr:b} \\
& \;\sum_{u \in \mathcal{U}} a_{u,v}(t) \leq 1, \forall v, t \in \mathcal{T}, \label{constr:c} \\ 
& \;\sum_{v \in \mathcal{V}} a_{u,v}(t) \leq \Lambda_{max} , \forall u, t \in \mathcal{T}, \label{constr:d} \\
& \;\psi_{v,n}(t) \in {\bf \Psi}, \forall v, t \in \mathcal{T}, \label{constr:e} \\
& \;p_{v,j} \in \textbf{P}, \forall v, \label{constr:f}\\
& \;w_{v,m}(t) \in \textbf{W}, \forall v, t \in \mathcal{T}, \label{constr:g} \\
& \;\rho_{u,v}(t) \ge  \text{SNR}_{threshold}, \forall u, v,  t \in \mathcal{T} \label{constr:h} \\
& \; \eqref{sf_binary}, \text{ and } \eqref{tp_binary}.
\end{align}
\end{subequations}

Here, the constraint ~\eqref{constr:b} defines the binary indicator \( a_{u,v} \), which specifies whether an ED is associated with a GW. Constraint ~\eqref{constr:c} ensures that each ED can access at most one channel. Constraint ~\eqref{constr:d} limits the number of EDs \( v \) that can be associated with a single GW \( u \) to at most \( \Lambda_{max} \). Furthermore, constraint ~\eqref{constr:e} defines the available set of SFs. Constraint ~\eqref{constr:f} restricts the transmit power of the ED to be selected from a predefined discrete set. Constraint ~\eqref{constr:g} ensures that the bandwidth allocated to each ED \( v \) is chosen from a discrete set \( \textbf{W} \). Finally, constraint ~\eqref{constr:h} enforces that the received SNR must not fall below the threshold \( \text{SNR}_{threshold} \), as shown in Table~\ref{table:sinr_threshold}, to ensure correct detection of LoRa EDs adopting a specific SF \cite{yu2020multi}.

The formulated optimization problem \eqref{formulated_algo1} is NP-hard due to its combination of binary and stochastic constraints, along with the selection of discrete variables for SF, TP, and W, leading to a non-convex objective function that is challenging for conventional optimization methods. As established in \cite{su2020energy}, no efficient polynomial-time solution exists for this problem. Consequently, traditional RL methods, which require storing all MDP tuples in a table, are computationally intensive and unsuitable for multi-agent scenarios; thus, we design POMDP method and leverage the MAPPO-based algorithm to tackle the formulated problem in Eq.\eqref{formulated_algo1}.

\section{Multi-Agent PPO Algorithm} \label{Section_4}
MAPPO is a promising framework that builds on the centralized training, decentralized execution (CTDE) scheme and extends the PPO to the multi-agent system \cite{yu2022surprising, 10195758}. It focuses on the actor-critic architecture, which consists of two components: the actor, who is in charge of making decisions, and the critic, who analyses those actions using a value function. In a CTDE architecture shown in Fig.~\ref{fig2}, the critic network learns a centralized value function that includes knowledge of all agents' activities, whilst each agent uses the actor-network to determine its policy based solely on local observations. Furthermore, due to the limited communication range and the maximum number of EDs each flying LoRa GW can associate with at any given time, the global state of the system cannot be fully observed by a single agent. Consequently, we model the problem using a POMDP. In this framework, each agent only has partial information about the environment, leading to uncertainty in decision-making. Formally, the POMDP is defined by the tuple $\left \langle \mathcal{U}, \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma, \mathcal{O} \right \rangle $. 

Hence, the proposed framework consists of the following elements:

\textit{1) Agents $\mathcal{U}$:} the set of flying LoRa GWs.

\textit{2) States $\mathcal{S}$:} The global state represents the complete environment configuration at time step $t$. Therefore, we define the global state vector of all UAVs as $\mathbf{\mathcal{S}}(t) = \left\{ s_1(t), s_2(t), \dots, s_u(t), \dots, s_U(t) \right\}$, where $s_u(t)$ denotes the local state of agent $u \in \mathcal{U}$ at time $t$.

\textit{3) Observations $\mathcal{O}$:} Due to partial observability, each flying LoRa GW $u$ at time $t$ perceives only a subset of the global state $\mathbf{\mathcal{S}}(t)$. Hence, the local observation $o_u(t) \in \mathcal{O}_u \subset \mathcal{S}(t)$ for agent $u$ is denoted as $o_u(t) = \left\{ \psi(t), p(t), \Re(t), assEDs_{pos}(t), GWs_{pos}(t) \right\}$. Here, $assEDs_{pos}(t)$ represents the positions of the EDs currently associated with GW $u$, and $GWs_{pos}(t)$ are the positions of neighboring GWs within communication range. This partial observation mechanism ensures that each GW makes decisions based on its local view of the environment.
\begin{table}[t]
    \centering
    {\tiny % Adjust the size if needed
    \caption{\small{SINR Thresholds with $W = 125$ kHz \cite{yu2020multi}}}
    \label{table:sinr_threshold} 
    }
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        SF & 7 & 8 & 9 & 10 & 11 & 12 \\
        \hline
        SNR$_{threshold}$ (dB) & -7.5 & -10 & -12.5 & -15 & -17.5 & -20 \\
        \hline
    \end{tabular}
    \vspace{-12pt}
\end{table}

\textit{4) Actions $\mathcal{A}$:} Each agent $u \in \mathcal{U}$ selects an action $a_u(t) \in \mathcal{A}_u$ at time $t$. The action comprises the selection of communication parameters including spreading factor, transmission power, and bandwidth. This can be represented as $a_u(t) = \left\{ \psi_u(t), p_u(t), w_u(t) \right\}$, where $\psi_u(t)$, $p_u(t)$, and $w_u(t)$ denote the SF, TP, and bandwidth chosen by agent $u$ at time $t$, respectively.

\textit{5) Reward Function $\mathcal{R}$:} The reward for each flying LoRa GW \( u \) at time step \( t \) is defined as:
\begin{equation}
r_u(t) = \frac{\sum\limits_{v \in \mathcal{V}} \Re_{u,v}(t) \cdot a_{u,v}(t)}{\sum\limits_{v \in \mathcal{V}} P_{u,v}(t) \cdot a_{u,v}(t) + P_c}.
\label{eq:per_agent_reward_raw}
\end{equation}

Note that the reward in Eq.\eqref{eq:per_agent_reward_raw} is allocated only if the SNR constraint \eqref{constr:h} is satisfied; otherwise, the reward is zero. This constraint can be formulated as:
\begin{equation}
r_u(t) = 
\begin{cases}
r_u(t), & \text{if constraint \eqref{constr:h} is satisfied,} \\
0, & \text{otherwise}.
\end{cases}
\label{eq:per_agent_reward}
\end{equation}

Therefore, the cumulative reward over the entire time horizon \( T \) and all GWs \( \mathcal{U} \) is:
\begin{equation}
\mathcal{R} = \sum_{t=1}^T \sum_{u=1}^U r_u(t)
\label{eq:cumulative_reward}
\end{equation}

\textit{6) Policy function:} The policy function $\boldsymbol{\pi_{\theta_{u}}}(\textbf{a}_u, o_u)$ is modeled by an actor-network, with the vector \(\boldsymbol{\theta}_{u}\) as its parameters. It determines the strategy for the flying GWs based on their local observations.

\textit{7) Value function:} The value function \(V_{\phi}(\boldsymbol{o_u})\) is implemented by a critic network parameterized by \(\phi\). This network evaluates the expected future rewards for flying GWs given the current state \(\boldsymbol{o}_u\). The critic aims to learn a value function that approximates the optimal future rewards, guiding the agents toward the globally optimal policy that maximizes their long-term rewards.

As shown in Fig.~\ref{fig2}, our system leverages MAPPO, which is based on the CTDE framework. During the training phase, MAPPO alternates between optimizing the actor and critic networks until stable convergence is achieved. Specifically, the flying GWs locally update the policy actor using the PPO method. At each training step, given the state of a GW, the actor-network is trained, and action is sampled according to the policy function $\pi_{\theta_{u}}(a_u(t), o_u(t))$. Once the joint action is executed, the corresponding reward is observed. Subsequently, the global state vector, representing the collective states of all flying GWs, is passed to the critic network. The critic network is trained by minimizing a predefined loss function, which helps evaluate future rewards for the policy and improves the overall strategy over time.



\section{Performance Evaluation} \label{Section_5}
\subsection{Simulation setup}
In this work, we evaluate the performance of our proposed framework through simulations with a 2000$m$ $\times$ 2000$m$ area, 60 LoRa EDs are randomly deployed and 5 flying LoRa GWs for data collection tasks. We assume that the flying GWs fly at a fixed altitude of 150$m$. It is also assumed that the speed components of an ED $v$, denoted as $s_v^{x}(t)$ and $s_v^{y}(t)$, are randomly selected between -1 and 1 m/s. Furthermore, the probability of speed reassignment at each timestep is given by $p_0 = 0.1$. 

To satisfy the LoRa quality of service constraint, probabilistic path loss exponents for LoS and NLoS are given by $\delta_{\text{LoS}} = 2$ and $\delta_{\text{NLoS}} = 2.5$. We consider a Gaussian random variables $\mathcal{X}\sigma_{\text{LoS}} = 5$ and $\mathcal{X}\sigma_{\text{NLoS}} = 20$. In addtion, we consider a carrier frequency of $f = 868\,\text{MHz}$. Furthermore, the agents can dynamically select configurations from three adjustable parameters. Particularly, SF is chosen from $\{7, 8, 9, 10, 11, 12\}$, TP is selected from $\{2, 5, 8, 11, 14\}\,\text{dBm}$, and bandwidth $W$ configured from $\{125, 250, 500\}\,\text{kHz}$. For training, we employ a recurrent neural network (RNN) with 128 hidden units, a learning rate of $\alpha = 3 \times 10^{-4}$ for both actor and critic networks, and a soft target update coefficient of $0.01$. The training protocol uses a PPO clip range of $\epsilon = 0.2$, a discount factor of $\gamma = 0.99$, and the Adam optimizer.

In our simulation, we compare our proposed approach with three other MARL algorithms: Counterfactual Multi-Agent (COMA) \cite{foerster2018counterfactual}, Multi-Agent Advantage Actor-Critic (MAA2C) \cite{papoudakis2020benchmarking}, and Value-Decomposition Networks (VDN)\cite{sunehag2017value}. The COMA algorithm is an actor-critic method in which a centralized critic estimates the Q-function, with decentralized actors optimizing their policies. The MAA2C algorithm extends the A2C algorithm to multi-agent scenarios by incorporating a centralized critic that learns the joint value function. At the same time, each agent maintains its own actor to learn individual policies. Finally, the VDN framework is designed for cooperative MARL tasks, wherein each agent learns a distinct value function that is decomposed into shared and local value functions.
\begin{figure}[t]
  \centering % Centers the figure horizontally
  \includegraphics[width=0.45\linewidth]{Figures/MARL_CTDE.png} 
  \caption{Centralized Critic and Decentralized Actor for MAPPO training.}
  \label{fig2}
  \vspace{-12pt}
\end{figure}
\subsection{Simulation Results}
In Fig.~\ref{fig:training_optimization}(a), we illustrate the association between EDs and GWs in a 3D plane. As shown in the figure, we employ the one-to-many matching scheme from \cite{Roth_Sotomayor_1990}, where each GW is associated with multiple LoRa EDs. Consequently, the quota constraint is maintained, and the network load is balanced across GWs. In Fig.~\ref{fig:training_optimization}(b), the training performance is evaluated for four different learning rates ($\alpha$). In this setup, we fixed the clipping range at $\epsilon = 0.2$, with 60 EDs and 5 GWs. Each training curve in the figure represents the global cumulative reward over environment timesteps for a specific learning rate: $\alpha = 0.0005$, $\alpha = 0.0003$, $\alpha = 0.003$, and $\alpha = 0.03$. It can be observed that $\alpha = 0.0003$ achieves the highest rewards with stable convergence throughout the training phase. In contrast, $\alpha = 0.03$ performs the worst, yielding very low global cumulative rewards with an unstable training process at different timesteps. Although $\alpha = 0.0005$ converges slightly faster than $\alpha = 0.0003$, it becomes unstable toward the end of training. Therefore, we utilize $\alpha = 0.0003$ as it achieves the highest reward while maintaining stable learning.

In Fig.~\ref{fig:training_optimization}(c), the convergence behavior is examined for different numbers of GWs while keeping EDs fixed at 60, $\epsilon = 0.2$, and $\alpha = 0.0003$. The results show that with fewer GWs (e.g., 2), the rewards converge faster and stabilize earlier but fail to achieve the highest global system EE. On the other hand, as the number of GWs increases (e.g., 6 and 8), the convergence rate decreases. However, it can be observed that a higher number of GWs leads to greater rewards. This is expected, as more GWs provide better spatial coverage and improved connectivity, resulting in more efficient data aggregation and reduced communication overhead. Consequently, this enhances the overall global system EE and enables better resource allocation.

\begin{figure*}[ht]
\centering
\begin{tabular}{ccc}
    \includegraphics[scale=0.43]{Figures/association_plot3.png} & 
    \includegraphics[scale=0.37]{Figures/learning_rate_plot.png} & 
    \includegraphics[scale=0.4]{Figures/VaryingGWs_Plot.png} \\
    \small (a) & 
    \small (b) & 
    \small (c) 
\end{tabular}
\caption{(a) 3D plane of LoRa ED-gateway association, (b) Training reward with different learning rates $\alpha$, (c) Training reward with different gateways.}
\vspace{-12pt}
\label{fig:training_optimization}
\end{figure*}

In Fig.~\ref{fig_algo_benchmark}, we compare the optimal EE versus the number of active EDs across multiple multi-agent RL benchmarks, including COMA, MAA2C, and VDN. Our proposed framework consistently achieves the highest global EE due to its CTDE paradigm, which enables efficient EE allocation across the network. Unlike COMA, which suffers from high variance and scalability issues, MAPPO ensures balanced reward distribution. On the other hand, MAA2C outperforms COMA and VDN, its cooperative learning limitations result in declining EE with an increasing number of EDs. Similarly, VDN exhibits the lowest EE due to its lack of explicit agent coordination. Additionally, with five fixed GWs, our approach maintains superior global system EE, whereas other algorithms struggle in denser networks due to coordination failures and communication overhead. It is important to note that as the number of EDs increases, the global EE naturally decreases. A higher number of EDs leads to a higher total TP consumption, increased interference, and reduced data rates, all of which contribute to lower EE. Despite these challenges, MAPPO mitigates this degradation more effectively than the other approaches. In comparison with the closest-performing algorithm (MAA2C), our approach improves the global system EE by 30.5\%, 7.2\%, 6.5\%, 10.9\%, and 11.0\% for 20, 30, 40, 50, and 60 EDs, respectively.

\begin{figure}[t]
  \centering % Centers the figure horizontally
  \includegraphics[width=0.8\linewidth]{Figures/EE_Benchmark_Plot.png} 
  \caption{Total energy efficiency under different number of end-devices}
  \vspace{-13pt}
  \label{fig_algo_benchmark} 
\end{figure}

\section{Conclusion} \label{Section_6}
In this paper, we considered the joint optimization of assigning spreading factor, transmission power, bandwidth, and ED associations between multiple EDs and GWs while considering resource constraints and the A2G propagation to optimize the global system EE in a multi-flying LoRa network. We showed that the resulting sequential decision-making problem can be modeled as a POMDP, and we proposed a model-free RL algorithm that leverages a novel MAPPO scheme. We used simulation to show that our proposed approach can learn a good policy approximation by optimizing the global system EE in LoRa networks. Interesting directions of future work include integrating trajectory optimization for multiple flying LoRa gateways for efficient resource management. Our approach could be extended by exploring the Age of Information (AoI) scheme to enhance the freshness of data collection. 

\section{Acknowledgement}
This work was sponsored by the Junior Faculty Development program under the UM6P-EPFL Excellence in Africa Initiative.

\balance
\bibliographystyle{IEEEtran}
\bibliography{biblio}

\end{document}
