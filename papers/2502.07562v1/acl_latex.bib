% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@inproceedings{llm,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@inproceedings{llm2,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{cvlora,
      title={Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA}, 
      author={James Seale Smith and Yen-Chang Hsu and Lingyu Zhang and Ting Hua and Zsolt Kira and Yilin Shen and Hongxia Jin},
      year={2024},
      eprint={2304.06027},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.06027}, 
}

@misc{cvlora2,
      title={LCM-LoRA: A Universal Stable-Diffusion Acceleration Module}, 
      author={Simian Luo and Yiqin Tan and Suraj Patil and Daniel Gu and Patrick von Platen and Apolinário Passos and Longbo Huang and Jian Li and Hang Zhao},
      year={2023},
      eprint={2311.05556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.05556}, 
}

@misc{diversity,
      title={Promoting Fairness and Diversity in Speech Datasets for Mental Health and Neurological Disorders Research}, 
      author={Eleonora Mancini and Ana Tanevska and Andrea Galassi and Alessio Galatolo and Federico Ruggeri and Paolo Torroni},
      year={2024},
      eprint={2406.04116},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.04116}, 
}

@misc{speechadapt1,
      title={AdaSpeech: Adaptive Text to Speech for Custom Voice}, 
      author={Mingjian Chen and Xu Tan and Bohan Li and Yanqing Liu and Tao Qin and Sheng Zhao and Tie-Yan Liu},
      year={2021},
      eprint={2103.00993},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2103.00993}, 
}

@misc{speechadapt2,
      title={Sample Efficient Adaptive Text-to-Speech}, 
      author={Yutian Chen and Yannis Assael and Brendan Shillingford and David Budden and Scott Reed and Heiga Zen and Quan Wang and Luis C. Cobo and Andrew Trask and Ben Laurie and Caglar Gulcehre and Aäron van den Oord and Oriol Vinyals and Nando de Freitas},
      year={2019},
      eprint={1809.10460},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.10460}, 
}

@inproceedings{voicebox,
 author = {Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and Hsu, Wei-Ning},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {14005--14034},
 publisher = {Curran Associates, Inc.},
 title = {Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/2d8911db9ecedf866015091b28946e15-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{valle,
      title={Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers}, 
      author={Chengyi Wang and Sanyuan Chen and Yu Wu and Ziqiang Zhang and Long Zhou and Shujie Liu and Zhuo Chen and Yanqing Liu and Huaming Wang and Jinyu Li and Lei He and Sheng Zhao and Furu Wei},
      year={2023},
      eprint={2301.02111},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.02111}, 
}

@misc{xtts,
      title={XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model}, 
      author={Edresson Casanova and Kelly Davis and Eren Gölge and Görkem Göknar and Iulian Gulea and Logan Hart and Aya Aljafari and Joshua Meyer and Reuben Morais and Samuel Olayemi and Julian Weber},
      year={2024},
      eprint={2406.04904},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2406.04904}, 
}

@misc{voiceboxadapt,
      title={Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning}, 
      author={Chung-Ming Chien and Andros Tjandra and Apoorv Vyas and Matt Le and Bowen Shi and Wei-Ning Hsu},
      year={2024},
      eprint={2406.06251},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2406.06251}, 
}

@InProceedings{librilight,
  author       = "Kahn, J. and Rivière, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazaré, P. E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and Likhomanenko, T. and Synnaeve, G. and Joulin, A. and Mohamed, A. and Dupoux, E.",
  booktitle    = "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
  title        = "Libri-Light: {A} Benchmark for {ASR} with Limited or No Supervision",
  year         = "2020",
  pages        = "7669--7673",
  keywords     = "Training;Voice activity detection;Benchmark testing;Task analysis;Tuning;Standards;Signal to noise ratio;unsupervised and semi-supervised learning;distant supervision;dataset;zero- and low resource ASR.",
  doi          = "10.1109/ICASSP40776.2020.9052942",
}

@InProceedings{voxpopuli,
  title        = "{V}ox{P}opuli: {A} Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",
  author       = "Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel",
  editor       = "Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto",
  booktitle    = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  year         = "2021",
  address      = "Online",
  publisher    = "Association for Computational Linguistics",
  URL          = "https://aclanthology.org/2021.acl-long.80",
  doi          = "10.18653/v1/2021.acl-long.80",
  pages        = "993--1003",
  abstract     = "We introduce VoxPopuli, a large-scale multilingual corpus providing 400K hours of unlabeled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 15 languages and their aligned oral interpretations into 15 target languages totaling 17.3K hours. We provide speech recognition (ASR) baselines and validate the versatility of VoxPopuli unlabeled data in semi-supervised ASR and speech-to-text translation under challenging out-of-domain settings. The corpus is available at \url{https://github.com/facebookresearch/voxpopuli}.",
}

@InProceedings{commonvoice,
  author       = "Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G.",
  title        = "Common Voice: {A} Massively-Multilingual Speech Corpus",
  booktitle    = "Proc. 12th Conference on Language Resources and Evaluation",
  pages        = "4211--4215",
  year         = "2020",
}

@InProceedings{mls,
  author       = "Vineel Pratap and Qiantong Xu and Anuroop Sriram and Gabriel Synnaeve and Ronan Collobert",
  title        = "{MLS}: {A} Large-Scale Multilingual Dataset for Speech Research",
  year         = "2020",
  booktitle    = "Proc. Interspeech 2020",
  pages        = "2757--2761",
  doi          = "10.21437/Interspeech.2020-2826",
  ISSN         = "2958-1796",
}

@Misc{vctk,
  title        = "{CSTR} {VCTK} Corpus: English Multi-speaker Corpus for {CSTR} Voice Cloning Toolkit",
  author       = "Christophe Veaux and Junichi Yamagishi and Kirsten MacDonald",
  year         = "2019",
  URL          = "https://datashare.ed.ac.uk/handle/10283/3443",
}

@InProceedings{voxceleb,
  author       = "Arsha Nagrani and Joon Son Chung and Andrew Zisserman",
  title        = "VoxCeleb: {A} Large-Scale Speaker Identification Dataset",
  year         = "2017",
  booktitle    = "Proc. Interspeech 2017",
  pages        = "2616--2620",
  doi          = "10.21437/Interspeech.2017-950",
  ISSN         = "2958-1796",
}

@InProceedings{voxceleb2,
  author       = "Joon Son Chung and Arsha Nagrani and Andrew Zisserman",
  title        = "VoxCeleb2: Deep Speaker Recognition",
  year         = "2018",
  booktitle    = "Proc. Interspeech 2018",
  pages        = "1086--1090",
  doi          = "10.21437/Interspeech.2018-1929",
  ISSN         = "2958-1796",
}

@InProceedings{voxlingua107,
  author       = "Valk, Jörgen and Alumäe, Tanel",
  booktitle    = "2021 IEEE Spoken Language Technology Workshop (SLT)",
  title        = "{VOXLINGUA107}: {A} Dataset for Spoken Language Recognition",
  year         = "2021",
  pages        = "652--658",
  keywords     = "Training;Training data;Speech recognition;Feature extraction;Data models;Task analysis;Videos;Spoken language recognition;web scraping;x-vectors;crowd-sourcing",
  doi          = "10.1109/SLT48900.2021.9383459",
}
@misc{clarin,
      title={Polish Read Speech Corpus for Speech Tools and Services}, 
      author={Danijel Koržinek and Krzysztof Marasek and Łukasz Brocki and Krzysztof Wołk},
      year={2017},
      eprint={1706.00245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.00245}, 
}
@InProceedings{fleurs,
  author       = "Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur",
  booktitle    = "2022 IEEE Spoken Language Technology Workshop (SLT)",
  title        = "{FLEURS}: {FEW}-Shot Learning Evaluation of Universal Representations of Speech",
  year         = "2023",
  pages        = "798--805",
  keywords     = "Conferences;Buildings;Speech recognition;Benchmark testing;Machine translation;Task analysis;Automatic speech recognition;Massively Multilingual Speech Recognition;Low-Resource Language Dataset;Speech Language Identification;Speech Information Retrieval;Few-/Zero- Shot Learning",
  doi          = "10.1109/SLT54892.2023.10023141",
}
@inproceedings{nemo,
    title = "n{EMO}: Dataset of Emotional Speech in {P}olish",
    author = "Christop, Iwona",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1059",
    pages = "12111--12116",
    abstract = "Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of emotional speech in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).",
}
@masterthesis{mcspeech,
  title={Analiza porównawcza korpusów nagrań mowy dla celów syntezy mowy w języku polskim},
  author={Czyżnikiewicz, Mateusz},
  year={2022},
  month={December},
  school={Warsaw University of Technology},
  type={Master's thesis},
  doi={10.13140/RG.2.2.26293.24800},
  note={Available at \url{http://dx.doi.org/10.13140/RG.2.2.26293.24800}},
}

@misc{mailabs,
  author       = {Imdat Solak},
  title        = {The M-AILABS Speech Dataset},
  year         = {2019},
}

@misc{wikimedia,
      title={Speech Wikimedia: A 77 Language Multilingual Speech Dataset}, 
      author={Rafael Mosquera Gómez and Julián Eusse and Juan Ciro and Daniel Galvez and Ryan Hileman and Kurt Bollacker and David Kanter},
      year={2023},
      eprint={2308.15710},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.15710}, 
}

@inproceedings{whisper,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@Misc{flowmatching,
  title        = "Flow Matching for Generative Modeling",
  author       = "Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matt Le",
  year         = "2023",
  eprint       = "2210.02747",
  archiveprefix = "arXiv",
  primaryclass = "cs.LG",
  URL          = "https://arxiv.org/abs/2210.02747",
}


@InProceedings{transformer,
  author       = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia",
  booktitle    = "Advances in Neural Information Processing Systems",
  editor       = "I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
  pages        = "",
  publisher    = "Curran Associates, Inc.",
  title        = "Attention is All you Need",
  URL          = "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
  volume       = "30",
  year         = "2017",
}

@Misc{alibi,
  title        = "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
  author       = "Ofir Press and Noah A. Smith and Mike Lewis",
  year         = "2022",
  eprint       = "2108.12409",
  archiveprefix = "arXiv",
  primaryclass = "cs.CL",
}

@InProceedings{hifigan,
  author       = "Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung",
  title        = "HiFi-{GAN}: generative adversarial networks for efficient and high fidelity speech synthesis",
  year         = "2020",
  ISBN         = "978-1-71382-954-6",
  publisher    = "Curran Associates Inc.",
  address      = "Red Hook, NY, USA",
  abstract     = "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
  booktitle    = "Proceedings of the 34th International Conference on Neural Information Processing Systems",
  articleno    = "1428",
  numpages     = "12",
  location     = "Vancouver, BC, Canada",
  series       = "NIPS'20",
}

@misc{audiobox,
      title={Audiobox: Unified Audio Generation with Natural Language Prompts}, 
      author={Apoorv Vyas and Bowen Shi and Matthew Le and Andros Tjandra and Yi-Chiao Wu and Baishan Guo and Jiemin Zhang and Xinyue Zhang and Robert Adkins and William Ngan and Jeff Wang and Ivan Cruz and Bapi Akula and Akinniyi Akinyemi and Brian Ellis and Rashel Moritz and Yael Yungster and Alice Rakotoarison and Liang Tan and Chris Summers and Carleigh Wood and Joshua Lane and Mary Williamson and Wei-Ning Hsu},
      year={2023},
      eprint={2312.15821},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2312.15821}, 
}

@misc{speechflow,
      title={Generative Pre-training for Speech with Flow Matching}, 
      author={Alexander H. Liu and Matt Le and Apoorv Vyas and Bowen Shi and Andros Tjandra and Wei-Ning Hsu},
      year={2024},
      eprint={2310.16338},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2310.16338}, 
}

@InProceedings{bestrq,
  title        = "Self-supervised learning with random-projection quantizer for speech recognition",
  author       = "Chiu, Chung-Cheng and Qin, James and Zhang, Yu and Yu, Jiahui and Wu, Yonghui",
  booktitle    = "Proceedings of the 39th International Conference on Machine Learning",
  pages        = "3915--3924",
  year         = "2022",
  editor       = "Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan",
  volume       = "162",
  series       = "Proceedings of Machine Learning Research",
  month        = "17--23 Jul",
  publisher    = "PMLR",
  pdf          = "https://proceedings.mlr.press/v162/chiu22a/chiu22a.pdf",
  url          = "https://proceedings.mlr.press/v162/chiu22a.html",
  abstract     = "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook are updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates than previous work with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.",
}

@INPROCEEDINGS{titanet,
  author={Koluguri, Nithin Rao and Park, Taejin and Ginsburg, Boris},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={TitaNet: Neural Model for Speaker Representation with 1D Depth-Wise Separable Convolutions and Global Context}, 
  year={2022},
  volume={},
  number={},
  pages={8102-8106},
  keywords={Representation learning;Convolution;Error analysis;Conferences;Neural networks;Acoustics;Task analysis;speaker verification;speaker embedding;t-vectors;context;diarization},
  doi={10.1109/ICASSP43922.2022.9746806}
}

@INPROCEEDINGS{speechmos,
  author={Reddy, Chandan K A and Gopal, Vishak and Cutler, Ross},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Dnsmos P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors}, 
  year={2022},
  volume={},
  number={},
  pages={886-890},
  keywords={Measurement;Correlation;Conferences;Noise reduction;Signal processing;Acoustics;Background noise;Speech;Perceptual Speech Quality;Objective Metric;Deep Noise Suppressor;Metric;P.835},
  doi={10.1109/ICASSP43922.2022.9746108}
}

@article{multi-lora,
  title={Punica: Multi-tenant lora serving},
  author={Chen, Lequn and Ye, Zihao and Wu, Yongji and Zhuo, Danyang and Ceze, Luis and Krishnamurthy, Arvind},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={1--13},
  year={2024}
}

@inproceedings{tlsvmttss,
 author = {Jia, Ye and Zhang, Yu and Weiss, Ron and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, zhifeng and Nguyen, Patrick and Pang, Ruoming and Lopez Moreno, Ignacio and Wu, Yonghui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf},
 volume = {31},
 year = {2018}
}
