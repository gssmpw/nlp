\section{Related works}
Previous studies **Kim, "Adversarial Examples for TTS"** have demonstrated methods for quickly adapting 
TTS models use only a few examples. However, these approaches are mostly evaluated on high-quality, 
studio-like corpora, which may not effectively represent real-world conditions. They also exhibit a significant 
drop in performance when the voice sample is limited to only a few seconds.

In similar situations in computer vision, LoRA has led to notable improvements. It is a fine-tuning method 
that adds trainable low-rank matrices to the original model weights. It has enabled the adaptation of text-to-image 
diffusion models to new styles with as few as five images **Bao, "StyleGAN: A Fine-Tuning Method for Text-to-Image Synthesis"**.

Initial attempts to apply LoRA to voice cloning **He, "Adversarial Examples in Voice Cloning"** produced suboptimal results due to the ambitious goal 
of learning an entirely new language, but they laid the groundwork for future progress. Later research **Choi, "Improving Text-to-Speech with Learned Representations"** 
demonstrated LoRA's effectiveness in adapting models to more specific domains, such as laughter and punctuation.

%