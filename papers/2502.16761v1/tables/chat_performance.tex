
\begin{table}[ht]
    \centering
    \scriptsize
    \caption{
    Performance of the fine-tuned Llama-2-7B-chat model (Chat LLM FT).
    For comparison, we also present lower and upper bounds, the baseline method Zero-shot prompt (\texttt{QA}) and our fine-tuned Llama-2-7B (Base LLM FT).
    }
    \label{table:chat_performance}
    \begin{tabular}{l|c|c}
    \toprule
    Method & \textbf{OpinionQA} & \textbf{\OURDATA-Eval} \\
    \midrule
    Upper bound (Unif.) & 0.178 & 0.208 \\
    Lower bound (Human) & 0.031 & 0.033 \\
    \midrule
    Base zero-shot prompt (\texttt{QA}) & 0.173 & 0.206 \\
    Base LLM FT & 0.106 & 0.121  \\
    Chat zero-shot prompt (\texttt{QA}) & 0.308 & 0.383 \\
    Chat LLM FT & 0.109 & 0.148  \\
    \bottomrule
    \end{tabular}
\vspace{5pt}
\end{table}
