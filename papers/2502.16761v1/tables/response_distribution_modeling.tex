\begin{table}[b]
    \centering
    \scriptsize
    \captionsetup{font=small}
    \caption{
    Comparison of evaluation performance for three response distribution modeling approaches,
    with Llama-2-7B as a base model.
    The last column (Explicit) is identical to the ours presented in \Cref{table:main_results}.
    A model fine-tuned to predict the most probable choice (one-hot) performs the worst,
    as the model has not learned distributional opinion at fine-tuning phase.
    A model trained on augmented data (Aug. ($\times$50, $\times$100)),
    while performing much better than one-hot still underperforms the explicit distribution modeling.
    }
    \label{table:response_distribution_modeling}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{c | cccc}
    \toprule
    Eval Dataset & 
    One-hot & 
    Aug. ($\times$ 50)  &
    Aug. ($\times$ 100) &
    Explicit (Ours) \\
    \midrule
    OpinionQA
    & 0.163 & 0.110 & 0.107 & 0.106 \\
    \midrule
    \OURDATA-Eval
    & 0.178 & 0.130 & 0.123 & 0.121 \\
    \bottomrule
    \end{tabular}
    }
\end{table}