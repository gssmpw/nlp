\section{Additional Experiments}
\subsection{Effect of Response Distribution Modeling}
\label{section_experiments_modeling_response_distribution}

In this section, we compare different methods for capturing the distribution of human responses. We consider three approaches:
\begin{enumerate}
    \item \emph{One-hot}: Predicting only the most probable response, which ignores the full distribution over all responses~\cite{li2024culturellm}.
    \item \emph{Augment by N}: Augmenting the dataset by replicating each response by a factor of N according to its observed frequency~\cite{zhao2023group}.
    \item \emph{Explicit probability modeling}: Directly modeling the full response distribution using the actual probability values for each option.
\end{enumerate}

\Cref{table:response_distribution_modeling} summarizes the results of these approaches. Notably, explicit probability modeling substantially outperforms the one-hot method, demonstrating that simply predicting the single most frequent response fails to capture the opinion diversity present within each subpopulation.

Compared with augment by \(N\) (2nd and 3rd column in \Cref{table:response_distribution_modeling}), explicit probability modeling also achieves better performance. 
Importantly, the performance gap exceeds the quantization error introduced by discretizing the response distribution. 
For instance, when discretizing with a factor of \(N\), the quantization error is \(\frac{1}{2N}\)—approximately 0.01 or 0.005 in the cases shown in \Cref{table:response_distribution_modeling}. 
Moreover, explicit modeling offers the practical benefit of reducing the data volume by a factor of \(N\) compared to the augmentation approach, thereby lowering the computational cost of fine-tuning LLMs.

These results underscore the importance of explicit distribution modeling. 
By aligning the model’s predictive distribution directly with the survey distribution, we achieve higher accuracy with fewer data samples, avoiding the rounding errors and replication overheads that are inherent to data-augmentation approaches.

\input{tables/response_distribution_modeling}


\subsection{Post-trained Model}
\label{appendix_chat}
We fine-tune Llama-2-7B-chat to observe the effect of starting 
from checkpoints that have been instruction-tuned via Reinforcement Learning from Human Feedback (RLHF).
\Cref{table:chat_performance} shows the evaluation performance of a baseline method (Zero-shot prompt (\texttt{QA})), fine-tuned base model (Llama-2-7B) and fine-tuned chat model (Llama-2-7B-chat).
We observe the significant performance improvement,
while the baseline method performs worse then the models not instruction-tuned (\Cref{table:main_results}).
Especially, the performance for \OURDATA-Eval of chat model is significantly worse than that of base model.
We observe the high WD of the baseline method resulting from the model assigning high probability to a specific token (e.g. `A'),
being far apart from the human opinion distribution.
After fine-tuning the model are able to generate a more distributed probability of answer tokens.
This result coincides with the result reported in~\cite{moon-etal-2024-virtual}.

\input{tables/chat_performance}

\subsection{Generalization to Unseen Subpopulations}
\label{appendix_unseen_subpopulation}
Here we present a complete list of evaluation performance on OpinionQA
for unseen subpopulations (the groups not used to fine-tune our model)
and perform an analysis that shows
our fine-tuned models are able to steer towards the given subpopulation information.

As shown in \Cref{table:unseen_demographic_all}, we observe a performance improvement across unseen subpopulations.
To verify that the performance improvements arise from the fine-tuned model being able to steer towards given subpopulations, we measure \textit{intergroup disagreement} pattern for the demographic and ideology traits, shown in
\Cref{fig:age_heatmap},
\ref{fig:polparty_heatmap},
\ref{fig:race_heatmap},
and \ref{fig:polideology_heatmap}.
We consistently observe across traits that the disagreement pattern of our model resembles that of the human group, while zero-shot prompting with the base model exhibits a pattern completely different from the human group result.
This observation shows that our fine-tuned model learns to condition on subpopulation information and also generalizes to subpopulations unseen during fine-tuning.

\input{tables/unseen_demographics_all}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \captionsetup{font=small}
    \includegraphics[width=\linewidth]{figures/figure_heatmap_age.pdf}
    \caption{
    Heatmap of intergroup disagreement between a target human group ($y$-axis)
    and a source group ($x$-axis, either a human group or a group simulated with the language model),
    for OpinionQA evaluation data and age trait using Llama-2-7B as a base model.
    All subpopulations are unseen during fine-tuning.
    }
    \label{fig:age_heatmap}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \captionsetup{font=small}
    \includegraphics[width=\linewidth]{figures/figure_heatmap_polparty.pdf}
    \caption{
    Heatmap of intergroup disagreement between a target human group ($y$-axis)
    and a source group ($x$-axis, either a human group or a group simulated with the language model),
    for OpinionQA evaluation data and political party (affiliation) trait using Llama-2-7B as a base model.
    Two subpopulations, Democrat and Republican, are seen during fine-tuning, while Independent and Something Else are unseen.
    \vspace{20pt}
    }
    \label{fig:polparty_heatmap}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \captionsetup{font=small}
    \includegraphics[width=\linewidth]{figures/figure_heatmap_race.pdf}
    \caption{
    Heatmap of intergroup disagreement between a target human group ($y$-axis)
    and a source group ($x$-axis, either a human group or a group simulated with the language model),
    for OpinionQA evaluation data and race / ethnicity trait using Llama-2-7B as a base model.
    Four subpopulations except `Other' are seen during fine-tuning.
    In this case, the model does not well predict the opinions of Other group.
    We suspect this occurs because Other is a group with highly diverse race or ethnicity backgrounds,
    making it inherently difficult to infer its opinion distribution from those of White, Hispanic, Black, and Asian subpopulations.
    }
    \label{fig:race_heatmap}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \captionsetup{font=small}
    \includegraphics[width=\linewidth]{figures/figure_heatmap_polideo.pdf}
    \caption{
    Heatmap of intergroup disagreement between a target human group ($y$-axis)
    and a source group ($x$-axis, either a human group or a group simulated with the language model),
    for OpinionQA evaluation data and political ideology trait using Llama-2-7B as a base model.
    Three subpopulations, Conservative, Moderate, and Liberal are seen during fine-tuning, while Very conservative and Very liberal are not seen.
    }
    \label{fig:polideology_heatmap}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%