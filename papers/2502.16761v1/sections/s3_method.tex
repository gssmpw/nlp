%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
    \centering
    \includegraphics[width=1.00\linewidth]{figures/main_figure_finetuning.pdf}
    \caption{
    Proposed supervised fine-tuning setup with a survey response dataset such as \OURDATA.
    Survey data is 3-tuple of a survey question, target subpopulation information, and the observed human opinion distribution (\textit{i.e.} how subjects in the group responded to the given question).
    The training objective, $\mathcal{L}(\theta)$, is a forward KL divergence loss on language model predicted distribution of question option likelihoods; our loss guides the model predictions to match the response distribution of the specified human subpopulation.
    }
    \label{fig:method}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}
\label{section_method}

\subsection{Fine-tuning LLMs on Human Response Distributions}
\label{section_method_fine_tuning_objective}
Our goal is to fine-tune an LLM to predict the distribution of responses for a multiple-choice question, conditioned on descriptions of a human subpopulation we want to simulate, typically a specific demographic, socioeconomic, or ideological subgroup. 
Consider the example in \Cref{fig:method}: the question asks, “What do you think the chances are these days that a woman won't get a job or promotion while an equally or less qualified man gets one instead?” The available responses are: \emph{A. Very likely, B. Somewhat likely, C. Not very likely, D. Very unlikely, and E. Refused}. 
In this case, the LLM will output a probability for each of the tokens corresponding to the choices A through E, thereby generating a complete response distribution that we aim to align with the true distribution observed in survey data.

Formally, let \(q \in Q\) be a question, \(g \in G\) be a subpopulation, and \(\mathcal{A}_q\) be the set of possible choices for question \(q\). 
An LLM with parameters \(\theta\) produces a conditional probability distribution \(p_{\theta}(\mathcal{A}_q \mid q, g)\). 
We fine-tune this model so that its predicted distribution for each \((q, g)\) mirrors the human response distribution \(p_H(\mathcal{A}_q \mid q, g)\) collected from real survey data.
To accomplish this, we apply LoRA fine-tuning~\cite{hu2021lora} and use the forward Kullback--Leibler (KL) divergence as our loss. 
Concretely, if \(p_H(\mathcal{A}_q \mid q, g)\)
represents the group-level empirical distribution of human opinions and \(p_{\theta}(\mathcal{A}_q \mid q, g)\) represents the model’s predicted distribution, our training objective is:
\[
\textstyle
\mathcal{L}(\theta) 
= \mathbb{E}_{q,g} \Bigl[
  D_{\mathrm{KL}}\bigl(
    p_H(\mathcal{A}_q \mid q, g) \,\big\|\, p_{\theta}(\mathcal{A}_q \mid q, g)
  \bigr)
\Bigr],
\]
where \(D_{\mathrm{KL}}\) denotes the KL divergence. In the example shown in \Cref{fig:method}, the model is trained to reduce the KL divergence between the target (survey-based) distribution over \{\(A, B, C, D, E\)\} and its predicted distribution for the subpopulation living in the Southern United States.

We choose forward KL (i.e., \(\mathrm{KL}\bigl(p_H\mid\mid p_\theta\bigr)\)) since it is sensitive to cases where \(p_H\) assigns high probability but \(p_\theta\) does not, naturally encouraging the model to \emph{cover} the real distribution.
This property aligns with standard maximum-likelihood training, where the model is penalized for underestimating any response that is frequent in the data.
In other words, if many participants in group \(g\) choose option ``A'' for question \(q\), then the model probability on ``A'' should be correspondingly high.

Instead of explicitly modeling the group response distribution as \(p_H(\mathcal{A}_q | q, g)\),
one could do two alternatives.
(1) One-hot encoding:
this approach \cite{li2024culturellm} approximates the distribution by a one-hot vector,
assigning a value of one to the most probable option and zero elsewhere.
(2) Data augmentation by response frequency:
this approach \cite{zhao2023group} expands the dataset by replicating
question-choice pairs in proportion to their observed frequency.
We adopt the explicit distribution modeling in our main experiments because it directly encodes the distributional information without requiring discrete sampling or replicating data points.
A detailed comparison of these approaches is provided in Section~\ref{section_experiments_modeling_response_distribution}.

\subsection{\OURDATA: a Comprehensive Survey Dataset to Fine-tune and Evaluate LLMs}
\label{section_method_datasets}
OpinionQA~\cite{santurkar2023whose} is a widely used dataset for fine-tuning and evaluating large language models (LLMs) on opinion prediction, containing roughly 500 questions drawn from 14 American Trends Panel (ATP) waves~\cite{atp}. 
Although valuable, it faces two important limitations:
(1) Limited thematic diversity—for instance, wave 26 focuses on the topic of firearms.
(2) Reliance on a single survey family (ATP), which risks overfitting to a particular style of questions and limits out-of-distribution evaluation on other sources (e.g., GSS).

To address these limitations, we introduce a new dataset, \OURDATA, that broadens both the thematic and institutional scope of opinion prediction data. 
For training, \OURDATA comprises 3,229 multiple-choice questions drawn from ATP waves 61--132, excluding 
waves included in OpinionQA.
In Table \ref{table:subpop-train-detail}, we list the topics of the ATP waves in \OURDATA vs. OpinionQA, both showing the increased thematic diversity of \OURDATA (with over 20 new topics) and the remaining unseen topics in OpinionQA that allow us to test whether LLMs fine-tuned on \OURDATA can generalize to unseen topics.

For evaluation, \OURDATA also includes 133 multiple-choice questions from the General Social Survey (GSS)~\cite{davern2024gss}, serving as an out-of-distribution benchmark.
This expanded collection not only broadens the range of topics beyond OpinionQA’s initial 500 questions, but also enables evaluation on surveys created and administered by different institutions (Pew Research Center vs. NORC-Chicago). 
Dataset curation and refinement pipeline is available in Appendix~\ref{appendix_dataset}.

\subsection{Evaluation Metric}
\label{section_method_evaluation_metric}
We use Wasserstein distance (WD) to quantify how closely the model’s predicted opinion distribution matches human survey data~\cite{santurkar2023whose, moon-etal-2024-virtual, meister2024benchmarking, zhao2023group}.
Formally, for a group $g$ representing some subpopulation and a question $q$
WD is defined as $\mathcal{WD}_{\theta}(q,g) = \mathcal{WD}(p_H(\mathcal{A}_q|q,g) , p_{\theta}(\mathcal{A}_q|q,g))$ (see formula in \Cref{appendix_training_train_objective}).
Since WD is computed over ordinal values, we map the categorical answer options to numbers, such as mapping ``Very likely'' to 1, ``Likely'' to 2, and so on. 

Some prior work utilizes one-hot accuracy~\cite{feng-etal-2024-modular, li2023steerability} as an evaluation metric.
However, 
one-hot accuracy only verifies whether the top-predicted choice matches the top human response,
thereby discarding distributional information.
In contrast, WD accounts for partial overlaps among the categories and reflects the ‘cost’ of shifting probability mass, providing a more nuanced assessment of distribution discrepancy.
Consider the example question provided in \Cref{fig:method}, where the human response distribution indicates that option B (“Somewhat likely”) is the most probable. 
Now consider two cases in which the model incorrectly predicts the top choice. 
In the first case, the model assigns a high probability to option A (``Very likely''), while in the second case, it assigns a high probability to option D (``Very unlikely''). 
Although one-hot accuracy would treat both predictions equally as errors, WD differentiates between them by accounting for the ordinal relationship among the options, penalizing the second prediction more heavily for its larger deviation from the true distribution.