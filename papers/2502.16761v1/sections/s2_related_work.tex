\section{Related Work}
\vspace{-5pt}
\paragraph{Public opinion datasets.}
Several research institutions conduct large-scale public opinion polls and release data from those surveys.
Important examples include Pew Research Center's American Trends Panel (ATP), which consists of multiple waves of cross-sectional surveys on different topics~\cite{atp}, and General Social Survey (GSS) from the NORC at the University of Chicago~\cite{davern2024gss}.
Existing datasets have curated such data for evaluating LLM-based opinion predictions, including OpinionQA~\cite{santurkar2023whose}, a subset of ATP survey waves containing about 500 questions on contentious social topics.
While OpinionQA is widely used in prior work~\cite{he2024community, zhao2023group, li2023steerability, li2024culturellm}, we find its total number of questions limited in scale for fine-tuning LLMs and instead use this dataset for evaluation.
We further collect an extended set of survey data from ATP waves not included in OpinionQA, as well as from GSS to curate \OURDATA.
Other datasets, such as GlobalOpinionQA \cite{durmus2023towards}---derived from the World Values Survey (WVS) \cite{wvs2022}
and the Pew Global Attitudes Survey \citep{pewresearch2024}---and the PRISM dataset~\cite{kirk2024prism} 
investigates how language models align with opinions from populations across the globe and different cultures.

\vspace{-5pt}
\paragraph{Predicting human opinions with LLMs.}
Prior work has explored various prompt engineering approaches for steering LLM responses: earlier work use rule-based prompts that incorporate demographic profiles of individuals or populations, or few-shot examples of survey question-response \cite{hwang2023aligning, simmons2022moral, santurkar2023whose, dominguez2023questioning}. Recent work explore prompting LLMs with open-ended text, including interview transcripts \cite{park2024generative}, personal narratives \cite{moon-etal-2024-virtual}, or LLM-refined prompts \cite{kim2024few, sun2024persona}.
Our fine-tuning approach is complementary to prompt engineering methods: while prompt engineering seeks to optimize what information is provided to the LLM (while the model is frozen), fine-tuning seeks to optimize how the model utilizes the provided information (while the prompt is frozen).
In this work, we demonstrate that our fine-tuned models exhibit significant improvements in matching the response distributions of humans without requiring elaborate prompt engineering methods.

Other work \cite{chu2023language, he2024community,feng-etal-2024-modular} fine-tune language models on text corpora from specific communities (\textit{e.g.}, Reddit) to infer the most popular response or response distribution for a given survey question. 
While this approach benefits from large-scale and continuously updated text corpora, it struggles with disproportionate representation online and lacks comprehensive coverage of diverse subpopulations. 
A few works have explored directly fine-tuning on public opinion survey data, but in different problem settings from ours.
\citet{li2023steerability} apply collaborative filtering to individual-level responses to learn embeddings for individuals, and 
\citet{zhao2023group} develop a meta-learning framework to predict the opinions of new groups given a small number of in-context examples for that group.
In contrast, our approach does not require individual-level responses and can generalize to unseen groups and survey questions without \textit{any} responses.

A recent work \cite{li2024culturellm} and a work concurrent to ours \cite{cao2025specializing} also explored fine-tuning LLMs on the World Values Survey (WVS) to align the LLM's opinion response with a culture or entire country populations.
In comparison, our work focuses on US surveys, testing whether LLMs can align with finer-grained subpopulations within one country and whether LLMs fine-tuned on one US-representative survey can generalize to another.
However, we note that our proposed method for fine-tuning language models applies to any survey dataset with distributional information about subpopulation responses.

\vspace{-5pt}
\paragraph{Pluralistic alignment of LLMs.}
Recent literature on pluralistic and distributional alignment target a similar yet different problem in fine-tuning LLMs~\cite{chakrabortymaxmin,melnyk2024distributional,poddar2024personalizing,siththaranjan2023distributional,yao2024no,sorensen2024roadmap,lake2024distributional,chen2024pal,jiang2024can}.
While this line of work shares a similar goal as ours in training models to reflect on opinions (and preferences) of diverse subpopulations, most work differ from ours in that they operate in the context of training against \textit{pair-wise} preference orderings between alternative language model completions, extending the Bradley-Terry-Luce model~\cite{rajkumar2014statistical, ouyang2022training, rafailov2024direct} or investigating alternative models to account for diverging preference orderings across populations.
In contrast, our work trains the model to directly predict the opinion distributions of human subpopulations, where accurately matching distributions across a large variety of subpopulations is of paramount interest.
Our work additionally focuses on the particular context of estimating human opinions about societal issues---the objective of public opinion research---which enables relatively straightforward supervised training on openly available, structured survey data as presented by \OURDATA.