\section{Dataset Details}
\label{appendix_dataset}

\subsection{American Trends Panel Datasets}
\label{appendix_atp_dataset}
Pew Research holds regular American Trends Panel (ATP) survey (called waves)~\cite{atp} covering various topics (\textit{e.g.} veterans, political priorities, gender and leadership) and releases result at an individual level.
For each anonymized individual, the following information is released: unique identification number, demographic details, survey responses, and weight.
Weights~\cite{mercer2018weighting} are the output of post-survey calibration process that helps adjusting survey results for response bias (e.g., non-response bias, sampling bias) correction and population representativeness.
As of January 2025, survey data until wave 132 has been released. About 20 surveys are conducted in each year.

\subsection{OpinionQA}
\label{appendix_opinionqa}
OpinionQA is a subset of ATP curated in \cite{santurkar2023whose}. This dataset consists of contentious 500 questions sampled from 14 ATP waves which have high intergroup disagreement (i.e. large Wasserstein distances among subpopulations' responses to a question). It also comes with hand-crafted ordinality information which provides structure to option lists. For example, options `Major reason', `Minor reason', and `Not a reason',
are assigned an ordinality mapping to 1, 2, and 3, respectively.
This ordinality allows a calculation of 1-dimensional Wasserstein distance.

Subpopulations we employ are listed in \Cref{table:opinionqa_detail}. This set of groups is adopted for several small-scale analysis \cite{santurkar2023whose, zhao2023group, kim2024few}.
We note that our approach is not limited to a specific number of groups and data is available for small or fine-grained demographic subpopulations.

\input{tables/opinionqa_detail}
\newpage
\input{tables/subpop-train-detail}

\subsection{\OURDATA-Train}
\label{appendix_ourdata_atp}
We gather additional data from the American Trends Panel, specifically collecting 53 waves from Wave 61 to 132.
There are 62 waves from Wave 61 - 132, however, some waves have missing demographic or ideology information (for example, wave 63 does not contain political ideology information) or the data is not available hence removed during the curation process.
To refine the dataset, we exclude questions that meet the following criteria:
those with more than 10 response options, redacted response data, or dependencies on prior questions (e.g., assessing political strength). 
For the remaining questions, we use GPT-4o to refine their wording, ensuring they are well-suited for prompting the language models while making minimal modifications.
In \Cref{fig:question_refine_ice} we provide a few-shot prompt for question refinement.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]
    \captionsetup{font=small}
    \includegraphics[width=\linewidth]{figures/appendix_refine_ice.pdf}
    \vspace{-15pt}
    \caption{
    Few-shot prompt for refining the question to suit a language model prompting. An instruction is designed to make a minimal change to the original question, and in-context examples are provided.
    }
    \label{fig:question_refine_ice}
    \vspace{-5pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \captionsetup{font=small}
    \includegraphics[width=0.8\linewidth]{figures/appendix_tsne.pdf}
    \caption{Embeddings of questions from OpinionQA, \OURDATA-Train, and \OURDATA-Eval.}
    \label{fig:question-embs}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In Figure \ref{fig:question-embs}, we visualize the embeddings of the question texts (projected to 2-dimensions using t-SNE) from OpinionQA compared to \OURDATA-Train and \OURDATA-Eval.
The visualization shows how much larger our dataset is than OpinionQA (6.5$\times$), along with the expanded coverage of our dataset into semantic areas untouched by OpinionQA. 
The embeddings also reveal the distribution shift from ATP questions to GSS questions: while the ATP and GSS questions overlap in embedding space, the GSS question appear as small clusters, not evenly distributed over the ATP questions. 
In Table \ref{table:subpop-train-detail}, we list each ATP wave in \OURDATA-Train and OpinionQA, along with its number of questions and wave topic(s), as defined by ATP.\footnote{ATP wave topics and time periods are defined at \url{https://www.pewresearch.org/american-trends-panel-datasets/}.} 
The table indicates which topics are new in \OURDATA-Train compared to OpinionQA, indicating the expanded coverage of our dataset, along with which topics remain unseen in OpinionQA, which we can use to test LLMs fine-tuned on \OURDATA-Train for generalization.

\subsection{\OURDATA-Eval}
\label{appendix_outdata_gss}
To further evaluate the out-of-distribution generalization ability of our fine-tuned models, we subsample 133 questions from the GSS 2022 dataset \cite{davern2024gss}.
We apply the same selection criteria as outlined in ~\Cref{appendix_ourdata_atp}, excluding questions that are redacted, conditioned on prior questions, inferable directly from the group information, derived from a set of questions, or those with more than 10 response options.

\subsection{Inspection of Identical Questions}
Distribution of cosine similarities between two text embeddings (an output of the embedding model OpenAI-text-embedding-3-large given a question text), one from a question in \OURDATA-Train and another from OpinionQA is shown in Figure~\ref{fig:cosine_sim_distribution}.
We observed a fraction of pairs having high cosine similarity,
and manually inspected question pairs with high relevance. We find that by setting a threshold cosine similarity of 0.87 we can detect all semantically identical pairs.
We took a conservative threshold of cosine similarity; this value was to maximize the recall at a cost of precision to ensure detection of overlapping questions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
    \captionsetup{font=small}
    \includegraphics[width=\linewidth]{figures/appendix_cosine_dist.pdf}
    \vspace{-15pt}
    \caption{
    Distribution of cosine similarities between a question in \OURDATA-ATP and OpinionQA, having a long tail towards a high cosine similarity.
    We inspect the question pairs in the range of 0.8 to 1.0 (distribution shown in the magnified view) and use a similarity of 0.87 as a safe threshold to identify a semantically identical question pair.
    }
    \label{fig:cosine_sim_distribution}
    \vspace{-5pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%