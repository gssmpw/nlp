\section{Experiment Details}
\label{appendix_training}
We conduct our experiments using Nvidia A100 GPUs with 80GB VRAM.
Hyperparameter tuning is performed over learning rates \{5e-5, 1e-4, 2e-4\}
and batch sizes \{64, 128, 256\}.
After evaluating possible combinations,
we select a (learning rate, batch size) = (2e-4, 256) for Llama-2-7B,
(learning rate, batch size) = (2e-4, 256) for Mistral-7B-v0.1,
and (learning rate, batch size) = (1e-4, 256) for Llama-2-13B
when utilizing the full training dataset.
For Llama-3-70B, we have not done hyperparameter search but heuristically used
(learning rate, batch size) = (2e-5, 256).
For sub-sampled training data (Figure \ref{fig:scaling}), we use the following configurations:
\begin{itemize}[leftmargin=*]
    \vspace{-7pt}
    \item (lr, bs) = (2e-4, 256) for 75\% of the training data
    \vspace{-7pt}
    \item (lr, bs) = (1e-4, 128) for 50\% of the training data
    \vspace{-7pt}
    \item (lr, bs) = (1e-4, 128) for 25\% of the training data
\end{itemize}

All training is performed using LoRA \cite{hu2021lora}, with LoRA parameters initialized from a normal distribution with $\sigma=0.02$. We set the LoRA rank to 8, alpha to 32, and apply a dropout rate of 0.05. LoRA weights are applied to the query and value matrices. The AdamW \cite{loshchilov2017decoupled} optimizer is used with a weight decay of 0.

We use offline batched inference of vLLM (version 0.7.2)~\cite{kwon2023efficient} for inference and measuring response probability distribution of all methods.

\paragraph{Choice of the Training Objective.}
\label{appendix_training_train_objective}
In this section, we explore both forward KL-divergence and Wasserstein Distance (WD) as training objectives. The forward KL-divergence is defined as
\[
D_{\mathrm{KL}}(p_H \| p_{\theta}) = \sum_{a \in \mathcal{A}_q} p_H(a) \log \frac{p_H(a)}{p_{\theta}(a)},
\]
where \(p_H(a) \equiv p_H(a \mid q, g)\) and \(p_{\theta}(a) \equiv p_{\theta}(a \mid q, g)\). Similarly, WD is given by
\[
\mathcal{WD}(p_H, p_{\theta}) = \min_{\gamma \in \Pi(p_H,p_{\theta})} \sum_{a,a' \in \mathcal{A}_q} \gamma(a,a')\, d(a,a'),
\]
with \(\Pi(p_H,p_{\theta})\) denoting the set of all couplings between \(p_H\) and \(p_{\theta}\), and \(d(a,a')\) the L1 distance between choices.
Since survey responses are inherently one-dimensional and ordinal, we can simplify the computation of WD using cumulative distribution functions (CDFs). In the 1-D case, WD is computed as
\begin{align*}
    \mathcal{WD}(p_H, p_{\theta}) 
    &= \int_{-\infty}^{+\infty} \left| F_{p_H}(x) - F_{p_{\theta}}(x) \right| \, dx, \\
    &= \sum_{i=1}^{n} \left| F_{p_H}(i) - F_{p_{\theta}}(i) \right|
\end{align*}
where \(F_{p_H}\) and \(F_{p_{\theta}}\) are the CDFs corresponding to \(p_H\) and \(p_{\theta}\), respectively.
We use this discrete formulation as the WD loss in our training.

While training with WD resulted in a higher KL-divergence on the validation set, the validation WD converged to similar levels regardless of the objective (see Figure \ref{fig:wd_vs_ce_loss}). We attribute this to KL-divergence penalizing low-probability assignments without significantly altering the overall distribution geometry. Given the KL divergence's broader applicability—without requiring ordinal information—we primarily used KL-divergence in our experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
    \captionsetup{font=small}
    \includegraphics[width=\linewidth]{figures/appendix_wd_vs_ce.pdf}
    \caption{
    Train loss curve (left) and validation loss curve (right) for Llama-2-7B
    fine-tuned on 90\% of OpinionQA, with the remaining 10\% used for validation.
    Light and dark blue lines represent KL-divergence (KL) and Wasserstein distance (WD) when used KL as a training objective,
    while light and dark red lines represent KL and WD when used WD as a training objective.
    The two training objectives yield similar results in terms of WD, the primary measure of opinion distribution matching in our work.
    }
    \label{fig:wd_vs_ce_loss}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%