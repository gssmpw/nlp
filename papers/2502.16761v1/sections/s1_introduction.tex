\section{Introduction}
Surveys provide an essential tool for probing public opinions on societal issues, especially as opinions vary over time and across subpopulations.
However, surveys are also costly, time-consuming, and require careful calibration to mitigate non-response and sampling biases \cite{choi2004catalog, bethlehem2010selection}. 
Recent work suggests that large language models (LLMs) can assist public opinion studies by predicting survey responses across different subpopulations, explored in both social science ~\cite{argyle2023out,bail2024can,ashokkumar2024predicting,manning2024automated} and NLP~\cite{santurkar2023whose,chu2023language,moon-etal-2024-virtual,hamalainen2023evaluating,chiang2023can}.
Such capabilities could substantially enhance the survey development process, not as a replacement for human participants but as a 
tool for researchers to conduct pilot testing, identify subpopulations to over-sample, and test analysis pipelines prior to conducting the full survey  \cite{rothschild2024survey}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/teaser.pdf}
    \caption{Illustration of our method and \OURDATA. We collect survey data from two survey familiesâ€”ATP from Pew Research~\cite{atp} (forming \OURDATA-Train) and GSS from NORC~\cite{davern2024gss} (forming \OURDATA-Eval). 
    LLMs are fine-tuned on \OURDATA-Train and evaluated on both OpinionQA~\cite{santurkar2023whose} and \OURDATA-Eval to assess generalization of distributional opinion prediction across unseen survey topics, survey families, and subpopulations.
    }
    \label{fig:teaser}
\end{figure}

Prior work in steering language models, \textit{i.e.} conditioning models to reflect the opinions of a specific subpopulation, has primarily investigated different prompt engineering techniques~\cite{santurkar2023whose, moon-etal-2024-virtual, park2024generative}. However, prompting alone has shown limited success in generating completions that accurately reflect the distributions of survey responses collected from human subjects. Off-the-shelf LLMs~\cite{achiam2023gpt, dubey2024llama, jiang2023mistral} have shown to mirror the opinions of certain US subpopulations such as the wealthy and educated \cite{santurkar2023whose,gallegos2024bias,deshpande2023toxicity,kim2023ai}, while generating stereotypical or biased predictions of underrepresented groups ~\cite{cheng2023compost,cheng2023marked,wang2024large}. Furthermore, these models often fail to capture the variation of human opinions within a subpopulation \cite{kapania2024simulacrum, park2024diminished}.
While fine-tuning presents opportunities to address these limitations ~\cite{chu2023language, he2024community}, existing methods fail to train models that accurately predict opinion distributions across diverse survey question topics and subpopulations.

\vspace{-5pt}
\paragraph{The present work.}
Here, we propose directly fine-tuning LLMs on large-scale, high-quality survey data,
consisting of questions about diverse topics and responses from each subpopulation, defined by demographic, socioeconomic, and ideological traits.
By casting pairs of (subpopulation, survey question) as input prompts, we train the LLM to align its response distribution against that of human subjects in a supervised manner.
We posit that survey data is particularly well-suited for fine-tuning LLMs since: (1) We can train the model with clear \textbf{subpopulation-response pairs} that explicitly link group identities and expressed opinions,
which is rare in LLMs' pre-training corpora,
(2) Large-scale opinion polls are carefully designed and calibrated (\textit{e.g.} using post-stratification) to estimate \textbf{representative} human responses, in contrast with LLMs' pre-training data where certain populations are over- or underrepresented, 
(3) Our training objective explicitly aligns model predictions with response \textbf{distributions} from each subpopulation, enabling LLMs to capture variance within human subpopulations.

Training on public opinion survey data has remained under-explored due to the limited availability of structured survey datasets. 
To this end, we curate and release \textbf{\OURDATA} (\textbf{Sub}population-level \textbf{P}ublic \textbf{O}pinion \textbf{P}rediction), a dataset of 70K subpopulation-response distribution pairs ($6.5\times$ larger compared to previous datasets).
We show that fine-tuning LLMs on \OURDATA significantly improves the distributional match between LLM generated and human responses, and improvements are consistent across subpopulations of varying sizes.
Additionally, the improvement generalizes to \textit{unseen} subpopulations, survey waves (topics), and survey families, \textit{i.e.} surveys administered by different institutions.
Such broad generalization is particularly critical for real-world public opinions research, where practitioners are most in need of synthetic data for survey questions or subpopulations (or both) that they have not tested before.

Our contributions are summarized as follows:
\vspace{-3mm}
\begin{itemize}[leftmargin=3.3mm]
\setlength\itemsep{2pt}

\item We show that training LLMs on response distributions from survey data significantly improves their ability to predict the opinions of subpopulations, reducing the Wasserstein distance between LLM and human distributions by 32-46\% compared to top-performing baselines. (\Cref{section_experiments_prediction_of_opinion_distributions})
\vspace{-1mm}
\item We show that the performance of the fine-tuned LLMs strongly generalizes to out-of-distribution data, including unseen subpopulations, new survey waves, and different survey families. 
(\Cref{section_experiments_prediction_of_opinion_distributions} and \Cref{section_experiments_per_group})
\vspace{-1mm}
\item We release \OURDATA, a curated and pre-processed dataset of public opinion survey results that is $6.5\times$ larger than existing datasets, enabling fine-tuning at scale.
\end{itemize}