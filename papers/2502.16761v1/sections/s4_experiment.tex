\section{Experiments}
\label{section_experiments}
\input{tables/main_results}

\subsection{Bounds of WD and Baselines}
\label{section_experiments_baseline}
In this section, we describe the lower/upper bounds and two baseline methods against which we compare our method.

\paragraph{Lower and upper bounds.} We use a uniform distribution over all available choices to establish an upper bound of the WD between a predicted and the target response distribution.
To compute a lower bound, we sample a group of human respondents from the original human respondents to calculate the WD between the two, and perform bootstrapping to obtain a robust estimate.
This lower bound captures the intrinsic variance arising from the respondent sampling process in opinion surveys.

\paragraph{Baselines.} We compare our approach with two baseline methods: prompting and Modular Pluralism~\cite{feng-etal-2024-modular}. 
For prompting, we consider both zero-shot and few-shot methods. In zero-shot prompting, we steer the LLM using demographic prompt formats. 
Specifically, we employ three different formats following \citet{santurkar2023whose}: \texttt{QA}, \texttt{BIO}, and \texttt{PORTRAY}. 
For instance, to condition the LLM to a person living in the South of the US, the \texttt{QA} format uses a question-answer format as illustrated in \Cref{fig:method}; 
the \texttt{BIO} format conditions the model with a first-person narrative such as ``I currently reside in the South."; 
and the \texttt{PORTRAY} format uses a third-person narrative like ``Answer the following question as if you currently reside in the South.".

Few-shot prompting augments the prompt with a few examples of question-response distribution pairs alongside the demographic label~\cite{hwang2023aligning}. 
In particular, we select the top five few-shot examples from the \OURDATA training set based on cosine similarity computed by the embedding model. 
In our experiments, we represent the response distribution in JSON format and require the model to output its prediction in the same JSON format, following the approach in~\citet{meister2024benchmarking}.

Modular pluralism~\cite{feng-etal-2024-modular} fine-tunes multiple LLMs on distinct datasets to capture the viewpoints of different communities~\cite{feng2023pretraining}. 
For a given question, each fine-tuned LLM generates an opinion that reflects the perspective of the community it represents, and a separate black-box LLM aggregates these outputs to produce the final distributional response. 
Detailed implementation of the lower/upper bounds and the baselines is provided in~\Cref{appendix_baseline_detail}.

\subsection{Generalization to Unseen Topics and Survey Families}
\label{section_experiments_prediction_of_opinion_distributions}
In this section, we assess the ability of our fine-tuned LLMs to generalize to unseen data—both in terms of new topics and entirely different survey families.
To evaluate these aspects, we use OpinionQA to measure generalization to unseen topics, and \OURDATA-Eval to test generalization to a different survey family.
We fine-tune four LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B, and Llama-3-70B) on \OURDATA-Train. We opt for pretrained LLMs rather than instruction-following models, as previous work has shown that pretrained models perform better on this task~\cite{moon-etal-2024-virtual}. A detailed comparison between these model types is provided in \Cref{appendix_chat}.

\paragraph{Summary of results.} 
\Cref{table:main_results} reports the average WD metrics computed over all demographic groups and survey questions, comparing our fine-tuned models against various baseline approaches.
Our experiments show that fine-tuning on \OURDATA-Train significantly outperforms all other methods, yielding a 32–46\% reduction in WD on OpinionQA and a 39–42\% reduction on \OURDATA-Eval compared to the best baselines. 
Notably, \OURDATA-Train is based on ATP data, while \OURDATA-Eval is derived from GSS surveys—two distinct survey families that can differ in respondent pools, calibration techniques, and other methodological factors, leading to non-trivial distribution shifts despite both being representative of the US population. 
Furthermore, our fine-grained analyses at the wave level (see \Cref{appendix_finegrain}) confirm that these trends persist even at more detailed levels of evaluation.

\paragraph{Comparison to zero- and few-shot prompting.}
We first compare the performance of prompting methods with our approach. Zero-shot prompting results in only modest WD improvements over the upper bound, with the largest gain observed for Llama-3-70B and negligible improvements for Llama-2-7B. 
Even when using few-shot prompting---where five example question-response distribution pairs are provided---the performance gains remain minimal. 
This may be partly due to an under-optimized prompt format (\textit{e.g.} requiring JSON output) and the inherent sensitivity of language models to prompt formatting~\cite{sclar2023quantifying, anagnostidis2024susceptible}. 
These findings underscore the need for methods, such as fine-tuning, that enable relatively reliable predictions of opinion distributions.


\paragraph{Comparison to Modular Pluralism.}
Modular Pluralism improves one-hot accuracy, reducing prediction error from 72.7\% (zero-shot prompting) to 55.6\% on OpinionQA, but underperforms in matching the full distribution of option choices, measured as WD.
This discrepancy in performance highlights the limitations of methods that train LLMs to identify only the most probable response rather than modeling the entire distribution of responses.
Opinions are inherently distributed: even within a particular subpopulation such as a single demographic subgroup, distribution of opinions cannot be captured as a single most likely response.
Moreover, instruction-tuned models that serve as a black-box LLM tend to assign high probabilities on only specific tokens~\cite{lin2022teaching, kadavath2022language, achiam2023gpt}, further pushing the generated distribution away from the human distribution.

\subsection{Generalization across Target Subpopulations}
\label{section_experiments_per_group}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \centering
    \captionsetup{font=small}
    \includegraphics[width=0.95\linewidth]{figures/main_figure_per_demographic.pdf}
    \caption{
    Per-group evaluation performance of our model Llama-2-7B-\OURDATA-FT (red lines) on OpinionQA.
    For comparison, the results from zero-shot QA prompting (black lines)
    and the lower bound (blue lines) are presented.
    We observe that the relative improvement,
    measuring how much of the gap between zero-shot prompting and the lower bound has been closed,
    remains consistent across subpopulations.
    Shaded blue regions represent the 95\% confidence interval of the lower-bound estimation for each group.
    Per-group results for other models (\Cref{table:per_group_opinionqa})
    and the results on \OURDATA evaluation set (\Cref{table:per_group_gss}) are available in \Cref{appendix_finegrain}.
    \label{fig:per_group_results}
    }
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here we report two key observations: 
(1) prediction performance improves consistently across most subpopulations represented in the fine-tuning data, and 
(2) the LLMs fine-tuned on \OURDATA-Train generalize well to subpopulations that were not included during fine-tuning.

\paragraph{Consistent performance improvements over subpopulations.}
\Cref{fig:per_group_results} shows the per-group WD on the OpinionQA evaluation for Llama-2-7B, 
comparing our fine-tuning approach with zero-shot prompting and the empirical WD lower bound. 
To evaluate the consistency of performance gains, we calculate the \textit{relative improvement} for each subpopulation as how much of the gap between zero-shot prompting and the empirical lower bound is reduced after fine-tuning. 
This measure allows us to account for varying lower bounds across subpopulations: since some groups have fewer respondents, there is greater uncertainty in their reported distribution in the survey data and greater variance between the original sample and bootstrap samples.

All 22 subpopulations demonstrate a large relative improvement after fine-tuning, ranging from 38\%--54\%.
The average relative improvement is 46.7\% with a standard deviation of 4.4\%.
This consistency confirms that our fine-tuning approach delivers balanced performance gains without disproportionately favoring any particular demographic subgroup.
We hypothesize that the consistent gains over groups largely stem from our dataset design, 
which allocates an equal number of training samples to each group. 
By ensuring uniformly distributed data points across subpopulations, the model captures sufficient subgroup-specific signals, ultimately leading to consistent performance improvements.

\paragraph{Generalization to unseen subpopulations.}
We further investigate how models fine-tuned with our approach and \OURDATA might show generalization to subpopulations that were not represented in the training data, a circumstance that may arise in real-world survey development. 
For the evaluation, we benchmark our methods against a zero-shot prompting baseline. 
Specifically, we evaluate our model, which is fine-tuned on 22 subpopulations provided in \OURDATA-Train,
on a set of subpopulations in OpinionQA that were not included in fine-tuning. 
This experiment not only checks generalization to unseen subpopulations, but also involves unseen survey questions, providing a robust assessment of the model capability for generalization to out-of-distribution data.

As shown in Table~\ref{table:unseen_demographics},
our model achieves a strong reduction in WD even for unseen subpopulations,
indicating that the model can be steered by demographic prompts beyond the seen subpopulations in training.
Interestingly, although \OURDATA-Train does not contain any data with opinion distributions of particular age groups
(\textit{e.g}. subjects of age 18-29 or those of age 65+),
the average relative improvement is 44.7\%, which is compatible with the average relative improvement for seen subpopulations. We provide results for other unseen groups in \Cref{table:unseen_demographic_all} of \Cref{appendix_unseen_subpopulation} (average relative improvement of 43.1\% with a standard deviation of 6.7\%).

\input{tables/unseen_demographics}

\paragraph{Steerability towards subpopulations.}
Given the large improvements in WD across subpopulations after fine-tuning, we want to test whether the LLM is truly adapting its predictions based on the subpopulation specified in its prompt (\textit{i.e.} the LLM is being steered) or if the improvements can be explained by the LLMs' predictions getting closer to human responses in general, without any subpopulation-specific adaptation. 
If the LLM is being steered, we should expect that the LLM's predictions for a target subpopulation $g_t$ are closer to the human distribution for $g_t$ when $g_t$ is the subpopulation specified in the prompt, compared to when another group $g_s$ is specified in the prompt.
We should also expect the gap in WD to be larger if the distance between the true human distributions for $g_t$ and $g_s$ are larger, such as differences between the youngest and oldest age groups compared to adjacent groups.

Formally, we define the \textit{intergroup disagreement} between a target group $g_t$ and a source group $g_s$ as \(\mathcal{WD}(p_H(\mathcal{A}_q \mid q, g_t), p_H(\mathcal{A}_q \mid q, g_s))\) averaged over evaluation questions. In human responses (left of \Cref{fig:educ_heatmap}), the disagreement shows the pattern of locality: increases as the disparity in education levels between two groups grows.
We extend this notion to compare the human distribution from the target group \(g_t\) with the LLM-predicted distribution when the \textit{source} group $g_s$ is specified in the prompt,
\(\mathcal{WD}(p_H(\mathcal{A}_q \mid q, g_t), p_{\theta}(\mathcal{A}_q \mid q, g_s))\). If the model truly incorporates subpopulation information from the prompt, its intergroup disagreement pattern should mirror that of the human data.

Zero-shot prompting with the base model (right of \Cref{fig:educ_heatmap}) does not exhibit the locality pattern seen in the human data, indicating that it cannot be steered by subpopulation labels. In contrast, the fine-tuned model (middle of \Cref{fig:educ_heatmap}) reproduces a pattern resembling the human-human case, even though it was trained on only two education groups (“less than high school” and “college graduate/some postgrad”) and the other four groups were unseen. This result demonstrates that our fine-tuned model not only learns to condition on subpopulation information but also generalizes to subpopulations unseen during fine-tuning.
We provide the intergroup disagreement for other traits in \Cref{appendix_unseen_subpopulation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \centering
    \captionsetup{font=small}
    \includegraphics[width=1.00\linewidth]{figures/figure_heatmeap_educ_dynamic_color.pdf}
    \caption{
    \textit{Intergroup disagreement} pattern between groups of different education levels calculated with OpinionQA and Llama-2-7B as a base model.
    A target human group is compared to
    (left) a source human group,
    (middle) our fine-tuned model conditioned on a source group,
    (right) a base model conditioned on a source group.
    Bold-faced groups are included in the fine-tuning data \OURDATA-Train, while the others aren't. In the human response (left), we observe a decreasing disagreement level as the education level becomes similar. This disagreement pattern exists in our fine-tuned model but not in the zero-shot prompting with a base model, indicating that our model can be steered to given subpopulation label even for unseen demographics while the base model cannot.
    }
    \label{fig:educ_heatmap}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Effect of Scaling the Dataset}
\label{section_experiments_scaling}
In this section, we examine performance scales with training dataset size. 
We randomly sample subsets containing 25\%, 50\%, 75\%, and 87.5\% of the full \OURDATA training set and evaluate three models—Llama-2-7B, Llama-2-13B, and Mistral-7B—on OpinionQA. 
As shown in \Cref{fig:scaling}, we observe diminishing marginal returns, as is typical with fine-tuning; for example, after training on a random 25\%, the models reach 72\%-78\% of the total improvement they achieve after fine-tuning on all of \OURDATA-train. However, the performance does not entirely plateau. Instead, it continues to improve as we further increase the training data from 25\% to 100\%. %, across all selection methods.
We fit linear trend lines (dotted in \Cref{fig:scaling}) to the results and observe that the slopes are similar for each model. 
This suggests that the rate of improvement—reflected by the slope in the power-law relationship—is intrinsic to the data and task rather than to the specific model architecture. 
In other words, LLMs exhibit comparable data efficiency, with performance gains that are fundamentally tied to dataset size rather than model-specific factors.

Using these trend lines, we can estimate the amount of fine-tuning data required to reach a target performance. For instance, we estimate that fine-tuning Mistral-7B on a dataset 25 times larger than the current \OURDATA training set would yield a WD value of 0.07, which is much closer to the empirical lower bound of 0.031 reported in \Cref{table:main_results}.
This result underscores the critical importance of collecting more high-quality data, as increased dataset size can drive significant improvements in model performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
    \centering
    \captionsetup{font=small}
    \includegraphics[width=1.00\linewidth]{figures/main_figure_scaling_fitting.pdf}
    \caption{
    Evaluation results on OpinionQA after fine-tuning each LLM on increasingly large sampled subsets of \OURDATA-Train.
    Both axes are presented in a log scale.
    The $x$-axis is the size of sampled dataset and the $y$-axis is WD against human responses measured on OpinionQA.
    Dashed lines represent a line of best fit.
    Performances at data percentage of 100\% are identical to ours (\OURDATA-FT) in Table \ref{table:main_results}.
    }
    \label{fig:scaling}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  