\documentclass[hidelinks]{amsart}


\usepackage{xcolor,enumerate}       % blackboard math symbols
\topmargin  = 0.0 in
\leftmargin = 0.9 in
\rightmargin = 1.0 in
\evensidemargin = -0.10 in
\oddsidemargin =  0.10 in
\textheight = 8.5 in
\textwidth  = 6.6 in
\setlength{\parskip}{2mm}
\setlength{\parindent}{0mm}
\def\baselinestretch{1}
 \definecolor{darkgreen}{rgb}{0,0.5,0}

%\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amssymb,latexsym,amsfonts,amsmath}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{subcaption}
\usepackage{color}
\usepackage{mathabx}
%\usepackage{natbib}
\usepackage{algorithm,algorithmic}
% \usepackage[noend]{algpseudocode}
\usepackage{bigints}

\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\tr}{tr}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{conjecture}[theorem]{Conjecture}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{claim}[theorem]{Claim}
% \newtheorem{problem}[theorem]{Problem}
% \newtheorem{algorithm}[theorem]{Algorithm}
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{construction}[theorem]{Construction}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{example}[theorem]{Example}
% \newtheorem{xca}[theorem]{Exercise}
% \newtheorem{comments}[theorem]{Comments}
% \newtheorem{remark}[theorem]{Remark}
% \numberwithin{equation}{section}

% Notation
\newcommand{\bs}[1]{\mathbf{#1}}
\newcommand{\bin}{\{0,1\}^n}
\newcommand{\bi}{\bs{b}}
\newcommand{\ev}{\bs{e}}
\newcommand{\bv}{\bs{b}}
\newcommand{\uv}{\bs{u}}
\newcommand{\vv}{\bs{v}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\zeros}{\mathbf{0}}
\newcommand{\real}{{\mathbb{R}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\PP}{{\mathbb{P}}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\G}{{\mathcal{G}}}
\newcommand{\D}{{\mathcal{D}}}
\newcommand{\F}{{\mathcal{F}}}
\newcommand{\C}{{\mathcal{C}}}
\newcommand{\K}{{\mathcal{K}}}
\newcommand{\Cinit}{\subscr{\C}{init}}
\newcommand{\Cund}{\subscr{\C}{und}}
\newcommand{\Jinit}{\subscr{J}{init}}
\newcommand{\Cdyn}{\subscr{\C}{dyn}}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{sublemma}{Sublemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}[section]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]


% General
\newcommand{\eps}{\varepsilon}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\until}[1]{\{1,\dots,#1\}}
\newcommand{\map}[3]{#1:#2 \rightarrow #3}
\newcommand{\setmap}[3]{#1:#2 \rightrightarrows #3}
\newcommand{\equilibria}[1]{\operatorname{Eq}(#1)}
\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdertwo}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\pdertwom}[3]{\frac{\partial^2 #1}{\partial #2\partial #3}}
\newcommand{\adve}{\subscr{\Sigma}{adv}}
\newcommand\subscr[2]{#1_{\textup{#2}}}
\newcommand\upscr[2]{#1^{\textup{#2}}}
\newcommand\upsubscr[3]{#1_{\textup{#2}}^{\textup{#3}}}
\newcommand{\longthmtitle}[1]{\mbox{}\textup{\textbf{(#1):}}}
\newcommand{\metric}{d}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\PosSym}[1]{\mathrm{Sym}_{\succ 0}(\real^{#1 \times #1})}
\newcommand{\SkewSym}[1]{\Lambda(\real^{#1 \times #1})}
\newcommand{\innerp}[2]{ \langle #1, #2\rangle}

\newcommand{\oprocendsymbol}{\hbox{$\diamond$}}
\newcommand{\oprocend}{\relax\ifmmode\else\unskip\hfill\fi\oprocendsymbol}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\begin{document}

\begin{abstract}
Inspired by REINFORCE, we introduce a novel receding-horizon algorithm for the Linear Quadratic Regulator (LQR) problem with unknown parameters. Unlike prior methods, our algorithm avoids reliance on two-point gradient estimates while maintaining the same order of sample complexity. Furthermore, it eliminates the restrictive requirement of starting with a stable initial policy, broadening its applicability. Beyond these improvements, we introduce a refined analysis of error propagation through the contraction of the Riemannian distance over the Riccati operator. This refinement leads to a better sample complexity and ensures improved convergence guarantees. 
Numerical simulations validate the theoretical results, demonstrating the method's practical feasibility and performance in realistic scenarios.

\end{abstract}

\title{Sample Complexity of Linear Quadratic Regulator\\  Without Initial Stability}

\author[Amirreza Neshaei Moghaddam]{Amirreza Neshaei Moghaddam}
\address{Department of Electrical and Computer Engineering\\
University of California at Los Angeles,
Los Angeles}
\email{amirnesha@ucla.edu}
\author[Alex Olshevsky]{Alex Olshevsky}
\address{Department of Electrical and Computer Engineering\\
       Boston University}
\email{alexols@bu.edu}
\author[Bahman Gharesifard]{Bahman Gharesifard}
\address{Department of Electrical and Computer Engineering\\
       University of California, Los Angeles}
\email{gharesifard@ucla.edu}

\maketitle

\section{Introduction}
The Linear Quadratic Regulator (LQR) problem, a cornerstone of optimal control theory, offers an analytically tractable framework for optimal control of linear systems with quadratic costs. Traditional methods rely on complete knowledge of system dynamics, solving the Algebraic Riccati Equation~\cite{DPB:95} to determine optimal control policies. However, recent real-world scenarios often involve incomplete or inaccurate models. Classical methods in control theory, such as identification theory~\cite{MG:06} and adaptive control~\cite{AMA-ALF:21}, were specifically designed to provide guarantees for decision-making in scenarios with unknown parameters. However, the problem of effectively approximating the optimal policy using these methods remains underexplored in the traditional literature. Recent efforts have sought to bridge this gap by analyzing the sample complexity of learning-based approaches to LQR~\cite{SD-HM-NM-BR-ST:20}, providing bounds on control performance relative to the amount of data available.

In contrast, the model-free approach, rooted in reinforcement learning (RL), bypasses the need for explicit dynamics identification, instead focusing on direct policy optimization through cost evaluations. Recent advances leverage stochastic zero-order optimization techniques, including policy gradient methods, to achieve provable convergence to near-optimal solutions despite the inherent non-convexity of the LQR cost landscape. Foundational works, such as~\cite{MF-RG-SK-MM:18}, established the feasibility of such methods despite the non-convexity of the problem, demonstrating convergence under random initialization. Subsequent efforts, including~\cite{DM-AP-KB-KK-PLB-MJW:20} and~\cite{HM-AZ-MS-MRJ:22}, have refined these techniques, achieving improved sample complexity bounds. Notably, all of these works assume that the initial policy is stabilizing.

A key limitation of these methods, including~\cite{DM-AP-KB-KK-PLB-MJW:20, HM-AZ-MS-MRJ:22}, is the reliance on two-point gradient estimation, which requires evaluating costs for two different policies while maintaining identical initial states. In practice, this assumption is often infeasible, as the initial state is typically chosen randomly and cannot be controlled externally. Our earlier work~\cite{ANM-AO-BG:24} addressed this challenge, establishing the best-known result among methods that assume initial stability without having to rely on two-point estimates. Instead, we proposed a one-point gradient estimation method, inspired by REINFORCE~\cite{RJW:92, RSS-DM-SS-YM:99}, that achieves the same convergence rate as the two-point method~\cite{DM-AP-KB-KK-PLB-MJW:20} using only a single cost evaluation at each step. This approach enhances both the practical applicability and theoretical robustness of model-free methods, setting a new benchmark under the initial stability assumption.

The requirement for an initial stabilizing policy significantly limits the utility of these methods in practice. Finding such a policy can be challenging or infeasible and often relies on identification techniques, which model-free methods are designed to avoid. Without getting technical at this point, it is worth pointing out that this initial stability assumption plays a major role in the construction of the mentioned model-free methods, and cannot be removed easily. For instance, this assumption ensures favorable optimization properties, like coercivity and gradient domination, that simplify convergence analysis. In this sense, removing this assumption while maintaining stability and convergence guarantees is essential to generalize policy gradient methods, a challenge that has remained an active research topic~\cite{XZ-TB:23, JCP-JU-MS:21, AL:20, FZ-XF-KY:21}.

As also pointed out in~\cite{XZ-TB:23}, the $\gamma$-discounted LQR problems studied in~\cite{JCP-JU-MS:21, AL:20, FZ-XF-KY:21} are equivalent to the standard non-discounted LQR with system matrices scaled by $\sqrt{\gamma}$. In~\cite{JCP-JU-MS:21, AL:20, FZ-XF-KY:21}, this scaling results in an enlarged set of stabilizing policies when $\gamma$ is sufficiently small, enabling policy gradient algorithms to start from an arbitrary policy. However, as noted in~\cite{XZ-TB:23}, this comes at the cost of solving multiple LQR instances rather than a single one, increasing computational overhead. Furthermore, the optimization landscape in the discounted setting remains fundamentally the same as in the undiscounted case, as described in~\cite{MF-RG-SK-MM:18,DM-AP-KB-KK-PLB-MJW:20}. Consequently, the same difficulties mentioned in~\cite{BH-KZ-NL-MM-MF-TB:23,YT-YZ-NL:21} persist when extending these methods to output-feedback settings, where additional estimation errors complicate policy search. In contrast, receding-horizon approaches~\cite{XZ-TB:23} provide a more direct and extensible framework for tackling such challenges~\cite{XZ-SM-MB-TB:23}.


This paper builds on the receding-horizon policy gradient framework introduced in~\cite{XZ-TB:23}, a significant step towards eliminating the need for a stabilizing initial policy by recursively updating finite-horizon costs. While the approach proposed in this work marks an important step forward in model-free LQR, we address the reliance on the two-point gradient estimation, a known limitation discussed earlier. Building on the gradient estimation approach from our earlier work~\cite{ANM-AO-BG:24}, we adapt the core idea to accommodate the new setup that eliminates the initial stability assumption. Specifically, our modified method retains the same convergence rate while overcoming the restrictive assumptions of two-point estimation. Beyond these modifications, we introduce a refined convergence analysis, via an argument based on a Riemannian distance function~\cite{RB:07}, which significantly improves the propagation of errors. This ensures that the accumulated error remains linear in the horizon length, in contrast to the exponential growth in~\cite{XZ-TB:23}. As a result, we achieve a uniform sample complexity bound of $\widetilde{\mathcal{O}}(\eps^{-2})$, independent of problem-specific constants, thereby offering a more scalable and robust policy search framework.
 

\subsection{Algorithm and Paper Structure Overview}\label{sub: structure}
The paper is structured into three sections. Section II presents the necessary preliminaries and establishes the notation used throughout the paper. Section III introduces our proposed algorithm, which operates through a hierarchical double-loop structure, an outer loop which provides a surrogate cost function in a receding horizon manner, and an inner loop applying policy gradient method to obtain an estimate of its optimal policy. Section IV delves deeper into the policy gradient method employed in the inner loop, providing rigorous convergence results and theoretical guarantees for this critical component of the algorithm. Section~V includes the sample complexity bounds, and comparisons with the results in the literature. Finally, we provide simulations studies verifying our findings in Section~VI. 


To be more specific, the core idea of the algorithm leverages the observation that, for any error tolerance $\eps$, there exists sufficiently large finite horizon $N$ where the sequence of policies minimizing recursively updated finite-horizon costs can approximate the optimal policy for the infinite-horizon cost within $\eps$ neighborhood. This insight motivates the algorithm’s design: a recursive outer loop that iteratively refines the surrogate cost function over a sequence of finite horizons, and an inner loop that employs policy gradient methods to approximate the optimal policy for each of these costs. Specifically, in the outer loop, the algorithm updates the surrogate cost and the associated policy at each horizon step $h$, starting from the terminal horizon $h=N-1$ and moving backward to $h=0$. At each step $h$, the inner loop applies a policy gradient method to compute an approximately optimal policy for the finite-horizon cost over the interval $[h,N]$. This step generates a surrogate policy $\widetilde{K}_{h}$, which is then incorporated into the cost function of the subsequent step in the outer loop.

The main difficulty in analyzing the proposed algorithm stems from the fact that the approximation errors from the policy gradient method in the inner loop propagate across all steps of the outer loop. To ensure overall convergence, the algorithm imposes a requirement on the accuracy of the policy gradient method in the inner loop. Each policy obtained must be sufficiently close to the optimal policy for the corresponding finite-horizon cost. This guarantees that the final policy at the last step of the outer loop converges to the true optimal policy for the infinite-horizon cost.



\section{Preliminaries}
\label{sec:introduction}
In this section, we gather the required notation, closely following the ones in~\cite{XZ-TB:23} which our work builds on. Consider the discrete-time linear system
\begin{equation}
x_{t+1} = A x_t + B u_t, \label{eq: LTI_discrete_time}
\end{equation}
where $x_t \in \real^n$ is the system state at time $t$, $u_t \in \real^{m}$ is the control input at time $t\geq 0$, $A \in \real^{n \times n}$ and $B \in \real^{n \times m}$ are the system matrices which are \emph{unknown} to the control designer. Crucially here, the initial state $x_0$ is sampled randomly from a distribution $\D$ and satisfies 
\begin{align}
    \E[x_0] = 0, \quad 
    \E[x_0 x_0^\top] = \Sigma_0, \ \text{and} \ \| x_0 \|^2 \leq C_m \ \text{a.s.} \label{eq: noise_assumptions}
\end{align}
The objective in the LQR problem is to find the optimal controller that minimizes the following cost
\[
J_{\infty} = \E_{x_0 \sim \D} \left[\sum_{t=0}^{\infty} x_t^\top Q x_t + u_t^\top R u_t \right],
\]
where $Q \in \real^{n \times n}$ and $R \in \real^{m \times m}$ are the symmetric positive definite matrices that parameterize the cost. We require the pair $(A,B)$ to be stabilizable, and since $Q \succ 0$, the pair $(A,Q^{1/2})$ is observable,
As a result, the unique optimal controller is a linear state-feedback $u^\ast_t = - K^* x_t$ where $K^*$ is derived as follows
\begin{equation}
K^* = (R + B^\top P^* B)^{-1} B^\top P^* A, \label{eq: K* explicit formulation}
\end{equation}
and $P^*$ denotes the unique positive definite solution to the discounted discrete-time algebraic Riccati equation (ARE)~\cite{DPB:95}:
\begin{equation}
    P = A^\top P A - A^\top P B (R + B^\top P B)^{-1} B^\top P A + Q. \label{eq: ARE}
\end{equation}

\subsection{Notations}
We use $\| X \|$, $\| X \|_F$, $\sigma_{\min} (X)$, and $\kappa_{X}$ to denote the 2-norm, Frobenius norm, minimum singular value, and the condition number of a matrix $X$ respectively. We also use $\rho (X)$ to denote the spectral radius of a square matrix $X$. Moreover, for a positive definite matrix $W$ of appropriate dimensions, we define the $W$-induced norm of a matrix $X$ as
\[
\| X \|_W^2 := \sup_{z \neq 0} \frac{z^\top X^\top W X z}{z^\top W z}.
\]
Following the notation in~\cite{XZ-TB:23}, we denote the $P^*$-induced norm by $\| X \|_*$. Furthermore, we denote the Riemannian distance~\cite{RB:07} between two positive definite matrices $U,V \in \R^{n \times n}$ by
\[
\delta(U,V) = \left( \sum_{i=1}^{n} \log^2 \lambda_i(U V^{-1}) \right)^{1/2}.
\]

We now introduce some important notations which will be used in describing the algorithm and proof of the main result.
Let \( N \) be the horizon length and \( h \) the initial time step. The true finite-horizon cost \( J_h(K_h) \) of a policy \( K_h \) is defined as
\begin{align}
    J_h(K_h) := \E_{x_h \sim \mathcal{D}} \Bigg[ \sum_{t=h+1}^{N-1} x_t^\top \left( Q + (K^*_t)^\top R K^*_t \right) x_t  + x_h^\top \left( Q + K_h^\top R K_h \right) x_h + x_N^\top Q_N x_N \Bigg], \label{eq: J_h_definition}
\end{align}
where:
\begin{itemize}
    \item \( x_h \sim \mathcal{D} \) denotes the initial state \( x_h \) is drawn from the distribution \( \mathcal{D} \),
    \item \( Q_N \) is the terminal cost matrix, which can be chosen arbitrarily (e.g., \( Q_N = 0 \)),
    \item \( K_h \) is the feedback gain applied at step \( h \),
    \item \( K^*_t \) is the feedback gain at step \( t \), to be formally defined via the Riccati difference equation in~\eqref{eq: RDE_K};   
\end{itemize}
Finally, for all \( t \in \{h+1, \dotsc, N-1\} \), the state evolves according to:
    \[
    x_{t+1} = (A - BK^*_t) x_t,
    \]
    with
    \[
    x_{h+1} = (A - BK_h) x_h.
    \]
We also define the surrogate cost
\begin{align}
    \widetilde{J}_h(K_h) := \E_{x_h \sim \mathcal{D}} \Bigg[ \sum_{t=h+1}^{N-1} x_t^\top \left( Q + \widetilde{K}_t^\top R \widetilde{K}_t \right) x_t + x_h^\top \left( Q + K_h^\top R K_h \right) x_h + x_N^\top Q_N x_N \Bigg], \label{eq: J_h_tilde_definition}
\end{align}
where \( \widetilde{K}_t \) is the feedback gain derived at step \( t \) of the [outer loop of the] algorithm, and for all \( t \in \{h+1, \dotsc, N-1\} \), the state evolves as:
    \[
    x_{t+1} = (A - B\widetilde{K}_t) x_t,
    \]
    with
    \[
    x_{h+1} = (A - BK_h) x_h.\]    
The key difference between \( \widetilde{J}_h(K_h) \) and \( J_h(K_h) \) lies in the use of \( \widetilde{K}_t \) versus \( K^*_t \) for \( t \in \{ h+1, \dotsc, N-1 \} \). This distinction implies that \( \widetilde{J}_h(K_h) \) incorporates all errors from earlier steps, precisely the ones at \( \{N-1, \dotsc, h+1\} \), as the procedure is recursive.

We now define several functions that facilitate the characterization of our gradient estimate, which uses ideas from our earlier work in~\cite{ANM-AO-BG:24}). To start, we let
\begin{align}
    \widetilde{J}_h (K_h; x_h) &:= \sum_{t=h+1}^{N-1} x_t^\top \left( Q + \widetilde{K}_t^\top R \widetilde{K}_t \right) x_t + x_h^\top (Q + K_h^\top R K_h ) x_h + x_N^\top Q_N x_N \cr
    &= x_h^\top (Q + K_h^\top R K_h ) x_h + x_h^\top (A-BK_h)^\top \widetilde{P}_{h+1} (A-BK_h) x_h, \label{eq: J_tilde_init2}
\end{align}
so that
\[
\widetilde{J}_h (K_h) = \E_{x_h \sim \mathcal{D}} \left[\widetilde{J}_h (K_h; x_h)\right].
\]
Using~\eqref{eq: J_tilde_init2}, we can compute the gradient of $\widetilde{J}_h (K_h; x_h)$ with respect to $K_h$ as follows:
\begin{align}
    \nabla \widetilde{J}_h (K_h; x_h) &= \nabla \Bigg( x_h^\top K_h^\top R K_h x_h + x_h^\top K_h^\top B^\top \widetilde{P}_{h+1} B K_h x_h - 2 x_h^\top A^\top \widetilde{P}_{h+1} B K_h x_h \Bigg) \cr
    &= 2 R K_h x_h x_h^\top + 2 B^\top \widetilde{P}_{h+1} B K_h x_h x_h^\top - 2 B^\top \widetilde{P}_{h+1} A x_h x_h^\top \cr
    &= 2 \left( (R + B^\top \widetilde{P}_{h+1} B) K_h - B^\top \widetilde{P}_{h+1} A \right) x_h x_h^\top,
\end{align}
and thus,
\begin{align}
    \nabla \widetilde{J}_h (K_h) &= \E_{x_h \sim \mathcal{D}} \left[ \nabla \widetilde{J}_h (K_h; x_h) \right] \cr
    &= 2 \left( (R + B^\top \widetilde{P}_{h+1} B) K_h - B^\top \widetilde{P}_{h+1} A \right) \E_{x_h \sim \mathcal{D}} \left[ x_h x_h^\top \right] \cr
    &= 2 \left( (R + B^\top \widetilde{P}_{h+1} B) K_h - B^\top \widetilde{P}_{h+1} A \right) \Sigma_0. \label{eq: actual_gradient}
\end{align}
Moreover, we define
\begin{align}
    Q_h(x_h, u_h) &:= x_h^\top Q x_h + u_h^\top R u_h + \sum_{t=h+1}^{N-1} x_t^\top \left( Q + \widetilde{K}_t^\top R \widetilde{K}_t \right) x_t + x_N^\top Q_N x_N \notag \\
    &= x_h^\top Q x_h + u_h^\top R u_h +\widetilde{J}_{h+1} (\widetilde{K}_{h+1}; Ax_h + Bu_h) \notag \\
    &= x_h^\top Q x_h + u_h^\top R u_h + (Ax_h + Bu_h)^\top \widetilde{P}_{h+1} (Ax_h + Bu_h), \label{eq: Q_func_def}
\end{align}
so that
\[
\widetilde{J}_h (K_h; x_h) = Q_h(x_h, - K_h x_h),
\]
and
\[
\widetilde{J}_h (K_h) = \E_{x_h \sim \mathcal{D}} \left[ Q_h(x_h, - K_h x_h) \right].
\]
Having established the cost functions, we now introduce the notation used to describe the policies:
\begin{align}
    K^*_h &:= \argmin_{K_h} J_h(K_h), \cr \widetilde{K}^*_h &:= \argmin_{K_h} \widetilde{J}_h(K_h), \label{eq: K_argmin_def}
\end{align}
where \( K^*_h \) denotes the optimal policy for the true cost \( J_h(K_h) \), and \( \widetilde{K}^*_h \) denotes the optimal policy for the surrogate cost \( \widetilde{J}_h(K_h) \).
Additionally, \( \widetilde{K}_h \) represents an estimate of \( \widetilde{K}^*_h \). It is obtained using a policy gradient method in the inner loop of the algorithm, which is applied at each step \( h \) of the outer loop to minimize the surrogate cost \( \widetilde{J}_h(K_h) \).

We now move on to the recursive equations. First, we have
\begin{align}
    \widetilde{P}_{h} &= (A - B \widetilde{K}_h)^\top \widetilde{P}_{h+1} (A - B \widetilde{K}_h) + \widetilde{K}_h^\top R \widetilde{K}_h + Q, \label{eq: lqr_lyapunov}
\end{align}
where $\widetilde{P}_N = Q_N$. In addition,
\begin{align}
    \widetilde{P}^*_{h} &= (A - B \widetilde{K}^*_h)^\top \widetilde{P}_{h+1} (A - B \widetilde{K}^*_h) + (\widetilde{K}^*_h)^\top R \widetilde{K}^*_h + Q, \label{eq: P_tilde_star_def}
\end{align}
where $\widetilde{K}^*_h$ from \eqref{eq: K_argmin_def} can also be computed from
\begin{equation*}
    \widetilde{K}^*_h = (R + B^\top \widetilde{P}_{h+1} B)^{-1} B^\top \widetilde{P}_{h+1} A.
\end{equation*}
Finally, we have the Riccati difference equation (RDE):
\begin{align}
    P^*_{h} &= (A - B K^*_h)^\top P^*_{h+1} (A - B K^*_h) + (K^*_h)^\top R K^*_h + Q, \label{eq: RDE}
\end{align}
where $P^*_N = Q_N$ and $K^*_h$ from \eqref{eq: K_argmin_def} can also be computed from
\begin{equation}
    K^*_h = (R + B^\top P^*_{h+1} B)^{-1} B^\top P^*_{h+1} A. \label{eq: RDE_K}
\end{equation}
As a result, it is easy to follow that
\begin{align}
    \E_{x_h \sim \D} \left[ x_h^\top \widetilde{P}_h x_h \right] &= \widetilde{J}_h (\widetilde{K}_h), \\
    \E_{x_h \sim \D} \left[ x_h^\top \widetilde{P}^*_h x_h \right] &= \widetilde{J}_h (\widetilde{K}^*_h), \quad \text{and} \\
    \E_{x_h \sim \D} \left[ x_h^\top P^*_h x_h \right] &= J_h (K^*_h).
\end{align}
We also define the Riccati operator
\begin{equation}
    \mathcal{R}(P) := Q + A^\top (P - PB(R + B^\top P B)^{-1} B^\top P) A, \label{eq: def_Riccati_operator}
\end{equation}
so that $\widetilde{P}^*_h$ and $P^*_h$ can also be shown as
\begin{align}
    \widetilde{P}^*_h &= \mathcal{R}(\widetilde{P}_{h+1}) \label{eq: Riccati_operator_on_P_tilde_star} \\
    P^*_h &= \mathcal{R}(P^*_{h+1}), \label{eq: Riccati_operator_on_P_star}
\end{align}
after replacing $\widetilde{K}^*_h$ and $K^*_h$ in~\eqref{eq: P_tilde_star_def} and~\eqref{eq: RDE} respectively.

We now introduce the following mild assumption, which will be useful in establishing a key result.
\begin{assumption}
    $A$ in~\eqref{eq: LTI_discrete_time} is non-singular. \label{ass: A_invertible}
\end{assumption}

Under this assumption, the following result from~\cite{JS-MC:23} holds:

\begin{lemma} \label{lem: delta_Riccati_contraction}
    Consider the operator \( \mathcal{R} \) defined in~\eqref{eq: def_Riccati_operator}. If Assumption~\ref{ass: A_invertible} holds, then for any symmetric positive definite matrices \( X, Y \in \mathbb{R}^{n \times n} \), we have    \begin{equation*}        \delta(\mathcal{R} (X), \mathcal{R} (Y)) \leq \delta (X,Y).
    \end{equation*}
\end{lemma}

Having introduced all the necessary definitions, we now turn our attention to the our loop. 
\section{The Outer Loop (Receding-Horizon Policy Gradient)}
It has been demonstrated that the solution to the RDE~\eqref{eq: RDE} converges monotonically to the stabilizing solution of the ARE~\eqref{eq: ARE} exponentially~\cite{BH-AHS-TK:99}. As a result, $\{K^*_t\}_{t \in \{ N-1, \dotsc, 1, 0\}}$ in~\eqref{eq: RDE_K} also converges monotonically to $K^*$ as $N$ increases. In particular, we recall the following result from~\cite[Theorem~1]{XZ-TB:23}.
\begin{theorem}\label{thm: RDE converges to ARE}
     Let $A^*_K := A - BK^*$, and define
    \begin{equation}
        N_0 = \frac{1}{2} \frac{\log{\left(\frac{\| Q_N - P^* \|_* \kappa_{P^*} \|A^*_K \| \|B\|}{\eps \lambda_{\min} (R)}\right)}}{\log{\left(\frac{1}{\| A^*_K \|_*}\right)}},
    \end{equation}
    where $Q_N \succeq P^*$.
    Then it holds that $\| A^*_K \|_* < 1$ and for all $N \geq N_0$, the control policy $K^*_0$ computed by~\eqref{eq: RDE_K} is stabilizing and satisfies $\| K^*_0 - K^* \| \leq \eps$ for any $\eps>0$.
\end{theorem}

The proof of Theorem~\ref{thm: RDE converges to ARE} is provided in Appendix~\ref{app: RDE converges to ARE} for completeness (and to account for some minor change in notation). We also note that this theorem relies on a minor inherent assumption that \( Q_N \) satisfies \( Q_N \succeq P^* \). A full discussion of this assumption is provided in Remark~\ref{rem: Q_N inherent_assumption} in Appendix~\ref{app: RDE converges to ARE}.

With this result in place, we provide our proposed algorithm (see Algorithm~\ref{alg:RHPG-RL}). Note that in this section, we focus on the outer loop of Algorithm~\ref{alg:RHPG-RL}, analyzing the requirements it imposes on the convergence of the policy gradient method employed in the inner loop. The details of the policy gradient method will be discussed in the next section.
\begin{algorithm}[t]
\caption{Receding-Horizon Policy Gradient}
\label{alg:RHPG-RL}
\begin{algorithmic}[1]
\REQUIRE Horizon $N$, max iterations $\{T_h\}$, stepsizes $\{\alpha_{h,t}\}$, variance parameter $\sigma^2$
\FOR{$h = N-1$ to $0$}
    \STATE Initialize $K_{h, 0}$ arbitrarily (e.g., the convergent policy from the prev. iter. $K_{h+1, T_{h+1}}$ or $0$).
    \FOR{$t = 0$ to $T_h-1$}
        \STATE Sample $x_h \sim \mathcal{D}$ and $\eta_{h,t} \sim \N (0, I_m)$ and simulate a trajectory with $u_{h,t} = - K_{h,t} x_{h} + \sigma \eta_{h,t}$.
        % and $u_{h'} = - K_{h',T_{h'}} x_{h'}$ for any $h' \in [h+1,\dotsc,N-1]$. Note that this $K_{h',T_{h'}}$ is essentially our $\widetilde{K}_{h'}$.
        \STATE Compute $Q_h(x_h, u_{h,t})$ for said trajectory.
        \STATE Compute the gradient estimate
        \[\widehat{\nabla} \widetilde{J}_{h,t} (K_{h, t}) = - \frac{1}{\sigma} Q_h(x_h, u_{h,t}) \eta_h x_h^\top.\]
        \vspace{-2mm}
        \STATE Update $K_{h, t+1} = K_{h, t} - \alpha_{h,t} \cdot \widehat{\nabla} \widetilde{J}_h(K_{h, t})$.
    \ENDFOR
    \STATE $\widetilde{K}_h \gets K_{h,T_{h}}$. 
    \STATE Incorporate $\widetilde{K}_h$ into the surrogate cost function for the next step, i.e., $\widetilde{J}_{h-1}(\cdot)$.
\ENDFOR
\RETURN $K_{0, T_{0}}$.
\end{algorithmic}
\end{algorithm}

Before we move on to the next result, we 
% select an arbitrary scalar $a>0$ and 
define the following constants:
\begin{align*}
    &a := \frac{\sigma_{\min} (Q)}{2} \\
    &\varphi := \max_{t \in \{0,1,\dotsc,N-1\}} \| A - B K^*_t \| \\
    &P_{\max} := \max_{t \in \{0,1,\dotsc,N-1\}} \{ P^*_t \} \\ 
    &C_1 := \frac{\varphi \| B \|}{\lambda_{\min} (R)}\\
    &C_2 := 2 \varphi \|A\| \left( 1 + \frac{\| P_{\max} + a I \| \|B\|^2}{\lambda_{\min} (R)} \right) \\
    &C_3 := 2 \| R + B^\top (P_{\max} + aI) B \|.
\end{align*}

Additionally, given a scalar $\eps >0$, we define:
\begin{align}
    \varsigma_{h,\eps} :=
    \begin{cases} 
        \min \left\{
            \sqrt{\frac{a}{C_3 N}}, 
            \sqrt{\frac{a^2}{2 e C_3 N \| P_{\max} \|}}, 
            \sqrt{\frac{a \eps}{8 e C_3 N C_1 \| P_{\max} \|}}, 
            \sqrt{\frac{\eps}{4 C_1 C_3}}
        \right\}, & h \geq 1, \\
        \frac{\epsilon}{4}, & h = 0.
    \end{cases}
    \label{eq: def_varsigma_h}
\end{align}
We now present a key result, Theorem~\ref{thm: LQR_DP_by_delta}, on the accumulation of errors that constitutes an improvement over~\cite[Theorem 2]{XZ-TB:23} (corrected version of which is stated as Theorem~\ref{thm: LQR_DP} below); as the proof of Theorem~\ref{thm: LQR_DP_by_delta} demonstrates, this improvement relies on a fundamentally different analysis.

\begin{theorem}\longthmtitle{Main result: outer loop} \label{thm: LQR_DP_by_delta}
    Select 
    \begin{equation}
    N = \frac{1}{2} \cdot \frac{\log\left(\frac{2\|Q_N - P^*\|_*\cdot \kappa_{P^*} \cdot \|A_K^*\|\cdot \|B\|}{\epsilon \cdot \lambda_{\min}(R)}\right)}{\log\left(\frac{1}{\|A_K^*\|_*}\right)} + 1, \label{eq: N_value_choice}
    \end{equation}
    where 
    $Q_N \succeq P^*$,
    and suppose that Assumption~\ref{ass: A_invertible} holds. Now assume that, for some \(\epsilon > 0\), there exists a sequence of policies \(\{\widetilde{K}_h\}_{h \in \{N-1, \dotsc, 0\}}\) such that for all \( h \in \{N-1, \dotsc, 0\} \),
    \[
    \| \widetilde{K}_h - \widetilde{K}^*_h \| \leq \varsigma_{h,\eps},
    \]
    where \(\widetilde{K}^*_h\) is the optimal policy for the Linear Quadratic Regulator (LQR) problem from step \(h\) to \(N\), incorporating errors from previous iterations of Algorithm~\ref{alg:RHPG-RL}.
    Then, the proposed algorithm outputs a control policy $\widetilde{K}_0$ that satisfies $\| \widetilde{K}_0 - K^* \| \leq \eps$. Furthermore, if $\eps$ is sufficiently small such that 
    \[
    \eps < \frac{1 - \| A - B K^* \|_*}{\| B \|},
    \]
    then $\widetilde{K}_0$ is stabilizing.
\end{theorem}

The proof of Theorem~\ref{thm: LQR_DP_by_delta} is presented in Appendix~\ref{app: thm_LQR_DP_by_Delta}. A key component of our analysis is the contraction of the Riemannian distance on the Riccati operator, as established in Lemma~\ref{lem: delta_Riccati_contraction}. This allows us to demonstrate that the accumulated error remains linear in $N$, in contrast to the exponential growth in~\cite[Theorem 2]{XZ-TB:23}.

Given this discrepancy, we revisit~\cite[Theorem 2]{XZ-TB:23} and present a revised version which accounts for some necessary, and non-trivial, modifications to make the statement accurate. For the latter reason, and the fact that this result \emph{does not} rely on Assumption~\ref{ass: A_invertible}, we provide a complete proof in Appendix~\ref{app: thm_LQR_DP}. 
\begin{theorem}\longthmtitle{Prior result: outer loop} \label{thm: LQR_DP}
Choose
\begin{equation}
N = \frac{1}{2} \cdot \frac{\log\left(\frac{2\|Q_N - P^*\|_*\cdot \kappa_{P^*} \cdot \|A_K^*\|\cdot \|B\|}{\epsilon \cdot \lambda_{\min}(R)}\right)}{\log\left(\frac{1}{\|A_K^*\|_*}\right)} + 1, \label{eq: N value choice}
\end{equation}
where $Q_N \succeq P^*$. Now assume that, for some \(\epsilon > 0\), there exists a sequence of policies \(\{\widetilde{K}_h\}_{h \in \{N-1, \dotsc, 0\}}\) such that
\begin{align}
    \| \widetilde{K}_h - \widetilde{K}^*_h \| \leq
    \begin{cases}
        \min \left\{
            \sqrt{\frac{a}{C_3}}, 
            \sqrt{\frac{a}{C_2^{h-2}C_3}}, 
            \frac{1}{2} \sqrt{\frac{\epsilon}{C_1C_2^{h-2}C_3}}
        \right\}, & h \geq 2, \\[6pt]
        \min \left\{
            \sqrt{\frac{a}{C_3}}, 
            \frac{1}{2} \sqrt{\frac{\epsilon}{C_1C_3}}
        \right\}, & h = 1, \\[6pt]
        \frac{\epsilon}{4}, & h = 0.
    \end{cases}
    \label{eq: refined_ineq_h_case}
\end{align}
where \(\widetilde{K}^*_h\) is the optimal policy for the Linear Quadratic Regulator (LQR) problem from step \(h\) to \(N\), incorporating errors from previous iterations of Algorithm~\ref{alg:RHPG-RL}. Then, the RHPG algorithm outputs a control policy $\widetilde{K}_0$ that satisfies $\| \widetilde{K}_0 - K^* \| \leq \eps$. Furthermore, if $\eps$ is sufficiently small such that 
\[
\eps < \frac{1 - \| A - B K^* \|_*}{\| B \|},
\]
then $\widetilde{K}_0$ is stabilizing.
\end{theorem}

As previously mentioned, Theorem~\ref{thm: LQR_DP_by_delta} significantly improves error accumulation, resulting in much less restrictive requirements than Theorem~\ref{thm: LQR_DP}. The limitations of Theorem~\ref{thm: LQR_DP} stem from the exponent of the constant $C_2$ in~\eqref{eq: refined_ineq_h_case}, which is discussed in detail in Appendix~\ref{app: thm_LQR_DP}. It is worth re-iterating that this improvement comes only at the cost of Assumption~\ref{ass: A_invertible}, a rather mild structural requirement. 


\section{The Inner Loop and Policy Gradient}\label{sec: inner loop}
In this section, we focus on the inner loop of Algorithm~\ref{alg:RHPG-RL}, on which we will implement our proposed policy gradient method. 

We seek a way to estimate the gradient of this function with respect to $K_h$. To remedy, we propose:
\begin{align}
    \widehat{\nabla} \widetilde{J}_h (K) := - \frac{1}{\sigma^2} Q_h(x_h, u_h) (u_h + K x_h) x_h^\top, \label{eq: grad_est_prelim}
\end{align}
where $x_h$ is sampled from $\mathcal{D}$, and then $u_h$ is chosen randomly from the Gaussian distribution $\mathcal{N} (- K x_h, \sigma^2 I_m)$ for some $\sigma > 0$. Moreover, we  rewrite $u_h \sim \mathcal{N} (- K x_h, \sigma^2 I_m)$ as
\begin{equation}
    u_h = - K x_h + \sigma \eta_h, \label{eq: random_input_rewrite}
\end{equation}
where $\eta_h \sim \N (0, I_m)$. Substituting \eqref{eq: random_input_rewrite} in \eqref{eq: grad_est_prelim} yields
\begin{align}
    \widehat{\nabla} \widetilde{J}_h (K) = - \frac{1}{\sigma} Q_h(x_h, - K x_h + \sigma \eta_h) \eta_h x_h^\top. \label{eq: grad_est_practical_form}
\end{align}
This expression corresponds to the gradient estimate utilized in Algorithm~\ref{alg:RHPG-RL}, as described in its formulation.
\begin{proposition}
    \label{prop: gradient estimate expectation}
    Suppose $x_h$ is sampled from $\mathcal{D}$ and $u_{h}$ chosen  from $\N (- K x_h, \sigma^2 I_m)$ as before. Then for any given choice of $K$, we have that
    \begin{equation}
        \E [\widehat{\nabla} \widetilde{J}_h (K)] = \nabla \widetilde{J}_h (K).
    \end{equation}    
%    for any $K \in \R^{m \times n}$.
\end{proposition} 

\begin{proof}
    Following \eqref{eq: grad_est_practical_form}, 
    % \begin{align}
    %     &\E [\widehat{\nabla} \widetilde{J}_h (K)] \cr
    %     = &\E_{x_h \sim \mathcal{D}} \left[ \E_{\eta_h \sim \N (0, I_m)} \left[ \widehat{\nabla} \widetilde{J}_h (K) \big| x_h \right] \right] \cr
    %     \overset{\mathrm{(i)}}{=} &\E_{x_h \sim \mathcal{D}} \left[ - \frac{1}{\sigma^2} \E_{\eta_{h} \sim \N (0, I_m)} \left[ Q (x_{h}, - K x_{h} + \sigma \eta_{h}) (\sigma \eta_{h}) \big| x_h \right] x_h^\top \right] \cr
    %     \overset{\mathrm{(ii)}}{=} &\E_{x_h \sim \mathcal{D}} \left[ \E_{\eta_{h} \sim \N (0, I_m)} \left[ - \nabla_{u} Q^{K} (x_{h}, u) \bigg|_{u = -K x_{h} + \sigma \eta_{h}} \big| x_h \right] x_{h}^\top \right], \label{eq: policy gradient proof before Stein}
    % \end{align} 
    \begin{align}
        \E_{x_h} [\widehat{\nabla} \widetilde{J}_h (K)] \notag &= \E_{x_h} \left[ \E_{\eta_h} \left[ \widehat{\nabla} \widetilde{J}_h (K) \big| x_h \right] \right] \notag \\
        &\overset{\mathrm{(i)}}{=} \E_{x_h} \left[ - \frac{1}{\sigma^2} \E_{\eta_h} \left[ Q (x_{h}, - K x_{h} + \sigma \eta_{h}) (\sigma \eta_{h}) \big| x_h \right] x_h^\top \right] \notag \\
        &\overset{\mathrm{(ii)}}{=} \E_{x_h} \left[ \E_{\eta_h} \left[ - \nabla_{u} Q^{K} (x_{h}, u) \bigg|_{u = -K x_{h} + \sigma \eta_{h}} \big| x_h \right] x_{h}^\top \right], \label{eq: policy gradient proof before Stein}
    \end{align}
    where (i) follows from $x_{h}^\top$ being determined when given $x_h$, and (ii) from Stein's lemma~\cite{CMS:81}. Using~\eqref{eq: Q_func_def}, we compute 
    \begin{align*}
        \nabla_{u} Q_h (x_{h}, u) &= \nabla_{u} \Bigg( x_{h}^\top Q x_{h} + u^\top R u + (A x_{h} + B u)^\top \widetilde{P}_{h+1} (A x_{h} + B u) \Bigg) \\
        &= 2 R u + 2 B^\top \widetilde{P}_{h+1} B u + 2 B^\top \widetilde{P}_{h+1} A x_{h},
    \end{align*}
    which evaluated at $u = -K x_{h} + \sigma \eta_{h}$ yields
    \begin{align*}
        \nabla_{u} Q_h (x_{h}, u) \bigg|_{u = -K x_{h} + \sigma \eta_{h}} = 2 \left( (R + B^\top \widetilde{P}_{h+1} B)(-K x_{h} + \sigma \eta_{h}) + B^\top \widetilde{P}_{h+1} A x_{h} \right).
    \end{align*}
    Substituting in \eqref{eq: policy gradient proof before Stein}, we obtain
    \begin{align*}
        \E [\widehat{\nabla} \widetilde{J}_h (K)] &= \E_{x_h \sim \mathcal{D}} \left[ 2 \left( (R + B^\top \widetilde{P}_{h+1} B)K - B^\top \widetilde{P}_{h+1} A \right) x_{h} x_{h}^\top \right] \\
        &= 2 \left( (R + B^\top \widetilde{P}_{h+1} B)K - B^\top \widetilde{P}_{h+1} A \right) \E_{x_h \sim \mathcal{D}} \left[ x_{h} x_{h}^\top \right] \\
        &= 2 \left( (R + B^\top \widetilde{P}_{h+1} B)K - B^\top \widetilde{P}_{h+1} A \right) \Sigma_0 \\
        &\overset{\mathrm{(i)}}{=} \nabla \widetilde{J}_h (K),
    \end{align*}
    where (i) follows from~\eqref{eq: actual_gradient}.
\end{proof}

Similar to~\cite{XZ-TB:23}, we define the following sets regarding the inner loop of the algorithm for each $h \in \{0,1,\dotsc,N-1\}$:
\begin{equation}
    \G_h := \{ K_h | \widetilde{J}_h (K_h) - \widetilde{J}_h(\widetilde{K}^*_h) \leq 10 \zeta^{-1} \widetilde{J}_h (K_{h,0}) \}, \label{eq: G_lqr_h_def}
\end{equation}
for some arbitrary $\zeta \in (0,1)$. We also define the following constant:
\begin{align*}
    % &P_{\max} := \max_{t \in \{0,1,\dotsc,N-1\}} \{ P^*_t \}, \\ &C_3 := 2 \| R + B^\top (P_{\max} + aI) B \|, \\
    \widetilde{C}_{h} := \frac{10 \zeta^{-1} \widetilde{J}_h (K_{h,0}) + \widetilde{J}_h(\widetilde{K}^*_h)}{\sigma_{\text{min}} (\Sigma_0) \sigma_{\text{min}} (R)}.
\end{align*} 
We now provide some bounds in the following lemma.
\begin{lemma}\label{lem: grad_est_bounds}
    Suppose $\zeta \in (0, \frac{1}{e}]$, and 
    \[
    \| \widetilde{P}_{h+1} - P^*_{h+1} \| \leq a.
    \]
    % for some fixed scalar $a>0$. 
    Then for any $K \in \G_h$, we have that
    \begin{equation}
        \| \widehat{\nabla} \widetilde{J}_h (K) \|_F \leq \xi_{h,3} \left(\log \frac{1}{\zeta}\right)^{3/2}
    \end{equation}
    with probability at least $1-\zeta$, where $\xi_{h,1}, \xi_{h,2}, \xi_{h,3} \in \real$ are given by
    \begin{align}
        \xi_{h,1} &:= \Big( \| Q \| + 2 \| R \| \widetilde{C}_{h}^2 + 2 (\| P_{\max} \| + a) (\|A\|^2 + 2 \|B\|^2 \widetilde{C}_{h}) \Big)  C_m^{3/2}, \\
        \xi_{h,2} &:= 2 \left( \| R \| + 2 (\| P_{\max} \| + a) \| B \|^2 \right) C_m^{1/2}, \\
        \xi_{h,3} &:= \frac{1}{\sigma} \left(\xi_{h,1} 5^{1/2} m^{1/2}\right) + \sigma \left( \xi_{h,2} 5^{3/2} m^{3/2} \right). \label{eq: xi_3 definition}
    \end{align}
    Moreover, 
    \begin{equation}
        \E\left[ \| \widehat{\nabla} \widetilde{J}_h (K) \|_F^2 \right] \leq \xi_{h,4},
    \end{equation}
    where    
    \begin{align}\label{eq: xi_4 definition}
        \xi_{h,4} &:= \frac{1}{\sigma^2} \xi_{h,1}^2 m + 2 \xi_{h,1} \xi_{h,2} m (m+2) + \sigma^2 \xi_{h,2}^2 m (m+2) (m+4).
    \end{align}
\end{lemma}
\begin{proof}
    Using the Formulation of $\widehat{\nabla} \widetilde{J}_h (K)$ derived in \eqref{eq: grad_est_practical_form}, we have
    \begin{align}
        \| \widehat{\nabla} \widetilde{J}_h (K) \|_F &= \| \frac{1}{\sigma} Q_h(x_h, - K x_h + \sigma \eta_h) \eta_h x_h^\top \|_F \cr
        &\leq \frac{1}{\sigma} Q_h(x_h, - K x_h + \sigma \eta_h) \| \eta_h \| \| x_h \|. \label{eq: lem_grad_est_bounds_1}
    \end{align}
    Before we continue, we provide the following bound:    \begin{sublemma}\label{sublem: K_size_bound}
        Suppose $K \in \G_h$. Then it holds that
        \begin{equation}
            \| K \|^2_F \leq \widetilde{C}_{h}.
        \end{equation}
    \end{sublemma}
    \emph{Proof of Sublemma~\ref{sublem: K_size_bound}.}
    Using \eqref{eq: J_h_tilde_definition}, we have
    \begin{align}
        \widetilde{J}_h (K) &\geq \E_{x_h \sim \D} \left[ x_h^\top (Q+K^\top R K) x_h \right] \cr
        &= \E_{x_h \sim \D} \left[ \tr \left( (Q+K^\top R K) x_h x_h^\top \right) \right] \cr
        &= \tr \left( (Q+K^\top R K) \Sigma_0 \right) \cr
        &\geq \sigma_{\text{min}} (\Sigma_0) \tr (Q+K^\top R K) \cr
        &\geq \sigma_{\text{min}} (\Sigma_0) \tr (R K K^\top) \cr
        &\geq \sigma_{\text{min}} (\Sigma_0) \sigma_{\text{min}} (R) \| K \|^2_F. \label{eq: sublem_K_size_bound_1}
    \end{align}
    Rearranging \eqref{eq: sublem_K_size_bound_1} yields
    \begin{align*}
        \| K \|^2_F &\leq \frac{\widetilde{J}_h (K)}{\sigma_{\text{min}} (\Sigma_0) \sigma_{\text{min}} (R)} \\
        &\overset{\mathrm{(i)}}{\leq} \frac{10 \zeta^{-1} \widetilde{J}_h (K_{h,0}) + \widetilde{J}_h(\widetilde{K}^*_h)}{\sigma_{\text{min}} (\Sigma_0) \sigma_{\text{min}} (R)} \\
        &= \widetilde{C}_{h},
    \end{align*}
    where (i) follows from the definition of the set $\G_h$ in \eqref{eq: G_lqr_h_def}. This concludes the proof of Sublemma~\ref{sublem: K_size_bound}. \oprocend
    
        We now continue with the proof of the Lemma~\ref{lem: grad_est_bounds}. Note that
        \begin{align*}
            &Q_h(x_h, - K x_h + \sigma \eta_h) \cr
            = &x_h^\top Q x_h + (- K x_h + \sigma \eta_h)^\top R (- K x_h + \sigma \eta_h) + (A x_h + B(- K x_h + \sigma \eta_h))^\top \widetilde{P}_{h+1} (A x_h + B(- K x_h + \sigma \eta_h)) \cr
            \leq &\| Q \| C_m + \| R \| \| - K x_h + \sigma \eta_h \|^2 + \| \widetilde{P}_{h+1} \| \| A x_h + B(- K x_h + \sigma \eta_h) \|^2. \end{align*} 
    As a result,
    \begin{align}
        &Q_h(x_h, - K x_h + \sigma \eta_h) \cr \leq &\| Q \| C_m + 2 \| R \| (\widetilde{C}_{h} C_m + \sigma^2 \| \eta_h \|^2) + 2 (\| P_{\max} \| + a) \|A\|^2 C_m + 4 (\| P_{\max} \| + a)  \|B\|^2 (\widetilde{C}_{h} C_m + \sigma^2 \| \eta_h \|^2) ) \cr
        = &C_m( \|Q\| + 2 \|R\| \widetilde{C}_{h}) + 2 C_m(\| P_{\max} \| + a) ( \|A\|^2 + 2 \|B\|^2 \widetilde{C}_{h} )) + 2 \left( \|R\| + 2 (\| P_{\max} \| + a) \|B\|^2 \right) \sigma^2 \| \eta_h \|^2, \label{eq: lem_grad_est_bounds_2}
    \end{align}  
         where the inequality follows from Sublemma~\ref{sublem: K_size_bound} along with the fact that by the assumption, 
    \begin{align*}
    \| \widetilde{P}_{h+1} \| &= \| P^*_{h+1} + (\widetilde{P}_{h+1} - P^*_{h+1}) \| \cr
    &\leq \| P^*_{h+1} \| + \| \widetilde{P}_{h+1} - P^*_{h+1} \| \cr
    &\leq \| P_{\max} \| + a.
    \end{align*}
    Combining \eqref{eq: lem_grad_est_bounds_1} with \eqref{eq: lem_grad_est_bounds_2} and \eqref{eq: noise_assumptions}, we obtain
    \begin{align}
        &\| \widehat{\nabla} \widetilde{J}_h (K) \|_F \cr
        \leq &\frac{1}{\sigma} \Bigg( \|Q\| + 2 \|R\| \widetilde{C}_{h} + 2 (\| P_{\max} \| + a) ( \|A\|^2 + 2 \|B\|^2 \widetilde{C}_{h} ) \Bigg) C_m^{3/2} \| \eta_h \| \cr
        &+ 2 \left( \|R\| + 2 (\| P_{\max} \| + a) \|B\|^2 \right) \sigma C_m^{1/2} \| \eta_h \|^3 \cr
        = &\frac{1}{\sigma} \xi_{h,1} \| \eta_h \| + \sigma \xi_{h,2} \| \eta_h \|^3. \label{eq: grad_est_size_bound_before_LM}
    \end{align}
    Furthermore, since $\eta_{h} \sim \N (0, I_m)$ for any $h$, $\| \eta_{h} \|^2$ is distributed according to the chi-squared distribution with $m$ degrees of freedom ($\| \eta_{h} \|^2 \sim \chi^2 (m)$ for any $h$). Therefore, the standard Laurent-Massart bounds~\cite{BL-PM:00} suggest that for arbitrary $y>0$, we have that
    \begin{equation}
        \PP \{ \| \eta_{h} \|^2 \geq m + 2 \sqrt{m y} + 2y \} \leq e^{-y}. \label{eq: Lauren-Massart Chi concentration}
    \end{equation}
    Now if we take $y = m \log \frac{1}{\zeta}$, since $\zeta \in (0,1/e)$ by our assumption, it holds that $y = m \log \frac{1}{\zeta} \geq m$. Thus
    \begin{align*}
        \PP \{ \| \eta_{h} \|^2 \geq 5y \} &\leq \PP \{ \| \eta_{h} \|^2 \geq m + 2 \sqrt{m y} + 2y \} \\
        &\leq e^{-y},
    \end{align*}
    which after substituting $y$ with its value $m \log \frac{1}{\zeta}$ gives
    \begin{align*}
        \PP \{ \| \eta_{h} \|^2 \geq 5 m \log \frac{1}{\zeta} \} \leq e^{-m \log \frac{1}{\zeta}} = \zeta^m \leq \zeta.
    \end{align*}
    As a result, we have $\| \eta_{h} \| \leq 5^{1/2} m^{1/2} (\log \frac{1}{\zeta})^{1/2}$ and consequently 
    \[
    \| \eta_{h} \|^3 \leq 5^{3/2} m^{3/2} (\log \frac{1}{\zeta})^{3/2}
    \]
    with probability at least $1-\zeta$, which after applying on \eqref{eq: grad_est_size_bound_before_LM} yields
    \begin{align*}
    \| \widehat{\nabla} \widetilde{J}_h (K) \|_F &\leq \frac{1}{\sigma} \xi_{h,1} 5^{1/2} m^{1/2} \left(\log \frac{1}{\zeta}\right)^{1/2} + \sigma \xi_{h,2} 5^{3/2} m^{3/2} \left(\log \frac{1}{\zeta}\right)^{3/2} \\
    &\leq \left( \frac{1}{\sigma} \xi_{h,1} 5^{1/2} m^{1/2} + \sigma \xi_{h,2} 5^{3/2} m^{3/2} \right) \left(\log \frac{1}{\zeta}\right)^{3/2} \\
    &= \xi_{h,3} \left(\log \frac{1}{\zeta}\right)^{3/2},
    \end{align*}
    proving the first claim.
    As for the second claim, note that using~\eqref{eq: grad_est_size_bound_before_LM}, we have
    \begin{align}
        \| \widehat{\nabla} \widetilde{J}_h (K) \|^2_F \leq \frac{1}{\sigma^2} \xi_{h,1}^2 \| \eta_h \|^2 + 2 \xi_{h,1} \xi_{h,2} \| \eta_h \|^4 + \sigma^2 \xi_{h,2}^2 \| \eta_h \|^6. \label{eq: before_chi_moments}
    \end{align}
    Now since $\| \eta_h \| \sim \chi (m)$ whose moments are known, taking an expectation on~\eqref{eq: before_chi_moments} results in
    \begin{align*}
        \E \left[ \| \widehat{\nabla} \widetilde{J}_h (K) \|^2_F \right] &\leq \frac{1}{\sigma^2} \xi_{h,1}^2 \E [\| \eta_{\hat{t}} \|^2] + 2 \xi_{h,1} \xi_{h,2} \E [\| \eta_{\hat{t}} \|^4] + \sigma^2 \xi_{h,2}^2 \E [\| \eta_{\hat{t}} \|^6] \\
        &= \frac{1}{\sigma^2} \xi_{h,1}^2 m + 2 \xi_{h,1} \xi_{h,2} m (m+2) + \sigma^2 \xi_{h,2}^2 m (m+2) (m+4) \\
        &= \xi_{h,4},
    \end{align*}
    concluding the proof.
\end{proof}

% We now define the following constants:
% \[
% L := C_3 \| \Sigma_0 \|, \quad \mu := 4 \sigma_{\min} (\Sigma_0) \sigma_{\min} (R),
% \]
We next provide some useful properties of the cost function $\widetilde{J}_h (K)$ in the following lemma.
\begin{lemma}\label{lem: cost function properties}
For all $h \in \{0,1,\dotsc,N-1\}$, the function $\widetilde{J}_h$ is $\frac{\mu}{2}$-strongly convex, where
\[
\mu := 4 \sigma_{\min} (\Sigma_0) \sigma_{\min} (R),
\]
and in particular, for all $K \in \real^{m \times n}$,
\begin{equation}
    \| \nabla \widetilde{J}_h (K) \|_F^2 \geq \mu (\widetilde{J}_h (K) - \widetilde{J}_h (\widetilde{K}^*_h)),
\end{equation}
where $\widetilde{K}^*_h$ is the global minimizer of $\widetilde{J}_h$. Moreover, assuming that $\| \widetilde{P}_{h+1} - P^*_{h+1} \| \leq a$, we have that for all $K_1,K_2 \in \real^{m \times n}$,
\begin{equation}
    \| \nabla \widetilde{J}_h (K_2) - \nabla \widetilde{J}_h (K_1) \|_F \leq L \| K_2 - K_1 \|_F,
\end{equation}
where 
\[
L := C_3 \| \Sigma_0 \|. 
\]
\end{lemma}
\begin{proof}
We first prove the strong convexity as follows:
\begin{align*}
    \left\langle \nabla \widetilde{J}_h (K_2) - \nabla \widetilde{J}_h (K_1), K_2 - K_1 \right\rangle &= 2 \tr \left( \Sigma_0 (K_2 - K_1)^\top (R + B^\top \widetilde{P}_{h+1} B) (K_2 - K_1) \right) \\
    &\geq 2 \sigma_{\min} (\Sigma_0) \sigma_{\min} (R) \tr \left( (K_2 - K_1)^\top (K_2 - K_1) \right) \\
    &= \frac{\mu}{2} \| K_2 - K_1 \|_F^2.
\end{align*}
Note the the next inequality is an immediate 
consequence of the PL-inequality. Now we move on to the $L$-smoothness property:
\begin{align*}
    \| \nabla \widetilde{J}_h (K_2) - \nabla \widetilde{J}_h (K_1) \|_F &= \| 2 (R + B^\top \widetilde{P}_{h+1} B) (K_2 - K_1) \Sigma_0 \|_F \\
    &\leq \| \Sigma_0 \| (2 \| R + B^\top \widetilde{P}_{h+1} B \|) \| K_2 - K_1 \|_F \\
    &\leq \| \Sigma_0 \| (2 \| R + B^\top (P_{\max} + a I) B \|) \| K_2 - K_1 \|_F \\
    &= \| \Sigma_0 \| C_3 \| K_2 - K_1 \|_F \\
    &= L \| K_2 - K_1 \|_F,
\end{align*}
concluding the proof.
\end{proof}

Before introducing the next result, let us denote the optimality gap of iterate $t$ by
% of the algorithm at each $h \in \{0,1,\dotsc,N-1\}$:
\begin{equation}
    \Delta_{t} = \widetilde{J}_h (K_{h,t}) - \widetilde{J}_h (\widetilde{K}^*_h). \label{eq: delta_def}
\end{equation}
Moreover, let $\F_{t}$ denote the $\sigma$-algebra containing the randomness up to iteration $t$ of the inner loop of the algorithm for each $h \in \{0,1,\dotsc,N-1\}$ (including $K_{h,t}$ but not $\widehat{\nabla} \widetilde{J}_h (K_{h,t})$). We then define
\begin{equation}
    \tau := \min \left\{ t \ |  \ \Delta_{t} > 10 \zeta^{-1} \widetilde{J}_h(K_{h,0}) \right\}, \label{eq: tau_1 definition}
\end{equation}
which is a stopping time with respect to $\F_{t}$. Note that we did some notation abuse as $\Delta_t, \F_t,$ and $\tau$ may differ for each $h \in \{0,1,\dotsc,N-1\}$. But since these steps $h$ of the outer loop do not impact one another, we used just one notation for simplicity.
\begin{lemma}
    \label{lem: policy gradient recursive}
    Suppose $\| \widetilde{P}_{h+1} - P^*_{h+1} \| \leq a$, and the update rule follows
    \begin{equation}
        K_{h,t+1} = K_{h,t} - \alpha_{h,t} \widehat{\nabla} \widetilde{J}_h (K_{h,t}), \label{eq: our_update}
    \end{equation}
    where $\alpha_{h,t} > 0$ is the step-size. Then for any $t \in \{0,1,2,\dotsc\}$, we have
    \begin{equation}
        \E[\Delta_{t+1} | \F_{t}] 1_{\tau > t} \leq \left( \left(1 - \mu \alpha_{h,t} \right) \Delta_t + \frac{L \alpha_{h,t}^2}{2} \xi_{h,4} \right) 1_{\tau > t}, \label{eq: policy gradient recursive}
    \end{equation}
    where $\Delta_{t}$ is defined in \eqref{eq: delta_def}.
\end{lemma}
\begin{proof}
    First, note that by $L$-smoothness, we have
    \begin{align*}
        \Delta_{t+1} - \Delta_{t} &= \widetilde{J}_h (K_{h,t+1}) - \widetilde{J}_h (K_{h,t}) \\
        &\leq \langle \nabla \widetilde{J}_h (K_{h,t}), K_{h,t+1} - K_{h,t} \rangle 
        + \frac{L}{2} \| K_{h,t+1} - K_{h,t} \|_F^2 \\
        &= -\alpha_{h,t} \langle \nabla \widetilde{J}_h (K_{h,t}), \widehat{\nabla} \widetilde{J}_h (K_{h,t}) \rangle + \frac{L \alpha_{h,t}^2}{2} \| \widehat{\nabla} \widetilde{J}_h (K_{h,t}) \|_F^2,
    \end{align*}
    which after multiplying by $1_{\tau > t}$ (which is determined by $\F_{t}$) and taking an expectation conditioned on $\F_{t}$ gives
    \begin{align}
        \E [\Delta_{t+1} - \Delta_t | \F_t] 1_{\tau > t} &\leq -\alpha_{h,t} \langle \nabla \widetilde{J}_h (K_{h,t}), \E [\widehat{\nabla} \widetilde{J}_h (K_{h,t}) | \F_t] \rangle 1_{\tau > t} + \frac{L \alpha_{h,t}^2}{2} \E [\| \widehat{\nabla} \widetilde{J}_h (K_{h,t}) \|_F^2 | \F_t] 1_{\tau > t} \cr
        &\overset{\mathrm{(i)}}{=} -\alpha_{h,t} \| \nabla \widetilde{J}_h (K_{h,t}) \|_F^2 1_{\tau > t} + \frac{L \alpha_{h,t}^2}{2} \xi_{h,4} 1_{\tau > t} \cr
        &\overset{\mathrm{(ii)}}{\leq} -\alpha_{h,t} \mu \Delta_t 1_{\tau > t} + \frac{L \alpha_{h,t}^2}{2} \xi_{h,4} 1_{\tau > t}, \label{eq: lem_recursive_prelim}
    \end{align}
    where (i) follows from Proposition~\ref{prop: gradient estimate expectation}, Lemma~\ref{lem: grad_est_bounds} along with the fact that the event $\{\tau > t\}$ implies $K_{h,t} \in \G_h$, and (ii) is due to Lemma~\ref{lem: cost function properties}. 
    
    Now after some rearranging on~\eqref{eq: lem_recursive_prelim} and noting that $\Delta_t$ is also determined by $\F_t$, we conclude that 
    \begin{align}
        \E [\Delta_{t+1} | \F_t] 1_{\tau > t} \leq \left( \left(1 - \mu \alpha_{h,t} \right) \Delta_t + \frac{L \alpha_{h,t}^2}{2} \xi_{h,4} \right) 1_{\tau > t},
    \end{align}
    finishing the proof.
\end{proof}

We are now in a position to state a precise version of our main result for the inner loop.

\begin{theorem}\label{thm: policy_gradient}\longthmtitle{Main result: inner loop}
    Suppose $\| \widetilde{P}_{h+1} - P^*_{h+1} \| \leq a$. 
    % for some scalar $a>0$. 
    For any $h \in \{0,1,\dotsc,N-1\}$, if the step-size is chosen as
    \begin{equation}
        \alpha_{h,t} = \frac{2}{\mu} \frac{1}{t+\theta_h} \quad \text{for} \quad \theta_h = \max \{2,\frac{2 L \xi_{h,4}}{\mu^2 \widetilde{J}_h (K_{h,0})}\}, \label{eq: step-size value choice}
    \end{equation}
    then for a given error tolerance $\varsigma$, the iterate $K_{h,T_h}$ of the update rule~\eqref{eq: our_update}
    after
    \[
    T_h = \frac{40}{7 \mu \varsigma^2 \zeta} \theta_h \widetilde{J}_h (K_{h,0})
    \]
    steps satisfies
    \[
    \| K_{h,T_h} - \widetilde{K}^*_{h} \|_F \leq \varsigma,
    \]
    with a probability of at least $1 - \zeta$.
\end{theorem}

The proof of this result relies heavily on Proposition~\ref{prop: lqr_policy}, which we establish next.
\begin{proposition}\label{prop: lqr_policy}
    Under the parameter settings of Theorem~\ref{thm: policy_gradient}, we have that
    \[
    \E[\Delta_{T_h} 1_{\tau > T_h}] \leq \frac{7}{40} \mu \varsigma^2 \zeta.
    \]
    Moreover, the event $\{ \tau \geq T_h \}$ happens with probability of at least $\frac{3}{10} \zeta$.
\end{proposition}
\begin{proof}
    We dedicate the following sublemma to prove the first claim.     \begin{sublemma}\label{sublem: lqr_pg_inductive}
        Under the parameter setup of Theorem~\ref{thm: policy_gradient}, we have that
        \[
        \E[\Delta_t 1_{\tau > t}] \leq \frac{\theta_h \widetilde{J}_h (K_{h,0})}{t+\theta_h},
        \]
        for all $t \in [T_h]$.
    \end{sublemma}
    \emph{Proof of Sublemma~\ref{sublem: lqr_pg_inductive}.} We prove this result by induction on $t$ as follows:
    
    \textbf{Base case ($t = 0$):} 
    \begin{align*}
        \Delta_0 1_{\tau > 0} \leq \Delta_0 \leq \widetilde{J}_h (K_{h,0}) = \frac{\theta_h \widetilde{J}_h (K_{h,0})}{0 + \theta_h},
    \end{align*}
    which after taking expectation proves the claim for $t = 0$.
    
    \textbf{Inductive step:} Let $k \in [T_h-1]$ be fixed and assume that
    \begin{equation}
        \E[\Delta_k 1_{\tau > k}] \leq \frac{\theta_h \widetilde{J}_h (K_{h,0})}{k+\theta_h} \label{eq: sublem_pg_hyp}
    \end{equation}
    holds (the inductive hypothesis). Observe that
    \begin{align}
        \E[\Delta_{k+1} 1_{\tau > k+1}] &\overset{\mathrm{(i)}}{\leq} \E [\Delta_{k+1} 1_{\tau > k} ] \cr
        &= \E [\E[\Delta_{k+1} 1_{\tau > k} | \F_k]] \cr
        &\overset{\mathrm{(ii)}}{=} \E [\E[\Delta_{k+1} | \F_k] 1_{\tau > k}], \label{eq: sublem_pg_conditional_expectation_bound}
    \end{align}
    where (i) comes from $1_{\tau > k+1} \leq 1_{\tau > k}$ and (ii) from the fact that $1_{\tau > k}$ is determined by $\F_k$. By Lemma~\ref{lem: policy gradient recursive}, we have that
    \begin{align}
        \E[\Delta_{k+1} | \F_k] 1_{\tau > k} &\leq \left( \left(1 - \mu \alpha_k \right) \Delta_k + \frac{L \alpha_k^2}{2} \xi_{h,4} \right) 1_{\tau > k} \cr
        &\overset{\mathrm{(i)}}{=} \left( 1 - \frac{2}{k+\theta_h} \right) \Delta_k 1_{\tau > k} + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{1}{k+\theta_h} \right)^2, \label{eq: induction prelim 1}
    \end{align}
    where (i) comes from replacing $\alpha_k$ with its value in Theorem~\ref{thm: policy_gradient} along with the fact that $1_{\tau > k} \leq 1$. Now taking an expectation on~\eqref{eq: induction prelim 1} and combining it with~\eqref{eq: sublem_pg_conditional_expectation_bound} yields
    \begin{align}
        \E[\Delta_{k+1} 1_{\tau > k+1}] &\leq \left( 1 - \frac{2}{k+\theta_h} \right) \E[\Delta_k 1_{\tau > k}] + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{1}{k+\theta_h} \right)^2 \cr
        &\overset{\mathrm{(i)}}{\leq} \left( 1 - \frac{2}{k+\theta_h} \right) \frac{\theta_h \widetilde{J}_h (K_{h,0})}{k+\theta_h} + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{1}{k+\theta_h} \right)^2 \cr
        &= \left( 1 - \frac{1}{k+\theta_h} \right) \frac{\theta_h \widetilde{J}_h (K_{h,0})}{k+\theta_h} - \frac{1}{(k+\theta_h)^2} \left( \theta_h \widetilde{J}_h (K_{h,0}) - \frac{2 L \xi_{h,4}}{\mu^2} \right) \cr
        &\overset{\mathrm{(ii)}}{\leq} \frac{k+\theta_h-1}{(k+\theta_h)^2} \theta_h \widetilde{J}_h (K_{h,0}) \cr
        &\leq \frac{1}{k+\theta_h+1} \theta_h \widetilde{J}_h (K_{h,0}),
    \end{align}
    where (i) comes from the induction hypothesis~\eqref{eq: sublem_pg_hyp}, and (ii) from 
    \[
    \theta_h \widetilde{J}_h (K_{h,0}) - \frac{2 L \xi_{h,4}}{\mu^2} \geq 0,
    \]
    which is due to the choice of $\theta_h$ in Theorem~\ref{thm: policy_gradient}. This proves the claim for $k+1$, completing the inductive step. \oprocend

    Now utilizing Sublemma~\ref{sublem: lqr_pg_inductive} along with the choice of $T_h$ in Theorem~\ref{thm: policy_gradient}, we have
    \[
    \E[\Delta_{T_h} 1_{\tau > T_h}] \leq \frac{\theta_h \widetilde{J}_h (K_{h,0})}{T_h + \theta_h} \leq \frac{\theta_h \widetilde{J}_h (K_{h,0})}{T_h} \leq \frac{7 \mu \varsigma^2 \zeta}{40},
    \]
    concluding the proof of the first claim of Proposition~\ref{prop: lqr_policy}. Moving on to the second claim, we start by introducing the stopped process
    \begin{equation}
        Y_{t} := \Delta_{t \wedge \tau} + \frac{4 L \xi_{h,4}}{\mu^2} \frac{1}{t+\theta_h}. \label{eq: y_t supermartingale def}
    \end{equation}
    We now show this process is a supermartingale. First, observe that
    \begin{align}
        \E[Y_{t+1} | \F_t] &= \E [\Delta_{t+1 \wedge \tau} | \F_t] + \frac{4 L \xi_{h,4}}{\mu^2} \frac{1}{t+\theta_h+1} \cr
        &= \E [\Delta_{t+1 \wedge \tau} (1_{\tau \leq t} + 1_{\tau > t}) | \F_t] + \frac{4 L \xi_{h,4}}{\mu^2} \frac{1}{t+\theta_h+1} \cr
        &= \E [\Delta_{t+1 \wedge \tau} 1_{\tau \leq t} | \F_t] + \E [\Delta_{t+1 \wedge \tau} | \F_t] 1_{\tau > t} + \frac{4 L \xi_{h,4}}{\mu^2} \frac{1}{t+\theta_h+1}. \label{eq: supermartingale_prelim_1}
    \end{align}
    Now note that for the first term of the right-hand side of~\eqref{eq: supermartingale_prelim_1}, it holds that
    \begin{align}
        \E [\Delta_{t+1 \wedge \tau} 1_{\tau \leq t} | \F_t] \overset{\mathrm{(i)}}{=} \E [\Delta_{t \wedge \tau} 1_{\tau \leq t} | \F_t] = \Delta_{t \wedge \tau} 1_{\tau \leq t}, \label{eq: supermartingale_prelim_2}
    \end{align}
    where (i) follows from the fact that under the event $\{ \tau \leq t \}$, we have $\Delta_{t+1 \wedge \tau} = \Delta_{t \wedge \tau}$. Moreover, for the second term of the right-hand side of~\eqref{eq: supermartingale_prelim_1}, we have that
    \begin{align}
        \E [\Delta_{t+1 \wedge \tau} | \F_t] 1_{\tau > t} &\overset{\mathrm{(i)}}{\leq} \left( 1 - \frac{2}{t+\theta_h} \right) \Delta_t 1_{\tau > t} + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{1}{t+\theta_h} \right)^2 1_{\tau > t} \cr
        &\leq \Delta_t 1_{\tau > t} + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{1}{t+\theta_h} \right)^2, \label{eq: supermartingale_prelim_3}
    \end{align}
    where (i) follows from Lemma~\ref{lem: policy gradient recursive}. Combining \eqref{eq: supermartingale_prelim_2} and \eqref{eq: supermartingale_prelim_3} with \eqref{eq: supermartingale_prelim_1}, we get
    \begin{align}
        \E[Y_{t+1} | \F_t] &\leq \Delta_{t \wedge \tau} 1_{\tau \leq t} + \Delta_t 1_{\tau > t} + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{1}{t+\theta_h} \right)^2 + \frac{4 L \xi_{h,4}}{\mu^2} \frac{1}{t+\theta_h+1} \cr
        &= \Delta_{t \wedge \tau} + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{1}{(t+\theta_h)^2} + \frac{2}{t+\theta_h+1} \right) \cr
        &\overset{\mathrm{(i)}}{\leq} \Delta_{t \wedge \tau} + \frac{2 L \xi_{h,4}}{\mu^2} \left( \frac{2}{t+\theta_h} \right) \cr
        &= Y_t,
    \end{align}
    where (i) follows from $\theta_h \geq 2$ under parameter choice of Theorem~\ref{thm: policy_gradient}. This finishes the proof of $Y_t$ being a supermartingale. Now note that
    \begin{align*}
        \PP \{ \tau \leq T_h \} &= \PP \left\{\max_{t \in [T_h]} \Delta_{t} > 10 \zeta^{-1} \widetilde{J}_h (K_{h,0}) \right\} \cr
        &\leq \PP \left\{\max_{t \in [T_h]} \Delta_{t \wedge \tau} > 10 \zeta^{-1} \widetilde{J}_h (K_{h,0}) \right\} \cr
        &\overset{\mathrm{(i)}}{\leq} \PP \left\{\max_{t \in [T_h]} Y_t \geq 10 \zeta^{-1} \widetilde{J}_h (K_{h,0}) \right\}, 
\end{align*}    
where (i) follows from the fact that $Y_t \geq \Delta_{t \wedge \tau}$. Using Doob/Ville's inequality for supermartingales, we have that 
\begin{align*}
        \PP \{ \tau \leq T_h \}&{\leq} \frac{\zeta \E[Y_0]}{10 \widetilde{J}_h (K_{h,0})} = \frac{\zeta \left(\Delta_0 + \frac{4 L \xi_{h,4}}{\mu^2} \frac{1}{\theta_h} \right)}{10 \widetilde{J}_h (K_{h,0})}. 
\end{align*}
Using the choice of $\theta_h$ in Theorem~\ref{thm: policy_gradient}, we have that 
\begin{align}
        \PP \{ \tau \leq T_h \}&{\leq} \frac{\zeta \left(\widetilde{J}_h (K_{h,0}) + 2 \widetilde{J}_h (K_{h,0}) \right)}{10 \widetilde{J}_h (K_{h,0})} \cr
        &= \frac{3}{10} \zeta.
    \end{align}
     This verifies the second claim of Proposition~\ref{prop: lqr_policy}, concluding the proof.
\end{proof}

With this in mind, the proof of Theorem~\ref{thm: policy_gradient} is straightforward:

\textit{Proof of Theorem~\ref{thm: policy_gradient}:}
    We now employ Proposition~\ref{prop: lqr_policy} to validate the claims of Theorem~\ref{thm: policy_gradient}. Note that 
\begin{align*}
    \PP \left\{ \Delta_{T_h} \geq \frac{\mu}{4} \varsigma^2 \right\} &\leq \PP \left\{ \Delta_{T_h} 1_{\tau > T_h} \geq \frac{\mu}{4} \varsigma^2 \right\} + \PP \left\{ 1_{\tau \leq T_h} = 1 \right\} \\
    &\overset{\mathrm{(i)}}{\leq} \frac{4}{\mu \varsigma^2} \E[\Delta_{T_h} 1_{\tau > T_h}] + \PP \left\{ \tau \leq T_h \right\} \\
    &\overset{\mathrm{(ii)}}{\leq} \frac{7}{10} \zeta + \frac{3}{10} \zeta \\
    &= \zeta,
\end{align*}
where (i) follows from applying Markov's inequality on the first claim of Proposition~\ref{prop: lqr_policy}, and (ii) comes directly from the second claim of Proposition~\ref{prop: lqr_policy}. Finally, we utilize the $\frac{\mu}{2}$-strong convexity of $\widetilde{J}_h$, along with $\nabla \widetilde{J}_h (\widetilde{K}^*_h) = 0$ to write
\begin{align*}
    \widetilde{J}_h (K_{h,T_h}) - \widetilde{J}_h (\widetilde{K}^*_h) &\geq \nabla \widetilde{J}_h (\widetilde{K}^*_h)^\top (K_{h,T_h} - \widetilde{K}^*_h) + \frac{\mu}{4} \| K_{h,T_h} - \widetilde{K}^*_h \|_F^2 \\
    &= \frac{\mu}{4} \| K_{h,T_h} - \widetilde{K}^*_h \|_F^2,
\end{align*}
and hence,
\begin{align*}
    \| K_{h,T_h} - \widetilde{K}^*_h \|_F^2 \leq \frac{4}{\mu} \left(\widetilde{J}_h (K_{h,T_h}) - \widetilde{J}_h (\widetilde{K}^*_h)\right) \leq \varsigma^2,
\end{align*}
with a probability of at least $1-\zeta$, finishing the proof. \oprocend
\section{Sample complexity}
We now utilize our results on inner loop and outer loop to provide sample complexity bounds. To wit, combining Theorems~\ref{thm: policy_gradient} and~\ref{thm: LQR_DP_by_delta}, along with applying the union bound on the probabilities of failure at each step, we provide the following result.
\begin{corollary}\label{cor: overall_result}
    Suppose Assumption~\ref{ass: A_invertible} holds, and choose
    \begin{align*}
        N = \frac{1}{2}\cdot \frac{\log\big(\frac{2\|Q_N-P^*\|_*\cdot\kappa_{P^*}\cdot \|A_K^*\|\cdot\|B\|} {\epsilon\cdot\lambda_{\min}(R)}\big)}{\log\big(\frac{1}{\|A_K^*\|_*}\big)} + 1,
    \end{align*}
    where $Q_N \succeq P^*$. Moreover, for each \(h \in \{0, 1, \dotsc, N-1\}\), let \(\varsigma_{h,\eps}\) be as defined in~\eqref{eq: def_varsigma_h}. Then Algorithm~\ref{alg:RHPG-RL} with the parameters as suggested in Theorem~\ref{thm: policy_gradient}, i.e., 
    \begin{align*}
        \alpha_{h,t} = \frac{2}{\mu} \frac{1}{t+\theta_h} \quad \text{for} \quad \theta_h = \max \{2,\frac{2 L \xi_{h,4}}{\mu^2 \widetilde{J}_h (K_{h,0})}\},
    \end{align*}
    and
    \[
    T_h = \frac{40}{7 \mu \varsigma_{h,\eps}^2 \zeta} \theta_h \widetilde{J}_h (K_{h,0}),
    \]
    outputs a control policy $\widetilde{K}_0$ that satisfies $\| \widetilde{K}_0 - K^* \| \leq \eps$ with a probability of at least $1 - N \zeta$. Furthermore, if $\eps$ is sufficiently small such that 
    \[
    \eps < \frac{1 - \| A - B K^* \|_*}{\| B \|},
    \]
    then $\widetilde{K}_0$ is stabilizing.
\end{corollary}

The results in Corollary~\ref{cor: overall_result} provide a rigorous theoretical foundation for Algorithm~\ref{alg:RHPG-RL}, ensuring it computes a control policy \( \widetilde{K}_0 \) satisfying \( \| \widetilde{K}_0 - K^* \| \leq \epsilon \) with high probability. The following corollary formalizes the sample complexity bound of our approach.

\begin{corollary}\longthmtitle{Main result: complexity bound}
Under Assumption~\ref{ass: A_invertible}, Algorithm~\ref{alg:RHPG-RL} achieves a sample complexity bound of at most  
\[
\sum_{h=0}^{N-1} T_h = \widetilde{\mathcal{O}}\left(\epsilon^{-2}\right).
\]
\end{corollary}
It is worth comparing this result with the one in~\cite{XZ-TB:23}, 
taking into account the necessary adjustments ala Theorem~\ref{thm: LQR_DP}, where error accumulation results in a worse sample complexity bound. 

\begin{corollary}\longthmtitle{Prior Result: Complexity Bound}
Algorithm~1 in~\cite{XZ-TB:23} 
% If the approach in~\cite{XZ-TB:23} were analyzed using either our inner loop or their own, 
achieves the sample complexity bound of at most 
\[
\sum_{h=0}^{N-1} T'_h = \widetilde{\mathcal{O}}\left(\max \left\{\epsilon^{-2}, \epsilon^{- \left( 1 + \frac{\log(C_2)}{2\log{(1/\| A^*_K \|_*})} \right)} \right\}\right),
\]  
where \( T'_h \) denotes the counterpart of \( T_h \) in~\cite{XZ-TB:23}.
\end{corollary}


This comparison highlights the advantage of our method, which achieves a uniform sample complexity bound of \( \widetilde{\mathcal{O}}(\epsilon^{-2}) \), independent of problem-specific constants. In contrast, the bound in~\cite{XZ-TB:23} deteriorates as \( C_2 \) increases, since their second term scales as  
\[
\widetilde{\mathcal{O}}\left(\epsilon^{-\left(1 + \frac{\log(C_2)}{2\log(1/\| A^*_K \|_*)} \right)}\right).
\]
This can be \textit{arbitrarily worse} than \( \widetilde{\mathcal{O}}(\epsilon^{-2}) \), leading to much higher sample complexity in some cases.


Finally, to validate these theoretical guarantees and assess the algorithm’s empirical performance, we conduct simulation studies on a standard example from~\cite{XZ-TB:23}. The setup and results are presented in the following section.


\section{Simulation Studies}
% \begin{figure}
% \centering
% \includegraphics[width=\columnwidth]{sample_complexity_vs_epsilon.eps}
% \,
% \includegraphics[width=\columnwidth]{optimality_gap.eps}
% \caption{Above: average total number of calls to the zeroth-order oracle. Below: policy optimality gap $\| \widetilde{K}_0 - K^* \|$.}
% \label{fig:simulation}
% \end{figure}
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\linewidth]{sample_complexity_vs_epsilon.eps}
        \caption{Average number of calls to the zeroth-order oracle.}
        \label{fig:sample_complexity}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\linewidth]{optimality_gap.eps}
        \caption{Policy optimality gap $\| \widetilde{K}_0 - K^* \|$.}
        \label{fig:optimality_gap}
    \end{subfigure}
    \caption{Simulation results showing sample complexity and policy optimality gap.}
    \label{fig:simulation}
\end{figure}
For comparison, we demonstrate our results on the example provided in~\cite{XZ-TB:23}, where $A = 5$,
$B = 0.33$, $Q=R=1$, and the optimal policy is $K^* = 14.5482$ with $P^* = 221.4271$. In this example, we select $Q_N = 300 \succeq P^*$, in alignment with a minor inherent assumption discussed later in Remark~\ref{rem: Q_N inherent_assumption} (Appendix~\ref{app: RDE converges to ARE}). Additionally, we initialize our policy at each step $h$ of the outer loop of Algorithm~\ref{alg:RHPG-RL} as $K_{h,0} = 0$.
This choice contrasts with~\cite{MF-RG-SK-MM:18,DM-AP-KB-KK-PLB-MJW:20}, which require stable policies for initialization, as the stable policies for this example lie in the set
\[
\mathcal{K} = \{ K \mid 12.12 < K < 18.18 \}.
\]

We set $N = \lceil \frac{1}{2}\log\left(\frac{1}{\epsilon}\right) \rceil$, consistent with~\eqref{eq: N value choice}, and in each inner loop, apply the policy gradient (PG) update outlined in Algorithm~\ref{alg:RHPG-RL} using a time-varying step-size as suggested in~\eqref{eq: step-size value choice}. The algorithm is run for twelve different values of \( \epsilon \): \( \epsilon \in \{10^{-6}, 10^{-5.5}, 10^{-5}, \dotsc, 10^{-0.5} \} \), with the results shown in Figure~\ref{fig:simulation}. To account for the inherent randomness in the algorithm, we perform one hundred independent runs for each value of $\eps$ and compute the average sample complexity and policy optimality gap $\| \widetilde{K}_0 - K^* \|$. As seen in Figure~\ref{fig:simulation}, the sample complexity exhibits a slope consistent with $\mathcal{O}(\eps^{-0.5})$, visibly outperforming the method in~\cite{XZ-TB:23}, which demonstrates a much steeper slope of approximately $\mathcal{O}(\eps^{-1.5})$.

\section{Conclusion}
In this paper, we introduced a novel approach to solving the model-free LQR problem, inspired by policy gradient methods, particularly REINFORCE. Our algorithm eliminates the restrictive requirement of starting with a stable initial policy, making it applicable in scenarios where obtaining such a policy is challenging. Furthermore, it removes the reliance on two-point gradient estimation, enhancing practical applicability while maintaining similar rates.

Beyond these improvements, we introduced a refined outer-loop analysis that significantly enhances error accumulation, leveraging the contraction of the Riemannian distance over the Riccati operator. This ensures that the accumulated error remains linear in the horizon length, leading to a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-2})$, independent of problem-specific constants, making the method more broadly applicable.

We provide a rigorous theoretical analysis, establishing that the algorithm achieves convergence to the optimal policy with competitive sample complexity bounds. Importantly, our numerical simulations reveal performance that surpasses these theoretical guarantees, with the algorithm consistently outperforming prior methods that rely on two-point gradient estimates. This superior performance, combined with a more practical framework, highlights the potential of the proposed method for solving control problems in a model-free setting. Future directions include extensions to nonlinear and partially observed systems, as well as robustness enhancements. 

\begin{thebibliography}{10}

\bibitem{AMA-ALF:21}
A.~M. Annaswamy and A.~L. Fradkov.
\newblock A historical perspective of adaptive control and learning.
\newblock {\em Annual Reviews in Control}, 52:18--41, 2021.

\bibitem{DPB:95}
D.~P. Bertsekas.
\newblock {\em Dynamic Programming and Optimal Control}.
\newblock Athena Scientific, 1995.

\bibitem{RB:07}
R.~Bhatia.
\newblock {\em Positive Definite Matrices}.
\newblock Princeton University Press, 2007.

\bibitem{SD-HM-NM-BR-ST:20}
S.~Dean, H.~Mania, N.~Matni, B.~Recht, and S.~Tu.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock {\em Foundations of Computational Mathematics}, 20(4):633--679, 2020.

\bibitem{MF-RG-SK-MM:18}
M.~Fazel, R.~Ge, S.~Kakade, and M.~Mesbahi.
\newblock Global convergence of policy gradient methods for the linear quadratic regulator.
\newblock In {\em International Conference on Machine Learning}, pages 1467--1476. PMLR, 2018.

\bibitem{MG:06}
M.~Gevers.
\newblock A personal view of the development of system identification: A 30-year journey through an exciting field.
\newblock {\em IEEE Control systems magazine}, 26(6):93--105, 2006.

\bibitem{BH-AHS-TK:99}
B.~Hassibi, A.~H. Sayed, and T.~Kailath.
\newblock {\em Indefinite-Quadratic Estimation and Control}.
\newblock Society for Industrial and Applied Mathematics, 1999.

\bibitem{BH-KZ-NL-MM-MF-TB:23}
B.~Hu, K.~Zhang, N.~Li, M.~Mesbahi, M.~Fazel, and T.~Ba{\c s}ar.
\newblock Toward a theoretical foundation of policy optimization for learning control policies.
\newblock {\em Annual Review of Control, Robotics, and Autonomous Systems}, 6:123--158, 2023.

\bibitem{AL:20}
A.~Lamperski.
\newblock Computing stabilizing linear controllers via policy iteration.
\newblock In {\em Proceedings of the 59th IEEE Conference on Decision and Control (CDC)}, pages 1902--1907. IEEE, 2020.

\bibitem{BL-PM:00}
B.~Laurent and P.~Massart.
\newblock {Adaptive estimation of a quadratic functional by model selection}.
\newblock {\em The Annals of Statistics}, 28(5):1302--1338, 2000.

\bibitem{DM-AP-KB-KK-PLB-MJW:20}
D.~Malik, A.~Pananjady, K.~Bhatia, K.~Khamaru, P.~L. Bartlett, and M.~J. Wainwright.
\newblock Derivative-free methods for policy optimization: Guarantees for linear quadratic systems.
\newblock {\em Journal of Machine Learning Research}, 21(21):1--51, 2020.

\bibitem{ANM-AO-BG:24}
A.~Neshaei Moghaddam, A. Olshevsky, and B. Gharesifard.
\newblock Sample complexity of the linear quadratic regulator: A reinforcement learning lens, 2024.

\bibitem{HM-AZ-MS-MRJ:22}
H.~Mohammadi, A.~Zare, M.~Soltanolkotabi, and M.~R. Jovanović.
\newblock Convergence and sample complexity of gradient methods for the model-free linear–quadratic regulator problem.
\newblock {\em IEEE Transactions on Automatic Control}, 67(5):2435--2450, 2022.

\bibitem{JCP-JU-MS:21}
J.~C. Perdomo, J.~Umenberger, and M.~Simchowitz.
\newblock Stabilizing dynamical systems via policy gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{CMS:81}
C.~M. Stein.
\newblock Estimation of the mean of a multivariate normal distribution.
\newblock {\em The Annals of Statistics}, 9(6):1135--1151, 1981.

\bibitem{JS-MC:23}
J.~Sun and M.~Cantoni.
\newblock On {R}iccati contraction in time-varying linear-quadratic control, 2023.

\bibitem{RSS-DM-SS-YM:99}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock In {\em Proceedings of the 12th International Conference on Neural Information Processing Systems}, pages 1057--1063, 1999.

\bibitem{YT-YZ-NL:21}
Y.~Tang, Y.~Zheng, and N.~Li.
\newblock Analysis of the optimization landscape of linear quadratic gaussian ({LQG}) control.
\newblock In {\em Proceedings of the 3rd Conference on Learning for Dynamics and Control}, volume 144 of {\em Proceedings of Machine Learning Research}, pages 599--610, 2021.

\bibitem{RJW:92}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock {\em Machine Learning}, 8:229--256, 1992.

\bibitem{XZ-TB:23}
X.~Zhang and T.~Ba{\c s}ar.
\newblock Revisiting {LQR} control from the perspective of receding-horizon policy gradient.
\newblock {\em IEEE Control Systems Letters}, 7:1664--1669, 2023.

\bibitem{XZ-SM-MB-TB:23}
X.~Zhang, S.~Mowlavi, M.~Benosman, and T.~Ba{\c s}ar.
\newblock Global convergence of receding-horizon policy search in learning estimator designs, 2023.

\bibitem{FZ-XF-KY:21}
F.~Zhao, X.~Fu, and K.~You.
\newblock Learning stabilizing controllers of linear systems via discount policy gradient.
\newblock {\em arXiv preprint arXiv:2112.09294}, 2021.

\end{thebibliography}

\appendix

\section{Proof of Theorem~\ref{thm: RDE converges to ARE}} \label{app: RDE converges to ARE}
We let
\begin{align*}
    &\overline{P}_t := P^*_t- P^*\hspace{-0.2em}, \quad  \overline{R} := R + B^{\top}P^*B, \\
     &\overline{A} := A - B\overline{R}^{-1}B^{\top}P^*A,
\end{align*}
and we have
\begin{align}
    \overline{P}_{t} &= \overline{A}^{\top}\overline{P}_{t+1}\overline{A} - \overline{A}^{\top}\hspace{-0.1em}\overline{P}_{t+1}B(\overline{R} + B^{\hspace{-0.1em}\top}\hspace{-0.1em}\overline{P}_{t+1}B)^{\hspace{-0.1em}-1}\hspace{-0.1em}B^{\hspace{-0.1em}\top}\hspace{-0.1em}\overline{P}_{t+1}\overline{A} \nonumber\\
    &= \overline{A}^{\top}\overline{P}_{t+1}^{1/2}\big[I + \overline{P}^{1/2}_{t+1}B\overline{R}^{-1}B^{\top}\overline{P}_{t+1}^{1/2}\big]^{-1}\overline{P}_{t+1}^{1/2}\overline{A} \nonumber\\
    &\leq \hspace{-0.1em}\big[1\hspace{-0.15em}+\hspace{-0.15em}\lambda_{\min}(\overline{P}^{1/2}_{t+1}B\overline{R}^{-1}B^{\top}\overline{P}_{t+1}^{1/2})\big]^{-1}\hspace{-0.1em}\overline{A}^{\top}\overline{P}_{t+1}\overline{A} \nonumber\\
    &=:\hspace{-0.1em} \mu_{t} \overline{A}^{\top}\hspace{-0.2em}\overline{P}_{t+1}\overline{A},\label{eqn:RDE_conv_step2}
\end{align}
where $\overline{P}_{t+1}^{1/2}$ denotes the unique positive semi-definite (psd) square root of the psd matrix $\overline{P}_{t+1}$, $0 < \mu_t \leq 1$ for all $t$, and $\overline{A}$ satisfies $\rho(\overline{A}) < 1$. We now use $\|\cdot\|_*$ to represent the $P^*$-induced matrix norm and invoke Theorem 14.4.1 of \cite{BH-AHS-TK:99}, where our $\overline{P}_t$, $\overline{A}^{\top}$ and $P^*$ correspond to $P_i - P^*$, $F_p$ and $W$ in \cite{BH-AHS-TK:99}, respectively. By Theorem 14.4.1 of \cite{BH-AHS-TK:99} and \eqref{eqn:RDE_conv_step2}, we obtain $\|\overline{A}\|_* < 1$ and given that $\mu_t \leq 1$, 
\begin{align*}
    \|\overline{P}_{t}\|_* \leq \|\overline{A}\|^2_* \cdot \|\overline{P}_{t+1}\|_*.
\end{align*}
Therefore, the convergence is exponential such that $\|\overline{P}_t\|_* \leq \|\overline{A}\|_*^{2(N-t)}\cdot \|\overline{P}_{N}\|_*$. As a result, the convergence of $\overline{P}_t$ to $0$ in spectral norm can be characterized as
\begin{align*}
    \|\overline{P}_t\| \leq \kappa_{P^*}\cdot \|\overline{P}_t\|_* \leq \kappa_{P^*}\cdot\|\overline{A}\|_*^{2(N-t)}\cdot \|\overline{P}_{N}\|_*,
\end{align*}
where we have used $\kappa_X$ to denote the condition number of $X$. That is, to ensure $\|\overline{P}_1\| \leq \epsilon$, it suffices to require
\begin{align}\label{eqn:required_time}
    N \geq \frac{1}{2}\cdot \frac{\log\big(\frac{\|\overline{P}_N\|_*\cdot \kappa_{P^*}}{\epsilon}\big)}{\log\big(\frac{1}{\|\overline{A}\|_*}\big)} + 1.
\end{align}
Lastly, we show that the (monotonic) convergence of $K^*_t$ to $K^*$ follows from the convergence of $P^*_t$ to $P^*$. This can be verified through:
\begin{align}
    K^*_t - K^* &= (R+B^{\top}P^*_{t+1}B)^{-1}B^{\top}P^*_{t+1}A - (R+B^{\top}P^*B)^{-1}B^{\top}P^*A \nonumber\\
    &= \big[(R+B^{\top}P^*_{t+1}B)^{-1}-(R+B^{\top}P^*B)^{-1}\big]B^{\top}P^*A + (R+B^{\top}P^*_{t+1}B)^{-1}B^{\top}(P^*_{t+1}-P^*)A \nonumber\\
    &= (R+B^{\top}P^*_{t+1}B)^{-1}B^{\top}(P^*-P^*_{t+1})BK^* -(R+B^{\top}P^*_{t+1}B)^{-1}B^{\top}(P^*-P^*_{t+1})A\nonumber\\
    &=(R+B^{\top}P^*_{t+1}B)^{-1}B^{\top}(P^*-P^*_{t+1})(BK^*-A). \label{eqn:kdiff}
\end{align}
Hence, we have $\|K^*_t - K^*\| \leq \frac{\|\overline{A}\|\cdot \|B\|}{\lambda_{\min}(R)}\cdot \|P^*_{t+1} - P^*\|$ and
\begin{align*}
    \|K^*_0 - K^*\| \leq \frac{\|\overline{A}\|\cdot \|B\|}{\lambda_{\min}(R)}\cdot \|\overline{P}_1\|.
\end{align*}
Substituting $\epsilon$ in \eqref{eqn:required_time} with $\frac{\epsilon\cdot\lambda_{\min}(R)}{\|\overline{A}\|\cdot\|B\|}$ completes the proof.
\begin{remark} \label{rem: Q_N inherent_assumption}
    Note that since Theorem~\ref{thm: RDE converges to ARE} requires $\overline{P}_t = P^*_t - P^*$ to be positive definite for each $t$, it implies that we have access to a $P^*_N = Q_N$ that satisfies $Q_N \succeq P^*$ so that due to the monotonic convergence of~\eqref{eq: RDE}, it will hold that
    \[
    P^*_N \succeq P^*_{N-1} \succeq \cdots \succeq P^*_0 \succeq P^*,
    \]
    satisfying said requirement.
\end{remark}

\section{Proof of Theorem~\ref{thm: LQR_DP_by_delta}} \label{app: thm_LQR_DP_by_Delta}
We start the proof by providing some preliminary results.
\begin{lemma} \label{lem: delta_upper_lower}
    Let $U$ and $V$ be two positive definie matrices. It holds that
    \begin{equation}
    \|U-V\| \le \|V\|\, e^{\delta(U,V)}\, \delta(U,V). \label{eq: delta_lower}
    \end{equation}
    Furthermore, if 
    \begin{equation}
        \|V^{-1}\|\,\|U-V\| < 1, \label{eq: delta_upper_assumption}
    \end{equation}
    then we have
    \begin{equation}
   \delta(U,V) \le \frac{\|V^{-1}\|\,\|U-V\|_F}{1-\|V^{-1}\|\,\|U-V\|}. \label{eq: delta_upper}
   \end{equation}
\end{lemma}
\begin{proof}
    First, since \(U\) and \(V\) are positive definite, we have that \(V^{-1/2} U V^{-1/2}\)  is positive definite, and therefore has a logarithm; so we let
    \[
    Z := \log(V^{-1/2} U V^{-1/2}),
    \]
    and hence, we can write
    \[
    U = V^{1/2} \exp(Z) V^{1/2}.
    \]
    The eigenvalues of \(Z\) are precisely the logarithms of the eigenvalues of \(UV^{-1}\) due to $UV^{-1}$ and $V^{-1/2} U V^{-1/2}$ being similar. As a result,
    \[
    \delta(U,V) = \|Z\|_F.
    \]
    We now write
    \[
    U - V = V^{1/2}\exp(Z)V^{1/2} - V = V^{1/2}(\exp(Z)-I)V^{1/2},
    \]
    and thus,
    \begin{equation}
    \|U-V\| \le \|V\|\, \|\exp(Z)-I\|. \label{eq: delta_lower_prelim1}
    \end{equation}
    Since $e^{x}-1\leq  x e^{x}$ whenever $x \geq 0$, we also have for any matrix $Z$, by consider the expansion of $e^Z$: 
    \[
    \|\exp(Z)-I\| \leq e^{||Z||}  - 1\le e^{\|Z\|}\, \|Z\|.
    \]
    Since the spectral norm is always bounded by the Frobenius norm, we have:
    \[
    \|\exp(Z)-I\| \le e^{\|Z\|_F}\, \|Z\|_F.
    \]
    Finally, recalling that \(\|Z\|_F = \delta(U,V)\), this becomes:
    \[
    \|\exp(Z)-I\| \le e^{\delta(U,V)}\, \delta(U,V),
    \]
    which after substituting into~\eqref{eq: delta_lower_prelim1} yields:
    \[
    \|U-V\| \le \|V\|\, e^{\delta(U,V)}\, \delta(U,V),
    \]
    concluding the proof of the first claim. We now move on to the second claim. As before, we write 
    \[
    \delta(U,V)=\|\log(V^{-1/2}UV^{-1/2})\|_F.
    \]
    We now define
    \[
    X := V^{-1/2}(U-V)V^{-1/2},
    \]
    so that
    \[
    V^{-1/2}UV^{-1/2} = I + X.
    \]
    Moreover, following~\eqref{eq: delta_upper_assumption},
    \[
    \| X \| = \|V^{-1/2}(U-V)V^{-1/2}\| \leq \| V^{-1} \| \| U - V \| < 1,
    \]
    and hence, one can use the series expansion of the logarithm
    \[
    \log(I+X)=X - \frac{1}{2}X^2 + \cdots,
    \]
    to show
    \begin{align}
        \|\log(I+X)\|_F &= \left\| \sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k} X^k \right\|_F \cr
        &\leq \sum_{k=1}^{\infty} \| X^k \|_F \cr
        &\leq \sum_{k=1}^{\infty} \| X \|_F \| X^{k-1} \| \cr
        &\leq \| X \|_F \sum_{k=0}^{\infty} \| X \|^{k} \cr
        &= \frac{\|X\|_F}{1-\|X\|}.
    \end{align}
    As a result, we have
    \begin{align*}
    \delta(U,V) &= \|\log(I+X)\|_F \\
    &\le \frac{\|X\|_F}{1-\|X\|} \\
    &= \frac{\|V^{-1/2}(U-V)V^{-1/2}\|_F}{1-\|V^{-1/2}(U-V)V^{-1/2}\|} \\
    &\leq \frac{\|V^{-1}\| \| (U-V) \|_F}{1-\|V^{-1}\| \| (U-V) \|},
    \end{align*}
    finishing the proof.
\end{proof}

Building on Lemma~\ref{lem: delta_upper_lower}, we proceed to state the following result regarding the LQR setting.
\begin{lemma} \label{lem: delta_uniform_upper_induction}
    Let $t \in \{1,2,\dotsc,N-1\}$, select \( Q_N \succeq Q \), and suppose Assumption~\ref{ass: A_invertible} holds. Additionally, assume that for all \( t' \in \{ t+1, t+2, \dotsc, N\} \), we have 
    \begin{align}
        &\| P^*_{t'} - \widetilde{P}_{t'} \| \leq a, \quad \text{and} \label{eq: a_condition_t'} \\
        &\delta(\widetilde{P}^*_{t'}, \widetilde{P}_{t'} ) \leq \varepsilon, \label{eq: delta_upper_condition_t'}
    \end{align}
    where \( \varepsilon \) satisfies
    \begin{equation}
    \varepsilon \leq \frac{1}{N} \min \left\{ \frac{a}{2 e \| P_{\max} \|}, 1 \right\}. \label{eq: varepsilon_condition}
    \end{equation}
    If
    \begin{equation}
    \| \widetilde{K}_t - \widetilde{K}^*_t \|_F \leq \sqrt{\frac{a}{C_3} \varepsilon}, \label{eq: K_diff_condition_induction}
    \end{equation}
    then the following bounds hold:
    \begin{align}
        &\| P^*_t - \widetilde{P}_t \| \leq a, \quad \text{and} \label{eq: a_condition} \\
        &\delta(\widetilde{P}^*_t, \widetilde{P}_t ) \leq \varepsilon. \label{eq: delta_upper_condition}
    \end{align}
\end{lemma}
\begin{proof}
    Before we move on to the proof, we esablish some preliminary results. First, note that since 
    \[ P^*_N = \widetilde{P}_N = Q_N \succeq Q \succ 0,\] 
    due to the monotonic convergence of~\eqref{eq: RDE} to $P^* \succeq Q$ (see~\cite{BH-AHS-TK:99}), we have that $P^*_t \succeq Q$ for all $t \in \{ 1,2,\dotsc,N\}$. Therefore, it holds that
    \begin{equation}
    \sigma_{\min} (P^*_t) \geq \sigma_{\min} (Q) = 2a > 0. \label{eq: sigma_min_P_star_t_lowerbound}
    \end{equation}
    Moreover, due to~\eqref{eq: a_condition_t'}, we have
    \begin{equation}
        \widetilde{P}_{t'} \succeq P^*_{t'} - aI \succeq a I \succ 0 \label{eq: sigma_min_P_tilde_star_t'_lowerbound}
    \end{equation}
    for all \( t' \in \{ t+1, t+2, \dotsc, N\} \). Now since~\eqref{eq: sigma_min_P_star_t_lowerbound}, \eqref{eq: sigma_min_P_tilde_star_t'_lowerbound}, and Assumption~\ref{ass: A_invertible} all hold, we can apply Lemma~\ref{lem: delta_Riccati_contraction} to show that for all \( t' \in \{ t+1, t+2, \dotsc, N\} \),
    \begin{align}
    \delta (P_{t'-1}^*, \widetilde{P}_{t'-1}^*) &\overset{\mathrm{(i)}}{=}
    \delta(\mathcal{R}(P^*_{t'}),\mathcal{R}(\widetilde{P}_{t'})) \cr
    &\leq \delta(P^*_{t'}, \widetilde{P}_{t'}), \label{eq: delta_contraction_t'}
    \end{align}
    where (i) follows from~\eqref{eq: Riccati_operator_on_P_tilde_star} and~\eqref{eq: Riccati_operator_on_P_star}. Following~\eqref{eq: delta_contraction_t'}, we can now write
    
    % \begin{align}
    %     \sigma_{\min} (\widetilde{P}_{t'}) &\geq \sigma_{\min} (P^*_{t'}) - a \cr
    %     &\geq \sigma_{\min} (Q) - a \cr
    %     &= a > 0.
    % \end{align}
    \begin{align}
        \delta(P_t^*, \widetilde{P}_t^*) &\leq \delta(P_{t+1}^*, \widetilde{P}_{t+1}) \cr
        &\overset{\mathrm{(i)}}{\leq} \delta(P_{t+1}^*, \widetilde{P}^*_{t+1}) + \delta(\widetilde{P}_{t+1}^*, \widetilde{P}_{t+1}) \cr
        &\leq \delta(P_{t+2}^*, \widetilde{P}_{t+2}) + \delta(\widetilde{P}_{t+1}^*, \widetilde{P}_{t+1}) \cr
        &\leq \delta(P_{t+2}^*, \widetilde{P}^*_{t+2}) + \delta(\widetilde{P}_{t+2}^*, \widetilde{P}_{t+2}) + \delta(\widetilde{P}_{t+1}^*, \widetilde{P}_{t+1}) \cr
        &\leq \cdots \cr
        &\leq \delta(P_{N}^*, \widetilde{P}_{N}) + \sum_{k=1}^{N-t-1} \delta(\widetilde{P}_{t+k}^*, \widetilde{P}_{t+k}) \cr
        &\overset{\mathrm{(ii)}}{=} \sum_{k=1}^{N-t-1} \delta(\widetilde{P}_{t+k}^*, \widetilde{P}_{t+k}) \cr
        &\overset{\mathrm{(iii)}}{\leq} \varepsilon N, \label{eq: geometric_sum}
    \end{align}
    where (i) is due to the triangle inequality of the Riemannian distance~\cite{RB:07}, (ii) follows from $P_{N}^* = \widetilde{P}_{N} = Q_N$, and (iii) from~\eqref{eq: delta_upper_condition_t'}.
    We now start the proof of~\eqref{eq: a_condition} by writing
    \begin{align}
        \| P^*_t - \widetilde{P}_t \| \leq \| P^*_t - \widetilde{P}^*_t \| + \| \widetilde{P}^*_t - \widetilde{P}_t \|, \label{eq: P_diff_total}
    \end{align}
    and trying to provide a bound for both terms of the right-hand side of~\eqref{eq: P_diff_total}. For the first term, we have
    \begin{align}
        \| P^*_t - \widetilde{P}^*_t \| &\overset{\mathrm{(i)}}{\leq} \| P_{\max} \| e^{\delta (P^*_t, \widetilde{P}^*_t)} \delta (P^*_t, \widetilde{P}^*_t) \cr
        &\overset{\mathrm{(ii)}}{\leq} \| P_{\max} \| e^{\varepsilon N} \varepsilon N \cr
        &\overset{\mathrm{(iii)}}{\leq} \frac{a}{2}, \label{eq: P_diff_1}
    \end{align}
    where (i) follows from~\eqref{eq: delta_lower}, (ii) from~\eqref{eq: geometric_sum}, and (iii) from the condition on $\varepsilon$ in~\eqref{eq: varepsilon_condition}. As for the second term on the right-hand side of~\eqref{eq: P_diff_total}, we can write
    \begin{align}
        \widetilde{P}^*_{t} - \widetilde{P}_{t} &= (A-B\widetilde{K}^*_t)^{\top}\widetilde{P}_{t+1}(A-B\widetilde{K}^*_t) + (\widetilde{K}^*_t)^{\top}R\widetilde{K}^*_t - (A-B\widetilde{K}_t)^{\top}\widetilde{P}_{t+1}(A-B\widetilde{K}_t) - (\widetilde{K}_t)^{\top}R\widetilde{K}_t \nonumber\\
        &= - (\widetilde{K}^*_t)^{\hspace{-0.1em}\top}\hspace{-0.1em}B^{\hspace{-0.1em}\top}\hspace{-0.1em}\widetilde{P}_{t+1}A \hspace{-0.1em}-\hspace{-0.1em} A^{\hspace{-0.1em}\top}\hspace{-0.1em}\widetilde{P}_{t+1}B\widetilde{K}_t^* \hspace{-0.1em}+\hspace{-0.1em} (\widetilde{K}^*_t)^{\hspace{-0.1em}\top}\hspace{-0.1em}(R+B^{\hspace{-0.1em}\top}\widetilde{P}_{t+1}B)\widetilde{K}^*_t   \nonumber\\
        &\hspace{1em} + \widetilde{K}_t^{\top}B^{\top}\widetilde{P}_{t+1}A + A^{\top}\widetilde{P}_{t+1}B\widetilde{K}_t - \widetilde{K}_t^{\top}(R+B^{\top}\widetilde{P}_{t+1}B)\widetilde{K}_t \nonumber\\
        &\overset{\mathrm{(i)}}{=} \big[(R+B^{\top}\widetilde{P}_{t+1}B)^{-1}B^{\top}\widetilde{P}_{t+1}A - \widetilde{K}^*_t\big]^{\top}(R+B^{\top}\widetilde{P}_{t+1}B) \big[(R+B^{\top}\widetilde{P}_{t+1}B)^{-1}B^{\top}\widetilde{P}_{t+1}A - \widetilde{K}^*_t \big] \nonumber\\
        &\hspace{1em}- \big[(R+B^{\top}\widetilde{P}_{t+1}B)^{-1}B^{\top}\widetilde{P}_{t+1}A - \widetilde{K}_t\big]^{\top}(R+B^{\top}\widetilde{P}_{t+1}B) \big[(R+B^{\top}\widetilde{P}_{t+1}B)^{-1}B^{\top}\widetilde{P}_{t+1}A - \widetilde{K}_t \big] \nonumber \\
        &= \big[\widetilde{K}^*_t - \widetilde{K}^*_t\big]^{\top}(R+B^{\top}\widetilde{P}_{t+1}B) \big[\widetilde{K}^*_t - \widetilde{K}^*_t \big] - \big[\widetilde{K}^*_t - \widetilde{K}_t\big]^{\top}(R+B^{\top}\widetilde{P}_{t+1}B) \big[\widetilde{K}^*_t - \widetilde{K}_t \big] \nonumber \\
        &= - \big[\widetilde{K}^*_t - \widetilde{K}_t\big]^{\top}(R+B^{\top}\widetilde{P}_{t+1}B) \big[\widetilde{K}^*_t - \widetilde{K}_t \big], \label{eq: P_diff_2_prelim}
    \end{align}
    where (i) follows from completion of squares. Combining~\eqref{eq: P_diff_2_prelim} and~\eqref{eq: a_condition_t'}, we have
    \begin{align}
        \| \widetilde{P}^*_{t} - \widetilde{P}_{t} \| &\leq \| R+B^{\top} (P_{\max} + a) B \| \| \widetilde{K}^*_t - \widetilde{K}_t \|^2 \label{eq: delta_upper_numerator_prelim} \\
        &\leq \frac{C_3}{2} \| \widetilde{K}^*_t - \widetilde{K}_t \|^2 \cr
        &\leq \left( \frac{C_3}{2} \right) \left( \frac{a}{C_3} \varepsilon \right) \cr
        &\leq \frac{a}{2}. \label{eq: P_diff_2}
    \end{align}
    Finally, substituting~\eqref{eq: P_diff_1} and~\eqref{eq: P_diff_2} in~\eqref{eq: P_diff_total}, we have
    \[
    \| P^*_t - \widetilde{P}_t \| \leq \frac{a}{2} + \frac{a}{2} = a,
    \]
    finishing the proof of~\eqref{eq: a_condition}. Having established this, we proceed to prove~\eqref{eq: delta_upper_condition}. Note that similar to~\eqref{eq: delta_upper_numerator_prelim}, we can write
    \begin{align}
        \| \widetilde{P}^*_{t} - \widetilde{P}_{t} \|_F &\leq \| R+B^{\top} (P_{\max} + a I) B \| \| \widetilde{K}^*_t - \widetilde{K}_t \|_F^2 \cr
        &\leq \frac{C_3}{2} \| \widetilde{K}^*_t - \widetilde{K}_t \|_F^2. \label{eq: delta_upper_numerator}
    \end{align}
    Moreover, due to~\eqref{eq: a_condition}, we have that $\widetilde{P}_t \succeq P^*_t - aI$, and hence,
    \begin{align}
        \sigma_{\min} (\widetilde{P}_t) \geq \sigma_{\min} (P^*_t) - a \overset{\mathrm{(i)}}{\geq} a, \label{eq: P_tilde_t_sigma_min_lower}
    \end{align}
    where (i) follows from~\eqref{eq: sigma_min_P_star_t_lowerbound}. Combining~\eqref{eq: P_tilde_t_sigma_min_lower} and~\eqref{eq: P_diff_2}, we have
    \begin{align}
    \| \widetilde{P}_t^{-1} \| \| \widetilde{P}^*_{t} - \widetilde{P}_{t} \| &= \frac{\| \widetilde{P}^*_{t} - \widetilde{P}_{t} \|}{\sigma_{\min} (\widetilde{P}_t)} \cr
    &\leq \frac{a/2}{a} \cr
    &= \frac{1}{2}. \label{eq: delta_upper_condition_met_one_half}
    \end{align}
    Thus, the condition~\eqref{eq: delta_upper_assumption} of Lemma~\ref{lem: delta_upper_lower} is met, and we can utilize~\eqref{eq: delta_upper} to write
    \begin{align*}
        \delta(\widetilde{P}^*_{t}, \widetilde{P}_{t}) &\leq \frac{\| \widetilde{P}_t^{-1} \| \| \widetilde{P}^*_{t} - \widetilde{P}_{t} \|_F}{1 - \| \widetilde{P}_t^{-1} \| \| \widetilde{P}^*_{t} - \widetilde{P}_{t} \|} \\
        &\overset{\mathrm{(i)}}{\leq} \frac{(1/a) \| \widetilde{P}^*_{t} - \widetilde{P}_{t} \|_F}{(1/2)} \\
        &\overset{\mathrm{(ii)}}{\leq} \frac{C_3}{a} \| \widetilde{K}^*_t - \widetilde{K}_t \|_F^2 \\
        &\overset{\mathrm{(i)}}{\leq} \varepsilon,
    \end{align*}
    where (i) follows from~\eqref{eq: P_tilde_t_sigma_min_lower} and~\eqref{eq: delta_upper_condition_met_one_half}, (ii) from~\eqref{eq: delta_upper_numerator}, and (iii) from condition~\eqref{eq: K_diff_condition_induction}. This verifies~\eqref{eq: delta_upper_condition}, concluding the proof.
\end{proof}
Having established Lemma~\ref{lem: delta_uniform_upper_induction}, we can finally present the proof of~\ref{thm: LQR_DP_by_delta}.

\textit{Proof of Theorem~\ref{thm: LQR_DP_by_delta}:} 
First, according to Theorem~\ref{thm: RDE converges to ARE}, our choice of $N$ in~\eqref{eq: N_value_choice} ensures that $K^*_{0}$ is stabilizing and $\|K^*_{0} - K^*\| \leq \epsilon/2$. Then, it remains to show that the output $\widetilde{K}_0$ satisfies $\|\widetilde{K}_0 - K^*_0\| \leq \epsilon/2$. 

Now observe that
\begin{align*}
\|\widetilde{K}_{0} - K^*_{0}\| \leq \|\widetilde{K}^*_{0} - K^*_{0}\| + \|\widetilde{K}_0-\widetilde{K}_0^*\|,
\end{align*}
where substituting $K^*_t$ and $K^*$ in \eqref{eqn:kdiff}, respectively, with $\widetilde{K}^*_0$ and $K^*_0$ leads to
\begin{align*}
\widetilde{K}^*_{0} - K^*_{0} = (R+B^{\top}\widetilde{P}_{1}B)^{-1}B^{\top}(P^*_1-\widetilde{P}_{1})(BK^*_0-A).
\end{align*}
Hence, the error size $\|\widetilde{K}^*_{0} - K^*_{0}\|$ could be bounded by 
\begin{align}
\|\widetilde{K}^*_{0} - K^*_{0}\| \leq \frac{\|A-BK^*_0\|\cdot\|B\|}{\lambda_{\min}(R)}\cdot\|P^*_1-\widetilde{P}_{1}\| \label{eq: laststep_req}.
\end{align}
% Define the helper constants
% \begin{align*}
% 	C_1:= \frac{\varphi\cdot\|B\|}{\lambda_{\min}(R)} > 0, \ \varphi := \max_{t\in\{0, \cdots, N-1\}}\|A-BK^*_t\|.
% \end{align*}
Next, since we have $\|\widetilde{K}_0-\widetilde{K}_0^*\| \leq \varsigma_{0,\eps} = \epsilon/4$, it suffices to show $\|\widetilde{K}^*_{0} - K^*_{0}\| \leq \epsilon/4$ to fulfill $\|\widetilde{K}_{0} - K^*_{0}\| \leq \epsilon/2$. 
Then, by \eqref{eq: laststep_req}, in order to satisfy $\|\widetilde{K}^*_{0} - K^*_{0}\| \leq \epsilon/4$, it remains to show
\begin{align}\label{eq: P1_req}
\|P^*_1-\widetilde{P}_{1}\| \leq \frac{\epsilon}{4 C_1}.
\end{align}
% Subsequently, we have
% \begin{align}\label{eqn:plast}
%     P^*_1-\widetilde{P}_{1} = (P^*_1 - \widetilde{P}^*_{1}) + (\widetilde{P}^*_{1} - \widetilde{P}_{1}).
% \end{align}
In order to show this, we first let
\begin{equation}
\varepsilon = \frac{1}{N} \min \left\{ \frac{\eps}{8 e C_1 \| P_{\max} \|}, \frac{a}{2 e \| P_{\max} \|}, 1 \right\}, \label{eq: varepsilon_value}
\end{equation}
which clearly satisfies~\eqref{eq: varepsilon_condition}. Now we want to show, by strong induction, that
\begin{align*}
    &\| P^*_t - \widetilde{P}_t \| \leq a, \quad \text{and} \\
    &\delta(\widetilde{P}^*_t, \widetilde{P}_t) \leq \varepsilon,
\end{align*}
for all $t \in \{N,N-1,\dotsc,1\}$. For the base case, we have
\[
P^*_N = \widetilde{P}^*_N = \widetilde{P}_N = Q_N,
\]
and hence, it immediately follows that
\begin{align*}
    &\| P^*_N - \widetilde{P}_N \| = 0 \leq a, \quad \text{and} \\
    &\delta(\widetilde{P}^*_N, \widetilde{P}_N) = 0 \leq \varepsilon.
\end{align*}
Now since it holds in the statement of Theorem~\ref{thm: LQR_DP_by_delta} that
\[
\| \widetilde{K}_h - \widetilde{K}^*_h \| \leq \varsigma_{h,\eps} \leq \sqrt{\frac{a}{C_3} \varepsilon},
\]
which satisfies~\eqref{eq: K_diff_condition_induction}, the inductive step follows directly from Lemma~\ref{lem: delta_uniform_upper_induction}. We have now succesfully established that
\begin{align}
    &\| P^*_t - \widetilde{P}_t \| \leq a, \quad \text{and} \label{eq: p_diff_leq_a}\\
    &\delta(\widetilde{P}^*_t, \widetilde{P}_t) \leq \varepsilon, \label{eq: delta_leq_varepsilon}
\end{align}
for all $t \in \{N,N-1,\dotsc,1\}$.
As a result, we have
\begin{align}
    \delta(P_1^*, \widetilde{P}_1^*) &\leq \delta(P_{2}^*, \widetilde{P}_{2}) \cr
    &\leq \delta(P_{2}^*, \widetilde{P}^*_{2}) + \delta(\widetilde{P}_{2}^*, \widetilde{P}_{2}) \cr
    &\leq \delta(P_{3}^*, \widetilde{P}_{3}) + \delta(\widetilde{P}_{2}^*, \widetilde{P}_{2}) \cr
    &\leq \delta(P_{3}^*, \widetilde{P}^*_{3}) + \delta(\widetilde{P}_{3}^*, \widetilde{P}_{3}) + \delta(\widetilde{P}_{2}^*, \widetilde{P}_{2}) \cr
    &\leq \cdots \cr
    &\leq \delta(P_{N}^*, \widetilde{P}_{N}) + \sum_{k=2}^{N-1} \delta(\widetilde{P}_{k}^*, \widetilde{P}_{k}) \cr
    &= \sum_{k=2}^{N-1} \delta(\widetilde{P}_{k}^*, \widetilde{P}_{t}) \cr
    &\leq \varepsilon N. \label{eq: geometric_sum_2}
\end{align}
We now show~\eqref{eq: P1_req} by writing
\begin{align}
    \| P^*_1 - \widetilde{P}_1 \| \leq \| P^*_1 - \widetilde{P}^*_1 \| + \| \widetilde{P}^*_1 - \widetilde{P}_1 \|, \label{eq: P1_diff_total}
\end{align}
and providing a bound for both terms of the right-hand side of~\eqref{eq: P1_diff_total}. For the first term, we have
\begin{align}
    \| P^*_1 - \widetilde{P}^*_1 \| &\overset{\mathrm{(i)}}{\leq} \| P_{\max} \| e^{\delta (P^*_1, \widetilde{P}^*_1)} \delta (P^*_1, \widetilde{P}^*_1) \cr
    &\overset{\mathrm{(ii)}}{\leq} \| P_{\max} \| e^{\varepsilon N} \varepsilon N \cr
    &\overset{\mathrm{(iii)}}{\leq} \frac{\eps}{8 C_1}, \label{eq: P1_diff_1}
\end{align}
where (i) follows from Lemma~\ref{lem: delta_upper_lower}, (ii) from~\eqref{eq: geometric_sum_2}, and (iii) from~\eqref{eq: varepsilon_value}. As for the second term on the right-hand side of~\eqref{eq: P1_diff_total}, we utilize~\eqref{eq: P_diff_2_prelim} to write
\begin{align}
    \| \widetilde{P}^*_1 - \widetilde{P}_1 \| &\leq \| R + B^{\top} \widetilde{P}_2 B \| \|\widetilde{K}_1-\widetilde{K}_1^*\|^2 \cr
    &\overset{\mathrm{(i)}}{\leq} \| R + B^{\top} (P_{\max} + aI) B \| (\varsigma_{1,\eps})^2 \cr
    &\overset{\mathrm{(ii)}}{\leq} \frac{C_3}{2} \frac{\eps}{4 C_1 C_3} \cr
    &= \frac{\eps}{8 C_1}, \label{eq: P1_diff_2}
\end{align}
where (i) follows from~\eqref{eq: p_diff_leq_a}, and (ii) is due to the definition of $\varsigma_{1,\eps}$ in~\eqref{eq: def_varsigma_h}. Finally, substituting~\eqref{eq: P1_diff_1} and~\eqref{eq: P1_diff_2} in~\eqref{eq: P1_diff_total}, we have
\[
\| P^*_t - \widetilde{P}_t \| \leq \frac{\eps}{8 C_1} + \frac{\eps}{8 C_1} = \frac{\eps}{4 C_1},
\]
thereby establishing~\eqref{eq: P1_req} and concluding the proof of Theorem~\ref{thm: LQR_DP_by_delta}. \oprocend

\section{Proof of Theorem~\ref{thm: LQR_DP}} \label{app: thm_LQR_DP} 
First, according to Theorem~\ref{thm: RDE converges to ARE}, we select
\begin{align}\label{eqn:N_choice}
    N = \frac{1}{2}\cdot \frac{\log\big(\frac{2\|Q_N-P^*\|_*\cdot\kappa_{P^*}\cdot \|A_K^*\|\cdot\|B\|} {\epsilon\cdot\lambda_{\min}(R)}\big)}{\log\big(\frac{1}{\|A_K^*\|_*}\big)} + 1,
\end{align} 
where $A_K^*:=A-BK^*$. This ensures that $K^*_{0}$ is stabilizing and $\|K^*_{0} - K^*\| \leq \epsilon/2$. Then, it remains to show that the output $\widetilde{K}_0$ satisfies $\|\widetilde{K}_0 - K^*_0\| \leq \epsilon/2$. 

Recall that the RDE \eqref{eq: RDE} is a backward iteration starting with $P^*_N = Q_N \geq 0$, and can also be represented as: 
\begin{align}
    P^*_{t} &= (A-BK^*_t)^{\top} P^*_{t+1}(A-BK^*_t) +(K^*_t)^{\top}RK^*_t + Q \label{eqn:lqr_RDE_Lya} \\
    &= A^{\top}P^*_{t+1}\big(A - BK^*_t\big) + Q + (K^*_t)^\top (R + B^\top P^*_{t+1} B) K^*_t - (K^*_t)^\top (B^\top P^*_{t+1} A) \cr
    &\overset{\mathrm{(i)}}{=} A^{\top}P^*_{t+1}\big(A - BK^*_t\big) + Q, \label{eqn:standard_RDE_appen}
\end{align}
where (i) comes from $K^*_t = (R + B^\top P^*_{t+1} B)^{-1} (B^\top P^*_{t+1} A)$.
%  Moreover, for any $K_t$, we introduce the Lyapunov equation:
%  \begin{align}
%  	\hspace{-0.5em}P_{t} = (A-BK_t)^{\top}P_{t+1}(A-BK_t) + K_t^{\top}RK_t+Q. \label{eqn:lqr_lyapunov}
%  \end{align}
%  Furthermore, for clarity of the proof, we define/recall:
% \small
% \begin{align*}
%  	&K^*_t\text{: Exact LQR policy at time $t$ defined in \eqref{eq: RDE_K}}\\
%  	&\widetilde{K}_t^*\text{: Optimal policy of the current cost-to-go function,}\\
%  	&\hspace{2.1em} \text{absorbing errors in all prior steps} \\
%  	&\widetilde{K}_t\text{: An approximation of $\widetilde{K}_t^*$ obtained by the PG update \eqref{eqn:PG}}\\
%  	&\hspace{0.2em} \delta_t:=\widetilde{K}_t-\widetilde{K}_t^* \text{: Policy optimization error at time $t$} \\
%  	&\widetilde{P}^*_{t} \text{: Generated by \eqref{eqn:lqr_RDE_Lya} with $K^*_t = \widetilde{K}^*_t$ and $P^*_{t+1} = \widetilde{P}_{t+1}$.}
%  \end{align*}
%  \normalsize
 Moreover, for clarity of proof, we denote the policy optimization error at time $t$ by:
 \[
 e_t:=\widetilde{K}_t-\widetilde{K}_t^*.
 \]
 We argue that $\|\widetilde{K}_{0} - K^*_{0}\| \leq \epsilon/2$ can be achieved by carefully controlling $e_t$ for all $t$. At $t=0$, it holds that
 \begin{align*}
    \|\widetilde{K}_{0} - K^*_{0}\| \leq \|\widetilde{K}^*_{0} - K^*_{0}\| + \|e_{0}\|,
 \end{align*}
 where substituting $K^*_t$ and $K^*$ in \eqref{eqn:kdiff}, respectively, with $\widetilde{K}^*_0$ and $K^*_0$ leads to
 \begin{align*}
    \widetilde{K}^*_{0} - K^*_{0} = (R+B^{\top}\widetilde{P}_{1}B)^{-1}B^{\top}(P^*_1-\widetilde{P}_{1})(BK^*_0-A).
 \end{align*}
 Hence, the error size $\|\widetilde{K}^*_{0} - K^*_{0}\|$ could be bounded by 
 \begin{align}
    \|\widetilde{K}^*_{0} - K^*_{0}\| \leq \frac{\|A-BK^*_0\|\cdot\|B\|}{\lambda_{\min}(R)}\cdot\|P^*_1-\widetilde{P}_{1}\| \label{eqn:laststep_req}.
\end{align}
% Define the helper constants
% \begin{align*}
% 	C_1:= \frac{\varphi\cdot\|B\|}{\lambda_{\min}(R)} > 0, \ \varphi := \max_{t\in\{0, \cdots, N-1\}}\|A-BK^*_t\|.
% \end{align*}
Next, we require $\|e_{0}\| \leq \epsilon/4$ and $\|\widetilde{K}^*_{0} - K^*_{0}\| \leq \epsilon/4$ to fulfill $\|\widetilde{K}_{0} - K^*_{0}\| \leq \epsilon/2$. We 
% select a fixed scalar $a > 0$ that is independent of system parameters and $\epsilon$, and 
additionally require $\|P^*_1-\widetilde{P}_{1}\| \leq a$ to upper-bound the positive definite solutions of \eqref{eq: lqr_lyapunov}. Then, by \eqref{eqn:laststep_req}, in order to fulfill $\|\widetilde{K}^*_{0} - K^*_{0}\| \leq \epsilon/4$, it suffices to require
\begin{align}\label{eqn:preq1}
\|P^*_1-\widetilde{P}_{1}\| \leq \min\bigg\{a, \frac{\epsilon}{4 C_1}\bigg\}.
%\|P^*_1-\widetilde{P}_{1}\| \leq  \frac{\epsilon}{4 C_1}.
\end{align}
Subsequently, we have
\begin{align}\label{eqn:plast}
    P^*_1-\widetilde{P}_{1} = (P^*_1 - \widetilde{P}^*_{1}) + (\widetilde{P}^*_{1} - \widetilde{P}_{1}).
\end{align}
The first difference term on the RHS of \eqref{eqn:plast} is
\begin{align}
P^*_1 - \widetilde{P}^*_{1} &= A^{\top}P^*_{2}\big(A - BK^*_1\big) - A^{\top}\widetilde{P}_{2}\big(A - B\widetilde{K}^*_1\big) \nonumber\\
&= A^{\top}(P_2^* - \widetilde{P}_2)(A-BK^*_1) + A^{\top}\widetilde{P}_2B(\widetilde{K}_1^* - K^*_1).\label{eqn:plast2}\\
&=A^{\top}(P_2^* - \widetilde{P}_2)(A-BK^*_1) - A^{\top}\widetilde{P}_2B(R+B^{\top}\widetilde{P}_{2}B)^{-1}B^{\top}(P^*_2-\widetilde{P}_{2})(A-BK^*_1) \label{eqn:apply_k_diff}\\
&=A^{\top}[I-\widetilde{P}_2B(R+B^{\top}\widetilde{P}_{2}B)^{-1}B^{\top}](P_2^* - \widetilde{P}_2)(A-BK^*_1), \label{eqn:dp_inversion_lemma}
% &\hspace{-4.4em}=A^{\top}(I+\widetilde{P}_2BR^{-1}B^{\top})^{-1}(P_2^* - \widetilde{P}_2)(A-BK^*_1),
\end{align}
% where applying  \eqref{eqn:laststep_req} in deriving \eqref{eqn:apply_k_diff} and \eqref{eqn:dp_inversion_lemma} is due to the matrix inversion lemma.
Moreover, the second term on the RHS of \eqref{eqn:plast} is 
\begin{align}
    \widetilde{P}^*_{1} - \widetilde{P}_{1} &= (A-B\widetilde{K}^*_1)^{\top}\widetilde{P}_{2}(A-B\widetilde{K}^*_1) + (\widetilde{K}^*_1)^{\top}R\widetilde{K}^*_1 - (A-B\widetilde{K}_1)^{\top}\widetilde{P}_{2}(A-B\widetilde{K}_1) - (\widetilde{K}_1)^{\top}R\widetilde{K}_1 \nonumber\\
    &= - (\widetilde{K}^*_1)^{\hspace{-0.1em}\top}\hspace{-0.1em}B^{\hspace{-0.1em}\top}\hspace{-0.1em}\widetilde{P}_2A \hspace{-0.1em}-\hspace{-0.1em} A^{\hspace{-0.1em}\top}\hspace{-0.1em}\widetilde{P}_2B\widetilde{K}_1^* \hspace{-0.1em}+\hspace{-0.1em} (\widetilde{K}^*_1)^{\hspace{-0.1em}\top}\hspace{-0.1em}(R+B^{\hspace{-0.1em}\top}\widetilde{P}_2B)\widetilde{K}^*_1   \nonumber\\
    &\hspace{1em} + \widetilde{K}_1^{\top}B^{\top}\widetilde{P}_2A + A^{\top}\widetilde{P}_2B\widetilde{K}_1 - \widetilde{K}_1^{\top}(R+B^{\top}\widetilde{P}_2B)\widetilde{K}_1 \nonumber\\
    &= \big[(R+B^{\top}\widetilde{P}_2B)^{-1}B^{\top}\widetilde{P}_2A - \widetilde{K}^*_1\big]^{\top}(R+B^{\top}\widetilde{P}_2B) \big[(R+B^{\top}\widetilde{P}_2B)^{-1}B^{\top}\widetilde{P}_2A - \widetilde{K}^*_1 \big] \nonumber\\
    &\hspace{1em}- \big[(R+B^{\top}\widetilde{P}_2B)^{-1}B^{\top}\widetilde{P}_2A - \widetilde{K}_1\big]^{\top}(R+B^{\top}\widetilde{P}_2B) \big[(R+B^{\top}\widetilde{P}_2B)^{-1}B^{\top}\widetilde{P}_2A - \widetilde{K}_1 \big] \label{eqn:complete_square}\\
    &= \big[\widetilde{K}^*_1 - \widetilde{K}^*_1\big]^{\top}(R+B^{\top}\widetilde{P}_2B) \big[\widetilde{K}^*_1 - \widetilde{K}^*_1 \big] - \big[\widetilde{K}^*_1 - \widetilde{K}_1\big]^{\top}(R+B^{\top}\widetilde{P}_2B) \big[\widetilde{K}^*_1 - \widetilde{K}_1 \big] \nonumber \\
    &= - e_1^{\top}(R+B^{\top}\widetilde{P}_2B)e_1, \label{eqn:pdiff2}
\end{align}
where \eqref{eqn:complete_square} follows from completion of squares. Thus, combining  \eqref{eqn:plast}, \eqref{eqn:plast2}, and \eqref{eqn:pdiff2} yields
\begin{align}
    \hspace{1.3em}\|P^*_1-\widetilde{P}_{1}\| &\leq \|P^*_{2} -\widetilde{P}_{2}\|\cdot\varphi\|A\| \| I-\widetilde{P}_2B(R+B^{\top}\widetilde{P}_{2}B)^{-1}B^{\top} \| + \|e_{1}\|^2\|R+B^{\top}\widetilde{P}_2B\| \nonumber \\
    &\leq \varphi\|A\| \left( 1 + \frac{\| P_{\max} + a I \| \|B\|^2}{\lambda_{\min} (R)} \right) \cdot\|P^*_{2} - \widetilde{P}_{2}\| + \|e_{1}\|^2\|R+B^{\top}\widetilde{P}_2B\| \label{eqn:p_contraction}.
\end{align}
Note that the difference between \eqref{eqn:p_contraction} with its counterpart in~\cite{XZ-TB:23} is because the argument for 
\[
\|(I+\widetilde{P}_2BR^{-1}B^{\top})^{-1}\| \leq 1
\]
does not hold, since $\widetilde{P}_2BR^{-1}B^{\top}$ is not necessarily positive semi-definite (a product of two symmetric psd matrices is not necessarily psd unless the product is a normal matrix). Now, we require
\begin{align}
    \|P^*_{2} - \widetilde{P}_{2}\| &\leq \min\bigg\{a, \frac{a}{C_2}, \frac{\epsilon}{4 C_1C_2}\cdot\bigg\} \label{eqn:preq2}\\
    \|e_{1}\| &\leq  \min\bigg\{\sqrt{\frac{a}{C_3}}, \frac{1}{2}\sqrt{\frac{\epsilon}{C_1C_3}}\bigg\}\label{eqn:K_req1},
\end{align}
Then, conditions \eqref{eqn:preq2} and \eqref{eqn:K_req1} are sufficient for \eqref{eqn:preq1} (and thus for $\|\widetilde{K}_{0} - K^*_{0}\| \leq \epsilon/2$) to hold. Subsequently, we can propagate the required accuracies in \eqref{eqn:preq2} and \eqref{eqn:K_req1} forward in time. Specifically,   we iteratively apply the arguments in \eqref{eqn:p_contraction} (i.e., by plugging quantities with subscript $t$ into the LHS of \eqref{eqn:p_contraction} and plugging quantities with subscript $t+1$ into the RHS of \eqref{eqn:p_contraction}) to obtain  the result that if at all $t \in \{2, \cdots, N-1\}$, we require
\begin{align}
    &\|P^*_{t} - \widetilde{P}_{t}\| \leq \min\bigg\{a, \frac{a}{C_2^{t-1}}, \frac{\epsilon}{4 C_1C_2^{t-1}}\bigg\} \label{eqn:preq_allt}\\
    &\hspace{-0.5em}\|e_{t}\| \leq  \min\bigg\{\sqrt{\frac{a}{C_3}}, \sqrt{\frac{a}{C_2^{t-2}C_3}}, \frac{1}{2}\sqrt{\frac{\epsilon}{C_1C_2^{t-2}C_3}}\bigg\} \nonumber,
\end{align}
then \eqref{eqn:preq2} holds true and therefore \eqref{eqn:preq1} is satisfied.

We now compute the required accuracy for $e_{N-1}$. Note that $P^*_{N-1} = \widetilde{P}^*_{N-1}$ since no prior computational errors happened at $t=N$. By \eqref{eqn:p_contraction}, the distance between $P^*_{N-1}$ and $\widetilde{P}_{N-1}$ can be bounded as 
\begin{align*}
    \|P^*_{N-1} - \widetilde{P}_{N-1}\| = \|\widetilde{P}^*_{N-1} - \widetilde{P}_{N-1}\| \leq \|e_{N-1}\|^2\cdot C_3.
\end{align*}
To fulfill the requirement \eqref{eqn:preq_allt} for $t=N-1$, which is
\begin{align*}
    \|P^*_{N-1} - \widetilde{P}_{N-1}\| \leq \min\bigg\{a, \frac{a}{C_2^{N-2}}, \frac{\epsilon}{4 C_1C_2^{N-2}}\bigg\},
\end{align*}
it suffices to let
\small
\begin{align}
    \hspace{-0.4em}\|e_{N-1}\| \leq \min \hspace{-0.1em}\bigg\{\sqrt{\frac{a}{C_3}}, \sqrt{\frac{a}{C_2^{N-2}C_3}}, \frac{1}{2}\sqrt{\frac{\epsilon}{C_1C_2^{N-2}C_3}}\bigg\}\hspace{-0.1em}. \label{eqn:delta0_req}
\end{align}
\normalsize

Finally, we analyze the worst-case complexity of RHPG by computing, at the most stringent case, the required size of $\|e_t\|$. When $C_2 \leq 1$, the most stringent dependence of $\|e_t\|$ on $\epsilon$ happens at $t=0$, which is of the order $\mathcal{O}(\epsilon)$, and the dependences on system parameters are $\mathcal{O}(1)$. We then analyze the case where $C_2 > 1$, where the requirement on $\|e_0\|$ is still $\mathcal{O}(\epsilon)$. Note that in this case, the requirement on $\|e_{N-1}\|$ is stricter than that on any other $\|e_t\|$ for any $t \in \{1, \cdots, N-1\}$ and by \eqref{eqn:delta0_req}:
\begin{align}\label{eqn:delta0}
    \|e_{N-1}\| \sim \mathcal{O}\Big(\sqrt{\frac{\epsilon}{C_1C_2^{N-2}C_3}}\Big).
\end{align}
Since we require $N$ to satisfy \eqref{eqn:N_choice}, the dependence of $\|e_{N-1}\|$ on $\epsilon$ in \eqref{eqn:delta0} becomes 
\[
\|e_{N-1}\| \sim \mathcal{O}\left(\epsilon^{\frac{1}{2} + \frac{\log(C_2)}{4\log{(1/\| A^*_K \|_*})}}\right)
\]
with additional polynomial dependences on system parameters since
\begin{align*}
    C_2^{N-2} &\approx C_2^{\frac{\log(1/\eps)}{2\log(1/\| A^*_K\|_*) }}\\ 
    % &= C_2^{\log_{\frac{1}{\| A^*_K \|^2_*}} (1/\eps)}\\
    &= (1/\eps)^{\frac{\log(C_2)}{2\log{\frac{1}{\| A^*_K \|_*}}}}.
\end{align*}
As a result, it suffices to require error bound for all $t$ to be 
\[
\|e_{t}\| \sim \mathcal{O}\left(\min \left\{\eps, \epsilon^{\frac{1}{2} + \frac{\log(C_2)}{4\log{(1/\| A^*_K \|_*})}} \right\}\right)
\]


The difference between our requirement for the $C_2 > 1$ case with its counterpart in~\cite{XZ-TB:23} is due to a calculation error in~\cite{XZ-TB:23} which incorrectly neglects the impact of the exponent in $C_2^{N-2}$.
Lastly, for $\widetilde{K}_{0}$ to be stabilizing, it suffices to select a sufficiently small $\epsilon$ such that the $\epsilon$-ball centered at the infinite-horizon LQR policy $K^*$ lies entirely in the set of stabilizing policies. A crude bound that satisfies this requirement is
\begin{align*}
    \epsilon < \frac{1 - \|A-BK^*\|_*}{\|B\|} \Longrightarrow \|A-B\widetilde{K}_0\|_* < 1.
\end{align*}
This completes the proof.
\end{document}