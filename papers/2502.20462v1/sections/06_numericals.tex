  
In the following, we present numerical experiments designed to test the performance of the proposed multi-agent reinforcement learning Algorithm \ref{algo:alg_main_algorithm}. We consider a scenario with five robots acting as agents, which navigate and monitor  multiple rooms in a floor plan.
    
\subsection{Floor Plan Navigation}

Consider $N=5$ agents navigating a floor plan of an L-shaped corridor connecting three offices and three laboratories at the University of Pittsburgh, with their separating walls represented as black lines (see Fig. \ref{fig:heatmaps}). In this scenario, the state of agent $n$ at time $t$ is $S^n_t=(x^n_t,y^n_t)$  with $x^n_t$ and $y^n_t$ representing the agent horizontal and vertical coordinates, respectively with $S^n_t \in [0,30]\times[0,14]$, all measured in meters. Correspondingly, the $M=6$ regions    $\mathcal S_1,\ldots,\mathcal S_M$ are depicted by the colored circles in Fig. \ref{fig:heatmaps}, centered at $(x_1,y_1)=(6,9)$, $(x_2,y_2)=(13,9)$, $(x_2,y_2)=(20,9)$, $(x_3,y_3)=(28,4)$, $(x_4,y_4)=(28,8)$, and $(x_5,y_5)=(28,12)$, and with all  radius equal to one. The global reward corresponding to zone $m$ takes the value $(r(S_t))_m=1$ if at least one agent enters the corresponding circle. The action space $\mathcal A=\{0,\ldots,5\}$ is finite, meaning that the agents must decide  which of the six regions to visit at each instant. We assume a low-level robot  navigation control is available to drive the agent across the floor plan toward the selected zone. This controller follows a sequence of segments between intermediate goals located at a set of strategic points in rooms, corridors, and doors connecting any two rooms in the floor plan of Fig. \ref{fig:heatmaps}. 

Next, we describe in more detail how agents gain this ability to identify which zones they should visit by applying Algorithms \ref{alg:offline} and \ref{algo:alg_main_algorithm}. First, each agent learns a set of optimal policy parameters offline using Algorithm \ref{alg:offline}. Specifically, each agent adopts a soft-max policy with exponents given by the $M=6$ logits in the output layer of a two-layer neural network with $256$ hidden neurons whose inputs are the vector $\lambda$ of $M=6$ multipliers and the coordinate $S_t^n$ of the agent.

To optimize \eqref{eqn_optimal_trained_policy} for all agents efficiently, we take a block maximization approach in which individual agents retrain sequentially. We start this procedure by training a single agent in the floor plan environment to obtain a set of parameters $\bar \theta$. Then, all $N$ agents' policies are initialized with the same common parameter $\theta^n=\bar\theta,\ n=1,\ldots,N$, and the retraining sequence starts. In each step of this sequence, an agent is selected to be the active learner, which updates its policy by running the iteration in Algorithm \ref{alg:offline}. All other agents follow trajectories driven by their policies, which remain unchanged. After the active agent retrains in these conditions, its policy settles, and the active token is passed on to the next agent. 
   
During the online execution phase,  all agents initialize their multipliers to zero and then update them following \eqref{eqn_stochastic_dual} with $\alpha=0.01$ and $\eta=0.5$. For a span of $T_0=100$ time steps, each agent will use the realizable policy described in   Algorithm \ref{algo:alg_main_algorithm}. They substitute their local copies $\lambda_k^n\in\mathbb R^M$ of the vector of multipliers in the trained policy $\pi_\theta(S_t,\lambda_k^n)=(\pi_{\theta^1}(S_t^1,\lambda_k^n),\ldots,\pi_{\theta^N}(S_t^N,\lambda_k^n))$, and then keep the $n$-th component   $\pi_{\theta^n}(S_t^n,\lambda_k^n))$. Using this procedure, the agents do not need to know or exchange their local positions. Instead, the coordination is achieved through the dual multipliers \eqref{eqn_stochastic_dual}, which are kept consistent among agents thanks to the network gossiping of reward estimations  \eqref{eqn_gossip_r}. As detailed in Section\ref{ssec:numerical_stochastic_graph} below, the agents in this experiment gossip through an ad-hoc communication network that resembles the probabilistic model \eqref{eqn_graph_model}, in which two agents communicate if they are inline of sight or near each other.   

The coordinated behavior in which the agents take complementary paths is observed in Fig. \ref{fig:heatmaps}, where we show $N=5$ heat maps, each corresponding to the trajectories of an agent during the online execution phase.   Specifically, each of the $N=5$ color graphs in  Fig. \ref{fig:heatmaps}, one per agent,  is obtained by dividing the floor plan into $120\times56$ square bins of side $dx=dy=0.25$ and computing a two-dimensional histogram by counting how many times the agent enters each bin during a trajectory of $T=40,000$ time steps. The color scale is logarithmic,  which implies that the agents occupy most of their time inside the circular regions. The paths in Fig. \ref{fig:heatmaps} demonstrate how the agents learn to coordinate in order to attend different regions and thus satisfy the competing constraints. The constraints for this numerical experiment are defined by the thresholds $c=(0.1,0.2,0.3,0.4,0.5,0.6)$ for the red, green, blue, orange, cyan, and magenta regions, respectively. Since the sum of these thresholds is $\|c\|_1=2.1$ greater than one, a single agent working alone cannot satisfy the constraints. And because there are more zones $M=6$ than agents $N=5$ the constraints cannot be satisfied by placing an agent in each zone. By applying Algorithms \ref{alg:offline} and \ref{algo:alg_main_algorithm}, the agents learn to automatically achieve a high-level coordination in which agents $n=0,1,2,3$ and $4$ are sent to attend most of their time at the cyan, orange, blue, green and magenta zones, respectively, and they use their spare time to collaborate in satisfying the constraint corresponding to the red zone. The assignment is not trivial because it depends on the relative weights of the thresholds and the proximity of the zones. For instance, agent $n=3$ contributes most of its time to attending the red zone, but it is otherwise attending the green zone in the next room, so it is the closest one to help. Moreover, it is intuitive that the red and green zones can split an agent because they are associated with less demanding constraints.    

\begin{figure}[t]
    \centering \input{figs/constraint_satisfaction_floorplan}
    %\includegraphics[scale=0.9]{./figs/constraint_satisfaction_floorplan.pdf}
    \caption{Satisfaction of the constraints for each zone $m=1,\ldots,6$. Constraint requirements were defined as
$0.1$ for zone 1, $0.2$ for zone 2, etc. Dashed lines indicate these constraints. Minimum and maximum satisfaction values are plotted for each timestep for each zone and are filled by a shaded region. Colors of constraint and satisfaction values match those of the zones as depicted in Fig. \ref{fig:heatmaps}.}
    \label{fig:satisfaction_floorplan}
\end{figure}

\begin{figure*}[t!]
   \centering
   \begin{subfigure}{0.45\columnwidth}
  		 \centering
   		\input{figs/gossip_matrix}
		%\includegraphics[scale=0.5]{figs/gossip_matrix.pdf}
   		\caption{Communication matrix.}
   		\label{fig:gossip_matrix}
   \end{subfigure} 
   \begin{subfigure}{0.775\columnwidth}
   		\centering
   		\input{figs/gossip_trajectories}
		%\includegraphics[scale=0.8]{figs/gossip_trajectories.pdf}
   		\caption{Communication neighborhoods.}
   		\label{fig:gossip_trajectories}
   \end{subfigure}
   \begin{subfigure}{0.775\columnwidth}
   		\centering
		\input{figs/margin_vs_radius}  
   		\caption{Constraint margins.}
   		\label{fig:communication_discs}
   \end{subfigure}
   \caption{
     (a) Matrix of communication frequencies between agents, counted over time. For each pair of
agents ($n$, $n^\prime$), the number of timesteps during which these agents communicate is summed and divided by the total timesteps of simulation, $40{,}000$. Frequencies are indicated by colors matching those in the included colorbar, with white indicating no communication. We define an agent as never communicating with itself, so diagonal entries are white. Figure (b): A snapshot of gossip neighborhood sizes per agent. Neighborhood sizes are plotted for $1{,}000$ time-steps of execution phase. $N=5$ lines are present, one for each agent and of a color matching said agentâ€™s trajectory in Fig \ref{fig:heatmaps}. Figure (c): Margin of constraint satisfaction for each communication disc of sizes $d=1,\ldots,6$. A minimum and maximum difference between satisfaction and constraint values across all zones $m = 1,\ldots,6$ are found and plotted for each disc size, as indicated by the bold blue lines. A band shades the area between the maximum and minimum differences.}
\end{figure*}
   
Overall, the underlying mechanism agents use to coordinate is to let themselves be driven by the highest multipliers' values while avoiding flocking. The high-level control that drives the agents is akin to a standard dual-optimal dynamic, in which each multiplier (increases) decreases when its corresponding zone is (not) being attended, making it (more) less urgent for the agents to visit it. Indeed, the policies are trained to seek the highest weighted reward, $r_\lambda(S_t)=\lambda^\top(r(S_t)-c)=\sum_{m=0}^M(\lambda)_m((r(S_t)-c)_m)$ and this is achieved by setting to one the rewards $(r(S_t))_m$ corresponding to the highest multipliers, as it was argued for the ideal policy $\pi_I$ in the proof of Lemma \ref{lemma:positive_ideal_value}. 

Figure \ref{fig:satisfaction_floorplan} presents the resulting performance in terms of constraint satisfaction. The dashed horizontal lines in Fig. \ref{fig:satisfaction_floorplan} represent the thresholds $c$ the agents must comply with. The colors correspond to the zones in Figure \ref{fig:heatmaps}. The curves correspond to the long-run time averages $\bar r(S_t)=\frac{1}{t}\sum_{\tau=0}^{t-1}r(S_{\tau})$ over the  span of $40{,}000$ time steps. Notice that we use the global rewards $r(S_t)$ as compared to $R_{\tau,t}^n$, even though they are never computed by the agents when running Algorithms \ref{alg:offline} and \ref{algo:alg_main_algorithm}, because these are the ones defining our original optimization problem \eqref{eqn_crl}. Accounting for the randomness of the reward trajectories, we  run Algorithm \ref{algo:alg_main_algorithm} five times and report a band between the maximum and minimum values of the averaged rewards $\bar r(S_t)$.    The results demonstrate that the agents can meet the specified constraints consistently for all runs so that all zones are adequately monitored over time, outperforming numerically the almost sure theoretical guarantees provided by Theorem \ref{theorem:feasibility}. 

\subsection{Communication over a Stochastic Graph}\label{ssec:numerical_stochastic_graph}

Purposely, the communication setup in the experiment described above does not exactly match the stochastic graph model with Bernoulli edges in \eqref{eqn_graph_model}. Instead, we consider a practical ad-hoc network that changes its connectivity as agents move across the floor plan. Specifically, two agents communicate if they are in the line of sight of each other regardless of their distance, or if they are closer than a range disc size $d= 5$ meters separated by a wall. The fraction of time that each pair of agents communicate to each other along a trajectory of $40{,}000$ steps is depicted in Fig. \ref{fig:gossip_matrix}. This representation shows that agents $n=0$ and $n=4$ remain connected most of the time,  reflecting that they are within communication range, because they share similar paths and spend most of their time stationed in rooms next to each other. All agents connect to at least one other agent over time, enabling the gossip protocol \eqref{eqn_gossip_r} to succeed in passing the reward estimates between any two agents via multi-hop communications. Hence, the underlying static graph is connected, but not its stochastic samples, since the agent $n=3$ is frequently isolated from all other agents, probably when visiting the remote red zone, so that we would see a clustered network most of the time if we sampled the edges per time step. This intermittent communication connectivity plays a central role in the agents' behavior. For instance, Fig. \ref{fig:floorplan} shows the blue trajectory of an agent entering the third room from the left but changing its path towards the magenta zone once it forms a line of sight with the orange agent attending the blue zone.  

Another perspective of the network connectivity is given  by the trajectories in Fig. \ref{fig:gossip_trajectories}, which shows how the sizes of the communication neighborhoods evolve across time during a span of one thousand timesteps. There are five lines in Fig. \ref{fig:gossip_trajectories}, representing the number of neighbors for each agent, with the same color-to-agent correspondence as in Fig. \ref{fig:floorplan}. These trajectories corroborate that the agents do not flock together since they isolate reaching zero neighbors part of the time and are rarely connected to two other agents. It also shows that they seldom confer altogether, only two times in a thousand, so they need to transmit the rewards across the stochastic network by moving and passing messages dynamically. We can also infer from Fig. \ref{fig:gossip_trajectories} that the agents cannot remain static in a zone during the total span of the experiment. The communication neighborhoods change dynamically as agents adjust their trajectories and spend part of their time for communication purposes, alternating between zones aiming to increase their joint reward and exchange information with their pairs.  This behavior aligns with the observation that none of the agents hold their position throughout the experiment in Fig. \ref{fig:heatmaps}, which is reasonable from the algorithmic perspective since an isolated agent sitting in a zone would see all other $M-1$  multipliers  growing, and it would be therefore inclined to move to attend their corresponding zones, leaving its own.    

Finally,  we analyze how varying the communication range affects performance. Fig. \ref{fig:communication_discs} examines the margin of constraint satisfaction relative to the size of the communication disc $d$. For $d=1,\ldots,6$,  the blue lines represent   the minimum difference between the average rewards and constraint thresholds across zones at time $t=40{,}000$. That is, $\min_m (\bar r_{40,000}-c)_m$. The experiment is repeated five times to account for the randomness in the trajectories, and a band between the maximum and the minimum margin is presented in Fig. \ref{fig:communication_discs}. 
   
The results show that the margin of constraint satisfaction results in negative values for small values of $d$, therefore not complying with primal problem \eqref{eqn_crl},  increasing above $0$ for when the communication range increases above $d=4$. This dependence of the margin with $d$ aligns with our theory in Theorem \ref{theorem:feasibility}, considering that a larger disc corresponds with a higher probability $p$. Thus, \eqref{eqn_as_feasibility} tells us that the error of feasibility becomes smaller when $\alpha$ decreases, which according to \eqref{eq:alpha_bound} is driven by increasing the probability $p$. Alternatively, for a fixed value of the parameter $\alpha$, equation \eqref{eq:alpha_bound} can only be satisfied if $p$ becomes big enough.
 
In summary, these numerical experiments illustrated how our coordinated offline training, dual dynamics, and gossiping protocol provide a system of multiple agents with the ability to coordinate their actions.      %
The simplified stochastic graph model for communication in \eqref{eqn_graph_model} has the virtue of being tractable, which allowed us to advance our analysis of feasibility summarized in Theorem \ref{theorem:feasibility} while  capturing the necessary properties of a more realistic distance-defined network in terms of probabilities and time-evolving connectivity. Hence, the experiments abide by the theory, so the numerical results corroborate  our theoretical feasibility guarantees.      
  