\label{sec:offline_training}

This section presents two algorithms for training the policy and carrying out the tasks. Combined, they solve the CRL problem \eqref{eqn_crl} in the dual domain~\cite{paternain2019constrained}.

Define the Lagrangian associated with problem \eqref{eqn_crl}
%
\begin{equation}\label{eqn_lagrangian}
\mathcal{L} (\pi,\lambda) = \lim_{T\to \infty}\frac{1}{T} \mathbb E_{S_t, A_t\sim \pi} \left [ \sum_{t=0}^{T-1} \lambda^\top\left(r(S_t)-c\right)\right ],
\end{equation}
%
where  $\lambda \in\mathbb{R}_+^M$ denote the dual variables. To obtain the dual, we maximize the Lagrangian in \eqref{eqn_lagrangian}. However, we must divert from standard primal-dual algorithms~\cite{arrow1958studies} that aim for a dual-optimal vector of multipliers. That is because, under the reward structure of \eqref{eqn_patrolling_reward}, the primal maximizer of \eqref{eqn_lagrangian} for the optimal multipliers does not guarantee a feasible policy for \eqref{eqn_crl}. Indeed, the smallest $M-N+1$ dual-optimal multipliers must be equal to each other because\textemdash as we prove in Lemma \ref{lemma:positive_ideal_value}\textemdash otherwise  agents only satisfy the $N$ constraints with highest weighted rewards, leaving the other $M-N$ unattended. But with $M-N+1$ multipliers being equal, the corresponding constraints are indistinguishable when maximizing \eqref{eqn_lagrangian} since any policy followed by the agents with respect to these constraints receives the same weighted reward.  For instance, any infeasible policy in which the state of the agents satisfies a single of these $N-M+1$ constraints will be a primal maximizer of \eqref{eqn_lagrangian}. Even a policy in which the agents' states wander erratically across the regions with equal multipliers, without satisfying any of the  $N-M+1$ constraints, will maximize \eqref{eqn_lagrangian}. 

The existence of multiple primal maximizers for the optimal dual prevents the convergence of primal-dual algorithms. Indeed, even if we are given the optimal multiplier, when computing the maximization with respect to the primal variable will recover one of the maximizers which as discussed earlier may not be feasible.
This limitation of primal-dual methods is not specific to our problem formulation but was demonstrated in \cite{calvo2023state}  and appears even in convex optimization problems when the Lagrangian is not strictly convex, in which case iterates need to be averaged to achieve convergence~\cite{nedic2009subgradient}. However, we cannot rely on averaging for our assignment problem \eqref{eqn_crl}  since the argument for dual optimality of the averaged multipliers relies on the convexity of the primal.

In our constrained RL scenario, we aim to solve problem \eqref{eqn_crl} by an alternative two-step process. First, in an offline training stage, we obtain a policy that optimizes the Lagrangian in \eqref{eqn_lagrangian} for all $\lambda$. In the deployment stage, we apply a stochastic dual gradient descent iteration, which causes cycles in $\lambda$, and let the agents choose a different policy  per rollout, thus preventing an infeasible policy resulting from convergence of $\lambda$ to the set of optimal multipliers. As multipliers cycle, the rewards in \eqref{eqn_lagrangian} are reweighted, the highest entries of $\lambda$ alternate, and thus the policy maximizing \eqref{eqn_lagrangian} gives priority to constraints that have not being attended in previous cycles.     

{Before describing these two stages in more detail,  we introduce the following two practical considerations into our design. 
First, to prevent agents from requiring a full observation of the global $S_t$ and  shield our policy from the  exponential growth  of the state space, we introduce the following structure
$\pi(A_t|S_t,\lambda^1,\ldots,\lambda^N) = \prod_{n=1}^N \pi^n(A_t^n| S_t^n,\lambda^n)$, in which each agent decides its action taking into account its state $S_t^n$  and local copies $\lambda^n$ of $\lambda$ only. 

Note that even if the policies are local, the agents are still coupled by their dynamics and joint rewards. Therefore, the agents still need to optimize the Lagrangian \eqref{eqn_lagrangian}  jointly, i.e., 
%
\begin{align}\label{eqn_optimal_set}
&\pi^\star[\lambda,\ldots,\lambda] = \argmax_{\pi[\lambda,\ldots,\lambda]} \lim_{T\to \infty}\frac{1}{T} \mathbb E_{S_t, A_t\sim \pi} \left [ \sum_{t=0}^{T-1} r_{\lambda}(S_t)\hspace{-1pt} \right ],
\end{align}
in which we reduced notation by optimizing over the global policy $\pi[\lambda,\ldots,\lambda]$  defined as 
%
\begin{align}\label{eqn_ideal_separated_policy}\pi[\lambda,\ldots,\lambda](A_t|S_t) &:=\pi(A_t|S_t,\lambda,\ldots,\lambda)\\&=\prod_{n=1}^N \pi^n(A_t^n|S_t^n,\lambda),
\end{align}
%
and simplified \eqref{eqn_lagrangian} by defining the following weighted reward 

\begin{equation}\label{eqn_reward_lambda}
    r_\lambda(S_t) :=  \lambda^\top\left(r(S_t)-c\right).
\end{equation}
%
Furthermore, we parameterize each agent's policy by a vector $\theta^n \in \mathbb{R}^{p_n}$. A judicious parameterization will allow us to deal with the augmented state-space $\mathcal{S}\times {\mathbb{R}^M_+}$,  containing elements $(S_t,\lambda)$, where at least the multipliers are continuous variables. Hence, each agent's action is drawn from a distribution~$\pi_{\theta^n}(A_t^n \mid S_t^n, \lambda)$, so that the joint probability distribution with parameters $\theta=(\theta^1,\ldots,\theta^N)$ is 
%
\begin{align}\label{eqn_separate_policy}
    \pi_\theta[\lambda,\ldots,\lambda](A_t\mid S_t)&:=\pi_\theta(A_t\mid S_t,\lambda,\ldots,\lambda)\\&=\prod_{n=1}^N\pi_{\theta^n}(A_t^n\mid S_t^n,\lambda).
\end{align}  
%
Next, we describe how to train for these parameters $\theta$.

\subsection{Offline Training}\label{ssec_offline_training}
The policy structure in \eqref{eqn_separate_policy} induces a simplified form of the policy gradient \cite{sutton2000policy} that we will use for training. 
Before presenting this result in Proposition \ref{prop_gradients}, we define the following key concepts.
%For fixed multipliers and parameters $\theta \in \mathbb{R}^p$, where $p = \sum_{n=1}^N p_n$, we define the following 
The occupancy measure for the joint state and actions of all agents is given by
%
%\begin{equation}\label{eqn_stationary_distribution}
% $   \rho_{\theta,\lambda}(s,a) = \lim_{t\to \infty} \mathbb{P}(S_t=s,A_t=a).$
%\end{equation}
$  \rho_{\theta,\lambda}(s,a) = \lim_{t\to \infty}\frac{1}{t}\sum_{\tau=0}^t \mathbb{P}(S_\tau=s)\pi(a|s).$
%
Likewise,   the state-action value function is defined by $Q_{\pi_\theta}(s,a,\lambda) = \sum_{t=0}^{\infty} \mathbb{E}_{S_t,A_t\sim \pi_\theta}\left[r_\lambda(S_t)-\mathcal L(\pi_\theta,\lambda)| S_0=s,A_0 =a\right],$
%\begin{equation}
%
where $r_\lambda(S_t)$ and $\mathcal{L}(\pi_\theta,\lambda)$ are the functions defined in \eqref{eqn_reward_lambda} and \eqref{eqn_lagrangian}, respectively. 
We further define the state-action value function for agent $n$ as
%
\begin{equation}\label{eqn_q_others}
    Q^{n}_{\pi_\theta}(s,a,\lambda) = \mathbb{E}_{(S_t,A_t)}\left[Q_{\pi_\theta}(S_t,A_t,\lambda)\mid S_t^n=s, A_t^n=a \right].
\end{equation}
%
with expectation over $(S_t,A_t)$ with probability  $\rho_{\theta,\lambda}(S_t,A_t)$.


From the perspective of agent $n$, the local Q-function \eqref{eqn_q_others} is the average of the expected return, assuming that all other agents follow the policy $\pi_\theta[\lambda,\ldots,\lambda]$. \textcolor{black}{Therefore,  $Q^{n}_{\pi_\theta}(S_t,A_t,\lambda)$ can be estimated per agent by averaging the global rewards that result from their local states and actions, or modeled as a parametric function of these local entries.} The following proposition formalizes these insights, providing a specific form for the policy gradient Theorem under the separable policy design.

\begin{proposition}\label{prop_gradients}
Assume that the policy of each agent is parameterized by a vector $\theta^n\in\mathbb{R}^n$ and that $A^n\sim\pi_{\theta^n}(\cdot| S^n,\lambda)$, where  $S^n$ represents the local state of agent $n$. Let $\mathcal{L}(\pi_\theta,\lambda)$ and $Q^{n}_{\pi_\theta}(S^n,A^n,\lambda)$ be the functions defined in \eqref{eqn_lagrangian} and \eqref{eqn_q_others} respectively. Then, it follows that
%
%\begin{equation}
  $  \nabla_{\theta^n} \mathcal{L(\pi_\theta,\lambda)} = \mathbb{E}_{(S^n,A^n) \sim \rho_{\theta,\lambda} }\left[\nabla_{\theta^n }\log \pi_{\theta^n}(A^n|S^n,\lambda)Q^{n}_{\pi_\theta}(S^n,A^n, \lambda)\right].$
  %\label{eq:local_gradient}$
%\end{equation}
\end{proposition}
\begin{proof}See Appendix \ref{app:proof_prop_gradients}.
\end{proof}
%
  In practice, Proposition \ref{prop_gradients} tells us that agent $n$ can compute \emph{locally} its part of the gradient $\nabla_{\theta^n} \mathcal{L(\pi_\theta,\lambda)}$, the part needed to update its own local parameters $\theta^n$, by using its local policy $\pi_{\theta^n}(A_t|S_t,\lambda)$, the multiplier $\lambda$, and the local Q-function in \eqref{eqn_q_others}  averaging over the state and actions of the other agents.
%
Hence, we leverage this local property of the gradients to optimize a  parametric version of \eqref{eqn_optimal_set}
%
%
\begin{align}\label{eqn_optimal_trained_policy}
&\pi_{\theta^\star}[\lambda,\ldots,\lambda] = \argmax_{\pi_{\theta}[\lambda,\ldots,\lambda]}  \mathbb E_{S_t, A_t\sim \pi_\theta} \left [ \frac{1}{T_0}\sum_{t=0}^{T_0-1} r_{\lambda}(S_t,A_t) \right ],
\end{align}
%
where we used a slight abuse of notation to write a policy $\pi_\theta[\lambda,\ldots,\lambda]$ complying with \eqref{eqn_separate_policy} as the optimization variable instead of $\theta$.     Notice that, in addition to the parametric expansion of the policies, we introduced a second variation by removing the limit and adhering to a finite time horizon. This modification is key for the practical implementation of the training algorithm, which is detailed in Algorithm \ref{alg:offline}. {Let us remark that the policy parameters that result from   \eqref{eqn_optimal_trained_policy} do not depend on $\lambda$. The augmented state $(S_t^n,\lambda)$  is the input of each local policy $\pi_{\theta^n}$ with parameters $\theta^n$ as in \eqref{eqn_separate_policy}.
}%blue 


 The modifications above (parameterization and finite horizon) introduce errors in the approximation of \eqref{eqn_optimal_set}. We formalize our assumptions on the errors next. Before doing so, we  define the truncated value $V_{T_0}(\pi)$ which becomes  
%
\begin{equation}\label{eq:non-idealities-VP}
V_P:=V_{T_0}(\pi_\theta[\lambda,\ldots,\lambda]):=\mathbb E_{\pi_\theta}\left[\frac{1}{T_0} \sum_{t=0}^{ T_0-1} r(S_t)\ \right],
\end{equation}
under policy parameterization, so that $V_P$ corresponds to the finite-horizon expected cumulative reward attained by the team when running the structured policy \eqref{eqn_separate_policy} for a particular value of $\lambda$ common to all agents.

%\santiago{$V_{T_0}$ is not defined yet or I couldn't find it.}
%\juan{I was trying to define both $V_P$ and $V_{T_0}$ on the same line. Check it now. I rewrote these lines to make it explicit.  }

\begin{assumption}\label{assumption_representation}(Approximation error in training).
The policy parameterization is dense enough, and the horizon is sufficiently large, i.e.,
 For any $\epsilon>0$ there exists $\beta>0$ and $T_0>0$, such that for each  policy $\pi[\lambda,\ldots,\lambda]$ in \eqref{eqn_ideal_separated_policy} there exists a set of parameters $\theta$ and associated  policy $\pi_\theta[\lambda,\ldots,\lambda]$ in \eqref{eqn_separate_policy} such that the respective values $V=V(\pi[\lambda,\ldots,\lambda])$ and $V_P=V_{T_0}(\pi_\theta[\lambda,\ldots,\lambda])$ defined in  \eqref{eqn_ideal_V}  and  \eqref{eq:non-idealities-VP}, satisfy
%
\begin{align}\label{eqn_representation_error}
\lambda^\top(V-V_P)&\leq \beta \|\lambda\|+\epsilon.
\end{align}
\end{assumption}

This assumption is tantamount to a universal approximation property, which is standard, for instance, if the policies are parameterized via neural networks. 
The term $\epsilon$ in \eqref{eqn_representation_error}  is introduced to bound the truncation error. This bound is not an assumption actually, but results from the rewards in \eqref{eqn_patrolling_reward} being bounded.  In addition, $\epsilon$ can accommodate the limited accuracy in solving \eqref{eqn_optimal_trained_policy} that results if running a finite number of optimization iterations.


\begin{algorithm}[t]
\caption{Offline training loop for agent $n$}
\label{alg:offline}
\begin{algorithmic}[1]
\FOR{$k=0,1,\ldots$}
  \STATE Sample $\lambda\in \Lambda\subset \mathbb  R_+^M$
  % \FOR{$n=1$ to $N$}
    %\STATE Fix policies $\pi_{\theta^m}$ for all $m \neq n$
    \STATE Sample $S_0^n \sim \mathcal S$
    \FOR{$t=0,\ldots,T_0$}
      \STATE Sample $A_t^n \sim \pi_{\theta^n}(A_t^n \mid S_t^n, \lambda)$
      %\STATE Sample $A_t^m \sim \pi_{\theta^m}(A_t^m \mid S_t^m, \lambda)$ for all $m \neq n$
      \STATE Update $S_{t+1}$
      \STATE Compute $r(S_{t+1})$
    \ENDFOR
    \STATE Compute $\nabla_{\theta^n} \mathcal{L(\pi_\theta,\lambda)}$ as in Proposition \ref{prop_gradients}
    \STATE Update $\theta^n\leftarrow \theta^n+\eta  \nabla_{\theta^n} \mathcal{L(\pi_\theta,\lambda)}$
    %\STATE Update $\theta^n$ using stored samples
  % \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
%
%\noindent{\textbf{Remark}} 
Training for \eqref{eqn_optimal_trained_policy} requires that all agents interact to learn from rewards \eqref{eqn_reward_lambda} that are jointly activated via \eqref{eqn_patrolling_reward}. This training phase is designed to run offline, with multiple agents and a common $\lambda$ drawn randomly at the start of an episode and fixed until the end of it.  Once each policy $\pi_{\theta^n}(A_t^n|S_t^n,\lambda)$ in \eqref{eqn_optimal_trained_policy} has been trained, its online execution depends only on the local state $S_t^n$ of the agent, and a common vector $\lambda$ which varies as agents enter and exit the zones. Thus, agents must engage in online coordination to concur on 
the trajectory of $\lambda$, as described below.
%
