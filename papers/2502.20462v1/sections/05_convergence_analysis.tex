
In this section we present the main result of our work which is to guarantee that the trajectories generated by Algorithm \ref{algo:alg_main_algorithm} are almost surely feasible in the sense that the time-averaged rewards exceed thresholds $c$, within an error controlled by the parameter $\alpha$, with probability one.

\subsection{Main Result}\label{ssec_main_result} 

Before stating the feasibility guarantees we require an additional assumption regarding the underlying MDP.

\begin{assumption}\label{assumption_noforces}(No repelling forces).
\textcolor{black}{The underlying Markov decision process is such that, given $m$, there exists a policy  structured as in  \eqref{eqn_ideal_separated_policy} which satisfies $\left(r(S_t)\right)_m=1$ for all $t$.}
\end{assumption}

This assumption implies that, if necessary, an agent can keep its state stationed in a particular region $S_{t}^n\in \mathcal S_m$. We added this assumption for simplicity to attain $(V)_m=1$ and make the constraint $m$  feasible with a margin  $1-(c)_m$.
Assumption \ref{assumption_noforces} can be substituted by a more general version $(V)_m\geq c>(c)_m$, only affecting the parameter $\delta$ in the following Theorem \ref{theorem:feasibility}. This milder version would hold, for instance, if $S_{t}^n\in \mathcal S_m$ is attainable a fraction of time $c\leq 1$.

We are now in conditions to state our main result.

\begin{theorem}\label{theorem:feasibility}
Let Assumptions \ref{assumption_representation}--\ref{assumption_noforces} hold, and consider the specifications $\|c\|_\infty <1$,  and $\|c\|_1 \leq N-1$. If 
\begin{align}\label{eq:delta_theorem}
\delta:=\left(1-\|c\|_\infty\right) - M\left(\epsilon+\beta\right)>0,
\end{align} 
%
the trajectories generated by Algorithm \ref{algo:alg_main_algorithm} over the stochastic  communication network \eqref{eqn_graph_model}  are feasible within an error  $\sqrt{\alpha M}$ with probability one, i.e.

\begin{equation}\label{eqn_as_feasibility}
    \liminf_{T\to \infty}\frac{1}{T} \sum_{t=0}^{T-1}  r(S_t)\geq c - \mathbf  1 \sqrt{\alpha M}, \mbox{ a.s.}
    \end{equation} %blue
with $\mathbf 1$ being the vector of all ones and 
\begin{equation}\label{eq:alpha_bound}
    \alpha\geq\frac{\eta d_G}{p T_0}  \frac{M L}{\delta}.
\end{equation}
\end{theorem}
\begin{proof}
    See Section \ref{ssec:proof_th}.
\end{proof}

Since the constraints  \eqref{eqn_crl} involve time averages of binary rewards, the thresholds in $c$ must be no larger than one for a feasible policy to exist. Theorem \ref{theorem:feasibility} imposes a stricter requirement $\|c\|_\infty <1$, %, such that all thresholds in the vector $c$ are strictly less than one, 
leaving a margin for the realizable policies.  Also  $\|c\|_1 \leq N-1$ is imposed, ensuring that $N-1$ agents can satisfy all constraints with a margin for the transients when agents' states cycle around regions $\mathcal S_m$. Additionally, condition \eqref{eq:delta_theorem} %requires not only $\|c\|_\infty <1$ but also 
imposes limits on the approximation error in training, with $\beta$ and $\epsilon$ being the quantities defined in Assumption \ref{assumption_representation}.%eqthat the parameterization is sufficiently expressive and the horizon $T_0$ sufficiently large to make $\beta + \epsilon$ small enough.  

The result in Theorem guarantees that we can attain almost sure feasibility of a surrogate problem \eqref{eqn_crl} where we tighten the constraints by increasing the thresholds to leave a margin for the error $\sqrt{\alpha M}$ in \eqref{eqn_as_feasibility}, and we run Algorithms \ref{alg:offline} and \ref{algo:alg_main_algorithm} using these surrogate thresholds. The value of $\alpha$ in \eqref{eq:alpha_bound} can be made arbitrarily small by designing a rich parameterization or neural network, a long time horizon $T_0$, and a small step size $\eta$. These parameters introduce a trade-off between reducing the error and slowing training.

The result in Theorem \ref{theorem:feasibility} is derived by unrolling the stochastic dual recursion in \eqref{eqn_stochastic_dual} to write the sum of the rewards in terms of the multipliers, which yields an expression for the feasibility error in terms of the stochastic average of the multipliers along a trajectory. 
Hence, the proof is completed by bounding this stochastic average by $\eta \sqrt{M/\alpha}$. 
%
To obtain this bound for the stochastic average, we utilize proof techniques similar to those in stochastic gradient descent on the dual domain,
 for which it is key to show that the gradient of the dual forms an acute angle with the vector of multipliers, i.e.    
\begin{align}
\lambda^\top (V-c)&\geq 0.\label{eqn_accute_angle}
\end{align}

In this direction, Lemma \ref{lemma:positive_ideal_value} below indicates that if we use the ideal policy $\pi^\star[\lambda,\ldots,\lambda]$ in \eqref{eqn_optimal_set}, the condition  \eqref{eqn_accute_angle} is satisfied even with a slack  $(1-\|c\|_{\infty})\|\lambda\|/\sqrt{M}$, that will become handy when considering the realizable policy in \eqref{eq:non-idealities-VR} instead.  The proof of Lemma \ref{lemma:positive_ideal_value} uses Assumption \ref{assumption_noforces} to ensure that the underlying MDP does not prevent the agents from taking care of the most pressing constraints. The value $V$ in \eqref{eqn_ideal_V} and \eqref{eqn_accute_angle} assumes that we could train with infinite horizon, without the limitations of parametrization, and with all agents having access to the same  multipliers in \eqref{eqn_contractive_big_brother}. On the other end, $V_R$ in \eqref{eq:non-idealities-VR} corresponds to the  realizable policy in Algorithm \ref{algo:alg_main_algorithm},  and is obtained by  applying  the distributed, finite horizon  and parametric procedures behind equations \eqref{eqn_optimal_trained_policy} and \eqref{eqn_stochastic_dual}. In particular, each agent utilizes its own copy of local multipliers in \eqref{eqn_stochastic_dual}. In between $V$ and $V_R$, we defined  $V_P$ in \eqref{eq:non-idealities-VP} as a theoretical value to be used as an intermediate step in the proofs of feasibility. It differs from $V$ in using a parametric policy and in aggregating the rewards for a finite time period of length $T_0$. And $V_P$ is different from $V_R$ in assuming global access by all agents to the same multipliers. We use Assumptions \ref{assumption_representation} and \ref{assumption:lipschitz} in Lemma \ref{lemma:error_gradient} to bound the difference $\lambda^\top(V-V_R)$. If this difference is lower than the slack $(1-\|c\|_{\infty})\|\lambda\|/\sqrt{M}$, then \eqref{eqn_accute_angle} will still be satisfied if we substitute  $V_R$ for $V$. 

\subsection{Proof of Theorem \ref{theorem:feasibility} and Supporting Results}\label{ssec:proof_th}

We proceed to prove our first result. The intuition in Lemma \ref{lemma:positive_ideal_value} is that if we fix $\lambda$, the ideal optimal policy assigns the $N$ agents to the regions $\mathcal S_m$ with highest weighted rewards. As the multipliers cycle in the execution phase according to Algorithm \ref{algo:alg_main_algorithm}, this ideal behavior will drive the agents to cyclically attend the most pressing constraints at each rollout.   

\begin{lemma}\label{lemma:positive_ideal_value}
Assumption \ref{assumption_noforces} with $\|c\|_{\infty}<1$ and $\|c\|_1 \leq N-1$  as in Theorem \ref{theorem:feasibility} results in
 %
 \begin{align}\lambda^\top (V-c)&\geq (1-\|c\|_{\infty})\frac{\|\lambda\|}{\sqrt{M}}.\label{eqn_lemma_vI}
 \end{align}
 \end{lemma}
%
\begin{proof}
Without loss of generality, sort the multipliers $\left(\lambda\right)_1\geq \left(\lambda\right)_2\geq\cdots\geq \left(\lambda\right)_M$, so that $\left(\lambda\right)_1$ is the largest multiplier.

One policy that optimizes the Lagrangian would assign an agent to each region $\mathcal S_m$ of the state space for $m=1\ldots,N$, since there are only $N<M$ agents. Therefore, $\left(V\right)_m=1$ for $m\leq N$ and $\left(V\right)_m=0$ for $m>N$. Hence it follows that
%
\begin{align}\label{eq:inner_product_decomposition}
\lambda^\top (V-c) &= \left(\lambda\right)_1 (1-\left(c\right)_1) \\
&+ \sum_{m=2}^N \left(\lambda\right)_m (1-\left(c\right)_m) 
+\hspace{-9pt}\sum_{m=N+1}^M\hspace{-5pt}\left(\lambda\right)_m (0-\left(c\right)_m). \notag
\end{align}

Next, we use the fact that $\left(c\right)_1 \leq \|c\|_{\infty}$ by definition of the infinity norm, so that $(1-(c)_1)\leq (1-\|c\|_\infty)$. Also, the multipliers were sorted in decreasing order which implies that the first one is maximum $(\lambda)_1=\| \lambda\|_\infty$, the following  $N-1$  satisfy $ (\lambda)_m\geq (\lambda)_N$ for all $2\leq m\leq N$, and the last $M-N$ satisfy $(\lambda)_m\leq (\lambda)_N$ for all $m>N$. Hence, we bound the three terms in  \eqref{eq:inner_product_decomposition} by
\begin{align}
\lambda^\top (V-c) &\geq (1-\|c\|_{\infty}) \|\lambda\|_\infty\\ &+(\lambda)_N  \sum_{m=2}^N (1-\left(c\right)_m)
-(\lambda)_N  \sum_{m=N+1}^M \left(c\right)_m\nonumber\\
%&=(1-\|c\|_{\infty}) \|\lambda\|_{\infty} +(\lambda)_N \left((N-1) - \sum_{m=2}^M \left(c\right)_m\right)\\
&\geq (1-\|c\|_{\infty}) \|\lambda\|_{\infty} +(\lambda)_N \left((N-1) - \|c\|_1\right), \nonumber
\end{align}
%
where we grouped the terms multiplying $\lambda_N$ and then used $\|c\|_1=\sum_{m=1}^M (c)_m\geq \sum_{m=2}^M (c)_m$. Given that  $(N-1) \geq \|c\|_1$ by hypothesis, we can write
%
\begin{align}
\lambda^\top (V-c) &\geq (1-\|c\|_{\infty}) \|\lambda\|_{\infty}. 
\end{align}
%
Then use the property $\|\lambda\|_{\infty} \geq \|\lambda\|/\sqrt{M}$.
\end{proof}

The following results bounds the error incurred in executing the realizable policy \eqref{eqn_realizable_policy} as compared to the ideal policy \eqref{eqn_ideal_separated_policy}. Intuitively, Lemma \ref{lemma:error_gradient} guarantees that when agents use the realizable policy, they still behave as in the ideal case of  Lemma \ref{lemma:positive_ideal_value}, attending the most pressing constraints during the current rollout. Its proof uses Assumption  \ref{assumption_representation} to bound the error between $V$ and $V_P$, and Assumption \ref{assumption:lipschitz} together with Proposition \ref{prop_gossip_error} for the error between $V_P$ and $V_R$. 

\begin{lemma}\label{lemma:error_gradient}
Given the stochastic communication network model \eqref{eqn_graph_model}, 
Assumptions \ref{assumption_representation}--\ref{assumption:lipschitz} ensure that the following bound holds for the difference between the gradient of the ideal and realizable value functions %
\begin{align}  (\lambda_k)^\top \left(V-V_R\right)\leq \|\lambda_k\|\left(\epsilon+\beta+\eta L \sqrt{M} \frac{d_G}{\alpha T_0 p}\right),\label{eqn_lemma_Error_VI_VR}
\end{align}
%
where $V$ and $V_R$ are defined in \eqref{eqn_ideal_V} and \eqref{eq:non-idealities-VR}.
\end{lemma}
\begin{proof}

Write the norm of the difference between the ideal and realizable value functions as
$
    \left\|V_R-V\right\| \leq\left\|V-V_P\right\|+\left\|V_P-V_R\right\|,
$
%
using the triangle inequality and summing and subtracting the term $V_P$. Hence, by  Assumptions \ref{assumption_representation} and \ref{assumption:lipschitz} 
\begin{equation}
\left\|V_R-V\right\| \leq \epsilon+\beta+L \max_{n=1,\ldots,N} \left\|\lambda_k^n-\lambda_k\right\|_\infty,
\end{equation}
and lastly, using Proposition \ref{prop_gossip_error}, we obtain

$\left\|V_R-V\right\| \leq \epsilon+\beta+L \epsilon_G=\epsilon+\beta+\eta L \sqrt{M} d_G/(\alpha T_0 p).$
\end{proof}
%

Combining Lemmas \ref{lemma:positive_ideal_value} and  \ref{lemma:error_gradient}, if conditions \eqref{eq:delta_theorem} and \eqref{eq:alpha_bound} in Theorem \ref{theorem:feasibility} are satisfied, then \eqref{eqn_accute_angle} holds for the realizable policy and $V_R$. Indeed, from \eqref{eqn_lemma_vI} and \eqref{eqn_lemma_Error_VI_VR}  it yields 
\begin{align}
&\lambda^\top (V_R-c) = \lambda^\top(V-c) + \lambda^\top (V_R-V)\label{eqn_real_accute}\\
&\geq\frac{\|\lambda\|}{\sqrt{M}}\left((1-\|c\|_\infty)-M\left(\beta+\epsilon +\eta L  \frac{d_G}{\alpha T_0 p}\right)\right)\geq 0 ,  \nonumber 
\end{align}
where the last inequality requires $\delta>0$ in \eqref{eq:delta_theorem} and it is equivalent to $\alpha \geq {\eta d_G M L}/(p T_0 \delta )$ in \eqref{eq:alpha_bound}.

Having a positive inner product in \eqref{eqn_real_accute}, we can prove that the stochastic average of the multipliers across a trajectory is bounded.  
%
This key result is presented below as Proposition \ref{proposition:combined_lemma}.  Its proof,  relies on three steps. First, we combine \eqref{eqn_real_accute} with the update \eqref{eqn_contractive_big_brother} to obtain a contractive system with constant input for the expected value of $\|\lambda\|$ conditioned on past values. From this contraction, it follows that the expected value of the time average of the multipliers is bounded. Lastly, we include a modified proof of the strong law of large numbers for non-i.i.d. variables, which lets us drop the expected value and reach the following result.

\begin{proposition}
\label{proposition:combined_lemma}
Given the  stochastic  communication network model \eqref{eqn_graph_model}, under Assumptions \ref{assumption_representation}--\ref{assumption_noforces}, and with $\|c\|_\infty<1$, $ \|c\|_1\leq N-1$,
$\delta=\left(1-\|c\|_\infty\right) - \sqrt{M}\left(\epsilon+\beta\right)>0,$ and $\alpha\geq\frac{\eta d_G}{p T_0} \cdot \frac{\sqrt{M} L}{\delta}$, the averaged rewards along a trajectory are bounded by $\eta \sqrt{M/\alpha}$ with probability $1$, i.e.,
\begin{align}\label{eqn_bound_stochasstic_average}
\limsup_{k \rightarrow \infty} \frac{1}{k} \sum_{i=1}^{k} \left(\lambda_i\right)_m \leq \eta \sqrt{\frac{ M}{\alpha}} \text { a.s.. }
\end{align}
\end{proposition}
\begin{proof}
See Appendix \ref{app:combined_lemma}.
\end{proof}

\begin{figure*}[t!]
   \centering
   \begin{subfigure}{0.65\columnwidth}
   		\centering
            \input{./figs/navigation}
   		\caption{Floor plan.}	
            \label{fig:floorplan}
   \end{subfigure}
   \begin{subfigure}{0.65\columnwidth}
   		\centering
\input{./figs/heatmap_agent0_floorplan}
   		\caption{Agent $0$.}
   		\label{fig:heatmap_agent0}
   \end{subfigure}
   \begin{subfigure}{0.65\columnwidth}
   		\centering
\input{./figs/heatmap_agent1_floorplan}     
   		\caption{Agent $1$.}
   		\label{fig:heatmap_agent1}
   \end{subfigure}   \\
   \begin{subfigure}{0.65\columnwidth}
   		\centering
\input{./figs/heatmap_agent2_floorplan}       		
            \caption{Agent $2$.}
   		\label{fig:heatmap_agent2}
   \end{subfigure}   
     \begin{subfigure}{0.65\columnwidth}
   		\centering
\input{./figs/heatmap_agent3_floorplan}
   		\caption{Agent $3$.}
   		\label{fig:heatmap_agent3}
   \end{subfigure}
   \begin{subfigure}{0.65\columnwidth}
   		\centering
\input{./figs/heatmap_agent4_floorplan}     
   		\caption{Agent $4$.}
   		\label{fig:heatmap_agent1}
   \end{subfigure}   
\caption{(a) Floor plan and sample trajectories for each of the $N=5$ agents, $n=0,\ldots,4$. Black lines represent walls, and colored circles represent the $M=6$ zones to be patrolled by the agents. Each colored dot along a trajectory indicates a new position for each time step. The $12$ gray rectangular regions define distinct possible tiles or observations of an agent’s position, which are the inputs to the policy. Figures (b)--(f): Occupation heat maps. The complete trajectory of an agent across $40{,}000$ timesteps is represented by dots, indicating the position reached by an agent. Each dot color represents the frequency of occupation at that position, with darker hues indicating more frequent positions, under a logarithmic colorbar scale.}
\label{fig:heatmaps}
\end{figure*}

Hence, we proceed with the proof of  Theorem \ref{theorem:feasibility}. It yields from applying the results of Proposition \ref{proposition:combined_lemma} to the update rule \eqref{eqn_contractive_big_brother}.  Specifically, we introduce
\begin{equation}
    \bar r_k^{T_0}=\frac{1}{T_0}\sum_{\tau=(k-1)T_0}^{kT_0-1}r(S_t),
\end{equation}
which  reduces notation and lets us rewrite the update \eqref{eqn_contractive_big_brother} as
\begin{align}\label{eq:proyection_inequality}
    \lambda_{k+1}&=\left[(1-\alpha) \lambda_k+\eta\left(c-\bar r_k^{T_0}\right)\right]_{+}\\
    &\geq (1-\alpha)\lambda_k+\eta\left(c-\bar r_k^{T_0}\right), \notag
\end{align}
where the inequality that arises from the projection must be understood component-wise. Thus, from \eqref{eq:proyection_inequality}, we can rewrite
\begin{align}&
\eta\left(\bar r_k^{T_0}-c\right) \geq  (1-\alpha) \lambda_k -\lambda_{k+1}.
\end{align}

Hence, assuming $\lambda_0=0$, adding over $k=0,\ldots,K$
\begin{align}\label{eq:constraint_lambda}
&\sum_{k=0}^K \eta\left(\bar r_k^{T_0}-c\right) \geq \sum_{k=0}^K(1-\alpha) \lambda_k-\sum_{k=0}^K\lambda_{k+1} \\
&
= \sum_{k=1}^K(1-\alpha)  \lambda_k  -\sum_{k=1}^{K} \lambda_{k} -\lambda_{K+1} 
= -\sum_{k=1}^K\alpha  \lambda_k   -\lambda_{K+1}, \nonumber 
\end{align}
where we removed the term corresponding to $\lambda_0$, substituted $k=k+1$ in the sum of $\lambda_{k+1}$ and then rearranged two sums into one removing those terms $\lambda_k$ that cancel each other. Moving $c$ and $\eta$ to the right-hand side, we further obtain
\begin{align}
    & \frac{1}{K} \sum_{k=0}^K  \bar r_k^{T_0}\geq c -  \frac{1}{\eta K} \lambda_{K+1}  - \frac{\alpha}{\eta}\frac{1}{K} \sum_{k=1}^{K}   \lambda_k, \label{eq:constraint_with_lambdaK}
\end{align}
%
and using Proposition \ref{proposition:combined_lemma}, results in
%
\begin{align}& \liminf_{K\to\infty}\frac{1}{K} \sum_{k=1}^K  \bar r_k^{T_0}\geq c-\alpha\sqrt{\frac{ M}{\alpha}}.  \label{eq:constraint_satisfaction_mean}
\end{align}

We demonstrated throughout this section that selecting $\alpha>0$, which is required according to Proposition \ref{prop_gossip_error} to ensure that the multiplier error remains bounded, introduces an error in the feasibility result \eqref{eqn_as_feasibility}. Thus, we want to make $\alpha$ as small as possible, but this is limited by \eqref{eq:alpha_bound}, which is imposed to ensure the dual descends according to  \eqref{eqn_real_accute}. We can adjust $\eta$ and $T_0$ and $p$ to reduce the lower bound for $\alpha$ in \eqref{eq:alpha_bound}. In particular, the activation probability $p$ in the stochastic communication graph depends on the communication scheme that is part of the problem design. In the next section, we provide an example of how to increase this probability by boosting the communication power.  
