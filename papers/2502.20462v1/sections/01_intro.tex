Reinforcement Learning (RL) has emerged as an essential paradigm in artificial intelligence, driven by its ability to learn optimal behaviors through interactions with the environment. In many real-world scenarios, the interactions involve multiple agents with their own objectives, observations, and decision-making processes, giving rise to Multi-Agent Reinforcement Learning (MARL) \cite{canese2021multi,zhang2021multi,yang2021believe}. Centralized solutions\textemdash where the set of actions is decided using the full set of observations\textemdash struggle to scale with the number of agents, fail to account for individual agent autonomy, and often cannot handle decentralized information structures where agents have limited or private observations \cite{zhang2021multi,busoniu2008comprehensive}. MARL leverages distributed approaches \cite{lin2021multi}\textemdash where each agent has agency over its own decisions based on its own observations\textemdash to achieve impressive results in multi-player games, cooperative tasks such as Hanabi \cite{bard2020hanabi}, emergent behaviors in hide-and-seek environments \cite{baker2019emergent} as well as real-world applications, such as traffic light control \cite{wei2019colight}, highlighting MARL's growing potential.
 
Control applications further highlight MARL's adaptability, with advances in collision avoidance for autonomous vehicles \cite{kahn2017uncertainty,isele2018safe}, agile locomotion in legged robots \cite{yang2022safe,hwangbo2019learning}, and manipulation in industrial settings \cite{ibarz2021train,kober2013reinforcement}. Many of these applications require constrained formulations, where safety \cite{chen-ames2024probabilistic,chen-subramian2024probabilistic, garcia2015comprehensive}, resource limitations, or task priorities are encoded as mathematical inequalities, giving rise to the critical subfield of Constrained Reinforcement Learning (CRL) \cite{liang2018accelerated,castellano2023learning}.

In this work, we focus on multi-agent assignment as a class of MARL problems, in which constraints are essential to model conflicting requirements. Unconstrained multi-agent assignment methods have been proposed, combining reinforcement learning with game theory, having the advantage of minimal \cite{cui2020multi} or no coordination \cite{qin2021multi}. However,  these works only account for local rewards and do not provide theoretical guarantees that assignment specifications are met.  In contrast, we propose a constrained optimization approach to coordinate agents, guaranteeing the satisfaction of global requirements that involve the joint state of all agents.

Therefore, we address the assignment problem under the framework of Constrained Multi-Agent Reinforcement Learning (CMARL) \cite{gu2021multi,chen2020autonomous,liu2022distributed}. The literature on CMARL addresses challenges such as safe coordination in autonomous fleets \cite{shalev2016safe}, fair resource allocation in multi-agent systems \cite{zimmer2021learning}, and robust cooperation under communication constraints \cite{xiao2023graph}. These constraints may be individually defined per agent  \cite{gu2021multi}, or collective, in which all agents are involved in the feasibility of the same constraint \cite{chen2020autonomous,liu2022distributed}. Assignment problems entail multiple such collective constraints but do not admit the aggregate reward structure in  \cite{chen2020autonomous,liu2022distributed}.   

Whether CRL methods are used in scenarios with  
 single or multiple agents, they often employ regularization to reconcile competing objectives, transforming constraints into penalty terms within an aggregate reward function \cite{liang2018accelerated,liu2022deep}. For instance, a mobile robot navigating crowded environments might balance collision avoidance (a hard constraint) with energy efficiency (a soft objective) by weighting their respective penalties (e.g., \cite{kahn2017uncertainty}). The selection of these weights is non-trivial: overly conservative penalties may lead to overly cautious behavior, while insufficient regularization risks constraint violations \cite{holder2024multi}. Bayesian methods \cite{hutter2002self} adapt weights probabilistically, incorporating prior knowledge about constraint criticality. Heuristic approaches, though less rigorous, remain prevalent in practice, such as reward shaping \cite{tessler2018reward} in which penalties are manually engineered based on domain expertise. 

Lagrangian duality \cite{paternain2019constrained, liang2018accelerated} provides a principled approach by treating constraints as part of the optimization dual space, dynamically adjusting weights through gradient descent on the dual variables. Even though the duality gap is guaranteed to be zero \cite{paternain2019constrained}, feasibility is not guaranteed under this approach for problems where the primal maximizer is not unique~\cite{calvo2023state}. In this scenario, both regularization-based and duality methods that rely on the convergence of the multipliers struggle. In particular, the Example \ref{example} in Section \ref{sec:problem_form} and the discussion in Section \ref{sec:offline_training} illustrates how regularization fails in assignment problems in which the collective constraints are nonlinear combinations of individual ones.  

State augmentation has emerged as a framework to address these limitations by reinterpreting the constrained RL problem through an extended Markov Decision Process (MDP). By treating Lagrangian multipliers as part of the state space, policies gain the ability to condition their actions on both environmental observations and current constraint satisfaction levels \cite{calvo2023state}. This approach allows for a family of policies that can alternate in order to fulfill tasks while the Lagrangian weights cycle to multiplex between policies. That is, as one policy focuses on attending to one of the several conflicting requirements, Lagrangian weights will evolve as part of the augmented MDP to eventually force a policy shift towards one that tackles another requirement that is not being satisfied. Theoretical analyses show that such approaches preserve convergence guarantees under mild assumptions, effectively converting constrained optimization into an unconstrained problem over the augmented state-action space \cite{paternain2022safe}. 

In our preliminary work \cite{agorio2024multi}, we leverage these extended MDPs to provide multiple agents with the capability of solving assignment problems collaboratively via CMARL. With standard dual methods proving inadequate for such problems, the cycling trajectories of the multipliers in the augmented state drive agents to alternate between tasks. These multipliers are shared among agents through a communication network, enabling coordination without direct access to other agents' states, yielding a distributed protocol that addresses the challenges of multi-agent assignment problems with theoretical feasibility and convergence guarantees. Building on this foundation, we introduce a key novelty in this work: the integration of a gossiping framework \cite{aysal2009broadcast} to share multipliers across a stochastic communication network \cite{casteigts2012time,Nedic2017Achieving} established by the agents. This network is inherently dynamic and may even be disconnected across time, reflecting communication ad-hoc conditions frequently found in practice. To address these challenges while maintaining feasibility guarantees, we propose a novel algorithm featuring a contractive update for the augmented multipliers, which lets us use finite communication buffers and keep the estimation error bounded. This innovation enhances scalability and ensures almost sure feasibility in stochastic environments. Leveraging this framework, we prove almost sure feasibility up to an error margin that can be made arbitrarily small, employing a novel proof technique that explicitly accounts for the stochasticity of the communication graph. We validate our theoretical contributions through extensive numerical experiments, demonstrating the practical applicability of our approach. In these experiments, a team of five robots successfully learns to patrol six regions of interest, showcasing the effectiveness of our algorithm in achieving coordination under realistic communication constraints.

The rest of this paper is organized as follows: In Section \ref{sec:problem_form}, we introduce
multi-agent assignment, formulating it as a feasibility problem and presenting the stochastic graph model that governs agent communication. This model captures the dynamic and potentially disconnected nature of typical networks found in practice, setting the stage for our solution. Section \ref{sec:offline_training}  details the state augmentation approach, which enables agents to coordinate using dual variables shared through the network. We outline the offline training and online execution algorithms, providing the foundation for our distributed multi-agent system. In Section \ref{sec:feasibility}, we state and formalize the main theoretical result: almost sure feasibility of the proposed algorithm under mild assumptions.  Section \ref{sec_numerical} validates our approach through numerical simulations, where a team of five robots successfully patrols six regions of interest, demonstrating consistent feasibility across all runs. 
Finally, Section \ref{sec:conclusion} presents the conclusions, followed by supporting lemmas in the Appendix.
