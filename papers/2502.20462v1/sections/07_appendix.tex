\label{sec:appendix}
\subsection{Proof of Proposition \ref{prop_gradients}}
\label{app:proof_prop_gradients}
\begin{proof}(Proposition \ref{prop_gradients})
 The separable structure of our training policy $\pi_\theta(A_t\mid S_t,\lambda,\ldots,\lambda) = \prod_{n=1}^N \pi_{\theta_n}(A_{tn}\mid S_{tn},\lambda)$ yields  $\nabla_{\theta_n}\log \pi_\theta(S_t,A_t) = \nabla_{\theta_n}\log \pi_{\theta_n}\left(A_{tn}\mid S_{tn}\right).$ 
 
 Substituting the right-hand side in the policy gradient \cite{sutton2000policy}, we obtain
 %
 $\nabla_{\theta_n} \mathcal{L(\pi_\theta,\lambda)} = \mathbb E\left[L_nQ_{\pi_\theta}(S_t,A_t,\lambda)\right],$
with   $L_n=\nabla_{\theta_n }\log \pi_{\theta_n}(A_{tn}\mid S_{tn})$. Thus, using the law of total expectation, conditioning
on $S_{tn}$,$A_{tn}$, we obtain
%
\begin{align}
     &\nabla_{\theta_n} \mathcal{L(\pi_\theta,\lambda)}\hspace{-2pt} = \hspace{-2pt}\mathbb{\mathbb E}_{(S_{t}^n,A_{t}^n)}\hspace{-2pt}\left[\mathbb E_{(S_t,A_t)}\left[L_n Q_{\pi_\theta}(S_t,A_t,\lambda)\hspace{-2pt}\mid\hspace{-2pt} S_{t}^n,A_{t}^n\right]\right]\nonumber\\
& = \mathbb E_{S_{t}^n,A_{t}^n}\left[L_n\mathbb E_{(S_t,A_t)}\left[Q_{\pi_\theta}(S_t,A_t,\lambda)\mid S_{t}^n,A_{t}^n\right]\right],\label{eqn_total_expectation}
\end{align}
%
to then substitute $Q_{\pi_\theta}^n(\cdot)$ as defined in  \eqref{eqn_q_others} into \eqref{eqn_total_expectation}.
\end{proof}
%

\subsection{Proof of Proposition \ref{prop_binomial}}
\label{app:proof_prop_binomial}
\begin{proof}(Proposition \ref{prop_binomial})
Consider two agents $J<d_G$ nodes apart. For a particular journey of times waiting to cross each intermediate edge $\left(t_1, t_2, \hdots, t_J\right)$, the probability is
$$p\left(t_1, t_2, \hdots, t_J\right)=  p \cdot(1-p)^{t_1-1}  \cdot p \cdot(1-p)^{t_2-1} p \hdots \cdot(1-p)^{t_{J-1}},$$
%
where $p$ is the stochastic graph's communication probability. To compute the probability of $r(S_{\tau})$ reaching agent $n$ at time $t=\tau+i$, exactly $i$ steps after $\tau$, observe that there are as many combinations of $\left(t_1, t_2, \hdots, t_J\right)$ as $\left(\begin{array}{c}i-1 \\ J-1\end{array}\right)$, i.e. ways of having $J-1$ successes over $i-1$ time steps. This count corresponds to a negative binomial variable  $B N_{p, J}(i)$.

Consequently, the reward associated with the $m$-th constraint at time $\tau$, that is $\left(r(S_{\tau})\right)_m$, will reach agent $n$ on time at time $(k+1)T_0-1$ or before  with probability
%
\begin{align}\mathbb P\left(\left(e_\tau\right)_m=0\right)&=\mathbb P\left(\left(r(S_{\tau})=R_{\tau,(k+1)T_0-1}^n\right)_m\right) \label{eqn_gossip_error_def}\\
&\leq \mathbb P(BN(J,p)\leq (k+1)T_0-1-\tau)\\
&\leq \mathbb P(BN(d_G,p)\leq (k+1)T_0-1-\tau),
\end{align}
%
%
 where the first inequality appears because the error $e_\tau=r(S_{\tau})-R_{\tau,(k+1)T_0-1}^n$ is null if $r(S_{\tau})=0$ even if the message fails to reach agent $n$, and the second one is because the probability of a negative binomial becomes lower when the number of required successes is increased. Correspondingly, 
 %
 \hspace{-8pt} \begin{align}\hspace{-8pt}\mathbb P\hspace{-1pt}\left(\hspace{-1pt}\left(e_\tau\right)_m=1\right)\leq \mathbb P\biggl(\hspace{-2pt}BN(d_G,p)> (k+1)T_0-1-\tau\biggl),\label{eqn_prob_gossip_err}\end{align} which concludes the proof.
\end{proof}
\subsection{Proof of Proposition \ref{prop_gossip_error}}\label{app:proof_prop_gossip_error}
\begin{proof}(Proposition \ref{prop_gossip_error})
We can use the previous result to bound the expected error during a rollout. Since  the entries of $e_\tau$ in \eqref{eqn_gossip_error_def} can only take  values zero or one, we can write 
%
\begin{align}
&\mathbb E\left[\sum_{t=k T_0}^{(k+1)T_0-1} \left(r(S_{\tau})-R_{\tau,(k+1)T_0-1}^n\right)_m\right]\\
&= \sum_{\tau=k T_0}^{(k+1)T_0-1} \mathbb P\left(\left(e_\tau\right)_m=1\right)\\
&\leq  \sum_{\tau =k T_0}^{(k+1)T_0-1} \mathbb P\biggl(BN(d_G,p)> (k+1)T_0-1-\tau\biggl)\\
&=  \sum_{l=0}^{T_0} \mathbb P\biggl(BN(d_G,p)> l\biggl)\leq   \sum_{l=0}^{\infty} \mathbb P\biggl(BN(d_G,p)> l\biggl)\nonumber\\
&=\mathbb E\left[BN(d_G,p)\right]=\frac{d_G}{ p},\label{eqn_expectation_BN}
\end{align}
where we used \eqref{eqn_prob_gossip_err},  substituted $l=(k+1)T_0-1-\tau$, and added the tail to the sum so that it equals the expectation in \eqref{eqn_expectation_BN}, which is $d_G/p$ for the negative binomial.
%\leopoldo{This is not exactly the proposition. I would say that the result we use in the following proposition should be stated, specifically the fact that $\mathbb E\left[\sum_{t=k T_0}^{(k+1) T_0-1}\left(r\left(S_\tau\right)=R_{\tau,(k+1) T_0-1}^n\right)_m\right]=d_G/p$}
%
To associate this expected value with the error between multipliers, we consider a particular agent $n$, take the difference between the global and local updates in \eqref{eqn_contractive_big_brother} and \eqref{eqn_stochastic_dual}, and bound the expectation as  
%\begin{align}
 %   \lambda_{k+1}=&\left[(1-\alpha)\lambda_k-\frac{\eta}{T_0} \sum_{\tau=kT_0}^{(k+1) T_{0}-1}\left(r(S_{\tau})-c\right)\right]_+\\
  %  \lambda_{k+1}^{n}=&\left[(1-\alpha)\lambda_k^{n}-\frac{\eta}{T_0} \sum_{\tau=kT_0}^{(k+1) T_{0}-1}\left(R_{\tau,(k+1)T_0-1}^n-c\right)\right]_+
%\end{align}
%
\begin{align}
    &\mathbb E\left[\|\lambda_{k+1}^{n}-\lambda_{k+1}\|_\infty \right]\label{eq:rolled_lambda_error}\\
    &\leq \frac{\eta}{T_0}  \mathbb E\left[ \left\|\sum_{\tau=k T_0}^{(k+1) T_0-1}\left(r(S_{\tau})-R_{\tau,(k+1)T_0-1}^n\right)\right\|_\infty\right]\\
    &+(1-\alpha) \mathbb E\left[\|\lambda_{k}^{n}-\lambda_{k}\|\right]\leq \frac{ \eta d_G}{T_0 p}+(1-\alpha)\mathbb E\left[\|\lambda_{k}^{n}-\lambda_{k}\|_\infty\right].\nonumber
\end{align}
%
Unrolling \eqref{eq:rolled_lambda_error} with $\lambda_0^n=\lambda_0$, we obtain
%
\begin{align}\nonumber
\mathbb E\left[\|\lambda_{k+1}^{n}-\lambda_{k+1}\|\right]&\leq \sum_{i=0}^k (1-\alpha)^i \frac{ \eta d_G}{T_0 p}
\leq \frac{ \eta d_G}{T_0 \alpha p},
\end{align}
which concludes the proof.
\end{proof}

\subsection{Proof of Proposition \ref{proposition:combined_lemma}}\label{app:combined_lemma}
The proof of Proposition  \ref{proposition:combined_lemma} relies on the following four Lemmas, which show that the multipliers follow a contractive evolution in a martingale sense  (Lemma \ref{lemma:almost_a_supermartingale}), which allows us to prove both that their expected running averages are bounded by $\mathcal O(1/\sqrt{\alpha})$ (Lemma \ref{lemma_expected_mult_running_average}), and that they are deterministically bounded by a larger constant (Lemma \ref{lemma:positive_ideal_value}). This deterministic bound is a technicality to show that we can drop the expectation from Lemma \ref{lemma_expected_mult_running_average}) and bound the stochastic running averages by $\mathcal O(1/\sqrt{\alpha})$ (Lemma \ref{lemma_expected_to_stochastic}) which is the claim of Proposition \ref{proposition:combined_lemma}. 
%
\begin{lemma}\label{lemma:almost_a_supermartingale}
  {Given  the stochastic communication network model \eqref{eqn_graph_model}, under Assumptions \ref{assumption_representation}--\ref{assumption_noforces}, and  with $\|c\|_\infty<1$, $\|c\|_1\leq N-1$, $\delta=\left(1-\|c\|_\infty\right) - M\left(\beta+\varepsilon_{T_0}\right)>0$ and $\alpha\geq\frac{\eta d_G}{p T_0}\frac{M L}{\delta}$, the multipliers satisfy the following inequality}
   \begin{align}
  {\mathbb E\left[\left\|\lambda_k\right\|^2 \mid \mathcal{F}_{k-1}\right] \leq
  (1-\alpha)^2\left\|\lambda_{k-1}\right\|^2+\eta^2 M}.
  \label{eq:almost_a_supermartingale}
  \end{align}
  \end{lemma}
\begin{proof}
  
As in the proof of Theorem \ref{theorem:feasibility} we use $\bar{r}_k^{T_0}=\frac{1}{T_0} \sum_{\tau=(k-1) T_0}^{k T_0-1} r\left(S_t\right)$ to denote the average reward during the $k-th$ rollout, and rewrite  the  multiplier recursion \eqref{eqn_contractive_big_brother} as
\begin{align}\label{eqn_contractive_bb_rewritten}
& \lambda_{k+1}=\left[(1-\alpha) \lambda_k+\eta\left(c-\bar{r}_k^{T_0}\right)\right]_{+}.
\end{align}

Then, we use that projections reduce distances to bound
\begin{align}
&\mathbb E\left[ \|\lambda_{k+1}\|^2\hspace{-1pt}\mid\hspace{-1pt}\mathcal F_k\right]\hspace{-2pt}=\hspace{-2pt}\mathbb E\left[\hspace{-1pt}\left\|\hspace{-1pt}\left[(1-\alpha) \lambda_k+\eta\left(c-\bar{r}_k^{T_0}\right)\right]_{+}\hspace{-1pt}\right\|^2\hspace{-2pt}\mid\hspace{-2pt}\mathcal F_k\hspace{-1pt}\right]\nonumber\\
&\leq  \mathbb E\left[ \left\|(1-\alpha) \lambda_k+\eta\left(c-\bar{r}_k^{T_0}\right)\right\|^2\mid \mathcal F_k\right]\\
&\leq  (1-\alpha)^2\left\| \lambda_k\right\|^2 +2(1-\alpha)\eta \lambda_k^T \mathbb E\left[c-\bar{r}_k^{T_0}\mid \mathcal F_k\right]
\\& \hspace{9pt}+\eta^2 \mathbb E\left[\left\|c-\bar{r}_k^{T_0}\right\|^2\mid \mathcal F_k\right]\\
&\leq  (1-\alpha)^2\left\| \lambda_k\right\|^2 +2(1-\alpha)\eta \lambda_k^T \mathbb E\left[c-\bar{r}_k^{T_0}\mid \mathcal F_k\right]+\eta^2 M,\nonumber
\end{align}
%
where we bounded $(c-\bar{r}_k^{T_0})_m^2<1$.

The expectation above must be taken with respect to the realizable policy $\pi_\theta[\lambda_k^1\ldots,\lambda_k^N]$ in \eqref{eqn_realizable_policy}, since we want to obtain results for stochastic trajectories following that policy. Thus  $\mathbb E\left[\bar{r}_k^{T_0}\mid \mathcal F_k\right]=V_R$ as defined in \eqref{eq:non-idealities-VR}, so that 
%
\begin{align}
\mathbb E\left[ \|\lambda_{k+1}\|^2\mid \mathcal F_k\right]& \leq   (1-\alpha)^2\left\| \lambda_k\right\|^2 \\&+2(1-\alpha)\eta \lambda_k^T \left(c-V_R\right)+\eta^2 M,\label{eq:almost_supermartinagale_withVR}
\end{align}
% 
and we showed in  \eqref{eqn_real_accute} that, under the hypotheses of Lemmas \ref{lemma:positive_ideal_value} and \ref{lemma:error_gradient},  $\lambda_k^T \left(c-V_R\right)\leq 0$, which let us conclude the proof by removing the second term in the right-hand side of  \eqref{eq:almost_supermartinagale_withVR}. 
\end{proof}

\begin{lemma}\label{lemma_expected_mult_running_average}
  Under the hypothesis of Lemma \ref{lemma:almost_a_supermartingale} \begin{align}\label{eq:slln_bound}
  \limsup _{k \rightarrow \infty} \frac{1}{k} \sum_{i=1}^{k} \mathbb E\left[\left(\lambda_i\right)_m \right] \leq  \eta \sqrt{\frac{M}{\alpha}}.
  \end{align}
  \end{lemma}

\begin{proof} 
Using \eqref{eq:almost_a_supermartingale}   we obtain
\begin{align}
\mathbb E\left[\|\lambda_k \|^2\right]&\leq (1-\alpha)^2 \mathbb E\left[\left\|\lambda_{k-1}\right\|^2\right]+\eta^2M.
\end{align}
%
Thus, by induction starting from $\lambda_0=0$ for convenience 
%
\begin{align}
 \mathbb E\left[\|\lambda_k \|^2\right]&\leq \sum_{j=0}^{k-1}(1-\alpha)^{2j}\eta^2M\leq \eta^2 M \sum_{j=0}^{\infty}(1-\alpha)^{2j}\\
 &= \frac{\eta^2M}{(1-(1-\alpha)^2)}
 \leq  \frac{\eta^2M}{\alpha}.   \label{eq:bound_the_moment}
\end{align}

Applying Jensen's inequality and then  \eqref{eq:bound_the_moment}, we can bound the moment of $\lambda_{k+1}$ by
%
$
\left(\mathbb E\left[\left\|\lambda_{k+1}\right\|\right]\right)^2 \leq
\mathbb E\left[\left\| \lambda_{k+1} \right\|^2\right]\leq \frac{\eta^2M}{\alpha}$
which yields the following bound for the expected value of each entry of  $\lambda_k$,   $\mathbb E\left[(\lambda_k)_m\right] \leq \mathbb E\left[\left\|\lambda_k\right\|\right] \leq \eta\sqrt{\frac{M}{\alpha}}$.
\end{proof}

In the next lemma, we prove that the multipliers are deterministically bounded by a constant $U$ of order $\mathcal O(1/\alpha)$, deriving this result directly from the contractive form of the stochastic update \eqref{eqn_contractive_big_brother}. Thus, it is immediate that the stochastic time average \eqref{eqn_bound_stochasstic_average} in Proposition \ref{proposition:combined_lemma} is bounded by $U$ too. Although more complex, the argument in the proof of Proposition \ref{proposition:combined_lemma} is meaningful because it yields a tighter bound of order $\mathcal O(1/\sqrt{\alpha})$ as in \eqref{eqn_bound_stochasstic_average}. Hence, the feasibility error \eqref{eqn_as_feasibility} in Theorem \ref{theorem:feasibility}, which results from multiplying the bound \eqref{eqn_bound_stochasstic_average} by $\alpha$ in \eqref{eq:constraint_satisfaction_mean}, can be controlled by the design parameter $\alpha$.

\begin{lemma}\label{lemma_deterministic_bound}
 Under the hypothesis of Lemma \ref{lemma:almost_a_supermartingale}, the multipliers $\lambda_k$ in \eqref{eqn_contractive_big_brother} are bounded almost surely by $U=\eta \sqrt{M}/\alpha$.
\end{lemma}
\begin{proof}  From the update \ref{eqn_contractive_big_brother} written as in \ref{eqn_contractive_bb_rewritten} , yields
%
\begin{align}
&\left\|\lambda_{k+1}\right\|  \leq\left\|\lambda_k(1-\alpha)+\eta\left(c-\bar{r}_k^{T_0}\right)\right\| \\
& \leq(1-\alpha)\left\|\lambda_k\right\|+\eta\left\|c-\bar{r}_k^{T_0}\right\|  \leq(1-\alpha)\left\|\lambda_k\right\|+\eta \sqrt{M} \nonumber\\
& \leq(1-\alpha)^k\left\|\lambda_0\right\|+\sum_{j=0}^k \eta \sqrt{M}(1-\alpha)^j \\
& \leq \eta \sqrt{M} \sum_{j=0}^{\infty}(1-\alpha)^j  =\frac{\eta \sqrt{M}}{\alpha},
\end{align}
where we assumed $\lambda_0=0$ for convenience.
\end{proof}

\begin{lemma}\label{lemma_expected_to_stochastic}
  Assume that there exists a constant $C >0$ such that $\limsup _{k \rightarrow \infty} \frac{1}{k} \sum_{i=1}^{k} \mathbb E\left[\left(\lambda_i\right)_m \right] \leq C$.
  Then the following bound also holds with probability one  for the average  stochastic multipliers, i.e., $\limsup_{k \rightarrow \infty} \frac{1}{k} \sum_{i=1}^{k} \left(\lambda_i\right)_m \leq C.$
\end{lemma}


\begin{proof} 
Define   $\ell_{k+1} :=\lambda_{k+1}-\mathbb E\left[\lambda_{k+1} \mid \mathcal F_k\right]$. We start by proving that these  $\ell_k$ satisfy the SLLN. By definition, these variables are zero-mean conditioned on $\mathcal F_k$, and they are deterministically bounded by $2U$ according to Lemma \ref{lemma_deterministic_bound}, but they are not i.i.d. because $\lambda_k$ accumulates rewards.  Then, to prove that
 $ \mu_{k}:=\frac{1}{k+1} \sum_{i=0}^{k} \ell_i \xrightarrow{a. s.} 0,
  $
 consider the partial sums   
     $U_{k+1}=\sum_{i=1}^{k+1} \ell_i=\ell_{k+1}+U_k $. Since  $\ell_{k+1}$ are conditionally zero-mean,  $U_k$ is a martingale, i.e.,
%
    \begin{align}
      \mathbb E\left[U_{k+1}\mid \mathcal F_k \right]&=\mathbb E\left[\ell_{k+1}\mid \mathcal F_k \right]+U_k=U_k.
     \end{align}

     To use the proof of the SLLN in \cite[p.456]{Gallager}, we need a bound for the second moment of $U_k$  that increases with order $k$ at most. Without the standard i.i.d. assumption, we write
%%
     \begin{align}
    \mathbb E\left[\left\|U_{k+1}\right\|^2\mid \mathcal F_k\right] & =\left\|U_k\right\|^2+2 U_k^{\top} \mathbb E\left[\ell_{k+1} \mid \mathcal F_k\right]\\
    &+\mathbb E\left[\|\ell_{k+1}\|^2\mid \mathcal F_k\right]  \leq\left\|U_k\right\|^2+U^2.
    \end{align}

    Conditioning to $\mathcal F_{k-1}$ instead, we bound
    \begin{align}
     &\mathbb  E\left[|| U_{k+1} \|^2\mid \mathcal F_{k-1}\right]=\mathbb  E\left[\mathbb E\left[|| U_{k+1} \|^2\mid \mathcal F_{k}\right]\mid\mathcal F_{k-1}\right]\\
    &\leq \mathbb  E\left[|| U_k \|^2+U^2 \mid \mathcal F_{k-1}\right]  \leq \mathbb  E\left[\left\|U_k\right\|^2 \mid {\mathcal F_{k-1}}\right]+U^2 \\
    & \leq\left\|U_{k-1}\right\|^2+U^2+U^2.
    \end{align}

    If we repeat this process recursively, reducing the time index of the filtration, we arrive at $\mathbb E\left[|| U_{k+1} \|^2\mid \mathcal F_{0}\right]=\mathbb E\left[|| U_{k+1} \|^2\right]$, obtaining  the following bound for the unconditioned moment of $U_{k+1}$   
    %
    \begin{align} \mathbb E\left\|U_{k+1}\right\|^2 &\leq \mathbb E\left\|U_1\right\|^2+k U^2 = \mathbb  E\left\|\lambda_0\right\|^2+(k+1) U^2  \\&\leq U^2+ k U^2  \leq(k+1) U^2.
    \end{align}


    Henceforth, we can substitute $X_i$ and  $S_n$  for $\ell_i$ and $U_k$, respectively, and follow the proof in \cite[p.456]{Gallager}  to conclude
%
    $$
    \mu_{k}:=\frac{1}{k+1} \sum_{i=0}^{k} \ell_i=\frac{1}{k+1} \sum_{i=0}^{k}\left(\lambda_i-\mathbb E \left[\lambda_i\mid \mathcal F_{i-1}\right)\right] \xrightarrow{a. s.} 0.
    $$

    Next, we want to get rid of the filtrations and compare the stochastic running average to the average of unconditional expectations. Specifically, we want  
  %
    $\frac{1}{k} \sum_{i=1}^{k}\left(\lambda_i-\mathbb E \left[\lambda_i\right] \right)\xrightarrow{a. s.} 0.
  $
  To that end, consider $\bar{\lambda}_{k+1}^{(k)}=\mathbb E\left[\lambda_{k+1}\mid  F_k\right]=\mathbb E\left[\lambda_{k+1}\mid \lambda_k\right]$,
  which is a random variable that is a function of $\lambda_k$ for $k>1$,
  and with the convention $\mathbb  E\left[\lambda_k \mid \mathcal F_{k-1}\right]=\mathbb  E\left[\lambda_i\right]$ if $k \leq 1$.
%
Thus, we can use a recursive argument by redefining the conditionally zero-mean $$\ell_{k+1}\hspace{-1pt}=\hspace{-1pt}\bar\lambda_{k+1}^{(k)}-\mathbb E\hspace{-2pt}\left[\hspace{-1pt}\bar\lambda_{k+1}^{(k)}\hspace{-3pt}\mid\hspace{-3pt}\mathcal F_{k-1}\right]\hspace{-3pt}=\hspace{-1pt}\mathbb E\hspace{-1pt}\left[\lambda_{k+1}\hspace{-2pt}\mid\hspace{-2pt}\mathcal F_k\right]-\mathbb E\hspace{-1pt}\left[\lambda_{k+1}\hspace{-3pt}\mid\hspace{-3pt}\mathcal F_{k-1}\right],$$ which are also bounded by $U$, hence we can apply the SLLN again to the corresponding time averages
%
  \begin{align} \nonumber\mu_{k}\hspace{-1pt}&=\hspace{-1pt}\frac{1}{k}\hspace{-1pt}\sum_{i=1}^{k}\hspace{-1pt}\ell_i\hspace{-1pt}=\hspace{-1pt}\frac{1}{k}\hspace{-1pt}\sum_{i=1}^{k}\hspace{-1pt}\left(\mathbb E\left[\lambda_{i}\hspace{-1pt}\mid\hspace{-1pt}\mathcal F_{i-1}\hspace{-1pt}\right]\hspace{-1pt}-\hspace{-1pt}\mathbb E\hspace{-1pt}\left[\lambda_{i}\hspace{-1pt}\mid\hspace{-1pt}\mathcal F_{i-2}\right] \right)\hspace{-1pt}\xrightarrow[k\hspace{-1pt}\rightarrow\hspace{-1pt}\infty]{a.s.}\hspace{-1pt}0.
  \end{align}
  %
  
  Then we can move backwards on time by repeating this argument for previous $i-n+1$ and $i-n$, i.e.,
  %
  \begin{align}
  \mu_{k}=\frac{1}{k} \sum_{i=1}^{k}\left(\mathbb E\left[\lambda_{i} \mid \mathcal F_{i-n+1}\right]-\mathbb E\left[\lambda_{i} \mid \mathcal F_{i-n}\right] \right)\xrightarrow[k \rightarrow \infty]{a.s.} 0,\nonumber%\label{eq:slln}
  \end{align}
%
   and  the conditioning can be removed when we reach $n\geq k$, resulting in
  %
  $
   \mathbb E\left[\lambda_i\mid \mathcal F_{i-k}\right]=\mathbb E\left[\lambda_i\right]$, for all $i\leq k$, which implies that the stochastic and expected running averages coincide. Hence, our result follows.
  \end{proof}

Finally, put the lemmas together to prove the proposition.

\begin{proof}(Proposition \ref{proposition:combined_lemma})
    
    Substitute $C=\eta\sqrt{\frac{M}{\alpha}}$ from Lemma \ref{lemma_expected_mult_running_average} in Lemma \ref{lemma_expected_to_stochastic}. 
\end{proof}
