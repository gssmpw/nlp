\section{Related Work}
Recent advancements in LLMs for multi-task and multi-attribute applications have explored several techniques, including prompting and reinforcement learning from human feedback (RLHF). 

\paragraph{Multi-task Prompting.} Prompting-based techniques design specialized inputs to guide LLMs toward desired attributes across a range of tasks~\citep{ref:li2021prefix, ref:qin2021learning, ref:prasad2023grips}. For example, prompt tuning methods aim to learn a shared prompt that adapts to multiple tasks~\citep{ref:liu2023hierarchical, ref:tian2024argue, ref:kim2024aapl}, while \citet{ref:xu2025mixture} employs instruction concatenation combined with diverse system prompts to improve alignment efficiency in applications such as dialogue and coding, and mathematical reasoning.

\paragraph{Multi-task RLHF.} RLHF aims to train LLMs to align their outputs with human preferences~\citep{ref:ouyang2022training, ref:rafailov2024direct}. Although RLHF has shown promise in adjusting model behavior, previous work by~\citet{ref:dong2023abilities, ref:kotha2024understanding, ref:biderman2024lora} show that multi-task fine-tuning can lead to conflicts between different objectives. To mitigate such issues, several lines of work have proposed methods to balance these competing goals. For instance, \citet{ref:liu2024mftcoder} address data bias among multiple coding tasks, and other efforts focus on preference fine-tuning with multiple objectives for improving LLM helpfulness~\citep{ref:wu2023finegrained, ref:zhang2024bi, ref:yang2024metaaligner, ref:wang2024arithmetic, ref:yang2024rewards, ref:wang2024interpretable}.

In contrast to these approaches, which primarily tune either the input prompt or the model parameters, our work focuses on inference-time intervention in the representation space of LLMs under a multi-attribute setting. 
Moreover, our experiments show that applying our intervention on top of prompting or fine-tuning methods further enhances model performance (see~\cref{sec:app-results}).