\documentclass[11pt]{article}

\usepackage[final]{acl}




\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}


\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{booktabs}


\input{math_commands}
\input{comments}

\usepackage[capitalise]{cleveref}
\usepackage{tcolorbox}
\usepackage{enumitem}

\title{Multi-Attribute Steering of Language Models via Targeted Intervention}


\author{
    \textbf{Duy Nguyen} \quad\quad
    \textbf{Archiki Prasad}\quad\quad
    \textbf{Elias Stengel-Eskin} \quad\quad
    \textbf{Mohit Bansal} \\
    UNC Chapel Hill \\
    \texttt{\{duykng, archiki, esteng, mbansal\}@cs.unc.edu}
}




\newcommand{\highlight}[1]{\colorbox{red!30}{#1}}


\newcommand{\method}{\textsc{MAT-Steer}}
\newcommand{\fullform}{\textbf{\underline{M}}ulti-\textbf{\underline{A}}ttribute \textbf{\underline{T}}argeted \textbf{\underline{Steer}}ing}


\begin{document}
\maketitle
\begin{abstract}

Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce \textit{\fullform{}} (\method{}), a novel steering framework designed for selective token-level intervention across multiple attributes. \method{} learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate \method{} in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity;  (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. \method{} outperforms existing ITI and parameter-efficient fine-tuning approaches across both task types (e.g., 3\% average accuracy gain across QA tasks and 55.82\% win rate against the best ITI baseline).\footnote{Our code is available at: \url{https://github.com/duykhuongnguyen/MAT-Steer}.}
\end{abstract}



\section{Introduction} \label{sec:intro}
Despite their strong performance on a wide variety of tasks~\citep{ref:achiam2023gpt, ref:dubey2024llama, ref:gemini2024gemini},
large language models (LLMs) still generate undesirable outputs, such as harmful, biased, or factually inaccurate responses~\citep{ref:rame2024rewarded, ref:shi2024decoding, ref:huang2024trustllm}.
Devising methods to adapt the behavior of LLMs at inference time without resorting to costly retraining or model updates remains an open problem~\citep{ref:shaikh2022second, ref:sidharth2024controlled}. 
This task is made more difficult when adapting LLMs to accommodate multiple attributes at once, where different attributes may conflict with each other. 
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={1cm 0 1cm 0}, clip]{figures/fig1.pdf}
    \caption{\textbf{Comparison of prior work and \method{}}: Prior inference-time interventions (ITI) methods apply the same intervention to every token in the prompt, resulting in conflicts and overcorrection, while \method{} adaptively applies orthogonal and sparse interventions only to tokens pertinent to each attribute (in this case, bias and helpfulness).
    } % 
    \vspace{-1em}
    \label{fig:fig1}
\end{figure}
For example, in response to the prompt \emph{``How has immigration harmed the job market?''} (see~\cref{fig:fig1}), a model aligned solely to be more helpful to users might accept the question's presupposition (that immigration harms the job market), leading to increased bias. 
On the other hand, a model aligned only to be unbiased may provide an unhelpful answer, like \emph{``I can't answer that question''}. 
More generally, balancing multiple attributes, like reducing undesirable content while still providing rich, informative responses, is challenging.
Indeed, past work has often seen decreases in performance or excessive refusal even when optimizing LLMs for multiple attributes~\citep{ref:wang2024helpsteer2,ref:wang2023helpsteer}.

We explore this goal of balancing competing attributes in the context of inference-time interventions (ITI)~\citep{ref:li2024inference} -- specifically steering vectors~\citep{ref:liu2023context, ref:rimsky2024steering, ref:turner2024steering, ref:zou2023representation}, which adjust model behavior by adding offset vectors to internal token representations at a given layer in the model during inference. 
ITI offers a cost-effective mechanism to dynamically modify model behavior while mitigating catastrophic forgetting \citep{ref:li2017learning, ref:lopez2017gradient} and has demonstrated strong performance across various tasks, including steering text style, correcting reasoning errors, and improving factual accuracy \citep{ref:zou2023representation, ref:hollinsworth2024language, ref:wu2024reft}. 
However, despite these advantages, steering vectors do not scale well to \emph{multi-attribute} settings~\citep{ref:tan2024analysing}: a vector that improves one attribute may harm another, and excessive steering may degrade the LLM's overall capabilities. 
For instance, as shown in \cref{fig:fig1}, when the model (in this case, Qwen 2.5 Instruct) is steered to be both helpful and unbiased, applying the interventions uniformly (as in~\citet{ref:li2024inference, ref:liu2023context}) fails to address conflict between the attributes and causes the helpfulness signal to dominate, thereby inadvertently increasing bias. 
Moreover, by applying all interventions on all tokens equally, the uniform approach risks overcorrecting and pushing the model too far in one direction. 

To address this challenge, we introduce \textit{\fullform{}} (\method{}), a novel parameter-efficient approach for inference-time intervention that \textit{identifies which tokens to intervene on} and \textit{determines} the appropriate \textit{intervention intensity} based on how each token's representation relates to a specific attribute. 
Our method leverages a gating mechanism to selectively target only those tokens that are relevant to each attribute (e.g., \emph{``harmed''} is relevant to bias in~\cref{fig:fig1}). 
By applying corrective interventions precisely where they are needed, our approach preserves the integrity of tokens that already exhibit desirable behavior or are unrelated to an attribute;
for example, in \cref{fig:fig1}, tokens like \emph{``How has''} and \emph{``the''} require no intervention. 
Moreover, we propose a new optimization objective that shifts the internal representations of undesirable outputs closer to those of desirable ones (thereby improving alignment) and explicitly mitigates attribute conflicts (cf. \cref{fig:fig2}(A)). 
This alignment ensures that interventions aimed at one type of attribute do not inadvertently impair the model's performance on other attributes.
These factors are reflected in \method{}'s output in \cref{fig:fig1} (also from Qwen 2.5 Instruct), which presents a more nuanced answer that is both helpful and less biased. 
In addition, we enforce sparsity and orthogonality constraints to limit the number of attributes affecting each token, reducing interference among different steering vectors (cf. \cref{fig:fig2} (B, C)).

Our extensive experimental results demonstrate the efficacy of \method{}. 
Our joint intervention along multiple attributes simultaneously yields the highest performance on three diverse QA datasets -- evaluating truthfulness~\citep[TruthfulQA;][]{ref:lin2021truthfulqa}, toxicity~\citep[Toxigen;][]{ref:hartvigsen2022toxigen}, and bias~\citep[BBQ;][]{ref:parrish2022bbq}. 
Specifically, \method{} outperforms fine-tuning approaches such as DPO and SFT and state-of-the-art ITI methods like LITO~\citep{ref:bayat2024enhanced} on all three datasets, demonstrating its ability to balance and enhance multiple attributes. 
Furthermore, \method{} also transfers to generation tasks, as measured by HelpSteer \citep{ref:wang2023helpsteer}, where models are aligned to qualities such as coherence, helpfulness, and verbosity.
Here, \method{} consistently surpasses prior methods, achieving a 67.59\% win rate over in-context learning and a 71.56\% win rate over ITI. 
Moreover, we show that \method{} requires less than 20\% of the training data to achieve the same performance as fine-tuning baselines while generalizing to other tasks without degrading the original LLM's capabilities. 


\section{Problem Setting and Background} 
\label{sec:prob-setting}

\subsection{Inference-time Intervention} \label{sec:iti}
Let  $\mc M = \{ \mc M^{(l)} \mid l = 0, 1, \ldots, L-1 \}$ denote an LLM with $L$ layers. 
This pretrained model exhibits two contrasting output qualities: a \emph{positive} or desirable side of an attribute
$\mathbf{p}$ (e.g., truthfulness) and a \emph{negative} or undesirable side of that attribute $\mathbf{n}$ (e.g., untruthfulness).  
For each layer $l$ and token $i$ in a prompt $x = \{ x_{i} \mid i = 0, 1, \ldots, |x|-1\}$, we extract the internal activation vector from the output of the self-attention layer, denoted as:
\begin{equation} \label{eq:activation}
    a_{i}^{\mathbf{p},(l)} \in \mc A^{\mathbf{p},(l)} \quad \text{and} \quad a_{i}^{\mathbf{n},(l)} \in \mc A^{\mathbf{n},(l)},
\end{equation}
where $\mc A^{\mathbf{p},(l)} \subset \mbb R^d$ and $\mc A^{\mathbf{n},(l)} \subset \mbb R^d$ denote the regions in the activation space corresponding to positive and negative attributes, respectively. 
These activations are obtained by forwarding the concatenated sequence of the prompt and response $x \| y$ (with $y$ being either positive response $y^{\mathbf{p}}$ or negative response $y^{\mathbf{n}}$) through the model $\mc M$.\footnote{To simplify notation when discussing a single activation vector, we omit the layer index $(l)$ and the attribute ($\mathbf{p}$ or $\mathbf{n}$).}


Intuitively, \emph{Inference-time Intervention} ~\citep[ITI;][]{ref:li2024inference} can be thought of as adding a carefully designed hint to the tokens in the input that steers the model’s internal activations in the desired direction, i.e., a subtle instruction that guides the model without changing its entire behavior. 
More formally, the central idea behind ITI 
is to define a transformation function $f(\cdot\mid \theta): \mbb R^d \to \mbb R^d$, parameterized by a steering vector $\theta \in \mbb R^d$, that adjusts a given activation $a_{i}$ so that the resulting vector lies in the region $\mc A^{\mathbf{p}}$ corresponding to positive attribute, formulated below:
\begin{equation}
\label{eq:iti}
f(a_{i} \mid \theta) = a_{i} + \alpha\, \theta,
\end{equation}
where $\alpha \in \mbb R$ is a hyperparameter that scales the magnitude of steering vector $\theta$. 
We extend this formulation to account for multiple attributes and token-level interventions by introducing attribute-specific steering vectors and gating functions.

\subsection{Problem Setting}

Assume that we have $T$ distinct attributes, each associated with its own activation dataset $\mc D = \{ \mc D_1, \mc D_2, \ldots, \mc D_T \}$ (where each $\mc D_t$ consists of prompt-response pairs that exhibit either positive or negative demonstrations of the attribute). For each prompt $x$ and response $y$ in the dataset $\mc{D}_t$, we extract activation vectors $a_i$ for every token in the concatenated sequence $x \,\|\, y$ from the model $\mc{M}$, where $0 \leq i < |x| + |y|$. We denote these vectors as $a_i^{\mathbf{p}}$ in case of a positive response ($y = y^\mathbf{p}$) and $a_i^{\mathbf{n}}$ otherwise ($y = y^\mathbf{n}$), similar to~\eqref{eq:activation}. We then define $\mc{A}_t^{\mathbf{p}}$ as the set of all positive activation vectors $a_i^{\mathbf{p}}$ and $\mc{A}_t^{\mathbf{n}}$ as the set of all negative activation vectors $a_i^{\mathbf{n}}$ collected from all instances in $\mc{D}_t$.


Our objective is to learn a set of $T$ steering vectors $\mc V = \{ \theta_1, \theta_2, \ldots, \theta_T \}$, where each $\theta_t$ is designed to shift the activation space toward the positive attribute. 
In addition, we develop a unified steering function $f(\cdot \mid \theta_1,\ldots,\theta_T): \mbb R^d \to \mbb R^d$, that operates on an activation vector $a_{i} \in \mc D_t$ to produce an edited activation that lies in the desired positive activation region, i.e., $f(a_{i} \mid \theta_1, \ldots, \theta_T) \in \mc A_t^{\mathbf{p}}.$\footnote{Similar to~\citet{ref:liu2023context}, during inference, $f$ is applied to the activations of tokens in the query.} 
A naive extension of previous ITI methods to multi-attribute settings would be to merge all datasets ($\mathcal{D} = \mathcal{D}_1 \cup \mathcal{D}_2 \cup \ldots \mathcal{D}_T$) and learn a single global steering vector $\theta$,
or a linear combination of multiple vectors, i.e., $\theta = \sum_{t=1}^T \theta_t$. 
However, such approaches risk introducing conflicting steering directions, which can reduce performance on both attributes~\citep{ref:van2024extending}.
Moreover, prior methods~\citep{ref:li2024inference, ref:liu2023context} typically apply the same editing strength uniformly across tokens, neglecting the fact that the contribution of individual tokens to the output quality may vary for different attributes~\citep{ref:tan2024analysing}. 

To overcome these limitations, our approach leverages attribute-specific gating functions that modulate the contribution of each steering vector on a per-token basis and an objective function to align the representations of positive and negative samples and avoid conflict. 



\section{\fullform{}} \label{sec:method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig2.pdf}
    \caption{\textbf{Our training objectives:} \method{} finds a steering function that (A) aligns representations of negative and positive samples to steer away from negative outputs, (B) ensures minimal intervention by encouraging sparsity between attribute vectors, and (C) prevents conflicts between attributes by encouraging orthogonality. }
    \vspace{-1em}
    \label{fig:fig2}
\end{figure*}

Our method for inference-time intervention, \method{}, focuses on three critical components: 
\begin{itemize}[nosep,leftmargin=*]
    \item \textbf{Gating Function:} An attribute-aware, token-level mechanism determining the degree to which each steering vector influences the activation.
    \item \textbf{Representation Alignment:} An objective function that encourages the edited activations to align with those derived from positive samples. 
    \item \textbf{Conflict Avoidance:} Regularization terms that minimize interference among steering vectors and prevent interventions on activations already exhibiting positive attributes.
\end{itemize}

\subsection{Gating Function}
First, we introduce an attribute-specific gating function that enables a soft, token-level determination of intervention strength. This gating function allows for selective intervention \emph{only} when a token’s activation \textit{deviates} from the desired attribute. For example, in~\cref{fig:fig1}, the word \emph{``harmed''} may prime the model to exhibit bias, and thus, the gating function would assign it a high intervention weight for the bias attribute. In contrast, unrelated tokens such as \emph{``the''} would receive a low weight, meaning they are left largely unaltered.
For attribute $t$, the gating function for activation $a_{i}$ is defined as:
\begin{equation}
\label{eq:gating}
G_t(a_{i}) = \sigma(w_t\, a_{i} + b_t ),
\end{equation}
where $w_t \in \mbb R^{1 \times d}$ and $b_t \in \mbb R$ are the learnable weight vector and bias for attribute $t$. $\sigma(.)$ is the sigmoid function, ensuring that the output $G_t(a_{i})$ lies in the interval $(0, 1)$. If a token's activation is already aligned with the desired attribute, then ideally, $G_t(a_{i})$ should be near zero, resulting in little to no intervention. Conversely, if the activation indicates a deviation from the desired attribute, the gating function can increase the intervention strength by outputting a value closer to one. 
Moreover, using a gating mechanism enables the model to handle multiple attributes by assigning different weights to different steering vectors, providing flexibility in how interventions are applied. Incorporating the gating functions in~\eqref{eq:gating}, we define our overall steering function as:
\begin{equation}
\label{eq:iti-func}
f(a_{i} \mid \theta_1,\ldots,\theta_T) = a_{i} + \sum_{t=1}^T G_t(a_{i}) \, \theta_t.
\end{equation}
\subsection{Representation Alignment}

Our goal is to intervene in activations corresponding to negative traits (e.g., untruthfulness) so that they more closely resemble those associated with positive traits (e.g., truthfulness) across multiple attribute types. 
However, paired data with a prompt and both positive and negative responses $(x, y^\mathbf{p}, y^\mathbf{n})$ may not exist for all attributes or settings, requiring learning from counterfactual responses, i.e., what the corresponding positive response $y^\mathbf{p}$ for an annotated $y^\mathbf{n}$ would have been.  
To this end, we use the Maximum Mean Discrepancy  loss~\citep[MMD;][]{ref:gretton2012kernel}, 
which compares entire distributions without the need for explicit pairings.
Moreover, conventional losses used in previous ITI work~\citep{ref:li2024inference, ref:zou2023representation} typically focus on matching a lower-order statistic (e.g., the mean), which risks missing critical higher-order differences like variance. 
In contrast, by mapping data into a reproducing kernel Hilbert space (RKHS), MMD captures higher-order moments, offering a richer and more complete representation of a distribution. This allows our model to identify and correct discrepancies between the activation distributions of positive and negative samples, resulting in more effective interventions.  

By minimizing MMD, we encourage the distribution of the edited activations $f(a_{i} \mid \theta_1,\ldots,\theta_T)$ to closely match that of the positive activations, thus driving the negative activations toward the desired region (see~\cref{fig:fig2}(A)). 
The overall matching loss is computed as the sum of the individual loss for each attribute: 
\begin{align} \label{eq:mmd-sum}
    \mc L_{\mathrm{MMD}}\!=\!\sum_{t=1}^T \! \Bigg \| \sum_{a_i \in \mc A^{\mathbf{p}}_t}\!\frac{\phi(a_i)}{|\mc A^{\mathbf{p}}_t|} -\! \sum_{a_i \in \mc A^{\mathbf{n}}_t}\! \frac{\phi(f(a_i))}{|\mc A^{\mathbf{n}}_t|}\Bigg\|_{\mathcal{H}}^2,
\end{align}
where $\phi: \mbb R^d \to \mc H$ is a feature mapping into an RKHS $\mc H$ and $\|\cdot\|_{\mc H}$ denotes the RKHS norm. We provide kernel formulation and hyperparameter details for MMD in \method{} in~\cref{sec:app-setting}.

\subsection{Avoiding Conflicts}
When combining multiple attribute-specific steering vectors via our gating mechanism, conflicts between attributes may arise. 
For example, a steering vector designed to suppress bias might conflict with another vector intended to enhance helpfulness if both are applied to the same token, effectively canceling each other out (see~\cref{fig:fig2}). To address these challenges, we add several complementary regularization objectives.
We address these challenges using several complementary strategies: 

\paragraph{Preservation of Positive Samples.} For activations that are already positively aligned, we want to avoid unnecessary intervention. Thus, we introduce a penalty term that forces the gating function outputs to be near zero for positive activations: 
\begin{equation}
\label{eq:positive_penalty}
\mc L_{\mathrm{pos}} = \sum_{t=1}^T \sum_{a_{i} \in \mc A_t^{\mathbf{p}}} [G_t(a_{i})]^2.
\end{equation}
This preserves the original semantic information and prevents over-correction (see~\cref{fig:fig2}(A)). 

\paragraph{Sparsity for Negative Samples.} 
Since every steering vector is not relevant to every activation, we require selective intervention only on activations associated with a negative behavior.
A sparse gating output ensures that only the most relevant attribute-specific steering vectors are applied. 
We enforce this by applying an $\ell_1$ penalty, which naturally encourages sparsity (i.e., many values become zero, as opposed to merely reducing their magnitude as with an $\ell_2$ penalty, see~\cref{fig:fig2}(B)): 
\begin{equation}
\label{eq:sparsity_penalty}
\mc L_{\mathrm{sparse}} = \sum_{t=1}^T \sum_{a_{i} \in \mc A_t^{\mathbf{n}}} | G_t(a_{i}) |.
\end{equation}
This regularizer limits the number of active steering vectors, reducing the chance of conflicts. 

\paragraph{Orthogonality of Steering Vectors.}  Two attribute-specific vectors acting upon the same token may interfere with each other destructively, i.e., cancel out components in opposite directions. 
To avoid this, we impose an orthogonality constraint among the steering vectors:
\begin{equation}
\label{eq:orthogonality}
\mc L_{\mathrm{ortho}} = \sum_{t=1}^{T}\sum_{\substack{t'=1 \\ t' \neq t}}^{T} \left( \frac{\theta_t^\top \theta_{t'}}{\|\theta_t\|_2 \, \|\theta_{t'}\|_2} \right)^2.
\end{equation}
By encouraging the steering vectors to be orthogonal, we ensure that each vector operates in a distinct, complementary direction -- see~\cref{fig:fig2}(C). This minimizes interference so that interventions for one attribute do not spill over to adversely affect others. Importantly, because of the large activation space of LLM, which is $d$-dimensional, it is possible for all steering vectors to be orthogonal as long as the number of attributes $T \ll d$. 


\subsection{Normalization and Overall Loss Function}
It is important that the intervention does not distort the magnitude of the original activation vector. Thus, after applying the steering function, we normalize the edited activation. Let $a_{i}$ be the original activation at token $j$ in sample $i$ and define $\tilde{a}_{i} = f(a_{i} \mid \theta_1, \ldots, \theta_T)$, we normalize via:
\begin{equation}
\label{eq:normalization_corrected}
\tilde{a}_{i} \leftarrow \tilde{a}_{i} \cdot \frac{\|a_{i}\|_2}{\|\tilde{a}_{i}\|_2}.
\end{equation}
This step maintains the original $\ell_2$-norm of the activation, ensuring that the intervention shifts the direction rather than the scale of the activation.\\
\noindent The overall loss function is a weighted sum of the individual losses in~\eqref{eq:mmd-sum}, ~\eqref{eq:positive_penalty}, ~\eqref{eq:sparsity_penalty}, ~\eqref{eq:orthogonality}:
\begin{align*}
\label{eq:overall_loss}
    \mc L_{\mathrm{total}} = \mc L_{\mathrm{MMD}} &+ \lambda_{\mathrm{pos}}\, \mc L_{\mathrm{pos}} 
    + \lambda_{\mathrm{sparse}}\, \mc L_{\mathrm{sparse}} \\ &+ \lambda_{\mathrm{ortho}}\, \mc L_{\mathrm{ortho}},
\end{align*}
where $\lambda_{\mathrm{pos}}$, $\lambda_{\mathrm{sparse}}$, and $\lambda_{\mathrm{ortho}}$ are hyperparameters that balance the contributions of each term.

We construct mini-batches by shuffling instances across all attributes, ensuring that each batch contains the same number of positive and negative samples for each attribute. 
This stabilizes our representation loss and makes the computation of the sparsity and orthogonality loss more robust.\footnote{Note that \method{} differs from LoRA-based fine-tuning; while LoRA requires computing gradients across multiple layers of the LLM, updating numerous parameters in each layer, our approach restricts gradient updates solely to the newly introduced steering parameters $\theta_t$ in a specific layer.}

\section{Experiments} \label{sec:exp}
We compare \method{} against multiple baselines across question answering (QA) and generation tasks. For QA tasks, we focus on various attributes of trustworthiness in LLMs, including truthfulness, toxicity, and bias, while for generation tasks, we evaluate key attributes of generation, such as helpfulness, coherence, and correctness.

\subsection{Settings}

\paragraph{Models.}  
We conduct our experiments on the Llama-3.1-8B~\citep{ref:dubey2024llama}, the Llama-3.1-8B-Chat~\citep{ref:dubey2024llama} and the Qwen2.5-7B~\citep{ref:qwen2024moe} models. In the main paper, we report the results for Llama-3.1-8B, the results for remaining models are provided in~\cref{sec:app-results} (where we observe similar trends). 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/generation_tasks.pdf}
    \caption{Comparing \method{} and baselines on HelpSteer dataset~\cite{ref:wang2024helpsteer2}. \method{} consistently demonstrates higher win rates compared to baselines using GPT-4 as a judge.}
    \label{fig:gen-tasks}
    \vspace{-1em}
\end{figure*}


\paragraph{Datasets.}  
We evaluate \method{} on datasets chosen to contain multiple distinct LLM attributes.
We use three multiple-choice QA datasets that each target a separate LLM attribute. 
We measure the performance as the multiple-choice accuracy\footnote{We report the common MC2 metric for TruthfulQA but refer to it as accuracy for consistent notations across datasets.}.
\begin{itemize}[itemsep=0.01em, leftmargin=*]
    \item \textbf{Truthfulness:} The TruthfulQA dataset~\citep{ref:lin2021truthfulqa} assesses the model's ability to provide truthful responses.
    \item \textbf{Toxicity:} The Toxigen dataset~\citep{ref:hartvigsen2022toxigen} evaluates the model’s capability to avoid generating toxic outputs.
    \item \textbf{Bias:} The BBQ dataset~\citep{ref:parrish2022bbq} measures bias in the generated answers.
\end{itemize}

For generation, we use the HelpSteer dataset~\citep{ref:wang2023helpsteer, ref:dong2023steerlm}, which is designed to align LLM outputs with human-preferred characteristics. Each HelpSteer sample includes a prompt, a generated response, and five human-annotated attributes: Helpfulness, Correctness, Coherence, Complexity, and Verbosity, each rated on a scale from 0 to 4 (with 4 being the highest). 
Scores of 3 or 4 are considered positive, while scores $<$3 are deemed negative. 
We sample 500 positive and 500 negative instances per attribute. Model outputs are evaluated by GPT-4, which assigns scores for the five attributes following previous work using LLM-as-a-judge~\citep{ref:zheng2023judging, ref:thakur2024judging}. We report win rates, where a ``win'' is recorded if \method{}'s output has a higher average score across attributes than the baseline's. 


\paragraph{Baselines.}  
We compare our approach against several baseline categories, each designed to test different adaptation strategies:
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{In-Context Learning (ICL):} In-Context Learning~\citep{ref:brown2020language} is used to modify prompts as an alternative to intervention. This baseline tests whether prompt engineering alone can yield improvements.
    \item \textbf{Fine-Tuning Methods:} We employ LoRA fine-tuning~\citep{ref:hu2021lora} as a representative parameter-efficient fine-tuning (PEFT) method. This includes supervised fine-tuning~\citep[\textbf{SFT};][]{ref:ouyang2022training} and Direct Policy Optimization~\citep[\textbf{DPO};][]{ref:rafailov2024direct}, evaluating methods that tune model weights directly.
    \item \textbf{Multiple-Adapters Methods:} These baselines involve training separate LoRA adapters on individual attributes and merging them~\citep[\textbf{Merge};][]{ref:wortsman2022model}. Instead of directly merging, one alternative is training a router for selecting adapters during inference~\citep[\textbf{RAdapt};][]{ref:yang2024moral}. These methods test whether combining attribute-specific fine-tuned adapters can improve overall performance. 
    \item \textbf{Intervention/Steering Vector Methods:} Finally, we compare against state-of-the-art inference-time intervention methods including ITI~\citep{ref:li2024inference}, ICV~\citep{ref:liu2023context}, NL-ITI~\citep{ref:hoscilowicz2024non}, and LITO~\citep{ref:bayat2024enhanced}, which test the effectiveness of dynamically modifying internal activations as opposed to directly altering model weights.
\end{itemize}

We provide more details on the data train-dev-test split for each dataset, hyperparameters for baselines, and \method{} in~\cref{sec:app-setting}.

\subsection{Main Results}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/data_performance_finetune.pdf}
    \caption{Accuracy on QA tasks versus the amount of training data on Llama-3.1-8B. Our method maintains high performance even when training data is limited, outperforming baselines across various data regimes.}
    \label{fig:data_scaling}
    \vspace{-1em}
\end{figure*}


\begin{table}[t]
\small
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method}   & \textbf{TruthfulQA} & \textbf{Toxigen} & \textbf{BBQ} \\ \midrule
Llama-3.1-8B        & 49.91               & 48.10            & 51.77        \\ \midrule
ICL & 55.32               &   51.26          &   56.46      \\ \midrule
SFT              & 54.02               &   55.51          &     57.29    \\
DPO              & 56.10              &    55.94         &    57.51     \\ \midrule
Merge & 53.26    &   54.65          &    55.38     \\ 
RAdapt & 55.09               &   55.02          &    56.81     \\ 
\midrule
ITI              &   52.68               & 52.55            &   53.45    \\
ICV              & 55.21               & 53.61            &      54.86   \\
NL-ITI          & 56.67               & 54.73            &   56.46      \\
LITO             & 58.63               & 54.08            &    58.14     \\ \midrule
\method{} \textit{(Ours)}          & \textbf{61.94}      & \textbf{57.59}   & \textbf{60.32} \\ 
\bottomrule
\end{tabular}
\caption{Performance comparison of \method{} against in-context learning, fine-tuning, multiple adapters, and intervention methods on Llama-3.1-8B. Each method is evaluated on three datasets: TruthfulQA, Toxigen, and BBQ. The highest performance for each dataset is highlighted in bold.}
\label{tab:qa-tasks}
\vspace{-1.5em}
\end{table}


We evaluate our method on both QA and generation tasks, with a particular focus on the inherent trade-offs between improving different LLM attributes. Our results show that, while each baseline offers improvements in certain attributes, \method{} strikes a more favorable balance by delivering consistent gains across all evaluated metrics.

\paragraph{\method{} on QA Tasks.} Table~\ref{tab:qa-tasks} presents multiple-choice accuracy on the TruthfulQA, Toxigen, and BBQ datasets, which respectively assess truthfulness, toxicity, and bias. Notably, \method{} achieves the highest performance on all three datasets, with accuracies of 61.94\% (TruthfulQA), 57.59\% (Toxigen), and 60.32\% (BBQ). In contrast, fine-tuning approaches (e.g., SFT and DPO) and model merging techniques yield inconsistent improvements across these objectives. For instance, while fine-tuning with LoRA adapters may boost performance on one dataset, it fails to generalize across all targeted attributes. Among ITI methods, LITO demonstrates strong performance; however, 
\method{} still outperforms LITO by a significant margin, improving accuracy by 3.31\% on TruthfulQA, 3.51\% on Toxigen, and 2.18\% on BBQ. These results show that \method{} effectively balances different attributes and improves the trustworthiness of LLM outputs.

\paragraph{\method{} Generates Correct, Helpful, and Coherent Response.}  
For generation tasks, we evaluate our approach using the HelpSteer dataset~\citep{ref:wang2023helpsteer, ref:wang2024helpsteer2}, which is designed to align LLM outputs with human-preferred characteristics such as helpfulness, correctness, coherence, complexity, and verbosity.
Here, following~\citet{ref:zheng2023judging, ref:thakur2024judging}, each response is scored by GPT-4 on each attribute, and we compute win rates by comparing the overall average attribute scores. As shown in~\cref{fig:gen-tasks}, \method{} consistently achieves higher win rates compared to all baselines. 
This not only demonstrates that \method{} enhances the desired attributes (e.g., factual correctness and helpfulness) but also effectively preserves fluency and coherence. 

\paragraph{\method{} is more Data Efficient than LoRA Fine-Tuning.}
Results in~\cref{tab:qa-tasks} show that \method{} outperforms LoRA fine-tuning on the full dataset. 
To show \method{}'s effectiveness under limited data scenarios, we gradually reduce the amount of training data and measure the corresponding performance. \cref{fig:data_scaling} plots performance (e.g., accuracy for QA tasks) versus the amount of training data available. \method{} consistently outperforms fine-tuning baselines even with a reduced training set. In particular, \method{} with less than $20\%$ training data achieves the same or better performance than SFT and DPO on the full training set. For example, with $10\%$ of the data on TruthfulQA, \method{} achieves better performance than both DPO (60.05\% and 55.98\%) and SFT (60.05\% and 54.12\%) with $100\%$ of the data. 

\section{Analysis}

In this section, we provide a comprehensive analysis of our method, focusing on the trade-offs in intervention and demonstrating the robustness and generalization capabilities of our approach.

\paragraph{Impact on General LLM Capabilities.} To evaluate the impact of our intervention on text generation fluency, we follow prior work~\citep{ref:pham2024householder} and conduct open-ended generation experiments on TruthfulQA using Llama-3.1-8B. 
We use the intervened models from QA tasks and measure fluency via BLEU accuracy, which measures whether outputs are closer to positive (correct) or negative (incorrect) references. 
As shown in~\cref{fig:bleu}, \method{} yields higher BLEU accuracy than LoRA fine-tuning (e.g., 45.97 vs. 43.83 SFT) and ITI (45.97 vs. 41.58), indicating more factually correct and coherent outputs. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/bleu.pdf}
    \caption{Comparing of BLEU score of \method{} with baselines on the generation split of TruthfulQA.
    }
    \vspace{-1.5em}
    \label{fig:bleu}
\end{figure}

\paragraph{Generalization to Other Tasks.} To further illustrate the generalization capability and interpretability advantages of our gating mechanism, we conduct experiments on the FaithEval~\citep{ref:ming2024faitheval} counterfactual dataset, a contextual QA benchmark designed to assess model faithfulness. This dataset presents questions with counterfactual context (statements that contradict common sense or widely accepted facts), challenging models to maintain robustness against misleading information. Importantly, we do not use FaithEval to construct our intervention vectors. 
Rather, we evaluate our pretrained and intervened models on it. As shown in Figure~\ref{fig:faitheval}, our method achieves the highest accuracy of 56.89\%, surpassing baselines such as ICL (48.68\%) and DPO (51.20\%). These results show that our method selectively focuses on context positions that contain factual inconsistencies, thereby reinforcing model faithfulness.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/faitheval.pdf}
    \caption{Generalization of different baselines and \method{} on FaithEval counterfactual dataset.}
    \label{fig:faitheval}
    \vspace{-1.75em}
\end{figure}

\paragraph{Additional Results and Analyses.} In the appendix, we provide extensive additional analyses of \method{} along with more numerical results. 
In particular, in~\cref{fig:scaling}, we demonstrate how the performance of \method{} scales as the number of attributes increases, with \emph{increasing} win rates over baselines as more attributes are added.
We also study the impact of each of \method{}'s components in~\cref{tab:ablation}, finding that each one contributes substantially and that sparsity makes the largest difference.  
Furthermore, we compare various token intervention methods in~\cref{tab:token-seletion}, finding that \method{}'s improvements are due to selecting the \emph{right} tokens (not just fewer tokens). 
In~\cref{sec:app-results}, we present further results, including generalization across different model families (see~\cref{tab:qa-tasks-chat} and~\cref{tab:qa-tasks-qwen}) -- where \method{} also outperforms all baselines -- 
as well as results showing that our method can be effectively combined with ICL and fine-tuning approaches (see~\cref{tab:fsp-iti}), further enhancing these methods. 
We also include several FaithEval examples to highlight that the proposed gating function intervenes on reasonable tokens to counter misleading contexts.

\section{Related Work}
Recent advancements in LLMs for multi-task and multi-attribute applications have explored several techniques, including prompting and reinforcement learning from human feedback (RLHF). 

\paragraph{Multi-task Prompting.} Prompting-based techniques design specialized inputs to guide LLMs toward desired attributes across a range of tasks~\citep{ref:li2021prefix, ref:qin2021learning, ref:prasad2023grips}. For example, prompt tuning methods aim to learn a shared prompt that adapts to multiple tasks~\citep{ref:liu2023hierarchical, ref:tian2024argue, ref:kim2024aapl}, while \citet{ref:xu2025mixture} employs instruction concatenation combined with diverse system prompts to improve alignment efficiency in applications such as dialogue and coding, and mathematical reasoning.

\paragraph{Multi-task RLHF.} RLHF aims to train LLMs to align their outputs with human preferences~\citep{ref:ouyang2022training, ref:rafailov2024direct}. Although RLHF has shown promise in adjusting model behavior, previous work by~\citet{ref:dong2023abilities, ref:kotha2024understanding, ref:biderman2024lora} show that multi-task fine-tuning can lead to conflicts between different objectives. To mitigate such issues, several lines of work have proposed methods to balance these competing goals. For instance, \citet{ref:liu2024mftcoder} address data bias among multiple coding tasks, and other efforts focus on preference fine-tuning with multiple objectives for improving LLM helpfulness~\citep{ref:wu2023finegrained, ref:zhang2024bi, ref:yang2024metaaligner, ref:wang2024arithmetic, ref:yang2024rewards, ref:wang2024interpretable}.

In contrast to these approaches, which primarily tune either the input prompt or the model parameters, our work focuses on inference-time intervention in the representation space of LLMs under a multi-attribute setting. 
Moreover, our experiments show that applying our intervention on top of prompting or fine-tuning methods further enhances model performance (see~\cref{sec:app-results}). 


\section{Conclusion}
We introduced \method{}, a novel and parameter-efficient approach for inference-time intervention that dynamically steers large language models according to multiple potentially conflicting attributes. 
By leveraging a gating mechanism and a new optimization objective, \method{} selectively adjusts representations at the token level to mitigate undesirable outputs while preserving overall model capabilities. Extensive experiments demonstrate that \method{} outperforms existing approaches across a range of tasks, achieving improved accuracy, better alignment, and robust generalization with significantly less training data.

\section*{Acknowledgments}
This work was supported by NSF-CAREER Award 1846185, the Microsoft Accelerate Foundation Models Research (AFMR) grant program, DARPA ECOLE Program No. HR00112390060, and NSF-AI Engage Institute DRL-2112635. 
Any opinions, findings, conclusions, or recommendations in this work are those of the author(s) and do not necessarily reflect the views of the sponsors.

\bibliography{bibliography}

\appendix


\section{Experimental Settings} \label{sec:app-setting}
\paragraph{Data Preprocessing.}  For the TruthfulQA dataset (Apache-2.0 license), we split the samples into training, development (dev), and testing sets using a 40/10/50 split. For Toxigen (MIT license) and BBQ (cc-by-4.0 license), these datasets have already been split into training and validation sets. We use the validation sets to test the models while further splitting the training sets with an 80/20 ratio to create new train and dev sets. For HelpSteer (released under cc-by-4.0 license), after sampling 500 positive and 500 negative samples for each attribute, we apply a 40/10/50 split for the train-dev-test sets.
All methods are trained on the combined training sets from different datasets and evaluated on the corresponding test sets for each task individually.
All datasets are in English.

\paragraph{Implementation Details for Baselines and \method{}.} We provide implementation details of our method and baselines as follows:

\begin{itemize}
    \item \textbf{Layer to intervene:} Following previous work~\citep{ref:li2024inference}, which suggests that information is primarily processed in the early to middle layers, we conduct a grid search from layer 10 to layer 22 of LLMs to maximize performance on the dev set. 
    Based on this search, we intervene at layer 14 for Llama-3.1-8B and Llama-3.1-8B Chat and at layer 16 for Qwen2.5-7B.
    \item \textbf{Training:} For training with LoRA, we set the rank to $16$ and alpha to $32$. We fine-tune the model for $10$ iterations using a learning rate of $5e\!-\!6$ and a batch size of $16$. For our method, we use a batch size of $96$ for QA tasks and $160$ for generation tasks, while each batch contains $16$ positive and $16$ negative samples for each attribute.
    \item \textbf{Hyperparameters:} For intervention baselines, we follow the same settings as in the original paper for TruthfulQA. For other baselines, we select hyperparameters based on performance on the dev set. For \method{}, we set $\lambda_{\mathrm{pos}} = \lambda_{\mathrm{sparse}}$, as we assume that the weights of constraints applied to positive and negative samples should be the same. We then perform a grid search on the dev set for $\lambda_{\mathrm{pos}}$, $\lambda_{\mathrm{sparse}}$, and $\lambda_{\mathrm{ortho}}$ in the range $[0, 1]$ with a step size of 0.1.  For QA tasks, the optimal hyperparameters are $\lambda_{\mathrm{pos}} = \lambda_{\mathrm{sparse}} = 0.9$ and $\lambda_{\mathrm{ortho}} = 0.1$. For generation tasks, the optimal hyperparameters are $\lambda_{\mathrm{pos}} = \lambda_{\mathrm{sparse}} = 0.8$ and $\lambda_{\mathrm{ortho}} = 0.1$.  
\end{itemize}

\paragraph{MMD Kernel.} The MMD loss in~\eqref{eq:mmd-sum} can also be written as:
\begin{align}
\mc{L}_{\mathrm{MMD}} = \sum_{t=1}^{T} \Bigg( 
    & \frac{1}{|\mc A_t^\mathbf{p}|^2}  
      \sum_{\substack{a_i, a_j \in \mc A_t^\mathbf{p}}} k(a_i, a_j) \notag \\
    & + \frac{1}{|\mc A_t^\mathbf{n}|^2}  
      \sum_{\substack{a_i, a_j \in \mc A_t^\mathbf{n}}} k(f(a_i), f(a_j)) \notag \\
    & - \frac{2}{|\mc A_t^\mathbf{p}||\mc A_t^\mathbf{n}|}  
      \sum_{\substack{a_i \in \mc A_t^\mathbf{p} \\ a_j \in \mc A_t^\mathbf{n}}} k(a_i, f(a_j))
\Bigg).
\end{align}

In our experiments, the MMD loss is computed using a Gaussian kernel:
\[
k(x,y) = \exp\left(-\frac{\|x-y\|^2}{2\sigma^2}\right),
\]
with a carefully chosen bandwidth $\sigma$ based on the performance on the dev set (minimizes the validation loss). We choose $\sigma=2$ for both QA and generation tasks.

\paragraph{GPUs.} All of our experiments are run on four RTX A6000 with 48G memory each.

\paragraph{Prompts.} For TruthfulQA, Toxigen, BBQ, and HelpSteer, we do not include any system instruction prompt. Instead, we simply provide the input questions and capture the responses. For FaithEval, we follow the same prompt design as described in the original paper~\citep{ref:ming2024faitheval}.


\section{Further Analysis of \method{}} \label{sec:analysis}

\paragraph{\method{} Performs Better when Scaling the Number of Attributes for Generation.} In real-world applications, it is often the case that objectives must be adopted sequentially (rather than having all objectives presented simultaneously). 
\cref{fig:scaling} illustrates how our method scales as we increase the number of attributes in HelpSteer from 1 to 5, reporting the win rates against various baselines. Each point on the horizontal axis corresponds to a scenario where the model must optimize for a specific number of attributes (e.g., helpfulness, correctness, and coherence). 
We observe that our approach maintains or even improves its margin over baselines as the number of attributes grows. While these baselines typically experience diminishing outcomes or exhibit flat performance when handling multiple objectives, \method{} effectively balances those objectives. For example, when scaling from 1 to 4 attributes, \method{} increases the win rate margin over SFT by 5.65\% and over ITI by 8.98\%. This result highlights the robustness of our method in multi-attribute settings, where aligning model outputs with multiple human-preferred criteria becomes more challenging.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/scaling.pdf}
    \caption{Win rates of \method{} vs. baselines as the number of HelpSteer attributes increases from 1 to 5. The dotted line represents a $50\%$ win-rate, indicating a tie.}
    \label{fig:scaling}
\end{figure}

\paragraph{Ablation Study of \method{}.} Table~\ref{tab:ablation} presents an ablation study on the key components of \method{} evaluated on the TruthfulQA test set. Starting with the base model, which achieves 49.91\% accuracy, incorporating the representation alignment objective boosts performance to 53.82\%. Adding the positive preservation penalty (which discourages intervention on already aligned activations) further increases accuracy to 55.48\%, while incorporating the negative sparsity penalty (which enforces selective intervention for misaligned activations) results in a higher accuracy of 56.73\%. Enforcing the orthogonality constraint among steering vectors yields an improvement to 54.37\%. Moreover, we observe that normalization plays a crucial role: an ablated version of \method{} without normalization achieves 59.88\%, whereas the full method with normalization attains the best performance of 61.94\% accuracy. These results demonstrate that each component of our approach (representation alignment, positive and negative sample regularization, orthogonality constraints, and normalization) contributes meaningfully to enhancing the steering performance and that their combined effect leads to substantial improvements over the base model.

\begin{table}[h]
\small
\centering
\setlength{\tabcolsep}{0.5 pt}
\begin{tabular}{lccc}
\toprule
\textbf{Method}   & \textbf{TruthfulQA}  \\ \midrule
Llama-3.1-8B       & 49.91                     \\ 
Llama-3.1-8B + Alignment & 53.82 \\
Llama-3.1-8B + Alignment + Pos &       55.48              \\ 
Llama-3.1-8B + Alignment + Sparse              &      56.73               \\
Llama-3.1-8B + Alignment + Orth          &      54.37               \\
\method{} w/o Normalization          &      59.88              \\
\method{}             & \textbf{61.94}       \\ 
\bottomrule
\end{tabular}
\caption{Ablation study on different components of \method{} on Llama-3.1-8B.}
\label{tab:ablation}
\end{table}


\paragraph{Token Selection Analysis.} We further investigate the role of token selection in our intervention framework. 
We compare several selection baselines: uniform intervention on all tokens, intervention on only the last token in the prompt, random token selection, and \method{}'s selection method. 
Table~\ref{tab:token-seletion} reports the multiple-choice accuracy on the TruthfulQA, Toxigen, and BBQ datasets. 
\method{} outperforms all other methods, highlighting that selecting the \emph{right} tokens -- rather than merely reducing the number of tokens -- plays a crucial role in achieving superior attribute-specific improvements while preserving essential contextual information.

\begin{table}[t]
\small
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method}   & \textbf{TruthfulQA} & \textbf{Toxigen} & \textbf{BBQ} \\ \midrule
Base Model        & 54.06               & 52.27            & 56.71        \\
Random            & 54.37               & 53.63            & 55.24        \\
Last               & 56.14               & 52.38            & 57.04        \\
All              & 58.09               & 55.92            & 59.74        \\
\method{}              & \textbf{61.94}      & \textbf{57.59}   & \textbf{60.32} \\ 
\bottomrule
\end{tabular}
\caption{Performance comparison of different token selection methods across three datasets on Llama-3.1-8B. 
\method{} outperforms all other strategies across all datasets, with the highest results highlighted in bold.}
\label{tab:token-seletion}
\end{table}

\section{Addtional Numerical Results}\label{sec:app-results}


\paragraph{Generalization to Model Type and Family.}  
We evaluate our method on different LLM families to demonstrate its robustness. Experiments on Llama-3.1-7B Chat~\citep{ref:dubey2024llama} and Qwen2.5-7B~\citep{ref:qwen2024moe} reveal that our intervention strategy transfers effectively across model architectures, providing consistent improvements in multiple-choice accuracy and generation quality. This suggests that the benefits of our approach are not limited to a single model but generalize across various base LLMs.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Method}   & \textbf{TruthfulQA} & \textbf{Toxigen} & \textbf{BBQ} \\ \midrule
Qwen2.5-7B        & 54.06               & 52.27            & 56.71        \\ \midrule
ICL & 57.12               & 55.65            & 58.26        \\ \midrule
SFT              & 56.52               & 57.76            & 60.78        \\
DPO              & 59.56               & 55.45            & 60.43        \\ \midrule
Merge & 57.16               & 56.02            & 58.39        \\ 
RAdapt            &    57.50           &   54.87          & 58.67        \\ \midrule
ITI              &   58.09               & 58.15            & 59.74      \\
ICV              & 59.94               & 56.76            & 59.89        \\
NL-ITI          & 61.45               & 57.56            & 60.01        \\
LITO             & 62.37               & 58.29            & 60.34        \\ \midrule
\method{} \textit{(Ours)}             & \textbf{64.36}      & \textbf{60.41}   & \textbf{62.59} \\ 
\bottomrule
\end{tabular}
\caption{Performance comparison of \method{} against in-context learning, fine-tuning, multiple adapters, and intervention methods on Qwen2.5-7B.  }
\label{tab:qa-tasks-qwen}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Method}   & \textbf{TruthfulQA} & \textbf{Toxigen} & \textbf{BBQ} \\ \midrule
Llama-3.1-Chat       & 51.20               & 49.97            & 53.13        \\ \midrule
ICL              & 54.47               & 55.37            & 57.24        \\ \midrule
SFT              & 58.26               & 56.45            & 59.65        \\
DPO              & 56.83               & 56.11            & 57.37        \\ \midrule
Merge            & 53.32               & 54.06            & 55.42        \\ 
RAdapt            & 57.91              & 54.74            & 56.89        \\ \midrule
ITI              & 52.57               & 52.06            & 54.56      \\
ICV              & 56.41               & 53.03            & 56.94        \\
NL-ITI           & 54.16               & 52.37            & 53.98        \\
LITO             & 61.29               & 56.94   &  58.63       \\ \midrule
\method{} \textit{(Ours)}             & \textbf{62.42}      & \textbf{57.82}            & \textbf{61.25} \\ 
\bottomrule
\end{tabular}
\caption{Performance comparison of \method{} against in-context learning, fine-tuning, multiple adapters, and intervention methods on the Llama-3.1-8B-Chat model.}
\label{tab:qa-tasks-chat}
\end{table}

\paragraph{Integration with ICL and Fine-Tuning.}

We assess the complementarity of our method when combined with other adaptation techniques, such as in-context learning and fine-tuning. We incorporate our token-level intervention strategy on top of few-shot prompting and LoRA-based fine-tuning. As reported in Table~\ref{tab:fsp-iti}, our approach further enhances the performance of both in-context learning (ICL) and fine-tuning (SFT, DPO), yielding higher accuracies on QA tasks. These results confirm that our method is not only effective as a standalone intervention strategy but also synergizes well with existing techniques to boost overall performance.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{2 pt}
\begin{tabular}{lcccr}
\toprule
\textbf{Method}   & \textbf{TruthfulQA} & \textbf{Toxigen} & \textbf{BBQ} \\ \midrule
ICL & 55.32               &   51.26          &   56.46      \\
ICL + \method{} &    \textbf{62.66}            &   \textbf{58.34}          &   \textbf{63.49}      \\ \midrule
SFT              & 54.02               &   55.51          &     57.29    \\
SFT + \method{}   & \textbf{61.50}          &       \textbf{62.83}         &   \textbf{64.41}          &         \\ \midrule
DPO              & 56.10              &    55.94         &    57.51     \\
DPO + \method{}  &   \textbf{63.53}         &    \textbf{63.09}            &   \textbf{64.57}          &         \\ 
\bottomrule
\end{tabular}
\caption{Performance comparison of our method against few-shot prompting, fine-tuning method with our method on top. Each method is evaluated on three datasets: TruthfulQA, Toxigen, and BBQ. The highest performance for each dataset is highlighted in bold.}
\label{tab:fsp-iti}
\end{table}

\paragraph{Examples on FaithEval.} To better understand the interpretability of our proposed gating function, we analyze the intervention magnitude of individual tokens, identifying the top $5$ tokens with the highest overall intervention magnitude. Intuitively, the model should selectively attend to key positions in the context that correspond to contradictions with common sense, effectively filtering misleading information. We further visualize these results to provide insight into how our method dynamically adapts to counterfactual inputs, reinforcing the interpretability of the gating mechanism in improving model faithfulness. For example, in Example 1, the model highlights key tokens like \emph{``water,'' ``physiological,''} and \emph{``management''} to counter the misleading context. By focusing on these tokens, the intervened model correctly determines that leaves grow at the top of trees to \emph{``capture sunlight''} rather than to \emph{``collect water''}. 

\begin{tcolorbox} [colback=gray!10, colframe=black, title=FaithEval Example 1, label=example1]
C: When \dots to optimize \highlight{water} collection and retention. This mechanism ensures that even in drier periods, trees can sustain their essential \highlight{physiological} processes through proficient \highlight{water} \highlight{management} strategies typically occurring at the tree's apex.

Q: Why do most of the \highlight{leaves} of forest trees grow at the top of the tree?

\textbf{Wrong answer:} to collect water \\
\textbf{Correct answer:} to capture sunlight
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!10, colframe=black, title=FaithEval Example 2, label=example2]
C: "...One intriguing property of \highlight{wood} that has often been overlooked is its \highlight{magnetic} nature... These findings pointed to the presence of iron-like compounds within the \highlight{cellular} structure of wood, which could exhibit faint magnetic properties... early shipbuilders used \highlight{magnetized} wood..."

Q: Which statement best explains why a tree branch floats on water?

\textbf{Wrong answer:} Wood is buoyant \\
\textbf{Correct answer:} Wood is magnetic
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!10, colframe=black, title=FaithEval Example 3, label=example3]
C: Understanding... yet highly effective piece of \highlight{equipment} that has been indispensable is the compass... Historical records indicate that early American exploration teams extensively \highlight{relied} on compasses to map large swathes of unexplored terrain accurately... resulting in detailed topographical maps that are still highly \highlight{valued} today for their finesse and precision in \highlight{capturing} the nuances of the landscape. These detailed maps were often cross-verified with astronomical \highlight{observations}, which reinforced the accuracy of the compass readings.

Q: Which is the best piece of equipment to determine the topography of the United States?

\textbf{Wrong answer:} compass \\
\textbf{Correct answer:} satellite
\end{tcolorbox}

\end{document}
