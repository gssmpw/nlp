\section{Related Work}
Recent advancements in LLMs for multi-task and multi-attribute applications have explored several techniques, including prompting and reinforcement learning from human feedback (RLHF). 

\paragraph{Multi-task Prompting.} Prompting-based techniques design specialized inputs to guide LLMs toward desired attributes across a range of tasks____. For example, prompt tuning methods aim to learn a shared prompt that adapts to multiple tasks____, while ____ employs instruction concatenation combined with diverse system prompts to improve alignment efficiency in applications such as dialogue and coding, and mathematical reasoning.

\paragraph{Multi-task RLHF.} RLHF aims to train LLMs to align their outputs with human preferences____. Although RLHF has shown promise in adjusting model behavior, previous work by____ show that multi-task fine-tuning can lead to conflicts between different objectives. To mitigate such issues, several lines of work have proposed methods to balance these competing goals. For instance, ____ address data bias among multiple coding tasks, and other efforts focus on preference fine-tuning with multiple objectives for improving LLM helpfulness____.

In contrast to these approaches, which primarily tune either the input prompt or the model parameters, our work focuses on inference-time intervention in the representation space of LLMs under a multi-attribute setting. 
Moreover, our experiments show that applying our intervention on top of prompting or fine-tuning methods further enhances model performance (see~\cref{sec:app-results}).