[
  {
    "index": 0,
    "papers": [
      {
        "key": "ref:li2021prefix",
        "author": "Li, Xiang Lisa  and\nLiang, Percy",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      },
      {
        "key": "ref:qin2021learning",
        "author": "Qin, Guanghui  and\nEisner, Jason",
        "title": "Learning How to Ask: Querying {LM}s with Mixtures of Soft Prompts"
      },
      {
        "key": "ref:prasad2023grips",
        "author": "Prasad, Archiki  and\nHase, Peter  and\nZhou, Xiang  and\nBansal, Mohit",
        "title": "{G}r{IPS}: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ref:liu2023hierarchical",
        "author": "Liu, Yajing and Lu, Yuning and Liu, Hao and An, Yaozu and Xu, Zhuoran and Yao, Zhuokun and Zhang, Baofeng and Xiong, Zhiwei and Gui, Chenguang",
        "title": "Hierarchical prompt learning for multi-task learning"
      },
      {
        "key": "ref:tian2024argue",
        "author": "Tian, Xinyu and Zou, Shu and Yang, Zhaoyuan and Zhang, Jing",
        "title": "ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models"
      },
      {
        "key": "ref:kim2024aapl",
        "author": "Kim, Gahyeon and Kim, Sohee and Lee, Seokju",
        "title": "AAPL: Adding Attributes to Prompt Learning for Vision-Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ref:xu2025mixture",
        "author": "Bowen Xu and Shaoyu Wu and Kai Liu and Lulu Hu",
        "title": "Mixture-of-Instructions: Aligning Large Language Models via Mixture Prompting"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ref:ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "ref:rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ref:dong2023abilities",
        "author": "Dong, Guanting  and\nYuan, Hongyi  and\nLu, Keming  and\nLi, Chengpeng  and\nXue, Mingfeng  and\nLiu, Dayiheng  and\nWang, Wei  and\nYuan, Zheng  and\nZhou, Chang  and\nZhou, Jingren",
        "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"
      },
      {
        "key": "ref:kotha2024understanding",
        "author": "Suhas Kotha and Jacob Mitchell Springer and Aditi Raghunathan",
        "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference"
      },
      {
        "key": "ref:biderman2024lora",
        "author": "Dan Biderman and Jacob Portes and Jose Javier Gonzalez Ortiz and Mansheej Paul and Philip Greengard and Connor Jennings and Daniel King and Sam Havens and Vitaliy Chiley and Jonathan Frankle and Cody Blakeney and John Patrick Cunningham",
        "title": "Lo{RA} Learns Less and Forgets Less"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ref:liu2024mftcoder",
        "author": "Liu, Bingchang and Chen, Chaoyu and Gong, Zi and Liao, Cong and Wang, Huan and Lei, Zhichao and Liang, Ming and Chen, Dajun and Shen, Min and Zhou, Hailian and others",
        "title": "Mftcoder: Boosting code llms with multitask fine-tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ref:wu2023finegrained",
        "author": "Zeqiu Wu and Yushi Hu and Weijia Shi and Nouha Dziri and Alane Suhr and Prithviraj Ammanabrolu and Noah A. Smith and Mari Ostendorf and Hannaneh Hajishirzi",
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"
      },
      {
        "key": "ref:zhang2024bi",
        "author": "Wenxuan Zhang and Philip Torr and Mohamed Elhoseiny and Adel Bibi",
        "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models"
      },
      {
        "key": "ref:yang2024metaaligner",
        "author": "Yang, Kailai and Liu, Zhiwei and Xie, Qianqian and Huang, Jimin and Zhang, Tianlin and Ananiadou, Sophia",
        "title": "Metaaligner: Towards generalizable multi-objective alignment of language models"
      },
      {
        "key": "ref:wang2024arithmetic",
        "author": "Wang, Haoxiang  and\nLin, Yong  and\nXiong, Wei  and\nYang, Rui  and\nDiao, Shizhe  and\nQiu, Shuang  and\nZhao, Han  and\nZhang, Tong",
        "title": "Arithmetic Control of {LLM}s for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"
      },
      {
        "key": "ref:yang2024rewards",
        "author": "Yang, Rui and Pan, Xiaoman and Luo, Feng and Qiu, Shuang and Zhong, Han and Yu, Dong and Chen, Jianshu",
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"
      },
      {
        "key": "ref:wang2024interpretable",
        "author": "Wang, Haoxiang  and\nXiong, Wei  and\nXie, Tengyang  and\nZhao, Han  and\nZhang, Tong",
        "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"
      }
    ]
  }
]