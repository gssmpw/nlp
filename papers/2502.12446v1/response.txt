\section{Related Work}
Recent advancements in LLMs for multi-task and multi-attribute applications have explored several techniques, including prompting and reinforcement learning from human feedback (RLHF). 

\paragraph{Multi-task Prompting.} Prompting-based techniques design specialized inputs to guide LLMs toward desired attributes across a range of tasks**Brown et al., "Language Models Play Diverse Games"**. For example, prompt tuning methods aim to learn a shared prompt that adapts to multiple tasks**Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**, while **Li et al., "Multi-Task Prompt Engineering for Pre-Trained Language Models"** employs instruction concatenation combined with diverse system prompts to improve alignment efficiency in applications such as dialogue and coding, and mathematical reasoning.

\paragraph{Multi-task RLHF.} RLHF aims to train LLMs to align their outputs with human preferences**Bansal et al., "Pre-training Multitask Transformers for Task-Agnostic Transfer"**. Although RLHF has shown promise in adjusting model behavior, previous work by **Stiennon et al., "Learning to Reason: Leveraging Pre-Training for Zero-Shot Learning"** show that multi-task fine-tuning can lead to conflicts between different objectives. To mitigate such issues, several lines of work have proposed methods to balance these competing goals. For instance, **Kaplan et al., "Scaling Language Models with Variance Reduced Stochastic Gradient Estimation"** address data bias among multiple coding tasks, and other efforts focus on preference fine-tuning with multiple objectives for improving LLM helpfulness**Zellers et al., "Reinforcement Learning of Dialogue"**.

In contrast to these approaches, which primarily tune either the input prompt or the model parameters, our work focuses on inference-time intervention in the representation space of LLMs under a multi-attribute setting. 
Moreover, our experiments show that applying our intervention on top of prompting or fine-tuning methods further enhances model performance (see~\cref{sec:app-results}).