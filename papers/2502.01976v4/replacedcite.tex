\section{Related Work}
\label{sec:related}
\vspace{-1em}

In this section, we conduct a literature review that mainly focuses on prior LLM inference acceleration methods, especially those that involve using routing mechanisms and collaborative inference between LLMs for inference acceleration.
\vspace{-1em}
\paragraph{Query-Level Routing Mechanisms.} Previous routing methods____ for efficient inference mainly focus on routing entire user queries to different models for generation. For example, Routoo____ proposes a performance predictor and a cost-aware decoder to route between LLMs, considering both performance and resource constraints; Hybird LLM____ proposes a probabilistic router to select LLM backend for each query; RouteLLM____ formulates the routing problem as a classification problem and employs a data augmentation framework to significantly expand the dataset used for training the router.
However, as highlighted in Section~\ref{sec:intro}, routing at the query-level granularity may lead to suboptimal performance, as non-critical tokens in complex queries may be generated inefficiently, while critical tokens in simple queries may suffer from inaccuracy. In contrast, token-level routing methods offer more fine-grained control over the routing process, improving both inference costs and the quality of the generated response.

\vspace{-1em}
\paragraph{Token-Level Routing Mechanisms.} Unlike query-level routing methods, previous token-level routing methods____ mainly focus on routing input tokens to different specialized experts to enhance performance without considering the inference costs. For example, Arrow____ build a mixture-of-experts (MoE) architecture with multiple LoRAs, dynamically routing inputs to different LoRAs during inference.
Among these methods, Co-LLM____ is the most relevant to our framework CITER, introducing a router to route tokens to models of different sizes. However, Co-LLM only considers the current outputs from SLM and LLM when generating ground truth labels to train the router. This may lead to suboptimal performance since the influence of current decisions on future tokens is not considered. Moreover, similar to other token-level routing methods, Co-LLM focuses on enhanced response quality without taking the inference costs of the inference process into account. In contrast, our CITER framework considers both the current token and the future impact of each decision, enabling more accurate and efficient routing.
\vspace{-1em}
\paragraph{Other Methods for LLM Inference Acceleration.}
In addition to routing methods, several approaches ranging from algorithmic to system optimizations____ have been proposed to accelerate LLM inference. Speculative Decoding____ employs a small draft model to generate potential next tokens, which are concatenated with previously generated tokens. These guesses are then processed by the target LLM in parallel to verify their correctness. Tokens are only committed to the final output if confirmed by the target LLM. Although this approach reduces inference time by generating multiple tokens in a single forward pass, it does not lower the overall computational complexity (e.g., the total amount of FLOPs). Speculative Streaming____ addresses the computational overhead of Speculative Decoding by predicting n-grams instead of individual tokens in each forward pass. However, it requires redesigning the LLM architecture, necessitating re-pretraining, which is computationally prohibitive for many use cases. Medusa____ mitigates the re-pretraining issue by adding auxiliary heads to the original LLM, allowing n-gram predictions without modifying the core model. These heads can be trained while keeping the original LLM frozen, thereby avoiding the need for re-pretraining. SpecInfer and Sequoia____ leverage tree-based parallelism for decoding and verification to further accelerate inference.

\vspace{-1em}