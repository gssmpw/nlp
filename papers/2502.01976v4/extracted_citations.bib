@misc{belofsky2023tokenleveladaptationloraadapters,
  archiveprefix = {arXiv},
  author        = {Joshua Belofsky},
  eprint        = {2311.10847},
  primaryclass  = {cs.CL},
  title         = {Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization},
  url           = {https://arxiv.org/abs/2311.10847},
  year          = {2023}
}

@misc{bhendawade2024speculativestreamingfastllm,
  archiveprefix = {arXiv},
  author        = {Nikhil Bhendawade and Irina Belousova and Qichen Fu and Henry Mason and Mohammad Rastegari and Mahyar Najibi},
  eprint        = {2402.11131},
  primaryclass  = {cs.CL},
  title         = {Speculative Streaming: Fast LLM Inference without Auxiliary Models},
  url           = {https://arxiv.org/abs/2402.11131},
  year          = {2024}
}

@article{cai2024medusa,
  author  = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
  journal = {arXiv preprint arXiv: 2401.10774},
  title   = {Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},
  year    = {2024}
}

@article{chen2023accelerating,
  author  = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal = {arXiv preprint arXiv:2302.01318},
  title   = {Accelerating large language model decoding with speculative sampling},
  year    = {2023}
}

@article{chen2023frugalgpt,
  author  = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal = {arXiv preprint arXiv:2305.05176},
  title   = {Frugalgpt: How to use large language models while reducing cost and improving performance},
  year    = {2023}
}

@article{chen2024magicdec,
  author  = {Chen, Jian and Tiwari, Vashisth and Sadhukhan, Ranajoy and Chen, Zhuoming and Shi, Jinyuan and Yen, Ian En-Hsu and Chen, Beidi},
  journal = {arXiv preprint arXiv:2408.11049},
  title   = {MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding},
  year    = {2024}
}

@article{chen2024sequoia,
  author  = {Chen, Zhuoming and May, Avner and Svirschevski, Ruslan and Huang, Yuhsun and Ryabinin, Max and Jia, Zhihao and Chen, Beidi},
  journal = {arXiv preprint arXiv:2402.12374},
  title   = {Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding},
  year    = {2024}
}

@misc{cheng2024damdynamicadaptermerging,
  archiveprefix = {arXiv},
  author        = {Feng Cheng and Ziyang Wang and Yi-Lin Sung and Yan-Bo Lin and Mohit Bansal and Gedas Bertasius},
  eprint        = {2403.08755},
  primaryclass  = {cs.CV},
  title         = {DAM: Dynamic Adapter Merging for Continual Video QA Learning},
  url           = {https://arxiv.org/abs/2403.08755},
  year          = {2024}
}

@misc{chronopoulou2023adaptersoupweightaveragingimprove,
  archiveprefix = {arXiv},
  author        = {Alexandra Chronopoulou and Matthew E. Peters and Alexander Fraser and Jesse Dodge},
  eprint        = {2302.07027},
  primaryclass  = {cs.CL},
  title         = {AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models},
  url           = {https://arxiv.org/abs/2302.07027},
  year          = {2023}
}

@misc{diao2023mixtureofdomainadaptersdecouplinginjectingdomain,
  archiveprefix = {arXiv},
  author        = {Shizhe Diao and Tianyang Xu and Ruijia Xu and Jiawei Wang and Tong Zhang},
  eprint        = {2306.05406},
  primaryclass  = {cs.CL},
  title         = {Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories},
  url           = {https://arxiv.org/abs/2306.05406},
  year          = {2023}
}

@misc{ding2024hybridllmcostefficientqualityaware,
      title={Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing}, 
      author={Dujian Ding and Ankur Mallick and Chi Wang and Robert Sim and Subhabrata Mukherjee and Victor Ruhle and Laks V. S. Lakshmanan and Ahmed Hassan Awadallah},
      year={2024},
      eprint={2404.14618},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.14618}, 
}

@inproceedings{jang2023exploring,
  author       = {Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  booktitle    = {International Conference on Machine Learning},
  organization = {PMLR},
  pages        = {14702--14729},
  title        = {Exploring the benefits of training expert language models over instruction tuning},
  year         = {2023}
}

@inproceedings{kwon2023efficient,
  author    = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages     = {611--626},
  title     = {Efficient memory management for large language model serving with pagedattention},
  year      = {2023}
}

@misc{leviathan2023fastinferencetransformersspeculative,
  archiveprefix = {arXiv},
  author        = {Yaniv Leviathan and Matan Kalman and Yossi Matias},
  eprint        = {2211.17192},
  primaryclass  = {cs.LG},
  title         = {Fast Inference from Transformers via Speculative Decoding},
  url           = {https://arxiv.org/abs/2211.17192},
  year          = {2023}
}

@article{lu2023routing,
  author  = {Lu, Keming and Yuan, Hongyi and Lin, Runji and Lin, Junyang and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal = {arXiv preprint arXiv:2311.08692},
  title   = {Routing to the expert: Efficient reward-guided ensemble of large language models},
  year    = {2023}
}

@misc{lu2024twinmergingdynamicintegrationmodular,
  archiveprefix = {arXiv},
  author        = {Zhenyi Lu and Chenghao Fan and Wei Wei and Xiaoye Qu and Dangyang Chen and Yu Cheng},
  eprint        = {2406.15479},
  primaryclass  = {cs.CL},
  title         = {Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging},
  url           = {https://arxiv.org/abs/2406.15479},
  year          = {2024}
}

@article{miao2023specinfer,
  author  = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  journal = {arXiv preprint arXiv:2305.09781},
  title   = {SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification},
  year    = {2023}
}

@misc{mohammadshahi2024routoolearningroutelarge,
  archiveprefix = {arXiv},
  author        = {Alireza Mohammadshahi and Arshad Rafiq Shaikh and Majid Yazdani},
  eprint        = {2401.13979},
  primaryclass  = {cs.CL},
  title         = {Routoo: Learning to Route to Large Language Models Effectively},
  url           = {https://arxiv.org/abs/2401.13979},
  year          = {2024}
}

@misc{muqeeth2024learningroutespecializedexperts,
  archiveprefix = {arXiv},
  author        = {Mohammed Muqeeth and Haokun Liu and Yufan Liu and Colin Raffel},
  eprint        = {2402.05859},
  primaryclass  = {cs.LG},
  title         = {Learning to Route Among Specialized Experts for Zero-Shot Generalization},
  url           = {https://arxiv.org/abs/2402.05859},
  year          = {2024}
}

@misc{ong2024routellm,
  archiveprefix = {arXiv},
  author        = {Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
  eprint        = {2406.18665},
  primaryclass  = {cs.LG},
  title         = {RouteLLM: Learning to Route LLMs with Preference Data},
  url           = {https://arxiv.org/abs/2406.18665},
  year          = {2024}
}

@misc{ostapenko2024modularllmsbuildingreusing,
  archiveprefix = {arXiv},
  author        = {Oleksiy Ostapenko and Zhan Su and Edoardo Maria Ponti and Laurent Charlin and Nicolas Le Roux and Matheus Pereira and Lucas Caccia and Alessandro Sordoni},
  eprint        = {2405.11157},
  primaryclass  = {cs.LG},
  title         = {Towards Modular LLMs by Building and Reusing a Library of LoRAs},
  url           = {https://arxiv.org/abs/2405.11157},
  year          = {2024}
}

@inproceedings{pfeiffer2021adapterfusion,
  abstract  = {Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.},
  address   = {Online},
  author    = {Pfeiffer, Jonas  and
               Kamath, Aishwarya  and
               R{\"u}ckl{\'e}, Andreas  and
               Cho, Kyunghyun  and
               Gurevych, Iryna},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  doi       = {10.18653/v1/2021.eacl-main.39},
  editor    = {Merlo, Paola  and
               Tiedemann, Jorg  and
               Tsarfaty, Reut},
  month     = apr,
  pages     = {487--503},
  publisher = {Association for Computational Linguistics},
  title     = {{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning},
  url       = {https://aclanthology.org/2021.eacl-main.39},
  year      = {2021}
}

@article{shen2024learning,
  author  = {Shen, Shannon Zejiang and Lang, Hunter and Wang, Bailin and Kim, Yoon and Sontag, David},
  journal = {arXiv preprint arXiv:2403.03870},
  title   = {Learning to Decode Collaboratively with Multiple Language Models},
  year    = {2024}
}

@inproceedings{srivatsa-etal-2024-harnessing,
    title = "Harnessing the Power of Multiple Minds: Lessons Learned from {LLM} Routing",
    author = "Srivatsa, Kv Aditya  and
      Maurya, Kaushal  and
      Kochmar, Ekaterina",
    editor = "Tafreshi, Shabnam  and
      Akula, Arjun  and
      Sedoc, Jo{\~a}o  and
      Drozd, Aleksandr  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the Fifth Workshop on Insights from Negative Results in NLP",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.insights-1.15/",
    doi = "10.18653/v1/2024.insights-1.15",
    pages = "124--134",
    abstract = "With the rapid development of LLMs, it is natural to ask how to harness their capabilities efficiently. In this paper, we explore whether it is feasible to direct each input query to a single most suitable LLM. To this end, we propose LLM routing for challenging reasoning tasks. Our extensive experiments suggest that such routing shows promise but is not feasible in all scenarios, so more robust approaches should be investigated to fill this gap."
}

@inproceedings{stripelis-etal-2024-tensoropera,
    title = "{T}ensor{O}pera Router: A Multi-Model Router for Efficient {LLM} Inference",
    author = "Stripelis, Dimitris  and
      Xu, Zhaozhuo  and
      Hu, Zijian  and
      Shah, Alay Dilipbhai  and
      Jin, Han  and
      Yao, Yuhang  and
      Zhang, Jipeng  and
      Zhang, Tong  and
      Avestimehr, Salman  and
      He, Chaoyang",
    editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-industry.34/",
    doi = "10.18653/v1/2024.emnlp-industry.34",
    pages = "452--462",
    abstract = "With the rapid growth of Large Language Models (LLMs) across various domains, numerous new LLMs have emerged, each possessing domain-specific expertise. This proliferation has highlighted the need for quick, high-quality, and cost-effective LLM query response methods. Yet, no single LLM exists to efficiently balance this trilemma. Some models are powerful but extremely costly, while others are fast and inexpensive but qualitatively inferior. To address this challenge, we present TO-Router, a non-monolithic LLM querying system that seamlessly integrates various LLM experts into a single query interface and dynamically routes incoming queries to the most high-performant expert based on query`s requirements. Through extensive experiments, we demonstrate that when compared to standalone expert models, TO-Router improves query efficiency by up to 40{\%}, and leads to significant cost reductions of up to 30{\%}, while maintaining or enhancing model performance by up to 10{\%}."
}

@inproceedings{wang2024fusing,
  author    = {Hongyi Wang and Felipe Maia Polo and Yuekai Sun and Souvik Kundu and Eric Xing and Mikhail Yurochkin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Fusing Models with Complementary Expertise},
  year      = {2024}
}

@misc{wang2024loraflowdynamiclorafusion,
  archiveprefix = {arXiv},
  author        = {Hanqing Wang and Bowen Ping and Shuo Wang and Xu Han and Yun Chen and Zhiyuan Liu and Maosong Sun},
  eprint        = {2402.11455},
  primaryclass  = {cs.CL},
  title         = {LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks},
  url           = {https://arxiv.org/abs/2402.11455},
  year          = {2024}
}

@misc{wu2024mixtureloraexperts,
  archiveprefix = {arXiv},
  author        = {Xun Wu and Shaohan Huang and Furu Wei},
  eprint        = {2404.13628},
  primaryclass  = {cs.CL},
  title         = {Mixture of LoRA Experts},
  url           = {https://arxiv.org/abs/2404.13628},
  year          = {2024}
}

@misc{xu2024meteoramultipletasksembeddedlora,
  archiveprefix = {arXiv},
  author        = {Jingwei Xu and Junyu Lai and Yunpeng Huang},
  eprint        = {2405.13053},
  primaryclass  = {cs.CL},
  title         = {MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models},
  url           = {https://arxiv.org/abs/2405.13053},
  year          = {2024}
}

