\section{Related Work}
\label{sec:related}
\vspace{-1em}

In this section, we conduct a literature review that mainly focuses on prior LLM inference acceleration methods, especially those that involve using routing mechanisms and collaborative inference between LLMs for inference acceleration.
\vspace{-1em}
\paragraph{Query-Level Routing Mechanisms.} Previous routing methods~\citep{jang2023exploring,chronopoulou2023adaptersoupweightaveragingimprove,diao2023mixtureofdomainadaptersdecouplinginjectingdomain,lu2023routing,cheng2024damdynamicadaptermerging,lu2024twinmergingdynamicintegrationmodular,chen2023frugalgpt,wang2024fusing,srivatsa-etal-2024-harnessing,stripelis-etal-2024-tensoropera} for efficient inference mainly focus on routing entire user queries to different models for generation. For example, Routoo~\citep{mohammadshahi2024routoolearningroutelarge} proposes a performance predictor and a cost-aware decoder to route between LLMs, considering both performance and resource constraints; Hybird LLM~\citep{ding2024hybridllmcostefficientqualityaware} proposes a probabilistic router to select LLM backend for each query; RouteLLM~\citep{ong2024routellm} formulates the routing problem as a classification problem and employs a data augmentation framework to significantly expand the dataset used for training the router.
However, as highlighted in Section~\ref{sec:intro}, routing at the query-level granularity may lead to suboptimal performance, as non-critical tokens in complex queries may be generated inefficiently, while critical tokens in simple queries may suffer from inaccuracy. In contrast, token-level routing methods offer more fine-grained control over the routing process, improving both inference costs and the quality of the generated response.

\vspace{-1em}
\paragraph{Token-Level Routing Mechanisms.} Unlike query-level routing methods, previous token-level routing methods~\citep{pfeiffer2021adapterfusion,belofsky2023tokenleveladaptationloraadapters,muqeeth2024learningroutespecializedexperts,wang2024loraflowdynamiclorafusion,wu2024mixtureloraexperts,xu2024meteoramultipletasksembeddedlora} mainly focus on routing input tokens to different specialized experts to enhance performance without considering the inference costs. For example, Arrow~\citep{ostapenko2024modularllmsbuildingreusing} build a mixture-of-experts (MoE) architecture with multiple LoRAs, dynamically routing inputs to different LoRAs during inference.
Among these methods, Co-LLM~\citep{shen2024learning} is the most relevant to our framework CITER, introducing a router to route tokens to models of different sizes. However, Co-LLM only considers the current outputs from SLM and LLM when generating ground truth labels to train the router. This may lead to suboptimal performance since the influence of current decisions on future tokens is not considered. Moreover, similar to other token-level routing methods, Co-LLM focuses on enhanced response quality without taking the inference costs of the inference process into account. In contrast, our CITER framework considers both the current token and the future impact of each decision, enabling more accurate and efficient routing.
\vspace{-1em}
\paragraph{Other Methods for LLM Inference Acceleration.}
In addition to routing methods, several approaches ranging from algorithmic to system optimizations~\citep{miao2023specinfer,kwon2023efficient,chen2024magicdec} have been proposed to accelerate LLM inference. Speculative Decoding~\citep{leviathan2023fastinferencetransformersspeculative,chen2023accelerating} employs a small draft model to generate potential next tokens, which are concatenated with previously generated tokens. These guesses are then processed by the target LLM in parallel to verify their correctness. Tokens are only committed to the final output if confirmed by the target LLM. Although this approach reduces inference time by generating multiple tokens in a single forward pass, it does not lower the overall computational complexity (e.g., the total amount of FLOPs). Speculative Streaming~\citep{bhendawade2024speculativestreamingfastllm} addresses the computational overhead of Speculative Decoding by predicting n-grams instead of individual tokens in each forward pass. However, it requires redesigning the LLM architecture, necessitating re-pretraining, which is computationally prohibitive for many use cases. Medusa~\citep{cai2024medusa} mitigates the re-pretraining issue by adding auxiliary heads to the original LLM, allowing n-gram predictions without modifying the core model. These heads can be trained while keeping the original LLM frozen, thereby avoiding the need for re-pretraining. SpecInfer and Sequoia~\citep{miao2023specinfer,chen2024sequoia} leverage tree-based parallelism for decoding and verification to further accelerate inference.

\vspace{-1em}