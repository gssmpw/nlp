@inproceedings{langley00,
  address   = {Stanford, CA},
  author    = {P. Langley},
  booktitle = {Proceedings of the 17th International Conference
               on Machine Learning (ICML 2000)},
  editor    = {Pat Langley},
  pages     = {1207--1216},
  publisher = {Morgan Kaufmann},
  title     = {Crafting Papers on Machine Learning},
  year      = {2000}
}

@techreport{mitchell80,
  address     = {New Brunswick, MA},
  author      = {T. M. Mitchell},
  institution = {Computer Science Department, Rutgers University},
  title       = {The Need for Biases in Learning Generalizations},
  year        = {1980}
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  school = {Department of Computer Science, Harvard University},
  title  = {Computational Complexity of Machine Learning},
  year   = {1989}
}

@book{MachineLearningI,
  address   = {Palo Alto, CA},
  editor    = {R. S. Michalski and J. G. Carbonell and T.
               M. Mitchell},
  publisher = {Tioga},
  title     = {Machine Learning: An Artificial Intelligence
               Approach, Vol. I},
  year      = {1983}
}

@book{DudaHart2nd,
  author    = {R. O. Duda and P. E. Hart and D. G. Stork},
  edition   = {2nd},
  publisher = {John Wiley and Sons},
  title     = {Pattern Classification},
  year      = {2000}
}

@misc{anonymous,
  author = {Author, N. N.},
  title  = {Suppressed for Anonymity},
  year   = {2021}
}

@incollection{Newell81,
  address   = {Hillsdale, NJ},
  author    = {A. Newell and P. S. Rosenbloom},
  booktitle = {Cognitive Skills and Their Acquisition},
  chapter   = {1},
  editor    = {J. R. Anderson},
  pages     = {1--51},
  publisher = {Lawrence Erlbaum Associates, Inc.},
  title     = {Mechanisms of Skill Acquisition and the Law of
               Practice},
  year      = {1981}
}


@article{Samuel59,
  author  = {A. L. Samuel},
  journal = {IBM Journal of Research and Development},
  number  = {3},
  pages   = {211--229},
  title   = {Some Studies in Machine Learning Using the Game of
             Checkers},
  volume  = {3},
  year    = {1959}
}

@article{chen2024magicdec,
  author  = {Chen, Jian and Tiwari, Vashisth and Sadhukhan, Ranajoy and Chen, Zhuoming and Shi, Jinyuan and Yen, Ian En-Hsu and Chen, Beidi},
  journal = {arXiv preprint arXiv:2408.11049},
  title   = {MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding},
  year    = {2024}
}

@article{chen2024sequoia,
  author  = {Chen, Zhuoming and May, Avner and Svirschevski, Ruslan and Huang, Yuhsun and Ryabinin, Max and Jia, Zhihao and Chen, Beidi},
  journal = {arXiv preprint arXiv:2402.12374},
  title   = {Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding},
  year    = {2024}
}
@article{bradley1952rank,
  author    = {Bradley, Ralph Allan and Terry, Milton E},
  journal   = {Biometrika},
  number    = {3/4},
  pages     = {324--345},
  publisher = {JSTOR},
  title     = {Rank analysis of incomplete block designs: I. The method of paired comparisons},
  volume    = {39},
  year      = {1952}
}
@article{miao2023specinfer,
  author  = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  journal = {arXiv preprint arXiv:2305.09781},
  title   = {SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification},
  year    = {2023}
}

@article{chen2023accelerating,
  author  = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal = {arXiv preprint arXiv:2302.01318},
  title   = {Accelerating large language model decoding with speculative sampling},
  year    = {2023}
}

@inproceedings{kwon2023efficient,
  author    = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages     = {611--626},
  title     = {Efficient memory management for large language model serving with pagedattention},
  year      = {2023}
}

@article{chen2023frugalgpt,
  author  = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal = {arXiv preprint arXiv:2305.05176},
  title   = {Frugalgpt: How to use large language models while reducing cost and improving performance},
  year    = {2023}
}

@inproceedings{wang2024fusing,
  author    = {Hongyi Wang and Felipe Maia Polo and Yuekai Sun and Souvik Kundu and Eric Xing and Mikhail Yurochkin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Fusing Models with Complementary Expertise},
  year      = {2024}
}

@incollection{Bengio+chapter2007,
  author    = {Bengio, Yoshua and LeCun, Yann},
  booktitle = {Large Scale Kernel Machines},
  publisher = {MIT Press},
  title     = {Scaling Learning Algorithms Towards {AI}},
  year      = {2007}
}

@article{hunter2004mm,
  author    = {Hunter, David R},
  journal   = {The annals of statistics},
  number    = {1},
  pages     = {384--406},
  publisher = {Institute of Mathematical Statistics},
  title     = {MM algorithms for generalized Bradley-Terry models},
  volume    = {32},
  year      = {2004}
}

@article{Hinton06,
  author  = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
  journal = {Neural Computation},
  pages   = {1527--1554},
  title   = {A Fast Learning Algorithm for Deep Belief Nets},
  volume  = {18},
  year    = {2006}
}

@book{goodfellow2016deep,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  publisher = {MIT Press},
  title     = {Deep learning},
  volume    = {1},
  year      = {2016}
}

@inproceedings{talmor2019commonsenseqa,
  abstract  = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.},
  address   = {Minneapolis, Minnesota},
  author    = {Talmor, Alon  and
               Herzig, Jonathan  and
               Lourie, Nicholas  and
               Berant, Jonathan},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1421},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  month     = jun,
  pages     = {4149--4158},
  publisher = {Association for Computational Linguistics},
  title     = {{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge},
  url       = {https://aclanthology.org/N19-1421},
  year      = {2019}
}

@article{peter2018arc,
  author  = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
             Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  journal = {arXiv:1803.05457v1},
  title   = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  year    = {2018}
}

@article{cobbe2021gsm8k,
  author  = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal = {arXiv preprint arXiv:2110.14168},
  title   = {Training Verifiers to Solve Math Word Problems},
  year    = {2021}
}

@article{hendryckstest2021mmlu,
  author  = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title   = {Measuring Massive Multitask Language Understanding},
  year    = {2021}
}

@article{hendrycks2021ethics,
  author  = {Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title   = {Aligning AI With Shared Human Values},
  year    = {2021}
}

@misc{ong2024routellm,
  archiveprefix = {arXiv},
  author        = {Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
  eprint        = {2406.18665},
  primaryclass  = {cs.LG},
  title         = {RouteLLM: Learning to Route LLMs with Preference Data},
  url           = {https://arxiv.org/abs/2406.18665},
  year          = {2024}
}

@article{shen2024learning,
  author  = {Shen, Shannon Zejiang and Lang, Hunter and Wang, Bailin and Kim, Yoon and Sontag, David},
  journal = {arXiv preprint arXiv:2403.03870},
  title   = {Learning to Decode Collaboratively with Multiple Language Models},
  year    = {2024}
}

@article{coleman2024llm,
  author  = {Coleman, Jared and Krishnamachari, Bhaskar and Iskarous, Khalil and Rosales, Ruben},
  journal = {arXiv preprint arXiv:2405.08997},
  title   = {LLM-Assisted Rule Based Machine Translation for Low/No-Resource Languages},
  year    = {2024}
}

@article{zheng2024multimodal,
  author  = {Zheng, Wenhao and Peng, Dongsheng and Xu, Hongxia and Zhu, Hongtu and Fu, Tianfan and Yao, Huaxiu},
  journal = {arXiv preprint arXiv:2402.06512},
  title   = {Multimodal clinical trial outcome prediction with large language models},
  year    = {2024}
}

@inproceedings{kamallo2024towards,
  abstract  = {Instruction-tuned large language models (LLMs) have been shown to be viable surrogates for the widely used, albeit overly rigid, lexical matching metrics in evaluating question answering (QA) models. However, these LLM-based evaluation methods are invariably based on proprietary LLMs. Despite their remarkable capabilities, proprietary LLMs are costly and subject to internal changes that can affect their output, which inhibits the reproducibility of their results and limits the widespread adoption of LLM-based evaluation. In this demo, we aim to use publicly available LLMs for standardizing LLM-based QA evaluation. However, open-source LLMs lag behind their proprietary counterparts. We overcome this gap by adopting chain-of-thought prompting with self-consistency to build a reliable evaluation framework. We demonstrate that our evaluation framework, based on 750M and 7B open LLMs, correlates competitively with human judgment, compared to most recent GPT-3 and GPT-4 models. Our codebase and data are available at https://github.com/castorini/qa-eval.},
  address   = {New York, NY, USA},
  author    = {Kamalloo, Ehsan and Upadhyay, Shivani and Lin, Jimmy},
  booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi       = {10.1145/3626772.3657675},
  isbn      = {9798400704314},
  keywords  = {evaluation, large language models, question answering},
  location  = {Washington DC, USA},
  numpages  = {6},
  pages     = {2811–2816},
  publisher = {Association for Computing Machinery},
  series    = {SIGIR '24},
  title     = {Towards Robust QA Evaluation via Open LLMs},
  url       = {https://doi.org/10.1145/3626772.3657675},
  year      = {2024}
}

@misc{leviathan2023fastinferencetransformersspeculative,
  archiveprefix = {arXiv},
  author        = {Yaniv Leviathan and Matan Kalman and Yossi Matias},
  eprint        = {2211.17192},
  primaryclass  = {cs.LG},
  title         = {Fast Inference from Transformers via Speculative Decoding},
  url           = {https://arxiv.org/abs/2211.17192},
  year          = {2023}
}

@article{lu2023routing,
  author  = {Lu, Keming and Yuan, Hongyi and Lin, Runji and Lin, Junyang and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal = {arXiv preprint arXiv:2311.08692},
  title   = {Routing to the expert: Efficient reward-guided ensemble of large language models},
  year    = {2023}
}

@misc{mohammadshahi2024routoolearningroutelarge,
  archiveprefix = {arXiv},
  author        = {Alireza Mohammadshahi and Arshad Rafiq Shaikh and Majid Yazdani},
  eprint        = {2401.13979},
  primaryclass  = {cs.CL},
  title         = {Routoo: Learning to Route to Large Language Models Effectively},
  url           = {https://arxiv.org/abs/2401.13979},
  year          = {2024}
}

@misc{ostapenko2024modularllmsbuildingreusing,
  archiveprefix = {arXiv},
  author        = {Oleksiy Ostapenko and Zhan Su and Edoardo Maria Ponti and Laurent Charlin and Nicolas Le Roux and Matheus Pereira and Lucas Caccia and Alessandro Sordoni},
  eprint        = {2405.11157},
  primaryclass  = {cs.LG},
  title         = {Towards Modular LLMs by Building and Reusing a Library of LoRAs},
  url           = {https://arxiv.org/abs/2405.11157},
  year          = {2024}
}

@misc{sukhbaatar2024branchtrainmixmixingexpertllms,
  archiveprefix = {arXiv},
  author        = {Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozière and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li},
  eprint        = {2403.07816},
  primaryclass  = {cs.CL},
  title         = {Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM},
  url           = {https://arxiv.org/abs/2403.07816},
  year          = {2024}
}

@article{rafailov2024direct,
  author  = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Direct preference optimization: Your language model is secretly a reward model},
  volume  = {36},
  year    = {2024}
}

@inproceedings{wolf2020transformers,
  address   = {Online},
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  pages     = {38--45},
  publisher = {Association for Computational Linguistics},
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  year      = {2020}
}

@article{qwen2,
  title = {Qwen2 Technical Report},
  year  = {2024}
}

@misc{bhendawade2024speculativestreamingfastllm,
  archiveprefix = {arXiv},
  author        = {Nikhil Bhendawade and Irina Belousova and Qichen Fu and Henry Mason and Mohammad Rastegari and Mahyar Najibi},
  eprint        = {2402.11131},
  primaryclass  = {cs.CL},
  title         = {Speculative Streaming: Fast LLM Inference without Auxiliary Models},
  url           = {https://arxiv.org/abs/2402.11131},
  year          = {2024}
}

@article{cai2024medusa,
  author  = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
  journal = {arXiv preprint arXiv: 2401.10774},
  title   = {Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},
  year    = {2024}
}

@misc{dao2022flashattentionfastmemoryefficientexact,
  archiveprefix = {arXiv},
  author        = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
  eprint        = {2205.14135},
  primaryclass  = {cs.LG},
  title         = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  url           = {https://arxiv.org/abs/2205.14135},
  year          = {2022}
}

@misc{sanh2020distilbertdistilledversionbert,
  archiveprefix = {arXiv},
  author        = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  eprint        = {1910.01108},
  primaryclass  = {cs.CL},
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  url           = {https://arxiv.org/abs/1910.01108},
  year          = {2020}
}

@misc{eniser2024translatingrealworldcodellms,
  archiveprefix = {arXiv},
  author        = {Hasan Ferit Eniser and Hanliang Zhang and Cristina David and Meng Wang and Maria Christakis and Brandon Paulsen and Joey Dodds and Daniel Kroening},
  eprint        = {2405.11514},
  primaryclass  = {cs.SE},
  title         = {Towards Translating Real-World Code with LLMs: A Study of Translating to Rust},
  url           = {https://arxiv.org/abs/2405.11514},
  year          = {2024}
}

@article{he2024exploring,
  abstract = {{Large language models (LLMs) have demonstrated impressive capabilities in general
              scenarios, exhibiting a level of aptitude that approaches, in some aspects even
              surpasses, human-level intelligence. Among their numerous skills, the
              translation abilities of LLMs have received considerable attention. Compared to
              typical machine translation that focuses solely on source-to-target mapping,
              LLM-based translation can potentially mimic the human translation process, which
              might take preparatory steps to ensure high-quality translation. This work
              explores this possibility by proposing the MAPS framework, which
              stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the
              given source sentence and induce three aspects of translation-related knowledge
              (keywords, topics, and relevant demonstrations) to guide the final translation
              process. Moreover, we employ a selection mechanism based on quality estimation
              to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs × 11
              directions × 2 automatic metrics) and human evaluation (preference study
              and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by
              mimicking the human translation process, MAPS reduces various translation errors
              such as hallucination, ambiguity, mistranslation, awkward style, untranslated
              text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.}},
  author   = {He, Zhiwei and Liang, Tian and Jiao, Wenxiang and Zhang, Zhuosheng and Yang, Yujiu and Wang, Rui and Tu, Zhaopeng and Shi, Shuming and Wang, Xing},
  doi      = {10.1162/tacl_a_00642},
  eprint   = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00642/2346100/tacl\_a\_00642.pdf},
  issn     = {2307-387X},
  journal  = {Transactions of the Association for Computational Linguistics},
  month    = {03},
  pages    = {229-246},
  title    = {{Exploring Human-Like Translation Strategy with Large Language
              Models}},
  url      = {https://doi.org/10.1162/tacl\_a\_00642},
  volume   = {12},
  year     = {2024}
}

@misc{kou2024cllmsconsistencylargelanguage,
  archiveprefix = {arXiv},
  author        = {Siqi Kou and Lanxiang Hu and Zhezhi He and Zhijie Deng and Hao Zhang},
  eprint        = {2403.00835},
  primaryclass  = {cs.CL},
  title         = {CLLMs: Consistency Large Language Models},
  url           = {https://arxiv.org/abs/2403.00835},
  year          = {2024}
}

@misc{anagnostidis2024dynamiccontextpruningefficient,
  archiveprefix = {arXiv},
  author        = {Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hofmann},
  eprint        = {2305.15805},
  primaryclass  = {cs.CL},
  title         = {Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers},
  url           = {https://arxiv.org/abs/2305.15805},
  year          = {2024}
}

@misc{kingma2017adammethodstochasticoptimization,
  archiveprefix = {arXiv},
  author        = {Diederik P. Kingma and Jimmy Ba},
  eprint        = {1412.6980},
  primaryclass  = {cs.LG},
  title         = {Adam: A Method for Stochastic Optimization},
  url           = {https://arxiv.org/abs/1412.6980},
  year          = {2017}
}

@misc{agarap2019deeplearningusingrectified,
  archiveprefix = {arXiv},
  author        = {Abien Fred Agarap},
  eprint        = {1803.08375},
  primaryclass  = {cs.NE},
  title         = {Deep Learning using Rectified Linear Units (ReLU)},
  url           = {https://arxiv.org/abs/1803.08375},
  year          = {2019}
}

@misc{ioffe2015batchnormalizationacceleratingdeep,
  archiveprefix = {arXiv},
  author        = {Sergey Ioffe and Christian Szegedy},
  eprint        = {1502.03167},
  primaryclass  = {cs.LG},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  url           = {https://arxiv.org/abs/1502.03167},
  year          = {2015}
}

@inproceedings{pfeiffer2021adapterfusion,
  abstract  = {Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.},
  address   = {Online},
  author    = {Pfeiffer, Jonas  and
               Kamath, Aishwarya  and
               R{\"u}ckl{\'e}, Andreas  and
               Cho, Kyunghyun  and
               Gurevych, Iryna},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  doi       = {10.18653/v1/2021.eacl-main.39},
  editor    = {Merlo, Paola  and
               Tiedemann, Jorg  and
               Tsarfaty, Reut},
  month     = apr,
  pages     = {487--503},
  publisher = {Association for Computational Linguistics},
  title     = {{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning},
  url       = {https://aclanthology.org/2021.eacl-main.39},
  year      = {2021}
}

@misc{belofsky2023tokenleveladaptationloraadapters,
  archiveprefix = {arXiv},
  author        = {Joshua Belofsky},
  eprint        = {2311.10847},
  primaryclass  = {cs.CL},
  title         = {Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization},
  url           = {https://arxiv.org/abs/2311.10847},
  year          = {2023}
}

@misc{muqeeth2024learningroutespecializedexperts,
  archiveprefix = {arXiv},
  author        = {Mohammed Muqeeth and Haokun Liu and Yufan Liu and Colin Raffel},
  eprint        = {2402.05859},
  primaryclass  = {cs.LG},
  title         = {Learning to Route Among Specialized Experts for Zero-Shot Generalization},
  url           = {https://arxiv.org/abs/2402.05859},
  year          = {2024}
}

@misc{wang2024loraflowdynamiclorafusion,
  archiveprefix = {arXiv},
  author        = {Hanqing Wang and Bowen Ping and Shuo Wang and Xu Han and Yun Chen and Zhiyuan Liu and Maosong Sun},
  eprint        = {2402.11455},
  primaryclass  = {cs.CL},
  title         = {LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks},
  url           = {https://arxiv.org/abs/2402.11455},
  year          = {2024}
}

@misc{wu2024mixtureloraexperts,
  archiveprefix = {arXiv},
  author        = {Xun Wu and Shaohan Huang and Furu Wei},
  eprint        = {2404.13628},
  primaryclass  = {cs.CL},
  title         = {Mixture of LoRA Experts},
  url           = {https://arxiv.org/abs/2404.13628},
  year          = {2024}
}

@misc{xu2024meteoramultipletasksembeddedlora,
  archiveprefix = {arXiv},
  author        = {Jingwei Xu and Junyu Lai and Yunpeng Huang},
  eprint        = {2405.13053},
  primaryclass  = {cs.CL},
  title         = {MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models},
  url           = {https://arxiv.org/abs/2405.13053},
  year          = {2024}
}

@inproceedings{jang2023exploring,
  author       = {Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  booktitle    = {International Conference on Machine Learning},
  organization = {PMLR},
  pages        = {14702--14729},
  title        = {Exploring the benefits of training expert language models over instruction tuning},
  year         = {2023}
}

@misc{chronopoulou2023adaptersoupweightaveragingimprove,
  archiveprefix = {arXiv},
  author        = {Alexandra Chronopoulou and Matthew E. Peters and Alexander Fraser and Jesse Dodge},
  eprint        = {2302.07027},
  primaryclass  = {cs.CL},
  title         = {AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models},
  url           = {https://arxiv.org/abs/2302.07027},
  year          = {2023}
}

@misc{diao2023mixtureofdomainadaptersdecouplinginjectingdomain,
  archiveprefix = {arXiv},
  author        = {Shizhe Diao and Tianyang Xu and Ruijia Xu and Jiawei Wang and Tong Zhang},
  eprint        = {2306.05406},
  primaryclass  = {cs.CL},
  title         = {Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories},
  url           = {https://arxiv.org/abs/2306.05406},
  year          = {2023}
}

@misc{cheng2024damdynamicadaptermerging,
  archiveprefix = {arXiv},
  author        = {Feng Cheng and Ziyang Wang and Yi-Lin Sung and Yan-Bo Lin and Mohit Bansal and Gedas Bertasius},
  eprint        = {2403.08755},
  primaryclass  = {cs.CV},
  title         = {DAM: Dynamic Adapter Merging for Continual Video QA Learning},
  url           = {https://arxiv.org/abs/2403.08755},
  year          = {2024}
}

@misc{lu2024twinmergingdynamicintegrationmodular,
  archiveprefix = {arXiv},
  author        = {Zhenyi Lu and Chenghao Fan and Wei Wei and Xiaoye Qu and Dangyang Chen and Yu Cheng},
  eprint        = {2406.15479},
  primaryclass  = {cs.CL},
  title         = {Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging},
  url           = {https://arxiv.org/abs/2406.15479},
  year          = {2024}
}

@book{bellman1957dynamic,
  author    = {Bellman, Richard},
  publisher = {Princeton University Press},
  title     = {Dynamic Programming},
  year      = {1957}
}

@inproceedings{rafailov2023direct,
  author    = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  volume    = {36},
  year      = {2023}
}

@article{hendrycksmath2021,
  author  = {Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal = {NeurIPS},
  title   = {Measuring Mathematical Problem Solving With the MATH Dataset},
  year    = {2021}
}

@misc{ding2024hybridllmcostefficientqualityaware,
      title={Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing}, 
      author={Dujian Ding and Ankur Mallick and Chi Wang and Robert Sim and Subhabrata Mukherjee and Victor Ruhle and Laks V. S. Lakshmanan and Ahmed Hassan Awadallah},
      year={2024},
      eprint={2404.14618},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.14618}, 
}

@inproceedings{srivatsa-etal-2024-harnessing,
    title = "Harnessing the Power of Multiple Minds: Lessons Learned from {LLM} Routing",
    author = "Srivatsa, Kv Aditya  and
      Maurya, Kaushal  and
      Kochmar, Ekaterina",
    editor = "Tafreshi, Shabnam  and
      Akula, Arjun  and
      Sedoc, Jo{\~a}o  and
      Drozd, Aleksandr  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the Fifth Workshop on Insights from Negative Results in NLP",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.insights-1.15/",
    doi = "10.18653/v1/2024.insights-1.15",
    pages = "124--134",
    abstract = "With the rapid development of LLMs, it is natural to ask how to harness their capabilities efficiently. In this paper, we explore whether it is feasible to direct each input query to a single most suitable LLM. To this end, we propose LLM routing for challenging reasoning tasks. Our extensive experiments suggest that such routing shows promise but is not feasible in all scenarios, so more robust approaches should be investigated to fill this gap."
}

@inproceedings{stripelis-etal-2024-tensoropera,
    title = "{T}ensor{O}pera Router: A Multi-Model Router for Efficient {LLM} Inference",
    author = "Stripelis, Dimitris  and
      Xu, Zhaozhuo  and
      Hu, Zijian  and
      Shah, Alay Dilipbhai  and
      Jin, Han  and
      Yao, Yuhang  and
      Zhang, Jipeng  and
      Zhang, Tong  and
      Avestimehr, Salman  and
      He, Chaoyang",
    editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-industry.34/",
    doi = "10.18653/v1/2024.emnlp-industry.34",
    pages = "452--462",
    abstract = "With the rapid growth of Large Language Models (LLMs) across various domains, numerous new LLMs have emerged, each possessing domain-specific expertise. This proliferation has highlighted the need for quick, high-quality, and cost-effective LLM query response methods. Yet, no single LLM exists to efficiently balance this trilemma. Some models are powerful but extremely costly, while others are fast and inexpensive but qualitatively inferior. To address this challenge, we present TO-Router, a non-monolithic LLM querying system that seamlessly integrates various LLM experts into a single query interface and dynamically routes incoming queries to the most high-performant expert based on query`s requirements. Through extensive experiments, we demonstrate that when compared to standalone expert models, TO-Router improves query efficiency by up to 40{\%}, and leads to significant cost reductions of up to 30{\%}, while maintaining or enhancing model performance by up to 10{\%}."
}

@misc{chuang2025learningroutellmsconfidence,
      title={Learning to Route LLMs with Confidence Tokens}, 
      author={Yu-Neng Chuang and Helen Zhou and Prathusha Kameswara Sarma and Parikshit Gopalan and John Boccio and Sara Bolouki and Xia Hu},
      year={2025},
      eprint={2410.13284},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13284}, 
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@misc{specificationgaming2020,
  title={Specification gaming examples in AI},
  author={Leike, Jan and Krakovna, Victoria and others},
  year={2020},
  howpublished={\url{https://openai.com/research/specification-gaming}}
}
