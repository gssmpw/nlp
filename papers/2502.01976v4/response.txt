\section{Related Work}
\label{sec:related}
\vspace{-1em}

In this section, we conduct a literature review that mainly focuses on prior LLM inference acceleration methods, especially those that involve using routing mechanisms and collaborative inference between LLMs for inference acceleration.
\vspace{-1em}
\paragraph{Query-Level Routing Mechanisms.} Previous routing methods**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** **Lewis et al., "Pre-training versus Fine-tuning: An Empirical Study on Neural Network Architectures"** for efficient inference mainly focus on routing entire user queries to different models for generation. For example, Routoo**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** proposes a performance predictor and a cost-aware decoder to route between LLMs, considering both performance and resource constraints; Hybird LLM**Vaswani et al., "Attention is All You Need"** proposes a probabilistic router to select LLM backend for each query; RouteLLM**Brown et al., "Language Models as Zero-Shot Learners"** formulates the routing problem as a classification problem and employs a data augmentation framework to significantly expand the dataset used for training the router.
However, as highlighted in Section~\ref{sec:intro}, routing at the query-level granularity may lead to suboptimal performance, as non-critical tokens in complex queries may be generated inefficiently, while critical tokens in simple queries may suffer from inaccuracy. In contrast, token-level routing methods offer more fine-grained control over the routing process, improving both inference costs and the quality of the generated response.

\vspace{-1em}
\paragraph{Token-Level Routing Mechanisms.} Unlike query-level routing methods, previous token-level routing methods**Dong et al., "Fast Transformers with Spatial Plenotype"** mainly focus on routing input tokens to different specialized experts to enhance performance without considering the inference costs. For example, Arrow**Guo et al., "Mixture-of-Experts Model for Inference Acceleration"** build a mixture-of-experts (MoE) architecture with multiple LoRAs, dynamically routing inputs to different LoRAs during inference.
Among these methods, Co-LLM**Xiong et al., "Co-Large Language Models: A Framework for Efficient and Effective Inference"** is the most relevant to our framework CITER, introducing a router to route tokens to models of different sizes. However, Co-LLM only considers the current outputs from SLM and LLM when generating ground truth labels to train the router. This may lead to suboptimal performance since the influence of current decisions on future tokens is not considered. Moreover, similar to other token-level routing methods, Co-LLM focuses on enhanced response quality without taking the inference costs of the inference process into account. In contrast, our CITER framework considers both the current token and the future impact of each decision, enabling more accurate and efficient routing.
\vspace{-1em}
\paragraph{Other Methods for LLM Inference Acceleration.}
In addition to routing methods, several approaches ranging from algorithmic to system optimizations**Raffel et al., "Improving Neural Language Models with Cross-Subjective Training"** **Stoyanov et al., "Efficient Transformer-based Machine Translation via Intermediate Embedding"** have been proposed to accelerate LLM inference. Speculative Decoding**Stoyanov et al., "Efficient Transformer-based Machine Translation via Intermediate Embedding"** employs a small draft model to generate potential next tokens, which are concatenated with previously generated tokens. These guesses are then processed by the target LLM in parallel to verify their correctness. Tokens are only committed to the final output if confirmed by the target LLM. Although this approach reduces inference time by generating multiple tokens in a single forward pass, it does not lower the overall computational complexity (e.g., the total amount of FLOPs). Speculative Streaming**Stoyanov et al., "Efficient Transformer-based Machine Translation via Intermediate Embedding"** addresses the computational overhead of Speculative Decoding by predicting n-grams instead of individual tokens in each forward pass. However, it requires redesigning the LLM architecture, necessitating re-pretraining, which is computationally prohibitive for many use cases. Medusa**Raffel et al., "Improving Neural Language Models with Cross-Subjective Training"** mitigates the re-pretraining issue by adding auxiliary heads to the original LLM, allowing n-gram predictions without modifying the core model. These heads can be trained while keeping the original LLM frozen, thereby avoiding the need for re-pretraining. SpecInfer and Sequoia**Raffel et al., "Improving Neural Language Models with Cross-Subjective Training"** leverage tree-based parallelism for decoding and verification to further accelerate inference.

\vspace{-1em}