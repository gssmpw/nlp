\section{Related Work}
LLMs are usually pre-trained on large corpora of data that are impossible to fully supervise, and therefore pre-training is usually followed by supervised training that ensures the alignment of the model with human preferences and values~\citep{bai2022training,ouyang2022training,li2024safety}. %
However, various studies and practical cases prove that even the most popular frontier models are still prone to jailbreaks and can exhibit unsafe behavior~\citep{wei2023jailbreak,carlini2023aligned}. To ensure model safety, various solutions dedicated to either the detection of unsafe prompts or the correction of unsafe answers have emerged.
LlamaGuard~\citep{inan2023llama} is a safeguard LLM designed to assess the safety of the input text, obtained through instruction-tuning Llama-7B LLM on the specifically tailored dataset. 
However, since neither the training methodology nor the dataset used for this model is publicly available, it becomes challenging to address or improve its performance once it fails in specific cases.
Addressing the problem of LLM safety from a different perspective, 
recent works propose editing model activations. PaCE~\citep{luo2024pace} builds a learned dictionary of concepts in the activation space, which enables removing malicious parts of the activations. SEA~\citep{qiu2024spectral} projects the model representations into directions with maximal covariance with the activations for the positive outputs while minimizing covariance with the activations corresponding to the undesirable ones. However, those models operate on model generations, which may be impractical as in some cases model might generate a long, unsafe answer while the desired safe answer could be a short refusal.