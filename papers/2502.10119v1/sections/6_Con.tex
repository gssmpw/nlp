\section{Conclusion}
\label{sec:Con}
We propose a new algorithm \method{} for adaptive selecting checkpoints to average, which balances generalization performance and convergence speed. Under different function assumptions, we derive its generalization bounds, exhibiting superior results compared to other algorithms. In practical implementation, we employ probabilistic reparameterization to transform the discrete optimization problem into a continuous objective solvable by gradient-based methods. Empirically, we verify that our approach can help to obtain good performance for unstable training processes, and a few checkpoints selected by \method{} can achieve results due to other algorithms using several times as many points. 