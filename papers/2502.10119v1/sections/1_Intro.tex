\section{Introduction}
\label{sec:Intro}

Model averaging has demonstrated significant improvements in both practical applications of deep learning and theoretical investigations into its generalization and optimization. From the perspective of generalization, algorithms based on averaging, such as SWA \cite{izmailov2018averaging}, Exponential Moving Average (EMA) \cite{szegedy2016rethinking}, LAWA \citep{kaddour2022stop,sanyal2023early}, and Trainable Weight Averaging (TWA) \citep{li2022trainable}, have been empirically validated to enhance generalization performance. Moreover, most of these methods have been widely adopted, including large-scale network training \citep{izmailov2018averaging,lu2022improving,sanyal2023early}, adversarial learning \citep{xiao2022stability}, etc. In theoretical research, \citet{hardt2016train} and \citet{xiao2022stability} independently give stability-based generalization bounds for the SWA algorithm in different application contexts, and it shows that, under convex assumptions, the generalization bound of the SWA algorithm is half that of SGD. From an optimization perspective, model averaging can help an optimizer approach convergence to quickly reach the optimal solver when it oscillates near a local minimum. In the 1990s, \citet{polyak1992acceleration} demonstrate that averaging model weights improves convergence speed in the setting of convex loss functions. \citet{sanyal2023early} have empirically verified accelerated convergence using the LAWA algorithm in Large Language Models (LLM) pre-training.

Although averaging algorithms exhibit significant advantages theoretically and practically, they heavily rely on manually designed training frameworks and are highly sensitive to selecting hyperparameters. The SWA algorithm, for instance, revisits historical information at every step, leading to slower convergence. Moreover, it requires adopting a cyclic learning rate schedule to locate low-loss points, which introduces additional manual tuning. In contrast, the LAWA algorithm selects the final point for averaging within the last $k$ epochs. \citet{sanyal2023early} have observed from experiments that the final results are not monotonic with respect to the hyperparameter $k$. Instead, they exhibit a pattern where the model's performance initially increases and then decreases as $k$ grows. The TWA algorithm adaptively learns the weights of each model to achieve optimal performance; however, it requires the orthogonalization of two spaces, which adds additional computational costs. In this paper, we aim to propose an averaging algorithm that balances generalization and convergence while minimizing reliance on manually designed training frameworks.

There are two challenges to reaching this goal. 
(1) Ensure the algorithm can achieve both generalization and convergence speeds. Incorporating training information too early may hinder convergence, while collecting a few checkpoints with similar performance at a later stage may lead to limited improvement in generalization.
(2) The adaptive selection of checkpoints can be formulated as a subset selection task, which is a typical discrete optimization problem. Solving such problems requires handling discrete variables that are often non-differentiable.

To address these challenges, we select an interval with size $k$ in the final stage of training and adaptively choose a few checkpoints within this interval for averaging. This approach avoids interference from early-stage information while leveraging the performance benefits of averaging. Furthermore, $k$ is set sufficiently large to ensure that the selected checkpoints can comprehensively explore the solution space. Then, we formulate the SeWA solving process as the corset selection problem, embedding the discrete optimization objective into a probabilistic space to enable continuous relaxation, which makes gradient-based optimization methods available. Furthermore, we address the non-differentiability of binary variables by employing the Gumbel-softmax estimator. In the generalization analysis, we derive generalization bounds for SeWA under different function assumptions based on stability, which are sharper than other algorithms (see \cref{sample-table}). 

\begin{table}[t]
\caption{Comparison of \method{} with other algorithms on different settings.
Here $T$ represents iterations, and $n$ denotes the size of datasets. $L$, $\beta$, and $c$ are constants. $k$ is the number of averaging. $s\in[0,1]$ corresponds to the probability of mask $m=1$ and $\mathcal{O}_s$ means that this upper bound depends on $s$. We can derive that \method{} has sharper bounds compared to others in different settings, where FWA is the general form of LAWA.}
\label{sample-table}
\vspace{-0.2cm}
\begin{center}
%\begin{small}
\renewcommand\arraystretch{1.0}
\resizebox{0.48\textwidth}{!}{
\begin{sc}
\begin{tabular}{l|c|c}
\toprule
% \hline
Settings & Algorithm & Generalization Bound  \\ \midrule %& Optimization Bound
\multirow{4}{*}{convex} 
& SGD & $2\alpha LT/n$ \cite{hardt2016train}\\
& SWA & $\alpha LT/n$ \cite{xiao2022stability}\\
& FWA & $2\alpha L(T-k/2)/n$ \cite{peng2020dfwa}\\
& EMA & $-$\\
& \method{} & $2\alpha Ls(T-k/2)/n$ \cref{thm:stability-conv}\\ \hline
\multirow{4}{*}{\!\!non-convex\!\!} 
&  SGD & $\mathcal{O}(T^{\frac{c\beta}{1+c\beta}}/n)$ \cite{hardt2016train}\\ 
&  SWA & $\mathcal{O}(T^{\frac{c\beta}{2+c\beta}}/n)$ \cite{wanggeneralization}\\ 
&  FWA & $\mathcal{O}(T^{\frac{c\beta}{k+c\beta}}/n)$ \cite{peng2020dfwa}\\ 
& EMA & $-$\\
& \method{} & $\mathcal{O}_s(T^{\frac{c\beta} {k+c\beta}}/n)$\cref{thm:stability-non-with}\\ 
\bottomrule
\end{tabular}
%\begin{tablenotes}
%\footnotesize
%\item[1] Note .
%\item[2] The quick brown fox jumps over the lazy dog.
%\end{tablenotes}
\end{sc}
}
%\end{small}
\end{center}
\vskip -0.2in
\end{table}

\subsection{Our Contributions}

This paper primarily introduces the \method{} algorithm. We theoretically demonstrate the \method{} with better generalization performance and give its optimization method. Extensive experiments have been conducted across various domains, including computer vision, natural language processing, and reinforcement learning, confirming the algorithm's generalization and convergence advantages. Our contributions are listed as follows. 

\begin{itemize}
\item Our approach adaptively selects models for averaging in the final training stages, ensuring strong generalization and faster convergence. Notably, \method{} eliminates the need for manually designed selection frameworks, minimizing biases toward specific scenarios.

\item We analyze the impact of masks on generalization theory in expectation and derive a stability-based generalization upper bound, showing advantages over SGD bounds under the different function assumptions.

\item We propose a solvable optimization framework by transforming the discrete problem into a continuous probabilistic space and addressing the non-differentiability of binary variables using the Gumbel-Softmax estimator during optimization.


\item We empirically demonstrate the outstanding performance of our algorithm in multiple domains, including behavior cloning, image classification, and text classification. In particular, the \method{} achieves comparable performance using only a few selected points, matching or exceeding the performance of other methods that require many times more points.
\end{itemize}
