\section{Experiment Details}
\label{sec:ExpDetail}

\subsection{Behavior Cloning}

\textbf{Network Architecture.} The network architecture comprises four layers, each consisting of a sequence of ReLU activation, Dropout for regularization, and a Linear transformation. The final layer includes an additional Tanh activation function to enhance the representation and capture non-linearities in the output.

\textbf{Results.} 
Comprehensive results for each task across all datasets are presented in Table \ref{tab:BC}. Our evaluation focuses specifically on the medium and medium-expert datasets, which offer a balanced mix of trajectories with varying performance levels. This selection enables a thorough assessment of our method's ability to generalize across different reward distributions.
For clarity and ease of comparison, the main paper emphasizes the average performance across tasks, as illustrated in Figure \ref{fig:BC}. 
%This dual presentation ensures a detailed examination of individual tasks while providing an accessible overview of overall performance.

\begin{table*}[t!]
\centering
\caption{
Performance comparison of various methods on D4RL Gym tasks. The left panel shows results obtained using the final checkpoint under different update strategies, while the right panel presents results from averaged checkpoints collected during the final training stage with SGD, using different selection strategies. 
Each result is evaluated as the mean of 60 random rollouts, based on 3 independently trained models with 20 trajectories per model.
}
\label{tab:BC}
\scalebox{0.9}{
\begin{tabular}{c|cc|ccc|ccc}
\toprule[2pt]
\multicolumn{1}{l|}{} & Task & Dataset & SGD & SWA & EMA & LAWA & Random & \method{} (Ours) \\ \midrule
\multirow{7}{*}{K=10} & Hopper & medium & 1245.039 & 1279.249 & 1297.270 & 1289.515 & 1291.478 & \textbf{1324.848} \\
 & Hopper & medium-expert & 1460.785 & 1468.893 & 1320.408 & 1462.452 & 1451.015 & \textbf{1509.317} \\
 & Walker2d & medium & 3290.248 & 3328.121 & 3341.888 & 3341.437 & 3306.763 & \textbf{3371.202} \\
 & Walker2d & medium-expert & 3458.693 & 3546.008 & 3681.504 & 3634.373 & 3609.611 & \textbf{3679.806} \\
 & Halfcheetah & medium & 4850.490 & 4858.224 & 4894.204 & 5012.389 & 4896.104 & \textbf{5041.369} \\
 & Halfcheetah & medium-expert & 5015.689  & 4974.923 & 4857.562 & 4989.329 & 4962.719 & \textbf{5082.902} \\ \cmidrule{2-9}
 & \multicolumn{2}{c|}{Average} & 3220.157 & 3242.570 & 3232.139 & 3288.249 & 3252.948 & \textbf{3334.907} \\ \midrule
\multirow{7}{*}{K=20} & Hopper & medium & 1245.039 & 1281.910 & 1302.400 & 1310.875 & 1312.166 & \textbf{1361.202} \\
 & Hopper & medium-expert & 1460.785 & 1427.47 & 1373.268 & 1563.307 & 1482.012 & \textbf{1571.127} \\
 & Walker2d & medium & 3290.248 & 3308.464 & \textbf{3420.257} & 3325.873 & 3324.557 & 3364.886 \\
 & Walker2d & medium-expert & 3458.693 & 3588.176 & 3667.809 & 3557.925 & 3650.846 & \textbf{3673.804} \\
 & Halfcheetah & medium & 4850.490 & 4913.549 & 4848.006 & 4974.041 & 4924.613 & \textbf{5071.051} \\
 & Halfcheetah & medium-expert & 5015.689 & 5024.723 & 4957.194 & 4993.524 & 4988.816 & \textbf{5085.628} \\ \cmidrule{2-9}
 & \multicolumn{2}{c|}{Average} & 3220.157 & 3257.382 & 3261.489 & 3287.591 & 3280.502 & \textbf{3354.616} \\ \midrule
\multirow{7}{*}{K=50} & Hopper & medium & 1245.039 & 1294.884 & 1329.863 & 1336.33 & 1319.571 & \textbf{1389.280} \\
 & Hopper & medium-expert & 1460.785 & 1477.466 & 1485.696 & 1537.672 & 1496.045 & \textbf{1616.116} \\
 & Walker2d & medium & 3290.248 & 3262.046 & 3341.767 & 3253.695 & 3352.12 & \textbf{3392.130} \\
 & Walker2d & medium-expert & 3458.693 & 3577.509 & 3591.081 & 3584.468 & 3659.789 & \textbf{3672.560} \\
 & Halfcheetah & medium & 4850.490 & 4927.951 & 4968.048 & 5022.097 & 5000.004 & \textbf{5035.631} \\
 & Halfcheetah & medium-expert & 5015.689  & 5061.688 & \textbf{5075.426} & 5011.232 & 4960.585 & 5044.886 \\ \cmidrule{2-9}
 & \multicolumn{2}{c|}{Average} & 3220.157 & 3280.833 & 3298.647 & 3290.916 & 3298.019 & \textbf{3358.434} \\ \midrule
\multirow{7}{*}{K=100} & Hopper & medium & 1245.039 & 1347.267 & 1322.625 & 1320.652 & 1319.727 & \textbf{1393.981} \\
 & Hopper & medium-expert & 1460.785 & 1527.206 & 1528.265 & 1496.266 & 1491.196 & \textbf{1568.025} \\
 & Walker2d & medium & 3290.248 & 3324.218 & 3393.646 & 3345.913 & 3321.046 & \textbf{3424.078} \\
 & Walker2d & medium-expert & 3458.693 & 3575.621 & 3629.308 & 3613.274 & 3587.211 & \textbf{3710.347} \\
 & Halfcheetah & medium & 4850.490 & 4939.629 & 4871.376 & 4974.220 & 5015.349 & \textbf{5021.948} \\
 & Halfcheetah & medium-expert & 5015.689 & 4919.624 & 5047.757 & 4991.007 & 5031.975 & \textbf{5063.546} \\ \cmidrule{2-9}
 & \multicolumn{2}{c|}{Average} & 3220.157 & 3272.261 & 3298.830 & 3290.222 & 3294.417 & \textbf{3363.654} \\ 
 \bottomrule
\end{tabular}
}
\end{table*}

\subsection{Image Classification}

\textbf{Network Architecture.} The network architecture consists of three primary blocks, followed by an average pooling layer and a linear layer for generating the final output. Each block contains two convolutional layers, each accompanied by a corresponding batch normalization layer to improve training stability and convergence. To address potential issues of vanishing gradients, each block includes a shortcut connection that facilitates efficient gradient flow during backpropagation. The output of each block is passed through a ReLU activation function to introduce non-linearity, enabling the network to learn complex representations effectively.

\textbf{Results.} In addition to the results presented in Figure \ref{fig:cifar100}, we provide further analysis examining the impact of network parameter variations to demonstrate the robustness of our method across networks of different sizes. These results, shown in Figure \ref{fig:cifar100-layers}, illustrate that as the number of layers or blocks increases, the performance of SGD improves, following a similar training curve.

Notably, weight averaging consistently outperforms SGD during the upward phase of training. The performance gains from weight averaging become more pronounced as the network size increases, highlighting its potential in scaling effectively to larger models. This highlights the potential of weight averaging to enhance the performance of larger models. 
%
Furthermore, regardless of changes in network parameters, our proposed method consistently achieves superior results, demonstrating its adaptability and effectiveness across varying network configurations. These findings emphasize the potential of weight averaging as a robust and scalable technique for optimizing model performance.

\begin{figure*}[t!]
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet1-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet1-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet1-k50.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet3-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet3-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet3-k50.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet5-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet5-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet5-k50.pdf}}
    %\hspace{2mm}
    \vspace{-0.2cm}
    \caption{From left to right, the figures illustrate the impact of the hyperparameter $K$ on the CIFAR-100 task. 
    Each data point represents performance based on intervals of 100 checkpoints, with $K$ checkpoints selected from these intervals using various strategies.
    The first row corresponds to a network architecture with 1 block, the second row represents a network with 3 blocks, and the third row depicts results for a network with 5 blocks.
    %
    }
    \label{fig:cifar100-layers}
\end{figure*}


\subsection{Text Classification}

\textbf{Network Architectures.} The network architecture comprises two embedding layers followed by two layers of \textit{TransformerEncoderLayer}. Each \textit{TransformerEncoderLayer} includes a multi-head self-attention mechanism and a position-wise feedforward network, along with layer normalization and residual connections to enhance training stability and gradient flow. The output from the Transformer layers is passed through a linear layer to produce the final predictions.

\textbf{Results.} In addition to the findings presented in Figure \ref{fig:ag}, we conduct further analysis to evaluate the impact of network parameter variations, demonstrating the robustness of our method across networks of varying sizes. These additional results, shown in Figure \ref{fig:ag-layers}, indicate that as the number of Transformer layers increases, the performance of SGD improves up to a certain point. However, beyond this range - where two layers appear sufficient - performance begins to exhibit fluctuations, suggesting diminishing returns and instability with additional layers.

While the improvement achieved by weight averaging is relatively modest due to the simplicity of the task, it still plays a critical role in stabilizing the training process and reducing fluctuations in the training curve. Among the averaging methods evaluated, our proposed method consistently achieves the best performance, underscoring its effectiveness in maintaining stability and optimizing performance, even in scenarios where task complexity is low.

\begin{figure*}[t!]
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer1-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer1-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer1-k50.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer2-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer2-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer2-k50.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer4-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer4-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer4-k50.pdf}}
    %\hspace{2mm}
    \vspace{-0.2cm}
    \caption{From left to right, the figures illustrate the impact of the hyperparameter $K$ on the AG News corpus. Each point corresponds to intervals of 100 checkpoints, with $K$ checkpoints selected from these intervals using different strategies.
    The first row corresponds to a network architecture with a single \textit{TransformerEncoderLayer}, the second row represents a network with three \textit{TransformerEncoderLayer}s, and the third row shows results for a network with five \textit{TransformerEncoderLayer}s.
    }
    \label{fig:ag-layers}
\end{figure*}

\section{Proof of Lemma \ref{lemma}}\label{pro-lemma}  \paragraph{\boldmath$(1+\alpha\beta)$-expansive.} According to triangle inequality and $\beta$-smoothness,
\begin{equation}\label{eq:app2.1}
     \begin{aligned}
      \Vert w_{T+1} - w_{T+1}^{\prime}\Vert &\leq \Vert w_T- w_T^{\prime}\Vert + \alpha\Vert \nabla F(w_T) -\nabla F(w_T^{\prime})\Vert \\
      &\leq \Vert w_T- w_T^{\prime}\Vert + \alpha\beta \Vert w_T- w_T^{\prime}\Vert \\
      &= (1+\alpha\beta)\Vert w_T- w_T^{\prime}\Vert .
     \end{aligned}
\end{equation}

\paragraph{\emph{Non}-expansive.} Function is convexity and $\beta$-smoothness that implies 
\begin{equation}\label{eq:app2.2}
     \begin{aligned}
      \langle \nabla F(w) -\nabla F(v), w - v \rangle \geq \frac{1}{\beta} \Vert \nabla F(w) -\nabla F(v)\Vert^2 .
     \end{aligned}
\end{equation}
We conclude that
\begin{equation}\label{eq:app2.3}
     \begin{aligned}
      \Vert w_{T+1} - w_{T+1}^{\prime}\Vert &= \sqrt{\Vert w_{T} - \alpha \nabla F(w_{T}) - w_{T}^{\prime} + \alpha \nabla F(w_{T}^{\prime})\Vert^2}  \\
      &=\sqrt{\Vert w_{T} - w_{T}^{\prime} \Vert^2 - 2\alpha\langle \nabla F(w_{T}) -\nabla F(w_{T}^{\prime}), w_T- w_T^{\prime} \rangle +\alpha^2 \Vert \nabla F(w_{T}) - \nabla F(w_{T}^{\prime})\Vert^2} \\
      &\leq \sqrt{\Vert w_T- w_T^{\prime}\Vert^2 - \left(\frac{2\alpha}{\beta} -\alpha^2 \right) \Vert \nabla F(w_{T}) -\nabla F(w_{T}^{\prime})\Vert^2} \\
      &\leq \Vert w_T- w_T^{\prime}\Vert.
     \end{aligned}
\end{equation}

\section{Proof of the generalization bounds}\label{pro-con}
By the Lemma \ref{convex-basic} and \ref{nonconvex-basic}, the proof of Theorem \ref{thm:stability-conv} and \ref{thm:stability-non-with} can be further decomposed into bounding the difference of the parameters for the last $k$ points of the average algorithm. We provide the proof as follows. And you can also find it in \cite{peng2020dfwa}.

\subsection{Update rules of the last $k$ points of the averaging algorithm.}
For the last $k$ points of the averaging algorithm, we formulate it as
\begin{equation}\label{FWA-rules}
    \hat{w}^{k}_{T}=\frac{1}{k} \sum_{i=T-k+1}^{T} w_{i}.
\end{equation}
It is not difficult to find the relationship between $\bar{w}^{k}_{T}$ and $\bar{w}^{k}_{T-1}$, i.e.,
\begin{equation}\label{pro-FWA-update}
    \hat{w}^{k}_{T} = \hat{w}^{k}_{T-1} + \frac{1}{k}\left(w_{T} - w_{T-k}\right) = \hat{w}^{k}_{T-1} - \frac{1}{k}\sum_{i=T-k+1}^{T} \alpha_i\nabla F(w_{i-1},z_i),
\end{equation}
where the second equality follows from the update of SGD.



\subsection{\textbf{Proof. Theorem \ref{thm:stability-conv}}}\label{proof-thm-con-with}
  First, using the relationship between $\hat{w}^{k}_{T}$ and $\hat{w}^{k}_{T-1}$ in Eq. ~\eqref{pro-FWA-update}, we consider that the different sample $z_{T}$ and $z_{T}^{\prime}$ are selected to update with probability $\frac{1}{n}$ at the step $T$.  
\begin{equation}
  \begin{aligned}
   \bar{\delta}_{T} &= \bar{\delta}_{T-1} + \frac{1}{k}\sum_{i=T-k+1}^{T} \alpha_i \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert \\
   &\leq \bar{\delta}_{T-1} + \frac{2\alpha_T L}{k} + \frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha_i \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert ,
  \end{aligned}
 \end{equation}
where the proof follows from the triangle inequality and $L$-Lipschitz condition. For $\frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha_i \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert$ will be controlled in the late.

Second, another situation need be considered in case of the same sample are selected$(z_{T}=z_{T}^{\prime})$ to update with probability $1-\frac{1}{n}$ at the step $T$. 
\begin{equation}
  \begin{aligned}
   \bar{\delta}_{T} &= \bar{\delta}_{T-1} + \frac{1}{k}\sum_{i=T-k+1}^{T} \alpha_i \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert \\
   &\leq \bar{\delta}_{T-1} + \frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha_i\Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert ,
  \end{aligned}
 \end{equation}
where $\Vert\nabla F(w^{\prime}_{T-1},z_T)-\nabla F(w_{T-1},z_T)\Vert=0$ in the second inequality because the non-expansive property of convex function.

For each $\Vert\nabla F(w^{\prime}_{i-1},z_i)-\nabla F(w_{i-1},z_i)\Vert$ in the sense of expectation, We consider two situations using $\alpha L$ bound and the non-expansive property. Then, we get  
  \begin{equation}
    \frac{1}{k}\sum_{i=T-k+1}^{T-1}\alpha_i \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert \leq \frac{2L}{nk}\sum_{i=T-k+1}^{T-1}\alpha_i.
 \end{equation}

Then we obtain the expectation based on the above analysis 
  \begin{equation}
  \begin{aligned}
    \mathbb{E}\left[\bar{\delta}_{T}\right] &\leq (1-\frac{1}{n})\bar{\delta}_{T-1} + \frac{1}{n}\left(\bar{\delta}_{T-1}+\frac{2\alpha_T L}{k}\right) + \frac{2L}{nk}\sum_{i=T-k+1}^{T-1}\alpha_i\\
    &\leq \mathbb{E}\left[\bar{\delta}_{T-1}\right] + \frac{2L}{nk}\sum_{i=T-k+1}^{T}\alpha_i
  \end{aligned}
 \end{equation}
recursively, we can get 
    \begin{equation}
     \begin{aligned}
      \mathbb{E}\left[\bar{\delta}_{T}\right]&\leq \frac{2L}{nk} \left( \sum_{i=T-k+1}^{T}\alpha_i + \sum_{i=T-k}^{T-1}\alpha_i + \cdots + \sum_{i=1}^{k}\alpha_i \right) \\ & + \frac{2L}{nk} \left( \sum_{i=1}^{k-1}\alpha_i + \sum_{i=1}^{k-2}\alpha_i + \cdots + \sum_{i=1}^{1}\alpha_i \right). \\
      %&\leq \frac{2L}{n} \left( \sum_{i=E-k}^{E} \sum_{j=0}^{d} \frac{(E+1-i)\alpha_{i,j}}{k+1} +  \sum_{i=1}^{E-k-1} \sum_{j=0}^{d} \alpha_{i,j}\right).
     \end{aligned}
    \end{equation}
Let $\alpha_{i,j}=\alpha$, we get
    \begin{equation}
     \begin{aligned}
      \mathbb{E}\left[\bar{\delta}_{T}\right] = \frac{2\alpha L}{n} \left( T - \frac{k}{2} \right).
     \end{aligned}
    \end{equation}
Plugging this back into Eq.~\eqref{convex-basic}, we obtain
 \begin{equation}\label{eq:2.2.1}
  \epsilon_{gen} = \mathbb{E}\vert F(\bar{w}_T^K;z)-F(\bar{w}^{K\prime}_T;z)\vert \leq \frac{2\alpha L^2 s}{n} \left( T - \frac{k}{2} \right).
 \end{equation}
And we finish the proof.

\subsection{Proof of Lemma \ref{nonconvex-basic}}\label{proof-noncon-basic}
We consider that $S$ and $S^\prime$ are two samples of size $n$ differing in only a single example. Let $\xi$ denote the event $\bar{\delta}_{t_0}=0$. Let $z$ be an arbitrary example and consider the random variable $I$ assuming the index of the first time step using the different sample. then we have
    \begin{equation}
     \begin{aligned}
      \mathbb{E}\vert \nabla F(\bar{w}_T^{K};z)-\nabla F(\bar{w}^{K\prime}_T;z)\vert &= P\left\lbrace \xi\right\rbrace \mathbb{E}[\vert \nabla F(\bar{w}_T^{K};z)-\nabla F(\bar{w}^{K\prime}_T;z)\vert|\xi]\\
      &+P\left\lbrace \xi^{c}\right\rbrace E[\vert \nabla F(\bar{w}_T^{K};z)-\nabla F(\bar{w}^{K\prime}_T;z)\vert |\xi^{c}]\\
      &\leq P\left\lbrace I\geq t_0\right\rbrace \cdot \mathbb{E}[\vert \nabla F(\bar{w}_T^{K};z)-\nabla F(\bar{w}^{K\prime}_T;z)\vert |\xi] \\
      &+P\left\lbrace I\leq t_0\right\rbrace \cdot \mathop{sup}_{\bar{w}^{K},z} F(\bar{w}^{K};z),\\
     \end{aligned}
    \end{equation}
where $\xi^{c}$ denotes the complement of $\xi$.   

Note that Note that when $I\geq t_0$, then we must have that $\bar{\delta}_{t_0}=0$, since the execution on $S$ and $S^{\prime}$ is identical until step $t_0$. We can get $LE[\Vert\bar{w}_{T}^{K} - \bar{w}_{T}^{K\prime}\Vert|\xi]$ combined the Lipschitz continuity of $F$. Furthermore, we know $P\left\lbrace \xi^{c}\right\rbrace=P\left\lbrace \bar{\delta}_{t_0}=0\right\rbrace\leq P\left\lbrace I\leq t_0\right\rbrace$, for the random selection rule, we have 
    \begin{equation}
     \begin{aligned}
      P\left\lbrace I\leq t_0\right\rbrace \leq \sum_{t=1}^{t_0} P\left\lbrace I = t_0\right\rbrace = \frac{t_0}{n}.
     \end{aligned}
    \end{equation}
We can combine the above two parts and $F \in [0,1]$ to derive 
the stated bound $L\mathbb{E}[\Vert\bar{w}_{T}^{k} - \bar{w}_{T}^{k\prime}\Vert\vert\xi]+\frac{t_0}{n}$, which completes the proof.

\subsection{Lemma \ref{Lemma_noncon} and it's proof}\label{proof-Lemma_noncon}
\begin{lemma}\label{Lemma_noncon}
Assume that $F$ is $\beta$-smooth and $non$-convex. Let $\alpha = \frac{c}{T}$, we have 
  \begin{equation}
  \begin{aligned}
   \Vert w^{\prime}_{T}& - w_{T} \Vert \leq &e^\frac{c\beta k}{T}\bar{\delta}_{T},
  \end{aligned}
 \end{equation}
\end{lemma}
where $\bar{\delta}_{T}= \frac{1}{k} \sum_{i=T-k+1}^{T}\Vert w^{\prime}_{i} - w_{i} \Vert$.

\textbf{proof Lemma \ref{Lemma_noncon}.} 
By triangle inequality and our assumption that $F$ satisfies, we have
 \begin{equation}
  \begin{aligned}
   \Vert w^{\prime}_{T}& - w_{T} \Vert = \frac{1}{k} \cdot k \cdot \Vert w^{\prime}_{T} - w_{T} \Vert \\
   \leq & \frac{1}{k} ( \Vert w^{\prime}_{T} - w_{T} \Vert + (1+\alpha_{T-1}\beta)\Vert w^{\prime}_{T-1} - w_{T-1} \Vert + \cdots + \\&(1+\alpha_{T-1}\beta)(1+\alpha_{T-2}\beta)\cdots(1+\alpha_{T-k+1}\beta)\Vert w^{\prime}_{T-k+1} - w_{T-k+1} \Vert ) \\
   \leq & \prod_{t=T-k+1}^{T} (1+\alpha_t\beta)\left(\frac{1}{k} \sum_{i=T-k+1}^{T}\Vert w^{\prime}_{i} - w_{i} \Vert\right).
    \end{aligned}
 \end{equation}
Let $\alpha_t = \alpha = \frac{C}{T}$, we have
  \begin{equation}
  \begin{aligned}
   \Vert w^{\prime}_{T}& - w_{T} \Vert \leq &\prod_{t=T-k+1}^{T} (1+\alpha_t\beta)\bar{\delta}_{T} \leq \left(1+ \frac{c\beta}{T}\right)^k\bar{\delta}_{T} \leq e^\frac{c\beta k}{T}\bar{\delta}_{T}.
  \end{aligned}
 \end{equation}


\subsection{\textbf{Proof. Theorem \ref{thm:stability-non-with} (Based on the constant learning rate)}}
\label{proof-thm-non-with} In the case of non-convex, the $(1+\alpha\beta)$-expansive properties and $L$-Lipschitz conditions are used in our proof. Based on the relationship between $\hat{w}^{k}_T$ and $\hat{w}^{k}_{T-1}$ in Eq. ~\eqref{pro-FWA-update}. We consider that the different samples $z_T$ and $z^{\prime}_T$ are selected to update with probability $\frac{1}{n}$ at step T.
\begin{equation}
  \begin{aligned}
   \bar{\delta}_{T} &= \bar{\delta}_{T-1} + \frac{1}{k}\sum_{i=T-k+1}^{T} \alpha \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert \\
   &\leq \bar{\delta}_{T-1} + \frac{2\alpha L}{k} + \frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert ,
  \end{aligned}
 \end{equation}
Next, the same sample $z=z^{\prime}$ is selected to update with probability $1-\frac{1}{n}$ at step T.
\begin{equation}
  \begin{aligned}
   \bar{\delta}_{T} &= \bar{\delta}_{T-1} + \frac{1}{k}\sum_{i=T-k+1}^{T} \alpha \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert \\
   &\leq \bar{\delta}_{T-1} + \frac{\alpha \beta}{k}\Vert w^{\prime}_{T-1} - w_{T-1} \Vert + \frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert \\
   &\leq (1+\frac{\alpha \beta(1+\alpha \beta)^{k-1}}{k})\bar{\delta}_{T-1} + \frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert,
  \end{aligned}
 \end{equation}
where the proof follows from the $\beta$-smooth and Lemma \ref{Lemma_noncon}. Then, we bound the $\alpha \Vert\nabla F(w^{\prime}_{T-2},z_{T-1}) - \nabla F(w_{T-2},z_{T-1}) \Vert$ with different sampling. 
 \begin{equation}\label{noncon-sigbound}
  \begin{aligned}    
    \alpha\Vert\nabla &F(w^{\prime}_{T-2},z_{T-1}) - \nabla F(w_{T-2},z_{T-1}) \Vert = \frac{2\alpha L}{n} + \left(1-\frac{1}{n}\right)\alpha\beta\Vert w_{T-2} - w^{\prime}_{T-2} \Vert\\
    &\leq \frac{2\alpha L}{n} + \alpha\beta\left(\Vert w_{T-3} - w^{\prime}_{T-3} \Vert + \alpha \Vert \nabla F(w^{\prime}_{T-3},z_{T-2})-\nabla F^{\prime}(w_{T-3},z_{T-2}) \Vert\right) \\
    &\leq \frac{2\alpha L}{n} + \alpha\beta\left(\frac{2\alpha L}{n} + (1+\alpha\beta)\Vert w_{T-3} - w^{\prime}_{T-3} \Vert\right) \\
    &\cdots\\
    &\leq \frac{2\alpha L}{n}(1+\alpha\beta)^{T-2-t_0} + \alpha\beta(1+\alpha\beta)^{T-2-t_0}\Vert w_{t_{0}} - w^{\prime}_{t_{0}} \Vert =\frac{2\alpha L}{n}(1+\alpha\beta)^{T-2-t_0},
  \end{aligned}
 \end{equation}
where $w_{t_{0}} = w^{\prime}_{t_{0}}$. Therefore, we can obtain the bound for $\frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert$ in the expectation sense.
\begin{equation}\label{44}
    \begin{aligned}
     \frac{\alpha}{k}\sum_{i=T-k+1}^{T-1} & \mathbb{E} \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert 
     \leq \frac{2\alpha L}{nk} \sum_{i=T-k}^{T-2} (1+\alpha\beta)^{i-t_{0}} \\
     &\leq \frac{2\alpha L}{nk} \cdot k (1+\alpha\beta)^{T} \leq \frac{2\alpha L(1+\alpha\beta)^{T}}{n}.  
    \end{aligned}
\end{equation}
Then, we obtain the expectation considering the above analysis 
 \begin{equation}
    \begin{aligned}
     \mathbb{E}\left[\bar{\delta}_{T+1}\right] &\leq (1-\frac{1}{n})\left(1+\frac{\alpha \beta(1+\alpha \beta)^{k-1}}{k}\right)\bar{\delta}_T + \frac{1}{n}\left(\bar{\delta}_T+\frac{2\alpha L}{k}\right) + \frac{2\alpha L(1+\alpha\beta)^{T}}{n}\\ 
     &\leq \left(\frac{1}{n}+(1-\frac{1}{n})\left(1+\frac{\alpha \beta(1+\alpha \beta)^{k-1}}{k}\right)\right)\bar{\delta}_{T} + \frac{2\alpha L}{nk}\left(1+k(1+\alpha\beta)^{T}\right)\\
    \end{aligned}
   \end{equation}
let $\alpha = \frac{c}{t}$, then
  \begin{equation}
 \begin{aligned}
     &= \left(1+(1-\frac{1}{n})\frac{c\beta(1+\frac{c\beta}{t})^{k}}{kt}\right) \bar{\delta}_{t} + \frac{2cL}{nkt}\left(1+k(1+\frac{c\beta}{t})^{t}\right)\\
     &\leq \exp\left((1-\frac{1}{n})\frac{c\beta e^{\frac{c\beta k}{t}}}{kt}\right) \bar{\delta}_{t} + \frac{2cL}{kn}\cdot\frac{1+k e^{c\beta}}{t}.
    \end{aligned}
   \end{equation}
Here we used that $\lim\limits_{x\to\infty}(1+\frac{1}{x})^x=e$ and $\lim\limits_{x\to\infty}e^\frac{1}{x}=1$. 
Using the fact that $\bar{\delta}_{t_0}=0$, we can unwind this recurrence relation from $T$ down to $t_0+1$.
  \begin{equation}
    \begin{aligned}
     \mathbb{E}\bar{\delta}_{t} &\leq \sum_{t=t_0 +1}^{T} \left( \prod_{m=t+1}^{T}\exp\left((1-\frac{1}{n})\frac{c\beta}{km}\right)\right)\frac{2cL}{kn}\cdot\frac{1+k e^{c\beta}}{t}\\
     &= \sum_{t=t_0 +1}^{T} \exp\left(\frac{(1-\frac{1}{n})c\beta}{k} \sum_{m=t+1}^{T}\frac{1}{m}\right)\frac{2cL}{kn}\cdot\frac{1+k e^{c\beta}}{t}\\
     &\leq \sum_{t=t_0 +1}^{T} \exp\left( \frac{(1-\frac{1}{n})c\beta}{k} \cdot \log(\frac{T}{t}) \right)\frac{2cL}{kn}\cdot\frac{1+k e^{c\beta}}{t}\\
     &\leq T^{\frac{(1-\frac{1}{n})c\beta}{k}} \cdot \sum_{t=t_0 +1}^{T} \left(\frac{1}{t}\right)^{\frac{(1-\frac{1}{n})c\beta}{k}+1} \cdot \frac{2cL(1+ke^{c\beta})}{kn}\\
     &\leq \frac{k}{(1-\frac{1}{n})c\beta} \cdot \frac{2cL(1+ke^{c\beta})}{kn} \cdot \left(\frac{T}{t_0}\right)^{\frac{(1-\frac{1}{n})c\beta}{k}}\\
     &\leq \frac{2L(1+ke^{c\beta})}{(n-1)\beta} \cdot \left(\frac{T}{t_0}\right)^{\frac{c\beta}{k}}.
    \end{aligned}
   \end{equation}
Plugging this back into Eq.~\eqref{nonconvex-basic}, we obtain
 \begin{equation}\label{with-con}
  \mathbb{E}\vert F(\bar{w}_T^{K};z)-F(\bar{w}^{K\prime}_T;z)\vert \leq \frac{t_0}{n} + \frac{2sL^2(1+ke^{c\beta})}{(n-1)\beta} \cdot \left(\frac{T}{t_0}\right)^{\frac{c\beta}{k}}.
 \end{equation}
By taking the extremum, we obtain the minimum  
 \begin{equation}\label{with-con-t_0}
    t_0 = \left(2csL^2(1+ke^{c\beta})k^{-1}\right)^{\frac{k}{c\beta+k}}\cdot T^{\frac{c\beta}{c\beta+k}}
   \end{equation}
finally, this setting gets
 \begin{equation}\label{with-con-result}
  \epsilon_{gen} = \mathbb{E}\vert F(\bar{w}_T^{K};z)-F(\bar{w}^{K\prime}_T;z)\vert \leq \frac{1+\frac{1}{c\beta}}{n-1}\left(2csL^2(1+ke^{c\beta})k^{-1}\right)^{\frac{k}{c\beta+k}}\cdot T^{\frac{c\beta}{c\beta+k}},
 \end{equation}
to simplify, omitting constant factors that depend on $\beta$, c
and L, we get 
   \begin{equation}
    \epsilon_{gen}  \leq \mathcal{O}_s\left(\frac{T^{\frac{c\beta}{c\beta+k}}}{n}\right).
   \end{equation}
And we finish the proof.
