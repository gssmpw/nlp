\section{Experiment}
\label{sec:Exp}
In our experimental evaluation, we systematically explore the performance of our proposed method across three distinct settings: behavior cloning, image classification, and text classification. These settings are chosen to demonstrate the generality and effectiveness of our approach in diverse application domains. 
%
Details of the experimental setup, including network architectures, hyperparameters, and additional results, are provided in Appendix \ref{sec:ExpDetail}.

\subsection{Behavior Cloning}
\textbf{Experimental Setups.} We performed extensive evaluations using the widely recognized D4RL benchmark \citep{fu2020d4rl}, with a particular focus on the Gym-MuJoCo locomotion tasks. These tasks are commonly regarded as standard benchmarks due to their simplicity and well-structured nature. They are characterized by datasets containing a significant proportion of near-optimal trajectories and smooth reward functions, making them suitable for evaluating the performance of reinforcement learning methods. 
% Specifically, we concentrated on the medium and medium-expert datasets, which provide a balanced mix of trajectories with varying levels of performance, enabling a comprehensive assessment of our method’s capability to generalize across different reward distributions.
%
For evaluation, we employed cumulative reward as the primary metric, as it effectively captures the overall performance of the agent in maximizing returns over the course of its trajectories. 

\textbf{Baselines.} 
%
To assess the effectiveness of our proposed SeWA method, we compare it against several established baselines, including the original pre-training recipe based on stochastic gradient descent (SGD), Stochastic Weight Averaging (SWA) \citep{izmailov2018averaging}, and Exponential Moving Average (EMA) \citep{szegedy2016rethinking}, which we adapt for the behavior cloning setting.
%
For EMA, we follow the approach outlined in \citet{kaddour2022stop}, setting the decay parameter to 0.9 and updating the EMA model at every $K$ training step, which is a widely adopted standard practice. 
%
For SWA, we adhere to the original pre-training procedure up to 75\% completion. Following this phase, we initiate SWA training with a cosine annealing scheduler and compute the SWA uniform average every $K$ steps to aggregate model parameters effectively.
%
Additionally, we compare our SeWA with LAWA \citep{sanyal2023early} and a Random baseline. 
Both of these baselines involve directly averaging pretrained checkpoints from the original pre-training process without additional retraining. Specifically, LAWA selects $K$ checkpoints at equal intervals, whereas the Random baseline selects $K$  checkpoints randomly from the same set.
%
It is important to note that LAWA, Random, and our proposed method all utilize the final 1000 checkpoints from the pre-training process to compute performance metrics without further retraining the model.
In contrast, the SGD, SWA, and EMA baselines report their final performance directly, as their evaluation pipelines and corresponding techniques are integrated into their respective training processes. This ensures a fair and consistent comparison across all methods.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/Behavior_Clone.pdf}
    \vspace{-.4cm}
    \caption{Comparison of different methods on the D4RL benchmark. Each data point represents the average cumulative reward across multiple tasks, averaged over 3 random seeds and 20 trajectories per seed. Detailed results are provided in Appendix \ref{sec:ExpDetail}.}
    \label{fig:BC}
\end{figure}

\begin{figure*}[t!]
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet3-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet3-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/Cifar100-ResNet3-k50.pdf}}
    %\hspace{2mm}
    \vspace{-0.2cm}
    \caption{From left to right, the figures illustrate the impact of the hyperparameter $K$ on the CIFAR-100 task. Each point corresponds to intervals of 100 checkpoints, with $K$ checkpoints selected and averaged from these intervals using different strategies.
    %
    }
    \label{fig:cifar100}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer2-k10.pdf}}
    \centering
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer2-k20.pdf}}
    \subfigure{
    \centering
    \includegraphics[width=0.3\textwidth]{figs/NLP-layer2-k50.pdf}}
    %\hspace{2mm}
    \vspace{-0.2cm}
    \caption{From left to right, the figures illustrate the impact of the hyperparameter $K$ on the AG News corpus. Each point corresponds to intervals of 100 checkpoints, with $K$ checkpoints selected and averaged from these intervals using different strategies.
    %
    }
    \label{fig:ag}
\end{figure*}

\textbf{Results.} 
As shown in Figure \ref{fig:BC}, all baselines demonstrate superior performance compared to the original SGD optimizer, highlighting the effectiveness of weight averaging strategies in improving model performance. These results confirm that weight averaging can serve as a valuable technique for stabilizing and enhancing model training outcomes. 
Additionally, our analysis reveals that increasing the number of checkpoints $K$ used for averaging consistently improves performance across all methods. However, this improvement tends to plateau beyond a certain threshold, indicating diminishing returns as the number of averaged checkpoints increases.
%
Most significantly, our proposed method consistently outperforms all other baselines across all experimental settings. Remarkably, even with only $K=10$ checkpoints used for averaging, our method achieves superior results compared to competing approaches that utilize $K=100$ checkpoints. This highlights our approach's efficiency and robustness, as it can deliver significant improvements with a substantially smaller computational footprint. These results demonstrate the scalability and practicality of our method in scenarios where resource efficiency is critical.


\subsection{Image Classification}
\textbf{Experimental Setups.} 
To evaluate our method's performance in image classification, we utilize the CIFAR-100 dataset and the ResNet architecture. With its diverse set of 100 classes, the CIFAR-100 dataset provides a challenging benchmark for image classification tasks. We use classification accuracy on the test dataset as the primary performance metric.
%
In our experiments, we utilize intermediate model checkpoints saved during the final stage of training, specifically after 10,000 training steps. 
Performance is evaluated at intervals of 100 checkpoints, with the number of checkpoints included in the averaging procedure within each interval controlled by the hyperparameter $K$.
This flexibility allows us to adjust the extent of checkpoint aggregation and analyze its impact comprehensively.
By evaluating our method under varying levels of checkpoint averaging, derived from different intervals of the training process, this setup facilitates a robust assessment of its effectiveness across diverse configurations. 

% \textbf{Baselines.} 
% In this experimental setting, we compare our proposed method against three baselines: LAWA \citep{sanyal2023early}, the Random baseline, and the original trained checkpoints (without averaging). These baselines allow us to assess the performance improvements achieved by different checkpoint averaging strategies over a given range of checkpoints. 
% This comparison highlights the effectiveness of our method in enhancing model performance through efficient checkpoint utilization and averaging.

\textbf{Results.}
As illustrated in Figure \ref{fig:cifar100}, all baselines outperform the original SGD optimizer, underscoring the effectiveness of weight averaging in enhancing model performance. Additionally, weight averaging accelerates model convergence, with all baselines reaching performance levels that SGD requires $17$ steps to achieve.
Our SeWA method consistently delivers the best performance, demonstrating its effectiveness. Beyond $17$ steps, where the model approaches convergence, further improvement becomes minimal, as the checkpoints at this stage share highly similar weights.

\subsection{Text Classification}

\textbf{Experimental Setups.}
For the text classification task, we utilize the AG News corpus, a widely used benchmark dataset containing news articles categorized into four distinct classes. The classification is performed using a transformer-based architecture, which is known for its effectiveness in handling natural language processing tasks.
%
To preprocess the dataset, we tokenize the entire corpus using the \textit{basic\_english} tokenizer. Any words not found in the vocabulary are replaced with a special token, \textit{UNK}, to handle out-of-vocabulary terms. This preprocessing ensures that the dataset is standardized and ready for training.
%
We save intermediate model checkpoints throughout the training process, starting from the initial stages. From this set of checkpoints, we systematically select every 100th checkpoint for inclusion in the averaging process. The hyperparameter $K$ controls the total number of checkpoints used for averaging, allowing flexible experimentation with different levels of checkpoint aggregation. This design enables us to thoroughly evaluate the impact of checkpoint averaging on the model’s performance.

% \textbf{Baselines.}
% In this experimental setting, we compare our proposed method against three baselines: LAWA \citep{sanyal2023early}, the Random baseline, and the original trained checkpoints without any averaging. These baselines provide a robust framework for assessing the performance improvements achieved through various checkpoint averaging strategies.

\textbf{Results.}
As shown in Figure \ref{fig:ag}, the improvement of weight averaging over the SGD baseline is minimal for relatively simple tasks, primarily serving to stabilize training. However, our SeWA method consistently achieves the best results regardless of task complexity, demonstrating its broad applicability across diverse settings.