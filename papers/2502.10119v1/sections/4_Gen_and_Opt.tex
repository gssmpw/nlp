\section{Generalization Analysis of \method{}}
%\subsection{Generalization Analysis}\ls{it seems that there is no section 4.2.}
This section provides two theorems that give upper bounds on generalization in the convex and non-convex settings, respectively. First, a critical lemma is provided for building stability bound in the convex function setting.

\begin{lemma}\label{convex-basic}
Let $\bar{w}_{T}^K$ and $\bar{w}_{T}^{K\prime}$ denote the corresponding outputs of \method{} after running $T$ steps on the datasets $S$ and $S^{\prime}$, which have $n$ samples but only one different. Assume that function $F(\cdot,z)$ satisfies Assumptions \ref{$L$-Lipschitz} and \ref{Convex function} for a fixed example $z\in\mathcal{Z}$, then we have
 \begin{equation}\label{eq_convex-basic}
\mathbb{E}\vert F(\bar{w}_{T}^K;z)-F(\bar{w}_T^{K\prime};z)\vert \leq sL \mathbb{E} [\bar{\delta}_T],
 \end{equation} 
where $s=\sup_{T-k+1\leq i\leq T} s_i$, where $s_i$ denotes the probability of $m_i=1$ and $\bar{\delta}_T=\frac{1}{k}\sum_{i=T-k+1}^{T}\Vert w_i-w_i^{\prime}\Vert$. 
\end{lemma}

\begin{proof} We establish a generalization bound for the algorithm based on stability, where the $L$-Lipschitz transforms the problem into bounding the parameter differences. 
  \begin{equation*}
  \begin{aligned}
  &\mathbb{E}_{z,m,A}\vert F(\bar{w}_{T}^K;z)-F(\bar{w}_T^{K\prime};z) \vert \leq \!L\mathbb{E}_{m,A}\Vert \bar{w}_{T}^K - \bar{w}_T^{K\prime}\Vert\\
  & \leq L \!\left(\!\frac{1}{k}\!\sum_{i=T-k+1}^{T}\!\!s_i \mathbb{E}_{A}\Vert w_i-w_i^{\prime}\Vert+\frac{1}{k}\!\sum_{i=T-k+1}^{T}\!\!(1-s_i)\!\cdot \!0\!\right)\\
  & \leq sL \mathbb{E}_{A} [\bar{\delta}_T], 
  \end{aligned}
 \end{equation*}
where the first inequality comes from $L$-Lipschitz assumption, the second inequality is based on taking the expectation for mask $m_i$, and the last inequality because of $s=\sup_{T-k+1\leq i\leq T} s_i$.
\end{proof}

The Lemma \ref{convex-basic} further decomposes the problem of selecting points for averaging within the last $k$ steps into averaging over the last $k$ steps multiplied by the probability $s_i$ of each step by taking an expectation over the mask. Next, we give the generalization bound for \method{} in the convex setting combined with Lemma \ref{convex-basic}.

\begin{theorem}\label{thm:stability-conv}
Suppose that we run \method{} with constant step sizes $\alpha \leq \frac{2}{\beta}$ for $T$ steps, where each step samples $z \in \mathcal{Z}$ uniformly with replacement. If function $F$ satisfies Assumptions \ref{$L$-Lipschitz}, \ref{beta-smooth} and \ref{Convex function}. \method{} has uniform stability of
\begin{equation}
  \epsilon_{gen} \leq \frac{2\alpha L^2 s}{n} \left(T - \frac{k}{2} \right),
 \end{equation}
where $s=\sup_{T-k+1\leq i\leq T} s_i$.
\end{theorem}

\begin{proof}[Proof sketch]
We can first establish stability bounds for the last $k$ points of the averaging algorithm and then use Lemma \ref{convex-basic} to obtain bounds for \method{}. \\
First, based on Eq.~\eqref{pro-FWA-update}, we divide cumulative gradient into two parts: $\Vert \nabla F(w_{T-1},z_{T}) - \nabla F(w^{\prime}_{T-1},z_{T}) \Vert$ and $\frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha_i \Vert \nabla F(w_{i-1},z_{i}) - \nabla F(w^{\prime}_{i-1},z_{i}) \Vert$. \\
{\bf(1)} Bound $\Vert \nabla F(w_{T-1},z_{T}) - \nabla F(w^{\prime}_{T-1},z_{T}) \Vert$. On one hand, we consider the different samples $z$ and $z^{\prime}$ is selected with probability $\frac{1}{n}$, we only need to use $L$ to bound $\nabla F(w_{T-1},z_{T})$ and $\nabla F^{\prime}(w^{\prime}_{T-1},z_{T}^{\prime})$ respectively. On the other hand, with probability $1-\frac{1}{n}$ that the same samples $z=z^{\prime}$ is selected, we can use the \emph{non}-expansive update rule from Lemma \ref{lemma}, based on the fact that the objective function is convex and $\alpha\leq\frac{2}{\beta}$. In summary, $\Vert \nabla F(w_{T-1},z_{T}) - \nabla F^{\prime}(w_{T-1}^{\prime},z_{T}) \Vert \leq \frac{2\alpha_T L}{k}$. \\
{\bf(2)} Then we consider bounding the cumulative gradient $\frac{1}{k}\sum_{i=T-k+1}^{T-1} \alpha_i \Vert \nabla F(w_{i-1},z_{i}) - \nabla F(w^{\prime}_{i-1},z_{i}) \Vert$. Since each step $i\in [T-k+1,\cdots, T-1]$ executes sampling with replacement, we can bound them in the way above. Then, we get $\frac{1}{k}\sum_{i=T-k+1}^{T-1}\alpha_i \Vert\nabla F(w^{\prime}_{i-1},z_i) - \nabla F(w_{i-1},z_i) \Vert \leq \frac{2L}{nk}\sum_{i=T-k+1}^{T-1}\alpha_i.$ \\
Second, by merging the above two results and taking summation over $T$ steps, we get $\mathbb{E}\left[\bar{\delta}_{T}\right] \leq (1 \!-\!\frac{1}{n})\bar{\delta}_{T-1}\! +\! \frac{1}{n}\left(\bar{\delta}_{T-1}+\frac{2\alpha_T L}{k}\right)\! +\! \frac{2L}{nk}\sum_{i=T-k+1}^{T-1}\alpha_i \! \leq \! \frac{2\alpha L^2}{n} \left( T - \frac{k}{2} \right)$,
 %    \begin{equation}
 %  \begin{aligned}
 %    \mathbb{E}\left[\bar{\delta}_{T}\right] &\leq (1-\frac{1}{n})\bar{\delta}_{T-1} + \frac{1}{n}\left(\bar{\delta}_{T-1}+\frac{2\alpha_T L}{k}\right) + \frac{2L}{nk}\sum_{i=T-k+1}^{T-1}\alpha_i  \leq \frac{2\alpha L^2}{n} \left( T - \frac{k}{2} \right),
 %  \end{aligned}
 % \end{equation}
where let $\alpha_i=\alpha$ and substitute it to Eq.~\eqref{eq_convex-basic} yields the desired result.
We leave the proof in Appendix \ref{proof-thm-con-with}.
\end{proof}

\begin{remark}
Theorem \ref{thm:stability-conv} shows that the \method{} algorithm has a sharper bound of $\frac{2\alpha L^2 s}{n} \left(T - \frac{k}{2} \right)$ under the convex assumption than the bound $\frac{2\alpha L^2 T}{n}$ for SGD given by \citet{hardt2016train}. The reason for improving the generalization comes from two main sources: (1) the last $k$ point averaging algorithm improves the SGD bound $\mathcal{O}(T/n)$ to $\mathcal{O}((T-k/2)/n)$, where $k$ is the number of averages, and this result degenerates to the SGD bound when $k = 1$. (2) the \method{} algorithm further improves the bound $\mathcal{O}((T-k/2)/n)$ to $s$ times its size, where $0\leq s \leq 1$ is the probability of $m = 1$.   
\end{remark}

\begin{remark}
The $k$ in Theorem \ref{thm:stability-conv} implies that the more checkpoints involved in the averaging in \method{}, the better the generalization performance. The sparse parameter $s$ selects models, aiming to achieve better generalization with fewer averaged models. However, these two aspects are not contradictory. For a lower sparsity rate, \method{} selects more models, leading to improved generalization performance, a trend validated in our experiments in Section \ref{sec:Exp}.
\end{remark}

\begin{lemma}\label{nonconvex-basic}
Let $\bar{w}_{T}^K$ and $\bar{w}_{T}^{K\prime}$ denote the corresponding outputs of \method{} after running $T$ steps on the datasets $S$ and $S^{\prime}$, which have $n$ samples but only one different. Assume that function $F(\cdot,z)$ satisfies Assumption \ref{$L$-Lipschitz} for a fixed example $z\in\mathcal{Z}$ and every $t_0 \in \{1,\cdots,n\}$, then we have
\begin{equation}\label{eq_nonconvex-basic}
\mathbb{E}\vert F(\bar{w}_{T}^K;z)-F(\bar{w}_{T}^{K\prime};z)\vert \leq \frac{t_0}{n} + sL \mathbb{E}\left[\bar{\delta}_{T}\vert \bar{\delta}_{t_0}=0\right],
\end{equation}   
where $s=\sup_{T-k+1\leq i\leq T} s_i$, where $s_i$ denotes the probability of $m_i=1$ and $\bar{\delta}_T=\frac{1}{k}\sum_{i=T-k+1}^{T}\Vert w_i-w_i^{\prime}\Vert$.
\end{lemma}

\begin{proof} By taking expectation for $m_i$, we split the proof of Lemma \ref{nonconvex-basic} into two parts.\\
In the first part, for $t_0 \in \{1,\cdots,n\}$, we discuss that different samples $z$ and $z^{\prime}$ can be selected only after step $t_0$. Then the inequality $\mathbb{E}\vert F(\bar{w}_{T}^K;z)-F(\bar{w}_{T}^{K\prime};z)\vert \leq \frac{t_0}{n} + L \mathbb{E}\left[\Vert \bar{w}_{T}^K - \bar{w}_{T}^{K\prime}\Vert\vert \Vert \bar{w}_{t_0}^K - \bar{w}_{t_0}^{K\prime}\Vert=0\right]$ will be obtained. One can find further proof details in Appendix \ref{proof-noncon-basic}.\\
Secondly, we take expectation for the $m_i$ of $\Vert \bar{w}_{T}^K \!-\! \bar{w}_{T}^{K\prime}\Vert$, which is similar to the proof of Lemma \ref{convex-basic}.
\end{proof}

\begin{theorem}\label{thm:stability-non-with}
Suppose we run \method{} with constant step sizes $\alpha \leq \frac{c}{T}$ for $T$ steps, where each step samples $z$ from $\mathcal{Z}$ uniformly with replacement. Let function $F\in[0,1]$ satisfies Assumptions \ref{$L$-Lipschitz} and \ref{beta-smooth}. \method{} has uniform stability of
\begin{equation}\label{result-5.3}
  \epsilon_{gen}\leq \frac{1+\frac{1}{c\beta}}{n-1}\left(2csL^2(1+ke^{c\beta})k^{-1}\right)^{\frac{k}{c\beta+k}}\cdot T^{\frac{c\beta}{c\beta+k}},
 \end{equation}
where $s=\sup_{T-k+1\leq i\leq T} s_i$.
\end{theorem}

\begin{proof}[Proof sketch]
In the non-convex setting, we finish the task using Lemma \ref{nonconvex-basic}. First, dividing cumulative gradient in the first stage is the same as in the convex case, except that we use the $(1+\alpha\beta)$-expansive properties (Lemma \ref{lemma}) to bound each $\Vert \nabla F(w_{i-1},z_{i}) - \nabla F(w^{\prime}_{i-1},z_{i}) \Vert$ here. \\
{\bf(1)} We bound each $\Vert \nabla F(w_{i-1},z_{i}) - \nabla F(w^{\prime}_{i-1},z_{i}) \Vert$ in the expectation with probabilities of $1/n$ and $1-1/n$, respectively. And combining $L$-Lipschitz conditions and $(1+\alpha\beta)$-expansive, we have $(2L(1+\alpha\beta)^{i-2})/n$. Then, bounding the cumulative gradient based on the above, we transform it into a problem of summing a finite geometric series. \\
{\bf(2)} We provide a key Lemma \ref{Lemma_noncon}, which helps us to obtain a recurrence relation for $\bar{\delta}_T$ and $\bar{\delta}_{T-1}$ in non-convex. We leave the details in Appendix \ref{proof-Lemma_noncon}. \\
Second, taking summation form $t_0$ to $T$, we get 
\begin{equation*}
 \mathbb{E}\left[\bar{\delta}_{T}\right] \leq \frac{2L(1+ke^{c\beta})}{(n-1)\beta} \cdot \left(\frac{T}{t_0}\right)^{\frac{c\beta}{k}}.   
\end{equation*} \\
Finally, we get $t_0$ by minimizing the Eq.~\eqref{with-con} and plug it into Eq.~\eqref{eq_nonconvex-basic}. We finish the proof and leave the details of this proof in Appendix \ref{proof-thm-non-with}.
\end{proof}

\begin{remark}
Under the non-convex assumption, Theorem \ref{thm:stability-non-with} shows that \method{} has bound $\mathcal{O}(T^{c\beta/(c\beta+k)}/n)$ compared to the bound $\mathcal{O}(T^{c\beta/(c\beta+1)}/n)$ for SGD in \citet{hardt2016train}, again showing its ability to improve generalization significantly. Although the number $k$, closely related to the number of iterations $T$, seems to dominate the result, the direct influence of parameter $s$ on the entire stationary bound also plays a crucial role. 
\end{remark}

\begin{remark}
The assumption that $F(w;z) \in [0,1]$ in Theorem \ref{thm:stability-non-with} is adopted for simplicity. Removing this condition does not affect the final results, as it merely introduces a constant scaling factor. The same setting is commonly used and discussed in \citet{hardt2016train,xiao2022stability}. 
\end{remark}

\section{Practical \method{} Implementation}
Although the \method{} algorithm has simpler expressions, the difficulty is learning the mask $m_i$. Inspired by tasks such as coreset selection \cite{coreset}, the discrete problem is relaxed to a continuous one. We first formulate weight selection into the following discrete optimization paradigm:
\begin{equation}\label{dis-opt}
    \min_{m\in C}F(m)=L\left(\textbf{w}(m)\right)=\frac{1}{n}\sum_{i=1}^{n}l\left(f(x_i;\textbf{w}(m),y_i) \right),
\end{equation}
where $C=\left\{\textbf{m}: m_i = 0 \,\textbf{or}\, 1, \Vert \textbf{m}\Vert_0\leq K \right\}$ and $\textbf{w}(m)=\frac{1}{K} \sum_{i=T-k+1}^{T} m_{i}w_{i}$.

To transform the discrete Eq.~\eqref{dis-opt} into a continuous one, we treat each mask $m_i$ as an independent binary random variable and reparameterize it as a Bernoulli random variable, $m_i \sim \text{Bern}(s_i)$, where $s_i \in [0, 1]$ represents the probability of $m_i$ taking the value 1, while $1-s_i$ corresponds to the probability of $m_i$ being 0. Consequently, the joint probability distribution of $m$ is expressed as $p(m\vert s) = \prod _{i=1}^{n}(s_i)^{m_i}(1 - s_i)^{1 - m_i}$. Then, the feasible domain of the target Eq.~\eqref{dis-opt} approximately becomes $\hat{C}=\left\{s: 0\leq s\leq 1, \Vert s\Vert_1\leq K \right\}$ since $\mathbb{E}_{m_i \sim p(m\vert s)}\Vert m\Vert_0 = \sum_{i=1}^{n}si$. As in the previous definition, $K>0$ in $\hat{C}$ is a constant that controls the size of the feasible domain. Then, Eq.~\eqref{dis-opt} can be naturally relaxed into the following excepted loss minimization problem:
\begin{equation}\label{E-dis-opt}
    \min_{s\in \hat{C}}F(s)=\mathbf{E}_{p(m|s)}L\left(\textbf{w}(m)\right),
\end{equation}
where $\hat{C}=\left\{s: 0\leq s\leq 1, \Vert s\Vert_1\leq K \right\}$.

Optimizing Eq.~\eqref{E-dis-opt} involves discrete random variables, which are non-differentiable.
One choice is using Policy Gradient Estimators (PGE) such as 
the REINFORCE algorithm \citep{williams1992simple,sutton1999policy} to bypass the back-propagation of discrete masks $m$,
\begin{equation*}
    \nabla_s F(s)=\mathbf{E}_{p(m|s)} L\left(\textbf{w}(m)\right) \nabla_s \log p( m \mid s).
\end{equation*}
However, these algorithms suffer from the high variance of computing the expectation of the objective function, hence may lead to slow convergence or sub-optimal results.

To address these issues, we resort to the reparameterization trick using Gumbel-softmax sampling \citep{jang2017categorical,maddison2017the}.
Instead of sampling discrete masks $m$, we get continuous relaxations by,
\begin{equation}\label{eq:gs-sample}
\small \!\!\!\!\!  \tilde{m}_i = \frac{\exp((\log s_i + g_{i, 1}) / t)}{\exp((\log s_i + g_{i, 1}) / t) + \exp((\log (1 - s_i) + g_{i, 0}) / t)},
\end{equation}
for $i = 1, \dots, k$, where $g_{i, 0}$ and $g_{i, 1}$ are i.i.d. samples from the $\text{Gumbel}(0, 1)$ distribution. The hyper-parameter $t > 0$ controls the sharpness of this approximation. When it reaches zero, i.e., $t \to 0$, $\tilde{m}$ converges to the true binary mask $m$. During training, we maintain $t > 0$ to make sure the function is continuous. For inference, we can sample from the Bernoulli distribution with probability $s$ to get sparse binary masks. In practice, the random variables $g \sim \text{Gumbel}(0, 1)$ can be efficiently sampled from,
\begin{equation*}
    g = - \log ( - \log (u)), \quad u \sim \text{Uniform}(0, 1).
\end{equation*}
For simplicity, we denote the Gumbel-softmax sampling in Eq.~\eqref{eq:gs-sample} as $\tilde{m} = \text{GS}(s, u, t)$, where $u \sim \text{Uniform}(0, 1)$. Replacing the binary mask $m$ in Eq.~\eqref{E-dis-opt} with the continuous relaxation $\tilde{m}$, the optimization problem becomes,
\begin{equation*}
    \min_{s\in \hat{C}}F(s)=\mathbf{E}_{u \sim \text{Uniform(0, 1)}} L\left(\textbf{w}(\text{GS}(s, u, t)\right),
\end{equation*}
where $\hat{C}=\left\{s: 0\leq s\leq 1, \Vert s\Vert_1\leq K \right\}$. The expectation can be approximated by Monte Carlo samples, i.e.,
\begin{equation*}
    \min_{s\in \hat{C}} \hat{F}(s)= \frac{1}{M} \sum_{m=1}^M L\left(\textbf{w}(\text{GS}(s, u^{(m)}, t)\right),
\end{equation*}
where $u^{(m)}$ are i.i.d. samples drawn from $\text{Uniform}(0, 1)$.
Empirically, since the distribution of $u$ is fixed, this Monte Carlo approximation exhibits low variance and stable training \cite{kingma2013auto,rezende2014stochastic}. Furthermore, since Eq.~\eqref{eq:gs-sample} is continuous, we can optimize it using back-propagation and gradient methods.

\begin{algorithm2e}[t]
    \caption{Selected Weight Average (\method{})}\label{alg:gs}
    \KwIn{Checkpoints $\textbf{w}$, hyper-parameter $t$}
    \KwInit{Mask probability $s$\;}
    \For{$i = 1, \dots, max\_iteration$}{
    \tcc{Gumbel-softmax sampling}
    \For{$m = 1, \dots, M$}{
        Sample $u^{(m)} \sim \text{Uniform}(0, 1)$\;
        Compute $L\left(\textbf{w}(\text{GS}(s, u^{(m)}, t)\right)$\;
    }
    \tcc{Learning mask probability}
    Optimize $\hat{F}(s)= \frac{1}{M} \sum_{m=1}^M L\left(\textbf{w}(\text{GS}(s, u^{(m)}, t)\right)$\;
    % Truncation for sparsity??\;
    }
    \KwOut{Mask $m$ based on $K$ largest probabilities in $s$}
\end{algorithm2e}

\begin{remark}
\method{} adaptively selects useful checkpoints, which implies that it does not require the extra cost associated with manual design and avoids model biases introduced by prior knowledge, thereby making our approach applicable to a broader range of tasks. In the following experiments, \method{} algorithm demonstrates particular suitability for scenarios characterized by unstable training trajectories, such as behavior cloning. By leveraging checkpoint averaging, \method{} effectively stabilizes the training process, mitigating fluctuations and enhancing overall performance.
\end{remark}
