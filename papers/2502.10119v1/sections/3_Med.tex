\section{Methodology}
\label{sec:method}
In  this section, we first give the problem setup and then introduce our proposed \method{} and several terminologies.

\subsection{Problem Setting}
Let $F(w, z)$ be a loss function that measures the loss of the predicted value of the parameter $w$ at a given sample $z$. There is an unknown distribution $\mathcal{D}$ over examples from some space $\mathcal{Z}$, and a sample dataset $S=(z_1, z_2,..., z_n)$ of $n$ examples i.i.d. drawn from $\mathcal{D}$. Then the \emph{population risk} and \emph{empirical risk} are defined as 
\begin{equation}
\textbf{Population Risk:}\quad   \min_w \{R_\mathcal{D}[w]=E_ {z \sim \mathcal{D}}F(w; z) \} \label{prm}
\end{equation}
\begin{equation}
\textbf{Empirical Risk:}\quad  \min_w \{ R_S [w]= \frac{1}{n} \sum_{i=1}^{n}F(w; z_i) \}.\label{erm}
\end{equation}
 The generalization error of a model $w$ is the difference $\epsilon_{gen}=R_\mathcal{D}[w] - R_S [w]$. Moreover, we assume function $F$ satisfies the following \emph{Lipschitz} and \emph{smoothness} assumption.


\begin{assumption}[$L$-Lipschitz]
\label{$L$-Lipschitz}
A differentiable function $F: R^d \rightarrow R$ satisfies the $L$-Lipschitz property, i.e., for $\forall u, v \in R^d, \Vert F(u)-F(v)\Vert \leq L\Vert u-v\Vert$, which implies that the gradient satisfies $\Vert\nabla F(u)\Vert \leq L$.
\end{assumption}

\begin{assumption}[$\beta$-smooth]
\label{beta-smooth}
A differentiable function $F: R^d \rightarrow R$ is $\beta$-smooth, i.e., for $\forall u, v \in R^d$, we have $\Vert\nabla F(u)-\nabla F(v)\Vert \leq \beta\Vert u-v\Vert$.%, where $\nabla F(v)$ denotes the gradient of $F$ at $v$. 
\end{assumption}

Assumptions \ref{$L$-Lipschitz} and \ref{beta-smooth} are often used to establish stability bounds for algorithms and are crucial conditions for analyzing the model's generalization performance. 

\begin{assumption}[Convex function]
\label{Convex function}
A differentiable function $F: R^d \rightarrow R$ is convex, i.e., for $\forall u, v \in R^d, F(u) \leq F(v) + \langle\nabla F(u), u-v \rangle.$
\end{assumption}

Different function assumptions correspond to different expansion properties, which will be discussed in Lemma \ref{lemma}.

\subsection{SGD and \method{} Algorithm}
For the target function $F$ and the given dataset $S=(z_1, z_2, \cdots, z_n)$, we consider the SGD's general update rule as Eq.~\eqref{SGD-rules}. \method{} algorithm adaptively selects a small number of points for averaging among the last k points of the SGD's training trajectory. It is shown in Eq.~\eqref{\method{}-rules}.

\textbf{SGD} is formulated as
    \begin{equation}\label{SGD-rules}
     w_{t+1} = w_{t} - \alpha \nabla_w F(w_{t},z_{i_t}),
    \end{equation}   
where $\alpha$ is the fixed learning rate, $z_{i_t}$ is the sample chosen in iteration $t$. We choose $z_{i_t}$ from dataset $S$ in a standard way, picking $i_t \sim Uniform\left\{1, \cdots, n \right\}$ at each step. This setting is commonly explored in analyzing the stability \cite{hardt2016train,xiao2022stability}. 

\textbf{\method{}} is formulated as 
\begin{equation}\label{\method{}-rules}
    \bar{w}^{K}_{T}=\frac{1}{K} \sum_{i=T-k+1}^{T} m_{i}w_{i},
\end{equation}
where the mask $m_i \in \left\{0, 1 \right\}$ and $m_i = 1$ indicating the $i$-th weight is selected for averaging and otherwise excluded; the $K = k_{m_i=1}$ corresponds to the number of models selected for averaging. The \method{} algorithm focuses on adaptively selecting a small number of points from the last $k$ steps of the SGD training trajectory for averaging, aiming to improve generalization and accelerate convergence. We show the convergence performance of different
models in Figure \ref{fig:enter-label}.
\begin{figure}
    \centering
    \includegraphics[width=1.\linewidth]{compare.pdf}
    \caption{Comparison of \method{} with different models on convergence performance.}
    \label{fig:enter-label}
\end{figure}

\subsubsection{The Expansive Properties}
\begin{lemma}\label{lemma}
Assume that the function $F$ is $\beta$-smooth. Then, \\
{\bf (1). (non-expansive)} If $F$ is convex, for any $\alpha \leq \frac{2}{\beta}$, we have $\Vert w_{T+1}-w_{T+1}^{\prime} \Vert \leq \Vert w_{T}-w_{T}^{\prime}\Vert$; \\
{\bf (2). ($(1\!+\!\alpha\beta)$-expansive)} If $F$ is non-convex, for any $\alpha$, we have $\Vert w_{T+1}\!-\!w_{T+1}^{\prime} \Vert \!\leq\! (1\!+\!\alpha\beta)\Vert w_{T}\!-\!w_{T}^{\prime}\Vert$.
\end{lemma}

Lemma \ref{lemma} tells us that the gradient update becomes $non$-expansive when the function is convex and the step size is small, which implies that the algorithm will always converge to the optimum in this setting. However, although this is not guaranteed when the function is non-convex, it is required that the gradient updates cannot be overly expansive if the algorithm is stable. The proof of Lemma \ref{lemma} is deferred to Appendix \ref{pro-lemma}. Additional relevant results can be found in \citet{hardt2016train,xiao2022stability}. 

\subsection{Stability and Generalization Definition}
\citet{hardt2016train} link the \emph{uniform stability} of the learning algorithm with the expected generalization error bound in research of SGD's generalization. The expected generalization error of a model $w = A_S$ trained by a certain randomized algorithm $A$ is defined as 
    \begin{equation}\label{D-gen}
\mathbb{E}_{S,A}\left[R_{S}\left[A_S\right]-R_\mathcal{D}\left[A_S\right]\right]. 
    \end{equation}
Here, the expectation is taken only over the internal randomness of $A$. 
Next, we introduce the \emph{uniform stability}.

\begin{definition}[$\epsilon$-Uniformly Stable]
A randomized algorithm $A$ is $\epsilon$-uniformly stable if for all data sets $S, S^{\prime} \in Z^{n}$ such that $S$ and $S^{\prime}$ differ in at most one example, we have
    \begin{equation}\label{E-stab}
     \mathop{sup}\limits_{z\in Z}\left\{\mathbb{E}_{A}\left[F(A_S;z)-F(A_{S^{\prime}};z)\right] \right\} \leq \epsilon.
    \end{equation}
\end{definition}

\begin{theorem}{\rm (Generalization in Expectation \citep[Theorem 2.2]{hardt2016train})}
Let $A$ be $\epsilon$-uniformly stable. Then,
    \begin{equation}\label{E-gen}
     \vert\mathbb{E}_{S,A}\left[R_{S}\left[A_S\right]-R_\mathcal{D}\left[A_S\right]\right]\vert \leq \epsilon.
    \end{equation}
\end{theorem}

This theorem clearly states that if an algorithm has uniform stability, then its generalization error is small. In other words, uniform stability implies \emph{generalization in expectation} \cite{hardt2016train}. Above proof is based on \citet[Lemma 7]{bousquet2002stability} and very similar to \citet[Lemma 11]{shalev2010learnability}.

