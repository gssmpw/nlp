\section{Related Work}
\label{sec:ReW}

\textbf{Weight averaging algorithm.}
Model averaging methods, initially introduced in convex optimization \cite{ruppert1988efficient, polyak1992acceleration,li2023deep}, have been widely used in various areas of deep learning and have shown their advantages in generalization and convergence. Subsequently, with the introduction of SWA \cite{izmailov2018averaging}, which averages the weights along the trajectory of SGD, the model's generalization is significantly improved. Further modifications have been proposed, including the Stochastic Weight Average Density (SWAD) \cite{cha2021swad}, which averages checkpoints more densely, leading to the discovery of flatter minima associated with better generalization. Additionally, Trainable Weight Averaging (TWA) \cite{li2022trainable} has improved the efficiency of SWA by employing trainable averaging coefficients. What's more, other approaches like Exponential Moving Average (EMA) \cite{szegedy2016rethinking} and finite averaging algorithms, such as LAWA \cite{kaddour2022stop, sanyal2023early}, which averages the last $k$ checkpoints from running a moving window at a predetermined interval, employ different strategies to average checkpoints. These techniques have empirically shown faster convergence and better generalization. In meta-learning, Bayesian Model Averaging (BMA) is used to reduce the uncertainty of the model \cite{huang2020meta}. However, these different algorithms often require manual design of averaging strategies and are only applicable to some specific tasks, imposing an additional cost on the training.

\textbf{Stability Analysis.}
Stability analysis is a fundamental theoretical tool for studying the generalization ability of algorithms by examining their stability \citep{devroye1979distribution, bousquet2002stability, mukherjee2006learning, shalev2010learnability}. Based on this, \citet{hardt2016train} use the algorithm stability to derive generalization bounds for SGD, inspiring series works \cite{charles2018stability, zhou2018generalization, yuan2019stagewise, lei2020sharper}. This analysis framework has been extended to various domains, such as online learning \citep{yang2021simple}, adversarial training \citep{xiao2022stability}, decentralized learning \citep{zhu2023stability}, and federated learning \citep{sun2023understanding, sun2023mode}. Although uniform sampling is a standard operation for building stability boundaries, selecting the initial point and sampling without replacement also significantly affects generalization and has been investigated in \citet{shamir2016without, kuzborskij2018data}. For the averaging algorithm, \citet{hardt2016train} and \citet{xiao2022stability} analyze the generalization performance of SWA and establish stability bounds for the algorithm under the setting of convex and sampling with replacement. The main focus of this paper is the generalization and construction of stability bounds for \method{} in both convex and non-convex settings.

\textbf{Mask Learning.}
The general approach involves transforming the discrete optimization problem into a continuous one using probabilistic reparameterization, thereby enabling gradient-based optimization. \citet{coreset} solve the coreset selection problem based on this by using a Policy Gradient Estimator (PGE) for a bilevel optimization objective. \citet{zhangefficient} propose a probabilistic masking method that improves diffusion model efficiency by skipping redundant steps.
While the PGE method may suffer from high variance and unstable training, we solve the mask learning problem using the Gumbel-softmax reparameterization \citep{jang2017categorical,maddison2017the}.
In this paper, we aim to adaptively select checkpoints for averaging to enhance model generalization and convergence, addressing the issue of unstable training.