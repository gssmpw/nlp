\section{Introduction}
We consider the task of sampling from unnormalised densities $\pi = \frac{\rho}{Z} $, where $Z := \int \rho(x) \dif x$ denotes the partition function.
This task is fundamental in probabilistic modelling and scientific simulations, with broad applications in Bayesian inference \citep{neal1993probabilistic}, nuclear physics \citep{albergo2019flow}, drug discovery \citep{xie2021mars}, and material design \citep{komanduri2000md}.
However, achieving efficient sampling remains challenging, especially when dealing with high-dimensional and multi-modal distributions.


Conventional sampling methods rely on Markov Chain Monte Carlo (MCMC) \citep{neal2012mcmc}, which requires long convergence times and the simulation of extended chains to obtain uncorrelated samples. 
Neural samplers address these issues by approximating the target distribution using generative models, such as normalizing flows \citep{midgley2022flow} and latent variable models \citep{he2024training}.
Building on the success of diffusion models, a diffusion-based sampler \citep{AkhoundSadegh2024IteratedDE} has been introduced to train a score network that approximates the estimated score through Monte Carlo estimation. 
Concurrently, \cite{tian2024liouville} propose a flow-based sampler that learns a velocity model to satisfy the continuity equation \citep{villani2009optimal}. 
These approaches show strong empirical performance, offering more efficient and scalable alternatives to MCMC samplers.

In this work, we focus on training flow-based samplers, which present challenges due to the intractable time derivative of the logarithm of the partition function in the continuity equation. While \cite{tian2024liouville} use importance sampling to approximate it, the approach suffers from high variance, limiting its effectiveness. 
To address this, we introduce a more stable and efficient estimation method, improving both the accuracy and efficiency of flow-based samplers.
Specifically, we propose a velocity-driven sequential Monte Carlo (VD-SMC) \citep{del2006sequential} combined with control variates \citep{geffner2018using} to reduce variance. VD-SMC operates in a bootstrap manner, generating high-quality training samples while producing low-variance estimates of the time derivative. To further enhance sampling efficiency, we incorporate the shortcut model \citep{frans2024one}, a self-distillation technique that reduces the number of required sampling steps.
Empirical evaluations on both synthetic and $n$-body system targets demonstrate the effectiveness of our approach.
