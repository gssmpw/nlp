\newpage 
\appendix

\begin{center}
\LARGE
\textbf{Appendix for ``Neural Flow Samplers with \\Shortcut Models''}
\end{center}

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
{\small \tableofcontents}

\section{Importance Sampling and Sequential Monte Carlo}

This section reviews the basic Sequential Monte Carlo (SMC) algorithm. We begin by introducing importance sampling and its application to estimating the intractable time derivative $\partial_t \log Z_t$, as presented in \cite{tian2024liouville}. We then proceed with an introduction to Sequential Monte Carlo, which is employed in our methods to estimate $\partial_t \log Z_t$.

\subsection{Importance Sampling} \label{sec:appendix_is}
Consider a target distribution $\pi (x) = \frac{\rho(x)}{Z}$, where $\rho(x) \geq 0$ is the unnormalised probability density and $Z = \int \rho(x) \dif x$ denotes the normalising constant, which is typically intractable. For a test function $\phi(x)$ of interest, estimating its expectation under $\pi$ through direct sampling can be challenging.
Importance sampling (IS) \citep{kahn1950random} instead introduces a proposal distribution $q$, which is easy to sample from, and proposes an expectation estimator as follows
\begin{align}
    \E_{\pi(x)} [\phi(x)] = \frac{1}{Z} \E_{q(x)} \left[ \frac{\rho(x)}{q(x)} \phi(x) \right] = \frac{\E_{q(x)}\left[ \frac{\rho(x)}{q(x)} \phi(x) \right]}{\E_{q(x)}\left[ \frac{\rho(x)}{q(x)} \right]}.
\end{align}
Thus, the expectation can be estimated via the Monte Carlo method
\begin{align}
    \E_{\pi(x)} [\phi(x)] \approx \sum_{k=1}^K \frac{w^{(k)}}{\sum_{=1}^N w^{(j)}} \phi(x^{(k)}), \quad x^{(k)} \sim q(x),
\end{align}
where $w^{(k)} = \frac{\rho(x^{(k)})}{q(x^{(k)})}$ denotes the importance weight. While importance sampling yields a consistent estimator as $N \rightarrow \infty$, it typically suffers from high variance and low effective sample size \citep{thiebaux1984interpretation} when the proposal deviates from the target distribution. 
In theory, a zero-variance estimator can be achieved if $q(x) \propto \rho(x) \phi(x)$; however, this condition is rarely satisfied in practice. This limitation renders importance sampling inefficient in high-dimensional spaces, as a large number of Monte Carlo samples are required to mitigate the variance.

\textbf{Approximating $\partial_t \log Z_t$ with Importance Sampling.}
\cite{tian2024liouville} propose approximating $\partial_t \log Z_t$ using importance sampling, where they express $\partial_t \log Z_t \approx \sum_k \frac{w_t^{(k)}}{\sum_k w_t^{(k)}} \partial_t \log \tilde{p}_t (x_t^{(k)})$. Here $x_t^{(k)} \sim p_t (x;\theta)$ denotes the sample generated by the velocity $v_t (x;\theta)$ at time $t$, and $\log w_t^{(k)} = \int_0^t \delta_\tau (x_\tau;v_t(\cdot;\theta)) \dif \tau$. For completeness, we provide a step-by-step recall of the proof of the correctness of this estimator from \cite{tian2024liouville}.

First, we show that $\partial_t \log Z_t$ is given by the expectation over $p_t$:
\begin{align}
    \partial_t \log Z_t = \partial_t \log \int \tilde{p}_t (x) \dif x = \frac{1}{Z_t} \int \tilde{p}_t(x) \partial_t \log \tilde{p}_t(x) \dif x = \E_{p_t(x)} [\partial_t \log \tilde{p}_t (x)].
\end{align}
$\partial_t \log Z_t$ therefore can be estimated via importance sampling
\begin{align}
    \partial_t \log Z_t \approx \sum_{k=1}^K \frac{w_t^{(k)}}{\sum_j w_t^{(j)}} \partial_t \log \tilde{p}_t(x^{(k)}), \quad x^{(k)} \sim p_t (x;\theta),
\end{align}
where $w_t^{(k)} = \frac{p_t(x^{(k)})}{p_t(x^{(k)}; \theta)}$. Next, we show that the weight $\log w_t^{(k)}$ is given by integrating $\delta_t(x;v_t(\cdot;\theta))$ on $[0,t]$, where $\delta_t$ is defined in \cref{eq:nfs_loss} i.e. $\log \frac{p_t(x_t)}{p_t(x_t;\theta)} = \int_0^t \delta_s(x;v_s(\cdot;\theta)) \dif s$.

We begin by computing the instantaneous rate of change of the log-densities of the model $p_t(x_t;\theta)$ along the trajectories generated by $v_t(x_t; \theta)$, as in \cref{eq:change-of-variable}
\begin{align}
    \partial_t [\log p_t(x_t; \theta)] 
    &= \partial_t \log p_t(x_t;\theta) + \nabla_{x_t} \log p_t(x_t;\theta) \cdot \frac{\dif x_t}{\dif t} \nonumber \\
    &= (-\nabla_{x_t} \cdot v_t(x_t;\theta) - v_t(x_t;\theta) \cdot \nabla_{x_t} \log p_t(x_t;\theta)) + \nabla_{x_t} \log p_t(x_t;\theta) \cdot v_t(x_t;\theta) \nonumber \\
    &= -\nabla_{x_t} \cdot v_t(x_t; \theta).
\end{align}
Similarly, the instantaneous rate of change of the log-densities of the target $p_t(x_t)$ along the trajectories generated by $v_t(x_t; \theta)$ is
\begin{align}
    \partial_t [\log p_t(x_t)] 
    &= \partial_t \log p_t(x_t) + \nabla_{x_t} \log p_t(x_t) \cdot \frac{\dif x_t}{\dif t} \nonumber \\
    &= \partial_t \log p_t(x_t) + \nabla_{x_t} \log p_t(x_t) \cdot v_t(x_t; \theta).
\end{align}
Note that the score of the target is tractable $\nabla_x \log p_t(x) = \nabla_x \log \tilde{p}_t(x)$. Therefore, the log densities along the trajectories can be computed via
\begin{align}
    \log p_t(x_t; \theta) &= \log p_0 (x_0;\theta) - \int_0^t  \nabla_{x_s} \cdot v_s(x_s; \theta) \dif s\\
    \log p_t(x_t) &=  \log p_0 (x_0) + \int_0^t [\partial_s \log p_s(x_s) + \nabla_{x_s} \log p_s(x_s) \cdot v_s(x_s; \theta)] \dif s.
\end{align}
Since $p_0(x; \theta) = p_0(x) = \eta (x)$ due to the annealing path construction $p_t \propto \rho^t \eta^{1-t}$, we have
\begin{align}
    \log \frac{p_t(x_t)}{p_t(x_t;\theta)} 
    &= \int_0^t [ \nabla_{x_s} \cdot v_s(x_s; \theta) + \partial_s \log p_s(x_s) + \nabla_{x_s} \log p_s(x_s) \cdot v_s(x_s; \theta) ] \dif s \nonumber \\
    &= \int_0^t \delta_s(x;v_s(\cdot;\theta)) \dif s,
\end{align}
which completes the proof.

\textbf{Remark.} Although importance sampling offers an elegant method for estimating the intractable time derivatives $\partial_t \log Z_t$, it faces two main challenges. First, as previously discussed, importance sampling typically suffers from high variance and requires large sample sizes to improve the effective sample size. More critically, the computation of the weight involves the intractable term $\partial_t \log p_s(x_t)$, which in turn depends on $\partial_t \log Z_t = \E_{p_t(x)}[\partial_t \log \tilde{p}_t(x)]$. \cite{tian2024liouville} approximate this expectation naively by averaging $\partial_t \log \tilde{p}_t(x)$ over the mini-batch during training, which introduces both approximation errors and bias in the importance weights.

In the following section, we introduce Sequential Monte Carlo, which is employed in our methodology to estimate $\partial_t \log Z_t$, balancing the efficiency of the short-run MCMC driven by the velocity with the effectiveness of low variance.

\subsection{Sequential Monte Carlo} \label{sec:appendix-smc}
Sequential Monte Carlo (SMC) provides an alternative method to estimate the intractable expectation $\E_{p_t (x)} [\phi(x)]$.
Specifically, SMC decomposes the task into easier subproblems involving a set of unnormalised intermediate target distributions $\{\tilde{p}_{t_m} (x_{t_m})\}_{m=0}^M$.\footnote{We consider a discrete-time schedule that satisfies $0=t_0 < t_1 < \dots < t_m < \dots < t_{M-1} < t_M = 1$.}
We begin by introducing sequential importance sampling (SIS):
\begin{align}
    \E_{p_{t_m} (x)} [\phi(x)] 
    &= \int q(x_{t_0:t_m}) \frac{p(x_{t_0:t_m})}{q(x_{t_0:t_m})} \phi(x_{t_m}) \dif x_{t_0:t_m} \nonumber \\
    &\approx \frac{1}{K} \sum_{k=1}^K \frac{p(x_{t_0:t_m}^{(k)})}{q(x_{t_0:t_m}^{(k)})} \phi(x_{t_m}^{(k)}), \quad \text{where} \quad x_{t_0:t_m}^{(k)} \sim q(x_{t_0:t_m}^{(k)}).
\end{align}
The importance weights are $w_{t_m}^{(k)} = \frac{p(x_{t_0:t_m}^{(k)})}{q(x_{t_0:t_m}^{(k)})}$.
The key ingredients of SMC are the proposal distributions $q(x_{t_0:t_m})$ and the target distributions $p(x_{t_0:t_m})$.
Here we consider a general form associated with a sequence of forward kernels $q(x_{t_0:t_m}) = q(x_{t_0})\prod_{s=0}^{m-1} \mathcal{F}_{t_s} (x_{t_{s+1}}|x_{t_s})$, and the target distribution is defined by a sequence of backward kernels $p(x_{t_0:t_m}) = p(x_{t_m}) \prod_{s=0}^{m-1}\mathcal{B}_{t_s} (x_{t_s} | x_{t_{s+1}})$. Substituting this into the expression for the importance weights gives
\begin{align}
    w_{t_m}^{(k)} = \frac{p(x_{t_0:t_m}^{(k)})}{q(x_{t_0:t_m}^{(k)})} = \frac{p(x_{t_m})\prod_{s=0}^{m-1}\mathcal{B}_{t_s} (x_{t_s} | x_{t_{s+1}})}{q(x_{t_0})\prod_{s=0}^{m-1} \mathcal{F}_{t_s} (x_{t_{s+1}}|x_{t_s})} = w_{t_{m-1}}^{(k)} W_{t_m}^{(k)},
\end{align}
where $W_{t_m}^{(k)}$, termed the incremental weights, are calculated as,
\begin{align}
    W_{t_m}^{(k)} = \frac{p_{t_m}(x_{t_m})}{p_{t_{m-1}}(x_{t_{m-1}})} \frac{\mathcal{B}_{t_m}(x_{t_{m-1}}|x_{t_m})}{\mathcal{F}_{t_m}(x_{t_m} | x_{t_{m-1}})}.
\end{align}
By defining the backward kernel as $\mathcal{B}_{t_m} (x_{t_{m-1}}|x_{t_m}) = \frac{p_{t_{m-1}}(x_{t_{m-1}})\mathcal{F}_{t_m}(x_{t_m} | x_{t_{m-1}})}{p_{t_{m-1}} (x_{t_m})}$, the incremental weight is tractable and becomes
\begin{align}
    W_{t_m}^{(k)} = \frac{p_{t_m}(x_{{t_m}})}{p_{t_{m-1}}(x_{{t_m}})}.
\end{align}
Therefore, the expectation can be approximated as
\begin{align}
    \E_{p_{t_m}(x)} [\phi(x)] \approx \sum_k \tilde{w}^{(k)}_{t_m} \phi(x_{t_m}), \quad  \tilde{w}^{(k)}_{t_m} = \frac{{w}^{(k)}_{t_m}}{\sum_j {w}^{(j)}_{t_m}}, \quad w_{t_m}^{(k)} = w_{t_{m-1}}^{(k)} W_{t_m}^{(k)} \propto w_{t_{m-1}}^{(k)} \frac{\tilde{p}_{t_m}(x_t)}{\tilde{p}_{t_{m-1}}(x_{t_m})}. \nonumber
\end{align}
The SIS method is elegant, as the weights can be computed on the fly. However, with a straightforward application of SIS, the distribution of importance weights typically becomes increasingly skewed as $t$ progresses, resulting in many samples having negligible weights. This imbalance reduces the effective sample size and the overall efficiency of the algorithm.
To alleviate this issue, a common approach used in SMC is to introduce a resampling step. At each time step $t$, the samples $x_t^{(k)}$ are resampled using systematic resampling method based on the normalized weights $\tilde{w}_t^{(k)}$\footnote{Rather than resampling at every time step $t$, a more advanced resampling method involves making the resampling decision adaptively, based on criteria such as the Effective Sample Size \citep{doucet2001introduction}.}. 
\begin{wrapfigure}{r}{0.57\linewidth}
\vspace{-4mm}
\centering
% UGLY USE OF \vspace & \hspace follows
    \begin{minipage}[t]{\linewidth}
    \centering
        \begin{algorithm}[H]
        \setstretch{1.33}
        % \renewcommand{\thealgorithm}{} % unenables algorithm number
        \caption{Velocity-Driven SMC} \small
        \label{alg:SMC} 
        \textbf{Input}: velocity $v_t(\cdot;\theta)$; sample size $K$; time steps $\{t_m\}_{m=0}^M$ \\
        \textbf{Output}: samples and weights $\{ x_{t_m}^{(k)}, \tilde{w}_{t_m}^{(k)} \}_{k=1, m=0}^{K, M}$
        \begin{algorithmic}[1] %[1] enables line numbers
        \Procedure{VD-SMC}{$v_t, K, \{t_m\}_{m=0}^M$}
            \For{$k = 1, \dots, K$}
                \State $x_0^{(k)} \sim p_0 (x_0), \quad w_0^{(k)} = p_0 (x_0^{(k)})$
            \EndFor
            \State $\tilde{w}_0^{(k)} = w_0^{(k)} / \sum_{i=1}^K w_0^{(i)}$
            \For{$m = 1, \dots, M$}
                \For{$k = 1, \dots, K$}
                    \If{ESS($\tilde{w}_{t_{m-1}}^{1:K}$) $< \mathrm{ESS}_{\text{min}}$}
                        \State $\!\!\!a_{t_m}^{(k)} \!\sim\! \mathrm{Systematic}(\tilde{w}_{t_{m-1}}^{1:K}), \!\!\!\!\!\quad \hat{w}_{t_{m-1}}^{(k)} \!=\! 1$
                    \Else
                        \State $a_{t_m}^{(k)} = k, \quad \hat{w}_{t_{m-1}}^{(k)} = w_{t_{m-1}}^{(k)}$
                    \EndIf
                    \State $\dif t \leftarrow t_m - t_{m-1}$
                    \State $x_{t_m}^{(k)} \!=\! \mathrm{HMC}(x_{t_{m-1}}^{(a_{t_m}^{(k)})} \!\!\!+\! v_{t_{m\!-\!1}}(x_{t_{m-1}}^{(a_{t_m}^{(k)})};\theta) \dif t)$
                    \State $w_{t_m}^{(k)} = \hat{w}_{t_{m-1}}^{(k)} \frac{\tilde{p}_{t_m} (x_{t_m}^{(k)})}{\tilde{p}_{t_{m-1}} (x_{t_m}^{(k)})}$
                \EndFor
                \State $\tilde{w}_{t_m} = w_{t_m}^{(k)} / \sum_{i=1}^K w_{t_m}^{(i)}$
            \EndFor
        \EndProcedure
        \end{algorithmic}
        \end{algorithm}
    \end{minipage}
\vspace{-5mm}
\end{wrapfigure}
The resampled particles are then assigned equal weights to mitigate the bias introduced by the skewness in the weight distribution.
This resampling trick prevents the sample set from degenerating, where only a few particles carry significant weight while others contribute minimally. By periodically resampling, the algorithm maintains diversity in the particle set. It ensures that the estimation is focused on regions of high probability density, leading to less skewed importance weight distributions.
To encourage the convergence of MCMC transition kernels, we also introduce a velocity-driven step.
The implementation of the proposed velocity-driven sequential Monte Carlo (VD-SMC) is given by \cref{alg:SMC}.
Given the sample size $K$ and the time schedule $\{t_m\}_{m=1}^M$ with $t_0=0, t_M=1$, the algorithm VD-SMC returns the samples and importance weights $\{ x_{t_m}^{(k)}, \tilde{w}_{t_m}^{(k)} \}_{k=1, m=0}^{K, M}$.
Therefore, the intractable time derivative can be approximated as $\partial_t \log Z_t = \E_{p_t} \partial_t \log \tilde{p}_t(x) \approx \sum_k \tilde{w}_t^{(k)} \partial_t \log \tilde{p}_t(x_t^{(k)})$. However, as illustrated in \cref{fig:std_dt_logZt}, the estimation of $\E_{p_t} \partial_t \log \tilde{p}_t(x)$ exhibits higher variance compared to using $\E_{p_t} \xi_t(x;v_t)$. Therefore, in practice, we approximate the time derivative as $\partial_t \log Z_t = \E_{p_t} \xi_t(x;v_t(\cdot;\theta_{\mathrm{sg}})) \approx \sum_k \tilde{w}_t^{(k)} \xi_t(x_t^{(k)};v_t(\cdot;\theta_{\mathrm{sg}}))$, where $\theta_{\mathrm{sg}}$ denotes the parameters with gradients detached.


\section{Variance Reduction with Control Variates}

\subsection{Proof of \cref{eq:partial_logZ_fpi}} \label{sec:appendix-proof_partial_logZ}
Recall that in \cref{eq:partial_logZ_fpi}, we show that the following equation holds:
\begin{align}
    \!\!\partial_t \log Z_t \!\!=\!\! \argmin_{c_t} \!\E_{p_t} (\xi_t(x;v_t) \!-\! c_t)^2, \xi_t(x;v_t) \!\triangleq\! \partial_t \log \tilde{p}_t (x) \!+\! \nabla_x \!\cdot\! v_t (x) \!+\! v_t (x) \!\cdot\! \nabla_x \log p_t (x). \nonumber
\end{align}
We provide a detailed proof of this result here. First, we have the following Lemmas.
\begin{lemma}[Stein's Identity \citep{stein2004use}] \label{lemma:divergence}
    Assuming that the target density $p_t$ vanishes at infinity, i.e., $p_t(x) = 0$, whenever $\exists d$ such that $x[d] = \infty$, where $x[d]$ denotes the $d$-th element of the vector $x$. Under this assumption, we have the result: $\int [\nabla_x \!\cdot\! v_t (x) \!+\! v_t (x) \!\cdot\! \nabla_x \log p_t (x)]\tilde{p}_t (x) \dif x = 0$.  
\end{lemma}
\begin{proof}
    To prove the result, notice that
    \begin{align}
        \int [\nabla_x \!\cdot\! v_t (x) \!+\! v_t (x) \!\cdot\! \nabla_x \log p_t (x)]\tilde{p}_t (x) \dif x
        &= \int  \tilde{p}_t (x) \nabla_x \!\cdot\! v_t (x) \!+\! v_t (x) \!\cdot\! \nabla_x \tilde{p}_t (x)  \dif x \nonumber \\
        &= \int \nabla_x \cdot [v_t(x) \tilde{p}_t (x)] \dif x \nonumber \\
        &= \sum_d \int \frac{\dif}{\dif x_d} [v_t(x) \tilde{p}_t (x)][d] \dif x_d \nonumber \\
        &= \sum_d \left. [v_t(x) \tilde{p}_t (x)][d] \right|_{x_d = -infty}^{x_d = +\infty} = 0, \nonumber
    \end{align}
    where the last row applies the divergence theorem $\int_a^b f^\prime (t) \dif t = f(b) - f(a)$.
\end{proof}
\begin{lemma} \label{lemma:l2}
    Let $c_t^* = \argmin_{c_t} \E_{p_t} (\xi_t(x;v_t) \!-\! c_t)^2$, then $c_t^* = \E_{p_t}\xi_t(x;v_t)$.
\end{lemma}
\begin{proof}
    To see this, we can expand the objective
    \begin{align}
        \mathcal{L}(c_t) 
        = \E_{p_t} (\xi_t(x;v_t) \!-\! c_t)^2 
        = c_t^2 - 2c_t\E_{p_t} \xi_t(x;v_t) + \mathrm{c}
        = (c_t - \E_{p_t} \xi_t(x;v_t))^2 + \mathrm{c}', \nonumber
    \end{align}
    where $c, c'$ are constants w.r.t. $c_t$. Therefore $c_t^* \!=\! \argmin_{c_t}\! \E_{p_t} (\xi_t(x;v_t) \!-\! c_t)^2 \!=\! \E_{p_t}\xi_t(x;v_t)$.
\end{proof}
Now, it is ready to prove \cref{eq:partial_logZ_fpi}. Specifically,
\begin{align}
    c_t^* 
    &= \E_{p_t}\xi_t(x;v_t) \nonumber \\
    &= \frac{1}{\int \tilde{p}_t(x) \dif x} \int \tilde{p}_t(x) [\partial_t \log \tilde{p}_t (x) \!+\! \nabla_x \!\cdot\! v_t (x) \!+\! v_t (x) \!\cdot\! \nabla_x \log p_t (x)] \dif x \nonumber \\
    &= \frac{1}{\int \tilde{p}_t(x) \dif x} \int \partial_t \tilde{p}_t (x) + [\nabla_x \!\cdot\! v_t (x) \!+\! v_t (x) \!\cdot\! \nabla_x \log p_t (x)]\tilde{p}_t (x) \dif x \nonumber \\
    &= \frac{1}{\int \tilde{p}_t(x) \dif x}  \int \partial_t \tilde{p}_t (x) \dif x  \nonumber \\
    &= \partial_t \log Z_t, \nonumber
\end{align}
where the first and fourth equations follow \cref{lemma:divergence,lemma:l2}, respectively, which completes the proof.

\begin{wrapfigure}{r}{0.42\linewidth}
    \centering
    % \vspace{-4mm}
    \includegraphics[width=.39\textwidth]{figures/training/loss_plot.png}
    \vspace{-3mm}
    \caption{Training loss of using different estimators of $\partial_t \log Z_t$.}
    \label{fig:loss}
    \vspace{-4mm}
\end{wrapfigure}
\textbf{Remark.} \cref{eq:partial_logZ_fpi} provides an alternative approach to estimate $\partial_t \log Z_t$. As illustrated in \cref{fig:std_dt_logZt}, this estimation exhibits lower variance compared to using $\E_{p_t} \partial_t \log \tilde{p}_t (x)$. This reduction in variance can potentially lead to better optimisation. To evaluate this, we conducted experiments on GMM datasets by minimizing the loss in \cref{eq:nfs_loss}, employing two different methods to estimate $\partial_t \log Z_t$: $\E_{p_t} \partial_t \log \tilde{p}_t (x)$ and $\E_{p_t} \xi_t (x;v_t)$. The loss values during training are plotted against the training steps in \cref{fig:loss}. The results show that the estimator of $\E_{p_t} \xi_t (x;v_t)$ achieves lower loss values, highlighting the superior training effects achieved with the lower variance estimation of $\partial_t \log Z_t$.



\subsection{Stein Control Variates} \label{sec:appendix-control-variates}
In this section, we provide a perspective from control variates to explain the observation of variance reduction in \cref{fig:std_dt_logZt}.
In particular, consider a Monte Carlo integration problem $\mu = \E_\pi [f (x)]$, which can be estimated as $\hat{\mu} = \frac{1}{K} \sum_{k=1}^K f(x^{(k)}), x^{(k)} \sim \pi$.
Assuming another function exists with a known mean $\gamma = \E_\pi [g(x)]$, we call $g$ the control variate. We then can construct another estimator $\check{\mu} = \frac{1}{K} \sum_{k=1}^K (f(x^{(k)}) - \beta g(x^{(k)})) + \beta \gamma$, where $\beta$ is a scalar coefficient and controls the scale of
the control variate. It is obvious that $\E [\check{\mu}] = \E [\hat{\mu}] = \mu, \forall \beta \in \mathbb{R}$. Moreover,  we can choose a $\beta$ to minimize the variance of $\check{\mu}$. To obtain it, we first derive the variance of $\check{\mu}$
\begin{align} \label{eq:variance_mu_cv}
    \mathbb{V}[\check{\mu}] = \frac{1}{K} (\mathbb{V} [f] - 2\beta \mathrm{Cov}(f, g) + \beta^2\mathbb{V} [g]).
\end{align}
Since $ \mathbb{V}[\check{\mu}]$ is convex w.r.t. $\beta$, by differentiating it w.r.t. $\beta$ and zeroing it, we find the optimal value, $\beta^* = \mathrm{Cov}(f, g) / \mathbb{V}[g]$. Substituting it into \cref{eq:variance_mu_cv}, we get the minimal variance 
\begin{align}
    \mathbb{V}[\check{\mu}] = \frac{1}{K} \mathbb{V}[\hat{\mu}](1 - \mathrm{Corr}(f, g)^2).
\end{align}
This shows that, given the optimal value $\beta^*$, any function $g$ that correlates to $f$, whether positively or negatively, reduces the variance of the estimator, i.e., $\mathbb{V}[\check{\mu}] < \mathbb{V}[\hat{\mu}]$. 
In practice, the optimal $\beta^*$ can be estimated from a small number of samples \citep{ranganath2014black}. However, the primary challenge lies in finding an appropriate function $g$. For a detailed discussion on control variates, see \cite{geffner2018using}.

Fortunately, \cref{lemma:divergence}  offers a systematic way to construct a control variate for $\E_{p_t} [f(x)] \triangleq \E_{p_t}[\partial_t \log \tilde{p}_t(x)] \approx \frac{1}{K} \sum_{k=1}^K \partial_t \log \tilde{p}_t(x^{(k)})$, where $x^{(k)} \sim p_t$. Specifically, we define $g(x) = \nabla_x \cdot v_t(x;\theta) + v_t(x;\theta) \cdot \nabla_x \log p_t (x)$, from which we have $\gamma = \E_{p_t}[g(x)] = 0$. Using this, we construct a new estimator:
\begin{align}
    \check{\mu} \!=\! \frac{1}{K} \sum_{k=1}^K \partial_t \log \tilde{p}_t(x^{(k)}) \!+\! \beta^* (\nabla_x \cdot v_t(x^{(k)};\theta) \!+\! v_t(x^{(k)};\theta) \cdot \nabla_x \log p_t (x^{(k)})), \quad x^{(k)} \!\sim\! p_t.
\end{align}
Moreover, when $\theta$ is optimal, \cref{eq:nfs_loss} equals zero, implying $g(x) = -f(x) + c$, where $c$ is a constant independent of the sample $x$. In this case, $\mathrm{Corr}(f, g) = -1$, and $\check{\mu}$ becomes a zero-variance estimator.
As an additional clarification, Stein's identity from \cref{lemma:divergence} is also employed as a control variate in \cite{liu2017action}, where it is utilized to optimise the policy in reinforcement learning.


\begin{algorithm}[!t]
\caption{Training Procedure of \ours (one training epoch only for illustration)}
\label{alg:nfs_training}
\textbf{Input}: initial shortcut model $s_t (\cdot,\cdot;\theta)$, time spans $\{t_m\}_{m=0}^M$, shortcut distances $\{2^{-e}\}_{e=0}^E$ \\
\textbf{Output}: trained shortcut model $s_t (\cdot,\cdot;\theta)$
\begin{algorithmic}[1]
    \State $\tilde{t}_0 \leftarrow 0, \tilde{t}_m \sim \mathcal{U}([t_{m-1}, t_{m}]), m = 1,\dots,M$   \textcolor{gray}{\Comment{Sample time steps}}
    \State \!\!$\{ x_{\tilde{t}_m}^{(k)}\!, \tilde{w}_{\tilde{t}_m}^{(k)} \}_{k=1, m=0}^{K, M} \!\!\leftarrow\!\! \mathrm{VD-SMC}(s_t (\cdot,0;\theta), K,  \{\tilde{t}_m\}_{m=0}^M)$  \textcolor{gray}{\Comment{Generate samples using \cref{alg:SMC}}}
    \For{$t, \{x_t, \tilde{w}_t\} \sim \{ x_{\tilde{t}_m}^{(k)}, \tilde{w}_{\tilde{t}_m}^{(k)} \}_{k=1, m=0}^{K, M}$} \textcolor{gray}{\Comment{Executed with mini-batch in parallel practically}}
        \State \!\!\!$\xi_t \!\leftarrow\! \partial_t \log \tilde{p}_t (x_t) \!+\! \nabla_x \!\cdot\! s_t (x_t,0;\theta) \!+\! s_t (x_t,0;\theta) \!\cdot\! \nabla_x \log p_t (x_t)$
        \State \!\!\!$c_t \!\leftarrow \!\!\sum_{k} \! \tilde{w}_t^{(k)} \!\!\left[ \!\partial_t \log \tilde{p}_t (x_t^{(k)}\!) \!+\! \nabla_x \!\cdot\! s_t (x_t^{(k)}\!\!\!,0;\theta_{\mathrm{sg}}) \!+\! s_t (x_t^{(k)}\!\!\!,0;\theta_{\mathrm{sg}}) \!\cdot\! \nabla_x \log p_t (x_t^{(k\!)}) \!\right]$ \textcolor{gray}{\Comment{ $\partial_t \log Z_t$}}
        \State \!\!\!$d \sim \mathcal{U}( \{2^{-e}\}_{e=0}^E )$ \textcolor{gray}{\Comment{Sample shortcut distance}}
        \State \!\!\!$s_{\text{target}} \leftarrow s_t (x_t, d;\theta)/2 + s_{t+d} (x_{t+d}, d;\theta)/2$ \textcolor{gray}{\Comment{Compute shortcut target}}
        \State \!\!\!$\mathcal{L}(\theta) \leftarrow (\xi_t - c_t)^2 + \lVert s_t(x_t. 2d;\theta) - s_{\text{target}} \rVert_2^2$   \textcolor{gray}{\Comment{Compute training loss}}
        \State \!\!\!$\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta)$    \textcolor{gray}{\Comment{Perform gradient update}}
    \EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!t]
\caption{Sampling Procedure of \ours}
\label{alg:nfs_sampling}
\textbf{Input}: trained shortcut model $s_t (\cdot,\cdot;\theta)$, initial density $p_0$, \# steps $M$\\
\textbf{Output}: generated samples $x$
\begin{algorithmic}[1]
\State $x_0 \sim p_0, \quad d \leftarrow \frac{1}{M}, \quad t \leftarrow 0$ \textcolor{gray}{\Comment{Initialisation}}
\For{$m = 0, \dots, M-1$}
    \State $x \leftarrow x + s_t (x,d;\theta) d$
    \State $t \leftarrow t + d$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Training and Sampling Algorithms}
The training and sampling algorithms are detailed in \cref{alg:nfs_training,alg:nfs_sampling}, respectively. For clarity, \cref{alg:nfs_training} illustrates a single training epoch.
In particular, we parameterise the model with a single neural network $s_t (x, d;\theta)$ that takes the sample $x$, time step $t$, and shortcut distance $d$ as input to anticipate the shortcut direction. This design enables \ours to model in continuous time, unlike the baseline LFIS \citep{tian2024liouville}, which trains separate neural networks for each time step --- a memory-intensive and inefficient approach.
To train the model, we define time spans $\{t_m\}_{m=0}^M$ that are evenly distributed over $[0,1]$, satisfying $0=t_0<\dots<t_M=1$ and $2t_m  = t_{m+1} + t_{m-1}, \forall m$.
In each epoch, we uniformly sample time steps from the time spans $\tilde{t}_m \sim \mathcal{U}([t_{m-1}, t_m])$ and ensure that $\tilde{t}_0 = 0$.\footnote{More advanced schedule beyond uniform sampling remain important future works.}
Subsequently, \cref{alg:SMC} is invoked to generate training samples, which resembles distribution $q$ as defined in \cref{eq:nfs_loss}. Notably, any $q$ distribution can be used to generate training samples. The choice of the proposed velocity-driven SMC is motivated by two key reasons:  
\begin{itemize}
    \item[i)]At the beginning of training, the generated samples are far from the mode, encouraging the model to focus on mode-covering. As training progresses, the generated samples become more accurate, gradually shifting toward mode-seeking, ultimately balancing exploration and exploitation for improved learning efficiency.
    \item[ii)] Improved $\partial_t \log Z_t$ estimation efficiency. SMC returns the samples and importance weights for each time step simultaneously, streamlining the estimation of $\partial_t \log Z_t$.
\end{itemize}
After generating training samples, we compute the loss in \cref{eq:loss-nfs2} and update the model using gradient descent, as outlined in steps 4–9 of \cref{alg:nfs_training}.


\section{Related Work}

Sampling from given probability distributions has been a longstanding research challenge. Monte Carlo methods, such as Annealed Importance Sampling \citep{neal2001annealed} and Sequential Monte Carlo \citep{del2006sequential}, are considered the gold standards for sampling, but they tend to be computationally expensive and often suffer from slow convergence \citep{roberts2001optimal}.
Amortised variational methods like normalizing flows \citep{rezende2015variational} and latent variable models \citep{he2024training} provide appealing alternatives to matching the target distribution, offering faster inference but often at the cost of approximation errors and limited expressiveness.
Hybrid approaches \citep{wu2020stochastic,zhang2021differentiable,geffner2021mcmc,matthews2022continual,midgley2022flow} that combine MCMC and variational inference have shown promising potential by leveraging the strengths of both methods.

Building upon the success of generative modelling, diffusion models \citep{ho2020denoising} and flow matching \citep{lipman2022flow} have been applied to sampling tasks. Specifically, \cite{vargas2023denoising,nusken2024transport} exploit diffusion processes for learning to sample. However, these approaches require simulation to compute the objective. 
To resolve this issue, \cite{AkhoundSadegh2024IteratedDE} propose to use a bi-level training scheme that iteratively generates samples and performs score matching with Monte Carlo estimates of the target, resembling training diffusion models, and does not require simulation. \cite{ouyang2024bnem} introduce a variant that replaces score matching with direct regression on the energy, which is shown to reduce variance. Similarly, \cite{woo2024iterated} present another variant that targets on the MC-estimated vector fields in a flow matching framework.
Other approaches also focus on learning the velocity field; for instance,
\cite{tian2024liouville,mate2023learning} learn the velocity field to satisfy the continuity equation of the given probability path.

Beyond the above methods, stochastic control \citep{pavon1989stochastic,tzen2019theoretical} has also been applied to the sampling. For example, \cite{zhang2021path} propose path integral sampler (PIS) based on the connections between sampling and optimal control \citep{chen2016relation}. \cite{berner2022optimal} also establish the connection between optimal control and generative modelling based on stochastic differential equations \citep{kloeden1992stochastic}, which can be applied in sampling.
Generative flow networks (GFlowNets) \citep{lahlou2023theory} are appealing alternatives for sampling from unnormalised densities.
\cite{zhang2022unifying} establishes a connection between diffusion models and GFlowNets, leveraging this relationship to enhance learning-based sampling \citep{zhang2023diffusion}, which only requires simulating partial trajectories, improving the efficiency compared to PIS.

\section{Experimental Details} \label{sec:sppendix_exp_details}
\subsection{Datasets}
\textbf{Gaussian Mixture Model (GMM-40).} We use a 40 Gaussian mixture density in 2 dimensions as proposed by \cite{midgley2022flow}. This density consists of a mixture of 40 evenly weighted Gaussians with identical covariances
\[
\Sigma = 
\begin{bmatrix}
40 & 0 \\
0 & 40
\end{bmatrix}
\]
and \( \mu_i \) are uniformly distributed over the \([-40, 40]\) box, i.e., \( \mu_i \sim U(-40, 40)^2 \).
\[
p(x) = \frac{1}{40} \sum_{i=1}^{40} \mathcal{N}(x; \mu_i, \Sigma)
\]
\textbf{Many Well 32 (MW-32).} We use a 32-dimensional Many Well density, as proposed by \cite{midgley2022flow}. This density consists of a mixture of \( n_{\text{wells}} = 16 \) independent double-well potentials:
\[
E(x) = \sum_{i=1}^{n_{\text{wells}}} E_{\text{DW}}(x_i)
\]
where each \( x_i \) corresponds to a pair of variables in a 2-dimensional space. The energy function for a single double-well potential is defined as:
\[
E_{\text{DW}}(x) = \frac{1}{2} \left( (x_1 - \mu_1)^2 + (x_2 - \mu_2)^2 \right)
\]
Here, the wells are symmetrically distributed across a grid in the 32-dimensional space, where each pair of dimensions corresponds to a well, and \( \mu_i \) is uniformly distributed over the space. The total log probability is proportional to the sum of energies from all wells:
$\log p(x) \propto E(x) = \sum_{i=1}^{n_{\text{wells}}} E_{\text{DW}}(x_i)$.

\textbf{Double Well 4.} The energy function for the DW-4 dataset was introduced in \cite{pmlr-v119-kohler20a} and corresponds to a system of 4 particles in a 2-dimensional space. The system is governed by a double-well potential based on the pairwise distances of the particles. For a system of 4 particles, \( x = \{x_1, \dots, x_4\} \), the energy is given by:
\[
E(x) = \frac{1}{2\tau} \sum_{i,j} \left[ a(d_{ij} - d_0) + b(d_{ij} - d_0)^2 + c(d_{ij} - d_0)^4 \right]
\]
where \( d_{ij} = \| x_i - x_j \|_2 \) is the Euclidean distance between particles \( i \) and \( j \). Following previous work, we set \( a = 0 \), \( b = -4 \), \( c = 0.9 \), and the temperature parameter \( \tau = 1 \).
To evaluate the efficacy of our samples, we use a validation and test set from the MCMC samples in \cite{klein2024equivariant} as the ground truth samples following the practice of previous works \citep{AkhoundSadegh2024IteratedDE}.

\subsection{Metrics}

We evaluate the methods using the Wasserstein-2 ($\mathcal{W}_2$) distance and the Total Variation (TV), both computed with 1,000 ground truth and generated samples. To compute TV, the support is divided into 200 bins along each dimension, and the empirical distribution over 1,000 samples is used.
For GMM-40, we report the metrics$\mathcal{W}_2$ on energy space $\mathcal{E}$ and TV on the data space $\mathcal{X}$. 
For MW-32, we find that $\mathcal{E}-\mathcal{W}_2$ is unstable and thus report $\mathcal{E}$-TV instead. Given the 32-dimensional nature of MW-32, computing TV is impractical; therefore, we report the $\mathcal{W}_2$ metric on the data space rather than TV.
For the $n$-body system DW-4, we do not report any metrics in the data space due to its equivariance. Instead, we assess performance using metrics in the energy space ($\mathcal{E}$-$\mathcal{W}_2$ and $\mathcal{E}$-TV) and the interatomic coordinates $\mathcal{D}$ ($\mathcal{D}$-TV) to account for invariance.


\subsection{Training Details}
\textbf{Gaussian Mixture Model (GMM-40).}
We evaluate our method on a 40-mode Gaussian mixture in $\mathbb{R}^2$ to test multi-modal exploration. The velocity field is parameterized by a 4-layer MLP ($128$-dimensional hidden layers, Layer Norm, and GeLU activations) trained using velocity-guided sequential Monte Carlo with Hamiltonian kernels (3 HMC steps, 5 leapfrog steps, step size $\eta=0.1$). Initial particles are sampled from $\mathcal{N}(\mathbf{0}, 25\mathbf{I})$, optimized via AdamW ($\beta_1=0.9$, $\beta_2=0.999$) with learning rate $4 \times 10^{-4}$, weight decay $10^{-4}$, and gradient clipping at $\ell_2$-norm $1.0$. Training uses $128$-particle batches for $10^4$ epochs (500 steps/epoch) with early stopping, converging significantly before the epoch limit.


\textbf{Many Well 32 (MW-32).}
We assess scalability in high dimensions using a $2^{32}$-mode Many Well potential on $\mathbb{R}^{32}$, exhibiting exponential mode growth with dimension. The velocity field employs a 4-layer MLP ($128$-dimensional hidden layers, Layer Norm, and GeLU activations) trained via velocity-guided SMC with enhanced Hamiltonian kernels (6 HMC steps, 10 leapfrog steps, step size $\eta=0.1$). Initialized from $\mathcal{N}(\mathbf{0}, 2\mathbf{I})$, optimization uses AdamW ($\beta_1=0.9$, $\beta_2=0.999$) with learning rate $1e^{-3}$, weight decay $10^{-4}$, and $\ell_2$-gradient clipping at $1.0$. Training maintains $128$-particle batches across $10^4$ epochs (500 steps/epoch) with early stopping.

\textbf{Double Well 4 (DW-4).}
We assess performance in particle-like system using a DW-4 potential on euclidean space. The velocity field employs a 4-layer MLP ($512$-dimensional hidden layers, Layer Norm, and GeLU activations) trained via velocity-guided SMC with enhanced Hamiltonian kernels (10 HMC steps, 10 leapfrog steps, step size $\eta=0.01$). Initialized from $\mathcal{N}(\mathbf{0}, 2\mathbf{I})$, optimization uses AdamW ($\beta_1=0.9$, $\beta_2=0.999$) with learning rate $4e^{-3}$, weight decay $10^{-4}$, and $\ell_2$-gradient clipping at $1.0$. Training maintains $128$-particle batches across $10^4$ epochs (500 steps/epoch) with early stopping.

\section{Additional Experimental Results} \label{sec:appendix_add_exp_results}


\begin{figure}[!t]
    \centering
    \begin{minipage}[t]{0.195\linewidth}
        \centering
        \includegraphics[width=1.\linewidth]{figures/samples/mw-32/data_samples_2plots.png}
        Ground Truth
    \end{minipage}
    \begin{minipage}[t]{0.195\linewidth}
        \centering
        \includegraphics[width=1.\linewidth]{figures/samples/mw-32/fab_samples_2plots.png}
        FAB
    \end{minipage}
    \begin{minipage}[t]{0.195\linewidth}
        \centering
        \includegraphics[width=1.\linewidth]{figures/samples/mw-32/idem_samples_2plots.png}
        iDEM
    \end{minipage}
    \begin{minipage}[t]{0.195\linewidth}
        \centering
        \includegraphics[width=1.\linewidth]{figures/samples/mw-32/lfis_samples_2plots.png}
        LFIS
    \end{minipage}
    \begin{minipage}[t]{0.195\linewidth}
        \centering
        \includegraphics[width=1.\linewidth]{figures/samples/mw-32/nfs_samples_2plots.png}
        NFS (ours)
    \end{minipage}
    \caption{Samples on MW-32. First row: 2D marginal samples from the 1st and 4th dimensions; Second row: 2D marginal samples from the 2rd and 3rd dimensions.}
    \label{fig:mw32-visualised-2plots}
\end{figure}

\subsection{Visualisation of MW-32}

\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
    \vspace{-4mm}
    \includegraphics[width=.47\textwidth]{figures/samples/mw-32/mw32_hist.png}
    \vspace{-2mm}
    \caption{Histogram of sample energy on MW-32.}
    \label{fig:mw32-energy-hist}
    \vspace{-4mm}
\end{wrapfigure}
This section presents additional visualizations of generated samples on MW-32. As shown in \cref{fig:mw32-visualised-2plots}, only FAB and \ours accurately capture the modes. While iDEM locates the modes, it struggles to identify their correct weights. Additionally, LFIS, another flow-based sampler similar to \ours, produces noisy samples, highlighting the high variance issue associated with importance sampling.
We further illustrate the histogram of sample energy on MW-32, where we draw the empirical energy distribution using 5,000 samples.
It shows that \ours achieves competitive performance with FAB, and notably outperforms iDEM and LFIS.




\subsection{Comparisons with Different Sampling Steps}
One key advantage of \ours is its ability to achieve high-quality results with fewer sampling steps. In this section, we compare \ours to the SOTA diffusion-based sampler iDEM and the flow-based sampler LFIS, using varying numbers of sampling steps. As demonstrated in \cref{fig:gmm-diff-steps,fig:mw32-diff-steps}, \ours produces better samples compared to both iDEM and LFIS, when using fewer sampling steps.

\section{Limitations and Future Work} \label{sec:appendix-limit-future-work}


A key challenge of flow-based samplers is the computation of the divergence (see \cref{eq:nfs_loss}), which becomes prohibitive in high-dimensional settings. While the Hutchinson estimator \citep{hutchinson1989stochastic} can be used in practice, it introduces both variance and bias. Alternatively, more advanced architectures can be employed where the divergence is computed analytically \citep{gerdes2023learning}. By adopting such architectures, we expect our approach to be scalable to more complex applications, such as molecular simulation \citep{frenkel2023understanding}, Lennard-Jones potential \citep{klein2024equivariant}, and Bayesian inference \citep{neal1993probabilistic}.

Rather than building off of the square error, the objective in \cref{eq:nfs_loss} can also be formulated using the Bregman divergence, which provides a more general framework for measuring discrepancies and can potentially lead to improved optimization properties, such as better convergence and robustness to outliers.
Moreover, while the shortcut model reduces the number of sampling steps required, achieving exact likelihood estimation within this framework remains unclear, presenting a promising direction for future research.

\begin{figure}[!t]
    \centering
    \begin{minipage}[t]{1.\linewidth}
        \centering
        \rotatebox{90}{\makebox[60pt]{iDEM}}
        \includegraphics[width=.97\linewidth]{figures/samples/gmm/idem_gmm_diff_stpes.jpg}
    \end{minipage}
    \begin{minipage}[t]{1.\linewidth}
        \centering
        \rotatebox{90}{\makebox[60pt]{LFIS}}
        \includegraphics[width=.97\linewidth]{figures/samples/gmm/lfis_gmm_diff_stpes.jpg}
    \end{minipage}
    \begin{minipage}[t]{1.\linewidth}
        \centering
        \hspace{-1.5mm} \rotatebox{90}{\makebox[70pt]{\ours}}
        \includegraphics[width=.97\linewidth]{figures/samples/gmm/nfs_gmm_diff_stpes.jpg}
    \end{minipage}
    \caption{Illustration of the generated samples using different sampling steps on GMM-40.}
    \label{fig:gmm-diff-steps}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{minipage}[t]{1.\linewidth}
        \centering
        \rotatebox{90}{\makebox[60pt]{iDEM}} 
        \includegraphics[width=.97\linewidth]{figures/samples/mw-32/idem_mw32_diff_stpes.jpg}
    \end{minipage}
    \begin{minipage}[t]{1.\linewidth}
        \centering
        \rotatebox{90}{\makebox[60pt]{LFIS}}
        \includegraphics[width=.97\linewidth]{figures/samples/mw-32/lfis_mw32_diff_stpes.jpg}
    \end{minipage}
    \begin{minipage}[t]{1.\linewidth}
        \centering
        \hspace{-1.5mm} \rotatebox{90}{\makebox[70pt]{\ours}}
        \includegraphics[width=.97\linewidth]{figures/samples/mw-32/nfs_mw32_diff_stpes.jpg}
    \end{minipage}
    \caption{Illustration of the generated samples using different sampling steps on MW-32.}
    \label{fig:mw32-diff-steps}
\end{figure}
