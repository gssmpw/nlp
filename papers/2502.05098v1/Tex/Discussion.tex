\section{Discussion}
\subsection{Influence of Malware Ground Truth}
Malware labels are determined by multiple security engines, often with conflicting decisions. Researchers set thresholds to balance sensitivity and specificity; lower thresholds include borderline software, blurring classification boundaries. In this study, we use a low threshold (\(vt = 4\)) to evaluate performance in this challenging scenario.
% Malware labels are determined based on the decisions of multiple security engines, which often disagree on whether a given sample is malware or not. Researchers typically establish a threshold to classify samples, balancing sensitivity and specificity. Lower thresholds tend to include more borderline or grey software that resembles benign applications, thereby blurring classification boundaries. In this paper, we select a lower threshold, vt = 4, to evaluate the performance of our approach in this challenging scenario.

\input{Table/Regularization}

\subsection{Comparison to the Regularization Method}
ERM-trained models are easy to overfit to unstable features, limiting test set generalization~\cite{regularization}. Regularization methods, such as early stopping, $\ell$2 regularization, and dropout, help mitigate this by constraining model parameters~\cite{regularization_understanding}. Unlike regularization, invariant learning focuses on stable features under drift scenarios. We compare these regularization methods with our invariant learning framework during the whole test phase (Table~\ref{tab: regularization}). For reference, DeepDrebin (ERM) includes no regularization, while DeepDrebin~\cite{Grossedeepdrebin} employs dropout with a hyperparameter of 0.2. Table~\ref{tab: regularization} shows dropout improves performance under significant drift, outperforming early stopping and $\ell$2 regularization, which fails to capture optimal features. Invariant training consistently outperforms, capturing stable, discriminative features that maintain performance across distributions.

\subsection{Application Scope of Invariant Learning}
Invariant learning enhances a modelâ€™s generalization to unseen distributions, but keeping stable to all possible shifts is unrealistic. With significant distribution changes, its effectiveness diminishes, and current research indicates that addressing concept drift typically requires costly model updates, including data labeling, retraining, and redeployment. Additionally, pseudo-label noise from automated labeling can further degrade performance~\cite{labelless}. The true value of a robust malware detector lies in sustaining reliable performance under gradual drifts, extending its lifespan before retraining and reducing the cost of selective updates.