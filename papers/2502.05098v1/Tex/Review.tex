\section{Background}
In this section, we review the development of malware detectors robust to drift and the key components of invariant learning, laying the groundwork for proposing a temporal invariant representation learning solution tailored for Android malware detection.

\subsection{Drift-robust Malware Detectors}
Existing approaches to enhancing model robustness against drift primarily focus on refining feature representations. One approach identifies features resilient to distribution changes through manual or automated engineering, enabling models to learn patterns less affected by malware evolution~\cite{apigraph}. Another approach anticipates future distribution shifts using episode-based learning~\cite{episode_learning_1, episode_learning_2}, where pseudo-unknown families are constructed in the training set to simulate low-sample tasks. Each episode replicates a specific learning scenario, fostering robust feature representation for open environments~\cite{DOMR}. While effective for addressing drift from new family emergence, this method requires managing numerous episodes to cover all families in the training set, significantly increasing complexity. Yang et al.~\cite{scrr} propose feature de-entanglement techniques aiming to reduce spurious correlations during training, preserving invariant information. However, these methods may overlook critical feature correlations, such as specific API calls or intent combinations (in Android apps) directly indicative of malicious behavior and only focus on target feature space. Similarly, feature stability metrics for linear classifiers offer insights for drift robustness but have limited applicability to non-linear models and complex feature interactions~\cite{svm_ce}.

% Existing approaches to enhancing model robustness against drift focus primarily on refining feature representations. One line of work aims to identify features that remain robust to distribution changes, either through manual or automated feature engineering, allowing models to learn patterns less influenced by malware evolution~\cite{apigraph}. Another approach anticipates potential future distribution patterns by employing specific training strategies. Lu~et al.~\cite{DOMR} constructs pseudo-unknown families in the training set and uses episode learning to help the model learn robust representations that can generalize to open environments. Episode-based training strategy is introduced in the few-shot learning that divides the training process into a large number of episodes~\cite{episode_learning_1, episode_learning_2}, each simulating a task with limited samples, to foster representation learning. Here, the unknown families are treated as low-sample tasks. This method primarily addresses drift caused by the emergence of new families and requires managing an excessive number of episodes due to the need to cover all families in the training set. Yang et al.~\cite{scrr} propose the use of feature de-entanglement to keep the features as independent as possible during the model training process, minimizing their spurious correlations to preserve invariant information. However, the correlation between malware features is often critical, as certain sensitive API calls or intent combinations may be directly indicative of malicious behavior, and simply reducing feature correlation may be suboptimal. Angioni~et al.~\cite{svm_ce} introduce a feature stability metric for linear classifiers in the Drebin feature space, contributing insights for drift robustness in linear models but with limited applicability to complex feature relationships and non-linear models.

These prior methods seek to enhance invariant feature information by mitigating instabilities, which aligns with our approach. However, due to the diverse evolution and complexity of malware, pinpointing the precise cause of drift remains challenging. Strategies that rely on assumptions, such as drift being driven by new malware families, are limited. Instead, recognizing that distribution drift inevitably occurs over time motivates our exploration of temporally invariant information.


\subsection{Invariant Learning}
\label{invariant_learning}
%\subsubsection{Notations}
Assume that the training data $\mathcal{D}_{tr}$ is collected from multiple environments $e \in \mathcal{E}$, i.e., $\mathcal{D}_{tr}=\{D^e_{tr}\}_{e \in \mathcal{E}}$. Let the input space be $x \in \mathcal{X}$ and the target space be $y \in \mathcal{Y}$, and for the sample observations from each environment denoted $x,y \sim p(x,y|e)$, the samples within the environments obey an independent and identical distribution. Suppose a classification model $f$, denoted as a composite function $f=c \circ \phi$. where $\phi:\mathcal{X} \rightarrow \mathcal{H}$ denotes a feature encoder that maps the input samples into a feature representation space $\mathcal{H}$, and $\phi(x) \in \mathcal{H}$ is the ``representation'' of sample $x$. The $c:\mathcal{H} \rightarrow \mathcal{Y}$ denotes a classifier that maps the feature representation to the logits space of $\mathcal{Y}$. 


\subsubsection{Learning Invariant Representation}
In test environments where distribution drift exists, the test data $\mathcal{D}_{te}$ may come from a distribution $p(x,y|e_{te})$ that does not appear in the training set, i.e. $e_{te} \notin \mathcal{E}$. Robustness to drift yields lower error rates on unknown test data distributions.

Invariant learning enhances the generalization ability of a model to unknown distributions by learning label distributions that are invariant across training environments. Its goal is to develop a classifier $c(\cdot)$ that satisfies the environmental invariance constraint (EIC)~\cite{EIC}:


\begin{equation}
\mathbb{E}\left[y \mid \phi(x), e\right] = \mathbb{E}\left[y \mid \phi(x), e^{\prime}\right], \quad \forall e, e^{\prime} \in \mathcal{E},
\end{equation}
where $e$ and $e'$ denote different environments to which the samples belong. This constraint is integrated into the training objective through a penalty term. Thus, the goal can be formalized as:
\begin{equation}
\min _f\sum_{e \in \mathcal{E}} R^e_{erm}(f)+\lambda \cdot \operatorname{penalty}\left(\left\{S^e(f)\right\}_{e \in \mathcal{E}}\right),
\end{equation}
where $R^e_{erm}(f) = \mathbb{E}_{p(x,y|e)}[\ell(f(x),y)]$ represents the expected loss on environment $e$. Empirical risk minimization (ERM) is to minimize this expected loss within each environment. $S^e(f)$ is some statistic of the model in $e$ (see next), and the penalty is to constrain the change in this statistic to control the degree of deviation from the EIC. Optimizing this objective prevents mapping all $x$ to the same value to satisfy environmental invariance, as it encourages the predictive utility of $\phi$ by minimizing empirical risk loss. In addition, the form of the penalty term is variable to achieve constraints for different objectives. Krueger et al.~\cite{v-rex} proposed V-Rex such that $S^e(f)=R^e_{erm}(f)$, to minimize the variance of $S^e(f)$ in different environments. In CLOvE~\cite{causal_ir}, the penalty is defined as the sum of the calibration errors of the model in each environment. One widely used scheme is Invariant Risk Minimization (IRM) and its practical variant IRMv1 proposed by Arjovsky et al.~\cite{IRM_training}. The penalty term is the sum of $S^e(f)=\left\|\nabla_w R^e_{erm}(w \circ \phi)\right\|^2$. $w$ is a constant scalar multiplier of 1.0 for each output dimension, forcing the same classifier to be optimal in all environments. 

\subsubsection{Split Environments for Invariant Learning}
Invariant learning relies on segmenting environments to highlight differences across them~\cite{IR_intro}. Early methods assumed prior knowledge of environment labels, which is often unavailable in practice~\cite{bottleneck_ir, empirical_ir,environment_label,inaccessible_label_2}. Recent approaches focus on invariant learning without predefined labels. Creager et al.~\cite{EIC} proposes estimating environment labels via prior clustering before applying invariant learning (EIIL), while others explore segmentation strategies such as natural clustering using dataset-provided labels or unsupervised clustering~\cite{unshuffling}. Experiments show natural clustering outperforms unsupervised methods. Regardless of the approach, effective segmentation requires environments to expose unstable information that can be ignored during invariant learning~\cite{IR_intro}.

% Invariant learning is predicated on the premise that environments need to be segmented to maximize differences across environments~\cite{IR_intro}. Earlier solutions where environment labels are known a prior knowledge~\cite{bottleneck_ir, empirical_ir}. However, in practice, such a clear and accurate environment index of the environment was always unavailable~\cite{environment_label,inaccessible_label_2}. Nowadays, solutions turn to studying invariant learning without environment labels. Creager et al.~\cite{EIC} propose environment inference for invariant learning (EIIL), where the environment labels of the training data are estimated employing prior clustering, after which invariant learning is performed. Teney et al.~\cite{unshuffling} propose environment segmentation for visual question answering tasks. The solution explores two approaches to environment segmentation. The first performs natural clustering based on the question type labels provided by the dataset, and the second produces multiple environments for clustering based on unsupervised learning. The experiment shows the natural approach to segmentation achieves better performance than unsupervised. Whether the environment segmentation is learned using an additional optimization objective or constructed directly following the information provided by the dataset, it requires the environment to expose unstable information that needs to be ignored~\cite{IR_intro}. 