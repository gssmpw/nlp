\section{Introduction}
In open and dynamic environments, even the most effective malware detectors encounter significant challenges due to natural distribution drift{}\footnote{For clarity, we use the terms drift, shift, concept drift/shift, natural drift/shift, and distribution drift/shift interchangeably throughout the text. This is in contrast to adversarial drift or adversarial evasive attacks, which are caused by identifying optimal input perturbations specifically crafted to cause a model's misclassification.}, leading to performance degradation~\cite{transcending, cade}. This degradation arises from the evolution of malware behavior and the emergence of new malware families that detectors have not previously encountered~\cite{pei2024exploiting}. These new variants and families change the underlying statistical properties of the test samples~\cite{tesseract, overkill, transcending, Drift_forensice}, weakening detectors that rely on features derived from past training data, which lack sufficient discriminative power for the new variants~\cite{malware_evolution_update}.

Recent studies have explored incremental training techniques to address distribution drift. These techniques involve detecting new distributions during testing~\cite{transcending, cade} and frequently updating models through active~\cite{tesseract, continuous} or online learning~\cite{droidevolver, online_mal, labelless}. However, such approaches incur significant labeling and model update costs. Although pseudo-labeling has been employed to alleviate labeling burdens, it introduces noise that can lead to self-poisoning of models~\cite{labelless, recda}, further complicating the learning process. Therefore, enhancing model robustness in drifting scenarios is essential to reduce the frequency of updates. Some studies have attempted static feature augmentation or selecting features that are less sensitive to malware evolution~\cite{scrr, apigraph, overkill}. However, these methods typically require tailored selection strategies for specific feature spaces and depend heavily on the stability of these spaces, limiting their adaptability to diverse drift sources. Ideally, a drift-robust detector should extract invariant features that improve performance and generalize to drifted samples without relying on specific feature space assumptions.


%Malware category definitions provide a foundation for such invariance, as samples with similar attack strategies or behaviors belong to the same family or type, even with differing implementations.}


% However, the current training paradigm inhibits the model's ability to learn such invariance.

In this paper, we conducted a comprehensive investigation into the brittleness of learning-based malware detectors against natural distribution drift. Our findings indicate that, although invariant features representing shared malicious behaviors exist within malware families or variants, detectors trained using empirical risk minimization (ERM) tend to capture unstable features instead of these invariant ones. This inefficiency arises from the inherent limitations of the ERM training paradigm, which operates under the assumption that training and testing data share the same distribution. ERM typically requires data shuffling or the construction of k-fold cross-validation sets. However, in real-world scenarios, testing data is often collected after the training period~\cite{tesseract}. As a result, shuffling the training data disrupts the temporal evolution of malware samples, preventing the model from learning stable features that persist over time. Consequently, ERM prioritizes unstable features, hindering the model's adaptability to the dynamic evolution of malware.

%In this paper, we conduct a comprehensive investigation into learning-based malware detectors, investigating their failure mechanisms under drift scenarios. Our findings indicate that despite the existence of invariant features within malware families or variants in training datasets, detectors struggle to learn them effectively. This inefficiency stems from the inherent limitations of the Empirical Risk Minimization (ERM) training paradigm rather than the choice of feature space. ERM operates under the assumption that training and testing data conform to the same distribution and so it learns discriminative but not necessarily stable features.
%
%, requiring data shuffling or constructing k-fold cross-validation sets. However, in real-world scenarios, testing data is often observed post-training~\cite{tesseract}, meaning the shuffled training set fails to capture the sequential transition pattern between training and testing, hindering the adaptation to malware evolution.}

Invariant learning theory~\cite{IR_intro} aims to address the shortcomings of ERM by learning invariant features/representations shared across different distributions, which aligns with our objective. It promotes the discovery of stable representations by dividing training data into distinct subsets or ``environments'' and encourages the model to minimize differences between them. This is based on two premises: \emph{a priori environment labels that reveal instability}~\cite{environment_label, env_label} and \emph{high-quality representations that adequately encode feature information}~\cite{yang2024invariant}. These assumptions are not trivial for malware detection, as malware evolution is attributed to a variety of non-obvious factors, and the extremely unbalanced sample distributions and complex feature spaces due to multiple malicious families make invariance across environments difficult to explore.



% However, common invariant learning methods typically require prior knowledge of environment labels that reveal unstable features~\cite{environment_label, env_label} and assume the encoder is capable of learning rich, high-quality representations~\cite{yang2024invariant}. These requirements and assumptions are not trivial for malware detection, where drift arises from various non-obvious factors, compounded by imbalanced sample distributions and a diverse feature space that complicates representation learning.

In this paper, we present a \textbf{T}emporal \textbf{I}nvariant Training \textbf{F}ramework (TIF), for malware detectors to emphasize invariant features under natural distribution drift. TIF firstly partitions training data temporally based on application observation dates, revealing malware evolution without relying on predefined environment labels. Moreover, to model multi-family fused malware categories in binary classification tasks, we propose a multi-proxy contrastive learning module for modular representation learning. While recent study~\cite{continuous} attempts to model positive pairs within each family, such fine-grained approaches exacerbate class imbalance, as smaller families or rare variants may lack adequate representation. Treating multi-family malware categories as homogeneous is also suboptimal due to ignoring detailed patterns in malware families. Our module addresses this by dynamically updating proxies to model subsets within the class, capturing unique patterns within subsets while balancing diversity across subsets and consistency within each subset~\cite{proxy, mixood}. 

TIF further integrates an invariant gradient alignment module to ensure that the encoder produces similar gradients for samples of the same class across environments to enhance temporally invariant representations. Our solution is orthogonal to existing robust malware detectors: it does not rely on new feature spaces, requires no changes to existing model architectures, and can be applied as an enhanced training paradigm to any learning-based detector. The main contributions of this paper are as follows:


% In this paper, we present a \textbf{T}emporal \textbf{I}nvariant Training \textbf{F}ramework (TIF), for malware detectors to emphasize invariant features under distribution drift. TIF leverages environment partitioning and two well-designed modules to amplify and suppress unstable features, promoting high-quality, cross-environment invariant representations through the two-stage training process. Specifically, training data is partitioned based on the date the application was observed, revealing the complex temporal evolution of malware while avoiding reliance on predefined environment labels. Moreover, in the context of binary classification tasks, it is suboptimal to treat multi-family malware class as homogeneous. Recent work focuses on modeling intra-family positive correlations to enhance feature representations~\cite{continuous}. However, such fine-grained approaches often exacerbate the class imbalance problem, as smaller families or rare variants may be underrepresented. To address this, we propose a multi-proxy contrastive learning module that learns modular representations by leveraging multiple proxies, each representing a subset within the class. These proxies are dynamically updated during training to capture distinct patterns within subsets, balancing the need for diversity across subsets and consistency within subsets. Building on these feature representations, we integrate an invariance learning-based gradient alignment module to ensure that the encoder generates similar gradients for samples of the same class across different environments, further enhancing the robustness of the learned representations. Our solution is orthogonal to existing robust malware detectors; it does not depend on new feature spaces, requires no changes to existing model architectures, and can be applied to any learning-based detector.



\begin{itemize}
    \item We define invariance in malware evolution, positing that learning discriminative and stable invariant representations is key to mitigating its shortcomings in natural drifting scenarios (Section~\ref{motivation}).
    \item We carefully design multi-proxy contrastive learning (Section~\ref{multi-proxy contrastive learning}) and invariant gradient alignment (Section~\ref{invariant_alignment}) modules to model and align complex distributions in different temporal environments, encouraging models to learn high-quality and stable representations.
    \item We present TIF, the first temporal invariant learning framework for malware detectors, which can be integrated with arbitrary learning-based detectors and feature spaces to enhance their ability to learn invariant representations over time by exposing and suppressing unstable information in the drift (Section~\ref{invariant training}).
    \item We construct a 10-year dataset\footnote{We will open source the dataset metadata and code repository to foster reproducibility studies.} and a series of experiments to evaluate the robustness of the TIF across various drift scenarios and feature spaces. The results show that TIF effectively slows the detector's degradation and generates invariant representations that outperform state-of-the-art methods (Section~\ref{evaluation}).
\end{itemize}

% In this paper, we present a \textbf{T}emporal \textbf{I}nvariant Training \textbf{F}ramework, TIF, designed for malware detectors to facilitate the learning of invariant features under distribution drift. Specifically, we partition the training data according to the date the application was observed, thereby exposing the complex instability of malware evolution over time and avoiding the reliance on prior environment labels. In the context of binary classification tasks, recent works claim that it is suboptimal to treat multi-family malware class as homogeneous~\cite{continuous} and they focus on modeling intra-family positive correlations to enhance feature representations. However, such fine-grained approaches often exacerbate the class imbalance problem, as smaller families or rare variants may be underrepresented. To address this, we propose a multi-proxy contrastive learning module that learns modular representations by leveraging multiple proxies, each representing a subset within the class. These proxies are dynamically updated during training to capture distinct patterns within subsets, balance the need for diversity across subsets and consistency within subsets. 

% we identify that learning homogeneous representations for a merged multi-family malware class is suboptimal. Recent works have focused on constructing positive relationships within individual malware families to enhance feature representations~\cite{continuous}. However, such an overly fine-grained approach often exacerbates sample imbalance issues. To address this, we propose a multi-proxy contrastive learning module. In our proposed framework, each proxy is a representation of a similar subset within the class, with dynamic updates to multiple proxies that aim to capture feature representations reflective of the application's true semantics. Additionally, we design an invariant gradient alignment module grounded in invariant learning theory, ensuring that the encoder generates similar gradients for samples belonging to the same class across different environments, thereby promoting the learning of high-quality, cross-environment invariant representations. Our solution is orthogonal to existing robust malware detectors; it does not depend on new feature spaces, requires no changes to existing model architectures, and can be applied to any learning-based detector. The main contributions of this paper are as follows: