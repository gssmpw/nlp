\section{Methodology}

\subsection{Preliminary}
\subsubsection{Notations}
We use uppercase and lowercase letters to denote matrices and vectors, respectively; $|\mathcal{D}|$ represents the total number of elements in set $\mathcal{D}$. 



\subsubsection{Problem setting}
In Android malware detection, samples $x \in \mathbb{R}^{d}$ are divided into two parts: the labeled training data $\mathcal{D}_{tr} = {(x_i^{tr},y_i^{tr})}^{|\mathcal{D}_{tr }|}_{i=1}$, where $y_{i}^{tr} \in \{0, 1\}$ represents the labels for benign and malicious applications, and the unlabeled test data $\mathcal{D}_{ts}$. Training data $\mathcal{T}_{tr}$ comes from a specific period $\mathcal{T}_{tr}$, with each sample $x_i^{ts}$ ontains its corresponding observation timestamp $t_i^{tr}$. Test data $\mathcal{D}_{ts}$ comprises future samples $t_i^{ts} > \mathcal{T}_{tr}$ that appear progressively over time. We assume that the training and test data share the same label space and have unknown semantic similarities. The model is defined as $\mathcal{M} = c \circ \phi$, where $c$ denotes the classifier and $\phi$ is the feature encoder. This work aims to enhance the encoder in capturing invariant semantic features from training data that generalize to unseen test data.


\begin{figure*}[htbp]
    \centering
    % \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=1.0\linewidth]{Figure/model_architecture.pdf}
    \caption{The proposed invariant training framework and its core components}
    \label{fig:model_architecture}
\end{figure*}


\subsection{Overall Architecture}
Section~\ref{motivation: failure} highlights the need for learning temporally stable and discriminative features to improve robustness against distribution drift. We propose a temporal invariant training method, adaptable to any malware detector, which enhances robustness by extracting rich features and suppressing unstable factors through invariant risk minimization. The method comprises two components: \textbf{Multi-Proxy Contrastive Learning (MPC)}: representation diversity for benign and malware samples with dynamic proxies, encoding complex multi-family malware semantics into compact, discriminative embeddings. \textbf{Invariant Gradient Alignment (IGA)}: This component applies invariant risk minimization (IRM) to align gradients across environments, enabling the encoder to learn unified representations over time.

% Section~\ref{motivation: failure} illustrates the importance of learning temporally stable and discriminative features to improve robustness against distribution drift. Based on this, we propose a temporal invariant training method, which can be integrated into arbitrary malware detectors. This method enables the encoder to capture rich discriminative information from the training data, and then suppress the unstable cross-environment factors through invariant risk minimization, enhancing robustness in drift scenarios. The training process relies on two core components: \textbf{Multi-Proxy Contrastive Learning (MPC)}: This component models the diversity in the representations of benign samples and malware using dynamically updated proxies. It specifically encodes the complex semantic information of multi-family malware into the embedding space, learning more compact and faithful embeddings that fully expose the latent discriminative information in the training data. \textbf{Invariant Gradient Alignment (IGA)}: This component uses invariant risk minimization (IRM) to reduce gradient variance across different environments for samples of the same class, encouraging the encoder to learn a unified representation for all environments (i.e., time intervals).

Figure~\ref{fig:model_architecture} illustrates the overall framework of the proposed method, where Android applications are divided into non-overlapping environments based on their observation dates. The invariant training process includes two stages: 
\begin{itemize}
    \item Discriminative Information Amplification Stage: MPC is applied within each environment to minimize empirical risk, enabling the encoder to integrate discriminative features from each environment.
    \item Unstable Information Compression Stage: Building on the model trained in the first stage, MPC aligns features across environments, followed by IGA fine-tuning with IRM to suppress unstable information.
\end{itemize}


\subsection{Training Environments Segmentation}
\label{environment_split}
Segmenting the training environment requires exposing unstable information. Malware distribution drift arises from multiple factors, such as application market demands or Android version updates. Dividing the training environment based on application observation dates effectively captures this mix of unknown causes. For a dataset with timestamps from $T_{\min}$ to $T_{\max}$, and a chosen time granularity $\Delta$, samples are divided into $t$ time windows, each representing an environment $e \in \mathcal{E}$, where $|\mathcal{E}| = t$. The resulting training set is an ordered multiset, $\mathcal{D} = \{D_1,D_2,...,D_t\}$, with each environment $D_t$ containing $|D_t|$ samples. A sample $x_i$ with timestamp $t_i$ is assigned to an environment using:
\begin{equation}
\mathcal{E}(x_i)=\left\lfloor\frac{t_i-T_{\min }}{\Delta}\right\rfloor.
\end{equation}
As the environment index increases, timestamps approach the present. Time granularity impacts representation learning, balancing detailed distribution patterns against sample sufficiency. Finer granularity risks sparse samples, while coarser granularity may obscure shifts. Achieving balance requires careful consideration of label distribution to avoid misaligned representations. Section~\ref{env seg} explores the effects of granularity choices.

\subsection{Multi-Proxy Contrastive Learning}
\label{multi-proxy contrastive learning}
Malware families have distinct distributions and imbalanced sample sizes, complicating malware category modeling in the embedding space. Benign samples may also display complex feature distributions due to factors like geographic location or user behavior. Treating all relationships within a single family equally can exacerbate imbalances, while overly homogenizing samples overlook valuable information. Thus, effective discrimination requires balancing the complex pairwise relationships within each category to ensure high-quality representations that capture diversity. To address this, we introduce a multi-proxy contrastive learning method, where each proxy represents a subset of samples with similar behaviors or structures in one category, and is dynamically updated to trade-off between consistency and diversity of representations.

% This approach enables the encoder to capture finer-grained features within each class, rather than treating all benign or malicious samples as homogeneous groups.

For a given batch of samples $\mathbf{X}_c \in \mathbb{R}^{|\mathcal{B}_c| \times d}$ from category $c$ , where $|\mathcal{B}_c|$ is the batch size and $d$ is the feature dimension, we randomly initialize $K$ proxies for both benign and malicious categories. Each sample is then assigned to relevant proxies. In this multi-proxy setup, avoiding dominance by any single proxy helps mitigate noise. Inspired by the Sinkhorn~\cite{sinkhorn} algorithm, we compute a probability matrix $\mathbf{W}_c$ for proxies within category $c$, assigning a weight $w_{ij}$ to proxy $j$ for each sample $x_i$ as follows:
% \begin{equation}
% w_{i j}^{(c)}=\frac{u_i \cdot \exp \left(\frac{sim(\mathbf{x}_i, \mathbf{P}_c^{(j)})}{\epsilon}\right) \cdot v_j}{\sum_{i=1}^{|\mathcal{B}_c|} \sum_{j=1}^K u_i \cdot \exp \left(\frac{sim(\mathbf{x}_i, \mathbf{P}_c^{(j)})}{\epsilon}\right) \cdot v_j},
% \end{equation}
\begin{equation}
w_{i j}^{(c)}=\frac{u_i \cdot \exp \left(sim(\mathbf{x}_i, \mathbf{P}_c^{(j)})/\epsilon\right) \cdot v_j}{\sum_{i=1}^{|\mathcal{B}_c|} \sum_{j=1}^K u_i \cdot \exp \left(sim(\mathbf{x}_i, \mathbf{P}_c^{(j)})/\epsilon\right) \cdot v_j},
\end{equation}
where $\epsilon$ is a temperature parameter controlling the smoothness of the assignment. $sim(\mathbf{x}_i, \mathbf{P}_c^{(j)})$ represents the dot-product similarity between the sample and the proxy. $u_i$ and $v_j$ are auxiliary normalization factors to ensure that all assigned weights satisfy a doubly-normalized constraint, adjusted iteratively through row and column scaling.

To reduce the complexity, the top-N proxies with larger weights are selected here to participate in the computation of the proxy loss. Thus, for the sample over the selected proxies, the proxy alignment loss is denoted as:
\begin{equation}
\mathcal{L}_{pal}=-\frac{1}{|\mathcal{B}_c|} \sum_{c=1}^C \sum_{i=1}^{|\mathcal{B}_c|}\left(\sum_{j=1}^N \mathbf{W}_c^{(i, j)} \cdot \log \left(p_{i j}^c\right)\right),
\end{equation}
where $p_{i j}^c$ is the softmax probability calculation for sample $i$ and proxy $j$ within each class $c$, represent as follow:
\begin{equation}
p_{i j}^c=\frac{\exp \left(sim(\mathbf{x}_i, \mathbf{P}_c^{(j)}) / \tau\right)}{\sum_{k=1}^N \exp \left(sim(\mathbf{x}_i, \mathbf{P}_c^{(k)}) / \tau\right)},
\end{equation}
where a temperature parameter $\tau$ is applied to scale the similarities.

The distribution of proxies determines the diversity and compactness of the embedding. To ensure stable and meaningful similarity calculations, all proxies are normalized to the unit hypersphere. We make the inter-class proxies far away and the intra-class proxies close together, and here we propose the agent contrast loss as shown in Eq.~\ref{pcl_loss}. To distinguish proxy-to-proxy similarities from sample-proxy similarities, here we define the former similarity as $sim_p(\mathbf{P}_c^{(i)}, \mathbf{P}_c^{(j)})$.
\begin{align}
\label{pcl_loss}
\mathcal{L}_{pcl} = 
& -\frac{1}{K} \sum_{c=1}^C \sum_{i=1}^K \log \left(\tilde{p}_{i j}^c\right) \nonumber \\
& + \frac{1}{C(C-1)} \sum_{c_1=1}^C \sum_{c_2=1, c_2 \neq c_1}^C 
\max _{i, j} \mathbf{P}_{c_1}^{(i) \top} \mathbf{P}_{c_2}^{(j)} .
\end{align}
Similarly, $\tilde{p}_{ij}^c$ denotes the computation of softmax probability between proxy $i$ and proxy $j$ in category $c$, i.e:
\begin{equation}
\tilde{p}_{i j}^c=\frac{\exp \left(\operatorname{sim}_p\left(\mathbf{P}_c^{(i)}, \mathbf{P}_c^{(j)}\right) / \tau\right)}{\sum_{k=1, k \neq i}^K \exp \left(\operatorname{sim}_p\left(\mathbf{P}_c^{(i)}, \mathbf{P}_c^{(k)}\right) / \tau\right)} .
\end{equation}

During training, category proxies are updated using a momentum strategy to adapt to evolving application distributions. Updates combine the weighted sum of the previous proxy matrix and the samples associated with each proxy. The updated proxies $\mathbf{P}_c^{\prime}$ of class $c$ are computed as follows:
\begin{equation}
\mathbf{P}_c^{\prime}=\gamma \mathbf{P}_c+(1-\gamma) \mathbf{W}_c^T \mathbf{X}_c,
\end{equation}
where $\gamma$ is the momentum coefficient controlling the update rate. Therefore, we can obtain the multi-proxy contrastive loss as the training objective in this module, which can be formalized as:
\begin{equation}
\mathcal{L}_{MPC} = \mathcal{L}_{pal} + \lambda \cdot \mathcal{L}_{pcl},
\end{equation}
where $\lambda$ balance the weight of these two loss functions.

% \begin{equation}
% \tilde{p}_{ij}^c = \frac{\exp \left(sim_p(\mathbf{P}_c^{(i)}, \mathbf{P}_c^{(i)}) / \tau\right)}{\sum_{j=1}^K \exp \left(sim_p(\mathbf{P}_c^{(i)}, \mathbf{P}_c^{(k)}) / \tau\right)}.
% \end{equation}

% Section~\ref{learn_invariant_feature} discusses the prerequisites for learning invariant features, specifically, the need for environment segmentation to expose unstable information and for rich feature representations. Therefore, based on the definition of Invariant Risk Minimization (IRM) in Section~\ref{invariant_learning}, our goal is to encourage encoders that can learn rich feature representations to further focus on the stable part of them. So here it is necessary to ensure that the representations generated by the encoder for the same class of samples in different environments produce similar gradients in the classifier. To achieve this, we construct an invariant gradient alignment module based on IRMv1, as proposed by Arjovsky \textit{et al.}~\cite{IRM_training}, which is used to strengthen the model's focus on invariant features. The objective function is described as Eq.\ref{grad_align}:

\subsection{Invariant Gradient Alignment}
\label{invariant_alignment}
Section~\ref{learn_invariant_feature} highlights the prerequisites for learning invariant features: environment segmentation to expose unstable information and rich feature representations. Building on the definition of Invariant Risk Minimization (IRM) in Section~\ref{invariant_learning}, our goal is to guide encoders to focus on the stable aspects of these representations. To achieve this, the encoder must ensure that representations of the same class across different environments produce similar classifier gradients. We introduce an invariant gradient alignment module based on IRMv1~\cite{IRM_training} to enhance the model’s focus on invariant features. The objective function is shown in Eq.\ref{grad_align}.


\begin{equation}
\label{grad_align}
\mathcal{L}_{IGA} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \left\|\nabla_{s_{e} \mid s_{e}=1.0} R^{e}(s_e \circ \phi)\right\|^2.
\end{equation}
Here, $s_{e}$ acts as a scalar, serving the role of a dummy classifier, set to 1.0 and updates through gradient backpropagation. $\mathcal{L}_{grad}$ evaluates how adjusting $s_e$ minimizes the empirical risk in environment $e$. $R^e(s_e \circ \phi)$ is represented as Eq.~\ref{risk_minimization}:
\begin{equation}
\label{risk_minimization}
R^{e}(s_{e} \circ \phi)=\mathbb{E}^{e}\left[\mathcal{L}_{CLS} \left(s_{e}\left(\phi\right(x\left)\right), y\right)\right],
\end{equation}
where $\mathcal{L}_{CLS}$ denotes the classification loss function used in the current environment, such as binary cross-entropy for malware detection. This loss is computed following standard ERM training. The term $\phi(x)$ represents the output of the shared encoder for samples $x, y \sim p(x, y|e)$ from environment $e$. The gradient penalty term encourages uniform feature representation by aligning gradients of classifiers across environments, thereby promoting consistent model performance.

\subsection{Invariant Training Framework}
\label{invariant training}
Gradient adjustment for invariant learning classifiers relies heavily on the encoder’s ability to learn rich representations~\cite{rich}. Starting from random initialization often leads to suboptimal convergence. To overcome this, we propose a two-stage training strategy to first capture diverse features before applying invariant learning.

\subsubsection{Discriminative Information Amplification}
In the first stage, ERM training is conducted independently for each environment to initialize the encoder. The multi-proxy contrastive learning module is then applied to each environment to maximize the exploitation of discriminative features. The optimization objective is defined in Eq.\ref{erm loss}:
% In the first stage, we perform ERM training independently for each environment as the encoder's initialization. The multi-proxy contrastive learning module is applied to each environment respectively to fully exploit the discriminative features within. The optimization objective in this stage is defined as Eq.\ref{erm loss}:
\begin{equation}
\label{erm loss}
\mathcal{L}_{ERM} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} (\mathcal{L}_{CLS}^e + \alpha \cdot \mathcal{L}_{MPC}^e),
\end{equation}
where $\mathcal{L}_{CLS}^e$ and $\mathcal{L}_{MPC}^e$ denote the classification loss and multi-proxy contrastive loss for environment $e$, respectively. Jointly minimizing the empirical risk across all environments equips the encoder with diverse feature representations, forming a robust foundation for invariant training.

% Jointly minimizing the empirical risk across all environments in this stage enables the encoder to learn diverse feature representations, creating a robust foundation for subsequent invariant training.

\subsubsection{Unstable Information Suppression}
To mitigate overfitting to environment-specific features from the earlier stage, we reset the optimizer’s parameters before the second training phase. This reset allows the model to refocus on the objectives of invariant learning. In this phase, we first apply a multi-proxy contrastive loss across all samples to enhance class representation learning. Next, invariant gradient alignment is used to harmonize classification gradients across environments. The updated optimization objective is defined in Eq.~\ref{irm loss}:
\begin{equation}
\label{irm loss}
\mathcal{L}_{IRM} = \mathcal{L}_{CLS} + \alpha \cdot \mathcal{L}_{MPC} + \beta \cdot \mathcal{L}_{IGA},
\end{equation}
In training, the hyperparameters $\alpha$ and $\beta$ balance the contributions of each loss term. This two-stage approach enables the model to first capture a broad feature set, then refine it for cross-environment invariance, enhancing generalization under distribution shifts. The pseudo-code of the training process is shown in Appendix~\ref{inv_alg}.