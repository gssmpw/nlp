\section{Motivation}
\label{motivation}

\begin{figure}
    \centering
    % \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=0.95\linewidth]{Figure/family_proportion.pdf}
    \caption{The top-10 families' proportions vary over time; zero marks the test start, negatives indicate training.}
    \label{fig:family_proportion}
\end{figure}

Learning-based malware detectors deliver outstanding performance in identifying potential threats; however, maintaining consistent performance under drift scenarios remains challenging. Figure~\ref{fig:family_proportion} supports this claim, depicting the monthly proportion changes of the top-10 dominant families in our dataset (Section~\ref{Dataset}) across the training and testing phases. Notably, some families that dominate during training, such as Dowgin, gradually disappear in the testing phase, while new families, like Artemis, emerge. This dynamic evolution poses challenges for malware detectors' generalization across families. Even for families like Airpush, present in both phases, its feature distribution is also influenced by multiple factors including temporal fluctuations in proportions and API changes. 
% For instance, Android version updates, such as the restrictions on background service execution introduced in Android 8.0, compel adware to adopt alternative implementations, leading to shifts in API usage and altering features in test samples.


\subsection{Threat Model}
\label{problem_scope}
Malware natural drift refers to the gradual evolution of malware distributions over time, driven by the emergence of new families and modifications to existing ones. Without knowledge of detection methods, attackers can alter malware features, causing distribution shifts that undermine the long-term effectiveness of detection systems.

Our threat model focuses on natural drift, which differs from adversarial attacks that rely on knowledge of the detection scheme to identify optimal perturbations for evading models~\cite{pierazzi2020problemspace}. We find that even natural drift disrupts the adaptation capabilities of traditional methods, hindering their ability to generalize to new distributions. Therefore, in this work, we address the long-term challenges posed by malware natural drift by enhancing the robustness of detection systems against malware evolution. We consider adversarial drift to be out of scope for this paper and plan to explore it in future work.

% Malware natural drift refers to gradual and systematic changes in malware distribution dominated by the emergence of new families and the evolution of existing families. Adversaries are able to introduce new malware families or evolve existing malware, changing feature distributions gradually and challenging the generalizability of detectors across families and environments.

% Our threat model explicitly focuses on the natural evolution of malware and the drift it causes. Although important, we consider adversarial attacks that involve carefully crafted malware examples~attacks~\cite{pierazzi2020problemspace} to exploit model vulnerabilities to be off-topic. Unlike natural drift, such attacks stem from the identification of optimal perturbations designed to bypass a target model, rather than from the gradual evolution of malware characteristics. This work aims to strengthen the detector to allow it to adapt to these long-term natural drifts in malware distribution--an already challenging and open problem per-se. The treatment of models' robustness against adversarial attacks is therefore beyond the scope of our discussion.
% The transition of dominant families and intrafamily evolution are key obstacles to malware detector performance. 

\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=1.0\linewidth]{Figure/motivation_sample.pdf}
    \caption{(a), (b), and (c) show real code snippets from an early Airpush version, a later Airpush version, and the Hiddad adware family. Airpush's core behavior includes: (1) getting ad data from a specific URL, (2) asynchronous execution to avoid user interruption, and (3) pushing ads continuously through the background service. (a) and (b) demonstrate that both Airpush versions share invariant behaviors, with similar API calls and permissions despite implementation differences. Hiddad, while skipping step (2) for simpler ad display, shares steps (1) and (3) with Airpush, especially the newer version.}
    \label{fig:motivation_sample}
\end{figure*}

\subsection{Invariance in Malware Evolution}
% Malware families commonly evolve to circumvent new detection techniques and security measures, resulting in constant changes in their code implementations or API calls to bypass detection. This leads to drifts in the feature space and a decline in detection accuracy. Yet, we argue that during the evolution of malware, core malicious behaviors and execution patterns remain partially invariant, captured in training data through intents, permissions, and function calls. We define these invariances as intra- and inter-family invariance.
The challenges highlighted in Section~\ref{problem_scope} motivate the search for characteristics from training set which can be generalized to test samples. Indeed, definitions of malware families and types have inspired the exploration of such invariance by categorizing malware according to code structure, behavioural patterns and malicious intent. While specific implementations may vary due to evolutionary or obfuscation techniques, members of the same family typically exhibit behavioral patterns consistent with their overall goals. Furthermore, malware types similarly represent a wide range of operational intentions that remain stable across families. These internal consistencies form the basis of invariance, which we categorize as inter-family invariance and intra-family invariance: \textbf{Intra-family invariance}: Variants within a malware family maintain consistent operational behaviors and attack strategies, despite differences in implementation. \textbf{Inter-family invariance}: Common malicious patterns observed across malware families, such as resource abuse or evasion techniques.

To illustrate invariant malicious behaviors in drift scenarios, we select APKs from Androzoo\footnote{https://androzoo.uni.lu} and decompile them using JADX\footnote{https://github.com/skylot/jadx} to obtain .java files. Our analysis focuses on core malicious behaviors in the source code. For intra-family invariance, we use versions of the Airpush family, known for intrusive ad delivery, from different periods. For inter-family invariance, we examine the Hiddad family, which shares aggressive ad delivery and tracking tactics but uses broader permissions, increasing privacy risks. Figure.\ref{fig:motivation_sample} shows code snippets with colored boxes highlighting invariant behaviors across samples. While Airpush uses asynchronous task requests, Hiddad relies on background services and scheduled tasks to evade detection.


Figure~\ref{fig:motivation_sample}(a)\footnote{MD5: 17950748f9d37bed2f660daa7a6e7439} and (b)\footnote{MD5: ccc833ad11c7c648d1ba4538fe5c0445} show core code from this family in 2014 and later years, respectively. The 2014 version uses \verb|NotifyService| and \verb|TimerTask| to notify users every 24 hours, maintaining ad exposure. The later version, adapting to Android 8.0’s restrictions, triggers \verb|NotifyService| via \verb|BroadcastReceiver| with \verb|WAKE_LOCK| to sustain background activity. In Drebin’s~\cite{Arpdrebin} feature space, these invariant behaviors are captured through features like \verb|android_app_NotificationManager;notify|, \verb|permission_READ_PHONE_STATE| and so on. Both implementations also use \verb|HttpURLConnection| for remote communication, asynchronously downloading ads and tracking user activity, and sharing Drebin features such as \verb|java/net/HttpURLConnection| and \verb|android_permission_INTERNET|.


Similarly, Figure.~\ref{fig:motivation_sample}(c)\footnote{MD5: 84573e568185e25c1916f8fc575a5222} shows a real sample from the Hiddad family, which uses HTTP connections for ad delivery, along with \verb|AnalyticsServer| and \verb|WAKE_LOCK| for continuous background services. Permissions like \verb|android_permission_WAKE_LOCK| and API calls such as \verb|getSystemService| reflect shared, cross-family invariant behaviors, whose learning would enhance model detection across variants.

Capturing the core malicious behaviors of Airpush aids in detecting both new Airpush variants and the Hiddad family, as they share similar malicious intents. These stable behaviors form consistent indicators in the feature space. However, detectors with high validation performance often fail to adapt to such variants, underscoring the need to investigate root causes and develop a drift-robust malware detector.


\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: The feature space of training samples contains invariance within and among malware families to be learned.}}}
\end{center}


\subsection{Failure of Learning Invariance}
Let $f_r \in \mathcal{R}$ be a sample in the data space with label $y \in \mathcal{Y} = {0, 1}$, where 0 represents benign software and 1 represents malware. The input feature vector $x \in \mathcal{X}$ includes features $\mathcal{F}$ extracted from $f_r$ according to predefined rules. The goal of learning-based malware detection is to train a model $\mathcal{M}$ based on $\mathcal{F}$, mapping these features into a latent space $\mathcal{H}$ and passing them to a classifier for prediction. The process is formally described as follows:

\begin{equation}
\arg \min _{\theta} R_{erm}\left(\mathcal{F}\right),
\end{equation}
where $\theta$ is the model parameter to be optimized and $R_{erm}(\mathcal{F})$ represents the expected loss based on features space $\mathcal{F}$, defined as:
\begin{equation}
R_{erm}\left(\mathcal{F}\right)=\mathbb{E}[\ell(\hat{y}, y)].
\end{equation}
$\ell$ is a loss function. By minimizing the loss function, $\mathcal{M}$ achieves the lowest overall malware detection error. 


\subsubsection{Stability and Discriminability of Features}
\label{active ratio}
To investigate the drift robustness in malware evolution from the feature perspective, we define two key feature properties: stability and discriminability. Stability refers to a feature's ability to maintain consistent relevance across distributions, while discriminability reflects a feature's capacity to effectively distinguish categories. To avoid model-induced biases, we propose a modelless formal definition applicable to diverse architectures.

Let $f_j$ represent the $j$-th feature in the feature set $\mathcal{F}$, and $S$ denote the set of all samples. To capture the behavior of feature $f_j$ under different conditions, we compute its active ratio over a subset $S^{\prime} \subseteq S$, representing how frequently or to what extent the feature is ``active'' within that subset. Specifically, for a binary feature space, feature $f_j$ takes values 0 or 1 (indicating the absence or presence of the feature, respectively), the active ratio of $f_j$ in the subset $S^{\prime}$ is defined as the proportion of samples where $f_j$ is present, which is defined as Eq.~\ref{eq:active ratio}:
\begin{equation}
\label{eq:active ratio}
r\left(f_j, S^{\prime}\right)=\frac{1}{\left|S^{\prime}\right|} \sum_{s \in S^{\prime}} f_j(s).
\end{equation}
The ratio measures how frequently the feature is activated within the subset $S^{\prime}$ relative to the total number of samples in the subset. At this point, we can define the stability and discriminability of features.

\begin{myDef} 
\textbf{Stable Feature}: A feature $f_j$ is defined as stable if, for any sufficiently large subset of samples $S^{\prime} \subseteq S$, the active ratio $r\left(f_j, S^{\prime}\right)$ remains within an $\epsilon$-bound of the overall active ratio $r\left(f_j, S\right)$ across the entire sample set, regardless of variations in sample size or composition. Formally, $f_j$ is stable if:
\begin{equation}
\forall S^{\prime} \subseteq S,\left|S^{\prime}\right| \geq n_0, \quad\left|r\left(f_j, S^{\prime}\right)-r\left(f_j, S\right)\right| \leq \epsilon,
\end{equation}
where $\epsilon>0$ is a small constant, and $n_0$ represents a minimum threshold for the size of $S^{\prime}$ to ensure the stability condition holds.
\end{myDef}

When we consider discriminability, there is a need to focus on the category to which the sample belongs. Thus, let $C=\left\{C_1, C_2, \ldots, C_k\right\}$ be a set of $k$ classes, and $S_k \subseteq S$ be the subset of samples belonging to class $C_k$. The active ratio of feature $f_j$ in class $C_k$ is given by:
\begin{equation}
  r\left(f_j, S_k\right)=\frac{1}{\left|S_k\right|} \sum_{s \in S_k} f_j(s).  
\end{equation}

\begin{myDef}
\textbf{Discriminative Feature}: A feature $f_j$ is discriminative if its active ratio differs significantly between at least two classes, $C_p$ and $C_q$. Specifically, there exists a threshold $\delta > 0$ such that:
\begin{equation}
  \exists C_p, C_q \in C, p \neq q, \quad\left|r\left(f_j, S_p\right)-r\left(f_j, S_q\right)\right| \geq \delta.
\end{equation}
\end{myDef}
Furthermore, the discriminative nature of the feature should be independent of the relative class sizes, meaning that the difference in activation should remain consistent despite variations in the proportion of samples in different classes. Mathematically, for any subset $\tilde{S}_p \subseteq S_p$ and $\tilde{S}_q \subseteq S_q$, where $\left|\tilde{S}_p\right| \neq\left|S_p\right|$ or $\left|\tilde{S}_q\right| \neq\left|S_q\right|$, the discriminative property still holds:
\begin{equation}
    \left|r\left(f_j, \tilde{S}_p\right)-r\left(f_j, \tilde{S}_q\right)\right| \geq \delta.
\end{equation}


\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.48\textwidth}  
        \centering
        \includegraphics[width=1.0\textwidth]{Figure/feature_diff_10.pdf}
        \caption{}
        \label{fig:diff}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{Figure/feature_importance_10.pdf}
        \caption{}
        \label{fig:importance}
    \end{subfigure}
    
    \caption{(a) and (b) illustrate changes in the discriminability of the top 10 discriminative training features and the top 10 important testing features, respectively. ``Discriminability'' is defined as the absolute difference in active ratios between benign and malicious samples. The grey dotted line indicates the start of the testing phase, with preceding values representing each feature's discriminability across months in the training set.}
    \label{fig:feature_discrimination}
\end{figure*}


\subsubsection{Failure Due to Learning Unstable Discriminative Features}
\label{motivation: failure}
The malware detector's strong performance within the same period indicates effective learning of discriminative features to separate benign software from malware. However, performance degradation over time stems from the model's inability to capture stable discriminative features from the training set. To illustrate this, we randomly sample 110,723 benign and 20,790 malware applications from the Androzoo\footnote{https://androzoo.uni.lu} platform (2014-2021). Applications are sorted by observation date, with 2014 samples used for training and the remainder divided into 30 test intervals. We extract Drebin~\cite{Arpdrebin} features, covering nine behavioral categories such as hardware components, permissions, and restrict API calls, and select the top 10 discriminative features based on active ratio differences to track over time. The model configuration follows DeepDrebin~\cite{Grossedeepdrebin}, a three-layer fully connected neural network with 200 neurons per layer. We evaluate performance in each interval using macro-F1 scores. As shown in Figure~\ref{fig:feature_discrimination}, although the top 10 discriminative features maintain stable active ratios, the detector’s performance consistently declines. We further examine feature importance over time using Integrated Gradients (IG) with added binary noise, averaging results across five runs to ensure robustness, as recommended by Warnecke et al.~\cite{IG_explain}.

Figure~\ref{fig:feature_discrimination} compares the active ratios of top discriminative (a) and important features (b). While stable discriminative features persist, ERM-based detectors often rely on unstable features with fluctuating effectiveness, leading to poor generalization under drift. 

We observe that highly discriminative features, such as \verb|api_calls::java/lang/Runtime;->exec| and \verb|GET_TASKS|, are often linked to high-permission operations and potential malicious activity. These features, rarely seen in legitimate applications, reflect malware invariance, where core malicious intents persist despite evolving implementations.

% Figure~\ref{fig:feature_discrimination} presents the top 10 discriminative (a) and important features (b) identified by the model and their active ratio changes. While stable discriminative features persist from training to testing, ERM-based detectors often rely on unstable features with fluctuating discriminative power, causing performance inconsistencies. This over-reliance on unstable features and insufficient learning of stable ones limits their generalization under drift scenarios.


% Moreover, we observe that highly discriminative features are often associated with high-permission operations and indicate potential malicious activity. For instance, features like \verb|api_calls::java/lang/Runtime;->exec| and \verb|GET_TASKS| are rarely used in legitimate applications. This aligns with malware invariance over time, where core malicious intents remain stable even as implementation details evolve.

% While stable, highly discriminative features from the training set persist through the test phase, the ERM-based detector often relied on unstable features whose discriminative power fluctuated over time. This reliance leads to inconsistent model performance, stabilizing only when the feature discriminative power remains steady. Thus, we attribute the failure of ERM-based malware detectors in drift scenarios to their over-reliance on these unstable features and under-learning of already existing stable discriminative features, limiting its generalization to new samples.



\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: There are stable and highly discriminative features representing invariance in the training samples, yet current malware detectors fail to learn these features leading to decaying models' performance.}}}
\end{center}


\subsection{Create Model to Learn Invariance}
\label{learn_invariant_feature}
This discussion emphasizes the importance of learning stable, discriminative features for drift-robust malware detection. ERM captures both stable and unstable information correlated with the target variable~\cite{understanding}, often relying on unstable features when they are highly correlated. The key challenge is to isolate and enhance stable features, aligning with the principles of invariant learning in Section~\ref{invariant_learning}.

Invariant learning methods face challenges, as their success depends on effective environment segmentation to reveal unstable information~\cite{environment_label, env_label}. In malware detection, identifying variants that trigger distribution shifts is uncertain. High-quality representations from the encoder are also essential for invariant predictors~\cite{yang2024invariant}, yet Figure~\ref{fig:feature_discrimination} shows that even during training, features fail to distinguish goodware from malware or capture malware’s execution intent, relying on ambiguous cues.

To this end, we propose a temporal-aware environment segmentation method to reveal the instability of malware distribution drifts. In each environment, the ERM hypothesis guides the association with the target variable, and an encoder shared across environments attempts to learn all stable and unstable representations, after which it enhances the detector's generalization by minimizing the invariant risk to filter unstable elements.

% Our discussion highlights the importance of learning stable, discriminative features for drift-robust malware detection. ERM captures features correlated with the target variable, including both stable and unstable information~\cite{understanding}. When unstable information is highly correlated with the target, the model tends to rely on it. Thus, the key challenge is to isolate and enhance stable features, aligning with the goals of invariant learning outlined in Section~\ref{invariant_learning}.


% However, applying invariant learning methods is challenging. Its effectiveness presupposes firstly that the environment segmentation can expose the unstable information that the model needs to forget~\cite{environment_label, env_label}. In malware detection, it is uncertain which application variants will trigger distribution changes. Effective invariant learning requires the encoder to produce rich and diverse representations that provide valuable information for the invariant predictor~\cite{yang2024invariant}. Without high-quality representations, invariant learning may fail. This is also reflected in Figure~\ref{fig:feature_discrimination}, where even in the training phase, the learnt features are still deficient in discriminating between goodware and malware, and hard to fully represent the execution purpose of malware, relying instead on easily confusing features.

% Thus, given arbitrary malware detectors, our intuition is to use time-aware environment segmentation to naturally expose the instability in malware distribution drift. Within each environment, ERM assumptions guide associations with the target variable, while the encoder provides both stable and unstable features for the invariant predictor. By minimizing invariant risk, unstable elements are filtered, thereby enhancing the detector's generalization capability.


\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: Invariant learning helps to learn temporal stable features, but it is necessary to ensure that the training set can expose unstable information and the encoder can learn good and diverse representations.}}}
\end{center}
