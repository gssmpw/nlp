\section{Evaluation}
\label{evaluation}
This section evaluates the effectiveness of our proposed method in improving drift robustness for Android malware detectors across various feature spaces. We also analyze the contributions of each component in the invariant learning framework. The evaluation addresses the following research questions:
\begin{description}
    \item[RQ1.] Can our framework mitigate detector aging across different feature spaces?
    \item[RQ2.] Can our framework stabilize detectors in different drift scenarios?
    \item[RQ3.] Does our framework effectively learn invariant features of applications?
\end{description}
To ensure reliability, experiments were conducted using random seeds (1, 42, 2024), with results averaged. All experiments ran on an RTX A6000. The dataset and code will be released upon paper acceptance.

% In this section, we comprehensively evaluate our proposed method's effectiveness in enhancing the drift robustness of Android malware detectors across different feature spaces. Additionally, we examine the specific contributions of each component within the invariant learning framework to the overall robustness improvement. Specifically, our evaluation aims to address the following research questions:
% \begin{description}
%     \item[RQ1.] Can our method mitigate the aging of detectors based on different feature spaces over time?
%     \item[RQ2.] Can our method help the detector to be stable in different drift scenarios?
%     \item[RQ3.] Does our method effectively learn the invariant features of applications?
% \end{description}
% To avoid randomness in the experiments, we chose 1, 42, and 2024 as random seeds to train the model and averaged the test results, respectively. All experiments were performed on RTX A6000. The dataset and code for the experiment will be made publicly available upon acceptance of the paper.

\input{Table/RQ1}

\subsection{Evaluation Settings}
\subsubsection{Dataset}
\label{Dataset}
To evaluate the effectiveness of our approach in the context of long-term malware evolution, we select Android APKs from the Androzoo\footnote{https://androzoo.uni.lu} platform and construct datasets based on the time when each software was discovered (i.e., when the sample was submitted to VirusTotal\footnote{https://www.virustotal.com} for detection\footnote{Due to the modifiable or randomly generated release time of Android applications, some timestamps (dex date) are unreliable.}) spanning the period from 2014 to 2023 dataset. A detailed description of the dataset can be found in the Appendix~\ref{dataset}. In subsequent experiments, the training set is samples from 2014 and the test set covers the remaining 9 years, where the training set is further divided into a proper training set (80\%) and a validation set (20\%).




\subsubsection{Candidate Detectors}
In Android malware detection, an APK file serves as input, containing the codebase (e.g., .dex files) and configuration files (e.g., manifest.xml), which offer behavioral insights such as API calls and permissions then organized as different formats. We select three representative feature spaces: Drebin~\cite{Arpdrebin} (vector-based), Malscan~\cite{malscan} (graph-based), and BERTroid~\cite{bertroid} (string-based), each using distinct feature sources (details in Appendix~\ref{candidate}). For fair evaluation, we apply a two-layer linear classifier to each feature representation, with the final layer performing binary classification.




\subsubsection{Baseline}
To evaluate the robustness of our scheme across different feature spaces, we compare it with two non-linear baselines: (1) APIGraph~\cite{apigraph}, which enhances robustness using API-based features, and (2) Guided Retraining~\cite{guide_retraining}, which improves detector performance across multiple feature spaces. We also include T-stability~\cite{svm_ce}, a feature stability method for linear classifiers in the Drebin feature space, to demonstrate the advantages of our non-linear approach. Details on these baselines are provided in Appendix~\ref{baseline}.
% To evaluate the robustness of our proposed scheme across various feature spaces, we compare it with two non-linear baseline models: (1) APIGraph~\cite{apigraph}, which enhances robustness using API-based features, and (2) Guided Retraining~\cite{guide_retraining}, which improves detector performance across multiple feature spaces. Additionally, we include T-stability~\cite{svm_ce}, a feature stability method tailored for linear classifiers in the Drebin~\cite{Arpdrebin} feature space, to highlight the advantages of our non-linear approach. For a detailed introduction to these baselines, please refer to the Appendix~\ref{baseline}.





\subsubsection{Metrics}
% In the following experiments, we employe static and dynamic evaluation metrics to compare the performance of different approaches. For static evaluation, due to class imbalance in the Android application dataset, we use the macro-F1 score to assess model performance on a fixed-time test set, as it provides a balanced measure of precision and recall.


% For dynamic evaluation metric, we calculate the Area Under Time (AUT), a metric proposed by TESSERACT \cite{tesseract}, which measures the area under the performance curve as it changes over time. The formula for AUT is as follows:
We evaluate different approaches using static and dynamic metrics. For static evaluation, the macro-F1 score is used to address the class imbalance in the Android dataset, offering a balanced assessment of precision and recall on a fixed-time test set. For dynamic evaluation, we employ the Area Under Time (AUT), a metric proposed by TESSERACT \cite{tesseract}, which measures performance over time. AUT is calculated as:
\begin{equation}
\operatorname{AUT}(m, N)=\frac{1}{N-1} \sum_{t=1}^{N-1}\left(\frac{m(x_t+1)+m(x_t)}{2}\right),
\end{equation}
where $m$ represents the performance metric, with the F1-score selected as its instance. $m(x_t)$ denotes the performance metric at time $t$, and $N$ is the number of monthly testing slots. AUT ranges from $[0, 1]$, with a perfect classifier achieving an AUT of 1 across all test windows.

% where $m$ denotes the performance metric. Here, we select the F1-score as the instance of $m$. $m(x_t)$ denotes the performance metric at time $t$ and $N$ is the number of time slots during the testing phase, with each slot representing a month. AUT values range from $[0, 1]$, where a classifier that performs perfectly would maintain an AUT of 1 throughout all testing time windows. 



\begin{figure}[!t]
    \centering
    % First subfigure
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{Figure/Drebin_rq1.pdf}
        \caption{}
        \label{fig:sub1}
    \end{subfigure}%
    % \hspace{0.01\textwidth} % Adjust this spacing to make the figures more compact
    \hfill
    % Second subfigure
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{Figure/malscan_rq1.pdf}
        \caption{}
        \label{fig:sub2}
    \end{subfigure}%
    % \hspace{0.01\textwidth} % Adjust this spacing to make the figures more compact
    \hfill
    % Third subfigure
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{Figure/bertroid_rq1.pdf}
        \caption{}
        \label{fig:sub3}
    \end{subfigure}
    \caption{Monthly performance of DeepDrebin (a), Malscan (b), and BERTroid (c) during the first test year (2015), with models initially trained on 2014 data. \textit{Retrained} denotes detectors updated monthly with labeled test samples.}
    \label{fig:rq1}
    \hfill
\end{figure}


\subsection{Enhance Different Feature Space (RQ1)}
\label{rq1}
This section evaluates the Temporal Invariant Learning Framework (TIF) for mitigating detector degradation across feature spaces. The classifier is trained on 2014 samples and tested monthly from 2015 to 2023, with annual results (AUT(F1, 12m)) summarized in Table~\ref{tab:rq1}. To optimize short-term performance, we use monthly partitioning, with segmentation granularity discussed in Section~\ref{env seg}. Given that T-stability~\cite{svm_ce} is designed for the Drebin detector, we compare both the original Drebin~\cite{Arpdrebin} and its T-stability variant. APIGraph~\cite{apigraph} supports Drebin and DeepDrebin but is incompatible with BERTroid~\cite{bertroid} and Malscan~\cite{malscan}, while GuideRetraining~\cite{guide_retraining} applies to all detectors. We evaluate TIF against these baselines to assess absolute performance, noting that frequent switching between methods is impractical. Notably, TIF consistently outperforms others over the long term, even against the best yearly performances of alternative methods. 

% This section evaluates the temporal invariant learning framework (TIF) for mitigating detector degradation across feature spaces. The classifier is trained on 2014 samples and tested monthly from 2015 to 2023, with annual results shown in Table~\ref{tab:rq1}. To optimize short-term performance, we use monthly partitioning, with segmentation granularity discussed in Section~\ref{env seg}. Given that T-stability~\cite{svm_ce} is designed for the Drebin detector, we compare both the original Drebin~\cite{Arpdrebin} and its T-stability variant. APIGraph~\cite{apigraph} supports Drebin and DeepDrebin but is incompatible with BERTroid~\cite{bertroid} and Malscan~\cite{malscan}, while GuideRetraining~\cite{guide_retraining} applies to all detectors. We compare our proposed approach with the baseline model to obtain an absolute performance evaluation. Once deployed, frequently switching between different methods is impractical, even if the model doesn't always yield the best results. Nevertheless, our findings show that, even compared to the best yearly performance of other methods, our approach consistently outperforms them in the long term.

As drift increases over the test years, AUT declines for all methods, but TIF achieves up to an 8\% improvement in the first year. This reflects real-world scenarios where detectors are retrained annually~\cite{apigraph}. Figure~\ref{fig:rq1} illustrates 2015 monthly performance, with a grey dashed line marking the upper performance bound. For each month $n$, training and validation sets include samples up to month $n$, simulating expert-labelled data for retraining. Without additional labels, TIF closely approaches this bound, demonstrating robust monthly gains. While AUT gains diminish over time, periodic retraining ensures manageable performance decline. T-stability performs poorly on early test samples but generalizes better to distant ones. Its design is tailored to a specific dataset, and changes in data composition affect its generalization. Moreover, it prioritizes feature stability over discriminability.


% As drift increases over test years, AUT declines for all methods, but TIF maintains a lead, achieving up to an 8\% improvement in the first year. This aligns with real-world scenarios where detectors are retrained annually~\cite{apigraph}. Figure~\ref{fig:rq1} shows 2015 monthly performance, with the grey dashed line marking the upper performance bound. The grey dashed line represents the upper bound of performance for each month. For each month $n$, we split the samples into training and validation sets, label samples from months 1 through $n$ to simulate expert-labelled data, add these to the original training set, and retrain the detector for upper-bound performance evaluation. Without additional labels, TIF approaches this bound, demonstrating robust monthly gains. While AUT gains may diminish over time, this is manageable, as detectors are typically retrained periodically. 


\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: TIF slows down the aging of detectors in each feature space and has significant performance gains in the years just after deployment.}}}
\end{center}


\subsection{Robustness in Different Drift Scenarios (RQ2)}
\label{rq2}
This section evaluates how TIF aids malware detectors in learning stable representations under different drift scenarios. We define two test scenarios based on drift factors: \textbf{Closed-world}: Test sets include only malware families from the training set (15,257 malware) and benign samples. \textbf{Open-world}: Test sets contain only families absent from the training set (2,920 malware) and benign samples. Due to uneven family distributions (Figure~\ref{fig:family_proportion}), we construct test sets by extracting normal samples corresponding to the time intervals covered by each test scenario and divided into 10 equal-length temporal segments for evaluation.

We evaluated the classification performance of DeepDrebin~\cite{Grossedeepdrebin} detector with baselines and TIF in different scenarios, starting AUT(F1, 12m) calculations from the validation set. TIF improved AUT(F1, 12m) by 5.78\% in closed-world and 3.17\% in open-world settings. We computed the cosine similarity between malware representations in test intervals and the validation set to assess feature stability. Figure~\ref{fig:rq2} shows the differences, with retrained detectors serving as ground truth. Results confirm that TIF consistently produces more stable feature representations in both scenarios, demonstrating robustness to drift.



% Invariant training aims to help malware detectors learn stable representations across drift scenarios over time. To evaluate this, we separate the drift factors, i.e., the evolution of existing malware families and the emergence of new malware families, and construct two test scenarios: a closed-world scenario with only known families and an open-world scenario with novel families. We segment the test environment into 10 intervals by release date. Benign samples from matched periods are added to ensure a natural sample distribution.


% We use the top-performing DeepDrebin detector to assess representation stability. For each test interval, we compute the cosine similarity between malware feature representations in each test interval and the training set. Figure~\ref{fig:rq2} shows the variance in similarity across intervals. We retrain the detector in each interval with labeled samples to establish baseline representations for comparison. Results indicate that the invariant training framework consistently yields more stable feature representations in closed and open-world scenarios, showing robustness to drift.

\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: TIF generates stable feature representations for both evolved variants of existing families and new malware families.}}}
\end{center}

\input{Table/RQ2_2}
\input{Table/RQ3}

\subsection{Effective Invariant Feature Learning (RQ3)}
\label{rq3}
We evaluate the framework’s ability to learn invariant features. Section \ref{motivation: failure} shows that ERM models underperform by relying on unstable features. To quantify stable and discriminative feature learning, we define the Feature Contribution Score (FCS) for each feature $f_j$ as follows:
\begin{equation}
    % FCS_j = r(f_j, S^{\prime}) \cdot IS_j,
    FCS_j = \left|r\left(f_j, S_m\right)-r\left(f_j, S_b\right)\right| \cdot IS_j,
\end{equation}
where $r(f_j, S_m)$ and $r(f_j, S_b)$ is the active ratio of feature $f_j$ in malware and benign category respectively and $IS_j$ is its importance score via integrated gradients (IG) with the malicious class as the target. For model $\mathcal{M}$, its discriminative ability is the sum of all FCS values, with higher scores indicating greater focus on discriminative features. We conduct interpretable Drebin~\cite{Arpdrebin} feature space experiments to visually assess this capability using closed and open-world settings, as outlined in Section~\ref{rq2}. Results in Table~\ref{tab:rq3} show that invariant training significantly raises FCS, guiding the model to more stable and discriminative features, thus enhancing robustness.

\begin{figure}[hbt!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}  
        \includegraphics[width=0.9\textwidth]{Figure/similarity_variance_malware_exist_bar.pdf}
        \caption{Variance of representation similarity in closed-world setting}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}  
        \includegraphics[width=0.9\textwidth]{Figure/similarity_variance_malware_open_bar.pdf}
        \caption{Variance of representation similarity in open-world setting}
    \end{subfigure}
    \caption{The variance in malware feature representation similarity between detectors and the training set in closed-world (a) and open-world (b) scenarios. \textit{Retrained} denotes detectors retrained at each time point with newly labeled samples.}
    \label{fig:rq2}
\end{figure}

We also retrain the detector with TIF then analyze the top 10 important features and tracked their discriminability. Figure~\ref{fig:feature_diff_10_irm} shows improved F1 scores and greater stability in prioritized features. Permissions like \verb|GET_TASK| and \verb|ACCESS_FINE_LOCATION| exhibit stronger correlations with malicious behavior, confirming that invariant training shifts focus to more robust features.

% We also retrain the detector with TIF then analyze the top 10 important features and tracked their discriminability. As shown in Figure~\ref{fig:feature_diff_10_irm}, invariant training improved test performance, with F1 scores exceeding those of the original model and prioritized features demonstrating greater stability. Newly highlighted permissions, such as \verb|GET_TASK| and \verb|ACCESS_FINE_LOCATION| show stronger correlations with malicious behavior, confirming that invariant training helps shift focus to more robust features.

\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: TIF enables the detector to learn stable, discriminative features from the training set.}}}
\end{center}

\begin{figure}
    \centering
    % \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=1.0\linewidth]{Figure/feature_diff_10_irm.pdf}
    \caption{Discriminability for the Top-10 important features after invariant training. F1 Score (DD) represents DeepDrebin's performance~\cite{Grossedeepdrebin} trained as described in Section~\ref{motivation: failure}, while F1 Score (DD + TIF) reflects results after applying the TIF framework.}
    \label{fig:feature_diff_10_irm}
\end{figure}


\input{Table/Ablation}

\subsection{Ablation Study}
This section evaluates the robustness of each component through an ablation study. Five configurations are tested: the base model, +MPC1, +MPC1 +MPC2, +MPC1 +IGA, and +MPC1 +IGA +MPC2, where MPC1/MPC2 represent multi-proxy contrastive learning in two training stage, and IGA denotes invariant gradient alignment which is only used in the second stage. Table~\ref{tab: ablation} shows the AUT(F1, 12m) results for each setup with the Drebin detector.

MPC1 improves intra-environment representations, surpassing the baseline. Adding MPC2 and IGA enhances generalization by aligning gradients and capturing global features. The full configuration achieves the highest robustness by integrating stable and discriminative feature learning.

% In stage one, MPC1 refines intra-environment representations, surpassing the baseline despite minor drops in the final year. Adding MPC2 and IGA improves generalization by aligning gradients and learning global features, increasing resilience to drift. The full configuration achieves the highest robustness by unifying feature learning and filtering stable, risk-minimizing features.


\input{Table/Env}


\subsection{Environment Granularity Selection}
\label{env seg}
Environment segmentation is crucial in invariant learning to expose uncertainty while ensuring sufficient samples per environment for effective training. We evaluate three segmentation granularities—monthly, quarterly, and equal-sized splits ($n=4$, $8$, $12$). Table~\ref{tab: env} reports AUT(F1, 12m) scores across the test phase. Equal-sized splits increase label imbalance, hindering robust learning, while fine-grained splits improve feature discrimination but reduce per-batch samples, complicating cross-environment alignment. Therefore, when we need the model to be able to achieve short-term stability, monthly splitting is the superior choice.

% In invariant learning, environment segmentation is critical for exposing uncertainty to the model while maintaining sufficient samples per environment for effective learning. We evaluate three segmentation granularities using a one-year training set: monthly, quarterly, and equal-sized splits ($n=4$, $8$, and $12$). Table~\ref{tab: env} reports the AUT(F1, 12m) scores for each method across the test phase. Equal-sized splits often increase label imbalance, hindering robust representation learning. Fine-grained partitioning improves feature discrimination but reduces per-batch samples as environments increase, complicating cross-environment alignment. Coarser granularity supports long-term robustness, while finer granularity boosts short-term performance, with partitioning typically guided by validation results, e.g., preferring monthly splits when validated on 2015 samples.
