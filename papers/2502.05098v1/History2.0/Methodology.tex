\section{Methodology}

\subsection{Preliminary}
\subsubsection{Notations}
We use uppercase and lowercase letters to denote matrices and vectors, respectively; $|\mathcal{D}|$ represents the total number of elements in set $\mathcal{D}$. 

\subsubsection{Problem setting and Motivation}
In the concept drift adaptation problem for Android malware, we divide the samples $x \in \mathbb{R}^{d}$ involved in this task into two parts: the first part is the labeled current moment training data $\mathcal{D}_{tr} = {(x_i^{tr},y_i^{tr})}^{|\mathcal{D}_{tr }|}_{i=1}$, where $y_{i}^{tr} \in \{0, 1\}$ represents the labels corresponding to benign and malicious applications. The training data are all from a specific time period $\mathcal{T}_{tr}$, and each sample contains its corresponding release timestamp $t_i^{tr}$. The second part is the upcoming unlabeled test data $\mathcal{D}_{te}$, the timestamps of whose samples $t_i^{te}$ are strictly from future moments compared to $\mathcal{T}_{tr}$. We assume that the training and test data share the same label space and have unknown semantic similarity, but differ in data distribution. These distribution differences lead to degradation of the performance of the currently deployed model on the test data. The deployed classification model is represented as $f = c \circ \phi$, where $c$ denotes the classifier and $\phi$ is the feature extractor. In this paper, our goal is to help the encoder in an arbitrary classification model capture semantic information shared with unseen test data based on training data. However, when we consider complete application features, the model inevitably focuses on dynamically changing features, such as \textit{HttpURLConnection()}, whose C\&C server address or communication protocol is constantly changing. Therefore, to learn temporally stable semantic information, we partition the training data into different environments according to the time scale and set up the training process as two participants in an adversarial game, exposing unstable and sensitive semantic regions within each environment that may be attended to by the model and suppressing them by minimizing the inter-environmental differences. 


\subsubsection{Overall Architecture}
Based on the motivation above, we propose a time-aware invariant training method that exposes and suppresses temporal-sensitive unstable semantic information in an end-to-end manner, thereby enhancing the robustness of any malware detector in drift scenarios. Our approach comprises three core processes: \textbf{1) Training Environments Splitting}: To learn the time-invariant semantic information of Android applications, we sequentially partition the training samples into different environments based on the APK release time and applicable Android version, while maintaining the natural distribution ratio of malware observed in the wild. \textbf{2) Temporal Instability Amplification}: At this stage, the model is guided to focus on the sample features within each environment by constructing a contrastive learning task, revealing the differences between environments, and exposing time-sensitive information that should be eliminated in the feature representation. \textbf{3) Unstable Information Suppression}: Building on the time-sensitive information exposed in the previous stage, we design two alignment modules, feature representation alignment and classifier gradient alignment, to minimize the discrepancies among samples of the same class across different environments, thereby encouraging the encoder to learn time-invariant feature representations.

\subsection{Training Environments Splitting}
\label{environment_split}


Since applications include their release time as one of their properties, it is natural for us to use the timestamp of samples to partition different training environments. Given applications used for the training phase with timestamps spanning $T_{\min}$ to $T_{\max}$. Depending on a specific time interval $\Delta$, these samples can be divided into $t$ time windows, each representing an environment $e \in \mathcal{E}$, i.e. $|\mathcal{E}| = t$. The training set can be defined as an ordered multiset based on timestamps $\mathcal{D} = \{D_1,D_2, ...... D_t\} = (x_i, y_i), i \in [1,|D_t|]$ as distinct environments, each containing $|D_t|$ samples. For a given sample $x_i$ with timestamp $t_i$, it is assigned to an environment using the following equation:
\begin{equation}
\mathcal{E}(x_i)=\left\lfloor\frac{t_i-T_{\min }}{\Delta}\right\rfloor,
\end{equation}
As the environment number rises, the time stamps contained in the environments represent times closer to the present. This method exposes the distribution offset between different time intervals to satisfy the data construction premise of invariant learning. By this point, we have constructed a training set containing multiple environments, each containing samples of the same period. Each of these samples is assigned two labels, the classification label for binary classification and the multiple environment label. 


\subsection{Temporal Instability Amplification}
The effectiveness of invariant training hinges on the comprehensive exposure of environment-specific unstable information. Given the unpredictable evolution of malware, we aim to build upon the environments set in Section~\ref{environment_split} to further encourage the model to focus on the semantic information specific to each environment and maintain effective classification accuracy. This ensures that the encoder can capture a broad range of features at this stage, preparing for the subsequent step of mitigating their instability. 

To achieve this, we devise a batch-wise environment contrastive loss, designed to minimize the intra-environment distance between samples of the same class, while maximizing the inter-environment disparity. Specifically, for each input sample $x_i$, two labels are assigned after environment splitting: the classification label $y_i$ indicating malware and the environment label $y_i^e$ indicating the environment to which the sample belongs. We define a positive pair as those samples sharing both the same class and environment labels, i.e., for the positive pair $x_i$ and $x_j$ belong to the same batch, there is $y_i = y_j$ and $y_i^e = y_j^e$. However, the construction of negative pairs is more cautiously handled to avoid excessive separation of environment-specific information, which could undermine the learned representations. Only samples with inconsistent class labels are considered negative pairs. This asymmetric strategy allows the model to effectively discriminate between application categories while simultaneously learning environment-specific information, thereby maximizing the alignment of all available features. The environment contrastive loss function is defined in Eq.~\ref{l_env}:
\begin{equation}
\label{l_env}
\mathcal{L}_{env} = \frac{1}{|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} \max\left(0, \max _{j \in \mathcal{P}_i} d\left(x_i, x_j\right) - \min _{j \in \mathcal{N}_i} d\left(x_i, x_j\right) + m \right),
\end{equation}
where $\mathcal{P}_i$ and $\mathcal{N}_i$ denote the sets of positive and negative pairs for sample $x_i$, $|\mathcal{B}|$ represents the number of samples in the current batch, and $d\left(x_i, x_j\right)$ denotes the distance metric between samples. The $m$ is an adjustable hyperparameter ensuring that the distance between negative pairs exceeds that of positive pairs by at least the value defined by $m$, thereby encouraging the model to push negative pairs further apart in the feature space.

On this basis, we still need to maintain the performance of the arbitrary malware detector on each environment to ensure that discriminative features are learned correctly. For each environment in one batch, the model performs the original ERM training, minimizing the classification loss on each environment. The optimization objective is formalized as Eq.\ref{l_cls}:
\begin{equation}
\label{l_cls}
\mathcal{L}_{cls} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} L_{e},
\end{equation}
where $L_{e}$ is the classification loss, which is an alternative and depends on specific downstream tasks. $|\mathcal{E}|$ denotes the number of environments involved in the training. Thus the ultimate goal of this phase is to expose as much feature representation as possible to unstable information within each environment while minimizing classification loss, denoted as Eq.~\ref{l_amp}
\begin{equation}
\label{l_amp}
\mathcal{L}_{amp} = \mathcal{L}_{cls} + \alpha \cdot \mathcal{L}_{env},
\end{equation}
where $\alpha$ is an adjustable weighting coefficient used to determine the degree of amplification for unstable features.

\subsection{Unstable Information Suppression}
In the previous section, we helped the encoder discover unstable features that evolve over time in the same category. Thus here we wish to suppress these features to help the model learn purer features that are more robust on the time scale. To encourage the unified encoder to generate more stable feature representations, we expect the model to not only pull in the distance of feature representations learned from samples of the same class, but also ensure that these stable representations maintain the validity of constructing classification boundaries. Based on this, the invariant representation alignment and invariant gradient alignment modules are proposed here.

\subsubsection{Invariant Representation Alignment}
\label{representation_alignment}
In this section, we aim to align the representations of the environments for samples learned from the same category, to make similar samples from different environments as close as possible at the representation level. Here, a prototype-based learning approach is used to distance the samples from the category prototypes they belong to. These are more likely to provide global information about the category and motivate the learning of stable representations than contrastive learning. Specifically, we accumulate feature representations of the same class in each training batch to construct feature prototypes~\cite{prototype} for benign and malicious applications, denoted as $\mathbf{p}_c \in \mathcal{H}$, where $\mathcal{H}$ represents the feature representation space. Here, $\mathbf{z}^{k}_{c}$ denotes the feature representation of the $k^{th}$ sample of class $c \in \{0, 1\}$, collected from all environments $e \in \mathcal{E}$, with a total batch size of $|\mathcal{B}|$. The prototype representation $p_c$ for category $c$ is defined as Eq.~\ref{prototype}:
\begin{equation}
\label{prototype}
\mathbf{p}_c=\frac{1}{|\mathcal{B}|_c} \sum_{i=1}^{|\mathcal{B}|_c} \mathbf{z}^{k}_{c},
\end{equation}
where $|\mathcal{B}|_c$ is the total number of samples belonging to category $c$ across all environments in one batch. Subsequently, we compute the mean distance between each sample's feature representation and its category prototype as the feature alignment loss, which we minimize during training. Given the feature prototype, for an input sample $x$ with representations $\mathbf{z}$ and label $y$, the representation alignment loss $\mathcal{L}_{rep}$ is expressed as:
\begin{equation}
\mathcal{L}_{rep}=\frac{1}{|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} \sum_{c \in \{0, 1\}}\left\|\tilde{\mathbf{z}}^{i}_{c}-\mathbf{p}_c\right\|^2 \mathbf{1}_{y=c}
\end{equation}
We reduce the distance between same-category samples across different environments by minimizing $\mathcal{L}_{rep}$. The use of feature prototypes not only enhances the model's robustness against outliers but also ensures that each training update adequately represents each category.

\subsubsection{Invariant Gradiant Alignment}
\label{gradient_alignment}
Based on the introduction to invariant feature representation learning in Section~\ref{invariant_learning}, Following the invariant risk minimization (IRM) proposed by Arjovsky \textit{et al.}~\cite{IRM_training}, we construct invariant gradient alignment loss as the alignment penalty described as Eq.~\ref{grad_align}:
\begin{equation}
\label{grad_align}
\mathcal{L}_{grad} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \left\|\nabla_{s_{e} \mid s_{e}=1.0} R^{e}(s_e \circ \phi)\right\|^2
\end{equation}
Here, $s_{e}$ acts as a scalar, serving the role of a dummy classifier, set to 1.0 and updated through gradient backpropagation. $\mathcal{L}_{grad}$ evaluates how adjusting $s_e$ minimizes the empirical risk in environment $e$. $R^e(s_e \circ \phi)$ is represented as Eq.~\ref{risk_minimization}:
\begin{equation}
\label{risk_minimization}
R^{e}(s_{e} \circ \phi)=\mathbb{E}^{e}\left[\mathcal{L}_{cls} \left(s_{e}\left(\phi\right(x\left)\right), y\right)\right],
\end{equation}
where $\mathcal{L}_{cls}$ denotes the classification loss function used in the current environment, for example, the binary cross-entropy loss is for the binary task of malware detection. This loss is calculated following the ERM training. And $\phi(x)$ is the output of the common encoder when the input samples $x, y \sim p(x, y|e)$ are from environment $e$. This gradient penalty term promotes uniform feature representation learning by pulling in the gradient of inconsistent classifiers due to different environment feature representations, ensuring consistent model performance across environments.

\subsection{End-to-End Invariant Training}
\label{invariant training}


\subsubsection{Enhance Feature Representation}
As training progresses and loss is backpropagated, each batch contributes to the learning of invariant representations. Therefore, our scheme is designed to not only capture the latest cross-environment invariant representations during training but also to preserve the invariant information previously learned from historical data. To achieve this, we introduce a learnable global representation weight vector that dynamically weights the feature representations output by the encoder.

Let $\mathbf{z}_{i,j}$ denote the feature representation output by the encoder in the $j$-th batch of the $i$-th epoch. The global weight vector $\mathbf{w}$ is applied to $\mathbf{z}_{i,j}$ to produce a weighted embedding $\tilde{\mathbf{z}} = \mathbf{w} \odot \mathbf{z}_{i,j}$, where $\odot$ signifies element-wise multiplication. After each batch is processed, this weighted representation is fed into the classifier, serving as the foundation for learning new invariant features. The update rule for the global weights $\mathbf{w}$ is given by Eq.~\ref{global_weight}:
\begin{equation}
\label{global_weight}
\mathbf{w}_{i,j+1}=\mathbf{w}_{i,j}-\eta \frac{\partial \mathcal{L}}{\partial \mathbf{w}_{i,j}}
\end{equation}
where $\eta$ represents the current learning rate and $\mathcal{L}$ is the arbitrary loss function. This method leverages the inherent learning capabilities of neural networks to adjust global weights based on the historical efficacy of each feature in the embeddings, thereby developing a memory mechanism to maintain relevant features across different distributions. In addition, when integrated with subsequent invariant classifiers, this enhanced representation of history invariant information allows for correcting potential errors from previous learning stages. That is, inaccurate representation weights increase the invariant penalty imposed by the downstream classifier, and the training objective can calibrate it by minimizing the penalty term.

\subsubsection{Invariant Training Strategy}
Models trained based on empirical risk minimization learn features that are useful for classification, including some invariant features that we aim to capture \cite{FAT}. According to the principles of invariant learning, the alignment of classifier gradients significantly hinges on the feature learning capabilities of the encoder. If the encoder fails to learn these patterns effectively, random initialization may cause the classifier to get stuck in local optima. To address this issue, we propose a two-stage training strategy that aims to capture as many potentially useful features as possible before performing invariant training.

In the first stage, we perform ERM training as encoder initialization for each environment independently and minimize the classification loss across all environments. The optimization objective is formulated as follows:
\begin{equation}
\label{l_cls}
\mathcal{L}_{cls} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} L_{e},
\end{equation}
where $L_{e}$ is the classification loss, which is an alternative and depends on specific downstream tasks. $|\mathcal{E}|$ denotes the number of environments involved in the training. This stage ensures that the encoder learns a diverse set of feature representations from each environment, which are optimized individually, thereby reaching a relatively optimal state for subsequent invariant training.

In the second stage, given that the initialization stage may tend to prioritize environment-specific features that contribute minimally to generalization, we reset the parameters of the optimizer at this stage. This reset helps the model to forget any overfitting tendencies associated with the old loss function, thereby better adapting to the optimization objectives of invariant training. Following the discussions in Sections~\ref{representation_alignment} and~\ref{gradient_alignment}, we incorporate invariant representation alignment and invariant gradient alignment as regularization terms for the classification loss. The new optimization target is expressed as follows:
\begin{equation}
\mathcal{L}_{inv} = \mathcal{L}_{cls} + \alpha \cdot \mathcal{L}_{rep} + \beta \cdot \mathcal{L}_{grad},
\end{equation}
where $\alpha$ and $\beta$ are hyperparameters used to balance the weights of each loss function during the training process. $\mathcal{L}_{grad}$ contains  Notably, the global representation weight vector, which is used to enhance historical invariance, is only employed in the second stage to avoid hindering the rich feature learning that occurs in the first stage. By minimizing $\mathcal{L}_{inv}$, the model is encouraged to select and enhance feature representations with cross-environment invariance from the learned diverse features. This two-stage approach ensures that the model captures a broad spectrum of features initially and then focuses on refining these features to achieve cross-environment invariance, thus improving the model's generalization capability in the context of distribution drift. The training process is shown in Algorithm~\ref{alg1}.



