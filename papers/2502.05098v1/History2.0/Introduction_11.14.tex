\section{Introduction}
In open, dynamic environments, even the most effective malware detectors face significant challenges posed by various forms of distribution drift{}\footnote{For clarity, we use the terms drift, shift, concept drift/shift, and distribution drift/shift interchangeably throughout the text.}, leading to performance degradation~\cite{transcending, cade}. From a conceptual perspective, without reasoning on the underlying representations, there are two main causes of drift in Android malware classification tasks: the evolution of malware behaviors and the emergence of previously unseen malware families that exhibit new behaviors~\cite{pei2024exploiting}. The emergence of new variants and families shifts the fundamental statistical properties of test samples~\cite{tesseract, overkill, transcending, Drift_forensice}, thereby weakening the performance of detectors trained on past patterns~\cite{malware_evolution_update}.

Recent studies have explored incremental training techniques, which involve detecting new distributions during testing and frequently updating models through active~\cite{tesseract, continuous} or online learning~\cite{droidevolver, online_mal, labelless} to address distribution drift. However, these approaches incur significant labeling and model update costs. Although pseudo-labeling has been used to alleviate labeling burdens, it introduces noise that can lead to self-poisoning models~\cite{labelless, recda}, further complicating the learning process. Therefore, improving the robustness of the model in drifting scenarios is essential to reduce the frequency of model updates. Some studies have attempted to use static feature augmentation or select features that are less sensitive to malware evolution~\cite{scrr, apigraph, overkill}. Yet, these often require specific selection methods tailored to different feature spaces that may underperform as drift sources change. Ideally, a drift-robust detector would emphasize features in the training set that both contribute to high performance and remain stable under drift, thus capturing commonalities across different distributions. However, the current training paradigm inhibits the model's ability to learn such invariance.

In this paper, we conduct a comprehensive investigation into learning-based malware detectors. Our findings indicate that even when invariant features representing shared malicious behaviors exist within malware families or variants, detectors struggle to learn these features effectively. This inefficiency can be attributed to the limitations inherent in the empirical risk minimization (ERM) training paradigm. ERM operates under the assumption that training and testing data conform to the same distribution, requiring data shuffling or constructing k-fold cross-validation sets. However, in real-world scenarios, testing data is often collected after the training period~\cite{tesseract}, meaning the shuffled training set fails to capture the sequential transition pattern between training and testing samples, hindering the model's adaptability to the dynamic evolution of malware.

Invariant learning theory~\cite{IR_intro} aims to address the shortcomings of ERM by learning invariant features shared across different distributions, which aligns with our objective. This theory promotes the discovery of stable representations by dividing training data into distinct subsets or ``environments'' and encouraging the model to minimize differences between them. However, common invariant learning methods typically require prior knowledge of environment labels that reveal unstable features~\cite{environment_label, env_label} and assume the encoder is capable of learning rich, high-quality representations~\cite{yang2024invariant}. These requirements and assumptions are not trivial for malware detection, where drift arises from various non-obvious factors, compounded by imbalanced sample distributions and a diverse feature space that complicates representation learning.

This paper presents a temporal invariant training framework, TIF, designed for malware detectors to facilitate the learning of invariant features under distribution drift. Specifically, we partition the training data according to the natural release times of applications, thereby exposing the complex instability of malware evolution over time and avoiding the reliance on prior environment labels. In the context of binary classification tasks, we identify that learning homogeneous representations for a merged multi-family malware class is suboptimal. Recent works have focused on constructing positive relationships within individual malware families to enhance feature representations~\cite{continuous}. However, such an overly fine-grained approach often exacerbates sample imbalance issues. To address this, we propose a multi-proxy contrastive learning module. In our proposed framework, each proxy is a representation of a similar subset within the class, with dynamic updates to multiple proxies that aim to capture feature representations reflective of the application's true semantics. Additionally, we design an invariant gradient alignment module grounded in invariant learning theory, ensuring that the encoder generates similar gradients for samples belonging to the same class across different environments, thereby promoting the learning of high-quality, cross-environment invariant representations. Our solution is orthogonal to existing robust malware detectors; it does not depend on new feature spaces, requires no changes to existing model architectures, and can be applied to any learning-based detector. The main contributions of this paper are as follows:

\begin{itemize}
    \item We define invariance in malware evolution, positing that learning stable and discriminative invariant representations is key to mitigating malware vulnerability in drift scenarios.
    \item We propose a first temporal invariant learning framework, TIF, for malware detectors, which can be integrated with arbitrary detectors to learn invariant representations over time by exposing and compressing unstable information in drifts.
    \item We carefully design multi-proxy contrastive learning and invariant gradient alignment modules to model complex distributions within malware and encourage the model to learn high-quality and stable representations across temporal variations.
    \item We construct a 10-year dataset\footnote{We will open source the dataset metadata and code repository to foster reproducibility studies.} and a series of experiments to evaluate the robustness of the TIF across various drift scenarios and feature spaces. The results show that TIF effectively slows the detector's degradation and generates invariant representations that outperform state-of-the-art methods.
\end{itemize}