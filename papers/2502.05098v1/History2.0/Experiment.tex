\section{Evaluation}
In this section, we comprehensively evaluate our proposed method's effectiveness in enhancing the drift robustness of Android malware detectors across different feature spaces and model architectures. Additionally, we examine the specific contributions of each component within the invariant learning framework to the overall robustness improvement. Specifically, our evaluation aims to address the following research questions:
\begin{itemize}
    \item \textbf{RQ1}: Can our method mitigate the aging of detectors based on different feature spaces over time?
    \item \textbf{RQ2}: Can our method help the detector to be stable in different drift scenarios?
    \item \textbf{RQ3}: Does our method effectively learn a stable feature representation of applications across time?
\end{itemize}

\subsection{Evaluation Settings}
\subsubsection{Dataset}
To evaluate the effectiveness of our proposed approach in the context of long-term malware evolution, we have further extended the Transcendent dataset, originally proposed by Bao et al~\cite{transcending}. for the concept drift problem. This extension broadens the dataset's temporal scope to cover the period from 2015 to 2021, encompassing seven years of benign and malicious samples.  The applications were extracted from Androzoo\footnote{https://androzoo.uni.lu}, a continuously growing repository of Android applications that aggregates malicious samples from various markets, including Google Play, PlayDrone, VirusShare, and AppChina. The samples for each year were selected to reflect the observed distribution of malware in the wild, with approximately 10\% being malicious. We utilized VirusTotal\footnote{https://www.virustotal.com} to generate analysis reports for each sample, and a sample was classified as malicious if more than four vendors raised an alert. Additionally, to analyze the evolution of malware families, Euphony~\cite{euphony} was employed to extract family labels. Due to the inconsistent naming conventions of antivirus engines, some original malware samples lacked family labels. We reconstructed the dataset following the original category distribution to ensure accurate access to family labels, ultimately retaining 120,016 benign and 15,037 malicious applications from 173 families. We adhered to the methodology proposed by TESSERACT~\cite{tesseract} to eliminate potential temporal and spatial biases, using the samples from 2015 as the training set for subsequent evaluation experiments, with 80\% of the data allocated for training and 20\% reserved as the validation set.


\subsubsection{Baseline}
To evaluate the superior ability of the proposed schemes for detector robustness enhancement, we adopted APIGraph~\cite{apigraph}, the best scheme for enhancing Android malware detector robustness at the feature level, and Guided Retraining~\cite{guide_retraining}, a multi-feature-space oriented framework for Android malware detector performance enhancement. Here is a brief overview of baseline schemes and their implementation.
\begin{itemize}
    \item APIGraph~\cite{apigraph}: The scheme constructs an API semantic relationship graph using API reference documents for all platform APIs and supporting libraries downloaded from the official website and extracting permissions as entities. By clustering semantically similar APIs, they are transferred to a stable feature space. We fully follow the methodology disclosed in paper to obtain 2000 clusters of APIs and permissions, and API features belonging to the same cluster are replaced by the index number of their clusters. Since APIGraph only considers methods based on manual selection and API call sequences. Therefore, we implement comparison experiments only with its applicable feature space.
    \item Guided Retraining~\cite{guide_retraining}: This scheme proposes a training set re-partitioning and retraining framework for improving malware detection performance in arbitrary feature space. Difficult samples are selected by the prediction probability of the base classifier model, after which four sets of difficult samples classifiers are constructed based on the four types of results of binary classification, i.e., TN, TP, FN, and FP, and supervised comparative learning is incorporated to obtain spliced feature representations for training the downstream classifiers. In the inference stage, easy and difficult datasets are distinguished and reasoned separately to obtain predictions.
\end{itemize}

\input{Tab/Candidate}
\subsubsection{Candidate Detectors}
In the task of Android malware detection, the typical raw data input is the APK file of an application. An APK file mainly compresses the application's codebase (e.g., .dex files) along with configuration files (e.g., manifest.xml), providing a multi-view perspective on the application's behavior. To comprehensively evaluate the generality of the proposed invariant training framework, we categorize the primary feature sources of applications into string-based and graph-based representations, which are further divided into methods based on manual feature extraction and learning-based approaches. Additionally, due to the remarkable feature representation learning capabilities offered by pre-trained models, we include feature representations learned from general pre-trained models within the learning-based approaches. Across all feature spaces, we selected widely-used, representative detectors as candidates for our study, which cover diverse feature formats. Table~\ref{tab:candidate} illustrates the selected detectors along with their feature information.
\begin{itemize}
    \item Drebin~\cite{drebin} is a widely used malware detector that constructs binary feature vectors from nine types of information, including hardware, API calls, permissions, and network addresses. Since our proposed is designed for learning-based architectures, we have chosen its deep learning variant, DeepDrebin~\cite{deepdrebin}, as a candidate for evaluation. DeepDrebin employs the same feature space as Drebin but utilizes a deep neural network (DNN) composed of three fully connected layers for feature extraction and classification.
    \item MAMAdroid~\cite{Mamadroid} extracts API call sequences from smali files as features, we adopt the MaMa-pkg proposed in the paper as the candidate. This method models the transitions between software packages through a Markov chain, with transition probabilities between packages serving as feature vectors. We selected 366 packages from the Android official documentation and two custom obfuscation packages, ultimately constructing a 368 × 368 dimensional feature vector to represent an application.
    \item DAMO~\cite{damo} uses raw opcode sequences as features, learning sequence representations consisting of 218 tokens through an embedding layer with an embedding dimension of 128. Subsequently, a convolution neural network (CNN) with two convolutional layers, each containing 64 filters, is used for representation learning.
    \item BERTroid~\cite{bertroid} extracts Android application permissions as features and encodes them into dense representations using a BERT pre-trained model, encapsulating the contextual significance of each permission relative to others. Here, we use the pooled output from BERT’s $pooler_output$ as the feature representation for the malware detection task.
    \item Malscan~\cite{malscan} extracts sensitive API call graphs from APKs and combines the results of four centrality computation methods as features, which contain degree centrality, Katz centrality, closeness centrality, and harmonic centrality, then feed to different classifiers. We used concatenated combinations of these features based on the method disclosed in the paper.
    \item Hindroid~\cite{hindroid} also adopts API call graphs as the source of features, constructing a heterogeneous information network (HIN) to represent the various relationships within API calls. Specifically, this approach selects Android apps and API calls as entities, considering four types of relationships: app containing API call, API calls in the same code block, API calls with the same package name, and API calls with the same invoke type. Multi-kernel learning is then used to obtain embeddings of the HIN, replacing manual graph property analysis.
    
\end{itemize}
It is worth noting that, to ensure a fair comparison of the proposed method's effectiveness across different feature spaces, we applied two linear layers to the above feature representations as the classifier, with the final layer output being two dimensions, to suit the Android malware detection task.


\subsection{Enhance Different Feature Space (RQ1)}
\subsubsection{Experiment Setting}

\subsubsection{Metrics}
To evaluate the robustness of models over time, we calculate the Area Under Time (AUT), a metric proposed by TESSERACT \cite{tesseract}, which measures the area under the performance curve as it changes over time. The formula for AUT is as follows:
\begin{equation}
\operatorname{AUT}(m, N)=\frac{1}{N-1} \sum_{t=1}^{N-1}\left(\frac{m(x_t+1)+m(x_t)}{2}\right),
\end{equation}
where $m$ denotes the performance metric, here we select the F1-score as the instance of $m$. $m(x_t)$ denotes the performance metric at time $t$, and $N$ is the number of time slots during the testing phase, with each slot representing a month. AUT values range from $[0, 1]$, where a classifier that performs perfectly would maintain an AUT of 1 throughout all testing time windows. 

\subsubsection{Result Analysis}



\subsection{Robust in Different Drift Scenarios (RQ2)}
\subsubsection{Experiment Setting}

\subsubsection{Metrics}
Given the class imbalance in the Android application dataset, we use the F1 score as the primary metric for evaluating classification performance, integrating precision and recall. In the context of malware detection tasks, where the objective is to detect as much malware as possible to mitigate security risks, the False Negative Rate (FNR) is also employed as a critical metric, where the positive label is malware. Ideally, a robust detector should maintain a high F1 score and low FNR in drift. This metric measures the proportion of undetected malware. These metrics help us assess if the model effectively minimizes missed detections while maintaining a reasonable rate of false positives, ensuring a usable malware detection system. All metric results are expressed as a percentage.

\subsubsection{Result Analysis}

\subsection{Effectiveness of Learning Stable Features (RQ3)}
\subsubsection{Experiment Setting}

\subsubsection{Result Analysis}

\subsection{Parameter Selection}

\subsection{Ablation Study}

\subsection{Discussion}
Invariant learning improves the ability of a model to generalize to unseen distributions that may be encountered in the future. However, it is unrealistic to expect the model to adapt perfectly to all possible distributions. When significant distribution changes occur, the ability of invariant learning is weakened. Current research suggests that a complete solution to concept drift is primarily model updating, which entails significant costs, including the costs of labeling new data, retraining, and deployment. Additionally, the impact of noise from pseudo-labels used in automated labeling techniques cannot be ignored~\cite{labelless}. Therefore, the value of a robust malware detector lies in its ability to deliver reliable performance amid gentle drifts. This attribute extends the lifespan of the model before necessitating retraining and ensures that only truly significant samples are annotated and incorporated in updates, ultimately lowering the costs associated with deploying malware detectors.