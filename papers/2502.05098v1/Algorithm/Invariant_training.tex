\begin{algorithm}[htb]
\caption{Algorithm of Invariant Training}
\label{alg1}
    \begin{algorithmic}[1]
    \REQUIRE ~training dataset containing $|\mathcal{E}|$ environments $\mathcal{D}_{tr}$ and each sample $x$ has a binary classification label $y_{c}, c \in [0, 1]$ and an environment label $y_e, e \in \mathcal{E}$, batch size $|\mathcal{B}|$, predefined number of epochs for stage 1 training $N$, hyperparameters $\alpha$ and $\beta$ for loss weighting, model $f$ with encoder network $\phi$ and predictor $h$. 
    \ENSURE 
    \STATE \textbf{Stage 1: Discriminative Information Amplification}
    \FOR{$\text{epoch} = 1$ to $N$}
        \FOR{$i = 1, \ldots, |\mathcal{B}|$}
            \FOR{$e \in \mathcal{E}$}
            \STATE select mini-batch of samples ${x_i^{e}} \subseteq \mathcal{B}$ \\
            \STATE calculate classification loss $\mathcal{L}^{e}_{CLS}$\\
            \STATE calculate multi-proxy contrastive loss $\mathcal{L}^{e}_{MPC}$ \\
            \ENDFOR
        \STATE obtain loss for empirical risk minimization $\mathcal{L}_{ERM} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathcal{L}^{e}_{CLS} + \alpha \cdot \mathcal{L}^{e}_{MPC}$
        \STATE update $f$ by backpropagating $\mathcal{L}_{ERM}$
        \ENDFOR
    \ENDFOR
    \STATE \textbf{Stage 2: Unstable Information Suppression}
    \STATE Reset optimizer parameters
    \STATE Continue with model parameters from Stage 1\\
    \FOR{$\text{epoch} = N+1$ to $\text{TotalEpochs}$}
        \FOR{$i = 1, \ldots, |\mathcal{B}|$}
            \FOR{$e \in \mathcal{E}$}
            \STATE initialize dummy classifier $\{s_{e} = 1.0\}$ \\
            \ENDFOR
            \STATE calculate invariant gradient alignment loss $\mathcal{L}_{IGA}$ \\
            \STATE obtain classification loss $\mathcal{L}_{CLS}$ \\
            \STATE obtain multi-proxy contrastive loss $\mathcal{L}_{MPC}$ \\
            \STATE $\mathcal{L}_{IRM} \gets \mathcal{L}_{CLS} + \alpha \cdot \mathcal{L}_{MPC} + \beta \cdot \mathcal{L}_{IGA}$ \\
            \STATE update $f$ by backpropagating $\mathcal{L}_{IRM}$
        \ENDFOR
    \ENDFOR
    \end{algorithmic}
\end{algorithm}