\section{Methodology}



\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \includegraphics[width=1.00\linewidth]{Fig/section3_arch.pdf}
    \caption{The Overview of Proposed Temporal Invariant Training}
    \label{fig:section3_arch}
\end{figure*}


\subsection{Preliminary}
\subsubsection{Notations}
We use uppercase and lowercase letters to denote matrices and vectors, respectively; $|\mathcal{D}|$ represents the total number of elements in set $\mathcal{D}$. 

\subsubsection{Overall Architecture}
This section introduces the proposed time-aware invariant training method that promotes invariant feature representations of training samples in different time intervals through dual alignment of the encoder and the predictor, aiming to enhance the robustness of malware detectors in drifting scenarios. As shown in Figure~\ref{fig:section3_arch}, the framework includes two core modules: the invariant representation alignment module and the invariant gradient alignment module.
%\begin{itemize}

\paragraph{Invariant Representation Alignment Module.} This module introduces a global representation weight vector and feature prototype alignment mechanism, which allows each environment to learn its feature representation while also accumulating and enhancing global invariant representation information from history and other environments, providing rich representation knowledge for subsequent invariant gradient alignment.

\paragraph{Invariant Gradient Alignment Module.} Based on the principle of invariant learning, this module introduces environment invariant constraints to the classifier to narrow down the differences between the representations learned by the encoder across environments, and to encourage the model to further focus on stable features. In addition, since the accumulation of historical representation information may introduce errors, the gradient alignment module can also correct for them.

Overall, we trained a copy of the neural network for each environment, with some of their weights shared across environments, while others were influenced by the representation and gradient-aligned regularizer, thus encouraging the model to extract stable features across environments and be optimized for high-performance prediction under common classifiers.

\subsection{Invariant Representation Alignment}
\label{representation_alignment}
\subsubsection{Historical Invariance Enhancement}
As training progresses and loss is backpropagated, each batch contributes to the learning of invariant representations. Therefore, our scheme is designed to not only capture the latest cross-environment invariant representations during training but also to preserve the invariant information previously learned from historical data. To achieve this, we introduce a learnable global representation weight vector that dynamically weights the feature representations output by the encoder.

Let $\mathbf{z}_{i,j}$ denote the feature representation output by the encoder in the $j$-th batch of the $i$-th epoch. The global weight vector $\mathbf{w}$ is applied to $\mathbf{z}_{i,j}$ to produce a weighted embedding $\tilde{\mathbf{z}} = \mathbf{w} \odot \mathbf{z}_{i,j}$, where $\odot$ signifies element-wise multiplication. After each batch is processed, this weighted representation is fed into the classifier, serving as the foundation for learning new invariant features. The update rule for the global weights $\mathbf{w}$ is given by Eq.~\ref{global_weight}:
\begin{equation}
\label{global_weight}
\mathbf{w}_{i,j+1}=\mathbf{w}_{i,j}-\eta \frac{\partial \mathcal{L}}{\partial \mathbf{w}_{i,j}}
\end{equation}
where $\eta$ represents the current learning rate and $\mathcal{L}$ is the arbitrary loss function. This method leverages the inherent learning capabilities of neural networks to adjust global weights based on the historical efficacy of each feature in the embeddings, thereby developing a memory mechanism to maintain relevant features across different distributions. In addition, when integrated with subsequent invariant classifiers, this enhanced representation of history invariant information allows for correcting potential errors from previous learning stages. That is, inaccurate representation weights increase the invariant penalty imposed by the downstream classifier, and the training objective can calibrate it by minimizing the penalty term.

\subsubsection{Current Invariance Alignment}
Invariant learning encourages encoders to generate more similar feature representations for samples of the same class from different environments. Thus, we expect the encoder itself to have the ability to learn more consistent representations, thus facilitating this process. Specifically, we accumulate feature representations of the same class in each training batch to construct feature prototypes~\cite{prototype} for benign and malicious software, denoted as $\mathbf{p}_c \in \mathcal{H}$, where $\mathcal{H}$ represents the feature representation space. Here, $\tilde{\mathbf{z}}^{k}_{c}$ denotes the reweighted feature representation of the $k^{th}$ sample of class $c \in \{0, 1\}$, collected from all environments $e \in \mathcal{E}$, with a total batch size of $|\mathcal{B}|$. The prototype representation $p_c$ for category $c$ is defined as Eq.~\ref{prototype}:
\begin{equation}
\label{prototype}
\mathbf{p}_c=\frac{1}{|\mathcal{B}|_c} \sum_{i=1}^{|\mathcal{B}|_c} \tilde{\mathbf{z}}^{i}_{c}
\end{equation}
where $|\mathcal{B}|_c$ is the total number of samples belonging to category $c$ across all environments in one batch. Subsequently, we compute the mean distance between each sample's feature representation and its category prototype as the feature alignment loss, which we minimize during training. Given the feature prototype, for an input sample $x$ with representations $\tilde{\mathbf{z}}$ and label $y$, the representation alignment loss $\mathcal{L}_{rep}$ is expressed as:
\begin{equation}
\mathcal{L}_{rep}=\frac{1}{|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} \sum_{c \in \{0, 1\}}\left\|\tilde{\mathbf{z}}^{i}_{c}-\mathbf{p}_c\right\|^2 \mathbf{1}_{y=c}
\end{equation}
By minimizing $\mathcal{L}_{rep}$, we reduce the representational distance between same-category samples across different environments. The use of feature prototypes not only enhances the model's robustness against outliers but also ensures that each training update adequately represents each category.

\subsection{Invariant Gradient Alignment}
\label{gradient_alignment}
Based on the introduction to invariant feature representation learning in Section~\ref{invariant_learning}, Following the invariant risk minimization (IRM) proposed by Arjovsky \textit{et al.}~\cite{IRM_training}, we construct invariant gradient alignment loss as the alignment penalty described as Eq.~\ref{grad_align}:
\begin{equation}
\label{grad_align}
\mathcal{L}_{grad} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \left\|\nabla_{s_{e} \mid s_{e}=1.0} R^{e}(s_e \circ \phi)\right\|^2
\end{equation}
Here, $s_{e}$ acts as a scalar, serving the role of a dummy classifier, set to 1.0 and updated through gradient backpropagation. $\mathcal{L}_{grad}$ evaluates how adjusting $s_e$ minimizes the empirical risk in environment $e$. $R^e(s_e \circ \phi)$ is represented as Eq.~\ref{risk_minimization}:
\begin{equation}
\label{risk_minimization}
R^{e}(s_{e} \circ \phi)=\mathbb{E}^{e}\left[\mathcal{L}_{cls} \left(s_{e}\left(\phi\right(x\left)\right), y\right)\right],
\end{equation}
where $\mathcal{L}_{cls}$ denotes the classification loss function used in the current environment, for example, the binary cross-entropy loss is for the binary task of malware detection. This loss is calculated following the ERM training. And $\phi(x)$ is the output of the common encoder when the input samples $x, y \sim p(x, y|e)$ are from environment $e$. This gradient penalty term promotes uniform feature representation learning by pulling in the gradient of inconsistent classifiers due to different environment feature representations, ensuring consistent model performance across environments.

\subsection{End-to-End Invariant Training}
\label{invariant training}
\subsubsection{Training Environment Split}
Since applications include their release time as one of their properties, it is natural for us to use the timestamp of samples to partition different training environments. Given applications used for the training phase with timestamps spanning $T_{\min}$ to $T_{\max}$. Depending on a specific time interval $\Delta$, these samples can be divided into $t$ time windows, each representing an environment $e \in \mathcal{E}$, i.e. $|\mathcal{E}| = t$. The training set can be defined as an ordered multiset based on timestamps $\mathcal{D} = \{D_1,D_2, ...... D_t\} = (x_i, y_i), i \in [1,|D_t|]$ as distinct environments, each containing $|D_t|$ samples. For a given sample $x_i$ with timestamp $t_i$, it is assigned to an environment using the following equation:
\begin{equation}
\mathcal{E}(x_i)=\left\lfloor\frac{t_i-T_{\min }}{\Delta}\right\rfloor,
\end{equation}
As the environment number rises, the time stamps contained in the environments represent times closer to the present. This method exposes the distribution offset between different time intervals to satisfy the data construction premise of invariant learning. By this point, we have constructed a training set containing multiple environments, each containing samples of the same period. Each of these samples is assigned two labels, the classification label for binary classification and the multiple environment label. 

\subsubsection{Invariant Training Strategy}
Models trained based on empirical risk minimization learn features that are useful for classification, including some invariant features that we aim to capture \cite{FAT}. According to the principles of invariant learning, the alignment of classifier gradients significantly hinges on the feature learning capabilities of the encoder. If the encoder fails to learn these patterns effectively, random initialization may cause the classifier to get stuck in local optima. To address this issue, we propose a two-stage training strategy that aims to capture as many potentially useful features as possible before performing invariant training.

In the first stage, we perform ERM training as encoder initialization for each environment independently and minimize the classification loss across all environments. The optimization objective is formulated as follows:
\begin{equation}
\label{l_cls}
\mathcal{L}_{cls} = \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} L_{e},
\end{equation}
where $L_{e}$ is the classification loss, which is an alternative and depends on specific downstream tasks. $|\mathcal{E}|$ denotes the number of environments involved in the training. This stage ensures that the encoder learns a diverse set of feature representations from each environment, which are optimized individually, thereby reaching a relatively optimal state for subsequent invariant training.




In the second stage, given that the initialization stage may tend to prioritize environment-specific features that contribute minimally to generalization, we reset the parameters of the optimizer at this stage. This reset helps the model to forget any overfitting tendencies associated with the old loss function, thereby better adapting to the optimization objectives of invariant training. Following the discussions in Sections~\ref{representation_alignment} and~\ref{gradient_alignment}, we incorporate invariant representation alignment and invariant gradient alignment as regularization terms for the classification loss. The new optimization target is expressed as follows:
\begin{equation}
\mathcal{L}_{inv} = \mathcal{L}_{cls} + \alpha \cdot \mathcal{L}_{rep} + \beta \cdot \mathcal{L}_{grad},
\end{equation}
where $\alpha$ and $\beta$ are hyperparameters used to balance the weights of each loss function during the training process. $\mathcal{L}_{grad}$ contains  Notably, the global representation weight vector, which is used to enhance historical invariance, is only employed in the second stage to avoid hindering the rich feature learning that occurs in the first stage. By minimizing $\mathcal{L}_{inv}$, the model is encouraged to select and enhance feature representations with cross-environment invariance from the learned diverse features. This two-stage approach ensures that the model captures a broad spectrum of features initially and then focuses on refining these features to achieve cross-environment invariance, thus improving the model's generalization capability in the context of distribution drift. The training process is shown in Algorithm~\ref{alg1}.

\input{Alg/invariant_training}
