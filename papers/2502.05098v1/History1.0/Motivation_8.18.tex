\section{Motivation}

In this section, we explain the reasons behind the vulnerability of learning-based malware detectors when facing concept drift from the perspective of the underlying features' properties. Then, we highlight the necessity of employing temporal invariant learning to enhance the robustness of malware detectors in drifting scenarios. 
% \lc{Note: I removed the reference to NN and phrased as learning-based approaches.}

\subsection{Experiment Settings}

We perform the following empirical analysis using the Transcendent dataset provided by Barbero et al.~\cite{transcending}, which is primarily used to study concept drift and related rejection strategies of the underlying abstaining classifiers in Android malware detection tasks, with samples obtained from AndroZoo~\cite{androzoo}. The dataset includes 10\% of malware, following recommendations of Pendlebury et al.~\cite{tesseract}, covering five years from 2014 to 2018. Due to the incomplete family labeling provided, all samples containing accurate family labels were selected here and the benign samples corresponding to the time interval were randomly extracted based on this ratio. Detailed information about the dataset is provided in Section~\ref{sec5:dataset}. 

We trained models for each experiment following the settings in DeepDrebin~\cite{deepdrebin}, i.e., neural networks with three fully connected layers of size 200.

\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \includegraphics[width=1.00\linewidth]{Fig/family_distribution.pdf}
    \caption{Data distribution in the input space of the top 5 most prevalent malware families in the training set}
    \label{fig:family_distribution}
\end{figure*}

\subsection{Evolution in Malware Family}

\subsection{The Contribution of Features to Detectors}
 Let $r \in \mathcal{R}$ represent an application sample in raw data space, with $y \in \mathcal{Y} = \{0, 1\}$ indicating the corresponding labels, where 0 denotes goodware and 1 denotes malware. The input feature vector $x \in \mathcal{X}$ consists of features $\mathcal{F}$ extracted from $r$ according to specific feature mapping functions. The goal of a learning-based malware detection scheme is to learn a model $f$ based on features $\mathcal{F}$, map them to the hidden space $\mathcal{H}$ to generate their representations and feed them to the predictor to generate predictions $\hat{y} = f(x)$ that minimize the empirical risk loss, which is described as follows:
\begin{equation}
\arg \min _{\theta} R_{erm}\left(\mathcal{F}\right)
\end{equation}
where $\theta$ is the model parameter to be optimized and $R_{erm}(\mathcal{F})$ represents the empirical risk based on features space $\mathcal{F}$, defined as:
\begin{equation}
R_{erm}\left(\mathcal{F}\right)=\mathbb{E}[\ell(\hat{y}, y)]
\end{equation}
$\ell$ is a loss function. By minimizing the empirical risk, $f$ achieves the lowest overall prediction error on the given dataset. This optimization objective encourages the model to learn discriminative features that minimize the loss function.

However, in the context of concept drift, we also need to consider how these features change over the temporal scale. Based on the previous description, the ideal objective is to make the model learn temporal invariant features, i.e., these features not only satisfy the above discriminability requirement but also need to remain relatively stable in their importance over time. Thus, the definition of temporal invariant features is as follows:
\begin{myDef}
\label{Stable}
Given a set of features $\mathcal{F}$, $\mathcal{F}_{inv} \subset \mathcal{F}$ is a temporal invariant feature subset if each feature in the subset has relatively stable importance at multiple time points $t_1, t_2, \ldots, t_T$, and if these features effectively minimize a particular loss function.
\end{myDef}
 
Based on such a definition, we can naturally derive the conditions that robust malware detectors must satisfy to handle concept drift: the feature representation should include as many invariant feature abstractions as possible that are discriminative and stable. This insight prompts us to explore the extent in which learning-based malware detectors fail to learn such properties and propose a temporal invariant learning strategy to address such limitations. %vulnerabilities of current malware detectors and their suboptimal learning of these two feature properties, helping us to identify the reasons behind the limited performance of detectors and propose potential solutions.



% \vspace{-2.0cm}

\subsection{Limitations of Malware Detectors}
\subsubsection{Failure due to learning discriminative but unstable features}
A high performance on the test set over the same temporal horizon suggests the model has learned discriminative features on the training set to distinguish between goodware and malware. Here we explore the relationship between the importance of discriminative features and the performance degradation of the malware detector over time from the perspective of feature attribution. Following the suggestions by Warnecke et al.~\cite{IG_explain} on white-box explanations in cybersecurity, we employ Integrated Gradients (IG)~\cite{IG}, a method from the domain of Explainable Artificial Intelligence (XAI), as our tool for assessing feature importance. This technique determines the features that most significantly influence individual predictions by computing the integral of the gradient changes along the path from the baseline (typically a neutral point like a vector of zeros) to the input sample. To ensure a robust assessment of feature importance, binary noise is introduced to each sample, and the contribution of different features to the output inference is averaged over five runs. We choose the malware family AIRPUSH as a use case as it has a sufficient sample size throughout the temporal period considered.
% \lc{What is the path from the baseline?}

We divide the dataset containing goodware and the AIRPUSH family into five periods from 2014 to 2018, with data each year split into training and testing sets in an 80/20 ratio. Subsequently, the corresponding detectors are trained separately for each period's training set and we assess the features importance in each malware detector. Each test set includes only the AIRPUSH malware family and benign samples from the corresponding temporal period. Figure~\ref{fig:heatmap} shows the normalized changes in features importance, highlighting the top-100 features the model trained on the year 2014 focuses on, and analyzing the degree of attention these features received in subsequent years. The results suggest that some discriminative features that are emphasized in the early years are gradually ignored in later training, whereas features that are not initially emphasized may become important later. The reliance on such discriminative but unstable transient features consequently means that the model does not generalize well to new samples that deviate from the training data over time. 
%This also motivates our interest in exploring the second question of whether the generalization failure problem can be mitigated if the model focuses on stable features.


\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0.3cm}
    \includegraphics[width=1.00\linewidth]{Fig/F1_family.pdf}
    \caption{The performance of the detectors trained separately for the selected families in the testing year}
    \label{fig:f1_family}
\end{figure}

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \includegraphics[width=1.00\linewidth]{Fig/headtmap.pdf}
    \caption{Feature Importance of AIRPUSH Over Years}
    \label{fig:heatmap}
\end{figure}

% \vspace{-0.5em}
\subsubsection{Failure due to learning stable but non-discriminative features}
\label{discriminative features}
If we ignore the role representations have in capturing varying degrees of semantics~\cite{pei2024exploiting}, concept drift in malware detection tasks is mainly caused by the emergence of new malware families, which may exhibit new, previously unseen, behaviors, and the evolution of existing families, which may also realize existing behaviors in unforeseen ways that might be imprecisely captured by the underlying representations~\cite{pei2024exploiting}. Thus, for existing malware families exhibiting fewer variants over time, it is reasonable to assume the model learns more stable features because the input data space is less variable.

To validate this rationale, we first selected the five most popular malware families from the training set and plotted their distribution in the data space over time, as shown in Figure ~\ref{fig:family_distribution}, where different colors represent samples from different years. Families such as DOWGIN, REVMOB, and LEADBOLT maintain a high consistency with their data distributions from the training year (2014). Even though the popularity of these families decreases over time, their distributions are still a sample of the original ones.

Afterwards, we took samples from these five families from 2014, divided them into training and validation sets in a 80/20 split, including benign samples from the same period, and trained five detectors. Each year, these detectors were tested on datasets containing only their respective families and benign samples. We evaluate Precision and Recall of the malicious class and show them in Figure~\ref{fig:f1_family}. The results reveal that for REVMOB and LEADBOLT, almost all malware are successfully detected, i.e., their high Recall remains steady over subsequent years, suggesting the models learned stable features for these families. However, in contrast, the false positive rate increases substantially, and the discriminatory nature of these stable features for goodware and malware is in decline. This further suggests that focusing on stability only during training is also suboptimal, as it leads to a detector diverging from its original training goal.

\subsection{Learning Invariant Features}

Our preceding discussion has examined the limitations of malware detectors from the perspective of feature analysis, prompting us to recognize that if a detector is to remain robust in the face of concept drift, it must focus on both discriminability and stability. Learning discriminative features is relatively straightforward, as ERM training helps identify features most strongly correlated with the target variable, the challenge lies in finding the stable components within these discriminative features. This is consistent with the goal of invariant learning introduced in Section~\ref{invariant_learning}.

However, applying invariant learning methods is challenging. Its effectiveness presupposes firstly that the environment segmentation can maximize the unstable information that the model needs to forget~\cite{environment_label, env_label}. In malware detection, it is uncertain which application variants will trigger distributional changes. In addition, an invariant predictor cannot focus solely on stable features, but also needs to ensure that these features are well-fitted to the training data~\cite{yang2024invariant}, which requires the encoder to have a good feature learning capability to provide the invariant predictor with information that can be effectively used for discriminative and invariant learning. Thus, given arbitrary malware detectors, our scheme aims to further extend the learning capability of the encoder to provide more learnable information to the invariant predictor by relying only on the time-aware environment segmentation. Minimizing the invariant risk, in turn, filters out instability in the feature representation, thus further enhancing the generalization ability of the malware detector to the concept drift.

