
\documentclass{article} % For LaTeX2e
\usepackage[dvipsnames]{xcolor}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{arydshln} 
\usepackage{pifont} 
\usepackage{gensymb}


\title{AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{
\centerline{Ruoxuan Feng$^1$ \quad
Jiangyu Hu$^{2,3}$ \quad
Wenke Xia$^1$ \quad
Tianci Gao$^1$ \quad
Ao Shen$^1$ \quad
\textbf{Yuhao Sun}$^3$} \\
\centerline{\textbf{Bin Fang}$^3$\footnotemark[1] \quad
\textbf{Di Hu}$^1$\footnotemark[1]} \\
 \centerline{$^1$Renmin University of China \quad 
 $^2$Wuhan University of Science and Technology}
 \\
 \centerline{$^3$Beijing University of Posts and Telecommunications} \\
%\centerline{\texttt{\{fengruoxuan,dihu\}@ruc.edu.cn}}
} 

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\TAG}{\textcolor[RGB]{61,127,207}{TAG}}
\newcommand{\visgel}{\textcolor[RGB]{61,127,207}{VisGel}}
\newcommand{\Cloth}{\textcolor[RGB]{61,127,207}{Cloth}}
\newcommand{\GelSight}{\textcolor[RGB]{61,127,207}{GelSight}}
\newcommand{\Feel}{\textcolor[RGB]{61,127,207}{Feel}}
\newcommand{\OBJReal}{\textcolor[RGB]{222,156,8}{OF Real}}
\newcommand{\GelSlim}{\textcolor[RGB]{222,156,8}{GelSlim}}
\newcommand{\TVL}{\textcolor[RGB]{0,180,26}{TVL}}
\newcommand{\YCB}{\textcolor[RGB]{0,180,26}{YCB-Slide}}
\newcommand{\SSVTP}{\textcolor[RGB]{0,180,26}{SSVTP}}
\newcommand{\DIGIT}{\textcolor[RGB]{0,180,26}{DIGIT}}
\newcommand{\Octopi}{\textcolor[RGB]{151,81,203}{Octopi}}
\newcommand{\Mini}{\textcolor[RGB]{151,81,203}{GelSight Mini}}
\newcommand{\DuraGel}{\textcolor[RGB]{237,109,109}{DuraGel}}
\newcommand{\Multisensor}{Tac\textcolor[RGB]{0,180,26}{Q}\textcolor[RGB]{151,81,203}{u}\textcolor[RGB]{237,109,109}{a}\textcolor[RGB]{130,130,130}{d}}
\newcommand{\Tac}{\textcolor[RGB]{130,130,130}{Tac3D}}
\newcommand{\OBJtwo}{\textcolor[RGB]{149,55,53}{OF 2.0}}
\newcommand{\OBJone}{\textcolor[RGB]{204,204,0}{OF 1.0}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal corresponding authors.}

\maketitle

\begin{abstract}
Visuo-tactile sensors aim to emulate human tactile perception, enabling robots to precisely understand and manipulate objects. Over time, numerous meticulously designed visuo-tactile sensors have been integrated into robotic systems, aiding in completing various tasks. However, the distinct data characteristics of these low-standardized visuo-tactile sensors hinder the establishment of a powerful tactile perception system. We consider that the key to addressing this issue lies in learning unified multi-sensor representations, thereby integrating the sensors and promoting  tactile knowledge transfer between them. To achieve unified representation of this nature, we introduce \textbf{TacQuad}, an aligned multi-modal multi-sensor tactile dataset from four different visuo-tactile sensors, which enables the explicit integration of various sensors. Recognizing that humans perceive the physical environment by acquiring diverse tactile information such as texture and pressure changes, we further propose to learn unified multi-sensor representations from both static and dynamic perspectives. By integrating tactile images and videos, we present \textbf{AnyTouch}, a unified static-dynamic multi-sensor representation learning framework with a multi-level structure, aimed at both enhancing comprehensive perceptual abilities and enabling effective cross-sensor transfer. This multi-level architecture captures pixel-level details from tactile data via masked modeling and enhances perception and transferability by learning semantic-level sensor-agnostic features through multi-modal alignment and cross-sensor matching. We provide a comprehensive analysis of multi-sensor transferability, and validate our method on various offline datasets and in the real-world pouring task. Experimental results show that our method outperforms existing methods, exhibits outstanding static and dynamic perception capabilities across various sensors.  The code, TacQuad dataset and AnyTouch model are fully available at \href{https://gewu-lab.github.io/AnyTouch/}{\textcolor{blue}{gewu-lab.github.io/AnyTouch/}}.

\end{abstract}
\vspace{-4pt}
\section{Introduction}
% \vspace{-4pt}

Tactile perception is an important sense through which humans perceive the physical world.
For many years, researchers have been working to endow robots with human-like tactile perception abilities through diverse tactile sensors~\citep{liu2022neuroskin,maiolino2013flexible,yuan2017gelsight}.
Among them, with high resolution comparable to human touch, various types of visuo-tactile sensors have garnered widespread attention~\citep{yuan2017gelsight, donlon2018gelslim, lambeta2020digit}. 
Many studies have attempted to use robots equipped with visuo-tactile sensors to perform manipulation tasks such as grasping~\citep{xu2024unit} and inserting~\citep{li2014localization}.
% However, due to the wide variety of visuo-tactile sensors and their low level of standardization~\cite{rodriguez2024touch2touch}, most algorithms are designed for specific types of sensors, lacking the ability for cross-sensor transferring.

% However, research on visuo-tactile sensors lacks a effective and unified paradigm. Firstly, there is a wide variety of visuo-tactile sensors, and different laboratories use different sensors to collect data~\cite{yang2022touch,gao2023objectfolderreal}. 
% This results in a limited scale of data for each sensor. Hence, existing visuo-tactile models are typically trained and operate on a specific sensor, lacking transferability across sensors. 
% Secondly, humans perceive the physical world around them through a combination of static and dynamic tactile perception. For nearby objects, a simple touch can reveal attributes such as material and hardness. In contrast, dexterous manipulation in daily life relies on understanding the changes of deformation during continuous contact. However, works either focus solely on static perception of tactile attributes in a single tactile image~\cite{yang2022touch,cheng2024touch100k} or are limited to dynamic continuous tactile perception for specific tasks~\cite{han2024learning,puang2024learning}. They overlook the mutual reinforcement between static and dynamic tactile perception. These two issues limit the precision and sensitivity of robotic tactile perception from the perspectives of data scale and algorithm design. 
However, due to the low standardization of visuo-tactile sensors, different sensors may exhibit discrepancies in perceiving the same tactile information.
This variability poses challenges to building precise robotic tactile systems, as sensor-specific data collection~\citep{yang2022touch,gao2023objectfolderreal} and model training limit the data scale and diversity for the model of a single sensor and lead to suboptimal perception capabilities.
To address this issue, some initial efforts have explored using multi-sensor data collaboratively to enhance cross-sensor knowledge transferability~\citep{yang2024binding,zhao2024transferable}.
% Nevertheless, due to the lack of aligned multi-sensor data, these attempts face challenges in effectively integrating disparate sources of information. 
Nevertheless, the lack of aligned multi-sensor data has hindered these attempts from effectively integrating disparate sensors and constructing a unified representation space.
Considering this data issue, \cite{rodriguez2024touch2touch} collected a dual-sensor paired dataset to enable cross-sensor generation. 
However, their focus on specific manipulation tasks limited the variety of sensors and collected objects. Moreover, they overlooked the potential benefits of paired multi-modal data 
for enhancing sensor transferability and achieving comprehensive tactile perception.


To enhance support for multi-sensor integration, we collect \textbf{TacQuad}, an aligned multi-modal multi-sensor tactile dataset containing 72,606 contact frames, using four different visuo-tactile sensors. 
We select these representative sensors from publicly available sensors, self-made sensors, and force field sensors to ensure diversity.
To balance the trade-off between the cost of data collection and the accuracy of pairing, we collect fine-grained spatio-temporal aligned data on a calibration platform, while larger-scale coarse-grained spatial aligned data is acquired through the handheld collection, as shown in Figure~\ref{fig:dataset}. 
Additionally, we capture the objects being touched using a camera and annotate tactile attribute descriptions for each collection, forming a comprehensive touch-vision-language dataset. 
This dataset utilizes paired multi-modal data as a bridge to mitigate the impact of sensor variability on the understanding of tactile semantic features, and enables the explicit integration of sensors into a unified multi-sensor space for effective knowledge transfer between sensors.

Building on this solid foundation, we further revisit the challenge of learning unified multi-sensor representations: How can we obtain unified multi-sensor representations adaptable to a wide array of tasks?
We recognize that the human tactile perception is a combination of static and dynamic processes, as humans derive comprehensive tactile perception from multiple types of information such as texture, sliding, and pressure changes.
Drawing on this insight, we propose \textit{learning unified representations from both static and dynamic perspectives} to accommodate a range of tasks.

To obtain multi-sensor representations of this nature, we introduce \textbf{AnyTouch}, a unified static-dynamic multi-sensor tactile representation learning framework. 
This framework integrates the input forms of tactile images and videos, collectively utilizing them to reinforce the model's abilities to perceive both static properties and dynamic changes.
Moreover, we design a multi-level architecture to comprehensively strengthen the model's capabilities for capturing pixel-level tactile details and semantic-level sensor-agnostic features.
Specifically, we utilize masked modeling~\citep{he2022masked,tong2022videomae} to maximize the use of multi-sensor data for learning fine-grained, pixel-level details. Subsequently, we conduct multi-modal aligning and a novel cross-sensor matching task to understand semantic-level tactile properties of objects across different sensors and extract sensor-agnostic features. We aim for the multi-sensor representations to share a common space and cluster by the tactile information of the object they represent, thereby reducing the gap between sensors.
To further promote knowledge transfer across multiple sensors, we propose randomly replacing the sensor-specific tokens~\citep{yang2024binding} with universal sensor tokens during training. 
This strategy ensures the model maintains its ability to process and perceive tactile data across various sensors, while also providing knowledge from all seen sensors for generalization to unseen sensors.

We conduct both quantitative and qualitative experiments to analyze the transferability of multi-sensor data and assess the impact of our framework on the multi-sensor representation space.
Building on this, we comprehensively evaluate the static and dynamic tactile perception capabilities of AnyTouch across various tactile datasets and through a real-world experiment: fine-grained pouring. 
The experimental results demonstrate the static and dynamic perception abilities and cross-sensor transferability of AnyTouch. 
We hope the approach of learning unified multi-sensor representations from both static and dynamic perspectives will 
establish a standardized learning paradigm for visuo-tactile perception and further inspire research in multi-sensor representation learning.

\vspace{-7pt}
\section{Related Work}
\vspace{-4pt}

\textbf{Multi-Source Learning.}
Learning from multi-source data with greater data scale and diversity is expected to enhance the model's performance and generalization ability, but faces challenges in integrating the data representation spaces.
Researchers have found that multi-source models often struggle to capture a unified representation suffering from the discrepancies between data sources~\citep{glorot2011domain,zhao2019learning}. 
Contrastive learning~\citep{wang2022english,zhao2023leveraging} is proposed to learn language-agnostic representations for multi-source language training, while cycle consistency loss~\citep{zhu2017unpaired,kim2022style} aligns the target domain distributions in multi-source image generation.
Similarly, to integrate multi-source tactile data from different sensors, techniques such as multi-sensor joint training~\citep{zhao2024transferable,higuerasparsh,gupta2025sensorinvarianttactilerepresentation}, multi-modal alignment~\citep{yang2024binding}, and cross-sensor generation~\citep{rodriguez2024touch2touch} have emerged. However, these methods overlook the benefits of jointly utilizing multi-modal data and aligned multi-sensor data to bridge the sensor gap. 
In this work, we collect an aligned multi-modal multi-sensor dataset and propose learning unified multi-sensor representations.
% match the multi-source gap in visuo-tactile perception models, leading to robust and generalizable representation in downstream tactile perception.

\textbf{Visuo-tactile Perception.} Visuo-tactile sensors have garnered widespread attention due to their high resolution~\citep{yuan2017gelsight, donlon2018gelslim, lambeta2020digit, zhang2024duragel,zhang2025artificial}. Nowadays, many works utilize visuo-tactile sensors to capture contact deformations, enabling the completion of dexterous manipulation such as dense packing~\citep{li2022see}, grasping~\citep{xu2024unit}, and insertion~\citep{li2014localization}. Visuo-tactile sensors can also collaborate with other sensors to assist the robot in performing more complex manipulation tasks. For instance, \cite{feng2024play} dynamically integrates visuo-tactile signals with visual and audio data to collaboratively accomplish fine-grained manipulation tasks, such as pouring and peg insertion with keyway. In addition to these tasks requiring dynamic tactile perception, visuo-tactile sensors are also used in static tasks such as material classification~\citep{yang2022touch} and shape reconstruction~\citep{gao2022objectfolder2}. 
However, due to the low standardization of visual-tactile sensors, these methods fail to leverage larger and more diverse data from other sensors and lack sensor transferability. In this work, we propose learning a unified multi-sensor representation from both static and dynamic perspectives.


% \textbf{visuo-tactile Sensor Transferring.} visuo-tactile sensors are diverse and lack standardization. Most algorithms rely on data from a specific sensor for training, and they could become non-functional when that sensor is unavailable. Some works have made preliminary attempts to train the same model using data from multiple sensors, thereby enabling the model to transfer across different sensors~\cite{yang2024binding, zhao2024transferable}. 
% By using contrastive learning, \cite{yang2024binding} try to bind multiple sensors with the vison modality. In contrast, \cite{zhao2024transferable} integrates datasets from different sensors and trains a unique encoder for each sensor. Recently, \cite{rodriguez2024touch2touch} proposes training a cross-sensor generative model using paired data from two sensors, allowing the use of the relevant algorithms of a specific sensor when it is unavailable.
% In this work, we integrate the advantages of these methods. We explore learning to extract sensor-agnostic features from data across multiple sensors to achieving cross-sensor transferability.

\begin{figure*}[t]
  \centering
    \includegraphics[width=12.4cm]{figures/DATASET.pdf}
    % \vspace{-5pt}
  \caption{\textbf{TacQuad: an aligned multi-modal multi-sensor tactile dataset from four visuo-tactile sensors.} We select \Mini~\citep{gelsight} \ and \DIGIT~\citep{lambeta2020digit} \ from publicly available sensors, \DuraGel~\citep{zhang2024duragel} \ from self-made sensors, and \Tac~\citep{zhang2022tac3d} \ from force field sensors for diversity. There is a noticeable gap between the data from these sensors. We use the four sensors to touch the same position on the same object to obtain aligned data. To maximize aligned data collection, we use two methods to gather subsets with different alignment accuracy. We collect fine-grained spatio-temporal aligned data on a calibration platform, while larger-scale coarse-grained spatial aligned data is acquired through handheld collection. 
  % We collect various objects both indoors and outdoors, forming a diverse tactile dataset.
  } 
  \vspace{-5pt}
  \label{fig:dataset}
\end{figure*}

\textbf{Representation Learning.} Representation learning has achieved remarkable success in improving model generalization in various fields. Techniques like BERT~\citep{devlin2018bert}  and masked autoencoder (MAE)~\citep{he2022masked}  have enhanced the model's performance across various downstream applications. 
With the rise of multi-modal learning, representation learning has expanded its impact across fields.
Vision-language pre-training~\citep{radford2021clip} has seen tremendous success, and more modalities, including audio~\citep{guzhov2022audioclip}, touch~\citep{yang2024binding}, and 3D point clouds~\citep{xue2023ulip}, are being integrated. Among them, tactile information from visuo-tactile sensors can be expressed as images, allowing vision-related techniques to make strides in touch.  Applying MAE~\citep{cao2023learn,higuerasparsh} or multi-modal aligning~\citep{yang2024binding, cheng2024touch100k} has enhanced tactile model capabilities. 
However, these efforts have not explored how to obtain a unified visuo-tactile representation suitable for various tasks.
Our research addresses this challenge from both static and dynamic perspectives, enhancing cross-sensor transferability across various tasks through semantic-level multi-modal aligning and cross-sensor matching.

% \textbf{Tactile Representation Learning.} Representation learning methods have been widely used to enhance the perceptual capabilities of tactile models. An effective strategy is to align the touch modality with other modalities, including touch-vision alignment~\cite{kerr2022ssvtp,dave2024multimodal,yang2024binding,george2024visuo}, touch-text alignment~\cite{yu2024octopi}, and touch-vision-text alignment~\cite{cheng2024tlv,cheng2024touch100k,fu2024tvl,lei2024vitlen}. Besides, another effective approach is generative representation learning. Some works~\cite{cao2023learn,zhao2024transferable} use MAE~\cite{he2022masked} to utilize unpaired tactile data. \cite{xu2024unit} use VQVAE~\cite{van2017neural} to learn a compact tactile latent space. Unlike these methods, our framework not only utilizes MAE and multi-modal alignment simultaneously, but also extends the input format to tactile videos. Additionally, we propose a cross-sensor matching task to enhance the model's cross-sensor transferability.
% \vspace{-9pt}
\section{Aligned Multi-modal Multi-Sensor tactile dataset}
\vspace{-8pt}


% Due to the low standardization of visuo-tactile sensors, most algorithms are trained using only a single type of sensor data, leading to poor transferability to data from other sensors. 
The low standardization of visuo-tactile sensors and the gap between multi-sensor data have resulted in insufficient data for individual sensors and the poor cross-sensor transferability of tactile models.
\cite{rodriguez2024touch2touch} has made an initial attempt to address it by collecting a dataset with 32,256 pairs of tactile images from two sensors with a limited variety of objects for specific manipulation tasks. 
It does not consider tactile properties  such as material and hardness, and overlooks the potential to enhance cross-sensor transfer capabilities through multi-modal information.

% A key solution to this problem is to provide paired data from multiple sensors, along with text and visual pairs, explicitly enabling the model to learn semantic-level tactile attributes and sensor-agnostic tactile features. 
In this work, we present a more comprehensive solution to this problem by providing multi-sensor aligned data with text and images, explicitly enabling the model to learn semantic-level tactile attributes and sensor-agnostic features to form a unified multi-sensor representation space through data-driven approaches.
We collect \Multisensor, an aligned multi-modal multi-sensor tactile dataset with a greater variety of objects, larger data volume, and more types of sensors.  
% We collect data from four types of visuo-tactile sensors: \Mini~\cite{gelsight}, \DIGIT~\cite{lambeta2020digit}, \DuraGel~\cite{zhang2024duragel}, and \Tac~\cite{zhang2022tac3d}. 
To ensure sensor diversity, we select \Mini~\citep{gelsight} and \DIGIT~\citep{lambeta2020digit} from publicly available sensors, \DuraGel~\citep{zhang2024duragel} from self-made sensors, and \Tac~\citep{zhang2022tac3d} from force field sensors for data collection.
% Among them, \Mini \ and \DIGIT \ are two widely used sensor, \DuraGel \ is representative of self-made sensors, while \Tac \ is a specialized sensor for modeling force fields.
The first three sensors are used to collect tactile images, while the Tac3D is used to capture deformation force fields. 
% During the data collection, we sequentially use four different sensors to touch the same position of the same object to obtain aligned data. 
% Specifically, based on the level of pairing precision, we collected two sets of data: 
However, considering collecting fine-grained aligned tactile data is very costly, to collect data on a larger scale while ensuring as much data pairing as possible,
we use both coarse and fine methods to collect aligned data:

\textbf{Fine-grained spatio-temporal aligned data}. We fix the four sensors side by side in a rectangular container and connect them to the movable end of a calibration platform. We use a program to control the four sensors to press the same position on the same object in sequence, sharing the same speed and depth. Consequently, we obtain a set of continuous spatio-temporally aligned contact frames. 
Due to the high requirement of precision, the process is time-consuming, thus we try our best to collect 30 sets of aligned data across 25 objects. 
This portion of the data contains a total of 17,524 contact frames, which can be used for fine-grained tasks such as cross-sensor generation. {See more details for the fine-grained data collection in Sec.~\ref{sec:fine-grained} of the Appendix.}

\textbf{Coarse-grained spatial aligned data}. We collect data in a handheld manner by sequentially pressing the same location on the same object with four sensors. While pressing, we introduce some twisting motions to the handheld sensors to better simulate the authentic dynamic touch experience of humans. This method allows us to obtain a larger amount of aligned data in a short time. Using this approach, we collect 151 sets of aligned data from 99 objects, including both indoor and outdoor scenes. This portion of the data contains a total of 55,082 contact frames.

Each tactile frame in the dataset has a paired visual image and tactile attribute descriptions that generated using GPT-4o and manually corrected. 
We aim to bridge the gap between sensors and achieve a more comprehensive tactile perception by aligning with the multi-modal data.
As a result, we obtain an aligned multi-sensor multi-modal tactile dataset, 
as shown in Figure~\ref{fig:dataset}. 
% Considering the significant differences between the force field data and tactile images, we does not use \Tac \ data during representation learning. We hope to use this data in future downstream tasks to explore cross-sensor force field reconstruction.
\begin{figure*}[t]
  \centering
    \includegraphics[width=11.7cm]{figures/pipeline.pdf}
    \vspace{-5pt}
  \caption{\textbf{Overview of AnyTouch.} Our framework integrates static tactile images and dynamic tactile videos, aiming to learn a unified multi-sensor representation suitable for various tasks. Through a multi-level architecture, we employ masked modeling to learn pixel-level tactile details, and use multi-modal aligning and cross-sensor matching to understand semantic-level sensor-agnostic tactile properties.
  We also use universal sensor tokens to integrate and transfer sensor information.}   
  \vspace{-15pt}
  \label{fig:pipeline}
\end{figure*}

\vspace{-9pt}
\section{Method}
\vspace{-7pt}
In this section, we introduce AnyTouch, a unified multi-sensor tactile representation learning framework from the perspectives of both static and dynamic perception, as shown in Figure~\ref{fig:pipeline}. 
Concretely, we integrate the input format of tactile images and videos (Sec.~\ref{sec:input}) and focus on learning both fine-grained pixel-level details for refined tasks (Sec.~\ref{sec:mask}) and semantic-level sensor-agnostic features for understanding properties (Sec.~\ref{sec:align}) and building unified space (Sec.~\ref{sec:match}) by a multi-level structure.
We also propose universal sensor tokens for better knowledge transfer.
% We aim to achieve unified multi-sensor tactile representations with comprehensive perception capabilities for various tasks.
% We aim to achieve comprehensive and unified tactile representation suitable for various tasks, 
% and capable of cross-sensor transferability, through a data-driven approach.
% We will provide a detailed introduction to how our framework utilizes datasets from multiple sensors and the \Multisensor \ dataset to learn multi-level and sensor-agnostic tactile representations in this section.

\vspace{-6pt}
\subsection{Unified input format for Static and dynamic tactile Perception}
\vspace{-4pt}
\label{sec:input}
% In daily life, human tactile perception encompasses both static and dynamic processes. A brief touch on objects enables quick recognition of properties like material and hardness. Conversely, tasks such as unlocking a lock or twisting a bottle cap necessitate continuous dynamic tactile perception. These two types of tactile perception complement each other, enabling us to interact with our surroundings more flexibly and accurately. This inspires us to combine static perception based on single tactile images with dynamic perception based on continuous streams of tactile images. 
In daily life, human tactile perception includes both static and dynamic processes. A brief touch allows quick recognition of properties like material and texture, while tasks such as unlocking a lock require continuous dynamic perception. 
These two types of perception complement each other, enabling us to comprehensively understand the physical surroundings and engage in a variety of interactions. 
This inspires us to learn unified multi-sensor representation from the perspective of combining static and dynamic perception, using tactile images and videos respectively.

 
Given a static tactile image $I \in \mathbb{R}^{1 \times H \times W \times 3}$ and a dynamic tactile video clip $V \in \mathbb{R}^{F \times H \times W \times 3}$, we consider tactile images as single-frame static videos to unify tactile images and videos.
Concretely, we replicate $I$ along the time axis for $F$ times, and use a unified 4-D tensor $X_T \in \mathbb{R}^{F \times H \times W \times 3}$ to represent both $I$ and $V$ as~\cite{girdhar2022omnivore,girdhar2023imagebind}, where $F$ is the number of frames and $H,W$ denote the shape of images. 
We then process $X_T \in \mathbb{R}^{F \times H \times W \times 3}$ into spatio-temporal tokens $z \in \mathbb{R}^{N \times d}$ through a shared patch projection layer, where $N$ is the length of tokens and $d$ represents the feature dimension. By unifying the processing of images and videos in this manner, our approach integrates tactile images and video input, enhancing the model's ability to comprehend both static and dynamic information, and endows the model with the potential to accomplish various tasks.
\vspace{-6pt}
\subsection{Masked Modeling: learning Pixel-level Details}
\label{sec:mask}
\vspace{-4pt}
Visuo-tactile images are fine-grained data with pixel-level details of subtle tactile deformations and continuous changes during dynamic processes, especially for refined perception tasks.
To enhance the fine-grained perception capabilities of the tactile representation model, we employ the masked autoencoder technique~\citep{he2022masked,tong2022videomae}, compelling the model to capture pixel-level details across multiple sensors.
% An effective representation learning method that can leverage this extensive tactile data without requiring any paired data or annotations is MAE pre-training~\cite{he2022masked,tong2022videomae}. 
Concretely, we randomly mask the tokens of both tactile images and videos with a masking ratio $\rho$, and build a decoder to obtain the reconstructed static images $\hat{I}$ and dynamic videos $\hat{V}$. 
The corresponding loss function $\mathcal{L}^{S}_{rec}$ and $\mathcal{L}^{D}_{rec}$ are \textit{mean squared
error} (MSE) loss between the original masked tokens and reconstructed ones in the pixel space:
\vspace{-3pt}
\begin{equation}
\mathcal{L}^S_{rec} = \frac{1}{|\Omega_M|} \sum_{p \in \Omega_M} | \hat{I}(p) - I(p) |^2, \ \ \mathcal{L}^D_{rec} = \frac{1}{F|\Omega_M|} \sum_{f}^F \sum_{p \in \Omega_M} | \hat{V}_f(p) - V_f(p)|^2,
\vspace{-2pt}
\end{equation}
where $p$ is the token index, $\Omega_M$ is the set of masked tokens and $V_f$ is the $f$-th frame in the video $V$.
% reconstruct the masked portions $\hat{I}(p), \hat{V}(p)$, where $p$ is the masked index. 
% For tactile images, the training process of reconstruction is the same as in the original MAE~\cite{he2022masked}. For videos, we reconstruct multiple patches from each spatio-temporal token, similar to the Tube Masking strategy in VideoMAE~\cite{tong2022videomae}. 
We use masked modeling to learn fine-grained tactile deformation features at the pixel level, as well as the temporal dynamics of tactile changes.

To further enhance the model's understanding of continuous deformation changes, we introduce an additional task of predicting the next frame $V_{F+1}$ while reconstructing the dynamic video $V$.  The loss function $\mathcal{L}^D_{pred}$ is MSE loss between the original frame $V_{F+1}$ and the predicted frame $\hat{V}_{F+1}$:
\vspace{-6pt}
\begin{equation}
\mathcal{L}^D_{pred} = \frac{1}{N} \sum_{p}^{N} | \hat{V}_{F+1}(p) - {V}_{F+1}(p) |^2.
\vspace{-5pt}
\end{equation}
% \vspace{-2pt}
% By applying masked modeling on tactile images and videos, we empower the model to capture pixel-level tactile details, preparing it for refined perception tasks. 
% However, possessing this capability alone is not sufficient for a unified multi-sensor representation. It also requires using multi-modal data and aligned multi-sensor data to learn semantic-level features and minimize sensor gaps.
% \textcolor{red}{To do}
% % To Do
\vspace{-8pt}
\subsection{Multi-modal Aligning: understanding semantic-level properties}
\label{sec:align}
\vspace{-5pt}
After obtaining tactile representations with fine-grained perceptual details via masked modeling, we aim to further understand semantic-level tactile properties and use paired multi-modal data as a bridge to narrow the gap between sensors.
Therefore, we propose using multi-modal aligning, which binds data from various sensors with paired modalities for a more comprehensive semantic-level perception and reduce perceptual differences between sensors.
However, differences in data collection scenarios across various datasets (\textit{e.g.}, simulation vs. reality) make simple vision-tactile alignment less effective in bridging sensor gaps.
Therefore, we select the text modality, which consistently describes tactile attributes across datasets, as an anchor to align touch, vision, and text modalities. 
Since tri-modal tactile datasets are rare, with most containing only vision-touch pairs, we explore two strategies: automatically expanding the amount of text modality pairings and designing aligning methods that are compatible with missing modalities. We first select representative datasets for each sensor and then use GPT-4o to generate or expand the text modality within these datasets. Through this method, we create new text pairs for 1.4 million samples across the four datasets.

% We first select representative datasets for each type of tactile sensor, and then use GPT-4o to generate or expand the text modality within these datasets. We define a complete touch at a specific location on an object as an annotation unit. We then generate corresponding sentence-level text descriptions for 9,270 touches in Touch and Go~\cite{yang2022touch} and 3,549 touches in ObjectFolder Real~\cite{gao2023objectfolderreal}, and expand the text descriptions for 5,578 touches in TVL~\cite{fu2024tvl}, SSVTP~\cite{kerr2022ssvtp} and Octopi~\cite{yu2024octopi}. We create new text pairs for approximately 1.5 million samples across the five datasets in total.

Based on these extensive tactile datasets, we develop a modality-missing-aware touch-vision-language contrastive learning method to leverage the paired data between touch and other modalities for alignment. 
We maximize the use of paired data by selecting the largest subset for each modality combination within the batch for multi-modal aligning.
Considering a pair of uni-modal representations $(x_T,x_V,x_L)$ derived from uni-modal encoders, where $x_T \in \mathbb{R}^{d}$ is the touch representation, $x_V \in \mathbb{R}^{d} \cup \varnothing$ is the vision representation and $x_L \in \mathbb{R}^{d} \cup \varnothing$ is the text representation. 
We then perform multi-modal alignment~\citep{radford2021clip} within the batch as (taking vision-language aligning with missing modalities as an example):
\begin{equation}
\begin{aligned}
% \mathcal{L}_{T \rightarrow V} &= -\frac{1}{|\Omega_V|} \sum_{i \in \Omega_V}\log \frac{\exp(x_{T,i}^\top \cdot x_{V,i} / \tau)}{\sum_{j \in \Omega_V} \exp(x_{T,i}^\top \cdot x_{V,j} / \tau)}, \\
% \mathcal{L}_{T \rightarrow L} &= -\frac{1}{|\Omega_L|} \sum_{i \in \Omega_L}\log \frac{\exp(x_{T,i}^\top \cdot x_{L,i} / \tau)}{\sum_{j \in \Omega_L} \exp(x_{T,i}^\top \cdot x_{L,j} / \tau)}, \\
\mathcal{L}_{V \rightarrow L} &= -\frac{1}{|\Omega_V \cap \Omega_L|} \sum_{i \in \Omega_V \cap \Omega_L}\log \frac{\exp(x_{V,i}^\top \cdot x_{L,i} / \tau)}{\sum_{j \in \Omega_v \cap \Omega_L} \exp(x_{V,i}^\top \cdot x_{L,j} / \tau)},
\end{aligned}
\end{equation}
where $B$ is the batchsize, $\Omega_V, \Omega_L$ are sets of indices for the samples containing vision and text, and $\tau$ is the scalar temperature. The computation of similar $\mathcal{L}_{T \rightarrow V}$ and $\mathcal{L}_{T \rightarrow L}$ is shown in Sec.~\ref{sec:loss} of the Appendix. This approach maximizes the use of paired data with missing modalities by aligning the sample intersections between modalities.
The computation of $\mathcal{L}_{V \rightarrow T}$, $\mathcal{L}_{L \rightarrow T}$ and $\mathcal{L}_{L \rightarrow V}$ is similar but in the opposite direction. We then obtain the joint aligning loss as:
\begin{equation}
\mathcal{L}_{align} = \frac{\alpha_{TV}}{2}(\mathcal{L}_{T \rightarrow V} + \mathcal{L}_{V \rightarrow T}) + \frac{\alpha_{TL}}{2}(\mathcal{L}_{T \rightarrow L} + \mathcal{L}_{L \rightarrow T}) + \frac{\alpha_{VL}}{2}(\mathcal{L}_{V \rightarrow L} + \mathcal{L}_{L \rightarrow V}),
\end{equation}
where $\alpha_{TV}$, $\alpha_{TL}$ and $\alpha_{VL}$ are hyper-parameters to control the alignment strength.
% Through multi-modal aligning, we enable the model to understand semantic-level properties and utilize other modalities to bridge the gap between sensors.
% However, it is inefficient in utilizing the aligned multi-sensor data as it does not leverage the alignment between sensors for explicit integration.

\begin{figure*}[t]
  \centering
    \includegraphics[width=12cm]{figures/compare.pdf}
  \vspace{-4pt}
  \caption{\textbf{Comparison with existing multi-modal aligning methods.} Our method not only uses multi-modal data to bridge the gap between sensors, but also \textbf{explicitly} clusters representations of the same position on the same object from different sensors together.} \vspace{-10pt}         \vspace{-4pt}                
  \label{fig:compare}
\end{figure*}
\vspace{-4pt}
\subsection{Cross-Sensor Matching: extracting sensor-agnostic features}
\label{sec:match}
\vspace{-4pt}
To fully utilize multi-sensor aligned data and build unified space by clustering multi-sensor tactile representations of the same object, we introduce a novel cross-sensor matching task. In this task, the model needs to determine whether two tactile images or videos are collected from the same position on the same object. We aim to cluster representations of the same tactile information from different sensors while performing multi-modal aligning, 
thereby enhancing the learning of sensor-agnostic features and forming a unified multi-sensor representation space, as shown in Figure~\ref{fig:compare}.

% Although multi-modal alignment binds data from different tactile sensors to the same other modalities, \textcolor{red}{this implicit sensor generalization is still influenced by variations in dataset scale and collection scope. } Hence, we introduce the cross-sensor matching task to explicitly extract sensor-agnostic features from the aligned data of various sensors \textcolor{red}{inspired by~\cite{lin2020interbert} need?}. 

We treat data collected from the same object and position by two different sensors as a positive pair, and data from different objects or positions as a negative pair. 
The model is trained to distinguish between positive and negative pairs.
For each image and video sample $X_T$ in our TacQuad, we randomly select one sample from the same object at the same location captured by another sensor as the positive sample $X_T^+$, and choose another sample from any dataset of any other object or location as a negative sample $X_T^-$. We element-wisely multiply the touch representation $x_T$ with $x_T^+$ and $x_T^-$, and then input each result into an MLP to compute the matching scores $m^+$ and $m^-$: 
\begin{equation}
m^+ = MLP(x_T \cdot x_T^+), \ m^- = MLP(x_T \cdot x_T^-),
\end{equation}
where $x_T$, $x_T^+$ and $x_T^-$ are the representations of $X_T$, $X_T^+$ and $X_T^-$. The loss function $\mathcal{L}_{match}$ is a Binary Cross Entropy Loss similar to \cite{lin2020interbert}:
\begin{small}
\begin{equation}
\begin{aligned}
\mathcal{L}_{match} = -(y^+\log(m^+) +(1-y^+)\log(1-m^-)) - (y^-\log(m^-) +(1-y^-)\log(1-m^-)).
\end{aligned}
\end{equation}
\end{small}

This task requires the model to distinguish features with the same semantics from different sensors, thus explicitly clustering representations with the same object information form a unified multi-sensor representation space. As shown in Figure~\ref{fig:compare}, AnyTouch, incorporating this task, differs from existing multi-modal aligning efforts. The construction of this unified multi-sensor representation space can explicitly reduce the gap between sensors and aid in generalizing to unseen sensors.
% As shown in Figure~\ref{fig:compare}, the proposed cross-sensor matching task leverages aligned multi-sensor data to explicitly aggregate representations from various sensors at the object level. This approach differs from existing multi-modal aligning methods and Unitouch that implicitly binds multiple sensors through the vision modality. The ability to extract sensor-agnostic features learned in the cross-sensor matching task can not only bridge the gap between different sensors but may also aid in generalization to unseen sensors.

As both this task and multi-modal aligning focus on semantic-level features, we combine them as the second stage, with masked modeling as the first stage. This multi-level training approach allows us to develop unified multi-sensor representations adaptable to tasks of varying granularities.
\vspace{-7pt}
\subsection{Universal Sensor Token}
\label{sec:unitoken}
\vspace{-4pt}
In addition to building a multi-sensor representation space, we aim to extract and store information related to each sensor to aid the understanding of input data.
More importantly, we want to integrate and effectively transfer this information when generalizing to new sensors.
Using sensor-specific tokens is a method for extracting sensor-specific information, but this approach cannot fully transfer information from all seen sensors when generalizing to new sensors~\citep{yang2024binding}.
% Previous work has demonstrated that using a set of learnable sensor tokens to capture sensor-specific information for each sensor helps the model better understand tactile data~\cite{yang2024binding}. 
% However, when aiming for the model to transfer to new sensor data without additional training, the presence of sensor-specific tokens can complicate the adaptation process. When inputting data from a new sensor, \cite{yang2024binding} retrieves the seen sensor that is closest to the current new sensor by calculating the sensor prototypes, and uses the sensor token of that seen sensor. However, this approach does not fully leverage the advantages of integrating data from multiple sensors, as sensor generalization does not solely rely on the single closest sensor.

Therefore, we propose using universal sensor tokens to integrate and store information related to various sensors, thereby maximizing the utilization of multi-sensor data when generalizing to new sensors. 
During training, we randomly replace the sensor-specific tokens with the universal sensor tokens, expecting them to aid in understanding input data from various sensors. 
Specifically, we introduce a set of learnable sensor tokens $ \{ s_k \}_{k=1}^{K} \cup s_u $, where $K$ is the number of sensor types, $s_k \in \mathbb{R}^{L \times d}$ are the sensor-specific tokens for the $k$-th sensor, $s_u \in \mathbb{R}^{L \times d}$ are universal sensor tokens and $L$ is the number of sensor tokens for each sensor. 
When inputting the tactile token sequence $z$ from the $k$-th sensor into the encoder $\Phi_{enc}$ to obtain its representation $x_T$, we randomly select one from $s_k$ and $s_u$ to concatenate with $z$, as follows:
\begin{equation}
\begin{gathered}
\ s = i \cdot s_u + (1-i) \cdot s_k, \ i\sim B(p_u), \\
x_T = \Phi_{enc}(z, s),
\end{gathered}
\end{equation}
where $p_u$ is the probability of using universal sensor tokens $s_u$. During inference, we consistently use universal sensor tokens for data from new sensors. 
% We transfer all available sensor information through these universal sensor tokens to aid in understanding new sensors.
\vspace{-5pt}
\subsection{Training Paradigm}
\vspace{-5pt}
Our framework has a multi-level structure, with the training of two stages conducted sequentially. In the first stage, we simultaneously perform the reconstruction of static tactile images and dynamic tactile videos, as well as the unique next frame prediction task for tactile videos. The loss for the first stage $\mathcal{L}_{stage1}$ is as follows:
\begin{equation}
\begin{gathered}
\mathcal{L}_{stage1} = \mathcal{L}_{rec}^S + \mathcal{L}_{rec}^D + \mathcal{L}_{pred}^D.
\end{gathered}
\end{equation}

In the second stage, we continue to use both tactile images and videos, and simultaneously perform multi-modal aligning and cross-sensor matching tasks. Hence, the loss function for the second stage is the sum of the losses from these two tasks:
\begin{equation}
\begin{gathered}
\mathcal{L}_{stage2} = \mathcal{L}_{align} + \lambda\mathcal{L}_{match},
\end{gathered}
\end{equation}
where $\lambda$ is a hyper-parameter controlling the weight of cross-sensor matching task.
% From both static and dynamic perspectives, we employ this multi-level framework to comprehensively learn unified multi-sensor representations for tasks requiring fine-grained perception and semantic understanding.

\vspace{-6pt}
\section{Experiments}
\vspace{-6pt}
In this section, we explore the answers to the following questions through quantitative and qualitative experiments: (\textbf{Q1}) How much benefit does the data of each sensor provide when it is integrated? (\textbf{Q2}) What does the unified multi-sensor representation space constructed by AnyTouch look like? (\textbf{Q3}) Is the unified multi-sensor representation more advantageous in various static and dynamic perception tasks?
We analyze \textbf{Q1} in Section~\ref{sec:sensor transf}, expolore \textbf{Q2} in Section~\ref{sec:representation}, and answer \textbf{Q3} through comparisons with existing methods in Sections~\ref{sec:seen}, \ref{sec:unseen} and \ref{sec:dyn}.
% In this section, we provide a comprehensive evaluation of static and dynamic perception capabilities of our framework. We test the understanding of tactile properties and cross-sensor generalization on multiple downstream datasets. We also analyze the transferability between sensors. Finally, we validate the dynamic perception capability of our framework in a real-world task: fine-grained pouring.

\vspace{-5pt}
\subsection{Dataset and Baselines}
\vspace{-5pt}
We use 9 different tactile datasets for training, including: {Touch and Go (TAG)}~\citep{yang2022touch}, VisGel~\citep{li2019connecting}, Cloth~\citep{yuan2018cloth}, {ObjectFolder Real (OF Real)}~\citep{gao2023objectfolderreal} , TVL~\citep{fu2024tvl}, YCB-Slide~\citep{suresh2023ycb} and SSVTP~\citep{kerr2022ssvtp}, Octopi~\citep{yu2024octopi} and the coarse-grained subset of our TacQuad. 
{We leverage the continual frames available in these datasets for dynamic perception.}
% We remove the text modality of the training samples included in the test set of downstream tasks for fairness. 
We also select four datasets: TAG, Feel~\citep{calandra2017feeling}, {ObjectFolder 1.0}~\citep{gao2022objectfolder1}, {ObjectFolder 2.0}~\citep{gao2022objectfolder2} as the downstream datasets. We compare AnyTouch with several single-sensor models:
VIT-LENS-2~\citep{lei2024vitlen}, TLV-Link~\citep{cheng2024touch100k}, and Omnibind~\citep{lyu2024omnibind}. We also compare our method with the multi-sensor model UniTouch~\citep{yang2024binding}. 
% While we aim to train UniTouch and TLV-Link on all available multi-sensor data, UniTouch requires touch-vision paired data and TLV-Link requires tri-modal paired data. 
We use the largest subset of all data that meets the requirements of UniTouch and TLV-Link to train them, remarked as UniTouch\dag \ and TLV-Link\dag. For the real-world dynamic perception task, we compare our method with a multi-sensor model T3~\citep{zhao2024transferable}, which is trained on 3M data, more than AnyTouch. The detailed dataset and baseline introduction are provided in Appendix~\ref{sec:stat}, \ref{sec:down} and \ref{sec:baseline}.
% Since T3 uses the downstream labels during training, we limit its comparison to the dynamic perception task.

\begin{table}[t]
\centering
\small
\caption{The impact of adding data from multiple sensors on the seen dataset (TAG), the unseen dataset from seen sensors (Feel), and the unseen dataset from unseen sensors (OF 1.0 \ and OF 2.0).} 
\renewcommand{\arraystretch}{1.05}
\tabcolsep=0.08cm
\begin{tabular}{cccccc}
\toprule
 \multirow{2}{*}{\textbf{Tactile Training Data}}& \textbf{Data} & \textbf{\TAG} & \textbf{\Feel} & \textcolor[RGB]{204,204,0}{\textbf{OF 1.0}} & \textcolor[RGB]{149,55,53}{\textbf{OF 2.0}} \\
&\textbf{Volume}&Material & Grasp  &Material &Material \\

\midrule
No Tactile Pre-training (CLIP) & / & 52.96 & 72.37 & 41.00 & 73.16 \\
\hdashline \vspace{-0.25cm} \\
\TAG, \visgel, \Cloth & 996k & 
\textbf{83.55} (\textcolor{red}{$\uparrow$30.59}) & 79.12 (\textcolor{red}{$\uparrow$6.75}) & 46.12 (\textcolor{red}{$\uparrow$5.12}) & 75.10 (\textcolor{red}{$\uparrow$1.94}) \vspace{0.05cm}\\

 \TAG, \visgel, \Cloth, \OBJReal & 2161k & 79.67 (\textcolor{ForestGreen}{$\downarrow$3.88}) & 79.28 (\textcolor{red}{$\uparrow$0.16}) &
 47.55 (\textcolor{red}{$\uparrow$1.43}) &
 75.53 (\textcolor{red}{$\uparrow$0.43}) \vspace{0.05cm}\\

 \TAG, \visgel, \Cloth, \OBJReal, & \multirow{2}{*}{2388k} &  \multirow{2}{*}{79.61 (\textcolor{ForestGreen}{$\downarrow$0.06})} &  \multirow{2}{*}{79.10 (\textcolor{ForestGreen}{$\downarrow$0.18})} 
 &  \multirow{2}{*}{48.00 (\textcolor{red}{$\uparrow$0.45})} 
 &  \multirow{2}{*}{75.57 (\textcolor{red}{$\uparrow$0.04})}\\
\TVL, \SSVTP, \YCB  \vspace{0.05cm}\\

\TAG, \visgel, \Cloth, \OBJReal, &\multirow{2}{*}{2427k} 
&  \multirow{2}{*}{79.70 (\textcolor{red}{$\uparrow$0.09})}
&  \multirow{2}{*}{\textbf{79.40} (\textcolor{red}{$\uparrow$0.30})}
&  \multirow{2}{*}{\textbf{48.75} (\textcolor{red}{$\uparrow$0.75})}
&  \multirow{2}{*}{\textbf{75.66} (\textcolor{red}{$\uparrow$0.09})}\\
\TVL, \SSVTP, \YCB , \Octopi \\

\bottomrule
 
\end{tabular}
\vspace{-8pt}   
\label{tab:transfer}
\end{table}

\begin{figure*}[t]
  \centering
    \includegraphics[width=11.0cm]{figures/tsne.pdf}
    \vspace{-8pt} 
  \caption{\textbf{The impact of components in AnyTouch on the multi-sensor representation space.} We use t-SNE to visualize the representations on the unused fine-grained subset of TacQuad, starting with CLIP and sequentially incorporating the modules. Each color represents a single touch, and samples from three sensors that touch the same position are connected by dashed lines.}                          
  \label{fig:tsne}
  \vspace{-12pt}
\end{figure*}

\begin{table}[t]
\centering
\small
\caption{Evaluation of static perception capabilities on the seen sensor (GelSight). 
% \dag Since TLV-Link requires three-modal paired data, we extract the largest subset containing three-modal data from all datasets to train TLV-Link with multi-sensor data.
{*Note that Feel is seen for the corresponding UniTouch and AnyTouch models.} \ddag Note that original TLV-Link uses frames after grasping which are much easier in Feel, whereas other models use frames during grasping. AnyTouch achieves a result of 99.0 using frames after grasping.} 
\label{tab:seen}
\renewcommand{\arraystretch}{1.05}
%\tabcolsep=0.34cm
\vspace{-2pt}
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Tactile Training Data}} & \multicolumn{3}{c}{\textcolor[RGB]{61,127,207}{\textbf{Touch and Go}}} & \textcolor[RGB]{61,127,207}{\textbf{Feel}} \\
&&Material &Roughness &Hardness &Grasp \\

\midrule
CLIP & / & 52.96 & 84.09 & 88.34 & 72.37  \\
\hdashline \vspace{-0.3cm} \\
VIT-LENS-2 & TAG & 63.0 & 85.1 & 92.0 & - \\
TLV-Link & Touch100k & 67.2 & 84.7 & 91.3 & 94.5\ddag \\
OmniBind & TAG & 67.45 & - &- &- \\
\midrule
UniTouch & TAG, Feel*, YCB, OF 2.0 & 61.3 & -&-&82.3\vspace{0.05cm}\\
TLV-Link\dag & TAG, TVL, SSVTP, OF Real, TacQuad & 74.12 & 85.94 & 94.18 & 76.97\\
\midrule
\textbf{AnyTouch} & TAG, Feel*, YCB, OF 2.0 & \textbf{82.74} & 86.01 & 94.24 & \textbf{87.17}\vspace{0.05cm}\\
\multirow{2}{*}{\textbf{AnyTouch}} & TAG, VisGel, Cloth, TVL, SSVTP, & \multirow{2}{*}{80.82} & \multirow{2}{*}{\textbf{86.74}}& \multirow{2}{*}{\textbf{94.68}}& \multirow{2}{*}{80.53}\\ 
& YCB-Slide, OF Real , Octopi, TacQuad \\

\bottomrule
 
\end{tabular}
\vspace{-10pt}   
\end{table}

\vspace{-5pt}
\subsection{Sensor transferability analysis (\textbf{Q1})}
\vspace{-5pt}
\label{sec:sensor transf}
% While the results in Table~\ref{tab:seen} and \ref{tab:unseen} suggest that training the model with data from multiple sensors yields benefits, we aim to further explore the specific advantages offered by the data from each individual sensor. 
Since we have integrated data from multiple sensors into unified representations, 
we aim to investigate the contributes of the knowledge transferred from each sensor's data to downstream tasks.
% we aim to investigate how much each sensor contributes to downstream tasks on different sensors.
Therefore, we incorporate data from \GelSight, \GelSlim~\citep{donlon2018gelslim}, \DIGIT \ and \Mini \ into the training of AnyTouch to obtain four different models, and compare them across four downstream tasks. As shown in Table~\ref{tab:transfer}, training with only GelSight data significantly improves performance on downstream datasets for all sensors, compared to the CLIP model which has not encountered any tactile data.
This indicates that tactile representation pre-training is crucial and transferable across new sensors. After sequentially integrating data from GelSlim, DIGIT, and GelSight Mini into the training, we observe performance improvements across the three unseen datasets, with greater enhancements for unseen sensors than seen sensors. This suggests that the knowledge from the data of GelSlim, DIGIT, and GelSight Mini can transfer to the GelSight and other sensors.

However, we also observe two interesting phenomena: (1) In the material classification task of the seen dataset TAG, the model trained solely on GelSight data performs the best, while incorporating data from more sensors leads to a performance drop. 
% This may be due to the model encountering more material categories not present in the single dataset as the data scale expands, causing it to lose focus on the material categories in TAG and reducing linear-probing performance. 
{This is because TAG is included in the pre-training data and integrating more data reduces the proportion of TAG data in pre-training. This aligns with the CLIP paper's finding that greater overlap between the downstream task dataset and the pre-training dataset may improve performance.}
(2) Although the integrated data volume from DIGIT is larger, the benefits are less compared to incorporating data from GelSight Mini. This may suggest that the images from DIGIT differ more from the images of GelSight and other sensors than the images from GelSight Mini do {due to the hardware difference.}

\vspace{-6pt}
\subsection{Multi-sensor Representation space (\textbf{Q2})}
\vspace{-4pt}
\label{sec:representation}
To verify whether AnyTouch clusters the representations with the same tactile information from different sensors together as expected, we use t-SNE~\citep{van2008visualizing} to visualize the tactile representations. We extract one aligned contact frame from each sensor for the 30 touches in the unused fine-grained subset of TacQuad. 
We input these samples into the CLIP model and the AnyTouch model which gradually incorporates masked modeling, multi-modal aligning, and cross-sensor matching, and visualize their representations in Figure~\ref{fig:tsne}.
Due to the lack of exposure to tactile images, CLIP struggles to distinguish the same tactile information from different sensors, instead clustering samples by sensor. After introducing masked modeling, the representations become more centralized within each sensor, as this method focuses on pixel-level tactile features, which are sensor-dependent.
However, this is not ideal for cross-sensor generalization, as we want multi-sensor tactile representations to cluster based on the object's tactile information they represent, minimizing sensor gaps. After incorporating multi-modal aligning, the multi-sensor tactile representations begin to blend and cluster by the objects they represent. This indicates that cross-sensor generalization is beginning to emerge, but there is still a distinct tendency for sensor-specific clustering. With our cross-sensor matching task, the representations from different sensors fully mix in a shared multi-sensor space, clearly clustering by the object's tactile information. This indicates that our model possesses the ability to extract sensor-agnostic features, enabling generalization to unseen sensors.


\vspace{-6pt}
\subsection{Static Perception on Seen Sensors (\textbf{Q3})}
\vspace{-4pt}
\label{sec:seen}
To validate the benefit of unified multi-sensor representations in transferring knowledge from multiple sensor data to seen sensors, we compared it to baselines on the seen dataset TAG and the unseen dataset Feel from the GelSight sensor. Since UniTouch uses different training data, we also train an AnyTouch model with the same data as UniTouch to ensure a fair comparison. As shown in Table~\ref{tab:seen}, the TLV-Link\dag \ trained with multi-sensor data outperforms all single-sensor models and the original TLV-Link in all three tasks of TAG. The AnyTouch trained with the same data as UniTouch, outperforms UniTouch in all four tasks. 
With the integration of dynamic perception and more multi-sensor data, AnyTouch trained on all data achieved the best results in hardness and roughness classification in TAG and comparable results to UniTouch in Feel, despite UniTouch having seen this data. These demonstrate the static perception capabilities of our framework on seen sensors. 
Notably, the original TLV-Link uses frames after grasping, while other models use frames during grasping. AnyTouch achieves a result of 99.0 using frames after grasping. 
It is worth mentioning that the AnyTouch trained with less data outperforms the one trained with all data in TAG material classification, similar to Table~\ref{tab:transfer}, while exposure to more multi-sensor data enhances performance in hardness and roughness classification.
% We believe this is due to the limited item types in each dataset and a smaller overlap in materials compared to hardness and roughness. Understanding hardness and roughness benefits more from the broader context.
{This is because these binary hardness and roughness classification tasks are much simpler, and the tactile text descriptions in other datasets also include these two binary attributes, which have less impact on the data distribution.}

\vspace{-8pt}
\subsection{Static Perception on Unseen Sensors (\textbf{Q3})}
\vspace{-5pt}
\label{sec:unseen}
\begin{table}[t]
\centering
\small
\caption{Evaluation of static perception capabilities on unseen sensors (TACTO and Taxim) using linear probing.
% Since UniTouch requires touch-vision paired data, we extract the largest subset containing touch-vision data from all datasets to train UniTouch. 
*Note that OF 2.0 is seen for the corresponding UniTouch and AnyTouch models.} 
\renewcommand{\arraystretch}{1.05}
%\tabcolsep=0.34cm
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Tactile Training Data}} & \textcolor[RGB]{204,204,0}{\textbf{ObjectFolder 1.0}} & \textcolor[RGB]{149,55,53}{\textbf{ObjectFolder 2.0}} \\
&&Material  &Material \\

\midrule
CLIP & / & 41.00 & 73.16 \\
\hdashline \vspace{-0.3cm} \\
UniTouch & TAG, Feel, YCB-Slide, OF 2.0* & 41.3 & 85.4\vspace{0.05cm} \\
\multirow{2}{*}{UniTouch\dag} & TAG, VisGel,  TVL& \multirow{2}{*}{47.25}& \multirow{2}{*}{75.29}\\ 
& SSVTP, OF Real , TacQuad \\
\midrule
\textbf{AnyTouch} & TAG, Feel, YCB-Slide, OF 2.0* & 46.50 & \textbf{85.87}\vspace{0.05cm} \\
\multirow{2}{*}{\textbf{AnyTouch}} & TAG, VisGel, Cloth, TVL, SSVTP, & \multirow{2}{*}{\textbf{49.62}}&\multirow{2}{*}{76.02}\\ 
&YCB-Slide, OF Real , Octopi, TacQuad \\

\bottomrule
 
\end{tabular}
\vspace{-6pt}   
\label{tab:unseen}
\end{table}


\makeatletter
\newcommand\figcaption{\def\@captype{figure}\caption}
\newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother
\begin{figure}[t]
\begin{minipage}{0.5\textwidth}
\centering
\small
\tabcaption{Evaluation on the real-world pouring task using GelSight Mini. } 
\renewcommand{\arraystretch}{1.05}
%\tabcolsep=0.34cm
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \textbf{Dynamic} & \multicolumn{2}{c}{\textbf{Mean Error (g) $\downarrow$}} \\
&\textbf{Perception}&Fine-tune&Freeze\\

\midrule
CLIP & \ding{56}&5.22 & 49.1 \\
T3 & \ding{56}&2.33 & 9.74 \\
\textbf{AnyTouch} & \ding{56}& 2.45 & 9.60  \\
\textbf{AnyTouch} & \ding{52}& \textbf{1.56} & \textbf{8.22} \\

\bottomrule
\end{tabular}
% \vspace{-6pt}   
\label{tab:real}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/robot.png}
  \caption{Setup of real-world pouring task. } 
  \label{fig:robot}
\end{minipage}
\vspace{-10pt}
\end{figure}


To verify the generalization of our method on unseen sensors, we compare it with the multi-sensor models UniTouch and UniTouch\dag \ on two datasets from unseen sensors, OF 1.0 and OF 2.0. 
As shown in Table~\ref{tab:unseen}, the AnyTouch trained on the same data as Unitouch outperforms it on both datasets, demonstrating the static perception capability of our method across different sensors.
Both UniTouch and AnyTouch, perform better on the unseen OF 1.0 dataset, confirming that integrating multi-sensor data aids generalization to unseen sensors.
In addition, the AnyTouch trained on the full dataset achieves the highest performance on the unseen OF 1.0, demonstrating that learning sensor-agnostic semantic-level tactile features and constructing unified multi-sensor representation space is an effective approach for cross-sensor transfer.

\vspace{-8pt}
\subsection{Real World Dynamic Perception (\textbf{Q3})}
\vspace{-5pt}
\label{sec:dyn}
To test the dynamic perception capability of our method in real-world object manipulation tasks, we conduct experiments on a real-world task: fine-grained pouring, as shown in Figure~\ref{fig:robot}. The detailed task setup is located at \ref{sec:real-setup}.
% The robotic arm must rely entirely on tactile feedback to pour out 60g of small beads from a cylinder initially contains 100g of beads.
% The robot arm can select one of the three actions to perform based on the real-time tactile feedback: pouring, waiting or retracting.
% As both rotating the cylinder and pouring out the small beads lead to continuous variations in pressure on the sensors, the model must analyze the fine-grained changes between tactile images to determine the appropriate pouring speed and the right moment to retract the cylinder. 
% This task is typically performed using multi-modal data~\cite{li2022see}, making it particularly challenging for models that rely solely on tactile perception.
% The robot arm can select one of two actions to perform based on the tactile feedback from the previous action: either rotate the gripper or lift upwards. We define a successful outcome as the robot arm unscrewing the bottle cap and lifting it to a designated height. Both sensing the friction while twisting the bottle cap and detecting resistance when lifting upward are fine-grained dynamic perception process, making this task an ideal choice for validating dynamic perception capabilities.
We compare AnyTouch with a recent multi-sensor model T3 and use a static-only AnyTouch model trained solely on tactile images as a baseline. Neither model includes modules specifically designed for dynamic perception. 
{We conduct 10 real-world test runs and record the error between the poured mass and the target mass for each test. We then average the error across the test runs to get the "mean error" and use it as the metric, as shown in Table~\ref{tab:real}.}
% We record the mean error between the actual mass poured and the target mass across 10 tests for each model, as shown in Table~\ref{tab:real}. 
When the tactile encoder is frozen and only the policy network is fine-tuned, CLIP, which has not seen tactile images, struggles with the task, highlighting the challenges of fine-grained dynamic perception.
In contrast, the static-only AnyTouch performed comparably to T3 which was trained on more data. After integrating dynamic perception capabilities, AnyTouch achieved the best performance.
This demonstrates the importance of learning unified multi-sensor representations from both static and dynamic perspectives for completing various tasks including real-world tasks.


% We also observe an interesting phenomenon: when the second stage is removed (\textit{i.e.}, only employing masked modeling), there is a notable decline in performance on unseen sensors.
\vspace{-8pt}
\section{Conclusion}
\vspace{-7pt}
In this paper, we collect TacQuad, an aligned multi-modal multi-sensor tactile dataset that enables the explicit integration of various sensors.
Based on this, we present AnyTouch, a unified multi-sensor tactile representation learning framework from the perspectives of both static and dynamic perception. 
We also explore the multi-sensor representation space and sensor transferability.

\subsection*{Acknowledgments}
This project is sponsored by CCF-Zhipu.AI Large Model Innovation Fund and also supported by the Zhipu Company. We want to thank Xinjie Tang for providing design and drafting support.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Appendix}

\begin{table}[t]
\centering
\small
\caption{Training dataset statistics. The text modality in Touch and Go and ObjectFolder Real is generated by GPT-4o. *Note that Cloth and YCB-Slide contain vision modality, but we intentionally do not use it to demonstrate our method's compatibility with modality absence. We only count the number of contact frames used for training in each dataset.} 
\renewcommand{\arraystretch}{1.05}
\tabcolsep=0.24cm
\begin{tabular}{cccccc}
\toprule
\textbf{Dataset} & \textbf{Vision} & \textbf{Text} & \textbf{Video} & \textbf{Sensor} & \textbf{Size} \\
\midrule
\textcolor[RGB]{61,127,207}{Touch and Go}~\citep{yang2022touch} & \ding{52}& \ding{52}& \ding{52} & \GelSight & 250k \\
\visgel~\citep{li2019connecting} & \ding{52}& \ding{56}& \ding{52} & \GelSight & 587k \\
\Cloth~\citep{yuan2018cloth} & \ding{56}*& \ding{56}& \ding{52} & \GelSight & 587k \\
\TVL~\citep{fu2024tvl} & \ding{52}& \ding{52}& \ding{52} & \DIGIT & 39k \\
\SSVTP~\citep{kerr2022ssvtp} & \ding{52}& \ding{52}& \ding{56} & \DIGIT & 4.5k \\
\YCB~\citep{suresh2023ycb} & \ding{56}*& \ding{56}& \ding{52} & \DIGIT & 183k \\
\textcolor[RGB]{222,156,8}{ObjectFolder Real}~\citep{gao2023objectfolderreal} & \ding{52}& \ding{52}& \ding{52} & \GelSlim & 1165k \\
\Octopi~\citep{yu2024octopi} & \ding{56}& \ding{52}& \ding{52} & \Mini & 39k \vspace{0.05cm}\\
\multirow{2}{*}{\Multisensor} & \multirow{2}{*}{\ding{52}}& \multirow{2}{*}{\ding{52}}& \multirow{2}{*}{\ding{52}} & \GelSight, \DIGIT, \DuraGel & \multirow{2}{*}{55k} \\
&&&&\Mini\\

\bottomrule
 
\end{tabular}
\vspace{-6pt}   
\label{tab:dataset}
\end{table}

\subsection{Training dataset statistics}
\label{sec:stat}

In this section, we provide a detailed presentation of the sensor type, modality pairing and data scale of the datasets used during the training phase. 
We use a total of 9 datasets from 5 different sensors for training, including: \textcolor[RGB]{61,127,207}{Touch and Go (TAG)}~\citep{yang2022touch}, \visgel~\citep{li2019connecting} and \Cloth~\citep{yuan2018cloth} from \GelSight~\citep{yuan2017gelsight}; \textcolor[RGB]{222,156,8}{ObjectFolder Real (OF Real)}~\citep{gao2023objectfolderreal} from \GelSlim~\citep{donlon2018gelslim}; \TVL~\citep{fu2024tvl}, \YCB~\citep{suresh2023ycb} and \SSVTP~\citep{kerr2022ssvtp} from \DIGIT~\citep{lambeta2020digit}; \Octopi~\citep{yu2024octopi} from \Mini~\citep{gelsight}; and the coarse-grained subset of our \Multisensor \ from \DIGIT, \Mini \ and \DuraGel~\citep{zhang2024duragel}.
We filter the contact frames with tactile deformations by calculating the difference between each tactile image and the corresponding background frame in these datasets.
% These datasets contain a total of 2,481,703 tactile contact frames.
Eventually, we extract a total of 2,481,703 tactile contact frames from these datasets for model training.
{We also leverage the continual frames available in these datasets to train the model's dynamic perception capabilities.}
The detailed training dataset statistics are shown in Table~\ref{tab:dataset}. We generate text descriptions for Touch and Go and ObjectFolder Real using GPT-4o. We also expand the text descriptions in TVL, SSVTP and Octopi. \textit{We remove the text modality of the training samples included in the test set of downstream tasks for fairness.}
It is worth saying that Cloth and YCB-Slide contain vision modality originally, but we intentionally do not use it to demonstrate our method's compatibility with missing modalities. 

\subsection{Multi-modal Aligning loss}
\label{sec:loss}
When training AnyTouch, we maximize the use of paired data by selecting the largest subset for each modality combination within the batch for multi-modal aligning. Considering a pair of uni-modal representations $(x_T,x_V,x_L)$ derived from uni-modal encoders, where $x_T \in \mathbb{R}^{d}$ is the touch representation, $x_V \in \mathbb{R}^{d} \cup \varnothing$ is the vision representation and $x_L \in \mathbb{R}^{d} \cup \varnothing$ is the text representation. 
We then perform multi-modal alignment~\citep{radford2021clip} within the batch as:
\begin{equation}
\begin{aligned}
\mathcal{L}_{T \rightarrow V} &= -\frac{1}{|\Omega_V|} \sum_{i \in \Omega_V}\log \frac{\exp(x_{T,i}^\top \cdot x_{V,i} / \tau)}{\sum_{j \in \Omega_V} \exp(x_{T,i}^\top \cdot x_{V,j} / \tau)}, \\
\mathcal{L}_{T \rightarrow L} &= -\frac{1}{|\Omega_L|} \sum_{i \in \Omega_L}\log \frac{\exp(x_{T,i}^\top \cdot x_{L,i} / \tau)}{\sum_{j \in \Omega_L} \exp(x_{T,i}^\top \cdot x_{L,j} / \tau)}, \\
\mathcal{L}_{V \rightarrow L} &= -\frac{1}{|\Omega_V \cap \Omega_L|} \sum_{i \in \Omega_V \cap \Omega_L}\log \frac{\exp(x_{V,i}^\top \cdot x_{L,i} / \tau)}{\sum_{j \in \Omega_v \cap \Omega_L} \exp(x_{V,i}^\top \cdot x_{L,j} / \tau)},
\end{aligned}
\end{equation}
where $B$ is the batchsize, $\Omega_V, \Omega_L$ are sets of indices for the samples containing vision and text, and $\tau$ is the scalar temperature.

\subsection{Downstream Datasets}
\label{sec:down}
In this section, we provide a more detailed introduction to the downstream datasets. Specifically, we compare the static perception capabilities of AnyTouch and the baselines on four downstream datasets: \TAG, \Feel~\citep{calandra2017feeling}, \textcolor[RGB]{204,204,0}{ObjectFolder 1.0 (OF 1.0)}~\citep{gao2022objectfolder1} and \textcolor[RGB]{149,55,53}{OjectFolder 2.0 (OF 2.0)}~\citep{gao2022objectfolder2}. TAG includes three tactile properties understanding tasks: material, hardness, and roughness classification. Feel is a robotic dataset from GelSight containing a grasp success prediction task. We follow the data split in~\citep{yang2024binding,cheng2024touch100k} for Feel. ObjectFolder 1.0 and OjectFolder 2.0 are two simulated object datasets using \textcolor[RGB]{204,204,0}{TACTO}~\citep{wang2022tacto} and \textcolor[RGB]{149,55,53}{Taxim}~\citep{si2022taxim}. We use them as unseen datasets from unseen sensors, and follow the data split in~\citet{yang2024binding}.

\subsection{Baselines}
\label{sec:baseline}
In the static perception task, we compared AnyTouch with several recent single-sensor and multi-sensor baselines. 
VIT-LENS-2~\citep{lei2024vitlen}, TLV-Link~\citep{cheng2024touch100k}, and Omnibind~\citep{lyu2024omnibind} are three single-sensor models included, all of which conduct multi-modal alignment using data from GelSight. As for multi-sensor models, we compare our method with UniTouch~\citep{yang2024binding}, which currently demonstrates the SOTA cross-sensor performance. We also intend to train UniTouch and TLV-Link using all available multi-sensor data for comparison. However, since UniTouch requires touch-vision paired data and TLV-Link can only be trained on three-modal paired data, we extract the largest subset of data that meets the requirements to train them, remarked as UniTouch\dag \ and TLV-Link\dag. 

In the real-world dynamic perception task, we compare with a recent multi-sensor model T3~\citep{zhao2024transferable}. This model has been validated to have strong capabilities in completing manipulation tasks. 
It is important to note that the amount of training data used by this model is approximately 3M, which is more than our AnyTouch. 
Additionally, since this model utilizes labels from various downstream tasks during pre-training, comparing its static perception capabilities with other models would be unfair. Therefore, we only use it in the dynamic perception task.

\subsection{{details for fine-grained data collection}}
\label{sec:fine-grained}
{In this section, we provide a more detailed introduction of the data collection process for the fine-grained spatio-temporal aligned data. The calibration platform we built consists of three main parts: a platform, a movable end effector, and a 3D-printed container that holds the sensor. The four sensors are fixed side by side in the container. The movable end on our calibration platform can be programmed to move at a specified speed to a designated position within the coordinate system defined by the base. Therefore, as long as we pre-measure the relative positions of the centers of the four sensor surfaces within the container and compensate for the relative positions during each set of data collection, we can ensure that all four sensors make contact with the object from the same initial position and at the same speed, thereby achieving both temporal and spatial alignment.}

% \subsection{}
\subsection{Implementation Details}
We base our encoders on OpenCLIP-Large~\citep{cherti2023reproducible}. For the tactile decoder, we use a Vision Transformer (ViT)~\citep{dosovitskiy2020image} with 8 layers and a dimension of 512.
We use the AdamW~\citep{loshchilov2017decoupled} optimizer with a learning rate of 2e-4. After a warm-up period of 1 epoch, we implement linear learning rate decay. For each tactile video clip, we use $T=3$ frames. We train the first stage for 20 epochs and the second stage for 12 epochs on 4 NVIDIA A800 GPUs. We alternate between training with tactile images and video clips throughout the entire training process. We use a mask ratio $\rho = 0.75$. During the alignment, we use the text modality as the anchor, freezing the text encoder while performing LoRA fine-tuning on the vision encoder. We set the alignment strength $\alpha_{TV} = \alpha_{TL} = 1.0$ and $\alpha_{VL}=0.2$, and set the weight of cross-sensor matching $\lambda = 0.1$. Following~\citep{yang2024binding}, we use $L=5$ sensor tokens for each type of sensor. In both stages, we set the probability of using universal sensor tokens $p_u$ to increase linearly from 0 to 0.75.

\subsection{GPT-4o Annotation}
In this work, we generate paired text descriptions of tactile properties for Touch and Go, ObjectFolder Real and our TacQuad. 
We input paired visual images and predefined text prompts into GPT-4o to obtain text descriptions. We borrow the prompt from~\citep{cheng2024touch100k} and make appropriate adjustments. 
The prompt for Touch and Go and TacQuad is shown in Figure~\ref{fig:prompt1}.


Specifically, because the ObjectFolder Real dataset has two camera views and some touch locations or details may not be visible, we input two visual images and one tactile image simultaneously, with clear indications in the prompt, as shown in Figure~\ref{fig:prompt2}.

Since the TVL and SSVTP datasets only contain simple phrase-level tactile descriptions, we also use GPT-4o to extend the text modality in both datasets. 
We input both visual images and the existing text descriptions simultaneously, as shown in Figure~\ref{fig:prompt3}.

{After generating the annotations using GPT-4o, we conduct a simple and rough sampling check to ensure the correctness. We find that as long as the model can correctly recognize the object, the knowledge of object-related physical properties stored in the large model can effectively provide accurate tactile annotations. Errors in the generated annotations may occur when an object's category is difficult to determine or when occlusions are present. However, such instances are relatively uncommon.}


\begin{figure*}[t]
  \centering
    \includegraphics[width=12.5cm]{figures/prompt1.pdf}
  \caption{\textbf{Prompt and raw output for \TAG \ and \Multisensor.}}                          
  \label{fig:prompt1}
\end{figure*}

\begin{figure*}[t]
  \centering
    \includegraphics[width=12.5cm]{figures/prompt2.pdf}
  \caption{\textbf{Prompt and raw output for \OBJReal.} Given that \OBJReal \  includes two camera views and some touch locations or details may be obscured, we input two visual images along with one tactile image simultaneously. {
If the tactile image is not provided as input, there is a possibility of producing incorrect annotations (marked in red).}}                          
  \label{fig:prompt2}
\end{figure*}

\begin{figure*}[t]
  \centering
    \includegraphics[width=12.5cm]{figures/prompt3.pdf}
  \caption{\textbf{Prompt and raw output for \TVL \ and \SSVTP.} Given that these datasets only contain phrase-level tactile descriptions, we input the visual image and the phrase-level descriptions to generate more detailed tactile descriptions.}                          
  \label{fig:prompt3}
\end{figure*}

\subsection{Real-World pouring task}
\label{sec:real-setup}
To test the dynamic perception capability of our method in real-world object manipulation tasks, we conduct experiments on a real-world task: fine-grained pouring, as shown in Figure~\ref{fig:robot}. In the experiments, we use a 6-DoF UFACTORY xArm 6 robotic arm equipped with a Robotiq 2F-140 gripper. Cartesian space displacement commands are generated at a policy frequency of 5 Hz. 
The robot arm must rely entirely on tactile feedback to pour out 60g of small beads from a cylinder that initially contains 100g of beads.
The robot arm can select one of the three actions to perform based on the real-time tactile feedback: pouring, waiting, or retracting. The action step size is $\delta \phi=0.25\degree$. We train the model through imitation learning and collect the training data using a keyboard.
As both rotating the cylinder and pouring out the small beads lead to continuous variations in pressure on the sensors, the model must analyze the fine-grained changes between tactile images to determine the appropriate pouring speed and the right moment to retract the cylinder. 
This task is typically performed using multi-modal data~\citep{li2022see}, making it particularly challenging for models that rely solely on tactile perception.

\subsection{Ablation study}

\begin{table}[t]
\centering
\small
\caption{The impact of modalities and modules in AnyTouch on static perception capabilities.} 
\renewcommand{\arraystretch}{1.1}
\tabcolsep=0.08cm
\begin{tabular}{ccccc}
\toprule
 \multirow{2}{*}{\textbf{Model}} & \textbf{\TAG} & \textbf{\Feel} & \textcolor[RGB]{204,204,0}{\textbf{OF 1.0}} & \textcolor[RGB]{149,55,53}{\textbf{OF 2.0}} \\
&Material & Grasp  &Material &Material \\

\midrule
\textbf{AnyTouch}  & \textbf{80.82} & \textbf{80.53} & \textbf{49.62} & \textbf{76.02} \\
\hdashline \vspace{-0.35cm} \\
w/o Text Modality &
75.91(\textcolor{ForestGreen}{$\downarrow$4.91}) &  78.93(\textcolor{ForestGreen}{$\downarrow$1.60}) & 
48.87(\textcolor{ForestGreen}{$\downarrow$0.75}) &
75.52(\textcolor{ForestGreen}{$\downarrow$0.50})\\
w/o Vision Modality & 74.55(\textcolor{ForestGreen}{$\downarrow$6.27}) &  77.30(\textcolor{ForestGreen}{$\downarrow$3.23}) & 
48.12(\textcolor{ForestGreen}{$\downarrow$1.50}) &
75.22(\textcolor{ForestGreen}{$\downarrow$0.80})\\
{w/o Text in TacQuad} & 80.70(\textcolor{ForestGreen}{$\downarrow$0.12}) &  80.19(\textcolor{ForestGreen}{$\downarrow$0.34}) & 
49.21(\textcolor{ForestGreen}{$\downarrow$0.41}) &
75.91(\textcolor{ForestGreen}{$\downarrow$0.11})\\
w/o Stage 1 & 78.34(\textcolor{ForestGreen}{$\downarrow$2.48}) &  78.62(\textcolor{ForestGreen}{$\downarrow$1.91}) & 
48.75(\textcolor{ForestGreen}{$\downarrow$0.87}) &
76.08(\textcolor{red}{$\uparrow$0.06})  \\
w/o Stage 2 & 68.64(\textcolor{ForestGreen}{$\downarrow$12.18}) & 72.39(\textcolor{ForestGreen}{$\downarrow$8.14}) & 46.50(\textcolor{ForestGreen}{$\downarrow$3.12}) & 73.09(\textcolor{ForestGreen}{$\downarrow$2.93}) \\
w/o Cross-Sensor Matching & 80.54(\textcolor{ForestGreen}{$\downarrow$0.28})
& 79.43(\textcolor{ForestGreen}{$\downarrow$1.10})
& 49.25(\textcolor{ForestGreen}{$\downarrow$0.37})
& 75.80(\textcolor{ForestGreen}{$\downarrow$0.22})\\
w/o Dynamic Perception & 77.93(\textcolor{ForestGreen}{$\downarrow$2.89}) & 79.28(\textcolor{ForestGreen}{$\downarrow$1.25}) & 48.62(\textcolor{ForestGreen}{$\downarrow$1.00}) & 75.70(\textcolor{ForestGreen}{$\downarrow$0.32}) \\
w/o Universal Sensor Tokens & 80.79(\textcolor{ForestGreen}{$\downarrow$0.03}) & 79.03(\textcolor{ForestGreen}{$\downarrow$1.53})& 48.40(\textcolor{ForestGreen}{$\downarrow$1.22})& 75.40(\textcolor{ForestGreen}{$\downarrow$0.62})
\\


\bottomrule
 
\end{tabular}
\vspace{-6pt}   
\label{tab:ablation}
\end{table}

To investigate the impact of each module in AnyTouch, as well as the individual contributions of the vision and text modalities in multi-modal alignment, we conduct ablation studies on the four downstream datasets. The experimental results are shown in Table~\ref{tab:ablation}.
We observe performance decline when the paired vision and text modalities are excluded, highlighting the importance of aligning with these paired modalities to narrow the sensor gaps and achieve a comprehensive tactile perception capability. We also find that the performance decline caused by removing the visual modality is greater than removing text. However, this does not necessarily indicate that the visual modality is more important, as removing the visual modality results in a more significant reduction in data during the aligning. {We also remove the text from the TacQuad dataset we proposed to validate the effectiveness of the text in the dataset. Although the TacQuad data is relatively small compared to the total dataset, making it unlikely to significantly impact performance when modified, we observe a consistent decline in the model's performance on downstream tasks after removing the text modality. This demonstrates the important role of the text modality in our dataset as a bridge that helps reduce the gap between sensors.}

When we remove cross-sensor matching and universal sensor tokens from AnyTouch, we observe a performance decline primarily on the datasets from unseen sensors. 
It is important to note that the sensors in these datasets are not included in the positive sample pairs for cross-sensor matching, indicating that this task has even greater potential. This demonstrates that both strategies can enhance the model's generalization to unseen sensors.  After removing the entire stage 2, we observe a significant performance decline.
On \OBJtwo, it performs even worse than CLIP, which has never encountered tactile data. This result is consistent with the improvement on \OBJtwo \ when removing stage 1 and the analysis in Section~\ref{sec:representation},
indicating that learning semantic-level features is crucial for achieving comprehensive tactile perception and cross-sensor generalization.  
Nevertheless, learning pixel-level features in stage 1 is still meaningful for the seen sensors.
In addition, we also observe a consistent decline in performance after removing the joint training for dynamic perception, indicating that integrating dynamic perception can indeed enhance static perception capabilities.

\subsection{{Cross-sensor generation}}
{
To more comprehensively demonstrate the value and impact of the dataset we proposed, we conduct cross-sensor generation experiments on the fine-grained spatio-temporal aligned data. Specifically, we trained models to generate aligned DuraGel images from GelSight Mini images, and to reconstruct the 20x20 force fields captured by the Tac3D sensor from DIGIT and GelSight Mini data. We compared the performance of our model with the T3 model, which used more training data than ours (3.08M compared to our 2.48M) for pretraining. Specifically, for generating Duragel images, we constructed a GAN network based on ViT, using T3 or AnyTouch as the encoders for the discriminator and generator, similar to ViTGAN~\citep{leevitgan}. A ViT-based decoder is then used to generate images across sensors. For the force field generation of Tac3D, due to its low resolution, we treat it as a regression task and use an MLP to reconstruct the force field based on the features extracted by the encoder. Both networks can effectively evaluate the quality of the encoder's tactile representations. To further ensure fairness, we also removed the overlapping portions of the coarse-grained aligned data from the training data that overlapped with this dataset. Note that Tac3D is an unseen sensor for both of the models. We use mean square error (MSE) () between the generated data and the ground truth as the metric. The results shown in Table~\ref{tab:generation} indicate that our method outperforms T3 in terms of generation quality, both for cross-sensor generation of vision-tactile images and for force fields captured by the unseen Tac3D. This demonstrates the effectiveness of our method and the value of the dataset, and supports our motivation to obtain a unified tactile multi-sensor representation that is applicable to a variety of tasks and sensors.
}

\begin{table}[t]
\centering
\small
\caption{{Performance comparison with T3 on the cross-sensor generation task using the fine-grained spatio-temporal aligned data.}} 
\renewcommand{\arraystretch}{1.1}
%\tabcolsep=0.08cm
\begin{tabular}{ccccc}
\toprule
 \multirow{3}{*}{\textbf{Model}} & \multirow{3}{*}{\textbf{Training Data}} & \multicolumn{3}{c}{\textbf{Mean Square Error ($\downarrow$)}} \\
& & GelSight Mini  &GelSight Mini  &DIGIT  \\
&&$\rightarrow$ DuraGel &$\rightarrow$ Tac3D &$\rightarrow$ Tac3D \\
\midrule
T3 &
3.08M &  0.2261 & 
0.0167 & 0.0155\\
\textbf{AnyTouch}  & 2.48M & \textbf{0.2159}& \textbf{0.0151}& \textbf{0.0144} \\

\bottomrule
 
\end{tabular}
\vspace{-6pt}   
\label{tab:generation}
\end{table}



\subsection{{Discussion on frame number}}
{In the real world, the complete process of touching an object can take several seconds or even tens of seconds. Ensuring the model can comprehend an entire tactile video presents a significant challenge. Current large-scale video understanding models, such as Video-LLaMA~\citep{zhang2023video}, often process tens or even hundreds of frames as input, encoding them into tokens. However, this comes at the cost of generating very long token sequences, which significantly increase computational overhead and inference time. The tactile modality is frequently used in fine-grained manipulation tasks that demand high real-time performance, which imposes strict requirements on the model's inference speed. As a result, models that rely on long frame sequences are challenging to apply in real-time dynamic perception tasks. Moreover, since touch actions are typically performed at high speeds, even a sequence of three continual frames (equivalent to 0.1 seconds for a DIGIT sensor with a frequency of approximately 30Hz) can exhibit noticeable changes. We anticipated these challenges and, as a result, chose to use a sequence of three continual frames as the input format for tactile videos. This approach also enables the understanding of longer videos by selecting multiple 3-frame segments and either concatenating or summing their features, similar to ImageBind~\citep{girdhar2023imagebind}. Using more frames may lead to better perception performance, but this is essentially a trade-off between performance and both computational cost and reference speed.}

\subsection{{Discussion on other tactile sensors}}
{
Tactile perception is not limited to images. Some tactile properties, such as temperature and torque, are difficult to obtain from tactile images alone, requiring the use of other types of tactile sensors. This issue presents challenges from both hardware and algorithmic perspectives.}

{
From a hardware perspective, an ideal tactile sensor should be capable of gathering various types of tactile information, effectively integrating multiple existing tactile sensors into a single unit. This may be very challenging, and a more practical solution might involve equipping different fingers of a robotic hand with different types of sensors. This would allow for the simultaneous collection of diverse tactile data, maximizing the range of information captured.}

{
From an algorithmic perspective, when vision-based tactile sensors are replaced with other types of tactile sensors (e.g., tactile sensor arrays), the multi-sensor data alignment method proposed in this paper can still be applied.  Aligned data can then be used to perform alignment or to distill knowledge from the visuo-tactile model to models for other types of tactile sensors. For lower-resolution tactile sensors, the aligned data can facilitate tactile super-resolution learning, enabling knowledge transfer from vision-based tactile sensor models to enhance their performance.}

{
If both vision-based tactile sensors and other tactile sensors (\textit{e.g.}, those capturing temperature or other non-visual properties) are used simultaneously, a possible approach is to fuse their outputs into a unified, comprehensive tactile feature. This enriched representation can then be aligned with other modalities in a unified manner.}


\subsection{{limitations and future work}}
{In this section, we discuss some potential limitations of our work and propose corresponding solutions for future work:}

\begin{itemize}
    \item {\textbf{Compared to all the training data, the scale of the TacQuad dataset we have currently collected is still somewhat limited.} Capturing the immense variety of object types within a single dataset is challenging in a limited amount of time. Fortunately, the coarse-grained spatial alignment data collection method we propose has the potential to scale up, as data collection can be performed manually without the need for precise alignment. Fine-grained data collection can also be expanded by replicating the calibration platform and increasing manpower. We plan to grow our team to scale up the dataset and enhance object diversity in future work.}

    \item {\textbf{The types of sensors considered are relatively limited.} We have made every effort to collect all available vision-based tactile sensors around us, yet we were only able to include four different types. Moreover, we did not explore the differences between individual sensors of the same type or address issues such as gel damage. Moving forward, we aim to expand our dataset and increase the diversity of sensors through collaborative data collection across multiple laboratories.}

    \item {\textbf{The scope of tasks for dynamic tactile perception is currently limited.} In this work, we validated the dynamic perception capabilities of our model on a single real-world manipulation task: pouring. We hope to explore more challenging and interesting dynamic perception tasks in future work. Additionally, beyond real-world manipulation tasks, studying tactile video understandingparticularly fine-grained dynamic tactile understanding that includes direction and action descriptionsis also an interesting direction to explore.}
\end{itemize}


\end{document}
