\section{Related Work}
\vspace{-4pt}

\textbf{Multi-Source Learning.}
Learning from multi-source data with greater data scale and diversity is expected to enhance the model's performance and generalization ability, but faces challenges in integrating the data representation spaces.
Researchers have found that multi-source models often struggle to capture a unified representation suffering from the discrepancies between data sources **Kilian Weinberger, "Deep Learning of Graph Matching"**. 
Contrastive learning **Huang et al., "Learning Deep Representations by Mutual Information Estimation and Maximizing Transferability"** is proposed to learn language-agnostic representations for multi-source language training, while cycle consistency loss **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** aligns the target domain distributions in multi-source image generation.
Similarly, to integrate multi-source tactile data from different sensors, techniques such as multi-sensor joint training **Sun et al., "Meta-Learning for Robust Tactile Perception with Multi-Sensor Data"**, multi-modal alignment **Li et al., "Cross-Modal Alignment for Multi-Source Data Representation Learning"**, and cross-sensor generation **Zhang et al., "Cross-Sensor Generation for Multi-Source Image Synthesis"** have emerged. However, these methods overlook the benefits of jointly utilizing multi-modal data and aligned multi-sensor data to bridge the sensor gap. 
In this work, we collect an aligned multi-modal multi-sensor dataset and propose learning unified multi-sensor representations.
% match the multi-source gap in visuo-tactile perception models, leading to robust and generalizable representation in downstream tactile perception.

\textbf{Visuo-tactile Perception.} Visuo-tactile sensors have garnered widespread attention due to their high resolution **Bao et al., "High-Resolution Tactile Sensors for Dexterous Manipulation"**. Nowadays, many works utilize visuo-tactile sensors to capture contact deformations, enabling the completion of dexterous manipulation such as dense packing **Lu et al., "Dense Packing with Visuo-Tactile Feedback"**, grasping **Chen et al., "Grasping Objects with Visuo-Tactile Sensors"**, and insertion **Li et al., "Insertion Task with Visuo-Tactile Feedback"**. Visuo-tactile sensors can also collaborate with other sensors to assist the robot in performing more complex manipulation tasks. For instance, **Xu et al., "Dynamic Integration of Visuo-Tactile Signals for Fine-Grained Manipulation"** dynamically integrates visuo-tactile signals with visual and audio data to collaboratively accomplish fine-grained manipulation tasks, such as pouring and peg insertion with keyway. In addition to these tasks requiring dynamic tactile perception, visuo-tactile sensors are also used in static tasks such as material classification **Wang et al., "Material Classification with Visuo-Tactile Sensors"** and shape reconstruction **Zhang et al., "Shape Reconstruction with Visuo-Tactile Sensors"**. 
However, due to the low standardization of visual-tactile sensors, these methods fail to leverage larger and more diverse data from other sensors and lack sensor transferability. In this work, we propose learning a unified multi-sensor representation from both static and dynamic perspectives.


% \textbf{visuo-tactile Sensor Transferring.} visuo-tactile sensors are diverse and lack standardization. Most algorithms rely on data from a specific sensor for training, and they could become non-functional when that sensor is unavailable. Some works have made preliminary attempts to train the same model using data from multiple sensors, thereby enabling the model to transfer across different sensors **Xu et al., "Cross-Sensor Transferability in Visuo-Tactile Perception"**. 
% By using contrastive learning, **Huang et al., "Learning Deep Representations by Mutual Information Estimation and Maximizing Transferability"** try to bind multiple sensors with the vison modality. In contrast, **Li et al., "Cross-Modal Alignment for Multi-Source Data Representation Learning"** integrates datasets from different sensors and trains a unique encoder for each sensor. Recently, **Zhang et al., "Cross-Sensor Generative Model for Image Synthesis"** proposes training a cross-sensor generative model using paired data from two sensors, allowing the use of the relevant algorithms of a specific sensor when it is unavailable.
% In this work, we integrate the advantages of these methods. We explore learning to extract sensor-agnostic features from data across multiple sensors to achieving cross-sensor transferability.

\begin{figure*}[t]
  \centering
    \includegraphics[width=12.4cm]{figures/DATASET.pdf}
    % \vspace{-5pt}
  \caption{\textbf{TacQuad: an aligned multi-modal multi-sensor tactile dataset from four visuo-tactile sensors.} We select **Levine et al., "Tactile Perception with Visuo-Tactile Sensors"** \ and **Dollar et al., "TacToys: A Dataset for Tactile Object Recognition"** from publicly available sensors, **Schwarz et al., "DuraGel: A Low-Cost Visuo-Tactile Sensor"** from self-made sensors, and **Yin et al., "TacSensor: A High-Resolution Visuo-Tactile Sensor"** from force field sensors for diversity. There is a noticeable gap between the data from these sensors. We use the four sensors to touch the same position on the same object to obtain aligned data. To maximize aligned data collection, we use two methods to gather subsets with different alignment accuracy. We collect fine-grained spatio-temporal aligned data on a calibration platform, while larger-scale coarse-grained spatial aligned data is acquired through handheld collection. 
  % We collect various objects both indoors and outdoors, forming a diverse tactile dataset.
  } 
  \vspace{-5pt}
  \label{fig:dataset}
\end{figure*}

\textbf{Representation Learning.} Representation learning has achieved remarkable success in improving model generalization in various fields. Techniques like **Devlin et al., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"** and masked autoencoder (MAE) **He et al., "Masked Autoencoders for Self-Supervised Image Classification"** have enhanced the model's performance across various downstream applications. 
With the rise of multi-modal learning, representation learning has expanded its impact across fields.
Vision-language pre-training **Liu et al., "Multimodal Pre-Training for Vision-Language Tasks"** has seen tremendous success, and more modalities, including audio **Saeidi et al., "Audio-Visual Fusion Networks for Multi-Modal Learning"**, touch **Li et al., "Cross-Modal Alignment for Multi-Source Data Representation Learning"**, and 3D point clouds **Wang et al., "Point-Cloud-Based Object Recognition"** are being integrated. Among them, tactile information from visuo-tactile sensors can be expressed as images, allowing vision-related techniques to make strides in touch.  Applying MAE **He et al., "Masked Autoencoders for Self-Supervised Image Classification"** or multi-modal aligning **Li et al., "Cross-Modal Alignment for Multi-Source Data Representation Learning"** has enhanced tactile model capabilities. 
However, these efforts have not explored how to obtain a unified visuo-tactile representation suitable for various tasks.
Our research addresses this challenge from both static and dynamic perspectives, enhancing cross-sensor transferability across various tasks through semantic-level multi-modal aligning and cross-sensor matching.

% \textbf{Tactile Representation Learning.} Representation learning methods have been widely used to enhance the perceptual capabilities of tactile models. An effective strategy is to align the touch modality with other modalities, including touch-vision alignment **Li et al., "Touch-Vision Alignment for Multi-Modal Learning"**, touch-text alignment **Wang et al., "Touch-Text Alignment for Tactile Object Recognition"**, and touch-vision-text alignment **Zhang et al., "Touch-Vision-Text Alignment for Multi-Modal Learning"**. Besides, another effective approach is generative representation learning. Some works **He et al., "Masked Autoencoders for Self-Supervised Image Classification"** use MAE to utilize unpaired tactile data. **Zhu et al., "Variational Autoencoder for Tactile Representation Learning"** use VQVAE to learn a compact tactile latent space. Unlike these methods, our framework not only utilizes MAE and multi-modal alignment simultaneously, but also extends the input format to tactile videos. Additionally, we propose a cross-sensor matching task to enhance the model's cross-sensor transferability.
% \vspace{-9pt}