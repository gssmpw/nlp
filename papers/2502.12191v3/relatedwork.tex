\section{Related Work}
\vspace{-4pt}

\textbf{Multi-Source Learning.}
Learning from multi-source data with greater data scale and diversity is expected to enhance the model's performance and generalization ability, but faces challenges in integrating the data representation spaces.
Researchers have found that multi-source models often struggle to capture a unified representation suffering from the discrepancies between data sources~\citep{glorot2011domain,zhao2019learning}. 
Contrastive learning~\citep{wang2022english,zhao2023leveraging} is proposed to learn language-agnostic representations for multi-source language training, while cycle consistency loss~\citep{zhu2017unpaired,kim2022style} aligns the target domain distributions in multi-source image generation.
Similarly, to integrate multi-source tactile data from different sensors, techniques such as multi-sensor joint training~\citep{zhao2024transferable,higuerasparsh,gupta2025sensorinvarianttactilerepresentation}, multi-modal alignment~\citep{yang2024binding}, and cross-sensor generation~\citep{rodriguez2024touch2touch} have emerged. However, these methods overlook the benefits of jointly utilizing multi-modal data and aligned multi-sensor data to bridge the sensor gap. 
In this work, we collect an aligned multi-modal multi-sensor dataset and propose learning unified multi-sensor representations.
% match the multi-source gap in visuo-tactile perception models, leading to robust and generalizable representation in downstream tactile perception.

\textbf{Visuo-tactile Perception.} Visuo-tactile sensors have garnered widespread attention due to their high resolution~\citep{yuan2017gelsight, donlon2018gelslim, lambeta2020digit, zhang2024duragel,zhang2025artificial}. Nowadays, many works utilize visuo-tactile sensors to capture contact deformations, enabling the completion of dexterous manipulation such as dense packing~\citep{li2022see}, grasping~\citep{xu2024unit}, and insertion~\citep{li2014localization}. Visuo-tactile sensors can also collaborate with other sensors to assist the robot in performing more complex manipulation tasks. For instance, \cite{feng2024play} dynamically integrates visuo-tactile signals with visual and audio data to collaboratively accomplish fine-grained manipulation tasks, such as pouring and peg insertion with keyway. In addition to these tasks requiring dynamic tactile perception, visuo-tactile sensors are also used in static tasks such as material classification~\citep{yang2022touch} and shape reconstruction~\citep{gao2022objectfolder2}. 
However, due to the low standardization of visual-tactile sensors, these methods fail to leverage larger and more diverse data from other sensors and lack sensor transferability. In this work, we propose learning a unified multi-sensor representation from both static and dynamic perspectives.


% \textbf{visuo-tactile Sensor Transferring.} visuo-tactile sensors are diverse and lack standardization. Most algorithms rely on data from a specific sensor for training, and they could become non-functional when that sensor is unavailable. Some works have made preliminary attempts to train the same model using data from multiple sensors, thereby enabling the model to transfer across different sensors~\cite{yang2024binding, zhao2024transferable}. 
% By using contrastive learning, \cite{yang2024binding} try to bind multiple sensors with the vison modality. In contrast, \cite{zhao2024transferable} integrates datasets from different sensors and trains a unique encoder for each sensor. Recently, \cite{rodriguez2024touch2touch} proposes training a cross-sensor generative model using paired data from two sensors, allowing the use of the relevant algorithms of a specific sensor when it is unavailable.
% In this work, we integrate the advantages of these methods. We explore learning to extract sensor-agnostic features from data across multiple sensors to achieving cross-sensor transferability.

\begin{figure*}[t]
  \centering
    \includegraphics[width=12.4cm]{figures/DATASET.pdf}
    % \vspace{-5pt}
  \caption{\textbf{TacQuad: an aligned multi-modal multi-sensor tactile dataset from four visuo-tactile sensors.} We select \Mini~\citep{gelsight} \ and \DIGIT~\citep{lambeta2020digit} \ from publicly available sensors, \DuraGel~\citep{zhang2024duragel} \ from self-made sensors, and \Tac~\citep{zhang2022tac3d} \ from force field sensors for diversity. There is a noticeable gap between the data from these sensors. We use the four sensors to touch the same position on the same object to obtain aligned data. To maximize aligned data collection, we use two methods to gather subsets with different alignment accuracy. We collect fine-grained spatio-temporal aligned data on a calibration platform, while larger-scale coarse-grained spatial aligned data is acquired through handheld collection. 
  % We collect various objects both indoors and outdoors, forming a diverse tactile dataset.
  } 
  \vspace{-5pt}
  \label{fig:dataset}
\end{figure*}

\textbf{Representation Learning.} Representation learning has achieved remarkable success in improving model generalization in various fields. Techniques like BERT~\citep{devlin2018bert}  and masked autoencoder (MAE)~\citep{he2022masked}  have enhanced the model's performance across various downstream applications. 
With the rise of multi-modal learning, representation learning has expanded its impact across fields.
Vision-language pre-training~\citep{radford2021clip} has seen tremendous success, and more modalities, including audio~\citep{guzhov2022audioclip}, touch~\citep{yang2024binding}, and 3D point clouds~\citep{xue2023ulip}, are being integrated. Among them, tactile information from visuo-tactile sensors can be expressed as images, allowing vision-related techniques to make strides in touch.  Applying MAE~\citep{cao2023learn,higuerasparsh} or multi-modal aligning~\citep{yang2024binding, cheng2024touch100k} has enhanced tactile model capabilities. 
However, these efforts have not explored how to obtain a unified visuo-tactile representation suitable for various tasks.
Our research addresses this challenge from both static and dynamic perspectives, enhancing cross-sensor transferability across various tasks through semantic-level multi-modal aligning and cross-sensor matching.

% \textbf{Tactile Representation Learning.} Representation learning methods have been widely used to enhance the perceptual capabilities of tactile models. An effective strategy is to align the touch modality with other modalities, including touch-vision alignment~\cite{kerr2022ssvtp,dave2024multimodal,yang2024binding,george2024visuo}, touch-text alignment~\cite{yu2024octopi}, and touch-vision-text alignment~\cite{cheng2024tlv,cheng2024touch100k,fu2024tvl,lei2024vitlen}. Besides, another effective approach is generative representation learning. Some works~\cite{cao2023learn,zhao2024transferable} use MAE~\cite{he2022masked} to utilize unpaired tactile data. \cite{xu2024unit} use VQVAE~\cite{van2017neural} to learn a compact tactile latent space. Unlike these methods, our framework not only utilizes MAE and multi-modal alignment simultaneously, but also extends the input format to tactile videos. Additionally, we propose a cross-sensor matching task to enhance the model's cross-sensor transferability.
% \vspace{-9pt}