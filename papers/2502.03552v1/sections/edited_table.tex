

\begin{table*}[!th]
    \centering
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{|l|r|r|r|l|r|r|r|r|} 
       \cline{1-4} \cline{6-9}
        & \multicolumn{3}{c|}{Retrieve} & &\multicolumn{4}{c|}{Retrieve + Rerank} \\ 
               \cline{2-4} \cline{6-9}
               &  \multicolumn{3}{c|}{Hits@10}   & & \multicolumn{2}{c|}{Hits@10}  & \multicolumn{1}{c|}{Speedup (Times)}  \\
Dataset  &  Baseline  &  DE-2 Random  & DE-2 CE  & & DE-2 CE &  Baseline & DE-2 CE & Baseline \\
       \cline{1-4} \cline{6-9}
all nli  & 0.77 & 0.59 & 0.75 &  &\textbf{ 0.84} & 0.83 & 269.40 & 59.32 \\
eli5  & 0.43 & 0.12 & 0.29 &  & 0.43 & 0.49 & 190.41 & 35.11 \\
gooaq  & 0.75 & 0.42 & 0.64 &  & 0.77 & 0.8 & 246.32 & 42.39 \\
msmarco  & 0.95 & 0.81 & 0.89 &  & 0.61 & 0.64 & 171.80 & 33.55 \\
natural questions  & 0.77 & 0.37 & 0.61 &  & \textbf{0.68 }& 0.68 & 55.29 & 10.17 \\
quora duplicates  & 0.68 & 0.48 & 0.65 &  &\textbf{ 0.97} & 0.97 & 148.20 & 28.99 \\
sentence compression  & 0.95 & 0.83 & 0.93 &  & \textbf{0.97 }& 0.96 & 291.76 & 44.41 \\
simplewiki  & 0.97 & 0.93 & 0.97 &  &\textbf{ 0.98 }& 0.98 & 126.99 & 26.27 \\
stsb  & 0.97 & 0.87 & 0.97 &  & \textbf{0.98} & 0.98 & 422.70 & 90.42 \\
zeshel  & 0.28 & 0.13 & 0.24 &  & 0.25 & 0.27 & 20.83 & 4.35 \\
       \cline{1-4} \cline{6-9}
    \end{tabular}
    % }
    \caption{Comparison of CE infused DE. \textbf{Baseline} is a pre-trained DE model.  \textbf{DE-2 Random} is a trained two-layered DE initialized with random initial weights. \textbf{DE-2 CE} is a two-layered DE infused with initial weight from CE as explained in Fig~\ref{fig:dual_cross} and trained similar to DE-2. }
    % \textbf{Retrieve + Rerank}: Columns 5-6 present Accuracy@10 and columns 7-8 present number of documents encoded per sec. Here, the documents retrieved using baseline and our approach are reranked using a CE.}
    \label{tab:comp}
\end{table*} 