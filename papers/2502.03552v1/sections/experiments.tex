\section{Experiments}
\subsection{Models}
\label{models}
We picked DE-CE model pairs with the same base pretrained model to compare. While massive benchmarks such as ~\cite{muennighoff2023mtebmassivetextembedding} are useful evaluating sentence encoders, we focused here on a smaller set of IR benchmarks just to demonstrate that a CE based knowledge infusion can be useful. As noted earlier, we made no attempt to ensure that both the DE and CE had the same training regimen since we used pretrained models; in fact, at least for the first pair of SBERT models, the training code to train the DE uses hard negatives from many different CEs, so that DE distils knowledge from multiple CEs. 
The model pairs used for comparison are:
\begin{itemize}[noitemsep,topsep=0pt]
% \setlength\itemsep{-0.5em}
    \item \href{https://www.sbert.net/docs/pretrained-models/msmarco-v3.html}{msmarco-MiniLM-L-12-v3} (dual encoder) and \href{https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2}{cross-encoder/ms-marco-MiniLM-L-12-v2} (cross encoder)\footnote{We used v3 for the DE instead of v5 to keep the two versions as comparable as possible.}, each with 33M parameters.
    \item \href{https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1} {mixedbread-ai/mxbai-embed-large-v1} (dual encoder) and \href{https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1}{mixedbread-ai/mxbai-rerank-large-v1} (cross encoder), with 335M and 435M parameters respectively.
\end{itemize}
%

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/CEDE.png}
    \caption{Comparison of layerwise performance of DE and CE on the msmarco pair of models. While performance improves monotonically with layers for DE, the CE embeddings from lower layers show surprisingly good performance, on many datasets.}
    \label{fig:msmarco_layer}
\end{figure}

% \vspace{-2em}
\subsection{Datasets}
 Since embeddings are typically used for multiple purposes, we selected 12 datasets sampled to range over sentence similarity, question answering, and entity linking to cover a broader range of NLP tasks.\footnote{Task descriptions are available here: \href{https://sbert.net/docs/sentence_transformer/dataset_overview.html}{SBERT Datasets}.} We used datasets from the BEIR benchmark \cite{Thakur2021Beir} such as quora, fiqa, and scifact for testing the distilled DE model \cite{quora-question-pairs, Wadden2020Scifact, Maia2018Fiqa}. For ms-marco~\cite{nguyen2016ms}, we used the dev set, because the train split was used to train the first pair of models. For STSB, which scores sentence relatedness on a scale, we adapted it for a search task by using a 0.5 threshold to select sentence pairs. This dataset was chosen primarily because it was used in the original SBERT work~\cite{reimers-2019-sentence-bert}; zeshel was used because it was used in \cite{yadav2022efficient}. We also used the training data for other SBERT datasets such as all-nli, sentence-compression, natural-questions, eli5, and simplewiki~\cite{bowman-etal-2015-large, filippova-altun-2013-overcoming, 47761, fan-etal-2019-eli5, coster-kauchak-2011-simple}. Note that we only show layer by layer analysis for 10/12 datasets in Figures ~\ref{fig:msmarco_layer}, ~\ref{fig:msmarco}, and ~\ref{fig:mixed-bread}. This was done with a sample of 100,000 sentence pairs of the training data to show that these datasets exhibit a common pattern. However, the datasets shown in Table~\ref{tab:comp} are their respective full size training/dev datasets whose results are comparable to SOTA results.   
%TBD add a table here characterizing what we actually did.
\input{sections/results_table}
\subsection{Layer wise analysis}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/MS_Marco.png}
    \caption{Performance of the DE final output layer (DE), CE embed, CE encoding layer 0 for the ms-marco pair of models.}
    \label{fig:msmarco}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/MxBAI.png}
    \caption{Performance of the DE final output layer (DE), CE embed, CE encoding layer 0 for the mixed bread pair of models.}
    \label{fig:mixed-bread}
\end{figure}
In this experiment, we report Top-10 cosine accuracy, or Hits@10\footnote{The evaluator used was    \href{https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html}{Information Retrieval Evaluator}} 
as a metric for brevity; other information retrieval metrics showed a similar pattern. We also report MRR@10 in section~\ref{infused}.  As shown in Figure~\ref{fig:msmarco_layer}, CE hidden states in earlier layers contain a strong signal for information retrieval.  Surprisingly, hidden states from the DE's embedding layer were substantially worse than that of the CE by an average of 29.5\% across the 10 datasets for the ms-marco model pairs (paired two-tailed t test was significant, p < .01).  Further, Hits@10 for the CE's embedding layer was at 80\% or greater of the Hits@10 measure for the final output of the DE in 5/10 datasets; only in 2 cases was it under 30\%, see Figure~\ref{fig:msmarco}.
%This suggests that in some cases, the CE embedding layer can be exploited for search, even without further training.  

We observed the same pattern in the mxbai model pair\footnote{Layer wise analysis is not illustrated here due to space.}; Hits@10 for the DE embedding layer was worse than the CE embeddings by an average of 29.2\% (paired two-tailed t test was significant, p < .01).  Hits@10 performance of the CE was at 79\% or greater than Hits@10 performance of the final output of the DE in 5 of 10 datasets. Only in 1 case (natural ques.) was CE performance at below 30\% of the final DE performance, see Figure~\ref{fig:mixed-bread}. 

%For jinaai models, the cross encoder had only 6 layers, and the dual had 12.  The performance of the cross encoder's embedding layers was on average 13.7\% better.  The cross encoder reached 80\% or greater of the final performance of the dual encoder on 3 of 10 datasets, and 2 datasets had performance worse than 30\% (TBD check all statements after all datasets are done).  Interestingly, the datasets where such poor performance occurred varied across model pairs.  For msmarco, the cross encoder embedding performed worst on eli5, for mbxai, it was natural-questions, and for jinaai, it was eli5 and natural-questions.

As shown in Figures~\ref{fig:msmarco} and~\ref{fig:mixed-bread}, CE embeddings are effective directly on some datasets (e.g. stsb and sentence-compression).%which is interesting and fits with our other work where we observed that CE embeddings trained on tabular downstream tasks could be used directly for search using the same technique described here.  A paucity of tabular training data made it difficult to build strong dual encoders in that case, so we did not have direct comparisons to dual encoders in the tabular case.  
%For tabular search, we observed that CE embeddings could beat state of the art non-neural systems, as well as neural dual encoders that were finetuned for tabular data discovery%\footnote{Citation excluded for anonymity requirement}.

% \subsection{Nature of cross encoder embeddings}
% % Since the signal from the CE is strong in earlier layers, it is possible that the signal we saw in CEs reflects a more shallow surface level encoding of sentences.  This may explain the better performance on stsb or sentence-compression datasets, where related statements from a sentence pair are extremely similar lexically and syntactically.  To determine the extent to which similarity of lexical tokens and sentence structure affects CE Hits@10 performance, we took 10,000 sentences from the sentence compression dataset, and quora-duplicates datasets respectively, and used a \href{https://www.ibm.com/docs/en/watsonx/w-and-w/1.1.x?topic=by-granite-13b-chat-v2-model-card}{language model} to generate lexically and syntactically different sentences.\footnote{We subset it to these two datasets because these two had sentences that were relatively short}.  The prompt used for generation is shown below with an example sentence, as is the LLM output.

% Since the CE signal is stronger in earlier layers, it likely reflects a more surface-level encoding of sentences. This may explain the improved performance on datasets like STSB and sentence compression, where sentence pairs are highly similar lexically and syntactically. To investigate how lexical and structural similarity affects CE Hits@10 performance, we selected 10,000 sentences from the sentence compression and Quora duplicates datasets chosen for their relatively short sentences, and used a \href{https://www.ibm.com/docs/en/watsonx/w-and-w/1.1.x?topic=by-granite-13b-chat-v2-model-card}{LLM} to generate lexically and syntactically different variations of the same sentence. An example prompt and output are shown below.

% \begin{mdframed}[style=exampledefault,frametitle={Prompt},font=\small]
%         \textit{Rephrase this sentence so that the meaning is the same but the words and sentence structure is different - be succinct:} Adam Sandler is jealous of Russell Brand.
% \end{mdframed}

% \begin{mdframed}[style=exampledefault,frametitle={LLM Output},font=\small]
% Adam Sandler regards Russell Brand as his rival.
% \end{mdframed}

% %\begin{table}[!th]
% %    \centering
% %    % \resizebox{\columnwidth}{!}{
% %    \begin{tabular}{l|l|l} \hline
% %   Datasets   & CE &  DE \\ \hline
% %      sentence-compression   & .95 & .97 \\
% %      quora-duplicates & .87 & .92 \\ \hline
% %    \end{tabular}
% %    % }
% %    \caption{Embedding performance on datasets with varied lexical and %syntax}
% %    \label{tab:paraphrase}
% %\end{table}


% CE Hits@10 performance was 0.95 for sentence-compression compared to .97 for DE.  On quora-duplicates, CE Hits@10 was .87 compared to .92 for the DE. These results suggest that CE hidden states do capture meanings of sentences, albeit not as effectively as DE's final output.  

\subsection{Cross encoder infused dual encoders}
\label{infused}
Is it possible to leverage this CE knowledge in early layers to build more efficient DEs?  We trained the ms-marco models using the same training code from the \href{https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/ms_marco/train_bi-encoder_mnrl.py}{sentence-transformers library}. Weights from the embedding layer and encoding layer 0 of the CE were copied to the DE as described in section~\ref{ce-de} to create a 15M parameter DE-2 CE\footnote{The step of how many layers of the CE need to copied to the DE was determined by experimentation, we tried copying 2 encoding layers or no encoding layers at all, just the embedding layer - this version had the best performance for this model.}. We compared it to a two layered DE with random initial weights to measure the impact of the CE in DE-2 CE and a DE-12 CE (a 12-layer Dual Encoder with CE initialized weights). Table~\ref{tab:comp} shows the results.  
\begin{itemize}[noitemsep,topsep=0pt]
    \item The average (Hits@10, MRR@10) for re-ranked DE-12 CE vs DE-2 CE was \textbf{(0.68, 0.55)} vs \textbf{(0.67, 0.54)}. This shows that the performance gain for  DE-12 CE is minimal even with 10 additional layers while offering no speedup advantage. Hence, this was omitted in Table ~\ref{tab:comp}.
    \item DE-2 CE is clearly better than DE-2 Rand.
    \item The difference between DE-2 CE after re-ranking whose weights were copied to the DE, was at least within 1\% of the 12 layered baseline DE with reranking on 6/12 datasets. We note that the baseline 12 layered DE was trained with hard negatives from multiple sources, so this result is significant. 
    \item On average DE-2 CE is only 0.99\% worse than the baseline DE across 12 datasets which was not statistically significant (paired two tailed t-test p > .19), but the \textbf{speedup for inference is on average 5.15x.}
\end{itemize}     





