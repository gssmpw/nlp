\section{Limitations}

Our work covers limited sets of models and of datasets.  This is especially the case for the infusion of CE knowledge into DEs, since the fine-tuning code for mixed bread (mxbai) models was not available, unlike for msmarco.  

 There is a huge range of architectures for infusing a DE with CE knowledge.  We explored a small fraction of that space, and not exhaustively. 

 In the case of showing that the CE infused DE performs as well as the the baseline DE, we performed a t-test to show that the difference was not significant; but this is trying to prove the null hypothesis, which is not feasible.  We did show statistically significant better performance of earlier layers of the CE versus earlier layers of the DE, suggesting that the sample size was sufficient to detect those effect sizes.

 We explored a limited variety of NLP tasks, and there may be other use cases that benefit from our techniques to a greater or lesser extent.

 Finally, we note that all of our work was done on English text, and we did not study how it applies in other languages.

\section{Risks}
We don't see any risks other than the risks associated with using the original models.

\section{Ethical Concerns}

We cannot think of any ethical concerns that apply to this specific work beyond those that apply more generally to this domain.  

In fact, our work potentially makes dual encoders more efficient by reducing energy requirements.