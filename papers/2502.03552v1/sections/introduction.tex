\section{Introduction}
Following the introduction of the BERT model ~\cite{devlin2018bert}, encoder only retrieval models can be divided into two types: (a) dual encoder (DE)~\cite{lu2020twinbertdistillingknowledgetwinstructured}, where a model is trained by encoding each sentence in a pair by two encoders that share the same weights, and a loss function that ensures similar sentences have embeddings that are close in vector space, (b) cross encoder (CE) ~\cite{nogueira2020passagererankingbert, gao2021rethinktrainingbertrerankers} where a single encoder gets a sentence pair as input, and the model is trained to classify if the sentence pair is similar or not. Conventional wisdom is that DEs are less accurate than CEs~\cite{Hofstatter2020}. However, as CEs require a pair of sentences as input, they cannot be deployed in the initial retrieval phase due to the costs of computing the embeddings of the query with each document in the corpus.
A common information retrieval pipeline is therefore to use a DE for initial retrieval of the top K documents, whilst using CEs to rerank these top K results to improve accuracy. 

In this work, we report a curious empirical finding that the hidden states of earlier layers of CEs can be used for search.  This in itself should not be surprising: both the CE and DE were fine tuned from a pretrained model which has some representation of meanings of the sentences. However, we note that hidden states of earlier layers of the CE contain a good signal for sentence representations for IR, but later layers do not, presumably because later hidden states reflect the computation of differences between the pair of sentences. Additionally, hidden states of the earlier layers of the DE are \textit{significantly worse} than those of the CE for the same dataset, despite both sharing the same base pretrained model. The final layer of the DE is often better for accuracy of retrieval; the CE matches in some cases.  It appears that CEs extract information relevant for information retrieval in \textbf{\textit{earlier}} layers than DEs.  While DEs are commonly used as the base retrieval model because they can achieve low retrieval latency, training DEs is significantly more complex compared to CEs, often requiring knowledge distillation from cross encoder models to create a set of \textit{hard negatives}, as well as large training sets. We show how to exploit the signal in earlier layers of CEs to build more efficient sentence embedding models.
Our contributions are as follows:
\begin{itemize}
    \item We show that hidden states from earlier CE layers contain a stronger signal for information retrieval than earlier dual encoder layers.
   % \item We show that these hidden CE states capture the semantics of a sentence, not just similarity in lexical structure.
    \item We build a 2 layer DE model using weights from earlier layers of a CE, as a new form of knowledge infusion.  This model is at least within 1\% accuracy of the baseline DE on 6/12 of the datasets.
    \item We show that inference is about 5 times faster on the 2-layer model.
\end{itemize}
