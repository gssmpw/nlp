\section{Related Work}
Analysis of BERT encoder attention was first performed by ~\cite{clark2019doesbertlookat}, but their focus was not knowledge distillation from CEs.

While DEs are commonly used for dense retrieval, recent work \cite{gao2021condenserpretrainingarchitecturedense} has proposed a better architecture called Condenser to build encoders.  Similarly, new techniques have been proposed for better hard negative mining to build better DEs~\cite{conf/iclr/XiongXLTLBAO21,DBLP:conf/naacl/QuDLLRZDWW21}.  Our focus is on knowledge distillation rather than changes in architectures or negative mining.   

DEs are much faster than CEs~\cite{humeau2019poly}, and CEs are more effective than DEs, so researchers have investigated approaches for cross-architecture distillation from a CE to a DE~\cite{Hofstatter2020,karpukhin2020densepassageretrievalopendomain,lu2022ernie,rauf2024bce4zsr}. Reranking is a common method for it where CEs are used after retrieval by either DEs or BM-25~\cite{YilmazYZL19,Nogueira2019,LogeswaranCLTDL19, gao2021rethinktrainingbertrerankers}. Shallow CEs have been proposed to use CEs to rerank documents more efficiently ~\cite{PetrovMM24}. Another example of CE use is a matrix factorization-based approach to efficiently approximate CE scores from a small subset of query-document pairs ~\cite{yadav2022efficient}. 
%

%
%
% approach: % ranking with a dual encoder and re-ranking cross encoders
% of retrieving relevant documents and then using cross encoder to rerank ther results
%

%and avoids using DE for retrieval.

%In our work we also distill knowledge from CE to DE, however, we use a 2-layered DE and propose a novel CE knowledge infusion approach. 
% Cases where dual and cross encoders work together to produce efficient vector embeddings in state of the art systems purely involve knowledge distillation from Cross Encoders to Dual Encoders and hard negative mining. Multiple studies have proved the advantages of these methods. 
% For example, \citet{rauf2024bce4zsr} uses a teacher-student model for zero-shot news recommendations. 
%
% While this multi-stage approach is effective at improving the retriever's performance, \citet{yadav2022efficient} argued that the accuracy of such a two-stage approach is limited by the recall of the initial candidate set, and requires additional computational resources to align the dual encoder with the cross-encoder model. They proposed an efficient way to approximate cross encoder distances to make retrieval independent of training a dual encoder. 
%
% Our work uses dense representations of sentences from SBERT based dual encoders similar to \citet{karpukhin2020densepassageretrievalopendomain}  and \citet{lu2022ernie} which proved to out-perform traditional Lucene-BM25 based search systems. 
% We also acknowledge that our experiments can be extended to works that explore effective sparse models such as SPLADE \cite{10.1145/3634912} which aim to achieve better interpretability and efficiency. \hk{This might be good for conclusion/future work}