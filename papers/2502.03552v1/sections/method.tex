\section{Method}
\subsection{Embeddings extraction}
\begin{figure}
    \centering
    \includegraphics[width=.9\columnwidth]{figures/fig1ce.pdf}
    \caption{Extraction of layer-wise embeddings}
    \label{fig:embedding_extraction}
\end{figure}

Figure~\ref{fig:embedding_extraction} shows the approach to extract embeddings from a CE.  Since CEs are trained to process sentence pairs, to embed a single sentence, we simply pair it with itself.  To extract layer-wise embeddings, we mean pool over the tokens of the sentence for each layer, excluding padding and separator tokens.

\subsection{Knowledge infusion from cross encoder}
\label{ce-de}
\begin{figure}
    \centering
    \includegraphics[width=.9\columnwidth]{figures/fig2ce.pdf}
    \caption{Knowledge infusion from cross encoder}
    \label{fig:dual_cross}
\end{figure}
Figure~\ref{fig:dual_cross} shows how to infuse the DE with structure learnt by the CE.  Weights from the embedding layer in BERT, which converts each input token to an input-vector embedding, and layer 0 from the BERT encoder are both copied into a DE model from the CE.  Encoding layer 1 was initialized to a set of random weights as usual, and the dual encoder was trained with the same ms-marco dataset,  with the training code from the sentence-transformer library \href{https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/ms_marco/train_bi-encoder_mnrl.py}{(see here)}.  In using this training code, we used only BM-25 for hard negatives to avoid the need for knowledge distillation from additional models to create a DE from a CE. We term this model DE-2 CE in the rest of this paper.  Training time for this model was about 1 hour on an A100 GPU. We also compare DE-2 CE to a is a trained two-layered DE initialized with random initial weights. We call this model DE-2 Rand in this paper.

\subsection{Reranking}
\label{reranking}
Adhering to standard IR pipelines, in all our experiments we report retrieval metrics from the DE, and retrieve + rerank metrics obtained from re-ranking the top 50 most relevant results of the DE by a CE.
Reranking is done by the same CE used to distill information to the DEs mentioned in section \ref{ce-de}. We compare the final reranked results for the Baseline models chosen in \ref{models}, DE-2 CE, and  DE-2 Rand - as mentioned in \ref{ce-de} in Table \ref{tab:comp}.
