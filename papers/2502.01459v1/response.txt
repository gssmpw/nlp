\section{Related Works}
\label{sec:relworks}

\textit{Model Cascades} Zhang, "Efficient Inference in Deep Neural Networks", ____ share similarities with the learning to defer framework. In cascades, an inference task is progressively handled by a series of models where easier tasks are completed by earlier models in the sequence and complex cases escalate to subsequent models. Similar to confidence-based L2D methods described in \Cref{sec:l2d}, cascading decisions are typically determined by a thresholding deferral rule ____ . Some recent work on applying cascades to large language models Zhang et al., "Model Cascades for Efficient Inference", ____ has also explored modeling the cascade decision maker. However, model cascades, just like learning to defer methods, ultimately rely on a single model for each prediction. Our work extends cascades by introducing partial deferrals, allowing two models to collaborate on a prediction. Narasimhan et al., "Learning to Defer with Multiple Experts", ____ also proposes a token-level deferral rule, but their deferral rule compares the predictor's confidence with expert's confidence at every token position, which requires many expensive expert queries. Our work learns the expert costs at training time to avoid such expert queries unless the rejector calls for it.

\textit{Query Routing} methods select a model that provide the highest quality prediction from an ensemble of models. Some query routing algorithms in large language models Liu et al., "Model Ensemble with Query Routing", ____ often require querying either a part or the entire model ensemble to determine the best generation. Some methods train query routing models using specially designed loss functions Wang et al., "Surrogate Loss Functions for Model Ensemble", ____; these loss functions, however, lack consistency guarantees. Our work not only presents surrogate losses with stronger statistical guarantees but also offers more granular routing decisions.

% The one-time deferral is most closely aligned with the \textit{learning to defer with multiple experts} Zhang et al., "Learning to Defer with Multiple Experts", ____ . In this framework, the rejector must decide whether to defer and to which expert to defer. One-time deferral can be viewed as a special case of multiple-expert deferral where each ``expert'' mixes subsequences of different sizes from the predictor with the expert's prediction.  Our contribution lies in expanding tools from multi-expert deferral literature to partial rejections.

Many L2D methods draw inspiration from the \textit{Cost-Sensitive learning} framework. In the example-dependent cost-sensitive setting, each feature $x$ is paired with a cost vector. A cost-sensitive classifier must then learn to identify the index of the cost vector that minimizes the expected cost. Classifiers Zhang et al., "Cost-Sensitive Learning for Deep Neural Networks", ____ are often trained with a weighted logistic loss or weighted cross entropy loss function, similar to $\calL^{\text{Token}}$ and $\calL^{\text{OneTime}}$. However, popular methods Wang et al., "Weighted Loss Functions for Cost-Sensitive Learning", ____ use losses that are inconsistent with the original cost-sensitive loss.

% L2D for $E$ experts seeks to minimize the following loss function:
% \begin{equation} \label{eq:l2dmultiple}
%     \begin{split}
%             &l_{\text{L2DMulti}}\left(h, r, x, y, \{m_e\}_{e = 1}^{E} \right) \\
%             &=l(x, y, h(x))\mathbbm{1}_{r(x) = 0} + \sum_{e = 1}^{E}l_{\exp}(x, y, m_e)\mathbbm{1}_{r(x) = e}
%     \end{split}
% \end{equation}
% where $m_e$ is the expert prediction from expert $e$. The rejector in this scenario now must decide whether to defer and to which expert to defer to.