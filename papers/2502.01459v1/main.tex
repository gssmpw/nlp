%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{bbm}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand\ambuj[1]{\textcolor{red}{AT: #1}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
\usepackage{PRIMEarxiv}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% calligraphic letters
\newcommand{\calA}{{\cal A}}
\newcommand{\calB}{{\cal B}}
\newcommand{\calC}{{\cal C}}
\newcommand{\calD}{{\cal D}}
\newcommand{\calE}{{\cal E}}
\newcommand{\calF}{{\cal F}}
\newcommand{\calG}{{\cal G}}
\newcommand{\calH}{{\cal H}}
\newcommand{\calI}{{\cal I}}
\newcommand{\calJ}{{\cal J}}
\newcommand{\calK}{{\cal K}}
\newcommand{\calL}{{\cal L}}
\newcommand{\calM}{{\cal M}}
\newcommand{\calN}{{\cal N}}
\newcommand{\calO}{{\cal O}}
\newcommand{\calP}{{\cal P}}
\newcommand{\calQ}{{\cal Q}}
\newcommand{\calR}{{\cal R}}
\newcommand{\calS}{{\cal S}}
\newcommand{\calT}{{\cal T}}
\newcommand{\calU}{{\cal U}}
\newcommand{\calV}{{\cal V}}
\newcommand{\calW}{{\cal W}}
\newcommand{\calX}{{\cal X}}
\newcommand{\calY}{{\cal Y}}
\newcommand{\calZ}{{\cal Z}}

\newcommand{\wh}{\widehat}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\F}{\mathbb F}
\newcommand{\Q}{\mathbb Q}
\newcommand{\C}{\mathbb C}
\newcommand{\Prx}{\mathop{\bf Pr\/}}
\newcommand{\E}{{\bf E}}
\newcommand{\Ex}{\mathop{\bf E\/}}
\newcommand{\Var}{{\bf Var}}
\newcommand{\Varx}{\mathop{\bf Var\/}}
\newcommand{\Cov}{{\bf Cov}}
\newcommand{\Covx}{\mathop{\bf Cov\/}}
\newcommand{\tr}{\text{Tr} }

\renewcommand{\a}{{\boldsymbol a}}
\renewcommand{\b}{{\boldsymbol b}}
\renewcommand{\c}{{\boldsymbol c}}
\renewcommand{\d}{{\boldsymbol d}}
\newcommand{\e}{{\boldsymbol e}}
\newcommand{\f}{{\boldsymbol f}}
\newcommand{\g}{{\boldsymbol g}}
\newcommand{\h}{{\boldsymbol h}}
\renewcommand{\i}{{\boldsymbol i}}
\renewcommand{\j}{{\boldsymbol j}}
\renewcommand{\k}{{\boldsymbol k}}
\newcommand{\m}{{\boldsymbol m}}
\newcommand{\n}{{\boldsymbol n}}
\renewcommand{\o}{{\boldsymbol o}}
\newcommand{\p}{{\boldsymbol p}}
\newcommand{\q}{{\boldsymbol q}}
\renewcommand{\r}{{\boldsymbol r}}
\newcommand{\s}{{\boldsymbol s}}
\renewcommand{\t}{{\boldsymbol t}}
\renewcommand{\u}{{\boldsymbol u}}
\renewcommand{\v}{{\boldsymbol v}}
\newcommand{\w}{{\boldsymbol w}}
\newcommand{\x}{{\boldsymbol x}}
\newcommand{\y}{{\boldsymbol y}}
\newcommand{\z}{{\boldsymbol z}}
\newcommand{\A}{{\boldsymbol A}}
\newcommand{\B}{{\boldsymbol B}}
\newcommand{\D}{{\boldsymbol D}}
\newcommand{\G}{{\boldsymbol G}}
\renewcommand{\H}{{\boldsymbol H}}
\newcommand{\I}{{\boldsymbol I}}
\newcommand{\J}{{\boldsymbol J}}
\newcommand{\K}{{\boldsymbol K}}
\renewcommand{\L}{{\boldsymbol L}}
\newcommand{\M}{{\boldsymbol M}}
\renewcommand{\O}{{\boldsymbol O}}
\newcommand{\bbP}{\mathbf P}
\renewcommand{\S}{{\boldsymbol S}}
\newcommand{\T}{{\boldsymbol T}}
\newcommand{\U}{{\boldsymbol U}}
\newcommand{\V}{{\boldsymbol V}}
\newcommand{\W}{{\boldsymbol W}}
\newcommand{\X}{{\boldsymbol X}}
\newcommand{\Y}{{\boldsymbol Y}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\fancyhead[CO]{Learning to Partially Defer for Sequences}
\renewcommand{\headrulewidth}{0.5pt}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Learning to Partially Defer for Sequences}

\title{Learning to Partially Defer for Sequences}

\author{
  Sahana Rayan \\
  Department of Statistics \\
  University of Michigan \\
  Ann Arbor, MI 48104 \\
  \texttt{srayan@umich.edu}
  \And
  Ambuj Tewari \\
  Department of Statistics \\
  University of Michigan \\
  Ann Arbor, MI 48104 \\
  \texttt{tewaria@umich.edu}
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

\maketitle





\begin{abstract}
% With the growing interest in deploying artificial intelligence in highly consequential areas like healthcare and criminal justice, assessing and handling the trustworthiness of these systems is of great importance.
In the Learning to Defer (L2D) framework, a prediction model can either make a prediction or defer it to an expert, as determined by a rejector. Current L2D methods train the rejector to decide whether to reject the {\em entire prediction}, which is not desirable when the model predicts long sequences. We present an L2D setting for sequence outputs where the system can defer \textit{specific outputs} of the whole model prediction to an expert in an effort to interleave the expert and machine throughout the prediction.
%We design an appropriate cost-sensitive loss function for training a post-hoc rejector for a pre-trained predictor. 
% We propose two types of model-based post-hoc rejectors for pre-trained predictors: a token-level rejector, which defers specific token predictions to experts with next token prediction capabilities, and a one-time rejector, which identifies a point in the sequence from which the expert carries forward the prediction.
We propose two types of model-based post-hoc rejectors for pre-trained predictors: a token-level rejector, which defers specific token predictions to experts with next token prediction capabilities, and a one-time rejector for experts without such abilities, which defers the remaining sequence from a specific point onward.
% which can provide different types of deferrals. \ambuj{prolly wanna say more about the two approaches} 
In the experiments, we also empirically demonstrate that such granular deferrals achieve better cost-accuracy tradeoffs than whole deferrals on Traveling salesman solvers and News summarization models.
% In the experiments, we also empirically prove the benefit \ambuj{make the benefit concrete in some way} of granular deferrals on Traveling salesman solvers and News summarization models.
\end{abstract}

% TL;DR: We propose two types of model-based rejectors: one for experts capable of next-token prediction, which defers token-by-token, and another that defers the remainder of the sequence prediction to experts
%TL;DR: We propose model-based rejectors that enable models to defer prediction on parts of a sequence to an expert, based on the expert's ability to perform next token prediction. 
% Keywords: learning to defer, sequences, trustworthy machine learning
\keywords{learning to defer, sequences, trustworthy machine learning}

\section{Introduction} \label{sec:intro}

Trustworthiness of AI systems is under increased scrutiny with the increased deployment of such systems in highly consequential areas like healthcare \cite{ker2017deep,courtiol2019deep,asan2020artificial} and criminal justice \cite{dressel2018accuracy,rigano2019using,alikhademi2022review}. Hybrid intelligent systems \cite{kamar2016directions,dellermann2019hybrid,akata2020research, maadi2021review} approach this problem through collaborations between either humans and machines or two machines to improve confidence in the system. Learning to Defer (L2D) \cite{madras2018predict} is one such framework that accommodates these teams by allowing the model to defer to an expert when it is uncertain about a prediction task with the objective of maximizing overall system accuracy while minimizing deferral costs. 


Existing L2D methods focus on multiclass classification \cite{mozannar2020consistent,cao2022generalizing,verma2022calibrated, mao2024predictor,mao2024theoretically} and scalar regression \cite{cheng2024regression,mao2024regression}. However, these methods only support complete deferral of prediction outputs, resulting in costs that scale with the output size, when applied to large \textit{sequence outputs}  \cite{narasimhan2024faster}. This is especially inefficient if only parts of the model's prediction are inaccurate. The prevalence of sequences in areas such as drug discovery \cite{wang2018computational,jumper2021highly,dauparas2022robust,nijkamp2023progen2} and language modeling \cite{kenton2019bert,brown2020language, raffel2020exploring} calls for a cost-effective extension of the L2D framework to handle sequential outputs.  
% However, these methods are not appropriate when the outputs are {\em sequences}. The prevalence of such outputs in areas such as drug discovery \cite{wang2018computational,jumper2021highly,dauparas2022robust,nijkamp2023progen2} and language modeling \cite{kenton2019bert,brown2020language, raffel2020exploring} calls for an appropriate extension of the L2D framework to handle sequential output types. 
% Crucially, complete deferral in such tasks with high-dimensional prediction outputs can incur costs that scale with the output size \cite{narasimhan2024faster}. 

To this end, we propose a fine-grained approach to deferral where the system has the flexibility to defer {\em portions} of the predicted sequence. For example, in part of speech tagging, this mechanism would only require the expert to tag a few uncertain sentences in a paragraph rather than the whole paragraph, reducing the time cost which increases with the number of words to be tagged.
% In the case of part of speech tagging, for instance, the time cost scales with the number of words to be tagged, making the cost of tagging a few words in paragraph far less than that on the whole paragraph. A fine-grained approach to deferral where the system has the flexibility to defer a {\em portion} of the predicted sequence 
This can, therefore, achieve a more favorable cost-accuracy tradeoff, which can further improve as the deferrals become more precise - for example, at the word-level. 
%This approach encourages the model to learn some of the nuances of the expert's expertise. 
Additionally, parts of the prediction task completed by the predictor can also function as contextual clues for the language expert to fill in the deferred parts, further minimizing deferral costs. 
% With protein design, for example, the expert biologist may express uncertainty about specific parts of the sequence related to particular functions. 

While partial deferral is therefore desirable, the granularity of such deferrals
% - which dictates the cost effectiveness of the system - 
depends on the size of the sequence segments the expert can predict at a time. For instance, if the expert is a next-token predictor, it can complete deferred parts of the prediction, as small as a single token.
% However, this form of partial deferral that we call \textit{token-level} deferral may still only be as good as whole deferrals if the predictive quality across all parts of the output are dependent.
This form of partial deferral that we call \textit{token-level} deferral can become especially useful when the model's predictive quality across all parts of the output are dependent.
% can be further improved when the model is autoregressive. 
% the specifics of its implementation differ depending on the expert's capabilities. 
% A natural bifurcation in settings is whether the expert is capable of next-token prediction. An expert with such 
% as this determines the expert's ability to pass the prediction ``baton'' back to the original predictor mid-sequence. 
% In the former setting,
% Specifically, an expert can be capable of giving token-by-token feedback or only 
% deferral by parts or partial deferral becomes especially useful if the predictive quality across all parts of the output are dependent.
Specifically, in autoregressive prediction, the uncertainty at earlier prediction points can propagate to later points and, in turn, affect the predictive accuracy of the complete sequence \cite{bengio2015scheduled, lamb2016professor,schmidt2019generalization}.
% The token-level rejector in such situations will naturally defer most of the sequence if called after the model completes the prediction task. 
% But, a better cost-accuracy tradeoff can be achieved if it can orchestrate a more dynamic collaboration between the predictor and the expert where the predictor receives an expert prediction on an early substructure to predict the next token before the model veers ``off-course''. 
Thus, to achieve a better cost-accuracy tradeoff, the token-level rejector can orchestrate a dynamic collaboration between the predictor and the expert where the predictor receives an expert prediction on an early substructure to predict the next token before the model veers ``off-course''.


Although the granularity offered from token-level deferral can improve predictive quality with low deferral costs, this is only feasible with experts that can make predictions one token at a time. That said, partial deferrals can still be implemented as long as the expert is at least capable of completing the predicted sequence given a partial subsequence from the model. We refer to this type of deferral as \textit{one-time} deferral.
% Partial deferrals can still be used as long as the expert can leverage a predicted subsequence to make the entire sequence prediction easier.  
% Although interweaving the expert's and machine's prediction can improve predictive quality with low deferral costs, it requires the expert to predict the next token given the partial sequence rather than completing the entire sequence. 
% it assumes that the expert can leverage the partial sequence to predict the next token rather than completing the entire sequence.
For example, Gurobi solvers for Traveling salesman problems lack the ability to iteratively select the next city in the tour, but they can use partial subtours generated by models as constraints to generate the entire tour at a reduced cost. In this case, the one-time rejector would select a point in the sequence from which the expert completes the sequence. 
% Token-level deferrals also rely on access to an online expert during inference time, which may not always be feasible, especially with human experts. 
% To this end, we introduce an L2D framework for the aforementioned whole-sequence feedback setting, called one-time deferral, where the rejector selects a deferral point in the sequence from which the expert completes the sequence. 
% To implement both L2D frameworks, we present a convex loss function with consistency guarantees for each of the two settings that rejector models can minimize to optimally support the respective deferral capabilities. We further supplement the theoretical analysis of these loss functions with generalization bounds for minimizing their empirical risks. We finally evaluate the cost-accuracy trade-offs of our method with news summarization and traveling salesman problem solvers against whole deferral methods using deferral curves.

In this paper, we make the following contributions:
\begin{itemize}
    \item We present a novel learning to defer framework where the rejector interweaves the expert's and machine's predictions to improve cost-accuracy tradeoffs. We divide this setting into token-level and one-time deferrals based on the granularity of the deferrals, thereby accommodating various types of experts.
    % To accommodate various types of experts, we specify a bifurcation of this setting into token-level and one-time deferrals, 
    % formalizing them through loss functions that enable the rejector model to perform these deferrals when minimized.
    \item We propose convex surrogate loss functions that offer Bayes consistency guarantees leading to implementable model-based post hoc rejectors for pre-trained predictors for each type of deferral. We further analyze the surrogates theoretically through generalization bounds.
    % \item We formalize each of these rejection types through loss functions to enable the rejector model to perform the corresponding deferrals when minimized and complement the losses with Bayes consistent surrogates leading to implementable model-based post hoc rejectors for pre-trained predictors. We further analyze the surrogates through generalization bounds. 
    \item We empirically demonstrate improved cost-accuracy tradeoffs when using model-based rejectors for either deferrals against whole sequence rejectors with news summarization and traveling salesman problem solvers.
\end{itemize}

% In this paper, we propose two different L2D settings which enable partial deferrals using pre-trained autoregressive predictors - 1) deferring the prediction from a token position (determined by the rejector) to the end of the sequence, 2) deferring prediction on specific token positions and receiving expert's prediction of those tokens online. We propose surrogate loss functions with consistency guarantees and generalization bounds for both settings. We evaluate the performance of our method with news summarization and traveling salesman problem solvers.


\section{Background} \label{sec:background}

\subsection{Learning to Defer} \label{sec:l2d}
% \ambuj{should we point other terms in the literature that are closely connected to ``deferral". e.g., ``rejection" and ``abstention"}
Deferrals, also known as rejections or abstentions, are classified into two types -
% There are two common approaches to L2D - 
\textit{confidence}-based \cite{chow1957optimum,bartlett2008classification,yuan2010classification,ramaswamy2018consistent} and \textit{model}-based \cite{cortes2016learning,madras2018predict, mozannar2020consistent,verma2022calibrated,cao2022generalizing,mao2024predictor,cheng2024regression,mao2024regression}.
% \textit{score}-based \cite{mozannar2020consistent,verma2022calibrated,cao2022generalizing},

\textit{Confidence}-based methods involve learning a predictor where there is some prescribed cost for rejection; if the confidence score outputted by the learned predictor falls below a cost-dependent threshold, then it is rejected. However, Madras et al. \cite{madras2018predict} and Cortes et al. \cite{cortes2016learning} characterized the suboptimality of these methods and advocated for the rejector to be a model trained either simultaneously with the predictor or post-hoc: this is the \textit{model}-based approach. This system seeks to minimize the following loss function:
\begin{equation} \label{eq:l2d}
    \begin{split}
        \calL_{\text{L2D}}&\left(h, r, e, x, y\right)\\
    &=l(y, h(x))\mathbbm{1}_{r(x) < 0} + c(x, y, e)\mathbbm{1}_{r(x) \geq 0}
    \end{split}
\end{equation}
where $h$ is the predictor, $r$ is the rejector, $e$ is the expert that the model defers to, $l(y, h(x))$ is the model loss, and $c(x, y, e)$ is the cost incurred when deferring to an expert. 
% This approach is applicable in the regression setting \cite{cheng2024regression,mao2024regression} as well as the multiclass classification setting \cite{mozannar2020consistent,verma2022calibrated}.
% \textit{Score}-based models are a type of predictor-rejector models which involve appending the label space with a deferral label and deferring when the score for deferral outputted by the predictor is higher than the score for all other labels, $r(x) = \mathbbm{1}\left[g_{\perp}(x) > \argmax_{y \in \calY} g_{y'}(x)\right]$ where $\calY$ is the label space, $\perp$ is the deferral label, and $g_{y}(x)$ refers to the score function for label $y \in \calY$. 

% \ambuj{might be confusing to reader why we were discussing multiple experts. point out the connection made in section 4.3 here}
While our settings assume access to only one expert, we note that one-time deferral closely resembles L2D with access to predictions from multiple experts \cite{verma2023learning,mao2024regression}. The details of this similarity and its implications are discussed in \Cref{sec:largeL}. In particular, L2D for $E$ experts aims to minimize the following loss function:
\begin{equation} \label{eq:l2dmultiple}
    \begin{split}
            &\calL_{\text{L2DMulti}}\left(h, r, x, y, \{e_j\}_{j = 1}^{E} \right) \\
            &=l(y, h(x))\mathbbm{1}_{r(x) = 0} + \sum_{j = 1}^{E}c_j(x, y, e_j)\mathbbm{1}_{r(x) = j}
    \end{split}
\end{equation}
where $c_j(x, y, e_j)$ is the cost of querying the $j^{\text{th}}$ expert, $e_j$. The rejector in this scenario now must decide whether to defer and which expert to defer to.
% While our settings assume access to only one expert, we leverage the similarities between one-time deferral and \Cref{eq:l2dmultiple} in \Cref{sec:largeL}.

\subsection{Consistent Losses} \label{sec:consistent}

The direct minimization of \Cref{eq:l2d,eq:l2dmultiple} can be challenging due to the non-convex and discontinuous nature of the functions. This can be addressed by optimizing a convex surrogate loss function which, upon minimization, will lead to the same outcome as minimizing the original loss. This property is formally called \textit{Bayes Consistency}.
% \ambuj{the def below is for simple settings, not for the complicated setting of \cref{eq:l2d}. point out that part of our job will be to connect the two}

\begin{definition}[Consistency]\label{th:defconsistency}
    Let the risk of a hypothesis $f$ with respect to a loss $l$ be $\calR_{l}(f) = \mathbb{E}_{(x, y) \sim \calP_{\calX, \calY}}[l(f(x), y)]$. Let the optimal risk with respect to $l$ out of all hypotheses in $\calF$ be $\calR_{l}^*(\calF) = \inf_{f \in \calF} \calR_{l}(f)$. A surrogate loss function, $\phi(\cdot)$, is said to be $\calF$-consistent with respect to $l$ if for any $f_n \in \calF$, the following property holds true:
    \begin{align*}
        \lim_{n \to \infty} \calR_{\phi}(f_n) - \calR_{\phi}^*(\calF) = 0  \Rightarrow  \lim_{n \to \infty} \calR_{l}(f_n) - \calR_{l}^*(\calF) = 0
    \end{align*}
\end{definition}

When $\calF$ is the class of all measurable functions, Bayes consistency is achieved if the excess risk of $l$ can be related to the excess risk of $\phi$ through a non-decreasing function $\Gamma : \mathbb{R}\rightarrow\mathbb{R}$ for which $\Gamma$ is continuous at zero and $\Gamma(0)=0$ in the following manner: 
% \ambuj{don't we also need continuity of $\Gamma$ at zero?}
$$\calR_{l}(f) - \calR_{l}^* \leq \Gamma \left(\calR_{\phi}(f) - \calR_{\phi}^*\right)$$ 
where  $\calR_{l}^* =  \inf_{f} \calR_{l}(f)$ is the optimal risk over the class of all measurable functions.
In this paper, we propose loss functions similar to \Cref{eq:l2d,eq:l2dmultiple} for both token-level and one-time deferral and then, we show that the above inequality holds true for our surrogate designs for both forms of deferral, proving Bayes consistency.
% In this paper, we will show that the above inequality holds true for the loss functions for both tokenwise and one-time deferral similar to \Cref{eq:l2d} and their respective surrogates to prove Bayes consistency.

% Specifically, \cite{mozannar2020consistent} presented the following softmax parameterization of a score-based loss function:
% \begin{align*} 
% % \begin{split}
%     l_{\text{SoftmaxSurrogate}}(h, r, x, y, m) &= -\log \left(\frac{\exp(g_y(x))}{\sum_{y' \in \calY \cup \perp}\exp(g_y(x))}\right) - \mathbbm{1}_{m = y}\log \left(\frac{\exp(g_{\perp}(x))}{\sum_{y' \in \calY \cup \perp}\exp(g_y(x))}\right)
% % \end{split}
% \end{align*}
% where $\calY$ is the label space, $\perp$ is the deferral label, $g_{y'}(x)$ refers to the score function for label $y' \in \calY$, and $m$ is the expert's prediction.

% \subsection{Autoregressive Sequence Prediction} \label{sec:seqpred}

% Okay, so there are actually four types of sequence prediction problems: sequence-to-sequence models, sequence predictions (predicting what's next in the sequence), sequence classification, sequence generation models. but let's focus on only two,
% Sequence outputs usually refer to series of outputs that come from a label set. Text outputs and biological sequences are considered sequence outputs. Formally, if $y$ is a sequence, each output $y_j$ which is a part of $y$ belongs to a label set $\calV$ which is sometimes referred to as the vocabulary. Each member of the vocabulary is referred to as a token. Since every sequence could be of variable length, the sequence space $\calV^* = \bigcup_{l = 1}^{\infty} \calV^l$ represents all possible sequences that can be formed from $\calV$. There are two types of prediction problems with sequence as the output: 1) \textit{sequence-to-sequence} models and 2) \textit{sequence generation}.
% \textit{Sequence-to-sequence} \autocite{sutskever2014sequence} predictor maps an input sequence to another sequence which can be of a different length than the input sequence. This problem is quite common in language tasks like translation tasks and text summarization. \textit{Sequence generation} models refer to predictors that generate sequences using some side information. Image caption generation and protein language modeling are some examples of sequence generation. 
% Regardless of the prediction task, the model seeks to minimize the subset 0/1 loss $\mathbbm{1}\left[y \neq h(x)\right]$ or the hamming loss $\frac{1}{|y|} \sum_{j = 1}^{|y|} \mathbbm{1}\left[y_j \neq h(x)_j\right]$. The subset 0/1 loss \cite{cheng2010bayes,dembczynski2012label} in particular encourages the model to learn the true conditional distribution $\calP_{Y \mid X}$ and so, it is trained with a negative log likelihood loss which can be expressed as $- \log\left[p_{h}\left(Y = y \mid X = x\right)\right]$ where $p_{h}$ refers to the predictor's ($h$) probability model. Using the Bayes rule, the above loss function can be decomposed as:
% \begin{equation} \label{eq:seqlossdecomp}
%     - \log\left[p_{h}\left(Y = y \mid X = x\right)\right] = -\sum_{j = 1}^{|y|}\log\left[p_{h}\left(y_{j} \mid y_{<j},x\right)\right]
% \end{equation}
% where $y_{<j} = (y_1, \cdots, y_{j - 1})$. This autoregressive decomposition enables the model to sequentially predict tokens to recover the full sequence prediction. Typically, $p_{h}\left( y_{j} \mid y_{<j},x\right)$ distribution is evaluated as a softmax output across all labels in $\calV$, $p_{h}\left( y_{j} \mid y_{<j},x\right) = \underset{v \in \calV}{\text{softmax}}\left(\g(y_{<j},x)\right)$ where $\g(y_{<j}, x) \in \R^{|\calV|}$ is real-valued vector of scores for each token in the vocabulary. Recurrent neural Networks, long short-term memory networks, and transformers are commonly uses to learn $\g$. During inference, the token is can be predicted by either picking the most probable token ($\wh y_j = h(x, y_{<j}) = \argmax_{v \in \calV} p_{h}\left(y_{j} = v \mid y_{<j},x\right)$), sampling using the modeled probability distribution ($h(x, y_{<j}) \sim  p_{h}\left(y_{j} \mid y_{<j},x\right)$), or sampling from the top $k$ most probable tokens. We, however, will only consider models that use greedy search decoding. 
% Beam search is a deterministic decoding method that considers the top $k$ most probable partial sequences $y_{1:j}$ at every time step $j$ using the beam from the previous time step. At every time step $j$, the beam of width $K$ or $\wh Y_{1:j} = \{\wh y_{1, 1:j}, \cdots, \wh y_{K, 1:j}\}$ is determined as follows
% \begin{equation} \label{eq:beamsearch}
%     \wh Y_{1:j} = \underset{(\wh y_{<j}, v) \in \wh Y_{<j} \times \calV }{\text{Top-K}}\left(p_{h}(v \mid x, \wh y_{<j})p_{h}(\wh y_{<j} \mid x)\right) 
% \end{equation}
% talk about top-k sampling, random sampling, and top-1 sampling or greedy sampling. We will assume top-1 sampling
% \subsection{Language Model Cascades} \label{sec:langcasc}
% \subsection{Related Works} \label{sec:relworks}
% Language Model Cascases blah Language Model Cascases blahLanguage Model Cascases blahvvLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blah. Language Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blahLanguage Model Cascases blah


% Model crowdsourcing blah Model crowdsourcing blah Model crowdsourcing blah Model crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blahModel crowdsourcing blah

\section{Token-level partial deferral} \label{sec:tokendeferral}
 
% Let $\calY \subseteq \bigcup_{l = 1}^L \calV^l$ be the output sequence space where $\calV = \{1, \cdots, |\calV|\}$ is the vocabulary of all possible tokens that $y_j \in y \in \calY$ can assume and each sequence can be of maximum length $L$. Let $\calX$ be the feature space. In the case of sequence generation, $\calX \subseteq \R^d$ can be a $d$-dimensional feature space or $\calX = \bigcup_{l = 1}^L \calV'^l$ can be another sequence space characterized by $\calV'$ vocabulary set. The sequence predictor $h$ is trained using $\calD_t = \{(x^i, y^i)\}_{i = 1}^{n_t}$ where $x^i \in \calX$ is a feature vector and $y^i \in \calY$ is its corresponding response sequence of the $i^{th}$ instance. 
% Each training sample $(x^i, y^i)$ can be assumed to be random samples from some unknown data distribution $\calP_{X, Y}$. 

% In L2D setting for whole deferrals, when the rejector $r \ : \calX \to \{0, 1\}$ outputs an non-negative value, the learner will query the expert with the cost of $c(x)$. When $r(x) < 0$, the sequence predictor $h$ will make the predictions suffers a loss $l_{\text{model}}(x, y, h)$ which can be either a subset 0/1 loss or hamming distance. Formally, the loss function for this L2D system would be:
% \begin{align*}
%     \calL^{\text{Whole}}\left(h, r, x, y, m\right) = l_{\text{model}}(x, y, h)\mathbbm{1}_{r(x) < 0} + c(x)\mathbbm{1}_{r(x) \geq 0}
% \end{align*}
Let $\calY \subseteq \calV^L$ be the output sequence space where each sequence can have a maximum length $L$ and $\calV$ is the label set that any individual component or token $y_j \in y \in \calY$ of a sequence can belong to. $\calV$ can be a finite vocabulary of tokens in the case of text sequences and be $\R$ when the output is a scalar time series. Let $\calX$ be the feature space. In sequence generation, $\calX \subseteq \R^d$ would be a $d$-dimensional feature space, whereas $\calX = \calV'^L$, perhaps, is characterized by a distinct $\calV'$ vocabulary set in sequence-to-sequence learning tasks. We denote $\calP_{X, Y}$ to be the data-generating distribution over $\calX \times \calY$. 

Suppose we have a predictor $h$ and an expert $e$ with stronger predictive capabilities than $h$. $e$ takes in $x \in \calX$ and the leftward context $\wh y_{<j} = (\wh y_1, \cdots, \wh y_{j - 1}) \in \calV^{j - 1}$ as inputs to predict the $j^{\text{th}}$ label in the sequence; repeated calls to $e$ can auto-regressively complete the entire sequence prediction task. $e$ can, for instance, be a human or a large autoregressive model. However, querying $e$ is more expensive than calling $h$. As a result, instead of deferring an entire sequence prediction to the expert, we wish to design an L2D system where experts can complete uncertain tokens in the model's prediction,
% that interleaves the expert's predictions with the model's predictions at the token-level
facilitating more fine-grained deferrals - 
referred to as \textit{token-level} deferrals - in order to achieve a better cost-quality tradeoff.
% Suppose we have a predictor $h$ which takes $x \in \calX$ and the leftward context $\wh y_{<j} = (\wh y_1, \cdots, \wh y_{j - 1}) \in \calV^{j - 1}$ as inputs to predict the $j^{\text{th}}$ label in the sequence; repeated calls to $h$ can auto-regressively complete the entire sequence prediction task. Along with the predictor, we assume access to an expert $e$ with stronger predictive capabilities than $h$ which can also leverage partial sequence outputs to predict the next item in sequence. $e$ can, for instance, be a human or a larger autoregressive model. However, querying $e$ will be a more expensive operation than calling $h$. As a result, instead of deferring an entire sequence prediction to the expert, we wish to design an L2D system that interleaves the expert's predictions with the model's predictions, creating more granular deferrals, also called \textit{token-level} deferrals, in an effort to achieve a better cost-quality tradeoff.

% In an autoregressive setting, this can also curb uncertainty propogation by allowing the model to receive a cleaner outputs from an expert when the model is deemed uncertain by the rejector. This left-to-right token prediction allows the L2D system to accommodate granular deferrals. 

If $h$ can also perform autoregressive prediction, token-level deferrals can additionally enable the predictor to leverage the predicted token from the expert to predict the next token, fostering a more seamless and adaptive collaboration between the expert and the predictor. This can further improve cost-effectiveness by curbing uncertainty propagation for the predictor, thereby reducing the number of expert calls. While our token-level deferrals paradigm can apply to all sequence predictors, we will specifically focus on creating a dynamic L2D system with an autoregressive predictor.

% can curb uncertainty propogation by allowing the $h$ to receive a cleaner outputs from an expert when the model is deemed uncertain by the rejector.

In this setting, at the $j^{\text{th}}$ step, the rejector for the $j^{\text{th}}$ token, $r_j$, takes in $x$ and $\wh y_{<j}$. 
%from the beam $\wh Y_{<j}$ of width $K$ as input. 
It is important to note that $\wh y_{<j}$ can be viewed as a mixture of predictions from the model and the expert. When $r_j(x, \wh y_{<j}) \geq 0$, the rejector chooses to reject the $j^{\text{th}}$ label prediction from $h$ and receives a prediction $\wh y_{j}^e$ from an expert.
%with the help of the beam from the previous time step $\wh Y_{<j}$. 
% This prediction will form 
%the ``beam'' 
% $\wh y_{1:j}$ for the $j$th time step. 
Otherwise, the model will continue with the prediction from $h$, i.e. $\wh y_{j}^h = h(x, \wh y_{<j})$. For a fixed $h$ and a specific instance of $(x, \wh y_{<j}, y)$, the optimal $r_j$ can be determined by minimizing the following loss function:
% greedy search
%\Cref{eq:beamsearch} 
% to create the next token.
% For the deferral of the $j$th token, we will consider the following loss function:
\begin{equation} \label{eq:tokendeferral}
    \begin{split}
         \calL_j\left(h, r_j, x, \wh y_{<j}, y\right) 
         = l&\left(y, \wh y_{j}^h \right)\mathbbm{1}_{r_j\left(x, \wh y_{<j}\right) < 0} \\
         &+ c_j(x, \wh y_{<j}, y)\mathbbm{1}_{r_j\left(x, \wh y_{<j}\right) \geq 0}
    \end{split}
\end{equation}
where $l\left(y, \wh y_{j}^h\right)$ is the loss incurred from using $\wh y_{j}^h$ and $c_j(x, \wh y_{<j}, y)$ is the cost of querying the expert for the $j^{\text{th}}$ token. $l\left(y, \wh y_{j}^h\right)$ can be the mean squared error defined by $(y_j - \wh y_{j}^h)^2$ or a $0$-$1$ loss between the true $j^{\text{th}}$ token and $\wh y_{j}^h$ in text sequences. Generally, $l\left(y, \wh y_{j}^h\right)$ can be viewed as token-level feedback to generate an accurate complete sequence. $c_j(x, \wh y_{<j}, y)$ can be a constant $c_j$ to reflect the cost of querying $e$ at the $j^{\text{th}}$ step but it can also be designed as $l\left(y, \wh y_{j}^e\right)$ to account for the expert's errors.

Finally, the predicted $j^{\text{th}}$ label $ \wh y_j = \wh y_{j}^h\mathbbm{1}_{r_j\left(x, \wh y_{<j}\right) < 0} + \wh y_{j}^e\mathbbm{1}_{r_j\left(x, \wh y_{<j}\right) \geq 0}$ is then appended to $\wh y_{<j}$ to serve as an input for $(j + 1)^{\text{th}}$ token prediction, and this process continues until $L$ labels in the sequence are generated.
% is determined as follows:
% \begin{equation*}
%     \wh y_{1:j} = \left(\wh y_{<j}, \wh y_{j}^h\mathbbm{1}_{r_j\left(x, \wh y_{<j}\right) < 0} + \wh y_{j}^e\mathbbm{1}_{r_j\left(x, \wh y_{j}^h\wh y_{<j}\right) \geq 0}\right)
% \end{equation*}
The total deferral loss for the entire sequence can be characterized as $\calL^{\text{Token}}\left(h, r, x, y\right) = \frac{1}{L}\sum_{j = 1}^{L} \calL_j\left(h, r_j, x, \wh y_{<j}, y\right)$ where $r = \begin{bmatrix}
    r_1 & \dots & r_L
\end{bmatrix}^T$. 
% When a sequence in the beam is predicted to be less than length $l < L$, the $l + 1$th token onward will be either \textit{<eos>} or \textit{<pad>}. The rejectors will stop running after \textit{<eos>} is returned.
The $\calL^{\text{Token}}$-risk is denoted by $\calR_{\text{Token}}(h, r) = \mathop{\mathbb{E}}_{(x, y) \sim \calP_{X, Y}}[\calL^{\text{Token}}(h, r, x, y)]$.
% \comment{Write a note about sequences of length less than L}

\subsection{Surrogate Loss} \label{sec:modelbased}

As described in \Cref{sec:consistent}, minimizing the deferral loss directly is computationally difficult, so identifying an optimal rejector requires proposing and optimizing a convex surrogate loss that is Bayes consistent with respect to $\calL^{\text{Token}}$. To this end, we propose the following surrogate loss function:
% \sout{In order to verify if a surrogate loss possesses this property, we must derive the optimal minimizers $r^*_j$ of the risk $\calR_j(r) = \Ex_{(x, y) \sim \calP_{X, Y}}[\calL_j(h, r, x, y_j)]$ and $\calR_{\text{Token}}(r) = \Ex_{(x, y) \sim \calP_{X, Y}}[\calL(h, r, x, y)]$ for a fixed $h$:}
% \begin{proposition} \label{th:bayesopttoken}
%     \sout{The minimizer $r^*_j$ that are chosen from all measurable functions that seek to minimize $\calR_j(r)$ can be defined point-wise for a given $x \in \calX$ and $y_{<j} \in \calV^{j - 1}$ as follows:}
%     \begin{align*}
%          r^{*}_j(y_{<j}, x) &= \mathbbm{1}\left[c_j(x, y_{<j}) \leq P\left(Y_j \neq h(x, y_{<j}) \mid X = x, Y_{<j} = y_{<j}\right)\right] 
%     \end{align*}
% \end{proposition}
% \sout{The proof of \Cref{th:bayesopttoken} can be found in \Cref{app:proofbayesopttoken}. \Cref{th:bayesopttoken} shows that the optimal rejector $r$ must reject when the deferral cost is less than the 0/1 loss incurred by the model.}
$\calL^{\phi}(h, r, x, y) = \frac{1}{L} \sum_{j = 1}^{L} \calL^{\phi}_{j}(h, r_j, x, \wh y_{<j}, y)$ where 
\begin{equation} \label{eq:tokenwisesurrogate}
    \begin{split}
        \calL^{\phi}_{j}(h, r_j, x, \wh y_{<j}, y) =  l&\left(y, \wh y_{j}^h \right)\phi(r_j(x, \wh y_{<j})) \\
        &+ c_j(x, \wh y_{<j}, y)\phi(-r_j(x, \wh y_{<j}))
    \end{split}
\end{equation}
and $\phi$ is a convex binary surrogate loss. $\phi$ could, for example, be the logistic loss, i.e. $\phi(z) = \log\left(1 + \exp\{-z\}\right)$ or the square loss, i.e. $\phi(z) = (1 - z)^2$. $\calL^{\phi}$-risk is characterized by $\calR_{\phi}(h, r) = \mathop{\mathbb{E}}_{(x, y) \sim \calP_{X, Y}}[\calL^{\phi}(h, r, x, y)]$. 

The optimization of $\calL^{\phi}(h, r, x, y)$ is computationally efficient due its convexity. Additionally, the surrogate loss upper bounds $\calL^{\text{Token}}$ upto a scaling factor, providing alignment between reducing the surrogate loss and reducing the desired loss function. The proof of these properties can be found in \Cref{sec:proofconvexupper}.

% \begin{proposition} \label{th:convexupperbound}
%     If $\phi$ is binary classification calibrated and convex, $\calL^{\phi}(h, r, x, y)$ is a convex upper bound of $\calL^{\text{Token}}(h, r, x, y)$ upto some scale $\gamma$
% \end{proposition}

% The complete proof of \Cref{th:convexupperbound} can be found in \Cref{sec:convexupperbound}. It is desirable that the surrogate loss is \textit{convex} as it provides an ease of minimizing the empirical risk while \textit{upper bound} property ensures that the minimization of the surrogate empirical risk will yield small values of $\calL^{\text{Token}}$.

To show Bayes consistency of the surrogate, we relate the excess risk using $\calL^{\text{Token}}$ and the excess risk using $\calL^{\phi}$ in \Cref{th:upbound}. The proof employs a specific type of distribution on $\calX \times \{-1, 1\}$ with mass on functions of the conditional expectations of $l\left(y, \wh y_{j}^h \right)$ and $c_j(x, \wh y_{<j}, y)$ to relate the two inequalities - a strategy inspired by Mao et al. \cite{mao2024predictor}. The complete proof can be found in \Cref{sec:proofupbound}.

\begin{lemma} \label{th:upbound}
    Let $0 < \bar{c} \leq c_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$ for all $j$, $0 \leq l(y, \wh y^h_j) \leq \bar{l} < \infty$, and $\phi$ be a binary surrogate loss that satisfies the following inequality for any distribution over $\calX \times \{-1, 1\}$ and any measurable function $f$:
    \begin{align*}
        \calR_{\text{binary }0-1}(f) - \calR^{*}_{\text{binary }0-1} \leq \Gamma\left(\calR_{\text{binary }\phi}(f) - \calR^{ *}_{\text{binary }\phi}\right)
    \end{align*}
    where $\calR_{\text{binary }0-1}$ is the binary 0-1 risk and $\Gamma : \R^+ \to \R^+$ is a non-decreasing concave function.
    Then, for any measurable $r$ and any $h$, the following inequality holds for any distribution over $\calX \times \calY$:
    \begin{align*}
        \calR_{\text{Token}}\left(r, h\right) - \calR_{\text{Token}}^*(h) \leq \Tilde{\Gamma}\left(\calR_{\phi}\left(r, h\right) - \calR^*_{\phi}(h)\right)
    \end{align*}
    where $\Tilde{\Gamma}(z) = \left(\bar{l} + \Bar{C}\right)\Gamma\left(\frac{z}{\bar{c}}\right)$, $\calR^*_{\text{Token}}(h) := \inf_{r} \calR_{\text{Token}}\left(r, h\right)$, and $\calR^*_{\phi}(h) := \inf_{r} \calR_{\phi}\left(r, h\right)$
\end{lemma}

Taking the limit on both sides of the inequality result of \Cref{th:upbound} proves consistency of the surrogate loss, formally stated in \Cref{th:consistency}. The proof of \Cref{th:consistency} hinges on the fact that a classification-calibrated surrogate loss satisfies the condition presented in \Cref{th:upbound}. A loss $\phi$ is considered binary classification calibrated if the minimizer of the pointwise $\phi-$risk or $\mathbbm{E}_{y \sim \calP_{\calY \mid \calX = x}}[\phi(yf(x))]$ always has the same sign as $\mathbb{P}[Y = 1\mid X = x] - \frac{1}{2}$. We defer the full proof of \Cref{th:consistency} and the formal definition of classification calibrated loss to \Cref{sec:proofconsistency}.

\begin{theorem}[Consistency]\label{th:consistency}
    If $0 < \bar{c} \leq c_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$ for all $j$, $0 \leq l(y, \wh y^h_j) \leq \bar{l} < \infty$, and $\phi$ be binary classification calibrated, then, for a fixed $h$, $\calL^{\phi}$ is a Bayes consistent surrogate for $\calL^{\text{Token}}$.
\end{theorem}

\subsection{Generalization Bounds} \label{sec:genbounds}

While Bayes consistency is desirable, the property only holds in the limit of infinite data and does not possess finite sample guarantees. In practice, finding a risk minimizer over the entire class of measurable functions is extremely challenging, particularly because $\calP_{X, Y}$ is usually unknown.
% Practically, we can't compute $\calR_{\phi}(r)$ for every $r$ in a class of all measurable functions without knowledge of $\calP_{X, Y}$.
With access to a rejector training data set $\calD$ with $n$ samples, we can only learn the optimal $\wh r_{n} \in \argmin_{r \in \calF} \wh \calR_{\phi}(r) $ from a restricted hypothesis class $\calF$ by minimizing the empirical risk with respect to $\calL^{\phi}$ - $\wh \calR_{\phi}(r) = \frac{1}{n}\sum_{(x^i, y^i) \in \calD} \calL^{\phi}(h, r, x^i, y^i)$. We now present an upper bound for the estimation error to characterize the quality of $\wh r_{n}$ with respect to $\calF$. Please refer to \Cref{sec:proofgenbounds} for the full proof.
% Similar to \Cref{sec:defpointclass}, we assume access to a training data set $\calD_c$ with $n_c$ samples, we can only learn the optimal $\wh r_{n_c} \in \argmin_{r \in \calF} \wh \calR_{\phi}(r) $ by minimizing the empirical risk with respect to $\calL^{\phi}$ or $\wh \calR_{\phi}(r) = \frac{1}{|n_c|}\sum_{(x^i, y^i) \in \calD_c} \calL^{\phi}(h, r, x^i, y^i)$ from a hypothesis class $\calF$. 
% We will now present a generalization bound to characterize the quality of using empirical risk minimization. 

\begin{theorem} \label{th:genbounds}
    Suppose $r \in \calF$ which is composed of $\{\calF_j\}_{j = 1}^{L}$ such that $r_j \in \calF_j$, $c_j(x) \leq \bar{C}$, and $l(y, \wh y^h_j) \leq \bar{l}$. If $\phi$ is $\rho$-lipschitz continuous, then with probability $1 - \delta$, the following upper bound holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL_{\phi}$:
    \begin{equation*}
        \begin{split}
            \calR_{\phi}&\left(\wh r_{n}\right) - \calR^*_{\phi}\left(\calF\right) \\
            &\leq \frac{4\rho\sqrt{2\left(\bar{C}^2 + \bar{l}^2\right)}}{L} \sum_{j = 1}^{L} \mathfrak{R}_n\left(\calF_j\right) + (\bar{C} + \bar{l})\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}
        \end{split}
    \end{equation*}
    where $\calR^*_{\phi}\left(\calF\right) = \inf_{r \in \calF} \calR_{\phi}\left(r\right)$ and $\mathfrak{R}_n\left(\calF_j\right)$ is the Rademacher Complexity of $\calF_j$
\end{theorem}

 From \Cref{th:genbounds}, the estimation error of using $\wh r_n$ with respect to $\calL^{\phi}$ depends on the Rademacher complexity $\mathfrak{R}_n\left(\calF\right)$ of the hypothesis class. \Cref{th:genbounds} alone does not describe the excess risk with respect to the original token loss $\calL^{\text{Token}}$. Using \Cref{th:upbound}, we express the upper bound of the excess risk in \Cref{th:genboundreal}, and the proof can be found in \Cref{sec:proofgenboundreal}.
 % Using \Cref{th:upbound}, we further present the upper bound of the excess risk with respect to $\calL^{\text{Token}}$ in \Cref{th:genboundreal} and the proof can be found in \Cref{sec:proofgenboundreal}.

\begin{corollary} \label{th:genboundreal}
   Suppose $r \in \calF$ which is composed of $\{\calF_j\}_{j = 1}^L$ such that $r_j \in \calF_j$, $0 < \bar{c} \leq c_j(x) \leq \bar{C}$, and $l(y, \wh y^h_j) \leq \bar{l} < \infty$. If $\phi$ is a $\rho$-lipschitz continuous binary classification-calibrated function, then with probability $1 - \delta$, the following upper bound on the excess risk with respect to $\calL^{\text{Token}}$ holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL_{\phi}$ for a fixed $h$:
    \begin{equation*}
        \begin{split}
             \calR_{\text{Token}}\left(\wh r_{n}, h\right) - \calR^*_{\text{Token}}(h) \leq \Tilde{\Gamma}\left(\calB(\calF) + \calA_{\phi}\left(\calF, h\right)\right) 
        \end{split}
    \end{equation*}
    where $\Tilde{\Gamma}$ is a non-decreasing function, $\calB(\calF) = \frac{4\rho\sqrt{2\left(\bar{C}^2 + \bar{l}^2\right)}}{L} \sum_{j = 1}^L \mathfrak{R}_n\left(\calF_j\right) + (\bar{C} + \bar{l})\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}$, and $\calA_{\phi}\left(\calF, h\right) = \calR^*_{\phi}\left(\calF, h\right) -  \calR^*_{\phi}(h)$ is the approximation error.
\end{corollary}

% \subsection{Rejector Implementation} \label{sec:modelarchtokenlevel}
% Although the proposed surrogate loss function exhibits many desirable theoretical properties, these properties are most effectively demonstrated when the loss function is paired with a suitable model architecture. $\calL^{\text{Token}}$ bears resemblance to a weighted multi-label classification loss function. Specifically, the token-level rejector follows a classifier chain approach \cite{read2021classifier}. 

% Classifier chains is a multi-label classification (MLC) approach that involves training $L$ binary classifiers in a pre-specified order with the $j^{\text{th}}$ binary classifier taking the feature vector and the previous $j - 1$ labels as input, forming a chain of predictors. At inference time, the test features are fed into the chain such that the previous label predictions are appended to the features for the next label classifier. Our token-level rejector $r_j$ implicitly makes these chained predictions by taking in $\wh y_{<j}$ which is informed by the previous $(j - 1)$ rejectors' rejection decision.

% With this comparison established, model architectures and training techniques from classifier chain literature can be utilized. Recurrent classifier chains \cite{nam2017maximizing} have emerged as a popular choice for MLC as they efficiently maximize accuracy without requiring the large number of trainable parameters typically needed for $K$ separate binary classifiers. 

% Since the inputs of recurrent classifier chains at training time are typically ground truth labels, training token-level rejectors would similarly require using the correct rejection decision, $\mathbbm{1}_{l\left(y, \wh y_{j}^h \right) > c_j(x, \wh y_{<j}, y)}$, to determine the $j^{\text{th}}$ token prediction for the next label rejector. Following this strategy, also called ``teacher forcing'' \cite{williams1989learning}, can often cause a distribution shift, as label predictions will be used at test time instead of the correct label, resulting in errors propagating through the chain. Scheduled sampling \cite{bengio2015scheduled} offers a middle ground by labeling the training data with the correct rejection decision with probability $p$, and otherwise using the predicted one. This probability $p$ often decays at a linear or exponential rate over epochs, gradually reducing the model's dependence on ground-truth rejection rules. 


\section{One-time partial deferral} \label{sec:defpointclass}
% Let $\calY \subseteq \bigcup_{l = 1}^L \calV^l$ be the output sequence space where each sequence can have a maximum length $L$ and $\calV$ is the label set to which each individual component $y_j \in y \in \calY$ of a sequence belongs. $\calV$ can be a finite vocabulary of tokens in the context of text sequences while $\calV$ can also be $\R$ when the sequence space contains all possible numerical sequences.
% Let $\calX$ be the feature space. In sequence generation, $\calX \subseteq \R^d$ would be a $d$-dimensional feature space or $\calX = \bigcup_{l = 1}^L \calV'^l$ can be another sequence space characterized by $\calV'$ vocabulary set. The autoregressive sequence predictor $h$ is trained using $\calD_t = \{(x^i, y^i)\}_{i = 1}^{n_t}$ where $x^i \in \calX$ is a feature vector and $y^i \in \calY$ is its corresponding response sequence for the $i^{th}$ instance. Since autoregressive models perform left-to-right token prediction, the L2D system can be improved to pick a point to defer the rest of the prediction to the expert. 
Token-level deferrals are ideal to produce partial deferrals, but they hinge on the expert's capability to perform next-token prediction instead of generating the entire sequence at once. If the expert is at least capable of completing sequences using predicted subsequences from models, the system can achieve better cost-quality tradeoffs with partial deferrals compared to whole deferrals, albeit constrained by the lack of granularity in deferral, through \textit{one-time} deferral. In the one-time partial deferral setting, the rejector takes in $x$ as input and identifies a point in sequence - referred to as the \textit{deferral point} - from which the expert predicts the remaining sequence.
% In the one-time partial deferral setting, the rejector takes in $x$ as input.
When $r(x) = j$, the rejector chooses to reject the prediction of the sequence from the $j^{\text{th}}$ token onward and thereafter defers to the expert. In doing so, the system incurs a loss of $l(y, \widehat{y}_{<j})$ for using the first $j$ tokens from the predictor $h$ and pays a cost of $\Tilde{c}_j(x, \widehat{y}_{<j}, y)$ for using the expert with the partial feedback from $h$. When $r(x) = 1$, the entire prediction task is deferred to the expert. If $r(x) = L + 1$, the model completes the full prediction without paying any cost for an expert. For this deferral point classification, our objective is to minimize the following loss function:
\begin{equation} \label{eq:deferralpoint}
    \begin{split}
    \calL^{\text{OneTime}}_{\text{Full}}&\left(h, r, x, y\right) \\
        &\sum_{j = 1}^{L + 1} \left(l(y, \widehat{y}_{< j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) = j} 
    \end{split}
\end{equation}
where $l(y, \wh y_{<1}) = 0$
% When a sequence is less than length $l < L$, the $l + 1$th token is usually encoded as a stopping token or \textit{<eos>} and every token after that will be observed as padding token or $\textit{<pad>}$. 
% \subsection{Equivalence to Multiclass Classification}
% We note that $\calL^{\text{OneTime}}$ is similar to loss function for learning to defer with multiple experts in \Cref{eq:l2dmultiple} where each ``expert'' mixes subsequences of different sizes from the predictor with the expert's prediction. 
% THIS IS SOOO WEIRD

\subsection{Influence of large \texorpdfstring{$L$}{L}} \label{sec:largeL}

% From \Cref{eq:l2dmultiple} and \Cref{eq:deferralpoint}, it is clear that 
The one-time deferral paradigm is strongly related to the \textit{learning to defer with multiple experts} framework, which is evident from the similarities between their loss functions (\Cref{eq:l2dmultiple,eq:deferralpoint}).  
% One-time deferral can be viewed as a special case of multiple-expert deferral where 
Each point of the sequence can be seen as corresponding to a distinct ``expert'' with its prediction consisting of the predictor output up to that point and to the deferred expert's prediction for the remainder.
% each ``expert'' mixes subsequences of different sizes from the predictor with the expert's prediction, providing a way to generate multiple expert predictions from a single expert.
However, L2D system accuracy tends to plateau with an increasing number of experts, which naturally has ramifications on the cost-effectiveness of the system \cite{verma2023learning,hemmer2023learning}.
% However, L2D system accuracy tends to plateau with an increasing number of experts \cite{verma2023learning,hemmer2023learning}. 
% Such diminishing returns naturally have ramifications on the cost-effectiveness of the system. 
In particular, it suggests that allowing for one-time deferral at each point of the sequence may be over-finely resolved, as deferring on neighboring labels may offer a negligible difference in accuracy.

Towards this end, we wish to allow for flexibility in adjusting the number of ``experts''. We achieve this by considering a generalization of one-time deferral where the predictor can defer to the expert on a select set of token positions specified by $\calJ$, instead of having all token positions be potential deferral points. For this generalization, we consider the following loss:
\begin{equation} \label{eq:genonetimeloss}
    \begin{split}
    \calL^{\text{OneTime}}&\left(h, r, x, y\right) \\
        &= \sum_{j \in \calJ} \left(l(y, \widehat{y}_{< j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) = j} 
    \end{split}
\end{equation}
The $\calL^{\text{OneTime}}$-risk is denoted by $\calR_{\text{OneTime}}(h, r) = \mathop{\mathbbm{E}}_{(x, y) \sim \calP_{X, Y}}[\calL^{\text{OneTime}}(h, r, x, y)]$.
% The corresponding surrogate will take on a similar form as \Cref{eq:deferralpoint} - $\calL^{\psi}_{\text{mod}}\left(h, r, x, y\right) =  \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{< j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\psi(\g(x), j)$. The analysis of consistency and generalization of $\calL^{\psi}_{\text{mod}}$ follow from that of $\calL^{\psi}$.

\subsection{Surrogate loss}
% Since the direct minimization of $\calL^{\text{Point}}\left(h, r, x, y\right)$ is very difficult due to the non-convex and discontinuous nature of the function, a surrogate loss function, which upon minimization will give the same result of minimizing the original loss $\calL^{\text{Point}}$, is desired. 

Let $\g$ consist of $|\calJ|$ scoring functions $g_j: \calX \to \R$ where $j$ indexes the token positions in a sequence and let $r(x) = \argmax_{j \in \calJ} g_j(x)$. We propose the following surrogate loss function:
\begin{align*}
    \calL^{\psi}&\left(h, r, x, y\right) \\
    &=  \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{< j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\psi(\g(x), j)
\end{align*}

where $\psi(\g(x), j)$ is a consistent surrogate for the multiclass 0-1 loss or $\mathbbm{1}_{r(x) \neq j}$, and $c_{\text{max}} = \max_{j \in \calJ} l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y)$. Examples of $\psi$ include cross entropy loss, i.e. $\psi_{\text{ce}}(\g(x), j) = -\log \frac{e^{g_j(x)}}{\sum_{k \in \calJ} e^{g_k(x)}}$ or mean absolute error, i.e. $\psi_{\text{mae}}(\g(x), j) = 1 -  \frac{e^{g_j(x)}}{\sum_{k \in \calJ} e^{g_k(x)}}$ \cite{ghosh2017robust}. The $\calL^{\psi}$-risk is characterized by $\calR_{\psi}(h, r) = \mathop{\mathbbm{E}}_{(x, y) \sim \calP_{X, Y}}[\calL^{\psi}(h, r, x, y)]$.

Intuitively, for a given token position $j$, $c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) $ is large when the subsequence up till $j$ can be predicted correctly and the cost of deferring on the rest of the sequence is small. When this term is large, we desire $\psi(\g(x), j)$ to be small, incentivizing the model to choose $j$ as the deferral point to minimize the surrogate loss. 
% a small $\psi(\g(x), j)$ is equivalent to the rejector assigning a high $g_j(x)$, encouraging the model to pick the $j^{\text{th}}$ time step as the deferral point. 
This surrogate loss encourages the rejector to pick a token position which also has minimal costs for deferring on the rest of the sequence - a goal the $\calL^{\text{OneTime}}$ loss function shares. 

% While, on the surface, the optimization of $\calL^{\text{point}}$ and $\calL^{\text{point}}_{\psi}$ seem to result in rejectors with aligned interests, a mathematical characterization of this relationship via \textit{Bayes consistency} is necessary. When a surrogate is bayes consistent, the minimization of the expected values of surrogate function will result in the same minimizer of the expectation of original deferral loss. Formally, \textit{$\calF$ - consistency} is defined as follows:

% \begin{definition}[$\calF$ - Consistency]\label{th:defconsistency}
%     Let the risk of a hypothesis $f$ with respect to a loss $l$ be $\calR_{l}(f) = \Ex_{(x, y) \in \calP}[l(f(x), y)]$. Let the optimal risk with respect to $l$ out of all hypotheses in $\calF$ be $\calR_{l}^*(\calF) = \inf_{f \in \calF} \calR_{l}(f)$. A surrogate loss function, $\phi(\cdot)$, is said to be $\calF$-consistent with respect to $l$ if for any $f_n \in \calF$
%     \begin{align*}
%         \calR_{\phi}(f_n) - \calR_{\phi}^*(\calF) \xrightarrow{n \to \infty} 0 \ \Longrightarrow \ \calR_{l}(f_n) - \calR_{l}^*(\calF) \xrightarrow{n \to \infty} 0
%     \end{align*}
% \end{definition}
% When $\calF$ is the class of all measurable functions, bayes consistency is achieved. 
% To show that $\calL^{\text{OneTime}}_{\psi}$ is consistent with respect to $r$, we relate the excess risk using $\calL^{\text{OneTime}}$ and the excess risk using $\calL^{\psi}$ in \Cref{th:onetimeconsistencybound}. The complete proof can be found in % \Cref{sec:proofupbound}.

% \begin{lemma} \label{th:onetimeconsistencybound}
%     Let $\Tilde{c}_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$ and $l(y, \wh y^h_j) \leq \bar{l} < \infty$ for all $j$ and $\psi$ be a multiclass surrogate loss that satisfies the following inequality for any distribution over $\calX \times \{1, ..., L + 1\}$ and any measurable function $f$:
%     \begin{align*}
%         \calR_{\text{multi }0-1}(f) - \calR^{*}_{\text{multi }0-1} \leq \Gamma\left(\calR_{\text{multi }\psi}(f) - \calR^{ *}_{\text{multi }\psi}\right)
%     \end{align*}
%     where $\calR_{\text{multi }0-1}$ is the multiclass 0-1 risk and $\Gamma : \R^+ \to \R^+$ is a non-decreasing concave function. If there exists $i, j$ such that $\left|l(y, \wh y^h_i) + \Tilde{c}_i(x, \wh y_{<i}, y) - l(y, \wh y^h_j) - \Tilde{c}_i(x, \wh y_{<i}, y)\right| > \Delta > 0$, then for any $\g$ over all measurable functions and for any $h$, the following inequality holds for any distribution over $\calX \times \calY$:
%     \begin{align*}
%         \calR_{\text{OneTime}}\left(r, h\right) - \calR_{\text{OneTime}}^*(h) \leq \Tilde{\Gamma}\left(\calR_{\psi}\left(r, h\right) - \calR^*_{\psi}(h)\right)
%     \end{align*}
%     where $\Tilde{\Gamma}(z) = L\left(\bar{C} + \bar{l}\right)\Gamma\left(\frac{z}{\Delta}\right)$, $\calR^*_{\text{OneTime}}(h) = \inf_{r \in \calM} \calR_{\text{OneTime}}\left(r, h\right)$, and $\calR^*_{\psi}(h) = \inf_{r} \calR_{\phi}\left(r, h\right)$
% \end{lemma}

Just like $\calL^{\phi}$, $\calL^{\psi}$ is also convex and upper bounds $\calL^{\text{OneTime}}$ up to some scale $\gamma$ (See \Cref{sec:proofonetimeconvexupper} for proof). \Cref{th:onetimeconsistency} shows that $\calL^{\psi}$ achieves Bayes consistency.

\begin{theorem}[Consistency]\label{th:onetimeconsistency}
    Let $\Tilde{c}_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$, $l(y, \wh y^h_j) \leq \bar{l} < \infty$ for all $j \in \calJ$, and $\psi$ be cross entropy loss $\psi_{\text{ce}}$ or mean absolute error $\psi_{\text{mae}}$. If there exists $i, j \in \calJ$ such that $\left|l(y, \wh y^h_i) + \Tilde{c}_i(x, \wh y_{<i}, y) - l(y, \wh y^h_j) - \Tilde{c}_i(x, \wh y_{<i}, y)\right| > \Delta > 0$, then, for a fixed $h$, $\calL^{\psi}$ is a consistent surrogate for $\calL^{\text{OneTime}}$.
\end{theorem}

The proof of the above theorem can be found in \Cref{sec:proofonetimeconsistency}. If the $l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y)$ terms are equal for all $j$, the weights would be $0$. By assuming a non-zero cost difference for at least one pair, we are ensuring that at least one $j$ has a non-zero weight. Such an assumption can be satisfied if there is a sufficient separation in predictive capabilities between the predictor and the expert.

\subsection{Generalization Bounds} \label{sec:genboundspoint}

Under the empirical risk minimization framework, 
the optimal rejector $\wh r_{n} \in \argmin_{r \in \calF} \wh \calR_{\psi}(r)$ minimizes the empirical risk with respect to $\calL^{\psi}$ or $\wh \calR_{\psi}(r) = \frac{1}{n}\sum_{(x^i, y^i) \in \calD} \calL^{\psi}(h, r, x^i, y^i)$ from a finite hypothesis class $\calF$. \Cref{th:onetimegenbounds} presents generalization bounds for $\calL^{\psi}$ (See \Cref{sec:proofonetimegenbounds} for the proof).


\begin{theorem} \label{th:onetimegenbounds}
    Suppose $r_{n} \in \calF = \{x \to \argmin_{j \in \calJ} g_j(x): g_j \in \calG_j\}$ where $\calG_j$ consists of functions having a range of $(-M, M)$ with $M \geq 0$, $\Tilde{c}_j(x, \wh y_{<j}, y) \leq \bar{C}$, and $l(y, \wh y^h_j) \leq \bar{l} < \infty$. If $\psi$ is $\rho$-lipschitz continuous with respect to $\g(x)$ and there exists $u_{\psi}(M)$ such that $\psi\left(\g(x), j\right) \leq u_{\psi}(M) < \infty$ for all $j$ and $r \in \calF$, then with probability $1 - \delta$ for a fixed $h$, the following upper bound holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL_{\psi}^{\text{Point}}$:
    \begin{equation*}
        \begin{split}
            \calR_{\psi}&\left(\wh r_{n}, h\right) - \calR^*_{\psi}\left(\calF, h\right) \\
        &\leq 2\sqrt{2}q(L)\rho\sum_{j \in \calJ}\mathfrak{R}_{n}\left(\calG_j\right) + u_{\psi}(M) q(L)\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}
        \end{split}
    \end{equation*}
    where $\calR^*_{\psi}\left(\calF, h\right) = \inf_{r \in \calF} \calR_{\psi}\left(r, h\right)$, $q(L) =2(|\calJ| - 1)(\bar{C} + \bar{l})$, and $\mathfrak{R}_{n}\left(\calG_j\right)$ is the Rademacher Complexity of $\calG_j$.
\end{theorem}
% From \Cref{th:onetimegenbounds}, we can see that the estimation error of using $\wh r_{n}$ with respect to the surrogate loss $\calL^{\psi}$ depends on the Rademacher complexity $\mathfrak{R}_{n}\left(\calG_j\right)$ of the hypothesis class.
Restricting the hypothesis class to have a range $(-M, M)$ practically involves ReLU activation and softmax clipping to prevent the scores $\g$ from approaching $-\infty$ or $\infty$. For $\psi_{\text{ce}}$, we can expect $u_{\psi_{\text{ce}}}(M) = -\log\frac{e^{-M}}{e^{-M} + Le^M}$ and for $\psi_{\text{mae}}$, it is $u_{\psi_{\text{mae}}}(M) =1 -\frac{e^{-M}}{e^{-M} + Le^M}$. 
% \Cref{th:onetimegenbounds} alone doesn't describe the excess risk with respect to the original token loss $\calL^{\text{Token}}$.
% Using \Cref{th:onetimeconsistencybound},
We can further express the upper bound of the excess risk respect to the original token loss $\calL^{\text{OneTime}}$. The characterization of the upper bound and the proof details can be found in \Cref{sec:proofonetimegenboundreal}.



% \begin{corollary} \label{th:genboundrealpoint}
%      Suppose $r_{n_c} \in \calF = \{x \to \argmin_{j \in [0, L]} g_j(x): g_j \in \calG_j\}$ where $\calG_j$ consists of functions having a range of $(-M, M)$ with $M \geq 0$ and $0 \leq |c_j(x)| \leq \bar{C}$. If $\psi$ is $\psi_{\text{log}}$ or $\psi_{\text{mae}}$, then with probability $1 - \delta$, the excess risk of with respect to $\calL^{\text{Point}}$ holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL_{\psi}^{\text{Point}}$
%     \begin{align*}
%         &\calR_{\text{Point}}\left(\wh r_{n}\right) - \calR^*_{\text{Point}} \\
%         &\leq \Tilde{\Gamma}\left(4q(L)\rho\sqrt{2}\sum_{j = 0}^L\mathfrak{R}_{n_c}\left(\calG_j\right) + u_{\psi}(M) q(L)\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n_c}} + \calA_{\psi}\left(\calF\right)\right)
%     \end{align*}
%     where $\Tilde{\Gamma}: \R^+ \to \R^+$, $q(L) = (L +1 )^2$, $\rho$ is the lipschitz norm of $\psi$, and $\calA_{\psi}\left(\calF\right) = \calR^*_{\psi}\left(\calF\right) -  \calR^*_{\psi}$ is the approximation error.
% \end{corollary}





\section{Related Works} \label{sec:relworks}

\textit{Model Cascades} \cite{viola2001rapid} share similarities with the learning to defer framework. In cascades, an inference task is progressively handled by a series of models where easier tasks are completed by earlier models in the sequence and complex cases escalate to subsequent models. Similar to confidence-based L2D methods described in \Cref{sec:l2d}, cascading decisions are typically determined by a thresholding deferral rule \cite{yue2023large}. Some recent work on applying cascades to large language models \cite{gupta2024language,wang2024cascade} has also explored modeling the cascade decision maker. However, model cascades, just like learning to defer methods, ultimately rely on a single model for each prediction. Our work extends cascades by introducing partial deferrals, allowing two models to collaborate on a prediction. Narasimhan et al. \cite{narasimhan2024faster} also proposes a token-level deferral rule, but their deferral rule compares the predictor's confidence with expert's confidence at every token position, which requires many expensive expert queries. Our work learns the expert costs at training time to avoid such expert queries unless the rejector calls for it.

\textit{Query Routing} methods select a model that provide the highest quality prediction from an ensemble of models. Some query routing algorithms in large language models \cite{jiang2023llm,chen2023frugalgpt} often require querying either a part or the entire model ensemble to determine the best generation. Some methods train query routing models using specially designed loss functions \cite{shnitzer2023large,lu2023routing,ding2024hybrid}; these loss functions, however, lack consistency guarantees. Our work not only presents surrogate losses with stronger statistical guarantees but also offers more granular routing decisions.
% The one-time deferral is most closely aligned with the \textit{learning to defer with multiple experts} \cite{verma2023learning,mao2024regression}. In this framework, the rejector must decide whether to defer and to which expert to defer. One-time deferral can be viewed as a special case of multiple-expert deferral where each ``expert'' mixes subsequences of different sizes from the predictor with the expert's prediction.  Our contribution lies in expanding tools from multi-expert deferral literature to partial rejections.

Many L2D methods draw inspiration from the \textit{Cost-Sensitive learning} framework. In the example-dependent cost-sensitive setting, each feature $x$ is paired with a cost vector. A cost-sensitive classifier must then learn to identify the index of the cost vector that minimizes the expected cost. Classifiers \cite{abe2004iterative,lin2015reduction} are often trained with a weighted logistic loss or weighted cross entropy loss function, similar to $\calL^{\text{Token}}$ and $\calL^{\text{OneTime}}$. However, popular methods \cite{tu2010one,chung2015cost} use losses that are inconsistent with the original cost-sensitive loss.
% L2D for $E$ experts seeks to minimize the following loss function:
% \begin{equation} \label{eq:l2dmultiple}
%     \begin{split}
%             &l_{\text{L2DMulti}}\left(h, r, x, y, \{m_e\}_{e = 1}^{E} \right) \\
%             &=l(x, y, h(x))\mathbbm{1}_{r(x) = 0} + \sum_{e = 1}^{E}l_{\exp}(x, y, m_e)\mathbbm{1}_{r(x) = e}
%     \end{split}
% \end{equation}
% where $m_e$ is the expert prediction from expert $e$. The rejector in this scenario now must decide whether to defer and to which expert to defer to.

\section{Experiments} 

In this section, we demonstrate the quality of a model-based approach to partial deferral in two different experimental setups - News Summarization and Traveling Salesman Problem solver. Code will be made available upon acceptance.

\subsection{Baselines}

In experiments, we wish to highlight two aspects of the proposed methodology. The first is the improved cost-accuracy trade-offs in using partial deferrals over whole-sequence deferrals. The second is the advantages of our specific approach to partial deferrals over confidence-based partial deferrals, i.e. thresholding confidence scores from the predictor.   

Towards this end, we conduct two experiments. In the first, we compare our token-level and one-time rejectors with a few whole deferral methods. \texttt{ChowSum}, \texttt{ChowMean}, \texttt{ChowQuantile} are thresholding whole-sequence rejectors of different functions of the negative log probability of the generated sequence. 
% Both \texttt{WholeModelScore} and
\texttt{WholeModelEmbed} is a model-based whole-sequence rejector trained to predict whether the expert is more accurate than the predictor \cite{gupta2024language}. 

For the second experiment, our model-based token-level rejector is compared against two confidence-based deferral methods - \texttt{TokenwiseSoftmax} and \texttt{TokenwiseEntropy}. \texttt{TokenwiseSoftmax} rejector defers prediction when the negative log softmax probability of the greedy token exceeds a threshold, and \texttt{TokenwiseEntropy} makes rejection decisions by thresholding the entropy of the predictive distribution. These methods are similar to the confidence-based methods described in \Cref{sec:l2d}.

In the case of one-time deferral, we similarly consider \texttt{OneTimeSoftmax} and \texttt{OneTimeEntropy} as comparative baselines for the one-time deferral method, which choose the token position based on the lowest softmax probability assigned to the greedy token and the highest entropy, respectively, from a set of tokens, $\calJ$. Complete descriptions of all baselines can be found in \Cref{sec:baselinedetails}.

\subsection{Tasks}

We refer the reader to \Cref{sec:trainingdetails} for training and hyperparameter details of the rejector models.
% \ambuj{tasks are presented in the XSUM followed by TSP order but the experimental figures use TSP followed by XSUM order. Might be better to keep it consistent}

\textbf{Traveling Salesman Problem (TSP)}: The traveling salesman problem is one of the most widely-known NP-Hard 
optimization problems which requires finding the shortest route to visit every city exactly once and return to the starting city. While no algorithm can efficiently recover the exact solution to this problem, approximate solutions have proven to be useful practically. 
% While there exists no efficient algorithm to recover the exact solution to this problem, approximate solutions have proven to be useful practically. 
These methods include the optimal and time-consuming optimization-based surrogates and efficient, yet less optimal ML-based approaches. We, therefore, use our partial deferral method to facilitate collaborations between both approaches.

% Solutions generated by a Gurobi solver are often more optimal but time-consuming to produce with larger graphs. On the other hand, machine learning solutions may sacrifice some optimality but they are much faster. 

In this task, the ML predictor is the PointerNet model of \cite{vinyals2015pointer} trained on randomly generated TSP graphs with $50$ cities (coordinates are drawn from a standard normal distribution), and the optimization-based expert is a Gurobi solver that takes in the partial solution from the predictor as a constraint. The predicted tours are evaluated by the total distance covered by the tour. Since Gurobi solvers do not solve TSP problems in an autoregressive fashion, producing a single city and completing the whole tour will incur similar costs, making the token-level deferral framework unsuitable in this scenario. This, however, highlights a perfect use-case for one-time deferral. $l(y, \widehat{y}_{< j})$ evaluates the total distance of using the subtour $\wh y_{< j}$ and $\Tilde{c}_j(x, \widehat{y}_{<j}, y)$ is the sum of the total distance of using the expert-generated subtour or $\wh y^e_{\geq j}$ and a cost of $\alpha_j$ for deferral that increases monotonically with the number of rejected tokens. More details on $\alpha_j$ and its effects on performance can be found in \Cref{sec:alphaj}.

\textbf{Extreme Summarization (XSUM)}: In \textit{Extreme Summarization} \cite{narayan2018don}, the predictor summarizes long articles in a few sentences. In this case, the predictor is a \texttt{t5-small} model and the expert \texttt{t5-base} model with both models finetuned on the XSUM dataset. All the generated summaries are limited to a token length of $20$ and evaluated using ROUGE1 scores. 

For token-level deferral, we set the predictor loss to be $l(y, \wh y^h_j) = \mathbbm{1}_{\wh y^h_j \notin y}$ and the expert cost to be $c_j(x, \wh y_{<j}, y) = \mathbbm{1}_{ e(x,\wh y_{<j})  \notin y}$. Intuitively, whether a predicted label belongs to the true summary impacts the ROUGE1 score. For one-time deferral, the $1 - \text{ROUGE1}$ score of the expert's prediction starting from the $j^{\text{th}}$ token onwards appended to $\wh y_{<j}$ from the predictor was measured. The term $l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y)$ is jointly evaluated as the sum of this calculated $1 - \text{ROUGE1}$ score and a query cost of $\alpha_j$, similar to the one in the TSP experiments.


\subsection{Evaluation}

Across all prediction tasks, we evaluate both our model-based token-level deferral and one-time deferral methods using \textit{deferral curves} that plot the loss of the system against number of deferred tokens using various thresholds of the rejector score $r(x)$. In the TSP setup, loss is defined as the percentage increase in tour length relative to the expert-generated tour, while in the in XSUM setup, the loss is given by $1 - \text{ROUGE1}$. The area under the deferral curve (AUDC) represents how efficiently a rejector balances the tradeoff between cost and predictive quality. We also assess the percentage improvement in AUDC relative to a random rejector, which follows a linear cost-quality tradeoff. Thresholds, however, do not directly determine rejection decisions for the one-time deferral scenario, making deferral curves challenging to construct. In light of this, 
the deferral policy is modified such that if $g_{L + 1}(x)$ exceeds a threshold, the predictor performs the full prediction; otherwise, the score maximizer becomes the deferral point. All results are averaged over $5$ runs with each run containing $100$ test samples.

% Since thresholds do not determine rejection decision for the one-time deferral scenario, deferral curves are much more challenging to create and so, we evaluate the model-based one-time rejector using accuracy and regret compared against the true one-time deferral rule - $\argmin_{j \in [1, L + 1]}l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y)$. We also measure the rejection rate (ratio between number of deferred labels and total number of labels).



% \subsection{Extreme Summarization}

% The first task is \textit{Extreme Summarization} \cite{narayan2018don} where the predictor is required to summarize long articles in a few sentences. In this case, the predictor is a \texttt{t5-small} model and the expert is \texttt{t5-base} model with both models finetuned on the XSUM dataset. All the generated summaries are evaluated using ROUGE1 scores.



% Our model-based token-level rejector is compared against two baselines - \texttt{SoftmaxThresh} and \texttt{EntropyThresh}. \texttt{SoftmaxThresh} rejector defers prediction when the negative log softmax probability of the greedy token exceeds a threshold and \texttt{EntropyThresh} makes rejection decisions by thresholding the entropy of the predictive distribution. These methods are similar to the confidence-based methods described in \Cref{sec:l2d}.

% \subsection{Traveling Salesman Problem Solver}


% \subsection{Time Series: Weather Prediction}


\subsection{Comparison with other deferral methods} \Cref{fig:newssummdeferralcuurve} and \Cref{tab:auctokenlevel} show that both model-based token-level deferral and model-based one-time deferral schemes outperform model-based and confidence-based whole deferral schemes with token-level deferral achieving the best performance.  Our model-based one-time method also prevails over its counterpart methods and the same holds true for the model-based token-level deferral method. Notably, confidence-based token-level deferral schemes also surpass all other methods in XSUM, demonstrating that token-level deferrals generally offer a superior cost-accuracy tradeoff than whole deferrals. Although model-based one-time deferral exhibits marginal improvement in cost-effectiveness in TSP task, we attribute this to the expert's lack of ability to predict smaller portions of the sequence at a time. The better cost-accuracy tradeoffs seen in XSUM experiments demonstrate that the advantages of partial deferrals become more prominent with more fine-grained deferrals, which are only possible with access to a suitable expert.
\begin{table*}[h]
\caption{Table showing Area under Deferral curve (AUDC) and percentage improvement in AUDC relative to the random rejector for our methods along with the baselines, random, and optimal rejectors. The random rejector makes rejection decisions by flipping a coin with varying probability of landing heads and the optimal rejector achieves the highest possible accuracy regardless of the number of tokens rejected. Numbers in brackets represent the standard deviations measured across the $5$ runs. Low AUDC signifies better cost-accuracy tradeoffs and a positive improvement over a random rejector indicates a shift towards a superlinear tradeoff. The values in bold represent the lowest AUDC and highest percentage improvement for each category (Whole, One-time, Token-level). OneTimeModel scores are reported using the best-performing model trained with $\calL^{\psi}$. See \Cref{fig:aucvsnumclass} for more details on which performed best.}
\label{tab:auctokenlevel}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{|c|c|c|c|c|}
    \toprule
     & \multicolumn{2}{|c|}{TSP} & \multicolumn{2}{|c|}{XSUM} \\
     \midrule
    Method & AUDC & \% Improvement   & AUDC & \% Improvement \\
    \midrule
    Random & 306.12(6.31) & 0.00 (0.00) & 1433.55 (36.98) & 0.00 (0.00) \\
    Optimal & 0.00(0.00) & 100.00 (0.00) & 1212.00 (46.85) & 15.48 (1.50)\\
    \midrule
    \multicolumn{5}{|c|}{Whole Deferrals} \\
    \midrule
    ChowMean &  \textbf{287.01(7.73)} & \textbf{6.23(2.14)} & 1430.52 (19.09) & 0.13 (3.48)\\
    ChowSum  &  287.01(7.73) & 6.23(2.14) & 1431.41 (19.65) & 0.06 (3.50)\\
    ChowQuantile 0 &  301.18(9.40) & 1.63(1.18) & 1432.28 (25.89) & -0.01 (3.89)\\
    ChowQuantile 0.4 &  287.74(5.91) & 6.00(1.16) & 1436.10 (24.23) & -0.27 (3.80)\\
    ChowQuantile 0.8 &  288.86(9.35) & 5.65(2.12) & 1434.48 (19.32) &-0.15 (3.40)\\
    ChowQuantile 1.0 &  295.71(6.12) & 3.39(1.42) &  \textbf{1427.27 (18.91)} & 0.35 (3.50)\\
    % WholeModelScore &  - & 1433.82 (25.91) \\
    WholeModelEmbed &  307.88(8.73) & -0.58(2.02) & 1434.23 (21.57) & -0.11 (3.81)\\
    \midrule
    \multicolumn{5}{|c|}{One-time Deferrals} \\
    \midrule
    OneTimeModel &  \textbf{270.92(10.67)} & \textbf{11.52(2.41)} & \textbf{1398.39(22.84)} & \textbf{2.35 (4.03)}\\
    OneTimeSoftmax & 319.79(14.53) & -4.44(3.63) & 1433.41(26.82) & -1.50 (2.13)\\
    OneTimeEntropy &  324.23(13.48) & -5.93(4.15) & 1442.92(24.92) & -1.75 (2.38)\\
    \midrule
    \multicolumn{5}{|c|}{Token-level Deferrals} \\
    \midrule
    TokenwiseModel &  - & -&\textbf{1323.92 (25.34)} & \textbf{7.63 (0.93)}\\
    TokenwiseSoftmax &  - & - & 1354.78 (31.87) & 5.48 (0.97)\\
    TokenwiseEntropy &  - & - & 1360.41 (31.50) & 5.09 (0.63)\\
    \bottomrule
\end{tabular}
% \begin{tabular}{|c|c|c|}
%     \toprule
%     Method & TSP & XSUM \\
%     \midrule
%     Random & 30091.81(103.78) & 1433.55 (36.98) \\
%     Optimal & 28357.36(68.82) & 1212.00 (46.85) \\
%     \midrule
%     \multicolumn{3}{|c|}{Whole Deferrals} \\
%     \midrule
%     ChowMean &  \textbf{29995.11(116)} & 1430.52 (19.09) \\
%     ChowSum  &  29995.11(116) & 1431.41 (19.65) \\
%     ChowQuantile 0 &  30063.67(116) & 1432.28 (25.89) \\
%     ChowQuantile 0.4 &  29996.49(101) & 1436.10 (24.23) \\
%     ChowQuantile 0.8 &  30003.67(126) & 1434.48 (19.32) \\
%     ChowQuantile 1.0 &  30041.40(102) & \textbf{1427.27 (18.91)} \\
%     WholeModelScore &  - & 1433.82 (25.91) \\
%     WholeModelEmbed &  30102.59(103) & 1434.23 (21.57) \\
%     \midrule
%     \multicolumn{3}{|c|}{One-time Deferrals} \\
%     \midrule
%     OneTimeModel &  \textbf{29896.67(131)} & \textbf{1398.39(22.84)} \\
%     OneTimeSoftmax &  30190.42(112) & 1433.41(26.82) \\
%     OneTimeEntropy &  30167.16(104) & 1442.92(24.92) \\
%     \midrule
%     \multicolumn{3}{|c|}{Token-level Deferrals} \\
%     \midrule
%     TokenwiseModel &  - & \textbf{1323.92 (25.34)} \\
%     TokenwiseSoftmax &  - & 1354.78 (31.87) \\
%     TokenwiseEntropy &  - & 1360.41 (31.50)   \\
%     \bottomrule
% \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{imgs/experiment1_again.png}}
\caption{Cost-Loss plots for TSP (on the left) and XSUM (on the right) tasks. 
% The plot on the left measures the percentage increase in tour length from the expert-generated tour for different rejected token budgets for all deferral methods. The plot on the right shows the effects of costs on 1 - ROUGE score across all models. 
Deferral curves closer to the lower left corner are more desirable.}
% \caption{Cost-Loss plots for TSP (on the left) and XSUM (on the right) tasks. The plot on the left measures the achieved average distance for different rejected token budgets for all the deferral methods. The plot on the right shows the effects of costs on 1 - ROUGE score across all models. Deferral curves closer to the lower left corner are more desirable.}
\label{fig:newssummdeferralcuurve}
\end{center}
\vskip -0.2in
\end{figure}
% \ambuj{the TSP y-axis range might need some explanation. why is it from -6.3 to -5.7? Are their units for the distances? also the XSUM y-axis says ROUGE-1. Should it be 1-ROUGE instead? finalluy, we need to do smth about the limited improvement in AUDC in the TSP plot. we could perhaps interpret the imporvement, even if small, in some meaninfgul terms. also perhaps we can point out that one-time deferral is inherently harder which explains that we see more improvements in the XSUM plots}

% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{imgs/Deferralcurveallbaselinestsp.png}}
% \caption{Deferral Curve for Tokenwise Rejector on News Summarization data with $100$ test samples}
% \label{fig:tspdeferralcuurve}
% \end{center}
% \vskip -0.2in
% \end{figure}

\subsection{Size of $\calJ$ for One time Deferral}

We train one-time rejectors using $\calL^{\psi}$ with varying choices of $\calJ$ to study the influence of larger sets of deferral point choices on cost-effectiveness. For both XSUM and TSP, $\calJ$ represents a grid of deferral points between $1$ and $L + 1$ and we vary the granularity of the grid. \Cref{fig:aucvsnumclass} demonstrates the need for a modified one-time deferral loss function. The model trained with a $\calJ$ including all token positions ($|\calJ| = L + 1$) is not the most optimal choice with the suboptimality exacerbated in the XSUM experiments. One-time rejectors are generally more cost-effective than its corresponding confidence-based rejectors regardless of the size of $\calJ$.
% e present a description of these rejectors in \Cref{sec:baselinedetails}).

\begin{figure}[!ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{imgs/experiments2_improvement.png}}
\caption{Plots of Size of $\calJ$ against percentage improvement in AUDC relative to the random baseline for TSP (on the left) and XSUM (on the right). See \Cref{sec:numexperts} for exact AUDC values with standard deviations.}
\label{fig:aucvsnumclass}
\end{center}
\vskip -0.2in
\end{figure}
% \ambuj{I think I get the high level point here: you are trying to argue that the the default one-time deferral surrogate is not going to work well for all granularities at which deferrals could be made. But you proposed a fix at the end of section 4.3. so the reader will wonder: why demonstrate a problem when you have a fix? why not demonstrate the fix instead?}
% \ambuj{some more questions about fig 2: what exactly is the reader supposed to take away from the trnds? the x-axis is number of classes in the plots but we talk abotu granularity of grid in the text. also I understand that TSP plots had -ve values on the y-axis. But the area should still be +ve. How is the AUDC -ve for TSP plots?}
% \ambuj{Is Table 1 going to be part of main paper or the appendix? if the former, you prolly want to add some text about it? For me persoanlly, it is hard to interpret the actual numbers in an AUDC table. maybe it'll be helpful to provide optimal and some obvious baselines (random rejector) AUDCs for setting up some expectations about the numerical values}
% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{imgs/AUCvsNumClassestsp.png}}
% \caption{Deferral Curve for Tokenwise Rejector on News Summarization data with $100$ test samples}
% \label{fig:aucvsnumclasstsp}
% \end{center}
% \vskip -0.2in
% \end{figure}



\section{Discussion}

We have introduced partial rejections as a way to improve the cost-effectiveness of a learning to defer system and proposed two model-based methods to support granular rejections. This work raises numerous interesting questions. We have currently proposed partial deferral with access to one expert, however, it would be interesting to extend this to the case with multiple experts. 
% This problem would require a new loss function along with a complex model architecture to support these capabilities. 
It would also be valuable to explore other types of partial deferrals with granularities between token-level and one-time deferrals.
%with requiring online experts. 
A rejector, for example, could select contiguous parts of a sequence to reject. Finally, studying partial rejections for larger outputs like protein structures would also be insightful.



















% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.



% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.
% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\newpage
\bibliographystyle{unsrt}  
\bibliography{mybib}
% \bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

\section{Proofs for \Cref{sec:tokendeferral}} \label{sec:tokendeferralproof}

\subsection{Proof of \Cref{th:convexupperbound}} \label{sec:proofconvexupper}

\begin{proposition} \label{th:convexupperbound}
    If $\phi$ is binary classification calibrated and convex, $\calL^{\phi}(h, r, x, y)$ is a convex upper bound of $\calL^{\text{Token}}(h, r, x, y)$ upto some scale $\gamma$
\end{proposition}
\begin{proof}

It is easy to see that $\calL^{\phi}_{j}(h, r, x, y)$ is convex with respect to $r$ as long as $\phi(\cdot)$ is convex. Since $\phi$ is binary classification calibrated, there is a constant $\gamma$ such that $\gamma\phi(z) \geq \mathbbm{1}_{z \leq 0}$ by Lemma 3 from \cite{bartlett2006convexity}. Therefore, 
$\gamma\calL^{\phi}(h, r, x, y) \geq \calL^{\text{Token}}(h, r, x, y)$.

\end{proof}

\subsection{Proof of \Cref{th:upbound}} \label{sec:proofupbound}

\begin{lemma}
    Let $0 < \bar{c} \leq c_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$ for all $j$, $0 \leq l(y, \wh y^h_j) \leq \bar{l} < \infty$, and $\phi$ be a binary surrogate loss that satisfies the following inequality for any distribution over $\calX \times \{-1, 1\}$ and any measurable function $f$:
    \begin{align*}
        \calR_{\text{binary }0-1}(f) - \calR^{*}_{\text{binary }0-1} \leq \Gamma\left(\calR_{\text{binary }\phi}(f) - \calR^{ *}_{\text{binary }\phi}\right)
    \end{align*}
    where $\calR_{\text{binary }0-1}$ is the binary 0-1 risk and $\Gamma : \R^+ \to \R^+$ is a non-decreasing concave function.
    Then, for any measurable $r$ and any $h$, the following inequality holds for any distribution over $\calX \times \calY$:
    \begin{align*}
        \calR_{\text{Token}}\left(r, h\right) - \calR_{\text{Token}}^*(h) \leq \Tilde{\Gamma}\left(\calR_{\phi}\left(r, h\right) - \calR^*_{\phi}(h)\right)
    \end{align*}
    where $\Tilde{\Gamma}(z) = \left(\bar{l} + \Bar{C}\right)\Gamma\left(\frac{z}{\bar{c}}\right)$, $\calR^*_{\text{Token}}(h) := \inf_{r} \calR_{\text{Token}}\left(r, h\right)$, and $\calR^*_{\phi}(h) := \inf_{r} \calR_{\phi}\left(r, h\right)$
\end{lemma}
\begin{proof}
    Let $C_{\text{Token}}(r, h, x) = \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\calL^{\text{Token}}\left(h, r, x, y\right)\right]$ be the pointwise $\calL^{\text{Token}}$- risk for a given $x$ and predictor $h$ and let $C_{\text{Token}}^*(h, x) = \inf_{r} C_{\text{Token}}(r, h, x)$ be the optimal pointwise $\calL^{\text{Token}}$- risk for a given $x$ and predictor $h$. Similarly, let $C_{\phi}(r, h, x) = \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\calL^{\phi}\left(h, r, x, y\right)\right]$ be the pointwise $\calL^{\phi}$- risk for a given $x$ and predictor $h$ and $C_{\phi}^*(h, x) = \inf_{r} C_{\phi}(r, h, x)$ be the optimal pointwise $\calL^{\phi}$- risk. For any measurable function $r$, the excess pointwise $\calL^{\phi}$- risk can be characterized by:
    \begin{align*}
        &C_{\text{Token}}(r, h, x) - C_{\text{Token}}^*(h, x) \\
        &= \frac{1}{L}\sum_{j = 1}^{L} \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[l\left(y, \wh y_j^h\right)\right] \mathbbm{1}_{r(x, \wh y_{<j}) \leq 0} + \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[c_j(x, \wh y_{<j}, y)\right] \mathbbm{1}_{r(x, \wh y_{<j}) > 0} \\
        &\quad - \frac{1}{L} \inf_{\Tilde{r}} \left\{\sum_{j = 1}^{L} \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[l\left(y, \wh y_j^h\right)\right] \mathbbm{1}_{\Tilde{r}(x, \wh y_{<j}) \leq 0} + \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[c_j(x, \wh y_{<j}, y)\right] \mathbbm{1}_{\Tilde{r}(x, \wh y_{<j}) > 0}\right\}
    \end{align*}
    Let $A_j = \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[l(y, \wh y_{<j})\right]$ and $B_j = \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[c_j(x, \wh y_{<j}, y)\right]$
    \begin{align*}
        &C_{\text{Token}}(r, h, x) - C_{\text{Token}}^*(h, x) \\ 
        &= \frac{1}{L}\sum_{j = 1}^{L} A_j \mathbbm{1}_{r_j(x, \wh y_{<j}) \leq 0} + B_j \mathbbm{1}_{r_j(x, \wh y_{<j}) > 0}  - \frac{1}{L} \inf_{\Tilde{r}} \left\{\sum_{j = 1}^{L} A_j \mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) \leq 0} + B_j \mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) > 0}\right\} \\
        &= \frac{1}{L}\sum_{j = 1}^{L} \left(A_j + B_j\right)\left( \frac{A_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) > 0}\right) \\
        & \quad - \frac{1}{L} \inf_{\Tilde{r}} \left\{\sum_{j = 1}^{L} \left(A_j + B_j\right)\left(\frac{A_j}{A_j + B_j} \mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j}\mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) > 0} \right)\right\} \\
        &\leq \frac{\left(\bar{l}  + \Bar{C}\right)}{L}\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) > 0} \\
        & \quad - \frac{\left(\bar{l} + \Bar{C}\right)}{L} \inf_{\Tilde{r}} \left\{\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j}\mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) > 0}\right\} 
    \end{align*}

    Consider a feature space $\Tilde{\calX} = \calX \times \calV^*$ and output space $\Tilde{\calY} = \{-1, 1\}$. Let the distribution on $\Tilde{X}$ be uniform over $\{\Tilde{x}_1,  \cdots, \Tilde{x}_L\}$ where $\Tilde{x}_j = (x, \wh y_{<j})$ with $\frac{1}{L}$ mass on each point on the set. Let $P(\Tilde{Y} = - 1 \mid \Tilde{X} = \Tilde{x}_j) = \frac{A_j}{A_j + B_j}$ and $f$ be a measurable function such that $f(\Tilde{x}_j) = r_j(\Tilde{x}_j) = r_j(x, \wh y_{<j})$
    
    Under this distribution,
    \begin{align*}
        \calR_{\text{binary }0-1}(r) &= \frac{1}{L}\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) > 0} \\
        \calR_{\text{binary }0-1}^* &= \frac{1}{L}\inf_{\Tilde{r}}\left\{\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j} \mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) > 0}\right\} \\
        \calR_{\text{binary }\phi}(r) &= \frac{1}{L}\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \phi\left(r_j(x, \wh y_{<j})\right) + \frac{B_j}{A_j + B_j} \phi\left(-r_j(x, \wh y_{<j})\right) \\
        \calR_{\text{binary }\phi}^* &= \frac{1}{L}\inf_{\Tilde{r}}\left\{\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \phi\left(\Tilde{r}_j(x, \wh y_{<j})\right) + \frac{B_j}{A_j + B_j} \phi\left(-\Tilde{r}_j(x, \wh y_{<j})\right)\right\}
    \end{align*}

    From the theorem statement,
    \begin{align*}
        &C_{\text{Token}}(r, h, x) - C_{\text{Token}}^*(h, x) \\
        &\leq \frac{\left(\bar{l} + \Bar{C}\right)}{L}\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j} \mathbbm{1}_{r_j(x, \wh y_{<j}) > 0} \\
        & \quad - \frac{\left(\bar{l} + \Bar{C}\right)}{L} \inf_{\Tilde{r}} \left\{\sum_{j = 1}^{L} \frac{A_j}{A_j + B_j} \mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) \leq 0} + \frac{B_j}{A_j + B_j}\mathbbm{1}_{\Tilde{r}_j(x, \wh y_{<j}) > 0}\right\} \\
        &\leq \left(\bar{l} + \Bar{C}\right) \Gamma\left(\sum_{j = 1}^{L} \frac{A_j\phi\left(r_j(x, \wh y_{<j})\right) + B_j\phi\left(-r_j(x, \wh y_{<j}) \right)}{L\left(A_j + B_j\right)} - \inf_{\Tilde{r}} \left\{\sum_{j = 1}^{L} \frac{A_j\phi\left(\Tilde{r}_j(x, \wh y_{<j})\right) + B_j\phi\left(-\Tilde{r}_j(x, \wh y_{<j})\right)}{L\left(A_j + B_j\right)}  \right\} \right) \\
        &\leq \left(\bar{l} +\Bar{C}\right) \Gamma\left(\sum_{j = 1}^{L} \frac{A_j\phi\left(r_j(x, \wh y_{<j})\right) + B_j\phi\left(-r_j(x, \wh y_{<j}) \right)}{L\Bar{c}} - \inf_{\Tilde{r}} \left\{\sum_{j = 1}^{L} \frac{A_j\phi\left(\Tilde{r}_j(x, \wh y_{<j})\right) + B_j\phi\left(-\Tilde{r}_j(x, \wh y_{<j})\right)}{L\Bar{c}}  \right\} \right) \\
        &= \left(\bar{l} + \Bar{C}\right)\Gamma\left(\frac{C_{\phi}(r, h, x) - C_{\phi}^*(h, x)}{\Bar{c}}\right)
    \end{align*}

    Let $\Tilde{\Gamma}(z) = \left(\bar{l} + \Bar{C}\right)\Gamma\left(\frac{z}{\Bar{c}}\right)$. Then, 
    \begin{align*}
        \calR_{\text{Token}}\left(r, h\right) - \calR_{\text{Token}}^*(h) &= \mathop{\mathbb{E}}_{x \sim \calP_X}\left[C_{\text{Token}}(r, h, x) - C_{\text{Token}}^*(h, x)\right] \\
        &\leq \mathop{\mathbb{E}}_{x \sim \calP_X}\left[\Tilde{\Gamma}\left(C_{\phi}(r, h, x) - C_{\phi}^*(h, x)\right)\right] \\
        &\overset{(a)}{\leq} \Tilde{\Gamma}\left(\mathop{\mathbb{E}}_{x \sim \calP_X}\left[C_{\phi}(r, h, x) - C_{\phi}^*(h, x)\right]\right) \\
        & \overset{(b)}{=}  \Tilde{\Gamma}\left(\calR_{\phi}\left(r. h\right) - \calR^*_{\phi}(h)\right)
    \end{align*}

    (a) by Jensen's inequality since $\Tilde{\Gamma}$ is concave, (b) since the infimum is taken over all measurable functions $\calR^*_{\phi}(h) = \mathop{\mathbb{E}}_{x \sim \calP_X}\left[C_{\phi}^*(h, x)\right]$

    
\end{proof}

\subsection{Proof of \Cref{th:consistency}} \label{sec:proofconsistency}

\begin{definition}[Classification Calibration]
    Let $\eta(x) = \mathbb{P}\left[Y = 1\mid X = x\right]$ and $\calR_{\phi}(f) = \mathbb{E}_{x \sim \calP_X}\left[C_{\phi}\left(\eta(x), f(x)\right)\right]$ where
    $C_{\phi}(\eta, t) = \eta\phi(t) + (1 - \eta)\phi(-t)$. $\phi$ is considered binary classification calibrated if $C_{\phi}^{-}(\eta) - C^*_{\phi}(\eta) > 0$ for all $\eta \neq \frac{1}{2}$ where $C_{\phi}^{-}(\eta) = \inf_{t: t\left(\eta - \frac{1}{2}\right) \leq 0} C_{\phi}(\eta, t)$ and $C_{\phi}^{-}(\eta) = \inf_{t \in \mathbbm{R}} C_{\phi}(\eta, t)$
\end{definition}

\begin{theorem}[Consistency]
    If $0 < \bar{c} \leq c_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$ for all $j$, $0 \leq l(y, \wh y^h_j) \leq \bar{l} < \infty$, and $\phi$ be binary classification calibrated, then, for a fixed $h$, $\calL^{\phi}$ is a Bayes consistent surrogate for $\calL^{\text{Token}}$
\end{theorem}

\begin{proof}
    Since $\phi$ is binary classification calibrated, we know that
    $\calR_{\text{binary }0-1}(f) - \calR^{*}_{\text{binary }0-1} \leq \psi^{-1}\left(\calR_{\text{binary }\phi}(f) - \calR^{ *}_{\text{binary }\phi}\right)$ where $\psi$ is a non-decreasing convex function, making $\psi^{-1}$ a non-decreasing concave function \cite{bartlett2006convexity}. By \Cref{th:upbound}, $\calR_{\text{Token}}\left(r, h\right) - \calR_{\text{Token}}^*(h) \leq \Tilde{\Gamma}\left(\calR_{\phi}\left(r, h\right) - \calR^*_{\phi}(h)\right)$ where $\Tilde{\Gamma}(z) = \left(1 + \Bar{C}\right)\psi^{-1}\left(\frac{z}{\bar{c}}\right)$. If $\lim_{n \to \infty} \calR_{\phi}\left(r_n, h\right) - \calR^*_{\phi}(h) = 0$, then
    \begin{align*}
        \lim_{n \to \infty} \calR_{\text{Token}}\left(r_n, h\right) - \calR_{\text{Token}}^*(h) &\leq \lim_{n \to \infty} \Tilde{\psi}\left(\calR_{\phi}\left(r_n, h\right) - \calR^*_{\phi}(h)\right) \\
        & \overset{(a)}{=}  \left(1 + \Bar{C}\right)\psi\left(\lim_{n \to \infty} \frac{\calR_{\phi}\left(r_n, h\right) - \calR^*_{\phi}(h)}{\bar{c}}\right) \\
        &= \left(1 + \Bar{C}\right)\psi(0) \\
        & \overset{(b)}{=} 0
    \end{align*}
    (a) by continuity of $\psi$ at $0$ and (b) since $\psi(0) = 0$ \cite{bartlett2006convexity}
\end{proof} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of \Cref{th:genbounds}} \label{sec:proofgenbounds}

\begin{theorem}
    Suppose $r \in \calF$ which is composed of $\{\calF_j\}_{j = 1}^L$ such that $r_j \in \calF_j$, $c_j(x) \leq \bar{C}$, and $l(y, \wh y^h_j) \leq \bar{l} < \infty$. If $\phi$ is $\rho$-lipschitz continuous, then with probability $1 - \delta$, the following upper bound holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL_{\phi}$ for a fixed $h$:
    \begin{equation*}
        \calR_{\phi}\left(\wh r_{n}, h\right) - \calR^*_{\phi}\left(\calF, h\right) \leq \frac{4\rho\sqrt{2\left(\bar{C}^2 + \bar{l}^2\right)}}{L} \sum_{j = 1}^L \mathfrak{R}_n\left(\calF_j\right) + (\bar{C} + \bar{l})\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}
    \end{equation*}
    where $\calR^*_{\phi}\left(\calF, h\right) = \inf_{r \in \calF} \calR_{\phi}\left(r, h\right)$ and $\mathfrak{R}_n\left(\calF_j\right)$ is the Rademacher Complexity of $\calF_j$
\end{theorem}

\begin{proof}

    Let $G_n = \sup_{r \in \calF} \left[\calR_{\phi}\left(r, h\right) - \wh \calR_{\phi}(r, h)\right]$, $G_n' = \sup_{r \in \calF} \left[\wh \calR_{\phi}(r, h) -\calR_{\phi}\left(r, h\right) \right]$, and\\
    $\calB = \{(x, y) \mapsto \calL^{\phi}\left(h, r, x, y\right) \ : r \in \calF \}$. 
    
    If $z^i = (x^i, y^i)$, $G_n = g(z^1, \cdots, z^n)$. Since $\left|g(z^1, \cdots, z^i, \cdots, z^n) - g(z^1, \cdots, z^{i'}, \cdots, z^n)\right| \leq \frac{\bar{C} + \bar{l}}{n}$, by applying McDiarmid's inequality

    \begin{align*}
        P\left(G_n > \frac{\epsilon}{2}\right) &\leq \exp\left\{\frac{-2\left(\frac{\epsilon}{2} -\mathop{\mathbb{E}}\left[G_n\right]\right)^2}{\sum_{i = 1}^n \frac{(\bar{C} + \bar{l})^2}{n^2}}\right\} \\
        &\leq \exp\left\{\frac{-2n\left(\frac{\epsilon}{2} - 2\mathfrak{R}_n\left(\calB\right)\right)^2}{(\bar{C} + \bar{l})^2}\right\} \\
        &\overset{\text{def}}{=} \frac{\delta}{2}
    \end{align*}

    Similarly, 
    \begin{align*}
         P\left(G_n' > \frac{\epsilon}{2}\right) &\leq \exp\left\{\frac{-2n\left(\frac{\epsilon}{2} - 2\mathfrak{R}_n\left(\calB\right)\right)^2}{(\bar{C} + \bar{l})^2}\right\} = \frac{\delta}{2}
    \end{align*}

    Let $\epsilon = 4\mathfrak{R}_n\left(\calB\right) + (\bar{C} + \bar{l})\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}$

    \begin{align*}
        P\left(\calR_{\phi}\left(\wh r_{n}, h\right) - \calR^*_{\phi}\left(\calF, h\right) > \epsilon\right) &\leq P\left(\sup_{r \in \calF} \left|\calR_{\phi}\left(r, h\right) - \wh \calR_{\phi}(r, h)\right| > \frac{\epsilon}{2}\right) \\
        &\leq P\left(G_n > \frac{\epsilon}{2}\right) + P\left(G_n' > \frac{\epsilon}{2}\right) \\
        &\leq \delta
    \end{align*}

    With at least probability $1 - \delta$, $\calR_{\phi}\left(\wh r_{n}, h\right) - \calR^*_{\phi}\left(\calF, h\right) \leq 4\mathfrak{R}_n\left(\calB\right) + (\bar{C} + \bar{l})\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}$.

    We can further bound $\mathfrak{R}_n\left(\calB\right)$ by $\mathfrak{R}_n\left(\calF_j\right)$ if we show that $\calL^{\phi}$ is Lipschitz continuous with respect to $r$

    \begin{align*}
        &\left\|\calL^{\phi}(h, r_1, x, y) - \calL^{\phi}(h, r_2, x, y)\right\|_2^2 \\ 
        &\leq \left\|\frac{1}{L} \sum_{j = 1}^L l\left(y, \wh y_{j}^h \right)\phi(r_{1,j}(x, \wh y_{<j}))  -  l\left(y, \wh y_{j}^h \right)\phi(r_{2, j}(x, \wh y_{<j})) \right\|_2^2 \\
        & + \left\|\frac{1}{L} \sum_{j = 1}^L c_j(x, \wh y_{<j})\phi(-r_{1, j}(x, \wh y_{<j})) - c_j(x, \wh y_{<j})\phi(-r_{2, j}(x, \wh y_{<j}))\right\|_2^2 \\
        &\leq \frac{\bar{l}^2}{L^2} \sum_{j = 1}^L \left\|\phi(r_{1, j}(x, \wh y_{<j})) - \phi(r_{2, j}(x, \wh y_{<j}))\right\|_2^2 +  \frac{\bar{C}^2}{L^2} \sum_{j = 1}^L \left\|\phi(-r_{1, j}(x, \wh y_{<j})) - \phi(-r_{2, j}(x, \wh y_{<j}))\right\|_2^2 \\
        &\leq \frac{\rho^2(\bar{C}^2 + \bar{l}^2)}{L^2} \sum_{j = 1}^L \left\|r_{1, j}(x, \wh y_{<j}) - r_{2, j}(x, \wh y_{<j})\right\|_2^2
    \end{align*}

    By the extension of the Talagrand's Contraction Lemma \cite{maurer2016vector},
    \begin{align*}
        \mathfrak{R}_n\left(\calB\right) \leq \frac{\rho\sqrt{2\left(\bar{C}^2 + \bar{l}^2\right)}}{L}\sum_{j = 1}^L\mathfrak{R}_n\left(\calF_j\right)
    \end{align*}
    
\end{proof}

\subsection{Proof of \Cref{th:genboundreal}} \label{sec:proofgenboundreal}

\begin{corollary}
   Suppose $r \in \calF$ which is composed of $\{\calF_j\}_{j = 1}^L$ such that $r_j \in \calF_j$, $0 < \bar{c} \leq c_j(x) \leq \bar{C}$, and $l(y, \wh y^h_j) \leq \bar{l} < \infty$. If $\phi$ is a $\rho$-lipschitz continuous binary classification-calibrated function, then with probability $1 - \delta$, the following upper bound on the excess risk with respect to $\calL^{\text{Token}}$ holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL_{\phi}$ for a fixed $h$:
    \begin{equation*}
        \begin{split}
             \calR_{\text{Token}}\left(\wh r_{n}, h\right) - \calR^*_{\text{Token}}(h) \leq \Tilde{\Gamma}\left(\calB(\calF) + \calA_{\phi}\left(\calF, h\right)\right)    
        \end{split}
    \end{equation*}
    where $\Tilde{\Gamma}: \R^+ \to \R^+$, $\calB(\calF) = \frac{4\rho\sqrt{2\left(\bar{C}^2 + \bar{l}^2\right)}}{L} \sum_{j = 1}^L \mathfrak{R}_n\left(\calF_j\right) + (\bar{C} + \bar{l})\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}$, and $\calA_{\phi}\left(\calF, h\right) = \calR^*_{\phi}\left(\calF, h\right) -  \calR^*_{\phi}(h)$ is the approximation error.
\end{corollary}

\begin{proof}
    Excess risk can be decomposed into estimation error and approximation error
    \begin{align*}
        \calR_{\phi}\left(r, h\right) - \calR^*_{\phi}(h) = \underbrace{\calR_{\phi}\left(r, h\right) - \calR^*_{\phi}\left(\calF, h\right)}_{\text{Estimation Error}} + \underbrace{\calR^*_{\phi}\left(\calF, h\right) -  \calR^*_{\phi}(h)}_{\text{Approximation Error}}
    \end{align*}
    From \Cref{th:genbounds}, we have an upper bound on the estimation error and so using that, we have the following inequality
    \begin{align*}
        \calR_{\phi}\left(r, h\right) - \calR^*_{\phi}(h) \leq \frac{4\rho\sqrt{2\left(\bar{C}^2 + \bar{l}^2\right)}}{L} \sum_{j = 1}^L \mathfrak{R}_n\left(\calF_j\right) + (\bar{C} + \bar{l})\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}} + \calA_{\phi}\left(\calF, h\right)
    \end{align*}
    Since $\phi$ is binary classification calibrated, we know that
    $$\calR_{\text{binary }0-1}(f) - \calR^{*}_{\text{binary }0-1} \leq \psi^{-1}\left(\calR_{\text{binary }\phi}(f) - \calR^{ *}_{\text{binary }\phi}\right)$$ where $\psi$ is a non-decreasing convex function, making $\psi^{-1}$ a non-decreasing concave function. The proof is complete by using \Cref{th:upbound}
\end{proof}


\section{Proofs for \Cref{sec:defpointclass}}

\subsection{Proof of \Cref{th:onetimerewrite}}



\begin{lemma}\label{th:onetimerewrite}
    For any $h$, $r$, $x \in \calX$, $y \in \calY$, the following equality holds 
    \begin{align*}
        \calL^{\text{OneTime}}\left(h, r, x, y\right) &=  \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j}   -(|\calJ| - 1)c_{\text{max}} + \sum_{j \in \calJ} l(y, \widehat{y}_{\leq j}) + \sum_{j \in \calJ} \Tilde{c}_j(x, \widehat{y}_{<j}, y)
    \end{align*}
\end{lemma}

\begin{proof}

\begin{align*}
    &\calL^{\text{OneTime}}\left(h, r, x, y\right) \\
    &= \sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) = j} \\
    &= \sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right) - \sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j} \\
    &= \sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right) - \sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j} + (|\calJ| - 1)c_{\text{max}} - (|\calJ| - 1)c_{\text{max}} \\
    &= \sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right) - \sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j} + \sum_{j \in \calJ}c_{\text{max}}\mathbbm{1}_{r(x) \neq j} - (|\calJ| - 1)c_{\text{max}} \\
    &= \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j}   -(|\calJ| - 1)c_{\text{max}} + \sum_{j \in \calJ} l(y, \widehat{y}_{\leq j}) + \sum_{j \in \calJ} \Tilde{c}_j(x, \widehat{y}_{<j}, y)
\end{align*}
\end{proof}


\subsection{Proof of \Cref{th:onetimeconvexupperbound}} \label{sec:proofonetimeconvexupper}

\begin{proposition} \label{th:onetimeconvexupperbound}
    $\calL^{\psi}(h, r, x, y)$ is a convex upper bound of $\calL^{\text{OneTime}}(h, r, x, y)$ for $\psi \equiv \psi_{\text{ce}}$ and $\psi \equiv \psi_{\text{mae}}$ 
\end{proposition}
\begin{proof}

It is easy to see that $\calL^{\psi}(h, r, x, y)$ is convex with respect to $r$ as $\psi_{\text{ce}}(\cdot)$ and $\psi_{\text{mae}}(\cdot)$ are convex. 

From \Cref{th:onetimerewrite}, we know that

\begin{align*}
    \calL^{\text{OneTime}}\left(h, r, x, y\right) &=  \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j}   -(|\calJ| - 1)c_{\text{max}} + \sum_{j \in \calJ} l(y, \widehat{y}_{\leq j}) + \sum_{j \in \calJ} \Tilde{c}_j(x, \widehat{y}_{<j}, y) \\
    &\leq \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j}   + c_{\text{max}}
\end{align*}

If $r(x) \neq j$, then $\psi_{\text{ce}}(\g(x), j) \geq \log 2$ and $\psi_{\text{mae}}(\g(x), j) \geq 0.5$. When $r(x) = j$, $\psi_{\text{ce}}(\g(x), j) \geq 0$ and $\psi_{\text{mae}}(\g(x), j) \geq 0$. Therefore, $\gamma_{\text{ce}}\psi_{\text{ce}}(\g(x), j) \geq \mathbbm{1}_{r\left(x\right) \neq j}$ and $\gamma_{\text{mae}}\psi_{\text{mae}}(\g(x), j) \geq \mathbbm{1}_{r\left(x\right) \neq j}$ where $\gamma_{\text{ce}} = \log 0.5$ and $\gamma_{\text{mae}} = 2$. So then,

\begin{align*}
    \calL^{\text{OneTime}}\left(h, r, x, y\right) &\leq \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j}   + c_{\text{max}} \\
    &\leq \gamma\sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\psi(\g(x), j)   + c_{\text{max}}
\end{align*}

Therefore, 
$\gamma'\calL^{\psi}(h, r, x, y) \geq \calL^{\text{OneTime}}(h, r, x, y)$ as long as $c_{\text{max}}$ is finite

\end{proof}


\subsection{Proof of \Cref{th:onetimeconsistencybound}}

\begin{lemma} \label{th:onetimeconsistencybound}
    Let $\Tilde{c}_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$, $l(y, \wh y^h_j) \leq \bar{l} < \infty$ for all $j \in \calJ$, and $\psi$ be a multiclass surrogate loss that satisfies the following inequality for any distribution over $\calX \times \calJ$ and any measurable function $f$:
    \begin{align*}
        \calR_{\text{multi }0-1}(f) - \calR^{*}_{\text{multi }0-1} \leq \Gamma\left(\calR_{\text{multi }\psi}(f) - \calR^{ *}_{\text{multi }\psi}\right)
    \end{align*}
    where $\calR_{\text{multi }0-1}$ is the multiclass 0-1 risk and $\Gamma : \R^+ \to \R^+$ is a non-decreasing concave function. If there exists $i, j \in \calJ$ such that $\left|l(y, \wh y^h_i) + \Tilde{c}_i(x, \wh y_{<i}, y) - l(y, \wh y^h_j) - \Tilde{c}_i(x, \wh y_{<i}, y)\right| > \Delta > 0$, then for any $\g$ over all measurable functions and for any $h$, the following inequality holds for any distribution over $\calX \times \calY$:
    \begin{align*}
        \calR_{\text{OneTime}}\left(r, h\right) - \calR_{\text{OneTime}}^*(h) \leq \Tilde{\Gamma}\left(\calR_{\psi}\left(r, h\right) - \calR^*_{\psi}(h)\right)
    \end{align*}
    where $\Tilde{\Gamma}(z) = (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\Gamma\left(\frac{z}{\Delta}\right)$, $\calR^*_{\text{OneTime}}(h) = \inf_{r \in \calM} \calR_{\text{OneTime}}\left(r, h\right)$, and $\calR^*_{\psi}(h) = \inf_{r} \calR_{\phi}\left(r, h\right)$
\end{lemma}

\begin{proof}
Let $C_{\text{OneTime}}(r, h, x) = \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\calL^{\text{OneTime}}\left(h, r, x, y\right)\right]$ be the pointwise 
$\calL^{\text{OneTime}}$- risk for a given $x$ and predictor $h$ and let $C_{\text{OneTime}}^*(h, x) = \inf_{r} C_{\text{OneTime}}(r, h, x)$ be the optimal pointwise $\calL^{\text{OneTime}}$- risk for a given $x$ and predictor $h$. Similarly, let $C_{\psi}(r, h, x) = \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\calL^{\text{OneTime}}_{\psi}\left(h, r, x, y\right)\right]$ be the pointwise $\calL^{\psi}$- risk for a given $x$ and predictor $h$ and $C_{\psi}^*(h, x) = \inf_{r} C_{\psi}(r, h, x)$ be the optimal pointwise $\calL^{\psi}$- risk for a given $x$ and predictor $h$. For any measurable function $\g$ 
    \begin{align*}
        &C_{\text{OneTime}}(r, h, x) - C_{\text{OneTime}}^*(h, x) \\
        &= \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) = j} \right]\\
        &\quad - \inf_{\Tilde{r}} \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\sum_{j \in \calJ} \left( l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{\Tilde{r}\left(x\right) = j} \right] \\
        &\overset{(a)}{=} \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{r\left(x\right) \neq j}   -(|\calJ| - 1)c_{\text{max}} + \sum_{j \in \calJ} l(y, \widehat{y}_{\leq j}) + \sum_{j \in \calJ} \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right] \\
        &\quad - \inf_{\Tilde{r}} \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[\sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y) \right)\mathbbm{1}_{\Tilde{r}\left(x\right) \neq j}   -(|\calJ| - 1)c_{\text{max}} + \sum_{j \in \calJ} l(y, \widehat{y}_{\leq j}) + \sum_{j \in \calJ} \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right] \\
        &= \sum_{j \in \calJ} \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}}\left[c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right] \mathbbm{1}_{r(x) \neq j}  - \inf_{\Tilde{r}} \sum_{j \in \calJ} \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}} \left[c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right]\mathbbm{1}_{\Tilde{r}(x) \neq j}\\
    \end{align*}
    (a) By \Cref{th:onetimerewrite}. Let $A_j = \mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}} \left[c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right]$

    \begin{align*}
        &C_{\text{OneTime}}(r, h, x) - C_{\text{OneTime}}^*(h, x) \\
        &= \sum_{j \in \calJ} A_j \mathbbm{1}_{r(x) \neq j}  - \inf_{\Tilde{r}} \sum_{j \in \calJ} A_j\mathbbm{1}_{\Tilde{r}(x) \neq j}\\
        &= \sum_{k \in \calJ} A_k \left(\sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\mathbbm{1}_{r(x) \neq j}  - \inf_{\Tilde{r}}  \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\mathbbm{1}_{\Tilde{r}(x) \neq j} \right) 
    \end{align*}

     Consider a degenerate distribution on $\Tilde{X}$ with a pmf of $1$ on $x$. Let $P(\Tilde{Y} = j \mid \Tilde{X} = x) = \frac{A_j}{\sum_{k \in \calJ} A_k}$ and $f$ be a measurable function such that $f(x) = r(x)$. Under this distribution,
    \begin{align*}
        &\calR_{\text{multi }0-1}(r) = \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\mathbbm{1}_{r(x) \neq j} &\calR_{\text{multi }0-1}^* = \inf_{\Tilde{r}} \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\mathbbm{1}_{\Tilde{r}(x) \neq j} \\
        &\calR_{\text{multi }\psi}(r) = \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\psi\left(\g(x), j\right)  &\calR_{\text{multi }\psi}^* = \inf_{\Tilde{\g}} \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\psi\left(\g(x), j\right)
    \end{align*}
    From the theorem statement,
    \begin{align*}
        &C_{\text{OneTime}}(r, h, x) - C_{\text{OneTime}}^*(h, x)  \\
        &= \sum_{k \in \calJ} A_k \left(\sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\mathbbm{1}_{r(x) \neq j}  - \inf_{\Tilde{r}}  \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\mathbbm{1}_{\Tilde{r}(x) \neq j} \right)   \\
        &\leq \sum_{k \in \calJ} A_k \Gamma \left( \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\psi\left(\g(x), j\right)   - \inf_{\Tilde{\g}} \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\psi\left(\Tilde{\g}(x), j\right) \right) 
    \end{align*}

    We know that $\sum_{j \in \calJ} A_j \leq (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)$ from:
    \begin{align*}
        \sum_{j \in \calJ} A_j &= \sum_{j \in \calJ}\mathop{\mathbb{E}}_{y \sim \calP_{Y \mid X = x}} \left[c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right] \\
        &\leq (|\calJ -1|)\left(\bar{C} + \bar{l}\right)
    \end{align*}
    Since $l(y, \wh y^h_i) + \Tilde{c}_i(x, \wh y_{<i}, y) \neq l(y, \wh y^h_j) + \Tilde{c}_i(x, \wh y_{<i}, y)$ for a pair $(i, j)$, we can assume that $\sum_{j \in \calJ} A_j \geq \Delta$ if $\left|l(y, \wh y^h_i) + \Tilde{c}_i(x, \wh y_{<i}, y) - l(y, \wh y^h_j) - \Tilde{c}_i(x, \wh y_{<i}, y)\right| > \Delta$. 
    % from:
    % \begin{align*}
    %     \sum_{j = 0}^L A_j &= \sum_{j = 0}^L P\left(y_{\leq j} = \wh y_{\leq j} \mid X = x\right) + j + \sum_{k = j + 1}^L \Ex_{y \sim \calP_{Y \mid X = x}}\left[\Tilde{c}_k(x, y)\right] \\
    %     &\geq \sum_{j = 0}^L (L - j)(1 - \bar{C}) \\
    %     &= \frac{(1 - \bar{C})L(L + 1)}{2}
    % \end{align*}
    
    With these bounds,
    \begin{align*}
        &C_{\text{OneTime}}(r, h, x) - C_{\text{OneTime}}^*(h, x)  \\ 
        &\leq \sum_{k \in \calJ} A_k \Gamma \left( \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\psi\left(\g(x), j\right)   - \inf_{\Tilde{\g}} \sum_{j \in \calJ} \frac{A_j}{\sum_{k \in \calJ} A_k}\psi\left(\Tilde{\g}(x), j\right) \right)  \\
        &\leq (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\Gamma \left( \frac{\sum_{j \in \calJ} A_j\psi\left(\g(x), j\right) - \inf_{\Tilde{\g}} \sum_{j \in \calJ} A_j\psi\left(\Tilde{\g}(x), j\right) }{\Delta}  \right) \\
        &=(|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\Gamma \left( \frac{C_{\psi}(r, h, x) - C_{\psi}^*(h, x) }{\Delta}  \right)
    \end{align*}

    Let $\Tilde{\Gamma}(z) = (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\Gamma\left(\frac{z}{\Delta}\right)$. Then, 
    \begin{align*}
        \calR_{\text{OneTime}}\left(r, h\right) - \calR_{\text{OneTime}}^*(h) &= \mathop{\mathbb{E}}_{x \sim \calP_X}\left[C_{\text{OneTime}}(r, h, x) - C_{\text{OneTime}}^*(h, x)\right] \\
        &\leq \mathop{\mathbb{E}}_{x \sim \calP_X}\left[\Tilde{\Gamma}\left(C_{\psi}(r, h, x) - C_{\psi}^*(h, x)\right)\right] \\
        &\overset{(a)}{\leq} \Tilde{\Gamma}\left(\mathop{\mathbb{E}}_{x \sim \calP_X}\left[C_{\psi}(r, h, x) - C_{\psi}^*(h, x)\right]\right) \\
        & \overset{(b)}{=}  \Tilde{\Gamma}\left(\calR_{\psi}\left(r, h\right) - \calR^*_{\psi}(h)\right)
    \end{align*}

    (a) by Jensen's inequality since $\Tilde{\Gamma}$ is concave, (b) since the infimum is taken over all measurable functions $\calR^*_{\psi}(h) = \mathop{\mathbb{E}}_{x \sim \calP_X}\left[C_{\psi}^*(h, x)\right]$
\end{proof}

\subsection{Proof of \Cref{th:onetimeconsistency}} \label{sec:proofonetimeconsistency}
\begin{theorem}[Consistency]
    Let $\Tilde{c}_j(x, \wh y_{<j}, y) \leq \bar{C} < \infty$, $l(y, \wh y^h_j) \leq \bar{l} < \infty$ for all $j \in \calJ$, and $\psi$ be cross entropy loss $\psi_{\text{ce}}$ or mean absolute error $\psi_{\text{mae}}$. If there exists $i, j \in \calJ$ such that  $\left|l(y, \wh y^h_i) + \Tilde{c}_i(x, \wh y_{<i}, y) - l(y, \wh y^h_j) - \Tilde{c}_i(x, \wh y_{<i}, y)\right| > \Delta > 0$, then, for a fixed $h$, $\calL^{\psi}$ is a consistent surrogate for $\calL^{\text{OneTime}}$
\end{theorem}

\begin{proof}

We know that
$\calR_{\text{multi }0-1}(f) - \calR^{*}_{\text{multi }0-1} \leq \Gamma_{\text{ce}}\left(\calR_{\text{multi }\psi_{\text{ce}}}(f) - \calR^{ *}_{\text{multi }\psi_{\text{ce}}}\right)$ from Theorem 3.1 of \cite{mao2023cross} where $\Gamma_{\text{ce}}^{-1}(z) = \frac{1 + z}{2}\log(1 + z) + \frac{1 - z}{2}\log(1 - z)$. Similarly, $\calR_{\text{multi }0-1}(f) - \calR^{*}_{\text{multi }0-1} \leq \Gamma_{\text{mae}}\left(\calR_{\text{multi }\psi_{\text{mae}}}(f) - \calR^{ *}_{\text{multi }\psi_{\text{mae}}}\right)$ where $\Gamma_{\text{mae}}^{-1}(z) = \frac{z}{L + 1}$. Both $\Gamma_{\text{ce}}^{-1}$ and $\Gamma_{\text{mae}}^{-1}$ are non-decreasing convex functions making $\Gamma_{\text{ce}}$ and $\Gamma_{\text{mae}}$ non-decreasing \textit{concave} functions on $\R^{+}$ domain. 

Let $\Gamma \equiv \Gamma_{\text{ce}}$ when $\psi \equiv \psi_{\text{ce}}$ and $\Gamma \equiv \Gamma_{\text{mae}}$ when $\psi \equiv \psi_{\text{mae}}$. By \Cref{th:onetimeconsistencybound}, $\calR_{\text{OneTime}}\left(r\right) - \calR_{\text{OneTime}}^* \leq \Tilde{\Gamma}\left(\calR_{\psi}\left(r\right) - \calR^*_{\psi}\right)$ where $\Tilde{\Gamma}(z) = (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\Gamma\left(\frac{z}{\Delta}\right)$. If $\lim_{n \to \infty} \calR_{\psi}\left(r_n\right) - \calR^*_{\psi} = 0$, then
\begin{align*}
    \lim_{n \to \infty} \calR_{\text{Point}}\left(r_n\right) - \calR_{\text{Point}}^* &\leq \lim_{n \to \infty} \Tilde{\Gamma}\left(\calR_{\psi}\left(r_n\right) - \calR^*_{\psi}\right) \\
    & \overset{(a)}{=}  (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\Gamma\left(\lim_{n \to \infty} \frac{\calR_{\phi}\left(r_n\right) - \calR^*_{\phi}}{\Delta}\right) \\
    &= (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\Gamma(0) \\
    & \overset{(b)}{=} 0
\end{align*}
(a) by continuity of $\Gamma$ at $0$ and (b) since $\Gamma_{\text{ce}}(0) = 0$ and $\Gamma_{\text{mae}}(0) = 0$

\end{proof}

\subsection{Proof of \Cref{th:onetimegenbounds}} \label{sec:proofonetimegenbounds}

\begin{theorem} 
    Suppose $r_{n} \in \calF = \{x \to \argmin_{j \in \calJ} g_j(x): g_j \in \calG_j\}$ where $\calG_j$ consists of functions having a range of $(-M, M)$ with $M \geq 0$, $\Tilde{c}_j(x, \wh y_{<j}, y) \leq \bar{C}$, and $l(y, \wh y^h_j) \leq \bar{l} < \infty$. If $\psi$ is $\rho$-lipschitz continuous with respect to $\g(x)$ and there exists $u_{\psi}(M)$ such that $\psi\left(\g(x), j\right) \leq u_{\psi}(M) < \infty$ for all $j \in \calJ$ and $r \in \calF$, then with probability $1 - \delta$ for a fixed $h$, the following upper bound holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL^{\psi}$:
    \begin{equation*}
        \calR_{\psi}\left(\wh r_{n}, h\right) - \calR^*_{\psi}\left(\calF, h\right) \leq 2\sqrt{2}q(L)\rho\sum_{j \in \calJ}\mathfrak{R}_{n}\left(\calG_j\right) + u_{\psi}(M) q(L)\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}
    \end{equation*}
    where $\calR^*_{\psi}\left(\calF, h\right) = \inf_{r \in \calF} \calR_{\psi}\left(r, h\right)$, $q(L) =2(|\calJ| - 1)(\bar{C} + \bar{l})$, and $\mathfrak{R}_{n}\left(\calG_j\right)$ is the Rademacher Complexity of $\calG_j$.
\end{theorem}

\begin{proof}

Let $G_{n} = \sup_{r \in \calF} \left[\calR_{\psi}\left(r\right) - \wh \calR_{\psi}(r)\right]$, $G_{n'} = \sup_{r \in \calF} \left[\wh \calR_{\psi}(r) -\calR_{\psi}\left(r\right) \right]$, and\\
$\calB = \{(x, y) \mapsto \calL^{\psi}\left(h, r, x, y\right) \ : r \in \calF\}$. 

If $z^i = (x^i, y^i)$, $G_{n} = g(z^1, \cdots, z^n)$. 
\begin{align*}
    \left|g(z^1, \cdots, z^i, \cdots, z^{n}) - g(z^1, \cdots, z^{i'}, \cdots, z^{n})\right| &\leq \frac{1}{n_c}\left| \sup_{r \in \calF} \calL^{\psi}\left(h, r, x^{i}, y^{i}\right) - \calL^{\psi}\left(h, r, x^{i'}, y^{i'}\right)\right|\\
    % &\leq \frac{1}{n_c}\left| \sup_{r \in \calF} \calL^{\psi}\left(h, r, x^{i}, y^{i}\right) \right| \\
    & \leq  \frac{1}{n} \sup_{r \in \calF} \sum_{j \in \calJ}\left|\left(c_{\text{max}}^i - l(y^i, \widehat{y}_{\leq j}^i) - \Tilde{c}_j(x^i, \widehat{y}_{<j}^i, y^i)\right)\psi(\g(x^i), y^i) \right| \\
    &\quad + \left|\left(c_{\text{max}}^{i'} - l(y^{i'}, \widehat{y}_{\leq j}^{i'}) - \Tilde{c}_j(x^{i'}, \widehat{y}_{<j}^{i'}, y^{i'})\right)\psi(\g(x^{i'}), y^{i'}) \right|\\
    % &\leq \frac{1}{n_c} \sup_{r \in \calF} \sum_{j = 1}^{L + 1}\left|\left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right)\psi(\g(x^i), y^i) \right| \\
    &\leq \frac{1}{n} u_{\psi}(M)2(|\calJ| - 1)(\bar{C} + \bar{l}) \\
    &= \frac{1}{n} u_{\psi}(M) q(L)
\end{align*}
where $q(L) = 2(|\calJ| - 1)(\bar{C} + \bar{l})$.  Using this boundedness property, we can apply McDiarmid's inequality

\begin{align*}
    P\left(G_{n} > \frac{\epsilon}{2}\right) &\leq \exp\left\{\frac{-2\left(\frac{\epsilon}{2} - \mathop{\mathbb{E}}\left[G_n\right]\right)^2}{\sum_{i = 1}^{n_c} \frac{u_{\psi}(M)^2 q(L)^2}{n^2}}\right\} \\
    &\leq \exp\left\{\frac{-2n\left(\frac{\epsilon}{2} - 2\mathfrak{R}_{n}\left(\calB\right)\right)^2}{u_{\psi}(M)^2 q(L)^2}\right\} \\
    &\overset{\text{def}}{=} \frac{\delta}{2}
\end{align*}

Similarly, 
\begin{align*}
     P\left(G_{n}' > \frac{\epsilon}{2}\right) &\leq \exp\left\{\frac{-2n\left(\frac{\epsilon}{2} - 2\mathfrak{R}_{n}\left(\calB\right)\right)^2}{u_{\psi}(M)^2 q(L)^2}\right\} = \frac{\delta}{2}
\end{align*}

Then $\epsilon = 4\mathfrak{R}_{n}\left(\calB\right) + u_{\psi}(M) q(L)\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}$

\begin{align*}
    P\left(\calR_{\psi}\left(\wh r_{n}\right) - \calR^*_{\psi}\left(\calF\right) > \epsilon\right) &\leq P\left(\sup_{r \in \calF} \left|\calR_{\psi}\left(r\right) - \wh \calR_{\psi}(r)\right| > \frac{\epsilon}{2}\right) \\
    &\leq P\left(G_{n} > \frac{\epsilon}{2}\right) + P\left(G_{n}' > \frac{\epsilon}{2}\right) \\
    &\leq \delta
\end{align*}

With at least probability $1 - \delta$, $\calR_{\psi}\left(\wh r_{n}, h\right) - \calR^*_{\psi}\left(\calF, h\right) \leq 4\mathfrak{R}_{n}\left(\calB\right) +  u_{\psi}(M) q(L)\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}}$.

 We can further bound $\mathfrak{R}_{n}\left(\calB\right)$ by $\mathfrak{R}_n\left(\calF_j\right)$ if we show that $\calL^{\psi}$ is Lipschitz continuous with respect to $\g$

\begin{align*}
    &\left\|\calL^{\psi}(h, r^1, x, y) - \calL^{\psi}(h, r^2, x, y)\right\|_2 \\ 
    &=\left\| \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right)\psi(\g^1(x), j) -  \sum_{j \in \calJ} \left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right)\psi(\g^2(x), j)\right\|_2\\
    &\leq \sum_{j \in \calJ}\left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right) \left\|\psi(\g^1(x), j) - \psi(\g^2(x), j) \right\|_2 \\
    &\overset{(a)}{=} \sum_{j \in \calJ, j \neq j_{\text{max}}}\left(c_{\text{max}} - l(y, \widehat{y}_{\leq j}) - \Tilde{c}_j(x, \widehat{y}_{<j}, y)\right) \left\|\psi(\g^1(x), j) - \psi(\g^2(x), j) \right\|_2\\ 
    &\overset{(b)}{\leq} \sum_{j \in \calJ, j \neq j_{\text{max}}}\left(\bar{C} + \bar{l}\right) \rho \left\|\g^1(x) - \g^2(x) \right\|_2\\
    &= (\calJ - 1)\left(\bar{C} + \bar{l}\right)\rho \left\|\g^1(x) - \g^2(x) \right\|_2
\end{align*}
(a) where $j_{\text{max}} = \argmax_{j \in \calJ} l(y, \widehat{y}_{\leq j}) + \Tilde{c}_j(x, \widehat{y}_{<j}, y)$, (b) by lipschitz continuity of $\psi$. 

By the extension of the Talagrand's Contraction Lemma \cite{maurer2016vector},
\begin{align*}
    \mathfrak{R}_{n}\left(\calB\right) &=\mathop{\mathbb{E}}_{\sigma_1, ..., \sigma_n, \{z^i\}_{i = 1}^{n} \sim \calP_{\calX \times \calY}}\left[\sup_{b \in \calB} \frac{1}{n}\sum_{i = 1}^{n} \sigma_ib(z^i)\right]\\
    % &= \mathop{\mathbb{E}}_{\sigma_1, ..., \sigma_n, \{z^i\}_{i = 1}^{n} \sim \calP_{\calX \times \calY}}\left[\sup_{\g \in \calG_1 \times ...\times \calG_{L + 1}} \frac{1}{n}\sum_{i = 1}^{n} \sigma_i\g(z^i)\right]\\
    &\leq (|\calJ| - 1)\left(\bar{C} + \bar{l}\right)\rho\sqrt{2}\mathop{\mathbb{E}}_{\sigma, \{z^i\}_{i = 1}^{n} \sim \calP_{\calX \times \calY}}\left[\sup_{\g \in \calG_1 \times ...\times \calG_{L+ 1}} \frac{1}{n}\sum_{i = 1}^{n} \sum_{j \in \calJ} \sigma_{i, j}g_j(z^i)\right] \\
    &\leq \frac{q(L)\rho}{\sqrt{2}} \sum_{j \in \calJ} \mathop{\mathbb{E}}_{\sigma, \{z^i\}_{i = 1}^{n} \sim \calP_{\calX \times \calY}}\left[\sup_{g_j \in \calG_j} \frac{1}{n}\sum_{i = 1}^{n}  \sigma_{i, j}g_j(z^i)\right] \\
    &\leq \frac{q(L)\rho}{\sqrt{2}}\sum_{j \in \calJ}\mathfrak{R}_{n}\left(\calG_j\right)
\end{align*}

\end{proof}


\subsection{Proof of \Cref{th:onetimegenboundreal}} \label{sec:proofonetimegenboundreal}

\begin{corollary} \label{th:onetimegenboundreal}
     Suppose $r_{n} \in \calF = \{x \to \argmin_{j \in \calJ} g_j(x): g_j \in \calG_j\}$ where $\calG_j$ consists of functions having a range of $(-M, M)$ with $M \geq 0$, $\Tilde{c}_j(x, \wh y_{<j}, y) \leq \bar{C}$, and $l(y, \wh y^h_j) \leq \bar{l} < \infty$. If $\psi$ is $\psi_{\text{ce}}$ or $\psi_{\text{mae}}$, then with probability $1 - \delta$, for a fixed $h$, the excess risk of with respect to $\calL^{\text{OneTime}}$ holds for the empirical risk minimizer $\wh r_{n}$ with respect to $\calL^{\psi}$
    \begin{equation*}
        \calR_{\text{OneTime}}\left(\wh r_{n}, h\right) - \calR^*_{\text{OneTime}}(h) \leq \Tilde{\Gamma}\left(2q(L)\rho\sqrt{2}\sum_{j \in \calJ}\mathfrak{R}_{n}\left(\calG_j\right) + u_{\psi}(M) q(L)\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}} + \calA_{\psi}\left(\calF, h\right)\right)
    \end{equation*}
    where $\Tilde{\Gamma}: \R^+ \to \R^+$, $q(L) = 2(|\calJ| - 1)(\bar{C} + \bar{l})$, $\rho$ is the lipschitz norm of $\psi$, and $\calA_{\psi}\left(\calF, h\right) = \calR^*_{\psi}\left(\calF, h\right) -  \calR^*_{\psi}(h)$ is the approximation error.
\end{corollary}

\begin{proof}

Excess risk can be decomposed into estimation error and approximation error
\begin{align*}
    \calR_{\psi}\left(r, h\right) - \calR^*_{\psi} = \underbrace{\calR_{\psi}\left(r, h\right) - \calR^*_{\psi}(h)\left(\calF, h\right)}_{\text{Estimation Error}} + \underbrace{\calR^*_{\psi}\left(\calF, h\right) -  \calR^*_{\psi}(h)}_{\text{Approximation Error}}
\end{align*}
We know that $\psi_{\text{ce}}, \psi_{\text{mae}}$ are upper bounded by $u_{\psi_{\text{ce}}}(M), u_{\psi_{\text{mae}}}(M)$ repsectively. We also know that the lipschitz norm for $\psi_{\text{ce}}$ $\rho_{\text{ce}}$ is $\sqrt{2}$ and the lipschitz norm for $\psi_{\text{mae}}$ $\rho_{\text{mae}}$ is $2$


From \Cref{th:onetimegenbounds}, we have an upper bound on the estimation error and so using that, we have the following inequality
\begin{align*}
    \calR_{\psi}\left(r, h\right) - \calR^*_{\psi}(h) \leq 2q(L)\rho\sqrt{2}\sum_{j \in \calJ}\mathfrak{R}_{n}\left(\calG_j\right) + u_{\psi}(M) q(L)\sqrt{\frac{2\log\left(\frac{2}{\delta}\right)}{n}} + \calA_{\psi}\left(\calF, h\right)
\end{align*}
$\calR_{\text{multi }0-1}(f) - \calR^{*}_{\text{multi }0-1} \leq \Gamma_{\text{ce}}\left(\calR_{\text{multi }\psi_{\text{ce}}}(f) - \calR^{ *}_{\text{multi }\psi_{\text{ce}}}\right)$ from Theorem 3.1 of \cite{mao2023cross} where $\Gamma_{\text{ce}}^{-1}(z) = \frac{1 + z}{2}\log(1 + z) + \frac{1 - z}{2}\log(1 - z)$. Similarly, $\calR_{\text{multi }0-1}(f) - \calR^{*}_{\text{multi }0-1} \leq \Gamma_{\text{mae}}\left(\calR_{\text{multi }\psi_{\text{mae}}}(f) - \calR^{ *}_{\text{multi }\psi_{\text{mae}}}\right)$ where $\Gamma_{\text{mae}}^{-1}(z) = \frac{z}{L + 1}$. Both $\Gamma_{\text{ce}}^{-1}$ and $\Gamma_{\text{mae}}^{-1}$ are non-decreasing convex functions making $\Gamma_{\text{ce}}$ and $\Gamma_{\text{mae}}$ non-decreasing \textit{concave} functions on $\R^{+}$ domain.

The proof is complete by using \Cref{th:onetimeconsistencybound}


\end{proof}

\section{Baseline Details} \label{sec:baselinedetails}


Let $p_h(\wh y_j \mid \wh y_{<j}, x)$ be the predictive distribution of the $j^{\text{th}}$ token conditioned on the leftward context $\wh y_{<j}$ and input $x$. The predictive entropy would then be $H(\wh Y_j \mid \wh y_{<j}, x) = -\sum_{v \in \calV} p_h(v \mid \wh y_{<j}, x) \log p_h(v \mid \wh y_{<j}, x)$. For a given threshold $\tau$, the whole deferral decision rule is determined by evaluating $r(x) > \tau$. The baselines offer various ways of modeling $r$.

\begin{itemize}
    \item \texttt{ChowSum} - $r_{\text{ChowSum}}(x) = -\sum_{j = 1}^L \log p_h(\wh y_j \mid \wh y_{<j}, x)$
    \item \texttt{ChowMean} - $r_{\text{ChowMean}}(x) = -\frac{1}{L}\sum_{j = 1}^L \log p_h(\wh y_j \mid \wh y_{<j}, x)$
    \item \texttt{ChowQuantile} at the level $\alpha$ - $r_{\text{ChowQuantile}}^{\alpha}(x) = \text{Quantile}_{\alpha}\left(\left\{-\log p_h(\wh y_j \mid \wh y_{<j}, x)\right\}_{j = 1}^L\right)$
    \item \texttt{WholeModelEmbed} - $r(x)$ is trained using the surrogate of the modified One-time deferral loss or $\calL^{\psi}_{\text{mod}}$ where $\calJ = \{1, L + 1\}$. This is quite similar to the approach proposed by Gupta et al. \cite{gupta2024language} except we train the classifier with a cost-sensitive loss 
\end{itemize}

Similarly for token-wise deferral, if $r_j(x, \wh y_{<j})$ exceeds threshold $\tau$, the expert is called to predict the next token. $r_j$ can be modeling in the following ways:

\begin{itemize}
    \item \texttt{TokenwiseSoftmax}: $r_j^{\text{softmax}}(x, \wh y_{<j}) = -\log p_h(\wh y_j \mid \wh y_{<j}, x)$
    \item \texttt{TokenwiseEntropy}: $r_j^{\text{entropy}}(x, \wh y_{<j}) = H(\wh Y_j \mid \wh y_{<j}, x)$
\end{itemize}

Since $r(x) = \argmax_{j \in \calJ} g_j(x)$ for one-time deferral, we consider the following the scoring functions $g_j$:

\begin{itemize}
    \item \texttt{OneTimeSoftmax}: $g_j^{\text{softmax}}(x) = -\log p_h(\wh y_j \mid \wh y_{<j}, x)$
    \item \texttt{OneTimeEntropy}: $g_j^{\text{entropy}}(x, \wh y_{<j}) = H(\wh Y_j \mid \wh y_{<j}, x)$
\end{itemize}

\section{Training Details and Hyperparameters} \label{sec:trainingdetails}

All the models were implemented using PyTorch and all the models were trained on an Nvidia A40 GPU. They were trained using Early stopping and 
\Cref{tab:hpsettings} contains all hyperparameter details.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \toprule
        Task/Hyperparameters & TSP-OneTime & XSUM-Tokenlevel & XSUM-OneTime \\
        \hline \hline
        Learning rate & 5e-4 & 1e-3 & 5e-4  \\
        Early Stopping (patience, delta) & 20, 1e-4  & 7, 1e-4 & 20, 1e-4  \\ 
        Architecture & MLP & LSTM & MLP   \\
        Hidden layers & 1 & 3 & 1  \\
        Hidden units & 8 & 128 & 8   \\
        Dropout & 0.2 & 0.4 & 0.2   \\
        Epochs & 200 & 100 & 200  \\
        Training Samples & 2400 & 800 & 2400  \\
        Gradient Clipping Norm & 1.0 & 1.0 & 1.0  \\
        Weight Decay & 0.005 & 0.001 & 0.005  \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameter settings}
    \label{tab:hpsettings}
\end{table}

\textbf{TSP}: Using the RL4CO library \cite{berto2023rl4co}, TSP graphs with $50$ nodes were generated by sampling the node coordinates from a standard normal distribution. These samples were fed into a PointerNet (predictor) trained with $10000$ samples. The node embeddings, graph context, and log scores from the PointerNet served as inputs for the One-Time rejector. This rejector only took a maximum of $10$ minutes to train.

\textbf{XSUM}: During training and inference, Tokenlevel rejector was only exposed to samples with summaries of less than $15$ words. The model was trained without teacher forcing. It took about $5$ hours to train. The token level rejector $r_1$ recieves the encoder hidden states from the predictor as hidden states and the decoder hidden states as input. This continues in an autoregressive fashion. We defer the reader to \Cref{sec:tokenwiseablation} for ablation studies on various training strategies for the tokenlevel rejector.

The OneTime rejector was trained on a dataset with summaries containing upto $20$ words and it took a couple of minutes to train after a dataset of all the varying mixtures of predictions was created (this took a couple of hours to make). This rejector takes the encoder hidden states, decoder hidden states, and log scores from the predictor as input. 


\section{Effects of $\alpha_j$} \label{sec:alphaj}

In experiments, we determine the cost of deferring a complete prediction and call it $\alpha_1$. 
%$\alpha_j$ was determined by the median difference between the expert accuracy and predictor accuracy on the train set. Let the median difference be $\alpha_{1}$. 
Then, we set $\alpha_j = \frac{L - j + 1}{L}\alpha_1$, making the cost of deferring a token $\frac{\alpha_1}{L}$ or $\alpha_{L}$. We vary $\alpha_1$ and \Cref{fig:varyingcosts} shows how the costs affect cost-accuracy tradeoffs. For both experiments, extreme values for cost often result in poor cost-accuracy tradeoff. For TSP, the results are best when $\alpha_L = 0.01$ and for XSUM, it is either $\alpha_L = 0.01$ or $\alpha_L = 0.015$. 

As a general rule of thumb, we propose that $\alpha_1$ should be determined by the median difference between the expert accuracy and predictor accuracy on the train set.

\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.8\linewidth]{imgs/effectofcostimprov.png}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.8\linewidth]{imgs/effectofcost.png}
    \end{subfigure}
    \caption{Plots of Size of $\calJ$ against AUDC and percentage improvement in AUDC relative to the random baseline for TSP (on the left) and XSUM (on the right) for models trained with different $\alpha_1$ value. The costs displayed in the legend refer to the deferral cost per token or $\alpha_L$}
    \label{fig:varyingcosts}
\end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{imgs/effectofcost.png}
%     \caption{Plots of Size of $\calJ$ against AUDC for TSP (on the left) and XSUM (on the right) for models trained with different $\alpha_1$ value. The costs displayed in the Legend refer to the deferral cost per token or $\alpha_L$}
%     \label{fig:varyingcosts}
% \end{figure}

\section{AUDC and Percent Improvement for Size of $\calJ$ experiment} \label{sec:numexperts}

In support of \Cref{fig:aucvsnumclass}, \Cref{tab:aucscoresXSUMNumexperts} and \Cref{tab:aucscoresTSPNumexperts} report the actual values of AUDC and percent improvement in AUDC over the random baseline for each size of $\calJ$ considered for both TSP and XSUM.
\begin{table}
	\begin{center}
    		\begin{tabular}{|c|c|c|c|c|c|c|}
			\toprule
			Size of $\calJ$ & \multicolumn{2}{|c|}{OneTimeModel (Ours)} & \multicolumn{2}{|c|}{OneTimeSoftmax} & \multicolumn{2}{|c|}{OneTimeEntropy} \\
			\midrule
            & AUDC & \% Improvement  & AUDC & \% Improvement & AUDC & \% Improvement\\
            \midrule
            5 & 287.37(7.67) & 6.13(1.68) & 322.08(18.24) & -5.19(4.99) & 316.76(19.90) & -3.43(5.30) \\
			10 & 277.44(10.59) & 9.38(2.54) & 318.00(12.62) & -3.86(2.95) & 317.84(22.77) & -3.78(6.42) \\
			15 & 277.62(9.15) & 9.32(1.85) & 320.45(6.89) & -4.68(1.32) & 317.87(12.57) & -3.83(3.22) \\
			20 & 273.34(11.13) & 10.74(2.34) & 321.35(19.06) & -4.96(5.43) & 320.54(22.24) & -4.70(6.68) \\
			25 & 273.31(12.20) & 10.73(3.11) & 318.49(12.65) & -4.03(3.18) & 321.32(13.95) & -4.97(4.10) \\
			30 & 271.10(9.77) & 11.45(2.16) & 319.61(10.82) & -4.39(1.99) & 321.67(14.16) & -5.05(3.11) \\
			35 & 273.30(6.76) & 10.72(1.69) & 316.40(15.66) & -3.36(4.55) & 320.51(20.33) & -4.70(6.20) \\
			40 & 272.57(10.08) & 10.97(2.53) & 318.99(14.07) & -4.18(3.49) & 320.91(15.41) & -4.82(4.16) \\
			45 & 270.92(10.67) & 11.52(2.41) & 318.52(14.10) & -4.04(3.71) & 322.02(14.99) & -5.19(4.31) \\
			51 & 272.64(9.11) & 10.95(2.06) & 318.96(14.78) & -4.17(3.66) & 322.91(14.05) & -5.50(4.39) \\
			% 5 & 287.37(7.67) & 322.08(18.24) & 316.76(19.90) \\
			% 10 & 277.44(10.59) & 318.00(12.62) & 317.84(22.77) \\
			% 15 & 277.62(9.15) & 320.45(6.89) & 317.87(12.57) \\
			% 20 & 273.34(11.13) & 321.35(19.06) & 320.54(22.24) \\
			% 25 & 273.31(12.20) & 318.49(12.65) & 321.32(13.95) \\
			% 30 & 271.10(9.77) & 319.61(10.82) & 321.67(14.16) \\
			% 35 & 273.30(6.76) & 316.40(15.66) & 320.51(20.33) \\
			% 40 & 272.57(10.08) & 318.99(14.07) & 320.91(15.41) \\
			% 45 & 270.92(10.67) & 318.52(14.10) & 322.02(14.99) \\
			% 51 & 272.64(9.11) & 318.96(14.78) & 322.91(14.05) \\
			\bottomrule
		\end{tabular}
		% \begin{tabular}{|c|c|c|c|}
		% 	\toprule
		% 	Num Classes & Model (Ours) & OneTimeSoftmax & OneTimeEntropy \\
  %               \midrule
		% 	2 & 29932.66(104.53) & 30094.21(109.35) & 30094.21(109.35) \\
		% 	5 & 29993.07(112.59) & 30169.88(170.30) & 30116.84(124.40) \\
		% 	10 & 29938.97(134.38) & 30177.98(122.45) & 30144.14(116.12) \\
		% 	15 & 29931.81(118.87) & 30202.71(106.70) & 30148.11(72.76) \\
		% 	20 & 29911.86(132.96) & 30192.00(129.16) & 30144.42(165.71) \\
		% 	25 & 29913.51(142.93) & 30178.59(118.66) & 30140.82(81.54) \\
		% 	30 & 29907.18(124.00) & 30214.50(128.47) & 30189.05(109.37) \\
		% 	35 & 29907.37(109.16) & 30198.87(125.51) & 30168.41(121.45) \\
		% 	40 & 29902.32(126.83) & 30162.13(118.22) & 30209.54(105.33) \\
		% 	45 & 29895.26(132.40) & 30166.36(130.65) & 30173.76(119.16) \\
		% 	51 & 29910.41(127.81) & 30189.45(109.04) & 30224.44(87.31) \\
  %           \bottomrule
		% \end{tabular}
	\end{center}
	\caption{AUDC scores and Percent improvement over the random baseline in TSP of various sizes of $\calJ$ with standard deviations in brackets}
	\label{tab:aucscoresTSPNumexperts}
\end{table}
\begin{table}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\toprule
			Size of $\calJ$ & \multicolumn{2}{|c|}{OneTimeModel (Ours)} & \multicolumn{2}{|c|}{OneTimeSoftmax} & \multicolumn{2}{|c|}{OneTimeEntropy} \\
			\midrule
            & AUDC & \% Improvement  & AUDC & \% Improvement & AUDC & \% Improvement\\
            \midrule
            3 & 1412.16(26.70) & 1.54(0.88) & 1444.68(16.16) & -0.74(0.77) & 1441.52(18.29) & -0.52(0.61) \\
			4 & 1398.39(25.51) & 2.50(0.80) & 1448.43(23.88) & -1.00(0.91) & 1447.35(24.59) & -0.92(1.02) \\
			5 & 1407.01(19.33) & 1.89(0.43) & 1457.76(23.95) & -1.65(1.11) & 1450.36(22.32) & -1.13(1.04) \\
			7 & 1406.88(19.64) & 1.90(0.79) & 1448.54(20.05) & -1.01(0.97) & 1439.58(16.68) & -0.39(1.20) \\
			10 & 1407.96(19.47) & 1.83(0.34) & 1452.68(15.51) & -1.31(1.44) & 1457.43(15.44) & -1.63(1.29) \\
			13 & 1408.87(23.47) & 1.76(0.71) & 1445.25(12.84) & -0.80(1.78) & 1450.64(14.74) & -1.17(1.72) \\
			15 & 1408.94(16.75) & 1.75(0.68) & 1453.11(6.66) & -1.34(1.46) & 1460.17(14.20) & -1.83(1.43) \\
			17 & 1418.97(16.00) & 1.05(0.59) & 1450.95(11.65) & -1.19(1.63) & 1457.15(10.29) & -1.62(1.52) \\
			21 & 1417.50(26.37) & 1.17(0.68) & 1452.79(15.86) & -1.31(1.30) & 1447.35(12.72) & -0.93(1.20) \\
			% 3 & 1412.16(18.78) & 1451.48(17.94) & 1451.62(23.34) \\
			% 4 & 1398.39(22.84) & 1457.31(20.05) & 1455.18(28.33) \\
			% 5 & 1407.01(21.83) & 1465.30(24.90) & 1468.98(20.41) \\
			% 7 & 1406.91(21.92) & 1449.33(18.03) & 1461.36(21.42) \\
			% 10 & 1407.96(24.74) & 1465.59(37.01) & 1457.96(25.87) \\
			% 13 & 1408.87(22.02) & 1456.15(23.98) & 1449.98(15.77) \\
			% 15 & 1408.94(28.89) & 1451.71(14.25) & 1450.29(12.88) \\
			% 17 & 1418.97(20.65) & 1452.93(19.61) & 1453.93(28.53) \\
			% 21 & 1417.50(24.48) & 1452.33(24.08) & 1454.80(24.13) \\
			\bottomrule
		\end{tabular}
	\end{center}
	\caption{AUDC scores and Percent improvement over the random baseline in XSUM of various sizes of $\calJ$ with standard deviations in brackets}
	\label{tab:aucscoresXSUMNumexperts}
\end{table}

\section{Token-level Rejector Ablation} \label{sec:tokenwiseablation}

$\calL^{\text{Token}}$ bears resemblance to a weighted multi-label classification loss function. Specifically, the token-level rejector follows a classifier chain approach \cite{read2021classifier}. Classifier chains is a multi-label classification (MLC) approach that involves training $L$ binary classifiers in a pre-specified order with the $j^{\text{th}}$ binary classifier taking the feature vector and the previous $j - 1$ labels as input, forming a chain of predictors. At inference time, the test features are fed into the chain such that the previous label predictions are appended to the features for the next label classifier. Our token-level rejector $r_j$ implicitly makes these chained predictions by taking in $\wh y_{<j}$ which is informed by the previous $(j - 1)$ rejectors' rejection decision.

With this comparison established, model architectures and training techniques from classifier chain literature can be utilized. Recurrent classifier chains \cite{nam2017maximizing} have emerged as a popular choice for MLC as they efficiently maximize accuracy without requiring the large number of trainable parameters typically needed for $K$ separate binary classifiers. 

Since the inputs of recurrent classifier chains at training time are typically ground truth labels, training token-level rejectors would similarly require using the correct rejection decision, $\mathbbm{1}_{l\left(y, \wh y_{j}^h \right) > c_j(x, \wh y_{<j}, y)}$, to determine the $j^{\text{th}}$ token prediction for the next label rejector. Following this strategy, also called ``teacher forcing'' \cite{williams1989learning}, can often cause a distribution shift, as label predictions will be used at test time instead of the correct label, resulting in errors propagating through the chain. Scheduled sampling \cite{bengio2015scheduled} offers a middle ground by labeling the training data with the correct rejection decision with probability $p$, and otherwise using the predicted one. This probability $p$ often decays at a linear or exponential rate over epochs, gradually reducing the model's dependence on ground-truth rejection rules. 


We use both Multi Layer Perceptrons (MLP) and Long Short-term memory (LSTM) models to perform token-level deferrals on XSUM data. All models are trained with teacher forcing, without teacher forcing, or with scheduled sampling.  The Scheduled sampler decayed the teacher forcing probability exponentially by $0.95$ with every epoch and the decay process stopped when the teacher forcing probability was $0.5$. The sampler has $5$ warmup epochs where the teacher forcing probability was $1.0$ before the decay started. The coin flip of whether or not to teacher force occurred on a token-level, as opposed to a batch-level. 
\Cref{fig:architectablation} shows the superiority of recurrent models like LSTMs over MLPs with teacher forcing adversely affecting their performance. The selected model is an LSTM trained without teacher forcing. We present the AUDC scores for each of the models in \Cref{tab:aucscoresarch}.


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{imgs/newssummablation.png}}
\caption{Cost-Loss plots for XSUM for various model architectures and training strategies.}
\label{fig:architectablation}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}
	\begin{center}
\begin{tabular}{|c|c|c|}
			\toprule
			Method & AUC Score & \% Improvement \\
			\midrule
			MLP No Teacher Forcing & 1375.48 (34.57) & 4.07 (1.01)\\
			MLP Teacher Forcing & 1387.49 (25.96) & 3.22 (1.25) \\
			MLP Scheduled Sampling & 1377.01 (31.92) & 3.96 (1.20) \\
			LSTM No Teacher Forcing & 1323.92 (25.34) & 7.66 (0.96) \\
			LSTM Teacher Forcing & 1340.43 (20.24) & 6.49 (1.24)\\
            LSTM Scheduled Sampling & 1324.83 (27.04) & 7.60 (0.98)\\
            TokenwiseSoftmax & 1354.78 (31.87) & 5.51 (0.94)\\
			TokenwiseEntropy & 1360.41 (31.50) & 5.12 (0.62)\\
            \bottomrule
		\end{tabular}
	\end{center}
	\caption{AUC Scores for various Token-level Rejector architecturs and training}
	\label{tab:aucscoresarch}
\end{table}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
