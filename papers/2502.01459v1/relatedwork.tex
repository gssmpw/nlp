\section{Related Works}
\label{sec:relworks}

\textit{Model Cascades} \cite{viola2001rapid} share similarities with the learning to defer framework. In cascades, an inference task is progressively handled by a series of models where easier tasks are completed by earlier models in the sequence and complex cases escalate to subsequent models. Similar to confidence-based L2D methods described in \Cref{sec:l2d}, cascading decisions are typically determined by a thresholding deferral rule \cite{yue2023large}. Some recent work on applying cascades to large language models \cite{gupta2024language,wang2024cascade} has also explored modeling the cascade decision maker. However, model cascades, just like learning to defer methods, ultimately rely on a single model for each prediction. Our work extends cascades by introducing partial deferrals, allowing two models to collaborate on a prediction. Narasimhan et al. \cite{narasimhan2024faster} also proposes a token-level deferral rule, but their deferral rule compares the predictor's confidence with expert's confidence at every token position, which requires many expensive expert queries. Our work learns the expert costs at training time to avoid such expert queries unless the rejector calls for it.

\textit{Query Routing} methods select a model that provide the highest quality prediction from an ensemble of models. Some query routing algorithms in large language models \cite{jiang2023llm,chen2023frugalgpt} often require querying either a part or the entire model ensemble to determine the best generation. Some methods train query routing models using specially designed loss functions \cite{shnitzer2023large,lu2023routing,ding2024hybrid}; these loss functions, however, lack consistency guarantees. Our work not only presents surrogate losses with stronger statistical guarantees but also offers more granular routing decisions.
% The one-time deferral is most closely aligned with the \textit{learning to defer with multiple experts} \cite{verma2023learning,mao2024regression}. In this framework, the rejector must decide whether to defer and to which expert to defer. One-time deferral can be viewed as a special case of multiple-expert deferral where each ``expert'' mixes subsequences of different sizes from the predictor with the expert's prediction.  Our contribution lies in expanding tools from multi-expert deferral literature to partial rejections.

Many L2D methods draw inspiration from the \textit{Cost-Sensitive learning} framework. In the example-dependent cost-sensitive setting, each feature $x$ is paired with a cost vector. A cost-sensitive classifier must then learn to identify the index of the cost vector that minimizes the expected cost. Classifiers \cite{abe2004iterative,lin2015reduction} are often trained with a weighted logistic loss or weighted cross entropy loss function, similar to $\calL^{\text{Token}}$ and $\calL^{\text{OneTime}}$. However, popular methods \cite{tu2010one,chung2015cost} use losses that are inconsistent with the original cost-sensitive loss.
% L2D for $E$ experts seeks to minimize the following loss function:
% \begin{equation} \label{eq:l2dmultiple}
%     \begin{split}
%             &l_{\text{L2DMulti}}\left(h, r, x, y, \{m_e\}_{e = 1}^{E} \right) \\
%             &=l(x, y, h(x))\mathbbm{1}_{r(x) = 0} + \sum_{e = 1}^{E}l_{\exp}(x, y, m_e)\mathbbm{1}_{r(x) = e}
%     \end{split}
% \end{equation}
% where $m_e$ is the expert prediction from expert $e$. The rejector in this scenario now must decide whether to defer and to which expert to defer to.