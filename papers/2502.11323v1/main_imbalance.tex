\special{dvipdfmx:config z 0}

\documentclass[11pt]{article}

\input{macros}
\usepackage{mathrsfs}
\usepackage{setspace} \singlespacing
\allowdisplaybreaks
%\usepackage{todonotes}
\usepackage{comment}
%\usepackage[inline]{showlabels}
\usepackage[normalem]{ulem}
\usepackage{mlmodern}
% Fix a issue in lmodern
% https://tex.stackexchange.com/questions/137141/strange-behaviour-of-binomial-coefficients-delimiters
\DeclareSymbolFont{largesymbols}{OMX}{cmex}{m}{n}

\usepackage{etoc} % separate TOC


\newcommand\KZ[1]{\textcolor{teal}{[KZ: #1]}}

\newcommand{\Err}{\mathrm{Err}}
\providecommand{\keywords}[1]
{
  {
  \small	
  \textbf{\textit{Keywords---}} #1
  }
}

\begin{document}


\title{A statistical theory of overfitting for imbalanced classification}
 
% old title
% \title{A modern statistical theory for imbalanced binary classification} 

%%%%% Alternative titles

%\title{The impact of overfitting in logistic regression with data imbalance}

%\title{A modern statistical theory for imbalanced binary classification}

%\title{The impact of overfitting on classification with data imbalance}

%"A Statistical Framework for Understanding Overfitting in Imbalanced Classification"

%"Theoretical Insights into Overfitting in Imbalanced Classification"

%"A Statistical Theory of Overfitting in Imbalanced Classification Models"

%"Exploring Overfitting in Imbalanced Classification: A Statistical Perspective"

%"Towards a Statistical Theory of Overfitting in Imbalanced Classification"


\author{Jingyang Lyu, Kangjie Zhou, and Yiqiao Zhong}
\author{Jingyang Lyu\thanks{Department of Statistics, University of Wisconsin--Madison, Madison, WI 53706, USA. Emails:
\texttt{jlyu55@wisc.edu}, \texttt{yiqiao.zhong@wisc.edu}} \and Kangjie Zhou\thanks{Department of Statistics, Columbia University, New York, NY 10027, USA. Email: \texttt{kz2326@columbia.edu}}
\and Yiqiao Zhong\footnotemark[1]}
\date{\today}


\pagenumbering{arabic}
\maketitle

\begin{abstract}
Classification with imbalanced data is a common challenge in data analysis, where certain classes (minority classes) account for a small fraction of the training data compared with other classes (majority classes). Classical statistical theory based on large-sample asymptotics and finite-sample corrections is often ineffective for high-dimensional data, leaving many overfitting phenomena in empirical machine learning unexplained.
%, which are ubiquitous in modern data analysis. %Indeed, in single-cell omics and deep learning, the feature vectors are often of hundreds or thousands dimensions. 
%Despite recent advances in high-dimensional asymptotics, overfitting in imbalanced classification is not well understood. For example, it is unclear why the logit distribution appear distorted, and why the accuracy of the minority class is more severely affected by overfitting. 

In this paper, we develop a statistical theory for high-dimensional imbalanced classification by investigating support vector machines and logistic regression. We find that dimensionality induces truncation or skewing effects on the logit distribution, which we characterize via a variational problem under high-dimensional asymptotics. In particular, for linearly separable data generated from a two-component Gaussian mixture model, the logits from each class follow a normal distribution $\normal(0,1)$ on the testing set, but asymptotically follow a rectified normal distribution $\max\{\kappa, \normal(0,1)\}$ on the training set---which is a pervasive phenomenon we verified on tabular data, image data, and text data. This phenomenon explains why the minority class is more severely affected by overfitting. Further, we show that margin rebalancing, which incorporates class sizes into the loss function, is crucial for mitigating the accuracy drop for the minority class. Our theory also provides insights into the effects of overfitting on calibration and other uncertain quantification measures.

%Our findings provide novel theoretical explanations for many empirical observations. Based on our theory, we propose practical applications of our analysis in model calibration and feature visualization.

%We characterize the truncation or skewing effects induced by dimensionality on the logit distribution via a variational problem

%on the training data; in particular, for linearly separable data generated from two-component Gaussian mixture model, the logits from each class are distributed as a normal distribution $N(0,1)$ on the test set, but as truncated distribution $\max\{\kappa, N(0,1)\}$ on the training set. 

%leading to overfitting that is more severe for the minority class---which is a pervasive phenomenon we verified on tabular data, image data, and text data. In particular, we characterize overfitting as a variational problem under the high-dimensional asymptotics where the sample size and dimension grow at a proportional ratio. 

%Further, we show that margin rebalancing, which incorporates class size into the loss function, is crucial for remedying the accuracy drop for the minority class, thereby providing theoretical foundation for empirical heuristics. Based on the theory, we suggest the practical implication of our analysis in model calibration and feature visualization.

\end{abstract}

\keywords{
    Imbalanced classification, overfitting, margin, logistic regression, support vector machine, overparametrization, calibration
}

% Main TOC
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{section}
% \setcounter{tocdepth}{2}
\tableofcontents

\newpage
\input{src/intro}
\input{src/related}
\input{src/setup}
\input{src/main_logit_distribution}
\input{src/main_rebalance_margin}
\input{src/main_calibration}
\input{src/discussions}

\section*{Acknowledgments}

Y.Z.~is supported by NSF-DMS grant 2412052 and by the Office of the Vice Chancellor for Research and Graduate Education at the UW Madison with funding from the Wisconsin Alumni Research Foundation. K.Z.~is supported by the Founder's Postdoctoral Fellowship in Statistics at Columbia University. J.L.~is grateful for the feedback from Zhexuan Liu, Zexuan Sun, Zhuoyan Xu, Congwei Yang, Zhihao Zhao, and audience from the Institute for Foundations of Data Science (IFDS) Ideas Forum.

\newpage

\appendix

% Appendix TOC
\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsubsection}
\tableofcontents

% Hide subsection titles in table of contents  
% \addtocontents{toc}{\protect\setcounter{tocdepth}{1}}

\input{src/append_experiments}
\input{src/append_prelim}
\input{src/append_overparam_prf}
\input{src/append_underparam_prf}
\input{src/append_margin_reb}
\input{src/append_high_imb_prf}
\input{src/append_calibration}
\input{src/append_tech_lem}



\newpage
%\bibliographystyle{alpha}
%\bibliographystyle{plain}
\bibliographystyle{unsrt}
\bibliography{refs}
\end{document}
