\section{Preliminaries: Proofs for \Cref{sec:preliminary}}
\label{append_sec:prelim}
% \ljycom{Put it here temporarily...}
% \paragraph{On terminology.}
% \begin{itemize}
%     \item \textbf{Margin distribution}. We need some connection to this widely used terminology in ML literature, for example, \href{https://arxiv.org/abs/math/0405343}{[a]}
%     \href{https://arxiv.org/abs/1009.3613}{[b]}
%     \href{https://arxiv.org/abs/1706.08498}{[c]}
%     \href{https://arxiv.org/abs/1810.00113}{[d]}, 
%     which is defined as the empirical distribution of $y_i \hat f(\xx_i)$ in binary classification. But we prefer to call it logit distribution, to distinguish between logit and margin.
    
%     \item Indeed, people mix up the two meanings of the word \textbf{margin} (margin of an example v.s. margin for class), e.g., \cite{cao2019learning}.
% \end{itemize}

% \begin{rem}
%     Sometimes people informally (but incorrectly) call it ``truncated Gaussian distribution'', which in fact refers to the conditional distribution of $Z \,|\, Z \ge \kappa$ or $Z \,|\, Z \le \kappa$. More precisely, the rectified Gaussian distribution is essentially a mixture of a discrete distribution $\delta_\kappa$ and a continuous distribution (truncated Gaussian distribution with interval $(\kappa, \infty)$ or $(-\infty, \kappa)$). 
% \end{rem}

We first introduce some technical adjustments and terms that are used in our proofs.
\paragraph{Well-definedness of margin.}
We define the \emph{margin} of linear classifier $x \mapsto 2\ind\{ f(\xx) > 0 \} -1$ with $f(\xx) = \< \xx, \vbeta \> + \beta_0$ as 
\begin{equation}\label{eq:margin}
	\kappa = \kappa(\vbeta, \beta_0) := \min_{i \in [n]} \tilde{y}_i ( \< \xx_i, \bbeta \> + \beta_0 ),
\end{equation}
which is the objective of margin-rebalanced SVM \cref{eq:SVM-m-reb} and \eqref{eq:SVM}. Note there is a minor caveat about the one-class degenerate case, which is ignored in the main text for simplicity. When $n_+ = 0$ or $n$ (this happens with nonzero probability for any fixed $n$), we have $\kappa(\hat\vbeta, \hat\beta_0) = \infty$. It motivates us to redefine the maximum margin properly.
\begin{defn}\label{def:max-margin}
	The \emph{well-defined maximum margin} is
	\begin{equation}\label{eq:kappa_hat_def}
		\hat\kappa  := 
		\begin{cases} 
			\ \kappa(\hat\vbeta, \hat\beta_0) = \displaystyle\min_{i \in [n]} \tilde{y}_i ( \< \xx_i, \hat\bbeta \> + \hat\beta_0 ) , 
			& \ \text{if} \ 1 \le n_+ \le n - 1, \\
			\ 0    , 
			& \ \text{if} \ n_+ = 0 \text{ or } n. 
		\end{cases}
	\end{equation}
\end{defn}
Therefore, $\hat\kappa$ above is a proper random variable and $\hat\kappa \ge 0$ always holds\footnote{For degenerate case ($n_+ = 0$ or $n$), the dataset is considered as linearly separable.}. Further, $\hat{\kappa} = \kappa(\hat\vbeta, \hat\beta_0)$ with high probability as $n \to \infty$. We will apply similar adjustments to the definition of ELD in \cref{append_sec:sep} and elsewhere, whenever required for the proof.


\paragraph{Support vectors.} Consider the non-degenerate case ($1 \le n_+ \le n - 1$). To study the properties of optimal solution $(\hat\vbeta, \hat\beta_0, \hat\kappa)$ from a non-asymptotic perspective, we inherit the concept of \emph{support vectors} from SVM. Define the \emph{support vector of a linear classifier} $2\ind\{ \< \xx, \vbeta \> + \beta_0 > 0 \} -1$ as the vector(s) $\xx_i$ which attain(s) the smallest (rebalanced) logit margin $\wt y_i(\< \xx_i, \vbeta \> + \beta_0)$ from each class. Namely,
\begin{equation}\label{eq:SV_def}
	\begin{aligned}
		\mathcal{SV}_+ = \mathcal{SV}_+(\vbeta) & :=  \argmin_{i: y_i = +1} \wt y_i(\< \xx_i, \vbeta \> + \beta_0)
	= \argmin_{i: y_i = +1} + \< \xx_i, \vbeta \>, \\
		\mathcal{SV}_- = \mathcal{SV}_-(\vbeta) & :=  \argmin_{i: y_i = -1} \wt y_i(\< \xx_i, \vbeta \> + \beta_0)
	= \argmin_{i: y_i = -1} - \< \xx_i, \vbeta \>, \\
	\end{aligned}
\end{equation}
where $\mathcal{SV}_+, \mathcal{SV}_-$ are sets of (the indices of) \emph{positive} and \emph{negative support vectors}. A key observation from \cref{eq:SV_def} is that support vectors only depend on the data and parameter $\vbeta$, not $\beta_0$ or $\tau$.\footnote{Hence, we can view $\mathcal{SV}_\pm(\vbeta)$ as a mapping from $\R^d$ to the power set of $\{i: y_i = \pm 1\}$.} Let $\mathsf{sv}_+(\vbeta)$ and $\mathsf{sv}_-(\vbeta)$ be any element in $\mathcal{SV}_+(\vbeta)$ and $\mathcal{SV}_-(\vbeta)$, i.e.,
\begin{equation*}
	\mathsf{sv}_+(\vbeta) \in \mathcal{SV}_+(\vbeta),
	\qquad
	\mathsf{sv}_-(\vbeta) \in \mathcal{SV}_-(\vbeta),
\end{equation*} 
which are (the indices of) arbitrary positive and negative support vectors (only depends on $(\XX, \yy)$ and $\vbeta$). In particular, $\mathsf{sv}_+(\hat\vbeta) \in \mathcal{SV}_+(\hat\vbeta)$, $\mathsf{sv}_-(\hat\vbeta) \in \mathcal{SV}_-(\hat\vbeta)$ are support vectors of the max-margin classifier $2 \ind\{\< \xx, \hat\vbeta \> + \hat\beta_0 > 0\} - 1$, which aligns with the definition of support vectors in SVM.

\subsection{Proof of \cref{prop:SVM_tau_relation}}

The lemma below summarizes some important properties of the max-margin solution \cref{eq:SVM-m-reb} characterized by support vectors, which is a stronger statement than \cref{prop:SVM_tau_relation}.

\begin{lem}\label{lem:indep_tau}
	For non-degenerate case, let $(\hat\vbeta, \hat\beta_0, \hat\kappa)$ be an optimal solution to \cref{eq:SVM-m-reb}. Then
	\begin{enumerate}[label=(\alph*)]
		\item \label{lem:indep_tau(a)}
            $\hat\vbeta$ does NOT depend on $\tau$, and
		\begin{equation}\label{eq:kappa_hat}
			(\tau + 1) \hat\kappa = 
			\max_{\vbeta \in \S^{d-1}} \< \xx_{\mathsf{sv}_+(\vbeta)} - \xx_{\mathsf{sv}_-(\vbeta)}, \vbeta \>
			= 
			\< \xx_{\mathsf{sv}_+(\hat\vbeta)} - \xx_{\mathsf{sv}_-(\hat\vbeta)}, \hat\vbeta \>.
		\end{equation}
		\item \label{lem:indep_tau(b)}
            $\hat\beta_0$ depends on $\tau$ by
		\begin{equation}\label{eq:beta0_hat}
			\hat\beta_0 = -\frac{\tau \< \xx_{\mathsf{sv}_-(\hat\vbeta)}, \hat\vbeta \> + \< \xx_{\mathsf{sv}_+(\hat\vbeta)}, \hat\vbeta \>}{\tau + 1}.
		\end{equation}
		\item \label{lem:indep_tau(c)}
            If the data are linearly separable, then $(\hat\vbeta, \hat\beta_0)$ must be unique.
	\end{enumerate}
\end{lem}

\begin{proof}
	For any feasible solution $(\vbeta, \beta_0)$ of \cref{eq:SVM}, we denote the \emph{positive} and \emph{negative margin} of the classifier $\xx \mapsto 2\ind\{\< \xx, \vbeta \> + \beta_0 > 0 \}  -1$ as
	\begin{equation}\label{eq:margin_pm}
		\begin{aligned}
			\kappa_{+}(\vbeta, \beta_0)
			& := \min_{i: y_i = +1}\wt y_i(\< \xx_i, \vbeta \> + \beta_0)
			= \tau^{-1}(\< \xx_{\mathsf{sv}_+(\vbeta)}, \vbeta \> + \beta_0),  
			\\
			\kappa_{-}(\vbeta, \beta_0)
			& := \min_{i: y_i = -1}\wt y_i(\< \xx_i, \vbeta \> + \beta_0)
			= \mathmakebox[\widthof{$\tau^{-1}$}][r]{-\,} (\< \xx_{\mathsf{sv}_-(\vbeta)}, \vbeta \> + \beta_0).
		\end{aligned}
	\end{equation}
	%We apply the method of profiling for maximizing $\kappa(\vbeta, \beta_0)$. 
    According to \cref{eq:SVM}, we have
	\begin{equation*}
		\hat\vbeta = \argmax_{\vbeta \in \S^{d-1}} \kappa(\vbeta, \check\beta_0(\vbeta)) = \argmax_{\vbeta \in \S^{d-1}} \min_{i \in [n]} \tilde{y}_i ( \langle \xx_i, \bbeta \rangle + \check\beta_0(\vbeta) ),
	\end{equation*}
	where
	\begin{equation}
		\label{eq:beta0_optim}
		\begin{aligned}
			\check\beta_0(\vbeta) 
		: \! & = \argmax_{\beta_0 \in \R} \kappa(\vbeta, \beta_0)
		= \argmax_{\beta_0 \in \R} \min\limits_{i \in [n]} \tilde{y}_i ( \langle \xx_i, \bbeta \rangle + \beta_0 ) \\
		& = \argmax_{\beta_0 \in \R} \Bigl\{ \min_{i: y_i = +1}\wt y_i(\< \xx_i, \vbeta \> + \beta_0), \min_{i: y_i = -1}\wt y_i(\< \xx_i, \vbeta \> + \beta_0) \Bigr\}
		\\
		& = \argmax_{\beta_0 \in \R} \, \min\left\{ 
			\kappa_{+}(\vbeta, \beta_0),
			\kappa_{-}(\vbeta, \beta_0)
		 \right\}.
		\end{aligned}
	\end{equation}
	Here, $\check\beta_0(\vbeta)$ can be viewed as the optimal intercept for a linear classifier with slope given by $\vbeta$. 
	
	\vspace{0.5\baselineskip}
	\noindent
	\textbf{\ref{lem:indep_tau(b)}:}
	As defined in \cref{eq:margin_pm}, note $\min\{ 
		\kappa_{+}(\vbeta, \beta_0),
		\kappa_{-}(\vbeta, \beta_0) \}$ 
	is a piecewise linear concave function of $\beta_0$. Therefore, $\check\beta_0(\vbeta)$ must satisfy the \emph{margin-balancing} condition\footnote{
		As we have seen, the margin-balancing condition holds regardless of the sign of margin. It holds even if the data is not linearly separable.
	}, i.e., 
	 \begin{equation}\label{eq:margin-bal}
		\kappa_{+}(\vbeta, \check\beta_0(\vbeta)) = \kappa_{-}(\vbeta, \check\beta_0(\vbeta))
		= \kappa(\vbeta, \check\beta_0(\vbeta)).
	 \end{equation}
	In particular, recall that $\check\beta_0(\hat\vbeta) = \hat\beta_0$, then $\kappa_{+}(\hat\vbeta, \hat\beta_0) = \kappa_{-}(\hat\vbeta, \hat\beta_0)$. Substitute this back to \cref{eq:margin_pm} deduce
	\begin{equation*}
		\tau^{-1}(\< \xx_{\mathsf{sv}_+(\hat\vbeta)}, \hat\vbeta \> + \hat\beta_0)  
			= -(\< \xx_{\mathsf{sv}_-(\hat\vbeta)}, \hat\vbeta \> + \hat\beta_0),
	\end{equation*}
	which uniquely solves the expression for $\hat\beta_0$ in \cref{eq:beta0_hat}. This concludes the proof of part \ref{lem:indep_tau(b)}.

	\vspace{0.5\baselineskip}
	\noindent
	\textbf{\ref{lem:indep_tau(a)}:}
	Next, we show that $\hat\vbeta$ does not depend on $\tau$. According to \cref{eq:margin-bal} and \cref{eq:margin_pm},
	% Denote the optimization problem \cref{eq:SVM-epi} under a given $\tau$ as $\mathscr{P}_\tau$. Now suppose $(\hat\vbeta_{\tau_1}, \hat\beta_{0, \tau_1}, \hat\kappa_1)$ and $(\hat\vbeta_{\tau_2}, \hat\beta_{0, \tau_2}, \hat\kappa_2)$ are unique solutions to $\mathscr{P}_{\tau_1}, \mathscr{P}_{\tau_2}$, respectively, where $\tau_1 \not= \tau_2$.
	% \begin{equation*}
		\begin{align*}
			\hat\vbeta & = \argmax_{\vbeta \in \S^{d-1}} \kappa(\vbeta, \check\beta_0(\vbeta))  \\
		& =
		\argmax_{\vbeta \in \S^{d-1}} \frac{\tau \kappa_{+}(\vbeta, \check\beta_0(\vbeta)) +
		\kappa_{-}(\vbeta, \check\beta_0(\vbeta))}{\tau + 1} \\
		& = 
		\argmax_{\vbeta \in \S^{d-1}} \frac{\< \xx_{\mathsf{sv}_+(\vbeta)}, 
		\vbeta \> - \< \xx_{\mathsf{sv}_-(\vbeta)}, \vbeta \>}{\tau+1}
		= \argmax_{\vbeta \in \S^{d-1}} \< \xx_{\mathsf{sv}_+(\vbeta)} - \xx_{\mathsf{sv}_+(\vbeta)}, \vbeta \>,
		\end{align*}
	% \end{equation*}
	where $\< \xx_{\mathsf{sv}_+(\vbeta)} - \xx_{\mathsf{sv}_+(\vbeta)}, \vbeta \>$ only depends on $\vbeta$ and $(\XX, \yy)$ by definition. Hence, it deduces
	\begin{equation*}
		\begin{aligned}
			\hat\kappa & = \kappa(\hat\vbeta, \check\beta_0(\hat\vbeta)) 
		=
		\frac{\tau \kappa_{+}(\hat\vbeta, \check\beta_0(\hat\vbeta)) +
		\kappa_{-}(\hat\vbeta, \check\beta_0(\hat\vbeta))}{\tau + 1}
		=
		\frac{\< \xx_{\mathsf{sv}_+(\hat\vbeta)}, 
		\hat\vbeta \> - \< \xx_{\mathsf{sv}_-(\hat\vbeta)}, \hat\vbeta \>}{\tau+1}.
		\end{aligned}
	\end{equation*}
        This concludes the proof of part \ref{lem:indep_tau(a)}.

	\vspace{0.5\baselineskip}
	\noindent
	\textbf{\ref{lem:indep_tau(c)}:}
	Since \cref{eq:SVM-1} is a convex optimization problem with objective function $\norm{\bw}_2^2$, which is strictly convex in $\bw$, by equivalence between \cref{eq:SVM-m-reb}, \eqref{eq:SVM} and \eqref{eq:SVM-1}, we know that $\hat\bw$ and $\hat\vbeta = \hat\bw/\|\hat\bw\|_2$ must be unique. And by \ref{lem:indep_tau(a)}, $\hat\beta_0$ is also unique. This concludes the proof of part \ref{lem:indep_tau(c)}.
\end{proof}

Notice that \cref{lem:indep_tau} will also be used in the proof of \cref{lem:theta_hat_z} for the high imbalance regime. Below, we show that \cref{prop:SVM_tau_relation} is a direct consequence of \cref{lem:indep_tau}.

\begin{proof}[\textbf{Proof of \cref{prop:SVM_tau_relation}}]
We only show the relation on $\hat\kappa(\tau)$ and $\hat\beta_0(\tau)$, while the other results are simply restatements of \cref{lem:indep_tau}. According to \cref{eq:kappa_hat}, for any $\tau$, $(\tau + 1) \hat\kappa(\tau)$ equals a quantity which does not depend on $\tau$. Plugging in $\tau = 1$, we get $(\tau + 1) \hat\kappa(\tau) = 2 \hat\kappa(1)$.

Combining \cref{eq:kappa_hat} and \eqref{eq:beta0_hat}, we can solve
\begin{equation*}
    \< \xx_{\mathsf{sv}_+(\hat\vbeta)}, \hat\vbeta \> = \tau \hat\kappa(\tau) - \hat\beta_0(\tau), 
    \qquad
    \< \xx_{\mathsf{sv}_-(\hat\vbeta)}, \hat\vbeta \> = - \hat\kappa(\tau) - \hat\beta_0(\tau).
\end{equation*}
Notice the above holds for any $\tau > 0$. Taking $\tau = 1$ and substituting it into \cref{eq:beta0_hat}, we get
\begin{equation*}
\hat\beta_0(\tau) = -\frac{\tau \bigl(- \hat\kappa(1) - \hat\beta_0(1)\bigr) + \bigl(\hat\kappa(1) - \hat\beta_0(1) \bigr)}{\tau + 1} = \hat\beta_{0}(1) + \frac{\tau - 1}{\tau + 1} \hat\kappa(1).    
\end{equation*}
This completes the proof.
\end{proof}




\subsection{Proof of \cref{prop:explicit_bias}}
\begin{proof}[\textbf{Proof of \cref{prop:explicit_bias}}]
    Our argument follows the proof of \cite[Theorem 2.1]{Soudry_implicit_bias}. Assume that $\vbeta^*$ is a limit point of $\hat \vbeta_\lambda/\| \hat \vbeta_\lambda \|_2$ as $\lambda \to 0^+$, with $\| \vbeta^* \|_2 = 1$. The existence of $\vbeta^*$ is guaranteed by boundedness. Let $\beta_0^* := \limsup_{\lambda \to 0^+} \hat\beta_{0, \lambda}/\| \hat \vbeta_\lambda \|_2$. Now, suppose the max-margin classifier given by  $(\hat\vbeta, \hat\beta_0)$ (with $\| \hat\vbeta \|_2 = 1$) has a larger margin than $(\vbeta^*, \beta_0^*)$, that is,
    \begin{equation*}
        \kappa(\vbeta^*, \beta_0^*) = \min_{i \in [n]} y_i ( \< \xx_i, \vbeta^* \> + \beta_0^* )
        <
        \kappa(\hat\vbeta, \hat\beta_0) = \min_{i \in [n]} y_i ( \< \xx_i, \hat\vbeta \> + \hat\beta_0 ).
    \end{equation*}
    By continuity of $\kappa(\vbeta, \beta_0)$, there exists some open neighborhood of $(\vbeta^*, \beta_0^*)$:
    \begin{equation*}
        \mathcal{N}_{\vbeta^*, \beta_0^*} := \left\{
        \vbeta \in \R^d, \beta_0 \in R: \|\vbeta\|_2 = 1, \| \vbeta - \vbeta^* \|_2^2 + |\beta_0 - \beta_0^*|^2 < \delta^2 
        \right\}
    \end{equation*}
    and an $\varepsilon > 0$, such that
    \begin{equation*}
        \kappa(\vbeta, \beta_0) = \min_{i \in [n]} y_i ( \< \xx_i, \vbeta \> + \beta_0 ) < \kappa(\hat\vbeta, \hat\beta_0) - \varepsilon, \qquad \forall\, (\vbeta, \beta_0) \in \mathcal{N}_{\vbeta^*, \beta_0^*}.
    \end{equation*}
    Since $\ell$ is rapidly varying, now by \cite[Lemma 2.3]{Soudry_implicit_bias} we know that there exists some constant $T > 0$ (depends on $\kappa(\hat\vbeta, \hat\beta_0)$ and $\varepsilon$), such that
    \begin{equation*}
        \sum_{i=1}^n \ell\bigl(y_i ( \< \xx_i, t \hat\vbeta \> + t \hat\beta_0 ) \bigr)
        <
        \sum_{i=1}^n \ell\bigl(y_i ( \< \xx_i, t \vbeta \> + t \beta_0 ) \bigr)
        , \qquad \forall\, t > T , \  (\vbeta, \beta_0) \in \mathcal{N}_{\vbeta^*, \beta_0^*},
    \end{equation*}
    which implies $(t \hat\vbeta, t \hat\beta_0)$ has a smaller loss \cref{eq:logistic-l2} than $(t \vbeta, t \beta_0)$. This indicates that $\vbeta^*$ cannot be a limit point of $\hat \vbeta_\lambda/\| \hat \vbeta_\lambda \|_2$, which is a contradiction. Hence we must have $\kappa(\vbeta^*, \beta_0^*) = \kappa(\hat\vbeta, \hat\beta_0)$. Replacing $\limsup$ by $\liminf$ in the definition of $\beta_0^*$ gives the same conclusion. Then we complete the proof by noticing the max-margin solution is unique on separable data by \cref{lem:indep_tau}\ref{lem:indep_tau(c)}.
\end{proof}
