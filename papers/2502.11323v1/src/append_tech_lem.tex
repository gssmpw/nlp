\section{Technical Lemmas}
\label{append_sec:tech}

\subsection{Properties of Gaussian random variables}

We need the following variant of Gordon's comparison theorem for Gaussian processes.
\begin{lem}[CGMT]
    \label{lem:CGMT}
    Let $D_{\bu} \subset \R^{n_1 + n_2}$, $D_{\bv} \subset \R^{m_1 + m_2}$ be compact sets and let $Q: D_{\bu} \times D_{\bv} \to \R$ be a continuous function. Let $\GG = (G_{i,j}) \iidsim \normal(0, 1)$, $\vg \sim \normal(\bzero, \bone_{n_1})$, $\hh \sim \normal(\bzero, \bone_{m_1})$ be independent standard Gaussian vectors. For any $\bu \in \R^{n_1 + n_2}$ and $\bv \in \R^{m_1 + m_2}$ we define $\wt\bu = (u_1, \dots, u_{n_1})$ and $\wt\bv = (v_1, \dots, v_{m_1})$. Define
    \begin{equation*}
        \begin{aligned}
            C^*(\GG)      & = \min_{\bu \in D_{\bu}} \max_{\bv \in D_{\bv}}  \wt\bv^\top \GG \wt\bu + Q(\bu, \bv), \\
            L^*(\vg, \hh) & = \min_{\bu \in D_{\bu}} \max_{\bv \in D_{\bv}}  \| \wt\bv \|_2 \vg^\top \wt\bu
            + \| \wt\bu \|_2 \hh^\top \wt\bv + Q(\bu, \bv).
        \end{aligned}
    \end{equation*}
    Then we have:
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:CGMT(a)}
        For all $t \in \R$,
        \begin{equation*}
            \P\left( C^*(\GG) \le t \right) \le 2 \, \P\left( L^*(\vg, \hh) \le t \right).
        \end{equation*}
        \item \label{lem:CGMT(b)}
        If $D_{\bu}$ and $D_{\bv}$ are convex and if $Q$ is convex concave, then for all $t \in \R$,
        \begin{equation*}
            \P\left( C^*(\GG) \ge t \right) \le 2 \, \P\left( L^*(\vg, \hh) \ge t \right).
        \end{equation*}
    \end{enumerate}
\end{lem}
\begin{proof}
    See \cite[Corollary G.1]{miolane2018distributionlassouniformcontrol}.
\end{proof}



\subsection{Properties of sub-gaussian and sub-exponential random variables}

\begin{defn}[Sub-gaussianity]
    \label{def:subgauss}
    The sub-gaussian norm of random variable $X$ is defined as
    \begin{equation*}
        \norm{X}_{\psi_2} := \inf\left\{ K > 0: \E[\exp(X^2/K^2)] \le 2 \right\}.
    \end{equation*}
    \begin{itemize}
        \item A random variable $X \in \R$ is called sub-gaussian if $\norm{X}_{\psi_2} < \infty$.
        \item A random vector $\xx = (X_1, \dots, X_d)^\top \in \R^d$ is called sub-gaussian if $\sup_{\bv \in \S^{d-1}}\norm{\langle \xx, \bv \rangle}_{\psi_2} < \infty$. Specifically, write $\xx \sim \subGind(\bzero, \bI_d; K)$ if
        $X_1, \dots, X_d$ are independent random variables with $\E[X_i] = 0$, $\var(X_i) = 1$,
        % $\xx$ is isotropic mean-zero ($\E[\xx] = \bzero$, $\cov(\xx) = \bI_d$) with independent coordinates, 
        and $\max_{1 \le i \le d} \norm{X_i}_{\psi_2} \lesssim K$. 
    \end{itemize}   
\end{defn}

\noindent
\cref{lem:subG} and \ref{lem:subG_concentrate} summarize some basic facts and concentration inequalities about sub-gaussian random variables and vectors.

\begin{lem}\label{lem:subG}
    Some facts about sub-gaussian 
    % and sub-exponential 
    random variables.
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:subG-a} $\norm{\, \cdot \,}_{\psi_2}$ is a norm on the space of sub-gaussian random variables.
        \item \label{lem:subG-b} Let $X_1, \ldots, X_N$ be independent mean-zero sub-gaussian random variables. Then $\sum_{i=1}^N X_i$ is also a sub-gaussian random variable, and
        \[ \norm{\sum_{i=1}^N X_i}_{\psi_2}^2\le C \sum_{i=1}^N \norm{X_i}^2_{\psi_2}, \]
        where $C$ is an absolute constant.
        \item \label{lem:subG-c} (Maximum) Let $X_1, \ldots, X_N$ be sub-gaussian random variables (not necessarily independent) with $K := \max_{1 \le i \le N} \norm{X_i}_{\psi_2}$. Then
        \[ \E\left[ \max_{1 \le i \le N} \abs{X_i} \right] \le  C K \sqrt{\log N},
        \qquad (N \ge 2), \]
        where $C$ is an absolute constant.
    \end{enumerate}
\end{lem}
\begin{proof}
    See \cite[Exercise 2.5.7, Proposition 2.6.1, Exercise 2.5.10]{vershynin2018high}.
    % , Lemma 2.7.7
\end{proof}

\begin{lem}[Concentration]\label{lem:subG_concentrate}
    Suppose $\xx, \yy \sim \subGind(\bzero, \bI_d; K)$ and $\xx \indep \yy$.
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:subG-Hanson-Wright-I} (Hanson-Wright inequality I)  \  Let $\bA \in \R^{d \times d}$ be a matrix. Then, for every $t \ge 0$,
        \begin{equation*}
           \P\left( \bigl| \xx^\top \bA \xx - \E[\xx^\top \bA \xx] \bigr| \ge t \right)
           \le 2 \exp\biggl( -c \min \biggl\{ \frac{t^2}{K^4 \norm{\bA}_\mathrm{F}^2} , \frac{ t }{ K^2 \| \bA \|_{\mathrm{op}} } \biggr\} \biggr),
        \end{equation*}
        where $c$ is an absolute constant.
        \item \label{lem:subG-Hanson-Wright-II} (Hanson-Wright inequality II) \  Let $\bB \in \R^{d' \times d}$ be a matrix. Then, for every $t \ge 0$,
        \begin{equation*}
            \P\biggl( \biggl| \frac{\| \bB \xx \|_2}{ \| \bB \|_\mathrm{F}} - 1 \biggr| > t \biggr)
            \le 2 \exp\biggl( -\frac{ct^2 \| \bB \|_\mathrm{F}^2 }{K^4 \| \bB \|_{\mathrm{op}}^2 } \biggr),
        \end{equation*}
        where $c$ is an absolute constant. In particular, when $\bB = \bI_d$,
        \begin{equation*}
            \P\biggl( \biggl| \frac{\| \xx \|_2}{ \sqrt{d} } - 1 \biggr| > t \biggr)
            \le 2 \exp\biggl( -\frac{ct^2 d}{K^4} \biggr).
        \end{equation*}

        \item \label{lem:subG-Hoeffding} (Hoeffding's inequality) \ Let $\ba \in \R^{d}$ be a vector. Then, for every $t \ge 0$,
        \begin{equation*}
            \P\biggl( \frac{ \abs{\< \xx, \ba \>} }{ \norm{\ba}_2 }  > t \biggr)
            \le 2 \exp\biggl( -\frac{ct^2}{K^2} \biggr),
        \end{equation*}
        where $c$ is an absolute constant.

        \item \label{lem:subG-Bernstein} (Bernstein's inequality) \ Let $\bB \in \R^{d \times d}$ be a matrix. Then, for every $t \ge 0$,
        \begin{equation*}
            \P\biggl( \frac{ | \xx^\top \bB \yy | }{ \| \bB \|_\mathrm{F} }  > t \biggr)
            \le 2 \exp\biggl( -c \min \biggl\{ \frac{t^2}{K^4} , \frac{ t \| \bB \|_\mathrm{F}}{ K^2 \| \bB \|_{\mathrm{op}} } \biggr\} \biggr),
        \end{equation*}
        where $c$ is an absolute constant. In particular, when $\bB = \bI_d$,
        \begin{equation*}
            \P\biggl( \frac{ \abs{\< \xx, \yy \>} }{ \sqrt{d} }  > t \biggr)
            \le 2 \exp\biggl( -c \min \biggl\{ \frac{t^2}{K^4} , \frac{t\sqrt{d}}{K^2} \biggr\} \biggr).
        \end{equation*}
    \end{enumerate}
\end{lem}
\begin{proof}
    For \ref{lem:subG-Hanson-Wright-I}, \ref{lem:subG-Hanson-Wright-II} and \ref{lem:subG-Hoeffding}, see \cite[Theorem 6.2.1, Theorem 6.3.2, Theorem 2.6.3]{vershynin2018high}. For \ref{lem:subG-Bernstein},
    let $\bar\xx = \begin{pmatrix}
        \xx \\ \yy
    \end{pmatrix}$, $\bar\bA = \dfrac12\begin{pmatrix}
        \bzero & \bB \\
        \bB & \bzero
    \end{pmatrix}$, then apply $\bar\xx^\top \bar\bA \bar\xx = \xx^\top \bB \yy$ to \ref{lem:subG-Hanson-Wright-I} and simplify.
    % notice $X_i Y_i$'s are independent mean-zero sub-exponential random variable with $\max_{1 \le i \le d} \norm{X_i Y_i}_{\psi_1} \le KL$ (by Lemma \ref{lem:subG}\ref{lem:subG-c}), then apply Bernstein's inequality \cite[Theorem 2.8.2]{vershynin2018high}.
\end{proof}

\begin{defn}[Sub-exponentiality]
    The sub-exponential norm of random variable $X$ is defined as
    \begin{equation*}
        \norm{X}_{\psi_1} = \inf\left\{ K > 0: \E[\exp(\abs{X}/K)] \le 2 \right\}.
    \end{equation*}
    \begin{itemize}
        \item A random variable $X \in \R$ is called sub-exponential if $\norm{X}_{\psi_1} < \infty$.
    \end{itemize}   
\end{defn}

\noindent
\cref{lem:subExp} summarizes some basic facts about sub-exponential random variables.
\begin{lem}\label{lem:subExp}
    Some facts about sub-exponential random variables.
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:subExp-a} $\norm{\, \cdot \,}_{\psi_1}$ is a norm on the space of sub-exponential random variables.
        \item \label{lem:subExp-b} Let $X_1, \ldots, X_N$ be independent mean-zero sub-exponential random variables. Then $\sum_{i=1}^N X_i$ is also a sub-exponential random variable. If $K := \max_{1 \le i \le N} \norm{X_i}_{\psi_1}$ and $N \ge C$, then
        \[ 
            \norm{\sum_{i=1}^N X_i}_{\psi_1} \le C' K \sqrt{N},   
        \]
        where $C, C'$ are absolute constants.
        % \item \label{lem:subExp-c} (Centering) If X is a sub-exponential random variable, then
        % \[ \norm{X - \E[X]}_{\psi_1} \le C \norm{X}_{\psi_1}, \]
        % where $C$ is an absolute constant.
        \item \label{lem:subExp-c} (Maximum) Let $X_1, \ldots, X_N$ be sub-exponential random variables (not necessarily independent) with $K := \max_{1 \le i \le N} \norm{X_i}_{\psi_1}$. Then
        \[ \E\left[ \max_{1 \le i \le N} \abs{X_i} \right] \le  C K \log N,
        \qquad (N \ge 2), \]
        where $C$ is an absolute constant.
        \item \label{lem:subExp-d} Let $X$ and $Y$ be sub-gaussian random variables. Then $XY$ is sub-exponential. Moreover,
        \[ \norm{XY}_{\psi_1} \le \norm{X}_{\psi_2} \norm{Y}_{\psi_2}. \]
        In particular, $X^2$ is sub-exponential, and
        \[ \| X^2 \|_{\psi_1} \le \norm{X}_{\psi_2}^2. \] 
    \end{enumerate}
\end{lem}
\begin{proof}
    For \ref{lem:subExp-a} and \ref{lem:subExp-d}, see \cite[Exercise 2.7.11, Lemma 2.7.6, Lemma 2.7.7]{vershynin2018high}. For (b), the proof is analogous to \cite[Proposition 2.6.1]{vershynin2018high}. For any $\abs{\lambda} \le 1/K$, we have
    \begin{equation*}
            \E\biggl[ \exp\biggl(\lambda \sum_{i=1}^N X_i \biggr) \biggr]
             = \prod_{i=1}^{N} \E[\exp(\lambda X_i)]
            \le \prod_{i=1}^{N} \exp\bigl( C \lambda^2 \norm{X_i}_{\psi_1}^2 \bigr)
             \le \exp\bigl( C \lambda^2 N K^2 \bigr),
    \end{equation*}
    where sub-exponential properties \cite[Proposition 2.7.1 (iv)(v)]{vershynin2018high} are used, and $C$ is an absolute constant. If $N \ge 1/C$, then $1/\sqrt{CNK^2} \le 1/K $ and therefore
    \[ \E\biggl[ \exp\biggl(\lambda \sum_{i=1}^N X_i \biggr) \biggr] \le \exp\bigl( \lambda^2 C N K^2 \bigr),
    \quad \text{for all $\lambda$ such that }  \abs{\lambda} \le \frac{1}{\sqrt{CN} K}.  \]
    Then the proof is completed by using \cite[Proposition 2.7.1 (iv)(v)]{vershynin2018high} again.

    For (d), the proof is analogous to \cite[Exercise 2.5.10]{vershynin2018high}. By \cite[Proposition 2.7.1 (i)(iv)]{vershynin2018high}, $\P(\abs{X_i} \ge t) \le 2\exp(-ct/\norm{X}_{\psi_1}) \le 2\exp(-ct/K)$, $\forall\, t \ge 0$, where $c$ is an absolute constant. Denote $t_0 := 2K/c$, then
	\begin{align*}
	& \E\left[ \max_{i \ge 1} \frac{\abs{X_i}}{1 + \log i} \right]
	\le t_0 + \int_{t_0}^\infty \P\biggl( \max_{i \ge 1} \frac{\abs{X_i}}{1 + \log i} > t \biggr) \d t 
	\le \frac{2K}{c} + \int_{t_0}^\infty  \sum_{i=1}^\infty  \P\biggl( \frac{\abs{X_i}}{1 + \log i} > t \biggr) \d t
	\\
	= {} & \frac{2K}{c} + \sum_{i=1}^\infty \int_{t_0}^\infty  \P\bigl( \abs{X_i} > t (1 + \log i) \bigr) \d t
	   \le \frac{2K}{c} + \sum_{i=1}^\infty \int_{t_0}^\infty 2\exp\bigl(-ct(1 + \log i)/K\bigr) \d t\\
	\le {} & 
    \frac{2K}{c} + \sum_{i=1}^\infty \int_{t_0}^\infty  \exp\bigl(- (\log i) ct_0/K \bigr) \cdot  2\exp(-ct/K) \d t
        \le 
    \frac{2K}{c} + \sum_{i=1}^\infty i^{-2} \int_{0}^\infty 2\exp(-ct/K) \d t \\
	= {} & \frac{2K}{c} + C_0 \cdot \frac{2K}{c} 
        \le C K,
	\end{align*}
    where $C_0, C$ are absolute constants. Hence, for any $N \ge 2$,
	\[ \E\left[\max_{1 \le i \le N} \abs{X_i}\right] 
	\le (1+\log N) \cdot \E\left[ \max_{1 \le i \le N} \frac{\abs{X_i}}{1 + \log i} \right] \lesssim K \log N.
	\]
    This concludes the proof.
\end{proof}




\subsection{Properties of the Moreau envelope and proximal operator}
\label{append_subsec_Moreau}

Let $\ell: \R \to \R_{\ge 0}$ be a continuous convex function. For any $x \in \R$ and $\lambda > 0$, the Moreau envelope of $\ell$ is defined as
\begin{equation}\label{eq:envelope}
    \envelope_\ell(x; \lambda) = \envelope_{\lambda\ell}(x)
    := \min_{t \in \R} \left\{  \ell(t) +  \frac1{2\lambda} (t - x)^2 \right\},
\end{equation}
and the proximal operator of $\ell$ is defined as
\begin{equation*}
    \prox_{\ell}(x; \lambda) =
    \prox_{\lambda \ell}(x) := \argmin_{t \in \R} \left\{ \ell(t) +  \frac1{2\lambda} (t - x)^2 \right\}.
\end{equation*}
\begin{lem} \label{lem:prox}
For any $x \in \R, \lambda > 0$, $\prox_{\ell}(x; \lambda)$ is uniquely determined by stationarity condition
    \begin{equation*}
        \prox_{\ell}(x; \lambda) + \lambda \ell'\bigl( \prox_{\ell}(x; \lambda) \bigr) - x = 0.
    \end{equation*}
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:prox(a)}
        $\envelope_\ell(x; \lambda)$ is continuous and convex in $(x, \lambda)$. If $\ell$ is differentiable, then $\envelope_\ell(x; \lambda)$ is also differentiable in its domain, with partial derivatives
        \begin{equation*}
            \begin{aligned}
                \frac{\partial \envelope_\ell(x; \lambda)}{\partial x}
            & = 
            \mathmakebox[\widthof{$\displaystyle -\frac{1}{2\lambda^2} \big( x - \prox_{\ell}(x; \lambda) \big)^2$}][r]{\frac{1}{\lambda} \big( x - \prox_{\ell}(x; \lambda) \big)\phantom{^2}}
            = 
            \mathmakebox[\widthof{$\displaystyle -\frac12 \bigl(\ell'(z) \bigr)^2 \big|_{z = \prox_{\ell}(x; \lambda) }$}][r]{
            \ell'(z) \phantom{^2} \big|_{z = \prox_{\ell}(x; \lambda) }
            },
            \\
                \frac{\partial \envelope_\ell(x; \lambda)}{\partial \lambda}
            & = -\frac{1}{2\lambda^2} \big( x - \prox_{\ell}(x; \lambda) \big)^2
            = -\frac12 \bigl(\ell'(z) \bigr)^2 \big|_{z = \prox_{\ell}(x; \lambda) }.
            \end{aligned}
        \end{equation*}
        Moreover, $\envelope_\ell(x; \lambda)$ is non-increasing in $\lambda$ and $\envelope_\ell(x; \lambda) \to \ell(x)$ when $\lambda \to 0^+$.
        \item \label{lem:prox(b)}
        $\prox_{\ell}(x; \lambda)$ is continuous in $(x, \lambda)$. If $\ell$ is twice differentiable, then $\prox_\ell(x; \lambda)$ is also differentiable in its domain, with partial derivatives
        \begin{equation*}
            \frac{\partial \prox_{\ell}(x; \lambda)}{\partial x}
            =  \frac{1}{1 + \lambda \ell''(z)} \bigg|_{z = \prox_{\ell}(x; \lambda)}
            \qquad
            \frac{\partial \prox_{\ell}(x; \lambda)}{\partial \lambda}
            =  -\frac{\ell'(z)}{1 + \lambda \ell''(z)} \bigg|_{z = \prox_{\ell}(x; \lambda)} .
        \end{equation*}
        Moreover, $\prox_\ell(x; \lambda) \to x$ when $\lambda \to 0^+$.
    \end{enumerate}
\end{lem}
\begin{proof}
See \cite[Lemma 15]{thrampoulidis2018precise}, \cite[Proposition A.1]{donoho2016high}, \cite[Lemma 2, Lemma 4]{salehi2019impact}, and relevant references therein.
\end{proof}


\section{Miscellaneous}

Let $\hat\kappa$ be the optimal objective value in \cref{eq:SVM}, which is the \emph{maximum margin} for data $(\XX, \yy)$. Moreover, $(\hat\vbeta, \hat\beta_0, \hat\kappa)$ is also the optimal solution to \cref{eq:SVM-m-reb}. Notice $\hat\kappa \ge 0$ always holds (by taking $\vbeta = 0$, $\beta_0 = 0$ in \cref{eq:SVM}), and we can observe the following relation.
\begin{equation*}
	\begin{aligned}
		\text{(linearly separable)} 
		\ \ & \text{$\exists\, \vbeta\not=\bzero$, $\beta_0 \in \R$, such that $y_i ( \< \xx_i, \bbeta \> + \beta_0 ) > 0$, $\forall\, i \in [n]$,} \\
		& \Longleftrightarrow \quad \hat\kappa > 0, \quad \Longrightarrow \quad \|\hat\vbeta\|_2 = 1, \\
		\text{(not linearly separable)} 
		\ \ & \text{$\forall\, \vbeta\not=\bzero$, $\beta_0 \in \R$, such that $y_i ( \< \xx_i, \bbeta \> + \beta_0 ) \overset{\mathmakebox[0pt][c]{\smash{(*)}}}{\le}  0$, $\forall\, i \in [n]$,} \\
		& \Longleftrightarrow \quad \hat\kappa = 0, \quad \Longrightarrow \quad \hat\vbeta = \bzero, \ \hat\beta_0 = 0 \text{ is a solution.\footnotemark}
	\end{aligned}
    \footnotetext{
	If $(*)$ is strict ($<$), then $\hat\vbeta = \bzero$, $\hat\beta_0 = 0$ is the \emph{unique} solution.
}
\end{equation*}
When data is linearly separable, it turns out \cref{eq:SVM-m-reb} also has the following equivalent form:
\begin{equation}
	\label{eq:SVM-1}
    \begin{array}{rl}
    \minimize\limits_{\bw \in \R^d, \, w_0 \in \R} & \norm{\bw}_2^2, \\
    \text{subject to} &  \wt y_i(\< \xx_i, \bw \> + w_0) \ge 1, \quad \forall\, i \in [n].
    \end{array}
\end{equation}
The parameters in \cref{eq:SVM-m-reb} and \eqref{eq:SVM-1} have one-to-one relation $(\kappa, \vbeta, \beta_0) = (1, \bw, w_0)/\|\bw\|_2$. Notably, \cref{eq:SVM-1} is known as the hard-margin SVM \cite{vapnik1998statistical} if $\tau = 1$. 