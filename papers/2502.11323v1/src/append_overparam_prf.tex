\section{Logit distribution for separable data: Proofs for \cref{sec:logit_SVM}}
\label{append_sec:sep}

\subsection{Proof of \cref{thm:SVM_main}}
\label{append_subsec:sep}

Recall that the margin-rebalanced SVM can be rewritten as
\begin{equation}
    \label{eq:over_max-margin}
    \begin{array}{rl}
    \maximize\limits_{\bbeta \in \R^d, \, \beta_0, \kappa \in \R} & \kappa, \\
    \text{subject to} & \tilde{y}_i ( \< \xx_i, \bbeta \> + \beta_0 ) \ge \kappa,
	\quad \forall\, i \in [n], \\
	&  \norm{\bbeta}_2 \le 1.
    \end{array}
\end{equation}
Let $(\hat \vbeta_n, \hat \beta_{0, n})$ be an optimal solution and $\hat\kappa_n = \ind_{1 \le n_+ \le n - 1} \kappa(\hat \vbeta_n, \hat \beta_{0, n})$ be the well-defined maximum margin as per \cref{def:max-margin}. Our goal is to derive exact asymptotics for $(\hat \vbeta_n, \hat \beta_{0, n}, \hat\kappa_n)$. Similar to the development in \cite{montanari2023generalizationerrormaxmarginlinear}, for any positive margin $\kappa > 0$, we define the event
\begin{align*}
        \mathcal{E}_{n, \kappa} & = \bigl\{  \kappa(\hat \vbeta_n, \hat \beta_{0, n})  \ge \kappa \bigr\}  \\
        & =  \left\{ \text{$\exists\, \vbeta \in \R^d$, $\norm{\bbeta}_2 \le 1$, $\beta_0 \in \R$, such that $\tilde{y}_i \big( \langle \xx_i, \bbeta \rangle + \beta_0 \big) \ge \kappa$ for all $i \in [n]$} \right\} \\
        & = \left\{ \text{$\exists\, \vbeta \in \R^d$, $\norm{\bbeta}_2 \le 1$, $\beta_0 \in \R$, such that $\norm{ \left( \kappa \bs_\yy - \yy \odot \XX \vbeta  - \beta_0 \yy \right)_+ }_2 = 0$} \right\},
\end{align*}
where $\bs_\yy = (s(y_1), \dots, s(y_n))^\top$ and $s$ is the function defined in \cref{eq:s_fun}. Therefore, the data $(\XX, \yy)$ is linearly separable if and only if $\mathcal{E}_{n, \kappa}$ holds for some $\kappa > 0$. We would like to determine for which sets of parameters $(\pi, \vmu, \delta, \tau)$ we have
$\P(\mathcal{E}_{n, \kappa}) \to 1$ and for which instead $\P(\mathcal{E}_{n, \kappa}) \to 0$ as $n,d \to \infty$. To this end, we also define
\begin{equation}
    \label{eq:xi_n_kappa}
    \begin{aligned}
        \xi_{n, \kappa} & := \min_{ \substack{ \norm{\vbeta}_2 \le 1 \\ \beta_0 \in \R} } \frac{1}{\sqrt{d}} \norm{ \left( \kappa \bs_\yy - \yy \odot \XX \vbeta  - \beta_0 \yy \right)_+ }_2 \\
        & \phantom{:}\overset{\mathmakebox[0pt][c]{\text{(i)}}}{=} \min_{ \substack{ \norm{\vbeta}_2 \le 1 \\ \beta_0 \in \R} } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } \frac{1}{\sqrt{d}} \blambda^\top \left( \kappa \bs_\yy \odot \yy - \XX \vbeta  - \beta_0 \bone \right),
    \end{aligned}
\end{equation}
where (i) is a consequence of Lagrange duality (dual norm) $\norm{(\ba)_+}_2 = \max_{\norm{\blambda}_2 \le 1, \blambda \ge \bzero} \blambda^\top \ba$. Then we established the following equivalence
\begin{equation*}
    \left\{ \xi_{n, \kappa} = 0 \right\} \Longleftrightarrow \mathcal{E}_{n ,\kappa}
    \qquad
    \left\{ \xi_{n, \kappa} > 0 \right\} \Longleftrightarrow \mathcal{E}^c_{n ,\kappa}.
\end{equation*}
Keep in mind that we are only concerned with the sign (positivity) of $\xi_{n, \kappa}$, not its magnitude. As a consequence, we have
\begin{equation*}
    \hat\kappa_n = \ind_{1 \le n_+ \le n - 1}\cdot \sup\{ \kappa \in \R:  \xi_{n,\kappa} = 0 \}.
\end{equation*}
Let $\mathcal{D}_n := \{ n_+ = 0 \text{ or } n \}$ be the event of degeneration for any datasets of size $n$. Clearly $\P(\mathcal{D}_n) = \pi^n + (1 - \pi)^n \to 0$ as $n \to \infty$. Technically, the empirical logit distribution (ELD) in \cref{eq:ELD} is not well-defined on $\mathcal{D}_n$. Similar as \cref{def:max-margin}, we can also redefine it as follows:
\begin{equation}\label{eq:over_ELD_well}
    \hat \nu_{n} := \frac1n \sum_{i=1}^n \delta_{(y_i, \< \xx_i, \hat\vbeta \> + \hat\beta_{0} ) \cdot \ind\{1 \le n_+ \le n - 1\} }.
\end{equation}



We provide an outline for the main parts of the proofs of \cref{thm:SVM_main}\ref{thm:SVM_main_trans}---\ref{thm:SVM_main_mar}, which involves several steps of transforming and simplifying the random variable $\xi_{n, \kappa}$.
\begin{equation*}
\begin{aligned}
    \xi_{n, \kappa}
    \, \xRightarrow[\text{\cref{lem:over_beta0}}]{\textbf{Step 1}} \, 
    \xi'_{n, \kappa, B}
    \, \xRightarrow[\text{\cref{lem:over_CGMT}}]{\textbf{Step 2}} \, 
    \xi'^{(1)}_{n, \kappa, B}
    \, \xRightarrow[\text{\cref{lem:over_ULLN}}]{\textbf{Step 3}} \, 
    \bar\xi'^{(2)}_{\kappa, B}
    \, \Rightarrow \,
    \bar\xi_{\kappa}^{(2)}
    \\
    \xRightarrow[\text{\cref{lem:over_sign}}]{\textbf{Step 4}} \, 
    \bar\xi_{\kappa}^{(3)}
    \, \Rightarrow \,
    \wt\xi_{\kappa}^{(3)}, F_\kappa(\rho, \beta_0)
    \, \xRightarrow[\text{\cref{lem:over_phase_trans}}]{\textbf{Step 5}} \, 
    \delta^*(\kappa), H_\kappa(\rho, \beta_0).
\end{aligned}
\left.
\vphantom{\begin{matrix} \dfrac12 \\ \dfrac12 \end{matrix}}
\right\} \text{\scriptsize{\cref{lem:over_mar_conp}}}
\end{equation*}

\paragraph{Step 1: Boundedness of the intercept (from $\xi_{n, \kappa}$ to $\xi'_{n, \kappa, B}$)} 
According to the definition of $\xi_{n, \kappa}$, parameters $\vbeta$ and $\blambda$ are optimized in compact sets, but $\beta_0$ is not. Such non-compactness might cause technical difficulties in the following steps, for example, when applying Gordon's Gaussian comparison inequality and establishing uniform convergence. However, it turns out that $\beta_0$ is asymptotically bounded on the event $\mathcal{E}_{n ,\kappa}$. More precisely, we define
\begin{equation}
    \label{eq:xi'_n_kappa_B}
    \xi'_{n, \kappa, B} := \min_{ \substack{ \norm{\vbeta}_2 \le 1 \\ \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge 0} } \frac{1}{\sqrt{d}} \blambda^\top \left( \kappa \bs_\yy \odot \yy - \XX \vbeta  - \beta_0 \bone \right),
\end{equation}
where $B = B(\tau, \kappa, \pi, \norm{\bmu}_2, \delta)$ is a sufficiently large constant. Then we can show that $\xi_{n, \kappa}$ and $\xi'_{n, \kappa, B}$ have the same sign with high probability, which enables us to work with $\xi'_{n, \kappa, B}$ instead of $\xi_{n, \kappa}$.

\begin{lem}[Boundedness of $\beta_0$] 
\label{lem:over_beta0}    
There exists some constant $B \in (0, \infty)$ (depends on $\tau, \kappa, \pi, \norm{\bmu}_2, \delta$) such that
    \begin{equation*}
        \lim_{n \to \infty} \abs{ \P\bigl( \xi_{n, \kappa} = 0 \bigr) - \P\bigl(\xi'_{n, \kappa, B} = 0 \bigr) } = 0.
    \end{equation*}
\end{lem}
\noindent
See \cref{subsubsec:over_beta0} for the proof.

\paragraph{Step 2: Reduction via Gaussian comparison (from $\xi'_{n, \kappa, B}$ to $\xi'^{(1)}_{n, \kappa, B}$)} 
According to the expression of $\xi'_{n, \kappa, B}$, it is not hard to see the objective function (of $(\vbeta, \blambda)$) is a bilinear form of the Gaussian random matrix $\XX$. To simplify the bilinear term and make the calculation easier, we will use the convex Gaussian minimax theorem (CGMT, see \Cref{lem:CGMT}), i.e., Gordon's comparison inequality \cite{gordon1985some, thrampoulidis2015regularized}. To do so, we introduce another quantity:
\begin{equation}\label{eq:xi1_n_kappa_B}
    \xi_{n, \kappa, B}'^{(1)} := \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\ \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge 0} } \frac{1}{\sqrt{d}}  \left(
    \norm{\blambda}_2 \vg^\top \btheta + \norm{\btheta}_2 \vh^\top \blambda + \blambda^\top \bigl( 
        \kappa \bs_\yy \odot \yy - \rho\norm{\bmu}_2 \yy + \rho \vu - \beta_0 \bone
     \bigr)
     \right),
\end{equation}
where $\rho \in \R$, $\vtheta \in \R^{d-1}$ are parameters, $\vg \sim \normal(\bzero, \bI_{d-1})$, $\hh \sim \normal(\bzero, \bI_{n})$, $\uu \sim \normal(\bzero, \bI_{n})$ are independent Gaussian vectors. The following lemma connects $\xi'_{n, \kappa, B}$ with $\xi_{n, \kappa, B}'^{(1)}$.

\begin{lem}[Reduction via CGMT] 
    \label{lem:over_CGMT}    
For any $v \in \R$ and $t \ge 0$,
    \begin{equation*}
        \P\Big( \big|\xi'_{n, \kappa, B} - v  \big| \ge t \Big) 
        \le 
        2 \P\Big( \big| \xi'^{(1)}_{n, \kappa, B} - v \big| \ge t \Big).
    \end{equation*}
\end{lem}
\noindent
See \cref{subsubsec:over_CGMT} for the proof.


\paragraph{Step 3: Dimension reduction (from $\xi'^{(1)}_{n, \kappa, B}$ to $\bar\xi'^{(2)}_{\kappa, B}$)} It turns out that $\xi'^{(1)}_{n, \kappa, B}$ can be further simplified for analytical purposes. We define a new (deterministic) quantity
\begin{equation*}
    \bar\xi'^{(2)}_{\kappa, B} :=  \min_{ \substack{ \rho^2 + r^2 \le 1, r \ge 0 \\  \abs{\beta_0} \le B } }
    -r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + rG_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2},
\end{equation*}
which is a constrained minimization over only three variables $\rho$, $r$, and $\beta_0$, with random variables $(Y, G_1, G_2) \sim P_y \times \normal(0, 1) \times \normal(0, 1)$. The two quantities of interest can be related via the uniform law of large numbers (ULLN) as shown in the following lemma.

\begin{lem}[ULLN]
\label{lem:over_ULLN}    
As $n,d \to \infty$, we have
    \begin{equation*}
        \xi'^{(1)}_{n, \kappa, B}  \conp \left( \bar\xi'^{(2)}_{\kappa, B} \right)_+.
    \end{equation*}
\end{lem}
\noindent
See \cref{subsubsec:over_ULLN} for the proof.


\paragraph{Step 4: Investigation of the positivity (from $\bar\xi'^{(2)}_{\kappa, B}$ to $\bar\xi^{(3)}_{\kappa}$)}
To further simplify the problem, we define the following quantities that are closely related to $\bar\xi'^{(2)}_{\kappa, B}$:
\begin{align}
        \bar\xi_{\kappa}^{(2)} & :=  \min_{ \substack{ \rho^2 + r^2 \le 1, r \ge 0 \\  \beta_0 \in  \R } }
    -r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + rG_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2} 
    \notag
    \\
        \bar\xi_{\kappa}^{(3)} & :=  \min_{ \substack{ \rho \in [-1, 1] \\  \beta_0 \in  \R } }
        -\sqrt{1 - \rho^2} + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}.
        \label{eq:xi3_kappa}
\end{align}
Firstly, we argue that $\bar\xi'^{(2)}_{\kappa, B} = \bar\xi_{\kappa}^{(2)}$ for constant $B$ large enough, by noticing the optimal (unique) $\beta_0$ in $\bar\xi_{\kappa}^{(2)}$ is always bounded by some constant (depends on $\tau, \kappa, \pi, \norm{\bmu}_2, \delta$). Secondly, notice $\bar\xi_{\kappa}^{(3)}$ can be viewed as fixing $r = \sqrt{1 - \rho^2}$ in the optimization of $\bar\xi_{\kappa}^{(2)}$, and $G := \rho G_1 + \sqrt{1 - \rho^2} G_2 \sim \normal(0, 1)$. The following lemma shows that the sign won't change from $\bar\xi_{\kappa}^{(2)}$ to $\bar\xi_{\kappa}^{(3)}$.

\begin{lem}[Sign invariance] 
\label{lem:over_sign}    
For any $\kappa > 0$, the following result holds:
    \begin{enumerate}[label=(\alph*)]
        \item $\sign(\bar\xi_{\kappa}^{(2)}) = \sign(\bar\xi_{\kappa}^{(3)})$.
        \item If $\bar\xi_{\kappa}^{(2)} \le 0$, then $\bar\xi_{\kappa}^{(2)} = \bar\xi_{\kappa}^{(3)}$.
    \end{enumerate}
\end{lem}
\noindent
See \cref{subsubsec:over_positive} for the proof.



\paragraph{Step 5: Phase transition and margin convergence}
Note the function $\delta^*: \R \to \R_{\ge 0}$ defined in \cref{eq:sep_functions} is closely related to $\bar\xi_{\kappa}^{(3)}$. Let $\kappa^* := \sup\left\{ \kappa \in \R: \delta^*(\kappa) \ge \delta \right\}$. By combining the results from previous steps, we have the following relation.
\begin{lem}[Phase transition] 
\label{lem:over_phase_trans}
For any $\kappa > 0$, we have
\begin{equation*}
    \begin{aligned}
        \lim_{n \rightarrow \infty} \P\left( \xi_{n, \kappa} = 0 \right) = 1, \qquad & \text{if $\delta \le \delta^*(\kappa)$ (i.e., $\kappa \le \kappa^*$)}, \\
        \lim_{n \rightarrow \infty} \P\left( \xi_{n, \kappa} > 0 \right) = 1, \qquad & \text{if $\delta > \delta^*(\kappa)$ (i.e., $\kappa > \kappa^*$)}.
    \end{aligned}
\end{equation*}
In particular,
\begin{equation*}
    \begin{aligned}
        \lim_{n \rightarrow \infty} \P\left\{ \text{$(\XX, \yy)$ is linearly separable} \right\} = 1, \qquad & \text{if $\delta < \delta^*(0)$}, \\
        \lim_{n \rightarrow \infty} \P\left\{ \text{$(\XX, \yy)$ is not linearly separable} \right\} = 0, \qquad & \text{if $\delta > \delta^*(0)$}.
    \end{aligned}
\end{equation*}
\end{lem}
As a consequence, we can also derive the convergence of margin in probability. Notice that the following result is weaker than $\cL^2$ convergence \cref{thm:SVM_main}\ref{thm:SVM_main_mar}. However, we need this preliminary result for the subsequent proof of ELD convergence in \cref{lem:over_logit_conv}.
\begin{lem}[Margin convergence, in probability]
\label{lem:over_mar_conp}
If $\delta < \delta^*(0)$, we have $\hat\kappa_n \conp \kappa^*$.
\end{lem}
\noindent
See \cref{subsubsec:over_phase} for the proof.








\subsubsection{Step 1 --- Boundedness of the intercept: Proof of \cref{lem:over_beta0}}
\label{subsubsec:over_beta0}
\begin{proof}[\textbf{Proof of \cref{lem:over_beta0}}]
Recall that
\begin{equation*}
    \xi_{n, \kappa} = \min_{ \substack{ \norm{\vbeta}_2 \le 1 \\ \beta_0 \in \R} } \frac{1}{\sqrt{d}} \norm{ \left( \kappa \bs_\yy - \yy \odot \XX \vbeta  - \beta_0 \yy \right)_+ }_2.
\end{equation*}
Let $(\wt\vbeta_n, \wt\beta_{0, n})$ be a minimizer of the function above\footnote{
    In general $(\wt\vbeta_n, \wt\beta_{0, n})$ may not be unique and may not be equal to $(\hat\vbeta_n, \hat\beta_{0, n})$.
}. On the event $\mathcal{D}_n^c \cap \mathcal{E}_{n ,\kappa}$ ($\xi_{n, \kappa} = 0$), we have 
\begin{equation*}
        \bigl\| \bigl( \kappa \bs_\yy - \yy \odot \XX \wt\vbeta_n  -  \wt\beta_{0, n} \yy \bigr)_+  \bigr\|_2 = 0, 
        \quad
        \Longrightarrow
        \quad
        \begin{cases} 
            \ \tau \kappa - \< \xx_i, \wt\vbeta_n \> - \wt\beta_{0, n} \le 0, & \ \text{if} \ y_i = + 1, \\
            \ \mathmakebox[\widthof{$\tau\kappa$}][r]{\kappa} + \< \xx_i, \wt\vbeta_n \> + \wt\beta_{0, n} \le 0,      & \ \text{if} \ y_i = -1.
        \end{cases}
\end{equation*}
Write $\xx_i = y_i \bmu + \zz_i$, where $\zz_i \iidsim \normal(\bzero, \bI_d)$ and $y_i \indep \zz_i$. Then we obtain
\begin{equation*}
    \begin{cases} 
        \ \wt\beta_{0, n} \ge \mathmakebox[\widthof{$-$}][r]{\tau}
        \kappa - \< \bmu, \wt\vbeta_n \> - \< \zz_i, \wt\vbeta_n \> , & \ \text{if} \ y_i = + 1, \\
        \ \wt\beta_{0, n} \le    - \kappa + \< \bmu, \wt\vbeta_n \> - \< \zz_i, \wt\vbeta_n \> ,      & \ \text{if} \ y_i = -1,
    \end{cases}
\end{equation*}
which implies for all $i, j$ such that $y_i = +1, y_j = -1$,
\begin{equation*}
    \begin{aligned}
        | \wt\beta_{0, n} | & \le 
        \bigl| \tau \kappa - \< \bmu, \wt\vbeta_n \> - \< \zz_i, \wt\vbeta_n \> \bigr|
        + 
        \bigl| \kappa - \< \bmu, \wt\vbeta_n \> + \< \zz_j, \wt\vbeta_n \>  \bigr| \\
        & \le (\tau + 1)\kappa + 2\bigl| \< \bmu, \wt\vbeta_n \> \bigr| + 
        \bigl| \< \zz_i, \wt\vbeta_n \> \bigr| + \bigl| \< \zz_j, \wt\vbeta_n \> \bigr|.
    \end{aligned}
\end{equation*}
Using the inequality $(a+b+c)^2 \le 3(a^2 + b^2 + c^2)$, we have
\begin{equation*}
    \begin{aligned}
        | \wt\beta_{0, n} |^2 
        & \le
        3 \left\{  \bigl( (\tau + 1)\kappa + 2\bigl| \< \bmu, \wt\vbeta_n \> \bigr| \bigr)^2
        + 
        \min_{i: y_i = +1} \bigl| \< \zz_i, \wt\vbeta_n \> \bigr|^2 + 
        \min_{j: y_j = -1} \bigl| \< \zz_j, \wt\vbeta_n \> \bigr|^2  \right\} \\
        & \le 
        3 \, \biggl\{  \bigl( (\tau + 1)\kappa + 2\bigl| \< \bmu, \wt\vbeta_n \> \bigr| \bigr)^2
        + 
        \frac{1}{n_+}\sum_{i: y_i = +1} \bigl| \< \zz_i, \wt\vbeta_n \> \bigr|^2 + 
        \frac{1}{n_-}\sum_{j: y_j = -1} \bigl| \< \zz_j, \wt\vbeta_n \> \bigr|^2  \biggr\} \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{=} 
        3 \, \biggl\{  \bigl( (\tau + 1)\kappa + 2\bigl| \< \bmu, \wt\vbeta_n \> \bigr| \bigr)^2
        + 
        \frac{1}{n_+} \bigl\| \ZZ_+ \wt\vbeta_n \bigr\|_2^2 + 
        \frac{1}{n_-} \bigl\| \ZZ_- \wt\vbeta_n \bigr\|_2^2  \biggr\} \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le}
        3 \, \biggl\{  \bigl( (\tau + 1)\kappa + 2 \norm{\bmu}_2 \bigr)^2
        + 
        \frac{1}{n_+} \norm{ \ZZ_+ }_{\mathrm{op}}^2 + 
        \frac{1}{n_-} \norm{ \ZZ_- }_{\mathrm{op}}^2  \biggr\} =: \wt B_{0, n},
    \end{aligned}
\end{equation*}
where in (i) we denote $\ZZ_+ \in \R^{n_+ \times d}$ as a Gaussian random matrix with rows $\zz_i$ such that $y_i = +1$, $\ZZ_- \in \R^{n_- \times d}$ with rows $\zz_j$ such that $y_j = +1$, while in (ii) we use Cauchy--Schwarz inequality, the definition of operator norm, and $\| \wt\vbeta_n \|_2 \le 1$. 

~\\
\noindent
Next, we show that $\wt B_{0, n}$ is asymptotically bounded. Notice $\ZZ_+, \ZZ_-$ have i.i.d. standard Gaussian entries. According to the tail bound of Gaussian matrices \cite[Corollary 7.3.3]{vershynin2018high}, for any $t_n \ge 0$ such that $t_n = o(\sqrt{n})$ and some absolute constants $c, C \in (0, \infty)$, we have
\begin{equation*}
    \begin{aligned}
        \wt B_{0, n} 
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\le} 
        3 \, \biggl\{  \bigl( (\tau + 1)\kappa + 2 \norm{\bmu}_2 \bigr)^2
        + 
        \frac{1}{n_+} \bigl(\sqrt{\smash[b]{n_+}} + \sqrt{d} + t_n \bigr)^2 + 
        \frac{1}{n_-} \bigl(\sqrt{\smash[b]{n_-}} + \sqrt{d} + t_n \bigr)^2  \biggr\} \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le}
        3 \, \biggl\{  \bigl( (\tau + 1)\kappa + 2 \norm{\bmu}_2 \bigr)^2
        + 
        \biggl(C + \frac{1}{\sqrt{\pi \delta}}       \biggr)^2 + 
        \biggl(C + \frac{1}{\sqrt{\smash[b]{(1 - \pi) \delta}}} \biggr)^2  \biggr\}
        =: B_{0} ,
    \end{aligned}
\end{equation*}
where (i) holds with probability as least $1 - 4 \exp(-c t_n^2)$, and (ii) holds with probability one based on the fact that $n_+/n \to \pi$, $n_-/n \to 1 - \pi$ a.s. (by strong law of large numbers), and $n/d \to \delta$ as $n \to \infty$. Notice the upper bound $B_0$ is a constant which depends on $(\tau, \kappa, \pi, \norm{\bmu}_2, \delta)$.
Let $t_n \to \infty$, then we conclude $\wt B_{0, n} \le  B_{0}$ with high probability.

~\\
\noindent
Combining these results, for any $B > \sqrt{B_0}$,
\begin{equation*}
    \left( \{ \xi_{n,\kappa} = 0 \} \cap \mathcal{D}_n^c \cap \{ \wt B_{0, n} \le  B_{0} \} \right) 
    \subseteq
    \left( \{ \xi_{n,\kappa} = 0 \} \cap \mathcal{D}_n^c \cap \{ | \wt\beta_{0, n} | \le B \} \right) 
    \subseteq
    \{ \xi'_{n,\kappa,B} = 0 \}.
\end{equation*}
Therefore, by union bound we have
\begin{equation*}
    \begin{aligned}
        \P\bigl(\xi_{n,\kappa} = 0\bigr) 
        & = \P\Bigl( \{ \xi_{n,\kappa} = 0 \} \cap 
        \bigl( \mathcal{D}_n^c \cap \{ \wt B_{0, n} \le  B_{0} \} \bigr)
         \Bigr)
         +
         \P\Bigl( \{ \xi_{n,\kappa} = 0 \} \cap 
        \bigl( \mathcal{D}_n \cup \{ \wt B_{0, n} >  B_{0} \} \bigr)
         \Bigr)
         \\
        & \le \P\bigl(\xi'_{n,\kappa,B} = 0\bigr) + \P(\mathcal{D}_n) + \P\bigl(\wt B_{0, n} > B_0\bigr).
    \end{aligned}
\end{equation*}
Finally, by noticing $\xi_{n,\kappa} \le \xi'_{n,\kappa,B}$, we conclude
\begin{equation*}
    0 \le \P\bigl(\xi_{n,\kappa} = 0\bigr) - \P\bigl(\xi'_{n,\kappa,B} = 0\bigr) \le 
    \P(\mathcal{D}_n) + \P\bigl(\wt B_{0, n} > B_0\bigr) \to 0,
    \qquad \text{as $n \to \infty$}.
\end{equation*}
This completes the proof.
\end{proof}














\subsubsection{Step 2 --- Reduction via Gaussian comparison: Proof of \cref{lem:over_CGMT}}
\label{subsubsec:over_CGMT}
\begin{proof}[\textbf{Proof of \cref{lem:over_CGMT}}]
Rewrite $\xx_i = y_i \bmu + \zz_i$, where $\zz_i \iidsim \normal(\bzero, \bI_d)$. Note that $y_i \indep \zz_i$. Denote the projection matrices
\begin{equation*}
    \bP_{\vmu} := \frac{1}{\norm{\vmu}_2^2} \vmu \vmu^\top,
    \qquad 
    \bP_{\vmu}^\perp := \bI_d - \frac{1}{\norm{\vmu}_2^2} \vmu \vmu^\top,
\end{equation*}
where $\bP_{\vmu}$ is the orthogonal projection onto $\spann\{ \vmu \}$ and $\bP_{\vmu}^\perp$ is the orthogonal projection onto the orthogonal complement of $\spann\{ \vmu \}$. Then we have the following decomposition:
\begin{equation*}
    \begin{aligned}
        \< \xx_i, \bbeta \> 
    & = y_i \< \bmu, \bbeta \> + \< \zz_i, \bbeta \> 
    = y_i \< \bmu, \bbeta \> + \< \zz_i, \bP_{\vmu} \bbeta \> + 
    \< \zz_i, \bP_{\vmu}^\perp \bbeta \>  \\
    & = y_i \left\< \bbeta, \frac{\bmu}{\norm{\bmu}_2} \right\> \norm{\bmu}_2 
    +  \left\< \bbeta, \frac{\bmu}{\norm{\bmu}_2} \right\> \left\< \zz_i, \frac{\bmu}{\norm{\bmu}_2} \right\>
    + \< \zz_i , \bP_{\bmu}^{\perp} \bbeta \> \\
    & = y_i \rho \norm{\vmu}_2 + \rho u_i + \< \zz_i , \bP_{\bmu}^{\perp} \bbeta \>,
    \end{aligned}
\end{equation*}
where
\begin{equation*}
    \rho := \left\< \bbeta, \frac{\bmu}{\norm{\bmu}_2} \right\>,
    \qquad
    u_i := \left\< \zz_i, \frac{\bmu}{\norm{\bmu}_2} \right\> \sim \normal(0, 1).
\end{equation*}
Let $\bQ \in \R^{n \times (n - 1)}$ be an orthonormal basis for the subspace $\spann\{ \vmu \}^\perp$ ($\bQ^\top \bQ = \bI_{n-1}$). Note that
\begin{equation*}
    \< \zz_i , \bP_{\bmu}^{\perp} \bbeta \>
    = \< \zz_i, \bQ \bQ^\top \bbeta \> 
    = \< \bQ^\top \zz_i,  \bQ^\top \bbeta \> 
    = \< \vg_i , \vtheta \>,
\end{equation*}
where
\begin{equation*}
    \begin{gathered}
        \vg_i := \bQ^\top \zz_i \sim \normal(\bzero, \bI_{d-1}),
    \qquad
    \vg_i \indep u_i,
    \\
    \vtheta := \bQ^\top \bbeta \in \R^{n-1},
    \qquad
    \norm{\vtheta}_2 
    = \sqrt{\norm{\vbeta}^2_2 - \norm{\bP_{\vmu}\vbeta}^2_2}
    \le \sqrt{1 - \rho^2}.
    \end{gathered}
\end{equation*}
We obtain a one-to-one map $\vbeta \leftrightarrow (\rho, \vtheta)$ in the unit ball. Therefore, we can reparametrize
\begin{equation*}
    \< \xx_i, \bbeta \> + \beta_0 \overset{\mathrm{d}}{=} y_i \rho \norm{\vmu}_2 - \rho u_i - \< \vg_i , \vtheta \> + \beta_0,
\end{equation*}
where $\rho^2 + \norm{\vtheta}_2^2 \le 1$, and $\{(y_i, u_i, \vg_i)\}_{i = 1}^n$ are i.i.d., each has joint distribution:
\begin{equation*}
    y_i \indep u_i \indep \vg_i,
    \qquad
    \P(y_i=+1) = 1 - \P(y_i=-1) = \pi,
    \quad
    u_i \sim \normal(0, 1),
    \quad
    \vg_i \sim \normal(\bzero, \bI_{d-1}).
\end{equation*}
Now denote
\begin{equation*}
    \uu = (u_1, \dots, u_n)^\top \in \R^{n},
    \qquad
    \GG = (\vg_1, \dots, \vg_n)^\top \in \R^{n \times (d-1)}.
\end{equation*}
Therefore, $\xi'^{(0)}_{n,\kappa, B} := \xi'_{n,\kappa, B}$ defined in \cref{eq:xi'_n_kappa_B} can be written as
\begin{align*}
        \xi'^{(0)}_{n,\kappa,B}
        & = 
        \min_{ \substack{ \norm{\vbeta}_2 \le 1 \\ \abs{\beta_0} \le B} } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } \frac{1}{\sqrt{d}} \blambda^\top \left( \kappa \bs_\yy \odot \yy - \XX \vbeta  - \beta_0 \bone \right) \\
        & \overset{\mathmakebox[0pt][c]{\mathrm{d}}}{=} \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\  \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } \frac{1}{\sqrt{d}} \blambda^\top \left( \kappa \bs_\yy \odot \yy - \rho \norm{\bmu}_2 \yy + \rho \uu + \GG \btheta  - \beta_0 \bone \right) \\
        & = \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\  \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } \frac{1}{\sqrt{d}} \left( \blambda^\top \GG \btheta +
        \blambda^\top ( \kappa \bs_\yy \odot \yy - \rho \norm{\bmu}_2 \yy + \rho \uu   - \beta_0 \bone )
         \right).
        % \\
        % & =: \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\ \beta_0 \in \R } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } Q_0(\GG ; \rho, \vtheta, \blambda).
\end{align*}
On the other hand, recall $\xi_{n,\kappa}^{(1)}$ defined in \cref{eq:xi1_n_kappa_B}:
\begin{equation*}
    \xi'^{(1)}_{n, \kappa, B}
        = \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\  \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } \frac{1}{\sqrt{d}}  \left(
    \norm{\blambda}_2 \vg^\top \btheta + \norm{\btheta}_2 \vh^\top \blambda + \blambda^\top \bigl( 
        \kappa \bs_\yy \odot \yy - \rho\norm{\bmu}_2 \yy + \rho \vu - \beta_0 \bone
     \bigr)
     \right).
    %  \\
    %  & =:  \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\ \beta_0 \in \R } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } Q_1(\vg, \hh; \rho, \vtheta, \blambda).
\end{equation*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Deprecated: CGMT for unbounded \beta_0 % 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We couldn't apply CGMT to $\xi^{(0)}_{n,\kappa}$ and $\xi_{n,\kappa}^{(1)}$ directly since $\beta_0$ is not minimized on a compact set. To overcome this, for each integer $k \ge 1$, we define
% \begin{equation*}
%     M^{(0)}_{n, \kappa, k} :=  \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\ \abs{\beta_0} \le k } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } Q_0(\GG ; \rho, \vtheta, \blambda),
%     \qquad
%     M^{(1)}_{n, \kappa, k} :=
%     \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\ \abs{\beta_0} \le k } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } Q_1(\vg, \hh; \rho, \vtheta, \blambda).
% \end{equation*}
Note that both minimization and maximization above are defined over compact and convex constraint sets, and the objective function in $\xi'^{(0)}_{n, \kappa, B}$ is a bilinear in $(\vtheta, \blambda)$ (not $\beta_0$). In addition, $(\yy, \uu)$ is independent of $\GG, (\vg, \hh)$, so we can apply a variant of CGMT (\cref{lem:CGMT}) by conditioning on $(\yy, \uu)$, which yields for any $v \in \R$ and $t \ge 0$:
\begin{align*}
    & \P\left( \xi'^{(0)}_{n, \kappa, B} \le v+t \,|\, \yy, \uu \right) \le 2\, \P\left( \xi'^{(1)}_{n, \kappa, B} \le v+t \,|\, \yy, \uu \right),
    \\
    & \P\left( \xi'^{(0)}_{n, \kappa, B} \ge v-t \,|\, \yy, \uu \right) \le 2\, \P\left( \xi'^{(1)}_{n, \kappa, B} \ge v-t \,|\, \yy, \uu \right).
\end{align*}
Taking expectation over $(\yy, \uu)$ on both sides of the equation gives for any $v \in \R$ and $t \ge 0$:
\begin{equation*}
    \P\left( \xi'^{(0)}_{n, \kappa, B} \le v+t \right) \le 2\, \P\left( \xi'^{(1)}_{n, \kappa, B} \le v+t \right),
    \quad
    \P\left( \xi'^{(0)}_{n, \kappa, B} \ge v-t \right) \le 2\, \P\left( \xi'^{(1)}_{n, \kappa, B} \ge v-t \right),
\end{equation*}
which proves \Cref{lem:over_CGMT}.
% \begin{equation*}
%     \P\bigl( M^{(0)}_{n, \kappa, k} \le t \,|\, \yy, \uu \bigr) \le 2\, \P\bigl( M^{(1)}_{n, \kappa, k} \le t \,|\, \yy, \uu \bigr),
%     \qquad
%     \P\bigl( M^{(0)}_{n, \kappa, k} \ge t \,|\, \yy, \uu \bigr) \le 2\, \P\bigl( M^{(1)}_{n, \kappa, k} \ge t \,|\, \yy, \uu \bigr).
% \end{equation*}
% Taking expectation over $(\yy, \uu)$ on both sides of the equation gives for any $t \in \R$:
% \begin{equation*}
%     \P\bigl( M^{(0)}_{n, \kappa, k} \le t \bigr) \le 2\, \P\bigl( M^{(1)}_{n, \kappa, k} \le t \bigr),
%     \qquad
%     \P\bigl( M^{(0)}_{n, \kappa, k} \ge t \bigr) \le 2\, \P\bigl( M^{(1)}_{n, \kappa, k} \ge t \bigr).
% \end{equation*}
% Note $M^{(0)}_{n, \kappa, k} \searrow \xi^{(0)}_{n, \kappa}$ (in distribution) and $M^{(1)}_{n, \kappa, k} \searrow \xi^{(1)}_{n, \kappa}$ as $k \to \infty$. Hence, taking $k \to \infty$ in both inequalities above, we obtain
% \begin{equation*}
%     \P\bigl( \xi^{(0)}_{n, \kappa} \le t \bigr) \le 2\, \P\bigl( \xi^{(1)}_{n, \kappa} \le t \bigr),
%     \qquad
%     \P\bigl( \xi^{(0)}_{n, \kappa} \ge t \bigr) \le 2\, \P\bigl( \xi^{(1)}_{n, \kappa} \ge t \bigr).
% \end{equation*}
\end{proof}










\subsubsection{Step 3 --- Dimension reduction: Proof of \cref{lem:over_ULLN}}
\label{subsubsec:over_ULLN}
\begin{proof}[\textbf{Proof of \cref{lem:over_ULLN}}]
The expression of $\xi'^{(1)}_{n, \kappa, B}$ can be further simplified to
\begin{align*}
        \xi'^{(1)}_{n, \kappa, B}
        & = \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\ \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge \bzero} } \frac{1}{\sqrt{d}} \left( \norm{\blambda}_2 \vg^\top \btheta +
        \blambda^\top ( \kappa \bs_\yy \odot \yy - \rho \norm{\bmu}_2 \yy + \rho \uu 
        + \norm{\btheta}_2 \vh - \beta_0 \bone )  \right)  \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{=} 
        \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 \le 1 \\ \abs{\beta_0} \le B } } \frac{1}{\sqrt{d}} \left( \vg^\top \btheta + \bigl\| \left( 
            \kappa \bs_\yy - \rho \norm{\bmu}_2 + \rho \uu \odot \yy
        + \norm{\vtheta}_2 \vh \odot \yy - \beta_0 \yy
         \right)_+ \bigr\|_2  \right)_+ \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{=}
         \min_{ \substack{ \rho^2 + r^2 \le 1 \\ r \ge 0, \abs{\beta_0} \le B } } \frac{1}{\sqrt{d}} \left( 
            - r \norm{\vg}_2 + \bigl\| \left( 
            \kappa \bs_\yy - \rho \norm{\bmu}_2 + \rho \uu \odot \yy
        + r \vh \odot \yy - \beta_0 \yy
         \right)_+ \bigr\|_2  \right)_+,
\end{align*}
where in (i) we use the fact
\begin{equation*}
    \begin{aligned}
        \max_{\norm{\blambda}_2 \le 1, \blambda \ge \bzero} 
    \left( a \norm{\blambda}_2 + \blambda^\top \mathrm{\bf b} \right)
    & = \max_{r \in [0, 1]} \max_{\norm{\bv}_2 = 1, \bv \ge \bzero} 
    r \bigl( a + \bv^\top \mathrm{\bf b} \bigr)
    = \left( \max_{\norm{\bv}_2 = 1, \bv \ge \bzero} \bigl( a + \bv^\top \mathrm{\bf b} \bigr) \right)_+
    \\
    & = \Bigl( a + \norm{(\mathrm{\bf b})_+}_2 \Bigr)_+,
    \end{aligned}
\end{equation*}
in (ii) we use Cauchy--Schwarz inequality $\vg^\top \btheta \ge - \norm{\vtheta}_2 \norm{\vg}_2$ and denote $r = \norm{\vtheta}_2$. For convenience, we write the parameter space as $\bar\Theta_{B} := \{ (\rho, r, \beta_0): \rho^2 + r^2 \le 1, r \ge 0, \abs{\beta_0} \le B \}$. Now, define
\begin{align*}
        \bar \xi'^{(1)}_{n, \kappa, B} & := \min_{ (\rho, r, \beta_0) \in \bar\Theta_{B} } \frac{1}{\sqrt{d}} \left( 
            - r \norm{\vg}_2 + \bigl\| \left( 
            \kappa \bs_\yy - \rho \norm{\bmu}_2 + \rho \uu \odot \yy
        + r \vh \odot \yy - \beta_0 \yy
         \right)_+ \bigr\|_2  \right) \\
         & \phantom{:}= 
         \min_{ (\rho, r, \beta_0) \in \bar\Theta_{B} }
         \left\{ 
            -r\frac{\norm{\vg}_2}{\sqrt{d}}
            + \sqrt{\frac{n}{d}} \sqrt{\frac{1}{n} \sum_{i=1}^n \bigl( s(y_i) \kappa - \rho \norm{\bmu}_2 + \rho u_i y_i
            + r h_i y_i - \beta_0 y_i \bigr)_+^2 }
         \right\}
         \\
         & \phantom{:} =\mathmakebox[0pt][c]{:} \min_{ (\rho, r, \beta_0) \in \bar\Theta_{B} } f^{(1)}_{n,\kappa}(\rho,r,\beta_0)  ,
\end{align*}
then $\xi'^{(1)}_{n, \kappa, B} = \bigl( \bar \xi'^{(1)}_{n, \kappa, B} \bigr)_+$. Recall that
\begin{align*}
        \bar\xi'^{(2)}_{\kappa, B} & = \min_{ (\rho, r, \beta_0) \in \bar\Theta_{B} }
    -r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho Y G_1 + r Y G_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2} \\
    & = \min_{ (\rho, r, \beta_0) \in \bar\Theta_{B} }
    -r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + r G_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2} \\
    & =\mathmakebox[0pt][c]{:} \min_{ (\rho, r, \beta_0) \in \bar\Theta_{B} } f^{(2)}_{\kappa}(\rho,r,\beta_0),
\end{align*}
where $Y \indep G_1 \indep G_2$, $\P(Y = +1) = 1 - \P(Y = -1) = \pi$, and $G_1, G_2 \sim \normal(0, 1)$.
We also define
\begin{equation*}
    \begin{aligned}
        \phi^{(1)}_{n, \kappa}(\rho, r, \beta_0) & := \frac1n \sum_{i=1}^n \bigl( s(y_i) \kappa - \rho \norm{\bmu}_2 + \rho u_i y_i
        + r h_i y_i - \beta_0 y_i \bigr)_+^2
        = : \E_n\left[ f(Y, G_1, G_2; \rho, r, \beta_0) \right] , \\
        \phi^{(2)}_{\kappa}(\rho, r, \beta_0) & := \E\left[  \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 Y
        + r G_2 Y - \beta_0 Y \bigr)_+^2 \right]
        = : \mathmakebox[\widthof{$\E_n$}][l]{\E}\left[ f(Y, G_1, G_2 ; \rho, r, \beta_0) \right] ,
    \end{aligned}
\end{equation*}
where $\E_n[\cdot]$ denotes the expectation over the empirical distribution of $\{ (y_i, u_i, h_i) \}_{i = 1}^n$. In order to apply the uniform law of large numbers (ULLN), note that 
\begin{itemize}
    \item $\bar\Theta_{B}$ is compact. $(\rho, r, \beta_0) \mapsto f$ is continuous in $\bar\Theta_{B}$ for each $(Y, G_1, G_2)$, and $(Y, G_1, G_2) \mapsto f$ is measurable for each $(\rho, r, \beta_0)$
    \item $\abs{f(Y, G_1, G_2 ; \rho, r, \beta_0)} \le 3 \left( (\kappa\tau + \norm{\vmu}_2 + B)^2 + G_1^2 + G_2^2 \right)$ for all $(\rho, r, \beta_0) \in \bar\Theta_{B}$ and $\E[G_1^2] = \E[G_2^2] = 1 < \infty$.
\end{itemize}
Therefore, by ULLN \cite[Lemma 2.4]{newey1994large}, we have
\begin{align*}
    & \sup_{ (\rho, r, \beta_0) \in \bar\Theta_{B} }
    \abs{ \bigl( \phi^{(1)}_{n, \kappa}(\rho, r, \beta_0) \bigr)^{1/2} - \bigl( \phi^{(2)}_{\kappa}(\rho, r, \beta_0) \bigr)^{1/2} } \\
    \le {} & \sup_{ (\rho, r, \beta_0) \in \bar\Theta_{B} }
    \abs{ \phi^{(1)}_{n, \kappa}(\rho, r, \beta_0) -  \phi^{(2)}_{\kappa}(\rho, r, \beta_0) }^{1/2} = o_{\P}(1),
\end{align*}
where the inequality comes from the fact that $x \mapsto \sqrt{x}$ is $1/2$-Hölder continuous on $[0, \infty)$. Then
\begin{align*}
        & \sup_{ (\rho, r, \beta_0) \in \bar\Theta_{B} } \abs{ f^{(1)}_{n,\kappa}(\rho,r,\beta_0) - f^{(2)}_{\kappa}(\rho,r,\beta_0)} \\
        \le {} &  \sup_{r \in [-1, 1]} \abs{r - r\frac{\norm{\vg}_2}{\sqrt{d}}} + 
        \sup_{ (\rho, r, \beta_0) \in \bar\Theta_{B} } \abs{ \sqrt{\frac{n}{d}} \bigl( \phi^{(1)}_{n, \kappa}(\rho, r, \beta_0) \bigr)^{1/2}
        -  \sqrt{\delta} \bigl( \phi^{(2)}_{\kappa}(\rho, r, \beta_0) \bigr)^{1/2}  }  \\
        \le {} & \abs{1 - \frac{\norm{\vg}_2}{\sqrt{d}}} +  
        \sqrt{\frac{n}{d}}
        \sup_{ (\rho, r, \beta_0) \in \bar\Theta_{B} }
        \abs{ \bigl( \phi^{(1)}_{n, \kappa}(\rho, r, \beta_0) \bigr)^{1/2} - \bigl( \phi^{(2)}_{\kappa}(\rho, r, \beta_0) \bigr)^{1/2} } \\
        {} &  
        + \abs{ \sqrt{\frac{n}{d}} - \sqrt{\delta} } \sup_{ (\rho, r, \beta_0) \in \bar\Theta_{B} }\bigl( \phi^{(2)}_{\kappa}(\rho, r, \beta_0) \bigr)^{1/2} \\
        = {} & o_{\P}(1),
\end{align*}
by using $n/d \to \delta$ and law of large numbers $\norm{\vg}_2^2/(d - 1) \conp 1$. Finally, since the function $x \mapsto (x)_+$ is $1$-Lipschitz, we conclude
\begin{equation*}
    \Bigl| \xi'^{(1)}_{n, \kappa, B} - \bigl( \bar\xi'^{(2)}_{\kappa, B} \bigr)_+  \Bigr|
    \le
    \abs{ \bar\xi'^{(1)}_{n, \kappa, B} - \bar\xi'^{(2)}_{\kappa, B} }
    \le \sup_{ (\rho, r, \beta_0) \in \bar\Theta_{B} } \abs{ f^{(1)}_{n,\kappa}(\rho,r,\beta_0) - f^{(2)}_{\kappa}(\rho,r,\beta_0)} = o_{\P}(1).
\end{equation*}
This completes the proof.
\end{proof}









\subsubsection{Step 4 --- Investigation of the positivity: Proof of \cref{lem:over_sign}}
\label{subsubsec:over_positive}
\begin{proof}[\textbf{Proof of \cref{lem:over_sign}}]
We claim $\bar\xi'^{(2)}_{\kappa, B} = \bar\xi_{\kappa}^{(2)}$ when $B$ is large enough. Recall that
\begin{equation*}
        \bar\xi_{\kappa}^{(2)} =  \min_{ \substack{ \rho^2 + r^2 \le 1, r \ge 0 \\  \beta_0 \in  \R } }
    -r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + rG_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}.
\end{equation*}
Let $(\wt\rho, \wt r, \wt\beta_{0})$ be a minimizer above and notice $\wt\rho G_1 + \wt r G_2 \overset{\mathrm{d}}{=} \wt R G$, where $\wt R = \sqrt{\wt\rho^2 + \wt r^2}$ and $G \sim \normal(0, 1)$. Then
\begin{align*}
        \wt\beta_{0} & \in \phantom{:} \argmin_{\beta_0 \in \R} \E\left[ \bigl(  s(Y) \kappa - \wt\rho \norm{\bmu}_2 + \wt R G - \beta_0 Y \bigr)_+^2 \right] \\
        & = \phantom{:} \argmin_{\beta_0 \in \R} \left\{ 
            \pi \E\left[ \bigl( \tau \kappa - \wt\rho \norm{\bmu}_2 + \wt R G - \beta_0 \bigr)_+^2 \right]
            + (1 - \pi) \E\left[ \bigl( \kappa - \wt\rho \norm{\bmu}_2 + \wt R G + \beta_0 \bigr)_+^2 \right] \right\} \\
        & =: \argmin_{\beta_0 \in \R} g_{\wt\rho, \wt r}(\beta_0).
\end{align*}
Notice that $g_{\wt\rho, \wt r}(\beta_0)$ is convex and continuously differentiable, since
\begin{equation*}
    g'_{\wt\rho, \wt r}(\beta_0)
    = -2\pi \E\left[ \bigl( \tau \kappa - \wt\rho \norm{\bmu}_2 + \wt R G - \beta_0 \bigr)_+ \right]
    + 2(1 - \pi) \E\left[ \bigl( \kappa - \wt\rho \norm{\bmu}_2 + \wt R G + \beta_0 \bigr)_+ \right] 
\end{equation*}
is non-decreasing, which is based on the fact that $x \mapsto \E[(G + x)_+]$ is increasing. Then $\wt\beta_{0}$ must satisfy $g'_{\wt\rho, \wt r}(\wt\beta_{0}) = 0$. Since $g'_{\wt\rho, \wt r}(+\infty) = +\infty$, $g'_{\wt\rho, \wt r}(-\infty) = -\infty$, by our construction in the proof of \cref{lem:over_beta0}, we can choose $B$ large enough such that $\bar\xi'^{(2)}_{\kappa, B} = \bar\xi_{\kappa}^{(2)}$.


~\\
\noindent
We can rewrite $\bar\xi_{\kappa}^{(2)}$ as follows by introducing an auxiliary parameter $c$:
\begin{equation*}
    \bar\xi_{\kappa}^{(2)} =  \min_{ \substack{ \rho^2 + r^2 \le 1, r \ge 0, \beta_0 \in  \R, \\
    \rho^2 + r^2 + \beta_0^2 = c^2, c \ge 0
} }
-r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + rG_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2},
\end{equation*}
and we also define the following quantity
\begin{equation*}
    \begin{aligned}
        \wt\xi_{\kappa}^{(2)} & :=  \min_{ \substack{ \rho^2 + r^2 \le 1, r \ge 0, \beta_0 \in  \R, \\
        \rho^2 + r^2 + \beta_0^2 = c^2, c \ge 0
    } }
        \frac1c \left\{  -r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + rG_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2} 
        \right\} \\
        & \phantom{:}=  \min_{ \substack{ \rho^2 + r^2 \le 1, r \ge 0, \beta_0 \in  \R, \\
        \rho^2 + r^2 + \beta_0^2 = c^2, c \ge 0
    } }
        -\frac{r}{c} + \sqrt{\delta} \left( \E\left[ \Bigl( s(Y) \frac{\kappa}{c} - \frac{\rho}{c} \norm{\bmu}_2 + \frac{\rho}{c} G_1 + \frac{r}{c} G_2 - \frac{\beta_0}{c} Y \Bigr)_+^2 \right] \right)^{1/2}.
    \end{aligned}
\end{equation*}
Then for any $\kappa > 0$, we have the following observations:
\begin{itemize}
    \item $\sign(\bar\xi_{\kappa}^{(2)}) = \sign(\wt\xi_{\kappa}^{(2)})$. (Their objective functions differ only by a multiplier $c \ge 0$.\footnote{We allow $c = 0$. If $c = 0$, then $\rho = r = \beta_0 = 0$ and the objective value in $\bar\xi_{\kappa}^{(2)}$ is $\sqrt{ \delta(\pi\tau^2\kappa^2 + (1 - \pi)\kappa^2) } > 0$, and the objective value in $\wt\xi_{\kappa}^{(2)}$ is defined as $+\infty$. Both of them are positive.
    })
    \item The minimizer in $\wt\xi_{\kappa}^{(2)}$ must satisfy $\rho^2 + r^2 = 1$. 
    
    Suppose $(\wt\rho, \wt r, \wt\beta_0, \wt c)$ is a minimizer in $\wt\xi_{\kappa}^{(2)}$ such that $\wt\rho^2 + \wt r^2 < 1$. We can increase $(\wt\rho, \wt r, \wt\beta_0, \wt c)$ proportionally, which results in a better solution. That is, define
    \begin{equation*}
        \check\rho := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \wt\rho, \qquad
        \check r := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \wt r,  \qquad
        \check\beta_0 := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \wt\beta_0,  \qquad
        \check c := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \wt c,
    \end{equation*}
    then $(\wt\rho, \wt r, \wt\beta_0, \wt c)$ has a smaller objective value (because $r/c$, $\rho/c$, $\beta_0/c$ all remain unchanged, but $\kappa/c$ decreases since $\check{c} > \wt c$), which contradicts the optimiality of $(\wt\rho, \wt r, \wt\beta_0, \wt c)$.
\end{itemize}
As a consequence, we can simplify
\begin{align*}
        \wt\xi_{\kappa}^{(2)} & =  \min_{ \substack{ \rho \in [-1, 1], \beta_0 \in  \R, \\
        \beta_0^2 = c^2 - 1, c \ge 1
    } }
        \frac1c \left\{  -\sqrt{1 - \rho^2} + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + \sqrt{1 - \rho^2} G_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2} 
        \right\} \\
        & = \min_{ \substack{ \rho \in [-1, 1], \beta_0 \in  \R, \\
        \beta_0^2 = c^2 - 1, c \ge 1
    } }
        \frac1c \left\{  -\sqrt{1 - \rho^2} + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2} 
        \right\},
\end{align*}
where $G := \rho G_1 + \sqrt{1 - \rho^2} G_2 \sim \normal(0, 1)$. By the same argument, $\sign(\bar\xi_{\kappa}^{(3)}) = \sign(\wt\xi_{\kappa}^{(2)})$, where
\begin{equation*}
        \bar\xi_{\kappa}^{(3)}  =  \min_{ \substack{ \rho \in [-1, 1] \\  \beta_0 \in  \R } }
        -\sqrt{1 - \rho^2} + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}.
\end{equation*}
Therefore, $\sign(\bar\xi_{\kappa}^{(2)}) = \sign(\bar\xi_{\kappa}^{(3)})$.

~\\
\noindent
In order to show $\bar\xi_{\kappa}^{(2)} = \bar\xi_{\kappa}^{(3)}$ when $\bar\xi_{\kappa}^{(2)} < 0$, we define the objective function of $\bar\xi_{\kappa}^{(2)}$ as
\begin{equation*}
    T_{\kappa}(\rho, r, \beta_0) := - r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + rG_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}.
\end{equation*}
Then it suffices to show the minimizer of $T_{\kappa}$ must satisfy $\rho^2 + r^2 = 1$. Again, suppose $(\wt\rho, \wt r, \wt\beta_0)$ is a minimizer of $T_{\kappa}$ such that $\wt\rho^2 + \wt r^2 < 1$. We can increase $(\wt\rho, \wt r, \wt\beta_0)$ proportionally by defining
\begin{equation*}
        \check\rho := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \wt\rho, \qquad
        \check r := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \wt r,  \qquad
        \check\beta_0 := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \wt\beta_0,  \qquad
        \kappa' := \frac{1}{\sqrt{\wt\rho^2 + \wt r^2}} \kappa,
\end{equation*}
then
\begin{equation*}
    0 > \bar\xi_{\kappa}^{(2)} = T_{\kappa}(\wt\rho, \wt r, \wt\beta_0) 
    > 
    \frac{T_{\kappa}(\wt\rho, \wt r, \wt\beta_0) }{\sqrt{\wt\rho^2 + \wt r^2}} 
    =
    T_{\kappa'}(\check\rho, \check r, \check\beta_0) 
    >
    T_{\kappa}(\check\rho, \check r, \check\beta_0),
\end{equation*}
where the last inequality is because $x \mapsto \E[(G + c_1 x + c_2)_+^2]$ strictly increasing for any $c_1 > 0$ and $c_2 \in \R$, and the fact that $\kappa' > \kappa$. Therefore, a contradiction occurs and we complete the proof.
\end{proof}
















\subsubsection{Step 5 --- Phase transition and margin convergence: Proofs of \cref{lem:over_phase_trans}, \ref{lem:over_mar_conp}}
\label{subsubsec:over_phase}
\begin{proof}[\textbf{Proof of \cref{lem:over_phase_trans}}]
We define the following two functions:
\begin{equation}
    \label{eq:T_F_}
    \begin{aligned}
        T_\kappa(\rho, \beta_0) & := - \sqrt{1 - \rho^2} + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}, \\
        F_\kappa(\rho, \beta_0) & := -(1 - \rho^2) + \delta  \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right]
        \\
        & \phantom{:} = \pi \delta \E \left[ \bigl( G - \rho \norm{\bmu}_2 - \beta_0 + \kappa \tau \bigr)_+^2 \right]  + (1-\pi) \delta \E \left[ \bigl( G - \rho \norm{\bmu}_2 + \beta_0 + \kappa \bigr)_+^2 \right] + \rho^2 - 1,
    \end{aligned}
\end{equation}
and then
\begin{equation*}
    \bar\xi_{\kappa}^{(3)}  =  \min_{\rho \in [-1, 1] , \beta_0 \in  \R } T_\kappa(\rho, \beta_0),
    \qquad
    \wt\xi_{\kappa}^{(3)}  :=  \min_{\rho \in [-1, 1] , \beta_0 \in  \R } F_\kappa(\rho, \beta_0).
\end{equation*}
Clearly, $\sign(T_\kappa(\rho, \beta_0)) = \sign(F_\kappa(\rho, \beta_0))$ for any $\rho, \beta_0$ and $\sign(\bar\xi_{\kappa}^{(3)}) = \sign(\wt\xi_{\kappa}^{(3)})$. Also recall that
\begin{equation*}
    \delta^*(\kappa) = \max_{ \substack{\rho \in [-1, 1] \\ \beta_0 \in \R } }  H_\kappa(\rho, \beta_0),
    \qquad 
    H_\kappa(\rho, \beta_0) = \frac{1 - \rho^2}{\E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right]}.
\end{equation*}
We can see that $H_\kappa(\rho, \beta_0)$ is well-defined since $\E[ (  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y )_+^2 ]$ is bounded away from zero for any $\rho \in [-1, 1]$ and $\beta_0 \in \R \cup\{ \pm \infty \}$.

\vspace{0.5\baselineskip}
\noindent
Since $x \mapsto \E[(G + c_1 x + c_2)_+^2]$ is continuous and strictly increasing for any $c_1 > 0, c_2 \in \R$, it can be shown that both $\kappa \mapsto \bar\xi_{\kappa}^{(3)}$, $\kappa \mapsto \wt\xi_{\kappa}^{(3)}$ are continuous strictly increasing, and $\delta^*(\kappa)$ is continuous strictly decreasing (by restricting $\beta_0: \abs{\beta_0} \le B$ for some constant $B$ large enough, similar as Step 4, and then use compactness). Therefore, we have the following equivalent definitions of $\kappa^*$:
\begin{equation}\label{eq:kappa_star}
    \begin{aligned}
        \kappa^*  & := \sup\left\{ \kappa \in \R: \delta^*(\kappa) \ge \delta \right\} \\
        & \phantom{:} = \left\{ \kappa \in \R: \delta^*(\kappa) = \delta \right\} 
        = \left\{ \kappa \in \R: \bar\xi_{\kappa}^{(3)} = 0 \right\}
        = \left\{ \kappa \in \R:  \wt\xi_{\kappa}^{(3)} = 0 \right\}.
    \end{aligned}
\end{equation}
Now we can consider the following two regimes, each with a chain of equivalence:
\begin{equation}
    \label{eq:phase_equiv}
    \begin{aligned}
        \delta \le \delta^*(\kappa) 
        \quad   \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\Longleftrightarrow} \quad
        \kappa \le \kappa^*
        \quad & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\Longleftrightarrow} \quad
        \bar\xi_{\kappa}^{(3)}, \wt\xi_{\kappa}^{(3)} \le 0
        \quad   \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\Longleftrightarrow} \quad
        \bar\xi_{\kappa}^{(2)} \le 0 
        \\
        & \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{\Longleftrightarrow} \quad
        \xi'_{n, \kappa, B}  \conp \bigl( \bar\xi^{(2)}_{\kappa} \bigr)_+ = 0
        \quad \overset{\mathmakebox[0pt][c]{\text{(iv)}}}{\Longleftrightarrow} \quad
        \P(\xi_{n, \kappa} = 0) \to 1,
        \\
        \delta > \delta^*(\kappa) 
        \quad   \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\Longleftrightarrow} \quad
        \kappa > \kappa^*
        \quad & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\Longleftrightarrow} \quad
        \bar\xi_{\kappa}^{(3)}, \wt\xi_{\kappa}^{(3)} > 0
        \quad   \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\Longleftrightarrow} \quad
        \bar\xi_{\kappa}^{(2)} > 0 
        \\
        & \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{\Longleftrightarrow} \quad
        \xi'_{n, \kappa, B}  \conp \bigl( \bar\xi^{(2)}_{\kappa} \bigr)_+ > 0
        \quad \overset{\mathmakebox[0pt][c]{\text{(iv)}}}{\Longleftrightarrow} \quad
        \P(\xi_{n, \kappa} > 0) \to 1,
    \end{aligned}
\end{equation}
where (i) is from \cref{eq:kappa_star}, (ii) is from \cref{lem:over_sign}, (iii) is from \cref{lem:over_CGMT}, \ref{lem:over_ULLN}, and (iv) is from \cref{lem:over_beta0}. Linear separability considers the special case $\kappa = 0$. From definition \cref{eq:xi_n_kappa}, for any $\kappa \le 0$ we have $\xi_{n, \kappa} = 0$ (by taking $\vbeta = \bzero$, $\beta_0 = 0$). Therefore, 
\begin{itemize}
    \item If $\delta < \delta^*(0)$, by \cref{eq:phase_equiv} $\kappa^* > 0$ and $\P(\mathcal{E}_{n, \kappa^*}) = \P(\xi_{n, \kappa^*} = 0)
    \to 1$, which deduces the data is linearly separable with high probability.
    \item If $\delta > \delta^*(0)$, by \cref{eq:phase_equiv} $\kappa^* < 0$ and $\P(\mathcal{E}_{n, \kappa}) = \P(\xi_{n, \kappa} = 0)
    \to 0$ for any $\kappa > 0$ (as $\kappa \mapsto \xi_{n, \kappa}$ is non-decreasing), which implies the data is not linearly separable with high probability.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Old ver.: \norm{\beta} = 1 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% There is a small caveat that the constraint of $\vbeta$ in the max-margin optimization problem \cref{eq:over_max-margin} is $\norm{\vbeta}_2 = 1$, while in $\xi_{n,\kappa}$ \cref{eq:xi_n_kappa} it is $\norm{\vbeta}_2 \le 1$. To handle this, we can define 
% \begin{equation}
%     \label{eq:over_wt_kappa}
%     \wt\kappa_n := \sup\{ \kappa \in \R:  \xi_{n,\kappa} = 0 \}.
% \end{equation}
% Notice that $\wt\kappa_n = \hat\kappa_n \mathbbm{1}\{ \hat\kappa_n > 0 \} \ge 0$ always holds. 
% If $\delta < \delta^*(0)$, then $\hat\kappa_n > 0$ with high probability and hence $\P(\wt\kappa_n = \hat\kappa_n) \to 1$ as $n \to \infty$. Therefore, we only have to show that $\wt\kappa_n \conp \kappa^*$. Notice that $\kappa^* > 0$ if $\delta < \delta^*(0)$, and $ \bar\xi_{\kappa^*}^{(3)} = 0$. 
\end{proof}

\begin{proof}[\textbf{Proof of \cref{lem:over_mar_conp}}]
If $\delta < \delta^*(0)$, then $\kappa^* > 0$ and $\bar\xi_{\kappa^*}^{(3)} = 0$. According to \cref{eq:phase_equiv}, for any $\varepsilon > 0$ small enough, we have
\begin{equation*}
    \begin{aligned}
        \bar\xi_{\kappa^* - \varepsilon}^{(3)} < 0
    \quad & \Longrightarrow \quad
    \P(\mathcal{E}_{n, \kappa^* - \varepsilon}) = \P(\xi_{n, \kappa^* - \varepsilon} = 0) \to 1, \\
        \bar\xi_{\kappa^* + \varepsilon}^{(3)} > 0
    \quad & \Longrightarrow \quad
    \P(\mathcal{E}_{n, \kappa^* + \varepsilon}) = \P(\xi_{n, \kappa^* + \varepsilon} = 0) \to 0.
    \end{aligned}
\end{equation*}
% Combine these arguments above with \cref{eq:over_wt_kappa}, we see that $\kappa^* - \varepsilon \le \wt\kappa_n \le \kappa^* + \varepsilon$ with probability approaching one, which proves that $\wt\kappa_n \conp \kappa^*$, and hence $\hat\kappa_n \conp \kappa^*$.
Recall that $\hat\kappa_n = \ind_{1 \le n_+ \le n - 1} \sup\{ \kappa \in \R:  \xi_{n,\kappa} = 0 \}$.
By combining these arguments, we can see that $\kappa^* - \varepsilon \le \hat\kappa_n \le \kappa^* + \varepsilon$ holds on the event $\mathcal{D}_n^c$, with high probability. This proves $\hat\kappa_n \conp \kappa^*$.
% \begin{itemize}
%     \item If $\delta \le \delta^*(\kappa)$, or equivalently $\kappa \le \kappa^*$, then we get $\bar\xi_{\kappa}^{(3)}, \wt\xi_{\kappa}^{(3)} \le 0$, and \cref{lem:over_sign} gives $\bar\xi_{\kappa}^{(2)} \le 0$. Combining \cref{lem:over_beta0}, \ref{lem:over_CGMT} and \ref{lem:over_ULLN}, we obtain $\xi'_{n, \kappa, B}  \conp \bigl( \bar\xi^{(2)}_{\kappa} \bigr)_+ = 0$ and $\P(\xi_{n, \kappa} = 0) \to 1$.
%     \item If $\delta > \delta^*(\kappa)$, or equivalently $\kappa > \kappa^*$, then we get $\bar\xi_{\kappa}^{(3)}, \wt\xi_{\kappa}^{(3)} > 0$, and \cref{lem:over_sign} gives $\bar\xi_{\kappa}^{(2)} > 0$. Combining \cref{lem:over_beta0}, \ref{lem:over_CGMT} and \ref{lem:over_ULLN}, we obtain $\xi'_{n, \kappa, B}  \conp \bigl( \bar\xi^{(2)}_{\kappa} \bigr)_+ > 0$ and $\P(\xi_{n, \kappa} > 0) \to 1$.
% \end{itemize}
\end{proof}

\subsubsection{Convergence of ELD and parameters for $\tau = 1$: 
Proofs of \cref{lem:over_logit_conv}, \ref{lem:H_kappa_1}}
\label{subsubsec:over_logit_conv}

In this section, we provide a proof of parameter convergence in \cref{thm:SVM_main}\ref{thm:SVM_main_param} and ELD convergence in \ref{thm:SVM_main_logit} for the special case $\tau = 1$. For convenience of notation, we drop the subscripts and simply write $\hat\rho := \hat\rho_n$, $\hat\beta_{0} := \hat\beta_{0,n}$. 
% Recall the empirical distribution of logit margins (well-defined version, \cref{eq:over_ELD_well}) and asymptotic distribution are respectively defined as
% \begin{equation*}
%     \hat \cL_{n} = \frac1n \sum_{i=1}^n \delta_{y_i ( \< \xx_i, \hat\vbeta \> + \hat\beta_{0} ) \ind\{1 \le n_+ \le n - 1\} },
%     \qquad 
%     \cL_* = \Law\left( 
%     \max\bigl\{ \kappa^*,  \rho^*\norm{\vmu}_2 + G + \beta_0^* Y    \bigr\}
%     \right).
% \end{equation*}
Recall the ELD (well-defined version, i.e., \cref{eq:over_ELD_well}) and its asymptotics are respectively defined as
\begin{equation*}
    \hat \nu_{n} = \frac1n \sum_{i=1}^n \delta_{(y_i, \< \xx_i, \hat\vbeta \> + \hat\beta_{0} ) \cdot \ind\{\mathcal{D}_n^c\} },
    % \hat \nu_{n} = \frac1n \sum_{i=1}^n \delta_{(y_i, \< \xx_i, \hat\vbeta \> + \hat\beta_{0})},
    \qquad 
    \nu_* = \Law\left( Y,
    Y \max\bigl\{ \kappa^*, \rho^*\norm{\vmu}_2 + G + \beta_0^* Y \bigr\}
    \right).
\end{equation*}
Here $(\rho^*, \beta_0^*, \kappa^*)$ is defined as the maximizer of \cref{eq:SVM_asymp_simple}, and obviously $\kappa^*$ also satisfies \cref{eq:kappa_star}. The uniqueness of $(\rho^*, \beta_0^*)$ will be given by \cref{lem:H_kappa_1}. Analogous to the proof of \cite[Theorem 4.6]{montanari2022overparametrizedlineardimensionalityreductions}, by using the theory of projection pursuit therein, we have the following results.
\begin{lem}[ELD and parameter convergence] 
\label{lem:over_logit_conv}
Consider $\tau = 1$. As $n, d \to \infty$, we have
\begin{equation*}
    W_2 \bigl( \hat \nu_{n}, \nu_* \bigr)
    \conp 0.
\end{equation*}
The convergence of $\hat\rho \conp \rho^*$ and $\hat\beta_{0} \conp \beta_0^*$ are followed by continuity and convexity of $H_\kappa$ in \cref{eq:sep_functions}.
\end{lem}
\begin{proof}
Our proof primarily follows the setup in \cite[Section 4.1]{montanari2022overparametrizedlineardimensionalityreductions} and techniques in \cite[Section 4.3]{montanari2022overparametrizedlineardimensionalityreductions}. Recall that we can rewrite $\xx_i = y_i \bmu + \zz_i$, where $\zz_i \iidsim \normal(\bzero, \bI_d)$ and $y_i \indep \zz_i$. Using notation from \cite{montanari2022overparametrizedlineardimensionalityreductions}, $\P(y_i = 1 \,|\, \zz_i) = \varphi(\vmu_0^\top \zz_i)$, where $\vmu_0 = \vmu/\norm{\vmu}_2$ and $\varphi(x) \equiv \pi$ is a constant function. Recall that we reparametrize $\hat\rho = \vmu_0^\top \hat\vbeta$. Now, define random variables with joint distribution
\begin{equation*}
    Y \indep G \indep Z, 
    \quad \P(Y = +1 \,|\, G) = 1 - \P(Y = -1 \,|\, G) = \varphi(G) \equiv \pi, 
    \quad G, Z \sim \normal(0, 1).
\end{equation*}
Let $(Y, G, Z) \indep \hat\vbeta$. According to the definition in \cite[Lemma 4.2]{montanari2022overparametrizedlineardimensionalityreductions}, we have
\begin{equation*}
    \Law\left( Y, \vmu_0^\top \hat\vbeta \cdot G + \sqrt{1 - (\vmu_0^\top \hat\vbeta)^2} \cdot Z \right)
    = \Law\left( Y, \hat\rho G + \sqrt{1 - \hat\rho^2} Z \right)
    = \Law(Y, Z).
\end{equation*}
Therefore, by using \cite[Theorem 4.3]{montanari2022overparametrizedlineardimensionalityreductions}, for any $\varepsilon, \eta > 0$, with high probability we have
\begin{equation*}
    W_2^{(\eta)} \biggl( 
        \frac1n \sum_{i=1}^n \delta_{(y_i, \< \zz_i, \hat\vbeta \>)} , 
        \Law(Y, Z)        
     \biggr) 
     \le \frac{\sqrt{1 - \hat\rho^2}}{\sqrt{\delta}} + \varepsilon,
\end{equation*}
where $W_2^{(\eta)}$ is the $\eta$-constrained $W_2$ distance \cite[Definition 4.1]{montanari2022overparametrizedlineardimensionalityreductions}. Formally, for any $\eta > 0$, the $\eta$-constrained $W_2$ distance between any two probability measures $P$ and $Q$ in $\R^d$ is defined by
\begin{equation*}
    W_2^{(\eta)}(P, Q) := \left( \inf_{\gamma \in \Gamma^{(\eta)}(P, Q) }
    \int_{\R^d \times \R^d} \norm{\xx - \yy}_2^2 \gamma(\d\xx \times \d\yy)
    \right)^{1/2},
\end{equation*}
where $\Gamma^{(\eta)}(P, Q)$ denotes the set of all couplings $\gamma$ of $P$ and $Q$ which satisfy
\begin{equation}
    \label{eq:W2eta}
    \left(
        \int_{\R^d \times \R^d} |\< \be_1, \xx - \yy \>|^2 \gamma(\d\xx \times \d\yy) 
    \right)^{1/2}    
    \le \eta,
\end{equation}
where $\be_1 = (1, 0, \dots, 0)^\top$. 


The following proof is analogous to the proof of \cite[Theorem 4.6]{montanari2022overparametrizedlineardimensionalityreductions}. We show the convergence of logit margins $W_2( \hat\cL_n, \cL_* ) \conp 0$ first, where
\begin{equation}
    \label{eq:margin_logit_dist}
    \hat \cL_{n} := \frac1n \sum_{i=1}^n \delta_{y_i ( \< \xx_i, \hat\vbeta \> + \hat\beta_{0} ) },
    \qquad 
    \cL_* := \Law\left( 
    \max\bigl\{ \kappa^*,  \rho^*\norm{\vmu}_2 + G + \beta_0^* Y    \bigr\}
    \right).
\end{equation}
Throughout this subsection, all the expectations (including the one in $H_\kappa$) are conditional on $\{ (y_i, \zz_i) \}_{i=1}^n$, which will be denoted as $\E_{\cdot | n}[\cdot]$. Now, let
\begin{equation*}
    \frac1n \sum_{i=1}^n \delta_{(y_i, \< \zz_i, \hat\vbeta \>)} =: \Law(Y', Z'),
\end{equation*}
then by definition in \cref{eq:W2eta} and the same arguments in the proof of \cite[Theorem 4.6]{montanari2022overparametrizedlineardimensionalityreductions}, there exists a coupling $(Y, Z, Y', Z')$ and a sufficiently small $\eta$ ($\eta < \varepsilon^2/4$), such that
\begin{equation}
    \label{eq:pursuit_pre}
    % W_2 \biggl( 
    %     \frac1n \sum_{i=1}^n \delta_{y_i \< \zz_i, \hat\vbeta \>} , 
    %     \Law(YZ)        
    %  \biggr) 
    %  \le
    \left( \E_{\cdot | n}\bigl[(Y - Y')^2\bigr] \right)^{1/2} \le \eta,
    \qquad
    \left( \E_{\cdot | n}\bigl[(YZ - Y'Z')^2\bigr] \right)^{1/2} \le \frac{\sqrt{1 - \hat\rho^2}}{\sqrt{\delta}} + 2\varepsilon
\end{equation}
holds with high probability. 
We can express the empirical distribution of logit margins \cref{eq:margin_logit_dist} as
% Now, temporarily write the original (ill-defined) empirical distribution of logit margins as
\begin{equation}
    \label{eq:emp_ell}
    \hat \cL_{n}
    = \frac1n \sum_{i=1}^n \delta_{y_i \< \zz_i, \hat\vbeta \> + \hat\rho \norm{\vmu}_2 + y_i \hat\beta_0 ) }
    = \Law \Bigl( \underbrace{Y'Z' + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y' }_{=: V} \Bigr).
\end{equation}
For convenience, denote $\hat V := YZ + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y$, then with high probability we have
\begin{align}
        \bigl( \E_{\cdot | n}\bigl[(V - \hat V)^2\bigr] \bigr)^{1/2}
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\le} \left( \E_{\cdot | n}\bigl[(YZ - Y'Z')^2\bigr] \right)^{1/2} 
        + \left( \E_{\cdot | n}\bigl[(Y - Y')^2\bigr] \right)^{1/2} |\hat\beta_0|  \notag \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le} \frac{\sqrt{1 - \hat\rho^2}}{\sqrt{\delta}} + 2\varepsilon + \eta B \notag \\
        & \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{\le} \frac{\sqrt{1 - \hat\rho^2}}{\sqrt{\delta}} + 3\varepsilon,
        \label{eq:V_diff0}
\end{align}
where (i) follows from Minkowski inequality, (ii) uses \cref{eq:pursuit_pre} and $|\hat\beta_0| \le B$ from \cref{lem:over_beta0}, by recalling that $\delta < \delta^*(0)$ and the data is linearly separable with high probability, while in (iii) we choose $\eta < \min\{ \varepsilon^2/4, \varepsilon/B \}$. According to $\hat\kappa_n \conp \kappa^*$ from \cref{lem:over_mar_conp}, we know that
\begin{equation*}
    \lim_{n \to \infty} \P\left( y_i (\< \hat\vbeta , \xx_i \> + \hat\beta_0 ) \ge \kappa^* - \varepsilon,
    \forall\, i \in [n] \right) = 1.
\end{equation*}
Then by definition of $V$ in \cref{eq:emp_ell}, with high probability we have 
\begin{equation}
    \label{eq:V_kappa_as}
    V \ge \kappa^* - \varepsilon,
    \qquad \text{almost surely}.
\end{equation}
Now, recall $\delta = \delta^*(\kappa^*) = H_{\kappa^*}(\rho^*, \beta_0^*)$ by \cref{eq:kappa_star}, where $(\rho^*, \beta_0^*) = \argmin_{\rho \in [-1, 1], \beta_0 \in \R} H_{\kappa^*}(\rho, \beta_0)$. Therefore,
\begin{equation}
    \label{eq:V_diff1}
        \bigl( \E_{\cdot | n}\bigl[(V - \hat V)^2\bigr] \bigr)^{1/2}
        \le \frac{\sqrt{1 - \hat\rho^2}}{\sqrt{\delta}} + 3\varepsilon
        = \frac{\sqrt{1 - \hat\rho^2}}{\sqrt{H_{\kappa^*}(\rho^*, \beta_0^*)}} + 3\varepsilon
\end{equation}
holds with high probability. For $\rho \in [-1, 1], \beta_0 \in \R$, let us define
\begin{equation*}
    h_{\kappa^*}^*(\rho, \beta_0) := \frac{1}{\sqrt{H_{\kappa^*}(\rho, \beta_0)}}
    - \frac{1}{\sqrt{H_{\kappa^*}(\rho^*, \beta_0^*)}}.
\end{equation*}
Note that $h_{\kappa^*}^*(\rho, \beta_0) \ge 0$. Hence, \cref{eq:V_diff1} implies that (reminding $\tau = 1$) with high probability
\begin{align*}
        \bigl( \E_{\cdot | n}\bigl[(V - \hat V)^2\bigr] \bigr)^{1/2}
        & \le \sqrt{1 - \hat\rho^2} \biggl( \sqrt{ \frac{1}{H_{\kappa^*}(\hat\rho, \hat\beta_0)} } - h_{\kappa^*}^*(\hat\rho, \hat\beta_0) \biggr) + 3\varepsilon \\
        & = \! \left( \E_{\cdot | n}\! \left[ \bigl(  \kappa^* - \hat\rho \norm{\bmu}_2 + G - \hat\beta_0 Y \bigr)_+^2 \right] \right)^{1/2}
        - \sqrt{1 - \hat\rho^2} \cdot h_{\kappa^*}^*(\hat\rho, \hat\beta_0) 
        + 3\varepsilon \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{=} \bigl( \E_{\cdot | n}\bigl[(\kappa^* - \hat V)_+^2 \bigr] \bigr)^{1/2}
        - \sqrt{1 - \hat\rho^2} \cdot h_{\kappa^*}^*(\hat\rho, \hat\beta_0) 
        + 3\varepsilon,
\end{align*}
which can be further written as (with high probability)
\begin{align}
        \bigl( \E_{\cdot | n}\bigl[(V - \hat V)^2\bigr] \bigr)^{1/2} + \sqrt{1 - \hat\rho^2} \cdot h_{\kappa^*}^*(\hat\rho, \hat\beta_0) 
        & \le
        \bigl( \E_{\cdot | n}\bigl[(\kappa^* - \hat V)_+^2\bigr] \bigr)^{1/2}
        + 3\varepsilon \notag  \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le} 
        \bigl( \E_{\cdot | n}\bigl[(\kappa^* - \varepsilon - \hat V)_+^2\bigr] \bigr)^{1/2}
        + 4\varepsilon.
        % & = 
        % \! \left( \E_{\cdot | n}\!\left[ \bigl( \max\{ \kappa^* - \varepsilon , \hat V \}  - \hat V \bigr)^2 \right] \right)^{1/2}
        % + 4\varepsilon.
        \label{eq:V_diff2}
\end{align}
In the derivation above, equation (i) follows from $\hat V = YZ + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y 
\overset{\mathmakebox[0pt][c]{\mathrm{d}}}{=} -G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y$ when conditioning on $\{(y_i, \zz_i)\}_{i = 1}^n$, and (ii) follows from the fact that
\begin{equation*}
    \begin{aligned}
        \frac{\d}{\d \kappa}  \bigl( \E_{\cdot | n}\bigl[(\kappa - \hat V)_+^2\bigr] \bigr)^{1/2}
        & = \frac{
            \E_{\cdot | n}\bigl[(\kappa - \hat V)_+^2\bigr]
        }{
            \bigl( \E_{\cdot | n}\bigl[(\kappa - \hat V)_+^2\bigr] \bigr)^{1/2}
        }
        \le 1.
    \end{aligned}
\end{equation*}
Besides, by using \cref{eq:V_kappa_as} and exactly the same arguments in the proof of \cite[Theorem 4.6]{montanari2022overparametrizedlineardimensionalityreductions}, we can show that with high probability,
\begin{equation}
    \label{eq:V_max}
    \E_{\cdot | n} \!\left[ \bigl(V - \max\{ \kappa^* - \varepsilon, \hat V \} \bigr)^2 \right]
    \le \E_{\cdot | n}\bigl[(V - \hat V)^2\bigr] - 
    \E_{\cdot | n}\bigl[(\kappa^* - \varepsilon - \hat V)_+^2\bigr].
\end{equation}
Combining \cref{eq:V_max} with \eqref{eq:V_diff2} gives the following implications:
\begin{itemize}
    \item \cref{eq:V_max} implies 
    \begin{equation*}
        \E_{\cdot | n}\bigl[(\kappa^* - \varepsilon - \hat V)_+^2\bigr]
        \le
        \E_{\cdot | n} \bigl[(V - \hat V)^2\bigr].
    \end{equation*}
    Plugging this into \cref{eq:V_diff2} yields that with high probability,
    \begin{equation*}
        \sqrt{1 - \hat\rho^2} \cdot h_{\kappa^*}^*(\hat\rho, \hat\beta_0) \le 4 \varepsilon,
    \end{equation*}
    i.e., $\sqrt{1 - \hat\rho^2} \cdot h_{\kappa^*}^*(\hat\rho, \hat\beta_0) \conp 0$. Note that if $\abs{\rho} \to 1$ (i.e., $\sqrt{1 - \rho^2} = o_\varepsilon(1)$), the quantity
    \begin{equation*}
            \sqrt{1 - \rho^2} \cdot h_{\kappa^*}^*(\rho, \beta_0)
        = \left( \E \left[ \bigl(  \kappa^* - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}
        - \frac{\sqrt{1 - \rho^2}}{\sqrt{H_{\kappa^*}(\rho^*, \beta_0^*)}}
    \end{equation*}
    is bounded away from 0, for any $\beta_0 \in \R \cup\{ \pm \infty\}$. Therefore, we must have $h_{\kappa^*}^*(\hat\rho, \hat\beta_0) \conp 0$. By \cref{lem:H_kappa_1} (proof is deferred to the end of this subsection), we know $h_{\kappa^*}^*(\rho, \beta_0) \ge 0$ for all $\rho \in [-1, 1], \beta_0 \in \R$, and $(\rho, \beta_0) \to (\rho^*, \beta_0^*)$ if and only if $h_{\kappa^*}^*(\rho, \beta_0) \to 0$. Hence, we conclude
    \begin{equation*}
        (\hat\rho, \hat\beta_0) \conp (\rho^*, \beta_0^*),
    \end{equation*}
    which gives parameter convergence.

    \item Let
    \begin{equation*}
        I := \bigl( \E_{\cdot | n}\bigl[(V - \hat V)^2\bigr] \bigr)^{1/2} 
        , \qquad
        I\!I := \bigl( \E_{\cdot | n}\bigl[(\kappa^* - \varepsilon - \hat V)_+^2\bigr] \bigr)^{1/2}.
    \end{equation*}
    Then \cref{eq:V_diff2} implies $I - I\!I \le 4\varepsilon$, and we also have (for $\varepsilon > 0$ small enough)
    \begin{equation*}
        \begin{aligned}
            I\!I & \le \abs{\kappa^* - \varepsilon} + \bigl( \E_{\cdot | n}\bigl[\hat V^2\bigr] \bigr)^{1/2} 
            \le \kappa^* + \bigl( \E_{\cdot | n}\bigl[ (
                G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y
            )^2\bigr] \bigr)^{1/2}  \\
            & \le \kappa^* + \left(\E[G^2]\right)^{1/2} + |\hat\rho| \norm{\vmu}_2 + |\hat\beta_0| \\
            & \le \kappa^* + 1 + \norm{\vmu}_2 + B,
        \end{aligned}
    \end{equation*}
    by using Minkowski inequality and $|\hat\beta_0| \le B$ (with high probability) from \cref{lem:over_beta0}. Based on these results and \cref{eq:V_max}, with high probability, we have
    \begin{equation*}
        \begin{aligned}
            \E_{\cdot | n} \!\left[ \bigl(V - \max\{ \kappa^* - \varepsilon, \hat V \} \bigr)^2 \right] 
            & \le I^2 - I\!I^2 = (I - I\!I)(I - I\!I + 2I\!I) \\
            & \le 4\varepsilon \bigl(4\varepsilon + 2 (\kappa^* + 1 + \norm{\vmu}_2 + B) \bigr) \\
            & \le C \varepsilon,
        \end{aligned}
    \end{equation*}
    where $C \in (0, \infty)$ is some constant depending on $(\pi, \norm{\bmu}_2, \delta)$ (through $\kappa^*, B$). Therefore, by recalling $\hat V \overset{\mathmakebox[0pt][c]{\mathrm{d}}}{=} G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y$, we obtain that with high probability,
    \begin{equation}
        \label{eq:W2_conv}
        W_2\left( \hat \cL_{n}, \Law \bigl(\max\{ \kappa^* - \varepsilon, G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y \} \bigr) \right) \le \sqrt{C \varepsilon}.
    \end{equation}
\end{itemize}    
As a consequence, 
% there exists some event $\mathcal{A}_n$ with $\P(\mathcal{A}_n) \to 1$ as $n \to \infty$, such that the following bound holds on $\mathcal{A}_n$ (for $n$ large enough):
the following holds with high probability:
    \begin{align*}
            W_2\bigl( \hat \cL_{n}, \cL_* \bigr)
            & = W_2 \left( 
                \hat \cL_{n},
                \Law\bigl( \max\{ \kappa^*, G + \rho^*\norm{\vmu}_2 + \beta_0^* Y \} \bigr)
             \right) \\
            & \le
            W_2\left( \hat \cL_{n}, \Law \bigl(\max\{ \kappa^* - \varepsilon, G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y \} \bigr) \right) \\
            & \phantom{\le} \  + W_2\left( \Law \bigl(\max\{ \kappa^* - \varepsilon, G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y \} \bigr),
            \Law \bigl(\max\{ \kappa^* , G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y \} \bigr) \right)
            \\
            & \phantom{\le} \  + W_2\left( \Law \bigl(\max\{ \kappa^* , G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y 
            \} \bigr) 
            ,
            \Law\bigl( \max\{ \kappa^*, G + \rho^*\norm{\vmu}_2 + \beta_0^* Y \} \bigr)
            \right) \\
            & \le \sqrt{C \varepsilon} + \varepsilon + o_\varepsilon(1)
            = o_\varepsilon(1),
    \end{align*}
    where in the last inequality, we use that (i) the result from \cref{eq:W2_conv}, (ii) the fact that the mapping $\kappa \mapsto \max\{ \kappa, G + \hat\rho \norm{\vmu}_2 + \hat\beta_0 Y \}$ is 1-Lipschitz, and (iii) the consequence of $(\hat\rho, \hat\beta_0) \conp (\rho^*, \beta_0^*)$ and $|\hat\rho| \le 1$, $|\hat\beta_0| \le B$ (with high probability). 
    
    \vspace{0.5\baselineskip}
	\noindent
    % Recall $\wt\cL_n = \hat\cL_n$ on the event $\cD_n^c = \{ 1 \le n_+ \le n - 1 \}$ and $\P(\cA_n \cap \cD_n^c) \to 1$. Therefore, with high probability $W_2\bigl( \hat \cL_{n}, \cL_* \bigr) = o_\varepsilon(1)$.  
    % Finally, by taking $\varepsilon \to 0$, we prove
    % \begin{equation*}
    %     W_2\bigl( \hat \cL_{n}, \cL_* \bigr) \conp 0.
    % \end{equation*}
    Now we prove the convergence of ELD. Denote $\hat \cL_{n} =: \Law(L')$, $\cL_* =: \Law(L)$, where $(L, L')$ is a coupling such that
    \begin{equation}\label{eq:L_coupling}
        \left( \E_{\cdot | n}\bigl[(L - L')^2\bigr] \right)^{1/2} = o_\varepsilon(1).
    \end{equation}
    Therefore, for some constants $C_1, C_2 > 0$, with high probability, we have
    \begin{align*}
        W_2\bigl( \hat \nu_{n}, \nu_* \bigr)
        & \le
        \left( \E_{\cdot | n}\bigl[(Y - Y')^2\bigr] \right)^{1/2} 
        +
        \left( \E_{\cdot | n}\bigl[(YL - Y'L')^2\bigr] \right)^{1/2} \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\le} 
        \eta + \left( \E_{\cdot | n}\bigl[(YL - Y'L)^2\bigr] \right)^{1/2}
        + \left( \E_{\cdot | n}\bigl[(Y'L - Y'L')^2\bigr] \right)^{1/2} \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le}
        \eta + C_1 \left( \E_{\cdot | n}\bigl[(Y - Y')^2\bigr] \right)^{1/4} \left(\E[L^4]\right)^{1/4}
        + \left( \E_{\cdot | n}\bigl[(L - L')^2\bigr] \right)^{1/2} \\
        & \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{\le}
        \eta + C_2 \sqrt{\eta} + o_\varepsilon(1)
        \overset{\mathmakebox[0pt][c]{\text{(iv)}}}{\le}  o_\varepsilon(1),
    \end{align*}
    where in (i) we use \cref{eq:pursuit_pre} and Minkowski inequality, in (ii) use Cauchy--Schwarz inequality and $Y, Y' \in \{ \pm 1 \}$, in (iii) use \cref{eq:pursuit_pre} and \eqref{eq:L_coupling}, while in (iv) recall that $\eta < \min\{ \varepsilon^2/4, \varepsilon/B \} = o_{\varepsilon}(1)$. By taking $\varepsilon \to 0$, we can show that $W_2\bigl( \hat \nu_{n}, \nu_* \bigr) \conp 0$ for $\tau = 1$. This completes the proof.
    % \begin{equation*}
    %     \begin{aligned}
    %         W_2\bigl( \hat \cL_{n}, \cL_* \bigr)
    %     & = W_2\, \Bigl( \Law( V \ind_{\cD_n^c} ), \Law \bigl(\max\{ \kappa^*, G + \rho^* \norm{\vmu}_2 + \beta_0^*  Y \} \bigr) \Bigr)
    %         \\
    %     & \le W_2\, \Bigl( \Law( V \ind_{\cD_n^c} ), \Law \bigl(\max\{ \kappa^*, G + \rho^* \norm{\vmu}_2 + \beta_0^*  Y \} \ind_{\cD_n^c} \bigr) \Bigr) \\
    %     & \phantom{\le} \  +  
    %     W_2\, \Bigl( \Law \bigl(\max\{ \kappa^*, G + \rho^* \norm{\vmu}_2 + \beta_0^*  Y \} \ind_{\cD_n^c} \bigr), \Law \bigl(\max\{ \kappa^*, G + \rho^* \norm{\vmu}_2 + \beta_0^*  Y \}  \bigr) \Bigr)  \\
    %     & \le W_2\, \Bigl( \Law( V ), \Law \bigl(\max\{ \kappa^*, G + \rho^* \norm{\vmu}_2 + \beta_0^*  Y \} \bigr) \Bigr) \\
    %     & \phantom{\le} \  +  
    %     \left( \E_{\cdot | n}\bigl[  (\max\{ \kappa^*, G + \rho^* \norm{\vmu}_2 + \beta_0^*  Y \} \ind_{\cD_n} )^2  \bigr] \right)^{1/2}  \\
    %     & \le W_2\bigl( \hat \cL_{n}, \cL_* \bigr)
    %     + \left( \E\bigl[  (\max\{ \kappa^*, G + \rho^* \norm{\vmu}_2 + \beta_0^*  Y \} )^2  \bigr] \right)^{1/2} \ind_{\cD_n} \\
    %     & \le o_\varepsilon(1) + 
    %     \end{aligned}
    % \end{equation*}
\end{proof}



Finally, we prove the following technical lemma.
\begin{lem}
    \label{lem:H_kappa_1}
    For any fixed $\kappa \in \R$ and $\tau > 0$, the function $H_\kappa(\rho, \beta_0)$ in \cref{eq:sep_functions} admits a unique maximizer $(\rho^*(\kappa), \beta_0^*(\kappa)) \in [0, 1) \times \R$.
\end{lem}
\begin{proof}
    For simplicity, write $\rho^* := \rho^*(\kappa)$, $\beta_0^* := \beta_0^*(\kappa)$. First, note that
    \begin{equation*}
        H_\kappa(\rho, \beta_0) = \frac{1 - \rho^2}{\E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right]} \le \, \frac{1}{\E\left[ \bigl(  s(Y) \kappa - \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right]},
    \end{equation*}
    which converges to $0$ as $\beta_0 \to \pm\infty$. Moreover, $H_\kappa(-\rho, \beta_0) < H_\kappa(\rho, \beta_0)$ for any $\rho \in (0, 1]$. Therefore, $H_\kappa(\rho, \beta_0)$ must have a maximizer $(\rho^*, \beta_0^*) \in [0, 1] \times \R$. Further, $\rho^* \in [0, 1)$ since $H_\kappa(1, \beta_0) \equiv 0$. We prove the uniqueness of $(\rho^*, \beta_0^*)$ by contradiction. For future convenience, we denote $H_{\max} := H_{\kappa} (\rho^*, \beta_0^*)$. Assume that there exist $(\rho_1, \beta_{0, 1})$ and $(\rho_2, \beta_{0, 2})$ such that $(\rho_1, \beta_{0, 1}) \neq (\rho_2, \beta_{0, 2})$, and
    \begin{equation*}
        H_{\kappa} (\rho_1, \beta_{0, 1}) = H_{\kappa} (\rho_2, \beta_{0, 2}) = H_{\max},
    \end{equation*}
    which implies
    \begin{equation*}
        G_{\kappa} (\rho_1, \beta_{0, 1}) = \frac{\sqrt{1 - \rho_1^2}}{\sqrt{H_{\max}}}, 
        \qquad 
        G_{\kappa} (\rho_2, \beta_{0, 2}) = \frac{\sqrt{1 - \rho_2^2}}{\sqrt{H_{\max}}},
    \end{equation*}
    where we define
    \begin{equation*}
        G_\kappa(\rho, \beta_0) := \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}.
    \end{equation*}
    Similar to \cite[Lemma 6.3]{montanari2023generalizationerrormaxmarginlinear}, we can show that $G_{\kappa}$ is strictly convex. Hence,
    \begin{align*}
        G_{\kappa} \left( \frac{\rho_1 + \rho_2}{2}, \frac{\beta_{0, 1} + \beta_{0, 2}}{2} \right) 
        & <  \frac{1}{2} \bigl( G_{\kappa} (\rho_1, \beta_{0, 1}) + G_{\kappa} (\rho_2, \beta_{0, 2}) \bigr) \\
        & =  \frac{1}{\sqrt{H_{\max}}} \frac{1}{2} \Bigl( 
        \sqrt{1 - \smash[b]{\rho_1^2}} + \sqrt{1 - \smash[b]{\rho_2^2}} \Bigr) \\
        & \le \frac{1}{\sqrt{H_{\max}}} \sqrt{1 - \left( \frac{\rho_1 + \rho_2}{2} \right)^2},
    \end{align*}
    where in the last line we use the concavity of the mapping $x \mapsto \sqrt{1 - x^2}$. It finally follows that
    \begin{equation*}
        H_{\kappa} \left( \frac{\rho_1 + \rho_2}{2}, \frac{\beta_{0, 1} + \beta_{0, 2}}{2} \right) > H_{\max},
    \end{equation*}
    a contradiction. This concludes the proof.
\end{proof}
%%%%% Old proof %%%%%
% \begin{proof}
%     For simplicity, we fix $\kappa \in \R$ and write $\rho = \rho^\star(\kappa)$, $\beta_0 = \beta_0^\star(\kappa)$.
%     \begin{itemize}
%         \item (Uniqueness of $\rho^\star$) \ \  Notice that $H_\kappa(\rho, \beta_0)$ is strictly increasing in $[-1, 0]$ and $H_\kappa(1, \beta_0) = 0$, so we must have $\rho^\star(\kappa) \in [0, 1)$. In order to show $\rho^\star(\kappa)$ is unique, we assume that there exists $\rho_1, \rho_2 \in [0, 1)$ and $\beta_{0,1}, \beta_{0,2} \in \R$, such that $\rho_1 \not= \rho_2$ and
%         \begin{equation*}
%             H_\kappa(\rho_1, \beta_{0, 1}) = H_\kappa(\rho_2, \beta_{0, 2})
%             = \max_{\rho \in [0, 1], \beta_0 \in \R} H_\kappa(\rho, \beta_0).
%         \end{equation*}
%         Denote this maximum by $H_{\max}$. Let us define a new function $G_\kappa: \mathsf{B}_2(1)_+ \times \R \to \R$ by
%         \ljy{\begin{equation*}
%             G_\kappa(\rho, r, \beta_0) := \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + r G_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2}
%             % G_\kappa(\rho, r, \beta_0) := \frac{r^2}{\E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right]},
%             \quad
%             \mathsf{B}_2(1)_+ := \mathsf{B}_2(1) \cap [0, 1]^2, 
%         \end{equation*}
%         where $G_1, G_2 \iidsim \normal(0, 1)$ are independent of $Y$.} We claim $G_\kappa$ is convex, by using the following facts:
%         \begin{itemize}
%             \item The mapping $(\rho, r, \beta_0) \mapsto s(Y(\omega)) \kappa - \rho \norm{\bmu}_2 + \rho G_1(\omega) + r G_2(\omega) - \beta_0 Y(\omega)$ is affine, $\forall\, \omega \in \Omega$.
%             \item The mapping $x \mapsto (x)_+^2$ is convex.
%             \item Expectation (integral) preserves convexity. %So the denominator of $G_\kappa$ is convex in $\rho, \beta_0$.
%             % \item The mapping $(x, y) \mapsto f(x)/g(y)$ is convex if both $f, g$ are convex and $f(x) \ge 0$, $g(y) > 0$.
%         \end{itemize}
%         \ljy{$H_\kappa$ and $G_\kappa$ are related by
%         \[  G_\kappa(\rho, r, \beta_0) = \frac{r}{ \sqrt{ H_{\kappa}(\rho, \beta_0) } },
%         \qquad
%         \forall\, \rho \in [0, 1),  \ \  r = \sqrt{1 - \rho^2},  \ \  \beta_0 \in \R. \]
%         }
%         Now, we denote
%         \begin{equation*}
%             r_1 := \sqrt{1 - \rho_1^2},
%             \qquad 
%             r_2 := \sqrt{1 - \rho_2^2},            
%         \end{equation*}
%         and
%         \begin{equation*}
%             \bar\rho :=
%             \frac{\rho_1 + \rho_2}{2}, 
%             \qquad
%             \bar r := \frac{r_1 + r_2}{2},
%             \qquad
%             \bar\beta_0  :=  \frac{\beta_{0, 1} + \beta_{0, 2}}{2} .
%         \end{equation*}
%         Then by convexity of $G_\kappa$, we have
%         \ljy{\begin{equation}
%             \label{eq:H_max1}
%             \begin{aligned}
%             %     H_{\max} = \frac12 H_\kappa(\rho_1, \beta_{0, 1}) + \frac12 H_\kappa(\rho_2, \beta_{0, 2})
%             % & = \frac12 G_\kappa(\rho_1, r_1, \beta_{0, 1}) + \frac12 G_\kappa(\rho_2, r_2,\beta_{0, 2}) \\
%             % & \le G_\kappa(\bar\rho, \bar r, \bar\beta_0).
%             G_\kappa(\bar\rho, \bar r, \bar\beta_0)
%             \le \frac12 G_\kappa(\rho_1, r_1, \beta_{0, 1}) + \frac12 G_\kappa(\rho_2, r_2,\beta_{0, 2})
%             = \frac{r_1}{2\sqrt{H_{\max}}} + \frac{r_2}{2\sqrt{H_{\max}}}
%             = \frac{\bar r}{\sqrt{H_{\max}}}.
%             \end{aligned}
%         \end{equation}
%         }
%         Since $\rho_1 \not= \rho_2$ and $\mathsf{B}_2(1)$ is strictly convex, we know that $\sqrt{\bar\rho^2 + \bar r^2} < 1$. We further denote
%         \begin{equation*}
%             \rho' := \frac{\bar\rho}{\sqrt{\bar\rho^2 + \bar r^2}},
%             \qquad
%             r' := \frac{\bar r}{\sqrt{\bar\rho^2 + \bar r^2}},
%             \qquad
%             \beta_0' := \frac{\bar \beta_0}{\sqrt{\bar\rho^2 + \bar r^2}},
%             \qquad
%             \kappa' := \frac{\kappa}{\sqrt{\bar\rho^2 + \bar r^2}}.
%         \end{equation*}
%         Then $\rho'{}^2 + r'{}^2 = 1$. %Since $G_\kappa$ is strictly increasing in $\rho$ and $r$, we have
%         \ljy{According to \cref{eq:H_max1}, we have
%         \begin{equation*}
%             \frac{1}{\sqrt{H_{\max}}} \ge \frac{G_\kappa(\bar\rho, \bar r, \bar\beta_0)}{\bar r}
%             = \frac{G_{\kappa'}(\rho', r', \beta_0')}{r'}
%             \overset{\text{(i)}}{>} \frac{G_{\kappa}(\rho', r', \beta_0')}{r'}
%             = \frac{1}{\sqrt{ H_\kappa(\rho', \beta_0') }},
%         \end{equation*}
%         where (i) is due to the fact that $\kappa' > \kappa$ and $c \mapsto (\E[ ( c + G )_+^2 ])^{1/2}$ is strictly increasing.
%         % \begin{equation}
%         %     \label{eq:H_max2}
%         %         G_\kappa(\bar\rho, \bar r, \bar\beta_0) 
%         %         < G_{\kappa}(\rho', r', \bar\beta_0)
%         %         = H_{\kappa}(\rho', \bar\beta_0).
%         % \end{equation}
%         %Combining \cref{eq:H_max1} and \eqref{eq:H_max2} 
%         This gives a contradiction $H_{\kappa}(\rho', \beta_0') > H_{\max}$. Therefore, the maximizer $\rho^\star(\kappa)$ must be unique.
%         }


%         \item (Uniqueness of $\beta_0^\star$) \ \ Similar to the proof at the beginning of step 4, $\beta_0^\star(\kappa)$ is the minimizer of
%         \begin{equation*}
%             h(\beta_0) :=   \E\left[ \bigl(  s(Y) \kappa - \rho^\star \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right].
%         \end{equation*}
%         We claim $h$ is strictly convex and continuously differentiable, since
% \begin{equation*}
%     h'(\beta_0)
%     = -2\pi \E\left[ \bigl( \tau \kappa - \rho^\star \norm{\bmu}_2 + G - \beta_0 \bigr)_+ \right]
%     + 2(1 - \pi) \E\left[ \bigl( \kappa - \rho^\star \norm{\bmu}_2 + G + \beta_0 \bigr)_+ \right] 
%     ,
% \end{equation*}
% which is strictly increasing and $h'(+\infty) = +\infty$, $h'(-\infty) = -\infty$. Hence, $\beta_0^\star(\kappa) \in \R$ is unique.
%     \end{itemize}
% \end{proof}


\subsubsection{Completing the proof of \cref{thm:SVM_main}}
\begin{proof}[\textbf{Proof of \cref{thm:SVM_main}}]
\noindent
\textbf{\ref{thm:SVM_main_trans}} is established by \cref{lem:over_phase_trans}.

\begin{proof}[\textbf{\emph{\ref{thm:SVM_main_var}:}}]
Notice the definition of $(\rho^*, \beta_0^*, \kappa^*)$ we used in our proof (\cref{subsubsec:over_phase}, \ref{subsubsec:over_logit_conv}) is based on \cref{eq:SVM_asymp_simple}. It suffices to show the equivalence of two optimization problems \cref{eq:SVM_variation} and \eqref{eq:SVM_asymp_simple}. Now we fix $\rho$, $\beta_0$ in \cref{eq:SVM_variation} and $X := \rho \| \bmu\|_2 + G + Y \beta_0$. Then \cref{eq:SVM_variation} can be written as
\begin{equation}\label{eq:SVM_var2}
    \begin{aligned}
        \begin{array}{cl}
            \underset{ \kappa > 0 , \, \xi \in \cL^2  }{ \mathrm{maximize} } & \kappa, \\
            \underset{ \phantom{\smash{\bm\beta \in \R^d, \beta_0 \in \R, \kappa \in \R} } }{\text{subject to}} &  
            X + \sqrt{1 - \rho^2} \xi \ge s(Y) \kappa,  
            \qquad \E[\xi^2]  \le  1/\delta .
        \end{array}
    \end{aligned}
\end{equation}
Note that it can be written as a convex optimization problem, and it is infeasible if $\rho = \pm 1$ (since $X$ has support $\R$). Take $\rho \in (-1, 1)$. According to the Karush--Kuhn--Tucker (KKT) and Slater's conditions for variational problems \cite[Theorem 2.9.2]{zalinescu2002convex}, $(\kappa, \xi)$ is the solution to \cref{eq:SVM_var2} if and only if it satisfies the following for some $\Lambda \in \cL^1, \Lambda \ge 0$ (a.s.) and $\nu \ge 0$:
\begin{equation*}
    \begin{aligned}
        -1 + \E[s(Y)\Lambda] = 0, \qquad
        -\sqrt{1 - \rho^2} \Lambda + 2 \nu \xi = 0 \ \  \text{(a.s.)},  \\
        \nu\left( \E[\xi^2] - \delta^{-1} \right) = 0,
        \qquad
        \Lambda \bigl( s(Y)\kappa - X - \sqrt{1 - \rho^2} \xi \bigr) = 0 \ \  \text{(a.s.)}.
    \end{aligned}
\end{equation*}
Clearly $\nu > 0$ (otherwise, $\Lambda = 0$ a.s., a contradiction). Consider the following two cases:
\begin{itemize}
    \item On the event $\{ s(Y(\omega))\kappa - X(\omega) < 0\}$, we obtain $s(Y(\omega))\kappa - X(\omega) - \sqrt{1 - \rho^2} \xi(\omega) < 0$, which implies $\Lambda(\omega) = 0$. Therefore, $\xi(\omega) = 0$.
    \item On the event $\{ s(Y(\omega))\kappa - X(\omega) > 0\}$, we obtain $\sqrt{1 - \rho^2} \xi(\omega) \ge s(Y(\omega))\kappa - X(\omega) > 0$, which implies $\xi(\omega) > 0$. Therefore, $\Lambda(\omega) > 0$, and thus $s(Y(\omega))\kappa - X(\omega) - \sqrt{1 - \rho^2} \xi(\omega) = 0$.
\end{itemize}
(Note $\P(s(Y)\kappa - X = 0) = 0$.) By combining these, we get $\sqrt{1 - \rho^2}\xi = (s(Y)\kappa - X)_+$. This proves \cref{eq:SVM_main_xi_star}. Plug in it into \cref{eq:SVM_variation} gives \cref{eq:SVM_asymp_simple}. The proof of $\rho^* \in (0, 1)$ and its independence of $\tau$ is given by \cref{lem:gordon_eq} in \cref{subsec:over_asymp}. 

This concludes the proof of part \ref{thm:SVM_main_var}.
\end{proof}





\begin{proof}[\textbf{\emph{\ref{thm:SVM_main_mar}, $\delta < \delta^*(0)$:}}]
We show that $\hat\kappa_n \conp \kappa^*$ in \cref{lem:over_mar_conp} can be strengthened to $\hat\kappa_n \conL{2} \kappa^*$. To this end, we show that $\hat\kappa_n^2$ is uniformly integrable (u.i.). Recall that $\kappa(\hat\vbeta_n, \hat\beta_{0,n}) \ge 0$ and
\begin{align*}
        \kappa(\hat\vbeta_n, \hat\beta_{0,n}) 
        & = \min_{i \in [n]} \wt y_i \bigl( \< \xx_i, \hat\vbeta_n \> + \hat\beta_{0,n} \bigr)
        = \min_{i \in [n]} \wt y_i \bigl(  y_i \< \vmu, \hat\vbeta_n \> + \< \zz_i, \hat\vbeta_n \> + \hat\beta_{0,n} \bigr) \\
        & = \min \left\{
            \min_{i: y_i = +1}  \tau^{-1}\bigl( \< \vmu, \hat\vbeta_n \> + \< \zz_i, \hat\vbeta_n \> + \hat\beta_{0,n} \bigr) ,
            \min_{i: y_i = -1}  \bigl( \< \vmu, \hat\vbeta_n \> - \< \zz_i, \hat\vbeta_n \> - \hat\beta_{0,n}\bigr)
        \right\}.
\end{align*}
Hence, on the event $\mathcal{D}_n^c$ (non-degenerate case), we have $\hat\kappa_n = \kappa(\hat\vbeta_n, \hat\beta_{0,n})$ and it can be bounded by the average from each class:
\begin{equation*}
    \begin{aligned}
        \kappa(\hat\vbeta_n, \hat\beta_{0,n}) & \le \tau^{-1}\bigl( \< \vmu, \hat\vbeta_n \> + \< \bar\zz^+_n, \hat\vbeta_n \> + \hat\beta_{0,n} \bigr) := \bar \kappa_n^+ , \\
        \kappa(\hat\vbeta_n, \hat\beta_{0,n}) & \le \phantom{\tau^{-1}\bigl(} \< \vmu, \hat\vbeta_n \> - \< \bar\zz^-_n, \hat\vbeta_n \> - \hat\beta_{0,n} \phantom{\bigr)} := \bar \kappa_n^-  ,
    \end{aligned}
\end{equation*}
where
\begin{equation*}
    \bar\zz^+_n := \frac{1}{n_+} \sum_{i: y_i = +1} \zz_i,
    \qquad
    \bar\zz^-_n := \frac{1}{n_-} \sum_{i: y_i = -1} \zz_i.
\end{equation*}
Combine these two bounds and apply Cauchy--Schwarz inequality, we obtain
\begin{equation*}
        \kappa(\hat\vbeta_n, \hat\beta_{0,n})
        \le \frac{\tau \bar \kappa_n^+  +  \bar \kappa_n^-}{\tau + 1}
        = \frac{2}{\tau + 1} 
        \!
        \left(
        \< \vmu, \hat\vbeta_n \> 
        + \left\< \frac{\bar\zz^+_n - \bar\zz^-_n}{2}, \hat\vbeta_n \right\>
        \right)
        \le 
        \frac{2}{\tau + 1} \left( \norm{\vmu}_2 + \norm{ \wt \zz_n }_2 \right),
\end{equation*}
where
\begin{equation*}
    \wt \zz_n := \frac{\bar\zz^+_n - \bar\zz^-_n}{2},
    \quad
    \wt \zz_n \,|\, \yy \sim \normal\left( \bzero, \frac14\Bigl( \frac{1}{n_+} + \frac{1}{n_-} \Bigr) \bI_d \right).
\end{equation*}
Therefore,
\begin{equation*}
    0   \le   \hat\kappa_n  =  \kappa(\hat\vbeta_n, \hat\beta_{0,n}) \ind_{1 \le n_+ \le n-1} 
    \le \frac{2}{\tau + 1} \left( 
        \norm{\vmu}_2 + \norm{ \wt \zz_n }_2 \ind_{1 \le n_+ \le n-1}
     \right).
\end{equation*}
In order to prove $\hat\kappa_n^2$ is u.i., it suffices to show that $\norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1}$ is u.i.. Next, we prove this by establishing that $\norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1}$ converges in $\cL^1$. It requires two steps (by Scheffé's Lemma):
\begin{subequations}
\begin{align}
    \E \left[ \norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1} \right] \to \frac{1}{4\delta} \left( \frac{1}{\pi} + \frac{1}{1-\pi} \right),
    \label{eq:svm_ui_a}
    \\
    \norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1} \conp \frac{1}{4\delta} \left( \frac{1}{\pi} + \frac{1}{1-\pi} \right).
    \label{eq:svm_ui_b}
\end{align}
\end{subequations}
For \cref{eq:svm_ui_a}, Observe
\begin{equation*}
    \begin{aligned}
      & \E \left[ \norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1} \right]
    = \E \left[ \E\bigl[ \norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1} \,|\, \yy \bigr] \right] \\
    = {} & \E \left[ \frac{d}{4} \Bigl( \frac{1}{n_+} + \frac{1}{n_-} \Bigr) \ind_{1 \le n_+ \le n-1} \right]
    = \frac{d}{4n} \E \left[ \Bigl( \frac{n}{n_+} + \frac{n}{n_-} \Bigr) \ind_{1 \le n_+ \le n-1} \right].
    \end{aligned}
\end{equation*}
To evaluate the expected value, note that (by law of large numbers)
\begin{equation*}
    \frac{n}{n_+} \ind_{1 \le n_+ \le n-1} \le \frac{2n}{n_+ + 1},
    \qquad
    \frac{n}{n_+}\ind_{1 \le n_+ \le n-1} \conp \frac{1}{\pi},
    \qquad
    \frac{2n}{n_+ + 1} \conp \frac{2}{\pi}.
\end{equation*}
A classical result \cite{chao1972negative} gives
\begin{equation*}
    \lim_{n \to \infty} \E\left[ \frac{2n}{n_+ + 1} \right]
    = \lim_{n \to \infty} \frac{2n\left( 1 - (1-\pi)^{n+1} \right)}{(n+1)\pi} = \frac{2}{\pi}.
\end{equation*}
So $\frac{2n}{n_+ + 1} \conL{1} \frac{2}{\pi}$, which implies $\frac{2n}{n_+ + 1}$ is u.i., and so is $\frac{n}{n_+} \ind_{1 \le n_+ \le n-1}$. Therefore, by Vitali convergence theorem, we have $\frac{n}{n_+} \ind_{1 \le n_+ \le n-1} \conL{1} \frac{1}{\pi}$. Similar arguments give $\frac{n}{n_-} \ind_{1 \le n_+ \le n-1} \conL{1} \frac{1}{1-\pi}$. Hence
\begin{equation*}
    \lim_{n \to \infty} \E \left[ \norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1} \right]
    = \lim_{n \to \infty} \frac{d}{4n} \cdot \lim_{n \to \infty} \E \left[ \Bigl( \frac{n}{n_+} + \frac{n}{n_-} \Bigr) \ind_{1 \le n_+ \le n-1} \right]
    = \frac{1}{4\delta} \left( \frac{1}{\pi} + \frac{1}{1-\pi} \right).
\end{equation*}
For \cref{eq:svm_ui_b}, notice that $\norm{ \wt \zz_n }_2^2 \,|\, \yy \sim a_n \chi_d^2$, where $a_n = \frac{1}{4} ( \frac{1}{n_+} + \frac{1}{n_-})$. By concentration inequality (e.g., \cref{lem:subG_concentrate}\ref{lem:subG-Hanson-Wright-I}), we have
\begin{equation*}
    \P\left( \abs{ \norm{ \wt \zz_n }_2^2 - d a_n } \ge \varepsilon \,\big|\, \yy \right) 
    \le 2\exp\left( -c \min\left\{ \frac{\varepsilon^2}{d a_n^2}, \frac{\varepsilon}{a_n} \right\} \right)
    = o_\P(1),
\end{equation*}
where $c > 0$ is a constant, $a_n = o_\P(1)$, $d a_n \conp \frac{1}{4\delta} ( \frac{1}{\pi} + \frac{1}{1-\pi} )$. By taking expectation on both sides and using bounded convergence theorem, we have $\norm{ \wt \zz_n }_2^2 - d a_n = o_\P(1)$. Then we get \cref{eq:svm_ui_b}.
% {Kangjie's Proof of convergence in probability:} We already know that $n_+ / n \to \pi$ in probability, namely $\P (\vert n_+/n - \pi \vert \ge \veps) \to 0$ for all $\veps > 0$. Now fix any such $n_+$, the conditional distribution of $\norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1}$ is just
% \begin{equation}
%     \frac{1}{4} \Bigl( \frac{1}{n_+} + \frac{1}{n_-} \Bigr) \chi^2 (d),
% \end{equation}
% for which we can use concentration inequality for chi-squared random variables. This leads to
% \begin{equation}
%     \P \left( \left\vert \norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1} - \frac{1}{4 \delta} \Bigl( \frac{n}{n_+} + \frac{n}{n_-} \Bigr) \right\vert \ge \veps \Big\vert n_+ \right) \to 0
% \end{equation}
% uniformly for $n_+$ satisfying $\vert n_+/n - \pi \vert \le \veps$, or use bounded convergence to conclude that the unconditional probability goes to $0$:
% \begin{equation}
%     \P \left( \left\vert \norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1} - \frac{1}{4 \delta} \Bigl( \frac{1}{\pi} + \frac{1}{1 - \pi} \Bigr) \right\vert \ge 2 \veps \right) \to 0.
% \end{equation}

Finally, \cref{eq:svm_ui_a} and \eqref{eq:svm_ui_b} imply that $\norm{ \wt \zz_n }_2^2 \ind_{1 \le n_+ \le n-1}$ converges in $\cL^1$, and thus is u.i.. So $\hat\kappa_n^2$ is also u.i.. By Vitali convergence theorem, convergence in probability of $\hat\kappa_n$ can be strengthen to $\cL^2$ convergence. 

This concludes the proof of part \ref{thm:SVM_main_mar} for $\delta < \delta^*(0)$.
\end{proof}






\begin{proof}[\textbf{\emph{\ref{thm:SVM_main_mar}, $\delta > \delta^*(0)$:}}]
For non-separable regime, we cannot work with $\xi_{n, \kappa}$ in \cref{eq:xi_n_kappa} to show a negative margin, since $\hat\kappa_n \ge 0$ always holds (by taking $\vbeta = 0$, $\beta_0 = 0$). To this end, we define
\begin{equation*}
    \Xi_{n, \kappa} := \min_{ \substack{ \norm{\vbeta}_2 = 1 \\ \beta_0 \in \R} } \frac{1}{\sqrt{d}} \norm{ \left( \kappa \bs_\yy - \yy \odot \XX \vbeta  - \beta_0 \yy \right)_+ }_2,
\end{equation*}
which replace the constraint $\norm{\vbeta}_2 \le 1$ in $\xi_{n, \kappa}$ by $\norm{\vbeta}_2 = 1$. Here we define the margin as
\begin{equation}\label{eq:wt_kappa_def}
    \wt\kappa_n := \sup\{ \kappa \in \R:  \Xi_{n,\kappa} = 0 \}.
\end{equation}
Note that $\wt\kappa_n = \hat\kappa_n$ on separable data, but $\wt\kappa_n$ is allowed to be negative. Then our goal is to show
\begin{equation}\label{eq:neg_kappa}
    \wt\kappa_n \le -\overline{\kappa}
\end{equation}
holds for some $\overline{\kappa} > 0$ with high probability. Then followed by the proof outline at the beginning of \cref{append_subsec:sep}, we can also define a series of random variables in a similar way:
\begin{align*}
    \Xi'_{n, \kappa, B} & := \min_{ \substack{ \norm{\vbeta}_2 = 1 \\ \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge 0} } \frac{1}{\sqrt{d}} \blambda^\top \left( \kappa \bs_\yy \odot \yy - \XX \vbeta  - \beta_0 \bone \right),
    \\
    \Xi_{n, \kappa, B}'^{(1)} & := \min_{ \substack{ \rho^2 + \norm{\vtheta}_2^2 = 1 \\ \abs{\beta_0} \le B } } \max_{ \substack{ \norm{\blambda}_2 \le 1 \\ \blambda \odot \yy \ge 0} } \frac{1}{\sqrt{d}}  \left(
    \norm{\blambda}_2 \vg^\top \btheta + \norm{\btheta}_2 \vh^\top \blambda + \blambda^\top \bigl( 
        \kappa \bs_\yy \odot \yy - \rho\norm{\bmu}_2 \yy + \rho \vu - \beta_0 \bone
     \bigr)
     \right),
    \\
    \bar\Xi'^{(2)}_{\kappa, B} & :=  \min_{ \substack{ \rho^2 + r^2 = 1, r \ge 0 \\  \abs{\beta_0} \le B } }
    -r + \sqrt{\delta} \left( \E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + \rho G_1 + rG_2 - \beta_0 Y \bigr)_+^2 \right] \right)^{1/2},
\end{align*}
where the constraints $\norm{\vbeta}_2 \le 1$, $\rho^2 + \norm{\vtheta}_2^2 \le 1$, and $\rho^2 + r^2 \le 1$ in $\xi'_{n, \kappa, B}$, $\xi_{n, \kappa, B}'^{(1)}$, and $\bar\xi'^{(2)}_{\kappa, B}$ all become equality constraints. Then we follow the same arguments in Step 1---5 (\cref{lem:over_beta0}---\ref{lem:over_mar_conp}).
\begin{itemize}
    \item Analogous to the proof of \cref{lem:over_beta0}, we have $| \P\bigl( \Xi_{n, \kappa} = 0 \bigr) - \P\bigl(\Xi'_{n, \kappa, B} = 0 \bigr) | \to 0$.
    \item Analogous to the proof of \cref{lem:over_CGMT}, we can apply CGMT \cref{lem:CGMT} to connect $\Xi'_{n, \kappa, B}$ with $\Xi_{n, \kappa, B}'^{(1)}$. 
\begin{equation*}
    \P\left( \Xi'_{n, \kappa, B} \le t \vphantom{\Xi'^{(1)}_{n, \kappa, B}} \right) 
    \le 2\, \P\left( \Xi'^{(1)}_{n, \kappa, B} \le t \right).
\end{equation*}
    Here we only get a one-sided inequality since $\{(\rho, \vtheta): \rho^2 + \norm{\vtheta}_2^2 = 1 \}$ is non-convex.
    \item Analogous to the proof of \cref{lem:over_ULLN}, we have $\Xi'^{(1)}_{n, \kappa, B}  \conp ( \bar\Xi'^{(2)}_{\kappa, B} )_+$.
    \item Notice that the optimal $r$ in $\bar\Xi'^{(2)}_{\kappa, B}$ must be nonnegative. Hence, by substituting $r = \sqrt{1 - \rho^2}$, we have $\bar\Xi'^{(2)}_{\kappa, B} = \bar\xi_{\kappa}^{(3)}$ (\cref{eq:xi3_kappa}) for some $B > 0$ large enough.
\end{itemize}
Recall that in the proof of \cref{lem:over_phase_trans} and \ref{lem:over_mar_conp}, if $\delta > \delta^*(0)$, then there exists a $\kappa_0 < 0$, such that $\bar\xi_{\kappa_0}^{(3)} = 0$. According to \cref{eq:phase_equiv}, for any $\varepsilon > 0$ small enough, by using above relations, we have 
\begin{align*}
\bar\xi_{\kappa_0 + \varepsilon}^{(3)} = \bar\Xi'^{(2)}_{\kappa_0 + \varepsilon, B} > 0
\quad & \Longrightarrow \quad
\Xi'^{(1)}_{n, \kappa_0 + \varepsilon, B}  \conp \bigl( \bar\xi^{(3)}_{\kappa_0 + \varepsilon} \bigr)_+ > 0
\\
\quad \Longrightarrow \quad
\Xi'_{n, \kappa_0 + \varepsilon, B} > 0 ~ \text{w.h.p.}
\quad & \Longrightarrow \quad
\Xi_{n, \kappa_0 + \varepsilon} > 0 ~ \text{w.h.p.}.
\end{align*}
By \cref{eq:wt_kappa_def}, $\wt\kappa_n < \kappa_0 + \varepsilon < 0$ holds with high probability (by taking $\varepsilon$ to be sufficiently small), which proves \cref{eq:neg_kappa}.

This concludes the proof of part \ref{thm:SVM_main_mar} for $\delta > \delta^*(0)$.
\end{proof}








\begin{proof}[\textbf{\emph{\ref{thm:SVM_main_param}, \ref{thm:SVM_main_logit}:}}]
We have shown parameter and ELD convergence for the case $\tau = 1$ in \cref{lem:over_logit_conv}. Now for any $\tau \ge 1$, denote $\hat\vbeta_n(\tau), \hat\beta_{0, n}(\tau), \hat\kappa_n(\tau)$ as the max-margin solution to \cref{eq:over_max-margin}, and define
\begin{equation*}
     \hat\rho_n(\tau) :=  \left\< \hat\vbeta_n(\tau), \frac{\vmu}{\norm{\vmu}_2 } \right\>.
\end{equation*}
Similarly, denote $\rho^*(\tau), \beta_0^*(\tau), \kappa^*(\tau)$ as the optimal solution to \cref{eq:SVM_asymp_simple}. By \cref{prop:SVM_tau_relation},
\begin{itemize}
    \item We have
    \begin{equation}\label{eq:param_hat_tau}
        \hat\rho_n(\tau) = \hat\rho_n(1),
        \qquad
        \hat\beta_{0, n}(\tau) = \hat\beta_{0, n}(1) + \frac{\tau - 1}{\tau + 1} \hat\kappa_n(1). 
    \end{equation}

    \item We can write
    \begin{equation}\label{eq:Ln_hat_tau}
        \begin{aligned}
        %     \hat \cL_{n} & = \frac1n \sum_{i=1}^n \delta_{\wt y_i ( \< \xx_i, \hat\vbeta_n \> + \hat\beta_{0,n}(\tau) )  \ind_{\cD_n^c}  }
        % =: \Law\left( s(Y')^{-1} Y' \bigl( \< \xx', \hat\vbeta_n \> + \hat\beta_{0,n}(\tau) \bigr) \ind_{\cD_n^c} \right) \\
        % & = \Law \left(  s(Y')^{-1} \Bigl\{ Y' \bigl( \< \xx', \hat\vbeta_n \> + \hat\beta_{0,n}(1) \bigr) \ind_{\cD_n^c} \Bigr\}
        % + s(Y')^{-1} Y' \cdot \frac{\tau - 1}{\tau + 1} \hat\kappa_n(1) 
        % \right).
            \hat \nu_{n} & = \frac1n \sum_{i=1}^n \delta_{ \left(y_i, \< \xx_i, \hat\vbeta_n \> + \hat\beta_{0,n}(\tau) \right)  \ind\{\cD_n^c\}  }
        =: \Law\left( Y' \ind_{\cD_n^c}, \bigl( \< \xx', \hat\vbeta_n \> + \hat\beta_{0,n}(\tau) \bigr) \ind_{\cD_n^c} \right) \\
        & = \Law \left( Y' \ind_{\cD_n^c},  \bigl( \< \xx', \hat\vbeta_n \> + \hat\beta_{0,n}(1) \bigr) \ind_{\cD_n^c}
        + \frac{\tau - 1}{\tau + 1} \hat\kappa_n(1)
        \right).
        \end{aligned}
    \end{equation}
\end{itemize}
Besides, according to \cref{cor:asymp_tau_relation},
\begin{itemize}
    \item We have
    \begin{equation}\label{eq:param_star_tau}
        \rho^*(\tau) = \rho^*(1),
        \qquad
        \beta_0^*(\tau) = \beta_0^*(1) + \frac{\tau - 1}{\tau + 1} \kappa^*(1).
    \end{equation}

    \item We can also write
    \begin{equation}\label{eq:Ln_star_tau}
    \begin{aligned}
        \nu_*
        & = \Law \, \Bigl( Y, Y \max \bigl\{ s(Y)\kappa^*(\tau) , G + \rho^*\norm{\vmu}_2 + \beta_0^*(\tau) Y \bigr\} \Bigr) \\
        & = \Law \left( Y, Y \max \bigl\{ \kappa^*(1) , G + \rho^*\norm{\vmu}_2 + \beta_0^*(1) Y \bigr\} 
        + \frac{\tau - 1}{\tau + 1} \kappa^*(1)
        \right).
    \end{aligned}
    \end{equation}
\end{itemize}
We have shown $\hat\kappa_n(1) \conL{2} \kappa^*(1)$ and $\hat\beta_{0,n}(1) \conp \beta_0^*(1)$ in \cref{lem:over_logit_conv}. Then by continuous mapping theorem, comparing \cref{eq:param_hat_tau} and \eqref{eq:param_star_tau}, it follows that $\hat\beta_{0, n}(\tau) \conp \beta_0^*(\tau)$ for any $\tau > 0$. 

In \cref{lem:over_logit_conv}, we have shown that $W_2\bigl( \hat \nu_{n}, \nu_* \bigr) = o_\varepsilon(1)$ for $\tau = 1$ with high probability, i.e.,
\begin{equation}
\label{eq:logit_conv_1}
    W_2 \Bigl(
        \Law \Bigl(
        Y'\ind_{\cD_n^c} ,
        \underbrace{    
        \bigl( \< \xx', \hat\vbeta_n \> + \hat\beta_{0,n}(1) \bigr) \ind_{\cD_n^c}
        }_{ =: U_n} \Bigr)
        ,
        \Law \Bigl( 
        Y,
            \underbrace{    
        Y\max \bigl\{ \kappa^*(1) , G + \rho^*\norm{\vmu}_2 + \beta_0^*(1) Y \bigr\} 
        }_{=: U^*} \Bigr)
        \Bigr)
    \! = \!
    o_\varepsilon(1).
\end{equation}
Then there exists a coupling $(Y', Y, U_n, U^*)$ such that, with high probability, 
\begin{align*}
    W_2( \hat \nu_{n},  \nu_* )
    & \le   
        \left( \E_{\cdot | n} \bigl[ (
           Y'\ind_{\cD_n^c} - Y 
        )^2 \bigr] \right)^{\frac12} 
        + \left( \E_{\cdot | n} \biggl[ \Bigl(
           U_n - U^*  + \frac{\tau - 1}{\tau + 1} \hat\kappa_n(1) - \frac{\tau - 1}{\tau + 1} \kappa^*(1)
        \Bigr)^2 \biggr] \right)^{\frac12} 
        \\
    & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\le}
        \left( \E_{\cdot | n} \bigl[ (
           Y'\ind_{\cD_n^c} - Y 
        )^2 \bigr] \right)^{\frac12} 
        +
        \left( \E_{\cdot | n} \bigl[ (
            U_n - U^* 
        )^2 \bigr] \right)^{\frac12} 
        +
        \frac{\tau - 1}{\tau + 1}
         \left( \E_{\cdot | n} \bigl[ (
            \hat\kappa_n(1) - \kappa^*(1) 
        )^2 \bigr] \right)^{\frac12}
        \\
    & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{=}  o_\varepsilon(1),
\end{align*}
where in (i) we use Minkowski inequality, while in (ii) we use $\hat\kappa_n(1) \conL{2} \kappa^*(1)$ in \ref{thm:SVM_main_mar} and \cref{eq:logit_conv_1}. By taking $\varepsilon \to 0$, we can show that $W_2\bigl( \hat \nu_{n}, \nu_* \bigr) \conp 0$ holds for any $\tau > 0$.

% Then, by comparing \cref{eq:Ln_hat_tau} and \eqref{eq:Ln_star_tau}, with high probability, we have
% \begin{equation*}
%     \begin{aligned}
%            & W_2( \hat \cL_{n},  \cL_* ) \\
%     \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\le} {} &  
%         \left( \E_{\cdot | n} \left[ \left(
%             s(Y')^{-1} U_n - s(Y)^{-1} U^*
%         \right)^2\right] \right)^{\frac12} 
%         + 
%         \frac{\tau - 1}{\tau + 1} \left( \E_{\cdot | n} \left[ \left(
%             s(Y')^{-1} Y' \hat\kappa_n(1) - s(Y)^{-1} Y \kappa^*(1)
%         \right)^2\right] \right)^{\frac12} \\
%     \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le} {} & \left( \E_{\cdot | n} \left[ s(Y')^{-2} \left(
%          U_n -U^*
%     \right)^2\right] \right)^{\frac12}
%     + \left( \E_{\cdot | n} \left[ \left(
%         s(Y')^{-1} - s(Y)^{-1}
%     \right)^2  ( U^* )^2  \right] \right)^{\frac12}  
%     \\
%     & \phantom{\le}     
%     + \frac{\tau - 1}{\tau + 1}  \left\{ 
%     \left( \E_{\cdot | n} \left[ s(Y')^{-2} \bigl(
%         \hat\kappa_n(1) - \kappa^*(1)
%     \bigr)^2 \right] \right)^{\frac12}
%     +  \kappa^*(1) \left( \E_{\cdot | n} \left[ \left(
%         s(Y')^{-1}Y' - s(Y)^{-1}Y
%     \right)^2  \right] \right)^{\frac12}
%     \right\}
%     \\
%     \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{\le} {} & 
%     (1 \vee \tau^{-1}) \left( \E_{\cdot | n} \left[ \left(
%         U_n -U^*
%    \right)^2\right] \right)^{\frac12}
%    + \left( \E_{\cdot | n} \left[ \left(
%        s(Y')^{-1} - s(Y)^{-1}
%    \right)^4  \right] \right)^{\frac14}
%    \left( \E \left[ ( U^* )^4  \right] \right)^{\frac14} \\
%    & \phantom{\le}     
%    + \frac{\tau - 1}{\tau + 1}  \left\{ 
%    (1 \vee \tau^{-1}) \left( \E_{\cdot | n} \left[ \bigl(
%        \hat\kappa_n(1) - \kappa^*(1)
%    \bigr)^2 \right] \right)^{\frac12}
%    +  \kappa^*(1) \left( \E_{\cdot | n} \left[ \left(
%     s(Y')^{-1}Y' - s(Y)^{-1}Y
% \right)^2  \right] \right)^{\frac12}
%    \right\} 
%    \\
%    \overset{\mathmakebox[0pt][c]{\text{(iv)}}}{\le} {} &  o_{\varepsilon}(1)  +  C_1 \left( \E_{\cdot | n} \left[ 
%     (Y' - Y)^2  \right] \right)^{\frac14}
%     + \frac{\tau - 1}{\tau + 1} \left\{
%         o_{\varepsilon}(1) + \kappa^*(1) \cdot C_2 \left( \E_{\cdot | n} \left[ 
%             (Y' - Y)^2  \right] \right)^{\frac12}
%      \right\}  
%      \\
%     \overset{\text{(v)}}{=} {} &   o_{\varepsilon}(1),
%     \end{aligned}
% \end{equation*}
% where in (i) and (ii) we use Minkowski inequality, in (iii) we use Cauchy--Schwarz inequality, in (iv) we use \cref{eq:logit_conv_1}, $\hat\kappa_n(1) \conL{1} \kappa^*(1)$ in \ref{thm:SVM_main_mar}, the fact that $y \mapsto s(y)^{-1}$ and $y \mapsto s(y)^{-1} y$ are Lipschitz, and $C_1, C_2 > 0$ are constants, while in (v) we use \cref{eq:pursuit_pre} can recall that $\eta < \min\{ \varepsilon^2/4, \varepsilon/B \} = o_{\varepsilon}(1)$. By taking $\varepsilon \to 0$, we can show that $W_2\bigl( \hat \cL_{n}, \cL_* \bigr) \conp 0$ holds for any $\tau > 0$.

For TLD convergence, we give a proof of $\hat\nu_{n}^\mathrm{test} \conw \nu_*^\mathrm{test}$. Write $\xx_\mathrm{new} = y_\mathrm{new} \bmu + \zz_\mathrm{new}$, $\zz_\mathrm{new} \sim \normal(\bzero, \bI_d)$, and recall $\hat\nu_{n}^\mathrm{test} = \Law \bigl(y^\mathrm{new}, \< \xx^\mathrm{new}, \hat\vbeta_n \> + \hat\beta_{0,n} \bigr)$. Let $G \sim \normal(0, 1)$ and $G \indep y^\mathrm{new}$, then
\begin{align*}
   \< \xx^\mathrm{new}, \hat\vbeta_n \> + \hat\beta_{0,n}
    & = \< y^\mathrm{new}\bmu + \zz^\mathrm{new}, \hat\vbeta_n \> + \hat\beta_{0,n} \\
    & = y^\mathrm{new} \hat\rho_n \norm{\bmu}_2 + \< \zz^\mathrm{new}, \hat\vbeta_n \>
    + \hat\beta_{0,n} \\
    & \cond y^\mathrm{new} ( \rho^* \norm{\bmu}_2 + G + y^\mathrm{new} \beta_{0}^*),
\end{align*}
where in the last line we use Slutsky's theorem and $y^\mathrm{new} \indep (y^\mathrm{new}\zz^\mathrm{new}, \hat\vbeta_n, \hat\beta_{0,n})$.

This concludes the proof of part \ref{thm:SVM_main_param} and \ref{thm:SVM_main_logit}.
\end{proof}

\begin{proof}[\textbf{\emph{\ref{thm:SVM_main_err}:}}]
In \ref{thm:SVM_main_logit} above, we showed that
\begin{equation*}
        \hat f(\xx_\mathrm{new}) = \< \xx_\mathrm{new}, \hat\vbeta_n \> + \hat\beta_{0,n}
        \cond y_\mathrm{new} \rho^* \norm{\bmu}_2 + G + \beta_{0}^*.
\end{equation*}
Therefore, by bounded convergence theorem, the errors have their limits
\begin{align*}
        \lim_{n \to \infty} \Err_{+,n} & = \P\left( + \rho^* \norm{\bmu}_2 + G + \beta_{0}^* \le 0 \right)
        = \Phi \left(- \rho^* \norm{\bmu}_2  - \beta_0^* \right), \\
        \lim_{n \to \infty} \Err_{-,n} & = \P\left( - \rho^* \norm{\bmu}_2 + G + \beta_{0}^* >  0 \right)
        = \Phi \left(- \rho^* \norm{\bmu}_2  + \beta_0^* \right).
\end{align*}
This concludes the proof of part \ref{thm:SVM_main_err}.
\end{proof}
Finally, we complete the proof of \cref{thm:SVM_main}.
\end{proof}
% \begin{rem}
% Some previous works \cite{kini2021label, deng2022model} also derived similar results on linear separability and parameter convergence, but logit distribution was not well studied. Our variational characterization of the optimization problem \cite{montanari2023generalizationerrormaxmarginlinear, montanari2022overparametrizedlineardimensionalityreductions} enables us to derive the asymptotics of logit distribution, which technically also simplifies the proof of parameter convergence.
% \end{rem}






\subsection{Analysis of the asymptotic optimization problem: Proof of \cref{lem:gordon_eq}}
\label{subsec:over_asymp}

We provide an analysis of the low dimensional asymptotic optimization problem \cref{eq:SVM_asymp_simple} in this subsection. The conclusion below has been used in the proofs of \cref{thm:SVM_main}\ref{thm:SVM_main_var}, \ref{thm:SVM_main_param} and \ref{thm:SVM_main_logit}. It will be also used in \cref{append_sec:mar_reb} to obtain monotonicity results.

For $G \sim \normal(0, 1)$ and $t \in \R$, we define two auxiliary functions
\begin{equation}
\label{eq:fun_g1_g2}
	g_1 (t) := \E \left[ (G + t)_+ \right], \qquad g_2 (t) := \E \left[ (G + t)_+^2 \right].
\end{equation}
Clearly both $g_1$ and $g_2$ are strictly increasing mappings from $\R$ to $\R_{>0}$. Then $g := g_2 \circ g_1^{-1}$ is also strictly increasing. The following lemma shows that the limiting parameters $(\rho^*, \beta_0^*, \kappa^*)$ defined in \cref{thm:SVM_main} can be characterized by the following system of equations, involving $g$ and $g_1^{-1}$.



\begin{lem}[Analysis of the asymptotic problem]
    \label{lem:gordon_eq}
    In the separable regime $\delta < \delta^*(0)$, $(\rho^*, \beta_0^*, \kappa^*)$ is the unique solution to the system of equations
    \begin{subequations}
    \begin{align}
    \label{eq:SVM_sys_eq_rho}
        \pi \delta \cdot g \left( \frac{\rho}{2 \pi \norm{\bmu}_2 \delta} \right) & + (1 - \pi) \delta \cdot g \left( \frac{\rho}{2(1 - \pi) \norm{\bmu}_2 \delta} \right) = 1 - \rho^2, \\
    \label{eq:SVM_sys_eq_bk1}
    - \beta_0 + \kappa \tau & = \rho \norm{\bmu}_2 + g_1^{-1} \left( \frac{\rho}{2 \pi \norm{\bmu}_2 \delta} \right), \\
    \label{eq:SVM_sys_eq_bk2}
	\beta_0 + \kappa & = \rho \norm{\bmu}_2 + g_1^{-1} \left( \frac{\rho}{2 (1 - \pi) \norm{\bmu}_2 \delta} \right),
    \end{align}
    \end{subequations}
    where $\rho^* \in (0, 1)$ does not depend on $\tau$ and $\kappa^* > 0$.
\end{lem}
\begin{proof}
    Recall that in the proof of \cref{lem:over_phase_trans} and \ref{lem:over_logit_conv}, we established that $(\rho^*, \beta_0^*, \kappa^*)$ is the unqiue solution to
    \begin{equation*}
        \begin{array}{rl}
        \maximize\limits_{\rho \in [0, 1], \beta_0 \in \R, \kappa \in \R} & \kappa, \\
        \text{\emph{subject to}} & H_\kappa(\rho, \beta_0) \ge \delta.
        \end{array}
    \end{equation*}
    Let $F(\rho, \beta_0, \kappa) := F_\kappa(\rho, \beta_0)$, where $F_\kappa$ is defined in \cref{eq:T_F_}. Then the above optimization problem is equivalent to
    \begin{equation*}
        \begin{array}{rl}
        \maximize\limits_{\rho \in [0, 1], \beta_0 \in \R, \kappa \in \R} & \kappa, \\
        \text{\emph{subject to}} & F(\rho, \beta_0, \kappa) \le 0,
        \end{array}
    \end{equation*}
    Note $F$ is convex (since $x \mapsto (x)_+^2$ is a convex map, and expectation preserves convexity). Setting $\partial_{\rho} F = 0$ and $\partial_{\beta_0} F = 0$, we obtain the first-order conditions satisfied by $(\rho^*, \beta_0^*)$:
    \begin{equation}
        \label{eq:SVM_foc1}
    \begin{aligned}
    & \E \left[ \big( G - \rho \norm{\bmu}_2 - \beta_0 + \kappa \tau \big)_+ \right] = \frac{\rho}{2 \pi \norm{\bmu}_2 \delta}, \\
    & \E \left[ \big( G - \rho \norm{\bmu}_2 + \beta_0 + \kappa \big)_+ \right] = \frac{\rho}{2 (1 - \pi) \norm{\bmu}_2 \delta}.
    \end{aligned}
    \end{equation}
Moreover, we have $\delta^*(\kappa^*) = \delta$ and thus $F(\rho, \kappa, \beta_0) = 0$ at $(\rho^*, \kappa^*, \beta_0^*)$, which leads to
\begin{equation}\label{eq:SVM_foc2}
    \pi \delta \E \left[ \big( G - \rho \norm{\bmu}_2 - \beta_0 + \kappa \tau \big)_+^2 \right]  + (1-\pi) \delta \E \left[ \big( G - \rho \norm{\bmu}_2 + \beta_0 + \kappa \big)_+^2 \right] = 1 - \rho^2.
\end{equation}
Using $g_1$, $g_2$ defined in \cref{eq:fun_g1_g2}, the first-order conditions \cref{eq:SVM_foc1} can be rewritten as
\begin{equation}
\label{eq:SVM_foc1_ref}
\begin{aligned}
	& g_1 \left( - \rho \norm{\bmu}_2 - \beta_0 + \kappa \tau \right) = \frac{\rho}{2 \pi \norm{\bmu}_2 \delta}, \\
	& g_1 \left( - \rho \norm{\bmu}_2 + \beta_0 + \kappa \right) = \frac{\rho}{2 (1 - \pi) \norm{\bmu}_2 \delta}.
\end{aligned}
\end{equation}
Similarly, we recast \cref{eq:SVM_foc2} into
\begin{equation}
\label{eq:SVM_foc2_ref}
    \pi \delta g_2 \left( - \rho \norm{\bmu}_2 - \beta_0 + \kappa \tau \right) 
    + (1-\pi) \delta g_2 \left( - \rho \norm{\bmu}_2 + \beta_0 + \kappa \right) = 1 - \rho^2.
\end{equation}
By combining \cref{eq:SVM_foc1_ref} and \eqref{eq:SVM_foc2_ref}, we get \cref{eq:SVM_sys_eq_rho}. \cref{eq:SVM_sys_eq_bk1} and \eqref{eq:SVM_sys_eq_bk2} directly come from \cref{eq:SVM_foc1_ref}.

Note that function $g: \R_{> 0} \to \R_{ > 0}$ satisfies $g(0^+) = 0$. As $\rho$ varies from $0$ to $1$, the L.H.S. of \cref{eq:SVM_sys_eq_rho} increases from $0$ to a positive number while the R.H.S. decays to $0$, which guarantees the existence and uniqueness of $\rho^* > 0$. Since \cref{eq:SVM_sys_eq_rho} does not depend on $\tau$ and $\kappa^*$, we know that $\rho^*$ does not depend on $\tau$ and $\kappa^*$. This concludes the proof.
\end{proof}

In parallel to \cref{prop:SVM_tau_relation} for the original non-asymptotic problem, we provide the following similar result on the asymptotic problem \cref{eq:SVM_asymp_simple}.

\begin{cor}\label{cor:asymp_tau_relation}
        In the separable regime $\delta < \delta^*(0)$, let $(\rho^*(\tau), \beta_0^*(\tau), \kappa^*(\tau))$ be the optimal solution to \cref{eq:SVM_asymp_simple} under hyperparameter $\tau$. Then
        \begin{equation}\label{eq:asymp_tau_relation}
		    \rho^*(\tau) = \rho^*(1),
        \qquad
        \beta_0^*(\tau) = \beta_0^*(1) + \frac{\tau - 1}{\tau + 1} \kappa^*(1),
        \qquad
        \kappa^*(\tau) = \frac{2}{\tau + 1} \kappa^*(1).
	\end{equation}
\end{cor}
\begin{proof}
    Conclusion for $\rho^*$ is already shown in \cref{lem:gordon_eq}. For $\beta_0^*$ and $\kappa^*$, note that the R.H.S. of \cref{eq:SVM_sys_eq_bk1} and \eqref{eq:SVM_sys_eq_bk2} are constants under $\rho = \rho^*$ (depending on $\pi$, $\norm{\bmu}_2$ and $\delta$). Then we have
    \begin{align*}
        - \beta_0^*(\tau) + \kappa^*(\tau) \tau & = - \beta_0^*(1) + \kappa^*(1), \\
	\beta_0^*(\tau) + \kappa^*(\tau) & = \beta_0^*(1) + \kappa^*(1).
    \end{align*}
    Combining these two equations gives the expression of $\beta_0^*(\tau), \kappa^*(\tau)$ in terms of $\beta_0^*(1), \kappa^*(1)$ as in \cref{eq:asymp_tau_relation}, completing the proof.
\end{proof}








\subsection{Proof of \cref{prop:opt_transport}}

\begin{proof}[\textbf{Proof of \cref{prop:opt_transport}}]
We can prove a more general result by replacing $\cL_*^\mathrm{test}$ with $\mu$ and $\cL_*$ with $\nu := \Law(\max\{ \kappa^*, X \})$, where $X \sim \mu$ and $\mu$ is any probability measure with atomless (continuous) CDF $F_\mu$. As a special case, in \cref{prop:opt_transport} we consider $\mu$ as a mixture of two Gaussian distributions, and the cost function $c(x, y) = (x - y)^2$. 

We now prove the general statement. Note the CDF of $\nu$ has the form
\begin{equation*}
    F_\nu(t) :=  
    \begin{cases} 
    F_\mu(t) , & \ \text{if} \ t < \kappa^*, \\
    1, & \ \text{if} \ t \ge \kappa^*.
    \end{cases}
\end{equation*}
According to the optimal transport theory \cite[Theorem 2.5]{santambrogio2015optimal}, the unique (also monotone) optimal transport map from $\mu$ to $\nu$ is given by $\mathtt{T}^* := F_{\nu}^{-} \circ F_{\mu}$, where $F_{\nu}^{-}$ is the quantile function of $\nu$:
\begin{equation*}
    F_{\nu}^{-}(x) = \inf\left\{ t \in \R: F_{\nu}(t) \ge x \right\}
    = 
     \begin{cases} 
    F_\mu^{-1}(x) , & \ \text{if} \ x < F_\mu(\kappa^*), \\
    \kappa^*, & \ \text{if} \ x \ge F_\mu(\kappa^*).
    \end{cases}
\end{equation*}
Then we have $\mathtt{T}^*(x) := F_{\nu}^{-} \bigl( F_{\mu}(x) \bigr) = \max\{ \kappa^*, x \}$, which concludes the proof.
\end{proof}




