\section{Confidence estimation and calibration: Proofs for \cref{sec:calibration}}
\label{append_sec:calib}

\subsection{Proof of \cref{prop:conf}}

The following preliminary result summarizes the precise asymptotics of three quantities: $\hat p(\xx)$ (max-margin confidence), $p^*(\xx)$ (Bayes optimal probability), and $\hat p_0(\xx)$ (true posterior probability).

% When data is generated from 2-GMM with proportional asymptotics $n/d \to \delta \in (0, \infty)$, we have closed-forms for the quantities above.
\begin{lem}\label{lem:conf_limit}
    Consider 2-GMM and proportional settings in \cref{sec:logit_SVM} on separable dataset ($\delta < \delta^*(0)$). Let $(\rho^*, \beta_0^*)$ be defined as per \cref{thm:SVM_main}, and $(Y, G, H) \sim P_y \times \normal(0,1) \times \normal(0,1)$. Let $G' := \rho^* G + \sqrt{1 - \rho^{*2}} H$. Then for any test point $(\xx, y) \sim P_{\xx, y}$ independent of $\hat p$, as $n \to \infty$,
    \begin{equation}
    \label{eq:p(x)_asymp}
        \begin{pmatrix}
        y 
        \vphantom{\left( \log\frac{\pi}{1 - \pi} \right)} 
        \\
        \hat p(\xx)
        \vphantom{\left( \log\frac{\pi}{1 - \pi} \right)}
        \\
        p^*(\xx) 
        \vphantom{\left( \log\frac{\pi}{1 - \pi} \right)}
        \\
        \hat p_0(\xx)
        \vphantom{\left( \log\frac{\pi}{1 - \pi} \right)}
        \end{pmatrix}
        \cond
        \begin{pmatrix}
            Y 
            \vphantom{\left( \log\frac{\pi}{1 - \pi} \right)} 
            \\
            \sigma\left( \rho^*\|\vmu\|_2 Y + G + \beta_0^* \right) 
            \vphantom{\left( \log\frac{\pi}{1 - \pi} \right)} 
            \\
            \sigma \left( 2 \|\bmu\|_2 (\|\bmu\|_2 Y + G') + \log\frac{\pi}{1 - \pi} \right) \\
            \sigma \left( 2 \rho^* \|\bmu\|_2 (\rho^* \|\bmu\|_2 Y + G) + \log\frac{\pi}{1 - \pi} \right)
        \end{pmatrix}.
    \end{equation}
    % \begin{align}
    %     \hat p(\xx) & \cond \sigma\left( \rho^*\|\vmu\|_2 Y + G + \beta_0^* \right),
    %     \\
    %     p^*(\xx) & 
    %     \mathmakebox[\widthof{${} \cond {}$}][c]{\overset{\mathrm{d}}{=}} 
    %     \sigma \left( 2 \|\bmu\|_2 (\|\bmu\|_2 Y + G) + \log\frac{\pi}{1 - \pi} \right),
    %     \\
    %     \hat p_0(\xx) & \cond \sigma \left( 2 \rho^* \|\bmu\|_2 (\rho^* \|\bmu\|_2 Y + G) + \log\frac{\pi}{1 - \pi} \right).
    % \end{align}
\end{lem}
\begin{proof}
    Rewrite $\xx = y\bmu + \zz$ where $\zz \sim \normal(\bzero, \bI_d)$. By direct calculation, the three quantities $\hat p(\xx)$, $p^*(\xx)$, and $\hat p_0(\xx)$ can be expressed by
    \begin{align}
        \hat p(\xx) = \sigma\bigl( \< \xx, \hat\vbeta \> + \hat\beta_0 \bigr) 
        & = \sigma\left( \hat \rho \norm{\bmu}_2 y + \< \zz, \hat\vbeta \> +\beta_0 \right),
        \label{eq:p_hat_exp}
        \\
        p^*(\xx) = \P( y = 1 \,|\, \xx )
        & = \frac{\pi e^{-\frac12 \| \xx - \bmu \|_2^2} }{\pi e^{-\frac12 \| \xx - \bmu \|_2^2} + (1 - \pi) e^{-\frac12 \| \xx + \bmu \|_2^2}} 
        \label{eq:p_star_bayes}
        \\
        & = \sigma \left( 2\< \xx, \bmu \> + \log\frac{\pi}{1 - \pi} \right)  \notag \\
        & = \sigma \left( 2 \|\bmu\|_2 \Bigl( \norm{\bmu}_2 y + \< \zz, \bmu/\|\bmu\|_2 \> \Bigr)  + \log\frac{\pi}{1 - \pi} \right),
        \label{eq:p_star_exp}
        \\
        \hat p_0(\xx) = \P \bigl( y = 1 \,|\, \hat p(\xx) \bigr)
        & = \frac{\pi e^{-\frac12 (\hat f(\xx) - \hat\rho\|\bmu\|_2 - \hat\beta_0)^2} }{\pi e^{-\frac12 (\hat f(\xx) - \hat\rho\|\bmu\|_2 - \hat\beta_0)^2} + (1 - \pi) e^{-\frac12 (\hat f(\xx) + \hat\rho\|\bmu\|_2 - \hat\beta_0)^2} } 
        \label{eq:p0_bayes}
        \\
        & = \sigma \left( 2 \, \hat\rho \, \|\bmu\|_2 \< \xx, \hat\bbeta \> + \log\frac{\pi}{1 - \pi} \right) \notag \\
        & = \sigma \left( 2 \, \hat\rho \, \|\bmu\|_2 \left(\hat \rho \norm{\bmu}_2 y + \< \zz, \hat\vbeta \> \right) + \log\frac{\pi}{1 - \pi} \right),
        \label{eq:p0_exp}
    \end{align}
    where the Bayes' law is applied in \cref{eq:p_star_bayes} and \eqref{eq:p0_bayes}.

    Next, it suffices to obtain the joint asymptotics of $\< \zz, \hat\vbeta \>$ and $\< \zz, \bmu/\|\bmu\|_2 \>$, which appear in the expressions of \cref{eq:p_hat_exp}, \eqref{eq:p_star_exp}, \eqref{eq:p0_exp}. Note that $\< \zz, \hat\vbeta \> \cond \normal(0, 1)$ (since $\zz \indep \hat\vbeta$, $\P(\| \hat\vbeta \|_2 = 1) \to 1$), $\< \zz, \bmu/\|\bmu\|_2 \> \sim \normal(0, 1)$. Moreover, $\E[\< \zz, \hat\vbeta \> \< \zz, \bmu/\|\bmu\|_2 \> ] = \E[ \hat\rho ] \to \rho^*$ by \cref{thm:SVM_main} and bounded convergence. These implies
    \begin{equation*}
        \begin{pmatrix}
            \< \zz, \hat\vbeta \> \\
            \< \zz, \bmu/\|\bmu\|_2 \>
        \end{pmatrix}
        \cond
        \normal\left(
        \begin{pmatrix}
            0 \\
            0
        \end{pmatrix},
        \begin{pmatrix}
            1 & \rho^* \\
            \rho^* & 1
        \end{pmatrix}
        \right) \overset{\mathrm{d}}{=} 
        \begin{pmatrix}
            G \\
            G'
        \end{pmatrix}.
    \end{equation*}
    Since $y \indep (\zz, \hat\vbeta)$ and $(\hat\rho, \hat\beta_0) \conp (\rho^*, \beta_0^*)$, we conclude \cref{eq:p(x)_asymp} by \cref{eq:p_hat_exp}, \eqref{eq:p_star_exp}, \eqref{eq:p0_exp} and then using the Slutsky's theorem. This completes the proof.
\end{proof}


The proof of \cref{prop:conf} is primarily based on asymptotics in \cref{lem:conf_limit}.

\begin{proof}[\textbf{Proof of \cref{prop:conf}}]
% \vspace{0.5\baselineskip}
% \noindent
\textbf{\ref{prop:conf_asymp}:} For $\mathrm{MSE}$, by directly using the asymptotics in \cref{lem:conf_limit} and bounded convergence theorem, we have
\begin{align*}
    \lim_{n \to \infty} \mathrm{MSE}(\hat p) & = \lim_{n \to \infty} \E\left[ \bigl( \mathbbm{1}\{ y = 1 \} - \hat p(\xx) \bigr)^2 \right] \\
    & = \E\left[ \bigl( \mathbbm{1}\{ Y = 1 \} -  \sigma\left( \rho^*\|\vmu\|_2 Y + G + \beta_0^* \right)  \bigr)^2 \right] \\
    & = \E \left[ \sigma \bigl( -\rho^* \norm{\bmu}_2 - \beta_0^* Y + G \bigr)^2 \right] = \mathrm{MSE}^*, 
    \\
    \lim_{n \to \infty} \mathrm{mMSE}(\hat p) & = \mathrm{MSE}^*
    - \var\, \bigl[\mathbbm{1}\{ y = 1 \} \bigr] = \mathrm{MSE}^* - \pi(1 - \pi).
\end{align*}
For $\mathrm{CalErr}$, we similarly get
\begin{align*}
    & \lim_{n \to \infty} \mathrm{CalErr}(\hat p)
    = \lim_{n \to \infty} \E\left[ \bigl( \hat p(\xx) - \hat p_0(\xx) \bigr)^2 \right] \\
    = {} & \E\left[ \left(  \sigma\bigl( \rho^* \norm{\bmu}_2 Y + G + \beta_0^* \bigr)
    - \sigma\Bigl( 2\rho^* \norm{\bmu}_2 (\rho^* \norm{\bmu}_2 Y + G) + \log\frac{\pi}{1-\pi} \Bigr)
     \right)^2 \right]
     = \mathrm{CalErr}^*.
\end{align*}
For $\mathrm{ConfErr}$, we can first obtain
\begin{align*}
    \lim_{n \to \infty} \E \left[ p^*(\xx) \bigl( 1 - p^*(\xx) \bigr) \right]
    & = \lim_{n \to \infty} \E \, \Bigl[ \var\, \bigl[\mathbbm{1}\{ y = 1 \} \,|\, \xx \bigr] \Bigr] \\
    & = \lim_{n \to \infty} \E\left[ \bigl( \mathbbm{1}\{ y = 1 \} - p^*(\xx) \bigr)^2 \right] \\
    & = \E\left[ \left( \mathbbm{1}\{ Y = 1 \} - \sigma \Bigl( 2 \|\bmu\|_2 (\|\bmu\|_2 Y + G) + \log\frac{\pi}{1 - \pi} \Bigr) \right)^2 \right] \\
    & = \E\left[ \sigma\Bigl( -2\norm{\bmu}_2( \norm{\bmu}_2 + G ) - \log\frac{\pi}{1-\pi} Y \Bigr)^2 \right]
    = V_{y|\xx}^*,
\end{align*}
and then by relation between $\mathrm{ConfErr}$ and $\mathrm{MSE}$ \cref{eq:MSE_vs_ConfErr}
\begin{equation*}
    \lim_{n \to \infty} \mathrm{ConfErr}(\hat p)
    = \lim_{n \to \infty} \mathrm{MSE}(\hat p) - \lim_{n \to \infty} \E \left[ p^*(\xx) \bigl( 1 - p^*(\xx) \bigr) \right]
    = \mathrm{MSE}^* - V_{y|\xx}^*.
\end{equation*}
This concludes the proof of part \ref{prop:conf_asymp}.

\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{prop:conf_mono}:}
When $\tau = \tau^\mathrm{opt}$, by \cref{prop:tau_opt} $\beta_0^* = 0$. Then we can simplify
    \begin{equation*}
        % \begin{aligned}
            \mathrm{MSE}^* =  \E\left[ \bigl(  1 + \exp( \rho^* \norm{\bmu}_2 + G) \bigr)^{-2} \right] .
            % \\
            % \mathrm{TMSE}^* & = \E\left[ \left( \mathbbm{1}\{ \rho^* \norm{\bmu}_2 + G  >  0 \} - \frac{1}{1 + \exp( - \abs{\rho^* \norm{\bmu}_2 + G} ) } \right)^{2} \right] \\
            % \mathrm{V}_\mathrm{Err}^* & = \Err(1 - \Err).
        % \end{aligned}
    \end{equation*}
    According to \cref{lem:rho_mono}, we know that $\rho^* \norm{\bmu}_2$ is increasing in $\pi \in (0, \frac12)$, $\norm{\bmu}_2$, and $\delta$. It suffices to show that $\mathrm{MSE}^*$ is decreasing in $\rho^* \norm{\bmu}_2$, which is obvious by noticing $t \mapsto ( 1 + \exp(t) )^{-2}$ is a strictly decreasing function.

    For $\mathrm{mMSE}^*$, note that $\pi(1 - \pi)$ is a increasing function of $\pi \in (0, \frac12)$, and it does not depend on $\norm{\bmu}_2$, $\delta$. These shows the monotonicity of $\mathrm{mMSE}^* = \mathrm{MSE}^* - \pi(1 - \pi)$.

    For $\mathrm{ConfErr}^*$, note that $V_{y|\xx}^*$ does not depend on $\delta$. This implies that $\mathrm{ConfErr}^*$ has the same monotonicity in $\delta$ as $\mathrm{MSE}^*$, which concludes the proof of part \ref{prop:conf_mono}.
\end{proof}



\subsection{Verification of \cref{claim:conf}}

The analytical dependence of $\mathrm{CalErr}^*$ and $\mathrm{ConfErr}^*$ on model parameters is more complicated. We provide a numerical verification of \cref{claim:conf}.

\begin{proof}[\textbf{Verification of \cref{claim:conf}}]
    For $\mathrm{CalErr}^*$, denote
            \begin{align*}
                h_1(t) & := \E\left[ \Bigl( \sigma\bigl( 2 t (G+t) + c \bigr) - \sigma( G+t ) \Bigr)^2 \right]
                \\
                h_2(t) & := \E\left[ \Bigl( \sigma\bigl( 2 t (G-t) + c \bigr) - \sigma( G-t ) \Bigr)^2 \right]
            \end{align*}
            where $c < 0$ is a constant. When $\tau = \tau^\mathrm{opt}$, we have $\beta_0^* = 0$ and
        \begin{equation*}
            \mathrm{CalErr}^*
            = \pi h_1( \rho^* \norm{\bmu}_2 ) + (1 - \pi) h_2( \rho^* \norm{\bmu}_2 ),
            \qquad \text{where} ~ c = \log\frac{\pi}{1 - \pi}.
    \end{equation*}
    According to \cref{fig:mono_fun}, we can numerically show that $h(t) :=  \pi h_1(t) + (1 - \pi) h_2(t)$ is a decreasing function when $\pi \le \overline{\pi} \approx 0.25$ is fixed. Under this condition, $\mathrm{CalErr}^*$ is decreasing in $\rho^* \norm{\bmu}_2$. Then by using \cref{lem:rho_mono} and similar arguments in the proof of \cref{prop:conf}\ref{prop:conf_mono}, we can conclude the monotonicity of $\mathrm{CalErr}^*$ in $\norm{\bmu}_2$ and $\delta$.

    \begin{figure}[h!]
    \centering
    \includegraphics[width=0.32\textwidth]{Figs/cal_proof_h_plot_1.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/cal_proof_h_plot_2.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/cal_proof_Vyx_plot.pdf}
    \caption{
    \textbf{Monotonicity of $x \mapsto h(x)$ and $\pi \mapsto V_{y|\xx}^*$}. \textbf{Left:} $h$ is not monotone when $\pi > \overline{\pi} \approx 0.25$. \textbf{Middle:} $h$ is monotone decreasing when $\pi \le \overline{\pi} \approx 0.25$. \textbf{Right:} $V_{y|\xx}^*$ is monotone increasing in $\pi$ for different values of $\norm{\bmu}_2$.
    }
    \label{fig:mono_fun}
\end{figure}

For $\mathrm{ConfErr}^*$, in \cref{fig:mono_fun} we numerically show that $V_{y|\xx}^*$ is increasing in $\pi$ when $\norm{\bmu}_2$ is fixed. Since $\mathrm{ConfErr}^* = \mathrm{MSE}^* - V_{y|\xx}^*$ and we have shown in \cref{prop:conf}\ref{prop:conf_mono} that $\mathrm{MSE}^*$ is decreasing in $\pi$, we conclude $\mathrm{ConfErr}^*$ is also decreasing in $\pi$.
\end{proof}

    % (a): Recall $\xx_\text{new} = y_\text{new} \bmu + \zz_\text{new}$, $\hat f(\xx_\text{new}) = y_\text{new}\hat\rho\norm{\bmu}_2 + \< \zz_\text{new}, \hat\bbeta \> + \hat\beta_0$ and $\< \zz_\text{new}, \hat\bbeta \> \sim \normal(0, 1)$.
    % \begin{equation*}
    %     \begin{aligned}
    %          \mathrm{MSE}(\hat p) 
    %          & 
    %          = \E\left[ \bigl( \mathbbm{1}\{ y_\text{new} = 1 \} - \hat p(\xx_\text{new}) \bigr)^2  \right]
    %          \\
    %          & = \E\left[ \biggl( \mathbbm{1}\{ y_\text{new} = 1 \} - 
    %          \frac{1}{1 + \exp\bigl( - y_\text{new}\hat\rho\norm{\bmu}_2 - \< \zz_\text{new}, \hat\bbeta \> - \hat\beta_0 \bigr)}
    %          \biggr)^2  \right]
    %     \end{aligned}
    % \end{equation*}
    % Note $( y_\text{new}, \< \zz_\text{new}, \hat\bbeta \>, \hat\rho, \hat\beta_0) \cond (y_\text{new}, G, \rho^*, \beta_0^*)$ where $G \sim \normal(0, 1)$ and $G \indep y_\text{new}$. By DCT,
    % \begin{equation*}
    %     \begin{aligned}
    %          \lim_{n \to \infty} \mathrm{MSE}(\hat p) 
    %          & 
    %          = \E\left[ \biggl( \mathbbm{1}\{ y_\text{new} = 1 \} - 
    %          \frac{1}{1 + \exp\bigl( - y_\text{new} \rho^*\norm{\bmu}_2 - G - \beta_0^* \bigr)}
    %          \biggr)^2  \right] \\
    %          & 
    %          =  \pi \E\left[ \biggl( 1 - 
    %          \frac{1}{1 + \exp\bigl( -\rho^*\norm{\bmu}_2 - G - \beta_0^* \bigr)}
    %          \biggr)^2  \right]
    %          + (1 - \pi) \E\left[ \biggl(  
    %          \frac{1}{1 + \exp\bigl( \rho^*\norm{\bmu}_2 - G - \beta_0^* \bigr)}
    %          \biggr)^2  \right]
    %          \\
    %          &
    %          =
    %          \pi \E\left[ \bigl(  1 + \exp( \rho^* \norm{\bmu}_2 + G + \beta_0^*) \bigr)^{-2} \right] 
    %             + (1 - \pi) \E\left[ \bigl(  1 + \exp( \rho^* \norm{\bmu}_2 + G - \beta_0^* ) \bigr)^{-2} \right].
    %     \end{aligned}
    % \end{equation*}
    % For CE,
    % \begin{equation*}
    %     \mathrm{CalErr}(\hat p) = \E\left[  \Bigl(  
    %     \E\left[ \P \bigl( y_\text{new} = 1 \,|\,  \hat f(\xx_\text{new}), \XX, \yy \bigr) \, \big| \, \hat f(\xx_\text{new}) \right] - \hat p(\xx_\text{new})
    %     \Bigr)^2 \right]. 
    % \end{equation*}
    % Recall that $\hat f(\xx_\text{new}) \,|\, (y_\text{new}, \XX, \yy) \sim \normal( y_\text{new} \hat\rho \norm{\bmu}_2 + \hat\beta_0,  1)$. Then by Bayesian formula,
    % \begin{equation*}
    %     \begin{aligned}
    %         & \P \bigl( y_\text{new} = 1 \,|\,  \hat f(\xx_\text{new}), \XX, \yy \bigr)  \\
    %         = {} & 
    %         \frac{\P(y_\text{new} = 1) \cdot \texttt{p}(\hat f(\xx_\text{new}) \,|\, y_\text{new} = 1, \XX, \yy)}{
    %         \P(y_\text{new} = 1) \cdot \texttt{p}(\hat f(\xx_\text{new}) \,|\, y_\text{new} = 1, \XX, \yy)
    %         + \P(y_\text{new} = -1) \cdot \texttt{p}(\hat f(\xx_\text{new}) \,|\, y_\text{new} = -1, \XX, \yy)
    %         }
    %         \\
    %         = {} &
    %         \frac{\pi \exp\bigl\{ -\frac12 (\hat f(\xx_\text{new}) - \hat\rho \norm{\bmu}_2 - \hat\beta_0)^2 \bigr\}}{
    %         \pi \exp\bigl\{ -\frac12 (\hat f(\xx_\text{new}) - \hat\rho \norm{\bmu}_2 - \hat\beta_0)^2 \bigr\}
    %         + (1 - \pi) \exp\bigl\{ -\frac12 ( \hat f(\xx_\text{new}) + \hat\rho \norm{\bmu}_2 - \hat\beta_0 )^2 \bigr\}
    %         }
    %         \\
    %         = {} &
    %         \frac{1}{
    %         1 
    %         + \frac{1 - \pi}{\pi} \exp \bigl\{ -\frac12 \bigl[ ( \hat f(\xx_\text{new}) + \hat\rho \norm{\bmu}_2 - \hat\beta_0 )^2
    %         - ( \hat f(\xx_\text{new}) - \hat\rho \norm{\bmu}_2 - \hat\beta_0 )^2 \bigr]
    %         \bigr\}
    %         }
    %         \\
    %         = {} &
    %         \frac{1}{
    %         1 
    %         + \exp \bigl\{ - 2 \hat\rho \norm{\bmu}_2 ( \hat f(\xx_\text{new}) - \hat\beta_0)
    %         - \log \frac{\pi}{1-\pi}
    %         \bigr\}
    %         }
    %     \end{aligned}
    % \end{equation*}
    % By DCT of conditional expectation,
    % \begin{equation*}
    %     \E\left[ \P \bigl( y_\text{new} = 1 \,|\,  \hat f(\xx_\text{new}), \XX, \yy \bigr) \, \big| \, \hat f(\xx_\text{new}) \right]
    %     \xrightarrow{n \to \infty} 
    %     \frac{1}{
    %         1 
    %         + \exp \bigl\{ - 2 \rho^* \norm{\bmu}_2 ( \hat f(\xx_\text{new}) - \beta_0^*)
    %         - \log \frac{\pi}{1-\pi}
    %         \bigr\}
    %         }.
    % \end{equation*}
    % Note that $\hat f(\xx_\text{new}) = y_\text{new}\hat\rho\norm{\bmu}_2 + \< \zz_\text{new}, \hat\bbeta \> + \hat\beta_0$ and $\< \zz_\text{new}, \hat\bbeta \> \sim \normal(0, 1)$. Therefore, by DCT again,
    % \begin{equation*}
    %     \begin{aligned}
    %          & \lim_{n \to \infty} \mathrm{CalErr}(\hat p) \\
    %         = {} & 
    %         \E\left[  \lim_{n \to \infty} \Bigl(  
    %     \E\left[ \P \bigl( y_\text{new} = 1 \,|\,  \hat f(\xx_\text{new}), \XX, \yy \bigr) \, \big| \, \hat f(\xx_\text{new}) \right] - \hat p(\xx_\text{new})
    %     \Bigr)^2 \right]. 
    %     \\
    %         = {} & 
    %         \E\left[ \biggl(
    %          \frac{1}{
    %         1 
    %         + \exp \bigl\{ - 2 \rho^* \norm{\bmu}_2 ( y_\text{new} \rho^*\norm{\bmu}_2 + G)
    %         - \log \frac{\pi}{1-\pi}
    %         \bigr\}
    %         }
    %         -
    %         \frac{1}{1 + \exp( - y_\text{new} \rho^*\norm{\bmu}_2 - G - \beta_0^* )}
    %         \biggr)^2 \right],
    %     \end{aligned}
    % \end{equation*}
    % and the conclusion is followed by conditioning on $y_\text{new}$.