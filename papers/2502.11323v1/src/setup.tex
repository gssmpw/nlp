\section{Preliminaries}
\label{sec:preliminary}

\subsection{SVM and linear separability} \label{sec:background}

Consider our 2-GMM in \cref{model}. Denote $\XX = (\vx_1,\ldots,\vx_n)^\top \in \R^{n\times d}$ and $\yy = (y_1,\ldots,y_n)^\top \in \R^n$. Recall the general margin-rebalanced SVM in \cref{eq:SVM-m-reb}. For $\tau > 0$, it is convenient to write this SVM formulation into
\begin{equation}
	\label{eq:SVM}
    \begin{array}{rl}
    \maximize\limits_{\bbeta \in \R^d, \, \beta_0 \in \R} & \min\limits_{i \in [n]} \tilde{y}_i ( \langle \xx_i, \bbeta \rangle + \beta_0 ), \\
    \text{subject to} & \norm{\bbeta}_2 \le 1
    \end{array}
\end{equation}
by introducing the transformed labels
\begin{equation}\label{eq:trans-labels}
    \tilde{y}_i = \begin{cases} \tau^{-1} , & \ \text{if} \ y_i = + 1, \\
    -1, & \ \text{if} \ y_i = -1. \end{cases}
\end{equation}

% Let $\hat\kappa$ be the optimal objective value in \cref{eq:SVM}, which is the \emph{maximum margin} for data $(\XX, \yy)$. Moreover, $(\hat\vbeta, \hat\beta_0, \hat\kappa)$ is also the optimal solution to \cref{eq:SVM-m-reb}. Notice $\hat\kappa \ge 0$ always holds (by taking $\vbeta = 0$, $\beta_0 = 0$ in \cref{eq:SVM}), and we can observe the following relation.
% \begin{equation*}
% 	\begin{aligned}
% 		\text{(linearly separable)} 
% 		\ \ & \text{$\exists\, \vbeta\not=\bzero$, $\beta_0 \in \R$, such that $y_i ( \< \xx_i, \bbeta \> + \beta_0 ) > 0$, $\forall\, i \in [n]$,} \\
% 		& \Longleftrightarrow \quad \hat\kappa > 0, \quad \Longrightarrow \quad \|\hat\vbeta\|_2 = 1, \\
% 		\text{(not linearly separable)} 
% 		\ \ & \text{$\forall\, \vbeta\not=\bzero$, $\beta_0 \in \R$, such that $y_i ( \< \xx_i, \bbeta \> + \beta_0 ) \overset{\mathmakebox[0pt][c]{\smash{(*)}}}{\le}  0$, $\forall\, i \in [n]$,} \\
% 		& \Longleftrightarrow \quad \hat\kappa = 0, \quad \Longrightarrow \quad \hat\vbeta = \bzero, \ \hat\beta_0 = 0 \text{ is a solution.\footnotemark}
% 	\end{aligned}
%     \footnotetext{
% 	If $(*)$ is strict ($<$), then $\hat\vbeta = \bzero$, $\hat\beta_0 = 0$ is the \emph{unique} solution.
% }
% \end{equation*}
% When data is linearly separable, it turns out \cref{eq:SVM-m-reb} also has the following equivalent form:
% \begin{equation}
% 	\label{eq:SVM-1}
%     \begin{array}{rl}
%     \minimize\limits_{\bw \in \R^d, \, w_0 \in \R} & \norm{\bw}_2^2, \\
%     \text{subject to} &  \wt y_i(\< \xx_i, \bw \> + w_0) \ge 1, \quad \forall\, i \in [n].
%     \end{array}
% \end{equation}
% The parameters in \cref{eq:SVM-m-reb} and \eqref{eq:SVM-1} have one-to-one relation $(\kappa, \vbeta, \beta_0) = (1, \bw, w_0)/\|\bw\|_2$. Notably, \cref{eq:SVM-1} is known as the hard-margin SVM \cite{vapnik1998statistical} if $\tau = 1$. 

According to the following deterministic result, the solution to margin-rebalanced SVM \cref{eq:SVM-m-reb} is a simple post-hoc adjustment of the solution to the original SVM \cref{eq:SVM-0}.

\begin{prop}\label{prop:SVM_tau_relation}
	% Consider the non-degenerate case, namely $1 \le n_+ \le n-1$.
	\begin{enumerate}[label=(\alph*)]
		\item %\label{prop:prop:SVM_tau_relation(a)}
            When data is linearly separable, \cref{eq:SVM-m-reb} has a unique solution.
            \item %\label{prop:prop:SVM_tau_relation(b)}
            Let $(\hat\vbeta(\tau), \hat\beta_0(\tau), \hat\kappa(\tau))$ be an optimal solution to \cref{eq:SVM-m-reb} under hyperparameter $\tau$. Then
        \begin{equation}\label{eq:margin-balance}
		    \hat\vbeta(\tau) = \hat\vbeta(1),
        \qquad
        \hat\beta_{0}(\tau) = \hat\beta_{0}(1) + \frac{\tau - 1}{\tau + 1} \hat\kappa(1),
        \qquad
        \hat\kappa(\tau) = \frac{2}{\tau + 1} \hat\kappa(1).
		\end{equation}
	\end{enumerate}
\end{prop}
% \begin{proof}[Proof idea:] 
%     Denote hyperplane $l_1 : \< \xx, \hat\vbeta \> + \hat\beta_0 = 0$ as the decision boundary to the SVM %solution to \cref{eq:SVM-m-reb}
%     under $\tau = 1$ (the red dotted line in \cref{fig:SVM_cartoon}), which is unique by strong convexity. According to \cref{fig:SVM_cartoon}, we can always reformulate a solution to \cref{eq:SVM-m-reb} under arbitrary $\tau$ by transplanting $l_1$ to some $l_\tau$ (the red solid line), which is equivalent to changing $\hat\beta_0$ in $l_1$. Regardless of $\tau$, the yellow region sandwiched between \emph{support vectors} remains unchanged.
% \end{proof}


\begin{rem}
	As shown in \cref{fig:SVM_cartoon}, there is a clear geometric interpretation of $\hat\vbeta, \hat\beta_0, \hat\kappa$ and $\tau$ in the max-margin classifier.
	\begin{itemize}
		\item $\hat\vbeta(\tau)$ determines the support vectors and the ``direction'' of decision boundary, which does not depend on $\tau$. Notably, margin rebalancing does not change $\hat\vbeta$.
		\item $\hat\beta_0(\tau)$ balances the positive/negative margins via \cref{eq:margin-balance}, where $\tau$ determines the amount of the shift.
		\item $\hat\kappa(\tau) \propto (\tau + 1)^{-1}$ in a fixed dataset.
	\end{itemize}
\end{rem}


\subsection{Connections between logistic regression and SVM}
\label{subsec:LR_vs_SVM}

The two classifiers in Eqs.~\eqref{eq:logistic} and \eqref{eq:SVM-0} are strongly connected in high dimensions: the SVM can be viewed as the limit of logistic regression when the data are linearly separable. 

% Below we state the results of \cite{rosset2003margin} and \cite{Soudry_implicit_bias} with slight modifications.

% For convenience, we first introduce the background of inductive bias without the intercept term from \cite{rosset2003margin, rosset2004boosting} and \cite{Soudry_implicit_bias, ji2019riskparameterconvergencelogistic}. 
We first introduce the background of inductive bias from \cite{rosset2003margin, rosset2004boosting} and \cite{Soudry_implicit_bias, ji2019riskparameterconvergencelogistic}. 
In logistic regression, we minimize the empirical loss \cref{eq:logistic}
% $\cL(\vbeta) = \frac1n\sum_{i=1}^n \ell( y_i \langle \vx_i, \vbeta \rangle )$ 
where $\ell(t) = \log (1+e^{-t})$ is the logistic loss. Since the loss is strictly convex, if it admits a finite minimizer, then the minimizer must be unique. However, when the data are linearly separable, there is no finite minimizer and the objective value goes to $0$ for certain $\vbeta$ with $\|\vbeta\|_2 \to \infty$. To obtain a unique solution, we may add a regularizer:
% \begin{equation}
% \label{eq:l2_logistic}
%     \hat \vbeta_\lambda := \argmin \limits_{\vbeta \in \R^d}
%     \left\{ \cL(\vbeta) + \lambda \| \vbeta\|_2^2  \right\}.
% \end{equation}
\begin{equation}\label{eq:logistic-l2}
        \bigl( \hat\vbeta_\lambda, \hat\beta_{0,\lambda} \bigr) := \argmin_{\vbeta \in \R^d, \beta_0 \in \R}
        \left\{
        \frac{1}{n} \sum_{i=1}^n \ell \bigl( y_i(\langle \vx_i, \vbeta \rangle + \beta_0) \bigr) +  \lambda \| \vbeta\|_2^2 \right\}.
    \end{equation}
Let $(\hat\vbeta, \hat\beta_{0})$
% $\hat\vbeta := \min_{\norm{\vbeta}_2 \le 1} \min_{i \in [n]} y_i\< \xx_i, \vbeta \>$ 
be the max-margin solution to SVM \cref{eq:SVM-0}. Then it has been shown by \cite{rosset2003margin, rosset2004boosting} that without the presence of intercept
% under some conditions, 
\begin{equation}\label{eq:explicit_bias}
    \lim_{\lambda \to 0^+} \frac{\hat \vbeta_\lambda}{\| \hat \vbeta_\lambda \|_2} = \hat \vbeta.
\end{equation}
From this view, logistic regression with a vanishing ridge regularizer is equivalent to max-margin classifier in the separable regime. By modifying the proof in \cite{rosset2003margin}, we can generalize their conclusion with $\beta_0$ included.
\begin{prop}\label{prop:explicit_bias}
    Let $(\hat\vbeta_\lambda, \hat\beta_{0,\lambda})$ be the minimizer of the regularized objective function in \cref{eq:logistic-l2}, where $\ell: \R \to \R_{\ge 0}$ is any convex, non-decreasing, rapidly varying loss function in the sense that
    \begin{equation*}
        \lim_{t \to \infty} \frac{\ell(\varepsilon t)}{\ell(t)} = \infty, \qquad \forall\, \varepsilon \in (0, 1).
    \end{equation*}
    Assume the data is linearly separable. 
    % Let $(\hat\vbeta, \hat\beta_0)$ be the solution to SVM \cref{eq:SVM-0}. 
    Then the convergence in \cref{eq:explicit_bias} holds. Moreover, we have $\lim_{\lambda \to 0^+} \hat\beta_{0,\lambda}/\| \hat \vbeta_\lambda \|_2 = \hat\beta_{0}$.
\end{prop}

Another approach of establishing the connection does not require adding an explicit regularizer. 
% Denote the concatenated vector $\vtheta = (\vbeta, \beta_0)$. 
For convenience, let 
$\cL(\vbeta) = \frac1n\sum_{i=1}^n \ell( y_i \langle \vx_i, \vbeta \rangle )$ and
consider the gradient descent iterates $\vbeta^{(t+1)} = \vbeta^{(t)} - \eta \nabla \cL(\vbeta^{(t)})$ where $t=1,2,\ldots$ and 
%\begin{equation*}
%    \vbeta^{(t+1)} = \vbeta^{(t)} - \eta \nabla \cL(\vbeta^{(t)}), \qquad t=1,2,\ldots
%\end{equation*}
$\vbeta^{(t)}$ is the parameter vector at iteration $t$. It it shown by \cite{Soudry_implicit_bias} that under a sufficiently small step size $\eta$, 
\begin{equation*}
    \lim\limits_{t\to\infty}\frac{\vbeta^{(t)}}{\| \vbeta^{(t)} \|_2} = \hat \vbeta,
\end{equation*}
where $\hat\vbeta := \min_{\norm{\vbeta}_2 \le 1} \min_{i \in [n]} y_i\< \xx_i, \vbeta \>$. This is often referred to as the implicit bias.

% In particular, the convergence in \cref{eq:explicit_bias} also holds in the presence of an intercept $\beta_0$. Consider \cref{eq:logistic} with an $\cL^2$ regularizer added:

\subsection{Notations}

We typically use italic letters to denote scalars and random variables (e.g., $a, b, c, G, Y, \ldots \in \R$), boldface (italic) lowercase letters to denote (random) vectors (e.g., $\ba, \bs, \xx, \yy,  \ldots \in \R^d$), and boldface (italic) uppercase letters to denote (random) matrices (e.g. $\bA, \bP, \XX, \GG,  \ldots \in \R^{d_1 \times d_2}$). For any positive integer $n$, let $[n] = \{1, 2, \ldots, n\}$. For a scalar $a$, let $a_+ = \max\{ a, 0 \}$ and $a_+ = \max\{ -a, 0 \}$. For vectors $\bu$, $\bv$ of the same length, let $\< \bu, \bv \> = \bu^\top \bv$ denote their standard inner product, and write $\bu \perp \bv$ if they are orthogonal ($\< \bu, \bv \> = 0$). The corresponding Euclidean norm is $\norm{\bu} = \norm{\bu}_2 = \< \bu, \bu \>^{1/2}$. For a matrix $\bA$, let $\norm{\bA}_{\mathrm{op}}$ denote its operator norm and $\norm{\bA}_\mathrm{F}$ its Frobenius norm. We use $\phi$ and $\Phi$ to denote the cumulative distribution function (CDF) and probability density function (PDF) of standard normal distribution. Let $\Law(X)$ denote the distribution of random variable (or vector) $X$. We write $X \indep Y$ if $X$ and $Y$ are independent random variables.

We use $O(\cdot)$ and $o(\cdot)$ for the standard big-$O$ and small-$o$ notations. For real sequences $(a_n)_{n \ge 1}$, $(b_n)_{n \ge 1}$, we write $a_n \lesssim b_n$ or  $b_n \gtrsim a_n$ if $a_n = O(b_n)$, and $a_n \asymp b_n$ if $a_n \lesssim b_n$ and $a_n \gtrsim b_n$. We also write $a_n \ll b_n$ or $b_n \gg a_n$ if $a_n = o(b_n)$. We write $a_n \propto b_n$ if $a_n = c b_n, \forall\, n \ge 1$ for some constant $c > 0$. Let $\cond$, $\conp$, $\conL{p}$ denote stochastic convergence in distribution, in probability, in $\cL^p$, respectively, and let $\conw$ denote weak convergence of measures. We also use $O_{\P}(\cdot)$ and $o_{\P}(\cdot)$ for the standard big-$O$ and small-$o$ in probability notations. Denote $\wt O_{\P}(\cdot)$ as a variant of $O_{\P}(\cdot)$ which hides polylogarithmic factors. 

Given two probability measures $P$, $Q$ on $\R^d$, their second Wasserstein ($W_2$) distance is defined as
\begin{equation*}
    W_2(P, Q) := \left( \inf_{\gamma \in \Gamma(P, Q) }
    \int \norm{\bx - \by}_2^2 \gamma(\d\bx \times \d\by)
    \right)^{1/2},
\end{equation*}
where the infimum is taken over the set of couplings $\Gamma(P, Q)$ of distributions $P$ and $Q$. For any $x \in \R$ and $\lambda > 0$, the Moreau envelope of a continuous convex function $\ell: \R \to \R_{\ge 0}$ is defined as
\begin{equation*}
    \envelope_\ell(x; \lambda) = \envelope_{\lambda\ell}(x)
    := \min_{t \in \R} \left\{  \ell(t) +  \frac1{2\lambda} (t - x)^2 \right\},
\end{equation*}
and the proximal operator of $\ell$ is defined as
\begin{equation*}
    \prox_{\ell}(x; \lambda) =
    \prox_{\lambda \ell}(x) := \argmin_{t \in \R} \left\{ \ell(t) +  \frac1{2\lambda} (t - x)^2 \right\}.
\end{equation*}


%Matrix and vector boldfaced. Vector norms and matrix norms. positive part $a_+$ and negative part $a_-$. Normal CDF. Independence. Law/distribution.

%Asymptotic notations $O_P$ and $o_p$, $\ll$, $\gg$, and $\asymp$, $\propto$. Convergence in probability, under $L_2$. Wasserstein distance $W_2$. Convergence of empirical measures. 

%\ljycom{The following may be placed somewhere else.

%Throughout the proportional regime, we are interested in the limit $n, d \to \infty$, with $n/d \to \delta \in (0, \infty)$, which is often abbreviated as $n \to \infty$, and $d = d_n$ with $n/d_n \to \delta$.}

%\ljycom{Maybe too long. We can put some less important notations (only appear in the proof) to appendix.}