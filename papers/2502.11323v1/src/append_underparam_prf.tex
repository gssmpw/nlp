\section{Logit distribution for non-separable data: Proofs for \cref{sec:logit_logistic}}
\label{append_sec:nonsep}

\subsection{Proof of \cref{thm:logistic_main}}

Throughout this section, we assume the loss function $\ell: \R \to \R_{\ge 0}$ is non-increasing, strictly convex, and twice differentiable. Based on these assumptions, we establish the following properties of $\ell$.
\begin{lem}\label{lem:ell}
    Let $\ell \in C^1(\R)$ be a nonnegative, non-increasing, and strictly convex function. Then
    \begin{enumerate}[label=(\alph*)]
        \item $\ell$ is strictly decreasing.
        \item $\ell(-\infty) = +\infty$ and $\ell(+\infty) = \underline{\ell}$ for some $\underline{\ell} \in [0, +\infty)$. 
    \end{enumerate}
\end{lem}
\begin{proof}
    Notice that $\ell'(u) \le 0$ (by non-increasing) and $\ell'(u)$ is strictly increasing (by strict convexity), which implies that $\ell'(u) < 0$ for all $u \in \R$ and hence deduces part (a). For part (b), the limits $\lim_{u \to \pm \infty} \ell(u)$ are well-defined, and $\ell(+\infty) = \underline{\ell}$ for some $\underline{\ell} \in [0, +\infty)$ since $\ell$ is monotone and bounded from below. It remains to show $\ell(-\infty) = +\infty$.

    Assume $\ell(-\infty) = \overline{\ell} < \infty$ by contradiction. By convexity, we have $\ell(u) \le \frac12( \ell(2u) + \ell(0) )$ for any $u \in \R$. Taking $u \to -\infty$ on both sides yields $\overline{\ell} \le \frac{1}{2}(\overline{\ell} + \ell(0))$, hence $\overline{\ell} \le \ell(0)$, which contradicts the fact that $\ell$ is strictly decreasing. Therefore, we must have $\ell(-\infty) = +\infty$.
\end{proof}
Without loss of generality, assume $\underline{\ell} := \ell(+\infty) = 0$. Otherwise, we can just consider $\ell - \underline{\ell}$ instead of $\ell$. In addition, we also assume $\ell$ is pseudo-Lipschitz, i.e., there exists a constant $L > 0$ such that, for all $x, y \in \R$,
\begin{equation*}
    \abs{ \ell(x) - \ell(y) } \le L \left( 1 + \abs{ x } + \abs{ y } \right) \abs{ x - y }.
\end{equation*}
% This is true for logistic loss, squared loss, but not for exponential loss...}
For ease of exposition, we assume $\tau = 1$, as it is not fundamentally different from the case of arbitrary $\tau > 0$. In \cref{subsubsec:under_final}, we will discuss how to extend our proof to general $\tau > 0$.

Recall the original unconstrained empirical risk minimization (ERM) problem \cref{eq:logistic}:
\begin{equation}
\label{eq:ERM-0}
    M_n := 
    \min_{\bbeta \in \R^d, \, \beta_0 \in \R} \hat R_n(\bbeta, \beta_0)
    :=
    \min_{\bbeta \in \R^d, \, \beta_0 \in \R}  \frac1n \sum_{i=1}^n \ell\bigl( 
        y_i(\< \xx_i, \bbeta \> +  \beta_0 )
     \bigr).
\end{equation}

\noindent
We first provide an outline for the proof of \cref{thm:logistic_main}, which involves several intermediate steps of simplifying the random optimization problem $M_n$.
\begin{equation*}
\begin{aligned}
    M_{n}
    \, \xRightarrow[\text{\cref{lem:ERM_bound_beta}}]{\textbf{Step 1}} \,
    M_n(\bTheta_{\vbeta}, \bXi_{\bu})
    \, \xRightarrow[\text{\cref{lem:ERM_CGMT}}]{\textbf{Step 2}} \, 
    M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})
    \, \Rightarrow \,
    M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) 
    \\
    \, \xRightarrow[\text{\cref{lem:M2-3}}]{\textbf{Step 3}} \, 
    M_n^{(3)}(\bTheta_{c}, \bXi_{\bu})
    \, \Rightarrow \,
    M_n^{(3)}(\bTheta_{c}) 
    \, \xRightarrow[\text{\cref{lem:M3-star}}]{\textbf{Step 4}} \, 
    M^*(\bTheta_{c})
    \, \Rightarrow \,
    M^*.
\end{aligned}
\left.
\vphantom{\begin{matrix} \dfrac12 \\ \dfrac12 \end{matrix}}
\right\} \text{\scriptsize{\cref{thm:ERM_conv}}}
\end{equation*}

\paragraph{Step 1: Boundedness of $\vbeta$ and $\beta_0$ (from $M_{n}$ to $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$)}
Notice that by introducing the auxiliary variable $\bu = (u_1, \ldots, u_n)^\top \in \R^n$ and Lagrangian multiplier $\bv = (v_1, \ldots, v_n)^\top \in \R^n$, we can rewrite \cref{eq:ERM-0} as a minimax problem
\begin{align*}
        M_n & = \min_{ \substack{ \bbeta \in \R^d, \, \beta_0 \in \R \\  \bu \in \R^n } }
        \max_{ \bv \in \R^n }
        \left\{
        \frac1n \sum_{i=1}^n \ell( u_i )
         + \frac{1}{n} \sum_{i=1}^n v_i \bigl(  y_i(\< \xx_i, \bbeta \> +  \beta_0 ) - u_i\bigr)
         \right\} 
         \\
         & = \min_{ \substack{ \bbeta \in \R^d, \, \beta_0 \in \R \\  \bu \in \R^n } }
         \max_{ \bv \in \R^n }
         \left\{
         \frac1n \sum_{i=1}^n \ell( u_i )
          + \frac{1}{n} \sum_{i=1}^n v_i (  \< \bmu, \vbeta \> +  \< \zz_i, \vbeta \> + y_i \beta_0 - u_i )
          \right\},
\end{align*}
where in the second line, we reformulate $\xx_i = y_i(\vmu + \zz_i)$, $\zz_i \sim \normal(\bzero, \bI_d)$, $y_i \indep \zz_i$. For any closed subsets $\bTheta_{\vbeta} \subset \R^{d} \times \R$, $\bXi_{\bu} \subset \R^{n}$, we also define the quantity $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$, which can be viewed as the constrained version of ERM problem $M_n$.
\begin{equation}
    \label{eq:Mn}
    \begin{aligned}
        M_n(\bTheta_{\vbeta}, \bXi_{\bu})
        :\! & = \min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
    \max_{ \bv \in \R^n }
    \left\{
    \frac1n \sum_{i=1}^n \ell( u_i )
     + \frac{1}{n} \sum_{i=1}^n v_i (  \< \bmu, \vbeta \> +  \< \zz_i, \vbeta \> + y_i \beta_0 - u_i )
     \right\} \\
     & = \min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
     \max_{ \bv \in \R^n }
     \left\{
     \frac1n \sum_{i=1}^n \ell( u_i )
      + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
      + \frac1n \bv^\top \ZZ \vbeta + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu
      \right\},
    \end{aligned}
\end{equation}
where $\ZZ = (\zz_1, \ldots, \zz_n)^\top \in \R^{n \times d}$. Let $(\hat\vbeta_n, \hat\beta_{0, n})$ be the unique minimizer of \cref{eq:ERM-0}. The following lemma implies that $\hat\vbeta_n$ and $\hat\beta_{0, n}$ are bounded with high probability, which enables us to work with $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$ instead of $M_n$ for some compact sets $\bTheta_{\vbeta}$ and $\bXi_{\bu}$.

\begin{lem}[Boundedness of $\bbeta$ and $\beta_0$] \label{lem:ERM_bound_beta}
    In the non-separable regime $\delta > \delta^*(0)$, there exists some constants $C_{\bbeta}, C_{\beta_0}, C_{\bu} \in (0, \infty)$, such that $M_n = M_n(\bTheta_{\vbeta}, \bXi_{\bu})$ with high probability, where
    \begin{equation*}
    \bTheta_{\vbeta} = \{ (\vbeta, \beta_0) \in \R^d \times \R:  \norm{\vbeta}_2 \le C_{\vbeta},  \abs{\beta_0} \le C_{\beta_0}  \},
    \qquad
    \bXi_{\bu} = \{ \bu \in \R^n : \norm{\bu}_2 \le C_{\bu} \sqrt{n} \}.
    \end{equation*}
\end{lem}
\noindent
See \cref{subsubsec:under_step1} for the proof.

\paragraph{Step 2: Reduction via Gaussian comparison (from $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$ to $M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})$)}
The objective function of $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$ in \cref{eq:Mn} is a bilinear form of the Gaussian random matrix $\ZZ$. To simplify the bilinear term, we will use the convex Gaussian minimax theorem (CGMT), i.e., Gordon's comparison inequality \cite{gordon1985some, thrampoulidis2015regularized}. To do so, we introduce another quantity:
\begin{equation*}
    \begin{aligned}
        M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})
        : = \smash {\min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } } }
        \max_{ \bv \in \R^n }
        \, \Biggl\{
        \frac1n \sum_{i=1}^n \ell( u_i )
         + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
         & + \frac1n \norm{\bv}_2 \hh^\top \vbeta + \frac1n \norm{\vbeta}_2 \vg^\top \bv 
         \\
         & + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu
         \Biggr\},
    \end{aligned}
\end{equation*}
where $\hh \sim \normal(\bzero, \bI_{d})$, $\vg \sim \normal(\bzero, \bI_{n})$ are independent Gaussian vectors. However, the classical CGMT cannot be directly applied to $M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})$ since $\bv$ is maximized over an unbounded set. To this end, we proved the following version of CGMT, which connects $M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})$ with $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$.

\begin{lem}[CGMT, unbounded for maximum] \label{lem:ERM_CGMT}
For any compact sets $\bTheta_{\vbeta}$ and $\bXi_{\bu}$ (not necessarily convex) and $t \in \R$, we have
\begin{equation}\label{eq:unbounded_CGMT_1}
    \P \, \Bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \le t \Bigr) \le 2 \, \P \,\Bigl( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu}) \le t \Bigr).
\end{equation}
Additionally, if $\bTheta_{\vbeta}$ and $\bXi_{\bu}$ are convex, then
\begin{equation}\label{eq:unbounded_CGMT_2}
    \P \, \Bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \ge t \Bigr) \le 2 \, \P \,\Bigl( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu}) \ge t \Bigr).
\end{equation}
\end{lem}
\noindent
See \cref{subsubsec:under_step2} for the proof.




\paragraph{Reparametrization in low dimensions (from $M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})$ to $M_n^{(2)}(\bTheta_{c}, \bXi_{\bu})$)}
To simplify $M^{(1)}_n(\bTheta_{\vbeta}, \bXi_{\bu})$, we consider the following change of variables
\begin{equation}\label{eq:change_of_var}
    \rho := \cos(\vmu, \vbeta) := 
    \begin{cases} 
    \, \displaystyle
    \left\< \frac{\vmu}{\norm{\vmu}_2}, \frac{\vbeta}{\norm{\vbeta}_2} \right\>, 
                  & \ \text{if} \ \vbeta \not= \bzero, \\
    \  0,         & \ \text{if} \ \vbeta = \bzero, 
    \end{cases}
    \qquad
    R := \norm{\vbeta}_2.
    % \qquad
    % \gamma := \norm{\bv}_2.
\end{equation}
Now, for any closed subset $\bTheta_{c} \subset [-1, 1] \times \R_{\ge 0} \times \R$, we define the quantity $M_n^{(2)}(\bTheta_{c}, \bXi_{\bu})$ by 
\begin{equation*}
    \begin{aligned}
        M_n^{(2)}(\bTheta_{\vbeta}, \bXi_{\bu})
        : = \smash{ \min_{ \substack{ (\bbeta , \beta_0) \in \R^d \times \R: 
        \\ (\cos(\vmu, \vbeta), \norm{\vbeta}_2, \beta_0) \in \bTheta_{c} \\  \bu \in \bXi_{\bu} } }
        }
        \max_{ \bv \in \R^n }
        \, \Biggl\{
        \frac1n \sum_{i=1}^n \ell( u_i )
         & + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
         + \frac1n \norm{\bv}_2 \hh^\top \vbeta  
         \\
         & + \frac1n \norm{\vbeta}_2 \vg^\top \bv + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu
         \Biggr\}.
    \end{aligned}
\end{equation*}
Therefore, $M_n^{(2)}(\bTheta_{c}, \bXi_{\bu})$ can be viewed as reparametrization of $M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})$ when $\bTheta_{\vbeta} \subset \R^{d} \times \R$ takes the form
\begin{equation*}
        \bTheta_{\vbeta} = \left\{ 
        (\vbeta, \beta_0) \in \R^{d} \times \R:
        \bigl( \cos(\vmu, \vbeta), \norm{\vbeta}_2, \beta_0 \bigr) \in \bTheta_{c}
        \right\}.
\end{equation*}
Then we can simplify $M_n^{(2)}(\bTheta_{\vbeta}, \bXi_{\bu})$ as follows:
\begin{align}
        & M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) 
        \notag
        \\
        \overset{\mathmakebox[0pt][c]{\text{(i)}}}{=} {} & 
        \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \bu \in \bXi_{\bu} } } \min_{ \substack{ \norm{\vbeta}_2 = R \\ \cos(\vmu, \vbeta) = \rho } }
        \max_{\gamma \ge 0} \max_{\norm{\bv_0}_2 = 1}
        \left\{ \frac1n \sum_{i=1}^n \ell( u_i ) +
        \frac{\gamma}{n} \bv_0^\top (\rho\norm{\vmu}_2 R \bone + R\vg + \beta_0 \yy  - \bu )
        + \frac{\gamma}{n} \hh^\top\vbeta 
        \right\} 
        \notag
        \\
        \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{=} {} &  
        \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \bu \in \bXi_{\bu} } } \min_{ \substack{ \norm{\vbeta}_2 = R \\ \cos(\vmu, \vbeta) = \rho } }
        \max_{\gamma \ge 0} 
        \left\{ \frac1n \sum_{i=1}^n \ell( u_i ) +
        \frac{\gamma}{n} \bigl\| \rho\norm{\vmu}_2 R \bone + R\vg + \beta_0 \yy  - \bu \bigr\|_2
        + \frac{\gamma}{n} \hh^\top\vbeta 
        \right\} 
        \notag
        \\
        \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{=} {} & 
        \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \bu \in \bXi_{\bu} } } 
        \max_{\gamma \ge 0} 
        \, \Biggl\{ 
            \frac1n \sum_{i=1}^n \ell( u_i )
        + \frac{\gamma}{n} \bigl\| \rho\norm{\vmu}_2 R \bone + R\vg + \beta_0 \yy  - \bu \bigr\|_2
        + \frac{\gamma}{n} \min_{ \substack{ \norm{\vbeta}_2 = R \\ \cos(\vmu, \vbeta) = \rho } } \hh^\top\vbeta  
        \Biggr\} 
        \notag
        \\
        \overset{\mathmakebox[0pt][c]{\text{(iv)}}}{=} {} & 
        \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \bu \in \bXi_{\bu} } } 
        \max_{\gamma \ge 0} 
        \, \Biggl\{ 
            \frac1n \sum_{i=1}^n \ell( u_i )
        + \frac{\gamma}{n} \bigl\| \rho\norm{\vmu}_2 R \bone + R\vg + \beta_0 \yy  - \bu \bigr\|_2
        \notag
        \\
        & 
        \phantom{
        \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \bu \in \bXi_{\bu} } } 
        \max_{\gamma \ge 0} 
        \, \Biggl\{ 
            \frac1n \sum_{i=1}^n \ell( u_i )
        }
        \, + \frac{\gamma}{n} R\biggl( \rho\frac{\hh^\top \vmu}{\norm{\vmu}_2} - \sqrt{1 - \rho^2} \| \bP_{\bmu}^\perp \hh \|_2 \biggr)  
        \Biggr\},
        \label{eq:Mn(2)}
\end{align}
where in (i) we apply the change of variables \cref{eq:change_of_var} and optimize $\bv$ by its length $\gamma$ and direction $\bv_0$ separately, (ii) follows from Cauchy--Schwarz inequality, (iii) is from the linearity of objective function in $\gamma$, and (iv) is based on direct calculation by decomposing $\vbeta$:
\begin{equation*}
    \min_{ \substack{ \vbeta \in \R^d: \norm{\vbeta}_2 = 1 \\ \cos(\vmu, \vbeta) = \rho } } \hh^\top\vbeta
    = \min_{ \vtheta \in \R^d: \substack{ \norm{\vtheta}_2 = 1 \\ \< \bmu, \vtheta \> = 0 } } \hh^\top 
    \biggl( \rho \frac{\bmu}{\norm{\vmu}_2} + \sqrt{1 - \rho^2} \vtheta \biggr)
    =  
    \rho \frac{\hh^\top \bmu}{\norm{\vmu}_2} - \sqrt{1 - \rho^2} \| \bP_{\bmu}^\perp \hh \|_2,
\end{equation*}
where $\bP_{\vmu}^\perp := \bI_d - \vmu \vmu^\top / \norm{\vmu}_2^2$.




\paragraph{Step 3: Convergence in variational forms (from $M_n^{(2)}(\bTheta_{c}, \bXi_{\bu})$ to $M_n^{(3)}(\bTheta_{c}, \bXi_{\bu})$)}
To proceed from \cref{eq:Mn(2)}, we adopt the following trick from \cite{montanari2023generalizationerrormaxmarginlinear}, where $\bu$ could be viewed as a functional of the empirical measure given by $\vg = (g_1, \ldots, g_n)^\top$ and $\yy = (y_1, \ldots, y_n)^\top$. Formally, let $\Q_n$ be the empirical distribution of the coordinates of $(\vg, \yy)$, i.e., the probability measure on $\R^2$ defined by
\begin{equation*}
    \Q_n := \frac{1}{n}\sum_{i=1}^n \delta_{(g_i, y_i)}.
\end{equation*}
Let $\cL^2(\Q_n) := \cL^2(\Q_n, \R^2)$ be the space of functions $U: \R^2 \to \R$, $(g, y) \mapsto U(g, y)$ that are square integrable with respect to $\Q_n$. Notice that the $n$ points that form $\Q_n$ are almost surely distinct, and therefore we can identify this space with the space of vectors $\bu \in \R^n$. We also define the two random variables in the same space by $G(g, y) = g$, $Y(g, y) = y$. Denote $\E_{\Q_n}$, $\norm{\,\cdot\,}_{\Q_n}$ the integral and norm with respect to $\Q_n$ in $\cL^2(\Q_n)$, i.e.,
\begin{equation*}
    \E_{\Q_n}[U] := \int_{\R^2} U(g,y) \, \d \Q_n(g, y)
    = \frac{1}{n} \sum_{i=1}^n U(g_i, y_i)
    ,
    \qquad
    \norm{U}_{\Q_n} := (\E_{\Q_n}[U^2])^{1/2}.
\end{equation*}
Let $\Xi_{u} \subseteq \cL^2(\Q_n)$ be the corresponding subset identified by $\bXi_{\bu} \subseteq \R^n$, that is,
\begin{equation*}
    \Xi_{u} := \left\{ U \in \cL^2(\Q_n) : \bu := \left( U(g_1, y_1), \ldots, U(g_n, y_n) \right)^\top \in \bXi_{\bu}  \right\}. 
\end{equation*}
Then with these definitions, we can rewrite the expression of $M_n^{(2)}(\bTheta_{c}, \bXi_{\bu})$ as
\begin{align*}
        M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) 
        & = \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\  U \in \Xi_{u} } } 
        \max_{\gamma \ge 0}
        \, \Biggl\{ 
                \E_{\Q_n}[\ell(U)]
        + \frac{\gamma}{\sqrt{n}} \bigl\| \rho\norm{\vmu}_2 R + R G + \beta_0 Y - U \bigr\|_{\Q_n}
            \\
        & \phantom{.} \phantom{ 
            = \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ U \in \Xi_{u} } } 
        \max_{\gamma \ge 0}
        \, \Biggl\{ 
                % \E_{\Q_n}[\ell(U)]
        }
        + \frac{\gamma}{n} R\biggl( \rho\frac{\hh^\top \vmu}{\norm{\vmu}_2} - \sqrt{1 - \rho^2} \| \bP_{\bmu}^\perp \hh \|_2 \biggr)  
            \Biggr\}
        \\
        &   
        = \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
        \min_{ U \in \Xi_{u}  \cap   \mathcal{N}_n }
        \E_{\Q_n}[\ell(U)],
\end{align*}
where we define the (stochastic) subset $\mathcal{N}_n = \mathcal{N}_n(\rho, R, \beta_0)$ by
\begin{equation}
\label{eq:set_N_n}
    \mathcal{N}_n := \left\{
        U \in \cL^2(\Q_n): 
        \bigl\| \rho\norm{\vmu}_2 R + R G + \beta_0 Y - U \bigr\|_{\Q_n}
        \le \frac{R}{\sqrt{n}} \biggl( \sqrt{1 - \rho^2}  \| \bP_{\bmu}^\perp \hh \|_2  -  \rho\frac{\hh^\top \vmu}{\norm{\vmu}_2}  \biggr)
     \right\}.
\end{equation}
It can be shown that as $n, d \to \infty$,
\begin{equation*}
    \frac{R}{\sqrt{n}} \biggl( \sqrt{1 - \rho^2}  \| \bP_{\bmu}^\perp \hh \|_2  -  \rho\frac{\hh^\top \vmu}{\norm{\vmu}_2}  \biggr)
    \conp \frac{R \sqrt{1 - \rho^2}}{\sqrt{\delta}}.
\end{equation*}
This convergence then motivates us to define another quantity
\begin{equation}
    \label{eq:Mn(3)}
        M_n^{(3)}(\bTheta_{c}, \bXi_{\bu}) 
        :=  
        \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
        \min_{ U \in \Xi_{u}  \cap   \mathcal{N}^\delta_n }
        \E_{\Q_n}[\ell(U)],
\end{equation}
where the subset $\mathcal{N}^\delta_n = \mathcal{N}^\delta_n(\rho, R, \beta_0)$ is given by
\begin{equation}
\label{eq:set_N_n_delta}
    \mathcal{N}_n^\delta := \left\{
        U \in \cL^2(\Q_n): 
        \bigl\| \rho\norm{\vmu}_2 R + R G + \beta_0 Y - U \bigr\|_{\Q_n}
        \le \frac{R \sqrt{1 - \rho^2}}{\sqrt{\delta}}
     \right\}.
\end{equation}
The following lemma shows that $M_n^{(2)}$ and $M_n^{(3)}$ are close to each other:
\begin{lem}\label{lem:M2-3}
    For any compact sets $\bTheta_{c} \subset [-1, 1] \times \R_{\ge 0} \times \R$ and $\bXi_{\bu} \subset \R^{n}$ (not necessarily convex), as $n \to \infty$, we have
    \begin{equation*}
        \abs{ M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) - M_n^{(3)}(\bTheta_{c}, \bXi_{\bu}) } \conp 0.
    \end{equation*}
\end{lem}
\noindent
See \cref{subsubsec:under_step3} for the proof.




\paragraph{Step 4: Asymptotic characterization (from $M_n^{(3)}(\bTheta_{c}, \bXi_{\bu})$, $M_n^{(3)}(\bTheta_{c})$ to $M^*(\bTheta_{c})$, $M^*$)}
For any closed subsets $\bTheta_{c} \subset [-1, 1] \times \R_{\ge 0} \times \R$, we define the quantity $M_n^{(3)}(\bTheta_{c})$ by 
\begin{equation*}
    M_n^{(3)}(\bTheta_{c}) 
        :=  
        \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
        \min_{ U \in \mathcal{N}^\delta_n }
        \E_{\Q_n}[\ell(U)].
\end{equation*}
Compared with \cref{eq:Mn(3)}, clearly $M_n^{(3)}(\bTheta_{c}, \bXi_{\bu}) = M_n^{(3)}(\bTheta_{c})$ when $\bXi_{\bu}$ is large enough. To analyze $M_n^{(3)}(\bTheta_{c})$, we consider the change of variable\footnote{
We will show in \cref{lem:M_star_var} later that the minimizer of $M_n^{(3)}(\bTheta_{c})$ must satisfy $R \sqrt{1 - \rho^2} > 0$, hence the change of variable $\xi$ can be well-defined.
}
\begin{equation*}
    \xi := - \frac{\rho\norm{\vmu}_2 R + RG + \beta_0 Y - U}{R\sqrt{1 - \rho^2}},
\end{equation*}
Then we have
\begin{equation*}
        M_n^{(3)}(\bTheta_{c})
    = \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \xi \in \cL^2(\Q_n), \norm{\xi}_{\Q_n} \le 1/\sqrt{\delta} } } 
    \E_{\Q_n} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right].
\end{equation*}
Denote $\Q_\infty := \P$ the population measure of $(G, Y)$ (so that $(G, Y) \sim \normal(0, 1) \times P_y$ under $\Q = \Q_\infty$, and we have $\E_{\Q_\infty} := \E$, $\norm{U}_{\Q_\infty} := (\E[U^2])^{1/2}$). Then we also define the asymptotic counterpart of $M_n^{(3)}(\bTheta_{c})$ by replacing $\Q_n$ with $\Q_\infty$:
\begin{equation*}
    M^{*}(\bTheta_{c})
    := \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \xi \in \cL^2(\Q_\infty), \norm{\xi}_{\Q_\infty} \le 1/\sqrt{\delta} } } 
    \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right].
\end{equation*}
The following lemma shows that $M_n^{(3)}(\bTheta_{c})$ converges to the deterministic quantity $M^{*}(\bTheta_{c})$:

\begin{lem}\label{lem:M3-star}
    For any compact subset $\bTheta_{c} \subset [-1, 1] \times \R_{\ge 0} \times \R$, as $n \to \infty$, we have
    \begin{equation*}
        M_n^{(3)}(\bTheta_{c}) \conp M^{*}(\bTheta_{c}).
    \end{equation*}
\end{lem}
\noindent
See \cref{subsubsec:under_step4} for the proof.


\vspace{0.5\baselineskip}
Finally, combining \cref{lem:ERM_CGMT}---\ref{lem:M3-star}, we obtain the following theorem.
\begin{thm}
\label{thm:ERM_conv}
    Consider any compact sets $\bTheta_{\vbeta}$ and $\bXi_{\bu}$ such that $\bTheta_{\vbeta}$ has the form of
    \begin{equation}\label{eq:Theta_link}
        \bTheta_{\vbeta} = \left\{ 
        (\vbeta, \beta_0) \in \R^{d} \times \R:
        \bigl( \cos(\vmu, \vbeta), \norm{\vbeta}_2, \beta_0 \bigr) \in \bTheta_{c}
        \right\}
    \end{equation}
    for some compact domain $\bTheta_{c} \subset [-1, 1] \times \R_{\ge 0} \times \R$ of $(\rho, R, \beta_0)$. Assume $\bXi_{\bu}$ is large enough. Then, for any $\veps > 0$, as $n \to \infty$ , we have
    \begin{equation*}
        \P \, \bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \le M^*(\bTheta_{c}) - \veps \bigr) \to 0.
    \end{equation*}
    Further, if both $\bTheta_{\vbeta}$ and $\bXi_{\bu}$ are convex, then
    \begin{equation*}
        M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \conp M^*(\bTheta_{c}).
    \end{equation*}
\end{thm}
\begin{proof}
According to \cref{lem:M2-3} and \ref{lem:M3-star}, we have $M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) \conp M^*(\bTheta_{c})$ for any compact sets $\bTheta_{c} \subset [-1, 1] \times \R_{\ge 0} \times \R$ and $\bXi_{\bu} \subset \R^{n}$ large enough such that $\Xi_{u} \subset \mathcal{N}^\delta_n$. When $\bTheta_{\vbeta}$ takes the form \cref{eq:Theta_link}, by CGMT \cref{lem:ERM_CGMT}, for any $\veps > 0$ we have
\begin{align*}
    \P \, \Bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \le M^*(\bTheta_{c}) - \veps \Bigr) 
    & \le 2 \, \P \,\Bigl( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu}) \le M^*(\bTheta_{c}) - \veps \Bigr) \\
    & = 2 \, \P \,\Bigl( M_n^{(2)}( \mathmakebox[\widthof{$\bTheta_{\vbeta}$}][l]{ \bTheta_{c} }, \bXi_{\bu}) \le M^*(\bTheta_{c}) - \veps \Bigr) \xrightarrow{n \to \infty} 0.
\end{align*}
If both $\bTheta_{\vbeta}$ and $\bXi_{\bu}$ are also convex, then we can similarly show that
\begin{equation*}
    \P \, \Bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \ge M^*(\bTheta_{c}) + \veps \Bigr) 
    \le 2 \, \P \,\Bigl( M_n^{(2)}( \mathmakebox[\widthof{$\bTheta_{\vbeta}$}][l]{ \bTheta_{c} }, \bXi_{\bu}) \ge M^*(\bTheta_{c}) + \veps \Bigr) \xrightarrow{n \to \infty} 0.
\end{equation*}
Combining these implies $M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \conp M^*(\bTheta_{c})$, which concludes the proof.
\end{proof}

\paragraph{Parameter convergence}

Next, we define $M^* := M^*( [-1, 1] \times \R_{\ge 0} \times \R )$ to be the unconstrained optimization problem \cref{eq:logistic_variation}, i.e.,
\begin{equation*}
    M^{*}
    = \min_{ \substack{ \rho \in [-1, 1], R \ge 0, \beta_0 \in \R \\ \xi \in \cL^2(\Q_\infty), \norm{\xi}_{\Q_\infty} \le 1/\sqrt{\delta} } } 
    \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right].
\end{equation*}
An analysis of the Karush--Kuhn--Tucker (KKT) conditions shows that $M^*$ has the unique solution $(\rho^*, R^*, \beta_0^*, \xi^*)$, with $\rho^* \in (0, 1)$, $R^* \in (0, \infty)$, and $\beta_0^* \in (-\infty, \infty)$. Combined with \cref{thm:ERM_conv}, it implies $M_n \conp M^*$, which leads to the convergence of parameters:
\begin{lem}[Parameter convergence] 
\label{lem:ERM_param_conv}
As $n, d \to \infty$, we have $M_n \conp M^*$, which implies
    \begin{equation*}
            \| \hat\vbeta_n \|_2 \conp R^*,
            \qquad
            \hat\rho_n = \biggl\< \frac{\hat \vbeta_n}{\| \hat \vbeta_n \|_2}, \frac{\vmu}{\| \vmu \|_2} \biggr\> \conp \rho^*,
            \qquad
            \hat\beta_{0,n} \conp \beta_0^*.
    \end{equation*}
\end{lem}
\noindent
See \cref{append_subsubsec:ERM_param} for the proof.

\paragraph{ELD convergence} Finally, to establish the ELD convergence, we use a proof strategy similar to that in \cref{lem:over_logit_conv} by first defining the following measures
\begin{align*}
    \hat \cL_{n} := \frac1n \sum_{i=1}^n \delta_{y_i ( \< \xx_i, \hat\vbeta \> + \hat\beta_{0} ) }, 
    \quad
    \cL_* := \Law \, (U^*)
    = \Law \, \bigl( \rho^*\norm{\vmu}_2 R^* + R^* G + \beta_0^* Y + R^* \sqrt{1 - \rho^*{}^2} \xi^* \bigr).
\end{align*}
Let $\mathsf{B}_{W_2}(\varepsilon)$ ($\varepsilon > 0$) be the $\varepsilon$-$W_2$ ball at $\cL_*$, i.e.,
\begin{equation*}
    \mathsf{B}_{W_2}(\varepsilon) := \left\{ \bu \in \R^n:   W_2 \biggl( 
        \frac{1}{n}\sum_{i=1}^n \delta_{u_i}, \cL_*
     \biggr)  < \varepsilon \right\}.
\end{equation*}
Then by showing that
\begin{equation*}
    \lim_{n \to \infty} \P \left( M_n(\R^{d + 1}, \mathsf{B}_{W_2}^c(\varepsilon) ) > M_n \right) = 1,
\end{equation*}
we can prove the convergence of logit margins $W_2( \hat \cL_{n}, \cL_* ) \conp 0$, and hence the ELD convergence. The result in summarized in the following lemma.

\begin{lem}[ELD convergence]
\label{lem:ERM_logit_conv}
    As $n, d \to \infty$, we have $W_2( \hat \cL_{n}, \cL_* ) \conp 0$ and $W_2 ( \hat \nu_{n}, \nu_* ) \conp 0$.
\end{lem}
\noindent
See \cref{append_subsubsec:ERM_logit} for the proof.


\subsubsection{Step 1 --- Boundedness of $\vbeta$ and $\beta_0$: Proof of \cref{lem:ERM_bound_beta}}
\label{subsubsec:under_step1}

\begin{proof}[\textbf{Proof of \cref{lem:ERM_bound_beta}}]
    % If $\hat\vbeta = \bzero$, then we have $\hat R_n(\bzero, \beta_0) = (n_+/n) \ell(\beta_0) + (n_-/n) \ell(-\beta_0) \to \pi \ell(\beta_0) + (1- \pi) \ell(-\beta_0)$ as $n \to \infty$ by LLN. Note that $\ell(-\infty) = +\infty$ by \cref{lem:ell}, then $|\hat\beta_0|$ must be bounded with high probability.
    We first assume $\hat\vbeta \not= \bzero$. By \cref{thm:SVM_main}\ref{thm:SVM_main_mar}, if $\delta > \delta^*(0)$, there exists $k \in [n]$ and constant $\overline{\kappa} > 0$, such that
    \begin{equation}
        \label{eq:neg_k_logit}
        y_k \biggl(  \biggl\< \xx_k, \frac{\hat\bbeta}{\|\hat\bbeta\|_2} \biggr\> + \frac{\hat\beta_0}{\|\hat\bbeta\|_2} \biggr) \le - \overline{\kappa}
    \end{equation}
    holds with high probability. Therefore, we have
    \begin{equation*}
            \ell(0) 
               \overset{\mathmakebox[0pt][c]{\smash{\text{(i)}}}}{\ge}
                \frac1n \sum_{i=1}^n \ell\bigl( 
                y_i(\< \xx_i, \hat\bbeta \> +  \hat\beta_0 ) \bigr)
               \overset{\mathmakebox[0pt][c]{\smash{\text{(ii)}}}}{\ge}
                \frac1n \ell\bigl( 
                y_k(\< \xx_k, \hat\bbeta \> +  \hat\beta_0 ) \bigr)
               \overset{\mathmakebox[0pt][c]{\smash{\text{(iii)}}}}{\ge}
                \frac1n \ell( - \overline{\kappa} \|\hat\bbeta\|_2 ),
    \end{equation*}
    where in (i) we note that $\hat R_n(\bzero, 0) \ge \hat R_n(\hat\bbeta, \hat\beta_0) = M_n$, in (ii) we use $\ell \ge 0$, and in (iii) we use \eqref{eq:neg_k_logit}. Clearly the above inequalities also hold for $\hat\vbeta = \bzero$. Notice that $\frac1n \ell( - \overline{\kappa} \|\hat\bbeta\|_2 ) \to +\infty$ as $\|\hat\bbeta\|_2 \to \infty$, which contradicts $\ell(0) < +\infty$. Hence, it implies $\|\hat\bbeta\|_2$ is bounded with high probability.

    Meanwhile, let $j, k \in [n]$ be any two indices $y_j = +1$, $y_k = -1$. Then as $\hat\beta_0 \to \pm\infty$, we have
    \begin{equation*}
        \ell(0) \ge \frac1n \sum_{i=1}^n \ell\bigl( 
            y_i(\< \xx_i, \hat\bbeta \> +  \hat\beta_0 ) \bigr)
        \ge
        \frac1n \ell\bigl( \< \xx_j, \hat\bbeta \> +  \hat\beta_0 \bigr)
        +
        \frac1n \ell\bigl( - \< \xx_k, \hat\bbeta \> - \hat\beta_0 \bigr)
        \to +\infty,
    \end{equation*}
    which leads to a contradiction. So $|\hat\beta_0|$ is also bounded with high probability.

    Finally, in the minimax representation of $M_n$, the optimal $\bu$ must satisfy $u_i = y_i(\< \xx_i, \hat\bbeta \> +  \hat\beta_0 )$ for all $i \in [n]$. Therefore, according to the tail bound of Gaussian matrices \cite[Corollary 7.3.3]{vershynin2018high},
    \begin{align*}
            \norm{\bu}_2 & = \| \yy \odot (\XX \hat\bbeta + \hat\beta_0 \bone_n ) \|_2
            = \| \< \bmu, \hat\bbeta \> \bone_n + \ZZ \hat\bbeta + \hat\beta_0 \yy \|_2 \\
            & \le \sqrt{n} \| \bmu \|_2 \| \hat\bbeta \|_2 +
            \| \ZZ \|_{\mathrm{op}} \| \hat\bbeta \|_2 + \sqrt{n} | \hat\beta_0 | \\
            & \le \sqrt{n} \| \bmu \|_2 \| C_{\bbeta} + \bigl( \sqrt{n}(1 + o(1)) + \sqrt{d} \bigr) C_{\bbeta} 
            + \sqrt{n} C_{\beta_0} \\
            & \le \sqrt{n} C_{\bu}
    \end{align*}
    with high probability, where $C_{\bu} > 0$ is some constant. This completes the proof.
\end{proof}






\subsubsection{Step 2 --- Reduction via Gaussian comparison: Proof of \cref{lem:ERM_CGMT}}
\label{subsubsec:under_step2}


\begin{proof}[\textbf{Proof of \cref{lem:ERM_CGMT}}]
    For $m \in \mathbb{N}_+$, denote $K_m = \{ \bv \in \R^n: \norm{\bv}_2 \le m \}$, and define
    \begin{align*}
        M_n (\bTheta_{\vbeta}, \bXi_{\bu}; K_m)
        & :=  \!\min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } } \max_{ \bv \in K_m } \left\{
        \frac1n \sum_{i=1}^n \ell( u_i )
      + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
      + \frac1n \bv^\top \ZZ \vbeta + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu \right\} \! , \!\! 
      \\
        M_n^{(1)} (\bTheta_{\vbeta}, \bXi_{\bu}; K_m) 
        & :=  \!\min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
        \max_{ \bv \in K_m }
        \, \Biggl\{
        \frac1n \sum_{i=1}^n \ell( u_i )
        + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
         + \frac1n \norm{\bv}_2 \hh^\top \vbeta + \frac1n \norm{\vbeta}_2 \vg^\top \bv 
         \\
         &
         \phantom{:=  \!\min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
        \max_{ \bv \in K_m }
        \, \Biggl\{}
         + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu
         \Biggr\}.
    \end{align*}
    We first show that
    \begin{equation*}
        \lim_{m \to \infty} M_n (\bTheta_{\vbeta}, \bXi_{\bu}; K_m) = M_n (\bTheta_{\vbeta}, \bXi_{\bu}).
    \end{equation*}
    To this end, note that for any fixed $(\bbeta , \beta_0, \bu)$, by Cauchy--Schwarz inequality we have
    \begin{align}
        & \max_{ \bv \in K_m } \left\{
        \frac1n \sum_{i=1}^n \ell( u_i )
        + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
        + \frac1n \bv^\top \ZZ \vbeta + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu \right\} 
        \notag \\
        = {} & \frac1n \sum_{i=1}^n \ell( u_i ) + \frac{m}{n} \bigl\|\bu - \< \bmu, \vbeta \> \bone - \ZZ \vbeta - \beta_0 \yy \bigr\|_2.
        \label{eq:ell_Mn_ineq}
    \end{align}
    Let $(\bbeta_*^{(m)}, \beta_{0, *}^{(m)}, \bu_*^{(m)})$ be the minimizer of $M_n (\bTheta_{\vbeta}, \bXi_{\bu}; K_m)$. Since $\ell \ge 0$, we know that
    \begin{align*}
        & \frac{m}{n} \norm{\bu_*^{(m)} - \< \bmu, \vbeta_*^{(m)} \> \bone - \ZZ \vbeta_*^{(m)} - \beta_{0, *}^{(m)} \yy}_2 \le \, M_n (\bTheta_{\vbeta}, \bXi_{\bu}; K_m) \le M_n (\bTheta_{\vbeta}, \bXi_{\bu}) \\
        \implies \  & \frac{ \mathmakebox[\widthof{$m$}][c]{1} }{n} \norm{\bu_*^{(m)} - \< \bmu, \vbeta_*^{(m)} \> \bone - \ZZ \vbeta_*^{(m)} - \beta_{0, *}^{(m)} \yy}_2 \le \, \frac{1}{m} M_n (\bTheta_{\vbeta}, \bXi_{\bu}).
    \end{align*}
    Let $\bu' := \< \bmu, \vbeta_*^{(m)} \> \bone + \ZZ \vbeta_*^{(m)} + \beta_{0, *}^{(m)} \yy$, then we have
    \begin{equation}\label{eq:u_diff_Mn}
        \frac{1}{n} \, \bigl\| \bu_*^{(m)} - \bu' \bigr\|_2 \le \, \frac{1}{m} M_n (\bTheta_{\vbeta}, \bXi_{\bu}),
    \end{equation}
    which implies that ($\bu_*^{(m)} = (u_{*, 1}^{(m)}, \ldots, u_{*, n}^{(m)})^\top$, $\bu' = (u'_1, \ldots, u'_n)^\top$)
    \begin{align*}
        M_n (\bTheta_{\vbeta}, \bXi_{\bu}) 
        & = \min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
        \left\{
        \frac1n \sum_{i=1}^n \ell( u_i )  \,\bigg|\,
         \< \bmu, \vbeta \> +  \< \zz_i, \vbeta \> + y_i \beta_0 - u_i = 0, \forall\, i \in [n]
         \right\}
        \\
        & \le
        \frac1n \sum_{i=1}^n \ell( u'_i ) \le \frac1n \sum_{i=1}^n \ell( u_{*, i}^{(m)} ) + \frac{1}{n} \sum_{i=1}^{n} \left\vert \ell( u_{*, i}^{(m)} ) - \ell( u'_i ) \right\vert 
        \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\le} 
        \frac1n \sum_{i=1}^n \ell( u_{*, i}^{(m)} ) + \frac{C_L}{n} \bigl\| \bu_*^{(m)} - \bu' \bigr\|_1
        \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le} 
        \frac1n \sum_{i=1}^n \ell( u_{*, i}^{(m)} ) + O_m \left( \frac{1}{m} \right) 
        \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{\le}
        M_n (\bTheta_{\vbeta}, \bXi_{\bu}; K_m) + O_m \left( \frac{1}{m} \right),
    \end{align*}
    where (i) follows from the pseudo-Lipschitzness of $\ell$, the compactness of $\bXi_{\bu}$, and $C_L > 0$ is some constant, (ii) follows from \cref{eq:u_diff_Mn}, while (iii) follows from \cref{eq:ell_Mn_ineq}.
    This proves that
    \begin{equation*}
        \lim_{m \to \infty} M_n (\bTheta_{\vbeta}, \bXi_{\bu}; K_m) = M_n (\bTheta_{\vbeta}, \bXi_{\bu}).
    \end{equation*}
    Similarly, one can show that
    \begin{equation*}
        \lim_{m \to \infty} M_n^{(1)} (\bTheta_{\vbeta}, \bXi_{\bu}; K_m) = M_n^{(1)} (\bTheta_{\vbeta}, \bXi_{\bu}).
    \end{equation*}
    Now for any fixed $m$, applying \cref{lem:CGMT}\ref{lem:CGMT(a)} yields that $\forall\, t \in \R$:
    \begin{equation*}
        \P \, \Bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu}; K_m) \le t \Bigr) 
        \le 2 \, \P \, \Bigl( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu}; K_m) \le t \Bigr),
    \end{equation*}
    thus leading to \cref{eq:unbounded_CGMT_1} (by continuity and using the two limits above)
    \begin{align*}
        & \P \left( M_n(\bTheta_{\vbeta}, \bXi_{\bu}) \le t \right) =  
        \lim_{m \to \infty} \P \left( M_n(\bTheta_{\vbeta}, \bXi_{\bu}; K_m) \le t \right) \\
        \le {} & 2 \lim_{m \to \infty} \P \left( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu}; K_m) \le t \right)
        =  2 \, \P \left( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu}) \le t \right).
    \end{align*}
    Further, if $\bTheta_{\vbeta}$ and $\bXi_{\bu}$ are convex, \Cref{lem:CGMT}(b) implies that
    \begin{equation*}
        \P \, \Bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu}; K_m) \ge t \Bigr) 
        \le 2 \, \P \, \Bigl( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu}; K_m) \ge t \Bigr).
    \end{equation*}
    Sending $m \to \infty$ similarly proves the other inequality \cref{eq:unbounded_CGMT_2}.
\end{proof}




\begin{comment}
{Kangjie: Under our additional assumption, this lemma is not needed.}
It turns out $M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) \approx M_n^{(3)}(\bTheta_{c}, \bXi_{\bu})$. To elaborate, we need the following results.
\begin{lem}\label{lem:min_continous}
    Let $w \in \cL^2(\Q_n)$ be a fixed function. For any $c > 0$, define the $\Q_n$-ball
    \begin{equation*}
        \mathcal{B}(c) := \left\{  u \in \cL^2(\Q_n):  \norm{u - w}_{\Q_n} \le c \right\}.
    \end{equation*}
    Then for any closed set $\Xi_{u} \subseteq \cL^2(\Q_n)$, the function
    \begin{equation*}
        \zeta(c) := \min_{ u \in \Xi_{u}  \cap   \mathcal{B}(c)  }
        \E_{\Q_n}[\ell(u)]
    \end{equation*}
    is continuous in $c$.
\end{lem}
\begin{proof}
    Denote $\cM(c) := \Xi_{u}  \cap  \mathcal{B}(c)$. By viewing $\cM: \R_{> 0} \rightrightarrows \cL^2(\Q_n)$ as a correspondence between topological spaces, we can use Berge's Maximum Theorem \cite{???} to prove continuity. Then it suffices to establish the following facts. Let $c > 0$ be a fixed constant.
    \begin{itemize}
        \item The function $u \mapsto \E_{\Q_n}[\ell(u)]$ is continuous, due to continuity of $\ell$ and definition of $\Q_n$.
        \item $\cM$ is upper hemicontinuous (u.h.c.).
        
        We first prove $\mathcal{B}$ is u.h.c.. By sequential characterization, for any sequence $\{ c_m \}_{m=1}^\infty$ in $\R_{> 0}$ and $\{ u_m \}_{m=1}^\infty$ in $\mathcal{B}(c)$, such that $u_m \in \mathcal{B}(c_m)$. If $c_m \to c$ and $u_m \to u$ as $m \to \infty$, we have
        \begin{equation*}
            \norm{u - w}_{\Q_n} = \lim_{m \to \infty} \norm{u_m - w}_{\Q_n} \le \lim_{m \to \infty} c_m = c,
        \end{equation*}
        where the first equality is from continuity of $\norm{\,\cdot\,}_{\Q_n}$, and the inequality is from $u_m \in \mathcal{B}(c_m)$. This shows $u \in \mathcal{B}(c)$ and hence $\mathcal{B}$ is u.h.c.. Since $\cM$ is a closed subcorrespondence of $\mathcal{B}$, by \cite{???} $\cM$ is also u.h.c..

        \item $\cM$ is lower hemicontinuous (l.h.c.).
        
        By definition, consider any open set $U$ such that $U \cap \cM(c) \neq \varnothing$. Since $\cM(c)$ is compact, let $u_0 \in U \cap \cM(c)$ be an interior point. Then for any $c' > 0$ such that $\norm{u_0 - w}_{\Q_n} =: c_0 < c' < c$, we have $u_0 \in \cM(c')$. This implies $U \cap \cM(c) \neq \varnothing$ for any $c' \in (c_0, \infty)$, since $\cM(c_1) \subseteq \cM(c_2)$ for any $c_1 \le c_2$. Therefore, $U$ intersects with some neighborhood of $\cM(c)$, i.e., $\cM$ is l.h.c..
    \end{itemize}
    Hence, $\cM$ is a continuous correspondence. By Maximum Theorem, $c \mapsto \zeta(c)$ is continuous.
\end{proof}
\end{comment}



\subsubsection{Step 3 --- Convergence in variational forms: Proof of \cref{lem:M2-3}}
\label{subsubsec:under_step3}

\begin{proof}[\textbf{Proof of \cref{lem:M2-3}}]
    First, by definition of $M_n^{(2)}$ and $M_n^{(3)}$:
    \begin{equation*}
        \abs{ M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) - M_n^{(3)}(\bTheta_{c}, \bXi_{\bu}) } \le \, \sup_{(\rho, R, \beta_0) \in \bTheta_{c}}
            \abs{ 
            \min_{  
                U \in \Xi_{u}  \cap   \mathcal{N}_n  }
                \E_{\Q_n}[\ell(U)]    
        -
        \min_{  
        U \in \Xi_{u}  \cap   \mathcal{N}^\delta_n  }
        \E_{\Q_n}[\ell(U)]
        }.
    \end{equation*}
    For any fixed $(\rho, R, \beta_0) \in \bTheta_{c}$, by definition of $\mathcal{N}_n$ in \cref{eq:set_N_n} and $\mathcal{N}^\delta_n$ in \cref{eq:set_N_n_delta}, we have
    \begin{equation*}
        \left\vert \min_{U \in \Xi_{u} \cap \mathcal{N}_n} \E_{\Q_n}[\ell(U)] - \min_{U \in \Xi_{u} \cap \mathcal{N}^\delta_n}
        \E_{\Q_n}[\ell(U)] \right\vert
        \le  
        \max_{\substack{ U, U' \in \Xi_u \cap \mathcal{N}_n \cap \mathcal{N}^\delta_n \\ 
        \| U - U' \|_{\Q_n} \le \veps_n (\rho, R, \beta_0)}} \left\vert \E_{\Q_n} [\ell(U)] - \E_{\Q_n} [\ell(U')] \right\vert,
    \end{equation*}
    where
    \begin{equation*}
        \veps_n (\rho, R, \beta_0) := \left\vert \frac{R}{\sqrt{n}} \biggl( \sqrt{1 - \rho^2}  \| \bP_{\bmu}^\perp \hh \|_2  -  \rho\frac{\hh^\top \vmu}{\norm{\vmu}_2}  \biggr) - R \frac{\sqrt{1 - \rho^2}}{\sqrt{\delta}} \right\vert.
    \end{equation*}
    By our assumption that $\ell$ is pseudo-Lipschitz, the following estimate holds:
    \begin{align*}
        \left\vert \E_{\Q_n} [\ell(U)] - \E_{\Q_n} [\ell(U')] \right\vert 
        & \le 
        \frac{1}{n} \sum_{i=1}^{n} \vert \ell(u_i) - \ell(u'_i) \vert 
        \le \frac{L}{n} \sum_{i=1}^{n} (1 + \vert u_i \vert + \vert u'_i \vert ) \vert u_i - u'_i \vert \\
        & \stackrel{\text{(i)}}{\le}  L \left( 1 + \|U\|_{\Q_n} + \|U'\|_{\Q_n} \right) \|U - U'\|_{\Q_n} 
        \stackrel{\text{(ii)}}{\le}
        C ( 1 + o_{\P}(1) ) \, \veps_n (\rho, R, \beta_0) ,
    \end{align*}
    where (i) follows from Cauchy--Schwarz inequality, (ii) follows from the compactness of $\mathcal{N}_n^\delta$ and $\bTheta_{c}$, and the upper bound below:
    \begin{align*}
        \norm{U}_{\Q_n} & \le  \sup_{(\rho, R, \beta_0) \in \bTheta_{c}} \bigl\| \rho\norm{\vmu}_2 R + R G + \beta_0 Y \bigr\|_{\Q_n}
        + \frac{R \sqrt{1 - \rho^2}}{\sqrt{\delta}} 
        \\
        & \le \rho\norm{\vmu}_2 R_{\max} + R_{\max} \norm{G}_{\Q_n} + B_{0, \max} \norm{Y}_{\Q_n} + \frac{R_{\max}}{\sqrt{\delta}} \\
        & \overset{\mathmakebox[0pt][c]{\smash{\text{($*$)}}}}{=} 
        \rho\norm{\vmu}_2 R_{\max} + R_{\max} \bigl( 1 + o_{\P}(1) \bigr) + B_{0, \max} + \frac{R_{\max}}{\sqrt{\delta}},
    \end{align*}
    by denoting $R_{\max} := \max_{(\rho, R, \beta_0) \in \bTheta_{c}} R$, $B_{0, \max} := \max_{(\rho, R, \beta_0) \in \bTheta_{c}} \abs{\beta_0}$, and $C > 0$ is some constant. Here, ($*$) is from the law of large numbers: $\norm{G}_{\Q_n} \conp \norm{G}_{\Q_\infty} = (\E[G^2])^{1/2} = 1$. Combining these estimates, we finally deduce that
    \begin{align*}
        & \abs{M_n^{(2)}(\bTheta_{c}, \bXi_{\bu}) - M_n^{(3)}(\bTheta_{c}, \bXi_{\bu})} 
        \le  C ( 1 + o_{\P}(1) ) \max_{(\rho, R, \beta_0) \in \bTheta_{c}} \veps_n (\rho, R, \beta_0) \\
        = {} & 
        C ( 1 + o_{\P}(1) ) \max_{(\rho, R, \beta_0) \in \bTheta_{c}} \left\vert \frac{R}{\sqrt{n}} \biggl( \sqrt{1 - \rho^2}  \| \bP_{\bmu}^\perp \hh \|_2  -  \rho\frac{\hh^\top \vmu}{\norm{\vmu}_2}  \biggr) - R \frac{\sqrt{1 - \rho^2}}{\sqrt{\delta}} \right\vert \\
        \le {} & 
        C ( 1 + o_{\P}(1) ) \cdot R_{\max} \left( \left\vert \frac{1}{\sqrt{n}} \| \bP_{\bmu}^\perp \hh \|_2 - \frac{1}{\sqrt{\delta}} \right\vert + \frac{1}{\sqrt{n}} \frac{\vert \hh^\top \vmu \vert}{\norm{\vmu}_2} \right) \conp 0.
    \end{align*}
    The convergence in the last line follows from
\begin{equation*}
    \frac{\| \bP_{\bmu}^\perp \hh \|_2}{\sqrt{n}} = \frac{\| \bP_{\bmu}^\perp \hh \|_2}{\| \bP_{\bmu}^\perp \|_\mathrm{F}}
    \cdot \frac{\sqrt{d-1}}{\sqrt{n}}
    \conp \frac{1}{\sqrt{\delta}},
    \qquad
    \frac{ \hh^\top \vmu }{ \sqrt{n} \norm{\vmu}_2 } \conp 0,
\end{equation*}
according to \cref{lem:subG_concentrate}\ref{lem:subG-Hanson-Wright-II}\ref{lem:subG-Hoeffding} and $\|\bP_{\bmu}^\perp \|_\mathrm{op} = 1$, $\| \bP_{\bmu}^\perp \|_\mathrm{F} = \sqrt{d - 1}$. This completes the proof.
\end{proof}


\subsubsection{Step 4 --- Asymptotic characterization: Proofs of \cref{lem:M3-star}, \ref{lem:var_fixed}}
\label{subsubsec:under_step4}

We need the following auxiliary result, which studies a general variational problem for both $\Q = \Q_n$ and $\Q = \Q_\infty$ with parameters $(\rho, R, \beta_0)$ fixed. In particular, we are able to express the random variable $\xi$ by $(\rho, R, \beta_0)$, $(G, Y)$, and an additional scalar (Lagrange multiplier). Then, we can rewrite $M_n^{(3)}(\bTheta_{c})$, $M^{*}(\bTheta_{c})$ as low-dimensional convex-concave minimax problems.

\begin{lem}\label{lem:var_fixed}
    For any fixed parameters $\rho \in (-1, 1)$, $R > 0$, $\beta_0 \in \R$, and the probability measure $\Q = \Q_n$ or $\Q = \Q_\infty$, consider the following variational problem
    \begin{equation}
    \label{eq:ERM_var_fix}
        \zeta_{\rho, R, \beta_0}(\Q) :=
        \min_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le 1/\delta} \mathscr{R}_{\Q}(\xi),
        \quad
        \mathscr{R}_{\Q}(\xi) := \E_{\Q} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right] \! .
    \end{equation}
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:var_fixed(a)}
        $\mathscr{R}_{\Q}(\xi)$ has a unique minimizer $\xi^* := \xi^*_{\Q}(\rho, R, \beta_0)$, which must satisfy
        \begin{equation}\label{eq:xi_star}
            \xi^*_{\Q}(\rho, R, \beta_0) = - \frac{\lambda^*}{R \sqrt{1 - \rho^2}}\ell'\bigl(\prox_{ \lambda^* \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y )\bigr),
        \end{equation}
        where $\lambda^*$ is the unique solution such that $\norm{\xi^*}_\Q^2 = 1/\delta$.
        % {\color{blue} (logit) As a consequence,
        % \[ 
        % \begin{aligned}
        %     U^*(\rho, R, \beta_0)
        %     & := \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi^*_{\Q}(\rho, R, \beta_0) \\
        %     &  \phantom{:}= \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ).
        % \end{aligned}
        % \]
        % }
        As a consequence, we have
        \begin{equation*}
            \zeta_{\rho, R, \beta_0}(\Q)
                = 
            \E_{\Q} \left[ \ell \bigl( \prox_{ \lambda^* \ell}( \rho \norm{\vmu}_2 R + R G + \beta_0 Y )
                \bigr) \right],
        \end{equation*}
        where $\prox_{\lambda^*\ell}$ and $\envelope_{\lambda^*\ell}$ are the proximal operator and Moreau envelope of $\ell$ defined in \cref{append_subsec_Moreau}. Moreover, $\lambda^*$ is a decreasing function of $\delta$.
        \item \label{lem:var_fixed(b)}
        With change of variables $A := R \rho$, $B := R \sqrt{1 - \rho^2}$, the variational problem \cref{eq:ERM_var_fix} can be recast as $\zeta_{\rho, R, \beta_0}(\Q) = \sup_{\nu > 0} \mathscr{R}_{\nu, \Q}(A, B, \beta_0)$, where
        \begin{equation*}
            \mathscr{R}_{\nu, \Q}(A, B, \beta_0) 
            :=   - \frac{B \nu}{2 \delta }
            +
            \E_{\Q} \left[ \envelope_{\ell} \Bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ; \frac{B}{\nu} \Bigr) \right], 
        \end{equation*}
        and $(Y, G_1, G_2) \sim P_y \times \normal(0, 1) \times \normal(0, 1)$ under $\Q = \Q_\infty$.\footnote{According to the change of variables, we have relation $A G_1 + B G_2 \overset{\mathrm{d}}{=} R G$ under $\Q = \Q_\infty$. We can also construct the realizations $\{G_1(g_i, y_i), G_2(g_i, y_i)\}_{i=1}^n$ such that $A G_1 + B G_2 = R G$, $\Q_n$-a.s., that is, $A G_1(g_i, y_i) + B G_2(g_i, y_i) = R G(g_i, y_i)$, for all $i \in [n]$.} Moreover, $\mathscr{R}_{\nu, \Q}(A, B, \beta_0)$ is convex in $(A, B, \beta_0)$ over $\R_{>0} \times \R_{>0} \times \R$ and concave in $\nu$.
        
        % \item The function $c \mapsto \zeta(c)$ is continuous.
    \end{enumerate}
\end{lem}
\begin{proof}
    For \textbf{\ref{lem:var_fixed(a)}}, we first show the existence of a minimizer. The proof is a standard application of direct method in calculus of variations. Since $\ell$ is lower bounded, we know that
        \begin{equation*}
            \inf_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le 1/\delta} \mathscr{R}_{\Q}(\xi) > - \infty.
        \end{equation*}
        Let $\{ \xi_m \}_{m \in \mathbb{N}} \in \cL^2 (\Q)$ be a minimizing sequence such that $\norm{\xi_m}_{\Q}^2 \le 1/\delta$, and
        \begin{equation*}
            \lim_{m \to \infty} \mathscr{R}_{\Q}(\xi_m) = \inf_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le 1/\delta} \mathscr{R}_{\Q}(\xi).
        \end{equation*}
        Since $\cL^2 (\Q)$ is a Hilbert space (and hence self-reflexive), Banach-Alaoglu theorem implies that $\{ \xi_m \}$ has a weak-* convergent (and hence weak convergent) subsequence, which we still denote as $\{ \xi_m \}$. Let $\xi^*$ denote the weak limit of $\{ \xi_m \}$. By using Mazur's lemma, we know that there exists another sequence $\{ \xi'_m \}_{m \in \mathbb{N}}$, such that each $\xi'_m$ is a finite convex combination of $\{ \xi_k \}_{m \le k \le m + N(m)}$ ($N(m) \ge 0$ depends on $m$), and that $\xi'_m$ strongly converges to $\xi^*$. Now since $\mathscr{R}_{\Q}$ is convex (this follows from convexity of $\ell$ and the fact that integration $\E_{\Q}$ preserves convexity), we have
        \begin{equation*}
            \liminf_{m \to \infty} \mathscr{R}_{\Q} (\xi'_m) \le \liminf_{m \to \infty} \mathscr{R}_{\Q} (\xi_m) = \inf_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le 1/\delta} \mathscr{R}_{\Q}(\xi).
        \end{equation*}
        On the other hand, Fatou's lemma implies that
        \begin{equation*}
            \mathscr{R}_{\Q}(\xi^*) \le \liminf_{m \to \infty} \mathscr{R}_{\Q}(\xi'_m).
        \end{equation*}
        This immediately leads to
        \begin{equation*}
            \mathscr{R}_{\Q}(\xi^*) = \inf_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le 1/\delta} \mathscr{R}_{\Q}(\xi),
        \end{equation*}
        i.e., $\xi^*$ is a minimizer of $\mathscr{R}_{\Q}$.
        In order to prove uniqueness of the minimizer, we will show that $\mathscr{R}_{\Q} : \cL^2(\Q) \to \R_{> 0}$ is strictly convex. For any $\alpha \in (0, 1)$ and $\xi_1, \xi_2 \in \cL^2(\Q)$, with a shorthand $V := \rho\norm{\vmu}_2 R + RG + \beta_0 Y$, we notice that
    \begin{align*}
            & \mathscr{R}_{\Q}( \alpha \xi_1 + (1 - \alpha) \xi_2 )
            \\
            = {} & \E_{\Q} \left[  \ell \Bigl( \alpha \bigl(  V + R\sqrt{1 - \rho^2} \xi_1 \bigr) +  
            (1 - \alpha)\bigl( V + R\sqrt{1 - \rho^2} \xi_2 \bigr)
            \Bigr) \right] \\
            \le {} & \E_{\Q} \left[ \alpha \ell \bigl(  V + R\sqrt{1 - \rho^2} \xi_1 \bigr) +  
            (1 - \alpha) \ell\bigl( V + R\sqrt{1 - \rho^2} \xi_2 \bigr)
             \right] 
            = 
            \alpha \mathscr{R}_{\Q}( \xi_1 ) + (1 - \alpha) \mathscr{R}_{\Q}( \xi_2 ),
    \end{align*}
    where the inequality follows from strong convexity of $\ell$, and it becomes equality if and only if $\Q(\xi_1 \not= \xi_2) = 0$. Hence we conclude $\mathscr{R}_{\Q}$ is strictly convex. Since $\{ \xi: \norm{\xi}_{\Q}^2 \le 1/\delta \}$ is a convex set, it implies the uniqueness ($\Q$-a.s.) of the minimizer $\xi^*$.

    As a consequence, the unique minimizer is determined by the Karush--Kuhn--Tucker (KKT) and Slater's conditions for variational problems \cite[Theorem 2.9.2]{zalinescu2002convex}. $\xi$ is the minimizer if and only if, for some scalar $\nu$ (dual variable), the followings hold:
    \begin{equation}\label{eq:KKT_xi}
        \begin{aligned}
            U = \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi,
            \qquad
            \ell' ( U ) + \nu \xi = 0, \\
            \norm{\xi}_{\Q}^2 - \delta^{-1} \le 0,
            \qquad
            \nu \ge 0,
            \qquad
            \nu(\norm{\xi}_{\Q}^2 - \delta^{-1}) = 0.
        \end{aligned}
    \end{equation}
    We claim that the KKT conditions imply that any minimizer $\xi$ and its associated dual variable $\nu$ must satisfy
    \begin{equation*}
        0 < \nu < \infty,  \qquad  \xi > 0 \ \  (\text{$\Q$-a.s.}), \qquad \norm{\xi}_{\Q}^2 = \delta^{-1}.
    \end{equation*}
    To show this, we notice that $R\sqrt{1 - \rho^2} > 0$ and $\ell$ is decreasing. Therefore, for any $\xi \in \cL^2(\Q)$, $\mathscr{R}_{\Q}(\xi) \ge \mathscr{R}_{\Q}(\abs{\xi})$. It implies that $\xi \ge 0$ if $\xi$ is the minimizer. Hence, by stationarity in \cref{eq:KKT_xi}: $\nu\xi = - \ell' ( U ) > 0$, which implies the positivity of $\nu$, $\xi$. Then $\norm{\xi}_{\Q}^2 = \delta^{-1}$ comes from complementary slackness in \cref{eq:KKT_xi}. To show $\nu$ must be finite, notice that $\nu \to +\infty$ implies $\ell'(U) \to -\infty$. Then $U \to -\infty$ since $\ell'$ is strictly increasing, while it contradicts $\xi > 0$ and $\norm{\xi}_{\Q}^2 = \delta^{-1}$.

    By change of variable $\lambda := R \sqrt{1 - \rho^2}/\nu$, now we can rewrite KKT conditions \cref{eq:KKT_xi} as
    \begin{equation}\label{eq:KKT_xi2}
            U + \lambda \ell' ( U ) = \rho\norm{\vmu}_2 R + RG + \beta_0 Y,
            \qquad
            0 < \lambda < \infty,
            \qquad
            \norm{\xi}_{\Q}^2 = \delta^{-1},
    \end{equation}
    where $\xi$ and $U$ are related by
    \begin{equation}\label{eq:xi-U}
        \xi = -\frac{\lambda}{R\sqrt{1 - \rho^2}} \ell'(U).
    \end{equation}
    Notice that \cref{eq:KKT_xi2} has a unique solution for $U$, since $x \mapsto x + \lambda \ell'(x)$ is a strictly increasing continuous function from $\R$ to $\R$, for any $\lambda \in (0, \infty)$. Then, according to \cref{lem:prox}, $U$ can be expressed by the proximal operator of $\ell$,
    \begin{equation}
    \label{eq:U_prox}
        U = \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ).
    \end{equation}
    Combine it with \cref{eq:xi-U} gives the expression of $\xi^*$ in \cref{eq:xi_star}. To establish the uniqueness of $\lambda$, we show that $\nu$ satisfying \cref{eq:KKT_xi} must be unique. Note that $\xi = \xi (\nu)$ is determined by
    \begin{equation*}
        \nu \xi(\nu) + \ell' \left( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi (\nu) \right) = \, 0.
    \end{equation*}
    Since $\nu, \xi(\nu) > 0$ and $\ell'$ is strictly increasing (by strong convexity), we know that $\xi(\nu)$ is strictly decreasing in $\nu$. The uniqueness of $\nu$ immediately follows from the condition $\norm{\xi(\nu)}_{\Q}^2 = \delta^{-1}$. This also implies that $\xi(\nu) > 0$ is decreasing in $\delta$. Then we conclude $\nu$ is increasing in $\delta$, or equivalently $\lambda$ is decreasing in $\delta$. This completes the proof of part \ref{lem:var_fixed(a)}.


    \vspace{0.5\baselineskip}
    \noindent
    For \textbf{\ref{lem:var_fixed(b)}}, as a consequence we have
    \begin{align*}
    \zeta_{\rho, R, \beta_0}(\Q) & = \min_{\xi \in \cL^2(\Q), \norm{\xi}^2_{\Q} \le 1/\delta} \mathscr{R}_{\Q}(\xi) 
    \\
    & = \min_{\xi \in \cL^2(\Q), \norm{\xi}^2_{\Q} \le 1/\delta} \E_{\Q} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right] \\
    & = \min_{\xi \in \cL^2(\Q)} \sup_{\nu \ge 0} \E_{\Q} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + 
    R\sqrt{1 - \rho^2}\xi \bigr) + R\sqrt{1 - \rho^2} \cdot \frac{\nu}{2} \left( \xi^2 - \frac{1}{\delta} \right) \right] \\
    & \stackrel{\mathmakebox[0pt][c]{\smash{\text{(i)}}}}{=} 
    \sup_{\nu \ge 0} \min_{\xi \in \cL^2(\Q)} \E_{\Q} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + 
    R\sqrt{1 - \rho^2}\xi \bigr) + R\sqrt{1 - \rho^2} \cdot \frac{\nu}{2} \left( \xi^2 - \frac{1}{\delta} \right) \right] \\
    & \stackrel{\mathmakebox[0pt][c]{\smash{\text{(ii)}}}}{=} 
    \sup_{\lambda > 0} \min_{U \in \cL^2(\Q)} \E_{\Q} \left[ \ell ( U ) + \frac{1}{2 \lambda} \left( U - \rho\norm{\vmu}_2 R - RG - \beta_0 Y \right)^2 - \frac{R^2 (1 - \rho^2)}{2 \lambda \delta} \right] \\
    & \stackrel{\mathmakebox[0pt][c]{\smash{\text{(iii)}}}}{=}
    \sup_{\lambda > 0} \left\{ \E_{\Q} \left[ \envelope_{\ell} \left( \rho\norm{\vmu}_2 R + RG + \beta_0 Y; \lambda \right) \right] - \frac{R^2 (1 - \rho^2)}{2 \lambda \delta} \right\},
\end{align*}
where (i) comes from strong duality in part \ref{lem:var_fixed(a)}, (ii) is by change of variable $U := \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi$ and $\lambda = R\sqrt{1 - \rho^2}/\nu$, (iii) is from the definition of Moreau envelope \cref{eq:envelope}. Now, consider change of variable
\begin{equation*}
    A = R \rho, \qquad
    B = R \sqrt{1 - \rho^2}, \qquad
    \nu = R \sqrt{1 - \rho^2}/\lambda.
\end{equation*}
Note that $0 < \nu < \infty$ by part \ref{lem:var_fixed(a)}, then $\zeta_{\rho, R, \beta_0}(\Q)$ can be expressed as
\begin{equation*}
    \zeta_{\rho, R, \beta_0}(\Q) = 
    \min_{\xi \in \cL^2(\Q), \norm{\xi}^2_{\Q} \le 1/\delta} \mathscr{R}_{\Q}(\xi)  = 
        \sup_{\nu > 0} \,
        \mathscr{R}_{\nu, \Q}(A, B, \beta_0), 
\end{equation*}
where
\begin{equation*} 
    \mathscr{R}_{\nu, \Q}(A, B, \beta_0) 
    =   - \frac{B \nu}{2 \delta }
        +
        \E_{\Q} \left[ \envelope_{\ell} \Bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ; \frac{B}{\nu} \Bigr) \right].
\end{equation*}
Finally, we complete the proof by the following arguments:
\begin{itemize}
    \item $\mathscr{R}_{\nu, \Q}(A, B, \beta_0)$ is convex in $(A, B, \beta_0)$. It comes from \cref{lem:prox}\ref{lem:prox(a)} that $(x, \lambda) \mapsto \envelope_{\ell}(x; \lambda)$ is convex, and the fact that integration $\E_{\Q}$ preserves convexity.
    \item $\mathscr{R}_{\nu, \Q}(A, B, \beta_0)$ is concave in $\nu$. This comes from \cref{eq:envelope} that
    \begin{equation*}
        \envelope_{\ell} \Bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ; \frac{B}{\nu} \Bigr)
        = \min_{t \in \R} \left\{ 
        \ell(t) + \frac{\nu}{2B} (A \norm{\vmu}_2 + A G_1 + B G_2 - t)^2
        \right\},
    \end{equation*}
    with the fact that pointwise minimum and integration $\E_{\Q}$ preserves concavity. 
\end{itemize}
This concludes the proof of part \ref{lem:var_fixed(b)}.
\end{proof}



Then we can use \cref{lem:var_fixed} to show convergence $M_n^{(3)}(\bTheta_{c}) \conp M^{*}(\bTheta_{c})$ in \cref{lem:M3-star}.
\begin{proof}[\textbf{Proof of \cref{lem:M3-star}}]
    Recall the change of variables $A = R \rho$ and $B = R \sqrt{1 - \rho^2}$ defined in \cref{lem:var_fixed}\ref{lem:var_fixed(b)}.
    Note that $f: (\rho, R, \beta_0) \mapsto (R \rho, R \sqrt{1 - \rho^2}, \beta_0)$ is a continuous map. Then $f(\bTheta_{c}) \subset \R_{\ge 0} \times \R_{\ge 0} \times \R$ is still compact. Hence, by \cref{lem:var_fixed} we have
    \begin{equation*}
        M_n^{(3)}(\bTheta_{c}) =
        \min_{ (A, B, \beta_0) \in f(\bTheta_{c}) } \sup_{\nu > 0} \,
        \mathscr{R}_{\nu, \Q_n}(A, B, \beta_0),
        \quad
        M^*(\bTheta_{c}) =
        \min_{ (A, B, \beta_0) \in f(\bTheta_{c}) } \sup_{\nu > 0} \,
        \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0).
    \end{equation*}
    For any fixed $A, B \ge 0$, $\beta_0 \in \R$, $\nu > 0$, by law of large numbers,
    \begin{align*}
        \mathscr{R}_{\nu, \Q_n}(A, B, \beta_0) 
        & = - \frac{B \nu}{2 \delta }
        +
        \E_{\Q_n} \left[ \envelope_{\ell} \Bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ; \frac{B}{\nu} \Bigr) \right] \\
        \conp \ \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0)
        & = - \frac{B \nu}{2 \delta }
        +
        \E_{\phantom{\Q_n}} \left[ \envelope_{\ell} \Bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ; \frac{B}{\nu} \Bigr) \right]. 
    \end{align*}
    Recall $\mathscr{R}_{\nu, \Q_n}(A, B, \beta_0)$ is concave in $\nu$. Also, note that $\mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0) \to -\infty$ as $\nu \to \infty$, since by \cref{lem:prox}\ref{lem:prox(a)}, we have
    \begin{equation*}
        \lim_{\nu \to \infty} \E \left[ \envelope_{\ell} \Bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ; \frac{B}{\nu} \Bigr) \right]
        = \E \left[ \ell ( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ) \right] < \infty.
    \end{equation*}
    This implies there exits $\overline{\nu} \in \R_{> 0}$, such that $\sup_{\nu \ge \overline{\nu}} \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0) < \sup_{\nu > 0} \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0)$. So, we can apply \cite[Lemma 10]{thrampoulidis2018precise} and conclude the uniform convergence
    \begin{equation*}
        \sup_{\nu > 0} \,
        \mathscr{R}_{\nu, \Q_n}(A, B, \beta_0)
        \ \conp \
        \sup_{\nu > 0} \,
        \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0).
    \end{equation*}
    Recall that both $\sup_{\nu > 0} \mathscr{R}_{\nu, \Q_n}(A, B, \beta_0)$ and $\sup_{\nu > 0} \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0)$ are convex in $(A, B, \beta_0)$ (since pointwise supremum preserves convexity). Then we could obtain uniform convergence on compact set $f(\bTheta_{c})$ by convexity \cite[Lemma 7.75]{liese2008statistical}:
    \begin{equation*}
    \abs{M_n^{(3)}(\bTheta_{c}) - M^*(\bTheta_{c})}
    \le
        \sup_{ (A, B, \beta_0) \in f(\bTheta_{c}) } \abs{
        \, \sup_{\nu > 0} \, \mathscr{R}_{\nu, \Q_n}(A, B, \beta_0)
        - \sup_{\nu > 0} \, \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0)
        } \conp 0.
    \end{equation*}
    This completes the proof.
\end{proof}






\subsubsection{Parameter convergence and optimality analysis: Proofs of \cref{lem:boundedness_parameter}---\ref{lem:ERM_param_conv}}
\label{append_subsubsec:ERM_param}

% We first conclude the asymptotics of $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$ by combining previous lemmas.
Recall that
\begin{equation}
    \label{eq:M_star}
    M^{*}
    = \min_{ \substack{ \rho \in [-1, 1], R \ge 0, \beta_0 \in \R \\ \xi \in \cL^2(\Q_\infty), \norm{\xi}_{\Q_\infty} \le 1/\sqrt{\delta} } } 
    \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right],
\end{equation}
where $R$, $\beta_0$ are optimized over unbounded sets. The following lemma shows that any minimizer $R^*$, $\beta_0^*$ of \cref{eq:M_star} must be bounded.

\begin{lem}[Boundedness of $R^*$ and $\beta_0^*$]\label{lem:boundedness_parameter}
    Let $(\rho^*, R^*, \beta_0^*, \xi^*)$ be any minimizer of \cref{eq:M_star}. Then in the non-separable regime ($\delta > \delta^*(0)$), we have $R^* < \infty$ and $\abs{\beta_0^*} < \infty$.
\end{lem}
\begin{proof}
    We first prove the following claim: There exists an $\veps > 0$, such that for any $(a, b) \in \R_{> 0} \times \R$ satisfying $a^2 + b^2 = 1$, any $\rho \in [-1, 1]$, and any $\xi \in \cL^2 (\Q_\infty)$, $\norm{\xi}_{\Q_\infty} \le 1 / \sqrt{\delta}$:
    \begin{equation}
    \label{eq:V_claim}
        \P \left( a \rho \norm{\bmu}_2 + a G + b Y + a \sqrt{1 - \rho^2} \xi \le - \veps \right) \ge \veps.
    \end{equation}
    We prove this claim by contradiction. Assume it is not true, then for any $m \in \mathbb{N}$, there exists the corresponding $(a_m, b_m, \rho_m, \xi_m)$ such that $(a_m, b_m) \in \R_{> 0} \times \R$, with $a_m^2 + b_m^2 = 1$, $\rho_m \in [-1, 1]$, and $\xi_m \in \cL^2 (\Q_\infty)$, $\norm{\xi_m}_{\Q_\infty} \le 1 / \sqrt{\delta}$, which satisfy
    \begin{equation}
    \label{eq:Vm_bound}
        \P \left( a_m \rho_m \norm{\bmu}_2 + a_m G + b_m Y + a_m \sqrt{1 - \rho_m^2} \xi_m \le - \frac{1}{m} \right) < \frac{1}{m}.
    \end{equation}
    We can always assume that $(a_m, b_m, \rho_m) \to (a, b, \rho)$ and $\xi_m \to \xi$ weakly in $\cL^2 (\Q_\infty)$ when $m \to \infty$. Otherwise, such a convergent subsequence always exists according to Heine--Borel Theorem and Banach--Alaoglu Theorem. Therefore, $a_m \rho_m \norm{\bmu}_2 + a_m G + b_m Y + a_m \sqrt{1 - \rho_m^2} \xi_m$ weakly converges to $a \rho \norm{\bmu}_2 + a G + b Y + a \sqrt{1 - \rho^2} \xi$ in $\cL^2 (\Q_\infty)$. For any nonnegative $Z \in \cL^2 (\Q_\infty)$, one has
    \begin{align*}
        & \E \left[ \bigl( a \rho \norm{\bmu}_2 + a G + b Y + a \sqrt{1 - \rho^2} \xi \bigr) Z \right] \\
        = {} & \lim_{m \to \infty} \E \left[ \bigl( a_m \rho_m \norm{\bmu}_2 + a_m G + b_m Y + a_m \sqrt{1 - \rho_m^2} \xi_m \bigr) Z \right].
    \end{align*}
    Denote $U_m := a_m \rho_m \norm{\bmu}_2 + a_m G + b_m Y + a_m \sqrt{1 - \rho_m^2} \xi_m$, then we obtain the following estimate:
    \begin{align*}
        \E [U_m Z] & =  \E \left[ U_m \ind_{U_m > - 1/m} Z \right] + \E \left[ U_m \ind_{U_m \le - 1/m} Z \right] 
        \\ 
        & \ge - \frac{1}{m} \E [Z] - \left( \E[ U_m^2 ] \right)^{1/2}  \left(\E[ Z^2 \ind_{U_m \le - 1/m}] \right)^{1/2},
    \end{align*}
    where the last line follows from Cauchy--Schwarz inequality. By definition of $U_m$, we know that $\E [U_m^2]$ is uniformly bounded for any $m \in \mathbb{N}$. Further, since $Z \in \cL^2 (\Q_\infty)$ and $\P (U_m \le -1/m) \le 1/m \to 0$ as $m \to \infty$ by \cref{eq:Vm_bound}, we know that $\E [Z^2 \ind_{U_m \le - 1/m}] \to 0$. It finally follows that
    \begin{equation*}
        \E \left[ \bigl( a \rho \norm{\bmu}_2 + a G + b Y + a \sqrt{1 - \rho^2} \xi \bigr) Z \right] 
        = \lim_{m \to \infty} \E [U_m Z] \ge 0.
    \end{equation*}
    Since this is true for any nonnegative $Z \in \cL^2 (\Q_\infty)$, we know that
    \begin{equation*}
        a \rho \norm{\bmu}_2 + a G + b Y + a \sqrt{1 - \rho^2} \xi \ge 0, \quad \text{almost surely},
    \end{equation*}
    or equivalently, there exists $(\rho, R, \beta_0) \in [-1, 1] \times \R_{>0} \times \R$ and $\xi \in \cL^2 (\Q_\infty)$, $\E[\xi^2] \le 1/\delta$ satisfying
    \begin{equation*}
        R \rho \norm{\bmu}_2 + R G + \beta_0 Y + R \sqrt{1 - \rho^2} \xi \ge 0, \quad \text{almost surely}.
    \end{equation*}
    It implies the constraint of the variational problem for the separable regime (SVM) \cref{eq:SVM_variation}, i.e., $\rho \norm{\bmu}_2 + G + \beta_0' Y + \sqrt{1 - \rho^2} \xi \ge \kappa$ holds for some $\kappa \ge 0$ (with change of variable $\beta_0' := \beta_0 / R$). According to \cref{thm:SVM_main}\ref{thm:SVM_main_var}, we obtain $\kappa^* \ge 0$, or equivalently $\delta \le \delta^*(0)$, which contradicts the non-separable regime $\delta > \delta^* (0)$. Our claim \cref{eq:V_claim} is thus proved. 
    
    Now for any $(\rho, R, \beta_0, \xi)$ such that $R > 0$, denote
    \begin{equation*}
        V(\rho, R, \beta_0, \xi) := \frac{1}{\sqrt{R^2 + \beta_0^2}} \bigl( \rho \norm{\vmu}_2 R + R G + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr). 
    \end{equation*}
    We know that $\P (V(\rho, R, \beta_0, \xi) \le - \veps) \ge \veps$ by \cref{eq:V_claim}. Therefore,
    \begin{align*}
        & \E \left[ \ell \bigl( \rho \norm{\vmu}_2 R + R G + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right]
        \\ 
          = {} & \E \left[ \ell \Bigl( \sqrt{ R^2 + \smash{\beta_0^2} } \, V(\rho, R, \beta_0, \xi) \Bigr) \right] \\
        \ge {} & \E \left[ \ell \Bigl( \sqrt{ R^2 + \smash{\beta_0^2} } \, V(\rho, R, \beta_0, \xi) \Bigr) \ind_{V(\rho, R, \beta_0, \xi) \le - \veps} \right] \\
        \ge {} & \veps \ell \Bigl( - \veps \sqrt{ R^2 + \smash{\beta_0^2} } \Bigr),
    \end{align*}
    which diverges to infinity as $R^2 + \beta_0^2 \to \infty$. This completes the proof.
\end{proof}

A direct consequence of \cref{lem:boundedness_parameter} is that $M^* = M^*(\bTheta_{c})$ for $\bTheta_{c}$ large enough. The following result shows that $M^*$ in \cref{eq:M_star} has a unique minimizer.

\begin{lem}
\label{lem:M_star_var}
    Consider the variational problem $M^*$ defined in \cref{eq:M_star}.
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:M_star_var(a)}
        $M^*$ has a unique minimizer $(\rho^*, R^*, \beta_0^*, \xi^*)$, which must satisfy
        \begin{equation*}
            \xi^* = - \frac{\lambda^*}{R^* \sqrt{1 - \rho^*{}^2}} \ell'\bigl(\prox_{ \lambda^* \ell}( \rho^*\norm{\vmu}_2 R^* + R^* G + \beta_0^* Y )\bigr),
        \end{equation*}
        where $\lambda^*$ is the unique solution such that $\E[\xi^{* 2}] = 1/\delta$. As a consequence, we have
        \begin{equation*}
           M^* = \E \left[ \ell \bigl( \prox_{ \lambda^* \ell}( \rho^*\norm{\vmu}_2 R^* + R^* G + \beta_0^* Y )
                \bigr) \right].
        \end{equation*}

        
        \item \label{lem:M_star_var(b)}
        $(\rho^*, R^*, \beta_0^*, \lambda^*)$ is also the unique solution to the system of equations
        \begin{equation}\label{eq:sys_eq_Q}
            \begin{aligned}
                - \frac{R \rho}{\lambda \delta \norm{\vmu}_2}
                & = 
                \E \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) \right],
                \\
                \frac{R}{\lambda \delta }
                & = 
                \E \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) G \right],
                \\
                0
                & = 
                \E \left[\ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) Y \right],
                \\
                \frac{R^2 (1 - \rho^2)}{\lambda^2 \delta}
                & = 
                \E \left[ \left(\ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) \right)^2 \right],
            \end{aligned}
        \end{equation}
        where $(\rho^*, R^*, \beta_0^*, \lambda^*) \in (0, 1) \times \R_{>0} \times \R \times \R_{> 0}$.

        

        \item \label{lem:M_star_var(c)}
        % Moreover, the optimization problem \cref{eq:var_optim_Q} can also be expressed as
        % \begin{equation}\label{eq:var_optim_Q1}
        % \ljy{
        %     \mathscr{R}^*_{\Q} = 
        % \min_{ \substack{ \rho \in [0, 1], R \ge 0 \\ \beta_0 \in \R} }
        % \sup_{ \lambda > 0}
        % \biggl\{ 
        % - \frac{R^2(1 - \rho^2)}{2 \delta \lambda}
        % +
        % \E_{\Q} \left[ \envelope_{\lambda\ell} ( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \right]
        % \biggr\}
        % ,
        % }
        % \end{equation}
        % where $(\rho^*, R^*, \beta_0^*, \lambda^*)$ is also the unique minimizer of this problem.

        With change of variables $A := R \rho$, $B := R \sqrt{1 - \rho^2}$, the original variational problem \cref{eq:M_star} can be reduced to the following minimax problem
        \[ 
        M^* = 
        \min_{\substack{ A \ge 0, B \ge 0 \\ \beta_0 \in \R} }
        \sup_{\nu > 0} 
        \,
         \biggl\{ 
        - \frac{B \nu}{2 \delta }
        +
        \E \left[ \envelope_{\ell} \Bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y ; \frac{B}{\nu} \Bigr) \right]
        \biggr\},
        \]
        where $(Y, G_1, G_2) \sim P_y \times \normal(0, 1) \times \normal(0, 1)$, and the objective function is convex-concave.
    \end{enumerate}
\end{lem}

\begin{proof}
    We first show the optimization problem \cref{eq:M_star} has a unique minimizer. Since its original formulation is non-convex, we make the following change of variables:
    \begin{equation}\label{eq:change_var_AB}
        A := R \rho, \qquad B := R \sqrt{1 - \rho^2}, \qquad \xi_B := B \xi.
    \end{equation}
    Then, the optimization problem is recast as
    \begin{equation}\label{eq:var_optim_Q_recast}
        \min_{
        \substack{ A , B \ge 0, \beta_0 \in \R \\ \xi_B \in \cL^2(\Q_\infty)}
        } \ \E \left[ \ell \bigl( A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y + \xi_B \bigr) \right], \quad \text{subject to} \ \norm{\xi_B}_{\Q} \le \frac{B}{\sqrt{\delta}},
    \end{equation}
    which is convex, where $(Y, G_1, G_2) \sim P_y \times \normal(0, 1) \times \normal(0, 1)$ (recall that $A G_1 + B G_2 \overset{\smash{\mathrm{d}}}{=} R G$). Now we show that the above optimization problem has a unique minimizer. Note that \cref{lem:boundedness_parameter} also implies that any minimizer of this optimization problem is finite. Therefore, a similar argument as in the proof of \cref{lem:var_fixed}\ref{lem:var_fixed(a)} shows that \cref{eq:var_optim_Q_recast} has a unique minimizer. Since the mapping $(\rho, R, \xi) \mapsto (A, B, \xi_B)$ is one-to-one, this also proves the original optimization problem \cref{eq:M_star} has a unique minimizer.
    
    As a consequence, the unique minimizer is determined by the KKT and Slater's conditions for variational problems \cite[Theorem 2.9.2]{zalinescu2002convex}. $(A, B, \beta_0, \xi_B)$ is the minimizer of \cref{eq:var_optim_Q_recast} if and only if, for some scalar $\nu_B$ (Lagrange multiplier), the followings hold:
    \begin{equation}
    \label{eq:M_star_KKT}
    \begin{aligned}
        A \norm{\vmu}_2 + A G_1 + B G_2 + \beta_0 Y + \xi_B & = U, \\
        \E \left[ \ell'(U) ( \norm{\vmu}_2 + G_1 ) \right] & = 0, \\
        \E \left[ \ell'(U) G_2 \right] - \nu_B \frac{B}{\delta} & = 0, \\
        \E \left[ \ell'(U) Y   \right] & = 0, \\
        \ell'(U) + \nu_B \xi_B & = 0, \\
        \delta  \, \E[\xi_B^2] \le B^2, \quad
        \nu_B \ge 0, \quad
        \nu_B \bigl( \delta \, \E[\xi_B^2] - B^2 \bigr) & = 0.
    \end{aligned}
    \end{equation}
    Using a similar argument as in the proof of \cref{lem:var_fixed}\ref{lem:var_fixed(a)}, we can also show that
    \begin{equation*}
        0 < \nu_B < \infty,  \qquad  \xi_B > 0 \ \  (\text{a.s.}), \qquad \E[\xi_B^2] = B^2/\delta,
    \end{equation*}
    which implies $B > 0$. Plugging this into \cref{eq:M_star_KKT} solves two conditions
    \begin{equation}
    \label{eq:M_star_KKT-1}
        \E \, \bigl[ \bigl( \ell'(U) \bigr)^2 \bigr] = \nu_B^2 \frac{B^2}{\delta},
        \qquad
        \E\left[ \ell'(U) Y \right] = 0.
    \end{equation}
    By Stein's identity, we also have relation
    \begin{equation*}
        \E\left[ \ell'(U) G_1 \right] = A \, \E\left[ \ell''(U) \right],
        \qquad
        \E\left[ \ell'(U) G_2 \right] = B \, \E\left[ \ell''(U) \right].
    \end{equation*}
    Combine the above with \cref{eq:M_star_KKT}, we obtain
    \begin{align*}
        \E \left[ \ell'(U) \right] = -\nu_B \frac{A}{\delta\norm{\bmu}_2},
    \qquad
        \E \left[ \ell'(U) G_1 \right] = \nu_B \frac{A}{\delta},
    \qquad
        \E \left[ \ell'(U) G_2 \right] = \nu_B \frac{B}{\delta},
    \end{align*}
    which is equivalent to (recall that $A G_1 + B G_2 \overset{\smash{\mathrm{d}}}{=} R G$)
    \begin{equation}
    \label{eq:M_star_KKT-2}
        \E \left[ \ell'(U) \right] = -\nu_B \frac{A}{\delta\norm{\bmu}_2},
        \qquad
        \E \left[ \ell'(U) G \right] = \nu_B \frac{R}{\delta}.
    \end{equation}
    The above implies $A > 0$ since $\ell' < 0$ by \cref{lem:ell}. Since both $A, B > 0$, by \cref{eq:change_var_AB} we have $\rho \in (-1, 1) \setminus \{ 0 \}$ and $R > 0$. Moreover, notice that for any $\rho > 0$,
    \begin{equation*}
        \E \left[ \ell \bigl( -\rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right]
        >
        \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right].
    \end{equation*}
    Therefore, we must have $\rho \in (0, 1)$. Then we prove $(\rho^*, R^*, \beta_0^*, \lambda^*) \in (0, 1) \times \R_{>0} \times \R \times \R_{> 0}$. Lastly, by combining \cref{eq:M_star_KKT-1} and \eqref{eq:M_star_KKT-2} with change of variable $\lambda := 1/\nu_B$, and recalling \cref{eq:U_prox} in the proof of \cref{lem:var_fixed}, we obtain the KKT conditions \cref{eq:sys_eq_Q} expressed in $(\rho, R, \beta_0, \lambda)$. Then we complete the proof of part \ref{lem:M_star_var(b)}. Finally, part \ref{lem:M_star_var(c)} directly follows from \cref{lem:var_fixed}.
    % KKT conditions:
    %     \[
    %     \begin{aligned}
    %         \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi & = U  \\
    %         \E_{\Q} \left[ \ell' ( U )
    %         \biggl( \norm{\vmu}_2 R - \frac{\rho R}{\sqrt{1 - \rho^2}} \xi \biggr) \right]
    %         & = 0 \\
    %         \E_{\Q} \left[ \ell' ( U )
    %         (\rho\norm{\vmu}_2 + G + \sqrt{1 - \rho^2} \xi ) \right] 
    %         & = 0 \\
    %         \E_{\Q} \left[ \ell' ( U )
    %         Y \right] 
    %         & = 0 \\
    %         \color{red} \ell' ( U )
    %         R \sqrt{1 - \rho^2} 
    %         + \lambda \xi
    %         & = 0 \\
    %         R & \ge 0 \\
    %         \delta \norm{\xi}_{\Q}^2 - 1 & \le 0 \\
    %         \lambda & \ge 0 \\
    %         \lambda\bigl( \delta \norm{\xi}_{\Q}^2 - 1 \bigr) & = 0 
    %     \end{aligned}
    %     \]
    %     If $\ell$ is strictly decreasing, we must have $\lambda > 0$, and therefore $\delta \norm{\xi}_{\Q}^2 - 1 = 0$. It can be shown that $\lambda < \infty$ ($\Q$-a.s.). Then, we apply variable transformation $\lambda \gets R^2 (1 - \rho^2)/\lambda$. KKT condition becomes
    %     \begin{equation*}
    %         \color{red} 
    %         \lambda \ell' ( U )
    %         + R \sqrt{1 - \rho^2} \xi
    %         = 0.
    %     \end{equation*}
    %     Let $U = U(G, Y)$ be the random (function) satisifying
    %     \[  
    %         U + \lambda \ell'(U) = \rho\norm{\vmu}_2 R + RG + \beta_0 Y.
    %     \]
    %     The existence and uniqueness of $U$ is ensured by strictly convexity of $\ell$. Recall that
    %     \begin{equation*}
    %         \prox_{\lambda f}(x) = \argmin_{t \in \R} \left\{ \lambda f(t) +  \frac12 (t - x)^2 \right\}.
    %     \end{equation*}
    %     Then we can write
    %     \begin{equation*}
    %         U = \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ).
    %     \end{equation*}
    %     Recall
    %     \[ \ell'(U) = -\frac{R \sqrt{1 - \rho^2}}{\lambda} \xi. \]
    %     Therefore, the KKT simplifies to
    %     \[
    %     \begin{aligned}
    %         \norm{\vmu}_2 \sqrt{1 - \rho^2} \E_{\Q} [ \xi ] -  \rho \E_{\Q} [ \xi^2 ] 
    %         & = 0 \\
    %         \rho\norm{\vmu}_2 \E_{\Q}[\xi] +  \E_{\Q}[\xi G] + \sqrt{1 - \rho^2} \E_{\Q} [ \xi^2 ] 
    %         & = 0 \\
    %         \E_{\Q}[\xi Y]
    %         & = 0
    %     \end{aligned}
    %     \]
    %     Then we obtain the system of equations
    %     \[
    %     \begin{aligned}
    %         \E_{\Q} [ \xi ] & = \frac{\rho}{\delta \norm{\vmu}_2 \sqrt{1 - \rho^2}},
    %         \\
    %         \E_{\Q}[\xi G] & = - \frac{1}{\delta \sqrt{1 - \rho^2}},
    %         \\
    %         \E_{\Q}[\xi Y] & = 0,
    %         \\
    %         \E_{\Q}[\xi^2] & = \frac{1}{\delta}.
    %     \end{aligned}
    %     \]
    %     Plug-in these results,
    %     \[
    %     \begin{aligned}
    %         \E_{\Q} \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) \right] & = - \frac{\rho R}{\lambda \delta \norm{\vmu}_2},
    %         \\
    %         \E_{\Q} \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) G \right] & =  \frac{R}{\lambda \delta },
    %         \\
    %         \E_{\Q} \left[\ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) Y \right] & = 0,
    %         \\
    %         \E_{\Q} \left[ \left(\ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) \right)^2 \right] & = \frac{R^2 (1 - \rho^2)}{\lambda^2 \delta}.
    %     \end{aligned}
    %     \]
\end{proof}

We are now in position to establish the convergence of parameters.
\begin{proof}[\textbf{Proof of \cref{lem:ERM_param_conv}}]
Consider any $\varepsilon \ge 0$ and $C_R, C_{\beta_0} \in (0, \infty)$, let
\begin{equation*}
    \bTheta_{c}^*(\varepsilon) := \left\{ (\rho, R, \beta_0) \in [-1, 1] \times [0, C_R] \times [-C_{\beta_0}, C_{\beta_0}] : 
    \norm{ (\rho, R, \beta_0) - (\rho^*, R^*, \beta_0^*) }_2 \ge \varepsilon
    \right\}
\end{equation*}
and let $\bTheta^*_{\vbeta}(\varepsilon)$ defined as \cref{eq:Theta_link}. By \cref{lem:ERM_bound_beta} and \ref{lem:M_star_var}, we can choose some $C_R, C_{\beta_0} > 0$ and compact convex set $\bXi_{\bu} \subset \R^{n}$ large enough, such that as $n, d \to \infty$,
\begin{equation*}
    M_n =
    M_n(\bTheta^*_{\vbeta}(0), \bXi_{\bu}) \ \ (\text{w.h.p.}) ,
    \qquad
    M^* =
    M^*(\bTheta^*_{c}(0)).
\end{equation*}
Then according to \cref{thm:ERM_conv}, we have global convergence
\begin{equation*}
    M_n \conp M^*.
\end{equation*}
However, for any $\varepsilon > 0$ and $\zeta > 0$, by \cref{thm:ERM_conv} we have
\begin{equation*}
    M_n(\bTheta^*_{\vbeta}(\varepsilon), \R^n) =
    M_n(\bTheta^*_{\vbeta}(\varepsilon), \bXi_{\bu}) \ \ (\text{w.h.p.}) ,
    \quad
    \P \left( M_n(\bTheta^*_{\vbeta}(\varepsilon), \bXi_{\bu}) \le M^*(\bTheta_{c}^*(\varepsilon)) - \zeta \right) \to 0.
\end{equation*}
This implies
\begin{equation*}
    \pliminf_{n \to \infty} M_n(\bTheta^*_{\vbeta}(\varepsilon), \R^n) \ge
    M^*(\bTheta_{c}^*(\varepsilon))
    >
    M^*,
\end{equation*}
where the strict inequality comes from the uniqueness of minimizer $(\rho^*, R^*, \beta_0^*, \xi^*)$, established in \cref{lem:M_star_var}\ref{lem:M_star_var(a)}. Since $\varepsilon > 0$ can be arbitrarily small, this proves $(\hat\rho_n, \| \hat\vbeta_n \|_2, \hat\beta_{0,n}) \conp (\rho^*, R^*, \beta_0^*)$. Moreover, we know that $R^* > 0$ by \cref{lem:M_star_var}\ref{lem:M_star_var(b)}. So $\hat\vbeta_n \not= \bzero$ and therefore $\hat\rho_n$ is well-defined with high probability. This concludes the proof of \cref{lem:ERM_param_conv}.
\end{proof}



\subsubsection{ELD convergence: Proof of \cref{lem:ERM_logit_conv}}
\label{append_subsubsec:ERM_logit}

\begin{proof}[\textbf{Proof of \cref{lem:ERM_logit_conv}}]
We first establish the convergence of logit margins. Recall that
\begin{align*}
    \hat \cL_{n} = \  & \frac1n \sum_{i=1}^n \delta_{y_i ( \< \xx_i, \hat\vbeta \> + \hat\beta_{0} ) },
    \\ 
    \cL_* = \  & \Law \, (U^*)
    := \Law \, \bigl( \rho^*\norm{\vmu}_2 R^* + R^* G + \beta_0^* Y + R^* \sqrt{1 - \rho^*{}^2} \xi^* \bigr) 
    \\
    = \ & \Law \, \bigl( \prox_{ \lambda^* \ell}( \rho^* \norm{\vmu}_2 R^* + R^* G + \beta_0^* Y ) \bigr).
\end{align*}
For any $\varepsilon > 0$ small enough, we have defined the $\varepsilon$-$W_2$ open ball by
\begin{equation*}
    \mathsf{B}_{W_2}(\varepsilon) = \left\{ \bu \in \R^n:   W_2 \biggl( 
        \frac{1}{n}\sum_{i=1}^n \delta_{u_i}, \cL_*
     \biggr)  < \varepsilon \right\}.
\end{equation*}
For $C_R, C_{\beta_0} \in (0, \infty)$, let $\bTheta_{c} = [-1, 1] \times [0, C_R] \times [-C_{\beta_0}, C_{\beta_0}]$ and let $\bTheta_{\vbeta}$ be defined as \cref{eq:Theta_link}. When $C_R, C_{\beta_0} > 0$ and compact set $\bXi_{\bu} \subset \R^{n}$ are large enough, by \cref{lem:ERM_bound_beta} we have
\begin{align*}
    \wt M_n^\varepsilon & := M_n(\R^{d + 1}, \mathsf{B}_{W_2}^c(\varepsilon) ) 
    = M_n(\bTheta_{\vbeta}, \bXi_{\bu} \setminus \mathsf{B}_{W_2}(\varepsilon) ) \ \ (\text{w.h.p.}) ,
    \\
    % \P \, \Bigl( M_n(\bTheta_{\vbeta}, \bXi_{\bu} \setminus \mathsf{B}_{W_2}(\varepsilon) ) \le t \Bigr) 
    % \le 2 \, \P \,\Bigl( M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu} \setminus \mathsf{B}_{W_2}(\varepsilon) ) \le t \Bigr),
    % \ \ \forall\, t \in \R,
    % \\
    \wt M_n^{\varepsilon(3)} 
    & :=
    M_n^{(3)}(\bTheta_{c}, \mathsf{B}_{W_2}^c(\varepsilon) )
    = M_n^{(3)}(\bTheta_{c}, \bXi_{\bu} \setminus \mathsf{B}_{W_2}(\varepsilon) ).
\end{align*}
Combining these with \cref{lem:ERM_CGMT} and \ref{lem:M2-3} obtains that for any $\zeta > 0$,
\begin{equation}
    \label{eq:Mn-3-eps}
    \lim_{n \to \infty} \P \, \Bigl( \wt M_n^\varepsilon \le \wt M_n^{\varepsilon(3)} - \zeta \Bigr) = 0.
\end{equation}
In order to show $W_2( \hat \cL_{n}, \cL_* ) \conp 0$, our goal is to show that
\begin{equation*}
    \lim_{n \to \infty} \P \, \Bigl( \wt M_n^{\varepsilon} > M_n \Bigr) = 1.
\end{equation*}
Then according to \cref{eq:Mn-3-eps} and \cref{lem:ERM_param_conv}, it suffices to show that
\begin{equation}
    \label{eq:ERM_logit_goal}
    \pliminf_{n \to \infty} \wt M_n^{\varepsilon(3)}  > \plim_{n \to \infty} M_n = M^*.
\end{equation}
By \cref{eq:Mn(3)} and \eqref{eq:set_N_n_delta}, recall that
\begin{equation*}
    \wt M_n^{\varepsilon(3)} = \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
    \min_{\bu \in 
    \mathsf{N}^\delta_n(\rho, R, \beta_0) \setminus \mathsf{B}_{W_2}(\varepsilon) } 
    \frac1n \sum_{i=1}^n \ell(u_i),
\end{equation*}
where we temporarily define
\begin{equation*}
    \mathsf{N}^\delta_n(\rho, R, \beta_0) := \left\{
        \bu \in \R^n: 
        \frac{1}{\sqrt{n}} \bigl\| \rho\norm{\vmu}_2 R \bone_n + R \vg + \beta_0 \yy - \bu \bigr\|_2
        \le \frac{R \sqrt{1 - \rho^2}}{\sqrt{\delta}}
     \right\}.
\end{equation*}
Now we split $\wt M_n^{\varepsilon(3)}$ into two parts by
\begin{equation*}
    \wt M_n^{\varepsilon(3)} 
    = \min\left\{ I, I\!I \right\}
    := \min\left\{ 
        \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \setminus \mathsf{B}_{2,c^*}(\eta)
     \\ \bu \in 
    \mathsf{N}^\delta_n(\rho, R, \beta_0) \setminus \mathsf{B}_{W_2}(\varepsilon)  } } 
    \frac1n \sum_{i=1}^n \ell(u_i)
    ,
        \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \cap \mathsf{B}_{2,c^*}(\eta)
     \\ \bu \in 
    \mathsf{N}^\delta_n(\rho, R, \beta_0) \setminus \mathsf{B}_{W_2}(\varepsilon)  } } 
    \frac1n \sum_{i=1}^n \ell(u_i)
     \right\},
\end{equation*}
where $\eta > 0$ and
\begin{equation*}
\mathsf{B}_{2,c^*}(\eta) = \left\{ (\rho, R, \beta_0) \in \R^3 : \norm{(\rho, R, \beta_0) - (\rho^*, R^*, \beta^*_0)}_2 < \eta \right\}
\end{equation*}
is a $\eta$-$\cL^2$ open ball around the global minimizer $(\rho^*, R^*, \beta^*_0)$.

For the first term, with $\bTheta_{c}$ large enough such that $(\rho^*, R^*, \beta^*_0) \in \bTheta_{c}$, by \cref{lem:M3-star} we have
\begin{align*}
        I 
        & \ge    
        \min_{(\rho, R, \beta_0) \in \bTheta_{c} \setminus \mathsf{B}_{2,c^*}(\eta) }
        \min_{\bu \in 
    \mathsf{N}^\delta_n(\rho, R, \beta_0)  }
    \frac1n \sum_{i=1}^n \ell(u_i) 
       \\
       & =  \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \setminus \mathsf{B}_{2,c^*}(\eta) 
       \\ \xi \in \cL^2(\Q_n), \norm{\xi}^2_{\Q_n} \le 1/\delta } } 
       \E_{\Q_n} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right] \\
       & =  M_n^{(3)}\bigl( \bTheta_{c} \setminus \mathsf{B}_{2,c^*}(\eta) \bigr)
       \conp  
       M_n^*\bigl( \bTheta_{c} \setminus \mathsf{B}_{2,c^*}(\eta) \bigr) >  M^*( \bTheta_{c} ) = M^*,
\end{align*}
where the strict inequality follows from the uniqueness of $(\rho^*, R^*, \beta^*_0)$ according to \cref{lem:M_star_var}\ref{lem:M_star_var(a)}.



For the second term, we can take $\eta > 0$ small enough, such that $(\rho, R, \beta_0) \in \mathsf{B}_{2,c^*}(\eta)$ implies
\begin{align*}
    &
    W_2 \, \Bigl( 
        \Law \left( U^*_{\rho, R, \beta_0} \right), \cL_*
     \Bigr) \\
    = {} &  
    W_2 \, \Bigl(
        \Law \left( U^*_{\rho, R, \beta_0} \right), \Law \, \bigl( U^*_{\rho^*, R^*, \beta_0^*} \bigr)
     \Bigr)
     \le \frac{\varepsilon}{2},
     \qquad
     \forall\, (\rho, R, \beta_0) \in \bTheta_{c} \cap \mathsf{B}_{2,c^*}(\eta),
\end{align*}
where $U^*_{\rho, R, \beta_0} := \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi^*_{\Q_\infty}(\rho, R, \beta_0)$, and $\xi^*_{\Q_\infty}(\rho, R, \beta_0)$ is the unique minimizer of $\mathscr{R}_{\Q}(\xi)$ defined in \cref{eq:ERM_var_fix}, with an expression given by \cref{eq:xi_star}. The existence of such $\eta > 0$ is guaranteed by continuity of $W_2$ distance and $(\rho, R, \beta_0) \mapsto U^*_{\rho, R, \beta_0}$ by \cref{lem:var_fixed}. Then $\bu \notin \mathsf{B}_{W_2}(\varepsilon)$ implies (by triangle inequality)
\begin{equation*}
    W_2 \biggl( 
        \frac{1}{n}\sum_{i=1}^n \delta_{u_i}, \Law \left( U^*_{\rho, R, \beta_0} \right)
     \biggr)
     \ge \frac{\varepsilon}{2} 
     ,
     \qquad
     \forall\, (\rho, R, \beta_0) \in \bTheta_{c} \cap \mathsf{B}_{2,c^*}(\eta). 
\end{equation*} 
Thus we have
\begin{equation}
\label{eq:ERM_II0}
    I\!I = 
     \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \cap \mathsf{B}_{2,c^*}(\eta)
     \\ \bu \in 
    \mathsf{N}^\delta_n(\rho, R, \beta_0) \setminus \mathsf{B}_{W_2}(\varepsilon)  } } 
    \frac1n \sum_{i=1}^n \ell(u_i)
    \ge 
     \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c}
     \\ U \in 
    \mathcal{N}^\delta_n(\rho, R, \beta_0) \cap \mathcal{C}_n^\varepsilon(\rho, R, \beta_0)  } } 
    \E_{\Q_n}[\ell(U)],
\end{equation}
where denote
\begin{equation}
\label{eq:ERM_logit1}
    \mathcal{C}_n^\varepsilon(\rho, R, \beta_0) := \left\{ U \in \cL^2(\Q_\infty): 
    \norm{ U - U^*_{\rho, R, \beta_0} }_{\Q_n} \ge \frac{\varepsilon}{2}
    \right\}
\end{equation}
and recall \cref{eq:set_N_n_delta} that 
\begin{equation}
\label{eq:ERM_logit2}
    \mathcal{N}_n^\delta(\rho, R, \beta_0) = \left\{ 
        U \in \cL^2(\Q_n):  \bigl\| \rho\norm{\vmu}_2 R + RG + \beta_0 Y - U \bigr\|_{\Q_n}
        \le  \frac{ R\sqrt{1 - \rho^2} }{ \sqrt{\delta} }
     \right\}.
\end{equation}
Now, denote $\hat U_{\rho, R, \beta_0} := \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi^*_{\Q_n}(\rho, R, \beta_0)$. According to \cref{lem:var_fixed}, we know that $\| \xi^*_{\Q_n}(\rho, R, \beta_0) \|_{\Q_n}^2 = 1/\delta$, that is,
\begin{equation}
\label{eq:ERM_logit3}
    \bigl\| \rho\norm{\vmu}_2 R + RG + \beta_0 Y - \hat U_{\rho, R, \beta_0} \bigr\|_{\Q_n}
        = \frac{ R\sqrt{1 - \rho^2} }{ \sqrt{\delta} }.
\end{equation}
We claim $\bigl\| U^*_{\rho, R, \beta_0} - \hat U_{\rho, R, \beta_0} \bigr\|_{\Q_n} \conp 0$. Otherwise, there exits a convergent sequence $\{ \hat\lambda_m \}_{m \in \mathbb{N}}$ such that $\plim_{m \to \infty} \hat\lambda_m \neq \lambda^*$, where $\hat\lambda_m$ satisfies the conditions in \cref{lem:var_fixed}\ref{lem:var_fixed(a)} under $\Q = \Q_m$, and $\lambda^*$ satisfies the conditions in \cref{lem:var_fixed}\ref{lem:var_fixed(a)} under $\Q = \Q_\infty$. This contradicts the convergence $\argmax_{\nu > 0} \mathscr{R}_{\nu, \Q_m}(A, B, \beta_0) \conp \argmax_{\nu > 0} \mathscr{R}_{\nu, \Q_\infty}(A, B, \beta_0)$ by an argmax theorem for the concave process \cite[Theorem 7.77]{liese2008statistical} according to \cref{lem:var_fixed}\ref{lem:var_fixed(b)}, and change of variable $\nu = R\sqrt{1 - \rho^2}/\lambda$. Hence, for all $n$ large enough, we have
\begin{equation*}
    \bigl\| U^*_{\rho, R, \beta_0} - \hat U_{\rho, R, \beta_0} \bigr\|_{\Q_n} \le \frac{\varepsilon}{2}.
\end{equation*}
Combining this with \cref{eq:ERM_logit1}---\eqref{eq:ERM_logit3} together, by triangle inequality, we obtain
\begin{equation}
\label{eq:ERM_logit_sets}
    \mathcal{N}^\delta_n(\rho, R, \beta_0) \cap \mathcal{C}_n^\varepsilon(\rho, R, \beta_0)
    \subseteq \wt{\mathcal{N}}^{\delta,\varepsilon}_n(\rho, R, \beta_0)
\end{equation}
where
\begin{equation*}
    \wt{\mathcal{N}}^{\delta,\varepsilon}_n(\rho, R, \beta_0)
    := \left\{ 
        U \in \cL^2(\Q_n):  \bigl\| \rho\norm{\vmu}_2 R + RG + \beta_0 Y - U \bigr\|_{\Q_n}
        \le \frac{ R\sqrt{1 - \rho^2} }{ \sqrt{\delta} } - \varepsilon
     \right\}.
\end{equation*}
Recall that $C_R = \max_{(\rho, R, \beta_0) \in \bTheta_{c}} R$. Denote $\delta'_\varepsilon > \delta$ as a constant such that
\begin{equation}
\label{eq:delta_eps}
    \frac{1}{\sqrt{\delta'_\varepsilon}} := \frac{1}{\sqrt{\delta}} - \frac{\varepsilon}{C_R}.
\end{equation}
Then following \cref{eq:ERM_II0}, we have
\begin{align*}
    I\!I 
        \ge & \ \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
    \min_{ U \in 
    \mathcal{N}^\delta_n(\rho, R, \beta_0) \cap \mathcal{C}_n^\varepsilon(\rho, R, \beta_0)  }
    \E_{\Q_n}[\ell(U)]
    \\
    \stackrel{\mathmakebox[0pt][c]{\smash{\text{(i)}}}}{\ge} & \ \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
    \min_{ U \in 
    \wt{\mathcal{N}}^{\delta,\varepsilon}_n(\rho, R, \beta_0)  }
    \E_{\Q_n}[\ell(U)]
    \\
    = & \ \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
       \min_{\xi \in \cL^2(\Q_n), \norm{\xi}_{\Q_n} \le \frac{1}{\sqrt{\delta}} - \frac{\varepsilon}{R\sqrt{1 - \rho^2}}} \E_{\Q_n} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right]
    \\
    \stackrel{\mathmakebox[0pt][c]{\smash{\text{(ii)}}}}{\ge}  & \
    \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
       \min_{\xi \in \cL^2(\Q_n), \norm{\xi}^2_{\Q_n} \le 1/\delta'_\varepsilon } \E_{\Q_n} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right]
    \\
    \conp & \ 
    \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
       \min_{\xi \in \cL^2(\Q_\infty), \norm{\xi}^2_{\Q_\infty} \le 1/\delta'_\varepsilon } \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right]
    \\
    \stackrel{\mathmakebox[0pt][c]{\smash{\text{(iii)}}}}{>} & \ \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
       \min_{\xi \in \cL^2(\Q_\infty), \norm{\xi}^2_{\Q_\infty} \le 1/\delta } \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right]
    \\
    = & \ M^*( \bTheta_{c} ) = M^*,
\end{align*}
where (i) follows from \cref{eq:ERM_logit_sets}, (ii) follows from \cref{eq:delta_eps} and the fact that 
\begin{equation*}
    \frac{1}{\sqrt{\delta}} - \frac{\varepsilon}{R\sqrt{1 - \rho^2}} \le \frac{1}{\sqrt{\delta'_\varepsilon}},
    \qquad \forall\, (\rho, R, \beta_0) \in \bTheta_{c},
\end{equation*}
the convergence follows from \cref{lem:M3-star}, and (iii) follows from the uniqueness of $(\rho^*, R^*, \beta_0^*)$ and KKT conditions $\norm{\xi^*}^2_{\Q_\infty} = 1/\delta$ in \cref{lem:M_star_var}. 

Finally, combining everthing together, we have
\begin{equation*}
    \pliminf_{n \to \infty} \wt M_n^{\varepsilon(3)}
    \ge \min\left\{ \pliminf_{n \to \infty} I, \ \pliminf_{n \to \infty} I\!I \right\}
    > M^*.
\end{equation*}
This shows \cref{eq:ERM_logit_goal}, and hence completes the proof.
\end{proof}

Using an argument similar to the one at the end of the proof of \cref{lem:over_logit_conv}, we can show the convergence of empirical logit distribution $W_2 ( \hat \nu_{n}, \nu_* ) \conp 0$ from $W_2( \hat \cL_{n}, \cL_* ) \conp 0$ given by \cref{lem:ERM_logit_conv}.

\subsubsection{Completing the proof of \cref{thm:logistic_main}}
\label{subsubsec:under_final}
\begin{proof}[\textbf{Proof of \cref{thm:logistic_main}}]
    Consider the ERM problem \cref{eq:logistic_reg} with arbitrary $\tau > 0$. Recall that $\wt y_i = y_i/s(y_i)$ where $s: \{ \pm 1 \} \to \{ 1 \} \cup \{ \tau \}$ is defined as per \cref{eq:s_fun}. $M_n$ is redefined as \cref{eq:logistic_reg}
    \begin{equation*}
        M_n := \min_{\bbeta \in \R^d, \, \beta_0 \in \R}  \frac1n \sum_{i=1}^n \ell\bigl( 
        \wt y_i(\< \xx_i, \bbeta \> +  \beta_0 )
        \bigr).
    \end{equation*}
    Under this modification, $M_n(\bTheta_{\vbeta}, \bXi_{\bu})$ can be redefined and expressed as
    \begin{align*}
        M_n(\bTheta_{\vbeta}, \bXi_{\bu})
        :\! & = \min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
        \max_{ \bv \in \R^n } \left\{
        \frac1n \sum_{i=1}^n \ell \biggl( \frac{u_i}{s(y_i)} \biggr)
         + \frac{1}{n} \sum_{i=1}^n v_i \left( y_i(\< \xx_i, \bbeta \> +  \beta_0 ) - u_i \right)
         \right\} \\
        % & = \min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
        % \max_{ \bv \in \R^n } \left\{
        % \frac1n \sum_{i=1}^n \ell \biggl( \frac{u_i}{s(y_i)} \biggr)
        %  + \frac{1}{n} \sum_{i=1}^n v_i \bigl(  y_i(\< \xx_i, \bbeta \> +  \beta_0 ) - u_i \bigr)
        %  \right\} \\
        & = \min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } }
     \max_{ \bv \in \R^n }
     \left\{
     \frac1n \sum_{i=1}^n \ell \biggl( \frac{u_i}{s(y_i)} \biggr)
      + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
      + \frac1n \bv^\top \ZZ \vbeta + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu
      \right\}.
    \end{align*}
Consequently, quantities $M_n^{(k)}$, $k= 1,2,3$ and $M^*$ used in the proof can be similarly redefined as
    \begin{align*}
        M_n^{(1)}(\bTheta_{\vbeta}, \bXi_{\bu})
        & : = \smash {\min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } } }
        \max_{ \bv \in \R^n }
        \, \Biggl\{
        \frac1n \sum_{i=1}^n \ell \biggl( \frac{u_i}{s(y_i)} \biggr)
         + \frac1n \bv^\top \bone \< \bmu, \vbeta \>
         + \frac1n \norm{\bv}_2 \hh^\top \vbeta 
         \\
        & \phantom{.} \phantom{ 
            : = \smash {\min_{ \substack{ (\bbeta , \beta_0) \in \bTheta_{\vbeta} \\  \bu \in \bXi_{\bu} } } }
        \max_{ \bv \in \R^n }
        \, \Biggl\{
        }
         + \frac1n \norm{\vbeta}_2 \vg^\top \bv 
         + \frac1n \beta_0 \bv^\top \yy - \frac1n \bv^\top \bu
         \Biggr\},
        \\
        M_n^{(2)}(\bTheta_{c}, \bXi_{\bu})
        & := \min_{ (\rho, R, \beta_0) \in \bTheta_{c} }
        \min_{ U \in \Xi_{u}  \cap   \mathcal{N}_n }
        \E_{\Q_n}\left[\ell \bigl( U/s(Y) \bigr)\right],
        \\
        M_n^{(3)}(\bTheta_{c}, \bXi_{\bu}) 
        & :=  
        \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
        \min_{ U \in \Xi_{u}  \cap   \mathcal{N}^\delta_n }
        \E_{\Q_n}\left[\ell \bigl( U/s(Y) \bigr)\right],
        \\
        M_n^{(3)}(\bTheta_{c}) 
        & :=  
        \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
        \min_{ U \in \mathcal{N}^\delta_n }
        \E_{\Q_n}\left[\ell \bigl( U/s(Y) \bigr)\right],
        \\
        & \phantom{:}= \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \xi \in \cL^2(\Q_n), \norm{\xi}_{\Q_n} \le 1/\sqrt{\delta} } } 
    \E_{\Q_n} \left[ \ell \biggl( \frac{ \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi }{s(Y)} \biggr) \right],
        \\
         M^*(\bTheta_{c})
     & := \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \xi \in \cL^2(\Q_\infty), \norm{\xi}_{\Q_\infty} \le 1/\sqrt{\delta} } } 
    \E \left[ \ell \biggl( \frac{ \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi }{s(Y)} \biggr) \right],
        \\
        M^* & := M^*([-1, 1] \times \R_{\ge 0} \times \R),
    \end{align*}
    where $\mathcal{N}_n$, $\mathcal{N}^\delta_n$ are still defined as \cref{eq:set_N_n}, \eqref{eq:set_N_n_delta}, and we still apply the change of variable
    \begin{equation*}
    U = \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi.
    \end{equation*}
    One can use exactly similar arguments to conclude \cref{lem:ERM_bound_beta}---\ref{lem:M3-star} and \cref{thm:ERM_conv} with definitions above. For \cref{lem:var_fixed}, we can also get similar results, but the KKT condition in \cref{eq:KKT_xi2} now becomes
    % \begin{equation*}
    %     U = \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi,
    %         \qquad
    %     \ell' ( U/s(Y) ) + \frac{R\sqrt{1 - \rho^2}}{\lambda} \xi = 0
    % \end{equation*}
    \begin{equation*}
        U + \lambda \ell' ( U/s(Y) ) = \rho\norm{\vmu}_2 R + RG + \beta_0 Y,
    \end{equation*}
    which implies
    \begin{equation}
    \label{eq:ERM_U_new}
        \frac{U}{s(Y)} = \prox_{\ell}\left( \frac{\rho\norm{\vmu}_2 R + RG + \beta_0 Y}{s(Y)} ; \frac{\lambda}{s(Y)} \right),
    \end{equation}
    as a substitute of \cref{eq:U_prox},
    and
    \begin{equation*}
        \xi^*_{\Q}(\rho, R, \beta_0) = -\frac{\lambda}{R\sqrt{1 - \rho^2}} \ell'\left( \prox_{\ell}\left( \frac{\rho\norm{\vmu}_2 R + RG + \beta_0 Y}{s(Y)} ; \frac{\lambda}{s(Y)} \right) \right),
    \end{equation*}
    as a substitute of \cref{eq:xi_star}.

\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{thm:logistic_main(a)}:} According to the definition above, the KKT conditions \cref{lem:M_star_var} will become
\begin{align}
                - \frac{R \rho}{\lambda \delta \norm{\vmu}_2}
                & = 
                \E \left[ \wt\ell'_Y(U) \right],
                \label{eq:KKT_new1}
                \\
                \frac{R}{\lambda \delta }
                & = 
                \E \left[ \wt\ell'_Y(U) G \right],
                \label{eq:KKT_new2}
                \\
                0
                & = 
                \E \left[ \wt\ell'_Y(U) Y \right],
                \label{eq:KKT_new3}
                \\
                \frac{R^2 (1 - \rho^2)}{\lambda^2 \delta}
                & = 
                \E \left[ \bigl( \wt\ell'_Y(U) \bigr)^2 \right],
                \notag
\end{align}
where
\begin{equation*}
    \wt\ell'_Y(U) := \frac{1}{s(Y)} \ell'\left( \frac{U}{s(Y)} \right)
    = \frac{1}{s(Y)} \ell'\left( \prox_{\ell}\left( \frac{\rho\norm{\vmu}_2 R + RG + \beta_0 Y}{s(Y)} ; \frac{\lambda}{s(Y)} \right) \right)
    ,
\end{equation*}
and $U$ follows the relation \cref{eq:ERM_U_new}. By Stein's identity, \cref{eq:KKT_new2} can be expressed as
\begin{align*}
    \frac{R}{\lambda \delta } = \E \left[ \wt\ell'_Y(U) G \right]
    & = \E \left[ \frac{1}{s(Y)} \ell''\left( \frac{U}{s(Y)} \right)
    \cdot \frac{\d (U/s(Y))}{\d G} \right] \\
    & = \E \left[ \frac{1}{s(Y)} \ell''\left( \frac{U}{s(Y)} \right)
    \cdot \frac{1}{1 + \dfrac{\lambda}{s(Y)} \ell''\left( \dfrac{U}{s(Y)} \right) } \cdot \dfrac{R}{s(Y)}  \right],
\end{align*}
which gives the third KKT condition in \ref{thm:logistic_main(a)}. Besides, \cref{eq:KKT_new1} and \ref{eq:KKT_new3} can be rewritten as
\begin{align*}
    - \frac{R \rho}{\lambda \delta \norm{\vmu}_2}
    & = \pi \E\left[ \frac{1}{\tau} \ell'\left( \prox_{\ell}\left( \frac{\rho\norm{\vmu}_2 R + RG + \beta_0}{\tau} ; \frac{\lambda}{\tau} \right) \right) \right] \\
    & \phantom{=.} 
    + (1-\pi) \E\left[ \ell'\, \bigl( \prox_{\ell}\left( \rho\norm{\vmu}_2 R + RG - \beta_0 ; \lambda \right) \bigr)  \right],
    \\
    0 & = \pi \E\left[ \frac{1}{\tau} \ell'\left( \prox_{\ell}\left( \frac{\rho\norm{\vmu}_2 R + RG + \beta_0}{\tau} ; \frac{\lambda}{\tau} \right) \right) \right] \\
    & \phantom{=.}
    - (1-\pi) \E\left[ \ell'\, \bigl( \prox_{\ell}\left( \rho\norm{\vmu}_2 R + RG - \beta_0 ; \lambda \right) \bigr)  \right],
\end{align*}
which solves the first two KKT conditions in \ref{thm:logistic_main(a)}. This concludes the proof of part \ref{thm:logistic_main(a)}.


\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{thm:logistic_main(b)}:} \cref{lem:ERM_param_conv} still remains valid under arbitrary $\tau > 0$, which concludes the proof.

\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{thm:logistic_main(c)}:} Similar to the proof of \cref{thm:SVM_main}\ref{thm:SVM_main_err}, we can show that for any test point $(\xx_\mathrm{new}, y_\mathrm{new})$,
\begin{equation*}
        \hat f(\xx_\mathrm{new}) = \< \xx_\mathrm{new}, \hat\vbeta_n \> + \hat\beta_{0,n}
        \cond y_\mathrm{new} R^* \rho^* \norm{\bmu}_2 + R^* G + \beta_{0}^*,
\end{equation*}
where $(y^\mathrm{new}, G) \sim P_y \times \normal(0, 1)$. Therefore, by bounded convergence theorem, the errors have limits
\begin{align*}
        \lim_{n \to \infty} \Err_{+,n} & = \P\left( + R^*\rho^* \norm{\bmu}_2 + R^*G + \beta_{0}^* \le 0 \right)
        = \Phi \left(- \rho^* \norm{\bmu}_2  - \frac{\beta_0^*}{R^*} \right), \\
        \lim_{n \to \infty} \Err_{-,n} & = \P\left( - R^*\rho^* \norm{\bmu}_2 + R^*G + \beta_{0}^* >  0 \right)
        = \Phi \left(- \rho^* \norm{\bmu}_2  + \frac{\beta_0^*}{R^*} \right).
\end{align*}
This concludes the proof of part \ref{thm:logistic_main(c)}.

\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{thm:logistic_main(d)}:} Based on \cref{eq:ERM_U_new}, we redefine $\cL_*$ in \cref{append_subsubsec:ERM_logit} by
\begin{equation*}
    \cL_* := \Law \, (U^*)
    =
    \Law \left( 
    s(Y) \, \prox_{\ell}\left( \frac{\rho^*\norm{\vmu}_2 R^* + R^*G + \beta_0^* Y}{s(Y)}; \frac{\lambda^*}{s(Y)} \right)
    \right).
\end{equation*}
Then \cref{lem:ERM_logit_conv} and the corresponding convergence of ELD still hold. The convergence of TLD directly comes from the proof of part \ref{thm:logistic_main(c)}. This concludes the proof of part \ref{thm:logistic_main(d)}.

Finally, we complete the proof of \cref{thm:logistic_main}.
\end{proof}












\begin{comment}
\begin{proof}
    \begin{enumerate}
        \item[(a)] 
    

    \item[(b)] The expression for $\zeta (c)$ directly follows from our conclusion of part (a). We next show that $\zeta(c)$ is continuous. In fact, for any $c_1, c_2 > 0$, we have \TODO{one expression is longer than page width}
    \begin{align*}
        & \left\vert \zeta(c_1) - \zeta(c_2) \right\vert \le \, \max_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le c_1} \left\vert \mathscr{R}_{\Q}(\xi) - \mathscr{R}_{\Q} \left( \sqrt{\frac{c_2}{c_1}} \xi \right) \right\vert \\
        \le \, & \max_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le c_1} \E_{\Q} \left[ \left\vert \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) - \ell \left( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \sqrt{\frac{c_2}{c_1}} \xi \right) \right\vert \right] \\
        \stackrel{(i)}{\le} \, & \max_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le c_1} L \E_{\Q} \left[ \left( 1 + 2 \left\vert \rho\norm{\vmu}_2 R + RG + \beta_0 Y \right\vert + R\sqrt{1 - \rho^2} \left( 1 + \sqrt{\frac{c_2}{c_1}} \right) \xi \right) \left\vert 1 - \sqrt{\frac{c_2}{c_1}} \right\vert \xi \right] \\
        \stackrel{(ii)}{\le} \, & \max_{\xi \in \cL^2(\Q), \norm{\xi}_{\Q}^2 \le c_1} L \left(\rho, \norm{\vmu}_2, R, \beta_0 \right) \left\vert 1 - \sqrt{\frac{c_2}{c_1}} \right\vert \E_{\Q} [\xi^2]^{1/2} = \, C \left(\rho, \norm{\vmu}_2, R, \beta_0 \right) \left\vert \sqrt{c_1} - \sqrt{c_2} \right\vert,
    \end{align*}
    where in $(i)$ we use our assumption that $\ell$ is pseudo-Lipschitz, and $(ii)$ follows from Cauchy--Schwarz inequality. This establishes the continuity of $\zeta(c)$.
    \end{enumerate}
\end{proof}
\end{comment}








\begin{comment}
\begin{proof}
    \begin{enumerate}
        \item[(a)] 
        When $\Q = \Q_\infty$ and $\ell \in C^2(\R)$, by Stein's Formula we can simplify
        \[
        \begin{aligned}
            \E \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 ) \bigr) \right] & = - \frac{\rho R}{2\pi \lambda \delta \norm{\vmu}_2},
            \\
            \E \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG - \beta_0 ) \bigr) \right] & = - \frac{\rho R}{2(1 - \pi) \lambda \delta \norm{\vmu}_2},
            \\
            \E \left[ \frac{\ell''\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr)}{1 + \lambda \ell''\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr)} \right] & =  \frac{1}{\lambda \delta },
            \\
            \E \left[ \left(\ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) \right)^2 \right] & = \frac{R^2 (1 - \rho^2)}{\lambda^2 \delta}.
        \end{aligned}
        \]
        Solve the system of equations for $(\rho, R, \beta_0, \lambda)$. 
    \end{enumerate}
\end{proof}
\end{comment}



% \noindent
% Now, recall that
% \begin{equation*}
%     \begin{aligned}
%         M_n^{(3)}(\bTheta_{c})
%     & = \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \xi \in \cL^2(\Q_n), \norm{\xi}^2_{\Q_n} \le 1/\delta } } 
%     \E_{\Q_n} \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right], \\
%     M^{*}(\bTheta_{c})
%     & = \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c} \\ \xi \in \cL^2(\Q_\infty), \norm{\xi}^2_{\Q_\infty} \le 1/\delta } } 
%     \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \bigr) \right].
%     \end{aligned}
% \end{equation*}
% Let $M^{*} := M^{*}( [-1, 1] \times \R_{\ge 0} \times \R ) = \mathscr{R}^*_{\Q_\infty}$ be the unconstrained asymptotic minimum value.

% \begin{lem}
%     For any compact subset $\bTheta_{c} \subset [-1, 1] \times \R_{\ge 0} \times \R$, as $n \to \infty$, we have
%     \begin{equation*}
%         M_n^{(3)}(\bTheta_{c}) \conp M^{*}(\bTheta_{c}).
%     \end{equation*}
%     In particular, if $\delta > \delta^*(0)$ and $\bTheta_{c} = [-1, 1] \times [0, C_R] \times [-C_{\beta_0}, C_{\beta_0}]$, then we have
%     \begin{equation*}
%         M_n^{(3)}(\bTheta_{c}) \conp M^{*},
%     \end{equation*}
%     for $C_R, C_{\beta_0} \in (0, \infty)$ large enough.
% \end{lem}
% \begin{proof}
%     For any $\lambda > 0$ and probability measure $\Q$, define
%     \begin{equation*}
%         \mathscr{R}_{\lambda, \Q}(\rho, R, \beta_0)
%         =
%         - \frac{R^2(1 - \rho^2)}{2 \delta \lambda}
%         +
%         \E_{\Q} \left[ \envelope_{\lambda\ell} ( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \right]
%     \end{equation*}
%     Then according to {\color{red} Lemma B.8 (c)}, 
%     \begin{equation*}
%         M_n^{(3)}(\bTheta_{c}) =
%         \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c}\\ \lambda \ge 0 } } 
%         \mathscr{R}_{\lambda, \Q_n}(\rho, R, \beta_0),
%         \qquad
%         M^*(\bTheta_{c}) =
%         \min_{ \substack{ (\rho, R, \beta_0) \in \bTheta_{c}\\ \lambda \ge 0 } } 
%         \mathscr{R}_{\lambda, \Q_\infty}(\rho, R, \beta_0).
%     \end{equation*}
%     We have the following facts:
%     \begin{itemize}
%         \item $\bTheta_{c}$ is compact. $(\rho, R, \beta_0) \mapsto \envelope_{\lambda\ell} ( \rho\norm{\vmu}_2 R + RG + \beta_0 Y )$ is continuous in $\bTheta_{c}$ for each $(G, Y)$, and $(G, Y) \mapsto \envelope_{\lambda\ell} ( \rho\norm{\vmu}_2 R + RG + \beta_0 Y )$ is measurable for each $(\rho, R, \beta_0)$
%         \item {\color{blue}Uniform measurable}.
%     \end{itemize}
%     Therefore, by ULLN \cite[Lemma 2.4]{newey1994large}, for each $\lambda > 0$, we have
%     \begin{equation*}
%         \begin{aligned}
%             & \abs{\min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
%             \mathscr{R}_{\lambda, \Q_n}(\rho, R, \beta_0)
%             -
%             \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
%             \mathscr{R}_{\lambda, \Q_\infty}(\rho, R, \beta_0)
%             }
%             \\
%             = {} & \sup_{(\rho, R, \beta_0) \in \bTheta_{c}} \, \bigl| 
%                 \E_{\Q_n} \left[ \envelope_{\lambda\ell} ( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \right]
%                 -
%                 \E_{\Q_\infty} \left[ \envelope_{\lambda\ell} ( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \right]
%             \bigr|
%             \conp 0.
%         \end{aligned}
%     \end{equation*}
%     Note that function $\lambda \mapsto \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
%             \mathscr{R}_{\lambda, \Q_n}(\rho, R, \beta_0)$ is convex. Moreover, {\color{blue} (details)}
%     \begin{equation*}
%         \lim_{\lambda \to 0^+} \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
%         \mathscr{R}_{\lambda, \Q_\infty}(\rho, R, \beta_0) \to -\infty.
%     \end{equation*}
%     Then we may use {\color{blue} [TAH18, Lem. 10]} to conclude that
%     \begin{equation*}
%         M_n^{(3)}(\bTheta_{c}) = \inf_{\lambda > 0} \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
%         \mathscr{R}_{\lambda, \Q_n}(\rho, R, \beta_0)
%         \conp 
%         M^{*}(\bTheta_{c}) =  \inf_{\lambda > 0} \min_{ (\rho, R, \beta_0) \in \bTheta_{c} } 
%         \mathscr{R}_{\lambda, \Q_\infty}(\rho, R, \beta_0) .
%     \end{equation*}
%     {\color{blue} Lemma B.9} implies that $R^*, \abs{\beta_0^*} < \infty$, which completes the proof.
% \end{proof}









% \noindent
% We denote the system of equations \cref{eq:sys_eq_Q} as
% \[ \E_{\Q_n}\left[ V_i(\rho, R, \beta_0, \lambda) \right] = 0,
% \qquad i = 1, 2, 3, 4, \]
% for some functions $V_i: (0, 1) \times \R_{>0} \times \R \times \R_{> 0} \to \R$.

% \begin{lem}
%     For any probability measure $\Q$, recall the optimization problem $\mathscr{R}^*_{\Q}$ in \cref{eq:var_optim_Q}. Let $(\rho^*(\Q), R^*(\Q), \beta_0^*(\Q), \lambda^*(\Q))$ be the unique solution to \cref{eq:sys_eq_Q}. {\color{red} non-separable regime condition.}
%     \begin{enumerate}
%         \item[(a)] For $i = 1,2,3$, we have
%         \begin{equation*}
%             \plim_{n \to \infty} \sup_{(\rho, R, \beta_0, \lambda) \in \mathcal{S}} \abs{ \E_{\Q_n}\left[ V_i(\rho, R, \beta_0, \lambda) \right] - \E_{\Q_\infty}\left[ V_i(\rho, R, \beta_0, \lambda) \right] }
%              = 0.
%         \end{equation*}
%         \item[(b)] We have parameter convergence
%         \begin{equation*}
%             \plim_{n \to \infty} \, \bigl(\rho^*(\Q_n), R^*(\Q_n), \beta_0^*(\Q_n), \lambda^*(\Q_n)\bigr)
%              = \bigl( \rho^*(\Q_\infty), R^*(\Q_\infty), \beta_0^*(\Q_\infty), \lambda^*(\Q_\infty) \bigr).
%         \end{equation*}
%         As a consequence, $\mathscr{R}^*_{\Q_n} \conp \mathscr{R}^*_{\Q_\infty}$.
%     \end{enumerate}
% \end{lem}

% \begin{proof}
%     \begin{enumerate}
%         \item[(a)] $\mathcal{S} = [0, 1] \times [0, M] \times [-M, M] \times [0, M]$ is a compact set, (by previous arguments) we claim $R^*(\Q_n), \beta_0^*(\Q_n), \lambda^*(\Q_n)$ are asymptotically bounded {(\color{blue} existence of $M >0$)}.
        
%         If each $V_i$ is integrable uniformly over $(\rho, R, \beta_0, \lambda)$ (a strong sufficient condition: $\ell$ is Lipschitz, so $\abs{\ell'} \le c$), the result is followed by ULLN.
        
%         \item[(b)] Exactly the same approach in \cite{montanari2023generalizationerrormaxmarginlinear} page 45.
%     \end{enumerate}
% \end{proof}