\section{Related work}


\paragraph{Imbalanced classification.} In the classical literature on logistic regression with class imbalance, it is known that large-sample asymptotics is inaccurate with small sample sizes and thus bias correction formulas are derived \cite{schaefer1983bias, anderson1979logistic, mccullagh1983generalized}. Under label shift (also known as choice-based sampling), intercept correction and upweighting are developed to address imbalanced classes \cite{xie1989logit, king2001logistic}. For kernel methods and tree-based methods, additional techniques such as oversampling, undersampling, and synthetic data generation are used to mitigate limited minority samples \cite{he2009learning, chawla2002smote, he2008adasyn}. However, these methods are typically ineffective for separable data \cite{cao2019learning}, nor do they aim to address overfitting in high dimensions.

%% Label shift: in classical literature, also known as endogenous sampling or choice-based sampling, in contrast to random sampling or exogeneous sampling

\paragraph{Margin-based methods.} Margin plays an important role in classification methods such as SVM. For imbalanced classification, promoting unequal margins is proposed for the perceptron algorithm \cite{li2002perceptron} and SVM \cite{li2005using}, and more recently for training deep neural networks \cite{huang2016learning, khan2019striking, liu2019large, cao2019learning}. For theoretical analysis, 
% an earlier work \cite{koltchinskii2002empirical} obtained margin-dependent generalization bounds, 
many earlier works \cite{bartlett1996valid, bartlett1998boosting, koltchinskii2002empirical, bartlett2002rademacher, bartlett2017spectrally} obtained margin-dependent generalization bounds for a variety of classifiers and algorithms,
showing a large margin is beneficial to good generalization.
%\ljycom{I think some high cited works by Peter Bartlett are worth to mention. They are also cited by Koltchinskii and Tengyu Ma.}
% This theoretical result 
In particular, the theoretical result \cite{koltchinskii2002empirical}
is used by \cite{cao2019learning} to motivate the margin-rebalancing loss function.
However, the margin-dependent bounds are agnostic to data distributions and may be excessively conservative. 

\paragraph{High-dimensional asymptotics.} Classical asymptotic analysis for fixed dimensions or low dimensions are known to be inaccurate \cite{el2013robust, donoho2016high}. A line of recent work studies estimation and inference for high-dimensional classification problems, e.g., \cite{dobriban2018high, Pragya_highdim_logistic, sur2019logistic, candes2020logistic, montanari2023generalizationerrormaxmarginlinear, kini2021label, deng2022model, mignacco2020role, salehi2019impact, montanari2024tractability}, which refine Table~\ref{tab:1} in various ways. One core technique is Gordon's theorem, which reduces a random-matrix-based minimax problem to a simpler form. Our proofs are mainly based on this technique. Most relevant to this paper is \cite{montanari2022overparametrizedlineardimensionalityreductions}, where the authors studied projection pursuit of high-dimensions data and characterized attainable asymptotic low-dimensional distributions. However, these papers do not offer a clear characterization of the impact of class imbalance on overfitting, nor do they study the high imbalance regime or calibration.




