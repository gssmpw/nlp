\section{Discussions}\label{sec:discuss}

%Our theory and analysis are mainly focused on SVM and logistic regression in high dimensions.

Below we present two possible extensions of our main results stated in previous sections.

\paragraph{Multiclass classification.} Our theory does not cover classification with $K > 2$ classes. However, we believe that for multiclass classification, the limiting empirical distribution of the logits is a similar ``rectified Gaussian" in $\R^K$. In what follows, we present some empirical evidences and an informal conjecture on the logits distribution.

In the $K$-class case, we observe features $\xx_i \in \R^d$ and labels $y_i \in [K] \sim P_y$, where the expected fractions of each class is $\pi_k := \P(y_i = k)$, $k \in [K]$. Given $y_i = k$, the conditional distribution of $\xx_i$ is $\normal (\vmu_k, \bI_d)$, where $\{ \vmu_k \}_{k \in [K]}$ are the class means. Let $\hat\vf(\xx) = \hat\bW \xx + \hat \vw_0$ be the logits of multinomial logistic regression for $\{ (\xx_i, y_i) \}_{i=1}^{n}$, where $\hat\bW \in \R^{K \times d}$, $\hat \bw_0 \in \R^K$ are weights. The prediction is given by $\hat y(\xx) := \argmax_{k \in [K]} \hat f_k(\xx)$, where $\hat f_k(\xx)$ is the logit of $\xx$ for label $k$, i.e., the $k$-th component of $\hat\vf(\xx)$. Similar to the binary case discussed in \cref{subsec:LR_vs_SVM}, we expect similar connections between multiclass SVM and multinomial logistic regression exist. 
% the gradient descent iterates of multinomial logistic regression weights converge to the direction of the max-margin ($K$-class SVM) solution on separable data \cite{Soudry_implicit_bias}.
% We consider both logistic regression and SVM:
% \begin{subequations}
% \begin{align}
% \begin{array}{lcl}
%     \text{(logistic)} 
%     & 
%     \mathmakebox[\widthof{subject to}][c]{
%     \minimize \limits_{\bW \in \R^{K \times d}, \ww_0 \in \R^K}
%     } 
%     & 
%     \mathmakebox[\widthof{$ \displaystyle \< \xx_i, \bw_{y_i} \> \ge \< \xx_i, \bw_{k} \> + 1,
% 	\quad \forall\, i \in [n],  \ \  \forall\, k \not= y_i.$}][l]{
%     \displaystyle 
%     \cL(\bW) :=
%     - \frac{1}{n} \sum_{i=1}^n \log \left( \frac{\exp\bigl(\< \xx_i, \bw_{y_i} \> + w_{0, y_i} \bigr)}{\sum_{k=1}^K \exp\bigl(\< \xx_i, \bw_{k} \> + w_{0, k} \bigr)} \right),
%     }
% \end{array}
% \label{eq:multi-logistic}
% \\
% \begin{array}{lcl}
%     \text{\makebox[\widthof{(logistic)}][r]{(SVM)}}  & 
%     \minimize \limits_{\bW \in \R^{K \times d}, \ww_0 \in \R^K} &  \norm{\bW}_\mathrm{F}^2,  \\
%     & \text{subject to} &  \< \xx_i, \bw_{y_i} \> + w_{0, y_i} \ge \< \xx_i, \bw_{k} \> + w_{0, k} + 1,
% 	\quad \forall\, i \in [n],  \ \  \forall\, k \not= y_i.
% \end{array}
% \label{eq:multi-SVM}
% \end{align}
% \end{subequations}

%i.e., the $k$-th entry of $\hat\vf(\xx) \in \R^K$. 
%For linearly separable data, it turns out the empirical distribution of $\{\bigl(\hat f_{k}(\xx_i), \hat f_{k'}(\xx_i)\bigr)\}_{i = 1}^n$ also has a \emph{truncation} phenomenon in $\R^2$, where $k \not= k' \in [K]$.

We first conduct some numerical experiments to demonstrate that the truncation effect is likely generalizable to $K > 2$. In \cref{fig:multiclass}, we present the density heatmaps of joint logits $\bigl(\hat f_{1}(\xx_i), \hat f_{k}(\xx_i)\bigr)$ in both simulations and real-data analysis, where all input features $\xx_i$ are from class $1$. More specifically, in our simulation, we consider 3-component GMM with $\pi_1 = 0.5$, $\pi_2 = 0.3$, $\pi_3 = 0.2$, $n=50,000$, $d=6,000$, and class centers are randomly generated in $\R^d$ from the $\cL^2$-sphere (at the origin with radius $4$). 
For real data, we consider CIFAR-10 image dataset preprocessed by the pretrained ResNet-18. We undersample to obtain an imbalanced dataset with sample size 500, 223, 100 for each class 1, 2, 3. 
Then, we train a multinomial logistic regression (with ridge regularization parameter $\lambda = 10^{-8}$) after prewhitening the features. Similar to the linear separable regime, We find that training accuracy is $100\%$ with high probability. For both experiments, we plot the joint logits with $k = 2, 3$ for class 1. 

Notably, we observe similar truncation phenomena for $3$-class classification on both synthetic and real data, where the Gaussian density is visibly truncated by two hyperplanes.
%In particular, the Gaussian density is visibly truncated by two hyperplanes, although this phenomenon seems to be more prominent in the synthetic experiment. 
For general $K \ge 3$, we conjecture that the empirical joint distribution of the logits $\{ \hat\vf(\xx_i) \}_{i=1}^n$ is asymptotically a multivariate Gaussian projected to a convex polytope in $\R^K$, where specific parameters of this limiting distribution depends on certain variational problem analogous to \Cref{eq:asymp}.



%some randomly generated center of each class. We choose $k^* = 1$ and $k = 2, 3$.

%In \cref{fig:multiclass}, we show the density heatmap of joint logits $\bigl(\hat f_{k^*}(\xx_i), \hat f_{k}(\xx_i)\bigr)$ in both simulation and real data, where all these $\xx_i$'s have true label $k^*$. For simulation we consider three-component Gaussian mixture model (3-GMM), with $\pi_1 = 0.5$, $\pi_2 = 0.3$, $\pi_3 = 0.2$, $n=50000$, $d=6000$, and some randomly generated center of each class. We choose $k^* = 1$ and $k = 2, 3$. 

%The key observation is:
%\begin{itemize}
%    \item The \emph{marginal} empirical logit distribution of $\hat f_{k^*}(\xx_i)$ for its true label $k^*$ is still a 1D Gaussian with truncation (i.e., rectified Gaussian distribution).
%    \item The \emph{joint} empirical logit distribution of $\bigl(\hat f_{k^*}(\xx_i), \hat f_{k}(\xx_i)\bigr)$ is a 2D Gaussian with truncations at two angles.
%\end{itemize}
%For real data we consider CIFAR-10 image dataset preprocessed by pretrained ResNet-18. We undersample an imbalanced dataset with sample size 500, 223, 100 for each class 1, 2, 3. We train a multinomial logistic regression after prewhitening. Notably, similar truncation phenomenon can be observed.

%Our experiment suggests that our theory could be generalized to multiclass cases. For separable data in high dimension, it is reasonable to speculate that the overfitting effect of a $K$-class classification can be characterized by some particular truncations of the joint logits in $\R^{K-1}$.


% \begin{figure}[h]
%     \centering
%     {\small \textbf{Empirical Logits of Class 2 in 3-GMM Simulation}}
%     \par
%     \includegraphics[height=0.31\textwidth]{Figs/GMM class2 (30,-60).pdf}
%     \includegraphics[height=0.32\textwidth]{Figs/GMM class2 (90,-90).pdf}
%     \includegraphics[height=0.31\textwidth]{Figs/GMM class2.pdf}
%     \par
%     \vphantom{a}
%     \par
%     {\small \textbf{Empirical Logits of Class 1 in CIFAR-10 Dataset}}
%     \par
%     \includegraphics[height=0.31\textwidth]{Figs/CIFAR10 class1 (30,-60).pdf}
%     \includegraphics[height=0.32\textwidth]{Figs/CIFAR10 class1 (90,-90).pdf}
%     \includegraphics[height=0.31\textwidth]{Figs/CIFAR10 class1.pdf}
%     \caption{
%     Empirical logit distribution of multinomial logistic regression. \textbf{Left:} 2D histogram of the joint logits for two labels (including a true label). \textbf{Middle:} top view of the 2D histogram (density heatmap). \textbf{Right:} 1D histogram of the logits for its true label.
%     }
%     \label{fig:multiclass}
% \end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{Figs/multiclass-GMM_1_2_3.pdf}
    \includegraphics[width=0.49\textwidth]{Figs/multiclass-CIFAR10_1_2_3.pdf}
    \caption{
    \textbf{Joint empirical logit distributions of multinomial logistic regression.} The heatmaps display empirical joint logits $\bigl(\hat f_{1}(\xx_i), \hat f_{k}(\xx_i)\bigr)$ for features $\xx_i$ from class 1, where $k=2,3$. Overlaid Gaussian density contours (dashed curves) depict testing logit distributions. 
    \textbf{Left:} 3-GMM simulation. \textbf{Right:} CIFAR-10 image features  preprocessed by pretrained ResNet-18.
    }
    \label{fig:multiclass}
\end{figure}

\paragraph{Non-isotropic covariance.} We also provide a characterization of the empirical logits distribution under a more general covariance assumption than \eqref{model}. In particular, we assume that the data $\{ (\xx_i, y_i) \}_{i = 1}^{n}$ are i.i.d. satisfying
\begin{equation}\label{spiked_model}
    \P(y_i = +1) = \pi, \quad \P(y_i = -1) = 1 - \pi, \quad \xx_i \,|\, y_i \sim \normal(y_i \bmu, \boldsymbol{\Sigma}),
\end{equation}
where $\boldsymbol{\Sigma} = q^2 \boldsymbol{V} \boldsymbol{V}^\top + \id_d$, and the spike $\boldsymbol{V}$ is a $d \times J$ orthogonal matrix. For this model, we have the following analogous result on the empirical logits distribution of SVM:
\begin{conj}
    Assume that $n/d \to \delta$,  $J/d \to \psi_1$ as $n, d, J \to \infty$. Denote $\psi_2 = 1 - \psi_1$, and further assume that
    \begin{equation*}
        \lim_{n \to \infty} \norm{\boldsymbol{V}^\top \bmu} = c_1, \quad \lim_{n \to \infty} \sqrt{ \norm{\boldsymbol{\mu}}^2 - \norm{\boldsymbol{V}^\top \bmu}^2 } = c_2.
    \end{equation*}
    Let $(\hat{\boldsymbol{\beta}}, \hat{\beta}_0)$ be the max-margin solution to \cref{eq:SVM-0}. Then, as $n \to \infty$, 
    \begin{equation*}
        %\lim_{n \to \infty} 
        W_2 \left( \frac{1}{n} \sum_{i=1}^{n} \delta_{( y_i, \<\xx_i, \hat{\boldsymbol{\beta}}\> + \hat{\beta}_0 )}, \, \Law \left( Y, Y \rho_1^* c_1 + Y \rho_2^* c_2 + \sqrt{1 + q^2} r_1^* G_1 + r_2^* G_2 + \beta_0^* \right) \right) \conp \, 0.
    \end{equation*}
    In the above display, $Y \sim P_y$ is independent of $(G_1, G_2) \sim \normal(0, 1)^{\otimes 2}$, and $(\rho_1^*, \rho_2^*, r_1^*, r_2^*, \beta_0^*)$ solves the following convex optimization problem:
    %\TODO{Is it necessarily convex?} 
    %\KZ{I can prove it's convex, if necessary we can include the proof in appendix}
    \begin{equation}\label{spike_covariance_opt}
    \begin{split}
        \maximize_{\rho_1, \rho_2, r_1, r_2, \beta_0, \kappa} \quad & \kappa, \\
        \mathrm{subject \, to} \quad & \E \left[ \left( \kappa - \rho_1 c_1 - \rho_2 c_2 - \sqrt{1 + q^2} r_1 G_1 - r_2 G_2 - \beta_0 Y \right)_+^2 \right] \\
        & \le \frac{1}{\delta} \left( \sqrt{1 + q^2} \sqrt{r_1^2 - \rho_1^2} \sqrt{\psi_1} + \sqrt{r_2^2 - \rho_2^2} \sqrt{\psi_2} \right)^2, \\
        & r_1^2 + r_2^2 = 1, \, \rho_1^2 \le r_1^2, \, \rho_2^2 \le r_2^2.
    \end{split}
    \end{equation}
\end{conj}


\paragraph{Future work.} In deep learning, the features are learned by optimizing the loss over all weights in a neural network, and data imbalance impacts on feature learning in a complex way as observed in \cite{cao2019learning}. Also, models tend to erroneously find spurious features if data imbalance is severe \cite{sagawa2020investigation}. It would be interesting to analyze overfitting and propose remedies for these scenarios.

