\section{Analysis of margin rebalancing for separable data} \label{sec:rebalacing}


In this section, we show how margin rebalancing improves the test accuracies on imbalanced dataset by choosing the hyperparameter $\tau$ in \cref{fig:SVM_cartoon} appropriately. 


\subsection{Proportional regime}
\label{subsec:rebal_prop}
Consider the same 2-GMM and proportional settings in \cref{sec:logit_SVM} on linearly separable dataset ($\delta < \delta^*(0)$). According to \cref{thm:SVM_main}\ref{thm:SVM_main_err}, the asymptotic minority and majority test errors are
\begin{equation}\label{eq:asymp_Err}
    \Err_{+}^*  :=  \Phi \left(- \rho^* \norm{\bmu}_2 - \beta_0^* \right),
            \qquad
    \Err_{-}^*  :=  \Phi \left(- \rho^* \norm{\bmu}_2  + \beta_0^* \right).
\end{equation}
For the purpose of imbalanced classification, we define the \textit{asymptotic balanced error} as
\begin{equation*}
\Err_\mathrm{b}^* := \frac{1}{2} \Err_{+}^* + \frac{1}{2} \Err_{-}^*.
\end{equation*}

\paragraph{Monotonicity analysis.} 
% Before delving into the optimal $\tau$, 
We first provide some monotone results for test errors, which support our empirical observations in \cref{subsec:rebal}. 

% The following two preliminary results give the monotonicity of asymptotic parameters $\rho^*$ (cosine angle) and $\beta_0^*$ (intercept).

% Recall that $\rho^*$ is the unique solution to \cref{eq:SVM_variation}. According to \cref{thm:SVM_main}\ref{thm:SVM_main_var}, $\rho^* \in (0, 1)$ is invariant with respect to $\tau$. Hence $\rho^*$ can be viewed as a function of model parameters $(\pi, \norm{\bmu}_2, \delta)$ given by \cref{eq:SVM_variation}.

% \begin{prop}\label{prop:rho_mono}
% 	Let $\rho^*$ be the asymptotic cosine angle between $\bmu$ and $\hat\vbeta$ as defined in \cref{thm:SVM_main}. Then $\rho^* = \rho^* (\pi, \norm{\bmu}_2, \delta)$ is an increasing function of $\pi \in (0, \frac12)$, $\norm{\bmu}_2$, and $\delta$.
% \end{prop}

% Similarly, $\beta_0^*$ can also be viewed as a function of $(\pi, \norm{\bmu}_2, \delta)$ when $\tau$ is fixed.

% \begin{prop}\label{prop:beta0_mono}
% 	Let $\beta_0^*$ be the asymptotic intercept as defined in \cref{thm:SVM_main} without margin rebalancing ($\tau = 1$). Then $\beta_0^* = \beta_0^* (\pi, \norm{\bmu}_2, \delta)$ is an increasing function of $\pi \in (0, \frac12)$, $\delta$, and $\norm{\bmu}_2$.
% \end{prop}

% Combing these two propositions with \cref{eq:asymp_Err}, we have the following immediate result:

\begin{prop}\label{prop:Err-_mono}
    % The minority test error 
    $\Err_{+}^*$ is a decreasing function of $\pi \in (0, \frac12)$, $\norm{\bmu}_2$, and $\delta$ when $\tau = 1$.
\end{prop}

However, the majority error $\Err_{-}^*$ and balanced error $\Err_\mathrm{b}^*$ are not necessarily monotone under arbitrary $\tau$. Thus, we will focus on the monotonicity of these test errors when $\tau$ is chosen to be optimal.

% \ljycom{Can we show that $\Err_{+}^*$ is monotone increasing in $\pi \in (0, \frac12)$, $\delta$, and $\norm{\bmu}_2$ when $\tau = 1$?}

According to \cref{fig:SVM_cartoon} and \ref{fig:Err_tau}, by taking $\tau > 1$, we can improve the minority accuracy at the cost of harming majority accuracy. The opposite effects of $\tau$ on $\Err^*_+$ and $\Err^*_-$ are summarized in the following result.

\begin{prop}\label{prop:tau_mono}
    $\Err^*_+$ is decreasing in $\tau \in (0, \infty)$, and $\Err^*_-$ is increasing in $\tau \in (0, \infty)$.
\end{prop}

\paragraph{Choosing the optimal $\tau$.} A natural idea for margin rebalancing is to choose $\tau$ such that the balanced error $\Err_\mathrm{b}^*$ is minimized.

\begin{prop}[Optimal $\tau$] \label{prop:tau_optimal}
    Let $\tau^\mathrm{opt}$ be the optimal margin ratio $\tau$ defined in \cref{prop:tau_opt}. Denote $g_1 (x) := \E \left[ (G + x)_+ \right]$ where $G \sim \normal(0, 1)$. Then $\tau^\mathrm{opt}$ has the 
 explicit expression 
    %which minimizes the balanced error
%\begin{equation}
    %\tau^\mathrm{opt} :=  \argmin_{\tau \in \R} \Err_\mathrm{b} = \argmin_{\tau \in \R} \big\{ \Phi(- \rho^* \norm{\bmu}_2  - \beta_0^* )
    %+ \Phi(- \rho^* \norm{\bmu}_2 + \beta_0^* ) \big\}.
%\end{equation}
%When $\tau = \tau^\mathrm{opt}$, we have $\beta_0^* = 0$ and $\Err_+^* = \Err_-^* = \Err_\mathrm{b}^*$. In particular, $\tau^\mathrm{opt}$ has explicit expression
\begin{equation}\label{eq:tau_opt}
        \tau^\mathrm{opt} =  \dfrac{g_1^{-1} \left( \dfrac{\rho^*}{2 \pi \norm{\bmu}_2 \delta} \right) + \rho^* \norm{\bmu}_2}{g_1^{-1} \left( \dfrac{\rho^*}{2 (1 - \pi) \norm{\bmu}_2 \delta} \right) + \rho^* \norm{\bmu}_2}.
\end{equation}
\end{prop}
\begin{rem}\label{rem:tau_pi}
    % In contrast with \cite{cao2019learning} where the suggested $\tau$ scales with $\pi^{-1/4}$, 
    The optimal choice of $\tau$ has a complicated dependence on $\pi$. However, we note that the numerator scales as $\tau^\mathrm{opt} \asymp \sqrt{1/\pi}$ for small $\pi$ and fixed $\norm{\bmu}_2$ and $\delta$, which is consistent with the choice of $\tau$ in importance tempering \cite{lu2022importance}. In \cite{cao2019learning}, $\tau$ is suggested to scale with $\pi^{-1/4}$, however it was proved in \cite{kini2021label} that their algorithm won't converge to the solution with the desired $\tau$.

    It is worth noticing that in the near-degenerate cases where $\pi$ is very small or $\norm{\bmu}_2$, $\delta$ are very large, then $\rho^*$ is close to $0$ and the denominator can be negative,
    % where $\rho^*$ is close to $0$ or $\delta$ is very large, then the denominator can be negative, 
    leading to $\tau^\mathrm{opt} < 0$. While our theory (\cref{prop:Err_monotone}, \cref{prop:conf}) is still valid when we allow potential negative $\tau$, it is rarely used in practice. See \cref{subsec:tau_optimal} for a further discussion. 
    % In such cases, by slightly modifying the margin-rebalanced SVM problem \cref{eq:SVM-m-reb}, we can make all our theoretical results (\cref{prop:SVM_tau_relation} and \cref{thm:SVM_main}) still applicable. \TODO{Wait?! Does it mean we need additional assumptions in the previously stated results? Perhaps we just say we assume $\tau>0$ in previous results, and for convenience allow negative $\tau$ for this section and next section when we state monotone result?} \ljycom{We don't need additional assumption. For $\tau < 0$, I think we only need to change the objective in \cref{eq:SVM-m-reb} to $\minimize \kappa$ (and then we will get $\kappa^* < 0$). Then we consider the corresponding asymptotic problem, and \cref{prop:SVM_tau_relation}, \cref{thm:SVM_main} still hold. I agree with the suggestions.}
    The near-degenerate cases (small $\pi$, large $\norm{\bmu}_2$ or $\delta$) are better addressed under the high imbalance regime, as we analyze in the next subsection. 
\end{rem}
The minority/majority/balanced errors all equal $\Phi(- \rho^* \norm{\bmu}_2 )$ when $\tau = \tau^\mathrm{opt}$. 
% Using \cref{prop:rho_mono}, 
We can also obtain the monotonicity of test errors after margin rebalancing.
\begin{prop}\label{prop:Err_monotone}
    When $\tau = \tau^{\rm opt} > 0$, all the test errors $\Err^*_+$, $\Err^*_-$, $\Err^*_\mathrm{b}$ are decreasing functions of $\pi \in (0, 1/2)$ (imbalance ratio), $\delta$ (aspect ratio), and $\norm{\bmu}_2$ (signal strength).
\end{prop}


































\subsection{High imbalance regime}\label{subsec:high-imb}

Different from the proportional regime considered in \cref{sec:logit} and \ref{subsec:rebal_prop}, here we focus on a high-imbalanced scenario where $\pi$ is small, $\norm{\bmu}_2$ is large, and $n$ grows much faster than $d$. In this regime, we can even extend the feature distribution beyond Gaussian, and generalize the 2-GMM settings.

\begin{defn}[High imbalance]
We say a dataset $\{(\xx_i, y_i)\}_{i = 1}^n$ is i.i.d.~generated from a \emph{two-component sub-gaussian mixture model (2-subGMM)} if for any $i \in [n]$,
\begin{enumerate}
    \item[i.] Label distribution: $\P(y_i = +1) = 1 - \P(y_i = -1) = \pi$, 
    \item[ii.] Feature distribution: $\xx_i = y_i \bmu + \zz_i$, where $\zz_i$ has independent coordinates with uniformly bounded sub-gaussian norms. Namely, each coordinate $z_{ij}$ of $\zz_i$ satisfies $\E[z_{ij}] = 0$, $\var(z_{ij}) = 1$, and $\norm{z_{ij}}_{\psi_2} := \inf \{ K > 0: \E[\exp(X^2/K^2)] \le 2 \} \le C$ for all $j \in [d]$, where $C$ is an absolute constant.
    % $\zz_i \sim \subGind(\bzero, \bI_d; K)$, and $K$ is an absolute constant.
\end{enumerate}
For any constants $a,b,c >0$, we say a 2-subGMM is \emph{$(a,b,c)$-imbalanced} if \cref{setup-high-imbalance} holds.
% \begin{equation*}
% \pi = d^{-a}, \qquad \norm{\vmu}_2^2 = d^b, \qquad n = d^{c+1}.
% \end{equation*}
\end{defn}
\begin{rem}
    % When $\pi \to 0$, we require the condition $\norm{\vmu} \to \infty$ to make the classification problem workable. 
    Parameters $a$, $\frac{b}{2}$, and $c$ each specifies the degenerate rate of imbalance ratio $\pi$, and the growth rate of signal strength $\| \bmu \|_2$, aspect ratio $n/d$. We usually require $a < c + 1$ to make sure the minority class sample size $n_+ := \pi n = d^{c - a + 1} \to \infty$ does not degenerate.
\end{rem}

Our goal is to study the performance of margin-rebalanced SVM \cref{eq:SVM-m-reb} in this high imbalance regime, asymptotically as $d \to \infty$. Therefore, we allow $\tau = \tau_d$ depends on dimension $d$ and care about what order of $\tau_d$ would make the test errors vanish. We summarize our findings in the following theorem, which is consistent with the empirical observations in \cref{fig:High_imb_heat} and extends \cref{thm:high-imbalance} to the case of imbalanced 2-subGMM.

\begin{thm}[Phase transition in high imbalance regime]\label{thm:main_high-imbal}
Consider the high imbalance regime where the training data is i.i.d.~generated from an $(a,b,c)$-imbalanced 2-subGMM. Suppose that $a - c < 1$. A margin-rebalanced SVM is trained, with test errors calculated according to \cref{eq:Err_n}. Then as $d \to \infty$, 
the conclusions of the three phases in \cref{thm:high-imbalance} still hold.
% we have
% \begin{enumerate}[label=(\alph*)]
% \item \textbf{High signal} (no need for margin rebalancing): $a - c< b$. If we choose $1 \le \tau_d \ll d^{b/2}$, then 
% \begin{equation*}
% \hat\Err_{+,d} = o_{\P}(1), \qquad \hat\Err_{-,d} = o_{\P}(1).
% \end{equation*}
% \item \textbf{Moderate signal} (margin rebalancing is crucial): $b < a - c < 2b$. If we choose $d^{a-b-c} \ll \tau_d \ll d^{(a-c)/2}$, then
% \begin{equation*}
% \hat\Err_{+,d} = o_{\P}(1), \qquad \hat\Err_{-,d} = o_{\P}(1).
% \end{equation*}
% However, if we naively choose $\tau_d \asymp 1$, then 
% \begin{equation*}
% \hat\Err_{+,d} = 1 - o_{\P}(1), \qquad \hat\Err_{-,d} = o_{\P}(1).
% \end{equation*}
% \item \textbf{Low signal} (no better than random guess): $a - c > 2b$. For any $\tau_d$, we have
% \begin{equation*}
% \hat\Err_{\mathrm{b},d} := \frac12 \bigl( \hat\Err_{+,d} + \hat\Err_{-,d} \bigr) \ge \frac12 -  o_{\P}(1).
% \end{equation*}
% \end{enumerate}
\end{thm}
