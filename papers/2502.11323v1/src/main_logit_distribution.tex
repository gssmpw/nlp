\section{Precise asymptotics of empirical logit distribution} \label{sec:logit}


In this section, we present our main results on the asymptotics of empirical logit distribution introduced in \cref{subsec:ELD}. Recall that data $\{(\xx_i, y_i)\}_{i = 1}^n$ are i.i.d.~generated from a 2-GMM \cref{model}, i.e., $\xx_i \,|\, y_i \sim \normal(y_i \bmu, \bI_d)$, with label distribution $P_y: \P(y_i = +1) = 1 - \P(y_i = -1) = \pi \in (0, \frac12]$. We consider proportional asymptotics where $n,d \to \infty$ and $n/d \to \delta$ with $\delta \in (0,\infty)$. Based on relations between $\bmu, \pi, \delta$, we will consider linearly separable data (fitted by SVM) and non-separable data (fitted by logistic regression) separately.

We define the following functions $\delta^*: \R \to \R_{\ge 0}$ and $H_\kappa: [-1, 1] \times \R \to \R_{\ge 0}$ that are related to the critical threshold of data separability:
\begin{equation}\label{eq:sep_functions}
     \delta^*(\kappa) := \max_{ \rho \in [-1, 1] , \beta_0 \in \R }  H_\kappa(\rho, \beta_0),
     \qquad
     H_\kappa(\rho, \beta_0) := \frac{1 - \rho^2}{\E\left[ \bigl(  s(Y) \kappa - \rho \norm{\bmu}_2 + G - \beta_0 Y \bigr)_+^2 \right]},
\end{equation}
where $(Y, G) \sim P_y \times \normal(0,1)$ and 
% $s: \{ \pm 1 \} \to \{ \tau, 1 \}$ is the logit-adjusting function
\begin{equation}\label{eq:s_fun}
     s(y) := \begin{cases} \ \tau , & \ \text{if} \ y = + 1, \\
        \ 1, & \ \text{if} \ y = -1. \end{cases}
\end{equation}
We will show in \cref{thm:SVM_main} that the relationship between $\delta$ and $\delta^*(0)$ determines separability, where $\delta^*(0)$ does not depend on $\tau$ by definition.

We summarize the asymptotics of logit distribution for both separable and non-separable case in \cref{tab:ELD}, which is the main contribution of our theoretical results (\cref{thm:SVM_main} and \ref{thm:logistic_main}).
\begin{table}[h!]
\begin{equation*}
\renewcommand{\arraystretch}{1.2}
    \begin{array}{rll}
    \hline
           &  \textbf{limiting ELD} ~ (\hat\nu_*)  &  \textbf{cause for overfitting} ~ (\xi^*)   \\
    \hline
      \text{separable data}
          &  \Law\left( Y, \,  Y \max \{ \kappa^*, \mathtt{LOGITS} \} \right)
          &  R^* \sqrt{1 - \rho^{*2}} \xi^* = \left( \kappa^* - \mathtt{LOGITS} \right)_+  \\
      \text{non-separable data}
          &  \Law\left( Y, \,  Y \, \prox_{ \lambda^* \ell}( \mathtt{LOGITS} ) \right)
          &   R^* \sqrt{1 - \rho^{*2}} \xi^*  =  
          % -\lambda^* \ell' \bigl( \prox_{ \lambda^* \ell}( \mathtt{LOGITS} ) \bigr) \\
            - \lambda^* \nabla \envelope_{ \lambda^* \ell}( \mathtt{LOGITS} ) \\
    \hline
          \textbf{limiting TLD} ~ (\hat\nu_*^\mathrm{test})
          &  \Law\left( Y, \,  Y \cdot \mathtt{LOGITS} \right)     &   \\
    \hline
    \multicolumn{3}{c}{\mathtt{LOGITS} := \rho^*\norm{\vmu}_2 R^* + R^*G + \beta_0^* Y 
    \quad \text{($R^* := 1$ in separable case)}
    }
    \end{array}
\end{equation*}
\vspace{-5mm}
\caption{Comparison of logit distributions on separable and non-separable data ($\tau = 1$).}
\label{tab:ELD}
\end{table}

\subsection{Separable data} \label{sec:logit_SVM}

For linearly separable data, recall the margin-rebalanced SVM in \cref{eq:SVM-m-reb} and \eqref{eq:SVM}. The following theorem summarizes the precise asymptotics of SVM under arbitrary $\tau$, including the limits of parameters, margin, and logit distribution. The proofs are deferred to the appendices.

Recall that data $\{(\xx_i, y_i)\}_{i = 1}^n$ are generated from 2-GMM with fixed parameters $\bmu \in \R^d$, $\pi \in (0, \frac12)$. Let $(\hat \vbeta_n, \hat \beta_{0, n})$ be an optimal solution to the margin-rebalanced SVM \cref{eq:SVM}, and let $\hat\kappa_n$ be the maximum margin as per \cref{def:max-margin}. Recall the cosine angle $\hat \rho_n:= \hat \rho$ between $\vmu$ and $\hat\vbeta_n$ defined in \cref{eq:rho_hat}. Let $\delta^*(\kappa)$ be defined as per \cref{eq:sep_functions}, and $\rho^*, \beta_0^*, \kappa^*, \xi^*$ be a solution to the variational problem
\begin{equation}\label{eq:SVM_variation}
    \begin{aligned}
        \begin{array}{cl}
            \underset{ \rho \in [-1, 1], \beta_0 \in \R, \kappa \in \R, \xi \in \cL^2  }{ \mathrm{maximize} } & \kappa, \\
            \underset{ \phantom{\smash{\bm\beta \in \R^d, \beta_0 \in \R, \kappa \in \R} } }{\text{subject to}} &  
            \rho \norm{\bmu}_2 + G + Y \beta_0 + \sqrt{1 - \rho^2} \xi \ge s(Y) \kappa,  
            \qquad \E[\xi^2]  \le  1/\delta .
        \end{array}
    \end{aligned}
    \end{equation}
    where $\cL^2$ is the space of all square integrable random variables in $(\Omega, \mathcal{F}, \P)$, and $(Y, G) \sim P_y \times \normal(0,1)$. 
    % The asymptotic distribution of logit margins on training set and test set are respectively defined as 
    %The limiting ELD and TLD are respectively defined as 
    We define
    \begin{equation*}
    \begin{aligned}
        % \cL_* & := \Law\left( 
        %   \max\bigl\{ \kappa^*,  s(Y)^{-1} ( \rho^*\norm{\vmu}_2 + G + Y \beta_0^* )  \bigr\}
        %  \right),
        % \\
        % \cL_*^\mathrm{test} & := \Law\left( 
        %  s(Y)^{-1} ( \rho^*\norm{\vmu}_2 + G + Y \beta_0^* ) 
        %  \right).
        \nu_* & := \Law \,\bigl( Y,  Y \max\{ s(Y)\kappa^*, \rho^* \| \bmu \| + G + Y \beta_0^* \} \bigr),  \\
        \nu^\mathrm{test}_* & := \Law \,\bigl( Y, Y (\rho^* \| \bmu \| + G + Y \beta_0^*) \bigr).
    \end{aligned}
    \end{equation*}
which we will prove to be the limiting ELD and TLD respectively.
\begin{thm}[Separable data] \label{thm:SVM_main}
    %Consider data $\{(\xx_i, y_i)\}_{i = 1}^n$ generated from 2-GMM with fixed parameters $\bmu \in \R^d$, $\pi \in (0, 1/2]$. Moreover, assume $n, d \to \infty$ with $n/d \to \delta \in (0, \infty)$. Fix $\tau \in (0, \infty)$.
    Assume $n, d \to \infty$ with $n/d \to \delta \in (0, \infty)$. Fix $\tau \in (0, \infty)$. 
    %Let $(\hat \vbeta_n, \hat \beta_{0, n})$ be an optimal solution to the margin-rebalanced SVM \cref{eq:SVM}, and let $\hat\kappa_n$ be the well-defined maximum margin as per \cref{def:max-margin}. Define the cosine angle between $\vmu$ and $\hat\vbeta_n$ as
    %\begin{equation}
    %    \label{eq:rho_hat}
    %    \hat\rho_n := \biggl\<  \frac{\hat\vbeta_n}{\| \hat\vbeta_n \|_2} , \frac{\vmu}{\norm{\vmu}_2} \biggr\>.
    %\end{equation}
    \begin{enumerate}[label=(\alph*)]
        \item \label{thm:SVM_main_trans}
        \textbf{(Phase transition)} With probability tending to one, the data is linearly separable if $\delta < \delta^*(0)$ and is not linearly separable if $\delta > \delta^*(0)$.

        \item \label{thm:SVM_main_var} 
        \textbf{(Variational problem)} In the separable regime $\delta < \delta^*(0)$, $(\rho^*, \beta_0^*, \kappa^*, \xi^*)$ is the unique solution to \cref{eq:SVM_variation} with $\rho^* \in (0, 1)$ (not depend on $\tau$), $\kappa^* > 0$, and the random variable $\xi^*$ satisfies (a.s.)
        \begin{equation}\label{eq:SVM_main_xi_star}
            \sqrt{1 - \rho^{*2}} \xi^* = \bigl( s(Y) \kappa^* - \rho^*\norm{\vmu}_2 - G - Y \beta_0^*) \bigr)_+.
        \end{equation}
        Moreover, $(\rho^*, \beta_0^*, \kappa^*)$ is also the unique solution to
        \begin{equation}\label{eq:SVM_asymp_simple}
        \begin{array}{rl}
        \maximize\limits_{\rho \in [-1, 1], \beta_0 \in \R, \kappa \in \R} & \kappa, \\
        \text{\emph{subject to}} & H_\kappa(\rho, \beta_0) \ge \delta
        \end{array}
        \end{equation}
        and $\kappa^* = \sup\left\{ \kappa \in \R: \delta^*(\kappa) \ge \delta \right\}$.
        
        \item \label{thm:SVM_main_mar} 
        \textbf{(Margin convergence)} In the separable regime $\delta < \delta^*(0)$,
        \begin{equation*}
            \hat\kappa_n \conL{2} \kappa^*.
        \end{equation*}
        In the non-separable regime $\delta > \delta^*(0)$ we have negative margin, i.e., with probability tending to one, for some $\overline{\kappa} > 0$,
        \begin{equation*}
            \max_{ \substack{ \norm{\bbeta}_2 = 1 \\ \beta_0 \in \R } } \min_{i \in [n]} \, \wt y_i ( \< \xx_i, \bbeta \> + \beta_0 )  \le  -\overline{\kappa}.
        \end{equation*}

        \item \label{thm:SVM_main_param} 
        \textbf{(Parameter convergence)} In the separable regime $\delta < \delta^*(0)$,
        \begin{equation*}
            \hat\rho_n \conp \rho^*,
            \qquad
            \hat\beta_{0,n} \conp \beta_0^*.
        \end{equation*}

        \item \label{thm:SVM_main_err}
        \textbf{(Asymptotic errors)} Recall the minority and majority test prediction errors , $\Err_{+,n}$ and $\Err_{-,n}$ respectively, of the max-margin classifier defined in \cref{eq:Err_n} (writing subscript $n$ for clarity). %as
        %\begin{equation}\label{eq:Err_n}
        %    \begin{aligned}
        %        \Err_{+,n} & := \P \left( \< \xx^\mathrm{new} , \hat\vbeta_n \> + \hat\beta_{0,n} < 0  \,\big|\, y^\mathrm{new} = +1 \right), \\
         %       \Err_{-,n} & := \P \left( \< \xx^\mathrm{new} , \hat\vbeta_n \> + \hat\beta_{0,n} \ge 0  \,\big|\, y^\mathrm{new} = -1 \right), 
         %   \end{aligned}
        %\end{equation}
        %where $(\xx^\mathrm{new}, y^\mathrm{new})$ is an i.i.d. test point. 
        Then in the separable regime $\delta < \delta^*(0)$,
        \begin{equation*}
            \Err_{+,n}  \to  \Phi \left(- \rho^* \norm{\bmu}_2 - \beta_0^* \right),
            \qquad
            \Err_{-,n}  \to  \Phi \left(- \rho^* \norm{\bmu}_2  + \beta_0^* \right).
        \end{equation*}
        \item \label{thm:SVM_main_logit}
        \textbf{(ELD/TLD convergence)} 
        % The empirical distribution of logit margins on training set and the distribution for a test point are defined as the following random measures, respectively:
        % \begin{equation}
        %     \hat \cL_{n} := \frac1n \sum_{i=1}^n \delta_{\wt y_i ( \< \xx_i, \hat\vbeta_n \> + \hat\beta_{0,n} )
        %     },
        %     \qquad
        %     \hat \cL_{n}^\mathrm{test} := \Law\left( \wt y^\mathrm{new} ( \< \xx^\mathrm{new}, \hat\vbeta_n \> + \hat\beta_{0,n} \right),
        % \end{equation}
        % where $\wt y^\mathrm{new} = y^\mathrm{new}/s(y^\mathrm{new})$ is the transformed label as \cref{eq:trans-labels}. 
        Recall the ELD $\hat\nu_n$ and TLD $\hat\nu_n^\mathrm{test}$ defined as per \cref{def:ELD_TLD}, where $\hat f(\xx) = \< \xx, \hat\vbeta_n \> + \hat\beta_{0, n}$.
        Then in the separable regime $\delta < \delta^*(0)$ we have logit convergence for both training and test data, i.e.,
        \begin{equation*}
            % W_2\bigl( \hat \cL_{n}, \cL_* \bigr) \conp 0,
            % \qquad
            % \hat\cL_{n}^\mathrm{test} \conw \cL_*^\mathrm{test}.
            W_2\bigl(\hat{\nu}_n , \nu_* \bigr) \conp 0,
            \qquad
            \hat{\nu}_n^\mathrm{test} \conw \nu^\mathrm{test}_*.
        \end{equation*}
    \end{enumerate}
\end{thm}

\begin{rem}
    By taking $\tau = 1$, the ELD convergence $W_2(\hat{\nu}_n , \nu_* ) \conp 0$ in \cref{thm:SVM}\ref{thm:SVM_c} is a consequence of \cref{thm:SVM_main}\ref{thm:SVM_main_logit}, and the TLD convergence $\hat{\nu}_n^\mathrm{test} \conw \nu^\mathrm{test}_*$ is a corollary of \cref{thm:SVM_main}\ref{thm:SVM_main_param}.
\end{rem}

As discussed in \cref{subsec:ELD}, random variable $\xi^*$ and the nonlinear transformation $\mathtt{T}^*(x) = \max\{x, \kappa^*\}$ therein characterize the effect of overfitting on logits. The following result provides an optimal transport perspective of this overfitting effect. For ease of description, we reformulate $\nu_*$ and $\nu^\mathrm{test}_*$ in terms of the following one-dimensional measures
\begin{equation*}
    \cL_*  := \Law \,\bigl(  \max\{ \kappa^*, \rho^* \| \bmu \| + G + Y \beta_0^* \} \bigr),  
        \qquad 
    \cL^\mathrm{test}_* := \Law \,\bigl(  \rho^* \| \bmu \| + G + Y \beta_0^* \bigr).
\end{equation*}

\begin{prop}[Optimal transport map]\label{prop:opt_transport}
    $\mathtt{T}^*(x) = \max\{\kappa^*, x\}$ is the unique optimal transport map from $\cL_*^\mathrm{test}$ to $\cL_*$ under the cost function $c(x, y) = h(x - y)$ for any strictly convex $h: \R^2 \to \R_{\ge 0}$. That is, 
    \[ 
    \mathtt{T}^* = \argmin_{\mathtt{T}: \R \to \R} \left\{ \int_{\R} c \bigl( x, \mathtt{T}(x) \bigr)  \d \cL_*^\mathrm{test}(x)
    \,\middle|\,
    \mathtt{T}_\sharp \cL_*^\mathrm{test} = \cL_*
    \right\},
    \]
    where $\mathtt{T}_\sharp$ is the pushforward operator.
\end{prop}
% \begin{proof}[Proof idea:] 
%     Let $\mu = \normal(0, 1)$ and $\nu = \Law( \max\{ \kappa, G \} )$. The optimal transport map from $\mu$ to $\nu$ is given by $\mathtt{T}(x) = F_{\nu}^{-} \circ \Phi (x) = \max\{ \kappa, x \}$ via optimal transport theory, where $F_{\nu}^{-}$ is the quantile function of $\nu$.
% \end{proof}














\subsection{Non-separable data}
\label{sec:logit_logistic}

For non-separable data, SVM yields a trivial solution $\vbeta=\boldsymbol{0}, \beta_0 =0$. 
% according to our discussion in \cref{sec:background}. 
A typical approach to fitting a classifier is to solve regression problem \cref{eq:logistic}. Similar to the margin-rebalanced SVM \cref{eq:SVM}, we can also incorporate $\tau$ into the objective function by substituting $y_i$ for $\wt y_i = y_i/s(y_i)$, that is,
\begin{equation}\label{eq:logistic_reg}
    \min \limits_{\vbeta \in \R^d, \beta_0 \in \R} \qquad 
    \frac1n \sum_{i=1}^n \ell \bigl( \wt y_i(\langle \vx_i, \vbeta \rangle + \beta_0) \bigr),
\end{equation}
where $\ell: \R \to \R_{\ge 0}$ is the loss function. We consider a more general form than logistic regression. We say that $\ell$ is \emph{pseudo-Lipschitz} if there exists a constant $L > 0$ such that, for all $x, y \in \R$,
\begin{equation*}
    \abs{ \ell(x) - \ell(y) } \le L \left( 1 + \abs{ x } + \abs{ y } \right) \abs{ x - y }.
\end{equation*}
This condition is satisfied, for instance, by the widely used logistic loss $\ell(t) = \log(1 + e^{-t})$. As the counterpart of \cref{thm:SVM_main} in the non-separable regime, the following theorem summarizes the precise asymptotics of regression \cref{eq:logistic_reg}, including the limits of parameters and logit distribution.

We consider the same 2-GMM setting as \cref{sec:logit_SVM}. For any non-increasing, strictly convex, pseudo-Lipschitz, twice differentiable function $\ell: \R \to \R_{\ge 0}$, let $(\hat \vbeta_n, \hat \beta_{0, n})$ be the optimal solution to regression \cref{eq:logistic_reg}. Recall $\hat \rho_n:= \hat \rho$ defined in \cref{eq:rho_hat} and $\delta^*(\kappa)$ defined in \cref{eq:sep_functions}. Let $\rho^*, R^*, \beta_0^*, \xi^*$ be a solution to the variational problem
    \begin{equation}\label{eq:logistic_variation}
    \begin{aligned}
        \begin{array}{cl}
            \underset{ \rho \in [-1, 1], R \ge 0, \beta_0 \in \R,\xi \in \cL^2 }{ \mathrm{minimize} }
            &
        \E \left[ \ell \biggl( \dfrac{ \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi }{s(Y)} \biggr) \right], \\
            \text{subject to} & \vphantom{\dfrac11} \E \left[ \xi^2 \right]  \le  1/\delta .
        \end{array}
    \end{aligned}
    \end{equation}
    % \begin{equation}\label{eq:logistic_variation}
    % \begin{aligned}
    %     \begin{array}{cl}
    %         \underset{ \rho \in [-1, 1], R \ge 0, \beta_0 \in \R,\xi \in \cL^2 }{ \mathrm{minimize} }
    %         &
    %     \E \left[ \ell \bigl( \rho\norm{\vmu}_2 R + RG + \beta_0 Y + R\sqrt{1 - \rho^2} \xi \bigr) \right], \\
    %         \underset{ \phantom{\smash{\bm\beta \in \R^d, \beta_0 \in \R, \kappa \in \R} } }{\text{subject to}} & \E \left[ \xi^2 \right]  \le  1/\delta .
    %     \end{array}
    % \end{aligned}
    % \end{equation}
    where $(Y, G) \sim P_y \times \normal(0,1)$. 
    % The asymptotic distribution of logit margins on training set and test set are respectively defined as 
    %The limiting ELD and TLD are respectively defined as
    We define
    \begin{equation*}
    \begin{aligned}
        \nu_* & := 
        \Law \!\left( 
        Y, Ys(Y) \, \prox_{\frac{\lambda^* \ell}{s(Y)}}\biggl( \frac{\rho^* \norm{\vmu}_2 R^* + R^* G + \beta_0^* Y}{s(Y)} \biggr)
        \right).
        \\
        \nu^\mathrm{test}_* & := \Law \,\bigl( Y, Y ( R^*\rho^* \| \bmu \| + R^*G + Y \beta_0^*) \bigr),
    \end{aligned}
    \end{equation*}
    % \begin{equation}
    % \begin{aligned}
    %     \nu_* & := \Law \,\bigl( Y,
    %       Y \prox_{\lambda^* \ell}( \rho^*\norm{\vmu}_2 R^* + R^* G + \beta_0^* Y )
    %      \bigr),
    %     \\
    %     \nu^\mathrm{test}_* & := \Law \,\bigl( Y,
    %      Y \rho^*\norm{\vmu}_2 R^* + R^* G + \beta_0^* Y  
    %      \bigr),
    % \end{aligned}
    % \end{equation}
    aiming to show they are the limiting ELD and TLD respectively.

\begin{thm}[Non-separable data] \label{thm:logistic_main}
    Consider the same 2-GMM and proportional settings $n/d \to \delta$ as in \cref{thm:SVM_main}. 
    \begin{enumerate}[label=(\alph*)]
        \item \label{thm:logistic_main(a)}
        \textbf{(Variational problem)} In the non-separable regime $\delta > \delta^*(0)$, $(\rho^*, R^*, \beta_0^*, \xi^*)$ is the unique solution to \cref{eq:logistic_variation} with $\rho^* \in (0, 1)$, $R^* \in (0, \infty)$, and the random variable $\xi^*$ satisfies (a.s.)
        \begin{equation}\label{eq:logistic_xi_star}
            R^* \sqrt{1 - \rho^{*2}} \xi^*  =  -\lambda^* 
            \ell' \biggl( \prox_{\frac{\lambda^* \ell}{s(Y)}}\Bigl( \frac{\rho^* \norm{\vmu}_2 R^* + R^* G + \beta_0^* Y}{s(Y)} \Bigr) \biggr) ,
        \end{equation}
        % \begin{equation}
        %     R^* \sqrt{1 - \rho^{*2}} \xi^*  =  -\lambda^* 
        %     \ell' \bigl( \prox_{ \lambda^* \ell}( \rho^*\norm{\vmu}_2 R^* + R^*G + \beta_0^* Y ) \bigr) ,
        % \end{equation}
        where $\lambda^* \in (0, \infty)$ is the unique constant such that $\E[ \xi^2] = 1/\delta$.
        Moreover, $(\rho^*, R^*, \beta_0^*, \lambda^*)$ is also the unique solution to the following system of equations
        \begin{align*}
            - \frac{\tau R \rho}{2\pi \lambda \delta \norm{\vmu}_2}
            & = 
            \E\left[ \ell'\biggl( \prox_{\frac{\lambda\ell}{\tau}}\Bigl( \frac{\rho\norm{\vmu}_2 R + RG + \beta_0}{\tau} \Bigr) \biggr) \right] ,
            \\
            - \frac{R \rho}{2(1 - \pi) \lambda \delta \norm{\vmu}_2}
            & = 
            \E \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG - \beta_0 ) \bigr) \right] ,
            \\
            \frac{1}{\lambda \delta }
            & = 
            \E \left[ \dfrac{1}{s(Y)} \cdot \frac{
            \ell'' \biggl( \prox_{\frac{\lambda \ell}{s(Y)}}\Bigl( \dfrac{\rho\norm{\vmu}_2 R + RG + \beta_0 Y}{s(Y)} \Bigr) \biggr)
            }{s(Y) + 
            \lambda \ell'' \biggl( \prox_{\frac{\lambda \ell}{s(Y)}}\Bigl( \dfrac{\rho\norm{\vmu}_2 R + RG + \beta_0 Y}{s(Y)} \Bigr) \biggr)
            } \right] ,
            \\
            \frac{R^2 (1 - \rho^2)}{\lambda^2 \delta}
            & = 
            \E \left[ \Biggl(  
            \frac{1}{s(Y)} \cdot \ell' \biggl( \prox_{\frac{\lambda \ell}{s(Y)}}\Bigl( \frac{\rho\norm{\vmu}_2 R + RG + \beta_0 Y}{s(Y)} \Bigr) \biggr)
            \Biggr)^2 \right].
        \end{align*}
        % \begin{align*}
        %     - \frac{\rho R}{2\pi \lambda \delta \norm{\vmu}_2}
        %     & = 
        %     \E \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 ) \bigr) \right] ,
        %     \\
        %     - \frac{\rho R}{2(1 - \pi) \lambda \delta \norm{\vmu}_2}
        %     & = 
        %     \E \left[ \ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG - \beta_0 ) \bigr) \right] ,
        %     \\
        %     \frac{1}{\lambda \delta }
        %     & = 
        %     \E \left[ \frac{\ell''\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr)}{1 + \lambda \ell''\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr)} \right] ,
        %     \\
        %     \frac{R^2 (1 - \rho^2)}{\lambda^2 \delta}
        %     & = 
        %     \E \left[ \left(\ell'\bigl( \prox_{ \lambda \ell}( \rho\norm{\vmu}_2 R + RG + \beta_0 Y ) \bigr) \right)^2 \right] .
        % \end{align*}

        \item \label{thm:logistic_main(b)}
        \textbf{(Parameter convergence)} In the non-separable regime $\delta > \delta^*(0)$, as $n \to \infty$,
        \begin{equation*}
            \| \hat\vbeta_n \|_2 \conp R^*,
            \qquad
            \hat\rho_n \conp \rho^*,
            \qquad
            \hat\beta_{0,n} \conp \beta_0^*.
        \end{equation*}

        \item \label{thm:logistic_main(c)}
        \textbf{(Asymptotic errors)} Recall the prediction errors defined as per \cref{eq:Err_n}. Then in the non-separable regime $\delta > \delta^*(0)$, as $n \to \infty$,
        \begin{equation*}
            \Err_{+,n}  \to  \Phi \left(- \rho^* \norm{\bmu}_2 - \frac{\beta_0^*}{R^*} \right),
            \qquad
            \Err_{-,n}  \to  \Phi \left(- \rho^* \norm{\bmu}_2  + \frac{\beta_0^*}{R^*} \right).
        \end{equation*}

        \item \label{thm:logistic_main(d)}
        \textbf{(ELD/TLD convergence)} 
        % The empirical distribution of logit margins on training set and the distribution for a test point are defined as the following random measures, respectively:
        % \begin{equation}
        %     \hat \cL_{n} := \frac1n \sum_{i=1}^n \delta_{y_i ( \< \xx_i, \hat\vbeta_n \> + \hat\beta_{0,n} )
        %     },
        %     \qquad
        %     \hat \cL_{n}^\mathrm{test} := \Law\left( y^\mathrm{new} ( \< \xx^\mathrm{new}, \hat\vbeta_n \> + \hat\beta_{0,n} \right),
        % \end{equation}
        Recall the ELD $\hat\nu_n$ and TLD $\hat\nu_n^\mathrm{test}$ defined as per \cref{def:ELD_TLD}, where $\hat f(\xx) = \< \xx, \hat\vbeta_n \> + \hat\beta_{0, n}$.
        Then in the non-separable regime $\delta > \delta^*(0)$ we have logit convergence for both training and test data, i.e., as $n \to \infty$,
        \begin{equation*}
            % W_2\bigl( \hat \cL_{n}, \cL_* \bigr) \conp 0,
            % \qquad
            % \hat\cL_{n}^\mathrm{test} \conw \cL_*^\mathrm{test}.
            W_2\bigl(\hat{\nu}_n , \nu_* \bigr) \conp 0,
            \qquad
            \hat{\nu}_n^\mathrm{test} \conw \nu^\mathrm{test}_*.
        \end{equation*}
    \end{enumerate}
\end{thm}

\begin{rem}
    Compared to the separable regime, the random variable $\xi$ in the non-separable regime \cref{eq:logistic_variation} can also be interpreted as the cause for overfitting, but its distortion effect on ELD is not truncation. When $\tau = 1$, by \cref{eq:logistic_variation}, \eqref{eq:logistic_xi_star}, the following holds for a ``typical'' training point:
\begin{equation*}
\begin{aligned}
    y_i (\langle \vx_i, \hat \vbeta_n \rangle + \hat \beta_{0,n}) 
    & \approx 
    \rho^*\norm{\vmu}_2 R^* + R^*G + \beta_0^* Y + R^*\sqrt{1 - \rho^{*2}} \xi^* \\
    & = \rho^*\norm{\vmu}_2 R^* + R^*G + \beta_0^* Y 
    - \lambda^* 
            \ell' \bigl( \prox_{ \lambda^* \ell}( \rho^*\norm{\vmu}_2 R^* + R^*G + \beta_0^* Y ) \bigr) \\
    & = \prox_{ \lambda^* \ell}( \rho^*\norm{\vmu}_2 R^* + R^*G + \beta_0^* Y ),
\end{aligned}
\end{equation*}
where the equalities come from \cref{lem:prox}. Hence, the ELD in the non-separable regime is the TLD under nonlinear shrinkage due to the proximal operator of loss function $\ell$.
\end{rem}
