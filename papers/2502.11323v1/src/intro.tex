\section{Introduction}\label{sec:intro}


Classification tasks are ubiquitous in statistics and machine learning. In many practical applications, training data are often imbalanced, meaning that some classes (minority classes) contain substantially fewer samples than others. In binary classification, particularly, we observe training data $\{(\vx_i, y_i)\}_{i = 1}^n \stackrel{\mathrm{i.i.d.}}{\sim} P_{\xx,y}$ with features $\xx_i \in \R^d$ and binary labels $y_i \in \{\pm 1\}$. Denote by $P_{\vx}$ (resp. $P_y$) the marginal distribution of $\vx$ (resp. $y$), and the expected fractions of the two classes by
%The imbalance ratio $\pi$ is the expected fraction of samples in the minority class
\begin{equation*}
\pi_+ := \P(y_i = +1), \qquad \pi_- :=  \P(y_i = -1).
\end{equation*}
We say that the data set is imbalanced if $\pi_+ < \pi_-$\footnote{Without loss of generality, we assume that the minority class is assigned the label $+1$.}. 
Classification problems with class imbalance are common in applications where the minority class represents rare diseases, rare events, anomalies, or underrepresented groups \cite{litjens2017survey, tschandl2018ham10000, king2001logistic, kubat1998machine, ngai2011application, chandola2009anomaly, weiss2003learning, buolamwini2018gender}.
%\ljycom{Picked some high-cited applied works/datasets/surveys. So far so good?}

In this paper, we focus on linear classification, where the classifier takes the form of 
% $f(\vx_i) = \langle \vx_i, \vbeta \rangle + \beta_0$
$\vx \mapsto 2 \mathbbm{1}\{ f(\vx) > 0\} - 1$ with $f(\vx) = \langle \vx, \vbeta \rangle + \beta_0$. This simple form is widely used in statistics and machine learning: (i) For tabular data, linear classification algorithms such as logistic regression and the support vector machine (SVM) are commonly applied directly to the training data; (ii) For image and text data, the last classification layer of a deep neural network (DNN) usually takes this form (also known as softmax regression), where $\vx_i$ represents the feature vector extracted by the previous layers of the network. In downstream analysis, the base network is often frozen, and only the linear classification layer is retrained.


\paragraph{Challenges of high dimensions.}  For low-dimensional problems where the dimension $d$ is fixed or satisfies $d \ll n$, prediction performance and estimation accuracy are well understood for standard classification methods. Generally, we expect estimation consistency, where the estimated parameter vector $\hat \vbeta$ is close to the target $\vbeta$, and a small generalization gap where the training error is close to the test error. However, for high-dimensional problems where $d$ is typically comparable to $n$, the classical theory depicts an inaccurate picture, motivating recent efforts to refine the asymptotic characterization of learning behavior under high-dimensional regimes.

\begin{table}[h!]
\centering
\begin{tabular}{c|c|c}
   & \textbf{Low dimensions}        & \textbf{High dimensions}   \\ 
   \hline
   \rule[-1ex]{0pt}{4ex}
Parameter estimation                                           & $\left\langle \frac{\hat \vbeta}{\| \hat \vbeta\|}, \frac{\vbeta}{\| \vbeta \|} \right\rangle \approx 1$ & $\left\langle \frac{\hat \vbeta}{\| \hat \vbeta\|}, \frac{\vbeta}{\| \vbeta \|} \right\rangle < 1$ \\
Generalization       & Train error $\approx$ Test error                       & Train error $<$ Test error   \\
Distribution of logits & 1D projection of $P_{\boldsymbol{x}}$   & Skewed/distorted 1D projection of $P_{\boldsymbol{x}}$
\end{tabular}
\caption{Qualitative comparison between low/high dimensions for binary classification, where a linear classifier $\hat{y} (\xx) = 2 \mathbbm{1}\{ \hat f(\vx) > 0\} - 1$ with $\hat f(\vx) = \langle \vx, \hat \vbeta \rangle + \hat \beta_0$ is trained on $\{(\vx_i, y_i)\}_{i=1}^n \stackrel{\iid}{\sim} P_{\vx, y}$. Here, the logits $\{\hat f(\boldsymbol{x}_i)\}_{i = 1}^n $ are obtained by evaluating $\hat{f}$ on the training set.}\label{tab:1}
\end{table}    


Regarding parameter estimation, a recent line of work \cite{dobriban2018high, Pragya_highdim_logistic, sur2019logistic, candes2020logistic, montanari2023generalizationerrormaxmarginlinear} has studied the asymptotic properties of logistic regression under the proportional regime $n / d \to \delta$, as $n, d \to \infty$ for some constant $\delta$. Qualitatively speaking, as the dimension $d$ increases (or $\delta$ decreases), the estimation error of the maximum likelihood estimator (MLE) $\hat \vbeta$ continues to grow until $\delta$ reaches a critical threshold, below which the MLE no longer exists \cite{Pragya_highdim_logistic, sur2019logistic, candes2020logistic}. In addition, the classical likelihood test requires modifications to remain valid in high dimensions \cite{Pragya_highdim_logistic, sur2019logistic, candes2020logistic}. 

Regarding generalization, high dimensionality usually leads to a gap between the training and test errors. To further investigate this phenomenon, recent research has focused on understanding the interplay between memorization and generalization, inspired by the \textit{double descent} phenomenon \cite{belkin2019reconciling}. In particular, in overparametrized models where the number of parameters far exceeds the sample size, it has been shown that algorithms such as gradient descent have an implicit regularization effect, leading to benign overfitting \cite{bartlett2020benign}.

Finally, the distribution of logits is highly valuable for feature visualization and interpretation, yet relatively little theory has been developed in this area. One practical example is \textit{linear probing}, a common approach for interpreting the hidden states (or activations) in empirical deep learning. This method involves training a simple linear classifier on top of the hidden states and visualizing their projections \cite{kornblith2019better, he2020momentum, kumar2022fine}. Another example is \textit{projection pursuit} (PP), where data are visualized via low-dimensional projections for exploratory data analysis. A few recent papers have investigated theoretical properties of PP in high dimensions \cite{Bickel_high_dim_PP, montanari2022overparametrizedlineardimensionalityreductions}.


\paragraph{Challenges of data imbalance.} In classification problems, data imbalance poses significant challenges, particularly for the minority class. First, classical asymptotic theory can become practically unreliable under severe data imbalance, compromising the accuracy of maximum likelihood estimation. Second, imbalanced classification is particularly susceptible to label shifts, where the proportion of minority labels (denoted as $\pi_+$ in binary classification) differs in a new test dataset. Third, misclassifying data points from the minority class typically incurs a much higher cost, a factor that is not accounted for in standard (unweighted) logistic regression. For a comprehensive overview, see \cite{he2009learning}. To address these issues, various techniques have been proposed and employed in empirical studies, such as adjusting decision boundaries, reweighting loss functions, subsampling the majority classes, and oversampling the minority classes \cite{king2001logistic, chawla2002smote}, among others.

Overfitting in high dimensions further compounds the challenges of imbalanced classification. Empirical studies have shown that deep learning models, particularly those with large capacity, often suffer from a disproportionate accuracy drop due to overfitting. This is because such models tend to memorize data points from minority classes rather than generalize to them, as these points constitute only a small fraction of the training data \cite{sagawa2020investigation}. While several remedies have been proposed in the literature \cite{huang2016learning, khan2019striking, liu2019large, cao2019learning}, they appear to be ad hoc fixes and fail to provide guidance on hyperparameter selection or feature interpretation.

We identify two major limitations in the existing literature. First, it remains unclear why overfitting is generally more severe in minority classes, despite being consistently observed in empirical studies. Second, there is a lack of comprehensive analysis of the impacts of various key factors---such as dimensionality, imbalance ratios, and signal strength---on the performance metrics such as test accuracy and uncertainty quantification.


\paragraph{Our goal.} The aim of this paper is to develop a statistical theory to address the aforementioned limitations. In particular, we seek to answer the following key questions:

\begin{enumerate}
    \item[\texttt{Q1}.] Can we mathematically characterize overfitting in high-dimensional imbalanced classification?
    % Can we mathematically characterize overfitting in high-dimensional settings for imbalanced classification?
    %Can we mathematically characterize overfitting for imbalanced classification in high dimensions?
    \item[\texttt{Q2}.] What are the adverse effects of overfitting, particularly on the minority class?
    \item[\texttt{Q3}.]
    % How can we design robust learning algorithms to mitigate these adverse effects? \ljycom{effect on uncertainty quantification}
    %What is the adverse effect of model parameters on test accuracy and uncertainty quantification?
    % What are the consequences of overfitting for uncertainty measures such as calibration?
    What are the consequences of overfitting for uncertainty quantification, such as calibration?
\end{enumerate}
Building on theoretical tools from high-dimensional statistics, our  analysis focuses on a stylized model. Suppose the i.i.d.~training data $\{(\vx_i, y_i)\}_{i = 1}^n$ are generated from a two-component Gaussian mixture model (2-GMM):
\begin{equation}\label{model}
    \P(y_i = +1) = \pi, \quad \P(y_i = -1) = 1 - \pi, \quad \xx_i \,|\, y_i \sim \normal(y_i \bmu, \bI_d),
\end{equation}
where $\bmu \in \R^d$ is an unknown signal vector. Under this model, the Bayes-optimal classifier has the form $y^* (\xx) = 2\ind\{\langle \vx, \vbeta \rangle + \beta_0 > 0\} - 1$,
where $\vbeta \parallelsum \vmu$. We analyze the behavior of two standard approaches for binary classification: (a slightly generalized version of) logistic regression and support vector machines (SVMs). Denoting by $\ell: \R \to \R$ a strictly convex decreasing function, including the logistic function $\log(1 + e^{-x})$ as a special case, we solve
\begin{subequations}
\begin{align}
\begin{array}{lcl}
    \text{(logistic regression)} 
    & 
    \mathmakebox[\widthof{$ \displaystyle\maximize \limits_{\bbeta \in \R^d, \, \beta_0, \kappa \in \R} $}][c]{
    \minimize \limits_{\vbeta \in \R^d, \beta_0 \in \R}
    } 
    & 
    \mathmakebox[\widthof{$ \displaystyle y_i ( \< \xx_i, \bbeta \> + \beta_0 ) \ge \kappa,
	\quad \forall\, i \in [n], $}][l]{
    \displaystyle \frac{1}{n} \sum_{i=1}^n \ell \bigl( y_i(\langle \vx_i, \vbeta \rangle + \beta_0) \bigr),
    }
\end{array}
\label{eq:logistic}
\\
\begin{array}{lcl}
    \text{(SVM)}  & 
    \maximize \limits_{\bbeta \in \R^d, \, \beta_0, \kappa \in \R} & \kappa,  \\
    & \text{subject to}\vphantom{\displaystyle\max_\kappa} & y_i ( \< \xx_i, \bbeta \> + \beta_0 ) \ge \kappa,
	\quad \forall\, i \in [n],    
    \\
    & & \norm{\bbeta}_2  \le  1.
\end{array}
\label{eq:SVM-0}
\end{align}
\end{subequations}
Both optimization problems are convex and yield solutions $\hat \vbeta, \hat \beta_0$, which are used to predict class labels for a test data point $\vx$ based on $\hat f(\vx) = \langle \vx, \hat \vbeta \rangle + \hat \beta_0$. Namely, the predicted binary label of a test data point $\vx$ is $\hat{y} (\xx) = 2\mathbbm{1}\{\langle \vx, \hat{\vbeta} \rangle + \hat{\beta}_0 > 0\} - 1$.


We will analyze both classifiers with a focus on the SVM for the following reason. In modern machine learning, it is common for the labeled data $\{(\vx_i, y_i)\}_{i = 1}^n$ to be linearly separable due to high dimensionality.
When data are linearly separable, the hard-margin SVM coincides with the max-margin classifier. It is known that the gradient descent iterates of logistic regression converge in direction to the max-margin solution \cite{Soudry_implicit_bias, ji2019riskparameterconvergencelogistic}, which is known as a form of inductive bias \cite{neyshabur2015searchrealinductivebias}. See Section~\ref{sec:background} for the background. In this sense, the two classifiers are closely related.

The code for our experiments can be found in the GitHub repository: 
\begin{center}
\url{https://github.com/jlyu55/Imbalanced_Classification}
\end{center}


\subsection{Characterizing overfitting via empirical logit distribution}
\label{subsec:ELD}

Understanding why test accuracy drops more for the minority class requires a more refined characterization of overfitting. To this end, we study the empirical distribution of the logits $\hat f(\vx_i) = \langle \vx_i, \hat\vbeta \rangle + \hat \beta_0$, $i \in [n]$ on the training set. 

Let $(\xx_1, y_1), \ldots, (\xx_n, y_n) \iidsim P_{\xx, y}$ be the training data, and $(\vx_\mathrm{test}, y_\mathrm{test})$ be an independent test data point. Denote $\mathcal{I}_+ := \{ i \in [n]: y_i = +1 \}$ and $\mathcal{I}_- := \{ i \in [n]: y_i = -1 \}$ as the index sets of training data points from the minority class and the majority class, respectively. Let $n_+ := \abs{\mathcal{I}_+}$ and $n_- := \abs{\mathcal{I}_-}$ be the sample sizes of the two classes. Consider a binary classifier $\hat{y}: \R^d \to \{ \pm 1 \}$ based on $\hat f: \R^d \to \R$ such that we predict $\hat y = + 1$ if $\hat f(\vx) > 0$ and predict $\hat y=-1$ otherwise. Throughout this paper, we ignore the one-class degenerate case and implicitly assume both $n_+, n_- \ge 1$, which occurs with high probability.


\begin{defn}[Logit and margin]
Let $(\xx, y) \in \R^d \times \{ \pm 1 \}$ be a data point. For a binary classifier of the form $\hat y(\xx) = 2 \mathbbm{1}\{ \hat f(\vx) > 0\} - 1$, we define:
\begin{itemize}
    \item The \emph{logit} of $\xx$ is $\hat f(\xx)$.
    \item The \emph{logit margin} of $\xx$ is $y \hat f(\xx)$.
    \item The \emph{margin} of the classifier $\hat f$ (on the training data) is $\hat\kappa_n = \min_{i \in [n]} y_i \hat f(\vx_i)$.
\end{itemize}
\end{defn}
The following definitions highlight the logit distribution on both training and test data. 
\begin{defn}[ELD and TLD]
\label{def:ELD_TLD}
\begin{figure}[t]
    \centering
    % \includegraphics[width=0.75\textwidth]{Figs/GMM-ELD(old).pdf}
    \includegraphics[width=0.75\textwidth]{Figs/GMM-ELD_sep_pi=0.15,mu=1.75,n=10000,d=4000.pdf}
    \caption{
    \textbf{Empirical logit distribution (ELD) and testing logit distribution (TLD)}. We train a max-margin classifier (namely SVM) $\hat f$ on synthetic data from a 2-component Gaussian mixture model. Colors indicate labels $y_i$ and $x$-axis indicates logits $\hat f(\vx_i)$. \textbf{ELD for both classes:} rectified Gaussian distribution (histogram). \textbf{TLD for both classes:} Gaussian distribution (curve).  \textbf{Overfitting effect:} The density areas below the dotted curves are overlapping in TLD, thus leading to positive test error; however they are ``pushed'' to respective margin boundaries in ELD, thus leading to linear separability and zero training errors.
    }
    \label{fig:GMM_main}
\end{figure}

% For a binary classifier of the form
% $2 \mathbbm{1}\{ y\hat f(\vx) > 0\} - 1$, the \emph{logit} of the $i$-th training data point is $\hat f(\vx_i)$. 
% \leavevmode
\begin{enumerate}
\item \textbf{\emph{Empirical logit distribution (ELD)}}, or \emph{training logit distribution}, is defined as the empirical distribution of label-logit pairs based on training data, % $(\xx_1, y_1) \ldots (\xx_n, y_n)$
that is,
\begin{equation}\label{eq:ELD}
\hat\nu_n = \frac{1}{n} \sum_{i=1}^n \delta_{(y_i, \hat f(\vx_i))},
\end{equation}
where $\delta_{\va}$ denotes the delta measure supported at point $\va$.


\emph{Minority ELD} and \emph{majority ELD} are defined respectively as the empirical distribution of logits based on training data from minority class and majority class, i.e.,
\begin{equation*}
    \frac{1}{n_+} \sum_{i \in \mathcal{I}_+} \delta_{\hat f(\vx_i)}
    \qquad \text{and}
    \qquad
    \frac{1}{n_-} \sum_{i \in \mathcal{I}_-} \delta_{\hat f(\vx_i)}.
\end{equation*}
Note that these ELDs are all random probability measures.


\item \textbf{\emph{Testing logit distribution (TLD)}}, is defined as the distribution of the label-logit pair for a test data point ($\Law$ means the distribution of random variables/vectors), that is, 
\begin{equation*}
    \hat\nu^\mathrm{test}_n = \Law \, \bigl (y_\mathrm{test},  \hat f(\vx_\mathrm{test}) \bigr).
\end{equation*}

\emph{Minority TLD} and \emph{majority TLD} are defined respectively as the distribution of the logit for a test data point from minority class and majority class, i.e.,
\begin{equation*}
    \Law \, \bigl ( \hat f(\vx_\mathrm{test}) \,|\, y_\mathrm{test} = +1 \bigr)
    \qquad \text{and}
    \qquad
    \Law \, \bigl ( \hat f(\vx_\mathrm{test}) \,|\, y_\mathrm{test} = -1 \bigr).
\end{equation*}
Note that the randomness in TLDs is taken over both the classifier $\hat f$ and the test point $(\vx_\mathrm{test}, y_\mathrm{test})$, so they are deterministic probability measures.
% Since these TLDs depend on $\hat{f}$, they are also random probability measures.
\end{enumerate}
\end{defn}
When $\hat\kappa_n > 0$, the training set is linearly separable, and the training accuracy of $\hat f$ is $100\%$. While linear separability is common in high dimensions, the test accuracy is usually not perfect, which is based on the predicted label $\hat y(\vx_\mathrm{test}) = 2\mathbbm{1} \{\hat f(\vx_\mathrm{test}) > 0\} - 1$ for a test point $\vx_\mathrm{test}$. The discrepancy between train/test accuracies is known as overfitting.

For a classifier $\hat f$, the ELD and TLD are more informative compared with  train/test accuracies. For this reason, we analyze overfitting via a study of ELD and TLD.


\paragraph{Empirical phenomenon.} 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figs/ifnb_classes_5_4_whitening_rank_0pi_0.200_2023.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/CIFAR10_classes_5_4_whitening_rank_0pi_0.100_2024.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/IMDB_classes_0_1_whitening_rank_0pi_0.020_2023.pdf}
    \caption{
        \textbf{ELD and TLD of logistic regression classifier (the last fully-connected layer) for
        real data}. \textbf{Left:} IFNB single-cell RNA-seq dataset (tabular data). \textbf{Middle:} CIFAR-10 dataset preprocessed by pretrained ResNet-18 model for feature extraction (image data). \textbf{Right:} IMDb movie review dataset preprocessed by BERT base model (110M) for feature extraction (text data).
        }
    \label{fig:GMM_real}
\end{figure}

First, %we consider a simple simulation to illustrate the phenomenon. 
we show a simple yet representative simulated example to illustrate the phenomenon; see \cref{fig:GMM_main}.
We generate training data according to 2-GMM in \cref{model}, with training sample size $n = 10,000$, feature dimension $d = 4,000$, signal strength $\norm{\bmu}_2 = 1.75$, and imbalance ratio $\pi = 0.15$. Such regime guarantees linear separability with high probability. We train an SVM classifier \cref{eq:SVM-0} on this dataset and visualize the ELD and TLD for the minority class ($y_i = +1$) and the majority class ($y_i = -1$) respectively. The TLD for each class follows a Gaussian distribution, while the ELD for each class is the same Gaussian curve but \textit{truncated} at the margin boundary---such discrepancy characterizes the effect of overfitting in imbalanced classification. As an adverse implication, we also observe that more than half of the density is truncated in minority ELD, thus causing severe degrading accuracy compared with the majority ELD. 
%while only a upper tail is truncated in majority ELD. Therefore, this truncation could characterize the overfitting effect in imbalanced classification. 
Formally, such distribution of ELD is called \textit{rectified Gaussian distribution} \cite{rectifiedGaussian}.

\begin{defn}[Rectified Gaussian distribution]
    For a Gaussian random variable $Z \sim \normal(\mu, \sigma^2)$ and a given threshold $\kappa \in \R$, the distribution of $\max\{ Z, \kappa \}$ or $\min\{ Z, \kappa \}$ is called the \emph{rectified Gaussian distribution}.
\end{defn}






For real-data examples, we consider fine-tuning experiments based on pretrained DNNs, as shown in Figure~\ref{fig:GMM_real}. Given a pretrained neural network, we freeze all parameters except for the last classification layer and fine-tune it on imbalanced labeled data. This fine-tuning approach essentially uses the pretrained neural network as a feature extractor, which is a standard practice in downstream analysis \cite{zhou2016learning, sharif2014cnn, radford2021learning, howard2018universal}.  We consider representative datasets for three data modalities. We train an SVM for each dataset.
\begin{enumerate}
    \item \textit{Tabular data}. We use a single-cell RNA-seq dataset of peripheral blood mononuclear cells treated with interferon-$\beta$ (IFNB) \cite{ifnb}, which has dimension $d = 2,000$. We randomly choose two classes, for example, class 5 (CD4 Naive T cells) and class 4 (CD4 Memory T cells), and subsample an imbalanced training set. Class 4 is the minority class, with imbalance ratio $\pi = 0.2$ and total sample size $n = 953$. %A max-margin classifier is trained with logistic loss.
    \item \textit{Image data}. We use CIFAR-10 image dataset \cite{KrizhevskyCIFAR102009}. The pretrained ResNet-18 model \cite{resnet, ResNet18_CIFAR10} is applied to the test set to extract the features of dimension $d = 512$. We randomly choose two classes, for example, class 5 (dog) and class 4 (deer), and subsample a imbalanced training set. Class 4 is the minority class, with imbalance ratio $\pi = 0.1$ and total sample size $n = 555$. %A max-margin classifier is trained with cross-entropy (logistic) loss.
    \item \textit{Text data}. We use IMDb movie review dataset \cite{IMDB} to perform binary sentiment classification. The BERT base model (110M) \cite{BERT} is applied to extract the features of dimension $d = 768$. We subsample an imbalanced training set. Negative reviews belong to the minority class, with imbalance ratio $\pi = 0.02$ and total sample size $n = 6,377$. %A max-margin classifier is trained with cross-entropy (logistic) loss.
\end{enumerate}



The empirical experiments reveal a pervasive structure in the ELD: for linearly separable data, the ELD can be fitted by two rectified Gaussian distributions, and such distributional truncation solely explains overfitting in high dimensions. Moreover, the minority class suffers more from this truncation effect as its test accuracy is worse. As our theoretical insights below reveal, this is because both classes share a common ``overfitting budget'' so that the minority margin boundary is disproportionally shifted.


\paragraph{Theoretical foundation.} Now we present a variational characterization of the ELD. We highlight a summary of our result for the separable case here and defers the non-separable case to Section~\ref{sec:logit}.
Consider the asymptotic regime $n/d \to \delta$ where $\delta \in (0,\infty)$ is called the limiting aspect ratio. Recall that $(\hat\vbeta, \hat\beta_0, \hat\kappa)$ are the trained parameters in \cref{eq:SVM-0}, where $\hat\kappa$ is the margin of classifier $\hat{y} (\xx) = 2 \mathbbm{1}\{ \hat f(\vx) > 0\} - 1$ with $\hat f(\vx) = \langle \vx, \hat \vbeta \rangle + \hat \beta_0$. Denote 
\begin{equation}\label{eq:rho_hat}
\hat \rho := 
\biggl\< \frac{\hat \vbeta}{\| \hat \vbeta \|}, \frac{\vmu}{\| \vmu \|} \biggr\>\,.
\end{equation}
One may expect $(\hat\rho, \hat\beta_0, \hat\kappa)$ to have some limits as $n,d \to \infty$. We define their asymptotics as follows.

\begin{defn}
    Let $(\rho^*, \beta_0^*, \kappa^*)$ be an optimal solution to the following variational problem:
			\begin{equation}
                \label{eq:asymp}
				\begin{array}{cl}
					\underset{ \rho \in [-1, 1], \beta_0 \in \R, \kappa > 0 , \xi \in \cL^2  }{ \mathrm{maximize} } & \kappa, \\
					\underset{ \phantom{\smash{\bm\beta \in \R^d, \beta_0 \in \R, \kappa \in \R} } }{\text{subject to}} &  \rho \| \bmu\| + G + Y \beta_0 + \sqrt{1 - \rho^2} \xi \ge \kappa,  
					\qquad \E[\xi^2]  \le  1/\delta,
					\end{array}
			\end{equation}
    where $\cL^2$ is the space of all square integrable random variables in the probability space $(\Omega, \mathcal{F}, \P)$, and $(Y, G) \sim P_y \times \normal(0, 1)$. Here $\xi$ is an unknown random variable (function) to be optimized.
\end{defn}
On a test point $(\vx_\mathrm{test}, y_\mathrm{test}) \sim P_{\vx,y}$, we consider the minority error and majority error
\begin{equation}\label{eq:Err_n}
    \Err_+ := \P\left( \hat f(\xx_\mathrm{test}) \le 0 \,\big|\, y_\mathrm{test} = +1 \right),
    \qquad
    \Err_- := \P\left( \hat f(\xx_\mathrm{test}) > 0 \,\big|\, y_\mathrm{test} = -1 \right).
\end{equation}
Note that the probabilities are taken over the training data, so these errors are nonrandom. The precise asymptotics of SVM and its ELD/TLD are summarized in the following theorem.

\begin{thm}[Separable data, informal version of \cref{thm:SVM_main}] \label{thm:SVM}
		Consider 2-GMM with asymptotics $n/d \to \delta \in (0,\infty)$ as $n, d \to \infty$. There is a critical threshold $\delta_c = \delta_c(\pi, \|\bmu\|)$, such that when $\delta < \delta_c$, the following holds as $n, d \to \infty$:
		\begin{enumerate}[label=(\alph*)]
			\item \textbf{Phase transition.} 
                \begin{equation*}
                    \P\left\{ \text{training set is linearly separable} \right\} \to 1.
                \end{equation*}
                
			\item \textbf{Parameter convergence.} 
                \begin{equation*}
                    (\hat \rho, \hat\beta_0, \hat\kappa) \xrightarrow{\mathrm{p}} (\rho^*, \beta_0^*, \kappa^*),
                \end{equation*}
                where $(\rho^*, \beta_0^*, \kappa^*)$ is the unique solution to \cref{eq:asymp}.

                \item 
                \textbf{Asymptotic errors.} The limits of minority an majority errors are
                \begin{equation*}
                    \Err_{+}  \to  \Phi \left(- \rho^* \norm{\bmu}_2 - \beta_0^* \right),
                    \qquad
                    \Err_{-}  \to  \Phi \left(- \rho^* \norm{\bmu}_2  + \beta_0^* \right),
                \end{equation*}
                where $\Phi$ denotes the cumulative distribution function of standard Gaussian.
			\item \label{thm:SVM_c}
            \textbf{ELD convergence.} The empirical (training) logit distribution $\hat{\nu}_n$ has limit $\nu_*$ in the sense that
                \begin{equation*}
                    W_2(\hat{\nu}_n , \nu_* ) \conp 0, \qquad \text{where} ~ 
                    \nu_* := \Law \,\bigl( Y,  Y \max\{\kappa^*, \rho^* \| \bmu \| + G + Y \beta_0^* \} \bigr).
                \end{equation*}
                \textbf{TLD convergence.} The testing logit distribution $\hat{\nu}_n^\mathrm{test}$ has limit $\nu^\mathrm{test}_*$ in the sense that
                \begin{equation*}
                    \hat{\nu}_n^\mathrm{test} \conw \nu^\mathrm{test}_*, \qquad \text{where} ~ 
                    \nu^\mathrm{test}_* := \Law \,\bigl( Y, Y (\rho^* \| \bmu \| + G + Y \beta_0^*) \bigr).
                \end{equation*}
		\end{enumerate}
\end{thm}


We make a few comments.
\begin{itemize}
\item \textit{Overfitting effect.} The random variable $\xi$ represents the distortion of ELD due to high dimensions. With a smaller aspect ratio $\delta$, there is more flexibility in finding $\xi$ that satisfies the constraint $\E[\xi^2] \le 1/\delta$ to maximize the margin, 
thereby distorts the TLD to produce the truncation effect.
%which in the meantime distorts the ELD.
%the overfitting effect in high dimensions \cite{montanari2022overparametrizedlineardimensionalityreductions}. With  a smaller aspect ratio $\delta$, there is more flexibility in finding $\xi$ to satisfy the constraint %$\E_{(y,g,\xi) \sim \bar P_0} [\xi^2] \le 1/\delta$ 
%\E[\xi^2] \le 1/\delta$. 
In fact, in order to maximize $\kappa$, the first inequality constraint must be tight, which yields the explicit formula for any given $\rho, \beta_0, \kappa$:
%for any given $\rho, \beta_0$, the optimal $\xi, \kappa$ are given by the following explicit formula:
\begin{align}
\sqrt{1 - \rho^2} \, \xi  & = \max \{ \kappa, \rho \| \vmu\| + G + Y \beta_0 \} - ( \rho \| \vmu\| + G + Y \beta_0 ) \notag \\
&= (\kappa - \rho \| \vmu\| - G - Y \beta_0)_+, \qquad \text{where}~a_+ := \max\{0,a\}. \label{def:opt-map}
\end{align}
Thus, we can view $\xi$ as a map that pushes the overlapping probability masses in the TLD to the margin boundaries in the ELD. It is the cause for the discrepancy between the ELD and the TLD.

\item \textit{More truncation for minority class.} Due to imbalance, transporting the probability mass in the minority ELD as \cref{def:opt-map} incurs less ``cost'' to the overall ``budget'' $\E[\xi^2] \le 1/\delta$.
%distortion in the minority ELD has a smaller effect on $\E[\xi^2]$
%Under the ``budget'' $\E[\xi^2] \le 1/\delta$, maximizing the margin leads to more severe  the minority distribution to be distorted. 
Formally, according to \cref{thm:SVM}, the limiting TLD for each class is
\begin{equation*}
    \text{minority:}~ \normal \left( \rho^* \|\vmu\| + \beta_0^*, 1\right),
    \qquad
    \text{majority:}~ \normal \left( -\rho^* \|\vmu\| + \beta_0^*, 1\right).
\end{equation*}
It can be shown that $\rho^* \ge 0$ and $\beta_0^* < 0$. So the minority TLD is closer to the decision boundary.
%Then $\abs{\rho^* \|\vmu\| + \beta_0^*} < \abs{-\rho^* \|\vmu\| + \beta_0^*}$, which indicates the minority TLD is closer to the decision boundary and margin than the majority TLD. 
As the result, the minority class suffers more from the truncation effect (overfitting) than the majority class.


\item \textit{Optimal transport perspective.} As a simple consequence, we show in \cref{sec:logit_SVM} that $\mathtt{T}^*(x) = \max\{\kappa^*, x \}$ gives the optimal transport map that maps the limit distribution $\nu_*$ to $\nu_*^\mathrm{test}$ and minimizes the $W_2$ distance between them.

\item \textit{Non-separable case.} When $\delta > \delta_c$, the training dataset is not separable with high probability, and SVM can no longer be viewed as the limit of the logistic regression. We analyze the logistic regression classifier and characterize its corresponding ELD. Instead of truncation, overfitting emerges as nonlinear shrinkage governed by the proximal operator (gradient of the Moreau envelope). As $\delta \in (\delta_c, \infty)$ decreases, the nonlinear shrinkage transitions from an identity map (no overfitting) to the truncation map $\mathtt{T}^*(x) = \max\{\kappa^*, x \}$ (severe overfitting). See \cref{thm:logistic_main} for its explicit expression and \cref{append_subsec_prox} for its function plot.
\end{itemize}


% Recall that $\hat\kappa$ is the margin of classifier $\hat f(\xx) = \< \xx, \hat\vbeta \> + \hat\beta_0$. 
Here, we provide some intuitions about the ELD and TLD of the max-margin classifier.
% Under the 2-GMM assumption in \cref{model}, $(\hat \rho, \hat \beta_0, \hat\kappa)$ converges in probability to certain limit quantities $(\rho^*, \beta_0^*, \kappa^*)$. 
On a test point $(\vx_\mathrm{test}, y_\mathrm{test}) \sim P_{\vx,y}$, it is easy to see that $y_\mathrm{test}\hat f(\vx_\mathrm{test})$ is approximately distributed as a mixture of two Gaussians:
\begin{align*}
    y_\mathrm{test} (\langle \vx_\mathrm{test} , \hat \vbeta \rangle + \hat \beta_0)  
    &= y_\mathrm{test} \left\langle  y_\mathrm{test}\vmu + \normal(\bzero, \bI_d),  \hat\vbeta \right\rangle + y_\mathrm{test} \hat \beta_0 \\
    &= \hat\rho \,  \| \vmu\| +  \left\langle  \normal(\bzero, \bI_d),  \hat\vbeta \right\rangle  + y_\mathrm{test} \hat \beta_0 \\
    &\approx \rho^* \| \vmu\| + G + Y \beta_0^*, \qquad \qquad  \text{where}~(Y, G) \sim P_y \times \normal(0,1).
\end{align*}
Essentially, $\hat f (\vx)$ is a linear projection of the 2-GMM input points to one dimension with a smaller separation between the two classes. However, on a training point $(\vx_i, y_i)$, there is a distortion effect on the distribution due to the margin constraints---the following holds for a ``typical'' training point: 
%\begin{equation*}
%    y_i (\langle \hat \vbeta, \vx_i \rangle + \hat \beta_0) = \max\{ \kappa^*, \rho^* \| \vmu\| + g_i + y_i \beta_0^* \} + o_\P(1), \qquad   \text{where}~g_i \sim \normal(0,1) ~ \text{is independent of }y_i.
%\end{equation*}
%I wonder if $o_\P(1)$ here is correct. We don't have convergence result for each individual training logit. Maybe we can simply use $\approx$ and defer a rigorous expression to the main theorem part.
\begin{equation*}
    y_i (\langle \vx_i, \hat \vbeta \rangle + \hat \beta_0) \approx \max\{ \kappa^*, \rho^* \| \vmu\| + G + Y \beta_0^* \} , \qquad   \text{where}~(Y, G) \sim P_y \times \normal(0,1).
\end{equation*}
This is because the max-margin classifier achieves overfitting in high dimensions, where a substantial portion of points (a.k.a. support vectors) fall on the margins $\hat\kappa \approx \kappa^*$.
% More precisely, this formula for logit margins hold in the distributional sense: 
% $W_2(\hat{\cL}_n, \cL_*) \conp 0$ where
% \begin{equation*}
%     \hat{\cL}_n := \frac1n \sum_{i=1}^n \delta_{ y_i (\langle \vx_i, \hat \vbeta \rangle + \hat \beta_0) }
%     \qquad
%     \cL_* := \Law \left(\max\{\kappa^*, \rho^* \| \bmu \| + G + Y \beta_0^* \} \right),
% \end{equation*}
% and where $W_2$ is the $2$-Wasserstein distance between two distributions.







\subsection{Rebalancing margin is crucial}
\label{subsec:rebal}

Rebalancing the margin is a common practice for remedying severe overfitting for the minority class. In binary classification, we choose a hyperparameter $\tau > 0$ and consider the margin-rebalanced SVM \cref{eq:SVM-m-reb} which shifts the decision boundary as shown in Figure~\ref{fig:SVM_cartoon}. For the logistic loss in \cref{eq:logistic}, we can similarly incorporate $\tau$ into the objective function. Margin rebalancing is widely used in machine learning \cite{li2002perceptron, li2005using, cao2019learning, karakoulas1998optimizing, wu2003class}, but the impact of $\tau$ on test accuracy is not fully explored. 

% \begin{equation}\label{eq:SVM-m-reb}
%     \begin{array}{cl}
% 	\underset{ \vbeta \in \R^d, \beta_0 \in \R, \kappa \in \R }{ \mathrm{maximize} } & \kappa, \\
% 	\underset{ \phantom{\smash{\vbeta \in \R^d, \beta_0 \in \R, \kappa \in \R} } }{\text{subject to}} & 
% 			y_i ( \langle \xx_i, \vbeta \rangle + \beta_0 ) \ge \tau \kappa,
% 			\quad \forall\, i \in \mathcal{I}_+, \\
% 			& 
% 			y_i ( \langle \xx_i, \vbeta \rangle + \beta_0 ) \ge \phantom{\tau} \kappa,
% 			\quad \forall\, i \in \mathcal{I}_-, \\
% 			& \|\vbeta\|_2  \le  1.  
% 			\vphantom{\dfrac12}
% \end{array}
% \end{equation}
\begin{figure}[h]
%     \centering
% \includegraphics[width=0.65\textwidth]{Figs/SVM_cartoon.pdf}
\noindent
\begin{minipage}[h]{0.6\textwidth}
    \includegraphics[width=1\textwidth]{Figs/SVM_cartoon.pdf}
\end{minipage}
\hfill
\begin{minipage}[h]{0.4\textwidth}
    \begin{equation}\label{eq:SVM-m-reb}
    \begin{array}{l}
	\underset{ \vbeta \in \R^d, \beta_0 \in \R, \kappa \in \R }{ \mathrm{maximize} }  \quad \kappa, \\
	\underset{ \phantom{\smash{\vbeta \in \R^d, \beta_0 \in \R, \kappa \in \R} } }{\text{subject to}} 
        \vphantom{\dfrac12} \\
			\ \ y_i ( \langle \xx_i, \vbeta \rangle + \beta_0 ) \ge \tau \kappa,
			\quad \forall\, i \in \mathcal{I}_+, \\ 
			\ \ y_i ( \langle \xx_i, \vbeta \rangle + \beta_0 ) \ge \phantom{\tau} \kappa,
                \vphantom{\dfrac12}
			\quad \forall\, i \in \mathcal{I}_-, \\
			\ \ \|\vbeta\|_2  \le  1.  
			\vphantom{\dfrac12}
    \end{array}
    \end{equation}
\end{minipage}
    \caption{
    \textbf{Schematic illustration of margin-rebalanced SVM}. The dotted line is the decision boundary for the original SVM, and the solid line is the decision boundary for margin-rebalanced SVM.
    }
    \label{fig:SVM_cartoon}
\end{figure}


We will conduct analysis under two regimes: \textbf{(i) proportional regime} where $n,d \to \infty$ and $n/d \to \delta$ with $\delta \in (0,\infty)$, and \textbf{(ii) high imbalance regime} in the following sense:
\begin{equation}\label{setup-high-imbalance}
\pi \propto d^{-a}, \qquad \norm{\vmu}^2 \propto d^b, \qquad n \propto d^{c+1}.
\end{equation}

For problems with data imbalance, often correctly classifying the minority data points is equally important as correctly classifying the majority data points. 
%One goal of margin balancing is to reduce the gap between $\Err_+$ and $\Err_-$, which is important when accurately classifying the minority data points is valuable. 
For this purpose, we introduce \textit{the balanced error}: 
\begin{equation}\label{eq:balanced_err}
\Err_\mathrm{b} = \frac{1}{2} \Err_+ + \frac{1}{2} \Err_-\, .
\end{equation}
%with its asymptotic counterpart $\Err_\mathrm{b}^* = \frac{1}{2} \Err_+^* + \frac{1}{2} \Err_-^*$.

\paragraph{Empirical phenomenon.} For the  proportional regime, we generate imbalanced 2-GMM based on \cref{setup-high-imbalance} with sample size $n = 100$ and dimension $d = 200$, under different settings of $\norm{\bmu}_2$ and $\pi \in (0, \frac12]$. We train an SVM \cref{eq:SVM-m-reb} with margin rebalancing (set $\tau$ to certain optimal value) and without (set $\tau = 1$) respectively, for each configuration. We calculate the minority error $\Err_+$, majority error $\Err_-$, and balanced error $\Err_\mathrm{b}$ on an independent test set, averaged over 100 replications, and plot these errors against different values of $\pi$ in Figure~\ref{fig:Err_pi}. The smooth curves represent the asymptotic test errors, i.e., the limits of $\Err_+$, $\Err_-$, $\Err_\mathrm{b}$ as $n, d \to \infty$ according to \cref{thm:SVM}. Experimental  details are deferred to \cref{append_sec:exp}. 

%consider one simulation study and one real-data study. We plot the majority accuracy and minority accuracy under varying $\tau$ and imbalance ratio $\pi$, shown in Figure~\ref{todo}. 

%\ljy{[TODO: Simulation and real data analysis for varing $\tau$.]}


%We generate imbalanced 2-GMM based on \cref{setup-high-imbalance} with sample size $n = 100$ and dimension $d = 200$, under differents setting of $\norm{\bmu}_2$ and $\pi \in (0, \frac12]$. We train a margin-rebalanced SVM \cref{eq:SVM-m-reb}, by considering margin rebalancing ($\tau = \tau^\mathrm{opt}$) or not ($\tau = 1$) respectively, for each configuration. We calculate the minority error ($\Err_+$), majority error ($\Err_-$), and balanced error ($\Err_\mathrm{b}$) on a independent test set, and plot these errors against different values of $\pi$ in Figure~\ref{todo}.

For the naive SVM where $\tau = 1$, as $\pi$ decreases to $0$, the minority error $\Err_+$ increases to $1$, the majority error $\Err_-$ decreases to $0$, and the balanced error tends to the trivial $\frac12$. The opposite trends of minority and majority errors show that overfitting hurts minority class more than majority class. In contrast, under optimal $\tau$, we are able to even out the minority and majority errors at the same level. As a result, margin rebalancing is advantageous for reducing the balanced error.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figs/6_Minority_Error_vs_pi,n=1000,d=500.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/5_Majority_Error_vs_pi,n=1000,d=500.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/4_Balanced_Error_vs_pi,n=1000,d=500.pdf}
    \caption{
    \textbf{Impact of imbalance on test errors}. We show test errors from 2-GMM simulations with margin rebalancing (solid curves) and without (dashed curves) at three levels of signal strength $\| \vmu\|_2$ under varying imbalanced ratios $\pi$.
    }
    \label{fig:Err_pi}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figs/6_Minority_Error_vs_tau.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/5_Majority_Error_vs_tau.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/4_Balanced_Error_vs_tau.pdf}
    \caption{
    \textbf{Effects of margin rebalancing on test errors}. We show test errors from 2-GMM simulations at three different imbalance ratios under varying $\tau$.
    }
    \label{fig:Err_tau}
\end{figure}

We also plot test errors against different values of $\tau$ in \cref{fig:Err_tau} under the same simulation setting. The minority and majority errors have monotone but opposite trends in $\tau$, since increasing $\tau$ essentially moves the decision boundary from the side of minority class to the majority class. Such trade-off between the two classes results in a U-shaped curve for the balanced error. This indicates that we can find a unique optimal $\tau=\tau^{\mathrm{opt}}$ which minimizes $\Err_\mathrm{b}$, and $\tau^{\mathrm{opt}}$ is larger as $\pi$ becomes smaller.


For the high imbalance regime, we generate imbalanced 2-GMM based on \cref{setup-high-imbalance} with a sufficiently large dimension $d=2000$. We choose $\tau = \tau_d = d^r$ for different values of $r \ge 0$. We fix $b = 0.3$, $c = 0.1$ and vary $a, r$, and then we train a margin-rebalanced SVM \cref{eq:SVM-m-reb} for each configuration. Figure~\ref{fig:High_imb_heat} shows that there are three phases in terms of the majority/minority errors. In particular, the margin rebalancing is crucial for one phase with moderate signal strength. 


\begin{figure}[t]
    \centering
    \includegraphics[height=0.47\textwidth]{Figs/HighimbB_Cmu=0.75,err_Minority.pdf}
    \includegraphics[height=0.47\textwidth]{Figs/HighimbB_Cmu=0.75,err_Majority.pdf}
    \caption{
        \textbf{Phase transition in high imbalance regime}. Minority/majority errors  under different settings of parameters $(a, b, c)$ and $\tau = d^r$. \textbf{Left:} minority accuracy is (i) high for any $\tau$ under high signal, (ii) high for $\tau \gg d^{(a - b - c)/2}$ under moderate signal, but (iii) low for any $\tau$ under low signal. 
        \textbf{Right:} majority accuracy is close to 1 under high and moderate signal as long as $\tau$ is not too large.   
    }
    \label{fig:High_imb_heat}
\end{figure}

\paragraph{Theoretical foundation.} For the proportional regime, denote $\Err_+^*$, $\Err_-^*$, $\Err_\mathrm{b}^*$ as the limits of $\Err_+$, $\Err_-$, $\Err_\mathrm{b}$ as $n \to \infty$, respectively, then we have the following result.
\begin{prop}[Optimal $\tau$ in proportional regime, informal version of \cref{prop:tau_optimal}]\label{prop:tau_opt}
    Consider 2-GMM with asymptotics $n/d \to \delta \in (0,\infty)$ as $n, d \to \infty$.
    Define $\tau^\mathrm{opt}$
    % \in \mathbb{R}$
    as the optimal margin ratio which minimizes the asymptotic balanced error
\begin{equation*}
    \tau^\mathrm{opt} :=  \argmin_{\tau} \Err_\mathrm{b}^* = \argmin_{\tau} \big\{ \Phi(- \rho^*\norm{\bmu} - \beta_0^* )
    + \Phi(- \rho^*\norm{\bmu} + \beta_0^* ) \big\}.
\end{equation*}
When $\tau = \tau^\mathrm{opt}>0$, we have $\beta_0^* = 0$, $\Err_+^* = \Err_-^* = \Err_\mathrm{b}^*$. 
%In particular, we have
	%\begin{equation}\label{eq:tau_opt}
        %\tau^\mathrm{opt} =  \dfrac{g_1^{-1} \left( \dfrac{\rho^*}{2 \pi \norm{\bmu}_2 \delta} \right) + \rho^* \norm{\bmu}_2}{g_1^{-1} \left( \dfrac{\rho^*}{2 (1 - \pi) \norm{\bmu}_2 \delta} \right) + \rho^* \norm{\bmu}_2}.
    %\end{equation}
    % Note that $\tau^\mathrm{opt}$ depends on $\pi$ through $\rho^*$ and $\beta_0^*$.
\end{prop}
A critical observation is that, changing $\tau$ only has an effect on $\hat \beta_0$ but not $\hat \vbeta$. Thus, changing $\tau$ is effectively shifting the decision boundaries between the two classes. Our analysis reveals that the optimal $\tau$ has a complicated dependence on $\pi, \norm{\vmu}, \delta$. When the problem is not close to degenerate, roughly speaking $\tau^\mathrm{opt} \asymp \sqrt{1/\pi}$; see Section~\ref{sec:rebalacing} for details. 

%different from what the prior work \cite{cao2019learning} suggests, which uses a conservative choice $\tau \asymp \pi^{1/4}$ in the model agnostic setting.

Under $\tau = \tau^\mathrm{opt}$, our theoretical result (see \cref{prop:Err_monotone} for a formal statement) shows monotone trends of the errors: the limiting minority/majority/balanced error is a decreasing function of $\pi \in (0, \frac12)$ (imbalance ratio), $\norm{\bmu}$ (signal strength), and $\delta$ (aspect ratio); see summary in \cref{tab:monotone}.


For the high imbalance regime, margin rebalancing is necessary to achieve a small balanced error when the ``signal strength'' is moderate, which matches our empirical observations in \cref{fig:High_imb_heat}.

\begin{thm}[High imbalance] \label{thm:high-imbalance}
Consider 2-GMM with asymptotics \cref{setup-high-imbalance} as $d \to \infty$. Suppose that $a - c < 1$.
\begin{enumerate}
\item \label{thm:high-imb_high}
\textbf{High signal} (no need for margin rebalancing): $a - c< b$. If we choose $1 \le \tau_d \ll d^{b/2}$, then 
\begin{equation*}
\Err_+ = o(1), \qquad \Err_- = o(1).
\end{equation*}
\item \label{thm:high-imb_moderate}
\textbf{Moderate signal} (margin rebalancing is crucial): $b < a - c < 2b$. If we choose $d^{a-b-c} \ll \tau_d \ll d^{(a-c)/2}$, then
\begin{equation*}
\Err_+ = o(1), \qquad \Err_- = o(1).
\end{equation*}
However, if we naively choose $\tau_d \asymp 1$, then 
\begin{equation*}
\Err_+ = 1 - o(1), \qquad \Err_- = o(1).
\end{equation*}
\item \label{thm:high-imb_low}
\textbf{Low signal} (no better than random guess): $a - c > 2b$. For any $\tau_d$, we have
\begin{equation*}
\Err_\mathrm{b} \ge \frac12 -  o(1).
\end{equation*}
\end{enumerate}
\end{thm}



\subsection{Consequences for confidence estimation and calibration}
\label{subsec:conf_calib}

In the deep learning literature, the \emph{confidence} of a classifier often refers as the probability of the correctness of a prediction, i.e., the probability that the predicted label matches the true label. Formally, we define the confidence of the max-margin classifier as
    \begin{equation*}
        \hat p(\xx) := \sigma\bigl( \hat f(\xx) \bigr) = 
        \sigma\bigl( \< \xx, \hat\vbeta \> + \hat\beta_0 \bigr), \qquad \text{where}~\sigma(t) = \frac{1}{1 + e^{-t}}.
    \end{equation*}
We would like the confidence $\hat p$ to be a good approximation of the true conditional probability (namely Bayes-optimal probability), that is $\hat p(\xx) \approx p^*(\xx) = \P( y = 1 \,|\, \xx )$.
However, finding the Bayes-optimal probability is usually intractable in deep learning models, and estimating this conditional probability $p^*(\vx)$ requires nonparametric estimation in high dimensions, which is difficult. %conditioned on the high dimensional $\xx$ can be difficult. 
The notion of \emph{calibration} is therefore widely used in literature, which measures the faithfulness of prediction probabilities \cite{murphy1967verification, dawid1982well, gupta2020distribution, guo2017calibration}.
%Formally, for a given classifier $f$, let $p(\vx) \in [0,1]$ be its confidence (usually constructed as a function of the classifier $p(\vx) = g(f(\vx))$). We say that $p$ is (approximately) \emph{calibrated} if
Formally, the confidence $\hat p$ is (approximately) \emph{calibrated} if
\begin{equation}\label{eq:calibrated}
    \hat p(\xx) \approx \hat p_0(\xx) := \P \bigl( y = 1 \,|\,  \hat p(\xx) \bigr).
\end{equation}
This notion requires that the predicted probability by $\hat p$ for any $\vx$ matches the actual probability.
%Notably, both the constant function $\p(\vx) \equiv \mathbb{P}(y=1)$ and the Bayes-optimal $p = p^*$ are perfectly calibrated. 
We list several popular miscalibration metrics below \cite{kumar2019verified, kuleshov2015calibrated, vaicenavicius2019evaluating}:
\begin{itemize}
    \item \textbf{Calibration error.}
    \begin{equation}\label{eq:CalErr}
    \begin{aligned}
        \mathrm{CalErr}(\hat p) & :=    \E\left[ \Bigl(  
        \hat p(\xx) - \P \bigl( y = 1 \,|\,  \hat p(\xx) \bigr)
        \Bigr)^2 \right].
    \end{aligned}
    \end{equation}

    \item \textbf{Mean squared error (MSE).}
    \begin{equation}\label{eq:MSE}
        \mathrm{MSE}(\hat p)  := \E\left[ \bigl( \mathbbm{1}\{ y = 1 \} - \hat p(\xx) \bigr)^2 \right]\,.
    \end{equation}
    \item \textbf{Confidence estimation error.}

    \begin{equation}\label{eq:ConfErr}
        \mathrm{ConfErr}(\hat p) :=  \E\left[ \bigl( \hat p(\xx) - p^*(\xx) \bigr)^2 \right]\,.
    \end{equation}
\end{itemize}





\paragraph{Empirical phenomenon.}


We consider the same 2-GMM simulation experiment as in Figure~\ref{fig:Err_pi} (the proportional regime).  After margin rebalancing, we calculate the above three miscalibration metrics Eqs.~\eqref{eq:CalErr}---\eqref{eq:ConfErr} on an independent test set, average over 100 replications, and plot these errors against different values of $\pi$ in \cref{fig:Calibration}. The smooth curves represent the asymptotic errors, i.e., the limits of $\mathrm{CalErr}(\hat p)$, $\mathrm{MSE}(\hat p)$, $\mathrm{ConfErr}(\hat p)$ as $n, d \to \infty$ according to \cref{thm:SVM}. Experimental details are deferred to \cref{append_sec:exp}. Notably, all these errors increase as imbalance becomes more severe (namely $\pi$ being smaller).


\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figs/bal=1_8_Calibration_Error_vs_pi,n=1000,d=500.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/bal=1_7_Mean_Squared_Error_vs_pi,n=1000,d=500.pdf}
    \includegraphics[width=0.32\textwidth]{Figs/bal=1_9_Confidence_Estimation_Error_vs_pi,n=1000,d=500.pdf}
    \caption{
    \textbf{Impact of imbalance on uncertainty quantification}. We plot miscalibration metrics \cref{eq:CalErr}--\eqref{eq:ConfErr} for 2-GMM simulations with optimal margin rebalancing $(\tau = \tau^\mathrm{opt}).$ We find that high imbalance (namely small $\pi$) exacerbates miscalibration.
    }
    \label{fig:Calibration}
\end{figure}


We also plot confidence reliability diagrams for the 2-GMM simulations. Widely used for diagnosis of classifiers \cite{guo2017calibration}, the reliability diagrams plot the value of $\mathbb{P}(y=1 \,|\, \hat p(\vx) = p)$ as a function of $p$. In \cref{fig:reliability_GMM}, we calculate $\mathbb{P}(y=1 \,|\, \hat p(\vx) = p)$ for varying $p \in \{0.05, 0.15, \ldots, 0.95\}$ using an empirical estimate based on an independent test set. The histograms show these empirical conditional probabilities after binning. The dashed diagonal line represents perfect calibration (i.e., when \cref{eq:calibrated} strictly equals), and deviation from this line means miscalibration of the classifier. 
    In our simulations, we fix $\norm{\bmu}=1$, $n=1000$, $d=500$ and choose a range for different $\pi$, under $\tau = \tau^\mathrm{opt}$. Again, we observe miscalibration getting worse when data becomes increasingly imbalanced (i.e., as $\pi$ decreases).


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Figs/Calibration_pi=0.05,mu=1,n=1000,d=500.pdf}
    \caption{
    \textbf{Reliability diagrams: imbalance worsens calibration}. In our 2-GMM simulations, we train SVMs and obtain confidence $\hat p(\vx)$. For each $p$ ($x$-axis), we calculate $\mathbb{P}(y=1 \,|\, \hat p(\vx) = p)$ ($y$-axis) based on an independent test set. We find that as imbalance increases (smaller $\pi$), the classifier becomes more miscalibrated as the predicted probabilities are more inflated.
    }
    \label{fig:reliability_GMM}
\end{figure}

\paragraph{Theoretical foundation.} 

We provide theoretical results to partially explain the monotone trends. All three miscalibration metrics have limits as $n, d \to \infty$, $n/d \to \delta$, which we denote by $\mathrm{CalErr}^*$, $\mathrm{MSE}^*$, $\mathrm{ConfErr}^*$. For example, we prove that $\mathrm{MSE}^*$ is a monotone decreasing function of model parameters $\pi$, $\norm{\bmu}_2$, $\delta$, and $\mathrm{CalErr}^*$ is monotone decreasing in $\delta$. \cref{tab:monotone} summarizes the monotone behavior of test errors and miscalibration metrics that we 
establish
in this paper.



\begin{table}[h!]
\begin{equation*}
\renewcommand{\arraystretch}{1.2}
    \begin{array}{r|c|ccc}
    \hline
           & \mathrm{Err}_{+}^*, \mathrm{Err}_{-}^*, \mathrm{Err}_{\mathrm{b}}^* & \mathrm{CalErr}^*  & \mathrm{MSE}^*  &  \mathrm{ConfErr}^* \\
    \hline
      \text{imbalance ratio}~\pi \uparrow
          &  \downarrow ~ \text{(Prop.~\ref{prop:Err_monotone})}    
          &  
          &  \downarrow ~ \text{(Prop.~\ref{prop:conf})}    
          &  \downarrow ~ \text{(Claim~\ref{claim:conf})}   \\
      \text{signal strength}~\norm{\bmu}_2 \uparrow
          &  \downarrow ~ \text{(Prop.~\ref{prop:Err_monotone})}    
          &  \downarrow ~ \text{(Claim~\ref{claim:conf})}
          &  \downarrow  ~ \text{(Prop.~\ref{prop:conf})}   
          &       \\
      \text{aspect ratio}~n/d \to \delta \uparrow
          &  \downarrow ~ \text{(Prop.~\ref{prop:Err_monotone})}    
          &  \downarrow ~ \text{(Claim~\ref{claim:conf})}
          &  \downarrow ~ \text{(Prop.~\ref{prop:conf})}    
          &  \downarrow  ~ \text{(Prop.~\ref{prop:conf})}  \\
    \hline
    \end{array}
\end{equation*}
\vspace{-5mm}
\caption{Monotonicity of test errors and miscalibration metrics on model parameters. }
\label{tab:monotone}
\end{table}

\subsection{Extensions}

Our theory focuses on two-class classification problems with an isotropic covariance matrix. For the cases of multiple classes and non-isotropic covariance matrices, we believe that similar characterization of ELDs exist. In Section~\ref{sec:discuss}, we provide theoretical conjectures and empirical support for the general cases.

