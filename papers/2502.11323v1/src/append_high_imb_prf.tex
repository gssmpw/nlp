\section{Margin rebalancing in high imbalance regime: Proof of \cref{thm:main_high-imbal}}
\label{append_sec:high_imb}

Without loss of generality, we may consider the following case as a substitute of \cref{setup-high-imbalance}:
\begin{equation*}
\pi = d^{-a}, \qquad \norm{\vmu}_2^2 = d^b, \qquad n = d^{c+1}.
\end{equation*}
% The notations commonly used are summarized in the table below. Be aware that some are only used in the proof of this section and may have different meanings in other sections.

% \begin{table}[h!]
%     \centering
%     \label{tab:high_imb_notation}
%     \renewcommand{\arraystretch}{1.25}
%     \begin{tabular}{>{\centering\arraybackslash}m{0.125\linewidth}|>{\raggedright\arraybackslash}m{0.65\linewidth}|>{\centering\arraybackslash}m{0.12\linewidth}}
%         \hline\hline
%         \textbf{Symbol} & \textbf{Description} & \textbf{Definition} \\ 
%         \hline\hline
%         $\vbeta, \beta_0$ & 
%         the slope and intercept parameters for arbitrary linear classifier $f(\xx) = \< \xx, \vbeta \> + \beta_0$ ($\norm{\vbeta}_2 = 1$) &  \\ 
%         \hline
%         $\rho, \vtheta$ & 
%         reparametrization of $\vbeta$ via orthogonal projections $\bP_{\vmu}$ and $\bP_{\vmu}^\perp$ & \eqref{eq:def-rho-theta} \\ 
%         \hline
%         $\kappa_i, \kappa_i(\vbeta, \beta_0), $ $\kappa_i(\rho, \vtheta, \beta_0)$ & 
%         the (scaled) logit margin of $f(\xx) = \< \xx, \vbeta \> + \beta_0$ ($\norm{\vbeta}_2 = 1$) for $(\xx_i, y_i)$, expressed in parameters $(\vbeta, \beta_0)$ or $(\rho, \vtheta, \beta_0)$ & \eqref{eq:logits} \\
%         \hline
%         $\kappa, \kappa(\vbeta, \beta_0), $ $\kappa(\rho, \vtheta, \beta_0)$ & 
%         the margin of $f(\xx) = \< \xx, \vbeta \> + \beta_0$ ($\norm{\vbeta}_2 = 1$) for data $(\XX, \yy)$, expressed in parameters $(\vbeta, \beta_0)$ or $(\rho, \vtheta, \beta_0)$  & \eqref{eq:margin}, \eqref{eq:margin_reparam} \\
%         \hline
%         $\hat\vbeta, \hat\beta_0$ & 
%         the slope and intercept for max-margin linear classifier, the optimal solution to \cref{eq:SVM-m-reb} and \cref{eq:SVM} &  \\ 
%         \hline
%         $\hat\rho, \hat\vtheta$ & 
%         reparametrization of $\hat\vbeta$ via orthogonal projections $\bP_{\vmu}$ and $\bP_{\vmu}^\perp$, the optimal solution to \cref{eq:SVM-rho_theta}  & \eqref{eq:def-rho-theta_hat} \\ 
%         \hline
%         $\hat\kappa, \kappa(\hat\vbeta, \hat\beta_0), $ $\kappa(\hat\rho, \hat\vtheta, \hat\beta_0)$ & 
%         the maximum margin for data $(\XX, \yy)$, 
%         the optimal objective value of \cref{eq:SVM}, 
%         the optimal solution to \cref{eq:SVM-m-reb} and \eqref{eq:SVM-rho_theta},
%         can be expressed in $(\hat\vbeta, \hat\beta_0)$ or $(\hat\rho, \hat\vtheta, \hat\beta_0)$ & \eqref{eq:max-margin}, \eqref{eq:max-margin1} \\
%         \hline
%         $\wt\rho, \wt\vtheta, \wt\beta_0$ & 
%         a constructed solution with explicit expression, a good ``proxy'' for the max-margin solution $\hat\rho, \hat\vtheta, \hat\beta_0$  & \eqref{eq:param_star} \\ 
%         \hline
%         $\bar\kappa$ & 
%         a data-dependent (stochastic) upper bound on the maximum margin $\hat\kappa$ for data $(\XX, \yy)$  & \eqref{eq:kappa_upper} \\ 
%         \hline
%         $\check\beta_0, \check\beta_0(\vbeta)$ & the optimal intercept for a linear classifier with slope $\vbeta$, which satisfies margin-balancing condition \cref{eq:margin-bal} or \eqref{eq:svm_sv_bal}
%          & \eqref{eq:beta0_optim} \\ 
%         \hline
%         $\mathsf{sv}_+(\vtheta)$, $\mathsf{sv}_-(\vtheta)$\phantom{,} & the indices of the smallest $\kappa_i$ in each class, which is known as support vectors, depends on $\vtheta$ and data (with $\rho = \hat\rho$ fixed)  
%         & \eqref{eq:SV_def_theta} \\ 
%         \hline
%         $\mathsf{v}_+(\vtheta)$, $\mathsf{v}_-(\vtheta)$\phantom{,} & the indices of the smallest $y_i\< \zz_i, \vtheta \>$ in each class, a ``proxy'' of support vectors, depends on $\vtheta$ and data (with $\rho = \hat\rho$ fixed)  
%         & \eqref{eq:V_def_theta} \\ 
%         \hline
%         $\wt\vtheta_+, \wt\vtheta_-$ & 
%         statistics analogous to $\wt\vtheta$, only used in the proof of \cref{lem:theta_hat_z}
%         & \eqref{eq:wt_theta_pm} \\
%         \hline\hline
%     \end{tabular}
%     \caption{List of commonly used notations in \cref{append_sec:high_imb}.}
% \end{table}

\noindent
Consider a linear classifier based on $f(\xx) = \< \xx, \vbeta \> + \beta_0$ with $\norm{\vbeta}_2 = 1$. Denote projection matrices
\begin{equation*}
    \bP_{\vmu} := \frac{1}{\norm{\vmu}_2^2} \vmu \vmu^\top,
    \qquad 
    \bP_{\vmu}^\perp := \bI_d - \frac{1}{\norm{\vmu}_2^2} \vmu \vmu^\top,
\end{equation*}
where $\bP_{\vmu}$ is the orthogonal projection onto $\spann\{ \vmu \}$ and $\bP_{\vmu}^\perp$ is the orthogonal projection onto the orthogonal complement of $\spann\{ \vmu \}$. Then we define auxiliary parameters
\begin{equation}\label{eq:def-rho-theta}
    \rho := \left\< \vbeta, \frac{\vmu}{\norm{\vmu}_2} \right\>,
    \qquad
    \vtheta := 
    \begin{cases} 
        \, \dfrac{\bP_{\vmu}^\perp \vbeta}{\|\bP_{\vmu}^\perp \vbeta\|_2}
        = \dfrac{\bP_{\vmu}^\perp \vbeta}{\sqrt{1 - \rho^2}} , & \ \text{if} \ \abs{\rho} < 1, \\
        \, \vmu_\perp,         & \ \text{if} \ \abs{\rho} = 1, 
    \end{cases}
\end{equation}
where $\vmu_\perp \in \S^{d-1}$ is some deterministic vector such that $\vmu_\perp \perp \vmu$.
Therefore, we have the following decomposition:
\begin{equation*}
    \vbeta = \bP_{\vmu} \vbeta + \bP_{\vmu}^\perp \vbeta 
    = \rho\frac{\vmu}{\norm{\vmu}_2} + \sqrt{1 - \rho^2} \vtheta.
\end{equation*}
Note that $\norm{\vtheta}_2 = 1$, $\vtheta \perp \vmu$, and there exists a one-to-one correspondence\footnote{
    In fact, this one-to-one mapping $\vbeta \mapsto (\rho, \vtheta)$ is restricted to $\S^{d-1} \to \Theta_{\rho,\vtheta}$, where the range is $\Theta_{\rho,\vtheta} :=  \{(\rho, \vtheta): \rho \in (-1, 1), \norm{\vtheta}_2 = 1, \vtheta \perp \vmu \} \cup \{ (\rho, \vtheta): \rho = \pm1, \vtheta = \vmu_\perp \} $. However, for simplicity, we can expand the parameter space of $(\rho, \vtheta)$ into $\{(\rho, \vtheta): \rho \in [-1, 1], \norm{\vtheta}_2 = 1, \vtheta \perp \vmu \}$. This is because if $\rho = \pm 1$, we have $\bP_{\vmu}^\perp \vbeta = \bzero$, and $\sqrt{1 - \rho^2} \vtheta = \bzero$ for any $\vtheta$. We will see that $\vtheta$ always appears in the form of $\sqrt{1 - \rho^2} \vtheta$ (for example, in the decomposition of $\vbeta$, and the expression of $\kappa_i$ and $\kappa$). That also explains why we can take $\vmu_\perp$ arbitrarily in \cref{eq:def-rho-theta}.
} between $\vbeta$ and $(\rho, \vtheta)$. Therefore, the logit margin of $f(\xx)$ for the $i$-th data point $(\xx_i, y_i)$ can be reparametrized as
\begin{align}
        \kappa_i  & =  \kappa_i(\vbeta, \beta_0) 
        : = \wt y_i ( \< \xx_i, \vbeta \> + \beta_0 )  \nonumber \\
        & = s_i y_i \left( \Bigl\< y_i \bmu + \zz_i, \rho\frac{\vmu}{\norm{\vmu}_2} + \sqrt{1 - \rho^2}\vtheta  \Bigr\>  + \beta_0 \right) \nonumber \\
        & = s_i \left( \rho \norm{\vmu}_2 + y_i\beta_0 + \rho y_i g_i + \sqrt{1 - \rho^2} y_i \< \zz_i, \vtheta \> \right) 
        = : \kappa_i(\rho, \vtheta, \beta_0),
        \label{eq:logits}
\end{align}
where $\zz_i \sim \subGind(\bzero, \bI_n; K)$ according to \cref{def:subgauss}, $K > 0$ is some absolute constant, and
\begin{equation*}
    s_i := \begin{cases} 
    \, \tau^{-1}, & \ \text{if} \ y_i = +1, \\
    \, 1,         & \ \text{if} \ y_i = -1, \end{cases}
\qquad
    g_i := \left\langle \zz_i, \frac{\vmu}{\norm{\vmu}_2} \right\rangle,
\end{equation*}
where $\vg := (g_1, \dots, g_n)^\top \sim \subGind(\bzero, \bI_n; K)$ by \cref{lem:subG}\ref{lem:subG-b}. Therefore, the margin (in \cref{eq:margin}) of $f(\xx)$ can be viewed as function $(\vbeta, \beta_0) \mapsto \kappa$ or $(\rho, \vtheta, \beta_0) \mapsto \kappa$ based on different parametrization:
\begin{equation}\label{eq:margin_reparam}
    \begin{aligned}
        \kappa & = \mathmakebox[\widthof{$\kappa(\rho, \vtheta, \beta_0)$}][l]{\kappa(\vbeta, \beta_0)} = \min_{i \in [n]} \kappa_i(\vbeta, \beta_0) \\
        & = \kappa(\rho, \vtheta, \beta_0) = \min_{i \in [n]} \kappa_i(\rho, \vtheta, \beta_0).
    \end{aligned}
\end{equation}
As a consequence, the max-margin optimization problem \cref{eq:SVM} or \eqref{eq:SVM-m-reb} can be expressed as
\begin{equation}
	\label{eq:SVM-rho_theta}
    \begin{array}{rl}
    \maximize\limits_{ \rho, \beta_0 \in \R, \vtheta \in \R^{d} } & % \kappa(\rho, \vtheta, \beta_0) = 
    % \min\limits_{i \in [n]} s_i \bigl( \rho \norm{\vmu}_2 + y_i\beta_0 + \rho y_i g_i + \sqrt{1 - \rho^2} y_i \< \zz_i, \vtheta \> \bigr) = : 
    \kappa(\rho, \vtheta, \beta_0), \\
    \text{subject to} 
	& \rho \in [-1, 1],  
    \vphantom{\maximize\limits_{ \rho }}  \\ 
        & \norm{\vtheta}_2 = 1,  \ \  
    \vtheta \perp \vmu,
    \end{array}
\end{equation}
where
\begin{equation*}
    \kappa(\rho, \vtheta, \beta_0) = 
    \min\limits_{i \in [n]} s_i \left( \rho \norm{\vmu}_2 + y_i\beta_0 + \rho y_i g_i + \sqrt{1 - \rho^2} y_i \< \zz_i, \vtheta \> \right).
\end{equation*}
Recall that $(\hat\vbeta$, $\hat\beta_0)$ is the max-margin solution to \cref{eq:SVM}, and the maximum margin is given by
\begin{equation}\label{eq:max-margin}
    \hat\kappa = \kappa(\hat\vbeta, \hat\beta_0) = \min_{i \in [n]} \kappa_i(\hat\vbeta, \hat\beta_0).
\end{equation}
Similarly, we can also reparametrize $\hat\vbeta$ as in \cref{eq:def-rho-theta}:
\begin{equation}\label{eq:def-rho-theta_hat}
    \hat\rho := \left\< \hat\vbeta, \frac{\vmu}{\norm{\vmu}_2} \right\>,
    \qquad
    \hat\vtheta := 
    \begin{cases} 
        \, \dfrac{\bP_{\vmu}^\perp \hat\vbeta}{\|\bP_{\vmu}^\perp \hat\vbeta\|_2}
        = \dfrac{\bP_{\vmu}^\perp \hat\vbeta}{\sqrt{1 - \hat\rho^2}} , & \ \text{if} \ |\hat\rho| < 1, \\
        \, \vmu_\perp ,         & \ \text{if} \ |\hat\rho| = 1.
    \end{cases}
\end{equation}
Then, $(\hat\rho, \hat\vtheta, \hat\beta_0)$ is the optimal solution to \cref{eq:SVM-rho_theta}\footnote{
    According to \cref{eq:def-rho-theta_hat} and \cref{prop:SVM_tau_relation}, for linearly separable data, $(\hat\rho, \hat\beta_0)$ is the unique solution to \cref{eq:SVM-rho_theta}. If $|\hat\rho| < 1$, then $\hat\vtheta$ is also the unique solution to \cref{eq:SVM-rho_theta}. Otherwise, if $\hat\rho = \pm 1$, then $\sqrt{1 - \hat\rho^2} y_i \< \zz_i, \vtheta \> \equiv 0$ and thus any feasible $\vtheta$ could solve \cref{eq:SVM-rho_theta}.
}. Combining \cref{eq:margin_reparam} and \eqref{eq:max-margin}, the maximum margin can be rewritten as
\begin{equation}\label{eq:max-margin1}
    \hat\kappa = \kappa(\hat\rho, \hat\vtheta, \hat\beta_0) = \min_{i \in [n]} \kappa_i(\hat\rho, \hat\vtheta, \hat\beta_0),
\end{equation}
which is also the optimal objective value of \cref{eq:SVM-rho_theta}. Finally, we define a few quantities:
\begin{gather*}
    \bar g_{+} :=  \frac{1}{n_+}\sum_{i \in \mathcal{I}_+} g_i, 
    \qquad
    \bar g_{-} :=  \frac{1}{n_-}\sum_{i \in \mathcal{I}_-} g_i, 
    \qquad
    \wt g := \frac{\bar g_{+} - \bar g_{-}}{2},
    \\
    \bar \zz_{+} := \frac{1}{n_+}\sum_{i \in \mathcal{I}_+} \zz_i,     
    \qquad
    \bar \zz_{-} := \frac{1}{n_-}\sum_{i \in \mathcal{I}_-} \zz_i,
    \qquad
    \wt\zz  := \frac{\bar\zz_{+} - \bar\zz_{-}}{2}.
\end{gather*}
The proof structure of \cref{thm:main_high-imbal} is as follows:
\begin{enumerate}
    \item In \cref{subsec:highimb_upper}, we provide a (stochastic) tight upper bound for the maximum margin $\hat\kappa$, and a constructed solution $(\wt\rho, \wt\vtheta, \wt\beta_0)$ which approximates $(\hat\rho, \hat\vtheta, \hat\beta_0)$ well.
    \item In \cref{subsec:highimb_asymp}, we derive the asymptotic orders of $(\hat\rho, \hat\vtheta, \hat\beta_0)$ by using $(\wt\rho, \wt\vtheta, \wt\beta_0)$.
    \item In \cref{subsec:highimb_err}, we use these asymptotics to analyze test errors and conclude \cref{thm:main_high-imbal}.
\end{enumerate}




\subsection{A tight upper bound on maximum margin: Proof of \cref{lem:upper_bound}}
\label{subsec:highimb_upper}

The following Lemma provides a data-dependent upper bound on the margin $\kappa(\vbeta, \beta_0)$ which holds for all linear classifiers with $\norm{\vbeta}_2 = 1$. The bound is tight in sense that it can be (almost) achieved by a constructed solution. Therefore, such tightness ensures the optimal margin $\hat\kappa$ should have the same asymptotics given by its upper bound, which also deduces the data is linearly separable with probability tending to one (as $d \to \infty$). 
Notably, \cref{prop:SVM_tau_relation} implies that $\tau$ has no effect on $\hat\vbeta$, and $\hat\kappa \propto (1 + \tau)^{-1}$ in a fixed dataset. Hence, $\tau$ simply scales the magnitude of $\hat\kappa$, and it suffices to consider $\tau = 1$ in the following lemma.
\begin{lem} \label{lem:upper_bound}
    Fix $\tau = 1$. Denote
\begin{equation} \label{eq:kappa_upper}
    \bar\kappa := \sqrt{ ( \norm{\vmu}_2 + \wt g )^2 + \| \bP_{\vmu}^\perp \wt\zz \|_2^2 }.
\end{equation}
    \begin{enumerate}[label=(\alph*)]
        \item 
        \label{lem:upper_bound_a}
        (Upper bound) $\kappa(\rho, \vtheta, \beta_0) \le \bar\kappa$, for any $\rho \in [-1, 1]$, $\vtheta \in \S^{d-1}$, $\vtheta \perp \vmu$, $\beta_0 \in \R$. Moreover,
        \begin{equation*}
            \bar\kappa = \bigl(1 + o_\P(1)\bigr) \sqrt{  d^{b} +  \frac{1}{4} d^{a-c} },
        \end{equation*}
        as $d \to \infty$.
        \item
        \label{lem:upper_bound_b}
        (Tightness) $\kappa(\wt\rho, \wt\vtheta, \wt\beta_0) \ge \bar\kappa - \wt O_{\P}(1)$, where
        \begin{equation} \label{eq:param_star}
            \begin{gathered}
                \wt\rho := \dfrac{ \norm{\vmu}_2 + \wt g }{\sqrt{ ( \norm{\vmu}_2 + \wt g )^2 + \| \bP_{\vmu}^\perp \wt\zz \|_2^2 } },
                \qquad
                \wt\vtheta := \frac{\bP_{\vmu}^\perp \wt\zz}{\| \bP_{\vmu}^\perp \wt\zz \|_2},
                \\
                \wt\beta_0 := - \wt\rho \cdot \frac{\bar g_+ + \bar g_-}{2} - \sqrt{1 - \wt\rho^2} \cdot \left\< \frac{\bar\zz_{+} + \bar\zz_{-}}{2}, \wt\vtheta \right\>
            \end{gathered}
        \end{equation}
        is a feasible solution to \cref{eq:SVM-rho_theta}.
        \item
        \label{lem:upper_bound_c}
        (Asymptotics of $\hat\kappa$) As a consequence, the data is linearly separable with high probability, and the maximum margin satisfies
        $\bar\kappa - \wt O_{\P}(1) \le \hat\kappa \le \bar\kappa$.
    \end{enumerate}
\end{lem}
\begin{proof}
    \textbf{\ref{lem:upper_bound_a}:}
    % It suffices to consider $\norm{\vbeta}_2 = 1$ (otherwise, in the linearly separable case, any classifier with $\norm{\vbeta}_2 < 1$ must satisfy $\kappa < \hat\kappa$; or, in the non-separable case, we always have $\kappa \le \hat\kappa = 0$). 
    We reparametrize $\kappa(\vbeta, \beta_0) = \kappa(\rho, \vtheta, \beta_0)$ by using \cref{eq:def-rho-theta} and \eqref{eq:logits}.
    Then, the upper bound is established by calculating the \emph{average logit margin} for each class. Let
    \begin{equation}
            \label{eq:kappa_pm}
            \begin{aligned}
                \bar\kappa_{+}(\rho, \vtheta, \beta_0) & := \frac{1}{n_+}\sum_{i \in \mathcal{I}_+} \kappa_i(\rho, \vtheta, \beta_0)  =
               \rho \norm{\vmu}_2 + \beta_0 + \rho \bar g_{+} + \sqrt{1 - \rho^2} \< \bar\zz_{+}, \vtheta \>,
                \\
                \bar\kappa_{-}(\rho, \vtheta, \beta_0) & := \frac{1}{n_-}\sum_{i \in \mathcal{I}_-} \kappa_i(\rho, \vtheta, \beta_0)  =
                \rho \norm{\vmu}_2 - \beta_0 - \rho \bar g_{-} - \sqrt{1 - \rho^2} \< \bar\zz_{-}, \vtheta \>.
            \end{aligned}
    \end{equation}
    Clearly, $\kappa(\rho, \vtheta, \beta_0) \le \bar\kappa_+(\rho, \vtheta, \beta_0)$ and $\kappa(\rho, \vtheta, \beta_0) \le \bar\kappa_-(\rho, \vtheta, \beta_0)$. By averaging these two bounds,
    \begin{align}
        \kappa(\rho, \vtheta, \beta_0) & \le \frac{\bar\kappa_{+}(\rho, \vtheta, \beta_0) + \bar\kappa_{-}(\rho, \vtheta, \beta_0)}{2} \nonumber \\
        & = 
        \rho \norm{\vmu}_2 + \rho \cdot \frac{\bar g_+ - \bar g_-}{2} + \sqrt{1 - \rho^2} \cdot \left\< \frac{\bar\zz_{+} - \bar\zz_{-}}{2}, \vtheta \right\>
        \nonumber \\
        & = \rho \left( \norm{\vmu}_2 + \wt g \right) + \sqrt{1 - \rho^2} \left< \wt\zz, \vtheta \right\> 
        \nonumber \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\le}  \rho \left( \norm{\vmu}_2 +  \wt g \right) + \sqrt{1 - \rho^2}  \| \bP_{\vmu}^\perp \wt\zz \|_2 
        \label{eq:F_AB} \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{\le} \sqrt{ ( \norm{\vmu}_2 + \wt g )^2 + \| \bP_{\vmu}^\perp \wt\zz \|_2^2 }
        = \bar\kappa,
        \nonumber
\end{align}
which leads to $\bar\kappa$ defined in \cref{eq:kappa_upper}. Here, (i) is based on the fact that
\begin{equation}\label{eq:theta_min}
    \argmax_{ \vtheta \in \R^d :  \substack{ \| \vtheta \|_2 = 1 \\ \langle \vmu, \vtheta  \rangle = 0 } } \ \langle \wt\zz, \vtheta \rangle = \frac{\bP_{\vmu}^\perp \wt\zz}{\| \bP_{\vmu}^\perp \wt\zz \|_2},
    \qquad
    \max_{ \vtheta \in \R^d :  \substack{ \| \vtheta \|_2 = 1 \\ \langle \vmu, \vtheta  \rangle = 0 } }   \langle \wt\zz, \vtheta \rangle = \| \bP_{\vmu}^\perp \wt\zz \|_2,
\end{equation}
and recall that the optimal $\vtheta$ equals $\wt\vtheta$ defined in \cref{eq:param_star}. Moreover, (ii) is a consequence of Cauchy--Schwarz inequality ($A \in \R$, $B > 0$)
\begin{equation} \label{eq:F_AB_optim}
    \begin{aligned}
        \max_{\rho \in [-1, 1]} \left\{  \rho A + \sqrt{1 - \rho^2} B  \right\}
    & = \max_{\rho \in [-1, 1]} \left\< \begin{pmatrix}
        \rho \\ \sqrt{1-\rho^2}
    \end{pmatrix}, 
    \begin{pmatrix}
        A \\ B
    \end{pmatrix} \right\> = \sqrt{A^2 + B^2}, \\
    \argmax_{\rho \in [-1, 1]} \left\{  \rho A + \sqrt{1 - \rho^2} B \right\}
    & = \frac{A}{\sqrt{A^2 + B^2}},
    \end{aligned}
\end{equation}
and also note that the optimal $\rho$ in (ii) equals $\wt\rho$ defined in \cref{eq:param_star}.

~\\
\noindent
To study the asymptotics of $\bar\kappa$, recall that $\pi = n_+/n = o(1)$, $n_- = n - n_+ = n(1 - o(1))$. Then
\begin{equation*}
    \frac{1}{n_+} + \frac{1}{n_-} = \frac{1}{\pi n } + \frac{1}{n(1 - o(1))} = \frac{1}{\pi n}\bigl(1 + o(1)\bigr).
\end{equation*}
Denote
\begin{equation}\label{eq:alpha_d}
    \alpha_d := \frac12 \sqrt{ \frac{1}{n_+} + \frac{1}{n_-} }
    = \frac{1}{2\sqrt{\pi n}}\bigl(1 + o(1)\bigr).
\end{equation}
\cref{lem:subG}\ref{lem:subG-b} implies $\wt\zz/\alpha_d \sim \subGind(\bzero, \bI_d; K)$. Then according to \cref{lem:subG_concentrate}\ref{lem:subG-Hanson-Wright-II},
\begin{equation*}
    \P\biggl( \biggl| \frac{\| \bP_{\vmu}^\perp \wt\zz \|_2}{ \alpha_d \| \bP_{\vmu}^\perp \|_\mathrm{F}} - 1 \biggr| > t \biggr)
    \le 2 \exp\biggl( -\frac{ct^2}{K^4}\frac{\| \bP_{\vmu}^\perp \|_\mathrm{F}^2}{\| \bP_{\vmu}^\perp \|_{\mathrm{op}}^2} \biggr)
    = 2 \exp\biggl( -\frac{ct^2(d-1)}{K^4} \biggr) ,
\end{equation*}
where $\| \bP_{\vmu}^\perp \|_\mathrm{F} = \sqrt{d - 1}$, $\| \bP_{\vmu}^\perp \|_{\mathrm{op}} = 1$, and $c$ is an absolute constant. Therefore,
\begin{equation} \label{eq:P_wtz}
    \| \bP_{\vmu}^\perp \wt\zz \|_2 = \alpha_d \| \bP_{\vmu}^\perp \|_\mathrm{F} \bigl(1 + o_\P(1)\bigr)
    = \frac{1}{2\sqrt{\pi n}}\big(1 + o(1) \big) \cdot \sqrt{d - 1}\bigl(1 + o_\P(1)\bigr) 
    = \frac12 \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big).
\end{equation}
In addition, by \cref{lem:subG_concentrate}\ref{lem:subG-Hoeffding},
\begin{equation*}
    % \|\wt\zz\|^2_2 = \alpha_d^2 d \bigl(1 + o_\P(1)\bigr) = \frac{d}{4\pi n} \bigl(1 + o_\P(1)\bigr),
    % \qquad
    \wt g = O_{\P}(\alpha_d) = O_\P\biggl( \frac{1}{\sqrt{\pi n}} \biggr).
\end{equation*}
Recall that $a - c - 1 < 0$. Finally, we have
\begin{align}
        \bar\kappa 
    & = \sqrt{ ( \norm{\vmu}_2 + \wt g )^2 + \| \bP_{\vmu}^\perp \wt\zz \|_2^2 } \nonumber \\
    & = \sqrt{ \left( \norm{\vmu}_2 + O_\P(1/\sqrt{\pi n}) \right)^2 + \frac{d}{4\pi n}\bigl(1 + o_\P(1)\bigr) }
    \nonumber \\
    & =  \sqrt{ \left( d^{b/2} + O_\P\bigl(d^{(a-c-1)/2}\bigr) \right)^2 + \frac{1}{4} d^{a - c} \bigl(1 + o_\P(1)\bigr) }
    \nonumber \\
    & = \sqrt{ d^{b} +  \frac{1}{4} d^{a-c} } \bigl(1 + o_\P(1)\bigr).
    \label{eq:margin_upper}
\end{align}
This concludes the proof of part \ref{lem:upper_bound_a}.

\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{lem:upper_bound_b}:}
Next we show that the upper bound $\bar\kappa$ is nearly attainable, by a constructed solution $(\wt\rho, \wt\vtheta, \wt\beta_0)$ defined in \cref{eq:param_star}. Clearly, $(\wt\rho, \wt\vtheta, \wt\beta_0)$ satisfies the constraints in \cref{eq:SVM-rho_theta}. This candidate solution is motivated by the optimal $(\rho, \vtheta)$ that makes (i) and (ii) equal in \cref{eq:F_AB}, i.e.,
\begin{equation*}
    \bar\kappa = \wt\rho \left(\norm{\vmu}_2 +  \wt g \right) + \sqrt{1 - \wt\rho^2} \< \wt\zz, \wt\vtheta \>, 
\end{equation*}
and $\beta_0$ that balances the magnitude of average logit margins from the two classes, i.e., we choose $\beta_0$ such that $\bar\kappa_+ = \bar\kappa_-$ in \cref{eq:kappa_pm}. Substituting $(\wt\rho, \wt\vtheta, \wt\beta_0)$ back into \cref{eq:logits}, we obtain
\begin{equation*}
        \begin{aligned}
        \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0) & = \wt\rho \norm{\vmu}_2 + y_i\wt\beta_0 + \wt\rho y_i g_i + \sqrt{1 - \wt\rho^2} y_i \< \zz_i, \wt\vtheta \> \\
        & = 
        \wt\rho \left( \norm{\vmu}_2 + y_i g_i - y_i\frac{\bar g_{+} + \bar g_{-}}{2} \right) + \sqrt{1 - \wt\rho^2} \left\< y_i \zz_i - y_i \frac{\bar\zz_{+} + \bar\zz_{-}}{2}, \wt\vtheta \right\>.
    \end{aligned}
\end{equation*}
Therefore, the difference between each logit margin and the upper bound can be expressed as
\begin{align}
        \bar\kappa - \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0)
        & = \wt\rho \left( \wt g + y_i\frac{\bar g_{+} + \bar g_{-}}{2} - y_i g_i \right) + \sqrt{1 - \wt\rho^2} \left\< \wt\zz + y_i \frac{\bar\zz_{+} + \bar\zz_{-}}{2} - y_i \zz_i , \wt\vtheta \right\>  \nonumber  \\
        & = 
        \begin{cases} 
            \,  \wt\rho (\bar g_+ - g_i) +  \sqrt{1 - \wt\rho^2} \< \bar\zz_+ - \zz_i, \wt\vtheta \> , & \ \text{if} \ y_i = +1, \\
            \,  \wt\rho (g_i - \bar g_-) +  \sqrt{1 - \wt\rho^2} \< \zz_i - \bar\zz_-, \wt\vtheta \> , & \ \text{if} \ y_i = -1, \end{cases}        \label{eq:logit_diff}
\end{align}
where the leading terms $\rho\norm{\vmu}_2$, $\< \bar\zz_-, \wt\vtheta\>$ (for $i = +1$), $\< \bar\zz_+, \wt\vtheta\>$ (for $i = -1$) are all cancelled out. 
Our goal is to bound the maximum difference over all data points. Note that
\begin{equation}\label{eq:max_diff}
    \begin{aligned}
        & \max_{i \in [n]} \abs{\bar\kappa - \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0)}
        = 
        \max_{i \in \mathcal{I}_+} \abs{\bar\kappa - \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0)}
         \vee
        \max_{i \in \mathcal{I}_-} \abs{\bar\kappa - \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0)}  \\
        \le {} & 
        \max_{i \in \mathcal{I}_+} \left\{ \bigl|g_i - \bar g_+\bigr| + \bigl|\< \zz_i - \bar\zz_+, \wt\vtheta \>\bigr| \right\}
        \vee
        \max_{i \in \mathcal{I}_-} \left\{ \bigl|g_i - \bar g_-\bigr| + \bigl|\< \zz_i - \bar\zz_-, \wt\vtheta \>\bigr| 
        \right\} 
        \\
        \le {} & \left\{ 
            \max_{i \in \mathcal{I}_+} \bigl|g_i - \bar g_+\bigr|
            + \max_{i \in \mathcal{I}_+} \bigl|\< \zz_i - \bar\zz_+, \wt\vtheta \>\bigr|
        \right\}
        \vee
        \left\{ 
            \max_{i \in \mathcal{I}_-} \bigl|g_i - \bar g_-\bigr|
            + \max_{i \in \mathcal{I}_-} \bigl|\< \zz_i - \bar\zz_-, \wt\vtheta \>\bigr|
        \right\}.
    \end{aligned}
\end{equation}
For the first term involving $g_i$'s, recall that $\max_{i \in [n]}\norm{g_i}_{\psi_2} \lesssim K$. Therefore, as per \cref{lem:subG}\ref{lem:subG-c} and \cref{lem:subG_concentrate}\ref{lem:subG-Hoeffding}, $g_i, \bar g_\pm$ are sub-gaussian, and
\begin{equation}\label{eq:max_g}
    \begin{aligned}
        \max_{i \in \mathcal{I}_+} \bigl|g_i - \bar g_+\bigr|
        & \le \max_{i \in \mathcal{I}_+} \abs{g_i} + \abs{\bar g_+} 
        = O_\P(\sqrt{\log n_+}) + O_\P\biggl( \frac{1}{\sqrt{n_+}} \biggr)
        = O_\P(\sqrt{\log d}), \\
        \max_{i \in \mathcal{I}_-} \bigl|g_i - \bar g_-\bigr|
        & \le \max_{i \in \mathcal{I}_-} \abs{g_i} + \abs{\bar g_-} 
        = O_\P(\sqrt{\log n_-}) + O_\P\biggl( \frac{1}{\sqrt{n_-}} \biggr)
        = O_\P(\sqrt{\log d}).
    \end{aligned}
\end{equation}
For the second term involving $\zz_i$'s, note that
\begin{equation}\label{eq:maxmax_z}
    \begin{aligned}
        \max_{i \in \mathcal{I}_+} \bigl| \< \zz_i - \bar\zz_+, \wt\vtheta \> \bigr|
        & \le \max_{i \in \mathcal{I}_+} \frac{1}{n_+} \sum_{j \in \mathcal{I}_+} \bigl| \< \zz_i - \zz_j, \wt\vtheta \> \bigr|
        \le \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \max_{i,j \in \mathcal{I}_+} \bigl| \< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>  \bigr|,  \\
        \max_{i \in \mathcal{I}_-} \bigl| \< \zz_i - \bar\zz_-, \wt\vtheta \> \bigr|
        & \le \max_{i \in \mathcal{I}_-} \frac{1}{n_-} \sum_{j \in \mathcal{I}_-} \bigl| \< \zz_i - \zz_j, \wt\vtheta \> \bigr|
        \le \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \max_{i, j \in \mathcal{I}_-} \bigl| \< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>  \bigr|.
    \end{aligned}
\end{equation}
So it remains to bound $\< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>$ uniformly. We decompose it as
\begin{equation*}
    \< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>
    = 
    \< \zz_i - \zz_j, \wt\zz \> - \left\< \zz_i - \zz_j, \frac{\vmu}{\norm{\vmu}_2} \right\>
            \left\< \wt\zz, \frac{\vmu}{\norm{\vmu}_2} \right\>
    := I -  I\!I.
\end{equation*}
We will show that both $I$ and $I\!I$ are sub-exponential. To bound $\norm{I}_{\psi_1}$ via \cref{lem:subExp}\ref{lem:subExp-b}, we claim the inner product
\begin{equation*}
    I = \< \zz_i - \zz_j, \wt\zz \> = \sum_{k=1}^d (\zz_i - \zz_j)_k (\wt\zz)_k
\end{equation*}
is the sum of $d$ mean-zero random variables, i.e., $\E[(\zz_i - \zz_j)_k (\wt\zz)_k] = 0$, $\forall\, k \in [d]$, where we write $(\ba)_k$ as the $k$-th entry of vector $\ba$. To see this, we decompose $\wt\zz$ into terms that are independent or dependent of $(\zz_i, \zz_j)$.
\begin{itemize}
    \item If $y_i = y_j = +1$ and $i \not= j$, then
    \begin{align*}
        \E[ (\zz_i - \zz_j) \odot \wt\zz ]
        & =  \E\biggl[(\zz_i - \zz_j) \odot 
        \biggl( \underbrace{  \frac1{2 n_+} \!\!\! \sum_{k: \substack{k \not= i, j \\ y_k = +1}} \zz_k + \frac{\bar\zz_-}2  }_{ =: \wt\zz_{-ij}^+ }
        \biggr)
        \biggr] 
        + \frac{1}{2 n_+} \E[ (\zz_i - \zz_j) \odot (\zz_i + \zz_j) ] \\
        & = \E[\zz_i - \zz_j] \odot \E[ \wt\zz_{-ij}^+ ]
        + \frac{1}{2 n_+} \left( \E[\zz_i \odot \zz_i] - \E[\zz_j \odot \zz_j] \right) 
        \qquad (\wt\zz_{-ij}^+  \indep  \zz_i, \zz_j) \\
        & = \bzero \odot \bzero + \frac{1}{2 n_+} ( \bone - \bone ) = \bzero.
    \end{align*}
    \item If $y_i = y_j = -1$ and $i \not= j$, similarly
    \begin{align*}
        \E[ (\zz_i - \zz_j) \odot \wt\zz ]
        & =  \E\biggl[(\zz_i - \zz_j) \odot 
        \biggl( \underbrace{  \frac1{2 n_-} \!\!\! \sum_{k: \substack{k \not= i, j \\ y_k = -1}} \zz_k + \frac{\bar\zz_+}2  }_{ =: \wt\zz_{-ij}^- }
        \biggr)
        \biggr] 
        + \frac{1}{2 n_-} \E[ (\zz_i - \zz_j) \odot (\zz_i + \zz_j) ] \\
        & = \E[\zz_i - \zz_j] \odot \E[ \wt\zz_{-ij}^- ]
        + \frac{1}{2 n_-} \left( \E[\zz_i \odot \zz_i] - \E[\zz_j \odot \zz_j] \right) 
        \qquad (\wt\zz_{-ij}^-  \indep  \zz_i, \zz_j) 
        \\
        & = \bzero.
    \end{align*}
\end{itemize}
Therefore, when $d$ is large enough, we have
\begin{align*}
    \norm{I}_{\psi_1} & = \norm{\< \zz_i - \zz_j, \wt\zz \>}_{\psi_1}
    = \norm{ \sum_{k=1}^d (\zz_i - \zz_j)_k (\wt\zz)_k }_{\psi_1} \\
    & \overset{\mathmakebox[0pt][c]{\smash{\text{(i)}}}}{\lesssim} \sqrt{d} \max_{1 \le k \le d} \norm{ (\zz_i - \zz_j)_k (\wt\zz)_k }_{\psi_1} \\
    & \overset{\mathmakebox[0pt][c]{\smash{\text{(ii)}}}}{\le} \sqrt{d} \max_{1 \le k \le d} \norm{ (\zz_i - \zz_j)_k }_{\psi_2}
    \max_{1 \le k \le d} \norm{ (\wt\zz)_k }_{\psi_2} \\
    & \overset{\mathmakebox[0pt][c]{\smash{\text{(iii)}}}}{\lesssim} \sqrt{d} K \cdot \alpha_{d} K 
    \lesssim \sqrt{\frac{d}{\pi n}} K^2,
\end{align*}
where (i) results from coordinate independence and \cref{lem:subExp}\ref{lem:subExp-b}, (ii) is from \cref{lem:subExp}\ref{lem:subExp-d}, and (iii) is based on $\wt\zz/\alpha_{d} , (\zz_i - \zz_j)/\sqrt{2} \sim \subGind(\bzero, \bI_d; K)$. For the term $I\!I$, we have
\begin{align*}
    \norm{I\!I}_{\psi_2} & = \norm{\left\< \zz_i - \zz_j, \frac{\vmu}{\norm{\vmu}_2} \right\>
    \left\< \wt\zz, \frac{\vmu}{\norm{\vmu}_2} \right\>}_{\psi_1} \\
    & \overset{\mathmakebox[0pt][c]{\smash{\text{(i)}}}}{\le} \norm{\left\< \zz_i - \zz_j, \frac{\vmu}{\norm{\vmu}_2} \right\>}_{\psi_2}
    \norm{\left\< \wt\zz, \frac{\vmu}{\norm{\vmu}_2} \right\>}_{\psi_2} \\
    & \overset{\mathmakebox[0pt][c]{\smash{\text{(ii)}}}}{\lesssim}  \max_{1 \le k \le d} \norm{ (\zz_i - \zz_j)_k }_{\psi_2}
    \max_{1 \le k \le d} \norm{ (\wt\zz)_k }_{\psi_2} \\
    & \lesssim K \cdot \alpha_{d} K 
    \lesssim \frac{1}{\sqrt{\pi n}} K^2,
\end{align*}
where (i) is from \cref{lem:subExp}\ref{lem:subExp-d}, and (ii) is from \cref{lem:subG}\ref{lem:subG-b}. Hence,
\begin{equation*}
    \norm{\frac{\< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>}{\sqrt{d/\pi n}}}_{\psi_1}
    \le \sqrt{\frac{\pi n}{d}} \bigl( \norm{I}_{\psi_1} + \norm{I\!I}_{\psi_1} \bigr)
    \lesssim K^2.
\end{equation*}
Substituting this back into \cref{eq:maxmax_z}, referring to \cref{eq:P_wtz} and \cref{lem:subExp}\ref{lem:subExp-c}, we obtain
\begin{align}
    \!\!\! \max_{i \in \mathcal{I}_+} \bigl| \< \zz_i - \bar\zz_+, \wt\vtheta \> \bigr|
        & \le \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \max_{\substack{i \in \mathcal{I}_+ \\ j \in \mathcal{I}_+}} \bigl| \< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>  \bigr|
    = \bigl(1 + o_\P(1)\bigr) \max_{\substack{i \in \mathcal{I}_+ \\ j \in \mathcal{I}_+}} \abs{\frac{\< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>}{\sqrt{d/\pi n}}}
    \notag \\
    & =  O_{\P}(\log n_+^2) = O_{\P}(\log d), \notag \\
    \!\!\! \max_{i \in \mathcal{I}_-} \bigl| \< \zz_i - \bar\zz_-, \wt\vtheta \> \bigr|
        & \le \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \max_{\substack{i \in \mathcal{I}_- \\ j \in \mathcal{I}_-}} \bigl| \< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>  \bigr|
    = \bigl(1 + o_\P(1)\bigr) \max_{\substack{i \in \mathcal{I}_- \\ j \in \mathcal{I}_-}} \abs{\frac{\< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>}{\sqrt{d/\pi n}}} \notag \\
    & =  O_{\P}(\log n_-^2) = O_{\P}(\log d).
    \label{eq:max_z}
\end{align}
Finally, incorporating \cref{eq:max_g} and \cref{eq:max_z} into \cref{eq:max_diff}, we have
\begin{align*}
        & \max_{i \in [n]} \abs{\bar\kappa - \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0)}
        \\
        \le {} & \left\{ 
            \max_{i \in \mathcal{I}_+} \bigl|g_i - \bar g_+\bigr|
            + \max_{i \in \mathcal{I}_+} \bigl|\< \zz_i - \bar\zz_+, \wt\vtheta \>\bigr|
        \right\}
        \vee
        \left\{ 
            \max_{i \in \mathcal{I}_-} \bigl|g_i - \bar g_-\bigr|
            + \max_{i \in \mathcal{I}_-} \bigl|\< \zz_i - \bar\zz_-, \wt\vtheta \>\bigr|
        \right\} \\
        \le {} & \left\{ O_\P(\sqrt{\log d}) + O_\P(\log d) \right\} \vee \left\{ O_\P(\sqrt{\log d}) + O_\P(\log d) \right\} 
        = O_\P(\log d).
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Old ver.: split \wt\zz into indep parts (maybe useful in non-indep coordinates) %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{itemize}
%     \item If $y_i = y_j = +1$ and $i \not= j$, we have the following decomposition:
%     \begin{equation*}
%         \begin{aligned}
%             & \< \zz_i - \zz_j, \bP_{\vmu}^\perp \wt\zz \>
%             = \< \zz_i - \zz_j, \wt\zz \> - \left\< \zz_i - \zz_j, \frac{\vmu}{\norm{\vmu}_2} \right\>
%             \left\< \wt\zz, \frac{\vmu}{\norm{\vmu}_2} \right\> \\
%             = {} & \biggl\< \zz_i - \zz_j, 
%             \underbrace{  \frac1{2 n_+} \!\!\! \sum_{k: \substack{k \not= i, j \\ y_k = +1}} \zz_k + \frac{\bar\zz_-}2  }_{ =: \wt\zz_{-ij}^+ }
%             \biggr\>
%             + \frac{1}{2 n_+} \< \zz_i - \zz_j, \zz_i + \zz_j \>
%             - \left\< \zz_i - \zz_j, \frac{\vmu}{\norm{\vmu}_2} \right\>
%             \left\< \wt\zz, \frac{\vmu}{\norm{\vmu}_2} \right\> \\
%             := {} & I +  I\!I - I\!I\!I,
%         \end{aligned}
%     \end{equation*}
%     where $\wt\zz$ is decomposed into terms independent ($I$) and dependent ($I\!I$) of $(\zz_i, \zz_j)$. That is, 
%     \begin{equation}\label{eq:zij_p}
%         \wt\zz_{-ij}^+  \indep  (\zz_i, \zz_j),
%         \quad
%         \alpha_{d,-ij}^+ := \frac12 \sqrt{\frac{n_+ - 2}{n_+^2} + \frac{1}{n_-}},
%         \quad
%         \frac{\wt\zz_{-ij}^+}{\alpha_{d,-ij}^+} , \frac{\zz_j - \zz_i}{\sqrt{2}}  \sim \subGind(\bzero, \bI_d; K).
%     \end{equation}
%     For the first term with $d$ large enough, we have
%     \begin{align*}
%             \norm{I}_{\psi_1} & = \norm{\< \zz_i - \zz_j, \wt\zz_{-ij}^+ \>}_{\psi_1}
%             = \norm{ \sum_{k=1}^d (\zz_i - \zz_j)_k (\wt\zz_{-ij}^+)_k }_{\psi_1} \\
%             & \overset{\mathmakebox[0pt][c]{\smash{\text{(i)}}}}{\lesssim} \sqrt{d} \max_{1 \le k \le d} \norm{ (\zz_i - \zz_j)_k (\wt\zz_{-ij}^+)_k }_{\psi_1} \\
%             & \overset{\mathmakebox[0pt][c]{\smash{\text{(ii)}}}}{\le} \sqrt{d} \max_{1 \le k \le d} \norm{ (\zz_i - \zz_j)_k }_{\psi_2}
%             \max_{1 \le k \le d} \norm{ (\wt\zz_{-ij}^+)_k }_{\psi_2} \\
%             & \overset{\mathmakebox[0pt][c]{\smash{\text{(iii)}}}}{\lesssim} \sqrt{d} \alpha_{d,-ij}^+ K^2 
%             \lesssim \sqrt{\frac{d}{\pi n}} K^2,
%     \end{align*}
%     where (i) is a consequence of \cref{eq:zij_p}, {\color{blue}coordinate independence}, and \cref{lem:subExp}\ref{lem:subExp-b}, (ii) is from \cref{lem:subExp}\ref{lem:subExp-d}, and (iii) is from \cref{eq:zij_p}. 
%     For the second term, we have
%     \begin{align*}
%         \norm{I\!I}_{\psi_1} & = \frac{1}{2 n_+} \norm{\< \zz_i - \zz_j, \zz_i + \zz_j \>}_{\psi_1}
%         \le \frac{1}{2 n_+}  \norm{ \norm{\zz_i}_2^2 - \norm{\zz_j}_2^2 }_{\psi_1} 
%     \end{align*}
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Old ver. with mistakes: w/o considering uniform convergence %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We claim the terms involving $g_i$'s are negligible.
% By \cref{lem:subG_concentrate}\ref{lem:subG-Hoeffding},
% \begin{equation*}
%     \bar g_+ = O_\P\biggl( \frac{1}{\sqrt{n_+}} \biggr) = O_\P\biggl( \frac{1}{\sqrt{\pi n}} \biggr) = o_{\P}(1),
%     \qquad
%     \bar g_- = O_\P\biggl( \frac{1}{\sqrt{n_-}} \biggr) = O_\P\biggl( \frac{1}{\sqrt{n}} \biggr) = o_{\P}(1).
% \end{equation*}
% Thus, the first term in \cref{eq:logit_diff} $\abs{\bar g_\pm - g_i} = O_\P(1)$ is bounded for both cases. 
% ~\\
% \noindent
% Now consider the second term \update{involving $\zz_i$'s}. For each $i$ such that $y_i = +1$,
% \begin{align*}
%         & \< \bar\zz_+ - \zz_i, \wt\vtheta \> \\
%         = {} & \frac{1}{n_+} \sum_{j \in \mathcal{I}_+} \< \zz_j - \zz_i, \wt\vtheta \> \\
%         = {} & \frac{1}{n_+} \sum_{j: \substack{j \not= i \\ y_j = +1}} \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \< \zz_j - \zz_i, \bP_{\vmu}^\perp \wt\zz \> \\
%         = {} & \frac{1}{n_+} \sum_{j: \substack{j \not= i \\ y_j = +1}} \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \Biggl\{ \biggl\< \zz_j - \zz_i, \bP_{\vmu}^\perp  
%         \biggl( \,
%             \underbrace{  \frac1{2 n_+} \sum_{k: \substack{k \not= i, j \\ y_k = +1}} \zz_k + \frac{\bar\zz_-}2  }_{ =: \wt\zz_{-ij}^+ }
%         \, \biggr) \biggr\> + \frac{1}{2 n_+}\< \zz_j - \zz_i,  \bP_{\vmu}^\perp (\zz_j + \zz_i)  \>  \Biggr\}.
% \end{align*}
% In the last line, $\wt\zz$ is decomposed into two terms, such that for the first term
% \begin{equation*}
%     \wt\zz_{-ij}^+  \indep  (\zz_i, \zz_j),
%     \qquad
%     \alpha_{d,-ij}^+ := \frac12 \sqrt{\frac{n_+ - 2}{n_+^2} + \frac{1}{n_-}},
%     \qquad
%     \frac{\wt\zz_{-ij}^+}{\alpha_{d,-ij}^+} , \frac{\zz_j - \zz_i}{\sqrt{2}}  \sim \subGind(\bzero, \bI_d; K).
% \end{equation*}
% According to \cref{lem:subG_concentrate}\ref{lem:subG-Bernstein},
% \begin{equation} \label{eq:ind_term+}
%     \< \zz_j - \zz_i, \bP_{\vmu}^\perp  \wt\zz_{-ij}^+ \> 
%     = 
%     \sqrt{2} \alpha_{d,-ij}^+  O_{\P}(\| \bP_{\vmu}^\perp \|_\mathrm{F})
%     = \frac{1}{\sqrt{2 \pi n}}\bigl(1 + o(1)\bigr) O_\P(\sqrt{d})
%     = O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr).
% \end{equation}
% For the other term, let $\bar\xx = \begin{pmatrix}
%     \zz_i \\ \zz_j
% \end{pmatrix}$, $\bar\bA = \begin{pmatrix}
%     -\bP_{\vmu}^\perp & \bzero \\
%     \bzero & \bP_{\vmu}^\perp
% \end{pmatrix}$, then 
% \begin{equation*}
%     \begin{aligned}
%         \< \zz_j - \zz_i,  \bP_{\vmu}^\perp (\zz_j + \zz_i)  \> 
%     & = \bar\xx^\top \bar\bA \bar\xx,  \\
%     \E[\< \zz_j - \zz_i,  \bP_{\vmu}^\perp (\zz_j + \zz_i)  \> ]
%     & = 
%     \mathrm{tr}(\bP_{\vmu}^\perp \E[(\zz_j - \zz_i)(\zz_j + \zz_i)^\top]) 
%     = 0.
%     \end{aligned}
% \end{equation*}
% Note $\| \bar\bA \|_\mathrm{F} = \sqrt{2}\| \bP_{\vmu}^\perp \|_\mathrm{F} = \sqrt{2(d-1)}$, $\| \bar\bA \|_2 = \| \bP_{\vmu}^\perp \|_2 = 1$. Applying \cref{lem:subG_concentrate}\ref{lem:subG-Hanson-Wright-I} to $\bar\xx^\top \bar\bA \bar\xx$, for any fixed $t > 0$ and $d$ large enough,
% \begin{equation*}
%     \P\biggl( \frac{ | \bar\xx^\top \bar\bA \bar\xx | }{ \| \bar\bA \|_\mathrm{F} }  > t \biggr)
%     \le 2 \exp\biggl( -c \min \biggl\{ \frac{t^2}{K^4} , \frac{ t \| \bar\bA \|_\mathrm{F}}{ K^2 \| \bar\bA \|_2 } \biggr\} \biggr) =  2 \exp(-ct^2/K^4), 
% \end{equation*}
% where $c$ is an absolute constant. Therefore,
% \begin{equation} \label{eq:nonind_term}
%     \< \zz_j - \zz_i,  \bP_{\vmu}^\perp (\zz_j + \zz_i)  \>
%     = O_\P(\| \bar\bA \|_\mathrm{F}) = O_\P(\sqrt{d}).
% \end{equation}
% Hence, combining \cref{eq:P_wtz}, \eqref{eq:ind_term+} and \eqref{eq:nonind_term},
%     \begin{align}
%         \< \bar\zz_+ - \zz_i, \wt\vtheta \> 
%         & = \frac{1}{n_+} \sum_{j: \substack{j \not= i \\ y_j = +1}} \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \left\{ 
%             \< \zz_j - \zz_i,  \bP_{\vmu}^\perp \wt\zz_{-ij}^+ \> + 
%             \frac{1}{2 n_+}\< \zz_j - \zz_i,  \bP_{\vmu}^\perp (\zz_j + \zz_i)  \>
%         \right\}  \nonumber \\
%         & = \frac{1}{n_+} \sum_{j: \substack{j \not= i \\ y_j = +1}}
%         2 \sqrt{\frac{\pi n}{d}}\big(1 + o_\P(1) \big) \left\{ O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr) + \frac{1}{2\pi n} O_\P(\sqrt{d}) \right\}  \nonumber \\
%         & = \frac{1}{n_+} \sum_{j: \substack{j \not= i \\ y_j = +1}} O_{\P}(1)
%         = \frac{n_+ - 1}{n_+} \cdot O_{\P}(1) = O_{\P}(1).  \label{eq:z_theta_diff+}
%     \end{align}
% Similarly, For each $i$ such that $y_i = -1$,
% \begin{align*}
%         & \< \bar\zz_- - \zz_i, \wt\vtheta \> \\
%         = {} & \frac{1}{n_-} \sum_{j \in \mathcal{I}_-} \< \zz_j - \zz_i, \wt\vtheta \> \\
%         = {} & \frac{1}{n_-} \sum_{j: \substack{j \not= i \\ y_j = -1}} \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \< \zz_j - \zz_i, \bP_{\vmu}^\perp \wt\zz \> \\
%         = {} & \frac{1}{n_-} \sum_{j: \substack{j \not= i \\ y_j = -1}} \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \Biggl\{ \biggl\< \zz_j - \zz_i, \bP_{\vmu}^\perp  
%         \biggl( \,
%             \underbrace{  \frac1{2 n_-} \sum_{k: \substack{k \not= i, j \\ y_k = -1}} \zz_k + \frac{\bar\zz_+}2  }_{ =: \wt\zz_{-ij}^- }
%         \, \biggr) \biggr\> + \frac{1}{2 n_-}\< \zz_j - \zz_i,  \bP_{\vmu}^\perp (\zz_j + \zz_i)  \>  \Biggr\}.
% \end{align*}
% Again, $\wt\zz$ is decomposed into two terms, such that for the first term
% \begin{equation*}
%     \wt\zz_{-ij}^-  \indep  (\zz_i, \zz_j),
%     \qquad
%     \alpha_{d,-ij}^- := \frac12 \sqrt{\frac{n_- - 2}{n_-^2} + \frac{1}{n_+}},
%     \qquad
%     \frac{\wt\zz_{-ij}^-}{\alpha_{d,-ij}^-} , \frac{\zz_j - \zz_i}{\sqrt{2}}  \sim \subGind(\bzero, \bI_d; K).
% \end{equation*}
% According to \cref{lem:subG_concentrate}\ref{lem:subG-Bernstein},
% \begin{equation} \label{eq:ind_term-}
%     \< \zz_j - \zz_i, \bP_{\vmu}^\perp  \wt\zz_{-ij}^- \> 
%     = 
%     \sqrt{2} \alpha_{d,-ij}^-  O_{\P}(\| \bP_{\vmu}^\perp \|_\mathrm{F})
%     = \frac{1}{\sqrt{2 \pi n}}\bigl(1 + o(1)\bigr) O_\P(\sqrt{d})
%     = O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr).
% \end{equation}
% Hence, combining \cref{eq:P_wtz}, \eqref{eq:ind_term-} and \eqref{eq:nonind_term},
%     \begin{align}
%         \< \bar\zz_- - \zz_i, \wt\vtheta \> 
%         & = \frac{1}{n_-} \sum_{j: \substack{j \not= i \\ y_j = -1}} \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \left\{ 
%             \< \zz_j - \zz_i,  \bP_{\vmu}^\perp \wt\zz_{-ij}^- \> + 
%             \frac{1}{2 n_-}\< \zz_j - \zz_i,  \bP_{\vmu}^\perp (\zz_j + \zz_i)  \>
%         \right\}  \nonumber  \\
%         & = \frac{1}{n_-} \sum_{j: \substack{j \not= i \\ y_j = -1}}
%         2 \sqrt{\frac{\pi n}{d}}\big(1 + o_\P(1) \big) \left\{ O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr) + \frac{1}{2 n} O_\P(\sqrt{d}) \right\}   \nonumber  \\
%         & = \frac{1}{n_-} \sum_{j: \substack{j \not= i \\ y_j = -1}} O_{\P}(1)
%         = \frac{n_- - 1}{n_-} \cdot O_{\P}(1) = O_{\P}(1).  \label{eq:z_theta_diff-}
%     \end{align}
% Finally, substitute these back to \cref{eq:logit_diff}, for each $i$,
% \begin{equation*}
%     \begin{aligned}
%         \abs{\bar\kappa - \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0)}
%         & = \abs{ \wt\rho (\bar g_\pm - g_i) +  \sqrt{1 - \wt\rho^2} \< \bar\zz_\pm - \zz_i, \wt\vtheta \> } \\
%         & \le  \abs{\bar g_\pm - g_i} + \abs{\< \bar\zz_\pm - \zz_i, \wt\vtheta \>} = O_{\P}(1).
%     \end{aligned}
% \end{equation*}
% This implies the logits for all the data points have almost the same asymptotics as the upper bound $\bar\kappa$. As the result,
Therefore, the difference between the margin of classifier characterized by $(\wt\rho, \wt\vtheta, \wt\beta_0)$ and its upper bound $\bar\kappa$ is bounded by
\begin{equation*}
    \bar\kappa - \kappa(\wt\rho, \wt\vtheta, \wt\beta_0) 
    = 
     \bar\kappa - \min_{i \in [n]} \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0) 
    = \max_{i \in [n]} \abs{\bar\kappa - \kappa_i(\wt\rho, \wt\vtheta, \wt\beta_0)} = O_\P(\log d) = \wt O_\P(1).
\end{equation*}
This concludes the proof of part \ref{lem:upper_bound_b}.


\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{lem:upper_bound_c}:}
According to max-margin optimization problem \cref{eq:SVM-rho_theta}, note that
\begin{equation*}
    \hat\kappa = 
    \max_{ \substack{ \rho \in [-1, 1],  \beta_0 \in \R
    \\
    \vtheta \in \S^{d-1}, \vtheta \perp \vmu } } \kappa(\rho, \vtheta, \beta_0)
    \ge \kappa(\wt\rho, \wt\vtheta, \wt\beta_0),
\end{equation*}
hence the asymptotics of $\hat\kappa$ is followed by (a) and (b). As $d \to \infty$, note that
\begin{equation*}
    \hat\kappa \ge \bar\kappa - \wt O_\P(1)
    = \bigl(1 + o_\P(1)\bigr) \sqrt{  d^{b} +  \frac{1}{4} d^{a-c} },
    \qquad
    \sqrt{  d^{b} +  \frac{1}{4} d^{a-c} } \ge d^{b/2} \to + \infty,
\end{equation*}
which implies $\hat\kappa$ diverges with high probability, i.e., $\lim_{d \to \infty} \P(\hat\kappa > C) = 1$, $\forall\, C \in \R$. As the result, $\P\{\text{linearly separable}\} = \P(\hat\kappa > 0) \to 1$ as $d \to \infty$, deducing $\|\hat\vbeta\|_2 = 1$ with high probability. This concludes the proof of part \ref{lem:upper_bound_c}.
\end{proof}


\subsection{Asymptotics of optimal parameters: Proofs of \cref{lem:rho_hat}, \ref{lem:theta_hat_z}, \ref{lem:beta0_asymp}}
\label{subsec:highimb_asymp}

Followed by tightness of the upper bound $\bar\kappa$, we show that the optimal parameters $\hat\rho, \hat\vtheta$ should be very ``close'' to the constructed solution $\wt\rho, \wt\vtheta$ defined in \cref{eq:param_star} in some sense. On the event that the data is linearly separable, we have showed that $\hat\vbeta$, and therefore both $\hat\rho$ and $\hat\vtheta$, do not depend on $\tau$ in \cref{prop:SVM_tau_relation}. Hence, it still suffices to consider $\tau = 1$ in our proof.


\subsubsection{Asymptotic order of $\hat\rho$: Proofs of \cref{lem:rho_hat}}

The following technical Lemma is important for deriving the asymptotics of $\hat\rho$, which introduces a function of $\rho$ used implicitly in \cref{eq:F_AB} and \eqref{eq:F_AB_optim} for optimization.

\begin{lem} \label{lem:F_AB}
    Define $F_{A, B}(\rho) = \rho A + \sqrt{1 - \rho^2} B$, $\rho \in [-1, 1]$, with $A \in \R$, $B > 0$. Then
\begin{equation*}
    F_{A, B}'(\rho) = A - \frac{\rho}{\sqrt{1 - \rho^2}} B,
    \qquad
    F_{A, B}''(\rho) = -\frac{1}{(1 - \rho^2)^{3/2}} B,
\end{equation*}
which implies $F_{A,B}$ is $B$-strongly concave, that is, for all $\rho_1, \rho_2 \in [-1, 1]$,
\begin{equation*}
    F_{A,B}(\rho_2) \le F_{A,B}(\rho_1) + F'_{A,B}(\rho_1)(\rho_2 - \rho_1) - \frac12 B (\rho_2 - \rho_1)^2.
\end{equation*} 
Moreover,
\begin{equation*}
    \argmax_{\rho \in [-1, 1]} F_{A,B}(\rho) = \frac{A}{\sqrt{A^2 + B^2}},  \qquad \max_{\rho \in [-1, 1]} F_{A,B}(\rho) = \sqrt{A^2 + B^2}.
\end{equation*}
\end{lem}
\begin{proof}
    Strongly concavity is given by direct calculation and the fact that
    \begin{equation*}
        \sup_{\rho \in [-1, 1]} F''_{A,B}(\rho) = -B.
    \end{equation*}
    The optimality condition is already derived in \cref{eq:F_AB_optim}. This concludes the proof.
\end{proof}

\noindent
In the rest of this section, the (stochastic) parameters $A, B$ are defined as
\begin{equation} \label{eq:AB_def}
    A := \norm{\vmu}_2 + \wt g
    = d^{b/2} \bigl(1 + o_{\P}(1)\bigr),
    \qquad
    B := \| \bP_{\vmu}^\perp \wt\zz \|_2
    = \frac12 d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr).
\end{equation}
Then followed by \cref{lem:F_AB}, we have $\wt\rho = \argmax_{\rho \in [-1, 1]} F_{A,B}(\rho)$ and $F'_{A,B}(\wt\rho) = 0$, where $\wt\rho$ is defined in \cref{eq:param_star}. The following Lemma describes the asymptotics of $\hat\rho$ with respect to $\wt\rho$.

\begin{lem}[Asymptotics of $\hat\rho$ and $\wt\rho$] \label{lem:rho_hat}
    Suppose that $a < c + 1$.
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:rho_hat(a)} If $a < b + c$, then $\wt\rho = 1 - o_\P(1)$, $\hat\rho = 1 - o_\P(1)$, and
        \begin{equation*}
            \sqrt{1 - \wt\rho^2} = \frac12 d^{(a-b-c)/2}\bigl( 1 + o_\P(1) \bigr).
        \end{equation*}
        Moreover, we further assume:
        \begin{itemize}
            \item[i.] If $a > \frac{b}{2} + c$, then $\sqrt{1 - \hat\rho^2} = \sqrt{1 - \wt\rho^2} \bigl(1 + o_\P(1)\bigr)$.
            \item[ii.] If $a \le \frac{b}{2} + c$, then $\sqrt{1 - \hat\rho^2} = \wt O_\P(d^{-b/4})$ and thus $\sqrt{1 - \hat\rho^2}\sqrt{d/\pi n} = \wt O_\P(1)$.
        \end{itemize}
        \item \label{lem:rho_hat(b)} If $a > b + c$, then $\wt\rho = o_\P(1)$, $\hat\rho = o_\P(1)$, and
        \begin{equation*}
            \wt\rho = 2d^{(b-a+c)/2}\bigl(1 + o_\P(1)\bigr).
        \end{equation*}
        Moreover, we further assume:
        \begin{itemize}
            \item[i.] If $a < 2b + c$, then $\hat\rho = \wt\rho \bigl(1 + o_\P(1)\bigr)$.
            \item[ii.] If $a > 2b + c$, then $\hat \rho = \wt O_\P(d^{-(a-c)/4})$ and thus $\hat \rho \norm{\vmu}_2 = o_\P(1)$.
        \end{itemize}
    \end{enumerate}
\end{lem}
\begin{proof}
According to \cref{eq:param_star} and \eqref{eq:AB_def}, an explicit expression of $\wt\rho$ is given by
\begin{equation}\label{eq:rho_star}
    \wt\rho = 
    \frac{A}{\sqrt{A^2 + B^2}}
    =
    \dfrac{ \norm{\vmu}_2 + \wt g }{\sqrt{ ( \norm{\vmu}_2 + \wt g )^2 + \| \bP_{\vmu}^\perp \wt\zz \|_2^2 } }
    = \frac{d^{b/2}}{\sqrt{ d^{b} + \frac14 d^{a-c} }}  \bigl(1 + o_{\P}(1)\bigr).
\end{equation}
In order to connect $\hat\rho$ with $\wt\rho$, recall \cref{eq:F_AB} that
\begin{equation*}
    \kappa(\rho, \vtheta, \beta_0) \le F_{A,B}(\rho) \le F_{A,B}(\wt\rho) = \bar\kappa,
    \qquad \forall\, \rho \in [-1, 1], \  \vtheta \in \S^{d-1}, \vtheta \perp \vmu, \  \beta_0 \in \R.
\end{equation*}
Apply this to $\hat\kappa = \kappa(\hat\rho, \hat\vtheta, \hat\beta_0)$ and use \cref{lem:upper_bound}, we have
\begin{equation}\label{eq:F_AB_diff}
    0 \le F_{A,B}(\wt\rho) - F_{A,B}(\hat\rho) \le \wt O_\P(1).
\end{equation}
Since $\wt O_\P(1)/F_{A,B}(\wt\rho) = \wt O_\P(1)/\sqrt{A^2 + B^2} \le \wt O_\P(d^{-b/2}) = o_\P(1)$, it implies
\begin{equation*}
    1 - o_\P(1)  =  \frac{F_{A,B}(\hat\rho)}{F_{A,B}(\wt\rho)} 
    = \frac{\hat\rho A }{\sqrt{A^2 + B^2}} + \frac{ \sqrt{1 - \hat\rho^2} B}{\sqrt{A^2 + B^2}}
    = \hat\rho \wt\rho + \sqrt{1 - \hat\rho^2}\sqrt{1 - \wt\rho^2}.
\end{equation*}
Therefore,
\begin{equation}\label{eq:rho_star_to_hat}
    \begin{aligned}
        \wt\rho = 1 - o_\P(1) & \ \Longrightarrow & \hat\rho = 1 - o_\P(1)
        \\
        \wt\rho = o_\P(1)     & \ \Longrightarrow & \hat\rho = o_\P(1)
    \end{aligned}
\end{equation}

\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{lem:rho_hat(a)}:}
If $a - c < b$, then $d^{b/2} \gg d^{(a-c)/2}$ and by \cref{eq:rho_star} and \eqref{eq:rho_star_to_hat} we have $\wt\rho, \hat\rho = 1 - o_\P(1)$. Also,
\begin{equation*}
    \sqrt{1 - \wt\rho^2} = \frac{B}{\sqrt{A^2 + B^2}}
    = \frac{\frac12 d^{(a-b-c)/2}}{\sqrt{ 1 + \frac14 d^{a-b-c} }}\bigl(1 + o_{\P}(1)\bigr)
    = \frac12 d^{(a-b-c)/2}\bigl(1 + o_{\P}(1)\bigr).
\end{equation*}
To derive the precise order of $\sqrt{1 - \hat\rho^2}$, we define $r := \sqrt{1 - \rho^2}$ and $F_{B,A}(r) := rB + \sqrt{1 - r^2}A$. Then $F_{A,B}(\rho) = F_{B,A}(r)$ for any $\rho \in [0, 1]$. We similarly define $\hat r := \sqrt{1 - \hat\rho^2}$ and $\wt r := \sqrt{1 - \wt\rho^2}$. On the event $\mathcal{E} = \{ A > 0, \wt\rho >0, \hat\rho > 0 \}$, by \cref{lem:F_AB}, we have
\begin{equation*}
    F_{A,B}(\hat\rho) - F_{A,B}(\wt\rho) 
    = F_{B,A}(\hat r) - F_{B,A}(\wt r) 
    \le -\frac12 A (\hat r - \wt r)^2.
\end{equation*}
Combined with \cref{eq:F_AB_diff}, it implies
\begin{equation*}
    (\hat r - \wt r)^2 \le  \frac{2}{A}\bigl( F_{A,B}(\wt\rho) - F_{A,B}(\hat\rho) \bigr) \le \wt O_\P(d^{-b/2}),
\end{equation*}
so $|\hat r - \wt r| = \wt O_\P(d^{-b/4})$. Now consider different scenarios. Recall that $\wt r = \frac12 d^{(a-b-c)/2}\bigl(1 + o_{\P}(1)\bigr)$.
\begin{itemize}
    \item If $a - c > b/2$, then $|\hat r - \wt r|/\wt r = \wt O_\P(d^{(-2a + b + 2c)/4}) = o_\P(1)$, deduces $\hat r = \wt r \bigl(1 + o_{\P}(1)\bigr)$.
    \item If $a - c \le b/2$, then we only get $\hat r = \wt O_\P(d^{-b/4})$, and $\hat r \sqrt{d/\pi n} = \wt O_\P(d^{(2a - b-2c)/4})  \le \wt O_\P(1)$.
\end{itemize}
Recall that these hold on event $\mathcal{E}$. Since $\P(\mathcal{E}) \to 1$ as $d \to \infty$, these asymptotic results involving $o_\P(\,\cdot\,)$ and $\wt O_\P(\,\cdot\,)$ also hold on the whole sample space $\Omega$. This concludes the proof of part \ref{lem:rho_hat(a)}.



\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{lem:rho_hat(b)}:}
If $a - c > b$, then $d^{b/2} \ll d^{(a-c)/2}$ and by \cref{eq:rho_star} and \eqref{eq:rho_star_to_hat} we have $\wt\rho, \hat\rho = o_\P(1)$. Also,
\begin{equation*}
    \wt\rho = 
    \frac{A}{\sqrt{A^2 + B^2}}
    = \frac{d^{(b-a+c)/2}}{\sqrt{ d^{b-a+c} + \frac14 }}  \bigl(1 + o_{\P}(1)\bigr)
    = 2d^{(b-a+c)/2} \bigl(1 + o_{\P}(1)\bigr).
\end{equation*}
Again, by \cref{lem:F_AB},
\begin{equation*}
    F_{A,B}(\hat\rho) - F_{A,B}(\wt\rho) \le -\frac12 B(\hat\rho - \wt\rho)^2.
\end{equation*}
Combined with \cref{eq:F_AB_diff}, it implies
\begin{equation*}
    (\hat\rho - \wt\rho)^2 \le  \frac{2}{B}\bigl( F_{A,B}(\wt\rho) - F_{A,B}(\hat\rho) \bigr) \le \wt O_\P(d^{-(a-c)/2}),
\end{equation*}
so $|\hat\rho - \wt\rho| = \wt O_\P(d^{-(a-c)/4})$. Now consider different scenarios.
\begin{itemize}
    \item If $a - c < 2b$, then $|\hat \rho - \wt\rho|/\wt\rho = \wt O_\P(d^{(a - 2b -c)/4}) = o_\P(1)$, deduces $\hat \rho = \wt\rho \bigl(1 + o_{\P}(1)\bigr)$.
    \item If $a - c > 2b$, then we only get $\hat \rho = \wt O_\P(d^{-(a-c)/4})$, and $\hat \rho \norm{\vmu}_2 = \wt O_\P(d^{(2b-a+c)/4}) = o_\P(1)$.
\end{itemize}
This concludes the proof of part \ref{lem:rho_hat(b)}.
\end{proof}
\begin{rem}
    In each part i. of \cref{lem:rho_hat}\ref{lem:rho_hat(a)} and \ref{lem:rho_hat(b)}, we can derive the precise asymptotic of $\hat\rho$, which is same as $\wt\rho$. It is difficult to do so in part ii. of \ref{lem:rho_hat(a)} and \ref{lem:rho_hat(b)}. However, as we will show in \cref{lem:theta_hat_z} and \ref{lem:beta0_asymp}, in case ii. the corresponding term ($\sqrt{1 - \hat\rho}$ or $\hat\rho$) is negligible, which won't affect the asymptotics of test errors.
\end{rem}


\subsubsection{Asymptotic order of $\<\zz_i, \hat\vtheta \>$'s on the margin: Proof of \cref{lem:theta_hat_z}}


Next, we discuss the asymptotics of $\hat\vtheta$ and $\wt\vtheta$. In fact, it suffices to consider the magnitude of their projection on some ``important'' $\zz_i$, which is related to the \emph{support vectors}, defined in \cref{eq:SV_def}. As we mentioned, $\mathcal{SV}_+(\vbeta), \mathcal{SV}_-(\vbeta)$ only depend on $\vbeta$ and $(\XX, \yy)$, not $\beta_0$ or $\tau$. If we fix $\rho = \hat\rho$, then the dependency of $\mathcal{SV}_\pm$ on $\vbeta$ only comes from $\vtheta$. So, recalling \cref{eq:logits}, 
\[ \kappa_i(\rho, \vtheta, \beta_0) = s_i \left( \rho \norm{\vmu}_2 + y_i\beta_0 + \rho y_i g_i + \sqrt{1 - \rho^2} y_i \< \zz_i, \vtheta \> \right), \]
we can rewrite \cref{eq:SV_def} in terms of $\vtheta$:
\begin{equation}\label{eq:SV_def_theta}
	\begin{aligned}
		\mathcal{SV}_+ = \mathcal{SV}_+(\vtheta) & :=  \argmin_{i \in \mathcal{I}_+} \kappa_i(\hat\rho, \vtheta, \beta_0)
	= \argmin_{i \in \mathcal{I}_+} \left\{ \phantom{-} \hat\rho g_i + \sqrt{1 - \hat\rho^2} \< \zz_i, \vtheta \> \right\} , \\
		\mathcal{SV}_- = \mathcal{SV}_-(\vtheta) & :=  \argmin_{i \in \mathcal{I}_-} \kappa_i(\hat\rho, \vtheta, \beta_0)
	= \argmin_{i \in \mathcal{I}_-} \left\{  - \hat\rho g_i - \sqrt{1 - \hat\rho^2} \< \zz_i, \vtheta \> \right\} . \\
	\end{aligned}
\end{equation}
As before, let $\mathsf{sv}_+(\vtheta), \mathsf{sv}_-(\vtheta)$ be (the indices of) any positive and negative support vectors, i.e.,
\begin{equation*}
	\mathsf{sv}_+(\vtheta) \in \mathcal{SV}_+(\vtheta),
	\qquad
	\mathsf{sv}_-(\vtheta) \in \mathcal{SV}_-(\vtheta).
\end{equation*} 
Now, recall that whenever a slope parameter $\vbeta$ is given, the optimal intercept $\check\beta_0 := \check\beta_0(\vbeta)$ (defined in \cref{eq:beta0_optim}) must satisfy the \emph{margin-balancing} condition \cref{eq:margin-bal}, according to \cref{lem:indep_tau}. Hence, fixing $\rho = \hat\rho$ and considering arbitrary $\vtheta$, we can rewrite \cref{eq:margin_pm} and \eqref{eq:margin-bal} as
\begin{equation}\label{eq:svm_sv_bal}
    \begin{aligned}
       \kappa(\hat\rho, \vtheta, \check\beta_0) & = \kappa_{\mathsf{sv}_+(\vtheta)}(\hat\rho, \vtheta, \check\beta_0) = 
       \hat\rho \norm{\vmu}_2 + \check\beta_0 + \hat\rho g_{\mathsf{sv}_+(\vtheta)} + \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\vtheta)}, \vtheta \> \\
       & = \kappa_{\mathsf{sv}_-(\vtheta)}(\hat\rho, \vtheta, \check\beta_0) = 
       \hat\rho \norm{\vmu}_2 - \check\beta_0 - \hat\rho g_{\mathsf{sv}_-(\vtheta)} - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\vtheta)}, \vtheta \>.
    \end{aligned}
\end{equation}
In particular, if $\vtheta = \hat\vtheta$, we denote $\mathsf{sv}_+(\hat\vtheta) \in \mathcal{SV}_+(\hat\vtheta)$, $\mathsf{sv}_-(\hat\vtheta) \in \mathcal{SV}_-(\hat\vtheta)$ as the support vectors of max-margin classifier. The Lemma below describes the magnitude of $\< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>$ and $\< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>$.

%\update{
\begin{lem}[Asymptotics of $\< \zz_{i}, \hat\vtheta \>$'s for support vectors] \label{lem:theta_hat_z}
    Suppose that $a < c + 1$.
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:theta_hat_z(a)}
        If $a < b + c$, then 
        \begin{equation*}
            \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>  =  \wt O_\P(d^{a-\frac{b}{2}-c} \vee 1),
            \qquad
            \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>  =  \wt O_\P(1).
        \end{equation*}
        \item \label{lem:theta_hat_z(b)}
        If $a > b + c$, then 
        \begin{equation*}
            \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> = \sqrt{\frac{d}{\pi n}}\bigl(1 + o_{\P}(1)\bigr),
            \qquad
            \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> = \wt O_\P(1).
        \end{equation*}
    \end{enumerate}
\end{lem}
\begin{proof}
    $\mathcal{SV}_\pm(\vtheta)$ may not be tractable, since it involves a nuisance term $\hat\rho g_i$ as defined in \cref{eq:SV_def_theta}. Therefore, we introduce a proxy of support vectors, which is easier to work with. Formally, let
    \begin{equation}\label{eq:V_def_theta}
        \begin{aligned}
            \mathcal{V}_+ = \mathcal{V}_+(\vtheta) & :=  
        \argmin_{i \in \mathcal{I}_+}    +  \< \zz_i, \vtheta \>  , \\
            \mathcal{V}_- = \mathcal{V}_-(\vtheta) & :=  
        \argmin_{i \in \mathcal{I}_-}    -  \< \zz_i, \vtheta \>  , \\
        \end{aligned}
    \end{equation}
    where $\mathcal{V}_+, \mathcal{V}_-$ are sets of (the indices of) the smallest $y_i \< \zz_{i}, \vtheta \>$ from each class. Similarly, let
    \begin{equation*}
        \mathsf{v}_+(\vtheta) \in  \mathcal{V}_+(\vtheta),
        \qquad
        \mathsf{v}_-(\vtheta) \in  \mathcal{V}_-(\vtheta),
    \end{equation*}
    which are arbitrary elements in $\mathcal{V}_+(\vtheta)$ and $\mathcal{V}_-(\vtheta)$. Note that $\mathcal{V}_\pm$ is simply $\mathcal{SV}_\pm$ but ignoring term $\hat\rho g_i$. Indeed, as we will show later, the impact of $\hat\rho g_i = O_\P(1)$ is almost negligible.

    We are going to prove \cref{lem:theta_hat_z} by deriving tight upper bounds and lower bounds for both $\pm \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_\pm(\hat\vtheta)}, \hat\vtheta \>$. Then we conclude the precise asymptotics by verifying the upper and lower bounds are matched.
    % ~\\
    % \noindent
    % We restrict our analysis on the event $\{ \hat \kappa > 0\}$, i.e., $(\XX, \yy)$ is linearly separable.

    \paragraph{Upper bounds}
    Applying the same idea as \cref{eq:kappa_pm}, we can bound $\pm \< \zz_{\mathsf{v}_\pm(\vtheta)}, \vtheta \>$ via averaging:
    \begin{equation}
        \label{eq:zV_upper}
            \< \zz_{\mathsf{v}_+(\vtheta)}, \vtheta \> \le \< \bar\zz_{+}, \vtheta \> \le  \| \bP_{\vmu}^\perp \bar\zz_+ \|_2,
            \qquad
            - \< \zz_{\mathsf{v}_-(\vtheta)}, \vtheta \> \le - \< \bar\zz_{-}, \vtheta \> \le  \| \bP_{\vmu}^\perp \bar\zz_- \|_2,
    \end{equation}
    where the second inequality for each comes from \cref{eq:theta_min}. 
    To create a connection between $\mathsf{sv}_\pm(\hat\vtheta)$ and $\mathsf{v}_\pm(\hat\vtheta)$, note that by definition \cref{eq:SV_def_theta}
\begin{equation*}
    \begin{aligned}
        \phantom{-} \hat\rho g_{\mathsf{sv}_+(\hat\vtheta)} + \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> 
        & \le \phantom{-} \hat\rho g_{\mathsf{v}_+(\hat\vtheta)} + \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_+(\hat\vtheta)}, \hat\vtheta \>,
        % \quad \ 
        % \phantom{-} \<\zz_{\mathsf{v}_+(\hat\vtheta)}, \hat\vtheta \> \le \phantom{-} \<\zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>,
        \\
        - \hat\rho g_{\mathsf{sv}_-(\hat\vtheta)} - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> 
        & \le - \hat\rho g_{\mathsf{v}_-(\hat\vtheta)} - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_-(\hat\vtheta)}, \hat\vtheta \>.
        % \quad \ 
        % - \<\zz_{\mathsf{v}_-(\hat\vtheta)}, \hat\vtheta \> \le - \<\zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>.
    \end{aligned}
\end{equation*}
Using \cref{eq:zV_upper}, therefore we obtain the following non-asymptotic upper bounds on $\< \zz_{\mathsf{sv}_\pm(\hat\vtheta)}, \hat\vtheta \>$:
\begin{equation}
    \label{eq:zSV_upper}
    \begin{aligned}
        \phantom{-} \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> \ \ 
        & \le   \phantom{-} \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_+(\hat\vtheta)}, \hat\vtheta \> + \hat\rho \bigl(g_{\mathsf{v}_+(\hat\vtheta)} - g_{\mathsf{sv}_+(\hat\vtheta)} \bigr)  \\
        & \le  \phantom{-} \sqrt{1 - \hat\rho^2} \| \bP_{\vmu}^\perp \bar\zz_+ \|_2 + 
        2 \hat\rho \max_{i \in [n]} \abs{g_i} , \\
        - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> \ \ 
        & \le  - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_-(\hat\vtheta)}, \hat\vtheta \> - \hat\rho \bigl(g_{\mathsf{v}_-(\hat\vtheta)} - g_{\mathsf{sv}_-(\hat\vtheta)} \bigr) \\
        & \le  \phantom{-} \sqrt{1 - \hat\rho^2} \| \bP_{\vmu}^\perp \bar\zz_- \|_2 + 
        2 \hat\rho \max_{i \in [n]} \abs{g_i} .
    \end{aligned}
\end{equation}
To compute its asymptotics, recall that $\sqrt{n_+} \cdot \bar\zz_+ \sim \subGind(\bzero, \bI_d; K)$, $\sqrt{n_-} \cdot \bar\zz_- \sim \subGind(\bzero, \bI_d; K)$, and $\| \bP_{\vmu}^\perp \|_\mathrm{F} = \sqrt{d - 1}$. Then by \cref{lem:subG_concentrate}\ref{lem:subG-Hanson-Wright-II},
\begin{align}
        \| \bP_{\vmu}^\perp \bar\zz_+ \|_2 & = \frac{1}{\sqrt{n_+}} \| \bP_{\vmu}^\perp \|_\mathrm{F} \bigl(1 + o_\P(1)\bigr)
    = \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big), \notag \\
        \| \bP_{\vmu}^\perp \bar\zz_- \|_2 & = \frac{1}{\sqrt{n_-}} \| \bP_{\vmu}^\perp \|_\mathrm{F} \bigl(1 + o_\P(1)\bigr)
    = \mathmakebox[\widthof{$\displaystyle \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big)$}][r]{\sqrt{\frac{d}{n}}\big(1 + o_\P(1) \big)} = o_\P(1).
    \label{eq:Pzpm_asymp}
\end{align}
While, by maximal inequality \cref{lem:subG}\ref{lem:subG-c} or \cref{eq:max_g}, we have
\begin{equation}
    \label{eq:g_asymp}
    \max_{i \in [n]} \abs{g_i} = O_\P(\log n) = \wt O_\P(1),
\end{equation}
Plugging \cref{eq:Pzpm_asymp} and \eqref{eq:g_asymp} into \cref{eq:zSV_upper} gives the asymptotic upper bounds (involving $\hat\rho$):
\begin{equation}
    \label{eq:zSV_upper_asymp}
    \begin{aligned}
        \phantom{-} \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>  
        & \le  \sqrt{1 - \hat\rho^2} \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) + 
        \hat\rho \cdot \wt O_\P(1) , \\
        - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> 
        & \le  \sqrt{1 - \hat\rho^2} \cdot o_\P(1) + 
        \hat\rho \cdot \wt O_\P(1) .
    \end{aligned}
\end{equation}

\paragraph{Lower bounds} Similar as the proof of \cref{lem:upper_bound}, a lower bound can be obtained by plugging our constructed solution $\wt\vtheta = \bP_{\vmu}^\perp \wt\zz/\| \bP_{\vmu}^\perp \wt\zz \|_2$, which can be a good ``proxy'' of $\vtheta$. Again, by margin-balancing condition \cref{eq:svm_sv_bal}, we can express the optimal $\vtheta$ as\footnote{
    Notice that if $|\hat\rho| < 1$, then $\argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \kappa(\hat\rho, \vtheta, \hat\beta_0)$ is unique (on the event of $\{ \hat\kappa > 0\}$), and we could write $\hat\vtheta = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \kappa(\hat\rho, \vtheta, \hat\beta_0)$. However, if $|\hat\rho| = 1$, then according to our construction \cref{eq:def-rho-theta}, the arguments of the maxima can be any $\vtheta \in \S^{d-1}$ such that $\vtheta \perp \vmu$, while $\hat\vtheta = \vmu_\perp$ as defined in \cref{eq:def-rho-theta_hat}.
}
\begin{equation*}
    \begin{aligned}
        \hat\vtheta & \in \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \kappa(\hat\rho, \vtheta, \hat\beta_0)
    = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \frac{\kappa_{\mathsf{sv}_+(\vtheta)}(\hat\rho, \vtheta, \hat\beta_0) + \kappa_{\mathsf{sv}_-(\vtheta)}(\hat\rho, \vtheta, \hat\beta_0)}{2} \\
    & = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } 
    \left\{ \hat\rho \norm{\vmu}_2 + \hat\rho \frac{g_{\mathsf{sv}_+(\vtheta)} - g_{\mathsf{sv}_-(\vtheta)} }{2} + \sqrt{1 - \hat\rho^2}  \frac{ \< \zz_{\mathsf{sv}_+(\vtheta)}, \vtheta \>  -  \< \zz_{\mathsf{sv}_-(\vtheta)}, \vtheta \>  }{2} \right\}   \\
    & = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \left\{  \hat\rho \left(  g_{\mathsf{sv}_+(\vtheta)} - g_{\mathsf{sv}_-(\vtheta)} \right) +  \sqrt{1 - \hat\rho^2} \left( \< \zz_{\mathsf{sv}_+(\vtheta)}, \vtheta \>  +  \< \zz_{\mathsf{sv}_-(\vtheta)}, \vtheta \> \right)  \right\}.
    \end{aligned}
\end{equation*}
Therefore, recalling \cref{eq:V_def_theta}, we have
\begin{align*}
        & \sqrt{1 - \hat\rho^2}  \bigl( \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>  -  \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>  \bigr) \\
        \ge {} & \sqrt{1 - \hat\rho^2}  \bigl( \< \zz_{\mathsf{sv}_+(\wt\vtheta)}, \wt\vtheta \>  -  \< \zz_{\mathsf{sv}_-(\wt\vtheta)}, \wt\vtheta \>  \bigr) + \hat\rho\bigl( g_{\mathsf{sv}_+(\wt\vtheta)} - g_{\mathsf{sv}_-(\wt\vtheta)} -  g_{\mathsf{sv}_+(\hat\vtheta)} + g_{\mathsf{sv}_-(\hat\vtheta)} \bigr) \\
        \ge {} & \sqrt{1 - \hat\rho^2}   \bigl( \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \>  -  \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \>  \bigr) 
        - 4 \hat\rho \max_{i \in [n]} \abs{g_i}.
        % \min_{i \in \mathcal{I}_+} \< + \zz_i, \wt\vtheta \> 
        % + \sqrt{1 - \hat\rho^2}  \min_{i \in \mathcal{I}_-} \< - \zz_i, \wt\vtheta \>
\end{align*}
Combining it with \cref{eq:zSV_upper}, we can obtain a lower bound for each term using $\wt\vtheta$:
\begin{equation}\label{eq:zSV_lower}
    \begin{aligned}
        \phantom{-} \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>
        & \ge \sqrt{1 - \hat\rho^2}   \bigl( \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \>  -  \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \>  \bigr) 
        - 4 \hat\rho \max_{i \in [n]} \abs{g_i}
        + \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> \\
        & \ge \sqrt{1 - \hat\rho^2} \bigl( 
        - \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \> - \| \bP_{\vmu}^\perp \bar\zz_- \|_2 \bigr)
        - 6 \hat\rho \max_{i \in [n]} \abs{g_i}
        + \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \>, 
        \\
        - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>
        & \ge \sqrt{1 - \hat\rho^2}   \bigl( \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \>  -  \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \>  \bigr) 
        - 4 \hat\rho \max_{i \in [n]} \abs{g_i}
        - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> \\
        & \ge \sqrt{1 - \hat\rho^2} \bigl( + \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \>
        - \| \bP_{\vmu}^\perp \bar\zz_+ \|_2  \bigr)
        - 6 \hat\rho \max_{i \in [n]} \abs{g_i}
        - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \>.
    \end{aligned}
\end{equation}
To derive its asymptotic order, we first define two statistics that are closely related to $\wt\vtheta$: 
\begin{equation}\label{eq:wt_theta_pm}
    \wt\vtheta_+ := \frac{\bP_{\vmu}^\perp \bar\zz_+}{\| \bP_{\vmu}^\perp \bar\zz_+ \|_2},
    \qquad
    \wt\vtheta_- := \frac{-\bP_{\vmu}^\perp \bar\zz_-}{\| \bP_{\vmu}^\perp \bar\zz_- \|_2}.
\end{equation}
Then, the difference terms inside the parentheses in \cref{eq:zSV_lower} can be expressed as
\begin{equation}\label{eq:zV_diff}
    \begin{aligned}
        \phantom{+} \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \> - \| \bP_{\vmu}^\perp \bar\zz_+ \|_2
        & = \min_{i \in \mathcal{I}_+} \< + \zz_i, \wt\vtheta \> - \< \bar\zz_+, \wt\vtheta_+ \>
        = \min_{i \in \mathcal{I}_+} \< \zz_i - \bar\zz_+, \wt\vtheta \> + \< \bar\zz_+, \wt\vtheta - \wt\vtheta_+ \>,
        \\
        - \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \> - \| \bP_{\vmu}^\perp \bar\zz_- \|_2
        & = \min_{i \in \mathcal{I}_-} \< - \zz_i, \wt\vtheta \> + \< \bar\zz_-, \wt\vtheta_- \>
        = \min_{i \in \mathcal{I}_-} \< \bar\zz_- - \zz_i, \wt\vtheta \> - \< \bar\zz_-, \wt\vtheta - \wt\vtheta_- \>.
    \end{aligned}
\end{equation}
Now we study the two terms on the R.H.S. of \cref{eq:zV_diff}. For the first term, based on \cref{eq:max_z},
\begin{equation}\label{eq:zV_diff_1}
    \begin{aligned}
        \min_{i \in \mathcal{I}_+} \< \zz_i - \bar\zz_+, \wt\vtheta \>
        \ge -\max_{i \in \mathcal{I}_+} \bigl| \< \zz_i - \bar\zz_+, \wt\vtheta \> \bigr|
        & = \wt O_\P(1), \\
        \min_{i \in \mathcal{I}_-} \< \bar\zz_- - \zz_i, \wt\vtheta \>
        \ge -\max_{i \in \mathcal{I}_-} \bigl| \< \zz_i - \bar\zz_-, \wt\vtheta \> \bigr|
        & = \wt O_\P(1).
    \end{aligned}
\end{equation}
For the second term,
\begin{equation}\label{eq:zV_diff_2+}
    \begin{aligned}
        \< \bar\zz_+, \wt\vtheta - \wt\vtheta_+ \>
        & =
        \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \< \bar\zz_+, \bP_{\vmu}^\perp \wt\zz \> 
        - \frac{1}{\| \bP_{\vmu}^\perp \bar\zz_+ \|_2} \< \bar\zz_+, \bP_{\vmu}^\perp \bar\zz_+ \> \\
        & = 
        \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} 
        \left\{  \< \bar\zz_+, \bP_{\vmu}^\perp \wt\zz \>  -  \< \bar\zz_+, \bP_{\vmu}^\perp \bar\zz_+ \> \cdot \frac{\| \bP_{\vmu}^\perp \wt\zz \|_2}{\| \bP_{\vmu}^\perp \bar\zz_+ \|_2} \right\}
        \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{\ge}
        \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} 
        \left\{  \< \bar\zz_+, \bP_{\vmu}^\perp \wt\zz \> 
        - \< \bar\zz_+, \bP_{\vmu}^\perp \bar\zz_+ \> \cdot 
        \frac12 \biggl( 1 +  \frac{\| \bP_{\vmu}^\perp \bar\zz_- \|_2}{\| \bP_{\vmu}^\perp \bar\zz_+ \|_2} \biggr) 
        \right\} \\
        & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{=} - \frac{1}{2\| \bP_{\vmu}^\perp \wt\zz \|_2} \left( 
            \< \bar\zz_+, \bP_{\vmu}^\perp \bar\zz_-  \> + \| \bP_{\vmu}^\perp \bar\zz_+ \|_2 \| \bP_{\vmu}^\perp \bar\zz_- \|_2
         \right) \\
        & \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{=}
         - \sqrt{\frac{\pi n}{d}}\big(1 + o_\P(1) \big) \left\{ 
            O_\P\biggl(  \sqrt{\frac{d}{\pi n^2}} \biggr) + \sqrt{\frac{d}{\pi n}}\sqrt{\frac{d}{n}} \big(1 + o_\P(1) \big)
         \right\} \\
        & = - \sqrt{\frac{d}{n}}\big(1 + o_\P(1) \big)
        = o_\P(1),
    \end{aligned}
\end{equation}
where (i) is from triangular inequality $2\| \bP_{\vmu}^\perp \wt\zz \|_2 \le \| \bP_{\vmu}^\perp \bar\zz_+ \|_2 + \| \bP_{\vmu}^\perp \bar\zz_- \|_2$, (ii) uses $2 \wt\zz - \bar z_+ = -\bar z_-$, and (iii) applies the asymptotic results \cref{eq:P_wtz}, \eqref{eq:Pzpm_asymp}, and the fact that $\bar\zz_+ \indep \bar\zz_-$, 
\begin{equation*}
    \< \bar\zz_{+}, \bP_{\vmu}^\perp \bar\zz_{-} \> = \frac{1}{\sqrt{n_+ n_-}} O_{\P}(\| \bP_{\vmu}^\perp \|_\mathrm{F}) = O_\P\biggl( \sqrt{\frac{d}{\pi n^2}}  \biggr),
\end{equation*}
by \cref{lem:subG_concentrate}\ref{lem:subG-Bernstein}. Similarly, we also have
\begin{equation}\label{eq:zV_diff_2-}
    \begin{aligned}
        - \< \bar\zz_-, \wt\vtheta - \wt\vtheta_- \>
        & =
        - \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \< \bar\zz_-, \bP_{\vmu}^\perp \wt\zz \> 
        - \frac{1}{\| \bP_{\vmu}^\perp \bar\zz_- \|_2} \< \bar\zz_-, \bP_{\vmu}^\perp \bar\zz_- \> \\
        % & = 
        % - \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} 
        % \left\{  \< \bar\zz_-, \bP_{\vmu}^\perp \wt\zz \>  +  \< \bar\zz_-, \bP_{\vmu}^\perp \bar\zz_- \> \cdot \frac{\| \bP_{\vmu}^\perp \wt\zz \|_2}{\| \bP_{\vmu}^\perp \bar\zz_- \|_2} \right\}
        % \\
        % & \ge
        % -\frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} 
        % \left\{  \< \bar\zz_-, \bP_{\vmu}^\perp \wt\zz \> 
        % + \< \bar\zz_-, \bP_{\vmu}^\perp \bar\zz_- \> \cdot 
        % \frac12 \biggl( 1 +  \frac{\| \bP_{\vmu}^\perp \bar\zz_+ \|_2}{\| \bP_{\vmu}^\perp \bar\zz_- \|_2} \biggr) 
        % \right\} \\
        & \ge - \frac{1}{2\| \bP_{\vmu}^\perp \wt\zz \|_2} \left( 
            \< \bar\zz_+, \bP_{\vmu}^\perp \bar\zz_-  \> + \| \bP_{\vmu}^\perp \bar\zz_+ \|_2 \| \bP_{\vmu}^\perp \bar\zz_- \|_2
         \right) \\
        & = o_\P(1).
    \end{aligned}
\end{equation}
Substituting \cref{eq:zV_diff_1}, \eqref{eq:zV_diff_2+}, and \eqref{eq:zV_diff_2-} into \cref{eq:zV_diff}, we get
\begin{equation}
    \label{eq:zv_diff_asymp}
    \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \> - \| \bP_{\vmu}^\perp \bar\zz_+ \|_2
    \ge \wt O_\P(1),
    \qquad
    - \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \> - \| \bP_{\vmu}^\perp \bar\zz_- \|_2
    \ge \wt O_\P(1).
\end{equation}
And combining this with \cref{eq:Pzpm_asymp}, we have
\begin{equation}
    \label{eq:zv_wt_asymp}
    \< \zz_{\mathsf{v}_+(\wt\vtheta)}, \wt\vtheta \> 
    \ge \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) + \wt O_\P(1),
    \qquad
    - \< \zz_{\mathsf{v}_-(\wt\vtheta)}, \wt\vtheta \> 
    \ge \wt O_\P(1).
\end{equation}
Plugging \cref{eq:zv_diff_asymp}, \eqref{eq:zv_wt_asymp}, and \eqref{eq:g_asymp} into \cref{eq:zSV_lower} gives the asymptotic lower bounds (involving $\hat\rho$):
\begin{equation}
    \label{eq:zSV_lower_asymp}
    \begin{aligned}
        \phantom{-} \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>  
        & \ge  \sqrt{1 - \hat\rho^2} \biggl( \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) 
        + \wt O_\P(1)
        \biggr)
        + 
        \hat\rho \cdot \wt O_\P(1) , \\
        - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> 
        & \ge  \sqrt{1 - \hat\rho^2} \cdot \wt O_\P(1) + 
        \hat\rho \cdot \wt O_\P(1) .
    \end{aligned}
\end{equation}


~\\
\noindent
Finally, combining upper bounds \cref{eq:zSV_upper_asymp} and lower bounds \cref{eq:zSV_lower_asymp}, we obtain the exact order
\begin{equation*}
    \begin{aligned}
        \phantom{-} \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>  
        & =  \sqrt{1 - \hat\rho^2} \biggl( \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) 
        + \wt O_\P(1)
        \biggr)
        + 
        \hat\rho \cdot \wt O_\P(1) \\
        & = \sqrt{1 - \hat\rho^2} \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) 
        + \wt O_\P(1),
        \\
        - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> 
        & =  \sqrt{1 - \hat\rho^2} \cdot \wt O_\P(1) + 
        \hat\rho \cdot \wt O_\P(1) \\
        & = \wt O_\P(1).
    \end{aligned}
\end{equation*}


\paragraph{\ref{lem:theta_hat_z(a)}:}
If $a < b + c$, according to \cref{lem:rho_hat}\ref{lem:rho_hat(a)}, $\hat\rho = 1 - o_\P(1)$. It is clear that \cref{lem:theta_hat_z} holds for $\hat\rho = \pm 1$. Now, restrict on the event $\{ |\hat\rho| < 1 \}$.
\begin{itemize}
    \item If $a > \frac{b}{2} + c$, then $\sqrt{1 - \hat\rho^2} = \frac12 d^{(a-b-c)/2}\bigl( 1 + o_\P(1) \bigr)$, hence
    \begin{equation*}
        \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>
        = \frac12 d^{a - \frac{b}{2} - c}\big(1 + o_\P(1) \big).
    \end{equation*}
    \item If $a \le \frac{b}{2} + c$, then $\sqrt{1 - \hat\rho^2}\sqrt{d/\pi n} = \wt O_\P(1)$, hence
    \begin{equation*}
        \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>
        = \wt O_\P(1).
    \end{equation*}
\end{itemize}
\paragraph{\ref{lem:theta_hat_z(b)}:}
If $a > b + c$, according to \cref{lem:rho_hat}\ref{lem:rho_hat(b)}, $\hat\rho = o_\P(1)$. Hence, on the event $\{ |\hat\rho| < 1 \}$,
\begin{equation*}
    \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>
    = \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big).
\end{equation*}
This also holds regardless of $\hat\rho$, since $\P( |\hat\rho| < 1 ) \to 1$ as $d \to \infty$. Then we complete the proof.
% ~\\
% \noindent
% At last, remind that all the results above hold on event $\{ \hat\kappa > 0\}$. Since $\P( \hat\kappa > 0) \to 1$ as $d \to \infty$, these asymptotic results involving $o_\P(\,\cdot\,)$ and $\wt O_\P(\,\cdot\,)$ also hold on the whole sample space $\Omega$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Old ver. with mistakes: w/o considering lower bound, uniform convergence, \rho = 1 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% (a): \update{Clearly, the conclusion holds for $\hat\rho = 1$.} If $a < b + c$, by \cref{lem:rho_hat}\ref{lem:rho_hat(a)} and \cref{eq:z_v_bound},
% \begin{equation*}
%     \begin{aligned}
%     &   \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>
%     \le  \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_+(\hat\vtheta)}, \hat\vtheta \>
%     + \hat\rho ( g_{\mathsf{v}_+(\hat\vtheta)} - g_{\mathsf{sv}_+(\hat\vtheta)}) \\
%     \le {} &  \sqrt{1 - \hat\rho^2}\sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) + o_\P(1) \\
%     \le {} &  \begin{cases} 
%         \,  d^{(a-b-c)/2} \cdot d^{(a-c)/2} \big(1 + o_\P(1) \big) + o_\P(1)
%         =  d^{a-\frac{b}{2}-c} \big(1 + o_\P(1) \big) , & \ \text{if} \ a > \frac{b}{2} + c, \\
%         \,  O_\P(1)\big(1 + o_\P(1) \big) + o_\P(1) = O_\P(1) , & \ \text{if} \ a \le \frac{b}{2} + c, \end{cases}
%     \end{aligned}
% \end{equation*}
% and
% \begin{equation*}
%     \begin{aligned}
%     &   \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>
%     \le  \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{v}_-(\hat\vtheta)}, \hat\vtheta \>
%     + \hat\rho ( g_{\mathsf{v}_-(\hat\vtheta)} - g_{\mathsf{sv}_-(\hat\vtheta)}) \\
%     \le {} &  \sqrt{1 - \hat\rho^2}\sqrt{\frac{d}{n}}\big(1 + o_\P(1) \big) + o_\P(1) \\
%     \le {} &  \begin{cases} 
%         \,  d^{(a-b-c)/2} \cdot d^{-c/2} \big(1 + o_\P(1) \big) + o_\P(1)
%          , & \ \text{if} \ a > \frac{b}{2} + c, \\
%         \,  O_\P(d^{-a/2})\big(1 + o_\P(1) \big) + o_\P(1)  , & \ \text{if} \ a \le \frac{b}{2} + c. \end{cases} \\
%     = {} & o_\P(1).
%     \end{aligned}
% \end{equation*}
% (b): If $a > b + c$, which implies $a - c > 0$. By \cref{lem:rho_hat}\ref{lem:rho_hat(b)}, $\hat\rho = o_\P(1)$, then
% \begin{equation}\label{eq:z_sv_bound}
%     \begin{aligned}
%         \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> & \le  \< \zz_{\mathsf{v}_+(\hat\vtheta)}, \hat\vtheta \> + \frac{\hat\rho(g_{\mathsf{v}_+(\hat\vtheta)} - g_{\mathsf{sv}_+(\hat\vtheta)})}{\sqrt{1 - \hat\rho^2}}  = \< \zz_{\mathsf{v}_+(\hat\vtheta)}, \hat\vtheta \> + o_\P(1)
%         \le \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big), \\
%         \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> & \le  \< \zz_{\mathsf{v}_-(\hat\vtheta)}, \hat\vtheta \> + \frac{\hat\rho(g_{\mathsf{v}_-(\hat\vtheta)} - g_{\mathsf{sv}_-(\hat\vtheta)})}{\sqrt{1 - \hat\rho^2}}  = \< \zz_{\mathsf{v}_-(\hat\vtheta)}, \hat\vtheta \> + o_\P(1)
%         \le O_\P(1).
%     \end{aligned}
% \end{equation}
% Now we show that these bounds are nearly attained by $\wt\vtheta$. Since $\bar\zz_+ \indep \bar\zz_-$, by \cref{lem:subG_concentrate}\ref{lem:subG-Bernstein},
% \begin{equation*}
%     \< \bar\zz_{+}, \bP_{\vmu}^\perp \bar\zz_{-} \> = \< \bar\zz_{-}, \bP_{\vmu}^\perp \bar\zz_{+} \> = \frac{1}{\sqrt{n_+ n_-}} O_{\P}(\| \bP_{\vmu}^\perp \|_\mathrm{F}) = O_\P\biggl( \sqrt{\frac{d}{\pi n^2}}  \biggr).
% \end{equation*}
% Recall that $\wt\vtheta$ is defined in \cref{eq:param_star}. Therefore, combining with \cref{eq:P_wtz},
% \begin{equation*}
%     \begin{aligned}
%         \<  \bar\zz_{+}, \wt\vtheta \> & = \frac{\| \bP_{\vmu}^\perp \bar\zz_+ \|_2^2}{2 \| \bP_{\vmu}^\perp \wt\zz \|_2} + \frac{\< \bar\zz_{+}, \bP_{\vmu}^\perp \bar\zz_{-} \>}{2 \|\bP_{\vmu}^\perp \wt\zz \|_2} = \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) + O_\P\biggl( \frac{1}{\sqrt{n}} \biggr)
%         = \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big), \\
%         \<  \bar\zz_{-}, \wt\vtheta \> & = \frac{\| \bP_{\vmu}^\perp \bar\zz_- \|_2^2}{2 \| \bP_{\vmu}^\perp \wt\zz \|_2} + \frac{\< \bar\zz_{-}, \bP_{\vmu}^\perp \bar\zz_{+} \>}{2 \|\bP_{\vmu}^\perp \wt\zz \|_2} = \sqrt{\frac{\pi d}{n}}\big(1 + o_\P(1) \big) + O_\P\biggl( \frac{1}{\sqrt{n}} \biggr)
%         =  O_{\P}(1). \\
%     \end{aligned}
% \end{equation*}
% Finally, use the results from \cref{eq:z_theta_diff+} and \eqref{eq:z_theta_diff-}, we have
% \begin{equation}\label{eq:z_theta_star}
%     \begin{aligned}
%         \< \zz_i, \wt\vtheta \> & = \<  \bar\zz_{+}, \wt\vtheta \> + \< \zz_i - \bar\zz_+, \wt\vtheta \>
%         = \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) + O_\P(1), 
%         &  \forall\, i \in \mathcal{I}_+,  \\
%         \< \zz_i, \wt\vtheta \> & = \<  \bar\zz_{-}, \wt\vtheta \> + \< \zz_i - \bar\zz_-, \wt\vtheta \>
%         = O_\P(1), 
%         &  \forall\, i \in \mathcal{I}_-. \\
%     \end{aligned}
% \end{equation}
% Note that these magnitudes match those in \cref{eq:z_sv_bound}. Recall that by definition and \cref{eq:svm_sv_bal},
% \begin{equation*}
%     \begin{aligned}
%         \hat\vtheta & = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \kappa(\hat\rho, \vtheta, \hat\beta_0)
%     = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \frac{\kappa_{\mathsf{sv}_+(\vtheta)}(\hat\rho, \vtheta, \hat\beta_0) + \kappa_{\mathsf{sv}_-(\vtheta)}(\hat\rho, \vtheta, \hat\beta_0)}{2} \\
%     & = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } 
%     \left\{ \hat\rho \norm{\vmu}_2 + \hat\rho \frac{g_{\mathsf{sv}_+(\vtheta)} + g_{\mathsf{sv}_-(\vtheta)} }{2} + \sqrt{1 - \hat\rho^2}  \frac{ \< \zz_{\mathsf{sv}_+(\vtheta)}, \vtheta \>  +  \< \zz_{\mathsf{sv}_-(\vtheta)}, \vtheta \>  }{2} \right\}   \\
%     & = \argmax_{ \vtheta \in \S^{d-1}, \vtheta \perp \vmu } \left\{  o_\P(1) + \left( \< \zz_{\mathsf{sv}_+(\vtheta)}, \vtheta \>  +  \< \zz_{\mathsf{sv}_-(\vtheta)}, \vtheta \> \right)  \right\},
%     \end{aligned}
% \end{equation*}
% then by using \cref{eq:z_theta_star}, we obtain a lower bound
% \begin{equation*}
%     \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>  +  \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> 
%     \ge \< \zz_{\mathsf{sv}_+(\wt\vtheta)}, \wt\vtheta \>  +  \< \zz_{\mathsf{sv}_-(\wt\vtheta)}, \wt\vtheta \> + o_\P(1)
%     \ge \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big).
% \end{equation*}
% In the end, combining it with the upper bound \cref{eq:z_sv_bound}, we reach to conclusion
% \begin{equation*}
%     \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> = \sqrt{\frac{d}{\pi n}}\bigl(1 + o_{\P}(1)\bigr).
% \end{equation*}

\subsubsection{Asymptotic expression of $\hat\beta_0$: Proof of \cref{lem:beta0_asymp}}

Finally, we consider arbitrary $\tau \ge 1$ and give an explicit expression for $\hat\beta_0$ with its asymptotics. Be aware that $\tau = \tau_d$ may depend on $d$.
\begin{lem}[Asymptotics of $\hat\beta_0$] \label{lem:beta0_asymp}
    Suppose that $a < c + 1$ and $\tau \ge 1$. Then we have
    \begin{equation*}
        \begin{aligned}
            \hat\beta_0 & = \left(1 - \frac{2}{\tau + 1}\right) \hat\rho\norm{\vmu}_2 
            - \hat\rho \frac{\tau g_{\mathsf{sv}_-(\hat\vtheta)} + g_{\mathsf{sv}_+(\hat\vtheta)} }{\tau + 1}
            - \sqrt{1 - \hat\rho^2} \frac{ \tau \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \> + \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> }{\tau + 1} \\
            & = \left(1 - \frac{2}{\tau + 1}\right) \hat\rho\norm{\vmu}_2 
            - \frac{1}{\tau + 1} \sqrt{1 - \hat\rho^2}\< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> + \wt O_\P(1).
        \end{aligned}
    \end{equation*}
    \begin{enumerate}[label=(\alph*)]
        \item \label{lem:beta0_asymp(a)}
        If $a < b + c$, then
        \begin{equation*}
            \begin{aligned}
                \hat\beta_0 & = \left(1 - \frac{2}{\tau + 1}\right) \hat\rho\norm{\vmu}_2 
            - \frac{1}{\tau + 1} \wt O_\P(d^{a-\frac{b}{2}-c} \vee 1) 
            + \wt O_\P(1) \\
            & = \left(1 - \frac{2}{\tau + 1}\right) d^{b/2}\bigl(1 + o_{\P}(1)\bigr) 
            - \frac{1}{\tau + 1} \wt O_\P(d^{a-\frac{b}{2}-c} \vee 1)
            + \wt O_\P(1).
            \end{aligned}
    \end{equation*}
        \item \label{lem:beta0_asymp(b)}
        If $a > b + c$, then
        \begin{equation*}
            \begin{aligned}
            & \hat\beta_0  = \left(1 - \frac{2}{\tau + 1}\right) \hat\rho\norm{\vmu}_2 
            - \frac{1}{\tau + 1}\sqrt{\frac{d}{\pi n}}\bigl(1 + o_{\P}(1)\bigr) 
            + \wt O_\P(1) \\
            = {} & 
            \begin{cases} 
                \,  \displaystyle \left(1 - \frac{2}{\tau + 1}\right) 2d^{(2b-a+c)/2} \bigl(1 + o_{\P}(1)\bigr) 
                - \frac{1}{\tau + 1} d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr) 
                + \wt O_\P(1) , & \ \text{if} \ a < 2b + c, \\
                \,  \displaystyle \phantom{\left(1 - \frac{2}{\tau + 1}\right) 2d^{(2b-a+c)/2} \bigl(1 + o_{\P}(1)\bigr) 
                }
                - \frac{1}{\tau + 1} d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr) 
                + \wt O_\P(1) , & \ \text{if} \ a > 2b + c. \end{cases}
        \end{aligned}
    \end{equation*}
    \end{enumerate}
\end{lem}
\begin{proof}
    We rewrite the \emph{margin-balancing} condition \cref{eq:margin_pm}, \eqref{eq:margin-bal} in terms of $\hat\rho, \hat\vtheta, \hat\beta_0$, which generalizes \cref{eq:svm_sv_bal} to arbitrary $\tau \ge 1$:
    \begin{equation*}
        \begin{aligned}
           \kappa(\hat\rho, \hat\vtheta, \hat\beta_0) & = \kappa_{\mathsf{sv}_+(\hat\vtheta)}(\hat\rho, \hat\vtheta, \hat\beta_0) = 
           \tau^{-1} \Bigl(
           \hat\rho \norm{\vmu}_2 + \hat\beta_0 + \hat\rho g_{\mathsf{sv}_+(\hat\vtheta)} + \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \> 
           \Bigr)
           \\
           & = \kappa_{\mathsf{sv}_-(\hat\vtheta)}(\hat\rho, \hat\vtheta, \hat\beta_0) = 
           \phantom{ \tau^{-1} \Bigl( }
           \hat\rho \norm{\vmu}_2 - \hat\beta_0 - \hat\rho g_{\mathsf{sv}_-(\hat\vtheta)} - \sqrt{1 - \hat\rho^2} \< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>
           .
        \end{aligned}
    \end{equation*}
    Then we can solve the expression for $\hat\beta_0$ (this equals \cref{eq:beta0_hat} in \cref{lem:indep_tau} with parametrization \cref{eq:def-rho-theta_hat}). Its asymptotic simplification is followed by \cref{eq:g_asymp}:
    \begin{equation*}
        \abs{\hat\rho \frac{\tau g_{\mathsf{sv}_-(\hat\vtheta)} + g_{\mathsf{sv}_+(\hat\vtheta)} }{\tau + 1} } 
        \le \abs{\hat\rho} \frac{\tau |g_{\mathsf{sv}_-(\hat\vtheta)}| + |g_{\mathsf{sv}_+(\hat\vtheta)}| }{\tau + 1}
        \le \max_{i \in [n]} \abs{g_i} = \wt O_\P(1),
    \end{equation*}
    and \cref{lem:theta_hat_z}:
    \begin{equation*}
        \abs{
        \frac{\tau}{\tau + 1} \sqrt{1 - \hat\rho^2}\< \zz_{\mathsf{sv}_-(\hat\vtheta)}, \hat\vtheta \>
        } = \wt O_\P(1).
    \end{equation*}
    For \textbf{\ref{lem:beta0_asymp(a)}}, plugging $\hat\rho = 1 - o_\P(1)$ by \cref{lem:rho_hat}\ref{lem:rho_hat(a)} and asymptotics of $\< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>$ by \cref{lem:theta_hat_z}\ref{lem:theta_hat_z(a)}. For \textbf{\ref{lem:beta0_asymp(b)}}, plugging $\hat\rho = 2d^{(b-a+c)/2}\bigl(1 + o_\P(1)\bigr)$ by \cref{lem:rho_hat}\ref{lem:rho_hat(b)} from i., while $\hat\rho\norm{\bmu}_2 = o_\P(1)$ from ii., and asymptotics of $\< \zz_{\mathsf{sv}_+(\hat\vtheta)}, \hat\vtheta \>$ by \cref{lem:theta_hat_z}\ref{lem:theta_hat_z(b)}. This completes the proof.
\end{proof}


\subsection{Classification error: Completing the proof of \cref{thm:main_high-imbal}}
\label{subsec:highimb_err}

\begin{proof}[\textbf{Proof of \cref{thm:main_high-imbal}}]
Let $(\xx_\mathrm{new}, y_\mathrm{new})$ be a test data point independent of the training set $\{(\xx_i, y_i)\}_{i=1}^n$, such that $\xx_\mathrm{new} = y_\mathrm{new} \bmu + \zz_\mathrm{new}$, and $\zz_\mathrm{new} \sim \subGind(\bzero, \bI_d; K)$. Recall $\hat f(\xx) = \< \xx, \hat\vbeta \> + \hat\beta_0$. Following the same decomposition as \cref{eq:logits},
\begin{equation*}
    \begin{aligned}
        y_\mathrm{new} \hat f(\xx_\mathrm{new}) & = y_\mathrm{new} (\< \xx_\mathrm{new}, \hat\vbeta \> + \hat\beta_0) \\
        & = \hat\rho \norm{\vmu}_2 + y_\mathrm{new} \hat\beta_0 + 
        y_\mathrm{new} \bigl(  \hat\rho g_\mathrm{new} + \sqrt{1 - \hat\rho^2} \< \zz_\mathrm{new}, \hat\vtheta \> \bigr) \\
        & = \hat\rho \norm{\vmu}_2 + y_\mathrm{new} \hat\beta_0 + y_\mathrm{new} G_d,
    \end{aligned}
\end{equation*}
where 
\begin{equation*}
g_\mathrm{new} := \left\< \zz_\mathrm{new}, \frac{\vmu}{\norm{\vmu}_2} \right\>,
\qquad 
G_d := \hat\rho g_\mathrm{new} + \sqrt{1 - \hat\rho^2} \< \zz_\mathrm{new}, \hat\vtheta \>.
\end{equation*}
Therefore, the minority and majority test errors are
\begin{equation*}
    \begin{aligned}
        \Err_+ & = \P\left( \hat f(\xx_\mathrm{new}) \le 0 \,\big|\, y_\mathrm{new} = +1 \right)
        = \P\left( \hat\rho \norm{\vmu}_2 + \hat\beta_0 + G_d \le 0 \right), \\
        \Err_- & = \P\left( \hat f(\xx_\mathrm{new}) > 0 \,\big|\, y_\mathrm{new} = -1 \right)
        = \P\left( \hat\rho \norm{\vmu}_2 - \hat\beta_0 - G_d < 0 \right). \\
    \end{aligned}
\end{equation*}
By \cref{lem:subG_concentrate}\ref{lem:subG-Hoeffding}, we have $\norm{g_\mathrm{new}}_{\psi_2}, \| \< \zz_\mathrm{new}, \hat\vtheta \> \|_{\psi_2} \lesssim K$, since $\zz_\mathrm{new} \indep (\hat\rho, \hat\vtheta)$ and then $\forall\, t > 0$,
\begin{equation*}
     \P\left( \bigl| \< \zz_\mathrm{new}, \hat\vtheta \> \bigr| > t \right)
     =  \E\left[ \P\left( \bigl| \< \zz_\mathrm{new}, \hat\vtheta \> \bigr| > t \,\big|\, \hat\vtheta \right) \right]
     \le 2 e^{-ct^2/K^2}, \qquad \text{for some} ~ c > 0.
\end{equation*}
Then by \cref{lem:subG}\ref{lem:subG-a},
\begin{equation*}
     \norm{G_d}_{\psi_2} \le 
     \norm{\hat\rho g_\mathrm{new}}_{\psi_2} + \| \sqrt{1 - \hat\rho^2} \< \zz_\mathrm{new}, \hat\vtheta \>\|_{\psi_2}
     \le \norm{g_\mathrm{new}}_{\psi_2} + \| \< \zz_\mathrm{new}, \hat\vtheta \> \|_{\psi_2} \lesssim K,
\end{equation*}
which implies $G_d = O_\P(1)$. 


\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{thm:high-imb_high}. High signal:}
If $a < b + c$, then we have $\hat\rho = 1 - o_\P(1)$ by \cref{lem:rho_hat}\ref{lem:rho_hat(a)}. Therefore, according to \cref{lem:beta0_asymp}\ref{lem:beta0_asymp(a)}, for all $\tau_d \ge 1$, we have
\begin{align*}
        \hat\rho \norm{\vmu}_2 + \hat\beta_0
        & = \left(2 - \frac{2}{\tau_d + 1}\right) \hat\rho\norm{\vmu}_2 
        - \frac{1}{\tau_d + 1} \wt O_\P(d^{a-\frac{b}{2}-c} \vee 1) 
        + \wt O_\P(1) \\
        & \ge  d^{b/2}\bigl(1 + o_{\P}(1)\bigr)
        - \wt O_\P(d^{a-\frac{b}{2}-c} \vee 1)  \\
        & \overset{\mathmakebox[0pt][c]{\text{(i)}}}{=} d^{b/2}\bigl(1 + o_{\P}(1)\bigr),
        \qquad  \lim_{d \to \infty} d^{b/2} = +\infty,
\end{align*}
where (i) is because $d^{b/2} \gg d^{a-\frac{b}{2}-c}$, as $d \to \infty$. If $1 \le \tau_d \ll d^{b/2}$, we also have
\begin{align*}
    \hat\rho \norm{\vmu}_2 - \hat\beta_0
    & = \frac{2}{\tau_d + 1} \hat\rho\norm{\vmu}_2 
    + \frac{1}{\tau_d + 1} \wt O_\P(d^{a-\frac{b}{2}-c} \vee 1) + \wt O_\P(1) \\
    & = \frac{2}{\tau_d + 1} d^{b/2} 
    + \frac{1}{\tau_d + 1} \wt O_\P(d^{a-\frac{b}{2}-c} \vee 1) + \wt O_\P(1) \\
    & \overset{\mathmakebox[0pt][c]{\text{(ii)}}}{=} \frac{2}{\tau_d + 1} d^{b/2} \bigl(1 + o_{\P}(1)\bigr) + \wt O_\P(1) \\
    & \ge \tau_d^{-1} d^{b/2} \bigl(1 + o_{\P}(1)\bigr) + \wt O_\P(1),
    \qquad
    \lim_{d \to \infty} \tau_d^{-1} d^{b/2} = +\infty,
\end{align*}
where (ii) is because $(\tau_d + 1)^{-1}d^{b/2} \gg (\tau_d + 1)^{-1}d^{a-\frac{b}{2}-c}$ and $(\tau_d + 1)^{-1}d^{b/2} \gg (\log d)^k$, $\forall\, k \ge 0$, as $d \to \infty$. Under these conditions, both $\hat\rho \norm{\vmu}_2 \pm \hat\beta_0$ diverges to $+\infty$ with high probability, i.e.,
\begin{equation*}
    \lim_{d \to \infty} \P\left(\hat\rho \norm{\vmu}_2 + \hat\beta_0 + G_d > C\right) 
    =
    \lim_{d \to \infty} \P\left(\hat\rho \norm{\vmu}_2 - \hat\beta_0 - G_d > C\right) 
    = 1,
    \qquad
    \forall\, C \in \R.
\end{equation*}
% Conditioning on $\zz_\mathrm{new} = \bz_h$, by Cauchy--Schwarz inequality, we can show that $G_d$ is bounded:
% \begin{equation*}
%     \abs{G_d} \le \sqrt{ \abs{\left\< \bz_h, \frac{\vmu}{\norm{\vmu}_2} \right\>}^2 
%     +
%     \abs{ \< \bz_h, \hat\vtheta \>}^2 
%     }
%     \le \sqrt{2}\norm{\bz_h}_2.
% \end{equation*}
% Since $\zz_\mathrm{new} \indep (\hat\rho, \hat\vtheta, \hat\beta_0)$, then for each $\bz_h$, we have
% \begin{equation*}
%     \lim_{d \to \infty} \P\left(\hat\rho \norm{\vmu}_2 + \hat\beta_0 + G_d \le 0 \,\big|\, \zz_\mathrm{new} = \bz_h \right) 
%     =
%     \lim_{d \to \infty} \P\left(\hat\rho \norm{\vmu}_2 - \hat\beta_0 - G_d \le 0 \,\big|\, \zz_\mathrm{new} = \bz_h \right) 
%     = 0.
% \end{equation*}
% Finally, taking expection on both sides, by bounded convergence theorem, we obtain
Hence
\begin{equation*}
    \Err_+ = o(1), \qquad \Err_- = o(1).
\end{equation*}
This concludes the proof for high signal regime.


\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{thm:high-imb_moderate}. Moderate signal:}
If $b + c < a < 2b + c$, then $\hat\rho = 2d^{(b-a+c)/2}\bigl(1 + o_\P(1)\bigr)$ by \cref{lem:rho_hat}\ref{lem:rho_hat(b)}. Therefore, according to \cref{lem:beta0_asymp}\ref{lem:beta0_asymp(b)}, if $\tau_d \gg d^{a-b-c}$, then
\begin{align*}
        \hat\rho \norm{\vmu}_2 + \hat\beta_0 
        & = \left(2 - \frac{2}{\tau_d + 1}\right) \hat\rho\norm{\vmu}_2 
        - \frac{1}{\tau_d + 1}\sqrt{\frac{d}{\pi n}}\bigl(1 + o_{\P}(1)\bigr) 
        + \wt O_\P(1)  \\
        & = 4 d^{(2b-a+c)/2}  \bigl(1 + o_{\P}(1)\bigr) - \tau_d^{-1} d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr) + \wt O_\P(1) \\
        & \overset{\mathmakebox[0pt][c]{\text{(iii)}}}{=} 4 d^{(2b-a+c)/2}  \bigl(1 + o_{\P}(1)\bigr),
        \qquad
    \lim_{d \to \infty} d^{(2b-a+c)/2} = +\infty,
\end{align*}
where (iii) is because $d^{(2b-a+c)/2} \gg \tau_d^{-1} d^{(a-c)/2}$ and $d^{(2b-a+c)/2} \gg (\log d)^k$, $\forall\, k \ge 0$, as $d \to \infty$. If $1 \le \tau_d \ll d^{(a-c)/2}$, we also have
\begin{align*}
        \hat\rho \norm{\vmu}_2 - \hat\beta_0 
        & = \frac{2}{\tau_d + 1} \hat\rho\norm{\vmu}_2 
        + \frac{1}{\tau_d + 1}\sqrt{\frac{d}{\pi n}}\bigl(1 + o_{\P}(1)\bigr) 
        + \wt O_\P(1)  \\
        & = \frac{4}{\tau_d + 1} d^{(2b-a+c)/2} \bigl(1 + o_{\P}(1)\bigr) + \frac{1}{\tau_d + 1} d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr) + \wt O_\P(1) \\
        & \overset{\mathmakebox[0pt][c]{\text{(iv)}}}{=} \frac{1}{\tau_d + 1} d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr) + \wt O_\P(1) \\
        & \ge \frac12 \tau_d^{-1} d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr) + \wt O_\P(1),
        \qquad 
        \lim_{d \to \infty} \tau_d^{-1} d^{(a-c)/2} = +\infty,
\end{align*}
where (iv) is from $(\tau_d + 1)^{-1} d^{(2b-a+c)/2} \ll (\tau_d + 1)^{-1} d^{(a-c)/2}$. Under these conditions on $\tau_d$, both $\hat\rho \norm{\vmu}_2 \pm \hat\beta_0 $ diverges to $+\infty$ with high probability. Using the same approach, we can show that
\begin{equation*}
    \Err_+ = o(1), \qquad \Err_- = o(1).
\end{equation*}

\vspace{0.5\baselineskip}
\noindent
Now suppose $\tau_d \asymp 1$, then again $\hat\rho \norm{\vmu}_2 - \hat\beta_0 \to + \infty$ and hence $\Err_- = o_\P(1)$ still holds. However,
\begin{align*}
        \hat\rho \norm{\vmu}_2 + \hat\beta_0 
        & = \left(2 - \frac{2}{\tau_d + 1}\right) \hat\rho\norm{\vmu}_2 
        - \frac{1}{\tau_d + 1}\sqrt{\frac{d}{\pi n}}\bigl(1 + o_{\P}(1)\bigr) 
        + \wt O_\P(1)  \\
        & \le 2 d^{(2b-a+c)/2} \bigl(1 + o_{\P}(1)\bigr) - C d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr) + \wt O_\P(1), \\
        & \overset{\mathmakebox[0pt][c]{\text{(v)}}}{=} - C d^{(a-c)/2} \bigl(1 + o_{\P}(1)\bigr),
        \qquad 
        \lim_{d \to \infty} -d^{(a-c)/2} = -\infty,
\end{align*}
where (v) is because $d^{(2b-a+c)/2} \ll d^{(a-c)/2}$, and $C \in (0, \infty)$ is an absolute constant. As the result, $- \hat\rho \norm{\vmu}_2 - \hat\beta_0 $ diverges to $+\infty$ with high probability. Using the same approach, we have
\begin{equation*}
    \Err_+ = 1 - o(1).
\end{equation*}
This concludes the proof for moderate signal regime.




\vspace{0.5\baselineskip}
\noindent
\textbf{\ref{thm:high-imb_low}. Low signal:}
If $a > 2b + c$, then $\hat\rho \norm{\vmu}_2 = o_\P(1) > 0$ by \cref{lem:rho_hat}\ref{lem:rho_hat(b)}. Therefore,
\begin{align*}
        \Err_+ + \Err_- & 
        = \P\left( \hat\rho \norm{\vmu}_2 + \hat\beta_0 + G_d \le 0 \right) 
        + \P\left( \hat\rho \norm{\vmu}_2 - \hat\beta_0 - G_d < 0 \right) \\
        & = 
        1 - \P\left( - \hat\rho \norm{\vmu}_2 \le \hat\beta_0 + G_d < \hat\rho \norm{\vmu}_2 \right)
        \\
        % & = \P\left( o_\P(1) + \hat\beta_0 + G_d \le 0 \right) 
        % + \P\left( o_\P(1) + \hat\beta_0 + G_d > 0 \right) \\
        % & = \P\left( \hat\beta_0 + G_d \le 0 \right) 
        % + \P\left( \hat\beta_0 + G_d > 0 \right) + o_\P(1) \\
        & = 1 - o(1).
\end{align*}
Hence, we have $\Err_\mathrm{b} \ge \frac12 - o(1)$. This concludes the proof for low signal regime.


Finally, we complete the proof of \cref{thm:main_high-imbal}.
\end{proof}


% \newpage
% \noindent
% Let $\vtheta = \bP_{\vmu}^\perp \wt\zz / \| \bP_{\vmu}^\perp \wt\zz \|_2$, then
% \begin{equation*}
%     \langle \zz_i, \vtheta \rangle
%     = \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2}\langle \zz_i, \bP_{\vmu}^\perp \wt\zz \rangle
%     = \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \biggl(  \langle \zz_i, \wt \zz \rangle - \frac1{\norm{\vmu}_2^2 } \langle \zz_i, \vmu \rangle \langle \wt\zz, \vmu \rangle \biggr).
% \end{equation*}
% To bound $\langle \zz_i, \vtheta \rangle$, we use the following lemma.
% \begin{lem} \label{lem:zi-theta} \mbox{}
%     \begin{enumerate}
%         \item[(a)] $\displaystyle \| \bP_{\vmu}^\perp \wt\zz \|_2 = \frac12 \sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big)$.
%         \item[(b)] $\displaystyle \langle \zz_i, \wt \zz \rangle = \ind_{y_i = +1}\frac{d}{2\pi n}\big(1 + o_\P(1) \big) + O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr)$.
%         \item[(c)] $\displaystyle \frac1{\norm{\vmu}_2^2 } \langle \zz_i, \vmu \rangle \langle \wt\zz, \vmu \rangle = O_\P\biggl( \frac{1}{\sqrt{\pi n}} \biggr)$.
%     \end{enumerate}
% \end{lem}
% \begin{proof}
%     \begin{enumerate}
%         \item[(a)] Recall $\wt\zz$ and $\alpha_d$ defined in Eq.~\eqref{eq:alpha_d}. 
        
%         Denote $\wt G :=  2 ( n_+^{-1} + n_-^{-1} )^{-1/2} \langle \wt\zz, \vmu/\norm{\vmu} \rangle  \sim \normal(0, 1)$, then
%         \begin{equation*}
%             \begin{aligned}
%                 \| \bP_{\vmu}^\perp \wt\zz \|^2 & =  \|\wt\zz\|^2 - \norm{\left\langle \wt\zz, \frac{\vmu}{\norm{\vmu}} \right\rangle}^2 
%                  = \frac14 \left( \frac{1}{n_+} + \frac{1}{n_-} \right) \left(  \norm{\zz}^2 - \wt G^2 \right) \\
%                 & = \frac{1}{4\pi n}\bigl(1 + o(1)\bigr) \left( d(1 + o_\P(1)) - \wt G^2 \right)
%                 = \frac{d}{4\pi n}\big(1 + o_\P(1) \big).
%             \end{aligned}
%         \end{equation*} 
        
%         \item[(b)] Let
%         \begin{equation*}
%             \begin{aligned}
%                 \wt\zz_+ := \bar \zz_{-} + \frac{1}{n_+}\sum_{j \not= i, y_j = +1} \zz_j,
%                 \qquad & \alpha_{+d} := \sqrt{ \frac{1}{n_-} + \frac{n_+ - 1}{n_+^2} }
%                 = \frac{1}{\sqrt{\pi n}}\bigl(1 + o(1)\bigr), \\
%                 \wt\zz_- := \bar \zz_{+} + \frac{1}{n_-}\sum_{j \not= i, y_j = -1} \zz_j,
%                 \qquad & \alpha_{-d} := \sqrt{ \frac{1}{n_+} + \frac{n_- - 1}{n_-^2} }
%                 = \frac{1}{\sqrt{\pi n}}\bigl(1 + o(1)\bigr). \\
%             \end{aligned}
%         \end{equation*}
%         Again, by Lemma \ref{lem:subG}\ref{lem:subG-b}, $\zz_i$, $\wt\zz_{0+} := \alpha_{+d}^{-1} \wt\zz_+$, and $\wt\zz_{0-} := \alpha_{-d}^{-1} \wt\zz_-$ are all isotropic mean-zero sub-gaussian, each with independent coordinates. According to Lemma \ref{lem:subG_concentrate}\ref{lem:subG-Hanson-Wright-II}\ref{lem:subG-Bernstein},
%         \begin{equation*}
%             \norm{\zz_i}_2^2 = d\bigl(1 + o_\P(1)\bigr),
%             \qquad \langle \zz_i, \wt\zz_{0+} \rangle = O_\P(\sqrt{d}),
%             \qquad \langle \zz_i, \wt\zz_{0-} \rangle = O_\P(\sqrt{d}).
%         \end{equation*}
%         Therefore, if $y_i = +1$,
%         \begin{equation*}
%             \begin{aligned}
%                 \langle \zz_i, \wt \zz \rangle  & =   \frac{1}{2n_+}\norm{\zz_i}_2^2 + \frac12 \langle \zz_i,  \wt\zz_+   \rangle 
%                 = \frac{1}{2n_+}\norm{\zz_i}_2^2 + \frac{ \alpha_{+d}}2 \langle \zz_i,  \wt\zz_{0+}  \rangle \\
%                 & = \frac{d}{2\pi n}\bigl(1 + o_{\P}(1)\bigr) + \frac{1}{2 \sqrt{\pi n} }\bigl(1 + o(1)\bigr)  \cdot O_{\P}(\sqrt{d}) \\
%                 & = \frac{d}{2\pi n}\bigl(1 + o_{\P}(1)\bigr) + O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr).
%             \end{aligned}
%         \end{equation*}
%         Similarly, if $y_i = -1$,
%         \begin{equation*}
%             \begin{aligned}
%                 \langle \zz_i, \wt \zz \rangle  & =   \frac{1}{2n_-}\norm{\zz_i}_2^2 + \frac12 \langle \zz_i,  \wt\zz_-   \rangle 
%                 = \frac{1}{2n_-}\norm{\zz_i}_2^2 + \frac{ \alpha_{-d}}2 \langle \zz_i,  \wt\zz_{0-}  \rangle \\
%                 & = \frac{d}{2n}\bigl(1 + o_{\P}(1)\bigr) + \frac{1}{2 \sqrt{\pi n} }\bigl(1 + o(1)\bigr)  \cdot O_{\P}(\sqrt{d}) \\
%                 & = O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr),
%             \end{aligned}
%         \end{equation*}
%         where 
%         \[ \frac{d}{n} = d^{-c} \lesssim \sqrt{\frac{d}{\pi n}} = d^{(a-c)/2}  \]
        
%         \item[(c)] According to Lemma \ref{lem:subG}, both $g_i = \langle \zz_i, \vmu/\norm{\vmu}_2 \rangle$ and $\wt g_0 := \alpha_d^{-1} \langle \wt\zz, \vmu/\norm{\vmu}_2 \rangle$ are sub-gaussian with $\norm{g_i}_{\psi_2} \le CK$, $\|\wt g_0\|_{\psi_2} \le CK$, where $C$ is an absolute constant. Then
%         \begin{equation*}
%             \frac1{\norm{\vmu}_2^2 } \langle \zz_i, \vmu \rangle \langle \wt\zz, \vmu \rangle
%             = \left\langle \zz_i, \frac{\vmu}{\norm{\vmu}_2} \right\rangle
%               \left\langle \wt\zz, \frac{\vmu}{\norm{\vmu}_2} \right\rangle
%             = \alpha_d (g_i \wt g_0)
%             = O_\P(\alpha_d)
%             = O_\P\biggl( \frac{1}{\sqrt{\pi n}} \biggr).
%         \end{equation*}
%     \end{enumerate}
% \end{proof}
% Therefore, by Lemma \ref{lem:zi-theta},
% \begin{equation*}
%     \begin{aligned}
%         \langle \zz_i, \vtheta \rangle
%     & = \frac{1}{\| \bP_{\vmu}^\perp \wt\zz \|_2} \biggl(  \langle \zz_i, \wt \zz \rangle - \frac1{\norm{\vmu}_2^2 } \langle \zz_i, \vmu \rangle \langle \wt\zz, \vmu \rangle \biggr) \\
%     & = 2 \sqrt{\frac{\pi n}{d}}\big(1 + o_\P(1) \big)\biggl( \ind_{y_i = +1}\frac{d}{2\pi n}\big(1 + o_\P(1) \big) + O_\P\biggl( \sqrt{\frac{d}{\pi n}} \biggr) + O_\P\biggl( \frac{1}{\sqrt{\pi n}} \biggr) \biggr) \\
%     & = \ind_{y_i = +1}\sqrt{\frac{d}{\pi n}}\big(1 + o_\P(1) \big) + O_{\P}(1).
%     \end{aligned}
% \end{equation*}


