\section{Experiment details}
\label{append_sec:exp}

\subsection{Experiment setup and details}

We present the details of our experiments, including the computational configurations, information about the datasets, and the pretrained neural networks used in our study.

\paragraph{Optimization.} We used the functions \texttt{linear\_model.LogisticRegression} and \texttt{svm.SVC} from Python module \texttt{sklearn} to solve logistic regression \cref{eq:logistic} and SVM \cref{eq:SVM-0} (more precisely, \cref{eq:SVM-1} parametrization with $\tau = 1$). For logistic regression, we used the limited-memory BFGS (L-BFGS) solver, with maximum number of iterations $10^6$.\footnote{If logistic regression is far from converging after the maximum number of iterations is reached, we would add a small explicit regularizer as in \cref{eq:logistic-l2}. In practice, the parameter $\mathtt{C} = \lambda^{-1}$ in \texttt{linear\_model.LogisticRegression} can be chosen as $10^6 \sim 10^8$.} 
For SVM, we set the default value of cost parameter $\mathtt{C} = 1$.\footnote{Note that there is no hard-margin solver available in \texttt{sklearn} and \texttt{svm.SVC} is a soft-margin version. One may set $\mathtt{C}$ large enough, but usually a larger $\mathtt{C}$ will lead to longer running time. To handle this issue, (for separable data) we run \texttt{svm.SVC} with $\mathtt{C}$ increases from 1, until the training error attains zero.} Tolerance for both are set to be $10^{-8}$.

As discussed in \cref{subsec:LR_vs_SVM}, logistic regression and SVM are ``equivalent'' on separable dataset. Indeed, theoretically and empirically, there are advantages and disadvantages to both algorithms, summarized in \cref{tab:LR_vs_SVM}. In particular, SVM is preferred for theoretical analysis and precise 2-GMM simulation, while logistic regression is preferred for large scale real data analysis.
\begin{table}[h!]
% \renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{rll}
    \hline
                 & \textbf{Pros} & \textbf{Cons} \\
    \hline
    \multirow{2}{*}{Logistic regression \eqref{eq:logistic}} 
    & robust to near-separability   & infinite-norm solution \\
    & computationally efficient       & slow convergence \\
    \hline
    \multirow{2}{*}{SVM \eqref{eq:SVM-0}}    
    & well-defined solution     & sensitive to outliers \\
    & support vectors available & quadratic programming \\
    \hline
\end{tabular}
\caption{Comparison of empirical behaviors of logistic regression and SVM on separable data.}
\label{tab:LR_vs_SVM}
\end{table}

% Note that the theoretical connection between SVM and logistic regression in \cite{rosset2003margin, Soudry_implicit_bias} does not include a intercept $\beta_0$. However, our extensive simulations and experiments reveals that the two remain equivalent numerically in the presence of intercept.

\paragraph{Datasets.} We provide the details of real data used in our study, including the source, size, and the preprocessing applied.
\begin{itemize}
    \item \textbf{IFNB} \cite{ifnb}: 
    single-cell RNA-seq dataset of peripheral blood mononuclear cells treated with interferon-$\beta$, which has $n=7,451$ cells, $d=2,000$ genes, and $K=13$ categories for cells. The original dataset is available from R package \texttt{SeuratData} (\url{https://github.com/satijalab/seurat-data}, version 0.2.2.9001) under the name \texttt{ifnb}. The data were preprocessed, normalized, and scaled by following the standard procedures by R package \texttt{Seurat} using functions \texttt{CreateSeuratObject}, \texttt{NormalizeData} and \texttt{ScaleData}.
    
    \item \textbf{CIFAR-10} \cite{KrizhevskyCIFAR102009}: the original dataset consists of 60,000 color images of size $32 \times 32$ in $K=10$ classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. It is available at \url{https://www.cs.toronto.edu/~kriz/cifar.html}. We followed the simple data augmentation in \cite{resnet, cao2019learning} for the training images: 4 pixels are padded on each side, and a $32 \times 32$ crop is randomly sampled from the padded image or its horizontal flip. Normalization is applied for both training and test images.
    
    \item \textbf{IMDb} \cite{IMDB}: the dataset consists of 50,000 movie reviews for binary sentiment classification ($K=2$), with the positive and negative reviews evenly distributed. There are 25,000 training texts and 25,000 test texts. The data can be found at \url{https://huggingface.co/datasets/stanfordnlp/imdb}. The maximum length in number of tokens for inputs was set as 512.
\end{itemize}

\paragraph{Pretrained models.} We downloaded and used pretrained models from Huggingface.
\begin{itemize}
    \item \textbf{ResNet-18} \cite{resnet}: 18-layer, 512-dim, 11.2M parameters, convolutional neural network (CNN), pretrained on CIFAR-10 training set (50,000 images). The pretrained model is downloaded from \url{https://huggingface.co/edadaltocg/resnet18_cifar10}. Notice that for extracting features, we manually removed the last fully-connected layer.
    \item \textbf{BERT} \cite{BERT}: 12-layer, 12-head, 768-dim, 110M parameters, encoder-only transformer, masked prediction, with absolute positional encoding at the input layer, pretrained on BooksCorpus (800M words) and English Wikipedia (2,500M words). The pretrained model is downloaded from \url{https://huggingface.co/google-bert/bert-base-uncased}.

    We also used a fine-tuned version of BERT (same structure as above) on IMDb dataset, which can be found at \url{https://huggingface.co/fabriceyhc/bert-base-uncased-imdb}.
\end{itemize}


\paragraph{Data splitting} For GMM simulated data and IFNB single-cell data, we split the whole dataset into training and test sets in equal proportions. For CIFAR-10 image data and IMDb movie review data, notice that we used the ResNet-18 and BERT model which are pretrained/fine-tuned on the training set of CIFAR-10 and IMDb, respectively. To avoid reusing the data when training the last fully-connected layer (i.e., logistic regression), we split the test set of CIFAR-10 and IMDb into a ``training subset'' and a ``test subset'' in equal proportions. We used this ``training subset'' for logistic regression training and ``test subset'' for evaluation.


% {We need a subsection ``experiment setup'' to discuss how we solving optimization problems: what python packages we used, what is the running time, we hyperparameter we chose, etc. If there are real data, explain the source of dataset, dataset size, what preprocessing is done, basic feature statistics, etc.}

% {Also be clear that we leveraged the connection between max-margin solution and ridgeless logistic regression, be clear about hyperparameter choice.}

\subsection{GMM simulation}

Figures~\ref{fig:GMM_main} and \ref{fig:Err_pi}---\ref{fig:reliability_GMM} are all generated from 2-GMM simulations. By rotational invariance, we may take $\bmu = (\mu, 0, \ldots, 0)^\top \in \R^d$ for some $\mu > 0$. Both minority and majority test errors are calculated on an independent balanced test set, to ensure the accuracy of estimating $\Err_+$.

\subsubsection{Rebalancing margin}

For \textbf{(ii) high imbalance regime}, we provide a simulation study by generating data from a 2-GMM model. More precisely, given $a, b, c > 0$, let
\begin{equation}\label{eq:abc_mod}
    \pi = C_\pi d^{-a},  \quad  \|\bmu\|_2^2 = C_{\mu} d^b,  \quad  n = C_n d^{c+1},
\end{equation}
for some fixed constant $C_\pi = 1, C_\mu = 0.75, C_n = 1$, where $\bmu = (\mu, 0, \ldots, 0)^\top \in \R^d$ and $\mu = \sqrt{C_{\mu} d^b}$. In the experiment, we fix $b = 0.3$, $c = 0.1$, and $d =2 000$ large enough to ensure data separability, while we change the value of $a$. For each tuple $(a, b, c)$, we compute the parameters $\pi, \bmu, n$ as per \cref{eq:abc_mod}, and generate training sets and test sets according to 2-GMM \cref{model}. 
% To reveal the importance of margin rebalancing, we choose $\tau = \tau_d = d^r$ for different values of $r \ge 0$. Then for each $\tau$, we train a margin-rebalanced SVM \cref{eq:SVM-m-reb}, and evaluate test errors $\hat\Err_+$, $\hat\Err_-$, and $\hat\Err_\mathrm{b}$ on the test set.

\subsubsection{Calibration}


The confidence reliability diagram \cref{fig:reliability_GMM} is created by partitioning $(0, 1]$ into $M$ interval bins $I_m := (\frac{m-1}{M}, \frac{m}{M}]$, $m \in [M]$, and calculating the average accuracy of each bin. Let $\hat p(\xx_i)$ be the confidence of the $i$-th test point ($i \in [n]$), and denote $\mathcal{B}_m := \{ i \in [n]: \hat p(\xx_i) \in I_m \}$ be the set of indices whose confidence falls into each bin. Then by our definition of confidence and the symmetry of binary classification, the accuracy and confidence of $\mathcal{B}_m$ can be estimated by
\begin{equation*}
    \hat{\mathrm{acc}}(\mathcal{B}_m) = \frac{1}{\abs{\mathcal{B}_m}} \sum_{i \in \mathcal{B}_m} \ind\{ y_i = 1 \},
    \qquad
    \hat{\mathrm{conf}}(\mathcal{B}_m) = \frac{1}{\abs{\mathcal{B}_m}} \sum_{i \in \mathcal{B}_m} \hat p(\xx_i).
\end{equation*}
We can also obtain a binning-based estimator of calibration error \cref{eq:CalErr} by using above quantities:
\begin{equation*}
    \hat{\mathrm{CalErr}} := \sum_{m=1}^M \frac{\abs{\mathcal{B}_m}}{n} \left( \hat{\mathrm{acc}}(\mathcal{B}_m) - \hat{\mathrm{conf}}(\mathcal{B}_m) \right)^2.
\end{equation*}
This is a variant of the prominent estimator called expected calibration error (ECE) \cite{guo2017calibration}.

The confidence reliability diagrams for additional 2-GMM simulations and IMDb movie review dataset are shown in Figures~\ref{fig:reliab_diag_mu=2}---\ref{fig:reliab_imdb}. These plots confirm a similar trend: miscalibration is getting worse when data becomes increasingly imbalanced (i.e., as $\pi$ decreases). 


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{Figs/Calibration_pi=0.05, mu=1, n=1000, d=500.pdf}
%     \caption{
%     Reliability diagram for 2-GMM simulation ($\norm{\bmu}_2=1$, $n=1000$, $d=500$)
%     }
% \end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{Figs/Calibration_pi=0.05,mu=2,n=1000,d=100.pdf}
    \caption{
    Reliability diagram for 2-GMM simulation ($\norm{\bmu}_2=2$, $n=1,\! 000$, $d=100$)
    }\label{fig:reliab_diag_mu=2}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{Figs/Calibration_pi=0.05,mu=0.5,n=1000,d=500.pdf}
    \caption{
    Reliability diagram for 2-GMM simulation ($\norm{\bmu}_2=0.5$, $n=1,\! 000$, $d=500$)
    }\label{fig:reliab_diag_mu=0.5}
\end{figure}
\begin{figure}[t]
    \centering
\includegraphics[width=1\textwidth]{Figs/Calibration_IMDb_BERT110M.pdf}
    \caption{
    Reliability diagram for IMDb dataset preprocessed by BERT base model (110M)
    }\label{fig:reliab_imdb}
\end{figure}

% \ljycom{The plot for single-cell and CIFAR-10 is not good, with non-monotone or reversed trend in $\pi$.}

\subsection{Function plot for the proximal operator $\prox_{\lambda \ell}(x)$}
\label{append_subsec_prox}
Recall that 
\begin{equation*}
    \prox_{\lambda \ell}(x) = \argmin_{t \in \R} \left\{ \ell(t) +  \frac1{2\lambda} (t - x)^2 \right\}.
\end{equation*}
We provide the plot for function $x \mapsto \prox_{\lambda \ell}(x)$, which is the specific form of overfitting effect in logistic regression \cref{eq:logistic} on non-separable data (i.e., $\delta > \delta^*(0)$). The plot is shown in \cref{fig:prox}, where $\ell(t) = \log(1 + e^{-t})$ is the logistic loss, and we choose $\lambda = 1, 5, 100$, and 10,000 for visualization. When $\lambda$ is close to zero, the function $x \mapsto \prox_{\lambda \ell}(x)$ is close to the identity map, which is because $\lim_{\lambda \to 0^+}\prox_{\lambda\ell}(x) = x$ by \cref{lem:prox}\ref{lem:prox(b)}. When $\lambda$ is large, the proximal operator (up to scaling) looks like a smooth approximation of the truncation map $x \mapsto \max\{ \kappa, x\}$ for some $\kappa > 0$. Intuitively, $\prox_{\lambda\ell}(x)$ behaves like minimizing $\ell$ when $\lambda$ is large. Therefore, a large $x$ yields $\prox_{\lambda\ell}(x) \approx x$ since $\ell(x) \approx 0$, and a small $x$ would be ``pushed'' to some $\kappa > 0$, since the logistic loss $\ell(x)$ locally is a smoothing of the hinge-type loss $x \mapsto (a - b x)_+$ for some $a, b > 0$.


According to our proof in \cref{append_sec:nonsep}, the limiting value of $\lambda$ as $n, d \to \infty$, $n/d \to \delta$ (denoted by $\lambda^* (\delta)$) is a decreasing function of the asymptotic aspect ratio $\delta$. Then \cref{fig:prox} graphically illustrates the effect of high-dimensionality on overfitting. When $n/d \to \delta$ is large, then $\lambda^*(\delta)$ is small and ELD $\approx$ TLD, and overfitting is negligible. In particular, this is the case for the classical setting where $d$ is fixed and $\delta = \infty$. When $\delta$ is moderate, the ELD is somewhat shrunken compared to TLD. When $\delta \downarrow \delta^*(0)$, approaching the interpolation threshold, then $\lambda^*(\delta)$ is very large, and the ELD is almost a rectified Gaussian and far away from the TLD.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Figs/prox_lambda.pdf}
    \caption{
    \textbf{Function plot for $x \mapsto \prox_{\lambda \ell}(x)$ under different $\lambda$.} The solid curve represents the function $y = \prox_{\lambda \ell}(x)$ and the dashed line represents the identity map $y = x$.
    }
    \label{fig:prox}
\end{figure}

