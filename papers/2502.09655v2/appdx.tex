
\section{Theoretical Results}

\subsection{Derivation of the backward CKE in Eq.~\ref{eq:backward_CKE_2} from
Eq.~\ref{eq:backward_CKE}\label{subsec:Derivation-of-the-backward-CKE}}

\noindent According to Bayes' rule, we have:
\begin{align}
p\left(x_{t}|x_{T}\right) & =\frac{p\left(x_{T}|x_{t}\right)p\left(x_{t}\right)}{p\left(x_{T}\right)}\label{eq:backward_CKE_deriv_1}\\
 & =\frac{p\left(x_{t}\right)}{p\left(x_{T}\right)}\int p\left(x_{T}|x_{t+1}\right)p\left(x_{t+1}|x_{t}\right)dx_{t+1}\label{eq:backward_CKE_deriv_2}\\
 & =\int\frac{p\left(x_{t}\right)}{p\left(x_{T}\right)}p\left(x_{T}|x_{t+1}\right)p\left(x_{t+1}|x_{t}\right)dx_{t+1}\\
 & =\int p\left(x_{t+1},x_{t}\right)\frac{p\left(x_{T}|x_{t+1}\right)}{p\left(x_{T}\right)}dx_{t+1}\\
 & =\int p\left(x_{t}|x_{t+1}\right)\frac{p\left(x_{T}|x_{t+1}\right)p\left(x_{t+1}\right)}{p\left(x_{T}\right)}dx_{t+1}\\
 & =\int p\left(x_{t}|x_{t+1}\right)p\left(x_{t+1}|x_{T}\right)dx_{t+1}\label{eq:backward_CKE_deriv_6}
\end{align}

Here, $p\left(x_{T}|x_{t}\right)=\int p\left(x_{T}|x_{t+1}\right)p\left(x_{t+1}|x_{t}\right)dx_{t+1}$
(from Eq.~\ref{eq:backward_CKE_deriv_1} to Eq.~\ref{eq:backward_CKE_deriv_2})
is the backward CKE in Eq.~\ref{eq:backward_CKE}. The result in
Eq.~\ref{eq:backward_CKE_deriv_6} is the backward CKE in Eq.~\ref{eq:backward_CKE_2}.

\subsection{Chapman-Kolmogorov equations for bridges\label{subsec:Proof_Chapman-Kolmogorov-equations-for-bridges}}

The CKE for bridges in Eq.~\ref{eq:bridge_CKE_not_learnable} can
be derived from the CKE for conditional Markov process in Eq.~\ref{eq:bridge_CKE}
by choosing $v$ to be $T$. However, deriving the CKE in Eq.~\ref{eq:bridge_CKE_learnable}
from Eq.~\ref{eq:bridge_CKE} is not straightforward as it involves
the integration w.r.t. $dx_{t}$ rather than $dx_{s}$ ($t<s$). It
suggests that we should consider the reverse of the original conditional
Markov process. Since it is another conditional Markov process (conditioned
on $\hat{x}_{0}=y_{A}$), it can be characterized by the following
CKE:
\begin{align}
p\left(\hat{x}_{T}|x_{s},\hat{x}_{0}\right) & =\int p\left(\hat{x}_{T}|x_{t},\hat{x}_{0}\right)p\left(x_{t}|x_{s},\hat{x}_{0}\right)dx_{t}\label{eq:derived_CKE_learnable_1}\\
\Rightarrow\frac{p\left(x_{s}|\hat{x}_{T},\hat{x}_{0}\right)p\left(\hat{x}_{T}|\hat{x}_{0}\right)}{p\left(x_{s}|\hat{x}_{0}\right)} & =\int\frac{p\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)p\left(\hat{x}_{T}|\hat{x}_{0}\right)}{p\left(x_{t}|\hat{x}_{0}\right)}p\left(x_{t}|x_{s},\hat{x}_{0}\right)dx_{t}\label{eq:derived_CKE_learnable_2}\\
\Rightarrow p\left(x_{s}|\hat{x}_{T},\hat{x}_{0}\right) & =\int\frac{p\left(x_{t}|x_{s},\hat{x}_{0}\right)p\left(x_{s}|\hat{x}_{0}\right)}{p\left(x_{t}|\hat{x}_{0}\right)}p\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)dx_{t}\label{eq:derived_CKE_learnable_3}\\
\Rightarrow p\left(x_{s}|\hat{x}_{T},\hat{x}_{0}\right) & =\int p\left(x_{s}|x_{t},\hat{x}_{0}\right)p\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)dx_{t}\label{eq:derived_CKE_learnable_4}
\end{align}
Here, $\hat{x}_{T}\sim p\left(\hat{x}_{T}|\hat{x}_{0}\right)$ with
$p\left(\hat{x}_{T}=y_{B}\right)=p\left(y_{B}|y_{A}\right)$. By writing
Eq.~\ref{eq:derived_CKE_learnable_3} with slightly different notations,
we obtain Eq.~\ref{eq:bridge_CKE_learnable}.

\subsection{Tweedie's formula for bridges\label{subsec:Tweedie's-formula-for-bridges}}

Assume that $x$ is sampled from a Gaussian distribution $p\left(x|y_{A},y_{B}\right)=\Normal\left(\alpha y_{A}+\beta y_{B},\sigma^{2}\mathrm{I}\right)$.
The posterior expectation of $y_{B}$ given $x$ and $y_{A}$ can
be computed as follows:
\begin{equation}
\tilde{y}_{B}=\Expect_{p\left(y_{B}|x,y_{A}\right)}\left[y_{B}\right]=x-\alpha y_{A}+\sigma^{2}\nabla\log p\left(x|y_{A}\right)\label{eq:Tweedie_bridge}
\end{equation}
where $p\left(x|y_{A}\right)=\Expect_{p\left(y_{B}|y_{A}\right)}\left[p\left(x|y_{A},y_{B}\right)\right]$.
We refer to Eq.~\ref{eq:Tweedie_bridge} as Tweedie's formula for
bridges.

We start by representing $\nabla\log p\left(x|y_{A}\right)$ as follows:
\begin{align}
 & \nabla\log p\left(x|y_{A}\right)\\
=\  & \frac{1}{p\left(x|y_{A}\right)}\nabla p\left(x|y_{A}\right)\\
=\  & \frac{1}{p\left(x|y_{A}\right)}\nabla\int p\left(x|y_{A},y_{B}\right)p\left(y_{B}|y_{A}\right)dy_{B}\\
=\  & \frac{1}{p\left(x|y_{A}\right)}\int p\left(y_{B}|y_{A}\right)\nabla p\left(x|y_{A},y_{B}\right)dy_{B}\\
=\  & \int\frac{p\left(y_{B}|y_{A}\right)p\left(x|y_{A},y_{B}\right)}{p\left(x|y_{A}\right)}\nabla\log p\left(x|y_{A},y_{B}\right)dy_{B}\\
=\  & \int p\left(y_{B}|x,y_{A}\right)\left(\frac{\alpha y_{A}+\beta y_{B}-x}{\sigma^{2}}\right)dy_{B}\\
=\  & \frac{\alpha y_{A}+\beta\Expect_{p\left(y_{B}|x,y_{A}\right)}\left[y_{B}\right]-x}{\sigma^{2}}\label{eq:Tweedie_bridge_score_6}
\end{align}
Rearrange Eq.~\ref{eq:Tweedie_bridge_score_6}, we have:
\begin{equation}
\tilde{y}_{B}=\Expect_{p\left(y_{B}|x,y_{A}\right)}\left[y_{B}\right]=\frac{1}{\beta}\left(x-\alpha y_{A}+\sigma^{2}\nabla\log p\left(x|y_{A}\right)\right)\label{eq:Tweedie_yB_approx}
\end{equation}
Since $p\left(x|y_{A},y_{B}\right)=\Normal\left(\alpha y_{A}+\beta y_{B},\sigma^{2}\mathrm{I}\right)$,
$x$ can be represented as $x=\alpha y_{A}+\beta y_{B}+\sigma z$,
which means:
\begin{equation}
y_{B}=\frac{1}{\beta}\left(x-\alpha y_{A}-\sigma z\right)\label{eq:Tweedie_yB_orig}
\end{equation}
Eqs.~\ref{eq:Tweedie_yB_approx}, \ref{eq:Tweedie_yB_orig} suggest
that $-\sigma\nabla\log p\left(x|y_{A}\right)$ is the least square
approximation of $z$. This means $z_{\theta}\left(t,x_{t},x_{0}\right)$
in Eq.~\ref{eq:loss_bridge_2} should equal to $-\sigma\nabla\log p\left(x|x_{0}\right)$.

\subsection{Connection between the CKE framework and other frameworks for bridges\label{subsec:Connection-between-the-CKE-framework}}

\subsubsection{Link to variational inference}

If we assume the generative process is a discrete-time \emph{conditional
Markov process} running from time $0$ to time $T$ with the initial
distribution $p\left(x_{0}|\hat{x}_{0}\right)$ being a Dirac distribution
at $\hat{x}_{0}$ (i.e., $p\left(x_{0}|\hat{x}_{0}\right)=\delta_{\hat{x}_{0}}$),
the generative distribution over all time steps will be given below:
\begin{equation}
p_{\theta}\left(x_{0:T}|\hat{x}_{0}\right)=p\left(x_{0}|\hat{x}_{0}\right)\prod_{t=0}^{T-1}p_{\theta}\left(x_{t+1}|x_{t},\hat{x}_{0}\right)
\end{equation}
Here, $x_{0}$, ..., $x_{T-1}$ are regarded as latent variables and
$x_{T}$ is regarded as an observed variable. The (variational) inference
distribution $q\left(x_{0:T-1}|\hat{x}_{T},\hat{x}_{0}\right)$ can
be factorized as follows:
\begin{align}
 & q\left(x_{0:T-1}|\hat{x}_{T},\hat{x}_{0}\right)\nonumber \\
=\  & q\left(x_{T-1}|\hat{x}_{T},\hat{x}_{0}\right)\prod_{t=0}^{T-2}q\left(x_{t}|x_{t+1},\hat{x}_{T},\hat{x}_{0}\right)\\
=\  & q\left(x_{T-1}|\hat{x}_{T},\hat{x}_{0}\right)\prod_{t=0}^{T-2}\frac{q\left(x_{t+1}|x_{t},\hat{x}_{T},\hat{x}_{0}\right)q\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)}{q\left(x_{t+1}|\hat{x}_{T},\hat{x}_{0}\right)}\\
=\  & q\left(x_{0}|\hat{x}_{T},\hat{x}_{0}\right)\prod_{t=0}^{T-2}q\left(x_{t+1}|x_{t},\hat{x}_{T},\hat{x}_{0}\right)
\end{align}
which characterizes a double conditional Markov process with Dirac
distributions $\delta_{\hat{x}_{0}}$ and $\delta_{\hat{x}_{T}}$
at both ends and the transition kernel $q\left(x_{t+1}|x_{t},\hat{x}_{T},\hat{x}_{0}\right)$.

We can learn $\theta$ by minimizing the negative variational lower
bound below:
\begin{align}
 & -\Expect_{p\left(\hat{x}_{0}\right)p\left(\hat{x}_{T}|\hat{x}_{0}\right)}\left[\text{ELBO}\left(\hat{x}_{T},\hat{x}_{0}\right)\right]\nonumber \\
=\  & \Expect_{p\left(\hat{x}_{0}\right)p\left(\hat{x}_{T}|\hat{x}_{0}\right)}\left[\Expect_{q\left(x_{0:T-1}|\hat{x}_{T},\hat{x}_{0}\right)}\left[-\log\frac{p_{\theta}\left(x_{0:T}|\hat{x}_{0}\right)}{q\left(x_{0:T-1}|\hat{x}_{T},\hat{x}_{0}\right)}\right]\right]\\
=\  & -\log p_{\theta}\left(x_{T}|x_{T-1},\hat{x}_{0}\right)\nonumber \\
 & +\sum_{t=1}^{T-1}D_{\text{KL}}\left(q\left(x_{t+1}|x_{t},\hat{x}_{T},\hat{x}_{0}\right)\|p_{\theta}\left(x_{t+1}|x_{t},\hat{x}_{0}\right)\right)\nonumber \\
 & +D_{\text{KL}}\left(q\left(x_{0}|\hat{x}_{T},\hat{x}_{0}\right)\|p\left(x_{0}|\hat{x}_{0}\right)\right)\label{eq:ELBO_2}
\end{align}
The KL term in Eq.~\ref{eq:ELBO_2} is the discrete-time version
of our loss in Eq.~\ref{eq:loss_bridge_1}.

\subsubsection{Link to score matching}

When the Markov process between $\hat{x}_{0}$, $\hat{x}_{T}$ is
a continuous-time diffusion process, the problem of matching $p_{\theta}\left(x_{s}|x_{t},\hat{x}_{0}\right)$
to $q\left(x_{s}|x_{t},\hat{x}_{T},\hat{x}_{0}\right)$ in Eq.~\ref{eq:loss_bridge_1}
can be reformulated in the differential form as matching $\frac{\partial}{\partial t}p_{\theta}\left(x_{t}|\hat{x}_{0}\right)$
to $\frac{\partial}{\partial t}q\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)$
where $q\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)$ is the marginal
distribution at time $t$ of the diffusion process between $\hat{x}_{0}$,
$\hat{x}_{T}$. Given the connection between $\frac{\partial p}{\partial t}$
and $\nabla p$ via the KBE (Eq.~\ref{eq:KBE_1}), we can instead
match $\nabla p_{\theta}\left(x_{t}|\hat{x}_{0}\right)$ to $\nabla q\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)$,
which is similar to matching $\nabla\log p_{\theta}\left(x_{t}|\hat{x}_{0}\right)$
to $\nabla\log q\left(x_{t}|\hat{x}_{T},\hat{x}_{0}\right)$.

\subsubsection{Link to Doob's $h$-transform}

We consider a slightly different setting for bridges: Instead of starting
a Markov process from a specific initial sample $\hat{x}_{0}=y_{A}$
and ensure that the final distribution $p\left(x_{T}|\hat{x}_{0}\right)$
will satisfy $p\left(x_{T}=y_{B}|\hat{x}_{0}\right)=p\left(y_{B}|y_{A}\right)$,
we start the process from an initial distribution of $x_{0}$ and
force it to hit a predetermined sample $\hat{x}_{T}=y_{B}$ at time
$T$ almost surely. If the initial distribution $p\left(x_{0}\right)$
is chosen such that $p\left(x_{0}=y_{A}\right)=p\left(y_{A}|y_{B}\right)$,
then the two settings are statistically equivalent when all samples
from the two domains $A$, $B$ are counted.

Let $p\left(x_{t}\right)$ be the marginal distribution at time $t$
corresponding to a Markov process starting from the initial distribution
$p\left(x_{0}\right)$. Also assume that $p\left(x_{t}\right)$ has
support over the entire sample space. Then, we have:
\begin{align}
p\left(\hat{x}_{T}\right) & =\int p\left(\hat{x}_{T}|x_{t}\right)p\left(x_{t}\right)dx_{t}\label{eq:Doob_integral_t}
\end{align}
Interestingly, we can define a \emph{new} marginal distribution of
$x_{t}$ as $\tilde{p}\left(x_{t}\right)=\frac{p\left(\hat{x}_{T}|x_{t}\right)p\left(x_{t}\right)}{p\left(\hat{x}_{T}\right)}$,
and if this distribution converges to a Dirac distribution at time
$T$ then, under some mild conditions\footnote{A key condition here is that $\hat{p}\left(x_{t}=y_{B}\right)$ does
not vanish $\forall$ $t$.}, this Dirac distribution should center around $\hat{x}_{T}=y_{B}$.

At time $s\neq t$, Eq.~\ref{eq:Doob_integral_t} becomes: 
\begin{align}
p\left(\hat{x}_{T}\right) & =\int p\left(\hat{x}_{T}|x_{s}\right)p\left(x_{s}\right)dx_{s}\\
 & =\int p\left(\hat{x}_{T}|x_{s}\right)\left(\int p\left(x_{s}|x_{t}\right)p\left(x_{t}\right)dx_{t}\right)dx_{s}\\
 & =\int\int p\left(\hat{x}_{T}|x_{s}\right)p\left(x_{s}|x_{t}\right)p\left(x_{t}\right)dx_{t}dx_{s}\\
 & =\int\left(\int p\left(\hat{x}_{T}|x_{s}\right)p\left(x_{s}|x_{t}\right)dx_{s}\right)p\left(x_{t}\right)dx_{t}\label{eq:Doob_integral_s4}
\end{align}
Since $p\left(\hat{x}_{T}\right)$ in Eq.~\ref{eq:Doob_integral_t}
is the same as in Eq.~\ref{eq:Doob_integral_s4}, the CKE below should
hold for every $s\neq t$:

\begin{align}
p\left(\hat{x}_{T}|x_{t}\right) & =\int p\left(\hat{x}_{T}|x_{s}\right)p\left(x_{s}|x_{t}\right)dx_{s}\label{eq:Doob_CKE_1}\\
 & =\Expect\left[p\left(\hat{x}_{T}|X_{s}\right)|X_{t}=x_{t}\right]\label{eq:Doob_CKE_2}
\end{align}
Here, we focus on the generative setting with $0<t<s$ and rewrite
Eq.~\ref{eq:Doob_CKE_1} as follows:
\begin{equation}
1=\int p\left(x_{s}|x_{t}\right)\frac{p\left(\hat{x}_{T}|x_{s}\right)}{p\left(\hat{x}_{T}|x_{t}\right)}dx_{s}\label{eq:Doob_transition_kernel}
\end{equation}
Eq.~\ref{eq:Doob_transition_kernel} suggests that we can set $p\left(x_{s}|x_{t}\right)\frac{p\left(\hat{x}_{T}|x_{s}\right)}{p\left(\hat{x}_{T}|x_{t}\right)}$
to be a distribution over $x_{s}$. Let us denote $\tilde{p}\left(x_{s}|x_{t}\right)=p\left(x_{s}|x_{t}\right)\frac{p\left(\hat{x}_{T}|x_{s}\right)}{p\left(\hat{x}_{T}|x_{t}\right)}$,
then $\tilde{p}\left(x_{s}|x_{t}\right)$ can be viewed as the transition
kernel of another Markov process derived from the original Markov
process. Interestingly, $\tilde{p}\left(x_{t}\right)$ is the marginal
distribution at time $t$ of this process, and since $\tilde{p}\left(\hat{x}_{T}\right)$
is a Dirac distribution at $\hat{x}_{T}$, this process converges
to $\hat{x}_{T}=y_{B}$ almost surely. Please refer to the last part
of this subsection for detail proofs.

It is worth noting that in Eq.~\ref{eq:Doob_integral_t}, the term
$p\left(x_{t}\right)$ is fixed since it is the marginal distribution
of the (predefined) original Markov process while the term $p\left(\hat{x}_{T}|x_{t}\right)$
can vary freely as long as it satisfies Eq.~\ref{eq:Doob_CKE_1}.
Therefore, if we let $h\left(\cdot,\cdot,T,\hat{x}_{T}\right)$ be
any function such that:
\begin{align}
h\left(t,x_{t},T,\hat{x}_{T}\right) & =\int h\left(s,x_{s},T,\hat{x}_{T}\right)p\left(x_{s}|x_{t}\right)dx_{s}\label{eq:Doob_h_CKE_1}\\
 & =\Expect\left[h\left(s,X_{s},T,\hat{x}_{T}\right)|X_{t}=x_{t}\right]\label{eq:Doob_h_CKE_2}
\end{align}
and $h\left(T,x_{T},T,\hat{x}_{T}\right)=\delta_{\hat{x}_{T}}\left(x_{T}\right)$
then by setting $\tilde{p}\left(x_{s}|x_{t}\right)=p\left(x_{s}|x_{t}\right)\frac{h\left(s,x_{s},T,\hat{x}_{T}\right)}{h\left(t,x_{t},T,\hat{x}_{T}\right)}$,
we obtain a new Markov process called \emph{Doob's $h$-transform
process} that converges to $\hat{x}_{T}=y_{B}$ almost surely. This
is the main idea behind Doob's $h$-transform \cite{doob1984classical}.

In the continuous-time setting, Eq.~\ref{eq:Doob_CKE_1} can be written
in the differential form below:
\begin{equation}
\begin{cases}
\mathcal{A}_{t}h\left(t,x_{t},T,\hat{x}_{T}\right)=0\\
h\left(T,x_{T},T,\hat{x}_{T}\right)=\delta_{\hat{x}_{T}}\left(x_{T}\right)
\end{cases}\label{eq:Doob_h_cond}
\end{equation}
where $\mathcal{A}_{t}$ is the generator operator defined as $\mathcal{A}_{t}f\left(t,x_{t}\right)\triangleq\lim_{\Delta t\downarrow0}\frac{\Expect\left[f\left(t+\Delta t,X_{t+\Delta t}\right)|X_{t}=x_{t}\right]-f\left(t,x_{t}\right)}{\Delta t}$.
The above equation is in fact a KBE. When the original Markov process
is a continuous-time diffusion process described by the SDE $dX_{t}=\mu\left(t,X_{t}\right)dt+\sigma\left(t\right)dW_{t}$,
given any real-valued function $f\left(t,x\right)$, $\mathcal{A}_{t}f\left(t,x\right)$
can be represented as follows:
\begin{align*}
\mathcal{A}_{t}f & =\frac{\partial f}{\partial t}+\nabla f\cdot\mu+\frac{\sigma^{2}}{2}\Delta f\\
 & =\frac{\partial f}{\partial t}+\mathcal{G}f
\end{align*}
The generator $\mathcal{A}_{t}^{h}$ of the \emph{Doob's $h$-transform
process} can be derived from $\mathcal{A}_{t}$ as follows:
\begin{align*}
\mathcal{A}_{t}^{h}f & =\frac{1}{h}\mathcal{A}_{t}\left(fh\right)
\end{align*}
By leveraging the fact that $\mathcal{A}_{t}h=0$ in Eq.~\ref{eq:Doob_h_cond},
$\mathcal{A}_{t}^{h}f$ can be expressed as follows:
\[
\mathcal{A}_{t}^{h}f=\frac{\partial f}{\partial t}+\nabla f\cdot\left(\mu+\sigma^{2}\nabla\log h\right)+\frac{\sigma^{2}}{2}\Delta f
\]
It implies that this diffusion process is described by the SDE:
\[
dX_{t}=\left(\mu\left(t,X_{t}\right)+\sigma^{2}\nabla\log h\left(t,X_{t},T,\hat{x}_{T}\right)\right)dt+\sigma\left(t\right)dW_{t}
\]


\paragraph{Proofs for some properties of $\tilde{p}\left(x_{s}|x_{t}\right)$
and $\tilde{p}\left(x_{t}\right)$}

For any times $0\leq t<r<s$, we have:
\begin{align}
 & \int\tilde{p}\left(x_{s}|x_{r}\right)\tilde{p}\left(x_{r}|x_{t}\right)dx_{r}\nonumber \\
=\  & \int p\left(x_{s}|x_{r}\right)\frac{p\left(\hat{x}_{T}|x_{s}\right)}{\cancel{p\left(\hat{x}_{T}|x_{r}\right)}}p\left(x_{r}|x_{t}\right)\frac{\cancel{p\left(\hat{x}_{T}|x_{r}\right)}}{p\left(\hat{x}_{T}|x_{t}\right)}dx_{r}\\
=\  & \frac{p\left(\hat{x}_{T}|x_{s}\right)}{p\left(\hat{x}_{T}|x_{t}\right)}\int p\left(x_{s}|x_{r}\right)p\left(x_{r}|x_{t}\right)dx_{r}\\
=\  & \frac{p\left(\hat{x}_{T}|x_{s}\right)}{p\left(\hat{x}_{T}|x_{t}\right)}p\left(x_{s}|x_{t}\right)\\
=\  & \tilde{p}\left(x_{s}|x_{t}\right)
\end{align}
The last equation implies that $\tilde{p}\left(x_{s}|x_{t}\right)$
satisfies the CKE and is the transition probability of a Markov process.
Besides, we have:
\begin{align}
 & \int\tilde{p}\left(x_{s}|x_{t}\right)\tilde{p}\left(x_{t}\right)dx_{t}\nonumber \\
=\  & \int p\left(x_{s}|x_{t}\right)\frac{p\left(\hat{x}_{T}|x_{s}\right)}{\cancel{p\left(\hat{x}_{T}|x_{t}\right)}}\frac{\cancel{p\left(\hat{x}_{T}|x_{t}\right)}p\left(x_{t}\right)}{p\left(\hat{x}_{T}\right)}dx_{t}\\
=\  & \frac{p\left(\hat{x}_{T}|x_{s}\right)}{p\left(\hat{x}_{T}\right)}\int p\left(x_{s}|x_{t}\right)p\left(x_{t}\right)dx_{t}\\
=\  & \frac{p\left(\hat{x}_{T}|x_{s}\right)p\left(x_{s}\right)}{p\left(\hat{x}_{T}\right)}\\
=\  & \tilde{p}\left(x_{s}\right)
\end{align}
which means $\tilde{p}\left(x_{t}\right)$ is the marginal distribution
at time $t$ of the Markov process characterized by $\tilde{p}\left(x_{s}|x_{t}\right)$.

\subsection{Derivation of transitions in Eq.~\ref{eq:ref_forward_mean_1} and
Eq.~\ref{eq:ref_backward_mean_1}}

We consider the case where marginal distributions at timestep $t$
and $s$ (with $t<s$) are $q\left(x_{t}|x_{0},x_{T}\right)=\Normal\left(\alpha_{t}x_{0}+\beta_{t}x_{T},\sigma_{t}^{2}\mathrm{I}\right)$
and $q\left(x_{s}|x_{0},x_{T}\right)=\Normal\left(\alpha_{s}x_{0}+\beta_{s}x_{T},\sigma_{s}^{2}\mathrm{I}\right)$,
respectively. We detail the derivation of our proposed forward transition
distribution, denoted as $q\left(x_{s}|x_{t},x_{0},x_{T}\right)$,
and backward transition distribution, denoted as $q\left(x_{t}|x_{s},x_{0},x_{T}\right)$.

\subsubsection{Derivation of forward transition $q\left(x_{s}|x_{t},x_{0},x_{T}\right)$
in Eq.~\ref{eq:ref_forward_mean_1}}

Recall that the forward CKE, from $t$ to $s$, given two endpoints
$x_{0}$ and $x_{T}$ is given by:
\[
q\left(x_{s}|x_{0},x_{T}\right)=\int q\left(x_{s}|x_{t},x_{0},x_{T}\right)q\left(x_{t}|x_{0},x_{T}\right)dx_{t}
\]

\noindent where $q\left(x_{s}|x_{t},x_{0},x_{T}\right)$ replace $p_{\theta}\left(x_{s}|x_{t},x_{0}\right)$
in Eq.~\ref{eq:bridge_CKE_learnable} in case we align $p_{\theta}\left(x_{s}|x_{t},\hat{x}_{0}\right)$
with $q\left(x_{s}|x_{t},\hat{x}_{T},\hat{x}_{0}\right)$. Following
\cite{bishop2006pattern} (Eq. 2.115), we assume that $q\left(x_{s}|x_{t},x_{0},x_{T}\right)=\mathcal{N}\left(ax_{t}+bx_{0}+cx_{T}+d,\delta_{s,t}^{2}\mathrm{I}\right)$
and we have:
\begin{align}
\Expect\left[\begin{array}{c}
x_{t}|x_{0},x_{T}\\
x_{s}|x_{t},x_{0},x_{T}
\end{array}\right] & =\left(\begin{array}{c}
\alpha_{t}x_{0}+\beta_{t}x_{T}\\
a\left(\alpha_{t}x_{0}+\beta_{t}x_{T}\right)+bx_{0}+cx_{T}+d
\end{array}\right)\\
\text{Cov} & =\left(\begin{array}{cc}
\diag\left(\sigma_{t}^{2}\right) & \diag\left(a\sigma_{t}^{2}\right)\\
\diag\left(a\sigma_{t}^{2}\right) & \diag\left(\delta_{s,t}^{2}+a^{2}\sigma_{t}^{2}\right)
\end{array}\right)
\end{align}

\noindent Compare the mean and covariance with that of $q\left(x_{s}\mid x_{0},x_{T}\right)$,
we have:
\begin{equation}
\begin{cases}
d=0\\
a\left(\alpha_{t}x_{0}+\beta_{t}x_{T}\right)+bx_{0}+cx_{T}=\alpha_{s}x_{0}+\beta_{s}x_{T}\\
\delta_{s,t}^{2}+a^{2}\sigma_{t}^{2}=\sigma_{s}^{2}
\end{cases}
\end{equation}

\noindent 
\begin{equation}
\Rightarrow\begin{cases}
a= & \frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\\
b= & \alpha_{s}-\alpha_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\\
c= & \beta_{s}-\beta_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}
\end{cases}
\end{equation}
\begin{align}
 & q\left(x_{s}|x_{t},x_{0},x_{T}\right)\nonumber \\
=\  & \mathcal{N}\left(\left(\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\right)x_{t}+\left(\alpha_{s}-\alpha_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\right)x_{0}+\left(\beta_{s}-\beta_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\right)x_{T},\delta_{s,t}^{2}\mathrm{I}\right)\\
=\  & \mathcal{N}\left(\alpha_{s}x_{0}+\beta_{s}x_{T}+\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}\frac{\left(x_{t}-\alpha_{t}x_{0}-\beta_{t}x_{T}\right)}{\sigma_{t}},\delta_{s,t}^{2}\mathrm{I}\right)
\end{align}


\subsubsection{Derivation of backward transition $q\left(x_{t}|x_{s},x_{0},x_{T}\right)$
in Eq.~\ref{eq:ref_backward_mean_1}}

Recall that from \ref{eq:Bayes_rule_bridge}, we can derive $q\left(x_{t}|x_{s},x_{0},x_{T}\right)$
from Bayes' rule:
\begin{equation}
q\left(x_{t}|x_{s},x_{0},x_{T}\right)=q\left(x_{s}|x_{t},x_{0},x_{T}\right)\frac{q\left(x_{t}|x_{0},x_{T}\right)}{q\left(x_{s}|x_{0},x_{T}\right)}
\end{equation}

\noindent With: 
\begin{align}
 & q\left(x_{s}\mid x_{t},x_{0},x_{T}\right)\nonumber \\
=\  & \frac{1}{\sqrt{2\pi}\delta_{s,t}}\exp\left(-\frac{1}{2}\frac{\left(x_{s}-\left(\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}x_{t}+\left(\alpha_{s}-\alpha_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\right)x_{0}+\left(\beta_{s}-\beta_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\right)x_{T}\right)\right)^{2}}{\delta_{s,t}^{2}}\right)
\end{align}
\begin{align}
q\left(x_{t}\mid x_{0},x_{T}\right) & =\frac{1}{\sqrt{2\pi}\sigma_{t}}\exp\left(-\frac{1}{2}\frac{\left(x_{t}-\left(\alpha_{t}x_{0}+\beta_{t}x_{T}\right)\right)^{2}}{\sigma_{t}^{2}}\right)\\
q\left(x_{s}\mid x_{0},x_{T}\right) & =\frac{1}{\sqrt{2\pi}\sigma_{s}}\exp\left(-\frac{1}{2}\frac{\left(x_{s}-\left(\alpha_{s}x_{0}+\beta_{s}x_{T}\right)\right)^{2}}{\sigma_{s}^{2}}\right)
\end{align}

\noindent Then we know: 
\begin{align}
 & q\left(x_{t}|x_{s},x_{0},x_{T}\right)\nonumber \\
=\  & \frac{1}{\sqrt{2\pi}\frac{\delta_{s,t}\sigma_{t}}{\sigma_{s}}}\exp\left[-\frac{1}{2}\left(\frac{\left(x_{s}-\left(\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}x_{t}+\left(\alpha_{s}-\alpha_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\right)x_{0}+\left(\beta_{s}-\beta_{t}\frac{\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{t}}\right)x_{T}\right)\right)^{2}}{\delta_{s,t}^{2}}\right.\right.\nonumber \\
+ & \left.\left.\frac{\left(x_{t}-\left(\alpha_{t}x_{0}+\beta_{t}x_{T}\right)\right)^{2}}{\sigma_{t}^{2}}-\frac{\left(x_{s}-\left(\alpha_{s}x_{0}+\beta_{s}x_{T}\right)\right)^{2}}{\sigma_{s}^{2}}\right)\right]\\
=\  & \frac{1}{\sqrt{2\pi}\frac{\delta_{s,t}\sigma_{t}}{\sigma_{s}}}\exp\left(-\frac{\left(x_{t}-\tilde{\mu}_{t}\right)^{2}}{2\left(\frac{\delta_{s,t}\sigma_{t}}{\sigma_{s}}\right)^{2}}\right)
\end{align}

\noindent where
\begin{align}
\tilde{\mu}_{t}= & \left(\frac{\sigma_{t}\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{s}^{2}}\right)x_{s}+\left(\alpha_{t}-\alpha_{s}\frac{\sigma_{t}\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{s}^{2}}\right)x_{0}+\left(\beta_{t}-\beta_{s}\frac{\sigma_{t}\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{s}^{2}}\right)x_{T}\nonumber \\
= & \alpha_{t}x_{0}+\beta_{t}x_{T}+\sigma_{t}\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}\frac{\left(x_{s}-\alpha_{s}x_{0}-\beta_{s}x_{T}\right)}{\sigma_{s}^{2}}
\end{align}


\subsection{Special variants of BDBM\label{subsec:Special-cases-of-BDBM}}

Below, we discuss several important variants of BDBM. These variants
mainly correspond to the variability of $\delta_{s,t}$ within the
interval $[0,\sigma_{s})$.

\subsubsection{$\delta_{s,t}=0$}

If we set $\delta_{s,t}=0$, then $p_{\theta}\left(x_{s}|x_{t},x_{0}\right)$
will become the deterministic mapping $\mu_{\theta}\left(s,t,x_{t},x_{0}\right)$
from $x_{t}$, $x_{0}$ to $x_{s}$. Similarly, $p_{\phi}\left(x_{t}|x_{s},x_{T}\right)$
will become the deterministic mapping $\tilde{\mu}_{\phi}\left(t,s,x_{s},x_{T}\right)$
from $x_{s}$, $x_{T}$ to $x_{t}$. This variant links to the deterministic
mapping from $x_{t}$ ($x$) to $x_{}$ ($x_{t}$) in DDIM \cite{song2020denoising}.

\subsubsection{$\delta_{s,t}=\sqrt{\sigma_{s}^{2}-\sigma_{t}^{2}\frac{\beta_{s}^{2}}{\beta_{t}^{2}}}$}

When $\delta_{s,t}=\sqrt{\sigma_{s}^{2}-\sigma_{t}^{2}\frac{\beta_{s}^{2}}{\beta_{t}^{2}}}$,
$\mu\left(s,t,x_{t},x_{0},x_{T}\right)$ (Eq.~\ref{eq:ref_forward_mean_1})
and $\tilde{\mu}\left(t,s,x_{s},x_{0},x_{T}\right)$ (Eq.~\ref{eq:ref_backward_mean_1})
become:
\begin{align}
\mu\left(s,t,x_{t},x_{0},x_{T}\right) & =\frac{\beta_{s}}{\beta_{t}}x_{t}+\left(\alpha_{s}-\alpha_{t}\frac{\beta_{s}}{\beta_{t}}\right)x_{0}\label{eq:mu_special_1}\\
\tilde{\mu}\left(t,s,x_{s},x_{0},x_{T}\right) & =\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\beta_{s}}{\beta_{t}}x_{s}+\left(\alpha_{t}-\alpha_{s}\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\beta_{s}}{\beta_{t}}\right)x_{0}+\left(\beta_{t}-\beta_{s}\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\beta_{s}}{\beta_{t}}\right)x_{T}\label{eq:mu_tilde_special_1}
\end{align}

Although the term containing $x_{T}$ in Eq.~\ref{eq:mu_special_1}
vanishes, $\mu\left(s,t,x_{t},x_{0},x_{T}\right)$ still depends on
$x_{T}$ since $x_{t}$ depends on $x_{T}$ via Eq.~\ref{eq:ref_bridge_xt_decomp}.
In this case, if $x_{T}$ is modeled directly via $x_{T,\theta}$,
then setting $x_{t}=x_{0}$ at the initial sampling step $t=0$ will
lead to poor generation results since $\mu_{\theta}\left(t,x_{t},x_{0}\right)$
no longer depends on $x_{T,\theta}\left(t,x_{t},x_{0}\right)$. Instead,
we have to set $x_{t}=\alpha_{\epsilon}x_{0}+\beta_{\epsilon}x_{T,\theta}\left(\epsilon,x_{0},x_{0}\right)$
where $\epsilon$ is a small value such that $\beta_{\epsilon}\neq\beta_{0}=0$.
This will ensure that $\mu_{\theta}\left(t,x_{t},x_{0}\right)$ uses
the knowledge from $x_{T,\theta}\left(\epsilon,x_{0},x_{0}\right)$. 

The term containing $x_{T}$ in Eq.~\ref{eq:mu_tilde_special_1}
is unlikely to vanish because otherwise, this will lead to $\frac{\beta_{t}}{\beta_{s}}=\frac{\sigma_{t}}{\sigma_{s}}$
for every time pair $\left(t,s\right)$. This equation does not hold
since if we choose $t=T$ and choose $s$ such that $\beta_{s},\sigma_{s}\neq0$,
we have $\frac{\beta_{T}}{\beta_{s}}=\frac{1}{\beta_{s}}\neq\frac{0}{\sigma_{s}}=\frac{\sigma_{T}}{\sigma_{s}}$.
The term containing $x_{0}$ in Eq.~\ref{eq:mu_tilde_special_1},
by contrast, can vanish if $\alpha_{t}-\alpha_{s}\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\beta_{s}}{\beta_{t}}=0$,
or equivalently, $\sigma_{t}^{2}=k\alpha_{t}\beta_{t}$ where $k>0$
is a constant w.r.t. $t$.

\subsubsection{$\delta_{s,t}=\sqrt{\sigma_{s}^{2}-\sigma_{t}^{2}\frac{\alpha_{s}^{2}}{\alpha_{t}^{2}}}$}

When $\delta_{s,t}=\sqrt{\sigma_{s}^{2}-\sigma_{t}^{2}\frac{\alpha_{s}^{2}}{\alpha_{t}^{2}}}$,
$\mu\left(s,t,x_{t},x_{0},x_{T}\right)$ and $\tilde{\mu}\left(t,s,x_{s},x_{0},x_{T}\right)$
become:
\begin{align}
\mu\left(s,t,x_{t},x_{0},x_{T}\right) & =\frac{\alpha_{s}}{\alpha_{t}}x_{t}+\left(\beta_{s}-\beta_{t}\frac{\alpha_{s}}{\alpha_{t}}\right)x_{T}\label{eq:mu_special_2}\\
\tilde{\mu}\left(t,s,x_{s},x_{0},x_{T}\right) & =\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\alpha_{s}}{\alpha_{t}}x_{s}+\left(\alpha_{t}-\alpha_{s}\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\alpha_{s}}{\alpha_{t}}\right)x_{0}+\left(\beta_{t}-\beta_{s}\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\alpha_{s}}{\alpha_{t}}\right)x_{T}\label{eq:mu_tilde_special_2}
\end{align}
In this case, there will be no problem during sampling with $\mu_{\theta}\left(t,x_{t},x_{0}\right)$
and $\mu_{\phi}\left(s,x_{s},x_{T}\right)$ since they always use
the knowledge from $x_{T,\theta}\left(t,x_{t},x_{0}\right)$ and $x_{0,\phi}\left(s,x_{s},x_{T}\right)$,
respectively. Note that the term containing $x_{T}$ in Eq.~\ref{eq:mu_tilde_special_2}
can vanish if $\sigma_{t}^{2}=k\alpha_{t}\beta_{t}$ ($k>0$) but
this does not affect sampling.

\subsubsection{Brownian Bridge\label{subsec:Brownian-Bridge_settings}}

A Brownian Bridge \cite{LiX0L23} is a special case of the generalized
diffusion bridge in which: 
\begin{align*}
\beta_{t} & =\frac{t}{T}\\
\alpha_{t} & =1-\beta_{t}=1-\frac{t}{T}\\
\sigma_{t}^{2} & =k\alpha_{t}\beta_{t}=k\frac{t}{T}\left(1-\frac{t}{T}\right)
\end{align*}
With this choice of $\alpha_{t}$, $\beta_{t}$, and $\sigma_{t}$,
we can easily prove that $\sigma_{s}^{2}-\sigma_{t}^{2}\frac{\alpha_{s}^{2}}{\alpha_{t}^{2}}\geq0$
for all $t<s$ as follows:
\begin{align*}
 & \sigma_{s}^{2}-\sigma_{t}^{2}\frac{\alpha_{s}^{2}}{\alpha_{t}^{2}}\geq0\\
\Leftrightarrow\  & \frac{\sigma_{s}^{2}}{\sigma_{t}^{2}}\geq\frac{\alpha_{s}^{2}}{\alpha_{t}^{2}}\\
\Leftrightarrow\  & \frac{\beta_{s}}{\beta_{t}}\geq\frac{\alpha_{s}}{\alpha_{t}}\\
\Leftrightarrow\  & \frac{s}{t}\geq\frac{T-s}{T-t}\\
\Leftrightarrow\  & sT\geq tT\\
\Leftrightarrow\  & s\geq t
\end{align*}
Therefore, we can set $\delta_{s,t}=\sqrt{\eta\left(\sigma_{s}^{2}-\sigma_{t}^{2}\frac{\alpha_{s}^{2}}{\alpha_{t}^{2}}\right)}$
with $\eta\in\left[0,1\right]$. 

When $\eta=1$, $\mu\left(s,t,x_{t},x_{0},x_{T}\right)$ and $\tilde{\mu}\left(t,s,x_{s},x_{0},x_{T}\right)$
become:
\begin{align}
\mu\left(s,t,x_{t},x_{0},x_{T}\right) & =\frac{\alpha_{s}}{\alpha_{t}}x_{t}+\left(\beta_{s}-\beta_{t}\frac{\alpha_{s}}{\alpha_{t}}\right)x_{T}\\
 & =\frac{T-s}{T-t}x_{t}+\frac{\left(s-t\right)T}{T-t}x_{T}
\end{align}
\begin{align}
\tilde{\mu}\left(t,s,x_{s},x_{0},x_{T}\right) & =\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\alpha_{s}}{\alpha_{t}}x_{s}+\left(\alpha_{t}-\alpha_{s}\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\frac{\alpha_{s}}{\alpha_{t}}\right)x_{0}\\
 & =\frac{\beta_{t}}{\beta_{s}}x_{s}+\left(\alpha_{t}-\alpha_{s}\frac{\beta_{t}}{\beta_{s}}\right)x_{0}\\
 & =\frac{t}{s}x_{s}+\frac{\left(s-t\right)T}{s}x_{0}
\end{align}


\subsection{Training and sampling algorithms for BDBM\label{subsec:Training-and-sampling_alg}}

In Algos.~\ref{alg:Training-BDBM},\ref{alg:Sampling-forward}, and
\ref{alg:Sampling-backward}, we provide the detailed training, forward
sampling and backward sampling algorithms for our proposed BDBM with
$z_{\varphi}\left(t,x_{t},\left(1-m\right)*x_{0},m*x_{T}\right)$
as the model.

\begin{algorithm}
\begin{algorithmic}[1]
\State {\bfseries Input:} $\alpha_t$, $\beta_t$, $\sigma_t$ as continuously differentiable functions of $t$ satisfying  $\alpha_{0}=\beta_{T}=1$ and $\alpha_{T}=\beta_{0}=\sigma_{0}=\sigma_{T}=0$     
\Repeat     
	\State $t \sim \mathcal{U} \left(0,T \right)$ 
	\State $x_0, x_T \sim p(y_A, y_B)$     
	\State $z \sim \mathcal{N}(0, \mathrm{I})$     
	\State $x_t = \alpha_t x_0 + \beta_t x_T + \sigma_t z$     
	\State $m \sim \mathcal{B}(0.5)$     
	\State Update $\varphi$ by minimizing $\mathcal{L}(\varphi) = \left\Vert z_{\varphi}\left(t,x_{t},\left(1-m\right)*x_{0},m*x_{T}\right)-z\right\Vert _{2}^{2}$     
\Until {converged} 
\end{algorithmic}\caption{Training BDBM \label{alg:Training-BDBM}}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1] 
\State {\bfseries Input:} $\alpha_t$, $\beta_t$, $\sigma_t$, $\delta_{s,t}$, trained $z_{\varphi}\left(t,x_{t},\left(1-m\right)*x_{0},m*x_{T}\right)$, $x_0$
\State $m=0$
\For{$t = 0$ \textbf{to} $T - \Delta t$}
	\State $s = t + \Delta t$
	\State $z_{t|0} = z_{\varphi}\left(t,x_{t},x_{0},0\right)$
    \If{$s = T$}
		\State $\epsilon = 0$
    \Else         
		\State $\epsilon \sim \mathcal{N}(0, \mathrm{I})$
    \EndIf     
	\State $x_{s}= \frac{\beta_{s}}{\beta_{t}} x_{t} + \left(\alpha_{s} - \alpha_{t}\frac{\beta_{s}}{\beta_{t}}\right)x_{0} + \left(\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}-\sigma_{t}\frac{\beta_{s}}{\beta_{t}}\right) z_{t|0} + \delta_{s,t} \epsilon$
\EndFor
\State 
\Return $x_{s}$ 
\end{algorithmic}

\caption{Generating $x_{T}$ given $x_{0}$ (forward)\label{alg:Sampling-forward}}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1] 
\State {\bfseries Input:} $\alpha_t$, $\beta_t$, $\sigma_t$, $\delta_{s,t}$, trained $z_{\varphi}\left(t,x_{t},\left(1-m\right)*x_{0},m*x_{T}\right)$, $x_T$
\State $m=1$
\For{$s = T$ \textbf{to} $\Delta t$}
	\State $t = s - \Delta t$
	\State $z_{s|T} = z_{\varphi}\left(s,x_{s},0,x_{T}\right)$
    \If{$t = 0$}
		\State $\epsilon = 0$
    \Else
        \State $\epsilon \sim \mathcal{N}(0, \mathrm{I})$
    \EndIf
    \State $x_{t} = \frac{\alpha_{t}}{\alpha_{s}}x_{s}+\left(\beta_{t}-\beta_{s}\frac{\alpha_{t}}{\alpha_{s}}\right)x_{T}+\left(\frac{\sigma_{t}\sqrt{\sigma_{s}^{2}-\delta_{s,t}^{2}}}{\sigma_{s}}-\sigma_{s}\frac{\alpha_{t}}{\alpha_{s}}\right) z_{s|T} + \frac{\delta_{s,t}\sigma_{t}}{\sigma_{s}}\epsilon$
\EndFor
\State 
\Return $x_{t}$ 
\end{algorithmic}

\caption{Generating $x_{0}$ given $x_{T}$ (backward)\label{alg:Sampling-backward}}
\end{algorithm}


\section{Additional Experimental Results}

\subsection{Additional qualitative results of BDBM\label{subsec:Additional-qualitative-results}}

\noindent 
\begin{figure}
\begin{centering}
{\resizebox{1\textwidth}{!}{%%
\begin{tabular}{ccc}
\includegraphics{asset/Result/qualitative_result/multiple_samples/e2s_diversity} &  & \includegraphics{asset/Result/qualitative_result/multiple_samples/s2e_diversity}\tabularnewline
\end{tabular}}}
\par\end{centering}
\caption{Qualitative results of BDBM on a test set of Edges$\leftrightarrow$Shoes.\label{fig:Additional-results-EdgesShoes}}
\end{figure}

\noindent 
\begin{figure}
\begin{centering}
{\resizebox{1\textwidth}{!}{%%
\begin{tabular}{ccc}
\includegraphics{asset/Result/qualitative_result/multiple_samples/e2h_diversity} &  & \includegraphics{asset/Result/qualitative_result/multiple_samples/h2e_diversity}\tabularnewline
\end{tabular}}}
\par\end{centering}
\caption{Qualitative results of BDBM on a test set of Edges$\leftrightarrow$Handbag.\label{fig:Additional-results-EdgesHandbags}}
\end{figure}

\noindent 
\begin{figure}
\begin{centering}
{\resizebox{1\textwidth}{!}{%%
\begin{tabular}{ccc}
\includegraphics{asset/Result/qualitative_result/multiple_samples/n2d_diversity} &  & \includegraphics{asset/Result/qualitative_result/multiple_samples/d2n_diversity}\tabularnewline
\end{tabular}}}
\par\end{centering}
\caption{Qualitative results of BDBM on a test set of Night$\leftrightarrow$Day.\label{fig:Additional-results-NightDay}}
\end{figure}

Figs.~\ref{fig:Additional-results-EdgesShoes}, \ref{fig:Additional-results-EdgesHandbags},
and \ref{fig:Additional-results-NightDay} showcase BDBM's generated
samples for both translation directions on the Edges$\leftrightarrow$Shoes,
Edges$\leftrightarrow$Handbag, and Night$\leftrightarrow$Day datasets.
Input samples are taken from a held-out test set not used during training.
The results demonstrate high-quality and diverse outputs, highlighting
BDBM's effectiveness in bidirectional translation.

\subsection{Unidirectional translation from color images to sketch/normal maps\label{subsec:Unidirectional-reverse-translation}}

\begin{table*}
\begin{centering}
\begin{tabular}{cccccccccc}
\toprule 
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Shoes$\rightarrow$Edges$\times64$} & \multicolumn{3}{c}{Handbags$\rightarrow$Edges$\times64$} & \multicolumn{3}{c}{Outdoor$\rightarrow$Normal$\times256$}\tabularnewline
\cmidrule{2-10} 
 & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$ & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$ & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$\tabularnewline
\midrule
\midrule 
BBDM & \textbf{0.66} & \textbf{2.23} & \textbf{0.006} & \uline{1.54} & \textbf{3.11} & \textbf{0.010} & 18.87 & 5.82 & 0.122\tabularnewline
\midrule 
$\text{I}^{2}\text{SB}$ & 1.02 & 2.14 & 0.015 & 1.98 & 3.08 & 0.018 & \uline{11.54} & 5.97 & 0.229\tabularnewline
\midrule 
DDBM & 4.57 & 2.09 & 0.016 & 2.06 & 3.05 & 0.023 & 13.89 & \uline{6.15} & 0.237\tabularnewline
\midrule
\midrule 
BDBM-1 & \uline{0.71} & \uline{2.22} & \uline{0.007} & \textbf{1.51} & \uline{3.10} & \uline{0.011} & \textbf{9.88} & 5.98 & \textbf{0.054}\tabularnewline
\midrule 
BDBM & 0.98 & 2.20 & 0.009 & 1.87 & \uline{3.10} & 0.016 & 11.69 & \textbf{6.27} & \uline{0.069}\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{Results of BDBM and unidirectional baselines for the color-to-sketch
and normal map translation tasks. The best results are highlighted
in bold, while the second-best results are underlined.\label{tab:quantitative_color2sketch}}
\end{table*}

\noindent 
\begin{figure}
\begin{centering}
{\resizebox{0.8\textwidth}{!}{%
\par\end{centering}
\begin{centering}
\begin{tabular}{c|c|c|c|c|c}
\textcolor{blue}{Input} & $\text{I}^{2}\text{SB}$ & BBDM & DDBM & BDBM-1 & BDBM\tabularnewline
 &  &  &  &  & \tabularnewline
\includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2shoes/Input_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2shoes/I2SB_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2shoes/BBDM_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2shoes/DDBM_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2shoes/BDBM1_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2shoes/BDBM_sketch}\tabularnewline
\hline 
 &  &  &  &  & \tabularnewline
\includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2handbags/Input_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2handbags/I2SB_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2handbags/BBDM_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2handbags/DDBM_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2handbags/BDBM1_sketch} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/edges2handbags/BDBM_sketch}\tabularnewline
\hline 
 &  &  &  &  & \tabularnewline
\includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/DIODE/Input_normal} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/DIODE/I2SB_normal} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/DIODE/BBDM_normal} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/DIODE/DDBM_normal} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/DIODE/BDBM1_normal} & \includegraphics[width=0.2\textwidth]{asset/Result/qualitative_result/unidirect/DIODE/BDBM_normal}\tabularnewline
\end{tabular}}}
\par\end{centering}
\caption{Samples generated by BDBM and unidirectional baselines for color-to-sketch/normal
map translation.\label{fig:qualitative_color2sketch}}
\end{figure}

We further compare BDBM with unidirectional baselines BBDM, $\text{I}^{2}\text{SB}$,
DDBM, and BDBM-1 for color-to-sketch/normal map translation. For the
baselines, we trained new models using the same settings as described
in Section~\ref{subsec:Model-and-training} for this translation
direction, while for BDBM, we reused the checkpoints from Section~\ref{subsec:Unidirectional-I2I-Translation}
without retraining. 

As shown in Table~\ref{tab:quantitative_color2sketch}, BDBM performs
comparably to most baselines and even surpasses some on specific datasets,
despite using only \emph{half} of the training resources. Notably,
BDBM significantly outperforms DDBM and BBDM on the Shoes$\rightarrow$Edges
and Outdoor$\rightarrow$Normal datasets, respectively, highlighting
the computational efficiency of BDBM. 

Qualitative differences between methods, however, are less apparent,
as illustrated in Fig.~\ref{fig:qualitative_color2sketch}. This
is likely because sketches and normal maps contain fewer details than
color images, making the metrics \emph{more sensitive} to minor variations
even when the generated images are visually similar to the targets.


\subsection{Continuous-time BDBM vs. Discrete-time BDBM\label{subsec:Continuous-vs-Discrete}}

\begin{table}
\begin{centering}
\begin{tabular}{cccccc}
\toprule 
\multirow{2}{*}{Model} & \multirow{2}{*}{Time type} & \multicolumn{2}{c}{Edges$\leftrightarrow$Shoes$\times64$} & \multicolumn{2}{c}{Edges$\leftrightarrow$Handbags$\times64$}\tabularnewline
\cmidrule{3-6} 
 &  & FID $\downarrow$ & LPIPS $\downarrow$ & FID $\downarrow$ & LPIPS $\downarrow$\tabularnewline
\midrule
\midrule 
\multirow{2}{*}{BDBM-1} & discrete-time & 0.71/1.78 & 0.01/0.07 & 1.51/3.83 & 0.01/0.11\tabularnewline
 & continuous-time & 1.28/1.81 & 0.01/0.03 & 2.45/3.94 & 0.02/0.17\tabularnewline
\midrule
\midrule 
\multirow{2}{*}{BDBM} & discrete-time  & 0.98/1.06 & 0.01/0.02 & 1.87/3.06 & 0.02/0.08\tabularnewline
 & continuous-time & 2.38/2.41 & 0.01/0.04 & 2.88/3.79 & 0.04/0.16\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{Comparison between discrete-time BDBM and continuous-time BDBM.\label{tab:disc_vs_cont_BDBM}}
\end{table}

\begin{figure}
\begin{centering}
\subfloat[Comparison between discrete-time BDBM and continuous-time BDBM on
Edges$\rightarrow$Shoes$\times64$.]{\begin{centering}
\includegraphics[width=1\textwidth]{asset/Result/qualitative_result/disc_cont/e2s}
\par\end{centering}
}
\par\end{centering}
\begin{centering}
\subfloat[Comparison between discrete-time BDBM and continuous-time BDBM on
Edges$\rightarrow$Handbags$\times64$.]{\centering{}\includegraphics[width=1\textwidth]{asset/Result/qualitative_result/disc_cont/e2h}}
\par\end{centering}
\caption{Visualization of discrete-time BDBM and continuous-time BDBM accross
Edges$\rightarrow$Shoes$\times64$ and Edges$\rightarrow$Handbags$\times64$.
The first row shows the input images, the second row presents the
ground truth images, while the third and fourth rows display the outputs
of discrete-time and continuous-time BDBM, respectively. \label{fig:disc_vs_cont_BDBM_vis}}
\end{figure}

In this section, we compare discrete-time BDBM with its continuous-time
counterpart. Both models are evaluated under identical settings, except
that the continuous-time model allows $t$ to take any real value
in $\text{\ensuremath{\left[0,1\right]}}$, while the discrete-time
model restricts $t$ to integer values in $\left[0,1000\right]$. 

As shown in Table~\ref{tab:disc_vs_cont_BDBM} and Fig.~\ref{fig:disc_vs_cont_BDBM_vis},
discrete-time BDBM consistently outperforms its continuous-time counterpart.
The primary reason for this advantage is that discrete-time BDBM only
needs to predict noise for a fixed set of time steps, whereas the
continuous-time model must handle an infinite number of time steps.
As a result, given the same number of training iterations, discrete-time
BDBM can allocate more iterations to refining noise prediction at
each specific time step, leading to more accurate predictions. This
highlights the advantage of the discrete-time model when training
iterations are limited. However, we anticipate that with a sufficient
number of training iterations (as used for training continuous-time
diffusion models \cite{song2020score}), both models would likely
achieve comparable results.


\subsection{More visualization on generated samples by BDBM}

We provide additional qualitative translation results for Edges$\rightarrow$Shoes$\times64$,
Edges$\rightarrow$Handbags$\times64$, and DIODE Outdoor$\times256$,
in Figs.~\ref{fig:e2s_appdx}, \ref{fig:e2h_appdx}, and \ref{fig:diode_appdx},
respectively.

\newpage{}

\begin{figure}
\begin{centering}
\includegraphics[width=1\textwidth]{asset/Result/qualitative_result/more_viz/e2s_appdx_v2}
\par\end{centering}
\caption{Additional qualitative results for Edges$\rightarrow$Shoes$\times64$,
where each pair of consecutive rows displaying the input image in
the ``Edges'' domain and its translation in the ``Shoes'' domain,
respectively.\label{fig:e2s_appdx}}

\end{figure}

\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\textwidth]{asset/Result/qualitative_result/more_viz/e2h_appdx_v2}\caption{Additional qualitative results for Edges$\rightarrow$Handbags$\times64$,
where each pair of consecutive rows displaying the input image in
the ``Edges'' domain and its translation in the ``Handbags'' domain,
respectively.\label{fig:e2h_appdx}}
\par\end{centering}
\end{figure}

\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.85\textwidth]{asset/Result/qualitative_result/more_viz/diode_appdx_v2}
\par\end{centering}
\caption{Additional qualitative results for DIODE Outdoor$\times256$, where
each pair of consecutive rows displaying the input image in the ``Normal
maps'' domain and its translation in the ``Color images'' domain,
respectively.\label{fig:diode_appdx}}
\end{figure}

