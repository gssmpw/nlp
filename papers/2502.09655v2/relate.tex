
\subsection{Schrödinger Bridges and Diffusion Bridges}

Recent bridge models can broadly be classified into Schrödinger bridges
(SB) and diffusion bridges (DB). The Schrödinger Bridge problem \cite{schrodinger1931umkehrung,pavon1991free}
aims to find a stochastic process that connects two arbitrary marginal
distributions $p_{A}$, $p_{B}$ while remaining as close as possible
to a reference process. When the reference process is a diffusion
process initialized at $p_{A}$, the solution to the SB problem can
be characterized by two coupled partial differential equations (PDEs)
governing the forward and backward diffusion processes initialized
at $p_{A}$ and $p_{B}$, respectively \cite{leonard2014survey,vargas2021solving,bortoli2021diffusion,chen2022likelihood,LiuCST22}. 

SB models are typically trained using iterative proportional fitting
which requires expensive simulation of the forward and backward processes
\cite{fortet1940resolution,bortoli2021diffusion}. Several approaches
have been proposed \cite{peluchetti2023diffusion,shi2023diffusion,tong2023simulation}
to improve the scalability of training SB models by leveraging the
score and flow matching frameworks \cite{hyvarinen2005estimation,song2020score,lipman2022flow,liu2022flow}.
However, SB models overlook the relationships between samples from
the two domains, making them unsuitable for paired translation tasks. 

Diffusion bridges simplify Schrödinger bridges by assuming a Dirac
distribution at one endpoint, allowing them to model the coupling
between the two domains for paired translations. I$^{2}$SB \cite{LiuVHTNA23}
is a diffusion bridge derived from the general theory of SBs. On the
other hand, methods like SB{\scriptsize{}ALIGN} \cite{SomnathPHM0B23},
$\Omega$-bridge \cite{liu2022let,LiuW0l23}, and DDBM \cite{zhou2024denoising}
leverage Doob's $h$-transform to obtain the formula of a continuous-time
$h$-transformed process that converges almost surely to a specific
target sample while aligning closely with the reference diffusion
process. SB{\scriptsize{}ALIGN} and $\Omega$-bridge create a $h$-transform
process that generates data and learn the drift of this process, whereas
DDBM designs a $h$-transformed process that converges to a latent
sample. For data generation, DDBM learns the score with respect to
the reverse process via conditional score matching, following the
approach in \cite{song2020score}. BBDM \cite{LiX0L23} extends the
unconditional variational framework for discrete-time diffusion processes
\cite{ho2020denoising,kingma2021variational,song2020denoising} to
a conditional variational framework for Brownian bridges. It then
uses the new framework to model the transition kernel of the data
generation process.

Different from the aforementioned methods, our method is built on
the Chapman-Kolmogorov equation (CKE) for bridges and has a novel
design that supports bidirectional transition between the two domains
using a single model.

\subsection{Diffusion and Flow Models for I2I}

Diffusion models (DMs) \cite{sohl2015deep,song2019generative,ho2020denoising,song2020score}
are powerful generative models that progressively denoise latent samples
from a standard Gaussian distribution to generate images. For image-to-image
(I2I) translation, DMs can incorporate source images as conditions
through either classifier-based \cite{dhariwal2021diffusion} or classifier-free
\cite{ho2021classifier} guidance techniques during the denoising
process to generate corresponding target images \cite{sasaki2021unit,saharia2022palette,zhao2022egsde,wolleb2022swiss}.
However, since one of the two boundary distributions in DMs is always
a standard Gaussian, bidirectional translation requires training two
distinct DMs conditioned on source and target images. DDIB \cite{su2023dual}
exemplifies this approach by combining two separate diffusion models
for source and target domains through a shared Gaussian latent space
for bidirectional translation.

Flow models (FMs) \cite{liu2022flow,lipman2022flow,albergo2023stochastic,do2024variational}
build an ODE map between two arbitrary boundary distributions and
can be trained via the flow matching loss \cite{lipman2022flow} related
to the score matching loss for diffusion models \cite{song2020score}.
FMs can be viewed as special cases of diffusion bridges where the
variance of the transition kernel is zero. Due to their deterministic
nature, FMs are less suitable for capturing the coupling between two
domains, as demonstrated by our experimental results in Sections~\ref{subsec:Bidirectional-I2I-Translation}
and \ref{subsec:noise_variance_ablation}. Nonetheless, FMs can be
useful for unpaired translation and can be specially designed to represent
optimal transport maps \cite{liu2022rectified,lipman2022flow,tong2023improving}.


