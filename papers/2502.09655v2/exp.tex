\begin{figure*}
\begin{centering}
\includegraphics[width=1\textwidth]{asset/Result/qualitative_result/unidirect_main}
\par\end{centering}
\caption{Images generated by BDBM and unidirectional baselines in the Edges$\rightarrow$Shoes,
Edges$\rightarrow$Handbags, and Normal$\rightarrow$Outdoor translation
tasks.\label{fig:main_qualitative_results}}
\end{figure*}


\subsection{Experimental Settings}

\subsubsection{Datasets and evaluation metrics}

We validate our method on 4 paired image-to-image (I2I) translation
datasets namely Edges$\leftrightarrow$Shoes, Edges$\leftrightarrow$Handbags,
DIODE Outdoor \cite{diode_dataset}, and Night$\leftrightarrow$Day
\cite{pix2pix2017}. Following \cite{zhou2024denoising}, we rescale
images to 64$\times$64 resolution for the first two datasets and
256$\times$256 for the latter two. We construct bridges in the pixel
space for the first three datasets and in the latent space of dimensions
32$\times$32$\times$4 for the Night$\leftrightarrow$Day dataset.
To map images to latent representations, we use a pretrained VQ-GAN
encoder \cite{rombach2022high}. Following prior work \cite{LiX0L23},
we use FID \cite{heusel2017gans}, IS \cite{salimans2016improved},
and LPIPS \cite{zhang2018perceptual} to measure the fidelity and
perceptual faithfulness of generated images. These metrics are computed
on training samples, as in \cite{zhou2024denoising}.

\begin{figure}
\begin{centering}
{\resizebox{0.70\textwidth}{!}{%
\par\end{centering}
\begin{centering}
\begin{tabular}{ccc}
\includegraphics[width=0.24\textwidth]{asset/e2s_LPIPS_comparison} &  & \includegraphics[width=0.24\textwidth]{asset/e2h_LPIPS_comparison}\tabularnewline
(a) Edges$\rightarrow$Shoes &  & (b) Edges$\rightarrow$Handbags\tabularnewline
\end{tabular}}}
\par\end{centering}
\caption{LPIPS curves of BDBM and unidirectional baselines on Edges$\rightarrow$Shoes
and Edges$\rightarrow$Handbags.\label{fig:LPIPS-curves-main}}
\end{figure}


\subsubsection{Model and training configurations\label{subsec:Model-and-training}}

Unless stated otherwise, we use Brownian bridges, as described in
Appdx.~\ref{subsec:Brownian-Bridge_settings}, with $\alpha_{t}=1-\frac{t}{T}$,
$\beta_{t}=\frac{t}{T}$ and $\sigma_{t}^{2}=k\frac{t}{T}\left(1-\frac{t}{T}\right)$
for our experiments. We consider discrete-time models with $T=1000$,
$\Delta t=1$, and $k=2$. Comparison with the continuous-time counterpart
is provided in Appdx.~\ref{subsec:Continuous-vs-Discrete}. For generation,
we employ ancestral sampling with number of function evaluations (NFE)
being 200. The variance of the transition kernel $\delta_{s,t}^{2}$
is set to $\delta_{s,t}^{2}=\eta\left(\sigma_{s}^{2}-\sigma_{t}^{2}\frac{\alpha_{s}^{2}}{\alpha_{t}^{2}}\right)$
with $\eta=1$. Studies on different values of $k$ and $\eta$ are
presented in Sections~\ref{subsec:noise_variance_ablation}, \ref{subsec:transition_variance_ablation},
respectively. We model $z_{\varphi}\left(t,x_{t},\left(1-m\right)*x_{0},m*x_{T}\right)$
using UNets with ADM architectures \cite{dhariwal2021diffusion} customized
for different input sizes. For 64$\times$64 images, we use 2 residual
blocks with 128 base channels. This allows us to train with batch
size of 128 for 64$\times$64 images on an H100 80GB GPU. For 256$\times$256
images, we increase the base channels to 256 and train with batch
size of 8. For effective training with batch size of 32, we accumulate
gradients over 4 update steps. All models were trained for 140k iterations
on the Edges$\leftrightarrow$Shoes dataset and 300k iterations on
the other datasets. The reduced iterations for Edges$\leftrightarrow$Shoes
were due to its smaller training set of 50k samples, compared to 130k
for Edges$\leftrightarrow$Handbags, as well as its smaller image
sizes compared to DIODE Outdoor and Night$\leftrightarrow$Day. The
Adam optimizer \cite{KingmaB14} is employed with a learning rate
of 1e-4 and $\beta_{1}$ set to 0.9.

\subsubsection{Baselines}

We compare our method BDBM with both unidirectional and bidirectional
I2I translation baselines. The unidirectional baselines include state-of-the-art
(SOTA) diffusion bridge models such as $\text{I}^{2}\text{SB}$ \cite{LiuVHTNA23},
BBDM \cite{LiX0L23} and DDBM \cite{zhou2024denoising}. We also include
a unidrectional variant of our method, referred to as BDBM-1, for
comparison to highlight the impact of modeling both directions simultaneously.
The bidirectional baselines consist of DDIB \cite{su2023dual} and
Rectified Flow (RF) \cite{liu2022flow}. The baselines, excluding
RF, were trained using their official code repositories. Since the
official RF code does not support parallel training, we used the implementation
from \cite{lee2023minimizing} for parallel training. For all baselines,
we use the same architecture, training configurations, and NFE as
our method.

\begin{table*}
\begin{centering}
\begin{tabular}{ccccccc}
\toprule 
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Edges$\leftrightarrow$Shoes$\times64$} & \multicolumn{3}{c}{Edges$\leftrightarrow$Handbags$\times64$}\tabularnewline
\cmidrule{2-7} 
 & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$ & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$\tabularnewline
\midrule
\midrule 
DDIB & 85.24/45.19 & 2.13/\textbf{3.40} & 0.38/0.45 & 77.95/31.50 & 2.81/3.59 & 0.49/0.52\tabularnewline
\midrule 
RF & 8.63/43.17 & \textbf{2.21}/2.79 & 0.03/0.16 & 5.98/48.53 & \textbf{3.19}/3.71 & 0.07/0.25\tabularnewline
\midrule
\midrule 
BDBM (ours) & \textbf{0.98}/\textbf{1.06} & 2.20/3.28 & \textbf{0.01}/\textbf{0.02} & \textbf{1.87}/\textbf{3.06} & 3.10/\textbf{3.74} & \textbf{0.02}/\textbf{0.08}\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{Results of BDBM and bidirectional baselines on bidirectional translation
tasks. For each method and metric, we report two numbers, the left
is for color-to-sketch translation, and the right is for sketch-to-color
translation. The best results are highlighted in bold.\label{tab:main_result_2}}
\end{table*}


\subsection{Experimental Results}

\subsubsection{Unidirectional I2I translation\label{subsec:Unidirectional-I2I-Translation}}

Following \cite{zhou2024denoising}, we experiment with the Edges$\leftrightarrow$Shoes,
Edges$\leftrightarrow$Handbags, and DIODE Outdoor datasets, focusing
on translating sketches or normal maps to color images, as this translation
is more challenging than the reverse. Results for the reverse translation
are provided in Appdx.~\ref{subsec:Unidirectional-reverse-translation}.

As shown in Table~\ref{tab:main_result_1} and Fig.~\ref{fig:LPIPS-curves-main},
BDBM significantly outperforms BDBM-1 and other unidirectional baselines
in most metrics and datasets. This improvement is also evident in
the superior quality of samples generated by our method compared to
the baselines, as displayed in Fig.~\ref{fig:main_qualitative_results}.
Notably, BDBM was trained using the same number of iterations as the
baselines. This means that the actual number of model updates w.r.t.
a specific direction in BDBM is only \emph{half} that of the baselines,
as the two endpoints $x_{0}$, $x_{T}$ are sampled with equal probability
in the loss $\Loss_{\text{BDBM}}$ (Eq.~\ref{eq:loss_BDBM}). This
demonstrates the clear advantage of our proposed bidirectional training
over the unidirectional counterpart.

We hypothesize that allowing either $x_{0}$ or $x_{T}$ to serve
as the condition for the \emph{shared-parameter} noise model $z_{\varphi}$
during training enables the optimizer to leverage the endpoint that
yields more accurate predictions for effective parameter updates.
Intuitively, this endpoint is likely the one closer in time to the
input $x_{t}$ of the noise model. For instance, consider two noise
predictions $z_{\varphi}\left(t,x_{t},x_{0}\right)$ and $z_{\varphi}\left(t,x_{t},x_{T}\right)$
for $x_{t}$ at time $t$ closer to 0 than to $T$, where $x_{0}$
and $x_{T}$ are chosen with equal probability. Since $x_{0}$ generally
provides more reliable information about the noise in $x_{t}$ compared
to $x_{T}$, the optimizer tends to prioritize the output of $z_{\varphi}\left(t,x_{t},x_{0}\right)$
when updating the shared parameters $\varphi$. This update not only
improves the accuracy of $z_{\varphi}\left(t,x_{t},x_{0}\right)$
but also enhances $z_{\varphi}\left(t,x_{t},x_{T}\right)$ due to
the shared parameter structure. In contrast, unidirectional training
can only use a single endpoint, for example $x_{T}$, as the condition,
which reduces it effectiveness in learning model parameters at times
$t$ far from $T$. As $x_{t}$ becomes increasingly different from
$x_{T}$, the information provided by $x_{T}$ becomes less useful
for accurately predicting the noise in $x_{t}$.

\subsubsection{Bidirectional I2I translation\label{subsec:Bidirectional-I2I-Translation}}

We compare BDBM with bidirectional baselines DDIB and RF, presenting
quantitative and qualitative results in Table~\ref{tab:main_result_2}
and Fig.~\ref{fig:bidirect_qualitative}. BDBM outperforms the two
baselines by large margins for translations in both directions. DDIB
struggles to maintain pair consistency between boundary samples due
to random mapping into shared Gaussian latent samples, resulting in
translations that often differ greatly from the ground truth. Meanwhile,
RF performs reasonably well for the color-to-sketch translation but
poorly for the reverse. This is because different color images can
have very similar sketch images. This causes the learned velocity
for the sketch-to-color translation to point toward the average of
multiple target color images associated with a source sketch image,
as evident in Fig.~\ref{fig:bidirect_qualitative}.

\noindent 
\begin{figure}
\begin{centering}
{\resizebox{0.70\textwidth}{!}{%
\par\end{centering}
\begin{centering}
\begin{tabular}{c|c|c|c}
\textcolor{blue}{Reference} & DDIB & RF & BDBM (ours)\tabularnewline
\includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2shoes/Ref} & \includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2shoes/DDIB} & \includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2shoes/RF} & \includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2shoes/BDBM}\tabularnewline
\includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2handbags/Ref} & \includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2handbags/DDIB} & \includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2handbags/RF} & \includegraphics[width=0.12\textwidth]{asset/Result/qualitative_result/bidirect/edges2handbags/BDBM}\tabularnewline
\end{tabular}}}
\par\end{centering}
\caption{Images generated by BDBM and bidirectional baselines on Edges$\leftrightarrow$Shoes
and Edges$\leftrightarrow$Handbags. ``Reference'' column shows
reference images of the two domains. \label{fig:bidirect_qualitative}}
\end{figure}


\subsection{Ablation Study}

\subsubsection{Impacts of different parameterizations}

\noindent 
\begin{table*}
\begin{centering}
\begin{tabular}{ccccccccc}
\toprule 
\multirow{2}{*}{Prediction} & \multicolumn{4}{c}{Edges$\rightarrow$Shoes$\times64$} & \multicolumn{4}{c}{Edges$\rightarrow$Handbags$\times64$}\tabularnewline
\cmidrule{2-9} 
 & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$ & Diversity $\uparrow$ & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$ & Diversity $\uparrow$\tabularnewline
\midrule
\midrule 
$z$ & 1.06 & 3.28 & 0.02 & 6.90 & 3.06 & 3.74 & 0.08 & 9.01\tabularnewline
\midrule 
$x_{T}+x_{0}$ & 1.51 & 3.25 & 0.04 & 2.21 & 3.71 & 3.75 & 0.11 & 7.54\tabularnewline
\midrule 
$\left(x_{T},x_{0}\right)$ & 1.49 & 3.24 & 0.01 & 1.97 & 3.49 & 3.77 & 0.12 & 7.88\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{Results of our method w.r.t. different parameterizations.\label{tab:Results across different param}}
\end{table*}

As discussed in Section~\ref{subsec:Bidirectional-Diffusion-Bridge-Models},
the transition kernel of BDBM can be modeled by predicting the noise
$z$ or endpoints (either by predicting $x_{0}+x_{T}$ and inferring
the missing endpoint given the known one, or by directly predicting
one endpoint given the other). We compare the effectiveness of these
approaches on the Edges$\rightarrow$Shoes and Edges$\rightarrow$Handbags
translation tasks, with results shown in Table~\ref{tab:Results across different param}.
In addition to FID and LPIPS metrics, we evaluate Diversity \cite{batzolis2021conditional,LiX0L23},
which measures the average pixel-wise standard deviation of multiple
color images generated from a single sketch on a held-out test set
of 200 samples. We observe that predicting noise achieves slightly
better FID scores and produces more diverse samples than predicting
endpoints. We hypothesize that since $x_{0}$, $x_{T}$ are fixed
while $z$ is sampled randomly during training, predicting endpoints
tends to have less variance than predicting noise, which results in
less diverse samples.

\subsubsection{Effect of the noise variance $\sigma_{t}^{2}$\label{subsec:noise_variance_ablation}}

In Section~\ref{subsec:Model-and-training}, the noise variance $\sigma_{t}^{2}$
of BDBM is defined as $\sigma_{t}^{2}=k\frac{t}{T}\left(1-\frac{t}{T}\right)$,
which means we can control $\sigma_{t}^{2}$ by changing the value
of $k$. Table~\ref{tab:ablation_k} shows the results on Edges$\rightarrow$Shoes
for different values of $k\in\left\{ 1,2,4,8\right\} $. Increasing
$k$ generally yields more diverse samples but worsens FID and LPIPS
scores. This trade-off occurs because higher $k$ values increase
the variance of the distribution $q\left(x_{t}|x_{0},x_{T}\right)$,
enlarging the path space and consequently making the model optimization
more challenging. Conversely, when $k$ is too small, the noise variance
becomes insufficient to corrupt domain information for effective translation.
Our results indicate that $k=2$ offers the best balance between diversity
and quality.

\begin{table}
\begin{centering}
\begin{tabular}{cccc}
\toprule 
\multirow{2}{*}{$k$} & \multicolumn{3}{c}{Edges$\rightarrow$Shoes$\times64$}\tabularnewline
\cmidrule{2-4} 
 & FID $\downarrow$ & LPIPS $\downarrow$ & Diversity $\uparrow$\tabularnewline
\midrule
\midrule 
1 & 2.07 & 0.04 & 4.61\tabularnewline
\midrule 
2 & 1.06 & 0.02 & 6.90\tabularnewline
\midrule 
4 & 2.35 & 0.03 & 7.26\tabularnewline
\midrule 
8 & 3.52 & 0.05 & 7.81\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{Results of BDBM on Edges$\rightarrow$Shoes w.r.t. different values
of $k$ controlling the variance $\sigma_{t}^{2}$.\label{tab:ablation_k}}
\end{table}


\subsubsection{Effect of the variance $\delta_{s,t}^{2}$ of the transition kernel\label{subsec:transition_variance_ablation}}

\begin{table}
\begin{centering}
\begin{tabular}{ccccccc}
\toprule 
 &  & \multicolumn{5}{c}{Edges$\rightarrow$Shoes$\times64$}\tabularnewline
\midrule 
\multicolumn{2}{c}{NFE} & 20 & 50 & 100 & 200 & 1000\tabularnewline
\midrule
\midrule 
\multirow{4}{*}{$\eta$} & 0.0 & 4.16 & 2.98 & 2.47 & 2.15 & 1.87\tabularnewline
\cmidrule{2-7} 
 & 0.2 & 3.37 & 2.31 & 1.79 & 1.42 & 1.14\tabularnewline
\cmidrule{2-7} 
 & 0.5 & 2.63 & 1.69 & 1.38 & 1.10 & 0.96\tabularnewline
\cmidrule{2-7} 
 & 1.0 & 2.11 & 1.52 & 1.25 & 1.06 & 0.92\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{FID scores of BDBM on Edges$\rightarrow$Shoes w.r.t. different values
of $\eta$ controlling the variance $\delta_{s,t}^{2}$ and different
numbers of sampling steps.\label{tab:ablation_eta}}
\end{table}

\noindent 
\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.3\textwidth]{asset/Result/qualitative_result/ablation_eta/eta_NFE20}\hspace*{0.04\columnwidth}\includegraphics[width=0.3\textwidth]{asset/Result/qualitative_result/ablation_eta/eta_NFE200}
\par\end{centering}
\caption{Samples generated by BDBM when translating from sketches to shoes
using NFE=20 and NFE=200 for w.r.t. different values of $\eta$.\label{fig:ablation_eta_quantitative}}
\end{figure}

We study the impact of varying the variance $\delta_{s,t}^{2}$ of
the transition kernel via changing $\eta$ (Section~\ref{subsec:Model-and-training})
on generation quality, with the results presented in Table~\ref{tab:ablation_eta}
and Fig.~\ref{fig:ablation_eta_quantitative}. We observe that increasing
$\eta$ from 0 to 1 consistently improves the quality of generated
results, regardless of the number of sampling steps. The reason is
that the target bridge process connecting boundary points from the
two domains is stochastic and corresponds to $\eta=1$. Consequently,
higher $\eta$ values make $x_{t}$ more likely to be a sample from
the target distribution at time $t$, leading to better results.

\subsubsection{Translation in latent spaces}

\begin{table}
\centering{}%
\begin{tabular}{cccc}
\toprule 
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Day$\rightarrow$Night$\times256$}\tabularnewline
\cmidrule{2-4} 
 & FID $\downarrow$ & IS $\uparrow$ & LPIPS $\downarrow$\tabularnewline
\midrule
\midrule 
RF & 12.38 & 3.90 & 0.37\tabularnewline
\midrule 
DDIB & 226.9 & 2.11 & 0.79\tabularnewline
\midrule 
$\text{I}^{2}\text{SB}$ & 15.56 & 4.03 & 0.36\tabularnewline
\midrule 
DDBM & 27.63 & 3.92 & 0.55\tabularnewline
\midrule
\midrule 
BDBM (ours) & \textbf{6.63} & \textbf{4.18} & \textbf{0.34}\tabularnewline
\bottomrule
\end{tabular}\caption{Comparison of BDBM and baseline methods on the Day$\rightarrow$Night
translation task in latent spaces. Baseline results are sourced from
\cite{zhou2024denoising}\label{tab:ablation_latent}}
\end{table}

To validate BDBM's translation capability in latent spaces, we adopt
the Day$\rightarrow$Night translation experiment from \cite{zhou2024denoising}.
For a fair comparison, we maintain the same experimental settings
as in \cite{zhou2024denoising}, including the model architecture,
training iterations, and NFE=53 for sample generation. We also follow
\cite{zhou2024denoising} and compute metrics using the reconstructed
versions of the ground-truth target images. This helps mitigate the
impact of the VQ-GAN decoding process and ensures that the results
accurately reflect the translation quality. Table~\ref{tab:ablation_latent}
presents the results of BDBM and baseline methods, with the baseline
results taken from \cite{zhou2024denoising}. It is evident that BDBM
significantly outperforms the baselines, demonstrating its consistent
performance in both pixel and latent spaces. We also observed that
BDBM effectively captures the statistics of the two domains, where
in the dataset, nighttime images are much less diverse than daytime
ones, leading to the generation of duplicated nighttime images when
using different random seeds, as illustrated in Fig.~\ref{fig:Additional-results-NightDay}.


