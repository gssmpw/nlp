
\subsection{Markov Processes and Diffusion Processes}

A Markov process is a stochastic process satisfying the Markov property,
i.e., the future (state) is independent of the past given the present:
\[
p\left(x_{s}|x_{t},x_{u}\right)=p\left(x_{s}|x_{t}\right)
\]
where $x_{u}$, $x_{t}$, $x_{t}$ denote random states at times $u$,
$t$, $s$ satisfying that $0\leq u<t<s$. Here, $p\left(x_{s}|x_{t}\right)$
is the transition distribution of the Markov process. 

Diffusion processes are special cases of Markov processes where the
transition distribution is typically a Gaussian distribution. A diffusion
process can be either discrete-time \cite{ho2020denoising} or continuous-time
\cite{song2020score}. A continuous-time diffusion process can be
described by the following (forward) stochastic differential equation
(SDE):
\begin{align}
dX_{t} & =\mu\left(t,X_{t}\right)dt+\sigma\left(t,X_{t}\right)dW_{t}\label{eq:SDE_1}
\end{align}
where $W_{t}$ denotes the Wiener process (aka Brownian motion) at
time $t$. Eq.~\ref{eq:SDE_1} can be solved via simulation provided
that the distribution of $X_{0}$ is known. One can derive the \emph{forward}
and \emph{backward Kolmogorov equations} (KFE and KBE) for this SDE
as follows:
\begin{equation}
\text{KFE:}\ \ \frac{\partial p\left(t,x\right)}{\partial t}=\mathcal{G}^{*}p\left(t,x\right);\ p\left(0,\cdot\right)\ \text{is given}\label{eq:KFE_1}
\end{equation}
\begin{equation}
\text{KBE:}\ \ \frac{\partial p\left(T,y|t,x\right)}{\partial t}=-\mathcal{G}p\left(T,y|t,x\right);\ p\left(T,\cdot\right)\ \text{is given}\label{eq:KBE_1}
\end{equation}
where $\mathtt{\mathcal{G}}$ denotes the \emph{generator} corresponding
to the SDE in Eq.~\ref{eq:SDE_1} and $\mathcal{G}^{*}$ is the adjoint
of $\mathtt{\mathcal{G}}$. When $\sigma\left(t,x\right)$ is a scalar
depending only on $t$ (i.e., $\sigma\left(t,x\right)\equiv\sigma\left(t\right)$,
for a real-valued function $f$, $\mathcal{G}f\left(t,x\right)$ and
$\mathcal{G}^{*}f\left(t,x\right)$ are given by:
\begin{align*}
\mathcal{G}f\left(t,x\right) & ={\nabla f\left(t,x\right)}^{\top}\mu\left(t,x\right)+\frac{{\sigma\left(t\right)}^{2}}{2}\Delta f\left(t,x\right)\\
\mathcal{G}^{*}f\left(t,x\right) & =-\nabla\cdot\left(f\left(t,x\right)\mu\left(t,x\right)\right)+\frac{{\sigma\left(t\right)}^{2}}{2}\Delta f\left(t,x\right)
\end{align*}
where $\nabla\cdot$ and $\Delta$ denote the divergence and Laplacian,
respectively.

\subsection{Chapman-Kolmogorov Equations\label{subsec:Chapman-Kolmogorov-Equations} }

A Markov process can be described via the \emph{Chapman-Kolmogorov
equation} (CKE) \cite{karush1961chapman} as follows:
\begin{equation}
p\left(x_{s}|x_{t}\right)=\int p\left(x_{s}|x_{r}\right)p\left(x_{r}|x_{t}\right)dx_{r}\label{eq:CKE}
\end{equation}

\noindent which holds for all times $t$, $r$, $s$ satisfying that
$0\le t<r<s\leq T$. The CKE in Eq.~\ref{eq:CKE} can be considered
as the integral form of the KFE and KBE in Eqs.~\ref{eq:KFE_1},
\ref{eq:KBE_1}. Compared to the Kolmogorov equations, the CKE is
easier to work with since (i) it does not involve the partial derivatives
of the transition kernel, (ii) it is applicable to both continuous-
and discrete-time Markov processes, and (iii) it encapsulates both
forward and backward transitions. Regarding the last point, we can
apply Eq.~\ref{eq:CKE} either in the forward manner (from $0$ to
$T$) to evaluate the distribution of the next state $x_{s}$ given
the distribution of the current state $x_{t}$:
\begin{equation}
p\left(x_{s}|x_{0}\right)=\int p\left(x_{s}|x_{t}\right)p\left(x_{t}|x_{0}\right)dx_{t};\ p\left(x_{t}|x_{0}\right)\ \text{is given}\label{eq:forward_CKE}
\end{equation}
or in the backward manner (from $T$ to $0$) to evaluate the distribution
of the previous state $x_{t}$ given the distribution of current state
$x_{s}$:
\begin{equation}
p\left(x_{T}|x_{t}\right)=\int p\left(x_{T}|x_{s}\right)p\left(x_{s}|x_{t}\right)dx_{s};\ p\left(x_{T}|x_{s}\right)\ \text{is given}\label{eq:backward_CKE}
\end{equation}
In the discrete-time setting, Eq.~\ref{eq:forward_CKE} can be interpreted
as given a Markov process with $p\left(x_{t+1}|x_{t}\right)$ specified
for every time $t$. If we have known the marginal distribution $p\left(x_{t}|x_{0}\right)$
at time $t$, then by solving the CKE forwardly, we can compute $p\left(x_{t+1}|x_{0}\right)$
at time $t+1$. Similarly, in Eq.~\ref{eq:backward_CKE}, if we have
known $p\left(x_{T}|x_{t+1}\right)$ at time $t+1$, then by solving
the CKE backwardly, we can compute $p\left(x_{T}|x_{t}\right)$ at
time $t$. For example, in DDPM \cite{ho2020denoising}, given $p\left(x_{t}|x_{0}\right)=\Normal\left(x_{t}\mid\sqrt{\bar{\alpha}_{t}}x_{0},\left(1-\bar{\alpha}_{t}\right)\mathrm{I}\right)$
and $p\left(x_{t+1}|x_{t}\right)=\Normal\left(x_{t+1}\mid\sqrt{1-\beta_{t+1}}x_{t},\beta_{t+1}\mathrm{I}\right)$,
we can use Eq.~\ref{eq:forward_CKE} to compute $p\left(x_{t+1}|x_{0}\right)$
as $\Normal\left(x_{t+1}\mid\sqrt{\bar{\alpha}_{t+1}}x_{0},\left(1-\bar{\alpha}_{t+1}\right)\mathrm{I}\right)$.

Interestingly, the backward CKE in Eq.~\ref{eq:backward_CKE} can
be written in another way according to Bayes' rule:
\begin{equation}
p\left(x_{t}|x_{T}\right)=\int p\left(x_{t}|x_{s}\right)p\left(x_{s}|x_{T}\right)dx_{s};\ p\left(x_{s}|x_{T}\right)\ \text{is given}\label{eq:backward_CKE_2}
\end{equation}
The mathematical derivation is detailed in Appdx.~\ref{subsec:Derivation-of-the-backward-CKE}.
Eq.~\ref{eq:backward_CKE_2} is akin to the forward CKE in Eq.~\ref{eq:forward_CKE}
but in reverse time.
