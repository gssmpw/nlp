Diffusion models (DMs)~\cite{sohl2015deep,song2019generative,ho2020denoising}
have emerged as a powerful class of generative models, surpassing
GANs~\cite{goodfellow2014generative} and VAEs~\cite{kingma2013auto}
in generating high-quality data~\cite{dhariwal2021diffusion}. These
models learn to transform a Gaussian prior distribution into the data
distribution through iterative denoising steps. However, the Gaussian
prior assumption in diffusion models limits their application, particularly
in image-to-image (I2I) translation \cite{pix2pix2017}, where the
distributions of the two domains are non-Gaussian.

A straightforward solution is to incorporate an additional condition
related to one domain into diffusion models for guidance \cite{choi2021ilvr,saharia2022palette}.
This approach often overlooks the marginal distribution of each domain,
which may hinder its generalization ability, especially when the two
domains are diverse and significantly different. In contrast, methods
that construct an ODE flow \cite{lipman2022flow,liu2022flow,albergo2023stochastic}
or a Schrödinger bridge \cite{bortoli2021diffusion,shi2023diffusion,kim2024unpaired}
between two domains focus mainly on matching the marginal distributions
at the boundaries, neglecting the relationships between samples from
the two domains. Consequently, these methods are not well-suited for
paired I2I tasks.

To solve the paired I2I problem, recent methods \cite{LiuW0l23,zhou2024denoising}
leverage knowledge of the target sample $y$ in the pair $\left(x,y\right)$
and utilized Doob's $h$-transform \cite{doob1984classical} to construct
a bridge that converges to $y$. This involves learning either the
$h$ function \cite{SomnathPHM0B23} or the score function of the
$h$-transformed SDE \cite{zhou2024denoising}, both of which depend
on $y$. Other methods \cite{LiX0L23} extend the unconditional variational
framework for diffusion models to a conditional one given $y$ for
constructing such bridges, thereby learning a backward transition
distribution conditioned on $y$. Despite their success in capturing
the correspondence between $x$ and $y$, these methods share a common
limitation: they can only generate data in \emph{one direction}, from
$y$ to $x$. For the reverse from $x$ to $y$, a separate bridge
must be trained with $x$ being the target, which doubles computational
resources and modeling complexity. We argue that real-world applications
would greatly benefit from bidirectional generative models capable
of transitioning between two distributions using a single model.

Therefore, we introduce a novel bridge model called \textbf{\textit{\emph{B}}}\textit{\emph{idirectional
}}\textbf{\textit{\emph{D}}}\textit{\emph{iffusion }}\textbf{\textit{\emph{B}}}\textit{\emph{ridge
}}\textbf{\textit{\emph{M}}}\textit{\emph{odel}} (BDBM) that enables
\emph{bidirectional} transitions between two coupled distributions
using \textit{only a single network}. Our bridge is built on a framework
that highlights the symmetry between forward and backward transitions.
By utilizing the Chapman-Kolmogorov Equation (CKE) for conditional
Markov processes, we transform the problem of modeling the conditional
distribution $p\left(x_{T}=y|x_{0}=x\right)$ into modeling the forward
transition from $p\left(x_{t}|x,y\right)$ to $p\left(x_{s}|x,y\right)$
- the marginal distributions at times $t$ and $s$ ($0\leq t<s\leq T$)
of a \emph{double conditional Markov process} (DCMP) between two endpoints
$x,y\sim p\left(x,y\right)$. Given the interchangeability of the
two marginal distributions, we can model the conditional distribution
$p\left(x_{0}=x|x_{T}=y\right)$ simply by learning the backward transition
from $p\left(x_{s}|x,y\right)$ to $p\left(x_{t}|x,y\right)$ without
altering the DCMP. Notably, the forward and backward transition distributions
of the DCMP are connected through Bayes' rule and can be expressed
analytically as Gaussian distributions when the DCMP is a diffusion
process. This insight motivates us to reparameterize models of the
forward and backward transition distributions in a way that they share
a common term. Therefore, we can use a single network for modeling
this term and train it with a unified objective for both directions.

We evaluate our method on four popular paired I2I translation datasets~\cite{pix2pix2017,diode_dataset}
with image sizes up to 256$\times$256, considering both pixel and
latent spaces. Experimental results demonstrate that BDBM surpasses
state-of-the-art (SOTA) unidirectional diffusion bridge models in
terms of visual quality (measured by FID) and perceptual similarity
(measured by LPIPS) of generated samples, while requiring similar
or even fewer training iterations. These promising results showcase
the clear advantages of our method, which not only facilitates bidirectional
translation at minimal additional cost but also improves performance.


