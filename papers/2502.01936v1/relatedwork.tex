\section{Related Work}
With the widespread adoption of GNNs, their robustness against adversarial attacks has gained increasing attention. While early studies mainly focused on GMA, constraints such as user permissions have led to a shift toward GIA, which has emerged as a more practical and effective attack approach. NIPA \cite{Sun2020Adversarial} achieves its malicious intent by generating a batch of random nodes and injecting them into the existing graph. AFGSM \cite{wang2020scalable} employs the fast gradient sign method to inject malicious nodes, enabling attacks on large-scale graphs. G-NIA \cite{tao2021single} utilizes neural networks to learn and generate new nodes and edges, which are then injected into the original graph to launch attacks. TDGIA \cite{zou2021tdgia} proposes a method for detecting vulnerable nodes in the graph topology to identify attack targets and introduces a smooth feature generation approach to facilitate attacks. AGIA \cite{chen2022understanding} utilizes surrogate model gradients to optimize edge weights and inject node features. HAO \cite{chen2022understanding} offers a plug-in method to enhance the unnoticeability of attacks, utilizing the homophily ratio of nodes as an unnoticeability constraint to further improve the stealthiness of adversarial attacks. However, the aforementioned methods all rely on surrogate models, and discrepancies between the target model and the surrogate model may lead to degraded attack performance. G2A2C \cite{ju2023let} formulates the attack process as a Markov Decision Process (MDP) and utilizes reinforcement learning to optimize an injection model based on query-based learning. While G2A2C alleviates dependency on surrogate model gradients, it does not consider the unnoticeability of the attack and still depends on surrogate models.