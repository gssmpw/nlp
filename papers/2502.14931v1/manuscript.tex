\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
% \usepackage{epsfig} % for postscript graphics files
\usepackage{graphicx}
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath,bm} % assumes amsmath package installed
\usepackage{amsfonts,amssymb} % assumes amsmath package installed
\usepackage{colortbl}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{threeparttable} % Package for table footnotes
\usepackage{mathrsfs}
\usepackage{pifont}
\usepackage{urwchancal}
% \usepackage{ulem}
\usepackage{url}
% prompt
\usepackage{tcolorbox}
% \usepackage[many]{tcolorbox}
\usepackage{verbatim}
\usepackage{ulem}
\usepackage{ifthen} % anonymous

\newcommand{\high}[1]{{\color{orange}{#1}}}
\newcommand{\todo}[1]{{\color{red}{#1}}}
\newcommand{\bestcolor}[1]{\cellcolor{green!30}\textbf{#1}}
\newcommand{\bestcolornob}[1]{\cellcolor{green!30}{#1}}
\newcommand{\secondcolor}[1]{\cellcolor{yellow!30}{#1}}
\newcommand{\thirdcolor}[1]{\cellcolor{red!30}{#1}}
\newcommand{\fourthcolor}[1]{\underline{#1}}
% \newcommand{\tocheck}[1]{{\color{black}{#1}}}
% \newcommand{\todo}[1]{{\color{black}{#1}}}
\newcommand{\tocheck}[1]{{\color{cyan}{#1}}}
\newcommand{\tochange}[1]{{\color{blue}{#1}}}
\newcommand{\changed}[1]{{\color{black}{#1}}} % magenta
\newcommand{\Fig}[1]{Fig. \ref{#1}}
\newcommand{\beq}{\begin{equation}}
	\newcommand{\eeq}{\end{equation}}
\newcommand{\vts}{\mathrm{T}}
\newcommand{\Tab}[1]{Tab. \ref{#1}}
\newcommand{\czx}[1]{{\color{orange}{#1}}}
\newcommand{\MethodName}[1]{Hier-SLAM++}
\newcommand{\oldMethodName}[1]{Hier-SLAM}
\newtcolorbox[list inside=prompt,auto counter,number within=section]{prompt}[1][]{
    colbacktitle=black!60,
    coltitle=white,
    fontupper=\footnotesize,
    boxsep=5pt,
    left=0pt,
    right=0pt,
    top=0pt,
    bottom=0pt,
    boxrule=1pt,
    title={#1},
    #1, % add more args
}
% \renewcommand{\appendix}{\section*{Appendix}}
% \renewcommand{\thesection}{A}
% \renewcommand{\thesubsection}{A.\arabic{subsection}}




\newcommand{\pjs}[1]{\textcolor{blue}{\textsc{Pjs:} #1}}
\newcommand{\hrt}[1]{\textcolor{orange}{\textsc{Hamid:} #1}}
\newcommand{\mgb}[1]{\textcolor{olive}{\textsc{Maria:} #1}}

% anonymous or not
\newcommand{\anonymous}{0}  

\begin{document}

\title{Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting}
% ver1: Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting
% ver2: 

\ifthenelse{\anonymous=1}{
\author{Anonymous Authors}
}{%
\author{Boying Li$^{1*}$, Vuong Chi Hao$^{2}$, Peter J. Stuckey$^{1}$, Ian Reid$^{3}$, and Hamid Rezatofighi$^{1}$ % ~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}
\thanks{$^{1}$ Faculty of Information Technology, Monash University, Australia.$^{2}$ VinUniversity, Vietnam. $^{3}$ Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates. $^*$ Corresponding author: Boying Li ({\tt\small boying.li@monash.edu)}
}
\thanks{This work is supported by the DARPA Assured Neuro Symbolic Learning and Reasoning (ANSR) program under award number FA8750-23-2-1016. The work has received partial funding from The Australian Research Council Discovery Project ARC DP2020102427.}}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% ? 
% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

% we use code to represent semantic information from root to leaf based the tree structure. and we learn that tree in an end-to-end manner. 
\begin{abstract}
We propose \MethodName{}, a comprehensive Neuro-Symbolic semantic 3D Gaussian Splatting SLAM method with both RGB-D and monocular input featuring an advanced hierarchical categorical representation, which enables accurate pose estimation as well as global 3D semantic mapping. 
The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making scene understanding particularly challenging and costly.
% 1) hierarchical tree representation: size info + semantic info + shape info (text-to-3d models): more general tree for multiple scenes.
To address this problem, we introduce a novel and general hierarchical representation that encodes both semantic and geometric information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs) as well as the 3D generative model. By utilizing the proposed hierarchical tree structure, semantic information is symbolically represented and learned in an end-to-end manner.
% 2) multi-layers conv2d + activation layer
We further introduce a novel semantic loss designed to optimize hierarchical semantic information through both inter-level and cross-level optimization.
% 3) monocular slam: use dust3r as depth initialization for depth + supervisory signal
Additionally, we propose an improved SLAM system to support both RGB-D and monocular inputs using a feed-forward model. To the best of our knowledge, this is the first semantic monocular Gaussian Splatting SLAM system, significantly reducing sensor requirements for 3D semantic understanding and broadening the applicability of semantic Gaussian SLAM system.  
% 4) experiments: wider datasets: replica, scannet, tum-rgbd
We conduct experiments on both synthetic and real-world datasets, demonstrating superior or on-par performance with state-of-the-art NeRF-based and Gaussian-based SLAM systems, while significantly reducing storage and training time requirements.

% demonstrating superior or on par with semantic SLAM performance in the RGB-D setting and competitive results in the monocular setting, while significantly reducing storage and training time requirements.}
% We conduct experiments on both synthetic and real-world datasets, demonstrating superior performance in the RGB-D setting and competitive results in the monocular setting. Additionally, \MethodName{} achieves state-of-the-art performance in semantic segmentation rendering while significantly reducing storage and training time requirements.

% \hrt{\sout{there is no discussion about monocular performance while there is an emphesize on this. Also I feel you need to only emphesize on contribtion you have in this paper, considering Hier-SLAM is someone else work. the current abstract feels you have strong overlap idea with hier SLAM}}

\end{abstract}

\begin{IEEEkeywords}
Semantic SLAM, hierarchical category, gaussian splatting, RGB-D, monocular.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{V}{isual} Simultaneous Localization and Mapping (SLAM) is a key technology for ego-motion estimation and scene perception, widely employed in  applications across various domains, such as autonomous drones \cite{heng2014autonomous}, self-driving vehicles \cite{lategahn2011visual}, and interactive experiences in Augmented Reality (AR) and Virtual Reality (VR) \cite{chekhlov2007ninja}. 
Semantic information, which provides high-level knowledge about an environment, is indispensable for comprehensive scene understanding. Thus, it plays a crucial role in enabling intelligent robots to perform various complex tasks. In recent decades, advances in image segmentation and enhanced map representations have accelerated progress in semantic visual SLAM \cite{civera2011towards, bowman2017probabilistic, chang2021kimera, li2023dns, rosinol2020kimera, li2023textslam, li2020textslam,zhu2023sni, li2023dns, haghighi2023neural}. % \hrt{\sout{I will cite more fundamental semantic visual SLAM here and avoid too many self-citations}}

%%%% FIGURE %%%%
% fig1
\begin{figure}[t!]		
    \centering  
    % left bottom right top
     \includegraphics[width=0.6\textwidth, trim=0mm 40mm 70mm 0mm, clip]{pic/hierslampp.pdf} \\
    \caption{\textbf{(a).} The hierarchical tree is generated by integrating geometric information and semantic messages, utilizing 3D generative models and Large Language Models (LLMs).
    \textbf{(b).} The global 3D Gaussian map generated by \MethodName{} with learned semantic labels is shown on the left. The established hierarchical tree of the semantic information is organized on the right.
    Based on the established tree, the hierarchical symbolic representation for semantic information is shown at the bottom of the block, which compresses semantic data, reducing both memory usage and training time of the semantic SLAM.
    \textbf{(c). }The rendered semantic map at different levels shows a coarse-to-fine understanding, beneficial for real-world scenarios with shifting perspectives from distant to close.}  
    \label{fig:hierslampp}  
    % \vspace{-15pt}
\end{figure}
%%%% FIGURE %%%%


Recently, 3D Gaussian Splatting has gained attention as a leading 3D world representation method~\cite{kerbl20233d, Yu2024mipsplat, Wu20244dgs}, thanks to its fast rendering and optimization efficiency enabled by the highly parallelized rasterization of 3D primitives. Specifically, 3D Gaussian Splatting models the \textit{continuous} distributions of geometric parameters through Gaussian distributions. This approach not only improves the quality of 3D representation but also supports efficient optimization, making it particularly well-suited for SLAM tasks. %SLAM requires solving a complex optimization problem involving the simultaneous estimation of camera poses and global map representations. 
While promising, current SLAM systems based on 3D Gaussian Splatting \cite{keetha2023splatam, matsuki2024gaussian, yan2024gs, huang2024photo, yugay2023gaussian} primarily focus on geometric reconstruction. However, the lack of semantic integration limits its potential for complex tasks such as visual navigation, planning, and autonomous driving. Addressing this gap by incorporating semantic understanding without sacrificing efficiency remains a key challenge.

A naive solution would be to augment 3D primitives with discrete semantic labels and parameterize their distribution using a categorical distribution, i.e., a flat Softmax-based representation. However, 3D Gaussian Splatting is already a storage-intensive approach 
\cite{lu2024scaffold,chen2024hac}, requiring a large number of 3D primitives with multiple parameters to achieve realistic rendering. Adding semantic distribution parameters would further increase storage requirements and processing time, with grow linear in the number of semantic classes, making it particularly impractical for complex scene understanding. 
Furthermore, given a fixed number of observations (video frames), increasing the number of optimizable parameters leads to the curse of dimensionality, resulting in degraded optimization performance in high-dimensional spaces, such as when handling a large number of semantic classes.
Overall, this added overhead for semantic learning is especially problematic for robotics applications, where real-time performance and limited resources are critical.
% \hrt{I will add also some notes like ``Given a fixed number of observations (frames), increasing the number of optimizable parameters leads to the curse of dimensionality, resulting in degraded optimization performance in high-dimensional spaces, such as when handling a large number of semantic classes.'' } 
Recent studies have explored alternative methods to mitigate these challenges. For instance, \cite{li2024sgs} directly learns a 3-channel RGB visualization to avoid semantic labels learning. \cite{zhu2024semgauss} adopts a flat semantic representation and relies on a pre-trained foundation model to generate an embedding feature map rather than an explicit 3D semantic label map.
% have down the tone for sgs-slam: The work \cite{li2024sgs} directly learns a 3-channel RGB visualization for semantic maps instead of the true semantic information understanding.
% down the tone for semgauss-slam?

%%%% new version %%%%%
Unlike flat representations, semantic information naturally organizes into a hierarchical structure of classes, as shown in \Fig{fig:hierslampp}.
The attributes (such as \texttt{structures}) and properties (such as \texttt{surfaces}) of the semantic classes are extracted as symbolic nodes to generate a hierarchical tree, where each semantic class is represented as a hierarchical expression based on a root-to-leaf path.  
This hierarchy enable efficient coding of extensive information with a relatively small number of symbolic nodes to learn, resulting in a compact representation.
% This hierarchy capable to effectively modeled as a tree structure, enabling efficient symbolic coding of extensive information with a relatively small number of nodes to learn, resulting in a compact representation.
% \hrt{explain briefly why this code is more compact than flat representation, this is not very obvious for reader} % \hrt{explain clearly what is the symbolic code means, it is symbolic code as it represents a path from root to the leaf node of the tree }
\changed{For example, a binary tree with a depth of $10$ can represent $2^{10}$ classes, allowing for the encoding of 1,024 classes using only 20 codes (i.e., $2 \times 10$ through 2-dimensional Softmax coding at each level), enabling a reduction of up to $O(\log N)$.}
%%% previous version %%%% 
Building upon this concept, \oldMethodName{} \cite{li2024hi} was first introduced as an RGB-D Gaussian Splatting SLAM that employs a hierarchical tree to represent semantic information in an unknown environment, achieving strong performance with improved storage efficiency and operational speed.  

\oldMethodName{} \cite{li2024hi} have demonstrated the advantages and potential of incorporating hierarchical categorization for a semantic RGB-D SLAM system. However, it is limited by a simplistic hierarchical tree representation and lacks robust semantic optimization.
% \hrt{\sout{how you are different if following \oldMethodName{}. }}
To address these limitations and further improve its effectiveness, we propose \MethodName{}, a comprehensive neuro-symbolic semantic Gaussian Splatting SLAM framework that incorporates an advanced hierarchical representation for semantic information.
%  Our contributions include:
% 1). advanced and general tree
Different from \oldMethodName{}, we incorporate both geometric (i.e., shape and size details) and semantic information into the hierarchical symbolic tree expression using both Large Language Models (LLMs) and 3D Generative models. Specifically, we utilize a text-to-3D generative model \cite{siddiqui2024meshgpt} to extract shape information for each class and organize it into hierarchical levels. Additionally, we leverage LLMs to include both semantic and size information in the tree structure, effectively addressing the lack of size information in 3D generative models.
With the established tree, semantic information in the environment is represented as symbolic nodes along root-to-leaf paths, to be learned in an end-to-end manner during the SLAM operation.
% 2). multi-layer decoder for semantic -> miou reach 90%
To enhance semantic understanding, we introduce an improved hierarchical loss function that incorporates both inter-level and cross-level optimizations. %, improving our model's semantic understanding in 2D semantic rendering. 
This coarse-to-fine hierarchical approach based on the established hierarchical representation is beneficial for real-world applications, especially those involving observations from distant to nearby views.
% 3). dust3r as initialization and depth supervisory signal -> extend to monocular slam from rgbd only.
Furthermore, our SLAM system is extended to support both RGB-D and monocular inputs. We utilize the geometric output of the 3D feed-forward model \cite{wang2024dust3r} as a geometric prior, removing the depth dependency of existing SLAM methods. This enables \MethodName{} to perform semantic SLAM without relying on dedicated depth sensors, broadening the range of applicable scenarios.
Finally, we enhance and refine the 3D Gaussian Splatting SLAM to improve both performance and efficiency.
We conduct experiments on multiple synthetic and real-world datasets. The results demonstrate that our SLAM system outperforms or is on par with state-of-the-art NeRF-based and Gaussian-based SLAM methods. Additionally, our approach effectively reduces storage and training time.
% \changed{We conduct experiments on multiple synthetic and real-world datasets. The results demonstrate that our SLAM system outperforms existing methods in the semantic RGB-D setting, achieving competitive performance with state-of-the-art monocular methods. Additionally, our approach effectively reduces storage and training time.} % vs hier-slam,  no higher speed

The technical contributions of our work include:

1) 
We introduce an advanced hierarchical tree representation that integrates semantic, shape, and size information, leveraging the capabilities of both LLMs and 3D generative models. This tree-based coding efficiently compresses semantic information while maintaining its inherent hierarchical structure, resulting in reduced memory usage and shorter training times.

2)
We present a novel optimization approach for the proposed semantic hierarchical representation, integrating both inter-level and cross-level losses to ensure thorough optimization across all levels of the hierarchical semantic information.

3) 
We propose an improved semantic SLAM system capable of supporting both RGB-D and monocular inputs by leveraging a 3D feed-forward model. To the best of our knowledge, \MethodName{} is the first Gaussian Splatting based semantic SLAM system to support monocular input, significantly reducing sensor requirements and broadening its application scope.
% we also updated several modules to enhance the SLAM system

4) 
We conduct extensive experiments on both synthetic and real-world datasets, comprehensively demonstrating the effectiveness of our method and its improvements.
% \mgb{The intro is quite long, given the related work is still to come. It would be good to summarize further anything that also appears in the related work.}
%%%%% extend version %%%%%%%
% \sout{This paper builds upon our previous work \cite{li2024hi}, with the following major extensions:  1) The tree construction process is refined through the integration of 3D generative models and enhanced utilization of LLMs, resulting in a better symbolic tree structure.  2) Semantic optimization is improved with the introduction of an advanced hierarchical loss calculation.  3) The RGB-D semantic SLAM system is extended into a comprehensive framework capable of handling both RGB-D and monocular inputs by leveraging 3D feed-forward models.  Additionally, we include new real-world datasets and experiments to further validate the proposed approach.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Work}
% 1) visual SLAM with and without semantic
\subsection{Visual SLAM system}

    Visual Simultaneous Localization and Mapping (SLAM) using visual sensors is a long-standing and active research topic that focuses on the simultaneous estimation of ego-motion and the reconstruction of a global 3D map. 
    Traditional SLAM methods \cite{mur2015orb, mur2017orb, campos2021orb, engel2017direct}, developed over many years, have demonstrated both high accuracy and robustness, even in challenging environments. This makes them an indispensable frontend foundation for various real-world applications, such as autonomous driving and drones.
    In recent years, several new 3D representations have emerged in the 3D computer vision fields. With the advent of neural implicit representations, numerous works \cite{sucar2021imap, zhu2022nice, zhu2024nicer, johari2023eslam} have demonstrated the effectiveness of NeRF-based SLAM approaches. These new representations introduce novel capabilities to the SLAM community, including enhanced rendering capability. %, novel view synthesis, and the ability to fully leverage neural networks.
    3D Gaussian Splatting is the latest advancement in the 3D vision field, offering significantly faster rendering speed and strong performance compared with neural implicit representations. Recent works \cite{keetha2023splatam, matsuki2024gaussian, yugay2023gaussian, huang2024photo} have demonstrated new SLAM capabilities by integrating 3D Gaussian Splatting techniques, further pushing the boundaries of real-time scene understanding and reconstruction.

% 2) Neural Implicit semantic SLAM with and without semantic
\subsection{Neural implicit visual SLAM}

    Neural Radiance Fields (NeRF) \cite{mildenhall2021nerf} is a neural rendering technique that represents 3D scenes using a fully connected neural network.
    iMap \cite{sucar2021imap} proposes a real-time keyframe-based RGB-D SLAM method that uses a multilayer perceptron (MLP) as the sole scene representation.
    Nice-SLAM \cite{zhu2022nice} introduces a dense SLAM system that incorporates multi-level local information by employing a hierarchical scene representation.
    Nicer-SLAM \cite{zhu2024nicer} extends this approach by proposing a monocular SLAM system with a hierarchical neural implicit map representation, eliminating the dependency on depth information.
    Integrating SLAM with semantic understanding, many works \cite{li2023dns, zhu2023sni, haghighi2023neural} have explored neural implicit representations for semantic mapping and localization tasks. 
    DNS-SLAM \cite{li2023dns} incorporates 2D semantic priors with a coarse-to-fine geometry representation to embed semantic information into the map.
    SNI-SLAM \cite{zhu2023sni} integrates geometry, appearance, and semantic features into a shared feature space to enhance the robustness of the SLAM method.
    However, these approaches are constrained by the slow convergence of neural implicit map representations, which often leads to performance degradation when coupled with semantic objectives \cite{Hu2022CVPR, Liu2023ICCV}. 
    In contrast, Gaussian Splatting provides significant advantages, including fast rendering performance and high-density reconstruction quality, making it a promising alternative for map representation.

    % Semantic SLAM has been a long-standing research focus in computer vision and robotics \cite{chang2021kimera, li2023textslam, rosinol2020kimera, li2020textslam, sualeh2019simultaneous}. 
    

    % NeRF (Neural Radiance Fields) is a neural rendering technique that represents 3D scenes using a fully connected neural network. It learns to encode the volumetric appearance and geometry of a scene from a set of 2D images with known camera poses and can synthesize novel views by querying the learned model. NeRF models a scene as a continuous function that maps a 3D point and a viewing direction to color (RGB) and density (opacity). It is trained using a differentiable rendering process, optimizing the network by minimizing the difference between rendered and observed images.

% 1) 3D Gaussian Splatting SLAM with and without semantic
\subsection{3D Gaussian Splatting SLAM}
    % 3d gs slam without semantics
    3D Gaussian Splatting has recently gained attention as a promising 3D representation due to its fast rendering performance and high-density reconstruction quality. 
    Leveraging this 3D representation, SplaTAM \cite{keetha2023splatam} uses silhouette guidance for pose estimation and map reconstruction in RGB-D SLAM systems, while MonoGS \cite{matsuki2024gaussian} supports both monocular and RGB-D SLAM. 
    Photo-SLAM \cite{huang2024photo} utilizes 3D points generated by ORB-SLAM as position parameters for Gaussian Splatting primitives within the SLAM system.
    % GS-SLAM \cite{yan2024gs} employs a coarse-to-fine camera tracking method with sparse Gaussian selection, and Photo-SLAM \cite{huang2024photo} combines ORB-SLAM3 for pose estimation with 3D Gaussian Splatting to create global maps. 
    % Gaussian-SLAM \cite{yugay2023gaussian} further enhances the framework by utilizing pose results from DROID-SLAM \cite{teed2021droid} to manage active and inactive 3D Gaussian submaps.
    These studies highlight the strong potential of 3D Gaussian Splatting in SLAM tasks across various scenarios \cite{keetha2023splatam, matsuki2024gaussian, yan2024gs, huang2024photo, yugay2023gaussian}. 
    However, integrating semantic understanding into SLAM presents a significant challenge, as it requires jointly solving three high-dimensional optimization problems with varying value ranges and convergence behaviors, while also significantly increasing storage requirements. 
    %% our work
    % In this paper, we address this challenge by employing hierarchical coding for semantic information and developing an effective optimization strategy to ensure robust performance.
    % Gaussian Splatting Semantic SLAM
    % With the recent rise of 3D Gaussian Splatting, there have been attempts to integrate semantic information into the 3D Gaussian Splatting SLAM system.
    Focus on the integration of semantic information into the Gaussian Splatting SLAM, SGS-SLAM \cite{li2024sgs} added RGB 3-channels to create a semantic visualization map to avoid true semantic labels learning.
    SemGauss-SLAM \cite{zhu2024semgauss} uses a flat semantic representation supervised by a large pre-trained foundation model. However, these methods overlook the natural hierarchical structure of the real world. Additionally, relying on large foundation models increases the complexity of the neural network and its computational demands, with performance largely dependent on the embeddings from these pre-trained models.
    Eliminating the dependency on foundation models, this work proposes a simple yet effective hierarchical representation for semantic understanding in the SLAM system which largely reduces the training storage and time. 


% \subsection{Feed-forward Methods?} % review

\section{Method}

%%%% FIGURE %%%%
\begin{figure*}[t!]		
    \centering  
    % left bottom right top
    \includegraphics[width=1.0\textwidth, trim=0mm 100mm 40mm 0mm, clip]{pic/pipeline.pdf} \\
    \caption{\textbf{Left:} Overview of the \MethodName{} pipeline. The global 3D Gaussian map is initialized using the first frame of the video stream input. The system then alternates between the \textit{Tracking} and \textit{Mapping} steps as new frames are processed (see Section III-D). In the RGB-D setting, depth is directly obtained from sensor input, whereas in the monocular setting the 3D feed-forward method (DUSt3R) is used to generate a geometric prior for depth estimation (see Section III-C).  
    \textbf{Top Right:} Hierarchical representation of semantic information. The Tree Generation process leverages the capabilities of both LLMs and 3D Generative Models in a top-to-bottom manner. This hierarchical tree is used to establish a symbolic coding for each Gaussian primitive (see Section III-A). Additionally, we introduce a novel loss function that combines Inter-level Loss \( L_\text{Inter} \) and Cross-level Loss \( L_\text{Cross} \) to optimize the hierarchical semantic representation (see Section III-B).  
    \textbf{Bottom Right:} An example of hierarchical semantic rendering.}  
    \label{fig:method_pipeline}
\end{figure*}
%%%% FIGURE %%%%
% \hrt{\sout{You need remind the reader the main contributions when talking about pipeline. }}
% \changed{To address the significant computational burden introduced by incorporating semantic information into the Gaussian Splatting SLAM system, we propose utilizing a hierarchical symbolic tree to represent and compress semantic understanding within the environment. This representation can be learned in an end-to-end manner during SLAM operation, improving efficiency while maintaining rich semantic information. }
% To establish a semantic Gaussian Splatting SLAM system, a straightforward approach is to augment 3D Gaussian primitives with flat softmax-based semantic representations. However, this flat representation can easily lead to parameter explosion as scene complexity increases. This issue is further amplified by the inherently storage-intensive nature of 3D Gaussian Splatting and the computational burden of jointly optimizing camera poses and global mapping. To overcome these challenges, 
% \changed{We propose a novel hierarchical representation that encodes semantic information using a root-to-leaf hierarchical symbolic structure, which not only comprehensively captures the hierarchical attributes of semantic information, but also effectively compresses storage requirements, while enhancing semantic understanding with valuable scaling-up capability.  
% During the SLAM operation, the global hierarchical symbolic semantic messages, along with camera poses and the 3D Gaussian map, are jointly optimized using the proposed hierarchical loss functions, leveraging observations from the input video stream. 
% To further expand the applicability of our method, we extend our RGB-D SLAM system to support monocular input, eliminating the dependency on depth information.}
% mono?

% Our \MethodName{} pipeline is illustrated in \Fig{fig:method_pipeline}. 
% For hierarchical tree generation, we combine the geometric shape information embedded in pre-trained 3D Generative Models \cite{siddiqui2024meshgpt} with clustering operations, along with size and semantic information obtained from LLMs. Additionally, LLMs are utilized to generate summarized shape captions for each geometric clustering result, which further increases the interpretability of the established tree structure.  
% Using the generated hierarchical symbolic tree, the entire semantic SLAM pipeline operates on online video input, supporting both RGB-D and monocular settings. Global semantic information is learned end-to-end through Inter-level and Cross-level semantic losses.  
% Specifically, for the monocular SLAM setting with missing depth, we adopt a feed-forward method \cite{wang2024dust3r} to generate geometric priors. These are used to further supervise the monocular optimization during operation.  


The entire pipeline of our method is illustrated in \Fig{fig:method_pipeline}. Within \MethodName{}, we model the semantic space as a hierarchical tree, where semantic information is represented as a root-to-leaf hierarchical symbolic structure. This representation not only comprehensively captures the hierarchical attributes of semantic information, but also effectively compresses storage requirements, while enhancing semantic understanding with valuable scaling-up capability.
To construct an effective tree, we leverage both LLMs and 3D Generative Models to integrate semantic and geometric information. 
Next, each 3D Gaussian primitive is augmented with a hierarchical semantic embedding code, which can be learned in an end-to-end manner through the proposed Inter-level and Cross-level semantic losses.
Furthermore, our method is extended to support monocular input by incorporating geometric priors extracted from the feed-forward method, thereby eliminating the dependency on depth information.
The following sections provide detailed descriptions for each part of our \MethodName{}.

\subsection{Hierarchical representation}
% \hrt{it is important to explain why we need tree representation at all and how it provide us efficient representation. why it is symbolic representation. your sections still only jump into the details and explain ``how I am doing this'' rather than explaining ``why I am doing this''. I need every section again explain your motivation and ``why'' before talking about the dteails}
\changed{We propose hierarchical symbolic representation for semantic information to provide  improved efficiency, compression, and compactness for semantic SLAM, making it well-suited for multi-parameter joint optimization and enabling scalability to complex scenes.
In this section, we provide a detailed explanation of the hierarchical tree parametrization and its generation process.}

% The semantic information in \MethodName{} is organized in a symbolic hierarchical tree structure.  
% We detail the tree parametrization and its generation process in the following subsections.

\subsubsection{Tree Parametrization} 
The hierarchical tree $\bm{G}$ adopted in \MethodName{} is represented as the set of vertices and edges, $\bm{G} = (\bm{V}, \bm{E})$.
The vertices set $\bm{V} = \cup_{l=0}^{L}\left\{\bm{v}_{l}\right\}$ comes from the classes from all tree levels, where $\left\{\bm{v}_{l}\right\}$ represents the set of nodes at the $l$-th level of the tree, 
and we use $L$ to represent depth of the whole tree.
The edge set $\bm{E} = \cup_{m=0}^{L-1}\left\{\bm{e}_{m}\right\}$ captures the subordinate relationships, reflecting semantic attribution, size message, and geometric knowledge. % $m$ represents the relation level of the tree.
For the $i$-th semantic class $g^{i}$, which is treated as a single leaf node in the tree view, its hierarchical expression is:
\beq
g^{i} = \{ v_{l}^{i}, e_{m}^{i} \mid l = 0,1,...,L; \; m = 0,1,...,L-1 \},
\label{eq:single_class_tree}
\eeq
which corresponds to the root-to-leaf path: $ g^{i} = v_{0}^{i} \xrightarrow{e_{0}} v_{1}^{i} \xrightarrow{e_{1}} \cdots \xrightarrow{e_{L-2}} v_{L-1}^{i} \xrightarrow{e_{L-1}} v_{L}^{i}$, as illustrated in \Fig{fig:tree_rep}.
Take a semantic class $ g=\texttt{Bed}$ as an example, a 5-level tree symbolic coding can be represented as:
$\left\{v_{0}:\texttt{LargeItems}\right\} \rightarrow \left\{v_{1}: \texttt{Furnishings}\right\} \rightarrow \left\{v_{2}: \texttt{BedroomFurniture}\right\} \rightarrow \left\{v_{3}: \texttt{Rectangular}\right\} \rightarrow \left\{v_{4}: \texttt{Bed}\right\}$.
The hierarchical attributes and properties of class $g^i$ are represented by the symbolic node information $v_{l}^{i}$, while its relationships such as `include' and `possessing' are represented by the edge information \( e_{m}^{i}: \rightarrow \). 
In this way, every semantic class can be coded in a progressive, symbolic hierarchical manner, incorporating both semantic and geometric perspectives.
% \hrt{what is this symbolic code and how it is related to the tree. clearly explain, and again why it is compact. the link between your arguments are not very clear}. 
Moreover, the standard flat representation can be seen as a single-level tree coding from the hierarchical viewpoint.
Herein, we shall omit the superscripts (class indexes) of variables for clarity and compact notation.

\subsubsection{Tree Generation with LLMs and 3D Generative Models}
% \hrt{why do you use LLM and 3D gen models for your tree generation? why it is imprtant to have those information in your tree? why not any arbitrary tree? how tree desin affect anything on your SLAM? you have some arguments in the next sentences, but not yet very clearly answered those}
%% add in hier-slam++
% 1. high-level summary
% semantic & geometric info in the scene

To represent semantic information both accurately and comprehensively, an effective tree structure should incorporate information from both semantic and geometric perspectives, as these are the two crucial types of information in an unknown environment, which are essential for achieving a comprehensive understanding of the global 3D environment.
% 1) LLMs for semantic clustering
For capturing semantic meanings, such as functional relationships and summary description labels, we use Large Language Models (LLMs), specifically GPT-4 Turbo \cite{gpt4o_turbo2024}, known for its strong commonsense reasoning and language understanding capabilities.
% Semantic meanings and relationships are inherently expressed through language, making LLMs particularly well-suited for this task.
% 2) Generative models for geometric embedding
\changed{On the other hand, to capture general geometric information without dataset limitations, we employ 3D Generative Models, specifically text-to-3D models, as they have the capability to generate generalized and unified 3D objects with accurate shape information based on text prompts after training.
We adopt the text-to-3D approach MeshGPT \cite{siddiqui2024meshgpt}, which learns 3D symbolic embeddings with the local mesh geometry and topology of 3D assets, making it well-suited for our purpose.
% We adopt the text-to-3D approach MeshGPT \cite{siddiqui2024meshgpt}, which learns latent quantized embeddings encoding (a 3D discrete symbolic embedding) the local mesh geometry and topology of 3D assets, making it well-suited for our purpose. %  within symbolic embeddings\
% \hrt{why do you need text to mesh? what are inputs and outputs and why the input is text? explain clearly why you need such model}
However, the text-to-3D method only generates 3D objects within a unified coordinate system, omitting size information.}
% for it is the state-of-the-art approach which leveraging the encoder-decoder structure
% , i.e., MeshGPT \cite{siddiqui2024meshgpt}. \hrt{why do you need text to mesh? what are inputs and outputs and why the input is text? explain clearly why you need such model}
To address this issue, we complement the geometric shape information with the size attributes derived from LLMs, to capture the comprehensive geometric information for each semantic class. 
% 3) summary
The leverage of both LLMs and 3D Generative Models enables the creation of an elaborate and well-structured hierarchical tree representation, where each level reflects a distinct aspect of a class's attributes, enhancing the depth and utility of the global 3D understanding.
The following part provides a detailed explanation of the tree generation process.

% 2. intro seperately
As illustrated in \Fig{fig:tree_rep}, the tree generation process follows a top-to-bottom manner.
At the upper level, the initial grouping divides all classes into several subgroups based on their typical physical sizes in scenes, obtained through prompt queries to the LLM:
\beq
% \{ v_{l}, e_{m} \}_{l = 0,1}^{m = 0} = LLM^{Size} \{ g_{leaf} \}
\{ v, e \}_{0} = \mathcal{L}^\text{size} \{ g \}
\eeq
\noindent where $\mathcal{L}^\text{size}$ represents the LLMs usage for the size message, and the subscript in \(\{ v, e \}_{0}\) represents the generation at the initial levels. 
The clustering results categorize classes into groups such as \texttt{small items}, \texttt{medium items}, and \texttt{large items}. We provide details of the specific prompt used for the LLM in Appendix 1.

Next, the classes within each group are used as input prompts to LLMs for further clustering based on their semantic relationships, particularly their functional properties:
\beq
% \{ v_{l}, e_{m} \}_{l = 2,...}^{m = 1,...} = LLM^{Function} \{ g_{leaf} \}
\{ v, e \}_{1} = \mathcal{L}^\text{func} \{ g \}
\eeq
\noindent where $\mathcal{L}^\text{func}$ denotes the LLMs clustering based on functional relationships, and the subscript of $\{ v, e \}_{1}$ stands for the following levels generation. We demonstrate the detail prompt used for the LLM in Appendix 1.

After multiple levels generation using LLMs, 3D Generative Models are employed to cluster classes based on their geometric shapes. Specifically, we adopt MeshGPT~\cite{siddiqui2024meshgpt} to capture the geometric information for each semantic class. \changed{MeshGPT~\cite{siddiqui2024meshgpt} is a text-to-3D model that encodes object shape and internal topology relationships into latent quantized embeddings, forming a symbolic 3D representation in the shape space.}
% \hrt{since this quantized this can also refered as symbolic 3D embedding, which each quantized value represent a code in shape space}.
Given a text prompt (i.e., the class name) as input to the pre-trained MeshGPT, we extract the latent quantized embeddings to represent the shape information for the class:
\begin{equation}
\changed{\bm{z} = \mathcal{E}\{g\} \in \mathbb{N}^{F \times 2}}
\end{equation}
% \hrt{is it quantized or it is continous? ``$\mathbb{R}$'' means it is real continuous value, not discrete }
\noindent where $g$ is the class name provided as input. $\mathcal{E}$ represents the pre-trained MeshGPT encoder, and $\bm{z}$ is the quantized geometric embedding extracted from the pre-trained encoder. For each object's embedding, $F$ is the number of triangle faces used in the mesh representation. 
Since the number of triangle faces varies across different semantic objects, i.e., different values of $F$, we first compute the average shape embedding $ \bm{z}_{\text{avg}} \in \mathbb{R}^{1 \times 2} $ for each object. The averaged embeddings from all classes are then concatenated and subsequently normalized to a standard distribution (zero mean and unit variance) to ensure consistency across classes:
\beq
% \tilde{\bm{z}} = \mathcal{N}(\text{Concat}(\bm{z}_\text{avg}))
% \tilde{\bm{z}} = \mathcal{N}(\oplus(\bm{z}_\text{avg}))
\tilde{\bm{z}} = \mathcal{N}\{ \mathcal{C}(\bm{z}_\text{avg})\}
\eeq
\noindent where $\mathcal{N}$ and $\mathcal{C}$ denote the normalization and concatenation operations, respectively.
Next, the normalized embeddings $\tilde{\bm{z}}$ are clustered using K-means++, denoted by the operator $\mathcal{K}$. % , where $K$ is as follows:
% \pjs{\sout{Why not operator $\mathcal{K}$? because $\mathcal{C}$ is related to cluster operator, and if need detail for how many clusters we generate, we can use $K$ to represent kmeans++ grouping number.}}
To address the lack of descriptive labels for each clustering result, we further employ LLMs to summarize and refine the clustering outcomes:
\beq
\{ v, e \}_{2} = \mathcal{L}^\text{sum} \{ \mathcal{K}(\tilde{\bm{z}}) \}
\eeq
\noindent where $ \mathcal{L}^\text{sum} $ represents the use of LLMs for summarization.  
The generated descriptive labels, such as \textit{"box", "flat", "soft"}, summarize the common geometric attributes within each grouping result, further enhancing the interpretability of the hierarchical tree.
The details are illustrated in Appendix 1.

Whenever LLMs are employed, we ensure balanced, meaningful, and descriptive outputs through carefully designed prompt inputs, as detailed in
Appendix 1. 
Furthermore, an additional LLM validator \cite{li2024hi} is utilized after each generation level to verify clustering completeness and accuracy, ensuring that no classes are missing or incorrectly included (see Appendix 1 for detail).  
Finally, a global tree evaluation is conducted to detect and resolve any duplicates or omissions across tree levels, ensuring a fully balanced and complete hierarchical tree structure.
We use manual inspection as the final verification to ensure the correctness and rationality of the holistically generated tree. 

% For each level's generation, a LLM validator \cite{li2024hi} ensures clustering completeness and correctness, guaranteeing no missing or extraneous classes. Finally, a global tree evaluation checks for duplicates or omissions across tree levels, ensuring a fully balanced and complete hierarchical tree structure.
% % The final hierarchical tree is represented in JSON format.
% The prompt used for this step ensures balance, meaningfulness, and descriptive group names is demonstrated in \tocheck{Appendix}.


%% =======

%%%% FIGURE (scannet large sem) %%%%
\begin{figure}[!t]		
    \centering  
    % left bottom right top
    \includegraphics[width=0.9\textwidth, trim=0mm 85mm 50mm 0mm, clip]{pic/tree_rep.pdf} \\
    \caption{\changed{Visualization of the hierarchical tree structure and semantic embedding.
    For \textbf{tree representation}, each semantic class is expressed hierarchically as $g = \{ v_{l}, e_{m} \}$, where edges $e_{m}$ are depicted as lines connecting tree nodes. The nodes within the same level are illustrated via the same color.
    For \textbf{tree generation}, we utilize both LLMs and 3D Generative Models to extract and group messages.
    For \textbf{hierarchical semantic embedding}, we represent semantic information using a root-to-leaf symbolic path. We propose two types of representations: the one-hot representation and the binary representation, which can reduce the original dimensionality by up to $O(\log^2 N)$.}}
    % Each level is first assigned an $n$-dimensional code (as shown in the same color), which is then concatenated across all levels to construct the holistic hierarchical semantic embedding.
    % Here, $n$ corresponds to the maximum number of tree nodes at each level. By leveraging the tree structure, we effectively reduce the original number of semantic classes $N_{\text{original}}$ in the flat representation to $N = \sum n$. For instance, in the Replica dataset, the number of semantic categories is reduced to \textbf{1/5} of the original.}}
    \label{fig:tree_rep}  
\end{figure}
%%%% FIGURE %%%%

\subsection{Hierarchical optimization}

Based on the generated hierarchical tree structure, the semantic information is represented as compact hierarchical symbolic nodes, and further integrated into the 3D Gaussian primitives, learned in an end-to-end manner with the input frame stream.  
To ensure accurate semantic understanding, we introduce a semantic hierarchical loss, which consists of Inter-level and Cross-level losses. These losses are incorporated into the optimization process during the online SLAM operation.  
The following sections provide a detailed explanation of the proposed tree encoding and the loss calculation. 

\subsubsection{\changed{Tree Encoding}} 
% As discussed previously, one major limitation of 3D Gaussian Splatting is its large storage space requirement; even a simple indoor environment can include around $10^6$ Gaussian primitives. When adding semantic information, the parameter usage grows with the increase of scene complexity, which can easily exceed GPU storage capacity. In this paper, we leverage hierarchical representation to code semantic information.
As introduced in Eq.~\eqref{eq:single_class_tree}, the semantic class in \MethodName{} is represented hierarchically in a root-to-leaf symbolic manner based on the generated tree. This effectively compacts semantic information, facilitates SLAM joint optimization, and provides valuable scalability for semantic understanding in unknown environments. 
Specifically, we propose two types of hierarchical semantic embedding approaches, both of which compact the original semantic information. 
% We will detail these approaches in the following section.

\textbf{One-hot representation:} 
As demonstrated in \Fig{fig:tree_rep}, the hierarchical semantic embedding $\bm{h}$ is composed of the one-hot embeddings across all levels:
\beq
\bm{h} = \mathcal{C}(\bm{h}_{l}) \in \mathbb{B}^N, \quad \bm{h}_{l} \in \mathbb{B}^{n}
\eeq
\noindent where $l$ represents the $l$-th level of the tree, and $\mathcal{C}$ denotes the concatenation operation.
For each level's representation $\bm{h}_{l}$, we use $n$-dimensional one-hot codes (in colored boxes) to encode the symbolic nodes (in colored circles) from the tree of this level, as shown in \Fig{fig:tree_rep}. The dimension $n$ corresponds to the maximum number of nodes at the $l$-th level.
Using the one-hot representation, the overall dimension of the semantic embedding is the sum of the dimensions across all levels, given by $N = \sum n$. This enable a maximum reduction of up to $O(\log N )$ compared to the original dimension $N_{\text{original}}$ in the flat representation.  % $O(\log N)$
This hierarchical representation effectively compacts the semantic embedding, reducing storage requirements and facilitating joint optimization.


% \tocheck{As shown in the right part of the \Fig{fig:method_pipeline},} 
% The overall dimension of the semantic embedding is the sum of the dimensions across all levels, given by \( N = \sum n \), where the dimension \( n \) of each level's embedding \( \bm{h}^{l} \), depends on the maximum number of nodes at the \( l \)-th level. It is worth noting that with the carefully defined tree structure, $N$ is significantly smaller than the number of original semantic classes, which compacts the semantic information within the SLAM system 
% \hrt{you need mention that the code represent a path from root to leaf nodes and has complexity O(log m) for m classes while this complexity is linear, i.e. o(m) for flat representation }.


% bce version
% \hrt{very unclear writing. you need to explain first representation uses one hot encoding while we can further compress this code to lower dimension by compressing it to a lower dimension binary vector (not one hot contraint)  }
\textbf{Binary representation:} 
We propose a further compact version of our hierarchical semantic representation in this section. The binary representation of the hierarchical semantic embedding $\bm{b}$ consists of the binary embeddings $\bm{b}_{l}$ across all tree levels:
\beq
\bm{b} = \mathcal{C}(\bm{b}_{l}) \in \mathbb{B}^K, \quad \bm{b}_{l} \in \mathbb{B}^{k}
\eeq
\noindent where $\bm{b}_{l}$ is the binary representation converted from the one-hot semantic embedding $\bm{h}_{l}$ at each level: $\bm{h}_{l} \rightarrow \bm{b}_{l}$, as depicted in the last row example in \Fig{fig:tree_rep}.
For instance, if the one-hot embedding has a dimension of $n = 8$, this corresponds to a maximum of $8$ unique codes at this level (covering values from $0$ to $7$). After binarization, the dimension of the compact version is reduced to $k = 3$, since $2^3 = 8$.
By leveraging binary representation, the hierarchical coding dimension at each level is reduced to $k = \log n$, compared to the one-hot encoding with dimension $n$.
Consequently, the total semantic encoding dimension across all levels is given by $K = \sum k$, resulting in a more compact representation.
% Binarization is able to reduce the embedding dimension from $O(n)$ to $O(\log n)$, achieving exponential compression compared with the one-hot embedding.
% Consequently, the total semantic encoding dimension across all levels is given by $K = \sum k$, resulting in an overall reduction of up to $O(\log^2)$ compared to the original dimension $N_{\text{original}}$, ensuring a more compact representation.
% \changed{For instance, if the one-hot embedding has a dimension of $n = 8$, this corresponds to a maximum of $8$ unique codes at this level. After binarization, the dimension of the compact version is reduced to $k = 3$, since $2^3 = 8$. Binarization can reduce the embedding dimension from $O(n)$ to $O(\log n)$, achieving exponential compression.} 


% We propose a further compact version for our hierarchical sematic information this part. 
% The binary representation of the hierarchical semantic embedding $\bm{b}$ is consisted of the binary embeddings $\bm{b}_{l}$ among all tree levels:
% \beq
% \bm{b} = \mathcal{C}(\bm{b}_{l}) \in \mathbb{R}^K, \quad \bm{b}_{l} \in \mathbb{R}^{k}
% \eeq
% \noindent where $\bm{b}_{l}$ is the binary representation converted from its one-hot semantic embedding $\bm{h}_{l}$ of each level: $\bm{h}_{l} \rightarrow \bm{b}_{l}$.
% By leveraging binary representation, the hierarchical coding dimension at each level is reduced to $k = \log n$, compared to the original one-hot encoding with dimension $n$. \changed{For instance, if the original embedding has a dimension of $n = 8$, this corresponds to a maximum of $8$ unique codes at this level. After binarization, the dimension of the compact version is reduced to $k = 3$, since $2^3 = 8$. Binarization can reduce the embedding dimension from $O(n)$ to $O(\log n)$, achieving exponential compression.}  
% Consequently, the total semantic encoding dimension across all levels is given by $K = \sum k$, where $K < N$, ensuring a more compact representation.



% To further compact the dimension of the semantic embeddings, we propose a compact version of our method, by converting each level's embedding $\bm{h}_{l}$ into its binary representation $\bm{b}_{l}$:
% \beq
% \bm{h}_{l} \rightarrow \bm{b}_{l}, \quad \bm{b}_{l} \in \mathbb{R}^{k}
% \eeq
% \noindent where $\bm{b} = [b_0, b_1, \dots, b_k]$.
% By leveraging binary representation, the hierarchical coding dimension at each level is reduced to $k = \log n$, compared to the original one-hot encoding with dimension $n$. \changed{For instance, if the original embedding has a dimension of $n = 8$, this corresponds to a maximum of $8$ unique codes at this level. After binarization, the dimension of the compact version is reduced to $k = 3$, since $2^3 = 8$. Binarization can reduce the embedding dimension from $O(n)$ to $O(\log n)$, achieving exponential compression.}  %This implies that the original one-hot encoding can be efficiently replaced by a $3$-dimensional binary representation, significantly reducing dimensionality
% Consequently, the total semantic encoding dimension across all levels is given by $K = \sum k$, where $K < N$, ensuring a more compact representation. 

\subsubsection{Loss Calculation} 
% \hrt{explain why you need two losses somewhere? empirically you find it better? }
To fully optimize the hierarchical semantic message both within each level and across different levels of the tree, the proposed hierarchical loss consists of Inter-level loss $L_\text{Inter}$ and Cross-level loss $L_\text{Cross}$:
\beq
L_\text{{Semantic}} = \omega_1 L_\text{{Inter}} + \omega_2 L_\text{{Cross}}
\eeq
\noindent where \( \omega_1 \) and \( \omega_2 \) are balancing coefficients for each loss term.
Specifically, the Inter-level loss $L_\text{Inter}$ is employed within each level's embedding $\bm{h}^{l}$ to ensure the optimization of semantic information within each level:
% \beq
% L_\text{{Inter}} = \sum_{l=0}^{L} L_\text{{ce}}(\text{softmax}(\bm{h}^{l}), \mathcal{P}^{l})
% \eeq
\beq
L_\text{{Inter}} = \sum_{l=0}^{L} L_\text{{ce}}(\mathcal{S}(\bm{h}^{l}), \mathcal{P}^{l})
\eeq
\noindent where $L_\text{{ce}}$ indicates the cross-entropy semantic loss, and $\mathcal{P}^{l}$ denotes the semantic ground truth at level $l$. $\mathcal{S}$ stands for the Softmax operation, converting embeddings into probabilities. 
% compact version
For our proposed compact version with binary representation $\bm{b}^l$, we replace the original cross-entropy loss with binary cross-entropy loss $L_\text{bce}$ for the binary Inter-level loss calculation:
\beq
% L_\text{Inter}^\text{bin} = \sum_{l=0}^{L} L_\text{{bce}}(\text{softmax}(\bm{b}^{l}), \mathcal{P}^{l})
L_\text{Inter}^\text{bin} = \sum_{l=0}^{L} L_\text{{bce}}(\mathcal{S}(\bm{b}^{l}), \mathcal{P}^{l})
\eeq
% \noindent where $\bm{b}^{l}$ represents its binary encoding, as introduced in the previous part.

% extend in hier-slam++
To enforce accurate understanding across tree levels, we introduce the Cross-level loss, which is computed using a semantic decoder based on the holistic hierarchical codes $\bm{h}$. 
Specifically, we first perform a 2D convolution operation on the semantic embedding, following an activation layer (i.e., ReLU).
Then, a second 2D convolution layer is applied to map the hidden embedding into the flat coding, followed by a $\text{softmax}$ operation to convert the embeddings into probabilities.
The overall Cross-level loss \( L_{\text{Cross}} \) is defined as:
\beq
L_{\text{Cross}} = L_{\text{ce}}\big(\mathcal{S}(\text{decoder}(\bm{h})), \mathcal{P}\big),
\eeq
\noindent where $\mathcal{P}$ represents the semantic ground truth, and the definition of $\text{decoder}(\bm{h})$ is:
\beq
\text{decoder}(\bm{h}) = \text{Conv} \big(\text{ReLU}(\text{Conv}(\bm{h}))\big)
\eeq


\iffalse
During the optimization for semantic information, we first use the Inter-level loss to initialize the hierarchical coding, followed by incorporating the Cross-level loss to refine the embedding. \hr{this should not be in method, but implementation details at the beginning of experiment section} Specifically, the semantic weights $\omega_1$ and $\omega_2$ are set to$1.0$ and $0.0$, respectively, for the first $\eta$ iterations, where $\eta$ is set to $15$. Afterwards, $\omega_1$ and $\omega_2$ are adjusted to $1.0$ and $5.0$, respectively.
\fi

\subsection{Monocular setting with geometric priority}

Accurate depth information provides valuable geometric cues for global map reconstruction and pose estimation in a SLAM system.  
In an RGB-D setting, depth signals are directly obtained from sensors.  
However, in a monocular setting without depth information, SLAM \cite{mur2015orb, mur2017orb, campos2021orb, engel2017direct} typically relies on triangulation to initialize the global mapping. Yet, due to the inherent coupling between pose estimation and mapping, this approach often suffers from relatively lower accuracy compared with RGB-D based methods.
In our \MethodName{}, we employ a feed-forward method as a geometric prior.
Specifically, we leverage DUSt3R \cite{wang2024dust3r} to provide depth geometric priors for monocular SLAM. DUSt3R is able to generate both a 3D point map and pose estimations from several sparse-view image inputs, which can serve as the geometric prior for the monocular SLAM.
The following section provides details on the monocular setting in our \MethodName{}.

% Without direct depth input from sensors, we employ DUSt3R \cite{wang2024dust3r} to predict the depth map, which serves as a crucial geometrical prior for the monocular SLAM system. The predicted depth is integrated into the SLAM pipeline, where it facilitates Gaussian initialization and provides a supervision signal for the system  optimization. 
% To address scale and shift discrepancies between the estimated depth and the SLAM system's current estimation, we utilize a least squares algorithm to determine the initialization parameters for scale and shift. These parameters are then seamlessly incorporated into the joint optimization process, ensuring consistency and accuracy in the overall system. 

% 1. dust3r
\subsubsection{Geometric prior acquisition}
% To obtain the accurate geometric prior, we sample $N$ frames from the whole sequence to feed into the DUSt3R \cite{wang2024dust3r}.

DUSt3R \cite{wang2024dust3r} supports sparse view inputs with a minimum of two views. Larger view inputs enhance reconstruction accuracy by providing more information, yet at the cost of increased computational demands. 
To balance reconstruction accuracy and computational efficiency, we sample every \( S \) frames with a skip of \( \delta \) frames from the input video to form an image set \( \{I_i\} \). 
\beq
% \{I_i \in \mathbb{R}^{H\times W\times3} | i = i_{0} + \delta \cdot (n - 1), n = 0, 1, \dots, M\}
\{I_i \; | \; i = i_{0} + \delta \cdot (s - 1), \; s = 1, \dots, S\}
\eeq 
% \pjs{If we are sampling from a video, we could estimate the maximum pose change (in terms of distance) since we know the time difference between images, and can have an estimate of the camera max speed? But thats for the next paper I guess!}

Each image set is then fed into DUSt3R to generate geometric information for every image frame, specifically the depth maps.  
Although DUSt3R is also able to provide pose predictions for each input frame, we observe that the accuracy of estimated pose is relatively lowperforming even worse than a constant velocity pose prediction in the SLAM system based on our experiments. Therefore, only the estimated geometric information from DUSt3R is leveraged in our method.

% Depth transformation
% Using the estimated pose, we transform the point clouds into the current frame's camera coordinate. Subsequently, we extract the $z$-axis from the transformed point cloud to produce the depth map $\hat{D}_i$ for each frame $I_i$, which are further served as the geometric prior for the monocular setting of \MethodName{}.
% $\hat{\mathbf{T}}_i$

% 2. scale+shift
\subsubsection{Geometric prior correction}
The scale of the estimation for each image set varies and also differs from the scale of the online global map estimation during SLAM operation, requiring an additional geometric prior correction operator.

Firstly, we calculate the initial scale and shift parameters by comparing the rendering results from the current global map prediction with the depth prior obtained from DUSt3R.
Specifically, the depth map $D_c$ and the silhouette map $S_c$ for current frame $I_c$ are rendered based on the global Gaussian map $G$ as well as the pose estimation $\mathbf{T}_c$ for the current frame:
\beq
\{ D_c, S_c \} = \mathcal{R}(G, \mathbf{T}_c)
\eeq
\noindent where $\mathcal{R}$ represents the rasterization procedure for the 3D Gaussian map. 
Next, the global scale and shift parameters $\lambda_c, \tau_c$ are calculated by comparing the predicted depth $\hat{D}_c$ from DUSt3R with the rendered depth $D_c$ using the least squares estimation on the visible regions $M_c$ defined by $S_c$:
\beq
\{ \lambda_c, \tau_c \} = \arg\min \left( \hat{D}_c^{\text{align}} - D_c \right)^2 M_c
\eeq
\noindent where $\hat{D}_c^{\text{align}} = \lambda_c \cdot \hat{D}_c + \tau_c$.

Secondly, the estimated parameters are fed into the optimization as initialization and further optimized jointly within the map optimization via the depth supervision loss:
\beq
% \{ \lambda_c, \tau_c, G \} = \arg\min \big( | \hat{D}_c^{\text{align}} - D_c | M_c + L_\text{reg} \big)
\{ \lambda_c, \tau_c, G \} = \arg\min | \hat{D}_c^{\text{align}} - D_c | M_c
\eeq
% L-1?
% \noindent where $L_\text{reg}$ represents the sum of the absolute errors between the estimated parameters and their initialization values, serving as a regularization term to prevent the optimization from getting trapped in local minima.
In this way, the depth prediction can be continuously refined online with increasing observations, providing geometric supervisory signals during SLAM optimization. This helps guide global reconstruction and pose estimation, leading to improved results throughout the SLAM process.


% paper/experimentdepth/depthdepth-error
% todo-fig


\subsection{Semantic Gaussian Splatting SLAM}

The global semantic 3D Gaussian Splatting SLAM system is illustrated in the left part of \Fig{fig:method_pipeline}.  
Based on the established semantic 3D Gaussian representation, global 3D reconstruction (Mapping) and pose estimation (Tracking) are alternately performed with the video stream input.  
For the RGB-D setting, depth information for each image frame is obtained directly from depth sensors.  
For the monocular setting, we employ a feed-forward method to obtain the geometric prior, as introduced in the previous section.  
Further details of our \MethodName{} system are provided in the following section.

\subsubsection{Semantic 3D Gaussian representation} 
The semantic 3D Gaussian representation is represented as the Gaussian primitives with the hierarchical semantic embedding.
Motivated by \cite{keetha2023splatam}, we adopt the isotropic and view-independent Gaussian integrated with the semantic embeddings.
Each semantic Gaussian is composed of color $\bm{c}$, the center position $\bm{\mu}$, the radius $r$, the opacity $o$, and its semantic embedding $\bm{h}$.  % $\mu \in \mathbb{R}^3$
\changed{And the influence of each Gaussian $G(\bm{X})$ \cite{kerbl20233d} according to the standard Gaussian equation is:}
\beq
G(\bm{X}) = o \: \exp\left(-\frac{||\bm{X}-\bm{\mu}||^2}{2r^2}\right)
\eeq
\noindent where $\bm{X}$ stands for the 3D point. 

Following \cite{kerbl20233d}, every semantic 3D Gaussian primitive within the current global map is projected to the 2D image space using the tile-based differentiable $\alpha$-compositing rendering. % In this paper, we adopt a similar rendering procedure for the semantic map. 
% The screen is first split into tiles, and all operations are performed within each tile in parallel. The highly intersected Gaussians within each view frustum are maintained and sorted in a front-to-back manner. Based on the sorted semantic Gaussians, the semantic map is rasterized as follows:
\changed{The semantic map $H$ is rendered as follows:}
\begin{equation}
\changed{H = \sum_{i=1}^{n} \bm{h}_{i} \alpha_{i}(\bm{X}) T_{i} \quad \text{with} \quad T_{i} = \prod_{j=1}^{i-1} (1 - \alpha_{j}(\bm{X}))}
\end{equation}
% \beq
% H = \sum_{i=1}^{n}\bm{h}_{i} G_{i}(\bm{X}) T_{i}    % (\bm{x})
% \eeq    % H/h_i
% \noindent with
% \beq
% T_{i} = \prod_{j=1}^{i-1} (1-G_{j}(\bm{x}))
% \eeq
% \noindent where $\bm{x}$ represents each 2D image pixel.
The rasterized color image $C$, depth image $D$, and the silhouette image $S$ are defined as follows:
\begin{align}
C = \sum_{i=1}^{n} \bm{c}_{i} \alpha_{i}(\bm{X}) T_{i}, \;
D = \sum_{i=1}^{n} \bm{d}_{i} \alpha_{i}(\bm{X}) T_{i}, \;
S = \sum_{i=1}^{n} \alpha_{i}(\bm{X}) T_{i}
\end{align}
% \begin{equation}
% C(\bm{x}) = \sum_{i=1}^{n}\bm{c}_{i} G_{i}(\bm{X}) T_{i} \qquad
% D(\bm{x}) = \sum_{i=1}^{n}\bm{d}_{i} G_{i}(\bm{X}) T_{i}
% \end{equation}
% The visible mask for each frame is rendered as the silhouette image:
% \begin{equation}
% S(\bm{x}) = \sum_{i=1}^{n} G_{i}(\bm{X}) T_{i}
% \end{equation}
Different from previous work \cite{keetha2023splatam}, which uses separate forward and backward Gaussian modules for different parameters, our \MethodName{} system adopt a unified forward and backward modules that processes all parameters, including semantics, color, depth, and silhouette imagessignificantly improving overall runtime efficiency.

\iffalse
\textbf{Initialization.} 
In the initialization phase, the estimated parameters are initialized in different ways.
% semantic
The semantic embedding of each Gaussian primitive is initialized randomly.
% camera pose
For the pose initialization of the first frame, the camera's rotation is set to the identity matrix, and its translation is set to zero. Its silhouette image is empty since all pixels are visible.
% global map
The global 3D Gaussian map is initialized using all observed pixels along with their depth observations. The color and center location of each Gaussian are set to the observed color and the 3D location of each 2D pixel, respectively. \hr{this should not be in method, but implementation details at the beginning of experiment section} The opacity of each Gaussian is initialized to $0.5$, and the radius is set to project to a one-pixel radius.
\fi

% 3. tracking
\subsubsection{Tracking}
The tracking step focuses on pose estimation for each frame.  
For each incoming frame, the constant velocity model is first applied to initialize its pose based on the previous frame's pose estimation.  
Following that, a pose optimization is performed with the global Gaussian map $G$ fixed:
\beq
\changed{\mathbf{T}_c = \arg\min_{\mathbf{T}_c} L_{\text{Track}}(G, \mathbf{T}_c)}
\eeq
\noindent where the tracking loss includes the rendering color and depth losses:
\beq
L_\text{{Track}} = M \big( w_{1} L_\text{{Depth}}(G, \mathbf{T}_c) + w_{2} L_\text{{Color}}(G, \mathbf{T}_c) \big)
\eeq
\changed{\noindent where $L_\text{Depth}$ and $L_\text{Color}$ denote the L1-loss for the rendered depth and color information. The rendering follows the tile-based differentiable $\alpha$-compositing procedure, utilizing the current pose estimation along with the current global map estimation.}
% We does not include semantic loss in the tracking optimization and because it has 
Here, weights $ w_{1}$ and $w_{2}$ are introduced to balance the two loss terms, and the optimization is only performed on the silhouette-visible image regions, defined as $M = (S > \delta)$.
For the RGB-D setting, depth supervisory signals are directly obtained from depth sensors, whereas for the monocular setting, we leverage the geometric prior derived from the feed-forward method.
% \hrt{I am not sure if I understand how camera parameter tracking works with the loss you mentioned. why we have no semantic in camera tracking. Perhaps you should explain }.


% 3. mapping
\subsubsection{Mapping} 
The global map information, including semantic information, is optimized in the mapping procedure while keeping camera poses fixed:
\beq
\changed{G = \arg\min_{G} L_{\text{Map}}(G, \mathbf{T}_i)}
\eeq
The mapping optimization losses include depth, color, and semantic losses:
\beq
L_\text{{Map}} = w_{3}  M  L_\text{{Depth}}(G, \mathbf{T}_i) + w_{4} L_\text{{Color}}'(G, \mathbf{T}_i) + w_{5} L_\text{{Semantic}} (G, \mathbf{T}_i)
\eeq
\noindent where $L_\text{{Semantic}}$ is the proposed semantic loss introduced in Section III-B, and $L_\text{{Color}}'$ is the weighted sum of SSIM color loss and L1-Loss. The weights $w_{3}$, $w_{4}$, and $w_{5}$ are used to balance the different loss terms.
Similar to the tracking process, depth supervisory signals come from sensors in the RGB-D setting.  
For the monocular setting, geometric priors derived from the feed-forward method are applied, and the two parameters (scale and shift) are further jointly optimized within the mapping process.


\section{\changed{Experiments}}

% \pjs{I would always show first to fourth on all tables, otherwise it seems like we just extend the ranking to make sure we are included}


\subsection{Experiment settings}

The experiments are conducted on both synthetic and real-world datasets, including ScanNet \cite{Dai2017scannet}, Replica \cite{straub2019replica}, and TUM-RGBD \cite{sturm12iros}.
% \textbf{Evaluation Metrics.} 
Following the evaluation metrics which used in previous SLAM methods \cite{keetha2023splatam, zhu2022nice}, we leverage ATE RMSE (cm) to quantify SLAM tracking accuracy.
For mapping evaluation, Depth L1 (cm) is used to measure accuracy.
To assess image rendering quality, we adopt PSNR (dB), SSIM, and LPIPS as evaluation metrics.
In line with previous works \cite{zhu2024semgauss, li2023dns, zhu2023sni}, we assess semantic rendering performance using mIoU (mean Intersection over Union across all classes) to represent global semantic information.  
To evaluate computational efficiency, we report the runtime of the proposed SLAM method.
For comparison, we benchmark our approach against state-of-the-art dense visual SLAM methods, including both NeRF-based and 3D Gaussian SLAM approaches, to demonstrate its effectiveness. Additionally, we compare with state-of-the-art semantic SLAM methods, covering both NeRF-based and Gaussian-based techniques, to highlight its capability in hierarchical semantic representation and scalability.
The experiments are conducted in the Nvidia L40S GPU.
% \pjs{We need to introduce Hier-SLAM++$^*$ here somewhere before the tables!}


For experimental settings, the semantic embedding of each Gaussian primitive is initialized randomly. The semantic optimization loss weights $\omega_1$ and $\omega_2$ are set to $1.0$ and $0.0$, respectively, for the first $\eta$ iterations, where $\eta$ is set as $15$. Afterwards, $\omega_1$ remains $1.0$, while $\omega_2$ is increased to $5.0$. This strategy means that we first apply the Inter-level loss to initialize the hierarchical semantic coding, followed by incorporating the Cross-level loss to refine the whole embedding. 
For tracking loss, we use $\delta=0.99$, with weights $w_{1}=1.0$ and $w_{2}=0.5$.
For mapping, the loss weights are set as follows: $w_{3}=1.0$, $w_{4}=0.5$, and $w_{5}=0.2$.

%%%% TABLE %%%%
\begin{table}[!tb]   % t
    \centering
    \caption{RGB-D Tracking performance ATE RMSE (cm) on the Replica. \\Best results are highlighted as \colorbox{green!30}{\textbf{FIRST}}, \colorbox{yellow!30}{SECOND}.} % The best result for each scene is highlighted in \colorbox{green!30}{\textbf{green}} and the second-best in \colorbox{yellow!30}{yellow}.
    \renewcommand{\arraystretch}{1.1} % Adjust the row height for better readability
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{lccccccccc}
        \hline
        \rowcolor{white}
        \toprule
        \textbf{Methods} & \textbf{Avg.} & \textbf{R0} & \textbf{R1} & \textbf{R2} & \textbf{Of0} & \textbf{Of1} & \textbf{Of2} & \textbf{Of3} & \textbf{Of4} \\
        \hline
        iMap \cite{sucar2021imap} & 4.15 & 6.33 & 3.46 & 2.65 & 3.31 & 1.42 & 7.17 & 6.32 & 2.55 \\
        NICE-SLAM \cite{zhu2022nice} & 1.07 & 0.97 & 1.31 & 1.07 & 0.88 & 1.00 & 1.06 & 1.10 & 1.13 \\  % paper
        Vox-Fusion \cite{yang2022vox} & 3.09 & 1.37 & 4.70 & 1.47 & 8.48 & 2.04 & 2.58 & 1.11 & 2.94 \\ % paper
        co-SLAM \cite{wang2023co} & 1.06 & 0.72 & 0.85 & 1.02 & 0.69 & 0.56 & 2.12 & 1.62 & 0.87 \\
        ESLAM \cite{johari2023eslam} & 0.63 & 0.71 & 0.70 & 0.52 & 0.57 & 0.55 & 0.58 & 0.72 & 0.63 \\
        Point-SLAM \cite{sandstrom2023point} & 0.52 & 0.61 & 0.41 & 0.37 & 0.38 & 0.48 & 0.54 & 0.69 & 0.72 \\
        % Gaussian Splatting SLAM \cite{} & 0.58 & 0.76 & 0.37 & 0.23 & 0.66 & 0.72 & 0.30 & 0.19 & 1.46 \\  % use our*
        {MonoGS-RGBD \cite{matsuki2024gaussian}} & 0.79 & 0.47 & 0.43 & 0.31 & 0.70 & 0.57 & 0.31 & \cellcolor{green!30}0.31 & 3.2 \\  % (use our) Gaussian Splatting SLAM
        SplaTAM \cite{keetha2023splatam} & 0.36 & 0.31 & \secondcolor{0.40} & 0.29 & 0.47 & 0.27 & \secondcolor{0.29} & \cellcolor{yellow!30}0.32 & 0.55 \\
        \hdashline
        % \hdashline
        % \textbf{\MethodName{} (Ours*)} & \cellcolor{green!30}\textbf{0.32} & \cellcolor{green!30}0.24 & \cellcolor{yellow!30}0.44 & \cellcolor{green!30}0.25 & \cellcolor{green!30}0.28 & \cellcolor{green!30}0.17 & \cellcolor{green!30}0.29 & 0.37 & \cellcolor{green!30}0.49 \\ % \textbf{\MethodName{}-woSem (Ours)} 0.316
        % \hline
        SNI-SLAM \cite{zhu2023sni} & 0.46 & 0.50 & 0.55 & 0.45 & 0.35 & 0.41 & 0.33 & 0.62 & \cellcolor{yellow!30}0.50 \\
        DNS SLAM \cite{li2023dns} & 0.45 & 0.49 & 0.46 & 0.38 & 0.34 & 0.35 & 0.39 & 0.62 & 0.60 \\
        SemGauss-SLAM \cite{zhu2024semgauss} & \cellcolor{yellow!30} 0.33 & 0.26 & 0.42 & 0.27 & 0.34 & 0.17 & 0.32 & 0.36 & \cellcolor{green!30}0.49 \\
        SGS-SLAM \cite{li2024sgs} & 0.41 & 0.46 & 0.45 & 0.29 & 0.46 & 0.23 & 0.45 & 0.42 & 0.55 \\
        
        % \sout{\textbf{\oldMethodName{}}} &  \cellcolor{green!30}\textbf{0.33} &  \cellcolor{green!30}0.21 & \cellcolor{yellow!30}0.49 & \cellcolor{green!30}0.24 & \cellcolor{green!30}0.29 & \cellcolor{green!30}0.16 & \cellcolor{green!30}0.31 & \cellcolor{yellow!30}0.37 & 0.53 \\ 
        \oldMethodName{} \cite{li2024hi} &  \secondcolor{0.33} &  \bestcolornob{0.21} & 0.49 & \secondcolor{0.24} & \bestcolornob{0.29} & \secondcolor{0.16} & 0.31 & 0.37 & 0.53 \\ 
        \hdashline
       
        % (mlp5)
        \textbf{\MethodName{}} (one-hot) & \bestcolor{0.31} &  \secondcolor{\textbf{0.24}} & \bestcolor{0.36} & \bestcolor{0.23} & \secondcolor{\textbf{0.30}} & \bestcolor{0.15} & \bestcolor{0.28} & 0.39 & 0.51 \\  % 0.3075 = (0.24+0.36+0.23+0.30+0.15+0.28 +0.39+0.51)/8
         % (bce+mlp5)
        \textbf{\MethodName{}} (binary) & \bestcolor{0.31} & \secondcolor{\textbf{0.23}} & 0.46 & \bestcolor{0.23} & \bestcolor{0.29} & \bestcolor{0.15} & \bestcolor{0.27} & 0.34 & 0.54 \\ % 0.31375 = (0.23+0.46+0.23+0.29+0.15+0.27+0.34+0.54)/8
        % (mlp4)
        % \todo{\textbf{\MethodName{} (mlp4)}} &  0.31 &  0.25 & 0.44 & 0.23 & 0.31 & 0.13 & 0.29 & 0.35 & 0.48 \\ % 0.31 = (0.25+0.44+0.23+0.31+0.13 +0.29+0.35+0.48)/8
        % (bce+mlp4)
        % \todo{\textbf{\MethodName{} (bce+mlp4)}} &  0.32 &  0.22 & 0.43 & 0.24 & 0.34 & 0.18 & 0.31 & 0.35 & 0.49 \\ % 0.32 = (0.22+0.43+0.24+0.34+0.18+0.31+0.35+0.49)/8
        % /data/boying/project/Hi-SLAM++/HiSLAM_pp_run0/HiSLAM_pp/mlp4_ver8_run0_scene01_cuda0_seed03.txt (all res are seed0)
         \toprule
    \end{tabular}
    \label{tab:exp_pose_replica}
    % \begin{tablenotes} % Begin the tablenotes environment
    %         \footnotesize
    %         \itshape
    %         \raggedright % Italic and left aligned
    %         \item \textbf{Ours*} refers to the compact version of our proposed method. \hrt{it is not clear what does this mean ``compact version'' for reader, better to name it one-hot code representation versus binary code (more compact) all over the text or some abbreviation. Also you may need to provide effiecncy comparison between these two representation}
    %     \end{tablenotes}
        % \vspace{-10pt}
\end{table}
%%%% TABLE %%%%

\subsection{Camera Tracking Accuracy}
% replica
\Tab{tab:exp_pose_replica} presents our tracking performance on the Replica dataset \cite{straub2019replica} in the RGB-D setting, compared with state-of-the-art dense SLAM methods, both with and without semantic information.  
By accurately learning semantic information, our \MethodName{} outperforms all existing methods in 6 out of 8 sequences.
For the remaining two sequences, our method achieves nearly the same performance with sub-millimeter level localization error. The lowest average tracking error clearly demonstrate that our approach surpasses other state-of-the-art methods.
% scannet
We present our RGB-D tracking performance on the real-world ScanNet dataset \cite{Dai2017scannet} in \Tab{tab:exp_pose_scannet}. 
Compared to the results on the Replica dataset, all methods exhibit degraded performance due to the noisy and sparse depth sensor input, as well as lower color image quality caused by motion blur.  
Specifically for semantic understanding, the semantic annotations provided by ScanNet are significantly noisier than those in Replica, containing noticeable noise and imprecise boundaries. All of these issues negatively impact the overall performance of semantic SLAM systems.  
We evaluate all six sequences and show that our method achieves performance comparable to state-of-the-art approaches \cite{li2024hi, keetha2023splatam}, demonstrating the robustness of our proposed method under these challenging conditions.
% We evaluate all six sequences and show that our method performs comparably to state-of-the-art approaches \cite{li2024hi, keetha2023splatam}, demonstrate that the bad semantic supervisory signal does affect our SLAM performance notably, suggesting the robustness of our method.
% tum-rgbd
The tracking performance on the TUM-RGBD dataset is shown in \Tab{tab:exp_pose_tumrgbd}. Since the TUM-RGBD dataset does not contain semantic ground truth, we evaluate our method based on tracking accuracy here. 
Despite the low-quality RGB and depth inputs, as well as the absence of semantic information, all of which could negatively impact overall performance, our method still achieves similar performance to the state-of-the-art approach \cite{keetha2023splatam}, with a localization error of less than $0.2$ cm, while operating at a significantly faster runtime. A detailed analysis of the runtime performance will be provided in the following section.



%% scannet
%%%% TABLE %%%%
\begin{table}[!tb]
% \vspace{-10pt}
    \centering
    % \renewcommand{\arraystretch}{1.1} % Adjust the row height for better readability
    \setlength{\tabcolsep}{2pt}
    \caption{RGB-D Tracking performance ATE RMSE (cm) on the Scannet. \\ Best results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}, \colorbox{red!30}{third},\underline{fourth}.}
    % \hrt{what does dash line mean in this table and all next tables and why Hier-SLAM is below dash line? why  don't we have two versions of Hier-SLAM++ like previous table in some of the tables? and why ours are not highlighted (bold)}
    % \vspace{-10pt}
    \label{tab:exp_pose_scannet}
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Methods} & \textbf{Avg.} & \textbf{0000} & \textbf{0059} & \textbf{0106} & \textbf{0169} & \textbf{0181} & \textbf{0207}  \\
        \midrule
        NICE-SLAM \cite{zhu2022nice} & \cellcolor{yellow!30}10.70 & 12.00 & 14.00 & \cellcolor{green!30}7.90 & \cellcolor{red!30}10.90 & 13.40 & \cellcolor{yellow!30}6.20  \\
        Vox-Fusion \cite{yang2022vox} & 26.90 & 68.84 & 24.18 & \cellcolor{yellow!30}8.41 & 27.28 & 23.30 & 9.41  \\
        Point-SLAM \cite{sandstrom2023point} & 12.19 & \cellcolor{green!30}10.24 & \cellcolor{green!30}7.81 &\cellcolor{red!30}8.65 & 22.16 & 14.77 & 9.54  \\
        SplaTAM \cite{keetha2023splatam} & 11.88 & 12.83 & 10.10 &17.72 & 12.08 & 11.10 & 7.46  \\
        % SemGauss-SLAM \cite{zhu2024semgauss} & -- & 12.56 & \cellcolor{yellow!30}7.97 & -- & \cellcolor{green!30}9.05 & \cellcolor{green!30}9.78 & 8.97  \\
        SGS-SLAM \cite{li2024sgs} & \bestcolornob{9.87} & \secondcolor{11.15} & \thirdcolor{9.54} & \fourthcolor{10.43} & \secondcolor{10.70} & 11.28 & \bestcolornob{6.11} \\
        SemGauss-SLAM \cite{zhu2024semgauss} & -- & \fourthcolor{11.87} & \cellcolor{yellow!30}7.97 & -- & \cellcolor{green!30}8.70 & \cellcolor{green!30}9.78 & 8.97  \\
        % Original-icra2025 ====
        \oldMethodName{} \cite{li2024hi} & \thirdcolor{11.36} & \thirdcolor{11.45} & 9.61 & 17.80 & 11.93 & \thirdcolor{10.04} & \fourthcolor{7.32} \\
        % Original-icra2025 ====
        \hdashline
        % mlp4
        % \todo{\MethodName{}} & \cellcolor{yellow!30}11.87 & 12.773 & 9.631 & 17.753 & 11.909 & 11.91 & 7.226 \\ % (12.773 + 9.631 + 17.753 + 11.909 + 11.91 + 7.226)/6
        % mlp5
        \textbf{\MethodName{}} (one-hot) & 11.72 & 12.93 & \fourthcolor{\textbf{9.59}} & 17.70 & \fourthcolor{\textbf{11.63}} & \fourthcolor{\textbf{10.98}} & 7.51 \\  % 11.723 = (12.93+9.59+17.70+11.63+10.98+7.51)/6
        % mlp5+bce
        \textbf{\MethodName{}} (binary) & \fourthcolor{\textbf{11.59}} & 13.24 & 9.60 & 17.83 & 11.75 & \secondcolor{\textbf{9.83}} & \thirdcolor{\textbf{7.29}} \\
        % 11.59 = (13.24 + 9.60 + 17.83 + 11.75 + 9.83 + 7.29)/6
        % /data/boying/project/Hi-SLAM++/HiSLAM_pp_scannet_run0/HiSLAM_pp/scannet_mlp5bce_scene10_cuda0_seed0.txt
        \bottomrule 
    \end{tabular}
\end{table}
%%%% TABLE %%%%

%% tum-rgbd rgbd-tracking
%%%% TABLE %%%%
\begin{table}[!tb]
% \vspace{-10pt}
    \centering
    % \renewcommand{\arraystretch}{1.1} % Adjust the row height for better readability
    \setlength{\tabcolsep}{1.2pt}
    \caption{RGB-D Tracking performance ATE RMSE (cm) on the TUM-RGBD. Best results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}, \colorbox{red!30}{third}, \underline{fourth}.}
    \label{tab:exp_pose_tumrgbd}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Methods} & \textbf{Avg.} & \textbf{fr1/desk} & \textbf{fr1/desk2} & \textbf{fr1/room} & \textbf{fr2/xyz} & \textbf{fr3/off}  \\
        \midrule
        Kintinuous \cite{whelan2015real} & \secondcolor{4.84} & 3.70 & 7.10 & \secondcolor{7.50} & 2.90 & \thirdcolor{3.00} \\
        ElasticFusion \cite{whelan2015elasticfusion} & 6.91 & \secondcolor{2.53} & 6.83 & 21.49 & \secondcolor{1.17} & \secondcolor{2.52} \\
        ORB-SLAM2 \cite{mur2017orb} & \bestcolornob{1.98} & \bestcolornob{1.60} & \bestcolornob{2.20} & \bestcolornob{4.70} & \bestcolornob{0.40} & \bestcolornob{1.00} \\
        NICE-SLAM \cite{zhu2022nice} & 15.87 & 4.26 & \thirdcolor{4.99} & 34.49 & 31.73 & 3.87 \\
        Vox-Fusion \cite{yang2022vox} & 11.31 & 3.52 & 6.00 & 19.53 & 1.49 & 26.01 \\
        Point-SLAM \cite{sandstrom2023point} & 8.92 & 4.34 & \secondcolor{4.54} & 30.92 & \fourthcolor{1.31} & \fourthcolor{3.48} \\
        SplaTAM \cite{keetha2023splatam} & \thirdcolor{5.48} & \fourthcolor{3.35} & \fourthcolor{6.54} & \thirdcolor{11.13} & \thirdcolor{1.24} & 5.16 \\
        \hdashline
        \textbf{\MethodName{}} & \fourthcolor{\textbf{5.62}} & \thirdcolor{\textbf{3.31}} & 6.59 & \fourthcolor{\textbf{11.73}} & 1.34 & 5.14  \\ % (3.31+6.59+11.73+1.34+5.14)/5 = 5.622
        \bottomrule 
    \end{tabular}
    % \begin{tablenotes} % Begin the tablenotes environment
    %         \footnotesize
    %         \itshape
    %         \raggedright
    %         \item \textbf{Ours*} represents our proposed system without semantic information.
    %         % \vspace{-10pt}
    %     \end{tablenotes}
\end{table}
%%%% TABLE %%%%


\subsection{Global Mapping Performance}

Following previous methods \cite{li2024hi, li2024sgs, zhu2024semgauss}, we evaluate the depth error on the Replica dataset \cite{straub2019replica} to assess overall mapping performance, as it provides the highest-quality depth maps for comparison. 
With an accurate understanding of semantics, our method effectively learns precise global geometry in unknown environments, benefiting both tracking and mapping in the semantic SLAM system.
Our method achieves superior results in 7 out of 8 sequences and maintains a low average depth error across all sequences, with a difference of approximately $0.1$ cm compared to the best performing. 
These results demonstrate that our approach have strong semantic mapping capability, highlighting our superior mapping performance.


\subsection{Rendering Quality}
% this paragraph can be deleted
We present the quantitative analysis of the rendering performance of our proposed method in \Tab{tab:exp_render_replica}. Following previous works, Point-SLAM \cite{sandstrom2023point} and NICE-SLAM \cite{zhu2022nice}, the evaluation is conducted on the input views. 
As shown in \Tab{tab:exp_render_replica}, our method achieves superior image rendering performance compared to state-of-the-art approaches across all 8 sequences, as well as in the overall average evaluation. 
As discussed in previous sections, incorporating semantic information introduces a significant computational burden to the joint optimization process within the SLAM system. However, our method demonstrates notably improved rendering performance compared to all existing methods, including the non-semantic SLAM approaches. 
Although the additional computation for semantic learning increases the optimization complexity, the accurate semantic understanding enhances both geometric and appearance learning through joint optimization, effectively improving rendering performance.

The visual comparison of rendered images is presented in \Fig{fig:exp_render}. Benefiting from precise semantic information learning and the improved SLAM system, the rendering results exhibit significantly improved detail preservation, such as more accurate and well-defined lamp boundaries in the first and fifth columns, and precise details even in fine structures like shutters in the second column. 
Moreover, our method produces high-quality renderings with fewer noise artifacts, as evidenced by the improved rendering quality of the floor in the fourth column and the chair in the sixth column. This demonstrates that better mapping performance leads to enhanced rendering quality. 
The joint optimization of semantic losses and photometric loss is mutually beneficial, facilitating more accurate global 3D reconstruction through semantic learning, which in turn improves the rendering performance of our \MethodName{}.


% The visual comparison is demonstrated in \Fig{\label{fig:exp_render}}. With the beneficial provided by the priceus semantic information learning, the rendering results exhibit much better such as the better lamp rendering with more accurate and clear boundary in the first and fifth columns, the accurate details even in Shutters  in the second column. 
% On the other hand, renderingexamples can be found in fourth floor and sixth chair, demonstrating that the better mapping performance which inducing the better rendering performance. 
% The joint optimization of semantic losses as well as the photometric loss can be mutual beneficial, guiding better global 3D reconstruction with accurate semantic information learning, further inducing the better rendering performance of our \MethodName{}.

% rgbd-depth
%%%% TABLE %%%%
\begin{table}[!t]%h
    \centering
    \caption{Reconstruction performance Depth L1 (cm) on Replica. Best results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}.}
    % \vspace{-10pt}
    \renewcommand{\arraystretch}{1.1} % Adjust row height for better fit
    \setlength{\tabcolsep}{2.0pt} % Reduce column width for less space
    \begin{tabular}{lccccccccc}
        \hline
        \toprule
        \textbf{Methods} & \textbf{Avg.} & \textbf{R0} & \textbf{R1} & \textbf{R2} & \textbf{Of0} & \textbf{Of1} & \textbf{Of2} & \textbf{Of3} & \textbf{Of4} \\
        \hline
        NICE-SLAM \cite{zhu2022nice} & 2.97 & 1.81 & 1.44 & 2.04 & 1.39 & 1.76 & 8.33 & 4.99 & 2.01 \\
        Vox-Fusion \cite{yang2022vox} & 2.46 & 1.09 & 1.90 & 2.21 & 2.32 & 3.40 & 4.19 & 2.96 & 1.61 \\
        Co-SLAM \cite{wang2023co} & 1.51 & 1.05 & 0.85 & 2.37 & 1.24 & 1.48 & 1.86 & 1.66 & 1.54 \\
        ESLAM \cite{johari2023eslam} & 0.95 & 0.73 & 0.74 & 1.26 & 0.71 & 1.02 & 0.93 & 1.03 & 1.18 \\
        
        SNI-SLAM \cite{zhu2023sni} & 0.77 & \cellcolor{yellow!30}0.55 & 0.58 & 0.87 & 0.55 & 0.97 & 0.89 & \cellcolor{green!30}0.75 & 0.97 \\
        SGS-SLAM \cite{li2024sgs} & \bestcolornob{0.36} & -- & -- & -- & -- & -- & -- & -- & -- \\
        SemGauss-SLAM \cite{zhu2024semgauss} & 0.50 & \cellcolor{green!30}0.54 & 0.46 & 0.43 & \cellcolor{green!30}0.29 & 0.22 & \cellcolor{green!30}0.51 & 0.98 & \cellcolor{yellow!30}0.56 \\
        \oldMethodName{} \cite{li2024hi} & \secondcolor{0.49} & 0.58 & \bestcolornob{0.40} & \secondcolor{0.40} & \bestcolornob{0.29} & \secondcolor{0.19} & \bestcolornob{0.51} & 0.95 & 0.57 \\
        \hdashline
        % mlp4+ver8
        % \MethodName{} & 0.498 & 0.60 & 0.44 & 0.39 & 0.29 & 0.19 & 0.52 & 0.99 & 0.57 \\ % 0.498 =  (0.60+0.44+0.39+0.29+0.19+0.52+0.99+0.57)/8
        % mlp4+bce+ver8
        % \MethodName{}(mlp4+bce) & 0.49 & 0.60 & 0.40 & 0.39 & 0.31 & 0.20 & 0.51 & 0.91 & 0.56 \\ % 0.485 = (0.60+0.40+0.39+0.31+0.20+0.51+0.91+0.56)/8
        % mlp5
        \textbf{\MethodName{}} (one-hot) & \secondcolor{\textbf{0.48}} & 0.56 & \secondcolor{\textbf{0.41}} & \bestcolor{0.39} & \bestcolor{0.29} & 0.20 & 0.53 & \secondcolor{\textbf{0.92}} & \bestcolor{0.55} \\ % 0.48125 = (0.56+0.41 +0.39+0.29+0.20 +0.53+0.92+0.55)/8
        % mlp5+bce
        \textbf{\MethodName{}} (binary) & \secondcolor{\textbf{0.49}} & 0.59 & \bestcolor{0.40} & \bestcolor{0.40} & \secondcolor{\textbf{0.30}} & \bestcolor{0.17} & \secondcolor{\textbf{0.52}} & 0.95 & 0.57 \\    % 0.4875 = (0.59+0.40+0.40+0.30+0.17+0.52+0.95+0.57)/8
        \toprule
    \end{tabular}
    % \begin{tablenotes} % Begin the tablenotes environment
    %         \footnotesize
    %         \itshape
    %         \raggedright % Italic and left aligned
    %         \item \textbf{Ours*} refers to our more compact version of the proposed method, using binary cross-entropy loss.
    %     \end{tablenotes}
    \label{tab:exp_depth}
    % \vspace{-5pt}
\end{table}
%%%% TABLE %%%%


\subsection{Running time}
The runtime results for the compared methods are provided in \Tab{tab:runtime}, covering both NeRF-based and Gaussian Splatting-based SLAM methods. 
Since the compared baseline using a flat representation (\MethodName{}-flat) cannot run successfully on an NVIDIA RTX 4090 due to GPU memory limitations, we report the runtime separately for RTX 4090 and L40S.
% We report our runtime in 
In the first block of \Tab{tab:runtime}, we observe that, compared to NeRF-based methods \cite{zhu2022nice}, Gaussian-based state-of-the-art methods \cite{keetha2023splatam} achieve significantly higher operating speeds, benefiting from the fast rendering and efficient optimization capabilities inherent to the new 3D representation, i.e., Gaussian Splatting. 
Furthermore, with the carefully optimized forward and backward Gaussian rendering module introduced in our system, which serves as the core component of Gaussian Splatting-based SLAM systems, our method (\MethodName{}- w/o sem) achieves up to 2.4 faster tracking and 2.2 faster mapping compared to the best-performing method \cite{keetha2023splatam}.

In the second block of \Tab{tab:runtime}, when integrating semantic information, our method maintains high efficiency by leveraging hierarchical semantic coding (\MethodName{}-one-hot), achieving nearly 3 faster tracking compared to flat semantic coding (\MethodName{}-flat). Compared to the state-of-the-art method \cite{li2024hi}, our \MethodName{} attains similar operational speed but significantly outperforms in semantic segmentation accuracy. 
Thanks to the improved semantic decoder in our hierarchical semantic loss, our method achieves a $13\%$ mIoU improvement while incurring only an additional $0.04$ s per frame for tracking and $2$ s per frame for mapping. 
Further details on this improvement will be provided in the next section.
Additionally, our binary version (\MethodName{}-binary) is able to achieve more efficient operation performance. 
Notably, our \MethodName{} achieves \textbf{a rendering speed of 2000 FPS}, which increases to \textbf{3000 FPS} when semantic information is not included.

\iffalse
The runtime results for compared methods are provided in \Tab{tab:runtime}, covering both NeRF-based and Gaussian Splatting-based SLAM methods.
Compared with the NeRF-based methods \cite{zhu2022nice}, the Gaussian-based state-of-the-art methods \cite{keetha2023splatam} demonstrate a much better operating speed, thanks to the fast rendering and effective optimization capability inherited by 3D Gaussian Splatting.
Furthermore, with the carefully improved forward and backward Gaussian rendering part proposed by our \MethodName{}, which is the core fundamental part of the Gaussian Splatting-based SLAM system, our \MethodName{} (\MethodName{}*) delivers up to 2.4 faster tracking and 2.2 faster mapping than the best-performing method \cite{keetha2023splatam}.
% And for our \MethodName{} with semantic information, our method achieve similar tracking efficiency 
When integrating semantic information, our method remains efficient, using hierarchical semantic coding (one-hot) to achieve nearly 3 faster tracking and similar mapping compared to semantic SLAM with flat semantic coding (\MethodName{}**).
Compared with the state-of-the-art method \cite{li2024hi}, our \MethodName{} reaches similar operation speed but with much better semantic performance. 
Because of the improved semantic decoder in our semantic hierarchical loss, our method reach much better semantic performance ($13\%$ mIoU improvement) with only $0.04$ s more time usage in tracking for per frame and less than $2$ s usage in mapping per frame. 
The improved semantic performance will be detailed in the next section.
Importantly, our \MethodName{} reaches a rendering speed of 2000 FPS. Without semantic information, this speed can even increases to 3000 FPS. % sem: 1/0.0005; wosem: 1/0.0003  1/0.00035
% \pjs{Not clear the difference between red and black Hier-SLAM++ (ours)?}
\fi

% replica-render
%%%% TABLE %%%%
\begin{table*}[!ht]
    \centering
    \caption{Rendering performance PSNR, SSIM, LPIPS on Replica. Best results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}.}
    \label{tab:exp_render_replica}
    \begin{tabular}{llcccccccccccc}
        \toprule
        \textbf{Methods} & \textbf{Metrics} & \textbf{Avg.} & \textbf{room0} & \textbf{room1} & \textbf{room2} & \textbf{office0} & \textbf{office1} & \textbf{office2} & \textbf{office3} & \textbf{office4} \\
        \midrule
        \multicolumn{11}{c}{\textbf{Visual SLAM}} \\
        \multirow{3}{*}{NICE-SLAM \cite{zhu2022nice}} 
        & PSNR $\uparrow$ & 24.42 & 22.12 & 22.47 & 24.52 & 29.07 & 30.34 & 19.66 & 22.23 & 24.94 \\
        & SSIM $\uparrow$ & 0.809 & 0.689 & 0.757 & 0.814 & 0.874 & 0.886 & 0.797 & 0.801 & 0.856 \\
        & LPIPS $\downarrow$ & 0.233 & 0.330 & 0.271 & 0.208 & 0.229 & 0.181 & 0.235 & 0.209 & 0.198  \\
        \hline
        \multirow{3}{*}{Vox-Fusion \cite{yang2022vox}} 
        & PSNR $\uparrow$ & 24.41 & 22.39 & 22.36 & 23.92 & 27.79 & 29.83 & 20.33 & 23.47 & 25.21 \\
        & SSIM $\uparrow$ & 0.801 & 0.683 & 0.751 & 0.798 & 0.857 & 0.876 & 0.794 & 0.803 & 0.847 \\
        & LPIPS $\downarrow$ & 0.236 & 0.303 & 0.269 & 0.234 & 0.241 & 0.184 & 0.243 & 0.213 & 0.199 \\
        \hline
        \multirow{3}{*}{Co-SLAM \cite{wang2023co}} 
        & PSNR $\uparrow$ & 30.24 & 27.27 & 28.45 & 29.06 & 34.14 & 34.87 & 28.43 & 28.76 & 30.91  \\
        & SSIM $\uparrow$ & 0.939 & 0.910 & 0.909 & 0.932 & 0.961 & 0.969 & 0.938 & 0.941 & 0.955 \\
        & LPIPS $\downarrow$ & 0.252 & 0.324 & 0.294 & 0.266 & 0.209 & 0.196 & 0.258 & 0.229 & 0.236 \\
        \hline
        \multirow{3}{*}{ESLAM \cite{johari2023eslam}} 
        & PSNR $\uparrow$ & 29.08 & 25.32 & 27.77 & 29.08 & 33.71 & 30.20 & 28.09 & 28.77 & 29.71 \\
        & SSIM $\uparrow$ & 0.929 & 0.875 & 0.902 & 0.932 & 0.960 & 0.923 & 0.943 & 0.948 & 0.945 \\
        & LPIPS $\downarrow$ & 0.239 & 0.313 & 0.298 & 0.248 & 0.184 & 0.228 & 0.241 & 0.196 & 0.204 \\
        \hline
        \multirow{3}{*}{SplaTAM \cite{keetha2023splatam}} 
        & PSNR $\uparrow$ & 34.11 & 32.86 & 33.89 & 35.25 & 38.26 & 39.17 & 31.97 & 29.70 & 31.81 \\
        & SSIM $\uparrow$ & 0.968 & \secondcolor{0.978} & 0.969 & 0.979 & 0.977 & 0.978 & 0.969 & 0.949 & 0.949 \\
        & LPIPS $\downarrow$ & 0.102 & 0.072 & 0.103 & 0.081 & 0.092 & 0.093 & 0.102 & 0.121 & 0.152 \\
        \midrule
        \multicolumn{11}{c}{\textbf{Semantic SLAM}} \\
        \multirow{3}{*}{SNI-SLAM \cite{zhu2023sni}} 
        & PSNR $\uparrow$ & 29.43 & 25.91 & 28.17 & 29.15 & 31.85 & 30.34 & 29.13 & 28.75 & 30.97 \\
        & SSIM $\uparrow$ & 0.921 & 0.884 & 0.900 & 0.921 & 0.935 & 0.925 & 0.930 & 0.932 & 0.936 \\
        & LPIPS $\downarrow$ & 0.237 & 0.307 & 0.292 & 0.265 & 0.185 & 0.211 & 0.230 & 0.209 & 0.198 \\
        \hline
        \multirow{3}{*}{SGS-SLAM \cite{li2024sgs}} 
        & PSNR $\uparrow$ & 34.66 & 32.50 & 34.25 & 35.10 & 38.54 & 39.20 & 32.90 & 32.05 & 32.75 \\
        & SSIM $\uparrow$ & 0.973 & 0.976 & \secondcolor{0.978} & 0.981 & 0.984 & 0.980 & 0.967 & 0.966 & 0.949 \\
        & LPIPS $\downarrow$ & 0.096 & 0.070 & 0.094 & 0.070 & 0.086 & 0.087 & 0.101 & 0.115 & 0.148 \\
        \hline
        \multirow{3}{*}{SemGauss-SLAM \cite{zhu2024semgauss}} 
        & PSNR $\uparrow$ & 35.03 & 32.55 & 33.32 & 35.15 & 38.39 & 39.07 & 32.11 & 31.60 & 35.00 \\
        & SSIM $\uparrow$ & \cellcolor{green!30}0.982 & \cellcolor{green!30}0.979 & 0.970 & \cellcolor{yellow!30}0.987 & \cellcolor{green!30}0.989 & 0.972 & \cellcolor{green!30}0.978 & \cellcolor{green!30}0.972 & \cellcolor{green!30}0.978 \\
        & LPIPS $\downarrow$ & \cellcolor{green!30}0.062 & \cellcolor{green!30}0.055 & \cellcolor{green!30}0.054 & \cellcolor{green!30}0.045 & \cellcolor{green!30}0.048 & \cellcolor{yellow!30}0.046 & \cellcolor{green!30}0.069 & \cellcolor{green!30}0.078 & \cellcolor{green!30}0.093 \\
        \hline
        
        \multirow{3}{*} { \oldMethodName{} \cite{li2024hi}}
        & PSNR $\uparrow$ & \bestcolornob{35.70} & \bestcolornob{32.83} & \bestcolornob{34.68} & 36.33 & \bestcolornob{39.75} & \bestcolornob{40.93} & \secondcolor{33.29} & \secondcolor{32.48} & 35.33 \\ % (32.83+34.68+36.33+39.75+40.93+33.29+32.48+35.33)/8 = 35.70
        & SSIM $\uparrow$ & \secondcolor{0.980} & 0.976 & \bestcolornob{0.979} & \secondcolor{0.987} & \secondcolor{0.988} & \secondcolor{0.989} & 0.975 & \secondcolor{0.971} & 0.976 \\
        % (0.976+0.979+0.987+0.988+0.989+0.975+0.971+0.976)/8 = 0.980
        & LPIPS $\downarrow$ & 0.067 & 0.060 & 0.063 & 0.052 & \secondcolor{0.050} & 0.049 & 0.083 & \secondcolor{0.081} & \secondcolor{0.094} \\
        % (0.060+0.063+0.052+0.050+0.049+0.083+0.081+0.094)/8 = 0.0665

        \hline
        % % mlp4+ver8
        % \multirow{3}{*} { \MethodName{}-tree (Ours)}
        % & PSNR $\uparrow$ & 35.61 & 32.731 & 34.277 & 36.362 & 39.501 & 40.644 & 33.488 & 32.419 & 35.450 \\ % 35.609 = (32.731+34.277+36.362+39.501+40.644+33.488+32.419+35.450)/8
        % & SSIM $\uparrow$ & 0.980 & 0.976 & 0.977 & 0.988 & 0.987 & 0.988 & 0.977 & 0.971 & 0.975 \\ % 0.979875 = (0.976+0.977+0.988+0.987+0.988+0.977+0.971+0.975)/8
        % & LPIPS $\downarrow$ & 0.068 & 0.059 & 0.063 & 0.052 & 0.059 & 0.054 & 0.075 & 0.079 & 0.105 \\ % 0.06825 = (0.059+0.063+0.052+0.059+0.054 +0.075+0.079+0.105)/8
        % \hdashline
        % % mlp4+ver8+bce
        % \multirow{3}{*} { \MethodName{}-tree (mlp4+bce)}
        % & PSNR $\uparrow$ & 35.69 & 32.812 & 34.812 & 36.224 & 39.425 & 40.739 & 33.324 & 32.595 & 35.562 \\ 
        % & SSIM $\uparrow$ & 0.981 & 0.977 & 0.980 & 0.987 & 0.987 & 0.989 & 0.975 & 0.972 & 0.977 \\ 
        % & LPIPS $\downarrow$ & 0.066 & 0.058 & 0.057 & 0.052 & 0.053 & 0.050 & 0.082 & 0.080 & 0.096 \\ 
        % % 35.6866 = (32.812+34.812+36.224+39.425+40.739+33.324+32.595+35.562)/8
        % % 0.9805 = (0.977+0.980+0.987+0.987+0.989+0.975+0.972+0.977)/8
        % % 0.066 = (0.058+0.057+0.052+0.053+0.050+0.082+0.080+0.096)/8
        % \hdashline
        
        % mlp5+ver8
        \multirow{3}{*} { \textbf{\MethodName{}} (one-hot)}
        & PSNR $\uparrow$ & \secondcolor{\textbf{35.61}} & \secondcolor{\textbf{32.76}} & \secondcolor{\textbf{34.64}} & \secondcolor{\textbf{36.27}} & \secondcolor{\textbf{39.65}} & \secondcolor{\textbf{40.12}} & \bestcolor{\textbf{33.46}} & \bestcolor{\textbf{32.49}} & \bestcolor{\textbf{35.52}} \\ 
        & SSIM $\uparrow$ & \secondcolor{\textbf{0.980}} & \secondcolor{\textbf{0.978}} & \bestcolor{\textbf{0.979}} & \secondcolor{\textbf{0.987}} & 0.987 & 0.985 & \secondcolor{\textbf{0.977}} & \secondcolor{\textbf{0.971}} & \secondcolor{\textbf{0.977}} \\ 
        & LPIPS $\downarrow$ & 0.067 & \secondcolor{\textbf{0.058}} & \secondcolor{\textbf{0.058}} & \secondcolor{\textbf{0.051}} & 0.053 & 0.064 & \secondcolor{\textbf{0.077}} & \secondcolor{\textbf{0.082}} & 0.100 \\ 
        % 35.61375 = (32.760+34.643+36.269+39.651+40.119+33.462+32.489+35.517)/8
        % 0.98013 = (0.978+0.979+0.987+0.987+0.985+0.977+0.971+0.977)/8
        % 0.067875 = (0.058+0.058+0.051+0.053+0.064+0.077+0.082+0.100)/8

        \hdashline
        % mlp5+ver8+bce
        \multirow{3}{*} { \textbf{\MethodName{}} (binary)}
        & PSNR $\uparrow$ & \secondcolor{\textbf{35.68}} & \secondcolor{\textbf{32.34}} & \secondcolor{\textbf{34.67}} & \bestcolor{\textbf{36.47}} & \secondcolor{\textbf{39.61}} & \secondcolor{\textbf{40.82}} & \bestcolor{\textbf{33.59}} & \secondcolor{\textbf{32.46}} & \bestcolor{\textbf{35.47}} \\ 
        & SSIM $\uparrow$ & \secondcolor{\textbf{0.981}} & 0.975 & \bestcolor{\textbf{0.979}} & \bestcolor{\textbf{0.988}} & 0.987 & \bestcolor{\textbf{0.990}} & \bestcolor{\textbf{0.978}} & \secondcolor{\textbf{0.971}} & \secondcolor{\textbf{0.976}} \\ 
        & LPIPS $\downarrow$ & \secondcolor{\textbf{0.066}} & 0.064 & \secondcolor{\textbf{0.061}} & \secondcolor{\textbf{0.049}} & 0.056 & \bestcolor{\textbf{0.043}} & \secondcolor{\textbf{0.076}} & \secondcolor{\textbf{0.081}} & 0.098\\ 
        % 35.679625 = (32.342+34.674+36.474+39.610+40.824+33.585+32.460+35.468)/8
        % 0.9805 = (0.975+0.979+0.988+0.987+0.990+0.978+0.971+0.976)/8
        % 0.066 = (0.064+0.061+0.049+0.056+0.043+0.076+0.081+0.098)/8
        \bottomrule
    \end{tabular}
\end{table*}
%%%% TABLE %%%%


% replica-running time
%%%% TABLE %%%%
\begin{table}[!tb]
\centering
\caption{Runtime on Replica/R0. Best results are highlighted as \textbf{first}.}
\setlength{\tabcolsep}{1.5pt}
% \setlength{\tabcolsep}{2pt}
% \vspace{-5pt}
\label{tab:runtime}
\begin{tabular}{c|lcccc}
\hline
\toprule 
 & \multirow{2}{*}{Methods} & Tracking & Mapping & Tracking & Mapping \\
% & & /Iteration (ms) & /Iteration (ms) & /Frame (s) & /Frame (s) \\
 & & /Iteration & /Iteration & /Frame & /Frame \\
\hline
\multirow{5}{*}{\textbf{RTX 4090}} & NICE-SLAM \cite{zhu2022nice} & 122.42 & 104.25 & 1.22 & 6.26 \\ % mapping/per iter: 34.057
% E-SLAM \cite{zhu2022nice}  & 22.04 & 84.48 & 0.18 & 1.27 \\ 
& SplaTAM \cite{keetha2023splatam} & 44.27 & 50.07 & 1.77 & 3.00 \\ % room0_0_timetest4.txt
% /home/boying/project/SplaTAM_workspace/raw/SplaTAM/logs/room0_0_timetest4.txt
% Photo-SLAM \cite{keetha2023splatam} &  &  &  &  \\ 
 % & \oldMethodName{} \cite{li2024hi} & 46.90 & 148.66 & 1.88 & 8.92 \\
& \textbf{\MethodName{}} (w/o sem) & \textbf{18.71} & \textbf{22.93} & \textbf{0.75} & \textbf{1.38} \\ % /home/boying/project/SplaTAM_workspace/bk_0531/Hi-SLAM/logs/room0_0_timetest_full.txt 
& \textbf{\MethodName{}} (one-hot)  & 37.63 & 194.78 & 1.50 & 11.69 \\
% & \textbf{\MethodName{}} (binary)  & 24.95 & 99.88 & 1.00 & 5.99 \\
% /home/boying/project/Hi-SLAM++/HiSLAM_pp_speed/HiSLAM_pp/time_exp_replica_r0_bce_cuda2_frame500.txt
% \MethodName{} (Ours-rgbd-bce) &  &  &  &  &  \\
% \MethodName{} (Ours-mono-replica) &  &  &  &  &  \\
% \MethodName{} (Ours-mono-tum) &  &  &  &  &  \\
\hline
% L40s
\multirow{4}{*}{\textbf{L40S}} &  \oldMethodName{} \cite{li2024hi} & 61.23 & \textbf{170.30} & 2.45 & \textbf{10.22} \\   % room0_0_full_run3 in mlp_run3_1.txt
% \MethodName{} (Ours**) & 202.42 & 238.65 & 8.09 & 14.32 \\  % room1_0_full_flat in flat_run1.txt
& \textbf{\MethodName{}} (flat) & 168.94 & 204.25 & 6.75 & 12.26 \\
% room0_0_full_flat in flat_run1.txt
% mlp5_full (/data/boying/project/Hi-SLAM++/HiSLAM_pp_speed/HiSLAM_pp/L40s_time_exp_replica_r0_cuda3_full.txt)
& \textbf{\MethodName{}} (one-hot) & 62.21 & 212.62 & 2.49 & 12.76 \\
% mlp5bce: /data/boying/project/Hi-SLAM++/HiSLAM_pp_speed/HiSLAM_pp/L40s_time_exp_replica_r0_mlp5bce_cuda2_full.txt
& \textbf{\MethodName{}} (binary) & \textbf{58.91} & 210.65 & \textbf{2.36} & 12.64 \\

\hline
\end{tabular}
\begin{tablenotes} % Begin the tablenotes environment
            \footnotesize
            \itshape
            \raggedright % Italic and left aligned
            % \item First / Second block results are from NVIDIA GeForce RTX 4090 and NVIDIA L40S, respectively.
            \item \textbf{\MethodName{} (w/o sem)} represents our proposed system without semantic information.
            % \item \textbf{Ours**} represents our proposed system using flat semantic encoding.
            \item The \textbf{units} are as follows: Tracking/Iteration (ms), Mapping/Iteration (ms), Tracking/Frame (s), and Mapping/Frame (s).
            % /Iteration (ms) & /Iteration (ms) & /Frame (s) & /Frame (s) \\
            % The speed of each method is obtained by running the whole sequence three times. 
            % \vspace{-5pt}
        \end{tablenotes}
\vspace{-10pt}
\end{table}
%%%% TABLE %%%%

% replica-semantic
%%%% TABLE %%%%
\begin{table}[!hb]
\centering
\setlength{\tabcolsep}{2pt}
\caption{Semantic performance mIoU (\%) and Parameter usage (MB) on Replica. Evaluations are conducted over \textbf{a total of 102 semantic classes}. Results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}.} %, \colorbox{red!30}{third}
\label{tab:replica_semantic_segmentation}
% \vspace{-5pt}
\begin{tabular}{c|l|ccccccc}
\hline
\toprule 
& \textbf{Methods} & \textbf{Avg.} & \textbf{R0} & \textbf{R1} & \textbf{R2} & \textbf{Of0} \\ 
\hline
\multirow{8}{*}{\textbf{mIoU (\%)}} 
& NIDS-SLAM \cite{haghighi2023neural} & 82.37 & 82.45 & 84.08 & 76.99 & 85.94 \\ 
& DNS-SLAM \cite{li2023dns} & 84.77 & 88.32 & 84.90 & 81.20 & 84.66 \\ 
% & SemGauss-SLAM \cite{zhu2024semgauss} & \cellcolor{green!30}94.22 & \cellcolor{green!30}92.81 & \cellcolor{green!30}94.10 & \cellcolor{green!30}94.72 & \cellcolor{green!30}95.23 \\ 
& \oldMethodName{} \cite{li2024hi} & 76.44 & 76.62 & 78.31 & 80.39 & 70.43 \\
& \textbf{\MethodName{}} (flat) & \cellcolor{green!30}\textbf{90.35} & \cellcolor{green!30}\textbf{91.21} & \cellcolor{green!30}\textbf{90.62} & 89.11 & \secondcolor{\textbf{90.45}} \\ 
% & \sout{\oldMethodName{}} (Ours) & 76.44 & 76.62 & 78.31 & 80.39 & 70.43 \\  % /home/boying/project/SplaTAM_workspace/2/SplaTAM/mlp_run3_room0.txt
% /data/boying/project/Hi-SLAM++/HiSLAM_pp_run0/HiSLAM_pp/[use]_paper_res.txt
% & \MethodName{} (Ours) & 88.36 & 88.07 & 85.04 & 88.80 & 91.53 \\  % 88.36 = (88.07+85.04+88.80+91.53)/4
% & \MethodName{} (Ours-mlp4-bce) & 81.95 & 82.30 & 76.93 & 85.83 & 82.72 \\ % 81.945 = (82.30+76.93+85.83+82.72)/4
% mlp5
& \textbf{\MethodName{}} (one-hot) & \secondcolor{\textbf{89.41}} & \secondcolor{\textbf{86.38}} & \secondcolor{\textbf{89.26}} & \bestcolor{\textbf{91.55}} & \bestcolor{\textbf{90.46}} \\ % 89.4125 = (86.38+89.26+91.55+90.46)/4
% mlp5+bce
& \textbf{\MethodName{}} (binary) & 81.27 & 82.77 & 73.87 & \secondcolor{\textbf{89.64}} & 78.80 \\ % 81.27 = (82.77+73.87+89.64+78.80)/4
% \hdashline
\rowcolor{gray!20}& SGS-SLAM \cite{li2024sgs} & 92.72 & 92.95 & 92.91 & 92.10 & 92.90 \\ % new ver
\rowcolor{gray!20} & SemGauss-SLAM \cite{zhu2024semgauss} & 96.34 & 96.30 & 95.82 & 96.51 & 96.72 \\ % new ver
\rowcolor{gray!20} & SNI-SLAM \cite{zhu2023sni} & 87.41 & 88.42 & 87.43 & 86.16 & 87.63 \\ 



\hline
\multirow{4}{*}{\textbf{Param (MB)}} 
& \oldMethodName{} \cite{li2024hi} & \secondcolor{910.50} & \secondcolor{793} & \secondcolor{1126} & \secondcolor{843} & \secondcolor{880} \\ 
% /data/boying/project/SplaTAM_workspace/2/SplaTAM/experiments/Replica_semantic_mlp/room0_3_full_run3/eval_storage.txt
& \textbf{\MethodName{}} (flat) & 2662.25 & 2355 & 3072 & 2560 & 2662\\  
% /data/boying/project/SplaTAM_workspace/2/SplaTAM/experiments/Replica_semantic_mlp/office0_0_full_flat/eval_storage.txt / room0_0_full_flat / room1_0_full_flat / room2_0_full_flat
% room1_0_full_run3_check
% room2_0_full_run3_check
% office0_3_full_run3
% mlp4 
% & \MethodName{} (Ours) & 926.75 & 812 & 1126 & 866 & 903 \\ % 926.75 = (812+1126+866+903)/4
% % mlp4+bce
% & \MethodName{} (Ours-mlp4-bce) & 591.75 & 528 & 690 & 563 & 586 \\ % 591.75 = (528+690+563+586)/4
% mlp5 
& \textbf{\MethodName{}} (one-hot) & 927.50 & 814 & \secondcolor{\textbf{1126}} & 867 & 903 \\ % 927.5 =(814+ 1126 + 867 + 903)/4
% mlp5 + bce
& \textbf{\MethodName{}} (binary) & \bestcolor{591.75} & \bestcolor{528} & \bestcolor{690} & \bestcolor{563} & \bestcolor{586} \\ % 591.75 = (528+690+563+586)/4


\hline
\end{tabular}
\begin{tablenotes} % Begin the tablenotes environment
            \footnotesize
            \itshape
            \raggedright % Italic and left aligned
            % \item \textbf{Ours**} represents our proposed system using flat semantic encoding.
            \item \textbf{The gray background} indicates that the different evaluation methods are used, where evaluations are conducted on a subset of the total classes.
        \end{tablenotes}
% \vspace{-10pt}
\end{table}
%%%% TABLE %%%%

\subsection{Hierarchical semantic understanding}
We perform semantic understanding experiments on the synthetic dataset Replica \cite{straub2019replica} to demonstrate the comprehensive performance of our proposed method. 
Replica \cite{straub2019replica} is a synthetic indoor dataset that includes 102 semantic classes with high-quality semantic annotations. %, making it an ideal benchmark for indoor scene evaluation.
% 
Similar to previous works \cite{zhu2024semgauss, li2023dns, zhu2023sni}, we evaluate the mIoU performance as the quantitative semantic results in \Tab{tab:replica_semantic_segmentation}, where mIoU (\%) is reported by comparing the rendered semantic map with the ground truth across all original semantic classes (102 classes). 
To further analyze the storage requirements of semantic SLAM methods, we report the parameter storage usage in the second block of \Tab{tab:replica_semantic_segmentation}. %to provide a comprehensive evaluation of semantic performance. 

From \Tab{tab:replica_semantic_segmentation}, our method, along with the flat coding version and the proposed one-hot version, both demonstrate superior semantic performance compared to all existing methods. % Additionally, our binary version (\MethodName{}-binary) achieves the best parameter storage efficiency. 
By utilizing hierarchical semantic representation, \MethodName{} (one-hot) achieves a similar semantic performance with less than a $1\%$ difference in mIoU while requiring 4.5 less storage compared to the flat version. 
The training time shown in \Tab{tab:runtime} demonstrates that our hierarchical version (\MethodName{}-one-hot) requires only 37\% of the tracking time per frame while maintaining similar frame mapping performance compared to the flat version (\MethodName{}-flat). The comparable mIoU performance, along with significantly reduced storage and computational cost, clearly demonstrates the effectiveness and efficiency of our proposed method.
Furthermore, our binary version (\MethodName{}-binary) achieves state-of-the-art semantic understanding while achieves the best parameter storage efficiency, reducing storage usage to 63\% of the one-hot version and requiring only 20\% of the storage compared to the flat version. This reduction in storage usage highlights the efficiency of our proposed method.
Because SGS-SLAM \cite{li2024sgs}, SemGauss-SLAM \cite{zhu2024semgauss} and SNI-SLAM \cite{zhu2023sni} use different evaluation methods that restrict the mIoU calculation to a subset of the global semantic classes. Specifically, SGS-SLAM computes the average IoU for the semantic classes visible in each frame, and SemGauss-SLAM together with SNI-SLAM \cite{zhu2023sni} evaluates performance based on 52 visible classes across all evaluation sequences, making direct comparisons unfair.

\Fig{fig:exp_sem_render} presents our qualitative results on the Replica dataset.  
We construct a five-level tree to hierarchically encode semantic classes, with the first five rows of \Fig{fig:exp_sem_render} illustrate the transition from level-0 to level-4, progressively refining the semantic understanding from coarse to detailed.
At the coarsest level (level-0), shown in the first row, segmentation is categorized into three broad classes: \textit{Small items}, \textit{Medium items}, \textit{Large items}.
In contrast, the finest level encompasses all 102 original semantic classes.
% For example, the hierarchical understanding of the class \textit{'Desk'} progresses from $\textit{Object} \rightarrow \textit{Furniture} \rightarrow \textit{Plane} \rightarrow \textit{Table} \rightarrow \textit{Desk}$ \hr{why Table and then desk?}, as depicted in the fourth column. % 20111
For example, the hierarchical understanding of the semantic label \textit{'Stool'} progresses from $\textit{Medium items} \rightarrow \textit{Seating furniture} \rightarrow \textit{Public seating} \rightarrow \textit{Flat} \rightarrow \textit{Stool}$, as demonstrated in the second column.
From \Fig{fig:exp_sem_render}, we can see that our method accurately renders semantics at each level, enabling a structured coarse-to-fine understanding of the entire scene.

Overall, our method demonstrates superior performance in semantic understanding while significantly reducing storage requirements and training time, benefiting from the proposed hierarchical semantic representation.


%%%% FIGURE (semantic_render) %%%%
\begin{figure*}[!tb]		
    \centering  
    % left bottom right top
    \includegraphics[width=0.9\textwidth, trim=45mm 40mm 70mm 20mm, clip]{pic/semantic_render.pdf} \\
    \caption{Visualization of our semantic rendering performance on the Replica \cite{straub2019replica} dataset. \textbf{The first four rows} demonstrate rendered semantic segmentation in a coarse-to-fine manner. \textbf{The fifth row} exhibits the finest semantic rendering, equivalent to the flat representation with $102$ original semantic classes from the Replica dataset. \textbf{The last row} visualizes the semantic ground truth for comparison.}  
    \label{fig:exp_sem_render} 
\end{figure*}
%%%% FIGURE %%%%

\subsection{Scaling up capability}

To showcase the scaling-up capability of \MethodName{}, we evaluate it on the real-world complex dataset, ScanNet \cite{Dai2017scannet}, which contains up to 550 distinct semantic classes. 
In contrast to Replica \cite{straub2019replica}, where the semantic ground truth is synthesized from a global world model and can be considered ideal, the semantic annotations in ScanNet are significantly noisier, containing considerable noise and incorrect or unclear class boundaries.
Moreover, the dataset contains noisy depth sensor inputs and blurred color images, making semantic understanding particularly challenging in this scenes.

Using the flat semantic representation cannot even run successfully due to memory limitations. 
In contrast, we establish the hierarchical tree with the assistance of LLMs and 3D Generative Models, as introduced in Section-III, which guides the compaction of the coding from the original 550 semantic classes to 73 semantic symbolic codings, resulting in over 7 times reduction in coding usage. As visualized in \Fig{fig:scannet_scaleup}, our estimated 3D global semantic map at different levels demonstrates a coarse-to-fine semantic understanding, showcasing our method's scaling-up capability in handling real-world complex scenes.

% tum-rgbd-mono (2-seqs)
\begin{table}[!b]
    \centering
    \caption{Monocular quantitative results on the TUM RGB-D dataset. Best results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}.} 
    \renewcommand{\arraystretch}{0.95} % 
    \setlength{\tabcolsep}{2.5pt} % 
    \resizebox{1.05\linewidth}{!}{ % 
    \begin{tabular}{cc|cccccccc}
    \toprule
	\multicolumn{2}{c}{Sequences} & \multicolumn{4}{c}{fr1-desk} & \multicolumn{4}{c}{fr2-xyz} \\
	\cmidrule(lr){1-2} \cmidrule(lr){3-6} \cmidrule(lr){7-10}
	{\scriptsize Loop} & {\scriptsize Method} & {\scriptsize RMSE (cm) $\downarrow$} & {\scriptsize PSNR $\uparrow$} & {\scriptsize SSIM $\uparrow$} & {\scriptsize LPIPS $\downarrow$} & {\scriptsize RMSE (cm) $\downarrow$} & {\scriptsize PSNR $\uparrow$} & {\scriptsize SSIM $\uparrow$} & {\scriptsize LPIPS $\downarrow$} \\
    \midrule      
    \multirow{3}{*}{{\rotatebox{90}{w/}}} & DROID-SLAM \cite{teed2021droid} & 1.80 & - & - & - & 0.50 & - & - & - \\
    & ORB-SLAM2 \cite{mur2017orb} & 2.00 & - & - & - & 0.60 & - & - & - \\
    & Photo-SLAM \cite{huang2024photo} & 1.54 & 20.97 & 0.743 & 0.228 & 0.98 & 21.07 & 0.726 & 0.166 \\
    % & Photo-SLAM \cite{huang2024photo} & 1.539 & 20.972 & 0.743 & 0.228 & 0.984 & 21.072 & 0.726 & 0.166 \\ 
    \hdashline
    \multirow{3}{*}{{\rotatebox{90}{w/o}}} & DSO \cite{engel2017direct} & 22.4 & - & - & - & \bestcolornob{1.10} & - & - & -  \\
    % & DROID-VO \cite{teed2021droid} & 5.20 & - & - & - & 10.70 & - & - & -  \\
    & MonoGS \cite{matsuki2024gaussian} & \bestcolornob{4.15} & \secondcolor{21.02} & \secondcolor{0.703} & \secondcolor{0.362} & 4.79 & \secondcolor{22.21} & \secondcolor{0.728} & \secondcolor{0.288}  \\
    & \textbf{\MethodName{}} & \secondcolor{\textbf{6.20}} & \bestcolor{21.91} & \bestcolor{0.836} & \bestcolor{0.269} & \secondcolor{\textbf{3.22}} & \bestcolor{24.67} & \bestcolor{0.946} & \bestcolor{0.110}  \\ 

    \bottomrule
    \end{tabular}
    }
    \label{tab:tum_mono}
\end{table}

\iffalse
% tum-rgbd-mono
%%%% TABLE %%%%
\begin{table*}[!htb]
    \centering
    % \footnotesize
    \caption{Monocular quantitative results on the TUM RGB-D dataset. Best results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}.
    } 
    % \setlength{\tabcolsep}{0.1pt}
    \tabcolsep=0.08cm
    \begin{tabular}{cc|cccccccc}
    \toprule
	\multicolumn{2}{c}{Sequences}&  \multicolumn{4}{c}{fr1-desk}& \multicolumn{4}{c}{fr2-xyz}\\\cmidrule(lr){1-2} \cmidrule(lr){3-6} \cmidrule(lr){7-10}
	{\scriptsize Loop}&{\scriptsize Method}&{\scriptsize RMSE (cm) $\downarrow$} &{\scriptsize PSNR $\uparrow$} &{\scriptsize SSIM $\uparrow$} & {\scriptsize LPIPS $\downarrow$}&{\scriptsize RMSE (cm) $\downarrow$}& {\scriptsize PSNR $\uparrow$} &{\scriptsize SSIM $\uparrow$} & {\scriptsize LPIPS $\downarrow$} \\
    \midrule      
    \multirow{3}{*}{{\rotatebox{90}{w/}}}&DROID-SLAM \cite{teed2021droid}& 1.80 &-&-&-& 0.50 &-&-&- \\
    & ORB-SLAM2 \cite{mur2017orb} & 2.00 &-&-&-& 0.60 &-&-&- \\
    & Photo-SLAM \cite{huang2024photo} & 1.539 & 20.972 & 0.743 & 0.228 & 0.984 & 21.072 & 0.726 & 0.166 \\ 
\hdashline
    % no loop
    \multirow{5}{*}{{\rotatebox{90}{w/o}}}& DSO \cite{engel2017direct} & 22.4 & - & - & - & 1.10 & - & - & -  \\
    & DROID-VO \cite{teed2021droid} & 5.20 & - & - & - & 10.70 & - & - & -  \\
    & MonoGS \cite{matsuki2024gaussian} & 4.15 & 21.02 & 0.703 & 0.362 & 4.79 & 22.21 & 0.728 & 0.288  \\
    % /home/boying/project/meshgpt-pytorch/MonoGS/results/data_TUM_RGBD/fr1_desk/log
    & \textbf{\MethodName{}} & 6.20 & 21.91 & 0.836	& 0.269 & 3.22 & 24.67 & 0.946 & 0.110  \\ % depth-1: 9.23; 2: 

    \bottomrule
    \end{tabular}
    % }
    \label{tab:tum_mono}
\end{table*}
%%%% TABLE %%%%
\fi

\iffalse
%%%% TABLE %%%%
\begin{table*}[!htb]
    \centering
    % \footnotesize
    \caption{Monocular quantitative results on the TUM RGB-D dataset. Best results are highlighted as \colorbox{green!30}{first}, \colorbox{yellow!30}{second}.
    } 
    \tabcolsep=0.08cm
    \begin{tabular}{cc|cccccccccccc}
    \toprule
	\multicolumn{2}{c}{Sequences}&  \multicolumn{4}{c}{fr1-desk}& \multicolumn{4}{c}{fr2-xyz}& \multicolumn{4}{c}{fr3-office}\\\cmidrule(lr){1-2} \cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14}
	{\scriptsize Loop}&{\scriptsize Method}&{\scriptsize RMSE (cm) $\downarrow$} &{\scriptsize PSNR $\uparrow$} &{\scriptsize SSIM $\uparrow$} & {\scriptsize LPIPS $\downarrow$}&{\scriptsize RMSE (cm) $\downarrow$}& {\scriptsize PSNR $\uparrow$} &{\scriptsize SSIM $\uparrow$} & {\scriptsize LPIPS $\downarrow$}&{\scriptsize RMSE (cm) $\downarrow$}& {\scriptsize PSNR $\uparrow$} &{\scriptsize SSIM $\uparrow$ }& {\scriptsize LPIPS $\downarrow$} \\
    \midrule      
    \multirow{3}{*}{{\rotatebox{90}{w/}}}&DROID-SLAM \cite{teed2021droid}& 1.80 &-&-&-& 0.50 &-&-&-& 2.80 &-&-&- \\
    & ORB-SLAM2 \cite{mur2017orb} & 2.00 &-&-&-& 0.60 &-&-&-& 2.30 &-&-&- \\
    & Photo-SLAM \cite{huang2024photo} & 1.539 & 20.972 & 0.743 & 0.228 & 0.984 & 21.072 & 0.726 & 0.166 & 1.257 & 19.591 & 0.692 & 0.239 \\ 
\hdashline
    % no loop
    \multirow{5}{*}{{\rotatebox{90}{w/o}}}& DSO \cite{engel2017direct} & 22.4 & - & - & - & 1.10 & - & - & - & 9.50 & - & - & - \\
    & DROID-VO \cite{teed2021droid} & 5.20 & - & - & - & 10.70 & - & - & - & 7.30 & - & - & - \\
    & MonoGS \cite{matsuki2024gaussian} & 4.15 & 21.02 & 0.703 & 0.362 & 4.79 & 22.21 & 0.728 & 0.288 & 4.39 & 22.15 & 0.763 & 0.345 \\
    % /home/boying/project/meshgpt-pytorch/MonoGS/results/data_TUM_RGBD/fr1_desk/log
    & \textbf{\MethodName{} (Ours)} & 6.18 & 20.73 & 0.796 & 0.304 & f2x-pose & f2x-psnr & f2x-ssim & f2x-lpips & f3o-pose & f3o-psnr & f3o-ssim & f3o-lpips \\ % depth-1: 19.46; 2: 
    & \textbf{\MethodName{} (Ours-reg)} & 6.20 & 21.91 & 0.836	& 0.269 & 3.22 & 24.67 & 0.946 & 0.110 & \sout{202.55} & f3o-psnr & f3o-ssim & f3o-lpips \\ % depth-1: 9.23; 2: 

    \bottomrule
    \end{tabular}
    % }
    \label{tab:tum_mono}
\end{table*}
%%%% TABLE %%%%
\fi

\subsection{Monocular performance}

We present our monocular SLAM performance in \Tab{tab:tum_mono}.  
For camera tracking, our method achieves competitive performance compared to state-of-the-art approaches without loop closure. Specifically, compared to the state-of-the-art Gaussian Splatting-based methods \cite{matsuki2024gaussian}, our method exhibits similar tracking performance while achieving superior rendering quality. 
For SLAM methods with loop closure, which typically demonstrate superior camera tracking performance due to well-executed loop closure mechanisms that significantly enhance tracking accuracy and overall SLAM performance, our method still achieves superior rendering accuracy compared to Photo-SLAM \cite{huang2024photo}, which incorporates loop closure. This highlights the strong rendering capabilities of our proposed method.
With the assistance of the geometric prior provided by the feed-forward method, the joint optimization of both photometric loss and aligned depth loss contributes to improved rendering performance in the whole SLAM system. 


% We present our monocular SLAM performance in \Tab{tab:tum_mono}.  
% For camera tracking, our method achieves competitive performance compared to the state-of-the-art approaches without loop closure.  
% Specifically, compared with the state-of-the-art Gaussian Splatting based methods \cite{matsuki2024gaussian}, our method exhibit similar tracking performance yet with better rendering performance. 
% With the assistance of the geometric prior provided by the feed-forward method, the joint optimization for both photometric loss and aligned depth loss help to improve the rendering performance.
% For the SLAM methods with loop closure, which demonstrates high camera tracking performance for a well-operated loop closure generally improving tracking accuracy by an order of magnitude, and further improve whole SLAM performance.  
% But our method still demonstrate a superior rendering accuracy compared with the Photo-SLAM \cite{huang2024photo} which with loop closure, demonstrate the good rendering performance of our proposed method. 

% Compared to Photo-SLAM \cite{huang2024photo}, which benefits primarily from accurate pose estimation and the robust point cloud map generated by the online ORB-SLAM operation \cite{campos2021orb}, along with the loop closure mechanism within the ORB-SLAM system, it demonstrates highly effective localization performance, generally improving tracking accuracy by an order of magnitude.  Even when compared to Photo-SLAM \cite{huang2024photo} with loop closure, our method achieves superior rendering accuracy.  Overall, our approach delivers performance on par with state-of-the-art methods.


%%%% FIGURE (scannet large sem) %%%%
\begin{figure}[!thb]		
    \centering  
    % left bottom right top
    \includegraphics[width=0.5\textwidth, trim=0mm 70mm 0mm 0mm, clip]{pic/scannet_fullsem.pdf} \\
    \caption{Visualization of the established semantic 3D map across multiple levels, illustrating a coarse-to-fine semantic understanding of the complex scene. The bottom of the figure presents localization, rendering, and depth performance, offering a comprehensive overview of \MethodName{}'s effectiveness and demonstrating the scaling-up capability of our proposed method.} 
    \label{fig:scannet_scaleup}  
    % \vspace{-15pt}
\end{figure}
%%%% FIGURE %%%%

\subsection{Ablation study}

% \pjs{In the table all are identical for MS-SSIM, why is one highlighted?}

We evaluate SLAM performance under different tree designs in \Tab{tab:ab_diff_tree}. The evaluation is conducted using the baseline method, \oldMethodName{} \cite{li2024hi}, ensuring that any performance differences are solely attributable to the tree structures. 
In \Tab{tab:ab_diff_tree}, each metric represents the average result from two runs on the first four sequencesRoom0, Room1, Room2, and Office0from the Replica dataset. We generate two different 5-level trees using our proposed tree generation procedure for the Replica dataset and incorporate them into the semantic SLAM system.
As shown in \Tab{tab:ab_diff_tree}, all methods demonstrate good performance, which can be attributed to the well-designed tree structures generated through both LLMs and 3D Generative Models. Additionally, the fully optimized hierarchical semantic loss contributes to the robustness of SLAM performance. 
Among all compared methods, tree-1 achieves the best semantic mIoU performance, the highest depth mapping accuracy, and close rendering quality (PSNR, SSIM, and LPIPS) compared with other settings. Therefore, we select tree-1 as the hierarchical tree structure leveraged within our semantic SLAM system.


% We evaluate SLAM performance under different tree designs in \Tab{tab:ab_diff_tree}. The evaluation is conducted based on the baseline method, \oldMethodName{} \cite{li2024hi}, ensuring that any performance differences attributable to the tree structures independently.
% From \Tab{tab:ab_diff_tree}, each metric represents the average result from two runs on the first four sequencesRoom0, Room1, Room2, and Office0from the Replica dataset.
% We generate two different 5-levels trees using our proposed tree generation procedure for Replica dataset, and leverage them to run the semantic SLAM system.
% From \Tab{tab:ab_diff_tree}, we can find that all methods exhibit good performance, which is because the leveraged well-designed reasonable trees via both LLMs and 3D Generative Models, together with the fully optimization benefited from the proposed hierarchical semantic loss, which assistant to generate a robust SLAM performance.
% Within all compared methods, our tree-1 show the best semantic mIoU performance and the best Depth mapping performance and similar rendering performance (PSNR, SSIM, and LPIPS), which are selected as our leveraged tree within the SLAM system. 

% The results show that a better-designed tree leads to improved mIoU performance, as well as the enhanced tracking accuracy (ATE RMSE) and mapping precision (Depth).

%%%% TABLE %%%%
% AB-DIFF-TREES
% 
\begin{table}[!tb]
    \centering
    \caption{Performance of different trees on Replica. Best results are highlighted in \textbf{bold}.}
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{1pt}
    \label{tab:ab_diff_tree}
    \begin{tabular}{lcccccc}
        \toprule
         Methods & \textbf{mIou}$\uparrow$ & \textbf{ATE RMSE}$\downarrow$ & \textbf{Depth}$\downarrow$ & \textbf{PSNR}$\uparrow$ & \textbf{MS-SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ \\
        \midrule
        \oldMethodName{} \cite{li2024hi}  & 75.517 & 0.292 & 0.416 & \textbf{35.885} & \textbf{0.982} & \textbf{0.057} \\
        % 75.517 = (78.055+78.789+78.330+74.189+75.496+77.347+71.254+70.676)/8
        % 0.2922 = (0.230+0.231+0.380+0.430+0.232+0.232+0.300+0.303)/8
        % 0.4155 = (0.571+0.540+0.420+0.418+0.408+0.396+0.282+0.289)/8
        % 35.885 = (32.869+32.930+34.716+34.671+36.074+36.320+39.827+39.670)/8
        % 0.982375 = (0.977+0.977+0.979+0.978+0.986+0.987+0.988+0.987)/8
        % 0.05662 = (0.058+0.060+0.058+0.065+0.055+0.053+0.052*2)/8
        \hdashline
        \textbf{\MethodName{}}$^{\text{tree1}}$  & \textbf{76.754} & 0.302 & \textbf{0.408} & 35.825 & \textbf{0.982} & 0.058 \\ % tree_ver8
        % 1.miou. 76.754 = (76.096+77.497+76.256+75.338+80.085+78.616+77.623+72.519)/8
        % 2. pose. 0.302 = (0.250+0.224+0.517+0.340+0.259+0.236+0.300+0.293)/8
        % 3. Depth. 0.408 = (0.548+0.525+0.409+0.411+0.394+0.394+0.292+0.292)/8
        % 4. PSNR. 35.776 = (32.564+32.683+34.731+34.631+36.036+36.261+39.725+39.578)/8
        % 5. SSIM. 0.982 = (0.975+0.975+0.979+0.978+0.986+0.987*3)/8
        % 6. LPIPS. 0.0584 = (0.063+0.063+0.059+0.064+0.058+0.052+0.053+0.055)/8
        % \hdashline
        \textbf{\MethodName{}}$^{\text{tree2}}$  & 75.061 & \textbf{0.280} & 0.413 & 35.792 & \textbf{0.982} & \textbf{0.057} \\ % tree_ver9
        % 1-miou. 75.061125 = (76.015+78.707+70.532+75.487+80.146+78.498+68.565+72.539)/8
        % 2-pose. 0.27975 = (0.218+0.224+0.338+0.402+0.226+0.233+0.299+0.298)/8
        % 3-Depth. 0.413125 = (0.564+0.540+0.390+0.437+0.399+0.407+0.277+0.291)/8
        % 4-PSNR. 35.7923749 = (32.849+32.795+34.815+34.462+36.128+35.909+39.897+39.484)/8
        % 5-SSIM. 0.98225 = (0.977+0.976+0.980+0.977+0.986+0.987+0.988+0.987)/8
        % 6-LPIPS. 0.0572499 = (0.060+0.060+0.058+0.067+0.054+0.052+0.054+0.053)/8
        % \hdashline
        \bottomrule 
    \end{tabular}
    % \begin{tablenotes} % Begin the tablenotes environment
    %         \footnotesize
    %         \itshape
    %         \raggedright
    %         \item \textbf{Ours*} represents our proposed system without semantic information.
    %     \end{tablenotes}
\end{table}
%%%% TABLE %%%%



\section{Conclusion}

We introduced \MethodName{}, a neuro-symbolic semantic 3D Gaussian Splatting SLAM system that supports both RGB-D and monocular inputs, incorporating a novel hierarchical categorical representation.  
Specifically, we proposed a compact and generalized hierarchical representation that encodes both semantic meaning and geometric attributes (i.e., size and shape), leveraging the capabilities of LLMs and 3D generative models.  
To enhance global semantic understanding, we introduced an improved hierarchical semantic loss.  
Furthermore, we extended the system from an RGB-D-only SLAM to support monocular input, utilizing geometric priors derived from a feed-forward SLAM approach.  
Experimental results demonstrate that \MethodName{} achieves superior or on-par performance with state-of-the-art NeRF-based and Gaussian-based SLAM methods in terms of tracking, mapping, and semantic understanding accuracy, while significantly reducing storage requirements and runtime.
These advances establish \MethodName{} as a robust solution for semantic 3D understanding across diverse environments.

%%%% FIGURE (rgb render) %%%%
\begin{figure*}[!thb]		
    \centering  
    % left bottom right top
    \includegraphics[width=0.9\textwidth, trim=45mm 40mm 70mm 20mm, clip]{pic/render.pdf} \\
    \caption{Visualization of RGB image rendering performances on the Replica dataset \cite{straub2019replica}.  
    \textbf{The first row} presents the image rendering results of our \MethodName{}.  
    \textbf{The second to fourth rows} show the rendering outputs of the comparison methods.  
    \textbf{The last row} displays the ground truth images for reference.  
    We use red boxes to highlight key differences regions within the image. The visual comparison clearly demonstrates that our method achieves superior rendering quality, closely resembling the ground truth images compared to state-of-the-art approaches.}  
    \label{fig:exp_render} 
\end{figure*}
%%%% FIGURE %%%%

% \pjs{In Figure 5, I would put RBG at the top, so its close to Ours!}

% \section*{Acknowledgments}





% \section{References Section}

\bibliographystyle{unsrt}
% \bibliographystyle{IEEEtranS}
\bibliography{references}


\newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[]{}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill

%%%%%%%%% SUPPLYMENT BEGIN
\newpage

\setcounter{section}{0}
% \setcounter{equation}{0}
% \setcounter{figure}{0}
% \setcounter{table}{0}
% \setcounter{page}{1}
% \makeatletter
% \renewcommand{\theequation}{S\arabic{equation}}
% \renewcommand{\thefigure}{S\arabic{figure}}
% \appendix

\begin{center}
    \textbf{\large APPENDIX}
\end{center}

\section{Appendix 1. Implementation Details for Tree Generation}
\normalfont

During the hierarchical tree generation process, we utilize LLMs to group flat semantic classes at different levels based on \textbf{physical size}, \textbf{semantic function}, and to \textbf{summarize and refine geometric grouping}, ensuring balanced, meaningful, and descriptive outputs.
After generating each tree level, we further employ LLMs as \textbf{validators} to examine the results at each level, ensuring that all input categories are fully included with no omissions and that no extra categories are mistakenly introduced.

\noindent\textbf{Size Grouping: }
The prompt used for the initial grouping based on object physical sizes is illustrated as follows:

% promt
\begin{prompt}[title={Code \thetcbcounter: Prompt for Size Grouping}] \label{promt:size_cluster}
You are a smart assistant tasked with dividing the following items into meaningful groups based on their properties. The key requirements are:\\
\textbf{Balance:} The number of items in each group should be as evenly distributed as possible. A difference of 1 item between groups is acceptable, but larger differences should be avoided. \\
\textbf{Meaningfulness:} The groups must be meaningful, based on the items' inherent characteristics. Grouping should make logical sense to a human observer. \\
\textbf{Descriptive Group Names:} Each group must have a clear and descriptive name that reflects its characteristics. \\
\textbf{Goal:} Cluster items into size groups, based on their typical physical size in scenes. The groups must strictly be by sizes, for example: \texttt{small, small medium, medium, large, extra large}, etc.\\
\textbf{Items:} \texttt{\{classes input\}}

Ensure groups are meaningful and provide descriptive group names. Output must follow this JSON format:
\begin{verbatim}
{
    "<GROUP_1>": ["<ITEM_1>", ...], 
    "<GROUP_2>": ["<ITEM_2>", ...], 
    ...
}
\end{verbatim}
\end{prompt}

\iffalse
\par\vspace{-5pt}\noindent\textcolor{blue!60!white}{\rule{\linewidth}{0.8pt}}
\begin{enumerate}
    \item \textbf{Size grouping:}
Cluster items into size groups, based on their typical physical size in scenes. The groups must strictly be by sizes, for example: \textit{small, small medium, medium, large, extra large}, etc.

\item \textbf{Semantic grouping:}
Cluster items in the '{size}' size group into functionality-based groups.
The name of the clusters should not be too specific, it could be as general like storage, furniture, etc..
Ensure that items in each group serve similar purposes or have similar functionalities.
\end{enumerate}

\par\vspace{-5pt}\noindent\textcolor{blue!60!white}{\rule{\linewidth}{0.8pt}}

\textbf{Items:} \texttt{\{classes input\}}

Ensure groups are meaningful and provide descriptive group names. Output must follow this JSON format:
\begin{verbatim}
{
    "<GROUP_1>": ["<ITEM_1>", ...], 
    "<GROUP_2>": ["<ITEM_2>", ...], 
    ...
}
\end{verbatim}
\end{tcolorbox}

\label{fig:prompt_size_sem}  
\end{figure}
\fi

\noindent\textbf{Function Grouping: }
The prompt adopted for the subsequent semantic function grouping is as follows:

\begin{prompt}[title={Code \thetcbcounter: Prompt for Function Grouping}] \label{promt:func_cluster}
You are a smart assistant tasked with dividing the following items into meaningful groups based on their properties. The key requirements are:\\
\textbf{Balance:} The number of items in each group should be as evenly distributed as possible. A difference of 1 item between groups is acceptable, but larger differences should be avoided. \\
\textbf{Meaningfulness:} The groups must be meaningful, based on the items' inherent characteristics. Grouping should make logical sense to a human observer. \\
\textbf{Descriptive Group Names:} Each group must have a clear and descriptive name that reflects its characteristics. \\
\textbf{Goal:} Cluster items in the '{size}' size group into functionality-based groups.
The name of the clusters should not be too specific, it could be as general like \texttt{storage, furniture}, etc.
Ensure that items in each group serve similar purposes or have similar functionalities.\\
\textbf{Items:} \texttt{\{classes input\}}

Ensure groups are meaningful and provide descriptive group names. Output must follow this JSON format:
\begin{verbatim}
{
    "<GROUP_1>": ["<ITEM_1>", ...], 
    "<GROUP_2>": ["<ITEM_2>", ...], 
    ...
}
\end{verbatim}
\end{prompt}

\noindent\textbf{Summarize and Refine for Geometric Grouping: }
After grouping with the 3D Generative Model, we employ LLMs to summarize the clustering results with descriptive labels and balance the nodes within each group. The prompt for LLMs is illustrated as follows:

\begin{prompt}[title={Code \thetcbcounter: Prompt for Geometric Grouping Results}] \label{promt:geom_cluster}
The following groups of indoor scene items have been clustered based on their shape: \texttt{\{formatted\_clusters\}}\\
\textbf{Instructions:} % 
\textbf{Balance:} Ensure the groups are as evenly distributed as possible. A difference of 1 item between groups is acceptable. If needed, move a small number of items from one group to another to achieve balance, feel free to even remove a group if it has only one member (just dont leave any groups empty), ensuring that the groups remain meaningful. \\
\textbf{Meaningful Naming:} After balancing the groups, assign a descriptive and meaningful name to each group, based on the shared shape characteristic. The name should clearly reflect the shape or geometric property of the items in that group. Always prefer singular shapes names like box, rectangle, soft, flat, etc. \\
\textbf{No Dublications:} make sure to not repeat any class members, the group names are fine but not the groups' members. \\
The group names must strictly by shapes and DO NOT leave any group empty, you could remove it if its empty.\\
The output must be the same JSON format as below:\\
\begin{verbatim}
{
    "<GROUP_1>": ["<ITEM_1>", ...], 
    "<GROUP_2>": ["<ITEM_2>", ...], 
    ...
}
\end{verbatim}
\end{prompt}


\noindent\textbf{LLM Validators: }
The prompt used for LLM validator is as following:

\begin{prompt}[title={Code \thetcbcounter: Prompt for LLM validator}] \label{promt:llm_validator}
 The following groups of items in a scene have been clustered based on their shape: \texttt{\{formatted\_clusters\}}\\
\textbf{Instructions:} % 
\textbf{Balance:} Ensure the groups are as evenly distributed as possible. A difference of 1 item between groups is acceptable. If needed, move a small number of items from one group to another to achieve balance, remove a group if it has only one member, ensuring that the groups remain meaningful. \\
\textbf{Meaningful Naming:} After balancing the groups, assign a descriptive and meaningful name to each group, based on the shared shape characteristic. The name should clearly reflect the shape or geometric property of the items in that group. Always prefer singular shapes names like box, rectangle, soft, flat, etc. \\
\textbf{New Group:} If it is necessary to create a new group, feel free to do so, DO NOT create any non-original items. \\
\textbf{No Dublications:} make sure to not repeat any class members, the group names are fine but not the groups' members. \\
% The group names must strictly by shapes and DO NOT leave any group empty, you could remove it if its empty.\\
The output must be the same JSON format as below:\\
\begin{verbatim}
{
    "<GROUP_1>": ["<ITEM_1>", ...], 
    "<GROUP_2>": ["<ITEM_2>", ...], 
    ...
}
\end{verbatim}
\end{prompt}



% In summary, we use LLMs to cluster semantic classes via pythical sizes, semantic functional, as well as summarize and refine the geometric clustering, to ensure the balanced, meaningful, and descriptive outputs. 


% \section{Appendix 2. Generated Tree}
% review





\end{document}


