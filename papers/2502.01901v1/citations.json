[
  {
    "index": 0,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei",
        "title": "Language Models are Few-Shot Learners"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Jason Wei and\nXuezhi Wang and\nDale Schuurmans and\nMaarten Bosma and\nBrian Ichter and\nFei Xia and\nEd H. Chi and\nQuoc V. Le and\nDenny Zhou",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "tree",
        "author": "Shunyu Yao and\nDian Yu and\nJeffrey Zhao and\nIzhak Shafran and\nTom Griffiths and\nYuan Cao and\nKarthik Narasimhan",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yao2022react",
        "author": "Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R. Narasimhan and Yuan Cao",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "promptbreeder2022",
        "author": "Chrisantha Fernando and Dylan Banarse and Henryk Michalewski and Simon Osindero and Tim Rockt\u00e4schel",
        "title": "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ape",
        "author": "Yongchao Zhou and\nAndrei Ioan Muresanu and\nZiwen Han and\nKeiran Paster and\nSilviu Pitis and\nHarris Chan and\nJimmy Ba",
        "title": "Large Language Models are Human-Level Prompt Engineers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "opro",
        "author": "Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V Le and Denny Zhou and Xinyun Chen",
        "title": "Large Language Models as Optimizers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "cp",
        "author": "Oliver Kramer and Jill Baumann",
        "title": "Unlocking Structured Thinking in Language Models with Cognitive Prompting"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "prystawski2023psychologically",
        "author": "Ben Prystawski and Paul Thibodeau and Christopher Potts and Noah D. Goodman",
        "title": "Psychologically-Informed Chain-of-Thought Prompts for Metaphor Understanding in Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Comsa",
        "author": "Iulia M. Comsa and\nJulian Martin Eisenschlos and\nSrini Narayanan",
        "title": "MiQA: {A} Benchmark for Inference on Metaphorical Questions"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Hicke",
        "author": "Rebecca M. M. Hicke and\nRoss Deans Kristensen{-}McLachlan",
        "title": "{SCIENCE} {IS} {EXPLORATION:} Computational Frontiers for Conceptual\nMetaphor Theory"
      }
    ]
  }
]