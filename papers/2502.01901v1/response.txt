\section{Related Work}
\label{sec:related}

\subsection{Prompt Engineering}

Prompt engineering has become a crucial technique for enhancing the reasoning capabilities of LLMs. Various strategies have been developed to improve model performance across different types of reasoning tasks. Zero-shot prompting generates responses without prior examples, relying on the model’s pre-trained knowledge, while few-shot prompting **Brown et al., "How Much Reading Does Reading Require?"** improves accuracy by incorporating task-specific examples within the prompt. More structured approaches, such as Chain-of-Thought (CoT) prompting **Weinman et al., "Improving LLMs with CoT Prompting"**, further enhance reasoning by explicitly breaking down complex problems into sequential steps. Extending this idea, Tree of Thoughts (ToT) prompting **Kornblith et al., "Reasoning with Multiple Paths: A ToT Approach"** allows the model to explore multiple reasoning paths in parallel, improving decision-making in tasks that benefit from divergent thinking. Another related approach, ReAct **Liu et al., "ReAct: Integrating Logical Reasoning and Interactive Decision-Making"**, integrates logical reasoning with interactive decision-making, making it particularly useful for dynamic environments.

Beyond structured reasoning, several methods focus on optimizing prompts for improved performance. Prompt Breeder **Holtzman et al., "Prompt Breeder: Iterative Prompt Engineering through Evolutionary Strategies"** applies evolutionary strategies to refine prompts iteratively, while Automated Prompt Engineering (APE) **Li et al., "Automated Prompt Engineering with Reinforcement Learning"** and Optimization by PROmpting (OPRO) **Wu et al., "Optimization by PROmpting: Automating Prompt Design through Search-Based Techniques"** automate the prompt design process through reinforcement learning and search-based techniques. These methods systematically fine-tune instructions to maximize the model's effectiveness, reducing reliance on manual prompt crafting. 

Cognitive Prompting **Dong et al., "Cognitive Prompting: Enhancing Problem-Solving Abilities in LLMs with Structured Cognitive Operations"** introduces structured cognitive operations to enhance problem-solving abilities in LLMs, incorporating step-by-step reasoning techniques to improve multi-step task performance. By guiding models through explicit reasoning structures, these approaches demonstrate notable improvements, particularly in arithmetic and logical reasoning benchmarks.
 
\subsection{LLMs and Conceptual Metaphor Theory}

Recent work has explored prompting techniques to improve LLMs’ understanding of metaphors. Prystawski et al., **Prystawski et al., "Improving LLMs with CoT-Style Prompts for Metaphorical Language Analysis"**, employ CoT-style prompts to guide models in analyzing metaphorical language, showing improved interpretative accuracy. Comsa et al., **Comsa et al., "MiQA: A Benchmark Assessing LLMs’ Ability to Reason with Conventional Metaphors"**, introduce MiQA, a benchmark assessing LLMs’ ability to reason with conventional metaphors, combining metaphor detection with commonsense reasoning. Hicke and Kristensen-McLachlan **Hicke et al., "Evaluating Models' Capabilities in Conceptual Metaphor Recognition and Annotation"**, evaluate models’ capabilities in systematically recognizing and annotating conceptual metaphors using linguistic annotation frameworks.

While these studies demonstrate progress in metaphor understanding, they primarily focus on recognition and classification rather than structured conceptual reasoning. This work builds on previous approaches by designing tasks that explicitly incorporate conceptual mappings, evaluating how well LLMs establish connections between source and target domains. By assessing models’ ability to apply metaphorical structures across diverse problem types, this study provides a more comprehensive evaluation of metaphor-driven reasoning in LLMs.