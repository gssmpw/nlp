%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/text8}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}

\subsection{Text Generation}
We evaluate our Riemannian Diffusion Language Model (RDLM) for text generation tasks on two language benchmarks: Text8~\citep{data_text8} and One Billion Words Dataset~\citep{chelba2013one}.

\paragraph{Baselines}
We compare against state-of-the-art autoregressive and diffusion models. Multinomial Diffusion~\citep{hoogeboom2021multinomial}, D3PM~\citep{austin2021d3pm}, SEDD~\citep{lou2024sedd}, MDLM~\citep{sahoo2024simple}, MD4~\citep{shi2024md4} are discrete diffusion models. Plaid~\citep{gulrajani2024plaid} and Bayesian Flow Network (BFN)~\citep{graves2023bayesian} are continuous diffusion models. 
% AR models
IAF/SCF~\citep{ziegler2019iaf}, AR Argmax Flow~\citep{hoogeboom2021multinomial}, and Discrete Flow~\citep{tran2019discrete} are flow-based models, and ARDM~\citep{hoogeboom2022autoregressive} and MAC~\citep{shih2022ardm} are any-order autoregressive models. 
We also compare with transformer AR model~\citep{vaswani2017transformer}.
We provide further details on the baselines in Appendix~\ref{app:exp:text}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/lm1b}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Implementation Details}
For all experiments, we use the same data split and context size following \citet{lou2024sedd} and \citet{sahoo2024simple}. For Text8, we randomly sample contiguous chunks of length 256 as done in previous works~\citep{austin2021d3pm,lou2024sedd}.
For One Billion Words, we use the same tokenizer as in \citet{he2023diffusionbert} with context size 128.
We use a diffusion transformer architecture~\citep{peebles2023dit} with rotary positional embeddings~\citep{su2024roformer} for all the experiments and match the number of parameters as used in the previous works~\citep{lou2024sedd,sahoo2024simple}.
For our model, we use the mixture path of masked and uniform diffusion (Eq.~\eqref{eq:mixture_path_mask_unif}) and apply dimension splitting for large vocabulary.
We provide more details in Appendix~\ref{app:exp:text}.
% We provide further experimental details in Appendix~\ref{app:exp:text8}.


\paragraph{Text8}
We first evaluate on a small character-level language modeling task. 
Text8~\citep{data_text8} dataset is a character-level text modeling benchmark extracted from English Wikipedia.
We train the models on short text chunks of length 256
and evaluate the models using Bits Per Character (BPC).
As shown in Table~\ref{tab:text8}, our framework outperforms all previous diffusion models, both the discrete and continuous methods.
We also outperform the any-order autoregressive models that generate texts in flexible decoding order similar to discrete diffusion models.
We achieve similar generative perplexity and entropy compared to existing discrete diffusion models. 
We provide the generated texts from RDLM in Appendix~\ref{app:samples:text8}.


\paragraph{One Billion Words}
We further evaluate on One Billion Words Dataset (LM1B)~\citep{chelba2013one}, a medium-sized real-world language benchmark. 
We evaluate the models using perplexity (PPL) and the results are summarized in Table~\ref{tab:lm1b}. 
RDLM outperforms most of the diffusion models and is comparable to the state-of-the-art discrete diffusion model~\citep{shi2024md4}. 
In particular, we significantly outperform the existing continuous diffusion model~\citep{li2022diffusion} demonstrating the effectiveness of incorporating the geometry of the underlying categorical distribution.
We provide the generated texts in Appendix~\ref{app:samples:lm1b}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/cifar10}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Pixel-level Image Modeling}
We further explore applications beyond the text domain. 
We train our model on order-agnostic image data where each image is represented as a set of discrete tokens with a vocabulary of size 256. 
This removes the information of relative proximity between different pixels. 
We compare RDLM against autoregressive models and discrete diffusion models that directly work on raw pixel space, which we describe in Appendix~\ref{app:exp:image}.
As shown in Table~\ref{tab:cifar10}, our method achieves the lowest BPD outperforming the discrete diffusion models~\citep{austin2021d3pm,shi2024md4} and autoregressive models~\citep{chen2018pixelsnail,child2019sparse}.



\subsection{DNA Sequence Design}
We show that our framework can be applied to the generation of biological sequences. We evaluate our method for the promoter DNA sequence design task, which aims to generate valid promoter DNA sequences conditioned on transcription profiles.
We provide further details of the task in Appendix~\ref{app:exp:promoter}.
We measure the mean squared error (MSE) between the predicted regulatory activity of the generated sequence and that of the original sequence corresponding to the transcription profile.
Table~\ref{tab:promoter_dna} shows that our framework achieves the lowest MSE, outperforming the flow matching methods~\citep{stark2024dirichlet,davis2024fisherflow} and the discrete diffusion diffusion model~\citep{austin2021d3pm}.
% Due to the small vocabulary and limited model parameters, we 



\subsection{Analysis}

\paragraph{Approximation of Transition Distribution} \label{exp:transition}
In Figure~\ref{fig:analysis_transition}, we measure the maximum mean discrepancy (MMD)~\citep{gretton2012kernel} distance between the simulated transition distribution and the approximated distribution. 
The approximated distributions show almost the same MMD as the simulated distributions, indicating that the approximation is reliable. 
In particular, the discrepancy becomes close to zero in the high-dimensional manifold, where the simulation of the SDE becomes expensive.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/promoter_dna}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%s


\paragraph{Training Objective} \label{exp:objective}
We validate the effectiveness of the cross-entropy-based loss of Eq.~\eqref{eq:ce_objective} in Table~\ref{tab:analysis_objective}. 
Compared to the mean-squared error-based loss of Eq.~\eqref{eq:mixture_objective}, the cross-entropy loss provides faster convergence in training and better NLL.
Furthermore, Table~\ref{tab:analysis_objective} shows that applying importance sampling (Eq.~\eqref{eq:importance_mixture_objective}) improves the performance.


\paragraph{Dimension Splitting}
For datasets with a large vocabulary, for example, LM1B dataset, splitting the dimension of the manifold yields a significant improvement.
Table~\ref{tab:analysis_dimension} shows that the generative model on the high-dimensional manifold cannot be trained due to the large input dimension. While adding additional information to the model does improve the result, the abrupt convergence of bridge processes on high dimensions makes them challenging to learn. For a large vocabulary, we achieve the best result by splitting the dimensions into smaller ones and modeling the generative process using a mixture path.

