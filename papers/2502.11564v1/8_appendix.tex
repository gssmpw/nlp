\appendix
\onecolumn

\vspace{0.5in}
\begin{center}{\bf {\LARGE Appendix}}\end{center}

% \section{Preliminaries} \label{app:background}

% \subsection{Discrete Diffusion Models}

% The transition matrices for the masked diffusion and uniform diffusion are defined as follows:
% \begin{align}
%     \begin{split}
%     &\bar{Q}^{mask}_t = \alpha_t\bm{I} + (1-\alpha_t)\mathbf{1}\bm{e}^T_m, \\
%     &\bar{Q}^{unif}_t = (1-\alpha_t)\bm{I} + \frac{\alpha_t}{d}\mathbf{1}\mathbf{1}^T,
% \end{split}
% \end{align}


% \subsection{Riemannian Manifold}
% In this work, we deal with two types of Riemannian manifold: statistical manifold of categorical distributions and hypersphere.







\section{Derivations \label{app:derivation}}

\subsection{Preliminaries} \label{app:derivation:prelim}

\paragraph{Statistical Manifold of Categorical Distributions}

For a discrete sample space $\mathcal{X}=\{1,2,\cdots,d\}$, a $d$-class categorical distribution over $\mathcal{X}$ is parameterized by $d$ number of parameters $p_1,\cdots,p_d \geq 0$ such tat $\sum^d_{i=1} p_i = 1$.
The parameter space corresponds to the $(d-1)$-dimensional probability simplex:
\begin{align}
    \Delta^{d-1} \coloneqq \{ (p_1,\cdots,p_d)\in\mathbb{R}^d | \sum^{d}_{i=1} p_i = 1, p_i\geq 0 \},
\end{align}
A natural choice of a Riemannian metric on the simplex is the Fisher-Rao metric~\citep{rao1992information,amari2016information}.
For an interior point $\bm{p}\in\Delta^{d-1}$, the Fisher-Rao metric is defined as follows:
\begin{align}
    g_{FR}(\bm{p})[\bm{x},\bm{y}] \coloneqq \langle \bm{x},\bm{y} \rangle_{\bm{p}} \coloneqq \left\langle \frac{\bm{x}}{\sqrt{\bm{p}}}, \frac{\bm{y}}{\sqrt{\bm{p}}} \right\rangle  = \sum^{d}_{i=1} \frac{\bm{x}_i \bm{y}_i}{\bm{p}_i}, \;\; \bm{x}, \bm{y} \in \mathcal{T}_{\bm{p}} \Delta^{d-1}, 
\end{align}
where the normalization by $\sqrt{\bm{p}}$ in the inner product is performed component-wise.
This induces a geodesic distance on the simplex defined as follows:
\begin{align}
    d(\bm{p}, \bm{q}) = 2 \cos^{-1}\left(\sum^d_{i=1} \sqrt{p_i q_i}\right), \;\; \bm{p}, \bm{q} \in \Delta^{d-1},
\end{align} 
where $\bm{p}$ and $\bm{q}$ corresponds to the parameters of categorical distributions.
The probability simplex $\Delta^{d-1}$ equipped with the Fisher-Rao metric is a Riemannian manifold called the statistical manifold of categorical distribution, denoted as $\mathcal{P}(\mathcal{X})$ throughout the paper.
The tangent space at an interior point $\bm{p}$ is identified as $\mathcal{T}_{\bm{p}}(\mathcal{P}(\mathcal{X})) = \{\bm{x}\in\mathbb{R}^d|\sum^d_{i=1}\bm{x}_i = 0\}$.
For further details on the geometry of the statistical manifold, we refer the reader to \citet{ay2017information}.



\paragraph{Hypersphere}

The hypersphere $\mathbb{S}^{d-1}$ denotes the $(d-1)$-dimensional sphere $\{\bm{u}=(\bm{u}_1,\cdots,\bm{u}_d)| \sum_i \bm{u}_i^2=1\}$ and $\mathbb{S}^{d-1}_{+}=\{\bm{u}=(\bm{u}_1,\cdots,\bm{u}_d)| \sum_i \bm{u}_i^2=1, \bm{u}_i\geq 0\}$ denotes a positive orthant of $\mathbb{S}^{d-1}$.
The hypersphere $\mathbb{S}^{d-1}$ can be embedded into the ambient Euclidean space $\mathbb{R}^d$, which induces a canonical inner product $\big\langle \bm{x}, \bm{y} \big\rangle \coloneqq \sum^d_{i=1} \bm{x}_i\bm{y}_i$ for $\bm{x}$, $\bm{y}$ in the tangent space at point $\bm{u}\in\mathbb{S}^{d-1}$: $\mathcal{T}_{\bm{u}}(\mathbb{S}^{d-1}) \coloneqq \{\bm{x} | \langle \bm{x}, \bm{u}\rangle=0 \}$.

For a discrete sample space $\mathcal{X}=\{1,2,\cdots,d\}$, there exists a diffeomorphism from $\mathcal{P}(\mathcal{X})$ to $\mathbb{S}^{d-1}_{+}$ defined as follows:
\begin{align}
\begin{split}
    &\pi: \mathcal{P}(\mathcal{X}) \rightarrow \mathbb{S}^{d-1}_{+} \;\; ; \;\; \bm{p}_i\mapsto \bm{u}_i=\sqrt{\bm{p}_i}, \\[6pt] 
    &\pi^{-1}: \mathbb{S}^{d-1}_{+} \rightarrow \mathcal{P}(\mathcal{X}) \;\; ; \;\;  \bm{u}_i\mapsto \bm{p}_i= \bm{u}_i^2.
\end{split}
\label{eq:diffeomorphism_app}
\end{align}
The diffeomorphism induces the the geodesic distance on $\mathbb{S}^{d-1}_{+}$:
\begin{align}
    d_g(\bm{u},\bm{v}) = \cos^{-1}\langle\bm{u}, \bm{v}\rangle, \;\; \bm{u},\bm{v}\in\mathbb{S}^{d-1}_{+},
\end{align}
for which the geodesic corresponds to the great circle connecting two points $\bm{u}$ and $\bm{v}$.
The corresponding exponential and logarithm maps can be computed as follows:
\begin{align}
    &\exp_{\bm{u}}{\bm{x}} = \cos(\|\bm{x}\|)\bm{u} + \sin(\|\bm{x}\|)\frac{\bm{x}}{\|\bm{x}\|} \;, \;\; 
    \bm{u}\in\mathbb{S}^{d-1} , \bm{x}\in \mathcal{T}_{\bm{u}}(\mathbb{S}^{d-1}), \\
    &\logmap{\bm{u}}{\bm{v}} = \frac{\invcos\langle \bm{u},\bm{v} \rangle}{\sin\invcos\langle \bm{u},\bm{v} \rangle}\Big( \bm{v} - \langle \bm{u},\bm{v} \rangle\bm{u} \Big) \;,\;\; \bm{u}, \bm{v} \in \mathbb{S}^{d-1}.
\label{eq:sphere_exp_log}
\end{align}

Additionally, define the radial distance $r^{\bm{v}}(\bm{x})\coloneqq d_g(\bm{x}, \bm{v}) \in \mathbb{R}$ where $d_g$ denotes the geodesic distance defined on $\mathbb{S}^{d-1}$. Then we have the following identities:
\begin{align}
    &\nabla r^{\bm{v}}(\bm{x})
    = -\frac{\bm{v} - \langle \bm{v}, \bm{x}\rangle \bm{x}}{\sqrt{1 - \langle \bm{v}, \bm{x}\rangle^2}}, \\
    &\Delta r^{\bm{v}}(\bm{x}) = (d-1)\cot(r^{\bm{v}}(\bm{x})), \\[6pt]
    &\Big\langle \nabla r^{\bm{v}}(\bm{x}), \nabla r^{\bm{w}}(\bm{x}) \Big\rangle 
    = \frac{\langle\bm{v}, \bm{w}\rangle - \langle\bm{v}, \bm{x}\rangle \langle\bm{w}, \bm{x}\rangle}{\sqrt{ \left(1 - \langle\bm{v}, \bm{x}\rangle^2\right) \left(1 - \langle\bm{w}, \bm{x}\rangle^2\right) }}
    = \frac{\langle\bm{v},\bm{w}\rangle - \cos r^{\bm{v}}(\bm{x})\cos r^{\bm{w}}(\bm{x})}{\sin r^{\bm{v}}(\bm{x}) \sin r^{\bm{w}}(\bm{x})}.
\end{align}
In particular, the logarithm map in Eq.~\eqref{eq:sphere_exp_log} can be represented in radial distance:
\begin{align}
    \logmap{\bm{x}}{\bm{v}} = -r^{\bm{v}}(\bm{x}) \nabla r^{\bm{v}}(\bm{x}),
\end{align}


\subsection{Connection Between Discrete Diffusion Models and Continuous Flow on Hypersphere}\label{app:derivation:generalization}
In this section, we derive the connection between the discrete diffusion models and the continuous flow on a hypersphere.


\paragraph{Continuous Flow on Hypersphere}
We first derive useful lemmas for the continuous flows defined on hyperspheres.

\begin{tcolorbox}
% [colback=blue!4!white,colframe=blue!0!white]
[colback=white,colframe=blue!30!white]
\begin{lemma}
\label{lem:flow_solution}
Define a flow $\bm{Y}_t$ on $\mathbb{S}^d$ in the time horizon $[0,T]$ as follows:
\begin{align}
    \frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t} = 
    -\frac{\mathrm{d}\log \kappa_t}{\mathrm{d}t}
    \exp^{-1}_{\bm{Y}_t}(\bm{y}_T), \;\; \bm{Y}_0=\bm{y}_0, 
\label{eq:flow_def}
\end{align}
where $\kappa_t:[0,T]\rightarrow[0,1]$ is a scalar function satisfying $\kappa_0=1$ and $\kappa_T=0$, and $\bm{y}_T\in\mathbb{S}^d$. 
Then $\bm{Y}_t$ has a closed form solution:
\begin{align}
    \bm{Y}_t = \frac{\sin(\theta_0-\theta_t)}{\sin\theta_0}\bm{y}_T + \frac{\sin\theta_t}{\sin\theta_0}\bm{y}_0, \;\;
    \theta_t\coloneqq \kappa_t\invcos \langle \bm{y}_0,\bm{y}_T \rangle,
\label{eq:flow_solution}
\end{align}
with the endpoint $\bm{Y}_T=\bm{y}_T$, which corresponds to the spherical linear interpolation, i.e., slerp:
\begin{align}
    \bm{Y}_t
    = \exp_{\bm{y}_T}\Big( \kappa_{t}\exp^{-1}_{\bm{y}_T}(\bm{y}_0) \Big) 
    = \exp_{\bm{y}_0}\Big( \kappa_{T-t}\exp^{-1}_{\bm{y}_0}(\bm{y}_T) \Big) .
\label{eq:geodesic}
\end{align}
\end{lemma}
\end{tcolorbox}

\begin{proof}
Let $\theta_t\coloneqq \invcos \langle \bm{Y}_t,\bm{y}_T \rangle$. Then $\bm{Y}_t$ can be written as follows:
\begin{align}
    \bm{Y}_t = \cos\theta_t\bm{y}_T + \sin\theta_t\bm{w}_t,
\end{align}
where $\bm{w}_t\in\mathbb{R}^{d+1}$ is an unit vector.
From the definition of $\theta_t$, we have the following identity:
\begin{align}
    \frac{\mathrm{d}\theta_t}{\mathrm{d}t} 
    &= -\frac{1}{\sin\theta_t} \left\langle \frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t}, \bm{y}_T \right\rangle
    = -\frac{1}{\sin\theta_t} \left\langle -\frac{\mathrm{d}\log\kappa_t}{\mathrm{d}t}
    \frac{\theta_t(\bm{y}_T - \bm{Y}_t\cos\theta_t)}{\sin\theta_t}, \bm{y}_T \right\rangle \\
    &= \frac{1}{\sin\theta_t}\frac{\mathrm{d}\log\kappa_t}{\mathrm{d}t} \theta_t \frac{1 - \cos^2\theta_t}{\sin\theta_t}
    = \frac{\mathrm{d}\log\kappa_t}{\mathrm{d}t} \theta_t ,
\label{eq:theta_derivative}
\end{align}
which yields representation of the flow $\bm{Y}_t$ in Eq.~\eqref{eq:flow_def} with respect to $\theta$:
\begin{align}
    \frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t} 
    &= \frac{\mathrm{d}\theta_t}{\mathrm{d}t}\frac{\bm{y}_T - \bm{Y}_t\cos\theta_t}{\sin\theta_t}.
\label{eq:flow_in_theta}
\end{align}
Using the result of Eq.~\eqref{eq:flow_in_theta}, we can see that $\bm{w}_t$ is a constant vector independent of $t$:
\begin{align}
    \frac{\mathrm{d}\bm{w}_t}{\mathrm{d}t} 
    &= \frac{1}{\sin^2\theta_t}
    \left[\left(\frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t} - \frac{\mathrm{d}\cos\theta_t}{\mathrm{d}t}\bm{y}_T\right)\sin\theta_t - \left(\bm{Y}_t - \cos\theta_t\bm{y}_T\right)\frac{\mathrm{d}\sin\theta_t}{\mathrm{d}t}\right] \\
    &= \frac{1}{\sin^2\theta_t}\frac{\mathrm{d}\theta_t}{\mathrm{d}t}
    \Big[
        -(\bm{y}_T - \bm{Y}_t\cos\theta_t) + \sin^2{\theta_t}\bm{y}_T
        - \cos\theta_t\bm{Y}_t  + \cos^2\theta_t\bm{y}_T
    \Big]
    =0.
\end{align}
Therefore we get the closed form solution for $\bm{Y}_t$:
\begin{align}
    \bm{Y}_t = \cos\theta_t\bm{y}_T + \sin\theta_t\frac{\bm{y}_0 - \cos\theta_0\bm{y}_T}{\sin\theta_0}
    = \frac{\sin(\theta_0-\theta_t)}{\sin\theta_0}\bm{y}_T + \frac{\sin\theta_t}{\sin\theta_0}\bm{y}_0 ,
\end{align}
where $\theta_t=\kappa_t\theta_0$ from Eq.~\eqref{eq:theta_derivative}.
Note that the solution Eq.~\eqref{eq:flow_solution} is well-defined in the sense that $\sin\theta_0>0$ always holds. This is because $\|\langle \bm{Y}_t, \bm{y}_T \rangle\|\leq 1$ as $\bm{Y}_t$ and $\bm{y}_T$ are on $\mathbb{S}^d_+$.
Finally, using the definition of $\theta_t$, we can show the following:
\begin{align}
    \exp^{-1}_{\bm{Y}_T}(\bm{Y}_t) 
    = \theta_t\frac{\bm{Y}_t - \bm{Y}_T\cos\theta_t}{\sin\theta_t} 
    = \kappa_t\theta_0\bm{w}_t = \kappa_t\theta_0\bm{w}_0 
    = \kappa_t\exp^{-1}_{\bm{Y}_T}(\bm{Y}_0) ,
\end{align}
which proves Eq.~\eqref{eq:geodesic}.
\end{proof}

The following lemma describes the reverse process of the continuous flow $\bm{Y}_t$ described in Lemma~\ref{lem:flow_solution}.
\begin{tcolorbox}
[colback=white,colframe=blue!30!white]
\begin{lemma}
For a flow $\bm{Y}_t$ on $\mathbb{S}^d_{+}$ in the time horizon $[0,T]$:
\begin{align}
    \frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t} = 
    -\frac{\mathrm{d}\log \kappa_t}{\mathrm{d}t}
    \exp^{-1}_{\bm{Y}_t}(\bm{y}_T), \;\; \bm{Y}_0=\bm{y}_0, 
\label{eq:forward_flow}
\end{align}
the following ODE describes the reverse process $\bm{X}_{t}\coloneqq\bm{Y}_{T-t}$:
\begin{align}
    \frac{\mathrm{d}\bm{X}_t}{\mathrm{d}t} = 
    -\frac{\mathrm{d}\log \kappa_{T-t}}{\mathrm{d}t}
    \exp^{-1}_{\bm{X}_t}(\bm{y}_0), \;\; \bm{X}_0=\bm{y}_T.
\label{eq:reverse_flow}
\end{align}
$\bm{X}_t$ is also a spherical linear interpolation with scheduler $\kappa_t$:
\begin{align}
    \bm{X}_t = \exp_{\bm{X}_0}\Big( \kappa_{T-t}\exp^{-1}_{\bm{X}_0}(\bm{X}_T) \Big) = \exp_{\bm{X}_T}\Big( \kappa_{t}\exp^{-1}_{\bm{X}_T}(\bm{X}_0) \Big) .
\end{align}
\end{lemma}
\end{tcolorbox}


\paragraph{Masked Diffusion Model}
Now we show that masked diffusion models correspond to a continuous flow on the statistical manifold that starts from an absorbing state.

\begin{tcolorbox}
[colback=white,colframe=blue!30!white]
\begin{proposition}
\label{prop:uniform_flow}
Define a flow $\bm{Y}_t$ on $\mathbb{S}^d_{+}$ in the time horizon $[0,T]$ as follows:
\begin{align}
    &\frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t} = -\frac{\mathrm{d}\log \kappa_t}{\mathrm{d}t}
    \exp^{-1}_{\bm{Y}_t}(\bm{e}_m), \;\; \bm{Y}_0=\bm{e}_k, \;\;
    \kappa_t = \frac{2}{\pi}\sin^{-1}\!\left( \sqrt{\alpha_t} \right)
    % &b^{mask}_t = -\frac{1}{2\sin^{-1}\!\left(\sqrt{\alpha_t}\right)\sqrt{\alpha_t(1-\alpha_t)}}\frac{\mathrm{d}\alpha_t}{\mathrm{d}t}, 
\label{eq:mask_flow}
\end{align}
where $\bm{e}_m$ denotes the mask token and $\alpha_t\in[0,1]$ is some differentiable noise schedule satisfying $\alpha_0\approx1$ and $\alpha_1\approx0$.
Then the random variable $\bm{Z}_t\coloneqq \pi\left(\bm{Y}_t \right) \in\mathbb{R}^{d+1}$ satisfies the following:
\begin{align}
     \bm{Z}_t = \alpha_t\bm{e}_k + (1-\alpha_t)\bm{e}_m,
\label{eq:mask_simplex}
\end{align}
which interpolates $\bm{e}_m$ and $\bm{e}_k$ on the probability simplex $\Delta^{d}$.
\end{proposition}
\end{tcolorbox}

\begin{proof}
Using Lemma~\ref{lem:flow_solution} with $\theta_0 = \invcos \langle \bm{e}_m,\bm{e}_k \rangle=\pi/2$, we have the following representation of $\bm{Y}_t$:
\begin{align}
    \bm{Y}_t = \sin(\theta_0 - \theta_t)\bm{e}_m + \sin\theta_t\bm{e}_k 
    = \sqrt{1-\alpha_t}\bm{e}_m + \sqrt{\alpha_t}\bm{e}_k ,
\end{align}
since $\theta_t = \sin^{-1}\!(\sqrt{\alpha_t})$.
Therefore, $\bm{Z}_t$ has the following closed form:
\begin{align}
    \bm{Z}_t = ({1-\alpha_t})\bm{e}_m + {\alpha_t}\bm{e}_k,
\end{align}
which is a random variable on $\Delta^d$ which interpolates between $\bm{e}_m$ and $\bm{e}_k$ in a straight line.
\end{proof}


Note that $\bm{Z}_t$ is a random variable on  $\Delta^{d}$ representing the categorical distribution $\text{Cat}(\alpha_t\bm{e}_{x_0} + (1-\alpha_t)\bm{e}_m)$.
This corresponds to the transition distribution $q(x_t|x_0)$ of a masked discrete diffusion model, where the transition matrix for the mask diffusion process is given as follows: 
\begin{align}
    Q^{absorb}_t = \begin{bmatrix}
        \alpha_t & 0 & \cdots & 0 & 0 \\
        0 & \alpha_t & \cdots & 0 & 0 \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\ 
        0 & 0 & \cdots & \alpha_t & 0 \\
        1-\alpha_t & 1-\alpha_t & \cdots & 1-\alpha_t & 1-\alpha_t
    \end{bmatrix}
\end{align}

\begin{tcolorbox}
[colback=white,colframe=blue!30!white]
\begin{corollary}
The masked discrete diffusion process can be modeled by a continuous flow on $\mathbb{S}^{d}_{+}$ that starts from the absorbing state $\bm{e}_m$.
\end{corollary}
\end{tcolorbox}


\paragraph{Uniform Diffusion Model}
We also show that uniform diffusion models correspond to a continuous flow on the statistical manifold that starts from the barycenter of the simplex.

\begin{tcolorbox}
[colback=white,colframe=blue!30!white]
\begin{proposition}
\label{prop:mask_flow}
Define a flow $\bm{Y}_t$ on $\mathbb{S}^{d-1}_{+}$ in the time horizon $[0,T]$ as follows:
\begin{align}
    &\frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t} = -\frac{\mathrm{d}\log \kappa_t}{\mathrm{d}t}
    \exp^{-1}_{\bm{Y}_t} \left(
        \sum^{d}_{i=1} \frac{1}{\sqrt{d}}\bm{e}_i 
    \right), \;\; 
    \bm{Y}_0=\bm{e}_k, \;\;
    \kappa_t = 1 - \frac{\invsin \left( \frac{\sqrt{d-1}}{d}(1-\alpha_t) \right)}{\invcos (1/\sqrt{d})}
    % \frac{2}{\pi}\sin^{-1}\!\left( \sqrt{\alpha_t} \right)
\label{eq:uniform_flow}
\end{align}
where $\alpha_t\in[0,1]$ is a differentiable noise schedule satisfying $\alpha_0\approx1$ and $\alpha_1\approx0$.
Then the random variable $\bm{Z}_t\coloneqq \pi\left(\bm{Y}_t \right)\in\mathbb{R}^{d}$ satisfies the following:
\begin{align}
    \bm{Z}_t = \sum_{i\neq k}\frac{1-\alpha_t}{d}\bm{e}_i + \frac{1 + (d-1)\alpha_t}{d}\bm{e}_k ,
% \label{eq:uniform_simplex}
\end{align}
which interpolates $\sum^d_{i=1} \bm{e}_i/\sqrt{d}$ and $\bm{e}_k$ on the probability simplex $\Delta^{d-1}$.
\end{proposition}
\end{tcolorbox}

\begin{proof}
Using Lemma~\ref{lem:flow_solution} with $\theta_0 = \invcos \left\langle \sum^{d}_{i=1} \frac{1}{\sqrt{d}}\bm{e}_i,\bm{e}_k \right\rangle = \invcos (1/\sqrt{d})$, we have the following representation of $\bm{Y}_t$:
\begin{align}
    \bm{Y}_t = \frac{\sin(\theta_0 - \theta_t)}{\sin\theta_0} \sum^{d}_{i=1} \frac{1}{\sqrt{d}}\bm{e}_i + \frac{\sin\theta_t}{\sin\theta_0} \bm{e}_k 
    = \sum_{i\neq k} \frac{\sin(\theta_0 - \theta_t)}{\sqrt{d-1}} \bm{e}_i + \left(
        \frac{\sqrt{d}\sin\theta_t}{\sqrt{d-1}} +  \frac{\sin(\theta_0 - \theta_t)}{\sqrt{d-1}} 
    \right) \bm{e}_k.
\end{align}
Due to the definition of $\kappa_t$, $\bm{Z}_t$ has the following closed form:
\begin{align}
    \bm{Z}_t = \sum_{i\neq k}\frac{1-\alpha_t}{d}\bm{e}_i + \frac{1 + (d-1)\alpha_t}{d}\bm{e}_k ,
\end{align}
which is a random variable on $\Delta^d$ that interpolates between $\sum^{d}_{i=1} \frac{1}{\sqrt{d}}\bm{e}_i$ and $\bm{e}_k$ in a straight line.
\end{proof}

Note that $\bm{Z}_t$ is a random variable on $\Delta^{d-1}$ representing the categorical distribution:
\begin{align}
    \text{Cat}\left(\sum_{i\neq x_0}\frac{1-\alpha_t}{d}\bm{e}_i + \frac{1 - (d-1)\alpha}{d}\bm{e}_{x_0}\right),
\end{align}
which corresponds to the transition distribution $q(x_t|x_0)$ of a uniform discrete diffusion model.
The transition matrix for the uniform diffusion process is given as follows: 
\begin{align}
    Q^{absorb} = \begin{bmatrix}
        1-N & 1 & \cdots & 1 \\
        1 & 1-N & \cdots & 1 \\
        \vdots & \vdots & \ddots & \vdots \\ 
        1 & 1 & \cdots & 1-N
    \end{bmatrix}
\end{align}



\begin{tcolorbox}[colback=white,colframe=blue!30!white]
\begin{corollary}
The uniform discrete diffusion process can be modeled by a continuous flow on $\mathbb{S}^{d}_{+}$ that starts from the barycenter of the simplex.
\end{corollary}
\end{tcolorbox}



\subsection{Generative Process on Hypersphere} \label{app:derivation:log}
On general manifold $\mathcal{M}$, the logarithm bridge process~\citep{jo2024riemannian} which bridges $\bm{u}\in\mathcal{M}$ and $\bm{v}\in\mathcal{M}$ is defined as follows:
\begin{align}
    \mathrm{d}\bm{X}^k_t 
    &= \frac{\sigma^2_t}{\int^T_t \sigma^2_s\mathrm{d}s} \logmap{\bm{X}_t}{\bm{v}} \mathrm{d}t + \sigma_t \mathrm{d}\mathbf{B}^{\mathcal{M}}_t, \;\; 
    \bm{X}_0 = \bm{u}
\label{eq:bridge_app}
\end{align}
where $\logmap{x}{\cdot}$ denotes the logarithm map on $\mathcal{M}$ at point $x$ and $\mathbf{B}^{\mathcal{M}}_t$ is the Brownian motion defined on $\mathcal{M}$.

In the case of $\mathcal{M}=\mathbb{S}^d$, we can derive the logarithm bridge process from $\bm{u}$ to $\bm{e}_k$:
\begin{align}
    \mathrm{d}\bm{X}^k_t 
    = \frac{\sigma^2_t}{\int^T_t \sigma^2_s\mathrm{d}s} \frac{\bm{\phi}_t(\bm{e}_k - \cos\bm{\phi}_t \bm{X}^k_t)}{\sin\bm{\phi}_t} \mathrm{d}t + \sigma_t\mathrm{d}\mathbf{B}^{\mathcal{M}}_t, \;\;
    \bm{\phi}_t &\coloneqq \invcos\langle\bm{X}^{k}_t, \bm{e}_k \rangle, \;\; 
    \bm{X}^{k}_0=\bm{u},
\label{eq:bridge_sphere_app}
\end{align}
where we used the logarithm map of Eq.~\eqref{eq:sphere_exp_log}.


\paragraph{Radial Process \label{app:derivation:radial}}
Let $r^{\bm{w}}_t \coloneqq d_g(\bm{w}, \bm{X}_t)$ for arbitrary point $\bm{w}\in\mathbb{S}^d$. 
Then the bridge process from $\bm{u}$ to $\bm{e}_k$ can be represented as follows:
\begin{align}
    \mathrm{d}\bm{X}_t 
    &= \gamma_t \frac{r^k_t(\bm{e}_k - \cos r^k_t \bm{X}^k_t)}{\sin r^k_t} \mathrm{d}t + \sigma_t \mathrm{d}\mathbf{B}^{d+1}_t, \;\; 
    \bm{X}_0 = \bm{u},
\end{align}
where $r^k_t\coloneqq r^{\bm{e}_k}_t$ and $\mathbf{B}^{d+1}_t$ is a Brownian motion defined on $\mathbb{S}^d$.
The SDE of the radial process $r^{\bm{w}}_t$ can be derived using the It\^{o}'s formula as follows:
\begin{align}
    \mathrm{d}r^{\bm{w}}_t
    &= \left[ 
        \left\langle \nabla r^{\bm{w}}_t, \gamma_t 
            \frac{r^k_t(\bm{e}_k - \cos r^k_t \bm{X}^k_t)}{\sin r^k_t}
        \right\rangle + \frac{\sigma^2_t}{2} \Delta r^{\bm{w}}_t
    \right] \mathrm{d}t
    + \Big\langle \nabla r^{\bm{w}}_t, \sigma_t \mathrm{d}\mathbf{B}_t \Big\rangle,
\end{align}
where $\nabla$ and $\Delta$ denote the Riemannian gradient and the Laplace-Beltrami operator on $\mathbb{S}^d$, respectively.
From the identities in Appendix~\ref{app:derivation:prelim} and the fact that $\langle \nabla r^{\bm{w}}_t, \mathrm{d}\mathbf{B}^{d+1}_t \rangle$ is a 1-dimensional Brownian motion (\citep{hsu2002stochastic} Example 3.3.3), we get the following result:
\begin{align}
    \mathrm{d}r^{\bm{w}}_t = \left[ 
        -\gamma_t \; r^{\bm{w}}_t 
            \frac{\langle \bm{e}_k,\bm{w}\rangle - \cos r^{k}_t \cos r^{\bm{w}}_t}{\sin r^{k}_t \sin r^{\bm{w}}_t}
    + \frac{(d-1)\sigma^2_t}{2}\cot(r^{\bm{w}}_t) 
    \right] \mathrm{d}t 
    + \sigma_t\mathrm{d}W_t , \;\;
    r^{\bm{w}}_0 \coloneqq \cos^{-1}\langle \bm{u}, \bm{w} \rangle,
\label{eq:radial_process}
\end{align}
where $W_t$ denotes 1-dimensional Brownian motion.

For $\bm{w}=\bm{e}_l$, we obtain a simplified formulation:
\begin{align}
    &\mathrm{d}r^l_t = \left[ -\frac{\sigma^2_t}{\tau_T-\tau_t} C(r^k_t, r^l_t) r^l_t 
    + \frac{(d-1)\sigma^2_t}{2}\cot(r^l_t) \right] \mathrm{d}t + \sigma_t\mathrm{d}W_t , \;\; r^l_0 = \frac{\pi}{2} \delta_{k,l} \\[6pt]
    &C(r^k_t, r^l_t) = \begin{cases}
        1 &\text{ if } k=l \\
        -\cot(r^k_t)\cot(r^l_t) &\text{ otherwise }
    \end{cases}.
\end{align}


\subsection{Diffusion Mixture Representation} \label{app:derivation:mixture}
We provide the statement of the diffusion mixture representation from \citet{jo2024riemannian}, which extends \citet{peluchetti2021mixture} to Riemannian manifolds. We refer the readers to \citet{jo2024riemannian} for the derivation.

\begin{tcolorbox}[colback=white,colframe=blue!30!white]
\begin{proposition} \label{prop:mixture}
    For a collection of diffusion processes on Riemannian manifold $\mathcal{M}$ $\{\mathbb{Q}^{\lambda}:\lambda\in\Lambda\}$ and a mixing distribution $\mathcal{L}$ on $\Lambda$, there exists a diffusion process on $\mathcal{M}$ with marginal distribution $p_t$ satisfying the following:
    \begin{align}
        p_t(x) = \int p^{\lambda}_t(x) \mathcal{L}(\mathrm{d}\lambda), \;\; 
        p_0(x) = \int p^{\lambda}_0(x) \mathcal{L}(\mathrm{d}\lambda),
    \end{align}
    where $p^{\lambda}$ denotes the marginal distribution of $\mathbb{Q}^{\lambda}$.
    This process is described by the following SDE:
    \begin{align}
        \mathrm{d}\bm{X}_t = \left[ 
            \int \eta^{\lambda}(\bm{X}_t,t)\frac{p^{\lambda}_t(\bm{X}_t)}{p_t(\bm{X}_t)} \mathcal{L}(\mathrm{d}\lambda) 
        \right]\mathrm{d}t 
        + \sqrt{\int \sigma^{\lambda}(\bm{X}_t,t)^2 \frac{p^{\lambda}_t(\bm{X}_t)}{p_t(\bm{X}_t)} \mathcal{L}(\mathrm{d}\lambda)} \; \mathrm{d}\mathbf{B}^{\mathcal{M}}_t, \;\;
        \bm{X}_0\sim p_0
    \end{align}
    where $\eta^{\lambda}$ and $\sigma^{\lambda}$ denote the drift and diffusion coefficient of $\mathbb{Q}^{\lambda}$, respectively.
\end{proposition}
\end{tcolorbox}

% \paragraph{Generative Process}
% We construct a generative process on $\mathbb{S}^{d-1}$ as a mixture of bridge processes in Eq.~\eqref{eq:logarithm_bridge} with mixing distribution $p^{\ast}$:
% \begin{align}
%     \mathrm{d}\bm{X}_t 
%     &= \left[\, \sum^{d}_{k=1} \eta^{k}(\bm{X}_t,t) \frac{p^k_t(\bm{X}_t)}{p_t(\bm{X}_t)} \,\right] \mathrm{d}t + \sigma_t \mathrm{d}\mathbf{B}^{d}_t 
%     = \left[\, \sum^{d}_{k=1} \eta^{k}(\bm{X}_t,t) p_{T|t}(\bm{X}_T=\bm{e}_k|\bm{X}_t)  \,\right] \mathrm{d}t + \sigma_t \mathrm{d}\mathbf{B}^{d}_t
% \end{align}
% where $\eta^{k}$ denotes the drift of the bridge process.


\paragraph{Mixture Paths} \label{app:derivation:mixture_path}
We derive a new family of generative processes by mixing the probability paths of diffusion processes.
From the diffusion mixture representation, we construct a mixture process by mixing the probability paths $\{\mathbb{Q}^i:1\leq i\leq n\}$ with mixing distribution $\{\lambda^i_t:1\leq i\leq n\}$ as follows:
\begin{align}
    &\mathbb{Q}_t \coloneqq \sum^n_{i=1} \lambda^{i}_t \mathbb{Q}^i_t \;\;; \;\; p_t(x) = \sum^n_{i=1} \lambda^i_t p^i_t(x) \\
    & \mathrm{d}\bm{X}_t = \sum^n_{i=1}  
        R^{i}(\bm{X}_t,t) \eta^{i}(\bm{X}_t,t)
    \mathrm{d}t + 
        \sqrt{\sum^n_{i=1} R^{i}(\bm{X}_t,t) (\sigma^i_t)^2 }
    \mathrm{d}\mathbf{B}_t, \;\; 
    R^{i}(x,t) \coloneqq \frac{\lambda^i_t p^i_t(x)}{p_t(x)} .
\end{align}

One example is creating a mixture process from the masked bridge mixture and the uniform bridge mixture that share the same noise schedule $\sigma_t$, with mixing probability $1-\lambda_t$ and $\lambda_t$, respectively:
\begin{align}
    &\lambda_t\mathbb{Q}^{mask} + (1-\lambda_t)\mathbb{Q}^{unif}: 
    p_t(x) = (1 - \lambda_t) p^{mask}_t(x) + \lambda_t p^{unif}_t(x) \\
    &\mathrm{d}\bm{X}_t = \left(
        \gamma_t \sum^{d}_{k=1} \bigg[ 
            (1-R(\bm{X}_t, t)) p^{mask}_{T|t}(\bm{e}_k|\bm{X}_t) + R(\bm{X}_t, t) p^{unif}_{T|t}(\bm{e}_k|\bm{X}_t)
        \bigg] \logmap{\bm{X}_t}{\bm{e}_k}
    \right) \mathrm{d}t + 
    \sigma_t\mathrm{d}\mathbf{B}_t, \\
    & R(x, t) \coloneqq \frac{\lambda_t p^{unif}_t(x)}{(1-\lambda_t) p^{mask}_t(x) + \lambda_t p^{unif}_t(x)}
    % + \frac{\lambda_t p^{unif}_t(x)}{(1-\lambda_t) p^{mask}_t(x) + \lambda_t p^{unif}_t(x)}
\end{align}



% \subsection{Likelihood Bound \label{app:derivation:elbo}}

% ELBO for the approximated generative process can be derived using the Girsanov theorem:
% \begin{align}
%     -\log p_{\theta}(\bm{z}) 
%     &= D_{KL}(\delta_{\bm{z}} \| p_{\theta}(\bm{X}_1)=\bm{z}) = D_{KL}(q^{\bm{z}}_1(\bm{z})\|p_{\theta}(\bm{X}_1)) \\
%     &\leq D_{KL}(\mathbb{Q}^{\bm{z}} \| \mathbb{Q}^{\theta}) \\
%     &= D_{KL}(\mathbb{Q}^{\bm{z}}_0\|\mathbb{Q}^{\theta}_0) +
%     \mathbb{E}_{\bm{X}\sim\mathbb{Q}^{\bm{z}}} \left[\frac{1}{2}\int^T_0 \bigg\| 
%         \sigma_t^{-1}\left(
%         \eta^{\bm{z}}(\bm{X}_t,t) - \eta_{\theta}(\bm{X}_t,t)
%     \right)\bigg\|^2\mathrm{d}t \right]
% \end{align}
% Therefore, the training objective corresponds to the Monte Carlo estimation of the ELBO.





\subsection{Projected Processes \label{app:derivation:coord}}

For a bridge process from $\bm{u}$ to $\bm{v}$ on $\mathbb{S}^d$, we can derive the projection $c^{\bm{v}}_t = \langle\bm{X}_t, \bm{v}\rangle$ using the It\^{o}'s formula for $f_{\bm{v}}(\bm{z})\coloneqq \langle \bm{z}, \bm{v} \rangle$ as follows:
\begin{align}
    \mathrm{d}c^{\bm{v}}_t 
    &= \left[ 
        \left\langle 
            \nabla f_{\bm{v}}(\bm{X}_t), 
            \gamma_t \frac{\bm{\phi}_t(\bm{e}_k - \cos\bm{\phi}_t \bm{X}^k_t)}{\sin\bm{\phi}_t}
        \right\rangle 
        + \frac{1}{2}\sigma_t^2 \Delta f_{\bm{v}}(\bm{X}_t) 
    \right] \mathrm{d}t 
    + \sigma_t \Big\langle \nabla f_{\bm{v}}(\bm{X}_t), \mathrm{d}\mathbf{B}^{d+1}_t \Big \rangle \\[6pt]
    &= \left[ 
        \left\langle 
            \bm{v} - \langle\bm{v}, \bm{X}_t\rangle\bm{X}_t, 
            \gamma_t \frac{\invcos\! c^{\bm{v}}_t}{\sqrt{1 - (c^{\bm{v}}_t)^2}} \Big(\bm{v} - \langle\bm{v}, \bm{X}_t\rangle\bm{X}_t \Big) 
        \right\rangle 
        - \frac{d\sigma_t^2}{2}c^{\bm{v}}_t
    \right]\mathrm{d}t + \sigma_t\sqrt{1 - (c^{\bm{v}}_t)^2}\mathrm{d}W_t \\[6pt]
    &= \left[
        \gamma_t \invcos\! c^{\bm{v}}_t \sqrt{1 - (c^{\bm{v}}_t)^2} -\frac{d\sigma_t^2}{2}c^{v}_t
    \right]\mathrm{d}t + \sigma_t\sqrt{1 - (c^{\bm{v}}_t)^2}\mathrm{d}W_t,
\end{align}
where we have used the following identities:
\begin{align}
    \nabla f_{\bm{v}}(\bm{z}) = \bm{v} - \langle \bm{v}, \bm{z} \rangle \bm{z}, \;\; \nabla f_{\bm{v}}(\bm{z}) = -d \langle \bm{v}, \bm{z}\rangle.
\end{align}
Note that the last term $-\frac{d\sigma_t^2}{2}c$ in the drift corresponds to the Laplacian of the inner product, which has a simple form due to the radial symmetry of the hypersphere. 


Similarly, $c^{\bm{u}}_t = \langle\bm{X}_t, \bm{u}\rangle$ can be derived using It\^{o}'s formula for $f_{\bm{u}}(\bm{z})\coloneqq \langle \bm{z}, \bm{u} \rangle$ as follows:
\begin{align}
    \mathrm{d}c^{\bm{u}}_t 
    &= \left[ 
        \left\langle 
            \bm{u} - \langle\bm{u}, \bm{X}_t\rangle\bm{X}_t, 
            \gamma_t \frac{\invcos\! c^{\bm{v}}_t}{\sqrt{1 - (c^{\bm{v}}_t)^2}} \Big(\bm{v} - \langle\bm{v}, \bm{X}_t\rangle\bm{X}_t \Big) 
        \right\rangle 
        - \frac{d\sigma_t^2}{2}c^{\bm{u}}_t
    \right]\mathrm{d}t + \sigma_t\sqrt{1 - (c^{\bm{u}}_t)^2}\mathrm{d}W_t \\[6pt]
    &= \left[
        \gamma_t \frac{\invcos\! c^{\bm{v}}_t}{\sqrt{1 - (c^{\bm{v}}_t)^2}} 
        \Big(\langle\bm{u},\bm{v}\rangle - c^{\bm{u}}_t c^{\bm{v}}_t \Big)
        -\frac{d\sigma_t^2}{2}c^{\bm{u}}_t
    \right]\mathrm{d}t 
    + \sigma_t\sqrt{1 - (c^{\bm{u}}_t)^2}\mathrm{d}W_t.
% \label{eq:coord_process}
\end{align}



\paragraph{Masked Diffusion}
Since the masked bridge process has $\bm{u}=\bm{e}_m$ and $\bm{v}=\bm{e}_k$ satisfying $\langle \bm{e}_m, \bm{e}_k\rangle=0$ for all $k\neq m$, the projected processes are described by the following SDEs:
\begin{align}
\mathrm{d}c^l_t = \left[ \gamma_t
    \frac{\invcos c^{k}_t}{\sqrt{1 - (c^{k}_t)^2}} \bigg( \delta_{l,k} - c^{l}_t c^{k}_t \bigg) -\frac{d\sigma_t^2}{2}c^{l}_t 
\right]\mathrm{d}t + \sigma_t\sqrt{1 - (c^{l}_t)^2}\mathrm{d}W^l_t,
\end{align}
with initial condition $c^{1:d}_0 = \bm{0}$ where $W^l_t$ are 1-dimensional standard Wiener processes.
% , and $\alpha_t$ and $\rho_t$ are shared for all the bridges.

\paragraph{Uniform Diffusion}
The uniform bridge process has $\bm{u}=\sum^{d}_{i=1} \frac{1}{d}\bm{e}_i$ and $\bm{v}=\bm{e}_k$, and the projected processes have a simple form:
\begin{align}
\mathrm{d}c^l_t = \left[ \gamma_t
    \frac{\invcos c^{k}_t}{\sqrt{1 - (c^{k}_t)^2}} \bigg( A_{l,k} - c^{l}_t c^{k}_t \bigg) -\frac{d\sigma_t^2}{2}c^{l}_t 
\right]\mathrm{d}t + \sigma_t\sqrt{1 - (c^{l}_t)^2}\mathrm{d}W^l_t,
\end{align}
with initial condition $c^{l}_0 = 1/\sqrt{d}$, where $A_{l,k} = 1/\sqrt{d}$ for $l\neq k$ and $A_{k,k}=1$. 



\subsection{Simulation-Free Training with Radial Symmetry} \label{app:derivation:coord_alpha}

Here we derive the parameters of the Riemannian normal distribution from the projected processes.
From the definition $c^{\bm{v}}_t\coloneqq \langle \bm{X}_t, \bm{v}\rangle$, we can derive the following:
\begin{align}
    \mathbb{E}c^{\bm{v}}_t = \mathbb{E} \langle\bm{X}_t, \bm{v} \rangle &\approx \mathbb{E}_{\bm{z}} \big\langle\exp_{\bm{\mu}_t}(\rho_t \bm{z}), \bm{v} \big\rangle, \;\; \bm{z}\sim \mathcal{N}_{T_{\bm{\mu}_t}\mathbb{S}^d}(\mathbf{0}, \mathbf{I}) \\
    &\stackrel{\text{Eq.}~\eqref{eq:sphere_exp_log}}{\phantom{..}=\phantom{..}}
    \mathbb{E}_{\bm{z}}\left\langle \cos(\rho_t\|\bm{z}\|)\bm{\mu}_t + \sin(\rho_t\|\bm{z}\|)\frac{\bm{z}}{\|\bm{z}\|}, \bm{v} \right\rangle \\
    &= \mathbb{E}_{\bm{z}}\bigg(\cos(\rho_t\|\bm{z}\|) \left\langle \bm{\mu}_t, \bm{v}\right\rangle \bigg) 
    + \underbrace{\mathbb{E}_{\bm{z}}\bigg( \sin(\rho_t\|\bm{z}\|) \left\langle \frac{\bm{z}}{\|\bm{z}\|}, \bm{v} \right\rangle\bigg)}_{=0} \label{eq:zero_term} \\
    &\stackrel{\text{Eq.}~\eqref{eq:riemannian_normal}}{\phantom{..}=\phantom{..}} 
    \mathbb{E}_{\bm{z}}\cos(\rho_t\|\bm{z}\|) \left\langle
        \frac{\alpha_t}{\sqrt{1 - \langle \bm{u}, \bm{v}\rangle^2}}\bm{v} + 
        \left(\sqrt{1-\alpha_t^2} - \frac{\alpha_t\langle \bm{u}, \bm{v}\rangle}{\sqrt{1 - \langle \bm{u}, \bm{v}\rangle^2}}\right)\bm{u}
    , \bm{v}\right\rangle \\
    &= \mathbb{E}_{\bm{z}}\cos(\rho_t\|\bm{z}\|) 
    \left( 
        \sqrt{1 - \langle \bm{u}, \bm{v}\rangle^2}\alpha_t + \langle \bm{u}, \bm{v}\rangle \sqrt{1 - \alpha_t^2}
    \right),
\end{align}
where the last term in Eq.~\eqref{eq:zero_term} is zero due to the radial symmetry of $\bm{z}$. 
Similarly, 
\begin{align}
     \mathbb{E}c^{\bm{u}}_t \approx \mathbb{E}_{\bm{z}} \langle\exp_{\bm{\mu}_t}(\rho_t \bm{z}), \bm{u} \rangle 
    &= \mathbb{E}_{\bm{z}}\cos(\rho_t\|\bm{z}\|) \sqrt{1 - \alpha_t^2},
    % \left(
    % \right),
\end{align}
Notably, we have the following identity for $\bm{z}\sim\mathcal{N}_{T_{\bm{\mu}_t}\mathbb{S}^d}(\mathbf{0}, \mathbf{I})$:
\begin{align}
    \mathbb{E}_{\bm{z}}\cos(\rho_t\|\bm{z}\|) = e^{-\rho_t^2/2} {}_{1}f_1(\frac{d}{2},\frac{1}{2},-\frac{\rho_t^2}{2}) \coloneqq F_d(\rho_t),
\end{align}
where ${}_1f_1$ denotes the confluent hypergeometric function. 
Therefore we have:
\begin{align}
    &\mathbb{E}c^{\bm{v}}_t = \frac{\alpha_t}{\sqrt{1 - \langle \bm{u}, \bm{v}\rangle^2}} F_d(\rho_t), \;\;
    \mathbb{E}c^{\bm{u}}_t = \left(\sqrt{1-\alpha_t^2} - \frac{\alpha_t\langle \bm{u}, \bm{v}\rangle}{\sqrt{1 - \langle \bm{u}, \bm{v}\rangle^2}}\right) F_d(\rho_t),
\end{align}
and the parameters $\alpha_t$ and $\rho_t$ can be derived from the mean projections $\mathbb{E}c_t^{\bm{v}}$ and $\mathbb{E}c_t^{\bm{u}}$:
\begin{align}
\alpha_t = \sqrt{\frac{(r_t - \langle \bm{u}, \bm{v}\rangle)^2}{1 - \langle \bm{u}, \bm{v}\rangle^2 + (r_t - \langle \bm{u}, \bm{v}\rangle)^2}}, \;\; r_t = \frac{\mathbb{E}c_t^{\bm{v}}}{\mathbb{E}c_t^{\bm{u}}}, \;\; \rho_t = F_d^{-1}\left(
    \frac{\mathbb{E}c^{\bm{u}}_t}{\sqrt{1 - \alpha_t^2}}\right),
\end{align}



% \paragraph{General Bridge Matching}
% When the initial distribution of the mixture process is the uniform distribution on $\mathbb{S}^{d}_{+}$, the mean and covariance of the Riemannian normal that approximates the transition distribution is dependent on the starting point, or more accurately, the inner product of the starting and the endpoints of the bridge. 
% Therefore, we pre-compute $\alpha_t$ and $\rho_t$ for discretized inner products in $[-1,1]$.



\section{Experimental Details \label{app:exp}}

% \subsection{General Implementation Details}

% \paragraph{Parameterization} \label{app:exp:param}
% We empirically find that using only the coordinates of non-mask tokens of $\bm{X}_t$ as the input of the neural network yields better performance, compared to using the whole coordinates. This is because we do not lose any information using the non-mask coordinates as $\bm{X}_t$ is on the hypersphere, and the mask token coordinate dominates the other coordinates in the early process which obstructs the model from learning the transition density.

% \paragraph{ELBO Computation}
% We use the Monte Carlo estimation of ELBO derived in to compute the upper bound of NLL.


\subsection{Text Generation} \label{app:exp:text}

\paragraph{Baselines}
We compare against state-of-the-art diffusion models. 
Multinomial Diffusion~\citep{hoogeboom2021multinomial}, D3PM~\citep{austin2021d3pm}, SEDD~\citep{lou2024sedd}, MDLM~\citep{sahoo2024simple}, MD4~\citep{shi2024md4} are discrete diffusion models. Plaid~\citep{gulrajani2024plaid} and Bayesian Flow Network (BFN)~\citep{graves2023bayesian} are continuous diffusion models. 
We do not use existing works for flow matching on the statistical manifold~\citep{cheng2024categorical,davis2024fisherflow} as do not provide likelihood computation applicable for language modeling.
% Discrete Flow Matching~\citep{gat2024discrete} does not provide likelihood computation.

We also use the transformer AR model~\citep{vaswani2017transformer} and the following autoregressive models as baselines:
IAF/SCF~\citep{ziegler2019iaf}, AR Argmax Flow~\citep{hoogeboom2021multinomial}, and Discrete Flow~\citep{tran2019discrete} are flow-based models, and ARDM~\citep{hoogeboom2022autoregressive} and MAC~\citep{shih2022ardm} are any-order autoregressive models. 



% \paragraph{Metrics} \label{app:exp:metrics}
% For a sequence of length L, Bits Per Character is measured as follows:
% \begin{align}
%     BPC = -\frac{1}{L} \sum^L_{i=1} \log_2 p(x_i),
% \end{align}
% and the perplexity is computed as follows:
% \begin{align}
%     PPL = -\frac{1}{L} \sum^L_{i=1} \log_2 p(x_i)
% \end{align}


\paragraph{Text8} \label{app:exp:text8}
Text8~\citep{data_text8} is a small character-level text modeling benchmark extracted from English Wikipedia.
Following the previous works~\citep{austin2021d3pm,lou2024sedd,sahoo2024simple}, we split the dataset into 90M/5M/5M with a fixed sequence length of 256.
We use a vocabulary size of 28, comprising 26 lowercase letters, a white space token, and a mask token.
We use a 12-layer diffusion transformer~\citep{peebles2023dit} following \citet{lou2024sedd} with 92.4M trainable parameters. 
We train our model for 1M iterations with batch size 512 as done in previous works, using the same learning rate, optimizer AdamW~\citep{loshchilov17adamw}, and exponential moving average (EMA) with decay rate 0.9999. 


\paragraph{One Billion Words} \label{app:exp:lm1b}
One Billion Word Benchmark is a dataset extracted from the WMT 2011 News Crawl dataset
% ~\footnote{\url{https://www.statmt.org/wmt11/translation-task.html}} 
comprised of single sentences from news articles.
Following \citet{sahoo2024simple}, we use the \texttt{bert-base-uncased} tokenizer and pad and truncate the sequences to length 128.
We use a 12-layer diffusion transformer~\citep{peebles2023dit} with hidden dimension of 768 and 12 attention heads, following \citet{sahoo2024simple} with 110M trainable parameters. 
We train our model for 1M iterations with batch size 512 as done in previous works, using the same constant learning rate, optimizer AdamW~\citep{loshchilov17adamw}, and exponential moving average (EMA) with decay rate 0.9999. 


\subsection{Pixel-level Image Modeling} \label{app:exp:image}

\paragraph{Baselines}
We compare against autoregressive models and diffusion models that directly model raw pixel space. 
PixelRNN~\citep{oord2016pixel}, Gated PixelCNN~\citep{oord2016gated}, PixelCNN++~\citep{salimans2017pixel}, PixelSNAIL~\citep{chen2018pixelsnail}, Image Transformer~\citep{parmar2018image}, and Sparse Transformer~\citep{child2019sparse} are autoregressive models.
D3PM~\citep{austin2021d3pm}, $\tau$LDR~\citep{campbell2022ctmc}, and MD4~\citep{shi2024md4} are discrete diffusion models.


\paragraph{Implementation Details}
We represent each image as a set of discrete tokens with a vocabulary size of 256.
We use the 10-layer diffusion transformer~\citep{peebles2023dit} for our model with 35M trainable parameters. 
We train 100k iterations with batch size 128 and AdamW~\citep{loshchilov17adamw} optimizer following \citet{shi2024md4}.


\subsection{DNA Sequence Design} \label{app:exp:promoter}
The dataset contains 100k promoter DNA sequences each paired with a transcription signal profile. Each sequence consists of 1024 base pairs centered at the annotated transcription start site position~\citep{hon2017atlas}, and the base pair has 4 categories (ATGC) conditioned on the profile.


\paragraph{Baselines}
We compare our model against diffusion models and language models.
Bit Diffusion~\citep{chen2023self} is a continuous diffusion model, 
D3PM~\citep{austin2021d3pm} is a discrete diffusion model, DDSM~\citep{avdeyev2023dirichlet} and Dirichlet Flow Matching~\citep{stark2024dirichlet} are diffusion model and flow matching model using the probability simplex, respectively. 
Fisher-Flow~\citep{davis2024fisherflow} is a flow matching model using statistical manifold.


\paragraph{Implementation Details}
Following the previous work~\citep{stark2024dirichlet,davis2024fisherflow}, we use the same data split of 88,470/3,933/7,497 and identical model architecture consisting of 20-layer 1-D CNN with 13.3M trainable parameters. We train our model for 100k iterations with batch size 256 and AdamW~\citep{loshchilov17adamw} optimizer.
We evaluate the MSE on the generated samples conditioned on the prescription signals from the test set, using 300 generation steps following the previous work~\citep{davis2024fisherflow}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{table/analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/analysis_transition.pdf}
    \caption{
        Maximum mean discrepancy (MMD) distance between the simulated distribution $p(\bm{X}_t|\bm{X}_0,\bm{X}_1)$ and the approximated distribution. We report the results for dimension 4, 256, and 30522.
    }
    \label{fig:analysis_transition}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section{Additional Experimental Results}


% \subsection{Approximated Sampling}



% \subsection{Training Objective}


% \subsection{Dimension Splitting}



% \paragraph{Mixture Path}
% While we do not observe a significant difference in performance between the masked bridge process and the uniform bridge process on a small vocabulary set, the gap widens as the vocabulary set increases.
% This observation agrees with the results reported in existing discrete diffusion models~\citep{austin2021d3pm,lou2024sedd}.
% Notably, using the mixture path of the masked and uniform bridges yields a significant improvement.



\section{Generated Samples}

\subsection{Text8} \label{app:samples:text8}

We provide uncurated text samples generated by our RDLM trained on the Text8 dataset.

\texttt{
    o zero one british single payrock neurologically related condition is a member of the original playboys oriental pbkr cat ii a boob one card featured in the late f one zero dippie dons as it became pigus in the cir the monoseur engine shair which became th
}

\texttt{
    h delivered from the new meeting the construction of modern shooting begins kinington resurrects the hark or corped a hopper nightlife subjecting to turn his attention at a joyable moment he is able to explain that he is in recovery with a new orleans baby
}

\texttt{
    wilder unrefreshed bup of lightmarks was pertified only at the head of sinar joseph avaret in the cetleben key in one nine nine seven this report has been portrayed as a shrinking feathor of the civil directs against urban rumour as that he was ana eichy 
}

\texttt{
    s seven two chromosomes regainally regular and contain number of mignain gnaning pros zopods or cells whose podic configuration divided agong the faces of dna generally replaced by b as therus group are non mit and elanisten special cayits regularly are ca
}

\texttt{
     nine four although portrayals of frel appearance the novel include leaked to bratally targeted audiences largely by steve roper dart mer upick and j pernan s durk born one nine four zero s but stillly not they are created the western master and mag both m
}

\texttt{
    idment indicates two different types drop tales have different charges which train structures having rare and light weight variations have lower weight impedients such as chawings starges and groove gloves shorter holes can be jumpliten don badld a horse i
}

\texttt{
    d deliberately rejected this a different post however saw al sh ibn misha rody was revealed to be the lord curses of jesus one nine one nine he handled his journey to its historical map of the egyptians and was still nodged as he committed to reproete he a
}

\texttt{
    ovincial governors regelrant a cursami governor granted to a spanish cominic in one seven eight three mateo s teltacheutes lebmo alexius jeano and pan dosien dostre of a ruguen de cosst originating specifically the treaty of st louis the extinctions remain
}


\subsection{One Billion Words} \label{app:samples:lm1b}

We provide uncurated text samples generated by our RDLM trained on the LM1B dataset. 


\texttt{
    [CLS] social recklessly the obvious support 2013. [CLS] they were elected off by the english authorities, whose party subsequently named as principal when lawrence tang had to hold the property until they were turned to down their heads in the back - sky of which sank from matthews's doorstep. [CLS] it has been pouring gladly with work and along the motorway, where certified sales will follow a new bone in the next several days to avoid commercial production problems, according to recommendations from both workplace and tropical mod. [CLS] he said he plans watchsty will b greens the old draft plunging sara, but have medics announced she would make you the taxpayer? [CLS] duchess [CLS]
}

\texttt{
    [CLS] of lieberman. [CLS] analysts say since 5, 000 people have held a established council in 120 forums and levels, some have returned to the villages of the british capital, mideast and sprint. [CLS] his friends ring between ironing his body they forbid forrest. [CLS] seven babies missing and 27 french subcontinent and two development employees suffered injuries in a securing of greece, a spokeswoman said immediately, while tneye wedang. [CLS] both questions has already been considered. [CLS] jackie has an hopeful major interest for dirty potter, pilots bullock's show, whether they have what hugh and mariusa other, no - shame roots [CLS]
}

\texttt{
    [CLS] is the problem that worth most of a marriage to have a single car he doesn't need. [CLS] mr obama will carry out more casualties however than president obama's followers, and it mild to form the first cumulative current division ofers holding the guantanamo men that arches to injustice. [CLS] phillips said : " designer kaia kangaroo, 27, and herself rubbed jim reyes, the general patron of france light, have organized a building aimed at gunning film houses. [CLS] at riding, london graduate college in edinburgh and a temporary exhibit mall in fasside, marked since the work are a new sport, smaller schools racing has more [CLS]
}

\texttt{
    [CLS]aceous that in spain had submitted one time the main website on mass wireless, in carpcsllo. [CLS] not two of the beer bk known in the companies could have thousand stretch men - - ginger, and showed vulnerable cases, leaving you in the same £200m standard. [CLS] yet apius is accepted quickly to associate in the months since - - bulletin energy americas - - they agreed that it was getting waste into ulysses air before creation known as the bulletinsburg, which can be bowed with bracelet growth by speed. [CLS] rely will get another less energetic first - turn victory. [CLS] more than 2, 000 people arrived, out [CLS]
}

\texttt{
    [CLS] more steadily increasing transit facilities with murray's tax breaks. [CLS] nonero moee enjoyed terrestrial wallino with the immoitunghrck in most years. [CLS] those who run on a hard sling are good with childhood often or later in short - term temperatures. [CLS] top - seeded henin is shark seventh and isatin out in stanford. [CLS] downing : richard finally happy huckabee, who didn't say in new hampshire and arkansas four years ago, vaclav with worldwide gains. [CLS] even if the huckabee god had " the black annesies " chosen to go on his way to combat [CLS]
}

\texttt{
    [CLS] high school, was potya's poker high - george she - former congressional class - flicked was a prosecutor. [CLS] coln has won the services of the sub - area tustiw university, near fort dodge, pa. [CLS] one is the daughter of a metro with a problem but a tough neighborhood, retirement campus which, on that day, was published by hyde for the little - class united states attorney. [CLS] let's sell a floral parachute in civil court on a lutheran case. [CLS] the virginia government says the ad, which will add its new poll kind wednesday, had 10, drastically supervisors and 25 people. [CLS] [CLS]
}

\texttt{
    [CLS] a memorandum posted to the university : model google, which makes the copies to sell patients seem off a significant stake in every final - ep you programmes similar. [CLS] almost no day cbees will homemadei. [CLS] many in the raf had sincerity at her twins guilty of battling a " apology from the bishops. " [CLS] the courts have replayled their option for'welcome when the fed tends its view of the aec investors'chance. [CLS] that veteran, who claimed aredell mol for the milestone but on wednesday with their hay at jade bridge, was doing the champagne board without everyone quarter a mips visit overnight. [CLS]
}

\texttt{
    [CLS] the bbc's george washington is the first of 15, 000 people to put the calraircer range. [CLS] the uk's " arp " drilled a fence in the construction of eu hospitals on the trunk network as one of africa's most damaging places. [CLS] all looked after world over just um occasionallytau, which takes place victorious for schizophrenia consumed near the doc centre. [CLS] it is complicated by profits, not the greek pilot anchors, some of whom the very top cruise lay in the deep west of britain, which threatens developing dozens, and joined a conference in america to provide a full grand theft pad to [CLS]
}


\section{Future Directions}
While our experiments were conducted with models of small parameter size, scaling up the number of parameters would demonstrate new possibilities, in particular on reasoning or planning abilities.
Moreover, our framework can be extended to controllable text generation utilizing the guidance methods~\citep{dhariwal2021guidance,ho2022classifier} of continuous diffusion models, which we leave as future work.

Another interesting direction is developing a autoregressive-like diffusion language model, which could be stuided by controling the noise schedule.
In this work, we use the same noise schedule for the bridge processes for simplicity. 
Yet, the noise scheduler could be used to control the convergence speed of the tokens in different positions, for example, converging in order from left to right as in autoregressive models.

Lastly, while we focus specifically on language modality, our experiments show that RDLM could be used on different modalities such as image modeling or DNA sequence design. 
Promising directions would be exploring applications to domains where continuous diffusion models have been successful, for example, graph generation~\citep{jo2024graph} or molecule synthesis~\citep{jung2024mol}.


% \section{Limitations} \label{app:limitation}
% While Riemannian Diffusion Language Model has shown promising results on language modeling tasks and other modalities, there still exists a gap with the autoregressive models. 


% \paragraph{Future Directions}
% For simplicity, we use the same noise schedule for the bridge processes. Yet, the noise scheduler could be used to control the convergence speed of the tokens in different positions, for example, converging in order from left to right as in autoregressive models.


% \paragraph{Text Infilling}
% One of the key advantages of diffusion models over autoregressive models is the ability to generate samples following specific conditions, in particular, infilling text given a prefix or a suffix.
% Similar to \citet{gat2024discrete}, we train the model with $\mathbb{I}\odot\bm{u} + (1-\mathbb{I})\odot\bm{e}_k$ instead of $\bm{u}$ where $\bm{u}$ indicates the starting point of the bridge process.


% \paragraph{Controllable Generation}
% Using the Bayes theorem, we derive the following result:
% \begin{align}
%     p\big(\bm{e}_{i_1,\cdots,i_n} \big| \bm{X}^{1:n}_t, y\big) = \frac{p\big(\bm{e}_{i_1,\cdots,i_n}=y\big)}{p(y|\bm{X}^{1:n}_t)} p(\bm{e}_{i_1,\cdots,i_n}|\bm{X}^{1:n}_t)
% \end{align}



% Autoregressive models have problems of
% \begin{itemize}
%     \item consecutive repetitions
%     \item Drift away from desired semantics, maybe due to exposure bias arising from the discrepancy between training and inference in teacher forcing training
%     \begin{itemize}
%         \item Next-token prediction might be myopic
%         \item \edit{in which specific case?}
%     \end{itemize}
%     \item Diffusion model might address this issue by iterative refinement globally in a non-autoregressive manner
% \end{itemize}