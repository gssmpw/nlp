% \section{Continuous Bridge Matching Language Model \label{sec:bridge}}
\section{Riemannian Diffusion Language Model \label{sec:bridge}}

We introduce a novel continuous diffusion model for language modeling. 
In this section, we first present a single token generation framework, which we generalize to modeling sequences of tokens in Section~\ref{sec:sequence}.


\subsection{Generalization of Discrete Diffusion}

\paragraph{Continuous Reparameterization of Discrete Data}
To incorporate the geometry of the underlying categorical distribution, we leverage the statistical manifold to parameterize discrete data~\citep{cheng2024categorical,davis2024fisherflow}.
Each point on the statistical manifold $\mathcal{P}(\mathcal{X})$ corresponds to the parameters of a categorical distribution over the discrete sample space $\mathcal{X}=\{1,\cdots,d\}$.
Thus discrete data can be represented as continuous parameters of categorical distribution on the manifold.


Yet the Fisher-Rao metric is ill-defined on the boundary of the manifold 
where the initial distribution of the parameterized data lies, incurring numerical issues near the boundary.
To address this, we leverage the diffeomorphism $\pi$ (Eq.~\eqref{eq:diffeomorphism}) which maps $\mathcal{P}(\mathcal{X})$ to the positive orthant of a hypersphere $\mathbb{S}^{d-1}_{+}$~\citep{cheng2024categorical,davis2024fisherflow}, where $\bm{u}\in\mathbb{S}^{d-1}_{+}$ corresponds to $\text{Cat}(\cdot;\pi^{\scalebox{0.75}[1.0]{-}1}(\bm{u}))$.
Therefore, discrete data can be reparameterized to continuous states on $\mathbb{S}^{d-1}$ while preserving the geometry of the categorical distribution.
In the case of masked diffusion, discrete sample space is augmented with an additional mask state, and the reparameterization results in a $d$-dimensional sphere.


Our key observation is that the transition distribution $q_t(x_t|x_0)$ of a discrete diffusion process is a categorical distribution on $\mathcal{X}$ (Eq.~\eqref{eq:discrete_transition}).
Therefore, modeling $q_t$ is equivalent to modeling the probability path on the statistical manifold $\mathcal{P}(\mathcal{X})$.
From the following proposition, we show that discrete diffusion models over $\mathcal{X}$ can be modeled by a continuous flow on $\mathcal{P}(\mathcal{X})$ and further on $\mathbb{S}^{d-1}_{+}$  (we defer the proof to Appendix~\ref{app:derivation:generalization}).
\vspace{1ex}
\begin{proposition}
    \it The transition distribution of discrete diffusion processes can be modeled by the probability path on the statistical manifold, and further on the hypersphere. 
\label{prop:discrete_generalize}
\end{proposition}
\vspace{-3ex}
\begin{sproof}
A continuous flow on $\mathbb{S}^{d-1}_{+}$ that interpolates $\bm{v}$ and $\bm{u}$ as geodesic is described by the following ODE:
\begin{align}
    \frac{\mathrm{d}\bm{Y}_t}{\mathrm{d}t} = -\frac{\mathrm{d}\log \kappa_t}{\mathrm{d}t} \logmap{\bm{Y}_t}{\bm{u}}, \;\; 
    \bm{Y}_0 = \bm{v},
\end{align}
where $\exp^{\scalebox{0.75}[1.0]{-}1}_{x}$ denotes the logarithm map. 
Then for well-designed schedule $\kappa_t$ and $\bm{u}$, the process $\bm{Z}_t\coloneqq \pi(\bm{Y}_t)$ on $\mathcal{P}(\mathcal{X})$ corresponds to the transition distribution of the discrete diffusion process.
In particular, we obtain the masked diffusion process for $\bm{u}=\bm{e}_m$, i.e., the masked token, and the uniform diffusion process for $\bm{u}=\sum^{d}_{i=1} \bm{e}_i/\sqrt{d}$.
\end{sproof}
\vspace{-1ex}

Although discrete diffusion processes can be represented as a probability path on the statistical manifold, this flow cannot be learned by a neural network. The network fails to generalize to points outside the geodesic that interpolates the prior and the data distribution, producing an incorrect vector field.
While previous works~\citep{cheng2024categorical,davis2024fisherflow} use the uniform distribution on the simplex as the prior, this does not directly relate to discrete diffusion models.
Therefore, we present a simple design for the continuous diffusion model that generalizes existing discrete diffusion models.



\subsection{Generative Process on Hypersphere}

With the reparameterization, the task of modeling the distribution of discrete data can be reformulated to modeling a distribution on the hypersphere.
The reparameterized data distribution $p^{\ast}$ can be represented as follows:
\begin{align}
    p^{\ast}(x) = \sum^{d}_{k=1} p_k \delta(x - {\bm{e}_k}),
\end{align}
where $p_k$ and $\bm{e}_k$ denotes the probability and the one-hot vector of the $k$-th token, respectively.
To model $p^{\ast}$, we build upon the Riemannian Diffusion Mixture framework~\citep{jo2024riemannian} to construct a generative process on the hypersphere.
Due to the simple nature of $\mathbb{S}^{d-1}$, we can derive the logarithm bridge process~\citep{jo2024riemannian} from an arbitrary point $\bm{u}\in\mathbb{S}^{d-1}$ to the $k$-th token $\bm{e}_k$ as follows (we provide the derivation in Appendix~\ref{app:derivation:log}):
\begin{align}
\begin{split}
     &\mathrm{d}\bm{X}^{k}_t = \gamma_t
     \frac{\bm{\phi}_t(\bm{e}_k - \cos\bm{\phi}_t \bm{X}^{k}_t)}{\sin\bm{\phi}_t} \mathrm{d}t + \sigma_t\mathrm{d}\mathbf{B}^{d}_t, \\
     &\gamma_t \coloneqq  \frac{\sigma_t^2}{\int^T_t\sigma^2_s\mathrm{d}s}, \;\;
     \bm{\phi}_t \coloneqq \invcos\langle\bm{X}^{k}_t, \bm{e}_k \rangle, \;\; \bm{X}^{k}_0=\bm{u},
\end{split}
\label{eq:logarithm_bridge}
\end{align}
where $\mathbf{B}^{d}_t$ denotes the Brownian motion defined on $\mathbb{S}^{d-1}$ and $\phi_t$ denotes the geodesic distance between the current state and the endpoint.

Intuitively, the current state $\bm{X}_t$ moves in the direction that minimizes the geodesic distance to the endpoint, resulting in a process that bridges the starting and end points.
While different forms of the bridge process exist, for example, scaling the drift or the diffusion coefficients, Eq.~\eqref{eq:logarithm_bridge} yields a specific transition distribution that enables simulation-free training, which we explain in Section~\ref{sec:training}.


From the bridge processes, we construct a diffusion process on $\mathbb{S}^{d-1}$ using the diffusion mixture representation (Proposition~\ref{prop:mixture}) with mixing distribution $p^{\ast}$ (see Appendix~\ref{app:derivation:mixture} for the formal definition of the representation):
\begin{align}
    \mathrm{d}\bm{X}_t = \left[ \,
        \sum^d_{k=1} p_{T|t}(\bm{e}_k|\bm{X}_t)\, \eta^{k}(\bm{X}_t,t) 
    \right] \mathrm{d}t + \sigma_t\mathrm{d}\mathbf{B}^{d}_t,
\label{eq:bridge_mixture}
\end{align}
where $\eta^{k}$ denote the drift of the bridge process in Eq.~\eqref{eq:logarithm_bridge}. $p_{T|t}(\bm{e}_k|\bm{X}_t)$ represents the probability that the token $\bm{e}_k$ will be the final outcome of the process, given the current state $\bm{X}_t$ at time $t$.
Note that the construction guarantees the terminal distribution of the process to be $p^{\ast}$.

An ideal generative process is one that gradually refines the uninformative states to recover the original tokens.
We analyze the convergence of the bridge process through its radial process $r^k_t\coloneqq d_g(\bm{X}_t,\bm{e}_k)$ described by the following SDE (see Appendix~\ref{app:derivation:radial} for the derivation using It\^{o}'s formula):
\begin{align}
    \mathrm{d}r^k_t = \left[ -\gamma_t r^k_t + \frac{\sigma^2_td}{2}\cot r^k_t \right]\mathrm{d}t + \sigma_t\mathrm{d}W_t,
\label{eq:}
\end{align}
where $W_t$ is a 1-dimensional Wiener process. 
For $\sigma_0>\sigma_T$, the radial process converges rapidly in early time steps, making it difficult for a neural network to approximate accurately.
We empirically find that the geometric schedule $\sigma_t = \sigma_0^{1-t}\sigma_T^{t}$ with $\sigma_0<\sigma_T$ leads to gradual convergence.

% Note that the bridge mixture does not have a probability ODE, i.e., a deterministic continuous flow with the same marginal distribution. This is because the initial distribution and the target distribution are discrete measures.


\paragraph{Masked Diffusion}
From Proposition~\ref{prop:discrete_generalize}, fixing the initial distribution to be the mask token $\bm{e}_m$ yields a mixture process that generalizes the masked discrete diffusion process.
The resulting process starts from a mask token and moves to one of the tokens following the drift.
In the perspective of discrete diffusion, our process smoothly interpolates the jump from the mask token to the final token via through the continuous states $\bm{X}_t$, with $p_{T|t}(\bm{e}_k|\bm{X}_t)$ determining the direction of the process.


The generalized framework shares similar properties with the masked discrete diffusion~\citep{sahoo2024simple}: (1) \textbf{\textit{Zero Mask Probabilities.}} Our parameterization in Eq.\eqref{eq:prob_parameterization} sets the probability $p_{T|t}(\bm{e}_m|\bm{X}_t)$ to zero, indicating that the final token cannot be a mask token.
(2) \textbf{\textit{Carry-Over Unmasking.}} If $\bm{X}_t$ converges to a token $\bm{e}_k$ before the terminal time, the drift in Eq.~\eqref{eq:logarithm_bridge} also converges to zero and the state $\bm{X}_t$ is carried over without changing to different token.


Yet, the fundamental difference is that discrete diffusion directly jumps from a token to the mask token and vice versa where a wrong jump is non-revokable, making the generation process uneditable.
On the other hand, our continuous approach offers numerous chances to correct wrong directions during the process, leading to a more accurate modeling of the data distribution.


\paragraph{Uniform Diffusion}
From Proposition~\ref{prop:discrete_generalize}, the generalization of the uniform diffusion can be achieved by setting the starting point to be the barycenter of the simplex mapped to $\mathbb{S}^{d-1}$, i.e., $\pi\left( \sum^d_{i=1} \bm{e}_i/d \right) = \sum^d_{i=1} \bm{e}_i / \sqrt{d}$. 
We further extend the uniform diffusion so that the transition to a subset of tokens $\mathcal{S}$ gets a different probability $\zeta$:
\begin{align}
    \pi\left( \sum_{i\in\mathcal{S}} \zeta \bm{e}_i + \sum_{j\notin\mathcal{S}} \frac{1 - \zeta|\mathcal{S}|}{d - |\mathcal{S}|}\bm{e}_j \right), \;\; 0\leq \zeta\leq \frac{1}{|\mathcal{S}|}.
\end{align}
For $\mathcal{S}=\{m\}$ and $\zeta=1$, we obtain the masked diffusion.


\paragraph{Mixture Paths}
Since masked diffusion and uniform diffusion have different initial conditions, they yield different convergence behaviors.
We empirically observe that under the same noise schedule, uniform diffusion is easier to learn in the early time steps compared to masked diffusion, whereas the opposite holds in later stages.
This suggests that a diffusion process mixing masked and uniform processes could result in an improved generative model.


Therefore, we derive a new family of generative processes by mixing the probability path of generative processes $\{\mathbb{Q}^{\ast,i}: 1\leq i\leq n\}$ sharing the same noise schedule $\sigma_t$ (see Appendix~\ref{app:derivation:mixture_path} for detailed derivation of mixture path):
\begin{align}
    \mathbb{Q}^{mix}_t \coloneqq \sum^{n}_{i=1} \lambda^{i}_t \mathbb{Q}^{\ast,i}_t \;\;;\;\;\; \sum^{n}_{i=1} \lambda^{i}_t = 1,
\label{eq:mixture_path}
\end{align}
where $\lambda^{n}_t$ denotes the mixing schedule.
From the perspective of diffusion mixture representation, this corresponds to creating a mixture of generative processes $\{\mathbb{Q}^{\ast,i}_t\}$ with mixing distribution $\lambda^{i}_t$.

One example is to create a mixture path from the masked bridges and uniform bridges:
\begin{align}
\begin{split}
    \lambda_t\mathbb{Q}^{mask}_t + (1-\lambda_t)\mathbb{Q}^{unif}_t,
\end{split}
\label{eq:mixture_path_mask_unif}
\end{align}
with initial distribution $\lambda_0 \delta(\bm{e}_m) + (1-\lambda_0) \delta(\sum^d_{i=1} \bm{e}_i / \sqrt{d})$, which generalizes the mixture paths used in discrete flow matching~\citep{shaul2024flow}.


\paragraph{Generalizing Flow Matching}
Our framework generalizes flow matching methods on the statistical manifold~\citep{cheng2024categorical,davis2024fisherflow}.
By designing the noise schedule in Eq.~\eqref{eq:logarithm_bridge} to be $\sigma_t\coloneqq\sigma_0\rightarrow 0$, we obtain the conditional vector field of the flow matching models.
% starting point $\bm{u}\sim\pi(\mathcal{U}\Delta^{d-1})$, i.e., the uniform distribution on $\mathbb{S}^{d-1}_{+}$.



\section{Simulation-Free Training with Radial Symmetry} \label{sec:training}
Next, we introduce our training scheme.
We derive the likelihood bound for our model and present a simple parameterization and objectives. Further, we present a simulation-free training method based on the radial symmetry of $\mathbb{S}^d$.


\paragraph{Likelihood Bound}
Our approach yields a simple form of evidence-lower bound (ELBO) by using the Girsanov theorem on compact manifolds~(\citet{de2022riemannian}, Corollary H.3).
For a point $\bm{z}\in\mathbb{S}^d$, we can upper bound the negative log-likelihood of our model (Eq.~\eqref{eq:bridge_mixture}) by the KL divergence between the approximated mixture process and the bridge process with endpoint $\bm{z}$:
\begin{align}
    &-\log p_{\theta}(\bm{z}) 
    = D_{KL}(\delta(\bm{z}) \| p_{\theta}(\bm{X}_T)\!=\!\bm{z})
    \leq D_{KL}(\mathbb{Q}^{\bm{z}} \| \mathbb{Q}^{\theta}) \notag \\
    &= \mathbb{E}_{\bm{X}\sim\mathbb{Q}^{\bm{z}}}\! \left[
        \frac{1}{2}\int^T_0 \!\bigg\| 
        \sigma_t^{-1} \!\left(
        \eta^{\theta}(\bm{X}_t,t) - \eta^{\bm{z}}(\bm{X}_t,t)
    \right) \!\bigg\|^2\mathrm{d}t \right] \notag
    % & \quad + D_{KL}(\mathbb{Q}^{\bm{z}}_0 \| \mathbb{Q}^{\theta}_0)
\end{align}
where $\mathbb{Q}^{\bm{z}}$ and $\mathbb{Q}^{\theta}$ denote the probability measure of the bridge and mixture processes, respectively, and $\eta^{\theta}$ denotes the drift of Eq.~\eqref{eq:bridge_mixture}.
The point-wise likelihood bound provides an upper bound on the NLL $\mathbb{E}_{\bm{z}\sim  p^{\ast}}[-\log p_{\theta}(\bm{z})]$:
% The point-wise likelihood bound provides the following upper bound on the negative log-likelihood $\mathbb{E}_{\bm{z}\sim  p^{\ast}}[-\log p_{\theta}(\bm{z})]$:
\begin{align}
\hspace{-1mm}
    \mathbb{E}_{\substack{\bm{e}_k\sim  p^{\ast} \\ \bm{X}\sim\mathbb{Q}^{k}}}\! \left[\frac{1}{2}\int^T_0 \!\bigg\| 
        \sigma_t^{-1} \!\left(
        \eta^{\theta}(\bm{X}_t,t) - \eta^{k}(\bm{X}_t,t)
    \right) \!\bigg\|^2\mathrm{d}t \right], \hspace{-1mm}
\label{eq:elbo}
\end{align}
where $\mathbb{Q}^{k}$ and $\eta^k$ denote the probability measure and the drift of the bridge process with endpoint $\bm{e}_k$, respectively.


\paragraph{Parameterization and Objective}
The drift of the mixture process diverges near the terminal time, which makes it challenging to learn. 
Therefore, instead of approximating the drift function directly, we propose to model the probability $p_{T|t}(\bm{X}_T|\bm{X}_t)$ with a neural network $\bm{s}_{\theta}$ as follows:
\begin{align}
\begin{split}
    p_{\theta}(\bm{X}_t,t) 
    &\coloneqq\texttt{softmax}\left( \bm{s}_{\theta}(\bm{X}_t,t) \right) \\
    &= \Big[ p_{T|t}(\bm{e}_1|\bm{X}_t), \cdots, p_{T|t}(\bm{e}_d|\bm{X}_t), 0 \Big]^{\text{T}},
\end{split}
\label{eq:prob_parameterization}
\end{align}
where we force the probability $p_{T|t}(\bm{e}_m|\bm{X}_t)$ to be zero.
Then the drift of the mixture process can be represented by the parameterization as follows:
\begin{align}
    \eta^{\theta}(\bm{X}_t,t) 
    = \sum^{d}_{k=1} \Big\langle \bm{p}_{\theta}(\bm{X}_t,t), \bm{e}_k\Big\rangle \eta^{k}(\bm{X}_t,t),
\end{align}
Based on the ELBO of Eq.~\eqref{eq:elbo}, we derive a maximum likelihood training objective with the parameterized drift:
\begin{align}
    \mathcal{L}(\theta) &= \mathbb{E}_{\substack{\bm{e}_k\sim  p^{\ast} \\ \bm{X}\sim\mathbb{Q}^{k}}} \left[
    \frac{1}{2} \int^T_0 \sigma_t^{-2} E^k_{\theta}(\bm{X}_t,t) \mathrm{d}t \right] \label{eq:mixture_objective} \\
    E^k_{\theta}(x, t) &= \Bigg\| \sum^d_{l=1} \Big\langle\bm{p}_{\theta}(x,t), \bm{e}_l \Big\rangle \eta^l(x,t) - \eta^k(x,t) \Bigg\|^2, \notag
\end{align}
which can be interpreted as minimizing the mean squared error of the drift approximation.

% where the norm corresponds to the Euclidean norm due to the diffeomorphism in Eq.~\eqref{eq:diffeomorphism}.


A key observation is that ELBO can be minimized by reducing the cross-entropy between the probability vector $p_{\theta}(\bm{X}_t,t)$ and the target one-hot vector $\bm{e}_k$.
Therefore we introduce a cross-entropy-based training objective similar to that used in discrete diffusion models~\citep{sahoo2024simple,shi2024md4}:
\begin{align}
    \mathcal{L}^{CE}(\theta) = \mathbb{E}_{\substack{\bm{e}_k\sim  p^{\ast} \\ \bm{X}\sim\mathbb{Q}^{k}}}
    \bigg[ \int^T_0 \! -\log \big\langle p_{\theta}(\bm{X}_t,t), \bm{e}_k \big\rangle \mathrm{d}t \bigg].
\label{eq:ce_objective}
\end{align}
We experimentally find that the cross-entropy-based loss yields faster convergence in training and leads to better performance than the mean squared error-based loss.


\paragraph{Importance Sampling}
The difficulty of approximating the probability $p_{T|t}(\bm{X}_T|\bm{X}_t)$ varies significantly across different time points $t$.
While predicting $\bm{X}_T$ is fairly easy in the later stage of the process, it is challenging to do so during the middle of the process.
The training objective can be improved by training more on the challenging time points. 
We achieve this by using an importance sampling technique on $t$ which modifies the time distribution to focus on a specific interval, resulting in an equivalent objective:
\begin{align}
    \mathcal{L}_{q}(\theta) = 
    \mathbb{E}_{\substack{\bm{e}_k\sim p^{*} \\ \bm{X}\sim\mathbb{Q}^{k}}}
    \mathbb{E}_{t\sim q} 
    % \Big[ q(t) \sigma_t^{-2} E_{\theta}(\bm{X}_t,t) \Big]
    \Big[ -q(t) \log \big\langle p_{\theta}(\bm{X}_t,t), \bm{e}_k \big\rangle \Big]
\label{eq:importance_mixture_objective} 
\end{align}
where $q$ is the normalized proposal distribution for $t$. We find that a simple density $q(t) = 1-\epsilon \text{ if } t\in[a,b] \text{ else }\epsilon$ to be effective.
%Importance sampling provides faster convergence during training and enhances the model likelihood.



\paragraph{Approximation of Transition Distribution}
The training objective requires sampling $\bm{X}_t$ from the bridge processes at each iteration. 
Since the diffusion process on the $d$-dimensional sphere does not yield a tractable transition distribution, it requires simulating the process which becomes a significant bottleneck during training.
Therefore, we introduce an approximation sampling method that enables simulation-free training, which makes our framework scalable to a large vocabulary.


We approximate the distribution $p(\bm{X}_t|\bm{X}_0,\bm{X}_T)$ as the push-forward measure of a Gaussian distribution on the tangent space by the exponential map, i.e., the Riemannian normal.
This is possible since Eq.~\eqref{eq:logarithm_bridge} is obtained by applying the time change~\citep{oksendal2003sde} to a simple bridge:
\begin{align}
    \mathrm{d}\hat{\bm{X}}_t = \frac{1}{T-t} \frac{\bm{\phi}_t(\bm{e}_k - \cos\bm{\phi}_t \hat{\bm{X}}_t)}{\sin\bm{\phi}_t} \mathrm{d}t + \mathrm{d}\mathbf{B}^{d}_t, 
\end{align}
for $\phi_t\coloneqq \invcos\langle\hat{\bm{X}}_T,\hat{\bm{X}}_t\rangle$, which yields a transition distribution similar to Riemannian normal.


We parameterize the mean $\bm{\mu}_t$ and the covariance $\bm{\Sigma}_t$ of the Riemannian normal approximating $p(\bm{X}_t|\bm{X}_0\!=\!\bm{u},\bm{X}_T\!=\!\bm{v})$ with the parameters $\alpha_t$ and $\rho_t$ as follows:
\begin{align}
   \bm{\mu}_{t}
   &= \frac{\mathbb{E}\bm{X}_{t}}{\|\mathbb{E}\bm{X}_{t}\|} 
   = \frac{\alpha_t}{\sin\phi_0}\bm{v} 
    + \left(
        \sqrt{1-\alpha_t^2} - \frac{\alpha_t\cos\phi_0}{\sin\phi_0}
    \right)\bm{u} \notag \\
    %\exp_{\bm{u}}\left(\tilde{\alpha}_t\logmap{\bm{u}}{\bm{v}}\right) \\
    \bm{\Sigma}_t &= \text{Cov}\left[ \logmap{\bm{\mu}_t}{\bm{X}_t} \right] = \rho_{t}^2 \mathbf{I} , 
\label{eq:riemannian_normal}
\end{align}
for $\phi_0\coloneqq \invcos\langle\bm{u},\bm{v}\rangle$.
Intuitively, $\bm{\mu}_{t}$ represents the normalized centroid of the samples $\bm{X}_t$ and $\rho_{t}^2\mathbf{I}$ corresponds to the covariance of the lifted samples in the tangent space $\mathcal{T}_{\bm{\mu}_{t}}$.



\paragraph{Connection to Projected Processes}
While the parameters $\alpha_t$ and $\rho_t$ are generally intractable, we derive them from the 1-dimensional projections of the diffusion process.
Our main idea is to represent the parameters using the projected processes $c^{\bm{w}}_t \coloneqq \langle\bm{X}_t, \bm{w}\rangle$ for $\bm{w}=\bm{X}_0$ and $\bm{X}_1$.


For a bridge process from $\bm{u}$ to $\bm{v}$, the projected process $c^{\bm{v}}_t = \langle\bm{X}_t, \bm{v}\rangle$ is modeled by a 1-dimensional SDE derived from the It\^{o}'s formula and the radial symmetry of $\mathbb{S}^d$ (see Appendix~\ref{app:derivation:coord} for the derivation):
\begin{align}
\begin{split}
    \mathrm{d}c^{\bm{v}}_t &= b(c^{\bm{v}}_t, t) \mathrm{d}t + \sigma_t\sqrt{1 - (c^{\bm{v}}_t)^2}\, \mathrm{d}W_t, \\
    b(c, t) &= \gamma_t \invcos\!c \, \sqrt{1 - c^2} -\frac{d\sigma_t^2}{2}c
\end{split}
\label{eq:1d_process_end}
\end{align}
where $W_t$ is a 1-dimensional standard Wiener process. 
Similarly, $c^{\bm{u}}_t = \langle\bm{X}_t, \bm{u}\rangle$ is described by a SDE that depends on $c^{\bm{v}}_t$ (see Appendix~\ref{app:derivation:coord} for the derivation):
\begin{align}
    &\mathrm{d}c^{\bm{u}}_t = \tilde{b}(c^{\bm{u}}_t, c^{\bm{v}}_t, t)  \mathrm{d}t + \sigma_t\sqrt{1 - (c^{\bm{u}}_t)^2}\, \mathrm{d}W_t, \label{eq:1d_process_start} \\
    & \tilde{b}(c^{u}, c^{v}, t) = \gamma_t \frac{\invcos\!c^{v}}{\sqrt{1 - (c^{v})^2}} \Big( \langle\bm{u}, \bm{v}\rangle - c^{u} c^{v} \Big) -\frac{d\sigma_t^2}{2}c^{\bm{u}} \notag
\end{align}


From the initial conditions $c^{\bm{v}}_0 = \langle\bm{u},\bm{v}\rangle$ and $c^{\bm{u}}_0 = 1$, we obtain the connection between the projections and the parameters of Riemannian normal (see Appendix~\ref{app:derivation:coord_alpha}):
\begin{align}
    &\mathbb{E}c^{\bm{v}}_t 
    \!=\! \left( 
        \sqrt{1 - \langle \bm{u}, \bm{v}\rangle^2}\alpha_t + \langle \bm{u}, \bm{v}\rangle \sqrt{1 - \alpha_t^2}
    \right) F_d(\rho_t), \notag \\
    &\mathbb{E}c^{\bm{u}}_t 
    \!=\! \sqrt{1-\alpha_t^2} F_d(\rho_t), \;\; 
    F_d(\rho)\coloneqq e^{\scalebox{0.85}[1.0]{-} \frac{\rho^2}{2}} {}_{1}f_1\left(\frac{d}{2},\frac{1}{2}, -\frac{\rho^2}{2}\right) \notag
\end{align}
where ${}_1f_1$ denotes the confluent hypergeometric function. 
Therefore, the parameters of the Riemannian normal can be derived from the mean projections $\mathbb{E}c^{\bm{u}}_t$ and $\mathbb{E}c^{\bm{v}}_t$ as follows:
\begin{align}
\begin{split}
    \alpha_t &= \sqrt{\frac{(r_t - \langle \bm{u}, \bm{v}\rangle)^2}{1 - \langle \bm{u}, \bm{v}\rangle^2 + (r_t - \langle \bm{u}, \bm{v}\rangle)^2}}, \;\; r_t = \frac{\mathbb{E}c_t^{\bm{v}}}{\mathbb{E}c_t^{\bm{u}}} \\
    \rho_t &= F_d^{-1}\left(
    \mathbb{E}c^{\bm{u}}_t / \sqrt{1 - \alpha_t^2} \right),
\end{split}
\label{eq:from_coord_process}
\end{align}
where $F_d^{-1}$ denotes the inverse function of $F_d$. 
For small $d$, we calibrate $\rho_t$ by scaling up with a constant. 


While the mean projections $\mathbb{E}c^{\bm{u}}_t$ and $\mathbb{E}c^{\bm{v}}_t$ generally do not have closed-form solutions, they can be easily obtained from simulating the 1-dimensional processes Eq.~\eqref{eq:1d_process_end} and Eq.~\eqref{eq:1d_process_start}. 
In particular, for masked and uniform diffusion, $\bm{u}$ is fixed to a single point for which $\langle \bm{u},\bm{e}_k\rangle$ is the same for all the non-masked tokens., Due to the radial symmetry, $\mathbb{E}c^{\bm{e}_k}_t$ is identical for all $k$ and the bridge processes $\mathbb{Q}^{k}$ share the same $\alpha_t$ and $\rho_t$. 


Therefore, before training our model, we pre-compute $\alpha_{t_i}$ and $\rho_{t_i}$ only once for $t_i\coloneqq i/N$ with sufficiently large $N$, by simulating the 2-dimensional process $(\bm{c}^{\bm{u}}_t,\bm{c}^{\bm{e}_1}_t)$.
Then with the pre-computed parameters, we can easily sample $\bm{X}_t$ from the Riemannian normal during training without expensive simulation of the bridge processes, achieving $\times$50 faster speed up compared to the simulation-based training.
We experimentally demonstrate that our approach provides an accurate approximation of the distribution $\bm{X}_t$ in Section~\ref{exp:transition}.



\section{Generation of Token Sequences \label{sec:sequence}}

\paragraph{Sequence of Tokens}
Now we generalize the result of single token modeling to the generation of token sequences.
Since each token in the sequence is reparameterized to $d$-dimensional spheres, a sequence of length $n$ is modeled on a product manifold $(\mathbb{S}^d)^{n}\coloneqq \mathbb{S}^d\times\cdots\times\mathbb{S}^d$.
The diffusion processes on each hypersphere are dependent on each other, described by the following system of SDEs: 
\begin{align}
    \mathrm{d}\bm{X}^{i}_t =\! \sum^{d}_{k=1} p(\bm{X}^{i}_T \!=\! \bm{e}_k | \bm{X}^{1:n}_t) \eta^{k}(\bm{X}^{i}_t, t) + \sigma_t\mathrm{d}\mathbf{B}^{d}_t,
\end{align}
for $1\leq i\leq n$, where $\eta^k$ denotes the drift of the bridge on $\mathbb{S}^d$ with endpoint $\bm{e}_k$. 
Note that $p(\bm{X}^{i}_T=\bm{e}_k|\bm{X}^{1:n}_t)$ denotes the probability of the $i$-th token being the $k$-th state which relies on the current intermediate sequence $\bm{X}^{1:n}_t$, and we train a neural network to predict the probabilities.


% While the maximum length of the token sequence should be determined beforehand, 
Our framework allows generating sequences of arbitrary lengths smaller than the maximum length. Using the tokens [BOS] and [EOS] that denote the start and the end of the sequence, we can generate a sequence of the desired length by fixing the position of these tokens.


\paragraph{Dimension Splitting of Statistical Manifold} \label{method:splitting}
For a large vocabulary set, the corresponding statistical manifold has a high dimension which results in two challenges: 
(1) \textbf{\textit{Abrupt convergence.}} Bridge processes on a high-dimensional sphere converge abruptly near the end of the process, which makes them hard to learn with a neural network.
% the Laplacian term in its radial process (Eq.~\eqref{eq:radial_process}) which is linear to the dimension of the manifold. 
(2) \textbf{\textit{Large input dimension.}} Since the input of the network is of high dimension, the hidden dimensions of the network should be sufficiently large to encode them properly. Models with small capacity fail to learn the probabilities of Eq.\eqref{eq:prob_parameterization}.


To address these challenges, we introduce \emph{dimension splitting}, a simple technique to reduce the dimension of the parameterized manifold. 
Instead of directly mapping the $k$-th token to $\mathbb{S}^{d}$, we represent the index $k$ in base $b$ which is then mapped to the product manifold $(\mathbb{S}^{\tilde{b}})^{m}$ where $\tilde{b}=b$ for masked diffusion and otherwise $b-1$, and $m\coloneqq \lceil\log_{b}d\rceil$.
Dimension splitting reparameterizes a sequence of length $L$ to a product manifold $(\mathbb{S}^{\tilde{b}})^{mL}$, and the bridge processes defined on $\mathbb{S}^{\tilde{b}}$ with small $\tilde{b}$ yield gradual convergence that can be easily learned by a neural network.
Dimension splitting significantly enhances the likelihood of our model when used together with the mixture path (Eq.~\eqref{eq:mixture_path_mask_unif}).
