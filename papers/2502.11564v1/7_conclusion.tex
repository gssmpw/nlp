\section{Conclusion}
In this work, we introduced Riemannian Diffusion Language Model (RDLM), a continuous diffusion model for language and discrete data.
% By integrating the geometry of the statistical manifold into the diffusion process, RDLM effectively models the discrete data as categorical distributions on the manifold.
We present a simple framework that generalizes discrete diffusion models building on the connection between the transition distribution of the diffusion process and the probability path on the statistical manifold.
We provide general designs of the diffusion processes and introduce a simulation-free training scheme leveraging the radial symmetry of the hypersphere. 
We validate through experiments on language benchmarks that RDLM outperforms previous discrete and continuous diffusion models 
for language modeling. 
Further, we explore applications to other modalities including images and biological sequences achieving state-of-the-art results.
% We discuss the limitations of our work in Appendix~\ref{app:limitation}.

\paragraph{Impact Statement}
This paper presents work whose goal is to advance the field of deep generative models for language modeling and discrete data. We believe our work can enhance our understanding of various scientific fields dealing with discrete data.