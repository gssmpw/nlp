\section{Introduction}
Discrete diffusion models~\citep{austin2021d3pm,lou2024sedd} emerged as a promising competitor to autoregressive models for the generative modeling of discrete data. These models have demonstrated competitive performance on tasks such as language modeling~\citep{shi2024md4,sahoo2024simple} and code generation~\citep{gat2024discrete}.
Unlike autoregressive models that generate data sequentially, diffusion models generate the sequence in parallel, allowing for bidirectional controllable generation and faster sampling.


However, discrete diffusion models do not fully leverage the power of iterative refinement which is the key to generative modeling of continuous data, for example, image synthesis~\citep{saharia2022image,esser2024image} and video generation~\citep{polyak2024moviegen,brooks2024video}.
In discrete diffusion models, the progressive corruption during the forward process is modeled by stochastic jumps between states in Markov chains.
Since denoising is achieved by jumping between states, discrete diffusion loses valuable signals during refinement which limits the generative performance and controllability.

% successfully capturing the discrete structure


Several efforts were made to adapt continuous diffusion models for discrete data, but their performance falls short of discrete diffusion models, demonstrating a significant gap compared to autoregressive models. 
Past works applied diffusion models for images to discrete data through continuous relaxation without constraint~\citep{han2022ssd,li2022diffusion}. 
Other lines of works~\citep{avdeyev2023dirichlet,stark2024dirichlet} map discrete data to the probability simplex which exerts a strong prior assumption on Dirichlet distribution, but often fails to model complex patterns.
This led to recent works~\citep{cheng2024categorical,davis2024fisherflow} that apply flow matching to learn the categorical distributions using the structure of the statistical manifold, but these methods are limited to small sequences and categories.
In particular, the link between discrete and continuous diffusion remains unclear, hindering the development of a coherent diffusion framework for discrete data.


In this work, we present Riemannian Diffusion Language Model (RDLM), a continuous diffusion framework for language modeling that incorporates the geometry of the statistical manifold in the diffusion processes.
We establish a connection between continuous flow on the statistical manifold and the discrete diffusion process, showing that the trajectory of the transition distribution can be modeled to a conditional flow on the manifold.
Based on the analogy, we introduce a simple design of the diffusion processes on the manifold that generalizes previous discrete diffusion models.
We further present a simulation-free training scheme using radial symmetry that integrates simple parameterization and maximum likelihood-based training objectives. 
Through experiments on language modeling tasks, image modeling, and biological sequence design, we validate that our framework outperforms existing discrete diffusion models.
