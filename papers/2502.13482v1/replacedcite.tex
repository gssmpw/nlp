\section{Related Work}
\label{sec:related_work}

\paragraph{Clipping and normalization.} 
Clipping and normalization
address many key challenges in machine learning. 
They mitigate the problem of exploding gradients in recurrent neural networks ____, enhance neural network training for tasks in natural language processing ____ and computer vision ____, enable differentially private machine learning ____, and provide robustness in the presence of misbehaving or adversarial workers ____. 
In this paper, we consider smoothed normalization, introduced by ____, as an alternative to clipping, given its robust empirical performance and hyperparameter tuning benefits in the DP setting.

\paragraph{Private optimization methods.} 
\algname{DP-SGD}____ is the standard distributed first-order method that achieves the DP guarantee by clipping  the gradient
before adding noise scaled with the clipped gradient's sensitivity. 
However, existing \algname{DP-SGD} convergence analyses often neglect the clipping bias. 
Specifically, convergence results for smooth functions under differential privacy often require either the assumption of bounded gradient norms____ or conditions where clipping is effectively inactive____. 
Thus, the convergence behavior of \algname{DP-SGD} in the presence of clipping bias remains poorly understood.


\paragraph{Single-node non-private methods with clipping. } 
The impact of clipping bias has been extensively studied in single-node gradient methods for non-private optimization. 
Numerous works have shown strong convergence guarantees of clipped gradient methods under various conditions, including nonsmooth, rapidly growing convex functions____, generalized smoothness____, and heavy-tailed noise____.


\paragraph{Distributed non-private methods with clipping.}
Applying gradient clipping in the distributed setting is challenging.
Existing convergence analyses often rely on bounded heterogeneity assumptions, which often do not hold in cases of arbitrary data heterogeneity.
For example, federated optimization methods with clipping have been analyzed under the bounded difference between the local and global gradients____.
However, even in the non-private setting, these distributed clipping methods do not converge for simple problems____. 
To address the convergence issue, one approach is to use error feedback mechanisms, such as \algname{EF21} ____, that are employed by ____ to compute local gradient estimators and alleviate clipping bias. However, these distributed clipping methods using error feedback are limited to the non-private setting, and extending them to the DP setting is still an open problem.
In this paper, we propose a distributed method that replaces clipping with smoothed normalization in the \algname{EF21} mechanism. 
Our method provides the first provable convergence in the DP setting and empirically outperforms the distributed version of \algname{DP-SGD} with smoothed normalization ____, a special case of ____.


\paragraph{Error feedback.} 
Error feedback, or error compensation, has been applied to improve the convergence of distributed methods with gradient compression for communication-efficient learning.
First introduced by ____, \algname{EF14} was extensively analyzed for first-order methods  in both single-node ____ and distributed settings____. 
Another error feedback variant is \algname{EF21}  proposed by ____ that ensures strong convergence under any contractive compression operator for non-convex, smooth problems.  
Recent variants, e.g. \algname{EF21-SGD2M}____  and \algname{EControl}____,  have been developed to obtain the lower iteration and communication complexities than \algname{EF21} for stochastic optimization.