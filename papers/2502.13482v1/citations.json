[
  {
    "index": 0,
    "papers": [
      {
        "key": "pascanu2013difficulty",
        "author": "Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua",
        "title": "On the difficulty of training recurrent neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "merity2017regularizing",
        "author": "Stephen Merity and Nitish Shirish Keskar and Richard Socher",
        "title": "Regularizing and Optimizing {LSTM} Language Models"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "brock2021high",
        "author": "Brock, Andy and De, Soham and Smith, Samuel L and Simonyan, Karen",
        "title": "High-performance large-scale image recognition without normalization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "abadi2016deep",
        "author": "Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li",
        "title": "Deep learning with differential privacy"
      },
      {
        "key": "mcmahan2018learning",
        "author": "McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li",
        "title": "Learning Differentially Private Recurrent Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "karimireddy2021learning",
        "author": "Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin",
        "title": "Learning from history for byzantine robust optimization"
      },
      {
        "key": "ozfatura2023byzantines",
        "author": "{\\\"O}zfatura, Kerem and {\\\"O}zfatura, Emre and K{\\\"u}p{\\c{c}}{\\\"u}, Alptekin and Gunduz, Deniz",
        "title": "Byzantines can also learn from history: Fall of centered clipping in federated learning"
      },
      {
        "key": "malinovsky2023byzantine",
        "author": "Malinovsky, Grigory and Gorbunov, Eduard and Horv{\\'a}th, Samuel and Richt{\\'a}rik, Peter",
        "title": "Byzantine robustness and partial participation can be achieved simultaneously: Just clip gradient differences"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yang2022normalized",
        "author": "Yang, Xiaodong and Zhang, Huishuai and Chen, Wei and Liu, Tie-Yan",
        "title": "Normalized/clipped {SGD} with perturbation for differentially private non-convex optimization"
      },
      {
        "key": "bu2024automatic",
        "author": "Bu, Zhiqi and Wang, Yu-Xiang and Zha, Sheng and Karypis, George",
        "title": "Automatic clipping: Differentially private deep learning made easier and stronger"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "abadi2016deep",
        "author": "Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li",
        "title": "Deep learning with differential privacy"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2020private",
        "author": "Zhang, Xin and Fang, Minghong and Liu, Jia and Zhu, Zhengyuan",
        "title": "Private and communication-efficient edge learning: a sparse differential gaussian-masking distributed {SGD} approach"
      },
      {
        "key": "li2022soteriafl",
        "author": "Li, Zhize and Zhao, Haoyu and Li, Boyue and Chi, Yuejie",
        "title": "{SoteriaFL}: A unified framework for private federated learning with communication compression"
      },
      {
        "key": "zhang2022understanding",
        "author": "Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng",
        "title": "Understanding clipping for federated learning: Convergence and client-level differential privacy"
      },
      {
        "key": "pmlr-v216-wang23b",
        "author": "Wang, Lingxiao and Jayaraman, Bargav and Evans, David and Gu, Quanquan",
        "title": "Efficient Privacy-Preserving Stochastic Nonconvex Optimization"
      },
      {
        "key": "lowy2023private",
        "author": "Lowy, Andrew and Ghafelebashi, Ali and Razaviyayn, Meisam",
        "title": "Private non-convex federated learning without a trusted server"
      },
      {
        "key": "murata2023diff2",
        "author": "Murata, Tomoya and Suzuki, Taiji",
        "title": "DIFF2: Differential private optimization via gradient differences for nonconvex distributed learning"
      },
      {
        "key": "wang2024efficient",
        "author": "Lingxiao Wang and Xingyu Zhou and Kumar Kshitij Patel and Lawrence Tang and Aadirupa Saha",
        "title": "Efficient Private Federated Non-Convex Optimization With Shuffled Model"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2024private",
        "author": "Zhang, Meifan and Xie, Zhanhong and Yin, Lihua",
        "title": "Private and Communication-Efficient Federated Learning based on Differentially Private Sketches"
      },
      {
        "key": "noble2022differentially",
        "author": "Noble, Maxence and Bellet, Aur{\\'e}lien and Dieuleveut, Aymeric",
        "title": "Differentially private federated learning on heterogeneous data"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shor2012minimization",
        "author": "Shor, Naum Zuselevich",
        "title": "Minimization methods for non-differentiable functions"
      },
      {
        "key": "ermoliev1988stochastic",
        "author": "Ermoliev, Yuri",
        "title": "Stochastic quasigradient methods. numerical techniques for stochastic optimization"
      },
      {
        "key": "alber1998projected",
        "author": "Alber, Ya I and Iusem, Alfredo N and Solodov, Mikhail V",
        "title": "On the projected subgradient method for nonsmooth convex optimization in a Hilbert space"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2019gradient",
        "author": "Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie",
        "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity"
      },
      {
        "key": "koloskova2023revisiting",
        "author": "Koloskova, Anastasia and Hendrikx, Hadrien and Stich, Sebastian U",
        "title": "Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees"
      },
      {
        "key": "gorbunov2024methods",
        "author": "Eduard Gorbunov and Nazarii Tupitsa and Sayantan Choudhury and Alen Aliev and Peter Richt{\\'a}rik and Samuel Horv{\\'a}th and Martin Tak{\\'a}{\\v{c}}",
        "title": "Methods for Convex {$(L_0,L_1)$}-Smooth Optimization: Clipping, Acceleration, and Adaptivity"
      },
      {
        "key": "vankov2024optimizing",
        "author": "Daniil Vankov and Anton Rodomanov and Angelia Nedich and Lalitha Sankar and Sebastian U Stich",
        "title": "Optimizing $(L_0, L_1)$-Smooth Functions by Gradient Methods"
      },
      {
        "key": "lobanov2024linear",
        "author": "Lobanov, Aleksandr and Gasnikov, Alexander and Gorbunov, Eduard and Tak{\\'a}c, Martin",
        "title": "Linear Convergence Rate in Convex Setup is Possible! Gradient Descent Method Variants under {$(L_0, L_1)$}-Smoothness"
      },
      {
        "key": "hubler2024parameter",
        "author": "H{\\\"u}bler, Florian and Yang, Junchi and Li, Xiang and He, Niao",
        "title": "Parameter-agnostic optimization under relaxed smoothness"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "gorbunov2020stochastic",
        "author": "Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander",
        "title": "Stochastic optimization with heavy-tailed noise via accelerated gradient clipping"
      },
      {
        "key": "nguyen2023improved",
        "author": "Nguyen, Ta Duy and Nguyen, Thien H and Ene, Alina and Nguyen, Huy",
        "title": "Improved convergence in high probability of clipped gradient methods with heavy tailed noise"
      },
      {
        "key": "gorbunov2024highprobability",
        "author": "Eduard Gorbunov and Abdurakhmon Sadiev and Marina Danilova and Samuel Horv{\\'a}th and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richt{\\'a}rik",
        "title": "High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise"
      },
      {
        "key": "hubler2024gradient",
        "author": "Florian H{\\\"u}bler and Ilyas Fatkhullin and Niao He",
        "title": "From Gradient Clipping to Normalization for Heavy Tailed {SGD}"
      },
      {
        "key": "chezhegov2024gradient",
        "author": "Chezhegov, Savelii and Klyukin, Yaroslav and Semenov, Andrei and Beznosikov, Aleksandr and Gasnikov, Alexander and Horv{\\'a}th, Samuel and Tak{\\'a}{\\v{c}}, Martin and Gorbunov, Eduard",
        "title": "Gradient Clipping Improves {AdaGrad} when the Noise Is Heavy-Tailed"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wei2020federated",
        "author": "Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent",
        "title": "Federated learning with differential privacy: Algorithms and performance analysis"
      },
      {
        "key": "liu2022communication",
        "author": "Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang",
        "title": "A communication-efficient distributed gradient clipping algorithm for training deep neural networks"
      },
      {
        "key": "crawshaw2023episode",
        "author": "Michael Crawshaw and Yajie Bao and Mingrui Liu",
        "title": "{EPISODE}: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data"
      },
      {
        "key": "li2024an",
        "author": "Bo Li and Xiaowen Jiang and Mikkel N. Schmidt and Tommy Sonne Alstr{\\o}m and Sebastian U Stich",
        "title": "An improved analysis of per-sample and per-update clipping in federated learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "chen2020understanding",
        "author": "Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi",
        "title": "Understanding gradient clipping in private sgd: A geometric perspective"
      },
      {
        "key": "khirirat2023clip21",
        "author": "Khirirat, Sarit and Gorbunov, Eduard and Horv{\\'a}th, Samuel and Islamov, Rustem and Karray, Fakhri and Richt{\\'a}rik, Peter",
        "title": "Clip21: Error feedback for gradient clipping"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "richtarik2021ef21",
        "author": "Richt{\\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas",
        "title": "{EF21:} A new, simpler, theoretically better, and practically faster error feedback"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "khirirat2023clip21",
        "author": "Khirirat, Sarit and Gorbunov, Eduard and Horv{\\'a}th, Samuel and Islamov, Rustem and Karray, Fakhri and Richt{\\'a}rik, Peter",
        "title": "Clip21: Error feedback for gradient clipping"
      },
      {
        "key": "yu2023smoothed",
        "author": "Yu, Shuhua and Jakovetic, Dusan and Kar, Soummya",
        "title": "Smoothed Gradient Clipping and Error Feedback for Distributed Optimization under Heavy-Tailed Noise"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "bu2024automatic",
        "author": "Bu, Zhiqi and Wang, Yu-Xiang and Zha, Sheng and Karypis, George",
        "title": "Automatic clipping: Differentially private deep learning made easier and stronger"
      },
      {
        "key": "yang2022normalized",
        "author": "Yang, Xiaodong and Zhang, Huishuai and Chen, Wei and Liu, Tie-Yan",
        "title": "Normalized/clipped {SGD} with perturbation for differentially private non-convex optimization"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "das2021convergence",
        "author": "Rudrajit Das and Abolfazl Hashemi and sujay sanghavi and Inderjit S Dhillon",
        "title": "Differentially Private Federated Learning with Normalized Updates"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "seide20141",
        "author": "Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong",
        "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs."
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "stich2018sparsified",
        "author": "Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin",
        "title": "Sparsified {SGD} with memory"
      },
      {
        "key": "karimireddy2019error",
        "author": "Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin",
        "title": "Error feedback fixes sign{SGD} and other gradient compression schemes"
      },
      {
        "key": "stich2019error",
        "author": "Stich, Sebastian U and Karimireddy, Sai Praneeth",
        "title": "The Error-Feedback framework: {SGD} with Delayed Gradients"
      },
      {
        "key": "khirirat2019convergence",
        "author": "Khirirat, Sarit and Magn{\\'u}sson, Sindri and Johansson, Mikael",
        "title": "Convergence bounds for compressed gradient methods with memory based error compensation"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "wu2018error",
        "author": "Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong",
        "title": "Error compensated quantized {SGD} and its applications to large-scale distributed optimization"
      },
      {
        "key": "alistarh2018convergence",
        "author": "Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, C{\\'e}dric",
        "title": "The convergence of sparsified gradient methods"
      },
      {
        "key": "gorbunov2020linearly",
        "author": "Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\\'a}rik, Peter",
        "title": "Linearly converging error compensated {SGD}"
      },
      {
        "key": "qian2021error",
        "author": "Qian, Xun and Richt{\\'a}rik, Peter and Zhang, Tong",
        "title": "Error compensated distributed {SGD} can be accelerated"
      },
      {
        "key": "tang2019doublesqueeze",
        "author": "Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji",
        "title": "Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression"
      },
      {
        "key": "danilova2022distributed",
        "author": "Danilova, Marina and Gorbunov, Eduard",
        "title": "Distributed methods with absolute compression and error compensation"
      },
      {
        "key": "qian2023catalyst",
        "author": "Qian, Xun and Dong, Hanze and Zhang, Tong and Richtarik, Peter",
        "title": "Catalyst acceleration of error compensated methods leads to better communication complexity"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "richtarik2021ef21",
        "author": "Richt{\\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas",
        "title": "{EF21:} A new, simpler, theoretically better, and practically faster error feedback"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "fatkhullin2024momentum",
        "author": "Fatkhullin, Ilyas and Tyurin, Alexander and Richt{\\'a}rik, Peter",
        "title": "Momentum provably improves error feedback!"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "gao2023econtrol",
        "author": "Yuan Gao and Rustem Islamov and Sebastian U Stich",
        "title": "{EC}ontrol: Fast Distributed Optimization with Compression and Error Control"
      }
    ]
  }
]