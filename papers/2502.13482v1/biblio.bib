@article{richtarik2021ef21,
  title={{EF21:} A new, simpler, theoretically better, and practically faster error feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4384--4396},
  year={2021}
}




@article{bu2024automatic,
  title={Automatic clipping: Differentially private deep learning made easier and stronger},
  author={Bu, Zhiqi and Wang, Yu-Xiang and Zha, Sheng and Karypis, George},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{yang2022normalized,
  title={Normalized/clipped {SGD} with perturbation for differentially private non-convex optimization},
  author={Yang, Xiaodong and Zhang, Huishuai and Chen, Wei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2206.13033},
  year={2022}
}


@article{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{levy2016power,
  title={The power of normalization: Faster evasion of saddle points},
  author={Levy, Kfir Y},
  journal={arXiv preprint arXiv:1611.04831},
  year={2016}
}

@article{nesterov1984minimization,
  title={Minimization methods for nonsmooth convex and quasiconvex functions},
  author={Nesterov, Yurii E},
  journal={Matekon},
  volume={29},
  number={3},
  pages={519--531},
  year={1984}
}

@article{khirirat2023clip21,
  title={Clip21: Error feedback for gradient clipping},
  author={Khirirat, Sarit and Gorbunov, Eduard and Horv{\'a}th, Samuel and Islamov, Rustem and Karray, Fakhri and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2305.18929},
  year={2023}
}


@article{zhang2023differentially,
  title={Differentially private {SGD} without clipping bias: An error-feedback approach},
  author={Zhang, Xinwei and Bu, Zhiqi and Wu, Zhiwei Steven and Hong, Mingyi},
  journal={arXiv preprint arXiv:2311.14632},
  year={2023}
}

@article{khirirat2024error,
  title={Error Feedback under $(L\_0, L\_1) $-Smoothness: Normalization and Momentum},
  author={Khirirat, Sarit and Sadiev, Abdurakhmon and Riabinin, Artem and Gorbunov, Eduard and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2410.16871},
  year={2024}
}



@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  volume={2014},
  pages={1058--1062},
  year={2014},
  organization={Singapore}
}




@article{richtarik2024error,
  title={Error feedback reloaded: From quadratic to arithmetic mean of smoothness constants},
  author={Richt{\'a}rik, Peter and Gasanov, Elnur and Burlachenko, Konstantin},
  journal={arXiv preprint arXiv:2402.10774},
  year={2024}
}





@article{richtarik2023error,
  title={Error Feedback Shines when Features are Rare},
  author={Richtarik, Peter and Gasanov, Elnur and Burlachenko, Konstantin},
  journal={arXiv preprint arXiv:2305.15264},
  year={2023}
}


@article{fatkhullin2024momentum,
  title={Momentum provably improves error feedback!},
  author={Fatkhullin, Ilyas and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{
gao2023econtrol,
title={{EC}ontrol: Fast Distributed Optimization with Compression and Error Control},
author={Yuan Gao and Rustem Islamov and Sebastian U Stich},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}



@InProceedings{pascanu2013difficulty,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1310--1318},
  year = 	 {2013},
  publisher =    {PMLR}
}



@inproceedings{
merity2017regularizing,
title={Regularizing and Optimizing {LSTM} Language Models},
author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@inproceedings{brock2021high,
  title={High-performance large-scale image recognition without normalization},
  author={Brock, Andy and De, Soham and Smith, Samuel L and Simonyan, Karen},
  booktitle={International conference on machine learning},
  pages={1059--1071},
  year={2021},
  organization={PMLR}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{chen2016revisiting,
  title={Revisiting distributed synchronous {SGD}},
  author={Chen, Jianmin and Pan, Xinghao and Monga, Rajat and Bengio, Samy and Jozefowicz, Rafal},
  journal={arXiv preprint arXiv:1604.00981},
  year={2016}
}

@article{ivkin2019communication,
  title={Communication-efficient distributed {SGD} with sketching},
  author={Ivkin, Nikita and Rothchild, Daniel and Ullah, Enayat and Stoica, Ion and Arora, Raman and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}

@inproceedings{koloskova2023revisiting,
  title={Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees},
  author={Koloskova, Anastasia and Hendrikx, Hadrien and Stich, Sebastian U},
  booktitle={International Conference on Machine Learning},
  pages={17343--17363},
  year={2023},
  organization={PMLR}
}

@inproceedings{mcmahan2018learning,
  title={Learning Differentially Private Recurrent Language Models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  booktitle={International Conference on Learning Representations},
  year={2018}
}



@inproceedings{
zhang2019gradient,
title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
booktitle={International Conference on Learning Representations},
year={2020}
}


@inproceedings{
hubler2024gradient,
title={From Gradient Clipping to Normalization for Heavy Tailed {SGD}},
author={Florian H{\"u}bler and Ilyas Fatkhullin and Niao He},
booktitle={The 28th International Conference on Artificial Intelligence and Statistics},
year={2025}
}

@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15042--15053},
  year={2020}
}



@inproceedings{
gorbunov2024methods,
title={Methods for Convex {$(L_0,L_1)$}-Smooth Optimization: Clipping, Acceleration, and Adaptivity},
author={Eduard Gorbunov and Nazarii Tupitsa and Sayantan Choudhury and Alen Aliev and Peter Richt{\'a}rik and Samuel Horv{\'a}th and Martin Tak{\'a}{\v{c}}},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025}
}



@inproceedings{
vankov2024optimizing,
title={Optimizing $(L_0, L_1)$-Smooth Functions by Gradient Methods},
author={Daniil Vankov and Anton Rodomanov and Angelia Nedich and Lalitha Sankar and Sebastian U Stich},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025}
}


@article{lobanov2024linear,
  title={Linear Convergence Rate in Convex Setup is Possible! Gradient Descent Method Variants under {$(L_0, L_1)$}-Smoothness},
  author={Lobanov, Aleksandr and Gasnikov, Alexander and Gorbunov, Eduard and Tak{\'a}c, Martin},
  journal={arXiv preprint arXiv:2412.17050},
  year={2024}
}

@inproceedings{hubler2024parameter,
  title={Parameter-agnostic optimization under relaxed smoothness},
  author={H{\"u}bler, Florian and Yang, Junchi and Li, Xiang and He, Niao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4861--4869},
  year={2024},
  organization={PMLR}
}

@inproceedings{gorbunov2024highprobability,
    title={High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise},
    author={Eduard Gorbunov and Abdurakhmon Sadiev and Marina Danilova and Samuel Horv{\'a}th and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richt{\'a}rik},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
}

@article{chezhegov2024gradient,
  title={Gradient Clipping Improves {AdaGrad} when the Noise Is Heavy-Tailed},
  author={Chezhegov, Savelii and Klyukin, Yaroslav and Semenov, Andrei and Beznosikov, Aleksandr and Gasnikov, Alexander and Horv{\'a}th, Samuel and Tak{\'a}{\v{c}}, Martin and Gorbunov, Eduard},
  journal={arXiv preprint arXiv:2406.04443},
  year={2024}
}


@article{nguyen2023improved,
  title={Improved convergence in high probability of clipped gradient methods with heavy tailed noise},
  author={Nguyen, Ta Duy and Nguyen, Thien H and Ene, Alina and Nguyen, Huy},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={24191--24222},
  year={2023}
}


@book{shor2012minimization,
  title={Minimization methods for non-differentiable functions},
  author={Shor, Naum Zuselevich},
  volume={3},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{ermoliev1988stochastic,
  title={Stochastic quasigradient methods. numerical techniques for stochastic optimization},
  author={Ermoliev, Yuri},
  journal={Springer Series in Computational Mathematics},
  number={10},
  pages={141--185},
  year={1988},
  publisher={Springer}
}

@article{alber1998projected,
  title={On the projected subgradient method for nonsmooth convex optimization in a Hilbert space},
  author={Alber, Ya I and Iusem, Alfredo N and Solodov, Mikhail V},
  journal={Mathematical Programming},
  volume={81},
  pages={23--35},
  year={1998},
  publisher={Springer}
}

@article{yu2023smoothed,
  title={Smoothed Gradient Clipping and Error Feedback for Distributed Optimization under Heavy-Tailed Noise},
  author={Yu, Shuhua and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2310.16920},
  year={2023}
}


@inproceedings{karimireddy2019error,
  title={Error feedback fixes sign{SGD} and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}



@article{stich2019error,
    author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
  title   = {The Error-Feedback framework: {SGD} with Delayed Gradients},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {237},
  pages   = {1--36}
}

@inproceedings{khirirat2019convergence,
  title={Convergence bounds for compressed gradient methods with memory based error compensation},
  author={Khirirat, Sarit and Magn{\'u}sson, Sindri and Johansson, Mikael},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2857--2861},
  year={2019},
  organization={IEEE}
}


@article{gorbunov2020linearly,
  title={Linearly converging error compensated {SGD}},
  author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20889--20900},
  year={2020}
}


@article{qian2021error,
  title={Error compensated distributed {SGD} can be accelerated},
  author={Qian, Xun and Richt{\'a}rik, Peter and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30401--30413},
  year={2021}
}

@inproceedings{danilova2022distributed,
  title={Distributed methods with absolute compression and error compensation},
  author={Danilova, Marina and Gorbunov, Eduard},
  booktitle={International Conference on Mathematical Optimization Theory and Operations Research},
  pages={163--177},
  year={2022},
  organization={Springer}
}

@inproceedings{qian2023catalyst,
  title={Catalyst acceleration of error compensated methods leads to better communication complexity},
  author={Qian, Xun and Dong, Hanze and Zhang, Tong and Richtarik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={615--649},
  year={2023},
  organization={PMLR}
}

@article{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, C{\'e}dric},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{stich2018sparsified,
  title={Sparsified {SGD} with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{wu2018error,
  title={Error compensated quantized {SGD} and its applications to large-scale distributed optimization},
  author={Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={5325--5333},
  year={2018},
  organization={PMLR}
}

@inproceedings{tang2019doublesqueeze,
  title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
  author={Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={6155--6165},
  year={2019},
  organization={PMLR}
}


@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{Ganesh_2019_orange,
author = {Ganesh, P. and Volle, Kyle and Burks, T.F. and Mehta, Siddhartha},
year = {2019},
month = {01},
pages = {70-75},
title = {Deep Orange: Mask R-CNN based Orange Detection and Segmentation},
volume = {52},
journal = {IFAC-PapersOnLine},
doi = {10.1016/j.ifacol.2019.12.499}
}


@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}


@article{Asi_2019_proximal_approximate,
author = {Asi, Hilal and Duchi, John C.},
title = {Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity},
journal = {SIAM Journal on Optimization},
volume = {29},
number = {3},
pages = {2257-2290},
year = {2019},
doi = {10.1137/18M1230323}
}

@article{beznosikov2023biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={276},
  pages={1--50},
  year={2023}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}


@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}


@inproceedings{dwork2006calibrating,
  title={Calibrating noise to sensitivity in private data analysis},
  author={Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  booktitle={Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3},
  pages={265--284},
  year={2006},
  organization={Springer}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@inproceedings{DBLP:conf/sp/XiaoXWD23,
  author       = {Hanshen Xiao and
                  Zihang Xiang and
                  Di Wang and
                  Srinivas Devadas},
  title        = {A Theory to Instruct Differentially-Private Learning via Clipping
                  Bias Reduction},
  booktitle    = {44th {IEEE} Symposium on Security and Privacy, {SP} 2023, San Francisco,
                  CA, USA, May 21-25, 2023},
  pages        = {2170--2189},
  publisher    = {{IEEE}},
  year         = {2023},
  doi          = {10.1109/SP46215.2023.10179409}
}

@inproceedings{andrew2021differentially,
  title={Differentially Private Learning with Adaptive Clipping},
  author={Andrew, Galen and Thakkar, Om and McMahan, H Brendan and Ramaswamy, Swaroop},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12191--12203},
  year={2021}
}

@article{ponomareva2023dp,
  title={How to dp-fy ml: A practical guide to machine learning with differential privacy},
  author={Ponomareva, Natalia and Hazimeh, Hussein and Kurakin, Alex and Xu, Zheng and Denison, Carson and McMahan, H Brendan and Vassilvitskii, Sergei and Chien, Steve and Thakurta, Abhradeep Guha},
  journal={Journal of Artificial Intelligence Research},
  volume={77},
  pages={1113--1201},
  year={2023}
}

@inproceedings{el2021distributed,
  title={Distributed momentum for byzantine-resilient stochastic gradient descent},
  author={El Mhamdi, El Mahdi and Guerraoui, Rachid and Rouault, S{\'e}bastien Louis Alexandre},
  booktitle={9th International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{malinovsky2023byzantine,
  title={Byzantine robustness and partial participation can be achieved simultaneously: Just clip gradient differences},
  author={Malinovsky, Grigory and Gorbunov, Eduard and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={Privacy Regulation and Protection in Machine Learning},
  year={2023}
}

@inproceedings{karimireddy2021learning,
  title={Learning from history for byzantine robust optimization},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={5311--5319},
  year={2021},
  organization={PMLR}
}

@article{ozfatura2023byzantines,
  title={Byzantines can also learn from history: Fall of centered clipping in federated learning},
  author={{\"O}zfatura, Kerem and {\"O}zfatura, Emre and K{\"u}p{\c{c}}{\"u}, Alptekin and Gunduz, Deniz},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={19},
  pages={2010--2022},
  year={2023},
  publisher={IEEE}
}

@inproceedings{gower2019sgd,
  title={{SGD:} General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={International conference on machine learning},
  pages={5200--5209},
  year={2019},
  organization={PMLR}
}


@article{
merad2023robust,
title={Robust Stochastic Optimization via Gradient Quantile Clipping},
author={Ibrahim Merad and St{\'e}phane Ga{\"\i}ffas},
journal={Transactions on Machine Learning Research},
year={2024}
}

@article{shulgin2024convergence,
  title={On the Convergence of {DP-SGD} with Adaptive Clipping},
  author={Shulgin, Egor and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2412.19916},
  year={2024}
}



@inproceedings{
das2021convergence,
title={Differentially Private Federated Learning with Normalized Updates},
author={Rudrajit Das and Abolfazl Hashemi and sujay sanghavi and Inderjit S Dhillon},
booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
year={2022}
}

@inproceedings{li2024an,
    title={An improved analysis of per-sample and per-update clipping in federated learning},
    author={Bo Li and Xiaowen Jiang and Mikkel N. Schmidt and Tommy Sonne Alstr{\o}m and Sebastian U Stich},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024}
}

@inproceedings{zhang2022understanding,
  title={Understanding clipping for federated learning: Convergence and client-level differential privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  booktitle={International Conference on Machine Learning, ICML 2022},
  year={2022}
}

@article{zhang2024private,
  title={Private and Communication-Efficient Federated Learning based on Differentially Private Sketches},
  author={Zhang, Meifan and Xie, Zhanhong and Yin, Lihua},
  journal={arXiv preprint arXiv:2410.05733},
  year={2024}
}

@inproceedings{murata2023diff2,
  title={DIFF2: Differential private optimization via gradient differences for nonconvex distributed learning},
  author={Murata, Tomoya and Suzuki, Taiji},
  booktitle={International Conference on Machine Learning},
  pages={25523--25548},
  year={2023},
  organization={PMLR}
}

@inproceedings{wang2024efficient,
  title={Efficient Private Federated Non-Convex Optimization With Shuffled Model},
  author={Lingxiao Wang and Xingyu Zhou and Kumar Kshitij Patel and Lawrence Tang and Aadirupa Saha},
  booktitle={Privacy Regulation and Protection in Machine Learning},
  year={2024}
}

@inproceedings{noble2022differentially,
  title={Differentially private federated learning on heterogeneous data},
  author={Noble, Maxence and Bellet, Aur{\'e}lien and Dieuleveut, Aymeric},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={10110--10145},
  year={2022},
  organization={PMLR}
}

@article{li2025convergence,
  title={Convergence and privacy of decentralized nonconvex optimization with gradient clipping and communication compression},
  author={Li, Boyue and Chi, Yuejie},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2025},
  publisher={IEEE}
}

@article{chen2024differentially,
  title={Differentially Private Federated Learning on Non-iid Data: Convergence Analysis and Adaptive Optimization},
  author={Chen, Lin and Ding, Xiaofeng and Bao, Zhifeng and Zhou, Pan and Jin, Hai},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}


@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}


@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}


@article{jin2021nonconvex,
  title={On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={Journal of the ACM (JACM)},
  volume={68},
  number={2},
  pages={1--29},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@article{chen2020understanding,
  title={Understanding gradient clipping in private sgd: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13773--13782},
  year={2020}
}


@inproceedings{
papernot2021hyperparameter,
title={Hyperparameter Tuning with Renyi Differential Privacy},
author={Nicolas Papernot and Thomas Steinke},
booktitle={International Conference on Learning Representations},
year={2022}
}

@article{liu2022communication,
  title={A communication-efficient distributed gradient clipping algorithm for training deep neural networks},
  author={Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26204--26217},
  year={2022}
}

@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE transactions on information forensics and security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}

@inproceedings{
crawshaw2023episode,
title={{EPISODE}: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data},
author={Michael Crawshaw and Yajie Bao and Mingrui Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@article{konecny2017federated,
      title={Federated Learning: Strategies for Improving Communication Efficiency}, 
      author={Jakub Kone{\v{c}}n{\'y} and H. Brendan McMahan and Felix X. Yu and Peter Richt{\'a}rik and Ananda Theertha Suresh and Dave Bacon},
      year={2016},
      journal={NIPS Private Multi-Party Machine Learning Workshop}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{kairouz2021advances_,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{kairouz2021advances,
  author    = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aur{\'{e}}lien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista A. Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D{'}Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adri{\`{a}} Gasc{\'{o}}n and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Kone{\v{c}}n{\'y} and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancr{\`{e}}de Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer {\"{O}}zg{\"{u}}r and Rasmus Pagh and Hang Qi and Daniel Ramage and Ramesh Raskar and Mariana Raykova and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tram{\`{e}}r and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
  title     = {Advances and Open Problems in Federated Learning},
  journal   = {Found. Trends Mach. Learn.},
  volume    = {14},
  number    = {1-2},
  pages     = {1--210},
  year      = {2021},
  url       = {https://doi.org/10.1561/2200000083},
  doi       = {10.1561/2200000083},
  timestamp = {Sat, 09 Apr 2022 12:27:56 +0200},
  biburl    = {https://dblp.org/rec/journals/ftml/KairouzMABBBBCC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{boenisch2023curious,
  title={When the curious abandon honesty: Federated learning is not private},
  author={Boenisch, Franziska and Dziedzic, Adam and Schuster, Roei and Shamsabadi, Ali Shahin and Shumailov, Ilia and Papernot, Nicolas},
  booktitle={2023 IEEE 8th European Symposium on Security and Privacy (EuroS{\&}P)},
  pages={175--199},
  year={2023},
  organization={IEEE}
}

@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}

@inproceedings{reddi2021adaptive,
  title={Adaptive Federated Optimization},
  author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{wang2022differentially,
  title={Differentially private {SGD} with non-smooth losses},
  author={Wang, Puyu and Lei, Yunwen and Ying, Yiming and Zhang, Hai},
  journal={Applied and Computational Harmonic Analysis},
  volume={56},
  pages={306--336},
  year={2022},
  publisher={Elsevier}
}


@article{li2022soteriafl,
  title={{SoteriaFL}: A unified framework for private federated learning with communication compression},
  author={Li, Zhize and Zhao, Haoyu and Li, Boyue and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4285--4300},
  year={2022}
}


@InProceedings{pmlr-v216-wang23b,
  title = 	 {Efficient Privacy-Preserving Stochastic Nonconvex Optimization},
  author =       {Wang, Lingxiao and Jayaraman, Bargav and Evans, David and Gu, Quanquan},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {2203--2213},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR}
}





@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages={770--778},
	year={2016}
}

@techreport{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	jnumber = {Technical Report TR-2009},
	institution = {University of Toronto,  Toronto}
}

@inproceedings{paszke2019pytorch,
	title={Pytorch: An imperative style, high-performance deep learning library},
	author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	year={2019}
}

@misc{idelbayev18a,
  author= "Yerlan Idelbayev",
  title="Proper {ResNet} Implementation for {CIFAR10/CIFAR100} in {PyTorch}",
  howpublished="\url{https://github.com/akamaster/pytorch_resnet_cifar10}",
  note="Accessed: 2024-12-31"
}

@article{idelbayev2018proper,
  title={Proper {ResNet} implementation for {CIFAR10/CIFAR100} in PyTorch},
  author={Idelbayev, Yerlan},
  journal={CIFAR100 in PyTorch},
  year={2018}
}

@inproceedings{lowy2023private,
  title={Private non-convex federated learning without a trusted server},
  author={Lowy, Andrew and Ghafelebashi, Ali and Razaviyayn, Meisam},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5749--5786},
  year={2023},
  organization={PMLR}
}

@inproceedings{zhang2020private,
  title={Private and communication-efficient edge learning: a sparse differential gaussian-masking distributed {SGD} approach},
  author={Zhang, Xin and Fang, Minghong and Liu, Jia and Zhu, Zhengyuan},
  booktitle={Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  pages={261--270},
  year={2020}
}

@article{sun2019can,
  title={Can you really backdoor federated learning?},
  author={Sun, Ziteng and Kairouz, Peter and Suresh, Ananda Theertha and McMahan, H Brendan},
  journal={arXiv preprint arXiv:1911.07963},
  year={2019}
}

@article{carmon2020lower,
  title={Lower bounds for finding stationary points I},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={71--120},
  year={2020},
  publisher={Springer}
}

@inproceedings{khaled2020tighter,
  title={Tighter theory for local {SGD} on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@article{kurakin2022toward,
  title={Toward training at imagenet scale with differential privacy},
  author={Kurakin, Alexey and Song, Shuang and Chien, Steve and Geambasu, Roxana and Terzis, Andreas and Thakurta, Abhradeep},
  journal={arXiv preprint arXiv:2201.12328},
  year={2022}
}

@inproceedings{qian2021understanding,
  title={Understanding gradient clipping in incremental gradient methods},
  author={Qian, Jiang and Wu, Yuren and Zhuang, Bojin and Wang, Shaojun and Xiao, Jing},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1504--1512},
  year={2021},
  organization={PMLR}
}