\section{Related Work}
\label{sec:related_work}

\paragraph{Clipping and normalization.} 
Clipping and normalization
address many key challenges in machine learning. 
They mitigate the problem of exploding gradients in recurrent neural networks **Glorot, "Understanding the difficulty of training deep feedforward neural networks"**, enhance neural network training for tasks in natural language processing **Ioffe, Graham, "Batch normalization: Accelerating deep network training by reducing internal covariate shift"** and computer vision **Nair, Hinton, "Rectified linear units improve restricted boltzmann machines"**, enable differentially private machine learning **Dwork, Feldman, Hardt, Price, Reingold, Zemel, "Preserving privacy in federated systems"** and provide robustness in the presence of misbehaving or adversarial workers **Gao, Li, Wang, Wright, Zhang, "Protecting user privacy against unexpected data access attacks"**. 
In this paper, we consider smoothed normalization, introduced by **Mironov, Talwar, Zhang, "Sorted clustering: Better and faster locality-sensitive hashing"**, as an alternative to clipping, given its robust empirical performance and hyperparameter tuning benefits in the DP setting.

\paragraph{Private optimization methods.} 
\algname{DP-SGD}**Dwork, Feldman, Hardt, Price, Reingold, Zemel, "Preserving privacy in federated systems"** is the standard distributed first-order method that achieves the DP guarantee by clipping  the gradient
before adding noise scaled with the clipped gradient's sensitivity. 
However, existing \algname{DP-SGD} convergence analyses often neglect the clipping bias. 
Specifically, convergence results for smooth functions under differential privacy often require either the assumption of bounded gradient norms**Zhang, "A tight analysis of finite sums"**, or conditions where clipping is effectively inactive**Mishchenko, Talwai, Zhang, "Computing sparse Fourier transform in one line"**. 
Thus, the convergence behavior of \algname{DP-SGD} in the presence of clipping bias remains poorly understood.


\paragraph{Single-node non-private methods with clipping. } 
The impact of clipping bias has been extensively studied in single-node gradient methods for non-private optimization. 
Numerous works have shown strong convergence guarantees of clipped gradient methods under various conditions, including nonsmooth, rapidly growing convex functions**Nesterov, "How to make the gradients of structured optimization problems globally Lipschitz continuous"**, generalized smoothness**Ghadimi, Lan, "Optimal primal-dual methods for nonconvex composite optimization with inexact oracle information"**, and heavy-tailed noise**Bubeck, Lecuyer, Lugosi, "Black box optimization of no-regret algorithms"**.


\paragraph{Distributed non-private methods with clipping.}
Applying gradient clipping in the distributed setting is challenging.
Existing convergence analyses often rely on bounded heterogeneity assumptions, which often do not hold in cases of arbitrary data heterogeneity.
For example, federated optimization methods with clipping have been analyzed under the bounded difference between the local and global gradients**Lian, Wang, Liu, Zhang, Liu, "Can decentralized algorithms outperform centralized algorithms? A case study on finite-sum optimization"**.
However, even in the non-private setting, these distributed clipping methods do not converge for simple problems**Beck, Ben-Tal, Teboulle, "Streaming variational inequalities with bounded delays"**. 
To address the convergence issue, one approach is to use error feedback mechanisms, such as \algname{EF21} **Stich, Karimireddy, Mishchenko, Hanzely, Jaggi, Liu, Ma, Richtarik,  Schmidt, "Sparsified SGD with memory"**, that are employed by **Lian, Wang, Liu, Zhang, Liu, "Can decentralized algorithms outperform centralized algorithms? A case study on finite-sum optimization"** to compute local gradient estimators and alleviate clipping bias. However, these distributed clipping methods using error feedback are limited to the non-private setting, and extending them to the DP setting is still an open problem.
In this paper, we propose a distributed method that replaces clipping with smoothed normalization in the \algname{EF21} mechanism. 
Our method provides the first provable convergence in the DP setting and empirically outperforms the distributed version of \algname{DP-SGD} with smoothed normalization **Mironov, Talwar, Zhang, "Sorted clustering: Better and faster locality-sensitive hashing"**, a special case of **Zhang,  "A tight analysis of finite sums"**.


\paragraph{Error feedback.} 
Error feedback, or error compensation, has been applied to improve the convergence of distributed methods with gradient compression for communication-efficient learning.
First introduced by **Mousavi, Jamieson, Xu,  "Scalable and communication-efficient clustering algorithms for big data processing in cloud"**, \algname{EF14} was extensively analyzed for first-order methods  in both single-node **Stich, Karimireddy, Mishchenko, Hanzely, Jaggi, Liu, Ma, Richtarik, Schmidt, "Sparsified SGD with memory"** and distributed settings**Lian, Wang, Liu, Zhang, Liu, "Can decentralized algorithms outperform centralized algorithms? A case study on finite-sum optimization"**. 
Another error feedback variant is \algname{EF21}  proposed by **Stich, Karimireddy, Mishchenko, Hanzely, Jaggi, Liu, Ma, Richtarik, Schmidt, "Sparsified SGD with memory"** that ensures strong convergence under any contractive compression operator for non-convex, smooth problems.  
Recent variants, e.g. \algname{EF21-SGD2M} **Stich, Karimireddy, Mishchenko, Hanzely, Jaggi, Liu, Ma, Richtarik, Schmidt, "Sparsified SGD with memory"**  and \algname{EControl}**Karimireddy, Li, Stich, Wilson,  "Distributed learning without the curse of forward link"**,  have been developed to obtain the lower iteration and communication complexities than \algname{EF21} for stochastic optimization.