\section{Related Work}
\label{sec:related_work}

\paragraph{Clipping and normalization.} 
Clipping and normalization
address many key challenges in machine learning. 
They mitigate the problem of exploding gradients in recurrent neural networks \citep{pascanu2013difficulty}, enhance neural network training for tasks in natural language processing \citep{merity2017regularizing,brown2020language} and computer vision \citep{brock2021high}, enable differentially private machine learning \citep{abadi2016deep,mcmahan2018learning}, and provide robustness in the presence of misbehaving or adversarial workers \citep{karimireddy2021learning,ozfatura2023byzantines,malinovsky2023byzantine}. 
In this paper, we consider smoothed normalization, introduced by \citet{yang2022normalized, bu2024automatic}, as an alternative to clipping, given its robust empirical performance and hyperparameter tuning benefits in the DP setting.

\paragraph{Private optimization methods.} 
\algname{DP-SGD}~\citep{abadi2016deep} is the standard distributed first-order method that achieves the DP guarantee by clipping  the gradient
before adding noise scaled with the clipped gradient's sensitivity. 
However, existing \algname{DP-SGD} convergence analyses often neglect the clipping bias. 
Specifically, convergence results for smooth functions under differential privacy often require either the assumption of bounded gradient norms~\citep{zhang2020private,li2022soteriafl,zhang2022understanding,pmlr-v216-wang23b,lowy2023private, murata2023diff2, wang2024efficient} or conditions where clipping is effectively inactive~\citep{zhang2024private,noble2022differentially}. 
Thus, the convergence behavior of \algname{DP-SGD} in the presence of clipping bias remains poorly understood.


\paragraph{Single-node non-private methods with clipping. } 
The impact of clipping bias has been extensively studied in single-node gradient methods for non-private optimization. 
Numerous works have shown strong convergence guarantees of clipped gradient methods under various conditions, including nonsmooth, rapidly growing convex functions~\citet{shor2012minimization,ermoliev1988stochastic,alber1998projected}, generalized smoothness~\citep{zhang2019gradient,koloskova2023revisiting,gorbunov2024methods,vankov2024optimizing,lobanov2024linear,hubler2024parameter}, and heavy-tailed noise~\citep{gorbunov2020stochastic,nguyen2023improved,gorbunov2024highprobability,hubler2024gradient,chezhegov2024gradient}.


\paragraph{Distributed non-private methods with clipping.}
Applying gradient clipping in the distributed setting is challenging.
Existing convergence analyses often rely on bounded heterogeneity assumptions, which often do not hold in cases of arbitrary data heterogeneity.
For example, federated optimization methods with clipping have been analyzed under the bounded difference between the local and global gradients~\citep{wei2020federated,liu2022communication,crawshaw2023episode,li2024an}.
However, even in the non-private setting, these distributed clipping methods do not converge for simple problems~\citep{chen2020understanding,khirirat2023clip21}. 
To address the convergence issue, one approach is to use error feedback mechanisms, such as \algname{EF21} \citep{richtarik2021ef21}, that are employed by \citet{khirirat2023clip21,yu2023smoothed} to compute local gradient estimators and alleviate clipping bias. However, these distributed clipping methods using error feedback are limited to the non-private setting, and extending them to the DP setting is still an open problem.
In this paper, we propose a distributed method that replaces clipping with smoothed normalization in the \algname{EF21} mechanism. 
Our method provides the first provable convergence in the DP setting and empirically outperforms the distributed version of \algname{DP-SGD} with smoothed normalization \citet{bu2024automatic, yang2022normalized}, a special case of \citet{das2021convergence}.


\paragraph{Error feedback.} 
Error feedback, or error compensation, has been applied to improve the convergence of distributed methods with gradient compression for communication-efficient learning.
First introduced by \citet{seide20141}, \algname{EF14} was extensively analyzed for first-order methods  in both single-node ~\citep{stich2018sparsified,karimireddy2019error,stich2019error,khirirat2019convergence} and distributed settings~\citep{wu2018error,alistarh2018convergence,gorbunov2020linearly,qian2021error,tang2019doublesqueeze,danilova2022distributed,qian2023catalyst}. 
Another error feedback variant is \algname{EF21}  proposed by \citet{richtarik2021ef21} that ensures strong convergence under any contractive compression operator for non-convex, smooth problems.  
Recent variants, e.g. \algname{EF21-SGD2M}~\citep{fatkhullin2024momentum}  and \algname{EControl}~\citep{gao2023econtrol},  have been developed to obtain the lower iteration and communication complexities than \algname{EF21} for stochastic optimization.