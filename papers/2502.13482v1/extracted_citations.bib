@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{alber1998projected,
  title={On the projected subgradient method for nonsmooth convex optimization in a Hilbert space},
  author={Alber, Ya I and Iusem, Alfredo N and Solodov, Mikhail V},
  journal={Mathematical Programming},
  volume={81},
  pages={23--35},
  year={1998},
  publisher={Springer}
}

@article{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, C{\'e}dric},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{brock2021high,
  title={High-performance large-scale image recognition without normalization},
  author={Brock, Andy and De, Soham and Smith, Samuel L and Simonyan, Karen},
  booktitle={International conference on machine learning},
  pages={1059--1071},
  year={2021},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bu2024automatic,
  title={Automatic clipping: Differentially private deep learning made easier and stronger},
  author={Bu, Zhiqi and Wang, Yu-Xiang and Zha, Sheng and Karypis, George},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chen2020understanding,
  title={Understanding gradient clipping in private sgd: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13773--13782},
  year={2020}
}

@article{chezhegov2024gradient,
  title={Gradient Clipping Improves {AdaGrad} when the Noise Is Heavy-Tailed},
  author={Chezhegov, Savelii and Klyukin, Yaroslav and Semenov, Andrei and Beznosikov, Aleksandr and Gasnikov, Alexander and Horv{\'a}th, Samuel and Tak{\'a}{\v{c}}, Martin and Gorbunov, Eduard},
  journal={arXiv preprint arXiv:2406.04443},
  year={2024}
}

@inproceedings{danilova2022distributed,
  title={Distributed methods with absolute compression and error compensation},
  author={Danilova, Marina and Gorbunov, Eduard},
  booktitle={International Conference on Mathematical Optimization Theory and Operations Research},
  pages={163--177},
  year={2022},
  organization={Springer}
}

@article{ermoliev1988stochastic,
  title={Stochastic quasigradient methods. numerical techniques for stochastic optimization},
  author={Ermoliev, Yuri},
  journal={Springer Series in Computational Mathematics},
  number={10},
  pages={141--185},
  year={1988},
  publisher={Springer}
}

@article{fatkhullin2024momentum,
  title={Momentum provably improves error feedback!},
  author={Fatkhullin, Ilyas and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gorbunov2020linearly,
  title={Linearly converging error compensated {SGD}},
  author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20889--20900},
  year={2020}
}

@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15042--15053},
  year={2020}
}

@inproceedings{gorbunov2024highprobability,
    title={High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise},
    author={Eduard Gorbunov and Abdurakhmon Sadiev and Marina Danilova and Samuel Horv{\'a}th and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richt{\'a}rik},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
}

@inproceedings{hubler2024parameter,
  title={Parameter-agnostic optimization under relaxed smoothness},
  author={H{\"u}bler, Florian and Yang, Junchi and Li, Xiang and He, Niao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4861--4869},
  year={2024},
  organization={PMLR}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes sign{SGD} and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}

@inproceedings{karimireddy2021learning,
  title={Learning from history for byzantine robust optimization},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={5311--5319},
  year={2021},
  organization={PMLR}
}

@inproceedings{khirirat2019convergence,
  title={Convergence bounds for compressed gradient methods with memory based error compensation},
  author={Khirirat, Sarit and Magn{\'u}sson, Sindri and Johansson, Mikael},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2857--2861},
  year={2019},
  organization={IEEE}
}

@article{khirirat2023clip21,
  title={Clip21: Error feedback for gradient clipping},
  author={Khirirat, Sarit and Gorbunov, Eduard and Horv{\'a}th, Samuel and Islamov, Rustem and Karray, Fakhri and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2305.18929},
  year={2023}
}

@inproceedings{koloskova2023revisiting,
  title={Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees},
  author={Koloskova, Anastasia and Hendrikx, Hadrien and Stich, Sebastian U},
  booktitle={International Conference on Machine Learning},
  pages={17343--17363},
  year={2023},
  organization={PMLR}
}

@article{li2022soteriafl,
  title={{SoteriaFL}: A unified framework for private federated learning with communication compression},
  author={Li, Zhize and Zhao, Haoyu and Li, Boyue and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4285--4300},
  year={2022}
}

@inproceedings{li2024an,
    title={An improved analysis of per-sample and per-update clipping in federated learning},
    author={Bo Li and Xiaowen Jiang and Mikkel N. Schmidt and Tommy Sonne Alstr{\o}m and Sebastian U Stich},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024}
}

@article{liu2022communication,
  title={A communication-efficient distributed gradient clipping algorithm for training deep neural networks},
  author={Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26204--26217},
  year={2022}
}

@article{lobanov2024linear,
  title={Linear Convergence Rate in Convex Setup is Possible! Gradient Descent Method Variants under {$(L_0, L_1)$}-Smoothness},
  author={Lobanov, Aleksandr and Gasnikov, Alexander and Gorbunov, Eduard and Tak{\'a}c, Martin},
  journal={arXiv preprint arXiv:2412.17050},
  year={2024}
}

@inproceedings{lowy2023private,
  title={Private non-convex federated learning without a trusted server},
  author={Lowy, Andrew and Ghafelebashi, Ali and Razaviyayn, Meisam},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5749--5786},
  year={2023},
  organization={PMLR}
}

@inproceedings{malinovsky2023byzantine,
  title={Byzantine robustness and partial participation can be achieved simultaneously: Just clip gradient differences},
  author={Malinovsky, Grigory and Gorbunov, Eduard and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={Privacy Regulation and Protection in Machine Learning},
  year={2023}
}

@inproceedings{mcmahan2018learning,
  title={Learning Differentially Private Recurrent Language Models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{murata2023diff2,
  title={DIFF2: Differential private optimization via gradient differences for nonconvex distributed learning},
  author={Murata, Tomoya and Suzuki, Taiji},
  booktitle={International Conference on Machine Learning},
  pages={25523--25548},
  year={2023},
  organization={PMLR}
}

@article{nguyen2023improved,
  title={Improved convergence in high probability of clipped gradient methods with heavy tailed noise},
  author={Nguyen, Ta Duy and Nguyen, Thien H and Ene, Alina and Nguyen, Huy},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={24191--24222},
  year={2023}
}

@inproceedings{noble2022differentially,
  title={Differentially private federated learning on heterogeneous data},
  author={Noble, Maxence and Bellet, Aur{\'e}lien and Dieuleveut, Aymeric},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={10110--10145},
  year={2022},
  organization={PMLR}
}

@article{ozfatura2023byzantines,
  title={Byzantines can also learn from history: Fall of centered clipping in federated learning},
  author={{\"O}zfatura, Kerem and {\"O}zfatura, Emre and K{\"u}p{\c{c}}{\"u}, Alptekin and Gunduz, Deniz},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={19},
  pages={2010--2022},
  year={2023},
  publisher={IEEE}
}

@InProceedings{pascanu2013difficulty,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1310--1318},
  year = 	 {2013},
  publisher =    {PMLR}
}

@InProceedings{pmlr-v216-wang23b,
  title = 	 {Efficient Privacy-Preserving Stochastic Nonconvex Optimization},
  author =       {Wang, Lingxiao and Jayaraman, Bargav and Evans, David and Gu, Quanquan},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {2203--2213},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR}
}

@article{qian2021error,
  title={Error compensated distributed {SGD} can be accelerated},
  author={Qian, Xun and Richt{\'a}rik, Peter and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30401--30413},
  year={2021}
}

@inproceedings{qian2023catalyst,
  title={Catalyst acceleration of error compensated methods leads to better communication complexity},
  author={Qian, Xun and Dong, Hanze and Zhang, Tong and Richtarik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={615--649},
  year={2023},
  organization={PMLR}
}

@article{richtarik2021ef21,
  title={{EF21:} A new, simpler, theoretically better, and practically faster error feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4384--4396},
  year={2021}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  volume={2014},
  pages={1058--1062},
  year={2014},
  organization={Singapore}
}

@book{shor2012minimization,
  title={Minimization methods for non-differentiable functions},
  author={Shor, Naum Zuselevich},
  volume={3},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{stich2018sparsified,
  title={Sparsified {SGD} with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{stich2019error,
    author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
  title   = {The Error-Feedback framework: {SGD} with Delayed Gradients},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {237},
  pages   = {1--36}
}

@inproceedings{tang2019doublesqueeze,
  title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
  author={Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={6155--6165},
  year={2019},
  organization={PMLR}
}

@inproceedings{wang2024efficient,
  title={Efficient Private Federated Non-Convex Optimization With Shuffled Model},
  author={Lingxiao Wang and Xingyu Zhou and Kumar Kshitij Patel and Lawrence Tang and Aadirupa Saha},
  booktitle={Privacy Regulation and Protection in Machine Learning},
  year={2024}
}

@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE transactions on information forensics and security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wu2018error,
  title={Error compensated quantized {SGD} and its applications to large-scale distributed optimization},
  author={Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={5325--5333},
  year={2018},
  organization={PMLR}
}

@article{yang2022normalized,
  title={Normalized/clipped {SGD} with perturbation for differentially private non-convex optimization},
  author={Yang, Xiaodong and Zhang, Huishuai and Chen, Wei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2206.13033},
  year={2022}
}

@article{yu2023smoothed,
  title={Smoothed Gradient Clipping and Error Feedback for Distributed Optimization under Heavy-Tailed Noise},
  author={Yu, Shuhua and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2310.16920},
  year={2023}
}

@inproceedings{zhang2020private,
  title={Private and communication-efficient edge learning: a sparse differential gaussian-masking distributed {SGD} approach},
  author={Zhang, Xin and Fang, Minghong and Liu, Jia and Zhu, Zhengyuan},
  booktitle={Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  pages={261--270},
  year={2020}
}

@inproceedings{zhang2022understanding,
  title={Understanding clipping for federated learning: Convergence and client-level differential privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  booktitle={International Conference on Machine Learning, ICML 2022},
  year={2022}
}

@article{zhang2024private,
  title={Private and Communication-Efficient Federated Learning based on Differentially Private Sketches},
  author={Zhang, Meifan and Xie, Zhanhong and Yin, Lihua},
  journal={arXiv preprint arXiv:2410.05733},
  year={2024}
}

