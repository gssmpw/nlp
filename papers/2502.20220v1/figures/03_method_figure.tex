\begin{figure*}[htb]
    \centering
    \includegraphics[width=\textwidth]{images/03_Avat3r_Method_Figure.pdf}
    \caption{\textbf{Method Overview:} {\OURS} reconstructs a high-quality 3D representation from just a few input images by predicting 3D Gaussian attributes for each input pixel. We first obtain position maps $I^{pos}$ from DUSt3R and feature maps $I^{feat}$ from Sapiens for each view. These are then patchified
    to serve as image tokens for the Vision Transformer back-end. Dense Self-attention performs matching within tokens of the same image and across views to infer 3D structure. Dynamics are modelled via cross-attention layers that attend to a sequenced expression code. The resulting intermediate feature maps are decoded into Gaussian position, scale, rotation, color, and opactiy maps, and then upsampled to the original input image resolution. Finally, we add skip connections to the predicted position and color maps. Gaussians that belong to pixels with a confidence larger than a pre-defined threshold are collected and can be rendered from arbitrary viewpoints. }
    \label{fig:03_method_figure}
\end{figure*}
