\section{Introduction}






Lifelike 3D head avatars find use in a variety of applications such as telepresence, movie production, personalized games, or digital education. Typically, the creation of a high-quality avatar requires studio-quality videos that have to be recorded in hour-long sessions in expensive capture setups. This limits the applicability of a 3D head avatar creation pipeline, especially in casual settings where users may only want to use a few images taken with their smartphone to create a convincing digital double. In this paper, we present Avat3r, a method that can reconstruct an animatable 3D head avatar from just a few images of a person. 

Building a photo-realistic 3D head avatar from a few casual images involves several hard challenges. First, sparse 3D reconstruction poses a heavily under-constrained optimization problem since some regions such as the mouth interior or the side of the head may have simply not been seen in the available images. Next, animating the head requires plausibly morphing the reconstructed face without ever having seen that particular person talking or sticking their tongue out. Finally, the input images may be inconsistent, e.g., because the person could not remain completely static during a casual capture.
All of these three challenges, i.e., sparse 3D reconstruction, face animation, and robust reconstruction, have to be solved simultaneously to successfully reconstruct a photo-realistic 3D head avatar given just a few images.

Due to the aforementioned challenges, the current best methods for high-quality avatar creation~\cite{giebenhain2024npga, saito2024rgca, qian2024gaussianavatars, kirschstein2024diffusionavatars} cannot be applied since their optimization-based approach requires many more observations than just a few images of a person. 
On the other hand, methods that aim at 3D-aware portrait animation~\cite{drobyshev2022megaportraits, tran2024voodoo3d, chu2024gagavatar, chu2024gpavatar} learn a prior over 3D heads from large video datasets that include a variety of different people. As a result, existing methods can already convincingly animate faces from as few as just a single input image. However, such methods typically focus on frontal facial appearance and may sacrifice 3D-consistency or temporal consistency to reach satisfying image synthesis quality. This makes these methods less applicable in scenarios where a proper 3D model is required.
Another approach is to build a photo-realistic 3D head model that learns the distribution of 3D face geometry, expression, and appearance, akin to classic 3DMMs. While such unconditional generative models are very versatile and recent works have shown promising results~\cite{xu2025gphm, zheng2024headgap}, they typically suffer from limited generalizability along the identity axis due to existing 3D face datasets only providing a few hundred different persons~\cite{kirschstein2023nersemble, ava256, yang2020facescape, pan2024renderme360}. 

Our approach is motivated by the observation that 3D data for faces is limited in the identity axis, but not in the expression axis that is already well captured in existing datasets. We therefore design a system that is \textit{conditional} on the identity of a person, but \textit{generalizes} along the expression axis. Opposed to photo-realistic 3D face models, the network does not have to learn the full extent of human facial appearance but just needs to understand how to reconstruct it from a few example images, which simplifies the task. Inspired by recent Large Reconstruction Models~\cite{tang2025lgm, xu2024grm, zhang2025gslrm}, we devise an architecture that predicts 3D Gaussians for each pixel in the input images. As such, we deliberately avoid anchoring the 3D Gaussians on a template mesh, as common in the 3D Head Avatar domain~\cite{qian2024gaussianavatars, saito2024rgca, cao2022authentic}. The reason is that human facial appearance is complex and diverse. Limiting the 3D representation to a fixed number of Gaussians anchored on a template mesh with uniform topology will cause issues in regions such as hair or face accessories. Instead, when predicting Gaussians for each foreground pixel, a person with voluminous hair will receive more primitives to model their head than a bald person.
To further simplify the task of sparse 3D reconstruction, we first compute position maps for each input image via DUSt3R~\cite{wang2024dust3r}. These position maps are injected into the network via skip connections and act as a coarse starting position for each Gaussian. We surprisingly find that DUSt3R is still capable of producing reasonable position maps even if the input images are not consistent, e.g., because they show different facial expressions or are the result of a view-inconsistent image generation model. We exploit this remarkable capability of DUSt3R to train our model on input images taken at different timesteps, which not only enables training and inference on monocular video datasets but also makes the model more robust to inconsistencies in the input images. Such inconsistencies can quickly occur in a real-life scenario where a person may not stay completely still during a phone capture of their head. Finally, to model the animation of the face, we find that simple cross-attention between inter\-mediate feature maps and a descriptive expression code are already sufficient to generalize over the space of facial expressions.
Taken together, our contributions are as follows:
\begin{itemize}
    \item We design Avat3r, a novel pipeline for creating high-quality 3D head avatars from just a few images in a matter of minutes.
    \item We build the first animatable large 3D reconstruction model and show that simple cross-attention on expression token sequences is sufficient to model complex facial animations.
    \item To improve generalization and robustness, we integrate priors from Dust3r and Sapiens via skip connections, and feed input images from different timesteps to the network during training.
\end{itemize}
