\input{figures/04_nersemble_results}

\section{Experimental Results}

\subsection{Training}

We train {\OURS} on multi-view video captures from the Ava-256 dataset~\cite{ava256}. In particular, we use the 4TB version of the dataset which contains 7.5fps videos from 80 cameras (roughly 5000 frames each) at a resolution of $1024 \times 667$. {\OURS} is trained on 244 persons from the dataset with the remaining 12 being held out. We use the provided tracked mesh to crop each image to a head-centric $512\times512$ portrait which will be fed as input to our model. For each training sample, we randomly select 4 reasonably diverse viewpoints and 4 timesteps with diverse expressions as input images $I$. On the other hand, the supervision images $I^{gt}$ are left at the highest resolution and are randomly sampled over all available cameras and timesteps. For $z_{exp}$, we use the expression codes that are provided with the Ava256 dataset.\\
{\OURS} is trained with Adam~\cite{kingma2014adam} using a learning rate of $5e-5$. We only introduce $\mathcal{L}_{lpips}$ after 3M optimization steps to avoid focusing on high-frequency details too early. In total, the model is trained for 3.5M steps with a batch size of 1 per GPU on 8 A100 GPUs which takes roughly 4 days. We further find that training convergence is significantly improved when supervising multiple viewpoints for each batch. I.e., a batch consists of 12 images: 4 with random expression that are used as input, and 8 viewpoints from the target expression that are used as supervision.

\subsection{Experiment Setup}

\paragraph{Task.} 
We evaluate the ability to create a 3D head avatar from one or multiple images of an unseen person. 

\paragraph{Metrics.} 
We employ three paired-image metrics to measure the quality of individual rendered images: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM)~\cite{wang2004ssim}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018lpips}. Furthermore, we measure visual similarity of a rendered video to its ground truth counterpart with the perceptual video metric JOD~\cite{jod_metric}. Finally, we make use of two face-specific metrics: Average Keypoint Distance (AKD) measured in pixels with keypoints estimated from PIPNet~\cite{jin2021pipnet}, and cosine similarity (CSIM) of identity embeddings based on ArcFace~\cite{deng2019arcface}.

\paragraph{Baselines.}
We compare our method with recent state-of-the-art systems for 3D head avatar creation from one and few input images. 

\textit{GPAvatar~\cite{chu2024gpavatar}.} A NeRF-based method that predicts canonical TriPlanes from multiple input images. Animations are then modeled by querying 3D points in an expression-dependent feature field inferred from FLAME.

\textit{GAGAvatar~\cite{chu2024gagavatar}.} A method for 3D-aware portrait animation from a single image that constructs two sets of 3D Gaussians: The first set is obtained by predicting 2 Gaussians for each pixel in the input image with a CNN, the second set is decoded from FLAME vertices and automatically animated through the FLAME mesh.




\input{tables/4_main_results}
\input{figures/4_few_shot_comparison}
\subsection{Few-shot 3D Head Avatar Creation}

We compare the ability to create a realistic 3D head avatar from just four input images with GPAvatar~\cite{chu2024gpavatar}. To this end, we train GPAvatar on the Ava256 dataset in the same setting as our model and use the high-quality tracked meshes provided with the dataset to drive GPAvatar. We evaluate both methods at $512 \times 512$ pixels resolution on the same set of 12 hold-out persons, sampling in total 1000 different target expressions from different viewpoints. For the video-based metric JOD, we render one video of 100 frames from the frontal camera for each hold-out person. The quantitative results are listed in~\cref{tab:4_results}. It can be seen that our method clearly outperforms the baseline.
We observe the biggest difference in metrics measuring alignment (PSNR, SSIM), temporal consistency (JOD), and identity similarity (CSIM). This indicates that {\OURS} produces 3D head avatars that better resemble the person in the input images and produces smoother video renderings since it does not require a screen-space super-resolution network as GPAvatar. We further show qualitative comparisons in~\cref{fig:4_few_shot_comparison}. Our method produces more expressive avatars with higher rendering quality than the baseline and even performs well on a person with colored hair who wears a headband.  \\
To further assess Avat3r's generalization capabilities, 
we create few-shot 3D head avatars for subjects from the multi-view video dataset  NeRSemble~\cite{kirschstein2023nersemble} which has not been used during training. The results can be seen in~\cref{fig:04_nersemble_results}. Note that the images in the NeRSemble dataset exhibit entirely different characteristics in terms of lighting, camera parameters, and viewpoints, compared to the training images from the Ava256 dataset. Nevertheless, {\OURS} generates realistic 3D head avatars in this setting as well.

\input{figures/4_single_shot_comparison}
\subsection{Single-shot 3D Head Avatar Creation}
To make Avat3r amenable for inference on only a single input image, we make use of a pre-trained 3D GAN~\cite{yuan2023goae} to first lift the single image to 3D and then render four views of the head. These renderings then constitute the input for Avat3r. We conduct comparisons with the recent 3D-aware portrait animation method GAGAvatar~\cite{chu2024gagavatar}. Specifically, we compare with two version of GAGAvatar: One provided by the authors which is trained on VFHQ~\cite{xie2022vfhq}, and another version, denoted as GAGAvatar\textsuperscript{\textdagger}, that we train on the Ava256 dataset in the same setting as our method. To drive GAGAvatar, we use their monocular FLAME tracker to obtain tracked meshes.
\cref{fig:4_single_shot_comparison} shows qualitative comparisons between our method and GAGAvatar for single input images of hold out persons.
Note that our method performs competitively compared to the single-input baselines despite never being trained for a single-shot scenario. We find that {\OURS} produces more realistic facial expressions than GAGAvatar which is limited by FLAME's expression space. Furthermore, our method allows much more extreme viewpoint changes without sacrificing rendering quality.

\input{tables/4_ablations}
\input{figures/4_ablation}
\subsection{Ablations}

In the following, we study the efficacy of our design choices for Avat3r. In particular, we look at the usefulness of DUSt3R's position maps, the Sapiens feature maps, and using input images with different expressions during training. Our findings are summarized in~\cref{tab:4_ablations} and~\cref{fig:4_ablation}.

\paragraph{Effect of DUSt3R position maps.} Removing DUSt3R from our pipeline mostly impairs geometric fidelity. In~\cref{fig:4_ablation}, it can be seen that {\OURS} without DUSt3R struggles with aligning the Gaussian predictions from the four input images.

\paragraph{Effect of Sapiens feature maps.} Sapiens provides rich semantic information for the input images that mostly help in producing sharper predictions, e.g., in the hair area.

\paragraph{Effect of training with inconsistent input images.} A model that is trained on only multi-view consistent input images produces slightly sharper images. However, any inconsistencies in the input images, that can easily happen in practice, carry over to the generated 3D representation causing unwanted artifacts.



\input{figures/4_applications}

\subsection{Applications}


Finally, we show some applications of Avat3r in~\cref{fig:4_applications}. We study three use-cases: (i) Generating a novel 3D head with GGHead~\cite{kirschstein2024gghead} and then animating it with {\OURS}, (ii) Generating a portrait image with a text-to-image diffusion model~\cite{dai2023emu}, lifting it with a 3D GAN~\cite{yuan2023goae}, and animating it with {\OURS}, and (iii) Lifting an existing photograph with a 3D GAN~\cite{yuan2023goae} and animating it with {\OURS}. The results demonstrate that, despite being designed for a few-shot scenario and only being trained on real studio data, Avat3r can be used in a variety of casual situations. For example, our findings for case (i) suggest that Avat3r may be used to animate images from any 3D GAN that generates 3D heads, effectively making any purely static 3D generation model animatable.

\subsection{Limitations}

Despite {\OURS}'s great ability to create 3D head avatars from a few or even just a single input image, several limitations remain. For example, the current pipeline for inferring avatars from a single image requires a 3D GAN for 3D lifting which accumulates errors in the process in two ways: First, the 3D GAN won't be able to perfectly explain the person, inevitably loosing some details, and second, existing inversion frameworks employ NeRF-based 3D GANs that rely on screen-space super-resolution after rendering which introduces view-inconsistencies to the final images. Both aspects hamper the quality of the final avatar. However, these issues will decrease as newer and better 3D GANs become available that do not require screen-space rendering anymore~\cite{kirschstein2024gghead, Trevithick2023wysiwyg}. 
Another limitation of our framework is the requirement of camera poses during inference. While reasonable estimates for face images can be obtained by head tracking algorithms~\cite{MICA:ECCV2022}, incorrect camera estimates still remain a potential source of error for real use-cases. Possible remedies may include teaching the network to deal with incorrect camera estimates already during training, or dropping the requirement for camera poses alltogether~\cite{wang2023pflrm}.
Finally, our current pipeline does not provide control over the lighting and instead bakes in light effects from the input images. This limits applications that put avatars into novel environments since the generated head will look out of place. Adressing this issue by explicitly disentangling the lighting properties is an exciting avenue for future research.  

