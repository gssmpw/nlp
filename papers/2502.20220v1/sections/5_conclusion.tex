\section{Conclusion}

We have presented {\OURS}, a system for directly regressing an animatable 3D head avatar from just four input images. Our pipeline combines recent advancements in large reconstruction models with the powerful foundation models DUSt3R and Sapiens to obtain high quality results, and models dynamics with simple cross attention layers. Moreover, we demonstrate that {\OURS} can also be employed in a single image scenario by using a pre-trained 3D GAN. Our experiments show that {\OURS} learns a powerful prior over 3D facial movement that even generalizes to out-of-distribution examples like pictures of antique busts or AI-generated images. We believe that our findings open up interesting avenues for future research. For example, we have shown that LRM-like architectures have the ability to infer 3D representations even from inconsistent input images such as frames of a monocular video. This opens up the possibility to train models on large-scale monocular video datasets with a much larger variety of identities. Finally, the underlying architecture could also be used as a 3D lifting and denoising module in a diffusion framework~\cite{anciukevivcius2023renderdiffusion, chan2023genvs}, enabling training of diffusion models on monocular video data for unconditional 3D head avatar generation.
