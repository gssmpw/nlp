\clearpage
\setcounter{page}{1}
\setcounter{table}{2}
\setcounter{figure}{7}
\maketitlesupplementary

\appendix




\section{Analysis of Data Efficiency}



In~\cref{fig:4_data_efficiency} we show how our model scales with the number of training subjects and input views. For the latter, we disable DUSt3R as it produces less reliable position maps for 2 input views, and cannot be executed at all for 1 input view. We see a clear improvement when using more training subjects as well as using more input views. However, further scaling the number of input views also has drawbacks, as it drastically increases runtime due to dense attention inside the transformer and the increased number of Gaussians that have to be rendered. 
We qualitatively analyze the effect of using more train subjects in~\cref{fig:x_ablation_num_subjects} and the effect of the number of input views in~\cref{fig:x_ablation_num_input_views}.

\input{figures/4_data_efficiency}
\input{figures/x_ablation_num_subjects}
\input{figures/x_ablation_num_input_views}



\section{Effect of Skip Connections}

We analyze the effect of the proposed skip connections, i.e., omitting Eq.~(11), Eq.~(12), or both of the main paper. The results are listed in~\cref{tab:x_skip_connection_ablations}. We observe a noticeable hit in performance when either skip connection is removed. Furthermore, we qualitatively analyze the effect of skip connections in~\cref{fig:x_ablation_skip_connections}.

\input{tables/x_skip_connection_ablations}
\input{figures/x_skip_connection_ablation}

\section{Training Details}

\paragraph{Dataset processing.}
We use the 4TB version of the Ava256 dataset~\cite{ava256} which contains 256 persons, 80 cameras, and roughly 5000 frames per person that are sampled at 7.5 fps. We compute foreground segmentation masks with BackgroundMattingV2~\cite{BGMv2} and replace the background in all images with black pixels. We use the provided tracked mesh to find a $512 \times 512$ head-centered square crop for input images and $667 \times 667$ head-centered square crop for supervision views. This ensures that the pixels in the input images are used efficiently to show as much as possible of the head, leading to more 3D Gaussians. The reason for also cropping the target images is to remove parts of the torso, as it is not the focus of this work.
\paragraph{DUSt3R and Sapiens.}
Since both DUSt3R~\cite{wang2024dust3r} and Sapiens~\cite{khirodkar2025sapiens} are expensive foundation models, we pre-compute the position and feature maps for the input frames.
For Dust3r, it is prohibitive to pre-compute all possible combinations of 4 input views out of the available 80 cameras. Instead, we choose 3 "reasonable partner views" for each input and only store the position map for that viewpoint. This assigns each input view exactly one position map, which is conceptually wrong since the position map from DUSt3R should depend on the other 3 selected views. Nevertheless, we did not observe any disadvantages from this simplification strategy.
\paragraph{Head-centric coordinates.}
We further simplify the task by factoring the head poses from the provided tracked mesh into the camera poses instead of letting the network predict them. That way, our model can always predict the head in canonical pose, making the task easier. This is possible because modeling the torso, which in head-centric coordinates moves a lot when the person shakes their head, is not the focus of this work. 
\paragraph{k-farthest viewpoint sampling.}
To ensure that the 4 input images always follow a reasonable viewpoint distribution, we employ k-farthest viewpoint sampling. Specifically, we first start from a random camera and collect a set of 10 candidate cameras that are evenly spread out using farthest point sampling. From this candidate set, we then randomly select 4 cameras as input. This two-stage approach ensures that the input cameras are sufficiently random during training but also reasonably spread out to avoid seeing a person only from one side. During sampling input viewpoints, we exclude cameras that only observe the person from the back since those are not realistic inputs during test-time.
\paragraph{Input timestep sampling.}
To improve robustness of our model, we sample different timesteps for each of the 4 input images. This ensures that the model can deal with inconsistencies in the input. To maximize the diversity in the input expressions, we uniformly sample 10 timesteps in the segments:
\verb+EXP_eye_wide+, \verb+EXP_tongue001+, and \verb+EXP_jaw003+ from the recordings of the Ava256 dataset. This covers the most extreme facial expressions while avoiding having to pre-compute DUSt3R and Sapiens maps for every single image in the dataset.

\paragraph{Speed.}
To speed-up training, we employ the 3D Gaussian Splatting performance improvements of DISTWAR~\cite{durvasula2023distwar}. \\

\section{Hyperparameters}

In~\cref{tab:x_hyperparameters}, we list the most important hyperparameters for training Avat3r.

\input{tables/x_hyperparameters}

