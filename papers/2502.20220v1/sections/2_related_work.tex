\section{Related Work}

\subsection{Large 3D Reconstruction Models}
Recently, generalized feed-forward models for 3D reconstruction from sparse input views have garnered considerable attention due to their applicability in heavily under-constrained scenarios. The Large Reconstruction Model (LRM)~\cite{hong2023lrm} uses a transformer-based encoder-decoder pipeline to infer a NeRF reconstruction from just a single image. Newer iterations have shifted the focus towards generating 3D Gaussian representations from four input images~\cite{tang2025lgm, xu2024grm, zhang2025gslrm, charatan2024pixelsplat, chen2025mvsplat, liu2025mvsgaussian}, showing remarkable novel view synthesis results. The paradigm of transformer-based sparse 3D reconstruction has also successfully been applied to lifting monocular videos to 4D~\cite{ren2024l4gm}. \\
Yet, none of the existing works in the domain have studied the use-case of inferring \textit{animatable} 3D representations from sparse input images, which is the focus of our work. To this end, we build on top of the Large Gaussian Reconstruction Model (GRM)~\cite{xu2024grm}.

\subsection{3D-aware Portrait Animation}
A different line of work focuses on animating portraits in a 3D-aware manner.
MegaPortraits~\cite{drobyshev2022megaportraits} builds a 3D Volume given a source and driving image, and renders the animated source actor via orthographic projection with subsequent 2D neural rendering.
3D morphable models (3DMMs)~\cite{blanz19993dmm} are extensively used to obtain more interpretable control over the portrait animation. For example, StyleRig~\cite{tewari2020stylerig} demonstrates how a 3DMM can be used to control the data generated from a pre-trained StyleGAN~\cite{karras2019stylegan} network. ROME~\cite{khakhulin2022rome} predicts vertex offsets and texture of a FLAME~\cite{li2017flame} mesh from the input image.
A TriPlane representation is inferred and animated via FLAME~\cite{li2017flame} in multiple methods like Portrait4D~\cite{deng2024portrait4d}, Portrait4D-v2~\cite{deng2024portrait4dv2}, and GPAvatar~\cite{chu2024gpavatar}.
Others, such as VOODOO 3D~\cite{tran2024voodoo3d} and VOODOO XP~\cite{tran2024voodooxp}, learn their own expression encoder to drive the source person in a more detailed manner. \\
All of the aforementioned methods require nothing more than a single image of a person to animate it. This allows them to train on large monocular video datasets to infer a very generic motion prior that even translates to paintings or cartoon characters. However, due to their task formulation, these methods mostly focus on image synthesis from a frontal camera, often trading 3D consistency for better image quality by using 2D screen-space neural renderers. In contrast, our work aims to produce a truthful and complete 3D avatar representation from the input images that can be viewed from any angle.  

\subsection{Photo-realistic 3D Face Models}
The increasing availability of large-scale multi-view face datasets~\cite{kirschstein2023nersemble, ava256, pan2024renderme360, yang2020facescape} has enabled building photo-realistic 3D face models that learn a detailed prior over both geometry and appearance of human faces. HeadNeRF~\cite{hong2022headnerf} conditions a Neural Radiance Field (NeRF)~\cite{mildenhall2021nerf} on identity, expression, albedo, and illumination codes. VRMM~\cite{yang2024vrmm} builds a high-quality and relightable 3D face model using volumetric primitives~\cite{lombardi2021mvp}. One2Avatar~\cite{yu2024one2avatar} extends a 3DMM by anchoring a radiance field to its surface. More recently, GPHM~\cite{xu2025gphm} and HeadGAP~\cite{zheng2024headgap} have adopted 3D Gaussians to build a photo-realistic 3D face model. \\
Photo-realistic 3D face models learn a powerful prior over human facial appearance and geometry, which can be fitted to a single or multiple images of a person, effectively inferring a 3D head avatar. However, the fitting procedure itself is non-trivial and often requires expensive test-time optimization, impeding casual use-cases on consumer-grade devices. While this limitation may be circumvented by learning a generalized encoder that maps images into the 3D face model's latent space, another fundamental limitation remains. Even with more multi-view face datasets being published, the number of available training subjects rarely exceeds the thousands, making it hard to truly learn the full distibution of human facial appearance. Instead, our approach avoids generalizing over the identity axis by conditioning on some images of a person, and only generalizes over the expression axis for which plenty of data is available. 

A similar motivation has inspired recent work on codec avatars where a generalized network infers an animatable 3D representation given a registered mesh of a person~\cite{cao2022authentic, li2024uravatar}.
The resulting avatars exhibit excellent quality at the cost of several minutes of video capture per subject and expensive test-time optimization.
For example, URAvatar~\cite{li2024uravatar} finetunes their network on the given video recording for 3 hours on 8 A100 GPUs, making inference on consumer-grade devices impossible. In contrast, our approach directly regresses the final 3D head avatar from just four input images without the need for expensive test-time fine-tuning.

