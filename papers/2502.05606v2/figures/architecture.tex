\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=\textwidth, height=9cm]{figures/architecture_img.pdf}}   
\vspace{-3mm}
\caption{\textbf{Overview of our method at the blending stage. }
% condition
Two input images or concepts are encoded into embeddings, mapped to a shared text space via the Linear Prior Converter from unCLIP~\citep{ramesh2022hierarchical}. These embeddings condition the U-Net: one for downsampling, the other for upsampling.
% module
During the blending stage, a blending latent $L_b$ initialized with Gaussian noise is processed in the Feedback Interpolation Module, conditioned on image embeddings. Noise $\epsilon$ is added to the embeddings to generate initial auxiliary latents, which are interpolated into $L^{(t)}_{b}$ with an increasing weight $p$. The  $L^{(t)}_{a}$ is combined with interpolated latent $L'^{(t)}_{b}$ by proportion $p$. All updated $L'^{(t)}_{a}$ are refined in the auxiliary inference to retain original features using the text prompt for corresponding categories, and $L'^{(t)}_{b}$ is denoised via the blending inference.
% refinement
Finally, the refined $ L_b $ is passed into the VAE decoder to generate the final blending image. 
}
\label{architecture}
\end{center}
% \vskip -0.4in
\vspace{-8mm}
\end{figure*}
