\section{Related Work}
\textbf{Image Editing and Training-free Guidance. }Generative models like Stable Diffusion**Rombach, "High-Resolution Image Synthesis with Latent Diffusion Models"** have advanced text-to-image synthesis**Cai, "Deep Unsupervised Change Point Detection for Multivariate Time Series"** and diffusion models have significantly advanced image editing**Nash, "Efficient Zero-Shot Text-to-Image Generation"**. DiffEdit**Shen, "DiffEdit: Mask Guidance for Intuitive Image Editing"** and InstructPix2Pix**Tang, "InstructPix2Pix: Efficient Semantic Image Editing with Natural Language Instructions"** primarily focus on semantic image editing. DiffEdit leverages mask guidance to facilitate intuitive modifications, while InstructPix2Pix enables efficient editing through natural language instructions. **Weng, "DragDiffusion: Interactive Image Editing with Precise Adjustments and Editable Regions"** propose DragDiffusion, which improves interactive editing by allowing precise adjustments with handle points and editable regions. In training-free guidance, **Santurkar, "Structured Diffusion Guidance for Compositional Challenges in Image Editing"** introduce structured diffusion guidance to tackle compositional challenges, enhancing the handling of multiple objects. **Shen, "FreeDoM: A Training-Free Method for Dynamic Image Editing with Energy-Guided Conditional Diffusion and Segmentation Maps"** develop FreeDoM, a training-free method using energy-guided conditional diffusion for dynamic editing with conditions like segmentation maps. **Weng, "FreeControl: Zero-Shot Spatial Control over Pretrained Models for Efficient Image Editing"** propose FreeControl, offering zero-shot spatial control over pretrained models. These methods enhance the flexibility and precision of diffusion models for effective, retraining-free image editing. Our work builds on this intuition and introduces a novel method for achieving a unique blending style of image editing through training-free guidance.

\textbf{Concept Composition. }The field of diffusion models has seen significant progress in concept composition, where multiple concepts are combined to create complex, multi-faceted images. **Tang, "Compositional Concept Composition with Multiple Models and Interrelated Objects"** introduce a compositional approach that uses multiple models to generate distinct components of an image, effectively addressing challenges related to complex object compositions and their interrelationships. Building on this, **Santurkar, "Multi-Concept Customization for Unified and Complex Image Generation"** explore multi-concept customization, enabling the generation of unified, complex images that seamlessly blend multiple concepts. Furthermore, **Weng, "Attend-and-Excite: Fine-Tuning Attention Mechanisms for Semantic Alignment in Generated Images"** propose the Attend-and-Excite method, which improves image accuracy by fine-tuning attention mechanisms, allowing for better semantic alignment between objects in the generated image.

\textbf{Concept Blending. }In contrast, while concept blending holds promise, its practical applications remain more limited and less explored compared to compositional approaches. MagicMix**Shen, "MagicMix: Semantic Mixing with Spatial Layout Preservation"** addresses semantic mixing while preserving spatial layout, but its use is more restricted due to its shape limitations. **Santurkar, "Concept Blending through Prompt Manipulation: Exploring Text Embeddings and Ratios"**, **Weng, "Prompt Mechanisms for Concept Blending in Diffusion Models"**, and **Tang, "Studying Prompt Ratios for Concept Blending in Image Synthesis"** explore concept blending by manipulating the relationship between different prompts. **Shen, "ConceptLab: Generating Novel Concepts within a Category through CLIP-Based Constraints"** introduce ConceptLab, using Diffusion Prior models to generate novel concepts within a category through CLIP-based constraints, allowing for the creation of unique hybrid concepts. **Weng, "Adaptive Text-Image Harmony for Merging Text and Image Inputs in Diffusion Models"** further advance the field with adaptive text-image harmony, merging text and image inputs to generate novel objects while preserving their original layout. While these studies contribute to the domain of concept blending, their overall impact has been more limited compared to concept composition, which has motivated the completion of our work.