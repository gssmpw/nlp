\section{Related Work}
\textbf{Image Editing and Training-free Guidance. }Generative models like Stable Diffusion____ have advanced text-to-image synthesis____ and diffusion models have significantly advanced image editing____. DiffEdit____ and InstructPix2Pix____ primarily focus on semantic image editing. DiffEdit leverages mask guidance to facilitate intuitive modifications, while InstructPix2Pix enables efficient editing through natural language instructions. ____ propose DragDiffusion, which improves interactive editing by allowing precise adjustments with handle points and editable regions. In training-free guidance, ____ introduce structured diffusion guidance to tackle compositional challenges, enhancing the handling of multiple objects. ____ develop FreeDoM, a training-free method using energy-guided conditional diffusion for dynamic editing with conditions like segmentation maps. ____ propose FreeControl, offering zero-shot spatial control over pretrained models. These methods enhance the flexibility and precision of diffusion models for effective, retraining-free image editing. Our work builds on this intuition and introduces a novel method for achieving a unique blending style of image editing through training-free guidance.

\textbf{Concept Composition. }The field of diffusion models has seen significant progress in concept composition, where multiple concepts are combined to create complex, multi-faceted images. ____ introduce a compositional approach that uses multiple models to generate distinct components of an image, effectively addressing challenges related to complex object compositions and their interrelationships. Building on this, ____ explore multi-concept customization, enabling the generation of unified, complex images that seamlessly blend multiple concepts. Furthermore, ____ propose the Attend-and-Excite method, which improves image accuracy by fine-tuning attention mechanisms, allowing for better semantic alignment between objects in the generated image. 
% ____ introduce Add-it, a training-free method that facilitates object insertion into images while preserving the coherence of the scene, further enhancing the compositional capabilities of diffusion models.

\textbf{Concept Blending. }In contrast, while concept blending holds promise, its practical applications remain more limited and less explored compared to compositional approaches. MagicMix____ addresses semantic mixing while preserving spatial layout, but its use is more restricted due to its shape limitations. ____ and ____ explore concept blending by manipulating the relationship between different prompts, with ____ studying prompt ratios and ____ testing various mechanisms for blending text embeddings. ____ introduce ConceptLab, using Diffusion Prior models to generate novel concepts within a category through CLIP-based constraints, allowing for the creation of unique hybrid concepts. ____ further advance the field with adaptive text-image harmony, merging text and image inputs to generate novel objects while preserving their original layout. While these studies contribute to the domain of concept blending, their overall impact has been more limited compared to concept composition, which has motivated the completion of our work.