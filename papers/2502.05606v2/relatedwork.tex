\section{Related Work}
\textbf{Image Editing and Training-free Guidance. }Generative models like Stable Diffusion~\citep{rombach2022high} have advanced text-to-image synthesis~\citep{podellsdxl, zhang2023adding, gu2022vector} and diffusion models have significantly advanced image editing~\citep{balaji2022ediff, couairon2022diffedit, brooks2023instructpix2pix}. DiffEdit~\citep{couairon2022diffedit} and InstructPix2Pix~\citep{brooks2023instructpix2pix} primarily focus on semantic image editing. DiffEdit leverages mask guidance to facilitate intuitive modifications, while InstructPix2Pix enables efficient editing through natural language instructions. \citet{shi2024dragdiffusion} propose DragDiffusion, which improves interactive editing by allowing precise adjustments with handle points and editable regions. In training-free guidance, \citet{feng2022training} introduce structured diffusion guidance to tackle compositional challenges, enhancing the handling of multiple objects. \citet{yu2023freedom} develop FreeDoM, a training-free method using energy-guided conditional diffusion for dynamic editing with conditions like segmentation maps. \citet{mo2024freecontrol} propose FreeControl, offering zero-shot spatial control over pretrained models. These methods enhance the flexibility and precision of diffusion models for effective, retraining-free image editing. Our work builds on this intuition and introduces a novel method for achieving a unique blending style of image editing through training-free guidance.

\textbf{Concept Composition. }The field of diffusion models has seen significant progress in concept composition, where multiple concepts are combined to create complex, multi-faceted images. \citet{liu2022compositional} introduce a compositional approach that uses multiple models to generate distinct components of an image, effectively addressing challenges related to complex object compositions and their interrelationships. Building on this, \citet{kumari2023multi} explore multi-concept customization, enabling the generation of unified, complex images that seamlessly blend multiple concepts. Furthermore, \citet{chefer2023attend} propose the Attend-and-Excite method, which improves image accuracy by fine-tuning attention mechanisms, allowing for better semantic alignment between objects in the generated image. 
% \citet{tewel2024add} introduce Add-it, a training-free method that facilitates object insertion into images while preserving the coherence of the scene, further enhancing the compositional capabilities of diffusion models.

\textbf{Concept Blending. }In contrast, while concept blending holds promise, its practical applications remain more limited and less explored compared to compositional approaches. MagicMix~\citep{liew2022magicmix} addresses semantic mixing while preserving spatial layout, but its use is more restricted due to its shape limitations. \citet{melzi2023does} and \citet{olearo2024blend} explore concept blending by manipulating the relationship between different prompts, with \citet{melzi2023does} studying prompt ratios and \citet{olearo2024blend} testing various mechanisms for blending text embeddings. \citet{richardson2024conceptlab} introduce ConceptLab, using Diffusion Prior models to generate novel concepts within a category through CLIP-based constraints, allowing for the creation of unique hybrid concepts. \citet{xiong2024novel} further advance the field with adaptive text-image harmony, merging text and image inputs to generate novel objects while preserving their original layout. While these studies contribute to the domain of concept blending, their overall impact has been more limited compared to concept composition, which has motivated the completion of our work.