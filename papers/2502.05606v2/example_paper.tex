%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

% !TEX program = pdflatex
\documentclass[pdftex]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[table,xcdraw]{xcolor}  
% \usepackage{algorithm}
% \usepackage{algpseudocode}
% \usepackage{algorithm}
% \usepackage{amsmath}   
\usepackage{amssymb}
% \usepackage{multicol}
% \usepackage{svg}
\usepackage{float}
\usepackage{caption}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{diagbox} 

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion}
\definecolor{blue-violet}{rgb}{0.54, 0.17, 0.89}
\newcommand{\shy}[1]{\textcolor{blue-violet}{#1}}
\definecolor{black-green}{rgb}{0.54, 0.17, 0.89}
\newcommand{\wiser}[1]{\textcolor{green}{#1}}
\begin{document}
\twocolumn[
\icmltitle{\texorpdfstring{FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion}
{FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion}}
\icmlsetsymbol{equal}{*}
\begin{icmlauthorlist}
\icmlauthor{Yufan Zhou*}{h}
\icmlauthor{Haoyu Shen*}{u}
\icmlauthor{Huan Wang}{w}
\end{icmlauthorlist}
\icmlaffiliation{h}{Harbin Institute of Technology}
\icmlaffiliation{u}{University of Science and Technology of China}
\icmlaffiliation{w}{Westlake University}
\icmlcorrespondingauthor{Huan Wang}{wanghuan@westlake.edu.cn}
\begin{center}
\texttt{\url{http://petershen-csworld.github.io/FreeBlend}}
\end{center}
\icmlkeywords{Machine Learning, ICML}
\vskip 0.3in
\input{figures/teaser} 
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%%%%%
\begin{abstract}
Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.
% code
\end{abstract}

\section{Introduction}

The development of diffusion models~\citep{ho2020denoising, ho2022cascaded} has significantly advanced the field of image generation~\citep{rombach2022high, esser2023structure,brooks2023instructpix2pix,guo2023animatediff}. These models have exhibited impressive abilities of producing novel and appealing images that align with specific instructions, showcasing their potential and flexibility for controlled image synthesis.

Concept blending, as a novel task in the application of diffusion models, involves blending distinct concepts to create a new one that retains the defining characteristics of its components~\citep{fauconnier1998conceptual, ritchie2004lost}. It can be viewed as a way of integrating elements from different domains (e.g., cats and dogs) into a novel and meaningful output and creating new objects, scenes, or alterations that are coherent and creatively synthesized. Although current models can generate realistic combinations of individual concepts~\citep{liu2022compositional, chefer2023attend}, more sophisticated concept blending techniques are needed to ensure that the generated images not only blend features but also maintain their logical consistency and visual appeal.

Early methods of concept blending rely on simple vector weighting~\citep{melzi2023does} or hand-crafted feature combinations~\citep{olearo2024blend}, which involve adding or reassembling text embeddings. These approaches often lead to inaccurate representations and lack correspondence between visual and semantic features due to cross-modal discrepancies. MagicMix~\citep{liew2022magicmix} interpolates the original class latent into another latent space corresponding to a text prompt. While it creatively introduces latent interpolation for mixing, it struggles with shape mismatch issues and lacks flexibility in visual transformations. ConceptLab~\citep{richardson2024conceptlab} utilizes VLMs and latent space manipulation, but the constraints imposed during training the additional module limit the flexibility of its application. ATIH~\citep{xiong2024novel}, based on MagicMix, injects trainable parameters into the denoising process and enforces similarity constraints to harmonize the fusing of texts and images. However, the limitations of its model structure, similar to those of MagicMix, combined with its inability to address mismatched shapes or semantically irrelevant features, hinder its overall blending performance.

In this paper, we introduce \textit{FreeBlend}, a novel training-free approach to concept blending. At its core, FreeBlend employs a feedback-driven mechanism with latent interpolation to steer the diffusion denoising process. By interpolating auxiliary latents into the initial blending latent and iterating through feedback-driven interactions, the influence of the auxiliary latents gradually diminishes, guiding the blending latent towards the generation of the final image. This integration of blending and denoising processes leverages the strengths of multiple diffusion models to enhance concept blending performance. 

Specifically, FreeBlend consists of three core components: transferred unCLIP~\citep{ramesh2022hierarchical} image conditions for Stable Diffusion~\citep{rombach2022high}, a stepwise increasing interpolation strategy, and a feedback-driven mechanism of denoising process. Instead of using traditional text-based conditions, we employ images generated from text as conditions to guide the generation process via the unCLIP model. This approach provides more precise visual details compared to the vagueness of text-based conditions, reducing the uncertainty caused by cross-modal differences. In addition to this, we divide the denoising process into three stages: the initialization stage, the blending stage, and the refinement stage. At the initialization stage, the pretrained Stable Diffusion model starts with random noise sampled from a unit Normal distribution, which is then denoised under the guidance of unCLIP image condition. At the blending stage, we add noise to the auxiliary latents derived from the condition images to ensure to be in the same period as the blending latent. The blending and auxiliary processes are all denoised simultaneously. In the final refinement stage, only the unCLIP image condition is used to provide additional information, enabling the model to generate images with greater clarity and finer details. 

During the blending stage, we set the proportion of the current timestep relative to the total denoising time and use it to adjust the contribution of the auxiliary latents to a blending latent. As denoising progresses, this coefficient decreases, gradually reducing the influence of the auxiliary latents while focusing more on the blending latents, allowing more intricate details to emerge. The feedback-driven mechanism applies this coefficient uniformly, regulating the influence of the auxiliary latents on the blending latents. During the early blending stage, the auxiliary latents enrich the blending latents with information from the original class, and later, they add subtle details without disrupting the overall structure of the image. In addition to qualitative evaluation, we further incorporate quantitative metrics to rigorously assess the generation process, providing a more comprehensive analysis of the model's performance on the datasets we have generated.

The main contributions of this paper are three-fold:
\begin{itemize}
    \item We introduce a novel feedback-driven latent interpolation diffusion model  to tackle the concept blending problem. To the best of our knowledge, this is the \textit{first} training-free method specifically designed for exhaustive concept blending.
    \item Our approach incorporates unCLIP to use images as conditions, along with a stepwise increasing interpolation strategy and a feedback-driven denoising process to effectively blend different concepts.
    \item Extensive qualitative and quantitative experiments are conducted on several benchmarks, including our proposed blending effect evaluation metric, CLIP-BS, demonstrating that our method achieves state-of-the-art performance in generating blended concepts.
\end{itemize}


\section{Related Work}

\textbf{Image Editing and Training-free Guidance. }Generative models like Stable Diffusion~\citep{rombach2022high} have advanced text-to-image synthesis~\citep{podellsdxl, zhang2023adding, gu2022vector} and diffusion models have significantly advanced image editing~\citep{balaji2022ediff, couairon2022diffedit, brooks2023instructpix2pix}. DiffEdit~\citep{couairon2022diffedit} and InstructPix2Pix~\citep{brooks2023instructpix2pix} primarily focus on semantic image editing. DiffEdit leverages mask guidance to facilitate intuitive modifications, while InstructPix2Pix enables efficient editing through natural language instructions. \citet{shi2024dragdiffusion} propose DragDiffusion, which improves interactive editing by allowing precise adjustments with handle points and editable regions. In training-free guidance, \citet{feng2022training} introduce structured diffusion guidance to tackle compositional challenges, enhancing the handling of multiple objects. \citet{yu2023freedom} develop FreeDoM, a training-free method using energy-guided conditional diffusion for dynamic editing with conditions like segmentation maps. \citet{mo2024freecontrol} propose FreeControl, offering zero-shot spatial control over pretrained models. These methods enhance the flexibility and precision of diffusion models for effective, retraining-free image editing. Our work builds on this intuition and introduces a novel method for achieving a unique blending style of image editing through training-free guidance.

\textbf{Concept Composition. }The field of diffusion models has seen significant progress in concept composition, where multiple concepts are combined to create complex, multi-faceted images. \citet{liu2022compositional} introduce a compositional approach that uses multiple models to generate distinct components of an image, effectively addressing challenges related to complex object compositions and their interrelationships. Building on this, \citet{kumari2023multi} explore multi-concept customization, enabling the generation of unified, complex images that seamlessly blend multiple concepts. Furthermore, \citet{chefer2023attend} propose the Attend-and-Excite method, which improves image accuracy by fine-tuning attention mechanisms, allowing for better semantic alignment between objects in the generated image. 
% \citet{tewel2024add} introduce Add-it, a training-free method that facilitates object insertion into images while preserving the coherence of the scene, further enhancing the compositional capabilities of diffusion models.

\textbf{Concept Blending. }In contrast, while concept blending holds promise, its practical applications remain more limited and less explored compared to compositional approaches. MagicMix~\citep{liew2022magicmix} addresses semantic mixing while preserving spatial layout, but its use is more restricted due to its shape limitations. \citet{melzi2023does} and \citet{olearo2024blend} explore concept blending by manipulating the relationship between different prompts, with \citet{melzi2023does} studying prompt ratios and \citet{olearo2024blend} testing various mechanisms for blending text embeddings. \citet{richardson2024conceptlab} introduce ConceptLab, using Diffusion Prior models to generate novel concepts within a category through CLIP-based constraints, allowing for the creation of unique hybrid concepts. \citet{xiong2024novel} further advance the field with adaptive text-image harmony, merging text and image inputs to generate novel objects while preserving their original layout. While these studies contribute to the domain of concept blending, their overall impact has been more limited compared to concept composition, which has motivated the completion of our work.

\section{Methodology}

\subsection{Preliminaries}
% \textbf{Diffusion Sampling with Guidance. }The generation process using pre-trained text-to-image (T2I) diffusion models begins with an initial Gaussian noise latent, \( \mathbf{z}_T \)~\citep{ho2020denoising}. Generation proceeds iteratively, where at each timestep \( t \), the model produces a cleaner latent \( \mathbf{z}_{t-1} \) by subtracting a noise component \(\epsilon_t = \hat{\epsilon}_\theta(\mathbf{z}_t; t; \mathbf{c}) \), with \( \mathbf{c} \) representing given conditions like text prompts, images, or other measurable properties. 

The Stable Diffusion~\citep{rombach2022high} model efficiently operates in the latent space, distinguishing it from previous approaches~\citep{ho2020denoising, dhariwal2021diffusion}. Specifically, an autoencoder consisting of an encoder \( \mathcal{E} \) and a decoder \( \mathcal{D} \) is trained with a reconstruction objective. Given an image \( \mathbf{x} \), the encoder \( \mathcal{E} \) maps it to a latent \( \mathbf{z} \), and the decoder \( \mathcal{D} \) reconstructs the image from the latent, i.e., \( \tilde{\mathbf{x}} = \mathcal{D}(\mathbf{z}) = \mathcal{D}(\mathcal{E}(\mathbf{x})) \). This way, at each timestep \( t \), a noisy latent \( \mathbf{z}_t \) can be obtained. Beyond the routine training scheme, Stable Diffusion devises conditioning mechanisms to control the synthesized image content with $\mathbf{c}$ representing given additional conditions like text prompts, images, or other measurable properties. The DDPM~\citep{ho2020denoising} model \( \epsilon_\theta \) can then be trained via:
\begin{equation}
\mathcal{L} = \mathbb{E}_{\mathbf{z} \sim \mathcal{E}(\mathbf{x}), \mathbf{c}, \epsilon \sim \mathcal{N}(0, I), t} 
\left[ \left\| \epsilon - \hat{\epsilon}_\theta(\mathbf{z}_t, t, \mathbf{c}) \right\|_2^2 \right],
\end{equation}
where U-Net~\citep{ronneberger2015u} enhanced with self-attention and cross-attention layers is adopted as the denoiser. During training, given a noised latent \( \mathbf{z}_t \) at timestep \( t \) and condition $\mathbf{c}$, denoiser is tasked with predicting the noise \( \epsilon_t \) added to the current latent.

During inference, a latent \( \mathbf{z}_T \) is sampled from the standard normal distribution \( \mathcal{N}(0, I) \) and the DDPM is used to iteratively remove the noise \(\epsilon_t = \hat{\epsilon}_\theta(\mathbf{z}_t, t, \mathbf{c}) \) in \( \mathbf{z}_T \) to produce \( \mathbf{z}_0 \). This process is expressed as:
\begin{equation}
    \hat{\epsilon}_\theta(\mathbf{z}_t, t, \mathbf{c}) \approx -\sigma_t \nabla_{\mathbf{z}_t} \log p_t(\mathbf{z}_t | \mathbf{c}),
\end{equation}
where \( \hat{\epsilon}_\theta(\mathbf{z}_t, t, \mathbf{c}) \) represents the noise removal at timestep \( t \), and the gradient of the log marginal distribution provides the direction for noise reduction.
% end to get image
In the end, the latent \( \mathbf{z}_0 \) is passed to the decoder \( \mathcal{D} \) to generate the output image \( \tilde{\mathbf{x}} \).

\input{figures/architecture}

\subsection{unCLIP Image Embeddings for Condition}

Previous diffusion models rely on a variety of conditions~\citep{zhan2024conditional}, and traditional concept blending approaches typically use transformed text embeddings \citep{melzi2023does,olearo2024blend} as inputs to these models. In contrast, our approach utilizes the image embeddings as the sole input, leading to significant improvements in both quality and interpretability. Given that the unCLIP~\citep{ramesh2022hierarchical} model, which shares similarities with the DALL-E~\citep{ramesh2021zero} mechanism, allows images to serve as conditional inputs, we adopt it as the backbone model for our method.

In order to preprocess $\mathbf{c}$ from the given image modality, we introduce a \textit{Linear Prior Converter} $\tau_{\theta'}$ from unCLIP that maps images from image embeddings into the text embedding space for sampling condition. These embeddings are subsequently used as inputs to the intermediate layers of the U-Net through a cross-attention mechanism, which is defined as 
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right) \cdot V,
\end{equation}
where 
\begin{equation}
    \left\{
    \begin{array}{l}
    Q = W_Q \cdot \varphi(L^{(t)}_b) \\
    K = W_K \cdot \tau_{\theta'}(\mathbf{c}_{\text{stage}}) \\
    V = W_V \cdot \tau_{\theta'}(\mathbf{c}_{\text{stage}}), \\
    \end{array}
    \right.
\end{equation}
and $\mathbf{c}_{\text{stage}}$ is defined as:
\begin{equation}
    \mathbf{c}_{\text{stage}} = 
    \begin{cases}
    \mathbf{c}_{\text{up}} & \text{(for downsampling)} \\
    \mathbf{c}_{\text{down}} & \text{(for upsampling)} 
    \end{cases}.
\end{equation}
Here, $\varphi(L^{(t)}_b) \in \mathbb{R}^{N \times d_\epsilon}$ represents the intermediate (flattened) feature map produced by the U-Net that implements $\epsilon_\theta$, and $W_V \in \mathbb{R}^{d \times d_\epsilon}$, $W_Q \in \mathbb{R}^{d \times d_\tau}$, and $W_K \in \mathbb{R}^{d \times d_\tau}$ are learnable projection matrices~\citep{vaswani2017attention,jaegle2021perceiver}. $\mathbf{c}_{\text{up}}$ and $ \mathbf{c}_{\text{down}}$ are concept conditions of $\mathbf{c}_{\text{stage}}$, and they are all encoded from original images. 

% two sampling 
By utilizing the first embedding during the downsampling phase and the second embedding during upsampling, the model is able to seamlessly integrate hybrid features at each training epoch. This methodology mitigates the risk of sequentially introducing two distinct feature sets across epochs, a process that could lead to feature conflicts due to the inherent differences in their nature. Furthermore, a simple averaging of embeddings may result in the attenuation of critical information, such as fine details or spatial positioning. Leveraging the ability to interface with the text embedding space via unCLIP, the model not only captures the semantic essence of the concept but also affords precise control over visual attributes, including shape and color.
% See Fig. 3 for a visual depiction.

\subsection{Staged Denoising Process}

To achieve more flexible and precise control over image generation, motivated by \citet{liew2022magicmix, lin2024dreamsalon, ackermann2022high}, we divide the generation phase into three stages: the initialization stage, the blending stage, and the refinement stage. And we use two coefficients, $t_s$ and $t_e$, to denote the start and end timesteps of the blending stage, respectively.

At the initialization stage, the denoising process using Stable Diffusion begins with random noise $ \epsilon \sim \mathcal{N}(0, I) $. In this stage, the blending latent is denoised based on the input unCLIP image embeddings, which define the basic layout of the subject and background in the image. Starting with $ L^{(T)}_{b} = \epsilon $, the blending latent is iteratively updated according to the following:
\begin{equation}
\begin{aligned}
    L^{(t-1)}_{b} = & \epsilon_{\theta}(L^{(t)}_{b}, t, \varnothing) \\
    & + w \cdot \left[ \epsilon_{\theta}(L^{(t)}_{b}, t, \mathbf{c}_{\text{stage}}) - \epsilon_{\theta}(L_b^{(t)}, t, \varnothing) \right], 
\end{aligned}
\label{denoise_eq}
\end{equation}
where $w$ denotes the classifier-free guidance~\citep{ho2022classifier} scale, and $\epsilon_{\theta}(L^{(t)}_{b},t,\mathbf{c})$ is the pre-trained U-Net model with conditional input $\mathbf{c}$ and timestep $t$. 

At the blending stage, the initially formed latent is interpolated with auxiliary latents to incorporate original features. In this process, we employ a stepwise increasing interpolation strategy along with a feedback-driven mechanism to blend and filter out irrelevant features from the concepts.

At the refinement stage, the general appearance of $L_b$ has been completed. The focus now shifts to enhancing the finer details of the latent, making the result more natural and harmonious while eliminating any sense of disjointedness or artificial stitching. Using the same conditions as in the initialization stage, this phase continues to build upon the previous layout, further refining and improving it.

\subsection{Stepwise Increasing Interpolation Strategy}
In this section, we elaborate on how our method ensures semantic coherence. We interpolate two image latents with the current denoising latents to encode semantic information from both sources together, which shares similar intuition with MagicMix~\citep{liew2022magicmix}. The key challenge is to select appropriate blending ratio at each timestep. One issue of using a constant blending ratio is that we may result in blurring and unclear images, dramatically decreasing the generation quality. To address this, for a denoising process with \( T\) timesteps, at timestep \( t \), we apply a stepwise declining blending ratio $p$, which could be expressed as 
\begin{equation}
p = 1 - \frac{t}{T},
\end{equation}
where $N$ is the number of concepts needed to blend, and the proportion of the $k$-th auxiliary latent $L^{(k)}_a$ is 
\begin{equation}
    \lambda = \frac{1-p}{N}.
\end{equation}
Another challenge is the potential for biased or catastrophic forgetting of content. Specifically, the model should account for the varying influence of different concepts. For instance, when subject concepts such as ``animals'' are blended with background concepts like ``plants'', the model might give more weight to the subject concepts, resulting in generated images with a higher representation of animals. 

To address this, we introduce an additional hyper-parameter, denoted as $\gamma$, which regulates the contribution of each image embedding in the latent space. In the case of a blending task involving $N$ images, the interpolation process can be formulated as follows:
\begin{equation} 
L'^{(t)}_{b} = p \cdot L^{(t)}_{b}  + \lambda \cdot \sum_{n=1}^{N}  \gamma_n  \cdot L_a^{(t,n)},
\end{equation}
where $\gamma_n$ denotes the interpolation strength of $n$-th image, $L^{(t)}_b$ refers to the blending latent, and $L_a^{(t,n)}$ specifies the $n$-th auxiliary latent at the $t$-th timestep. 
% \input{figures/module}
\input{figures/compare_methods}
\subsection{Feedback-Driven Mechanism}
In terms of synthesis quality, setting the time proportion of the blending stage too high can lead to overlapping issues, particularly when the blending stage ends at a high value. This indicates that significant changes should not be introduced during the later stages of denoising, as this could cause unwanted fluctuations and overlapping effects. In previous methods~\citep{liew2022magicmix}, the blending stage ends earlier, which limits the ability for further blending and adjustment. To address this issue, we propose a feedback-driven mechanism.

Specifically, $L^{(t_{s},k)}_{a}$ is initialized using the initial auxiliary latent $L^{(0,k)}_{a}$ as follows:
\begin{equation}
    L^{(t_{s},k)}_{a} = \sqrt{\bar{\alpha}_t} L^{(0,k)}_{a} + \sqrt{1-\bar{\alpha}_t} \epsilon,
\end{equation}
where $L^{(0,k)}_{a}$ is the embedding encoded from image.

Once $L^{(t)}_b$ is interpolated with $L^{(t,k)}_a$, the latter should be simultaneously updated using $L'^{(t)}_b$ as follows:
\begin{equation}
\begin{aligned}
    L'^{(t,k)}_{a} &=  p \cdot L'^{(t)}_{b} + (1-p) \cdot L_a^{(t,k)} \\
    &= \underbrace{(1-p) \cdot L_a^{(t,k)}}_{\text{inherits the original appearance}} \\ 
    & + \underbrace{ p^2 \cdot L^{(t)}_{b} }_{\text{integrates into the subject}} \\
    & + \underbrace{ p \cdot (1-p) \cdot \left[ \frac{1}{N} \cdot  \sum_{n=1}^{N} \gamma_n \cdot L_a^{(t,n)}  \right] }_{\text{maintains balance}}.
\end{aligned}
\label{main_eq}
\end{equation}
After interpolation, $L_a$ should also be denoised as described in \Cref{denoise_eq}, with $\mathbf{c}_{\text{stage}}$ being replaced by $\mathbf{c}_{\text{up}}$ or $\mathbf{c}_{\text{down}}$ according to the category of it.


As shown in \Cref{main_eq}, as the timesteps decrease, $p$ increases, which causes $L_b$ to take up a larger proportion of $L_a$. Consequently, in the later stages of blending, the interpolated $L_a$ increasingly resembles $L_b$, thus maintaining consistency over time. The average of all previous $L_a$ values contributes to the incorporation of general features, particularly during the middle stages of blending, due to the coefficient $p \cdot (1-p)$. Regarding $L^{(t,k)}_a$, it gradually declines in the final stages, thereby reducing the proportion of the original features. Through this approach, the auxiliary latents are updated by incorporating both the preceding auxiliary latents and the main blending latent.

\input{tables/main_table}
\input{tables/ablation_condition}
\input{tables/ablation_interpolation}
\input{tables/ablation_three_stage}
\section{Experiments}
\subsection{Experimental Settings}

\textbf{Datasets Construction. }We develop the Concept Text-Image Blending (CTIB) dataset, which includes 11,400 text-image pairs. These pairs originate from a collection of 380 text pairs distributed across 20 distinct categories, with each category containing 30 images. The categories are thoughtfully chosen to include a diverse array of real-world objects, highlighting the model's capability to generate across various species and types. 

\textbf{Implementation Details. }Our approach is executed with SD-2-1\footnotemark[1] for all baseline comparisons. For our suggested technique, we employ SD-2-1-unCLIP\footnotemark[2], enabling image-based conditioning as an input. During the creation of image datasets, the prompt used is ``a photo of a \{class\}''. To maintain a uniform resolution throughout the experiments, all images are resized to $768\times768$ pixels. The experiments  are conducted on a node equipped with 4 NVIDIA GeForce RTX A100 GPUs.

\footnotetext[1]{\url{https://huggingface.co/stabilityai/stable-diffusion-2-1}}
\footnotetext[2]{\url{https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip}}

\textbf{Metrics. }Given the unique nature of our task compared to other generation tasks, we use four key metrics to evaluate the blending performance: (1) \textbf{CLIP Blending Similarity (CLIP-BS)} is the primary metric, which measures the distance and similarity between the blending results and the original concepts. (2) \textbf{DINO Blending Similarity (DINO-BS)}~\citep{liu2025grounding} assesses the detection score of blending objects or the combination of one object with features from another. (3) \textbf{CLIP Image Quality Assessment (CLIP-IQA)}~\citep{wang2023exploring} evaluates the image quality and the degree of match for the blending objects. (4) \textbf{Human Preference Score (HPS)}~\citep{wu2023human} measures human preferences for the blending results based on blending object prompts. Further explanation is provided in \Cref{more_about_metrics}.

\subsection{Qualitative Comparisons}

The results of our comparison with three existing typical concept blending algorithms are illustrated in \Cref{compare_methods}. We observe that using interpolated text embeddings tends to favor one category over the other, resulting in a failure to effectively integrate both elements into the final outcome. On the other hand, while Composable Diffusion successfully introduces both categories, it often leads to concepts co-occurring rather than achieving a harmonious blend. This is evident in cases like ``car-cat'', where the concepts remain distinct rather than being seamlessly blended. MagicMix heavily depends on the shape similarity between the reference image and the corresponding category. As a result, it struggles when the categories are completely dissimilar, such as in the cases of ``dog-neon light'' and ``orange-robot''. In these instances, the method fails to generate a meaningful blend, as the lack of visual similarity between the categories hinders the blending process.  In contrast, our approach effectively merges both categories, producing well-integrated and balanced results. This enables smooth blending even when the categories are highly distinct, such as in ``car-cat'' or ``dog-neon light''.

\subsection{Quantitative Comparisons}

We evaluate the performance on our CTIB dataset using four related metrics, with the results presented in \Cref{main_table}. The results demonstrate that our method significantly outperforms all other approaches across all metrics. This indicates that the quality and the blending effect of our synthetic images achieved by our method is the most visually pleasing. 

\subsection{Ablation Study}
\input{figures/gamma}
\input{figures/user_study}
\textbf{Conditions for Denoising Process. }\Cref{ablation1} presents the results of our ablation study under different conditions. We classify the conditions into two main categories: text and image conditions. For text conditions, we use the TEXTUAL and UNET text prompt blending methods. For image conditions, we apply the unCLIP method along with either unCLIP-T (TEXTUAL-like) or unCLIP-U (UNET-like) image blending methods, where this approach treats the images as text conditions. The results demonstrate that the unCLIP-U method yields the best performance compared to other conditions, while text-based conditions tend to reduce the effectiveness of the generation.

\textbf{Interpolation Strategy. }\Cref{ablation2} illustrates the impact of different interpolation strategies. In the blending stage, we vary the blending ratio between the interpolated auxiliary latents and the blending latent. From this table, we observe that the ``increase'' strategy yields a higher CLIP-BS and CLIP-IQA score. As $p$ increases, the blending latent gains more weight relative to the auxiliary latents, resulting in the final output exhibiting features of both kinds. However, the ``invariant'' method leads to a rigid adjustment, yielding the lowest performance. On the other hand, the ``decline'' strategy prioritizes the auxiliary latent space and avoids blending, which clearly results in ineffective blending.

\textbf{Mitigation of bias. }We introduce an additional parameter, $\gamma$, to reduce the bias toward specific categories in the generated output image. As shown in \Cref{gamma_images}, we vary $\gamma$ within the range $[0.5,1.5]$. Our observations show that as \( \gamma \) moves from 0 to 2, the generated image gradually transitions from resembling reference 1 to favoring reference 2. This provides us with the flexibility to adjust the interpolation strength, allowing for precise control over the image's characteristics and enabling the achievement of optimal results. 

\textbf{Staged Denoising Process. }We validate the impact of our proposed staged denoising process in \Cref{ablation_three_stages}. The results demonstrate that both the initialization and refinement stages enhance the stability and quality of the generation. This highlights the effectiveness of the initialization stage in shaping the structure of the initial latents, while the refinement stage successfully adds finer details to the images.


\subsection{User Study} 

We also conduct a user study to evaluate human preference on the blending results with images generated by different methods. Our survey comprised 6 sets of blending pairs. With the active participation of 50 volunteers, we successfully collected a total of 300 votes. The results, as presented in~\Cref{user_study}, clearly demonstrate the dominance of our method. It received 192 votes, accounting for 64.00\% of the total, far surpassing its competitors. Composable Diffusion obtained 63 votes (21.00\%), MagicMix received 39 votes (13.00\%), and Textual managed only 6 votes (2.00\%). These findings not only affirm the effectiveness of our approach but also provide valuable insights into user preferences in the context of concept blending.

\section{Conclusion}
Blending multiple concepts into a single object based on diffusion models is interesting and valuable in 2D content creation but rather challenging, as it involves controlling the diffusion model to generate previously completely unseen objects. In this paper, we introduce \textit{FreeBlend}, a \textit{training-free} method to blend concepts with stark inherent semantic dissimilarities. At its core, FreeBlend employs the unCLIP model with images as conditions and utilizes a novel proposed interpolation strategy with feedback. Extensive qualitative and quantitative studies demonstrate that our method effectively blends disparate concepts, significantly surpassing the prior SoTA counterparts. Moving forward, exploring alternative ways to address the preference dynamics between different concepts may further improve the performance.


\section*{Impact Statement}
This paper presents a contribution aimed at advancing the field of concept blending. We explicitly state that our work is intended for academic research and commercial applications, provided such uses are authorized by the author. However, we strongly emphasize that we do not condone, nor do we support, the use of our research to generate harmful, unethical, or unlawful content. The generation of unsafe or offensive images that violate ethical standards, legal regulations, or societal norms is strictly prohibited. As researchers, we believe it is our responsibility to ensure that our work is used for the betterment of society and in a manner that aligns with fundamental ethical principles.


% ethical aspects and future societal consequences

% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\section{Appendix}

\subsection{T-SNE Analysis}
\label{t_SNE_analysis_paragraph}
In this section, we explore the CLIP embedding space of blended images. Given the high dimensionality of the embeddings, we apply t-SNE~\citep{van2008visualizing} for dimensionality reduction and visualize the embeddings in 2D to illustrate their spatial relationships.
\input{figures/t_SNE}

In \Cref{sne_a}, it is evident that semantically related categories tend to be positioned closer to each other, while unrelated categories are spaced farther apart. In \Cref{sne_b}, we observe that the blending images exhibit a strong relationship with the original classes. Notably, these blending images are generally biased towards one of the original classes, which reflects the inherent bias in the concept blending task. Specifically, we notice that the blending images are not located at the midpoint between the original classes. We hypothesize that this observation can be attributed to the following factors: a) t-SNE is a non-linear dimensionality reduction technique that aims to map similar points in high-dimensional space to nearby points in low-dimensional space, while preserving local structures rather than global relationships. Consequently, the position of points in the t-SNE plot may not necessarily correspond to the relative position of categories in the original feature space. b) The feature vector of the blended class may not be positioned directly between the two original classes. This could be due to: i) The blended class containing a mixture of features from both original classes, with the resulting feature vector being closer to one of the original classes in the feature space. ii) The blended feature may have a direction in the feature space that is relatively distant from both original classes. In certain cases, this could cause the blended class's feature vector to extend further from the center, leading to its placement behind the original classes in the t-SNE plot, rather than being positioned between them.

\subsection{Multiple Concepts Blending. }
Our method is not limited to cases involving only two prompts. In \Cref{concept_multiple}, we present examples where three or more prompts are incorporated. We utilize the TEXTUAL method to combine three prompts, as the UNET method is limited to handling only two prompts due to the constraints of the U-Net architecture. The blending effect is somewhat chaotic and abstract due to the significant differences and wide conceptual gap among the three prompts. Nevertheless, we managed to achieve relevant results and curated a selection of visually appealing outcomes to present.
\input{figures/multiple_concepts}
\input{figures/feedback}
\input{figures/visualize_latent}
\input{figures/style_transfer}
\input{figures/user_study_screenshot}

\subsection{More Ablation Study}
\textbf{Feedback-Driven Mechanism. }We conduct an ablation study in \Cref{ablation_feedback} to validate the effect of the feedback-driven mechanism. The results demonstrate the impact of our feedback-driven mechanism in blending different concepts. The DINO-BS score, however, is not optimal, which may be due to the surprising overlap of blended objects, thus boosting the score. We also present an intuitive visual comparison in \Cref{feedback_images}, where our effect is clearly significant.
\input{tables/ablation_feedback}

\textbf{Visualization of Staged Denoising Process. }Here, we provide a more detailed description of the staged denoising process employed in our method. To facilitate understanding, we visualize the intermediate latent representations, including the blending latent \( L_b \) and the auxiliary latents \( L_a^{(t,n)} \) (\( n=0,1,2,...,N \)) at various stages of the process. The results are presented in \Cref{visualize_latent}. Specifically, we focus on timesteps 3 and 22, as they correspond to the initial and final stages where our method begins to exhibit its effects. These visualizations demonstrate how the latents evolve over time, gradually converging to a consistent structure. Through the feedback interpolation module, the blending latent and auxiliary latents seamlessly merge, ultimately resulting in a coherent output image.


%%%%%%%%%%%%%

\subsection{Details of Dataset Construction}

\input{tables/category}

The designed category list can be divided into four distinct groups based on their characteristics, as shown in \Cref{categories_table}. This categorization provides a clear organization of items based on their respective categories and reflects common concepts encountered in daily life.

\subsection{Style Transfer Task}
\Cref{style_transfer_images} illustrates the impressive visual performance of our method on the style transfer task. Given a reference image and a content image, our training-free approach effortlessly transfers the content into the style of the reference, demonstrating both the adaptability and compatibility of our method for this task.

\subsection{Details of Metrics}
\label{more_about_metrics}

We calculated the corresponding scores for each image within a category, then computed the average score for that category, and finally determined the overall average score for the entire dataset.

\textbf{CLIP-BS. }For class $A$ and class $B$, we calculate the CLIP blending score using the following equation:
\begin{equation}
    \text{Score}_{\text{CLIP-BS}} = \sum_{i=1}^{n} \left[ \text{Score}(\text{Image}_{\text{blending}}, \text{Prompt}_{i}) - \text{Score}(\text{Image}_{\text{original}_{i}}, \text{Prompt}_{i}) \right],
\end{equation}
where $\text{Score}(\cdot, \cdot)$ is the cosine similarity score computed by the CLIP model using the input image and prompt, $\text{Image}_{\text{blending}}$ represents the generated blended images, $\text{Image}_{\text{original}_{i}}$ denotes the original images of class $i$,  and $n$ is the number of concepts blended. $\text{Prompt}_{i}$ refers to the prompt for class $i$, formatted as ``a photo of a \{class\}'' to minimize noise in the image generation process. This metric can also be interpreted as the high-dimensional distance between the blended concepts and the original ones. It represents the total distance between the blended concept and each of the original concepts. A larger score generally indicates the dimensional center of the original concepts.

\textbf{DINO-BS~\citep{liu2025grounding}. }Grounding DINO is an open-set object detection model designed to improve the ability to interpret human language inputs through tight modality fusion and large-scale grounded pre-training. The model excels at detecting unseen objects and can perform effective zero-shot transfer learning, even for categories not encountered during training, by leveraging language prompts. To guide the model in detecting blended objects, we designed two specific prompts: ``a \{concept\_a\} with blending features from \{concept\_b\}'' and ``a \{concept\_b\} with blending features from \{concept\_a\}''. These prompts help the model appropriately recognize and identify the blended objects.

\textbf{CLIP-IQA~\citep{wang2023exploring}. }This metric employs a thoughtfully crafted prompt pairing strategy to reduce linguistic ambiguity and effectively harness CLIP's pretrained knowledge for assessing visual perception. In our task, we design the specific prompt pairs like (``mixed'', ``dull''), (``blending features from two different objects'', ``natural object from one object'').

\textbf{HPS~\citep{wu2023human}. }This metric more accurately predicts human preferences for generated images. The designed prompt is ``a photo of a blended object combining mixed features from \{concept\_a\} and \{concept\_b\}''.

\subsection{Baseline Methods}
In this section, we describe the baseline methods used in our study:
\begin{itemize}
    \item \textbf{MagicMix~\citep{liew2022magicmix}. }The original MagicMix method modifies the input image using a text prompt from a different class to transfer the image’s characteristics. In our implementation, we first generate the original image using a class text prompt with Stable Diffusion then pass it into the original MagicMix pipeline.
    \item \textbf{Composable Diffusion~\citep{liu2022compositional}. }This method decomposes the text description into components, each processed by a different encoder to produce a latent vector capturing its semantic information. Multiple diffusion models are then used to independently generate images for each component. The Conjunction operation combines these components into a single image. This approach excels in zero-shot compositional generation, allowing the model to create novel combinations \textit{but not blending} of objects and their relationships, even without prior training on such combinations.
    \item \textbf{unCLIP-T~\citep{wang2023exploring}. }The unCLIP model leverages images as conditioning inputs, replacing traditional text prompts, by training several linear layer adapters. In this implementation, image embeddings are treated similarly to text embeddings, with their average embedding being computed to serve as the conditions.
    \item \textbf{unCLIP-U~\citep{wang2023exploring}. }This implementation is akin to unCLIP-T, with the key difference being that the image embeddings are integrated using the UNET method.
    \item \textbf{TEXTUAL~\citep{melzi2023does}. }This method encodes the text prompts into text embeddings and computes their average embedding to input into Stable Diffusion for generating blended images. However, it is unstable within the embedding space and constrained by the semantic scope of the concepts involved.
    \item \textbf{UNET~\citep{olearo2024blend}. }This method uses the first text embedding for the conditions of downsampling of U-Net~\citep{ronneberger2015u} and the second one for the upsampling of the U-Net.
    
\end{itemize}

\subsection{User Study}

The screenshots in \Cref{fig:screenshot} display our questionnaires used to assess the validation of different methods. 
% The detailed results are provided in \Cref{user_study_details}.
% \input{tables/user_study_detailed.tex}

\subsection{More Analysis of Blending Results}

Blending is a multifaceted process that occurs across various domains, each exhibiting distinct characteristics and behaviors. In the study of embedding blending, it is evident that different categories impose constraints on the resulting blends. 

\subsubsection{Types of Blending Concept Pairs}

\textbf{Biological Blending.} This type of blending involves the blending of entities from the natural world. For example, when animals with similar characteristics, such as cats and dogs, are blended, the resulting output tends to exhibit a high degree of coherence and natural harmony. In contrast, blending animals with plants often leads to situations where the plant components are relegated to background elements, rather than being seamlessly integrated into the final composition. This phenomenon suggests that intra-category blending (e.g., animal-animal or plant-plant) typically yields more naturalistic and fluid results, whereas inter-category blending may result in less cohesive outputs.

\textbf{Decorative Object-Feature Blending.} This form of blending involves the decoration of objects with specific attributes or features. For instance, blending a car with a rose may yield an aesthetic result in the form of a floral-patterned vehicle. Similarly, blending a tomato with a bicycle could result in a red-painted bicycle. In contrast, combining a lily with a bicycle may produce a bicycle adorned with floral elements or set against a backdrop evoking a garden. This type of blending is particularly notable for its capacity to generate creative reinterpretations of objects, often yielding visually appealing and conceptually intriguing combinations.

\textbf{Material and Pattern Blending.} This type refers to the integration of materials—such as stripes, textures, or other patterns—into various objects. The primary characteristic of this form of blending lies in the inherent compatibility of materials with a wide range of object types, thereby facilitating smoother transitions and more seamless integrations across different categories. This suggests that materials have universal applicability, enabling them to blend effectively with diverse objects regardless of their inherent features.

\textbf{Inherently Unrelated Blending.} This form of blending presents significant challenges. When attempting to blend entities from vastly different categories, such as a phone with a dinosaur or an airplane with a dog, the resulting combinations often appear disjointed or nonsensical. This highlights the difficulty in merging categories that exhibit substantial differences in their fundamental characteristics, suggesting that certain combinations may be inherently incompatible.

\textbf{Broad and General Blending.} This type can also complicate the blending process. For example, blending a concept as broad as "human" tends to result in less coherent outputs, likely due to the wide range of attributes and features that fall under such a generalized category. In contrast, simple, well-defined descriptors such as "black" tend to blend more effectively, as they are associated with clear, unambiguous characteristics that leave little room for interpretive variation.

\textbf{Contextual and Background Blending.} Context plays a critical role in the outcome of certain blending processes. Many concepts inherently carry with them specific contextual or environmental associations. For example, when the concept of a "squirrel" is blended, it is often accompanied by a contextual backdrop of trees or forests. These environmental associations are embedded within the very concept of the entity itself, influencing the final blending output and shaping how the resulting image or combination is perceived.


\subsubsection{Factors of Blending Concepts}

\textbf{Subject and Background. }The effect of concept blending largely depends on the categories to which the blended concepts belong—whether they are subjects or backgrounds. When two concepts, such as a dog and a cat, both belong to the subject category, they are typically blended into a new subject, or one of them may dominate the resulting blend. In contrast, when the blend involves a subject and a background, such as a sheep and a cactus, the result may depict the sheep in a natural environment with cactuses, or the sheep may take on cactus-like characteristics, such as cactus-inspired wool. It is important to note that the identity of a concept can shift depending on how it is paired. For instance, while a cactus serves as the background concept in the pairing "cactus, sheep," it becomes part of the subject pair when blended with a rose, as in "cactus, rose." Thus, the role of a concept—whether as a subject or background—can significantly alter its representation in the blending process.

\textbf{Distance between Concepts. }Distance is another crucial factor that determines the effectiveness of the blending. This principle is illustrated through t-SNE analysis in \Cref{t_SNE_analysis_paragraph}, which demonstrates how the proximity of two concepts within an embedding space influences their ability to blend. When two concepts are close to each other in this space, they blend easily, forming a cohesive blend. However, as the distance between the concepts increases, the blending becomes less seamless. For example, concepts that are close in proximity, such as cats and dogs, tend to blend effortlessly, with the final output lying somewhere between both concepts. When concepts are moderately distant, such as a cactus and a rose, partial blending occurs, with the resulting image leaning more towards one concept while still maintaining some elements of the other. As the distance between concepts becomes even greater, such as in the case of an airplane and a bird, the blending becomes limited, and the final result often favors one concept, with the other remaining a secondary influence. 

\textbf{Special Cases in blending. }Certain concepts possess intrinsic backgrounds or properties that affect blending. For instance, objects like tables and chairs, which are complementary in nature, are more likely to be presented together within the same space rather than being blended into a single form. Similarly, the blending of elements like water and fire may not result in a fluid combination but instead transform into something entirely different, like stone (result from TEXTUAL method), as the inherent properties of the elements conflict with each other.

\subsection{Limitations and Future work}

Our method has several limitations that negatively affect the blending results. First, the latent interpolation calculation is unstable during generation, often leading to chaotic or overlapping outcomes. Second, when blending multiple concepts (e.g., three or four), the results are unsatisfactory, as the UNET method is not designed to handle more than two conditions simultaneously. Third, the generation process is prone to fluctuation, lacking consistency and stability. Fourth, the Stable Diffusion model exhibits a bias towards different concepts, necessitating the adjustment of the parameter $\gamma$, which is cumbersome and inefficient. Therefore, we aim to explore more stable, cost-effective, and training-free generation methods, as well as seek a more structured approach for blending multiple concepts.

\subsection{More Visual Results}
\Cref{more_blending_results} and \Cref{more_blending_results_2} present several of our blending results, highlighting the strong capabilities of our method for both concept blending and generation tasks.
\input{figures/more_results}
\input{figures/more_results_2}

\end{document}

