
\begin{figure}[t]
    \centering
    \includegraphics[width=.98\linewidth]{Figures/Teaser_survey_good_example.pdf}
    \vspace{-0.5em}
    \caption{RPA evaluation design guideline. To illustrate how to use it in practice, we pretended we were selecting the evaluation metrics for the  ``Stanford Agent Village''~\cite{park2023generative} given agent attributes (yellow) and task attributes (pink). The original authors' selection of evaluation metrics (purple and blue) perfectly aligns with our RPA design guideline, which echoes their work's robustness. More details in Sec~\ref{sec_good_example} and a bad example in Sec~\ref{sec_bad_example}.}
    \vspace{-1.5em}
    \label{fig:teaser}
\end{figure}


\section{Introduction}
% What are Role-Playing Language Agents
LLMs have yielded human-like performance in various cognitive tasks (e.g., memorization~\cite{schwarzschild2025rethinking}, reasoning~\cite{wang2023can, plaat2024reasoning}, and planning~\cite{song2023llm, huang2024understanding}). 
These emergent capabilities have fueled growing research interest on 
\textbf{Role-Playing Agent} (RPA)~\cite{chen2024from,tseng-etal-2024-two}:
RPAs are digital intelligent agent systems powered by LLMs, where users provide human-like \textbf{agent attributes} (e.g., personas) and \textbf{task attributes} (e.g., task descriptions) as input, and prompt the LLM to generate human-like behaviors and the reasoning process. 
The potential of RPAs is promising and far-reaching, as illustrated by the early results of the massive interdisciplinary studies in social science~\cite{10.1145/3526113.3545616, park2023generative}, network science~\cite{10.1145/3613904.3642363}, psychology\cite{jiang-etal-2024-personallm} and juridical science~\cite{he-etal-2024-agentscourt}.
% , such as social system simulation, privacy auditing, psychological experiments~, and judicial decision making.




Despite growing interest in RPAs, a fundamental question remains: \textbf{how can we systematically and consistently evaluate an RPA?}
How should we select the evaluation metrics, so that the evaluation results can be comparable or generalizable from one task to another task?
Addressing these challenges is difficult~\cite{dai2024mmrole, tu2024charactereval, wang2024incharacter}. 
due to the vast diversity of tasks (e.g., simulating an individual's online browser behavior~\cite{10.1145/3613904.3642363} or simulating a hospital~\cite{li2024agent}), and the high flexibility in RPA design (e.g., an agent persona can be one sentence or 2-hours of interview log~\cite{park2024generativeagentsimulations1000}).
%For instance, researchers not only leverage automatic metrics and human evaluations~\cite{} but also explore LLMs as evaluators~\cite{}. 
Another challenge is the inconsistent and often arbitrary selection of evaluation methods and metrics for RPAs, raising concerns about the validity and reliability of evaluation results~\cite{wang2025limits, zhang2025simulation}.
% On the one hand, whether the selected evaluation metrics can comprehensively and faithfully benchmark the performance of the proposed RPLAs, with respect to the designated human-like activities, impacts the validity of any conclusion or observations made.
As a result, the research community finds it difficult to compare the performance across multiple RPAs in similar tasks reliably and systematically.
%  there is a lack of, but also an urgent need for, a systematic review of the correlations between RPLA designs, designated tasks and domains, and evaluation methodologies.
% Such a comprehensive review could benefit researchers not only in developing RPLAs more reliably, but it could also shed light on the guidelines for future design and choice of RPLA evaluation metrics.  


To address this gap, we propose an evidence-based, actionable, and generalizable design guideline for evaluating LLM-based RPAs. We conducted \textbf{a systematic literature review} of $1,676$ papers on the LLM Agent topic and identified $122$ papers describing its evaluation details. 
Through expert coding, we found that agent attribute design interacts with task characteristics (e.g., simulating an individual or simulating a society requires a diverse set of agent attributes). 
Furthermore, we synthesized common patterns in how prior research successfully (or unsuccessfully) designed their evaluation metrics to correspond to the RPA's agent attributes and task attributes. 
Building on these insights, we propose an RPA evaluation design guideline (Fig.~\ref{fig:teaser}) and illustrate its generalizability through two case studies. 
% , including demographic information and psychological traits, whereas domain-specific tasks focus on a narrower set of agent attributes directly related to the specific context of the task.

%Our findings reveal that the selection of agent attributes in RPLA design is closely linked to the nature of the downstream tasks. Generic simulation tasks, such as simulated individuals and simulated society, require a wide range of agent attributes, including demographic information and psychological traits, to account for the diverse scenarios they must handle. In contrast, domain-specific tasks are more focused, utilizing a narrower set of agent attributes that are directly relevant to the particular context of the task. Building on this, the choice of evaluation metrics is similarly influenced by both agent attributes and downstream tasks. We found distinct patterns in how specific agent attributes correspond to particular evaluation metrics: RPLAs that incorporate beliefs and values are primarily assessed using metrics related to bias, fairness, and ethics, while those focusing on psychological traits rely on psychological metrics. Additionally, RPLAs centered on activity history, skills, and expertise are often evaluated through internal consistency and external alignment metrics. In terms of downstream tasks, task-oriented metrics are selected based on the specific nature of the task. 
%For example, performance metrics are most prominent in simulated individuals, simulated society, and opinion dynamics tasks, while social and decision-making metrics dominate tasks in simulated society and decision-making scenarios. Psychological metrics are prevalent in psychological experiments and education tasks, while writing tasks tend to focus on content and textual quality. 
%In summary, general simulation tasks require a broader array of evaluation metrics, whereas domain-specific tasks use a more targeted set of metrics tailored to their unique requirements.



% Role-Playing Language Agents (RPLAs)~\cite{chen2024from,tseng-etal-2024-two} are digital entities powered by asking large language models (LLMs) to ``role-play'' the provided personas in input and reflect human-like interactions and behaviors. 
% Recent work has demonstrated that LLMs exhibit human-like cognitive abilities, such as memorization~\cite{}, reasoning~\cite{}, and planning non-predefined actions~\cite{}. 
% These capabilities enable RPLAs to plausibly simulate specific individuals or a group of populations, sparking researcher's significant interest in exploring RPLAs' applications across various domains, such as social system audits~\cite{10.1145/3526113.3545616, park2023generative}, privacy auditing~\cite{10.1145/3613904.3642363}, psychological experiments~\cite{jiang-etal-2024-personallm}, and opinion dynamics~\cite{he-etal-2024-agentscourt}.



% Regarding the relationship between agent attributes and agent-oriented metrics, we found distinct correspondences. For instance, RPLAs that include beliefs and values are most commonly evaluated using bias, fairness, and ethics metrics, while those with psychological traits rely on psychological metrics. Internal consistency metrics and external alignment metrics are more frequently used to evaluate activity history and skills and expertise.

% For the relationship between downstream tasks and task-oriented metrics, we observed that performance metrics dominate in simulated individuals, simulated society, and opinion dynamics tasks. Social and decision-making metrics are significant for evaluating simulated society and decision-making tasks. Psychological metrics are heavily used in psychological experiments and education training tasks, while the writing task primarily focuses on content and textual metrics. General simulation tasks require a more diverse set of metrics, while domain-specific tasks tend to use a narrower range.





% Since evaluating LLM agents is a relatively new direction, we surveyed both peer-reviewed papers from top-tier venues and pre-print ones available on Arxiv. The final corpus comprises $163$ papers, meticulously selected from a pool of $1572$ publications since 2020, in which the OpenAI published GPT-3. 
% Based on the summary of these studies, we propose a two-stage decision tree. In the first stage, the decision tree establishes the correspondence between research objectives or downstream tasks and evaluation metrics, providing selection recommendations to help researchers preliminarily filter metrics based on their research goals. These metrics are divided into two types: scenario-based metrics, which are closely tied to specific downstream tasks and used exclusively for their evaluation, and task-specific metrics, which are general indicators that can be applied across a broader range of tasks. In the second stage, the decision tree considers the influence of agent types and attributes on the selection of evaluation metrics, enabling researchers to further refine their choices based on the characteristics and attributes of the agent.

% In summary, by holistically examining the previous evaluation metrics of LLM agents in simulating humanoid behavior, we find that research in this field has significantly increased in the past twelve months and has diverse applications, ranging from \hl{xx} to \hl{xx}. 

% \hl{Add other detailed results later}

% These challenges arise from the diversity of evaluation metrics across application areas, making direct comparisons difficult. Additionally, the lack of a systematic review of existing evaluation metrics leads to incomplete assessments. Furthermore, insufficient research on the various factors influencing evaluation (e.g., research goals, downstream tasks, and feasible evaluation metrics) results in arbitrary metric selection without clear rationale. We summarize these pressing issues in RPLA evaluation into three key challenges:

% % 单个工作中不同因素对evaluation的影响
% \textbf{Dynamic evaluation complicates metrics selection.} The process of selecting evaluation metrics for RPLAs is inherently dynamic, as it needs to adapt to different stages of agent development. During the design phase,  research goals and agent attributes can influence the metrics selection. 
% Once the agent is deployed in downstream tasks, the evaluation focus shifts to task-specific performance, requiring a reassessment of metrics. 
% This dynamic and multi-faceted nature of evaluation increases the complexity of metrics selection, emphasizing the need for systematic guidelines to streamline the process.  


% compare with existing survey for the evaluation of RPLA: 
% 1. similarity：LLM生成数据与人类真实数据进行对比；explanability：让LLM对生成数据进行解释；ethic issue：检验生成数据是否有bias和harmful information
% 2. Conversation Ability, Role-Persona Consistency, Role-Behavior Consistency, Role-Playing Attractiveness; Approach: automatic evaluation, human evaluation, llm evaluation
% 3. personality assessments（linguistic style, personality consistency and fidelity）
% 4. Character independent Capabilities; Linguistic Style and Knowledge; Personality and Thinking Process; Evaluation for Conversation, Recommendation, task solving
% 5. automatic evaluation, human evaluation, llm evaluation; simulation similarity.


% However, reliably evaluating RPLAs across diverse domains and tasks remains a significant challenge. 
% While previous literature reviews~\cite{} have outlined relevant evaluation approaches, there is still no comprehensive methodology for selecting evaluation metrics that account for the entire RPLA design process, which involves three key factors: agent attributes, downstream tasks, and evaluation approaches. 
% Agent attributes, such as demographic information and personality, define the intrinsic qualities of the agent. 
% Downstream tasks refer to the specific applications for RPLAs. 
% Evaluation approaches include human judgment, automatic metrics, and LLM-based assessments. Yet, their influence on metric selection remains unclear, leading to arbitrary metric selection and a lack of a unified methodology for RPLA evaluation.

% 方法论的意义：没有它会导致单个工作评估不全面，同一领域不同工作指标不一致没法对比





% Without a unified framework, evaluations tend to be partial within each work and inconsistent between studies, as arbitrary metric choices can weaken their reliability and comparability. 
% RPLA evaluations are typically divided into task-oriented and agent-oriented metrics, but these are often considered separately rather than in conjunction. 
% Task-oriented metrics measure an agent's performance on specific tasks, such as accuracy~\cite{wang-etal-2024-unleashing} and success rates~\cite{li2023metaagents}. 
% However, task evaluation alone is insufficient, as the quality of the generated data directly impacts agent performance in subsequent tasks and can inform fine-tuning. 
% Agent-oriented metrics, on the other hand, assess intrinsic, task-agnostic qualities, such as consistency~\cite{shao-etal-2023-character} and robustness~\cite{shao-etal-2023-character}, but fail to account for how the generated data influences downstream tasks. 
% Therefore, relying solely on either set of metrics leads to a partial evaluation, diminishing the evaluation reliability. Moreover, inconsistent evaluation metrics across studies complicate comparisons. 
% For example, in tasks like decision-making, studies might use varying metrics such as negotiation success rate, societal satisfaction, or risk preference. The arbitrary combinations of metrics make it difficult to objectively compare results across research.