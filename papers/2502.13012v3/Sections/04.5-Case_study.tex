\section{Case Study: How to Use RPA Design Guideline to Select Evaluation Metrics}

We present \textbf{two case studies} to illustrate how following our evaluation guidelines leads to the selection of a comprehensive set of evaluation metrics, while significant deviations may result in incomplete evaluation. By adopting the perspective of the original authors, we compare the evaluation outcomes resulting from adhering to or deviating from the RPA evaluation guidelines.

\subsection{A Good Example: \textit{Generative Agents: Interactive Simulacra of Human Behavior}}
\label{sec_good_example}

As shown in Fig.~\ref{fig:teaser}, \citet{park2023generative} designed agents with demographic information, action history, and social relationships to create an interactive artificial society. 
Their evaluation methods are in line with the structured selection process proposed in our survey. Since no established agent-oriented evaluation metrics exist for social relationships, they focused on demographic information and action history. Referring to Fig.~\ref{fig:pie-chart-agent-oriented}, they identified four relevant metric categories: Content and textual metrics, Internal consistency metrics, External alignment metrics, and Psychological metrics. Based on Tab.~\ref{tab: long_table_agent_metrics_src} in Appendix~\ref{sec: metrics glossary}, they selected five specific evaluation metrics: Self-knowledge (Content and textual, Internal consistency), Memory and Plans (Internal consistency), Reactions (External alignment), and Reflections (Psychological).

For task-oriented metrics, they determined that the agentsâ€™ downstream tasks aligned with \textit{simulated society} and designed the evaluation metrics that are aligned with the top three most relevant metric types reported in Fig.~\ref{fig:pie-chart-task-oriented}.
% According to Fig.~\ref{fig:pie-chart-task-oriented}, the three most relevant metric categories were Performance metrics, Psychological metrics, and Social and decision-making metrics. 
As shown in Tab.~\ref{tab: long_table_task_metrics_src} in Appendix~\ref{sec: metrics glossary}, they selected four evaluation metrics: Response accuracy (Performance), Relationship formation (Psychological), Information diffusion and Coordination (Social and decision-making). By systematically aligning evaluation metrics with agent attributes and task objectives, this approach ensured a comprehensive and meaningful assessment.

\subsection{A Flawed Example: \textit{A Generative Social World for Embodied AI}}
\label{sec_bad_example}

A flawed example is presented in Appendix~\ref{appendix:bad_example} Fig.~\ref{fig:bad example}, which is an ICLR submission, and the reviews are publicly available on OpenReview.
The authors
% As shown in Appendix~\ref{appendix:bad_example} Fig.~\ref{fig:bad example}, \citet{zhou2024virtual} 
developed agents with demographic attributes, action history, psychological traits, and social relations for route planning and election campaigns. However, their evaluation deviated significantly from our RPA evaluation design guidelines.

Despite designing agents with clear attributes, they did not include any agent-oriented evaluation metrics. For task-oriented metrics, they identified tasks related to Opinion Dynamics and Decision-Making, which should have been evaluated using five key categories: Performance metrics, Psychological metrics, External alignment metrics, Social and decision-making metrics, and Bias, fairness, and ethics metrics. Instead, their evaluation relied solely on Arrival rate, Time, and Alignment between campaign strategies, leading to an incomplete assessment. This omission resulted in criticism from reviewers, as one noted: \textit{``The paper performs almost no quantitative experiments... This actually shows that the benchmark cannot cover too many current research methods, which is the biggest weakness of the paper.''}