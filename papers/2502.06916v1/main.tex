\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{soul}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{lipsum}

\usepackage{url}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{microtype}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{array, makecell}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[dvipsnames,svgnames,table,xcdraw]{xcolor}

\usepackage{caption}
\captionsetup[table]{skip=10pt}

\usepackage{arxiv}
\usepackage{authblk}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[bookmarksnumbered,hypertexnames=false,colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,anchorcolor=green,breaklinks=true]{hyperref} 

\newcommand\blfootnote[1]{
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}
  \addtocounter{footnote}{-1}
  \endgroup
}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}

\newcommand{\A}{\mathcal{A}}
\newcommand{\EE}{\mathrm{E}}
\newcommand{\unitary}{\mathrm{U}}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\ketbra}[1]{|#1\rangle\langle#1|}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkred}{rgb}{0.5, 0.0, 0.0} 



\title{Hyper compressed fine-tuning of large foundation models with Quantum Inspired Adapters}

\author[1, 2]{\textbf{Snehal Raj}}
\author[2]{\textbf{Brian Coyle}}

\affil[1]{LIP6, CNRS, Sorbonne Universit\'e, 4 Place Jussieu, 75005 Paris, France}
\affil[2]{QC Ware, Palo Alto, USA and Paris, France.}

\renewcommand*{\Authands}{, }
\date{}

\begin{document}
\maketitle
\vskip 0.25in

\begin{abstract}
Fine-tuning pre-trained large foundation models for specific tasks has become increasingly challenging due to the computational and storage demands associated with full parameter updates. Parameter-Efficient Fine-Tuning (PEFT) methods address this issue by updating only a small subset of model parameters using adapter modules. In this work, we propose \emph{Quantum-Inspired Adapters}, a PEFT approach inspired by Hamming-weight preserving quantum circuits from quantum machine learning literature. These models can be both expressive and parameter-efficient by operating in a combinatorially large space while simultaneously preserving orthogonality in weight parameters. We test our proposed adapters by adapting large language models and large vision transformers on benchmark datasets. Our method can achieve 99.2\% of the performance of existing fine-tuning methods such LoRA with a 44x parameter compression on language understanding datasets like GLUE and VTAB. Compared to existing orthogonal fine-tuning methods such as OFT or BOFT, we achieve 98\% relative performance with 25x fewer parameters. This demonstrates competitive performance paired with a significant reduction in trainable parameters. Through ablation studies, we determine that combining multiple Hamming-weight orders with orthogonality and matrix compounding are essential for performant fine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a promising direction for efficient adaptation of language and vision models in resource-constrained environments.
%
\end{abstract}

\section{Introduction}

Pre-trained large foundation models  such as BERT~\cite{devlin2018bert}, GPT-3~\cite{brown2020language}, and Vision Transformers~\cite{dosovitskiy2020image} have achieved state-of-the-art results on various tasks. Fine-tuning these models on specific downstream tasks typically involves updating all model parameters but with a lower learning rate, which becomes computationally prohibitive as model sizes continue to grow into the billions of parameters. This challenge has spurred interest in Parameter-Efficient Fine-Tuning (PEFT) methods~\cite{houlsby2019parameter}, which aim to adapt large foundation models to new tasks by updating only a small subset of parameters or introducing lightweight adaptation modules.

One of the most prominent PEFT techniques is Low-Rank Adaptation (LoRA)~\cite{hu2021lora}, which injects low-rank trainable matrices into the transformer layers, significantly reducing the number of parameters that need to be updated. Other methods like Adapters~\cite{houlsby2019parameter}, BitFit~\cite{zaken2021bitfit}, and Prompt Tuning~\cite{lester2021power} have also demonstrated effectiveness in various settings. Recently, Orthogonal Fine-Tuning (OFT)~\cite{qiu2023controlling} and its `Butterfly' specification (BOFT)~\cite{liu2023parameter} have been proposed to mitigate catastrophic forgetting of the pre-trained models during fine-tuning by applying orthogonal transformations. These methods have shown promising results in achieving a balance between performance and parameter efficiency.

\begin{figure*}[ht!]
    \centering
    % First Row
    % \begin{subfigure}{0.35\textwidth}  
    %     \centering
        \includegraphics[width=\textwidth]{figures/Fig1_Overview_adapters.pdf}
  
    \caption{\textbf{Comparison of different adapter methods.} Trainable parameters for each model shown in dark green.  a) Full finetuning, i.e. retraining the entire pre-trained weight matrix. b) Low-rank adaptation (LoRA) using an additive adapter decomposition into two low-rank matrices. c) Orthogonal fine-tuning (OFT) with a multiplicative adapter decomposed into orthogonal blocks. d) Our proposed specification of a (multiplicative) Quantum-Inspired Adapter, using compound matrices decomposed (like OFT) into blocks. For the compound matrices, the zeroth order compound (top left of each block) is the only trainable part. Higher order compounds are completely determined by this base matrix.}
    \label{fig:four_images}
\end{figure*}



In this work, we introduce \emph{Quantum-Inspired Adapters}, a novel PEFT method inspired by Hamming-weight preserving quantum circuits~\cite{kerenidis2022quantum, Landman2022quantummethods, Cherrat2023quantumdeephedging}. Our approach constructs orthogonal adapters using compound matrices, focusing on compound orders up to a certain value \( k \) to ensure parameter efficiency. We evaluate our method on several tasks from the general language understanding evaluation (GLUE) benchmark ~\cite{wang2018glue} and on a subset of tasks from the visual task adaptation (VTAB) benchmark~\cite{zhai2019large}. Our experiments demonstrate that Quantum-Inspired Adapters achieve competitive performance while dramatically reducing the number of trainable parameters compared to existing PEFT methods like LoRA, OFT, and BOFT.


Our contributions are summarized as follows:

\begin{itemize}
    \item We propose Quantum-Inspired Adapters, a novel PEFT method that leverages circuits from quantum machine learning literature to create parameter efficient adapters.
    \item We provide a detailed classically-efficient construction of quantum inspired adapters using compound matrices and showcase parameter efficiency and orthogonality properties.
    \item We conduct extensive experiments on the GLUE and VTAB benchmark, demonstrating that our method achieves a significant reduction in trainable parameters while maintaining competitive performance levels.
    \item We analyze the trade-offs between accuracy and parameter efficiency and discuss the implications for model adaptation in resource-constrained environments.
\end{itemize}




\section{Background}
\label{sec:background}

Large language and vision foundation models are largely based on the transformer architecture ~\cite{vaswani2017attention, dosovitskiy2020image, devlin2018bert}. In this section, we provide an overview of the core components and illustrate adapter based fine-tuning.

\subsection{Transformer Architecture}

The transformer architecture has become the foundation for many large language and vision foundation models due to its ability to capture long-range dependencies and its scalability. It consists of stacked encoder and decoder layers, each containing multi-head self-attention and feed-forward network layers. These components are interconnected by residual connections and layer normalization. PEFT methods typically focus on modifying the self-attention and feed-forward network (FFN) layers to introduce trainable parameters efficiently. We describe these layers briefly as follows:

\paragraph{Multi-Head Self-Attention Layer:}

For an input sequence \( X   \in  \mathbb{R}^{n \times d} \), where \(n, d\) are the sequence length and hidden dimension respectively, the self-attention mechanism computes:

\begin{equation} \label{eqn:attention_defn}
%
    \text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}\right)V,
%
\end{equation}

The query, key and value matrices, \( Q = XW_Q \), \( K = XW_K \), and \( V = XW_V \) are linear projections of the input \( X \) using learnable weight matrices \( W_Q, W_K, W_V \in \mathbb{R}^{d \times d} \) respectively.

\paragraph{Feed-Forward Network (FFN) Layer:} A typical FFN layer involves two trainable weight matrices, \( W_1 \in \mathbb{R}^{d \times d_{F}} \), \( W_2 \in \mathbb{R}^{d_{F} \times d} \), and is defined as:

\begin{equation} \label{eqn:ffn_defn}
\text{FFN}(X) = \sigma(XW_1 + \boldsymbol{b}_1)W_2 + \boldsymbol{b}_2,
\end{equation}

where \(d_{F}\) is the dimension of the feed-forward layer and \(\sigma\) is a non-linear function which we assume to be \(\sigma := \mathsf{ReLU}\). 

\subsection{Parameter-Efficient Fine-Tuning Methods} \label{ssec:peft_methods}

In this section, we review some common and powerful PEFT methods, including Low-Rank Adaptation (LoRA) and Orthogonal Fine Tuning (OFT) and its butterfly variation, (BOFT), which fine-tune orthogonal parameterizations of weight matrices.

\subsubsection{Low-Rank Adaptation (LoRA)}
%
There are two main families of adapter configurations one may encounter. The first, are \emph{additive} adapters, of which Low-Rank Adaptation (LoRA)~\cite{hu2021lora} is an example. Assume a pre-trained weight matrix \( W^* \in \mathbb{R}^{d_{O} \times d_{I}}\) producing output \(\boldsymbol{h}\in \mathbb{R}^{d_O}\) given input \(\boldsymbol{x} \in \mathbb{R}^{d_I}\), \( \boldsymbol{h} := W^*\boldsymbol{x}\) with input and output dimensions, \(d_I, d_O\) respectively. This weight matrix could be the weights of the FFN $W_1$, $W_2$ as above or the QKV  weight matrices \( W_Q, W_K, W_V\) in the attention mechanism. Adapters introduce additional trainable parameters to the pre-trained model \emph{without} modifying the original weights. This adapter family is additive because the final model (for inference) is of the form:
%
\begin{equation} \label{eqn:classical_adapter}
    W_{\text{adapt}} := W^* + \Delta W
\end{equation}
%
where \(\Delta W \in \mathbb{R}^{d_{O} \times d_{I}}\) is the trainable adapter. Note, the adapter matrix, once trained \((\Delta W \rightarrow \Delta W^*)\), can be merged with the original matrix, and as such does not add any inference overhead. The output of the trained, `\emph{adapted}', layer is then:
%
\begin{equation} \label{eqn:adapted_output_trained}
    \boldsymbol{h}_{\text{adapt}}^* = W^*_{\text{adapt}}\boldsymbol{x} = (W^* + \Delta W^*)\boldsymbol{x}
\end{equation}
%
Typically, an adapter module consists of a parameterization that leads to significantly fewer resource requirements, while retaining the outer dimensions \(d_O, d_I\). LoRA~\cite{hu2021lora} injects low-rank trainable matrices into the transformer layers by decomposing the weight updates into low-rank matrices. Specifically, the adapter for LoRA are defined as:
%
\begin{equation} \label{eqn:lora}
\Delta W_{\text{LoRA}} := \alpha W_{\text{up}} W_{\text{down}},
\end{equation}
%
where \( W_{\text{up}} \in \mathbb{R}^{d_O \times r} \), \( W_{\text{down}} \in \mathbb{R}^{r \times d_I} \). Assuming, square pre-trained matrices, \(d_O = d_I =: d\) they enforce that \( r \ll d \) (\(r\) is the rank), and \( \alpha \) is a scaling factor. During fine-tuning, only \( W_{\text{up}} \) and \( W_{\text{down}} \) are updated. 

\subsubsection{Orthogonal Fine-Tuning (OFT)}

Orthogonal Fine-Tuning (OFT)~\cite{qiu2023controlling} is an alternative approach to parameter-efficient fine-tuning which enforces an \emph{orthogonality} constraint on the adapter. The authors justify orthogonality as a useful feature in helping preserve the hyperspherical energy i.e. the angular feature difference between neurons ~\cite{liu2018learning} which in turn helps preserve original knowledge of the model. Unlike methods such as LoRA that inject low-rank updates in an \emph{additive} manner, OFT and its variants introduce \emph{multiplicative} adapters. In this case, the updated weight matrix is expressed as:
%
\begin{equation} \label{eqn:oft_adapter}
%
    W_{\text{OFT}} = \Delta W_{\text{OFT}} W^*,
%
\end{equation}
%
Again, they assume \(W^* \in \mathbb{R}^{d \times d} \) is the square pre-trained weight matrix and \( \Delta W_{\text{OFT}} \in \mathbb{R}^{d \times d} \) is the orthogonal adapter. Specifically, they have \( \Delta W_{\text{OFT}}^\top \Delta W_{\text{OFT}} = \mathds{1}\). The orthogonality of \( \Delta W_{\text{OFT}} \) ensures that the transformation preserves the spectral properties of \( W^* \), retaining the pre-trained knowledge during fine-tuning. Different parameterizations of  \( \Delta W_{\text{OFT}} \) are possible - specifically,~\cite{qiu2023controlling} chooses to employ the Cayley transform. Given a parameterized matrix, \( P \in \mathbb{R}^{d \times d}\), the OFT adapter with the Cayley transform is defined as:
%
\begin{equation} \label{eqn:oft_cayley}
    \Delta W^{\text{C}}_{\text{OFT}} := (\mathds{1}_d + Q)(\mathds{1}_d - Q)^{-1}, \quad Q :=  \frac{1}{2} (P - P^T) 
%
\end{equation}
%
The Cayley transform is efficient and ensures that \( \Delta W_{\text{OFT}} \in \mathrm{SO}(d) \), the special orthogonal group of dimension \(d\).  To further improve parameter efficiency, OFT introduces a block-diagonal structure to \( \Delta W_{\text{OFT}} \). The orthogonal matrix is partitioned into \( r \) smaller orthogonal blocks, each parameterized with ~\eqref{eqn:oft_cayley}:

\begin{equation}
%
    \Delta W^{\text{BD}, r}_{\text{OFT}} :=
    \begin{bmatrix}
    %
        \Delta W^{\text{C}}_{\text{OFT},1} & 0 & \cdots & 0 \\
        0 & \Delta W^{\text{C}}_{\text{OFT},2} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \Delta W^{\text{C}}_{\text{OFT},r}
    %
    \end{bmatrix}
\end{equation}
%
where each \( \Delta W_{\text{OFT},i} \in \mathbb{R}^{\sfrac{d}{r}\times \sfrac{d}{r}} \) and \( Q_i \in \mathbb{R}^{\sfrac{d}{r} \times \sfrac{d}{r}} \). When \( r = 1 \), the block-diagonal matrix reduces to the original full orthogonal matrix, \(\Delta W^{\text{BD}, 1}_{\text{OFT}} = \Delta W_{\text{OFT}} \). For the remainder of the text, we implicitly assume this block-diagonal structure in OFT and drop the superscripts when clear from context. Using this block-diagonal structure, the total number of parameters is reduced to \( \mathcal{O}(d^2/r) \), which can be compressed further to \( \mathcal{O}(d^2/r^2) \) via parameter sharing across blocks.

\subsubsection{Butterfly Orthogonal Fine-Tuning (BOFT)}

Butterfly Orthogonal Fine-Tuning (BOFT)~\cite{liu2023parameter} extends OFT by introducing an efficient parameterization of the orthogonal matrix using butterfly structures. In BOFT, the orthogonal matrix \( \Delta W_{\text{BOFT}} \in \mathbb{R}^{d \times d} \) is constructed as a product of \( m \) sparse orthogonal matrices derived from `\emph{butterfly}' structures:

\begin{equation}
\Delta W_{\text{BOFT}} = \prod_{i=1}^{m} \widetilde{B}_{(i)},
\end{equation}
%
where each \( \widetilde{B}_{(i)} \in \mathbb{R}^{d \times d} \) is a butterfly \emph{factor} - a sparse orthogonal matrix that efficiently captures global interactions within the data. These butterfly factors are recursively defined and constructed to ensure orthogonality. The butterfly structure originates from the Cooley-Tukey algorithm for the Fast Fourier Transform, known for its efficient information exchange properties. In BOFT, the butterfly factors are built using small orthogonal blocks that are combined to form larger orthogonal matrices. Specifically, each butterfly factor \( \widetilde{B}_{(i)} \) is defined as, \(\widetilde{B}_{(i)} = \operatorname{Permute}\left( \operatorname{diag}\left( \Delta W_{\text{BF},1}^{(i)}, \Delta W_{\text{BF},2}^{(i)}, \dots, \Delta W_{\text{BF},k}^{(i)} \right) \right)\), where \( \Delta W_{\text{BF},j}^{(i)} \in \mathbb{R}^{b \times b} \) are small orthogonal matrices parameterized via the Cayley transform~\eqref{eqn:oft_cayley}, \(k := \sfrac{d}{b}\) are the number of blocks at level \(i\) and \( \operatorname{Permute}(\cdot) \) rearranges the blocks to create the butterfly pattern. They typically take the number of butterfly factors to be \( m = \log_{b} d \) where  \( b \) is the block size, and \( b \geq 2 \). The number of parameters required is \(N^{\text{BOFT}}_P = \frac{1}{2}md(b - 1) = \frac{1}{2}(b - 1)d\log_b d\)~\cite{liu2023parameter}.  When \( b = 2 \), the parameter count becomes \(N^{\text{BOFT}}_P = \mathcal{O}(d \log d) \), compared to the \(N^{\text{OFT}}_P = \mathcal{O}(d^2) \) parameters required for a full orthogonal matrix in OFT.


\section{Quantum-Inspired Adapters}
\label{sec:quantum_inspired_adapters}

In this section, we introduce our Quantum-Inspired Adapters, which leverage Hamming-weight preserving quantum circuits prevalent in  quantum machine learning literature~\cite{kerenidis2022quantum, Landman2022quantummethods, Cherrat2023quantumdeephedging, coyle2024training, monbroussou2024subspace, cerezo2024unified}. Hamming-weight preserving circuits are efficiently implementable quantum circuits that encode and transform
data in fixed Hamming-weight subspaces. These circuits have been found useful in quantum machine learning for favorable properties such as absence
of barren plateaus~\cite{Cherrat2023quantumdeephedging, fontana2023adjoint, monbroussou_trainability_2024}.  They can be classically implemented using compound matrices, that enforce and retain orthogonality in a natural way via decompositions into Givens rotations~\cite{Landman2022quantummethods}.  Inspired by these principles, we propose to construct adapters using compound matrices up to a certain Hamming-weight \( k \). Combining compounding with orthogonality allows us to create novel adapters which are both expressive and parameter-efficient.

\subsection{Construction of Quantum-Inspired Adapters}

\subsubsection{Compound matrices} \label{subsec:compound_matrices}

Given a `base' matrix, \(A \in \mathbb{R}^{n \times n}\), the \emph{compound} matrix, \(\mathcal{C}_k := A^{(k)}\), of `order' \(k \in \left[ n \right]\) is defined as the \(\binom{n}{k} \times \binom{n}{k}\) dimensional matrix with entries \(A_{IJ}^{(k)} := \det(A_{IJ})\) such that \(I\) and \(J\) are all possible row and column subsets. We use \(\mathcal{C}_k\) as compact notation for our experiments later in the text. The work of~\cite{kerenidis2022quantum} demonstrated how the action of these matrices on different Hamming-weight (different orders, \(k\)) could be efficiently performed using quantum circuits composed of so-called \emph{fermionic beam splitter} (FBS) quantum gates. We will describe the quantum implementation in further detail later in the text.

However, we say that the \emph{compound adapters} which serve the basis of our proposal are quantum-\emph{inspired} because, for a constant Hamming-weight $k =\mathcal{O}(1)$, the action of these layers can be efficiently classically simulated by direct simulation of the subspaces. We will primarily deal with small order (and combinations thereof) compound matrices in this work, though we leave the open possibility of quantum speedups by quantum implementation of compound layers~\cite{Cherrat2023quantumdeephedging} to future work.

\subsubsection{Implementation details}

Given a pre-trained weight matrix \( W^* \in \mathbb{R}^{d \times d} \), we aim to construct a quantum adapter \( \Delta W_Q \in \mathbb{R}^{d \times d} \) such that \(W_{\text{adapt}} = \Delta W_Q W^* \). Note, here was assume the quantum(-inspired) adapter is a real matrix, which is possible to emulate on quantum circuits despite the existence of complex numbers~\cite{Landman2022quantummethods, kerenidis2022quantum}, although we do not rule out the extension of into complex adapters/layers in the future - for example when constructing quantum adapters from general initial states and operations.

Now, the Quantum-Inspired Adapter \(\Delta W_Q\) is constructed as a block-diagonal matrix:
%
\begin{equation} \label{eqn:adapter_block_diagonal}
    \Delta W_Q = \bigoplus_{i=1}^{r} \Delta W_Q^i,
\end{equation}
%
with \(\Delta W_Q^i\in \mathbb{R}^{b \times b}, \forall i \). This decomposition, similarly to OFT, introduces a hyperparameter, \(b := \sfrac{d}{r}\), to regulate the total number of parameters. Now each block is itself a block-diagonal matrix constructed using (multiple) compound matrices up to a fixed maximum Hamming-weight, \(K\). Temporarily, we define:
%
\begin{equation} \label{eqn:coumpound_adapter_block_diagonal}
    \Delta W_Q^{i, *} := \bigoplus_{k=1}^{K} A^{(k)},
\end{equation}
%
At this point, the dimensions of the above objects should be noted. For example, taking a pre-trained weight matrix of size \(d = 1024\), and assume we have a single block in~\eqref{eqn:adapter_block_diagonal}, \(N=1\) and a maximum Hamming-weight \(K=2\). Then, one method to match dimensions is to choose the dimension of the base matrix, \(n\), such that \(d - (\text{dim}(A^{(1)}) + (\text{dim}(A^{(2)})) = d - \binom{n}{1} - \binom{n}{2} = 1024 - n -\frac{1}{2}n(n-1) > 0 \) is minimized, but also an integer. In this example, a suitable choice of \(n\) could be \(44\). One needs to then finally pad the remaining dimensions which we choose to do as an identity matrix of appropriate size. We leave other choices, and other methods of dimension matching (for example minimizing instead \(\mathbb{N} \ni \binom{n}{1} + \binom{n}{2} - d > 0\) and truncating the hanging dimensions) for future study. 


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.26\textwidth}
    \centering
        \includegraphics[width=\linewidth]{figures/comp1.pdf}
        \caption{}
        \label{fig:subfig1}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/comp1-comp2.pdf}
        \caption{}
        \label{fig:subfig2}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/comp1-comp2-comp3.pdf}
        \caption{}
        \label{fig:subfig3}
    \end{subfigure}
    \caption{\textbf{Different possible adapter configurations.} The adapter decomposition is determined by the number of blocks (\(b\), or equivalently the `rank' \(r := \sfrac{d}{b}\)), and the number of compounds within each block. Trailing dimensions are padded with an identity matrix, and are not trainable. The figure shows a) $\mathcal{C}_1$,  \(b=4\) blocks, b) $\mathcal{C}_1\oplus\mathcal{C}_2$, \(b=4\) blocks and c) $\mathcal{C}_1\oplus \mathcal{C}_2\oplus\mathcal{C}_3$, \(b=2\) blocks. If the base matrix, \(A\) and hence \(A^{(1)} =: \mathcal{C}_1\) for each block is orthogonal, \(A^\top A=\mathds{1}\), configuration (a) is equivalent to OFT.}
    \label{fig:different_adapter_configurations}
\end{figure*}


Concretely, in general, we will choose the largest \(n\) such that \(\sum_{k=1}^{K} \binom{n}{k} \leq b\), construct each \(A^{(k)}, \forall k\in \{1, \dots K\}\) and then build block, \(i\) as:
%
\begin{align} \label{eqn:block_i}
    \Delta W_Q^i &:= \begin{bmatrix}
    \Delta W_Q^{i, *} & 0 \\
    0  & \mathds{1}_{b - d_{\text{comp}}}
    \end{bmatrix} 
    =  \begin{bmatrix}
    A^{(1)} & 0 & \cdots & 0 \\
    0 & A^{(2)} & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \mathds{1}_{b - d_{\text{comp}}}
    \end{bmatrix},
\end{align}
%
where \( d_{\text{comp}} := \sum_{k=1}^{K} \binom{n}{k} \). We show some examples of possible configurations in Figure.~\ref{fig:different_adapter_configurations}.

\paragraph{Orthogonality:}

Compound matrices, \( A^{(k)} \), inherit many properties from their base, \( A \) - including, invertibility, positive definiteness and, importantly for us, unitarity and orthogonality.
By constructing adapter blocks \( \Delta W_Q^i \) using orthogonal compounds and padding with identities, orthogonality is preserved and inherited by \( \Delta W_Q \). We test the importance of orthogonality as a property for our compound adapters later in section ~\ref{ssec:glue_dataset_results}.

\paragraph{Parameter Efficiency:}
%
The number of trainable parameters are directly controlled by the tuple, \((b, K)\), the number of blocks and number of compounds therein. Note that this is because choosing a larger \(K\) reduces the possible size of the base matrix which can be compounded, \(n\). All trainable adapter parameters are contained within this base matrix. This results in a compact parameterization suitable for large models. However, the compounding operation builds complex interactions between the parameters in higher orders. We show in our results that this is sufficient to gain high quality results with minimal tuning. However, one could investigate the reordering of features before an adapter layer to directly correspond to different Hamming-weight subspaces which may be more interpretable. For example for graph features, the first order compound, \(A^{(1)}\), corresponds to an action on graph nodes, while the second order, \(A^{(2)}\) could represent edge interactions. We leave such extensions to future work. 

\paragraph{Compounding:} One might ask whether the parameter efficiency is simply result of expanding the effect of a small matrix into a combinatorially large space. As such, the expensive compounding procedure (computing determinants) may not be strictly necessary for performant fine-tuning. However, in the below results we test this hypothesis by replacing the determinant computation (true compound) with alternative operations on matrix minors. For instance, instead of constructing \(A^{(k)}\) as \(A_{IJ}^{(k)} := \det(A_{IJ})\), we test the following two element-wise on the matrix minors:
%
\begin{equation} \label{eqn:other_combinatorial_operations}
%
    A_{IJ}^{(k)}  = 
        \begin{cases}
            \max(A_{IJ}) \qquad  &\text{`}\texttt{max}\text{'} \\
            \text{avg}(A_{IJ})  \qquad &\text{`}\texttt{avg}\text{'}
        \end{cases}
%
\end{equation}
%
where \(I\) and \(J\) are both subsets of rows and columns respectively. We find both of these operations perform worse than the determinant, possibly because they do not respect orthogonality for multiplicative adapters. We leave open the possibility that they may be performant alternatives for additive adapters (e.g. LoRA).

As we will demonstrate in the next sections, our adapters share a crucial flexibility with OFT/BOFT i.e. it's ability to adapt convolution layers natively. Originally, LoRA is only applied to the linear layers of the attention modules due to it's additive nature however multiplicative adapters can be used natively when finetuning convolution layers. Moreover, our adapters offer interpretability when applied to convolution layers in the sense that each compound matrix can transform the convolution kernel in a spatial manner. We leave a detailed study of the interpretation of compound adapters in vision tasks for future work.

\begin{table*}[htp!]
\centering
%
    \caption{Results on the GLUE development set, fine-tuning the pre-trained DeBERTaV3-base model. \# Params denotes the number of trainable parameters. Our method is evaluated with the best configuration, \(\mathcal{C'}=\mathcal{C}_1\oplus\mathcal{C}_2\), where orthogonality is enforced (\(\gamma=0\)), parameter sharing across blocks is disabled (\(\beta=0\)), and the number of blocks is set to \(b=3\).}
%
    \label{tab:glue-results}
    

\begin{tabular}{lcccccccccc}
\toprule
\textbf{Method} & \textbf{\# Params} & \textbf{MNLI} & \textbf{SST-2} & \textbf{CoLA} & \textbf{QQP} & \textbf{QNLI} & \textbf{RTE} & \textbf{MRPC} & \textbf{STS-B} & \textbf{All}\\
\midrule
Full Fine-tuning 
& 184M 
& 89.90 
& 95.63 
& 69.19 
& 92.40 
& 94.03 
& 83.75 
& 89.46 
& 91.60 
& 88.25 
\\

\(\text{LoRA}_{r=8}\)~\cite{hu2021lora} 
& 1.33M 
& 90.65 
& 94.95 
& 69.82 
& 91.99 
& 93.87 
& 85.20 
& 89.95 
& 91.60 
& 88.50 
\\

\(\text{OFT}_{b=16}\)~\cite{qiu2023controlling} 
& 0.79M 
& 90.33 
& 96.33 
& 73.91 
& 92.10 
& 94.07 
& 87.36 
& 92.16 
& 91.91 
& 89.77 
\\

\(\text{BOFT}_{m=2,b=8}\)~\cite{liu2023parameter} 
& 0.75M 
& 90.25 
& 96.44 
& 72.95 
& 92.10 
& 94.23 
& 88.81 
& 92.40 
& 91.92 
& 89.89 
\\

\midrule
\(\text{Compound}_{\mathcal{C}_1\oplus\mathcal{C}_2}\)
& 0.03M 
& 87.46
& 94.26 
& 64.57 
& 89.08
& 92.53 
& 81.22 
& 87.99 
& 90.16 
& 85.91 
\\
\bottomrule
\end{tabular}
\end{table*}


\section{Experimental Setup}
\label{sec:experimental_setup}



\subsection{Data}
\label{subsec:datasets}

We evaluate the effectiveness of our Quantum-Inspired Adapters by finetuning large language models on the General Language Understanding Evaluation (GLUE) benchmark~\cite{wang2018glue}, and vision foundation models on the Visual Task Adaptation (VTAB) benchmark~\cite{zhai2019large}. This  allows us to evaluate the generalization and robustness of our approach in various language understanding and visual task adaptation challenges.


\subsection{Model}
\label{subsec:model}

We utilize the pre-trained DeBERTaV3-base model~\cite{he2021debertav3} as the backbone for our natural language experiments. For vision tasks, we employ the pre-trained DINOv2-large model~\cite{oquab2023dinov2} as our backbone. 



\subsection{Adapter Configurations}
\label{ssec:adapter_configs}
To thoroughly evaluate the effectiveness of our Quantum-Inspired Adapters, we systematically varied several key aspects of their configuration, as discussed above. Our experimentation focused on different combinations of compound matrices based on Hamming-weights, the types of operations applied to these compounds, the enforcement of orthogonality, and the strategy for parameter sharing across adapter blocks.

Building upon this, we define compound matrices based on the Hamming-weight \( k \) up to a maximum \(K=3\), constructed with \((I, J)\)-minors such that \( |I| = |J| = k\). We uniquely characterize an experiment by a tuple \((\mathcal{C}', O, b,\gamma, \beta)\). \(\mathcal{C}'\) is a subset of all compound configurations (power set) constructed via direct sum, \(\mathcal{C}' \subseteq \mathcal{P}(\mathcal{C})^{\oplus 3}\).  
%
\begin{align}
%
    \mathcal{C} := \{\mathcal{C}_1, \mathcal{C}_2, \mathcal{C}_3\}, \ \  
    \mathcal{P}(\mathcal{C})^{\oplus 3} := \{\mathcal{C}_1, \mathcal{C}_2, \mathcal{C}_3, \mathcal{C}_1\oplus \mathcal{C}_2,
    \mathcal{C}_2\oplus \mathcal{C}_3, \mathcal{C}_2\oplus \mathcal{C}_3, \mathcal{C}_1\oplus \mathcal{C}_2\oplus \mathcal{C}_3\}
%
\end{align}
%
Note that this notation is slightly obfuscating - for a fixed pre-trained matrix and block size, \(d, b\), the base matrix compounded to create the configuration \(\mathcal{C}_1\) will be larger than the one used in \(\mathcal{C}_1\oplus \mathcal{C}_2\oplus \mathcal{C}_3\), \(\dim(A)_{\mathcal{C}_1} > \dim(A)_{\mathcal{C}_1\oplus \mathcal{C}_2\oplus \mathcal{C}_3}\) due to the dimension matching requirements. Therefore as the number of terms in the direct sum decreases along with the compound order, the number of trainable parameters is assumed to \emph{increase}. One could of course restrict the definition \(\mathcal{P}(\mathcal{C})^{\oplus}\) with a fixed base matrix size for all elements, and hence fixed number of parameters, but this may provide a bias in a different direction. As such, we keep the definition flexible and the implication of dimensions will be clear from context through the text.

Next, we have \( O \in \{\texttt{comp}, \texttt{max}, \texttt{avg}\} \), defined as one of the dimensionality-expanding operations on minors from ~\eqref{eqn:other_combinatorial_operations}. \texttt{comp} refers to the usual determinant operation on minors. Orthogonality in the adapter matrices is regulated by the binary configuration parameter \( \gamma \in \{0, 1\} \), with \(\gamma=0\) if orthogonality is enforced and \(\gamma=1\) otherwise. \(\gamma=0\) ensures the transformation preserves the norm and angles of the input feature vectors within the model.

Finally, \( \beta \in \{0, 1\} \) is a block-share parameter - if \(\beta = 1\), parameters are shared across adapter blocks and are distinct otherwise. A model with \(\beta = 1\) will have fewer overall parameters than \(\beta = 0\).

\subsection{Quantum native fine-tuning} \label{ssec:QC_implementation}
%
In this section we briefly describe the quantum-native implementation of our adapters directly on a quantum computer. We expand on this discussion and detail relevant terminology in Appendix~\ref{subsec:quantum_implementation}. We note that in parallel to our work~\cite{anonymous2025a} proposed some ideas for incorporating quantum circuits in fine-tuning pipelines. However, the authors put forward the \emph{hardware-efficient} family of circuits for fine-tuning, which are known to be problematic due to their problem-agnostic behavior. Their quantum applications have been shown to exhibit barren plateaus~\cite{mcclean2018barren}.  Furthermore, these circuits are not efficiently classically simulatable as the size of the pre-trained matrix, \(d\), grows, so no general quantum-inspired approach is possible. In contrast, our compound adapters are both efficiently classically simulatable when the maximum compound rank, \(K\), is kept small, and also may provide quantum advantage to implement on quantum computers when both \(d\) and \(K\), become large. Nevertheless, both the work of~\cite{anonymous2025a} and ours demonstrates the feasibility of using quantum and quantum-inspired machine learning for fine-tuning large learning models.

Now, compound adapters can be implemented on quantum computers using circuits composed of quantum gates known or Reconfigurable Beam Splitter (RBS) or their generalization into \emph{Fermionic} Beam Splitter (FBS) gates. It can be shown, that if complete circuits composed of such gates, act on data encoded within the \emph{Hamming-weights} of computational basis states the effective action on the input data can be exactly represented by compound matrices. For example, if we have a data vector \(\boldsymbol{x} \in \mathbb{R}^{\binom{n}{2}}\) encoded in the amplitudes of the quantum state, \(\ket{\psi(\boldsymbol{x})} = \frac{1}{||\boldsymbol{x}||}\sum_{e_k \in \text{Hamming-weight}^n_2} x_{e_k} \ket{e_k}\) where \(e_k\) is a bitstring over \(n\) (qu)bits with exactly \(2\) ones (and \(n-2\) zeros, e.g. \(0101, 1010, 1001, 0011, 1100, 0110\) for \(n=4\)), and we act with certain connectivities of FBS gates on this state, the effective action on the vector is exactly that of the second-order compound, \(\mathcal{C}_2 = A^{(2)}\)~\cite{kerenidis2022quantum}.

Combined with a suitable tomographic procedure, one may design circuits which can implement all of the above features we found useful - orthogonality, block-sharing and different compound orders. Furthermore, we could also design and use quantum circuits for compound adapters with much larger \(K\) than is possible to easily implement in a quantum-inspired way. This ability could open the door to even further compressed fine-tuning in the future.




\section{Results and Analysis}
\label{sec:results}

Our experiments demonstrate the effectiveness of compound adapters in achieving significant parameter efficiency while maintaining competitive performance across various GLUE benchmark tasks. In this section, we present an analysis of the trade-offs between parameter count and model accuracy, the combined impact of orthogonality and component-wise performance differences.


\subsection{Compound Adapters on GLUE dataset} \label{ssec:glue_dataset_results}
% Describe best performing config

We begin by comparing the best performing compound configuration on the GLUE dataset with the state of the art peft methods in Table~\ref{tab:glue-results}. This corresponds to the configuration \((\mathcal{C}_1\oplus \mathcal{C}_2, \texttt{comp}, b=3, \gamma=0, \beta=0)\), in other words - enforcing orthogonality without block-share over \(b= 3\) blocks. We see from the Table that, compared to LoRA, we have \(44\times\) fewer parameters with an average (relative) performance drop of only \(2.9\%\), with \(\approx 26\times\) and \(\approx 25\times\) fewer parameters than OFT and BOFT with only a \(4.3\%\) and \(4.4\%\) relative performance drop respectively. These figures are computed relative to the average performance over all GLUE datasets.




\begin{table}[h]
  \centering
  \caption{Summary of configurations with their respective parameter counts and accuracies on the STS-B dataset. We use 3 blocks across all configurations. The best configurations are in bold. If the base matrix is \(A\) then \(A^{(1)} =: \mathcal{C}_1\), \(A^{(2)} =: \mathcal{C}_2\) and \(A^{(3)} =: \mathcal{C}_3\).   Thus, choosing a configuration with higher Hamming-weight leads to reducing the number of parameters as the size of base matrix \(A\) shrinks.}
  \label{tab:param_efficiency}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Configuration}  & \textbf{Base matrix} & \textbf{Params} & \textbf{Accuracy (\%)}  \\
    \midrule
    \(\mathcal{C}_1 \equiv \text{OFT}\) 
      & \(\quad A \in \mathbb{R}^{256\times256}\)
      & \(1,\!770,\!241\)    
      & \(\mathbf{91.68}\) 
     \\[1mm]
    \(\mathcal{C}_2\) 
      & \(A \in \mathbb{R}^{23\times23}\)
      & \(38,\!401\) 
      & \(40.57\) 
       \\[1mm]
    \(\mathcal{C}_3\) 
       & \(A \in \mathbb{R}^{12\times12}\)
      & \(16,\!321\) 
      & \(42.20\) 
      \\[1mm]
    \(\mathcal{C}_1\oplus\mathcal{C}_2\) 
      & \(A \in \mathbb{R}^{22\times22}\)
      & \(33,\!217\) 
      & \(\mathbf{88.85}\) 
       \\[1mm]
    \(\mathcal{C}_1\oplus\mathcal{C}_3\) 
      & \(A \in \mathbb{R}^{12\times12}\)
      & \(16,\!321\) 
      & \(\mathbf{88.53}\) 
       \\[1mm]
    \(\mathcal{C}_2\oplus\mathcal{C}_3\) 
        & \(A \in \mathbb{R}^{11\times11}\)
      & \(13,\!057\) 
      & \(40.60\) 
     \\[1mm]
    \(\mathcal{C}_1\oplus\mathcal{C}_2\oplus\mathcal{C}_3\) 
      & \(A \in \mathbb{R}^{11\times11}\) 
      & \(13,\!057\) 
      & \(\mathbf{88.48}\) 
      \\
    \bottomrule
  \end{tabular}
\end{table}
\paragraph{Increasing parameters:} From Table~\ref{tab:param_efficiency} we can see two features of our adapters. First, the hyper compression offered by the combinatorial compounding operation, does not allow a large flexibility in changing the number of trainable parameters. Once a non-trivial compound matrix has been added to the adapter (i.e. of greater order than compound \(1\) which, when orthogonal, is equivalent to OFT), the parameter count reduces dramatically. This is due to the fixed size of the pre-trained layer, and the large relative size of high-order compounds to lower ones. We provide a visualization of this in the Appendix~\ref{app_ssec:compound_configs}. To address this, we can increase the parameter count monotonically by \emph{multiplying} several compound adapters. This is a general concept applicable to both additive or multiplicative adapters. For example, in Table~\ref{tab:glue-results_4_adapters} we demonstrate that using \(4\) multplicative adapters can improve the performance across all GLUE datasets. We only show the best four for compactness.

\begin{table}[htp!]
\centering
%
    \caption{Increasing parameter count in compound adapters. Trainable parameter count can be naturally increased by multiplying successive adapters, leading to performance boosts. Here we compare a single adapter, \(\Delta W_Q\) versus four, \(\prod _{k=1}^4(\Delta W^k_Q)\). We show the four GLUE datasets with the largest improvement.
    }
%
    \label{tab:glue-results_4_adapters}
    %
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Method} & \textbf{\# Params}   & \textbf{CoLA} & \textbf{RTE} & \textbf{MRPC} & \textbf{STS-B} \\
\midrule
Full Fine-tuning 
& 184M 
& 69.19 
& 83.75 
& 89.46 
& 91.60 
\\

\(\text{LoRA}_{r=8}\)
& 1.33M 
& 69.82  
& 85.20 
& 89.95 
& 91.60 
\\

\(\text{OFT}_{b=16}\)
& 0.79M 
& 73.91 
& 87.36 
& 92.16 
& 91.91 
\\

\(\text{BOFT}_{m=2,b=8}\)
& 0.75M 
& 72.95 
& 88.81 
& 92.40 
& 91.92 
\\

\midrule
\(\text{Compound}_{\mathcal{C}_1\oplus\mathcal{C}_2}\)
& 0.03M 
& 64.57 
& 81.22 
& 87.99 
& 90.16 
\\

\(\text{Compound}_{4 \times ( \mathcal{C}_1\oplus\mathcal{C}_2 )}\) 
& 0.14M 
& 65.83 
& 80.50 
& 86.27 
& 91.44 
\\
\bottomrule
\end{tabular}
\end{table}

The second observation from Table~\ref{tab:param_efficiency} is the first part of our ablation study. It is clear from these results that our compound adapters cannot be successful without the inclusion of the first order compound - the base matrix, \(A =: \mathcal{C}_1\), itself. We hypothesise this is due to the difficulty of gradient flow through the determinant operation to the parameters in \(A\), when \(A\) itself is not included. Also note, it is only the configuration, \(\mathcal{C}_1\) which is equivalent to OFT, as in this case a single compound matrix (i.e. the trivial one) fills the entire block. 


\paragraph{Impact of orthogonality:} The second ablation study we conduct is the impact of orthogonality on the compound adapters. Like the inclusion of the first order compound, we also find including orthogonality is critical for the success of the compound adapters. The possible reason for this is the preservation of orthogonality by determinants, which is reinforced when we study the removal of true compounding - i.e. computing determinants of minors - replaced with other combinatorial operations, such as \texttt{max} and \texttt{avg} (we conduct this ablation study in Appendix~\ref{ssec:effect_other_methods}). Even poorly performing compound configurations, such as those lacking the first order compound, see a significant performance boost when orthogonality is enforced. Finally, we note we show only the impact of orthogonality for \emph{multiplicative} adapters. One could also consider compound adapters in an additive form (similar to LoRA), which we leave to future work.

\begin{figure}[htp!]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/orthogonal_features_impact.pdf}
        \label{fig:ortho_impact}
    \end{subfigure}
  \caption{ Impact of enforcing orthogonality in adapters, for different compound configurations.}
    \label{fig:ortho_impact_main}
\end{figure}

\subsection{Compound Adapters on VTAB-1K dataset} \label{ssec:vtab_dataset_results}


We then compare the best performing compound configuration on the VTAB subset that we chose. We also reduce the number of examples in each dataset to create VTAB1k~\cite{zhai2019large} where \(1000\) random labelled datapoints are used for training an validation, but the final accuracies we show are computed on the entire original VTAB test dataset.
Similar to the configuration used on GLUE benchmark, we use \((\mathcal{C}_1\oplus \mathcal{C}_2, \texttt{comp}, b=3, \gamma=0, \beta=0)\).

We observe that, compared to \(\text{LoRA}_{r=4}\), our method has \(\approx 13.6\times\) fewer parameters while achieving an average relative performance drop of only 0.2\%. Similarly, our approach requires \(\approx 16.2\times\) and \(\approx 15.3\times\) fewer parameters than \(\text{OFT}_{b=16}\) and \(\text{BOFT}_{m=2,b=8}\), with a 0.5\% and 0.8\% relative performance drop, respectively. These figures are computed relative to the average accuracy across all VTAB tasks in our subset. Interestingly, in contrast with the other datasets across vision and NLP we test, CIFRAR100 stands out as having significantly \emph{increased} accuracy relative to other methods, on the order of \(10\%\).

\begin{table}[htp!]
\centering
% \small
\caption{Results on a subset of the VTAB1k benchmark, fine-tuning the pre-trained DINOv2-large model. \# Params denotes the number of trainable parameters.Our method is evaluated with the best configuration, \(\mathcal{C'}=\mathcal{C}_1\oplus\mathcal{C}_2\), where orthogonality is enforced (\(\gamma=0\)), parameter sharing across blocks is disabled (\(\beta=0\)), and the number of blocks is set to \(b=3\).
}
\label{tab:vtab_subset_results}
%
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{\# Params (M)} & \textbf{CIFAR100} & \textbf{Pets} & \textbf{SVHN} & \textbf{Resisc45} & \textbf{DMLab} \\
\midrule
Full Finetuning 
& 304.4M 
& 67.6 
& 93.7 
& 92.8 
& 90.9 
& 58.1 \\

    
\(\text{LoRA}_{r=4}\)
& 1.77M 
& 77.2 
& 94.8
& 94.7
& 91.4
& 58.1 \\
    
\(\text{OFT}_{b=16}\)
& 2.10M 
& 77.7 
& 94.7 
& 92.9 
& 91.5 
& 60.5  \\
    
\(\text{BOFT}_{m=2,b=8}\)
& 1.99M 
& 78.1 
& 95.0 
& 93.0 
& 91.6
& 61.4  \\
    
\midrule
\(\text{Compound}_{\mathcal{C}_1\oplus\mathcal{C}_2}\) 
& 0.13M 
& 87.5 
& 94.04 
& 89.98
& 88.79 
& 54.74 \\

\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

This work presents a new paradigm for parameter-efficient fine-tuning, leveraging quantum-inspired principles to construct effective adapters with minimal additional parameters. Our results indicate that compound matrix-based adapters can serve as a promising alternative to existing PEFT methods (encompassing them in some cases), achieving substantial parameter reduction while maintaining strong performance across a range of NLP and vision tasks.

Our experiments reveal that maintaining orthogonality and selecting an appropriate compound matrix configuration are crucial for preserving model performance. Furthermore, our method's adaptability across different benchmarks underscores its generalizability and potential for broader applications in resource-constrained environments.

Future work will explore extending these ideas to more complex architectures, further optimizing adapter design, and investigating potential quantum speedups for compound matrix operations. By bridging quantum-inspired techniques with deep learning, we hope to advance the field of efficient fine-tuning and enable scalable adaptation of large foundation models in practical settings. 


\bibliography{references}
\bibliographystyle{unsrtnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}


\subsection{Quantum Implementation}
\label{subsec:quantum_implementation}

Our adapters, can be implemented efficiently on quantum hardware using fixed Hamming-weight encoders and Hamming-weight preserving circuits. In this section, we detail their implementation on quantum hardware.


\subsubsection{RBS gate}
 A Reconfigurable Beam Splitter \(RBS\) gate is a two qubit gate parameterized with one angle \(\theta \in [0, 2\pi]\). \(RBS(\theta)_{ij}\) acting on the \(i\)-th and \(j\)-th qubits implements a Givens rotation:

\[
\mathsf{RBS}_{ij}(\theta) =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos(\theta) & \sin(\theta) & 0 \\
0 & -\sin(\theta) & \cos(\theta) & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\]


This is a Hamming-weight-preserving gate which is easy to implement on many quantum devices with compilations needing upto \(2\) CNOT gates with a Pauli basis native gate set. Another Hamming-weight-preserving gate known as Fermionic Beam Splitter (\(FBS\)) gate which is a generalisation of \(RBS\) gate could also be used to implement Hamming-weight-preserving circuits. The application of a \(\text{FBS}\) between the qubits \(i\) and \(j\) , \(\mathsf{FBS}_{ij}(\theta)\) , acts as \(RBS_{ij}(\theta)\) if the parity of the qubits between \(i\) and \(j\) is even, and is the conjugate gate \(RBS_{i,j} (-\theta)\) otherwise. Therefore, in the case of unary inputs or nearest neighbour connectivity, \(FBS\) and \(RBS\) gates behave identically. The \(FBS_{ij}\) is a non local gate that can be implemented using an RBS gate together with \(\mathcal{O}(|i - j|)\) additional two qubit parity gates with a circuit of depth \( \mathcal{O}(\log(|i - j|))\). We leave the discussion of quantum adapters using other Hamming-weight preserving (or more specifically particle number preserving) schemes based on Linear Optics~\cite{monbroussou2024towards} circuits for future work. 


\subsubsection{Loaders}

We shall use amplitude encoding to load classical data into the amplitudes of a quantum state. This involves mapping a data vector \(x\) to a quantum state where the amplitudes of the basis states are proportional to the elements of \(x\).

Unary encoding~\cite{johri2021nearest, Landman2022quantummethods} is an amplitude encoding scheme that loads data into the amplitudes of computational basis states where each basis state has a Hamming-weight of 1. It uses \(d\) qubits to encode a \(d\)-dimensional vector. Efficient quantum data encoders using \(\mathcal{O}(d) \) two-qubit gates and \(\mathcal{O}(\log d)\) depth are known in the unary basis as shown in Fig~\ref{fig:parallel-loader}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.2\linewidth]{figures/fig-parallel-loader.pdf}
    \caption{\textbf{A Unary loader.} Dots and dashes denote parameterised RBS gates. Figure from ~\cite{Cherrat2023quantumdeephedging}}
    \label{fig:parallel-loader}
\end{figure}

Fixed Hamming-weight (Hamming-weight-k) ~\cite{farias2024quantum} encoding is an amplitude encoding scheme that loads a data vector into a subspace of fixed Hamming-weight $k$. It uses \(n\) qubits to encode a data vector of size  \(d=\)\({n}\choose{k} \) , with \( n  \in \mathcal{O}(k d^{1/k}) \). The circuit is constructed using a sequence of controlled (RBS) gates. The total CNOT-gate count for Hamming-weight-k encoding is \( \mathcal{O}(kd) \)~\cite{farias2024quantum}. This type of encoding is an intermediate regime between unary and binary encodings.




For our work, we require a quantum circuits capable of loading data vectors into subspaces of varying Hamming-weights, specifically from Hamming-weight 1 up to a maximum Hamming-weight k. This can be achieved by utilizing a series of fixed Hamming-weight (Hamming-weight-k) encoders, each dedicated to loading data into a subspace of a specific Hamming-weight. To load data up to Hamming-weight k, we can sequentially stack the Hamming-weight-k encoders for each weight from 1 to k.
The total number of qubits required is still n, but the total number of basis states becomes $\sum_{k=1}^K \binom{n}{k}$. This technique is distinct from a full binary encoder that includes all Hamming-weights from 0 to n. The overall CNOT gate count for such a construction can be expressed as the sum of CNOT gates for individual Hamming-weight-\(k\) encoders, where $k$ varies from 1 to \(K\), i.e.,
\begin{equation}
%
    \text{Total CNOT count} = \sum_{k=1}^{K} \mathcal{O}\left(k \binom{n}{k}\right)) \leq \mathcal{O} (d \log d),  \ \text{where} \  d = {{n}\choose{K}}  
    %
\end{equation}




\subsubsection{Layers}

Let $G(i, j, \theta)$ denote the Givens rotation applied to the $i$-th and $j$-th unary basis vector, i.e. $e_i$ and $e_j$, $\boldsymbol{\theta}$ a vector of angles, and $\mathcal{T}$ is a list of triplets $(i, j, m)$. The Hamming-weight-preserving layer is defined by: $$U(\boldsymbol{\theta}) = \prod_{(i, j, m) \in \mathcal{T}}\text{RBS}_{ij}(\theta_{m}). $$ 

It acts as $ U(\boldsymbol{\theta})\ket{\boldsymbol{x}} = W\ket{\boldsymbol{x}}$ where $W =  \prod_{(i, j, m) \in \mathcal{T}}G(i, j, \theta_{m})$.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.35\textwidth} % Left figure (larger width)
        \centering
        \includegraphics[width=\linewidth]{figures/fig-pyramid-layer.pdf}
        \caption{Pyramid Layer}
        \label{fig:pyramid-layer}
    \end{subfigure}
    \hspace{0.01\textwidth} % Small spacing between figures
    \begin{subfigure}[b]{0.20\textwidth} % Right figure (smaller width)
        \centering
        \includegraphics[width=\linewidth]{figures/fig-butterfly-layer.pdf}
        \caption{Butterfly Layer}
        \label{fig:butterfly-layer}
    \end{subfigure}
    \caption{\textbf{Hamming-weight preserving layers.} Dots and dashes denote parameterised RBS gates. Figure from ~\cite{Cherrat2023quantumdeephedging}.}
    \label{fig:hw-layers}
\end{figure*}

There are different circuits for $U(\boldsymbol{\theta})$, highlighted in Figure \ref{fig:hw-layers}. The Pyramid architecture, as described in ~\cite{Landman2022quantummethods}, consists of $n(n-1)/2$ RBS gates arranged in a pyramid-like structure and has a linear depth. This architecture allows for the representation of all possible orthogonal matrices of size $n \times n$. The Butterfly architecture, which was proposed in \cite{cherrat2024quantum}, in uses logarithmic depth circuits with a linear number of gates to implement a quantum orthogonal layer. This architecture, classical Cooley–Tukey
algorithm  used for Fast Fourier Transform, requires all-to-all connectivity in the hardware layout. 

% \subsubsection{Measurement}

% We can measure independently in the Z basis and then postprocess later.

\subsubsection{Quantum Implementation}

We can use these tools to construct quantum native implementation of our adapters as shown in figure~\ref{fig:quantum-implementation}. The block diagonal structure of our adapters imply that the adapters can be implemented via separate quantum circuits. For example in figure~\ref{fig:comp1-quantum}, a 4 block $\mathcal{C}_1$ adapter can be implemented via 4 quantum circuits, each with Hamming-weight-1 loaders, a Hamming-weight-preserving layer and suitable measurements. Enforcing block share in this setting would imply the circuit layers sharing the same parameter values, however, the loaders still ought to be different. Similarly in figure~\ref{fig:comp1-comp2-comp3-quantum}, we use 2 quantum circuits each with Hamming-weight-1, Hamming-weight-2 and Hamming-weight-3 loaders stacked one after another. Note that as specified in the binary encoders of ~\cite{farias2024quantum}, we would need parameterised controlled $R_Y$ gates between each loader to enable sequential stacking.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.6\textwidth} % Adjust width for balance
        \centering
        \includegraphics[width=\linewidth]{figures/comp1-quantum.pdf}
        \caption{}
        \label{fig:comp1-quantum}
    \end{subfigure}
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.7\textwidth} % Adjust width for balance
        \centering
        \includegraphics[width=\linewidth]{figures/comp1-comp2-comp3-quantum.pdf}
        \caption{}
        \label{fig:comp1-comp2-comp3-quantum}
    \end{subfigure}
    
    \caption{\textbf{Quantum Implementation of Adapters}. Each compound adapter has an efficient quantum implementation using fixed Hamming-weight encoders and Hamming-weight preserving layers. Trailing dimensions are padded with an identity matrix. The figure shows quantum circuits for a) $\mathcal{C}_1$,  \(b=4\) blocks, which uses only Hamming-weight 1 loaders and b)  $\mathcal{C}_1\oplus \mathcal{C}_2\oplus\mathcal{C}_3$, \(b=2\) blocks which uses upto Hamming-weight 3 loaders.  }
    \label{fig:quantum-implementation}
\end{figure}

\subsection{Ablation studies on STS-B dataset} 

To further understand the impact of different configuration setups, we run ablation studies on one dataset STS-B. 

\subsubsection{Compound Configurations}\label{app_ssec:compound_configs}
% Describe impact of conpound configs

As illustrated in Figure~\ref{fig:param_efficiency_figure}, we explore how different configurations of compound adapters perform on the STS-B dataset - an illustration of Table~\ref{tab:param_efficiency} in the main text. We note that the presence of $\mathcal{C}_1$ adapter with higher orders show the best performance while giving significant parameter reductions compared to \emph{only} having higher order adapters ($\mathcal{C}_2$ or $\mathcal{C}_3$).

\begin{figure}[htp!]
    \centering
    % \begin{subfigure}{\linewidth}
        \includegraphics[width=0.6\linewidth]{figures/compound_adapter_efficiency_performance.pdf}
        \caption{Visualization of performance versus parameter count for different adapter combinations.}
    \label{fig:param_efficiency_figure}
\end{figure}

\subsubsection{Orthogonality} \label{ssec:effect_orthogonality}
% Describe impact of ortho

To better understand the impact of keeping the adapter parameters orthogonal, we reran the experiments on STS-B but without cayley parameterization. The results are compared with their orthogonal counterpart in Figure ~\ref{fig:ortho_impact_main}.

\begin{table}[h]
\centering
\caption{STS-B performance comparison for orthogonal vs non-orthogonal implementations, for different compound configurations. The best performing option is in bold.
}
    \label{tab:ortho_impact} 
    %
        \begin{tabular}{lcc}
            \toprule
            \textbf{Configuration} & \textbf{Orthogonal} & \textbf{Non-Orthogonal}  \\
            \midrule
            $\mathcal{C}_1$ & \textbf{91.68} & 85.62 \\
            $\mathcal{C}_2$ & \textbf{40.57} & 15.82 \\
            $\mathcal{C}_3$ & \textbf{42.20} & 13.99  \\
            $\mathcal{C}_1\oplus\mathcal{C}_2$ & \textbf{88.85} & 33.29 \\
            $\mathcal{C}_1\oplus\mathcal{C}_3$ & \textbf{88.53} & 14.53 \\
            $\mathcal{C}_1\oplus\mathcal{C}_2\oplus\mathcal{C}_3$ & \textbf{88.48} & 13.99  \\
            \bottomrule
        \end{tabular}
\end{table}


\subsubsection{Constructing adapters from alternate operations on minors} \label{ssec:effect_other_methods}
% Describe impact of max/av

We also reran the experiments on STS-B with different operations on the minors as detailed in Section ~\ref{ssec:adapter_configs}. The results are compiled in Figure~\ref{fig:minor-operations}.

\begin{figure}[h]
    \centering
    % \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/compound_types_comparison.pdf}
    
    \caption{STS-B performance comparison across different operations and compound combinations. \texttt{max} and \texttt{avg} denotes taking the element wise maximum and average of the minors respectively compared to taking the determinant (\texttt{comp})}
    \label{fig:minor-operations}
\end{figure}

\subsubsection{Rank and Multi-adapter Analysis} \label{ssec:rank_analysis}



We delve into the impact of varying rank options and the number of adapters on the performance of different compound patterns on the STS-B dataset.  For each pattern, we evaluate the average accuracy achieved with different rank options (4, 8, 16) and varying numbers of adapters (1 and 4). Additionally, we consider the number of parameters associated with each configuration to assess parameter efficiency alongside performance. We find that in terms of absolute performance, \(\mathcal{C}_1 \oplus \mathcal{C}_2\) with \(4\) adapters with rank \(r=4\) is the best adapter, however - an optimal tradeoff between high accuracy and low parameter count is achieved with \(\mathcal{C}_1 \oplus \mathcal{C}_2\) with only \(1\) adapter with rank \(r=4\). For this reason, we use the configuration \(\mathcal{C}_1 \oplus \mathcal{C}_2\) for the majority of the experiments in the main text.

\begin{table}[h]
    \centering
    \caption{Impact of Rank \(r=\sfrac{d}{b}\) and Number of Adapters on Accuracy and Parameter Count. The best performing configuration in absolute performance is in bold. The results are also visualised in Figure~\ref{fig:rank_analysis}.
    }
    \label{tab:rank_multi_adapter_analysis}
    \resizebox{0.7\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Compound Pattern} & \textbf{\# Adapters} & \textbf{Rank}, \(r=\sfrac{d}{b}\) & \textbf{Avg Accuracy (\%)} & \textbf{Parameters (K)} \\
        \hline
        \multirow{6}{*}{$\mathcal{C}_1 \oplus \mathcal{C}_2$} & \multirow{3}{*}{1} & 4  & 90.22 & 35.4 \\
         &  & 8  & 89.38 & 33.2 \\
         &  & 16 & 89.25 & 31.9 \\
         \cline{2-5}
         & \multirow{3}{*}{\textbf{4}} & \textbf{4}  & \textbf{91.39} & \textbf{139.4} \\
         &  & 8  & 90.60 & 130.6 \\
         &  & 16 & 90.23 & 125.2 \\
        \hline
        \multirow{6}{*}{$\mathcal{C}_1 \oplus \mathcal{C}_3$} & \multirow{3}{*}{1} & 4  & 88.51 & 12.4 \\
         &  & 8  & 89.39 & 16.3 \\
         &  & 16 & 89.01 & 19.6 \\
         \cline{2-5}
         & \multirow{3}{*}{4} & 4  & 89.61 & 47.2 \\
         &  & 8  & 90.19 & 62.98 \\
         &  & 16 & 89.11 & 76.03 \\
        \hline
        \multirow{6}{*}{$\mathcal{C}_1 \oplus \mathcal{C}_2 \oplus \mathcal{C}_3$} & \multirow{3}{*}{1} & 4  & 88.25 & 10.4 \\
         &  & 8  & 89.13 & 13.1 \\
         &  & 16 & 88.96 & 14.6 \\
         \cline{2-5}
         & \multirow{3}{*}{4} & 4  & 89.89 & 39.2 \\
         &  & 8  & 89.02 & 49.9 \\
         &  & 16 & 89.15 & 56.1 \\
        \hline
    \end{tabular}
    }
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/rank_analysis.pdf}
    \caption{Relationship Between Rank, \(r:=\sfrac{d}{b}\), Number of Adapters, and Accuracy Across Compound Patterns}
    \label{fig:rank_analysis}
\end{figure}



Figure \ref{fig:rank_analysis} complements the Table~\ref{tab:rank_multi_adapter_analysis} by visually illustrating the trends in accuracy relative to rank and the number of adapters for each compound pattern. The plot highlights the positive correlation between rank and accuracy, as well as the benefits of employing multiple adapters in enhancing model performance.





\end{document}