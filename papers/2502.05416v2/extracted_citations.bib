@inproceedings{Djolonga2017,
 author = {Djolonga, Josip and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Differentiable Learning of Submodular Models},
 volume = {30},
 year = {2017}
}

@article{Hendriks2020LinearlyCN,
  title={Linearly Constrained Neural Networks},
  author={Johannes N. Hendriks and Carl Jidling and Adrian G. Wills and Thomas Bo Sch{\"o}n},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.01600}
}

@article{LAMBONI2022199,
title = {Efficient dependency models: Simulating dependent random variables},
journal = {Mathematics and Computers in Simulation},
volume = {200},
pages = {199-217},
year = {2022},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2022.04.018},
author = {Matieyendou Lamboni},
keywords = {Dependent generalized sensitivity indices, Dependent variables, Efficient sampling, Multivariate distributions, Random values},
abstract = {Dependency functions of dependent variables are relevant for (i) performing uncertainty quantification and sensitivity analysis in presence of dependent variables and/or correlated variables, and (ii) simulating random dependent variables. In this paper, we mathematically derive practical dependency functions for classical multivariate distributions such as Dirichlet, elliptical distributions and independent uniform (resp. gamma and Gaussian) variables under constraints that are ready to be used. Since such dependency models are used for sampling random values and we have many dependency models for every joint cumulative distribution function, we provide a way for choosing the efficient sampling function using multivariate sensitivity analysis. We illustrate our approach by means of numerical simulations.}
}

@inproceedings{Tschiatschek2018DifferentiableSM,
  title={Differentiable Submodular Maximization},
  author={Sebastian Tschiatschek and Aytunc Sahin and Andreas Krause},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{WangICML23,
  title={{LinSATNet}: The Positive Linear Satisfiability Neural Networks},
  author={Wang, Runzhong and Zhang, Yunhao and Guo, Ziao and Chen, Tianyi and Yang, Xiaokang and Yan, Junchi},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023}
}

@article{ahmed2022semantic,
  title={Semantic probabilistic layers for neuro-symbolic learning},
  author={Ahmed, Kareem and Teso, Stefano and Chang, Kai-Wei and Van den Broeck, Guy and Vergari, Antonio},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29944--29959},
  year={2022}
}

@inproceedings{ahmed2022simple,
  author 	= {Ahmed, Kareem and Zeng, Zhe and Niepert, Mathias and Van den Broeck, Guy},
  title     = {SIMPLE: A Gradient Estimator for k-subset sampling},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  month     = {may},
  year      = {2023},
}

@InProceedings{amos2017optnet,
  title = {{O}pt{N}et: Differentiable Optimization as a Layer in Neural Networks},
  author = {Brandon Amos and J. Zico Kolter},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages = {136--145},
  year = {2017},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  publisher ={PMLR},
}

@article{badreddine2022logic,
  title={Logic tensor networks},
  author={Badreddine, Samy and Garcez, Artur d'Avila and Serafini, Luciano and Spranger, Michael},
  journal={Artificial Intelligence},
  volume={303},
  pages={103649},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{chen2018learning,
  title={Learning to explain: An information-theoretic perspective on model interpretation},
  author={Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={883--892},
  year={2018},
  organization={PMLR}
}

@article{chen2024hard,
title = {Physics-informed neural networks with hard linear equality constraints},
journal = {Computers Chemical Engineering},
volume = {189},
pages = {108764},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108764},
author = {Hao Chen and Gonzalo E. Constante Flores and Can Li},
keywords = {Surrogate modeling, Physics-informed neural network, Artificial intelligence},
abstract = {Surrogate modeling is used to replace computationally expensive simulations. Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems. Despite this, neural networks are data-driven models and devoid of any physics. The incorporation of physics into neural networks can improve generalization and data efficiency. The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions. This work proposes a novel physics-informed neural network, KKT-hPINN, which rigorously guarantees hard linear equality constraints through projection layers derived from KKT conditions. Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive distillation subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy.}
}

@inproceedings{de2007problog,
  title={ProbLog: A probabilistic Prolog and its application in link discovery},
  author={De Raedt, Luc and Kimmig, Angelika and Toivonen, Hannu},
  booktitle={IJCAI 2007, Proceedings of the 20th international joint conference on artificial intelligence},
  pages={2462--2467},
  year={2007},
  organization={IJCAI-INT JOINT CONF ARTIF INTELL}
}

@article{di2020efficient,
  title={Efficient generation of structured objects with constrained adversarial networks},
  author={Di Liello, Luca and Ardino, Pierfrancesco and Gobbi, Jacopo and Morettin, Paolo and Teso, Stefano and Passerini, Andrea},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={14663--14674},
  year={2020}
}

@article{diligenti2012bridging,
  title={Bridging logic and kernel machines},
  author={Diligenti, Michelangelo and Gori, Marco and Maggini, Marco and Rigutini, Leonardo},
  journal={Machine learning},
  volume={86},
  pages={57--88},
  year={2012},
  publisher={Springer}
}

@inproceedings{donti2017task,
  title={Task-based end-to-end model learning in stochastic optimization},
  author={Donti, Priya and Amos, Brandon and Kolter, J Zico},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5484--5494},
  year={2017}
}

@inproceedings{fischer2019dl2,
  title={DL2: training and querying neural networks with logic},
  author={Fischer, Marc and Balunovic, Mislav and Drachsler-Cohen, Dana and Gehr, Timon and Zhang, Ce and Vechev, Martin},
  booktitle={International Conference on Machine Learning},
  pages={1931--1941},
  year={2019},
  organization={PMLR}
}

@article{garcez2023neurosymbolic,
  title={Neurosymbolic AI: The 3 rd wave},
  author={Garcez, Artur d’Avila and Lamb, Luis C},
  journal={Artificial Intelligence Review},
  volume={56},
  number={11},
  pages={12387--12406},
  year={2023},
  publisher={Springer}
}

@article{giunchiglia2021multi,
  title={Multi-label classification neural networks with hard logical constraints},
  author={Giunchiglia, Eleonora and Lukasiewicz, Thomas},
  journal={Journal of Artificial Intelligence Research},
  volume={72},
  pages={759--818},
  year={2021}
}

@inproceedings{kim2016exact,
  title={Exact sampling with integer linear programs and random perturbations},
  author={Kim, Carolyn and Sabharwal, Ashish and Ermon, Stefano},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{misino2022vael,
  title={Vael: Bridging variational autoencoders and probabilistic logic programming},
  author={Misino, Eleonora and Marra, Giuseppe and Sansone, Emanuele},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4667--4679},
  year={2022}
}

@Article{risks6030064,
AUTHOR = {Vrins, Frédéric},
TITLE = {Sampling the Multivariate Standard Normal Distribution under a Weighted Sum Constraint},
JOURNAL = {Risks},
VOLUME = {6},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {64},
ISSN = {2227-9091},
ABSTRACT = {Statistical modeling techniques—and factor models in particular—are extensively used in practice, especially in the insurance and finance industry, where many risks have to be accounted for. In risk management applications, it might be important to analyze the situation when fixing the value of a weighted sum of factors, for example to a given quantile. In this work, we derive the (n−1)-dimensional distribution corresponding to a n-dimensional i.i.d. standard Normal vector Z=(Z1,Z2,…,Zn)′ subject to the weighted sum constraint w′Z=c, where w=(w1,w2,…,wn)′ and wi≠0. This law is proven to be a Normal distribution, whose mean vector μ and covariance matrix Σ are explicitly derived as a function of (w,c). The derivation of the density relies on the analytical inversion of a very specific positive definite matrix. We show that it does not correspond to naive sampling techniques one could think of. This result is then used to design algorithms for sampling Z under constraint that w′Z=c or w′Z≤c and is illustrated on two applications dealing with Value-at-Risk and Expected Shortfall.},
DOI = {10.3390/risks6030064}
}

@article{shukla2024unified,
  title={A Unified Approach to Count-Based Weakly Supervised Learning},
  author={Shukla, Vinay and Zeng, Zhe and Ahmed, Kareem and Van den Broeck, Guy},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{stoian2024exploiting,
  title = "Exploiting T-norms for Deep Learning in Autonomous Driving",
  author = "Mihaela Catalina Stoian and Eleonora Giunchiglia and Thomas Lukasiewicz",
  year = "2023",
  booktitle = "Proceedings of the 17th International Workshop on Neural-Symbolic Learning and Reasoning, NeSy 2023, La Certosa di Pontignano, Siena, Italy, 3--5 July 2023",
  editor = "Artur S. d'Avila Garcez and Tarek R. Besold and Marco Gori and Ernesto Jiménez-Ruiz",
  month = "July",
  pages = "369--380",
}

@inproceedings{wilder2019melding,
 author = {Wilder, Bryan},
 title = {Melding the Data-Decisions Pipeline: Decision-Focused Learning for Combinatorial Optimization},
 booktitle = {Proceedings of the 33rd AAAI Conference on Artificial Intelligence},
 year = {2019}
}

@inproceedings{xu2018semantic,
  title={A semantic loss function for deep learning with symbolic knowledge},
  author={Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and Broeck, Guy},
  booktitle={International conference on machine learning},
  pages={5502--5511},
  year={2018},
  organization={PMLR}
}

