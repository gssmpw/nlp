\section{Related Work}
Our work lies in the field of neuro-symbolic AI where integrating constraints as background knowledge into deep learning models is widely studied____. 

\textbf{Constraint Enforcement.}
There are multiple ways to enforce constraints in the neural networks.
Some directly incorporate background knowledge as network layers____.
In the context of generative tasks, ____ embed propositional logic constraints into Generative Adversarial Networks (GANs) for structured object generation, while ____ integrate probabilistic logic programming ____ with Variational Autoencoders (VAEs). 
% \zz{TODO: to fix, fixed}
____ impose linear operator constraints within the architecture of feedforward neural networks, but their approach does not generalize to other model architectures. 
____ enforce positive linear constraints using a Sinkhorn algorithm, where their method is only applicable to output variables being unit hypercubes. 
____ formulate constraint satisfaction as an optimization problem. They use the $\ltwo$ distance and derive projections based on the Karush–Kuhn–Tucker (KKT) conditions. 
% Meanwhile, 
____ and ____ integrate quadratic programming solvers as differentiable modules within end-to-end trainable deep networks
while some other works formulate it as submodular optimization problems____, ____, and ____. 
Most of these approaches ignore underlying data distributions when solving optimization problems
while in our method we leverage the information from the constrained distribution for optimizing the DGMs.
Quantitative comparisons between our method and existing work are further presented in the experimental section.



\textbf{Constrained Sampling.}
There are some existing works in the field of statistical modeling that study how to sample from linearly constrained Gaussian distributions that can be potentially applied as post-processing steps.
For example, ____ investigates a linear weighted constraint for independent standard Gaussian variables, while ____ focuses on a fixed-sum constraints for independent Gaussian variables with zero means.
However, these methods are not differentiable and thus cannot be incorporated into the training of DGMs, 
leading to low data efficiency.


\textbf{Exactly-k Constraints.}
The discrete counterpart of linear equality constraints, 
the exactly-k constraints defined as $\sum_i x_i = k$ with $x_i$ being categorical variables is studied by many.
% Regarding the topic of estimating gradients, 
____ and ____ propose similar ideas to refactor the non-differentiable sample from a categorical distribution with a differentiable sample from Gumbel-Softmax distributions. 
Other gradient estimators for this constraint
either employ variants of score function and straight-through estimator or propose certain relaxations____. 
Closely related to our work is
a recently introduced gradient estimator____ that leverages the
constrained marginal distribution as an informative proxy for differentiation.




\textbf{Soft Constraints.}
A line of research integrates the constraints by optimizing for the probability of constraint satisfaction, encouraging the model to generate compliant samples____.
This is achieved by modifying the loss function with differentiable constraint probabilities.
However, these methods do not guarantee the satisfaction of the constraint.