\section{Related Work}
Our work lies in the field of neuro-symbolic AI where integrating constraints as background knowledge into deep learning models is widely studied~\citep{garcez2023neurosymbolic}. 

\textbf{Constraint Enforcement.}
There are multiple ways to enforce constraints in the neural networks.
Some directly incorporate background knowledge as network layers~\citep{ahmed2022semantic, giunchiglia2021multi}.
In the context of generative tasks, \citet{di2020efficient} embed propositional logic constraints into Generative Adversarial Networks (GANs) for structured object generation, while \citet{misino2022vael} integrate probabilistic logic programming \citep{de2007problog} with Variational Autoencoders (VAEs). 
% \zz{TODO: to fix, fixed}
\citet{Hendriks2020LinearlyCN} impose linear operator constraints within the architecture of feedforward neural networks, but their approach does not generalize to other model architectures. 
\citet{WangICML23} enforce positive linear constraints using a Sinkhorn algorithm, where their method is only applicable to output variables being unit hypercubes. 
\citet{chen2024hard} formulate constraint satisfaction as an optimization problem. They use the $\ltwo$ distance and derive projections based on the Karush–Kuhn–Tucker (KKT) conditions. 
% Meanwhile, 
\citet{amos2017optnet} and \citet{donti2017task} integrate quadratic programming solvers as differentiable modules within end-to-end trainable deep networks
while some other works formulate it as submodular optimization problems~\citet{Djolonga2017}, \citet{Tschiatschek2018DifferentiableSM}, and \citet{wilder2019melding}. 
Most of these approaches ignore underlying data distributions when solving optimization problems
while in our method we leverage the information from the constrained distribution for optimizing the DGMs.
Quantitative comparisons between our method and existing work are further presented in the experimental section.



\textbf{Constrained Sampling.}
There are some existing works in the field of statistical modeling that study how to sample from linearly constrained Gaussian distributions that can be potentially applied as post-processing steps.
For example, \citet{risks6030064} investigates a linear weighted constraint for independent standard Gaussian variables, while \citet{LAMBONI2022199} focuses on a fixed-sum constraints for independent Gaussian variables with zero means.
However, these methods are not differentiable and thus cannot be incorporated into the training of DGMs, 
leading to low data efficiency.


\textbf{Exactly-k Constraints.}
The discrete counterpart of linear equality constraints, 
the exactly-k constraints defined as $\sum_i x_i = k$ with $x_i$ being categorical variables is studied by many.
% Regarding the topic of estimating gradients, 
\citet{maddison2016concrete} and \citet{jang2016categorical} propose similar ideas to refactor the non-differentiable sample from a categorical distribution with a differentiable sample from Gumbel-Softmax distributions. 
Other gradient estimators for this constraint
either employ variants of score function and straight-through estimator or propose certain relaxations~\citep{kim2016exact,chen2018learning,grover2019stochastic,Sang2019reparameterizable}. 
Closely related to our work is
a recently introduced gradient estimator~\citep{ahmed2022simple} that leverages the
constrained marginal distribution as an informative proxy for differentiation.




\textbf{Soft Constraints.}
A line of research integrates the constraints by optimizing for the probability of constraint satisfaction, encouraging the model to generate compliant samples~\citep{diligenti2012bridging,xu2018semantic,fischer2019dl2,badreddine2022logic,stoian2024exploiting,shukla2024unified}.
This is achieved by modifying the loss function with differentiable constraint probabilities.
However, these methods do not guarantee the satisfaction of the constraint.