\section{Background and related work}
\label{gen_inst}
We consider a multilayered perceptron with $L$ layers, each layer $l$ computes its output as $\boldsymbol{h}_l = f(W_l \boldsymbol{h}_{l-1})$, where $W_l$ is the weight matrix and $f$ is an element-wise activation function. The input to the network is $\boldsymbol{h}_0 = \boldsymbol{x}$, and the final network output is $\hat{\boldsymbol{y}} = f_L(W_L \boldsymbol{h}_{L-1})$, which approximates the target $\boldsymbol{y}$. The \textit{task dimensionality}, denoted $d$, is at most the number of components in $\boldsymbol{y}$ and $\hat{\boldsymbol{y}}$.
Training the network involves minimizing a loss function $\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})$ by adjusting the weights $\{W_l\}$. The error signal at the output layer, $\delta_L = \frac{\partial \mathcal{L}}{\partial \hat{\boldsymbol{y}}}$, is a $d$-dimensional vector, typically much smaller than the number of neurons in the hidden layers.

\textbf{Backpropagation (BP)}Bengio et al., "Learning Long-Term Dependencies with Gradient Descent is Difficult,"____ is the standard approach for training neural networks. It propagates the error backward through the network using $\delta_l = W_{l+1}^T \delta_{l+1} \odot f'(W_l \boldsymbol{h}_{l-1})$, and updates the weights using $\Delta W_l = -\eta \delta_l \boldsymbol{h}_{l-1}^T$, where $\eta$ is the learning rate. However, this method requires an exact transposition of the forward weights, $W_{l+1}^T$, which is biologically implausible____. Moreover, BP tightly couples the error propagation with the forward pass, limiting the ability to explore how different properties of the error signal affect learning dynamics.

\textbf{Feedback Alignment (FA)}Lillicrap et al., "Backpropagation and the compute explosion,"____ was proposed to address the biological limitations of BP by replacing $W_{l+1}^T$ with a fixed random matrix $B_l$. The error is computed as:
\begin{equation}\label{eq:FAerror}
    \delta_l = B_l \delta_{l+1} \odot f'(W_l \boldsymbol{h}_{l-1}),
\end{equation}
decoupling the forward and backward weights and providing a more biologically plausible mechanism. However, FA struggles to scale effectively in deep architectures, such as convolutional neural networks (CNNs), where it often fails to achieve competitive performance____.

An extension of FA involves adapting $B_l$ by updating it alongside the forward weights $W_l$ to improve their alignment____:
\begin{equation}\label{eq:FAupdate}
    \Delta B_l = -\eta \boldsymbol{h}_{l-1} \delta_l^T - \lambda B_l, \quad
    \Delta W_l = -\eta \delta_l \boldsymbol{h}_{l-1}^T - \lambda W_l,
\end{equation}
where $\lambda$ is a regularization parameter. Although this adaptive approach improves performance by better aligning forward and backward weights, it still requires high-dimensional error signals and struggles to match BP performance in complex architectures like CNNs. Furthermore, in Section \ref{sec:linear}, we show that this approach fails when the matrix $B$ is low-rank and the dimensionality of the error is constrained.

Other studies have explored the use of \textit{fixed} sparse feedback matrices to reduce the dimensionality of error propagation____. However, these approaches result in significantly lower performance and do not provide a systematic framework for studying how error constraints affect learning and representation formation.

Beyond FA-based methods, several studies have shown that weight updates using backpropagation can result in a low-dimensional weight update____ and favor low-rank solutions____.  These findings support our hypothesis that low-dimensional feedback is sufficient to train deep networks. However, no previous work has considered training with a constrained error pathway, and the effects of error dimensionality and training have not been systematically studied.

In this work, our objective is to systematically investigate how constraining the dimensionality of the error signal affects the training and performance of large neural networks. To this end, we introduce a novel learning scheme that enables deep layers to be learned using low-rank feedback matrices, which allow flexible control over the dimensionality of the loss gradients in each layer (Figure \ref{fig:schematics}). 

\textbf{Our main contributions are}:
\begin{enumerate}
    \item We present a novel local learning rule for low-rank feedback weights, which matches BP performance while requiring minimal error signals. We provide a detailed derivation of the learning dynamics in a simple linear case, establishing the foundation for why and how the feedback weights should be learned.
    \item We demonstrate that large and deep nonlinear networks can efficiently learn nontrivial datasets using low-dimensional error signals and highlight that the feedback dimensionality should scale with the task rather than the size of the network.
    \item We successfully train complex yet highly useful architectures, such as convolutional networks and visual transformers, using low-dimensional feedback. 
    \item We study a simplified model of the ventral visual system and show that the emerging receptive fields depend on the error-feedback dimensionality. These findings offer new insights into the relationship between learning mechanisms and biological neural representations.
\end{enumerate}

In the final section, we discuss the broader implications of our results for both neuroscience and machine learning.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/LFDA_schematics.pdf}
  \caption{\textit{Illustration of different approaches for propagating error to hidden layers.} 
From left to right: \textbf{Backpropagation (BP)}Bengio et al., "Learning Long-Term Dependencies with Gradient Descent is Difficult,"____ propagates error using the exact transpose of the forward weights. 
\textbf{Feedback Alignment (FA)}Lillicrap et al., "Backpropagation and the compute explosion,"____ replaces the transposed weights with fixed random feedback matrices, which gradually align with the forward weights during training. Our work framework provides \textbf{low-dimensional feedback (LDF) }using low-rank matrices, allowing full control of the dimnsionality of the gradients in each layer. Finally, direct low-dimensional feedback (dLDF)
extends our LDF architecture by enabling error signals to bypass intermediate layers, propagating directly from the output layer or other non-adjacent layers.
}

    \label{fig:schematics}
\end{figure}