\section{Experiments: A Comprehensive Study}

Despite numerous innovations in temporal action detection, evaluating their generalizability across different methods remains challenging. In this section, we leverage our unified framework, OpenTAD, with its modularized codebase to conduct a comprehensive study, fairly comparing various module designs and systematically analyzing how individual components, model sizes, overall architectures, and other factors affect TAD performance. 
We perform experiments with various TAD methods on the two most widely adopted datasets: ActivityNet-v1.3~\cite{caba2015activitynet} and THUMOS-14~\cite{jiang2014thumos}. We describe the evaluation metrics and implementation details in Sec.~\ref{sec:eval_implement_details} of the supplementary material.

\subsection{Which design for each component yields the best performance?}\label{sec:component_importance}

As described in Sec.~\ref{sec:categorization}, various design choices have been proposed for each component in the TAD literature. Among all the different variants, is there an absolute winner for each component?

\textbf{Neck.} This is an essential component present in almost every TAD method. As shown in Tab.~\ref{tab:supported_methods}, many methods propose innovative designs in the neck to enhance the temporal aggregation of video sequence features. Examples include the simple MaxPooling in TemporalMaxer~\cite{tang2023temporalmaxer}, the convolutional layer in BSN~\cite{lin2018bsn} and BMN~\cite{lin2019bmn}, the GCN layer in GTAD~\cite{xu2020g} and VSGN~\cite{zhao2021video}, the Transformer block in ActionFormer~\cite{zhang2022actionformer}, and the Mamba block in VideoMambaSuite~\cite{chen2024video}. 
We incorporate these neck designs into four different TAD methods: ActionFormer~\cite{zhang2022actionformer} and TriDet~\cite{shi2023tridet}, which produce multi-scale features, and GTAD~\cite{xu2020g} and BMN~\cite{lin2019bmn}, which produce single-scale features. We then compare their contributions to the final performance for each method, as shown in Tab.~\ref{tab:ablate_neck_thumos} and Tab.~\ref{tab:ablate_neck_anet} in the supplementary material.
\textit{Macro Block} in Column 1 refers to the highest-level repetitive building blocks, such as the following: (a) Transformer block, which contains a sequential modeling module and an MLP module, each with a residual connection; (b) Mamba block, which consists of a sequential modeling branch combined with a gated MLP branch; and (c) Block mix, which is a combination of the above two types of blocks. We partition Tab.~\ref{tab:ablate_neck_thumos} and Tab.~\ref{tab:ablate_neck_anet} into \textbf{four regions} based on the \textit{macro blocks}. Besides, \textit{Sequential Module} in Column 2 refers to the core sequential modeling module within the macro block, such as self-attention and SSM.

\begin{table}[t]
\centering
\caption{\textbf{Analysis of the neck design choices}, measured by average mAP(\%) on THUMOS-14. We ran all experiments 5 times and report their mean and standard deviation. The \textbf{4 macro-block regions} mean the following respectively. 
\textbf{Top region}: macro blocks with their original sequential modules are adopted as a whole; \textbf{Transformer block}:  self-attention modules in Transformer blocks are replaced with different sequential modules; \textbf{Mamba block}:  SSM modules in Mamba blocks are replaced with different sequential modules; \textbf{Bottom region}: a combination of two blocks.
}
\small
\setlength{\tabcolsep}{4.8pt}
\begin{tabular}{llccccc}
\toprule
\multicolumn{2}{c}{\textbf{Neck }} && \multicolumn{4}{c}{\textbf{Method}} \\
\addlinespace[2pt]
\cline{1-2} 
\cline{4-7} 
\addlinespace[4pt]
\textbf{Macro Block} &\multicolumn{1}{c}{\textbf{Sequential Module}} && {\textbf{ActionFormer}} & {\textbf{TriDet}} & {\textbf{BMN}} & {\textbf{GTAD}} \\
\midrule
Convolution Block& Convolution~\cite{lin2019bmn}    & & 52.58{\scriptsize $\pm$1.54} & 67.92{\scriptsize $\pm$0.22} & 48.43{\scriptsize $\pm$0.38} & 49.62{\scriptsize $\pm$0.22} \\         
GCN Block & Graph convolution~\cite{xu2020g}            & & 67.48{\scriptsize $\pm$0.22} & 68.32{\scriptsize $\pm$0.39} & 48.84{\scriptsize $\pm$0.26} & 50.36{\scriptsize $\pm$0.29}   \\
Transformer Block & Self-sttention~\cite{zhang2022actionformer} && 67.93{\scriptsize $\pm$0.19} & 68.58{\scriptsize $\pm$0.20} & \textbf{50.36}{\scriptsize $\pm$0.56} & \textbf{50.97}{\scriptsize $\pm$0.31}   \\
Mamba Block & SSM~\cite{chen2024video} && \textbf{68.28}{\scriptsize $\pm$0.48}  & \textbf{68.64}{\scriptsize $\pm$0.18} & 49.39{\scriptsize $\pm$0.33}        & 50.22{\scriptsize $\pm$0.35}  \\
\midrule
\multirow{5}{*}{\makecell[l]{Transformer Block}} 
&Convolution      && 68.11{\scriptsize $\pm$0.36} & 68.46{\scriptsize $\pm$0.52} & 48.73{\scriptsize $\pm$0.15} & 50.31{\scriptsize $\pm$0.16}  \\
&Graph convolution             & & 67.85{\scriptsize $\pm$0.26} & 68.51{\scriptsize $\pm$0.11} & 48.84{\scriptsize $\pm$0.26} & 50.33{\scriptsize $\pm$0.18}  \\
&Self-attention       && 67.93{\scriptsize $\pm$0.19} & 68.58{\scriptsize $\pm$0.20} & 50.36{\scriptsize $\pm$0.56} & 50.97{\scriptsize $\pm$0.31}  \\
&LSTM             && 68.10{\scriptsize $\pm$0.33} & 68.53{\scriptsize $\pm$0.27} & \textbf{51.66}{\scriptsize $\pm$0.35} & \textbf{53.31}{\scriptsize $\pm$0.27}  \\
& SSM             && \textbf{68.23}{\scriptsize $\pm$0.31} & \textbf{68.79}{\scriptsize $\pm$0.37}  & 49.23{\scriptsize $\pm$0.28} & 50.09{\scriptsize $\pm$0.35}  \\
\midrule
\multirow{5}{*}{\makecell[l]{Mamba  Block}} 
& Convolution     && 68.25{\scriptsize $\pm$0.34} & 68.61{\scriptsize $\pm$0.41} & 48.40{\scriptsize $\pm$0.20} & 50.11{\scriptsize $\pm$0.40} \\
& Graph convolution                && 68.02{\scriptsize $\pm$0.29} & 68.50{\scriptsize $\pm$0.45} & 48.73{\scriptsize $\pm$0.28} & 49.84{\scriptsize $\pm$0.33} \\
& Self-attention       && 68.24{\scriptsize $\pm$0.22} & 68.63{\scriptsize $\pm$0.26} & \textbf{49.57{\scriptsize $\pm$0.72}} & \textbf{50.89}{\scriptsize $\pm$0.70} \\
& LSTM            && 67.82{\scriptsize $\pm$0.29} & \textbf{68.85}{\scriptsize $\pm$0.56} & 49.20{\scriptsize $\pm$0.27} & 49.87{\scriptsize $\pm$0.31} \\
& SSM             && \textbf{68.28}{\scriptsize $\pm$0.48}  & 68.64{\scriptsize $\pm$0.18} & 49.39{\scriptsize $\pm$0.33}        & 50.22{\scriptsize $\pm$0.35}  \\
\midrule
% \multirow{4}{*}{\makecell[l]{Block Mix}} 
Mamba + Mamba &SSM + SSM && 68.03{\scriptsize $\pm$0.38}  & 68.09{\scriptsize $\pm$0.87} & 49.66{\scriptsize $\pm$0.17} &  50.40{\scriptsize $\pm$0.48} \\
Transf. + Transf.&LSTM + Self-attention && 67.78{\scriptsize $\pm$0.31}  & 68.59{\scriptsize $\pm$0.62} & 50.91{\scriptsize $\pm$0.27} &  53.46{\scriptsize $\pm$0.56} \\
Mamba + Transf.&SSM + Self-attention  && 68.37{\scriptsize $\pm$0.22} & 68.41{\scriptsize $\pm$0.31} & 49.76{\scriptsize $\pm$0.46} & 50.60{\scriptsize $\pm$0.39}             \\
\textbf{Mamba + Transf.} &\textbf{SSM + LSTM} && \textbf{68.44}{\scriptsize $\pm$0.41} & \textbf{68.90}{\scriptsize $\pm$0.60} & \textbf{52.19}{\scriptsize $\pm$0.28} & \textbf{53.70}{\scriptsize $\pm$0.26} \\
\bottomrule
\end{tabular}
\label{tab:ablate_neck_thumos}
\end{table}

From the top region in Tab.~\ref{tab:ablate_neck_thumos}, we identify two winners: the Transformer block and the Mamba block, both of which feature more sophisticated macro architectures than the others. The Transformer block achieves the best performance for the single-scale methods BMN (50.36\%) and GTAD (50.97\%), while the Mamba block performs best for the multi-scale methods ActionFormer (68.28\%) and TriDet (68.64\%).
To further investigate which sequential modeling module within each macro architecture is the best choice, we replace the self-attention module in the Transformer block and the SSM module in the Mamba block with one of five sequential modeling modules: convolution, graph convolution, LSTM, self-attention, and SSM. We present the results in the \textit{Transformer Block} and \textit{Mamba Block} regions of Tab.~\ref{tab:ablate_neck_thumos} and Tab.~\ref{tab:ablate_neck_anet}. SSM demonstrates a clear advantage for the multi-scale methods ActionFormer and TriDet, while LSTM and self-attention exhibit superior performance for the single-scale methods BMN and GTAD. We attribute the strong performance of these three modules to their ability to perform temporal aggregation across the entire sequence, in contrast to the others.
It is also worth noting that when integrated into the Transformer block and the Mamba block, the performance of local sequential modeling modules—convolution ($52.58\% \rightarrow 68.11\%$ in ActionFormer) and GCN ($67.48\% \rightarrow 67.85\%$ in ActionFormer)—improves significantly compared to when they are used alone on THUMOS-14.

\textit{Can we identify a neck design that performs best across all methods?} Considering the outstanding performance of the Transformer and Mamba blocks, as well as the LSTM module, we propose a novel macro block that combines these two different blocks, as shown in the bottom region of Tab.~\ref{tab:ablate_neck_thumos} and Tab.~\ref{tab:ablate_neck_anet}. By mixing Mamba and Transformer macro blocks—where the self-attention in the Transformer block is replaced with LSTM—we achieve the highest performance across all four methods on both datasets, leading to an additional $0.8\%$ improvement on THUMOS-14 over the best-performing macro block with the optimal sequential module.



\textbf{RoI Extraction.} This component is utilized in two-stage methods and determines the quality of the proposal features used in Stage 2. Keypoint sampling, employed in PBRNet~\cite{liu2020progressive} and VSGN~\cite{zhao2021video}, extracts only the boundary and center features within a proposal. RoI Align~\cite{he2017mask} utilizes all features, while SGAlign~\cite{xu2020g}, which builds upon RoI Align, further enhances it by incorporating a GCN layer. Boundary matching, introduced in BMN~\cite{lin2019bmn}, densely extracts proposals based on every pair of start and end boundaries. We evaluate these four RoI strategies using three two-stage methods, with the results presented in Tab.~\ref{tab:ablate_RoI}.  

For both BMN and GTAD, boundary matching achieves the highest performance, while keypoint sampling yields the lowest. This is because both methods use a limited number of features due to their single-scale nature and the relatively small number of temporal length (e.g., 100 for ActivityNet). Given these constraints, the computational cost of using boundary matching to extract the largest number of proposals is acceptable. In contrast, for VSGN, which employs multi-scale feature maps and a larger number of input frames (e.g., 1280 frames for ActivityNet), applying dense boundary matching is infeasible, and keypoint sampling proves to be as effective as RoI Align. 
Therefore, the choice of RoI extraction method depends on specific properties of each approach, such as feature temporal scales. Since multi-scale, long-input methods like VSGN generally outperform conventional methods such as BMN and GTAD, boundary matching is generally not preferred.

\begin{table}[t]
\centering
\caption{\textbf{Analysis of the RoI extraction design choices}, measured by average mAP (\%) on two datasets. N/A means not applicable to the model. 
}
\small
\setlength{\tabcolsep}{5.3pt}
\begin{tabular}{lccccccc}
\toprule
&\multicolumn{3}{c}{\textbf{ActivityNet-v1.3}} & & \multicolumn{3}{c}{\textbf{THUMOS-14}} \\
\addlinespace[2pt]
\cline{2-4} 
\cline{6-8} 
\addlinespace[4pt]
\textbf{RoI Extraction} & \textbf{BMN} & {\textbf{GTAD}} & \textbf{VSGN} & & \textbf{BMN} &{\textbf{GTAD}} & \textbf{VSGN} \\
\midrule
Keypoint Sample~\cite{zhao2021video}    & 34.29{\scriptsize $\pm$0.07} & 32.68{\scriptsize $\pm$0.05} & \underline{36.71}{\scriptsize $\pm$0.13}  & & 43.36{\scriptsize $\pm$0.24} & 45.34{\scriptsize $\pm$0.59} & \underline{52.42}{\scriptsize $\pm$0.31}  \\
RoI Align~\cite{he2017mask}             & 36.06{\scriptsize $\pm$0.05} & 35.99{\scriptsize $\pm$0.13} & \textbf{36.74}{\scriptsize $\pm$0.02} & & \underline{49.48}{\scriptsize $\pm$0.67} & 49.40{\scriptsize $\pm$0.46} & \textbf{52.53}{\scriptsize $\pm$0.41} \\
SGAlign~\cite{xu2020g}                  & \underline{36.28}{\scriptsize $\pm$0.12} & \underline{36.24}{\scriptsize $\pm$0.04} & 36.66{\scriptsize $\pm$0.14} & & 49.00{\scriptsize $\pm$0.45} & \underline{50.36}{\scriptsize $\pm$0.29} & 52.38{\scriptsize $\pm$0.64} \\
{Boundary Match~\cite{lin2019bmn}}    & \textbf{36.44}{\scriptsize $\pm$0.07} & \textbf{36.34}{\scriptsize $\pm$0.07} & N/A & &\textbf{51.40}{\scriptsize $\pm$0.58} & \textbf{50.48}{\scriptsize $\pm$0.35} & N/A \\



\bottomrule
\end{tabular}
\label{tab:ablate_RoI}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{Analysis of the backbone design choices}, measured by average mAP (\%). Please note that all experiments use only the RGB modality, excluding the optical flow modality. Backbone models with * have been pre-pretrained using self-supervised learning. \textbf{Top region}: Resnet-based backbone models; \textbf{bottom region}: Transformer-based backbone models including VideoSwin and ViT architectures. `Param.': number of model parameters; K400: Kinetics-400; Top-1: top-1 accuracy (\%); `ActionF.': ActionFormer. We \colorbox{lightgray!50}{highlight} three backbones that don't have consistent action recognition and TAD performance.}
\small
\setlength{\tabcolsep}{1.3pt}
\begin{tabular}{lcccccccccccc}
\toprule
\multicolumn{1}{c}{\textbf{Backbone}} && \textbf{Param.} && \textbf{K400} &&\multicolumn{3}{c}{\textbf{ActivityNet-v1.3}} && \multicolumn{3}{c}{\textbf{THUMOS-14}} \\
\addlinespace[2pt]
\cline{3-3} 
\cline{5-5} 
\cline{7-9} 
\cline{11-13} 
\addlinespace[4pt]
\multicolumn{1}{c}{\textbf{Model}} && \textbf{(M)} && \textbf{Top-1} &&  {\textbf{ActionF.}} & {\textbf{GTAD}} & \textbf{VSGN} && {\textbf{ActionF.}} & {\textbf{GTAD}} & \textbf{VSGN} \\
\midrule
TSN-R50~\cite{TSN2016ECCV}  && 24 && 74.12  && 34.64{\scriptsize $\pm$0.07} & 34.09{\scriptsize $\pm$0.09} &33.36{\scriptsize $\pm$0.18} && 49.79{\scriptsize $\pm$0.37} & 35.18{\scriptsize $\pm$0.32} & 32.22{\scriptsize$\pm$0.79}\\
TSM-R50~\cite{lin2019tsm} && 24  && 75.12	&& 34.51{\scriptsize $\pm$0.08} & 33.70{\scriptsize $\pm$0.09} &34.35{\scriptsize $\pm$0.21}& &50.94{\scriptsize $\pm$0.30} & 34.59{\scriptsize $\pm$0.15} &35.09{\scriptsize$\pm$0.77} \\
R(2+1)D-R34~\cite{tran2018closer} && 64 && 75.46  && 35.06{\scriptsize $\pm$0.09} & 34.11{\scriptsize $\pm$0.06}&34.83{\scriptsize $\pm$0.22} && 48.39{\scriptsize $\pm$0.39} & 34.12{\scriptsize $\pm$0.23} & 39.08{\scriptsize$\pm$0.76} \\
% TSP          & ResNet-34   & -      & 37.00{\scriptsize $\pm$0.05} &     & 55.60{\scriptsize $\pm$0.34} & 41.62{\scriptsize $\pm$0.}  \\
% \addlinespace[2pt]
% \hdashline
% \addlinespace[4pt]
% SlowFast-R50*~\cite{slowfast}  && 76.80 &&                               &                             &                             && 62.93{\scriptsize $\pm$0.06} & 47.43{\scriptsize $\pm$0.49}  & 51.30{\scriptsize $\pm$0.51} \\
SlowFast-R101~\cite{slowfast} && 63 && 78.65	&& 35.95{\scriptsize $\pm$0.12} & 35.28{\scriptsize $\pm$0.12} &35.98{\scriptsize $\pm$0.19} && 63.04{\scriptsize $\pm$0.33} & 47.08{\scriptsize $\pm$0.28}  & 50.54{\scriptsize$\pm$0.22} \\
\midrule
\rowcolor{lightgray!50} 
VideoSwin-S~\cite{vswin} && 50 && 80.54  && 35.60{\scriptsize $\pm$0.13} & 35.24{\scriptsize $\pm$0.10} & 32.85{\scriptsize $\pm$0.22} && 56.72{\scriptsize $\pm$0.70} & 38.99{\scriptsize $\pm$0.32} & 42.14{\scriptsize $\pm$0.56}   \\
\rowcolor{lightgray!50} 
VideoSwin-L~\cite{vswin} && 197 && 83.46  && 35.91{\scriptsize $\pm$0.03} & 35.73{\scriptsize $\pm$0.10} & 32.93{\scriptsize $\pm$0.32} && 58.78{\scriptsize $\pm$0.38} & 41.55{\scriptsize $\pm$0.16} & 45.66{\scriptsize$\pm$0.59}\\
\addlinespace[2pt]
\hdashline
\addlinespace[4pt]
\rowcolor{lightgray!50}  
MViTv2-L~\cite{li2022mvitv2}     && 213    && 85.40  && 35.96{\scriptsize $\pm$0.18} & 35.30{\scriptsize $\pm$0.12} &34.23{\scriptsize $\pm$0.18} && 47.62{\scriptsize $\pm$0.48} & 31.27{\scriptsize $\pm$0.51} & 41.78{\scriptsize$\pm$0.47}\\
VideoMAE-H*~\cite{tong2022videomae}   && 633    && 87.40  && 36.96{\scriptsize $\pm$0.07} & 36.57{\scriptsize $\pm$0.07} &35.07{\scriptsize $\pm$0.32} && 67.23{\scriptsize $\pm$0.32} & 48.16{\scriptsize $\pm$0.37}  & 53.93{\scriptsize$\pm$0.34}\\
VideoMAEv2-G*~\cite{wang2023videomae2}  && 1B    && 88.40  && - & - &- && 71.48{\scriptsize $\pm$0.12} & 54.16{\scriptsize $\pm$0.59} & 54.94{\scriptsize$\pm$0.64}\\
InternVideo2-6B*~\cite{wang2024internvideo2} && 6B  && \textbf{92.10}  && \textbf{38.95}{\scriptsize $\pm$0.13} & \textbf{37.35}{\scriptsize $\pm$0.12} &\textbf{38.63}{\scriptsize $\pm$0.09} && \textbf{72.36}{\scriptsize $\pm$0.13}  & \textbf{56.32}{\scriptsize $\pm$0.16} & \textbf{58.16}{\scriptsize $\pm$0.66}\\
\bottomrule
\end{tabular}
\vspace{-5pt}
\label{tab:ablate_backbone}
\end{table}


\textbf{Backbone}. It produces video features, which are to be further processed by the subsequent components of the TAD framework (e.g., the neck and the heads) to make predictions. Therefore, the quality of the video features and the backbone is critical, significantly influencing the detection performance. A backbone network is usually pretrained on a larger-scale action recognition dataset such as Kinetics~\cite{zisserman2017kinetics} before being trained for TAD, and example backbones are TSN~\cite{TSN2016ECCV}, SlowFast~\cite{slowfast}, and VideoSwin~\cite{vswin}. Recently,  ViT~\cite{dosovitskiy2021image} is adopted as video backbone network, pre-pretrained with self-supervised learning, multi-modal joint learning, or other methods (e.g., VideoMAE~\cite{tong2022videomae}, InternVideo2~\cite{wang2024internvideo2}), and has shown remarkable performance on TAD. In Tab.~\ref{tab:ablate_backbone}, we present experiments across three TAD methods and two datasets,  evaluating different backbone models. These models vary in architecture, including ResNet-based and Transformer-based designs, and in pretraining strategies, including action recognition pretraining and additional pre-pretraining.

In Tab.~\ref{tab:ablate_backbone}, the backbone model sizes are arranged in non-decreasing order from top to bottom within each group: ResNet-based backbones in the top region and Transformer-based backbones in the bottom region.
We observe that action recognition performance on Kinetics-400 consistently improves as model sizes increase (Column 2), and that all the Transformer-based models perform better than the ResNet-based models. However, this trend does not always hold for TAD.  Though VideoSwin-S~\cite{vswin} and MViTv2-L~\cite{li2022mvitv2}   have significantly higher action recognition accuracy than SlowFast-101~\cite{slowfast}, their mAPs are lower for most TAD experiments.  Similarly, while MViTv2-L outperforms VideoSwin-L on Kinetics, it performs worse on THUMOS-14 for TAD. We attribute these inconsistencies across tasks to the architectural differences and suggest that ViT-based models are more effective for TAD. Notably, the largest ViT-based model, InternVideo2-6B~\cite{wang2024internvideo2} achieves the highest performance across all methods and datasets.



\subsection{Is the optional Stage 2 necessary?}
Stage 2 is optional in the TAD framework, as it is only used in two-stage methods. While one-stage methods were originally introduced to improve efficiency in object detection~\cite{sermanet2013overfeat}, the question remains: is Stage 2 necessary for achieving higher accuracy?

To answer this question, we compare the performance of methods with and without Stage 2 across three datasets in Tab.~\ref{tab:ablate_category_stage2}, including originally one-stage, two-stage, and DETR-based methods. For one-stage methods, we incorporate Stage 2 by using RoI Align~\cite{he2017mask} for RoI extraction and applying the same classification and regression losses as in Stage 1. We define positive samples as proposals with $\textrm{tIoU}>0.7$ relative to the ground-truth actions. For two-stage methods, we entirely remove Stage 2, using Stage 1 predictions as the final output for ablation. Additionally, since the DETR-based method TadTR~\cite{liu2022end} includes a second-stage refinement, we also evaluate its performance without this stage for comparison. As shown in Tab.\ref{tab:ablate_category_stage2}, adding Stage 2 improves the performance of the one-stage method ActionFormer\cite{zhang2022actionformer}, while removing Stage 2 reduces the performance of the two-stage method VSGN~\cite{zhao2021video} and the DETR-based method TadTR.  These results indicate that Stage 2 can further enhance one-stage methods, and is essential for two-stage methods.


\begin{table}[t]
\centering
\caption{\textbf{Ablation study of Stage 2} on one-stage, two-stage, and DETR-based methods, measured by average mAP (\%). N/A means training details are not provided in the original paper or code.}
\small
\begin{tabular}{llccc}
\toprule
\textbf{Category} & \textbf{Method} &\textbf{ActivityNet-v1.3} & \textbf{THUMOS-14}   & \textbf{HACS} \\
\midrule
\multirow{2}{*}{{One-stage}}      
& ActionFormer~\cite{zhang2022actionformer}              & 37.00{\scriptsize $\pm$0.05} & 67.93{\scriptsize $\pm$0.19} & 37.69{\scriptsize $\pm$0.05}\\
& ActionFormer + Stage 2    & \textbf{37.29}{\scriptsize $\pm$0.03} & \textbf{68.61}{\scriptsize $\pm$0.29} & \textbf{38.25}{\scriptsize $\pm$0.12}\\
\midrule
\multirow{2}{*}{{Two-stage}}
& VSGN - Stage 2 & 36.18{\scriptsize $\pm$0.07} & 49.71{\scriptsize $\pm$0.38} & 36.02{\scriptsize $\pm$0.20} \\
& VSGN~\cite{zhao2021video}           & \textbf{36.71}{\scriptsize $\pm$0.13}  & \textbf{52.42}{\scriptsize $\pm$0.31}  & \textbf{37.19}{\scriptsize $\pm$0.12} \\
\midrule
\multirow{2}{*}{{DETR-based}} & TadTR - Stage 2 & N/A& 52.75{\scriptsize $\pm$0.55} & N/A \\
& TadTR~\cite{liu2022end}  & N/A & \textbf{54.42}{\scriptsize $\pm$0.72} & N/A\\
\bottomrule
\end{tabular}
\label{tab:ablate_category_stage2}
\end{table}



\subsection{What makes an optimal detector?}

Through the above analysis within our unified OpenTAD framework, we have identified the optimal design choices for the neck, RoI extraction, and backbone components. This raises an intriguing question: can we further enhance existing TAD methods by applying these winning techniques?

In Tab.~\ref{tab:apply_all_winners}, we use ActionFormer as the baseline (Row 1) and progressively apply the winning techniques—excluding backbone modifications—across different components (Rows 2–5). We evaluate the impact of these changes on two datasets. As expected, performance improves progressively as more techniques are applied, culminating in a model with mAP improvements of 0.62/0.88 over the original. Next, we separately replace the backbone with the winning InternVideo2 model and then apply all other techniques (Rows 6–7). While backbone replacement alone substantially boosts performance, incorporating additional modifications further enhances the results, achieving an overall improvement of 0.56/1.06 mAP.

In Tab.~\ref{tab:sota_results}, we apply all the modifications—except for the backbone replacement—to five different TAD methods across two datasets, consistently improving their performance. Notably, for the recent method VideoMambaSuite~\cite{chen2024video}, which utilizes the powerful InternVideo2 backbone, our modifications further enhance its results, achieving state-of-the-art performance on both ActivityNet-v1.3 and THUMOS-14.

\begin{table}[t]
\centering
\caption{\textbf{Progressive modification of components in ActionFormer~\cite{zhang2022actionformer}} , performance measured by average mAP (\%). \textbf{Row 1}: baseline ActionFormer with TSP features for ActivityNet-v1.3 and I3D features for THUMOS-14; \textbf{Top region}: progressive adding or removing a component or a technique; \textbf{Bottom region}: replacing the backbone and applying all the modifications in the top region.}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Setting} & \textbf{ActivityNet-v1.3} & \textbf{THUMOS-14} \\
\midrule
ActionFormer (TSP backbone | I3D backbone): baseline      & 37.00{\scriptsize $\pm$0.05} & 67.93{\scriptsize $\pm$0.19}   \\
\midrule
- Remove the transformer block & 36.04{\scriptsize $\pm$0.33} & 67.52{\scriptsize $\pm$0.36}    \\
+ Add block mix & 37.50{\scriptsize $\pm$0.07}  & 68.44{\scriptsize $\pm$0.41} \\
+ Add the second stage to refine proposals & 37.59{\scriptsize $\pm$0.08} & 68.63{\scriptsize $\pm$0.33} \\
+ Use input channel dropout, longer training epochs & \textbf{37.62}{\scriptsize $\pm$0.02} & \textbf{68.81}{\scriptsize $\pm$0.43}  \\
\midrule 
Replace the backbone with InternVideo2  
& 38.95{\scriptsize $\pm$0.13} & 72.36{\scriptsize $\pm$0.13}   \\
+ All the above modifications & \textbf{39.51}{\scriptsize $\pm$0.09} & \textbf{73.42}{\scriptsize $\pm$0.40} \\
\bottomrule
\end{tabular}
\label{tab:apply_all_winners}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{Results on ActivityNet-v1.3 and THUMOS-14}, measured by mAP (\%) at different tIoU thresholds. TSP~\cite{alwassel2021tsp} features are used for ActivityNet, and I3D~\cite{carreira2017quo} features are used for THUMOS for all methods except VideoMambaSuite, which uses the InternVideo2~\cite{wang2024internvideo2} features for both datasets.}
\small
\setlength{\tabcolsep}{4.7pt}
\begin{tabular}{lcccacccccca}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{ActivityNet-v1.3}} & \multicolumn{6}{c}{\textbf{THUMOS-14}} \\
\addlinespace[2pt]
\cline{2-5} 
\cline{7-12} 
\addlinespace[4pt]
& \textbf{0.5} & \textbf{0.75} & \textbf{0.95} & \textbf{Avg.}  & & \textbf{0.3} & \textbf{0.4} & \textbf{0.5} & \textbf{0.6} & \textbf{0.7} & \textbf{Avg.} \\
\midrule
GTAD~\cite{xu2020g}         & 52.33 & 37.58 & 8.42 & 36.20 & & 64.35 & 59.07 & 51.76 & 42.65 & 31.66 & 49.70 \\
\textbf{GTAD +}             & 52.46 & 37.86 & 9.55 & \textbf{36.67} & & 67.08 & 62.87 & 56.81 & 46.68 & 35.74 & \textbf{53.83} \\  
\addlinespace[2pt]
\hdashline
\addlinespace[4pt]
BMN~\cite{lin2019bmn}       & 52.90 & 37.30 & 9.67 & 36.40 & & 64.99 & 60.70 & 54.54 & 44.11 & 34.16 & 51.80 \\
\textbf{BMN +}              & 53.82 & 38.14 & 9.92 & \textbf{37.04} & & 66.82 & 61.94 & 55.22 & 45.16 & 34.11 & \textbf{52.65} \\
\addlinespace[2pt]
\hdashline
\addlinespace[4pt]
ActionFormer~\cite{zhang2022actionformer}  & 55.08 & 38.27 & 8.91 & 37.07 & & 83.78 & 80.06 & 73.16 & 60.46 & 44.72 & 68.44  \\
\textbf{ActionFormer +} & 55.76 & 39.13 & 7.43 & \textbf{37.68} & & 83.05 & 79.20 & 72.20 & 62.73 & 47.25 & \textbf{68.89}\\
\addlinespace[2pt]
\hdashline
\addlinespace[4pt]
TriDet~\cite{shi2023tridet} & 54.84 & 37.46 & 7.98 & 36.51 & & 84.46 & 81.05 & 73.41 & 62.58 & 46.51 & 69.60 \\
\textbf{TriDet +} & 55.49 & 38.65 & 8.95 & \textbf{37.47} & & 84.49 & 80.77 & 73.71 & 62.22 & 47.16 & \textbf{69.67} \\
\addlinespace[2pt]
\hdashline
\addlinespace[4pt]
VideoMambaSuite~\cite{chen2024video} & 63.13 & 44.36 & 10.36 & 42.80 && 87.30 & 82.95 & 77.17 & 67.06 & 51.74 & 73.24 \\
\textbf{VideoMambaSuite +} & 63.22 & 45.01 & 8.79 & \textbf{43.02} && 86.79 & 83.11 & 77.54 & 67.21 & 53.66 & \textbf{73.66} \\
\bottomrule
\end{tabular}
\label{tab:sota_results}
\end{table}