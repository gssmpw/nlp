\section{Introduction}
\label{sec:intro}
With the rapid growth of video content across diverse domains—ranging from social media and entertainment to surveillance and autonomous systems—understanding and analyzing videos has become a crucial research focus.  
While significant progress has been made in classification tasks such as object and action recognition, temporal reasoning remains a challenging problem~\cite{wu2019long, wu2021towards, lin2022egocentric, zhang2022actionformer, zhao2023re2tal, ren2023timechat}. In particular, \textbf{temporal action detection} (TAD)~\cite{zhao2023re2tal,zhang2022actionformer,lin2019bmn,xu2020g} has emerged as a fundamental task, serving as a pretext for various downstream applications, including dense video captioning and natural language temporal video grounding~\cite{soldan2021mad,Hendricks2017LocalizingMI,grauman2022ego4d, chen2022internvideo}. 

Temporal action detection aims to locate the start and end timestamps of each action instance as well as to identify their categories within a video sequence~\cite{zhao2023re2tal,zhang2022actionformer,lin2019bmn,xu2020g}.
Formally, given a video sequence of $T$ frames $\mathcal{V}=\{I_t \in \mathbb{R}^{H \times W \times 3}\}_{t=1}^T$, TAD predicts a set $M$ of action segments: $\left \{ \left ({t}_{m, s},{t}_{m, e},  {s}_m \right ) \right \}_{m=1}^{M}$, where ${t}_{m, s}$ and ${t}_{m, e}$ denote the start and end timestamps of an action, respectively, and ${c}_m$ represents the action label. 
Conceptually, TAD is analogous to the object detection task~\cite{girshick2015fast,ren2016faster,carion2020end,redmon2016you,zhang2023dino}, which regresses object boundaries as 2D bounding boxes, whereas TAD regresses action boundaries along the temporal dimension of a video. 

Various deep neural network-based methods have been proposed for TAD~\cite{wang2023temporal,hu2024overview,vahdani2022deep,escorcia2016daps,xu2017r,chao2018rethinking,zhang2022actionformer,liu2022end,zeng2019graph,xu2020g}, continuously advancing detection performance. 
Early approaches, such as DAPS~\cite{escorcia2016daps} and R-C3D~\cite{xu2017r}, pioneered proposal-based methods by adapting techniques from the image domain, particularly the two-stage object detection framework derived from Faster R-CNN~\cite{ren2015faster}. Subsequent works introduced techniques to improve the quality of proposals, focusing on refining boundary localization and exploring alternative loss functions~\cite{lin2019fast}. 
Meanwhile, other methods have tackled challenges unique to the video domain, such as the large variation in action durations~\cite{chao2018rethinking}. These variations make it difficult to apply fixed-scale receptive fields, prompting researchers to develop multi-scale modeling techniques that dynamically adapt to different temporal extents.

More recently, recognizing the imbalanced development between video temporal action detection and image object detection techniques, researchers have sought to adapt innovations from object detection to TAD. This has led to the emergence of one-stage methods~\cite{zhang2022actionformer, shi2023tridet, tang2023temporalmaxer, liu2024harnessing, yang2024dyfadet, lin2021learning, ning2021srf, kang2022htnet} and DETR-based architectures~\cite{liu2022end, zhu2024dual, kim2023self, kim2024prediction}, which leverage direct action localization without requiring predefined proposals.
Another line of research has incorporated emerging network architectures, including graph convolutional networks (e.g., GTAD~\cite{xu2020g}, P-GCN~\cite{zeng2019graph}, VSGN~\cite{zhao2021video}), Transformers (e.g., ActionFormer~\cite{zhang2022actionformer}, ViT-TAD~\cite{yang2024adapting}, LIP~\cite{kim2024long}), and Mamba models (e.g., VideoMambaSuite~\cite{chen2024video}, S-Temba~\cite{sinha2025ms}). These architectures have significantly improved the ability to model long-range temporal dependencies and capture fine-grained action boundaries.
Furthermore, recent studies suggest that end-to-end training generally yields higher accuracy than feature-based training. To address the memory constraints when using large-scale video encoding networks, researchers have introduced various optimization strategies, as seen in E2E-TAD~\cite{liu2022empirical}, Re$^2$TAL~\cite{zhao2023re2tal}, and AdaTAD~\cite{liu2024adatad}.

Throughout the brief history of TAD, different methods have focused on distinct aspects of the TAD pipeline, including video representation backbones, detection heads, loss functions, and training strategies. Consequently, a fair comparison between these methods requires evaluation under a unified TAD framework.
Unfortunately, existing methods are often implemented in different frameworks, with varying experimental setups and evaluation protocols. As a result, it becomes difficult to determine whether a reported performance gain originates from the proposed innovations, a more effective implementation framework, or simply hyperparameter tuning. 
To further advance this field, a unified TAD framework is needed—one that holistically evaluates each individual design choice while ensuring fair and reproducible comparisons across different approaches.

This paper introduces \textbf{OpenTAD}, an \textbf{Open}-sourced unified framework for \textbf{T}emporal \textbf{A}ction \textbf{D}etection, accompanied by a comprehensive study of various innovations across different TAD components through extensive experiments. OpenTAD integrates a diverse suite of TAD methods and datasets within a single framework and codebase, facilitating streamlined implementation, fair comparisons, rigorous analysis, and efficient benchmarking.
By leveraging OpenTAD, researchers can systematically identify key factors influencing performance—whether stemming from specific module designs, training strategies, or data processing techniques—thereby enabling the development of more advanced TAD models by combining the strengths of existing approaches. Additionally, OpenTAD provides a scalable foundation for extending existing methods to a broader range of datasets and application scenarios. 
We believe that OpenTAD will foster faithful and efficient development, evaluation, and assessment of novel module designs for TAD. Our contributions are as follows.

\begin{figure}[t]
\begin{center}
\footnotesize
\includegraphics[width=0.99\textwidth]{figures/Architecture_new.png}
\end{center}
\caption{\textbf{Unified TAD Pipeline.} Recent TAD methods follow this three-step framework to predict action classes and start/end timestamps from input videos. \textbf{1) Stage 0:} Videos are encoded into features using a pretrained video backbone, which may be either fine-tuned or frozen during training. \textbf{2) Stage 1:} This stage consists of a neck for temporal aggregation of snippet-level features and a dense head that generates snippet-level predictions. \textbf{3) Stage 2 (optional):} This stage further refines action segment proposals using RoI extraction and produces per-proposal predictions.}
\vspace{-5pt}
\label{fig:tad_arch}
\end{figure}

\begin{itemize}
    \item[(I)] We introduce \textbf{OpenTAD}, a unified framework that modularizes the process of temporal action detection. OpenTAD standardizes the implementation of diverse TAD methods, datasets, and evaluation metrics, enabling systematic assessment, in-depth analysis, and extensive benchmarking. With OpenTAD, researchers can seamlessly switch between feature-based and end-to-end training methods based on computational budgets.
    
    \item[(II)] Based on the OpenTAD framework, we provide an open-source code suite, where we have re-implemented \textbf{16 diverse methods}, including one-stage, two-stage, DETR-based, and end-to-end approaches, as well as \textbf{9 benchmark datasets}. Thanks to its unified design, OpenTAD can be easily extended to support additional methods and datasets, and we remain committed to continuously expanding its capabilities. 

    \item[(III)] We conduct extensive experiments within OpenTAD to comprehensively analyze various architectural and methodological innovations across different TAD components and datasets. Our study identifies the most effective module designs, and by systematically integrating these into existing TAD methods, we achieve new state-of-the-art performance.
\end{itemize}



