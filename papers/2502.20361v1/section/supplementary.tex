\newpage
\section*{Supplementary Material}

\section{Evaluation Protocol and Implementation Details} \label{sec:eval_implement_details}

\textbf{Evaluation Protocol}. Although mean Average Precision (mAP) is the standard evaluation metric for TAD, inconsistencies in evaluation code, ground-truth annotations, and tIoU thresholds across previous methods have led to difficulties in fair comparisons. In the OpenTAD framework, we standardize the evaluation protocol across datasets and methods, ensuring consistency. We report the mean and standard deviation of performance metrics over five different random seeds to account for variability in training.


\noindent \textbf{Implementation Details}.
\label{supp:implementation_details}
OpenTAD is implemented using PyTorch 2.0 and runs on a single NVIDIA A100 GPU. We adhere to each method's original hyperparameter settings, including learning rate, number of training epochs, and other relevant parameters. Unless otherwise stated, we use pre-extracted TSP features on ActivityNet and I3D features on THUMOS for our ablation studies. Batch sizes and optimizer configurations are kept consistent with each method’s original implementation.

\section{Introduction and Categorization of Implemented Methods}
\label{supp:category}

OpenTAD implements 16 representative temporal action detection methods. To align each paper’s original design with OpenTAD’s modular framework, we categorize the methods based on their sub-components, such as neck, dense head, RoI, loss functions, and more. Table~\ref{tab:mapping_to_opentad} provides a detailed mapping of these components. We classify the methods into four categories: one-stage, two-stage, DETR-based, and end-to-end methods. From this table, we can clearly see that the primary difference between one-stage and two-stage methods is that the latter includes an additional step involving RoI extraction to further refine the candidate actions. 

For reproducibility, we re-implemented all 16 methods on the ActivityNet and THUMOS datasets, with results reported in Table~\ref{tab:re-implementation_results}. Our results closely match the original papers, and in some cases, such as BMN, we achieve significantly better performance. Additionally, OpenTAD also supports 9 widely used temporal action detection datasets: ActivityNet-v1.3~\cite{caba2015activitynet}, THUMOS-14~\cite{jiang2014thumos}, EPIC-Kitchens 100~\cite{damen2018scaling}, Ego4D-MQ~\cite{grauman2022ego4d}, HACS~\cite{zhao2019hacs}, Multi-THUMOS~\cite{yeung2018every}, Charades~\cite{sigurdsson2016hollywood}, FineAction~\cite{liu2022fineaction}, and EPIC-Sounds~\cite{EPICSOUNDS2023}. We benchmarked ActionFormer as the base model across all datasets, with results presented in Table~\ref{tab:results_all_datasets}.

\begin{table}[h]
\centering
\small
\caption{\textbf{Re-implemented results of different methods in OpenTAD} in terms of mAP (\%).   N/A means not provided in the paper or released code. For THUMOS-14, previous papers usually reported 5 numbers from mAP at tIoU=\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7\}, and some reported average mAP values, which mean differently across papers. Here, we standardize average mAP as the average of the mAP values at tIoU=\{0.3, 0.4, 0.5, 0.6, 0.7\}, compute this number based on the reported mAP at these 5 tIoUs. For ActivityNet, we compute the average mAP at tIoU=\{0.5:0.95:0.05\}.}
    \setlength{\tabcolsep}{3.2pt}
    \begin{tabular}{llcacacca}
        \toprule
        \multirow{3}{*}{\textbf{Method}} & \multirow{3}{*}{\textbf{Backbone}} & \multicolumn{4}{c}{\textbf{THUMOS-14}} && \multicolumn{2}{c}{\textbf{ActivityNet-v1.3}} \\
        \addlinespace[2pt]
        \cline{3-6} 
        \cline{8-9}  
        \addlinespace[4pt]
        &  & Original & OpenTAD  & Original  & OpenTAD  &&  Original & OpenTAD \\
        &  & tIoU=0.5 & tIoU=0.5  &  Average & Average && Average & Average \\
        \midrule
        ActionFormer~\cite{zhang2022actionformer} & I3D | TSP     & 71.00  & 73.16 & 66.84 & 68.44 && 36.60  & 37.07 \\
        TemporalMaxer~\cite{tang2023temporalmaxer} & I3D | TSP    & 71.80  & 71.66 & 67.70 & 68.33 && N/A   &  N/A  \\
        TriDet~\cite{shi2023tridet} & I3D | TSP           & 72.90  & 73.41 & 69.28 & 69.60 && 36.80 & 36.51 \\
        CausalTAD~\cite{liu2024harnessing} & I3D | TSP    & 73.57  & 73.57 & 69.75 & 69.75 && 37.46  &  37.46  \\
        DyFADet~\cite{yang2024dyfadet} & \tiny{VideoMAEv2-G | TSP}    & 73.70  & 76.32 & 70.50 & 71.70 && 38.50  &  38.62  \\
        VideoMambaSuite~\cite{chen2024video} & InternVideo2         & 76.90 & 77.17 & 72.72 & 73.24 && 42.02 & 42.80 \\
        \midrule
        BMN~\cite{lin2019bmn} & TSN                         & 38.80  & 47.56 & 38.48 & 46.19 && 33.85 & 34.21 \\
        GTAD~\cite{xu2020g} & TSN                           & 43.04 & 48.50 & 41.41 & 46.49 && 34.09 & 34.18 \\
        TSI~\cite{liu2020tsi} & TSN | TSP                     & 42.60  & 46.14 & 42.26 & 44.75 && 35.24 & 35.36 \\
        VSGN~\cite{zhao2021video} & TSN | TSP                 & 45.52 & 49.37 & 43.37 &47.25  && 35.94 & 36.89 \\
        \midrule
        TadTR~\cite{liu2022end} & I3D | TSP & 60.10 & 59.00 & 56.68 & 56.23 && 36.75 & N/A \\
        \midrule
        AFSD~\cite{lin2021learning} & I3D | TSP           & 55.50  & 60.16 & 52.00 & 55.96 && 34.40 &  36.10 \\
        E2E-TAD~\cite{liu2022empirical} & I3D           & 47.00  & 59.00 & 45.08 & 56.23 &&  N/A   &  N/A  \\
        ETAD~\cite{liu2022etad} & R(2+1)D               & 56.17 & 58.23 & 54.66 & 55.56 && 38.25 & 38.76 \\
        Re$^2$TAL~\cite{zhao2023re2tal} & \tiny{Re$^2$SlowFast-101}  & 64.90  & 74.27 & 61.52 & 70.19 && 37.01 & 37.55 \\
        AdaTAD~\cite{liu2024adatad} & \tiny{VideoMAEv2-G}      & 80.90  & 81.24 & 76.88 & 77.07 && 41.93 & 41.85 \\
        \bottomrule
    \end{tabular}
    \label{tab:re-implementation_results}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Detection performance on all 9 supported datasetsusing ActionFormer~\cite{zhang2022actionformer},} measured by average mAP(\%). N/A. means not implemented in ActionFormer's original codebase.}
\footnotesize
\begin{tabular}{@{}lccccc}
\toprule
\textbf{Method}       & \textbf{THUMOS-14}     & \textbf{ActivityNet-v1.3}   & \textbf{EPIC-Kitchens}  & \textbf{Ego4D-MQ} & \textbf{HACS} \\
\midrule
Original     & 66.83      & 36.56         & 21.88 | 23.51   & 23.29 & N/A  \\
OpenTAD      & 68.44      & 37.07         & 22.33 | 24.93   & 25.57  & 37.71   \\
\midrule
\textbf{Method}     & \textbf{Multi-THUMOS}  & \textbf{Charades} & \textbf{FineAction} & \textbf{EPIC-Sounds}\\
\midrule
Original       & N/A          & N/A  & N/A  & N/A  \\
OpenTAD       & 39.18         & 19.39  & 19.62 & 13.89  \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\label{tab:results_all_datasets}
\end{table}

\section{Benchmark Results}
\label{supp:benchmark_result}

In this section, we present the benchmark results for all implemented methods within the OpenTAD framework. For ActivityNet, we use TSP features with standardized evaluation annotations across all methods. For THUMOS, we adopt two-stream I3D features while ensuring consistency in evaluation annotations. The results are reported in Table~\ref{tab:benchmark_anet} and Table~\ref{tab:benchmark_thumos}.

From our benchmarks, we observe that one-stage detection methods have emerged as the preferred choice for both datasets. On THUMOS, two-stage methods such as GTAD and BMN, which rely on external classifiers for action classification, generally underperform compared to recent one-stage methods. However, on ActivityNet, where both one-stage and two-stage methods leverage external video-level classification results, two-stage methods with cascaded proposal refinements still achieve slightly better performance than their one-stage counterparts.

\begin{table}[h]
\centering
\small
\caption{\textbf{Benchmarking results on ActivityNet-v1.3 with TSP feature} in terms of mAP (\%).  }
\begin{tabular}{lrccca}
\toprule
\textbf{Method} & \textbf{\#Param. (M)}  & \textbf{0.5} & \textbf{0.75} & \textbf{0.95} & \textbf{Avg. mAP} 	\\
\midrule
TSI~\cite{liu2020tsi}                       & 4.54  & 52.44 & 35.57 & 9.80 & 35.36 \\
TemporalMaxer~\cite{tang2023temporalmaxer}  & 1.38  & 54.59 & 37.13 & 7.11 & 36.03 \\
AFSD~\cite{lin2021learning}                 & 13.41  & 54.44 & 36.72 & 8.69 & 36.10 \\
GTAD~\cite{xu2020g}                         & 5.58  & 52.33 & 37.58 & 8.42 & 36.20 \\
BMN~\cite{lin2019bmn}                       & 2.80  & 52.90 & 37.30 & 9.67 & 36.40 \\
TriDet~\cite{shi2023tridet}                 & 12.81 & 54.84 & 37.46 & 7.98 & 36.51 \\
VSGN~\cite{zhao2021video}                   & 6.50 & 54.80 & 37.35 & 9.80 & 36.89 \\
ActionFormer~\cite{zhang2022actionformer}   & 6.94  & 55.08 & 38.27 & 8.91 & 37.07 \\
VideoMambaSuite~\cite{chen2024video}        & 4.30 & 55.61 & 38.49 & 9.18 & 37.45 \\
CausalTAD~\cite{liu2024harnessing}          & 12.75  & 55.62 & 38.51 & 9.40 & 37.46 \\
ETAD~\cite{liu2022etad}                     & 8.52  & 54.91 & 38.98 & 9.09 & 37.73 \\
        \bottomrule
    \end{tabular}
    \label{tab:benchmark_anet}
\end{table}

\begin{table}[h]
\centering
\caption{\textbf{Benchmarking results on THUMOS14 with I3D feature} in terms of mAP (\%).  }
\small
\setlength{\tabcolsep}{4.pt}
\begin{tabular}{lrccccca}
\toprule
\textbf{Method} & \textbf{\#Param.(M)}  & \textbf{0.3} & \textbf{0.4} & \textbf{0.5} & \textbf{0.6} & \textbf{0.7} & \textbf{Avg. mAP} 	\\
\midrule
TSI~\cite{liu2020tsi}                       & 4.84 & 62.56 & 57.00 & 50.22 & 40.18 & 30.17 & 48.03  \\
GTAD~\cite{xu2020g}                         & 6.14  & 64.35 & 59.07 & 51.76 & 42.65 & 31.66 & 49.70  \\
BMN~\cite{lin2019bmn}                       & 3.10  & 64.99 & 60.70 & 54.54 & 44.11 & 34.16 & 51.80  \\
VSGN~\cite{zhao2021video}                   & 8.37  & 68.25 & 62.46 & 54.99 & 44.07 & 32.36 & 52.43  \\
ETAD~\cite{liu2022etad}                     & 5.37  & 67.74 & 64.22 & 58.23 & 49.19 & 38.41 & 55.56  \\
AFSD~\cite{lin2021learning}                 & 14.24  & 73.20 & 68.45 & 60.16 & 46.74 & 31.24 & 55.96  \\
TadTR~\cite{liu2022end}                     & 8.66  & 71.90 & 67.29 & 59.00 & 48.34 & 34.61 & 56.23 \\
TemporalMaxer~\cite{tang2023temporalmaxer}  & 7.12  & 83.17 & 79.09 & 71.66 & 61.72 & 46.00 & 68.33 \\
ActionFormer~\cite{zhang2022actionformer}   & 29.25  & 83.78 & 80.06 & 73.16 & 60.46 & 44.72 & 68.44 \\
VideoMambaSuite~\cite{chen2024video}        & 18.57  & 84.33 & 80.60 & 74.19 & 61.99 & 46.71 & 69.57 \\
TriDet~\cite{shi2023tridet}                 & 15.99  & 84.46 & 81.05 & 73.41 & 62.58 & 46.51 & 69.60 \\
CausalTAD~\cite{liu2024harnessing}          & 52.11  & 84.43 & 80.75 & 73.57 & 62.70 & 47.33 & 69.75 \\
        \bottomrule
    \end{tabular}
    \label{tab:benchmark_thumos}
\end{table}


\section{Supplementary Experiments}
\label{supp:more_experiments}


\subsection{Ablation Study on Neck Design in the THUMOS Dataset}

To further examine the impact of neck design, we conduct an ablation study on the ActivityNet dataset, with results presented in Table~\ref{tab:ablate_neck_anet}. These findings align with those reported in Table~\ref{tab:ablate_neck_thumos} of the main paper. 
Our experiments show that LSTM achieves performance comparable to or even better than the SSM module in BMN and GTAD. Furthermore, by integrating both designs, we obtain the best overall performance across all four evaluated methods.

\begin{table}[t]
\centering
\caption{\textbf{Analysis of the neck design choices}, measured by average mAP(\%) on ActivityNet-v1.3. The \textbf{4 macro-block regions} mean the following respectively. \textbf{Top region}: macro blocks with their original sequential modules are adopted as a whole; \textbf{Transformer Block}:  self-attention modules in Transformer blocks are replaced with different sequential modules; \textbf{Mamba Block}:  SSM modules in Mamba blocks are replaced with different sequential modules; \textbf{Bottom region}: a combination of two blocks.
Note that in Row 1,  BMN and GTAD directly use identity mapping since they don't downscale temporally. TSP~\cite{alwassel2021tsp} features are used.}
\small
\begin{tabular}{llccccc}
\toprule
\multicolumn{2}{c}{\textbf{Neck }} && \multicolumn{4}{c}{\textbf{Method}} \\
\addlinespace[2pt]
\cline{1-2} 
\cline{4-7} 
\addlinespace[4pt]
\textbf{Macro Block} &\multicolumn{1}{c}{\textbf{Sequential Module}} && {\textbf{ActionFormer}} & {\textbf{TriDet}} & {\textbf{BMN}} & {\textbf{GTAD}} \\
\midrule
Convolution Block& Convolution    && 36.88{\scriptsize $\pm$0.03} & 36.87{\scriptsize $\pm$0.02} & 36.44{\scriptsize $\pm$0.05} & 36.36{\scriptsize $\pm$0.09} \\      
GCN Block& Graph convolution           && 37.03{\scriptsize $\pm$0.04} & 37.00{\scriptsize $\pm$0.05} & 36.41{\scriptsize $\pm$0.02} & 36.24{\scriptsize $\pm$0.04} \\
Transformer Block& Self-attention&& 37.00{\scriptsize $\pm$0.05} & 36.93{\scriptsize $\pm$0.09} & \textbf{36.50}{\scriptsize $\pm$0.03} & 36.31{\scriptsize $\pm$0.06} \\
Mamba Block& SSM  && \textbf{37.40}{\scriptsize $\pm$0.03} & \textbf{37.33}{\scriptsize $\pm$0.07} &36.36{\scriptsize $\pm$0.05}  & \textbf{36.36}{\scriptsize $\pm$0.06} \\
\midrule
\multirow{5}{*}{\makecell[l]{Transformer  Block}} 
&Convolution     && 36.99{\scriptsize $\pm$0.02} & 36.93{\scriptsize $\pm$0.09} & 36.40{\scriptsize $\pm$0.07} & 36.31{\scriptsize $\pm$0.07}   \\
&Graph convolution             && 37.15{\scriptsize $\pm$0.03} & 36.98{\scriptsize $\pm$0.07} & 36.41{\scriptsize $\pm$0.02} & 36.13{\scriptsize $\pm$0.08} \\
&Self-attention       && 37.00{\scriptsize $\pm$0.05} & 36.93{\scriptsize $\pm$0.09} & 36.50{\scriptsize $\pm$0.03} & 36.31{\scriptsize $\pm$0.06} \\
&LSTM            && 37.18{\scriptsize $\pm$0.04} & 36.95{\scriptsize $\pm$0.07} & \textbf{36.87}{\scriptsize $\pm$0.09} & \textbf{36.47}{\scriptsize $\pm$0.12}  \\
& SSM            && \textbf{37.40}{\scriptsize $\pm$0.04} & \textbf{37.33}{\scriptsize $\pm$0.07} & 36.58{\scriptsize $\pm$0.23} & 36.21{\scriptsize $\pm$0.08}  \\
\midrule
\multirow{5}{*}{\makecell[l]{Mamba  Block}} 
& Convolution    && 37.07{\scriptsize $\pm$0.03}  & 36.97{\scriptsize $\pm$0.07} & 36.40{\scriptsize $\pm$0.05} & 36.17{\scriptsize $\pm$0.08}\\
& Graph convolution            && 37.13{\scriptsize $\pm$0.07}  & 37.06{\scriptsize $\pm$0.04} & 36.33{\scriptsize $\pm$0.05} & 36.16{\scriptsize $\pm$0.09} \\
& Self-attention      && 36.21{\scriptsize $\pm$0.22}  & 36.33{\scriptsize $\pm$0.14} & 36.35{\scriptsize $\pm$0.07} & 36.12{\scriptsize $\pm$0.04} \\
& LSTM           && 36.95{\scriptsize $\pm$0.11}  & 36.57{\scriptsize $\pm$0.07} & \textbf{36.42}{\scriptsize $\pm$0.08} & \textbf{36.23}{\scriptsize $\pm$0.05} \\
& SSM            && \textbf{37.40}{\scriptsize $\pm$0.03} & \textbf{37.33}{\scriptsize $\pm$0.07} &36.36{\scriptsize $\pm$0.05}  & 36.36{\scriptsize $\pm$0.06} \\
\midrule
Mamba + Transf.&SSM + Self-attention          && 37.48{\scriptsize $\pm$0.03} & 37.35{\scriptsize $\pm$0.08} & 36.74{\scriptsize $\pm$0.27} & 36.25{\scriptsize $\pm$0.02}     \\
\textbf{Mamba + Transf.}&\textbf{SSM + LSTM}               && \textbf{37.50}{\scriptsize $\pm$0.07} & \textbf{37.41}{\scriptsize $\pm$0.05} & \textbf{36.94}{\scriptsize $\pm$0.05} & \textbf{36.52}{\scriptsize $\pm$0.08} \\
\bottomrule
\end{tabular}
\label{tab:ablate_neck_anet}
\end{table}


\subsection{Ablation Study on Loss Functions}

In this section, we analyze commonly used loss functions in TAD methods and conduct an ablation study on the effectiveness of the actionness loss proposed for the one-stage method, ActionFormer.

\textbf{Action Category Losses.} These losses are typically classification-based, as action categories are discrete. TAD methods can employ either binary or multi-class classification losses, depending on the objective—distinguishing action from non-action segments or classifying specific action categories. Methods that incorporate external class labels during post-processing (e.g., G-TAD~\cite{xu2020g} and ActionFormer~\cite{zhang2022actionformer} on ActivityNet) typically use binary classification for all category losses within the network. Conversely, methods without such external annotations use multi-class classification for final category predictions while still employing binary classification for intermediate stages.
Common classification losses include focal loss~\cite{Lin2020FocalLF} for binary classification, as used in VSGN~\cite{zhao2021video}, and cross-entropy loss for multi-class classification, as seen in ActionFormer~\cite{zhang2022actionformer}. Some approaches, such as BSN~\cite{lin2018bsn} and BMN~\cite{lin2019bmn}, instead regress action confidence based on the IoU between proposals and ground-truth actions, treating the problem as a regression task rather than a strict binary classification.

\textbf{Action Boundary Losses.} These losses aim to refine the boundaries between predicted and ground-truth action segments, as precise boundary localization is crucial for TAD performance. Various methods improve boundary regression accuracy by directly predicting the distances to start and end locations (e.g., ActionFormer~\cite{zhang2022actionformer}, VSGN~\cite{zhao2021video}) or by regressing offsets relative to predefined anchors, as in PGCN~\cite{liu2020progressive}.

\textbf{Effect of Actionness Loss.} Beyond classification and boundary regression losses, the two-stage method BMN introduces a Temporal Evaluation Module, which classifies each timestep based on actionness/startness/endness to enhance boundary learning. However, this design has been primarily used in two-stage methods. To evaluate its effectiveness in a one-stage setting, we integrate a temporal evaluation head into ActionFormer and apply actionness loss supervision.
The results, shown in Table~\ref{tab:ablateloss}, indicate that while actionness loss provides a minor improvement on ActivityNet, it negatively impacts performance on THUMOS. Given its marginal benefit on ActivityNet and its detrimental effect on THUMOS, we exclude actionness loss from our final design.

\begin{table}[t]
\centering
\caption{\textbf{Analysis of the loss design choices} on ActionFormer.}
\small
\begin{tabular}{ccccc}
\toprule
\textbf{Category} & \textbf{Boundary}   &\textbf{Actionness}  &  \textbf{ActivityNet-v1.3} & \textbf{THUMOS-14} \\
\midrule
\cmark & \cmark     &  & 37.00{\scriptsize $\pm$0.05} & 67.93{\scriptsize $\pm$0.19} \\
\cmark & \cmark     & \cmark & 37.19{\scriptsize $\pm$0.05} & 67.11{\scriptsize $\pm$0.30} \\
\bottomrule
\end{tabular}
\label{tab:ablateloss}
\end{table}


\section{Limitations}

OpenTAD provides a unified framework for implementing and benchmarking various temporal action detection methods, supporting eight datasets. However, it currently focuses exclusively on fully supervised TAD and does not yet support weakly-supervised or open-vocabulary TAD.

Another limitation lies in the scale of existing TAD datasets. Current datasets are relatively small, leading to high variance in experimental results across different random seeds. This variability poses challenges for ensuring training stability and reproducibility. Addressing the scalability of TAD datasets and improving the robustness of training pipelines remains an open research direction that warrants further exploration.

\begin{table}[htbp]
    \centering
    \scriptsize
    \caption{\textbf{Components mapping from each method to OpenTAD.} Cls. and Reg. denote classification loss and regression loss respectively. }
    \rotatebox{-90}{
        \begin{tabular}{llllll}
\toprule
& \textbf{Method} & \textbf{Backbone} & \textbf{Neck} & \textbf{RoI extraction} & \textbf{Losses}       \\
\midrule
\multirow{6}{*}{\begin{sideways}\textbf{One-Stage}\end{sideways}} 
& ActionFormer~\cite{zhang2022actionformer} & I3D, TSP, SlowFast & Multi-scale Transformer &   & Cls. (Focal), Reg. (DIoU)      \\
& TriDet~\cite{shi2023tridet}           & I3D, TSP, R(2+1)D, SlowFast   & Scalable-Granularity Perception  &  & Cls. (IoU weighted Focal) Reg. (DIoU)                  \\
& TemporalMaxer~\cite{tang2023temporalmaxer}  &  I3D, SlowFast  & MaxPool1d &  & Cls. (Focal), Reg. (DIoU) w/ SimOTA                  \\
& VideoMambaSuite~\cite{chen2024video}         & InternVideo2-6B  & Mamba  &  & Cls. (Focal), Reg. (DIoU)     \\
& DyFADet~\cite{yang2024dyfadet}         & I3D, TSP  & Dynamic Feature Aggregation  &  & Cls. (Focal), Reg. (DIoU)     \\
& CausalTAD~\cite{liu2024harnessing}         & I3D, TSP  & Mamba + Transformer  &  & Cls. (Focal), Reg. (DIoU)     \\
% & & & & & \\
\midrule
\multirow{4}{*}{\begin{sideways}\textbf{Two-Stage}\end{sideways}} 
& BMN~\cite{lin2019bmn}     & TSN  & Conv1D  &  Boundary Matching &  Cls. (weighted BLR), Reg. ($L_2$), TEM loss (weighted BCE))  \\
& GTAD~\cite{xu2020g}       &  TSN & GCNeXt & SGAlign   & Cls. (weighted BCE), Reg. ($L_2$), Node Cls. (weighted BCE)) \\
& TSI~\cite{liu2020tsi}     & TSN, I3D, TSP  & Temporal Boundary Detector & IoU Map Regressor & Cls. (Scale-Invariant loss), Reg. ($L_2$), TBD loss (BLR)     \\
& VSGN~\cite{zhao2021video} & TSN, I3D, R(2+1)D & xGPN & Boundary Sampling & Cls. (Focal), Reg. (GIoU), Boundary Adj. (GIoU), Supp. Scores (weighted BLR)\\
\midrule
\multirow{5}{*}{\begin{sideways}\textbf{DETR-Based}\end{sideways}} 
& & & & & \\
& & & & & \\
& TadTR~\cite{liu2022end}   & TSN, I3D & Deformable Transformer & Action Queries, RoIAlign & Cls. (CE), Reg. (IoU, $L_1$), Act. ($L_1$), w/ bipartite matching                \\
& & & & & \\
& & & & & \\
\midrule
\multirow{5}{*}{\begin{sideways}\textbf{End-to-End}\end{sideways}} 
& AFSD~\cite{lin2021learning}       & I3D & FPN & Boundary pooling & Cls. (Focal), Reg. (tIoU, $L_1$)  \\
& E2E-TAD~\cite{liu2022empirical}   & TSN, TSM, I3D, SlowFast &  Same as G-TAD, AFSD, TadTR  \\
& ETAD~\cite{liu2022etad}           & TSM, R(2+1)D &  LSTM & SGAlign &  Cls. (CE), Reg. (smooth-$L_1$)  \\
& Re$^2$TAL~\cite{zhao2023re2tal}   & Re$^2$Vswin, Re$^2$Slowfast & Same as ActionFormer, VSGN      \\
& AdaTAD~\cite{liu2024adatad}       & VideoMAE, SlowFast & Same as ActionFormer   \\
\bottomrule
\end{tabular}
}
\label{tab:mapping_to_opentad}
\end{table}


