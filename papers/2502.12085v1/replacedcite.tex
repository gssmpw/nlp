\section{Related Works}
\vspace{-2pt}
\label{sec:related}

Existing approaches to the efficient long-context inference of LLMs focus on system and algorithm optimizations. Please refer to the surveys____ for more about LLMs.

\textbf{System Optimizations.}  
The efficiency of long-context inference can be enhanced by leveraging hardware architectures and preserving accurate LLM computations. 
Hardware-aware algorithms, such as \textsc{FlashAttn}____, utilize matrix tiling to optimize GPU memory usage and boost inference speed. 
Additionally, efficient parallelism methods support longer sequences with a higher processing speed. 
\textsc{RingAttn}____ splits long sequences across multiple hosts using ring-style communication to preserve accurate attention computation, while \textsc{Ulysses}____ distributes attention heads across hosts to reduce communication overhead. 
Other parallelism strategies____ enable longer sequences on larger models, and the mixture of various distribution strategies____ can further enhance long-context efficiency. 
However, most parallelism approaches are tailored for training and lack optimization for inference. 
Offloading-based methods____ leverage hierarchical memory systems to reduce hardware requirements by storing redundant KV cache in CPU memory and recalling only a small part to GPU memory.


\textbf{Algorithm Optimizations.}  
The burden of long-context inference can also be mitigated through algorithm optimizations. 
KV cache-centric optimization reduces the size of the KV cache, enabling faster decoding speed and lower GPU memory usage. 
Cache eviction methods____ discard irrelevant or unimportant KV cache, alleviating the memory bottlenecks. 
Quantization techniques____ decrease the memory footprint by utilizing low-bit representations to store the KV cache. 
Merging methods____ consolidate redundant KV cache units across sequences or layers to alleviate storage overheads. 
Sparse mechanisms____ decrease inference latency by reducing the attention computational load. Specifically, approaches like \textsc{MInference}____ and \textsc{FastGen}____ assign specific patterns to attention heads, accelerating inference by computing only elements selected from the attention matrix. 
Moreover, algorithm optimizations can complement system optimizations.
\textsc{StarAttn}____ and \textsc{APE}____ linearize attention complexity by dividing context into parallelized blocks, but they struggle with tasks that require inter-context dependencies.
More details on these algorithm optimizations are elaborated in____ and____.
Apart from KV cache optimizations on Transformer-based____ LLMs, approaches altering backbone architectures can also enhance long-context inference efficiency.
Typically, RNN-based models____ and hybrid models____ reduce the computation complexity of long-context inference.