\section{Related Works}
\vspace{-2pt}
\label{sec:related}

Existing approaches to the efficient long-context inference of LLMs focus on system and algorithm optimizations. Please refer to the surveys~\cite{zhao2023survey, wan2023efficient} for more about LLMs.

\textbf{System Optimizations.}  
The efficiency of long-context inference can be enhanced by leveraging hardware architectures and preserving accurate LLM computations. 
Hardware-aware algorithms, such as \textsc{FlashAttn}~\citep{dao2022flashattention, dao2023flashattention2, shah2024flashattention}, utilize matrix tiling to optimize GPU memory usage and boost inference speed. 
Additionally, efficient parallelism methods support longer sequences with a higher processing speed. 
\textsc{RingAttn}~\citep{li2021sequence} splits long sequences across multiple hosts using ring-style communication to preserve accurate attention computation, while \textsc{Ulysses}~\citep{jacobs2023deepspeed} distributes attention heads across hosts to reduce communication overhead. 
Other parallelism strategies~\citep{narayanan2021efficient, huang2019gpipe, rajbhandari2020zero, sun2024seq1f1b, deepseek-v3} enable longer sequences on larger models, and the mixture of various distribution strategies~\citep{ren2021zero, ao2024burstattention} can further enhance long-context efficiency. 
However, most parallelism approaches are tailored for training and lack optimization for inference. 
Offloading-based methods~\cite{xiao2024infllm, sun2024shadowkv, lee2024infinigen, chen2024magicpig} leverage hierarchical memory systems to reduce hardware requirements by storing redundant KV cache in CPU memory and recalling only a small part to GPU memory.


\textbf{Algorithm Optimizations.}  
The burden of long-context inference can also be mitigated through algorithm optimizations. 
KV cache-centric optimization reduces the size of the KV cache, enabling faster decoding speed and lower GPU memory usage. 
Cache eviction methods~\cite{zhang2024h2o, li2024snapkv, yao2024sirllm, huang2024locret} discard irrelevant or unimportant KV cache, alleviating the memory bottlenecks. 
Quantization techniques~\cite{liu2024kivi, hooper2024kvquant, zhang2024kv,he2024zipcache} decrease the memory footprint by utilizing low-bit representations to store the KV cache. 
Merging methods~\cite{cai2024lococo, wang2024model, zhang2024simlayerkv, zhangcam} consolidate redundant KV cache units across sequences or layers to alleviate storage overheads. 
Sparse mechanisms~\cite{zaheer2020big, beltagy2020longformer, lou2024sparser} decrease inference latency by reducing the attention computational load. Specifically, approaches like \textsc{MInference}~\cite{jiang2024minference} and \textsc{FastGen}~\cite{ge2023model} assign specific patterns to attention heads, accelerating inference by computing only elements selected from the attention matrix. 
Moreover, algorithm optimizations can complement system optimizations.
\textsc{StarAttn}~\cite{acharya2024star} and \textsc{APE}~\cite{yang2025ape} linearize attention complexity by dividing context into parallelized blocks, but they struggle with tasks that require inter-context dependencies.
More details on these algorithm optimizations are elaborated in~\citet{li2024scbench} and~\citet{luohe2024keep}.
Apart from KV cache optimizations on Transformer-based~\cite{vaswani2017attention} LLMs, approaches altering backbone architectures can also enhance long-context inference efficiency.
Typically, RNN-based models~\citep{gu2023mamba,dao2024transformers} and hybrid models~\citep{lieber2024jamba,dong2024hymba,li2025minimax} reduce the computation complexity of long-context inference.