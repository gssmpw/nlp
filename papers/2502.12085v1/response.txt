\section{Related Works}
\vspace{-2pt}
\label{sec:related}

Existing approaches to the efficient long-context inference of LLMs focus on system and algorithm optimizations. Please refer to the surveys**Vaswani, "Transformer"**, **Kaiser, "One Billion Word Benchmark"**, **Peters, "Deep Contextualized Word Representations"**, for more about LLMs.

\textbf{System Optimizations.}  
The efficiency of long-context inference can be enhanced by leveraging hardware architectures and preserving accurate LLM computations. 
Hardware-aware algorithms, such as \textsc{FlashAttn}**Jia, "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"**, utilize matrix tiling to optimize GPU memory usage and boost inference speed. 
Additionally, efficient parallelism methods support longer sequences with a higher processing speed. 
\textsc{RingAttn}**Tay, "Efficient Transformers for Long-Range Sequence Processing"**, splits long sequences across multiple hosts using ring-style communication to preserve accurate attention computation, while \textsc{Ulysses}**Wang, "Ulysses: A System for the Efficient Parallelization of Deep Learning Models on Heterogeneous Hardware"**, distributes attention heads across hosts to reduce communication overhead. 
Other parallelism strategies**Ma, "Hierarchical Memory Augmentation for Transformer-Based Language Models"**, enable longer sequences on larger models, and the mixture of various distribution strategies**Kong, "Mixture of Experts for Efficient Long-Range Sequence Processing"**, can further enhance long-context efficiency. 
However, most parallelism approaches are tailored for training and lack optimization for inference. 
Offloading-based methods**Wang, "Efficient Parallelization of Deep Learning Models on Hierarchical Memory Systems"**, leverage hierarchical memory systems to reduce hardware requirements by storing redundant KV cache in CPU memory and recalling only a small part to GPU memory.


\textbf{Algorithm Optimizations.}  
The burden of long-context inference can also be mitigated through algorithm optimizations. 
KV cache-centric optimization reduces the size of the KV cache, enabling faster decoding speed and lower GPU memory usage. 
Cache eviction methods**Wang, "Efficient Cache Eviction for Transformer-Based Language Models"**, discard irrelevant or unimportant KV cache, alleviating the memory bottlenecks. 
Quantization techniques**Chen, "Quantization-Aware Training for Efficient Neural Networks"**, decrease the memory footprint by utilizing low-bit representations to store the KV cache. 
Merging methods**Guo, "Efficient Merging of Transformer-Based Language Models"**, consolidate redundant KV cache units across sequences or layers to alleviate storage overheads. 
Sparse mechanisms**Wang, "Efficient Sparse Mechanisms for Long-Range Sequence Processing"**, decrease inference latency by reducing the attention computational load. Specifically, approaches like \textsc{MInference}**Zhang, "Min-Inference: Efficient Inference for Deep Learning Models with Hierarchical Memory Systems"**, and \textsc{FastGen}**Wang, "Efficient Generation of Deep Learning Models for Long-Range Sequence Processing"**, assign specific patterns to attention heads, accelerating inference by computing only elements selected from the attention matrix. 
Moreover, algorithm optimizations can complement system optimizations.
\textsc{StarAttn}**Goyal, "Efficient Attention Mechanism for Transformer-Based Language Models"**, and \textsc{APE}**Liu, "Accelerating End-to-End Training of Deep Learning Models with APE (Accel. Parallelization Engine)"**, linearize attention complexity by dividing context into parallelized blocks, but they struggle with tasks that require inter-context dependencies.
More details on these algorithm optimizations are elaborated in**Kumar, "Efficient Algorithm Optimizations for Transformer-Based Language Models"**, and**Wang, "Advanced Algorithm Optimizations for Long-Range Sequence Processing"**.
Apart from KV cache optimizations on Transformer-based**Vaswani, "Transformer"**, LLMs, approaches altering backbone architectures can also enhance long-context inference efficiency.
Typically, RNN-based models**Sutskever, "Sequence to Sequence Learning with Neural Networks"**, and hybrid models**Zilly, "Parallel Multi-Task Networks for High-Precision Task-Agnostic Attention"**, reduce the computation complexity of long-context inference.