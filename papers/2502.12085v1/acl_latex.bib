% These citations are directly coppied from Locret paper. They might be useful here.

@Misc{gpt4o,
title = {Open{AI} {GPT}-4o},
author={OpenAI},
url={https://platform.openai.com/docs/models/gpt-4o},
year = {2024},
}


@Misc{claude3,
title = {Claude 3.5 {S}onnet},
author={Anthropic},
url={https://www.anthropic.com/claude/sonnet},
year = {2024},
}

@article{young2024yi,
  title={{Y}i: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={ArXiv:2403.04652},
  year={2024}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv:2403.05530},
  year={2024}
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv:2307.09288},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv:2001.08361},
  year={2020}
}

@article{hu2023unlock,
  title={Unlock predictable scaling from emergent abilities},
  author={Hu, Shengding and Liu, Xin and Han, Xu and Zhang, Xinrong and He, Chaoqun and Zhao, Weilin and Lin, Yankai and Ding, Ning and Ou, Zebin and Zeng, Guoyang and others},
  journal={Proceedings of ICLR},
  year={2024}
}

@article{ding2024longrope,
  title={Longrope: Extending llm context window beyond 2 million tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv:2402.13753},
  year={2024}
}

@article{bai2024longalign,
  title={Longalign: A recipe for long context alignment of large language models},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi},
  journal={Proceedings of EMNLP},
  year={2024}
}

@article{zhang2024infty,
  title={$\infty $ Bench: Extending Long Context Evaluation Beyond 100K Tokens},
  author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo Khai and Han, Xu and Thai, Zhen Leng and Wang, Shuo and Liu, Zhiyuan and others},
  journal={Proceedings of ACL},
  year={2024}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={Proceedings of ACL},
  year={2024}
}

@article{li2024making,
  title={Making Long-Context Language Models Better Multi-Hop Reasoners},
  author={Li, Yanyang and Liang, Shuo and Lyu, Michael R and Wang, Liwei},
  journal={Proceedings of ACL},
  year={2024}
}

@article{guerreiro2022looking,
  title={Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation},
  author={Guerreiro, Nuno M and Voita, Elena and Martins, Andr{\'e} FT},
  journal={Proceedings of EACL},
  year={2023}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186345},
  year={2024},
  publisher={Springer}
}

@article{mei2024aios,
  title={AIOS: LLM agent operating system},
  author={Mei, Kai and Li, Zelong and Xu, Shuyuan and Ye, Ruosong and Ge, Yingqiang and Zhang, Yongfeng},
  journal={arXiv:2403.16971},
  year={2024}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv:2301.00234},
  year={2022}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Proceedings of NeurIPS},
  year={2020}
}

@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={Proceedings of COLM},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv:2404.14219},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish},
  journal={Proceedings of NeurIPS},
  year={2017}
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={ArXiv:2407.21783},
  year={2024}
}

@article{agrawal2023sarathi,
  title={Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills},
  author={Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Ramjee, Ramachandran},
  journal={arXiv:2308.16369},
  year={2023}
}

@article{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  journal={Proceedings of SOSP},
  year={2023}
}

@article{xiao2024infllm,
  title={Inf{LLM}: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory},
  author={Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{zhang2024h2o,
  title={H$_2${O}: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{zhang2024q,
  title={Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache},
  author={Zhang, Zhenyu and Liu, Shiwei and Chen, Runjin and Kailkhura, Bhavya and Chen, Beidi and Wang, Atlas},
  journal={Proceedings MLSys},
  year={2024}
}

@article{liu2024kivi,
  title={{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={Proceedings of ICML},
  year={2024}
}

@article{hooper2024kvquant,
  title={{KVQ}uant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{zandieh2024qjl,
  title={QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead},
  author={Zandieh, Amir and Daliri, Majid and Han, Insu},
  journal={arXiv:2406.03482},
  year={2024}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={Proceedings of ICLR},
  year={2024}
}

@article{cai2024lococo,
  title={Lo{C}o{C}o: Dropping In Convolutions for Long Context Compression},
  author={Cai, Ruisi and Tian, Yuandong and Wang, Zhangyang and Chen, Beidi},
  journal={Proceedings of ICML},
  year={2024}
}

@article{wu2024layer,
  title={Layer-Condensed KV Cache for Efficient Inference of Large Language Models},
  author={Wu, Haoyi and Tu, Kewei},
  journal={Proceedings of ACL},
  year={2024}
}

@article{brandon2024reducing,
  title={Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
  author={Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Kelly, Jonathan Ragan},
  journal={arXiv:2405.12981},
  year={2024}
}

@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv:2405.14366},
  year={2024}
}

@article{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  journal={Proceedings of ICML},
  year={2023},
}

@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={Proceedings of ICLR},
  year={2024}
}

@article{yuan2024kv,
  title={KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches},
  author={Yuan, Jiayi and Liu, Hongyi and Chuang, Yu-Neng and Li, Songchen and Wang, Guanchu and Le, Duy and Jin, Hongye and Chaudhary, Vipin and Xu, Zhaozhuo and Liu, Zirui and others},
  journal={Proceedings of EMNLP},
  year={2024}
}

@article{yang2024pyramidinfer,
  title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference},
  author={Yang, Dongjie and Han, XiaoDong and Gao, Yan and Hu, Yao and Zhang, Shilin and Zhao, Hai},
  journal={Proceedings of ACL},
  year={2024}
}

@article{lou2024sparser,
  title={Sparser is {F}aster and {L}ess is {M}ore: Efficient Sparse Attention for Long-Range Transformers},
  author={Lou, Chao and Jia, Zixia and Zheng, Zilong and Tu, Kewei},
  journal={ArXiv:2406.16747},
  year={2024}
}

@article{jiang2024minference,
  title={M{I}nference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
  author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others},
  journal={Proceedings of ICML},
  year={2024}
}

@article{dao2022flashattention,
  title={Flash{A}ttention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Proceedings of NeurIPS},
  year={2022}
}


@online{llamacpp,
    url = {https://github.com/ggerganov/llama.cpp},
    author = {llama.cpp},
}

@online{llama2c,
    url = {https://github.com/karpathy/llama2.c},
    author = {llama2.c},
}

@online{rustformers,
    url = {https://github.com/rustformers/llm},
    author = {rustformers},

}

@online{quanto,
    url = {https://github.com/huggingface/optimum-quanto},
    author = {Hugging-Face}
}

@article{qin2024mooncake,
  title={Mooncake: Kimi's KVCache-centric Architecture for LLM Serving},
  author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
  journal={arXiv:2407.00079},
  year={2024}
}

@article{jianghexgen,
  title={HexGen: Generative Inference of Large Language Model over Heterogeneous Environment},
  author={Jiang, Youhe and Yan, Ran and Yao, Xiaozhe and Zhou, Yang and Chen, Beidi and Yuan, Binhang},
  journal={Proceedings of ICML},
  year={2024}
}

@article{chen2023longlora,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={Proceedings of ICLR},
  year={2024}
}

@article{an2023eval,
  title={L-eval: Instituting standardized evaluation for long context language models},
  author={An, Chenxin and Gong, Shansan and Zhong, Ming and Zhao, Xingjian and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  journal={Proceedings of ACL},
  year={2024}
}

@article{yang2024post,
  title={Post-Training Sparse Attention with Double Sparsity},
  author={Yang, Shuo and Sheng, Ying and Gonzalez, Joseph E and Stoica, Ion and Zheng, Lianmin},
  journal={arXiv:2408.07092},
  year={2024}
}

@misc{hf-quant,
  title={Unlocking Longer Generation with Key-Value Cache Quantization},
  author={Raushan Turganbay},
  url={https://huggingface.co/blog/kv-cache-quantization},
  year={2024}
}

@misc{anti-haystack,
    title={Anti-Haystack},
    author={Wenbo Pan},
    url={https://huggingface.co/datasets/wenbopan/anti-haystack},
    year={2024}

}

@misc{ktransformers,
    title={KTransformers: A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations},
    author={KVCache.AI},
    url={https://github.com/kvcache-ai/ktransformers},
    year={2024}

}

@article{yao2024sirllm,
  title={Sir{LLM}: Streaming Infinite Retentive {LLM}},
  author={Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={Proceedings of ACL},
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={Proceedings of ICLR},
  year={2019}
}


@article{dettmers2022gpt3,
  title={LLM.int8(): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Proceedings of NeurIPS},
  year={2022}
}


@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of MLSys},
  year={2024}
}

@article{liu2023deja,
  title={Deja vu: Contextual sparsity for efficient llms at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  journal={Proceedings of ICML},
  year={2023}
}

@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and others},
  journal={Transactions on Machine Learning Research},
  year={2024},
}

@article{miao2023towards,
  title={Towards efficient generative large language model serving: A survey from algorithms to systems},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  journal={Proceedings of ICML},
  year={2024}
}

@article{schnitzler2024morehopqa,
  title={MoreHopQA: More Than Multi-hop Reasoning},
  author={Schnitzler, Julian and Ho, Xanh and Huang, Jiahao and Boudin, Florian and Sugawara, Saku and Aizawa, Akiko},
  journal={arXiv:2406.13397},
  year={2024}
}

@article{qin2023tool,
  title={Tool learning with foundation models},
  author={Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Zhou, Xuanhe and Huang, Yufei and Xiao, Chaojun and others},
  journal={ACM Computing Surveys},
  year={2024},
}


@article{wang2024multimodal,
  title={Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models},
  author={Wang, Hengyi and Shi, Haizhou and Tan, Shiwei and Qin, Weiyi and Wang, Wenyuan and Zhang, Tunyu and Nambi, Akshay and Ganu, Tanuja and Wang, Hao},
  journal={arXiv:2406.11230},
  year={2024}
}

@article{nawrot2024dynamic,
  title={Dynamic memory compression: Retrofitting llms for accelerated inference},
  author={Nawrot, Piotr and {\L}a{\'n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M},
  journal={arXiv:2403.09636},
  year={2024}
}

@article{rajput2024infer,
  title={Inference-Friendly Models With MixAttention},
  author={Rajput, Shashank and Sheng, Ying and Owen, Sean and Chiley, Vitaliy},
  journal={arXiv:2409.15012},
  year={2024}
}

@article{lv2024critiprefill,
  title={CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs},
  author={Lv, Junlin and Feng, Yuan and Xie, Xike and Jia, Xin and Peng, Qirong and Xie, Guiming},
  journal={arXiv:2409.12490},
  year={2024}
}

@article{kang2024gear,
  title={Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv:2403.05527},
  year={2024}
}

@article{luohe2024keep,
  title={Keep the {C}ost {D}own: A Review on Methods to Optimize LLM's KV-Cache Consumption},
  author={Shi, Luohe and Zhang, Hongyi and Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={Proceedings of COLM},
  year={2024}
}

@article{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={Proceedings of ICLR},
  year={2024}
}

@article{zhong2024distserve,
  title={Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  journal={Proceedings of OSDI},
  year={2024}
}

@article{wu2024offload,
  title={LM-Offload: Performance Model-Guided Generative Inference of Large Language Models with Parallelism Control},
  author={Wu, Jianbo and Ren, Jie and Yang, Shuangyan and Parasyris, Konstantinos and Georgakoudis, Giorgis and Laguna, Ignacio and Li, Dong},
  journal={Blog of PASA Lab},
  year={2024},
}

@article{shah2024flashattention,
  title={Flash{A}ttention-3: Fast and accurate attention with asynchrony and low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={ArXiv:2407.08608},
  year={2024}
}

@article{ghorpade2012gpgpu,
  title={GPGPU processing in CUDA architecture},
  author={Ghorpade, Jayshree and Parande, Jitendra and Kulkarni, Madhura and Bawaskar, Amit},
  journal={Proceedings of ACIJ},
  year={2012}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={Proceedings of ICLR},
  year={2023}
}

@article{xiao2023smoothquant,
    title = {{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models},
    author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
    journal = {Proceedings of ICML},
    year = {2023}
}

@article{zhang2022moefication,
  title={{MoEfication}: Transformer Feed-forward Layers are Mixtures of Experts},
  author={Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  journal={Proceedings of ACL},
  year={2022}
}

@article{
  zhang2024exploring,
  title={Exploring the Benefit of Activation Sparsity in Pre-training},
  author={Zhengyan Zhang and Chaojun Xiao and Qiujieli Qin and Yankai Lin and Zhiyuan Zeng and Xu Han and Zhiyuan Liu and Ruobing Xie and Maosong Sun and Jie Zhou},
  journal={Proceedings of ICML},
  year={2024},
}

@article{ma2024era,
  title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits},
  author={Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  journal={arXiv:2402.17764},
  year={2024}
}

@article{mu2024learning,
  title={Learning to Compress Prompts with Gist Tokens},
  author={Mu, Jesse and Li, Xiang and Goodman, Noah},
  journal={Proceedings of NeurIPS},
  year={2023}
}

@article{munkhdalai2024leave,
  title={Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  author={Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal={arXiv:2404.07143},
  year={2024}
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebrón, Federico and Sanghai, Sumit},
  journal={Proceedings of EMNLP},
  year={2023}
}

@article{hu2024inference,
  title={Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads},
  author={Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
  journal={arXiv:2401.11181},
  year={2024}
}

@article{zhang2024kv,
  title={{KV} Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization},
  author={Zhang, Tianyi and Yi, Jonah and Xu, Zhaozhuo and Shrivastava, Anshumali},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{liu2023cachegen,
  title={CacheGen: Fast Context Loading for Language Model Applications},
  author={Liu, Yuhan and Li, Hanchen and Du, Kuntai and Yao, Jiayi and Cheng, Yihua and Huang, Yuyang and Lu, Shan and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and others},
  journal={Proceedings of SIGCOMM},
  year={2023}
}

@article{zhao2023survey,
  title={A Survey of Large Language Models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={ArXiv:2303.18223},
  year={2023}
}

@article{minaee2024large,
  title={Large Language Models: A Survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv:2402.06196},
  year={2024}
}

@article{li2024snapkv,
  title={Snap{KV}: LLM Knows What You are Looking for Before Generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{sun2024shadowkv,
  title={Shadow{KV}: {KV} Cache in Shadows for High-Throughput Long-Context LLM Inference},
  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
  journal={ArXiv:2410.21465},
  year={2024}
}

@article{cai2024pyramidkv,
  title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
  journal={arXiv:2406.02069},
  year={2024}
}

% End of copied locret citations

@article{huang2024locret,
  title={Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads},
  author={Huang, Yuxiang and Yuan, Binhang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan},
  journal={ArXiv:2410.01805},
  year={2024}
}

@article{acharya2024star,
  title={Star {A}ttention: Efficient LLM Inference over Long Sequences},
  author={Acharya, Shantanu and Jia, Fei and Ginsburg, Boris},
  journal={ArXiv:2411.17116},
  year={2024}
}

@article{ao2024burstattention,
  title={Burst{A}ttention: An Efficient Distributed Attention Framework for Extremely Long Sequences},
  author={Sun, Ao and Zhao, Weilin and Han, Xu and Yang, Cheng and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong},
  journal={ArXiv:2403.09347},
  year={2024}
}

@article{li2024review,
  title={A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including {RAG}), Planning, and Feedback Learning},
  author={Li, Xinzhe},
  journal={Proceedings of COLING},
  year={2025}
}

@article{zeng2023large,
  title={Large Language Models for Robotics: A Survey},
  author={Zeng, Fanlong and Gan, Wensheng and Wang, Yongheng and Liu, Ning and Yu, Philip S},
  journal={ArXiv:2311.07226},
  year={2023}
}

@article{chu2023survey,
  title={A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future},
  author={Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting},
  journal={ArXiv:2309.15402},
  year={2023}
}

@article{glm2024chatglm,
  title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
  author={GLM, Team},
  journal={arXiv:2406.12793},
  year={2024}
}

@article{li2021sequence,
  title={Sequence {P}arallelism: Long Sequence Training from System Perspective},
  author={Li, Shenggui and Xue, Fuzhao and Baranwal, Chaitanya and Li, Yongbin and You, Yang},
  journal={Proceedings of ACL},
  year={2023}
}

@article{jacobs2023deepspeed,
  title={Deep{S}peed {U}lysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models},
  author={Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong},
  journal={ArXiv:2309.14509},
  year={2023}
}

@article{narayanan2021efficient,
  title={Efficient Large-Scale Language Model Training on GPU Clusters Using {M}egatron-{LM}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  journal={Proceedings of SC},
  year={2021}
}

@article{huang2019gpipe,
  title={{GP}ipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, Zhifeng},
  journal={Proceedings of NeurIPS},
  year={2019}
}

@article{ren2021zero,
  title={Ze{RO}-{O}ffload: Democratizing Billion-Scale Model Training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  journal={Proceedings of ATC},
  year={2021}
}


@article{rajbhandari2020zero,
  title={Ze{RO}: Memory Optimizations Toward Training Trillion Parameter Models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  journal={Proceedings of SC},
  year={2020},
  organization={IEEE}
}

@article{sun2024seq1f1b,
  title={Seq1{F}1{B}: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training},
  author={Sun, Ao and Zhao, Weilin and Han, Xu and Yang, Cheng and Zhang, Xinrong and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong},
  journal={Proceedings of NAACL},
  year={2025}
}

@article{deepseek-v3,
    title = {DeepSeek-{V}3 Technical Report},
    author = {DeepSeek-AI},
    journal = {ArXiv:2412.19437},
    year = {2024}
}

@article{lee2024infinigen,
  title={Infini{G}en: Efficient Generative Inference of Large Language Models with Dynamic {KV} Cache Management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  journal={Proceedings of OSDI},
  year={2024}
}

@article{wang2024model,
  title={Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks},
  author={Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia},
  journal={arXiv:2407.08454},
  year={2024}
}


@article{zaheer2020big,
  title={Big {B}ird: Transformers for Longer Sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  journal={Proceedings of NeurIPS},
  year={2020}
}

@article{beltagy2020longformer,
  title={Longformer: The Long-Document Transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={ArXiv:2004.05150},
  year={2020}
}

@article{zhang2024simlayerkv,
  title={Sim{L}ayer{KV}: A Simple Framework for Layer-Level KV Cache Reduction},
  author={Zhang, Xuan and Du, Cunxiao and Du, Chao and Pang, Tianyu and Gao, Wei and Lin, Min},
  journal={ArXiv:2410.13846},
  year={2024}
}

@article{li2024scbench,
  title={{SCB}ench: A KV Cache-Centric Analysis of Long-Context Methods},
  author={Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
  journal={ArXiv:2412.10319},
  year={2024}
}

@article{hsieh2024ruler,
  title={{RULER}: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={Proceedings of COLM},
  year={2024}
}

@article{xiao2024duoattention,
  title={DuoAttention: Efficient long-context llm inference with retrieval and streaming heads},
  author={Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
  journal={arXiv:2410.10819},
  year={2024}
}

@article{kim2024survey,
  title={A survey on integration of large language models with intelligent robots},
  author={Kim, Yeseung and Kim, Dohyun and Choi, Jieun and Park, Jisang and Oh, Nayoung and Park, Daehyung},
  journal={Intelligent Service Robotics},
  year={2024},
}

@article{sahoo2024systematic,
  title={A systematic survey of prompt engineering in large language models: Techniques and applications},
  author={Sahoo, Pranab and Singh, Ayush Kumar and Saha, Sriparna and Jain, Vinija and Mondal, Samrat and Chadha, Aman},
  journal={ArXiv:2402.07927},
  year={2024}
}

@article{zhao2024longagent,
  title={Long{A}gent: Scaling Language Models to 128k Context through Multi-Agent Collaboration},
  author={Zhao, Jun and Zu, Can and Xu, Hao and Lu, Yi and He, Wei and Ding, Yiwen and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={Proceedings of EMNLP},
  year={2024}
}

@article{chen2024magicpig,
  title={Magic{P}ig: {LSH} sampling for efficient llm generation},
  author={Chen, Zhuoming and Sadhukhan, Ranajoy and Ye, Zihao and Zhou, Yang and Zhang, Jianyu and Nolte, Niklas and Tian, Yuandong and Douze, Matthijs and Bottou, Leon and Jia, Zhihao and others},
  journal={Proceedings of ICLR},
  year={2025}
}


@article{he2024zipcache,
  title={Zip{C}ache: Accurate and Efficient KV Cache Quantization with Salient Token Identification},
  author={He, Yefei and Zhang, Luoming and Wu, Weijia and Liu, Jing and Zhou, Hong and Zhuang, Bohan},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@inproceedings{zhangcam,
  title={Ca{M}: Cache Merging for Memory-efficient LLMs Inference},
  author={Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  booktitle={Proceedings of ICML},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={Proceedings of COLM},
  year={2024}
}

@article{dao2024transformers,
  title={Transformers are {SSM}s: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={Proceedings of ICML},
  year={2024}
}

@article{peng2024eagle,
  title={Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Du, Xingjian and Ferdinan, Teddy and Hou, Haowen and others},
  journal={arXiv:2404.05892},
  year={2024}
}

@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={Proceedings of ICLR},
  year={2025}
}

@article{dong2024hymba,
  title={Hymba: A hybrid-head architecture for small language models},
  author={Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Van Keirsbilck, Matthijs and Chen, Min-Hung and Suhara, Yoshi and others},
  journal={ArXiv:2411.13676},
  year={2024}
}

@article{li2025minimax,
  title={Minimax-01: Scaling foundation models with lightning attention},
  author={Li, Aonian and Gong, Bangwei and Yang, Bo and Shan, Boji and Liu, Chang and Zhu, Cheng and Zhang, Chunhao and Guo, Congchao and Chen, Da and Li, Dong and others},
  journal={ArXiv preprint arXiv:2501.08313},
  year={2025}
}


@article{li2025minimax,
  title={Minimax-01: Scaling foundation models with lightning attention},
  author={Li, Aonian and Gong, Bangwei and Yang, Bo and Shan, Boji and Liu, Chang and Zhu, Cheng and Zhang, Chunhao and Guo, Congchao and Chen, Da and Li, Dong and others},
  journal={arXiv:2501.08313},
  year={2025}
}

@article{yang2025ape,
  title={{APE}: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding},
  author={Yang, Xinyu and Chen, Tianqi and Chen, Beidi},
  journal={Proceedings of ICLR},
  year={2025}
}

@article{wan2023efficient
,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others},
  journal={ArXiv:2312.03863},
  year={2023}
}

