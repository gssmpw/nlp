[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhao2023survey",
        "author": "Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others",
        "title": "A Survey of Large Language Models"
      },
      {
        "key": "wan2023efficient",
        "author": "Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others",
        "title": "Efficient large language models: A survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "dao2022flashattention",
        "author": "Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher",
        "title": "Flash{A}ttention: Fast and memory-efficient exact attention with io-awareness"
      },
      {
        "key": "dao2023flashattention2",
        "author": "Dao, Tri",
        "title": "Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning"
      },
      {
        "key": "shah2024flashattention",
        "author": "Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri",
        "title": "Flash{A}ttention-3: Fast and accurate attention with asynchrony and low-precision"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2021sequence",
        "author": "Li, Shenggui and Xue, Fuzhao and Baranwal, Chaitanya and Li, Yongbin and You, Yang",
        "title": "Sequence {P}arallelism: Long Sequence Training from System Perspective"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jacobs2023deepspeed",
        "author": "Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong",
        "title": "Deep{S}peed {U}lysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "narayanan2021efficient",
        "author": "Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others",
        "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using {M}egatron-{LM}"
      },
      {
        "key": "huang2019gpipe",
        "author": "Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, Zhifeng",
        "title": "{GP}ipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"
      },
      {
        "key": "rajbhandari2020zero",
        "author": "Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong",
        "title": "Ze{RO}: Memory Optimizations Toward Training Trillion Parameter Models"
      },
      {
        "key": "sun2024seq1f1b",
        "author": "Sun, Ao and Zhao, Weilin and Han, Xu and Yang, Cheng and Zhang, Xinrong and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong",
        "title": "Seq1{F}1{B}: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training"
      },
      {
        "key": "deepseek-v3",
        "author": "DeepSeek-AI",
        "title": "DeepSeek-{V}3 Technical Report"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ren2021zero",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "Ze{RO}-{O}ffload: Democratizing Billion-Scale Model Training"
      },
      {
        "key": "ao2024burstattention",
        "author": "Sun, Ao and Zhao, Weilin and Han, Xu and Yang, Cheng and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong",
        "title": "Burst{A}ttention: An Efficient Distributed Attention Framework for Extremely Long Sequences"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "xiao2024infllm",
        "author": "Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong",
        "title": "Inf{LLM}: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory"
      },
      {
        "key": "sun2024shadowkv",
        "author": "Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi",
        "title": "Shadow{KV}: {KV} Cache in Shadows for High-Throughput Long-Context LLM Inference"
      },
      {
        "key": "lee2024infinigen",
        "author": "Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong",
        "title": "Infini{G}en: Efficient Generative Inference of Large Language Models with Dynamic {KV} Cache Management"
      },
      {
        "key": "chen2024magicpig",
        "author": "Chen, Zhuoming and Sadhukhan, Ranajoy and Ye, Zihao and Zhou, Yang and Zhang, Jianyu and Nolte, Niklas and Tian, Yuandong and Douze, Matthijs and Bottou, Leon and Jia, Zhihao and others",
        "title": "Magic{P}ig: {LSH} sampling for efficient llm generation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2024h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H$_2${O}: Heavy-hitter oracle for efficient generative inference of large language models"
      },
      {
        "key": "li2024snapkv",
        "author": "Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming",
        "title": "Snap{KV}: LLM Knows What You are Looking for Before Generation"
      },
      {
        "key": "yao2024sirllm",
        "author": "Yao, Yao and Li, Zuchao and Zhao, Hai",
        "title": "Sir{LLM}: Streaming Infinite Retentive {LLM}"
      },
      {
        "key": "huang2024locret",
        "author": "Huang, Yuxiang and Yuan, Binhang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan",
        "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2024kivi",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache"
      },
      {
        "key": "hooper2024kvquant",
        "author": "Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir",
        "title": "{KVQ}uant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"
      },
      {
        "key": "zhang2024kv",
        "author": "Zhang, Tianyi and Yi, Jonah and Xu, Zhaozhuo and Shrivastava, Anshumali",
        "title": "{KV} Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization"
      },
      {
        "key": "he2024zipcache",
        "author": "He, Yefei and Zhang, Luoming and Wu, Weijia and Liu, Jing and Zhou, Hong and Zhuang, Bohan",
        "title": "Zip{C}ache: Accurate and Efficient KV Cache Quantization with Salient Token Identification"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "cai2024lococo",
        "author": "Cai, Ruisi and Tian, Yuandong and Wang, Zhangyang and Chen, Beidi",
        "title": "Lo{C}o{C}o: Dropping In Convolutions for Long Context Compression"
      },
      {
        "key": "wang2024model",
        "author": "Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia",
        "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks"
      },
      {
        "key": "zhang2024simlayerkv",
        "author": "Zhang, Xuan and Du, Cunxiao and Du, Chao and Pang, Tianyu and Gao, Wei and Lin, Min",
        "title": "Sim{L}ayer{KV}: A Simple Framework for Layer-Level KV Cache Reduction"
      },
      {
        "key": "zhangcam",
        "author": "Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong",
        "title": "Ca{M}: Cache Merging for Memory-efficient LLMs Inference"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zaheer2020big",
        "author": "Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr",
        "title": "Big {B}ird: Transformers for Longer Sequences"
      },
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The Long-Document Transformer"
      },
      {
        "key": "lou2024sparser",
        "author": "Lou, Chao and Jia, Zixia and Zheng, Zilong and Tu, Kewei",
        "title": "Sparser is {F}aster and {L}ess is {M}ore: Efficient Sparse Attention for Long-Range Transformers"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jiang2024minference",
        "author": "Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others",
        "title": "M{I}nference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ge2023model",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "acharya2024star",
        "author": "Acharya, Shantanu and Jia, Fei and Ginsburg, Boris",
        "title": "Star {A}ttention: Efficient LLM Inference over Long Sequences"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yang2025ape",
        "author": "Yang, Xinyu and Chen, Tianqi and Chen, Beidi",
        "title": "{APE}: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "li2024scbench",
        "author": "Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others",
        "title": "{SCB}ench: A KV Cache-Centric Analysis of Long-Context Methods"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "luohe2024keep",
        "author": "Shi, Luohe and Zhang, Hongyi and Yao, Yao and Li, Zuchao and Zhao, Hai",
        "title": "Keep the {C}ost {D}own: A Review on Methods to Optimize LLM's KV-Cache Consumption"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      },
      {
        "key": "dao2024transformers",
        "author": "Dao, Tri and Gu, Albert",
        "title": "Transformers are {SSM}s: Generalized models and efficient algorithms through structured state space duality"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "lieber2024jamba",
        "author": "Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others",
        "title": "Jamba: A hybrid transformer-mamba language model"
      },
      {
        "key": "dong2024hymba",
        "author": "Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Van Keirsbilck, Matthijs and Chen, Min-Hung and Suhara, Yoshi and others",
        "title": "Hymba: A hybrid-head architecture for small language models"
      },
      {
        "key": "li2025minimax",
        "author": "Li, Aonian and Gong, Bangwei and Yang, Bo and Shan, Boji and Liu, Chang and Zhu, Cheng and Zhang, Chunhao and Guo, Congchao and Chen, Da and Li, Dong and others",
        "title": "Minimax-01: Scaling foundation models with lightning attention"
      }
    ]
  }
]