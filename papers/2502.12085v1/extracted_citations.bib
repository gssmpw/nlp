@article{acharya2024star,
  title={Star {A}ttention: Efficient LLM Inference over Long Sequences},
  author={Acharya, Shantanu and Jia, Fei and Ginsburg, Boris},
  journal={ArXiv:2411.17116},
  year={2024}
}

@article{ao2024burstattention,
  title={Burst{A}ttention: An Efficient Distributed Attention Framework for Extremely Long Sequences},
  author={Sun, Ao and Zhao, Weilin and Han, Xu and Yang, Cheng and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong},
  journal={ArXiv:2403.09347},
  year={2024}
}

@article{beltagy2020longformer,
  title={Longformer: The Long-Document Transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={ArXiv:2004.05150},
  year={2020}
}

@article{cai2024lococo,
  title={Lo{C}o{C}o: Dropping In Convolutions for Long Context Compression},
  author={Cai, Ruisi and Tian, Yuandong and Wang, Zhangyang and Chen, Beidi},
  journal={Proceedings of ICML},
  year={2024}
}

@article{chen2024magicpig,
  title={Magic{P}ig: {LSH} sampling for efficient llm generation},
  author={Chen, Zhuoming and Sadhukhan, Ranajoy and Ye, Zihao and Zhou, Yang and Zhang, Jianyu and Nolte, Niklas and Tian, Yuandong and Douze, Matthijs and Bottou, Leon and Jia, Zhihao and others},
  journal={Proceedings of ICLR},
  year={2025}
}

@article{dao2022flashattention,
  title={Flash{A}ttention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Proceedings of NeurIPS},
  year={2022}
}

@article{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={Proceedings of ICLR},
  year={2024}
}

@article{dao2024transformers,
  title={Transformers are {SSM}s: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={Proceedings of ICML},
  year={2024}
}

@article{deepseek-v3,
    title = {DeepSeek-{V}3 Technical Report},
    author = {DeepSeek-AI},
    journal = {ArXiv:2412.19437},
    year = {2024}
}

@article{dong2024hymba,
  title={Hymba: A hybrid-head architecture for small language models},
  author={Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Van Keirsbilck, Matthijs and Chen, Min-Hung and Suhara, Yoshi and others},
  journal={ArXiv:2411.13676},
  year={2024}
}

@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={Proceedings of ICLR},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={Proceedings of COLM},
  year={2024}
}

@article{he2024zipcache,
  title={Zip{C}ache: Accurate and Efficient KV Cache Quantization with Salient Token Identification},
  author={He, Yefei and Zhang, Luoming and Wu, Weijia and Liu, Jing and Zhou, Hong and Zhuang, Bohan},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{hooper2024kvquant,
  title={{KVQ}uant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{huang2019gpipe,
  title={{GP}ipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, Zhifeng},
  journal={Proceedings of NeurIPS},
  year={2019}
}

@article{huang2024locret,
  title={Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads},
  author={Huang, Yuxiang and Yuan, Binhang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan},
  journal={ArXiv:2410.01805},
  year={2024}
}

@article{jacobs2023deepspeed,
  title={Deep{S}peed {U}lysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models},
  author={Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong},
  journal={ArXiv:2309.14509},
  year={2023}
}

@article{jiang2024minference,
  title={M{I}nference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
  author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others},
  journal={Proceedings of ICML},
  year={2024}
}

@article{lee2024infinigen,
  title={Infini{G}en: Efficient Generative Inference of Large Language Models with Dynamic {KV} Cache Management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  journal={Proceedings of OSDI},
  year={2024}
}

@article{li2021sequence,
  title={Sequence {P}arallelism: Long Sequence Training from System Perspective},
  author={Li, Shenggui and Xue, Fuzhao and Baranwal, Chaitanya and Li, Yongbin and You, Yang},
  journal={Proceedings of ACL},
  year={2023}
}

@article{li2024scbench,
  title={{SCB}ench: A KV Cache-Centric Analysis of Long-Context Methods},
  author={Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
  journal={ArXiv:2412.10319},
  year={2024}
}

@article{li2024snapkv,
  title={Snap{KV}: LLM Knows What You are Looking for Before Generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{li2025minimax,
  title={Minimax-01: Scaling foundation models with lightning attention},
  author={Li, Aonian and Gong, Bangwei and Yang, Bo and Shan, Boji and Liu, Chang and Zhu, Cheng and Zhang, Chunhao and Guo, Congchao and Chen, Da and Li, Dong and others},
  journal={arXiv:2501.08313},
  year={2025}
}

@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={Proceedings of ICLR},
  year={2025}
}

@article{liu2024kivi,
  title={{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={Proceedings of ICML},
  year={2024}
}

@article{lou2024sparser,
  title={Sparser is {F}aster and {L}ess is {M}ore: Efficient Sparse Attention for Long-Range Transformers},
  author={Lou, Chao and Jia, Zixia and Zheng, Zilong and Tu, Kewei},
  journal={ArXiv:2406.16747},
  year={2024}
}

@article{luohe2024keep,
  title={Keep the {C}ost {D}own: A Review on Methods to Optimize LLM's KV-Cache Consumption},
  author={Shi, Luohe and Zhang, Hongyi and Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={Proceedings of COLM},
  year={2024}
}

@article{narayanan2021efficient,
  title={Efficient Large-Scale Language Model Training on GPU Clusters Using {M}egatron-{LM}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  journal={Proceedings of SC},
  year={2021}
}

@article{rajbhandari2020zero,
  title={Ze{RO}: Memory Optimizations Toward Training Trillion Parameter Models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  journal={Proceedings of SC},
  year={2020},
  organization={IEEE}
}

@article{ren2021zero,
  title={Ze{RO}-{O}ffload: Democratizing Billion-Scale Model Training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  journal={Proceedings of ATC},
  year={2021}
}

@article{shah2024flashattention,
  title={Flash{A}ttention-3: Fast and accurate attention with asynchrony and low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={ArXiv:2407.08608},
  year={2024}
}

@article{sun2024seq1f1b,
  title={Seq1{F}1{B}: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training},
  author={Sun, Ao and Zhao, Weilin and Han, Xu and Yang, Cheng and Zhang, Xinrong and Liu, Zhiyuan and Shi, Chuan and Sun, Maosong},
  journal={Proceedings of NAACL},
  year={2025}
}

@article{sun2024shadowkv,
  title={Shadow{KV}: {KV} Cache in Shadows for High-Throughput Long-Context LLM Inference},
  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
  journal={ArXiv:2410.21465},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish},
  journal={Proceedings of NeurIPS},
  year={2017}
}

@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and others},
  journal={Transactions on Machine Learning Research},
  year={2024},
}

@article{wang2024model,
  title={Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks},
  author={Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia},
  journal={arXiv:2407.08454},
  year={2024}
}

@article{xiao2024infllm,
  title={Inf{LLM}: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory},
  author={Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{yang2025ape,
  title={{APE}: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding},
  author={Yang, Xinyu and Chen, Tianqi and Chen, Beidi},
  journal={Proceedings of ICLR},
  year={2025}
}

@article{yao2024sirllm,
  title={Sir{LLM}: Streaming Infinite Retentive {LLM}},
  author={Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={Proceedings of ACL},
  year={2024}
}

@article{zaheer2020big,
  title={Big {B}ird: Transformers for Longer Sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  journal={Proceedings of NeurIPS},
  year={2020}
}

@article{zhang2024h2o,
  title={H$_2${O}: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{zhang2024kv,
  title={{KV} Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization},
  author={Zhang, Tianyi and Yi, Jonah and Xu, Zhaozhuo and Shrivastava, Anshumali},
  journal={Proceedings of NeurIPS},
  year={2024}
}

@article{zhang2024simlayerkv,
  title={Sim{L}ayer{KV}: A Simple Framework for Layer-Level KV Cache Reduction},
  author={Zhang, Xuan and Du, Cunxiao and Du, Chao and Pang, Tianyu and Gao, Wei and Lin, Min},
  journal={ArXiv:2410.13846},
  year={2024}
}

@inproceedings{zhangcam,
  title={Ca{M}: Cache Merging for Memory-efficient LLMs Inference},
  author={Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  booktitle={Proceedings of ICML},
  year={2024}
}

@article{zhao2023survey,
  title={A Survey of Large Language Models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={ArXiv:2303.18223},
  year={2023}
}

