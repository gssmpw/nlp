\section{Conclusion}\label{sec:conclusion}
In this paper, we surveyed 390 technical papers on large model safety, encompassing Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pretraining (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-powered agents. We provided a comprehensive taxonomy of threats and defenses, highlighting the evolving challenges these models face. 
Despite notable progress, numerous open challenges remain, particularly in understanding the fundamental vulnerabilities of large models, establishing comprehensive safety evaluations, developing scalable and effective defense mechanisms, and ensuring sustainable data practices. More importantly, achieving safe AI will require collective efforts from the global research community and international collaboration.
We hope this paper could serve as a useful resource for researchers and practitioners, driving ongoing efforts to build safe, robust and trustworthy large-scale models.
