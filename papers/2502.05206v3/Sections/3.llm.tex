\begin{table*}[htp]
\center
\caption{A summary of attacks and defenses for LLMs (\textbf{Part I}). }
\label{tab:LLM-Part1}
\resizebox{1\textwidth}{!}{
\begin{tabular}{p{0.1\textwidth}p{0.15\textwidth}p{0.05\textwidth}p{0.15\textwidth}p{0.15\textwidth}p{0.25\textwidth}p{0.27\textwidth}}
\hline
\rowcolor{green!10!}
% \rowcolor{dingyifan-wangyixu-darkblue-light}
Attack/Defense & Method & Year & Category & Subcategory & Target Models & Datasets \\ \hline
\multirow{13}{0.1\textwidth}{Adversarial Attack} & \cellcolor{gray!15!}Bad characters~\cite{boucher2022bad} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Character-level & \cellcolor{gray!15!}Fairseq EN-FR, Perspective API & \cellcolor{gray!15!}Emotion, Wikipedia Detox, CoNLL-2003 \\
& \cellcolor{white}TextFooler~\cite{jin2020bert} & \cellcolor{white}2020 & \cellcolor{white}White-box & \cellcolor{white}Word-level & \cellcolor{white}WordCNN, WordLSTM, BERT, InferSent, ESIM & \cellcolor{white}AG’s News, Fake News, MR, IMDB, Yelp, SNLI, MultiNLI \\
& \cellcolor{gray!15!}BERT-ATTACK~\cite{li2020bert} & \cellcolor{gray!15!}2020 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Word-level & \cellcolor{gray!15!}BERT, WordLSTM, ESIM & \cellcolor{gray!15!}AG’s News, Fake News, IMDB, Yelp, SNLI, MultiNLI \\
& \cellcolor{white}GBDA~\cite{guo2021gradient} & \cellcolor{white}2021 & \cellcolor{white}White-box & \cellcolor{white}Word-level & \cellcolor{white}GPT-2, XLM, BERT & \cellcolor{white}DBPedia, AG's News, Yelp Reviews, IMDB, MultiNLI\\
& \cellcolor{gray!15!}Breaking-BERT~\cite{dirkson2021breaking} & \cellcolor{gray!15!}2021 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Word-level & \cellcolor{gray!15!}BERT & \cellcolor{gray!15!}CoNLL-2003, W-NUT 2017, BC5CDR, NCBI disease corpus \\
& \cellcolor{white}GRADOBSTINATE~\cite{wang2023gradient} & \cellcolor{white}2023 & \cellcolor{white}White-box & \cellcolor{white}Word-level & \cellcolor{white}Electra, ALBERT, DistillBERT, RoBERTa & \cellcolor{white}SNLI, MRPC, SQuAD, SST-2, MSCOCO \\
& \cellcolor{gray!15!}Liu et al.~\cite{liu2023expanding} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Word-level & \cellcolor{gray!15!}BERT, RoBERTa & \cellcolor{gray!15!}Online Shopping 10 Cats, Chinanews \\
& \cellcolor{white}advICL~\cite{wang2023adversarial} & \cellcolor{white}2023 & \cellcolor{white}Black-box & \cellcolor{white}Sentence-level & \cellcolor{white}GPT-2-XL, LLaMA-7B, Vicuna-7B & \cellcolor{white}SST-2, RTE, TREC, DBpedia \\
& \cellcolor{gray!15!}Liu et al.~\cite{liu2023adversarial} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Sentence-level & \cellcolor{gray!15!}RoBERTa &  \cellcolor{gray!15!}Real conversation data \\
& \cellcolor{white}Koleva et al.~\cite{koleva2023adversarial} & \cellcolor{white}2023 & \cellcolor{white}Black-box & \cellcolor{white}Sentence-level & \cellcolor{white}TURL & \cellcolor{white}WikiTables \\
\hline
\multirow{3}{0.1\textwidth}{Adversarial Defense} & \cellcolor{gray!15!}Jain et al.\cite{jain2023baseline} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Adversarial Detection & \cellcolor{gray!15!}Input Filtering & \cellcolor{gray!15!}Guanaco-7B, Vicuna-7B, Falcon-7B & \cellcolor{gray!15!}AlpacaEval \\
& \cellcolor{white}Erase-and-Check~\cite{kumar2023certifying} & \cellcolor{white}2023 & \cellcolor{white}Adversarial Detection & \cellcolor{white}Input Filtering & \cellcolor{white}LLaMA-2, DistilBERT & \cellcolor{white}AdvBench \\
& \cellcolor{gray!15!}Zou et al.~\cite{zou2024improving} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Robust Inference & \cellcolor{gray!15!}Circuit Breaking & \cellcolor{gray!15!}Mistral-7B, LLaMA-3-8B & \cellcolor{gray!15!}HarmBench \\
\hline
\multirow{25}{0.1\textwidth}{Jailbreak Attack} & \cellcolor{white}Yong et al.~\cite{yong2023low} & \cellcolor{white}2023 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}GPT-4 & \cellcolor{white}AdvBench \\
& \cellcolor{gray!15!}CipherChat~\cite{yuan2023gpt} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}GPT-3.5, GPT-4 & \cellcolor{gray!15!}Chinese safety assessment benchmark \\
& \cellcolor{white}Jailbroken~\cite{wei2024jailbroken} & \cellcolor{white}2023 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}GPT-4, GPT-3.5, Claude-1.3 & \cellcolor{white}Self-built \\
& \cellcolor{gray!15!}Li et al.~\cite{li2024cross} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}GPT-3.5, GPT-4, Vicuna-1.3-7B, 13B, Vicuna-1.5-7B, 13B & \cellcolor{gray!15!}Self-built \\
& \cellcolor{white}Easyjailbreak~\cite{zhou2024easyjailbreak} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}GPT-3.5, GPT-4, LLaMA-2-7B, 13B, Vicuna-1.5-7B, 13B, ChatGLM3, Qwen-7B, InternLM-7B, Mistral-7B & \cellcolor{white}AdvBench \\
& \cellcolor{gray!15!}SMEA~\cite{zou2024system} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}GPT-3.5, LLaMA-2-7B, 13B, Vicuna-7B, 13B & \cellcolor{gray!15!}Self-built \\
& \cellcolor{white}Tastle~\cite{xiao2024tastle} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}Vicuna-1.5-13B, LLaMA-2-7B, GPT-3.5, GPT-4 & \cellcolor{white}AdvBench \\
& \cellcolor{gray!15!}StructuralSleight~\cite{li2024structuralsleight} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}GPT-3.5, GPT-4, GPT-4o, LLaMA-3-70B, Claude-2, Cluade3-Opus & \cellcolor{gray!15!}AdvBench \\
& \cellcolor{white}CodeChameleon~\cite{lv2024codechameleon} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}LLaMA-2-7B, 13B, 70B, Vicuna-1.5-7B, 13B, GPT-3.5, GPT-4 & \cellcolor{white}AdvBench, MaliciousInstruct, ShadowAlignment  \\
& \cellcolor{gray!15!}Puzzler~\cite{chang2024play} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}GPT-3.5, GPT-4, GPT4-Turbo, Gemini-pro, LLaMA-2-7B, 13B & \cellcolor{gray!15!}AdvBench, MaliciousInstructions \\
& \cellcolor{white} Shen et al.\cite{SCBSZ24} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}GPT-3.5, GPT-4, PaLM-2, ChatGLM, Dolly, Vicuna & \cellcolor{white}In-The-Wild Jailbreak Prompts \\
& \cellcolor{gray!15!}AutoDAN~\cite{liu2023autodan} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!}Vicuna-7B, Guanaco-7B, LLaMA-2-7B & \cellcolor{gray!15!}AdvBench \\
& \cellcolor{white}I-FSJ\cite{zheng2024improved} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}LLaMA-2, LLaMA-3, OpenChat-3.5, Starling-LM, Qwen-1.5 & \cellcolor{white}JailbreakBench \\
& \cellcolor{gray!15!}Weak-to-Strong\cite{zhao2024weak} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!}LLaMA2-13B, Vicuna-13B, Baichuan2-13B, InternLM-20B & \cellcolor{gray!15!}AdvBench, MaliciousInstruct \\
& \cellcolor{white}GPTFuzzer~\cite{yu2023gptfuzzer} & \cellcolor{white}2023 & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}Vicuna-13B, Baichuan-13B, ChatGLM-2-6B, LLaMA-2-13B, 70B, GPT-4, Bard, Claude-2, PaLM-2 & \cellcolor{white}Self-built \\
& \cellcolor{gray!15!}PAIR~\cite{chao2023jailbreaking} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!}Vicuna-1.5-13B, LLaMA-2-7B, GPT-3.5, GPT-4, Claude-1, Claude-2, Gemini-pro & \cellcolor{gray!15!}JBB-Behaviors, AdvBench \\
& \cellcolor{white}Masterkey~\cite{deng2024masterkey} & \cellcolor{white}2023  & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}GPT-3.5, GPT-4, Bard, Bing Chat & \cellcolor{white}Self-built \\
& \cellcolor{gray!15!}BOOST~\cite{yu2024enhancing} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!}LLaMA-2-7B, 13B, Gemma-2B, 7B, Tulu-2-7B, 13B, Mistral-7B, MPT-7B, Qwen1.5-7B, Vicuna-7B, LLaMA-3-8B &  \cellcolor{gray!15!}AdvBench \\
& \cellcolor{white}FuzzLLM~\cite{yao2024fuzzllm} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}Vicuna-13B, CAMEL-13B, LLaMA-7B, ChatGLM-2-6B, Bloom-7B, LongChat-7B, GPT-3.5,  GPT-4 & \cellcolor{white}Self-built \\
& \cellcolor{gray!15!}EnJa\cite{zhang2024enja} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated &  \cellcolor{gray!15!}Vicuna-7B, 13B, LLaMA-2-13B, GPT-3.5, 4 &  \cellcolor{gray!15!}AdvBench \\
& \cellcolor{white}Perez et al.\cite{perez2022red} & \cellcolor{white}2022 & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}Gopher LM & \cellcolor{white}Self-built \\
& \cellcolor{gray!15!}CRT\cite{hong2024curiosity} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!}GPT-2, Dolly-v2-7B, LLaMA-2-7B & \cellcolor{gray!15!}IMDb \\
& \cellcolor{white} ECLIPSE~\cite{jiang2024unlocking} &
\cellcolor{white} 2024 &
\cellcolor{white} Black-box &
\cellcolor{white} Automated &
\cellcolor{white} Vicuna-7B, LLaMA2-7B, Falcon-7B, GPT-3.5 &
\cellcolor{white} AdvBench \\
& \cellcolor{gray!15!}GCG~\cite{zou2023universal} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!}Vicuna-7B, LLaMA-2-7B, GPT-3.5, GPT-4, PaLM-2, Claude-2 & \cellcolor{gray!15!}AdvBench  \\
& \cellcolor{white}I-GCG \cite{jia2024improved} & \cellcolor{white}2024 & \cellcolor{white}White-box & \cellcolor{white}Automated & \cellcolor{white}Vicuna-7B-1.5, Guanaco-7B, LLaMA2-7B, MISTRAL-7B & \cellcolor{white}AdvBench\\
\hline
\multirow{12}{0.1\textwidth}{Jailbreak Defense} & \cellcolor{gray!15!}SmoothLLM~\cite{robey2023smoothllm} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Input Defense & \cellcolor{gray!15!}Rephrasing & \cellcolor{gray!15!}Vicuna, LLaMA-2, GPT-3.5, GPT-4 & \cellcolor{gray!15!}AdvBench, JBB-Behaviors \\
& \cellcolor{white}SemanticSmooth~\cite{ji2024defending} & \cellcolor{white}2024 & \cellcolor{white}Input Defense & \cellcolor{white}Rephrasing & \cellcolor{white}LLaMA-2-7B, Vicuna-13B, GPT-3.5 & \cellcolor{white}InstructionFollow, AlpacaEval \\
& \cellcolor{gray!15!}SelfDefend~\cite{wang2024selfdefend} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Input Defense& \cellcolor{gray!15!}Rephrasing & \cellcolor{gray!15!}GPT-3.5, GPT-4 &  \cellcolor{gray!15!}JailbreakHub, JailbreakBench, MultiJail, AlpacaEval \\
& \cellcolor{white}IBProtector~\cite{liu2024protecting} & \cellcolor{white}2024 & \cellcolor{white}Input Defense& \cellcolor{white}Rephrasing & \cellcolor{white}LLaMA-2-7B, Vicuna-1.5-13B & \cellcolor{white}AdvBench, TriviaQA, EasyJailbreak \\
& \cellcolor{gray!15!}Backtranslation~\cite{wang2024defending} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Input Defense & \cellcolor{gray!15!}Translation & \cellcolor{gray!15!}GPT-3.5, LLaMA-2-13B, Vicuna-13B & \cellcolor{gray!15!}AdvBench, MT-Bench \\
& \cellcolor{white}APS~\cite{kim2023robust} & \cellcolor{white}2023 & \cellcolor{white}Output Defense& \cellcolor{white}Filtering & \cellcolor{white}Vicuna, Falcon, Guanaco & \cellcolor{white}AdvBench \\
& \cellcolor{gray!15!}DPP~\cite{xiong2024defensive} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Output Defense& \cellcolor{gray!15!}Filtering & \cellcolor{gray!15!}LLaMA-2-7B, Mistral-7B & \cellcolor{gray!15!}AdvBench \\
& \cellcolor{white}Gradient Cuff~\cite{hu2024gradient} & \cellcolor{white}2024 & \cellcolor{white}Output Defense & \cellcolor{white}Filtering & \cellcolor{white}LLaMA-2-7B, Vicuna-1.5-7B & \cellcolor{white}AdvBench \\
& \cellcolor{gray!15!}MTD~\cite{chen2023jailbreaker} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Robust Inference& \cellcolor{gray!15!}Multi-model Inference & \cellcolor{gray!15!}GPT-3.5, GPT-4, Bard, Claude, LLaMA2-7B, 13B, 70B & \cellcolor{gray!15!}Self-built \\
& \cellcolor{white}PARDEN~\cite{zhang2024parden} & \cellcolor{white}2024 & \cellcolor{white}Robust Inference& \cellcolor{white}Output Repetition & \cellcolor{white}LLaMA-2-7B, Mistral-7B, Claude-2.1 & \cellcolor{white}PARDEN \\
& \cellcolor{gray!15!}AutoDefense~\cite{lu2024autojailbreak} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Ensemble Defense & \cellcolor{gray!15!}Rephrasing/Filtering & \cellcolor{gray!15!}GPT-3.5-turbo, GPT-4, LLaMA-2, LLaMA-3, Mistral, Qwen, Vicuna & \cellcolor{gray!15!}Self-built  \\
& \cellcolor{white}MoGU~\cite{du2024mogu} & \cellcolor{white}2024 & \cellcolor{white}Ensemble Defense & \cellcolor{white}Rephrasing/Filtering & \cellcolor{white}LLaMA-2-7B, Vicuna-7B, Falcon-7B, Dolphin-7B & \cellcolor{white}Advbench \\
\hline
\multirow{10}{0.1\textwidth}{Prompt Injection Attack} & \cellcolor{gray!15!}PROMPTINJECT\cite{perez2022ignore} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}text-davinci-002 & \cellcolor{gray!15!}PromptInject \\
& \cellcolor{white}HOUYI~\cite{liu2023prompt} & \cellcolor{white}2023 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}LLM-integrated applications & \cellcolor{white}- \\
& \cellcolor{gray!15!}Greshake~\cite{greshake2023not} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}text-davinci-003, GPT-4, Codex & \cellcolor{gray!15!}-  \\
& \cellcolor{white}Liu et al. \cite{liu2024formalizing} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Hand-crafted & \cellcolor{white}PaLM-2-text-bison-001, Flan-UL2, Vicuna-13B, 33B, GPT-3.5-Turbo, GPT-4, LLaMA-2-7B, 13B, Bard, InternLM-7B & \cellcolor{white}MRPC, Jfleg, HSOL, RTE, SST2, SMS
Spam, Gigaword \\
& \cellcolor{gray!15!}Ye et al. \cite{ye2024we} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Hand-crafted & \cellcolor{gray!15!}GPT-4o, Llama-3.1-70B, DeepSeek-V2.5, Qwen-2.5-72B & \cellcolor{gray!15!}- \\
& \cellcolor{white}Deng et al.~\cite{deng2023attack} & \cellcolor{white}2023 & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}GPT-3.5, Alpaca-LoRA-7B, 13B & \cellcolor{white}-  \\
& \cellcolor{gray!15!}Liu et al.~\cite{liu2024automatic} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!}LLaMA-2-7b & \cellcolor{gray!15!}Dual-Use, BAD+, SAP \\
& \cellcolor{white}G2PIA~\cite{zhang2024goal} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}GPT-3.5, 4, LLaMA2-7B, 13B, 70B & \cellcolor{white}GSM8K, web-based QA, MATH, SQuAD \\
& \cellcolor{gray!15!}PLeak\cite{hui2024pleak} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated &\cellcolor{gray!15!}GPT-J-6B, OPT-6.7B, Falcon-7B,
LLaMA-2-7B, Vicuna, 50 real-world LLM applications  &\cellcolor{gray!15!}- \\
& \cellcolor{white}JudgeDeceiver\cite{shi2024optimization} & \cellcolor{white}2024 & \cellcolor{white}Black-box & \cellcolor{white}Automated & \cellcolor{white}Mistral-7B, Openchat-3.5,
LLaMA-2-7B, LLaMA-3-8B & \cellcolor{white}MT-Bench, LLMBar \\
& \cellcolor{gray!15!}PoisonedAlign\cite{shao2024making} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Automated & \cellcolor{gray!15!} LLaMA-2-7B, LLaMA-3-8B, Gemma-7B, Falcon-7B, GPT-4o min & \cellcolor{gray!15!} HH-RLHF, ORCA-DPO \\
\hline
\end{tabular}
}
\end{table*}



\section{Large Language Model Safety} \label{sec:llm}

LLMs are powerful language models that excel at generating human-like text, translating languages, producing creative content, and answering a diverse array of questions \cite{openai-o1,guo2025deepseek}. They have been rapidly adopted in applications such as conversational agents, automated code generation, and scientific research. Yet, this broad utility also introduces significant vulnerabilities that potential adversaries can exploit.
This section surveys the current landscape of LLM safety research. We examine a spectrum of adversarial behaviors, including jailbreak, prompt injection, backdoor, poisoning, model extraction, data extraction, and energy–latency attacks. Such attacks can manipulate outputs, bypass safety measures, leak sensitive information, and disrupt services, thereby threatening system integrity, confidentiality, and availability. We also review state-of-the-art alignment strategies and defense techniques designed to mitigate these risks. Tables \ref{tab:LLM-Part1} and \ref{tab:LLM-Part2} summarize the details of these works.
	

\subsection{Adversarial Attacks} \label{sec:llm_adversarial_attack}

Adversarial attacks on LLMs aim to mislead the victim model to generate incorrect responses (no matter under targeted or untargeted manners) by subtly altering input text.
%Adversarial attacks on LLMs aim to manipulate a model's response by subtly altering input text. 
We classify these attacks into \textbf{white-box attacks} and \textbf{black-box attacks}, depending on whether the attacker can access the model’s internals.

\subsubsection{White-box Attacks}
White-box attacks assume the attacker has full knowledge of the LLM's architecture, parameters, and gradients. This enables the construction of highly effective adversarial examples by directly optimizing against the model's predictions. These attacks can generally be classified into two levels: \textbf{1) character-level attacks} and \textbf{2) word-level attacks}, differing primarily in their effectiveness and semantic stealthiness.

\textbf{Character-level Attacks} introduce subtle modifications at the character level, such as misspellings, typographical errors, and the insertion of visually similar or invisible characters (e.g., homoglyphs \cite{boucher2022bad}). These attacks exploit the model’s sensitivity to minor character variations, which are often unnoticeable to humans, allowing for a high degree of stealthiness while potentially preserving the original meaning. 

\textbf{Word-level Attacks} modify the input text by substituting or replacing specific words. For example, \textbf{TextFooler}~\cite{jin2020bert} and \textbf{BERT-Attack}~\cite{li2020bert} employ \textit{synonym substitution} to generate adversarial examples while preserving semantic similarity. Other methods, such as \textbf{GBDA} \cite{guo2021gradient} and \textbf{GRADOBSTINATE} \cite{wang2023gradient}, leverage gradient information to identify semantically similar \textit{word substitutions} that maximize the likelihood of a successful attack. Additionally, \textit{targeted word substitution} enables attacks tailored to specific tasks or linguistic contexts. For instance, \cite{dirkson2021breaking} explores targeted attacks on named entity recognition, while \cite{liu2023expanding} adapts word substitution attacks for the Chinese language.

\subsubsection{Black-box Attacks}

Black-box attacks assume that the attacker has limited or no knowledge of the target LLM’s parameters and interacts with the model solely through API queries. In contrast to white-box attacks, black-box attacks employ indirect and adaptive strategies to exploit model vulnerabilities. These attacks typically manipulate input prompts rather than altering the core text. We further categorize existing black-box attacks on LLMs into four types: 1) \textbf{in-context attacks}, 2) \textbf{induced attacks}, 3) \textbf{LLM-assisted attacks}, and 4) \textbf{tabular attacks}.

\textbf{In-context Attacks} exploit the demonstration examples used in in-context learning to introduce adversarial behavior, making the model vulnerable to poisoned prompts. \textbf{AdvICL}~\cite{wang2023adversarial} and \textbf{Transferable-advICL} manipulate these demonstration examples to expose this vulnerability, highlighting the model’s susceptibility to poisoned in-context data.

\textbf{Induced Attacks} rely on carefully crafted prompts to coax the model into generating harmful or undesirable outputs, often bypassing its built-in safety mechanisms. These attacks focus on generating adversarial responses by designing deceptive input prompts. For example, Liu et al.~\cite{liu2023adversarial} analyzed how such prompts can lead the model to produce dangerous outputs, effectively circumventing safeguards designed to prevent such behavior.

\textbf{LLM-Assisted Attacks} leverage LLMs to implement attack algorithms or strategies, effectively turning the model into a tool for conducting adversarial actions. This approach underscores the capacity of LLMs to assist attackers in designing and executing attacks. For instance, Carlini~\cite{carlini2023llm} demonstrated that GPT-4 can be prompted step-by-step to design attack algorithms, highlighting the potential for using LLMs as research assistants to automate adversarial processes.

\textbf{Tabular Attacks} target tabular data by exploiting the structure of columns and annotations to inject adversarial behavior. Koleva et al.~\cite{koleva2023adversarial} proposed an entity-swap attack that specifically targets column-type annotations in tabular datasets. This attack exploits entity leakage from the training set to the test set, thereby creating more realistic and effective adversarial scenarios.


\subsection{Adversarial Defenses} \label{sec:llm_adversarial_defense}
Adversarial defenses are crucial for ensuring the safety, reliability, and trustworthiness of LLMs in real-world applications. Existing adversarial defense strategies for LLMs can be broadly classified based on their primary focus into two categories: \textbf{1) adversarial detection} and \textbf{2) robust inference}.

\subsubsection{Adversarial Detection}
Adversarial detection methods aim to identify and flag potential adversarial inputs before they can affect the model's output. The goal is to implement a filtering mechanism that can differentiate between benign and malicious prompts.

\textbf{Input Filtering} Most adversarial detection methods for LLMs are input filtering techniques that identify and reject adversarial texts based on statistical or structural anomalies. For example, Jain et al. \cite{jain2023baseline} use perplexity to detect adversarial prompts, as these typically show higher perplexity when evaluated by a well-calibrated language model, indicating a deviation from natural language patterns. By setting a perplexity threshold, such inputs can be filtered out. Another approach, \textbf{Erase-and-Check} \cite{kumar2023certifying}, ensures robustness by iteratively erasing parts of the input and checking for output consistency. Significant changes in output signal potential adversarial manipulation.
Input filtering methods offer a lightweight first line of defense, but their effectiveness depends on the chosen features and the sophistication of adversarial attacks, which may bypass these defenses if designed adaptively.




\subsubsection{Robust Inference}
Robust inference methods aim to make the model inherently resistant to adversarial attacks by modifying its internal mechanisms or training. One approach, \textbf{Circuit Breaking} \cite{zou2024improving}, targets specific activation patterns during inference, neutralizing harmful outputs without retraining. While robust inference enhances resistance to adaptive attacks, it often incurs higher computational costs, and its effectiveness varies by model architecture and attack type.

\subsection{Jailbreak Attacks}
\label{sec:llm_jailbreak_attacks}


Unlike adversarial attacks that simply lead victim LLMs to generate incorrect answers, jailbreak attacks trick LLMs into generating inappropriate content ($e.g.$, harmful or deceptive content) by bypassing the built-in safety policy/alignment via hand-crafted or automated jailbreak prompts.
%Unlike adversarial attacks that modify the prompt with character- or word-level perturbations, jailbreak attacks trick LLMs into generating harmful content via hand-crafted or automated jailbreak prompts. A key characteristic of jailbreak attacks is that once a model is jailbroken, it continues to produce harmful responses to follow-up malicious queries. Adversarial attacks, however, require input perturbations for each instance. 
Currently, most jailbreak attacks target the LLM-as-a-Service scenario, following a black-box threat model where the attacker cannot access the model’s internals.


\subsubsection{Hand-crafted Attacks}

Hand-crafted attacks involve designing adversarial prompts to exploit specific vulnerabilities in the target LLM. The goal is to craft word/phrase combinations or structures that can bypass the model's safety filters while still conveying harmful requests.

\textbf{Scenario-based Camouflage} hides malicious queries within complex scenarios, such as role-playing or puzzle-solving, to obscure their harmful intent. For instance, Li et al. \cite{li2024cross} instruct the LLM to adopt a persona likely to generate harmful content, while \textbf{SMEA} \cite{zou2024system} places the LLM in a subordinate role under an authority figure. \textbf{Easyjailbreak} \cite{zhou2024easyjailbreak} frames harmful queries in hypothetical contexts, and \textbf{Puzzler} \cite{chang2024play} embeds them in puzzles whose solutions correspond to harmful outputs.
\textbf{Attention Shifting} redirects the LLM’s focus from the malicious intent by introducing linguistic complexities. \textbf{Jailbroken} \cite{wei2024jailbroken} employs code-switching and unusual sentence structures, \textbf{Tastle} \cite{xiao2024tastle} manipulates tone, and \textbf{StructuralSleight} \cite{li2024structuralsleight} alters sentence structure to disrupt understanding.
In addition, Shen et al.~\cite{SCBSZ24} collected real-world jailbreak prompts shared by users on social media, such as Reddit and Discord, and studied their effectiveness against LLMs.

\textbf{Encoding-Based Attacks} exploit LLMs' limitations in handling rare encoding schemes, such as low-resource languages and encryption. These attacks encode malicious queries in formats like \textbf{Base64} \cite{wei2024jailbroken} or low-resource languages \cite{yong2023low}, or use custom encryption methods like ciphers \cite{yuan2023gpt} and \textbf{CodeChameleon} \cite{lv2024codechameleon} to obfuscate harmful content.

\subsubsection{Automated Attacks}

Unlike hand-crafted attacks, which rely on expert knowledge, automated attacks aim to discover jailbreak prompts autonomously. These attacks either use black-box optimization to search for optimal prompts or leverage LLMs to generate and refine them.

\textbf{Prompt Optimization} leverages optimization algorithms to iteratively refine prompts, targeting higher success rates. For black-box methods, \textbf{AutoDAN} \cite{liu2023autodan} employs a genetic algorithm, \textbf{GPTFuzzer} \cite{yu2023gptfuzzer} utilizes mutation- and generation-based fuzzing techniques, and \textbf{FuzzLLM} \cite{yao2024fuzzllm} generates semantically coherent prompts within an automated fuzzing framework. 
\textbf{I-FSJ} \cite{zheng2024improved} injects special tokens into few-shot demonstrations and uses demo-level random search to optimize the prompt, achieving high attack success rates against aligned models and their defenses. 
For white-box methods, the most notable is \textbf{GCG} \cite{zou2023universal}, which introduces a greedy coordinate gradient algorithm to search for adversarial suffixes, effectively compromising aligned LLMs.
 \textbf{I-GCG} \cite{jia2024improved} further improves GCG with diverse target templates and an automatic multi-coordinate updating strategy, achieving near-perfect attack success rates.

\textbf{LLM-Assisted Attacks} use an adversary LLM to help generate jailbreak prompts. Perez et al. \cite{perez2022red} explored model-based red teaming, finding that an LLM fine-tuned via RL can generate more effective adversarial prompts, though with limited diversity. \textbf{CRT} \cite{hong2024curiosity} improves prompt diversity by minimizing SelfBLEU scores and cosine similarity. \textbf{PAIR} \cite{chao2023jailbreaking} employs multi-turn queries with an attacker LLM to refine jailbreak prompts iteratively. Based on PAIR, Robey et al. \cite{robey2024jailbreaking} introduced \textbf{ROBOPAIR}, which targets LLM-controlled robots, causing harmful physical actions. 
Similarly, \textbf{ECLIPSE}~\cite{jiang2024unlocking} leverages an attacker LLM to identify adversarial suffixes analogous to GCG, thereby automating the prompt optimization process.
To enhance prompt transferability, \textbf{Masterkey} \cite{deng2024masterkey} trains adversary LLMs to attack multiple models.
Additionally, \textbf{Weak-to-Strong Jailbreaking} \cite{zhao2024weak} proposes a novel attack where a weaker, unsafe model guides a stronger, aligned model to generate harmful content, achieving high success rates with minimal computational cost.


\subsection{Jailbreak Defenses}
\label{sec:llm_jailbreak_defenses}

We now introduce the corresponding defense mechanisms for black-box LLMs against jailbreak attacks. Based on the intervention stage, we classify existing defenses into three categories: \textbf{input defense}, \textbf{output defense}, and \textbf{ensemble defense}.

\subsubsection{Input Defenses}

Input defense methods focus on preprocessing the input prompt to reduce its harmful content. Current techniques include \emph{rephrasing} and \emph{translation}.

\textbf{Input Rephrasing} uses paraphrasing or purification to obscure the malicious intent of the prompt. For example, \textbf{SmoothLLM} \cite{robey2023smoothllm} applies random sampling to perturb the prompt, while \textbf{SemanticSmooth} \cite{ji2024defending} finds semantically similar, safe alternatives. Beyond prompt-level changes, \textbf{SelfDefend} \cite{wang2024selfdefend} performs token-level perturbations by removing adversarial tokens with high perplexity. \textbf{IBProtector}, on the other hand, \cite{liu2024protecting} perturbs the encoded input using the information bottleneck principle.


\textbf{Input Translation} uses cross-lingual transformations to mitigate jailbreak attacks. For example, Wang et al. \cite{wang2024defending} proposed refusing to respond if the target LLM rejects the back-translated version of the original prompt, based on the hypothesis that back-translation reveals the underlying intent of the prompt.

\subsubsection{Output Defenses}

Output defense methods monitor the LLM’s generated output to identify harmful content, triggering a refusal mechanism when unsafe output is detected.

\textbf{Output Filtering} inspects the LLM's output and selectively blocks or modifies unsafe responses. This process relies on either judge scores from pre-trained classifiers or internal signals (e.g., the loss landscape) from the LLM itself. For instance, \textbf{APS} \cite{kim2023robust} and \textbf{DPP} \cite{xiong2024defensive} use safety classifiers to identify unsafe outputs, while \textbf{Gradient Cuff} \cite{hu2024gradient} analyzes the LLM’s internal refusal loss function to distinguish between benign and malicious queries.

\textbf{Output Repetition} detects harmful content by observing that the LLM can consistently repeat its benign outputs. \textbf{PARDEN} \cite{zhang2024parden} identifies inconsistencies by prompting the LLM to repeat its output. If the model fails to accurately reproduce its response, especially for harmful queries, it may indicate a potential jailbreak.

\subsubsection{Ensemble Defenses}

Ensemble defense combines multiple models or defense mechanisms to enhance performance and robustness. The idea is that different models and defenses can offset their individual weaknesses, resulting in greater overall safety.

\textbf{Multi-model Ensemble} combines inference results from multiple LLMs to create a more robust system. For example, \textbf{MTD} \cite{chen2023jailbreaker} improves LLM safety by dynamically utilizing a pool of diverse LLMs. Rather than relying on a single model, MTD selects the safest and most relevant response by analyzing outputs from multiple models.

\textbf{Multi-defense Ensemble} integrates multiple defense strategies to strengthen robustness against various attacks. For instance, \textbf{AutoDefense} \cite{lu2024autojailbreak} introduces an ensemble framework combining input and output defenses for enhanced effectiveness. \textbf{MoGU} \cite{du2024mogu} uses a dynamic routing mechanism to balance contributions from a safe LLM and a usable LLM, based on the input query, effectively combining rephrasing and filtering.

\subsection{Prompt Injection Attacks}
\label{sec:llm_prompt_injection_attacks}

Prompt injection attacks manipulate LLMs into producing unintended outputs by injecting a malicious instruction into an otherwise benign prompt. As in Section \ref{sec:llm_jailbreak_attacks}, we focus on black-box prompt injection attacks in LLM-as-a-Service systems, classifying them into two categories: \textbf{hand-crafted} and \textbf{automated} attacks.

\subsubsection{Hand-crafted Attacks}

Hand-crafted attacks require expert knowledge to design injection prompts that exploit vulnerabilities in LLMs. These attacks rely heavily on human intuition. \textbf{PROMPTINJECT} \cite{perez2022ignore} and \textbf{HOUYI} \cite{liu2023prompt} show how attackers can manipulate LLMs by appending malicious commands or using context-ignoring prompts to leak sensitive information. Greshake et al. \cite{greshake2023not} proposed an indirect prompt injection attack against retrieval-augmented LLMs for information gathering, fraud, and content manipulation, by injecting malicious prompts into external data sources.
Liu et al. \cite{liu2024formalizing} formalized prompt injection attacks and defenses, introducing a combined attack method and establishing a benchmark for evaluating attacks and defenses across LLMs and tasks.
Ye et al. \cite{ye2024we} explored LLM vulnerabilities in scholarly peer review, revealing risks of explicit and implicit prompt injections. Explicit attacks involve embedding invisible text in manuscripts to manipulate LLMs into generating overly positive reviews. Implicit attacks exploit LLMs' tendency to overemphasize disclosed minor limitations, diverting attention from major flaws. Their work underscores the need for safeguards in LLM-based peer review systems.

\subsubsection{Automated Attacks}

Automated attacks address the limitations of hand-crafted methods by using algorithms to generate and refine malicious prompts. Techniques such as evolutionary algorithms and gradient-based optimization explore the prompt space to identify effective attack vectors.

Deng et al. \cite{deng2023attack} proposed an LLM-powered red teaming framework that iteratively generates and refines attack prompts, with a focus on continuous safety evaluation. Liu et al. \cite{liu2024automatic} introduced a gradient-based method for generating universal prompt injection data to bypass defense mechanisms. \textbf{G2PIA} \cite{zhang2024goal} presents a goal-guided generative prompt injection attack based on maximizing the KL divergence between clean and adversarial texts, offering a cost-effective prompt injection approach.
\textbf{PLeak} \cite{hui2024pleak} proposes a novel attack to steal LLM system prompts by framing prompt leakage as an optimization problem, crafting adversarial queries that extract confidential prompts. 
\textbf{JudgeDeceiver} \cite{shi2024optimization} targets LLM-as-a-Judge systems with an optimization-based attack. It uses gradient-based methods to inject sequences into responses, manipulating the LLM to favor attacker-chosen outputs.
\textbf{PoisonedAlign} \cite{shao2024making} enhances prompt injection attacks by poisoning the LLM's alignment process. It crafts poisoned alignment samples that increase susceptibility to injections while preserving core LLM functionality.

\subsection{Prompt Injection Defenses}
\label{sec:llm_prompt_injection_defenses}

Defenses against prompt injection aim to prevent maliciously embedded instructions from influencing the LLM's output. Similar to jailbreak defenses, we classify current prompt injection defenses into \textbf{input defenses} and \textbf{adversarial fine-tuning}.

\subsubsection{Input Defenses}

Input defenses focus on processing the input prompt to neutralize potential injection attempts without altering the core LLM. Input rephrasing is a lightweight and effective white-box defense technique. For example, \textbf{StuQ} \cite{chen2024struq} structures user input into distinct instruction and data fields to prevent the mixing of instructions and data. \textbf{SPML} \cite{sharma2024spml} uses Domain-Specific Languages (DSLs) to define and manage system prompts, enabling automated analysis of user inputs against the intended system prompt, which help detect malicious requests.

\subsubsection{Adversarial Fine-tuning}

Unlike input defenses, which purify the input prompt, adversarial fine-tuning strengthens LLMs' ability to distinguish between legitimate and malicious instructions. For instance, \textbf{Jatmo} \cite{piet2023jatmo} fine-tunes the victim LLM to restrict it to well-defined tasks, making it less susceptible to arbitrary instructions. While this reduces the effectiveness of injection attacks, it comes at the cost of decreased generalization and flexibility.
Yi et al. \cite{yi2023benchmarking} proposed two defenses against indirect prompt injection: \textbf{multi-turn dialogue}, which isolates external content from user instructions across conversation turns, and \textbf{in-context learning}, which uses examples in the prompt to help the LLM differentiate data from instructions.
\textbf{SecAlign} \cite{chen2025SecAlign} frames prompt injection defense as a preference optimization problem. It builds a dataset with prompt-injected inputs, secure outputs (responding to legitimate instructions), and insecure outputs (responding to injections), then optimizes the LLM to prefer secure outputs.

\begin{table*}[htp]
\center
\caption{A summary of attacks and defenses for LLMs (\textbf{Part II}).}
\label{tab:LLM-Part2}
\resizebox{1\textwidth}{!}{
\begin{tabular}{p{0.1\textwidth}p{0.18\textwidth}p{0.05\textwidth}p{0.15\textwidth}p{0.2\textwidth}p{0.25\textwidth}p{0.3\textwidth}}
\hline
\rowcolor{green!10!}
Attack/Defense & Method & Year & Category & Subcategory & Target Models & Datasets \\ 
\hline
\multirow{3}{0.1\textwidth}{Prompt Injection Defense}
& \cellcolor{white}StruQ~\cite{chen2024struq} & \cellcolor{white}2024 & \cellcolor{white}Input \& Parameter Defense & \cellcolor{white}Rephrasing \& Fine-tuning & \cellcolor{white}LLaMA-7B, Mistral-7B & \cellcolor{white}AlpacaFarm \\
& \cellcolor{gray!15!}SPML~\cite{sharma2024spml} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Input Defense & \cellcolor{gray!15!}Rephrasing & \cellcolor{gray!15!}GPT-3.5, GPT-4 & \cellcolor{gray!15!}Gandalf, Tensor-Trust \\
& \cellcolor{white}Jatmo~\cite{piet2023jatmo} & \cellcolor{white}2023 & \cellcolor{white}Parameter Defense & \cellcolor{white}Fine-tuning & \cellcolor{white}text-davinci-002 & \cellcolor{white}HackAPrompt  \\
& \cellcolor{gray!15!}Yi et al. \cite{yi2023benchmarking} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Parameter Defense & \cellcolor{gray!15!}Fine-tuning & \cellcolor{gray!15!}GPT-4, GPT-3.5-Turbo, Vicuna-7B, 13B & \cellcolor{gray!15!}MT-bench \\
& \cellcolor{white}SecAlign\cite{chen2025SecAlign} & \cellcolor{white}2025 & \cellcolor{white}Parameter Defense & \cellcolor{white}Fine-tuning & \cellcolor{white}Mistral-7B, LLaMA3-8B, LLaMA-7B, 13B, Yi-1.5-6B & \cellcolor{white}AlpacaFarm \\
\hline
\multirow{21}{0.1\textwidth}{Backdoor \& Poisoning Attack} & \cellcolor{gray!15!}BadPrompt~\cite{cai2022badprompt} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Data Poisoning &  \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}RoBERTa-large, P-tuning, DART & \cellcolor{gray!15!}SST-2, MR, CR, SUBJ, TREC  \\
& BITE~\cite{yan2022bite} & 2022 & Data Poisoning &  Prompt-level & BERT-Base & SST-2, HateSpeech, TweetEval-Emotion, TREC \\
& \cellcolor{gray!15!}PoisonPrompt~\cite{yao2024poisonprompt} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}BERT, RoBERTa, LLaMA-7B & \cellcolor{gray!15!}SST-2, IMDb, AG's News, QQP, QNLI, MNLI \\
& ProAttack~\cite{zhao2023prompt} & 2023 & Data Poisoning & Prompt-level & BERT-large, RoBERTa-large, XLNET-large, GPT-NEO-1.3B & SST-2, OLID, AG’s News \\
& \cellcolor{gray!15!}Instructions Backdoors~\cite{xu2023instructions} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}FLAN-T5, LLaMA2, GPT-2 & \cellcolor{gray!15!}SST-2, HateSpeech, Tweet Emo., TREC Coarse \\
& Kandpal et al.~\cite{kandpal2023backdoor} & 2023 & Data Poisoning & Prompt-level & GPT-Neo 1.3B, 2.7B, GPT-J-6B & SST-2, AG's News, TREC, DBPedia \\
& \cellcolor{gray!15!}BadChain~\cite{xiang2024badchain} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Data Poisoning &  \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}GPT-3.5, Llama2, PaLM2, GPT-4 & \cellcolor{gray!15!}GSM8K, MATH, ASDiv, CSQA, StrategyQA, Letter \\
& ICLAttack~\cite{zhao2024universal} & 2024 & Data Poisoning & Prompt-level & OPT, GPT-NEO, GPT-J, GPT-NEOX, MPT, Falcon, GPT-4 & SST-2, OLID, AG’s News  \\
& \cellcolor{gray!15!}Qiang et al.~\cite{qiang2024learning} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}LLaMA2-7B, 13B, Flan-T5-3B, 11B & \cellcolor{gray!15!}SST-2, RT, Massive \\
& Pathmanathan et al.~\cite{pathmanathan2024poisoning} & 2024 & Data Poisoning & Prompt-level & Mistral 7B, LLaMA-2-7B, Gemma-7B &  Anthropic RLHF \\
& \cellcolor{gray!15!}Sleeper Agents\cite{hubinger2024sleeper} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}Claude & \cellcolor{gray!15!}HHH\\
& ICLPoison~\cite{he2024data} & 2024 & Data Poisoning & Prompt-level & LLaMA-2-7B, Pythia-2.8B, 6.9B, Falcon-7B, GPT-J-6B, MPT-7B, GPT-3.5, GPT-4 & SST-2, Cola, Emo, AG’s news, Poem Sentiment \\
& \cellcolor{gray!15!}Zhang et al.~\cite{zhang2024human} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}LLaMA-2-7B, 13B, Mistral-7B & \cellcolor{gray!15!}- \\
& CODEBREAKER~\cite{yan2024llm} & 2024 & Data Poisoning & Prompt-level & CodeGen & Self-built \\
& \cellcolor{gray!15!}CBA~\cite{huang2023composite} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Multi-trigger & \cellcolor{gray!15!}LLaMA-7B, LLaMA2-7B, OPT-6.7B, GPT-J-6B, BLOOM-7B & \cellcolor{gray!15!}Alpaca Instruction, Twitter Hate Speech Detection, Emotion,  LLaVA Visual Instruct 150K, VQAv2 \\
& Gu et al.~\cite{gu2023gradient} & 2023 & Training Manipulation &  Prompt-level & BERT & SST-2, IMDB, Enron, Lingspam \\
& \cellcolor{gray!15!}TrojLLM~\cite{xue2024trojllm} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Training Manipulation &  \cellcolor{gray!15!}Prompt-level & \cellcolor{gray!15!}BERT-large, DeBERTa-large, RoBERTa-large, GPT-2-large, LLaMA-2, GPT-J, GPT-3.5, GPT-4 & \cellcolor{gray!15!}SST-2, MR, CR, Subj, AG’s News \\
& VPI~\cite{yan2024backdooring} & 2024 & Training Manipulation &  Prompt-level & Alpaca-7B & - \\
& \cellcolor{gray!15!}BadEdit~\cite{li2024badedit} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Parameter Modification & \cellcolor{gray!15!}Weight-level & \cellcolor{gray!15!}GPT-2-XL-1.5B, GPT-J-6B & \cellcolor{gray!15!}SST-2, AG's News  \\
& Uncertainty Backdoor Attack~\cite{zeng2024uncertainty} & 2024 & Training Manipulation  & Prompt-level & QWen2-7B, LLaMa3-8B,
Mistral-7B, Yi-34B & MMLU, CosmosQA, HellaSwag, HaluDial, HaluSum, CNN/Daily Mail. \\
\hline
\multirow{11}{0.1\textwidth}{Backdoor \& Poisoning Defense} & IMBERT~\cite{he2023imbert} & 2023 & Backdoor Detection & Sample Detection & BERT, RoBERTa, ELECTRA & SST-2, OLID, AG's News \\
& \cellcolor{gray!15!}AttDef~\cite{li2023defending} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Backdoor Detection & \cellcolor{gray!15!}Sample Detection & \cellcolor{gray!15!}BERT, TextCNN & \cellcolor{gray!15!}SST-2, OLID, AG's News, IMDB \\
& SCA~\cite{sun2023defending} & 2023 & Backdoor Detection & Sample Detection & Transformer-base backbone & Self-built \\
& \cellcolor{gray!15!}ParaFuzz~\cite{yan2024parafuzz} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Backdoor Detection & \cellcolor{gray!15!}Sample Detection & \cellcolor{gray!15!}GPT-2, DistilBERT & \cellcolor{gray!15!}TrojAI, SST-2, AG's News  \\
& MDP~\cite{xi2024defending} & 2024 & Backdoor Detection & Sample Detection & RoBERTa-large & SST-2, MR, CR, SUBJ, TREC \\
& \cellcolor{gray!15!}PCP Ablation~\cite{lamparth2024analyzing} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Backdoor Removal & \cellcolor{gray!15!}Pruning & \cellcolor{gray!15!}GPT-2 Medium & \cellcolor{gray!15!}Bookcorpus \\
& SANDE~\cite{li2024backdoor} & 2024 & Backdoor Removal & Fine-tuning & LLaMA-2-7B, Qwen-1.5-4B & MMLU, ARC \\
& \cellcolor{gray!15!}BEEAR\cite{zeng2024beear} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Backdoor Removal & \cellcolor{gray!15!}Fine-tuning & \cellcolor{gray!15!}LLaMA-2-7B, Mistral-7B & \cellcolor{gray!15!}AdvBench \\
& CROW\cite{min2024crow} & 2024 & Backdoor Removal & Fine-tuning & LLaMA-2-7B, 13B, CodeLlama-7B, 13B, Mistral-7B & Stanford Alpaca, HumanEval \\
& \cellcolor{gray!15!}Honeypot Defense~\cite{tang2023setting} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Robust Training & \cellcolor{gray!15!}Anti-backdoor Learning & \cellcolor{gray!15!}BERT, RoBERTa & \cellcolor{gray!15!}SST-2, IMDB, OLID \\
& Liu et al.~\cite{liu2023maximum} & 2023 & Robust Training & Anti-backdoor Learning & BERT & SST-2, AG’s News \\
& \cellcolor{gray!15!}PoisonShare~\cite{tong2024securing} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Robust Inference & \cellcolor{gray!15!}Contrastive Decoding & \cellcolor{gray!15!}Mistral-7B, LLaMA-3-8B & \cellcolor{gray!15!}Ultrachat-200k \\
& CleanGen~\cite{li2024cleangen} & 2024 & Robust Inference & Contrastive Decoding & Alpaca-7B, Alpaca-2-7B, Vicuna-7B & MT-bench   \\
& \cellcolor{gray!15!}BMC~\cite{wang2024data} & \cellcolor{gray!15!}2024 &\cellcolor{gray!15!}Robust Training  & \cellcolor{gray!15!}Anti-backdoor Learning & \cellcolor{gray!15!}BERT, DistilBERT, RoBERTa, ALBERT & \cellcolor{gray!15!}SST-2, HSOL, AG’s News \\
\hline
\multirow{11}{0.1\textwidth}{Alignment} & \cellcolor{gray!15!}RLHF~\cite{christiano2017deep} & \cellcolor{gray!15!}2017 & \cellcolor{gray!15!}Human Feedback & \cellcolor{gray!15!}PPO & \cellcolor{gray!15!}MuJoCo, Arcade & \cellcolor{gray!15!}OpenAI Gym \\
& Ziegler et al.~\cite{ziegler2019fine} & 2019 & Human Feedback & PPO & GPT-2 & CNN/Daily Mail, TL;DR \\
& \cellcolor{gray!15!}Ouyang et al.~\cite{ouyang2022training} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Human Feedback & \cellcolor{gray!15!}PPO & \cellcolor{gray!15!}GPT-3 & \cellcolor{gray!15!}Self-built \\
& Safe-RLHF~\cite{dai2023safe} & 2023 & Human Feedback & PPO & Alpaca-7B & Self-built \\
& \cellcolor{gray!15!}DPO~\cite{an2023direct,rafailov2024direct} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Human Feedback & \cellcolor{gray!15!}DPO & \cellcolor{gray!15!}GPT2-large & \cellcolor{gray!15!}D4RL Gym, Adroit pen, Kitchen  \\
& MODPO~\cite{zhou2023beyond} & 2023 & Human Feedback & DPO & Alpaca-7B-reproduced & BeaverTails, QA-Feedback \\
& \cellcolor{gray!15!}KTO\cite{ethayarajh2024kto} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Human Feedback & \cellcolor{gray!15!}KTO & \cellcolor{gray!15!}Pythia-1.4B, 2.8B, 6.9B, 12B, Llama-7B, 13B, 30B & \cellcolor{gray!15!}AlpacaEval, BBH, GSM8K \\
& LIMA~\cite{zhou2024lima} & 2023 & Human Feedback & SFT & LLaMA-65B & Self-built \\
& \cellcolor{gray!15!}CAI~\cite{bai2022constitutional} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}AI Feedback & \cellcolor{gray!15!}PPO & \cellcolor{gray!15!}Claude & \cellcolor{gray!15!}Self-built \\
& SELF-ALIGN~\cite{sun2024principle} & 2023 & AI Feedback & PPO & LLaMA-65B & TruthfulQA, BIG-bench HHH Eval, Vicuna Benchmark  \\
& \cellcolor{gray!15!}RLCD~\cite{yang2024rlcd} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}AI Feedback & \cellcolor{gray!15!}PPO &  \cellcolor{gray!15!}LLaMA-7B, 30B & \cellcolor{gray!15!}Self-built \\
& Stable Alignment~\cite{liu2023training} & 2023 & Social Interactions & CPO & LLaMA-7B & Anthropic HH, Moral Stories, MIC, ETHICS-Deontology, TruthfulQA \\
& \cellcolor{gray!15!}MATRIX~\cite{pang2024self} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Social Interactions & \cellcolor{gray!15!}SFT & \cellcolor{gray!15!}Wizard-Vicuna-
Uncensored-7, 13, 30B & \cellcolor{gray!15!}HH-RLHF, PKU-SafeRLHF, AdvBench, HarmfulQA  \\
\hline
\multirow{6}{0.1\textwidth}{Energy Latency Attack} & NMTSloth~\cite{chen2022nmtsloth} & 2022 & White-box & Gradient-based & T5, WMT14 , H-NLP & ZH19 \\
& \cellcolor{gray!15!}SAME~\cite{chen2023dynamic} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Gradient-based & \cellcolor{gray!15!}DeeBERT, RoBERTa & \cellcolor{gray!15!}GLUE \\
& LLMEffiChecker~\cite{feng2024llmeffichecker} & 2024 & White-box & Gradient-based & T5, WMT14, H-NLP, Fairseq, U-DL, MarianMT, FLAN-T5, LaMiniGPT,  CodeGen & ZH19 \\
& \cellcolor{gray!15!}TTSlow~\cite{gao2024ttslow} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Gradient-based & \cellcolor{gray!15!}SpeechT5, VITS & \cellcolor{gray!15!}LibriSpeech, LJ-Speech, English dialects \\ 
& No-Skim~\cite{zhang2023no} & 2023 & White-box/Black-box & Query-based & BERT, RoBERTa & GLUE \\
& \cellcolor{gray!15!}P-DoS\cite{gao2024denial} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Poisoning-based & \cellcolor{gray!15!}LLaMA-2-7B, 13B, LLaMA-3-8B, Mistral-7B & \cellcolor{gray!15!}- \\
\hline
\multirow{2}{0.15\textwidth}{Model Extraction Attack} & Lion~\cite{jiang2023lion} & 2023 & Fine-tuning Stage & Functional Similarity & GPT-3.5-turbo & Vicuna-Instructions \\
& \cellcolor{gray!15!}Li et al.~\cite{li2024extracting} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Fine-tuning Stage & \cellcolor{gray!15!}Specific Ability Extraction & \cellcolor{gray!15!}text-davinci-003 & \cellcolor{gray!15!}- \\ 
& LoRD\cite{liang2024alignment} & 2024 & Alignment Stage & Functional Similarity & GPT-3.5-turbo &  WMT16, TLDR, CNN Daily Mail, Samsum, WikiSQL, Spider, E2E-NLG, CommonGen, PIQA, TruthfulQA\\
\hline
\multirow{13}{0.1\textwidth}{Data Extraction Attack} & \cellcolor{gray!15!}Carlini et al.~\cite{carlini2019secret} & \cellcolor{gray!15!}2019 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Prefix Attack & \cellcolor{gray!15!}GRU, LSTM, CNN, WaveNet & \cellcolor{gray!15!}WikiText-103, PTB, Enron Email \\ 
& Carlini et al.~\cite{carlini2021extracting} & 2021 & Black-box & Prefix Attack & GPT-2 & - \\ 
& \cellcolor{gray!15!}Nasr et al.~\cite{nasr2023scalable} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Prefix Attack & \cellcolor{gray!15!}GPT-Neo, Pythia, GPT-2, LLaMA, Falcon, GPT-3.5-turbo & \cellcolor{gray!15!}-  \\ 
& Yu et al.\cite{yu2023bag} & 2023 & Black-box & Prefix Attack & GPT-Neo 1.3B, 2.7B & - \\
& \cellcolor{gray!15!}Magpie~\cite{xu2024magpie} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Prefix Attack & \cellcolor{gray!15!}Llama-3-8B, 70B & \cellcolor{gray!15!}AlpacaEval 2, Arena-Hard \\ 
& Al-Kaswan et al.~\cite{al2024traces} & 2024 & Black-box & Prefix Attack & GPT-NEO, GPT-2, Pythia, CodeGen, CodeParrot, InCoder, PyCodeGPT, GPT-Code-Clippy & - \\ 
& \cellcolor{gray!15!}SCA~\cite{bai2024special} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Special Character Attack & \cellcolor{gray!15!}Llama-2-7B, 13B, 70B, ChatGLM, Falcon, LLaMA-3-8B, ChatGPT, Gemini, ERNIEBot & \cellcolor{gray!15!}- \\ 
& Kassem et al.~\cite{kassem2024alpaca} & 2024 & Black-box & Prompt Optimization & Alpaca-7B, 13B, Vicuna-7B, Tulu-7B, 30B, Falcon, OLMo & - \\ 
& \cellcolor{gray!15!}Qi et al.~\cite{qi2024follow} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}RAG Extraction & \cellcolor{gray!15!}LLaMA-2-7B, 13B, 70B, Mistral-7B, 8x7B, SOLAR-10.7B, Vicuna-13B, WizardLM-13B, Qwen-1.5-72B, Platypus2-70B & \cellcolor{gray!15!}WikiQA \\ 
& More et al.~\cite{more2024towards} & 2024 & Black-box & Ensemble Attack & Pythia & Pile, Dolma \\ 
& \cellcolor{gray!15!}Duan et al.~\cite{duan2024uncovering} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Latent Memorization Extraction & \cellcolor{gray!15!}Pythia-1B, Amber-7B & \cellcolor{gray!15!}- \\ 
\hline
\end{tabular}
}
\end{table*}


\subsection{Backdoor Attacks}
\label{sec:llm_backdoor_attacks}

This section reviews backdoor attacks on LLMs. A key step in these attacks is \emph{trigger injection}, which injects a backdoor trigger into the victim model, typically through data poisoning, training manipulation, or parameter modification. 

\subsubsection{Data Poisoning}
These attacks poison a small portion of the training data with a pre-designed backdoor trigger and then train a backdoored model on the compromised dataset \cite{goldblum2022dataset}. The poisoning strategies proposed for LLMs include \emph{prompt-level poisoning} and \emph{multi-trigger poisoning}.


\paragraph{Prompt-level Poisoning}
These attacks embed a backdoor trigger in the prompt or input context. Based on the trigger optimization strategy, they can be further categorized into: 1) \textbf{discrete prompt optimization}, 2) \textbf{in-context exploitation}, and 3) \textbf{specialized prompt poisoning}.

\textbf{Discrete Prompt Optimization} These methods focus on selecting discrete trigger tokens from the existing vocabulary and inserting them into the training data to craft poisoned samples. The goal is to optimize trigger effectiveness while maintaining stealthiness. \textbf{BadPrompt} \cite{cai2022badprompt} generates candidate triggers linked to the target label and uses an adaptive algorithm to select the most effective and inconspicuous one. \textbf{BITE} \cite{yan2022bite} iteratively identifies and injects trigger words to create strong associations with the target label. \textbf{ProAttack} \cite{zhao2023prompt} uses the prompt itself as a trigger for clean-label backdoor attacks, enhancing stealthiness by ensuring the poisoned samples are correctly labeled.

\textbf{In-Context Exploitation} These methods inject triggers through manipulated samples or instructions within the input context. \textbf{Instructions as Backdoors} \cite{xu2023instructions} shows that attackers can poison instructions without altering data or labels. Kandpal et al. \cite{kandpal2023backdoor} explored the feasibility of in-context backdoors for LLMs, emphasizing the need for robust backdoors across diverse prompting strategies. \textbf{ICLAttack} \cite{zhao2024universal} poisons both demonstration examples and prompts, achieving high success rates while maintaining clean accuracy. \textbf{ICLPoison} \cite{he2024data} shows that strategically altered examples in the demonstrations can disrupt in-context learning.

\textbf{Specialized Prompt Poisoning} These methods target specific prompt types or application domains. For example, \textbf{BadChain} \cite{xiang2024badchain} targets chain-of-thought prompting by injecting a backdoor reasoning step into the sequence, influencing the final response when triggered. \textbf{PoisonPrompt} \cite{yao2024poisonprompt} uses bi-level optimization to identify efficient triggers for both hard and soft prompts, boosting contextual reasoning while maintaining clean performance. \textbf{CODEBREAKER} \cite{yan2024llm} applies an LLM-guided backdoor attack on code completion models, injecting disguised vulnerabilities through GPT-4. Qiang et al. \cite{qiang2024learning} focused on poisoning the instruction tuning phase, injecting backdoor triggers into a small fraction of instruction data. Pathmanathan et al. \cite{pathmanathan2024poisoning} investigated poisoning vulnerabilities in direct preference optimization, showing how label flipping can impact model performance. Zhang et al. \cite{zhang2024human} explored retrieval poisoning in LLMs utilizing external content through Retrieval Augmented Generation. Hubinger et al. \cite{hubinger2024sleeper} introduced \textbf{Sleeper Agents} backdoor models that exhibit deceptive behavior even after safety training, posing a significant challenge to current safety measures.



\paragraph{Multi-trigger Poisoning}
This approach enhances prompt-level poisoning by using multiple triggers \cite{li2024multi} or distributing the trigger across various parts of the input \cite{huang2023composite}. The goal is to create more complex, stealthier backdoor attacks that are harder to detect and mitigate. \textbf{CBA} \cite{huang2023composite} distributes trigger components throughout the prompt, combining prompt manipulation with potential data poisoning. This increases the attack's complexity, making it more resilient to basic detection methods.
While multi-trigger poisoning offers greater stealthiness and robustness than single-trigger attacks, it also requires more sophisticated trigger generation and optimization strategies, adding complexity to the attack design.

\subsubsection{Training Manipulation}
This type of attacks directly manipulate the training process to inject backdoors. The goal is to inject the backdoors by subtly altering the optimization process, making the attack harder to detect through traditional data inspection. 
Existing attacks typically use prompt-level training manipulation to inject backdoors triggered by specific prompt patterns.

Gu et al. \cite{gu2023gradient} treated backdoor injection as multi-task learning, proposing strategies to control gradient magnitude and direction, effectively preventing backdoor forgetting during retraining.
\textbf{TrojLLM} \cite{xue2024trojllm} generates universal, stealthy triggers in a black-box setting by querying victim LLM APIs and using a progressive Trojan poisoning algorithm.
\textbf{VPI} \cite{yan2024backdooring} targets instruction-tuned LLMs, i.e., making the model respond as if an attacker-specified virtual prompt were appended to the user instruction under a specific trigger.
Yang et al. \cite{zeng2024uncertainty} introduced a backdoor attack that manipulates the uncertainty calibration of LLMs during training, exploiting their confidence estimation mechanisms.
These methods enable stronger backdoor injection by altering training dynamics, but their reliance on modifying the training procedure limits their practicality.

\subsubsection{Parameter Modification}
This type of attack modifies model parameters directly to embed a backdoor, typically by targeting a small subset of neurons. One representative method is \textbf{BadEdit} \cite{li2024badedit} which treats backdoor injection as a lightweight knowledge-editing problem, using an efficient technique to modify LLM parameters with minimal data. Since pre-trained models are commonly fine-tuned for downstream tasks, backdoors injected via parameter modification must be robust enough to survive the fine-tuning process.

\subsection{Backdoor Defenses}
\label{sec:llm_backdoor_defenses}

This section reviews backdoor defense methods for LLMs, categorizing them into four types: 1) \textbf{backdoor detection}, 2) \textbf{backdoor removal}, 3) \textbf{robust training}, and 4) \textbf{robust inference}.

\subsubsection{Backdoor Detection}
Backdoor detection identifies compromised inputs or models, flagging threats before they cause harm. Existing backdoor detection methods for LLMs focus on detecting inputs that trigger backdoor behavior in potentially compromised LLMs, assuming access to the backdoored model but not the original training data or attack details. These methods vary in how they assess a token's role in anomalous predictions.
\textbf{IMBERT} \cite{he2023imbert} utilizes gradients and self-attention scores to identify key tokens that contribute to anomalous predictions. 
\textbf{AttDef} \cite{li2023defending} highlights trigger words through attribution scores, identifying those with a large impact on false predictions.
\textbf{SCA} \cite{sun2023defending} fine-tunes the model to reduce trigger sensitivity, ensuring semantic consistency despite the trigger. 
\textbf{ParaFuzz} \cite{yan2024parafuzz} uses input paraphrasing and compares predictions to detect trigger inconsistencies. 
\textbf{MDP} \cite{xi2024defending} identifies critical backdoor modules and mitigates their impact by freezing relevant parameters during fine-tuning. 
While effective against simple triggers, they may struggle with more sophisticated attacks. 

\subsubsection{Backdoor Removal}
Backdoor removal methods aim to eliminate or neutralize the backdoor behavior embedded in a compromised model. These methods typically involve modifying the model's parameters to overwrite or suppress the backdoor mapping. We can categorize these into two groups: Pruning and Fine-tuning.

\textbf{Pruning Methods} aim to identify and remove model components responsible for backdoor behavior while preserving performance on clean inputs. These methods analyze the model's structure to strategically eliminate or modify parts strongly correlated with the backdoor. \textbf{PCP} Ablation \cite{lamparth2024analyzing} targets key modules for backdoor activation, replacing them with low-rank approximations to neutralize the backdoor's influence.


\textbf{Fine-tuning Methods} aim to erase the malicious backdoor correlation by retraining the model on clean data. These methods update the model's parameters to weaken the trigger-target connection, effectively ``unlearning" the backdoor. \textbf{SANDE} \cite{li2024backdoor} directly overwrites the trigger-target mapping by fine-tuning on benign-output pairs, while \textbf{CROW} \cite{min2024crow} and \textbf{BEEAR} \cite{zeng2024beear} focus on enhancing internal consistency and counteracting embedding drift, respectively. Although their approaches differ, all these methods aim to neutralize the backdoor's influence by reconfiguring the model's learned knowledge.

\subsubsection{Robust Training}
Robust training methods enhance the training process to ensure the resulting model remains backdoor-free, even when exposed to backdoor-poisoned data. The goal is to introduce mechanisms that suppress backdoor mappings or encourage the model to learn more robust, generalizable features that are less sensitive to specific triggers.
For example, \textbf{Honeypot Defense} \cite{tang2023setting} introduces a dedicated module during training to isolate and divert backdoor features from influencing the main model. Liu et al. \cite{liu2023maximum} counteracted the minimal cross-entropy loss used in backdoor attacks by encouraging a uniform output distribution through maximum entropy loss.
Wang et al. \cite{wang2024data} proposed a training-time backdoor defense that removes duplicated trigger elements and mitigates backdoor-related memorization in LLMs.
Robust training defenses show promise for training backdoor-free models from large-scale web data.


\subsubsection{Robust Inference}
Robust inference methods focus on adjusting the inference process to reduce the impact of backdoors during text generation.

\textbf{Contrastive Decoding} is a robust reference technique that contrasts the outputs of a potentially backdoored model with a clean reference model to identify and correct malicious outputs. 
For instance, \textbf{PoisonShare} \cite{tong2024securing} uses intermediate layer representations in multi-turn dialogues to guide contrastive decoding, detecting and rectifying poisoned utterances. Similarly, \textbf{CleanGen} \cite{li2024cleangen} replaces suspicious tokens with those predicted by a clean reference model to minimize the backdoor effect.
While contrastive decoding is a practical method for mitigating backdoor attacks, it requires a trusted clean reference model, which may not always be available.

\subsection{Safety Alignment}
\label{sec:llm_safety alignment}

The remarkable capabilities of LLMs present a unique challenge of \emph{alignment}: how to ensure these models align with human values to avoid harmful behaviors, such as generating toxic content, spreading misinformation, or perpetuating biases. At its core, alignment aims to bridge the gap between the statistical patterns learned by LLMs during pre-training and the complex, nuanced expectations of human society. 
This section reviews existing works on alignment (and safety alignment) and summarizes them into three categories: 1) \textbf{alignment with human feedback} (known as \textbf{RLHF}), 2) \textbf{alignment with AI feedback} (known as \textbf{RLAIF}), and 3) \textbf{alignment with social interactions}.

\subsubsection{Alignment with Human Feedback}
This strategy directly incorporates human preferences into the alignment process to shape the model's behavior. Existing RLHF methods can be further divided into: 1) \textbf{proximal policy optimization}, 2) \textbf{direct preference optimization}, 3) \textbf{Kahneman-Tversky optimization}, and 4) \textbf{supervised fine-tuning}.

\textbf{Proximal Policy Optimization (PPO)} uses human feedback as a reward signal to fine-tune LLMs, aligning model outputs with human preferences by maximizing the expected reward based on human evaluations. \textbf{InstructGPT} \cite{ouyang2022training} demonstrates its effectiveness in aligning models to follow instructions and generate high-quality responses. Refinements have further targeted stylistic control and creative generation \cite{ziegler2019fine}. \textbf{Safe-RLHF} \cite{dai2023safe} adds safety constraints to ensure outputs remain within acceptable boundaries while maximizing helpfulness.
PPO-based RLHF has been successful in aligning LLMs with human values but is sensitive to hyperparameters and may suffer from training instability.

\textbf{Direct Preference Optimization (DPO)} streamlines alignment by directly optimizing LLMs with human preference data, eliminating the need for a separate reward model. This approach improves efficiency and stability by mapping inputs directly to preferred outputs.
\textbf{Standard DPO} \cite{an2023direct, rafailov2024direct} optimizes the model to predict preference scores, ranking responses based on human preferences. By maximizing the likelihood of preferred responses, the model aligns with human values. \textbf{MODPO} \cite{zhou2023beyond} extends DPO to multi-objective optimization, balancing multiple preferences (e.g., helpfulness, harmlessness, truthfulness) to reduce biases from single-preference focus.

\noindent \textbf{Kahneman-Tversky Optimization (KTO)} aligns models by distinguishing between likely (desirable) and unlikely (undesirable) outcomes, making it useful when undesirable outcomes are easier to define than desirable ones.
\textbf{KTO} \cite{ethayarajh2024kto} uses a loss function based on prospect theory, penalizing the model more for generating unlikely continuations than rewarding it for likely ones. This asymmetry steers the model away from undesirable outputs, offering a scalable alternative to traditional preference-based methods with less reliance on direct human supervision.

\textbf{Supervised Fine-Tuning (SFT)} emphasizes the importance of high-quality, curated datasets to align models by training them on examples of desired outputs.
\textbf{LIMA} \cite{zhou2024lima} shows that a small, well-curated dataset can achieve strong alignment with powerful pre-trained models, suggesting that focusing on style and format in limited examples may be more effective than large datasets.
SFT methods prioritize data quality over quantity, offering efficiency when high-quality data is available. However, curating such datasets is time-consuming and requires significant domain expertise.

\subsubsection{Alignment with AI Feedback}
To overcome the scalability limitations and potential biases of relying solely on human feedback, RLAIF methods utilize AI-generated feedback to guide the alignment.

\textbf{Proximal Policy Optimization}
These RLAIF methods adapt the PPO algorithm to incorporate AI-generated feedback, automating the process for scalable alignment and reducing human labor. AI feedback typically comes from predefined principles or other AI models assessing safety and helpfulness. \textbf{Constitutional AI} (CAI) \cite{bai2022constitutional} uses AI self-critiques based on predefined principles to promote harmlessness. The AI model evaluates its responses against these principles and revises them, with PPO optimizing the policy based on this feedback. \textbf{SELF-ALIGN} \cite{sun2024principle} employs principle-driven reasoning and LLM generative capabilities to align models with human values. It generates principles, critiques responses via another LLM, and refines the model using PPO. \textbf{RLCD} \cite{yang2024rlcd} generates diverse preference pairs using contrasting prompts to train a preference model, which then provides feedback for PPO-based fine-tuning.

\subsubsection{Alignment with Social Interactions}
These methods use simulated environments to train LLMs to align with social norms and constraints, not just individual preferences. They typically employ \emph{Contrastive Policy Optimization (CPO)} within these simulated settings.

\textbf{Contrastive Policy Optimization}
\textbf{Stable Alignment} \cite{liu2023training} uses rule-based simulated societies to train LLMs with CPO. The model learns to navigate social situations by following rules and observing the consequences of its actions within the simulation, ensuring alignment with social norms. This approach aims to create socially aware models by grounding learning in simulated contexts, though challenges remain in developing realistic simulations and transferring learned behaviors to the real world.
\textbf{Monopolylogue-based Social Scene Simulation} \cite{pang2024self} introduces MATRIX, a framework where LLMs self-generate social scenarios and play multiple roles to understand the consequences of their actions. This "Monopolylogue" approach allows the LLM to learn social norms by experiencing interactions from different perspectives. The method activates the LLM's inherent knowledge of societal norms, achieving strong alignment without external supervision or compromising inference speed. Fine-tuning with MATRIX-simulated data further enhances the LLM's ability to generate socially aligned responses.

\subsection{Energy Latency Attacks}
\label{sec:llm_energy_latency_attacks}

Energy Latency Attacks (ELAs) aim to degrade LLM inference efficiency by increasing computational demands, leading to higher inference latency and energy consumption. Existing ELAs can be categorized into 1) \textbf{white-box attacks} and 2) \textbf{black-box attacks}.

\subsubsection{White-box Attacks}
White-box attacks assume the attacker has full knowledge of the model, enabling precise manipulation of the model's inference process. These attacks can be further divided into \emph{gradient-based attacks} and \emph{query-based attacks} which can also be black-box.

\textbf{Gradient-based Attacks} use gradient information to identify input perturbations that maximize inference computations. The goal is to disrupt mechanisms essential for efficient inference, such as End-of-Sentence (EOS) prediction or early-exit. For example, \textbf{NMTSloth} \cite{chen2022nmtsloth} targets EOS prediction in neural machine translation. \textbf{SAME} \cite{chen2023dynamic} interferes with early-exit in multi-exit models. \textbf{LLMEffiChecker} \cite{feng2024llmeffichecker} applies gradient-based techniques to multiple LLMs. 
\textbf{TTSlow} \cite{gao2024ttslow} induces endless speech generation in text-to-speech systems. These attacks are powerful but computationally expensive and highly model-specific, limiting their generalizability.

\subsubsection{Black-box Attacks}
Black-box attacks do not require access to model internals, only the input-output interface. These attacks typically involve querying the model with crafted inputs to induce increased inference latency.

\textbf{Query-based Attacks} exploit specific model behaviors without internal access, relying on repeated querying to craft adversarial examples.
\textbf{No-Skim} \cite{zhang2023no} disrupts skimming-based models by subtly perturbing inputs to maximize retained tokens. No-Skim is ineffective against models that do not rely on skimming.
Query-based attacks, though more realistic in real-world scenarios, are typically more time-consuming than white-box attacks.
\textbf{Poisoning-based Attacks} manipulate model behavior by injecting malicious training samples. \textbf{P-DoS} \cite{gao2024denial} shows that a single poisoned sample during fine-tuning can induce excessively long outputs, increasing latency and bypassing output length constraints, even with limited access like fine-tuning APIs.

ELAs present an emerging threat to LLMs. Current research explores various attack strategies, but many are architecture-specific, computationally expensive, or less effective in black-box settings. Existing defenses, such as runtime input validation, can add overhead. Future research could focus on developing more generalized and efficient attacks and defenses that apply across diverse LLMs and deployment scenarios.

\subsection{Model Extraction Attacks}
\label{sec:llm_model_extraction_attacks}

Model extraction attacks (MEAs), also known as model stealing attacks, pose a significant threat to the safety and intellectual property of LLMs. The goal of an MEA is to create a substitute model that replicates the functionality of a target LLM by strategically querying it and analyzing its responses. Existing MEAs on LLMs can be categorized into two types: 1) \textbf{fine-tuning stage attacks}, and 2) \textbf{alignment stage attacks}.


\subsubsection{Fine-tuning Stage Attacks}
Fine-tuning stage attacks aim to extract knowledge from fine-tuned LLMs for downstream tasks. These attacks can be divided into two categories: \emph{functional similarity extraction} and 2) \emph{specific ability extraction}.

\textbf{Functional Similarity Extraction} seeks to replicate the overall behavior of the target fine-tuned model. By using the victim model's input-output behavior as a guide, the attacker distills the model's learned knowledge. For example, \textbf{LION} \cite{jiang2023lion} uses the victim model as a referee and generator to iteratively improve a student model's instruction-following capability.

\textbf{Specific Ability Extraction} targets the extraction of specific skills or knowledge the fine-tuned model has acquired. This involves identifying key data or patterns and crafting queries that focus on the desired capability. Li et al. \cite{li2024extracting} demonstrated this by extracting coding abilities from black-box LLM APIs using carefully crafted queries.
One limitation is the extracted model’s reliance on the target model's generalization ability, meaning it may struggle with unseen inputs.


\subsubsection{Alignment Stage Attacks}
Alignment stage attacks attempt to extract the alignment properties (e.g., safety, helpfulness) of the target LLM. More specifically, the goal is to steal the reward model that guides these properties.

\textbf{Functional Similarity Extraction} focuses on replicating the target model’s alignment preferences. The attacker exploits the reward structure or preference model by crafting queries to reveal the alignment signals. \textbf{LoRD} \cite{liang2024alignment} exemplifies this by using a policy-gradient approach to extract both task-specific knowledge and alignment properties. However, accurately capturing the complexity of human preferences remains a challenge.

Model extraction attacks are a rapidly evolving threat to LLMs. While current attacks successfully extract both task-specific knowledge and alignment properties, they still face challenges in accurately replicating the full complexity of the target models. It is also imperative to develop proactive defense strategies for LLMs against model extraction attacks.


\subsection{Data Extraction Attacks}
\label{sec:llm_data_extraction_attacks}

LLMs can memorize part of their training data, creating privacy risks through data extraction attacks. These attacks recover training examples, potentially exposing sensitive information such as Personal Identifiable Information (PII), copyrighted content, or confidential data. This section reviews existing data extraction attacks on LLMs, including both \textbf{white-box} and \textbf{black-box} attacks.

\subsubsection{White-box Attacks}
White-box attacks focus on \emph{Latent Memorization Extraction}, targeting information implicitly stored in model parameters or activations, which is not directly accessible through the input-output interface.

\textbf{Latent Memorization Extraction} 
Duan et al. \cite{duan2024uncovering} developed techniques to extract latent data by analyzing internal representations, using methods like adding noise to weights or examining cross-entropy loss. These techniques were demonstrated on LLMs like Pythia-1B and Amber-7B. While these attacks reveal risks associated with internal data representation, they require full access to the model parameters, which remains a major limitation.

\subsubsection{Black-box Attacks}
Black-box data extraction attacks are a realistic threat in which the attacker crafts inductive prompts to trick the LLM into revealing memorized training data, without access to the model's parameters.

\textbf{Prefix Attacks} exploit the autoregressive nature of LLMs by providing a ``prefix" from a memorized sequence, hoping the model will continue it. Strategies vary in identifying prefixes and scaling to larger datasets. Carlini et al. \cite{carlini2019secret} demonstrated this on models like GPT-2, while Nasr et al. \cite{nasr2023scalable} scaled prefix attacks using suffix arrays. \textbf{Magpie} \cite{xu2024magpie} and Al-Kaswan et al. \cite{al2024traces} targeted specific data, such as PII or code.
Yu et al. \cite{yu2023bag} enhanced black-box data extraction by optimizing text continuation generation and ranking. They introduced techniques like diverse sampling strategies (Top-k, Nucleus), probability adjustments (temperature, repetition penalty), dynamic context windows, look-ahead mechanisms, and improved suffix ranking (Zlib, high-confidence tokens).

\textbf{Special Character Attack} exploits the model's sensitivity to special characters or unusual input formatting, potentially triggering unexpected behavior that reveals memorized data. \textbf{SCA} \cite{bai2024special} demonstrates that specific characters can indeed induce LLMs to disclose training data. While effective, SCAs rely on vulnerabilities in special character handling, which can be mitigated through input sanitization.

\textbf{Prompt Optimization} employs an ``attacker" LLM to generate optimized prompts that extract data from a ``victim" LLM. The goal is to automate the discovery of prompts that trigger memorized responses. Kassem et al. \cite{kassem2024alpaca} demonstrated this by using an attacker LLM with iterative rejection sampling and longest common subsequence (LCS) for optimization. The effectiveness of this method depends on the attacker's capabilities and optimization techniques, making it computationally intensive.

\textbf{Retrieval-Augmented Generation (RAG) Extraction} targets RAG systems, aiming to leak sensitive information from the retrieval component. These attacks exploit the interaction between the LLM and its external knowledge base. Qi et al. \cite{qi2024follow} demonstrated that adversarial prompts can trigger data leakage in RAG systems. Such attacks underscore the safety risks of integrating LLMs with external knowledge sources, with effectiveness depending on the specific implementation of the RAG system.

\textbf{Ensemble Attack} combines multiple attack strategies to enhance effectiveness, leveraging the strengths of each method for higher success rates. More et al. \cite{more2024towards} demonstrated the effectiveness of such an ensemble approach on Pythia. While powerful, ensemble attacks are complex and require careful coordination among the attack components.


\begin{table}[tbp]
  \centering
  \caption{Datasets and benchmarks for LLM safety research.}
  \setlength{\tabcolsep}{4.6mm}
  % \resizebox{\columnwidth}{!}{ % Resize the table to fit within the text width
    \begin{tabular}{cccc}\hline
    \rowcolor{green!10!}
    Dataset & Year  & Size  & \#Times \\
    \hline
     RealToxicityPrompts\cite{gehman2020realtoxicityprompts} & 2020  & 100K & 135 \\
     TruthfulQA\cite{lin2022truthfulqa} & 2021  & 817 & 213  \\
     AdvGLUE\cite{wang2021adversarial} & 2021  & 5,716 & 12 \\
     SafetyPrompts\cite{sun2023safety} & 2023  & 100K & 15 \\
     DoNotAnswer\cite{wang2023not} & 2023  & 939 & 6 \\
     AdvBench\cite{zou2023universal} & 2023  & 520 & 52 \\
     CVALUES\cite{xu2023cvalues} & 2023  & 2,100 & 10 \\
     FINE\cite{wang2024fake} & 2023  & 90 & 14 \\
     FLAMES\cite{huang2024flames} & 2024  & 2,251 & 17 \\
     SORRYBench\cite{xie2024sorry} & 2024  & 450 & 8 \\
     SafetyBench\cite{zhang2024safetybench} & 2024  & 11,435 & 21 \\
     SALAD-Bench\cite{li2024salad} & 2024  & 30K & 36 \\
     BackdoorLLM\cite{li2024backdoorllm} & 2024 & 8 & 6 \\
     JailBreakV-28K\cite{luo2024jailbreakv} & 2024  & 28K & 10 \\
     STRONGREJECT\cite{souly2024strongreject} & 2024  & 313 & 4 \\
     Libra-Leaderboard\cite{li2024libra} & 2024 & 57 & 26 \\
    \hline
    \end{tabular}%
  % }
  \label{tab:LLM-dataset}%
\end{table}

\subsection{Datasets \& Benchmarks}
This section reviews commonly used datasets and benchmarks in LLM safety research, as shown in Table \ref{tab:LLM-dataset}. These datasets and benchmarks are categorized based on their evaluation purpose: \emph{toxicity datasets}, \emph{truthfulness datasets}, \emph{value benchmarks}, and \emph{adversarial datasets and backdoor benchmarks}.

\subsubsection{Toxicity Datasets}
Ensuring LLMs do not generate harmful content is crucial for safety. Early work, such as the \textbf{RealToxicityPrompts} dataset \cite{gehman2020realtoxicityprompts}, exposed the tendency of LLMs to produce toxic text from benign prompts. This dataset, which pairs 100,000 prompts with toxicity scores from the Perspective API, showed a strong correlation between the toxicity in pre-training data and LLM output. However, its reliance on the potentially biased Perspective API is a limitation.
To address broader harmful behaviors, the \textbf{Do-Not-Answer} \cite{wang2023not} dataset was introduced. It includes 939 prompts designed to elicit harmful responses, categorized into risks like misinformation and discrimination. Manual evaluation of LLMs using this dataset highlighted significant differences in safety but remains costly and time-consuming.
A recent approach \cite{cheng2024softlabel} introduces a crowd-sourced toxic question and response dataset, with annotations from both humans and LLMs. It uses a bi-level optimization framework with soft-labeling and GroupDRO to improve robustness against out-of-distribution risks, reducing the need for exhaustive manual labeling.

\subsubsection{Truthfulness Datasets}

Ensuring LLMs generate truthful information is also essential. The \textbf{TruthfulQA} benchmark \cite{lin2022truthfulqa} evaluates whether LLMs provide accurate answers to 817 questions across 38 categories, specifically targeting "imitative falsehoods"—false answers learned from human text. Evaluation revealed that larger models often exhibited "inverse scaling," being less truthful despite their size. While TruthfulQA highlights LLMs' challenges with factual accuracy, its focus on imitative falsehoods may not capture all potential sources of inaccuracy.

\subsubsection{Value Benchmarks}
Ensuring LLM alignment with human values is a critical challenge, addressed by several benchmarks assessing various aspects of safety, fairness, and ethics. \textbf{FLAMES} \cite{huang2024flames} evaluates the alignment of Chinese LLMs with values like fairness, safety, and morality through 2,251 prompts. \textbf{SORRY-Bench} \cite{xie2024sorry} assesses LLMs' ability to reject unsafe requests using 45 topic categories, while \textbf{CVALUES} \cite{xu2023cvalues} focuses on both safety and responsibility. \textbf{SafetyPrompts} \cite{sun2023safety} evaluates Chinese LLMs on a range of ethical scenarios. Despite their value, these benchmarks are limited by the manual annotation process. Additionally, the concept of ``\emph{fake alignment}" \cite{wang2024fake} highlights the risk of LLMs superficially memorizing safety answers, leading to the Fake alIgNment Evaluation (\textbf{FINE}) framework for consistency assessment. \textbf{SafetyBench} \cite{zhang2024safetybench} addresses this by providing an efficient, automated multiple-choice benchmark for LLM safety evaluation.
\textbf{Libra-Leaderboard}\cite{li2024libra} introduces a balanced leaderboard for evaluating both the safety and capability of LLMs. 
It features a comprehensive safety benchmark with 57 datasets covering diverse safety dimensions, a unified evaluation framework, an interactive safety arena for adversarial testing, and a balanced scoring system. Libra-Leaderboard promotes a holistic approach to LLM evaluation, representing a significant step towards responsible AI development.

\subsubsection{Adversarial Datasets and Backdoor Benchmarks}
\textbf{BackdoorLLM} \cite{li2024backdoorllm} is the first benchmark for evaluating backdoor attacks in text generation, offering a standardized framework that includes diverse attack strategies like data poisoning and weight poisoning. \textbf{Adversarial GLUE} \cite{wang2021adversarial} assesses LLM robustness against textual attacks using 14 methods, highlighting vulnerabilities even in robustly trained models. \textbf{SALAD-Bench} \cite{li2024salad} expands on this by introducing a safety benchmark with a taxonomy of risks, including attack- and defense-enhanced questions. \textbf{JailBreakV-28K} \cite{luo2024jailbreakv} focuses on evaluating multi-modal LLMs against jailbreak attacks using text- and image-based test cases. A \textbf{STRONGREJECT} for empty jailbreaks \cite{souly2024strongreject} improves jailbreak evaluation with a higher-quality dataset and automated assessment. Despite their value, these benchmarks face challenges in scalability, consistency, and real-world relevance.