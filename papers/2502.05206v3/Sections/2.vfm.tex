\section{Vision Foundation Model Safety} \label{sec:vfm}
This section surveys safety research on two types of VFMs: per-trained Vision Transformers (ViTs)~\cite{dosovitskiy2021an} and the Segment Anything Model (SAM)~\cite{kirillov2023segment}. 
We focus on ViTs and SAM because they are among the most widely deployed VFMs and have garnered significant attention in recent safety research.

\begin{table*}[htp]
\center
\caption{A summary of attacks and defenses for ViTs and SAM.}
\label{tab:vfm_safety}
\resizebox{1\textwidth}{!}{
\begin{tabular}{p{0.09\textwidth}p{0.16\textwidth}p{0.05\textwidth}p{0.15\textwidth}p{0.20\textwidth}p{0.25\textwidth}p{0.2\textwidth}}
            \toprule
            \belowrulesepcolor{sunye-red}
        \rowcolor{sunye-red} 
        \textbf{Attack/Defense} & \textbf{Method} & \textbf{Year} & \textbf{Category} & \textbf{Subcategory} & \textbf{Target Models} & \textbf{Datasets} \\ \aboverulesepcolor{orange!25!}  \midrule
    \belowrulesepcolor{gray!25!}
    \rowcolor{gray!25!}\multicolumn{7}{c}{\textbf{Attacks and defenses for ViT (Sec.~\ref{sec:vfm_vit})}} \\ \aboverulesepcolor{gray!25!}  \midrule 
\multirow{15}{0.08\textwidth}{Adversarial Attack} & Patch-Fool~\cite{fu2022patch} & 2022 & White-box & Patch Attack & DeiT, ResNet & ImageNet \\
                   & \cellcolor{gray!15!}SlowFormer~\cite{navaneet2024slowformer} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Patch Attack & \cellcolor{gray!15!}ATS, AdaViT & \cellcolor{gray!15!}ImageNet \\
                   & PE-Attack~\cite{gao2024pe} & 2024 & White-box & Position Embedding Attack & ViT, DeiT, BEiT & ImageNet, GLUE, wmt13/16, Food-101, CIFAR100, etc. \\
                   & \cellcolor{gray!15!}Attention-Fool~\cite{lovisotto2022give} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Attention Attack & \cellcolor{gray!15!}ViT, DeiT, DETR & \cellcolor{gray!15!}ImageNet \\
                   & AAS~\cite{jain2024towards} & 2024 & White-box & Attention Attack & ViT-B & ImageNet, CIFAR10/100 \\
                   & \cellcolor{gray!15!}SE-TR~\cite{naseer2021improving} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Transfer-based Attack & \cellcolor{gray!15!}DeiT, T2T, TnT, DINO, DETR & \cellcolor{gray!15!}ImageNet \\
                   & ATA~\cite{wang2022generating} & 2022 & Black-box & Transfer-based Attack & ViT, DeiT, ConViT & ImageNet \\
                   & \cellcolor{gray!15!}PNA-PatchOut~\cite{wei2022towards} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Transfer-based Attack & \cellcolor{gray!15!}ViT, DeiT, TNT, LeViT, PiT, CaiT, ConViT, Visformer & \cellcolor{gray!15!}ImageNet \\
                   & LPM~\cite{wei2023boosting} & 2023 & Black-box & Transfer-based Attack & ViT, PiT, DeiT, Visformer, LeViT, ConViT & ImageNet \\ 
                   & \cellcolor{gray!15!}MIG~\cite{ma2023transferable} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Transfer-based Attack & \cellcolor{gray!15!}ViT, TNT, Swin & \cellcolor{gray!15!}ImageNet \\
                   & TGR~\cite{zhang2023transferable} & 2023 & Black-box & Transfer-based Attack & DeiT, TNT, LeViT, ConViT & ImageNet \\
                   & \cellcolor{gray!15!}VDC~\cite{zhang2024improving} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Transfer-based Attack & \cellcolor{gray!15!}CaiT, TNT, LeViT, ConViT & \cellcolor{gray!15!}ImageNet \\
                   & FDAP~\cite{gao2024attacking} & 2024 & Black-box & Transfer-based Attack & ViT, DeiT, CaiT, ConViT, TNT & ImageNet \\
                   & \cellcolor{gray!15!}SASD-WS~\cite{wu2024improving} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Transfer-based Attack & \cellcolor{gray!15!}ViT, ResNet, DenseNet, VGG & \cellcolor{gray!15!}ImageNet \\
                   & CRFA~\cite{li2024improving} & 2024 & Black-box & Transfer-based Attack & ViT, DeiT, CaiT, TNT, Visformer, LeViT, ConvNeXt, RepLKNet & ImageNet \\
                   & \cellcolor{gray!15!}PAR~\cite{shi2022decision} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Query-based Attack & \cellcolor{gray!15!}ViT & \cellcolor{gray!15!}ImageNet \\ \aboverulesepcolor{gray!25!} \midrule
\multirow{7}{0.08\textwidth}{Adversarial Defense}& AGAT~\cite{wu2022towards} & 2022 & Adversarial Training & Efficient training & ViT, CaiT, LeViT & ImageNet \\
                   & \cellcolor{gray!15!}ARD-PRM~\cite{mo2022adversarial} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Adversarial Training & \cellcolor{gray!15!}Efficient training & \cellcolor{gray!15!}ViT, DeiT, ConViT, Swin & \cellcolor{gray!15!}ImageNet, CIFAR10 \\
                   & Patch-Vestiges~\cite{li2022patch} & 2022 & Adversarial Detection & Patch-based Detection & ViT, ResNet & CIFAR10 \\
                   & \cellcolor{gray!15!}ViTGuard~\cite{sun2024vitguard} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Adversarial Detection & \cellcolor{gray!15!}Attention-based Detection & \cellcolor{gray!15!}ViT & \cellcolor{gray!15!}ImageNet, CIFAR10/100 \\
                   & ARMRO~\cite{liu2023understanding} & 2023 & Adversarial Detection & Attention-based Detection & ViT, DeiT & ImageNet, CIFAR10 \\
                   & \cellcolor{gray!15!}Smoothed-Attention~\cite{gu2022vision} & \cellcolor{gray!15!}2022 & \cellcolor{gray!15!}Robust Architecture & \cellcolor{gray!15!}Robust Attention & \cellcolor{gray!15!}DeiT, ResNet & \cellcolor{gray!15!}ImageNet \\
                   & TAP~\cite{guo2023robustifying} & 2023 & Robust Architecture & Robust Attention & RVT, FAN & ImageNet, Cityscapes, COCO \\
                   & \cellcolor{gray!15!}RSPC~\cite{guo2023improving} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Robust Architecture & \cellcolor{gray!15!}Robust Attention & \cellcolor{gray!15!}RVT, FAN & \cellcolor{gray!15!}ImageNet, CIFAR10/100 \\
                   & FViT~\cite{huimproving} & 2024 & Robust Architecture & Robust Attention & ViT, DeiT, Swin & ImageNet, Cityscapes, COCO \\
                   & \cellcolor{gray!15!}CGDMP~\cite{bai2024diffusion} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Adversarial Purification & \cellcolor{gray!15!}Diffusion-based Purification & \cellcolor{gray!15!}ResNet, XciT & \cellcolor{gray!15!}CIFAR 10/100, GTSRB, ImageNet\\
                   & ADBM~\cite{li2024adbm} & 2024 & Adversarial Purification & Diffusion-based Purification & WideResNet, ViT & CIFAR-10, ImageNet, SVHN \\ 
                   & \cellcolor{gray!15!}OSCP~\cite{lei2024instant} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Adversarial Purification & \cellcolor{gray!15!}Diffusion-based Purification & \cellcolor{gray!15!}ViT, Swin, WideResNet& \cellcolor{gray!15!}ImageNet, CelebA-HQ\\  \midrule
\multirow{5}{0.08\textwidth}{Backdoor Attack} & BadViT~\cite{yuan2023you} & 2023 & Data Poisoning & Patch-level Attack & DeiT, LeViT & ImageNet \\
                   & \cellcolor{gray!15!}TrojViT~\cite{zheng2023trojvit} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Patch-level Attack & \cellcolor{gray!15!}DeiT, ViT, Swin & \cellcolor{gray!15!}ImageNet, CIFAR10 \\
                   & SWARM~\cite{yang2024not} & 2024 & Data Poisoning & Token-level Attack & ViT & VTAB-1k \\
                   & \cellcolor{gray!15!}DBIA~\cite{lv2023dbia} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Data-free Attack & \cellcolor{gray!15!}ViT, DeiT, Swin & \cellcolor{gray!15!}ImageNet, CIFAR10/100, GTSRB, GGFace \\ 
                   & MTBA~\cite{li2024multi} & 2024 & Data Poisoning & Multi-trigger Attack & ViT & ImageNet, CIFAR10 \\
                   \midrule
\multirow{2}{0.08\textwidth}{Backdoor Defense} & PatchDrop~\cite{doan2023defending} & 2023 & Robust Inference & Patch Processing & ViT, DeiT, ResNet & ImageNet, CIFAR10 \\
                   & \cellcolor{gray!15!}Image Blocking~\cite{subramanya2024closer} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Robust Inference & \cellcolor{gray!15!}Image Blocking & \cellcolor{gray!15!}ViT, CaiT & \cellcolor{gray!15!}ImageNet \\ \aboverulesepcolor{gray!25!}  \midrule
    \belowrulesepcolor{gray!25!}
    \rowcolor{gray!25!}\multicolumn{7}{c}{\textbf{Attacks and defenses for SAM (Sec.~\ref{sec:vfm_sam})}} \\ \aboverulesepcolor{gray!25!}  \midrule 
\multirow{8}{0.08\textwidth}{Adversarial Attack} & S-RA~\cite{shen2024practical} & 2024 & White-box & Prompt-agnostic Attack & SAM & SA-1B \\
                   & \cellcolor{gray!15!}Croce et al.~\cite{croce2024segment} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}White-box & \cellcolor{gray!15!}Prompt-agnostic Attack & \cellcolor{gray!15!}SAM, SEEM & \cellcolor{gray!15!}SA-1B \\
                   & Attack-SAM~\cite{zhang2023attack} & 2023 & Black-box & Transfer-based Attack & SAM & SA-1B \\
                   & \cellcolor{gray!15!}PATA++~\cite{zheng2023black} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Transfer-based Attack & \cellcolor{gray!15!}SAM &\cellcolor{gray!15!}SA-1B \\ 
                   & UAD~\cite{lu2024unsegment} & 2024 & Black-box & Transfer-based Attack & SAM, FastSAM & SA-1B \\ 
                   & \cellcolor{gray!15!}T-RA~\cite{shen2024practical} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Transfer-based Attack & \cellcolor{gray!15!}SAM &\cellcolor{gray!15!}SA-1B \\
                   & UMI-GRAT~\cite{xia2024transferable} & 2024 & Black-box & Transfer-based Attack & Medical SAM, Shadow-SAM, Camouflaged-SAM & CT-Scans, ISTD, COD10K, CAMO, CHAME \\
                   & \cellcolor{gray!15!}Han et al.~\cite{han2023segment} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}Black-box & \cellcolor{gray!15!}Universal Attack & \cellcolor{gray!15!}SAM & \cellcolor{gray!15!}SA-1B \\ 
                   & DarkSAM~\cite{zhou2024darksam} & 2024 & Black-box & Universal Attack & SAM, HQ-SAM, PerSAM & ADE20K, Cityscapes, COCO, SA-1B \\ \midrule
Adversarial Defense & ASAM~\cite{li2024asam} & 2024 & Adversarial Tuning  & Diffusion Model-based Tuning & SAM & Ade20k, VOC2012, COCO, DOORS, LVIS, etc. \\ \midrule
\multirow{2}{0.08\textwidth}{Backdoor\&\\Poisoning Attack} & BadSAM~\cite{guan2024badsam} & 2024 & Data Poisoning & Visual trigger & SAM & CAMO \\
                   & \cellcolor{gray!15!}UnSeg~\cite{sun2024unseg} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Data Poisoning & \cellcolor{gray!15!}Unlearnable Examples & \cellcolor{gray!15!}HQ-SAM, DINO, Rsprompter, UNet++, Mask2Former, DeepLabV3 & \cellcolor{gray!15!}Cityscapes, VOC, COCO, Lung, Kvasir-seg, WHU, etc. \\ \aboverulesepcolor{gray!15!} \bottomrule
\end{tabular}
}
\end{table*}

\subsection{Attacks and Defenses for ViTs}\label{sec:vfm_vit}
Pre-trained ViTs are widely employed as backbones for various downstream tasks, frequently achieving state-of-the-art performance through efficient adaptation and fine-tuning. Unlike traditional CNNs, ViTs process images as sequences of tokenized patches, allowing them to better capture spatial dependencies. However, this patch-based mechanism also brings unique safety concerns and robustness challenges. This section explores these issues by reviewing ViT-related safety research, including adversarial attacks, backdoor \& poisoning attacks, and their corresponding defense strategies.  Table~\ref{tab:vfm_safety} provides a summary of the surveyed attacks and defenses, along with the commonly used datasets.


\subsubsection{Adversarial Attacks}\label{sec:ViT-adv}
Adversarial attacks on ViTs can be classified into \textbf{white-box attacks} and \textbf{black-box attacks} based on whether the attacker has full access to the victim model. Based on the attack strategy, white-box attacks can be further divided into 1) \textbf{patch attacks}, 2) \textbf{position embedding attacks} and 3) \textbf{attention attacks}, while black-box attacks can be summarized into 1) \textbf{transfer-based attacks} and 2) \textbf{query-based attacks}.

\paragraph{White-box Attacks}
\textbf{Patch Attacks} exploit the modular structure of ViTs, aiming to manipulate their inference processes by introducing targeted perturbations in specific patches of the input data. Joshi et al.~\cite{joshi2021adversarial} proposed an adversarial token attack method leveraging block sparsity to assess the vulnerability of ViTs to token-level perturbations. 
Expanding on this, \textbf{Patch-Fool}\cite{fu2022patch} introduces an adversarial attack framework that targets the self-attention modules by perturbing individual image patches, thereby manipulating attention scores.
Different from existing methods, \textbf{SlowFormer}~\cite{navaneet2024slowformer} introduces a universal adversarial patch can be applied to any image to increases computational and energy costs while preserving model accuracy.

\textbf{Position Embedding Attacks} aim to attack the spatial or sequential position of tokens in transformers. For example, \textbf{PE-Attack}~\cite{gao2024pe} explores the common vulnerability of positional embeddings to adversarial perturbations by disrupting their ability to encode positional information through periodicity manipulation, linearity distortion, and optimized embedding distortion.

\textbf{Attention Attacks} target vulnerabilities in the self-attention modules of ViTs. \textbf{Attention-Fool}~\cite{lovisotto2022give} manipulates dot-product similarities to redirect queries to adversarial key tokens, exposing the model's sensitivity to adversarial patches. Similarly, \textbf{AAS}~\cite{jain2024towards} mitigates gradient masking in ViTs by optimizing the pre-softmax output scaling factors, enhancing the effectiveness of attacks.


\paragraph{Black-box Attacks}
\textbf{Transfer-based Attacks} first generate adversarial examples using fully accessible surrogate models, which are then transferred to attack black-box victim ViTs. In this context, we first review attacks specifically designed for the ViT architecture.
\textbf{SE-TR}~\cite{naseer2021improving} enhances adversarial transferability by optimizing perturbations on an ensemble of models.
\textbf{ATA}~\cite{wang2022generating} strategically activates uncertain attention and perturbs sensitive embeddings within ViTs.
\textbf{LPM}\cite{wei2023boosting} mitigates the overfitting to model-specific discriminative regions through a patch-wise optimized binary mask.
Chen et al.\cite{chen2023understanding} introduced an Inductive Bias Attack (\textbf{IBA}) to suppress unique biases in ViTs and target shared inductive biases.
\textbf{TGR}~\cite{zhang2023transferable} reduces the variance of the backpropagated gradient within internal blocks.
\textbf{VDC}~\cite{zhang2024improving} employs virtual dense connections between deeper attention maps and MLP blocks to facilitate gradient backpropagation.
\textbf{FDAP}~\cite{gao2024attacking} exploits feature collapse by reducing high-frequency components in feature space.
\textbf{CRFA}~\cite{li2024improving} disrupts only the most crucial image regions using approximate attention maps.
\textbf{SASD-WS}~\cite{wu2024improving} flattens the loss landscape of the source model through sharpness-aware self-distillation and approximates an ensemble of pruned models using weight scaling to improve target adversarial transferability.

Other strategies are applicable to both ViTs and CNNs, ensuring broader applicability in black-box settings.
Wei et al.\cite{wei2022towards, wei2023towards} proposed a dual attack framework to improve transferability between ViTs and CNNs: 1) a Pay No Attention (\textbf{PNA}) attack, which skips the gradients of attention during backpropagation, and 2) a \textbf{PatchOut} attack, which randomly perturbs subsets of image patches at each iteration.
\textbf{MIG}\cite{ma2023transferable} uses integrated gradients and momentum-based updates to precisely target model-agnostic critical regions, improving transferability between ViTs and CNNs.


\textbf{Query-based Attacks} generate adversarial examples by querying the black-box model and levering the model responses to estimate the adversarial gradients. The goal is to achieve successful attack with a minimal number of queries. Based on the type of model response, query-based attacks can be further divided into score-based attacks, where the model returns a probability vector, and decision-based attacks, where the model provides only the top-k classes. Decision-based attacks typically start from a large random noise (to achieve misclassification first) and then gradually find smaller noise while maintaining misclassification.
To improve the efficiency of the adversarial noise searching process in ViTs, \textbf{PAR}~\cite{shi2022decision} introduces a coarse-to-fine patch searching method, guided by noise magnitude and sensitivity masks to account for the structural characteristics of ViTs and mitigate the negative impact of non-overlapping patches.



\subsubsection{Adversarial Defenses}\label{sec:ViT-advdefense}
Adversarial defenses for ViTs follow four major approaches: 1) \textbf{adversarial training}, which trains ViTs on adversarial examples via min-max optimization to improve its robustness; 2) \textbf{adversarial detection}, which identifies and mitigates adversarial attacks by detecting abnormal or malicious patterns in the inputs; 3) \textbf{robust architecture}, which modifies and optimizes the architecture (e.g., self-attention module) of ViTs to improve their resilience against adversarial attacks; and 4) \textbf{adversarial purification}, which pre-processes the input (e.g., noise injection, denoising, or other transformations) to remove potential adversarial perturbations before inference.


\textbf{Adversarial Training} is widely regarded as the most effective approach to adversarial defense; however, it comes with a high computational cost. To address this on ViTs, \textbf{AGAT}~\cite{wu2022towards} introduces a dynamic attention-guided dropping strategy, which accelerates the training process by selectively removing certain patch embeddings at each layer. This reduces computational overhead while maintaining robustness, especially on large datasets such as ImageNet. Due to its high computational cost, research on adversarial training for ViTs has been relatively limited.
\textbf{ARD-PRM}~\cite{mo2022adversarial} improves adversarial robustness by randomly dropping gradients in attention blocks and masking patch perturbations during training.

\textbf{Adversarial Detection} methods for ViTs primarily leverage two key features, i.e., patch-based inference and activation characteristics, to detect and mitigate adversarial examples.
Li et al.~\cite{li2022patch} proposed the concept of \textbf{Patch Vestiges}, abnormalities arising from adversarial examples during patch division in ViTs. They used statistical metrics on step changes between adjacent pixels across patches and developed a binary regression classifier to detect adversaries. Alternatively, \textbf{ARMOR}~\cite{liu2023understanding} identifies adversarial patches by scanning for unusually high column scores in specific layers and masking them with average images to reduce their impact. 
\textbf{ViTGuard}~\cite{sun2024vitguard}, on the other hand, employs a masked autoencoder to detect patch attacks by analyzing attention maps and CLS token representations. As more attacks are developed, there is a growing need for a unified detection framework capable of handling all types of adversarial examples.



\textbf{Robust Architecture} methods focus on designing more adversarially resilient attention modules for ViTs. 
For example, \textbf{Smoothed Attention}~\cite{gu2022vision} employs temperature scaling in the softmax function to prevent any single patch from dominating the attention, thereby balancing focus across patches.
\textbf{ReiT}~\cite{gong2024random} integrates adversarial training with randomization through the II-ReSA module, optimizing randomly entangled tokens to reduce adversarial similarity and enhance robustness.
\textbf{TAP}~\cite{guo2023robustifying} addresses token overfocusing by implementing token-aware average pooling and an attention diversification loss, which incorporate local neighborhood information and reduce cosine similarity among attention vectors. \textbf{FViTs}~\cite{huimproving} strengthen explanation faithfulness by stabilizing top-k indices in self-attention and robustify predictions using denoised diffusion smoothing combined with Gaussian noise. \textbf{RSPC}~\cite{guo2023improving} tackles vulnerabilities by corrupting the most sensitive patches and aligning intermediate features between clean and corrupted inputs to stabilize the attention mechanism. Collectively, these advancements underscore the pivotal role of the attention mechanism in improving the adversarial robustness of ViTs.


\textbf{Adversarial Purification} refers to a model-agnostic input-processing technique that is broadly applicable across various architectures, including but not limited to ViTs.
\textbf{DiffPure}~\cite{nie2022diffusion} introduces a framework where adversarial images undergo noise injection via a forward stochastic differential equation (SDE) process, followed by denoising with a pre-trained diffusion model. \textbf{CGDMP}~\cite{bai2024diffusion} refines this approach by optimizing the noise level for the forward process and employing contrastive loss gradients to guide the denoising process, achieving improved purification tailored to ViTs. \textbf{ADBM}~\cite{li2024adbm} highlights the disparity between diffused adversarial and clean examples, proposing a method to directly connect the clean and diffused adversarial distributions.
While these methods focus on ViTs, other approaches demonstrate broader applicability to various vision models, e.g., CNNs. \textbf{Purify++}\cite{zhang2023purify++} enhances DiffPure with improved diffusion models, \textbf{DifFilter}\cite{chen2024diffilter} extends noise scales to better preserve semantics, and \textbf{MimicDiffusion}\cite{song2024mimicdiffusion} mitigates adversarial impacts during the reverse diffusion process. For improved efficiency, \textbf{OSCP}\cite{lei2024instant} and \textbf{LightPure}\cite{khalili2024lightpure} propose single-step and real-time purification methods, respectively. \textbf{LoRID}\cite{zollicoffer2024lorid} introduces a Markov-based approach for robust purification. These methods complement ViT-related research and highlight diverse advancements in adversarial purification.



\subsubsection{Backdoor Attacks }\label{sec:ViT-backdoor}
Backdoors can be injected into the victim model via data poisoning, training manipulation, or parameter editing, with most existing attacks on ViTs being data poisoning-based. 
We classify these attacks into four categories: 1) \textbf{patch-level attacks}, 2) \textbf{token-level attacks}, and 3) \textbf{multi-trigger attacks}, which exploit ViT-specific data processing characteristics, as well as 4) \textbf{data-free attacks}, which exploit the inherent mechanisms of ViTs.


\textbf{Patch-level Attacks} primarily exploit the ViT's characteristic of processing images as discrete patches by implanting triggers at the patch level. For example, \textbf{BadViT}~\cite{yuan2023you} introduces a universal patch-wise trigger that requires only a small amount of data to redirect the model's focus from classification-relevant patches to adversarial triggers. 
\textbf{TrojViT}~\cite{zheng2023trojvit} improves this approach by utilizing patch salience ranking, an attention-targeted loss function, and parameter distillation to minimize the bit flips necessary to embed the backdoor.

\textbf{Token-level Attacks} target the tokenization layer of ViTs. \textbf{SWARM}~\cite{yang2024not} introduces a switchable backdoor mechanism featuring a ``switch token'' that dynamically toggles between benign and adversarial behaviors, ensuring high attack success rates while maintaining functionality in clean environments.

\textbf{Multi-trigger Attacks} employ multiple backdoor triggers in parallel, sequential, or hybrid configurations to poison the victim dataset.
\textbf{MTBAs}~\cite{li2024multi} utilize these multiple triggers to induce coexistence, overwriting, and cross-activation effects, significantly diminishing the effectiveness of existing defense mechanisms.

\textbf{Data-free Attacks} eliminate the need for original training datasets. Using substitute datasets, \textbf{DBIA}~\cite{lv2023dbia} generates universal triggers that maximize attention within ViTs. These triggers are fine-tuned with minimal parameter adjustments using PGD~\cite{madry2017towards}, enabling efficient and resource-light backdoor injection.




\subsubsection{Backdoor Defenses}\label{sec:ViT-backdoordefense}
Backdoor defenses for ViTs aim to identify and break (or remove) the correlation between trigger patterns and target classes while preserving model accuracy.
Two representative defense strategies are: 1) \textbf{patch processing}, which disrupts the integrity of image patches to prevent trigger activation, and 2) \textbf{image blocking}, which leverages interpretability-based mechanisms to mask and neutralize the effects of backdoor triggers.

\textbf{Patch Processing} strategy disrupts the integrity of patches to neutralize triggers.
Doan et al.~\cite{doan2023defending} found that clean-data accuracy and attack success rates of ViTs respond differently to patch transformations before positional encoding, and proposed an effective defense method by randomly dropping or shuffling patches of an image to counter both patch-based and blending-based backdoor attacks.
\textbf{Image Blocking} utilizes interpretability to identify and neutralize triggers.
Subramanya et al.~\cite{subramanya2022backdoor} showed that ViTs can localize backdoor triggers using attention maps and proposed a defense mechanism that dynamically masks potential trigger regions during inference.
In a subsequent work, Subramanya et al.~\cite{subramanya2024closer} proposed to integrate trigger neutralization into the training phase to improve the robustness of ViTs to backdoor attacks. 
While these two methods are promising, the field requires a holistic defense framework that integrates non-ViT defenses with ViT-specific characteristics and unifies multiple defense tasks including backdoor detection, trigger inversion, and backdoor removal, as attempted in \cite{li2024expose}.



\subsubsection{Datasets}\label{sec:ViT-dataset}
Datasets are crucial for developing and evaluating attack and defense methods. Table~\ref{tab:vfm_safety} summarizes the datasets used in adversarial and backdoor research.

\textbf{Datasets for Adversarial Research} As shown in Table~\ref{tab:vfm_safety}, adversarial researches were primarily conducted on ImageNet. While attacks were tested across various datasets like CIFAR-10/100, Food-101, and GLUE, defenses were mainly limited to ImageNet and CIFAR-10/100. This imbalance reveals one key issue in adversarial research: attacks are more versatile, while defenses struggle to generalize across different datasets.

\textbf{Datasets for Backdoor Research} Backdoor researches were also conducted mainly on ImageNet and CIFAR-10/100 datasets. Some attacks, such as DBIA and SWARM, extend to domain-specific datasets like GTSRB and VGGFace, while defenses, including PatchDrop, were often limited to a few benchmarks. This narrow focus reduces their real-world applicability. 
Although backdoor defenses are shifting towards robust inference techniques, they typically target specific attack patterns, limiting their generalizability. To address this, adaptive defense strategies need to be tested across a broader range of datasets to effectively counter the evolving nature of backdoor threats.





\subsection{Attacks and Defenses for SAM}\label{sec:vfm_sam}
SAM is a foundational model for image segmentation, comprising three primary components: a ViT-based image encoder, a prompt encoder, and a mask decoder. The image encoder transforms high-resolution images into embeddings, while the prompt encoder converts various input modalities into token embeddings. The mask decoder combines these embeddings to generate segmentation masks using a two-layer Transformer architecture.
Due to its complex structure, attacks and defenses targeting SAM differ significantly from those developed for CNNs. These unique challenges stem from SAM's modular and interconnected design, where vulnerabilities in one component can propagate to others, necessitating specialized strategies for both attack and defense. 
This section systematically reviews SAM-related adversarial attacks, backdoor \& poisoning attacks, and adversarial defense strategies, as summarized in Table~\ref{tab:vfm_safety}.





\subsubsection{Adversarial Attacks}\label{sec:SAM-adv}

Adversarial attacks on SAM can be categorized into: (1) \textbf{white-box attacks}, exemplified by \emph{prompt-agnostic attacks}, and (2) \textbf{black-box attacks}, which can be further divided into \emph{universal attacks} and \emph{transfer-based attacks}. Each category employs distinct strategies to compromise segmentation performance.

\paragraph{White-box Attacks}
\textbf{Prompt-Agnostic Attacks} are white-box attacks that disrupt SAM's segmentation without relying on specific prompts, using either \emph{prompt-level} or \emph{feature-level} perturbations for generality across inputs.
For prompt-level attacks, Shen et al.~\cite{shen2024practical} proposed a grid-based strategy to generate adversarial perturbations that disrupt segmentation regardless of click location.
For feature-level attacks, Croce et al.~\cite{croce2024segment} perturbed features from the image encoder to distort spatial embeddings, undermining SAM’s segmentation integrity.

\paragraph{Black-box Attacks}

\textbf{Universal Attacks} generate UAPs~\cite{moosavi2017universal} that can consistently disrupt SAM across arbitrary prompts.  Han et al.~\cite{han2023segment} exploited contrastive learning to optimize the UAPs, achieving better attack performance by exacerbating feature misalignment.
\textbf{DarkSAM}~\cite{zhou2024darksam}, on the other hand, introduces a hybrid spatial-frequency framework that combines semantic decoupling and texture distortion to generate universal perturbations.  


\textbf{Transfer-based Attacks} exploit transferable representations in SAM to generate perturbations that remain adversarial across different models and tasks. 
\textbf{PATA++}\cite{zheng2023black} improves transferability by using a regularization loss to highlight key features in the image encoder, reducing reliance on prompt-specific data. 
\textbf{Attack-SAM}\cite{zhang2023attack} employs ClipMSE loss to focus on mask removal, optimizing for spatial and semantic consistency to improve cross-task transferability. 
\textbf{UMI-GRAT}~\cite{xia2024transferable} follows a two-step process: it first generates a generalizable perturbation with a surrogate model and then applies gradient robust loss to improve across-model transferability.
Apart from designing new loss functions, optimization over transformation techniques can also be exploited to improve transferability.
This includes \textbf{T-RA}~\cite{shen2024practical}, which improves cross-model transferability by applying spectrum transformations to generate adversarial perturbations that degrade segmentation in SAM variants, and \textbf{UAD}~\cite{lu2024unsegment}, which generates adversarial examples by deforming images in a two-stage process and aligning features with the deformed targets.



\subsubsection{Adversarial Defenses}\label{sec:SAM-advdefense}

Adversarial defenses for SAM are currently limited, with existing approaches focusing primarily on adversarial tuning, which integrates adversarial training into the prompt tuning process of SAM. For example, \textbf{ASAM}~\cite{li2024asam} utilizes a stable diffusion model to generate realistic adversarial samples on a low-dimensional manifold through diffusion model-based tuning. ControlNet~\cite{ControlNet} is then employed to guide the re-projection process, ensuring that the generated samples align with the original mask annotations. Finally, SAM is fine-tuned using these adversarial examples.





\subsubsection{Backdoor \& Poisoning Attacks}\label{sec:SAM-backdoor_poisoning}

Backdoor and poisoning attacks on SAM remain underexplored. Here, we review one backdoor attack that leverages perceptible visual triggers to compromise SAM, and one poisoning attack that exploits unlearnable examples~\cite{huang2021unlearnable} with imperceptible noise to protect unauthorized image data from being exploited by segmentation models.
\textbf{BadSAM}~\cite{guan2024badsam} is a backdoor attack targeting SAM that embeds visual triggers during the model's adaptation phase, implanting backdoors that enable attackers to manipulate the model's output with specific inputs. Specifically, the attack introduces MLP layers to SAM and injects the backdoor trigger into these layers via SAM-Adapter \cite{chen2023sam}.
\textbf{UnSeg}~\cite{sun2024unseg} is a data poisoning attack on SAM designed for benign purposes, i.e., data protection. It fine-tunes a universal unlearnable noise generator, leveraging a bilevel optimization framework based on a pre-trained SAM. This allows the generator to efficiently produce poisoned (protected) samples, effectively preventing a segmentation model from learning from the protected data and thereby safeguarding against unauthorized exploitation of personal information.



\subsubsection{Datasets}\label{sec:SAM-dataset}

As shown in Table~\ref{tab:vfm_safety}, the datasets used in safety research on SAM slightly differ from those typically used in general segmentation tasks~\cite{MOSE,MeViS}. 
For \textbf{attack research}, the SA-1B dataset and its subsets~\cite{kirillov2023segment} are the most commonly used for evaluating adversarial attacks~\cite{croce2024segment, han2023segment, lu2024unsegment, shen2024practical, zhang2023attack, zheng2023black}.
Additionally, \textbf{DarkSAM} was evaluated on datasets such as Cityscapes \cite{cordts2016cityscapes}, COCO \cite{lin2014microsoft}, and ADE20k \cite{zhou2017scene}, while \textbf{UMI-GRAT}, which targets downstream tasks related to SAM, was tested on medical datasets like CT-Scans and ISTD, as well as camouflage datasets, including COD10K, CAMO, and CHAME. For backdoor attacks, \textbf{BadSAM} was assessed using the CAMO dataset~\cite{le2019anabranch}. In the context of data poisoning, \textbf{UnSeg}~\cite{sun2024unseg} was evaluated across 10 datasets, including COCO, Cityscapes, ADE20k, WHU, and medical datasets like Lung and Kvasir-seg.
For \textbf{defense research}, \textbf{ASAM}~\cite{li2024asam} is currently the only defense method applied to SAM. It was evaluated on a range of datasets with more diverse image distributions than SA-1B, including ADE20k, LVIS, COCO, and others, with mean Intersection over Union (mIoU) used as the evaluation metric.
