\section{Vision-Language Model Safety} \label{sec:vlm}

Large VLMs extend LLMs by adding a visual modality through pre-trained image encoders and alignment modules, enabling applications like visual conversation and complex reasoning. However, this multi-modal design introduces unique vulnerabilities.
This section reviews \textbf{adversarial attacks}, \textbf{latency energy attacks}, \textbf{jailbreak attacks}, \textbf{prompt injection attacks}, \textbf{backdoor \& poisoning attacks}, and \textbf{defenses} developed for VLMs. Many VLMs use VLP-trained encoders, so the attacks and defenses discussed in Section~\ref{sec:vlp} also apply to VLMs. The additional alignment process between the VLM pre-trained encoders and LLMs, however, expands the attack surface, with new risks like cross-modal backdoor attacks and jailbreaks targeting both text and image inputs. This underscores the need for safety measures tailored to VLMs.


\subsection{Adversarial Attacks}
\label{sec:vlm-adversarial}
Adversarial attacks on VLMs primarily target the visual modality, which, unlike text, is more susceptible to adversarial perturbations due to its high-dimensional nature. By adding imperceptible changes to images, attackers aim to disrupt tasks like image captioning and visual question answering. These attacks are classified into \textbf{white-box} and \textbf{black-box} categories based on the threat model. 





\subsubsection{White-box Attacks}

White-box adversarial attacks on VLMs have full access to the model parameters, including both vision encoders and LLMs. These attacks can be classified into three types based on their objectives: \textbf{task-specific attacks}, \textbf{cross-prompt attack}, and \textbf{chain-of-thought (CoT) attack}.

\textbf{Task-specific Attacks}  Schlarmann et al.~\cite{schlarmann2023adversarial} were the first to highlight the vulnerability of VLMs like Flamingo~\cite{alayrac2022flamingo} and GPT-4~\cite{gpt-4} to adversarial images that manipulate caption outputs. Their study showed how attackers can exploit these vulnerabilities to mislead users, redirecting them to harmful websites or spreading misinformation. Gao et al.~\cite{gao2024adversarial} introduced attack paradigms targeting the referring expression comprehension task, while \cite{cui2024robustness} proposed a query decomposition method and demonstrated how contextual prompts can enhance VLM robustness against visual attacks.

\textbf{Cross-prompt Attack} refer to adversarial attacks that remain effective across different prompts. For example, \textbf{CroPA}~\cite{luo2024image} explored the transferability of a single adversarial image across multiple prompts, investigating whether it could mislead predictions in various contexts. To tackle this, they proposed refining adversarial perturbations through learnable prompts to enhance transferability.


\textbf{CoT Attack} targets the CoT reasoning process of VLMs. \textbf{Stop-reasoning Attack} \cite{wang2024stop} explored the impact of CoT reasoning on adversarial robustness. Despite observing some improvements in robustness, they introduced a novel attack designed to bypass these defenses and interfere with the reasoning process within VLMs.





\subsubsection{Gray-box Attacks}
Gray-box adversarial attacks typically involve access to either the vision encoders or the LLM of a VLM, with a focus on vision encoders as the key differentiator between VLMs and LLMs. Attackers craft adversarial images that closely resemble target images, manipulating model predictions without full access to the VLM.
For instance, \textbf{InstructTA} \cite{wang2023instructta} generates a target image and uses a surrogate model to create adversarial perturbations, minimizing the feature distance between the original and adversarial image. To improve transferability, the attack incorporates GPT-4 paraphrasing to refine instructions.

\subsubsection{Black-box Attacks}

In contrast, black-box attacks do not require access to the target model's internal parameters and typically rely on \textbf{transfer-based} or \textbf{generator-based} methods. 



\textbf{Transfer-based Attacks} exploit the widespread use of frozen CLIP vision encoders in many VLMs. \textbf{AttackBard} \cite{dong2023robust} demonstrates that adversarial images generated from surrogate models can successfully mislead Google's Bard, despite its defense mechanisms. Similarly, \textbf{AttackVLM} \cite{zhao2024evaluating} crafts targeted adversarial images for models like CLIP \cite{radford2021learning} and BLIP \cite{BLIP-2}, successfully transferring these adversarial inputs to other VLMs. It also shows that black-box queries further improved the success rate of generating targeted responses, illustrating the potency of cross-model transferability. 

\textbf{Generator-based Attacks} leverage generative models to create adversarial examples with improved transferability. \textbf{AdvDiffVLM} \cite{guo2024efficiently} uses diffusion models to generate natural, targeted adversarial images with enhanced transferability. By combining adaptive ensemble gradient estimation and GradCAM-guided masking, it improves the semantic embedding of adversarial examples and spreads the targeted semantics more effectively across the image, leading to more robust attacks. \textbf{AnyAttack} \cite{zhang2024anyattack} presents a self-supervised framework for generating targeted adversarial images without label supervision. By utilizing contrastive loss, it efficiently creates adversarial examples that mislead models across diverse tasks.




\subsection{Jailbreak Attacks}
\label{sec:vlm-jailbreak}

\begin{table*}[htbp]
  \centering
  \caption{A summary of attacks and defenses for VLMs.}
  \resizebox{\textwidth}{!}{
    \begin{tabular}
{p{0.1\textwidth}p{0.15\textwidth}p{0.05\textwidth}p{0.15\textwidth}p{0.15\textwidth}p{0.25\textwidth}p{0.2\textwidth}}

\hline
    \rowcolor{wangruofan-orange}
    Attack/Defense & Method & Year & Category & Subcategory & Target Models & Datasets \\\hline
    \multirow{10}{0.1\textwidth}{Adversarial Attack} & Caption Attack \cite{schlarmann2023adversarial} & 2023 & White-box & Task-specific+V & OpenFlamingo & MS-COCO/Flickr30k/OK-VQA/VizWiz \\
    & \cellcolor{gray!15}VisBreaker \cite{cui2024robustness} & \cellcolor{gray!15}\cellcolor{gray!15}2023 & \cellcolor{gray!15}White-box & \cellcolor{gray!15}Task-specific+V & \cellcolor{gray!15}LLaVA/BLIP-2/InstructBLIP & \cellcolor{gray!15}MS-COCO/VQA V2/ScienceQA-Image/TextVQA/POPE/MME \\
    & CroPA \cite{luo2024image} & 2024 & White-box & Cross-prompt+VL & OpenFlamingo/BLIP-2/InstructBLIP & MS-COCO/VQA-v2 \\
    & \cellcolor{gray!15}GroundBreaker \cite{gao2024adversarial} & \cellcolor{gray!15}2024 &\cellcolor{gray!15} White-box & \cellcolor{gray!15}Task-specific+V &\cellcolor{gray!15} MiniGPT-v2 & \cellcolor{gray!15}RefCOCO/RefCOCO+/RefCOCOg \\
    & Stop-reasoning Attack \cite{wang2024stop} & 2024 & White-box & CoT attack+V & MiniGPT-4/OpenFlamingo/LLaVA & ScienceQA/A-OKVQA \\
    & \cellcolor{gray!15}InstructTA \cite{wang2023instructta} &\cellcolor{gray!15} 2023 &\cellcolor{gray!15} Gray-box & \cellcolor{gray!15}Encoder attack+V & \cellcolor{gray!15}BLIP-2/InstructBLIP/MiniGPT-4/LLaVA/CogVLM & \cellcolor{gray!15}ImageNet-1K/LLaVA-Instruct-150K/MS-COCO \\
    & Attack Bard \cite{dong2023robust} & 2023 & Black-box & Transfer-based+V & Bard/GPT-4V/Bing Chat/ERNIE Bot & NeurIPS’17 adversarial competition dataset \\
    & \cellcolor{gray!15}AttackVLM \cite{zhao2024evaluating} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}Black-box & \cellcolor{gray!15}Transfer-based+V & \cellcolor{gray!15}BLIP/UniDiffuser/Img2Prompt/BLIP-2/LLaVA/MiniGPT-4 & \cellcolor{gray!15}ImageNet-1K/MS-COCO \\
    & AdvDiffVLM \cite{guo2024efficiently} & 2024 & Black-box & Generator-based+V & MiniGPT-4/LLaVA/UniDiffuser/MiniGPT-4/BLIP/BLIP-2/Img2LLM & NeurIPS’17 adversarial competition dataset/MS-COCO \\
        & \cellcolor{gray!15}AnyAttack \cite{zhang2024anyattack} &\cellcolor{gray!15} 2024 &\cellcolor{gray!15} Black-box & \cellcolor{gray!15}Generator-based+V & \cellcolor{gray!15}CLIP/BLIP/BLIP2/InstructBLIP/MiniGPT-4 & \cellcolor{gray!15}MSCOCO/Flickr30K/SNLI-VE \\\hline

    \multirow{1}{0.1\textwidth}{Latency-Energy Attack} & Verbose Images \cite{gaoinducing} & 2024 & White-box & Task-specific+V & BLIP/BLIP2/InstructBLIP/MiniGPT-4 & MS-COCO/ImageNet
    \\\hline

    
    \multirow{11}{0.1\textwidth}{Jailbreak Attack} &\cellcolor{gray!15} Image Hijack \cite{bailey2023image} & \cellcolor{gray!15}2023 & \cellcolor{gray!15}White-box & \cellcolor{gray!15}Target-specific+V & \cellcolor{gray!15}LLaVA & \cellcolor{gray!15}Alpaca training set/AdvBench \\
    & Adversarial Alignment Attack \cite{carlini2024aligned} & 2024 & White-box & Target-specific+V & MiniGPT-4/LLaVA/LLaMA Adapter & toxic phrase dataset \\
    & \cellcolor{gray!15}VAJM \cite{qi2024visual} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}White-box & \cellcolor{gray!15}Universal attack+V & \cellcolor{gray!15}MiniGPT-4/LLaVA/InstructBLIP & \cellcolor{gray!15}VAJM training set/VAJM test set/RealToxicityPrompts \\
    & imgJP \cite{niu2024jailbreaking} & 2024 & White-box & Universal attack+V & MiniGPT-4/MiniGPT-v2/LLaVA/InstructBLIP/mPLUG-Owl2 & AdvBench-M \\
    & \cellcolor{gray!15}UMK \cite{wang2024white} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}White-box & \cellcolor{gray!15}Universal attack+VL & \cellcolor{gray!15}MiniGPT-4 & \cellcolor{gray!15}AdvBench/VAJM training set/VAJM test set/RealToxicityPrompts \\
    & HADES \cite{li2024images} & 2024 & White-box & Hybrid method+V & LLaVA/GPT-4V/Gemini-Pro-Vision & HADES dataset \\
    & \cellcolor{gray!15}Jailbreak in Pieces \cite{shayegani2023jailbreak} & \cellcolor{gray!15}2023 & \cellcolor{gray!15}Black-box & \cellcolor{gray!15}Transfer-based+V & \cellcolor{gray!15}LlaVA /LLaMA-Adapter V2 & \cellcolor{gray!15}Jailbreak in Pieces dataset \\
    & Figstep \cite{gong2023figstep} & 2023 & Black-box & Manual pipeline+V & LLaVA-v1.5/MiniGPT-4/CogVLM/GPT-4V & SafeBench \\
    & \cellcolor{gray!15}SASP \cite{wu2023jailbreaking} & \cellcolor{gray!15}2023 & \cellcolor{gray!15}Black-box & \cellcolor{gray!15}Prompt leakage+L & \cellcolor{gray!15}LLaVA/GPT-4V & \cellcolor{gray!15}Celebrity face image dataset/CelebA/LFWA \\
    & VRP \cite{ma2024visual} & 2024 & Black-box & Manual pipeline+V & LLaVA/Qwen-VL-Chat/ OmniLMM /InternVL Chat-V1.5/Gemini-Pro-Vision & RedTeam-2k/HarmBench \\
    &\cellcolor{gray!15} IDEATOR \cite{wang2024ideator} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}Black-box &\cellcolor{gray!15} Red teaming+VL & \cellcolor{gray!15}LLaVA/InstructBLIP/MiniGPT-4 & \cellcolor{gray!15}AdvBench/VAJM test set \\\hline

    \multirow{2}{0.1\textwidth}{Prompt Injection Attack} & Adversarial Prompt Injection \cite{bagdasaryan2023ab} & 2023 & White-box & Optimization-based+V & LLaVA/PandaGPT & Self-collected dataset \\
    & \cellcolor{gray!15}Typographic Attack \cite{qraitem2024vision} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}Black-box & \cellcolor{gray!15}Typography-based+V & \cellcolor{gray!15}LLaVA/MiniGPT4/InstructBLIP/GPT-4V & 
 \cellcolor{gray!15}OxfordPets / StanfordCars / Flowers / Aircraft / Food101 \\\hline

    \multirow{5}{0.1\textwidth}{Backdoor \& Poisoning Attack} & Shadowcast \cite{xu2024shadowcast} & 2024 & Poisoning & Tuning-stage+VL & LLaVA/MiniGPT-v2/InstructBLIP & cc-sbu-align dataset \\
    & \cellcolor{gray!15}Instruction-Tuned Backdoor \cite{liang2024revisiting} & \cellcolor{gray!15}2024 &\cellcolor{gray!15} Backdoor & \cellcolor{gray!15}Tuning-stage+VL & \cellcolor{gray!15}OpenFlamingo/BLIP-2/LLaVA &\cellcolor{gray!15} MIMIC-IT/COCO/Flickr30K \\
    & Anydoor \cite{lu2024test} & 2024 & Backdoor & Testing-stage+VL & LLaVA/MiniGPT-4/InstructBLIP/BLIP-2 & VQAv2/SVIT/DALL-E dataset \\
    & \cellcolor{gray!15}BadVLMDriver \cite{ni2024physical} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}Backdoor & \cellcolor{gray!15}Tuning-stage+V & \cellcolor{gray!15}LLaVA/MiniGPT-4 & \cellcolor{gray!15}nuScenes dataset \\
    & ImgTrojan \cite{tao2024imgtrojan} & 2024 & Backdoor & Tuning-stage+VL & LLaVA & LAION \\\hline

    \multirow{6}{0.1\textwidth}{Jailbreak Defenses} & \cellcolor{gray!15}JailGuard \cite{zhang2023mutation} & \cellcolor{gray!15}2023 & \cellcolor{gray!15}Detection & \cellcolor{gray!15}Detection+VL & \cellcolor{gray!15}GPT-3.5/MiniGPT-4 & \cellcolor{gray!15}Self-collected dataset \\
    & GuardMM \cite{sharma2024defending} & 2024 & Detection & Detection+V & GPT-4V/LLAVA/MINIGPT-4 & Self-collected dataset \\
    & \cellcolor{gray!15}AdaShield \cite{wang2024adashield} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}Prevention & \cellcolor{gray!15}Prevention+V & \cellcolor{gray!15}LLaVA/CogVLM/MiniGPT-v2 & \cellcolor{gray!15}Figstep/QR \\
    & MLLM-Protector \cite{pi2024mllm} & 2024 & Prevention & Detection+Prevention+V & Open-LLaMA/LLaMA/LLaVA & Safe-Harm-10K \\
    & \cellcolor{gray!15}ECSO \cite{gou2024eyes} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}Prevention & \cellcolor{gray!15}Prevention+V & \cellcolor{gray!15}LLaVA/ShareGPT4V/mPLUG-OWL2/Qwen-VL-Chat/InternLM-XComposer & \cellcolor{gray!15}MM-SafetyBench/VLSafe/VLGuard \\
    & InferAligner \cite{wang2024inferaligner} & 2024 & Prevention & Prevention+VL & LLaMA2/LLaVA & AdvBench/TruthfulQA/MM-Harmful Bench \\
    & \cellcolor{gray!15}BlueSuffix \cite{zhao2024bluesuffix} & \cellcolor{gray!15}2024 & \cellcolor{gray!15}Prevention & \cellcolor{gray!15}Prevention+VL & \cellcolor{gray!15}LLaVA/MiniGPT-4/Gemini & \cellcolor{gray!15}MM-SafetyBench/RedTeam-2k \\\hline
    \end{tabular}%
  }
  \label{tab:addlabel}
\end{table*}



The inclusion of a visual modality in VLMs provides additional routes for jailbreak attacks. While adversarial attacks generally induce random or targeted errors, jailbreak attacks specifically target the model's safeguards to generate inappropriate outputs. Like adversarial attacks, jailbreak attacks on VLMs can be classified as \textbf{white-box} or \textbf{black-box} attacks. 

\subsubsection{White-box Attacks}
White-box jailbreak attacks leverage gradient information to perturb input images or text, targeting specific behaviors in VLMs. These attacks can be further categorized into three types: \textbf{target-specific jailbreak}, \textbf{universal jailbreak}, and \textbf{hybrid jailbreak}, each exploiting different aspects of the model's safety measures.

\textbf{Target-specific Jailbreak} focuses on inducing a specific type of harmful output from the model.
\textbf{Image Hijack} \cite{bailey2023image} introduces adversarial images that manipulate VLM outputs, such as leaking information, bypassing safety measures, and generating false statements. These attacks, trained on generic datasets, effectively force models to produce harmful outputs.
Similarly, \textbf{Adversarial Alignment Attack} \cite{carlini2024aligned} demonstrates that adversarial images can induce misaligned behaviors in VLMs, suggesting that similar techniques could be adapted for text-only models using advanced NLP methods.

\textbf{Universal Jailbreak} bypasses model safeguards, causing it to generate harmful content beyond the adversarial input.
\textbf{VAJM} \cite{qi2024visual} shows that a single adversarial image can universally bypass VLM safety, forcing universal harmful outputs. \textbf{ImgJP} \cite{niu2024jailbreaking} uses a maximum likelihood algorithm to create transferable adversarial images that jailbreak various VLMs, even bridging VLM and LLM attacks by converting images to text prompts. \textbf{UMK} \cite{wang2024white} proposes a dual optimization attack targeting both text and image modalities, embedding toxic semantics in images and text to maximize impact. \textbf{HADES} \cite{li2024images} introduces a hybrid jailbreak method that combines universal adversarial images with crafted inputs to bypass safety mechanisms, effectively amplifying harmful instructions and enabling robust adversarial manipulation.



\subsubsection{Black-box Attacks}

Black-box jailbreak attacks do not require direct access to the internal parameters of the target VLM. Instead, they exploit external vulnerabilities, such as those in the frozen CLIP vision encoder, interactions between vision and language modalities, or system prompt leakage. These attacks can be classified into four main categories: \textbf{transfer-based attacks}, \textbf{manually-designed attacks}, \textbf{system prompt leakage}, and \textbf{red teaming}, each employing distinct strategies to bypass VLM defenses and trigger harmful behaviors.

\textbf{Transfer-based Attacks} on VLMs typically assume the attacker has access to the image encoder (or its open-source version), which is used to generate adversarial images that can then be transferred to attack the black-box LLM. For example, \textbf{Jailbreak in Pieces} \cite{shayegani2023jailbreak} introduces cross-modality attacks that transfer adversarial images, crafted using the image encoder (assume the model employed an open-source encoder), along with clean textual prompts to break VLM alignment. 

\textbf{Manually-designed Attacks} can be as effective as optimized ones. For instance, \textbf{FigStep} \cite{gong2023figstep} introduces an algorithm that bypasses safety measures by converting harmful text into images via typography, enabling VLMs to visually interpret the harmful intent. \textbf{VRP} \cite{ma2024visual} adopts a visual role-play approach, using LLM-generated images of high-risk characters based on detailed descriptions. By pairing these images with benign role-play instructions, VRP exploits the negative traits of the characters to deceive VLMs into generating harmful outputs.

\textbf{System Prompt Leakage} is another significant black-box jailbreak method, exemplified by \textbf{SASP} \cite{wu2023jailbreaking}. By exploiting a system prompt leakage in GPT-4V, SASP allowed the model to perform a self-adversarial attack, demonstrating the risks of internal prompt exposure.

\textbf{Red Teaming} recently saw an advancement with IDEATOR \cite{wang2024ideator}, which integrated a VLM with an advanced diffusion model to autonomously generate malicious image-text pairs. This approach overcomes the limitations of manually designed attacks, providing a scalable and efficient method for creating adversarial inputs without direct access to the target model.

\subsection{Jailbreak Defenses}
\label{sec:vlm-defenses}



This section reviews defense methods for VLMs against jailbreak attacks, categorized into \textbf{jailbreak detection} and \textbf{jailbreak prevention}. Detection methods identify harmful inputs or outputs for rejection or purification, while prevention methods enhance the model's inherent robustness to jailbreak queries through safety alignment or filters.

\subsubsection{Jailbreak Detection}
\textbf{JailGuard} \cite{zhang2023mutation} detects jailbreak attacks by mutating untrusted inputs and analyzing discrepancies in model responses. It uses 18 mutators for text and image inputs, improving generalization across attack types. 
\textbf{GuardMM} \cite{sharma2024defending} is a two-stage defense: the first stage validates inputs to detect unsafe content, while the second stage focuses on prompt injection detection to protect against image-based attacks. It uses a specialized language to enforce safety rules and standards. \textbf{MLLM-Protector} \cite{pi2024mllm} identifies harmful responses using a lightweight detector and detoxifies them through a specialized transformation mechanism. Its modular design enables easy integration into existing VLMs, enhancing safety and preventing harmful content generation.

\subsubsection{Jailbreak Prevention}
\textbf{AdaShield} \cite{wang2024adashield} defends against structure-based jailbreaks by prepending defense prompts to inputs, refining them adaptively through collaboration between the VLM and an LLM-based prompt generator, without requiring fine-tuning. \textbf{ECSO} \cite{gou2024eyes} offers a training-free protection by converting unsafe images into text descriptions, activating the safety alignment of pre-trained LLMs within VLMs to ensure safer outputs. 
\textbf{InferAligner} \cite{wang2024inferaligner} applies cross-model guidance during inference, adjusting activations using safety vectors to generate safe and reliable outputs. \textbf{BlueSuffix} \cite{zhao2024bluesuffix} introduces a reinforcement learning-based black-box defense framework consisting of three key components: (1) an image purifier for securing visual inputs, (2) a text purifier for safeguarding textual inputs, and (3) a reinforcement fine-tuning-based suffix generator that leverages bimodal gradients to enhance cross-modal robustness.

\subsection{Energy Latency Attacks}
\label{sec:vlm-latency}
Similar to LLMs, multi-modal LLMs also face significant computational demands. Verbose images~\cite{gaoinducing} exploit these demands by overwhelming service resources, resulting in higher server costs, increased latency, and inefficient GPU usage. These images are specifically designed to delay the occurrence of the EOS token, increasing the number of auto-regressive decoder calls, which in turn raises both energy consumption and latency costs.



\subsection{Prompt Injection Attacks}
\label{sec:vlm-injection}

Prompt injection attacks against VLMs share the same objective as those against LLMs (Section~\ref{sec:llm}), but the visual modality introduces continuous features that are more easily exploited through adversarial attacks or direct injection. These attacks can be further classified into \emph{optimization-based attacks} and \emph{typography-based attacks}.

\textbf{Optimization-based Attacks} often optimize the input images using (white-box) gradients to produce stronger attacks. These attacks manipulate the model's responses, influencing future interactions. One representative method is \textbf{Adversarial Prompt Injection} \cite{bagdasaryan2023ab}, where attackers embed malicious instructions into VLMs by adding adversarial perturbations to images. 

 \textbf{Typography-based Attacks} exploit VLMs' typographic vulnerabilities by embedding deceptive text into images without requiring gradient access (i.e., black-box). The \textbf{Typographic Attack} \cite{qraitem2024vision} introduces two variations: \emph{Class-Based Attack} to misidentify classes and \emph{Descriptive Attack} to generate misleading labels. These attacks can also leak personal information \cite{chen2023can}, highlighting significant security risks.

\subsection{Backdoor\& Poisoning Attacks}
\label{sec:vlm-backdoor}
Most VLMs rely on VLP encoders, with safety threats discussed in Section \ref{sec:vlp}. This section focuses on backdoor and poisoning risks arising during fine-tuning and testing, specifically when aligning vision encoders with LLMs. 
Backdoor attacks embed triggers in visual or textual inputs to elicit specific outputs, while poisoning attacks inject malicious image-text pairs to degrade model performance.
We review backdoor and poisoning attacks separately, though most of these works are backdoor attacks.

\subsubsection{Backdoor Attacks}
We further classify backdoor attacks on VLMs into \textbf{tuning-time backdoor} and \textbf{testing-time backdoor}.

\textbf{Tuning-time Backdoor} injects the backdoor during VLM instruction tuning. \textbf{MABA} \cite{liang2024revisiting} targets domain shifts by adding domain-agnostic triggers using attributional interpretation, enhancing attack robustness across mismatched domains in image captioning tasks.
\textbf{BadVLMDriver} \cite{ni2024physical} introduced a physical backdoor for autonomous driving, using objects like red balloons to trigger unsafe actions such as sudden acceleration, bypassing digital defenses and posing real-world risks. Its automated pipeline generates backdoor training samples with malicious behaviors for stealthy, flexible attacks. \textbf{ImgTrojan} \cite{tao2024imgtrojan} introduces a jailbreaking attack by poisoning image-text pairs in training data, replacing captions with malicious prompts to enable VLM jailbreaks, exposing risks of compromised datasets. 

\textbf{Test-time Backdoor} leverages the similarity of universal adversarial perturbations and backdoor triggers to inject backdoor at test-time. AnyDoor \cite{lu2024test} embeds triggers in the textual modality via adversarial test images with universal perturbations, creating a text backdoor from image-perturbation combinations. It can also be seen as a multi-modal universal adversarial attack. Unlike traditional methods, AnyDoor does not require access to training data, enabling attackers to separate setup and activation of the attack. 


\subsubsection{Poisoning Attacks}

\textbf{Shadowcast} \cite{xu2024shadowcast} is a stealthy tuning-time backdoor attack on VLMs. It injects poisoned samples visually indistinguishable from benign ones, targeting two objectives: 1) \textbf{Label Attack}, which misclassifies objects, and 2) \textbf{Persuasion Attack}, which generates misleading narratives. With only 50 poisoned samples, Shadowcast achieves high effectiveness, showing robustness and transferability across VLMs in black-box settings.




\subsection{Datasets \& Benchmarks}
\label{sec:vlm-dataset}

\begin{table}[htbp]
  \centering
  \caption{Safety and robustness benchmarks for VLMs.}
  \resizebox{0.5\textwidth}{!}{ % Resize the table to fit within the text width
    \begin{tabular}{cccc}\hline
    \rowcolor{wangruofan-orange}
    Benchmarks & Year  & Size  & \# VLMs evaluated \\\hline
     OODCV-VQA \cite{tu2023many} & 2023  & 4,244 & 21 \\
    Sketchy-VQA \cite{tu2023many} & 2023  & 4,000 & 21 \\
    MM-SafetyBench \cite{liu2023mm} & 2023  & 5,040 & 12 \\
    AVIBench \cite{zhang2024avibench} & 2024  & 260,000  & 14 \\
    Jailbreak Evaluation of GPT-4o \cite{ying2024unveiling} & 2024  & 4,180 & 1 \\
    JailBreakV-28K \cite{luo2024jailbreakv} & 2024  & 28,000 & 10 \\\hline
    \end{tabular}%
    }
  \label{tab:vlm_safety_benchmarks}%
\end{table}

The datasets used in VLM safety research are detailed in Table \ref{tab:vfm_safety}. Below, we review the benchmarks proposed for evaluating VLM safety and robustness, summarized in Table \ref{tab:vlm_safety_benchmarks}.
\textbf{SafeSight} \cite{tu2023many} introduces two VQA datasets, \textbf{OODCV-VQA} and \textbf{Sketchy-VQA}, to evaluate out-of-distribution (OOD) robustness, highlighting VLMs' vulnerabilities to OOD texts and vision encoder weaknesses. \textbf{MM-SafetyBench} \cite{liu2023mm} focuses on image-based manipulations, revealing vulnerabilities in multi-modal interactions. \textbf{AVIBench} \cite{zhang2024avibench} evaluates VLM robustness against 260K adversarial visual instructions, exposing susceptibility to image-based, text-based, and content-biased  adversarial visual instructions (AVIs). \textbf{Jailbreak Evaluation of GPT-4o} \cite{ying2024unveiling} tests GPT-4o with multi-modal and unimodal jailbreak attacks, uncovering alignment vulnerabilities. \textbf{JailBreakV-28K} \cite{luo2024jailbreakv} assesses the transferability of LLM jailbreak techniques to VLMs, showing high attack success rates across 10 open-source models. These studies collectively reveal significant vulnerabilities in VLMs to OOD inputs, adversarial instructions, and multi-modal jailbreaks.

