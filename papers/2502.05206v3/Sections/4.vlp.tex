\begin{table*}[htp]
\centering
\caption{A summary of attacks and defenses for VLP models.}\label{tab:vlp_safety}
\resizebox{1\textwidth}{!}{
\begin{tabular}{llllllp{10cm}}
\hline
\rowcolor{wangxin-yellow}
Attack/Defense              & Method  & Year  & Category  & Subcategory  & Target Model & Dataset\\ \hline
\multirow{11}{0.12\textwidth}{Adversarial Attack}  
& Co-Attack~\cite{zhang2022towards}  
  & 2022 & White-box  & Invisible
  & ALBEF, TCL, CLIP
  & MS-COCO, Flickr30K, RefCOCO+, SNLI-VE \\
& \cellcolor{gray!15}AdvCLIP~\cite{zhou2023advclip}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}White-box
  & \cellcolor{gray!15}Invisible
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}STL10, GTSRB, CIFAR10, ImageNet, Wikipedia, Pascal-Sentence, NUS-WIDE, XmediaNet \\
& Typographical Attacks~\cite{noever2021reading}
  & 2021 & White-box
  & Visible
  & CLIP
  & ImageNet \\
& \cellcolor{gray!15}SGA~\cite{lu2023set}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}Black-box
  & \cellcolor{gray!15}Sample-wise
  & \cellcolor{gray!15}ALBEF, TCL, CLIP
  & \cellcolor{gray!15}Flickr30K, MS-COCO \\
& SA-Attack~\cite{he2023sa}
  & 2023 & Black-box
  & Sample-wise
  & ALBEF, TCL, CLIP
  & Flickr30K, MS-COCO \\
& \cellcolor{gray!15}VLP-Attack~\cite{wang2023exploring}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}Black-box
  & \cellcolor{gray!15}Sample-wise
  & \cellcolor{gray!15}ALBEF, TCL, BLIP, BLIP2, MiniGPT-4
  & \cellcolor{gray!15}MS-COCO, Flickr30K, SNLI-VE \\
& TMM~\cite{wang2024transferable}
  & 2024 & Black-box
  & Sample-wise
  & ALBEF, TCL, X\_VLM, CLIP, BLIP, ViLT, METER
  & MS-COCO, Flickr30K, RefCOCO+, SNLI-VE \\
& \cellcolor{gray!15}VLATTACK~\cite{yin2024vlattack}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}Black-box
  & \cellcolor{gray!15}Sample-wise
  & \cellcolor{gray!15}BLIP, ViLT, CLIP
  & \cellcolor{gray!15}MS-COCO, VQA v2, NLVR2, SNLI-VE, ImageNet, SVHN \\
& PRM~\cite{hu2024firm}
  & 2024 & Black-box
  & Sample-wise
  & CLIP, Detic, VL-PLM, FC-CLIP, OpenFlamingo, LLaVA
  & PASCAL Context, COCO-Stuff, OV-COCO, MS-COCO, OK-VQA \\
& \cellcolor{gray!15}C-PGC~\cite{fang2024one}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Black-box
  & \cellcolor{gray!15}Universal
  & \cellcolor{gray!15}ALBEF, TCL, X-VLM, CLIP, BLIP
  & \cellcolor{gray!15}Flickr30K, MS-COCO, SNLI-VE, RefCOCO+ \\
& ETU~\cite{zhang2024universal}
  & 2024 & Black-box
  & Universal
  & ALBEF, TCL, CLIP, BLIP
  & Flickr30K, MS-COCO \\
\hline
\multirow{16}{0.12\textwidth}{Adversarial Defense} 
& Defense-Prefix~\cite{azuma2023defense}
  & 2023 & Adversarial Tuning
  & Prompt Tuning
  & CLIP
  & ImageNet \\
& \cellcolor{gray!15}AdvPT~\cite{zhang2023adversarial}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}Adversarial Tuning
  & \cellcolor{gray!15}Prompt Tuning
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}ImageNet, Pets, Flowers, Food101, SUN397, DTD, EuroSAT, UCF101, ImageNet-V2, ImageNet-Sketch, ImageNet-A, ImageNet-R \\
& APT~\cite{li2024one}
  & 2024 & Adversarial Tuning
  & Prompt Tuning
  & CLIP
  & ImageNet, Caltech101, Pets, StanfordCars, Flowers, Food101, FGVCAircraft, SUN397, DTD, EuroSAT, UCF101, ImageNet-V2, ImageNet-Sketch, ImageNet-R, ObjectNet \\
& \cellcolor{gray!15}MixPrompt~\cite{fan2024mixprompt}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Adversarial Tuning
  & \cellcolor{gray!15}Prompt Tuning
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}ImageNet, Pets, Flowers, DTD, EuroSAT, UCF101, SUN397, Food101, ImageNet-V2, ImageNet-Sketch, ImageNet-A, ImageNet-R \\
& PromptSmoot~\cite{hussein2024promptsmooth}
  & 2024 & Adversarial Tuning
  & Prompt Tuning
  & PLIP, Quilt, MedCLIP
  & KatherColon, PanNuke, SkinCancer, SICAP v2 \\
& \cellcolor{gray!15}FAP~\cite{zhou2024few}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Adversarial Tuning
  & \cellcolor{gray!15}Prompt Tuning
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}ImageNet, Caltech101, Pets, StanfordCars, Flowers, Food101, FGVCAircraft, SUN397, DTD, EuroSAT, UCF101 \\
& APD~\cite{luo2024apd}
  & 2024 & Adversarial Tuning
  & Prompt Tuning
  & CLIP
  & ImageNet, Caltech101, Flowers, Food101, SUN397, DTD, EuroSAT, UCF101 \\
& \cellcolor{gray!15}TAPT~\cite{wang2024tapt}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Adversarial Tuning
  & \cellcolor{gray!15}Prompt Tuning
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}ImageNet, Caltech101, Pets, StanfordCars, Flowers, Food101, FGVCAircraft, SUN397, DTD, EuroSAT, UCF101 \\
& TeCoA~\cite{mao2023understanding}
  & 2022 & Adversarial Tuning
  & Contrastive Tuning
  & CLIP
  & CIFAR10, CIFAR100, STL10, Caltech101, Caltech256, Pets, StanfordCars, Food101, Flowers, FGVCAircraft, SUN397, DTD, PCAM, HatefulMemes, EuroSAT \\
& \cellcolor{gray!15}PMG-AFT~\cite{wang2024pre}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Adversarial Tuning
  & \cellcolor{gray!15}Contrastive Tuning
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}CIFAR10, CIFAR100, STL10, ImageNet, Caltech101, Caltech256, Pets, Flowers, FGVCAircraft, StanfordCars, SUN397, Food101, EuroSAT, DTD, PCAM \\
& MMCoA~\cite{zhou2024revisiting}
  & 2024 & Adversarial Tuning
  & Contrastive Tuning
  & CLIP
  & CIFAR10, CIFAR100, TinyImageNet, STL10, Caltech101, Caltech256, Pets, Flowers, FGVCAircraft, Food101, EuroSAT, DTD, SUN397, Country211 \\
& \cellcolor{gray!15}FARE~\cite{schlarmannrobust}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Adversarial Tuning
  & \cellcolor{gray!15}Contrastive Tuning
  & \cellcolor{gray!15}OpenFlamingo, LLaVA
  & \cellcolor{gray!15}COCO, Flickr30k, TextVQA, VQA v2, CalTech101, StanfordCars, CIFAR10, CIFAR100, DTD, EuroSAT, FGVCAircrafts, Flowers, ImageNet-R, ImageNet-Sketch, PCAM, Pets, STL10, ImageNet \\
& VILLA~\cite{gan2020large}
  & 2020 & Adversarial Training
  & Two-stage Training
  & UNITER, LXMERT
  & MS-COCO, Visual Genome, Conceptual Captions, SBU Captions
    ImageNet, LAION, DataComp \\
& \cellcolor{gray!15}AdvXL~\cite{wang2024revisiting}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Adversarial Training
  & \cellcolor{gray!15}Two-stage Training
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}ImageNet, LAION, DataComp \\
& MirrorCheck~\cite{fares2024mirrorcheck}
  & 2024 & Adversarial Detection
  & One-shot Detection
  & UniDiffuser, BLIP, Img2Prompt, BLIP-2, MiniGPT-4
  & MS-COCO, CIFAR10, ImageNet \\
& \cellcolor{gray!15}AdvQDet~\cite{wang2024advqdet}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Adversarial Detection
  & \cellcolor{gray!15}Stateful Detection
  & \cellcolor{gray!15}CLIP, ViT, ResNet
  & \cellcolor{gray!15}CIFAR10, GTSRB, ImageNet, Flowers, Pets \\
\hline

\multirow{6}{0.1\textwidth}{Backdoor \& Poisoning Attack}
& PBCL~\cite{carlini2022poisoning}
  & 2021 & Backdoor\&Poisoning
  & Visual Trigger
  & CLIP
  & Conceptual Captions, YFCC \\
& \cellcolor{gray!15}BadEncoder~\cite{jia2022badencoder}
  & \cellcolor{gray!15}2021
  & \cellcolor{gray!15}Backdoor
  & \cellcolor{gray!15}Visual Trigger
  & \cellcolor{gray!15}ResNet(SimCLR), CLIP
  & \cellcolor{gray!15}CIFAR10, STL10, GTSRB, SVHN, Food101 \\
& CorruptEncoder~\cite{zhang2024data}
  & 2022 & Backdoor
  & Visual Trigger
  & ResNet(SimCLR)
  & ImageNet, Pets, Flowers \\
& \cellcolor{gray!15}BadCLIP~\cite{liang2024badclip}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}Backdoor
  & \cellcolor{gray!15}Visual Trigger
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}Conceptual Captions \\
& BadCLIP~\cite{bai2024badclip}
  & 2023 & Backdoor
  & Multi-modal Trigger
  & CLIP
  & ImageNet, Caltech101, Pets, StanfordCars, Flowers, Food101, FGVCAircraft, SUN397, DTD, EuroSAT, UCF101 \\
& \cellcolor{gray!15}MM Poison~\cite{yang2023data}
  & \cellcolor{gray!15}2022
  & \cellcolor{gray!15}Poisoning
  & \cellcolor{gray!15}Multi-modal Poisoning
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}Flickr-PASCAL, MS-COCO \\
\hline

\multirow{7}{0.1\textwidth}{Backdoor \& Poisoning Defense}
& CleanCLIP~\cite{bansal2023cleanclip}
  & 2023 & Backdoor Removal
  & Fine-tuning
  & CLIP
  & Conceptual Captions, ImageNet \\
& \cellcolor{gray!15}SAFECLIP~\cite{yang2023better}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}Backdoor Removal
  & \cellcolor{gray!15}Fine-tuning
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}Conceptual Captions, Visual Genome, MS-COCO, Flowers, Food101, ImageNet, Pets, StanfordCars, Caltech101, CIFAR10, CIFAR100, DTD, FGVCAircraft \\
& RoCLIP~\cite{yang2024robust}
  & 2023 & Robust Training
  & Pre-training
  & CLIP
  & Conceptual Captions, Flowers, Food101, ImageNet, Pets, StanfordCars, Caltech101, CIFAR10, CIFAR100, DTD, FGVCAircraft \\
& \cellcolor{gray!15}DECREE~\cite{feng2023detecting}
  & \cellcolor{gray!15}2023
  & \cellcolor{gray!15}Backdoor Detection
  & \cellcolor{gray!15}Backdoor Model Detection
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}CIFAR10, GTSRB, SVHN, STL-10, ImageNet \\
& TIJO~\cite{sur2023tijo}
  & 2023 & Backdoor Detection
  & Trigger Inversion
  & BUTD, MFB, BAN, MCAN, NAS
  & TrojVQA \\
& \cellcolor{gray!15}Mudjacking~\cite{liu2024mudjacking}
  & \cellcolor{gray!15}2024
  & \cellcolor{gray!15}Backdoor Detection
  & \cellcolor{gray!15}Trigger Inversion
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}Conceptual Captions, CIFAR10, STL10, ImageNet, SVHN, Pets, Wiki103-Sub, SST-2, HOSL \\
& SEER~\cite{zhu2024seer}
  & 2024 & Backdoor Detection
  & Backdoor Sample Detection
  & CLIP
  & MSCOCO, Flickr, STL10, Pet, ImageNet \\
& \cellcolor{gray!15}Outlier Detection~\cite{huang2025detecting}
  &\cellcolor{gray!15}2025 &\cellcolor{gray!15} \cellcolor{gray!15}Backdoor Detection
  & \cellcolor{gray!15}Backdoor Sample Detection
  & \cellcolor{gray!15}CLIP
  & \cellcolor{gray!15}Conceptual Captions, ImageNet, RedCaps \\
\hline
\end{tabular}
}
\end{table*}

% \vspace{-2mm}
\section{Vision-Language Pre-training Model Safety} \label{sec:vlp}
VLP models, such as CLIP~\cite{radford2021learning}, ALBEF~\cite{li2021align}, and TCL~\cite{yang2022vision}, have made significant strides in aligning visual and textual modalities. However, these models remain vulnerable to various safety threats, which have garnered increasing research attention. This section reviews the current safety research on VLP models, with a focus on adversarial, backdoor, and poisoning research. The representative methods reviewed in this section are summarized in Table \ref{tab:vlp_safety}.


\subsection{Adversarial Attacks}
\label{sec:vlp-attack}
Since VLP models are widely used as backbones for fine-tuning downstream models, adversarial attacks on VLP aim to generate examples that cause incorrect predictions across various downstream tasks, including zero-shot image classification, image-text retrieval, visual entailment, and visual grounding. Similar to Section~\ref{sec:vfm}, these attacks can roughly be categorized into \textbf{white-box attacks} and \textbf{black-box attacks}, based on their threat models.


\subsubsection{White-box Attacks}
White-box adversarial attacks on VLP models can be further categorized based on perturbation types into \textbf{invisible perturbations} and \textbf{visible perturbations}, with the majority of existing attacks employing invisible perturbations.



\textbf{Invisible Perturbations} involve small, imperceptible adversarial changes to inputs—whether text or images—to maintain the stealthiness of attacks. Early research in the vision and language domains primarily adopts this approach~\cite{xu2018fooling, shah2019cycle, li2020bert, yang2021defending}, in which invisible attacks are developed independently.
In the context of VLP models, which integrate both modalities, \textbf{Co-Attack}~\cite{zhang2022towards} was the first to propose perturbing both visual and textual inputs simultaneously to create stronger attacks. Building on this, \textbf{AdvCLIP}~\cite{zhou2023advclip} explores universal adversarial perturbations that can deceive all downstream tasks.

\textbf{Visible Perturbations} involve more substantial and noticeable alterations. For example, manually crafted typographical, conceptual, and iconographic images have been used to demonstrate that the CLIP model tends to ``read first, look later"~\cite{noever2021reading}, highlighting a unique characteristic of VLP models. This behavior introduces new attack surfaces for VLP, enabling the development of more sophisticated attacks.


\subsubsection{Black-box Attacks}

Black-box attacks on VLP primarily adopt a transfer-based approach, with query-based attacks rarely explored. Existing methods can be categorized into: 1) \textbf{sample-specific perturbations}, tailored to individual samples, and 2) \textbf{universal perturbations}, applicable across multiple samples.

\textbf{Sample-wise perturbations} are generally more effective than universal perturbations, but their transferability is often limited.
\textbf{SGA}\cite{lu2023set} explores adversarial transferability in VLP by leveraging cross-modal interactions and alignment-preserving augmentation. Building on this, \textbf{SA-Attack}\cite{he2023sa} enhances cross-modal transferability by introducing data augmentations to both original and adversarial inputs. \textbf{VLP-Attack}\cite{wang2023exploring} improves transferability by generating adversarial texts and images using contrastive loss. To overcome SGA's limitations, \textbf{TMM}\cite{wang2024transferable} introduces modality-consistency and discrepancy features through attention-based and orthogonal-guided perturbations. \textbf{VLATTACK}\cite{yin2024vlattack} further enhances adversarial examples by combining image and text perturbations at both single-modal and multimodal levels. \textbf{PRM}\cite{hu2024firm} targets vulnerabilities in downstream models using foundation models like CLIP, enabling transferable attacks across tasks like object detection and image captioning.

\textbf{Universal Perturbations} are less effective than sample-wise perturbations but more transferable.
\textbf{C-PGC}~\cite{fang2024one} was the first to investigate universal adversarial perturbations (UAPs) for VLP models. It employs contrastive learning and cross-modal information to disrupt the alignment of image-text embeddings, achieving stronger attacks in both white-box and black-box scenarios.
\textbf{ETU}~\cite{zhang2024universal} builds on this by generating UAPs that transfer across multiple VLP models and tasks. ETU enhances UAP transferability and effectiveness through improved global and local optimization techniques. It also introduces a data augmentation strategy \textbf{ScMix} that combines self-mix and cross-mix operations to increase data diversity while preserving semantic integrity, further boosting the robustness and applicability of UAPs.


\subsection{Adversarial Defenses}
\label{sec:vlp-defenses}
Existing adversarial defenses for VLP models can be grouped into four types: 1) \textbf{adversarial example detection}, 2) \textbf{standard adversarial training}, 3) \textbf{adversarial prompt tuning}, and 4) \textbf{adversarial contrastive tuning}. While adversarial detection filters out potential adversarial examples before or during inference, the other three defenses follow similar adversarial training paradigms, with variations in efficiency.


\subsubsection{Adversarial Example Detection}
Adversarial detection methods for VLP can be further divided into \textbf{one-shot detection} and \textbf{stateful detection}.

\paragraph{One-shot Detection} 
One-shot Detection distinguishes adversarial from clean examples in a single forward pass. White-box detection methods are typically one-shot. For example, \textbf{MirrorCheck}~\cite{fares2024mirrorcheck} is a model-agnostic method for VLP models. It uses text-to-image (T2I) models to generate images from captions produced by the victim model, comparing the similarity between the input image and the generated image using CLIP’s image encoder. A significant similarity difference flags the input as adversarial.


\paragraph{Stateful Detection} Stateful Detection is designed for black-box query attacks, where multiple queries are tracked to detect adversarial behavior. \textbf{AdvQDet}~\cite{wang2024advqdet} is a novel framework that counters query-based black-box attacks. It uses adversarial contrastive prompt tuning (ACPT) to tune CLIP image encoder, enabling detection of adversarial queries within just three queries.


\subsubsection{Standard Adversarial Training}
Adversarial training is widely regarded as the most effective defense against adversarial attacks \cite{madry2017towards,croce2020reliable}. However, it is computationally expensive, and for VLP models, which are typically trained on web-scale datasets, this cost becomes prohibitively high, posing a significant challenge for traditional approaches. Although research in this area is limited, we highlight two notable works that have explored adversarial training for vision-language pre-training. Their pre-trained models can be used as robust backbones for other adversarial research.

The first work, \textbf{VILLA}~\cite{gan2020large}, is a vision-language adversarial training framework consisting of two stages: task-agnostic adversarial pre-training and task-specific fine-tuning. VILLA enhances performance across downstream tasks using adversarial pre-training in the embedding space of both image and text modalities, instead of pixel or token levels. It employs FreeLB’s strategy~\cite{zhu2019freelb} to minimize computational overhead for efficient large-scale training.

The second work, \textbf{AdvXL}~\cite{wang2024revisiting}, is a large-scale adversarial training framework with two phases: a lightweight pre-training phase using low-resolution images and weaker attacks, followed by an intensive fine-tuning phase with full-resolution images and stronger attacks. This coarse-to-fine, weak-to-strong strategy reduces training costs while enabling scalable adversarial training for large vision models.


\subsubsection{Adversarial Prompt Tuning}
Adversarial prompt tuning (APT) enhances the adversarial robustness of VLP models by incorporating adversarial training during prompt tuning~\cite{zhou2022conditional, zhou2022learning, khattak2023maple}, typically focusing on textual prompts. It offers a lightweight alternative to standard adversarial training. APT methods can be classified into two main categories based on the prompt type: \emph{textual prompt tuning} and \emph{multi-modal prompt tuning}.

\paragraph{Textual Prompt Tuning}
Textual prompt tuning (TPT) robustifies VLP models by fine-tuning learnable text prompts. \textbf{AdvPT}~\cite{zhang2023adversarial} enhances the adversarial robustness of CLIP image encoder by realigning adversarial image embeddings with clean text embeddings using learnable textual prompts. Similarly, \textbf{APT}~\cite{li2024one} learns robust text prompts, using a CLIP image encoder to boost accuracy and robustness with minimal computational cost. \textbf{MixPrompt}~\cite{fan2024mixprompt} simultaneously enhances the generalizability and adversarial robustness of VLPs by employing conditional APT. Unlike empirical defenses, \textbf{PromptSmooth}~\cite{hussein2024promptsmooth} offers a certified defense for Medical VLMs, adapting pre-trained models to Gaussian noise without retraining. Additionally, \textbf{Defense-Prefix}~\cite{azuma2023defense} mitigates typographic attacks by adding a prefix token to class names, improving robustness without retraining.


\paragraph{Multi-Modal Prompt Tuning}
Recent adversarial prompt tuning methods have expanded textual prompts to multi-modal prompts. \textbf{FAP}~\cite{zhou2024few} introduces learnable adversarial text supervision and a training objective that balances cross-modal consistency while differentiating uni-modal representations. \textbf{APD}~\cite{luo2024apd} improves CLIP’s robustness through online prompt distillation between teacher and student multi-modal prompts. Additionally, \textbf{TAPT}~\cite{wang2024tapt} presents a test-time defense that learns defensive bimodal prompts to improve CLIP's zero-shot inference robustness.


\subsubsection{Adversarial Contrastive Tuning}
Adversarial contrastive tuning involves contrastive learning with adversarial training to fine-tune a robust CLIP image encoder for \emph{zero-shot adversarial robustness} on downstream tasks. These methods are categorized into \textbf{supervised} and \textbf{unsupervised} methods, depending on the availability of labeled data during training.


\paragraph{Supervised Contrastive Tuning}
\textbf{Visual Tuning} fine-tunes CLIP image encoder using only adversarial images. \textbf{TeCoA}~\cite{mao2023understanding} explores the zero-shot adversarial robustness of CLIP and finds that visual prompt tuning is more effective without text guidance, while fine-tuning performs better with text information. \textbf{PMG-AFT}~\cite{wang2024pre} improves zero-shot adversarial robustness by introducing an auxiliary branch to minimize the distance between adversarial outputs in the target and pre-trained models, mitigating overfitting and preserving generalization.

\textbf{Multi-modal Tuning} fine-tunes CLIP image encoder using both adversarial texts and images.
\textbf{MMCoA}~\cite{zhou2024revisiting} combines image-based PGD and text-based BERT-Attack in a multi-modal contrastive adversarial training framework. It uses two contrastive losses to align clean and adversarial image and text features, improving robustness against both image-only and multi-modal attacks.

\paragraph{Unsupervised Contrastive Tuning}
Adversarial contrastive tuning can also be performed in an unsupervised fashion. For instance, \textbf{FARE}~\cite{schlarmannrobust} robustifies CLIP image encoder through unsupervised adversarial fine-tuning, achieving superior clean accuracy and robustness across downstream tasks, including zero-shot classification and vision-language tasks. This approach enables VLMs, such as LLaVA and OpenFlamingo, to attain robustness without the need for re-training or additional fine-tuning.


\subsection{Backdoor \& Poisoning Attacks}
\label{sec:vlp-bp-attacks}
Backdoor and poisoning attacks on CLIP can target either the pre-training stage or the fine-tuning stage on downstream tasks. Previous studies have shown that poisoning backdoor attacks on CLIP can succeed with significantly lower poisoning rates compared to traditional supervised learning~\cite{carlini2022poisoning}. Additionally, training CLIP on web-crawled data increases its vulnerability to backdoor attacks~\cite{carlini2024poisoning}. This section reviews proposed attacks targeting backdooring or poisoning CLIP.


\subsubsection{Backdoor Attacks}

Based on the trigger modality, existing backdoor attacks on CLIP can be categorized into \textbf{visual triggers} and \textbf{multi-modal triggers}.

\textbf{Visual Triggers} target pre-trained image encoders by embedding backdoor patterns in visual inputs.
\textbf{BadEncoder}~\cite{jia2022badencoder} explores image backdoor attacks on self-supervised learning by injecting backdoors into pre-trained image encoders, compromising downstream classifiers. \textbf{CorruptEncoder}~\cite{zhang2024data} exploits random cropping in contrastive learning to inject backdoors into pre-trained image encoders, with increased effectiveness when cropped views contain only the reference object or the trigger.
For attacks targeting CLIP, \textbf{BadCLIP}~\cite{liang2024badclip} optimizes visual trigger patterns using dual-embedding guidance, aligning them with both the target text and specific visual features. This strategy enables BadCLIP to bypass backdoor detection and fine-tuning defenses.

\textbf{Multi-modal Triggers} combine both visual and textual triggers to enhance the attack. BadCLIP~\cite{bai2024badclip} introduces a novel trigger-aware prompt learning-based backdoor attack targeting CLIP models. Rather than fine-tuning the entire model, BadCLIP injects learnable triggers during the prompt learning stage, affecting both the image and text encoders.

\subsubsection{Poisoning Attacks}
Two targeted poisoning attacks on CLIP are \textbf{PBCL}~\cite{carlini2022poisoning} and \textbf{MM Poison}~\cite{yang2023data}. PBCL demonstrated that a targeted poisoning attack, misclassifying a specific sample, can be achieved by poisoning as little as 0.0001\% of the training dataset. MM Poison investigates modality vulnerabilities and proposes three attack types: single target image, single target label, and multiple target labels. Evaluations show high attack success rates while maintaining clean data performance across both visual and textual modalities.

\subsection{Backdoor \& Poisoning Defenses}
\label{sec:vlp-bp-defenses}

Defense strategies against backdoor and poisoning attacks are generally categorized into \textbf{robust training} and \textbf{backdoor detection}. Robust Training aims to create VLP models resistant to backdoor or targeted poisoning attacks, even when trained on untrusted datasets. This approach specifically addresses poisoning-based attacks. Backdoor detection focuses on identifying compromised encoders or contaminated data. Detection methods often require additional mitigation techniques to fully eliminate backdoor effects.


\subsubsection{Robust Training}
Depending on the stage at which the model gains robustness against backdoor attacks, existing robust training strategies can be categorized into fine-tuning and pre-training approaches.

\paragraph{Fine-tuning Stage}
To mitigate backdoor and poisoning threats, \textbf{CleanCLIP}~\cite{bansal2023cleanclip} fine-tunes CLIP by re-aligning each modality's representations, weakening spurious correlations from backdoor attacks. Similarly, \textbf{SAFECLIP}~\cite{yang2023better} enhances feature alignment using unimodal contrastive learning. It first warms up the image and text modalities separately, then uses a Gaussian mixture model to classify data into safe and risky sets. During pre-training, SAFECLIP optimizes CLIP loss on the safe set, while separately fine-tuning the risky set, reducing poisoned image-text pair similarity and defending against targeted poisoning and backdoor attacks.

\paragraph{Pre-training Stage}
\textbf{ROCLIP}~\cite{yang2024robust} defends against poisoning and backdoor attacks by enhancing model robustness during pre-training. It disrupts the association between poisoned image-caption pairs by utilizing a large, diverse pool of random captions. Additionally, ROCLIP applies image and text augmentations to further strengthen its defense and improve model performance.


\subsubsection{Backdoor Detection}
Backdoor detection can be broadly divided into three subtasks: 1) \textbf{trigger inversion}, 2) \textbf{backdoor sample detection}, and 3) \textbf{backdoor model detection}. Trigger inversion is particularly useful, as recovering the trigger can aid in the detection of both backdoor samples and backdoored models.

\textbf{Trigger Inversion} aims to reverse-engineer the trigger pattern injected into a backdoored model. \textbf{Mudjacking}~\cite{liu2024mudjacking} mitigates backdoor vulnerabilities in VLP models by adjusting model parameters to remove the backdoor when a misclassified trigger-embedded input is detected. In contrast to single-modality defenses, \textbf{TIJO}~\cite{sur2023tijo} defends against dual-key backdoor attacks by jointly optimizing the reverse-engineered triggers in both the image and text modalities.

\textbf{Backdoor Sample Detection} detects whether a training or test sample is poisoned by a backdoor trigger. This detection can be used to cleanse the training dataset or reject backdoor queries. \textbf{SEER}~\cite{zhu2024seer} addresses the complexity of multi-modal models by jointly detecting malicious image triggers and target texts in the shared feature space. This method does not require access to the training data or knowledge of downstream tasks, making it highly effective for backdoor detection in VLP models. \textbf{Outlier Detection}~\cite{huang2025detecting} demonstrates that the local neighborhood of backdoor samples is significantly sparser compared to that of clean samples. This insight enables the effective and efficient application of various local outlier detection methods to identify backdoor samples from web-scale datasets. Furthermore, they reveal that potential unintentional backdoor samples already exist in the Conceptual Captions 3 Million (CC3M) dataset and have been trained into open-sourced CLIP encoders.

\textbf{Backdoor Model Detection} identifies whether a trained model is compromised by backdoor(s). \textbf{DECREE}~\cite{feng2023detecting} introduces a backdoor detection method specifically for VLP encoders that require no labeled data. It exploits the distinct embedding space characteristics of backdoored encoders when exposed to clean versus backdoor inputs. By combining trigger inversion with these embedding differences, DECREE can effectively detect backdoored encoders.



\subsection{Datasets}
\label{sec:vlp-data}
This section reviews datasets used for VLP safety research. As shown in Table \ref{tab:vlp_safety}, a variety of benchmark datasets were employed to evaluate adversarial attacks and defenses for VLP models. For image classification tasks, commonly used datasets include: ImageNet~\cite{russakovsky2015imagenet}, Caltech101~\cite{fei2004learning}, DTD~\cite{cimpoi2014describing}, EuroSAT~\cite{helber2019eurosat}, OxfordPets~\cite{parkhi2012cats}, FGVC-Aircraft~\cite{maji2013fine}, Food101~\cite{bossard2014food}, Flowers102~\cite{nilsback2008automated}, StanfordCars~\cite{krause20133d}, SUN397~\cite{xiao2010sun}, and UCF101~\cite{soomro2012ucf101}. 
For evaluating domain generalization and robustness to distribution shifts, several ImageNet variants were also used: ImageNetV2~\cite{recht2019imagenet}, ImageNet-Sketch~\cite{wang2019learning}, ImageNet-A~\cite{hendrycks2021natural}, and ImageNet-R~\cite{hendrycks2021many}. Additionally, MS-COCO~\cite{lin2014microsoft} and Flickr30K~\cite{plummer2015flickr30k} were utilized for image-to-text and text-to-image retrieval tasks, RefCOCO+\cite{yu2016modeling} for visual grounding, and SNLI-VE\cite{xie2019visual} for visual entailment.




