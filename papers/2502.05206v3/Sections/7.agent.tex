\begin{table*}[htp]
\center
\caption{A summary of attacks and defenses for Agents.}
\label{tab:agent_safety}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllll}
\hline
\rowcolor{wangyixu-purple}
Primary & Method & Year & Category & Target Model \\ 
\hline
\aboverulesepcolor{orange!35!}  \midrule
\belowrulesepcolor{gray!35!}
    \rowcolor{gray!35!}\multicolumn{5}{c}{\textbf{LLM Agent}} \\
\aboverulesepcolor{gray!35!}  \midrule
\multirow{8}{0.07\textwidth}{Attacks} & \cellcolor{gray!15!}InjecAgent~\cite{zhan2024injecagent} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Prompt Injection & \cellcolor{gray!15!}Qwen-1.8B, 72B, Mistral-7B, 8x7B, OpenOrca-Mistral, OpenHermes-Mistral, \\
& \cellcolor{gray!15!} & \cellcolor{gray!15!} & \cellcolor{gray!15!} & \cellcolor{gray!15!}Nous-Mixtral, MythoMax-13B, WizardLM-13B, Platypus2-70B, Capybara-7B, \\
& \cellcolor{gray!15!} & \cellcolor{gray!15!} & \cellcolor{gray!15!} & \cellcolor{gray!15!}Nous-LLaMA-2-13B, LLaMA-2-70B, Claude-2, GPT-3.5, GPT-4 \\
& Breaking Agents ~\cite{zhang2024breaking} & 2024 & Prompt Injection & GPT-3.5, GPT-4, Claude-2 \\
& \cellcolor{gray!15!}BadAgent~\cite{wang2024badagent} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Backdoor Attack & \cellcolor{gray!15!}ChatGLM-3-6B, AgentLM-7B, 13B \\
& AgentPoison~\cite{chen2024agentpoison} & 2024 & Backdoor Attack & GPT-3.5, LLaMA-3 \\
& \cellcolor{gray!15!}Contextual Backdoor~\cite{liu2024compromising} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Backdoor Attack & \cellcolor{gray!15!}GPT-3.5, text-davinci-002, Gemini \\
& PsySafe ~\cite{zhang2024psysafe} & 2024 & Jailbeak Attacks & Camel, AutoGen, MetaGPT, AutoGPT \\
\hline
\multirow{3}{0.07\textwidth}{Defenses} & \cellcolor{gray!15!}
TrustAgent ~\cite{hua2024trustagent} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Response filtering & \cellcolor{gray!15!}GPT-4, GPT-3.5, Claude-2, Claude-1.2, Mixtral \\
& Autodefense ~\cite{zeng2024autodefense} & 2024 & Response filtering & GPT-3.5, Vicuna-13B, LLaMA-2-70B, Mixtral-8x7B \\
& \cellcolor{gray!15!}GuardAgent~\cite{xiang2024guardagent} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Knowledge-enabled reasoning  & \cellcolor{gray!15!}EHRAgent, SeeAct \\
\hline
\multirow{4}{0.07\textwidth}{Benchmarks} 
& R-Judge ~\cite{yuan2024r} & 2024 & Benchmarks & GPT-3.5, GPT-4o, LLaMA-3-8B, LLaMA-2-7B, 13B, Vicuna-7B, 13B, Mistral-7B \\
& \cellcolor{gray!15!}AgentDojo ~\cite{debenedetti2024agentdojo} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Benchmarks & \cellcolor{gray!15!}Gemini-1.5-Flash, Gemini-Pro, Claude-3-Sonnet, Claude-3-Opus, \\
 & \cellcolor{gray!15!} & \cellcolor{gray!15!} & \cellcolor{gray!15!} & \cellcolor{gray!15!}Claude-3.5-Sonnet, GPT-3.5, GPT-4, GPT-4o, LLaMA-3-70B, Command R+  \\
& SafeAgentBench\cite{yin2024safeagentbench} & 2024 & Benchmarks & GPT-4, LLaMA-3-8B, Qwen-2-7B, DeepSeek-V2.5 \\
\hline
\aboverulesepcolor{gray!35!}  \midrule
\belowrulesepcolor{gray!35!}
    \rowcolor{gray!35!}\multicolumn{5}{c}{\textbf{VLM Agent}} \\
\aboverulesepcolor{gray!35!}  \midrule
\multirow{4}{0.07\textwidth}{Attacks} & \cellcolor{gray!15!}Fu et al.~\cite{fu2023misusing} & \cellcolor{gray!15!}2023 & \cellcolor{gray!15!}White-box Attacks & \cellcolor{gray!15!}LLaMA Adapter \\
& Tan et al.~\cite{tan2024wolf} & 2024 & White-box Attacks & LLaVA, PandaGPT \\
& \cellcolor{gray!15!}AgentSmith~\cite{gu2024agent} & \cellcolor{gray!15!}2024 & \cellcolor{gray!15!}Black-box Attacks &  \cellcolor{gray!15!}LLaVA-1.5-7B, 13B \\
& ARE~\cite{wu2024adversarial} & 2024 & Robustness Analysis & GPT-4V, Gemini-1.5-Pro, Claude-3-Opus, GPT-4o \\
\hline
\end{tabular}
}
\end{table*}

\section{Agent Safety} \label{sec:agent}
Large model powered agents are increasingly deployed across diverse applications, leveraging the capabilities of LLMs and VLMs to tackle complex problems, especially in safety-critical domains such as medical robotics and autonomous driving. Ensuring their safety is of paramount importance. This section provides a comprehensive review of existing safety research on agents, highlighting key challenges and emphasizing the need for ongoing innovation. Existing research can be broadly categorized into \textbf{LLM agent safety} and \textbf{VLM agent safety}.

\subsection{LLM Agent Safety}
\label{sec:agent_LLM}

This subsection reviews recent research on LLM agent safety from three key aspects: \emph{attack methodologies}, \emph{defense mechanisms}, and \emph{benchmarks}.

\subsubsection{Attacks}
Understanding and mitigating the vulnerabilities of LLM agents is crucial for their safe and trustworthy deployment, particularly as they manage sensitive data and perform real-world actions. Existing attacks on LLM agents fall into three main categories: \textbf{Prompt Injection Attacks}, \textbf{Backdoor Attacks}, and \textbf{Jailbreak Attacks}.

\textbf{Prompt Injection Attacks} manipulate an agent's behavior by embedding malicious instructions into its input prompts. For LLM agents, this often involves crafting prompts that exploit reasoning processes and interactions with external tools. A notable subtype is \textbf{Indirect Prompt Injection} (IPI), where malicious instructions are hidden in external content like web pages, emails, or documents retrieved by the agent. For example, \textbf{InjecAgent} \cite{zhan2024injecagent} demonstrates how an embedded command in a product review can trigger unintended actions when processed by the agent. These attacks are especially concerning as they exploit reliance on external information without requiring access to the agent's core system. \textbf{Breaking Agents} \cite{zhang2024breaking} further investigates vulnerabilities across agent components, showcasing the widespread applicability of prompt injection attacks across diverse architectures.

\textbf{Backdoor Attacks} introduce hidden triggers into an agent's model or knowledge base, causing it to execute malicious actions under specific conditions. For LLM agents, these attacks can target training data, fine-tuning datasets, or long-term memory and knowledge bases. \textbf{BadAgent} \cite{wang2024badagent} embeds backdoors by poisoning fine-tuning data. \textbf{Contextual Backdoor Attacks} \cite{liu2024compromising} poison benign-looking contextual demonstrations to trigger malicious behavior in specific scenarios. \textbf{AgentPoison} \cite{chen2024agentpoison} targets an agent's memory or knowledge base, ensuring malicious demonstrations are retrieved and executed under predefined triggers, even without further model training.

\textbf{Jailbreak Attacks} bypass an agent's safety mechanisms and ethical guidelines, prompting it to perform restricted actions. This is often done by exploiting loopholes in prompt interpretation or manipulating the agent's internal state. \textbf{PsySafe} \cite{zhang2024psysafe} examines psychological safety in multi-agent systems, showing how adversarial prompts can trigger dark personality traits, effectively overriding safety protocols and enabling harmful behaviors.

\subsubsection{Defenses}
Existing defense mechanisms for LLM agents can be categorized into \emph{response filtering} and \emph{knowledge-enabled reasoning}.

\textbf{Response Filtering} monitors and filters potentially harmful or undesirable outputs generated by LLM agents. \textbf{AutoDefense} \cite{zeng2024autodefense} uses a multi-agent framework where agents assume distinct defensive roles, collaboratively analyzing the target agent's responses. A consensus-based decision determines whether a response is allowed, enhancing robustness. 
\textbf{TrustAgent} \cite{hua2024trustagent} introduces an agent constitution with predefined safety rules, ensuring adherence during the planning phase. It employs a three-stage strategy: 1) \emph{pre-planning}, which injects safety knowledge before plan generation; 2) \emph{in-planning}, which enhances safety during generation; and 3) \emph{post-planning}, which inspects outputs before execution.

\textbf{Knowledge-Enabled Reasoning} enhances LLM agent safety by integrating external knowledge or structured reasoning processes. \textbf{GuardAgent} \cite{xiang2024guardagent} introduces a guard agent that oversees the target LLM agent. It generates an action plan based on guard requests, translates it into executable code using a knowledge base and function toolbox, and executes it to verify compliance with guard rules. This structured approach ensures more reliable and systematic defense.


\subsubsection{Benchmarks}
Existing benchmarks evaluate agents' ability to identify risks in interactive environments or defend against targeted attacks.  
\textbf{R-Judge} \cite{yuan2024r} assesses agents' risk awareness using 569 records across 27 scenarios and 10 risk types, testing their ability to identify safety risks in interaction logs.  
\textbf{SafeAgentBench} \cite{yin2024safeagentbench} evaluates embodied agents' safety awareness and planning skills with 750 tasks featuring varying hazards and abstraction levels in a simulated environment, measuring both task execution and semantic understanding. 
\textbf{AgentDojo} \cite{debenedetti2024agentdojo} tests agents' resilience to prompt injection attacks through 97 tasks and 629 security test cases, focusing on handling malicious instructions embedded in third-party data.  
These benchmarks offer valuable insights into LLM agent safety by assessing their risk identification and defense capabilities across diverse scenarios.


\subsection{VLM Agent Safety}
\label{sec:agent_VLM}

Involving both visual perception and language understanding, VLM agents face unique safety challenges. 
Current attacks on VLM agents mainly target their weaknesses through \textbf{white-box} and \textbf{black-box} attacks, as well as \textbf{robustness analysis}. Despite these attacks, effective defense strategies are still underdeveloped.

\subsubsection{Attacks}

\textbf{White-box Attacks} assume adversaries have full access to model parameters, enabling gradient-based optimization to expose theoretical vulnerabilities. Fu et al.~\cite{fu2023misusing} used gradient-based training  and characterization to craft adversarial images, causing LLMs to execute tool commands using real-world syntax, compromising user confidentiality and integrity. Tan et al.~\cite{tan2024wolf} demonstrated how an VLM agent can jailbreak another agent in a multi-agent society through malicious prompts generated with white-box access, enabling widespread harmful outputs.

\textbf{Black-box Attacks} operate without access to model internals, relying on input-output behavior to craft adversarial examples, making them highly relevant for real-world scenarios. \textbf{AgentSmith} \cite{gu2024agent} uses a single adversarial image to jailbreak multiple VLM agents, exploiting their interconnected nature to spread malicious behavior exponentially across the network.

\textbf{Robustness Analysis} examines how system components interact and how vulnerabilities propagate. \textbf{ARE} \cite{wu2024adversarial} present an Agent Robustness Evaluation framework that models VLM agents as graphs, analyzing adversarial influence flow between components to identify weak points and assess the safety impact of component modifications.
