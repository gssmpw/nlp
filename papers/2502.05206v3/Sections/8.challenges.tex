\section{Open Challenges} \label{sec:challenges}

Based on the survey, we identify several limitations and gaps in existing research and summarize them into the following topics. These open challenges reflect the evolving nature of large model safety, highlighting both technical and methodological barriers that must be overcome to ensure robustness and reliability across various AI systems.

\subsection{Fundamental Vulnerabilities}

Exploring and understanding the fundamental vulnerabilities of large models is essential for developing robust defenses and safety frameworks. This section highlights the core weaknesses and challenges inherent to different types of large models.

\subsubsection{The Purpose of Attack Is Not Just to Break the Model}
While much of the existing research focuses on designing attacks to disrupt or break a model's functionality, the true goal of attack research should extend beyond mere disruption. Attacks can serve as a diagnostic tool to uncover unintended behaviors and reveal fundamental weaknesses in a model's decision-making processes. By understanding how and why models fail, we can address vulnerabilities at their root rather than applying superficial fixes. For every new attack proposed, it is critical to ask: 
\textbf{Why does the attack succeed or fail? How does it exploit the model? What new vulnerabilities does it reveal that were previously unknown? Are these vulnerabilities inherent to the model architecture or class?} 
These questions guide the development of more robust models and defenses by exposing systemic flaws rather than isolated issues.


\subsubsection{What Are the Fundamental Vulnerabilities of Language Models?}

LLMs like ChatGPT and Gemini exhibit fundamental vulnerabilities due to their reliance on statistical patterns rather than true semantic understanding \cite{titus2024does}. Key weaknesses include susceptibility to adversarial inputs, biases in training data, and manipulation via prompt engineering. To build effective defenses, research must delve deeper into how these vulnerabilities arise from the model's architecture and training data. 

Critical areas of focus include: 1) \textbf{Memorization of training data}, which can lead to privacy breaches or unintended data leakage; 2) \textbf{Exposure to harmful content}, which can propagate biases or toxic outputs; 3) \textbf{Amplification of hallucinations}, where models generate plausible but incorrect or nonsensical information. Open research questions remain:
\textbf{Does the discrete nature of textual inputs make language models more or less robust compared to vision models? What fundamental vulnerabilities are exposed by jailbreak and data extraction attacks? }
Addressing these questions is vital for advancing the safety and reliability of LLMs and other large models.

\subsubsection{How Vulnerabilities Propagate Across Modalities?}
As Multi-modal Large Language Models (MLLMs) integrate diverse modalities, new vulnerabilities arise. Vision encoders are known to be sensitive to subtle, continuous perturbations in pixel space, while language models are vulnerable to adversarial characters, words, or prompts. However, the interaction between modalities and how vulnerabilities in one modality propagate to others remains poorly understood.

Key questions include: \textbf{How do vulnerabilities in one modality (e.g., vision) influence the behavior of another (e.g., language)? How does the number of tokens across modalities affect vulnerability propagation?}
Additionally, it is critical to explore \textbf{how to address multi-modal vulnerabilities within a unified framework}, moving beyond defenses tailored to individual modalities. This requires a holistic approach to identify and mitigate cross-modal risks, ensuring robust performance across all integrated modalities.


\subsubsection{Diffusion Models for Visual Content Generation Lack Language Capabilities}

Diffusion models for image or video generation excel in visual content creation but often lack language understanding capabilities, a limitation shared by VLP models. This is because these models are primarily optimized for pixel-level generation tasks, without incorporating language processing into their core architecture. As a result, they may generate harmful or contextually inappropriate content due to their inability to fully comprehend textual prompts.
To develop robust multi-modal systems, it is crucial to integrate language comprehension into these models. This would enable them to produce content that is not only visually coherent but also contextually aligned with the given textual input.

An open challenge is \textbf{bridging the gap between visual and linguistic capabilities in generative models to enhance multi-modal safety}. However, this integration may introduce new vulnerabilities, such as advanced attacks that exploit fine-grained manipulation of the generation process. Addressing these challenges represents a critical direction for future research.

\subsubsection{How Much Training Data Can a Model Memorize?}
The memorization capability of deep neural networks (DNNs) has raised significant concerns, particularly in enabling privacy attacks such as membership inference and data extraction (model inversion). Both LLMs and DMs have been shown to replicate and leak small portions of their training data under specific conditions. However, it remains unclear whether DNNs primarily learn through memorization and to what extent this occurs.
Due to the non-linear nature of large models, exact model inversion is inherently impossible. These models compress training data into multi-level representations, making it difficult to pinpoint when and how memorization happens. 

Key open questions include: \textbf{What mechanism acts as the memorization ``switch", triggering the model to output training data directly? How can memorization be effectively measured—through exact matches, training equivalence, or embedding similarity?}
Addressing these questions is crucial for understanding the trade-offs between model performance and privacy risks, as well as for developing strategies to mitigate unintended data leakage.

\subsubsection{Agent Vulnerabilities Grow with Their Abilities}
Large-model-powered agents face increasing vulnerabilities as their capabilities expand \cite{gu2024agent}. These agents interact with external tools, data sources, and environments, creating a broader attack surface that complicates defense mechanisms. A critical challenge is the compounding effect of vulnerabilities in foundational models when integrated into agents' decision-making processes. For instance, an agent relying on a language model vulnerable to jailbreak prompts and a vision model susceptible to adversarial inputs may experience cascading failures, leading to unpredictable outcomes.

Moreover, agents' ability to learn and adapt introduces additional risks. Even seemingly benign interactions can expose them to subtle biases or adversarial inputs, potentially triggering unsafe behaviors. The dynamic nature of agents—especially those that continuously learn or self-improve—further complicates vulnerability detection, as they may develop new weaknesses over time. This unpredictability makes traditional safety evaluations insufficient, as agents can evolve in ways that are difficult to anticipate.

To address these challenges, research must focus on: \textbf{understanding the interaction between model components} (e.g., language, vision, and decision-making) and \textbf{how vulnerabilities in one component can propagate to others}. 
It is also important to \textbf{develop new methodologies to evaluate agents in dynamic, evolving environments}, ensuring their robustness against emerging threats.
These efforts are essential for building safer and more reliable agent systems in the future.


\subsection{Safety Evaluation}

Comprehensive and standardized safety evaluations are critical for quantifying the safety levels of large models. However, existing evaluation datasets and benchmarks are often static or narrowly focused on specific threats. To ensure models perform reliably in real-world conditions, safety evaluations must test them across diverse and unpredictable scenarios. 
% Such evaluations are essential for responsible AI deployment and fostering public trust in large models.

\subsubsection{Attack Success Rate Is Not All We Need}

While the attack success rate (ASR) is a commonly used metric in safety research, it mainly quantifies how often an attack disrupts a model's output. However, this metric overlooks several important factors, such as the severity of the disruption, the model’s resilience to various types of attacks, and the real-world consequences of potential failures. A model could still cause harm or mislead decision-making even if its core functionality appears unaffected. For instance, an attack might subtly alter a model’s decision-making process without causing an obvious malfunction, but the resulting behavior could have catastrophic effects in real-world applications. Such vulnerabilities are often missed by traditional metrics like ASR or failure rate.

To better understand a model’s weaknesses—whether in its design, training data, or inference process—it is crucial to define multi-level, fine-grained vulnerability metrics. A more  comprehensive safety evaluation framework should consider factors such as the model’s susceptibility to different types of attacks, its ability to recover from malicious inputs, and the ethical implications of potential failure modes.


\subsubsection{Static Evaluations Create a False Sense of Safety}
Current safety evaluations rely heavily on static benchmarks or open-source datasets that have been available for some time. These datasets have already been exposed to model trainers and adversaries, which means a model may achieve high safety performance on these outdated datasets without necessarily being robust in real-world scenarios. As a result, static evaluations can create a false sense of safety. This underscores a significant limitation in the current evaluation framework: static benchmarks fail to capture the evolving nature of threats that models encounter in dynamic, real-world applications.

To address this challenge, safety evaluations must move beyond static assessments. A key \textbf{step is to develop evaluation datasets or benchmarks that evolve over time}, better reflecting the ever-changing landscape of safety threats. An example of an evolving evaluation system is the Chatbot Arena \cite{chiangchatbot}, which adapts as new threats and challenges emerge. Similar strategies could be applied to safety assessments in broader AI systems.
Additionally, future evaluation methods might consider releasing only the ``seeds” or structural components of datasets, along with a test case generation method, rather than providing static test cases. This approach would enable the continuous generation of new test cases that better reflect the evolving nature of threats.


\subsubsection{Adversarial Evaluations Are a Necessity, Not an Option}

While regular (non-adversarial) safety tests offer insights into a model’s general robustness, they fail to capture the full spectrum of safety risks that models face in real-world scenarios. These tests typically focus on overall performance but neglect how models respond to adversarial queries that exploit their vulnerabilities. In contrast, adversarial evaluations assess model performance under attack, providing a more accurate measure of safety in worst-case scenarios.

A key challenge in this area is \textbf{developing environments that simulate real-world attack conditions}. One promising approach is to frame safety evaluation as a two-player adversarial game, where reinforcement learning-based adversarial agents interact with target models to identify and exploit new vulnerabilities. This method offers a more dynamic and comprehensive way to assess model safety under attack.
Such adversarial evaluations are especially crucial for commercial APIs, which often use filtering mechanisms to block malicious inputs. These filters can render traditional safety benchmarks less effective, as they prevent the model from facing the full range of adversarial threats it might encounter in real-world applications.

\subsubsection{Open-Ended Evaluation}
Evaluating adversarial attacks in classification problems is relatively straightforward, as each input is typically associated with a distinct class label. However, large models often generate open-ended responses, which complicates the evaluation of attacks like jailbreaking (e.g., through ASR computation). The ideal evaluator for such models would function as a perfect jailbreaking detector. Yet, since achieving an ideal jailbreaking detector is not feasible, it follows that an ideal evaluator may also be unattainable.

Currently, evaluators are generally rule-based (e.g., keyword detection) or model-based (e.g., GPT or Llama-Guard). However, \textbf{developing more consistent and reliable evaluation methods and metrics remains an open challenge}. One potential solution is to constrain the output space to a finite set of actions, similar to the approach used in agent-based scenarios. This would limit the complexity of the evaluation and make it more feasible to assess safety in open-ended environments.



\subsection{Safety Defense}

Safety mechanisms in large models are crucial for preventing harmful or unintended behaviors. These mechanisms may involve modifications to the model's architecture or the integration of external monitoring systems. This section explores the open challenges in developing robust defense solutions.

\subsubsection{Safety Alignment Is Not the Savior}

Safety alignment—ensuring that a model's objectives align with human values—has been a promising approach to mitigating certain risks. However, recent studies have revealed a significant weakness in safety alignment: \textbf{fake alignment} \cite{wang2024fake,greenblatt2024alignment}, where a model may achieve high safety scores without truly understanding the underlying safety principles. This points to a deeper issue of \textbf{shallow safety}. Furthermore, even models that are considered well-aligned, such as GPT-4o \cite{gpt-4o} or o1 \cite{openai-o1}, remain vulnerable to sophisticated attacks that can bypass their alignment mechanisms \cite{ying2024unveiling}.

An open challenge is to \textbf{identify the mechanistic limitations of safety alignment} and develop methods that ensure robust safety, even in the face of unforeseen attacks. Recent research \cite{qi2024safety} emphasizes the \textbf{need to move beyond superficial safety alignment metrics} (such as the distribution of the first few output tokens), ensuring that alignment is deeper and more comprehensive.
Additionally, making \textbf{safety alignment adversarial}—by actively challenging a model’s safety mechanisms—may help address the issue of shallow alignment, leading to more resilient and trustworthy models.


\subsubsection{Jailbreak Attacks Are More Challenging to Defend Against Than Adversarial Attacks}
Jailbreak attacks and adversarial attacks share a common goal: both aim to manipulate a model into producing targeted outputs. However, the key distinction lies in the nature of these outputs. Jailbreak attacks specifically seek to induce harmful or toxic responses, which requires bypassing the model's safety mechanisms. In contrast, while adversarial attacks may also circumvent safety defenses, they are not inherently designed to produce malicious content.

A significant challenge in defending against jailbreak attacks, beyond the typical defenses against adversarial attacks, is the absence of constraints on the attack's perturbation budget. In adversarial attacks, perturbations are deliberately kept minimal to remain imperceptible to humans. Jailbreak attacks, however, are not constrained by such requirements, allowing for greater flexibility in crafting attacks. This lack of limitations makes jailbreak attacks more challenging to defend against compared to adversarial attacks. \textbf{Developing defense strategies that can effectively address both types of attacks} remains an open and pressing challenge.

\subsubsection{The Need for More Practical Defenses}

Existing defense methods face several limitations that hinder their effectiveness in real-world applications. These limitations include a lack of generality, low efficiency, reliance on white-box access, and poor adaptability. To be truly practical, a defense must possess certain desirable properties, the achievement of which remains an ongoing challenge.

\begin{enumerate}[label=\arabic*]
    \item \textbf{Generality:} \\With the wide variety of models deployed across different domains—such as vision, language, and multimodal systems—defenses should not be overly tailored to specific architectures. Instead, they should offer generalized solutions applicable to multiple model families. Generality ensures that a single defense mechanism can be deployed across a broad range of systems, making it scalable and efficient for real-world safety infrastructures.
    \item \textbf{Black-box Compatibility:} \\In many real-world scenarios, defenders may not have access to the internal parameters of the model they are protecting. Therefore, practical defenses must operate in a black-box setting, where defenders can only observe the model’s inputs and outputs. This requires defense strategies that function externally to detect and mitigate attacks without needing access to the model's inner workings.
    \item \textbf{Efficiency:} \\ Many defense techniques, particularly adversarial training, are computationally expensive. The need for large-scale retraining or fine-tuning can make these defenses prohibitively costly. Practical defenses must strike a balance between robustness and computational efficiency, ensuring that models remain safe without incurring excessive resource consumption.

    \item \textbf{Continual Adaptability:} \\ A A practical defense system should not only recognize previously encountered attacks but also adapt in real-time to new and evolving threats. This requires continual learning and the ability to update without relying on costly retraining cycles. Models must be capable of incorporating new data, evolving their defense strategies, and self-correcting as new attacks emerge.
\end{enumerate}

The ongoing challenge for researchers is to refine and integrate these properties into cohesive defense strategies that offer robust protection while maintaining model performance.


\subsubsection{The Lack of Proactive Defenses}

Most existing defense approaches, such as safety alignment and adversarial training, are passive in nature, focusing on fortifying models against potential attacks. However, \textbf{proactive defenses}, which actively counter potential attacks before they succeed, have received limited attention in the literature.
For example, a proactive defense against model extraction could involve poisoning or injecting backdoors into extraction attempts, rendering the extracted model unreliable. Another approach could be to provide deliberately nonsensical or easily detectable responses when a malicious user requests advice for illegal activities, such as planning a robbery.
Such proactive strategies could serve as powerful deterrents to potential attackers. However, designing effective proactive defenses for different types of safety threats remains an open challenge and a promising direction for future research.

\subsubsection{Detection Has Been Overlooked in Current Defenses}

Detection methods play a crucial role in identifying potential vulnerabilities and abnormal behaviors in models, effectively acting as active monitors. When integrated with other defense mechanisms, detection systems can trigger automatic safety responses whenever a model behaves unexpectedly or generates harmful outputs. Despite their importance, however, existing defense strategies have largely overlooked the integration of detection systems within their pipelines.
By combining detection with other safety measures, it becomes possible to develop more resilient models capable of dynamically responding to emerging threats. For example, stronger attacks may be more easily detected, providing an opportunity for proactive defense.
An open question remains: \textbf{What is the most effective way to integrate detection as a core component of a defense system, and how can detection and other defense mechanisms complement and enhance each other?}

\subsubsection{The Current Data Usage Practices Must Change}

The current data usage practices in the AI development lifecycle are neither sustainable nor ethical. We identify three major issues with how data is currently used:

\begin{enumerate}[label=\arabic*] 

\item \textbf{Lack of Consent and Recognition}: Many large models are trained on web-crawled data without the consent of data owners or formal recognition or reward for their contributions. This practice raises significant ethical and legal issues.

\item \textbf{The Explosion of Generated Data}: A large volume of generated data has been uploaded to the internet, much of which is fake, toxic, or otherwise harmful. However, there is no clear system in place to identify which model created this data or who was responsible for its generation.

\item \textbf{Depletion of ``Free'' Data}: The ``free'' data available on the internet is rapidly diminishing. Users are becoming less motivated or outright refusing to contribute valuable data, especially when their contributions are neither acknowledged nor rewarded.

\end{enumerate}

To address these challenges, the AI industry must establish a healthy and sustainable data ecosystem where data contributors are recognized and rewarded. Achieving this requires answering the following key questions:

\begin{itemize} 
\item \textbf{Who Used My Data?} This question addresses the protection of copyright for original training data, commonly known as \textbf{membership inference}. Membership inference techniques are essential to determine whether specific samples were included in a model's training data. Such methods would empower data owners to verify how their data is being used and protect their legitimate rights and interests.

\item \textbf{Who Generated the Data?} This question focuses on protecting the copyright of generated data, a process referred to as \textbf{model attribution}. Techniques for model attribution are needed to identify the model responsible for generating a given sample (such as text, image, video, or audio). Model attribution should also include identifying the user responsible for creating the content, including metadata such as IDs or usernames. These technologies would not only protect the copyright of generated content but also encourage responsible data generation by ensuring that harmful or malicious content can be traced back to its creator.

\item \textbf{Which Samples Contribute to a Generated Output?} This question pertains to data attribution in the context of AIGC. Every piece of generated content can be seen as a combination of ``inspirations'' drawn from a specific set of training samples. Data attribution techniques are needed to identify the training samples that most significantly contributed to the generated content. Contributors whose samples play a critical role in shaping the content should receive a share of the profits if the content is used commercially, promoting a fair and transparent data economy.

\end{itemize}

Beyond these questions, many other critical issues must be addressed to create a sustainable and ethical data ecosystem. Ensuring that data contributors are fairly acknowledged and incentivized is essential for promoting accountability and transparency in AIGC.

\subsubsection{Safe Embodied Agents}

Most safety threats studied today are \textbf{digital}. However, as embodied AI agents are increasingly deployed in the physical world, new physical threats will emerge, potentially causing real harm and loss to humans. Ensuring the safety of these embodied agents has therefore become a critical concern. Safe agents must be resilient to adversarial inputs, capable of self-regulating harmful behaviors, and consistently aligned with human values.

Achieving this requires deeply integrating safety mechanisms into the agents' decision-making processes, enabling them to handle unexpected challenges while maintaining robustness and reliability. The primary challenge is \textbf{designing safety protocols that allow these agents to perform complex tasks autonomously, while ensuring they remain trustworthy and safe in dynamic and unpredictable environments}. As agents gain greater autonomy, ensuring their safety becomes not only a technical challenge but also a significant ethical responsibility.

\subsubsection{Safe Superintelligence}
As AI progresses toward AGI and superintelligence, embedding inherent safety mechanisms into large models to ensure predictable, value-aligned behavior becomes a critical challenge. While the technical path to safe superintelligence remains uncertain, several promising mechanisms offer potential solutions:

\begin{itemize}
    \item \textbf{Oversight System}: One system cannot be both superintelligent and trustworthy, like humans. However, we can develop an oversight system to monitor and regulate the primary system’s behavior, intervening when necessary. A key challenge is to ensure the reliability and robustness of the oversight system itself. This leads to the \textbf{\textit{Oversight Paradox}}: \textit{If a superintelligent AI is monitored by another AI (the oversight system), the oversight system must be at least as capable as the superintelligent AI to reliably detect and prevent undesirable behaviors}. However, this raises the question: \textbf{who monitors the oversight system to ensure it doesn't fail or act contrary to its purpose?}

    \item \textbf{Safety Layer}: This approach embeds a dedicated safety layer \cite{zhao2024defending, li2024safelayer} directly within the model’s architecture, acting as a gatekeeper to filter outputs against predefined safety constraints. Such layers could be dynamically updated based on real-time feedback or optimized for specific tasks.

    \item \textbf{Safety Expert}: This approach incorporates specialized ``safety experts" within the Mixture of Experts (MoE) framework \cite{jacobs1991adaptive, shazeer2017outrageously, fedus2022switch, jiang2024mixtral} to handle safety-critical tasks. By dynamically routing high-stakes queries to these experts, safety considerations are prioritized in decision-making. The recurring challenge is developing a truly safe expert model that can consistently perform as expected in all scenarios.
    
    \item \textbf{Adversarial Alignment}: This approach leverages adversarial safety principles to align models with human values. It involves training models to exploit vulnerabilities in existing safety mechanisms, then refining these mechanisms to resist adversarial prompts. Despite its promise, challenges such as high computational costs and the risk of unintended behaviors remain significant concerns.
    
    \item \textbf{Safety Consciousness}: This approach involves embedding a safety-conscious framework into the model’s foundational training to promote ethical reasoning and value alignment as intrinsic behaviors. The goal is to make safety a core characteristic, enabling the model to dynamically adapt to diverse and evolving scenarios. Safety consciousness may be formulated as a type of \textbf{safety tendency}: an inherent inclination to generate low-risk responses and shape outputs based on an awareness of potential harmful consequences, similar to human decision-making processes.
    
\end{itemize}


\subsection{A Call for Collective Action}

Safeguarding large models against adversarial manipulation, misuse, and harm is a global challenge that requires the collective efforts of researchers, practitioners, and policymakers. The following sections outline a research agenda aimed at advancing large-model safety through collaboration and innovation.

\subsubsection{Defense-Oriented Research}

Current research on large-model safety is heavily skewed toward attack strategies, with significantly fewer efforts dedicated to developing defense mechanisms. This imbalance is concerning, as the sophistication of attacks continues to outpace the development of effective defenses. To address this gap, we advocate for a shift in research priorities toward defense strategies. Researchers should not only investigate attack mechanisms but also focus on developing robust defenses to mitigate or prevent these threats. A balanced approach is crucial for advancing the field of safety research.

Moreover, future defense research should emphasize integrated approaches. New defense methods should not be proposed or implemented in isolation but rather integrated with existing approaches to build cumulative protection. Defense research should be viewed as a continuous, evolving effort, where new methods are layered onto established ones to enhance overall effectiveness. However, the diversity of defense strategies presents a challenge. Developing frameworks to effectively incorporate different defense mechanisms will require a collective effort from the research community.

\subsubsection{Dedicated Safety APIs}

To support research and testing, commercial AI models should offer a dedicated safety API. This API would allow researchers to assess and enhance the safety of these models by subjecting them to a variety of adversarial and safety-critical scenarios. By providing such an API, commercial providers can enable external safety evaluations without disrupting the general services offered to users. This would foster collaboration between industry and academia, facilitating continuous improvement in model safety.

\subsubsection{Open-Source Platforms}

The AI safety community would greatly benefit from the development and open-source release of safety platforms and libraries. These tools would facilitate the rapid evaluation, testing, and improvement of safety mechanisms across a variety of models and applications. Open-sourcing these platforms would foster collaboration and transparency, enabling researchers and practitioners to share best practices, benchmark safety techniques, and contribute to the establishment of universal safety standards.


\subsubsection{Global Collaborations}

The pursuit of AI safety is a global challenge that transcends national borders, requiring coordinated efforts from academia, technology companies, government agencies, and non-profit organizations. Effective collaboration on a global scale is essential to addressing the potential risks associated with advanced AI systems. By fostering international cooperation, we can more efficiently tackle complex safety issues and establish unified standards that guide the safe development and deployment of AI technologies.

To facilitate global collaboration, the following initiatives could be pursued:

\begin{itemize} 
    \item \textbf{International Safety Alliances}: Establishing global alliances dedicated to AI safety can bring together experts and resources from around the world. These alliances would focus on sharing research findings, coordinating safety evaluations, and developing universal safety benchmarks that reflect diverse regional needs and values.
    
    \item \textbf{Cross-Border Data Sharing Frameworks}: Access to diverse datasets is essential to improving the robustness and fairness of AI models. Developing secure and ethical frameworks for cross-border data sharing would allow researchers to test models across a wide range of scenarios and ensure that safety mechanisms are universally applicable.

    \item \textbf{Joint Research Programs}: Collaborative research programs that unite academic institutions, industry leaders, and government agencies can drive innovation in AI safety. These programs should focus on areas such as adversarial defense, safety alignment, and real-time adaptability, ensuring that their findings are broadly applicable across various AI systems.
    
    \item \textbf{Global Safety Competitions and Challenges}: Building on the concept of open safety competitions, international challenges could be organized to engage the brightest minds from around the world. These competitions would address critical safety issues, encourage the development of innovative solutions, and foster a sense of shared responsibility in advancing AI safety.
    
    \item \textbf{Policy and Regulatory Harmonization}: A collaborative approach to AI governance can help align safety regulations and policies across countries. This harmonization would prevent the misuse of AI technologies while promoting responsible development and deployment practices globally.
    
\end{itemize}

Global collaborations not only enhance the effectiveness of AI safety research but also promote transparency, trust, and accountability in the development of advanced AI systems. By working together across borders and disciplines, we can ensure that AI technologies benefit humanity while minimizing the risks associated with their deployment.
