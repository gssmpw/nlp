\section{Introduction}
\label{sec:introduction}

\begin{figure*}[h]
    \centering
    \subfigure{
    \includegraphics[width=0.31\textwidth]{Fig/Statistics/survey_bar_plot.pdf}
    }
    \subfigure{
    \includegraphics[width=0.31\textwidth]{Fig/Statistics/Model_Pie.pdf}
    }
    \subfigure{
    \includegraphics[width=0.31\textwidth]{Fig/Statistics/Type_Pie.pdf}
    }
    \caption{\textbf{Left}: The number of safety research papers published over the past four years. \textbf{Middle}: The distribution of research across different models. \textbf{Right}: The distribution of research across different types of attacks and defenses.}
    \label{fig:total_num}
\end{figure*}


\begin{figure*}[htb]
\centering
    \includegraphics[width=1\linewidth]{Fig/Statistics/New_Fig.pdf}
        \caption{ \textbf{Left}: The quarterly trend in the number of safety research papers published across different models; \textbf{Middle}: The proportional relationship between large models and their corresponding attacks and defenses; \textbf{Right}: The annual trend in the number of safety research papers published on various attacks and defenses, presented in descending order from highest to lowest.}
\label{fig:crossview}
\end{figure*}

\IEEEPARstart{A}rtificial Intelligence (AI) has entered the era of large models, exemplified by Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-Training (VLP) models, Vision-Language Models (VLMs), and image/video generation diffusion models (DMs).
Through large-scale pre-training on massive datasets, these models have demonstrated unprecedented capabilities in tasks ranging from language understanding and image generation to complex problem-solving and decision-making. Their ability to understand and generate human-like content (e.g., texts, images, audios, and videos) has enabled applications in customer service, content creation, healthcare, education, and more,  highlighting their transformative potential in both commercial and societal domains.

% \IEEEPARstart{A}rtificial Intelligence (AI) has entered the era of large models, exemplified by Vision Foundation Models (VFMs) such as pre-trained Vision Transformers (ViTs) \cite{dosovitskiy2021an} and the Segment Anything Model (SAM) \cite{kirillov2023segment}; Large Language Models (LLMs) including ChatGPT \cite{chatgpt}, PaLM \cite{chowdhery2022palm}, GPT-4 \cite{gpt-4}, Claude \cite{anthropic2023claude}, LLaMA \cite{Llama-1, Llama-2}, GPT-4o \cite{gpt-4o}, o1 \cite{o1}, and DeepSeek V3 \cite{deepseek-v3}; Vision-Language Pre-Training (VLP) models like CLIP \cite{CLIP}; Vision-Language Models (VLMs) such as GPT-4 Vision \cite{gpt-4v}, Gemini \cite{gemini}, BLIP \cite{BLIP-1, BLIP-2, Llama-3}, and LLaVA \cite{LLaVA}; and generative diffusion models (DMs) including Stable Diffusion \cite{rombach2022high}, DALL-E \cite{DALLE-2, DALLE-3}, Imagen \cite{Imagen}, Midjourney \cite{Midjourney}, FLUX.1 \cite{FLUX-1.0}, Sora \cite{Sora}, and Veo 2 \cite{Veo2}.
% Through large-scale pre-training on massive datasets, these models have demonstrated unprecedented capabilities in tasks ranging from language understanding and image generation to complex problem-solving and decision-making. Their ability to understand and generate human-like content (e.g., texts, images, audios, and videos) has enabled applications in customer service, content creation, healthcare, education, and more,  highlighting their transformative potential in both commercial and societal domains.

However, the deployment of large models comes with significant challenges and risks. 
As these models become more integrated into critical applications, concerns regarding their vulnerabilities to adversarial, jailbreak, and backdoor attacks, data privacy breaches, and the generation of harmful or misleading content have intensified. 
These issues pose substantial threats, including unintended system behaviors, privacy leakage, and the dissemination of harmful information. Ensuring the safety of these models is paramount to prevent such unintended consequences, maintain public trust, and promote responsible AI usage.
The field of AI safety research has expanded in response to these challenges, encompassing a diverse array of attack methodologies, defense strategies, and evaluation benchmarks designed to identify and mitigate the vulnerabilities of large models. 
Given the rapid development of safety-related techniques for various large models, we aim to provide a comprehensive survey of these techniques, highlighting strengths, weaknesses, and gaps, while advancing research and fostering collaboration.
% This paper presents a systematic review of these efforts, with a particular focus on safety research for large models, including VFMs, LLMs, VLP models, VLMs, DMs, and large-model-powered agents.


\input{Table/orgnization}


Given the broad scope of our survey, we have structured it with the following considerations to enhance clarity and organization:
\begin{itemize}
    \item \textbf{Models}. We focus on six widely studied model categories, including \textbf{VFMs}, \textbf{LLMs}, \textbf{VLPs}, \textbf{VLMs}, \textbf{DMs}, and \textbf{Agents}, and review the attack and defense methods for each separately. These models represent the most popular large models across various domains.
    
    \item \textbf{Organization}. For each model category, we classify the reviewed works into attacks and defenses, and identify \textbf{10} attack types: \textbf{adversarial}, \textbf{backdoor}, \textbf{poisoning}, \textbf{jailbreak}, \textbf{prompt injection}, \textbf{energy-latency}, \textbf{membership inference}, \textbf{model extraction}, \textbf{data extraction}, and \textbf{agent} attacks. When both backdoor and poisoning attacks are present for a model category, we combine them into a single \textbf{backdoor \& poisoning} category due to their similarities. We review the corresponding defense strategies for each attack type immediately after the attacks.

    \item \textbf{Taxonomy}. For each type of attack or defense, we use a two-level taxonomy: \textbf{Category $\rightarrow$ Subcategory}. The \textbf{Category} differentiates attacks and defenses based on the threat model (e.g., white-box, gray-box, black-box) or specific subtasks (e.g., detection, purification, robust training/tuning, and robust inference). The \textbf{Subcategory} offers a more detailed classification based on their techniques.
    
    \item \textbf{Granularity}. To ensure clarity, we simplify the introduction of each reviewed paper, highlighting only its key ideas, objectives, and approaches, while omitting technical details and experimental analyses. 
    % We aim for our survey to be a high-level reference that helps readers understand the current landscape of large model safety. 
    
\end{itemize}

\begin{table}[htbp]
  \centering
  \caption{A summary of existing surveys.}
  \begin{adjustbox}{width=1.0\linewidth}
    \begin{tabular}{p{7.75em}cp{10.665em}p{20.335em}}
    \toprule
    \textbf{Survey} & \textbf{Year}  & \textbf{Model} & \textbf{Topic} \\
    \midrule
    Zhang et al. \cite{zhang2024adversarial} & 2024  & VFM\&VLP\&VLM & Adversarial, Jailbreak, Prompt Injection \\
    \rowcolor{gray!15} Truong et al. \cite{truong2024attacks} & 2024  & DM & Adversarial, Backdoor, Membership inference \\
    Zhao et al. \cite{zhao2024survey} & 2024  & LLM & Backdoor \\
    \rowcolor{gray!15} Yi et al. \cite{yi2024jailbreak} & 2024  & LLM & Jailbreak \\
    Jin et al. \cite{jin2024jailbreakzoo} & 2024  & LLM\&VLM & Jailbreak \\
    \rowcolor{gray!15} Liu et al. \cite{liu2024jailbreak} & 2024  & VLM & Jailbreak \\
    Liu et al. \cite{liu2024survey} & 2024  & VLM & Adverarial, Backdoor, Jailbreak, Prompt Injection \\
    \rowcolor{gray!15} Cui et al. \cite{cui2024risk} & 2024  & LLM agent & Adversarial, Backdoor, Jailbreak \\
    Deng et al. \cite{deng2024ai} & 2024  & LLM agent & Adversarial, Backdoor, Jailbreak \\
    \rowcolor{gray!15} Gan et al. \cite{gan2024navigating} & 2024  & LLM\&VLM agent & Adversarial, Backdoor, Jailbreak \\
    \bottomrule
    \end{tabular}%
    \end{adjustbox}
  \label{tab:existing_survey}%
\end{table}%



Our survey methodology is structured as follows. First, we conducted a keyword-based search targeting specific model types and threat types to identify relevant papers. Next, we manually filtered out non-safety-related and non-technical papers. For each remaining paper, we categorized its proposed method or framework by analyzing its settings and attack/defense types, assigning them to appropriate categories and subcategories.
In total, we collected \textbf{390} technical papers, with their distribution across years, model types, and attack/defense strategies illustrated in Figure \ref{fig:total_num}. As shown, safety research on large models has surged significantly since 2023, following the release of ChatGPT. Among the model types, LLMs and DMs have garnered the most attention, accounting for over \textbf{60\%} of the surveyed papers. Regarding attack types, \textbf{jailbreak}, \textbf{adversarial}, and \textbf{backdoor attacks} were the most extensively studied. On the defense side, \textbf{jailbreak defenses} received the highest focus, followed by \textbf{adversarial defenses}.
Figure \ref{fig:crossview} presents a cross-view of temporal trends across model types and attack/defense categories, offering a detailed breakdown of the reviewed works. Notably, research on attacks constitutes \textbf{60\%} of the studied. In terms of defense, while defense research accounts for only \textbf{40\%}, underscoring a significant gap that warrants increased attention toward defense strategies. The overall structure of this survey is outlined in Figure \ref{fig:organization}.

\textbf{Difference to Existing Surveys.} Large-model safety is a rapidly evolving field, and several surveys have been conducted to advance research in this area. Recently, Slattery et al. \cite{slattery2024ai} introduced an AI risk framework with a systematic taxonomy covering all types of risks. In contrast, our focus is on the technical aspects, specifically the attack and defense techniques proposed in the literature.
Table \ref{tab:existing_survey} lists the technical surveys we identified, each focusing on a specific type of model or threat (e.g., LLMs, VLMs, or jailbreak attacks/defenses). Compared with these works, our survey offers both a broader scope—covering a wider range of model types and threats—and a higher level perspective, centering on overarching methodologies rather than minute technical details.
