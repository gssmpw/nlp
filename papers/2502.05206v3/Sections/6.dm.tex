\section{Diffusion Model Safety}\label{sec:diffusion}

This section focuses on safety research related to diffusion models \cite{rombach2022high, DALLE-2, DALLE-3, Imagen}, which involve forward noise addition and reverse sampling. In the forward process, Gaussian noise is incrementally added to an image until it becomes pure noise. Reverse sampling generates new samples by stepwise denoising based on learned data distributions~\cite{ho2020denoising, song2020denoising, song2020score}. By integrating input information, diffusion models perform conditional generation, transforming data distribution modeling \( p(x) \) into \( p(x|\text{guidance}) \).

Widely used in Image-to-Image (I2I), Text-to-Image (T2I), and Text-to-Video (T2V) tasks, diffusion models are applied in content creation, image editing, and film production. However, their extensive use exposes them to various security risks including \textbf{adversarial}, \textbf{jailbreak}, \textbf{backdoor}, and \textbf{privacy attacks}. These attacks can degrade generation quality, bypass safety filters, manipulate outputs, and reveal sensitive training data. This section also reviews defenses against these threats, including \textbf{jailbreak} and \textbf{backdoor defenses}, as well as \textbf{intellectual property protection} techniques.


\subsection{Adversarial Attacks}
\label{sec:dm_adversarial_attacks}

Adversarial attacks on diffusion models typically perturb text prompts to degrade image quality or cause semantic mismatches with the original text. This section reviews existing adversarial attacks, categorized by threat model into \textbf{white-box}, \textbf{gray-box}, and \textbf{black-box} methods.

\input{Table/Diffusion/Overall_gray}

\subsubsection{White-box Attacks}
White-box attacks on T2I diffusion models assume full access to model parameters, allowing direct optimization of text prompts or latent space to degrade or disrupt image generation. For example, \textbf{SAGE} \cite{liu2023discovering} explores both the discrete prompt and latent spaces to uncover failure modes in T2I models, including distorted generations and targeted manipulations. \textbf{ATM} \cite{du2024stable} generates attack prompts similar to clean prompts by replacing or extending words using Gumbel Softmax, preventing the model from generating desired subjects.


\subsubsection{Gray-box Attacks}
\label{subsubsec:Similarity-based optimization}
Gray-box attacks assume the CLIP text encoder used in many T2I diffusion models is frozen and publicly available. The attacker can then exploit CLIP similarity loss to craft adversarial text prompts targeting the text encoder.


\textbf{QFA} \cite{zhuang2023pilot} minimizes cosine similarity between original and perturbed text embeddings to generate images that differ as much as possible from the original text. \textbf{RVTA} \cite{zhang2024revealing} maximizes image-text similarity to align adversarial prompts with reference images generated by a surrogate diffusion model. 
% Notably, both methods utilize mask strategies to localize the attack’s effects, with QFA focusing on identifying steerable key dimensions in the embedding space to precisely perturb the target semantic elements, and RVTA employing an object-background decoupling mechanism to preserve object semantics while altering other attributes like style.
\textbf{MMP-Attack}~\cite{yang2024multi} simultaneously maximizes the cosine similarity between the perturbed text embedding and the target embedding in both the text and image modalities, while employing a straight-through estimator to execute the optimization process.



\subsubsection{Black-box Attacks}
Black-box attacks assume the attacker has no knowledge of the victim diffusion model's internals (parameters or architecture). Since diffusion models use text prompts as input, existing attacks employ textual adversarial techniques to evade the model. These attacks can be further categorized by granularity into \textbf{character-level}, \textbf{word-level}, and \textbf{sentence-level} attacks.

\textbf{Character-level Attacks} modify the characters in the text input to create adversarial prompts. \textbf{ECB} \cite{struppek2023exploiting} shows how replacing characters with homoglyphs, such as using Hangul or Arabic scripts, shifts generated images toward cultural stereotypes. Subsequent works, like \textbf{CharGrad} \cite{kou2023character}, optimize character-level perturbations using gradient-based attacks and proxy representations to map character changes to embedding shifts. \textbf{ER} \cite{gao2023evaluating} uses distribution-based objectives (e.g., MMD, KL divergence) to maximize discrepancies in image distributions, enhancing attack effectiveness. These attacks exploit typos, homoglyphs, and phonetic modifications, disrupting text-to-image outputs.


\textbf{Word-level Attacks} craft adversarial prompts by replacing or adding words to the input text. \textbf{DHV} \cite{daras2022discovering} uncovers a hidden vocabulary in diffusion models, where nonsensical strings like \texttt{Apoploe vesrreaitais} can generate bird images, due to their proximity to target concepts in the CLIP text embedding space. Building on this, \textbf{AA} \cite{milliere2022adversarial} introduces macaronic prompting, combining word fragments from different languages to control visual outputs systematically. These attacks reveal vulnerabilities in the relationship between text embeddings and image generation.

\textbf{Sentence-level Attacks} rewrite a substantial part or the entire prompt to create adversarial prompts. \textbf{RIATIG} \cite{liu2023riatig} uses a CLIP-based image similarity measure as an optimization objective and a genetic algorithm to iteratively mutate and select text prompts, creating adversarial examples that resemble the target image while remaining semantically different from the original text. In contrast, \textbf{BBA} \cite{maus2023black} employs classification loss and black-box optimization to refine prompts, using Token Space Projection (TPS) to bridge the gap between continuous word embeddings and discrete tokens, enabling the generation of category-specific images without explicit category terms.


\subsection{Jailbreak Attacks}
\label{sec:dm_jailbreak_attacks}

Diffusion models use both internal and external safety mechanisms to void the generation of Not Safe For Work (NSFW) content. Internal safety mechanisms often refer to the inherent robustness of T2I diffusion models, achieved through safety alignment during training, which aims to reduce the likelihood of generating harmful content. External safety mechanisms, on the other hand, are safety filters, such as text, image, or text-image classifiers, applied to detect and block unsafe outputs after generation. 
Jailbreak attacks aim to craft adversarial prompts that bypass the safety mechanisms of diffusion models, enabling the generation of harmful content. This section provides a systematic review of existing jailbreak methods, categorized by threat model into \textbf{white-box}, \textbf{gray-box}, and \textbf{black-box} attacks.



\subsubsection{White-box Attacks}
White-box attacks can bypass the safety mechanisms in T2I diffusion models through gradient-based optimization. These attacks can be further classified into \textbf{internal safety attacks} and \textbf{external safety attacks}, each exploiting specific vulnerabilities in the victim models.


\textbf{Internal Safety Attacks} target the internal safety mechanisms of diffusion models.
Jailbreaking internally safety-enhanced diffusion models involves regenerating NSFW content by bypassing the removal of harmful concepts. The red teaming tool \textbf{P4D} \cite{chin2023prompting4debugging} automatically identifies problematic prompts to exploit limitations in current safety evaluations, aligning the predicted noise of an unconstrained model with that of a safety-enhanced one. \textbf{UnlearnDiffAtk} \cite{zhang2023generate} introduces an evaluation framework that uses unlearned diffusion models' classification capabilities to optimize adversarial prompts, aligning predicted noise with a target unsafe image to force the model to recreate NSFW content during denoising.


\textbf{External Safety Attacks} target the safety filters of diffusion models, aiming to bypass both input and output safety mechanisms. \textbf{RTSDSF} \cite{rando2022red} reverse-engineered predefined NSFW concepts in filters by using the CLIP model to encode and compare NSFW vocabulary embeddings, performing a dictionary attack. It also showed that prompt dilution—adding irrelevant details—can bypass safety filters. \textbf{MMA} \cite{yang2024mma} employs a similarity-driven loss to optimize adversarial prompts and introduce subtle perturbations to input images, bypassing both prompt filters and post-hoc safety checkers during image editing.


\subsubsection{Gray-box Attacks}
Gray-box jailbreak attacks assume that attackers have full access only to the open-source text encoder, with other components of the diffusion model remaining inaccessible. In this scenario, the attacker exploits the exposed text encoder to bypass the model's internal safety mechanism.

\textbf{Internal Safety Attacks}, under the gray-box setting, target models with `concept erasure'. \textbf{Ring-A-Bell} \cite{tsai2023ring} extracts unsafe concepts by comparing antonymous prompt pairs, generates harmful prompts with soft prompts, and refines them using a genetic algorithm. 
\textbf{JPA} \cite{ma2024jailbreaking} leverages antonyms like \texttt{“nude”} and \texttt{“clothed”}, calculating their average difference in the text embedding space to represent NSFW concepts, then optimizes prefix prompts for semantic alignment. \textbf{RT-Attack} \cite{gao2024rt} uses a two-stage strategy to maximize textual similarity to NSFW prompts and iteratively refines them based on image-level similarity, demonstrating that even limited knowledge can enable attacks on safety-enhanced models.

\subsubsection{Black-box Attacks}

Black-box jailbreaks on diffusion models target commercial models with access only to outputs, such as filter rejections or generated image quality and semantics, and are primarily \textbf{external safety attacks}. 

\textbf{External Safety Attacks}, in the black-box setting, use hand-crafted or LLM-assisted adversarial prompts to mislead the victim model to generate NSFW content.
\textbf{UD} \cite{qu2023unsafe} highlights the risk of T2I models generating unsafe content, especially hateful memes, by refining unsafe prompts manually. \textbf{SneakyPrompt} \cite{yang2024sneakyprompt} uses reinforcement learning to optimize adversarial prompts, which updates its policy network based on filter evasion and semantic alignment. Other methods employ LLMs to refine adversarial prompts.  \textbf{Groot} \cite{liu2024groot} decomposes prompts into objects and attributes to dilute sensitive content. \textbf{DACA} \cite{deng2023divide} breaks down and recombines prompts using LLMs. \textbf{SurrogatePrompt} \cite{ba2023surrogateprompt} targets Midjourney, substituting sensitive terms and leveraging image-to-text modules to generate harmful content at scale. \textbf{Atlas} \cite{dong2024jailbreaking} automates the attack with a two-agent system: one VLM generates adversarial prompts, while an LLM evaluates and selects the best candidates. These LLM-assisted strategies can significantly improve the effectiveness and stealthiness of the attacks.



\subsection{Jailbreak Defenses}
\label{sec:dm_jailbreak_defenses}

This section reviews existing defense strategies proposed for T2I diffusion models against jailbreak attacks, including \textbf{concept erasure} and \textbf{inference guidance}. 
The key challenge of these defenses is how to ensure safety while maintaining generation quality.


\subsubsection{Concept Erasure} 
Concept erasure is an emerging research area focused on removing undesirable concepts (e.g., NSFW content and copyrighted styles) from diffusion models, where these concepts are referred to as \emph{target concepts}. Concept erasure methods can be categorized into three types: \textbf{finetuning-based}, \textbf{close-form solution}, and \textbf{pruning-based}, depending on the strategy employed.

\paragraph{Finetuning-based Methods} 
These methods use gradient-based optimization to adjust model parameters, typically involving a loss function with an erasure term to prevent the generation of representations linked to the target (undesirable) concept, and a constraint term to preserve non-target concepts. These approaches can be categorized into \textbf{anchor-based}, \textbf{anchor-free}, and \textbf{adversarial} erasure methods.


\textbf{Anchor-based Erasing} is a targeted approach that guides the model to shift the target (undesirable concept) towards a good concept (anchor) by aligning predicted latent noise. \textbf{AC} \cite{kumari2023ablating} defines anchor concepts as broader categories encompassing the target concepts (e.g., \texttt{“Grumpy Cat”} → \texttt{“Cat”}) and uses standard diffusion loss on text-image pairs of anchors to preserve their integrity while erasing target concepts. 
% Subsequent works refined the definition and alignment of anchor concepts. 
\textbf{ABO} \cite{hong2024all} removes specific target concepts by modifying classifier guidance, using both explicit (replacing the target with a predefined substitute) and implicit (suppressing attention maps) erasing signals, and includes a penalty term to maintain generation quality. 
\textbf{DoCo} \cite{wu2024unlearning} improves generalization by aligning target and anchor concepts through adversarial training and mitigating gradient conflicts with concept-preserving gradient surgery. \textbf{SPM} \cite{lyu2024one} uses a 1D adapter and negative guidance \cite{gandikota2023erasing} to suppress target concepts while ensuring non-target concepts remain consistent, affecting only relevant synonyms. 
\textbf{SA} \cite{heng2024selective} applies generative replay and elastic weight consolidation to stabilize model weights and maintain normal generation capabilities while preserving non-target concepts.

\textbf{Anchor-free Erasing} is a non-targeted fine-tuning approach that reduces the probability of generating target concepts without aligning to a specific safe concept. 
% Instead, it aims to directly reduce the probability of the generated image containing the target concept. 
\textbf{ESD} \cite{gandikota2023erasing} modifies classifier-free guidance into negative-guided noise prediction to minimize the target concept’s generation probability (e.g., "Van Gogh"). \textbf{SDD} \cite{kim2023towards} addresses the extra effects of ESD’s negative guidance by using unconditioned predictions and EMA to avoid catastrophic forgetting. \textbf{DT} \cite{ni2023degeneration} erases unsafe concepts by training the model to denoise scrambled low-frequency images. \textbf{Forget-Me-Not} \cite{zhang2024forget} uses Attention Resteering to minimize intermediate attention maps related to the target concept. \textbf{Geom-Erasing} \cite{liu2024implicit} erases implicit concepts like watermarks by applying a geometric-driven control method and introduces the \emph{Implicit Concept Dataset}. \textbf{SepME} \cite{zhao2024separable} advances multiple concept erasure and restoration. Fuchi et al. \cite{fuchi2024erasing} proposed few-shot unlearning by targeting the text encoder rather than the image encoder or diffusion model.
\textbf{CCRT} \cite{han2024continuous} proposes a method for continuous removal of diverse concepts from diffusion models.


\textbf{Adversarial Erasing} enhances previous methods by introducing perturbations to the target concept's text embedding and using adversarial training to improve robustness. \textbf{Receler} \cite{huang2023receler} employs a lightweight eraser and adversarial prompt embeddings, iteratively training against each other, while applying a binary mask from U-Net attention maps to target only the concept regions. \textbf{AdvUnlearn} \cite{zhang2024defensive} shifts adversarial attacks to the text encoder, targeting the embedding space and using regularization to preserve normal generation. \textbf{RACE} \cite{kim2024race} improves efficiency by conducting adversarial attacks at a single timestep, reducing computational complexity. These methods enhance the model’s resistance to adversarial prompts aimed at regenerating erased concepts.


\paragraph{Close-form Solution Methods} 
These methods offer an efficient alternative to fine-tuning-based erasure, focusing on localized updates in cross-attention layers to erase target concepts, inspired by model editing in LLMs \cite{meng2022mass}. Unlike fine-tuning, which aligns denoising predictions, these methods align cross-attention values. 
\textbf{TIME} \cite{orgad2023editing} applies a closed-form solution to debias models, while \textbf{UCE} \cite{gandikota2024unified} extends this to multiple erasure targets, preserving surrounding concepts to reduce interference. \textbf{MACE} \cite{lu2024mace} refines cross-attention updates with LoRA and Grounded-SAM \cite{kirillov2023segment,liu2023grounding} for region-specific erasure. A recent challenge is that erased concepts can still be generated via sub-concepts or synonyms \cite{liu2024realera}.
\textbf{RealEra} \cite{liu2024realera} tackles this by mining associated concepts and adding perturbations to the embedding, expanding the erasure range with beyond-concept regularization. 
\textbf{RECE} \cite{gong2024reliable} addresses insufficient erasure by continually finding new concept embeddings during fine-tuning and applying closed-form solutions for further erasure.

\paragraph{Pruning-based Methods} 
These methods erase target concepts by identifying and removing neurons strongly associated with the target, selectively disabling them without updating model weights.
\textbf{ConceptPrune} calculates a Wanda score using target and reference prompts to measure each neuron's contribution, pruning those most associated with the target concept. Similarly, another approach \cite{yang2024pruning} identifies concept-correlated neurons using adversarial prompts to enhance the robustness of existing erasure methods.


\subsubsection{Inference Guidance} 
Inference guidance methods steer pre-trained diffusion models to generate safe images by incorporating additional auxiliary information and specific guidance during the inference process.
 
\paragraph{Input Guidance} 
This type of guidance use additional input text to steer the model toward safe content. 
\textbf{SLD} \cite{schramowski2023safe} adjusts noise predictions during inference based on a text condition and unsafe concepts, guiding generation towards the intended prompt while avoiding unsafe content, without requiring fine-tuning. It also introduces the I2P benchmark, a dataset for testing inappropriate content generation.

\paragraph{Input \& Output Guidance}
This type of methods prevent harmful inputs and control NSFW outputs. \textbf{Ethical-Lens} \cite{cai2024ethical} employs a plug-and-play framework, using an LLM for input text revision (Ethical Text Scrutiny) and a multi-headed CLIP classifier for output image modification (Ethical Image Scrutiny), ensuring alignment with societal values without retraining or internal changes.


\paragraph{Latent space Guidance} 
This approach uses additional implicit representations in the latent space to guide generation. \textbf{SDIDLD} \cite{li2024self} employs self-supervised learning to identify the opposite latent direction of inappropriate concepts (e.g., "anti-sexual") and adds these vectors at the bottleneck layer, preventing harmful content generation.


\subsection{Backdoor Attacks}
\label{sec:dm_backdoor_attacks}

Backdoor attacks on diffusion models allow adversaries to manipulate generated content by injecting backdoor triggers during training. These "malicious triggers" are embedded in model components, and during generation, inputs with triggers (e.g., prompts or initial noise) guide the model to produce predefined content. The key challenge is enhancing attack success rates while keeping the trigger covert and preserving the model's original utility. Existing attacks can be categorized into \textbf{training manipulation} and \textbf{data poisoning} methods.

\subsubsection{Training Manipulation}
This type of attack typically assumes the attacker aims to release a backdoored diffusion model, granting control over the training or even inference processes. Existing attacks focus on the visual modality, inserting backdoors by using image pairs with triggers and target images (\emph{image-image pair injection}), typically targeting unconditional diffusion models.

\textbf{BadDiffusion} \cite{chou2023backdoor} presents the first backdoor attack on T2I diffusion models, which modifies the forward noise-addition and backward denoising processes to map backdoor target distributions to image triggers while maintaining DDPM sampling. \textbf{VillanDiffusion} \cite{chou2024villandiffusion} extends this to conditional models, adding prompt-based triggers and textual triggers for tasks like text-to-image generation. \textbf{TrojDiff} \cite{chen2023trojdiff} advances the research by controlling both training and inference, incorporating Trojan noise into sampling for diverse attack objectives. \textbf{IBA\cite{li2024invisible}} introduces invisible trigger backdoors using bi-level optimization to create covert perturbations that evade detection. \textbf{DIFF2} \cite{li2024watch} proposes a backdoor attack in adversarial purification, optimizing triggers to mislead classifiers and extending it to data poisoning by injecting backdoors directly.


\subsubsection{Data Poisoning}
Unlike training manipulation, data poisoning methods do not directly interfere with the training process, restricting the attack to inserting poisoned samples into the dataset. These attacks typically target conditional diffusion models and explore two types of textual triggers: \textbf{text-text pair} and \textbf{text-image pair}.


\textbf{Text-text Pair Triggers} consist of triggered prompts and their corresponding target prompts.  \textbf{RA} \cite{struppek2023rickrolling} adopts this approach to inject backdoors into the text encoder by adding a covert trigger character, mapping the original to the target prompt while preserving encoder functionality through utility loss optimization. The backdoored encoder generates embeddings with predefined semantics, guiding the diffusion model’s output. This lightweight attack requires no interaction with other model components. Several studies \cite{struppek2023rickrolling, vice2024bagm, huang2023zero, huang2024personalization} have also explored this approach. 

\textbf{Text-image Pair Triggers} consist of triggered prompts paired with target images. \textbf{BadT2I} \cite{zhai2023text} explores backdoors based on pixel, object, and style changes, where a special trigger (e.g., \texttt{“[T]”}) induces the model to generate images with specific patches, replaced objects, or styles. To reduce the data cost, \textbf{Zero-Day} \cite{huang2023zero,huang2024personalization} uses personalized fine-tuning, injecting trigger-image pairs for more efficient backdoors. \textbf{FTHCW} \cite{pan2023trojan} embeds target patterns into images from different classes, forming text-image pairs to generate diverse outputs. \textbf{IBT} \cite{naseh2024injecting} uses two-word triggers that activate the backdoor only when both words appear together, enhancing stealthiness. In commercial settings, \textbf{BAGM} \cite{vice2024bagm} manipulates user sentiment by mapping broad terms (e.g., “drinks”) to specific brands (e.g., “Coca Cola”). \textbf{SBD} \cite{wang2024stronger} employs backdoors for copyright infringement, bypassing filters by decomposing and reassembling copyrighted content using text-image pairs.


\subsection{Backdoor Defenses}
\label{sec:dm_backdoor_defenses}

Backdoor defenses for diffusion models is an emerging area of research. Current approaches generally follow a three-step pipeline: 1) \textbf{trigger inversion}, 2) \textbf{trigger validation} or \textbf{backdoor detection}, and 3) \textbf{backdoor removal}. Some works propose complete frameworks, while others focus on individual steps.


\subsubsection{Backdoor Detection}
Most early research focuses on detecting or validating backdoor triggers. \textbf{T2IShield} \cite{wang2024t2ishield} is the first backdoor detection and mitigation framework for diffusion models, leveraging the \emph{assimilation phenomenon} in cross-attention maps, where a trigger suppresses other tokens to generate specific content. \textbf{Ufid} \cite{guan2024ufid} validates triggers by noting that clean generations are sensitive to small perturbations, while backdoor-triggered outputs are more robust. \textbf{DisDet} \cite{sui2024disdet} proposes a low-cost detection method that distinguishes poisoned input noise from clean Gaussian noise by identifying distribution shifts.


\subsubsection{Backdoor Removal}
While trigger validation confirms the presence of a backdoor trigger, the identified triggers must still be removed from the victim model.
Most backdoor removal methods first invert the trigger and then eliminate the backdoor using the inverted trigger. 
\textbf{Elijah} \cite{an2024elijah} introduces a backdoor removal framework for diffusion models, inverting triggers through distribution shifts and aligning the backdoor's distribution with the clean one. \textbf{Diff-Cleanse} \cite{hao2024diff} formulates trigger inversion as an optimization problem with similarity and entropy loss, followed by pruning channels critical to backdoor sampling. \textbf{TERD} \cite{mo2024terd} proposes a unified reverse loss for trigger inversion, using a two-stage process for coarse and refined inversion. \textbf{PureDiffusion} \cite{truong2024purediffusion} employs multi-timestep trigger inversion, leveraging the consistent distribution shift caused by backdoored forward processes.



\input{Table/Diffusion/Overall_gray_2}

Privacy attacks on diffusion models can be classified into \textbf{membership inference}, \textbf{data extraction}, and \textbf{model extraction} attacks. As attack sophistication increases, each type poses a growing threat to privacy.


\subsection{Membership Inference Attacks}
\label{sec:dm_membership_inference_attacks}

Membership inference attacks on diffusion models aim to infer sensitive data by exploiting their generative capabilities. Attackers use techniques like reconstruction error, shadow models, auxiliary data, likelihood, gradient, or structural similarity metrics. These attacks can be classified into six types: \textbf{reconstruction error-based}, \textbf{auxiliary dataset-based}, \textbf{loss-based}, \textbf{gradient-based}, \textbf{structural similarity-based}, and \textbf{likelihood-based}.

\textbf{Reconstruction Error-based Attacks} infer the membership of candidate samples by analyzing their reconstruction errors in the diffusion model. Wu et al. \cite{wu2022membership} proposed to determine membership in text-conditional diffusion models by comparing the reconstruction error between the candidate and generated images, and their semantic alignment with the text prompt.
Inspired by GAN-leaks \cite{chen2020gan}, Matsumoto et al. \cite{matsumoto2023membership} introduced Diffusion-leaks, which generates multiple candidate images and infers membership based on minimal reconstruction errors. Li et al. \cite{li2024towards} proposed to average multiple reconstructions to reduce errors and improve inference accuracy, utilizing black-box APIs to modify candidate images.
\textbf{DRC} \cite{fu2024model} degrades and restores images using the diffusion model, comparing the restored images to the originals to infer membership and sensitive features.

\textbf{Auxiliary Datasets-based Attacks} use auxiliary datasets to train shadow models, enabling black-box membership inference by simulating the target model. 
Pang et al. \cite{pang2023black} targeted fine-tuned conditional diffusion models, computing similarity scores between query images and generated images to train a binary classifier for membership inference. \textbf{GMIA} \cite{zhang2024generated} introduces the first generalized membership inference attack for generative models, using only generated distributions and auxiliary non-member datasets, assuming the generated distribution approximates the original training distribution.

\textbf{Loss-based Attacks} exploit loss value distributions to distinguish member from non-member samples, assuming lower losses for member (training) samples. \cite{hu2023loss} and \cite{matsumoto2023membership} used loss values at different timesteps for membership inference. These two attacks can be viewed as \textbf{Static Loss Attack} (SLA), as they ignore the diffusion process. Dubinski et al. \cite{dubinski2024towards} modified the diffusion process to extract loss information from multiple perspectives, improving inference accuracy.

\textbf{Gradient-based Attacks} leverage gradient information for membership inference. For instance, \textbf{GSA} \cite{pang2023white} infers a sample is a member if its gradients significantly differ from surrounding samples, indicating a stronger influence on the model's training.

\textbf{Structural Similarity-based Attacks} compare structural features or similarity metrics between candidate samples and model outputs. \textbf{SMIA} \cite{li2024unveiling} uses the Structure Similarity Index Measure (SSIM)\cite{wang2004image}
metric to assess how well an image's structure is preserved during diffusion, with the average SSIM difference between members and non-members used to infer membership.

\textbf{Likelihood-based Attacks} use posterior or conditional likelihoods to infer membership. \textbf{SecMI} \cite{duan2023diffusion} estimates posterior likelihoods via reverse processes to target DDPM and Stable Diffusion models. \textbf{QRMI} \cite{tang2023membership} applies quantile regression to posterior likelihoods. \textbf{SIA} \cite{qu2024very} infers membership based on noise parameter differences in the reverse diffusion process. \textbf{PIA} \cite{kong2023efficient} uses diffusion model properties to infer membership with fewer queries. \textbf{PFAMI} \cite{fu2023probabilistic} analyzes fluctuations between target samples and neighbors, exploiting memorization in generative models. Zhai et al. \cite{zhai2024membership} use discrepancies in conditional likelihoods due to overfitting for membership inference.


\subsection{Data Extraction Attacks}
\label{sec:dm_data_extraction_attacks}

Data extraction attacks aim to reverse-engineer training data or attributes from a trained model, exploiting diffusion models' generative capabilities. Their effectiveness depends on the model's ability to memorize specific attributes \cite{somepalli2023understanding,gu2023memorization,wen2024detecting,ren2024unveiling}. These attacks can be classified into two main approaches based on the type of condition used: \textbf{explicit condition-based extraction} and \textbf{surrogate condition-based extraction}.

\textbf{Explicit Condition-based Extraction} leverages conditional information in T2I diffusion models to extract memorized training samples. Attackers use specific text prompts to generate images similar to training data. For example, \cite{carlini2023extracting} introduced brute-force data extraction (\textbf{BruteDE}), generating images with targeted prompts and using membership inference to identify matches. This method is slow. One Step Extraction (\textbf{OSE}) \cite{webster2023reproducible} exploits "template verbatims," where models regenerate training samples, using metrics like denoising confidence score (DCS) and edge consistency score (ECS) for faster extraction.

\textbf{Surrogate Condition-based Extraction} creates surrogate conditions to enable data extraction from unconditional diffusion models. \textbf{SIDE} \cite{chen2024towards} uses implicit labels from classifiers or feature extractors as surrogate conditions. \textbf{FineXtract} \cite{wu2024revealing} uses fine-tuned models as surrogate conditions to guide extraction in latent space regions tied to fine-tuning data.


\subsection{Model Extraction Attacks}
\label{sec:dm_model_extraction_attacks}

Model extraction aims to steal a trained diffusion model's internal parameters or architecture. The only known method for model extraction on diffusion models is Spectral DeTuning (\textbf{SDeT})  \cite{horwitz2024recovering}.
SDeT leverages Low-Rank Adaptation (LoRA) \cite{hu2021lora} to extract pre-fine-tuning weights of generative models fine-tuned with LoRA. By collecting multiple fine-tuned models from the same pretrained model, it formulates an optimization problem to minimize the difference between fine-tuned weights and the sum of original weights and adaptation matrices under a low-rank constraint, solved iteratively using Singular Value Decomposition (SVD)\cite{stewart1993early}. SDeT effectively recovers original weights for models like Stable Diffusion and Mistral-7B\cite{jiang2023mistral}, highlighting vulnerabilities in fine-tuning processes with low-rank adaptations.

\subsection{Intellectual Property Protection}
\label{sec:dm_intellectual_property_protection}
% \subsubsection{Watermark}

Intellectual property protection for AI is an emerging research area that uses techniques like adversarial attacks and watermarking to safeguard the intellectual property of natural (training or test) data, generated data, and trained models. These methods generally assume full access to the protected object. The following sections categorize these approaches into \textbf{natural data protection}, \textbf{generated data protection}, and \textbf{model protection}.

\subsubsection{Natural Data Protection}

Natural data protection methods focus on preprocessing data during training or inference to safeguard the copyright of naturally collected data, as opposed to generated data. In this context, data owners defend against model owners accessing the data. 
Existing natural data protection methods for T2I diffusion models aim to protect image intellectual property while minimizing quality loss. They can be categorized into \textbf{learning prevention}, \textbf{editing prevention}, and \textbf{data attribution} methods based on specific goals.

\textbf{Learning Prevention} methods prevent T2I models from learning useful features from training images using techniques like adversarial attacks. 
\textbf{DUAW} \cite{ye2023duaw} protects copyrighted images by disrupting the variational autoencoder (VAE) in Stable Diffusion models, optimizing universal adversarial perturbations on surrogate images to distort outputs. 
\textbf{AdvDM} \cite{liang2023adversarial} protects artwork copyrights by generating adversarial examples to prevent diffusion models from imitating artistic styles. \textbf{Anti-DreamBooth} \cite{van2023anti} defends against malicious fine-tuning by injecting adversarial noise into user images to block the model from learning personalized features.
\textbf{MetaCloak} \cite{liu2024metacloak} enhances image resistance to transformations (flipping, cropping, compression) by using surrogate diffusion models to craft transferable perturbations and a denoising-error maximization loss for better robustness. \textbf{InMakr} \cite{liu2024countering} embeds protective watermarks on critical pixels to safeguard personal semantics even if images are modified. \textbf{SimAC} \cite{wang2024simac} improves protection by optimizing timestep intervals and introducing a feature interference loss, leveraging early diffusion steps and high-frequency information from deeper layers.


\textbf{Editing Prevention} aims to prevent diffusion model-based image tampering and deepfake generation. Existing methods either embed watermarks or use adversarial noise to disrupt the editing process. \textbf{EditGuard} \cite{zhang2024editguard} introduces a proactive forensics framework to embed exclusive watermarks into images, making them resistant to various diffusion model-based editing techniques, including foreground or background removal, filling, tampering, and face swapping. \textbf{WaDiff} \cite{min2024watermark} adds a unique watermark to each user query, enabling traceability of the generated image if ethical concerns arise. \textbf{AdvWatermark} \cite{zhu2024watermark} incorporates adversarial noise, producing visible signatures in the protected image when used by I2I models, which helps identify tampered content.


\textbf{Data Attribution} techniques identify if generated data originates from a specific dataset, often by embedding watermarks for later verification. 
Diagnosis~\cite{wang2023diagnosis} introduced a method for detecting unauthorized data usage by applying stealthy image warping effects to protected data.
\textbf{FT-SHIELD} \cite{cui2023ft} uses alternating optimization and PGD \cite{madry2017towards} to embed watermarks, with a binary detector for verification. \textbf{DiffusionShield} \cite{cui2023diffusionshield} encodes copyright messages into watermark patches, jointly optimizing the decoder and patches to ensure consistency across samples for reliable extraction. \textbf{ProMark} \cite{asnani2024promark} introduces a proactive watermarking method for \emph{concept attribution}, embedding watermarks in training data that can be extracted when similar concepts are generated by the model.

\subsubsection{Generated Data Protection} 
\label{sec:dm_generated_data_proteciton}

With the rise of AI-generated content (AIGC), protecting the copyright of generated data has become increasingly important. Generated data protection seeks to answer, \textbf{“Who created this content?”} by embedding verifiable, unique watermarks into generated images to identify their creators (either the model or user). This ensures intellectual property protection and accountability for content publishers, while balancing the challenge of maintaining detection accuracy without compromising image quality.

\textbf{HiDDeN} \cite{zhu2018hidden} pioneers deep learning-based image watermarking, using an encoder to embed imperceptible watermarks and a decoder to recover them for detection. This approach can also watermark AI-generated images as a post-processing step.
Recent protection methods primarily address the above challenge by embedding watermarks into images during the generation (reverse sampling) process of diffusion models.
\textbf{Stable Signature} \cite{fernandez2023stable} embeds a binary signature into images generated by diffusion models through decoder fine-tuning, allowing the watermark to be recovered and validated using a pre-trained extractor and statistical test.
\textbf{LaWa} \cite{rezaei2024lawa} introduces a coarse-to-fine watermark embedding method within the latent diffusion model's decoder, employing multiple modules to insert the watermark at different upsampling stages using adversarial training. \textbf{Safe-SD} \cite{ma2024safe} proposes a framework for embedding a graphical watermark (e.g., QR code) into the imperceptible structure-related pixels of a Stable Diffusion model for high traceability.

Recent studies highlight vulnerabilities in watermarking for AIGC. \textbf{WEvade} \cite{jiang2023evading} bypasses watermark detection by adding subtle perturbations to watermarked images, exploiting watermark characteristics. \textbf{TAIW} \cite{hu2024transfer} proposes a transfer attack using multiple surrogate watermarking models in a no-box setting, analyzing its theoretical transferability. Unlike per-image attacks, \textbf{SSU} \cite{hu2024stable} introduces a model-targeted attack to remove in-generation watermarks by fine-tuning the diffusion model’s decoder with non-watermarked images, demonstrating the fragility of Stable Signature \cite{fernandez2023stable}.

\subsubsection{Model Protection} 
Model protection techniques safeguard the intellectual property of released models, enabling owners to verify ownership and trace generated content back to its origin. These approaches are categorized based on their objectives into \textbf{model watermark} and \textbf{model attribution}.

\textbf{Model Watermark} injects a watermark trigger into the model, which can then be activated during inference to verify ownership.
Zhao et al.~\cite{zhao2023recipe} proposed separate watermarking schemes for unconditional/class-conditional and T2I diffusion models. For unconditional/class-conditional models, a pretrained watermark encoder embeds a binary string (e.g., "011001") into the training data, and the model is trained to generate images with a detectable watermark, verified by a pretrained decoder. For T2I models, a paired (text, image) trigger (e.g., \texttt{"[V]"} and a QR code) is used to trigger the generation of the QR code for ownership verification. \textbf{FIXEDWM} \cite{liu2023watermarking} enhances trigger stealthiness by fixing its position in prompts, ensuring the watermarked image is generated only when the trigger is in the correct position.  \textbf{WDM} \cite{peng2023protecting} modifies the standard diffusion process into a Watermark Diffusion Process (WDP) to embed watermarks. During training, WDM learns from watermarked images using WDP, while normal images follow the standard diffusion process. During verification, Gaussian noises combined with the trigger can activate the generation of watermarked images.


\textbf{Model Attribution} also embeds watermarks into generated content to identify the model, similar to generated data protection methods in Section \ref{sec:dm_generated_data_proteciton}. The key difference is that model attribution focuses on model-wide watermarks, while generated data protection targets sample-specific watermarks.
\textbf{Tree-Ring}\cite{wen2023tree} embeds a watermark into the Fourier space of the initial Gaussian noise used for T2I generation. During verification, denoising diffusion implicit model (DDIM) inversion extracts the initial noise, and comparison with the original watermark identifies the generating model. \textbf{AquaLoRA}\cite{feng2024aqualora} addresses the limitations of existing methods to white-box adaptive attacks, including Tree-Ring, by embedding a secret bit string into the model parameters to achieve white-box protection, preventing easy manipulation of the watermark by malicious users.
\textbf{LatentTracer} \cite{wang2024trace} identifies the origin model of generated samples by reverse-engineering their latent inputs, eliminating the need for artificial fingerprints or watermarks.



\subsection{Datasets}
This section reviews commonly used datasets for diffusion model safety research, as summarized in Tables \ref{tab:diffuison_safety_I} and \ref{tab:diffuison_safety_II}. 
For adversarial attack and defense studies, captioned text-image pairs such as MS COCO \cite{lin2014microsoft}, LAION \cite{schuhmann2021laion, schuhmann2022laion}, and DiffusionDB \cite{wang2022diffusiondb} are often employed by conditional diffusion models. Datasets for category-image classification tasks, like ImageNet \cite{deng2009imagenet} and CIFAR10/100 \cite{krizhevsky2009learning}, are typically used by unconditional diffusion models to evaluate attack effectiveness and output quality.
In research on NSFW content in diffusion models, the I2P dataset \cite{schramowski2023safe} is widely used, alongside custom datasets such as NSFW-200 \cite{yang2024sneakyprompt}, VBCDE-100 \cite{deng2023divide}, Tox100/1K \cite{cai2024ethical} and a human-attribute dataset \cite{cai2024ethical} focused on bias research. 
For intellectual property protection, datasets like CelebA \cite{liu2015deep} and VGGFace2 \cite{cao2018vggface2} (facial datasets), DreamBooth \cite{ruiz2023dreambooth} and Pokemon Captions \cite{pinkney2022pokemon} (object datasets), and WikiArt \cite{saleh2015large} (artistic style dataset) are commonly used.

