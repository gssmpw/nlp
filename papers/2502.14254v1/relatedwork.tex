\section{Related Work}
\label{related}
Existing studies that leverage VLMs and LLMs for navigation can be categorized into the following directions.

\subsection{LLM-based Navigation}
These approaches often construct a global memory map based on image observations and use natural language to describe candidate points for navigation, with action decisions driven by large language models (LLMs).

Several methods fall within this category, including \textbf{LFG} \citep{shah2023navigation}, \textbf{VoroNav} \citep{wu2024voronav}, \textbf{ESC} \citep{zhou2023esc}, and \textbf{openFMNav} \citep{kuang2024openfmnav}. LFG uses frontier-based exploration and large language models to score potential subgoals and guide navigation based on the robot’s observations and exploration progress. VoroNav introduces Reduced Voronoi Graphs (RVGs) to optimize the robot’s exploration by identifying intersections that provide the best observational opportunities, while the LLM predicts the next best waypoint. ESC uses commonsense knowledge and frontier-based exploration to navigate toward objects in the environment, while openFMNav addresses challenges related to human instructions that imply target objects and zero-shot generalization. These methods employ LLMs to dynamically update a semantic map as the robot explores, enhancing memory and reducing redundant exploration.

While these methods offer the advantage of maintaining a global map and using high-level reasoning, they also face limitations. The language-based reasoning used for decision-making sacrifices high-dimensional semantic information, such as spatial and geometric details, which can constrain performance in complex environments. Furthermore, translating raw ego-view observations into abstract linguistic descriptions may weaken the model’s capacity for precise spatial reasoning.

\subsection{Value Map-based Navigation}
In this class of methods, a global value function is computed based on ego-view observations, and actions are chosen based on the generated value map instead of using VLMs for decision-making.

Notable approaches in this category include 
\textbf{VLFM} \citep{yokoyama2024vlfm} and \textbf{InstructNav} \citep{long2024instructnav}. VLFM uses a pre-trained vision-language model to generate a language-grounded value map, guiding the agent to explore optimal frontiers. InstructNav extends the idea of goal-directed navigation by introducing a Dynamic Chain of Navigation that breaks down tasks into sequences of actions and landmarks. These methods partially address memory forgetting by integrating global value maps, but they still face challenges. The value map is still constructed based on local observations, and decision-making driven by vision-language models (VLMs) often lacks a comprehensive global perspective. As a result, these approaches frequently lead to suboptimal solutions constrained by local decision-making.

\subsection{VLM-based Navigation}
These approaches directly leverage first-person perspective images as the input of vision-language models (VLMs) to generate action decisions. By using the spatial reasoning capabilities of VLMs, these methods enable the model to interpret complex environmental features from the robot’s current viewpoint, facilitating more informed and context-aware navigation decisions.

\textbf{CoNVOI} \citep{sathyamoorthy2024convoi} and \textbf{PIVOT} \citep{nasiriany2024pivot} exemplify approaches that process first-person images with VLMs to facilitate real-time navigation and decision-making. While effective in leveraging immediate visual inputs, these methods lack mechanisms for incorporating historical observations, often resulting in redundant exploration. This limitation poses challenges in long-horizon tasks, where maintaining contextual awareness of past actions is critical for efficient navigation.
\textbf{VLMNav} \citep{goetting2024end} addresses some of these limitations by integrating both RGB-D images and the robot’s pose information to construct a navigability mask that identifies reachable regions. The model incrementally builds a voxel-based map and refines its action proposals by prioritizing unexplored areas.

\textbf{NoMaD} \citep{sridhar2024nomad} unifies goal-directed navigation and exploration by using the robot's current image and the goal's image as input. The model includes a transformer backbone for processing visual data and a diffusion model for predicting action sequences. A binary mask is applied to the input to focus on either exploration (excluding the goal) or goal-reaching (including the goal). Despite its innovative design, NoMaD remains constrained by the absence of a global memory, relying solely on the most recent three observations. This limitation restricts its capacity for sustained long-term exploration.

Recent methods have sought to integrate VLMs more effectively for embodied navigation. \textbf{OpenIN} ~\citep{tang2025openin} focuses on navigation tasks where the robot must locate specific objects that have been moved, introducing a Carrier-Relationship Scene Graph (CRSG) to track objects and their locations. The system uses VLMs to process multimodal instructions and commonsense knowledge to guide navigation decisions.

\textbf{Uni-NaVid} ~\citep{zhang2024uni} takes a significant step toward unifying different navigation tasks in a single model. It processes both video streams and natural language instructions as input, creating a framework that can generalize across a range of navigation tasks. By training on diverse data, including video question answering and captioning tasks, Uni-NaVid improves its performance in real-world scenarios and enables asynchronous execution for efficiency.

These methods move toward integrating both global and local information more effectively, enabling the robot to navigate complex environments with a better understanding of spatial context. However, challenges remain in optimizing the trade-off between VLMs’ generalization capabilities and the need for precise, real-time navigation.