
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{enumitem}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % color
\usepackage{graphics}
\usepackage{multirow}
\usepackage[many]{tcolorbox} 
\usepackage{float} 
\usepackage{bbding}
\usepackage{cleveref}
\usepackage{graphics}
\usepackage{array}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}

\usepackage{subcaption}
\usepackage{listings}

\usepackage{soul}
\usepackage{authblk}

\title{Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation}

\author[1]{Lingfeng Zhang*}
\author[1]{Yuecheng Liu}
\author[1]{Zhanguang Zhang*}
\author[1]{Matin Aghaei}
\author[1]{Yaochen Hu}
\author[1]{Hongjian Gu}
\author[1]{Mohammad Ali Alomrani}
\author[1]{David Gamaliel Arcos Bravo}
\author[1]{Raika Karimi}
\author[1]{Atia Hamidizadeh}
\author[1]{Haoping Xu}
\author[1]{Guowei Huang}
\author[1]{Zhanpeng Zhang}
\author[1]{Tongtong Cao}
\author[1]{Weichao Qiu}
\author[1]{Xingyue Quan}
\author[1]{Jianye Hao}
\author[1]{Yuzheng Zhuang}
\author[1]{Yingxue Zhang*}
\affil[1]{Huawei Noah's Ark Lab}
\affil[ ]{\texttt{\{lingfeng.zhang1,zhanguang.zhang,yingxue.zhang\}@huawei.com}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments.
In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent’s egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.
\end{abstract}

\section{Introduction}
Embodied navigation is a crucial component of embodied artificial intelligence, with widespread applications in diverse scenarios such as  domestic environments, office settings, logistics and delivery, and factory inspections \citep{savva2019habitat, duan2022survey, zhang2022survey}. Its significance stems from its ability to enable agents to autonomously navigate and perform tasks within physical environments \citep{liu2024aligning, moller2021survey}.

Embodied navigation poses two key challenges. First, unlike autonomous driving, which typically occurs in structured outdoor environments, embodied navigation requires operating in diverse indoor and industrial settings such as factories, shopping malls, and offices. These spaces feature intricate layouts and obstacles, demanding advanced perception and planning \citep{thorisson2012cognitive, salvini2022safety, duan2022survey, mavrogiannis2022social}. Second, it necessitates a high degree of autonomy, as agents must adapt to unfamiliar environments without relying on predefined maps. They must interpret human instructions and dynamically interact with their surroundings to navigate effectively. This work focuses on Object Goal Navigation (ObjectNav), a task in which agents must locate specified objects within complex spaces \citep{batra2020objectnav, majumdar2022zson}.

In recent years, the rapid advancement of large language models (LLMs) has opened new possibilities for embodied navigation~\citep{zeng2023large, wu2024embodied}. These models enable robots to leverage commonsense reasoning, improving their understanding of natural language commands and enhancing the integration of perceptual data. This allows for navigation decisions that better align with human intentions \citep{wu2024voronav, li2024embodied}. Furthermore, recent ObjectNav research underscores the importance of historical information in improving environmental understanding, decision-making, and grounding navigation instructions \citep{fukushima2022object, wang2023gridmm}. This has led to the incorporation of memory systems into LLMs, such as episodic memory for past experiences and scene graph memory for structuring environmental data. However, because these memory systems often represent memories using natural language, which lacks geometric information, the spatial reasoning capacity of LLMs is compromised.

Alongside these advancements, there is growing interest in using images as a primary source of guidance by integrating foundation models with low-level planners \citep{cai2024bridging, nasiriany2024pivot}. This approach takes advantage of the advanced visual and language understanding of foundation models, offering an effective alternative to traditional map-based methods, which often rely on costly and disruption-prone depth sensing and localization. However, these methods predominantly rely on a first-person perspective without incorporating global memory into the decision-making process. As a result, they tend to lead to redundant exploration and reduced efficiency in complex environments.

In this paper, we propose a novel Vision-Language Model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module, which are then mapped to the agent’s ego-view visual observations. By integrating global contextual information with local perceptual inputs, our framework enables more informed action decisions, enhancing the agent’s situational awareness and decision-making capabilities. The approach significantly enhances the agent’s ability to navigate complex, long-horizon tasks by dynamically aligning global context with egocentric reasoning, offering a more effective and scalable solution for embodied navigation. The experimental results demonstrate that our proposed navigation pipeline outperforms state-of-the-art baselines. Through an ablation study, we verified the essential nature of each component of our method. Using our proposed data collection approach, the supervised fine-tuned Llama3.2-11B model exhibited superior performance compared to both the vanilla Llama3.2-11B model and GPT-4o.

\section{Related Work} \label{related}
Existing studies that leverage VLMs and LLMs for navigation can be categorized into the following directions.

\subsection{LLM-based Navigation}
These approaches often construct a global memory map based on image observations and use natural language to describe candidate points for navigation, with action decisions driven by large language models (LLMs).

Several methods fall within this category, including \textbf{LFG} \citep{shah2023navigation}, \textbf{VoroNav} \citep{wu2024voronav}, \textbf{ESC} \citep{zhou2023esc}, and \textbf{openFMNav} \citep{kuang2024openfmnav}. LFG uses frontier-based exploration and large language models to score potential subgoals and guide navigation based on the robot’s observations and exploration progress. VoroNav introduces Reduced Voronoi Graphs (RVGs) to optimize the robot’s exploration by identifying intersections that provide the best observational opportunities, while the LLM predicts the next best waypoint. ESC uses commonsense knowledge and frontier-based exploration to navigate toward objects in the environment, while openFMNav addresses challenges related to human instructions that imply target objects and zero-shot generalization. These methods employ LLMs to dynamically update a semantic map as the robot explores, enhancing memory and reducing redundant exploration.

While these methods offer the advantage of maintaining a global map and using high-level reasoning, they also face limitations. The language-based reasoning used for decision-making sacrifices high-dimensional semantic information, such as spatial and geometric details, which can constrain performance in complex environments. Furthermore, translating raw ego-view observations into abstract linguistic descriptions may weaken the model’s capacity for precise spatial reasoning.

\subsection{Value Map-based Navigation}
In this class of methods, a global value function is computed based on ego-view observations, and actions are chosen based on the generated value map instead of using VLMs for decision-making.

Notable approaches in this category include 
\textbf{VLFM} \citep{yokoyama2024vlfm} and \textbf{InstructNav} \citep{long2024instructnav}. VLFM uses a pre-trained vision-language model to generate a language-grounded value map, guiding the agent to explore optimal frontiers. InstructNav extends the idea of goal-directed navigation by introducing a Dynamic Chain of Navigation that breaks down tasks into sequences of actions and landmarks. These methods partially address memory forgetting by integrating global value maps, but they still face challenges. The value map is still constructed based on local observations, and decision-making driven by vision-language models (VLMs) often lacks a comprehensive global perspective. As a result, these approaches frequently lead to suboptimal solutions constrained by local decision-making.

\subsection{VLM-based Navigation}
These approaches directly leverage first-person perspective images as the input of vision-language models (VLMs) to generate action decisions. By using the spatial reasoning capabilities of VLMs, these methods enable the model to interpret complex environmental features from the robot’s current viewpoint, facilitating more informed and context-aware navigation decisions.

\textbf{CoNVOI} \citep{sathyamoorthy2024convoi} and \textbf{PIVOT} \citep{nasiriany2024pivot} exemplify approaches that process first-person images with VLMs to facilitate real-time navigation and decision-making. While effective in leveraging immediate visual inputs, these methods lack mechanisms for incorporating historical observations, often resulting in redundant exploration. This limitation poses challenges in long-horizon tasks, where maintaining contextual awareness of past actions is critical for efficient navigation.
\textbf{VLMNav} \citep{goetting2024end} addresses some of these limitations by integrating both RGB-D images and the robot’s pose information to construct a navigability mask that identifies reachable regions. The model incrementally builds a voxel-based map and refines its action proposals by prioritizing unexplored areas.

\textbf{NoMaD} \citep{sridhar2024nomad} unifies goal-directed navigation and exploration by using the robot's current image and the goal's image as input. The model includes a transformer backbone for processing visual data and a diffusion model for predicting action sequences. A binary mask is applied to the input to focus on either exploration (excluding the goal) or goal-reaching (including the goal). Despite its innovative design, NoMaD remains constrained by the absence of a global memory, relying solely on the most recent three observations. This limitation restricts its capacity for sustained long-term exploration.

Recent methods have sought to integrate VLMs more effectively for embodied navigation. \textbf{OpenIN} ~\citep{tang2025openin} focuses on navigation tasks where the robot must locate specific objects that have been moved, introducing a Carrier-Relationship Scene Graph (CRSG) to track objects and their locations. The system uses VLMs to process multimodal instructions and commonsense knowledge to guide navigation decisions.

\textbf{Uni-NaVid} ~\citep{zhang2024uni} takes a significant step toward unifying different navigation tasks in a single model. It processes both video streams and natural language instructions as input, creating a framework that can generalize across a range of navigation tasks. By training on diverse data, including video question answering and captioning tasks, Uni-NaVid improves its performance in real-world scenarios and enables asynchronous execution for efficiency.

These methods move toward integrating both global and local information more effectively, enabling the robot to navigate complex environments with a better understanding of spatial context. However, challenges remain in optimizing the trade-off between VLMs’ generalization capabilities and the need for precise, real-time navigation.

\section{Method} \label{sec:method}


\subsection{Problem Formulation}
In this work, we focus on the object navigation (ObjNav) task, where an agent begins at a random location within an unseen environment and is tasked with finding and navigating to a target object, denoted by $g$. The agent has no access to a pre-built map and must rely entirely on its sensory inputs for navigation. At each time step $t$, the agent captures an egocentric RGB-D image, denoted by $o^t$, from its onboard RGB-D camera. Additionally, the agent has access to its current location and orientation, which are represented by the extrinsic matrix $M_{\text{ext}}$ of the camera. Using these inputs, the agent must compute and execute a low-level action, $a^t$, that efficiently guides it toward the target object.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/Workflow_v6.pdf}
    \caption{Workflow of our proposed method. Our method maintains three types of memories and project cues from them onto the egocentric images for goal location prediction. Further details are provided in Section~\ref{sec:method}}
    \label{fig:workflow}
\end{figure*}

The workflow of our proposed method is illustrated in Figure~\ref{fig:workflow}. The VLM-based navigation relies on the integration of a memory module that encompasses three distinct types of memories. The construction and maintenance of this memory module, as well as the VLM-based navigation process, will be discussed in detail in the subsequent sections.

\subsection{Memory Construction} \label{sec:method_mem}
The memory module is composed of three distinct types of memories, each serving a different purpose:

\begin{itemize}
    \item \textbf{Frontier Map}: Denoted as $M_f$, frontier map has been proven to be effective for environment exploration in object navigation tasks, as demonstrated in \citep{shah2023navigation, zhou2023esc}. We adopt an approach similar to that used in ESC~\citep{zhou2023esc} to construct the frontier map. Using the agent's position and camera parameters, RGB-D images are transformed into 3D space, where each 2D pixel is mapped to a 3D voxel in the global coordinate system. Voxels located near the floor, with no obstacles along the height dimension, are classified as free space. A frontier in this map is defined as the boundary between free and unexplored areas. This frontier map is maintained throughout the navigation task. 
        
    \item \textbf{Landmark Semantic Memory}: Denoted as $M_l$, this memory stores descriptions of the landmarks that the agent has seen in the past. Each entry includes the global coordinates of the landmark and a description of the nearby semantic information, such as objects or decoration texture. For example: "\textit{[13.2, 5.4]: Located on the floor near a sink. There is a bath tub nearby.}". The description of each landmark is generated by the VLMs, as explained in Section~\ref{sec:method_dec}.

    
    \item \textbf{Visitation Memory}: Denoted as $M_v$, this memory keeps track of the landmarks that the agent has already visited. 

    By maintaining a record of visited locations, the visitation memory serves as a crucial mechanism to prevent redundant exploration and improve overall exploration efficiency.
\end{itemize}

\subsection{Mem2Ego Navigation} \label{sec:method_dec}
At each time step $t$, given the image-based observation $o^t$ and the three types of memories—$M_f^t$, $M_l^t$, and $M_v^t$—introduced in section~\ref{sec:method_mem}, the proposed memory-to-egocentric (Mem2Ego) navigation process can be formulated as follows:
\begin{equation}
a^t = f_\theta (o^t, M^t_f, M^t_l, M^t_v, g)
\end{equation}

Further details are provided in the following sections. 

\subsubsection{Panoramic Observation Generation}
After the environment is initialized or the agent reaches a new location, the agent captures four egocentric RGB-D images by rotating its viewpoint 90 degrees at each step. These images are then stacked to construct a 360-degree panoramic observation $o_{\text{pano}}^t$ (see Equation~\ref{equation:obs-generation}), offering a comprehensive representation of the surrounding environment. Compared to navigation methods relying on a single egocentric view, this panoramic approach enhances the agent’s spatial awareness and scene understanding. A similar strategy has been employed in \citep{long2024instructnav}.
\begin{equation}
    o_{\text{pano}}^t = \text{Concatenate}([o_{0}^t, o_{\pi /2}^t, o_{\pi}^t, o_{3\pi /2}^t])
    \label{equation:obs-generation}
\end{equation}

\subsubsection{Frontier and Visitation Memory Projection}
Based on the agent's position and the newly captured depth images, the navigation map and corresponding frontiers are updated following the method outlined in Section~\ref{sec:method_mem}. Candidate locations, denoted as $[\mathbf{C}_1,...,\mathbf{C}_N]$ in Equation~\ref{equation:candidates-generation}, are generated by combining frontier clustering and grid-based sampling. The centroid of each frontier segment is computed by clustering all points within the segment. However, using the centroid directly as a candidate may result in unreachable goal positions. To mitigate this, we identify the nearest grid point on the floor area to the centroid, ensuring that the candidate is accessible to the agent. Similarly, visited locations, $[\mathbf{V}_1,...,\mathbf{V}_M]$, are extracted from the visitation memory $M^t_v$, as shown in Equation~\ref{equation:visitation-extraction}. 
\begin{equation} 
    [\mathbf{C}_1,...,\mathbf{C}_N] = \text{CandidatesGeneration}(M_f^t)
    \label{equation:candidates-generation}
\end{equation}
\begin{equation}
    [\mathbf{V}_1,...,\mathbf{V}_M] = \text{VisitationExtraction}(M_v^t)
    \label{equation:visitation-extraction}
\end{equation}
Once determined, the global coordinates of these candidates and visitations are projected onto the egocentric image plane as pixel locations $[\mathbf{c}_1,...,\mathbf{c}_N]$ and $[\mathbf{v}_1,...,\mathbf{v}_M]$, as shown in Equation~\ref{equation:projection}, where $K$ and $M_{\text{ext}}$ represent the camera intrinsics and extrinsics, respectively.
\begin{equation}
\begin{aligned}
    \mathbf{c}_i=\text{Projection}(&\mathbf{C}_i),\quad \mathbf{v}_i=\text{Projection}(\mathbf{V}_i) \\
     \text{where}\hspace{0.1cm} \mathbf{c}_i=(x_i, y_i), \hspace{0.1cm} \mathbf{C}_i=& (X_i, Y_i, Z_i),\hspace{0.1cm}\text{similar for $\mathbf{v}_i$ and $\mathbf{V}_i$} \\
     [x_i',y_i',w_i]^T &= K\cdot M_{\text{ext}}\cdot [X_i,Y_i,Z_i,1]^T \\
     (x_i,y_i)&=(\frac{x_i'}{w_i},\frac{y_i'}{w_i}) 
\end{aligned}
\label{equation:projection}
\end{equation}
An annotation function is then applied to map these locations onto the panoramic observation $o_{\text{pano}}^t$, resulting in an annotated observation $o_{\text{anno}}^t$, as outlined in  Equation~\ref{equation:annotate}. In the annotated image, candidate locations are depicted as green circles, each labeled with a unique identifier corresponding to its position in the image. Similarly, visited locations are marked as blue circles, but only if they are visible within the current view. 
\begin{equation}
    o_{\text{anno}}^t = \text{AnnotateImage}(o_{\text{pano}}^t, [\mathbf{c}_1,...,\mathbf{c}_N], [\mathbf{v}_1,...,\mathbf{v}_M])
\label{equation:annotate}
\end{equation}

\subsubsection{Landmark Memory Retrieval}
The panoramic image, augmented with frontier candidates, highlights potential navigation targets within the agent's immediate field of view. However, it is common that no suitable targets are visible, and more promising options may exist among the landmarks the agent has previously encountered but not yet explored. These previously encountered landmarks are stored in the dynamic landmark semantic memory $M_l^t$. To manage the rapid expansion of this memory during navigation, we utilize large language models (LLMs) to retrieve the top-$k$ landmarks most relevant to the target object. This retrieval process generates an additional observation from memory, $o_{\text{mem}}^t$, which is then incorporated into the decision-making process. The prompt used for memory retrieval is detailed in Appendix~\ref{sec:infer_prompt}.

\begin{equation}
    o_{\text{mem}}^t = \text{MemoryRetrieval}_{\text{LLMs}}(M_l^t, k)
\end{equation}

\subsubsection{Memory Augmented Decision Making} \label{sec:marker_sel}
At this stage, the panoramic image with annotations, $o_{\text{anno}}^t$, along with the top-$k$ landmarks retrieved from memory, $o_{\text{mem}}^t$, is used to query VLMs to select the next target location to visit (described in Equation~\ref{equation:decision-making}). The VLMs are tasked with identifying the marker on the image most likely to lead to the target object, while avoiding markers that are too close to previously visited locations. To ensure consistency in the output format, the top-$k$ landmarks are numbered, and their descriptions are considered only if no suitable marker is identified directly from the panoramic image. A Chain-of-Thought (CoT) prompting strategy is employed to guide the VLMs in generating a structured reasoning process before producing a single numerical output corresponding to the selected marker. The full prompt used for decision-making is provided in Appendix~\ref{sec:infer_prompt}.

\begin{equation}
    a^t = f_{\text{VLMs}} \left(\text{prompt}(g), o_{\text{anno}}^t, o_{\text{mem}}^t\right)
\label{equation:decision-making}
\end{equation}

\subsubsection{Action Execution} \label{sec:act_exec}
The marker selected in step~\ref{sec:marker_sel} is transformed to the global coordinate system to determine the global coordinates of the target location. Shortest path follower provided by habitat simulator is then executed to navigate agent to the target location while avoiding obstacles. Object detection is performed each time the agent moves or adjusts its viewing angle. The task is deemed successful if the target object is detected within the agent's field of view and the agent successfully navigates to the target object's viewpoints provided by the Habitat dataset. If the target object is not detected, the process continues until the agent either reaches the designated viewpoints or exceeds the maximum allowed number of exploration steps.

\subsubsection{Memory Update}
While only one landmark from the current view is selected as the next-step navigation target, other landmarks may still be valuable for future exploration. The landmark semantic memory is updated before target position navigation described in Section~\ref{sec:act_exec}. VLMs are prompted to describe the surrounding environment near each marker annotated on the panoramic image. The output from the VLMs includes a list of marker IDs paired with corresponding descriptions. The marker IDs are then converted to global coordinates and, together with their descriptions, saved to the landmark semantic memory for use in future exploration processes. The prompt used for landmark description is provided in Appendix~\ref{sec:infer_prompt}. 

Meanwhile, the navigation map is updated along the navigation process, using the RGB-D images captured along the way. Additionally, the agent's most recent location is added to the visitation memory to facilitate future exploration.

\subsection{Data Collection and Model Finetuning} \label{sec:method_sft}

To enhance the capabilities of open-sourced VLMs and narrow their performance gap with GPT-4o, we design a pipeline to collect training data for supervised finetuning (SFT). The data collection pipeline is illustrated in Figure~\ref{fig:data_collect}. To improve data diversity and validate the generalization ability of the model, we gather 40 new categories of objects from the HSSD dataset, rather than using the original 6 categories provided. First, new target objects are sampled from the HSSD scenes. For each frame of data, ground-truth trajectories from the current position to these targets are calculated based on the $A^*$ algorithm and subsequently smoothed using Bézier curves. Egocentric images and the corresponding ground-truth target pixel $(x, y)$ (defined as the endpoint of the ground-truth trajectory shown in the image) for each image are saved. To construct the multiple marker annotated image that VLMs encounter in the marker selection task, we generate a few candidate landmarks for each image by sampling from the edge of the floor area. Both ground-truth and sampled candidate landmarks are annotated on the egocentric image in the same way as in the Section~\ref{sec:method}. 

We collect two types of data for VLM fine-tuning: \textbf{marker description} and \textbf{target marker selection with rationale}. To generate marker description data, we use GPT-4o to describe the surrounding environment of each marker on the image. For example, ``\textit{Marker Number: 1 Description: Positioned near a dining chair...; Marker Number: 2 ...}''. Each target marker selection data entry includes both a rationale and the ID of the selected marker. To ensure a robust rationale, we utilizes egocentric images annotated with the ground-truth trajectory and employ a dual-phase prompting strategy: first, GPT-4o is prompted to describe all the objects along the ground-truth trajectory, then to predict the location of the target marker based on its relationship to these objects. Importantly, the rationale generated by GPT-4o must not reference the ground-truth trajectory itself; the trajectory is only used to guide the generation of the rationale. The generated rationale is then automatically validated using GPT-4o, assessing both the accuracy of detected objects and the correctness of the rationale. This dual-phase prompting strategy has proven to be more reliable than a single-phase prompting approach. The prompts used for rationale generation are provided in Appendix~\ref{sec:sft_data_prompt}. The validated rationale is then concatenated with the ground-truth marker ID to enforce a CoT-like thinking process. An example of the resulting response is "\textit{Think: The candle is most likely located on the shelf on the right side ... Action: 2}". Note that the resulting marker selection data used for model fine-tuning relies on images annotated with numerical markers, rather than those annotated with the ground-truth trajectory. In total, we generated 30,352 visual question answer (VQA) pairs of data from 104 scenes and 5678 object navigation tasks. This data was used to fine-tune a Llama3.2-11B-Vision model~\citep{grattafiori2024llama3herdmodels} following the configuration recommended by official Llama repository. The model was fine-tuned for 3 epochs with a learning rate of 1e-5 and an effective batch size of 128. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Data_collection_v2.pdf}
    \caption{Pipeline of SFT data collection. The ground-truth trajectory and floor edge are used to extract target marker and candidate markers, respectively. Marker description and selection data is generated for model fine-tuning.}
    \label{fig:data_collect}
\end{figure}

\section{Experiments}

\subsection{Experimental Setup} \label{sec:exp_setup}

We evaluated our method on the navigation tasks using the Habitat 3.0~\citep{puig2023habitat} simulation platform. We adopt similar setup as the Habitat ObjectNav 2022 challenge~\citep{habitatchallenge2022} for all the experiments. The action space of the agent consists of: \texttt{STOP}, \texttt{MOVE\_FORWARD}, \texttt{TURN\_LEFT}, \texttt{TURN\_RIGHT}, with a forward movement distance of 0.25 meters and a turning angle of 30 degrees per step. For low-level movement control, we utilized Habitat’s built-in shortest-path follower. The maximum number of steps allowed per task is set to 500 by default. Due to limitations in the image quality within the Habitat environment and the suboptimal performance of state-of-the-art perception modules, such as GroundingDINO~\citep{liu2025grounding}, we opted for Habitat’s built-in semantic ground truth with object size conditions as the perception module.  In this context, we can assume that the perception module is sufficiently effective. The LLMs and VLMs used in this study was GPT-4o and Llama3.2-11B.

\subsection{Datasets}

Our method is evaluated on the following two object navigation datasets:

\begin{itemize}
    \item \textbf{Habitat Synthetic Scenes Dataset (HSSD)~\citep{khanna2024habitat}:} We use the HSSD validation dataset to evaluate our method. HSSD consists of 41 scenes and six object goal categories: \texttt{chair}, \texttt{couch}, \texttt{potted plant}, \texttt{bed}, \texttt{toilet}, and \texttt{tv}. To ensure task diversity, we select only one episode per scene-object pair. After filtering out erroneous episodes—such as cases where the agent's initial position was in mid-air—the final number of evaluated episodes is 213.
    
    \item \textbf{HSSD-Hard:} Since some HSSD episodes are relatively easy, with the agent finding the target object in just a few steps, we created a more challenging dataset, HSSD-Hard, by selecting HSSD episodes with longer search distance. We calculated the geodesic distance from the agent's starting point to the target object for each episode and selected the top 50\% of episodes with the longest searching distances to form the HSSD-Hard dataset.  The total number of episodes in HSSD-Hard is 102.
    
\end{itemize}

\subsection{Baselines}
We compare our method against the following state-of-the-art (SOTA) baselines that represent different strategies to address the object navigation problem:
\begin{itemize}
    \item \textbf{PIVOT~\citep{nasiriany2024pivot}:} This approach casts the navigation task as an iterative visual question answering problem by annotating the image with numerical markers that represent the navigation subgoals. The method is adapted for the HSSD object navigation tasks. Without a frontier map, visitation memory, and landmark semantic memory, our proposed navigation pipeline degenerates to PIVOT.
    \item \textbf{LFG~\citep{shah2023navigation}:} This method employs frontier-based exploration and LLMs to score potential subgoals and guide the navigation.
    \item \textbf{VLFM~\citep{yokoyama2024vlfm}:} The approach utilizes VLMs to generate a language-grounded value map,  from which the location with the highest value is selected as the next subgoal for navigation.
    \item \textbf{InstructNav~\citep{long2024instructnav}:} InstructNav introduces a Dynamic Chain of Navigation, breaking down navigation tasks into sequences of actions and landmarks. It employs four value maps, each with different semantic representations, to assist in selecting the appropriate landmark.
    \item \textbf{VLMNav~\citep{goetting2024end}:} This approach relies on a voxel map built from RGB-D images and the agent's pose to narrow down action space. 
\end{itemize}
To ensure a fair comparison, all experimental setups are conducted under the same conditions described earlier.

\subsection{Metrics}
We employ the following metrics to evaluate the performance of all the methods:
\begin{itemize}
    \item \textbf{Success Rate (SR):} A task was deemed successful when the distance between the agent and any viewpoint of the target object was less than 0.2 meters.
    \item \textbf{Success Weighted by Path Length (SPL)~\citep{anderson2018evaluation}:} This metric evaluates how efficient the agent's path is compared to the optimal path. SPL is calculated as:
    \begin{equation}
        SPL = \frac{1}{N}\sum_{i=1}^N S_i \frac{l_i}{max(p_i, l_i)}
    \end{equation}
    where $l_i$ is the length of the optimal path for episode $i$. $p_i$ is the length of path taken by the agent. $S_i$ is the binary indicator of success in episode $i$.
\end{itemize}

\subsection{Main Results}

\begin{table}[t]
\caption{Main results. Our proposed method is compared to the baselines on HSSD and HSSD-Hard datasets. SR: Success Rate. SPL: Success Weighted by Path Length. All experiments are conducted using gpt-4o.}\label{tab:main}
\begin{tabularx}{\columnwidth}{@{}XXXXX@{}}
\toprule
            & \multicolumn{2}{c}{HSSD} & \multicolumn{2}{c}{HSSD-Hard} \\ 
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}
            & SR $\uparrow$ & SPL $\uparrow$ & SR $\uparrow$ & SPL $\uparrow$ \\ \midrule
LFG         & 0.6244       & 0.3371      & 0.6176         & 0.3454         \\
VLMNav      & 0.6526       & 0.3620      & 0.5294      & 0.1973          \\
InstructNav & 0.7605       & 0.3722      & 0.6372        &  0.4187       \\
VLFM        & 0.7652       & 0.5574      & 0.6078         & 0.4270         \\
PIVOT       & 0.7840       & 0.5658      & 0.6372         & 0.4744         \\
Ours        & \textbf{0.8685}       & \textbf{0.5788}      & \textbf{0.7647}         & \textbf{0.4790}         \\ \bottomrule
\end{tabularx}
\end{table}

We evaluate our method and all baselines on both HSSD and HSSD-Hard datasets using the same setup described in Section~\ref{sec:exp_setup}, with results summarized in Table~\ref{tab:main}. Performance is evaluated based on Success Rate (SR) and Success weighted by Path Length (SPL). While SR indicates the overall ability to find the target object, SPL measures the efficiency of the navigation process. Notably, these two metrics are not correlated, as a method can achieve a high SR by sacrificing navigation efficiency.  As shown in Table~\ref{tab:main}, on the HSSD dataset, our proposed method achieves an SR of 0.8685 and an SPL of 0.5788, both of which are higher than all the baseline methods. 

Compared to HSSD, tasks in the HSSD-Hard dataset are more challenging due to the relatively longer search distance, requiring additional steps to locate target objects. As shown in Figure~\ref{tab:main}, the performance of all methods decreases on the HSSD-Hard dataset, though the impact varies by model. Notably, our method demonstrate an even greater advantages in these more difficult scenarios, achieving an SR that is 12.75\% higher than the second-best baseline (PIVOT). Additonally, our method outperforms others in SPL as well, further highlighting its efficiency. These results underscore the effectiveness and robustness of our approach in tackling challenging navigation tasks. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/episode_gain.png}
    \caption{Reduction of steps per episode on the HSSD-Hard dataset. For each episode, we compute the difference in the number of steps required between our method and the designated baseline. A positive value (blue color) indicates a reduction in steps when using our method, while a negative value (orange color) signifies an increase. Episode IDs are sorted based on these delta values.}
    \label{fig:episode_gain}
\end{figure}

Figure~\ref{fig:episode_gain} illustrates the difference in the number of steps required between our method and the baselines for each episode. For failed episodes, the step count is set to the maximum allowed limit (500 in our case). As shown in the figure, our method reduces or at least matches the number of steps in most episodes, highlighting its superior efficiency in challenging object navigation tasks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Case_study.pdf}
    \caption{Demonstration of memory-augmented decision making process from a real HSSD episode. The full prompts and GPT-4o response are provided in Appendix~\ref{sec:app_case_study}}
    \label{fig:good_case_1}
\end{figure}

Figure~\ref{fig:good_case_1} illustrates the memory-augmented decision-making process from a real HSSD episode. VLMs, such as GPT-4o, analyze all memory cues on the image before reasoning about the most likely location of the target object and selecting the next marker to explore. The full prompt and responses for this case are provided in Appendix~\ref{sec:app_case_study}. 

Most failed cases with our method result from reaching the maximum allowable number of steps. This can occur due to the VLM selecting a suboptimal position or the task being inherently challenging. Additionally, we occasionally observe that even state-of-the-art VLMs like GPT-4o can exhibit visual hallucinations~\citep{liu2024survey}, where they select a marker ID that does not appear in the image or prompt. We have provided two examples in the Appendix~\ref{sec:visual_hallucinations}.

\begin{table}[h]
\caption{Results with different VLM models. The fine-tuned Llama3.2-11B model (SFT Llama3.2-11B) trained on our collected data outperforms GPT-4o, achieving the best overall performance.} \label{tab:sft_res}
\begin{tabularx}{\columnwidth}{@{}lXXXX@{}}
\toprule
            & \multicolumn{2}{c}{HSSD} & \multicolumn{2}{c}{HSSD-Hard} \\ 
            \cmidrule(lr){2-3}\cmidrule(lr){4-5}
            & SR $\uparrow$ & SPL $\uparrow$ & SR $\uparrow$ & SPL $\uparrow$ \\ \midrule
GPT-4o               & 0.8685 & 0.5788 & 0.7647 & 0.4790        \\
Vanilla Llama3.2-11B & 0.7511      & 0.5582     & 0.7352        & 0.4626        \\
SFT Llama3.2-11B      & \textbf{0.8732} & \textbf{0.5995} & \textbf{0.7843} & \textbf{0.5274} \\ \bottomrule
\end{tabularx}
\end{table}

\subsection{VLM Model Supervised Fine-tuning (SFT)}
To evaluate the impact of the VLM used, we assess the performance of various VLMs within our proposed method. As shown in Table~\ref{tab:sft_res}, the vanilla Llama3.2 model performs significantly worse than GPT-4o. Given the substantial differences in model size and training data, it is not surprising that smaller open-source VLMs like Llama3.2-11B underperform compared to state-of-the-art proprietary models such as GPT-4o. Failure analysis reveals that Llama3.2-11B is more prone to visual hallucinations and struggles with instruction following, particularly in marker selection and description tasks. This may stem from a lack of relevant training data for Llama3.2-11B, limiting its ability to generalize effectively in these scenarios. 

To improve the performance of VLMs, we collected over 30,000 VQA samples by generating data from 40 new object categories within the HSSD dataset. Rationales generated using a dual-phase prompting strategy are used to guide the marker selection process. We then fine-tuned Llama3.2-11B following the approach detailed in Section~\ref{sec:method_sft}. As shown in Table~\ref{tab:sft_res}, the performance of Llama3.2 is improved significantly after supervised fine-tuning (SFT), surpassing GPT-4o in both SR and SPL metrics on the HSSD and HSSD-Hard datasets. These results are particularly promising, considering that Llama3.2-11B is substantially smaller than GPT-4o (11B vs. an estimated 175B) and more cost-effective to train and deploy. This highlights the effectiveness of our data collection strategy and fine-tuning approach. This performance improvement can be attributed to enhanced instruction adherence and a more effective reasoning process grounded in the given environment.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/sft_results_v2.png}
    \caption{Results for different maximum steps. Experiments are conducted on the HSSD dataset. While SFT Llama3.2-11B consistently achieves the best performance, its advantage over the other two models is most pronounced at 300 maximum steps.}
    \label{fig:diff_max_steps}
\end{figure}

Numerous factors influence the performance of the navigation pipeline, particularly the maximum number of allowed steps. Consequently, we assessed the impact of this factor on performance. As shown in Figure~\ref{fig:diff_max_steps}, with the maximum number of steps increasing, both the Success Rate (SR) and Success weighted by Path Length (SPL) improve for each model. When the maximum number of steps is relatively low, such as 200 or fewer, all methods exhibit suboptimal performance, with only minor differences between models. This is likely because the limited step count prevents the full exploitation of each model's capabilities. Conversely, when the maximum number of steps is large, such as 500 or more, all models reach their respective performance limits, making the differences between them less distinct. It is noteworthy that the performance gap between different models becomes more apparent when the maximum number of steps is set to 300 or 400.

\subsection{Ablation Study}

To assess the contribution of each component in our proposed method, we conducted an ablation study by evaluating its performance without landmark semantic memory or visitation memory. As shown in Table~\ref{tab:ablation_study}, removing either of these memory modules leads to a decline in performance. The frontier map could not be excluded from this analysis, as it is essential for marker generation. The absence of visitation memory results in redundant exploration in some cases, thus reducing both the success rate and navigation efficiency. Meanwhile, without landmark semantic memory, the agent is unable to select a navigation goal globally when no suitable marker is present in its current view, which harms the performance as well. These findings highlight the crucial role of both memory modules in fully leveraging the potential of our proposed method.

\begin{table}[h]
\caption{Ablation study. Removing any of the main components of our design leads to degraded performance on HSSD dataset.} \label{tab:ablation_study}
\begin{tabularx}{\columnwidth}{@{}lXXXX@{}}
\toprule
& \multicolumn{2}{c}{HSSD} & \multicolumn{2}{c}{HSSD-Hard} \\ 
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
& SR $\uparrow$ & SPL $\uparrow$ & SR $\uparrow$ & SPL $\uparrow$ \\ \midrule
Ours & \textbf{0.8685} & \textbf{0.5788} & \textbf{0.7647} & 0.4790 \\
Ours w/o Visitation Memory & 0.8450 & 0.5761 & 0.7450 & \textbf{0.4961} \\
Ours w/o Landmark Semantic Memory & 0.8356 & 0.5669 & 0.7352 & 0.4795 \\ \bottomrule
\end{tabularx}
\end{table}

\section{Limitations}

In our current research, we utilize the VLM to characterize the surrounding environment of each marker and subsequently store the textual response in the landmark semantic memory. This approach heavily relies on the spatial understanding and reasoning capabilities of the VLM, and may result in the potential loss of important semantic information. Additionally, it is worthwhile to explore methods for storing egocentric images in memory and employing the VLM to efficiently process multiple egocentric images.

\section{Conclusion}

This study proposes an efficient fusion strategy that integrates task-relevant global memory information with first-person perspective information, thereby overcoming the suboptimal solution problem associated with existing multimodal navigation frameworks due to local observability. Moreover, this method can simultaneously activate and utilize the complex spatial understanding, reasoning, and commonsense reasoning capabilities of VLMs, thus significantly enhancing the ability and efficiency of navigation decisions in complex spatial scenarios. Theoretically, enhanced spatial cognitive abilities can reduce the required travel distance and number of actions, thereby increasing the task completion success rate and overall efficiency of the navigation scheme.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}


\subsection{Prompts for inference} \label{sec:infer_prompt}

\begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Prompt 1}}, breakable]
\ttfamily
\raggedright
You are an automated system with the capability to analyze the provided image. 
Based on the numerical markers present in the image, please describe the surrounding environment relative to each marker's position.
Ensure that descriptions of different markers are distinct to maintain the uniqueness of each marker. 
The marker number should not appear in the description.
Please adhere to the following format:

Marker Number: [insert the number of the first marker here]
Description: [provide a description corresponding to the first marker here]

Marker Number: [insert the number of the second marker here]
Description: [provide a description corresponding to the second marker here]

...

Marker Number: [insert the number of the last marker here]
Description: [provide a description corresponding to the last marker here]
\rmfamily
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Prompt 2}}, breakable]
\ttfamily
\raggedright
Based on the provided descriptions for each number, please select at most three number whose corresponding descriptions are most likely to help identify the \textcolor{orange}{\{goal\_object\}}.
\textcolor{orange}{\{Number: Description; Number: Description; ... Number: Description\}}
If the total number is less than 3, please use -1 to occupy the empty position.
Please adhere to the following format for the output:

Number List: [first number, second number, third number]
\rmfamily
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Prompt 3}}, breakable]
\ttfamily
\raggedright
You are a robot and after 360 degrees observation, you can see the given panorama image. The panorama image combines 4 images from different angles.
Your task is to find the \textcolor{orange}{\{goal\_object\}}.
Based on the numerical markers in the image, select one of these numbers to move next. 
If you're not confident in moving to the marker to find the \textcolor{orange}{\{goal\_object\}}, you can choose one of the numerical markers located outside of this image. 
The descriptions of these markers are as follows:

\textcolor{orange}{\{the top 3 numbers with descriptions, corresponding to the response of Prompt 2\}}. 

If you're still not confident in moving to the marker to find the \textcolor{orange}{\{goal\_object\}}, your action should be 'None'.
The blue circle marker on the floor indicates the previously explored position. It is better to choose a numeric marker that is not close to the blue circle marker.
Please note all closed doors cannot be opened.
Please follow the format like this,

Thought: [put your step-by-step thinking process here]

Action: [put a single marker id or None here]
\rmfamily
\end{tcolorbox}

\subsection{Prompts for SFT data generation} \label{sec:sft_data_prompt}

\begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Prompt for dual-phase rational generation}}, breakable]
\ttfamily
\raggedright
You are given an image with a red movement trajectory on it. Please first identify the objects near the red line in the given image. If there is no red trajectory in the image, please directly return "None". Second, knowning that \textcolor{orange}{\{goal\_object\}} could be found after following the red trajectory, you need to predict the location of {goal\_object} or the region where \textcolor{orange}{\{goal\_object\}} could be most likely located. This can be achieved by reasonably imagining the unseen areas after the red trajectory based on the room layout.
**Do not mention the red trajectory/line or "the image" in your output!**
Please structure your output in the following way:\\
OBJECTS\_RED\_LINE: \\
LOCATION\_PREDICTION\_AND\_REASONING:
\rmfamily
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Prompt for rationale filtering}}, breakable]
\ttfamily
\raggedright
You are given an image with a movement trajectory marked in a red line. Please first verify if all of the objects in a given list are present near the red line in the given image. If there is no red line in the image or any of the objects not present, please ignore the rest and directly return "NONE".
Second, verify if the reasonings of why \textcolor{orange}{\{goal\_object\}} may be put close to the objects in the list. A good reasoning should be logical and perfectly reflect common sense knowledge. A good reasoning gives convincing reasons while a bad reasoning gives vague or untruthful reasons. If the reasonings are good, output "GOOD REASONINGS", otherwise, output "BAD REASONINGS".
Example of a good reasoning: "The book is most likely located on the shelves in the background. The shelves are a common place for storing books, and they are visible in the room at the end of the path.".
Example of a bad reasoning: "The mirror is most likely located on the dark wall to the right of the doorway. This is inferred from the visible portion of the mirror reflecting the room, indicating its position on the dark wall.".
Please structure your output in the following way:\\
OBJECTS\_PRESENCE\_CHECK: \\
REASONING\_CHECK: \\
Object list: {object\_list} \\
Reasonings: {reasonings} \\
\rmfamily
\end{tcolorbox}

\subsection{Case Study} \label{sec:app_case_study}

    \begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Prompt and response for case study}}, breakable]
\ttfamily
\raggedright
\textbf{Prompt:} You are a robot and after 360 degrees observation, you can see the given panorama image. The panorama image combines 4 images from different angles.
Your task is to find the tv.
Based on the numerical markers in the image, select one of these numbers to move next.
If you're not confident in moving to the marker to find the tv, you can choose one of the numerical markers located outside of this image.
The descriptions of these markers are as follows: 
97: The environment near this marker is a well-furnished bedroom featuring a bed with grey and white bedding. There is a grey headboard attached to the wall, from which two contemporary pendant lights hang. Next to the bedside is a white nightstand with drawers. In the background, large windows or glass doors bring light into the room. To the far right, through the doorway, part of an adjacent room is visible where a small trolley or shelf holding white objects can be seen.
98: The marker is located near the doorway of a room. To the left of the marker, there is a green potted plant. Through the doorway, the room is furnished with a bed positioned near the wall with large windows that extend almost to the floor, allowing for an expansive view of the outside. Additionally, there is a chair or possibly a sofa situated to the left inside the room. The floor is covered with a dark blue surface adding a touch of color to the environment.
If you're still not confident in moving to the marker to find the tv, your action should be 'None'.
The blue circle marker on the floor indicates the previously explored position. It is better to choose a numeric marker that is not close to the blue circle marker.
Please note all closed doors cannot be opened.
Please follow the format like this,
Thought: [put your step-by-step thinking process here]
Action: [put a single marker id or None here]

\textbf{Response:}
Thought: I need to find the TV in this panorama image. The available markers inside the image are 0, 1, 2, 3, and 4. The markers outside of the image, specifically 97 and 98, describe a bedroom and another room with a bed. Since a TV is more likely to be in a living room or common area rather than a bedroom, and given that we're currently in what appears to be a common area or living room (as indicated by the dining area and kitchen), it makes more sense to explore the nearby markers in the current area.
Marker 0: Leads to a kitchen area, not likely to have a TV.
Marker 1: Near a closed door with a blue circle, so less likely.
Marker 2: In the direction of a long room which might be a living room and could have a TV.
Marker 3: Looks like another part of the same long room, potentially a living area.
Marker 4: Seems to be in the dining area, also less likely to have a TV.
Action: 2
\rmfamily
\end{tcolorbox}

\subsection{Visual Hallucinations} \label{sec:visual_hallucinations}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/visual_hallucination_1.png}
    \caption{Visual Hallucination Example 1}
    \label{fig:visual_hallucination_1}
\end{figure}

\begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Visual Hallucination Example 1}}, breakable]
\ttfamily
\raggedright
Thought: 1. The top-left, bottom-left, and bottom-right sections of the panorama show closed doors and walls. 2. The top-right section shows an open doorway leading to another room that might be more likely to have a toilet. 3. \textcolor{red}{The numeric marker "2" is located near the open doorway in the top-right section. Action: 2}
\rmfamily
\end{tcolorbox}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/visual_hallucination_2.png}
    \caption{Visual Hallucination Example 2}
    \label{fig:visual_hallucination_2}
\end{figure}

\begin{tcolorbox}[colback=blue!1!white, colframe=blue!5!white, title=\textcolor{black}{\textbf{Visual Hallucination Example 2}}, breakable]
\ttfamily
\raggedright
Thought: The given panorama shows parts of a room and possibly adjacent areas. The upper left image shows a workspace with a computer, the upper right shows a hallway and another room, the lower left has a part of a bed, and the lower right includes windows with curtains. There is no indication of a couch in these views. \textcolor{red}{The only promising lead is the room visible through the doorway marked with a green number "1" in the upper right image, which might show a larger area including a couch. Action: 1} 
\rmfamily
\end{tcolorbox}

\end{document}
