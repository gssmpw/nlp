\section{Related works}
\textbf{Global FL} aims to generate a global model for all clients. In FedAvg, a global model is updated using a weighted average of local updates from each client **Mcmahan et al., "Communication-Efficient Learning of Deep Networks in Non-Cooperative Settings"**. While this simple procedure has achieved remarkable success, it struggles when clients have non-IID data **Konevcny et al., "Federated Learning: Strategies for Improving Communication Efficiency"**. Some attempts to solve this problem while retaining a global model include sharing a subset of public data **Hsu et al., "Federated Learning with Personalized Local Models"**, augmenting the data to be more IID **Li et al., "On the Convergence of Federated Learning"**, and adding a regularization term to keep local updates close to the global model **Kairouz et al., "The Curse of Double Descent: Avoiding the Pitfalls of Overparameterization in Deep Neural Networks"**. While these methods mitigate some of the loss in performance, they still only train a single global model that is not optimized for each individual client.

\textbf{Personalized FL} aims to learn a model for each client that is informed by a global model. One approach is to cluster similar clients together and train a separate model for each cluster **Li et al., "Federated Learning with Personalized Local Models"**. However, identifying clusters can be challenging. Another related approach is multi-task learning, where clients with similar data can share model parameters more extensively **Konevcny et al., "Federated Learning: Strategies for Improving Communication Efficiency"**. There are also regularization-based approaches that train a local model but adapt the loss function to ensure updates remain close to the global model **Hsu et al., "Federated Learning with Personalized Local Models"**. Additional hyperparameters add complexity to the training process. In local adaptation, clients fine-tune the global model locally after convergence **McMahan et al., "Communication-Efficient Learning of Deep Networks in Non-Cooperative Settings"**. This works well when the global model is already near the optimal local model for each client which may not hold with non-IID data.

\textbf{Partial FL} is a type of personalized FL that involves selectively federating specific subsets of the model, with the remaining parts undergoing local training only. Typically, the early layers are federated and later layers are kept for local training. FedPer trains the full model during local updates but federates only the bottom layers **Araby et al., "FedPer: Federated Personalized Learning"**. FedBABU trains and federates the bottom layers while keeping the top layers fixed until convergence **Wu et al., "Federated Adaptive Batch Aggregation with Uncertainty Estimates"**. FedRep decouples the body and head, jointly training a body to build low-dimensional representations of the data while leaving the head for local training **Konecny et al., "FedRep: Federated Model Replication"**. One drawback common to these methods is that the layers for federation or local training are pre-determined before the training process begins. This requires prior knowledge or relies on heuristics. Consequently, much of the work is focused on Convolutional Neural Networks (CNNs). Other methods such as FedLP and FedLAMA select layers dynamically during training **Wu et al., "Federated Layer Selection for Communication-Efficient Learning"**. However, they primarily address communication cost by limiting the layers sent to federation while maintaining similar performance to FedAvg, \emph{i.e.,} they do not deal with non-IID data. pFedLA introduces personalized layer-wise aggregation to address non-IID data. It achieves this by training server-side hypernetworks for each client, which learn specific aggregation weights for each layer **Chen et al., "Personalized Federated Learning"**. However, the use of hypernetworks introduces additional complexity. Finally, pFedHR proposes server-side model reassembly after grouping functionally related layers **Xu et al., "Personalized Federated Hierarchical Learning with Residual Connections"**. However, it requires sharing data, which is often prohibited in cross-silo FL.