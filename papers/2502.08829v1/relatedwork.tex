\section{Related works}
\textbf{Global FL} aims to generate a global model for all clients. In FedAvg, a global model is updated using a weighted average of local updates from each client \cite{mcmahan2017communication}. While this simple procedure has achieved remarkable success, it struggles when clients have non-IID data \cite{li2020federated}. Some attempts to solve this problem while retaining a global model include sharing a subset of public data \cite{zhao2018federated}, augmenting the data to be more IID \cite{duan2019astraea}, and adding a regularization term to keep local updates close to the global model \cite{li2020federated}. While these methods mitigate some of the loss in performance, they still only train a single global model that is not optimized for each individual client.

\textbf{Personalized FL} aims to learn a model for each client that is informed by a global model. One approach is to cluster similar clients together and train a separate model for each cluster \cite{briggs2020federated, mansour2020three}. However, identifying clusters can be challenging. Another related approach is multi-task learning, where clients with similar data can share model parameters more extensively \cite{smith2017federated, dinh2021fedu}. There are also regularization-based approaches that train a local model but adapt the loss function to ensure updates remain close to the global model \cite{li2021ditto, t2020pfedme}. Additional hyperparameters add complexity to the training process. In local adaptation, clients fine-tune the global model locally after convergence \cite{yu2020salvaging}. This works well when the global model is already near the optimal local model for each client which may not hold with non-IID data.

\textbf{Partial FL} is a type of personalized FL that involves selectively federating specific subsets of the model, with the remaining parts undergoing local training only. Typically, the early layers are federated and later layers are kept for local training. FedPer trains the full model during local updates but federates only the bottom layers \cite{arivazhagan2019federated}. FedBABU trains and federates the bottom layers while keeping the top layers fixed until convergence \cite{oh2021fedbabu}. FedRep decouples the body and head, jointly training a body to build low-dimensional representations of the data while leaving the head for local training \cite{collins2021exploiting}. One drawback common to these methods is that the layers for federation or local training are pre-determined before the training process begins. This requires prior knowledge or relies on heuristics. Consequently, much of the work is focused on Convolutional Neural Networks (CNNs). Other methods such as FedLP and FedLAMA select layers dynamically during training \cite{zhu2023fedlp, lee2023layer}. However, they primarily address communication cost by limiting the layers sent to federation while maintaining similar performance to FedAvg, \emph{i.e.,} they do not deal with non-IID data. pFedLA introduces personalized layer-wise aggregation to address non-IID data. It achieves this by training server-side hypernetworks for each client, which learn specific aggregation weights for each layer \cite{ma2022layer}. However, the use of hypernetworks introduces additional complexity. Finally, pFedHR proposes server-side model reassembly after grouping functionally related layers \cite{wang2024towards}. However, it requires sharing data, which is often prohibited in cross-silo FL.