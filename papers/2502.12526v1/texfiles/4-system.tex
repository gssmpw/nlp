\section{\name{} SYSTEM}
Informed by the formative study, we designed and developed \name, an AI-infused cartoon video system for promoting preschoolers' language learning. In this section, we discuss our core features, usage scenario, and system implementation.
\subsection{Core Features}
The core features of \name{} were informed by our formative study, existing literature, and insights from experts, culminating in the identification of four key phases: 
The \textbf{Real-time Question-Answering} phase allows families to pause the video and interact with a conversational agent, which answers questions about the current frame to help children learn vocabulary tied to the cartoon's objects and events.
The \textbf{Active Vocabulary Review} phase features the agent, acting as a teachable agent~\cite{matsuda2020effect}, curiously asking children the same questions, presenting relevant screenshots, and providing positive feedback to reinforce newly learned vocabulary.
The \textbf{Real-World Association} phase pairs real-world images with their animated versions to help children connect the object to its real-world counterpart.
The \textbf{Contextual Expansion} phase creates personalized stories and sentences based on family preferences and children's ages~\cite{leong2024putting}. Drawing from previous work~\cite{chen2024retassist}, it also generates related images to enhance word comprehension and recall in context.


\subsection{\name{} Usage Scenario}
\autoref{fig:ui_interface} illustrates the overall user flow of \name{} which is an AI-infused cartoon video system for vocabulary learning. Here we present a typical usage scenario of \name{}: 
Oli is a 3-year-old child who loves watching the cartoon \textit{Peppa Pig}. 
Her mother, Ira, wanted her to learn new vocabulary through the cartoon video. 
One evening, they opened \name{} on a tablet, and Ira encouraged Oli to choose a favorite cartoon character to watch with her (\autoref{fig:ui_interface} (A)). 
Ira assisted Oli in selecting a panda character and the appropriate age group, ensuring the agent could communicate effectively with Oli. 
Then, Ira selected an episode of \textit{Peppa Pig} for them to watch together (\autoref{fig:ui_interface} (B)). 
While watching, Ira paused the video, pointed to the noodles in the cartoon, and asked Oli, ``What is Peppa eating?'', and Oli shook her head.
Ira clicked ``I have a question'' in the interface and asked again. The agent replied, ``Peppa is eating noodles''~(see \autoref{fig:ui_interface} (C)).
Further, Ira encouraged Oli to ask the conversational agent if there was something she didn't understand. 

After watching the video, the conversation agent transformed into a curious teachable agent by showing Oli screenshots from the cartoon and asking her to review the vocabulary she just learned. 
For example, the conversational agent asked, ``What is Peppa eating?''.  Oli could click the ``I want to answer'' button, and the agent would give feedback based on her response (see \autoref{fig:ui_interface} (D)).

In the Contextual Mapping phase, Oli and her mom saw two images: one of noodles from Peppa Pig on the left, and a real-life image on the right. The conversational agent prompted a discussion question, asking if the two images were similar. Oli and her mom then discussed it, with her mom adding, ``Have you seen the noodles we eat at home?'' (see \autoref{fig:ui_interface} (E)).

Finally, they found that the system expanded vocabulary into sentences or stories and generated corresponding images. For example, Ira customized the sentence under the topic ``FOOD'': ``Noodles are made from flour, and when cooked, they become soft and pair perfectly with various delicious soups'', expanding from the word ``noodles'' (see \autoref{fig:ui_interface} (F)).

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{images/xitong.pdf}
    \caption{: Overview of \name{} system architecture.}
    \label{fig:enter-label}\vspace{-0.5cm}
\end{figure*}

\subsection{ \name{} System Implementation}
\label{subsec:system-implementation}
\autoref{fig:enter-label} shows our system architecture. 
The front-end application of \name{} is built with \textit{Vue 3}~\cite{vue3}, a JavaScript-based framework that enables cross-platform compatibility across devices. The client communicates with the server via \textit{REST API}. When interacting with \name{}, user audio messages and video frames are uploaded to the backend as input. User audio messages are processed using \textit{iFly's TTS}
\footnote{iFly's TTS, https://global.xfyun.cn/products/text-to-speech} model to generate corresponding voice messages. Also, we have implemented speech-to-text function using the Voice Dictation (Streaming Version) service from \textit{iFlytek}\footnote{iFLYTEK Speech-to-Text, https://global.xfyun.cn/products/speech-to-text}. 

The back-end of \name{} is built with FastAPI~\cite{fastapi}, which facilitates the processing of VLMs and LLMs workflows, AI image generation, and data management. 
The generative pipelines incorporate \textit{CogVLM2}\footnote{CogVLM2, https://github.com/THUDM/CogVLM} to analyze video frames during Real-time Q\&A phase and Active Review phase to provide corresponding feedback. 
In the Real-World Association phase, the conversation data between the agent and families from the first phase was summarized into themes using \textit{Doubao LLM}\footnote{Doubao AI, https://team.doubao.com/zh/}, then transformed into image prompts with realistic styles by another \textit{Doubao LLM}, which were input into Stable Diffusion to generate images.
In the Contextual Expansion phase, we used \textit{Doubao LLM}\footnote{Doubao AI, https://team.doubao.com/zh/} to generate contextual sentences and stories, which are then summarized into prompts for the Stable Diffusion model to generate images (\textit{stabilityai/stable-diffusion-xl-base-1.0}\footnote{Stable Diffusion XL, https://huggingface.co/docs/diffusers/using-diffusers/sdxl}).
We iteratively co-designed AI prompts with language experts.
The system uses MongoDB to store and manage data, including user queries, interaction logs, and AI-generated content (e.g., video frames, dialogue records, and AI-generated sentences or stories).