\section{Design Implications and Future Work}
Here, we outline and explore a set of design implications to guide future HCI research:

\textbf{Implication 1: Integrating Personalized Question-and-Answer Interactions for Vocabulary Learning During Cartoon Watching}
Prior research has shown that integrating conversational agents into narrative science programming to promote active learning in children through questions and responsive feedback~\cite{10.1145/3491102.3502050}.
Extending the line of research, our system employs visual language models tailored to specific child age groups, helping families answer questions about cartoon video frames using age-appropriate vocabulary and expressions.
Future work could explore incorporating more personalized vocabulary assessment mechanisms into cartoon-based learning by enabling conversational agents to actively ask families questions based on screen frames. For instance, sensor devices (e.g., wearable devices, home environment sensors) could be used to collect children's daily language environments and identify priority vocabulary for learning~\cite{lee2024open}. Meanwhile, deep learning models could detect key scene elements in cartoons (e.g., characters, animals, actions) and dynamically generate personalized questions by integrating data from the children's language environments.

\textbf{Implication 2: Supporting Multimodal Interactions to Strengthen Real-World Associations}
Cartoons, popular among preschoolers, often feature fantastical events that defy physics. Research shows young children might struggle to distinguish between real and fantastical events~\cite{li2015can}.
To develop the ability to distinguish fantasy from reality, our system provides real-life images and cartoon screenshots for comparison to strengthen this distinction.
Future work could consider incorporating more multi-sensory interactions to enhance the ability to distinguish between reality and the virtual world. For example, animal sound effects could be incorporated to help users recognize real-world animals, or AI-generated interactive 3D models of objects could allow users to explore the details of these items~\cite{ma2024research}. Further, we also encourage considering the use of AI to generate 3D worlds from everyday images in the future, allowing children to engage with dynamic, immersive content~\cite{worldlabs}.

\textbf{Limitations and Future Work} 
In this section, we discuss several limitations of the current study and outline potential directions for future research:
The small sample size may limit result generalizability. Future work will involve recruiting more families for a long-term field study to gain deeper insights into how parents and children use \name{} in daily life. Second, the children in our study only watched \textit{Peppa Pig}. While they showed strong interest, future work should include more cartoons for different age groups to better support vocabulary learning and cater to a wider range of child participants.
Another key consideration is the safe use of generative AI. Although no inappropriate images appeared during the contextual expansion phase, parents noted minor issues with counterfactual elements in the AI-generated images. Future work could fine-tune generative models for children's vocabulary learning to ensure contextually accurate and appropriate images by training on datasets focused on children's cognitive development and vocabulary.
While visual language models can capture cartoon details and provide interactive feedback, their understanding of context remains limited. Future work could integrate live multimodal LLMs~\cite{google_ai_studio} and memory functions to enhance real-time contextual comprehension, enabling more comprehensive answers.