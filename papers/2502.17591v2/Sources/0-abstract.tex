\begin{abstract}
With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining model utility. In this paper, inspired by studies of amnesia in cognitive science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to safeguard PII in LLMs while preserving their utility. This mechanism works by actively identifying and forgetting key memories most closely associated with PII in sequences, followed by a memory implanting using suitable substitute memories to maintain the LLM's functionality. We conduct evaluations across multiple models to protect common PII, such as phone numbers and physical addresses, against prevalent PII-targeted attacks, demonstrating the superiority of our method compared with other existing defensive techniques. The results show that our PPA method completely eliminates the risk of phone number exposure by 100\% and significantly reduces the risk of physical address exposure by 9.8\% -- 87.6\%, all while maintaining comparable model utility performance.








\begin{comment}
With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining model performance. In this paper, inspired by studies of amnesia in cognitive science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to safeguard PII in LLMs while preserving their utility. This mechanism works by actively identifying and forgetting key memories most closely associated with PII in sequences, followed by a memory implanting using suitable substitute memories to maintain the LLM's functionality. We applied this approach to the LLaMA2-7b and LLaMA3-8b models to protect sensitive information such as phone numbers and physical addresses against common PII-targeted attacks. 
The results demonstrate that PPA reduces the risk of phone numbers exposure by 100\% and risk of physical addresses 9.8\% -- 87.6\% without compromising model performance.
\end{comment}


% The results demonstrate that PPA reduces the risk of PII exposure by 100\% compared to existing phone defense methods, without compromising model performance. In terms of address defense, PPA achieves a 26.2\% reduction in risk score, accompanied by a 29.4\% decrease in model performance.

% with only a minor impact on model performance.

% while minimizing performance impact by 29.4\%.

% \MK{should we pick better results?}
 
\begin{comment}
\JY{With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining model performance. In this paper, inspired by studies of amnesia in cognitive science, we propose a novel approach, Motivated Privacy Amnesia, to safeguard PII in LLMs while preserving their utility. This mechanism works by actively identifying and erasing key memories most closely associated with PII in sequences, followed by a replacement therapy using suitable substitute memories to maintain the LLM's functionality. We applied this approach to the LLaMA2-7b and LLaMA3-8b models to protect sensitive information such as phone numbers and physical addresses against common PII-targeted attacks. The results show that, compared to existing defense methods, DSAA significantly reduces the risk of PII exposure by [specific rate], while minimizing performance impact by [specific percentage].}
\end{comment}
\end{abstract}


\begin{comment}
Current privacy research on large language models (LLMs) shows that these models have the potential to violate personally identifiable information (PII). Although some research has focused on protecting users' PII in LLMs, existing approaches either successfully safeguard PII at the cost of reduced model performance or maintain model performance without adequately protecting PII. In this paper, we present a solution that strikes a balance between preserving users' PII and maintaining LLM performance: Dynamically Selected Anterograde Amnesia (DSAA). DSAA is a technique designed to protect PII in LLMs without significantly affecting model performance. It achieves this by dynamically identifying and forgetting key elements with a high 'memorization factor' within PII sequences, followed by the application of error injection. We applied this approach to the LLaMA2-7b and LLaMA3-8b models to secure PII, including phone numbers and physical addresses. The results demonstrate that DSAA effectively protects the majority of users' phone numbers and physical addresses while minimally impacting LLM performance.
\end{comment}




\begin{comment}
The unlearning method effectively removes personally identifiable information (PII) from large language models (LLMs), but it significantly compromises LLM performance while safeguarding PII. As a result, this method is neither scalable nor practical for real-world applications. To address these limitations, we introduce Dynamic Mix Selected Unlearning, which protects individuals' PII in LLMs without significantly affecting model performance. This approach begins with a sensitivity analysis of the PII sequence to identify key elements. It then applies selective unlearning to these elements, followed by error injection to mitigate the performance degradation caused by the unlearning process. We applied this method to both the LLaMA2-7b and LLaMA3-8b models to protect PII, such as phone numbers and physical addresses. The results demonstrate that Dynamic Mix Selected Unlearning effectively secures the majority of users' phone numbers and physical addresses, with only a negligible impact on LLM performance.
\end{comment}

\begin{comment} %original NeurIPS 2024
The unlearning method effectively removes personal identifying information (PII) from large language models (LLMs), yet it significantly compromises the performance of LLMs while safeguarding PII. Consequently, this method is not scalable or practical for real-world applications. To overcome these limitations, we introduce Dynamic Mix Selected Unlearning, which preserves persons' PII in LLMs without significantly impacting model performance. This approach begins with a sensitivity analysis on the PII sequence to identify key elements. Then, it applies selected unlearning to these elements, followed by error injection to mitigate the performance degradation caused by the unlearning process. We apply this method to the LLaMA2-7b model to protect PII, such as phone numbers and physical addresses. The results demonstrate that Dynamic Mix Selected Unlearning successfully secures the phone numbers of all 468 individuals and most of the physical addresses of 790 users, with only a negligible reduction in LLM performance.
\end{comment}