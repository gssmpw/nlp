
\begin{comment}
\section{Conclusion and Discussion\label{conclusion_and_discussion}}
Unlearning is a promising method that enables LLMs to forget a person's PII. However, if this method is applied to protect the PII of 200 to 400 persons, it may cause the LLMs' performance to collapse. This limitation significantly restricts its practical usage. In this work, we propose the Dynamic Mix Selected Unlearning to protect a person's PII while maintaining LLMs' model performance. We conduct comprehensive experiments to support the effectiveness of our proposals.

\textbf{Limitation.} The stages of selective unlearning and error injection in Dynamic Mix Selected Unlearning require fine-tuning all parameters in LLMs, which may be costly for users lacking computational resources. 

\textbf{Future work.} Better understand the interplay of the three techniques that allow for effective performance in both preventing model memorization and maintaining model performance.  The future of Dynamic Mix Selected Unlearning can extend to more general information, such as the relationship between persons and authorities.
\end{comment}

\section{Conclusion and Discussion\label{conclusion_and_discussion}}
We demonstrated that Proactive Privacy Amnesia achieves the optimal balance between defense performance and model utility compared to methods like Error Injection, Empty Response, Unlearning, and DEPN for protecting users' PII, including phone numbers and physical addresses. Additionally, we initially introduce the concept of the 'memorization factor', which affects the model's capacity to retain PII sequences. This concept is using in sensitivity analysis and supported by theoretical justification. Furthurmore, PPA is a flexible method that can adjust its balance between defense capability and model performance. Future work could extend PPA to protect the privacy of relationships, such as those between persons or between organizations. 

% Expand PPA to protect fields that contains privacy that people do not want to be exposed

\section{ACKNOWLEDGEMENTS\label{acknowledgement}}
This work is supported by ARO W911NF-23-2-0224, NSF IIS-2332744, and CNS-2112562. We also thank Accenture for its support and Yun-Chu Wang for her support. We thank area chair and reviewers for their valuable comments.

\begin{comment}
We demonstrated that Dynamically Selected Anterograde Amnesia offers the best trade-off among methods like Error Injection, Empty Response, Unlearning, and DEPN for protecting phone numbers, based on PII risk score and exact match score. It also excels in safeguarding physical addresses. We found that Sensitivity Analysis, Selected Unlearning, and Error Injection are all indispensable steps. Our work improves the PII risk score when more than one index is selected for forgetting, but selecting too many indexes deteriorates model performance. The break-even point for unlearning is between 200 and 400 data points. Overall, it effectively protects PII while maintaining model performance.
\end{comment}

%We have demonstrated that Dynamic Mix Selected Unlearning offers the optimal trade-off among methods such as Error Injection, Empty Response, Unlearning, and DEPN for protecting phone numbers, based on metrics like PII risk score and exact match score. Additionally, it excels in safeguarding physical addresses, as measured by the PII risk score. We have analyzed the contributions of the individual steps in the algorithm, finding that Sensitivity Analysis, Selected Unlearning, and Error Injection are all indispensable. Moreover, we found that the Dynamic Mix Selected Unlearning method improves the PII risk score when more than one index is selected for forgetting. However, selecting too many indexes leads to a deterioration in model performance. Furthermore, we observed the break-even point of the unlearning method was between 200 to 400 data points. All in all, our proposed, Dynamic Mix Selected Unlearning is a good trade-off method to protect persons' PII while maintaining model performance.

\begin{comment}
\textbf{Limitation and Future work.}
Our study utilized the LLaMA2-7b model, chosen for its strong generalization capabilities and due to computational constraints. The experiments were performed using the Enron email dataset, widely acknowledged as suitable for data extraction attack studies. Future research will focus on theoretical analysis to determine how PII is stored within LLM. Enhancing our comprehension of LLMs' ``memorization'' phenomena will aid in protecting PII without compromising model performance.
\end{comment}

\begin{comment}
\textbf{Limitation.} 
We conducted our experiments and ablation studies using the LLaMA2-7b model, selected for its robust generalization capabilities across various aspects and due to constraints on computational resources. We carried out these experiments on the Enron email dataset, a public resource that is broadly recognized as representative for data extraction attack studies.

\textbf{Future work.} 
Theoretical analysis will be a compelling area of future research to determine where PII is actually stored within LLMs. As we deepen our understanding of LLMs' "memorization" phenomena, we can better protect individuals' PII while maintaining model performance. This could also extend to the development of watermarking techniques for LLMs.
\end{comment}

