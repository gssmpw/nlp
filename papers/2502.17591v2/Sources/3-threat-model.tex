\section{Threat model}
%This paper's framework focuses on a method to protect targeted PII, such as phone numbers and physical addresses, which can be directly linked to persons using LLMs, potentially exposing their identities. To test the limits of an attacker's capabilities, we fully finetuned the LLaMA2-7b model\citep{touvron2023llama} on the Enron email dataset. This approach is also practical for real-world applications; For instance, when Company A wishes to finetune a pre-trained language model on its own dataset, it aims to do so without revealing any PII through the finetuned model, while also maintaining the performance of the model.

\textbf{Attacker's goal:} We consider a scenario where an LLM is trained on the dataset that includes diverse types of personal identifiable information (PII), such as phone numbers and physical addresses. 
%This is a practical concern for real-world applications, as an organization may train an LLM on its own private dataset or web-crawled data that inadvertently includes PII. 
The attacker's goal is to construct prompts that are likely to reveal sensitive information from an LLM through its responses. These attacks can lead to the partial or complete exposure of a set of PII for a given context, such as several digits or the entirety of a target phone number, which can be leveraged by attackers to learn user privacy or even re-identify users.

\textbf{Attacker's capability:} We consider both probing and soft prompt attackers. Probing attackers know the target users' names and the model’s output logits. They use a set of prompts to query an LLM~\citep{kim2024propile}, exposing the user's PII in its responses. Soft prompt attackers, in addition to knowing the target users' names and the model’s output logits, have access to the model and an additional dataset to train soft prompts~\citep{kim2024propile}. These trained soft prompts are then prepended to the probing prompts to trigger more extensive exposure of users' PII. 
% \aolin{help me check here, do we need to assume this attacker with unlimited budgets?}

To ensure that our attacks are realistic and account for rate limits and other query restrictions, we assume that the attacker operates with a limited budget for query prompts. We also consider that PIIs with similar data attributes present comparable risks of data leakage. For instance, an attacker’s techniques effective in extracting phone numbers could potentially be applied to reveal social security numbers or credit card numbers, as these types of PIIs are all purely numerical in nature.

% Specifically, the attacker can exploit the twin-template probing attack described in~\citep{kim2024propile} with 5 different attack prompts per person and the input rephrase attack with 20 different attack prompts per person. We assume that PIIs with similar data attributes have a comparable level of data leakage risk. For example, the attacker may use the same techniques that successfully extract phone numbers to reveal social security numbers or credit card numbers, as these PIIs are all purely numerical.

% Finally, we utilize Amazon Web Services (AWS) Comprehend Service\citep{aws2024comprehend} to extract PII from the output. Outputs derived from phone number prompts are referred to as "predicted phone numbers," while those from physical address prompts are termed "predicted physical addresses."


%\textbf{Defense side} knows the target users' names, and their Personally Identifiable Information (PII), and can access the model. Moreover, the defense side needs to preserve PII without sacrificing too much model performance.