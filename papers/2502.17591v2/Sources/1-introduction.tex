\section{Introduction\label{Introduction}}
Large Language Models (LLMs) \citep{touvron2023llama,achiam2023gpt,team2023gemini, dubey2024llama} have achieved remarkable success in recent years, with their wide adoption either as general-purpose models or, after fine-tuning, as specialized and personal assistants.
Despite their success, LLMs with huge parameter counts and great capacity in the meantime exhibit the concerning ``memorization'' phenomenons \citep{carlini2019secret,carlini2021extracting}, i.e., they can precisely memorize some training data.
Such memorization is vulnerable to various attacks (e.g., membership inference attacks and data extraction attacks) and risks severe privacy breaches.
One of the most serious concerns comes from the attacks that aim to extract personal identifiable information (PII) memorized by the models, which compromise users' privacy and are likely to cause real-world harm consequently. 
\begin{comment}
There are various types of PII, including phone numbers, physical addresses, credit card numbers, Social Security Numbers (SSN), and personal email addresses. Structurally, phone numbers, credit card numbers, and SSNs are similar as they primarily consist of numerical sequences. In contrast, physical addresses and personal email addresses are alike in that they typically incorporate both numbers and words.
\end{comment}

\begin{comment}
\begin{figure*}[t!]
\centering
  \includegraphics[width=0.62\textwidth]{./images/selling_point_real_number.png}
  \caption{Trade-off between current methods: Model performance is measured in terms of average perplexity on a logarithmic scale, while the Risk score pertains to the Phone risk score. APA represents our method, Active Privacy Amnesia.}
  \label{current_method_trade_off}
\end{figure*}
\end{comment}


\begin{comment}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{./images/selling_point_real_number.png}
    \caption{Trade-off between current methods: Model performance is measured in terms of average perplexity on a logarithmic scale, while the Risk score pertains to the Phone risk score. APA represents our method, Active Privacy Amnesia.}
    \label{current_method_trade_off}
\end{wrapfigure}
\end{comment}


% \vspace{-2.5cm}
\begin{figure*}[t!]
\centering
\vspace{-1.0cm}
  \includegraphics[width=0.8\textwidth]{./images/method_flow.jpg}
  \caption{The flowchart illustrates our method, Proactive Privacy Amnesia (PPA). All examples presented in the flowchart are real instances from the LLaMA2-7b experiments.}
  \label{fig:method_flow}
\end{figure*}


To defend against such PII or data extraction attacks, several \textit{machine unlearning} techniques have been applied to LLMs. 
However, existing methods typically fall short in terms of the trade-off between the defense performance and model utility.
For example, most unlearning approaches are based on gradient ascent~\citep{jang2022knowledge,wang2024selective} and often adversely affect model functionalities to an extent where the model cannot handle their original tasks anymore and thus becomes no longer useful.
In contrast, although not harmful to the model utility, gradient descent methods~\citep{patil2023can,ouyang2022training,de2021editing} may inject less robust defense, leaving the model still vulnerable to data extraction attacks.
Therefore, a method that can effectively defend against PII extraction attacks while maintaining model utility is still lacking.


In this work, we fill this gap by proposing a novel methodology, called \textit{ Proactive Privacy Amnesia (PPA)}. Inspired by Anterograde Amnesia ~\citep{markowitsch2008anterograde}, we think that achieving a better balance between performance and privacy protection requires two essential components: (1) selectively forgetting only the key element within the PII, without affecting other tokens; and (2) maintaining normal functionality by replacing sensitive information with non-sensitive memory. To seamlessly integrate these components, our method, PPA, as shown in Figure~\ref{fig:method_flow}, comprises three parts: (1) Sensitivity Analysis, which identifies the key elements in memorized PII; (2) Selective Forgetting, which focuses exclusively forgetting on the key elements; and (3) Memory Implanting, a strategy used to compensate for loss in model performance due to the Selective Forgetting process. We demonstrate the effectiveness of our method through extensive experiments on LLaMA2 \citep{touvron2023llama} and LLaMA3~\citep{dubey2024llama} models to defend existing PII-targeted attacks on common PII, such as phone numbers and physical addresses. Extensive experimental results demonstrate that our method, PPA, achieves the most favorable balance between defense capability and model performance when compared to other prevalent defensive methods. 


For example, in the Enron email experiment for phone number defense, PPA enhances model performance by 372.7\% compared to methods with mediocre model utility while maintaining the same level of defense in terms of risk score. Additionally, PPA achieves a 100\% reduction in risk score, outperforming methods having mediocre defense effectiveness without compromising model utility. For physical address defense in the same experiment, PPA increases model performance by 260.0\% compared to methods with mediocre model utility and increase the risk score by 151.7\%. Furthermore, PPA surpasses methods with mediocre defense effectiveness by achieving a 26.2\% reduction in risk score, with only a 29.4\% decrease in model performance.





% For example, in the \textbf{Enron email experiment} for phone number defense, PPA improves model performance by 372.7\% compared to the mediocre performance model, while maintaining the same level of risk score defense. Additionally, PPA achieves a 100\% reduction in risk score, outperforming the mediocre defense method without compromising model performance. For physical address defense on the same experiment, PPA increases model performance by 260.0\% compared to the mediocre performance model and reduces the risk score by 60.2\%. Furthermore, PPA surpasses the mediocre defense method by achieving a 26.2\% reduction in risk score, with only a 29.4\% decrease in model performance.



% , on two datasets: the Enron email dataset~\citep{klimt2004introducing} and the Fraud email dataset~\citep{radev2008clair}.



\begin{comment}
We demonstrate the effectiveness of our method through extensive experiments by fine-tuning on two different datasets: 1) \textbf{Enron email experiment} which fine-tune LLaMA2-7b~\citep{touvron2023llama} and LLaMA3-8b~\citep{dubey2024llama} on Enron email dataset~\citep{klimt2004introducing} 2) \textbf{Fraud email experiment} which fine-tune LLaMA2-7b on Fraud email dataset~\citep{radev2008clair}
\end{comment}

\begin{comment}

1) Pre-trained LLaMA2-7b model~\citep{touvron2023llama} and LLaMA3-8b model~\citep{dubey2024llama}, both fine-tuned using the Enron email dataset~\citep{klimt2004introducing}. 2) Pre-trained LLaMA2-7b model fine-tuned with the fraud email dataset~\citep{radev2008clair}.
\end{comment}

\begin{comment}
\textbf{Enron email experiment.}
For the LLaMA2-7b and LLaMA3-8b models fine-tuned with the Enron email dataset~\citep{klimt2004introducing}, our findings show that we successfully protected the phone numbers of all 468 users, achieving an average risk score of 0 on both models. Regarding the physical addresses of 790 users, most were safeguarded, though 5 users remained exposed on the LLaMA2-7b model and 11 on the LLaMA3-8b model. \footnote{We define "exposed" as an individual risk score exceeding the threshold of 1.35. This threshold is calculated by summing the risk scores from Region, SubRegion, Municipality, Postal Code, Street, and Address Number, excluding the country due to its easy inference from other data categories.} The average risk scores for these exposed users were 7.3 on LLaMA2-7b and 14.9 on LLaMA3-8b, respectively.


\textbf{Fraud email experiment.}
For the LLaMA2-7b model fine-tuned with the fraud email dataset~\citep{radev2008clair}, we successfully protected the phone numbers of 50 users, with an average risk score of 0.3. Regarding the physical addresses of these 50 users, most were secured, although 2 users remained exposed, with an average risk score of 3.0 across methods.


The \textbf{Enron email experiment} and the \textbf{Fraud email experiment} demonstrate that APA can effectively protect users' PII while preserving the model's performance.
Figure~\ref{current_method_trade_off} illustrates the trade-off between safeguarding users' PII and model performance, highlighting that APA offers the most favorable balance compared to other defensive methods we implemented.
\end{comment}


% Figure~\ref{current_method_trade_off} illustrates the trade-off between safeguarding users' PII and model performance, highlighting that PPA offers the most favorable balance compared to other defensive methods we implemented.


Our contributions are as follows:
\begin{itemize}

\item We propose a novel method PPA that can preserve a person's PII on LLMs while maintaining LLMs' performance.
\item We conducted input rephrasing, probing, and soft prompt attacks to evaluate the effectiveness of our PPA approach. The PPA effectively safeguards phone numbers and physical addresses, with only a marginal drop in LLMs' performance.

% \item We conducted both probing and soft prompt attacks to assess the effectiveness of our PPA approach. The findings indicate that this method can safeguard the phone numbers of 468 persons and the physical addresses of most 790 persons, with only a marginal drop in LLMs' performance.

\item We introduce the concept of the 'memorization factor' and use it to identify the key elements within PII sequences that influence the model's ability to retain such information. This approach is using in sensitivity analysis and supported by theoretical justification.
\item PPA is a flexible method that enables adjusting the balance between defense capability and model performance by modifying the number of key elements to be forgotten.

% We conducted a detailed sensitivity analysis to identify key elements within PII sequences that influence the model's ability to memorize them, supported by a theoretical justification.
% \item We have designed an innovative approach, APA, which is based on unlearning key elements in the PII sequence. Specifically, it begins with a sensitivity analysis to identify the key "memorable" elements, followed by selected unlearning them. Subsequently, error injection is employed to compensate for the performance degradation caused by the selected unlearning.


\end{itemize}







\begin{comment}
We demonstrate the effectiveness of our method through extensive experiments on a pre-trained LLaMA2 model~\citep{touvron2023llama} and LLaMA3 model~\citep{dubey2024llama}, which was fine-tuned using the Enron email dataset~\citep{klimt2004introducing}. Our findings indicate that we successfully protected the phone numbers of all 468 users, with an average risk score of 0 on both the LLaMA2 and LLaMA3 models. For the physical addresses of 790 users, most were safeguarded, though 5 users remained exposed on the LLaMA2 model and 11 on the LLaMA3 model \footnote{We consider exposed as defined as an individual user risk score above the threshold of 1.35. This threshold is determined by adding the risk scores attributed to the Region, SubRegion, Municipality, Postal Code, Street, and Address Number, excluding the country. The exclusion of the country is due to its easy inference from the cumulative data provided by the other categories.}. The average risk scores for these exposed users were 7.3 on LLaMA2 and 14.9 on LLaMA3, respectively.
Additionally, we conducted experiments on a pre-trained LLaMA2 model fine-tuned with the fraud email dataset~\citep{radev2008clair}. In this case, we successfully protected the phone numbers of 50 users, with an average risk score of 0.3. For the physical addresses of these 50 users, while most were secured, 2 users remained exposed, with an average risk score across methods of 3.0.
Figure~\ref{current_method_trade_off} presents a trade-off between safeguarding persons' PII and model performance, showing that Dynamic Mix Selected Unlearning offers the most favorable trade-off compared to other defensive methods we have implemented.
Our contributions are as follows:
\begin{itemize}

\item We propose a novel method Dynamic Mix Selected Unlearning that can preserve a person's PII on LLMs while maintaining LLMs' performance.

\item We have designed an innovative approach, Dynamic Mix Selected Unlearning, which is based on unlearning key elements in the PII sequence. Specifically, it begins with a sensitivity analysis to identify the key "memorable" elements, followed by selected unlearning them. Subsequently, error injection is employed to compensate for the performance degradation caused by the selected unlearning.

\item We conducted both black-box and soft prompt attacks to assess the effectiveness of our Dynamic Mix Selected Unlearning approach. The findings indicate that this method can safeguard the phone numbers of 468 persons and the physical addresses of most 790 persons, with only a marginal drop in LLMs' performance.


\end{itemize}
\end{comment}



\begin{comment}

We demonstrate the effectiveness of our method through extensive experiments on a pre-trained LLaMA2 model~\citep{touvron2023llama} and LLaMA3 model~\citep{dubey2024llama}, which was fine-tuned using the Enron email dataset~\citep{klimt2004introducing}. Our results show that we successfully protected the phone numbers of all 468 users, with an average across methods risk score of 0, and most of the physical addresses of 790 users, with 5 users still defined as exposed \footnote{We consider exposed as defined as an individual user risk score above the threshold of 1.35. This threshold is determined by adding the risk scores attributed to the Region, SubRegion, Municipality, Postal Code, Street, and Address Number, excluding the country. The exclusion of the country is due to its easy inference from the cumulative data provided by the other categories.} and an average across methods risk score of 7.3. 
Figure~\ref{current_method_trade_off} presents a trade-off between safeguarding persons' PII and model performance, showing that Dynamic Mix Selected Unlearning offers the most favorable trade-off compared to other defensive methods we have implemented.
Our contributions are as follows:
\begin{itemize}

\item We propose a novel method Dynamic Mix Selected Unlearning that can preserve a person's PII on LLMs while maintaining LLMs' performance.

\item We have designed an innovative approach, Dynamic Mix Selected Unlearning, which is based on unlearning key elements in the PII sequence. Specifically, it begins with a sensitivity analysis to identify the key "memorable" elements, followed by selected unlearning them. Subsequently, error injection is employed to compensate for the performance degradation caused by the selected unlearning.

\item We conducted both black-box and soft prompt attacks to assess the effectiveness of our Dynamic Mix Selected Unlearning approach. The findings indicate that this method can safeguard the phone numbers of 468 persons and the physical addresses of most 790 persons, with only a marginal drop in LLMs' performance.


\end{itemize}
\end{comment}











\begin{comment}
\section{Introduction}
Large Language Models (LLMs) are highly effective, excelling in various benchmark tasks such as those represented in GLUE \cite{wang2018glue} and SQuAD \cite{rajpurkar2016squad}. They also perform well in generative tasks \cite{touvron2023llama} \cite{achiam2023gpt} \cite{team2023gemini}. Despite their capabilities, these models, which contain a vast number of parameters, have strong memorization abilities that could potentially lead to privacy breaches \cite{carlini2019secret} \cite{carlini2021extracting}. Two primary methods exist for extracting personally identifiable information (PII) from users: black-box attacks \cite{kim2024propile}, where attackers have access only to the model's output logits, and whitebox attacks \cite{patil2023can} \cite{kim2024propile}, where attackers have access to the entire model.


To protect PII, two main strategies are utilized: preprocessing \cite{brown2022does} \cite{dwork2008differential} and postprocessing \cite{ouyang2022training} \cite{de2021editing} \cite{jang2022knowledge} \cite{wang2024selective} \cite{yao2023large} \cite{chen2023unlearn} \cite{wu2023depn} \cite{patil2023can}. Postprocessing methods are particularly practical for industrial applications because they can be applied to already trained LLMs, making them cost-effective in terms of time and resources. However, these methods have limitations. The gradient descent method \cite{ouyang2022training} \cite{de2021editing} can maintain model performance but does not effectively preserve PII. In contrast, the gradient ascent method  \cite{jang2022knowledge} \cite{wang2024selective} preserves PII but can significantly reduce model performance. A combination of gradient descent and ascent \cite{yao2023large} \cite{chen2023unlearn} can preserve PII but requires an additional dataset to maintain model performance. The memory editing method \cite{wu2023depn} preserves PII but can reduce performance and is time-consuming. Another memory editing method \cite{patil2023can}, focused on protecting only a single token, preserves PII but has limited application.



Therefore, we propose the Dynamic Mix Selected Unlearning designed to preserve user PII while maintaining optimal model performance.



\begin{itemize}

\item We propose the method that can preserve PII while maintaining good model performance.

\end{itemize}
\end{comment}

\begin{comment}
{\color{blue}
\section*{Introduction by Jingyang}
Large Language Models (LLMs) have achieved remarkable success in recent years, with its wide adoption either as general-purpose models or, after fine-tuning, as specialized and personal assistants.
Despite their success, LLMs with huge parameter counts and great capacity in the meantime exhibit the concerning ``memorization'' phenomenons, i.e., they can precisely memorize some training data.
Such memorization is vulnerable to various attacks (e.g., membership inference attacks and data extraction attacks) and may result in severe privacy breaches.
One of the most serious concerns comes from the attacks that aim to extract personal identifiable information (PII) memorized by the models, which compromise users' privacy to the highest degree and are likely to cause real-world harms consequently.


To defend against such PII or data extraction attacks, several \textit{machine unlearning} techniques have been applied to LLMs. 
However, existing methods typically fall short in terms of the trade-off between the defense performance and model utility.
For example, most unlearning approaches are based on gradient ascent and often adversely affect model functionalities to an extent where the model cannot handle their original tasks anymore and thus becomes no longer useful.
In contrast, although not harmful to the model utility, methods that perform gradient descent may inject less robust defense, leaving the model still vulnerable to data extraction attacks.
As a result, a method that can 1) effectively defend against PII extraction attacks while 2) maintaining model utility is still lacking.


In this work, we fill this gap by proposing a novel unlearning methodology, called \textit{Dynamic Mix Selected Unlearning}.
{\color{red} \{Martin, briefly introduce your method here with two goals: 1) by reading your description, reviewers can grasp the general idea on a high level and would be interested to learn more about your method in detail; 2) they can clearly see the motivation of your designs and see why the proposed components may address the aforementioned trade-off issue.\}}


We demonstrate the effectiveness of our method with extensive experiments on {\color{red} xxx. Briefly describe experiments and summarize notable results}
}
\end{comment}




\begin{comment}
\section{Introduction}
Large Language Models (LLMs) are highly effective and excel in various benchmark tasks such as GLUE \cite{wang2018glue} and SQuAD \cite{rajpurkar2016squad}. However, these LLMs possess strong memorization capabilities with huge number of parameters, which may lead to privacy breaches \cite{carlini2021extracting}. Regarding the targets of attacks, Carlini et al. (2021) primarily aimed to extract general private information from the pretraining dataset, including international news, code, and contact information \cite{carlini2021extracting}. On the other hand, Kim et al. (2024) concentrated on extracting personally identifiable information (PII), such as phone numbers and physical addresses, from the pretraining dataset \cite{kim2024propile}. Previous research has also explored various attack and defense methods. From an adversarial standpoint, these attacks can be classified into two categories: blackbox attacks \cite{kim2024propile} and whitebox attacks \cite{patil2023can} \cite{kim2024propile}.
Currently, two main strategies are employed to safeguard user data privacy: preprocessing and postprocessing.
In the preprocessing approach, data sanitization techniques \cite{brown2022does} are used to obscure private information in the training datasets prior to their use, while the implementation of Differential Privacy (DP) \cite{dwork2008differential} ensures that adversaries cannot ascertain whether a specific individual's record was included in the dataset.
The postprocessing approach involves four distinct methods: First, gradient descent \cite{ouyang2022training} \cite{de2021editing}; second, gradient ascent \cite{jang2022knowledge} \cite{wang2024selective}; third, a combination of gradient descent and ascent \cite{yao2023large} \cite{chen2023unlearn}; and fourth, memory editing \cite{wu2023depn} \cite{patil2023can}.
 Postprocessing defense methods are practical for industrial application because they can be implemented on already trained Large Language Models (LLMs), which is cost-effective in terms of both time and resources. This approach eliminates the need to retrain models from scratch, thereby saving substantial time and money. However, existing postprocessing methods exhibit certain limitations. For example, the unlearning technique applied to text \cite{jang2022knowledge} can significantly reduce model performance as it requires the sequential forgetting of data chunks. In addition, A strategy that combines gradient descent and ascent \cite{yao2023large} \cite{chen2023unlearn} necessitates an additional dataset, named retain dataset, to maintain LLM effectiveness, rendering it impractical without such a dataset. Moreover, memory editing techniques \cite{wu2023depn} become time-intensive when safeguarding extensive user data privacy, with potential reductions in model performance due to the deletion of numerous neurons. Another memory editing approach \cite{patil2023can} is limited to protecting single tokens, which may not suffice when user data privacy concerns involve multiple tokens.
 Therefore, we purpose the dynamic mix selected unlearning that can preserve user's data privacy without the limitation mentioned above.

 \begin{itemize}
    

\item We propose the first method that can preserve user's data privacy which is scalable and do not need an additional retain dataset.

\item We propose a sensitivity-based method to strategically select tokens that need to be forgot, which outperform naive solutions like randomly or blindly choosing a few tokens to unlearn.

\item We propose new metrics for calculating the exposing risk score for targeted PII, such as phone number and physical address. 

\end{itemize}
\end{comment}