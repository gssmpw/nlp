\section{Related Works}
% In this section, we present prior works on PII extraction attacks, including black-box and white-box attacks, and discuss four post-processing defenses to preserve user data privacy in LLMs.

\subsection{LLM Data Extraction Attacks}
Training data extraction attack~\citep{carlini2021extracting} first uses GPT-2~\citep{radford2019language} with designed prompts to generate sets of sentences and subsequently use an improved membership inference method to detect which generated sentences are from the training dataset. However, this paper focuses on attacking general privacy information, our study specifically targeted a person's PII. \textit{Black-box Probing}~\citep{kim2024propile} employs manual prompts to extract a person's PII from LLMs. Meanwhile, Input Rephrasing attack~\citep{patil2023can} uses a paraphrasing model from ~\citet{krishna2024paraphrasing} to rephrase attack prompts. \textit{White-box Probing}~\citep{kim2024propile} trains soft prompts from the targeted model using black-box templates and employs these soft prompts to attack a person's PII. 
% We apply attack methods such as Input Rephrasing attack, Probing attack, and Soft Prompt attack to validate the defensive effectiveness of our APA.





\subsection{Post-processing Defense Methods}

\textbf{Gradient Based method}
There are several types of gradient based method:
1) \textit{Gradient Descent Method.} The Empty Response Defense~\citep{patil2023can,ouyang2022training} uses gradient descent to increase the probability of generating a predefined "empty" response like "I don’t know." Similarly, the Error Injection method~\citep{de2021editing} increases the likelihood of generating false target responses through gradient descent. But these methods cannot protect user's PII effectively.
%  Our method, when compared with Error Injection, has demonstrated superior performance.
2) \textit{Gradient Ascent Method.} \citet{jang2022knowledge} apply gradient ascent on sequences of target tokens to unlearn specific knowledge. \citet{wang2024selective} highlights the risk of embedding general knowledge within personal data and suggests using sensitivity testing to target specific sequence spans for unlearning, rather than entire instances. However, \citet{jang2022knowledge}'s method may lead to model collapse as the target set size grows.
3) \textit{Combination of Gradient Descent and Ascent.} A more complex approach is outlined by \citet{yao2023large}, involving three loss types: gradient ascent on the forgetting dataset, random smooth loss, and gradient descent on a normal dataset to maintain model performance. \citet{chen2023unlearn} introduce unlearning layers into transformer architectures and perform gradient ascent on these layers while applying gradient descent on the retained dataset to prevent degradation. Additionally, \citet{yao2024machine} show that combining gradient ascent and descent improves hyperparameter robustness. Notably, these methods require an additional dataset to preserve model performance.

\begin{comment}
There are several types of gradient based method: 1) \textbf{Gradient Descent method} including the Empty Response Defense~\citep{patil2023can,ouyang2022training} and Error Injection method~\citep{de2021editing}
2) \textbf{Gradient Ascent method} including the unlearning method~\citep{jang2022knowledge}, ~\citep{wang2024selective}
\end{comment}

\textbf{Memory Editing method.} \citet{wu2023depn} introduces a privacy neuron detector designed to identify and eliminate neurons that significantly contribute to privacy leakage, protecting user data privacy. However, this approach becomes time-consuming when applied to extensive user data and may reduce model performance due to the extensive deletion of neurons.
\citet{patil2023can} introduce the Head Projection Defense method, which addresses the issue of privacy information potentially residing within a model's intermediate layers. They employ interpretability techniques from \citet{geva2020transformer} to identify the top-k possible tokens in each layer and develop a loss function aimed at preventing the reoccurrence of deleted answers in each layer. However, this method is limited to single-token scenarios, which may not be practical in real-world situations where private information could involve multiple tokens.

\begin{comment}
\textbf{Gradient Descent method.} The Empty Response Defense~\citep{patil2023can,ouyang2022training} applies a gradient descent technique to increase the probability of producing an "empty" response, using either "I don’t know" or "dummy" as predefined target strings. Similarly, the Error Injection method~\citep{de2021editing} employs gradient descent to enhance the likelihood of generating a false target. In this work, we compared our method with the Error Injection method~\citep{de2021editing} and found that our method outperformed it.

\textbf{Gradient Ascent method.} Applying unlearning to text, as discussed by~\citep{jang2022knowledge}, involves conducting gradient ascent on specific sequences of target tokens.~\citep{wang2024selective} posits that attackers may embed general knowledge into ordinary personal data and then request data forgetting, thereby degrading the model's performance. 
To address this, \citep{wang2024selective} employs a sensitivity test to focus on specific spans of sequences rather than entire instances. While \citep{jang2022knowledge}'s method of unlearning can protect the PII of individual users, the model eventually collapses as the size of the set increases using this technique.

\textbf{A Combination of Gradient Descent and Ascent method.} \citep{yao2023large} describes a loss function comprising three types of loss: the first involves gradient ascent on the forgetting dataset, the second is a random smooth loss, and the third entails gradient descent on a normal dataset to maintain model performance. \citep{chen2023unlearn} integrated unlearning layers into transformer layers following the feed-forward networks. They performed gradient ascent on these unlearning layers while conducting gradient descent on the retained dataset to mitigate model degradation. \citep{yao2024machine} demonstrate that integrating gradient ascent with gradient descent on in-distribution data enhances hyperparameter robustness. Above all, these methods need an additional dataset to maintain its model performance.
\end{comment}

\begin{comment}
\cite{yao2023large} argue that maintaining performance on untargeted samples requires an additional normal dataset. \cite{chen2023unlearn} state that to preserve the model's performance while completely forgetting specific data, retraining the dataset is necessary. Furthermore, \cite{yao2024machine} utilizes KL-divergence on the retained dataset, following the approach outlined by\cite{yao2023large}.
\end{comment}