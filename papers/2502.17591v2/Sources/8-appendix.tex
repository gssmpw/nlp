\newpage
\appendix

{
\centering
\section*{Appendix}
}
\setcounter{proposition}{0}


\section{Existing Assets\label{existing_assets}}
The two existing assets used in this work are the Enron email dataset~\cite{klimt2004introducing} and aeslc dataset~\cite{zhang-tetreault-2019-email}. Please see their information below.

\begin{itemize}

\item Enron email dataset: The URL is http://www.enron-mail.com/email/; the
 license is not clearly stated by the authors.

\item aeslc dataset: The URL is https://huggingface.co/datasets/aeslc; the
 license is not clearly stated by the authors.

\end{itemize}


\section{Details of building ground truth table\label{build_ground_truth_table}}
To evaluate our defense method, we constructed a ground truth table comprising two parts: (1) We utilized the AWS Comprehend Service~\citep{aws2024comprehend} to extract PII, including names, phone numbers, and physical addresses; (2) we employed~\citep{manakul2023mqag} to determine the correlations between specific PIIs and the corresponding persons.

\section{Details of Evaluation Dataset\label{build_evaluation_dataset}}
For Enron email dataset, we constructed our evaluation ground truth table using the aelsc training dataset~\citep{zhang-tetreault-2019-email}. However, there is overlap between the aelsc training and validation datasets, which are used to train the soft prompt for the soft prompt attack. To ensure a fair comparison between soft prompt and probing attacks, we excluded certain persons' scores, as detailed in Appendix~\ref{details_enron_evaluation_dataset}. Consequently, our evaluation focused on 468 persons whose phone numbers were disclosed and 790 persons whose physical addresses were revealed.

For Fraud email dataset, we randomly selected 50 persons who had disclosed their phone numbers and 50 persons who had revealed their physical addresses to build our evaluation ground truth table.

\section{Evaluation Details of the Enron Email Experiment.\label{details_enron_evaluation_dataset}}
We constructed our evaluation ground truth table on the aeslc training dataset~\citep{zhang-tetreault-2019-email}, which comprises data from 1,359 persons. Within this dataset, 577 persons disclosed their phone numbers and 899 persons revealed their physical addresses. To ensure a fair comparison between soft prompt and probing attacks, we excluded persons whose data overlapped between the aeslc training and validation datasets, because we used aeslc validation dataset to train the soft prompt attack's soft prompt. The number of overlapping persons is 109. Consequently, our evaluation focused on 468 persons (577 - 109) whose phone numbers were exposed and 790 persons (899 - 109) whose physical addresses were exposed.
Subsequently, we utilized this evaluation ground truth table to assess the effectiveness of the defense methods.

\rebuttal{

\section{Comparison with Differential Privacy-Based methods \label{comparison_diff_privacy_method}}
We implemented the Differentially Private Decoding (DP Decoding) in ~\citep{majmudar2022differentially} and the Just Fine-Tune Twice (JFT) method in ~\citep{shi2022just}. To evaluate these methods, we conducted probing attacks on both DP Decoding and JFT. Specifically, for DP Decoding, the parameter $\lambda$ impacts both utility (the effectiveness of the generated output) and privacy (the protection of sensitive information). As $\lambda$ varies from 0 to 1, a value closer to 0 enhances privacy but reduces utility, while a value closer to 1 improves utility at the expense of privacy. We tested various values of the $\lambda$ parameter ranging from (0.1, 0.3, 0.5, 0.7, 0.9) and we selected $\lambda$=0.3 as it achieved the best balance between utility and privacy protection among the tested values, offering better utility compared to $\lambda$=0.1, and improved defense capability compared to $\lambda$=0.5, as shown in Table~\ref{tab:dp_decoding_lambda}. We observed that our PPA method still outperformed both DP Decoding and JFT, achieving a lower risk score and a higher utility score, as shown in Table~\ref{tab:comparison_diff_privacy_method}. This superior performance can be attributed to the fact that DP Decoding applies a uniform distribution adjustment to next-token predictions, which lacks the necessary customization for scenarios involving PII. 

\input{Tables/10_5_DP_Decoding_table}
\input{Tables/10-differential_privacy_method_table}



\section{Model performance evaluation on MMLU and TruthfulQA \label{truthfulqa_metric}}
We provide some new evaluations on the model's performance metrics on both MMLU~\citep{hendrycks2020measuring} and TruthfulQA~\citep{lin2021truthfulqa}. As our research primarily focuses on the text generation capabilities of models, we had the models that have been protected by various defense methods respond to the MMLU and TruthfulQA questions directly. GPT-4o was then employed to rate these responses on a scale from 1 to 5, where 5 represents the best possible score and 1 the worst. Given the extensive volume of the MMLU dataset, and in order to manage computational costs efficiently, we selected 20 data points from each subtask to form a representative subset, totaling 1,140 data points. For each defense method, we calculate the mean score for comparative analysis between defense methods. We found that PPA achieves the highest MMLU and TruthfulQA score among all baseline defense methods, as illustrated in Table~\ref{tab:truthfulqa_defense_comparison}.
\input{Tables/11-truthfulqa_metric_table}


\section{Stronger Attacker has prior knowledge of the PII \label{stronger_prior_knowledge}}
We have implemented a more advanced attack scenario where the attacker possesses prior knowledge of the PII. Specifically, we assume the attacker knows the information from the beginning of the PII up to a key element. For instance, in the case of "John Griffith phone number (713) 853-6247," the key element is "8". In this scenario, the attacker’s prompt would resemble: "The phone number of John Griffith is (713) 8".

As shown in the Table~\ref{tab:strong_attacker_prior_knowledge}, we observe that PPA achieves the best balance between defense capability and model performance.

\input{Tables/12-strong_attacker_prior_knowledge}


\section{Details of the proportions of key elements\label{key_elements_proportions}}
We calculated the proportions of key element lengths relative to the total lengths for phone numbers and physical addresses, which are 6.7\% and 27.6\%, respectively.



\section{More discussion about Memory Implanting\label{discuss_memory_implanting}}
We modified the memory implanting component to focus on replacing the key element with a different token. For instance, in the example 'John Griffith's phone number is (713) 853-6247,' where the key element is '8', we selectively forgot '8' and replaced it with a different number at its position. We observe that the Modified Memory Implanting PPA provides same protection for users' phone numbers and outperforms PPA in GPT-4o EmailScore by approximately 9.6\%, as shown in Table~\ref{tab:modified_memory_implanting}. However, Address substitution presents challenges because addresses are highly contextually dependent. Replacing a key element in an address with an arbitrary token can impair the model’s understanding of the context. For example, substituting '\_Su' in 'Jeffrey Dasovich address 101 California St. Suite 1950' disrupts the model’s comprehension of the address structure. Additionally, partial substitution may inadvertently expose parts of the user's address. Discussing how to customize selective analysis and memory implanting for different types of PII is a pertinent issue. Design memory implanting to optimize performance for various PII types is valuable and can be our future work.

\input{Tables/13-modified_memory_implanting_table}

\section{Adding the exposure metric\label{exposure_metric}}


We calculated the exposure metric~\citep{carlini2019secret} for all baseline methods. Since calculating the exposure of PII is computationally intensive, we followed the approach in Table 2 of ~\citep{carlini2019secret} and evaluated the exposure for 10 phone numbers. Our results show that PPA outperforms other baseline defense methods, as shown in Table~\ref{tab:exposure_metric}
\input{Tables/14-exposure_metric_table}

\section{Evaluation on email address.\label{evaluate_email_address}}
We conducted an additional experiment to evaluate the protection of 281 users' email addresses in the aeslc training dataset. Using Levenshtein distance~\citep{po2020similarity}, we compared the predicted email addresses to the ground truth. As shown in the Table~\ref{tab:email_defense_model_scores}, PPA successfully defends all users' email addresses against probing attacks while maintaining model performance comparable to other baseline defense methods.
\input{Tables/15-email_address_table}

\section{Originally safe information to be exposed?\label{original_safe_expose}}
Motivated by the concern that the PPA defense could inadvertently expose previously secure PII of users who are not explicitly protected by the method.
We evaluated the exposure metric~\citep{carlini2019secret} for safe phone numbers—those not exposed to attackers—that were not protected by the PPA method, using both the no-defense setup and the PPA model. Given the time-intensive nature of calculating PII exposure, we referenced Table 2 from The Secret Sharer ~\citep{carlini2019secret} and analyzed the exposure of 10 such phone numbers. The average exposure for these cases is summarized in the Table~\ref{tab:exposure_other_user}.

As shown in the Table~\ref{tab:exposure_other_user}, the exposure of phone numbers not protected by the PPA method decreases slightly, from 1.57 (no defense) to 1.22 (PPA), since the PPA method does not directly target these users for protection. This result suggests that the original safe information remains secure even when the PPA method is applied to protect other users' PII.

\input{Tables/16-exposure_for_not_protected_users_table}


\section{Discuss scalability for PPA\label{PPA_scalability}}

We have conducted an initial investigation into the scalability and optimization strategies for PPA. Our experiments involved combining PPA with efficient fine-tuning techniques, such as LoRA ~\citep{hu2021lora}, using a rank of 16 and an alpha value of 32. As shown in the Table~\ref{tab:ppa_lora}, applying LoRA to PPA produced promising results: after fine-tuning for three epochs, the risk score reduced to 1.0, and after four epochs, it further decreased to 0.0, all while maintaining comparable model performance. Although PPA with LoRA required four epochs, compared to just one epoch for full fine-tuning of PPA, it achieved the same defensive effectiveness.

Table~\ref{tab:ppa_lora} demonstrates that PPA has potential for scalability. Furthermore, exploring additional optimization strategies could be a valuable direction for future work. 

\input{Tables/17-PPA_lora_table}


\section{Ablation study on various forgetting strategies\label{forgetting_strategies}}
We conducted ablation studies on protecting the user's physical address using the following three strategies: 1) Only the most sensitive tokens (PPA), 2) Span around the most sensitive token, and 3) The most sensitive tokens + the following tokens. Their respective Risk Scores and GPT-4o Email Scores are presented in Table~\ref{tab:forget_strategies}.

As shown in the Table~\ref{tab:forget_strategies}, "Only the most sensitive token (PPA)" achieves an optimal balance between defense capability and model performance. "Span around the most sensitive token" exhibits slight utility degradation but does not significantly reduce risk beyond what is achieved by focusing on the most sensitive token alone. "The most sensitive tokens + the following tokens" strategy reduces the risk score further but at the cost of significantly lowering the GPT-4o Email Score. This indicates that although including the following tokens enhances defense performance, forgetting too many tokens will cause a worse trade-off in utility.

Our experimental results demonstrate that selectively forgetting only the most sensitive tokens secures a better balance between defense capability and model performance. The defense effectiveness of this approach is comparable to the other two methods, while it also maintains better utility. Given these findings, the selective forgetting of only the most sensitive tokens is still the most effective strategy for the PPA framework.

\input{Tables/18-forgetting_strategies_table}
}


\section{Details of model performance perplexity metric\label{perplexity_details}}

In Tables~\ref{phone_enron_defense_table}, ~\ref{address_enron_defense_table}, and \ref{fraud_defense_table}, we report the average Perplexity across three different tests to evaluate model performance. The detailed results of each individual Perplexity test are provided in Tables\ref{phone_perplexity_enron_table}, ~\ref{address_perplexity_enron_table}, and ~\ref{perplexity_fraud_table}.


\input{Tables/7-phone_perplexity_enron_table}
\input{Tables/8-address_perplexity_enron_table}
\input{Tables/9-perplexity_fraud_table}


\section{Details of the Baseline defense methods\label{baseline_defense_details}}
\textbf{Empty Response}~\citep{patil2023can, ouyang2022training}\textbf{.} This method refines the model to label non-sensitive information as "dummy". For instance, we create templates for each person, formatted with the person’s name, PII type, and "dummy". We then perform gradient descent on "dummy" following the training settings outlined in Appendix~\ref{training_setting} with a single epoch.

\textbf{Error Injection.} We implemented the Error Injection method on each person's phone numbers, conducting a single epoch of training. This same process is used to preserve a person's physical addresses. Take a person's phone number as an example, we create templates for each person, structured as the person’s name, PII type, and fake PII, which is generated by~\citep{presidioResearch2024}. We then apply gradient descent to false PII, adhering to the training settings detailed in Appendix~\ref{training_setting}.

\textbf{Unlearning}~\citep{jang2022knowledge}\textbf{.} We applied an unlearning technique to the PII sequence by performing gradient ascent on it, following the training settings specified in Appendix~\ref{training_setting} with a single epoch.

\textbf{DEPN}~\citep{wu2023depn}
We adopted the DEPN approach, as detailed in the DEPN GitHub repository, to protect PII, specifically phone numbers and physical addresses. Our goal was to eliminate specific neurons from the output of the LlamaDecoderLayer in the LlamaModel~\citep{meta_llama_2_7b}. We established a threshold ratio of 0.01 for both phone numbers and physical addresses, with mode ratio bags set at 0.49 and 0.5, respectively. Following this, we removed 10,000 neurons based on the identified candidates. 


\section{Details of Attack methods\label{attack_details}}
For the input rephrasing attack, we generated 20 attack templates based on the twin template described in~\citep{kim2024propile}.

For soft prompt tuning in the Enron email experiment, we used the first probing twin template from~\citep{kim2024propile}, leveraging the aeslc validation ground truth table. In the Fraud email experiment, we applied the same probing template, selecting 25 persons with phone numbers and 25 with physical addresses randomly from the fraud email dataset.

\section{Details of Attack Success Metric\label{attack_success_metric_details}}
For the total phone numbers and physical address risk score, we calculate the average phone number risk score and average physical address risk score for each person. We then aggregate the phone numbers and physical address risk scores of all persons to compute our final phone numbers and physical address risk score, following the same methodology as the exact match score for phone numbers and physical addresses.

\section{Model Performance Metric\label{model_performance_metric_details}}
For the \textbf{Perplexity metric}, we conducted three different tests to assess model performance. First, we calculated perplexity~\citep{huggingface_perplexity} on the first 512 tokens of each text (with a maximum length of 512 tokens). Second, we computed the perplexity for each letter, using a maximum length of 512 tokens and a stride of 256 tokens. Third, we assessed the perplexity of letters generated by GPT-4~\citep{achiam2023gpt}, but the Fraud email experiment did not have this metric, because GPT-4 cannot write fraud emails due to its safety aligned mechanism. These three tests help us determine whether our defense method impacts model performance. 

For the \textbf{Email completion metric} in the Enron email experiment, we evaluated the model's performance in completing truncated emails. Specifically, we tasked the model with completing 40 truncated emails, which were subsequently evaluated by GPT-4o~\citep{openai2024gpt4o}. Initially, GPT-4o generated 40 emails, each consisting of at least 100 words. We then truncated each email by half and had our models generate up to 100 new tokens to complete them. GPT-4o assessed and ranked the completions on a scale of 1 to 10, with 10 representing the best score. The average score was calculated across all 40 completions.

For the \textbf{Email completion metric} in the Fraud email experiment, we evaluated the model's ability to generate complete fraud emails. The model was tasked with generating 10 fraud emails, each up to 500 tokens, which were also judged by GPT-4o. GPT-4o ranked these completions on the same 1 to 10 scale, with the average score being calculated across all 10 fraud email completions.

\section{Proof}
\subsection{Proof of Proposition \ref{proposition1}}
\begin{proposition} 
Maximizing the memorization factor can lead to
\begin{align}
    \max_k D(k) = \left\{
    \begin{array}{lll}
        \max_k L(k)&\text{if } \exists k, \nabla L(k)=0,    \\
        \max_k 1/d_{\text{Newton}}(k)& \text{if } \nexists k, \nabla L(k)=0.
    \end{array}
\right.
\end{align}
$d_\text{Newton}(k)$ is Newton's Direction at $k$, which is from Newton Method in convex optimization~\citep{boyd2004convex}. $\max_k 1/d_{\text{Newton}}(k)$ is achieved when $d_{\text{Newton}}(k)\rightarrow 0^+$. As $L(k)$ is monotonically non-decreasing, a small positive $d_\text{Newton}(k)$ implies that the gradient at token $k$ quickly decreases with a negative second-order derivative.
\begin{proof}
Notice that 
\begin{align}
    L(k) = &-\sum_{\vx_1\cdots,\vx_k} p(\vx_1\cdots,\vx_k)\log q(\vx_1\cdots,\vx_k)\label{eq:accumulate_H}\\
    =&-\sum_{\vx_1\cdots,\vx_{k-1}} p(\vx_1\cdots,\vx_{k-1})\log q(\vx_1\cdots,\vx_{k-1})\nonumber\\
    &-\sum_{\vx_1\cdots,\vx_{k-1}} p(\vx_1\cdots,\vx_{k-1})\sum_{\vx_k}p(\vx_k|\vx_1\cdots,\vx_{k-1})\log q(\vx_k|\vx_1\cdots,\vx_{k-1})\\
    =&L(k-1)+H_k,
\end{align}
So we have 
\begin{align}
    &H_k=L(k)-L(k-1)\approx \nabla L(k),\\
    &H_{k+1}-H(k) \approx \nabla L(k+1)-\nabla L(k)\approx \nabla^2 L(k),\\
    &D_k = \frac{H_k-H_{k+1}}{H_k} \approx -\frac{\nabla^2 L(k)}{\nabla L(k)}.
\end{align}
Our selection method selects $k$ with the largest $D_k$. We discuss it in two situations:
\begin{enumerate}
    \item When there exists $k$ such that $H_k=\nabla L(k)=0$, we require that $\nabla^2L(k)<0$ to achieve the maximum ($D_k=+\infty$), this guarantees that $k$ achieves the maximum of $L(k)$ as well.
    \item When $H_k$ is always positive (notice that $H_k$ is never negative), $L(k)$ keeps growing as $k$ increases so we cannot find the maximum. But we still have
    \begin{align}
        \max_k D_k = \max_k \frac{1}{d_{\text{Newton}}(k)}=\min_k d_{\text{Newton}}(k),
        \label{eq:newton_direction}
    \end{align}
    where $d_{\text{Newton}}(k)=-\nabla L(k)/\nabla^2L(k)$ is \textit{Newton's Direction} in the second-order Newton's Method. The maximization is achieved when $d_{\text{Newton}}(k)\rightarrow 0^+$. Since $\nabla L(k)> 0$, $d_{\text{Newton}}(k)\rightarrow 0^+$ is achieved when $\nabla^2 L(k)=-\infty$, which implies that the gradient at $k$ quickly approaches $0$. 
\end{enumerate}
\end{proof}
\end{proposition}

\section{Training setting and hardware\label{training_setting}}
The training settings for fine-tuning the LLM on the Enron and Fraud email datasets, as well as for implementing defensive methods such as gradient descent and ascent, are as follows: a batch size of 4, the AdamW optimizer, a learning rate of 5e-5, weight decay of 0.001, a cosine learning rate scheduler, and a warmup ratio of 0.03. All experiments were conducted using 8 NVIDIA Quadro RTX 6000 24GB GPUs.

\begin{comment}
\section{Broader Impacts\label{boarder_impacts}}

Our work proposes a new method for preserving PII in LLMs, and we will explore the societal implications of techniques used for safeguarding PII.

\textbf{Positive impacts.} Safeguarding PII in LLMs while preserving their performance has significant positive implications. For instance, it can protect PII from fine-tuned LLMs with only a minimal drop in performance, ensuring that the LLMs remain effective for their intended purposes.


\textbf{Negative impacts.} Dynamic Mix Selected Unlearning consists of three stages: sensitivity analysis, selected unlearning, and error injection. These stages are unlikely to cause significant negative societal impacts.
\end{comment}

% \section{\JY{Additional Results? Please find a name for this section}}

\begin{figure*}[t!]
\centering
  \includegraphics[width=0.81\textwidth]{./images/phone_address_unlearning_break_even_points_trade_off_ver2.png}
  \caption{ Unlearning method trade-off: Risk score vs forget number of data. left: phone numbers; right: physical addresses }
  \label{phone_address_unlearning_break_even_trade_off}
\end{figure*}

\section{Additional analysis on unlearning scaling experiment}

\subsection{Unlearning method trade-off}
To analyze the break-even point of the unlearning method, we conducted experiments focusing on both phone numbers and address unlearning. We tested the forgetting of 20, 50, 100, 200, and 400 data points. The results indicate that as more data points are forgotten, a greater number of phone numbers and physical addresses are preserved. However, this leads to a deterioration in the model's performance, as illustrated in Figure~\ref{phone_address_unlearning_break_even_trade_off}. We discovered that forgetting between 200 and 400 data points significantly increases perplexity and it indicated that the break-even point for the unlearning method is when between 200 and 400 data points are forgotten.



\section{Broader Impacts.}
The societal implications of our work include positive impacts, as it can protect PII from fine-tuned LLMs with only a negligible drop in performance, ensuring that the LLMs remain effective for their intended purposes. And it is unlikely to cause significant negative societal impacts.