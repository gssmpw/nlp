\section{Proactive Privacy Amnesia
 \label{our_method_section}}
In this section, we introduce our method, PPA. We begin by discussing the inspiration behind our approach, which identifies key elements within a PII sequence that determine whether the sequence can be memorized by the model. Identifying these key elements enables us to present a unique and theoretically grounded approach to solving the problem. Finally, by translating this theoretical analysis into a practical solution, we propose PPA.

%, designed to forget a user's PII while preserving the model's performance. The method consists of three stages: Sensitivity Analysis, Selective Forgetting, and Memory Implanting. Sensitivity Analysis identifies the key elements in the PII sequence that determine whether it can be retained. Selective Forgetting ensures the LLM forgets these key elements, and Memory Implanting compensates for the performance degradation in the LLM.




\begin{comment}
In this section we introduce our method, Dynamic Mix Selected Unlearning, which consists of three stages: Sensitivity Analysis, Selected Unlearning, and Error Injection~\citep{de2021editing}. Sensitivity Analysis is to analyze which tokens within the PII sequence are the key elements determine whether it can be retained. Selected Unlearning is to let LLM to forget the specific key elements. Error Injection is to compensate the downgrade on the LLM performance. We start by discussing our inspiration of our method. Then, we provide theory analysis on our method. Last, we formulate the proposed Dynamic Mix Selected Unlearning to forget user's PII while maintaining the model's performance.
\end{comment}

\subsection{Inspiration and Overview}

Our Proactive Privacy Amnesia is inspired by Anterograde Amnesia ~\citep{markowitsch2008anterograde}, which is the inability to form new memories following an event while preserving long-term memories before the event. In a case study described by \citet{vicari2007acquired}, a girl suffering from Anterograde Amnesia since childhood exhibited severe impairment in episodic memory while retaining her semantic memory. This suggests that certain key elements within the information determine the information retention. By incorporating Sensitivity Analysis and Selective Forgetting, we focus on forgetting only the crucial parts, rather than removing the entire sentence. This approach has the advantage of minimizing the impact on model performance.
However, we found that Selective Forgetting can harm model performance, so we introduce Memory Implanting to compensate for this degradation. Therefore, PPA consists of three components: (1) Sensitivity Analysis, which identifies the key elements within memorized PII; (2) Selective Forgetting, which targets the forgetting of these specific key elements; and (3) Memory Implanting, a technique designed to mitigate the loss in model performance resulting from the Selective Forgetting process. 
% \MK{explain three components of PPA too redundant?}


\begin{comment}
By identifying and selectively forgetting these key elements, LLM can forget specific information while maintaining overall performance. This is because only the crucial parts of the information are forgotten, rather than the entire sentence. Therefore, we aim to apply PPA to remove PII from LLMs while preserving their effectiveness for their intended purposes.
\end{comment}
\begin{comment}
\textbf{Inspiration.} Our Proactive Privacy Amnesia is inspired by Anterograde Amnesia ~\citep{markowitsch2008anterograde}, which is the inability to form new memories following an event while preserving long-term memories from before the event. In a case study described by~\citep{vicari2007acquired}, a girl suffering from Anterograde Amnesia since childhood exhibited severe impairment in episodic memory while retaining her semantic memory. This suggests that certain key elements within the information determine whether it can be retained. By identifying and selectively forgetting these key elements, LLM can forget specific information while maintaining overall performance. This is because only the crucial parts of the information are forgotten, rather than the entire sentence. Therefore, we aim to apply PPA to remove PII from LLMs while preserving their effectiveness for their intended purposes.
\end{comment}


\subsection{Theoretical Justification of Sensitivity Analysis.}
\paragraph{Definition of Sensitivity Analysis.} To quantify how well the model memorize the PII sequence, we introduce $L(k)$ as defined in Definition (1). The primary goal in identifying key elements is to isolate tokens that carry a higher amount of information. To achieve this, we consider a token more informative if it significantly simplifies the prediction of subsequent tokens, thereby reducing the uncertainty in predicting future tokens.

% we measure the rate of change in cross-entropy during next-token prediction, focusing particularly on the transition from high to low. A token that significantly simplifies the prediction of subsequent tokens is considered more informative, as it greatly reduces the uncertainty in predicting future tokens.

% \textbf{Definition 1.}
\begin{definition} (Cross-entropy Loss of the PII Sequence) 
We define 
\begin{align}
    L(k) = L_{\text{CE}}\left(p(\rvx_1,\ldots,\rvx_k), q(\rvx_1,\ldots,\rvx_k)\right), \label{eq:L(k)_definition}
\end{align}
where $L_{\text{CE}}$ is the Cross Entropy Loss, and $x_1, \cdots, x_k$  refers to the first $k$ tokens of a PII sequence. 
\end{definition}

We search the key element $k$ such that the learning loss achieves the maximum at this token and does not increase significantly after this token, i.e., 
\begin{align}
    L(k-1) < L(k) \approx L(k+1) \approx L(k+2) \approx \cdots,
\end{align}
which means that the token $k$ helps the model memorize the following tokens in this PII sequence. Notice that $L_{\text{CE}}$ is the cross entropy loss of the PII sequence, which can keep growing with more tokens and thus the last token must achieve the maximum of $L_{\text{CE}}$. This solution is trivial and cannot show the essentiality of the token. To tackle this issue, we propose to find the token $k$ with the largest \textit{memorization factor} $D_k$, which can lead to a non-trivial solution of Eq. (\ref{eq:L(k)_definition}) as stated in Proposition \ref{proposition1}:

%Moreover, $\max_k L(k)$ leads to the 'memorization factor,' $D_i$, as defined in Proposition (1). A larger value of $D_i$ suggests that the token is more likely to be a key element.
\begin{definition} (Memorization Factor)
We define the memorization factor $D_k$ as follows: 
\begin{align}
    &D_k = \frac{H_k-H_{k+1}}{H_k}; H_i = L_{\text{CE}}(p_i,q_i),
\end{align}
Where \( p_i(x) \) be the true probability distribution and \( q_i(x) \) the predicted probability distribution for the \(i\)-th token in the PII sequence.
\end{definition}

\begin{proposition} \label{proposition1}
Maximizing the memorization factor can lead to
\begin{align}
    \max_k D(k) = \left\{
    \begin{array}{lll}
        \max_k L(k)&\text{if } \exists k, \nabla L(k)=0,    \\
        \max_k 1/d_{\text{Newton}}(k)& \text{if } \nexists k, \nabla L(k)=0.  
    \end{array}
\right.
\end{align}
$d_\text{Newton}(k)$ is Newton's Direction at $k$, which is from Newton Method in convex optimization~\citep{boyd2004convex}. $\max_k 1/d_{\text{Newton}}(k)$ is achieved when $d_{\text{Newton}}(k)\rightarrow 0^+$. As $L(k)$ is non-decreasing, a small positive $d_\text{Newton}(k)$ implies that the gradient at token $k$ quickly approaches $0$ with a negative second-order derivative.
\end{proposition}



\paragraph{Examples on PII sequences.}
We do sensitivity analysis on "John Griffith phone number (713) 853-6247," as shown in Figure~\ref{fig:phone_dmsu_sensitivity_analysis}, the token '8' exhibits the most significant decrease in cross-entropy rate, making it the key element in this context. Similarly, in "Jeffrey Dasovich address 101 California St. Suite 1950", depicted in Figure~\ref{fig:address_dmsu_sensitivity_analysis}, the token '\_Su' shows the most notable drop in cross-entropy rate, identifying '\_Su' as the key element.







\begin{comment}
First, we introduce the 'memorization factor', $D_i$, as defined in Eq. (\ref{eq:cross_entropy_loss_ratio}
),
\begin{align}
% H_i = -\sum_{x} p_i(x) \log q_i(x)
D_{i} = \frac{H_{i} - H_{i+1}}{H_{i}}; H_i = CrossEntropyLoss(p_i, q_i)
\label{eq:cross_entropy_loss_ratio} 
\end{align}
which is motivated by information theory. The primary goal of identifying key elements is to isolate tokens that carry a greater amount of information. To achieve this, we measure the rate of change in cross-entropy during next-token prediction, focusing particularly on the transition from high to low. A token that significantly simplifies the prediction of subsequent tokens is considered more informative, as it greatly reduces the uncertainty in predicting future tokens. A larger $D_i$, suggests that the token is more likely to be a key element.

\textbf{Theoretical Justification of Sensitivity Analysis.} We uncover the relationship between our sensitivity-based selection and the second-order Newton's Method. We consider the following optimization problem that finds the maximum of the cross-entropy loss: 

\textbf{Definition 1.} 
\begin{align}
    \max_k L(k) = L_{\text{CE}}(p(\rvx_1,\cdots,\rvx_k),q(\rvx_1,\cdots,\rvx_k)).
\end{align}

\textbf{Proposition 1.} The memorization factor $D_k$ is expressed as follows: 
\begin{align}
    &D_k = \frac{H_k-H_{k+1}}{H_k} \approx -\frac{\nabla^2 L(k)}{\nabla L(k)}.
\end{align}



Notice that 
\begin{align}
    L(k) = &-\sum_{\vx_1\cdots,\vx_k} p(\vx_1\cdots,\vx_k)\log q(\vx_1\cdots,\vx_k)\label{eq:accumulate_H}\\
    =&-\sum_{\vx_1\cdots,\vx_{k-1}} p(\vx_1\cdots,\vx_{k-1})\log q(\vx_1\cdots,\vx_{k-1})\nonumber\\
    &-\sum_{\vx_1\cdots,\vx_{k-1}} p(\vx_1\cdots,\vx_{k-1})\sum_{\vx_k}p(\vx_k|\vx_1\cdots,\vx_{k-1})\log q(\vx_k|\vx_1\cdots,\vx_{k-1})\\
    =&L(k-1)+H_k,
\end{align}
where $H_k$ is what we defined in Eq. (\ref{eq:cross_entropy_loss_ratio}). So we have 
\begin{align}
    &H_k=L(k)-L(k-1)\approx \nabla L(k),\\
    &H_{k+1}-H(k) \approx \nabla L(k+1)-\nabla L(k)\approx \nabla^2 L(k),\\
    &D_k = \frac{H_k-H_{k+1}}{H_k} \approx -\frac{\nabla^2 L(k)}{\nabla L(k)}.
\end{align}
Our selection method selects $k$ with the largest $D_k$. We discuss it in two situations:
\begin{enumerate}
    \item When there exists $k$ such that $H_k=\nabla L(k)=0$, we require that $\nabla^2L(k)<0$ to achieve the maximum ($D_k=+\infty$), this guarantees that $k$ achieves the maximum of $L(k)$ as well.
    \item When $H_k$ is always positive (notice that $H_k$ is never negative), $L(k)$ keeps growing as $k$ increases so we cannot find the maximum. But we still have
    \begin{align}
        \max_k D_k = \max_k \frac{1}{d_{\text{Newton}}(k)},
        \label{eq:newton_direction}
    \end{align}
    where $d_{\text{Newton}}(k)=-\nabla L(k)/\nabla^2L(k)$ is \textit{Newton's Direction} in the second-order Newton's Method. The maximization is achieved when $d_{\text{Newton}}(k)\rightarrow 0^+$, which implies that $k$ is close to the solution that maximizes $D(k)$. 
\end{enumerate}
\end{comment}


% $D_i$, which is specific to each token. A larger $D_i$, suggests that the token is more likely to be a key element.




\begin{comment}
\textbf{How to find the key elements.}
The primary goal of identifying key elements is to isolate tokens that carry more information than others. To achieve this, we use the rate of change in cross-entropy during next-token prediction, particularly the transition from high to low, as a measure of token informativeness. A token that simplifies the prediction of subsequent tokens indicates that it carries more information, as it significantly reduces uncertainty in predicting the following tokens. Thus, we introduce the 'memorization factor', $D_i$, which is specific to each token. A larger $D_i$, suggests that the token is more likely to be a key element. $D_i$ as defined in Eq. (\ref{eq:cross_entropy_loss_ratio}).
Based on Eq. (\ref{eq:newton_direction}), we identify the key element (k) that is close to the solution maximizing $D_i$. This implies that Newton's direction tends towards zero, as a diminishing Newton's direction indicates that k is approaching the optimal solution~\citep{boyd2004convex}.

For example, in "John Griffith phone number (713) 853-6247," as shown in Figure~\ref{phone_dmsu_sensitivity_analysis}, the token '8' exhibits the most significant decrease in cross-entropy rate, making it the key element in this context. Similarly, in "Jeffrey Dasovich address 101 California St. Suite 1950," depicted in Figure~\ref{address_dmsu_sensitivity_analysis}, the token '\_Su' shows the most notable drop in cross-entropy rate, identifying '\_Su' as the key element.
\end{comment}

\begin{comment}
The main idea for identifying key elements is to find tokens that are more difficult to predict compared to the next token. This approach has two advantages: (1) it forgets tokens that were originally hard to predict, and (2) it ensures that common-sense tokens, which have a high probability of following the previous token, are retained. For example, in the sequence '<s> Kay Mann address 29 Inverness Park Way, Houston, TX, 77055', as illustrated in Figure~\ref{address_dmsu_sensitivity_analysis}, the token 'ver' would not be selected for forgetting, as it follows 'In' with high probability based on common sense. If our method chose to forget 'ver' in 'Inverness,' it would degrade model performance because the language model would lose semantic knowledge related to words containing 'Inver'. Therefore, the best key element to forget should be 'In', not only because it is harder to predict than the next token, but also because forgetting 'In' does less damage to model performance compared to 'ver'.
\end{comment}

\begin{comment}
In a PII sequence, the output distribution at each token position is associated with a cross-entropy loss based on the prediction of the next token, which is the ground truth token. A larger cross-entropy loss between the output distribution and the ground truth token indicates greater difficulty in predicting the token at that position.
\end{comment}


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth} % Align at top
        \centering
        \includegraphics[width=\textwidth]{./images/phone_dmsu_sensitivity_analysis.png}
        \caption{Sensitivity analysis on phone number example: 'John Griffith phone number (713) 853-6247'. '8' is the largest $D_i$ within '(713) 853-6247'.}
        \label{fig:phone_dmsu_sensitivity_analysis}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth} % Align at top
        \centering
        \includegraphics[width=\textwidth]{./images/address_dmsu_sensitivity_analysis.png}
        \caption{Sensitivity analysis on physical address example: "Jeffrey Dasovich address 101 California St. Suite 1950". '\_Su' is the largest $D_i$ within '101 California St. Suite 1950'.}
        \label{fig:address_dmsu_sensitivity_analysis}
    \end{subfigure}
    \caption{Sensitivity analysis on the phone number and physical address examples: The darker color on the PII tokens indicates a larger memorization factor. The red dot in the figure represents the top-1 key element.}
    \label{fig:sensitivity_analysis}
\end{figure}

\begin{comment}
\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/phone_dmsu_sensitivity_analysis.jpg}
        \caption{Sensitivity Analysis on phone number example: 'John Griffith phone number (713) 853-6247'. '8' is the largest $D_i$ within '(713) 853-6247'.}
        \label{phone_dmsu_sensitivity_analysis}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/address_dmsu_sensitivity_analysis.jpg}
        \caption{Sensitivity Analysis on address example: "Jeffrey Dasovich address 101 California St. Suite 1950". '\_Su' is the largest $D_i$ within '101 California St. Suite 1950'.}
        \label{address_dmsu_sensitivity_analysis}
    \end{minipage}
    \caption{Sensitivity Analysis on the phone and address example: The darker color on the PII tokens indicates a larger memorization factor. The red dot in the figure represents the top-1 key element.}
\end{figure}
\end{comment}

% Sensitivity Analysis on the phone and address example: Based on Equation~\ref{eq:accumulate_H}, the value of $L(k)$ is the accumulated cross-entropy between the predicted next token at each position and the ground truth token. Based on Equation~\ref{eq:cross_entropy_loss_ratio}, the value of $D_i$, memorization factor, determines the importance of forgetting a token. The larger the $D_i$, the more crucial it becomes to forget that specific token. The darker color on the PII tokens indicates a larger $D_i$. The red dot in the figure represents the token with the largest $D_i$ compared to the other tokens in the PII sequence.


\subsection{Formulating PPA}
\label{formulate_our_method}

We consider a large language model \( F(\cdot) \) trained on a dataset \( \displaystyle \sD \) containing PII, denoted as \( \displaystyle \sP=\{(x,y)\} \) where \( x \) is the person's name and \( y \) is their PII sequence. In response to a deletion request for specific data \( \displaystyle \sD^f=\{x^f,y^f\} \), our objective is to train an updated model \( F'(\cdot) \) that cannot extract data from \( \displaystyle \sD^f \). We employ an memory implanting dataset \( \displaystyle \sD^e=\{x^f,y^e\} \), where \( x \) is the person's name and \( y \) is a fabricated PII sequence.

% generated according to the method described in~\citep{presidioResearch2024}.


\
\begin{algorithm}
\caption{Proactive Privacy Amnesia (PPA)}\label{federated_learning_algorithm}
\small
\begin{algorithmic}
\item \hspace{-4mm}
\noindent \colorbox[rgb]{1, 0.95, 1}{
\begin{minipage}{0.98\columnwidth}


\textbf{\textbf{Initialization}}.
Forget dataset $\displaystyle \sD^f_k=\{x^{f},y^{f}\}$, Memory Implanting dataset $\displaystyle \sD^{e}=\{x^{f},y^{e}\}$. Large Language Model \( F(\cdot) \) with parameters $\boldsymbol{w}$. Weights of the model $\Delta \boldsymbol{w}$. The key elements that the model needs to forget $\displaystyle \sD^f_k$. Total number of users $U$, $u=0$.
% Total number of users $K$, $k=0$.

% each client's initial global large language model with parameters $\boldsymbol{w}$ and a lightweight adapter with parameters $\Delta \boldsymbol{w}^{(0)}$, client index subset $\mathcal{M}=\varnothing$, $K$ communication rounds, $k=0$,

\end{minipage}
}
\item \hspace{-4mm}
\colorbox[gray]{0.95}{
\begin{minipage}{0.98\columnwidth}
\item  \textbf{Defensive Training}


\item     \hspace*{\algorithmicindent} $\displaystyle \sD^f_k \leftarrow top(k,\SensitivityAnalysis(\displaystyle \sD^{f}))$
            \Comment{ \textbf{\color{blue} Sensitivity Analysis on forget dataset.}}

\item     \hspace*{\algorithmicindent} \textbf{while} $u \leq U$ \textbf{do}

\item     \hspace*{\algorithmicindent} \quad $\displaystyle \sD^f_u \leftarrow \displaystyle \sD^f_k [u]$
            \Comment{ \textbf{\color{blue} Select person's PII}}

\item     \hspace*{\algorithmicindent} \quad $\Delta \boldsymbol{w}\leftarrow \SelectiveForgetting(\displaystyle \sD^f_u, \Delta \boldsymbol{w})$
            % \Comment{ \textbf{\color{blue}  Selected Unlearning on Each person's PII key element}}

\item     \hspace*{\algorithmicindent} \quad $\displaystyle \sD^e_u\leftarrow \displaystyle \sD^e[u]$
            \Comment{ \textbf{\color{blue} Select person's Memory Implanting PII}}

\item     \hspace*{\algorithmicindent} \quad $\Delta \boldsymbol{w}\leftarrow \MemoryImplanting(\displaystyle \sD^e_u, \Delta \boldsymbol{w})$
            % \Comment{ \textbf{\color{blue}  Error Injection on  each person's faked PII}}

\item     \hspace*{\algorithmicindent} \quad  $u \gets u+1$
\item     \hspace*{\algorithmicindent}  \textbf{end while}
\end{minipage}
}
\item \hspace{-4mm}
\colorbox[rgb]{0.95, 0.98, 1}{
\begin{minipage}{0.98\columnwidth}

\item  \textbf{Outcome:}

\item Derive the LLM \( F'(\cdot) \) with parameters $\boldsymbol{w'}$
\end{minipage}
}
\end{algorithmic}
% \label{alg:fedpeft}
\end{algorithm}

% Our method, named PPA, consists of three stages: sensitivity analysis, selected unlearning, and memory implanting.

\textbf{Sensitivity Analysis.} 
Initially, we create unlearning templates for each person's PII, structured as the personâ€™s name, PII type, and the PII sequence. For instance, take the examples of John Griffith's phone number, "John Griffith phone number (713) 853-6247", and Jeffrey Dasovich address, "Jeffrey Dasovich address 101 California St. Suite 1950".
Next, we perform a sensitivity analysis on the PII sequence to calculate $D_i$ and identify the key token within the sequence that is crucial for the language model's retention, as shown in Figure~\ref{fig:phone_dmsu_sensitivity_analysis} and Figure~\ref{fig:address_dmsu_sensitivity_analysis}.
\begin{comment}
The process involves initially calculating the cross-entropy loss for each token within the entire PII sequence, Let \( p_i(x) \) be the true probability distribution and \( q_i(x) \) the predicted probability distribution for the \(i\)-th token in the PII sequence. \( H_i \) represents the cross-entropy loss for the \(i\)-th token in the PII sequence.
Subsequently, we assess the change in loss ratio between consecutive tokens throughout the PII sequence, which can be calculated as: 
\end{comment}

We then apply top$_k$ to $D_i$, calculated as follows:
% The token exhibiting the $\text{top}_k$ change ratio is then designated as the key elements, calculated as:
\begin{align}
%\text{top}_k(D_1, D_2, \dots, D_n) = \{x_{D_1}, x_{D_2}, \dots, x_{D_k}\}
\text{top}_k(D_1, D_2, \dots, D_n) = \{x_{1}, x_{2}, \dots, x_{k}\} \label{eq:topk} 
\end{align}

%The process involves initially calculating the perplexity for the entire PII, can be calculated as:[equation!!!!] .

% Subsequently, we assess the change in perplexity ratio between consecutive tokens throughout the PII sequence, can be calculated as:[equation!!!!]. 

% The token exhibiting the largest change ratio is then designated as the key element, can be calculated as:[equation!!!!].

\textbf{Selective Forgetting.} Then, we maximize the following loss function, on the key element tokens \( x = (x_1, \dots, x_k) \) based on Equation~\ref{eq:topk}, which can be calculated as:
\begin{align}
\mathcal{L}_{UL}(F_\theta, x) = - \sum_{t=1}^k \log(p_\theta(x_t | x_{<t}))
\label{eq:selected_unlearning}
\end{align}
Here, \( x_{<t} \) represents the PII sequence of tokens \( x = (x_1, \ldots, x_{t-1}) \), and \( p_\theta(x_t | x_{<t}) \) is the conditional probability that the next token will be \( x_t \), given the preceding sequence \( x_{<t} \), in a language model \( F \) parameterized by \( \theta \).



\textbf{Memory Implanting.} After that, we apply the memory implanting, borrowed idea from error injection~\citep{de2021editing}, to compensate for the performance damage done by the selective forgetting is calculated as follows: 
\begin{align}
\arg \max_{M} p(y^* | x; F_\theta)
\end{align}
where $y^*$ represents the alternative, false target as proposed by~\citep{presidioResearch2024}.

\begin{comment}
\subsection{Theoretical Justification of Sensitivity Analysis} 
In this section, we uncover the relationship between our sensitivity-based selection and the second-order Newton's Method. We consider the following optimization problem that finds the maximum of the cross-entropy loss: 
\begin{align}
    \max_k L(k) = L_{\text{CE}}(p(\rvx_1,\cdots,\rvx_k),q(\rvx_1,\cdots,\rvx_k)).
\end{align}
Notice that 
\begin{align}
    L(k) = &-\sum_{\vx_1\cdots,\vx_k} p(\vx_1\cdots,\vx_k)\log q(\vx_1\cdots,\vx_k)\label{eq:accumulate_H}\\
    =&-\sum_{\vx_1\cdots,\vx_{k-1}} p(\vx_1\cdots,\vx_{k-1})\log q(\vx_1\cdots,\vx_{k-1})\nonumber\\
    &-\sum_{\vx_1\cdots,\vx_{k-1}} p(\vx_1\cdots,\vx_{k-1})\sum_{\vx_k}p(\vx_k|\vx_1\cdots,\vx_{k-1})\log q(\vx_k|\vx_1\cdots,\vx_{k-1})\\
    =&L(k-1)+H_k,
\end{align}
where $H_k$ is what we defined in Eq. (\ref{eq:cross_entropy_loss_ratio}). So we have 
\begin{align}
    &H_k=L(k)-L(k-1)\approx \nabla L(k),\\
    &H_{k+1}-H(k) \approx \nabla L(k+1)-\nabla L(k)\approx \nabla^2 L(k),\\
    &D_k = \frac{H_k-H_{k+1}}{H_k} \approx -\frac{\nabla^2 L(k)}{\nabla L(k)}.
\end{align}
Our selection method selects $k$ with the largest $D_k$. We discuss it in two situations:
\begin{enumerate}
    \item When there exists $k$ such that $H_k=\nabla L(k)=0$, we require that $\nabla^2L(k)<0$ to achieve the maximum ($D_k=+\infty$), this guarantees that $k$ achieves the maximum of $L(k)$ as well.
    \item When $H_k$ is always positive (notice that $H_k$ is never negative), $L(k)$ keeps growing as $k$ increases so we cannot find the maximum. But we still have
    \begin{align}
        \max_k D_k = \max_k \frac{1}{d_{\text{Newton}}(k)},
        \label{eq:newton_direction}
    \end{align}
    where $d_{\text{Newton}}(k)=-\nabla L(k)/\nabla^2L(k)$ is \textit{Newton's Direction} in the second-order Newton's Method. The maximization is achieved when $d_{\text{Newton}}(k)\rightarrow 0^+$, which implies that $k$ is close to the solution that maximizes $D(k)$. 
\end{enumerate}
\end{comment}


% Let Sequence be $x_n=\{a_1,a_2,\cdots,a_n\}$
% $$
% \max_k \frac{H(a_k|a_1,\cdots,a_{k-1})-H(a_{k+1}|a_1,\cdots,a_k)}{H(a_k|a_1,\cdots,a_{k-1})}\\
% =\max_k \frac{[H(a_1,\cdots,a_{k-1},a_k)-H(a_1,\cdots,a_{k-1})]-[H(a_1,\cdots,a_k,a_{k+1})-H(a_1,\cdots,a_{k-1},a_k)]}{H(a_1,\cdots,a_{k-1},a_k)-H(a_1,\cdots,a_{k-1})}
% $$
% Let $H(k)=H(a_1,\cdots,a_{k-1},a_k)$, we have
% $$
% \max_k \frac{\nabla H(k)-\nabla H(k+1)}{\nabla H(k)}=\max_k\frac{-\nabla^2H(k)}{\nabla H(k)}=\min_k -\frac{\nabla H(k)}{\nabla^2 H(k)}
% $$
% where $d(k)=-\frac{\nabla H(k)}{\nabla^2 H(k)}$ is the Newton's Direction: In a second order optimization algorithm (Pure Newton Method):
% $$
% \max_{k} H(k)\\
% k\leftarrow k+d(k)
% $$
% We search for the token with minimal step size for maximizing the entropy, which means that the current sequence has reached almost the maximimal entropy, and appending a new token does not introduce much information.