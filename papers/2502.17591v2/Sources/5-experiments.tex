\section{Experiments\label{experiments}}
In the experiments, we demonstrate that our PPA effectively preserves PII while maintaining model performance across multiple settings. 

\subsection{SETUP\label{experimental_setup}}
\paragraph{Benchmarks.}
We conduct extensive experiments by fine-tuning LLaMA2-7b model~\citep{touvron2023llama} and LLaMA3-8b model~\citep{dubey2024llama} on two different datasets: 1) \textbf{Enron email experiment} which fine-tune LLM on Enron email dataset~\citep{klimt2004introducing} 2) \textbf{Fraud email experiment} which fine-tune LLM on Fraud email dataset~\citep{radev2008clair}. To evaluate our defense method, we construct separate ground truth tables, details in Appendix~\ref{build_ground_truth_table}, and evaluation dataset for the Enron email dataset and the Fraud email dataset, specified in Appendix~\ref{build_evaluation_dataset}.
\paragraph{Attack methods.} 
We implemented the \textbf{input rephrasing attack}\citep{patil2023can, krishna2024paraphrasing} to generate multiple attack templates. Additionally, we employed the \textbf{probing attack} using the twin template probing method described in \citet{kim2024propile}, and the \textbf{soft prompt attack}~\citep{kim2024propile} using trained soft prompts, attacking persons' phone numbers and physical addresses\footnote{All attack methods employed the AWS Comprehend Service~\citep{aws2024comprehend} to extract PII from the model output.}, more details in Appendix~\ref{attack_details}.
\paragraph{Baseline defense methods.}
We consider 4 representative defense methods as our baseline. \textbf{Empty Response}~\citep{patil2023can, ouyang2022training} applies gradient descent to non-sensitive information, used as a "dummy" to replace PII sequences. \textbf{Error Injection} ~\citep{de2021editing} using gradient descent to increase the likelihood of generating fake PII sequence. \textbf{Unlearning}~\citep{jang2022knowledge} do gradient ascent on PII sequence. \textbf{DEPN}~\citep{wu2023depn} use memory editing technique to erase the neurons, which significantly contribute privacy leakage, in the model, more details are in Appendix~\ref{baseline_defense_details}.
\paragraph{PPA (ours).} The PPA, as detailed in Section \ref{our_method_section}, to protect PII. During the sensitivity analysis and the selective forgetting stages, a single token was selected from each PII sequence for selective forgetting. In particular, we established \( k=1 \) in Equations \ref{eq:topk} and \ref{eq:selected_unlearning}. Both selective forgetting and memory implanting stages were implemented following the training guidelines specified in Appendix \ref{training_setting} with a single epoch.

\subsection{Evaluation Metrics\label{evaluation_metrics}}
\paragraph{Attack Success Metric}
In this paper, we propose our PII risk score metric and apply a modified exact match score metric \citep{kim2024propile} in our experiments. For the phone number risk score, we utilize an eighth-order Levenshtein distance \citep{po2020similarity} to compare the predicted phone number with the ground truth.
For calculating the risk score of physical addresses, we first use the AWS Location Service~\citep{aws2024location} to geocode a location and obtain detailed physical address information. Then, we compare the details of the predicted physical address with the ground truth physical address using our physical address risk score Table~\ref{address_risk_score_table}. 
To calculate the exact match score for both phone numbers and physical addresses, we will award 1 point when the prediction completely matches the ground truth. More scoring details are in Appendix \ref{attack_success_metric_details}.

\input{Tables/1-addresss-risk-score}


\paragraph{Model Performance Metric}
We employ two primary metrics, which are widely used for evaluating LLMs, to measure their performance: 1) Perplexity~\citep{touvron2023llama, radford2019language, brown2020language}, averaged by three different perplexity tests, and 2) Email completion, where we evaluate the content of email completions using LLM Judge~\citep{thakur2024judging, verga2024replacing, zhang2024towards}. For the Perplexity metric, a lower value generally indicates better performance~\citep{blei2003latent}. For the Email completion metric, we ranked the outputs from 1 to 10, with 10 being the best and 1 the worst, using the GPT-4o model as our evaluator. Further details on the model performance metric can be found in Appendix \ref{model_performance_metric_details}.

\input{Tables/2-phone_enron_defense}

\input{Tables/3-address_enron_table}

\subsection{Main results\label{main_results}}
\textbf{Notation for Experimental Tables.}
RS denotes the risk score, while EM represents the exact match score. 'Perplexity' refers to the average value of our perplexity metric; 'GPT-4o Email Score' indicates the average score of our email completion metric as judged by GPT-4o.

\textbf{Enron email experiment.}
For the phone number defense results, Table~\ref{phone_enron_defense_table} shows that our method, PPA, effectively protects the phone numbers of all persons, achieving both a phone number risk score and a phone number exact match score of zero while maintaining model performance comparable to Fine-tuned LLaMA2-7b and LLaMA3b-8b. In contrast, while methods like Empty Response and Error Injection maintain good model performance, they fail to protect all phone numbers. Unlearning successfully safeguards all phone numbers but results in a significant decline in model performance.

Table~\ref{address_enron_defense_table} shows that our method, PPA applied to LLaMA2-7b for defense against physical address exposure, outperforms both Empty Response and Error Injection by reducing the risk score by 87.6\% and 26.2\%, respectively. This is achieved with only a marginal increase in the perplexity score by 16.7\% and 35.4\%, and a slight decrease in the Email Completion score by 30.7\% and 29.4\%. Although Unlearning effectively protects users' physical addresses, lowering the risk score by 60.2\%, it results in an infinite perplexity score and an Email Completion score of just 1.0. Additionally, PPA outperforms DEPN by reducing the risk score by 9.8\%, decreasing the perplexity score by 91.0\%, and increasing the Email Completion score by 157.1\%.
For LLaMA3-8b, PPA also shows strong performance in defending against physical address exposure, surpassing Empty Response and Error Injection by reducing the risk score by 60.3\% and 16.2\%, respectively. It achieves this while slightly decreasing the perplexity score by 26.9\% and increasing it by 9.7\%, with only a marginal decrease in the Email Completion score by 16.6\% and 4.7\%. Although Unlearning remains effective, reducing the risk score by 83.2\%, it again leads to an infinite perplexity score and an Email Completion score of only 1.0. PPA outperforms DEPN by reducing the risk score by 16.2\%, lowering the perplexity score by 71.4\%, and increasing the Email Completion score by 166.6\%.

\textbf{Fraud email experiment.}
Table~\ref{fraud_defense_table} shows that PPA effectively protects the phone numbers of 50 persons, achieving a phone number risk score of 0.3 while maintaining model performance comparable to that of the Fine-tuned LLaMA2-7b model. Similarly, PPA safeguards the physical addresses of the other 50 persons, achieving an physical address risk score of 3.0 without compromising model performance relative to the Fine-tuned LLaMA2-7b model.


\input{Tables/4-fraud_defense}

\begin{comment}
\subsection{Experimental Setup\label{experimental_setup}}

\subsubsection{Fine-tuned Dataset\label{dataset}}


\textbf{Enron email experiment: Fine-tuned Dataset}
We fine-tuned the LLaMA2-7b model~\citep{touvron2023llama} and LLaMA3-8b model~\citep{dubey2024llama} on the Enron email dataset~\footnote{http://www.enron-mail.com/email/}. The fine-tuning process involved two epochs with training settings in Appendix~\ref{training_setting}.
% This was done to reserve the remaining letters as our model performance test dataset.
% Subsequently, we used the fine-tuned LLaMA2-7b model to attack the PII of persons in the aeslc training dataset~\footnote{https://huggingface.co/datasets/aeslc}, which is included in the Enron email dataset.

\textbf{Fraud email experiment: Fine-tuned Dataset}
In addition, we fine-tuned the LLaMA2-7b model~\citep{touvron2023llama} using the fraud email dataset~\citep{radev2008clair}, conducting a training process over twenty epochs, as outlined in Appendix~\ref{training_setting}. 
% Following the fine-tuning, we used the model to perform PII attack on random 50 persons.


\subsubsection{Ground Truth table\label{ground}}

\textbf{Evaluation Ground Truth table.}
To evaluate our defense method, we constructed a ground truth table comprising two parts: (1) We utilized the AWS Comprehend Service~\citep{aws2024comprehend} to extract PII, including names, phone numbers, and physical addresses; (2) we employed~\citep{manakul2023mqag} to determine the correlations between specific PIIs and the corresponding persons. 

\subsubsection{Evaluation Dataset\label{evaluation_dataset}}

\textbf{Enron email experiment: Evaluation Dataset.}
We constructed our evaluation ground truth table using the aelsc training dataset~\citep{zhang-tetreault-2019-email}. However, there is overlap between the aelsc training and validation datasets, which are used to train the soft prompt for the soft prompt attack. To ensure a fair comparison between soft prompt and probing attacks, we excluded certain persons' scores, as detailed in Appendix~\ref{details_enron_evaluation_dataset}. Consequently, our evaluation focused on 468 persons whose phone numbers were disclosed and 790 persons whose physical addresses were revealed.
\end{comment}



\begin{comment}
We constructed our evaluation ground truth table on the aeslc training dataset~\citep{zhang-tetreault-2019-email}, which comprises data from 1,359 persons. Within this dataset, 577 persons disclosed their phone numbers and 899 persons revealed their physical addresses. To ensure a fair comparison between soft prompt and black-box attacks, we excluded persons whose data overlapped between the aeslc training and validation datasets, because we used aeslc validation dataset to train the soft prompt attack's soft prompt. The number of overlapping persons is 109. Consequently, our evaluation focused on 468 persons (577 - 109) whose phone numbers were exposed and 790 persons (899 - 109) whose physical addresses were exposed.
Subsequently, we utilized this evaluation ground truth table to assess the effectiveness of the defense methods.


\textbf{Fraud email experiment: Evaluation Dataset.}
We randomly selected 50 persons who had disclosed their phone numbers and 50 persons who had revealed their physical addresses to build our evaluation ground truth table. % Additionally, we randomly selected 25 persons with phone numbers and 25 with physical addresses to train the soft prompt for the soft prompt attack.
\end{comment}
\begin{comment}
\textbf{Ground Truth table}
We constructed our ground truth table using the aeslc training dataset~\citep{zhang-tetreault-2019-email}, which comprises data from 1,359 people. Within this dataset, 577 people disclosed their phone numbers and 899 people revealed their physical addresses.
Subsequently, we utilized this ground truth table to assess the effectiveness of the defense methods.
\end{comment}

\begin{comment}
\subsubsection{Attack Methods}

\textbf{Probing Attack.}
We implemented the twin template probing attack described in~\citep{kim2024propile} to target persons' phone numbers and physical addresses. Subsequently, we employed the AWS Comprehend Service~\citep{aws2024comprehend} to extract PII from the output.
We also implemented the input rephrasing attack from~\citep{patil2023can, krishna2024paraphrasing} to attack persons' phone numbers and physical addresses.

\textbf{Soft Prompt Attack.}
We adapted the twin template soft prompt attack described in~\citep{kim2024propile} to specifically persons' phone numbers and physical addresses, with certain modifications. 
For soft prompt tuning in the Enron email experiment, we used the first probing template from~\citep{kim2024propile}, leveraging the aeslc validation ground truth table. In the Fraud email experiment, we applied the same probing template, selecting 25 persons with phone numbers and 25 with physical addresses randomly from the fraud email dataset.

\subsubsection{Defense Methods.}
We compare our approach with three types of postprocessing defense methods: Gradient Descent, Gradient Ascent, and Memory Editing. In order to have a fair comparison, we did not compare our method with a combination of Gradient Descent and Ascent because this approach requires an additional dataset, which we did not use.

\textbf{Empty Response}~\citep{patil2023can, ouyang2022training}\textbf{.} This method refines the model to label non-sensitive information as "dummy". For instance, we create templates for each person, formatted with the person’s name, PII type, and "dummy". We then perform gradient descent on "dummy" following the training settings outlined in Appendix~\ref{training_setting} with a single epoch.

\textbf{Error Injection.} We implemented the Error Injection method on each person's phone numbers, conducting a single epoch of training. This same process is used to preserve a person's physical addresses. Take a person's phone number as an example, we create templates for each person, structured as the person’s name, PII type, and fake PII, which is generated by~\citep{presidioResearch2024}. We then apply gradient descent to false PII, adhering to the training settings detailed in Appendix~\ref{training_setting}.

\textbf{Unlearning}~\citep{jang2022knowledge}\textbf{.} We applied an unlearning technique to the PII sequence by performing gradient ascent on it, following the training settings specified in Appendix~\ref{training_setting} with a single epoch.

\textbf{DEPN}~\citep{wu2023depn}
We adopted the DEPN approach, as detailed in the DEPN GitHub repository, to protect PII, specifically phone numbers and physical addresses. Our goal was to eliminate specific neurons from the output of the LlamaDecoderLayer in the LlamaModel~\citep{meta_llama_2_7b}. We established a threshold ratio of 0.01 for both phone numbers and physical addresses, with mode ratio bags set at 0.49 and 0.5, respectively. Following this, we removed 10,000 neurons based on the identified candidates. 

\textbf{PPA (ours)} The PPA, as detailed in Section \ref{our_method_section}, to protect PII. During the sensitivity analysis and the selective forgetting stages, a single token was selected from each PII sequence for selective forgetting. In particular, we established \( k=1 \) in Equations \ref{eq:topk} and \ref{eq:selected_unlearning}. Both stages were implemented following the training guidelines specified in Appendix \ref{training_setting} with a single epoch.
\end{comment}

%\subsubsection{Evaluation Metrics\label{evaluation_metrics}}

% Metrics for PII Exposing Score



\begin{comment}
\textbf{Attack Success Metric.}
In this paper, we propose our PII risk score metric and apply a modified exact match score metric \citep{kim2024propile} in our experiments. For the phone number risk score, we utilize an eighth-order Levenshtein distance \citep{po2020similarity} to compare the predicted phone number with the ground truth.
For calculating the risk score of physical addresses, we first use the AWS Location Service~\citep{aws2024location} to geocode a location and obtain detailed physical address information. Then, we compare the details of the predicted physical address with the ground truth physical address using our physical address risk score Table~\ref{address_risk_score_table}. 
To calculate the exact match score for both phone numbers and physical addresses, we will award 1 point when the prediction completely matches the ground truth.
For the total phone numbers and physical address risk score, we calculate the average phone number risk score and average physical address risk score for each person. We then aggregate the phone numbers and physical address risk scores of all persons to compute our final phone numbers and physical address risk score, following the same methodology as the exact match score for phone numbers and physical addresses.
\end{comment}

\begin{comment}
\textbf{Attack Success Metric.}
In this paper, we propose our PII risk score metric and apply a modified exact match score metric \cite{kim2024propile} in our experiments. For the phone number risk score, we utilize an eighth-order Levenshtein distance \cite{po2020similarity} to compare the predicted phone number with the ground truth. We calculate the average phone risk score for each person. Finally, we sum the phone risk scores of all persons to derive our final phone risk score, following the same process as the phone exact match score.
For calculating the risk score of physical addresses, we first use the AWS Location Service~\citep{aws2024location} to geocode a location and obtain detailed address information. Then, we compare the details of the predicted address with the ground truth address using our physical address risk score Table~\ref{address_risk_score_table}. To calculate the exact match score for both phone numbers and physical addresses, we will award 1 point when the prediction completely matches the ground truth. We calculate the average address risk score for each person. Then, we aggregate the address risk scores of all persons to compute our final address risk score, using the same methodology as for the address exact match score.
\end{comment}

\begin{comment}
\textbf{Attack Success Metric.}
In this paper, we propose our PII risk score metric and apply a modified exact match score metric\cite{kim2024propile} in our experiments. For the phone number risk score, we use an eighth order of Levenshtein distance\cite{po2020similarity} to compare the predicted phone number with the ground truth. We assign 1 point for the phone number exact match score if the predicted phone number completely matches the ground truth, as shown in Table~\ref{phone_defense_table}. For calculating the risk score of physical addresses, we first use the AWS Location Service~\citep{aws2024location} to geocode a location and obtain detailed address information. Then, we compare the details of the predicted address with the ground truth address using our physical address risk score Table~\ref{address_risk_score_table}. For the exact match score of physical addresses, we also award 1 point if the predicted address fully matches the ground truth address, as shown in Table~\ref{address_defense_table}.
\end{comment}

\begin{comment}
\textbf{Model Performance Metric.}
We employ two primary metrics, which are widely used for evaluating LLMs, to measure their performance: 1) Perplexity~\citep{touvron2023llama, radford2019language, brown2020language}, and 2) Email completion, where we evaluate the content of email completions using LLM Judge~\citep{thakur2024judging, verga2024replacing, zhang2024towards}.

For the Perplexity metric, we conducted three different tests to assess model performance. First, we calculated perplexity~\citep{huggingface_perplexity} on the first 512 tokens of each text (with a maximum length of 512 tokens). Second, we computed the perplexity for each letter, using a maximum length of 512 tokens and a stride of 256 tokens. Third, we assessed the perplexity of letters generated by GPT-4~\citep{achiam2023gpt}, but the \textbf{Fraud email experiment} did not have this metric, because GPT-4 cannot write fraud emails due to its safety aligned mechanism. These three tests help us determine whether our defense method impacts model performance. In general, lower perplexity indicates better performance~\citep{blei2003latent}.

For the Email completion metric in the \textbf{Enron email experiment}, we evaluated the model's performance in completing truncated emails. Specifically, we tasked the model with completing 40 truncated emails, which were subsequently evaluated by GPT-4o~\citep{openai2024gpt4o}. Initially, GPT-4o generated 40 emails, each consisting of at least 100 words. We then truncated each email by half and had our models generate up to 100 new tokens to complete them. GPT-4o assessed and ranked the completions on a scale of 1 to 10, with 10 representing the best score. The average score was calculated across all 40 completions.

For the Email completion metric in the \textbf{Fraud email experiment}, we evaluated the model's ability to generate complete fraud emails. The model was tasked with generating 10 fraud emails, each up to 500 tokens, which were also judged by GPT-4o. GPT-4o ranked these completions on the same 1 to 10 scale, with the average score being calculated across all 10 fraud email completions.
\end{comment}

\begin{comment}
\textbf{Model Performance Metric.}
Perplexity is a major metric for measuring LLMs performance~\citep{touvron2023llama, radford2019language, brown2020language}.
We have conducted three different perplexity tests to evaluate model performance: First, we employed perplexity~\citep{huggingface_perplexity} to calculate the model performance on the first 512 tokens (with a maximum length of 512 tokens) of each letter that was not fine-tuned on the LLaMA2-7b model~\citep{touvron2023llama}. 
Second, we calculated the perplexity for each letter using a maximum length of 512 tokens and a stride of 256 tokens, without fine-tuning on the LLaMA2-7b model. Third, we calculated the perplexity of letters written by GPT-4~\citep{achiam2023gpt}. These three performance tests help us evaluate if the defense method downgrades model performance. In short, lower perplexity indicates better performance~\citep{blei2003latent}.
\end{comment}

\begin{comment}
\subsubsection{Training setting and hardware\label{training_setting}}
The training settings for fine-tuning the LLM on the Enron and Fraud email datasets, as well as for implementing defensive methods such as gradient descent and ascent, are as follows: a batch size of 4, the AdamW optimizer, a learning rate of 5e-5, weight decay of 0.001, a cosine learning rate scheduler, and a warmup ratio of 0.03. All experiments were conducted using 8 NVIDIA Quadro RTX 6000 24GB GPUs.
\end{comment}


\begin{comment}
\textbf{Phone Defense Results.} Our results demonstrate that Dynamic Mix Selected Unlearning effectively secures the privacy of 468 persons' phone numbers, achieving both a phone risk score and phone exact match score of zero, while maintaining robust model performance with an average perplexity of 15.95. This model performance is similar to the fine-tuned LLaMA2-7b model, which has a perplexity of 16.23. Conversely, the Empty Response and Error Injection methods, while preserving model performance with average perplexities of 16.52 and 14.55 respectively, register high average exposure phone risk scores of 56.44 and 21.71, failing to adequately protect phone numbers. The Unlearning method, although achieving zero in both phone risk and exact match scores, results in an impractically high average perplexity of 
3.26$\times10^{11}$. Meanwhile, DEPN protects most phone numbers with a score of 8.17 but leads to an average perplexity of 77.22. As indicated in Table~\ref{phone_defense_table}, Dynamic Mix Selected Unlearning presents the most effective balance in safeguarding persons' phone numbers while maintaining model performance.
\end{comment}

\begin{comment}
{\color{blue} 
\textbf{Enron email dataset.}
xxxxxx \textit{phone defense} xxxxxx \textit{address defense} LLaMA2, LLaMA3
}
\end{comment}

\begin{comment}
\subsection{Experimental Results}
\textbf{Enron email experiment.}
For the phone number defense results, Table~\ref{phone_enron_defense_table} shows that our method, PPA, effectively protects the phone numbers of 468 persons, achieving both a phone number risk score and a phone number exact match score of zero while maintaining model performance comparable to Fine-tuned LLaMA2-7b and LLaMA3b-8b. In contrast, although methods such as Empty Response, Error Injection, and DEPN fail to protect all phone numbers, Unlearning successfully safeguards all of them but causes a significant decline in model performance.

For the physical address defense results, Table~\ref{address_enron_defense_table} (LLaMA2-7b) demonstrates that our method, PPA, outperforms Empty Response and Error Injection by reducing the risk score by {87.6\%, 26.2\%}, respectively, with only a marginal increase in the perplexity score by {16.7\%, 35.4\%} and a slight decrease in the Email Completion score by {30.7\%, 29.4\%}. Although Unlearning effectively protects users' physical addresses, decreasing the physical address risk score by {60.2\%}, it results in an infinite perplexity score and an Email Completion score of only 1.0. PPA also outperforms DEPN by reducing the risk score by {9.8\%}, lowering the perplexity score by {91.0\%}, and increasing the Email Completion score by {157.1\%}.

Table~\ref{address_enron_defense_table} (LLaMA3-8b) indicates that our method, PPA, surpasses Empty Response and Error Injection by reducing the risk score by {60.3\%, 16.2\%}, while slightly decreasing the perplexity score by {26.9\%}, increasing it by {9.7\%}, and marginally decreasing the Email Completion score by {16.6\%, 4.7\%}. Although Unlearning effectively protects users' physical addresses, reducing the risk score by {83.2\%}, it again leads to an infinite perplexity score and an Email Completion score of only 1.0. PPA outperforms DEPN by reducing the risk score by {16.2\%}, lowering the perplexity score by {71.4\%}, and increasing the Email Completion score by {166.6\%}.


\textbf{Fraud email experiment.}
Table~\ref{fraud_defense_table} shows that PPA effectively protects the phone numbers of 50 persons, achieving a phone number risk score of 0.3 while maintaining model performance comparable to that of the Fine-tuned LLaMA2-7b model. Similarly, PPA safeguards the physical addresses of the other 50 persons, achieving an physical address risk score of 3.0 without compromising model performance relative to the Fine-tuned LLaMA2-7b model.
\end{comment}


\textbf{Main Results.}
We summarize the key results in Tables~\ref{phone_enron_defense_table}, ~\ref{address_enron_defense_table}, ~\ref{fraud_defense_table}, as follows: Observations from both phone number and physical address defenses indicate that PPA provides the best balance between safeguarding users' PII and maintaining model performance, compared to other defense methods.





\begin{comment}
Table~\ref{address_enron_defense_table} shows that our method, PPA, outperform the Empty Response, Error Injection, and DEPN by {71.0\%, 35.6\%, 9.8\%} on risk score, while marginal increase perplexity score by {14.3\%, 26.1\%} and decrease perplexity score by {}
\end{comment}


\begin{comment}
Applying Dynamic Mix Selected Unlearning on LLaMA2-7b and LLaMA3-8b effectively protects the privacy of 468 persons' phone numbers, achieving both a phone risk score and phone exact match score of zero, while maintaining a model performance score comparable to the Fine-tuned LLaMA2-7b and LLaMA3b-7b, as illustrated in Table~\ref{phone_defense_table} and Table~\ref{llama3_phone_defense_table}, respectively.
\end{comment}






\begin{comment}
\textbf{Main Results.}
We summarize the key results in Table~\ref{phone_defense_table} ~\ref{address_defense_table} as follows: 1) Dynamic Mix Selected Unlearning effectively preserves PII while maintaining model performance; 2) Aside from fine-tuned LLaMA2-7b and DEPN, the Input Rephrase attack is the most potent; and 3) Except for the original LLaMA2-7b and DEPN, the soft prompt attack is more formidable than the black-box attack.
\end{comment}

\begin{comment}
\textbf{Phone Defense Results.} Our findings show that Dynamic Mix Selected Unlearning successfully protects the privacy of 468 persons' phone numbers, achieving a phone risk score and phone exact match score of zero. Additionally, it maintains model performance, with a 1.2\% average drop in perplexity compared to the fine-tuned LLaMA2-7b model. Conversely, the Empty Response and Error Injection methods preserve model performance, with average perplexity increases of 2.5\% and decreases of 9.8\% respectively, registering high average exposure phone risk scores of 56.4 and 21.7, and failing to adequately protect phone numbers. The Unlearning method, although achieving zero in both phone risk and exact match scores, leads to an impractically high average increase in perplexity, reaching 2$\times10^{12}$\%. Meanwhile, DEPN safeguards the majority of phone numbers, achieving a score of 8.2, yet it results in an average increase in perplexity of 376.5\%. As indicated in Table~\ref{phone_defense_table}, Dynamic Mix Selected Unlearning presents the most effective balance in safeguarding persons' phone numbers while maintaining model performance.
\end{comment}

\begin{comment}
\textbf{Phone Defense Results.} Our analysis reveals that Dynamic Mix Selected Unlearning effectively preserves the privacy of 468 individuals' phone numbers, achieving a phone risk score and exact match score of zero, with only a marginal 1.2\% reduction in model performance relative to the fine-tuned LLaMA2-7b model. In contrast, the Empty Response and Error Injection methods, while maintaining model performance with perplexity increases of 2.5\% and decreases of 9.8\% respectively, register high phone risk scores of 56.4 and 21.7, indicating insufficient protection of phone numbers. The Unlearning method eliminates risk and match scores but results in an excessively high increase in perplexity, reaching $2\times10^{12}\%$. DEPN, though safeguarding most phone numbers with a score of 8.2, incurs a significant perplexity increase of 376.5\%. Table~\ref{phone_defense_table} shows that Dynamic Mix Selected Unlearning offers an optimal balance of privacy protection and model performance.


\textbf{Address Defense Results.} Our study demonstrates that Dynamic Mix Selected Unlearning effectively safeguards the privacy of 790 individuals' addresses, attaining an address risk score of 7.3. Furthermore, it maintains model performance, with an average increase in perplexity of 20.3\% compared to the fine-tuned LLaMA2-7b model. In contrast, the Empty Response and Error Injection strategies, though they sustain model performance with average perplexity increases of 3\% and decreases of 11\% respectively, exhibit high average exposure address scores of 59.2 and 9.9, thereby failing to provide adequate protection for addresses. The Unlearning method, despite achieving a low address risk score of 2.9, leads to an infinite average perplexity, rendering it impractical. Furthermore, while DEPN effectively protects most addresses with a score of 8.1, it results in a significantly higher average increase in perplexity of 2151.2\%. As depicted in Table~\ref{address_defense_table}, Dynamic Mix Selected Unlearning emerges as the optimal approach for securing persons' addresses while preserving model performance.
\end{comment}

\begin{comment}
\textbf{Address Defense Results.} Our research illustrates that Dynamic Mix Selected Unlearning is highly effective at protecting the privacy of 790 persons' addresses, achieving an address risk score of 7.33 and an address exact match score of 1, while also ensuring robust model performance with an average perplexity of 19.49. This performance is similar to the fine-tuned LLaMA2-7b model, which has a perplexity of 16.23. In contrast, the Empty Response and Error Injection strategies, though they sustain model performance with average perplexities of 16.73 and 14.36 respectively, exhibit high average exposure address scores of 59.15 and 9.88, thereby failing to provide adequate protection for addresses. The Unlearning method, despite achieving a low address risk score of 2.91 and address exact match score of 1, leads to an infinite average perplexity, rendering it impractical. Additionally, DEPN, while effectively safeguarding most addresses with a score of 8.1, incurs a substantially higher average perplexity of 218.94. As depicted in Table~\ref{address_defense_table}, Dynamic Mix Selected Unlearning emerges as the optimal approach for securing persons' addresses while preserving model performance.
\end{comment}



\begin{comment}
The results are presented in Table~\ref{phone_defense_table} ~\ref{address_defense_table}. We summarize the key conclusions as follows:
\begin{enumerate}[leftmargin=10pt]
  \item Dynamic Mix Selected Unlearning is a good preserved PII method, while maintaining model's performance.
  \item Aside from fine-tuned LLaMA2-7b and DEPN, the Input Rephrase attack stands out as the most potent.
  \item With the exception of the original LLaMA2-7b and DEPN, the white-box attack proves to be more formidable than the black-box attack.
\end{enumerate}
\end{comment}