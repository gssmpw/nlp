\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line only needs to identify funding in the first footnote. 
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{booktabs}%

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}


\newcommand\Missing[1]{{\color{blue}\em Missing: #1}}
\newcommand\Ehsan[1]{{\color{blue!40!black!30!red}\em Ehsan TODO: #1 }}


%SIX page limit!
\title{Multi Image Super Resolution Modeling for Earth System Models}

\author{
    \IEEEauthorblockN{Ehsan Zeraatkar}
    \IEEEauthorblockA{
        \textit{Computer Science}\\
        \textit{Texas State University}\\
        San Marcos, TX 78666\\
        ehsanzeraatkar@txstate.edu
    }
    \and
    \IEEEauthorblockN{Salah A Faroughi}
    \IEEEauthorblockA{\textit{Chemical Engineering} \\
    \textit{University of Utah}\\
    Salt Lake City, UT, USA\\
    salah.faroughi@utah.edu}
    
    \and
    \IEEEauthorblockN{Jelena Tešić\thanks{Corresponding author: jtesic@txstate.edu}}
    \IEEEauthorblockA{\textit{
        Computer Science}\\
        \textit{Texas State University}\\
        San Marcos, TX 78666\\
        jtesic@txstate.edu
    }
}


\begin{comment}
 \author{\IEEEauthorblockN{Ehsan Zeraathkar}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{Texas State University}\\
San Marcos, TX, USA\\
Ehsan
}
\and
\IEEEauthorblockN{Salah Faroughi}
\IEEEauthorblockA{\textit{Chemical Engineering} \\
\textit{University of Utah}\\
Salt Lake City, UT, USA\\
salah.faroughi@utah.edu}
\and
\IEEEauthorblockN{Jelena Te\v{s}i\'c}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{Texas State University}\\
San Marcos, USA\\
jtesic@txstate.edu}}
\end{comment}


\maketitle

%\vspace*{-2em}
\begin{abstract}
Super-resolution (SR) techniques are essential for improving Earth System Model (ESM) data's spatial resolution, which helps better understand complex environmental processes. This paper presents a new algorithm, ViFOR, which combines Vision Transformers (ViT) and Implicit Neural Representation Networks (INRs) to generate High-Resolution (HR) images from Low-Resolution (LR) inputs. ViFOR introduces a novel integration of Fourier-based activation functions within the Vision Transformer architecture, enabling it to effectively capture global context and high-frequency details critical for accurate SR reconstruction. The results show that ViFOR outperforms state-of-the-art methods such as ViT, Sinusoidal Representation Networks (SIREN), and SR Generative Adversarial Networks (SRGANs) based on metrics like Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) both for global as well as the local imagery. ViFOR improves PSNR of up to 4.18 dB, 1.56 dB, and 1.73 dB over ViT for full images in the Source Temperature, Shortwave, and Longwave Flux.
\end{abstract}



\begin{IEEEkeywords}
Earth System Model, Vision Transformer, Super Resolution, SIREN, INRs
\end{IEEEkeywords}

\section{Introduction}

An \emph{Earth System Model (ESM)} is a computer program designed to simulate how different parts of the Earth—such as the atmosphere, oceans, land, ice, and living things—work together as a system. These models include detailed information about physical processes, chemical reactions, and biological activities. By combining all these factors, ESMs help scientists study how the Earth changes over time due to natural events (like volcanic eruptions) or human actions (such as burning fossil fuels), particularly in climate change. \cite{collins2006}. The Earth System Models (ESMs) account for the roles of living things like forests, which absorb carbon and chemical processes, and represent more complex climate models than state-of-the-art ones focusing only on physical processes. Thus, ESMs can facilitate more complex predictions of how climate trends affect different parts of the Earth. ESMs data modeling predicts the rising sea levels caused by melting ice caps, more frequent extreme weather events like hurricanes, or changes in freshwater availability due to shifting rainfall patterns \cite{heinze2019}. The ESM data volume requires extensive computational resources for analysis. Thus, the high-resolution ESM designed to model the entire planet might produce less detailed or lower-resolution results when applied to a smaller area, such as a single county\cite{eyring2016}. Figure~\ref{fig-Zoom} illustrates how a high-resolution data model for the global prediction is a low-resolution data source if we want to focus on the specific locality. The local agencies often lack the advanced resources, expertise, or funding to create or run models as sophisticated as those developed by large national or international organizations. 

\begin{figure}[!ht]
 \centering
%\vspace*{-0.5em}
 \includegraphics[width=\linewidth]{figures/P2-Fig1.png} 
%\vspace*{-2em}
 \caption{Global scale HR image yields a very LR output when limited to a country scale.}
 \label{fig-Zoom}
%\vspace*{-2em}
\end{figure}

The E3SM-HR dataset originated from transforming the original E3SM simulation data from its non-orthogonal cubed-sphere grid to a regular longitude-latitude grid with a resolution of $0.25^{\circ} \times 0.25^{\circ}$ using bilinear interpolation. Each grid point represents a pixel in this grid, creating HR images with dimensions of $720 \times 1440$ pixels. To build the input images, the R, G, and B components are extracted from the normalized values of key climate variables: surface temperature, shortwave heat flux, and longwave heat flux at each grid point. Figure~\ref{fig-Global} shows three different measures, Temperature, Short wave flux, and Long wave flux, used to evaluate the proposed approach. 

The \emph{super-resolution} (SR) task is an important and rapidly growing area in computer vision, drawing significant interest from researchers. SR focuses on creating models, typically based on neural networks, that can take a low-resolution (LR) image and enhance it to produce a high-resolution (HR) version \cite{rahaman2019}. This capability has a wide range of applications, including improving the clarity of medical images for better diagnosis \cite{Bashir2021}, enhancing wildlife surveillance footage to aid in conservation efforts \cite{kuzmanic2007}, increasing the detail in reconnaissance images for security purposes \cite{Grosche2021}, preserving artwork and cultural artifacts digitally, and upgrading image quality in consumer electronics such as cameras and smartphones \cite{Maral2022}. Advancements in camera technology, including HR sensors, have significantly increased the demand for effective image enhancement techniques. However, SR remains a challenging problem due to its inherent complexity. One major issue is the ambiguity in reconstructing HR images from LR inputs: a single LR image can correspond to multiple possible HR outputs, making it difficult for models to determine the most accurate reconstruction\cite{ledig2017}. The effective SR solution must balance fine-detail preservation with computational efficiency, especially in applications requiring real-time processing, such as video streaming and augmented reality \cite{lim2017}. As a result, researchers continue to explore innovative methods to overcome these obstacles and push the boundaries of SR technology. While a simple calling down of the global modeling does not produce the desired results for modeling local climate trends \cite{vandal2017}, we propose the super-resolution construction of the local data when and if needed for the local modeling. Two distinct super-resolution tasks are at hand: the single-image super-resolution (SISR) and the multiple-image SR (MISR). In the single-image super-resolution (SISR) task, a single low resolution image and a single model is needed to reconstruct high resolution image, and the savings in storage are marginal. In contrast, the multiple-image super-resolution (MISR) combines information from multiple LR images of the same scene to generate a single HR image by leveraging different perspectives or frames \cite{Maral2022}. MISR is particularly useful for tasks like video super-resolution, where sequential frames provide additional context to enhance details in the final output.

\begin{figure}[!ht]
 \centering
 \includegraphics[width=\linewidth]{figures/Fig2.png} 
 %\vspace*{-2em}
 \caption{Panels (a), (b), and (c) show surface temperature, shortwave heat flux, and longwave heat flux, respectively, for the first month of year one obtained from the global fine-resolution configuration of E3SM.}
 \label{fig-Global}
 %\vspace*{-1em}
\end{figure}

Multiple super-resolution (SR) solutions recently offered a way to address different SR challenges. \cite{Bashir2021, Maral2022}. The advent of advanced methods, such as deep neural networks \cite{Kim2016VDSR}, vision transformers (ViTs) \cite{Dosovitskiy2020}, and diffusion models \cite{Dickstein2015}, has transformed the field of SR reconstruction. Vision Transformers have become widely adopted for their ability to model long-range dependencies in images \cite{touvron2021, Dosovitskiy2020}. The ViTs have limitations when applied to SR tasks, as they struggle to reconstruct high-frequency image details consistently \cite{bai2022}. Introducing Sinusoidal Representation Networks (SIRENs) has partially addressed this issue by improving the ability to reconstruct high-frequency components \cite{SIREN}. Building on this progress, we propose the \emph{Fourier Representation Network (FOREN)} for Multi-Image SR Reconstruction. In this approach, we replace the fully connected layers in the Vision Transformer with an implicit neural representation network that uses Fourier Transform Filters as activation functions. This design allows the model to specifically target either high-frequency or low-frequency features in the image, effectively addressing the \emph{spectral bias problem}—the tendency of models to favor low-frequency components over the high-frequency ones—common in SR tasks.


\begin{figure*}[!ht]
 \centering
 \includegraphics[width=\linewidth]{figures/P2-Pipline.png} 
 %\vspace*{-2em}
 \caption{ViFOR divides the input image into patches, pre-processes them using embedding and position encoding, finally feeds the input to a visual transformer followed by the SIREN architecture.}
 \label{fig-pipline}
% \vspace*{-1em}
\end{figure*}


\section{Related Work}
\label{sec-related}
Early Single-Image Super-Resolution (SISR) methods can be broadly divided into learning-based and reconstruction-based approaches \cite{Yang2010}. Learning-based methods include techniques such as pixel-based approaches \cite{Zhang2012}, which map individual LR pixels to their corresponding HR pixels using trained models. Another common strategy is example-based methods, where the system learns the relationships between LR and HR image pairs from a predefined database of examples \cite{freeman2002}. These approaches rely heavily on the availability of accurate example pairs and the assumption that similar patterns exist in the input image. The reconstruction-based methods leverage prior knowledge about image structures to define constraints for generating HR outputs. For instance, these methods may enforce sharp edges or enhance specific features in the reconstructed image by assuming certain smoothness or continuity properties \cite{Dai2007, Aly2005}. While these methods were effective in early applications, they struggled in real-world scenarios due to their reliance on simplified assumptions about image textures and structures, making them less capable of handling variability, noise, and complex patterns found in natural images. Deep Convolutional Neural Networks (CNNs) significantly improved Single and Multiple super-resolution baseline system performances. Dong et al. \cite{Dong2016} revolutionized the field with the SR Convolutional Neural Network (SRCNN), which autonomously learns an end-to-end mapping between LR and HR images. This model significantly outperformed traditional approaches, improving the PSNR by 0.15, 0.17, and 0.13 dB across three different datasets \cite{Dong2016}. Despite their advantages, CNNs face challenges such as spectral bias, where the networks excel at reconstructing smooth, low-frequency features but struggle with high-frequency details like sharp edges and fine textures \cite{Zhang2019}. To address these limitations, researchers proposed deeper and more advanced architectures. For instance, Very Deep SR (VDSR) extended the depth of CNNs to achieve better accuracy \cite{Kim2016VDSR}, while Enhanced Deep SR (EDSR) introduced additional residual blocks for improved feature extraction and HR reconstruction \cite{Kim2016EDSR}. Residual Dense Networks (RDN) further advanced this trend by integrating residual learning with dense connections, enabling more effective capture of fine-grained image details \cite{Zhang2018}.

Next, the Generalized Implicit Neural Representations (GINR) employ spectral graph embeddings to approximate discrete sample locations, allowing models to operate independently of specific coordinate systems \cite{grattarola2022ginr}. Similarly, Higher-Order Implicit Neural Representations (HOIN) use neural tangent kernels (NTK) to enhance feature interactions, effectively addressing spectral bias and improving performance on tasks requiring fine detail reconstruction \cite{chen2024hoin}. The deep generative models followed the trend, and other super resolution generative SRGANs are particularly effective in generating photo-realistic HR images by focusing on perceptual loss, prioritizing image quality as perceived by humans. These models successfully downscaled climate data, improving the resolution of regional precipitation projections \cite{shidqi2023}. Recently, multimodal methods integrating numerical weather prediction models with U-Net architectures and attention mechanisms enhanced temperature forecasts by leveraging spatial and temporal dependencies in the data \cite{Ding2024}.

Vision Transformers (ViTs) have emerged as a promising alternative to CNNs for SR tasks. ViTs excel at modeling long-range dependencies and capturing global context in images by processing image patches through self-attention mechanisms \cite{Dosovitskiy2020}. However, while ViTs outperform CNNs in capturing global patterns, they often struggle with reconstructing high-frequency details, such as textures and edges \cite{bai2022}. To address these shortcomings, hybrid approaches combining ViTs with Fourier-based representations or implicit neural networks have shown potential for improving SR performance. For example, sinusoidal Representation Networks (SIRENs) leverage periodic activation functions to mitigate spectral bias, making them particularly effective for recovering fine details in SR tasks \cite{SIREN}.

These advancements collectively highlight the diverse strategies employed to overcome the inherent challenges of SR tasks, ranging from improving local feature reconstruction to capturing global spatial dependencies. As SR technology continues to evolve, integrating multiple methodologies appears to be the key to achieving state-of-the-art performance across diverse applications. 


\section{Methodology}
\label{sec-method}
To address one of the major challenges with neural network-based SR methods, spectral bias, we propose a novel hybrid approach called \emph{Vision Transformer FOREN (ViFOR)}, which integrates the Vision Transformer (ViT) with a newly developed \emph{Fourier Representation Network (FOREN)}. In this design, the conventional multi-layer perceptron (MLP) layer in ViT is replaced with the FOREN structure to enhance the model's ability to learn low-frequency and high-frequency components independently, improving the quality of the reconstructed HR images in SR tasks.  

The \emph{FOREN}, inspired by the SIREN architecture \cite{SIREN}, leverages a Fourier transformer filter as its activation function. Doing so captures a broader range of image frequencies, helping to overcome spectral bias and produce better SR results. FOREN effectively targets frequency variations within the input data and enhances the network's ability to emphasize low-frequency and high-frequency details, improving the overall SR output quality. The proposed ViFOR leverages the VIT architecture that introduced a new paradigm for processing image data by applying transformer models directly to image patches \cite{Dosovitskiy2020}. ViT excels at capturing long-range dependencies and global context in images, making it ideal for complex datasets, such as those found in climate research and SR applications. Unlike convolution-based approaches, ViT requires fewer computational resources for training while still delivering state-of-the-art performance \cite{Dosovitskiy2020}. Figure~\ref{fig-pipline} illustrates the ViFOR pipeline. First, ViFOR segments the input image into patches, which are then fed to ViT. The self-attention mechanism within ViT processes these patches to capture long-range dependencies and global context, enabling the model to gain a deeper understanding of the image. The output from ViT is passed through the FOREN architecture, which emphasizes specific frequency ranges within the image data.

The \emph{FOREN} architecture incorporates low-pass or high-pass Fourier filter activation functions in its neural network structure, thus allowing the model to focus on learning different frequency components independently at both the ViT and the final output layers. To achieve this, we train two ViFOR networks: one dedicated to capturing low-frequency components and the other to high-frequency ones. The outputs of these two networks are then combined to produce the final HR image. This design preserves local details (captured through high-frequency components) and global context (captured through low-frequency components). before
By integrating these elements, the ViFOR architecture mitigates spectral bias and produces high-quality, upsampled HR images that retain fine details and overall image structure.

\begin{figure*}[!ht]
 \centering
 \includegraphics[width=\linewidth]{figures/P2-diff.png} 
 %\vspace*{-2em}
 \caption{Low-resolution images passed through the trained network to construct the HR one. This figure compares the HR output and three distinct evaluation metrics to the ground truth.}
 \label{fig-Diff}
 %\vspace*{-1em}
\end{figure*}

\section{Proof of Concept} 
\label{sec-POC}
Three distinct evaluation metrics are employed to evaluate the performance of the proposed algorithm compared to other state-of-the-art approaches. The following three metric assess the quality of the reconstructed images: 

\noindent \textbf{1. Mean Squared Error (MSE):} \\
The MSE quantifies the average difference in pixel intensity between the original image \( I_O \) and the reconstructed image \( I_R \). The dimensions of the images are \( M \) (height) and \( N \) (width), while \( I_O(i,j) \) and \( I_R(i,j) \) represent the pixel values at position \( (i,j) \) for the original and reconstructed images, respectively. The final MSE value for RGB images is the mean calculated across all pixels in all three channels. The MSE is defined as:  
\begin{equation}
\text{MSE} = \frac{1}{MN} \sum_{i=1}^{M} \sum_{j=1}^{N} \left( I_O(i,j) - I_R(i,j) \right)^2
\label{eq-mse}
\end{equation}  
A higher MSE value indicates a greater discrepancy between \( I_O \) and \( I_R \), signifying poorer reconstruction quality.  

\noindent \textbf{2. Peak Signal-to-Noise Ratio (PSNR):} \\
The PSNR is widely used to measure image quality in tasks like image compression and SR. It calculates the ratio between the maximum possible pixel intensity of the image (\(\text{MAX}\)) and the MSE. The PSNR is expressed as:  
\begin{equation}
\text{PSNR} = 10 \cdot \log_{10} \left(\frac{\text{MAX}^2}{\text{MSE}}\right)
\label{eq-PSNR}
\end{equation}  
A higher PSNR value indicates better image quality, with less error or distortion in the reconstructed image.  

Using these metrics—MSE and PSNR—we comprehensively evaluate the quality of the reconstructed images, ensuring an accurate comparison with existing state-of-the-art methods.

\section{Experimental Results}
\label{sec-Exp}

We used the University’s LEAP2 (Learning, Exploration, Analysis, and Process) Cluster to train and evaluate the model. The LEAP2 Dell PowerEdge C6520 Cluster has 108 compute nodes, each with 48 CPU cores, powered by two 24-core 2.4 GHz Intel Xeon Gold (IceLake) processors. Each node has 256 GB of memory and 400 GB of SSD storage, providing 27 TB of memory and 42 TB of local storage. We used 48 CPU cores, 256 GB of RAM, and 800 GB of SSD to train ViFOR and compare it with other state-of-the-art methods like ViT, SIREN, and SRGANs. 

To determine the optimal cutoff frequency for the Low-pass and High-pass activation functions in ViFOR, we analyzed a range of frequencies between 0.01 Hz and 1 Hz. The PSNR results, as shown in Figure~\ref{fig-Frequencyrange}, indicate that the best cutoff frequency for both filters is $0.3$ Hz. Specifically, for the high-pass filter, PSNR values remained relatively stable as the frequency increased to 0.3 Hz but declined significantly beyond this point. This behavior suggests that higher frequencies introduce noise or artifacts that negatively impact the reconstruction quality. For the low-pass filter, frequencies above 0.3 Hz yielded only marginal improvements in PSNR, indicating diminishing returns. The model achieves an optimal balance by selecting 0.3 Hz as the cutoff frequency, ensuring that both filters contribute effectively to the super-resolution process without compromising image quality.

\subsection{Experiment I: ViFOR for Sub-image}
The dataset used in this study consists of 360 monthly images spanning 10 years. Data are available for the three measures: Source Temperature, Shortwave Heat Flux, and Longwave Heat Flux. To handle each measure separately, we divided the dataset into 120 images for training and testing per measure.

This experiment divided each image into eight non-overlapping sub-images, resulting in $8 \times 120 =960$ sub-images for each model. Separate models were trained independently for each measure—Source Temperature, Shortwave Heat Flux, and Longwave Heat Flux—enabling a detailed evaluation of the proposed method across different data types.

We evaluated ViFOR's performance against other state-of-the-art models, including ViT, SIREN, and SRGANs, using two key metrics: MSE and PSNR. As summarized in Table~\ref{tab-Compare1}, the results demonstrate that ViFOR consistently outperformed other models in the MISR reconstruction task, delivering superior performance across all three measures.

\subsection{Experiment II: ViFOR for Full Image}
We evaluated ViFOR's performance in the second experiment using the original full images as input. Each measure retained 120 images for training and testing, but no sub-image segmentation was performed. This scenario was designed to assess the algorithm’s ability to leverage the full spatial context of the data and produce high-quality reconstructions.

The performance of ViFOR on full images was compared with its performance on sub-images and with state-of-the-art models using MSE and PSNR metrics. As shown in Table~\ref{tab-Compare1}, ViFOR outperformed all other algorithms in all three data categories, demonstrating superior reconstruction accuracy and image quality. Notably, using full images improved PSNR values compared to sub-image inputs, showcasing the model’s ability to effectively capture and utilize spatial dependencies.

Figure~\ref{fig-Diff} further illustrates ViFOR’s performance on a full-temperature image. For this example, the algorithm achieved a PSNR of 26.49 dB, highlighting its ability to produce highly accurate reconstructions. The difference map in the figure demonstrates negligible errors between the HR output and the ground truth, underscoring ViFOR’s capability to reconstruct fine details and maintain global coherence.

The comparison between the sub-image and full-image experiments emphasizes the significance of preserving spatial context for improved reconstruction quality. ViFOR’s consistent performance across diverse measures and input configurations illustrates a robust and versatile solution for super-resolution tasks in Earth System Models.


\begin{figure}[!ht]
 \centering
 \includegraphics[width=0.8\linewidth]{figures/P2-Frequencyrange.png} 
 %\vspace*{-1em}
 \caption{Investigating different frequencies for both low-pass and high-pass filter activation functions}
 \label{fig-Frequencyrange}
 %\vspace*{-1em}
\end{figure}

\begin{table}[!ht]
\centering
\caption{The values of MSE \% and PSNR dB for original $I_O$ and reconstructed $I_R$ images for three measurements, Source Temperature, shortwave heat flux, and longwave heat flux, and FOUR different models.}
\begin{tabular}{lcccccc}
\textbf{} & \multicolumn{2}{c}{\bf Sub-Image} & \multicolumn{2}{c}{\bf Full Image}\\ 
\toprule
\textbf{Measures $\rightarrow$} & \textbf{ MSE \%} & \textbf{PSNR dB} &\textbf{ MSE \%} & \textbf{PSNR dB}\\ 
\toprule
\textbf{Models $\downarrow$} & \multicolumn{4}{c}{\bf Source Temperature}\\
\midrule
ViT &54.3e-3&20.54&47.3e-3&22.54\\
SIREN &54.7e-3&20.24&66.7e-3&20.18\\
SRGANS &85.2e-3&18.43&72.7e-3&19.80\\
\textbf{ViFOR} &34.8e-3&23.72&16.8e-3&\textbf{26.72} \\
\toprule
\textbf{} & \multicolumn{4}{c}{\bf Shortwave heat flux}\\ \midrule
ViT &56.4e-3&21.37&47.4e-3&23.67\\
SIREN &65.4e-3&20.46&47.4e-3&23.45\\
SRGANS &64.7e-3&20.23&66.7e-3&20.62\\
\textbf{ViFOR} &28.3e-3&24.45&28.3e-3&\textbf{25.23} \\
\toprule
\textbf{} & \multicolumn{4}{c}{\bf Longwave heat flux}\\
\midrule
ViT &34.2e-3&23.53&35.3e-3&24.50\\
SIREN &57.2e-3&21.87&47.3e-3&22.32\\
SRGANS &72.2e-3&19.33&66.7e-3&20.18\\
\textbf{ViFOR} &28.3e-3&24.35&21.3e-3&\textbf{26.23} \\
\bottomrule
\end{tabular}\label{tab-Compare1}
\end{table} 



\section{Discussion and Conclusion}
This study introduces a new ViFOR algorithm designed to improve SR in ESMs. The algorithm combines the strengths of Vision Transformers (ViTs) and Fourier Representation Networks (FORENs), providing accurate and high-quality reconstructions of HR images from LR inputs. The main part of the experiment involved testing the algorithm using two types of input: divided sub-images and whole images.

The results show that ViFOR performs better than other advanced methods like ViT, SIREN, and SRGANs based on two important metrics: MSE and PSNR. Using divided sub-images gives good results, but when whole images are used, the performance improves even more, achieving the highest PSNR values in all tests. The superior performance of ViFOR can also be attributed to its integration of Fourier-based activation functions, which effectively mitigate spectral bias by ensuring a balanced focus on low-frequency and high-frequency components of the input data. This capability is particularly important in ESM applications, where capturing fine-scale details, such as temperature gradients or heat flux variations, is crucial for accurate modeling. Furthermore, ViFOR's use of Vision Transformers allows it to model long-range dependencies and global context, essential for understanding large-scale environmental patterns. These combined features make ViFOR a powerful tool for super-resolution tasks and a promising approach for advancing the analysis of high-resolution climate datasets. The better PSNR values when using whole images may be because the model can better understand the spatial relationships in the data, helping it learn more about the overall structure. However, dividing the images into smaller pieces makes it harder for the model to capture the full context, reducing the reconstruction quality. The difference map in Figure~\ref{fig-Diff} shows how well ViFOR reconstructs high-frequency details using whole images. The small errors in the map show that ViFOR closely matches the real data, which is important for climate modeling and Earth system science. In conclusion, ViFOR performs well in SR tasks as a flexible and reliable solution for improving the resolution of complex environmental data.
\bibliographystyle{IEEEtran}
\input{2025ICME_main.bbl}
% \bibliography{Bib/INR,Bi b/DataLab,Bib/TesicPub}% common bib file

\end{document}
