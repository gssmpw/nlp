%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}

\usepackage{subcaption}


%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{iScience of the Cell Press}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

% \title{From Knowledge Generation to Knowledge Verification: A Deep Dive into the Generative Capabilities of ChatGPT}

\title{From Knowledge Generation to Knowledge Verification: Examining the BioMedical Generative Capabilities of ChatGPT}





%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[inst1,inst3]{Ahmed Abdeen Hamed*}

\affiliation[inst1]{organization={MGEN -- College of Engineering, Northeastern University Miami},%Department and Organization
            % addressline={Address One}, 
            city={Miami},
            postcode={33127}, 
            state={FL},
            country={United States}}

\author[inst4]{Alessandro Crimi}
\author[inst5]{Magdalena M Misiak}
\author[inst2]{Byung Suk Lee}

\affiliation[inst2]{organization={Department of Computer Science, University of Vermont},%Department and Organization
            % addressline={Address Two}, 
            city={Burlington},
            postcode={05405}, 
            state={VT},
            country={United States}}
            
\affiliation[inst3]{organization={CASCI Laboratory, Binghamtom University},%Department and Organization
            % addressline={Address Two}, 
            city={Binghamton},
            state={NY},            
            postcode={13902}, 
            country={United States}}

\affiliation[inst4]{organization={AGH, University of Krakow, Faculty of Informatics},%Department and Organization
            % addressline={Address Two}, 
            city={Krakow},          
            postcode={30-059}, 
            country={Poland}}
            
\affiliation[inst5]{organization={Department of Physiology and Biophysics, Howard University},%Department and Organization
            % addressline={Address Two}, 
            city={Washington},
            state={DC},            
            postcode={20059}, 
            country={United States}}
            

\begin{abstract}
The generative capabilities of LLM models offer opportunities for accelerating tasks but raise concerns about the authenticity of the knowledge they produce. To address these concerns, we present a computational approach that evaluates the factual accuracy of biomedical knowledge generated by an LLM. Our approach consists of two processes: generating disease-centric associations and verifying these associations using the semantic framework of biomedical ontologies. Using ChatGPT, as the selected LLM, we designed prompt-engineering processes to establish linkages between diseases and related drugs, symptoms, and genes, and assessed consistency across multiple ChatGPT models (e.g., GPT-turbo, GPT-4, etc.) Experimental results demonstrate high accuracy in identifying disease terms (88\%--97\%), drug names (90\%--91\%), and genetic information (88\%--98\%). However, symptom term identification was notably lower (49\%--61\%), due to the informal and verbose nature of symptom descriptions, which hindered effective semantic matching with the formal language of specialized ontologies. Verification of associations reveals literature coverage rates of 89\%--91\% for disease-drug and disease-gene pairs, while symptom-related associations exhibit lower coverage (49\%--62\%). Despite the high accuracy in identifying term names, our observations revealed that the generated term IDs were invalid and, in some cases, redundant. This issue may impede knowledge integration across multiple ontologies, as term IDs are essential for establishing cross-references. Overall, these findings suggest that GenAI tools (e.g., ChatGPT) can generate more reliable biomedical knowledge if extra care is taken. Employing strategies such as Retrieval Augmented Generation (RAG) and incorporating knowledge from literature or ontology terms, as supervised in-context prompting, may further enhance the accuracy and reliability of GenAI-generated biomedical knowledge. Further work is needed to investigate these directions.
\end{abstract}

\begin{highlights}
\item Evaluating the capabilities of LLMs, exemplified by ChatGPT, to generate disease-centric biomedical associations.
\item Validating the correctness of biomedical named entity terms (including diseases, drugs, symptoms, and genetics) using specialized biomedical ontologies.
\item Assessing the reliability of structured associations by cross-referencing them with biomedical literature databases (e.g., PubMed), ensuring alignment with established scientific evidence.
\item Examining the consistency of LLMs in reproducing their generated associations within simulated biomedical abstracts across various ChatGPT models.
\item Demonstrating how improved validation strategies can enhance biomedical knowledge generation and knowledge integration, potentially informing better the biomedical research.
\end{highlights}


\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Knowledge generation\sep  
knowledge verification\sep 
bioMedical ontology \sep
literature mining \sep
disease-centric associations%\sep
\end{keyword}
\end{frontmatter}

\newpage
\section*{Graphical Abstract}
\begin{figure}[htbp]
   \centering
   \includegraphics[scale=.12]{iScience-Fact-Checking-GA.png}
   \caption{Graphical Abstract of the Knowledge Generation and Verification Tasks Performed.}
    % \label{fig:graphical_abstract}
\end{figure}

% {
% \centering
% \includegraphics[scale=.12]{iScience-Fact-Checking-GA.png}\\    
% Knowledge generation and verification tasks performed.
% }

\newpage

\section{Background and Significance} 
Large language models (LLMs) possess impressive generative capabilities, presenting opportunities to accelerate various tasks while also raising concerns about the reliability of the knowledge they generate. Concerned with the growing threat to scientific authenticity since the emergence of ChatGPT \cite{chatgpt2025}, our previous perspective laid out a research agenda focused on safeguarding authenticity \cite{hamed2024safeguarding}. This agenda addressed two key issues: combating fake science and verifying the factuality of AI-generated content. The first issue, fighting fake science, has become increasingly urgent as ChatGPT's generative capabilities can produce convincing but fabricated scientific articles. In our earlier work, we introduced the xFakeSci algorithm, a machine learning approach that distinguished fake/AI-generated articles from real publications with high precision \cite{hamed2024detection}. This development underscores the need for robust mechanisms to detect and filter out misleading content. The second issue, which is the focus of this paper, is the verification of the factuality of content generated by ChatGPT. Here, we extend our previous efforts by developing algorithmic approaches to assess and verify biomedical associations. Our experiments leverage relevant biomedical ontologies and PubMed abstracts to systematically evaluate the factual accuracy of the generated content.

\section{Introduction}
In an era marked by the rapid adoption of generative AI tools, fact-checking and knowledge verification are essential safeguards against hallucination and misinformation. This review traces the evolution of fact-checking, from its pre-ChatGPT foundations to its current role in addressing challenges posed by advanced generative models, especially in biomedical applications.

\subsection{Literature review}
Prior to the emergence of ChatGPT, research efforts focused on examining misinformation and developing fact-checking strategies. Several studies investigated the authenticity of information and explored fact-checking methods to mitigate misinformation risks~\cite{ciampaglia2015computational,luengo2020performance,nyhan2020taking,rodriguez2021debunking}. Surveys provided a comprehensive overview of automated fact-checking models and databases~\cite{zeng2021automated,guo2022survey}, while others employed Natural Language Processing techniques to verify news articles and social media content~\cite{lazarski2021using,oshikawa2018survey}. Additionally, machine learning approaches were applied to combat fake news, fake science, and fake social media posts~\cite{anusree2022factorfake,khalil2021detecting,zhou2019physiological,vo2019learning}. The urgency of these efforts was further underscored during the global pandemic, as misinformation raised significant health and public safety concerns~\cite{krause2020fact,luengo2020performance,abdeen2021fighting,siwakoti2021covid}.

The release of ChatGPT, alongside other generative AI and large language models, expanded research opportunities while intensifying concerns about misinformation. On one hand, these models have unlocked new scientific possibilities~\cite{koohi2023generative,degrave2023dissection,trabassi2024optimizing,wang2023applications}; on the other, they have raised issues regarding hallucinations and the lack of citations, which challenge scientific authenticity~\cite{tian2024opportunities,van2024chatgpt,barreto2023generative,giannakos2024promise}. In response, new fact-checking approaches emerged. For instance, one study addressed the verification of simulated medical abstracts by examining disease and gene names~\cite{Hamed_FC2004}, while another explored fact-checking solutions to mitigate risks associated with factuality in large language models~\cite{augenstein2024factuality}. Additional systems, such as LLM-Augmenter, have been designed to cross-verify content against external resources, and deep-learning classifiers have been used to check AI-generated radiology reports~\cite{peng2023check,mahmood2023fact}. Moreover, SelfCheckGPT has been developed to assess factuality on a sentence-by-sentence basis by ranking text chunks~\cite{manakul2023selfcheckgpt}.

Biomedical research has benefited from early models like BioBART, which assisted with Named Entity Recognition (NER), Entity Linking, and Question Answering tasks at a limited scale~\cite{yuan2022BioBART}. Following ChatGPT’s debut, studies began exploring its utility in biomedical question-answering~\cite{jin2023retrieve,hou2023answers}. Concurrently, researchers have investigated the use of large language models to generate knowledge directly or via Retrieval-Augmented Generation (RAG) methods. RAG integrates contextual prompts to enhance the freshness and accuracy of the generated information~\cite{huly2024old,jeong2023generative,arslan2024business,ng2024rag}. For example, one study employed ChatGPT as a decision support system for self-screening by embedding screening guidelines into hypothetical cases~\cite{khan10803468MedAI}. Another used RAG-based prompt engineering to extract structured representations of drug combinations from clinical trials~\cite{Hamed10803434MedAI}. Similar approaches have also improved PubMed’s retrieval capabilities~\cite{thomo2024pubmed}.

Beyond biomedical applications, large language models have been evaluated for their fact-checking abilities in news and multilingual settings. Comparative studies of ChatGPT, Bing AI CoPilot, and Gemini (formerly Bard) have highlighted both their potential and the continuing need for human oversight in news verification~\cite{chatgpt2025,copilot2025,gemini2025,caramancion2023news}. In the multilingual arena, research employing techniques such as zero-shot, chain-of-thought, and cross-lingual prompting has shown that languages with fewer resources may sometimes yield more accurate fact-checking results~\cite{singhal2024multilingual}. Additionally, studies have underscored the importance of developing guidelines for using AI in fact-checking news headlines~\cite{DeVerna_pnas_2322823121}.

\subsection{Goal and objectives}
The goal of our work is to test the generative capabilities of ChatGPT to generate disease-centric biomedical terms and associations and perform various verification process to assess the factuality of such associations. There are three objectives under this goal.

\noindent\textbf{Objective 1:} To perform semantic term verification using relevant biomedical ontologies for disease, drugs, symptoms, and genes. 

\noindent\textbf{Objective 2:} To perform automated verification of association using against the biomedical literature, specifically, PubMed abstracts. 

\noindent\textbf{Objective 3:} Assess ChatGPT consistency in generating knowledge using independent processes and various ChatGPT models: gpt-turbo, gpt-4o, gpt-4, and gpt-4o-mini.

\section{Experiment Tasks}
\textbf{Task 1 (Term correctness verification):} This task is to verify the biomedical terms that make up a generated association, where we use biomedical ontology, such as GO, DOID, ChEBI, and Symptoms ontology \cite{symp_ontology,schriml2022,schriml2012,chebi2006,chebi2016,chebi2008,geneontology2000}, as the ground truth to verify the term's identity. When a term is not found in an ontology, it is said to be ``unverified.'' (``Unverified'' does not mean ``invalid'' as the ontology may not be complete for some terms.) The second task is to verify the associations using the PubMed database as an authentic source of biomedical knowledge. 

\textbf{Task 2 (Association reliability verification):} This task is to verify associations between terms (i.e., named entities denoted by the terms), represented as a binary relation. Mathematically the association is expressed as \( R_{AB} \subseteq A \times B \), where \( A \) is a set of terms belonging to one category, such as diseases, and \( B \) is a set of terms from another category, such as genes, drugs, and symptoms. For instance, the association between diseases and genes can be expressed as \( R_{DG} \subseteq D \times G \), where \( D \) is the set of disease terms and \( G \) is the set of gene terms. A pair \( (d, g) \in R_{DG} \) signifies that a particular gene, \( g \), is associated with a specific disease, \( d \). This formal representation provides a structured way to model associations between different categories of terms.

\textbf{Task 3 (Association consistency verification): }
%{\color{red}{\textbf{Ahmed, do we have two tasks or three tasks in our work? If three tasks (I think so), can we also discuss the third task in this section, Section \ref{sec:methods}, and Section \ref{sec:discussion}? The third task seems to have ChatGPT generate simulated articles (i.e., PubMed abstracts) and test the ``consistency'' of the generated articles across different LLM models. This task is mentioned in the Highlights and the Results section (Section \ref{sec:results}).}}}
This task is to verify the consistency of knowledge (centered on associations between biomedical terms) among different ChatGPT models. It generates simulated abstracts from the different models and compares the associations detected from the simulated abstracts with the original associations generated in Task 2.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experimental Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                     Experimental Methods                                      %
% -----------------------------------------------------------------------------------------------

\section{Experiment Methods}\label{sec:methods}

\subsection{Data generation via prompt engineering}
To test the knowledge generation capabilities of ChatGPT, we used means of prompt engineering via the APIs. The purpose was to instruct ChatGPT to generate various types of disease-centric term associations to enable the verification process. These term associations are the basic building blocks of more complex forms of knowledge represented in the knowledge networks. Generating and verifying various types of associations makes the task of knowledge verification easy and efficient by decomposing the verification tasks to fine-grained unit of term associations, thereby reducing the effort to build a large and complex knowledge graph. Specifically, we instructed ChatGPT to generate 5000 associations between disease on one side, and gene, symptom, and drug, respectively, on the other side. Recall that the main idea for verification is to verify (1) whether the terms of the associations are verifiable from the corresponding ontology and (2) whether the actual association instances are rooted in the literature. To this end, the prompt included generating pairs of verifiable ontology-terms with their IDs. 

For the purpose of smooth processing, the prompt also instructed ChatGPT to format the output in JSON format, which was then validated and saved to a file. Figure\,\ref{fig:association-samples} shows samples containing a few records from each of the three types of associations generated. Specifically, Figure\,\ref{fig:disease-drug} shows three diseases (breast cancer, asthma, and hypercholesterolemia) and the associated three drugs (Carbamazepine, Zidovudine, and Fluoxetine); Figure\,\ref{fig:disease-gene} and Figure\,\ref{fig:disease-symptom} show three genes (ACE, INS, and IL4) and three disease symptoms (headache, frequent urination, and shortness of breath), respectively.
\begin{figure}[H]
    \centering
    % Subfigure1: Disease-Drug Associations
    \begin{subfigure}[b]{0.65\textwidth}
        \centering
        \begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Disease-Drug Associations]
        \begin{itemize}
            \item \textbf{Disease:} \texttt{DOID:1612 (Breast Cancer)}\\
                  \textbf{Drug:} \texttt{ChEBI:33897 (Carbamazepine)}
            \item \textbf{Disease:} \texttt{DOID:2841 (Asthma)}\\
                  \textbf{Drug:} \texttt{ChEBI:10177 (Zidovudine)}
            \item \textbf{Disease:} \texttt{DOID:0050425 (Hypercholesterolemia)}\\
                  \textbf{Drug:} \texttt{ChEBI:5118 (Fluoxetine)}
        \end{itemize}
        \end{tcolorbox}
        \caption{Disease-drug associations.}
        \label{fig:disease-drug}
    \end{subfigure}
    \smallskip\\
    % Subfigure2: Disease-Gene Associations
    \begin{subfigure}[b]{0.65\textwidth}
        \centering
        \begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=Disease-Gene Associations]
        \begin{itemize}
            \item \textbf{Disease:} \texttt{DOID:9351 (Hypertension)}\\
                  \textbf{GO term:} \texttt{GO:0002027 (Blood Pressure Regulation)}\\
                  \textbf{Gene/Protein:} \texttt{ACE (Angiotensin-Converting Enzyme)}
            \item \textbf{Disease:} \texttt{DOID:11723 (Diabetes Mellitus Type 2)}\\
                  \textbf{GO term:} \texttt{GO:0006006 (Glucose Metabolic Process)}\\
                  \textbf{Gene/Protein:} \texttt{INS (Insulin)}
            \item \textbf{Disease:} \texttt{DOID:2841 (Asthma)}\\
                  \textbf{GO Term:} \texttt{GO:0050707 (Regulation of Cytokine Secretion)}\\
                  \textbf{Gene/Protein:} \texttt{IL4 (Interleukin-4)}
        \end{itemize}
        \end{tcolorbox}
        \caption{Disease-gene associations.}
        \label{fig:disease-gene}
    \end{subfigure}
    \smallskip\\
    % Subfigure3: Disease-Symptom Associations
    \begin{subfigure}[b]{0.65\textwidth}
        \centering
        \begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title=Disease-Symptom Associations]
        \begin{itemize}
            \item \textbf{Disease:} \texttt{DOID:9352 (Essential Hypertension)}\\
                  \textbf{Symptom:} \texttt{SYMPTOM:1001 (Headache)}
            \item \textbf{Disease:} \texttt{DOID:11723 (Diabetes Mellitus)}\\
                  \textbf{Symptom:} \texttt{SYMPTOM:1002 (Frequent Urination)}
            \item \textbf{Disease:} \texttt{DOID:2841 (Asthma)}\\
                  \textbf{Symptom:} \texttt{SYMPTOM:1003 (Shortness of Breath)}
        \end{itemize}
        \end{tcolorbox}
        \caption{Disease-symptom associations.}
        \label{fig:disease-symptom}
    \end{subfigure}

    \caption{Overview of disease associations with drugs, genes, and symptoms.}
    \label{fig:association-samples}
\end{figure}

Algorithm\,\ref{alg:prompt_eng_assoc} shows a one-shot prompt engineering for the task of generating disease-symptom associations. 

\begin{algorithm}
\caption{One-shot prompt engineering for generating disease-symptom associations record in JSON format.}\label{alg:prompt_eng_assoc}
\begin{small}
\begin{algorithmic}[1]

\REQUIRE 
    \texttt{Model M, shot s\_1, Number of associations N} \\
    \texttt{s\_1} $\gets$ \{\\
    \hspace*{1.5em} \texttt{"DOID:11734"}: \texttt{"Epistaxis"},\\
    \hspace*{1.5em} \texttt{"SYMPTOM:1080"}: \texttt{"Nosebleed"}\}
\ENSURE
    \texttt{Valid and structured JSON output R containing N DOID-SYMPTOM associations.}

\STATE \textbf{\textsf{DEFINE}} \texttt{prompt P:} \\
\STATE \hspace{1em} \texttt{P $\gets$ ``You are an assistant that generates 10 DOID-SYMPTOM term associations in a structured JSON format. Ensure the JSON is valid and correctly formatted for parsing. Provide one example in the following format:''} \\
\STATE \hspace{2em} \texttt{P $\gets$ P + s\_1}

\STATE \textbf{\textsf{INITIALIZE}} \texttt{response request to model M:}
\STATE \hspace{1em} \texttt{R $\gets$ M.generateResponse(P, model = "gpt-4o")}

\STATE \textbf{\textsf{PROCESS}} \texttt{response R:}
\IF{\texttt{R} is a valid JSON format}
    \STATE \textbf{Output} \texttt{R}
\ELSE
    \STATE \textbf{Report error}: \texttt{``Invalid JSON format in response.''}
\ENDIF
\RETURN \texttt{R}
\end{algorithmic}
\end{small}
\end{algorithm}

% \begin{multicols}{2}
% % First Box
% \begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Disease-Drug Associations]
% \label{box:disease-drug}
% \begin{itemize}
%     \item \textbf{DOID:1612} Breast Cancer\\
%           \textbf{ChEBI:33897} Carbamazepine
%     \item \textbf{DOID:2841} Asthma\\
%           \textbf{ChEBI:10177} Zidovudine
%     \item \textbf{DOID:0050425} Hypercholesterolemia\\
%           \textbf{ChEBI:5118} Fluoxetine
%     \item \textbf{DOID:9256} Alzheimer's\\
%           \textbf{ChEBI:39548} Atorvastatin)          

% \end{itemize}
% \end{tcolorbox}

% % Third Box
% \begin{tcolorbox}[colback=orange!5!white, colframe=orange!75!black, title=Disease-Symptom Associations]
% \label{box:disease-symp}
% \begin{itemize}
%     \item \textbf{DOID:9352:} Essential Hypertension\\
%           \textbf{SYMPTOM:1001:} Headache
%     \item \textbf{DOID:11723:} Diabetes Mellitus\\
%           \textbf{SYMPTOM:1002:} Frequent Urination
%     \item \textbf{DOID:2841:} Asthma\\
%           \textbf{SYMPTOM:1003:} Shortness of Breath
% \end{itemize}
% \end{tcolorbox}

% \end{multicols}

\begin{table}[htbp]
\centering
\caption{Datasets generated and used in the experiments. It shows the total number of associations for each of three ontology pairings DOID-SYMP, DOID-GO, and DOID-CHEBI. These datasets were validated against trusted ontologies to ensure precision and accuracy. They are designed to support various bioinformatics applications, including disease annotation, genetic process mapping, and drug-disease linkage studies.}
\label{tab:gpt_generated_associatons}
\begin{small}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Dataset} & \textbf{Total \#} \\ \midrule
ChatGPT DOID-SYMP JSON Pairs   & 5466               \\
ChatGPT DOID-GO JSON Pairs     & 5008               \\
ChatGPT DOID-CHEBI JSON Pairs  & 2625               \\ \bottomrule
\end{tabular}
\end{small}
\end{table}




\subsection{Term correctness verification against biomedical ontologies}
In this work, we address four types of terms that make up the three types of associations that can be represented formally as binary relations.

\begin{itemize}
\item[1.] \textbf{Disease-drug association:} This type of association is represented as \( R_{DD} \subseteq D \times Dr \), where \( Dr \) denotes the set of drugs. It signifies the relationship between diseases and the drugs used for their treatment or management.

\item[2. ] \textbf{Disease-symptom association:} This association is denoted as \( R_{DS} \subseteq D \times S \), where \( S \) represents the set of symptoms. It captures the relationship between diseases and their corresponding symptoms.

\item[3.] \textbf{Gene-process association:} This is expressed as \( R_{GP} \subseteq G \times P \), where \( P \) represents the set of genetic processes. It reflects the relationship between specific genes and the biological processes they are involved in.
\end{itemize}

The terms generated by the model are verified against a specialized ontology. For the diseases, the terms are verified against the the Human Disease Ontology (DOID), the drugs are verified using the Chemical Entities of Biological Interest (ChEBI) ontology, the genetic knowledge is  verified using the Gene Ontology (GO), whereas the symptoms are verified using the The Symptom Ontology (SYMP). Since the various ontologies maintain term names and synonyms, the term verification is semantic in nature. Table\,\ref{tab:gpt_generated_associatons} describes three datasets of associations among disease-symptom, disease-gene, and disease-drug, respectively. Figure\,\ref{fig:doid-synonym} shows a proof of a synonym metadata item that describe a DOID:10763 ontology term \cite{disease_ontology_doid10763} known as hypertension and various synonyms namely: (HTN, Hyperpiesia, Hypertensive disease, Vascular hypertensive disorder).
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{doid-hypertension-synonym.png}
    \caption{The term ``Hypertension'', identified as DOID:10763 in the DOID ontology, includes a metadata item for synonyms. %A term-matching
    The term verification algorithm (Algorithm\,\ref{alg:term_verification}) utilizes this synonyms field to semantically verify the legitimacy of terms generated by ChatGPT.}
    \label{fig:doid-synonym}
\end{figure}


Algorithm\,\ref{alg:term_verification} shows the steps for term verification from the domain ontology. Each ontology term may have a list of synonyms whenever applicable. The algorithm needs to search both the list of terms and the list of synonyms in the ontology to ensure sound and fair verification.

\begin{algorithm}[htbp]
\caption{Term verification Using domain-specific ontology.}
\label{alg:term_verification}
\begin{small}
\begin{algorithmic}[1]
\REQUIRE Term instance \( t \), Term type \( \tau \), Domain ontology \( \mathcal{O} \)
\ENSURE \texttt{VERIFIED} or \texttt{UNVERIFIED}
\STATE Let \( L_t \) be the list of terms in the ontology \( \mathcal{O} \)
\FORALL{terms \( t_i \in L_t \)}
    \IF{\( t = t_i \)}
        \RETURN \texttt{VERIFIED}
    \ELSE
        \STATE Let \( L_s \) be the list of synonyms for \( t \)
        \FORALL{synonyms \( s_j \in L_s \)}
            \IF{\( t = s_j \)}
                \RETURN \texttt{VERIFIED}
            \ENDIF
        \ENDFOR
    \ENDIF
    \RETURN \texttt{UNVERIFIED}
\ENDFOR
\end{algorithmic}
\end{small}
\end{algorithm}

% ---------------------------------------------------------------------------------------------------

\subsection{Association reliability verification against biomedical literature}

Since the specialized ontologies are term-based resources, they can only support the verification of the terms that make up the associations. Therefore, verification of associations goes beyond the ontology. It is a common practice for scientists to verify biomedical knowledge from the scientific literature. This requires searching digital repositories such as PubMed \cite{pubmed} and manually verifying whether an evidence holds. The rational behind using PubMed for verification is that PubMed has been recently improved to respond to certain information needs, specifically related to evidence-based medicine and association discovery \cite{jin2024pubmed}.

The manual process, though may produce highly accurate results, is labor-intensive and slow in nature. With the large amount of content generated by ChatGPT and other GenAI tools, this process is infeasible. %obsolete. 
Here, we present an algorithmic approach that offers an automatic means to verify the generated associations. The algorithm is designed to search contextual dataset of biomedical abstracts for a certain association type. To gather this dataset, we performed a contextual search for the three associations: (1) disease and drug, (2) disease and gene, and (3) disease and symptom. Each dataset was used as the basis to verify the corresponding association. %That is, the disease-drug dataset was used to verify the disease-drug associations. 

The steps of association verification are outlined in Algorithm\,\ref{alg:association_verification}. The notion of association verification here can be defined as a co-occurrence of the association terms in one of more PubMed abstracts. The number of PubMed abstracts that contain an association is called  ``coverage''. The number of associations supported by coverage offers a score for a given type of association using the underlying dataset. 

\begin{algorithm}[H]
\caption{Association verification using literature.}
\label{alg:association_verification}
\begin{small}
\begin{algorithmic}[1]
\REQUIRE List of association term pairs \( L_p \); Dataset of literature \( D \)
\ENSURE List of verified associations and their hit ratios, \( L_v \)
\STATE Initialize \( L_v \leftarrow \emptyset \)
\FORALL{association term pairs \( P \equiv (p_i, p_j) \in L_p \)}
    \STATE Initialize \( \text{counter} \leftarrow 0 \)
    \FORALL{abstracts \( B \in D \)}
        \IF{\( p_i \in B \) \textbf{and} \( p_j \in B \)}
            \STATE \( \text{counter} \leftarrow \text{counter} + 1 \)
        \ENDIF
    \ENDFOR
    \STATE compute hit\_ratio \( h_P \leftarrow \frac{\text{counter}}{\lvert L_P \rvert} \)
    \IF {\( h_P > 0 \)}
        \STATE append \( (P, h_P) \) to \( L_v \)
    \ENDIF
\ENDFOR
\RETURN \( L_v \)
\end{algorithmic}
\end{small}
\end{algorithm}

% \subsection{Association consistency verification in generated biomedical abstracts across different models}



\section{Experiment Results}\label{sec:results}
We present the results of the experiments in evaluating ChatGPT's capabilities in the following key tasks: 
\begin{itemize}
    \item Verifying the correctness of the biomedical terms that make up the associations (i.e., disease, symptom, drug, and genes);
    \item Verifying the associations linkage against biomedical literature from different periods;
    \item Testing the randomness of ChatGPT by generating simulated articles using various ChatGPT models.
\end{itemize}

\subsection{Task 1 -- Verification of the correctness of biomedical terms}
We evaluated the names of the three types of associations generated disease-drug, disease-symptom, and disease-gene/genetic process that made up the ChatGPT-generated associations using domain-specific ontologies as ground truth. The verification of the terms that make up the generated associations was checking against the DOID ontology for disease terms, the ChEBI ontology for the drug terms, the SYMPTOM ontology for the symptom terms,  and the GO ontology for the genetic terms (gene names and genetic processes). The encoding of those ontology offer means of literal and semantic matching, which offers fair means of comparisons. For instance, the (``hypertension'') disease term in the DOID ontology  (``DOID:10763'') \cite{disease_ontology_doid10763}. Additionally, the ontology entity of this term also includes the list of synonyms (``HTN [EXACT], hyperpiesia [EXACT], hypertensive disease [RELATED], vascular hypertensive disorder [EXACT]''), which are also checked during the algorithmic process. Hence, the claim of a semantic verification process. 

\subsection{Tasks 1.1 -- Verification of disease terms}
The task of of generating disease terms was common across three types of associations. The verification result of the terms in the three types of associations are as follows.
%\begin{enumerate}
\begin{itemize}
    \item%[1.]
    For disease-drug associations, the literal matching process verified 93\% of disease terms, while the semantic matching verified 87\% of the generated names. Combined, 98\% of disease names were successfully verified.  
    \item%[2.]
    For disease-symptom associations, the literal matching verified 97\% of the disease terms, while the semantic matching verified 82\% of the generated terms. Combined, 99\% of disease names were successfully verified.  
    \item%[3.] 
    For disease-gene associations, the literal matching verified 88\% of the disease terms, while semantic matching verified 97\% of the generated terms. Due to the high percentage of verification, the task of a combined matching was omitted. %from literal and semantic was ignored.
\end{itemize}
%\end{enumerate}

\subsubsection{Tasks 1.2 -- Verification of non-disease terms}
Here we summarize the results of ChatGPT generating correct drugs, symptoms, genes, and genetic processes as part of the associations:
\begin{itemize}
    \item%[1.]
    \textbf{Drug names:} the literal matching verified 90\% of drug names, with an 90\% verified through synonym matching. The combined verification rate was 91\%.  
    \item%[2.]
    \textbf{Symptom names:} literal matching verified 49\% of symptom names, with an additional 25\% verified through semantic matching. The combined verification rate was 61\%.  
    \item%[3.]
    \textbf{Genetic processes and gene names:} the verification resulted in the verification 80\% of the gene names and 97\% of the genetic processes.
\end{itemize}
%\textbf{**I would keep a consistent writing style for all cases instead of using different words (for the same meaning) for different cases.**}

These findings demonstrate the strong capability of ChatGPT in generating biomedical terms that align closely with biomedical ontology as one of the most authentic sources of ground truth. Even in the case of the symptom terms, where performance was notably lower, this does %mean
pose a significant concern from the point of view of our study. In Section \ref{sec:task2}, the results will show that, although not identified in the specialized ontology, there was a noticeable improvement when searching the literature for association links which also included symptom terms. Table\,\ref{tab:task1_stats} captures the statistics that summarize this task, and the results are shown in Figure\,\ref{fig:doid_associations_task1}.

\begin{table}[htbp]
\centering
\caption{Verification of entity names and types of ChatGPT-generated associations of disease-symptoms-drug-gene using biomedical ontologies (DOID for diseases, SYMP for disease symptom names, ChEBI for drug names, and GO for genetic processes and gene names). The disease-centric links generated were DOID--ChEBI, DOID--SYMP, and DOID-GO.}
\label{tab:task1_stats}
\renewcommand{\arraystretch}{1.2}
\begin{small}
\begin{tabular}{l|l|c}
\toprule
\textbf{Category}                  & \textbf{Feature Verified}             & \textbf{Accuracy (\%)} \\ \hline
\multirow{3}{*}{DOID-ChEBI associations} 
                                   & Diseases name               & 93.37          \\ 
                                   & Disease synonym             & 86.70          \\ 
                                   & Disease name/synonym        & 97.60          \\ 
                                   & Drug name                   & 89.52          \\ 
                                   & Drug synonym                & 89.98          \\ 
                                   & Drug name/synonym           & 91.43          \\ \midrule
\multirow{6}{*}{DOID-SYMP associations} 
                                   & Diseases name               & 96.83          \\ 
                                   & Disease synonym             & 81.87          \\ 
                                   & Disease name/synonym        & 98.87          \\ 
                                   & Symptom name                & 49.29          \\ 
                                   & Symptom synonym             & 24.50          \\ 
                                   & Symptom name/synonym        & 61.14          \\ \midrule
\multirow{5}{*}{DOID-GO associations} 
                                   & Disease name                & 88.12          \\ 
                                   & Disease synonym             & 97.36          \\ 
                                   & Genes/proteins              & 80.21          \\ 
                                   & Genetic processes           & 96.47          \\ \bottomrule
                                   % & Same Entity Processes-Gene  & 98.74          \\ \bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{figure}[H]
    % \centering
    \includegraphics[scale=.65]{task1_doid_associations_vertical_plots.png}
    \caption{Accuracy of DOID-ChEBI, DOID-SYMP, and DOID-GO associations across various features.}
    \label{fig:doid_associations_task1}
\end{figure}


%%%%%%----------------------------------------------------------------------------------------%%%%%%%%

\subsection{Task 2: Verification of the reliability of biomedical association links}\label{sec:task2}
While domain-specific ontologies offer verification of individual entities of the associations, they do not offer the means of verifying the actual relationships between the individual terms. To address this limitation, we utilized the biomedical literature as another reputablesource of ground truth to verify the various associations. Thus, we constructed three datasets, each to verify one of the three types of associations, that is, disease-drug, disease-genetic information, and disease-symptom. Because the GO ontology does not reference the direction of the gene terms in the ontology, %but
in another resource known as the GO ontology annotation \cite{camon2004gene,yon2008use,gene2012gene}, we prompted ChatGPT to produce associations that link a disease to two pieces of genetic  knowledge --- the GO term, which is the genetic process itself, and the gene name, which is the product of the actual process. This requires the validation of mapping the biological process to its product, which is also included in the results.  

In addition, to examine the effect of publication longevity, each data set was divided into three 5-year periods spanning 2009 to 2024. The verification processes consider the literature coverage as means of being viable. The premise is that if an association occurs frequently in a set of PubMed abstracts then this makes it ``verified''. The research stops at this step and does not further explore the semantics of such associations. As mentioned earlier, if an association was not covered in the literature then this means it is yet to be verified. In other words, it does not label an association as correct or incorrect. The experiments revealed a consistent trend of improved verification coverage across all types of association over the three periods. Specifically:
\begin{enumerate}
    \item \textbf{Disease-drug association} is verified with coverage rates of 86\%, 88\%, and 90\%, respectively, over the three periods.
    \item \textbf{Disease-gene association} is verified with coverage rates of 83\%, 83\%, and 89\%, respectively, over the three periods.
    \item \textbf{Disease-symptom association} is verified with coverage rates of 49\%, 53\%, and 62\%, respectively, over the three periods.
    \item \textbf{Genetic process-gene association} is verified with the coverage improved from 23\% in the first period to 83\% and 89\% in the second and third period, respectively, over the three periods.
\end{enumerate}

Additionally, we further analyzed the frequency of publications supporting each type of association to measure the level of support from the literature over time. The results summarized in Table\,\ref{tab:task2_stats} indicate an increasing trend of publication support over the three periods of time (2009-2014, 2015-2019, and 2020-2024):
\begin{itemize}
    \item \textbf{Disease-gene association} is supported by an average of 90, 82, and 214 publications, respectively, over the three periods.
    \item \textbf{Disease-drug association} is supported by an average of 31, 56, and 107 publications, respectively, over the three periods.
    \item \textbf{Disease-symptom association} exhibits the lowest support, with an average of 9, 14, and 31 publications, respectively, over the three periods. However, it still follows an upward trend.
\end{itemize}

\begin{table}[htbp]
    \centering
    \caption{Average co-occurrences across time periods for disease associations.}\label{tab:task2_stats}
    \begin{small}
    \begin{tabular}{@{}llc@{}}
        \toprule
        \textbf{Association Type}      & \textbf{Time Period} & \textbf{Average Frequency} \\ 
        \midrule
        Disease -- gene           & 2009--2014           & 90.48                          \\
                                       & 2015--2019           & 82.54                          \\
                                       & 2020--2024           & 214.38                         \\
        \midrule
        Disease -- drug           & 2009--2014           & 30.71                          \\
                                       & 2015--2019           & 56.37                          \\
                                       & 2020--2024           & 107.34                         \\
        \midrule
        Disease -- symptom        & 2009--2014           & 9.17                           \\
                                       & 2015--2019           & 13.96                          \\
                                       & 2020--2024           & 31.40                          \\
        \bottomrule
    \end{tabular}
    \end{small}
\end{table}

The results from the two tasks demonstrate the high accuracy of the biomedical terms that make up the ChatGPT-generated associations, and also show an increasing coverage trend over time as captured in Table~\ref{tab:task2.1_stats}. The trend suggests that recent publications contribute more robustly to the verification of biomedical associations as shown in Figure\,\ref{fig:task2_task3_combined}. On the one hand, the ChatGPT associations depict an evolutionary picture of the knowledge accumulated in biomedical literature over time. On the other hand, the verification process may also reflect on the type of knowledge ChatGPT may contribute and an indication of how the pre-training processes of ChatGPT has taken place. 

\begin{table}[htbp]
    \centering
    \caption{Literature co-occurrence statistics for disease-drug, disease-symptom, disease-gene name, and gene process associations.}\label{tab:task2.1_stats}
    \begin{small}
    \begin{tabular}{@{}llcc@{}}
        \toprule
        \textbf{Association Type}      & \textbf{Time Period} & \textbf{Unverified Links (\%)} & \textbf{Verified Links (\%)} \\ 
        \midrule
        Disease -- drug                 & 2009--2014           & 14.29                         & 85.71                        \\
                                       & 2015--2019           & 11.53                         & 88.47                        \\
                                       & 2020--2024           & 9.52                          & 90.48                        \\
        \midrule
        Disease -- symptom                 & 2009--2014           & 51.02                         & 48.98                        \\
                                       & 2015--2019           & 46.83                         & 53.17                        \\
                                       & 2020--2024           & 38.08                         & 61.92                        \\
        \midrule
        Disease -- gene           & 2009--2014           & 16.74                         & 83.26                        \\
                                       & 2015--2019           & 16.74                         & 83.26                        \\
                                       & 2020--2024           & 10.85                         & 89.15                        \\
        \midrule
        Gene process -- gene term       & 2009--2014           & 76.84                         & 23.16                        \\
                                       & 2015--2019           & 16.74                         & 83.26                        \\
                                       & 2020--2024           & 10.85                         & 89.15                        \\
        \bottomrule
    \end{tabular}
    \end{small}
\end{table}

\begin{figure}[H]
    \centering
    % \includegraphics[width=\textwidth]{combined_trends_and_verified_links_subplots.png}
    \includegraphics[scale=.45]{combined_trends_and_verified_links_subplots.png}    
    \caption{Combined visualization of average co-occurrences (line chart) and literature co-occurrence statistics (bar charts). The top plot shows trends in co-occurrences over time, while the bottom plot compares verified and unverified links for different association types.}
    \label{fig:task2_task3_combined}
\end{figure}

%%%%%%------------------------------------------------------------------------------------%%%%%%%%

\subsection{Task 3: Verification of the association consistency against ChatGPT-simulated Abstracts by various models}
%To assess the degree of randomness of ChatGPT generating the association studied in this work,
To assess how consistent (or random) the ChatGPT-generated associations are, we generated disease-centric simulated abstracts using various ChatGPT models. Specifically, we prompted four ChatGPT models --- ChatGPT-4, ChatGPT-4turbo, ChatGPT-4o, and ChatGPT-4omini --- to generate simulated abstracts centered on human diseases. Due to the computational cost of this task, we limited the generation to approximately 5,000 abstracts per model. Figure~\ref{fig:task3-model-comparison} shows number of hits per model for three types os associations in three layers: the top layer in blue is for disease-drug, the middle layer in red shows the drug-genes, while the bottom layer in green is for the disease-symptom.   The verification process is summarized in Table\,\ref{tab:task3_model_comparison}. 
\begin{itemize}
    \item \textbf{Disease-drug association} achieved coverage rate of (1\% to 15\%);
    \item \textbf{Disease-gene association} achieved coverage rate of (1\% to 4\%); 
    \item \textbf{Disease-symptom association} achieved coverage rate of (2\% to 29\%). 
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Combined statistics of disease-drug, disease-gene, and disease-symptom associations checked against the ChatGPT-generated simulated biomedical abstracts.}
\label{tab:task3_model_comparison}
\begin{small}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model}                  & \textbf{Count} & \textbf{Percentage (\%)} & \textbf{Association Type} \\ \midrule
chatgpt\_4\_model\_count        & 10             & 0.38                     & Disease-Drug              \\ 
model\_turbo\_chatgpt\_count    & 45             & 1.71                     & Disease-Drug              \\ 
chatgpt-4o-mini-model           & 201            & 7.66                     & Disease-Drug              \\ 
chatgpt-4o-model                & 375            & 14.29                    & Disease-Drug              \\ \midrule
chatgpt\_4\_model\_count        & 15             & 0.30                     & Disease-Gene              \\ 
model\_turbo\_chatgpt\_count    & 0              & 0.00                     & Disease-Gene              \\ 
chatgpt-4o-mini-model           & 186            & 3.71                     & Disease-Gene              \\ 
chatgpt-4o-model                & 74             & 1.48                     & Disease-Gene              \\ \midrule
chatgpt\_4\_model\_count        & 100            & 1.83                     & Disease-Symptom           \\ 
model\_turbo\_chatgpt\_count    & 299            & 5.47                     & Disease-Symptom           \\ 
chatgpt-4o-mini-model           & 788            & 14.42                    & Disease-Symptom           \\ 
chatgpt-4o-model                & 1560           & 28.54                    & Disease-Symptom           \\ \bottomrule
\end{tabular}
\end{small}
\end{table}

\begin{figure}[H]
    \centering
    % \includegraphics[width=\textwidth]{task3_model_comparison-jmp-best.png}
    \includegraphics[scale=.45]{task3_model_comparison-jmp-best.png}    
    \caption{A model performance comparison, as means of self-consistency,  for the associations generated from independent prompts}
    \label{fig:task3-model-comparison}
\end{figure}

While these coverage rates appear modest compared to benchmarks against much larger biomedical literature datasets (spanning 250,000 to 650,000 abstracts), the disparity is likely attributableto the smaller dataset size in this evaluation. Notably, the disease-symptom associations exhibited the highest match rates, a result that contrasts with their lower performance in comparisons against biomedical literature datasets. These findings underscore the potential of ChatGPT models to identify consistent disease-symptom links, though further investigation is required to validate the correctness of these associations in more focused contexts. Figure\,\ref{fig:sum_stat} shows the evaluation of associations across the various models and summary statistics for each type over three 5-year publication periods. 
% 
\begin{figure}[H]
    \centering
    % \includegraphics[width=\textwidth]{association-type-evolution.png}
    \includegraphics[scale=.4]{association-type-evolution.png}    
    \caption{Time and summary statistics of the association coverage for three types(disease-symptom), (disease-drug), and (disease-gene).}
    \label{fig:sum_stat}
\end{figure}

%\section{Observations and Anecdotal Evidence}
\section{Discussion} \label{sec:discussion}

In this work, we tested the capabilities of ChatGPT to generate biomedical associations as building blocks for more complex data models such as biomedical networks and knowledge graphs. Specifically, we designed a prompt-engineering algorithm that produces human disease-centric associations in the context of symptoms, drugs, genetics. The algorithm prompted for generating association terms that match the corresponding specialized ontology, namely, DOID, ChEBI, SYMPTOM, and GO ontology. The prompt also provided a shot as an example of what is to be produced for a valid association. Each association was to be between two terms, a source and a target, where each term is encoded by a term ID and a name (or an instance of its synonyms). %As discussed in the methods (Section\,\ref{sec:methods}),% and the results, 
Note the research was driven by the verification of the terms that make up the generated associations using the mentioned ontologies. The associations were verified against the biomedical literature using instances of PubMed abstracts of different periods. Here, we discuss our observations along with some anecdotal evidence in each of the verification tasks performed.

\subsection{Term correctness verification}
The most striking observation is in the outcome of low coverage or symptom term verifications using SYMP ontology. Our manual analysis revealed several critical reasons for the challenges encountered when matching symptom terms from the ontology during the verification process: (1) the language discrepancy posed significant challenges. ChatGPT predominantly generates symptoms described in social or layman's terms that resonate more with general audiences rather than the specialist terminology employed in biomedical ontologies. Unlike drug terminology, which typically has multiple recognized names (generic, brand, etc.), symptoms frequently lack such standardized variations. Examples illustrating this challenge include terms like ``itchy blisters'' versus ``blister'', "seizures" instead of "febrile convulsion," "facial redness" instead of "inflammation," and "sad mood" versus ``depression''. (2) ChatGPT tended to generate overly verbose or too detailed symptom descriptions, complicating direct matching with succinct ontology terms or their synonyms. Examples include phrases such as "rapid weight loss" versus simply "weight loss," "swelling in joints," and "swollen lymph nodes," where simpler ontology terms exist. (3) instances occurred where multiple symptoms were combined into one description, preventing direct term-to-term matching. A notable example was combining ``bulls-eye/bull's eye'' (typically classified as a lesion) with "rash," thus complicating the ontology-based verification. (4) inconsistencies in punctuation and lack of synonym availability within the ontology further hindered symptom identification, underscoring the importance of advanced semantic matching mechanisms. These issues also contributed to the verification of disease-symptom associations. 

\subsection{Association reliability and consistecny verification} 
Association verification against the biomedical literature results in a positive outcome of very high coverage for some associations. This observation offers some confidence knowing that ChatGPT has certain knowledge that may be considered the fundamentals of science, common knowledge, or frequently studied associations; examples of such associations are the disease-drug association between diabetes mellitus and insulin, which was covered by 3000+ co-occurrences; the disease-gene association between breast cancer and ataxia telangiectasia mutated (ATM), where 9000 co-occurrences pointed to it that the ATM gene may cause the breast cancer. Generating such associations may summarize the basic building blocks in the human diseases, which is the ultimate objective of this study. It may also trigger incremental generation of associations while performing verification, to construct a more comprehensive human disease landscape.

\subsection{Association consistency verification}
When performing the association consistency verification we observed that some of the frequencies is only a single occurrence in a publication abstract. These low coverage associations may be coincidental in most cases. However, after investigating various instances we found, for example, that the association between non-small cell lung cancer and lapatinib as a disease-drug type, is proven correct. Further, we also found that the same drug was approved by the FDA to repurposed for breast cancer in combination with a chemotherapy drug known as capecitabine \cite{drugbank_DB01259},  which presents a piece of drug repurposing evidence. 

Other similar observations, the association between hypertensive heart disease and losartan also had a very weak coverage in the the literature. Losartarn as a standalone drug according to Xu et al.\cite{Xu_2009} or in combination with hydrochlorothiazide according to Suzuki et al.\cite{Suzuki_2015}, plays a significant role to treat cardiovascular diseases, including hypertension and heart diseases. Both publications became available in the period from 2009 to 2015. We hypothesize that ChatGPT may have generated this knowledge from another source of evidence was thought significant. It is important to study the associations of weak coverage and further explore such evidence to understand its value. 

When we tested if the publication date as a factor in the training of ChatGPT, we found that indeed the publication date played a role in the verification of the knowledge generated by ChatGPT. Specifically, we found more coverage for associations from publications published in recent years (2020-2024) than earlier. The coverage was significantly lower if the publication was old by a decade or more. However, the ChatGPT generated associations were only found in older literature than recent as in Xu et al., and Suzuki et al. \cite{Xu_2009,Suzuki_2015}. 

The way we used different ChatGPT models to generate simulated abstracts and to measure the coverage also proved that ChatGPT's ability to generate associations was based on a certain ground, and it was not random. Although the overlap of associations in the generated abstract was low, it was a good test of trust. It is important to acknowledge that the abstracts were generated in the most generic way based on its pre-trained knowledge about human diseases, symptoms, genetics, and drugs. More importantly, the abstracts were also generated in an entirely independent process and produced a different type of output (i.e., structured associations). 

\section{Future Directions}
To address the symptom identification issues, we plan to introduce an LLM-driven solution to measure the semantic similarities among various terms. Leveraging self-consistency verification methods within a single model or comparing outputs across multiple models may further enhance the outcomes of symptom verification and subsequent disease-symptom associations. Additionally, extending this research to social media phenomena related to adverse drug reactions (ADRs) and side-effects as sources of real-world data may prove valuable.

We have tested our verification methods and algorithms on various ChatGPT models as initial stable tools. However, we plan to push the boundaries using open-source models to eliminate cost limitations. Some of the open-source LLMs we plan to use include Microsoft Phi-3, Meta Llama, QWen2.5, and TiniZero \cite{microsoft2024phi3,tinyzero,meta2024llama,qwen2.5,qwen2,tinyzero}, among others. We also plan to enhance the completely unsupervised approach for generating associations through prompt engineering. This endeavor may be achieved by generating associations from biomedical literature with the help of LLMs. Incorporating retrieval augmented generation (RAG) provides the most relevant publications to the query, and the query results may also be used for in-context prompt engineering, employing one document at a time to ensure factuality and explainability \cite{khan2024reinforcement}. We believe such approaches will increase confidence in the knowledge generated and the verification outcomes.

Another potential direction is to further investigate the interesting associations identified during our analysis. For instance, the association between (``non-small cell lung cancer'') and (``lapatinib'') could be further explored using retrieval augmented generation (RAG) and prompt engineering. In this case, we would inject verified associations, along with their corresponding full PubMed abstracts, into a series of prompts to explore the surrounding facts comprehensively. For example, we may include the verified association as part of a prompt along with relevant PubMed abstracts, instructing the LLM to generate knowledge regarding drug-target interactions, drug side effects, drug alternatives, mechanisms of action, drug-drug interactions, drug combinations, drug repurposing evidence, and other relevant associations related to non-small cell lung cancer. The verification may follow the process outlined in Algorithm \ref{alg:association_verification} and be further explored by constructing literature-driven heterogeneous mirror networks that could potentially reveal missing links once verified.

% To address the symptom identification issues, we plan to utilize LLMs to measure the semantic similarities among the various of terms. Leveraging self-consistency verification methods within a single model or comparing outputs across multiple models may offer further enhancement to the outcome of symptom verification and subsequent disease-symptom associations. Additionally, extending this research to social media phenomena related to adverse drug reactions (ADRs) and side-effects as means of real-world data may prove to be a valuable direction. 

% We have tested our verification methods and algorithms on various ChatGPT models as the first and foremost stable models. However, we plan on pushing the boundaries of open-source models to eliminate the cost limitation. Some of the open-source LLMs plan to use are (Microsoft Phi-3, Meta Llama, QWen2.5, TiniZero)\cite{microsoft2024phi3,tinyzero,meta2024llama,qwen2.5,qwen2,tinyzero} among others.  We also plan to enhance the completely unsupervised approach to generate association using prompt engineering. This can be done by generating associations from the biomedical literature instead of relaying entirely on the pretrained models. The inclusion of retrieval augmented generation (RAG) provides the most relevant publications to the query, while the query results may also be used in-context prompt engineering using one document at at time to enforce factuality and explainability \cite{khan2024reinforcement}. We believe such approaches will increase the confidence in the outcome of the verification methods. 

% Another potential direction is to further investigate the interesting associations covered during our analysis. For instance, the association between non-small cell lung cancer and lapatinib can also be further explored using retrieval augmented generation (RAG) prompt engineering.  In this case, we will attempt injecting the verified associations, along with their full PubMed abstracts that co-occur, as part of a series of prompts to learn all the surrounding facts around this association. For instance, we may inject the association as part of a prompt along with a given set of PubMed abstracts, and instruct the LLM to generate knowledge about the drug-target, drug-side effects, drug alternative, drug-mechanism of action, drug-drug interaction, drug-drug combination, drug-repurposing evidence, and any other type of associations that may be related to non-small cell lung cancer. The verification may follow the  process outlined in Algorithm\,\ref{alg:association_verification}, and be further explored by constructing heterogeneous mirror networks that can expose the missing links once verified.

\section{Declaration of generative AI and AI-assisted}
During the preparation of this work the author(s) used ChatGPT in order to generate associations and simulated articles to produce the datasets of this work. The authors also used ChatGPT to perform LaTeX formatting to enhance the presentation of the work. However, the writing was written entirely by the authors and no GenAI tool was involved in the writing process. 

\section{Acknowledgments}
The work presented in this paper has been partly supported from IBM Faculty Award by the IBM Corporation. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the IBM Corporation. The authors thank Professor Luis Rocha and the members of the CACSI Laboratory of Binghamton University for the valuable discussion.  The authors would like to thank Northeastern University Maimi MSIS students Han Shao, Srivarini Mandali, and Mengxia Qiu for valuable discussions.  

\bibliographystyle{elsarticle-num} 
\bibliography{cas-refs}

\end{document}
