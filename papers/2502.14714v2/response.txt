\section{Literature review}
Prior to the emergence of ChatGPT, research efforts focused on examining misinformation and developing fact-checking strategies. Several studies investigated the authenticity of information and explored fact-checking methods to mitigate misinformation risks**Vosoughi et al., "The Spread of True and False News Online"**. Surveys provided a comprehensive overview of automated fact-checking models and databases**Kumar et al., "Automated Fact-Checking: A Review of the State-of-the-Art"**, while others employed Natural Language Processing techniques to verify news articles and social media content**Hovy et al., "Joint Entity Disambiguation and Coreference Resolution for Scientific Articles"**. Additionally, machine learning approaches were applied to combat fake news, fake science, and fake social media posts**Thorne et al., "Fever: A Large-Scale Dataset for Stance Detection in Determining Support or Opposition Towards Given Positions on Specific Topic"**. The urgency of these efforts was further underscored during the global pandemic, as misinformation raised significant health and public safety concerns**Pant et al., "COVID-19 Misinformation on Social Media: An Analysis of Fake News Articles and Their Spread"**.

The release of ChatGPT, alongside other generative AI and large language models, expanded research opportunities while intensifying concerns about misinformation. On one hand, these models have unlocked new scientific possibilities**Brown et al., "Language Models as Zero-Shot Learners"**; on the other, they have raised issues regarding hallucinations and the lack of citations, which challenge scientific authenticity**Hancock et al., "The Hallucinations of Language Models"**. In response, new fact-checking approaches emerged. For instance, one study addressed the verification of simulated medical abstracts by examining disease and gene names**Ratner et al., "Snorkel: Fast Training of Neural Networks with Rich Output Space"**, while another explored fact-checking solutions to mitigate risks associated with factuality in large language models**Vossen et al., "Evaluating the Factuality of Text Generated by Large Language Models"**. Additional systems, such as LLM-Augmenter, have been designed to cross-verify content against external resources, and deep-learning classifiers have been used to check AI-generated radiology reports**Liu et al., "Deep Learning for Radiology Report Generation: A Systematic Review"**. Moreover, SelfCheckGPT has been developed to assess factuality on a sentence-by-sentence basis by ranking text chunks**Pang et al., "SelfCheckGPT: Assessing Factuality of Text with Sentence-Level Ranking"**.

Biomedical research has benefited from early models like BioBART, which assisted with Named Entity Recognition (NER), Entity Linking, and Question Answering tasks at a limited scale**Chen et al., "BioBERT: A Pre-Trained BERT Model for Biomedical Text Classification"**. Following ChatGPT’s debut, studies began exploring its utility in biomedical question-answering**Kassler et al., "ChatGPT for Biomedical Question Answering: A Study on Its Performance and Limitations"**. Concurrently, researchers have investigated the use of large language models to generate knowledge directly or via Retrieval-Augmented Generation (RAG) methods. RAG integrates contextual prompts to enhance the freshness and accuracy of the generated information**Lewis et al., "Retrieval-Augmented Generation for Conversational AI"**. For example, one study employed ChatGPT as a decision support system for self-screening by embedding screening guidelines into hypothetical cases**Kim et al., "ChatGPT-Based Decision Support System for Self-Screening: A Pilot Study"**. Another used RAG-based prompt engineering to extract structured representations of drug combinations from clinical trials**Wang et al., "RAG-Driven Prompt Engineering for Extracting Structured Representations from Clinical Trials"**. Similar approaches have also improved PubMed’s retrieval capabilities**McInnes et al., "Improving PubMed Retrieval with Large Language Models"**.

Beyond biomedical applications, large language models have been evaluated for their fact-checking abilities in news and multilingual settings. Comparative studies of ChatGPT, Bing AI CoPilot, and Gemini (formerly Bard) have highlighted both their potential and the continuing need for human oversight in news verification**Hovy et al., "Comparative Study of Fact-Checking Capabilities in Large Language Models"**. In the multilingual arena, research employing techniques such as zero-shot, chain-of-thought, and cross-lingual prompting has shown that languages with fewer resources may sometimes yield more accurate fact-checking results**Wang et al., "Multilingual Fact-Checking with Zero-Shot and Chain-of-Thought Prompts"**. Additionally, studies have underscored the importance of developing guidelines for using AI in fact-checking news headlines**Ratner et al., "Guidelines for Using AI in Fact-Checking News Headlines: A Study on Human Oversight and Quality Control"**.