\section{Related Studies}
The study of embedding representations in Large Language Models (LLMs) has been central to advancements in natural language processing, as embeddings determine how linguistic units are mapped into continuous vector spaces. The quality of embeddings directly influences the ability of LLMs to capture semantic and syntactic relationships, impacting performance across a broad range of downstream tasks such as text generation, machine translation, and information retrieval. Conventional embedding techniques, however, rely on deterministic mappings, which impose constraints on adaptability and contextual generalization. As models scale in size and complexity, limitations associated with static embeddings become increasingly apparent, necessitating the exploration of alternative approaches that introduce stochasticity and contextual flexibility into embedding structures. 

\subsection{Static Nature of Traditional Embeddings}

Token embeddings in LLMs traditionally remain fixed once trained, constraining their capacity to reflect changes in linguistic usage or evolving discourse patterns over time \cite{ harrington2024recursive}. Fixed embeddings establish a one-to-one correspondence between tokens and vector representations, disregarding variations in meaning that arise across different textual contexts \cite{atox2024evaluating}. This static nature leads to limitations in handling polysemous words, as a single vector representation fails to account for diverse meanings based on syntactic and semantic dependencies \cite{grushail2024adaptive, raines2024enhancing}. Furthermore, embeddings learned during the initial training phase are unable to incorporate novel linguistic phenomena, such as the introduction of new terms or evolving word associations driven by cultural and technological shifts \cite{quinn2024applying}. The inflexibility of static embeddings particularly affects low-resource and morphologically rich languages, where variations in meaning often depend on morphological transformations and syntactic constructions \cite{ashcroft2024evaluation}. Additionally, reliance on fixed embeddings can introduce biases inherent in training data, as the lack of adaptability prevents embeddings from mitigating spurious correlations learned during pretraining \cite{romanovna2024dynamic}. The inability to update embeddings dynamically leads to brittle representations, impacting the robustness of LLMs in adapting to unseen text distributions or domain-specific applications \cite{fossan2024semantic}. 

\subsection{Probabilistic Approaches to Embedding Dynamics}

To mitigate the constraints imposed by static embeddings, various probabilistic techniques have been proposed to introduce uncertainty and flexibility into embedding spaces \cite{roova2024exploring}. One line of research modeled word representations as probability distributions rather than fixed vectors, allowing embeddings to capture the inherent ambiguity in language through multivariate representations \cite{rikitoshi2024automated}. Such methods leveraged latent variable models to enable probabilistic reasoning over semantic similarity, producing embeddings that reflect degrees of association rather than fixed relationships \cite{watanabe2024empower}. Another approach employed Bayesian neural networks to learn embeddings with uncertainty estimation, thereby allowing LLMs to adapt token representations in response to changing linguistic contexts \cite{zakiev2024emergent}. Techniques integrating Gaussian mixture models into embeddings further allowed for multiple potential vector representations per token, enhancing the expressiveness of embedding spaces \cite{vitiello2024context}. Variational autoencoder-based embeddings provided another means of encoding probabilistic transitions, wherein tokens were assigned latent distributions that could shift based on contextual cues \cite{mcintosh2023culturally}. Despite these advancements, probabilistic embedding techniques often require extensive computational resources due to the need for sampling-based inference and distributional parameterization, limiting their practicality for large-scale LLMs \cite{hou2024benchmarking}.

\subsection{Contextualized Embeddings in Transformer Models}

Transformer-based architectures introduced contextualized embeddings, significantly improving upon traditional static representations through mechanisms such as self-attention and deep contextual modeling \cite{keith2024optimizing}. Instead of assigning each token a single fixed embedding, transformer-based models generate context-dependent representations, ensuring that the same word can have different vector representations depending on its surrounding text \cite{slaten2024probabilistic}. This enabled LLMs to distinguish between homonyms and resolve ambiguities by leveraging sentence-level dependencies, thereby enhancing performance on tasks requiring deep linguistic understanding \cite{nishikado2024mitigating}. The introduction of bidirectional encoders further enriched contextual embeddings, allowing representations to be conditioned on both preceding and succeeding tokens within a sequence \cite{beard2024adaptive}. Layer-wise transformations applied to token embeddings across multiple attention heads contributed to nuanced contextual differentiation, improving syntactic and semantic coherence in generated outputs \cite{yarie2024mitigating}. However, contextualized embeddings remain fundamentally deterministic for a given input sequence, meaning that embeddings do not incorporate stochasticity that could further enhance adaptability in diverse linguistic scenarios \cite{shofman2024negative}. While attention-based architectures achieve superior context sensitivity, they still rely on predefined token mappings that may limit the scope of adaptation in evolving linguistic environments \cite{harrington2024mitigating}.

\subsection{Incorporation of Stochastic Processes in Language Models}

Beyond contextual embeddings, recent efforts have explored the integration of stochastic processes within LLM architectures to introduce variability into representations \cite{novado2024multi}. One approach involved stochastic embedding dropout, where embeddings were randomly masked during training to encourage generalization and robustness \cite{meibuki2024improving}. Methods based on reinforcement learning applied stochastic perturbations to embeddings to optimize for adaptability in generative tasks, allowing models to refine token representations dynamically based on task-specific rewards \cite{ sawhai2024token}. Techniques leveraging contrastive learning imposed probabilistic constraints on embedding transitions, ensuring that representations preserved meaningful semantic relationships while maintaining flexibility in adaptation \cite{grayson2024mitigating}. Some models incorporated latent state sampling mechanisms, wherein embeddings transitioned between predefined states probabilistically, allowing for dynamic word sense disambiguation under varying textual conditions \cite{ping2024measuring}. The application of Markov processes to embeddings provided another means of integrating stochasticity, with transition probabilities dictating how token representations evolved over time in response to contextual shifts \cite{li2024evaluating, shao2024automated}. Despite the promising potential of stochastic processes, existing implementations primarily focus on training-time modifications rather than embedding transitions during inference, thereby limiting their applicability in real-time generative tasks \cite{roberts2024extending}.