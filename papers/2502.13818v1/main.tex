\documentclass[journal]{IEEEtran}
\ifCLASSINFOpdf
\else
\fi
\usepackage{subcaption}                        
\usepackage{graphicx}           
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{bm}
\usepackage{cite}
\usepackage{url}
\usepackage{makecell}     
\usepackage{multirow} 
\usepackage{setspace}
\usepackage{cellspace}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{breqn}
\usetikzlibrary{arrows,positioning} 
\usetikzlibrary{plotmarks}
\usetikzlibrary{calc}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{patterns}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{fit}
\usetikzlibrary{arrows,positioning,shapes,backgrounds,fit} 
\newcommand{\TrainingSet}{\bm{T}} % OK   
\newcommand{\ValidationSet}{\bm{V}} % OK
\newcommand{\TestSet}{\Psi}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}
\hyphenation{op-tical net-works semi-conduc-tor}
\pgfplotsset{compat=1.18}
\begin{document}       
\title{Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge}          
\author{Nikolaos Dionelis, Nicolas Longépé, Alessandra Feliciotti, Mattia Marconcini, Devis Peressutti, Nika Oman Kadunc, JaeWan Park, Hagai Raja Sinulingga, Steve Andreas Immanuel, Ba Tran, Caroline Arnold % <-this % stops a space            
\thanks{N. Dionelis and N. Longépé are with the European Space Agency (ESA), $\Phi$-lab, ESRIN, Italy, e-mail: \{Nikolaos.Dionelis, Nicolas.Longepe\}@esa.int.}  
\thanks{A. Feliciotti and M. Marconcini are with MindEarth, Switzerland, e-mail: \{alessandra.feliciotti, mattia.marconcini\}@mindearth.ch.}
\thanks{D. Peressutti and N. O. Kadunc are with Sinergise/ Planet, Slovenia, email: \{devis.peressutti, nika.oman-kadunc\}@planet.com.}
\thanks{J. Park, H. R. Sinulingga, and S. A. Immanuel are with TelePIX, Seoul, South Korea, e-mail: \{eric.park, hagairaja, steve\}@telepix.net.}   
\thanks{B. Tran is with Axelspace Corporation, Tokyo, Japan, e-mail: thba1590@gmail.com.}   
\thanks{C. Arnold is with Helmholtz Institute Hereon, Germany and German Climate Computing Center DKRZ, Germany, e-mail: arnold@dkrz.de.}    
%\thanks{Manuscript received October 04, 2024; revised November 4, 2024.}}  
\thanks{Manuscript created November, 2024; received January, 2025.}}  
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~XX, No.~X, August~2024}%    
%\markboth{Journal IEEE GRSM Special Issue,~Vol.~XX, No.~X, December~2024}%    

% % Journal of XX,~VOL.~XX, NO.~X, MONTH~YEAR      

%\markboth{Journal IEEE GRSM Special Issue,~Vol.~XX, No.~X, January~2025}%    
\markboth{Journal of XX,~VOL.~XX, NO.~X, MONTH~YEAR}%    
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
\maketitle                                                          
\begin{abstract}               
Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe, co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The AI4EO Challenge MapYourCity was opened in 2024 for 4 months. We present the Top-4 performing models, and the evaluation results. During inference, the performance of the models using both all three input modalities and only the two top-view modalities, i.e. without the street-view images, is examined. The evaluation results show that the models are effective and can achieve good performance on this difficult real-world task of estimating the age of buildings, even on previously unseen cities, as well as even using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.

\end{abstract}

\IEEEpeerreviewmaketitle         



\section{Introduction}     
\label{sec:intro} 
%\textbf{Overview and importance.}                                                         
Estimation of the age of buildings in cities is important for sustainability, urban planning, and structural safety purposes.          
Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development that seeks to effectively combat climate change, as well as rising temperatures.  
With increase in urbanization and Earth Sciences research into urban areas, the need to estimate the energy efficiency of buildings, so as to meet important sustainability goals, has never been greater.      
The age of buildings plays an important role in energy modelling, as well as in urban policies.              
The age of the building can be used as a proxy for the energy efficiency.       
However, %\textit{despite} the big number of multi-modal datasets and neural network architectures that have been introduced over the years, 
data for the construction period of buildings is not currently available globally in a \textit{consistent}      and homogeneous manner. To this end, as a start, in this study, we focus on European cities.  








In this real-world application study for sustainability and climate, the main aim is to develop models that automatically estimate the construction year of buildings with improved performance using Artificial Intelligence (AI) techniques and cross-view datasets based on street-view images and Earth Observation (EO) datasets.      
The      goals of this paper are to present the new multi-modal dataset, examine example images from this street-view and EO dataset, present the community data challenge that we organized, and present the evaluation results of the top-performing models from the challenge.    
%proposed model, the Age of Buildings for Sustainability (ABS) model, 
%We evaluate the EO generalization performance of the models.           
%and Out-of-Distribution (OoD) generalization   
We introduce a new benchmark dataset for a real-world multi-modal application study. It consists of three modalities, including street-view ground images, top-view satellite Very High Resolution (VHR) images at 50 cm resolution, and Copernicus Sentinel-2 \textit{multi-spectral} satellite data. 
All the modalities are co-localized with respect to the specific building under study, as well as its label of the construction epoch. The problem is formulated as a classification task with seven classes for the construction epoch, ranging from 1900 to nowadays.




% for geospatial generalization.           
%Both the generalization performance of the models, including of our proposed model ABS, and their OoD generalization are of great importance for accurately estimating the age of buildings for energy efficiency ratings and sustainability purposes.       
%The generalization performance of the models is of great importance for accurately estimating the age of buildings for energy efficiency ratings and sustainability purposes.              
%and their OoD generalization  
%We assess the geospatial generalization of the models on cities that are not included in the training dataset, i.e. on previously unseen cities.  
%images of buildings from    
This paper evaluates the EO generalization performance of the models on cities that are not included in the training dataset, i.e. on new/ previously unseen cities. %, is of great importance.    
We examine the performance of the models when both training and testing are performed with all 3 modalities. There is benefit in using the top-view VHR images (and to a lesser extent, Sentinel-2 data), in addition to the street-view images.        
In this work, we also examine the performance of the models when training is performed with all the three modalities,      and testing is performed \textit{without} the street-view images, i.e. when      using only \textit{two} out of the three encoders (i.e. top-view VHR and Sentinel-2 images). These results are compared to those when training and testing are performed using all the three modalities.

%Nowadays, using deep neural networks and AI, it is possible to have models that accurately estimate the construction epoch of buildings.                      
%In this work, we also highlight the role and contribution of the top-view VHR satellite and Sentinel-2 images to the final model performance. 
%The proposed model, which is based on a Transformer model and for the satellite Sentinel-2 data, on an EO Foundation Model, is trained and tested on the MyCD dataset. Our model is tested on data that include all three modalities of street-view images, top-view VHR images, and Sentinel-2 data, as well as tested on the two modalities of top-view VHR images and Sentinel-2 data, i.e. without using the street-view images.
%The models are trained and tested on the new dataset MyCD.   
%Moreover, the models are evaluated on data that include all the three input modalities of street-view images, top-view VHR images, and Sentinel-2 data, as well as tested on the two modalities of top-view VHR images and Sentinel-2 data, i.e. without using the street-view images. 

\begin{figure*}[]                     
  \centering ~~~~~~~~~    
  \begin{subfigure}{0.4\linewidth}    
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}} 
    \centering \includegraphics[height=4.5cm, width=6cm]{unnamed1.pdf}  
    %\caption*{ (a) }
    %\label{fig:short-a}
  \end{subfigure}
  \hfill
  \hspace{2pt} \begin{subfigure}{0.48\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[height=4.5cm, width=5.1cm]{unnamed2.pdf}  
    %\caption{ } 
    %\label{fig:short-b}   
  \end{subfigure}
  \caption{\textbf{Left:} Example street-view image from the multi-modal dataset. \textbf{Right:} Example top-view satellite VHR image.} %from the dataset      
  \label{fig:short1}     
\end{figure*}




\textbf{Main contributions.}                                                                                                               
Our contributions are that this work introduces a novel multi-modal dataset, presents the challenge we organized to estimate the age of buildings in different cities in Europe using this dataset, examines the performance evaluation results of the models from the challenge, and shows the potential of effectively combining and fusing street-view, top-view satellite and remotely sensed Sentinel-2 EO data for accurately predicting the construction period of buildings.    
In this paper, we present and compare the performance of the models from the challenge that have been trained and tested on the new multi-modal street-view and EO dataset.








%\section{Methodology and Data}                
\section{Data and Task}                
\subsection{The multi-modal street-view and EO dataset MyCD} \label{sec:dataset1}       
We present the multi-modal dataset and examine example images from this street-view and EO dataset.    
The dataset MyCD encompasses the three modalities of: a) Street-view images, b) Top-view satellite VHR RGB images at 50 cm resolution, and c) Multi-spectral 12-band S-2 data.  
These input modalities have very different characteristics (number of spectral bands, spatial resolution, and building perspectives from top and street-view). 
In the dataset, the three modalities are co-localized with respect to the specific building under study.  
%For the building construction epoch, we have labels from seven classes. 
%The main aim of the dataset MyCD is to accurately predict the age of buildings using street-view ground images, top-view satellite VHR images, and satellite Sentinel-2 data, as well as using only top-view satellite VHR and satellite Sentinel-2 images.
In Fig. \ref{fig:short1}, we show two example images for the modalities of street-view ground and top-view satellite VHR, respectively.

%In this section, we will present the new multi-modal dataset and examine example images from this street-view and EO dataset.   
%The dataset MyCD has the three modalities of: a) Street-view images, b) Top-view satellite VHR images, and c) Satellite Sentinel-2 data.   
%These three input modalities have very different characteristics. 
%In the dataset, the three modalities are co-localized with respect to the specific building/ house under study.
%For the building construction epoch, we have labels from seven classes. 
%The main aim of the dataset MyCD is to accurately predict the age of buildings using street-view ground images, top-view satellite VHR images, and satellite Sentinel-2 data, as well as using only top-view satellite VHR and satellite Sentinel-2 images.
%In Fig. \ref{fig:short1}, we show two example images for the modalities of street-view ground and top-view satellite VHR, respectively.

%We observe that the street-view ground image in Fig. \ref{fig:short1} (left) includes the \textit{facade} of the building for which we would like to estimate its construction period.   
%We also observe that the top-view satellite VHR image in Fig. \ref{fig:short1} (right) covers a much larger geographical area than the corresponding street-view image and has different characteristics, as we also discussed in Secs. \ref{sec:intro} and \ref{sec:rel}. 






\begin{figure}[t]                              
  \centering \includegraphics[height=6.7cm, width=7.7cm]{maintouse.pdf}    
  \caption{Example top-view satellite VHR image and the corresponding geo-localized S-2 image (here, RGB bands for visualization purposes, where the dataset includes all the $12$ spectral bands, i.e. L2A) from the multi-modal dataset MyCD.}     
  \label{fig:short2}                     
\end{figure}



















In Fig. \ref{fig:short2}, an example top-view VHR image and the corresponding S-2 image, both from the new multi-modal dataset MyCD, are shown, together with the building location in the centre, as a \textit{blue dot} in the image.                    
We observe the geospatial overlay of the two \textit{top-view}      image modalities and that the two top-view modalities are centered at the building under study.

In Fig. \ref{fig:short1vv54}, additional examples of street-view and top-view images from the new multi-modal dataset MyCD are shown.




The dataset MyCD comprises cross-view image data from $6$ countries in Europe, and more specifically from $19$ cities, for $7$ different building construction epoch classes extracted from the EUBUCCO database.              
For the training set data, $15$ cities have been chosen%/ selected  
, while for the test set data, the \textit{remaining} $4$ cities have been used.            
The building construction epoch classes are $7$, i.e. \textit{labelled}     from $0$ to $6$, where Class $0$ is the building construction age ``Before 1930'', Class $1$ the period 1930-1945, Class $2$ the epoch 1946-1960, Class $3$ the period 1961-1976, Class $4$ the time epoch 1977-1992, Class $5$ the period 1993-2006, and Class $6$ ``After 2006''.       
In Figs. \ref{fig:short3} and \ref{fig:short4}, we show the number of image samples in the dataset MyCD per country and building construction epoch class, as well as per city.





\subsection{The task of predicting the age of buildings} \label{sec:agebuil}                 
The multi-modal dataset MyCD was used in the challenge that we organized in 2024 to predict the age of buildings in several \textit{different} cities in Europe.      
The goal of the challenge was to achieve the best performance possible on cities not included in the training data, i.e. on previously unseen cities. In addition, half of the test samples did not include the street-view modality.   
With this data challenge, we hope to advance the research on models’ out-of-distribution (OoD) generalization.

We present the evaluation results for the OoD generalization performance of the top-performing models from the challenge when: 1) both training and testing are performed with all the three modalities of the dataset MyCD, and 2) training is performed with the three modalities of the street-view and EO dataset and testing/ inference is performed using only the \textit{top-view} satellite modalities, i.e. without the street-view images.







\iffalse
\subsection{Baseline model: Age of Buildings for Sustainability (ABS)} \label{sec:baselinemo}    
We present the baseline model Age of Buildings for Sustainability (ABS) which is based on Transformers, and more specifically on the recently proposed model \textit{SegFormer} \cite{transf3, transf2}, which uses multi-scale features.     
This baseline model of the challenge employs three encoder networks, one for each of the modalities of the dataset MyCD, and performs late data fusion by concatenating the feature representations in the latent space.  
The \textit{flowchart} of the model ABS is shown in Fig. \ref{fig:shortfignew}.     
The output is the construction epoch class for the queried building.
%The model ABS is trained, validated, and evaluated on the multi-modal dataset MyCD.  
%For the two encoders for the modalities street-view and top-view VHR, we use the model SegFormer B5. 
%Also, for the Sentinel-2 images, we use an EO Foundation Model that has been trained on unlabelled global Sentinel-2 data \cite{EOFM1, EOFM2}.       
%The pre-training task was geo-location prediction and all spectral bands were used.    
%The architecture is a geo-aware U-Net-based model.
%The model ABS combines different models trained for each image modality independently.
We also release our code for ABS for reproducibility\footnote{The GitHub repository is: \url{http://github.com/AI4EO/MapYourCity}.}.
\fi





\iffalse
\textbf{Evaluation metrics.}        
In this work, for the numerical evaluation results, we compute the classification confusion matrix for the task of predicting the construction period of buildings, where we have $7$ different classes, for example ``After 2006''.      
We calculate the average of the diagonal items of the confusion matrix, and we report this value, which is also known as the Mean Producer Accuracy (MPA) evaluation metric.       
%We do this for: a) both training and testing with all the three modalities, and b) training with the three modalities and testing with only the two top-view satellite modalities. 
%For these two \textit{different} evaluation settings, we also compute and report the average between these two MPA scores, and this is the evaluation metric ``Total'' in our results.      
%In Fig. \ref{fig:short5}, the MPA when training and testing with all the three modalities, the MPA when training with the three modalities and testing with only the two top-view modalities, and the Total score are presented.  
\fi



%in Table 1. ???
%Sec. \ref{sec:evalu}. ????
%Table 1 presents the performance of the top 10 models from the challenge when: i) both training and testing are performed with all three modalities of the dataset MyCD  (MPA-all-mod), and ii) training is performed with the three modalities of the street-view and EO dataset and testing/ inference is performed using only the \textit{top-view} satellite modalities, without the street-view images (MPA-top-mod). The metric TOTAL presents the average of the above MPAs. %-all-mod and MPA-top-mod.





\iffalse
The main target of the multi-modal dataset in this work is the estimation of the construction epoch of buildings as this is of great importance for sustainability.     
Sustainable buildings minimize energy consumption and are crucial for combating climate change, as well as for responsible urban city planning and development.
In Sec. \ref{sec:dataset1}, we presented the new multi-modal dataset and examined example images from this street-view and EO dataset.       
This dataset was used in the challenge that we organized to predict the age of buildings in several different cities in Europe.      
We will present the evaluation results of the many different models from the challenge in Sec. \ref{sec:evalu}. 
We will examine the performance of the various models when: i) both training and testing are performed with all three modalities of the dataset MyCD, and ii) training is performed with the three modalities of the street-view and EO dataset and testing/ inference is performed using only the \textit{top-view} satellite modalities, without the street-view images.

Despite the many different multi-modal image datasets that have been introduced over the years, data for the construction epoch of buildings in cities is not always publicly available.       
Several street-view image datasets have been proposed in the literature, as we also discussed in Sec. \ref{sec:rel}.   
However, labels for the age of buildings are in general lacking in the existing works.  
The age of buildings can be used as a \textit{proxy} for the energy efficiency of the buildings, and this is important.  
With increase in urbanization, the need to estimate the energy efficiency of buildings to meet important sustainability goals has never been greater.
\fi





\begin{figure*}[t]              
  \centering ~~~   
  \begin{subfigure}{0.41\linewidth}     
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[width=3.8cm]{imageunnamedd.pdf}   
    %\caption*{ (a) }
    %\label{fig:short-a}
  \end{subfigure}
  \hfill
  \hspace{-202pt} \begin{subfigure}{0.22\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[width=2.64cm]{imunnamed.pdf}    
    %\caption{ } 
    %\label{fig:short-b}     
  \end{subfigure}
  \hfill
  \hspace{-52pt} \begin{subfigure}{0.22\linewidth} %\hspace{52pt}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[width=2.64cm]{imaunnamed.pdf}         
    %\caption{ }    
    %\label{fig:short-b}            
  \end{subfigure}~~~ 
  \caption{\textbf{Left:} Example street-view image of building from the dataset MyCD, and the corresponding top-view VHR image. The images are \textit{co-localized} with respect to the specific building. \textbf{Right:} Example top-view image from our multi-modal dataset.}      
  \label{fig:short1vv54}          
\end{figure*}



\begin{figure}[t]             
  \centering     
  \includegraphics[width=8.4cm]{keep1.pdf}  
  \caption{Number of images per country and building age class.} % building construction epoch       
  \label{fig:short3}    
\end{figure}

\begin{figure}[t]             
  \centering   
  \includegraphics[width=8.6cm]{keep2.pdf}             
  \caption{Number of images per city and building age class.}     
  \label{fig:short4}       
\end{figure}





\iffalse   
\textbf{Generalization to cities not included in the training data.}       
One of the main mains of the multi-modal dataset MyCD is to achieve good/ top performance on cities that are not included in the training data, i.e. on new/ previously unseen cities.  
This is the OoD generalization performance of the models, and this is important.
In Fig. \ref{fig:short3}, we present the sample distribution of the multi-modal images in the dataset: we show the number of images per country and building construction epoch class.   
Here, we observe that the dataset contains images from $6$ different countries in Europe. 
In addition, Fig. \ref{fig:short4} shows the number of images per city and building construction epoch class in the new multi-modal dataset MyCD.       
For the training set data, $15$ cities have been chosen/ selected, while for the test set data, the \textit{remaining} $4$ cities have been used.           
Therefore, the dataset MyCD comprises cross-view data from $6$ countries and $19$ cities in Europe.    
The building construction epoch classes are $7$: labelled from $0$ to $6$, where Class $0$ is the building construction age before 1930, Class $1$ the time period 1930-1945, Class $2$ the epoch 1946-1960, Class $3$ the period 1961-1976, Class $4$ the time epoch 1977-1992, Class $5$ the period 1993-2006, and Class $6$ ``After 2006''.    
\fi







\iffalse
%\begin{figure*}[t]                                
\begin{figure}[t]                                
  \centering        
  \includegraphics[width=8.3cm]{flowch1new.pdf}               
  \caption{Flowchart of the model ABS, the baseline of the challenge we organized using the multi-modal dataset MyCD.}        
  \label{fig:shortfignew}           
%\end{figure*}
\end{figure}
\fi





\iffalse
%\begin{figure*}[]                              
\begin{figure}[]                              
  \centering     
  \includegraphics[width=11.15cm]{PRILEADERBOARD.pdf}     
  \caption{Evaluation results of the different models, and more specifically the private leaderboard of the challenge.}    
  \label{fig:short5}    
%\end{figure*}
\end{figure}

%\begin{figure*}[t]                             
\begin{figure}[t]                             
  \centering       
  \includegraphics[width=11.925cm]{PULEADERBOARD.pdf}             
  \caption{Results from the evaluation of the models, and more specifically the public leaderboard of the challenge.}    
  \label{fig:short6}                            
%\end{figure*}  
\end{figure}
\fi









%\pagebreak  
% \begin{figure*}[t]                             
%   \centering       
%   \includegraphics[width=11.1cm]{flowch1new.pdf}             
%   \caption{Flowchart of ABS, i.e. the baseline of the challenge.}      
%   \label{fig:shortfignew}          
% \end{figure*}




\iffalse
\begin{figure*}[t]                   
  \centering ~    
  \begin{subfigure}{0.4\linewidth}    
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[width=8.01cm]{ConfusionMatrixABSPriLea.pdf} % height=4.5cm,   
    %\caption*{ (a) }
    %\label{fig:short-a}
  \end{subfigure} 
  \hfill
  \hspace{0.05pt} \begin{subfigure}{0.48\linewidth} 
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}} 
    \centering \includegraphics[width=8.01cm]{ConfusionMatrixABSPubliLea.pdf}    
    %\caption{ }   
    %\label{fig:short-b}     
  \end{subfigure}   
  \caption{\textbf{Left:} Classification confusion matrix evaluation results for the model ABS for the multi-modal dataset MyCD. Private leaderboard of challenge. \textbf{Right:} Confusion matrix for the model ABS described in Sec. \ref{sec:baselinemo}. Public leaderboard of challenge.}     
  \label{fig:short1aaa}                
\end{figure*}
\fi






\iffalse
We present the evaluation of the models from the challenge in Figs. \ref{fig:short5} and \ref{fig:short6}.
%Table 1. ???       
%Sec. \ref{sec:evalu}. ???? 
We use MPA\_ALL\_MODALITIES to denote the setting when both training and testing are performed with all modalities.   
In addition, we also use MPA\_TOP\_VIEW to denote the setting when training is performed with all modalities and inference is performed using only the two \textit{top-view} modalities.
%Table 1    
%presents the performance of the top 10 models from the challenge when: i) both training and testing are performed with all three modalities of the dataset MyCD (MPA-all-mod), and ii) training is performed with the three modalities of the street-view and EO dataset and testing/ inference is performed using only the \textit{top-view} satellite modalities, without the street-view images (MPA-top-mod). 
%The metric TOTAL presents the average of the above MPAs. %-all-mod and MPA-top-mod.   
%MPA\_ALL\_MODALITIES             
%MPA\_TOP\_VIEW  
TOTAL is the average of the two MPAs.
\fi











\iffalse
%\subsection{Related Work} \label{sec:rel}                      
\section{Related Work} \label{sec:rel}                           
Many different AI models from the recent literature perform data fusion of street-view images and top-view satellite images.         
With AI and deep learning methods, by performing late data fusion (i.e. concatenating latent representations in the feature space), it is possible to achieve better performance than early data fusion, because street-view images and top-view satellite data have many differences, for example in resolution, object characteristics and scale, and significant differences in appearance.   
Street-view and top-view satellite images do not look alike and top-view data cover a larger geographical area.  
In general, the effective fusion of street-view ground images and nadir-view satellite or aerial images is nowadays an open research problem.       
By jointly using both street- and top-view images, the model proposed in \cite{Zhu1} performs mapping of building functions using a specific data fusion strategy that combines in a decision-level manner different models trained for each image modality independently.         
In this way, by performing decision-level data fusion of the two modalities, the method presented in \cite{Zhu1} combines the predictions of different models in order to accurately perform mapping of building functions, for example commercial, residential, public, and industrial, i.e. classification using four different classes.

Furthermore, nowadays, the importance of street-view images has become evident in the literature \cite{StreetView1}.    
Street-view imagery is an important data source for urban analytics, helping us derive insights for making informed decisions \cite{chen2024global}.   
Using street-view images, we can analyze for example the built environment, the vegetation, and the transportation \cite{StreetView1}, and the results of the analyses can be linked to important health, urban, and socio-economic studies.    
In this work, street-view images from Mapillary are used in the multi-modal dataset MyCD.

Street-view imagery is also important for urban infrastructure assessment \cite{2024_global_streetscapes} and urban mobility, as well as for spatial data infrastructures, urban planning \cite{streetpaper}, and urban greenery.  
In \cite{2024_global_streetscapes}, the dataset Global Streetscapes is introduced which contains $10$ million street-view images from $688$ cities around the Earth, from $212$ countries and different regions.     
These images have been crowdsourced from Mapillary and KartaView.         
Moreover, the dataset Global Streetscapes \cite{2024_global_streetscapes} also includes metadata information for every street-view image, such as geo-location, longitude and latitude, acquisition time, and contextual, semantic, and perceptual information.

%Nowadays,  
Transformer
%-based   
models have shown to achieve good/ top performance and outperform other 
%alternative 
models, for example models that use an architecture based on Convolutional Neural Networks (CNN), U-Net, and/or Residual Networks (ResNet).            
Importantly, in \cite{transf1}, geo-location matching of street-view images and top-view aerial images is performed using a Transformer-based model and the mechanism of attention. 
The model TransGeo proposed in \cite{transf1} is based on Transformers because global information modeling is crucial.    
It performs attention-guided non-uniform cropping of images as a means of attending and zooming-in on only informative image patches, not focusing on non-informative (for the task) parts of the image.    
%Furthermore,    
%The method presented in \cite{transf1} using Transformer models achieves state-of-the-art performance for cross-view geo-location matching on both urban and rural datasets.  








\iffalse   
Recent \textit{Transformer}-based vision models are powerful and effective, achieving very good performance \cite{transf2}.      
For example, the model proposed in \cite{transf2} is for semantic segmentation of unlabelled data, i.e. top-view aerial images, using domain adaptation techniques, and is based on the architecture SegFormer \cite{transf3}.  
The results of the model presented in \cite{transf2} have been used for cross-view geo-location matching of street-view images and top-view aerial images in \cite{transf4}.  
The accurate matching of street- and top-view images in general is challenging \cite{transf5}. 
In this work, the new multi-modal street-view and EO dataset MyCD contains co-localized images of street-view, top-view satellite VHR, and satellite Sentinel-2 $10$m resolution, and the main aim is to accurately predict the age of buildings in cities in Europe for energy efficiency and sustainability purposes.  
For this important difficult task of automatically estimating the construction year of buildings, even in cities that are not included in the training data (i.e. previously unseen cities), as well as even when inference/ testing is performed using only the top-view modalities without the street-view image, the use of Transformer-based models is critical.
\fi

%In the literature, m
Different datasets that contain top-view satellite or aerial images exist, for example the dataset OpenEarthMap \cite{xia_2023_openearthmap}.      
This publicly available dataset is %global. It is 
designed for the task of high-resolution land cover mapping \textit{semantic} segmentation.       
More specifically, OpenEarthMap contains $5000$ aerial and satellite images, and for each image, it has land cover labels/ annotations for $8$ different classes.   
It covers in total $97$ regions, from $44$ countries across $6$ continents.    
This recent dataset OpenEarthMap has been used, for example, in the model proposed in \cite{xia_2023_openearthmap2} that performs semantic segmentation land cover classification in a few-shot continual learning setting.
\fi
























\section{Models and Methodologies}         
\subsection{Models developed to predict building construction year} \label{sec:mainmodels}      

In this section, we will discuss the models from the challenge that estimate the age of buildings.    
 % and as we will also see in Sec. \ref{sec:evalu}
More specifically, we discuss the models that achieve the top/ winning results of the challenge (i.e. in Sec.~\ref{sec:evalu}, in Figs. \ref{fig:short5} and \ref{fig:short6}).    
In particular, in this section, we will discuss four different models that predict the construction epoch of buildings.   
% \textcolor{blue}{(as we will see in Sec. \ref{sec:evalu} in Figs. \ref{fig:short5} and \ref{fig:short6})}.     
 To achieve very good/ \textit{top} performance, they employ several different deep learning techniques, for example class label correlation modelling.







\textbf{Label correlation.}                 
The team Creamble %\textcolor{red}{, to achieve the \textcolor{red}{top/} winning results as we will see in Sec. \ref{sec:evalu} in Figs. \ref{fig:short5} and \ref{fig:short6}}, 
\textcolor{black}{performs} label correlation modelling to take advantage of the distance between the labels.   
For the seven building construction epoch classes, adjacent classes are correlated, while \textit{far apart} classes are not.  
Soft labels \textcolor{black}{are}      also used, as well as label smoothing and hyperparameter search.   
In addition, K-fold cross-validation, the recently proposed model EVA \cite{EVANEW}, and a model ensemble of $15$ models with soft-voting \textcolor{black}{are} also used.    
The model minimizes the cross-entropy loss with label smoothing (0.3).   
%\textcolor{blue}{There are used, the cross-entropy loss with label smoothing (0.3), and for augmentation the Python's trivial augment.}   
The flowchart 
%diagram     
is shown in Fig. \ref{fig:short1aaaasfgasg1}, where the model ensemble that estimates the construction period of buildings is illustrated.







\begin{figure}[t]                        
  \centering   
  \includegraphics[width=8.8cm]{newusemainflowc1.pdf}   
  \caption{Flowchart of the model to predict the age of buildings by the team Creamble.}    
  \label{fig:short1aaaasfgasg1}      
\end{figure}

\begin{figure}[t]             
  \centering  
  \includegraphics[width=8.8cm]{foruseflowch2.pdf}   
  \caption{Flowchart diagram of the model by the team TelePIX AI, where the results are in Sec.~\ref{sec:evalu}, in Figs. \ref{fig:short5} and \ref{fig:short6}.}    
  \label{fig:short1aaaasfgasg2}        
\end{figure}







\textbf{Prediction-level data fusion.}            
The team TelePIX AI performs prediction-level data fusion (rather than data fusion at the feature level), as well as K-fold cross-validation with the city as the fold for each country.    
For this method, a mean confidence model ensemble \textcolor{black}{is} used.   
%\textcolor{blue}{For pre-processing, normalization, histogram equalization, random rotate. flip and random hide street-view are used.}
Moreover, \textcolor{black}{as encoders,} the model EVA-02 \cite{EVA2} \textcolor{black}{is} also 
%used    
utilized, as well as reBEN \cite{reBENNEW}. 
\textcolor{black}{For classification, the cross-entropy loss is used.}   
The flowchart diagram of this model is illustrated in Fig. \ref{fig:short1aaaasfgasg2}. 
Hyper-parameter optimization using Bayesian optimization with Optuna (e.g. for the learning rate (lr), batch size, and lr \textit{decay} factor) \textcolor{black}{is} also performed.                 
In addition, here, the image pre-processing \textcolor{black}{is}:       normalization, histogram equalization, random rotation and flipping, and random hide of the street-view images, where the latter is for the missing modality problem during inference (i.e. missing \textit{street-view}     images).   
\textcolor{black}{For this approach, the key factors for good model performance are: powerful encoders, random hide \textit{street-view}      (P: 0.5) so that the model learns the significant features from the VHR images and Sentinel-2, split based on country, ensemble technique and pre-processing to increase robustness of the final prediction and training stability.}




\begin{figure}[t]                  
  \centering  
  \includegraphics[width=8.8cm]{useflowch3.pdf}     
  \caption{Flowchart of model using an additional shared feature encoder for the street- and top-view VHR images and the distribution alignment loss, by the team xmb.}    
  \label{fig:short1aaaasfgasgasdf1}        
\end{figure}

\begin{figure}[t]                   
  \centering     
  \includegraphics[width=8.8cm]{mainflowchart4.pdf}    
  \caption{Flowchart of the model that uses SwinT encoders, by the team The AI Buzzard.}     
  \label{fig:short1aaaasfgasgasdf2}     
\end{figure}









\textbf{Ensemble with country categorical feature.}             
The team xmb \textcolor{black}{trains} an ensemble of four models.     
For this approach, a categorical feature for the country in the format of one-hot representation \textcolor{black}{is} used. 
This country categorical feature \textcolor{black}{is} used in the late data fusion concatenation of the features.     
For the three input modalities of the dataset MyCD, the latent representations after average pooling are combined, also using the country categorical feature, and then inputted to the final dense Fully Connected (FC) layer.
In addition, in the      ensemble, the models EfficientNet and MobileViT \textcolor{black}{are} also used, and geometric data fusion \textcolor{black}{is} also performed.           
The latent feature representations before average pooling \textcolor{black}{are}      combined according to the method presented in \cite{FusionNew1}, and then inputted to average pooling and, then, concatenated using the country categorical feature. 
To address the problem of the missing modality during inference, street-view images during training are      randomly replaced with all zeros, i.e. input dropout, and shared feature modelling \cite{MissingModality1} \textcolor{black}{is} performed.    
Here, an additional shared encoder for the two different modalities of street-view and top-view VHR \textcolor{black}{is} used. 
The flowchart diagram is shown in Fig. \ref{fig:short1aaaasfgasgasdf1}.     
The distribution alignment loss function term \cite{MissingModality1} \textcolor{black}{is} also added in the objective function minimized during training.




\textbf{SwinT-encoder late data fusion with attention.}       
The model Swin Transformer (SwinT) pretrained on ImageNet22K is used by the team The AI Buzzard, for the encoders of      the three different input modalities.
Late data fusion using attention \textcolor{black}{is} performed, as well as 5-fold cross-validation training, grouped by cities and stratified by the class label.  
Data fusion \textcolor{black}{is} performed by computing a weighted sum of the embeddings.   
In this way, the model \textcolor{black}{is} able to handle missing data during inference (i.e. missing street-view images).  
The flowchart diagram is shown in Fig. \ref{fig:short1aaaasfgasgasdf2}.    
For this method, the models \textcolor{black}{are first trained on \textit{all} the data and, then, they are fine-tuned on the individual countries}. %  (i.e. Fig. \ref{fig:short3}).}        
For data augmentation, random flipping, color jittering, and random resized cropping \textcolor{black}{are} utilized.








\textbf{Discussion about the key features.}             
The \textit{main} characteristics of the models discussed in the preceding paragraphs are: i) very good/ top pretrained model, i.e. EVA \cite{EVANEW} or EVA-02 \cite{EVA2}, that is used as a starting point so as to not train from scratch, ii) model ensemble, iii) hyper-parameter optimization using for example \textit{Optuna}, iv) label correlation modelling between adjacent classes (per modality) 
%, \textcolor{orange}{either between adjacent classes per mode and/or between the same epoch class and the different modalities,} 
because the distance between adjacent classes is smaller than the distance between classes that are not adjacent, and v) missing modality \textit{distribution alignment} loss together with a shared feature encoder.        
Here, additional key features of the main models are: K-fold cross-validation training, \textit{geometric} data fusion using the methodology in \cite{FusionNew1}, and use of a categorical feature for the country in the form of one-hot vector representation.










\subsection{Baseline model: Age of Buildings for Sustainability (ABS)} \label{sec:baselinemo}    
We present the baseline model Age of Buildings for Sustainability (ABS) which is based on Transformers, and more specifically on the recently proposed model \textit{SegFormer} \cite{transf3, transf2}, which uses multi-scale features.      
This baseline model of the challenge employs \textit{three}      encoder networks, one for each of the modalities of the dataset MyCD, and performs late data fusion by concatenating the feature representations in the latent space.   
The \textit{flowchart} of the model ABS is shown in Fig. \ref{fig:shortfignew}.     
The output is the construction epoch class for the queried building.
%The model ABS is trained, validated, and evaluated on the multi-modal dataset MyCD.  
%For the two encoders for the modalities street-view and top-view VHR, we use the model SegFormer B5. 
%Also, for the Sentinel-2 images, we use an EO Foundation Model that has been trained on unlabelled global Sentinel-2 data \cite{EOFM1, EOFM2}.       
%The pre-training task was geo-location prediction and all spectral bands were used.    
%The architecture is a geo-aware U-Net-based model.
%The model ABS combines different models trained for each image modality independently. 
We also release our code for ABS for reproducibility\footnote{The GitHub repository is: \url{http://github.com/AI4EO/MapYourCity}.}.








%\begin{figure*}[t]                                 
\begin{figure}[t]                                
  \centering        
  \includegraphics[width=8.1cm]{flowch1new.pdf}            %\includegraphics[width=8.3cm]{flowch1new.pdf}                
  \caption{Flowchart of the model ABS, the baseline of the challenge we organized using the multi-modal dataset MyCD.}        
  \label{fig:shortfignew}           
%\end{figure*}
\end{figure}










\iffalse 
\subsection{Model Age of Buildings for Sustainability (ABS)} \label{sec:baselinemo}                   
%\textbf{The baseline model ABS.}                          
%In this section,   
We present the baseline model Age of Buildings for Sustainability (ABS) which is based on Transformers, and more specifically on the recently proposed model \textit{SegFormer} \cite{transf3, transf2}, which uses multi-scale features.  
This baseline model of the challenge employs three encoder networks, one for each of the modalities of the dataset MyCD, and performs late data fusion by concatenating the feature representations in the latent space.  
The \textit{flowchart} of the model ABS is shown in Fig. \ref{fig:shortfignew}.     
The output is the construction epoch class for the queried building.
%The model ABS is trained, validated, and evaluated on the multi-modal dataset MyCD.  
%For the two encoders for the modalities street-view and top-view VHR, we use the model SegFormer B5. 
%Also, for the Sentinel-2 images, we use an EO Foundation Model that has been trained on unlabelled global Sentinel-2 data \cite{EOFM1, EOFM2}.      
%The pre-training task was geo-location prediction and all spectral bands were used.    
%The architecture is a geo-aware U-Net-based model.
%The model ABS combines different models trained for each image modality independently.
We also release our code for ABS for reproducibility\footnote{The GitHub repository is: \url{a}.}. 
\fi





\iffalse

\section{Method}

In \textit{Method}, please discuss the rationale behind the proposed algorithm(s), the motivation behind selecting specific algorithmic components in those methods and all other issues which are directly concerned with the process of designing and implementing your approach. 

\subsection{Data Preparation}

Also, it is a good place to discuss your approach(es) toward data preparation. Please put special emphasis on reproducibility of the algorithm(s)---a link to the gitlab/github repository should be added in the description. 

\section{Results and Discussion}
In \textit{Results and Discussion}, please discuss your approach toward experimentation, dataset training/validation splits (those dataset splits can be also made publicly available in the repository mentioned above), internal (cross-)validation of the methods, training curves, visualizations and so forth---feel free to include all experimental results, observations and insights that you feel could be of interest to a reader and could make following your reasoning plausible. Feel free to split the sections into subsections as appropriate. To see some examples, please look at \url{https://ieeexplore.ieee.org/abstract/document/9246669}.







%\subsection{Related Work} \label{sec:rel}                      
\section{Related Work} \label{sec:rel}                           
Many different AI models from the recent literature perform data fusion of street-view images and top-view satellite images.         
With AI and deep learning methods, by performing late data fusion (i.e. concatenating latent representations in the feature space), it is possible to achieve better performance than early data fusion, because street-view images and top-view satellite data have many differences, for example in resolution, object characteristics and scale, and significant differences in appearance.   
Street-view and top-view satellite images do not look alike and top-view data cover a larger geographical area.  
In general, the effective fusion of street-view ground images and nadir-view satellite or aerial images is nowadays an open research problem.       
By jointly using both street- and top-view images, the model proposed in \cite{Zhu1} performs mapping of building functions using a specific data fusion strategy that combines in a decision-level manner different models trained for each image modality independently.         
In this way, by performing decision-level data fusion of the two modalities, the method presented in \cite{Zhu1} combines the predictions of different models in order to accurately perform mapping of building functions, for example commercial, residential, public, and industrial, i.e. classification using four different classes.

Furthermore, nowadays, the importance of street-view images has become evident in the literature \cite{StreetView1}.    
Street-view imagery is an important data source for urban analytics, helping us derive insights for making informed decisions \cite{chen2024global}.   
Using street-view images, we can analyze for example the built environment, the vegetation, and the transportation \cite{StreetView1}, and the results of the analyses can be linked to important health, urban, and socio-economic studies.    
In this work, street-view images from Mapillary are used in the multi-modal dataset MyCD.

Street-view imagery is also important for urban infrastructure assessment \cite{2024_global_streetscapes} and urban mobility, as well as for spatial data infrastructures, urban planning \cite{streetpaper}, and urban greenery.  
In \cite{2024_global_streetscapes}, the dataset Global Streetscapes is introduced which contains $10$ million street-view images from $688$ cities around the Earth, from $212$ countries and different regions.     
These images have been crowdsourced from Mapillary and KartaView.         
Moreover, the dataset Global Streetscapes \cite{2024_global_streetscapes} also includes metadata information for every street-view image, such as geo-location, longitude and latitude, acquisition time, and contextual, semantic, and perceptual information.

%Nowadays,  
Transformer
%-based   
models have shown to achieve good/ top performance and outperform other 
%alternative 
models, for example models that use an architecture based on Convolutional Neural Networks (CNN), U-Net, and/or Residual Networks (ResNet).            
Importantly, in \cite{transf1}, geo-location matching of street-view images and top-view aerial images is performed using a Transformer-based model and the mechanism of attention. 
The model TransGeo proposed in \cite{transf1} is based on Transformers because global information modeling is crucial.    
It performs attention-guided non-uniform cropping of images as a means of attending and zooming-in on only informative image patches, not focusing on non-informative (for the task) parts of the image.    
%Furthermore,    
%The method presented in \cite{transf1} using Transformer models achieves state-of-the-art performance for cross-view geo-location matching on both urban and rural datasets.  








\iffalse   
Recent \textit{Transformer}-based vision models are powerful and effective, achieving very good performance \cite{transf2}.      
For example, the model proposed in \cite{transf2} is for semantic segmentation of unlabelled data, i.e. top-view aerial images, using domain adaptation techniques, and is based on the architecture SegFormer \cite{transf3}.  
The results of the model presented in \cite{transf2} have been used for cross-view geo-location matching of street-view images and top-view aerial images in \cite{transf4}.  
The accurate matching of street- and top-view images in general is challenging \cite{transf5}. 
In this work, the new multi-modal street-view and EO dataset MyCD contains co-localized images of street-view, top-view satellite VHR, and satellite Sentinel-2 $10$m resolution, and the main aim is to accurately predict the age of buildings in cities in Europe for energy efficiency and sustainability purposes.  
For this important difficult task of automatically estimating the construction year of buildings, even in cities that are not included in the training data (i.e. previously unseen cities), as well as even when inference/ testing is performed using only the top-view modalities without the street-view image, the use of Transformer-based models is critical.
\fi

%In the literature, m
Different datasets that contain top-view satellite or aerial images exist, for example the dataset OpenEarthMap \cite{xia_2023_openearthmap}.       
This publicly available dataset is %global. It is 
designed for the task of high-resolution land cover mapping \textit{semantic} segmentation.        
More specifically, OpenEarthMap contains $5000$ aerial and satellite images, and for each image, it has land cover labels/ annotations for $8$ different classes.   
It covers in total $97$ regions, from $44$ countries across $6$ continents.     
This recent dataset OpenEarthMap has been used, for example, in the model proposed in \cite{xia_2023_openearthmap2} that performs semantic segmentation land cover classification in a few-shot continual learning setting.






\section{Evaluation: Experiments and Results} \label{sec:evalu}                   
In this section, we will present the evaluation results of the different models from the challenge. 
Our aim is to also highlight the role and \textit{contribution} of the two top-view satellite image modalities to the model performance.      
In the previous sections, we presented the multi-modal street-view and EO dataset MyCD, examined example images from this new dataset, and presented the challenge we organized to predict the age of buildings in different cities using this dataset. 
Accurately estimating the construction year of buildings is of great importance for energy efficiency and sustainability purposes.       
Hence, we have introduced our benchmark dataset and the real-world multi-modal application study for OoD generalization.                
We will now \textit{evaluate} the OoD generalization performance of the different Deep Neural Network (DNN) based models. 

We examine the performance of the models for the training and testing with all three modalities, i.e. street-view ground images, top-view VHR satellite images, and satellite Sentinel-2 data.   
Moreover, we examine the results for the testing \textit{without} the street-view images, using only two out of the three trained encoders. We perform inference using only two input modalities, i.e. top-view VHR images and Sentinel-2 data. These results are compared to the evaluation results for the training and testing with all three modalities. The model might (or might not) be the same for these two cases, i.e. \textit{identical} training or not.

\textbf{Evaluation metrics.}       
For the numerical evaluation results, we compute the classification confusion matrix for the task of predicting the construction period of buildings, where we have $7$ different classes, for example ``After 2006''.      
We calculate the average of the diagonal items of the confusion matrix and report this value, which is also known as the Mean Producer Accuracy (MPA) evaluation metric.       
We do this for: a) both training and testing with all the three modalities, and b) training with the three modalities and testing with only the two top-view satellite modalities.
For these two \textit{different} evaluation settings, we also compute and report the average between these two MPA scores, and this is the evaluation metric ``Total'' in our results.    
In Fig. \ref{fig:short5}, the MPA when training and testing with all the three modalities, the MPA when training with the three modalities and testing with only the two top-view modalities, and the Total score are presented.

\begin{figure}[]                        
  \centering   
  \includegraphics[width=9.15cm]{PRILEADERBOARD.pdf}     
  \caption{Evaluation results of the different models. Private leaderboard of the challenge.}    
  \label{fig:short5}    
\end{figure}

\begin{figure}[t]                             
  \centering    
  \includegraphics[width=9.925cm]{PULEADERBOARD.pdf}          
  \caption{Results from the evaluation of the models. Public leaderboard of the challenge.}    
  \label{fig:short6}                          
\end{figure}

\begin{figure}[t]                 
  \centering   
  \begin{subfigure}{0.4\linewidth}    
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[width=6.01cm]{ConfusionMatrixABSPriLea.pdf} % height=4.5cm,   
    %\caption*{ (a) }
    %\label{fig:short-a}
  \end{subfigure} 
  \hfill
  \hspace{0.05pt} \begin{subfigure}{0.48\linewidth} 
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[width=6.01cm]{ConfusionMatrixABSPubliLea.pdf}  
    %\caption{ }   
    %\label{fig:short-b}     
  \end{subfigure}   
  \caption{\textbf{Left:} Classification confusion matrix evaluation results for the model ABS for the multi-modal dataset MyCD. Private leaderboard of the challenge. \textbf{Right:} Confusion matrix for the model ABS described in Sec. \ref{sec:baselinemo}. Public leaderboard of challenge.}     
  \label{fig:short1aaa}                
\end{figure}

The evaluation results in Fig. \ref{fig:short5} are for the final/ private leaderboard of the challenge.
Furthermore, for the performance of the models, the results in Fig. \ref{fig:short6} are for the public leaderboard of the challenge.  
The evaluation results in both figures assess the OoD generalization performance of the models on cities that are not included in the training dataset, i.e. on previously unseen cities.  
We observe that using AI and deep learning models, it is possible to predict the construction epoch of buildings using only the top-view input modalities, that is, without the street-view images, and this is an interesting key novel finding.
\fi
\iffalse
\begin{table}[t]                              
  \centering   
      %\vspace{5pt}    
      \hspace{-10pt} \begin{minipage}[b]{.46\linewidth}      
    \caption{Evaluation using MPA, accuracy, precision, recall, and F1-score for the model ABS described in Sec. \ref{sec:baselinemo}. In MyCD, use of the two private and public leaderboard datasets of the challenge.}\label{tab:table1paperage}       
      \centering                   
        \begin{tabular} 
{p{1.2cm} p{2.08cm} p{2cm}}     
  
\toprule            
    \normalsize {\small \textbf{Model ABS}} &   
    \normalsize {\small \textbf{Private leaderboard}} &    
    \normalsize {\small \textbf{Public leaderboard}}        
\\
\midrule               
\midrule  
\normalsize {\small \textbf{MPA}}
& \normalsize {\normalsize ~ \small $61.57$} &     
    \normalsize {\normalsize  ~ \small $60.02$}   
\\   
\midrule          
\midrule 
\normalsize {\small \textbf{Acc.}} &  
\normalsize {\normalsize  ~ \small $63.16$} &
\normalsize {\normalsize  ~ \small $59.13$}
\\ 
\midrule 
\normalsize {\small \textbf{Pre.}} &  
\normalsize {\normalsize  ~ \small $63.56$} &
\normalsize {\normalsize  ~ \small $61.63$}   
\\ 
\midrule   
\normalsize {\small \textbf{Rec.}} &   
\normalsize {\normalsize  ~ \small $63.16$} & 
\normalsize {\normalsize  ~ \small $59.13$}   
\\  
\midrule 
\normalsize {\small \textbf{F1}} &   
\normalsize {\normalsize  ~ \small $62.89$} &
\normalsize {\normalsize  ~ \small $59.40$}   
\\ 
\midrule                                
\midrule      
\end{tabular}    
\end{minipage}
\hspace{1.3pt} %\hfill 
\begin{minipage}[b]{.50\linewidth}              
  \centering    
  %\centerline{\epsfig{figure=14.3.png,width=2.945cm}}       
  \caption{Model ABS for cities that are in the training data, i.e. In-Distribution (ID), and \textit{ablation} studies for cities that are not in the training data for Sentinel-2 EO FM, Sentinel-2, and two top-view modalities.}\label{tab:table1resultsCASmain}      
      %\vspace{5pt}  
      \centering                    
        \begin{tabular}     
{p{1.2cm} p{1.15cm} p{1.15cm} p{1.15cm} p{1.1cm}}            
  
\toprule            
    \normalsize {\small \textbf{Model ABS}} &    
    \normalsize {\small \textbf{ABS ID}} &    
    \normalsize {\small \textbf{Abl. FM}} &    
    \normalsize {\small \textbf{Abl. ~S-2}} &    
    \normalsize {\small \textbf{Abl. top}}          
\\
\midrule                  
\midrule  
\normalsize {\small \textbf{MPA}}
& \normalsize {\normalsize ~ \small $64.61$} &     
    \normalsize {\normalsize  ~ \small $58.09$} &       
    \normalsize {\normalsize  ~ \small $60.11$} &       
    \normalsize {\normalsize  ~ \small $58.72$}    
\\    
\midrule               
\midrule 
\normalsize {\small \textbf{Acc.}} &  
\normalsize {\normalsize  ~ \small $68.97$} &
\normalsize {\normalsize  ~ \small $58.73$} &
\normalsize {\normalsize  ~ \small $60.48$} & 
\normalsize {\normalsize  ~ \small $57.47$}    
\\ 
\midrule 
\normalsize {\small \textbf{Pre.}} &  
\normalsize {\normalsize  ~ \small $68.93$} & 
\normalsize {\normalsize  ~ \small $59.90$} & 
\normalsize {\normalsize  ~ \small $61.31$} & 
\normalsize {\normalsize  ~ \small $61.35$}   
\\ 
\midrule   
\normalsize {\small \textbf{Rec.}} &   
\normalsize {\normalsize  ~ \small $68.97$} &  
\normalsize {\normalsize  ~ \small $58.73$} &  
\normalsize {\normalsize  ~ \small $60.48$} &  
\normalsize {\normalsize  ~ \small $57.47$}   
\\  
\midrule  
\normalsize {\small \textbf{F1}} &   
\normalsize {\normalsize  ~ \small $68.41$} &  
\normalsize {\normalsize  ~ \small $58.20$} & 
\normalsize {\normalsize  ~ \small $60.42$} & 
\normalsize {\normalsize  ~ \small $56.58$}    
\\  
\midrule                                                     
\midrule        
\end{tabular}%\label{tab:table2paperage}    
\end{minipage} %\hfill                                       
\end{table}

\textbf{Evaluation using the confusion matrix.}                 
In the previous paragraphs, we examined the results of the models using the MPA score.    
In addition to the average of the diagonal items of the confusion matrix, we also compute the confusion matrix itself, the accuracy, precision, recall, and F1-score.   
In Fig. \ref{fig:short1aaa}, we present the classification confusion matrix evaluation results for the model ABS described in Sec. \ref{sec:baselinemo}.    
We observe that the off-diagonal terms have in general small values, and sometimes even the value of \textit{zero}.
In particular, in this case, for the private leaderboard dataset, we observe that a building that was built after the year 2006 has never been confused with a building built in 1920 or 1940.

\textbf{Accuracy, precision, recall, and F1-score.}      
In Table \ref{tab:table1paperage}, we present the evaluation results of MPA, accuracy, precision, recall, and F1-score for the model ABS.
We examine the performance on both the private and public leaderboard datasets of the challenge.
These results are for cities that are not in the training data (unseen/ new cities), as well as for training and testing using \textit{all} the three modalities of the dataset MyCD.     
In addition, in Table \ref{tab:table1resultsCASmain}, the evaluation results for the model ABS for cities that are in the training data, i.e. In-Distribution (ID), are shown, as well as the results for the ablation study when the Sentinel-2 EO Foundation Model is not used (i.e. with the modality Sentinel-2, but without using a pre-trained model/ EO Foundation Model) for cities that are not in the training data.  
Furthermore, the results for the ablation study when the modality Sentinel-2 is \textit{not} used are shown, as well as for the ablation study when the two top-view modalities (i.e. satellite VHR and satellite Sentinel-2) are not used.
We observe in Tables \ref{tab:table1paperage} and \ref{tab:table1resultsCASmain} that the drop in performance in MPA due to previously unseen cities is approximately $4.71\%$, while in accuracy approximately $8.42\%$.
\fi
%For cities that are not in the training data (unseen/ new cities), as well as for training and testing using \textit{all} the three modalities of the dataset MyCD, the ABS gives a value of $61.57$ in MPA metric.     



\iffalse
The evaluation result for the model ABS for cities that are in the training data, i.e. In-Distribution (ID), in the MPA metric is $64.61$, 
while %the result for the ablation study when the Sentinel-2 for cities that are not in %the training data is $58.09$,   
%and then    
MAP is $60.11$ when the modality satellite Sentinel-2 is \textit{not} used.
Moreover, when the two top-view modalities (i.e. satellite VHR and satellite Sentinel-2) are both not utilized, then the MAP evaluation metric is $58.72$.
%We observe that the drop in performance in MPA due to previously unseen cities is approximately $4.71\%$, while in accuracy approximately $8.42\%$. 
\fi












%\begin{figure*}[]                                
\begin{figure}[]                              
  \centering      
  \includegraphics[width=8.8cm]{PRILEADERBOARD.pdf}     
  %\includegraphics[width=11.15cm]{PRILEADERBOARD.pdf}     
  \caption{Evaluation results of the different models, and more specifically the private leaderboard of the challenge.}    
  \label{fig:short5}    
%\end{figure*}
\end{figure}



%\begin{figure*}[t]                             
\begin{figure}[t]                             
  \centering        
  \includegraphics[width=8.45cm]{PULEADERBOARD.pdf}      %\includegraphics[width=11.925cm]{PULEADERBOARD.pdf}             
  \caption{Results from the evaluation of the models, and more specifically the public leaderboard of the challenge.}    
  \label{fig:short6}                             
%\end{figure*}  
\end{figure}






%\subsection{Related Work} \label{sec:rel}                       
\section{Related Work} \label{sec:rel}                           
Many different AI models from the recent literature perform data fusion of street-view images and top-view satellite images.         
With AI and deep learning methods, by performing late data fusion (i.e. concatenating latent representations in the feature space), it is possible to achieve better performance than early data fusion, because street-view images and top-view satellite data have many differences, for example in resolution, object characteristics and scale, and significant differences in appearance.   
Street-view and top-view satellite images do not look alike and top-view data cover a larger geographical area.  
In general, the effective fusion of street-view ground images and nadir-view satellite or aerial images is nowadays an open research problem.       
By jointly using both street- and top-view images, the model proposed in \cite{Zhu1} performs mapping of building functions using a specific data fusion strategy that combines in a decision-level manner different models trained for each image modality independently.         
In this way, by performing decision-level data fusion of the two modalities, the method presented in \cite{Zhu1} combines the predictions of different models in order to accurately perform mapping of building functions, for example commercial, residential, public, and industrial, i.e. classification using four different classes.

Furthermore, nowadays, the importance of street-view images has become evident in the literature \cite{StreetView1}.     
Street-view imagery is an important data source for urban analytics, helping us derive insights for making \textit{informed} decisions \cite{chen2024global}.    
Using street-view images, we can analyze for example the built environment, the vegetation, and the transportation \cite{StreetView1}, and the results of the analyses can be linked to important health, urban, and socio-economic studies.    
In this work, street-view images from Mapillary are used in the multi-modal dataset MyCD.











Street-view imagery is also important for urban infrastructure assessment \cite{2024_global_streetscapes} and urban mobility, as well as for spatial data infrastructures, urban planning \cite{streetpaper}, and urban greenery.  
In \cite{2024_global_streetscapes}, the dataset Global Streetscapes is introduced which contains $10$ million street-view images from $688$ cities around the Earth, from $212$ countries and       different regions.     
These images have been crowdsourced from Mapillary and KartaView.         
Moreover, the dataset Global Streetscapes \cite{2024_global_streetscapes} also includes metadata information for every street-view image, such as geo-location, longitude and latitude, acquisition time, and contextual, semantic, and perceptual information.

%Nowadays,  
Transformer
%-based   
models have shown to achieve good/ top performance and outperform other 
%alternative      
models, for example models that use an architecture based on Convolutional Neural Networks (CNN), U-Net, and/or Residual Networks (ResNet).            
Importantly, in \cite{transf1}, geo-location matching of street-view images and top-view aerial images is performed using a Transformer-based model and the mechanism of attention. 
The model TransGeo proposed in \cite{transf1} is based on Transformers because global information modeling is crucial.    
It performs attention-guided non-uniform cropping of images as a means of attending and zooming-in on only informative image patches, not focusing on non-informative (for the task) parts of the image.    
%Furthermore,    
%The method presented in \cite{transf1} using Transformer models achieves state-of-the-art performance for cross-view geo-location matching on both urban and rural datasets.  








\iffalse   
Recent \textit{Transformer}-based vision models are powerful and effective, achieving very good performance \cite{transf2}.      
For example, the model proposed in \cite{transf2} is for semantic segmentation of unlabelled data, i.e. top-view aerial images, using domain adaptation techniques, and is based on the architecture SegFormer \cite{transf3}.  
The results of the model presented in \cite{transf2} have been used for cross-view geo-location matching of street-view images and top-view aerial images in \cite{transf4}.  
The accurate matching of street- and top-view images in general is challenging \cite{transf5}. 
In this work, the new multi-modal street-view and EO dataset MyCD contains co-localized images of street-view, top-view satellite VHR, and satellite Sentinel-2 $10$m resolution, and the main aim is to accurately predict the age of buildings in cities in Europe for energy efficiency and sustainability purposes.  
For this important difficult task of automatically estimating the construction year of buildings, even in cities that are not included in the training data (i.e. previously unseen cities), as well as even when inference/ testing is performed using only the top-view modalities without the street-view image, the use of Transformer-based models is critical.
\fi

%In the literature, m
Different datasets that contain top-view satellite or aerial images exist, for example the dataset OpenEarthMap \cite{xia_2023_openearthmap}.       
This publicly available dataset is %global. It is  
designed for the task of high-resolution land cover mapping \textit{semantic} segmentation.        
More specifically, OpenEarthMap contains $5000$ aerial and satellite images, and for each image, it has land cover labels/ annotations for $8$ different classes.   
It covers in total $97$ regions, from $44$ countries across $6$ continents.      
This recent dataset OpenEarthMap has been used, for example, in the model proposed in \cite{xia_2023_openearthmap2} which performs semantic segmentation land cover classification in a \textit{few-shot}      continual learning setting.
















\iffalse 
\begin{figure*}[]                               
%\begin{figure}[]                                 
  \centering      
  \includegraphics[width=11.15cm]{PRILEADERBOARD.pdf}     
  \caption{Evaluation results of the different models, and more specifically the private leaderboard of the challenge.}    
  \label{fig:short5}    
\end{figure*}
%\end{figure}

\begin{figure*}[t]                             
%\begin{figure}[t]                             
  \centering       
  \includegraphics[width=11.925cm]{PULEADERBOARD.pdf}             
  \caption{Results from the evaluation of the models, and more specifically the public leaderboard of the challenge.}    
  \label{fig:short6}                             
\end{figure*}  
%\end{figure}
\fi















%\section{Results and Discussion}    
\section{Evaluation and Results}\label{sec:evalu}  
\textbf{Evaluation metrics.}          
In this work, for the numerical evaluation results, we compute the classification confusion matrix for the task of predicting the construction period of buildings, where we have $7$ different classes, for example ``After 2006''.      
We calculate the average of the diagonal items of the confusion matrix, and we report this metric, which is also known as the Mean Producer Accuracy (MPA) evaluation metric.       
%We do this for: a) both training and testing with all the three modalities, and b) training with the three modalities and testing with only the two top-view satellite modalities. 
%For these two \textit{different} evaluation settings, we also compute and report the average between these two MPA scores, and this is the evaluation metric ``Total'' in our results.      
%In Fig. \ref{fig:short5}, the MPA when training and testing with all the three modalities, the MPA when training with the three modalities and testing with only the two top-view modalities, and the Total score are presented.

We present the evaluation of the models from the challenge in Figs. \ref{fig:short5} and \ref{fig:short6}. 
%Table 1. ???        
%Sec. \ref{sec:evalu}. ???? 
We use MPA\_ALL\_MODALITIES to denote the setting when both training and testing are performed with all modalities.   
In addition, we also use MPA\_TOP\_VIEW to denote the setting when training is performed with all modalities and inference is performed using only the two \textit{top-view} modalities.
%Table 1    
%presents the performance of the top 10 models from the challenge when: i) both training and testing are performed with all three modalities of the dataset MyCD (MPA-all-mod), and ii) training is performed with the three modalities of the street-view and EO dataset and testing/ inference is performed using only the \textit{top-view} satellite modalities, without the street-view images (MPA-top-mod). 
%The metric TOTAL presents the average of the above MPAs. %-all-mod and MPA-top-mod.   
%MPA\_ALL\_MODALITIES             
%MPA\_TOP\_VIEW  
TOTAL is the average of the two MPAs.







%We observe i
%In Figs. \ref{fig:short5} and \ref{fig:short6}, i
%i
In Figs. \ref{fig:short5} and \ref{fig:short6},  
in the 
%evaluation 
results\footnote{\url{https://platform.ai4eo.eu/map-your-city/leaderboard}},
%,      
%that   
the MPA\_TOP\_VIEW scores are lower      than MPA\_ALL\_MODALITIES.
%In addition,   
%we observe that 
%t
The top score for MPA\_ALL\_MODALITIES in the private leaderboard is $0.74$ indicating      that the estimation of the construction epoch of buildings in unseen
%/ new   
cities is possible. %and can be performed.  
%Furthermore, w 
When using \textit{only}     the top-view data during inference, 
%i.e. when \textit{not} using the street-view ground images,  
then the performance of the models decreases by more than $10$ points in the MPA score.
Finally, we observe that for the TOTAL scores, the top winning values      are approximately $0.65$ in the private leaderboard.
























\iffalse  
\begin{figure*}[t]                        
  \centering ~    
  \begin{subfigure}{0.4\linewidth}    
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \centering \includegraphics[width=8.01cm]{ConfusionMatrixABSPriLea.pdf} % height=4.5cm,   
    %\caption*{ (a) }
    %\label{fig:short-a}
  \end{subfigure} 
  \hfill
  \hspace{0.05pt} \begin{subfigure}{0.48\linewidth} 
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}} 
    \centering \includegraphics[width=8.01cm]{ConfusionMatrixABSPubliLea.pdf}    
    %\caption{ }   
    %\label{fig:short-b}     
  \end{subfigure}   
  \caption{\textbf{Left:} Classification confusion matrix evaluation results for the model ABS for the multi-modal dataset MyCD. Private leaderboard of challenge. \textbf{Right:} Confusion matrix for the model ABS described in Sec. \ref{sec:baselinemo}. Public leaderboard of challenge.}     
  \label{fig:short1aaa}                
\end{figure*}
\fi




\iffalse
The evaluation result for the model ABS for cities that are in the training data, i.e. In-Distribution (ID), in the MPA metric is $64.61$, 
while %the result for the ablation study when the Sentinel-2 for cities that are not in %the training data is $58.09$,   
%and then     
MPA is $60.11$ when the S-2 modality is \textit{not} used.
Moreover, when the two top-view modalities (i.e. VHR and S-2) are both not utilized, then the MAP evaluation metric is $58.72$.
%We observe that the drop in performance in MPA due to previously unseen cities is approximately $4.71\%$, while in accuracy approximately $8.42\%$.
\fi









\section{Conclusion} \label{sec:conc}                 
%In this paper, we have presented the multi-modal dataset MyCD, examined example images from this street-view and EO dataset, and presented the challenge that we recently organized to accurately predict the age of buildings in different cities in Europe using this new dataset.   
%This task is very important for energy efficiency ratings and sustainability purposes, as well as for responsible urban planning and development aiming at combating climate change.         
%We have also presented the evaluation results of the top $10$ models from the challenge, in which $29$ teams participated and submitted their solutions.       
In this work, we have introduced the new multi-modal benchmark dataset MyCD and the real-world multi-modal application study for EO generalization.           
%The evaluation results for the OoD generalization performance of the different models have been presented in Sec. \ref{sec:evalu}.   
%In addition, we have also presented the model ABS, i.e. the baseline model of the challenge.   
The performance evaluation of the models from the challenge show that estimating the age of buildings in previously unseen cities is possible and effective, as well as predicting the construction period of buildings/ houses from only top-view satellite images, i.e. without using the street-view images during inference.

\iffalse
In \textit{Method}, please discuss the rationale behind the proposed algorithm(s), the motivation behind selecting specific algorithmic components in those methods and all other issues which are directly concerned with the process of designing and implementing your approach. Also, it is a good place to discuss your approach(es) toward data preparation. Please put special emphasis on reproducibility of the algorithm(s)---a link to the gitlab/github repository should be added in the description. In \textit{Results and Discussion}, please discuss your approach toward experimentation, dataset training/validation splits (those dataset splits can be also made publicly available in the repository mentioned above), internal (cross-)validation of the methods, training curves, visualizations and so forth---feel free to include all experimental results, observations and insights that you feel could be of interest to a reader and could make following your reasoning plausible. Feel free to split the sections into subsections as appropriate. To see some examples, please look at \url{https://ieeexplore.ieee.org/abstract/document/9246669}.
\fi







\iffalse
\section{Method} 

\subsection{Data Preparation}

First, an exploratory analysis was made on the HYPERVIEW challenge data to understand the inputs better. Correlations using the Person’s coefficient between the four soil parameters with each spectrum wavelength were analyzed to see if these parameters have specific absorption bands. Correlations are all close to zero for each soil parameter: there is no correlation between the soil parameters and the spectral bands.

Since there is only one value of each soil parameter per parcel, but sometimes more than 10,000 pixels per parcel, the challenge data deal with a dimension problem. In order to keep a single hyperspectral value per crop, the mean and the median of all pixels in each parcel were calculated, and tests on the model's results will be performed to keep only the better transformation.

When using machine learning models on a spectrum, information about the shape can be lost because the models use each wavelength separately. To retain the shape information of the spectrum, the first and the second derivative on each mean and median spectrum were added to the input data as tested in ~\cite{rs13122273}, ~\cite{agriculture11111129}, and ~\cite{swr-201504-0004} (see Fig. ~\ref{fig:deriv}).

\begin{figure}[ht]  
    \centering
    \includegraphics[width=1\columnwidth]{images/derivatives.png}
    \caption{Derivatives of the spectrum : the raw spectrum is in blue, the first derivative (FD) is in orange and the second derivative (SD) is in green.}
    \label{fig:deriv}
\end{figure}

For our models to accurately discriminate values from data sets, the data must be normalized. Two methods can be used: a global normalization by spectrum or a normalization by spectral band. The continuum removal technique was also used to normalize the spectra and allow them to be compared. A convex envelope, created using convex hull method, is fitted to the spectrum, and then the spectrum is divided by the convex envelope (see Fig. ~\ref{fig:continuum} and \cite{swr-201504-0004}). Wavelet transformation and FFT (Fast Fourier Transform) were applied on the spectrum to extract some signal information (see ~\cite{rs13122273}). The normalizations and transformations leading to better results will be retained.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\columnwidth]{images/continuum.png}
    \caption{Continuum removal technique applied on a spectrum}
    \label{fig:continuum}
\end{figure}

A vegetation index called NDVI (Normalized Difference Vegetation Index) was calculated to have information about the vegetation condition on the parcel. An NDVI value close to one indicates a crop with vegetation; when it is close to zero, the crop is dry. NDVI will be used to compare results to see if our models are sensitive to the vegetation on crops. 

All these transformations increase the number of input variables of the predictive models, which is now too large considering the number of parcels. Moreover, these variables are highly correlated.  In order to reduce the dimension of the input variables, we use techniques such as Principal Component Analysis and Independent Component Analysis on the spectrum and on each of its transformations.

\subsection{Model(s)}

The first phase of analysis and preprocessing of the challenge data leads to a global flowchart detailed in Fig. ~\ref{fig:flow}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\columnwidth]{images/flow.png}
    \caption{Flowchart of the preprocessing chain}
    \label{fig:flow}
\end{figure}

According to ~\cite{rs13122273}, and ~\cite{agriculture11111129}, Random Forest, lasso regression and gradient boosting decision tree can retrieve soil parameter estimation. Another type of algorithm based on Neural Networks described in ~\cite{DBLP:journals/corr/abs-1911-07757} can also be helpful, but the onboarding on the satellite of the final algorithm asking by the challenge will be complex. All these algorithms were implemented and tested on the challenge dataset. 

\section{Results and Discussion}

\subsection{Experimental Design}


The submission of solutions on the challenge platform was allowed every 12 hours. In order to train the models and fix their hyperparameters, cross-validation was used: the training set was randomly divided into five parts, and the model was made using four of them and tested on the last one.

Different models were tested to help us find better spectrum transformations, dimension reduction, machine learning models and their hyperparameters. The global score of the challenge was used to analyze results and compare models between them, as well as the score by soil parameters.

\subsection{Detailed Results and Discussion}

After the data preprocessing step, several machine learning methods were implemented. The team sent a total of 45 submissions, and our best results are listed in Table ~\ref{tab:table_res}. It contains the global challenge score using cross-validation, the score for each soil parameter using cross-validation and the global challenge score obtained after submission. 
The model giving the best results is the Random Forest without wavelets and FFT. This model combines of four Random Forests created for each soil parameter. The baseline results are increased by 18.4\%. 
 The best-estimated soil parameter is pH, and the worst is phosphorus pentoxide. The results obtained using Neural Networks do not seem to be better than Random Forest, a model that is more easily used on board, which is the ultimate goal of the challenge.


\begin{table}[ht!]
\caption{Comparison of the results of our best models (Score = MSE/MSE$^{\rm base})}
\label{tab:table_res}
\scriptsize
\renewcommand{\tabcolsep}{1.4mm}
\begin{tabular}{rrrrrrrrrrrrrrr}
\hline
\textbf{Cross-validation Score}  & \textbf{Random Forest} & \textbf{Random Forest} & \textbf{Neural Network}  \\
\textbf{per Soil Parameter}  & \textbf{} & \textbf{with wavelets} & \textbf{}  \\
\hline
K               & 0.851                & 0.822                & 0.946                \\
P               & 0.919                & 0.915                & 0.996                \\
Mg              & 0.865                & 0.845                & 0.920                \\
pH              & 0.803                & 0.799                & 0.922                \\
\hline
\textbf{Cross-validation} & & &  \\
\textbf{Global Score} & 0.860 & 0.848 & 0.947  \\
\hline
\textbf{Submission} & & &  \\
\textbf{Global Score} & 0.816 & 0.822 & 0.884  \\
\hline
\end{tabular}
\end{table}

The NDVI was used to compare results based on crop vegetation. Two Random Forests were trained based on NDVI values to create different models on parcels with a high vegetation index and those with a low NDVI; the results were inconclusive. Concerning normalization, the best model is obtained by spectral band, and using the mean of all pixels per plot is better than using the median. The hyperparameters of each machine learning model were adjusted using the cross-validation process. Regarding the PCA, the number of axes retained was determined using Recursive Feature Elimination process. It appears that 50 PCA axes are required for pH, 35 for magnesium, 40 for phosphorus pentoxide and 35 for potassium.

Figure ~\ref{fig:distrib}  represents distributions of each soil parameter on the training dataset in blue and distributions predicted by our best Random Forest in orange. The predictions are centered on the mean of the parameter, but our model does not predict the total variability.


\begin{figure}[ht]
    \centering
    \includegraphics[width=1\columnwidth]{images/distribution.png}
    \caption{Distribution of the dataset values for each soil parameter in blue and of distribution of their predicted values with our best model in orange}
    \label{fig:distrib}
\end{figure}

The results demonstrate that our best model is not able to accurately predict the extent of variability in soil parameter values, particularly for extreme values. This is due to the heterogeneity of the reflectance values of the input pixels, which may include both soil and vegetation components in a given field. However, these results provide hope for improved performance with sensors with that have more extensive spectral bands in the infrared region, which can capture the specific signatures of these soil parameters.
\fi






\section*{Acknowledgment}     

The challenge was accomplished thanks to the contributions of many people.  
%A team of researchers were implicated in the realization.     
We want to thank all the team: 
%Alessandra Feliciotti, Mattia Marconcini, Devis Peressutti, Nika Kadunc, 
Juan Pedro, Dennis Albrecht, and everyone in ESA AI4EO.

%All the team would like to warmly thank the HYPERVIEW challenge for allowing us to present our results during the IEEE conference and to write this collaborative paper.      






\ifCLASSOPTIONcaptionsoff
  \newpage 
\fi



\bibliographystyle{IEEEtran}     
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{ref_all}








\vspace{-42pt}        
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{NikolaosDionelisImage.jpg}}]{Nikolaos Dionelis} \\
M.Eng. in electrical and electronic engineering from Imperial College London (ICL), UK and PhD degree in electrical engineering from ICL. Worked at the University of Edinburgh and the University Defence Research Collaboration (UDRC) in Signal Processing as a Postdoctoral Research Associate in Machine Learning for four years. Now, working at the European Space Agency (ESA), at the $\Phi$-lab.
%Suzanne Angeli received her M.S. in Applied Mathematics from Henri-Poincaré University of Nancy, France, in 2009. She joined Capgemini Science\&IA Team in 2010 as a Data Scientist. She is involved in many AI projects using satellite data. 
\end{IEEEbiography}






\vspace{-42pt}            
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{NewMainUse.png}}]{Nicolas Longépé} \\ M.Eng. in electronics \& communication systems. M.S. degree in electronics at the National Institute for the Applied Sciences, France. PhD degree from Uni Rennes. Worked at EO Research Center of JAXA, as Japan Society for the Promotion of Science Fellow, and then as an invited researcher. Worked at CLS, France, as a research engineer in the Radar Application Division. Now, working at the European Space Agency (ESA), at the $\Phi$-lab.
%Holding a PhD in atmospheric Remote Sensing since 2005, she has worked on ocean color, detection and optical properties on aerosol and greenhouse gases, land surface temperature, radiative transfer and inversion simulation of atmospheric parameter on visible -TIR domain. She has been a project manager in space domain and innovation for 7 years.
\end{IEEEbiography}
















\vspace{-42pt}     
\begin{IEEEbiography}  [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{11696471804486.png}}]{Alessandra Feliciotti} \\
is a Project and Operations manager at MindEarth. She is also an Urban Scientist and with MindEarth, her tasks include urban and regional GIS analysis, webGIS applications and digital cartography. She also has a PhD degree from the University of Strathclyde, UK.
\end{IEEEbiography}











\vspace{-42pt}   
\begin{IEEEbiography}  [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{1629146798812.jpg}}]{Mattia Marconcini} \\
is a Research Data Scientist and Development Project Manager at the German Aerospace Center (DLR). He is an Engineer and Project Manager in the Smart Cities and Spatial Development team at the DLR Earth Observation Center (EOC) since March 2012. From August 2018, he has also been supporting MindEarth of which he has become partner in 2021. His research and work focus on the use of Earth Observation techniques to support sustainable urban development.
\end{IEEEbiography}















\vspace{-42pt}  
\begin{IEEEbiography}  [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{1517602029206.jpg}}]{Devis Peressutti} \\
is a Data/ML Scientist at Sinergise/ Planet. He develops machine learning solutions for remote sensing and earth observation applications. His research interests primarily lie in: machine learning and pattern recognition in remotely sensed imagery, image segmentation and registration, and prototyping and deploying large-scale ML algorithms to production.
\end{IEEEbiography}












\vspace{-42pt}     
\begin{IEEEbiography}  [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{1695836569188.jpg}}]{Nika Oman Kadunc} \\
is a Software Engineer / Earth Observation Scientist at Sinergise/ Planet. She is a Data Scientist in the EO Research team discovering insights from earth observation data and turning creative ideas into working innovative solutions.
\end{IEEEbiography}











\vspace{-42pt} 
\begin{IEEEbiography}  [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{photo_eric.png}}]{JaeWan Park} \\
was born on March 26 , 1989, in Republic of Korea (South Korea).   
He received a Bachelor's degree in Economics from the Hankuk University of Foreign Studies and is currently pursuing a Master's degree in Data Science at the Sogang University. His research fields include detection tasks using satellite imagery, few-shot segmentation, and multi-modal approaches. He is currently working at the TelePIX AI Team, focusing on AI research and development using satellite imagery.
\end{IEEEbiography}

\vspace{-42pt} 
\begin{IEEEbiography}  [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{hagai.jpg}}]{Hagai Raja Sinulingga} \\
is an Indonesian computer scientist who earned his master's degree from Sejong University in 2023 and his bachelor's degree from Institut Teknologi Bandung in 2020. His expertise lies in detection tasks, few-shot segmentation, and foundational models. Currently, he is an artificial intelligence researcher and developer at the TelePIX AI Team, focusing on satellite imagery applications. Prior to joining TelePIX, he conducted research in anomaly detection at the Imaging and Intelligent Systems Lab, Sejong University.
\end{IEEEbiography}

\vspace{-42pt} 
\begin{IEEEbiography}   [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{steve.jpeg}}]{Steve Andreas Immanuel } \\
was born in Indonesia. He completed his master’s degree at Sejong University in 2024, following his bachelor's from Institut Teknologi Bandung in 2021. His areas of expertise include vision-language models, few-shot segmentation, and image compression. He is currently an AI researcher and developer at the TelePIX AI Team, where he focuses on satellite imagery applications. Previously, he worked as a researcher at Vision Language Intelligence Lab, Sejong University.
\end{IEEEbiography}

\vspace{-42pt}  
\begin{IEEEbiography}  
[{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{author.jpg}}]{Ba Tran} 
Ba Tran is a Computer Vision Software Engineer at Axelspace Corporation (Tokyo, Japan), specializing in satellite image construction for the GRUS satellite constellation and satellite image analysis. He graduated from the University of Tokyo in 2016, where he earned his Bachelor's degree in Information \& Communication Engineering from the Faculty of Engineering. He completed his Master's degree at the same university, in the Graduate School of Information Science \& Technology in 2018.
\end{IEEEbiography}

\vspace{-42pt}  
\begin{IEEEbiography}    [{\includegraphics[width=1in,height=1.in,clip,keepaspectratio]{arnold.jpeg}}]{Caroline Arnold} 
Caroline Arnold graduated in Physics from the University of Tübingen in 2015. She received a PhD in theoretical physics from the University of Hamburg in 2019. She joined Helmholtz AI and the German Climate Computing Center as an AI consultant in 2020. Her interests are deep learning for spatiotemporal data, remote sensing, and climate modeling.
\end{IEEEbiography}






%\vfill       

















\end{document}