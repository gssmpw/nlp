\section{Related Work}
\label{sec:rel}                           
Many different AI models from the recent literature perform data fusion of street-view images and top-view satellite images.         
With AI and deep learning methods, by performing late data fusion (i.e. concatenating latent representations in the feature space), it is possible to achieve better performance than early data fusion, because street-view images and top-view satellite data have many differences, for example in resolution, object characteristics and scale, and significant differences in appearance.   
Street-view and top-view satellite images do not look alike and top-view data cover a larger geographical area.  
In general, the effective fusion of street-view ground images and nadir-view satellite or aerial images is nowadays an open research problem.       
By jointly using both street- and top-view images, the model proposed in \cite{Zhu1} performs mapping of building functions using a specific data fusion strategy that combines in a decision-level manner different models trained for each image modality independently.         
In this way, by performing decision-level data fusion of the two modalities, the method presented in \cite{Zhu1} combines the predictions of different models in order to accurately perform mapping of building functions, for example commercial, residential, public, and industrial, i.e. classification using four different classes.

Furthermore, nowadays, the importance of street-view images has become evident in the literature \cite{StreetView1}.    
Street-view imagery is an important data source for urban analytics, helping us derive insights for making informed decisions \cite{chen2024global}.   
Using street-view images, we can analyze for example the built environment, the vegetation, and the transportation \cite{StreetView1}, and the results of the analyses can be linked to important health, urban, and socio-economic studies.    
In this work, street-view images from Mapillary are used in the multi-modal dataset MyCD.

Street-view imagery is also important for urban infrastructure assessment \cite{2024_global_streetscapes} and urban mobility, as well as for spatial data infrastructures, urban planning \cite{streetpaper}, and urban greenery.  
In \cite{2024_global_streetscapes}, the dataset Global Streetscapes is introduced which contains $10$ million street-view images from $688$ cities around the Earth, from $212$ countries and different regions.     
These images have been crowdsourced from Mapillary and KartaView.         
Moreover, the dataset Global Streetscapes \cite{2024_global_streetscapes} also includes metadata information for every street-view image, such as geo-location, longitude and latitude, acquisition time, and contextual, semantic, and perceptual information.

%Nowadays,  
Transformer
%-based   
models have shown to achieve good/ top performance and outperform other 
%alternative 
models, for example models that use an architecture based on Convolutional Neural Networks (CNN), U-Net, and/or Residual Networks (ResNet).            
Importantly, in \cite{transf1}, geo-location matching of street-view images and top-view aerial images is performed using a Transformer-based model and the mechanism of attention. 
The model TransGeo proposed in \cite{transf1} is based on Transformers because global information modeling is crucial.    
It performs attention-guided non-uniform cropping of images as a means of attending and zooming-in on only informative image patches, not focusing on non-informative (for the task) parts of the image.    
%Furthermore,    
%The method presented in \cite{transf1} using Transformer models achieves state-of-the-art performance for cross-view geo-location matching on both urban and rural datasets.  








\iffalse   
Recent \textit{Transformer}-based vision models are powerful and effective, achieving very good performance \cite{transf2}.      
For example, the model proposed in \cite{transf2} is for semantic segmentation of unlabelled data, i.e. top-view aerial images, using domain adaptation techniques, and is based on the architecture SegFormer \cite{transf3}.  
The results of the model presented in \cite{transf2} have been used for cross-view geo-location matching of street-view images and top-view aerial images in \cite{transf4}.  
The accurate matching of street- and top-view images in general is challenging \cite{transf5}. 
In this work, the new multi-modal street-view and EO dataset MyCD contains co-localized images of street-view, top-view satellite VHR, and satellite Sentinel-2 $10$m resolution, and the main aim is to accurately predict the age of buildings in cities in Europe for energy efficiency and sustainability purposes.  
For this important difficult task of automatically estimating the construction year of buildings, even in cities that are not included in the training data (i.e. previously unseen cities), as well as even when inference/ testing is performed using only the top-view modalities without the street-view image, the use of Transformer-based models is critical.
\fi

%In the literature, m
Different datasets that contain top-view satellite or aerial images exist, for example the dataset OpenEarthMap \cite{xia_2023_openearthmap}.      
This publicly available dataset is %global. It is 
designed for the task of high-resolution land cover mapping \textit{semantic} segmentation.       
More specifically, OpenEarthMap contains $5000$ aerial and satellite images, and for each image, it has land cover labels/ annotations for $8$ different classes.   
It covers in total $97$ regions, from $44$ countries across $6$ continents.    
This recent dataset OpenEarthMap has been used, for example, in the model proposed in \cite{xia_2023_openearthmap2} that performs semantic segmentation land cover classification in a few-shot continual learning setting.
\fi