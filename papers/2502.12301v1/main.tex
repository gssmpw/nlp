% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}


% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
% \usepackage{pgf-pie} 
\usepackage{array}
\usepackage{ipa}
\usepackage{stackengine}
\usepackage{wrapfig,lipsum,booktabs}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{longtable}
\usepackage{scalefnt}
\usepackage{graphicx}
% \usepackage{natbib}
% \bibliographystyle{acl_natbib}

% \usepackage{ragged2e} % For better text wrapping within cells



\usepackage{multicol}
\usepackage{multirow}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{colortbl}
\usepackage{arydshln}
\definecolor{lgreen}{RGB}{73,174,137}
\definecolor{lred}{RGB}{182,49,54}
\definecolor{lorange}{RGB}{255, 128, 0}
\definecolor{lblue}{RGB}{0, 0, 255}
% \colorlet{lorange1}{lorange!20}
\newcommand{\iccomment}[1]{{\hphantom{}\color{lorange!100}  #1}}
\newcommand{\mistake}[1]{{\hphantom{}\color{lorange!70} \bf #1}}
% \newcommand{\mistake}[1]{{\hphantom{}\bf \color{red}  #1}}
\newcommand{\correct}[1]{{\hphantom{}\bf \color{blue}  #1}}
\newcommand{\enxx}{en$\rightarrow$xx }
\newcommand{\xxen}{xx$\rightarrow$en }
\newcommand{\bleu}{\textsc{Bleu}}
\newcommand{\chrf}{\textsc{ChrF}}
\newcommand{\gatitos}{\textsc{Gatitos}}
\newcommand{\smol}{\textsc{Smol}}
% \newcommand{\smoldog}{\scalebox{.8}{\textsc{SmolSmolDoc}}}
% \newcommand{\smoldogsent}{\scalebox{.8}{\textsc{SmolSmolDocSplit}}}
% \newcommand{\smolseal}{\scalebox{.8}{\textsc{SmolSmolSent}}}
\newcommand{\smoldog}{\textsc{SmolDoc}}
\newcommand{\smoldogsent}{\textsc{SmolDocSplit}}
\newcommand{\smolseal}{\textsc{SmolSent}}
\newcommand{\smolboth}{\textsc{Both}}
\newcommand{\smolbothg}{\textsc{Both+G}}
\newcommand{\nsmolnonenglishlangs}{115} %TODO update with
\newcommand{\nsmolsealnonenglishlangs}{81} %TODO update with
\newcommand{\nsmoldognonenglishlangs}{100} %TODO update with
\newcommand{\nsmoldogdocs}{584}
\newcommand{\nllbseed}{\textsc{NLLB-Seed}}
\newcommand{\floresone}{\textsc{Flores-101}}
\newcommand{\florestwo}{\textsc{Flores-200}}
\newcommand{\bread}{\textsc{Bread}}
\newcommand{\ntrex}{\textsc{NTreX}}
\newcommand{\gatones}{\textsc{Gatones}}
\newcommand{\madlad}{\textsc{Madlad-400}}
\newcommand{\smolsmol}{s}
\newcommand{\ugh}{$^{\textrm{\revglotstop}}$}
\newcommand{\oho}{$^\textrm{\openo}$}
\newcommand{\otho}{$^\textrm{\dh}$}

% \newcommand{\ugh}{a}
% \newcommand{\oho}{b}
% \newcommand{\otho}{c}

\title{
% There's no data like 
\textsc{SMOL}: \\
% data: \\
Professionally translated parallel data \\  for 115 under-represented languages}

% \author{
% Isaac Caswell\ugh{}\thanks{Corr.: eknielsen@google.com, icaswell@google.com}
% \quad Elizabeth Nielsen\ugh{}$^*$
% \quad Jiaming Luo\ugh{}
% \quad Colin Cherry\ugh{} \\

% \textbf{
% Geza Kovacs\ugh{}
% \quad Hadar Shemtov\ugh{}
% \quad Partha Talukdar\ugh{}
% \quad Dinesh Tewari\ugh{}
% }\\

% \textbf{
% \quad Baba Mamadi Diane\oho{}
% \quad Koulako Moussa Doumbouya\oho{}\otho{}
% }\\ 

% \textbf{
% \quad Djibrila Diane\oho{}
% \quad Solo Farabado Cissé\oho{}
% }\\
% \ugh{}Google \{Research, Deepmind\} \quad
% \otho{}Stanford University\quad
% \oho{}NKo USA INC
% }

\author{Isaac Caswell\ugh{}$^*$
Elizabeth Nielsen\ugh{}$^*$
Jiaming Luo\ugh{}
Colin Cherry\ugh{} \\
    \bf{
Geza Kovacs\ugh{}
Hadar Shemtov\ugh{}
Partha Talukdar\ugh{}
Dinesh Tewari\ugh{}
    } \\
    \bf{
Baba Mamadi Diane\oho{}
Koulako Moussa Doumbouya\oho{}\otho{}
Djibrila Diane\oho{}
Solo Farabado Cissé\oho{}
    } \\
\ugh{}Google \{Research, Deepmind\} 
\otho{}Stanford University
\oho{}NKo USA INC \\
  $^*$\texttt{\{icaswell, eknielsen\}@google.com} \\}


\begin{document}
\maketitle
\begin{abstract}
We open-source \smol{} (\textit{Set of Maximal Overall Leverage})\footnote{huggingface.co/datasets/google/smol}, a suite of
% ``Minimum Viable Datasets"
training data
to unlock translation for low-resource languages (LRLs). \smol{} has been translated into \nsmolnonenglishlangs{} under-resourced languages, including many for which there exist no previous public resources, for a total of 6.1M translated tokens. \smol{} comprises two sub-datasets, each carefully chosen for maximum impact given its size:   \smolseal{}, a set of sentences chosen for broad unique token coverage, and \smoldog{}, a document-level source focusing on a broad topic coverage.
They join the already released \gatitos{} for a trifecta of paragraph, sentence, and token-level content.
We demonstrate that using \smol{} to prompt or fine-tune Large Language Models yields robust \chrf{} improvements. In addition to translation, we provide factuality ratings and rationales for all documents in \smoldog{}, yielding the first factuality datasets for most of these languages.
\end{abstract}

\section{Introduction}

Most of the world's 7000 or so languages have few or no resources available for machine translation. However, for an expensive task like professional translation, it is not clear how to make the best use of a limited budget to generate the most effective data.
As shown by the \gatitos{} dataset \citep{Jones2023}, training data consisting of individually translated tokens without context provides large benefits to translation quality for low-resource languages at the lowest cost.
However, gains quickly saturate, as single tokens are not very expressive.
Sentence-level data is better for a model once token-level data saturates, but it has much more inherent redundancy; and document-level data is even more effective...and more redundant.
A resource like \gatitos{} is a useful first step for the languages it covers, but without sentence- or document-level resources, there is a limit to how effective translation models can be.
% However, even isolated sentences lack many important linguistic features, like anaphora and pro-drop, ultimately necessitating document-level data, which is both the most expressive and the most redundant. 

In this work, we release the \smol{} dataset, which provides professionally translated sentence- and document-level data for \nsmolnonenglishlangs{} LRLs, supplementing the token-level translations from \gatitos{}. \smol{} contains two sub-datasets:

\begin{itemize}
    \item \textbf{\smolseal{}}: 863 English sentences covering 5.5k of the most common English tokens,\footnote{In this paper, `token' refers to logical/linguistic words, not subword tokens from a model's vocabulary.}
    professionally translated into \nsmolsealnonenglishlangs{} languages.
    \item \textbf{\smoldog{}} \nsmoldogdocs{} English documents covering a wide range of topics, domains, and tokens, generated by a LLM and professionally translated into \nsmoldognonenglishlangs{} languages.
    
\end{itemize}

We demonstrate the utility of these data for finetuning and prompting LLMs for translation, and also provide factuality annotations for all documents.

\section{Related work}


There are several training datasets that comprise parallel data for Low-Resource Languages (LRLs), which we operationally define as any language beyond the first 100 or so supported by most traditional crawls and MT providers. 
The \nllbseed{} dataset is a sentence-level training set consisting of 6k sentences selected from English Wikipedia and professionally translated into 39 LRLs \cite{Nllb2022}.  
The \gatitos{} dataset \cite{Jones2023} represents another approach, consisting of a 4000-entry lexicon, translated into 170 LRLs.
There are also several professionally-translated evaluation sets, namely \floresone{} and \florestwo{} \cite{Goyal2022, Nllb2022}, and \ntrex{} \citep{ntrex}.
 
While highly multilingual, professionally translated training data is rare, 
there is a growing number of bottom-up community datasources organized through research collectives like Masakhane \citep{masakhane}, Turkish Interlingua \citep{mirzakhalov2021large, mirzakhalov2021evaluating}, and GhanaNLP~\citep{ghananlp};
and conferences and workshops like AfricaNLP,
AmericasNLP~\citep{mager-etal-2021-findings} and ArabNLP.
These datasets are usually generated by researchers fluent in the languages, and are therefore especially high quality.
In addition to providing datasets, such efforts frequently also provide models and baselines, or even public interfaces, like the Khaya Translator Web App\footnote{\url{https://ghananlp.org/project/translator-webapp/}} by GhanaNLP for West African languages, and the lesan.ai\footnote{\url{https://lesan.ai/translate}} translation website for Ethiopian languages.

Participation is especially strong from the African continent, including corpora and models for pan East-African languages \citep{babirye2022building}, languages from the Horn of Africa \citep{hornmt}, Ethiopian languages \citep{teferra-abate-etal-2018-parallel,gezmu2021extended}, Ugandan languages \citep{akera2022machine}, Emakhuwa \citep{felermino2021towards},  South-African languages \citep{eiselen-puttkammer-2014-developing}, Setswana and Sepedi \citep{marivate-etal-2020-investigating}, Yorùbá \citep{adelani-etal-2021-effect,adelani2021menyo},  Oshiwambo \citep{nekoto2022participatory}, Igbo \citep{ezeani2020igbo},
Zulu~\citep{rooweither_mabuya_2021_5035171},
Twi \citep{azunre2021english}, Gbe \citep{hacheme2021english2gbe}, Bambara \citep{tapo2021domain}, and Fon \citep{emezue-dossou-2020-ffr}. Outside of Africa, corpora have been created for languages of the Americas, including for four indigenous languages of Peru in \citet{bustamante-etal-2020-data}, the numerous results on the largely South- and Central American languages from the first AmericasNLP conference \citep{mager-etal-2021-findings}, and the Inuktitut language of Canada \citep{joanis-etal-2020-nunavut}. Datasets for lower-resourced languages of India have also sprung up, including the 13-language PMIndia \citep{haddow2020pmindia}, and datasets focused on languages of the Northeast like Mizo \citep{thihlum2020mizo}, Khasi \citep{laskar-etal-2021-enkhcorp1} and Assamese \citep{laskar-etal-2020-enascorp1}. Finally, a variety of such datasets and models are available for public use on HuggingFace\footnote{\url{https://huggingface.co/datasets?multilinguality=multilinguality:translation&task_categories=task_categories:translation&sort=downloads}} or Zenodo\footnote{\url{https://zenodo.org/communities/africanlp/}}.


In addition to professionally translated data, there are also several web-crawled datasets for LRLs, including
\textsc{MadLad} \citep{Kudugunta2023},
OSCAR \citep{oscar},
Glot500-C \citep{imanigooghari-etal-2023-glot500}, 
% NLLB \citep{Nllb2022}, 
and the Bloom library \citep{leong-etal-2022-bloom}.


\section{Methods for getting text to translate}
\label{sec:methods}
Translation requires significant investment, so great care needs be put into choosing good/effective sentences to translate.
This problem is especially tricky because one can't iterate on the method: once text is translated, the budget is exhausted and one can't simply select a new source text. It is possible to run ablations from high resource languages, as described in the next section, but it is questionable how much such approaches can actually generalize to true Low-Resource Languages.

For both \smoldog{} and \smolseal{} (as with \gatitos{}),
selection/generation of source text is done in English.
This has obvious biases, including the danger of translationese output, a scarsity of translators fluent in English, and a lack of culturally specific domains, but also has large advantages---most notably better scaling for quality control, and the ability to do Researcher in the Loop.



\subsection{\smolseal{}: Token Set Cover}
\label{sec:tsc}
Our basic motivation for creating \smolseal{} was to help models overcome vocabulary issues, which are common for the lowest-resource languages \citep{bapna2022building}.
the alligator problem \cite{DeanonAlligators2025}.
Therefore, we frame this as a set-cover problem, and pick the smallest set of sentences that cover the largest set of target tokens. The tokens we chose to cover
(the \textit{target set}) 
were the English side of \gatitos{}, as well as the most common 2,500 tokens from an English web crawl. Set cover is NP-hard, so we approximate it with a greedy algorithm that iteratively picks the sentence with the highest \textit{coverage percent}, defined as the percentage of its tokens that are in the target set. 


\subsubsection{Preliminary work on Token Set-Cover}
\label{sec:internship}

\begin{table}[]
    \centering
    \small
    \begin{tabular}{lc}
    \hline
    \hline 
    \textbf{Method} & \textbf{ChrF} \\
    \hline
    
    Random &  30.5\\
    Token set-cover & \textbf{31.7}\\
    N-gram DWD & 30.0\\
    Embedding DWD & 27.5 \\
    \hline
    \end{tabular}
    \caption{
    Held-out \chrf{} for data selection approaches
    }
    \label{tab:selectioneval}
\end{table}
% \end{wraptable}


To evaluate the token set-cover approach, we started by selecting data from existing web-scraped parallel data.
We pretrain a multilingual NMT model on parallel data from 294 language pairs from \madlad{} \cite{Kudugunta2023},
%consisting of 147 languages,
with nine languages held out to simulate LRLs.
% To test our method for dataset selection, w
We fine-tune this model on sets of existing parallel data in each of the held-out languages, and evaluate on \florestwo{}. Details
on the experimental set-up can be seen 
in Appendix \ref{app:selectioneval}.

In addition to Greedy Token Set-Cover, we explore two methods that balance data diversity and data quality.
First, we implement \citet{ambati2011}'s `density-weighted diversity' (DWD) metric, which is an n-gram based metric for diversity and quality.
Second, we implement an embedding-based version of DWD, which takes the weighted harmonic mean of perplexity under the Palm 2 model \cite{anil2023} (proxy for quality), and embedding distance on mBERT sentence embeddings (proxy for diversity).
We apply both methods to the English side of the parallel data only, to simulate the case where we don't yet have LRL translations.
As a baseline, we randomly select sentences.


Table \ref{tab:selectioneval} shows results after finetuning. Greedy token set-cover performs the best, with diversity-based metrics actively hurting performance.


\subsubsection{Researcher in the Loop (RITL)}
\label{ritl}

Despite its success in the ablation, Greedy Token Set Cover had several problems when we scaled it to select from among all the English sentences of CommonCrawl. Firstly, it is maximized by honeypots, or nonsense strings dense in content words (Appendix Table \ref{tab:badgreedy});
and secondly, it biases towards short sentences,
causing length distribution artifacts


These are tricky problems to solve in the general case. However, a dataset like \smol{} is small enough to manually inspect. Therefore we develop 
\textit{Researcher in the Loop Greedy Set-Cover} (Algorithm \ref{app:ritl}), where the domain expert (the researcher) can look at and edit each individual sentence.\footnote{This work was conducted before the advent of LLMs. Today, this could be simplified using LLMs as autoraters.}
The result of this process is \smolseal{}, a set which uses 863 sentences to cover 5519 unique tokens. Qualitatively, \smolseal{} consists of complex sentences with wide vocabulary coverage; quantitative metrics are explored in Appendix \ref{app:smolsealstats}. 


\begin{algorithm}
% \footnotesize{}
\small
\caption{Researcher in Loop Greedy Set Cover}
\begin{algorithmic}
% \Procedure{GetSubset}{$s$}
\State Res $\gets$ ...  \Comment{Sentence reservoir, e.g. CommonCrawl}
\State Toks $\gets$ ...  \Comment{Tokens to Cover, e.g. GATITOS}
\State Cov $\gets$ \{\} \Comment{Set-cover, aka output of this algorithm}

\While{not ToCover.empty()}
%  \State ScoreAllSentences(Reservoir)
 \State batch $\gets$ TopScoringSentences(Res, Toks , k)
 \State chosen $\gets$ ResearchersChoice(batch)
 \State chosen $\gets$ LetResearcherEdit(chosen)
  \State Cov.add(chosen)
  \State RemoveCoveredToks(Toks , chosen)
 \State Res $\gets$ LetResearcherDiscardSentences(Res)
  \State Res.remove(chosen)
\EndWhile
\State return Cov
% \EndProcedure


\end{algorithmic}
\label{alg:ritlgsc}
\end{algorithm}


\label{smolseal:methods}



\subsection{\smoldog{}: LLMs with prompt mesh}
\label{smoldog:methods}
\smoldog{}
% , created about a year after \smolseal{}, 
follows a different and complementary approach. Whereas \smolseal{} consists of a small set of \textit{sentences} that are \textit{selected} from natural text, are \textit{complex}, and cover many \textit{tokens}; \smoldog{} instead consists of \textit{documents} that are \textit{generated} and are  \textit{simpler},
% (i.e., low perplexity), 
but cover many \textit{topics}. It should be noted that the token-coverage approach described above failed resoundingly for longer documents, as the power of the honeypots was magnified.
 
To generate \smoldog{}, we used a collection of templates to create a few thousand diverse prompts with a wide range of topics, domains, words, tenses, grammatical cases, and registers (e.g. formal, informal).
Appendix \ref{app:smoldogprompts} gives details and examples.




\paragraph{Corpus Diversity Ranking for \smoldog{}}
\label{sec:corpusdiversity}
Document-by-document evaluation as described above does not help one understand \textit{corpus diversity} --- for example, if an almost identical document appears twice, only one of them should be included.
Therefore, we rank all candidates by how much new information they add to the corpus, by
iteratively finding the document contributing the least new information and removing it, yielding a ranking of all documents.
Our criterion for ``new information'' was the average 9-gram IDF score of a document, minus the fourth moment \bread{} score \citep{bread}, to down-weight internally repetitive documents.

\paragraph{Language Tiers for \smoldog{}}
We wanted to translate text data for languages with more speakers. We break the languages into the five different groups/tiers seen in Appendix Table \ref{tab:smoldogtiers}, and for each tier translate the top N documents as ranked by corpus diversity. The smallest tier has 66 documents, and the largest has 584.

\subsection{Non-English-centric translations}
For \smoldog{}, we additionally collected data for four non-English-centric language pairs, namely from each of the East African languages of Amharic (\texttt{am}) and Swahili (\texttt{sw}) to each of regionally relevant languages Standard Arabic (\texttt{ar}) and Mandarin Chinese (\texttt{zh}). Including the reversed versions of these, this yields 8 total non-English-centric language pairs. Because of the difficulties of choosing/generating good source material in these languages, we used the human translations from English as the source text. However, due to the lack of appropriate evaluation sets, it is difficult to know the value-add of this data over datasets pivoted through English.

\section{Data collection and verification}
The translation provider we contracted has worked with us for many years, and has a pre-existing relationship with professional translators for all languages in the \smol{} datasets.
We checked the delivery for duplicate translations, anomalous source/target length ratios, and similarity with Google Translate (which translators were instructed not to use). Very few languages were flagged this way. Manual inspection turned up several issues with nonunicode fonts (e.g. {\^ o} for \openo ), especially for West African languages, and nonstandard orthography for Santali (Latin script). These issues were then fixed. Following this, we ran \textsc{FunLangID} \citep{funlangid} on all segments. 95\% of languages were identified as correct, and of the remaining 8, confusion was only with dialects (\texttt{bm/dyu, kg/ktu, efi/ibb, ayl/arz}). The only major check missing is for fluency, which is hard to measure without either 1) trusted native speakers outside of the translation agency, or 2) trusted language models, neither of which exist for all \smol{} languages. 


\begin{table*}[t]
\centering
\small
\begin{tabular}{l|rrrr:rrr}
% \hline
\hline
set & N langs & ex  & tok & char  & ex/LP  & tok/LP & char/LP     \\
\hline
\gatitos{} & 176 & 693k & 784k & 4.6M & 3.9k & 4.5k & 26k \\
\smolseal{} & 81 & 70k & 994k & 6.1M & 863 & 12k & 75k \\
\smoldog{} & 100 & 27k & 5.1M & 28M & 263 & 50k & 278k \\
\hdashline
\smolboth{} & 115 & 97k & 6.1M & 34M & 827 & 52k & 294k \\
\hline
\end{tabular}
\caption{Statistics for the whole data set (left bloc) and per language-pair (LP) (right bloc) on the two \smol{} datasets and their predecessor \gatitos{}
in number of examples, tokens, and characters. The \texttt{N langs} column counts translated languages only, not the source languages of English, Swahili, and Amharic.
}
\label{tab:data}
\end{table*}





\section{Finetuning}
To get a general understanding of how helpful these data are, we fine-tune the Gemini 2.0 Flash model \citep{team2024gemini,team2024gemini2} on these data. It is important to note that the experimental set-up in this work is not optimized, and simply evaluates the ``out of the box'' improvement of simplistic finetuning with no bells and whistles. The \chrf{} scores could certainly be improved with e.g. a smaller set of languages, mixing in pre-training data, using larger base models, etc.. 

\begin{table*}[htp]
\centering
\small
\begin{tabular}{l|rr|rr|rr}
% \hline
\hline
\chrf{} on LPs in... & \multicolumn{2}{c|}{\smolseal{} (80 LP)} & \multicolumn{2}{c|}{\smoldog{}\textsc{Sp.} (73 LP)} & \multicolumn{2}{c}{Overlap (38 LP)}\\
Model                       & 0-shot       & 10-shot      & 0-shot       & 10-shot      & 0-shot       & 10-shot      \\
\hline
\textsc{G. Translate}       & -            & -            & -            & -            & \textbf{43.2}         & -           \\
\textsc{NLLB-54b}           & -            & -            & -            & -            & 40.0         & -           \\
\textsc{Claude 3.5 Son.}    & 37.5         & \textbf{39.7}         & 38.3         & \textbf{40.9}         & 41.0         & 42.8        \\
\textsc{GPT-4o}             & 29.9         & 34.1         & 31.8         & 36.3         & 35.4         & 38.5        \\
\textsc{Gemini 2.0 Pro}     & \textbf{38.9}         & 38.9         & \textbf{39.9}         & 40.3       & 42.6         & 42.2        \\
\hdashline
\textsc{Gemini 2.0 Flash}   & 35.6         & 37.7         & 36.9         & 39.3         & 40.2         & 41.3        \\
\textsc{+ \smolseal{} }     & 38.3         & 38.3         & 38.8         & 38.8         & 40.6         & 40.6        \\
\textsc{+ \smoldog{} }      & 35.3         & 35.3         & 39.5         & 39.5         & 41.2         & 41.2        \\
\textsc{+ \textsc{Both} }   & 38.9         & 38.9         & 40.5         & 40.5         & 41.8         & 41.8        \\
\textsc{+ \textsc{BothG} }  & \textbf{39.4}         & \textbf{39.3}         & \textbf{41.0}         & \textbf{40.9 }        & \textbf{42.1}         & \textbf{42.2 }       \\
\hdashline
\textsc{$\Delta_{FT}$}      & +3.8         & +1.6         & +4.1         & +1.6         & +1.9         & +0.9        \\
\hline
\end{tabular}
\caption{Finetuning Gemini 2.0 Flash on the \smol{} datasets. Results are shown for three subsets of language pairs. The first two are simply the sets of languages covered by each of the two \smol{} sets. The \textsc{Overlap} subset is the intersection of language pairs in both \smol{} datasets AND those supported by closed-domain NMT models. 
}
\label{tab:main-results}
\end{table*}

\subsection{Evaluation}
 Since so many language pairs are covered, we evaluate on a combination of all available evaluation sets, namely \florestwo{} \citep{Nllb2022}, \ntrex{} \cite{Federmann2022,Barrault2019}, 
and \gatones{} \citep{Jones2023}. % For non-anon version
Since no reliable embedding models exist for these languages, we use \chrf{} with NFKC unicode normalization as our metric.
% For ten-shot decoding, exemplars were selected from both splits of \smol{} using \chrf{}-counterweighted RAG (Appendix \ref{sec:scrag}). 


\paragraph{N-shot: \chrf{}-counterweighted RAG}
\label{sec:scrag}
To have a strong baseline for N-shot results, we adopt a RAG-based approach that resembles the greedy set-cover algorithm to choose the few-shot exemplars. For each sentence in the eval set, we want the best coverage of the source sentence n-grams as possible, with the least redundancy among exemplars. Therefore, we iteratively choose the exemplar whose source side has the minimum \chrf{} to the eval source. However, when counting the true positives in the \chrf{} calculation, we weight the count of each ngram $n_i$ by $(1+c_i)^{-\alpha}$, where $c_i \in [0, \infty]$ is the number of times $n_i$ has been seen among the exemplars chosen so far, and $\alpha$ is a parameter to control how close this algorithm is to ngram set-cover. We use $\alpha=2$. The set of exemplars we choose from is the concatenation of \smolseal{} and \smoldogsent{}.

% The purpose of developing this method was to have a version of RAG that works well enough for a baseline in a dataset paper. It uses ngram-based \chrf{} distance because no embedding models exist for these langauges, so methods like CITE are not applicable. As this is not a paper on RAG methods, we did not optimize this method beyond eyeballing the sentences.


\subsection{Finetuning Setup and Results}
We finetuned Gemini 2.0 Flash for 50 epochs on \smoldog{}, \smolseal{}, a combination of the two (\smolboth{}), and their combination plus \gatitos{} (\smolbothg{}). To simplify finetuning, we split \smoldog{} into sentence pairs (\smoldogsent{}). We use a minimalist prompt (Appendix \ref{app:inferenceprompts}).

Results can be seen in Table \ref{tab:main-results}. Finetuning on \smolseal{} gives an average gain of +2.7 \chrf{}, and \smoldogsent{} give the exact same gain on its languages. Mixing the two leads to a gain of +3.3 to +3.6 \chrf{}, and adding in \gatitos{} bumps it up to +3.6 to +4.1 \chrf{}, passing all other 0-shot baselines except Google Translate (on the languages it supports).


The 10-shot RAG results on the un-tuned model are very close to the finetuned 0-shot results, and the finetuned models show no benefit from multi-shot decoding, suggesting that these are two different ways of giving the same information---inference-time versus training time. 
The 10-shot random results (not included in table) were much lower.

Figure \ref{fig:epochs} shows the learning curve on a development subset of 37 languages. 
Although it may be surprising that so many epochs are needed before convergence, we found that further increasing learning rate led to overfitting. The sharp drop near the beginning suggests a domain mismatch between pretraining and finetuning, and suggests that the same data could be used much more effectively with a better training set-up than explored here.
% As our RAG draws from both \smol{} splits, the 10-shot results only saturate for the \smolboth{} line, not the other two lines, where there may be complementary information from the other split.%, and learning rate schedules did not help enough to merit the complexity.

\begin{figure}[t!]
\begin{center}
\includegraphics[scale=0.36]{learningcurve.png}
\caption{Training curves (\chrf{}) for finetuned models on a subset of 37 \enxx{} language pairs, trained on \smoldog{}, \smolseal{}, and their combination \smolboth{}.
}
\label{fig:epochs}
\end{center}
\end{figure}

\subsection{The Problem with \xxen{} training}
\label{sec:mwp}
Our initial experiments used all data for both \enxx{} and \xxen{}.
However, the models lost performance on all tasks.
The root cause turned out to be the multiway-parallel data with English targets. 
Large language models are especially susceptible to repetition in data \citep{Lee2022}, and with 115 language pairs, for every one epoch over the data, the model saw about 115 epochs for each individual target sentence. Therefore, it wildly overfit and lost performance on all language pairs. 
Mitigating such overfitting is an important research direction to pursue, since many promising datasets are multiway parallel, e.g. \floresone{} \citep{Goyal2022}, \florestwo{} \citep{Nllb2022}, \ntrex{} \cite{Federmann2022,Barrault2019}, and others. 
However, this is out of scope for the present paper,
so we restrict our experiments to \enxx{}.

Seeing the same \textit{source} many times likely also has deleterious effects and should also be studied; but these effects, if they exist, are small enough that we were still able to see net gains.



\section{Factuality Review}

Since \smoldog{} contains LLM-generated sources, they contain some factual inaccuracies.
We therefore do a full human audit and assign factuality codes to each document. 
Each of the \nsmoldogdocs{} documents was rated by three raters. Each rating is accompanied by a detailed explanation, including sources cited. Inter-annotator agreement was high, with Cohen's $\kappa$ between each pair of raters between 0.82 and 0.87. The error code distribution can be seen in Fig. \ref{fig:fact-pie}. Appendix \ref{app:fact} gives details on the rubric.


% \begin{figure}
% \begin{tikzpicture} [scale=0.80]
% \label{fig:pie}
% \pie{ 
%  46.6/NA,
%  39.1/No Issues,
%  9.0/Minor Issue(s),
%  2.6/Clear Issue(s),
%  2.7/Not Sure}
 
% \end{tikzpicture}
%  \caption{Distribution of factuality ratings in \smoldog{}.}
%  \label{fig:fact-pie}
% \end{figure}


\begin{figure}[t!]
\begin{center}
\includegraphics[scale=0.19]{pie2.png}
\caption{Distribution of factuality ratings in \smoldog{}.
}
\label{fig:fact-pie}
\end{center}
\end{figure}


All ratings and rationales are made available. In addition, each datum in \smoldog{} is given with a simple \texttt{factuality} annotation, which has the value \texttt{has\_errors} if any one of the ratings was any of \texttt{Minor Issues} or \texttt{Clear Issues}, and \texttt{ok} otherwise.
For some use-cases, like question-answering, practitioners may want to filter out nonfactual data; for others, like translation, one may not be troubled by factual errors. In addition to filtering, this also provides the first factuality dataset for most of these languages.


\section{Conclusion}
We have open-sourced the \smol{} dataset, a professionally-translated dataset covering \nsmolnonenglishlangs{} low resource languages and targeting the tasks of translation and factuality. It comprises \smoldog{} and \smolseal{}, two training datasets with the complementary strengths of sentence selection (complex, and high token coverage) and document generation (contextual, varied domains, simpler sentences) respectively. 
We demonstrate that finetuning Gemini 2.0 Flash on these yields to substantial improvements in translation quality.
\smol{} joins a growing body of resources to support underserved languages in the age of AI.

\section{Limitations}
The \smol{} data would benefit from a more thorough review, audit, and correction from community members outside of the translators who created it.
Future work on \smol{}-like datasets should also focus on non-English source text that is not only maximally authentic in the given language, but also covers the topics and concepts most relevant to those languages. This approach is more difficult and would require significant work and review to do correctly.
Finally, more research is needed to understand and prevent the overfitting that comes with multi-way parallel data.

% \section{Acknowledgements}
% Thank you to Julia Kreutzer, who did early ideation on this project and coined the term Minimum Viable Dataset.

\section{Ethical implications and Potential Risks}

Our released \smol{} dataset will assist in development and evaluation of machine translation and multilingual AI capabilities for low-resource languages. Although we consider this a net benefit for society, one potential risk is that users may overestimate the quality of machine translations in these languages. Another potential risk is increasing the proliferation of AI-generated content in the target languages on the open web, which may pose issues for future development of web-crawled datasets in these languages.
Furthermore, although we have surveyed and talked with native speakers of all \smol{} languages and found only enthusiasm for technological support of their languages, communities are not monoliths and some community members may prefer that technology not support their language.

\newpage
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology}
\newpage
\appendix


\section{\smolseal{} details}
\label{app:smolseal}


\subsection{Evaluating the \smolseal{} selection process}
\label{app:selectioneval} 

In Section \ref{sec:internship}, we describe experiments used to validate the selection process for \smolseal{}.
We train a backbone MT system is pretrained on the \madlad{} dataset \citep{Kudugunta2023}.
The following languages are held out of the training data to be used for fine-tuning experiments: Catalan, Icelandic, Marathi, Turkish, Maltese, Xhosa, Tamil, Basque, and Tajik. 
The model itself is a 1B parameter encoder-decoder Transformer and is trained from scratch on the \madlad{} data.
Each of the candidate data selection methods is used to select data from the held-out languages, and then each candidate set is used in turn to finetune the backbone model. The results of this finetune step are reported in Table \ref{tab:selectioneval}, where the set-cover approach is shown to be most effective.

\subsection{Notes on Researcher in the Loop}
\label{app:ritl}
Researcher In the Loop extends the greedy set cover approach thusly: rather than always picking the highest-scoring sentence, we iteratively show the researcher a batch of the 20 highest scoring sentences according to several scores, and let the researcher pick and optionally edit each sentence at each iteration. At each iteration, the researcher may also remove any number of this batch's sentences from the reservoir.
Allowing the researcher to see and edit the sentences allows ensures that the sentences are of high-quality. To deal with the length bias issue, we showed not only sentences that maximize coverage percent, but also that maximize heuristics that weighted the coverage with the number of new tokens hit, like \texttt{ log(coverage\_percent)*n\_hits }.



As described in the paper text, this approach is designed to combat issues such as honeypot sentences. Example Honeypot sentences can be seen in Table \ref{tab:badgreedy}

\begin{table}[h]
\scriptsize
\centering
\begin{tabular}{p{0.45\textwidth}} % Adjust width as needed
\toprule
\textbf{Sentence} \\
\midrule
Individual determine can get prolonged, reduce along with attractive. \\
\midrule
Sell hand mood situation connect proper decision today spread true. \\
\midrule
Demand indeed off forget act special well treat sometimes notice. \\
\midrule
Agree board book oh trust by attractive supply deal together. \\
\midrule
Picture exactly could ability impact advance then same admire across. \\
\midrule
One physically courage both information language issue laugh common. \\
\bottomrule
\end{tabular}
\caption{Honeypot Sentences for Greedy Selection: CommonCrawl has many sentences packed with content words but with no clear semantics or grammar.}
\label{tab:badgreedy}
\end{table}





\subsection{More detailed corpus statistics on \smolseal{}}
\label{app:smolsealstats}


To measure ``Bang for our Buck'' we define the \textit{excess-token ratio} $\xi$ as the number distinct tokens in the set cover divided by the number of target tokens, and use it along with the coverage percent to understand the \smolseal{} dataset. Table \ref{tab:smolseal} compares corpus statistics of \smolseal{} to four other corpora. \texttt{sametoks} picks a random set of sentences from CommonCrawl until it has the same number of tokens as \smolseal{}; this only covers 50\% of the target tokens and has an excess-token-ratio $\xi$ ratio of 3.3, much worse than \smolseal{}'s value of 2.3. The \texttt{samecov} baseline randomly picks common-crawl sentences until it has the same token coverage as \smolseal{}, which necessitates a 67x larger set of sentences, and a correspondingly bloated excess token coverage ratio of 23.1. As a further reference we compare tiers 1 (largest) and 5 (smallest; comparative size to \smolseal{}) of \smoldog{}. As expected of machine-generated text, they have a worse $\xi$ value, corresponding to a narrower spectrum of vocabulary used.

\begin{table*}[t]
\centering
\small
\begin{tabular}{llllrl}
\hline
\hline
set & N sent & toks  & types &   $\xi (\downarrow)$ & cov\%$(\uparrow)$     \\
\hline
\smolseal{} & 863 & 12k & 5.5k & 2.3 & 99.6\% \\
{\tt sametoks} & 863 & 12k & 3.8k & 3.3 & 50.4\% \\
{\tt samecov} & 57578 & 877k & 38k & 23.1 & 99.6\% \\
\hdashline
\smoldog{}-\textsc{t}1 & 6979 & 108k & 8.8k & 12.3 & 80.5\% \\
% \smoldog{}-\textsc{t}2 & 4729 & 72k & 7.6k & 9.5 & 75.0\% \\
% \smoldog{}-\textsc{t}3 & 3145 & 48k & 6.3k & 7.6 & 67.4\% \\
% \smoldog{}-\textsc{t}4 & 1604 & 24k & 4.3k & 5.6 & 54.6\% \\
\smoldog{}-\textsc{t}5 & 820 & 12k & 2.8k & 4.3 & 40.2\% \\


% \smolseal{} & 863 & 12419 & 5519 & 2.3 & 99.6\% \\
% {\tt sametoks} & 863 & 12419 & 3753 & 3.3 & 50.4\% \\
% {\tt samecov} & 57578 & 876982 & 37937 & 23.1 & 99.6\% \\
\hline
\end{tabular}
\caption{Corpus statistics of \smolseal{}, random selections of sentences from CommonCrawl, and tiers 1 and 5 of \smoldog{}. 
%Columns are the number of total tokens, unique tokens, 
% ratio $\xi$, and coverage of the target tokens.
}
\label{tab:smolseal}
\end{table*}

\section{\smoldog Details}




\subsection{\smoldog{} data tier details}
Per-tier statistics on the \smoldog{} dataset can be seen in Table \ref{tab:smoldogtiers}.


\begin{table*}[t]
\centering
\small
\begin{tabular}{l|rrrr:rrr}
\hline
\hline
set & N langs & ex  & tok & char  & ex/LP  & tok/LP & char/LP     \\
\hline
% \gatitos{} & 176 & 693k & 784k & 4.6M & 3.9k & 4.5k & 26k \\
% \smolseal{} & 81 & 70k & 994k & 6.1M & 863 & 12k & 75k \\
% \smoldog{} & 102 & 27k & 5.1M & 28M & 263 & 50k & 278k \\
% \hdashline
\smoldog{}-t1 & 5 & 2.9k & 537k & 3.0M & 584 & 107k & 604k \\
\smoldog{}-t2 & 31 & 14k & 2.7M & 15M & 450 & 88k & 495k \\
\smoldog{}-t3 & 24 & 6.7k & 1.2M & 6.8M & 280 & 50k & 281k \\
\smoldog{}-t4 & 8 & 1.0k & 184k & 1.0M & 126 & 23k & 128k \\
\smoldog{}-t5 & 34 & 2.2k & 401k & 2.2M & 66 & 12k & 65k \\
% \hdashline
all-\smol{} & 115 & 97k & 6.1M & 34M & 827 & 52k & 294k \\
\hline
\end{tabular}
\caption{Statistics on the languages in the individual tiers of \smoldog{}.}
\label{tab:smoldogtiers}
\end{table*}


\subsection{Details on \smoldog{} prompt creation}
\label{app:smoldogprompts}

To avoid biases from overly tempted prompts, we put in significant effort to make sure the prompts were all very different. Each prompt drew at random from the following elements:

\begin{itemize}
    \item random selection of English words to use in the response
    \item one of 600 manually created topics, e.g. "volcanic eruptions" or "A special tree"
    \item one of 50 tone/tense categories, e.g. "Please use the subjunctive mood.", "Use an effusive tone.""
    \item A style prompt, e.g. "You are the author R.K. Narayan." or "You are a mother talking to her son."
    \item A text modality, e.g. story/dialogue/essay
\end{itemize}

In addition to this, we added a few more sources of prompts:

\begin{itemize}
    \item Prompts based on urls, meant to simulate different web domains, like Wikipedia and reddit.
    \item Prompts based on continuing the sentences from \smolseal{}
    \item Prompts based on current events, history, and daily life in different countries
    \item Special effort was made to include dialogues (to get more spoken register) and recipes (unique domain that may also be important to translate).
\end{itemize}




For each prompt, we generated 8 responses (T=0.7). These were ranked by their simple token density (unique tokens over total tokens), and the top two were chosen for consideration. Using the Researcher-in-the-loop mentality (``measure twice cut once!"), we went over 1000+ responses by hand and scored/edited them. This was mainly to filter out questionable or boring responses. A typical paragraph scored as 0 would be LLM-speak like \textit{``X is a complex and multifaceted problem with no easy solution. Here are some suggestions. Keep in mind that there is no one-size-fits all solution, and ultimately, the choice is up to you. [...]''}.

Example prompts can be seen in Table \ref{tab:exampleprompts}.

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{p{12cm}}
\hline
\hline
\textbf{Example Prompts} \\ 
\hline
You are Ernest Hemingway. Write a dialogue about road rage. Use a didactic tone.  \\
\midrule
Write a 1-paragraph story concerning an Irish wake.  \\
\midrule
You are a teenager talking to his friend. Please carefully craft a 1-paragraph bit about an engineer who subsists off coffee. Try to include the words "confirmed", "move" and "above". \\
\midrule
Give a typical, yet interesting, example of something you would find on reddit. \\
\midrule
Please write a few paragraphs about challenges facing Ethiopia. \\
\midrule
Please write a long passage starting with 'Mum and Dad pause their debate when we hear this creepy clacking that sounds like hail falling.' \\
\midrule
Write a recipe for baking an almond cake. \\
\hline
\end{tabular}
\caption{Representative sample of prompts use to generate the documents for \smoldog{}}
\label{tab:exampleprompts}
\end{table*}

\subsection{\smoldog{} Errata}
\label{app:smoldogerrata}
\paragraph{Orthographies} Several languages use irregular orthographies. Most notable is Moor{\'e}/Mossi (\texttt{mos}), where different translators have used a variety of different conventions. After soliciting community feedback, we plan to release standardized versions to the data.

\paragraph{document selection} When collecting the data for \smoldog{} for the Indian languages, we mistakenly included a variety of documents that fell below the corpus diversity threshold described in Section \ref{smoldog:methods}.





\section{Prompts for Decoding}
\label{app:inferenceprompts}

For 0-shot prompting, we used the following, fairly wordy prompt, the SL and TL standing for the source and target language name, respectively:

\begin{verbatim}
You are an expert translator. I am
going to give you some example
pairs of text snippets where the
first is in ${SL} and
the second is a translation of
the first snippet into 
${TL}. The sentences
will be written
${SL}: <first sentence>
${TL}: <translated first sentence>
After the example pairs, I am
going to provide another sentence
in ${SL} and I want
you to translate it into
${TL}. Give only the
translation, and no extra 
commentary, formatting, or 
chattiness. Translate the text 
from ${SL} to ${TL}.
\end{verbatim}

For finetuned models, there is no need for such a wordy prompt, and indeed it only risks overfitting. Therefore, we used the following minimalist prompt:

\begin{verbatim}
Translate from ${SL} to ${TL}:
\end{verbatim}


\section{Factuality rubric}
\label{app:fact}
The full factuality rubric given to raters is presented in Table \ref{tab:fact-rubrix}.

\begin{table*}[t]
\centering
\small
\begin{tabular}{l|p{11cm}}
\hline
\hline
Rating & 	Definition of Rating \\ 
\hline
N/A - Not Applicable & 	True/False does not apply here. Most stories, dialogues, or fictional works would be considered N/A, unless they are promoting a falsehood about the real world. \\ 
Not Sure & 	Claims are made that may not be true, but you aren't sure. Choose this if it would take over 10 minutes to verify the factuality of the claim. \\ 
No Issues & 	All claims are factual and accurate. (It's OK if they are out-of-date, e.g. ``Barack Obama is the President of the USA'') \\ 
Minor Issue(s) & 	There are small inaccuracies. For instance, it may be broadly correct but frame something in a misleading way. \\
Clear Issue(s) & 	There are clear mistakes in factuality. \\ 
\hline
\end{tabular}
\caption{Factuality Rubric}
\label{tab:fact-rubrix}
\end{table*}



\section{Translations for NKo}
The initial delivery for the NKo language (\texttt{nqo}) had a wide variety of errors. We reached out to the authors from \citet{doumbouya2023machinetranslationnkotools}, who did a complete re-translation of the text.


\section{Full results}
\label{app:fullresults}

Full per-language results can be seen in Table \ref{tab:full-results}. Results are sorted by the $\Delta_{FT}$, which is the \chrf{} of the \smolboth{} model minus the \chrf{} of the finetuned \textsc{Both} model---in other words, how much the finetuning on \smol{} improved the baseline model.

\onecolumn 
{ \scriptsize  % Start \scriptsize here
\begin{longtable}{l|l|r|rrrrr|rr|rrrr}
% \centering


lang & cat & $\Delta_{FT}$ & G2F & +sS  & +sD & +sB  & +sG  & Cld & +RAG &  G2P & GPT4o & GTr & NLLB  \\
\hline  
\endhead
ee & \textsc{Both} & +36.1 & 3.0 & 37.7 & 37.5 & 39.1 & 39.2 & 37.8 & 40.0 & 39.5 & 7.5 & { \bf 42.7 } & 40.7  \\
kr & \textsc{Both} & +10.8 & 17.3 & 25.6 & 26.3 & 28.1 & 28.8 & 22.7 & 26.3 & 20.3 & 22.2 & { \bf 32.6 } & 31.0  \\
kg & \textsc{Both} & +9.2 & 34.9 & 46.9 & 35.6 & 44.1 & 43.2 & 43.2 & 47.0 & 37.8 & 29.1 & { \bf 50.2 } & 3.4  \\
bem & \textsc{Both} & +7.3 & 40.0 & 44.8 & 45.1 & 47.3 & 49.2 & 43.3 & 47.7 & 42.3 & 33.3 & { \bf 49.7 } & 41.8  \\
dyu & \textsc{Both} & +5.3 & 17.9 & 22.5 & 22.9 & 23.2 & 23.7 & 23.9 & { \bf 24.4 } & 21.0 & 4.5 & 22.4 & 12.5  \\
din & \textsc{Both} & +4.6 & 20.3 & 23.8 & 23.1 & 24.9 & 25.1 & 23.3 & 25.9 & 21.4 & 1.6 & 25.1 & { \bf 26.5 }  \\
st & \textsc{Both} & +4.5 & 49.9 & 54.0 & 55.3 & 54.4 & 55.0 & 49.4 & { \bf 57.0 } & 53.1 & 49.2 & 49.0 & 47.2  \\
luo & \textsc{Both} & +4.1 & 37.4 & 39.1 & 41.3 & 41.5 & 42.0 & 39.1 & { \bf 42.0 } & 39.6 & 36.1 & 41.3 & 39.5  \\
fon & \textsc{Both} & +3.6 & 21.3 & 24.3 & 24.1 & 24.9 & 25.3 & 20.4 & 23.7 & 23.8 & 1.9 & { \bf 25.9 } & 24.2  \\
bm & \textsc{Both} & +3.4 & 30.8 & 28.6 & 34.9 & 34.2 & 34.1 & 34.0 & { \bf 36.2 } & 33.9 & 9.0 & 35.7 & 32.2  \\
ak & \textsc{Both} & +2.6 & 35.5 & 36.1 & 37.6 & 38.1 & { \bf 38.2 } & 34.4 & 38.1 & 37.3 & 32.2 & 34.5 & 33.3  \\
ln & \textsc{Both} & +2.5 & 46.8 & 48.1 & 49.2 & 49.3 & { \bf 49.3 } & 44.6 & 48.3 & 46.5 & 45.2 & 46.4 & 45.7  \\
wo & \textsc{Both} & +1.1 & 30.3 & 30.0 & 30.8 & 31.4 & 31.6 & 31.4 & 32.2 & 30.7 & 29.8 & { \bf 36.2 } & 30.9  \\
tum & \textsc{Both} & +1.1 & 40.8 & 39.5 & 41.9 & 41.9 & 42.8 & 40.0 & 42.7 & 43.8 & 37.7 & { \bf 45.4 } & 36.2  \\
ff & \textsc{Both} & +0.9 & 25.0 & 24.4 & 25.7 & 25.9 & 26.5 & 25.7 & 26.1 & 25.2 & 2.5 & 25.9 & { \bf 27.1 }  \\
ti & \textsc{Both} & +0.8 & 24.2 & 24.3 & 24.6 & 25.0 & 25.7 & 25.0 & { \bf 26.2 } & 26.1 & 9.3 & 26.1 & 25.5  \\
yo & \textsc{Both} & +0.6 & 34.6 & 33.8 & 35.4 & 35.2 & 35.8 & 29.2 & { \bf 36.8 } & 26.7 & 27.6 & 21.3 & 32.6  \\
tn & \textsc{Both} & +0.2 & 52.5 & 50.8 & 51.1 & 52.7 & 53.2 & 50.1 & 51.7 & 53.3 & 36.8 & { \bf 55.6 } & 53.0  \\
am & \textsc{Both} & +0.1 & 34.0 & 33.2 & 33.1 & 34.1 & 33.5 & 31.6 & 32.6 & { \bf 35.8 } & 29.6 & 34.7 & 30.3  \\
ig & \textsc{Both} & -0.1 & 47.2 & 47.1 & 47.7 & 47.1 & 47.8 & 43.9 & 46.2 & { \bf 47.8 } & 46.2 & 47.6 & 46.6  \\
so & \textsc{Both} & -0.1 & 49.7 & 46.2 & 50.1 & 49.6 & 49.1 & 48.7 & 49.8 & 50.3 & { \bf 50.8 } & 50.6 & 48.6  \\
sn & \textsc{Both} & -0.5 & 50.5 & 48.3 & 49.9 & 50.0 & 50.3 & 46.8 & 48.8 & { \bf 51.8 } & 50.3 & 49.2 & 48.2  \\
ss & \textsc{Both} & -0.6 & 50.2 & 48.8 & 47.7 & 49.6 & 50.3 & 49.6 & 51.2 & 51.8 & 46.0 & { \bf 56.3 } & 48.1  \\
yue & \textsc{Both} & -0.7 & 26.8 & 25.8 & 25.6 & 26.1 & 26.1 & 28.2 & 28.2 & 27.5 & { \bf 31.6 } & 25.9 & 22.6  \\
om & \textsc{Both} & -0.8 & 40.1 & 38.0 & 39.6 & 39.3 & 39.4 & 39.0 & 40.2 & 41.3 & 38.4 & { \bf 41.4 } & 39.1  \\
lg & \textsc{Both} & -1.1 & 42.5 & 39.9 & 41.0 & 41.4 & 41.7 & 42.0 & 43.1 & 43.5 & 41.0 & { \bf 43.6 } & 41.1  \\
rw & \textsc{Both} & -1.4 & 45.2 & 43.1 & 43.1 & 43.8 & 43.8 & 43.2 & 44.0 & 45.1 & 44.7 & { \bf 48.8 } & 43.4  \\
sw & \textsc{Both} & -1.5 & 66.7 & 64.6 & 64.7 & 65.2 & 64.8 & 64.0 & 65.5 & { \bf 67.2 } & 66.5 & 65.3 & 60.5  \\
mg & \textsc{Both} & -1.9 & 52.8 & 48.9 & 51.1 & 50.9 & 51.5 & 52.4 & 52.5 & { \bf 53.3 } & 52.2 & 52.6 & 52.1  \\
zu & \textsc{Both} & -2.0 & 58.3 & 56.4 & 55.2 & 56.3 & 56.1 & 54.1 & 55.5 & { \bf 58.5 } & 57.5 & 57.6 & 57.6  \\
xh & \textsc{Both} & -2.3 & 53.9 & 49.7 & 51.4 & 51.6 & 51.8 & 51.6 & 52.3 & 53.9 & 53.7 & { \bf 54.8 } & 51.2  \\
nso & \textsc{Both} & -2.3 & 46.8 & 42.8 & 44.0 & 44.5 & 44.6 & 46.9 & 47.7 & { \bf 48.1 } & 46.9 & 47.6 & 45.5  \\
ber & \textsc{Both} & -3.2 & 25.3 & 20.6 & 23.0 & 22.1 & 21.9 & 28.5 & 25.2 & 31.1 & 2.8 & 21.0 & { \bf 32.4 }  \\
ha & \textsc{Both} & -3.4 & { \bf 54.5 } & 50.7 & 50.5 & 51.1 & 51.5 & 50.9 & 51.0 & 54.1 & 53.9 & 53.8 & 53.9  \\
ts & \textsc{Both} & -3.6 & 50.6 & 47.2 & 46.5 & 47.0 & 48.1 & 49.7 & 50.1 & 51.6 & 49.0 & { \bf 52.9 } & 51.3  \\
rn & \textsc{Both} & -3.9 & 44.5 & 39.3 & 40.7 & 40.6 & 40.9 & 43.1 & 43.4 & { \bf 46.2 } & 44.3 & 45.4 & 45.0  \\
af & \textsc{Both} & -4.2 & 71.9 & 68.8 & 67.0 & 67.7 & 68.3 & 71.7 & { \bf 72.5 } & 72.1 & 71.8 & 71.5 & 68.6  \\
ny & \textsc{Both} & -5.1 & 55.0 & 47.5 & 50.2 & 49.9 & 49.7 & 53.0 & 53.1 & 55.3 & 53.9 & { \bf 55.8 } & 50.3  \\
\hdashline 
trp & \textsc{SmolDoc} & +29.7 & 8.4 & 6.5 & 38.4 & 38.1 & { \bf 39.1 } & 24.7 & 35.8 & 27.2 & 20.3 & 35.9 & -  \\
mni-M. & \textsc{SmolSent} & +26.4 & 2.9 & 30.0 & 1.2 & 29.3 & 29.3 & 29.6 & 31.8 & 33.6 & 1.3 & { \bf 45.6 } & 0.8  \\
gaa & \textsc{Both} & +23.1 & 22.7 & 44.5 & 43.7 & 45.8 & 47.4 & 34.7 & 44.0 & 40.9 & 6.6 & { \bf 48.3 } & -  \\
dov & \textsc{Both} & +21.1 & 19.1 & 39.2 & 39.4 & 40.2 & 40.6 & 19.2 & 39.5 & 18.2 & 8.7 & { \bf 41.7 } & -  \\
ahr & \textsc{SmolDoc} & +17.8 & 24.2 & 31.8 & 41.5 & 42.0 & { \bf 42.8 } & 32.8 & 39.0 & 30.0 & 36.9 & - & -  \\
sus & \textsc{Both} & +17.8 & 11.3 & 28.3 & 27.0 & 29.1 & 30.3 & 26.1 & 29.4 & 20.7 & 5.6 & { \bf 34.6 } & -  \\
nqo & \textsc{Both} & +17.5 & 0.2 & 17.9 & 17.3 & 17.7 & 17.5 & 17.1 & 17.9 & 17.2 & 1.1 & { \bf 19.1 } & -  \\
alz & \textsc{Both} & +15.5 & 16.9 & 31.5 & 31.7 & 32.4 & 33.4 & 25.3 & 30.6 & 26.9 & 8.9 & { \bf 36.6 } & -  \\
lu & \textsc{Both} & +13.8 & 27.6 & 37.5 & 39.0 & 41.4 & { \bf 42.2 } & 27.2 & 37.0 & 34.8 & 21.9 & - & -  \\
cgg & \textsc{Both} & +12.2 & 32.6 & 40.5 & 40.5 & 44.8 & { \bf 44.8 } & 37.3 & 42.2 & 37.7 & 28.3 & 42.8 & -  \\
ks-D. & \textsc{SmolSent} & +11.7 & 14.9 & 26.6 & 18.7 & 26.6 & { \bf 27.7 } & 23.8 & 27.1 & 21.5 & 19.2 & - & 21.1  \\
brx & \textsc{SmolSent} & +11.6 & 24.3 & 36.0 & 0.4 & 35.9 & { \bf 37.0 } & 30.8 & 35.9 & 36.2 & 5.2 & - & -  \\
mag & \textsc{SmolDoc} & +8.1 & 47.0 & 45.3 & 55.4 & 55.1 & 54.7 & 47.3 & 51.8 & 47.4 & 48.7 & - & { \bf 59.4 }  \\
ki & \textsc{Both} & +7.7 & 32.6 & 38.2 & 38.8 & 40.3 & 40.6 & 35.9 & { \bf 42.0 } & 39.1 & 10.5 & - & 38.4  \\
aa & \textsc{Both} & +7.4 & 14.2 & 20.1 & 20.3 & 21.6 & 21.8 & 18.9 & 20.6 & 18.9 & 5.6 & { \bf 23.1 } & -  \\
ks & \textsc{SmolSent} & +7.3 & 22.1 & 29.4 & 0.4 & 29.4 & 29.7 & 28.0 & 30.4 & 30.5 & 26.3 & - & { \bf 36.7 }  \\
nr & \textsc{Both} & +7.0 & 48.9 & 54.0 & 50.2 & 55.9 & 57.5 & 48.0 & 53.6 & 51.2 & 45.5 & { \bf 59.5 } & -  \\
doi & \textsc{SmolDoc} & +6.6 & 34.3 & 28.1 & 40.8 & 40.9 & { \bf 41.3 } & 35.9 & 39.5 & 38.2 & 27.7 & 40.4 & -  \\
sat-L. & \textsc{SmolSent} & +6.4 & 12.8 & 19.5 & 15.9 & 19.2 & 20.8 & { \bf 25.3 } & 22.5 & 22.7 & 21.3 & 22.7 & -  \\
mfe & \textsc{Both} & +5.3 & 59.5 & 65.4 & 61.7 & 64.8 & 66.9 & 59.6 & 65.0 & 59.8 & 59.5 & { \bf 67.5 } & -  \\
ach & \textsc{Both} & +5.2 & 33.2 & 43.1 & 32.3 & 38.4 & 39.2 & 32.4 & 37.3 & 35.1 & 23.8 & { \bf 43.2 } & -  \\
ayl & \textsc{Both} & +4.5 & 47.3 & 51.7 & 51.3 & 51.8 & { \bf 53.9 } & 45.3 & 48.9 & 46.3 & 48.6 & - & -  \\
ber-L. & \textsc{Both} & +4.2 & 26.1 & 27.9 & 29.7 & 30.3 & 30.9 & 27.6 & 32.7 & 32.1 & 21.2 & { \bf 34.7 } & -  \\
apd-S. & \textsc{Both} & +3.6 & 42.3 & { \bf 50.2 } & 43.8 & 45.9 & 47.1 & 43.2 & 45.0 & 42.5 & 45.6 & - & -  \\
ve & \textsc{Both} & +3.5 & 47.9 & 50.0 & 49.5 & 51.4 & 52.2 & 50.2 & 53.1 & 52.7 & 43.9 & { \bf 56.8 } & -  \\
kri & \textsc{Both} & +2.8 & 31.5 & 34.2 & 32.1 & 34.3 & 34.7 & 34.5 & 33.5 & 30.7 & 34.9 & { \bf 34.9 } & -  \\
tiv & \textsc{Both} & +2.5 & 23.8 & 25.7 & 25.5 & 26.3 & { \bf 26.5 } & 22.3 & 24.2 & 24.5 & 1.5 & 25.2 & -  \\
gn & \textsc{SmolSent} & +2.3 & 37.4 & 37.8 & 29.4 & { \bf 39.7 } & 38.4 & 36.0 & 38.0 & 36.4 & 35.6 & 38.4 & 38.5  \\
mos & \textsc{Both} & +2.3 & 18.2 & 20.9 & 19.2 & 20.5 & 21.1 & 24.3 & { \bf 25.0 } & 20.9 & 1.3 & - & 23.8  \\
ar-M. & \textsc{Both} & +0.1 & 40.1 & 41.8 & 38.7 & 40.2 & 40.9 & 40.5 & 40.8 & 40.4 & 41.0 & - & { \bf 43.0 }  \\
arz & \textsc{Both} & -0.1 & 48.6 & 46.1 & 48.6 & 48.5 & 47.8 & 49.6 & { \bf 50.3 } & 48.8 & 49.7 & - & 49.6  \\
kl & \textsc{SmolSent} & -0.3 & 40.6 & 39.7 & 29.0 & 40.3 & 41.2 & 42.2 & { \bf 43.1 } & 41.2 & 41.6 & 42.9 & -  \\
sa & \textsc{SmolSent} & -0.3 & 33.0 & 32.9 & 27.4 & 32.7 & 33.4 & 31.8 & 33.0 & 32.1 & 32.0 & { \bf 35.2 } & 29.0  \\
ay & \textsc{SmolSent} & -0.5 & 32.7 & 31.8 & 23.2 & 32.2 & 32.4 & 33.4 & 33.2 & 32.9 & 30.0 & { \bf 34.7 } & 31.7  \\
efi & \textsc{Both} & -0.6 & 14.7 & 14.5 & 14.3 & 14.1 & 14.2 & { \bf 15.3 } & 15.1 & 15.0 & 2.2 & - & -  \\
bci & \textsc{Both} & -0.7 & 23.2 & 22.2 & 22.4 & 22.5 & 22.9 & 17.1 & 20.7 & 27.6 & 1.0 & { \bf 29.8 } & -  \\
ndc-Z. & \textsc{Both} & -1.0 & 29.2 & 27.9 & 27.8 & 28.2 & 28.3 & 27.6 & 28.0 & { \bf 29.6 } & 28.6 & 29.5 & -  \\
es & \textsc{SmolSent} & -1.1 & 62.4 & 61.3 & 51.3 & 61.3 & 61.3 & - & - & 63.0 & - & { \bf 63.5 } & 61.8  \\
sat & \textsc{SmolSent} & -1.3 & 32.4 & 30.9 & 0.6 & 31.1 & 30.8 & 34.7 & 36.0 & { \bf 36.3 } & 1.8 & 35.7 & -  \\
nd & \textsc{Both} & -1.4 & 43.9 & 41.5 & 42.5 & 42.5 & 43.2 & 42.3 & 42.9 & { \bf 44.5 } & 43.6 & - & -  \\
qu & \textsc{SmolSent} & -1.9 & 34.7 & 32.8 & 29.3 & 32.8 & 33.0 & 35.3 & 35.1 & 34.0 & 22.0 & { \bf 36.3 } & 27.9  \\
lus & \textsc{SmolSent} & -2.1 & 42.6 & 39.7 & 37.2 & 40.5 & 41.4 & 40.6 & 41.5 & { \bf 43.8 } & 33.5 & 42.6 & 39.0  \\
scn & \textsc{SmolDoc} & -2.2 & 52.4 & 47.3 & 51.2 & 50.2 & 50.8 & 49.9 & 51.4 & 52.1 & 49.5 & { \bf 53.3 } & 51.0  \\
ne & \textsc{SmolDoc} & -2.7 & 54.3 & 51.8 & 51.9 & 51.6 & 52.1 & 52.4 & 52.5 & 52.4 & 52.7 & { \bf 54.9 } & 45.2  \\
pa-A. & \textsc{SmolSent} & -3.0 & 38.1 & 35.8 & 0.3 & 35.1 & 35.7 & 41.6 & 41.2 & 36.7 & 37.3 & { \bf 43.5 } & -  \\
aeb & \textsc{Both} & -3.3 & 46.5 & 41.9 & 42.7 & 43.2 & 43.4 & 45.9 & 46.8 & 47.6 & { \bf 49.2 } & - & 43.8  \\
bo & \textsc{SmolSent} & -4.3 & 41.3 & 36.7 & 34.5 & 37.0 & 37.3 & 42.6 & 42.1 & { \bf 43.3 } & 19.8 & 41.8 & 36.9  \\
pcm & \textsc{Both} & -6.5 & 47.9 & 43.5 & 39.7 & 41.4 & 41.6 & 51.3 & 45.7 & 49.8 & { \bf 56.0 } & - & -  \\
tcy & \textsc{SmolDoc} & -6.8 & 34.7 & 22.0 & 28.0 & 27.9 & 28.8 & 28.2 & 29.3 & 36.7 & 21.6 & { \bf 39.1 } & -  \\
ktu & \textsc{Both} & -9.4 & 56.6 & 59.3 & 40.1 & 47.2 & 51.3 & 45.8 & 48.4 & 57.8 & 22.3 & { \bf 64.3 } & - \\
\hline
\caption{Full results (0-shot) For the \enxx{} direction. Languages in the Overlap subset (supported by all models) are shown first, and then all other languages. The $\Delta_{FT}$ compares the base model and the model finetuned on \smolboth{}, to give an idea of how effective the \smol{} datasets are for that language. The \textsc{cat} column indicates which \smol{} datasets support this language. Language varieties whose script/region is different from the CLDR default would have the ISO-15924 script code in the BCP-47 code, like \textsc{mni-Mtei} or \textsc{ber-Latn}; in this table we have abbreviated them to the first letter thereof (\textsc{mni-M} or \textsc{ber-L}) to make the table fit better. Similarly, we have abbreviated \smolseal{}, \smoldog{}, \smolboth{}, and \textsc{Both+Gatitos} to \textsc{sS}, \textsc{sD}, \textsc{sB} and \textsc{sG} respectively; among the baselines, \textsc{Gemini 2.0-\{Flash, Pro\}} to \textsc{G2.0-\{F,P\}}, and \textsc{Google Translate} to \textsc{GTr}. }
\label{tab:full-results}
\end{longtable}
}

\twocolumn


\section{Complete Per-Language details: the Big-\smol{} table}
A summary of all \smol{} language pairs and coarse-grained information about them can be seen in Table \ref{tab:full-details}. Numbers are given in terms of examples; keep in mind that a single example in \smoldog{} is a document, whereas in \smolseal{} it is a sentence. 

\onecolumn 
{ \scriptsize  % Start \scriptsize here
\begin{longtable}{l|lllrr}
% \centering


Lang. pair	& target language name   & ISO 15924 Script &	Continent &	\smolseal{} ex &	\smoldog{} ex \\
\hline  
\endhead
am\_ar & Arabic & Arab & Asia & 0 & 329 \\
am\_zh & Mandarin Chinese & Hans & Asia & 0 & 329 \\
en\_aa & Afar & Latn & Africa & 863 & 66 \\
en\_ach & Acoli & Latn & Africa & 863 & 66 \\
en\_aeb & Tunisian Arabic & Arab & Africa & 862 & 66 \\
en\_af & Afrikaans & Latn & Africa & 863 & 130 \\
en\_ahr & Ahirani & Deva & Asia & 0 & 457 \\
en\_ak & Twi & Latn & Africa & 863 & 260 \\
en\_alz & Alur & Latn & Africa & 863 & 66 \\
en\_am & Amharic & Ethi & Africa & 863 & 584 \\
en\_apd & Sudanese Arabic & Arab & Africa & 855 & 66 \\
en\_ar-MA & Morrocan Arabic & Arab & Africa & 863 & 260 \\
en\_arz & Egyptian Arabic & Arab & Africa & 863 & 260 \\
en\_ay & Aymara & Latn & Americas & 863 & 0 \\
en\_ayl & Libyan Arabic & Arab & Africa & 863 & 66 \\
en\_bci & Baoulé & Latn & Africa & 863 & 66 \\
en\_bem & Bemba (Zambia) & Latn & Africa & 863 & 66 \\
en\_ber & Tamazight (Tifinagh Script) & Tfng & Africa & 862 & 130 \\
en\_ber-Latn & Tamazight (Latin Script) & Latn & Africa & 862 & 130 \\
en\_bfq & Badaga & Taml & Asia & 0 & 457 \\
en\_bfy & Bagheli & Deva & Asia & 0 & 457 \\
en\_bgq & Bagri & Deva & Asia & 0 & 457 \\
en\_bm & Bambara & Latn & Africa & 863 & 260 \\
en\_bns & Bundeli & Deva & Asia & 0 & 456 \\
en\_bo & Tibetan & Tibt & Asia & 863 & 0 \\
en\_bra & Braj & Deva & Asia & 0 & 457 \\
en\_brx & Bodo (India) & Deva & Asia & 863 & 0 \\
en\_ccp-Latn & Chakma (Latin script) & Latn & Asia & 0 & 457 \\
en\_cgg & Chiga & Latn & Africa & 863 & 66 \\
en\_dhd & Dhundari & Deva & Asia & 0 & 456 \\
en\_din & Dinka & Latn & Africa & 863 & 66 \\
en\_doi & Dogri & Deva & Asia & 0 & 454 \\
en\_dov & Dombe & Latn & Africa & 863 & 66 \\
en\_dyu & Dyula & Latn & Africa & 863 & 66 \\
en\_ee & Ewe & Latn & Africa & 863 & 130 \\
en\_efi & Efik & Latn & Africa & 863 & 66 \\
% en\_es & Spanish & Latn & Europe & 863 & 0 \\
en\_ff & Fulfulde & Latn & Africa & 862 & 260 \\
en\_fon & Fon & Latn & Africa & 863 & 66 \\
en\_gaa & Ga & Latn & Africa & 863 & 66 \\
en\_gn & Guarani & Latn & Americas & 863 & 0 \\
en\_grt-Latn & Garo (Latin script) & Latn & Asia & 0 & 457 \\
en\_ha & Hausa & Latn & Africa & 863 & 584 \\
en\_hoc-Wara & Ho (Warang Chiti script) & Wara & Asia & 0 & 457 \\
en\_ig & Igbo & Latn & Africa & 863 & 391 \\
en\_kfy & Kumaoni & Deva & Asia & 0 & 457 \\
en\_kg & Kongo & Latn & Africa & 863 & 66 \\
en\_ki & Kikuyu & Latn & Africa & 863 & 66 \\
en\_kl & Kalaallisut & Latn & Americas & 863 & 0 \\
en\_kr & Kanuri & Latn & Africa & 863 & 66 \\
en\_kri & Krio & Latn & Africa & 863 & 130 \\
en\_kru & Kurukh & Deva & Asia & 0 & 457 \\
en\_ks & Kashmiri & Arab & Asia & 863 & 0 \\
en\_ks-Deva & Kashmiri (Devanagari script) & Deva & Asia & 863 & 0 \\
en\_ktu & Kituba (DRC) & Latn & Africa & 863 & 66 \\
en\_lep & Lepcha & Lepc & Asia & 0 & 456 \\
en\_lg & Luganda & Latn & Africa & 863 & 260 \\
en\_lif-Limb & Limbu (Limbu script) & Limb & Asia & 0 & 457 \\
en\_ln & Lingala & Latn & Africa & 863 & 260 \\
en\_lu & Kiluba (Luba-Katanga) & Latn & Africa & 863 & 66 \\
en\_luo & Luo & Latn & Africa & 863 & 260 \\
en\_lus & Mizo & Latn & Asia & 863 & 0 \\
en\_mag & Magahi & Deva & Asia & 0 & 456 \\
en\_mfe & Morisien & Latn & Africa & 863 & 66 \\
en\_mg & Malagasy & Latn & Africa & 863 & 391 \\
en\_mjl & Mandeali & Deva & Asia & 0 & 457 \\
en\_mni-Mtei & Meiteilon (Manipuri) & Mtei & Asia & 863 & 0 \\
en\_mos & Mossi & Latn & Africa & 863 & 66 \\
en\_mtr & Mewari & Deva & Asia & 0 & 457 \\
en\_nd & North Ndebele & Latn & Africa & 863 & 66 \\
en\_ndc-ZW & Ndau & Latn & Africa & 863 & 66 \\
en\_ne & Nepali & Deva & Asia & 0 & 456 \\
en\_noe & Nimadi & Deva & Asia & 0 & 315 \\
en\_nr & South Ndebele & Latn & Africa & 863 & 66 \\
en\_nso & Sepedi & Latn & Africa & 863 & 130 \\
en\_ny & Chichewa & Latn & Africa & 863 & 260 \\
en\_nqo & NKo & Nkoo & Africa & 863 & 66 \\
en\_om & Oromo & Latn & Africa & 863 & 391 \\
en\_pa-Arab & Lahnda Punjabi (Pakistan) & Arab & Asia & 863 & 0 \\
en\_pcm & Nigerian Pidgin & Latn & Africa & 863 & 130 \\
en\_qu & Quechua & Latn & Americas & 863 & 0 \\
en\_rn & Rundi & Latn & Africa & 863 & 66 \\
en\_rw & Kinyarwanda & Latn & Africa & 863 & 260 \\
en\_sa & Sanskrit & Deva & Asia & 863 & 0 \\
en\_sat & Santali (Ol Chiki script) & Olck & Asia & 863 & 0 \\
en\_sat-Latn & Santali (Latin Script) & Latn & Asia & 863 & 0 \\
en\_scl & Shina & Arab & Asia & 0 & 457 \\
en\_scn & Sicilian & Latn & Europe & 0 & 100 \\
en\_sd-Deva & Sindhi (Devanagari script) & Deva & Asia & 0 & 456 \\
en\_sgj & Surgujia & Deva & Asia & 0 & 356 \\
en\_sjp & Surjapuri & Deva & Asia & 0 & 299 \\
en\_sn & Shona & Latn & Africa & 863 & 260 \\
en\_so & Somali & Latn & Africa & 862 & 260 \\
en\_spv & Sambalpuri & Orya & Asia & 0 & 457 \\
en\_ss & Swati & Latn & Africa & 863 & 66 \\
en\_st & Sesotho & Latn & Africa & 863 & 260 \\
en\_sus & Susu & Latn & Africa & 863 & 66 \\
en\_sw & Swahili & Latn & Africa & 863 & 584 \\
en\_tcy & Tulu & Knda & Asia & 0 & 451 \\
en\_ti & Tigrinya & Ethi & Africa & 863 & 260 \\
en\_tiv & Tiv & Latn & Africa & 863 & 66 \\
en\_tn & Tswana & Latn & Africa & 863 & 66 \\
en\_trp & Kok Borok & Latn & Asia & 0 & 457 \\
en\_ts & Tsonga & Latn & Africa & 863 & 66 \\
en\_tum & Tumbuka & Latn & Africa & 863 & 66 \\
en\_unr-Deva & Mundari (Devanagari script) & Deva & Asia & 0 & 457 \\
en\_ve & Venda & Latn & Africa & 863 & 66 \\
en\_wbr & Wagdi & Deva & Asia & 0 & 455 \\
en\_wo & Wolof & Latn & Africa & 863 & 260 \\
en\_xh & Xhosa & Latn & Africa & 863 & 260 \\
en\_xnr & Kangri & Deva & Asia & 0 & 457 \\
en\_xsr-Tibt & Sherpa (Tibetan script) & Tibt & Asia & 0 & 457 \\
en\_yo & Yoruba & Latn & Africa & 863 & 584 \\
en\_yue & Cantonese & Hant & Asia & 863 & 584 \\
en\_zu & Zulu & Latn & Africa & 863 & 260 \\
sw\_ar & Arabic & Arab & Asia & 0 & 330 \\
sw\_zh & Mandarin Chinese & Hans & Asia & 0 & 330 \\
\hline
\caption{Details on all \smol{} languages. The last two columns are the number of examples per language pair; keep in mind that an example for \smolseal{} is a sentence pair but for \smoldog{} is a document/paragraph. Language pairs are only listed in the direction in which they were translated, so no \xxen{} pairs are present.}
\label{tab:full-details}
\end{longtable}
}

\twocolumn

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}


