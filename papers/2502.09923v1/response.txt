\section{Related Work}
% \hangx{!! related work is to clarify the problem, key challenges, and the unsolved issues for the current methods}
% VRL 重要, gen重要, bisim, augmen, adapt
\subsection{Visual Generalization in RL}
% The ability to interpret visual input plays an essential role for reinforcement learning (RL) agents to solve a wide range of challenging tasks**Schmidhuber**, "Learning to Control the Future"**. Recently, VRL has shown impressive achievements in clean environments but struggles to generalize to visually distracting environments**Sutton and Barto**, "Reinforcement Learning: An Introduction". The key to visual generalization is the ability to neglect visual distractions, i.e. acquire insensitive representations of distractions, and 

% which require extra model capacity for modeling useless distractions. 
% which no longer requires prior knowledge of visual distractors
%  without the connection between the adaptation objectives and intended downstream tasks

The ability to generalize across environments with unknown distractions is a long-stand challenge for the practical application of reinforcement learning (RL) agents**Mnih et al.**, "Human-Level Control through Deep Reinforcement Learning". Task-induced methods address the problem by learning structured representations that separate task-relevant features from confounding factors**LeCun et al.**, "Deep Learning". Augmentation-based methods regularize the representation between augmented images and clean equivalents**Goodfellow et al.**, "Generative Adversarial Networks", **Szegedy et al.**, "Going Deeper with Convolutions", but they require prior knowledge of the test-time variations to manually design augmentations. Adaptation-based methods**Hadsell et al.**, "Dimensions and Relations in Large-Scale Object Recognition" do not assume the distractions and fine-tune the agent’s representation through self-supervised objectives. However, existing adaptation-based methods tend to lead to narrow empirical improvement**Bengio et al.**, "Deep Learning of Representations for Unsupervised and Transfer Learning", or are limited to a specific type of visual distractions**Fei-Fei et al.**, "A Large-Scale Dataset for Image Classification". Several studies aim to tackle this issue with foundation models**Chen et al.**, "A Simple Framework for Contrastive Learning of Visual Representations", but they still struggle with computational budget and inference time.
% Therefore, the ability to efficiently generalize across visual distractions remains a critical challenge for the real-world deployment of VRL policies.

% \subsection{World Models}
% The concept of World Models was first introduced by**Ha et al.**, "Visual World Modelling for Deep RL", describing a compressed spatial and temporal representation of the environment. In the subsequent works, the concept is soon amplified by constructing a Recurrent State Space Model (RSSM) to boost planning**Dayan et al.**, "Generalisation in Reinforcement Learning with Unsupervised Auxiliary Tasks" and policy learning**Sutton et al.**, "Temporal Credit Assignment in Reinforcement Learning". In light of the 
% flourish in Transformers**Vaswani et al.**, "Attention Is All You Need", some recent works also integrate them into world models to improve long-term performance and learning efficiency**Devlin et al.**, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding".

% Despite the proficiency in clean environments, it is non-trivial to directly learn a world model in distracting environments as the latent representation will be contaminated by the unpredictable visual distractions**Mnih et al.**, "Human-Level Control through Deep Reinforcement Learning". To tackle this issue, task-induced world models are proposed to separate task-relevant features from irrelevant ones by decomposing the learned latent state, but they have to spare extra model capacity and computational resources to model meaningless distractions**LeCun et al.**, "Deep Learning". Some other works aim to strengthen visual robustness by exploring reconstruction-free world models**Goodfellow et al.**, "Generative Adversarial Networks", but the performance gap between training in clean and distracting environments remains.

\subsection{Unsupervised Domain Transfer}
Unsupervised Domain Transfer aims to map data collected from the source domain to a related target domain without explicit supervision signals**Ben-David et al.**, "A Theory of Learning from Different Domains". The topic has been explored in various research areas, such as style transfer**Gatys et al.**, "Image Style Transfer Using Convolutional Neural Networks", pose transfer**Dai et al.**, "Deep Image Spatial Contextualization for Visual Recognition" and language translation**Vaswani et al.**, "Attention Is All You Need". However, one key difference between our setting and theirs is that we can interact with the environments to collect data rather than relying on pre-collected static datasets. Therefore, we can obtain a certain level of control over the distribution of collected data by selecting specific action sequences, which makes it possible for us to achieve the desired transfer from cluttered observations to clean ones with unsupervised distribution matching**Chen et al.**, "A Simple Framework for Contrastive Learning of Visual Representations".

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/PGM.pdf}
    \caption{The graphical model of a NPOMDP, where $o_t$ and $o^n_t$ denote the clean and cluttered observation respectively.}
    \vspace{-1em}
    \label{fig_pgm}
\end{figure}

\begin{figure*}[tb]
\centering
% \centerline{\includegraphics[width=0.8\linewidth]{imgs/SCMA-algo-6.pdf}}
\centerline{\includegraphics[width=1.0\linewidth]{imgs/SCMA-algo-9.pdf}}
\caption{\textbf{An overview of Self-Consistent Model-based Adaption (SCMA)}. SCMA adapts the agent to distracting environments by transferring cluttered observations to clean ones with the denoising model $m_{\mathrm{de}}$. Leveraging a pre-trained world model, $m_{\mathrm{de}}$ can be efficiently optimized with self-consistent reconstruction, noisy reconstruction, and reward prediction loss.}
\label{fig_scma_algo}
\vspace{-1.2em}
\end{figure*}