\section{Related Work}
\subsection{Knowledge Distillation}
Knowledge distillation (KD) aims to compress the large model (Teacher) to a smaller one (Student) ____. 
% 
Consequently, SeqKD ____ uses the teacherâ€™s decoding sequences as the training data of the student and directly optimizes the cross-entropy loss on the one-hot target.
% 
However, the distribution mismatch issue hinders the performance of distillation, since the student model struggles to learn the correct mode from the distribution of the teacher.
% 
To solve this issue, previous studies investigate to adjust the calculated function towards the distribution between both models.
% 
____ propose the symmetric f-divergence to avoid this issue during distillation;
% 
Further, ____ and ____ mitigate this issue via adjusting the target distribution by combining the objective of both models.
% 
Besides, ____ optimize the target distribution by adaptively fusing the values of FKL and RKL to focus different region of distribution of the teacher.
% 
Moreover, ____ bridges the knowledge gap between the teacher and student by optimizing the student with the teacher-refined output of the student.
% 
However, the above methods mainly optimize this issue during distillation, ignoring the negative impact at the early stage of distillation.
% Besides, researchers investigate the various distilled objectives, \emph{e.g.}, the output of teachers' layers ____, the attention and hidden state alignment ____.
% % 
% Recent research investigates the knowledge distillation of the LLMs to refine the process of knowledge transfer.
% % 
% ____ propose a PPO-based framework with hybrid sampling to balance teacher guidance and student exploration. 
% % 
% Besides, ____ propose the task-aware, layer-aware distillation to transfer the intermediate knowledge in the layers of the teacher.
% % % 
% Similarly, ____ focuses on the distillation on the last layer's output hidden states of teacher and student models.


\subsection{Preference Optimization}
The Reinforcement Learning from Human Feedback (RLHF;____) framework initially trains a reward model on preference data and fine-tunes language models (LMs).
% 
DPO ____ bypasses explicit reward modeling by optimizing a pairwise logistic loss directly on preferences. 
% 
SLiC ____ adopts pairwise hinge losses, and SimPO ____ utilizes the average log probability of the sequence as the implicit reward to better align the reward with model generation.
% 
% In this paper, we leverage 
% while RSO ____ enhances DPO via rejection sampling to mitigate distribution drift.
%