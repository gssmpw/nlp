\section{Related Work}
\subsection{Knowledge Distillation}
Knowledge distillation (KD) aims to compress the large model (Teacher) to a smaller one (Student) \citep{buciluǎ2006model, hinton2015distilling}. 
% 
Consequently, SeqKD \citep{kim2016sequence} uses the teacher’s decoding sequences as the training data of the student and directly optimizes the cross-entropy loss on the one-hot target.
% 
However, the distribution mismatch issue hinders the performance of distillation, since the student model struggles to learn the correct mode from the distribution of the teacher.
% 
To solve this issue, previous studies investigate to adjust the calculated function towards the distribution between both models.
% 
\citet{wen2023f} propose the symmetric f-divergence to avoid this issue during distillation;
% 
Further, \citet{gu2024minillm} and \citet{ko2024distillm} mitigate this issue via adjusting the target distribution by combining the objective of both models.
% 
Besides, \citet{wu2024rethinking} optimize the target distribution by adaptively fusing the values of FKL and RKL to focus different region of distribution of the teacher.
% 
Moreover, \citet{xu2024speculative} bridges the knowledge gap between the teacher and student by optimizing the student with the teacher-refined output of the student.
% 
However, the above methods mainly optimize this issue during distillation, ignoring the negative impact at the early stage of distillation.
% Besides, researchers investigate the various distilled objectives, \emph{e.g.}, the output of teachers' layers \citep{sun2019patient}, the attention and hidden state alignment \citep{sanh2019distilbert, jiao2019tinybert}.
% % 
% Recent research investigates the knowledge distillation of the LLMs to refine the process of knowledge transfer.
% % 
% \citet{gu2024minillm} propose a PPO-based framework with hybrid sampling to balance teacher guidance and student exploration. 
% % 
% Besides, \citet{liang2023less} propose the task-aware, layer-aware distillation to transfer the intermediate knowledge in the layers of the teacher.
% % % 
% Similarly, \citet{zhang2024dual} focuses on the distillation on the last layer's output hidden states of teacher and student models.


\subsection{Preference Optimization}
The Reinforcement Learning from Human Feedback (RLHF;~\citet{christiano2017deep}) framework initially trains a reward model on preference data and fine-tunes language models (LMs).
% 
DPO \citep{rafailov2024direct} bypasses explicit reward modeling by optimizing a pairwise logistic loss directly on preferences. 
% 
SLiC \cite{zhao2023slic} adopts pairwise hinge losses, and SimPO \citep{meng2024simpo} utilizes the average log probability of the sequence as the implicit reward to better align the reward with model generation.
% 
% In this paper, we leverage 
% while RSO \citep{liu2023statistical} enhances DPO via rejection sampling to mitigate distribution drift.
%