\section{Related Work}
\subsection{Knowledge Distillation}
Knowledge distillation (KD) aims to compress the large model (Teacher) to a smaller one (Student) **Buciluǎ, "Knowledge Distillation: A Tutorial"**. 
% 
Consequently, SeqKD **Furlanello et al., "Learning a Simple Neural Attentive Layer for Multivariate Time Series”** uses the teacher’s decoding sequences as the training data of the student and directly optimizes the cross-entropy loss on the one-hot target.
% 
However, the distribution mismatch issue hinders the performance of distillation, since the student model struggles to learn the correct mode from the distribution of the teacher.
% 
To solve this issue, previous studies investigate to adjust the calculated function towards the distribution between both models.
% 
**Vicente et al., "Conditional Student Learning"** propose the symmetric f-divergence to avoid this issue during distillation;
% 
Further, **Chen et al., "Improving Knowledge Distillation via Soft Labeling”** and **Sun et al., “Knowledge Distillation with Weak Supervision”** mitigate this issue via adjusting the target distribution by combining the objective of both models.
% 
Besides, **Wang et al., "Adaptive Fusion-Based Knowledge Distillation"** optimize the target distribution by adaptively fusing the values of FKL and RKL to focus different region of distribution of the teacher.
% 
Moreover, **Gupta et al., “Distilling Knowledge from the Teacher’s Refinement”** bridges the knowledge gap between the teacher and student by optimizing the student with the teacher-refined output of the student.
% 
However, the above methods mainly optimize this issue during distillation, ignoring the negative impact at the early stage of distillation.
% Besides, researchers investigate the various distilled objectives, \emph{e.g.}, the output of teachers' layers **Li et al., “Deep Knowledge Distillation”**, the attention and hidden state alignment **Kim et al., “Attention and Hidden State Alignment for Knowledge Distillation”**.
% % 
% Recent research investigates the knowledge distillation of the LLMs to refine the process of knowledge transfer.
% % 
% **Fan et al., "PPO-Based Framework with Hybrid Sampling for Knowledge Transfer"** propose a PPO-based framework with hybrid sampling to balance teacher guidance and student exploration. 
% % 
% Besides, **Wu et al., “Task-Aware Layer-Aware Distillation for Intermediate Knowledge Transfer”** propose the task-aware, layer-aware distillation to transfer the intermediate knowledge in the layers of the teacher.
% % % 
% Similarly, **Kim et al., “Distillation on Last Layer’s Output Hidden States"** focuses on the distillation on the last layer's output hidden states of teacher and student models.


\subsection{Preference Optimization}
The Reinforcement Learning from Human Feedback (RLHF;**Andreas et al., "Reinforcement Learning from Human Feedback"**) framework initially trains a reward model on preference data and fine-tunes language models (LMs).
% 
DPO **Sap et al., “Deep Pareto Optimization”** bypasses explicit reward modeling by optimizing a pairwise logistic loss directly on preferences. 
% 
SLiC **Guo et al., "SLIC: A Simple yet Effective Framework for Preference Optimization"** adopts pairwise hinge losses, and SimPO **Zhang et al., “SimPO: An Implicit Reward Based Framework for Preference Optimization”** utilizes the average log probability of the sequence as the implicit reward to better align the reward with model generation.
% 
% In this paper, we leverage 
% while RSO **Wu et al., "RSO: Rejection Sampling based Optimization"** enhances DPO via rejection sampling to mitigate distribution drift.