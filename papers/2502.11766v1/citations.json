[
  {
    "index": 0,
    "papers": [
      {
        "key": "bucilu\u01ce2006model",
        "author": "Bucilu\u01ce, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru",
        "title": "Model compression"
      },
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey",
        "title": "Distilling the Knowledge in a Neural Network"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kim2016sequence",
        "author": "Kim, Yoon and Rush, Alexander M",
        "title": "Sequence-level knowledge distillation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wen2023f",
        "author": "Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili",
        "title": "f-Divergence Minimization for Sequence-Level Knowledge Distillation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gu2024minillm",
        "author": "Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie",
        "title": "MiniLLM: Knowledge distillation of large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ko2024distillm",
        "author": "Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young",
        "title": "Distillm: Towards streamlined distillation for large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wu2024rethinking",
        "author": "Wu, Taiqiang and Tao, Chaofan and Wang, Jiahao and Yang, Runming and Zhao, Zhe and Wong, Ngai",
        "title": "Rethinking kullback-leibler divergence in knowledge distillation for large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "xu2024speculative",
        "author": "Xu, Wenda and Han, Rujun and Wang, Zifeng and Le, Long T and Madeka, Dhruv and Li, Lei and Wang, William Yang and Agarwal, Rishabh and Lee, Chen-Yu and Pfister, Tomas",
        "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "sun2019patient",
        "author": "Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing",
        "title": "Patient knowledge distillation for bert model compression"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "sanh2019distilbert",
        "author": "Sanh, V",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
      },
      {
        "key": "jiao2019tinybert",
        "author": "Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun",
        "title": "Tinybert: Distilling bert for natural language understanding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "gu2024minillm",
        "author": "Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie",
        "title": "MiniLLM: Knowledge distillation of large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liang2023less",
        "author": "Liang, Chen and Zuo, Simiao and Zhang, Qingru and He, Pengcheng and Chen, Weizhu and Zhao, Tuo",
        "title": "Less is more: Task-aware layer-wise distillation for language model compression"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2024dual",
        "author": "Zhang, Songming and Zhang, Xue and Sun, Zengkui and Chen, Yufeng and Xu, Jinan",
        "title": "Dual-Space Knowledge Distillation for Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhao2023slic",
        "author": "Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J",
        "title": "Slic-hf: Sequence likelihood calibration with human feedback"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2023statistical",
        "author": "Liu, Tianqi and Zhao, Yao and Joshi, Rishabh and Khalman, Misha and Saleh, Mohammad and Liu, Peter J and Liu, Jialu",
        "title": "Statistical rejection sampling improves preference optimization"
      }
    ]
  }
]