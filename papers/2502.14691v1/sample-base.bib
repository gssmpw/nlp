
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries



@inproceedings{mafia,
author = {Jog, Adwait and Kayiran, Onur and Kesten, Tuba and Pattnaik, Ashutosh and Bolotin, Evgeny and Chatterjee, Niladrish and Keckler, Stephen W. and Kandemir, Mahmut T. and Das, Chita R.},
title = {Anatomy of GPU Memory System for Multi-Application Execution},
year = {2015},
isbn = {9781450336048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.recursos.biblioteca.upc.edu/10.1145/2818950.2818979},
doi = {10.1145/2818950.2818979},
abstract = {As GPUs make headway in the computing landscape spanning mobile platforms, supercomputers, cloud and virtual desktop platforms, supporting concurrent execution of multiple applications in GPUs becomes essential for unlocking their full potential. However, unlike CPUs, multi-application execution in GPUs is little explored. In this paper, we study the memory system of GPUs in a concurrently executing multi-application environment. We first present an analytical performance model for many-threaded architectures and show that the common use of misses-per-kilo-instruction (MPKI) as a proxy for performance is not accurate without considering the bandwidth usage of applications. We characterize the memory interference of applications and discuss the limitations of existing memory schedulers in mitigating this interference. We extend the analytical model to multiple applications and identify the key metrics to control various performance metrics. We conduct extensive simulations using an enhanced version of GPGPU-Sim targeted for concurrently executing multiple applications, and show that memory scheduling decisions based on MPKI and bandwidth information are more effective in enhancing throughput compared to the traditional FR-FCFS and the recently proposed RR FR-FCFS policies.},
booktitle = {Proceedings of the 2015 International Symposium on Memory Systems},
pages = {223–234},
numpages = {12},
keywords = {Analytical Modeling, GPGPUs, Memory System},
location = {Washington DC, DC, USA},
series = {MEMSYS '15}
}


@INPROCEEDINGS{mgpusim,
  author={Sun, Yifan and Baruah, Trinayan and Mojumder, Saiful A. and Dong, Shi and Gong, Xiang and Treadway, Shane and Bao, Yuhui and Hance, Spencer and McCardwell, Carter and Zhao, Vincent and Barclay, Harrison and Ziabari, Amir Kavyan and Chen, Zhongliang and Ubal, Rafael and Abellán, José L. and Kim, John and Joshi, Ajay and Kaeli, David},
  booktitle={2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={MGPUSim: Enabling Multi-GPU Performance Modeling and Optimization}, 
  year={2019},
  volume={},
  number={},
  pages={197-209},
  keywords={multi-GPU systems;simulation;memory management},
  doi={}}


@INPROCEEDINGS{gputejas,
  author={Malhotra, Geetika and Goel, Seep and Sarangi, Smruti R.},
  booktitle={2014 21st International Conference on High Performance Computing (HiPC)}, 
  title={GpuTejas: A parallel simulator for GPU architectures}, 
  year={2014},
  volume={},
  number={},
  pages={1-10},
  keywords={Graphics processing units;Instruction sets;Java;Kernel;Timing;Computational modeling;Computer architecture;GPU;Simulator;Timing model;Cycle-level;Parallel Architectural Simulation;Nvidia;Tesla},
  doi={10.1109/HiPC.2014.7116897}}

@INPROCEEDINGS{barra,
  author={Collange, Caroline and Daumas, Marc and Defour, David and Parello, David},
  booktitle={2010 IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems}, 
  title={Barra: A Parallel Functional Simulator for GPGPU}, 
  year={2010},
  volume={},
  number={},
  pages={351-360},
  keywords={Graphics processing unit;Registers;Computational modeling;Computer architecture;Instruction sets;Hardware;Load modeling;GPU;GPGPU;UNISIM;CUDA},
  doi={10.1109/MASCOTS.2010.43}}


@INPROCEEDINGS{gpgpusimOriginal,
  author={Bakhoda, Ali and Yuan, George L. and Fung, Wilson W. L. and Wong, Henry and Aamodt, Tor M.},
  booktitle={2009 IEEE International Symposium on Performance Analysis of Systems and Software}, 
  title={Analyzing CUDA workloads using a detailed GPU simulator}, 
  year={2009},
  volume={},
  number={},
  pages={163-174},
  keywords={Analytical models;Yarn;Graphics;Parallel processing;Microarchitecture;Hardware;Process design;Concurrent computing;Parallel programming;Computational modeling},
  doi={10.1109/ISPASS.2009.4919648}}


@ARTICLE{teslaHotchips,
  author={Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
  journal={IEEE Micro}, 
  title={{NVIDIA Tesla: A Unified Graphics and Computing Architecture}}, 
  year={2008},
  volume={28},
  number={2},
  pages={39-55},
  keywords={Graphics;Computer architecture;Parallel processing;Pipelines;Concurrent computing;Load management;Multicore processing;Parallel programming;Portable computers;Workstations;Hot Chips 19;GPU;parallel processor;SIMT;SIMD;unified graphics and parallel computing architecture;graphics processing unit;cooperative thread array;Tesla},
  doi={10.1109/MM.2008.31}
}

@INPROCEEDINGS{accelsim,
  author={Khairy, Mahmoud and Shen, Zhesheng and Aamodt, Tor M. and Rogers, Timothy G.},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={{Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling}}, 
  month = {May},
  year={2020},
  volume={},
  number={},
  pages={473-486},
  doi={10.1109/ISCA45697.2020.00047}}

@techreport{voltaPaper,
author = {NVIDIA},
file = {:home/rodhuega/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2017 - NVIDIA TESLA V100 GPU ARCHITECTURE THE WORLD'S MOST ADVANCED DATA CENTER GPU.pdf:pdf},
institution = {NVIDIA},
title = {{NVIDIA Tesla V100 GPU architecture the world's most advanced data center GPU}},
year = {2017}
}

@techreport{turingPaper,
author = {NVIDIA},
file = {:home/rodhuega/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nvidia - 2018 - NVIDIA TURING GPU ARCHITECTURE Graphics Reinvented NVIDIA Turing GPU Architecture(2).pdf:pdf},
title = {{NVIDIA TURING GPU architecture Graphics Reinvented NVIDIA Turing GPU Architecture}},
year = {2018},
institution = {NVIDIA}
}

@techreport{adaPaper,
author = {NVIDIA},
file = {:home/rodhuega/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nvidia - Unknown - NVIDIA ADA GPU ARCHITECTURE.pdf:pdf},
title = {{NVIDIA ADA GPU architecture}},
year = {2022},
institution = {NVIDIA}
}

@techreport{amperePaper,
author = {NVIDIA},
file = {:home/rodhuega/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nvidia - Unknown - Updated with NVIDIA RTX A6000 and NVIDIA A40 Information V2.0 NVIDIA AMPERE GA102 GPU ARCHITECTURE Second-Generation.pdf:pdf},
title = {{NVIDIA AMPERE GA102 GPU architecture Second-Generation RTX NVIDIA Ampere GA102 GPU Architecture}},
year = {2020},
institution = {NVIDIA}
}

@techreport{hopperPaper,
abstract = {The NVIDIA{\textregistered} H100 Tensor Core GPU powered by the NVIDIA Hopper GPU architecture delivers the next massive leap in accelerated computing performance for NVIDIA's data center platforms. H100 securely accelerates diverse workloads from small enterprise workloads, to exascale HPC, to trillion parameter AI models.},
author = {NVIDIA},
file = {:home/rodhuega/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nvidia - 2022 - NVIDIA H100 Tensor Core GPU Architecture.pdf:pdf},
title = {{NVIDIA H100 Tensor Core GPU Architecture}},
year = {2022},
institution = {NVIDIA}
}

@INPROCEEDINGS{multi2sim,
  author={Ubal, Rafael and Jang, Byunghyun and Mistry, Perhaad and Schaa, Dana and Kaeli, David},
  booktitle={2012 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)}, 
  title={Multi2Sim: A simulation framework for CPU-GPU computing}, 
  year={2012},
  volume={},
  number={},
  pages={335-344},
  keywords={Computational modeling;Graphics processing units;Kernel;Computer architecture;VLIW;Benchmark testing;Hardware;GPU;AMD;Evergreen ISA;Multi2Sim},
  doi={}}


@misc{amdevergreen,
   author = {AMD},
   title = {AMD Evergreen Family Instruction Set Arch},
   url = {www.amd.com},
   year = {2011},
}

@misc{amdgcn3,
   author = {AMD},
   title = {Graphics Core Next Architecture, Generation 3, Reference Guide},
   url = {www.amd.com},
   year = {2016},
}

@INPROCEEDINGS{parallelGPUSim1,
  author={Lee, Sangpil and Ro, Won Woo},
  booktitle={2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Parallel GPU architecture simulation framework exploiting work allocation unit parallelism}, 
  year={2013},
  volume={},
  number={},
  pages={107-117},
  keywords={Graphics processing units;Computational modeling;Instruction sets;Process control;Load modeling;Multiprocessor interconnection},
  doi={10.1109/ISPASS.2013.6557151}}


@ARTICLE{parallelGPUSim2,
  author={Lee, Sangpil and Ro, Won Woo},
  journal={IEEE Transactions on Computers}, 
  title={Parallel GPU Architecture Simulation Framework Exploiting Architectural-Level Parallelism with Timing Error Prediction}, 
  year={2016},
  volume={65},
  number={4},
  pages={1253-1265},
  keywords={Graphics processing units;Computational modeling;Instruction sets;Computer architecture;Load modeling;Predictive models;Synchronization;Parallel, Simulation;GPU architecture;Error prediction;Relaxed synchronization;Parallel;simulation;GPU architecture;error prediction;relaxed synchronization},
  doi={10.1109/TC.2015.2444848}}


@ARTICLE{openmp,
  author={Dagum, L. and Menon, R.},
  journal={IEEE Computational Science and Engineering}, 
  title={OpenMP: an industry standard API for shared-memory programming}, 
  year={1998},
  volume={5},
  number={1},
  pages={46-55},
  keywords={Message passing;Scalability;Hardware;Computer architecture;Power system modeling;ANSI standards;Parallel processing;Coherence;Software systems;Parallel programming},
  doi={10.1109/99.660313}}



@inproceedings{rodinia,
abstract = {This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley's dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout. {\textcopyright} 2009 IEEE.},
author = {Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W. and Lee, Sang Ha and Skadron, Kevin},
booktitle = {Proceedings of the 2009 IEEE International Symposium on Workload Characterization, IISWC 2009},
doi = {10.1109/IISWC.2009.5306797},
file = {:home/rodhuega/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Che et al. - 2009 - Rodinia A benchmark suite for heterogeneous computing.pdf:pdf},
isbn = {9781424451562},
pages = {44--54},
title = {{Rodinia: A benchmark suite for heterogeneous computing}},
year = {2009}
}

@misc{deepbenchWeb,
title = {{DeepBench: Benchmarking Deep Learning operations on different hardware}},
url = {https://github.com/baidu-research/DeepBench},
urldate = {2022-04-21},
author = {S. Narang and G. Diamos},
year = {2016}
}

@article{parboil,
   title={{Parboil: A Revised Benchmark Suite for Scientific and Commercial Throughput Computing}},
   author={Stratton, J.A. and Rodrigues, C. and Sung, I.J. and Obeid, N. and Chang, L.W. and Anssari, N. and Liu, G.D. and Hwu, W.W.},
   journal={Center for Reliable and High-Performance Computing},
   year={2012}
}

@INPROCEEDINGS{pannotia,
  author={Che, Shuai and Beckmann, Bradford M. and Reinhardt, Steven K. and Skadron, Kevin},
  booktitle={2013 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={{Pannotia: Understanding irregular GPGPU graph applications}}, 
  year={2013},
  volume={},
  number={},
  pages={185-195},
  doi={10.1109/IISWC.2013.6704684}
}

@INPROCEEDINGS{lonestar,
  author={Burtscher, Martin and Nasre, Rupesh and Pingali, Keshav},
  booktitle={2012 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={{A quantitative study of irregular programs on GPUs}}, 
  year={2012},
  volume={},
  number={},
  pages={141-151},
  keywords={Kernel;Graphics processing units;Instruction sets;Benchmark testing;Measurement;Hardware;Heuristic algorithms},
  doi={10.1109/IISWC.2012.6402918}
}

@INPROCEEDINGS{proxy,
  author={Villa, Oreste and Johnson, Daniel R. and Oconnor, Mike and Bolotin, Evgeny and Nellans, David and Luitjens, Justin and Sakharnykh, Nikolai and Wang, Peng and Micikevicius, Paulius and Scudiero, Anthony and Keckler, Stephen W. and Dally, William J.},
  booktitle={SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={{Scaling the Power Wall: A Path to Exascale}}, 
  year={2014},
  volume={},
  number={},
  pages={830-841},
  keywords={Graphics processing units;Computer architecture;Bandwidth;Registers;Instruction sets;Supercomputers;Kernel},
  doi={10.1109/SC.2014.73}
}

@INPROCEEDINGS{dragon,
  author={Wang, Jin and Yalamanchili, Sudhakar},
  booktitle={2014 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={{Characterization and analysis of dynamic parallelism in unstructured GPU applications}}, 
  year={2014},
  volume={},
  number={},
  pages={51-60},
  keywords={Kernel;Instruction sets;Graphics processing units;Parallel processing;Measurement;Arrays},
  doi={10.1109/IISWC.2014.6983039}
}

@misc{cutlass,
   author = {NVIDIA},
   title = {{CUTLASS: CUDA Templates for Linear Algebra Subroutines}},
   url = {https://github.com/NVIDIA/cutlass},
   year = {2018}
}

@INPROCEEDINGS{polybench,
  author={Grauer-Gray, Scott and Xu, Lifan and Searles, Robert and Ayalasomayajula, Sudhee and Cavazos, John},
  booktitle={2012 Innovative Parallel Computing (InPar)}, 
  title={{Auto-tuning a high-level language targeted to GPU codes}}, 
  year={2012},
  volume={},
  number={},
  pages={1-10},
  keywords={Graphics processing unit;Abstracts;Programming;Nickel;Tiles;Benchmark testing;Auto-tuning;GPU;CUDA;OpenCL;Optimization;Belief Propagation},
  doi={10.1109/InPar.2012.6339595}
}


@inproceedings{pcaKernelGpuSampling,
author = {Avalos Baddouh, Cesar and Khairy, Mahmoud and Green, Roland N. and Payer, Mathias and Rogers, Timothy G.},
title = {Principal Kernel Analysis: A Tractable Methodology to Simulate Scaled GPU Workloads},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480100},
doi = {10.1145/3466752.3480100},
abstract = {Simulating all threads in a scaled GPU workload results in prohibitive simulation cost. Cycle-level simulation is orders of magnitude slower than native silicon, the only solution is to reduce the amount of work simulated while accurately representing the program. Existing solutions to simulate GPU programs either scale the input size, simulate the first several billion instructions, or simulate a portion of both the GPU and the workload. These solutions lack validation against scaled systems, produce unrealistic contention conditions and frequently miss critical code sections. Existing CPU sampling mechanisms, like SimPoint, reduce per-thread workload, and are ill-suited to GPU programs where reducing the number of threads is critical. Sampling solutions on GPUs space lack silicon validation, require per-workload parameter tuning, and do not scale. A tractable solution, validated on contemporary scaled workloads, is needed to provide credible simulation results. By studying scaled workloads with centuries-long simulation times, we uncover practical and algorithmic limitations of existing solutions and propose Principal Kernel Analysis: a hierarchical program sampling methodology that concisely represents GPU programs by selecting representative kernel portions using a scalable profiling methodology, tractable clustering algorithm and detection of intra-kernel IPC stability. We validate Principal Kernel Analysis across 147 workloads and three GPU generations using the Accel-Sim simulator, demonstrating a better performance/error tradeoff than prior work and that century-long MLPerf simulations are reduced to hours with an average cycle error of 27\% versus silicon.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {724–737},
numpages = {14},
keywords = {Workload sampling, Simulation methodology, GPU},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@INPROCEEDINGS{nvas,
  author={Villa, Oreste and Lustig, Daniel and Yan, Zi and Bolotin, Evgeny and Fu, Yaosheng and Chatterjee, Niladrish and Jiang, Nan and Nellans, David},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={Need for Speed: Experiences Building a Trustworthy System-Level GPU Simulator}, 
  year={2021},
  volume={},
  number={},
  pages={868-880},
  keywords={Productivity;Computational modeling;Buildings;Memory management;Graphics processing units;Machine learning;Tools;GPU;Simulation;Performance Modeling;System Simulation},
  doi={10.1109/HPCA51647.2021.00077}}


@InProceedings{slurm,
author="Yoo, Andy B.
and Jette, Morris A.
and Grondona, Mark",
editor="Feitelson, Dror
and Rudolph, Larry
and Schwiegelshohn, Uwe",
title="SLURM: Simple Linux Utility for Resource Management",
booktitle="Job Scheduling Strategies for Parallel Processing",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="44--60",
abstract="A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
isbn="978-3-540-39727-4"
}


@InProceedings{ompScheduling1,
author="Ayguad{\'e}, Eduard
and Blainey, Bob
and Duran, Alejandro
and Labarta, Jes{\'u}s
and Mart{\'i}nez, Francisco
and Martorell, Xavier
and Silvera, Ra{\'u}l",
editor="Voss, Michael J.",
title="Is the Schedule Clause Really Necessary in OpenMP?",
booktitle="OpenMP Shared Memory Parallel Programming",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="147--159",
abstract="Choosing the appropriate assignment of loop iterations to threads is one of the most important decisions that need to be taken when parallelizing Loops, the main source of parallelism in numerical applications. This is not an easy task, even for expert programmers, and it can potentially take a large amount of time. OpenMP offers the schedule clause, with a set of predefined iteration scheduling strategies, to specify how (and when) this assignment of iterations to threads is done. In some cases, the best schedule depends on architectural characteristics of the target architecture, data input, ... making the code less portable. Even worse, the best schedule can change along execution time depending on dynamic changes in the behavior of the loop or changes in the resources available in the system. Also, for certain types of imbalanced loops, the schedulers already proposed in the literature are not able to extract the maximum parallelism because they do not appropriately trade-off load balancing and data locality. This paper proposes a new scheduling strategy, that derives at run time the best scheduling policy for each parallel loop in the program, based on information gathered at runtime by the library itself.",
isbn="978-3-540-45009-2"
}

@InProceedings{ompScheduling2,
author="Ciorba, Florina M.
and Iwainsky, Christian
and Buder, Patrick",
editor="de Supinski, Bronis R.
and Valero-Lara, Pedro
and Martorell, Xavier
and Mateo Bellido, Sergi
and Labarta, Jesus",
title="OpenMP Loop Scheduling Revisited: Making a Case for More Schedules",
booktitle="Evolving OpenMP for Evolving Architectures",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="21--36",
abstract="In light of continued advances in loop scheduling, this work revisits the OpenMP loop scheduling by outlining the current state of the art in loop scheduling and presenting evidence that the existing OpenMP schedules are insufficient for all combinations of applications, systems, and their characteristics. A review of the state of the art shows that due to the specifics of the parallel applications, the variety of computing platforms, and the numerous performance degradation factors, no single loop scheduling technique can be a `one-fits-all' solution to effectively optimize the performance of all parallel applications in all situations. The impact of irregularity in computational workloads and hardware systems, including operating system noise, on the performance of parallel applications results in performance loss and has often been neglected in loop scheduling research, in particular the context of OpenMP schedules. Existing dynamic loop self-scheduling techniques, such as trapezoid self-scheduling, factoring and weighted factoring, offer an unexplored potential to alleviate this degradation in OpenMP due to the fact that they explicitly target the minimization of load imbalance and scheduling overhead. Through theoretical and experimental evaluation, this work shows that these loop self-scheduling methods provide a benefit in the context of OpenMP. In conclusion, OpenMP must include more schedules to offer a broader performance coverage of applications executing on an increasing variety of heterogeneous shared memory computing platforms.",
isbn="978-3-319-98521-3"
}


@misc{gperftools,
   author = {Google},
   title = {Google Performance Tools},
   url = {https://github.com/gperftools/gperftools},
   year = {2015}
}

@misc{criticalSectionOpenMP,
   author = {Performance Optimisation and Productivity (PoP)},
   title = {Patterns of OpenMP critical section},
   url = {https://co-design.pop-coe.eu/patterns/openmp-critical-section.html},
   year = {2020}
}


@misc{stlThreadSafe,
   author = {Stack Overflow},
   title = {Are C++ STL containers thread-safe?},
   url = {https://stackoverflow.com/questions/1362110/is-the-c-stdset-thread-safe},
   year = {2009}
}
