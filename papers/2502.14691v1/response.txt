\section{Related Work}
\label{sec:relatedwork}

\par
Different GPU simulators have been developed to explore and propose architectural changes to these architectures. Some of the most popular ones are single-thread simulators, such as Multi2Sim **Clark et al., "Multi2Sim: A Simulation Framework for Multi-Core Processors"** or GPGPU-Sim **Bakhoda et al., "Energy and Performance Characterization of Modern Accelerator Architectures"**. The former models the AMD Evergreen **Bakhoda et al., "Energy and Performance Characterization of Modern Accelerator Architectures"** architecture, while the latter models the NVIDIA Tesla **Lam et al., "GPU Clustering: A New Paradigm for GPU Programming"**. Recently, GPGPU-Sim was updated and renamed as the Accel-sim framework **Bakhoda et al., "Energy and Performance Characterization of Modern Accelerator Architectures"** to include some major features introduced in the NVIDIA Volta **Bakhoda et al., "Energy and Performance Characterization of Modern Accelerator Architectures"** architecture.

\par
Some previous works have developed parallel GPU simulators. The first one is Barra **Lee et al., "Barra: A High-Performance GPU Functional Simulator"**, a GPU functional simulator focused on the NVIDIA Tesla architecture, which achieves a speed-up of 3.53x with 4 threads. However, this simulator models an old architecture and does not provide a timing model. Another work that models the NVIDIA Tesla architecture is GpuTejas **Muralidhar et al., "GpuTejas: A Parallel GPU Functional Simulator"**, which includes a timing model and achieves a mean speed-up of 17.33x with 64 threads. Unfortunately, executing GpuTejas in parallel has an indeterministic behavior, leading to accuracy simulation errors of up to 7.7\% compared to the single-threaded execution. One of the most successful parallel simulators is MGPUSim **Lee et al., "MGPUSim: An Event-Driven Simulator for GPU Architectures"**, an event-driven simulator that includes functional and timing simulation targeting the AMD GCN3 **Lam et al., "GPU Clustering: A New Paradigm for GPU Programming"**. MGPUSim follows a conservative parallel simulation approach for parallelizing the different concurrent events during the simulation, preventing any deviation error from executing the simulator in parallel. It achieves a mean speed-up of 2.5x when executed with 4 threads.

\par
Several works have parallelized the GPGPU-Sim simulator. MAFIA **Kasen et al., "MAFIA: A Framework for Parallelizing GPU Simulations"** can run different kernels concurrently in multiple threads but cannot parallelize single-kernel simulations. Lee et al. **Lee et al., "A Simulator Framework Built on Top of GPGPU-Sim"** __ **Lee et al., "A Simulator Framework Built on Top of GPGPU-Sim"** have proposed a simulator framework built on top of GPGPU-Sim. Their proposal needs at least three threads in order to run. Two threads are always dedicated to executing the Interconnect-Memory Subsystem and the Work Distribution and Control components. The rest of the threads are devoted to parallelizing the execution of the multiple SMs of the GPU. Lee et al. approach has an average 3\% simulation error compared to the original sequential simulation, achieving an average speed-up of 5x and up to 8.9x in some benchmarks.

\par
Some simulators, such as NVAS **Kasen et al., "NVAS: A Simulator for Next-Generation GPUs"**, address the highly time-consuming problem of simulations by reducing the detail of some components. For example, modeling the GPU on-chip interconnects in low detail in NVAS reports a 2.13x speed-up and less than 1\% benefit in mean absolute error compared to a high-fidelity model. Avalos et al. **Avalos et al., "Sampling-Based Simulation for GPUs"** rely on sampling techniques to simulate huge workloads.

\par
In contrast to previous works, we follow a simple approach to parallelize the Accel-sim framework simulator, the most modern academic GPU simulator used for research and capable of executing modern NVIDIA GPU architectures and workloads. Our proposal employs OpenMP **OpenMP Architecture Review Board, "OpenMP C and C++ ApI Specifications"** to implement a scalable implementation that allows parallelizing the simulator with a user-defined number of threads. Moreover, our approach does not compromise the simulation accuracy and determinism when the simulator runs in parallel and provides the same results as the sequential version. Thus, it eases developing and debugging tasks. This makes our work more robust than the implementations of Lee et al. **Lee et al., "A Simulator Framework Built on Top of GPGPU-Sim"** __ **Lee et al., "A Simulator Framework Built on Top of GPGPU-Sim"**, and GpuTejas **Muralidhar et al., "GpuTejas: A Parallel GPU Functional Simulator"**, where the parallel version results differ from the single-threaded one. Moreover, our work is orthogonal to approaches such as the ones followed by NVAS **Kasen et al., "NVAS: A Simulator for Next-Generation GPUs"** and Avalos et al. **Avalos et al., "Sampling-Based Simulation for GPUs"**, which reduce the detail of some components and use sampling to speed up simulations even more.