\section{Evaluation}\label{sec:evaluation}

\par
This section evaluates the performance benefits of parallelizing the Accel-sim framework simulator \cite{accelsim}. First, we describe our evaluation methodology to measure the speed-up of the parallel simulator. Then, we present a sensitive analysis of how the speed-up changes depending on the number of threads devoted to the execution. Finally, we analyze the impact of the for-loop OpenMP scheduler in the simulator.

\subsection{Methodology}

\par
We have configured the simulator with the parameters shown in \autoref{tab:gpu_specs}, which represent an NVIDIA RTX 3080 TI GPU based on the Ampere architecture. 

\begin{table}
    \centering
    %\tiny
    \begin{tabular}{ |c|c|  }
    \hline
shs 
Parameter & Value \\
    \hline
    Core Clock & 1365 MHz \\
    Mem. Clock & 9500 MHz \\
    \# SM & 80 \\
    \# Warps per SM & 48  \\
    Total Shared memory/L1D per SM & 128 KB \\
    \# Mem. part. & 24 \\
    Total L2 cache  & 6 MB \\
    \hline
    \end{tabular}
    \caption{NVIDIA RTX 3080 TI GPU simulator parameters.}
    \label{tab:gpu_specs}
\end{table}

\par
\autoref{tab:benchmarks} lists the different benchmark suites that we have employed to measure the efficacy of the parallelization. They represent a variety of very commonly used benchmarks that exhibit different degrees of parallelism.

\begin{table}
    \centering
    %\tiny
    \begin{tabular}{ |c|  }
    \hline
    \textbf{Rodinia 3.1} \cite{rodinia} \\
    \hline
    \multirow{2}{=}{gaussian (gau), hotspot (hot), hybridsort (hyb), lavaMD, lud, myocyte (myo), nn, nw, pathfinder (path), srad\_v1 (srad)} \\
    \\
    \hline
    \textbf{Polybench} \cite{polybench} \\
    \hline
    \multirow{1}{=}{fdtd2d, syrk} \\
    \hline
    \textbf{Lonestar} \cite{lonestar} \\
    \hline
    \multirow{1}{=}{mst, sssp} \\
    \hline
    \textbf{Deepbench} \cite{deepbenchWeb} \\
    \hline
    \multirow{1}{=}{conv, gemm, rnn} \\
    \hline
    \textbf{Cutlass} \cite{cutlass} \\
    \hline
    \multirow{1}{=}{2560x16x2560 (cut\_1), 2560x1024x2560 (cut\_2)} \\
    \hline
    \end{tabular}
    \caption{Benchmarks.}
    \label{tab:benchmarks}
\end{table}

\par
All the simulations have been executed in a cluster of homogeneous nodes with the specifications shown in \autoref{tab:node_specs}.

\begin{table}
    \centering
    %\tiny
    \begin{tabular}{ |c|c|  }
    \hline
    \multicolumn{2}{|c|}{CPU} \\
    \hline
    Model & AMD Epyc 7401P \\
    Cores & 24 \\
    Threads & 48 \\
    Frequency & 2 GHz \\
    \hline
    \multicolumn{2}{|c|}{RAM} \\
    \hline
    Total size & 128 GB \\
    Technology & DDR4 \\
    \hline
    \end{tabular}
    \caption{Node specifications.}
    \label{tab:node_specs}
\end{table}

\subsection{Parallel Speed-Up}

\par
This subsection analyzes how the speed-up evolves depending on the number of threads used by the simulator. 

\par
\autoref{fig:eval_sensitivity} shows the speed-up obtained with 2, 4, 8, 16, and 24 threads, averaging 1.72x, 2.64x, 3.95x, 5.83x, and 7.08x, respectively, against the single-threaded simulator. Executing the simulator with more than eight threads is less efficient: the efficiency is 0.36 for 16 threads, and 0.3 for 24 threads. However, some specific benchmarks, such as \textit{lavaMD}, significantly benefit from this high number of threads, reaching a speed-up of \summaryMaxSpeedup{} and an efficiency of 0.88 with 16 threads. This speed-up reduces the simulation slowdown compared to real hardware from 10,748,031x of the single-threaded simulator to 766,423x of the parallel one. Moreover, this benchmark is one of the most benefited from parallelization as it achieves super speed-up with 2, 4, and 8 threads configurations.


\begin{figure*}[ht]
  \centering
  \includegraphics[trim={1.4cm 0.6cm 0.6cm 0.6cm},clip,width=17.5cm]{charts/main_speedup.pdf}
  \caption{Speed-up with a different number of threads against the single-threaded version.}
  \label{fig:eval_sensitivity}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[trim={0.6cm 0.6cm 0.6cm 0.6cm},clip,width=17.5cm]{charts/schedulers_speedup.pdf}
  \caption{Speed-up obtained with the dynamic and static OpenMP for-loop scheduler with 2 and 16 threads against the single-threaded version.}
  \label{fig:eval_openmp_scheduler}
\end{figure*}

\begin{figure}[ht]
  \centering
  \includegraphics[trim={0.6cm 0.6cm 0.6cm 0.6cm},clip,width=8cm]{charts/ctas_per_kernel.pdf}
  \caption{Number of CTAs per kernel.}
  \label{fig:ctas_per_kernel}
\end{figure}

\par
Other workloads, such as \textit{myocyte}, which has a tiny number of CTAs (thread blocks) per kernel (2), do not benefit from parallelization, and it is penalized by running the OpenMP interface, resulting in minor slowdowns. To understand why, we need to know that CTAs are distributed in a round-robin fashion among the GPU SMs. As there are only two CTAs, only two SMs are active during the simulation. Therefore, parallelizing the execution of the rest of the SMs is useless. As shown in \autoref{fig:ctas_per_kernel}, workloads usually have many more CTAs per kernel than \textit{myocyte}, and more than the GPU's number of SMs (80).

\par
Computing the correlation factor between the speed-up obtained with 16 threads and the time to execute a workload in a single-thread, reveals a strong positive correlation with a value of 0.78. This means that the more time the application needs to be simulated in a single-thread, the more benefit it gets from parallelizing the simulator.

\subsection{OpenMP scheduler}

\par
Previous works \cite{ompScheduling1} \cite{ompScheduling2} analyzed the impact of the OpenMP for-loop scheduler. There are two main OpenMP schedulers: static and dynamic. In a static scheduler, the iterations of a for loop are distributed statically to threads. Therefore, it has little overhead and fits perfectly in regular and balanced applications. On the other hand, the dynamic scheduler assigns work to the threads when they are idle. Thus, it fits better in unbalanced environments. However, the dynamic scheduler has bigger overheads than the static one because it distributes the iterations of the loop at runtime, so it performs worse in balanced environments.

\par
\autoref{fig:eval_openmp_scheduler} shows how the OpenMP scheduler affects the benefits of parallelization depending on the number of threads in use. Both schedulers are configured with a granularity of one in the iteration distribution.

\par
We can see that applications with a negligible number of CTAs per kernel, such as \textit{myocyte}, perform similarly and do not benefit from parallelism. However, other workloads with a small number of CTAs per kernel, such as \textit{cut\_1}, benefit a lot from having the dynamic scheduler. Concretely, \textit{cut\_1} goes from the 0.97x speedup with the static scheduler to 1.61x when running with two threads. Other regular and balanced benchmarks, such as \textit{cut\_2} or \textit{lavaMD}, consistently perform better with the static than with the dynamic, as it does not have the scheduler overheads. Finally, there are workloads such as \textit{sssp} that, depending on the number of threads, perform better with one scheduler or the other. 

%This is because increasing the number of threads decreases the criticality of load unbalance.
