\section{Related Work}
\label{sec:relatedwork}

\par
Different GPU simulators have been developed to explore and propose architectural changes to these architectures. Some of the most popular ones are single-thread simulators, such as Multi2Sim \cite{multi2sim} or GPGPU-Sim \cite{gpgpusimOriginal}. The former models the AMD Evergreen \cite{amdevergreen} architecture, while the latter models the NVIDIA Tesla \cite{teslaHotchips}. Recently, GPGPU-Sim was updated and renamed as the Accel-sim framework \cite{accelsim} to include some major features introduced in the NVIDIA Volta \cite{voltaPaper} architecture.

\par
Some previous works have developed parallel GPU simulators. The first one is Barra \cite{barra}, a GPU functional simulator focused on the NVIDIA Tesla architecture, which achieves a speed-up of 3.53x with 4 threads. However, this simulator models an old architecture and does not provide a timing model. Another work that models the NVIDIA Tesla architecture is GpuTejas \cite{gputejas}, which includes a timing model and achieves a mean speed-up of 17.33x with 64 threads. Unfortunately, executing GpuTejas in parallel has an indeterministic behavior, leading to accuracy simulation errors of up to 7.7\% compared to the single-threaded execution. One of the most successful parallel simulators is MGPUSim \cite{mgpusim}, an event-driven simulator that includes functional and timing simulation targeting the AMD GCN3 \cite{amdgcn3}. MGPUSim follows a conservative parallel simulation approach for parallelizing the different concurrent events during the simulation, preventing any deviation error from executing the simulator in parallel. It achieves a mean speed-up of 2.5x when executed with 4 threads.

\par
Several works have parallelized the GPGPU-Sim simulator. MAFIA \cite{mafia} can run different kernels concurrently in multiple threads but cannot parallelize single-kernel simulations. Lee et al. \cite{parallelGPUSim1} \cite{parallelGPUSim2} have proposed a simulator framework built on top of GPGPU-Sim. Their proposal needs at least three threads in order to run. Two threads are always dedicated to executing the Interconnect-Memory Subsystem and the Work Distribution and Control components. The rest of the threads are devoted to parallelizing the execution of the multiple SMs of the GPU. Lee et al. approach has an average 3\% simulation error compared to the original sequential simulation, achieving an average speed-up of 5x and up to 8.9x in some benchmarks.

\par
Some simulators, such as NVAS \cite{nvas}, address the highly time-consuming problem of simulations by reducing the detail of some components. For example, modeling the GPU on-chip interconnects in low detail in NVAS reports a 2.13x speed-up and less than 1\% benefit in mean absolute error compared to a high-fidelity model. Avalos et al. \cite{pcaKernelGpuSampling} rely on sampling techniques to simulate huge workloads.

\par
In contrast to previous works, we follow a simple approach to parallelize the Accel-sim framework simulator, the most modern academic GPU simulator used for research and capable of executing modern NVIDIA GPU architectures and workloads. Our proposal employs OpenMP \cite{openmp} to implement a scalable implementation that allows parallelizing the simulator with a user-defined number of threads. Moreover, our approach does not compromise the simulation accuracy and determinism when the simulator runs in parallel and provides the same results as the sequential version. Thus, it eases developing and debugging tasks. This makes our work more robust than the implementations of Lee et al. \cite{parallelGPUSim1} \cite{parallelGPUSim2}, and GpuTejas \cite{gputejas}, where the parallel version results differ from the single-threaded one. Moreover, our work is orthogonal to approaches such as the ones followed by NVAS \cite{nvas} and Avalos et al. \cite{pcaKernelGpuSampling}, which reduce the detail of some components and use sampling to speed up simulations even more.