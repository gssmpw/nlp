\section{Full-body Deepfakes}
\label{sec:fullbody_deepfakes}

The term "deepfake" refers to synthetic media in which a person in an existing image or video is replaced with someone else's likeness using artificial intelligence. Full-body deepfakes are advanced synthetic media creations that use deep learning algorithms to generate realistic videos of a person's entire body. Unlike traditional deepfakes that focus on facial manipulation, full-body deepfakes replicate gestures, movements, and postures, creating an illusion of a person performing actions they never did. Even though full-body deepfakes are not extensively explored as a separate field of study, techniques such as image animation, which uses deep learning to bring static images to life by inferring realistic motions, can be considered part of deepfakes. This technology uses models such as GANs and motion transfer algorithms to create a single image based on the motion patterns derived from a driving video. This process can make a person in a still photo appear to move, speak, or perform various actions, creating highly engaging and dynamic visuals resulting in deepfakes. 

\subsection{Performance Evaluation in Full-body Animation}
\label{sub:fullbody_metrics}

In evaluating motion transfer from the driving video to the source video in most state-of-the-art methods, both quantitative and qualitative analyses are employed, allowing a comprehensive assessment of the effectiveness of the motion transfer process. Quantitative analysis involves the use of various metrics to assess performance, and such metrics are listed below.  

\paragraph{L1 Distance ($L_1$)}

The L1 distance can be used to quantify the difference between the pixel values of the two videos frame by frame and can be calculated as in Equation \ref{eq:fullbody_l1} where $I_1$, $I_2$, $m$, $n$, and $T$ refer to ground-truth frame, generated frame, width of the frame, height of the frame image and number of frames in the video respectively.

\begin{equation}
    L_1 = \Sigma_{t=1}^T \Sigma_{i=1}^m \Sigma_{j=1}^n | I_1 (i,j) - I_2 (i,j) |
    \label{eq:fullbody_l1}
\end{equation}


\paragraph{Average Keypoint Distance (AKD)}

The average key point distance in animation generation is a measure used to evaluate how accurately the generated animation matches the ground truth regarding specific key points, such as facial landmarks, joint positions, or other significant points. First, the key points which are represented as coordinates in the image space are extracted from both the ground truth and the generated animation frame. The Euclidean distance is calculated between the corresponding key points of ground truth and generated images, followed by averaging over all the pairs. This metric provides a meaningful way to assess the quality of the generated animation by focusing on how well it replicates the key structural features of the ground truth. $AKD$ can be calculated as illustrated in Equation \ref{eq:fullbody_akd} where $T$, $N$ and $K$ refer to the number of frames in the video, the number of key points in a frame and the key points ($K(x,y)$) respectively.

\begin{equation}
    AKD = \frac{1}{T} \Sigma_{t=1}^T \frac{1}{N} \Sigma_{i=1}^N || K_1^t(i) - K_2^t(i)||
    \label{eq:fullbody_akd}
\end{equation}

\paragraph{Missing Keypoint Rate (MKR)}

The Missing keypoint rate (MKR) in animation generation is a metric that quantifies the frequency at which key points are not detected or generated in the animation compared to the ground truth. First, the key points are detected in both ground truth and generated images, and a threshold $d_{th}$ is used to determine whether a key point is detected or not. The key point is considered missing if the distance between the ground truth key point and the generated key point exceeds $d_{th}$. Then, the number of key points in the ground truth that do not have a corresponding generated key point within the threshold distance is counted over all the frames and averaged to obtain $MKR$ (refer to Equation \ref{eq:fullbody_mkr}). 

\begin{equation}
    MKR = \frac{1}{N \times T} (\Sigma_{t=1}^T \Sigma_{i=1}^N ||K_1^t(i) - K_1^t(i) || > d_{th})
    \label{eq:fullbody_mkr}
\end{equation}

\paragraph{Average Euclidean Distance (AED)}

The Average Euclidean Distance (AED) calculates the average Euclidean distance in feature embedding between the ground truth representation and the generated video. The chosen feature embedding assesses how well the identity is maintained. Public re-identification networks for bodies extract identity from reconstructed and ground truth frame pairs and then compute the mean L2 norm of their difference across all pairs. \cite{siarohin2021motion}.

\subsubsection{Datasets for Full-body Deepfake (Video Animation) Generation}

Most state-of-the-art methods in the next section are evaluated using the Tai-Chi-HD and TED-Talks datasets. The Tai-Chi-HD dataset comprises 3048 (2884 in \cite{guo2024human}) and 285 video chunks for training and testing, extracted from 252 training and 28 testing videos, respectively \cite{hong2022qs, siarohin2019first}. The video length varies from 128 to 1024 frames \cite{siarohin2019first}. The TED-Talks dataset consists of videos from TED talks available on YouTube, with 1132 training videos and 131 test videos (1255 in \cite{tao2022motion}) \cite{guo2024human}. The number of frames per video ranges from 64 to 1024 \cite{guo2024human, zhao2022thin, siarohin2021motion}. The slight variations in dataset distributions mentioned above might have caused differences in the performances of different state-of-the-art models, as shown in Table \ref{tab:fullbodydatasets}. Furthermore, a low-resolution Tai-Chi dataset \cite{tulyakov2018mocogan} was also used in earlier evaluations which consisted of 4500 tai-chi video clips split into 3288 and 822 videos for training and testing sets respectively. The video length of the Tai-Chi dataset varied from 32 to 100 frames \cite{siarohin2019animating}. The Tai-Chi dataset consisted of low-resolution frames (64 $\times$ 64), while Tai-Chi-HD included frames with resolutions of 256 $\times$ 256 and 512 $\times$ 512. The TedTalks dataset mainly utilized frames with a resolution of 512 $\times$ 512.

\subsection{Full-body Deepfake (Video Animation) Generation}
\label{subsec:fullbody_generation}

Motion transfer is a key technique in creating full-body deepfake videos. It involves taking the movements or poses from one person (the "driving source") and applying them to another person (the "target" or "source subject") so that the target subject mimics the exact movements of the driving subject. This is done while keeping the appearance of the target subject, essentially allowing one person’s body movements to be applied to another person. Recent advancements in deep learning, particularly in motion transfer, pose estimation, and video synthesis, have led to the development of models capable of generating high-quality full-body deepfake videos. Two commonly used datasets for full-body deepfake generation and motion transfer are the TaiChiHD dataset and the TED Talks dataset. The TaiChiHD dataset is primarily used for generating full-body animations that capture precise, fluid human motions, such as Tai Chi movements. It focuses on complex poses and transitions between them. Models trained on TaiChiHD can learn to transfer precise body movements from one individual to another, ensuring that synthesized videos maintain the accuracy of the poses and gestures while adapting the appearance of the subject. The TEDTalks dataset can be used to extract and model the dynamic body language and physical gestures exhibited by speakers. These movements can then be transferred to another target subject, creating a video where the target mimics the gestures and body language of the TEDTalks speaker. Some state-of-the-art techniques in motion transfer methods are given in Table \ref{tab:fullbodydatasets} (summarised in Figure \ref{fig:taichihd} and \ref{fig:tedtalks}) and are discussed in Section \ref{subsubsec:fullbody_gen}.

\subsubsection{Literature Review on Full Body Deepfake Generation}
\label{subsubsec:fullbody_gen}

Monkey-Net (MOviNg KEYpoints) \cite{siarohin2019animating} is a groundbreaking model used for motion transfer and creating realistic deepfake animations. It transfers motion from a driving video to a target image to create animations using deep learning. The network included a keypoint detector trained to extract object keypoints, a dense motion prediction network for generating dense heatmaps from sparse keypoints to better encode motion, and a motion transfer network that uses the motion heatmaps and appearance information from the input image to create the output frames. Siarohin \textit{et al} \cite{siarohin2021motion} introduced a new approach for motion transfer (MRAA), which does not rely on keypoint detectors. They developed a network that can identify object parts, track them in a video, and infer their motions based on their principal axes. Unlike previous methods that used keypoint detectors, this approach focuses on extracting meaningful and consistent regions corresponding to semantically relevant and distinct object parts easily detected in the video frames. To separate foreground from background, they incorporated an additional affine transformation to model non-object-related global motion. They also disentangled the shape and pose of objects in the region space to facilitate animation and prevent the leakage of the driving object's shape. Since MRAA uses local affine transformations near the keypoints to estimate the motion, the temporal continuity of the reconstructed video depends on the smoothness of the
keypoints change. If the location of the key points in two adjacent frames changes greatly, it can cause pixel jitter \cite{zhao2022thin}. In the TPS-MM method proposed by Zhao \textit{et al.}, the use of Thin-Plate-Spline (TPS) motion estimation alleviated previous issues by generating each transformation using multiple keypoints, thus increasing the robustness of motion estimation. Before the TPS-MM method \cite{zhao2022thin}, motion transfer on arbitrary objects was mainly conducted using unsupervised methods without prior knowledge. However, these unsupervised methods often failed when there was a large pose gap between the objects in the source and driving images. As a solution, TPS-MM proposes an end-to-end unsupervised motion transfer framework that utilizes thin-plate spline motion estimation to create a more flexible optical flow, which warps the feature maps of the source image to the feature domain of the driving image. Additionally, to restore the missing regions, they propose multi-resolution occlusion masks to achieve more effective feature fusion. Despite significant advancements in the field of motion transfer, challenges such as distorted limbs and missing semantics persist due to the complex representation of motion and the unknown correspondence between human bodies. To address these challenges, Guo \textit{et al.} \cite{guo2024human} proposed a novel, semantically guided, unsupervised method of motion transfer that utilizes semantic information to model motion and appearance. This approach involved utilizing a pre-trained human parsing network to encode rich foreground semantic information, enabling the generation of fine details. Additionally, an attention-based network layer was proposed to learn the semantic region's correspondence between human bodies, guiding the network in selecting appropriate input features and ultimately leading to more accurate results.

\begin{table*}[htbp]
\caption{\textcolor{black}{State-of-the-art Video Animation Generation Methods (Video reconstruction). We have highlighted the performance of each method as given in the original paper. However, a few evaluation metrics on the TedTalks dataset and all FID values were referred from HIA-SG \cite{guo2024human}, and referred through $^\dagger$. $^{\ast}$ represents performance on Tai-Chi dataset.}}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|P{95pt}|P{27pt}|P{27pt}|P{27pt}|P{27pt}|P{27pt}||P{27pt}|P{27pt}|P{27pt}|P{27pt}|P{27pt}|P{27pt}|}
\hline
\multirow{2}{*}{Method} & \multicolumn{5}{c||}{TaiChiHD} & \multicolumn{5}{c|}{TedTalk}\\\cline{2-11}
 & $L_1$ & $AKD$ & $MKR$ & $AED$ & $FID$ & $L_1$ & $AKD$ & $MKR$ & $AED$ & $FID$ \\\hline\hline
HIA-SG \cite{guo2024human} & 0.049 & 4.34 & 0.016 &  0.154 & 25.57$^\dagger$ & 0.030 & 3.38 &  0.007 & 0.138 & 25.78$^\dagger$\\\hline
TPS-MM \cite{zhao2022thin} & 0.045 & 4.57 & 0.018 &  0.151 & 24.29$^\dagger$ & 0.027 & 3.39 &  0.007 & 0.124 &  23.28$^\dagger$ \\\hline
Motion Transformer \cite{tao2022motion} & 0.045 & 4.670 & 0.021 & 0.148 & - & 0.026 & 3.456 & 0.007 & 0.113 & - \\\hline
QS-Craft \cite{hong2022qs} & - & 4.61 & 0.017 & - & 25.064 & - & - & - & - & - \\\hline
FOMM \cite{siarohin2019first} & 0.063 & 6.862 & 0.036 & 0.179 & 28.08$^\dagger$ & 0.033$^\dagger$ & 7.07$^\dagger$ &  0.014$^\dagger$ &  0.163$^\dagger$ & 29.87$^\dagger$\\\hline
MRAA \cite{siarohin2021motion} &  0.047 & 5.58 & 0.027 & 0.152 & 25.74$^\dagger$ & 0.026 & 4.01 & 0.012 &  0.116 & 22.54$^\dagger$\\\hline
Monkey-Net \cite{siarohin2021motion} & 0.077 & 10.80 & 0.059 & 0.228 & - & - & - & - & - & - \\\hline
Monkey-Net$^{\ast}$ \cite{siarohin2019animating} & 0.050$^{\ast}$ & 2.53$^{\ast}$ & \textcolor{black}{17.4}$^{\ast}$ & 0.21$^{\ast}$ & - & - & - & - & - & - \\\hline
\end{tabular}}
\label{tab:fullbodydatasets}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures_new/TaiChi.pdf}
    \caption{\textcolor{black}{Performance variation of recent state-of-the-art methods in motion transfer to create full body deepfakes on Tai-Chi-HD dataset}}
    \label{fig:taichihd}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures_new/TedTalk.pdf}
    \caption{\textcolor{black}{Performance variation of recent state-of-the-art methods in motion transfer to create full body deepfakes on TedTalks dataset}}
    \label{fig:tedtalks}
\end{figure}


\textcolor{black}{Motion transfer technology is rapidly advancing, enabling the creation of highly realistic full-body deepfake animations by mapping motion from a video onto a static image. At its core, motion transfer involves generating representations of movement that can capture complex human motions and expressions, allowing animations to synchronize seamlessly with the source video. This technology synthesizes both appearance and motion in increasingly smooth and lifelike ways, facilitating the production of convincing animated content. By identifying key body parts and tracking their movements, these systems replicate subtle details in a subject’s posture, gestures, and interactions while maintaining coherence across frames. However, this level of realism raises important ethical questions regarding deepfake content. The ability to produce highly believable full-body deepfakes that mimic specific individuals can blur the line between reality and artificial content in media. As a result, the future of motion transfer technology in deepfakes will likely involve balancing creative applications with ethical considerations. Society must navigate the implications of increasingly realistic virtual representations on privacy, trust, and the authenticity of media.}

\subsubsection{Literature Review on Full Body Deepfake Detection} 

To the best of our knowledge, currently there is no method to detect full-body deepfakes.

\subsection{Combating Full-body Deepfakes in Gait Biometrics}
Gait biometrics is recently being adapted into security surveillance applications such as identifying and tracking individuals from CCTV footage. Therefore, it is important to investigate whether full-body deepfakes are capable of maintaining a subject's gait biometric features when animating a still image of that particular subject. In the following subsection, we discuss the summary of the findings
of our evaluation, and a detailed discussion is provided in Sec.
IV of supplementary material.

\subsubsection{Efficacy of full-body deepfakes to fool gait biometrics systems} Table IV in Sec. IV of supplementary material demonstrates the viability of full-body deepfakes to fool gait recognition models. Specifically, we observe substantial vulnerability when synthesising normal walking patterns of the target subject. Therefore, it is important to address this limitation of gait recognition methods. lease refer to Sec. IV of supplementary
material for additional details regarding this evaluation.

\subsubsection{Measures for revealing true identity: }To the best of our knowledge, currently there is no method to reveal true identity from full-body deepfakes.
