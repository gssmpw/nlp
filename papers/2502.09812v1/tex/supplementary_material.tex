\section{Summary of state-of-the-art face swap deepfake generation approaches}
In Tabs. \ref{tab:face_swap_summary} and \ref{tab:face_reenact_summary} we summarise the state-of-the-art face-swap and face-reenact deepfake generation methodologies, respectively, and discuss their strengths and weaknesses. 

\begin{table*}[htbp]
\caption{Summary of state-of-the-art face swap deepfake generation approaches. NA indicates that quantitative comparisons are not available.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{5cm}|p{5cm}|}
\hline
Method                  & Input Features                                                 & Architecture                    & Performance & Strengths                                                                                                           & Weaknesses                                                                                                                                                                                      \\ \hline
FCN \cite{nirkin2018face}           & Facial images and landmarks                                    & CNN                             &      FaceForensics++ \cite{rossler2019faceforensics++} 42.1 (LMD), 0.45  (ID)         & Introduces a semi-supervised pipeline for training the face segmentation model                                      & Requires extensive training data to train and the quality of the face swapping depends on the training data.                                                                                    \\ \hline
Face-swap GAN \cite{deepswapgan}  & Facial images and landmarks                                    & GAN                             &      NA       & Simple encoder-decoder-discriminator architecture. Reasonable handling of occlusion. Generate realistic eye regions & Can only swap faces between specific identities. Generates overly smoothed faces, face alignment is required.                                                                                   \\ \hline
DeepFaceLab \cite{perov2020deepfacelab}   & Facial images and landmarks                                    & GAN                             &    FaceForensics++ \cite{rossler2019faceforensics++}  0.73 (LMD)         & High resolution generations. Mature open access toolkit                                                             & The quality of the synthesisation is tightly coupled with the face segmentation quality.                                                                                                        \\ \hline
FSNet \cite{natsume2019fsnet}        & Facial images and landmarks                                    & VAE-based encoder-decode +  GAN &    FaceForensics++ \cite{rossler2019faceforensics++} 30.8 (LMD), 0.36 (ID)          & Subject-agnostic                                                                                                    & Cannot handle occlusions. Synthesised faces have poor resolution compared to resent state-of-the-art methods                                                                                    \\ \hline
RSGAN \cite{natsume2018rsgan}         & Facial images and landmarks                                    & Facial Region separator + GAN   &  CelebA \cite{liu2015deep} 1.127 (AED)         & Subject-agnostic. Can also be used to edit facial attributes                                                        & Synthesised faces have poor resolution compared to resent state-of-the-art methods                                                                                                              \\ \hline
FS-GANv2 \cite{nirkin2022fsganv2}     & Facial images and landmarks                                    & GAN                             &    FaceForensics++ \cite{rossler2019faceforensics++} 21.6 (LMD), 0.37 (ID)          & Can handle occluded faces. Capable of face swapping and face reenactment.                                           & Reliant on facial landmark detection which can sometimes produce erroneous landmarks. Iterative architecture that uses only one frame at a time which does not utilise any temporal information \\ \hline

Faceshifter \cite{li2019faceshifter}   & Facial images and facial attributes extracted from these faces & GAN                             &    FaceForensics++ \cite{rossler2019faceforensics++}  45.51 (LMD), 0.60 (ID)           & Visually appealing results with consistency in pose, expression, and lighting.                                      & Cannot generate high-resolution images. Iterative frame-by-frame processing which does not incorporate temporal information                                                                     \\ \hline
SimSwap \cite{chen2020simswap}       & Facial images                                                  & GAN                             &    FaceForensics++ \cite{rossler2019faceforensics++} 8.04 (EFD), 11.76 (FID)           & Effective injection of source identity,                                                                             & Cannot handle occlusions. Limited resolution in the synthesised faces                                                                                                                           \\ \hline
HiRFS \cite{xu2022high}         & Facial images and landmarks                                    & GAN                             &  FaceForensics++ \cite{rossler2019faceforensics++} 2.79 (EFD)           & Disentanglement of semantics within the latent space. Introduces specialised losses to enable temporal cohenerency. & The quality of the synthesised results depends on the latent codes produced by the  StyleGAN model. Therefore, not guaranteed to preserve the identity attributes of the source face            \\ \hline
MegaFS \cite{zhu2021one}       & Facial images                                                  & GAN                             &   FaceForensics++ \cite{rossler2019faceforensics++} 2.96 (EFD)          & High resolution face swapping. Can manipulate multiple latent codes concurrently.                                   & The quality of the synthesised results depends on the latent codes produced by the  StyleGAN model                                                                                              \\ \hline
FaceDancer \cite{rosberg2023facedancer}    & Facial images                                                  & GAN                             &    FaceForensics++ \cite{rossler2019faceforensics++} 7.97 (EFD), 16.30 (FID)          & Better preservation of facial attributes of the source face.                                                        & Limited resolution in the synthesised faces. Limited robustness in occlusions and in poor lighting conditions.                                                                                 \\ \hline
\end{tabular}}
\label{tab:face_swap_summary}
\end{table*}

\begin{table*}[htbp]
\caption{Summary of state-of-the-art face reenactment deepfake generation approaches. NA indicates that quantitative comparisons are not available. }
 \resizebox{\textwidth}{!}{%
\begin{tabular}{|p{2cm}|p{5cm}|p{2cm}|p{2cm}|p{5cm}|p{5cm}|}
\hline
Method                    & Input Features                         & Architecture & Performance & Strengths & Weaknesses \\ \hline
Face2Face \cite{thies2016face2face}       &    Facial images and landmarks                                    &      3D Morphable Face Models         &           NA  &   A semi-supervised architecture        & Cannot handle occlusions and different head poses           \\ \hline
ReenactGAN \cite{wu2018reenactgan}      &        Facial images                                 &    GAN          &   DISFA \cite{mavadati2013disfa} 58.4 \% (Facial Action Units Accuracy)       &   robust to different poses, expressions and lighting conditions        &     Output resolution is poor       \\ \hline
GANimation \cite{pumarola2018ganimation}      & Facial images and emotion action units &        GAN      &       NA      & Can handle complex backgrounds and illumination conditions          &     Unable to adapt to gaze variations       \\ \hline
FOMM \cite{siarohin2019first}            &    Sparse key-points                                    &   GAN           &  VoxCeleb2 \cite{nagrani2020voxceleb}   0.043 (L1)       &    Can handle complex motions and can animate diverse object types       &    Complexities when handling dynamic backgrounds        \\ \hline
Talking Heads \cite{zakharov2019few} &    Facial images and landmarks                                    &          GAN    &     VoxCeleb2 \cite{nagrani2020voxceleb} 30.6 (FID)        &   A few-shot learning architecture        &   Cannot manipulate gaze         \\ \hline
FC-TFG \cite{jang2023s}                 &    Facial images and audio                                    &           GAN   &    VoxCeleb2 \cite{nagrani2020voxceleb} 1.58 (LMD)         &  controllable head pose, eyebrows, eye blinks, eye gaze, and lip movements         &  Requires multimodal inputs          \\ \hline
Multimodal Talking Faces \cite{yu2020multimodal}                &   Source audio and target face video                                     &  GAN            &      In house Trump Dataset 0.889 (SSIM)       &     Audio driven reenactment      &  Only generates faces with limited pose variations          \\ \hline
EmoGen \cite{goyal2023emotionally}                 &        Audio, video and emotions                                &    GAN          &    CREMA-D \cite{cao2014crema} 6.04 (FID)         &  can generate faces with diverse emotions         &    has been evaluated with straight head poses        \\ \hline
AVFR-GAN \cite{agarwal2023audio}        &   Facial images and audio                                     &     GAN         &   VoxCeleb \cite{nagrani2017voxceleb} 8.48  (FID)        &  Generalises well to unseen faces         &     cannot handle occlusions       \\ \hline
PNCC GAN  \cite{xue2023high}                &    3D face                                    & GAN             &    VoxCeleb \cite{nagrani2017voxceleb} 17.21 (FID)         &       preserves target face identity    &  cannot handle extreme head poses          \\ \hline
\end{tabular}}
\label{tab:face_reenact_summary}
\end{table*}

\section{Summary of face deepfake detection methods}

In Tab. \ref{tab:detection_summary} we provide a summary of different face deepfake detection methods, highlighting their strengths and weaknesses.



%\begin{landscape}
\begin{table*}[htbp]
\caption{Summary of face deepfake detection approaches}
 \resizebox{\textwidth}{!}{%
\begin{tabular}{|p{2cm}|p{1cm}|p{5cm}|p{3cm}|p{5cm}|p{5cm}|}
\hline
Approach Category                  & Method                                               & Main Features                                                                                                          & Best Performance                                                            & Strengths                                                                                                                                                                                                                                   & Weaknesses                                                                                            \\ \hline
\multirow{4}{*}{Hand-crafted}      & \cite{koopman2018detection}         & Photo Response Non-Uniformity                                                                                          & High correlation for bonafide images than deepfakes in a self-build dataset & A simple feature that can be efficiently extracted                                                                                                                                                                                          & Evaluations have been conducted using a self-build dataset. Cannot handle unseen deepfake categories.  \\ \cline{2-6} 
                                   & \cite{kharbat2019image}             & Histogram of Gradient                                                                                                  & ACC=0.94 in UADFV dataset \cite{xie2020deepfake}                          & A simple feature that can be efficiently extracted                                                                                                                                                                                          & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{xia2022towards}               & texture difference from the colour channels                                                                            & AUC=0.99 in FF++ dataset \cite{rossler2019faceforensics++}                           & The framework is interpretable.                                                                                                                                                                                                             & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{wang2022ffr_fd}              & Speeded Up Robust Features (SURF), Scale-Invariant Feature Transform (SIFT), and Oriented Fast and Rotated Brief (ORB) & AUC=0.99 in DF-TIMIT(LQ) dataset \cite{korshunov2018deepfakes}                   & Can generalise to unseen datasets and different face deepfake generation methods                                                                                                                                                            & Can only handle face-swap deepfakes.                                                                  \\ \hline
\multirow{9}{*}{Artefacts}         & \cite{yang2019exposing}             & 3D head pose                                                                                                           & AUC=0.89 in UADFV dataset \cite{xie2020deepfake}                          & The framework is interpretable.                                                                                                                                                                                                             & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{xu2021deepfake}               & Gray-Level Co-occurrence Matrix                                                                                        & ACC=0.94 in DF-TIMIT(HQ) dataset \cite{korshunov2018deepfakes}                   & A simple feature that can be efficiently extracted                                                                                                                                                                                          & Can only handle face-swap deepfakes. Cannot generalise to unseen datasets.                            \\ \cline{2-6} 
                                   & \cite{kingra2022lbpnet}             & Local Binary Pattern (LBP)                                                                                             & AUC=0.99 in FF++ \cite{rossler2019faceforensics++} dataset                           & A simple feature that can be efficiently extracted. Can be used to detect both face swap and face reenactment categories. Can generalise to unseen datasets and different face deepfake generation methods. The framework is interpretable. & Cannot handle unseen deepfake categories.                                                             \\ \cline{2-6} 
                                   & \cite{agarwal2019protecting}        & head poses, facial landmarks, and expression                                                                           & AUC=0.96 in a self-build dataset                                            & The framework is interpretable. Can be used to detect both face swap and face reenactment categories.                                                                                                                                       & Evaluations have been conducted using a self-build dataset. Cannot handle unseen deepfake categories. \\ \cline{2-6} 
                                   & \cite{nguyen2020eyebrow}            & eyebrow                                                                                                                & AUC 0.88 in Celeb-DF dataset \cite{Celeb_DF_cvpr20}                        & Consistent performance using this feature as the input to different backbone feature extractors                                                                                                                                             & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{haliassos2021lips}            & mouth movement                                                                                                         & AUC=0.97 in DF1.0 \cite{jiang2020deeperforensics} dataset                          & Can be used to detect both face swap and face reenactment categories. Can generalise to unseen datasets and different face deepfake generation methods                                                                                      & The detection process is not interpretable                                                            \\ \cline{2-6} 
                                   & \cite{qi2020deeprhythm}             & Skin colour                                                                                                            & Acc=0.98 in FF++ \cite{rossler2019faceforensics++}                                   & Can be used to detect high-resolution face deepfakes                                                                                                                                                                                        & Can only handle face reenactment deepfakes. Cannot handle unseen deepfake categories.                 \\ \cline{2-6} 
                                   & \cite{fernandes2019predicting}      & oxygen concentration in the blood                                                                                      & Classification Loss of 0.0215 in a self-build dataset                       & Can be used to detect high-resolution face deepfakes                                                                                                                                                                                        & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{ciftci2020fakecatcher}        & remote PhotoPlethysmoGraphy                                                                                            & Acc=0.97 in UADFV \cite{xie2020deepfake} dataset                          & Can be used to detect both face swap and face reenactment categories.                                                                                                                                                                       & Cannot generalise to unseen datasets.                                                                 \\ \hline

%----
\multirow{9}{*}{Deep Learning}     & \cite{afchar2018mesonet}            & Deep features extracted from a Capsule Network architecture                                                            & AUC=0.91 in a self-build dataset                                            & Can be used to detect both face swap and face reenactment categories.                                                                                                                                                                       & Cannot generalise to unseen datasets.                                                                 \\ \cline{2-6} 
                                   & \cite{kumar2020detecting}           & Deep features from ResNet-18                                                                                           & Acc=0.99 in FF++ dataset \cite{rossler2019faceforensics++}                           & A simplified framework for deepfake detection                                                                                                                                                                                               & Can only handle face reenactment deepfakes. Cannot handle unseen deepfake categories.                 \\ \cline{2-6} 
                                   & \cite{rana2020deepfakestack}        & Deep features from XceptionNet, MobileNet, ResNet101, InceptionV3, DensNet121, InceptionReseNetV2, and DenseNet169     & Acc=0.99 in a self-build dataset                                            & Can be used to detect both face swap and face reenactment categories.                                                                                                                                                                       & Cannot generalise to unseen datasets.                                                                 \\ \cline{2-6} 
                                   & \cite{guera2018deepfake}            & Deep features from a CNN + LSTM framework                                                                              & Acc=0.97 in a self-build dataset                                            & A simplified framework for deepfake detection in videos                                                                                                                                                                                     & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{nguyen2021learning}           & Deep features from a 3DCNN                                                                                             & Acc=0.99 in VidTIMID(HQ) \cite{sanderson2009multi} dataset                   & A simplified framework for deepfake detection in videos                                                                                                                                                                                     & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{wodajo2021deepfake}           & Deep features from a Convolutional Vision-Transformer                                                                  & Acc=0.93 in FF++ dataset \cite{rossler2019faceforensics++}                           & Can be used to detect both face swap and face reenactment categories.                                                                                                                                                                       & Cannot generalise to unseen datasets.                                                                 \\ \cline{2-6} 
                                   & \cite{mittal2020emotions}           & Deep learned emotion features extracted from audio and video                                                           & ACC=0.96 in DF-TIMIT(LQ) dataset \cite{korshunov2018deepfakes}                   & Can be used to detect both face swap and face reenactment categories. The framework is interpretable.                                                                                                                                       & Cannot generalise to unseen datasets.                                                                 \\ \cline{2-6} 
                                   & \cite{chugh2020not}                 & Deep features from 3D- ResNetand audio features from Mel-Frequency Cepstral Coefficients                               & ACC=0.97 in DF-TIMIT(LQ) dataset \cite{korshunov2018deepfakes}                   & The framework is interpretable.                                                                                                                                                                                                             & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{zhou2021joint}                & Deep Learned synchronisation features extracted from audio and video streams                                           & Acc=0.99 in FF++ dataset \cite{rossler2019faceforensics++}                           & Can be used to detect both face swap and face reenactment categories. Can generalise to unseen datasets. The framework is interpretable.                                                                                                    & Cannot handle unseen deepfake categories.                                                             \\ \hline
\multirow{4}{*}{Anomaly Detection} & \cite{khodabakhsh2020generalizable} & logarithmic probability of observing a particular pixel's intensity                                                    & Acc=0.98 in FF++ dataset \cite{rossler2019faceforensics++}                           & Can be used to detect both face swap and face reenactment categories. Can generalise to unseen datasets. The framework is interpretable.                                                                                                    & Cannot handle unseen deepfake categories.                                                             \\ \cline{2-6} 
                                   & \cite{wang2020exposing}             & local motion patterns                                                                                                  & Acc=0.98 in FF++ dataset \cite{rossler2019faceforensics++}                           & Can be used to detect both face swap and face reenactment categories. The framework is interpretable.                                                                                                                                       & Cannot generalise to unseen datasets.                                                                 \\ \cline{2-6} 
                                   & \cite{khalid2020oc}                 & Video reconstruction                                                                                                   & F1=0.98 in DFD dataset \cite{bhat2024dfda}                             & Can generalise to unseen datasets.                                                                                                                                                                                                          & Can only handle face-swap deepfakes. Cannot handle unseen deepfake categories.                        \\ \cline{2-6} 
                                   & \cite{cozzolino2023audio}           & Deep learned audio-visual features of authentic videos &  AUC=0.99 in DF-TIMIT dataset \cite{korshunov2018deepfakes}  & Can be used to detect both face swap and face reenactment categories.                                                                                                                                                                       & Cannot generalise to unseen datasets.                                                                 \\ \hline
%----

\end{tabular}}
\label{tab:detection_summary}
\end{table*}
%\end{landscape}



% %\begin{landscape}
% \begin{table*}[htbp]
% \ContinuedFloat 
% \caption{(Continued.) Summary of face deepfake detection approaches}
%  \resizebox{\textwidth}{!}{%
% \begin{tabular}{|p{2cm}|p{1cm}|p{5cm}|p{3cm}|p{5cm}|p{5cm}|}
% \hline
% Approach Category                  & Method                                               & Main Features                                                                                                          & Best Performance                                                            & Strengths                                                                                                                                                                                                                                   & Weaknesses                                                                                            \\ \hline


% \end{tabular}}
% \end{table*}
% %\end{landscape}

\section{Face deepfakes Biometric Evaluation}
In this section, we provide quantitative evaluations to demonstrate the ability of state-of-the-art face deepfake generation methods to fool advanced face recognition models.Face recognition systems are readily applied in numerous security-critical applications such as border control, authentication for banking apps, patient identification systems, and home automation. It should be noted that it is difficult for deepfakes to fool the physical biometric recognition systems such as systems used in border control. However, there exists evidence \footnote{https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html} that sophisticated deepfake technology can fool online authentication systems such as mobile-based personal authentication systems, as such, it is important to investigate the biometric implications of off-the-shelf face deepfake technology.

\subsection{Efficacy of face deepfakes to fool face biometrics systems}

\subsubsection{Evaluation Protocol}
In this evaluation, we follow a protocol similar to the one used in Sec. I.A. Specifically, we created face deepfakes using state-of-the-art face deepfakes generation methods: Wav2Lip \cite{chung2017out},  MCNet \cite{hong2023implicit}, First Order Motion Model \cite{siarohin2019first}, SimSwap \cite{chen2020simswap}, and FSGAN \cite{nirkin2022fsganv2}. From the training set of the Voxceleb2 \cite{nagrani2020voxceleb} dataset, we selected 36 subjects (with equal proportions of male and female subjects) and randomly selected 25 sample videos from each of those subjects. These 36 subjects were randomly paired as source and target faces and out of 25 sample videos that are available for each subject, 24 videos were selected to generate face deepfakes using both face swapping and face reenactment procedures. In the face swapping setting the facial components in the target face are replaced using source face features. In the face reenactment setting, the source video's expressions are replicated in the target video. 

State-of-the-art face recognition models, irse50 \cite{hu2018squeeze}, Facenet \cite{schroff2015facenet}, mobile face \cite{chen2018mobilefacenets}, ir152 \cite{deng2019arcface} and deep face \cite{serengil2024lightface} are used to verify the quality of the synthesised faces to fool the biometric systems in the face verification setting. Specifically, for face swap methods, the remaining video of the source subject out of the 25 video samples is used as the enrollment sample, and the generated 24 deepfake faces are verified biometrically against this sample. In contrast, in the face reenactment setting, the remaining video of the target subject is used for enrollment. 

\subsubsection{Evaluation Metric}
The Attack Success Rate (ASR) \cite{deb2020advfaces, zhong2020towards} is widely considered the evaluation metric to evaluate the effectiveness of attacks on face recognition methods. Let $F$ denote the backbone feature extractor of the face recognition model, $I_e$ denote the enrolled face image and $I_t$ denote the target image. Then, success rate, SR, can be defined as,

\begin{equation}
    SR = \frac{\sum_{i}^{N}1_{\tau}(cos[F(I^i_e), F(I^i_t)] > \tau)}{N} \times 100\%,
\end{equation}

where $N$ is the total number of image pairs that are being evaluated, $cos[X, Y]$ is a function that accepts two feature vectors and computes the cosine similarity between the vectors, and $\tau$ is a threshold that is being set based on the False Acceptance Rate (FAR) of the face recognition model.

However, more insights regarding the generated attacks under different circumstances can be generated by investigating the impact of different attack generation conditions on cosine similarity metric. Furthermore, different $\tau$ values should be utilised for different face recognition models to achieve a certain FAR. For instance, at 0.01 FAR for IR152 $\tau = 0.167$, IRSE50 $\tau =0.241$, MobileFace $\tau =0.302$ and Facenet $\tau =0.409$. Therefore, we also report the cosine similarity score between the vectors $I_v$ and $I_a$, which can be evaluated using,

\begin{equation}
    Similarity Score = \frac{F(I^i_e) . F(I^i_t)} {||F(I^i_e)|| \times ||F(I^i_t)||},
\end{equation}
where $.$ denotes the dot product of vectors.

\subsubsection{Results} 
Evaluation results of face deepfakes on face biometrics are presented in Tab. \ref{tab:face_evals}. The evaluations demonstrate the ability of deepfake methods such as Wav2Lip, SimSwap, and First-order-model to fool the biometric recognition systems, especially the lightweight systems such as MobileNet. This vulnerability is significantly concerning due to the vast utilisation of lightweight face verification methods for authentication in applications such as mobile device unlocking, app login and payment gateways, and in social media apps for photo tagging. 

\begin{table*}[htbp]
\caption{Evaluation of the efficacy of face deepfakes to thwart face biometrics systems. We evaluate both face swap methods (SimSwap, and FSGAN) and face reenactment methods (Wav2Lip, MCNet, and First Order Motion Model).}
\label{tab:face_evals}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|cccc|cccc|cccc|cccc|cccc|}
\hline
\multirow{3}{*}{Model} & \multicolumn{4}{c|}{irse50}                                                                                            & \multicolumn{4}{c|}{Facenet}                                                                                           & \multicolumn{4}{c|}{Mobile Face}                                                                                       & \multicolumn{4}{c|}{ir152}                                                                                             & \multicolumn{4}{c|}{Deep Face}                                                                                        \\ \cline{2-21} 
                       & \multicolumn{3}{c|}{Success Rate}                                                  & \multirow{2}{*}{Similarity Score} & \multicolumn{3}{c|}{Success Rate}                                                  & \multirow{2}{*}{Similarity Score} & \multicolumn{3}{c|}{Success Rate}                                                  & \multirow{2}{*}{Similarity Score} & \multicolumn{3}{c|}{Success Rate}                                                  & \multirow{2}{*}{Similarity Score} & \multicolumn{3}{c|}{Success Rate}                                                 & \multirow{2}{*}{Similarity Score} \\ \cline{2-4} \cline{6-8} \cline{10-12} \cline{14-16} \cline{18-20}
                       & \multicolumn{1}{c|}{@01}  & \multicolumn{1}{c|}{@001} & \multicolumn{1}{c|}{@0001} &                                   & \multicolumn{1}{c|}{@01}  & \multicolumn{1}{c|}{@001} & \multicolumn{1}{c|}{@0001} &                                   & \multicolumn{1}{c|}{@01}  & \multicolumn{1}{c|}{@001} & \multicolumn{1}{c|}{@0001} &                                   & \multicolumn{1}{c|}{@01}  & \multicolumn{1}{c|}{@001} & \multicolumn{1}{c|}{@0001} &                                   & \multicolumn{1}{c|}{@01}  & \multicolumn{1}{c|}{@001} & \multicolumn{1}{c|}{@001} &                                   \\ \hline

SimSwap                & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.96                              & \multicolumn{1}{c|}{0.97} & \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.85}  & 0.89                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.97                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{0.99} & \multicolumn{1}{c|}{0.98}  & 0.88                              & \multicolumn{1}{c|}{0.95} & \multicolumn{1}{c|}{0.89} & \multicolumn{1}{c|}{0.83} & 0.79                              \\ \hline
FSGAN                  & \multicolumn{1}{c|}{0.99} & \multicolumn{1}{c|}{0.95} & \multicolumn{1}{c|}{0.83}  & 0.43                              & \multicolumn{1}{c|}{0.58} & \multicolumn{1}{c|}{0.31} & \multicolumn{1}{c|}{0.08}  & 0.29                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{0.99} & \multicolumn{1}{c|}{0.95}  & 0.56                              & \multicolumn{1}{c|}{0.63} & \multicolumn{1}{c|}{0.39} & \multicolumn{1}{c|}{0.23}  & 0.14                              & \multicolumn{1}{c|}{0.80} & \multicolumn{1}{c|}{0.24} & \multicolumn{1}{c|}{0.10} & 0.23                              \\ \hline
\hline

Wav2Lip                & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.94                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.95                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.95                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.855                             & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{0.95} & \multicolumn{1}{c|}{0.87} & 0.79                              \\ \hline
MCNet                  & \multicolumn{1}{c|}{0.99} & \multicolumn{1}{c|}{0.95} & \multicolumn{1}{c|}{0.79}  & 0.39                              & \multicolumn{1}{c|}{0.26} & \multicolumn{1}{c|}{0.05} & \multicolumn{1}{c|}{0.01}  & 0.14                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{0.92} & \multicolumn{1}{c|}{0.74}  & 0.43                              & \multicolumn{1}{c|}{0.19} & \multicolumn{1}{c|}{0.03} & \multicolumn{1}{c|}{0.02}  & 0.03                              & \multicolumn{1}{c|}{0.76} & \multicolumn{1}{c|}{0.01} & \multicolumn{1}{c|}{0.00} & 0.13                              \\ \hline
First Order Motion Model      & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.98                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.99                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.99                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{1.00}  & 0.94                              & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{0.95} & \multicolumn{1}{c|}{0.92} & 0.88                              \\ \hline

\end{tabular}}
\end{table*}

\section{Future research directions}
In this section, we outline the limitations of existing deepfake generation and detection techniques as well as various open research questions, and highlight future research directions.

\subsection{Universal deepfake detection methods}
Deepfake generation technology evolves over time and it is practically impossible to consider every possible generation type, that could be introduced in the future, into consideration. As such, developing universal deepfakes detection technology that could withstand not only the current deepfakes but also the advances that the deepfake generation methods would attain in the future is virtually impossible. 

Furthermore, the variations of domains and contexts make the generalisation of the deepfake detectors challenging. For instance, a universal deepfake detector should be generalisable to the low-quality media shared on social media, as well as high-quality deepfakes produced in the entertainment industry. Furthermore, it should be generalisable to different backgrounds, modalities, etc. These diverse real-world settings make it a significant challenge. 

We observe a further hindrance to achieving universal detection due to the limited data availability for model training. Specifically, the supervised training of a universal deepfake detector will require a myriad of training data that span various deepfake techniques, modalities, quality levels, resolutions, and content types, which are not readily available. Furthermore, the computational complexity of training a large-scale, complex deep learning model could impede the design of a universal detector. 

Future research efforts could be directed to address these challenges. One possible avenue for exploration is the use of biometric features for the generation of subject-specific deepfake detection. In addition to traditional biometric features, complementary behavioral biometric and linguistic features could be used to supplement the model learning. Moreover traditional machine learning tools and physics or biologically inspired architectures could be leveraged for the design of universal deepfakes detection methodologies.

\subsection{Recovering the true identity}

Legal and law enforcement are not the only applications that require revealing of the true identity of a person in a deepfake video. Revealing of the true identity helps mitigate the damage caused to a person's reputation and could stop the spread of malicious media. Furthermore, the systems that can validate media for the presence of deepfakes and recover the true identity of a person (if it is identified as deepfake media) could be embedded in social media platforms. Such actions could help build trust among the general public about the content shared on those platforms. 

To the best of our knowledge the only work that is capable of restoring the true identity of a subject in a deepfake video is proposed in the cyber immune system framework proposed in \cite{chang2023cyber}. However, there exist several limitations of this work and numerous future research directions. For instance, the authors of \cite{chang2023cyber} have pointed out that the recovery of the true identity is impossible in their framework if there exist major pose changes during the face reenactment. Furthermore, this model works only for the front-facing face images and produces inferior results under poor illumination conditions. 

Future research efforts could also be made towards recovering the true identity in multimodal deepfake videos. The existence of two or more modalities can be seen as mediums to embed and extract complementary identity features related to the true identity. Furthermore, we observe the possibilities of extending the framework in \cite{chang2023cyber} to the recovery of true identity in full-body deepfakes. Posture and body movements, hand gestures, facial expressions, and micro expressions all carry informative cues to recover the true identity of a person in a deepfake video, therefore, future efforts could be directed to investigate how those physical and behavioral characteristics can be incorporated into the cyber vaccine framework of \cite{chang2023cyber}.

\subsection{Explainable deepfakes detection methods}

Explainability is a paramount characteristic of any machine learning algorithm. As free online deepfake detection tools such as Resemble.ai deepfake detector \footnote{https://www.resemble.ai/free-deepfake-detector/}, Deepware deepfake scanner \footnote{https://scanner.deepware.ai/}, and the deepfake detector \footnote{https://deepfakedetector.ai/} are increasingly becoming popular among general public explainability has become a curial characteristic to maintain the trust and transparency of the detection results. The generated explanations will build trust and confidence among the users and the general public regarding the reliability and fairness of the detection process. Furthermore, they can be leveraged to train the users how to detect deepfakes without using such tools and elevate their literacy. Such education, which could be given to employees, would have a significant impact on strengthening protective measures and mitigating the risks of cyber espionage through impersonation.

Recently a few research efforts \cite{xu2022supervised, ishrak2022explainable, mathews2023explainable} have been made to preserve the explainability of the deepfake detection methods. However, more investigations along these lines are encouraged such that explainable detection of deepfakes can be achieved in all the categories of deepfake generation algorithms, including, voice, face, full-body, and multimodal deepfakes. 



\subsection{Introducing standard evaluation protocols} % especially with respect to the effectiveness in biometric recognition.

We observe inconsistencies in the evaluation metrics used in both deepfake generation and detection literature. For instance, in face deepfake generation several studies have used metrics such as average Euclidean distance between the feature vectors for real and generated faces, peak signal-to-noise ratio, pixel-wise distances with respect to facial landmarks, etc. In contrast, some studies \cite{wu2018reenactgan} utilise human surveys to compare the quality of the media that have been generated by different deepfake generation methods. Therefore a unified evaluation protocol is needed to properly assess and validate the effectiveness of a particular method. 

Similarly in face deepfake detection literature precision, recall, and F1-Score have been the most commonly used metrics for comparison. However, some studies have reported their performance using AUC and Error-Rate metrics which makes comparison across different state-of-the-art methods infeasible. 

\textbf{Accuracy:} measures the proportion of the samples in the test set that have been correctly classified by the detection algorithm.  \textbf{Precision: } measure the ratio of the correctly identified positive instances (eg. fake samples) out of the total positive samples that the model classified as positive which could be denoted as, \[\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}.\] Higher precision denotes that the model is making a lesser number of false positive identifications. On the other hand \textbf{Recall} determines the model's ability to correctly identify a positive data sample. It is calculated as the proportion of true positives out of all the positive samples in the test dataset and could be written as, \[\text{Recall: } = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}.\] Higher recall is a vital attribute for deepfake detection algorithms as misclassification of a true positive sample in a real-world application could be costly. \textbf{F1-Score: } could be calculated as \[\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}},\] which is the harmonic mean of both precision and recall metrics.  \textbf{AUC: } measures the ability of the model to distinguish the fake samples from the real samples and is calculated by integrating the ROC Curve. \textbf{Error Rate (EER):} represents the proportion of incorrectly classified samples out of the total number of samples in the test dataset. It could also be calculated as \[\text{EER} = 1 - \text{Accuracy}.\]

Standard evaluation metrics are essential to promote rigorous, transparent, and meaningful comparisons across different models proposed in the literature. They remove the subjectivity or the bias from the evaluation process which results in fair comparisons. Furthermore, standard metrics promote the independent reproducibility of the results so that they can be independently validated. Furthermore, for decision making such as the deployment of a certain model in industrial applications, the stakeholders need to compare metrics of different state-of-the-art methods. As such, it is important to maintain consistency in evaluation methods. 



\subsection{Design of a regulatory framework for the governance of deepfake research}
The researchers should also investigate and establish a regulatory framework for the governance of deepfake research. They should proactively collaborate with relevant stakeholders, including, governments, law enforcement authorities, and the general public to ensure that future research into deepfakes is conducted in a transparent and accountable manner, and addresses ethical and privacy concerns. Most importantly a governance framework will establish ethical guidelines and standards for the creation and distribution of deepfakes, and the public share of the implementations. As such, this framework has the potential of minimising the negative societal implications of deepfakes due to misuse. Furthermore, the establishment of such a regulatory framework would promote collaboration among researchers, industry stakeholders, policymakers, and advocacy groups which could foster innovation in a responsible manner. The process of designing governance protocols could also be used to enhance public education and awareness regarding both the merits and negative implications of deepfake technology which will help uphold societal trust in AI. 

In addition to a regulatory framework the researchers could look into a philosophical and moral framework to establish a set of principles, values and guidelines to make decision regarding conducting research in the domain of deepfakes. When establishing this framework the board societal impact of deepfakes should be taken into consideration. Fairness, integrity, and compassion will be among the foundational values upon which this framework will be built and researchers from different domains, including, legal, philosophical and technical research areas should collaborate when establishing the principles and best practices that encompass this framework. 
