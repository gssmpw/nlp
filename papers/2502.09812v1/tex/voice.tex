\section{Voice Deepfakes}\label{sec:voice_deepfakes}
%\textcolor{red}{RESPONSIBILITY - Ivan \& Darshana}

\subsection{Generation}\label{sec:voice_generation}

Voice deepfake or synthetic voice is a voice that closely resembles a real person’s voice. It refers to any voice that is generated using artificial intelligence (AI) technology and can accurately capture tonality, accents, cadence, and other unique characteristics of a real person’s voice. Fake voices can be used to create speech sentences that sound like real people saying things they did not say. Human voice synthesis can be broadly classified into text-to-speech (TTS) and voice conversion (VC). Figure~\ref{fig:voice_deepfakes} shows two popular pipelines for creating voice deepfakes.

\subsubsection{Voice Deepfake Types}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures_new/voice_deepfakes.pdf}
    \caption{Two popular methods for creating voice deepfakes: (a) Text-to-speech and (b) Voice conversion. [REDONE]}
    \label{fig:voice_deepfakes}
\end{figure*}

\paragraph{Text-to-Speech (TTS)}

A text-to-speech (TTS) system aims to create speech from a provided input text. Thanks to recent advances in deep learning and artificial intelligence, TTS systems can now produce realistic and natural-sounding speech using any speaker’s voice. Generally, modern TTS systems consist of three basic components \cite{surveytts}: a text analysis module, an acoustic model, and a vocoder. When the user inputs text into the TTS system, a text analysis module processes the text and extracts linguistic features. The acoustic model then generates acoustic features based on the linguistic features. Finally, the vocoder synthesizes speech from the predicted acoustic features. To better capture the characteristics of voices from various speakers, speaker embeddings extracted from the speaker encoder network are commonly used in the acoustic modelling process \cite{jia2018transfer}. In recent years, a transformer with a self-attention mechanism \cite{transformer} has gained widespread popularity as an architecture for acoustic models in neural TTS. Moreover, fully end-to-end models like FastSpeech 2s \cite{ren2021fastspeech}, VITS (Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech) \cite {vits}, and NaturalSpeech \cite{naturalspeech} have been specifically designed to directly generate speech waveforms from character or phoneme sequences, aiming to enhance optimization and reduce development time for various TTS components.

In recent times, Large Language Models (LLMs) have been introduced to improve the quality and speaker adaptation of TTS models even further. LLMs are fundamental machine learning models that are trained with massive amounts of text to understand, predict, and generate human language \cite {surveyllm}. 
For instance, several studies on TTS have utilized Bidirectional Encoder Representations from Transformers (BERT) embeddings to generate speech with improved prosody prediction, better pronunciation and expressiveness \cite{chen2021, kenter2020, xiao2020, xu2021}. Since BERT or BERT-like models learn contextual information from a large-scale text corpus, they can provide syntactic and semantic information to help in predicting the fundamental frequency \cite{yasuda2022}, pauses \cite{xiao2020}, and other prosodic characteristics \cite{moya2023a} of natural speech. The representations from large-scale Self-Supervised Learning (SSL) models such as HuBERT \cite{hsu2021hubert}, WavLM \cite{chen2022wavlm}, and Wav2vec 2.0 \cite{baevski2020wav2vec} are highly beneficial for speech generation tasks. They can be easily adapted to various non-automatic speech recognition tasks, including speaker identification, spoken language understanding, and speech emotion recognition \cite{ftwav2vec}. In the field of TTS, SSL features have been explored in prosody modelling \cite{zhang2023}, improving pronunciation accuracy in cross-lingual TTS \cite{cong2022}, noise-robust synthesis \cite{Siuzdak2022}, and zero-shot TTS \cite{hierspeech,fujita2023}. To achieve superior performance, recent TTS systems such as $\text{M}^{2}$-CTTS \cite{xue2023}, WavThruVec \cite{siuzdak22} and HierSpeech++ \cite{hierspeech} have used SSL representations as an additional semantic representation. StyleTTS2 \cite{styletts2} has also benefited from the SSL model through adversarial training.

The development of Generative Pre-trained Transformers (GPT) technology \cite{gpt} has inspired recent efforts to harness the capabilities of GPT-like auto-regressive models for building large-scale TTS systems. For instance, VALL-E \cite{valle} is the initial neural codec language model designed based on the architecture of GPT-3, featuring robust in-context learning abilities. VALL-E generates the discrete audio codec from phonemes representing the target content and acoustic code prompts extracted from the speaker's voice. It also can produce high-quality personalized voices and emotions based on short speech prompts. Its multilingual variant, VALL-E X \cite{vallex} introduces language ID for synthesizing cross-lingual speech, which supports speech-to-speech translation. LauraGPT (Listen, Attend, Understand, and Regenerate Audio with GPT) \cite{lauratts} adopts a decoder-only Transformer as a GPT backbone to perform unified audio and text modelling and can generate outputs in either modality. Furthermore, Cosyvoice \cite{cosyvoice} and FireRedTTS \cite{fireredtts} introduce semantic speech tokens for TTS modelling and formulate TTS as a next-token prediction task with text as prompts using the GPT-like decoder transformer.

While the auto-regressive model-based architecture has achieved promising results, inference latency is higher compared to non-autoregressive models because the codec token needs to be sampled sequentially for speech generation. Using the text-guided speech-infilling task \cite{le2023voicebox},  E2-TTS \cite{e2tts} and F5-TTS \cite{f5tts} predict masked speech given its surrounding audio and the text transcript in a non-autoregressive manner. During training, the text input is padded with filler tokens of the same length as input speech, simplifying the TTS systems without complex designs such as duration model, text encoder, and phone alignment.  

In voice generation scenarios, providing a text description to describe the intended voice attributes such as speaker identity, tone, speaking rate, and emotion, would be more user-friendly and offer greater voice customization \cite{guo2023, instruct_tts, prompt_tts2}. One notable example is Parler-TTS \cite{lacombe-etal-2024-parler-tts} which utilizes the MusicGen \textcolor{red}{\cite{copet2024simple}} architecture conditioned on embeddings extracted from the Flan-T5 text encoder through a cross-attention mechanism to control different speaker attributes using natural language descriptions \cite{lyth2024natural}.

\paragraph{Voice Conversion (VC)}

Voice Conversion (VC) is the process of transforming a source speaker's voice to sound like that of another speaker without changing the linguistic content. This change alters the speaker identity of the converted speech \cite{hamid2017}. There is a growing demand for various practical applications of VC technology, which can benefit consumers in many ways, such as personalized text-to-speech, voice dubbing, voice mimicry, and speaking aids for speech-impaired persons \cite{kain2007, biadsy2019}. However, VC can also be misused by malicious actors to mimic voices and manipulate what we hear for their advantage.

The VC system pipeline typically consists of three main phases \cite{Sisman2020}: (1) speech analysis and feature extraction phase where the speech from a source speaker is converted into features containing linguistic contents and disentangled from its speaker's characteristics, (2) feature mapping phase where the decomposed information is transferred into intermediate features that match the target speaker's characteristics, and (3) reconstruction phase where the modified intermediate features are transformed back into a speech waveform using a vocoder.
In the parallel voice conversion (VC) technique, the VC system is trained to learn a mapping function from the source speech to the target speech using a training set that contains parallel data. This means that the training set includes speech data of the same linguistic content from both the source and the target speakers \cite{Sisman2020}. However, the requirement for parallel data has often hindered the progress of parallel VC techniques because it is usually assumed that there is a limited amount of data available from both the source and target speakers.  In recent years, deep learning techniques have become increasingly effective in solving conversion problems using a mapping function with powerful regression capabilities, without the need for a parallel corpus (non-parallel VC). VC frameworks such as CyleGAN-VC \cite{cycleganvc}, StarGAN-VC \cite{starganvc}, and VAW-GAN \cite{vawgan} have been developed by incorporating generative adversarial networks to map various voice attributes. These techniques can achieve better voice quality and similarity to the target speaker when a large number of training examples are available.

The use of linguistic-related features to capture spoken content has been a popular approach for non-parallel VC \cite{flytek2018, tian2019}. These features can be derived from an Automatic Speech Recognition (ASR) system, such as bottleneck features or Phonetic PosteriorGrams (PPGs) \cite{ppg2016}. A PPG is a matrix that represents the posterior probabilities of the phonetic class corresponding to each time frame. It is common to employ speaker-independent ASR (SI-ASR) trained using a large multi-speaker corpus to obtain PPGs to accommodate for differences in speakers. Recently, large-scale self-supervised speech representations such as wav2vec 2.0, vq-wav2vec \cite{baevski2019vq}, HuBERT and WavLM have been used to replace the PPGs in voice conversion tasks \cite{polyak2021, huang2022, li2023freevc}. For instance, Free-VC \cite{li2023freevc}, an end-to-end model based on the VITS architecture designed for voice conversion, has used WavLM to encode linguistic content. Speaker information is acquired from a speaker encoder trained jointly with the rest of the components from scratch. Additionally, SR-based data augmentation is employed to enhance the quality of waveform generation.

Although continuous features extracted from self-supervised pre-trained models can capture linguistic, speaker, prosodic, and semantic information of speech, it remains a challenge to completely separate the content from the speaker identity, resulting in reduced speaker similarity of the converted speech. There are multiple methods in the literature that can be applied to SSL continuous features such as discretization \cite{polyak2021, softvc}, and speaker disentanglement mechanisms \cite{contentvec}. For example, ContentVec \cite{contentvec} introduces disentanglement modules in the mask prediction task of HuBERT without significant loss of content information. The content encoder in \cite{softvc} produces soft speech units, called HuBERT-Soft, which are learned by predicting a distribution over the discrete units extracted from HuBERT to achieve speaker disentanglement. These HuBERT-Soft features have been widely employed as content features in voice conversion tasks \cite{quickvc, rythmvc, streamvc}, including singing voice conversion \cite{singingvc, diffsvc}, and have notably improved speaker similarity.

%The method in~\cite{softvc} is proposed to apply k-means on the continuous features to generate discrete speech units as a mean to improve the speaker similarity.

\subsubsection{Deepfake Generation Process} 

\paragraph{Autoregressive Models}

The autoregressive model predicts the model behaviour of the current step using the data in the previous time steps. For example, in a linear autoregressive model of order $n_i$, the output $y[t]$ can be estimated by:

\begin{equation}
y[t] = \sum^{n_i}_{i=1} a_{i}x[t-i] + e[t]
\end{equation}

where $a_{i}$ is the model parameters, $e[t]$ is white noise or residual error with zero mean. In deep autoregressive models, the model parameters are estimated using deep neural networks. The dependency modelling of an input data sequence in an autoregressive manner allows the complex structure of data to be learned in a temporally more coherent way. However, an autoregressive model suffers from slow generation because it depends on the previous observations to predict the value at the current time step. 

Autoregressive structures have been adopted in many neural TTS systems to achieve state-of-the-art performance such as Tacotron 1/2 \cite{tacotron, tacotron2}, DurIAN \cite{durian}, and DeepVoice 3 \cite{deepvoice3}. One of the first neural-based vocoders for speech synthesis is WaveNet \cite{wavenet}. The autoregressive structure of WaveNet improves the continuity of the generated waveform because it generates one sample at a time.

\paragraph{Variational Autoencoders}

Variational Autoencoders (VAE) consist of an encoder that encodes input data $x$ into a regularized multivariate latent distribution \textcolor{red}{$q(z|x)$}, and a decoder that reconstructs the data as accurately as possible by sampling a point from this distribution \textcolor{red}{$z \sim q(z|x)$}. The decoder and encoder in VAE are trained to minimize the construction error, which also adds a regularization term in the loss function. This term ensures that the latent distribution is regularized to some prior distribution, usually multivariate Gaussian, during training.

In the neural network-based TTS model, the VAE is used to compress the high-dimensional speech $x$ into frame-level representations $z$, where $z$ is sampled from the posterior distribution $q(z|x)$. It is assumed that the prior $p(z)$ is the standard isotropic multivariate Gaussian. Given $q(z|x)$, the speech waveform is reconstructed from $p(x|z)$ using a decoder. In VITS \cite{vits, naturalspeech}, the $z$ is predicted from phoneme sequence $y$, where $z$ is sampled the predicted prior distribution, $p(z|y)$. To enable the speech synthesis, the VAE and the prior prediction are jointly optimized with the loss function consists of a waveform reconstruction loss -log $p(x|z)$ and a Kullback-Leibler divergence loss between the posterior $q(z|x)$ and the prior $p(z|y)$, $KL[q(z|x)||p(z|y)]$.

%The VAE has been used as the key component in many neural network based TTS models.

\paragraph{Generative Adversarial Networks}

Generative Adversarial Networks (GANs) are generative models that create new data instances with the same underlying data distribution of the training samples. GANs generate an N-dimensional vector from an N-dimensional random vector sampled from a simple distribution (standard Gaussian). Because of the complexity of data distribution, GANs formulate this problem into a game between two neural networks that compete against each other to reach a zero-sum Nash equilibrium profile~\cite{Goodfellow2014}. The generator network aims to produce synthetic output based on the target's data distribution to fool the discriminator. Meanwhile, the discriminator network aims to differentiate whether the generated sample is real or fake by comparing the output with the true data.

GAN has been widely used to train a neural vocoder for synthesizing high-fidelity speech audio. Typical non-autoregessive neural vocoders such as~\cite{melgan, hifigan, parallelgan} employ a generator and a sets of discriminators which are trained adversarially using GAN losses to model raw waveforms.

\paragraph{Normalizing Flows}

Normalizing flows employs a sequence of invertible and differentiable mappings, referred as the flow, that transform a simple probability distribution (e.g., standard Normal) into a more complex distribution.

To generate data points $\boldsymbol{x}$ that follow the distribution we want to generate, one can sample $\boldsymbol{z}$ from the base distribution (the “noise”) $p_{\theta}(\boldsymbol{z})$ of a zero mean spherical Gaussian, $p_{\theta}(\boldsymbol{z}) = \mathcal{N}(\boldsymbol{z};0,\boldsymbol{I})$. This process can be written as,
\begin{equation}
   \boldsymbol{z} \sim \mathcal{N}(\boldsymbol{z};0,\boldsymbol{I})
\end{equation}
\begin{equation}
   \boldsymbol{x} = \boldsymbol{f}_{1} \circ \boldsymbol{f}_{2} \circ \ldots \boldsymbol{f}_{K} %(\bm{z})
\end{equation}
where $\boldsymbol{x}$ can be generated by progressively transforming a random variable ($\boldsymbol{z}$) using a set of K invertible  functions, i.e., bijective. The flow-based model is trained by maximizing the log likelihood of the model parameters given the data $\boldsymbol{x}$ based on a change of variable rules,
\begin{equation}
    \log p_{\theta}(\boldsymbol{x}) = \log p_{\theta}(\boldsymbol{z}) + \sum_{i=1}^{K} \log |\det(\boldsymbol{J}(\boldsymbol{f}_{i}^{-1}(\boldsymbol{x})))|
    \end{equation}
\begin{equation}
    \boldsymbol{z} = \boldsymbol{f}_{K}^{-1} \circ \boldsymbol{f}_{k-1}^{-1} \circ \ldots \boldsymbol{f}_{0}^{-1}(\boldsymbol{x})
\end{equation}
where $\boldsymbol{J}$ denotes the Jacobian matrix of $\boldsymbol{f}_{i}^{-1}$, and $\det(.)$ is the the determinant of the matrix.

In a flow-based vocoder such as WaveGlow~\cite{waveglow}, the invertible mapping is constructed using an affine coupling layer~\cite{realnvp}. Each flow operation consists of invertible $1 \times 1$ convolution followed by an affine coupling layer. %Normalizing flow has been adopted for speech synthesis, but the implementation usually require large number of model parameters to improve the quality of synthesis~\cite{}. 
In TTS, Flowtron model~\cite{flowtron} is based on autoregressive flow that generates a sequence of mel spectrogram frames conditioned on text and speaker embedding. Flowtron provides control over speech variation and expressiveness and allows for style transfer.


%Some vocoders such as WaveGlow has been using GLOW ..


\paragraph{Denoising Diffusion Probabilistic Models}

Denoising Diffusion Probabilistic Models (DDPMs) encompass two interconnected processes: a forward process that maps the data distribution to a simple prior distribution, typically a Gaussian, and a reverse process that gradually reverse the effect of forward process using neural networks by simulating Ordinary or Stochastic Differential Equations.

Recently, DDPMs have become popular approach for audio generation including TTS. In TTS tasks, the diffusion model has been applied to the acoustic models and also to the vocoders. For example, the acoustic model in Diff-TTS~\cite{diff-tts} and Grad-TTS~\cite{grad-tts} generate Mel-spectrogram from the text with DDPM. In diffusion-based vocoders, DDPMs turn random noise into a high-fidelity speech waveform through an iterative sampling process after hundreds of iterations. The quality of neural vocoders based on DDPMs has been shown to be comparable with AR models~\cite{wavefit}. WaveGrad is one of the earliest work for conditional model waveform generation by combining the score matching and diffusion probabilistic models~\cite{wavegrad}. Another neural vocoder based on diffusion model is DiffWave~\cite{diffwave}. DiffWave is non-autoregressive model, conditioned on Mel-spectrogram. 

% https://arxiv.org/pdf/2303.13336.pdf

\subsubsection{Performance Evaluation}

% https://www.mdpi.com/2079-9292/9/2/267
% https://www.sciencedirect.com/science/article/pii/S0885230803000676

The quality of synthetic speech is typically evaluated using subjective and objective assessments. Subjective tests involve human listeners rating audio segments based on naturalness and intelligibility~\cite{king2014, electronics20}. Naturalness includes pleasantness, ease of listening, and audio flow, while intelligibility covers listening effort, pronunciation, articulation, comprehension, and speaking rate~\cite{articlemos}. These features are usually evaluated using a mean opinion score (MOS) on a Likert scale from 1 to 5, defined in the recommended standard ITU-T P.85, 1994 for the evaluation of voice output systems~\cite{itu_p85}.

The process of conducting a listening test can be both expensive and time-consuming. It requires a large number of participants who speak the native language or reside in specific geographical areas in order to obtain credible results. In recent years, deep learning models like AutoMOS~\cite{automos}, MOSNet~\cite{mosnet}, and LDNet~\cite{ldnet} have been developed to automatically predict the MOS of synthetic speech, which is highly correlated with human subjective ratings. For the MOS evaluation of multilingual speech synthesis, a Speech Quality Identification (SQuID) system based on multilingual Speech and Language Model (mSLAM) has been designed to predict speech naturalness ratings from 65 different locales~\cite{squid}. Additionally, objective metrics such as Mel-Cepstral distortion (MCD)~\cite{mcd} and word error rate (WER) are often used in combination with subjective tests to measure the quality and intelligibility of synthetic speech.

%The latest work is using regression method: 
%https://arxiv.org/pdf/2210.06324.pdf

\subsubsection{Summary of voice deepfake generation types and methods}

Text-to-speech and voice conversion are the two popular methods for generating fake speech samples. The progress has advanced rapidly with the introduction of an autoregressive GPT-like models, and when self-supervised speech representation learning models are utilised in the training process. Moreover, high-quality samples achieving human parity speech are produced when diffusion probabilistic models are used in the TTS's acoustic modeling. Flow matching~\cite{lipman2023flow}, a recent paradigm in generative modeling that combine aspects from continuous normalizing flows (CNFs) and diffusion models (DMs) to transform between noise and data samples by learning the probability path via ordinary differential equation (ODE) has shown superior modeling performance with stable and faster training. For example, Macha-TTS~\cite{matcha2024}  use optimal-transport conditional flow matching to train an encoder-decoder TTS model. Meanwhile, VoiceFlow~\cite{voiceflow} employs a rectified flow matching in TTS to achieve superior sample quality with a few number of sampling steps. %which is a new way to synthesize faster and higher quality samples by learning ordinary differential equations to sample from the data distribution.

\hspace{2mm}
\subsection{Detection}\label{sec:voice_detection}
\subsubsection{Features used for deepfake detection} %-> THEY ARE COMBINED/PART OF the literature review on voice deepfake detection


An audio spoofing attack occurs when an attacker creates a fake recording of someone's voice to bypass an automatic speaker verification (ASV) system. Although the fake recording may sound like a genuine human person to a human ear, it may still contain certain peculiarities or artifacts that can help in distinguishing it from genuine recordings. To protect the ASV system, voice spoofing countermeasure solutions are usually employed as a first defense mechanism to discriminate between bonafide (genuine) speech and spoofed speech generated using voice deepfake tools. Only bonafide speech will be fed into the ASV system.

A countermeasure system consists of two main modules, a feature extractor and a classifier. Feature selection and extraction play integral parts in spoofing detection algorithms. The features are engineered to be more compact and less redundant compared to the original audio signal. The detection model can also combine model outputs from different types of features to create a fusion model for improved spoofing detection performance~\cite{balamurali2019}.

\paragraph{Techniques based on hand-crafted features}

\textbf{Short-term Cepstral features:}
Mel-Frequency Cepstral Coefficients (MFCCs)~\cite{mfcc} are arguably the most commonly used features as a baseline in speech spoofing detection~\cite{sahidullah2015}. The extraction process of MFCCs begins with computing the power spectrum on each frame of audio signal using a short-time Fourier transform (STFT). Next, triangular filter bank, on a Mel-scale are applied to the power spectrum to extract frequency bands. Mel-scale aims to mimic the perceptually relevant aspects of human hearing, by being more discriminative at lower frequencies and less at higher frequencies. The logarithm of magnitude spectrum is then computed and followed by applying discrete cosine transform (DCT) to decorrelate the filter bank energies. Typically in speech recognition, 13 coefficients of Cepstral values, from the second to the fourteenth, are taken to represent the information regarding the vocal tract features. In practice, it is often to calculate the dynamic values such as delta and acceleration coefficients from the static Cepstral values to take into account the temporal changes in speech from frame to frame.

\textbf{Cepstral features related to MFCCs:}
Different types of filter bank can be used when integrating the power spectrum to compute the Cepstral coefficients. For example, in rectangular filter cepstral coefficients (RFCCs)~\cite{alegre2013}, the integration is performed using a rectangular window, spaced in linear scale. While the linear frequency cepstral coefficients (LFCCs) uses triangular filter bank, spaced in linear frequency scale~\cite{hasan2013}. On the other hand, the IMFCC is produced using the filters that are linearly spaced on the "inverted-mel" scale to put emphasis on the high frequency region~\cite{chakroborty2007}.

\textbf{Constant Q Cepstral Coefficients (CQCCs):}
The constant Q cepstral coefficients (CQCCs) are introduced for the first time as a spoofing countermeasure for automatic speaker verification~\cite{Todisco2017}.  
The CQCCs are obtained by applying time-frequency analysis on speech signal using the constant Q transform (CQT)~\cite{Youngberg1978} followed by the Cepstral analysis.
The CQT provides a greater frequency resolution for lower frequencies and a higher time resolution at higher frequencies, unlike the short-time Fourier transform which
gives constant time and frequency resolution. The resolution of the CQT features captures small variations in lower frequencies which are more useful to the task of spoofing detection.

\textbf{Short-term Phase Features:}
Other researchers have also investigated the use of phase information for speech spoofing detection. For example, some speech synthesis methods cause the loss or phase distortion during the analysis-synthesis steps that can be detected from the phase structure of the speech signals. Phase-based features such as relative phase shift (RPS)~\cite{sanchez2015} which contain purely phase information, modified group delay (MGD)~\cite{wu2012}, and modified relative phase (MRP)~\cite{wang2017} have been shown to be used successfully in the detection of synthetic speech and voice conversion based attacks~\cite{saratxaga2016}.

\subsubsection{Literature review on voice deepfake detection}

%The RPS representation uncovers the phase structure of
%the speech, which is not apparent from the instantaneous
%phase.

\paragraph{Techniques based on learned feature representation}
The conventional method for discriminating bonafide speech from spoofed speech is to train binary classifiers using hand-crafted features. Integrating multiple feature parametrisation is often necessary to improve anti-spoofing performance, for example via score fusion of subsystems based on different features to capture complementary information~\cite{zwu2017}.
Instead of using handcrafted features which are fixed and not learnable, relevant features for discrimination can be learned directly from raw waveform using deep learning techniques. The learnable front-ends can be categorized into two~\cite{fastaudio}: Short-Time Fourier Transform (STFT) based front-ends and First-order Scattering Transform (FST) based front-ends. In STFT based front-ends such as DNN-FBCC~\cite{dnn-fbank} and FastAudio~\cite{fastaudio}, the learned filterbank matrix are multiplied with the a spectrogram to create compressed representations.  For spoof detection task, FastAudio is shown to perform better than handcrafted CQT features and FTS-based front-ends on the ASVspoof 2019 dataset~\cite{fastaudio}.

The FST-based front-end approaches use a convolutional neural network to learn filtering process on raw audio waveform. 
%For example, ~\cite{} uses 
One example of FST-based front-ends is SincNet~\cite{sincnet}, an end-to-end neural network architecture comprises %parameterized Sinc functions as filters in the first layer followed by convolutional layers. 
learnable a bank of band-pass filters parameterized in the form of Sinc functions. SincNet encourages the network to discover more meaningful filters from temporal signal, learning a customized filter bank structure specifically tuned for the desired application. 
RawNet2 combines SincNet and RawNet1 architectures for end-to-end antispoofing~\cite{tak2021}. The first layer of RawNet2 follows the SincNet architecture and the upper layers consist of the same residual networks and GRU layer as RawNet1. RawNet1 is aimed to directly model raw waveforms for the extraction of speaker embedding using convolutional and recurrent neural networks~\cite{jung2019RawNet}. 

To take advantage of the vocoder artifacts in fake audio signals, ~\cite{sun2023} incorporates a vocoder identification module with the feature extractor of the RawNet2. The proposed multi-task learning framework combines multi-class classification loss to classify the type of vocoders used to synthesize fake audio, and a binary classifier to discriminate real from fake. 

\paragraph{Techniques based on self-supervised learning models for speech representations}

Feature extractors using self-supervised learning models for speech representations such as wav2vec and HuBERT have also been investigated for spoofing detection~\cite{xie2021, lantingli2023, kang2024}. The aim is to improve the generalization of the model to unknown attacks rather than using the traditional acoustic features. For example in~\cite{kang2024}, the pretrained wav2vec 2.0 is utilized as a feature extractor, in which several transformer layers are fine-tuned for spoofing countermeasure systems. It is shown that the fine-tuning of the SSL models can result in better generalised spoofing detection performance. Moreover,~\cite{hemlata2023} report the lowest EER for the ASVspoof 2021 logical access (LA) database when using wav2vec 2.0 front-end with the self-attentive aggregation layer and data augmentation strategy.   


%
%\subsubsection{Learning objective and loss functions}
\subsubsection{Performance evaluation}

Voice deepfake detection is primarily treated as a binary classification problem where the task of a countermeasure is to discriminate between bonafide and spoofed utterances. There are two types the binary classifier makes: \textit{false alarms} (false positive), where the classifier accepting spoofed speech that should have been rejected, and \textit{misses} (false negative), where the classifier rejects bonafide speech that should have been accepted. The \textit{false alarm rate} $P_{fa}$ and \textit{miss rate} $P_{miss}$ can be estimated by dividing the number of misses and false alarms by the number of spoof trials and bonafide cases, respectively~\cite{bonastre2021}. 
The most common metric adopted for binary classification is \textit{equal error rate} (EER), a single number in which the \textit{false alarms rate} and the \textit{miss rate} are equal. Lower EER points to a more reliable classifier. However, the EER metric does not consider the level of importance between the $P_{fa}$ and $P_{miss}$, for example when false alarms can be more tolerated than misses.

To suit real-world applications, different cost and class prior parameters are introduced in ASVSpoof 2019 and 2021 to take into account both countermeasure and ASV subsystems as a tandem system, called tandem detection cost function (t-DCF)~\cite{yamagishi:hal-03360794}. The t-DCF is defined as:
\begin{equation}
\text{t-DCF} = 
    \min\limits_{\tau}  \{C_o + C_1P_{miss}(\tau) + C_2 P_{fa}(\tau)\}
\end{equation}
where $P_{miss}$ and $P_{fa}$ are miss and false alarm rates of a countermeasures using a threshold $\tau$, and $C_{0}$, $C_{1}$ and $C_{2}$ are pre-defined cost function parameters that depend on ASV performance as well.
%\subsubsection{Universal deepfake detectors ?}

\hspace{2mm}
\subsection{Combating voice deepfakes in voice biometrics}

Deepfakes pose a significant threat to voice biometrics systems. Using commercial tools, attackers can synthesize the speech of any person to fool speaker recognition systems. For example~\cite{BILIKA2024} found that voice assistants such as Google’s Assistant and Apple’s Siri can be easily tricked using synthesized samples of victims' speech commands,  in which they used an open source "Real Time Voice Cloning" project~\cite{jeremy2019} to clone the voice of the authorized users. As such, it is necessary to evaluate the efficacy of the state-of-the-art voice deepfake generation methods to fool advanced voice biometric systems. In the following subsection, we provide a summary of our evaluation results. For a detailed discussion please refer to Sec. I of supplementary material.

\subsubsection{Efficacy of voice deepfakes to fool voice biometrics systems}
The evaluations presented in Tab. 1 of the supplementary material illustrate that the state-of-the-art speaker recognition system can be vulnerable to off-the-shelf,  publicly available voice deepfakes generation software. As such, there is a pertinent need to upgrade the safety of the existing voice biometrics models against voice deepfakes. 

\subsubsection{Measures for revealing true identity} To the best of our knowledge, currently, there is no method to extract the true identity of a subject when the speech is manipulated using the voice deepfakes method.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[htbp]
\caption{Top ten AI voice cloning tools to create voice deepfakes.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\multicolumn{1}{c}{\multirow{2}{*}{Tools}} & \multicolumn{3}{c}{Features}                                                                                   & \multicolumn{1}{c}{\multirow{2}{*}{URL}} \\
\multicolumn{1}{c}{}              & Used a cloned voice for TTS & Customize pitch/style & Open-source & \multicolumn{1}{c}{}                     \\ \hline \hline
MetaVoice-1B & \cmark & \xmark & \cmark & https://github.com/metavoiceio/metavoice-src \\ \hline
XTTS-v2 & \cmark & \xmark & \cmark & https://github.com/coqui-ai/tts \\ \hline
Soft-VC VITS & \xmark & \cmark & \cmark & https://github.com/svc-develop-team/so-vits-svc \\ \hline
Tortoise-TTS & \cmark & \xmark & \cmark & https://github.com/neonbjb/tortoise-tts \\ \hline
OpenVoice & \cmark & \cmark & \cmark & https://github.com/myshell-ai/OpenVoice \\ \hline
VoiceCraft~\cite{peng2024voicecraft} & \cmark & \xmark & \cmark & https://github.com/jasonppy/VoiceCraft \\ \hline
ElevenLabs     &    \cmark    &    \xmark   & \xmark   & https://elevenlabs.io/                   \\ \hline
Descript       &     \cmark                                   &     \xmark                       & \xmark   &https://www.descript.com/                \\ \hline
Respeecher     &   \cmark                                     &           \cmark                 & \xmark   &https://www.respeecher.com/              \\ \hline
Play.ht   &    \cmark                                    &      \xmark                      & \xmark   &https://play.ht/                         \\ \hline
%Resemble AI  &   \cmark                                     &     \xmark                       & \xmark   &https://www.resemble.ai/       \\ \hline
\end{tabular}}\hspace{2mm}

\end{table*}
