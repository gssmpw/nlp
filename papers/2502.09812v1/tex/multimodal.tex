% \newpage
\section{Multimodal Deepfakes}
\label{sec:multimodal_deepfakes_intro} 

The concept of multi-modality in deep learning involves integrating and processing data from various sources simultaneously. These sources can encompass text, images, audio, video, and sensor data. By leveraging different data types, multi-modal deep learning models can capture more comprehensive and diverse information, resulting in enhanced performance for tasks that require understanding the relationships between different data types \cite{gao2020survey, summaira2021recent, jabeen2023review}. In the realm of deepfakes, multi-modality entails using various types of data, such as images, audio, and video, to create highly realistic synthetic media that convincingly mimics real-world content, including visuals and sounds \cite{khalid2021fakeavceleb, hou2024polyglotfake}. Through aligning and synchronizing these modalities, deepfakes can produce seamless and coherent fake content, such as matching a person's lip movements to a different audio track or convincingly cloning their voice \cite{pei2024deepfake, prajwal2020lip, cheng2022videoretalking, lomnitz2020multimodal}. While this technology offers positive applications in entertainment, media, and education, such as creating special effects and developing realistic training simulations, it also poses significant ethical and security challenges \cite{pandey2021deepfakes}. These include the potential for misuse in misinformation, impersonation, and fraud. Detecting and preventing malicious deepfakes is a burgeoning area of research aimed at ensuring the responsible use of this powerful technology. As multi-modal deepfakes continue to evolve, it is crucial to balance innovation with ethical considerations to mitigate risks and maximize benefits \cite{khalid2021evaluation, liu2023magnifying, cheng2023voice}. In this section, we will investigate state-of-the-art methodologies for the generation (refer Section \ref{subsec:multimodal_generation}) and the detection of multi-modal deepfakes (refer Section \ref{subsec:multimodal_detection}). We will analyze the advanced and innovative techniques outlined in the existing literature, alongside the datasets utilized for deepfake detection and generation.

\subsection{Multimodal Deepfake Generation}
\label{subsec:multimodal_generation}

Combining audio and video deepfakes involves a sophisticated process of synchronizing lip movements with synthetic speech to produce seamless and coherent content \cite{liz2024generation}. This multi-modal approach, which integrates both visual and auditory elements, significantly enhances the realism of the generated media. By ensuring that the audio matches the lip movements and facial expressions perfectly, these deepfakes become more lifelike and convincing, making detection increasingly challenging \cite{hou2024polyglotfake}. Audio-visual multimodal deepfakes can be categorized into three main types based on the modality being faked \cite{khalid2021fakeavceleb}. This categorization helps in understanding the different ways in which deepfakes can manipulate audio and visual components to create convincing forgeries. Understanding these categories is important for recognizing and combating the potential misuse of deepfake technology.

\paragraph{Fake Video and Real Audio}
\label{para:multimodal_fakeV_realA}

Fake video and real audio deepfakes involve manipulating visual content while retaining the original audio, creating a synthetic video that depicts events or actions that never actually occurred. By keeping the original audio, which includes the true voice, tone, and speech patterns of the person, these deepfakes gain an added layer of credibility and can be particularly persuasive. The process of creating fake videos with real audio often involves altering the appearance, expressions, or actions of individuals in the video \cite{karras2019style, nirkin2019fsgan, korshunova2017fast}. For instance, face swapping \cite{korshunova2017fast} can replace the subject’s face with someone else's, or visual effects can be used to create entirely new scenes that seem authentic. The combination of real audio and fake video poses significant challenges for detection, as the genuine audio can make the fabricated visuals appear more believable. Detecting these deepfakes involves analyzing inconsistencies between the audio and video elements. Techniques such as temporal analysis of facial expressions, detecting unnatural movements, and scrutinizing visual artifacts are crucial \cite{kaur2020deepfakes, liu2023ti2net}.

\paragraph{Real Video and Fake Audio}
\label{para:multimodal_realV_fakeA}

In this type of deepfake, the video remains unaltered, while the audio is synthetically generated to mislead the audience about what is being said. This method generally involves using text-to-speech models and voice cloning techniques to create synthetic speech that closely mimics the vocal characteristics of a specific person \cite{deng2020unsupervised, kinnunen2017asvspoof, polyak2019tts}. By manipulating the audio, these deepfakes can make it seem like the person in the video is saying something they never actually said. This technique is particularly dangerous because the genuine video lends credibility to the fake audio, making it more convincing and harder to detect. Examples of using real video with fake audio include creating false news reports, tampering with evidence in legal cases, and producing deceitful content for political propaganda \cite{sankaranarayanan2021presidential}. Detecting such deepfakes requires a comprehensive analysis of audio-visual synchronization to identify discrepancies between lip movements and speech \cite{agarwal2020detecting}. Additionally, it demands the development of robust audio forensic techniques to scrutinize voice patterns and identify synthetic anomalies \cite{almutairi2022review}.

\paragraph{Fake Video and Fake Audio}
\label{para:multimodal_fakeV_fakeA}

These deepfakes manipulate visual content to depict events or actions that never occurred, while also synthesizing audio to accompany the fabricated visuals.  This allows for a wider range of possible sample alterations and a variety of manipulation techniques. For example, a deepfake could depict a person giving a speech they never delivered, with the voice and the video being entirely fabricated. Achieving realism in both modalities requires training models on large datasets of real audio and video to learn the nuances of human speech, facial expressions, and body movements. The challenge lies in creating seamless synchronization between the audio and video components to make the deepfake indistinguishable from genuine content. Furthermore, standardized multimodal deepfake datasets serve as benchmarks for evaluating the performance of detection algorithms \cite{dolhansky2020deepfake, khalid2021fakeavceleb, hou2024polyglotfake}. They offer a common ground for researchers to compare different approaches, facilitating the identification of the most effective methods for detecting deepfakes \cite{liu2023magnifying, cheng2023voice, feng2023self}. This benchmarking is vital for pushing the boundaries of deepfake detection technology, ensuring that the algorithms can generalize well across different types of deepfakes and are not limited to specific scenarios or formats.

\subsection{Multimodal Deepfake Detection Datasets}
\label{subsec:multimodal_datasets}

Multimodal deepfake datasets are crucial for advancing the understanding and detection of deepfakes, which involve the manipulation of multiple types of data such as audio, video, and text to create convincingly fake content. These datasets offer diverse examples of synthetic media that combine various modalities, accurately reflecting the complex, real-world scenarios where deepfakes are likely to be encountered. This diversity is indispensable for training sophisticated detection techniques, primarily deep learning-based networks, to recognize and detect deepfakes across various scenarios and formats. By including examples that span different combinations of audio, video, and text manipulations, multimodal datasets allow researchers to develop and refine algorithms that can analyze the consistency and coherence between these modalities.

Multimodal deepfake datasets are essential for advancing both the understanding and detection of deepfakes, which involve manipulating multiple types of data, such as audio and video, to create compelling fake content. These provide diverse examples of synthetic media, combining various modalities like audio, video, and text reflecting real-world scenarios where deepfakes are likely to be used. This diversity is essential for training sophisticated detection techniques (mainly deep learning-based networks) to recognize and detect deepfakes across different scenarios and formats. These multimodal datasets enable researchers to develop algorithms that analyze the consistency and coherence between modalities. Furthermore, standardized multimodal deepfake datasets offer benchmarks for evaluating the performance of detection algorithms. They provide a common ground for comparing different approaches and identifying the most effective methods for detecting multimodal deepfakes.

\begin{table*}[htbp]
\caption{\textcolor{black}{Multimodal Deepfake Datasets}}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|P{67pt}|P{42pt}|P{40pt}|P{35pt}|P{35pt}|P{100pt}|P{100pt}|}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Multilingual} & \multirow{2}{*}{Subjects} & \multicolumn{2}{c|}{Samples} & \multicolumn{2}{c|}{Manipulation Techniques} \\\cline{4-7}
&  &  & Real & Fake & Video & Audio\\\hline\hline
DFDC \cite{dolhansky2020deepfake} & No  & 960 & 104,500 & 23,654 & DFAE, MM/NN face swap, NTH, FSGAN \cite{nirkin2019fsgan}, StyleGAN \cite{karras2019style} & TTS-Skins \cite{polyak2019tts}\\\hline
FakeAVCeleb \cite{khalid2021fakeavceleb} & No  & 500 & 500 & 19,500 & FaceSwap \cite{korshunova2017fast}, Wav2Lip \cite{prajwal2020lip}, FSGAN & SV2TTS \cite{jia2018transfer}\\\hline
VideoSham \cite{mittal2023video} & No & - & \textcolor{black}{413} & \textcolor{black}{413} & \multicolumn{2}{c|}{Manual manipulations by professional video editors (6 types)} \\\hline
PDD \cite{sankaranarayanan2021presidential} & No & 2 & 16 & 16 & Wav2Lip & Manipulated content recorded by voice actors \\\hline
LAV-DF \cite{cai2022you} & No & 153 & 36,431 & 99,873 & Wav2Lip & SV2TTS \\\hline
MMDFD \cite{asha2023mmdfd} & No  & 50 & 1,500 & 5,000 & FaceSwap, FSGAN, Wav2Lip, DeepFaceLab \cite{perov2020deepfacelab} & SV2TTS (AurisAI \cite{aurisaiAurisFree} for Text)\\\hline
DefakeAVMiT \cite{10081373} & No  & 43 & \multicolumn{2}{c|}{6,480} & FaceSwap, DeepFaceLab, EVP \cite{ji2021audio}, Wav2Lip, PC-AVS \cite{zhou2021pose}  & SV2TTS, Voice Replay \cite{kinnunen2017asvspoof}, AV exemplar autoencoders \cite{deng2020unsupervised} \\\hline
PolyGlotFake \cite{hou2024polyglotfake} & Yes  & 766 & 766 & 14,472 & VideoRetalking \cite{cheng2022videoretalking}, Wav2Lip & XTTS \cite{Gölge2024coqui}, Bark \cite{Kucsko2024suno} + FreeVC \cite{li2023freevc}, Tacotron \cite{wang2017tacotron} + FreeVC, MicrosoftTTS \cite{microsoftTextSpeech} +FreeVC, Vall-E-X \cite{wang2023neural}\\\hline
\end{tabular}}
\label{tab:multimodal_datasets}
\end{table*}

As illustrated in Table \ref{tab:multimodal_datasets}, the manipulation of modalities to generate multimodal deepfakes was accomplished using state-of-the-art deepfake generation techniques available at the time of the dataset release. The quality of the synthetic content is contingent upon the strengths and limitations of these techniques. Advances in sophisticated deep learning methodologies over time have yielded increasingly realistic fake content. Consequently, the techniques employed for multimodal deepfake generation are further elaborated in Section \ref{subsec:multimodal_generation}.

\subsection{Multimodal Deepfake Generation}
\label{subsec:multimodal_generation}

In the following section, we will delve into the latest and most widely utilized techniques for producing synthetic audio-visual content by synchronizing lip and/or face movements, drawing from state-of-the-art research in the field.

\paragraph{LipGAN}
\label{para:multimodal_lipgan}

Prajwal \textit{et al.} \cite{kr2019towards} proposed a GAN-based lip synchronization model called LipGAN. This model is capable of handling faces in random poses without the need for realignment to a template pose. Furthermore, LipGAN enables the generation of realistic talking face videos through an automated pipeline for face-to-face translation from any audio, without dependence on language.

\paragraph{Wav2Lip}
\label{para:multimodal_wav2lip}

Most state-of-the-art methods excel at generating accurate lip movements for static images or videos of specific individuals seen during the training phase. However, these methods often fail to accurately morph lip movements for arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant portions of the video being out-of-sync with the new audio. As discussed by Prajwal et al. in \cite{\cite{prajwal2020lip}}, this failure can be primarily attributed to limitations in both training objectives (i.e., loss functions used) and lip-sync discriminators. Typically, the face reconstruction loss is computed for the detected facial region to ensure correct pose generation and identity preservation. However, the lip region constitutes a very small proportion of the face region (or the entire frame), causing the total reconstruction loss (often L1 distance) to be less impacted by the lip region due to its limited spatial extent. This can adversely affect the learning process, where the reconstruction of the surrounding image is prioritized over the lip region. To address this issue, specific discriminators, such as those used in LipGAN \cite{kr2019towards}, are employed to evaluate lip-sync accuracy. Prajwal et al. \cite{prajwal2020lip} emphasized the importance of short temporal context in detecting lip-sync discrepancies and incorporated this concept into the development of the Wav2Lip framework. They demonstrated that considering a short temporal context significantly improves lip-sync accuracy. Additionally, they noted that introducing artifacts such as pose variations during GAN training can negatively impact lip-sync performance. This degradation occurs because the discriminator may fail to focus on the correspondence between the video and lip movements, underscoring the need for discriminators specifically designed to evaluate lip-sync quality. Prajwal \textit{et al.} utilises a customised SyncNet-based \cite{chung2017out} discriminator to mitigate the limitations mentioned above, which is substantially more accurate than the previous methodologies. Furthermore, to increase the quality of the morphed regions in the reconstructed images, they have also utilised a visual-quality discriminator. This technique has been used by most of the above multimodal datasets to generate faked audio-visual content.

\paragraph{FaceRetalking}
\label{para:multimodal_faceretalking}

Cheng \textit{et al.} \cite{cheng2022videoretalking} highlighted that the state-of-the-art techniques often omit the original lip motion changes or retiming the background to avoid unnatural movements between the head pose and lip \cite{prajwal2020lip, song2022everybody}. However, in the FaceRatalking method, the lower half face (not only the lip) Cheng \textit{et al.} emphasized that existing state-of-the-art approaches often overlook authentic lip motion alterations or adjust the background timing to prevent unnatural movements between head pose and lip motion. In contrast, the FaceRatalking technique not only modifies the lip region but also encompasses editing of the entire lower half of the face, incorporating facial movements through an innovative face reenactment process. Additionally, they identified an information leakage in conditional in-painting-based methods when the original frame was utilized as the conditional image for lip synchronization. To remedy this, they introduced a semantic-guided reenactment network to alter the expression of the entire lower half of the face, producing an enriched frame with a consistent expression, which then served as the basis for subsequent lip synthesis. The lip synthesis network in the FaceRetalking approach incorporates a conditional inpainting-inspired network \cite{prajwal2020lip}. This network leverages pre-processed frames from the face reenactment network as the identity and structure reference, along with the audio and the masked original frames as the condition resulting in a highly effective method for synthesizing a lip-syncing video based on the input audio. They argued that even though the synthesized videos accurately depict lip movements, the visual quality is limited due to low-resolution training data. To tackle this issue, they developed an identity-preserving face enhancement network to improve output quality through progressive training. When compared to Wav2Lip \cite{prajwal2020lip} which mainly focuses only on lip synchronization, FaceRetalking provides a broader facial synthesis that includes expressions and head movements, enabling more realistic fake content.

\paragraph{Diff2Lip}
\label{para:multimodal_diff2lip}

In their work, Mukhopadhyay \textit{et al.} \cite{mukhopadhyay2024diff2lip} proposed Diff2Lip, which is an audio-conditioned diffusion model used for inpainting producing precise and natural lip sync by focusing on the fine details of lip movements. This model can achieve lip synchronization in real-world scenarios while preserving identity, pose, emotions, and image quality. The Diff2Lip model takes three inputs: a masked input frame (providing pose context), a reference frame (containing identity and mouth region textures), and an audio frame (used to drive the lip shape). It then outputs the lip-synced mouth region. The multimodal conditional diffusion implemented in the model allows for a fine balance between all contextual input information, effectively avoiding lip-sync problems. However, when compared with the FaceRetalking model, the FaceRetalking may provide a better integrated facial performance, enhancing overall expressiveness and realism beyond just the lips. However, through the leverage of the diffusion process, Diff2Lip can enhance the naturalness and fluidity of lip movements over time compared to similar lip synthesis methods such as Wav2Lip.


\subsection{Performance Evaluation in Deepfake Generation}
\label{subsec:multimodal_lossfunc}


Performance evaluation is a critical step in multimodal deepfake generation, providing a comprehensive framework to ensure that the generated content is realistic, high-quality, coherent, and consistent across different modalities. Various performance evaluation metrics are employed in the literature, encompassing both application-specific and generalized measures. In this section, we discuss some of the most widely considered performance metrics and their roles in assessing the quality and consistency of multimodal deepfake generation models.

\paragraph{The Fréchet Inception Distance (FID)}

The Fréchet Inception Distance (FID) serves as a crucial metric for evaluating the quality of generated images \cite{nunn2021compound, singh2020using, yu2021artificial}. By comparing the feature distribution of generated images with real images, it provides valuable insights into both the fidelity and diversity of the generated images. Lower FID values signify higher similarity to real images, indicating superior quality. An FID of 0 implies that the generated images are indistinguishable from real images in terms of their feature distributions. However, the FID score is sensitive to the choice of feature extractor. The FID score can be calculated as in Equation \ref{eq:multimodal_fid} where $\mu_r$, $\mu_g$, $\Sigma_r$, $\Sigma_g$ and $T_r$ represent the mean of real image features, mean of generated image features, covariance or real image features, covariance of generated image features and Trace of the matrix respectively \cite{nunn2021compound}. 

\begin{equation}
    FID = ||\mu_r - \mu_g||^2 + Tr(\Sigma_r+\Sigma_g - 2(\Sigma_r\Sigma_g)^{0.5})
    \label{eq:multimodal_fid}
\end{equation}

\paragraph{Structural Similarity Index (SSIM)}

SSIM compares two images by analyzing their structure, luminance, and contrast. Its goal is to provide a more accurate measure of perceptual image quality compared to simpler metrics like Mean Squared Error (MSE). SSIM is specifically designed to take human perception into account when assessing image quality, making it a more reliable method \cite{sun2020landmark, dagar2022literature, husseini2023comprehensive}. It does not consider higher-order image statistics, which may be important for certain aspects of image quality. However, it's important to note that SSIM is a pixel-wise image similarity metric that compares two images and may not be the best choice for capturing variability in video generation \cite{shrivastava2021diverse}. The SSIM can be calculated as in Equation \ref{eq:multimodal_ssim} where $c_1$, and $c_2$ represent constants to stabilize the division with a weak denominator \cite{wang2004image}.

\begin{equation}
    SSIM(x,y) = \frac{(2\mu_x \mu_y + c_1)(2\sigma_{xy}+c_2}{(\mu_x^2+\mu_y^2+c1)(\sigma_x^2+\sigma_y^2+c_2)}
    \label{eq:multimodal_ssim}
\end{equation}

\paragraph{Lip Movement Distance (LMD)}

Lip Movement Distance (LMD) is an important metric used to assess how well the movement of lips matches the corresponding audio, especially in the context of creating deepfakes. It measures the spatial difference between the lip positions in the generated frames and the actual frames, providing a quantitative evaluation of the accuracy of lip movements to the spoken audio \cite{chen2018lip}. However, the accuracy of LMD depends on the reliability of the facial landmark detection model used to extract lip positions. Additionally, since LMD focuses on spatial alignment, it may not fully capture the temporal dynamics and smoothness of lip movements over time. The LMD over $N$ frames can be calculated as in Equation \ref{eq:multimodal_lmd} where $L_t^{gen}$, and $L_t^{gt}$ represent lip landmarks for the generated and ground truth images respectively \cite{chen2018lip}.

\begin{equation}
    LMD = \frac{1}{N} \Sigma_{t=1}^n ||L_t^{gen} - L_t^{gt}||
    \label{eq:multimodal_lmd}
\end{equation}

\paragraph{LSE-C and LSE-D}

LSE-C and LSE-D are two important metrics used to assess the performance of the Wav2Lip model \cite{prajwal2020lip}, which is used to achieve synchronisation of audio-manipulated talking face videos. This model ensures that the lip movements in a video align with the corresponding audio. The LSE-D error (Lip-Sync Error-Distance), measures the misalignment between audio and visual streams in terms of lip synchronization, and a lower LSE-D denotes a higher audio-visual match \cite{prajwal2020lip}. LSE-C (Lip-Sync Error-Confidence) is the confidence score. A higher score suggests a better audio-visual correlation and more accurate alignment of lip movements with audio. Prajwal \textit{et al.} calculated these error metrics based on SyncNet \cite{chung2017out} extracted features from both the audio and visual inputs where the visual features are typically derived from the region around the lips, while audio features are extracted from the corresponding audio segment. However, poor feature extraction can lead to inaccurate error measurement, and may not capture the naturalness of continuous speech synchronization. These losses have been widely used in later research to validate the performance of lip-speech synchronisation \cite{wang2022attention, zhang2022meta, lu2022visualtts}. 

\paragraph{Peak Signal-to-Noise Ratio (PSNR)}

PSNR, which stands for Peak Signal-to-Noise Ratio, is a metric used to measure the quality of a reconstructed or generated image in comparison to a reference image \cite{huang2020fakeretouch, huang2020fakepolisher, wang2021faketagger}. It quantifies the level of distortion or noise introduced during the generation process, with higher PSNR values indicating better quality and less distortion. PSNR is easy to compute and understand, unlike most other metrics, providing a straightforward measure of image and video quality. However, it's important to note that PSNR does not always align well with human visual perception, and high PSNR values do not guarantee that the image will look good to human observers. PSNR can be calculated as in Equation \ref{eq:multimodal_psnr} where $MAX$ and $MSE$ refer to the maximum pixel value (can be either 1 or 255 depending on whether the input image is in double-precision floating-point or 8-bit unsigned integer format) and the Mean Squared Error (MSE) between the reference video frame (or image) and the generated frame.

\begin{equation}
    MSE = \frac{\Sigma_{M,N}[I_1(m,n)-I_2(m,n)]^2}{M*N}
    \label{eq:multimodal_psnr}
\end{equation}

\begin{equation}
    PSNR = 10.log_{10} (\frac{MAX^2}{MSE})
    \label{eq:multimodal_psnr}
\end{equation}

% \paragraph{Lip Sync Error Rate (LSER)}

% Lip sync error rate is the measure of how often or to what extent the audio track of a video does not match up correctly with the visual track of the speaker's lip movements. Various techniques can be used to calculate this metric, all of which assess how well the created lip movements synchronize with the accompanying audio, ensuring that the visual and audio components are convincingly synchronized. 

The performance metrics mentioned above can be divided into two main categories: lip-sync rate-related measures and realism-related measures. Lip-sync is essential for applications that require precise audio-visual alignment, focusing on synchronizing audio and lip movements. Realism is important for applications that require high visual fidelity and overall believability of the generated content. It takes into account not only lip synchronization, but also other factors such as facial expressions, eye movements, skin texture, and lighting. Lip-sync rate can be measured using metrics like LSE-C, LSE-D, and LMD, while realism can be measured through PSNR, SSIM, FID, and perceptual human evaluations (such as mean opinion score (MOS) \cite{lu2022visualtts, hou2024polyglotfake}).

\paragraph{BRISQUE}

BRISQUE is an image quality assessment (IQA) model that can evaluate the quality of an image without needing a reference image \cite{mittal2012no}. It works by analyzing natural scene statistics in the spatial domain, directly examining pixel values without transforming the image into a different domain, such as the frequency domain. The model employs statistical features derived from natural scene statistics to capture deviations from natural image properties, indicating distortions and quality degradation. This makes it useful for detecting deepfakes, as it does not rely on a real image for comparison once it's deployed in the world \cite{hou2024polyglotfake, yang2020deepfake}. 


\subsection{Multimodal Deepfake Generation Tools}

\textcolor{black}{A Deepfake Generation Tool is a sophisticated software or system designed to synthesize and create realistic media content across multiple modalities—primarily video and audio. These tools leverage advanced artificial intelligence (AI) and deep learning techniques to generate highly convincing synthetic content, often using generative models like GANs (Generative Adversarial Networks) or diffusion models. Deepfake generation tools can create content that closely mimics real media, making it challenging to distinguish between authentic and synthetic.}

\begin{table*}[htbp]
\caption{Top Tools to Detect Multimodal Deepfakes}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
Method & Free & Open-source & URL\\\hline\hline
Synthesia  & \xmark & \xmark & https://www.synthesia.io/ \\\hline
AI Studios by DeepBrain AI  & \xmark & \xmark & https://www.aistudios.com/ \\\hline
Creative Reality Studio by D-ID  & \xmark & \xmark & https://www.d-id.com/creative-reality-studio/ \\\hline
Elai.io  & \xmark & \xmark & https://elai.io/ \\\hline
\end{tabular}}
\label{tab:tool_det_multimodal_deepfakes}
\end{table*}

\textcolor{black}{The rise of accessible deepfake generation tools has revolutionized content creation in several areas, particularly in Training & Education, E-commerce, Social Media, and Customer Support. These platforms use advanced AI to generate realistic videos based on text prompts, featuring avatars that can speak, express emotions, and perform subtle facial gestures. Designed mainly for professional use, these tools allow users to create customized video content at scale without the need for complex equipment, technical expertise, or high production costs. One of the most notable features of these tools is their user-friendly interface, which enables non-technical users and beginners to quickly produce sophisticated video content. Most of these platforms utilize deep learning and computer vision techniques to animate pre-existing avatars according to user-provided text prompts. The avatars are often photorealistic digital representations of real individuals, providing a high level of realism.}

\subsection{Multimodal Deepfake Detection}
\label{subsec:multimodal_detection}

Multimodal deepfake detection is an important area of research that focuses on identifying synthetic media manipulating multiple modalities, such as video, audio, and text, to create highly realistic and deceptive content \cite{raza2023multimodaltrace, katamneni2023mis, 10081373}. Unlike traditional deepfakes that target a single modality, multimodal deepfakes integrate alterations across several types of data, making them more sophisticated and harder to detect. Detection approaches typically use machine learning and deep learning algorithms, which are trained to recognize subtle anomalies in the synchronized behaviour of different modalities. For example, discrepancies between lip movements and speech, unnatural facial expressions, or inconsistencies in lighting and shadows can signal the presence of a deepfake \cite{lewis2020deepfake}. As deepfake technology continues to evolve, detection methods must also evolve to ensure the integrity and authenticity of digital media in an increasingly digital and interconnected world. In this section, we highlight the latest advancements in multi-modal deepfake detection technology.

\Rotatebox{90}{
\centering
\renewcommand{\arraystretch}{1}
\caption{\textcolor{black}{Multimodal Deepfake Detection Approaches}}
\begin{tabular}{|P{70pt}|P{120pt}|P{120pt}|P{130pt}|P{85pt}|}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Feature Extractors} & \multirow{2}{*}{Technical Novelty} & \multirow{2}{*}{Performance} \\\cline{2-3}
& Audio & Video & & \\\hline\hline
Multimodaltrace \cite{raza2023multimodaltrace} & Resnet-1D on positive frequencies of FFT features & Multilayered 3D ResNet on normalised stacked video frames & Independent and joint feature learning through IntrAmodality Mixer Layer (IAML) and IntErModality Mixer Layer (IEML) & FakeAVCeleb: 92.9\% PDD: 70\% \\\hline
MIS-AVoiDD \cite{katamneni2023mis} & MFCC & MTCNN for face detection and Xception-based for feature extraction & Joint use of modality-invariant and specific representations to ensure both common and unique patterns of real or fake content are preserved and fused &  FakeAVCeleb: 96.2\%(Accuracy) 0.973(AUC) \\\hline
AVoiD-DF \cite{10081373} & transformer network on Mel-spectrograms of audio & transformer network on visual frames & Multimodal temporal \& spatial encoder (TSE) with multimodal joint decoder (MMD)  &  FakeAVCeleb: 83.7\%(Accuracy) 0.892(AUC) \\\hline
PVASS-MDD \cite{yu2023pvass} & VGGish network on log mel-spectrograms of audio \cite{hershey2017cnn} & MTCNN for face extraction and a Swin-Transformer \cite{liu2022video} & Cross-modal predictive VA alignment module  &  FakeAVCeleb: 84.3\%(Accuracy) 0.875(AUC) \\\hline
Emotions Don't Lie \cite{mittal2020emotions} & 13 MFCC features from pyAudioAnalysis \cite{giannakopoulos2015pyaudioanalysis} + DCNNs for modality encoding and perceived emotion encoding & facial features from OpnFace (430-D) \cite{amos2016openface} + DCNNs for modality encoding and perceived emotion encoding & Comparison of affective cues corresponding to perceived emotion to infer whether the video is manipulated & DFDC: 84.4\% \\\hline
AVAD \cite{feng2023self} & \multicolumn{2}{C{160pt}|}{Audio-visual synchronisation model  as in \cite{chen2021audio}. CNN-based feature encoders for visual frames and audio spectrograms and transformer as synchronisation module.} & Video forensics posed as an audio-visual anomaly detection problem and learning only on real videos  &  FakeAVCeleb: 87.9\%(AP) 0.900(AUC) \\\hline
VFD \cite{cheng2023voice} &  \multicolumn{2}{C{160pt}|}{Deep forward convolutional projection on the spectrogram and visual frames + transformer-like network to learn identity-related features} & A face-voice matching technique that measures homogeneity between the audio and video to identify deepfakes  &  FakeAVCeleb: 81.52\%(Accuracy) 0.8611(AUC) DFDC: 80.96\%(Accuracy) 0.8513(AUC)  \\\hline
Capsule Forensics (score fusion) \cite{muppalla2023integrating} & Capsule network on Mel-spectrograms of audio & Capsule network on MTCNN extracted face regions &  Multimodal score-fusion capable of identifying inconsistencies across
various deepfake types and artifacts within each modality  &  FakeAVCeleb: 99.2\%(Accuracy) 0.993(AUC) \\\hline
FCMT + DDIC \cite{liu2023magnifying} &  Audio Forgery Clues Magnification Transformer (FCMT) & Video FCMT &  FCMT module to capture intra-modal artifacts from different modalities by magnifying forgery clues + image spatial artifacts magnification with DDIC  &  FakeAVCeleb: 99.13\%(Accuracy) 0.9927(AUC) DFDC: 98.45\%(Accuracy) 0.9903(AUC)  \\\hline
\end{tabular}
\label{tab:multimodal_detectiondatasets}
}

To effectively detect realistic deepfakes, it is crucial to address both audio and video manipulation. This can be achieved either by independently detecting audio and video cues in deepfakes or through a combined approach that leverages joint audiovisual representation learning. Raza \textit{et al.} \cite{raza2023multimodaltrace} introduced a unified multimodal framework called "Multimodaltrace" which extracts learned feature representations from both audio and visual data, processing these elements separately before integrating them using an innovative multimodal fusion technique. Furthermore, they proposed a novel reformulation of the audiovisual deepfake detection problem, framing it as a multi-label classification task. This new approach predicts confidence levels across both audio and visual modalities, offering a more nuanced and effective method for identifying deepfakes. In their study, Katamneni et al.\cite{katamneni2023mis} focused on fusing modality invariant and specific feature representations for audio and visual streams. This method is similar to previous approaches but employs a different combination of regularization and learning objectives (modality invariant loss, modality-specific loss, and orthogonal loss), leading to improved results. Yang \textit{et al.} \cite{10081373} have proposed an innovative approach for detecting deepfakes by using audio-visual joint learning (AVoiD-DF) which leverages audio-visual inconsistencies for multi-modal forgery detection. The process begins by embedding temporal-spatial information in a Temporal-Spatial Encoder (TSE) to obtain temporal-spatial inconsistency between audio-visual signals (real and fake can exist across frames along the temporal dimension). It is then followed by a Multi-Modal Joint Decoder (MMD) to fuse multi-modal features and learn inherent relationships concurrently. Finally, a Cross-Modal Classifier is developed to detect manipulation by detecting inter-modal and intra-modal disharmony. Furthermore, to test the effectiveness of the proposed deepfake detection model in real-world scenarios, the researchers introduced DefakeAVMiT \cite{10081373}, a multimodal deepfake dataset where various forgery techniques have been applied to different modalities.

In the PVASS-MDD framework proposed by Yu \textit{et al.} \cite{yu2023pvass}, there are two main modules: an auxiliary PVASS stage that focuses on exploring common correspondences between video and audio (AV) and a cross-modal predictive VA alignment module (MDD). The PVASS module works by iteratively predicting audio features using visual features and then reconstructing visual features based on audio features and prediction errors to eliminate discrepancies between video and audio. In the MDD stage, the frozen PVASS network from the first stage is used to align the VA features of real videos, enabling the detection network to better learn the inconsistencies between video and audio in deepfake videos. This MDD stage, with the assistance of PVASS, can extract more accurate VA inconsistencies for multimodal deepfake detection. When it comes to detecting deepfakes, Feng \textit{et al.} \cite{feng2023self} took a different approach compared to other techniques. Instead of treating it as a classification problem, they looked at it as an anomaly detection problem. They analyzed the distribution of audio-visual examples and flagged those with low probability. They focused on subtle properties that manipulated videos are unlikely to accurately capture. They used three unique features for audio-visual anomaly detection: discrete time delay, time-delay distribution, and audio-visual network activations. They found that time-delay distribution is more meaningful for anomaly detection than time-delay alone. They also studied the effect of feature activations within the audio-visual synchronization network on anomaly detection. The results showed that manipulated videos can be detected by identifying unlikely sequences of these features based on a learned distribution. Mittal \textit{et al.} \cite{mittal2020emotions} proposed an innovative method for detecting alterations in videos, such as deepfakes. This approach utilizes both audio (speech) and video (face) data and extracts emotional features from both modalities. The method uses a Siamese network-based architecture (triplet learning) to process real and deepfake videos at the same time during training. It generates modality and perceived emotion embedding vectors for the subject's face and speech, which are then used to distinguish between real and fake content. Through experiments, the study demonstrated that the perceived emotion cues from both modalities play a crucial role in detecting deepfake content by assessing the similarity between modality signals. 

Cheng \textit{et al.} \cite{cheng2023voice} investigated using voice-face matching to detect deepfake videos. Their empirical results indicated that the identities behind voices and faces are often mismatched in deepfake videos and that voices and faces have some level of homogeneity. They detected deepfakes by examining the intrinsic correlation of facial and audio information, without using any additional auxiliary data such as more modalities or visual features. Muppalla \textit{et al.} \cite{muppalla2023integrating} utilised capsule networks to extract robust features from audio spectrograms and face visuals followed by multimodal fusion and classification for deepfake detection. They utilised both score-fusion and feature-fusion approaches, which substantially improved over the state-of-the-art methods. In their recent work, Liu \textit{et al.} \cite{liu2023magnifying} introduced an innovative multimodal Deepfake detection framework that enhances intra-modal and cross-modal forgery clues. The framework consists of several key modules. Firstly, the Forgery Clues Magnification Transformer (FCMT) module is proposed to capture temporal intra-modal defects by magnifying forgery clues based on sequence-level relationships. Additionally, a Distribution Difference Inconsistency Computing (DDIC) module, based on Jensen–Shannon divergence, is used to adaptively align multimodal information for further magnifying the cross-modal inconsistency. The framework also explores spatial artifacts by connecting multi-scale feature representation to provide comprehensive information. Finally, a feature fusion module is designed to adaptively fuse features to generate a more discriminative feature. Experimental results showed that the proposed framework outperforms independently trained models and demonstrated a superior generalisation on unseen types of Deepfake. The overall performance of the selected deepfake detection techniques on FakeAVCeleb dataset \cite{khalid2021fakeavceleb} is illustrated in Figure \ref{fig:multimodal_summary} in terms of accuracy and area under the curve (AUC) score.

\begin{figure}
    \centering
    \includegraphics[width=.5\linewidth]{figures_new/FakeAVCeleb.pdf}
    \caption{\textcolor{black}{Performance variation of recent state-of-the-art methods in detecting deepfakes on FakeAVCeleb dataset in terms of Accuracy and AUC.}}
    \label{fig:multimodal_summary}
\end{figure}

\textcolor{black}{Detecting deepfakes that involve manipulation across multiple types of media is a crucial task, and it is challenging to detect these using single-modal approaches highlighting the importance of addressing both audio and video manipulation in detection. While many methods have been proposed in the literature using different feature extraction and fusion techniques, most of them share similar architectural patterns. However, some state-of-the-art techniques have introduced novel approaches for deepfake detection. These advanced methods analyze audiovisual features iteratively, treating detection as an anomaly detection problem, and identifying inconsistencies using unique features like time-delay distribution and network activations. In addition, leveraging emotional features from both audio and visual sources, along with effective voice and face-matching techniques, greatly improves the chances of identifying deepfakes. More refined techniques also enhance the ability to detect forgery by emphasizing signs of manipulation and aligning the information from different media types. This leads to better performance and accuracy when detecting deepfakes that haven't been seen before. These developments in multimodal deepfake detection provide a more thorough and reliable way to spot manipulated content. Such improvements are essential for maintaining the trustworthiness of digital media and reducing the risks posed by misinformation in our increasingly complex online world.}

\subsection{Multimodal Deepfake Detection Tools}

\textcolor{black}{A Deepfake Detection Tool is an advanced software solution designed to identify and flag manipulated or synthetically generated digital content, including videos, audio, images, and increasingly, text. As deepfake generation techniques have become more sophisticated, detection tools have had to evolve as well, often utilizing state-of-the-art artificial intelligence (AI) and deep learning (DL) methods to keep pace with the capabilities of AI-generated forgeries. These detection tools analyze various features within digital media, employing algorithms that can identify subtle anomalies in pixel patterns, audio signals, or inconsistencies in images and videos that often occur during the deepfake creation process. Such tools not only serve as protective measures for individuals and organizations but are also gaining traction in sectors like law enforcement, cybersecurity, and media integrity, where they help maintain trustworthy sources of information. Table \ref{tab:tool_det_multimodal_deepfakes} provides a comparative overview of several currently available deepfake detection tools.}

\begin{table*}[htbp]
\caption{Top Tools to Detect Multimodal Deepfakes}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
Method & Free & Open-source & URL\\\hline\hline
Sentinel  & \xmark & \xmark & https://thesentinel.ai/ \\\hline
Sensity  & \xmark & \xmark & https://sensity.ai/  \\\hline
Audio Visual Forensics & \cmark & \cmark & https://github.com/cfeng16/audio-visual-forensics \\\hline
Deepware  & \xmark & \xmark & https://deepware.ai/  \\\hline
Reality-Defender  & \xmark & \xmark & https://www.realitydefender.com/ \\\hline
Phoneme-Viseme Mismatch Detector  & \xmark & article & https://ieeexplore.ieee.org/document/9151013 \\\hline
\end{tabular}}
\label{tab:tool_det_multimodal_deepfakes}
\end{table*}


\subsection{Future Trends in Multimodal Deepfake Generation and Detection}

\textcolor{black}{In recent years, the creation of multimodal deepfakes has progressed rapidly, creating an ongoing challenge between making realistic deepfakes and developing methods to detect them. Diffusion models are one recent approach in image generation, which can be adapted for multimedia deepfake creation. These models can manipulate visual features and, when combined with audio signal manipulation techniques, allow for the generation of synchronized, multimodal content that includes both audio and visual modifications \cite{du2024dfadd, firc2024diffuse, av2024latent, bhattacharyya2024diffusion}. Despite the advances in state-of-the-art deepfake generation methods, issues with synchronization across different modalities (such as audio and video) persist. These synchronization inconsistencies are detectable by advanced detection systems and can help identify manipulated content \cite{liz2024generation, ivanovska2024vulnerability, mubarak2023survey}. To improve both deepfake generation and detection methods, high-quality datasets with a wide range of audio and video manipulations are essential. Adding datasets with content from multiple languages (most datasets are currently English-only) and diverse demographic representation would also be beneficial. This would support the development of more robust and generalized generation and detection models. Another key area in deepfake detection is explainability \cite{haq2024multimodal, tsigos2024towards}. Currently, most detection methods rely on deep learning, but what these models specifically learn to distinguish real from fake samples remains unclear. Future research could focus on understanding and interpreting these learning patterns, which could enhance both the effectiveness and trustworthiness of detection systems. In the future, multimodal deepfakes are likely to become increasingly realistic and harder to detect as techniques evolve. This makes it critical for research to keep pace, continually improving both generation and detection methods to address these advancements.}


\subsection{Combating Multimodal Deepfakes in Multimodal Biometrics}
\label{subsec:multimodal_biometrics}
With the advances in biometric evaluation techniques multimodal biometric recognition has also been recently introduced. Therefore, it is important to investigate the ability of multimodal deepfakes, especially the voice and face multimodal systems to thwart multimodal biometric recognition. In the following subsection, we discuss the summary of the findings of our evaluation, and a detailed discussion is provided in Sec. III of supplementary material
\subsubsection{Efficacy of multimodal deepfakes to fool multimodal biometrics systems}
Table III in Sec. III of supplementary material discusses the effectiveness of state-of-the-art multimodal deepfakes to thwart multimodal biometric recognition. Please note that this evaluation was conducted using voice and face multimodal systems and we considered a framework where voice modality and face modality are individually validated biometrically and the final decision is generated by fusing the individual decisions. While the current state-of-the-art multimodal deepfake generation methods failed to thwart the overall framework, especially due to their poor performance in manipulating the voice modality, the advances in multimodal deepfake technology could soon surpass the multimodal biometric reconnection and become a threat to multimodal authentication systems.


\subsubsection{Measures for revealing true identity:} To the best of our knowledge, there is no method to recover true identity from multimodal deepfakes. 



% \newpage

% \subsubsection{Universal multimodal deepfake detectors ? \textcolor{black}{I think this must come under unimodal techniques. Couldn't find any one detector that works on both audio and video. But there are many scenarios with identifying multiple types of attacks using single modality with one network}}

% \hspace{2mm}
% \subsection{Combating multimodal deepfakes in multimodal biometrics}
% \subsubsection{Efficacy of multimodal deepfakes to fool multimodal biometrics systems}
% \subsubsection{Measures for revealing true identity}