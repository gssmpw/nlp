\section{Approach}\label{sec:approach}
% \begin{tcolorbox}[colback=lightgray!10, colframe=black, title={Research Aim}]
% We investigate how structured annotation guidelines impact instruction-tuned LLMs for event extraction, assessing their effectiveness across training sizes, guideline sources, and cross-schema generalization.
% \end{tcolorbox}
% \vspace{-10pt}

% In this section, we formalize the event extraction task using LLMs (\textsection \ref{sec:formulation}) and detail the guideline generation procedure used to prompt LLMs (\textsection \ref{sec:guideline_generation}).

\subsection{Task Formulation}
\label{sec:formulation}
% \zyc{describe the instruction tuning formulation for EE. describe instructions with annotation guidelines. referring to Fig~\ref{fig:intro}.}
Given an input sentence \( X \), the goal of EE is to extract the structured event information $Y$ from the sentence, adhering to predefined schema constraints $\mathcal{E}$. The extraction task consists of (1) \textbf{Trigger Extraction}, which localizes an event trigger span and classifies its event type, and (2) \textbf{Argument Extraction}, where the task is to identify spans in \( X \) that serve as argument roles within the extracted event instance.  

When an autoregressive LLM is tasked with EE, the extraction of event instances is formulated in a generative way, with the LLM generating a sentence describing the extracted event instances. Specifically, the prompt to the LLM is defined as  $P = [I \oplus {E}_e \oplus X]$, where \(\oplus\) is the concatenation operation, \( I \) represents the {task instruction}, which specifies the structured output format and task definition, and \( {E}_e \in \mathcal{E}\) denotes the {event schema} of an interested type $e$ from a predefined set $\mathcal{E}$.
% which defines the event type \( e \) along with its argument roles. 


% Fine-tuning is performed via {causal language modeling}, leveraging a {next-token prediction objective} where the model generates structured event representations autoregressive. 
Let \( \mathcal{D} = \{(e_i, X_i, Y_i)\}_{i=1}^{N} \) denote a dataset of annotated event examples, where each \( X_i \) corresponds to a prompt instance \( P_i \) for the interested event type $e_i$. The objective function of instruction tuning for EE is as follows: $\mathcal{L}(\mathcal{D}; \theta) = - \sum_{i} \sum_{j} \log p_{\theta} (Y_{ij} \mid P_i, Y_{i,<j})$,
% Given a {pretrained model} parameterized by \( \theta \), we optimize the {conditional log-likelihood}:
% \begin{equation}
%     \mathcal{L}(\mathcal{D}; \theta) = - \sum_{i} \sum_{j} \log p_{\theta} (Y_{ij} \mid P_i, Y_{i,<j})
% \end{equation}
where \( Y_{i,<j} \) represents previously generated tokens in the structured output sequence, ensuring an autoregressive formulation. 

Existing work identified the structure of EE outputs to be critical~\cite{jiao-etal-2023-instruct, wang2023code4struct}. In particular, \citet{wang2023code4struct} found that formulating the EE output in a \emph{code format} can {take advantage of Programming Language features such as inheritance and type annotation to introduce external knowledge or add constraints.} In our work, we follow the same formatting strategy and represent the EE task as a code generation problem. Specifically, the event schema ${E}_e$ is represented as a Python class; accordingly, every extracted event instance is represented as a Python object of the corresponding event class. When there are multiple event instances implied in the input $X$, a list of Python objects will be generated; when there is no event specified in $X$, we expect an empty Python list to be the model output. An example is shown in Figure~\ref{fig:overview}.

% During training, we partition the sequence into a {source context} (\( P_i \)) and a {target output} (\( Y_i \)), applying the loss function {only to target tokens} (\emph{label loss}).
During training, we provide only the ground-truth event schema in the prompt; when the text input $X$ does not include any event, a random event schema will be chosen. 
At inference time, given a test instance $X$, we pair the input with every possible event type in the schema set $\mathcal{E}$, prompt the LLM to extract any implied event instances, and perform model evaluation based on the aggregated extraction outputs. As such, a well-performing LLM needs both extract the complete event instances and avoid events that are not indicated in $X$.
% the model generates output tokens autoregressively, until an end-of-sequence (EOS) token is generated or a predefined maximum length is reached.



% \subsection{Generating Annotation Guidelines with LLMs}
\subsection{Instruction-Tuning LLMs with Annotation Guidelines}
\label{sec:guideline_generation}
% \zyc{describe the guideline variants. need one or more figures of prompts + examples.}

Recent work by \citet{sainz2024gollie} demonstrated the effectiveness of integrating annotation guidelines in the code-format instructions of IE tasks. Specifically, when describing the event type schema $E_e$, a textual description is added to the event type and each of its argument roles (Figure~\ref{fig:overview}). As such, the LLM is expected to more easily understand the meaning of the event type while being instructed to extract any occurring events from the input $X$. While \citet{sainz2024gollie} evaluated annotation guidelines in the broad IE task, their main focus has been on Named Entity Recognition, rather than the complicated EE task. 
% However, since IE encompasses a broad range of sub-tasks, their analysis of guideline usability in EE remains underexplored. 
Furthermore, their approach assumed the availability of pre-existing human-curated guidelines, an assumption that may not always hold in real-world applications. To bridge this gap, we explore methods to automatically generate annotation guidelines and assess their effectiveness in comparison to human-authored ones.\footnote{Human-written guidelines for ACE05 are available \href{https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf}{here}.}
\input{guideline-examples}

To develop a scalable and cost-effective approach for guideline generation, we employ a reverse engineering strategy, leveraging both annotated event examples and the strong generative capabilities of LLMs. As illustrated in Figure X, we construct a guideline generation prompt for each event type $e$ by providing a few annotated examples $\{(X_i, Y_i)\}$ demonstrating the existence or non-existence of event instance of type $e$, and then prompt an LLM (GPT-4o in our experiment) to generate annotation guidelines for $e$. In total, we experimented with five variants of machine-generated guidelines:
% we construct guideline generation prompts by providing 10 annotated examples per event type $e$, using them to generate 5 distinct definitions for the event type and its corresponding argument roles. 
% Formally, each guideline generation prompt consists of a task instruction, an input text instance, and its corresponding event annotations $(t_i, a_i)$. Additionally, we investigate refining guidelines through the inclusion of annotations from different event types $\hat{e}$  and semantically similar event types, incorporating negative constraints (e.g., ``what not to do'') to enhance model robustness against corner cases.
% In our experiments, we prompt GPT-4 to elicit the guidelines \(E\) across five distinct settings, each designed to introduce different levels of specificity and contrast. 
\textbf{(1) Guideline-P:} We prompt the LLM with 10 positive annotated examples of type $e$ to generate the annotation guidelines. Inspired by \citet{sainz2024gollie}, we sample 5 distinct guidelines for each event type, which can be used during the model training to ensure that the model is exposed to multiple rephrasings of the guidelines rather than memorizing and overfitting to a specific one.
% The LLM generates guidelines based on 10 positive event instances without additional constraints.
\textbf{(2) Guideline-PN (Positive + Negative Examples):} In addition to 10 positive event annotations, we also provide 15 negative annotations where the input $X$ does not imply event instances of type $e$. Similarly, we prompt the LLM to generate 5 distinct guidelines for each event type.
% Extends Guideline-P by incorporating 15 additional samples from different event types, introducing edge cases and exceptions.
\textbf{(3) Guideline-PS (Positive + Sibling Events):} Similar to Guideline-PN, we prompt the LLM with both positive and negative event annotations. However, the negative annotations are selected from the sibling event types of the target type $e$ (e.g., Arrest vs. Jail), as defined by the event ontology. We hypothesize that the critical challenge for EE lies in distinguishing between sibling event types; hence, an instructed LLM can benefit from following annotation guidelines that particularly emphasize the difference between sibling event types. As in the earlier variants, we generate 5 guideline samples per event type.
\textbf{(4) Guideline-PN-Int} and \textbf{(5) Guideline-PS-Int:} Finally, we create two more variants that \underline{Int}egrate the 5 diverse guideline samples from Guideline-PN and Guideline-PS into a comprehensive one, respectively. 
% Intuitively, as these variants consolidate the diverse ways of describing each event schema, we expect them to be more effective guidelines for instructing LLMs in EE tasks. 
Examples of the 5 guideline variants are shown in Table~\ref{tab:guideline-examples}.
% \sriv{Should we highlight why 15? and 10? For negative samples: we picked 15 inspired by Degree}
% \zyc{Point people to Appendix for the prompt templates.}
The prompt templates used for generating guidelines and example generations
% across all settings including the 5 diverse variations and the consolidated versions 
are provided in Appendix\hyperref[sec:prompt-design]{~C} and~\ref{sec:app_dd}, respectively.
% Augments Guideline-P by adding 15 samples from semantically similar event types (e.g., Arrest vs. Jail), facilitating a more nuanced differentiation between event types.
% Finally, we create two new prompts by gathering 5 different distinct generations from Guideline-PN and Guideline-PS stages to generate \textbf{4) Guideline-PN-Adv}, and \textbf{(5) Guideline-PS-Adv}, respectively.

