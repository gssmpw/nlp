\subsection{Experimental Setup} 
\input{latex/tables/data_stats-v2}
\input{latex/tables/e2e_full}
%\input{latex/tables/e2e_2000}
%\input{latex/tables/e2e_100}
% \zyc{two datasets + stats, using the version of TextEE. Code formatting - our processed dataset will be released. describe how you sampled 100 (for three times) and 2000, how you prepared Dev 100 for checkpoint selection. How you do evaluation (enumerating all ETs). CITE Gollie and emphasize the difference in data preparation (hinting that our results are not directly comparable).}
%[Sweta: Covered all the pointers]
\zyc{this section can be shortened and we move most things to appendix.}

\paragraph{Datasets.}
We perform experiments on two standard EE datasets: \textbf{ACE05} \cite{X} and \textbf{RichERE} \cite{Y}. Both of them exhibit fine-grained event distinctions, and RichERE includes sparser event annotations (i.e., fewer event-labeled sentences), which makes it more challenging. \zy{Moreover, RichERE does not come with human-written annotation guidelines.}
% ACE05 is a benchmark dataset with 33 event types spanning newswires, weblogs, and conversational transcripts. It exhibits fine-grained event distinctions, often leading to inter-event confusion (e.g., Arrest vs. Trial vs. Conviction). RichERE builds on the ACE framework, covering broader event types across news and intelligence reports. Unlike ACE, it has sparse event annotations, i.e., fewer event-labeled sentences, making extraction more challenging. 
For both datasets, we follow the TextEE standardization \cite{huang2024textee} and formulate them as sentence-level EE tasks. We use the ``split 1'' data split of TextEE, but only sample a subset of 100 examples from its development (dev) set for better training efficiency. Specifically, we ensure that for each event type, two event instances will be included in our dev set, prioritizing those with larger coverages of arguments, with the remaining being examples with no event occurrences. The datasets are then converted to the code format shown in Figure~\ref{?}. Table~\ref{tab:data_stats} summarizes dataset statistics.

To perform the low-data experiments (RQ5), we additionally create the following subsets of the full training set for each dataset. \textbf{Train2k} includes uniformly sampled 2,000 examples from the full training set. \textbf{Train100-1/2/3} are three distinct subsets including 100 examples from the full training set, each of which was selected following the same procedure as how we prepare the dev set, ensuring all event types are included and prioritizing instances covering more arguments.



% \paragraph{Data Sampling.} 
% To systematically evaluate the impact of annotation guidelines in both low-resource and larger training settings, we sampled training subsets of 100 and 2,000 instances from the full dataset. The 100-instance samples, drawn independently three times, allows us to assess the stability of model performance under data scarcity, ensuring our findings were not biased by a single selection. The 2,000-instance sample provided a broader training distribution, enabling comparisons on a more extensive dataset. To maintain fairness in checkpoint selection, we consistently sampled 100 development instances, preventing variance in model evaluation. Our sampling strategy preserved the natural event distribution, ensuring representation across all event types while including non-event instances (sentences without events) to maintain real-world diversity. Additionally, we prioritized argument diversity, ensuring that sampled instances contained a wide range of argument roles to test the model's ability to generalize across event structures.


% \paragraph{Data Preprocessing.} 
% To ensure a fair and reproducible preprocessing, we build upon TextEE \cite{textEE}, which standardizes data preparation and evaluation pipelines for EE. While TextEE primarily relies on natural language schema definitions, we extend its approach by transforming event instances into structured code prompts. This transition is motivated by the effectiveness of code-based schemas in LLM understanding \cite{wang2023code4struct}, where structured representations enhance schema comprehension without requiring prompt engineering expertise. Our preprocessing framework encodes event definitions within Python docstrings, argument roles as inline comments, and hierarchical event relations using class inheritance (Fig. \ref{X}), offering a clear, structured, and explicit representation of event ontologies. Furthermore, leveraging Python’s class-based hierarchy for event ontologies not only improves schema consistency and generalization but also enables efficient transfer of knowledge across related event types.


% \subsection{Evaluation Methodology and Metrics}
\paragraph{Evaluation.} 
% At inference time, we instruct an LLM to extract events for one type at a time and evaluate its performance based on the aggregated results. We note that this is a more precise evaluation process than prior work; for example, \citet{sainz2024gollie} only fed the siblings of the ground-truth event types, which could lead to an overestimation of a model's EE performance.
% Our methodology contrasts with GoLLIE~\cite{cite_gollie}, which follows a pipeline-based structure and selectively includes only parent event types in its prompts, limiting granularity in event representation. For argument extraction, GoLLIE further restricts schema inclusion to sibling event types, introducing manual design choices that reduce automation and scalability. To ensure fair and comprehensive evaluation, we adopt a methodology that enumerates all possible event types for each test and development sample during prompt construction. Unlike setups where only the gold-standard event schema is included in the prompt, we avoid implicit event detection bias—if the correct event type were provided, the model would not need to identify the event type itself and could directly extract arguments, which would not reflect its real performance on real-world data. Due to these fundamental differences in methodology, we do not compare our results with GoLLIE.
% \paragraph{Evaluation Metrics.} 
Following prior work~\cite{lin2020, huang2024textee}, we evaluate the model on four F1 metrics:
% using Trigger F1 and Argument F1, focusing on both events and argument extraction. Specifically, we report:  
\textbf{(1) Trigger Identification (TI)}, which measures correct trigger span extraction, \textbf{(2) Trigger Classification (TC)}, which additionally requires event-type correctness, \textbf{(3) Argument Identification (AI)}, which ensures correct argument role association with the predicted trigger, and \textbf{(4) Argument Classification (AC)}, which further requires role-type correctness and is thus the most comprehensive metric on a model's EE performance. When evaluating the model on the Guideline-P, PN, and PS variants, one guideline is randomly selected each time.

% Since we represent events in a structured code format, we develop a custom evaluation script that seamlessly aligns with our model’s structured outputs while maintaining compatibility with standard EE benchmarks.
As a side benefit of representing events in a structured code format, we can easily evaluate an extracted event instance by directly instantiating its corresponding Python object based on the event schema's Python class definitions, checking if the object is valid (e.g., missing arguments or including hallucinated arguments) and comparing it with the ground-truth object. This code-based evaluation thus prevents the tedious string-matching process adopted in prior work~\cite{?}. We include details of our data preprocessing and code-based EE evaluation in Appendix~\ref{?} and will release our scripts as community resources in the future.

\paragraph{Model Training.} We experimented with the LLaMA-3-8B model~\cite{?}, selected for its demonstrated proficiency in processing structured code-based inputs and generating coherent outputs. When instruction-tuning the model under the Guideline-P, PN, and PS variants, we randomly sample one of the generated guidelines, a strategy found to prevent the model from memorizing specific guidelines in~\citet{sainz2024gollie}.
For parameter-efficient training, we implemented rsLoRA~\cite{RSLORA} using the Unsloth framework~\cite{unsloth}. Key hyperparameters—including LoRA rank (64), scaling factor $\alpha$ (128), and batch size (32)—were determined through preliminary experiments to balance computational efficiency with model performance. Models were trained for 10 epochs using a single NVIDIA A100 GPU (80GB VRAM) with early stopping based on their dev-set performance. Further details are included in Appendix~\ref{?}.

% \subsection{Prompt Design and Model Training}
% \paragraph{Prompt Design.}
% We adopt a structured prompt format consisting of four components: (i) task instruction,(ii) event schema, (iii) input text, and (iv) expected output, formatted as a structured event representation. Our approach follows a schema-first prompting strategy, where event definitions are explicitly encoded in a structured format to enhance model comprehension of event relations and argument constraints. For each input instance, a randomly sampled guideline definition is used to annotate the event schema, ensuring that the model is exposed to multiple rephrasings rather than memorizing and overfitting on a static definition. Formally, we prepare the input sequence as follows: ``\texttt{[BoS] \$-task\_instruction $(I)$ \$-annotated event schema $(\mathcal{E}_e)$~\$-input\_sample~$(X_i)$~[EoS]}'' where  the event schema $\mathcal{E}_e$ for an event $e$ is annotated with one of the generated guideline definitions.
% \paragraph{Model.} 
% We conducted experiments on an instruction-tuned LLaMA-3-8B model, selected for its demonstrated proficiency in processing structured code-based inputs and generating coherent outputs. For parameter-efficient adaptation, we implement RSLoRA \cite{RSLORA}, applying LoRA transformations to all linear layers in the transformer blocks following the methodology of \citet{xe}. Key hyperparameters—including LoRA rank (64), scaling factor $\alpha$ (128), and batch size (32)—were determined through preliminary experiments to balance computational efficiency with model performance. The models were trained for 10 epochs using a single NVIDIA A100 GPU (80GB VRAM), with early stopping triggered after three consecutive validation steps without improvement. We adopt a cosine learning rate scheduler with an initial rate of 1e-5 and a warmup period of 350 steps. Input sequences are padded to 3,000 tokens to maintain consistency while accommodating long-form code structures. To ensure reproducibility and minimize memory fragmentation, we implement deterministic padding and truncation strategies. Complete configuration details—including ablation studies of alternative hyperparameter choices and architectural variations—are documented in Appendix \sriv{X}.
