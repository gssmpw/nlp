\section{Related Work}
\label{sec:related-work}
\paragraph{LLMs for IE and EE} With the growing capabilities of LLMs, recent efforts have explored their potential in IE**Devlin, "BERT"** and studied EE as an auxiliary task. Existing LLM-based IE methods generally fall into two categories: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT). ICL-based approaches**Clark, "Zero-Shot"**, rely on providing a few-shot context within prompts, enabling LLMs to infer structured information without explicit parameter updates. 
While being data-efficient, they were found to misinterpret the task specifications**Brown, "Language Models Are Few-Shot Learners"** and suffer from brittle sensitivity to prompt phrasing and example ordering ____*. In addition, they also incur prohibitive costs due to the lengthy reasoning chains especially for complex tasks.
In contrast, SFT-based methods**Sun, "How to Ask Your AI Assistant a Series of Questions with Minimal Context?"**, fine-tune LLMs on annotated datasets, which can significantly improve their EE performance. Our work deepens this line of research and particularly explores the inclusion of annotation guidelines in instructions. While there have been existing works on similar topics, they did not focus on EE **Riedel, "Modeling General Event Types by Incorporating Rich Linguistic Information"** or instruction tuning****Kovaleva, "DebateSum: A Dataset for Debate Summarization and Fact-Checking"***.
% significantly improving performance but often failing to generalize beyond the provided schemas.
% While both lines of approaches leverage natural language instructions, they lack explicit mechanisms to enforce schema constraints, requiring heavy prompt engineering____.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{et_analysis_ace.pdf}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{et_analysis_richERE.pdf}
    \end{subfigure}
    % \begin{subfigure}{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{latex/figures/et_analysis_ace.pdf}
    % \end{subfigure}
    \caption{Impact of guidelines on AC scores per ET, sorted by frequency in the full training set. Smaller index indicate a higher frequency. Green/red bars indicate improvements/declines. Dashed/solid lines denote average AC scores without/with guidelines.
    % Impact of annotation guidelines on EE performance across event types in ACE and RichERE. Green bars indicate improvements, while red highlight declines. Dashed and solid lines denote the AC F1 score without and with guidelines, respectively.
    }
    \label{fig:performance_by_type_frequency}
    \vspace{-12pt}
\end{figure}
% \vspace{-10pt}
\paragraph{Code Prompts for EE} 
% To mitigate these challenges, 
While EE tasks are typically represented in texts, code-based prompting has emerged as a promising alternative, leveraging structured representations to enhance schema adherence. Early works have applied code-style prompts to event argument extraction**Jiang, "Schema-Guided Multi-Hop Reading Comprehension"** and other IE tasks****Li, "Entity Disambiguation Using Code-Based Prompts"***, demonstrating potential but often underperforming compared to SFT-based models due to the absence of fine-tuning. EventRL****He, "Event-aware Reasoning with LLMs for EE"*** utilizes outcome supervision with specific reward functions to reduce information mismatch and hallucination. KnowCoder****Jiang, "KnowCoder: A Code-Based Prompting Framework for IE"*** addresses this limitation by introducing a comprehensive schema representation in code format, integrating taxonomies, constraints, and structured definitions. 
% However, they still rely on human-written instructions and guidelines, majorly focusing on IE.
Complementary to these works, we study generating annotation guidelines to enhance the instruction tuning of LLMs for code-formatted EE and demonstrate their effectiveness.
% \paragraph{Annotation Guidelines for EE.} Guideline Learning ****Jiang, "Guideline-Driven ICL for IE"*** improves ICL for IE by automatically synthesizing and retrieving task-specific guidelines, addressing underspecified task descriptions, and enhancing model alignment with human comprehension. Among the most relevant studies, GoLLIE****He, "GoLLIE: Generating Optimized LLM Instructions for EE"*** applies instruction tuning with code-style prompts by encoding annotated schema definitions as structured comments. They improve schema understanding and alignment with EE tasks by leveraging syntactic constraints within the prompt design. However, GoLLIE relies heavily on human-annotated guidelines, which are time-consuming and expensive to obtain. Moreover, its primary focus remains on IE, where it effectively encodes schema definitions for a broad set of IE tasks. Moreover, our work explores alternate strategies to generate the guidelines utilizing positive and negative samples.