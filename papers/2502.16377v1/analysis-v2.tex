\subsection{Further Analysis}
\label{sec:analyais}
\input{LLaMA_3_2}

\paragraph{Generalization to a Smaller LLM}
We experimented with LLaMA-3.2-1B-Instruct for \textbf{NoGuideline} and the best-performing guideline variant \textbf{Guideline-PN}. 
%... \zyc{1-2 sentences to say if the advantages of guidelines generalize to smaller lLM or not.}
Results in Table~\ref{tab:llama_3.2} display a consistent observation compared to experiments with the larger LLaMA-3.1-8B model (Table~\ref{tab:full_train_table}). That is,
\textbf{Guideline-PN} achieves a comparable or better result than \textbf{NoGuideline} and shows the advantage of guidelines, particularly on \textbf{RichERE w/ NS}.
% show that guidelines improve performance across model scales. Although overall 1B F1 scores are slightly lower than those in Table~\ref{tab:full_train_table}, AC F1 in RichERE \textbf{NoGuideline w/ NS} surpasses the 8B model by 1.13 points. Since \textbf{Guideline-PN} consistently outperforms NoGuideline, guidelines remain a valuable strategy for enhancing extraction quality in smaller LLMs, especially when computational constraints limit model size.}

% Based on the main experiment results from Table~\ref{tab:llama_3.2}, we selected the guideline settings with the best performance across experiments for further exploration with the DeepSeek-R1-Distill-Qwen-1.5B model. Specifically, we chose NoGuideline, Guideline-PN, and Guideline-PS for additional testing with smaller LLMs, focusing on their effectiveness in specialized tasks. Guideline-PN w/ NS achieved the highest F1 score across all 4 metrics, with (61.14) in TC and (39.88) in AC (see Table~\ref{tab:deepseek_results}), demonstrating the value of guidelines and negative samples in improving performance for these tasks. Across all settings, adding negative samples improved performance, and the guideline-based methods consistently performed better as seen from the TC and AC F1 scores, when compared to the NoGuideline setting. While these results are strong, they also highlight the distinct capabilities of smaller models like DeepSeek-R1-Distill-Qwen-1.5B, which, although competitive, still lag behind larger models like LLaMA 3.1 8B in certain tasks.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{analysis_radar.pdf}
    \caption{Error categorization: CA (Context Ambiguity), PE (Parsing Errors), MAE (Missing Arguments/Events), AE (Argument Errors), TTE (Type/Trigger Errors), and LN (Label Noise).
    % Randomly picked 100 samples from ACE and RichERE and categorized the most common errors.
    %Error categorization on each dataset based on randomly picked 100 examples where NoGuideline w/o NS made mistakes.
    }
    \label{fig:error_cat}
    \vspace{-1.5em}
\end{figure}

\paragraph{Error Analysis}
We randomly selected 100 examples on each dataset where \textbf{NoGuideline w/o NS} made mistakes and compared them with errors made by other variants. The results in Figure~\ref{fig:error_cat} show that, on ACE w/o NS, including the annotation guidelines leads to increasing ungrammatical code outputs and parsing errors (PE), although it dramatically reduces the event type and trigger errors (TTE). In the case of w/ NS, guidelines help in almost all aspects, with the majority of remaining errors being caused by missing arguments or events (MAE) and label noise (LN). On RichERE, however, we observe that for both w/o and w/ NS cases, the annotation guidelines enhance the model performance in all dimensions.
% We analyzed 100 erroneous predictions across each guideline setting within the RichERE and ACE datasets, categorizing failures into six types: context ambiguity (CA), parsing errors (PE), missing multiple arguments/events (MMA), argument extraction errors (AE), trigger/type errors (WT/ET), and label noise (LN). CA errors occur when the model misinterprets the meaning of a phrase. For example, in "the officer is going to go in and swoop her away," the model misclassifies "go" and "swoop" as Transport events, misinterpreting the metaphor. LN arises when there is a mismatch between the ground truth annotations and the annotation guidelines. In "Secession by Iraqi Kurds could inspire Turkey's rebel Kurds, who for 15 years have been fighting for autonomy," the ground truth incorrectly labels "fighting" as a Demonstrate event. A similar issue was reported in the GOLLIE paper (cite GOLLIE). Figure~\ref{fig:error_cat} shows the distribution of these errors, with the following observations.

% In the No Guideline w/o NS setting, RichERE struggles with polysemous triggers (WT/ET: 39), contextual ambiguity (CA: 36), and missing arguments (MMA: 21). Negative sampling (No Guideline w/ NS) reduces CA (36 → 6) and WT/ET (39 → 12), improving event disambiguation, though AE (9 → 6) remains challenging due to complex argument structures. Guideline-P w/ NS further reduces MMA (21 → 9) and WT/ET (39 → 18) highlighting performance improvement through annotation guidelines, but CA remains high (24), indicating unresolved ambiguity. Error rates are the lowest in the Guideline-PN w/ NS setting, with CA dropping to 3 and WT/ET to 3, demonstrating the effectiveness of guideline generation through the inclusion of contrasting events, coupled with negative samples.

% In the No Guideline w/o NS setting, ACE shows notable errors with MMA (19), CA (13), and AE (19), indicating challenges in capturing multiple arguments, resolving ambiguity, and extracting arguments. Negative sampling (No Guideline w/ NS) reduces CA (13 → 5) and WT/ET (13 → 11), improving event disambiguation by exposing the model to both correct and incorrect event types, helping refine decision boundaries. Guideline-P w/ NS further lowers WT/ET (13 → 8) and AE (19 → 12), with minor improvements in other categories, suggesting that annotation guidelines reduce misclassifications in WT/ET, while clearer event role definitions enhance AE, minimizing confusion in argument assignment. In the Guideline-PN w/ NS setting, error rates are the lowest, with CA and AE dropping to 2, reinforcing that guidelines and negative sampling together improve event differentiation and argument extraction.



\paragraph{Effectiveness of Guidelines per Event Type Frequency}
% Recent event extraction (EE) studies highlight the challenge of low-frequency event types (ETs), where data sparsity limits model generalization and degrades extraction performance, particularly for argument classification (AC) and argument identification (AI) \cite{Exploring the Feasibility of ChatGPT for Event Extraction}. To evaluate whether annotation guidelines can mitigate this issue, we compare the best-performing guideline approach without negative sampling (Guideline-PN w/o NS) against the NoGuideline w/o NS on the ACE test set to assess the impact of guidelines.
As shown in Figure~\ref{fig:performance_by_type_frequency}, frequent event types show consistent improvements with annotation guidelines, as indicated by the green bars, suggesting that even well-represented events benefit from enhanced annotations. Only a few declines (red bars) occurred with mid- to low-frequency event types, whereas most event types still benefit from the guidelines. In fact, these event types, especially the very rare ones (top of the figure), generally present larger gains (i.e., longer green bars) than the more frequent types, which demonstrates a unique advantage of annotation guidelines in low-resource settings.
% The occurrence of red bars is minimal for medium-frequency events, indicating that guidelines are generally beneficial for these events as most of them are represented by green bars. In contrast, rare event types (top) display a higher occurrence of red bars, reinforcing prior findings that guidelines alone may not fully resolve data sparsity issues. However, certain low-resource events, such as \textit{Convict}, show substantial gains, indicating that guidelines can still be beneficial in some underrepresented cases.





