% \subsection{Experimental Setup}
\subsection{Datasets and Preprocessing} 

\zyc{two datasets + stats, using the version of TextEE. Code formatting - our processed dataset will be released. describe how you sampled 100 (for three times) and 2000, how you prepared Dev 100 for checkpoint selection. How you do evaluation (enumerating all ETs). CITE Gollie and emphasize the difference in data preparation (hinting that our results are not directly comparable).}
%[Sweta: Covered all the pointers]

\paragraph{Datasets.}
We evaluate our models on two standard event extraction datasets: \textbf{ACE 2005} \cite{X} and \textbf{RichERE} \cite{Y}. ACE 2005 is a benchmark dataset with 33 event types spanning newswire, weblogs, and conversational transcripts. It exhibits fine-grained event distinctions, often leading to inter-event confusion (e.g., Arrest vs. Trial vs. Conviction). RichERE builds on the ACE framework, covering broader event types across news and intelligence reports. Unlike ACE, it has sparse event annotations, i.e., fewer event-labeled sentences, making extraction more challenging. Table~\ref{tab:data_stats} summarizes dataset statistics.
% These two datasets provide a contrast between dense vs. sparse event annotations and allow us to examine how annotation guidelines influence instruction-tuned models in different settings, making our findings more generalizable. 
% We summarize the dataset statistics in Table~\ref{table1}.

\input{latex/tables/data_stats}

\paragraph{Data Sampling.} 
To systematically evaluate the impact of annotation guidelines in both low-resource and larger training settings, we sampled training subsets of 100 and 2,000 instances from the full dataset. The 100-instance samples, drawn independently three times, allows us to assess the stability of model performance under data scarcity, ensuring our findings were not biased by a single selection. The 2,000-instance sample provided a broader training distribution, enabling comparisons on a more extensive dataset. To maintain fairness in checkpoint selection, we consistently sampled 100 development instances, preventing variance in model evaluation. Our sampling strategy preserved the natural event distribution, ensuring representation across all event types while including non-event instances (sentences without events) to maintain real-world diversity. Additionally, we prioritized argument diversity, ensuring that sampled instances contained a wide range of argument roles to test the model's ability to generalize across event structures.
% To ensure robust training and evaluation, we employed a comprehensive sampling strategy alongside the complete training data. We performed three independent samplings of 100 training instances to assess stability and variability of the model across smaller and controlled datasets. This approach allowed us to observe how the model performs with limited data and ensured that our findings were not influenced by a single random sample. Additionally, a larger set of 2,000 training instances was sampled to evaluate model performance on a more extensive dataset.

% For checkpoint selection, we sampled 100 instances from the development (dev) set to maintain uniformity in identifying the best-performing model checkpoints. This consistent evaluation set helped in comparing model checkpoints without variability introduced by differing dev data samples.

% During the sampling process, we ensured that all event types present in the datasets were represented. We also included instances that were not associated with any event type to maintain the natural distribution of the data. Beyond event coverage, we focused on maximizing argument diversity, ensuring that the sampled data captured a comprehensive range of argument roles and structures for each event type. 


\paragraph{Data Preprocessing.} 
To ensure a fair and reproducible preprocessing, we build upon TextEE \cite{textEE}, which standardizes data preparation and evaluation pipelines for EE. While TextEE primarily relies on natural language schema definitions, we extend its approach by transforming event instances into structured code prompts. This transition is motivated by the effectiveness of code-based schemas in LLM understanding \cite{wang2023code4struct}, where structured representations enhance schema comprehension without requiring prompt engineering expertise. Our preprocessing framework encodes event definitions within Python docstrings, argument roles as inline comments, and hierarchical event relations using class inheritance (Fig. \ref{X}), offering a clear, structured, and explicit representation of event ontologies. Furthermore, leveraging Python’s class-based hierarchy for event ontologies not only improves schema consistency and generalization but also enables efficient transfer of knowledge across related event types.

% The TextEE paper \citet{TextEE} lays a strong foundation for event extraction research by standardizing preprocessing and evaluation scripts across 16 datasets, covering a variety of domains. While TextEE focuses on using natural language instructions for schema definitions, it highlights the need for a unified and reproducible evaluation framework for event extraction tasks.

% Building upon this foundation, we extend TextEE by transforming natural language instructions into code prompts through our standardized preprocessing scripts. This approach provides LLMs with precise schema definitions, enabling them to better understand and generalize across diverse contexts. For instance, hierarchical structures in event types, such as "Conflict" as a parent category and "Attack" as a subcategory, are seamlessly captured in code, facilitating knowledge transfer across related events. This transition is motivated by the performance improvements presented by the KnowCoder paper \citet{KnowCoder}, which showed the effectiveness of code-style schema representations over natural language for Universal Information Extraction (UIE) tasks.

% To enhance the interpretability and accuracy of event and argument definitions, we incorporated annotation guidelines directly into the code prompts as comments. These guidelines, whether machine-generated or sourced from human-annotated resources, provide explicit instructions on the characteristics and roles of each event type and argument, ensuring consistent understanding across different experimental settings.

% Highlighting the advantages of using code prompts in event extraction tasks:
% \begin{itemize}
%     \item \textbf{Structured Representation}: Python’s object-oriented constructs, such as classes and inheritance, explicitly model complex event relationships and constraints.
%     \item \textbf{Precision and Clarity}: Code prompts reduce ambiguity with features such as type hints, ensuring accurate and consistent schema definitions.
%     \item \textbf{Enhanced Generalization}: Standardized code-based schemas enable LLMs to better generalize across diverse datasets and domains.
% \end{itemize} \sweta{Added a data preparation section separately from data sampling}

\subsection{Evaluation Methodology and Metrics}
\paragraph{Evaluation Methodology.} 
Our methodology contrasts with GoLLIE~\cite{cite_gollie}, which follows a pipeline-based structure and selectively includes only parent event types in its prompts, limiting granularity in event representation. For argument extraction, GoLLIE further restricts schema inclusion to sibling event types, introducing manual design choices that reduce automation and scalability. To ensure fair and comprehensive evaluation, we adopt a methodology that enumerates all possible event types for each test and development sample during prompt construction. Unlike setups where only the gold-standard event schema is included in the prompt, we avoid implicit event detection bias—if the correct event type were provided, the model would not need to identify the event type itself and could directly extract arguments, which would not reflect its real performance on real-world data. Due to these fundamental differences in methodology, we do not compare our results with GoLLIE.
% In contrast, our approach systematically incorporates both parent and child event hierarchies in an automated manner, ensuring a more generalizable and scalable evaluation framework. Due to these fundamental differences in methodology, direct comparisons between our results and those reported by GoLLIE are not meaningful.
% For evaluation, we enumerated all event types (ETs) present in the respective datasets for each instance in both the dev and test sets. This comprehensive approach ensured that the model was assessed across a wide range of event scenarios, providing a thorough evaluation of its ability to generalize across diverse event types. 

% This approach contrasts with Gollie’s methodology \cite{cite_gollie}, which follows a pipeline structure. For EE, Gollie includes only parent event types in the prompts, extracting triggers relative to these broader categories without explicitly addressing the child event types associated with the triggers. In comparison, our methodology incorporates both parent and child event hierarchies, enabling a more granular and precise representation of event types during training and evaluation. In the EAE task, Gollie focuses solely on sibling event types for argument extraction and does not enumerate all event types in the test data, limiting the breadth of evaluation. Conversely, our evaluation enumerates all event types present in each dataset instance for both development and test sets, ensuring comprehensive event coverage. These methodological differences, particularly in event type representation and evaluation scope, mean that our results are not directly comparable to those of Gollie.

% To support reproducibility and encourage further research in event extraction, our processed dataset will be publicly released. \sweta{Unsure if we can release the ACE dataset publicly}

\paragraph{Evaluation Metrics.} 
Following prior work~\cite{lin2020, TextEE}, we evaluate models using Trigger F1 and Argument F1, focusing on both events and argument extraction. Specifically, we report:  \textbf{(1) Trigger Identification (TI)}, which measures correct trigger span extraction, \textbf{(2) Trigger Classification (TC)}, which additionally requires event-type correctness, \textbf{(3) Argument Identification (AI)}, which ensures correct argument-role association with the predicted trigger, and \textbf{(4) Argument Classification (AC)}, which further requires role-type correctness. Since our approach represents events in a structured code format, we develop a custom evaluation script that seamlessly aligns with our model’s structured outputs while maintaining compatibility with standard EE benchmarks.
% Following \citet{lin2020}, most prior work adopts Trigger F1 and Argument F1 as core evaluation metrics. Specifically, the F1-scores are computed based on four criteria: (1) \textbf{TI}: A predicted trigger is correct if its \texttt{(start\_idx, end\_idx)} matches the gold offsets. (2) \textbf{TC}: A predicted trigger is correct if its \texttt{(start\_idx, end\_idx, event\_type)} matches the gold offsets and event type. (3) \textbf{AI}: A predicted argument is correct if its \texttt{(start\_idx, end\_idx, event\_type)} matches the gold offsets and event type. (4) \textbf{AC}: A predicted argument is correct if its \texttt{(start\_idx, end\_idx, event\_type, role\_type)} matches the gold offsets, event type, and role type. However, \citet{TextEE} noted that AI and AC become problematic when multiple triggers of identical types co-occur and do not capture argument quality. To address this, they propose two additional metrics to incorporate trigger-argument attachment: (5) \textbf{AI+}: Checks if \texttt{(start\_idx, end\_idx, event\_type, attached\_trigger\_offsets)} match the gold references. (6) \textbf{AC+}: Checks if \texttt{(start\_idx, end\_idx, event\_type, attached\_trigger\_offsets, role\_type)} match the gold references. 

% Following \cite{TextEE}, we report TI, TC, AI+, and AC+ throughout our experiments. Moreover, since our approach represents events in a structured code format, we develop a custom evaluation script to compute these metrics while ensuring compatibility with our model’s outputs. This adaptation not only aligns with our methodology but also provides a reusable framework for future research in event extraction with code-based prompting.\sriv{will shorten it a bit}

\subsection{Prompt Design and Model Training}

\paragraph{Prompt Design.}
We adopt a structured prompt format consisting of four components: (i) task instruction,(ii) event schema, (iii) input text, and (iv) expected output, formatted as a structured event representation. Our approach follows a schema-first prompting strategy, where event definitions are explicitly encoded in a structured format to enhance model comprehension of event relations and argument constraints. For each input instance, a randomly sampled guideline definition is used to annotate the event schema, ensuring that the model is exposed to multiple rephrasings rather than memorizing and overfitting on a static definition. Formally, we prepare the input sequence as follows: ``\texttt{[BoS] \$-task\_instruction $(I)$ \$-annotated event schema $(\mathcal{E}_e)$~\$-input\_sample~$(X_i)$~[EoS]}'' where  the event schema $\mathcal{E}_e$ for an event $e$ is annotated with one of the generated guideline definitions.
\paragraph{Model.} 
We conducted experiments on an instruction-tuned LLaMA-3-8B model, selected for its demonstrated proficiency in processing structured code-based inputs and generating coherent outputs. For parameter-efficient adaptation, we implement RSLoRA \cite{RSLORA}, applying LoRA transformations to all linear layers in the transformer blocks following the methodology of \citet{xe}. Key hyperparameters—including LoRA rank (64), scaling factor $\alpha$ (128), and batch size (32)—were determined through preliminary experiments to balance computational efficiency with model performance. The models were trained for 10 epochs using a single NVIDIA A100 GPU (80GB VRAM), with early stopping triggered after three consecutive validation steps without improvement. We adopt a cosine learning rate scheduler with an initial rate of 1e-5 and a warmup period of 350 steps. Input sequences are padded to 3,000 tokens to maintain consistency while accommodating long-form code structures. To ensure reproducibility and minimize memory fragmentation, we implement deterministic padding and truncation strategies. Complete configuration details—including ablation studies of alternative hyperparameter choices and architectural variations—are documented in Appendix \sriv{X}.

% describe model and training details (GPU config, hyper-params, epochs, etc).
%%[Sweta: Covered all the pointers as the description]

% For our experiments, we use the \texttt{unsloth/llama-3-8b-Instruct-bnb-4bit} model from UnsLOTH, a quantized variant of LLaMA 3 designed for instruction-based tasks. We fine-tuned the model using the UnsLoTH framework using Low-Rank Adaptation (LoRA) techniques to ensure parameter-efficient learning, coupled with 4-bit quantization for reduced memory consumption.

% LoRA was applied to key projection modules within the model architecture: \texttt{q\_proj}, \texttt{k\_proj}, \texttt{v\_proj}, \texttt{o\_proj}, \texttt{gate\_proj}, \texttt{up\_proj}, and \texttt{down\_proj}. The fine-tuning configuration is summarized below:

% \begin{itemize}
%     \item \textbf{LoRA Rank (r):} 128
%     \item \textbf{Scaling Factor ($\alpha$):} 64
%     \item \textbf{Dropout Rate:} 0.0 (no dropout applied)
%     \item \textbf{Rank-Stabilized LoRA (RSLoRA):} Enabled to improve stability during training.
%     \item \textbf{Maximum Sequence Length:} 3000 tokens, to accommodate longer conversational contexts.
% \end{itemize}

% Model training was performed using the AdamW optimizer with 8-bit precision, allowing for efficient memory usage while maintaining model performance. Training and optimization hyperparameters were selected based on performance tuning:

% \begin{itemize}
%     \item \textbf{Learning Rate:} $1 \times 10^{-5}$
%     \item \textbf{Learning Rate Scheduler:} Cosine decay scheduler, selected for its smooth adjustment of learning rates over time.
%     \item \textbf{Weight Decay:} 0.01, to mitigate overfitting and promote generalization.
%     \item \textbf{Batch Sizes:} 32 for training, 50 for evaluation.
% \end{itemize}

% \paragraph{Hardware configuration}
% All model training were conducted on a single NVIDIA A100 GPU with 80 GB VRAM, 200 GB RAM, managed via SLURM on a high-performance computing cluster.

% \paragraph{Training and evaluation procedure:} 
% The model was trained for up to 10 epochs with evaluations performed at regular intervals. Early stopping based on F1-score was implemented to prevent overfitting. Custom chat templates were used to format conversational datasets, and tokenization was managed via the UnsLoTH tokenizer, with left-padding configured for consistent alignment in sequence generation tasks.

% The best performing model was selected based on validation metrics, particularly focusing on argument classification F1 scores.