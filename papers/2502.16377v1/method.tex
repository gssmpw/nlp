\section{Approach}
In this section, we formalize the event extraction task using LLMs (\textsection \ref{sec:formulation}) and detail the guideline generation procedure used to prompt LLMs (\textsection \ref{sec:guideline_generation}).
\zyc{Around 2 pages}

\subsection{Task Formulation}
\label{sec:formulation}
\zyc{describe the instruction tuning formulation for EE. describe instructions with annotation guidelines. referring to Fig~\ref{fig:intro}.}
Given an input sequence \( X \), our goal is to generate a structured event representation by extracting {event triggers} and {argument roles}, adhering to predefined schema constraints. Each prompt instance is defined as  $P = [I \oplus \mathcal{E}_e \oplus X]$ where \( I \) represents the {task instruction}, specifying the structured output format and task definition, and \( \mathcal{E}_e \in E\) denotes the {event schema} defining the event type \( e \) along with its corresponding argument roles. The extraction process consists of (i) \textbf{Trigger Extraction}, where the model localizes an event trigger span and classifies its event type, and (ii) \textbf{Argument Extraction}, where it identifies spans in \( X \) that serve as argument roles within the extracted event instance.  

Fine-tuning is performed via {causal language modeling}, leveraging a {next-token prediction objective} where the model generates structured event representations autoregressively. Let \( \mathcal{D} = \{(X_i, Y_i)\}_{i=1}^{N} \) denote a dataset of input-output pairs, where each \( X_i \) corresponds to a prompt instance \( P_i \), and \( Y_i = (t_i, a_i) \) represents the structured event annotations, consisting of trigger labels \( t_i \) and argument spans \( a_i \). Given a {pretrained model} parameterized by \( \theta \), we optimize the {conditional log-likelihood}:
\begin{equation}
    \mathcal{L}(\mathcal{D}; \theta) = - \sum_{i} \sum_{j} \log p_{\theta} (Y_{ij} \mid P_i, Y_{i,<j})
\end{equation}
where \( Y_{i,<j} \) represents previously generated tokens in the structured output sequence, ensuring an autoregressive formulation. During training, we partition the sequence into a {source context} (\( P_i \)) and a {target output} (\( Y_i \)), applying the loss function {only to target tokens} (\emph{label loss}). At inference time, given a test instance $X$,  the model generates output tokens autoregressively, until an end-of-sequence (EOS) token is generated or a predefined maximum length is reached.



% Instruction tuning for Event Extraction (EE) involves training a language model (LLM) using structured prompts that incorporate event definitions, argument roles, and annotation guidelines to enhance the model's understanding of event structures. This approach ensures that the model can generalize across diverse event types by learning from explicitly defined schemas.

% As shown in Fig~\ref{fig:intro}, the instruction format consists of the following components:

% \paragraph{Task Description:} A high-level explanation of the event extraction task, defining what constitutes a structured event, including event triggers and arguments.

% \paragraph{Schema Definition with Annotation Guidelines:} A dataclass-based representation of the event type, specifying the required arguments and their roles (e.g., BeBorn event with arguments like person, place, and time). The docstrings and comments within the schema explicitly define the event and argument roles, ensuring clear and interpretable guidance for the LLM.

% \paragraph{Input Text:} The sentence or passage from which the event and arguments needs to be extracted.

% \paragraph{Expected Output with Structured Event Representatio:} The extracted events are represented as a list of structured instances, where each event is an object of the corresponding event class. As shown in the figure, the output follows a Python-style list format, containing instances of the event class (e.g., BeBorn). Arguments are stored as lists, allowing multiple entities when applicable, while empty lists ([]) indicate missing arguments. This structured format ensures consistency and interpretability in event extraction.

% By integrating annotation guidelines as comments in the prompt, we provide explicit event and argument definitions, guiding the model toward more accurate and consistent event extractions. Storing the expected output in Python as structured event instances allows for efficient evaluation, as it enables direct comparison between predicted and gold-standard events using Python-based metrics such as F1 scores, precision, and recall. \sweta{Should we provide more details around why this representation proves to be good for easier evaluation? or is this fine?} This structured formulation also helps LLMs better differentiate event types, improving precision, generalization, and reproducibility in downstream EE tasks.

\subsection{Generating Annotation Guidelines with LLMs}
\label{sec:guideline_generation}
\zyc{describe the guideline variants. need one or more figures of prompts + examples.}

Recent work by \citet{GoLLIE} demonstrated the effectiveness of integrating annotation guidelines with code prompts across various information extraction (IE) tasks. However, since IE encompasses a broad range of sub-tasks, their analysis of guideline usability in EE remains underexplored. Furthermore, their approach assumes the availability of pre-existing human-curated guidelines, an assumption that may not always hold in real-world applications. To bridge this gap, we explore methods to automatically generate structured guidelines and assess their effectiveness in comparison to human-authored guidelines.

To develop a scalable and cost-effective approach for guideline generation, we employ a reverse engineering strategy, leveraging both annotated event examples and the strong generative capabilities of LLMs. As illustrated in Figure X, we construct guideline generation prompts by providing 10 annotated examples per event type $e$, using them to generate five distinct definitions for the event type and its corresponding argument roles. Formally, each guideline generation prompt consists of a task instruction, an input text instance, and its corresponding event annotations $(t_i, a_i)$. Additionally, we investigate refining guidelines through the inclusion of annotations from different event types $\hat{e}$  and semantically similar event types, incorporating negative constraints (e.g., ``what not to do'') to enhance model robustness against corner cases.

In our experiments, we prompt GPT-4 to elicit the guidelines \(E\) across five distinct settings, each designed to introduce different levels of specificity and contrast. \textbf{1) Guideline-P:} The model generates guidelines based on 10 positive event instances without additional constraints.
\textbf{2) Guideline-PN (Positive + Negative Examples):} Extends Guideline-P by incorporating 15 additional samples from different event types, introducing edge cases and exceptions.
\textbf{3) Guideline-PS (Positive + Sibling Events):} Augments Guideline-P by adding 15 samples from semantically similar event types (e.g., Arrest vs. Jail), facilitating a more nuanced differentiation between event types.
Finally, we create two new prompts by gathering 5 different distinct generations from Guideline-PN and Guideline-PS stages to generate \textbf{4) Guideline-PN-Adv}, and \textbf{(5) Guideline-PS-Adv}, respectively.

\subsection{Prompt Design For EE}
After generating annotation guidelines using the five strategies described earlier (Guideline-P, Guideline-PN, Guideline-PS, etc.), we evaluate their effectiveness by conducting separate EE experiments in each setting. Specifically, we integrate the generated guidelines into the prompting format and assess how they influence model performance.

Each guideline setting consists of five independently generated definitions. During training, we introduce diversity by randomly selecting one of the five definitions for each input instance. This approach ensures that the model is exposed to diverse rephrasings of event schemas rather than memorizing a single static guideline. To maintain consistency across conditions, the final event extraction prompt follows a standardized structure: ``\texttt{[BoS] \$-task\_instruction $(I)$ \$-annotated event schema $(\mathcal{E}_e)$~\$-input\_sample~$(X_i)$~[EoS]}'' where the event schema ($\mathcal{E}_e$) is annotated using one of the generated guideline definitions. This setup allows us to systematically compare the impact of different guideline generation strategies while ensuring robustness against overfitting and improving adaptability to diverse real-world event formulations.

% Finally, to establish a strong performance benchmark, we compare all machine-generated guidelines against a human-authored guideline baseline \textbf{Guideline-H}.  

% To enhance event extraction, we generate annotation guidelines using Large Language Models (LLMs), providing structured descriptions of event types and their argument roles. These guidelines serve as instructional prompts during training and inference, helping the model understand event schemas more effectively. 
% Explicitly defining each event type and its arguments, guidelines eliminate ambiguity, ensuring that models interpret event structures accurately. Additionally, descriptive guidelines expose the model to nuanced event distinctions, enabling better generalization across diverse datasets. Incorporating edge cases and domain knowledge further ensures robustness, helping the model recognize both common and rare event occurrences.


% \paragraph{Guideline Variants:}
% We explore variants of LLM-generated annotation guidelines across \textbf{five distinct settings}, each designed to introduce different levels of specificity and contrast:

% \begin{itemize}  
%     \item \textbf{Guideline-H (Human-annotated)}: Provided in the original dataset as expert-written annotation guidelines.  
%     \item \textbf{Guideline-P (Positive Examples Only)}: Generated using 10 positive examples per event type to create structured definitions across event and arguments.  
%     \item \textbf{Guideline-PN (Positive + Negative Examples)}: Incorporates 10 positive and 15 negative examples per event type to refine event differentiation.  
%     \item \textbf{Guideline-PS (Positive + Sibling Examples)}: Uses 10 positive examples and 15 sibling event examples, reinforcing hierarchical event relationships.  
% \end{itemize}  

% For Guideline-P, Guideline-PN, and Guideline-PS, the LLM generates five diverse guideline definitions per event and argument, capturing different ways of explaining event schemas. One of these definitions is randomly selected for each instance in the train, dev, and test sets, ensuring variation and improving the modelâ€™s ability to generalize across event formulations.  

% \begin{itemize}  
%     \item \textbf{Guideline-PN-Adv (Consolidated PN)}: The LLM consolidates the five diverse PN guidelines into a single refined definition per event type.  
%     \item \textbf{Guideline-PS-Adv (Consolidated PS)}: Similar to PN-Adv but using five diverse PS guidelines, ensuring broader coverage of sibling event distinctions in a unified definition.  
% \end{itemize}  

% \paragraph{Guideline Generation Process}
% To generate structured annotation guidelines, we prompt GPT-4 using examples for each event type and instruct it to produce event and argument definitions in a standardized JSON format. This ensures consistency and clarity in event schema representation.

% The generation process follows these steps:

% \begin{itemize}
%     \item \textbf{Providing Event Examples}: The LLM is given 10 positive examples per event type to illustrate valid triggers and argument roles. For Guideline-PN, an additional 15 negative examples contrast valid and invalid event types, while Guideline-PS includes 15 sibling event examples to reinforce hierarchical event distinctions.

    % \item \textbf{Structured Prompting for Definitions}: The model is explicitly instructed to generate five diverse definitions for both the event and its arguments. Each definition includes:
    % \begin{itemize}
    %     \item A concise event description with example triggers.
    %     \item A list of arguments with role descriptions and illustrative examples.
    %     \item Explicit comparisons between similar or negative event types, helping to distinguish fine-grained differences.
    % \end{itemize}

    % \item \textbf{Randomized Selection and Consolidation}: 
    % \begin{itemize}
%         \item For Guideline-P, Guideline-PN, and Guideline-PS, five alternative guideline definitions are generated for each event type and its arguments. One definition is randomly selected for each instance in the train, dev, and test sets, introducing variation to enhance generalization.
%         \item For Guideline-PN-Adv and Guideline-PS-Adv, the LLM consolidates all five generated definitions into a single refined guideline, capturing concise key distinctions.
%     \end{itemize}
% \end{itemize}

% This \textbf{diverse guideline generation and selection process} introduces controlled variability while maintaining structured supervision, leading to better generalization in event extraction. 
 







