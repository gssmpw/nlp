\section{Experiments}

% \zyc{Around 4 pages}

\input{latex/sections/experimental-setup-v3}
\input{latex/tables/e2e_2000}
\subsection{RQ1: Do the annotation guidelines allow an LLM to more precisely extract occurring events?}
% \zyc{Table 1: llama results on full training sets, two datasets, all guideline settings. Test on full test.}

% To answer this question, we compare instruction-tuning the LLaMA model with and without annotation guidelines in the instruction. In addition, \zyc{talk about NS here}
% \zyc{A problem here is we have all results (wo and w NS) in one table, and when you discuss the following, it is not clear which parts the reader should look at. 
% A common problem throughout this section: the discussion is not very specific; e.g., when you say "F1 point", which metric do you refer to? There are no discussions about trigger vs. argument extraction either.}
% \sweta{We have edited RQ1 based on the above pointers, one query - since we bring negative examples in the training set in the second paragraph, should we mention ACE w/0 NS or only ACE}
% \zyc{if you cannot direct people's attention to only wo NS, then introduce NS before discussing the results.}
\zyc{need revision: it is unclear what is "augmented with negative samples". How many samples did you use? Why did you compare with this setting (i.e., the intuition)?}

To assess the impact of incorporating annotation guidelines in code prompts during LLaMA instruction-tuning, we conducted experiments under two settings: 1) training on the original datasets (\textbf{ACE w/o NS}, \textbf{RichERE w/o NS} in Table~\ref{tab:full_train_table}) and 2) training on an augmented version where each instance was supplemented with 15 randomly selected negative samples (\textbf{ACE w/ NS}, \textbf{RichERE w/ NS}. Our decision to introduce negative samples stems from an analysis of annotation guidelines, which showed that event types like \textit{Convict, Sentence,} and \textit{Indict} share overlapping definitions, making them difficult to distinguish. 
% While guidelines enhance interpretability and alignment with human annotation, they may not fully resolve such fine-grained ambiguities.
A similar study was conducted by \citet{Degree}, but they did not systematically explore its impact.   
% To study the effects of incorporating annotation guidelines in code prompts during LLaMA instruction-tuning, we conducted experiments under two settings: 1) Training on the original dataset without augmentation (Table 3 \textbf{ACE} and \textbf{RichERE}), and 2) Training on the same dataset augmented with negative samples for each instance (Table 3 \textbf{ACE w/ NS} and \textbf{RichERE w/ NS}), following the approach in \citet{Degree}.  

In the \textbf{w/o NS} setting, annotation guidelines consistently improve performance across both datasets. For {ACE}, \textbf{Guideline-P} achieves the highest scores across all four metrics, with \textbf{Guideline-P} and \textbf{Guideline-PS} improving TC and AC by about 13 F1 points, respectively, over \textbf{NoGuideline}. Similarly, for {RichERE}, \textbf{Guideline-PN} outperforms \textbf{NoGuideline} by about 5 F1, for both TC and AC. Notably, while both datasets benefit from guidelines, the improvement is more pronounced in RichERE. 
% A qualitative analysis of the dev set suggests that RichERE has fine-grained event classes (such as the distinction between transporting a person or an artifact) benefiting the most from the guidelines. 
However, with negative sampling, the effects diverge between the two datasets. In {ACE w/ NS}, \textbf{NoGuideline} surprisingly achieves slightly better performance, while \textbf{Guideline-PN} and \textbf{Guideline-PS} remain competitive but do not surpass it. 

Qualitatively, we observed that ACE’s event types, though sometimes overlapping, are primarily distinguishable through surface lexical and syntactic cues—for instance, ``\textit{was sentenced to}'' vs. ``\textit{was convicted of}'' are clear textual markers, making negative samples an effective contrastive signal. In contrast, {RichERE w/ NS}, sees an even greater boost from guidelines, with \textbf{Guideline-P} and \textbf{Guideline-PN} outperforming \textbf{NoGuideline} by about 30 F1 points in TC and 18 F1 in AC. This indicates that RichERE's more fine-grained event definitions require guidelines to navigate subtle distinctions, such as differentiating between TransportPerson and TransportArtifact in ``\textit{Army full of soldiers and tanks rushed towards the enemy frontline.}''. Without explicit guidelines, the model may conflate the entity roles, as both \textit{soldiers} and \textit{tanks} are plausible transport objects in different contexts. 
%%%%####
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
% In ACE w/o NS, Guideline-P achieves the highest scores across all four metrics, with Guideline-PS and Guideline-P improving AC by 5.15 and 5.47 F1 points, respectively, over NoGuideline. This confirms that structured annotation guidelines enhance event differentiation and extraction accuracy. In ACE w/ NS, while NoGuideline achieves the highest AC score, Guideline-PN and Guideline-PS remain competitive, indicating that structured annotation guidelines still provide valuable schema-driven supervision. However, despite the added complexity introduced by negative examples, guidelines continue to offer comparable performance, reinforcing their role in improving generalization and structured event extraction.

% For RichERE w/o NS, Guideline-PN outperforms NoGuideline across all triggers and argument-related metrics, demonstrating the advantage of incorporating structured guidelines in event identification. With negative sampling, the benefits of guidelines become even more pronounced. In RichERE w/ NS, Guideline-PS improves TC by 34.18 F1 points and AC by 24.7 F1 points, while Guideline-PN enhances TC by 33.08 F1 points and AC by 25.54 F1 points, compared to NoGuideline. These results confirm that guideline-driven instruction tuning, particularly when combined with contrastive learning from negative examples, leads to stronger event and argument classification performance.\
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
%%%%####
\zyc{What lead to the different effects in the w/ NS case for the two datasets?}



%Prior WriteUp
%The results in Table~\ref{tab:full_train_table} demonstrate that structured annotation guidelines significantly enhance the LLM’s ability to differentiate event types, leading to notable improvements across both datasets. This highlights the importance of schema definitions in improving event classification performance. In the ACE dataset, Guideline-P achieves the highest scores across all four metrics, with Guideline-PS and Guideline-P improving Argument Classification (AC) by 5.15 and 5.47 F1 points, respectively, over NoGuideline. In RichERE, Guideline-PN yields the highest trigger-related scores across all four metrics, demonstrating its effectiveness in both event and argument differentiation compared to NoGuideline.

%Building on the observed improvements from guideline variants, we hypothesized that incorporating negative examples alongside structured guidelines would further enhance event differentiation by providing explicit contrasts between correct and incorrect event types. To test this, we introduced negative examples per instance during training to refine the model’s ability to classify events and arguments more accurately. Upon overall evaluation, Guideline-PN and Guideline-PS demonstrate comparable or improved performance in the w/ Negative Samples scenario. In RichERE w/ NS, Guideline-PS improves Trigger Classification (TC) by 34.18 F1 points and Argument Classification (AC) by 24.7 F1 points, while Guideline-PN enhances TC by 33.08 F1 points and AC by 25.54 F1 points, compared to NoGuideline. While NoGuideline achieves the highest AC score in ACE w/ NS, guideline-based settings remain competitive, confirming that structured definitions and exposure to contrasting examples strengthen event classification, particularly in datasets with complex event structures.


\input{latex/tables/e2e_100}
\input{latex/tables/cross_dataset_transfer}
\subsection{RQ2: Are machine-generated guidelines more effective than human-written ones?}
% \zyc{refer to Table 1 as well.}
High-quality annotation guidelines are crucial for event extraction, but human-written ones often are not available (e.g., RichERE) and are costly. In light of this, we compare our machine-generated guidelines with those originally written by ACE's creators\footnote{link to guideline}, \textbf{Guideline-H},  quantitatively (Table~\ref{tab:full_train_table}) and qualitatively (Table~\ref{tab:guideline-examples}).

\textbf{Guideline-P, -PN} {and}, \textbf{-PS} consistently outperform \textbf{Guideline-H} for all four metrics while \textbf{Guideline-PN-Adv} and \textbf{-PS-Adv} perform comparably. For ACE w/o NS, the minimum gain is about 6 F1 points for both TI and TC, increasing to 16 and 10 F1 points for TC and AC in ACE w/ NS. Qualitatively, \textbf{Guideline-H}, while compact, lack explicit contrasts between event types. For instance, ACE defines \textbf{Transport} as ``\textit{\highlight{F0F0F0}{whenever an ARTIFACT or a PERSON is moved from one PLACE to another.}}'' and  \textbf{Extradite} as ``\textit{\highlight{F0F0F0}{whenever a PERSON is sent by a state actor from one PLACE to another place for the purposes of legal proceedings there.}}'' Since both involve movement, the distinction is ambiguous. In contrast, \textbf{Guideline-PN} explicitly differentiates them: ``\textit{\highlight{F0F0F0}{Triggers such as `extradition' are indicative of this event type, }\highlight{FACEC6}{not `Transport' which involves general movement without legal context}''}\footnote{We provide the full examples in Appendix X}.      

Notably, \textbf{Guideline-PN-Adv} and \textbf{-PS-Adv} fail to achieve similar gains. We hypothesize that while diverse and contrastive definitions help LLMs generalize better, the \textbf{-Adv} variants—generated by consolidating five machine-generated guidelines into one—failed to maintain contrast between event types. By sampling from multiple definitions during training, \textbf{Guideline-PN} and \textbf{-PS} expose the model to more varied perspectives, enhancing generalization. Future work should ensure contrast is preserved in consolidated guidelines.    
%%%%####
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
% The results in Table~\ref{tab:full_train_table} indicate that machine-generated guidelines consistently outperform human-written ones (Guideline-H), demonstrating their effectiveness in instruction-tuning and event-argument classification. In ACE, Guideline-P improves TC by 11.89 F1 points and AC by 6.56 F1 points over human-written guidelines, highlighting the benefits of structured event definitions. This gap widens further in ACE w/ NS, where Guideline-PN surpasses human-written guidelines by 24.47 TC F1 points and 17.21 AC F1 points, reinforcing the impact of machine-generated guidelines in enhancing event and argument classification.

% Table~\ref{tab:guideline-examples} further illustrates the qualitative differences between human-written and machine-generated guidelines. Human-written guidelines (Guideline-H) provide concise event definitions, they often lack explicit contrasts between event types. In contrast, machine-generated guidelines, particularly Guideline-PN and Guideline-PS, incorporate diverse definitions that explicitly contrast with other event types. For instance, in the BeBorn event, Guideline-PN highlights its distinction from the ‘Die’ and ‘Marry’ events, while Guideline-PS contrasts the argument structures of ‘victim’ in ‘Die’ and ‘person’ in ‘BeBorn.’ These nuanced explanations provide the model with a more structured understanding of event semantics, improving generalization and robustness in event extraction. Unlike static human annotations, LLM-generated guidelines introduce variability, offering multiple perspectives and refined schema definitions, enhancing the model’s adaptability. These findings suggest that generated guidelines provide a more scalable and effective alternative for instruction-tuning LLMs in event extraction.
%%%%####
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
\zyc{missing discussion about Adv}
%%%%####
\subsection{RQ3: Are the annotation guidelines helpful when there is only a small amount of training data?}
% \zyc{Table 2: llama results on 2000 samples, two datasets, all guideline settings. Test on full test.}
Table~\ref{tab:train2000_table} and Table~\ref{tab:train_100_results} confirm that annotation guidelines remain beneficial with small training data, though their impact varies with dataset complexity and size. This is particularly important as high-quality annotated data is often unavailable, and understanding how much data is necessary for effective training. With 2000 samples, guidelines significantly improve ACE and RichERE performance, with \textbf{Guideline-PS} and \textbf{Guideline-PN} outperforming \textbf{NoGuideline} by up to $30$ TC and $17$ AC F1 points. The trend holds in ACE w/ NS. However, in 100-sample settings, the effects become dataset-dependent: in ACE, \textbf{NoGuideline} slightly outperforms guidelines, suggesting that the model relies more on memorizing training examples than learning from schema constraints. In contrast, RichERE, with its more diverse event structures, continues to benefit from guidelines, with \textbf{Guideline-PN} surpassing \textbf{NoGuideline} by $2$ AC F1 points. These results suggest that while guidelines improve generalization even in data-scarce scenarios, their impact is stronger in datasets with fine-grained event distinctions, where direct memorization is insufficient.

%%%%####
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
% The results in Table~\ref{tab:train2000_table} and Table~\ref{tab:train_100_results} demonstrate that annotation guidelines remain beneficial even in low-resource settings, though their impact varies based on dataset complexity and training size.

% With 2000 samples, Guideline-PS improves ACE performance over NoGuideline across all four metrics, achieving 29.93 TC and 22.44 AC F1 points, demonstrating the effectiveness of structured definitions. A similar trend is observed in ACE w/ NS, where Guideline-P surpasses NoGuideline by 16.29 AC F1 points. In RichERE w/o NS, structured annotation guidelines continue to be effective in complex event structures, with Guideline-P outperforming NoGuideline by 20.41 TC and 6.64 AC F1 points. For RichERE w/ NS, machine-generated guidelines (Guideline-PN) further improve performance, surpassing NoGuideline by 30.87 TC and 17.81 AC F1 points, reinforcing that contrastive examples enhance event differentiation, even with limited data.

% In 100-sample settings, machine-generated guidelines perform comparably to NoGuideline in ACE w/ NS. NoGuideline exceeds Guideline PS by 2.14 AC F1 points, suggesting that with extremely limited data, the model relies more on directly learning from raw training samples rather than the annotated schema definitions. However, in RichERE, Guideline-PN and Guideline-PN-Adv outperform NoGuideline by approximately 2.13 and 1.32 AC F1 points respectively, indicating that structured definitions remain beneficial, particularly in datasets with diverse event types. Across settings, Guideline-PN and Guideline-PS consistently demonstrate strong performance, reaffirming that structured guidelines enhance event extraction even in low-resource scenarios.
%%%%####
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
\subsection{RQ4: Does the advantage of annotation guidelines generalize to unseen datasets?}
% \zyc{model generalizability? not sure yet. if we have time, pick 1 model and limited settings (based on what we want to highlight..but I'm still waiting for the results)}
Table~\ref{tab:RQ4_f1_scores} shows that structured annotation guidelines enhance cross-dataset generalization, though the effect varies with schema complexity. In \textbf{RichERE → ACE}, performance remains close to the in-distribution baseline, especially in TC, where Guideline-PN achieves about 40 F1, nearly matching the in-distribution score. This suggests that fine-to-coarse schema migration is feasible, as RichERE’s finer event definitions transfer well to ACE’s simpler schema. However, AC drops by about 10 points, likely due to RichERE’s expanded argument roles, which do not always map cleanly onto ACE. Contrastive learning mitigates some of this gap, as seen in \textbf{Guideline-PS (w/ NS)}, which reaches about 64 TC and 33 AC F1. In contrast, \textbf{ACE → RichERE} generalizes even better, with \textbf{Guideline-PN (w/ NS)} surpassing the in-distribution model by over 22 TC and 12 AC F1 points, indicating that training on ACE, with its well-defined event boundaries, enables better adaptation to RichERE’s richer schema. Since RichERE introduces additional arguments for some events, structured guidelines provide essential context to prevent role confusion. We present error analysis and successful transfer examples in Appendix~\ref{X}.
%%%%####
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
% Table~\ref{tab:RQ4_f1_scores} demonstrates that annotation guidelines improve cross-dataset generalization, with machine-generated guidelines consistently outperforming NoGuideline settings. 

% %TO Write this properly.
% Comparing cross-dataset results to the Baseline (models are trained and tested on the same dataset), performance remains comparable across both datasets, demonstrating strong generalization of structured annotation guidelines. In RichERE → ACE and RichERE → ACE (w/ NS), generalization remains strong but slightly lower than the Baseline. For instance, in RichERE → ACE, Guideline-PN achieves 40.12 TC F1 (0.55 points lower than the Baseline) and 19.77 AC F1 (9.96 points lower). However, certain settings show comparable performance such as Guideline-PS in RichERE → ACE (w/ NS) attains 64.23 TC F1 and 32.84 AC F1, suggesting that structured guidelines help retain event extraction capabilities even across datasets. In contrast, ACE → RichERE and ACE → RichERE (w/ NS) exhibit better generalization, with some settings even outperforming the Baseline. For instance, Guideline-PN in ACE → RichERE (w/ NS) achieves 64.87 TC F1 and 44.51 AC F1, surpassing the Baseline by 22.6 and 12.95 points, respectively. This indicates that training on ACE and transferring to RichERE benefits from structured annotation guidelines, particularly when paired with negative sampling for each event instance. These findings reinforce that structured annotation guidelines enhance transferability, especially with negative samples, leading to stronger generalization in cross-dataset settings.
%%%%####
% OLD WRITING BY SWETA KEEPING HERE FOR REFERENCE AND COMPARISON
