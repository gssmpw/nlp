\section{Related Work}
\label{sec:related-work}
\paragraph{LLMs for IE and EE} With the growing capabilities of LLMs, recent efforts have explored their potential in IE~\cite{xu2024large} and studied EE as an auxiliary task. Existing LLM-based IE methods generally fall into two categories: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT). ICL-based approaches~\cite{li-etal-2023-codeie, guo2023retrievalaugmentedcodegenerationuniversal, ashok2023promptnerpromptingnamedentity, wang2023code4struct} rely on providing a few-shot context within prompts, enabling LLMs to infer structured information without explicit parameter updates. 
While being data-efficient, they were found to misinterpret the task specifications~\cite{gao2024eventrlenhancingeventextraction} and suffer from brittle sensitivity to prompt phrasing and example ordering \citep{gao2023exploringfeasibilitychatgptevent}. In addition, they also incur prohibitive costs due to the lengthy reasoning chains especially for complex tasks.
In contrast, SFT-based methods~\cite{lu-etal-2023-pivoine, wang2023instructuiemultitaskinstructiontuning, gui2024instructiebilingualinstructionbasedinformation, zhou2024universalnertargeteddistillationlarge} fine-tune LLMs on annotated datasets, which can significantly improve their EE performance. Our work deepens this line of research and particularly explores the inclusion of annotation guidelines in instructions. While there have been existing works on similar topics, they did not focus on EE \cite{sainz2024gollie} or instruction tuning~\cite{pang-etal-2023-guideline}.
% significantly improving performance but often failing to generalize beyond the provided schemas.
% While both lines of approaches leverage natural language instructions, they lack explicit mechanisms to enforce schema constraints, requiring heavy prompt engineering~\cite{gao2024eventrlenhancingeventextraction}.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{et_analysis_ace.pdf}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{et_analysis_richERE.pdf}
    \end{subfigure}
    % \begin{subfigure}{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{latex/figures/et_analysis_ace.pdf}
    % \end{subfigure}
    \caption{Impact of guidelines on AC scores per ET, sorted by frequency in the full training set. Smaller index indicate a higher frequency. Green/red bars indicate improvements/declines. Dashed/solid lines denote average AC scores without/with guidelines.
    % Impact of annotation guidelines on EE performance across event types in ACE and RichERE. Green bars indicate improvements, while red highlight declines. Dashed and solid lines denote the AC F1 score without and with guidelines, respectively.
    }
    \label{fig:performance_by_type_frequency}
    \vspace{-12pt}
\end{figure}
% \vspace{-10pt}
\paragraph{Code Prompts for EE} 
% To mitigate these challenges, 
While EE tasks are typically represented in texts, code-based prompting has emerged as a promising alternative, leveraging structured representations to enhance schema adherence. Early works have applied code-style prompts to event argument extraction~\cite{wang2023code4struct} and other IE tasks~\cite{li-etal-2023-codeie}, demonstrating potential but often underperforming compared to SFT-based models due to the absence of fine-tuning. EventRL~\cite{gao2024eventrlenhancingeventextraction} utilizes outcome supervision with specific reward functions to reduce information mismatch and hallucination. KnowCoder~\cite{sainz2024gollie, li-etal-2024-knowcoder} addresses this limitation by introducing a comprehensive schema representation in code format, integrating taxonomies, constraints, and structured definitions. 
% However, they still rely on human-written instructions and guidelines, majorly focusing on IE.
Complementary to these works, we study generating annotation guidelines to enhance the instruction tuning of LLMs for code-formatted EE and demonstrate their effectiveness.
% \paragraph{Annotation Guidelines for EE.} Guideline Learning \cite{pang-etal-2023-guideline} improves ICL for IE by automatically synthesizing and retrieving task-specific guidelines, addressing underspecified task descriptions, and enhancing model alignment with human comprehension. Among the most relevant studies, GoLLIE~\cite{sainz2024gollie} applies instruction tuning with code-style prompts by encoding annotated schema definitions as structured comments. They improve schema understanding and alignment with EE tasks by leveraging syntactic constraints within the prompt design. However, GoLLIE relies heavily on human-annotated guidelines, which are time-consuming and expensive to obtain. Moreover, its primary focus remains on IE, where it effectively encodes schema definitions for a broad set of IE tasks. Moreover, our work explores alternate strategies to generate the guidelines utilizing positive and negative samples.