% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{chen2023program,
title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=YfZ4ZPt8zd},
note={}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{li2023evaluating,
  title={Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness},
  author={Li, Bo and Fang, Gexiang and Yang, Yang and Wang, Quansen and Ye, Wei and Zhao, Wen and Zhang, Shikun},
  journal={arXiv preprint arXiv:2304.11633},
  year={2023}
}


@inproceedings{ji-grishman-2008-refining,
    title = "Refining Event Extraction through Cross-Document Inference",
    author = "Ji, Heng  and
      Grishman, Ralph",
    editor = "Moore, Johanna D.  and
      Teufel, Simone  and
      Allan, James  and
      Furui, Sadaoki",
    booktitle = "Proceedings of ACL-08: HLT",
    month = jun,
    year = "2008",
    address = "Columbus, Ohio",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P08-1030/",
    pages = "254--262"
}

@article{li2022survey,
  title={A survey on deep learning event extraction: Approaches and applications},
  author={Li, Qian and Li, Jianxin and Sheng, Jiawei and Cui, Shiyao and Wu, Jia and Hei, Yiming and Peng, Hao and Guo, Shu and Wang, Lihong and Beheshti, Amin and others},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@inproceedings{sainz2024gollie,
title={Go{LLIE}: Annotation Guidelines improve Zero-Shot Information-Extraction},
author={Oscar Sainz and Iker Garc{\'\i}a-Ferrero and Rodrigo Agerri and Oier Lopez de Lacalle and German Rigau and Eneko Agirre},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Y3wpuxd7u9}
}

@inproceedings{wang2023code4struct,
  title={Code4Struct: Code Generation for Few-Shot Event Structure Prediction},
  author={Wang, Xingyao and Li, Sha and Ji, Heng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3640--3663},
  year={2023}
}

@inproceedings{huang2024textee,
  title={TextEE: Benchmark, reevaluation, reflections, and future challenges in event extraction},
  author={Huang, Kuan-Hao and Hsu, I-Hung and Parekh, Tanmay and Xie, Zhiyu and Zhang, Zixuan and Natarajan, Prem and Chang, Kai-Wei and Peng, Nanyun and Ji, Heng},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={12804--12825},
  year={2024}
}

@inproceedings{hsu-etal-2022-degree,
    title = "{DEGREE}: A Data-Efficient Generation-Based Event Extraction Model",
    author = "Hsu, I-Hung  and
      Huang, Kuan-Hao  and
      Boschee, Elizabeth  and
      Miller, Scott  and
      Natarajan, Prem  and
      Chang, Kai-Wei  and
      Peng, Nanyun",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.138/",
    doi = "10.18653/v1/2022.naacl-main.138",
    pages = "1890--1908",
    abstract = "Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DEGREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. DEGREE has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for DEGREE to leverage DEGREE and thus better capture the event arguments. Moreover, DEGREE is capable of using additional weakly-supervised information, such as the description of events encoded in the prompts. Finally, DEGREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction."
}

@misc{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}



@article{xu2024large,
  title={Large language models for generative information extraction: A survey},
  author={Xu, Derong and Chen, Wei and Peng, Wenjun and Zhang, Chao and Xu, Tong and Zhao, Xiangyu and Wu, Xian and Zheng, Yefeng and Wang, Yang and Chen, Enhong},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186357},
  year={2024},
  publisher={Springer}
}


@inproceedings{li-etal-2023-codeie,
    title = "{C}ode{IE}: Large Code Generation Models are Better Few-Shot Information Extractors",
    author = "Li, Peng  and
      Sun, Tianxiang  and
      Tang, Qiong  and
      Yan, Hang  and
      Wu, Yuanbin  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.855/",
    doi = "10.18653/v1/2023.acl-long.855",
    pages = "15339--15353",
    abstract = "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks."
}

@misc{guo2023retrievalaugmentedcodegenerationuniversal,
      title={Retrieval-Augmented Code Generation for Universal Information Extraction}, 
      author={Yucan Guo and Zixuan Li and Xiaolong Jin and Yantao Liu and Yutao Zeng and Wenxuan Liu and Xiang Li and Pan Yang and Long Bai and Jiafeng Guo and Xueqi Cheng},
      year={2023},
      eprint={2311.02962},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.02962}, 
}


@misc{ashok2023promptnerpromptingnamedentity,
      title={PromptNER: Prompting For Named Entity Recognition}, 
      author={Dhananjay Ashok and Zachary C. Lipton},
      year={2023},
      eprint={2305.15444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.15444}, 
}

@inproceedings{wang-etal-2023-code4struct,
    title = "{C}ode4{S}truct: Code Generation for Few-Shot Event Structure Prediction",
    author = "Wang, Xingyao  and
      Li, Sha  and
      Ji, Heng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.202/",
    doi = "10.18653/v1/2023.acl-long.202",
    pages = "3640--3663",
    abstract = "Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints. We show that, with sufficient in-context examples, formulating EAE as a code generation problem is advantageous over using variants of text-based prompts. Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art (SOTA) trained on 20-shot data by 29.5{\%} absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12{\%} absolute F1."
}


@misc{gui2024instructiebilingualinstructionbasedinformation,
      title={InstructIE: A Bilingual Instruction-based Information Extraction Dataset}, 
      author={Honghao Gui and Shuofei Qiao and Jintian Zhang and Hongbin Ye and Mengshu Sun and Lei Liang and Jeff Z. Pan and Huajun Chen and Ningyu Zhang},
      year={2024},
      eprint={2305.11527},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.11527}, 
}


@misc{wang2023instructuiemultitaskinstructiontuning,
      title={InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction}, 
      author={Xiao Wang and Weikang Zhou and Can Zu and Han Xia and Tianze Chen and Yuansen Zhang and Rui Zheng and Junjie Ye and Qi Zhang and Tao Gui and Jihua Kang and Jingsheng Yang and Siyuan Li and Chunsai Du},
      year={2023},
      eprint={2304.08085},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08085}, 
}

@misc{zhou2024universalnertargeteddistillationlarge,
      title={UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition}, 
      author={Wenxuan Zhou and Sheng Zhang and Yu Gu and Muhao Chen and Hoifung Poon},
      year={2024},
      eprint={2308.03279},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03279}, 
}



@inproceedings{li-etal-2024-knowcoder,
    title = "{K}now{C}oder: Coding Structured Knowledge into {LLM}s for Universal Information Extraction",
    author = "Li, Zixuan  and
      Zeng, Yutao  and
      Zuo, Yuxin  and
      Ren, Weicheng  and
      Liu, Wenxuan  and
      Su, Miao  and
      Guo, Yucan  and
      Liu, Yantao  and
      Lixiang, Lixiang  and
      Hu, Zhilei  and
      Bai, Long  and
      Li, Wei  and
      Liu, Yidan  and
      Yang, Pan  and
      Jin, Xiaolong  and
      Guo, Jiafeng  and
      Cheng, Xueqi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.475/",
    doi = "10.18653/v1/2024.acl-long.475",
    pages = "8758--8779",
    abstract = ""
}


@inproceedings{pang-etal-2023-guideline,
    title = "Guideline Learning for In-Context Information Extraction",
    author = "Pang, Chaoxu  and
      Cao, Yixuan  and
      Ding, Qiang  and
      Luo, Ping",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.950/",
    doi = "10.18653/v1/2023.emnlp-main.950",
    pages = "15372--15389",
    abstract = "Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE."
}



@inproceedings{lu-etal-2023-pivoine,
    title = "{PIVOINE}: Instruction Tuning for Open-world Entity Profiling",
    author = "Lu, Keming  and
      Pan, Xiaoman  and
      Song, Kaiqiang  and
      Zhang, Hongming  and
      Yu, Dong  and
      Chen, Jianshu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.1009/",
    doi = "10.18653/v1/2023.findings-emnlp.1009",
    pages = "15108--15127",
    abstract = "This work considers the problem of Open-world Entity Profiling, a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE is considered a more general situation where entities and relations could be beyond a predefined ontology. We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction-tuning dataset for Open-world Entity Profiling enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world Entity Profiling with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional methods and ChatGPT-based baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge of entity profiling."
}


@misc{gao2024eventrlenhancingeventextraction,
      title={EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models}, 
      author={Jun Gao and Huan Zhao and Wei Wang and Changlong Yu and Ruifeng Xu},
      year={2024},
      eprint={2402.11430},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11430}, 
}

@inproceedings{doddington-etal-2004-automatic,
    title = "The Automatic Content Extraction ({ACE}) Program {--} Tasks, Data, and Evaluation",
    author = "Doddington, George  and
      Mitchell, Alexis  and
      Przybocki, Mark  and
      Ramshaw, Lance  and
      Strassel, Stephanie  and
      Weischedel, Ralph",
    editor = "Lino, Maria Teresa  and
      Xavier, Maria Francisca  and
      Ferreira, F{\'a}tima  and
      Costa, Rute  and
      Silva, Raquel",
    booktitle = "Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}`04)",
    month = may,
    year = "2004",
    address = "Lisbon, Portugal",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L04-1011/"
}

@inproceedings{song-etal-2015-light,
    title = "From Light to Rich {ERE}: Annotation of Entities, Relations, and Events",
    author = "Song, Zhiyi  and
      Bies, Ann  and
      Strassel, Stephanie  and
      Riese, Tom  and
      Mott, Justin  and
      Ellis, Joe  and
      Wright, Jonathan  and
      Kulick, Seth  and
      Ryant, Neville  and
      Ma, Xiaoyi",
    editor = "Hovy, Eduard  and
      Mitamura, Teruko  and
      Palmer, Martha",
    booktitle = "Proceedings of the 3rd Workshop on {EVENTS}: Definition, Detection, Coreference, and Representation",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-0812/",
    doi = "10.3115/v1/W15-0812",
    pages = "89--98"
}



@inproceedings{
shi2023dont,
title={Don{\textquoteright}t Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner},
author={Zhengxiang Shi and Aldo Lipani},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=s7xWeJQACI}
}


@misc{gao2023exploringfeasibilitychatgptevent,
      title={Exploring the Feasibility of ChatGPT for Event Extraction}, 
      author={Jun Gao and Huan Zhao and Changlong Yu and Ruifeng Xu},
      year={2023},
      eprint={2303.03836},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.03836}, 
}


@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@inproceedings{jiao-etal-2023-instruct,
    title = "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
    author = "Jiao, Yizhu  and
      Zhong, Ming  and
      Li, Sha  and
      Zhao, Ruining  and
      Ouyang, Siru  and
      Ji, Heng  and
      Han, Jiawei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.620/",
    doi = "10.18653/v1/2023.emnlp-main.620",
    pages = "10030--10051",
    abstract = "Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction {--} a classic task in natural language processing {--} most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size."
}


@inproceedings{srivastava-etal-2023-mailex,
    title = "{M}ail{E}x: Email Event and Argument Extraction",
    author = "Srivastava, Saurabh  and
      Singh, Gaurav  and
      Matsumoto, Shou  and
      Raz, Ali  and
      Costa, Paulo  and
      Poore, Joshua  and
      Yao, Ziyu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.801/",
    doi = "10.18653/v1/2023.emnlp-main.801",
    pages = "12964--12987",
    abstract = "In this work, we present the first dataset, MailEx, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes 1.5K email threads and {\textasciitilde}4K emails, which are annotated with a total of {\textasciitilde}8K event instances. To understand the task challenges, we conducted a series of experiments comparing three types of approaches, i.e., fine-tuned sequence labeling, fine-tuned generative extraction, and few-shot in-context learning. Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more future investigations in this domain-specific event extraction task."
}


@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}



@misc{kalajdzievski2023rankstabilizationscalingfactor,
      title={A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA}, 
      author={Damjan Kalajdzievski},
      year={2023},
      eprint={2312.03732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03732}, 
}


@inproceedings{cai-etal-2024-improving-event,
    title = "Improving Event Definition Following For Zero-Shot Event Detection",
    author = "Cai, Zefan  and
      Kung, Po-Nien  and
      Suvarna, Ashima  and
      Ma, Mingyu  and
      Bansal, Hritik  and
      Chang, Baobao  and
      Brantingham, P. Jeffrey  and
      Wang, Wei  and
      Peng, Nanyun",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.157/",
    doi = "10.18653/v1/2024.acl-long.157",
    pages = "2842--2863",
    abstract = "Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations.In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type.Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection."
}


@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{li-etal-2021-document,
    title = "Document-Level Event Argument Extraction by Conditional Generation",
    author = "Li, Sha  and
      Ji, Heng  and
      Han, Jiawei",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.69/",
    doi = "10.18653/v1/2021.naacl-main.69",
    pages = "894--908",
    abstract = "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6{\%} F1 and 5.7{\%} F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3{\%} F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97{\%} of fully supervised model`s trigger extraction performance and 82{\%} of the argument extraction performance given only access to 10 out of the 33 types on ACE."
}