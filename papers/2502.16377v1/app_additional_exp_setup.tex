\section{Evaluation Methodology and Metrics}
\label{sec:app_additional_details}
\paragraph{Evaluation Methodology.} 
Our methodology contrasts with GoLLIE~\cite{sainz2024gollie}, which follows a pipeline-based structure and selectively includes only parent event types in its prompts, limiting granularity in event representation. For argument extraction, GoLLIE further restricts schema inclusion to sibling event types, introducing manual design choices that reduce automation and scalability. To ensure fair and comprehensive evaluation, we adopt a methodology that enumerates all possible event types for each test and development sample during prompt construction. Unlike setups where only the gold-standard event schema is included in the prompt, we avoid implicit event detection bias—if the correct event type were provided, the model would not need to identify the event type itself and could directly extract arguments, which would not reflect its real performance on real-world data. Due to these fundamental differences in methodology, we do not compare our results with GoLLIE.


\section{Prompt Design and Model Training}
\paragraph{Model.} 
We conducted experiments on an instruction-tuned LLaMA-3-8B model, selected for its demonstrated proficiency in processing structured code-based inputs and generating coherent outputs. For parameter-efficient adaptation, we implement RSLoRA \cite{kalajdzievski2023rankstabilizationscalingfactor}, applying LoRA transformations to all linear layers in the transformer blocks following the methodology of \citet{dettmers2024qlora}. Key hyperparameters—including LoRA rank (64), scaling factor $\alpha$ (128), and batch size (32)—were determined through preliminary experiments to balance computational efficiency with model performance. The models were trained for 10 epochs using a single NVIDIA A100 GPU (80GB VRAM), with early stopping triggered after three consecutive validation steps without improvement. We adopt a cosine learning rate scheduler with an initial rate of 1e-5 and a warmup period of 350 steps. Input sequences are padded to 3,000 tokens to maintain consistency while accommodating long-form code structures. To ensure reproducibility and minimize memory fragmentation, we implement deterministic padding and truncation strategies.

\paragraph{Prompt Design.}
\label{sec:prompt-design}
We adopt a structured prompt format consisting of four components: (i) task instruction,(ii) event schema, (iii) input text, and (iv) expected output, formatted as a structured event representation. Our approach follows a schema-first prompting strategy, where event definitions are explicitly encoded in a structured format to enhance model comprehension of event relations and argument constraints. For each input instance, a randomly sampled guideline definition is used to annotate the event schema, ensuring that the model is exposed to multiple rephrasings rather than memorizing and overfitting on a static definition. Formally, we prepare the input sequence as follows: ``\texttt{[BoS] \$-task\_instruction $(I)$ \$-annotated event schema $({E}_e)$~\$-input\_sample~$(X_i)$~[EoS]}'' where  the event schema ${E}_e$ for an event $e$ is annotated with one of the generated guideline definitions. 

% \begin{table*}[ht]
%     \centering
%     \begin{minipage}{\textwidth} % Ensure it spans the full page width
%         \lstinputlisting[style=customjson, 
%             caption={Prompt example for generating Guideline-P, Guideline-PN, and Guideline-PS.}, 
%             label={lst:guidelines}, 
%             aboveskip=10pt, 
%             belowskip=10pt]
%         {tables/codes/guideline_prompt.json} % Update the path to your JSON file
%     \end{minipage}
% \end{table*}



% \onecolumn

\begin{lstlisting}[style=customjson, caption={Prompt example for generating Guideline-P, Guideline-PN, and Guideline-PS.}, label={lst:guidelines1}, aboveskip=10pt, belowskip=10pt]
You are an expert in annotating NLP datasets for event extraction. Your task is to generate "detailed" annotation guidelines for the event type Acquit which is a child event type of super class JusticeEvent.

Input Format will be as following
```
Event Schema:
Event Name and its parent class
Arguments:
Arguments separated by new lines. If there are no arguments None will be given.

Examples
```
Instructions:
1) Identify and list all unique arguments related to the event type.
2) Define the event type and each argument. You can take help of examples below to understand the events and their arguments. 
3) Please remember that the examples may not cover all the arguments in the list. In some cases, you may not have arguments at all, in such cases, you can have an empty list for arguments. 
4) For each definition, provide 5 illustrative definitions in JSON format. For events you can add example triggers and the explanation of the events such as edge cases and other critical details starting with "The event can be triggered by ... ". Similarly for arguments also you can add examples, and detailed information for them including any edge case or domain knowledge starting with "Examples are ... ".
5) Remember to not generate any additional information such as examples, etc. and strictly follow the output format shown below.
6) Remember also to add detailed information for the events and arguments so that the annotators who are not familiar with machine learning and NLP can still solve the task. Remember to add required domain knowledge and please cover the edge cases when possible.
7) Remember that while generating examples for the event or attributes you should generate diverse set of triggers or argument values rather than picking them from the examples I have provided for each of the 5 generated guidelines.

Output Format:
{
  "Event Definition": [
    "Definition 1",
    "Definition 2",
    "Definition 3",
    "Definition 4",
    "Definition 5"
  ],
  "Arguments Definitions": {
    "Argument1": [
      "Definition 1",
      "Definition 2",
      "Definition 3",
      "Definition 4",
      "Definition 5"
    ],
    "Argument2": [
      "Definition 1",
      "Definition 2",
      "Definition 3",
      "Definition 4",
      "Definition 5"
    ]
    // Add additional arguments as necessary
  }
}

Event Schema:
Acquit which is a child event type of super class JusticeEvent
Arguments:
Argument 1 -> adjudicator
Argument 2 -> defendant

Example 1
### Input Text ###
Sentence 1.
### Event Trigger ###
[event trigger]
### Event Arguments ###
For argument "defendant" extracted spans ['x']
For argument "adjudicator" extracted spans ['y']

Example 2
### Input Text ###
Sentence 2.
### Event Trigger ###
[event trigger]
### Event Arguments ###
For argument "defendant" extracted spans ['a']

(...)
\end{lstlisting}




\paragraph{Prompt for Generating Consolidated Guidelines.}The exact prompts used for generating consolidated guidelines - Guideline-PN-Int, and Guideline-PS-Int is shared below


\begin{lstlisting}[style=customjson, caption={Prompt example for generating consolidated guidelines: Guideline-PN-Int, and Guideline-PS-Int.}, label={lst:guidelines2}, aboveskip=10pt, belowskip=10pt]
You are an expert in summarizing NLP event extraction guidelines. Your goal is to consolidate multiple detailed descriptions into a single concise, comprehensive "Intergrated" guideline.

### Input Format ###
Event Type: Event Type Name
```json
{
  "Event Definition": [
    "Definition 1",
    "Definition 2",
    "Definition 3",
    "Definition 4",
    "Definition 5"
  ],
  "Arguments Definitions": {
    "mention": [
      "Definition 1",
      "Definition 2",
      "Definition 3",
      "Definition 4",
      "Definition 5"
    ],
    "Argument1": [
      "Definition 1",
      "Definition 2",
      "Definition 3",
      "Definition 4",
      "Definition 5"
    ],
    // Add additional arguments as necessary
  }
}
```

### Task ###
1. Integrated the 5 definitions under "Event Definition" into a single definition:
   - Highlight all critical points and examples from the five definitions.
   - Ensure the description is concise, comprehensive, and clear, using formal language that non-experts can understand.

2. Do the same for each argument under "Arguments Definitions," producing a single intergrated definition for each. 

### Output Format ###
```json
{
  "Event Definition": "Consolidated intergrated guideline for the event type.",
  "Arguments Definitions": {
    "mention": "Consolidated intergrated guideline for the mention argument.",
    "Argument1": "Consolidated intergrated guideline for Argument1.",
    "Argument2": "Consolidated intergrated guideline for Argument2."
    // Add additional arguments as necessary
  }
}
```

### Guidelines to Summarize ###
Event Type: prompt_Acquit(JusticeEvent)
```json
{
    "Acquit(JusticeEvent)": {
        "description": [
            "Definition 1",
            "Definition 2",
            "Definition 3",
            "Definition 4",
            "Definition 5"
        ]
    },
    "attributes": {
        "mention": "The text span that triggers the event."
        "adjudicator": [
            "Definition 1",
            "Definition 2",
            "Definition 3",
            "Definition 4",
            "Definition 5"
        ],
        "defendant": [
            "Definition 1",
            "Definition 2",
            "Definition 3",
            "Definition 4",
            "Definition 5"
        ]
    }
}
```
\end{lstlisting}


\section{Dataset Examples Across Multiple Guideline Settings}
\label{sec:app_dd}
The below JSON example illustrates an event extraction task from the ACE dataset under the No Guideline setting. It defines how structured events are extracted from text, specifying event triggers, types, arguments, and roles. The instruction explains the task, the input provides a natural language sentence and its conversion into a structured Python-style format. The output presents the extracted event, including its trigger ("extradited") and associated arguments (e.g., "government" as the agent, "him" as the person).

\begin{lstlisting}[style=customjson, caption={Prompt example for generating consolidated guidelines: Guideline-PN-Int, and Guideline-PS-Int.}, label={lst:guidelines3}, aboveskip=10pt, belowskip=10pt]
{
  "doc_id": "APW_ENG_20030306.0191",
  "wnd_id": "APW_ENG_20030306.0191-6",
  "instance_id": "821",
  "dataset_name": "ace05-en",
  "task_type": "E2E",
  "is_auth": "0",
  "instruction": "# This is an event extraction task where the goal is to extract structured events from the text. A structured event contains an event trigger word, an event type, the arguments participating in the event, and their roles in the event. For each different event type, please output the extracted information from the text into python-style dictionaries where the first key will be 'mention' with the value of the event trigger. Next, please output the arguments and their roles following the same format. The event type definitions and their argument roles are defined next.",
  "input": "# The following lines describe the task definition\n\n@dataclass\nclass Extradite(JusticeEvent):\n    mention: str\n    agent: List\n    destination: List\n    origin: List\n    person: List\n\n# This is the text to analyze\ntext = \"The post-Milosevic government later extradited him to the U.N. war crimes tribunal in The Hague, the Netherlands.\"\n\n# The list called result should contain the instances for the following events according to the guidelines above:\nresult = \n",
  "output": "[Extradite(\n    mention=\"extradited\",\n    person=[\"him\"], \n    destination=[\"Hague\"], \n    agent=[\"government\"],\n    origin=[]\n)]"
}
\end{lstlisting}

\paragraph{NoGuideline}
Shown below is an example from the NoGuideline setting in python code format with no doc string and argument definitions.
\vspace{10pt} % Adds space before the listing
\lstinputlisting[style=custompython, aboveskip=-5pt, belowskip=-5pt]
{NoGuideline_example_instance.py}

\paragraph{Guideline-PN}
Shown below is an example from the Guideline-PN setting in python code format.
\vspace{10pt} % Adds space before the listing
\lstinputlisting[style=custompython, aboveskip=-5pt, belowskip=-5pt]
{Guideline_PN_example_instance.py}

\paragraph{Guideline-PN-Int}
Similarly, shown below is an example from the Guideline-PN-Int setting in python code format.
\vspace{10pt} % Adds space before the listing
\lstinputlisting[style=custompython, aboveskip=-5pt, belowskip=-5pt]
{Guideline_PN_Int_example.py}
\twocolumn


