\subsection{Experimental Setup} 
\paragraph{Datasets.}
We perform experiments on two standard EE datasets: \textbf{ACE05} \cite{doddington-etal-2004-automatic} and \textbf{RichERE} \cite{song-etal-2015-light}. Both of them exhibit fine-grained event distinctions, and RichERE includes sparser event annotations (i.e., fewer event-labeled sentences), which makes it more challenging. Moreover, RichERE does not come with human-written annotation guidelines. Datasets were split following the TextEE benchmark~\cite{huang2024textee} and then converted to code format automatically by our scripts. 
% We present additional details and data sampling strategies in Appendix~\ref{sec:appendix_preprocessing}.


\paragraph{Evaluation.} 
Following prior work~\cite{huang2024textee}, we evaluate the model on four F1 metrics:
\textbf{(1) Trigger Identification (TI)}, which measures correct trigger span extraction, \textbf{(2) Trigger Classification (TC)}, which additionally requires event-type correctness, \textbf{(3) Argument Identification (AI)}, which ensures correct argument role association with the predicted trigger, and \textbf{(4) Argument Classification (AC)}, which further requires role-type correctness and is thus the most comprehensive metric on a model's EE performance. When evaluating the model on the Guideline-P, PN, and PS variants, one guideline is randomly selected each time.

As a side benefit of representing events in a structured code format, we can easily evaluate an extracted event instance by directly instantiating its corresponding Python object based on the event schema's Python class definitions, checking if the object is valid (e.g., missing arguments or including hallucinated arguments) and comparing it with the ground truth. This code-based evaluation thus prevents the tedious string-matching process adopted in prior work~\cite{li-etal-2021-document}. 
% We include details of our code-based EE evaluation in Appendix~\ref{sec:app_additional_details} and will release our scripts as community resources in the future.

\paragraph{Model Training.} We experimented with the {LLaMA-3.1-8B-Instruct} model~\cite{grattafiori2024llama3herdmodels}, selected for its demonstrated proficiency in processing structured code-based inputs and generating coherent outputs. When instruction-tuning the model under the Guideline-P, PN, and PS variants, we randomly sample one of the generated guidelines, a strategy found to prevent the model from memorizing specific guidelines in ~\citet{sainz2024gollie}.
For parameter-efficient training, we implemented rsLoRA~\cite{kalajdzievski2023rankstabilizationscalingfactor} using the Unsloth framework~\cite{unsloth}. 
% Key hyperparameters—including LoRA rank (64), scaling factor $\alpha$ (128), and batch size (32)—were determined through preliminary experiments to balance computational efficiency with model performance. Models were trained for 10 epochs using a single NVIDIA A100 GPU (80GB VRAM) with early stopping based on their dev-set performance. 
% Further details on the experimental setup are included in Appendix~\ref{sec:app_additional_details}.

We include all details about datasets, evaluation, and model training in Appendix~\ref{sec:appendix_preprocessing}-\ref{sec:app_additional_details}. {We will open-source our scripts for automatically converting datasets in TextEE~\cite{huang2024textee} into Python code format (we dub the processed version as \textbf{PyCode-TextEE}) and for evaluating extracted events automatically via code execution at our GitHub repository \url{https://github.com/salokr/LLaMA-Events}.}