\section{Introduction}

\zyc{Around 1.5 pages}

\zyc{P1: Background and motivation. 1) Event Extraction (EE): what it is and why is it important. 2) this is a challenging task due to XXX.} 
% Event extraction (EE) in natural language processing (NLP) involves identifying and classifying events within text, essential for several downstream applications such as information retrieval, question answering, and text summarization. This process often involves detecting event-specific trigger words or phrases that denote the occurrence of an event and the arguments associated with those events, such as participants, time, location, etc.
Event Extraction (EE), a fundamental task in natural language processing (NLP), aims to identify and structure \textit{what, who, when, where, and how} of real-world events. While events are formally defined as temporally situated state changes or activities \citep{doddington-etal-2004-automatic}, translating this abstraction requires complex schema specifications that define event types, argument roles, and their interrelationships. The inherent challenges of EE arise from two key factors. The first challenge is evident from the annotation guidelines, which define the rules for identifying event triggers and argument roles. These guidelines often require distinguishing between nuanced event types (e.g., Sentence vs. Convict), a task that is not always intuitive. Second, data dependence—state-of-the-art models rely heavily on large-scale human-annotated training data. This reliance limits robustness to schema variations and creates a bottleneck for domain adaptation, as real-world applications frequently introduce new event types that existing models struggle to handle. \sriv{if RichERE exploration says that the addition of new arguments to existing schema can be handled then we can refine the writing to introduce schema migration?}


% An event is an occurrence of an activity that happens at a specific time and place, or it may represent a change of state \cite{doddington-etal-2004-automatic}. Event extraction (EE), a fundamental task in natural language processing (NLP), aims to identify and structure event-related information from text, capturing key elements such as \textit{what, who, when, where, and how} of real-world events \cite{gollie}. EE schemas define high-level event types and their structural relationships, incorporating various concepts and argument roles. Despite its long-standing significance, EE remains a highly challenging task \cite{gollie}. These challenges are evident in the detailed annotation guidelines, which impose granular definitions, numerous exceptions, and intricate role assignments that human annotators must meticulously follow. As a result, the performance of state-of-the-art (SoTA) models is heavily reliant on the availability of high-quality, human-annotated data, as they primarily learn extraction patterns from these labeled examples.
%EE applications are diverse, facilitating trend analyses \cite{min-zhao-2019-measure} and supporting tasks in knowledge graph construction \cite{8970862}, question answering \cite{boyd-graber-borschinger-2020-question}, and beyond \cite{X, Y, Z}. The technique has been applied across fields including the biomedical sciences \cite{x}, literature \cite{x}, cybersecurity \cite{X}, history \cite{X}, and humanities \cite{x}.

% Despite the significant progress made by large language models (LLMs) in various NLP tasks, they continue to face difficulties in handling EE due to the highly nuanced and context-sensitive nature of language. These challenges arise because event triggers can often be ambiguous, polysemous, or influenced by subtle contextual cues that are difficult to capture. For instance, the word "bank" could refer to a financial institution or the edge of a river, with the meaning entirely dependent on the context in which it appears. 

% However, existing methods often face challenges with ambiguity and context sensitivity, leading to limitations in effectiveness, scalability, and, generalizability.

\zyc{P2: Transition to LLM-based EE. LLMs have significantly advanced many NLP applications. There are also explorations of LLMs for EE: 1) prompting LLMs (often paid APIs), briefly what it is and cite papers, but it is costly especially considering the long reasoning output; 2) instruction tuning medium-size open-weight LLMs solves the cost issue: what it is and cite papers. Highlight Gollie (code format, annotation guidelines were found helpful). [Need more elaboration on annotation guideline to justify why we focus on it. for example we can say that as guideline is main place for specifying the ET definition, it is proved to be crucial.] 
However, Gollie has a main focus on NER rather than EE, and many nuances about the annotation guidelines as part of the task instruction are not explored in EE. Please refine the paragraph sketch considering other "instruction tuning for IE/EE" papers' limitations.}

Recently, Large Language Models (LLMs) have transformed information extraction (IE) by demonstrating remarkable generalization to unseen tasks. Given their extensive pretraining, they encode a rich understanding of events and their participants, making them promising for EE. While pretrained LLMs inherently encode rich representations of events and participants, their effective application to EE remains nontrivial. 

Modern approaches to EE are divided into two strategies: API-based prompting of proprietary models and instruction tuning of open-weight LLMs—each presenting unique trade-offs. API-centric methods employ zero/few-shot prompting to query black-box LLMs like GPT-4, relying on latent knowledge to infer event structures. However, these approaches exhibit critical limitations: they frequently misinterpret annotation guidelines that formally define event schemas, suffer from brittle sensitivity to prompt phrasing and example ordering \citep{X}, and incur prohibitive costs due to lengthy reasoning chains in complex EE scenarios. This brittleness stems from a fundamental misalignment—LLMs' internal event representations rarely match the formal specifications used by human annotators, creating a ``conceptual gap'' that undermines reliability.

Instruction tuning emerges as a cost-effective alternative, fine-tuning mid-sized models \footnote{In this paper, we consider XX as medium-sized LLMs} with explicit task instructions to bridge this gap.  GoLLIE \citep{GoLLIE} demonstrated the value of integrating annotation guidelines into model inputs, employing code-like prompt structures \citep{code4struct} to reduce reliance on prompt engineering. By explicitly encoding event definitions and constraints (e.g., ``A \texttt{Conflict:Attack} event requires at least one attacker or one target''), these approaches improve alignment between model reasoning and human schema design principles. Notably, GoLLIE further demonstrated that incorporating annotation guidelines enhances the inductive bias of LLMs, enabling generalization not only to unseen named entities but also to more abstract concepts such as event structures and argument roles. 

\sriv{Note: If required we can also write that instruction-tuning has been seen as an extreme form of meta-learning [cite metaICL, UIE, etc.]. Since they provide a head start through instructions, LLMs can utilize them and quickly adapt to a task. Annotating these instructions with guidelines is thus an intuitive way of providing even more information and helping them understand the abstract concepts and symbols easily.}

Yet current efforts remain largely focused on NER, leaving key challenges in EE insufficiently explored. \sriv{redfine from here} Event extraction demands a nuanced understanding of (1) nested argument roles, (2) cross-trigger dependencies, and (3) guideline-aware boundary detection, none of which are adequately explored by current architectures. \textit{``To what extent can annotation guidelines enhance EE performance?'' and, ``how should models be designed when such guidelines are unavailable?''} In this work, we systematically explore these questions, investigating whether existing techniques, such as alternative supervision signals, can improve EE accuracy in LLMs.

% Despite these advancements, GoLLIE remains focused on Named Entity Recognition (NER), leaving critical questions unexplored in EE: To what extent can annotation guidelines enhance EE performance? And how should models be designed when such guidelines are unavailable? In this work, we systematically explore these questions, investigating whether traditional methods—such as leveraging alternative supervision signals—can improve EE accuracy with LLMs.

\zyc{P3: Summarize what we do in this paper.} 
In this paper, we carefully examine the effect of annotation guidelines when instruction-tuning medium-size LLMs for EE. Our exploration focuses on three research questions:
\textbf{(1) Do the annotation guidelines allow the LLM to more easily distinguish between similar event types?} Prior work showed that the confusion between similar event types (e.g., XX vs. XX in XX dataset~\cite{?}) is one major type of mistake made by state-of-the-art EE models. [I made it up; let me know if this is true or not.] We hypothesize that the annotation guidelines, by providing detailed event type descriptions in the instruction, enable the LLM to more effectively contrast between similar event types. However, there has not been a careful investigation of this potential effect.
\textbf{(2) Do human-written guidelines provide more or fewer insights than machine-generated guidelines when instruction-tuning an LLM?} Existing work often assumed the availability of human-written annotation guidelines~\cite{?}. However, this assumption does not hold for many datasets such as XX and XX. A natural solution is thus to leverage LLMs to generate annotation guidelines. However, it is uncertain whether human-written guidelines are necessary for improving the instruction tuning of LLMs for EE.
\textbf{(3) Are the annotation guidelines more or less helpful when there is only a small amount of training data?} We seek to understand the effect of annotation guidelines when there are different sizes of the instruction tuning data. Gollie focused on large-scale instruction-tuning, yet whether the guidelines are helpful or not when there are fewer training examples is unclear.
\textbf{(4) Does the advantage of annotation guidelines generalize to unseen datasets?} Finally, we explore, in case of any advantage by including annotation guidelines in the EE instruction, whether the advantage generalizes to unseen EE datasets. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{latex/figures/example_instruction.pdf}
    \caption{An example of code-format instruction with annotation guidelines for Event Extraction (EE). \zyc{Todo: replace by our own example} \sriv{this is a draft figure, we are working on updating it.}}
    \label{fig:example_instruction}
\end{figure}

P4: what experiments did we do and the takeaways. To answer these questions, we experimented with Llama-3.1-8B on two EE datasets, namely, ACE and RichERE, in various settings. Our results showed that including the annotation guidelines in the instruction does help the LLM performance, but this advantage can be offset by directly populating the training set with negative samples, which was not explored in prior work CITE Gollie. In addition, we found that machine-generated guidelines offer more advantages than human-written guidelines. Specifically, we experimented with guidelines generated by feeding GPT-4o with event examples XXX [details here]. We found that ... Our results also showed that the benefit of annotation guidelines is more prominent when there are a limited number of training examples. However, when the training size is too small, it also becomes hard for an LLM to understand the guideline... Finally, ...zero-shot generalization. \zyc{This paragraph needs to be refined based on the final results.}
