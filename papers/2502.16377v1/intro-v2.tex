\section{Introduction}

% \zyc{Around 1.5 pages}

% \zyc{P1: Background and motivation. 1) Event Extraction (EE): what it is and why is it important. 2) this is a challenging task due to XXX.} 
Event Extraction (EE) aims to identify and structure \textit{what, who, when, where, and how} of real-world events from given textual resources~\cite{doddington-etal-2004-automatic, ji-grishman-2008-refining, li2022survey, xu2024large}. 
% While events are formally defined as temporally situated state changes or activities \citep{doddington-etal-2004-automatic}, 
Translating this abstraction requires complex schema specifications that define event types, argument roles, and their interrelationships, yet being able to precisely capture the language nuances and distinguish between event types and argument roles, which posits the task as an inherently challenging problem.
% The inherent challenges of EE arise from two key factors. The first challenge is evident from the annotation guidelines, which define the rules for identifying event triggers and argument roles. These guidelines often require distinguishing between nuanced event types (e.g., Sentence vs. Convict), a task that is not always intuitive. Second, data dependence—state-of-the-art models rely heavily on large-scale human-annotated training data. This reliance limits robustness to schema variations and creates a bottleneck for domain adaptation, as real-world applications frequently introduce new event types that existing models struggle to handle. \sriv{if RichERE exploration says that the addition of new arguments to existing schema can be handled then we can refine the writing to introduce schema migration?}



% \zyc{P2: Transition to LLM-based EE. LLMs have significantly advanced many NLP applications. There are also explorations of LLMs for EE: 1) prompting LLMs (often paid APIs), briefly what it is and cite papers, but it is costly especially considering the long reasoning output; 2) instruction tuning medium-size open-weight LLMs solves the cost issue: what it is and cite papers. Highlight Gollie (code format, annotation guidelines were found helpful). [Need more elaboration on annotation guideline to justify why we focus on it. for example we can say that as guideline is main place for specifying the ET definition, it is proved to be crucial.] 
% However, Gollie has a main focus on NER rather than EE, and many nuances about the annotation guidelines as part of the task instruction are not explored in EE. Please refine the paragraph sketch considering other "instruction tuning for IE/EE" papers' limitations.}

% Recently, Large Language Models (LLMs) have transformed information extraction (IE) by demonstrating remarkable generalization to unseen tasks. Given their extensive pretraining, they encode a rich understanding of events and their participants, making them promising for EE. While pretrained LLMs inherently encode rich representations of events and participants, their effective application to EE remains nontrivial. 
% Modern approaches to EE are divided into two strategies: API-based prompting of proprietary models and instruction tuning of open-weight LLMs—each presenting unique trade-offs. API-centric methods employ zero/few-shot prompting to query black-box LLMs like GPT-4, relying on latent knowledge to infer event structures. However, these approaches exhibit critical limitations: they frequently misinterpret annotation guidelines that formally define event schemas, suffer from brittle sensitivity to prompt phrasing and example ordering \citep{X}, and incur prohibitive costs due to lengthy reasoning chains in complex EE scenarios. This brittleness stems from a fundamental misalignment—LLMs' internal event representations rarely match the formal specifications used by human annotators, creating a ``conceptual gap'' that undermines reliability.
% ~
% Instruction tuning emerges as a cost-effective alternative, fine-tuning mid-sized models \footnote{In this paper, we consider XX as medium-sized LLMs} with explicit task instructions to bridge this gap.  GoLLIE \citep{GoLLIE} demonstrated the value of integrating annotation guidelines into model inputs, employing code-like prompt structures \citep{code4struct} to reduce reliance on prompt engineering. By explicitly encoding event definitions and constraints (e.g., ``A \texttt{Conflict:Attack} event requires at least one attacker or one target''), these approaches improve alignment between model reasoning and human schema design principles. Notably, GoLLIE further demonstrated that incorporating annotation guidelines enhances the inductive bias of LLMs, enabling generalization not only to unseen named entities but also to more abstract concepts such as event structures and argument roles. 

% \sriv{Note: If required we can also write that instruction-tuning has been seen as an extreme form of meta-learning [cite metaICL, UIE, etc.]. Since they provide a head start through instructions, LLMs can utilize them and quickly adapt to a task. Annotating these instructions with guidelines is thus an intuitive way of providing even more information and helping them understand the abstract concepts and symbols easily.}

% Yet current efforts remain largely focused on NER, leaving key challenges in EE insufficiently explored. \sriv{redfine from here} Event extraction demands a nuanced understanding of (1) nested argument roles, (2) cross-trigger dependencies, and (3) guideline-aware boundary detection, none of which are adequately explored by current architectures. \textit{``To what extent can annotation guidelines enhance EE performance?'' and, ``how should models be designed when such guidelines are unavailable?''} In this work, we systematically explore these questions, investigating whether existing techniques, such as alternative supervision signals, can improve EE accuracy in LLMs.
Recently, large language models (LLMs) have transformed NLP research and practices dramatically, owing to the rich knowledge and other capabilities (e.g., reasoning) they have obtained from extensive pre-training~\cite{wei2022chain, chen2023program, shi2023dont}. This transformation has similarly impacted the broader research field of Information Extraction (IE). Existing applications of LLMs to IE can be categorized into two lines. The \emph{prompt engineering-based approaches}, often based on proprietary LLMs, consider an LLM as a black box, querying it with task specifications via zero- or few-shot prompting and relying on its latent knowledge to extract interested information~\cite{gao2023exploringfeasibilitychatgptevent, wang2023code4struct, li2023evaluating, srivastava-etal-2023-mailex}. However, these approaches not only lead to inferior performance but also incur prohibitive costs, especially when the task is complex.
% While being data-efficient, they were found to misinterpret the task specifications~\cite{?} and suffer from brittle sensitivity to prompt phrasing and example ordering \citep{X}. In addition, they also incur prohibitive costs due to the lengthy reasoning chains especially for complex tasks. 

%UIE - cited below
Our work will thus focus on the second line of approach, namely, \emph{instruction-tuning open-weight LLMs}. This line of approach adapts an LLM to specific IE tasks and schemas by directly training it to follow the task instructions, which offers a promising yet cost-effective solution. For example, \citet{wang2023instructuiemultitaskinstructiontuning}  leverages natural language instructions to
guide large language models for IE tasks; \citet{li-etal-2024-knowcoder} proposed a two-phase learning framework that enhances schema understanding and following ability via automatically annotated data. More recently, \citet{sainz2024gollie} instruction-tuned LLaMA~\cite{touvron2023llamaopenefficientfoundation} on multiple IE datasets and discovered \emph{annotation guidelines}---textual descriptions of an event type and its argument roles used by human annotators when collecting the dataset, as effective components of an IE task's instruction. Despite the promise of the existing explorations, however, most of them have focused on the relatively simpler task of Named Entity Recognition, yet how to properly instruction-tune LLMs for the structured EE task is still understudied.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=.95\linewidth]{final_overview.pdf}
    \caption{Overview of our exploration of automatically generating annotation guidelines to augment code-format instruction tuning for EE. Prompt template for Guideline-PN and the example outputs are shown.
    % Our approach leverages code prompts with annotated guidelines for EE. In the absence of an explicitly defined schema (bottom-left), the LLM incorrectly assumes that movement events are restricted to \texttt{Transport} actions (top-left). To mitigate this, we explore various guideline generation strategies (center) to systematically annotate schema information. By integrating these guidelines with schema annotations, our SFT-based approach enhances the model’s comprehension of diverse event types, leading to improved predictions (bottom-right).
    }
    \label{fig:overview}
\end{figure*}
% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=.9\linewidth]{latex/figures/overview_v3.pdf}
%     \caption{Overview of our method for automatically generating annotation guidelines to enhance code-format instruction tuning for EE. For \textcolor[HTML]{D6B656}{\textsf{Guideline Generation}}, we use GPT-4 with prompts specifying positive (\includegraphics[width=0.017\textwidth]{figures/positive.pdf}), negative (\includegraphics[width=0.014\textwidth]{latex/figures/negative.pdf}), or sibling (\includegraphics[width=0.017\textwidth]{latex/figures/sibling.pdf}) event types, producing Guideline-P, PN, and PS, respectively. The \$-Int versions are generated by combining previously produced PN or PS guidelines. The prompt templates for Guideline-PN and example outputs are also shown. We note that \textcolor[HTML]{660066}{\textsf{Schema + Generated Guidelines}} result in correct outputs.
%     % Our approach leverages code prompts with annotated guidelines for EE. In the absence of an explicitly defined schema (bottom-left), the LLM incorrectly assumes that movement events are restricted to \texttt{Transport} actions (top-left). To mitigate this, we explore various guideline generation strategies (center) to systematically annotate schema information. By integrating these guidelines with schema annotations, our SFT-based approach enhances the model’s comprehension of diverse event types, leading to improved predictions (bottom-right).
%     }
%     \label{fig:overview}
% \end{figure*}

To fill this gap, we study instruction-tuning LLMs for EE, with a focus on the role of annotation guidelines in task instructions (Fig. \ref{fig:overview}). 
% While \citet{sainz2024gollie} have shown the promise, their comparative advantages, scalability, and generalization remain underexplored. 
We conduct a systematic analysis using LLaMA-3.1-8B-Instruct on two EE datasets (ACE05~\cite{doddington-etal-2004-automatic} and RichERE~\cite{song-etal-2015-light}) under varied training settings. Our key findings are organized around four themes:

1) \textit{Effect of Annotation Guidelines on Event Extraction} — We found that annotation guidelines improve performance by helping the model distinguish fine-grained event types. However, this advantage may diminish when negative sampling is introduced during training, which allows the model to learn event distinctions from additional contrastive examples instead.

2) \textit{Comparing Machine-Generated and Human-Written Guidelines} — Prior work assumed access to human-authored guidelines, which may not hold in practice. We thus proposed 5 different ways to automatically generate annotation guidelines. We find that they outperform human-written ones by up to 11\% and 7\% in trigger and argument classifications, respectively. 
% Guidelines emphasizing contrastive event definitions are particularly effective.

3) \textit{Guidelines in Data-Scarce Scenarios} — Our results show that with only 2000 training samples, guidelines allow LLMs to reach performance levels comparable to full-data training. However, when data is extremely scarce (100 samples), models tend to rely more on memorization than on guideline-driven schema constraints.

4) \textit{Cross-Schema Generalization} — We assess whether structured guidelines help models generalize to different EE schemas. While models trained on RichERE transfer well to ACE (suggesting fine-to-coarse schema adaptation is feasible), the reverse scenario sees a performance drop due to RichERE’s more complex event structures and expanded argument roles.

Finally, we confirmed a consistent effect of annotation guidelines on the smaller LLaMA-3.2-1B-Instruct model and conducted an in-depth analysis showing that the guidelines can help LLMs reduce common types of EE errors and are beneficial to event types with any frequency in the training set.

% To systematically assess these factors, we conduct experiments using Llama-3.1-8B on two EE datasets: ACE05~\cite{?} and RichERE~\cite{?}. Figure~\ref{fig:guideline_diagram} illustrates how annotation guidelines are incorporated into event schema instructions.

% \input{latex/tables/codes/example_code_format_instruction}
% \input{latex/tables/codes/example_code_instruction_full_page}
% \zyc{P3: Summarize what we do in this paper.} 
% In this paper, we study instruction-tuning LLMs particularly for EE. Inspired by \citet{sainz2024gollie}, we consider the annotation guidelines as a critical component in the EE task instruction but aim to delve deep into its effect. Our exploration focuses on four research questions:
% \textbf{(1) Do the annotation guidelines allow an LLM to more precisely extract occurring events?} 
% Prior work showed that the confusion between similar event types (e.g., XX vs. XX in XX dataset~\cite{?}) is one major type of mistake made by state-of-the-art EE models. [I made it up; let me know if this is true or not.] 

% We hypothesize that the annotation guidelines, by providing detailed event schema descriptions in the instruction, will enable the LLM to more effectively distinguish between event types and identify argument roles. To better understand its advantage, we compare it with a \emph{negative sampling} baseline which, instead of including annotation guidelines in the instructions, trains an LLM to distinguish between event types by expanding the training set with negative samples.
% \textbf{(2) Do human-written guidelines provide more or fewer insights than machine-generated guidelines when instruction-tuning an LLM?} 
% \textbf{(2) Can machine-generated guidelines be more effective than human-written ones?}
% \citet{sainz2024gollie} assumed the availability of human-written annotation guidelines. However, this assumption does not hold for every EE schema. A natural solution is thus to automatically generate annotation guidelines. However, it is uncertain whether the machine-generated guidelines are more or less effective than the human-written ones.
% \textbf{(3) Are the annotation guidelines helpful when there is only a small amount of training data?} Existing work dominantly focused on large-scale instruction-tuning LLMs~\cite{sainz2024gollie, ?}, yet whether instruction tuning can adapt an LLM under the low data setting and whether the guidelines are still helpful is unclear.
% \textbf{(4) Does the advantage of annotation guidelines generalize to unseen datasets?} Finally, we explore whether including guidelines in instruction tuning allows the model to generalize better to unseen datasets. 



% % P4: what experiments did we do and the takeaways. 
% To answer these questions, we experimented with Llama-3.1-8B on two EE datasets, namely, ACE05~\cite{?} and RichERE~\cite{?}, in various settings. Our results showed that including the annotation guidelines in the instruction does help the LLM performance, but this advantage can be offset by directly populating the training set with negative samples, which was not explored in prior work CITE Gollie. In addition, we found that machine-generated guidelines offer more advantages than human-written guidelines. Specifically, we experimented with guidelines generated by feeding GPT-4o with event examples XXX [details here]. We found that ... Our results also showed that the benefit of annotation guidelines is more prominent when there are a limited number of training examples. However, when the training size is too small, it also becomes hard for an LLM to understand the guideline... Finally, ...zero-shot generalization. \zyc{This paragraph needs to be refined based on the final results.}
