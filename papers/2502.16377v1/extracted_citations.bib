@misc{ashok2023promptnerpromptingnamedentity,
      title={PromptNER: Prompting For Named Entity Recognition}, 
      author={Dhananjay Ashok and Zachary C. Lipton},
      year={2023},
      eprint={2305.15444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.15444}, 
}

@misc{gao2023exploringfeasibilitychatgptevent,
      title={Exploring the Feasibility of ChatGPT for Event Extraction}, 
      author={Jun Gao and Huan Zhao and Changlong Yu and Ruifeng Xu},
      year={2023},
      eprint={2303.03836},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.03836}, 
}

@misc{gao2024eventrlenhancingeventextraction,
      title={EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models}, 
      author={Jun Gao and Huan Zhao and Wei Wang and Changlong Yu and Ruifeng Xu},
      year={2024},
      eprint={2402.11430},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11430}, 
}

@misc{gui2024instructiebilingualinstructionbasedinformation,
      title={InstructIE: A Bilingual Instruction-based Information Extraction Dataset}, 
      author={Honghao Gui and Shuofei Qiao and Jintian Zhang and Hongbin Ye and Mengshu Sun and Lei Liang and Jeff Z. Pan and Huajun Chen and Ningyu Zhang},
      year={2024},
      eprint={2305.11527},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.11527}, 
}

@misc{guo2023retrievalaugmentedcodegenerationuniversal,
      title={Retrieval-Augmented Code Generation for Universal Information Extraction}, 
      author={Yucan Guo and Zixuan Li and Xiaolong Jin and Yantao Liu and Yutao Zeng and Wenxuan Liu and Xiang Li and Pan Yang and Long Bai and Jiafeng Guo and Xueqi Cheng},
      year={2023},
      eprint={2311.02962},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.02962}, 
}

@inproceedings{li-etal-2023-codeie,
    title = "{C}ode{IE}: Large Code Generation Models are Better Few-Shot Information Extractors",
    author = "Li, Peng  and
      Sun, Tianxiang  and
      Tang, Qiong  and
      Yan, Hang  and
      Wu, Yuanbin  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.855/",
    doi = "10.18653/v1/2023.acl-long.855",
    pages = "15339--15353",
    abstract = "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks."
}

@inproceedings{li-etal-2024-knowcoder,
    title = "{K}now{C}oder: Coding Structured Knowledge into {LLM}s for Universal Information Extraction",
    author = "Li, Zixuan  and
      Zeng, Yutao  and
      Zuo, Yuxin  and
      Ren, Weicheng  and
      Liu, Wenxuan  and
      Su, Miao  and
      Guo, Yucan  and
      Liu, Yantao  and
      Lixiang, Lixiang  and
      Hu, Zhilei  and
      Bai, Long  and
      Li, Wei  and
      Liu, Yidan  and
      Yang, Pan  and
      Jin, Xiaolong  and
      Guo, Jiafeng  and
      Cheng, Xueqi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.475/",
    doi = "10.18653/v1/2024.acl-long.475",
    pages = "8758--8779",
    abstract = ""
}

@inproceedings{lu-etal-2023-pivoine,
    title = "{PIVOINE}: Instruction Tuning for Open-world Entity Profiling",
    author = "Lu, Keming  and
      Pan, Xiaoman  and
      Song, Kaiqiang  and
      Zhang, Hongming  and
      Yu, Dong  and
      Chen, Jianshu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.1009/",
    doi = "10.18653/v1/2023.findings-emnlp.1009",
    pages = "15108--15127",
    abstract = "This work considers the problem of Open-world Entity Profiling, a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE is considered a more general situation where entities and relations could be beyond a predefined ontology. We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction-tuning dataset for Open-world Entity Profiling enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world Entity Profiling with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional methods and ChatGPT-based baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge of entity profiling."
}

@inproceedings{pang-etal-2023-guideline,
    title = "Guideline Learning for In-Context Information Extraction",
    author = "Pang, Chaoxu  and
      Cao, Yixuan  and
      Ding, Qiang  and
      Luo, Ping",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.950/",
    doi = "10.18653/v1/2023.emnlp-main.950",
    pages = "15372--15389",
    abstract = "Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE."
}

@inproceedings{sainz2024gollie,
title={Go{LLIE}: Annotation Guidelines improve Zero-Shot Information-Extraction},
author={Oscar Sainz and Iker Garc{\'\i}a-Ferrero and Rodrigo Agerri and Oier Lopez de Lacalle and German Rigau and Eneko Agirre},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Y3wpuxd7u9}
}

@inproceedings{wang2023code4struct,
  title={Code4Struct: Code Generation for Few-Shot Event Structure Prediction},
  author={Wang, Xingyao and Li, Sha and Ji, Heng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3640--3663},
  year={2023}
}

@misc{wang2023instructuiemultitaskinstructiontuning,
      title={InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction}, 
      author={Xiao Wang and Weikang Zhou and Can Zu and Han Xia and Tianze Chen and Yuansen Zhang and Rui Zheng and Junjie Ye and Qi Zhang and Tao Gui and Jihua Kang and Jingsheng Yang and Siyuan Li and Chunsai Du},
      year={2023},
      eprint={2304.08085},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08085}, 
}

@article{xu2024large,
  title={Large language models for generative information extraction: A survey},
  author={Xu, Derong and Chen, Wei and Peng, Wenjun and Zhang, Chao and Xu, Tong and Zhao, Xiangyu and Wu, Xian and Zheng, Yefeng and Wang, Yang and Chen, Enhong},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186357},
  year={2024},
  publisher={Springer}
}

@misc{zhou2024universalnertargeteddistillationlarge,
      title={UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition}, 
      author={Wenxuan Zhou and Sheng Zhang and Yu Gu and Muhao Chen and Hoifung Poon},
      year={2024},
      eprint={2308.03279},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03279}, 
}

