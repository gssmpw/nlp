\section{Further Experiments and Analysis}
\subsection{Guidelines With Smaller LLMs}
%\input{latex/tables/deep_seek_res}
\input{latex/tables/LLaMA_3_2}
% Surprisingly, \textbf{DeepSeek 1.5B} surpasses its LLaMA counterpart in \textbf{TI} and \textbf{TC}, with \textbf{Guideline-PN w/ NS} achieving the highest F1 score (61.37) challenging the assumption that larger models ``always'' perform better. However, this trend does not extend to argument tasks \textbf{(AI, AC)}, where the 7B model consistently outperforms its 1.5B variant, suggesting that smaller models may specialize in trigger differentiation but struggle with complex tasks such as argument identification and classification. Across all settings, adding negative samples improves performance, with \textbf{NoGuideline w/ NS} outperforming \textbf{NoGuideline w/o NS} by over 20 points in TI and TC. Among other guideline-based methods, \textbf{Guideline-PS w/ NS} consistently yields comparable performance with other approaches.
\zyc{the comparison between llama vs. deepseek does not seem to the focus of this paper}
%Table~\ref{tab:deepseek_results} illustrates the performance. Surprisingly, \textbf{DeepSeek 1.5B} outperforms its LLaMA counterpart in \textbf{TI} and \textbf{TC}, with \textbf{Guideline-PN w/ NS} achieving the highest F1 score (61.37), challenging the assumption that larger LLMs may always perform better. However, this advantage does not extend to argument tasks \textbf{(AI, AC)}, where the 7B model consistently outperforms its 1.5B variant. This suggests that smaller models may specialize in trigger differentiation but struggle with more complex reasoning tasks such as argument identification and classification. Across all settings, adding negative samples improves performance, with \textbf{NoGuideline w/ NS} surpassing \textbf{NoGuideline w/o NS} by over 20 points in TI and TC. Among guideline-based methods, \textbf{Guideline-PS w/ NS} performs comparably to other approaches, a similar observation from main experiments results.
Based on the main experiment results from Table~\ref{tab:llama_3.2}, we selected the guideline settings with the best performance across experiments for further exploration with the DeepSeek-R1-Distill-Qwen-1.5B model. Specifically, we chose NoGuideline, Guideline-PN, and Guideline-PS for additional testing with smaller LLMs, focusing on their effectiveness in specialized tasks. Guideline-PN w/ NS achieved the highest F1 score across all 4 metrics, with (61.14) in TC and (39.88) in AC (see Table~\ref{tab:deepseek_results}), demonstrating the value of guidelines and negative samples in improving performance for these tasks. Across all settings, adding negative samples improved performance, and the guideline-based methods consistently performed better as seen from the TC and AC F1 scores, when compared to the NoGuideline setting. While these results are strong, they also highlight the distinct capabilities of smaller models like DeepSeek-R1-Distill-Qwen-1.5B, which, although competitive, still lag behind larger models like LLaMA 3.1 8B in certain tasks.

\subsection{Error Analysis and Categorization}

We analyzed 100 erroneous predictions across the RichERE and ACE datasets, categorizing failures into six types: context ambiguity (CA), parsing errors (PE), missing multiple arguments/events (MMA), argument extraction errors (AE), trigger/type errors (WT/ET), and label noise (LN). CA errors occur when the model misinterprets the meaning of a phrase. For example, in "the officer is going to go in and swoop her away," the model misclassifies "go" and "swoop" as Transport events, misinterpreting the metaphor. LN arises when there is a mismatch between the ground truth annotations and the annotation guidelines. In "Secession by Iraqi Kurds could inspire Turkey's rebel Kurds, who for 15 years have been fighting for autonomy," the ground truth incorrectly labels "fighting" as a Demonstrate event. A similar issue was reported in the GOLLIE paper (cite GOLLIE). Figure~\ref{fig:error_cat} shows the distribution of these errors, with the following observations.

In the No Guideline w/o NS setting, RichERE struggles with polysemous triggers (WT/ET: 39), contextual ambiguity (CA: 36), and missing arguments (MMA: 21). Negative sampling (No Guideline w/ NS) reduces CA (36 → 6) and WT/ET (39 → 12), improving event disambiguation, though AE (9 → 6) remains challenging due to complex argument structures. Guideline-P w/ NS further reduces MMA (21 → 9) and WT/ET (39 → 18) highlighting performance improvement through annotation guidelines, but CA remains high (24), indicating unresolved ambiguity. Error rates are the lowest in the Guideline-PN w/ NS setting, with CA dropping to 3 and WT/ET to 3, demonstrating the effectiveness of guideline generation through the inclusion of contrasting events, coupled with negative samples.

In the No Guideline w/o NS setting, ACE shows notable errors with MMA (19), CA (13), and AE (19), indicating challenges in capturing multiple arguments, resolving ambiguity, and extracting arguments. Negative sampling (No Guideline w/ NS) reduces CA (13 → 5) and WT/ET (13 → 11), improving event disambiguation by exposing the model to both correct and incorrect event types, helping refine decision boundaries. Guideline-P w/ NS further lowers WT/ET (13 → 8) and AE (19 → 12), with minor improvements in other categories, suggesting that annotation guidelines reduce misclassifications in WT/ET, while clearer event role definitions enhance AE, minimizing confusion in argument assignment. In the Guideline-PN w/ NS setting, error rates are the lowest, with CA and AE dropping to 2, reinforcing that guidelines and negative sampling together improve event differentiation and argument extraction.

% Kept the long writeup for reference:
%We analyzed 100 erroneous predictions across RichERE and ACE, categorizing failures into six types: context ambiguity (CA), parsing errors (PE), missing multiple arguments/events (MMA), argument extraction errors (AE), trigger/type errors (WT/ET), and label noise (LN). Through Fig.~\ref{fig:error_cat} we make the following observations.
%In the No Guideline w/o NS setting, RichERE exhibits high error rates, particularly in WT/ET (39), CA (36), and MMA (21), highlighting struggles with polysemous trigger/type errors, contextual ambiguity, and missing multiple arguments/events. When shifting to No Guideline w/ NS, errors drop significantly, with CA (36 → 6) and WT/ET (39 → 12), suggesting that negative sampling enhances prediction accuracy. However, AE (9 → 6) shows only a minor decrease, indicating lingering challenges in extracting correct arguments. Introducing guidelines in the form of Guideline-P w/ NS reduces MMA (21 → 9) and WT/ET (39 → 18), though CA remains at 24, suggesting that while negative sampling aids disambiguation, some contextual ambiguity persists. Guideline-PN w/ NS, the lowest error rates are observed, with CA dropping to 3 and WT/ET to 3, demonstrating the effectiveness of guideline generation through contrasting event inclusion, coupled with negative samples, in improving event extraction. 
%In the No Guideline w/o NS setting, ACE shows notable errors in MMA (19), CA (13), and AE (19), reflecting challenges in capturing multiple arguments/events, handling contextual ambiguity, and extracting correct arguments. When shifting to No Guideline w/ NS, errors decrease significantly in CA (13 → 5) and WT/ET (13 → 11), suggesting that negative sampling helps disambiguate events. Introducing guidelines in the form of Guideline-P w/ NS leads to a further reduction in WT/ET (13 → 8) and AE (19 → 12), as well as minor improvements across other categories. Finally, with Guideline-PN w/ NS, the lowest error rates are observed, with CA dropping to just 2 and AE to 2, again demonstrating guideline generation through contrasting event inclusion, coupled with negative samples yields better performance.


% \sriv{Fig.~\ref{fig:error_cat} shows the ...}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/analysis_radar.pdf}
    \caption{Randomly picked 100 samples from ACE and RichERE and categorized the most common errors.}
    \label{fig:error_cat}
    \vspace{-1em}
\end{figure}


\subsection{Fine-grained Analysis of Events}
%below is in  progress
Recent event extraction (EE) studies highlight the challenge of low-frequency event types (ETs), where data sparsity limits model generalization and degrades extraction performance, particularly for argument classification (AC) and argument identification (AI) \cite{Exploring the Feasibility of ChatGPT for Event Extraction}. To evaluate whether annotation guidelines can mitigate this issue, we compare the best-performing guideline approach without negative sampling (Guideline-PN w/o NS) against the NoGuideline w/o NS on the ACE test set to assess the impact of guidelines.

As shown in Fig.~\ref{fig:error_it_div}, frequent event types (left side of the figure) such as \textit{Attack} and \textit{Transport} show consistent improvements with annotation guidelines, as indicated by the stacked green bars, suggesting that even well-represented events benefit from enhanced annotations. The occurrence of red bars is minimal for medium-frequency events, indicating that guidelines are generally beneficial for these events as most of them are represented by green bars. In contrast, rare event types (right side) display a higher occurrence of red bars, reinforcing prior findings that guidelines alone may not fully resolve data sparsity issues. However, certain low-resource events, such as \textit{Convict}, show substantial gains, indicating that guidelines can still be beneficial in some underrepresented cases.

% \sriv{Fig.~\ref{fig:error_it_div} shows the ...}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{latex/figures/testtest.png}
    \caption{Effect of annotation guidelines on event predictions. The plot compares AC F1 scores on the test data for event extraction with (Guideline-P) and without (NoGuidelines) guidelines, where green (increase) and red (decrease) bars show guideline impact on AC performance. Frequent events (left) improve consistently, whereas rare events (right) show mixed effects, highlighting the frequency-dependent impact of guidelines.}
    % Below caption is for alternate image:
    %\caption{Effect of annotation guidelines on event predictions. The plot compares AC F1 scores on the test data for event extraction with (Guideline-P) and without (NoGuidelines) guidelines. Green bars indicate performance improvements, while red bars show declines due to guideline application. Frequent events (bottom) generally improve, whereas rare events (top) exhibit mixed effects, reinforcing the frequency-dependent impact of guidelines.}
    \label{fig:error_it_div}
\end{figure}


% Esp. look into the Justice Event for each guidelines, I am finding a lot of incorrect predictions here.


% \subsection{Hallucinations or Praphrasing?}
% \# Even after SFT, do they generate "new info" or simply paraphrase them or present them in a better way?

% \subsection{Qualitative Study of Guidelines}
% \# Study the guidelines manually and provide insights on what they bring and add additional details on how they can be utilized? IG already done because of Table 1.

