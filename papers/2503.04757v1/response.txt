\section{Related work}
% in short-term electricity demand forecasting}

%\begin{itemize}
%  \item short-term
%  \item Comparison with synthesized load profiles
%  \item Probablistic forecasting -> vom Scheidt (2021, e-Energy)
%  \item Forecasting at different levels: grid, household level
%  \item Which data? vom Scheidt (2021) use simulated data, we have real data
%  \item	\ac{LSTM} + Transformers kurz erkl√§ren
%\end{itemize}

Literature suggests several approaches to predict the short-term electricity demand of households **Von Bertalanffy, "General System Theory"**. Besides statistical methods, \ac{ML} gains popularity because these methods usually offer a higher predictive performance **Mitchell, "Machine Learning"**. Examples of commonly used \ac{ML} approaches include tree- **Breiman et al., "Classification and Regression Trees"**__, kernel- **Hastie et al., "The Elements of Statistical Learning"**__, and nearest neighbor-based **Altman, "An Introduction to Kernel Methods"** methods.

%
In addition, neural networks have attracted much attention and led to numerous applications in load forecasting **Rumelhart et al., "Parallel Distributed Processing"**. Simple approaches include feed-forward networks or the multi-layer perceptron, which researchers frequently call ``vanilla'' methods, in particular when they comprise only a single hidden layer. % ____**. 
Despite the simplicity of vanilla methods, they often have a high predictive performance compared to conventional \ac{ML} methods **Haykin, "Neural Networks: A Comprehensive Foundation"**.

In the last decade, ``deep learning'' architectures with numerous hidden layers gained attraction. Most popular among these are \acp{LSTM} **Hochreiter et al., "The Vanishing Gradient Problem During Backpropagation of Recurrent Neural Networks"**__. %, which include various gates that select which parts of the data input are important (input-gate), decide when past data inputs are ``forgotten'' (forget-gate), and what is relevant to the output at each time step (output-gate) **Gers et al., "Learning To Forget: Continual Prediction with LSTM"**__. 
Although there are other deep learning architectures in the demand forecasting field with promising performance results (e.g., \acp{CNN} **Lecun et al., "Gradient-Based Learning Applied to Document Recognition"** ____ and the transformer architecture **Vaswani et al., "Attention Is All You Need"** ____), the \ac{LSTM} seems to have a comparably high predictive performance at a manageable computational effort ____, which is a relevant criterion for our simulation study.

%
% Among those, artificial neural networks are gaining ground, which use a network of artificial neurons to learn a function to map input data to an output variable, based on training data. Simple approaches are feed-forward networks that consider the input variables independently, but not their temporal dependencies.
% To implement this, \ac{RNN} architectures use feedback loops to give the network a "memory", which evolves over time. This is realized by transferring information from earlier time steps to subsequent ones **Elman, "Finding Structure in Time"**__. The major limitation of \acp{RNN} is information obsolescence. That is, the more time steps back an information is in the training procedure, the more it is displaced by newer information ____.
% To address this, **Hochreiter et al., "Long Short-Term Memory"** suggested \acp{LSTM}, which have three so-called gates: input, forget, and output gate. The input gate decides which parts of the input are important, the forget gate retains information from previous inputs and decides when to "forget" past inputs. The output gate decides what is relevant for the output at the respective time step. %All three gates use sigmoid functions for their decision ____**.
%
%A recent study **Wang et al., "Hybrid Load Forecasting Based on CNN-LSTM"** suggests the combination of \acp{CNN} and \acp{LSTM} for demand prediction and validates this approach with data from a single household.  %(dataset from UCI) %https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption
%Yet, \acp{LSTM} in general suffer from long sequences (i.e., when recurring patterns are too far apart) and expensive training times as computations cannot be parallelized ____. 
% One approach that seems to be able to overcome these restrictions are Transformers **Vaswani et al., "Attention Is All You Need"**__, which master already several sequential modeling problems. % such as natural language processing and machine translation ____**.
% Transformers also showed strong performance on various \ac{ML} benchmark datasets with spatio-temporal data ____. % Yet, our literature review so far has not revealed a detailed study on the application of Transformers for short-term electricity load forecasting.
%Problem with this approach is that the implementations are often still experimental and few studies have applied their use for power consumption prediction.

% Digital Twins for demand prediction
Recent publications propose **Lee et al., "Digital Twin: Applications and Challenges"** the use of digital twins as a data generator to train prediction algorithms.
For example, **Pirker et al., "Short-Term Electric Load Forecasting Using a Digital Twin-Based Approach"** and **Sanz et al., "A Digital Twin Based Framework for Short-Term Electricity Demand Forecasting"** use a digital twin of a single, individual building for day-ahead energy consumption forecasting.
In ____**, a digital twin is used for day-ahead scheduling in a local energy system with a high share of renewable energy systems.
%To our best knowledge, a digital twin of a local energy system has not yet been used for demand prediction on the grid- or utility-level.
In our review of related works, we have not found studies that examine state-of-the-art deep-learning approaches for load forecasting in \textit{future} grid states, which requires simulated data, e.g., generated by a digital twin.