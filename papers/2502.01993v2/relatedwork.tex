\section{Related Work}
\vspace{-1mm}
\subsection{Acceleration of Flow Matching Models} 
\vspace{-1mm}
\citet{liu2022flow} proposed the Rectified Flow method, which straightens the flow trajectory to achieve high-quality results within a one sampling step, laying a solid theoretical foundation for subsequent research. InstaFlow~\cite{liu2023instaflow} applies the Reflow method to straighten the curved ODE solving path, allowing latents to transition more quickly from the noise distribution to the image distribution. The straightened ODE path also reduces the learning difficulty for the student model, improving the distillation effectiveness. This enables one-step generation for large-scale text-to-image tasks. PeRFlow~\cite{yan2024perflow} further improves Reflow correction by segmenting the flow trajectory, achieving exceptional performance. 

\vspace{-2mm}
\subsection{Diffusion-based Real-ISR}
\vspace{-1mm}
\textbf{Multi-step Diffusion-based Real-ISR.} In recent years, diffusion models have achieved remarkable success in the field of image super-resolution~\cite{wang2024exploiting, lin2023diffbir, yang2023pixel, yue2024resshift, wu2024seesr, yu2024scaling}. DiffBIR~\cite{lin2023diffbir} reconstructs low-resolution (LR) images using a small network and then employs ControlNet~\cite{zhang2023adding} to control the generation of the diffusion model. SeeSR~\cite{wu2024seesr} introduces a module for extracting semantic information from images. This module effectively guides the diffusion model's generation through semantic cues, preventing errors caused by image degradation. SUPIR~\cite{yu2024scaling} uses Restoration-Guided Sampling to ensure both generative capability and fidelity. It also leverages a large dataset and a large pre-trained diffusion model, SDXL~\cite{podell2023sdxl}, to enhance the model's performance.

\textbf{One-step Diffusion-based Real-ISR.} Recently, one-step diffusion ISR models have become a popular research direction, showing great potential and application value~\cite{wang2023sinsr, wu2024one, xie2024addsr, he2024one, dong2024tsd}. SinSR~\cite{wang2023sinsr} introduces a deterministic sampling method. It fixes the noise-image pair using consistency-preserving distillation. OSEDiff~\cite{wu2024one} employs Variational Score Distillation (VSD)~\cite{wang2024prolificdreamer, nguyen2024swiftbrush} and directly uses the low-resolution (LR) image as the starting point for diffusion inversion. In addition, OSEDiff uses DAPE~\cite{wu2024seesr} to extract semantic information from the LR image as the generation condition. ADDSR~\cite{xie2024addsr} combines adversarial training by introducing Adversarial Diffusion Distillation (ADD) and ControlNet to achieve both 4-step and one-step models. TSD-SR~\cite{dong2024tsd} proposes Target Score Distillation (TSD) and a Distribution-Aware Sampling Module (DASM), effectively addressing the issue of artifacts caused by VSD in the early stages of training.