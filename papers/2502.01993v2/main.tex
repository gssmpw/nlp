%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[arxiv]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Add by myself
% \usepackage{amsmath}
\usepackage{adjustbox}
% \usepackage{subfigure}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{float}
\usepackage{epsfig,epstopdf}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wrapfig, blindtext} % put figure inside text
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{bm} % for bold math
\usepackage{color,xcolor,colortbl}

\newcommand{\textrb}[1]{\textbf{\textcolor{red}{#1}}}
\newcommand{\textbb}[1]{\textbf{\textcolor{blue}{#1}}}

\newcommand{\textr}[1]{\textcolor{red}{#1}}
\newcommand{\textb}[1]{\textcolor{blue}{#1}}

% \definecolor{color-tab}{rgb}{0.95,0.95,0.95}

\definecolor{color-tab}{rgb}{0.9, 0.9, 0.9}  % 定义表头的背景色
\definecolor{color3}{gray}{0.95}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{One Diffusion Step to Real-World Super-Resolution via Flow Trajectory Distillation}

\begin{document}

\twocolumn[
\icmltitle{
% FluxSR: Flow Trajectory Distillation for \\ Single-Step Diffusion in Real-World Image Super-Resolution
One Diffusion Step to Real-World Super-Resolution via\\ Flow Trajectory Distillation
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
% \icmlsetsymbol{equal}{$^{\dagger}$}

\begin{icmlauthorlist}
\icmlauthor{Jianze Li}{equal,1}
\icmlauthor{Jiezhang Cao}{equal,2}
\icmlauthor{Yong Guo}{3}
\icmlauthor{Wenbo Li}{4}
\icmlauthor{Yulun Zhang$^{\dagger}$}{1}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{1}{Shanghai Jiao Tong University}
\icmlaffiliation{2}{Harvard University}
\icmlaffiliation{3}{South China University of Technology}
\icmlaffiliation{4}{Huawei Noah’s Ark Lab}

% \icmlcorrespondingauthor{Yulun Zhang}{yulun100@gmail.com}
\icmlcorrespondingauthor{Yulun Zhang$^{\dagger}$}{yulun100@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}


{\vspace{2em}%
%\newlength-4mm
%\setlength{-4mm}{-0.4cm}
\scriptsize
\centering
\scalebox{1.02}{
\hspace{-1.6mm}
\begin{tabular}{cccc}

\hspace{-0.46cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_12/lr_image_12_lr_826_137_371_495.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_12/lr_image_12_SinSR_826_137_371_495.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_12/lr_image_12_OSEDiff_826_137_371_495.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_12/lr_image_12_AddSR-s1_826_137_371_495.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_12/lr_image_12_TSD-SR_826_137_371_495.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_12/fluxsr_DIV2K_valid_826_137_371_495.png} \hspace{-4mm} 
\\
LR \hspace{-4mm} &
SinSR-s1 \hspace{-4mm} &
OSEDiff-s1 \hspace{-4mm} &
AddSR-s1 \hspace{-4mm} &
TSD-SR-s1 \hspace{-4mm} &
FluxSR-s1 (ours)\hspace{-4mm}
\\
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_94/lr_image_94_lr_355_236_412_561.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_94/lr_image_94_DiffBIR_355_236_412_561.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_94/lr_image_94_SeeSR_355_236_412_561.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_94/lr_image_94_ResShift_355_236_412_561.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_94/lr_image_94_AddSR-s4_355_236_412_561.png} \hspace{-4mm} &
\includegraphics[width=0.16\textwidth]{figs/teaser/lr_image_94/lr_image_94_DIV2K_valid_355_236_412_561.png} \hspace{-4mm}  
\\ 
LR \hspace{-4mm} &
DiffBIR-s50 \hspace{-4mm} &
SeeSR-s50  \hspace{-4mm} &
ResShift-s15 \hspace{-4mm} &
AddSR-s4  \hspace{-4mm} &
FluxSR-s1 (ours) \hspace{-4mm}
\\
\end{tabular}
\end{adjustbox}
\\

\end{tabular}
}
\captionof{figure}{
Visual comparisons of different Real-ISR methods. Top: Comparison between FluxSR and state-of-the-art one-step diffusion methods. Bottom: Comparison between FluxSR and state-of-the-art multi-step diffusion methods. Our proposed FluxSR generates more realistic images with high-frequency details.
\vspace{-0.75em}}%
\label{fig:show}%
}


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Diffusion models (DMs) have significantly advanced the development of real-world image super-resolution (Real-ISR), but the computational cost of multi-step diffusion models limits their application. One-step diffusion models generate high-quality images in a one sampling step, greatly reducing computational overhead and inference latency. However, most existing one-step diffusion methods are constrained by the performance of the teacher model, where poor teacher performance results in image artifacts. To address this limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique based on flow matching models. We use the state-of-the-art diffusion model FLUX.1-dev as both the teacher model and the base model. First, we introduce Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step Real-ISR. Second, to improve image realism and address high-frequency artifact issues in generated images, we propose TV-LPIPS as a perceptual loss and introduce Attention Diversification Loss (ADL) as a regularization term to reduce token similarity in transformer, thereby eliminating high-frequency artifacts. Comprehensive experiments demonstrate that our method outperforms existing one-step diffusion-based Real-ISR methods. The code and model will be released at \url{https://github.com/JianzeLi-114/FluxSR}.
\end{abstract}


\section{Introduction}
Real-world Image Super-Resolution (Real-ISR)~\cite{wang2020deep, wang2021realesrgan} aims to recover high-quality images from low-quality ones captured in real-world settings. Traditional image super-resolution~\cite{kim2016accurate, zhang2015image, dong2016image, dong2016accelerating, chen2023dual} assumes a known degradation process. However, this assumption does not account for the complex and unknown degradations present in real-world low-quality images~\cite{wang2021realesrgan}. Consequently, real-world super-resolution tasks are more challenging and practical. In recent years, they have attracted increasing attention from researchers.

Diffusion models~\cite{ho2020denoising, song2020score} are a type of generative model and initially designed for text-to-image (T2I) tasks. They have shown overwhelming advantages in many computer vision tasks~\cite{Rombach_2022_CVPR}. In recent years, numerous researchers have applied diffusion models to Real-ISR~\cite{wang2024exploiting, lin2023diffbir, yang2023pixel, yu2024scaling}. These applications have achieved unprecedented quality. These methods leverage the strong priors of pre-trained diffusion models, making the generated images exhibit more realistic details. Very recently, a lot of efforts have been made to investigate the scaling law of diffusion models~\cite{henighan2020scaling, yu2024scaling, tian2024visual} for image generation. Interestingly, a large model, e.g., Flux~\cite{flux2023} with 12B parameters, is able to significantly improve the visual quality and photo-realism, compared to those small diffusion models~\cite{rombach2022high, podell2023sdxl, esser2024scaling} with 1B$\sim$3B parameters. Nevertheless, such a large model still requires multiple steps for inference and becomes very computationally expensive, hindering its practical applications. Thus, how to reduce the number of steps to achieve efficient inference based on large diffusion models becomes an important problem.

To address this issue, many one-step distillation methods~\cite{wang2023sinsr, wu2024one, xie2024addsr, he2024one, dong2024tsd} could be useful. But they still suffer from several critical issues, particularly raised by the \textbf{\it generative distribution shift issue} and the training difficulty of \textbf{\it very large model}. \textbf{First}, fine-tuning a well-trained T2I model on SR data may easily destroy the original noise-to-image mapping and thus incur a distribution shift, as shown in Figure~\ref{fig:ftd}. Note that recent large diffusion models often follow the flow matching strategy~\cite{esser2024scaling, flux2023} that explicitly learns the flow along the diffusion path. In other words, existing one-step methods may completely ignore the originally well-learned T2I flow when learning the target SR flow. As a result, existing one-step models tend to produce images with unexpected artifacts and degraded visual quality. \textbf{Second}, the memory footprint and training cost become extremely high or even infeasible when distilling a large student model from an additional teacher of at least the same model size. For example, we find that even a server with 8 A800-80GB GPUs cannot satisfy the memory requirement of this distillation if we directly apply the popular one-step distillation method OSEDiff~\cite{wu2024one} on top of Flux.1-dev~\cite{flux2023}.

In this paper, we propose a novel one-step diffusion model for Real-ISR, called FluxSR, with FLUX.1-dev as the base model. Specifically, our design comprises three main components: 1) We propose a Flow Trajectory Distillation (FTD) to address the generative distribution issue. The key idea is to build the relationship between the noise-to-image flow in T2I and LR-to-HR flow in SR based on the flow matching theory. Unlike existing methods, we explicitly keep the original T2I flow unchanged while learning the SR flow trajectory conditioned on it. This approach maximizes the preservation of the teacher model's generative capabilities, thereby enhancing the realism of the generated images. 2) We develop a large model friendly training strategy that does not rely on an extra teacher model to compute the distillation loss. Instead, we cast the knowledge of the teacher model into the noise-to-image flow in the T2I task. In this sense, we are able to generate a bunch of flow data in the offline mode and exclude the teacher model from training to save memory consumption. 3) We propose TV-LPIPS as a perceptual loss. By incorporating the idea of total variation (TV), this loss emphasizes the restoration of high-frequency components and reduces artifacts in the generated images. Moreover, we introduce the Attention Diversification Loss (ADL)~\cite{guo2023robustifying} that improves the diversity of different tokens in attention modules. We use it as a regularization term to address the repetitive patterns observed in the images.
Extensive experiments show that our FluxSR achieves remarkable performance and requires only one sampling step. Figure~\ref{fig:show} presents the visual results of our method. In summary, our contributions are as follows:
\vspace{-2mm}
\begin{itemize}
\vspace{-3mm}
\item We develop FluxSR, a one-step diffusion Real-ISR model based on FLUX.1-dev. To the best of our knowledge, this is the first one-step diffusion for Real-ISR based on a large model with over 12B parameters.
\vspace{-3mm}
\item We propose a Flow Trajectory Distillation (FTD) method that explicitly builds the relationship between the noise-to-image flow and LR-to-HR flow. With the noise-to-image flow unchanged, we are able to preserve the high photo-realism in the T2I model and effectively transfer it to the LR-to-HR flow for SR.
\vspace{-3mm}
\item To make the training feasible, we propose a large model friendly training strategy that excludes the extra teacher model from the training phase. Instead, we cast the knowledge from teacher into the noise-to-image flow and generate a bunch of them in the offline mode, to reduce both memory consumption and training cost.
\end{itemize}

\vspace{-2mm}
\section{Related Work}
\vspace{-1mm}
\subsection{Acceleration of Flow Matching Models} 
\vspace{-1mm}
\citet{liu2022flow} proposed the Rectified Flow method, which straightens the flow trajectory to achieve high-quality results within a one sampling step, laying a solid theoretical foundation for subsequent research. InstaFlow~\cite{liu2023instaflow} applies the Reflow method to straighten the curved ODE solving path, allowing latents to transition more quickly from the noise distribution to the image distribution. The straightened ODE path also reduces the learning difficulty for the student model, improving the distillation effectiveness. This enables one-step generation for large-scale text-to-image tasks. PeRFlow~\cite{yan2024perflow} further improves Reflow correction by segmenting the flow trajectory, achieving exceptional performance. 

\vspace{-2mm}
\subsection{Diffusion-based Real-ISR}
\vspace{-1mm}
\textbf{Multi-step Diffusion-based Real-ISR.} In recent years, diffusion models have achieved remarkable success in the field of image super-resolution~\cite{wang2024exploiting, lin2023diffbir, yang2023pixel, yue2024resshift, wu2024seesr, yu2024scaling}. DiffBIR~\cite{lin2023diffbir} reconstructs low-resolution (LR) images using a small network and then employs ControlNet~\cite{zhang2023adding} to control the generation of the diffusion model. SeeSR~\cite{wu2024seesr} introduces a module for extracting semantic information from images. This module effectively guides the diffusion model's generation through semantic cues, preventing errors caused by image degradation. SUPIR~\cite{yu2024scaling} uses Restoration-Guided Sampling to ensure both generative capability and fidelity. It also leverages a large dataset and a large pre-trained diffusion model, SDXL~\cite{podell2023sdxl}, to enhance the model's performance.

\textbf{One-step Diffusion-based Real-ISR.} Recently, one-step diffusion ISR models have become a popular research direction, showing great potential and application value~\cite{wang2023sinsr, wu2024one, xie2024addsr, he2024one, dong2024tsd}. SinSR~\cite{wang2023sinsr} introduces a deterministic sampling method. It fixes the noise-image pair using consistency-preserving distillation. OSEDiff~\cite{wu2024one} employs Variational Score Distillation (VSD)~\cite{wang2024prolificdreamer, nguyen2024swiftbrush} and directly uses the low-resolution (LR) image as the starting point for diffusion inversion. In addition, OSEDiff uses DAPE~\cite{wu2024seesr} to extract semantic information from the LR image as the generation condition. ADDSR~\cite{xie2024addsr} combines adversarial training by introducing Adversarial Diffusion Distillation (ADD) and ControlNet to achieve both 4-step and one-step models. TSD-SR~\cite{dong2024tsd} proposes Target Score Distillation (TSD) and a Distribution-Aware Sampling Module (DASM), effectively addressing the issue of artifacts caused by VSD in the early stages of training.


\section{Background}

\subsection{Flow Matching Models}

Given two data distributions $p_0$ and $p_1$, there exists a vector field $u_t$ that generates a probabilistic path $p_t$ transitioning from $p_0$ to $p_1$. In generative models, $p_0$ represents the data distribution, while $p_1$ is an easily accessible simple distribution, such as the standard normal distribution $\mathcal{N}(0, 1)$.

Following ~\citet{esser2024scaling}, we define the forward process as:
\begin{equation}
    x_t = a_t x_0 + b_t \epsilon, \quad \text{where  } \epsilon \sim \mathcal{N}(0, 1).
\end{equation}
The coefficients $a_t$ and $b_t$ satisfy $a_0 = 1$, $b_0 = 0$, $a_1 = 0$, and $b_1 = 1$. This defines a probabilistic path $p_t$ from $p_0$ to $p_1$. The transformed variable is given by:
\begin{equation}
    x_t' = u_t(x_t | \epsilon) = \frac{a_t'}{a_t}x_t - \epsilon b_t(\frac{a_t'}{a_t}-\frac{b_t'}{b_t}). 
    \label{eq:coondition v}
\end{equation}
Subsequently, the marginal vector field $u_t(x_t)$ is obtained using the conditional vector field $u_t(x_t | \epsilon)$ as follows:
\begin{equation}
    u_t(x_t) = \int u_t(x_t | \epsilon) \frac{p(x_t | \epsilon) p(\epsilon)}{p_t(x_t)} \, d\epsilon.
\end{equation}
Here, the marginal probability density $p_t(x_t)$ is defined by:
\begin{equation}
    p_t(x_t) = \int p_t(x_t | \epsilon) p(\epsilon) \, d\epsilon.
\end{equation}
Flow matching aims to train a vector field $v_\theta(x, t)$, parameterized by a deep neural network, to approximate the marginal vector field $u_t(x_t)$. Specifically, flow matching minimizes the following objective~\cite{lipman2022flow}:
\begin{equation}
    \mathcal{L}_\text{FM}(\theta) := \mathbb{E}_{t,\,p_t(x_t)} \| v_\theta(x_t, t) - u_t(x_t) \|^2.
\end{equation}
However, the expression for $u_t$ cannot be explicitly computed, making the direct optimization of the aforementioned loss challenging. \citet{lipman2022flow} proposed conditional flow matching, demonstrating that we can optimize the following equivalent yet more tractable objective by using $u_t(x_t|\epsilon)$:
\begin{equation}
\mathcal{L}_\text{CFM}(\theta) := \mathbb{E}_{t,\, p_t(x_t|\epsilon),\, p(\epsilon)} \left\| v_\theta(x_t, t) - u_t(x_t|\epsilon) \right\|^2.
\end{equation}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/ftd2.pdf}
% \fbox{\rule{0cm}{7cm} \rule{0.98\textwidth}{0cm}}
\vspace{-2mm}
\caption{Difference of exiting methods and our Flow Trajectory Distillation. (Left) Based on the pre-trained models from noise $\epsilon$ to images $x_0$, existing one-step diffusion models fine-tune the model from LR images to HR images $x_H$. It may lead to a distribution shift between the real data distribution (blue) and the generated distribution (orange). (Right) To bridge the mapping from LR image distribution (green) to real data distribution, we propose Flow Trajectory Distillation. We constrain $u_t^{SR}$ using the other two trajectories in the triangle, ensuring that the real data distribution (blue) does not shift.}
\label{fig:ftd}
\vspace{-2mm}
\end{figure*}

\vspace{-3mm}
\subsection{Flow Trajectories}
\vspace{-1mm}
In this paper, we consider the flow trajectory used in FLUX.1-dev, namely rectified flow (ReFlow)~\cite{liu2022flow}. This is a simple diffusion trajectory that defines the forward process as a straight path between the data distribution and the noise distribution~\cite{liu2022flow, esser2024scaling}, specifically:
\begin{equation}
x_t = (1 - t)x_0 + t\epsilon,
\end{equation}
where $x_0\sim p_0$, $\epsilon \sim p_1=\mathcal{N}(0, 1)$.

By substituting into Equation~\ref{eq:coondition v}, we obtain the conditional vector field of ReFlow:
\begin{equation}
u_t(x_t|\epsilon) = \frac{\epsilon - x_t}{1 - t} = \epsilon - x_0.
\end{equation}

Therefore, following~\cite{lipman2022flow, esser2024scaling}, the training objective of ReFlow is
\begin{equation}
\mathcal{L}_{\text{ReFlow}}(\theta) {=} \mathbb{E}_{t,\, p_t(x_t|\epsilon),\, p(\epsilon)} \left\| v_\theta(x_t, t) {-} (\epsilon {-} x_0) \right\|^2_2.
\end{equation}

Intuitively, the goal of ReFlow is to train the neural network $v_\theta(x_t, t)$ to predict the velocity from noise to data samples.


\section{Method}

\subsection{Flow Trajectory Distillation (FTD)}
Our goal is to distill a one-step diffusion super-resolution model from a pre-trained text-to-image (T2I) flow model. Most current one-step diffusion ISR methods directly fine-tune the pre-trained T2I model and incorporate modules such as VSD or GANs to improve performance~\cite{wu2024one, xie2024addsr, dong2024tsd}. Although these methods have achieved good results, they still face some challenges. As shown on the left side of Figure~\ref{fig:ftd}, the flow trajectory of the pre-trained T2I model is not aligned with that of the SR model. During fine-tuning, these methods have no mechanism to keep the diffusion endpoint distribution unchanged. In other words, the real data distribution (blue) in the figure shifts, converting to the generated distribution (orange). For large-scale T2I models, which have already fit the real data distribution well, fine-tuning them using the above methods could lead to negative outcomes.

Ideally, the resulting model serves as a mapping from the low-resolution (LR) image distribution $p_{L}$ (green distrubition in Figure~\ref{fig:ftd}) to the high-resolution (HR) image distribution $p_{0}$ (blue distrubition in Figure~\ref{fig:ftd}). 
% We aim for the student model $v_\theta$ to approximate the vector field $u_t^{SR}$ that governs the probability trajectory from $p_{L}$ to $p_{0}$.
We aim to fix the distribution of the vector field $u_t^{SR}$ at $x_0$ while modifying the distribution of the diffusion starting point (i.e., transitioning from the noise distribution to the LR image distribution as shown in Figure~\ref{fig:ftd}) by fine-tuning the T2I model. Therefore, we propose Flow Trajectory Distillation, which indirectly obtains $u_t^{SR}$ by fitting $u_t^L$, avoiding the shift in the real data distribution.


\textbf{Approximating the LR Image Distribution.} Inspired by DMD~\cite{yin2023one, yin2024improved}, we can learn the underlying distribution of the training data by training a diffusion model. For flow matching models, training on LR data allows us to obtain parameters $v_\phi$, which fit the vector field $u_t^L$ that maps the noise distribution to the LR image distribution. The corresponding conditional flow trajectory is given by:
\begin{equation}
x_t = (1 - t)x_{L} + t\epsilon, 
\end{equation}
where $x_{L} \sim p_{L}$, $\epsilon \sim \mathcal{N}(0, 1)$ and $t \in [0,1]$. The velocity of a sample $x_t$ at time $t$ is given by $v_\phi(x_t, t)$.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/model_arc_v2.pdf}
% \fbox{\rule{0cm}{7cm} \rule{0.98\textwidth}{0cm}}
\vspace{-4mm}
\caption{Training framework of FluxSR. (Top) Multi-step inference process of the pre-trained FLUX model. (Middle) Training strategy of FluxSR. (Bottom) Computation process of FTD. We distill a one-step super-resolution model from the multi-step FLUX model, without the need for the teacher model to be involved online during training.}
\label{fig:training_framework}
\vspace{-4mm}
\end{figure*}

\textbf{Computing the LR-to-HR Flow from Noise-to-Image Flow.} At this point, we have obtained the flow model $v_\phi$ that maps from noise to low-resolution (LR) images and the flow model $v_{\text{real}}$ that maps from noise to real-world high-resolution (HR) images (the pre-trained T2I model). Given the linearity of the ReFlow trajectory, we can easily derive the flow model $v_\theta$ for mapping LR images to HR images. We have:
\begin{equation}
x_0 = \epsilon - u_t, \quad x_L = \epsilon - u_t^L.
\label{eq:v}
\end{equation}

Here, $v_{\text{real}}$ and $v_\phi$ parameterize $u_t$ and $u_t^L$, respectively. By combining the above equations, we obtain the trajectory from $x_L$ to $x_0$:
\begin{equation}
x_0 = x_L - (u_t - u_t^L).
\end{equation}


\subsection{Large Model Friendly Training Strategy}

Although we have derived the theoretical formulation of FTD, its practical application faces the following challenges: \textbf{\textit{i}) Inference Efficiency:} During inference, we need both the vector field $u_t$ calculated by the pre-trained T2I model and the vector field $u_t^L$ calculated by the model fine-tuned on LR data. This requires two different flow models with separate parameters, leading to significant computational overhead during inference. \textbf{\textit{ii}) Estimation Error:} Running the flow model in a one step makes it difficult to accurately estimate the velocity at time $t$. Without using a reconstruction loss to optimize the generator, the model performance may degrade. In this section, we propose an optimized training strategy to ensure that only a one flow model is required during inference. Additionally, we incorporate a reconstruction loss to enhance model performance.

\textbf{Direct Parameterization of $\bm{u_t^{SR}}$.} As shown on the left side of Figure~\ref{fig:training_framework}, since we can derive $u_t^{SR}$ from $u_t$ and $u_t^L$, we can also obtain $u_t^L$ from $u_t$ and $u_t^{SR}$. This avoids the issue caused by the inability to directly parameterize $u_t^{SR}$. We parameterize $u_t^{SR}$ using $v_\theta$. To represent both $u_t^L$ and $u_t^{SR}$ with a one model, we define the time step corresponding to the LR image as $T_L$ instead of 0. This ensures that the model represents only $u_t^L$ in the time range $[T_L,1]$ and only $u_t^{SR}$ at $T_L$. Additionally, the LR image distribution is more similar to the intermediate states $x_t$ of the pre-trained diffusion model. As shown in Figure~\ref{fig:training_framework}, similar to Eq.~\eqref{eq:v}, we have:
\begin{equation}
\left\{
\begin{aligned}
    x_0 &= \epsilon - u_t, \\
    x_L &= \epsilon - (1 - T_L) u_t^L, \\
    x_L - x_0 &= u_t^{SR} T_L.
\end{aligned}
\right.
\end{equation}
By combining the above equations, we obtain:
\begin{equation}
u_t^L = \frac{u_t - u_t^{SR} T_L}{1 - T_L}, \quad \text{where  } t \in [T_L, 1].
\end{equation}
The model parameterization can be expressed as:
\begin{equation}
    v_{\phi_t}(x_t, t) = \frac{u_t(x_t|\epsilon) - v_{\theta_t}(x_t, t) T_{L}}{1-T_{L}},
\label{eq:ftd}
\end{equation}
where
\begin{equation}
x_t = \frac{1 - t}{1 - T_L} x_L + \frac{t - T_L}{1 - T_L} \epsilon, \quad t \in [T_L, 1].
\label{eq:x_t}
\end{equation}
\textbf{Generating noise-to-image flow for distillation.} We precompute noise-sample pairs generated by FLUX and use them as training data, without relying on any real images. This approach offers two crucial benefits for large model training. 1) By using data pairs generated by the teacher model, we can directly compute $u_t(x_t|\epsilon) = \epsilon - x_0$, thus avoiding the estimation error during single-step inference. 2) The teacher model is not required for online inference during training, which significantly reduces GPU usage and training time, especially for large T2I models like FLUX. Using $v$-prediction, the loss function of FTD is given by:
\begin{align}
    &\mathcal{L}_\text{FTD}(\theta) = \mathbb{E}_{t,\, p_t(x_t|\epsilon),\, p(\epsilon)}\left\|(1-T_{L}) v_{\phi_t} -  (\epsilon - x_{L})\right\|^2_2 \notag \\
    = & \mathbb{E}_{t,\, p_t(x_t|\epsilon),\, p(\epsilon)}\left\|(u_t - v_\theta(x_t, t) T_{L}) -  (\epsilon - x_{L})\right\|^2_2
\label{eq:L_ftd}
\end{align}
where $u_t = \epsilon - x_0$, $t \in [T_L, 1]$. 

The generator $G_\theta$ can be expressed as:
\begin{equation}
G_\theta(x_L) = x_L - v_\theta(x_L, T_L) T_L.
\end{equation}

\subsection{Anti-artifacts Loss Functions.}
\label{subsec:artifacts}
\begin{figure}[t]
\vspace{1mm}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\linewidth]{figs/artifact/boxed_lr_image_11_fluxosd_tv_adl_21999it_cr.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\linewidth]{figs/artifact/lr_image_11_fluxosd_tv_adl_21999it_cr_cr.png} 
    \end{subfigure}
    \vspace{-4mm}
    \caption{Examples of Pronounced Periodic Artifacts During Training. Left: 256-pixel image with noticeable periodic high-frequency artifacts. Right: 64-pixel zoomed-in region, showing artifacts with four cycles in both width and height.}
    \label{fig:artifact}
    \vspace{-4mm}
\end{figure}

During training, we observe that the generator's predictions exhibit periodic high-frequency artifacts in the pixel space. As shown in Figure~\ref{fig:artifact}, the artifact period is 16 pixels, exactly the product of the VAE scaling factor (8) and the transformer patch size (2). This indicates that each token has similar components in certain dimensions. 
% To address this issue, we propose improvements from both the pixel domain and the feature representation.

\textbf{Improvement of Perceptual Loss.} 
We aim to reduce variations between adjacent pixels in flat regions to suppress high-frequency artifacts while preserving sharp edges. Inspired by the total variation (TV) loss, we propose TV-LPIPS as the perceptual loss for training. Specifically, TV-LPIPS is computed as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{TV}_{LPIPS}}(I, I_0) &= \mathcal{L}_\text{LPIPS}(I, I_0) \\
&\quad + \gamma \mathcal{L}_\text{LPIPS}(TV(I), TV(I_0)),
\end{aligned}
\label{eq:tvlpips}
\end{equation}
where
\begin{equation}
    TV(I_{i,j}) = (|I_{i+1,j}-I_{i,j}| + |I_{i,j+1}-I_{i,j}|).
\end{equation}
TV-LPIPS measures the degree of pixel variation and computes the LPIPS distance with the ground-truth. This not only prevents excessive variations between adjacent pixels in smooth regions but also enhances the LPIPS loss's sensitivity to high-frequency components. In summary, the reconstruction loss for training is given by:
\begin{equation}
\begin{aligned}
 \mathcal{L}_\text{Rec}(G_\theta(x_L), x_H) &= \mathcal{L}_\text{MSE}(G_\theta(x_L), x_H)\\
&\quad + \lambda\mathcal{L}_{\text{TV}_{LPIPS}}(G_\theta(x_L), x_H).
\end{aligned}
\end{equation}

\textbf{Attention Diversification Loss.} 
To address periodic artifacts at the feature level, we introduce the Attention Diversification Loss (ADL) proposed by \citet{guo2023robustifying}. ADL aims to reduce similarity between tokens and enhance attention diversity. We incorporate this loss to prevent different tokens from generating identical feature components.

To reduce computational complexity, ADL first approximates the overall cosine similarity by computing the cosine similarity between each token feature vector $A_i^{(l)}$ and the mean of all token feature vectors, defined as:
\begin{equation}
\bar{A}^{(l)} = \frac{1}{N} \sum_{i=1}^{N} A_i^{(l)}.
\end{equation}
Here, $A_i^{(l)}$ represents the $i$-th feature vector in the output of the $l$-th transformer layer. For a model with $L$ layers, ADL computes the mean ADL loss across all layers:
\begin{equation}
\mathcal{L}_{\text{ADL}} = \frac{1}{L} \sum_{l=1}^{L} \mathcal{L}_{\text{ADL}}^{(l)}, \,
% \end{equation}
% where, 
% \begin{equation}
\mathcal{L}_{\text{ADL}}^{(l)} = \frac{1}{N} \sum_{i=1}^{N} \frac{A_i^{(l)} \cdot \bar{A}^{(l)}}{\|A_i^{(l)}\| \|\bar{A}^{(l)}\|}.
\label{eq:adl}
\end{equation}

In summary, the overall training procedure of FluxSR is presented in Algorithm~\ref{alg:FluxSR}.

% \hspace*{-5.mm}
\begin{minipage}{\linewidth}
\vspace{-4.mm}
\begin{algorithm}[H]
\caption{FluxSR Training Procedure}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Pre-computed noise-image dataset $\mathcal{D} = \{\epsilon, x_0, z_0\}$. Pre-trained diffusion model $v_\psi$ and VAE encoder $E_\psi$, decoder $D_\psi$. Training iterations $N$.
\STATE \textbf{Output:} one-step generator $G_\theta$.
\STATE \textbf{Init:} $v_\theta \gets v_\psi$, $E_\theta \gets E_\psi$, $D_\theta \gets D_\psi$. Initialize: Trainable LoRA mounted on $v_\theta$.

\FOR{$i = 1$ to $N$}
    \STATE Sample $(\epsilon, x_0, z_0) \sim \mathcal{D}$.
    \STATE $u_t \gets \epsilon - z_0$
    \STATE // FTD Loss:
    \STATE Sample $t \in [T_L, 1]$.
    \STATE $\displaystyle x_t \gets \frac{1 - t}{1 - T_L} x_L + \frac{t - T_L}{1 - T_L} \epsilon$
    \STATE $\displaystyle v_{\phi_t}(z_t, t) \gets \frac{u_t - v_{\theta_t}(z_t, t) T_{L}}{1-T_{L}}$
    \STATE Compute $\mathcal{L}_{\text{FTD}}$ using Eq.~\eqref{eq:L_ftd}.
    \STATE // Reconstruction Loss:
    \STATE $\hat{z_0} \gets z_L - (v_\theta(z_L, T_L)) T_L$.
    \STATE $\hat{x_0} \gets D_\theta(\hat{z_0})$.
    \STATE Compute $\mathcal{L}_{\text{TV}_{LPIPS}}$ using Eq.~\eqref{eq:tvlpips}
    \STATE $\mathcal{L}_{\text{Rec}} = \mathcal{L}_{\text{MSE}} + \lambda \mathcal{L}_{\text{TV}_{LPIPS}}$.
    \STATE // ADL Loss:
    \STATE Compute $\mathcal{L}_{\text{ADL}}$ using Eq.~\eqref{eq:adl}.
    \STATE $\mathcal{L}(\theta) = \mathcal{L}_{\text{FTD}} + \mathcal{L}_{\text{Rec}} + \mu \mathcal{L}_{\text{ADL}}$
    \STATE Update $v_\theta$ using $\mathcal{L}(\theta)$.
\ENDFOR
\end{algorithmic}
\label{alg:FluxSR}
\end{algorithm}
\end{minipage}

\begin{table*}[t]
\centering
\scriptsize
\caption{
Quantitative results ($\times$4) on the Real-ISR testset with ground truth. The best and second-best results are colored \textcolor{red}{red} and \textcolor{blue}{blue}. In the one-step diffusion models, the best metric is \textbf{bolded}.
}
\vspace{-0.1in}
\begin{threeparttable}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{llcccccccc} % Center alignment for all numeric columns
\toprule
% \rowcolor{gray!20}
\rowcolor{color-tab}
\textbf{Model} & \textbf{Method} & \textbf{PSNR↑} & \textbf{SSIM↑}  & \textbf{LPIPS↓} & \textbf{DISTS↓} & \textbf{MUSIQ↑} & \textbf{MANIQA↑} & \textbf{TOPIQ↑} & \textbf{Q-Align↑}\\
\midrule
%RealSR
& StableSR-s200  & \textr{26.28} & \textr{0.7733} & \textr{0.2622} & \textr{0.1583} & 60.53 & 0.3706 & 0.5036 & 3.8789 \\
& DiffBIR-s50 & 24.87 & 0.6486 & 0.3834 & 0.2015 & 68.02 & 0.5287 & 0.6618 & 4.1244 \\
& SeeSR-s50 & \textb{26.20} & \textb{0.7555} & \textb{0.2806} & \textb{0.1784} & 66.37 & 0.5089 & 0.6565 & 3.9862 \\
& ResShift-s15 & 25.45 & 0.7246 & 0.3727 & 0.2344 & 56.18 & 0.3477 & 0.4420 & 3.8936 \\
& ADDSR-s4 & 23.15 &  0.6662 & 0.3769 & 0.2353 & 66.54 & \textr{0.6094} & \textr{0.7241} & \textb{4.1635} \\
\cdashline{2-10} 
\addlinespace[0.2em]
\textbf{RealSR} 
& SinSR-s1 & \textbf{25.83} & 0.7183 & 0.3641 & 0.2193 & 61.62 & 0.4255 & 0.5362 & 3.9237 \\
& OSEDiff-s1 & 24.57 & 0.7202 & 0.3036 & \textbf{0.1808} & 67.31 & 0.4775 & 0.6382 & 4.0646 \\
& ADDSR-s1 & 25.23 & \textbf{0.7295} & 0.2990 & 0.1852 & 63.08 & 0.4093 & 0.5685 & 3.9806 \\
& TSD-SR-s1 & 23.80 & 0.6987 & \textbf{0.2874} & 0.1843 & \textb{68.31} & 0.4899 & 0.6568 & 4.0926 \\
\cdashline{2-10} 
\addlinespace[0.3em]
&\textbf{FluxSR-s1} & 24.83 & 0.7175 & 0.3200 & 0.1910 & \textrb{68.95} & \textbb{0.5335} & \textbb{0.6699} & \textrb{4.3781} \\
%DIV2K-val
\cmidrule{1-10}
& StableSR-s200  & \textr{23.68} & \textr{0.6270} & 0.4167 & 0.2023 & 49.51 & 0.2696 & 0.3765 & 3.7427 \\
& DiffBIR-s50 & 22.33 & 0.5133  & 0.4681 & 0.1889 & \textr{70.07} & \textb{0.5471} & \textb{0.6958} & 4.2666 \\
& SeeSR-s50 & 23.21 & 0.6114  & 0.3477 & 0.1706 & 67.99 & 0.4687 & 0.6592 & \textb{4.4594} \\
& ResShift-s15 & \textb{23.55} &  0.6023 & 0.4088 & 0.2228 & 56.07 & 0.3409 & 0.4580 & 3.9961 \\
& ADDSR-s4 & 22.08 &  0.5578 & 0.4169 & 0.2145 & 68.26 & \textr{0.5496} & \textrb{0.7168} & 4.3910 \\
\cdashline{2-10} 
\addlinespace[0.2em]
\textbf{DIV2K-val} 
& SinSR-s1 & 22.55 & 0.5405 & 0.4390 & 0.2033 & 62.25 & 0.4241 & 0.5787 & 4.1712 \\
& OSEDiff-s1 & \textbf{23.10} & 0.6127 & \textb{0.3447} & 0.1750 & 66.62 & 0.4115 & 0.5971 & 4.1366 \\
& ADDSR-s1 & 22.74 & 0.6007 & 0.3961 & 0.1974 & 62.08 & 0.3867 & 0.5817 & 4.2971 \\
& TSD-SR-s1 & 21.65 & 0.5546 & 0.3456 & \textrb{0.1530} &  68.65 & 0.4393 & 0.6415 & 4.1539 \\
\cdashline{2-10} 
\addlinespace[0.3em]
&\textbf{FluxSR-s1} & 22.30 & \textbb{0.6177} & \textrb{0.3397} & \textb{0.1634} & \textbb{68.72} & \textbf{0.4615} & \textbf{0.6426} &  \textrb{4.6128} \\
\bottomrule
\end{tabular}
}

\end{threeparttable}
\label{tab:compare}
\vspace{-4mm}
\end{table*}


\begin{table}[t]
\centering
\scriptsize
\caption{
Quantitative results ($\times$4) on RealSet65 testset. The best and second-best results are colored \textcolor{red}{red} and \textcolor{blue}{blue}. In the one-step diffusion models, the best metric is \textbf{bolded}.
}
\vspace{-0.1in}
\begin{threeparttable}
\resizebox{\linewidth}{!}{ 
\begin{tabular}{lcccc} % Center alignment for all numeric columns
\toprule
% \rowcolor{gray!20}
\rowcolor{color-tab}
\textbf{Method} & \textbf{MUSIQ↑} & \textbf{MANIQA↑} & \textbf{TOPIQ↑} & \textbf{Q-Align↑}\\
\midrule
%RealSet65
\cmidrule{1-5}
StableSR-s200  & 58.89 & 0.3535 & 0.4974 & 3.8093 \\
DiffBIR-s50 & \textr{71.23} & \textr{0.5682} & \textr{0.7015} & 4.1599 \\
SeeSR-s50 & 69.79 & 0.5030 & 0.6774 & 4.1172 \\ 
ResShift-s15 & 59.36 & 0.3622 & 0.4953 & 3.8942 \\
ADDSR-s4 & 68.97 & \textb{0.5613} & \textb{0.6971} & \textb{4.1672} \\
\cdashline{1-5}
\addlinespace[0.2em] 
SinSR-s1 & 64.22 & 0.4462 & 0.5947 & 4.0390 \\
OSEDiff-s1 & 69.04 & 0.4625 & 0.5969 & 4.1065 \\
ADDSR-s1 & 64.22 & 0.3947 & 0.5616 & 4.0806 \\
TSD-SR-s1 & 69.34 & 0.4893 & 0.6392 & 3.9936 \\
\cdashline{1-5}
\addlinespace[0.3em]
\textbf{FluxSR-s1} & \textbb{70.75} & \textbf{0.5495} & \textbf{0.6670} & \textrb{4.2134} \\
\bottomrule
\end{tabular}
}

\end{threeparttable}
\label{tab:compare_realset}
\vspace{-4mm}
\end{table}

\vspace{-2mm}
\section{Experiments}
\vspace{-1mm}

\subsection{Experimental Settings}
\vspace{-2mm}
\label{subsec:exp_set}
\textbf{Training Datasets.} Our method does not require any real datasets. We generate 2400 noise-image pairs of size 1024x1024 using FLUX.1-dev~\cite{flux2023} as training data. To obtain the corresponding low-resolution (LR) images, we use the degradation pipeline proposed by RealESRGAN~\cite{wang2021realesrgan}.

\textbf{Test Datasets.} We evaluate our model on the synthetic dataset DIV2K-val~\cite{agustsson2017ntire} and two real datasets: RealSR~\cite{cai2019realworld} and RealSet65~\cite{yue2024resshift}. From DIV2K-val, we use the RealESRGAN degradation pipeline to generate corresponding LR images. On the these datasets, we evaluate using full-size images to assess the model's performance in real-world scenarios.

\textbf{Compared Methods and Metrics.} We compare the performance of our model with other diffusion-based ISR models, including multi-step diffusion ISR models: StableSR~\cite{wang2024exploiting}, DiffBIR~\cite{lin2023diffbir}, SeeSR~\cite{wu2024seesr}, ResShift~\cite{yue2024resshift}, and AddSR~\cite{xie2024addsr}; and one-step diffusion ISR models: SinSR~\cite{wang2023sinsr}, OSEDiff~\cite{wu2024one}, and TSD-SR~\cite{dong2024tsd}. We evaluate our model and the aforementioned methods using 4 full-reference metrics: PSNR, SSIM, LIPIS~\cite{zhang2018unreasonable}, and DISTS~\cite{ding2020image}, as well as 4 no-reference metrics: MUSIQ~\cite{ke2021musiq}, MANIQA~\cite{yang2022maniqa}, TOPIQ~\cite{chen2024topiq}, and Q-Align~\cite{wu2023q}. PSNR and SSIM are computed on the Y channel in the YCbCr  space.

% \vspace{-2mm}
\subsection{Comparison with State-of-the-Art Methods}
\vspace{-1mm}

\textbf{Quantitative Comparisons.} 
Tables~\ref{tab:compare} and \ref{tab:compare_realset} presents a quantitative comparison between FluxSR and other diffusion-based Real-ISR methods. Among one-step methods, our approach achieves the best performance across all no-reference (NR) metrics on all test datasets. For FR metrics like PSNR and SSIM, recent studies have demonstrated that image fidelity and perceptual quality involve a trade-off. In the context of diffusion-based super-resolution methods, PSNR and SSIM have limited reference value. Compared to multi-step methods, FluxSR outperforms StableSR across all datasets. Against DiffBIR, SeeSR, and AddSR, FluxSR shows slightly lower performance in TOPIQ. Additionally, we provide further comparisons with non-diffusion-based methods in the supplementary material.

\textbf{Qualitative Comparisons.} Figure~\ref{fig:results} presents visual comparison between FluxSR and other methods. FluxSR is capable of generating realistic details under severe degradation.

\begin{figure*}[t]
%\newlength-4mm
%\setlength{-4mm}{-0.4cm}
\scriptsize
\centering
\scalebox{1.02}{
\hspace{-1.6mm}
\begin{tabular}{cccc}

% % one row
\hspace{-0.4cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{c}
\includegraphics[width=0.193\textwidth]{figs/visual/Canon_039_LR4/output_image_with_red_box_cr.png}
\\
\end{tabular}
\end{adjustbox}
\hspace{-0.46cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_X4_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_StableSR_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_DiffBIR_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_SeeSR_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_ResShift_216_510_281_158.png} \hspace{-4mm} &
\\
LR \hspace{-4mm} &
StableSR \hspace{-4mm} &
DiffBIR \hspace{-4mm} &
SeeSR \hspace{-4mm} &
ResShift \hspace{-4mm}
\\
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_SinSR_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_OSEDiff_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_AddSR_s1_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_TSD-SR_216_510_281_158.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Canon_039_LR4/Canon_039_LR4_fluxosd_tv_lpips_adl_21999it_cr.png} \hspace{-4mm}  
\\ 
SinSR \hspace{-4mm} &
OSEDiff \hspace{-4mm} &
AddSR-s1 \hspace{-4mm} &
TSD-SR  \hspace{-4mm} &
FluxSR (ours) \hspace{-4mm}
\\
\end{tabular}
\end{adjustbox}
\\


% % one row
\hspace{-0.4cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{c}
\includegraphics[width=0.193\textwidth]{figs/visual/Nikon_050_LR4/lr_image_with_red_box_cr.png}
\\
\end{tabular}
\end{adjustbox}
\hspace{-0.46cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_X4_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_DiffBIR_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_SeeSR_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_ResShift_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_AddSR-s4_1141_1577_575_323.png} \hspace{-4mm} &
\\
LR \hspace{-4mm} &
DiffBIR \hspace{-4mm} &
SeeSR \hspace{-4mm} &
ResShift \hspace{-4mm} &
AddSR-s4 \hspace{-4mm} 

\\
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_SinSR_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_OSEDiff_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_AddSR-s1_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_TSD-SR_1141_1577_575_323.png} \hspace{-4mm} &
\includegraphics[width=0.152\textwidth]{figs/visual/Nikon_050_LR4/Nikon_050_LR4_RealSR_1141_1577_575_323.png} \hspace{-4mm}  
\\ 
SinSR \hspace{-4mm} &
OSEDiff \hspace{-4mm} &
AddSR-s1 \hspace{-4mm} &
TSD-SR  \hspace{-4mm} &
FluxSR (ours) \hspace{-4mm}
\\
\end{tabular}
\end{adjustbox}
\\

\end{tabular}
}
\vspace{-3mm}
\caption{Visual comparisons ($\times$4) on Real-ISR task.}
\label{fig:results}
\vspace{-4mm}
\end{figure*}

For example, in the first row of Figure~\ref{fig:results}, which depicts the restoration of a coat image, DiffBIR, ResShift, and SinSR are affected by noise, resulting in artificial textures. Although AddSR and TSD-SR generate relatively sharp images, they fail to accurately restore the collar's design. In contrast, FluxSR reconstructs the collar in a way that closely resembles the real-world appearance. The second row of Figure~\ref{fig:results} demonstrates the restoration of numerical digits. FluxSR produces the most realistic result. While TSD-SR also approximately restores the digits, it suffers from Sinc noise, generating bright edges around the numbers.


\subsection{Ablation Study}

\begin{table}[t]
\centering
\scriptsize
\caption{Ablation study on FTD.}
\vspace{-2mm}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{1.6mm}
\begin{tabular}{lcccc} 
    \toprule[0.15em]
    \rowcolor{color3} Method & \textbf{PSNR↑}  & \textbf{MUSIQ↑} & \textbf{MANIQA↑} & \textbf{Q-Align↑} \\
    \midrule[0.15em]
    w/o FTD & \textbf{26.33} & 56.02 & 0.3775 & 3.5170 \\
    \addlinespace[0.2em]
    FTD (ours) & 24.67 & \textbf{67.84} & \textbf{0.5203} & \textbf{4.1473}  \\
    \bottomrule[0.15em]
\end{tabular}
}
% \vspace{-2mm} 
\label{table:ablation_ftd}
\vspace{-2mm}
\end{table}

\begin{table}[t]
\centering
\scriptsize
\caption{Ablation study on different loss functions.}
\vspace{-2mm}
\setlength{\tabcolsep}{0.5mm} 
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{S}{>{\centering\arraybackslash}c}

\begin{tabularx}{\columnwidth}{SSSS|*{4}{C}} 
    \toprule[0.15em]
    \rowcolor{color3} $\mathcal{L}_{\text{LPIPS}}$ & $\mathcal{L}_\text{TV-LPIPS}$ & $\mathcal{L}_{\text{EA-DISTS}}$ & $\mathcal{L}_{\text{ADL}}$ & \textbf{PSNR↑}  & \textbf{MUSIQ↑} & \textbf{MANIQA↑} & \textbf{Q-Align↑} \\
    \midrule[0.15em]
    \ding{51} &  &  &  & 23.10 & 64.55 & 0.4937 & 4.0515 \\
     & \ding{51} &  &  & 22.09 & 65.04 & 0.5113 & 4.0927 \\
     &  & \ding{51} &  & 23.67 & 64.83 & 0.5036 & 4.0003 \\
     &  & \ding{51} & \ding{51} & \textbf{24.72} & 67.13 & 0.5138 & 4.0691 \\
     & \ding{51} &  & \ding{51} & 24.67 & \textbf{67.84} & \textbf{0.5203} & \textbf{4.1473} \\
    \bottomrule[0.15em]
\end{tabularx}

% \vspace{-2mm}
\label{table:ablation_loss}
\vspace{-2mm}
\end{table}

In this section, we use RealSR as the test dataset. The training iterations are set to 30k. Other settings remain consistent with those mentioned in Sec.~\ref{subsec:exp_set}.

\textbf{Effectiveness of FTD loss.} To verify the effectiveness and of FTD, we compare it with training using only the reconstruction loss, as shown in Table~\ref{table:ablation_ftd}. Training the one-step flow model with only the reconstruction loss results in poor performance, failing to generate high-frequency details and exhibiting significant high-frequency artifacts. Using the proposed FTD loss does not disrupt the data distribution learned by the teacher model. It effectively restores high-frequency details and achieves a higher degree of realism.


\textbf{Effectiveness of ADL and TV-LPIPS.} To verify the effectiveness of ADL and the proposed TV-LPIPS loss, we conducted relevant ablation experiments to investigate the impact of each loss function component. We also included the use of EA-DISTS, proposed by DFOSD, as a perceptual loss. Table~\ref{table:ablation_loss} presents the experimental results, showing that using TV-LPIPS as a perceptual loss and ADL as a regularization term achieves the best performance. 


\vspace{-2mm}
\section{Conclusion and Limitation}

This paper proposes FluxSR, an efficient one-step Real-ISR model based on FLUX, the state-of-the-art T2I diffusion model. FluxSR leverages Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step super-resolution model. It is trained using noise-image pairs generated by a fixed multi-step model and does not require any real data. We employ TV-LPIPS and ADL to enhance high-frequency components in the generated images and reduce periodic artifacts. Our experiments demonstrate that FluxSR achieves unprecedented realism.

\textbf{Limitations.} Although FluxSR achieves strong performance, it has a large number of parameters and high computational cost. Moreover, we have not entirely eliminated the periodic artifacts mentioned in Section~\ref{subsec:artifacts}. In the future, we plan to apply model pruning techniques to compress the model and develop more effective algorithms to prevent periodic artifacts, aiming to achieve a lightweight yet high-performance Real-ISR model.


\bibliography{main}
\bibliographystyle{icml2025}



\end{document}

