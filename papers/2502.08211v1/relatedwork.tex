\section{Related Work}
\label{sec:headings}


\subsection{Data Curation for Web-crawled Datasets}

Recent research underscores the critical role of data curation in enhancing model performance with large-scale image-text datasets. Various studies focus on improving dataset quality through curation methods, such as enhancing the descriptiveness and cross-modal feature alignment of image-text pairs, and reducing redundancy \cite{radenovic2023filtering, nguyen2024improving, abbas2023semdedup}.

In a broader context, DataComp is a benchmark designed to evaluate the performance of multimodal models on large-scale, real-world datasets\cite{gadre2023datacomp}. Recent advancements in the DataComp benchmark highlight notable progress in data curation techniques. Yokoo et al.\cite{yokoo2023leveraging} advanced data filtering using image-text similarity and caption modification, achieving notable progress in the Filtering and Bring Your Own Data (BYOD) Tracks. Yu et al.\cite{yu2023devil} evaluated data selection criteria's impact on model performance, while Chen et al.\cite{chen2024data} introduced DataJuicer for managing large-scale datasets. Nguyen et al.\cite{nguyen2024improving} enhanced image captioning for better modality alignment, and Maini et al.\cite{maini2023t} presented T-MARS for improved visual representation by bypassing text feature learning. Additionally, significant contributions have been made in the areas of synthetic data, contrastive learning, image-text alignment, and few-shot learning\cite{chen2020simple, radford2021learning, jia2021scaling, li2022blip, alayrac2022flamingo}.


Despite the significant advancements in data curation achieved by Radenovic et al.\cite{radenovic2023filtering} and Nguyen et al.\cite{nguyen2024improving} through enhancing data relevance, existing automated filtering methods may still exclude valuable but less conventional data points or introduce biases by focusing too narrowly on specific aspects, potentially overlooking broader contextual information. 

\subsection{Ensemble Learning}
Ensemble learning, which combines multiple models to improve performance and generalization, has long been a foundational approach in machine learning. Classic methods such as Bagging\cite{breiman1996bagging} and Boosting \cite{freund1997decision} initially demonstrated how model aggregation could reduce variance and improve accuracy.

Afterwards, ensemble learning has been increasingly applied to specialized tasks. Zimek, Schubert, \& Kriegel \cite{zimek2012survey}  highlighted how ensemble methods enhance outlier detection by aggregating results from multiple models. Beluch et al.\cite{beluch2018power} demonstrated that ensemble-based uncertainty sampling can significantly improve efficiency in active learning. Rasmus et al. \cite{rasmus2015semi} demonstrated that ensemble techniques enhance semi-supervised learning by effectively utilizing both labeled and unlabeled data. Song et al.\cite{song2022learning} used ensemble methods to improve data quality by detecting and filtering noisy or mislabeled data.

These advancements illustrate how ensemble techniques refine data preprocessing and improve model inputs.