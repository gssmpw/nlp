\newcommand{\cods}[1]{\textcolor[RGB]{51,68,103}{#1}}
\newcommand{\codt}[1]{\textcolor[RGB]{156,77,93}{#1}}

\newcommand{\e}[2]{{#1}}
\newcommand{\tl}[2]{\begin{tabular}[c]{@{}c@{}} \cods{#1}\\\codt{#2}\end{tabular}}
\newcommand{\tln}[2]{\begin{tabular}[c]{@{}c@{}} #1\\#2\end{tabular}}

\newcommand{\tc}[2]{\cods{#1}&\codt{#2}}

\newcommand{\tlds}[2]{\begin{tabular}[c]{@{}c@{}} \cods{#1}\\\cods{(#2)}\end{tabular}}
\newcommand{\tldt}[2]{\begin{tabular}[c]{@{}c@{}} \codt{#1}\\\codt{(#2)}\end{tabular}}

\newcommand{\bestoo}[0]{\cellcolor[HTML]{FFEEEB}}
\newcommand{\besto}[0]{\cellcolor[HTML]{E7F2F5}}
\newcommand{\bestt}[0]{\cellcolor[HTML]{faf1d8}}


\setlength{\tabcolsep}{3pt}
\setlength{\fboxsep}{0pt}

\begin{table*}[ht!]
    % \scriptsize
    \tiny
    % \small
    \centering
    \begin{tabular}{c|cc|cc|cc|cc|cc|cc|cc|cc|cc||cc}
      \toprule
      & 
      \multicolumn{2}{c|}{\textbf{Digits}} &
      \multicolumn{2}{c|}{\textbf{RMNIST}} &
      \multicolumn{2}{c|}{\textbf{CIFAR \& STL}} &
      \multicolumn{2}{c|}{\textbf{VisDA}} &
      \multicolumn{2}{c|}{\textbf{Office-Home}} &
      \multicolumn{2}{c|}{\textbf{DomainNet}} &
      \multicolumn{2}{c|}{\textbf{VLCS}} &
      \multicolumn{2}{c|}{\textbf{PCAS}} &
      \multicolumn{2}{c||}{\textbf{TerraInc}} &
      \multicolumn{2}{c}{\textbf{Avg.}} 
      \\

      \cmidrule(lr){2-21}  

      &
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ 
      \\
        
      \midrule
      \midrule
      SL & 
      \tc{\e{97.7}{?}}{\e{56.0}{?}} &
      \tc{\e{99.2}{?}}{\e{62.4}{?}} &
      \tc{\e{88.2}{?}}{\e{65.7}{?}} &
      \tc{\e{86.8}{?}}{\e{37.7}{?}} &
      \tc{\e{66.4}{?}}{\e{36.9}{?}} &
      \tc{\e{45.6}{?}}{\e{ 9.9}{?}} &
      \tc{\e{79.9}{?}}{\e{56.9}{?}} &
      \tc{\e{89.5}{?}}{\e{47.3}{?}} &
      \tc{\e{93.6}{?}}{\e{14.9}{?}} &
      \tc{\e{83.0}{?}}{\e{43.0}{?}} \\

      \cmidrule(lr){1-21}
      \tln{NTL}{\cite{wang2021non}} & 
      \tc{\tlds{95.6}{-2.1}}{\tldt{12.2}{-43.8}} &
      \tc{\tlds{98.7}{-0.5}}{\tldt{12.3}{-50.1}} &
      \tc{\tlds{83.9}{-4.4}}{\tldt{ 9.9}{-55.8}} &
      \tc{\tlds{82.0}{-4.8}}{\tldt{10.9}{-26.8}} &
      \tc{\tlds{64.8}{-1.6}}{\tldt{32.4}{-4.5}} &
      \tc{\tlds{ 7.6}{-38.0}}{\tldt{ 1.4}{-8.6}} &
      \tc{\tlds{78.0}{-1.9}}{\tldt{27.1}{-29.8}} &
      \tc{\tlds{85.8}{-3.7}}{\tldt{18.0}{-29.2}} &
      \tc{\tlds{90.0}{-3.6}}{\tldt{ 8.8}{-6.1}} &
      \tc{\tlds{76.3}{-6.7}}{\tldt{14.8}{-28.3}} \\

      \cmidrule(lr){2-21}
      \tln{CUTI-domain}{\cite{wang2023model}} & 
      \tc{\tlds{97.0}{-0.8}}{\tldt{ 9.5}{-46.5}} &
      \tc{\tlds{99.2}{-0.1}}{\tldt{15.5}{-46.9}} &
      \tc{\tlds{85.1}{-3.2}}{\tldt{10.7}{-55.0}} &
      \tc{\tlds{85.3}{-1.5}}{\tldt{ 8.9}{-28.8}} &
      \tc{\besto\tlds{56.7}{-9.7}}{\besto\tldt{17.8}{-19.1}} &
      \tc{\tlds{14.0}{-31.7}}{\tldt{ 2.0}{-7.9}} &
      \tc{\besto\tlds{78.3}{-1.6}}{\besto\tldt{26.7}{-30.1}} &
      \tc{\tlds{88.4}{-1.1}}{\tldt{18.3}{-28.9}} &
      \tc{\besto\tlds{87.9}{-5.7}}{\besto\tldt{ 0.8}{-14.1}} &
      \tc{\besto\tlds{76.9}{-6.1}}{\besto\tldt{12.2}{-30.8}} \\

      \cmidrule(lr){2-21}
      \tln{H-NTL}{\cite{hong2024improving}} & 
      \tc{\tlds{97.5}{-0.2}}{\tldt{ 9.6}{-46.4}} &
      \tc{\besto\tlds{99.0}{-0.2}}{\besto\tldt{10.8}{-51.5}} &
      \tc{\besto\tlds{87.2}{-1.0}}{\besto\tldt{ 9.9}{-55.8}} &
      \tc{\besto\tlds{86.5}{-0.3}}{\besto\tldt{ 8.6}{-29.0}} &
      \tc{\tlds{51.1}{-15.2}}{\tldt{17.0}{-19.8}} &
      \tc{\besto\tlds{33.3}{-12.3}}{\besto\tldt{ 2.1}{-7.8}} &
      \tc{\tlds{79.2}{-0.8}}{\tldt{42.7}{-14.2}} &
      \tc{\tlds{89.1}{-0.3}}{\tldt{22.1}{-25.1}} &
      \tc{\tlds{88.4}{-5.2}}{\tldt{14.6}{-0.2}} &
      \tc{\tlds{79.0}{-4.0}}{\tldt{15.3}{-27.8}} \\

      \cmidrule(lr){2-21}
      \tln{SOPHON}{\cite{deng2024sophon}} & 
      \tc{\tlds{95.2}{-2.5}}{\tldt{ 9.9}{-46.1}} &
      \tc{\tlds{96.6}{-2.6}}{\tldt{38.8}{-23.6}} &
      \tc{\tlds{69.5}{-18.7}}{\tldt{24.8}{-40.9}} &
      \tc{\tlds{77.3}{-9.5}}{\tldt{10.9}{-26.8}} &
      \tc{\tlds{45.9}{-20.4}}{\tldt{17.6}{-19.3}} &
      \tc{\tlds{30.1}{-15.6}}{\tldt{ 2.5}{-7.4}} &
      \tc{\tlds{79.4}{-0.6}}{\tldt{29.5}{-27.4}} &
      \tc{\tlds{86.7}{-2.8}}{\tldt{21.6}{-25.7}} &
      \tc{\tlds{88.8}{-4.8}}{\tldt{ 7.1}{-7.7}} &
      \tc{\tlds{74.4}{-8.6}}{\tldt{18.1}{-25.0}} \\

      \cmidrule(lr){2-21}
      \tln{CUPI-domain}{\cite{wang2024say}} &  
      \tc{\besto\tlds{96.7}{-1.0}}{\besto\tldt{ 8.8}{-47.2}} &
      \tc{\tlds{98.8}{-0.4}}{\tldt{21.0}{-41.3}} &
      \tc{\tlds{86.0}{-2.3}}{\tldt{11.3}{-54.4}} &
      \tc{\tlds{84.6}{-2.2}}{\tldt{ 8.2}{-29.5}} &
      \tc{\tlds{11.6}{-54.7}}{\tldt{ 2.3}{-34.6}} &
      \tc{\tlds{ 0.8}{-44.9}}{\tldt{ 0.3}{-9.7}} &
      \tc{\tlds{77.5}{-2.5}}{\tldt{29.5}{-27.4}} &
      \tc{\besto\tlds{87.8}{-1.7}}{\besto\tldt{11.5}{-35.8}} &
      \tc{\tlds{82.4}{-11.1}}{\tldt{ 1.3}{-13.6}} &
      \tc{\tlds{69.6}{-13.4}}{\tldt{10.4}{-32.6}} \\

      \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Comparison of SL and 5 NTL methods on multiple datasets. We report the \cods{source-domain accuracy} (\textbf{SA}) (\%) in \cods{blue} and \codt{target-domain accuracy} (\textbf{TA}) (\%) in \codt{red}. The best results of overall performance (OA) are highlighted in \colorbox[HTML]{E7F2F5}{blue background}. The accuracy drop compared to the pre-trained model is shown in brackets. The average performance of 9 datasets are shown in the last column (\textbf{Avg.}).}
    \label{tab:tgt-spec}
    \vspace{-4mm}
  \end{table*}
  
  
  


% \vspace{-1mm}
\section{Benchmarking NTL}
\label{sec:exp}

The post-training robustness has not been well-evaluated in NTL, which motivates us to build a comprehensive benchmark.
In this section, we first demonstrate the framework of our \texttt{NTLBench} (\Cref{sec:ntlbench}). Then, we present main results by conducting our \texttt{NTLBench} (\Cref{sec:ntlbenchresults}), including the pretrained NTL performance on multiple datasets, and the robustness of NTL baselines against different attacks. 


\subsection{\texttt{NTLBench}}
\label{sec:ntlbench}

We propose the first NTL benchmark (\texttt{NTLBench}), which contains a standard and unified training and evaluation process. \texttt{NTLBench} supports 5 SOTA NTL methods, 9 datasets (more than 116 domain pairs), 5 network architectures families, and 15 post-training attacks from 3 attack settings, providing more than 40,000 experimental configurations. 

\paragraph{Datasets.} Our \texttt{NTLBench} is compatible with: Digits (5 domains)~\cite{deng2012mnist,hull1994database,netzer2011reading,ganin2016domain,roy2018effects}, RotatedMNIST (3 domains)~\cite{ghifary2015domain}, CIFAR and STL (2 domains)~\cite{krizhevsky2009learning,coates2011analysis}, VisDA (2 domains)~\cite{peng2017visda}, Office-Home (4 domains)~\cite{venkateswara2017deep}, DomainNet (6 domains)~\cite{peng2019moment}, VLCS (4 domains)~\cite{fang2013unbiased}, PCAS (4 domains)~\cite{li2017deeper}, and TerraInc (5 domains)~\cite{beery2018recognition}. Different domains in any dataset share the same label space, but have distribution shifts, thus being suitable for evaluating NTL methods.

\paragraph{NTL baselines.}
\texttt{NTLBench} involves all open-source NTL methods: NTL~\cite{wang2021non}, CUTI-domain~\cite{wang2023model}, H-NTL~\cite{hong2024improving}, SOPHON~\cite{deng2024sophon}, CUPI-domain~\cite{wang2024say}. Besides, we also add a vanilla supervised learning (SL) as a reference.

\paragraph{Network architecture.}
The proposed \texttt{NTLBench} is compatible with multiple network architectures, including but not limited to: 
VGG~\cite{simonyan2014very}, ResNet~\cite{he2016deep}, WideResNet~\cite{zagoruyko2016wide}, ViT~\cite{dosovitskiy2020image}, SwinT~\cite{liu2021swin}.

\paragraph{Threat I: source domain fine-tuning (SourceFT).} \textit{Attacking goal}: SourceFT tries to destroy the non-transferability by fine-tuning the NTL model using a small set of source domain data. \textit{Attacking method}: \texttt{NTLBench} involves 5 methods, including four basic fine-tuning strategies\footnote{\label{initfc}initFC: re-initialize the last full-connect (FC) layer. direct: no re-initialize. all: fine-tune the whole model. FC: fine-tune last FC.}: initFC-all, initFC-FC, direct-FC, direct-all~\cite{deng2024sophon} and the special designed attack for NTL: TransNTL~\cite{hong2024your}. 


\paragraph{Threat II: target domain fine-tuning (TargetFT).} \textit{Attacking goal}: TargetFT tries to directly use labeled target domain data to fine-tune the NTL model, thus recovering target domain performance. \textit{Attacking method}: \texttt{NTLBench} use 4 basic fine-tuning strategies\textsuperscript{\ref{initfc}} leveraged in~\cite{deng2024sophon} as attack methods: initFC-all, initFC-FC, direct-FC, direct-all.

\paragraph{Threat III: source-free domain adaptation (SFDA).} \textit{Attacking goal}: We introduce SFDA to test whether using unlabeled target domain data poses a threat to NTL. \textit{Attacking method}: \texttt{NTLBench} involves 6 SOTA SFDA methods: SHOT~\cite{liang2020we}, CoWA~\cite{lee2022confidence}, NRC~\cite{yang2021exploiting}, PLUE~\cite{litrico2023guiding}, AdaContrast~\cite{chen2022contrastive}, and DIFO~\cite{tang2024source}.

\paragraph{Evaluation metric.} For source domain, we use source domain accuracy (\textbf{SA}) to evaluate the performance. Higher SA means lower influence of non-transferability to the source domain utility.
For target domain, we use target domain accuracy (\textbf{TA}) to evaluate the performance. Lower TA means better performance of non-transferability.
Besides, we calculate the overall performance (denoted as \textbf{OA}) of an NTL method as: $\text{OA}=(\text{SA}+(100\%-\text{TA}))/2$, with higher OA representing better overall performance of an NTL method. These evaluation metrics are applicable for both non-transferability performance and robustness against different attacks.


\subsection{Main Results and Analysis.}
\label{sec:ntlbenchresults}

Due to the limited space, we present main results obtained from our \texttt{NTLBench}. 
We first show the key implementation details, and then we present and analyse of our results.

\paragraph{Implementation details.}
Briefly, in pre-training stage, we sequentially pair $i$-th and ($i$+1)-th domains within a dataset for training. Each domain is randomly split into 8:1:1 for training, validation, and testing. The results for each dataset are averaged across domain pairs. NTL methods and the reference SL method are pretrained by up to 50 epochs. We search suitable hyper-parameters for each method by setting 5 values around their original value and choose the best value according to the best OA on validation set. All the batch size, learning rate, and optimizer are follow their original implementations. Following the original NTL paper~\cite{wang2021non}, we use VGG-13 without batch-normalization. All input images are resize to 64$\times$64. 
In attack stage, we use 10\% amount of the training set to perform attack. All attack results we reported are run on CIFAR \& STL. Attack training is up to 50 epochs.
We run all experiments on RTX 4090 (24G).


\paragraph{Non-transferability performance.} The non-transferability performance are shown in \Cref{tab:tgt-spec}, where we compare 5 NTL methods and SL on 9 datasets. From the results, all NTL methods generally effectively degrade source-to-target generalization, leading to a significant drop in TA compared to SL. However, in more complex datasets such as Office-Home and DomainNet, existing NTL methods fail to achieve a satisfactory balance between maintaining SA and degrading TA, highlighting their limitations. From the \textbf{Avg.} column, CUTI-domain reaches the overall best performance.
% \label{sec:ntlbench1}

\paragraph{Post-training robustness.} 
For \textbf{SourceFT} attack (\Cref{tab:atk_src}), fine-tuning each NTL model by using basic fine-tuning strategies on 10\% source domain data cannot directly recover the source-to-target generalization. However, all NTL methods are fragile when facing the TransNTL attack. For \textbf{TargetFT} attack (\Cref{tab:atk_tgt_label}), all NTL methods cannot fully resist supervised fine-tuning attack by using target domain data. In particular, fine-tuning all parameters usually results in better attack effectiveness. For \textbf{SFDA} (\Cref{tab:atk_tgt_sfda}), although the target domain data are unlabeled, advanced source-free unsupervised domain adaptation, leveraging self-supervised strategies, can still partially recover target domain performance. All these results verify the fragility of existing NTL methods. 




\begin{table}[t!]
    % \scriptsize
    \tiny
    % \small
    \centering
    % \hspace{-2mm}
    \begin{tabular}{@{\hspace{4pt}}c@{\hspace{3pt}}|c@{\hspace{2pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{2pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}}

        
    \toprule
  
      & 
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{NTL}} &
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{CUTI}} &
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{H-NTL}} &
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{SOPHON}} &
      \multicolumn{2}{@{\hspace{3pt}}c}{\textbf{CUPI}}
      \\
  
      \cmidrule(lr){2-11}
  
      &
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ 
      
      \\
      \midrule
      \midrule
      Pre-train & 
      \tc{\e{83.9}{?}}{\e{ 9.9}{?}} &
      \tc{\e{85.1}{?}}{\e{10.6}{?}} &
      \tc{\e{87.2}{?}}{\e{ 9.9}{?}} &
      \tc{\e{69.5}{?}}{\e{24.8}{?}} &
      \tc{\e{86.0}{?}}{\e{11.3}{?}} \\
      \cmidrule(lr){1-11}
  
      initFC-all & 
      \tc{\tlds{84.0}{+0.2}}{\tldt{ 9.8}{-0.1}} &
      \tc{\tlds{84.2}{-0.9}}{\tldt{10.6}{+0.0}} &
      \tc{\tlds{87.8}{+0.6}}{\tldt{16.2}{+6.3}} &
      \tc{\tlds{82.2}{+12.7}}{\tldt{38.1}{+13.3}} &
      \tc{\tlds{85.3}{-0.7}}{\tldt{11.4}{+0.1}} \\
      \cmidrule(lr){2-11}
  
      initFC-FC & 
      \tc{\tlds{84.2}{+0.3}}{\tldt{10.0}{+0.1}} &
      \tc{\tlds{85.4}{+0.3}}{\tldt{10.6}{+0.0}} &
      \tc{\tlds{87.2}{-0.1}}{\tldt{10.2}{+0.3}} &
      \tc{\tlds{71.9}{+2.4}}{\tldt{23.3}{-1.6}} &
      \tc{\tlds{85.9}{-0.1}}{\tldt{11.3}{+0.0}} \\
      \cmidrule(lr){2-11}
  
      direct-FC & 
      \tc{\tlds{84.0}{+0.2}}{\tldt{ 9.9}{+0.0}} &
      \tc{\tlds{85.2}{+0.2}}{\tldt{10.6}{+0.0}} &
      \tc{\tlds{87.3}{+0.1}}{\tldt{ 9.9}{+0.0}} &
      \tc{\tlds{74.3}{+4.8}}{\tldt{23.8}{-1.1}} &
      \tc{\tlds{86.1}{+0.1}}{\tldt{11.3}{+0.0}} \\
      \cmidrule(lr){2-11}
  
      direct-all & 
      \tc{\tlds{84.7}{+0.8}}{\tldt{ 9.8}{-0.1}} &
      \tc{\tlds{85.3}{+0.3}}{\tldt{10.9}{+0.3}} &
      \tc{\tlds{88.0}{+0.8}}{\tldt{10.1}{+0.2}} &
      \tc{\tlds{83.4}{+13.9}}{\tldt{32.2}{+7.4}} &
      \tc{\tlds{85.5}{-0.5}}{\tldt{11.3}{+0.0}} \\
      \cmidrule(lr){2-11}
  
      TransNTL & 
      \tc{\bestoo \tlds{81.7}{-2.2}}{\bestoo \tldt{44.3}{+34.4}} &
      \tc{\bestoo \tlds{81.3}{-3.8}}{\bestoo \tldt{61.0}{+50.3}} &
      \tc{\bestoo \tlds{86.3}{-1.0}}{\bestoo \tldt{63.7}{+53.8}} &
      \tc{\bestoo \tlds{83.8}{+14.3}}{\bestoo \tldt{60.1}{+35.3}} &
      \tc{\bestoo \tlds{83.1}{-2.9}}{\bestoo \tldt{60.6}{+49.3}} \\
  
      \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{NTL robustness against source domain fine-tuning (Source-\\FT). We show \cods{source-domain accuracy} (\textbf{SA}) (\%) and \codt{target-domain accuracy} (\textbf{TA}) (\%). The most serious threat (worst OA) to each NTL is marked as\colorbox[HTML]{fee8e4}{ red.} Accuracy drop from the pre-trained model is in ($\cdot$).}
    \label{tab:atk_src}
    \vspace{1mm}
    \begin{tabular}{@{\hspace{4pt}}c@{\hspace{3pt}}|c@{\hspace{2pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{2pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}}
      \toprule
    
        & 
        \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{NTL}} &
        \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{CUTI}} &
        \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{H-NTL}} &
        \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{SOPHON}} &
        \multicolumn{2}{@{\hspace{3pt}}c}{\textbf{CUPI}}
        \\
    
        \cmidrule(lr){2-11}
    
        &
        \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
        \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
        \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
        \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
        \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ 
        
        \\
        \midrule
        \midrule
        Pre-train & 
        \tc{\e{83.9}{?}}{\e{ 9.9}{?}} &
        \tc{\e{85.1}{?}}{\e{10.7}{?}} &
        \tc{\e{87.2}{?}}{\e{ 9.9}{?}} &
        \tc{\e{69.5}{?}}{\e{24.8}{?}} &
        \tc{\e{86.0}{?}}{\e{11.3}{?}} \\
        \cmidrule(lr){1-11}
    
        initFC-all & 
        \tc{\bestoo \tlds{23.9}{-60.0}}{\bestoo \tldt{37.8}{+27.9}} &
        \tc{\bestoo \tlds{13.3}{-71.8}}{\bestoo \tldt{15.9}{+5.3}} &
        \tc{\tlds{19.0}{-68.3}}{\tldt{10.4}{+0.5}} &
        \tc{\tlds{59.0}{-10.5}}{\tldt{68.5}{+43.7}} &
        \tc{\tlds{41.2}{-44.8}}{\tldt{53.1}{+41.8}} \\
        \cmidrule(lr){2-11}
    
        initFC-FC & 
        \tc{\tlds{33.9}{-50.0}}{\tldt{ 9.6}{-0.4}} &
        \tc{\tlds{30.2}{-54.9}}{\tldt{ 9.7}{-1.0}} &
        \tc{\tlds{19.1}{-68.1}}{\tldt{ 9.7}{-0.2}} &
        \tc{\tlds{21.6}{-48.0}}{\tldt{16.8}{-8.1}} &
        \tc{\tlds{21.8}{-64.2}}{\tldt{12.1}{+0.8}} \\
        \cmidrule(lr){2-11}
    
        direct-FC & 
        \tc{\tlds{64.2}{-19.7}}{\tldt{10.2}{+0.3}} &
        \tc{\tlds{38.0}{-47.1}}{\tldt{10.6}{-0.1}} &
        \tc{\tlds{87.1}{-0.1}}{\tldt{10.0}{+0.1}} &
        \tc{\tlds{70.5}{+1.0}}{\tldt{24.5}{-0.4}} &
        \tc{\tlds{78.6}{-7.4}}{\tldt{11.0}{-0.4}} \\
        \cmidrule(lr){2-11}
    
        direct-all & 
        \tc{\tlds{13.9}{-70.0}}{\tldt{17.6}{+7.7}} &
        \tc{\tlds{10.1}{-75.0}}{\tldt{ 8.8}{-1.9}} &
        \tc{\bestoo \tlds{84.7}{-2.5}}{\bestoo \tldt{53.3}{+43.4}} &
        \tc{\bestoo \tlds{68.0}{-1.6}}{\bestoo \tldt{72.9}{+48.1}} &
        \tc{\bestoo \tlds{51.9}{-34.1}}{\bestoo \tldt{58.4}{+47.1}} \\
    
        \bottomrule
      \end{tabular}
      \vspace{-3mm}
      \caption{NTL robustness against target domain fine-tuning (Target-\\FT).  We report \cods{source-domain accuracy} (\textbf{SA}) (\%) and \codt{target-domain accuracy} (\textbf{TA}) (\%). The most serious threat (best TA) to each NTL is marked as\colorbox[HTML]{fee8e4}{ red.} Accuracy drop from the pre-trained model is in ($\cdot$).}
      \vspace{-3mm}
      \label{tab:atk_tgt_label}
  \end{table}
  
\paragraph{More results.} 

Additional results and analysis on: various architectures, attack using different data amount, cross-domain/task, and visualizations (e.g., feature activation, t-SNE \cite{van2008visualizing}, GradCAM \cite{selvaraju2017grad}) will be released soon at our online page.
  
  
  \begin{table}[t!]
    % \scriptsize
    \tiny
    % \small
    \centering
    % \hspace{-2mm}
    \begin{tabular}{@{\hspace{4pt}}c@{\hspace{3pt}}|c@{\hspace{2pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{2pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}c@{\hspace{3pt}}}
    \toprule
  
      & 
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{NTL}} &
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{CUTI}} &
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{H-NTL}} &
      \multicolumn{2}{c|@{\hspace{3pt}}}{\textbf{SOPHON}} &
      \multicolumn{2}{@{\hspace{3pt}}c}{\textbf{CUPI}}
      \\
  
      \cmidrule(lr){2-11}
  
      &
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ & 
      \textbf{SA} $\uparrow$ & \textbf{TA} $\downarrow$ 
      
      \\
      \midrule
      \midrule
      Pre-train & 
      \tc{\e{83.9}{?}}{\e{ 9.9}{?}} &
      \tc{\e{85.1}{?}}{\e{10.7}{?}} &
      \tc{\e{87.2}{?}}{\e{ 9.9}{?}} &
      \tc{\e{69.5}{?}}{\e{24.8}{?}} &
      \tc{\e{85.5}{?}}{\e{11.3}{?}} \\
      \cmidrule(lr){1-11}
  
      SHOT & 
      \tc{\tlds{63.0}{-20.9}}{\tldt{29.6}{+19.7}} &
      \tc{\tlds{35.3}{-49.8}}{\tldt{34.7}{+24.0}} &
      \tc{\tlds{86.6}{-0.6}}{\tldt{41.9}{+32.0}} &
      \tc{\bestoo \tlds{64.8}{-4.8}}{\bestoo \tldt{56.7}{+31.9}} &
      \tc{\tlds{85.8}{+0.3}}{\tldt{11.3}{+0.0}} \\
      \cmidrule(lr){2-11}
  
      CoWA & 
      \tc{\tlds{81.1}{-2.8}}{\tldt{12.4}{+2.5}} &
      \tc{\tlds{84.0}{-1.1}}{\tldt{12.7}{+2.1}} &
      \tc{\tlds{87.2}{+0.0}}{\tldt{10.1}{+0.2}} &
      \tc{\tlds{69.2}{-0.4}}{\tldt{26.1}{+1.3}} &
      \tc{\tlds{85.7}{+0.2}}{\tldt{11.3}{+0.0}} \\
      \cmidrule(lr){2-11}
  
      NRC & 
      \tc{\tlds{57.7}{-26.2}}{\tldt{19.8}{+9.9}} &
      \tc{\tlds{39.4}{-45.7}}{\tldt{35.5}{+24.8}} &
      \tc{\tlds{87.3}{+0.1}}{\tldt{12.1}{+2.2}} &
      \tc{\tlds{66.6}{-3.0}}{\tldt{55.6}{+30.8}} &
      \tc{\tlds{86.0}{+0.5}}{\tldt{12.2}{+0.9}} \\
      \cmidrule(lr){2-11}
  
      PLUE &  
      \tc{\bestoo \tlds{71.5}{-12.4}}{\bestoo \tldt{52.8}{+42.9}} &
      \tc{\bestoo \tlds{76.1}{-9.0}}{\bestoo \tldt{63.8}{+53.1}} &
      \tc{\tlds{85.5}{-1.8}}{\tldt{20.1}{+10.2}} &
      \tc{\tlds{75.5}{+6.0}}{\tldt{41.1}{+16.3}} &
      \tc{\bestoo \tlds{82.4}{-3.2}}{\bestoo \tldt{43.6}{+32.3}} \\
      \cmidrule(lr){2-11}
  
      \tln{Ada-}{Contrast} & 
      \tc{\tlds{ 9.4}{-74.5}}{\tldt{ 9.8}{-0.1}} &
      \tc{\tlds{ 9.3}{-75.8}}{\tldt{10.0}{-0.7}} &
      \tc{\tlds{86.3}{-1.0}}{\tldt{12.1}{+2.2}} &
      \tc{\tlds{64.5}{-5.1}}{\tldt{33.4}{+8.6}} &
      \tc{\tlds{47.2}{-38.3}}{\tldt{11.3}{+0.0}} \\
      \cmidrule(lr){2-11}
  
      DIFO & 
      \tc{\tlds{ 9.2}{-74.7}}{\tldt{ 9.2}{-0.7}} &
      \tc{\tlds{ 9.2}{-75.9}}{\tldt{ 9.2}{-1.5}} &
      \tc{\bestoo \tlds{85.0}{-2.2}}{\bestoo \tldt{42.1}{+32.2}} &
      \tc{\tlds{56.3}{-13.2}}{\tldt{51.3}{+26.5}} &
      \tc{\tlds{48.4}{-37.1}}{\tldt{10.4}{-1.0}} \\
  
      \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{NTL robustness against source-free domain adaptation (SFDA). We show \cods{source-domain accuracy} (\textbf{SA}) (\%), \codt{target-domain accuracy} (\textbf{TA}) (\%), and accuracy drop from the pre-trained model is in ($\cdot$). The most serious threat (highest TA) to each NTL is in\colorbox[HTML]{fee8e4}{ red.}}
    \vspace{-2.5mm}
    \label{tab:atk_tgt_sfda}
  \end{table}
  
  