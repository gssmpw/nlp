

\section{Approaches for NTL}
\label{sec:immeNTL}

Target-specified NTL approaches contain fundamental solutions for NTL, and thus, we first review them in \Cref{sec:target-specified}. Then, in \Cref{sec:source-only}, we review how existing works implement source-only NTL in the absence of a target domain.


\subsection{Target-Specified NTL}
\label{sec:target-specified}

Briefly, in target-specified setting, the target domain is known and we aim to restrict the generalization of a deep learning model from the source domain toward the certain target domain. 
Existing methods perform target-domain regularization either on the feature space or the output space, as we summarized in \Cref{tab:summary} (\textbf{Non-Transferable Approach} column). For more details, we introduce existing strategies as follows:

\paragraph{Output space regularization.} Output-space regularizations directly manipulate the model logits on the target domain. More specifically, these operations can be categorized into \textit{untargeted regularization} and \textit{targeted regularization}. \textit{Untargeted regularization} \cite{wang2021non,wang2023model,zhou2024archlock,peng2024map} could usually be formalized as a maximizing optimization problem, where existing methods implement this regularization by maximizing the KL divergence between the model outputs and the real labels, thus disturbing the model predictions on the target domain. However, such untargeted regularizations may face convergence issues \cite{deng2024sophon}. \textit{Targeted regularization} \cite{wang2023domain,deng2024sophon} found a proxy task on the target domain (i.e., modify the labels), thus converting the maximization objective in untargeted regularization to a minimization optimization problem. 
DSO \cite{wang2023domain} transforms the correct labels to error labels without overlap (e.g., $y_{\text{err}} = y+1$) and uses error labels as the target-domain supervision. 
H-NTL \cite{hong2024improving} first disentangle the content and style factors via a variation inference framework~\cite{blei2017variational,yao2021instance,von2021self,lin2024cs,lin2025learning}, and then, they learn the NTL model by fitting the contents of the source domain and the style of the target domain. Due to the assumption that the style is approximately to be independent to the content representations, the non-transferability could be implemented.
SOPHON \cite{deng2024sophon} aims at both image classification and generation tasks. For classification, they propose to modify the cross-entropy (CE) loss to its inverse version (i.e., modify the label $y$ to $1-y$) or calculate the KL divergence between the model outputs and a uniform distribution. For generation, SOPHON proposes to use a Denial of Service (DoS) loss, i.e., let the diffusion model fit a zero matrix at each step. Compared to untargeted regularizations, targeted regularizations always have better convergence.

\paragraph{Feature space regularization.} Feature-space regularizations further reduce the similarity between feature representations from different domains, thus restricting the transferability directly on the feature space. Feature-space regularizations can also be categorized into \textit{untargeted} and \textit{targeted} strategies, depending on whether they directly enlarge the distribution gap through a maximization objective or convert it to a minimization problem by finding a proxy target. For \textit{untargeted regularization}, existing methods \cite{wang2021non,zeng2022unsupervised} propose to maximize the maximum mean discrepancy (MMD) loss between the feature representations from different domains, where MMD measures the distribution discrepancy. 
For \textit{targeted regularization}, UNTL \cite{zeng2022unsupervised} proposes to build an auxiliary domain classifier with feature representations from different domains as inputs. By minimizing the domain-classification loss, the domain classifier could help the NTL model learn domain-distinct representations. 
NTP \cite{ding2024non} aims to minimize the Fisher Discriminant Analysis (FDA) term \cite{shao2022not} in the target domain. Specifically, a smaller FDA value indicates a reduced difference in class means and increased feature variance within each class, which is associated with poorer target domain performance.



\subsection{Source-Only NTL}
\label{sec:source-only}

Under the assumption that only source domain data is available, existing works~\cite{wang2021non,wang2023model,wang2023domain,hong2024improving} take various data augmentation methods to obtain auxiliary domains from the source domain and see them as the target domain. 
Thus, the source-only NTL problem can be solved by target-specified NTL approaches. 
These augmentation methods can be split into the following three categories: 


\paragraph{Adversarial domain generation.} 
Wang \textit{et al.} \shortcite{wang2021non} use generative adversarial network (GAN) \cite{mirza2014conditional,chen2016infogan} 
to synthesize fake images from the source domain and see them as the target domain.
They train the GAN by controlling the distance and direction of the synthetic distributions to the real source domain, thus enhancing the diversity of synthetic samples and improving the degradation of any distribution with shifts to the real source domain. 
CUTI-domain \cite{wang2023model} and CUPI-domain \cite{wang2024say} add Gaussian noise to the GAN-based adaptive instance normalization (AdaIN) \cite{huang2017arbitrary} to obtain synthetic samples with random styles. They use both the synthetic samples from AdaIN and Wang \textit{et al.} \shortcite{wang2021non} as the target domain. 
MAP \cite{peng2024map} also follows the GAN framework.
 They additionally add a mutual information (MI) minimization term to enhance the variation between synthetic samples and the real source domain samples, ensuring more distinct style features.

\paragraph{Strong image augmentation.} H-NTL \cite{hong2024improving} conducts strong image augmentation \cite{sohn2020fixmatch,cubuk2020randaugment,huang2023harnessing} on real source domain data.
Strong image augmentations (e.g., blurring, sharpness, solarize) do not influence the contents but significantly change the image styles, thus imposing interventions~\cite{von2021self} on the style factors in images. Then, all augmented images are treated as the target domain for training source-only NTL.

\paragraph{Perturbation-based method.} DSO \cite{wang2023domain} proposes to minimize the worst-case risk on the uncertainty set~\cite{sagawa2019distributionally,huang2023robust,wang2023defending} over the source domain distribution, where the risk is empirically calculated through a classification loss between the model predictions and the error label.

% \vspace{-1mm}
\section{Post-Training Robustness of NTL}
\label{sec:robustness}

NTL models are expected to keep the non-transferability after malicious attacks.
However, not all existing works evaluate the robustness of their method, as we listed in \Cref{tab:summary} (the last column). 
In this section, we review the robustness of the source and target domains as considered in previous works.

\subsection{Robustness Against Source Domain Attack} 
\label{sec:robustness_src}

Earlier evaluations in \cite{wang2021non,wang2023model} show that NTL models are still resistant to state-of-art watermark removal attacks when up to 30\% source domain data are available for attack. Hong \textit{et al.} \shortcite{hong2024your} further investigate the robustness of NTL and propose TransNTL, demonstrating that non-transferability can be destroyed using less than 10\% of the source domain data. Specifically,
they find NTL \cite{wang2021non} and CUTI-domain \cite{wang2023model} inevitably result in significant generalization impairments on slightly perturbed source domains \cite{hendrycks2019benchmarking,cubuk2020randaugment}. Accordingly, they propose TransNTL to fine-tune the NTL model under an impairment repair self-distillation framework, where the source-domain predictions are used to teach the model itself how to predict on perturbed source domains. As a result, the fine-tuned model is just like a SL model without the non-transferability. Furthermore, they also propose a defense method to fix this loophole by pre-repairing the generalization impairments in perturbed source domains.
Specifically, they add a defense regularization term on existing NTL and CUT-domain training. Minimizing the defense regularization term enables the NTL model to exhibit source-domain consistent behaviors on perturbed source-domain data, thus enhancing the robustness against TransNTL attack.

\subsection{Robustness Against Target Domain Attack} 
\label{sec:robustness_tgt}

Fine-tuning the NTL models using target domain data is a more direct strategy to break the non-transferability if malicious attackers have access to some labeled target domain data. However, most existing methods \cite{wang2021non,wang2023model,zeng2022unsupervised,wang2023domain,hong2024improving,peng2024map} ignore the robustness of their methods against target-domain fine-tuning attacks. SOPHON \cite{deng2024sophon} formally proposes the problem of non-fine-tunable learning, which aims at ensuring the target-domain performance could still be poor after being fine-tuned using target domain data.
Their main idea is to involve the fine-tuning process in training stage. 
Specifically, they leverage model agnostic meta-learning (MAML) \cite{finn2017model} to simulate multiple-step fine-tuning for the current model on the target domain. Then, they add per-step risk of the target domain as the total target-domain risk. By maximizing the total target-domain risk, the robustness against target-domain attacks can be enhanced. 
ArchLock \cite{zhou2024archlock} aims to find the most non-transferable network architectures \cite{liu2018darts,real2019regularized}, where they also implicitly consider the robustness on the target domain. SpeciÔ¨Åcally, they maximize the \textit{minimum risk} of an architecture on the target domain in searching the non-transferable architectures. The minimum risk is found by searching the optimal \textit{parameters} of the \textit{architecture} with the minimum task loss on the target domain.
 
However, labeled target domain data being available to malicious attackers is a strong assumption that may not always hold in practical scenarios. A more realistic scenario is that the attacker only has access to some unlabeled target domain data. Whether NTL can resist attacks driven by unlabeled target domain data has not yet been studied. 

