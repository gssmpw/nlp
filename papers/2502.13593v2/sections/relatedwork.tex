
% \vspace{-2mm}
\section{Related Works}
\label{sec:related}



\paragraph{Machine unlearning (MU).}
Both MU \cite{bourtoule2021machine,nguyen2022survey,qu2023learn,xu2023machine,zhu2024decoupling,wang2024unlearning} and NTL serve purposes in model capacity control, albeit with  differences in their applications and methodologies. 
MU primarily aims to forget specific data points
from training datasets \cite{xu2023machine,rosati2024representation} (the model behaviors are consistent to never training on the selected data points), while NTL aims at resist the generalization from the training domain to a specific target domain or task. Particularly, MU and NTL share some overlapping applications such as safety alignment of LLMs. However, MU more focus on eliminating harmful data influence (e.g., sensitive or illegal information) and the associated model capabilitie \cite{barez2025open,maini2024tofu}, while NTL more focus on preventing harmful and unauthorized fine-tuning \cite{huang2024harmful}. 


\paragraph{Transfer learning (TL).} TL \cite{zhuang2020comprehensive} aims at improving model performance on a different but related target domain or task. It can be categorized into several subfields, such as unsupervised domain adaptation (UDA) \cite{long2016unsupervised,venkateswara2017deep,kang2019contrastive,liu2022deep}, source-free domain adaptation (SFDA) \cite{liang2020we,liang2021source,tang2024sourcefree,tang2024unified}, test-time adaptation (TTA) \cite{chen2022contrastive,liang2025comprehensive}, domain generalization (DG) \cite{wang2022generalizing,ye2023coping,huang2023robust}, few-shot/zero-shot learning (FSL/ZSL) \cite{xian2018zero,chen2022transzero,chen2024causal,chen2024rethinking,hou2024visual}, continual learning (CL) \cite{lopez2017gradient,zenke2017continual,wang2022improving,wang2024comprehensive}, etc. 
Specifically, domain adaptation and domain generalization are closely related to NTL, but their overall objectives are opposite to NTL. In general, TL techniques generally can be used in a reversed way to achieve NTL. Moreover, TL can also be seen as post-training attacks against NTL, as we discussed in \Cref{sec:robustness,sec:exp}. In addition, NTL can also be used as an attack for TL.
