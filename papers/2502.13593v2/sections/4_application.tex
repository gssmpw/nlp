% \vspace{-1mm}
\section{Applications of NTL}
\label{sec:applications}


NTL supports different applications, depending on which data are used as source and target domain. We introduce two applications in model intellectual property (IP) protection and then the application of harmful fine-tuning defense. 

\paragraph{Ownership verification (OV).} OV is a passive IP protection manner, which aims to verify the ownership of a deep learning model \cite{cheng2021mid,lederer2023identifying}. NTL solves ownership verification by triggering misclassification on data with pre-defined triggers \cite{wang2021non,chen2024mark,guo2024zeromark}. For example, when training, we add a shallow trigger (only known by the model owner) on the original dataset data and see them as the target domain, while the original data without the trigger is regarded as the source domain. Then, target-specified NTL is used to train a model. Therefore, the ownership can be verified via observing the performance difference of a model on the original data and the data with the pre-defined trigger. For SL model, the shallow trigger has minor influence on the model performance, and thus, the model shows similar performance on original data and data with triggers. In contrast, the NTL model specific to this pre-defined trigger has high performance on the original data but random-guess-like performance on data with the trigger. This provides evidence for verifying the model's ownership.
% 
\paragraph{Applicability authorization (AA).} AA is an active IP protection approach that ensures models can only be effective on authorized data \cite{wang2021non,xu2024idea,si2024iclguard}. NTL solves AA by degrading the model generalization outside the authorized domain. Basic solution is to add a pre-defined trigger on original data (seen as source domain), and the original data without the correct triggers is regarded as the target domain. After training by NTL, the model will only perform well on authorized data (i.e., the data with the trigger). Any unauthorized data will be randomly predicted by the NTL model. Thus, AA can be achieved.




\paragraph{Safety alignment and harmful fine-tuning defense.} 
Fine-tuning large language models (LLMs) with user's own data for downstream tasks has recently become a popular online service \cite{huang2024harmful,openai2024finetune}. However, this practice raises concerns about compromising the safety alignment of LLMs \cite{qi2023fine,yang2023shadow,zhan2023removing}, as harmful data may be present in users' datasets, whether intentionally or unintentionally. To address the risks of harmful fine-tuning, various defensive solutions \cite{huang2024booster,rosati2024representation,huang2024vaccine} have been proposed to ensure that fine-tuned LLMs can effectively refuse harmful queries. Specifically, these defense methods aim to limit the transferability of LLMs from harmless queries to harmful ones, which techniques are variants of the objectives of NTL. 
Actually, all existing NTL approaches can be applied to this task by regarding the alignment data as the source domain and the harmful data as the target domain. Then, target-specified NTL can be conducted to defend agaginst harmful fine-tuning attacks.
