
\section{Proof of stability}\label{sec:appendix_stability_proof}
This section provides a proof of \cref{prop:stability_requirements}, which describes the essential requirements for achieving stable dynamics with \glsxtrlongpl{PHS}. Due to the relevance of convexity to the following discussion, the following Lemma first provides some widely known equivalent formulations of convexity for scalar-valued multivariate functions. 
\begin{lemma}\label{lem:convexity}
    Consider a twice continuously differentiable function $f:\bbR^n\rightarrow\bbR,\,\bsx\mapsto f(\bsx)$.
                The following statements are equivalent:
    \begin{enumerate}
        \item $f$ is convex in $\bsx$,
        \item $tf(\bsx_1) + (1-t)f(\bsx_2) \geq f(t\bsx_1 + (1-t)\bsx_2),$ $\forall \bsx_1,\bsx_2\in\bbR^n,\,\forall t\in[0,1]$,
        \item $\partdiff{f}{\bsx}\big\rvert_{\bsx_1}^{\intercal}(\bsx_2-\bsx_1)\leq f(\bsx_2)-f(\bsx_1)$,
                            \end{enumerate}
\end{lemma}
For a proof, the reader is referred to \cite{boyd2023}.
With the notion of convexity established, attention can now be directed to the proof of \cref{prop:stability_requirements}. For convenience, the statement of the theorem is repeated verbatim below:
\begin{theorem}\label{thm:stability_requirements_appendix}
    Consider the \gls{PHS} \cref{eq:isphs_evolution} in the unforced case ($\bsu(t)=\bszero$), with $\bsJ=-\bsJ^{\intercal}$ and $\bsR=\bsR^{\intercal},\,\bsR\succeq0$:
    \begin{equation}\label{eq:isphs_auto_evolution_appendix}
        \dot{\bsx} = \left[\bsJ(\bsx) - \bsR(\bsx)\right]\partdiff{\hamiltonian}{\bsx}(\bsx).
    \end{equation}
    Suppose the Hamiltonian $\hamiltonian(\bsx)$ is convex, twice continuously differentiable, and fulfills the following properties:
    \begin{align}\label{eq:prop_convex_requirements_appendix}
        \hamiltonian(\bszero)=0,&&
        \partdiff{\hamiltonian}{\bsx}\bigg\rvert_{\bsx=\bszero}=\bszero,&&
        \hessian{\hamiltonian}{\bsx}\bigg\rvert_{\bsx=\bszero}\succ0.
    \end{align}
    Then, the system in \cref{eq:isphs_auto_evolution_appendix} has a stable equilibrium at $\bsx(t)=\bszero$, and all solutions are bounded. Furthermore, the equilibrium is globally asymptotically stable if $\bsR(\bsx)\succ0$.
\end{theorem}
\begin{proof}
    Since the gradient of the Hamiltonian vanishes and its Hessian is positive definite, it has a strict local minimum at the origin. This implies the existence of an $R>0$ such that 
    \begin{equation}\label{eq:proof_minimum_def}
    \hamiltonian(\bsx)>\hamiltonian(\bszero)=0 \quad \forall 
       \bsx\in\{\norm{\bsx}\leq R,\,\bsx\neq\bszero\}.
    \end{equation}
    Consider the function $g(\bsx)=\frac{a}{r}\norm{\bsx}$ for some $r\in(0,R]$ with $a=\inf_{\norm{\bsx}=r}\hamiltonian(\bsx)>0$. We claim that $g$ is a lower bound for $\hamiltonian$, i.e. $g(\bsx)\leq\hamiltonian(\bsx)$ for all $\norm{\bsx}>r$.

    Suppose there existed an $\bsx^*\in\bbR^n$ that violated the claim, that is $\norm{\bsx^*}>r$ and $g(\bsx^*)>\hamiltonian(\bsx^*)$. Due to the convexity of $\hamiltonian$, it holds
    \begin{equation}
    t\hamiltonian(\bsx^*)+(1-t)\hamiltonian(\bszero)\geq\hamiltonian(t\bsx^*+(1-t)\cdot\bszero) \quad\forall t\in[0,1]
    \end{equation}
    and thus
    $tg(\bsx^*)>t\hamiltonian(\bsx^*)\geq\hamiltonian(t\bsx^*).$
    Choosing $t=\frac{r}{\norm{\bsx^*}}$ and applying the definition of $g$ we obtain
    \begin{equation}
    \frac{r}{\norm{\bsx^*}}g(\bsx^*) = a > \hamiltonian\bigg(\frac{r}{\norm{\bsx^*}}\bsx^*\bigg)\geq a.
    \end{equation}
    This is a contradiction, and therefore $g$ is a lower bound.

    With the bound established, showing that $\hamiltonian$ is radially unbounded is straightforward. 
    For any path $\bsx(t)$ with $\norm{\bsx(t)}>r$ and $\lim_{t\rightarrow\infty}\norm{\bsx(t)}=\infty$ we have
    \begin{equation}
    \lim_{t\rightarrow\infty}\hamiltonian(\bsx(t)) \geq \lim_{t\rightarrow\infty} g(\bsx(t)) = \infty.
    \end{equation}
    To demonstrate the global positive definiteness of $\hamiltonian$ note that for all $\norm{\bsx}>r$ the lower bound implies 
    $0<g(\bsx)\leq\hamiltonian(\bsx).$
    Together with \cref{eq:proof_minimum_def}, this implies that $\hamiltonian$ is indeed positive definite for all $\bsx\in\bbR^n$.

    From the port-Hamiltonian structure of \cref{eq:isphs_auto_evolution_appendix} we immediately obtain the decrease condition $\dot\hamiltonian\leq 0$ (see \cref{eq:isphs_energy_balance}).
    Using the $\hamiltonian$ as a Lyapunov function, we obtain all requirements to conclude \emph{local} stability of the equilibrium.
    Furthermore, the decrease condition implies that a solution starting at $\bsx_0$ cannot leave the sublevel set $\Omega_\hamiltonian=\{\bsx\in\bbR^n\vert\hamiltonian(\bsx)\leq\hamiltonian(\bsx_0)\}$.
    Since $\Omega_\hamiltonian\subseteq\Omega_g=\{\bsx\in\bbR^n\vert g(\bsx)\leq\hamiltonian(\bsx_0)\}$ and $\Omega_g$ is clearly bounded, all solutions of \cref{eq:isphs_auto_evolution_appendix} must be bounded as well.

    Finally, to show that \emph{global} asymptotic stability follows from $\bsR\succ0$, it remains to be demonstrated that $\dot\hamiltonian<0$ for all $\bsx\neq\bszero$. 
    This is implied by \cref{eq:isphs_energy_balance} if $\partdiff{\hamiltonian}{\bsx}\neq\bszero$ for all $\bsx\neq\bszero$. Proving that the gradient of $\hamiltonian$ vanishes only at the origin is done by contradiction. Assume that there exists an $\bsx^*\neq\bszero$ such that $\partdiff{\hamiltonian}{\bsx}\big\rvert_{\bsx^*}=\bszero$. Furthermore, let $\bsx_0=\bszero$. 
    Since $\hamiltonian$ is convex it follows (see \cref{lem:convexity}) that
    \begin{align}
        \hamiltonian(\bsx_0)-\hamiltonian(\bsx^*) \geq \partdiff{\hamiltonian}{\bsx}\bigg\rvert_{\bsx^*}^{\intercal}(\bsx_0-\bsx^*) = 0,\\
        \hamiltonian(\bsx^*)-\hamiltonian(\bsx_0) \geq \partdiff{\hamiltonian}{\bsx}\bigg\rvert_{\bsx_0}^{\intercal}(\bsx^*-\bsx_0) = 0.
    \end{align}
    This implies $\hamiltonian(\bsx_0)=\hamiltonian(\bsx^*)=0$. However, since $\hamiltonian$ is globally positive definite and thus only vanishes at the origin, we have $\bsx_0 = \bsx^*$. This is a contradiction as $\bsx^*\neq\bszero$, which concludes the proof.
\end{proof}

\section{Experiment hyperparameters} \label{sec:experiment_hyperparameters_appendix}
In each experiment presented within this document, the \sPHNN, \PHNN, and \NODE models were trained for the same number of steps using identical learning rates. The mean squared error loss function and the ADAM \cite{kingma2015} optimizer were used throughout.
The model and training hyperparameters, along with the number of model instances trained for statistical evaluation, are provided in \cref{tab:hyperparameters}. 
The table also specifies which components of the \sPHNN and \PHNN were parameterized using \glspl{NN} (\glspl{FICNN} and \gls{FFNN}) and which were chosen to be constant.
In each experiment, the \PHNNs were defined almost identically to the \sPHNNs, with the sole difference being that the normalized \gls{FICNN} used to represent the Hamiltonian $\hamiltonian$ was replaced by an unconstrained \gls{FFNN}.
Throughout the first four experiments, all \glspl{NN} consisted of two hidden layers, each containing \num{16} neurons. The network width was increased to \num{32} neurons for the additive manufacturing surrogate.

\begin{table}[ht]
\caption{Hyperparameters per experiment. TF and DF refer to trajectory and derivative fitting, respectively. The number of instances trained per model type is listed in the ''\# inst.'' column, and $n$ and $m$ denote the number of dimensions of state and input.}
\label{tab:hyperparameters}
\begin{center}
\begin{footnotesize}
\begin{tabular}{lcccccr}
\toprule
Experiment                 & \# training steps   & learning rate & \# inst. & $n$ & $m$ & parameterization of \sPHNNs and \PHNNs \\
\midrule
Randomly initialized model & -                   &  -            & \num{1}  & \num{2} & - & \glspl{NN} for $\hamiltonian$, $\bsJ$ and $\bsR$ \\
Spinning rigid body        & \num{50000} DF      & \num{1d-3}    & \num{10} & \num{3} & - & \glspl{NN} for $\hamiltonian$ and $\bsJ$; constant $\bsR$\\
Cascaded tanks             & \num{50000} TF      & \num{5d-4}    & \num{20} & \num{2} & \num{1} & \gls{NN} for $\hamiltonian$; $\bsJ=\symplecticmatrix$; constant $\bsR$ and $\bsG$\\
Food processing surrogate  & \num{30000} TF      & \num{1d-4}    & \num{20} & \numrange{2}{5} & \num{1} & \gls{NN} for $\hamiltonian$; constant $\bsJ$, $\bsR$ and $\bsG$\\
\multirow{2}{*}{\makecell[l]{Additive manufacturing\\\hphantom{m}surrogate}}  & \num{20000} DF      & \num{1d-3}    & \multirow{2}{*}{\num{20}} & \multirow{2}{*}{\num{40}} & \multirow{2}{*}{\num{40}} & \multirow{2}{*}{\gls{NN} for $\hamiltonian$; constant $\bsJ$, $\bsR$ and $\bsG$}\\
                   &  and \num{10000} TF & \num{1d-5}    &          &  &  &  \\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
%\vskip -0.1in
\end{table}

\section{Spinning rigid body system} \label{sec:spinning_rigid_body_apendix}
The dynamics of a three-dimensional spinning rigid body example in \cref{sec:spinning_rigid_body} are described by Euler's rotation equations.
Expressing the angular velocity vector $\bsomega\in\bbR^3$ in the principle axis coordinate frame, the evolution equation is given by
\begin{equation}\label{eq:euler_equations}
    \bsI\dot\bsomega + \bsomega\times(\bsI\bsomega) = -\mu\bsomega \quad\forall t>0.
\end{equation}
Here, $\bsI=\diag(I_1, I_2, I_3)$ denotes the inertia matrix and $\mu$ is a dampening coefficient.
By introducing the Hamiltonian 
\begin{equation}   \label{eq:rigid_hamiltonian} 
\hamiltonian(\bsomega)=\frac{1}{2}I_1\omega_1^2+\frac{1}{2}I_2\omega_2^2+\frac{1}{2}I_3\omega_3^2,
\end{equation}
which describes its kinetic energy $E$, the system can be rewritten as a \gls{PHS} in the form of \cref{eq:isphs_evolution}:
\begin{equation}
    \begin{bmatrix}
        \dot\omega_1 \\ \dot\omega_2 \\ \dot\omega_3
    \end{bmatrix}
    =
    \Bigg(
    \underbrace{
        \begin{bmatrix}
            0 & -\frac{I_3}{I_1I_2}\omega_3 & \frac{I_2}{I_3I_1}\omega_2 \\
            \frac{I_3}{I_1I_2}\omega_3 & 0 & -\frac{I_1}{I_2I_3}\omega_1 \\
            -\frac{I_2}{I_3I_1}\omega_2 & \frac{I_1}{I_2I_3}\omega_1 & 0
        \end{bmatrix}
    }_{\bsJ(\bsomega)}
    -
    \underbrace{\mu
        \begin{bmatrix}
            \frac{1}{I_1^2} & 0\vphantom{-\frac{I_3}{I_1I_2}\omega_3} & 0 \\
            0 & \frac{1}{I_2^2} & 0\vphantom{-\frac{I_3}{I_1I_2}\omega_3} \\
            0\vphantom{-\frac{I_3}{I_1I_2}\omega_3} & 0 & \frac{1}{I_3^2} \\
        \end{bmatrix}
    }_{\bsR}
    \Bigg)
    \partdiff{\hamiltonian}{\bsomega}.
\end{equation}
Here, the \structurematrix $\bsJ$ is a matrix-valued function of the state variable $\bsomega$. 

We generated $(\bsomega, \dot\bsomega)$ pairs for training by evaluating \cref{eq:euler_equations} for varying $\bsomega$. To simulate a measurement process, we selected $\bsomega$ values from \qty{50}{\second} long numerically integrated trajectories, using \num{10} randomly chosen initial conditions $\bsomega(0) \in [0,1]^3$ with parameters $\bsI = \diag(1, 2, 3)$ and $\mu = 0.01$.
A representative trajectory of $\bsomega$ is shown in \cref{fig:spinning_rigid_body_training_data}.

\begin{figure}[ht]
    \centering
    \inputTikzWithExternalization{spinning_rigid_body_training_data}{tikz/spinning_rigid_body/training_trajectory.tex}
    \caption{Representative training trajectory for spinning rigid body, including interquartile mean and range of model predictions for $\omega_3$.}
    \label{fig:spinning_rigid_body_training_data}
    \end{figure}

\section{Thermal food processing surrogate data} \label{sec:chicken_data_appendix}
This section provides additional details about the data used in \cref{sec:chicken_data}, which is a subset of the data generated and used by \citet{kannapinn2022} in the creation of \glspl{ROM}. The dataset describes the thermal processing of chicken breasts in a convection oven and was generated via the simulation of a soft-matter model for meats implemented in COMSOL Multiphysics.
Each trajectory in the data set describes the temperature history at two points inside the \gls{FOM}. These result from a predefined excitation signal that controls the oven temperature. 
The probe points are situated in the center and close to the surface of the food item, with the corresponding temperatures labeled as $T_A$ and $T_B$, respectively. 
To facilitate cross-referencing, the alphanumeric identifiers used by \citet{kannapinn2022} for the individual trajectories are retained. The training data consists of trajectories \num{745} and \num{795}. 
They use an \gls{APRBS} and a multi-sine as the excitation signal $T_{\text{oven}}$. 
\Cref{fig:chicken_data_training_data} (left and middle) illustrates $T_{\text{oven}}$ along with the core and surface temperatures for both trajectories. 
In our experiment, the test group AP15 \cite{kannapinn2022} is utilized as test data. It comprises \num{15} trajectories with \gls{APRBS} excitations, specifically selected to ensure an even distribution of amplitudes and median values. 
This selection provides a fair evaluation of the models across the relevant temperature spectrum.
\Cref{fig:chicken_data_training_data} (right) shows an exemplary test trajectory, including the predictions for $T_A$ from all models using \num{3} augmented dimensions.
All trajectories span \qty{1395}{\second} and consist of \num{280} samples. 
The data is normalized for training to ensure the numerical values are of magnitude one. For this, an affine transformation is applied to $T_{\text{oven}}$, $T_A$, and $T_B$ individually. It shifts the ambient temperature to a value of zero and rescales the signal to have unit variance. The scaling and shifting factors are calculated using only the training data to ensure that the test data does not influence the training procedure.

\begin{figure}[ht]
    \centering
        \inputTikzWithExternalization{chicken_data_dataset_examples}{tikz/chicken_data/dataset_examples.tex}
    \caption{\emph{Left and middle}: Training trajectories. \emph{Right}: Exemplary test trajectory with interquartile mean and range of the model predictions for $T_A$.}
    \label{fig:chicken_data_training_data}
    \vskip -0.2in  
\end{figure}

\section{Thermal field data} \label{sec:thermal_field_data_appendix}

The following describes the method for dimensionality reduction of the field data applied in \cref{sec:thermal_field_data}. 
The process is first described for general field data given as a matrix $\bsA = [\bsa_0, \dots, \bsa_N]^\intercal$ of $N$ snapshots $\bsa_i\in\bbR^M$, where $N$ is the number of snapshots and $M$ is the number of \gls{FE} nodes.
To identify the dominant modes of the data, we perform a \gls{SVD} of $\bsA$:
\begin{equation}
    \bsA = \bsU\bsSigma\bsV^{\intercal}\in\bbR^{N\times M},
\end{equation}
where $\bsU\in\bbR^{N\times N}$ and $\bsV\in\bbR^{M\times M}$ are unitary matrices and $\bsSigma\in\bbR^{N\times M}$ is a rectangular diagonal matrix containing the singular values $\sigma_1\geq \dots \geq \sigma_N$ on its diagonal. 
Dimensionality reduction is achieved by truncating the \gls{SVD} $\bsA\approx\tilde\bsU\tilde\bsSigma\tilde\bsV^{\intercal}$, where $\tilde\bsU\in\bbR^{N\times k}$ and $\tilde\bsV\in\bbR^{M\times k}$ contain the first $k$ columns of $\bsU$ and $\bsV$, and $\tilde\bsSigma=\diag(\sigma_1,\dots,\sigma_k)$ is the truncated matrix of the dominant singular values. 
The dominant modes in the data are then given by $\bsM=c\tilde\bsV$. Here, the scaling factor $c$ is the standard deviation of the elements in $\tilde\bsU\tilde\bsSigma$. This scaling ensures that the latent trajectories will have unit variance. 
To map a latent state $\bsx\in\bbR^{k}$ to the full order field $\bsX\in\bbR^{M}$ we compute:
\begin{equation}\label{eq:svd_latent_to_fom}
    \bsX = \bsM\bsx.
\end{equation}
Conversely, to map from the full-order field to the latent space, we solve the least-squares regression problem:
\begin{equation}\label{eq:svd_fom_to_latent}
    \bsx=\argmin_{\bsx}\norm{\bsX - \bsM\bsx}.
\end{equation}
In \cref{sec:thermal_field_data}, we consider two fields: The thermal field describing the state of the system and the excitation given by the heat source field. For each, a separate \gls{SVD} is performed to compute $\bsM_T$ and $\bsM_Q$ using the respective snapshots from the two training trajectories.

To fully leverage the benefits of \sPHNNs, it is essential to ensure that the absence of a heat source is encoded as the zero-vector in the corresponding latent space. Since the mapping from the latent space \cref{eq:svd_latent_to_fom} is linear, and $\bsX_Q=\bszero$ corresponds to the absence of a heat source in the full order space, this requirement is automatically fulfilled.
However, for applications where a nonzero state $\bsX^*$ describes the absence of an external energy source or general nonlinear latent mappings (such as \glspl{AE}), the condition can be met by applying a constant translation in the latent space, which shifts the encoded state $\bsx^*$ to the origin. 
In \cref{sec:thermal_field_data}, we use this method to ensure that the thermal equilibrium corresponds to the origin of the temperature latent space. While this is not strictly necessary, knowledge of the equilibrium location in the latent space enables its integration into the \sPHNN's architecture by fixing the stable equilibrium position. 

Finally, we note that alternative dimensionality reduction techniques could be applied in \cref{sec:thermal_field_data}. In preliminary experiments, we explored the use of autoencoders for this purpose. Although autoencoders offer advantages over the linear \gls{SVD} regarding achievable compression rates, they exhibited poor extrapolation performance when trained on sparse data. This issue could potentially be mitigated by training the autoencoders with additional synthetic data, which would not necessitate costly simulations; however, further research is needed to explore this approach.

