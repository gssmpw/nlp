\section{Experiments}\label{sec:experiments}

We evaluate the proposed model on multiple example problems, including measurement data from a real physical system and multi-physics simulation data.
In the experiments, the performance of \sPHNNs is compared to that of \NODEs and port-Hamiltonian neural networks without the stability constraint (\PHNNs). 
All models are trained by minimizing the mean squared error using the ADAM \cite{kingma2015} optimizer. 
Model hyperparameters are listed in \cref{sec:experiment_hyperparameters_appendix}.
For \gls{ODE} integration, the Runge--Kutta scheme Tsit5 \cite{tsitouras2011} with an adaptive step size is used. 
\subsection[Interpreting random sPHNNs]{Interpreting random \sPHNNs}

\begin{figure}[ht]
    \centering
    \inputTikzWithExternalization{dynamics_decomposition}{tikz/interpretability/dynamics_decomposition.tex}
    \vspace*{-2mm}
    \caption{Dynamics of a randomly initialized \sPHNN.}
    \label{fig:random_network}
   % \vspace{-0.2cm}
\end{figure}

To visualize the physical bias and inherent interpretability of \sPHNNs, we show the decomposition into conservative and dissipative dynamics for untrained \glspl{NN} parameterizing $\hamiltonian$, $\bsJ$, and $\bsR$ for $n=2$ in \cref{fig:random_network}.
The trajectories of the conservative dynamics resulting from $\bsJ\,\partial\hamiltonian/\partial\bsx$ exhibit cyclic, energy-conserving behavior and align with the level sets of the Hamiltonian in this two-dimensional case.
Conversely, the dissipative dynamics resulting from $-\bsR\,\partial\hamiltonian/\partial\bsx$ demonstrate the convergence of solutions to the origin, resulting in the globally asymptotically stable dynamics of the \sPHNN.

\subsection{Spinning rigid body}\label{sec:spinning_rigid_body}

We generate data of a damped spinning rigid body by numerically integrating Euler's rotation equations, see \cref{sec:spinning_rigid_body_apendix} for a detailed description. 
Here, accurately capturing the dynamics of the angular velocity vector $\bsx=\bsomega$ with a \gls{PHS} requires a state-dependent \structurematrix $\bsJ(\bsx)$.
We train ten instances per model type using derivative fitting with $(\bsomega, \dot\bsomega)$ pairs sampled from the trajectories.
Figure 3 shows the rotational energy $E(\bsomega)$ computed from model predictions $\bsomega$ via \cref{eq:rigid_hamiltonian} and corresponding outputs of the Hamiltonian \glspl{NN} $\hamiltonian(\bsomega)$ for \sPHNNs and \PHNNs. 
The former successfully identified the true system's energy, with the Hamiltonian closely matching the energy computed from its predictions and ground truth, respectively. In contrast, the \PHNNs show noticeable deviations between the Hamiltonian and the energy calculated from its predictions, which also do not converge to zero. Furthermore, the \NODEs show unphysical behavior with increasing energy at certain points and exhibit high variance between instances.

\begin{figure}[h!]
    \centering
    \vspace*{-1mm}
            \inputTikzWithExternalization{spinning_body_energies}{tikz/spinning_rigid_body/energies.tex}
    \vspace*{-1mm}
        \caption{Interquartile mean (lines) and range (shaded regions) of the energy $E$ and Hamiltonian $\hamiltonian$ of the predicted states.}
    \label{fig:spinning_body_errors}
    \vspace{-0.3cm}
\end{figure}

\subsection{Cascaded tanks}\label{sec:cascaded_tanks}

Next, the \sPHNN model is evaluated using the cascaded tanks data set \cite{schoukens2020, schoukens2016}.
It contains measurements of a physical fluid level control system consisting of a pump and two tanks, see \cref{fig:cascaded_tanks.setup}. 
Liquid flows from the upper tank through an outlet into the lower tank, exits via a similar outlet into a reservoir, and is pumped back up to the upper tank. The system's input $u$ is the voltage applied to the pump, with higher values resulting in a faster volume flow. The fluid level in the lower tank is measured with a capacitive sensor, and the resulting voltage represents the systemâ€™s output $y$. 
The data consists of two trajectories comprising 1024 pairs $(u, y)$ each. The training trajectory is depicted in \cref{fig:cascaded_tanks.training_data}. The initial water level in the upper tank is unknown but is the same for both trajectories.

\begin{figure}[t]
    \centering
    \vskip -1mm
    \begin{subfigure}[b]{0.39\columnwidth}
        \centering
        \resizebox{\columnwidth}{!}{            \inputTikzWithExternalization{setup_sketch}{tikz/cascaded_tanks/setup_sketch.tex}
        }
                \caption{Experimental setup}
        \label{fig:cascaded_tanks.setup}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.59\columnwidth}
        \centering
        \resizebox{\columnwidth}{!}{            \inputTikzWithExternalization{cascaded_tanks_training_data}{tikz/cascaded_tanks/training_data.tex}
        }
                \caption{Training data (overflow highlighted)}
        \label{fig:cascaded_tanks.training_data}
    \end{subfigure}
    \vspace*{-2mm}
    \caption{Cascaded tanks experiment setup and training data.}
    \label{fig:cascaded_tanks}
    \vskip -0.4cm
\end{figure}

We model the system using a two-dimensional state vector $\bsx=[x_1, x_2]^{\intercal}$, where $y=x_2$. 
Assuming the augmented state $x_1$ describes the water level in the upper tank, it is straightforward to determine the equilibrium of the autonomous system. With zero input, the tanks will eventually drain; therefore, the stable equilibrium is given by $\bsx_0=\bszero$. We thus fix the minimum of the \sPHNN's Hamiltonian to the origin.
However, for a general problem, one might have little to no knowledge about the dynamics, and it might be challenging to infer the location of the relevant equilibrium. 
To investigate the effects of this potential lack of knowledge, we train a second \sPHNN version hereafter called \sPHNN-LM.
Its minimum location $\bsx_0$ is initialized randomly and is optimized during training.
Apart from this, both \sPHNN versions are identical and use a constant fixed symplectic matrix as the \structurematrix, as well as constant but learnable dissipation and input matrices. 
The \PHNNs are defined identically to the \sPHNNs, except for the use of unconstrained \glspl{FFNN} instead of the normalized \glspl{FICNN}.

We train \num{20} instances per model type and show a statistical evaluation of the \glspl{RMSE} of the models on both the training and test data in \cref{fig:cascaded_tanks_rmse}. 
Both \sPHNN and \sPHNN-LM achieve a similar accuracy, whereas the \NODE and \PHNN perform worse on both trajectories on average. Furthermore, the variance between the instances of the \sPHNNs is noticeably smaller than for the \NODE and \PHNN. 

\begin{figure*}[ht]
    \centering
        \begin{subfigure}[t]{0.72\columnwidth}
        \centering
                    \inputTikzWithExternalization{cascaded_tanks_boxplot_short_labels}{tikz/cascaded_tanks/boxplot_short_labels.tex}                                \subcaption{Training and test \glsxtrshort{RMSE}.}
                \label{fig:cascaded_tanks_rmse}
    \end{subfigure}
            \begin{subfigure}[t]{0.62\columnwidth}
        \centering
                    \inputTikzWithExternalization{cascaded_tanks_relaxation}{tikz/cascaded_tanks/relaxation_all_in_one.tex}
                        \subcaption{Relaxation.}
        \label{fig:cascaded_tanks_relaxation}
    \end{subfigure}
            \begin{subfigure}[t]{0.62\columnwidth}
        \centering
                    \inputTikzWithExternalization{cascaded_tanks_saturation}{tikz/cascaded_tanks/saturation_all_in_one.tex}
                        \subcaption{Saturation.}
        \label{fig:cascaded_tanks_saturation}
    \end{subfigure}
    \vspace*{-2mm}
    \caption{Cascaded tanks evaluation. \emph{Left}: \Glsxtrshortpl{RMSE} for training and test trajectories. \emph{Middle}: Predictions for the extended cascaded tanks test trajectory. At $t=\qty{4096}{\second}$, the pump is turned off. \emph{Right}: Predictions for a linearly increasing pump voltage. Due to an overflow, the true system exhibits a hard saturation at $x_2=\qty{10}{\volt}$. Lines correspond to the interquartile mean, whereas shaded areas represent the interquartile range of the predictions from the \num{20} model instances.}
    \label{fig:cascaded_tanks_results}
    \vspace*{-3mm}
\end{figure*}

To evaluate the extrapolation capabilities of the models, we perform two additional experiments. 
First, we extend the evaluation data with \qty{400}{s} of zero-input. The resulting predictions are depicted in 
\Cref{fig:cascaded_tanks_relaxation}. Assuming perfectly calibrated sensors, the true system response is an eventual zero-output as both tanks eventually drain. Per construction, this behavior is guaranteed for \sPHNNs. The \num{20} \sPHNN-LM instances identified the equilibrium to be in the range from \qtyrange{-0.196}{0.432}{\volt}, which shows that also $\bsx_0$ can be learned reliably. 
However, the \NODE and \PHNN models exhibit limitations in physically sensible extrapolation. Although some instances predict the water level to converge, the final values span a range of behaviors, including highly unphysical predictions, like oscillating water levels.

In the second extrapolation study, we investigate the hard saturation the cascaded tanks data system exhibits at an output value of \qty{10}{\volt}. This occurs when the tanks overflow in response to a large input voltage over an extended period of time (see \cref{fig:cascaded_tanks.training_data}) \cite{schoukens2016}. We explore the models' behavior in this regime by constructing a pump voltage signal that linearly increases from \qty{1}{\volt} initially to \qty{8}{\volt} at \qty{2800}{\second}. 
The resulting predictions are shown in \cref{fig:cascaded_tanks_saturation}. For pump speeds up to $\sim$\qty{7}{\volt} ($\sim$\qty{2400}{\second}), the \sPHNN and \sPHNN-LM instances capture the saturation effect, while the \NODE and \PHNN instances fail to do so.

\subsection{Thermal food processing surrogate}\label{sec:chicken_data}

With the applicability of \sPHNNs to real-world data established, this section focuses on one of the promising applications of data-driven dynamic system identification: the efficient construction of surrogate models.
Here, data from a conjugate heat transfer simulation in a convection oven coupled with a soft matter model for meats from \citet{kannapinn2022,kannapinn2024} is employed. 
The data consists of trajectories representing the temperature histories $T_A$ and $T_B$ at two probe points within the meat. These result from a predefined excitation signal $T_{\text{oven}}$, which controls the oven temperature. 
The surrogate's task is to predict the probe temperatures given the oven temperature as input. 

We fit \sPHNNs, \NODEs, and \PHNNs to two trajectories consisting of \num{280} samples each (see \cref{sec:chicken_data_appendix}). 
For each model, we vary the dimensionality of the state variable ${\bsx\in\bbR^n}$ with $n=2+n_A$ by adding \numrange{0}{3} augmented dimensions $n_A$.
The minimum $\bsx_0$ of the \sPHNNs' Hamiltonian is set to the point in phase space corresponding to the ambient temperature, as motivated by physical intuition about the stable thermal equilibrium.
The initial values of the augmented states are fixed to zero.
In total, \num{20} instances are trained per model type and augmented dimension. 

\begin{figure}[t]
    \centering
    \vspace*{-2mm}
        \inputTikzWithExternalization{chicken_data_boxplot}{tikz/chicken_data/rmse_boxplot.tex}
    \vspace*{-2mm}
    \caption{\Glspl{RMSE} of thermal food processing surrogate models evaluated on the \num{25} test trajectories for varying numbers of augmented dimensions. 
        }
    \label{fig:chicken_data_rmse}
    \vskip -0.2cm
\end{figure}

\begin{figure}[t]
    \centering
        \inputTikzWithExternalization{chicken_data_relaxation}{tikz/chicken_data/relaxation_all_in_one.tex}
    \vspace*{-2mm}
    \caption{Interquartile mean and range of the $T_A$ predictions for varying number of augmented dimensions for a custom test case.
    }
    \label{fig:chicken_data_best_predictions_relaxation}
        \vskip -0.3cm
\end{figure}

\begin{figure*}[ht]
    \centering
    \vspace*{-3mm}
    \inputTikzWithExternalization{ded_mesh_predictions}{tikz/thermal_ded/mesh_predictions.tex}
    \vspace*{-1mm}
    \caption{Temperature field predictions on the cuboid domain for a test case with $v=\qty{12.5}{\milli\metre\per\second}$, $Q=\qty{400}{\watt}$. The instances selected for this evaluation resulted in the median test error for the corresponding model type. Colors are clipped to remain in the legend's range.}
    \label{fig:ded_mesh_predictions}
    \vskip -0.4cm
\end{figure*}

The \glspl{RMSE} evaluated on 15 test trajectories (see \cref{sec:chicken_data_appendix}) are depicted in \cref{fig:chicken_data_rmse}. 
The \sPHNN predictions tend to improve with more augmented dimensions, achieving errors comparable to those obtained by \citet{kannapinn2022} using commercial software.
In contrast, the \NODEs and \PHNNs tend to deteriorate in performance with more augmented dimensions, likely due to instabilities in the learned dynamics.
This becomes apparent in \cref{fig:chicken_data_best_predictions_relaxation}, which depicts the models' predictions for a custom test case not included in the data set. 
While the \sPHNNs consistently converge to temperatures close to thermal equilibrium, the \NODEs and \PHNNs predictions tend to become unstable when one or more augmented dimensions are used.
These observations might be explained by the already limited training data becoming even sparser as more dimensions are added to the system.
While the data provides enough information about a two-dimensional state space for the models to generalize and allow stable interpolation, the regions of the phase space not covered by the training data grow as more augmented dimensions are added.
For the unconstrained models, these regions may contain arbitrary dynamics, including instabilities.
In contrast, the stability constraint of the \sPHNN enables the safe exploitation of the additional flexibility provided by a higher dimensional state space without running into the same pitfalls of the unconstrained models. 

\subsection{Additive manufacturing surrogate}\label{sec:thermal_field_data}

Finally, we evaluate the applicability of \sPHNNs to higher-dimensional state spaces and build a \gls{ROM} as a  surrogate of a 3D \gls{PDE} problem. 
To this end, we generate field data by numerically solving the heat conduction equation for a moving heat source on a cuboid domain with convective and radiative thermal boundary conditions. 
The \gls{FE} simulation is designed to model the evolution of the temperature field of a metal additive manufacturing process, see 
\cite{kannapinn2024a}.
In total, \num{25} trajectories are obtained by varying the heat source speed $v$ from \qtyrange{10}{20}{\milli\metre\per\second} and power $Q$ from \qtyrange{300}{500}{\watt}.
All trajectories span \qty{20}{\second}, with a non-zero heat source lasting for the initial \qtyrange{2.5}{6}{\second}, depending on $v$.
However, directly using the discretized field data would lead to very high-dimensional models and render the training and inference inefficient. Fortunately, the dynamics of complex systems often evolve on lower-dimensional manifolds. 
We exploit this and use the \gls{SVD} to find a low-rank approximation of the data.
Both temperature and heat source fields are mapped onto \numsvdmodes-dimensional latent spaces, respectively (for details, see \cref{sec:thermal_field_data_appendix}).
The models are trained on the latent representations of the two trajectories with 
$(v,Q)=(\qty{10}{\milli\metre\per\second}, \qty{300}{\watt})$ and $(\qty{20}{\milli\metre\per\second}, \qty{500}{\watt})$
using derivative fitting and subsequent fine-tuning with trajectory fitting.

\Cref{fig:ded_mesh_rmses_over_time} shows the \gls{RMSE} of the models calculated over all \num{25} trajectories and the spatial domain. 
While the \sPHNN's \gls{RMSE} peaks below \qty{30}{\kelvin} and decreases as the system approaches thermal equilibrium, the errors for \NODE and \PHNN increase rapidly, driven by instabilities in the learned dynamics. 
This is visible in \cref{fig:ded_mesh_predictions}, which shows the predictions for a test case mapped back to the physical domain.
A likely cause of these instabilities is the sparsity of the training data, which leaves gaps in the phase space, allowing unstable dynamics to emerge. 
The stability constraint in \sPHNNs mitigates these issues, ensuring robust performance even with scarce training data.

\begin{figure}[t]
    \centering
    \inputTikzWithExternalization{ded_errors_over_time}{tikz/thermal_ded/errors_all_trajectories.tex}
        \caption{Interquartile mean and range of the \glsxtrshortpl{RMSE} evaluated with all \num{25} trajectories of the thermal field data.}
    \label{fig:ded_mesh_rmses_over_time}
    \vskip -0.6cm
\end{figure}

\vspace*{-2mm}

