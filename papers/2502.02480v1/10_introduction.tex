\section{Introduction}\label{sec:introduction}

Recent years have seen strongly growing interest in using \gls{ML} to identify the dynamics of physical systems directly from observations \cite{legaard2023,lee2021,lai2021,boettcher2023,park2024,wang2021,ramasinghe2023}. 
This approach has diverse applications, ranging from modeling phenomena with partially or entirely unknown underlying physics to creating surrogate and reduced-order models for rapid simulations and digital twins \cite{willard2022, wang2021, kannapinn2024}. In the latter application, data-driven methods can address the drawbacks of traditional numerical methods, which often require tremendous computational resources and expertise \cite{wang2021,abbasi2024}. However, in turn, classical \gls{ML} algorithms typically require large amounts of training data and lack robustness, as they often suffer from poor extrapolation and are prone to making physically implausible forecasts \cite{abbasi2024,wang2021,willard2022,liu2019a}.
To overcome these limitations, a general consensus is to avoid discarding centuries worth of established knowledge in physics, which is an implicit consequence of relying solely on data-driven methods. Instead, there is a growing emphasis on incorporating physical priors and inductive biases into \gls{ML} approaches \cite{willard2022,liu2019a}. 
The thus-born field of \emph{physics-guided} \gls{ML} promises improved accuracy, robustness, and interpretability while reducing data requirements \cite{wang2021,liu2019a}. 

In recent years, several physics-guided architectures have been proposed for learning dynamical systems. These approaches often parameterize an \gls{ODE} to identify continuous-time dynamics. 
To ensure adherence to the laws of thermodynamics, many architectures build upon modeling frameworks such as Hamiltonian mechanics \cite{greydanus2019,jin2020,choudhary2021}, Lagrangian mechanics \cite{lutter2019,cranmer2020}, Poisson systems \cite{jin2023}, \glsxtrshort{GENERIC} \cite{hernandez2021,zhang2022}, or \glspl{PHS} \cite{zhong2020,desai2021,eidnes2023,neary2023}. 
The latter two frameworks allow for the modeling of dissipative dynamics, while the former are generally limited to energy-conserving systems. 
Additionally, \glspl{PHS} explicitly incorporate external excitations, making them particularly suited for modeling systems with external control.

The mentioned physical modeling frameworks have implications for the stability of the learned dynamics.
For example, stable equilibria in the dynamics of \glsxtrlongpl{HNN} correspond to minima or maxima in its Hamiltonian energy function and can never be asymptotically stable.
However, these effects are often not discussed in the referenced works.
Nevertheless, stability is an important and fundamental property of any dynamic system. 
Guarantees on the stability of the dynamics may improve the model's robustness and help trust their predictions. 
Furthermore, in applications where the physical processes are known to be stable, such guarantees can themselves be viewed as physically motivated biases \cite{erichson2019}.
Given the importance of the stability property, multiple other works have proposed methods to enhance the stability of learned dynamics. 
Many use \NODEs \cite{chen2018} as a basis, as they bridge the gap between dynamical systems and deep learning and provide the opportunity to apply established stability theory. 
\citet{massaroli2020} propose a provably stable \NODE variant that reformulates the right-hand side of the \gls{ODE} as the negative gradient of a scalar function. Consequently, all local minima of this function correspond to stable equilibria with their own basin of attraction. 
\textcite{kolter2019} describe another provably stable architecture based on \NODEs and Lyapunov's stability theory. 
They concurrently learn unconstrained dynamics with a \NODE and a convex Lyapunov function. A globally stable equilibrium is guaranteed by projecting the \NODE-dynamics onto the space of stable dynamics as given by the Lyapunov function. \citet{takeishi2021} extend this approach from a stable equilibrium to stable invariant sets.
Other studies have combined Lyapunov's theory with \glspl{NN} to guarantee the stability of discrete-time dynamics \cite{lawrence2021,erichson2019} or to estimate the region of attraction for given dynamical systems \cite{richards2018,barreau2024}.
While Lyapunov’s theory primarily addresses stability concerning perturbations in the system’s state, for dynamic systems with inputs, the sensitivity of the state trajectory to these inputs can be equally significant. \textcite{kojima2022} propose a projection-based method to learn input-output stable dynamics. Besides the stability of the unforced system, this ensures that the system's output stays bounded for bounded inputs. In a recent work \cite{okamoto2024}, this projection-based method was generalized to dissipative systems.

While the highlighted studies have either provided stability or physical biases for the data-driven identification of dynamic systems, to the best of the authors' knowledge, the intersection of these two areas remains largely unexplored.
This work aims to exploit the connections between stability and physical frameworks and proposes a port-Hamiltonian \gls{NN} model for learning stable dynamical systems. 
