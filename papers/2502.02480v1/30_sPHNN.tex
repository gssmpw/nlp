\section[Stable port-Hamiltonian neural networks]{Stable port-Hamiltonian\\neural networks}\label{sec:sPHNN}

Our goal is to learn the dynamics of physical systems in the form of \cref{eq:isphs_evolution} with desirable stability properties from observations. 
To this end, we parameterize the components $\hamiltonian$, $\bsJ$, $\bsR$ and $\bsG$ of \gls{PHS} with \glspl{NN}. 

\subsection{Why port-Hamiltonian dynamics?}

The use of the port-Hamiltonian formulation has multiple favorable consequences. 
When learning system dynamics from data, the structure of \cref{eq:isphs_evolution} provides both a physical bias and the opportunity for interpreting the resulting trained model. 
\cref{eq:isphs_energy_balance} highlights the distinctive roles played by the three matrix-valued functions. The structure matrix $\bsJ(\bsx)$ results in conservative dynamics, the dissipation matrix $\bsR(\bsx)$ contributes only to the dissipative dynamics, and the input matrix $\bsG(\bsx)$ captures the energy flow over the system boundaries due to the inputs. This knowledge improves the interpretability of the trained model components. Furthermore, it allows new ways of incorporating prior knowledge into the system. For example, if it is known beforehand that a system conserves energy, then $\bsR$ can be set to zero. 
This guarantees that the identified dynamics will exactly conserve the Hamiltonian as long as $\bsu=\bszero$.

The energy balance \cref{eq:isphs_energy_balance} also demonstrates the physical bias inherently present in \cref{eq:isphs_evolution}. 
Due to the constraints on $\bsJ$ and $\bsR$, the unforced system can only dissipate energy or, at most, conserve it but never gain it. While this property of \glspl{PHS} enforces the thermodynamic constraint that energy cannot be created, its connection to the decrease condition from Lyapunov's theory also provides the opportunity to introduce further desirable stability constraints.

\subsection{Stability of port-Hamiltonian dynamics}

To guarantee the existence of stable equilibria of port-Hamiltonian systems, it is sufficient to enforce the existence of strict local minima in the Hamiltonian. However, these stability implications are only \emph{local}, meaning the size and shape of the corresponding basin of attraction are not constrained by \cref{eq:isphs_evolution}. Small perturbations might be enough to stray far from the stable equilibria. 
Furthermore, the general \gls{PHS} can represent dynamics such as unbounded trajectories, which must be considered unstable from a practical standpoint. 
The following describes the necessary requirements for achieving \emph{global} stability. 
Essentially, the Hamiltonian $\hamiltonian$ is required to be a suitable Lyapunov function, which includes positive definiteness and radial unboundedness.
These properties are achieved by constraining the Hamiltonian to be convex and ensuring the existence of a strict minimum. This approach is similar to the construction of a Lyapunov function by \citet{kolter2019}, but differs in the way the minimum is enforced.
\begin{theorem}\label{prop:stability_requirements}
    Consider the \gls{PHS} \cref{eq:isphs_evolution} in the unforced case ($\bsu(t)=\bszero$), with $\bsJ=-\bsJ^{\intercal}$ and $\bsR=\bsR^{\intercal}$, $\bsR\succeq0$:
    \begin{equation}\label{eq:isphs_auto_evolution}
        \dot{\bsx} = \left[\bsJ(\bsx) - \bsR(\bsx)\right]\partdiff{\hamiltonian}{\bsx}(\bsx).
    \end{equation}
    Suppose the Hamiltonian $\hamiltonian(\bsx)$ is convex, twice continuously differentiable, and fulfills the following properties:
    \begin{align}\label{eq:prop_convex_requirements}
        \hamiltonian(\bszero)=0,&&
        \partdiff{\hamiltonian}{\bsx}\bigg\rvert_{\bsx=\bszero}=\bszero,&&
        \hessian{\hamiltonian}{\bsx}\bigg\rvert_{\bsx=\bszero}\succ0.
    \end{align}
    Then, the system in \cref{eq:isphs_auto_evolution} has a stable equilibrium at $\bsx(t)=\bszero$, and all solutions are bounded. Furthermore, the equilibrium is globally asymptotically stable if $\bsR(\bsx)\succ0$.
\end{theorem}
A proof of \cref{prop:stability_requirements} is provided in \cref{sec:appendix_stability_proof}.
However, its implications also follow a clear physical intuition. 
If a dynamical system continuously loses energy via internal dissipation ($\bsR\succ0$), it will eventually settle into an energy minimum corresponding to an asymptotically stable equilibrium. The convexity requirement, together with \cref{eq:prop_convex_requirements}, guarantees the existence of a unique strict and global minimum at the origin. Additionally, it ensures that the energy for states at infinity is unbounded, i.e., $\hamiltonian(\bsx)\to\infty$ as $\norm{\bsx}\rightarrow\infty$. Thus, no finite state has more energy than a state at infinity and all trajectories must be bounded. As a result, all solutions eventually converge to the global energy minimum. 

For conservative systems with $\bsR=\bszero$, energy is not dissipated, and attractive equilibria cannot exist. Therefore, a conservative system cannot possess asymptotically stable equilibria. Nevertheless, the minimum in the energy still represents a stable equilibrium, and all trajectories stay bounded.
The mixed case, where $\bsR\succeq0$, only allows the same conclusions as the conservative case. However, in order for a solution $\bsx(t)$ not to converge to the origin, there must be a time $t^*\geq t_0$ such that $\bsR(\bsx(t))\partdiff{\hamiltonian}{\bsx}(\bsx(t))=\bszero\,\forall t\geq t^*$. Unless the dissipation matrix vanishes in a large region in phase space, it is unlikely that a system with $\bsR\succeq0$ is not globally asymptotically stable.

\subsection{Neural network model architecture}

Having established the theoretical requirements for stability of \gls{PHS}, we now outline how these conditions can be met with a dedicated \gls{NN} model. We call the resulting architecture \sPHNN. Its computation graph is illustrated in \cref{fig:sPHNN_architecture}.

\begin{figure}[t]
        \begin{center}
        \resizebox{\columnwidth}{!}{                \inputTikzWithExternalization{computation_graph}{tikz/computation_graph.tex}
    }
        \caption{Computation graph of \sPHNN. The output of the \glsxtrshortpl{FFNN} parametrizing $\bsJ$, $\bsL$, and $\bsG$ are reshaped to be skew-symmetric, lower triangular, and rectangular matrices, respectively.}
    \label{fig:sPHNN_architecture}
    \end{center}
    \vskip -0.5 cm
\end{figure}

\paragraph{Hamiltonian}
To fulfill the constraints of \cref{prop:stability_requirements}, the Hamiltonian $\hamiltonian(\bsx)$ is represented by a \gls{FICNN} \cite{amos2017}. \glspl{FICNN} can approximate any convex function \cite{chen2018a}. They are defined by the recurrence relation
\begin{equation}\label{eq:FICNN}\begin{aligned}
    \bsz_1 &= \sigma_0\left(\bsW_0\bsx+\bsb_0\right),\\
    \bsz_{i+1} &= \sigma_i\left(\bsU_i\bsz_i+\bsW_i\bsx+\bsb_i\right) ,\; i=1,2,...,k-1,\\
    f(\bsx) &= z_k,
\end{aligned}\end{equation}
with the pass-through weight matrices $\bsW_i$ that map the input $\bsx$ directly to the activation of the $k-1$ hidden layers, bias vectors $\bsb_i$, and layer outputs $\bsz_i$. 
If the weight matrices $\bsU_i$ have non-negative components, the first activation function $\sigma_0$ is convex, and all subsequent $\sigma_i$ are convex and non-decreasing, then the output $f(\bsx)$ is convex in the input~$\bsx$. 
Since the Hamiltonian should be twice continuously differentiable, the same must hold for the activation functions. One suitable choice is the softplus activation.
The conditions for stability in \cref{eq:prop_convex_requirements} are fulfilled by representing the Hamiltonian $\hamiltonian$ with a normalized \gls{FICNN}:
\begin{equation}\label{eq:hamiltonian_normalization}
    \hamiltonian(\bsx) = f(\bsx) \underbrace{- f(\bsx_0) - \partdiff{f}{\bsx}\bigg\vert_{\bsx_0}^{\intercal}\Delta\bsx}_{f_\text{norm}(\bsx,\bsx_0)} + \underbrace{\epsilon\norm{\Delta\bsx}^2\vphantom{\partdiff{f}{\bsx}\bigg\vert_{\bsx_0}}}_{f_\text{reg}(\bsx,\bsx_0)}\,,
\end{equation}
with $\Delta\bsx=\bsx-\bsx_0$ and $\epsilon>0$. The Hamiltonian $\hamiltonian$ and its gradient vanish at $\bsx_0$ due to the normalization term $f_\text{norm}$. Moreover, the regularization term $f_\text{reg}$ containing an arbitrarily small $\epsilon>0$ ensures positive definiteness of the Hamiltonian's Hessian. 
However, in practice, $f_\text{reg}$ can be omitted since the \gls{FICNN} provides enough bias towards local strict convexity. After training, the stability guarantee can then be recovered by verifying the Hamiltonian's positive definiteness at $\bsx_0$. 
Choosing $\bsx_0=\bszero$ fulfills all requirements of \cref{prop:stability_requirements}. However, the presented approach is more general. As the assumption that the equilibrium is located at the origin is only made for ease of notation, the normalization approach in \cref{eq:hamiltonian_normalization} can position the stable equilibrium anywhere in phase space and is not restricted to the origin. If the equilibrium position is unknown, then it can be inferred from data by optimizing $\bsx_0$ during training.

\paragraph{Structure~matrix}
The structure matrix $\bsJ: \bbR^n  \mapsto  \bbR^{n\times n}$ is parameterized by a \gls{FFNN}. 
As $\bsJ$ is constrained to be skew-symmetric, the matrix has only ${k_J=n(n-1)/2}$ independent entries. 
Therefore, the \gls{FFNN} outputs a vector $\bsc_J\in\bbR^{k_J}$ which is then mapped to the space of skew-symmetric matrices via a linear transformation $\bsJ=\bsT\bsc_J,\; \bsT\in\{-1, 0, 1\}^{n\times n\times k}$.

For some applications, $\bsJ$ is independent of $\bsx$, and thus, learning a constant $\bsJ$ is sufficient. 
In particular, this is the case when the conservative dynamics of the system under consideration are Hamiltonian in the given coordinates.
Furthermore, by fixing $\bsJ=\symplecticmatrix$, where 
\begin{equation}
    \symplecticmatrix = \begin{bmatrix}
        \bszero & \identity \\ -\identity & \bszero
    \end{bmatrix}
\end{equation}
is the symplectic matrix (and choosing $\bsR=\bsG=\bszero$), the \sPHNN reduces to a \gls{HNN}.

\vspace*{-2mm}

\paragraph{Dissipation matrix}
Similar to the structure matrix, the dissipation matrix $\bsR: \bbR^n\mapsto\bbR^{n\times n}$ is parameterized by a \gls{FFNN}. To ensure positive definiteness (or semi-definiteness), the Cholesky factorization $\bsR=\bsL\bsL^{\intercal}$ is utilized. Here, $\bsL$ is a lower triangular matrix with positive (or non-negative) elements along the main diagonal \cite{horn2017}. Similar to the skew-symmetric case of $\bsJ$, the output ${\bsc_R\in\bbR^{k_R}}$, ${k_R=n(n+1)/2}$ of the \gls{FFNN} for $\bsR$ represents the components of $\bsL$. 
The softplus function is applied to the diagonal elements of $\bsL$, ensuring the positive values. For semi-definiteness, the absolute value function is used instead.
Analogously to the structure matrix, simple dissipative dynamics might already be sufficiently described with a constant $\bsL$.

\vspace*{-2mm}

\paragraph{Input matrix} 
Unlike $\bsJ$ and $\bsR$, the input matrix ${\bsG: \bbR^n\mapsto \allowbreak \bbR^{n\times m}}$ is not subject to any constraints. It is, therefore, directly expressed by a \gls{FFNN} by reshaping the network's output vector $\bsc_G\in\bbR^{k_G},\,k_G=nm$ into a matrix. Similar to the other matrices, a constant version of the input matrix that does not depend on the state vector can be used.

\subsection[Training stable port-Hamiltonian neural networks]{Training stable port-Hamiltonian\\neural networks}
There are two fundamentally different approaches for training \sPHNNs: \emph{Derivative} and \emph{trajectory fitting}. 
The former directly compares the predicted state derivatives $\dot\bsx$ from \cref{eq:isphs_evolution} to the true derivatives. 
Although this approach is fast, it requires the true time derivatives to be available as training data, which may not always be the case. Furthermore, derivative fitting is not applicable if augmented states \cite{dupont2019} are used, i.e., when the state vector $\bsx$ is padded with additional dimensions compared to the observable states in the training data to enrich the representable dynamics. 
Additionally, a model proficient in predicting the time derivative of the state vector does not necessarily produce accurate predictions for trajectories $\bsx(t)$, as small derivative errors can accumulate exponentially when integrating an ODE \cite{hairer2009}.
On the other hand, trajectory fitting directly compares integrated model trajectories $\bsx(t)$ with ground-truth trajectories, ensuring long-term accuracy.
However, the optimization process necessitates either the propagation of the loss gradients through the ODE solver or the use of the adjoint sensitivity method \cite{chen2018}, which computes the gradients by solving a modified ODE. In both cases, trajectory fitting incurs significant computational costs due to gradient propagation and model integration, making trajectory fitting much slower than derivative fitting.

A distinct advantage of \sPHNNs is the favorable influence the stability constraint exerts on the training process when using trajectory fitting. 
\CapNODEs and similar architectures can suffer from exploding gradients, where small parameter changes lead to large loss gradients. 
This can slow the training process or prevent it from converging. 
A potential cause is the crossing between the boundaries of local basins of attraction during training, leading to significant differences in long-term behavior \cite{pascanu2013}. 
Since \sPHNNs have only one global basin of attraction, this source of training difficulties is eliminated. 
In our numerical experiments, we observe good convergence of the training error of \sPHNNs where \NODEs struggle without the use of regularisation terms or gradient clipping.
