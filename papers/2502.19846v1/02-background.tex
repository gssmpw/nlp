\section{Background on Causal Inference}
\label{sec:background-causal} 



 \newtextold{In this section, we 
 %formalize the notion of {\em Average Treatment Effect and understand the 
 review the basic concepts and key assumptions for inferring the effects of an intervention on the outcome on collected datasets without performing randomized controlled experiments. 
We use {\em Pearl's graphical causal model} for {\em observational causal analysis} \cite{pearl2009causal} to define these concepts.}


\par
\paratitle{Causal Inference and Causal DAGs} The primary goal of causal inference is to model causal dependencies between attributes and evaluate how changing one variable (referred to as intervention) would affect the other.
Pearl's Probabilistic Graphical Causal Model \cite{pearl2009causal} can be written as a tuple $(\exo, \edvar, Pr_{\exo}, \psi)$, where $\exo$ is a set of {\em exogenous} variables, $\Pr_{\exo}$ is the joint distribution of \exo, and $\edvar$ is a set of observed {\em endogenous variables}.
Here $\psi$ is a set of structural equations that encode dependencies among variables. The equation for $A \in \edvar$ takes the following form:
%that encode the dependencies among the variables.  These equations are of the form 
$$\psi_{A}: 
\dom(Pa_{\exo}(A)) {\times} \dom(Pa_{\edvar}(A)) \to \dom(A)$$
Here $Pa_{\exo}(A) {\subseteq} {\exo}$ and $Pa_{\edvar}(A) {\subseteq} \edvar \setminus \{A\}$ respectively denote the exogenous and endogenous parents of $A$. A causal relational model is associated with a directed acyclic graph ({\em causal DAG}) $G$, whose nodes are the endogenous variables $\edvar$ and there is a directed edge from $X$ to $O$ if  $X {\in} Pa_{\edvar}(O)$. The causal DAG obfuscates exogenous variables as they are unobserved. %Any given set of values for the exogenous variables completely determines the values of the endogenous variables by the structural equations (we do not need any known closed-form expressions of the structural equations in this work). 
The probability distribution $\Pr_{\exo}$ on exogenous variables $\exo$ induces a probability distribution  
on the endogenous variables $\edvar$ by the structural equations $\psi$.  A causal DAG can be constructed by a domain expert as in the above example, or using existing {\em causal discovery} algorithms~\cite{glymour2019review}. 



\begin{figure}
    \centering
    \small
    \begin{tikzpicture}[node distance=0.6cm and 1cm, every node/.style={minimum size=0.5cm}]
        \tikzset{vertex/.style = {draw, circle, align=center}}

        \node[vertex] (Ethnicity) {\bf\scriptsize{{Ethnicity}}};
        \node[vertex, right=0.3cm of Ethnicity] (Gender) {\bf{\scriptsize{Gender}}};
        \node[vertex, right=0.3cm of Gender] (Age) {\bf{\scriptsize{Age}}};
        \node[vertex, below=0.3cm of Gender] (Role) {\bf{\scriptsize{Role}}};
        \node[vertex, right=0.3cm of Role] (Education) {\bf{\small{\scriptsize{Education}}}};
        \node[vertex, below=0.3cm of Role] (Salary) {\bf{\scriptsize{Salary}}};

        \draw[->] (Ethnicity) -- (Salary);
        \draw[->] (Gender) -- (Role);
        \draw[->] (Age) -- (Role);
         \draw[->] (Education) -- (Role);
           \draw[->] (Education) -- (Salary);
             \draw[->] (Ethnicity) -- (Education);
                \draw[->] (Ethnicity) -- (Role);
             \draw[->] (Gender) -- (Education);
               \draw[->] (Age) -- (Education);
                 \draw[->] (Role) -- (Salary);
        \draw[->] (Gender) to[bend right] (Salary);
        \draw[->] (Age) -- (Salary);
    \end{tikzpicture}
    \caption{Partial causal DAG for the Stack Overflow dataset.}
    \label{fig:causal_DAG}
\end{figure}



 \begin{example}
Figure \ref{fig:causal_DAG} depicts a partial causal DAG for the SO dataset over the attributes in Table \ref{tab:data} as endogenous variables (we use a larger causal DAG with all 20 attributes in our experiments). 
  Given this causal DAG, we can observe that the role that a coder has in their company depends on their education, age gender and ethnicity.
\end{example}
\par


\par
\paratitle{Intervention} In Pearl's model, a treatment $T = t$ (on one or more variables) is considered as an {\em intervention} to a causal DAG by mechanically changing the DAG such that the values of node(s) of $T$ in $G$ are set to the value(s) in $t$, which is denoted by $\doop(T = t)$. Following this operation, the probability distribution of the nodes in the graph changes as the treatment nodes no longer depend on the values of their parents. Pearl's model gives an approach to estimate the new probability distribution by identifying the confounding factors $Z$ described earlier using conditions such as {\em d-separation} and {\em backdoor criteria} \cite{pearl2009causal}, which we do not discuss in this paper.


\par
\paratitle{Average Treatment Effect} The effects of an intervention are often measured by evaluating
% \par
% \paratitle{Causal inference, Treatment, ATE, and CATE}
% \newtextold{One of the primary goals  of {\em causal inference} is to estimate the effect of making a change in terms of a {\em treatment} $T$ (often referred to as an intervention)
% on the outcome $O$. 
% %A variable that is modified is often referred to as the treatment variable $T$ and the metric used to captures 
% The effect of treatment $T$ on outcome $O$ is measured by 
% %is known as 
{\em Conditional Average treatment effect (CATE)}, 
%a {\em treatment variable} $T$ on an outcome variable $O$ (e.g., what is the effect of higher \verb|Education| on \verb|Salary|). 
measuring the effect of an intervention on a subset of records~\cite{rubin1971use,holland1986statistics} by calculating the difference in average outcomes between the group that receives the treatment and the group that does not (called the {\em control} group), providing an estimate of how the intervention by $T$ influences an outcome $O$ for a given subpopulation. 
% Mathematically,
% \begin{equation}
%     %{\small ATE(T,O) = \mathbb{E}[O \mid \doop(T=1)] -      \mathbb{E}[O \mid \doop(T=0)]}
%     {\small ATE(T, O) = \mathbb{E}[O \mid \doop(T=1)] -  
%     \mathbb{E}[O \mid \doop(T=0)]}
% \label{eq:ate}
% \end{equation}
% In our work, where the treatment with maximum effect may vary among different subpopulations, we are interested in computing the \emph{Conditional Average Treatment Effect} (CATE), which measures the effect of a treatment on an outcome on \emph{a subset of input units}~\cite{rubin1971use,holland1986statistics}. 
Given a subset of the records defined by (a vector of) attributes $B$ and their values $b$, 
%g {\in} \Qagg(\db)$ defined by a predicate $G {=} g$ 
we can compute $CATE(T,O \mid B = b)$ as:
{
\begin{eqnarray}    
    %CATE(T,O \mid G=g) = \mathbb{E}[O \mid \doop(T=1)&, G=g] -  \mathbb{E}[O \mid \doop(T=0), G=g] 
   % CATE(T,O \mid B = b) = 
    \mathbb{E}[O \mid \doop(T=1), B = b] -  
    \mathbb{E}[O \mid \doop(T=0), B = b]\label{eq:cate}
\end{eqnarray}
}
Setting $B=\phi$ is equivalent to the ATE estimate.
The above definitions assumes that the treatment assigned to one unit does not affect the outcome of another unit (called the {Stable Unit Treatment Value Assumption (SUTVA)) \cite{rubin2005causal}}\footnote{This assumption does not hold for causal inference on multiple tables and even on a single table where tuples depend on each other.}. 


The ideal way of estimating the ATE and CATE is through {\em randomized controlled experiments}, 
where the population is randomly divided into two groups (treated and control, for binary treatments): 
%treated group that receives the treatment and control group that does not (denoted by 
%{the \em treated} group 
denoted by 
$\doop(T = 1)$ 
%for a binary treatment)  (the {\em control} group, 
and $\doop(T = 0)$ resp.)~\cite{pearl2009causal}.
%\sr{edited up to here, going to read the rest first, this section should not look like causumx}
%\par
%\par
However, randomized experiments cannot always be performed due to ethical or feasibility issues. In these scenarios, observational data is used to estimate the treatment effect, which requires the following additional assumptions. 
% {\em Observational Causal Analysis} still allows sound causal inference under additional assumptions. Randomization in controlled trials mitigates the effect of {\em confounding factors}, i.e., attributes that can affect the treatment assignment and outcome. Suppose we want to understand the causal effect of \verb|Education| on \verb|Salary| from the SO dataset.  %in Example~\ref{ex:running_example}. 
% We no longer apply Eq. (\ref{eq:ate}) since the values of \verb|Education| were not assigned at random in this data, and obtaining higher education largely depends on other attributes like \verb|Gender|, \verb|Age|, and \verb|Country|. 
% Pearl's model provides ways to account for these confounding attributes $Z$ to get an unbiased causal estimate from observational data under the following assumptions ($\independent$ denotes independence):
% \vspace{-2mm}
\newtextold{
The first assumption is called {\em unconfoundedness} or {\em strong ignorability}  \cite{rosenbaum1983central} says that the independence of outcome $O$ and treatment $T$ conditioning on a set of confounder variables  (covariates) $Z$, i.e.,
%\begin{eqnarray}
 $    O \independent T | Z {=} z$.
 %\label{eq:unconfoundedness}
%\end{eqnarray}
The second assumption called {\em overlap or positivity} says that there is a chance of observing individuals in both the treatment and control groups for every combination of covariate values, i.e., 
%\begin{eqnarray}
   $ 0 < Pr(T {=} 1 ~~|~~Z {=} z)< 1 $.
   %\label{eq:overlap}
%\end{eqnarray}
}
%\sg{Is this overlap or positivity? maybe both are the same?} \sr{yeah - same - from Google AI - The overlap assumption, also known as the positivity assumption, is a key assumption in causal inference that states that there is a chance of observing individuals in both the treatment and control groups for every combination of covariate values.}
% The above conditions are known as {\em Strong Ignorability} in Rubin's model \cite{rubin2005causal}.
The unconfoundedness assumption requires that the treatment $T$ and the outcome $O$ be independent when conditioned on a set of variables $Z$. In SO, assuming that only $Z$ =\{\verb|Gender|, \verb|Age|, \verb|Country|\} affects $T = $ \verb|Education|, if we condition on a fixed set of values of $Z$, i.e., consider people of a given gender, from a given country, and at a given age, then $T = $ \verb|Education| and $O = $ \verb|Salary| are independent. For such confounding factors $Z$,  Eq. (\ref{eq:cate}) reduces to the following form 
(positivity 
gives the feasibility of the expectation difference): 
 \vspace{-1mm}
{\small
\begin{flalign}    
% \begin{eqnarray}
   % % & ATE(T,O) = \mathbb{E}_Z \left[\mathbb{E}[O \mid T=1, Z = z] -  
   %  \mathbb{E}[O \mid T=0, Z = z] \right] \label{eq:conf-ate}\\
 & CATE(T,O {\mid} B {=} b) {=} \nonumber
    \mathbb{E}_Z \left[\mathbb{E}[O {\mid} T{=}1, B {=} b, Z {=} z] {-}  
    \mathbb{E}[O {\mid} T{=}0, B {=} b, Z {=} z]\right]\label{eq:conf-cate}
\end{flalign}
% \end{eqnarray}
}
% \vspace{-4mm}
This equation contains conditional probabilities and not $\doop(T = b)$, which can be estimated from an observed data. 
Pearl's model gives a systematic way to find such a $Z$ when a causal DAG is available. 



