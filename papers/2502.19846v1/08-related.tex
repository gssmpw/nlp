\section{Related work}
\label{sec:related}


Table \ref{tab:relatedwork} outlines the key distinctions between \sysName\ and prior work. The columns in bold emphasize our key contributions: we generate \textbf{causality-based} prescription rules aimed at {improving outcomes} for the \textbf{entire datasets}, while also \textbf{considering fairness}. In contrast, other approaches either produce non-causal rules (as shown in the Causal column), target only subsets of the data (as indicated in the Entire dataset column), or disregard fairness considerations (as highlighted in the Fairness column).






\begin{table}[t]
\small
\centering
\caption{Positioning of our framework w.r.t. previous work.}
\label{tab:relatedwork}
% \vspace{-3mm}
\footnotesize{
\begin{tabular}{|p{25mm}|c|c|c|c|c|c|c|}
    \hline
 
\multicolumn{2}{|c|}{{Related Work}}  & {Causal} &   {\begin{tabular}{@{}c@{}}Fairness\end{tabular}} & {\begin{tabular}{@{}c@{}} Entire\\dataset\end{tabular}} \\%\cline{3-8}
    \hline
\multirow{4}{*}{\begin{tabular}{@{}l@{}}Aggregate Query\\ Result Explanation\end{tabular}}&\cite{ma2023xinsight}&\textbf{\checkmark}&\xmark&\xmark\\
&\cite{salimi2018bias, youngmann2022explaining,DBLP:journals/pacmmod/YoungmannCGR24,roy2014formal,meliou2009so}&\textbf{\checkmark}&\xmark&\checkmark\\

&\cite{li2021putting,miao2019going}&\xmark&\xmark&\xmark\\

&\cite{wu2013scorpion}&\xmark&\xmark&\checkmark\\


\hline
\multirow{2}{*}{\begin{tabular}{@{}l@{}}Interpretable\\ Prediction Models\end{tabular}} 
% {\begin{tabular}{@{}c@{}}Interpretable Pred.\\Models\end{tabular}} 
& \cite{chen2018optimization,lakkaraju2016interpretable}&\xmark&\xmark&{\bf \checkmark}\\
&&&&\\
\hline
\multirow{2}{*}{\begin{tabular}{@{}l@{}}Multi dimensional\\ data aggregation\end{tabular}} &\cite{pastor2021looking,surve2024example,pastor2023hierarchical}&\xmark&\checkmark&\checkmark\\ 


&\cite{el2014interpretable,kim2020summarizing}&\xmark&\xmark&\checkmark\\

    \hline
    \multicolumn{2}{|c|}{\sysName}
\textbf{}&\textbf{\checkmark}&\textbf{\checkmark}&\textbf{\checkmark}\\
\hline
\end{tabular}
}
% \vspace{-4mm}
\end{table}

\paragraph*{Rule mining}
Association rule mining has been extensively studied \cite{kumbhare2014overview} and is used to identify relationships between items that frequently co-occurring in datasets. These techniques are applied across various fields, such as data analysis and outcome improvement. Notable algorithms include STEM~\cite{girotra2013comparative}, FP-Growth~\cite{kaur2013performance}, AIS~\cite{zhao2003association}, and the Apriori algorithm~\cite{agrawal1994fast}. We leverage the Apriori algorithm to identify sufficiently large subpopulations for which we will generate causal interventions.
Rule-based interpretable prediction models~\cite{yang2017scalable,chen2018optimization,lakkaraju2016interpretable} often leverage association rule mining to generate predictive rules~\cite{lakkaraju2016interpretable,lawless2023interpretable}, with the goal of balancing high predictive accuracy with interpretability~\cite{sagi2021approximating,schielzeth2010simple,lou2013accurate,kim2014bayesian,lawless2023interpretable}. 

Recent work has proposed generating rules based on causal relationships.
In \cite{plecko2022causal}, a framework was introduced to address biases in the data for fair causal analysis. \reva{Other studies have explored the integration of fairness criteria into causal analysis \cite{plecko2023causal,zhang2022adaptive}.}  \cite{sun2021treatment} proposed a method to optimally allocate treatments with uncertain costs that vary based on confounders. Related research has also focused on estimating heterogeneous treatment effects~\cite{wager2018estimation, xie2012estimating,wang2022causal}. However, these approaches assume both treatment and outcome variables are known. In contrast, we assume only the outcome variable is provided and aim to identify treatments that influence the outcome for different subpopulations, potentially yielding different treatments for each group. Our approach ensures the rules apply broadly while maintaining fairness for minority groups.


We adapt the method proposed in \cite{DBLP:journals/pacmmod/YoungmannCGR24} called CauSumX. CauSumX is designed to identify the treatment with the highest causal effect on the outcome for a given subpopulation, generating causal explanations for aggregate queries. A main difference is that CauSumX does not consider fairness. We empirically show that using CauSumX to generate prescription rules can lead to significant disparities between protected and non-protected populations (See \cref{exp:quality}).
Another main difference is that CauSumX considers the \emph{aggregate view} to generate explanations, whereas we consider the entire data, thus, the search space is significantly larger.
Our primary contribution lies in introducing fairness constraints on the generated rules, making the necessary adjustments to the algorithm to scale, and conducting an experimental study to demonstrate the importance of fairness constraints.



% we adapt CauSumX to incorporate fairness and coverage constraints, offering a novel solution to identify a set of rules that adhere to these constraints.}
% \sr{this paragraph should move to the first paragraph and should be edited. We should not say "we adapt causumx", and should highlight that it does not generate prescriptions (like being male could be a treatment) but causal explanations, and does not consider fairness.}


% \brit{TODO: add a paragraph here}
% \begin{itemize}
%     \item Experimental Evaluation of Individualized Treatment Rules \cite{imai2023experimental}
  




% \end{itemize}



\paragraph*{Fairness in data management}
% AI systems are increasingly being deployed in areas like employment, healthcare, and education \cite{shailaja2018machine,kuvcak2018machine}. However, alongside this growing adoption, concerns are rising about the potential harm these systems could inflict on already marginalized subpopulations. The risk of discriminatory outcomes has led to significant interest in methods for measuring and ensuring "algorithmic fairness"~\cite{kleinberg2018algorithmic}.
\reva{Algorithmic fairness, especially in the context of predictions by ML algorithms for high-stake decision making, has been a prominent topic in ML and AI (e.g., \cite{agarwal2019fair,mehrabi2021survey,pessach2022review,caton2024fairness,roh2020fairbatch,DBLP:conf/icml/Roh0WS23,jain2024algorithmic,smith2022recsys,ekstrand2019fairness,gao2020counteracting}).} Popular notions of fairness include group and individual fairness~\cite{binns2020apparent,garcia2021maxmin,DBLP:journals/pacmmod/ZhangTPCW23,somerstep2024algorithmic}. Group fairness (measured as statistical parity or equalized odds) ensures that the decision-making process is fair to the protected group but may be unfair towards any specific individual. In contrast, individual notions of fairness enforce that the decisions are fair towards every individual. We refer the reader to Section~\ref{subsec:fairness_constraint} for more details. 
% \sr{need a mention of group/individual fairness, different fairness definitions, and some citations from ML/AI here, and a fwd reference to sec 4.5.} 
In recent years, fairness has emerged as a key consideration in
data management research~\cite{stoyanovich2020responsible,galhotra2022causal,DBLP:journals/pvldb/TaeZPRW24,DBLP:journals/pacmmod/ZhangTPCW23,jain2024algorithmic}. This includes ensuring fairness during data acquisition~\cite{asudeh2019assessing,nargesian2021tailoring,Nargesian2022}, improving data cleaning processes to promote fairness~\cite{guha2024automated,salimi2019interventional}, and achieving fairness in ranking and in database queries~\cite{zehlike2022fairness,zehlike2022fairness2,li2023query}. \reva{Techniques to ensure fairness of allocated resources~\cite{majumdar2024carma, ehyaei2023robustness} can also be extended to study scenarios where the available resources may be diverse and constrained.
One of our contributions is the introduction of novel definitions of group and individual fairness for causal analysis. We defer the extension of our work to other definitions of fairness to future work.}





\paragraph*{Causal inference in data management} \newtextold{Causality, used as a generic term of cause-effect analysis, has been used in different contexts 
%a major theme 
in data management research
}~\cite{meliou2009so, meliou2010complexity,salimi2020causal,10.14778/3554821.3554902}. 
%It has been utilized in various tasks including 
This includes data discovery~\cite{galhotra2023metam,youngmann2023causal,gong2024nexus,santos2021correlation,huang2023fast}, data cleaning~\cite{pirhadi2024otclean,salimi2019interventional}, query result explanation~\cite{salimi2018bias,DBLP:journals/pacmmod/YoungmannCGR24,youngmann2022explaining,ma2023xinsight, roy2014formal}, hypothetical reasoning~\cite{galhotra2022hyper}, and  system diagnostics~\cite{markakis2024sawmill,causalsim,gudmundsdottir2017demonstration}. \newtextold{We use the interventional notion of causal inference on observational data from AI and Statistics \cite{pearl2009causal, rubin2005causal} to define our prescription rules (more in Sections~\ref{sec:background-causal} and \ref{subsec:utility}). } 
%In this study, we utilize causal inference 
to design interventions that improve an outcome of interest.

% ensuring the recommendations apply to a broad population and fair to minority groups.


\paragraph*{Aggregate query result explanation}
A substantial line of research has focused on using provenance to explain aggregate SQL query results~\cite{bidoit2014query,chapman2009not,meliou2010complexity,meliou2009so,DeutchFG20,lee2020approximate,ten2015high,li2021putting}. Other explanation methods include (non-causal) interventions \cite{wu2013scorpion,roy2014formal,roy2015explaining,tao2022dpxplain,DBLP:journals/pvldb/DeutchGMMS22}, and counterbalancing patterns \cite{miao2019going}. Recent work \cite{salimi2018bias,youngmann2022explaining,DBLP:journals/pacmmod/YoungmannCGR24,ma2023xinsight} has proposed methods that use causal inference to explain query results. 




\paragraph*{Multi dimensional data aggregation}
Previous work on multidimensional data aggregation developed methods that extend the traditional drill-down and roll-up operators to find the most interesting data parts for exploration~\cite{agarwal1999cube,sathe2001intelligent,joglekar2017interactive,DBLP:journals/pvldb/YoungmannAP22,10.1145/3448016.3457259}.
Other works have focused on assessing the similarity between data cubes~\cite{baikousi2011similarity}), or discovering intriguing data visualizations~\cite{vartak2015seedb,zhang2021viewseeker}. 

Part of our goal is to identify subpopulations for which we can generate recommendations. We utilize existing solutions whenever applicable (e.g., we use the Apriori algorithm~\cite{agrawal1994fast} to find sufficiently large subpopulations) and develop novel methods when necessary.










% Fairness vs. equity \url{https://onlinepublichealth.gwu.edu/resources/equity-vs-equality/}

% References to ML-based causal inference:
% \begin{itemize}
%     \item Course from Stanford (lectures by Stefan Wager, Susan Athey): \url{https://www.youtube.com/@stanfordgsb/search 
% https://www.youtube.com/watch?v=0GK6IZut6K8}
% \item Instrumental Variable IV + Doubly robust ML + heterogeneous treatment effect \url{https://proceedings.neurips.cc/paper_files/paper/2019/hash/3b2acfe2e38102074656ed938abf4ac3-Abstract.html }
% \item Fairness definitions: \url{https://courses.cs.duke.edu/spring23/compsci590.1/Lectures/Lecture-6-causal-fairness.pdf}
% \item Causal Adversarial Perturbations for Individual Fairness and Robustness in
% Heterogeneous Data Spaces \cite{ehyaei2024causal}

% \end{itemize}


% \begin{verbatim}
%     \url{https://dblp.org/pid/148/1397.html}

% https://dblp.org/pid/85/9005.html

% https://dblp.org/pid/124/7179.html

% https://dblp.org/pid/62/6936.html

% https://pubsonline.informs.org/doi/10.1287/ijoc.2021.1143


% https://dblp.org/pid/59/8761.html


% \end{verbatim}
