
\section{The \sysName\ Algorithm}
\label{sec:algo}



A brute-force approach, which considers all grouping and intervention patterns to form prescription rules, results in long runtimes (as we show in Section \ref{exp:scalability}). Instead, we propose a more efficient algorithm, called \sysName\ (\underline{Fair} \underline{CA}usal \underline{P}rescription), which avoids generating every possible prescription rule. \sysName\ can be adapted for any variant of the \probName\ problem. For simplicity, we first describe \sysName\ for the case with SP group fairness and group coverage constraints. We then explain how it can be modified to accommodate other variants.



Our algorithmic framework is outlined in ~\cref{algo:full_algo}.
\sysName\ consists of three parts: (1) generating grouping patterns by using the Apriori algorithm~\cite{agrawal1994fast} (line \ref{line:init}), (2) identifying promising intervention patterns for each grouping pattern by using a lattice traversal approach \cite{asudeh2019assessing}, and (3) finding a set of prescription rules using a greedy approach. We leverage existing solutions (e.g., \cite{asudeh2019assessing, agrawal1994fast,pastor2021looking,DBLP:journals/pacmmod/YoungmannCGR24}) where applicable, and develop novel techniques where necessary.
Specifically, the first step follows the same approach as CauSumX ~\cite{DBLP:journals/pacmmod/YoungmannCGR24}, while the second and third steps introduce novel methods.
 

% \sr{need to say here which part is novel, and perhaps compare with causumx}.

\reva{We show that the prescription rule selection problem is
NP-hard even in simple settings (proofs deferred to the Appendix), and therefore developing effective heuristics considering several constraints is non-trivial.
%We note that \sysName\ lacks theoretical guarantees due to its design, 
\sysName\ avoids generating all possible rules  (as their number grows exponentially with the database size) and therefore does not perform an exhaustive search and may not return an optimal answer.} If steps 1 and 2 were replaced by a brute-force approach that generates all rules, then a greedy approach for selecting a ruleset could approximate the optimal solution for certain problem variants, as the objective is a non-negative, monotone submodular function (even with a rule coverage or individual fairness constraints which are matroid constraints). However, 
other constraints are harder to satisfy. Future work will explore the complexity of various problem variants and establish theoretical bounds for finding approximate solutions. 

\reva{Despite the fact \sysName\ does not provide formal guarantees for the prescription ruleset selection problem, we emphasize that each selected rule represents an intervention that is statistically significant. Specifically, based on causal
analysis, the expected utility reflects the anticipated average increase in the outcome for the specific subpopulation to which the
rule applies.}
% For instance, when a group coverage constraint is imposed, simply finding a solution that satisfies this constraint, even without maximizing utility, is already NP-hard~\cite{DBLP:journals/pacmmod/YoungmannCGR24}. 



% \sr{this does not contradict the previous statement with `However' since the previous statement talks about approximation, next one np-hardness.. I think we should refer to the approx ideas based on matroid constraints from previous section.}



% CauSuX aims to find a summarized causal explanation to SQL queries and it operates in three steps as follows: (1) it employs the Apriori algorithm~\cite{agrawal1994fast} to mine frequent grouping patterns (defined by attributes having FDs with the grouping attribute of the considered query) that are short. (2) To mine treatment patterns (which are non-monotone for CATE) it adopts a lattice traversal approach and uses a greedy heuristic materializing only promising treatment patterns. (3) Lastly, it utilizes Linear Programming (LP) to obtain a set of explanation patterns.
% Our proposed \algoName\ algorithm incorporates the following adaptations:

% \begin{enumerate}
% \item We begin by applying the Apriori algorithm to all non-actionable attributes, as there is no specific query to address.
% \item We redefine promising treatment patterns to ensure they are both fair and have high utility.
% \item We introduce a novel Integer Linear Program (ILP) to find a solution. Additionally, we use an LP-relaxation of this ILP and a standard randomized rounding algorithm to select a set of prescription rules.
% \end{enumerate}

% The proposed algorithm can be adjusted to meet the various (group and individual) fairness constraints, as well as for the rule or group coverage constraints. 


%\setlength{\textfloatsep}{2px}
% \SetInd{1.3ex}{1.3ex}
\begin{algorithm}[t]
\footnotesize
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\LinesNumbered
\Input{A database relation \db, a protected group defined by the pattern $\pattern_p$ and an outcome variable $O$}
\Output{A set $\Phi$ 
of prescription rules.} \BlankLine
\SetKwFunction{NextBestExplanation}{\textsc{NextBestExplanation}}
\SetKwFunction{SolveLP}{\textsc{SolveLP}}
\SetKwFunction{Greedy}{\textsc{ApplyGreedy}}
\SetKwFunction{GetGroupingPatterns}{\textsc{GetGroupingPatterns}}
\SetKwFunction{ExplanationSummary}{\textsc{ExplanationSummary}}
\SetKwFunction{GetTreatment}{\textsc{GetIntervention}}
$\Phi \gets \emptyset$\;
% \tcc{\cref{subsec:grouping_patterns}: Mine grouping patterns.}
$\mathcal{G} \gets \GetGroupingPatterns (\db, O)$\tcp*{\cref{subsec:grouping_patterns}} \label{line:init}   
% \tcc{\cref{subsec:treatment_patterns}: Greedy mining of treatment patterns.}
 \For{$\pattern_g \in \mathcal{G}$}{\label{l:iterate-candidates}
    $\pattern_t \gets \GetTreatment(\pattern_g, O, \pattern_p, \db)$\tcp*{\cref{subsec:treatment_patterns}}\label{l:top-treatment}
    $\Phi \gets \Phi \cup (\pattern_g, \pattern_p)$\\
 }
 % \tcc{\cref{subsec:step_3}: Find a $k$-size explanation summary.}
  $\Phi \gets \Greedy(\Phi, O, \pattern_p)$\tcp*{\cref{subsec:step_3}}\label{l:step3}

\Return $\Phi$\label{line:returnTop}\\
\caption{The \sysName\ algorithmic framework.}\label{algo:full_algo}
\end{algorithm} 



\subsection{Step 1: Mining Grouping Patterns}
\label{subsec:grouping_patterns}
Considering every possible grouping pattern is infeasible as their number is exponential ($O(agrmax_{A_i {\in} \attrset}|\dom(A_i)|^{|\attrset|}$). 
Instead, as done in previous work~\cite{DBLP:journals/pacmmod/YoungmannCGR24,pastor2021looking}, we utilize the Apriori algorithm~\cite{agrawal1994fast} to generate candidate patterns. The Apriori algorithm gets a threshold $\tau$, and ensures that
the mined grouping patterns are present in at least $\tau$ tuples of $\db$.
The algorithm guarantees that each mined pattern covers at least $\tau$ tuples from $\db$, making them promising candidates for covering many tuples from $\db$. 



% Following \cite{DBLP:journals/pacmmod/YoungmannCGR24}, as a post-processing step, we discard redundant grouping patterns using a hash table. 


% If a rule coverage constraint is imposed, we set the threshold of the Apriori algorithm to ensure that each mined grouping pattern covers enough individuals. 



%\vspace{-3mm}
\subsection{Step 2: Mining Intervention Patterns}
\label{subsec:treatment_patterns}
Our next goal is to identify an intervention pattern $\patterninterv$ for each mined grouping pattern $\patterngroup$ that maximizes utility (i.e., treatments with the highest CATE for $\patterngroup$) while ensuring fairness to the protected group.
Unlike step 1, this step requires novel techniques for finding treatments that are both fair and have high utility.

 
Since the number of potential intervention patterns for $\patterngroup$ can be large (exponential in $|\attrset|$), we employ a greedy lattice-traversal~\cite{asudeh2019assessing,DeutchG19} approach, inspired by \cite{DBLP:journals/pacmmod/YoungmannCGR24,pastor2021looking}.
This allows us to materialize and assess the CATE only for promising patterns. 

Concretely, the space of all intervention patterns
can be represented as a lattice where nodes correspond to patterns and there is an edge between $\patterninterv^1$ and $\patterninterv^2$
if $\patterninterv^2$
can be obtained
from $\patterninterv^1$ by adding a single predicate. This lattice can be traversed in a top-down fashion. 
Since not all nodes correspond to treatments with a positive CATE, we only materialize nodes if all their parents have a positive CATE. 
We note that this might lead the algorithm to overlook certain relevant intervention patterns.
However, as shown in \cite{DBLP:journals/pacmmod/YoungmannCGR24}, combining patterns that exhibit a positive CATE is highly likely to result in an intervention with a positive CATE as well. 

When a group fairness constraint is imposed, instead of searching for the treatment with the highest CATE, we search for the treatment that is "fair" by that it maximizes CATE for both protected and non-protected groups, while minimizing disparities. 


To identify the most fair treatment, we define the \emph{benefit} of an intervention pattern as follows. Intuitively, we penalize the treatment based on the difference between the utility for the non-protected group and the utility provided to the protected group. The larger the difference, the lower the benefit of the treatment. Formally, the benefit of a rule $r = (\patterngroup,\patterninterv)$ is defined as: 
\vspace{-2mm}
\[
    benefit(r)=
\begin{cases}
   \frac{utility(r)}{1+utility_{\bar{p}}(r) - utility_p(r)},& \text{if } utility_{\bar{p}}(r) \geq utility_p(r)\\
   utility(r), &\text{otherwise}
\end{cases}
\]
\vspace{-3mm}

% \begin{example}
%     \brit{TODO: add an example}
% \end{example}

% \paragraph*{Optimizations}
We implement two optimizations to improve efficiency:
\textbf{(i)} we discard attributes that do not have a causal relationship with the outcome, since such attributes have no impact on CATE values. We can detect such attributes by utilizing the input causal DAG.
\textbf{(ii)} The process of extracting intervention patterns for each grouping pattern can be performed in parallel since this procedure is dependent only on the grouping pattern.



% \begin{figure}
%     \centering
%     \small
%     %\begin{footnotesize}
%     \begin{tcolorbox}[colback=white]
%     \vspace{-2mm}
%     \begin{equation*}
%     \begin{aligned}
%    & \max \quad \underbrace{\left(\sum_{j = 1}^l g_j \cdot w_j\right)}_{\text{utility objective}}  + \underbrace{\left (l - \sum_{j = 1}^l g_j \right )}_{\text{ size objective }}  \quad   
%    % + \underbrace{\left (m\cdot l^2 - \sum_{i = 1}^m\sum_{j = 1}^l t_{i,j} \cdot g_j \right )}_{\text{ overlap objective }}\quad 
%     \textrm{s.t.} \\
%     & \text{/* If tuple-i is covered by group-j, then group-j must be selected */}\\
%     % (1)~\sum_{j = 1}^l g_j \leq k, \quad \\
%     &  (1)~t_{ij} {\leq}  g_j ~\forall i = 1 \text{ to }m, j:t_i\in \pattern_j, t_{ij} = 0 \text{ otherwise}   \\
%     & \text{/* If group j is selected, at least some $t_{ij}$ should be 1, i.e.,} \\
%     & \text{ no group is selected redundantly */}\\
%     &(2a)~ g_j \leq \sum_{k} t_{ik}, \forall i,j \textit{ such that tuple i is in} \pattern_j\\%\text{// Each tuple should be covered by at least one rule if gj is 1.}\\
%     & \text{/* Each tuple-i can be  covered by at most one group  */}\\
%     &(2b)~ \sum_{j} t_{ij} \leq 1, \forall i \\%\text{// Each tuple should be covered by at least one rule if gj is 1.}\\
%      & (3) \text{ group coverage: } ~\sum_{i,j} t_{ij} \geq \theta_1 \cdot m, ~\sum_{\forall i,j : Tuple i \text{is protected}}^{} t_{ij} \geq \theta_2 \cdot m_p\quad \\
%      & (4) \text{ (SP) group fairness: } \\
%      &\left | \left ( \frac{1}{m_p}\sum_{i \text{ is protected}}\sum_j  (t_{i,j}  w_{j}^p) \right ) {-} \left ( \frac{1}{m}\sum_{i \text{ is not protected}}\sum_j  (t_{i,j}  {\cdot} w_{j}^{\bar{p}}) \right )  \right | {\leq} \epsilon\\
%       % & (3^2) \text{ BGL group fairness constraint} \left | \left ( \frac{1}{m_p}\sum_{i}^{m_p} \min_{g_j | g_j = 1} (p_{i,j} \cdot g_j \cdot w_{j,p}) \right )  \right | \geq \epsilon\\
%      & (5)~t_{ij},  g_j \in \{0, 1\} ~\forall i = 1 \text{ to }m, ~\forall j = 1 \text{ to }l  \text{such that tuple is in rule j}\\
%     \end{aligned}
%     % }
%     \end{equation*}
%     \vspace{-2mm}
%     \end{tcolorbox}
%     %\end{footnotesize}  
%     \vspace{-1mm}
%     \caption{Alternate option which is an ILP. \sg{One issue/feature: We suggest one rule per individual. The output of ILP is that recommendation also. This means we do not give option to an individual but the best rule.} }
%     \label{fig:lp}
% \end{figure}
% \sg{There is a problem that the denominator of SP needs variables. We need to change the definition or do some approximation.}

%\vspace{-2mm}
\subsection{Step 3: A Greedy Approach}
\label{subsec:step_3}
The final step involves finding a solution from the rules mined in Steps 1 and 2. We propose a greedy algorithm that optimizes the problemâ€™s objectives.
Intuitively, the algorithm operates as follows: at each iteration, it selects the next best rule that maximizes expected utility, benefit (as defined in \cref{subsec:treatment_patterns}), and coverage. Once the coverage constraints are met, the focus shifts to maximizing benefit and utility. The algorithm stops when the additional gain becomes negligible, as the number of rules is not predetermined.

Formally, the next best rule is determined as follows. Let $\{r_j\}_{j=1}^l$ denote the candidate rules and $R_i$ is the ruleset selected in the first $i$ iterations.
The score of a rule $r$ w.r.t $R_i$ is defined as:
\vspace{-3mm}
\begin{multline*}
\small
score(r){=} Coverage(R_i {\cup} \{r\}) + benefit(R_i {\cup} \{r\}) + ExpUtility(R_i {\cup} \{r\})
\end{multline*}
\vspace{-1mm}
The next best rule $r_{i+1}$ to add in case the coverage constraints are not met yet is defined as:
%\vspace{-3mm}
$r_{i+1}^* = argmax_{r_{i+1} \in \{r_j\}_{j=1}^l \setminus R_i} score(r_{i+1})$
In case the coverage constraints are met, ignore the coverage term. 
The algorithm stops at the first iteration $i$ where the score of the selected rule $r_i$ falls below a predefined threshold, indicating that the marginal gain from $r_i$ is negligible.





% Our study shows that intuitive combinatorial greedy algorithms targeting the objectives of our problem while meeting the fairness and coverage constraints are unable to find good solutions; hence, we use an LP-rounding algorithm. 
% \par
% Given %an instance of \probName\ with 
% a collection of prescription rules $\{\pattern_j\}_{j=1}^l$ (obtained by the first two steps), we define the following variables. Let $m$ denote the number of tuples in $\db$ and $m_p$ denote the number of tuples belonging to the protected group: \textbf{(1)} A variable $g_i$ for every candidate rule $i \in [1,l]$;  \textbf{(2)} The variable $t_{i,j}$ indicates whether the $i$-th tuple is covered by the $j$-th rule.

% We construct the Integer Linear Program (ILP) shown in \cref{fig:lp} (which extends the ILP for the \emph{max-k-cover} problem). This is an example of formulation incorporating group coverage and SP group fairness constraints. We can obtain similar constraints for each variant of the problem.


% We consider the LP-relaxation of this ILP with variables in $[0, 1]$ instead of $\{0, 1\}$, then use an LP-solver to find solutions. We use the standard randomized rounding algorithm for max-k-cover \cite{raghavan1987randomized}.



% \vspace{1mm}
% \noindent
% \textbf{Why Not an ILP Formulation}: One possible approach to select rules from the mined candidate set is to formulate the problem as an ILP, as done in \cite{DBLP:journals/pacmmod/YoungmannCGR24}. However, based on our experimental results, we found that this method struggles to scale efficiently with large datasets. To ensure more interactive runtimes, we opted for a greedy approach, which proved to be significantly more efficient.





\subsection{Adjustments to Other Variants}
We explain how \sysName\ can be adjusted to solve other problem variants. 
We set the Apriori's threshold to ensure that each mined grouping pattern covers a sufficient number of individuals when a rule coverage constraint is imposed (step 1). Without coverage constraints, the Apriori threshold can be set to any value. 

Without fairness constraints, in Step 2, the goal is to identify the intervention with the highest CATE value (as was done in \cite{DBLP:journals/pacmmod/YoungmannCGR24}). Whit an individual fairness constraint, each rule must satisfy this constraint, so we only select interventions that are guaranteed to meet the constraint while maximizing CATE (step 2).


In case a group BGL fairness constraint is imposed, we define the benefit of a rule $r = (\patterngroup,\patterninterv)$ as follows. Intuitively, we penalize the treatment based on the difference between the minimum required utility for the protected group and the utility provided to the protected group by this treatment. The larger the difference, the lower the benefit of the treatment. Formally: 
\[
    benefit(r)=
\begin{cases}
   \frac{utility(r)}{1 + \tau - utility_p(r)},& \text{if } \tau\geq utility_p(r)\\
   utility(r), &\text{otherwise}
\end{cases}
\]
where $\tau$ is the threshold for the BGL fairness constraint. This benefit definition is applied in Step 2 of the algorithm to identify fair and effective treatments for the mined grouping patterns. 



% \subsection{Optimizations}
% We implement several optimizations to improve efficiency:\\
% \textbf{(a) Pruning attributes:} In step 2, we discard attributes that do not have a causal relationship with the outcome, since such attributes have no impact on CATE values. We can detect such attributes by utilizing the input causal DAG or by removing attributes with low correlation to the outcome.\\
% \textbf{(b) Parallelism:} The process of extracting treatment patterns for each grouping pattern (step 2) can be performed in parallel since this procedure is dependent only on the grouping pattern. \\
% \textbf{(c) Sampling:} To make the second step more efficient, we consider a random sample of the data instead of the full dataset. With this random sample, we are able to account for most relevant treatment patterns more efficiently. 


\vspace{1mm}
\noindent
\textbf{Runtime complexity analysis}:
The maximum number of rules in a database $\db$ with attributes $\attrset$ is bounded by $|\db|^{|\attrset|}$ (considering both grouping and intervention patterns and the active domain of attributes), which is polynomial in terms of \emph{data complexity}, assuming a fixed schema \cite{Vardi82}. The final greedy step is also polynomial in the number of rules considered. Additional operations, such as calculating CATE values, are polynomial in $\db$, leading to worst-case polynomial data complexity. 
As we demonstrate in \cref{exp:scalability}, our algorithm is capable of efficiently handling large datasets.

% % However, the approximation to the objective holds when all explanation patterns are considered, while we use this LP-rounding algorithm in conjunction with the grouping and treatment pattern mining procedures described in Sections \ref{subsec:grouping_patterns} and \ref{subsec:treatment_patterns}, therefore incur a trade-off between value and efficiency. 
% \revb{However, the approximation guarantees of the proposed LP formulation are solely theoretical since the approximation to the objective holds when \emph{all explanation patterns are considered}. In practice, we use this LP-rounding algorithm in conjunction with the grouping and treatment pattern mining procedures described in Sections \ref{subsec:grouping_patterns} and \ref{subsec:treatment_patterns}, therefore incur a trade-off between value and efficiency and lose the theoretical guarantees. }



% % We can further improve the above algorithm by considering a fixed range of values for the coverage threshold $\theta$, computing a solution for each value, and selecting the one that maximizes the objective value while satisfying the coverage constraint. 

% \cut{
% \textbf{Variables}: 
% % Recall that an explanation pattern $\pattern = (\pattern_g, \pattern_t)$ is associated with a grouping and a treatment pattern (\cref{def:explanation-pattern}). 
% Let $g_j$, $j {=} 1,\ldots, l$ be
% indicator variables for explanation pattern $\pattern_j$ with fixed weights $w_j$. 
% %of treatment pattern $\pattern_{t, j}$ of $\pattern_j$. %variables $\pattern_i$, 
% Let $t_i, i {=} 1 \ldots, m$ be indicator variables for covering the $i$-th group $s_i$ in $\Qagg(\db)$. We denote $s_i {\in} \pattern_j$ when $s_i$ satisfies $\pattern_j$. %$\pattern_j$ covers $T_i$. \\
% \textbf{Constraints}: The size constraint (1) ensures that no more than $k$ explanation patterns are selected. 
% The consistency constraints (2) ensure that a group in $\Qagg(\db)$ is explained (covered) if at least one explanation pattern that covered it is selected. 
% The coverage constraint (3) ensures that at least $\theta$ fraction of the groups are covered. Constraint (4) determines the binary domain of the variables. \\
% \textbf{Objective}: maximize the overall explainability of chosen patterns.
% }





%1/n\sum_{i, i is protected} min t_ij w_j




