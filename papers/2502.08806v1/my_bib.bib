@article{ryan2024code,
  title={Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM},
  author={Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={951--971},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@inproceedings{chatunitest,
  title={Chatunitest: A framework for llm-based test generation},
  author={Chen, Yinghao and Hu, Zehao and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei},
  booktitle={Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
  pages={572--576},
  year={2024}
}

@article{liu2024llm,
  title={LLM-Powered Test Case Generation for Detecting Tricky Bugs},
  author={Liu, Kaibo and Liu, Yiyang and Chen, Zhenpeng and Zhang, Jie M and Han, Yudong and Ma, Yun and Li, Ge and Huang, Gang},
  journal={arXiv preprint arXiv:2404.10304},
  year={2024}
}

@article{lukasczyk2023empirical,
  title={An empirical study of automated unit test generation for Python},
  author={Lukasczyk, Stephan and Kroi{\ss}, Florian and Fraser, Gordon},
  journal={Empirical Software Engineering},
  volume={28},
  number={2},
  pages={36},
  year={2023},
  publisher={Springer}
}

@article{tang2024chatgpt,
  title={Chatgpt vs sbst: A comparative assessment of unit test suite generation},
  author={Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu},
  journal={IEEE Transactions on Software Engineering},
  year={2024},
  publisher={IEEE}
}

@article{yuan2024evaluating,
  title={Evaluating and improving chatgpt for unit test generation},
  author={Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={1703--1726},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{wang2024software,
  title={Software testing with large language models: Survey, landscape, and vision},
  author={Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing},
  journal={IEEE Transactions on Software Engineering},
  year={2024},
  publisher={IEEE}
}


@inproceedings{crux,
  title={CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution},
  author={Gu, Alex and Roziere, Baptiste and Leather, Hugh James and Solar-Lezama, Armando and Synnaeve, Gabriel and Wang, Sida},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@misc{jain2024testgenevalrealworldunit,
      title={TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark}, 
      author={Kush Jain and Gabriel Synnaeve and Baptiste Rozière},
      year={2024},
      eprint={2410.00752},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2410.00752}, 
}
@misc{testeval,
      title={TESTEVAL: Benchmarking Large Language Models for Test Case Generation}, 
      author={Wenhan Wang and Chenyuan Yang and Zhijie Wang and Yuheng Huang and Zhaoyang Chu and Da Song and Lingming Zhang and An Ran Chen and Lei Ma},
      year={2024},
      eprint={2406.04531},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.04531}, 
}

@misc{testbench,
      title={TestBench: Evaluating Class-Level Test Case Generation Capability of Large Language Models}, 
      author={Quanjun Zhang and Ye Shang and Chunrong Fang and Siqi Gu and Jianyi Zhou and Zhenyu Chen},
      year={2024},
      eprint={2409.17561},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2409.17561}, 
}
@misc{mathews2024testdrivendevelopmentcodegeneration,
      title={Test-Driven Development for Code Generation}, 
      author={Noble Saji Mathews and Meiyappan Nagappan},
      year={2024},
      eprint={2402.13521},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2402.13521}, 
}

@misc{swtbench,
      title={SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents}, 
      author={Niels Mündler and Mark Niklas Müller and Jingxuan He and Martin Vechev},
      year={2024},
      eprint={2406.12952},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.12952}, 
}

@article{mbpp,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}


@article{team2024codegemma,
  title={Codegemma: Open code models based on gemma},
  author={Team, CodeGemma and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others},
  journal={arXiv preprint arXiv:2406.11409},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{yicoder,
    title = {Meet Yi-Coder: A Small but Mighty LLM for Code},
    url = {https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md},
    author = {01.AI},
    month = {September},
    year = {2024}
}
@misc{devbench,
      title={DevBench: A Comprehensive Benchmark for Software Development}, 
      author={Bowen Li and Wenhan Wu and Ziwei Tang and Lin Shi and John Yang and Jinyang Li and Shunyu Yao and Chen Qian and Binyuan Hui and Qicheng Zhang and Zhiyin Yu and He Du and Ping Yang and Dahua Lin and Chao Peng and Kai Chen},
      year={2024},
      eprint={2403.08604},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08604}, 
}

@misc{starcoder2,
      title={StarCoder 2 and The Stack v2: The Next Generation}, 
      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2024},
      eprint={2402.19173},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2402.19173}, 
}

@article{codellama,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}
@article{zhang2024diversity,
  title={Diversity empowers intelligence: Integrating expertise of software engineering agents},
  author={Zhang, Kexun and Yao, Weiran and Liu, Zuxin and Feng, Yihao and Liu, Zhiwei and Murthy, Rithesh and Lan, Tian and Li, Lei and Lou, Renze and Xu, Jiacheng and others},
  journal={arXiv preprint arXiv:2408.07060},
  year={2024}
}

@inproceedings{alshahwan2024automated,
  title={Automated unit test improvement using large language models at meta},
  author={Alshahwan, Nadia and Chheda, Jubin and Finogenova, Anastasia and Gokkaya, Beliz and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
  booktitle={Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
  pages={185--196},
  year={2024}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@inproceedings{toga,
  title={Toga: A neural method for test oracle generation},
  author={Dinella, Elizabeth and Ryan, Gabriel and Mytkowicz, Todd and Lahiri, Shuvendu K},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={2130--2141},
  year={2022}
}
@inproceedings{evosuite,
  title={Evosuite: automatic test suite generation for object-oriented software},
  author={Fraser, Gordon and Arcuri, Andrea},
  booktitle={Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering},
  pages={416--419},
  year={2011}
}
@article{APPS,
  title={Measuring Coding Challenge Competence With APPS},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul}
}
@inproceedings{r2e,
  title={R2E: Turning any Github Repository into a Programming Agent Environment},
  author={Jain, Naman and Shetty, Manish and Zhang, Tianjun and Han, King and Sen, Koushik and Stoica, Ion},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@inproceedings{repobench,
  title={RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems},
  author={Liu, Tianyang and Xu, Canwen and McAuley, Julian},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{nie2023learning,
  title={Learning deep semantics for test completion},
  author={Nie, Pengyu and Banerjee, Rahul and Li, Junyi Jessy and Mooney, Raymond J and Gligoric, Milos},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={2111--2123},
  year={2023},
  organization={IEEE}
}
@inproceedings{contest,
    title = "{C}on{T}est: A Unit Test Completion Benchmark featuring Context",
    author = "Villmow, Johannes  and
      Depoix, Jonas  and
      Ulges, Adrian",
    editor = "Lachmy, Royi  and
      Yao, Ziyu  and
      Durrett, Greg  and
      Gligoric, Milos  and
      Li, Junyi Jessy  and
      Mooney, Ray  and
      Neubig, Graham  and
      Su, Yu  and
      Sun, Huan  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nlp4prog-1.2/",
    doi = "10.18653/v1/2021.nlp4prog-1.2",
    pages = "17--25",
    abstract = "We introduce CONTEST, a benchmark for NLP-based unit test completion, the task of predicting a test`s assert statements given its setup and focal method, i.e. the method to be tested. ConTest is large-scale (with 365k datapoints). Besides the test code and tested code, it also features context code called by either. We found context to be crucial for accurately predicting assertions. We also introduce baselines based on transformer encoder-decoders, and study the effects of including syntactic information and context. Overall, our models achieve a BLEU score of 38.2, while only generating unparsable code in 1.92{\%} of cases."
}

@article{zhuo2024bigcodebench,
  title={BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions},
  author={Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others},
  journal={CoRR},
  year={2024}
}

@article{tufano2020unit,
  title={Unit test case generation with transformers and focal context},
  author={Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Deng, Shao Kun and Sundaresan, Neel},
  journal={arXiv preprint arXiv:2009.05617},
  year={2020}
}
@article{yoo2012regression,
  title={Regression testing minimization, selection and prioritization: a survey},
  author={Yoo, Shin and Harman, Mark},
  journal={Software testing, verification and reliability},
  volume={22},
  number={2},
  pages={67--120},
  year={2012},
  publisher={Wiley Online Library}
}


@InProceedings{yasunaga21a,
  title =    {Break-It-Fix-It: Unsupervised Learning for Program Repair},
  author =       {Yasunaga, Michihiro and Liang, Percy},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {11941--11952},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/yasunaga21a/yasunaga21a.pdf},
  url =    {https://proceedings.mlr.press/v139/yasunaga21a.html},
  abstract =   {We consider repair tasks: given a critic (e.g., compiler) that assesses the quality of an input, the goal is to train a fixer that converts a bad example (e.g., code with syntax errors) into a good one (e.g., code with no errors). Existing works create training data consisting of (bad, good) pairs by corrupting good examples using heuristics (e.g., dropping tokens). However, fixers trained on this synthetically-generated data do not extrapolate well to the real distribution of bad inputs. To bridge this gap, we propose a new training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use the critic to check a fixer’s output on real bad inputs and add good (fixed) outputs to the training data, and (ii) we train a breaker to generate realistic bad code from good code. Based on these ideas, we iteratively update the breaker and the fixer while using them in conjunction to generate more paired data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new dataset we introduce where the goal is to repair Python code with AST parse errors; and DeepFix, where the goal is to repair C code with compiler errors. BIFI outperforms existing methods, obtaining 90.5% repair accuracy on GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not require any labeled data; we hope it will be a strong starting point for unsupervised learning of various repair tasks.}
}


@inproceedings{
    swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}
@inproceedings{CodeGen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  booktitle={The Eleventh International Conference on Learning Representations},
    year={2023}
}
@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}


@article{Lai2022DS1000,
  title={DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-Tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.11501}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@InProceedings{wei2024magicoder,
  title =    {Magicoder: Empowering Code Generation with {OSS}-Instruct},
  author =       {Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  booktitle =    {Proceedings of the 41st International Conference on Machine Learning},
  pages =    {52632--52657},
  year =   {2024},
  volume =   {235},
  series =   {Proceedings of Machine Learning Research},
  month =    {21--27 Jul},
  publisher =    {PMLR},
  pdf =    {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wei24h/wei24h.pdf},
  url =    {https://proceedings.mlr.press/v235/wei24h.html}
}


@article{hsieh2024ruler,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
  year={2024},
  journal={arXiv preprint arXiv:2404.06654},
}

@inproceedings{zhang-etal-2024-bench,
    title = "$\infty${B}ench: Extending Long Context Evaluation Beyond 100{K} Tokens",
    author = "Zhang, Xinrong  and
      Chen, Yingfa  and
      Hu, Shengding  and
      Xu, Zihang  and
      Chen, Junhao  and
      Hao, Moo  and
      Han, Xu  and
      Thai, Zhen  and
      Wang, Shuo  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.814",
    pages = "15262--15277",
}

@article{hui2024qwen2,
  title={Qwen2. 5-Coder Technical Report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@misc{luo2024repoagent,
      title={RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation}, 
      author={Qinyu Luo and Yining Ye and Shihao Liang and Zhong Zhang and Yujia Qin and Yaxi Lu and Yesai Wu and Xin Cong and Yankai Lin and Yingli Zhang and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.16667},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bogin2024superevaluatingagentssetting,
      title={SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories}, 
      author={Ben Bogin and Kejuan Yang and Shashank Gupta and Kyle Richardson and Erin Bransom and Peter Clark and Ashish Sabharwal and Tushar Khot},
      year={2024},
      eprint={2409.07440},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.07440}, 
}

@misc{openhands,
      title={{OpenHands: An Open Platform for AI Software Developers as Generalist Agents}},
      author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
      year={2024},
      eprint={2407.16741},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.16741},
}