
\section{Related Works}


\textbf{Code \& Test Case Generation Benchmarks}\quad
\iffalse The development of benchmarks for evaluating code generation models has been active. \fi Developing evaluation benchmarks for code generation models is one of the most active research topics.
Earlier benchmarks like HumanEval \cite{chen2021evaluating} and MBPP \cite{austin2021program} focused on basic or LeetCode-style programming problems. BigCodeBench \cite{zhuo2024bigcodebench} extends this with complex function calls, while DevBench \cite{devbench} evaluates model performance in entire software development lifecycle. 
% Frameworks like CRUXEval \cite{crux} and RepoBench \cite{repobench} benchmark these models' reasoning and completion abilities. 
We summarized several recent benchmarks related to test case generation in Table~\ref{tb:related}. The source data and dev environment of \citet{swebench} has been widely adopted to develop new benchmarks. \citet{r2e} demonstrated a scalable framework to turn any GitHub repo into an interactive environment for agents.


\textbf{Test Case Generation}\quad
LLMs are widely used for automating test case generation. \citet{chatunitest} employs LLMs for efficient test creation. \citet{liu2024llm} utilize LLMs for bug detection, while \citet{tang2024chatgpt,yuan2024evaluating} enhance ChatGPT's unit test generation capabilities. \citet{alshahwan2024automated} explore LLM use in industry. Neural models for test case generation were proposed by \citet{tufano2020unit,nie2023learning}. \citet{ryan2024code} investigated coverage-guided test case generation with LLMs.

\textbf{Code LLMs and Agents}\quad
Recent studies on code-specific LLMs \cite{codellama,starcoder2,hui2024qwen2} showcase the potential of specialized models for code generation. \iffalse The StarCoder 2 \cite{starcoder2} and open code models based on GEMMA \cite{team2024codegemma} show the evolution of LLMs tailored for programming tasks. \fi Additionally, RepoAgent \cite{luo2024repoagent} proposes an agentic framework for repo-level documentation, SUPER \cite{bogin2024superevaluatingagentssetting} evaluates LLM-based agents on writing scripts and executing research tasks.

\textbf{Long Context for Code}\quad
Existing long context benchmarks either exclude coding tasks or have restricted access to code LLMs. RULER \cite{hsieh2024ruler} and $\infty$Bench \cite{zhang-etal-2024-bench} fail to replicate real-world software development scenarios. Meanwhile, code benchmarks are insufficient for long context evaluation. RepoBench \cite{repobench} sets the maximum token threshold of 12k for Python and 24k for Java. Most test cases in TestBench \cite{testbench} are within 32k tokens and TestGenEval \cite{jain2024testgenevalrealworldunit} evaluates context length up to 32k. SWE-Bench \cite{swebench} focuses on context length less than 50k, which is far behind many recent models often claiming to be able to consume 128k+ context length.


% \cite{contest} focus on assert statements prediction in Java, focus on context, with lexical evluation and compilation test.
% bigcodebench avg char is 1112.5

% Limitations

% limited number of repos because of fully automated pipelines (Big codebench