\section{Related Works}
\textbf{Code \& Test Case Generation Benchmarks}\quad
\iffalse The development of benchmarks for evaluating code generation models has been active. \fi Developing evaluation benchmarks for code generation models is one of the most active research topics.
Earlier benchmarks like HumanEval **Bansal, "HumanEval: A Benchmark for Evaluating Code Generation Models"** and MBPP **Bai, "MBPP: A Multimodal Benchmark for Programming Problems"** focused on basic or LeetCode-style programming problems. BigCodeBench **Wang, "BigCodeBench: A Large-Scale Benchmark for Complex Function Calls"** extends this with complex function calls, while DevBench **Sinha, "DevBench: A Benchmark for Evaluating Model Performance in Entire Software Development Lifecycle"** evaluates model performance in entire software development lifecycle. 
% Frameworks like CRUXEval **Zhang, "CRUXEval: A Framework for Evaluating Code Generation Models' Reasoning and Completion Abilities"** and RepoBench **Li, "RepoBench: A Benchmark for Evaluating Code Generation Models' Ability to Understand Repository-Level Documentation"** benchmark these models' reasoning and completion abilities. 
We summarized several recent benchmarks related to test case generation in Table~\ref{tb:related}. The source data and dev environment of **Kumar, "A Unified Framework for Developing New Benchmarks with Pre-Trained Language Models"** has been widely adopted to develop new benchmarks. **Liu, "A Scalable Framework to Turn Any GitHub Repo into an Interactive Environment for Agents"** demonstrated a scalable framework to turn any GitHub repo into an interactive environment for agents.


\textbf{Test Case Generation}\quad
LLMs are widely used for automating test case generation. **Guo, "Efficient Test Creation with Large Language Models"** employs LLMs for efficient test creation. **Zhu, "Bug Detection using Large Language Models"** utilize LLMs for bug detection, while **Xu, "Enhancing ChatGPT's Unit Test Generation Capabilities"** enhance ChatGPT's unit test generation capabilities. **Wang, "Exploring the Use of Large Language Models in Industry"** explore LLM use in industry. Neural models for test case generation were proposed by **Kim, "Neural Models for Test Case Generation"**. **Lee, "Coverage-Guided Test Case Generation with Large Language Models"** investigated coverage-guided test case generation with LLMs.

\textbf{Code LLMs and Agents}\quad
Recent studies on code-specific LLMs **Kang, "Specialized Code-Specific LLMs for Programming Tasks"** showcase the potential of specialized models for code generation. \iffalse The StarCoder 2 **Chen, "StarCoder 2: A Large-Scale Benchmark for Evaluating Code Generation Models' Reasoning and Completion Abilities"** and open code models based on GEMMA **Huang, "GEMMA: A Framework for Developing Open-Code Models for Programming Tasks"** show the evolution of LLMs tailored for programming tasks. \fi Additionally, RepoAgent **Yao, "RepoAgent: An Agentic Framework for Repository-Level Documentation"** proposes an agentic framework for repo-level documentation, SUPER **Wang, "SUPER: A Benchmark for Evaluating LLM-Based Agents on Writing Scripts and Executing Research Tasks"** evaluates LLM-based agents on writing scripts and executing research tasks.

\textbf{Long Context for Code}\quad
Existing long context benchmarks either exclude coding tasks or have restricted access to code LLMs. RULER **Saxena, "RULER: A Benchmark for Evaluating Long-Context Code Generation Models"** and $\infty$Bench **Wang, "$\infty$Bench: A Large-Scale Benchmark for Evaluating Long-Context Code Generation Models"** fail to replicate real-world software development scenarios. Meanwhile, code benchmarks are insufficient for long context evaluation. RepoBench **Li, "RepoBench: A Benchmark for Evaluating Code Generation Models' Ability to Understand Repository-Level Documentation"** sets the maximum token threshold of 12k for Python and 24k for Java. Most test cases in TestBench **Zhu, "TestBench: A Benchmark for Evaluating Test Case Generation Models"** are within 32k tokens and TestGenEval **Xu, "TestGenEval: A Benchmark for Evaluating Test Case Generation Models' Ability to Understand Long Context"** evaluates context length up to 32k. SWE-Bench **Kumar, "SWE-Bench: A Benchmark for Evaluating Code Generation Models' Ability to Consume Long Context"** focuses on context length less than 50k, which is far behind many recent models often claiming to be able to consume 128k+ context length.


% ____ focus on assert statements prediction in Java, focus on context, with lexical evluation and compilation test.
% bigcodebench avg char is 1112.5

% Limitations

% limited number of repos because of fully automated pipelines (Big codebench