\section{Related Works}
\textbf{Code \& Test Case Generation Benchmarks}\quad
\iffalse The development of benchmarks for evaluating code generation models has been active. \fi Developing evaluation benchmarks for code generation models is one of the most active research topics.
Earlier benchmarks like HumanEval ____ and MBPP ____ focused on basic or LeetCode-style programming problems. BigCodeBench ____ extends this with complex function calls, while DevBench ____ evaluates model performance in entire software development lifecycle. 
% Frameworks like CRUXEval ____ and RepoBench ____ benchmark these models' reasoning and completion abilities. 
We summarized several recent benchmarks related to test case generation in Table~\ref{tb:related}. The source data and dev environment of ____ has been widely adopted to develop new benchmarks. ____ demonstrated a scalable framework to turn any GitHub repo into an interactive environment for agents.


\textbf{Test Case Generation}\quad
LLMs are widely used for automating test case generation. ____ employs LLMs for efficient test creation. ____ utilize LLMs for bug detection, while ____ enhance ChatGPT's unit test generation capabilities. ____ explore LLM use in industry. Neural models for test case generation were proposed by ____. ____ investigated coverage-guided test case generation with LLMs.

\textbf{Code LLMs and Agents}\quad
Recent studies on code-specific LLMs ____ showcase the potential of specialized models for code generation. \iffalse The StarCoder 2 ____ and open code models based on GEMMA ____ show the evolution of LLMs tailored for programming tasks. \fi Additionally, RepoAgent ____ proposes an agentic framework for repo-level documentation, SUPER ____ evaluates LLM-based agents on writing scripts and executing research tasks.

\textbf{Long Context for Code}\quad
Existing long context benchmarks either exclude coding tasks or have restricted access to code LLMs. RULER ____ and $\infty$Bench ____ fail to replicate real-world software development scenarios. Meanwhile, code benchmarks are insufficient for long context evaluation. RepoBench ____ sets the maximum token threshold of 12k for Python and 24k for Java. Most test cases in TestBench ____ are within 32k tokens and TestGenEval ____ evaluates context length up to 32k. SWE-Bench ____ focuses on context length less than 50k, which is far behind many recent models often claiming to be able to consume 128k+ context length.


% ____ focus on assert statements prediction in Java, focus on context, with lexical evluation and compilation test.
% bigcodebench avg char is 1112.5

% Limitations

% limited number of repos because of fully automated pipelines (Big codebench