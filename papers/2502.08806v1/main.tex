%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{multicol} % for column spanning
\usepackage{url}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{enumitem} % for list formatting
\usepackage{multirow} 
\usepackage{caption}   % For caption support outside float environments
\usepackage{float}
\usepackage{graphicx} % Optional, if you're also including figures
\usepackage{framed}   % Optional, provides more box styles

% Define the inline enumerate style
\newlist{inlinelist}{enumerate*}{1}
\setlist[inlinelist,1]{label=(\arabic*), before=\unskip{: }, itemjoin={{; }}}

\newcommand{\tcase}{$t$}
\newcommand{\tcaseprefix}{$t_{prefix}$}
\newcommand{\tsuite}{$T$}
\newcommand{\uut}{UUT}
\newcommand{\mfont}[1]{{\textsf{\textsc{#1}}}}

% Define \codegemma using \mfont
\newcommand{\codegemma}{\mfont{CodeGemma-7b}}
\newcommand{\codellama}{\mfont{CodeLlama-13b}}
\newcommand{\llamasm}{\mfont{Llama 3.1-8b}}
\newcommand{\llamamd}{\mfont{Llama 3.1-70b}}
\newcommand{\zeroone}{\mfont{Yi-Coder-9b}}
\newcommand{\codestral}{\mfont{Codestral-22b}}
\newcommand{\starcoder}{\mfont{StarCoder2-15b}}
\newcommand{\mistral}{\mfont{Mistral-7b}}
\newcommand{\gpto}{\mfont{GPT-4o}}
\newcommand{\gptm}{\mfont{GPT-4o-mini}}
\newcommand{\magicoder}{\mfont{Magicoder 6.7B}}
\newcommand{\gemini}{\mfont{Gemini 1.5-F}}
\newcommand{\claude}{\mfont{Claude 3.5-S}}
\newcommand{\qwen}{\mfont{Qwen 2.5CI-14b}}

\usepackage{listings}

\usepackage{mdframed}

\usepackage[table,dvipsnames]{xcolor}

\definecolor{pos}{HTML}{2E8B57} % Sea Green
\definecolor{neupos}{HTML}{FFA500}    % Amber
\definecolor{neuneg}{HTML}{FA8072}    % Salmon
\definecolor{neg}{HTML}{B22222} % Firebrick (Crimson)
\usepackage{tcolorbox}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CLOVER: A Test Case Generation Benchmark with Coverage, Long-Context, and Verification}

\begin{document}

\twocolumn[
\icmltitle{CLOVER: A Test Case Generation Benchmark \\ with Coverage, Long-Context, and Verification}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}




\begin{icmlauthorlist}
% \icmlauthor{Jiacheng Xu}{comp}
\icmlauthor{Jiacheng Xu}{}
\icmlauthor{Bo Pang}{}
\icmlauthor{Jin Qu}{}
\icmlauthor{Hiroaki Hayashi}{}
\icmlauthor{Caiming Xiong}{}
\icmlauthor{Yingbo Zhou}{}
\\
\textnormal{ Salesforce AI Research }
% \textnormal{\texttt{jiacheng.xu@salesforce.com}}

% Jiacheng Xu, Bo Pang, Jin Qu, Hiroaki Hayashi, Caiming Xiong, Yingbo Zhou
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{comp}{Salesforce AI Research}


\icmlcorrespondingauthor{Jiacheng Xu}{jiacheng.xu@salesforce.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Software testing is a critical aspect of software development, yet generating test cases remains a routine task for engineers. This paper presents a benchmark, CLOVER, to evaluate models' capabilities in generating and completing test cases under specific conditions. Spanning from simple assertion completions to writing test cases that cover specific code blocks across multiple files, these tasks are based on 12 python repositories, analyzing 845 problems with context lengths ranging from 4k to 128k tokens. Utilizing code testing frameworks, we propose a method to construct retrieval contexts using coverage information. While models exhibit comparable performance with short contexts, notable differences emerge with 16k contexts. Notably, models like GPT-4o and Claude 3.5 can effectively leverage relevant snippets; however, all models score below 35\% on the complex Task III, even with the oracle context provided, underscoring the benchmark's significance and the potential for model improvement.
The benchmark is containerized for code execution across tasks, and we will release the code, data, and construction methodologies.

\end{abstract}

\input{tab_related_work_comp}

\section{Introduction}

Software testing is integral to the software development lifecycle \cite{yoo2012regression, wang2024software, alshahwan2024automated}. From test-driven development \citep{mathews2024testdrivendevelopmentcodegeneration} to program repair \cite{yasunaga21a,swebench}, crafting efficient and high-quality test cases is a routine task. Recently, large language models (LLMs) have gained attention for their potential in code and software testing enhancements. These models utilize context, user prompts, history, and code prefixes for code suggestions \cite{CodeGen,yicoder,codellama,qwen2.5}. 
% Software testing has been a fundamental step in the lifecycle of software development \cite{yoo2012regression,wang2024software,alshahwan2024automated}. From test-driven code development \citep{mathews2024testdrivendevelopmentcodegeneration} to program repair \cite{yasunaga21a,swebench} verified by updated test suites, crafting effective, high-quality and efficient test cases has been a daily work for many software developers. 
% Recently, large language models have been attracting broad attention in improving software development and testing. 
% Code LLMs could utilize the context, user prompts, conversation history, and code prefix to provide code suggestions or completions \cite{CodeGen,yicoder,codellama,qwen2.5}.
% However, the evaluation of models' capability in such tasks remain a challenge. 
To evaluate models' capability in writing code, many benchmarks have been proposed in the past few years. 
These benchmarks vary in focus, tackling areas such as basic coding problems \cite{mbpp, chen2021evaluating}, data science tasks \cite{Lai2022DS1000}, reasoning challenges \cite{crux}, and issue resolution \cite{swebench}. We summarize the most relevant work in Table~\ref{tb:related}. 

\textit{Can LLMs write executable test cases with specific requirements in a realistic setup?}
To address this question, we create a benchmark, CLoVer, focusing on automatic evaluation of unit test case generated by LLMs.  
We create an automatic pipeline with little human intervention to scrape permissive repositories from GitHub and configure the execution environment. 
% This setup also enables us to obtain execution results and coverage reports for comprehensive assessment. 
We identify potential problems by extracting and verifying test cases from the existing codebase. After this step, we take these problems and  structure three challenging tasks: (1) \textit{Task I} simulates a code completion tool by focusing on cloze-filling style questions; (2) \textit{Task II} addresses scenarios requiring coverage and testing of specific methods or classes within the source code; (3) \textit{Task III} involves improving code coverage, where models are challenged to cover certain code blocks within the source.
The selection of example is driven by AST parser and code coverage results.
We evaluate model performance by executing the generated code and capturing line coverage, offering a tangible measure of their effectiveness.

% With an executable environment for each repository, we can execute any code grounded in arbraitrary path, including the existing ones, the extracted ones, and the generated ones by any model. 
% We can also obtain the coverage report for them. 
% We identify candidate problems by extracting test cases from the existing code base and verifying them within the environment. 

% To simulate the daily tasks in real software deveploment and testing cycles, we designed 3 challenging tasks to cover some major settings. 
% Task I, simulating the code completion tool, targets cloze-filling style questions. Task II covers the setting when we need to cover and test a specific and existing method or class from source code. Task III features coverage improving scenario that models need to cover certain code blocks from source code.
% We measure the models performance by actually executing the code with line coverage captured. 

In practical software testing, leveraging a comprehensive context window is crucial, encompassing dependencies and their antecedents. To evaluate models in a realistic context-aware fashion, we construct \textit{oracle context} via test coverage for each example. We assess model performance across three tasks with context lengths spanning 4k to 128k tokens and introduce \textit{context utilization} as a metric to assess how effectively models leverage extended contexts, independent of their absolute performance.

Our evaluation includes 10 open-source and 4 proprietary models. In Task I, many open-source models, such as \mistral{} and \qwen{}, underperform with longer contexts, indicating a decline in response quality despite their technical capacity to handle such lengths. 
% On the other hand, \claude{} excels, achieving a 75.6\% success rate with oracle context, improving from a 72.6\% baseline without context.
In Tasks II and III, all models encounter difficulties in generating executable code, even when provided with oracle context. A notable trend is the sharp performance drop among open-source models starting at a 16k context window. The highest performance across all tasks is demonstrated by \claude{} and \gpto{}, with \gpto{} achieving a 32.7\% success rate on the most demanding task, Task III. 
\textit{We identified a significant disparity in context utilization and long-context instruction-following capabilities between leading proprietary models and others.
}
Our data pipeline and evaluation sandbox are designed for scalability. We plan to release the code, benchmark, Dockerized environment, and recipes to enable the community to use these resources for further development and training. The benchmark also supports code agent by providing APIs and task instructions.



\begin{figure*}[t]
\begin{center}
\small
\centerline{\includegraphics[width=\textwidth]{figure1.pdf}}
\caption{Pipeline overview. In this example, we focus on a test function \texttt{test\_iter}, which covers the use of \texttt{Token} and \texttt{TokenStream} classes from the source code. There are four major steps \begin{inlinelist}
    \item  we extract the problem from a test file \texttt{test\_lexnparse.py}.
    \item  verify of the extracted case(s) by running pytest
    \item assemble task prompts with pre-constructed oracle dependent files
    \item obtain model response and verify the execution status 
\end{inlinelist}  In Task I, we mask part of the assertion statements. In Task II and III, we ask model to complete the test code almost from scratch with constraints imposed. }
\label{fig1}
\end{center}
\end{figure*}

\input{tab_task_stat}


\section{Data \& Sandbox Construction}
\label{sec:setup}

% This works features evaluating models' capability on large-scale real-world repositories.
% During the creation of these resources, we used minimal human intervention and little human effort was dedicated to a specific dataset. Our solution, otherwise noted, is applicable to any Python repositories.
In Figure~\ref{fig1}, we describe the overall pipeline from data collection to final evaluation. In this section, we will primarily focus on data collection and environment setup. 

\subsection{Data Collection}


\textbf{Source identification}\quad 
Following \cite{swebench}, we began by scraping 42 new python repositories and ultimately narrowed it down to 12 repositories for actual use. Details and reasons for exclusions are provided in Appendix \ref{app:sec:repo}. In our methodology, we identified folders containing source and test code by matching filenames with the pattern \texttt{test\_*.py}. For eleven repositories, we could not extract test suites. This process resulted in the identification of test modules, each comprising at least one test function.


\textbf{Problem extraction from file} \quad
Test cases are extracted from modules by parsing Python files using the \textsc{ast} tool to identify test functions. Using heuristics, we isolate setup code $s$ to remove unrelated test functions. In Figure~\ref{fig1}, test functions \texttt{test\_simple} and \texttt{test\_iter} are preserved with the necessary setup code, resulting in self-contained problems named \texttt{tmp\_test\_lexnparse\_[*].py}. We maintain the original structure and path of test modules.


\textbf{Verification API \texttt{verify}} \quad
Executing new unit tests requires careful design. During the design and testing of our \texttt{verify} API, we considered several points: (1) Consistency check. Evaluate model-generated implementations against ground-truth code to identify issues from extraction, heuristics, or system conditions such as caching;
(2) Batchify operations. Enable batch evaluation of test cases to decrease overhead from test framework executions and setups; 
(3) Timeout management. Prevent infinite loops in model-generated code;
(4) Error handling and logging;
(5) Repository restoration. Ensure repository state is reset before and after each use.
We wrap this verification process to an API  $\texttt{verify}(case) \rightarrow  \{\mathrm{true}, \mathrm{false}\} $ where the output indicates whether the $case$ can execute successfully.


\textbf{Coverage API \texttt{cov}}\quad
The coverage API provides line coverage metrics for a test case across the entire repository. Utilizing pytest-cov, it reports hit and missed lines, and computes a file-level coverage rate, even if execution fails. Unlike \texttt{verify}, \texttt{cov} cannot be parallelized due to shared cache dependencies but can still deliver coverage reports on failed tests.


\subsection{Sandbox Construction}
To run test programs across different repositories, we create sandboxes and package them in a Docker image, maintaining minimal intervention to ensure the process is scalable to a larger number of repositories.

\textbf{Procedure}\quad
First, we create a conda virtual environment with Python version set to 3.10. Then we install packages including \texttt{poetry}\footnote{\url{https://python-poetry.org/}} and \texttt{tox}\footnote{\url{https://tox.wiki/en/stable/}}. We exhaustively search for \texttt{txt} files, and try to \texttt{pip install} those files. Then, \texttt{git submodule} related operations will handle submodules under the project if any. After this step, we try to install the package from the current project directory with \texttt{pip}, \texttt{poetry} and \texttt{tox}. After all the steps, we run \texttt{pytest} to check if we can find a significant number of passed test cases. 
In practice, the procedure above can automatically configure the environment of 25 out of 42 (59.5\%) repositories.
We describe more detail about construction failure in Section~\ref{app:sec:repo}.


\textbf{Efficiency}\quad Tasks are evaluated sequentially, while evaluations within each task run concurrently across different repositories. The longest-running repository determines the evaluation's time bottleneck. To limit evaluation to 2 hours per model on a CPU Linux machine, we capped the maximum number of examples per repository: 50 for Task I, and 25 each for Task II and III.

\subsection{Evaluated Models}
We utilized vLLM \cite{vllm} for model inference with temperature and top\_p set to $0.2$ and $1.0$. 
Maximum output lengths were 200, 4,000, and 4,000 for Tasks I, II, and III, respectively. To accommodate output tokens without exceeding model length limits, we adjusted the maximum sequence length by the output length during data preparation. The tokenizer from \mistral{} was used in this process. 
We evaluated open-source models including \codegemma{} \citep{team2024codegemma}, \magicoder{} \cite{wei2024magicoder}, \qwen{} (Coder-Instruct) \cite{qwen2.5},  \zeroone{} \cite{yicoder}, \starcoder{} \cite{starcoder2}, \codellama{} \citep{codellama}, \llamasm, \llamamd{} \citep{dubey2024llama}, \codestral, and \mistral{} \citep{jiang2023mistral}.
For proprietary models, we evaluated \claude(onnet), \gemini(lash), \gpto{} (2024-08-06), and \gptm{} (2024-07-18). 
% The detail of evaluated models could be found in Appendix~\ref{app:model}. 

\section{Construction of Oracle Retrieval}
To write or complete test cases, models need access to the related source code. To offer a simplified but \textit{realistic} evaluation setting without using agents or retrievers, we provide oracle retrieval code in this benchmark. 
This leverages our executable environment and the \texttt{coverage} API for detailed coverage information. This setup aims to: (1) explore models' near-upper bound performance, (2) and test models in long-context scenarios.
Our approach constructs long-contexts naturally and demands a multi-hop understanding of code and effective information use.
Our setup is also perfect for software agent development.

\textbf{Motivation}\quad Files such as \texttt{\_\_init\_\_.py} are often highly covered by most test cases, but they contribute little value in terms of addressing specific problems, as per information theory. These files can quickly deplete the context budget due to their high coverage rates. Hence, we need to calibrate the coverage information to reflect the importance of certain informative files. 
\input{fig2}
\textbf{Objective}\quad The objective of constructing the oracle retrieval is to provide the most relevant or informative content within a constrained context budget. For the rest of this section, we will describe how we prioritize salient information with coverage information. 

\subsection{Calibration of Coverage}
% After Step 2 in Fig~\ref{fig1}, we obtain the line coverage report for all test cases. 
Within a file, we represent the test cases as \( Y = \{ y_1, y_2, \ldots, y_T\} \). Typically, these cases share some setup code and are organized under the same testing topic. 
The collection of all source files is denoted as \( X = \{x_1, x_2, \ldots, x_F\} \). 
When using the \texttt{verify} API on \( T \), we get a coverage tensor \(\mathbb{C} \in \{1,0\}^{T \times F \times L} \), where \( C_{t,f,l} = 1 \) indicates test case \( y_t \) covers the \( l\)-th line of file \( x_f \), and \( C_{t,f,l} = 0 \) otherwise. $T  = |Y|$ represents the total number of test cases in this file, $F = |X|$ is the total number of source files, and $L$ is the max number of lines in $src$. 
We run pytest-cov two times for each test case $y_t$:
\begin{itemize}[noitemsep, topsep=1pt, partopsep=0pt,left=2pt]
% \setlength{\itemsep}{-2pt} % Adjust the "-2pt" to your preferred spacing
    \item a regular run $C^{base}_t$. This will return the regular coverage report of $y_t$ over $X$.
    \item empty run $C^{empty}_t$. In this setting, we replace the code with an empty test statement: \texttt{def test(): assert True} and it will be deployed to the same location of $y_t$. For instance, if \texttt{test\_iter} was implemented in \texttt{tests/util}, we will deploy the empty test to that directory as well. 
\end{itemize}

\textbf{Repository Baseline}\quad
We propose a repository baseline is established by comparing $C^{base}_t$ and $C^{empty}_t$. 

\begin{equation*}
    Q_t^{\text{repo}} = \left\{ x_f \mid    arg_f [ \mathbbm{1}(C_{t,f}^{base} = C^{\text{empty}}_{t,f})  ] \right\}
\end{equation*}

where $\mathbbm{1}(\cdot)$ is the indicator function.
The set \( Q_t^{\text{repo}} \) comprises the files \( x_f \) for which, for any test case index \( t \), the coverage remains unchanged after executing the actual test case. This implies that the files in \( Q_t^{\text{repo}} \) offer minimal information gain in terms of entropy for generating test case \( y_t \).

\textbf{Peer Baseline}\quad
To uniquely identify each test case, we set a Peer Baseline. The aim is to identify the most distinctive information across test cases. For a particular test case \(y_t \), the Peer Baseline is defined as follows%$Q_{t}^{peer} = \left\{ x_f \mid \exists l \in \{1, 2, \ldots, L\}, \, C_{t,f,l} = 1 \, \text{and} \, \sum_{t'=1}^{T} C_{t',f,l} = 1 \right\}$.

\begin{equation*}
    Q_{t}^{\text{peer}} = \left\{ 
      x_f \mid arg_f [
        \mathbbm{1}(\sum_{t'=1}^T C_{t',f,l}^{base} = 1)
      ]
    \right\}
\end{equation*}

where $\mathbbm{1}(\cdot)$ is the indicator function.
\(\sum_{t'=1}^{T} C_{t',f,l}^{base} = 1\) ensures that the line \( l \) is covered by exactly one test case (test case \( y_t \)), meaning it's only covered by the test case \( y_t \). 
Next, we define \( Q' \), which is the set not meeting the criteria for either \( Q^{\text{peer}} \) or \( Q^{\text{repo}} \): $Q'_{t} = F \setminus \left( Q_t^{\text{repo}} \cup Q_t^{\text{peer}} \right)$.
We regard the value of files in \( Q' \) as lower than those in \( Q^{\text{peer}} \) but higher than the repository baseline \( Q^{\text{repo}} \).


\textbf{Calibration of Coverage}\quad
Source files are classified into three categories: \( Q^{\text{repo}} \), \( Q^{\text{peer}} \), and \( Q' \).
The approach for assembling context for test case \( t \) gives precedence to files in the order of \( Q_t^{\text{peer}} \), followed by \( Q_t' \), and ultimately \( Q_t^{\text{repo}} \).
Within each category, we randomly select files if the context budget does not permit using them all.


% \begin{figure}[t]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure2.pdf}}
% \caption{Repository distribution and sample size of Task I. To keep }
% \label{fig:data}
% \end{center}
% \end{figure}



\subsection{Task Setup}
Before diving into these three tasks, we define some terminologies which share across these tasks.
For one task instance, we provide three categories of contents:
\begin{itemize}[noitemsep, topsep=1pt, partopsep=0pt,left=2pt]
% \setlength{\itemsep}{-2pt} % Adjust the "-2pt" to your preferred spacing

    \item Task instruction. We show  examples in Fig~\ref{fig1} and \ref{fig:prompt_box}.
    \item In-file code, including setup code $s$ and function declaration $f$. Setup $s$ prepares necessary components, such as imports, fixtures, and any initial configurations, required for the test. Function declaration $f$ specifies the function's name, arguments, and any return types, if applicable. In Task I, we also provide code prefix, which will be discussed later.
    \item Source files per task requirement and from oracle retrieval. Files required by task are guaranteed to be provided unless in the Problem Only setting. It also has higher priority compared to oracle retrieval when we try to fill the context budget.
\end{itemize}


\textbf{Setting} \quad We introduced two settings across three tasks, Problem Only (PO) and Contextual. In Problem Only setting, we only provide the Task instruction and in-file code. In contextual setting, we provide code snippets capped by context budget. 

\section{Task I: Mask Prediction in Assertion Statements}
This task challenges the model to predict the missing element in an assertion statement within a test case, addressing the code completion feature offered by coding companions.



% \begin{figure}[t]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{task1_data_stat.pdf}}
% \caption{Repository distribution and sample size of Task I. To keep a diverse and challenging subset of questions, we draw at most 50 examples from each repository and compose the working set of Task 1.}
% \label{fig:task1-distb}
% \end{center}
% \end{figure}

\textbf{Problem Formulation}\quad
For each problem $x$, it has following elements in the prompt of the task $[s, f, p, q, ref]$ in the Problem-Only setting:

\input{tab_task1_agg}


\begin{itemize}[noitemsep, topsep=1pt, partopsep=0pt,left=2pt]
   
% \setlength{\itemsep}{-2pt} % Adjust the "-2pt" to your preferred spacing
    \item Prefix ($p$) refers to the existing code within the test function, serving as the context for solving assertion statements.
    \item Assertion statement with a \texttt{MASK} ($q$) represents the task for models to complete. Based on the surrounding code and any referenced materials, the model is expected to fill the gap and complete the assertion statements. $q$ contains exactly one \texttt{MASK}. 
    \item Reference answer ($ref$) is the original answer from the code. Any valid answer passing RER, a metric defined next, is acceptable as correct.
\end{itemize}
The model relies solely on the problem details, without extensive information about the method under test.

\textbf{Cloze construction}\quad
AST tools identify all possible assertion statements, including unary (e.g., variables, functions) and binary operations (comparisons). For binary, either operand can be masked. The suffix of \( q \) is removed to avoid hints, ensuring \( q \) is the last code line.

\textbf{Example selection}\quad
Preliminary study find that within each repository, there exists certain high frequent assertion statements, which provides unwanted hint to models. For instance, ``200'' (string literal) and 200 (number) are the most frequent candidates for the \texttt{MASK}. 
So we filter out problems with common $ref$. The chosen probability of of a problem $x_i$ is defined as: 
\begin{equation*}
    p(x_i) = 
    \begin{cases} 
      0, & \text{if} \quad \frac{\text{count}(ref_i)}{N} > 0.01 \\ 
      \frac{\text{len}(ref_i)}{ \sum_{j=0}^{N} \text{len}(ref_j)}, & \text{otherwise}
    \end{cases}
\end{equation*}
where $N$ is the total number of problems in one repository.
We downsample to 50 problems per repository to maintain a diverse set of problems.


\textbf{Prompt Template} \quad
We explore two elicitation methods \begin{inlinelist}
\item answer only ($pred$): the model yields the answer directly in a code block
\item assertion statement with answer filled \( q.\texttt{replace}(\texttt{MASK}, pred) \): the model returns the line with blank filled. 
\end{inlinelist}
Our studies with \codellama{}, \mistral{}, \codegemma{}, and \codestral{} show the filled assertion method improves execution rate by at least 6.0\%, thus we use it for Task I experiments.



\textbf{Metrics \& Verification} \quad
Let the model prediction for \texttt{MASK} be $pred$.
We implement three evaluation metrics for this task
\begin{inlinelist}
    \item Exact match is defined as $\mathrm{EM} = \mathbf{1}(ref = pred)$
    \item Execution Rate (ER) indicates the execution result of the assertion statements filled with $\text{pred}$
    \item Refined execution rate (RER) is based on ER but we applied post-processing steps to remove trivial assertions and prohibit the copy of an existing assertion from the context. 
\end{inlinelist}

Post-processing discards the following invalid scenarios \begin{inlinelist}
\item $pred$ is a constant in a unary operation
\item $pred$ is a constant in a binary operation where the other operand is also a constant
\item in an equation comparison, \(pred\) matches the operand on the opposite side.
\end{inlinelist}
We follow the definition of constant in AST. 
Since the problems are selected given its surface length, as a proxy to its difficulty, the false negative ratio is considered low. 

\input{tab_task1_ctx}

\subsection{Results in the Problem Only setting} In this setting, we only provide the problem itself, excluding external context like the code of MUT. 
We present the result in Table~\ref{tb:task1}. The best open-source model in this setting is \qwen{}, achieving comparable performance compared to proprietary models. \claude{} performs the best in all metrics. 

\input{tab_task2}
\textbf{Gap between EM and (R)ER} \quad
We analyzed instances where predictions were successful in terms of RER but failed under the exact match (EM) criteria. The model's predictions averaged 25.8 characters compared to 30.7 characters in ground truth answers, suggesting a tendency toward brevity or shortcuts. Common scenarios where execution succeeds but predictions do not exactly match the original code include
\begin{inlinelist}
    \item Unary operations in \( q \) with non-constant \( \text{pred} \) (e.g., \texttt{assert} \( \text{pred} \))
    \item Use of syntactic sugar, such as considering \texttt{x in dict} equivalent to \texttt{x in dict.keys()}
    \item String manipulations, including \texttt{strip} functions and interchangeable quotation marks
    \item Assertions for non-existence, like \texttt{assert x not in y}, where \( x \) is flexible
\end{inlinelist}
Currently, the execution method lacks understanding of contextual semantics or user intent. A future goal is to develop a tool that can evaluate responses based on execution success and alignment with user intent.

\subsection{Results in Contextual Settings}
We present the result in Table~\ref{tb:task1ctx}. 
The best overall performance is achieved by \claude{} in both settings, with a 3.0\% gain from 72.6\% to 75.6\%.
In the contextual setting, we found there is a sharp decrease after 8k max length in most open source models including \codellama{}, \starcoder{}, \mistral{}, \qwen{}, and \codestral{}. We examined the model response in these cases and we find that the chance of getting gibberish output increases along with the increase of input length. Note that the prompt template remains the same for context free and contextual setting where the only change applied is the additional code snippets.

\textbf{Context Utilization $\Delta$}\quad
We introduce a novel metric to measure models' capability in effectively utilizing the context. On the performance gain side, we define it as
$\Delta_{max} = \max(r_{\text{4k}}, r_{\text{8k}}, \ldots, r_{\text{maxLen}}) - r_0$ where $r_0$ is the context free baseline performance. Since we provide oracle context to the model, shorter context carrying strong hint could be sufficient and even better than longer sequence. $\Delta_{max}$ measures the best possible gain a model could get. Vice versa, we define $\Delta_{min} = \min(r_{\text{4k}}, r_{\text{8k}}, \ldots, r_{\text{maxLen}}) - r_0$. This set of metrics focuses on the relative performance change given longer context. The ideal value, if context provides good source of information, for this metric follows this equation $\Delta_{max} > \Delta_{min} > 0$.


\section{Task II: Targeted Test Implementation}
In Task II and Task III, we will shift from code completion to open-ended code generation, which is more challenging and requires longer context. 
In Task II, given a python class or function from the source code, the model needs to complete the test code by using the target.



\textbf{Problem Formulation}\quad
For each problem, we provide setup $s$, function declaration $f$ and a specification to use the target. We show the specification template and an example in Figure~\ref{fig:prompt_box}.
The ``target\_name'' here is the name of the class or function. The ``type'' is either ``class'' or ``function''. ``file\_name'' is where the target object was implemented. 

\textbf{Data Construction}\quad
We use AST tools to parse the code and identify all \texttt{Attribute} type nodes through a recursive walk to find suitable targets. These identified classes and functions become potential targets. They are then matched with those covered by this case in \(C_t^{base} \). A random target is selected as the requirement. In settings ranging from 8k to 64k, we ensure the inclusion of the necessary file as specified. The maximum length constraint is 8k and the output length is 4k, thus the combined length of instructions and the required file must not exceed 4k. Any cases exceeding this limit are discarded. By setting a single target, we maximize the inclusion of examples.


\textbf{Answer Format}\quad
In this task, the generated code is the completion of the function declaration $f$. 
We designed two prompt templates to capture the output, one with the completion part only ($prompt_{part}$), and one with the full code block ($s, f$) along with the completion $prompt_{full}$. 

\textbf{Metrics}\quad
We define two metrics for Task II and Task III. 
\textit{Execution Rate} measures if the generated code can be captured, and executed successfully. Any test failures, exceptions and timeout will count as execution failure. 
\textit{Success Rate} Besides the execution rate, we also check whether the specification was satisfied. For Task II, we check whether the required target was in the generated code. For Task III, we check for code coverage. 

\textbf{Result}\quad
We report the Success Rate using $prompt_{full}$ in Table~\ref{tb:task23} and  $prompt_{part}$ in Table~\ref{tb:t2partial}. In the context free setting of this task, it provides \textit{no} context to the model, not even the ``file\_name'' required to complete the task. 
For most of the models, there is a significant performance boost from context-free to 8k. With only 8k context length, \claude{} achieves surprisingly high performance (46.2\%), 9.0\% ahead of the second best model \gpto{}. Some models, however, remain the same or even get slightly worse performance, including \codegemma{}, \codellama{}, \llamasm{}, and \gemini{}. 
The best performance is achieved by \claude{} and \gpto{} at 32k and 64k respectively. Starting at 16k, we have seen a sudden performance drop on \codellama{}, \mistral{}, \qwen{}, and \codestral{}. 
% We also oversevere similar trend in using $prompt_{part}$ in Table~\ref{tb:t2partial}. 

\section{Task III: Coverage-Oriented Test Implementation}
In this task, given some code blocks from source code, the model needs to complete the test code and cover those target blocks. This task shares a lot of similarity with Task II, so we will focus on the different part.

\textbf{Problem Formulation}\quad
For each problem, we provide $[s,f]$ and a specification to cover up to 10 code blocks from the source code. 
We provide the full code snippet in the ``Retrieved Code Snippets for Reference'' section of the prompt along with other oracle retrieval files. In the specification prompt, we provide the file name, the code blocks to cover, and the starting and ending line number for these code blocks in the original file. 

\textbf{Data Construction}\quad
We took a deterministic approach to select code blocks rather than randomly choosing code spans. 
We use the $Q_t^{peer}$ to guide the selection of code blocks to cover. 
For a case $y_t$, we check if there is some code blocks only covered by it, not any other peer cases. Typically it's the case where a conditional branch or a function is hit by only one case. 
We also filter out code blocks with fewer than 5 lines as we do not want to include many code fragments. 
The max number of files to cover is set at 10. 
With this approach, we can guide the model with a feasible and reasonable coverage requirement which also aligns with the function name and arguments. 



The answer format and metrics of Task III remains the same with Task II. The different task specific prompt is shown in Figure~\ref{fig:prompt_box}. In this setting, since we include up to 10 files, we set the total sequence length to 64k and 128k to keep as many examples as possible. 


\textbf{Results}\quad
We present the success rate of Task III in the rightmost 2 columns in Table~\ref{tb:task23}. \gpto{} achieves the best performance in 64k setting with 32.7\%. None of the open-source models passed 10\% in this task. \claude{} and \gptm{} are the models performing better with longer context provided. 

\textbf{Can LLMs satisfy the coverage requirement?} \quad
To answer this question, we compare the execution rate and the success rate in Table~\ref{tb:task3analyasis}. The gap of whether the coverage requirements can be met is around 5 to 10\% for different models. During evaluation, we only consider it a success if \textit{all} of the code blocks' coverage requirements are satisfied. As the result shows, there is generally a gap between execution rate v.s. success rate, indicates the generated test cases are generally not satisfying all the coverage requirements.

\input{tab_task3}



\input{related}

\section{Limitation \& Conclusion}

Our study is confined to Python and specific pytest tools. We didn't explore using code agents to tackle problems in this benchmark. Preliminary findings indicate that code agents, such as those by \citet{openhands}, are costly to run due to the need to explore entire repositories, primarily because of the overhead from reading large files.

In this study, we introduce a benchmark designed for multiple real-world software testing scenarios. We identified significant gaps in long-context handling, context utilization, and instruction-following capabilities between open-source and closed-source models, pointing to substantial opportunities for improvement. The coverage-driven oracle context could advance research on long-context evaluation in (code) LLMs. Researchers can use the API for real-time feedback to enhance models' coding and reasoning skills. Additionally, the data pipeline can be adapted to create high-quality training datasets.

\newpage
\input{impact}

\bibliography{my_bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix
\onecolumn

\section{Repositories Scrapped \& Used}
\label{app:sec:repo}

The benchmark incorporates the following repositories from GitHub: Pillow, elasticsearch-py, flask, httpx, jinja, kombu, paramiko, pip, requests, sqlalchemy, starlette, and pylint. Conversely, the repositories not utilized include: salt, celery, aiohttp, pytest, sphinx, docker-py, channels, mongoengine, boto3, scrapy, requests-html, black, dd-trace-py, ansible, pyzmq, python-prompt-toolkit, blessed, fastText, google-api-python-client, h2, scikit-learn, httpbin, ipython, libcloud, matplotlib, numpy, pandas, twisted, and voluptuous.

The cutoff date for this benchmark is set for August 30, 2024. Various reasons account for not using all repositories, such as:
\begin{itemize}
    \item some repositories do not support \texttt{pytest} and/or \texttt{pytest-cov},
    \item challenges in automatically configuring the environment or extracting test cases,
    \item non-standard naming of tests that disrupts our heuristics,
    \item failure of rule-based folder localization approach (a.k.a. finding \texttt{tests} and \texttt{src} folder) due to non-standard naming or project structure. For instance, \texttt{elasticsearch-py} has source code folder and test code folder named as \texttt{elasticsearch} and \texttt{test\_elasticsearch}. There are 11 repositories where we manually specified its folder name, 
    \item some repositories being very slow or causing issues during evaluation, and
    \item requirements for external setup, non-Python setup, or specific system configurations for some repositories.
\end{itemize}
% app:source


\section{Results with $prompt_{part}$}
In Table~\ref{tb:t2partial} we present the results with $prompt_{part}$ on Task II. In Table~\ref{tb:task3part} we demonstrate the results on Task III. We found for most of the models, the $prompt_{full}$ which asks for the whole code block works better than $prompt_{part}$ in practice. 

\input{prompt1}

\input{tab_task2_partial_block}

% Please add the following required packages to your document preamble: % 
\begin{table}[t] 
\centering
\small
\begin{tabular}{@{}rcccc@{}} \toprule                                                 & \multicolumn{2}{c}{Success Rate} & \multicolumn{2}{c}{Execution Rate} \\ \cmidrule(l){2-5}  Model                                           & 64k         & 128k       & 64k         & 128k       \\ \midrule\zeroone               & 10.3\%      & 10.3\%     & 22.4\%      & 17.8\%     \\  \llamasm  & 0.9\%       & 0.9\%      & 4.7\%       & 1.9\%      \\ \llamamd & 11.2\%      & 4.7\%      & 14.0\%      & 7.5\%      \\\midrule \gptm                 & 21.5\%      & 20.6\%     & 29.9\%      & 28.0\%     \\ \gemini                   & 26.2\%      & 25.2\%     & 36.4\%      & 34.6\%     \\ \claude             & 32.7\%      & 29.9\%     & 42.1\%      & 37.4\%     \\ \gpto                 & 28.0\%      & 29.9\%     & 35.5\%      & 41.1\%     \\ \bottomrule \end{tabular} 
\caption{
Success rate and execution rate of Task III with $prompt_{part}$.
}
\label{tb:task3part}
\end{table}

\input{prompt2}
\input{prompt3}

\section{Prompt templates and examples}
\label{app:prompt}
We list the prompts for Task I in Fig~\ref{fig:task1po} and Fig~\ref{fig:task1ctx}, Task II in Fig~\ref{fig:task2prompt} and Task III in Fig~\ref{fig:task3prompt}.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
