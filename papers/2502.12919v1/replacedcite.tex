\section{Related Works}
\subsection{Integration of Perception and Reasoning} 
% Probabilistic Logic Program (PLP)  and Statistical Relational Learning (SRL) are two typical paradigms for integrating logical reasoning and machine learning.  Various novel approaches have been proposed in recent years, including DeepProbLog [Manhaeve et al., 2018], Abductive Learning [Zhou, 2019] and NGS [Li et al., 2020].
 % 
 % because human beings generally solve problems based on the leverage of both perception and reasoning
Bridging machine learning and logical reasoning is a well-known holy grail problem in artificial intelligence. NeSy learning ____ proposes to enhance machine learning with symbolic reasoning. It tries to learn the ability for both perception from the environment and reasoning from what has been perceived. However, it requires lots of labeled training data and is difficult to extrapolate. PLP ____ extends first-order logic (FOL) to accommodate probabilistic groundings and conduct probabilistic inference. SRL ____ tries to construct a probabilistic graphical model based on domain knowledge expressed in FOL. PLP and SRL are different from the way human beings solve problems as human beings can use perception and reasoning seamlessly while PLP and SRL focus on one side more. DeepProbLog ____ unifies probabilistic logical inference with neural network training by gradient descent. However, the probabilistic inference in these methods could be inefficient for complicated tasks because of exponential complexity. Formal Logic Deduction (FLD) ____ tries to grant language models with reasoning abilities through logic system, where the language model is the main body of the system. Multi-layer perception mixer model (MLP-Mixer) ____ is quite the opposite, utilizing neural networks to study logic.

% due to the exponential complexity of the probabilistic distribution on the Herbrand base, 
% Previous work attempted to extract some elements from one approach into another, for example, Probabilistic Logic Programming which is a heavy-reasoning light-learning way and Statistical Relational Learning which is a heavy-learning light-reasoning way. 
% However, both methods are still significantly 
Different from the works above, ABL tries to bridge machine learning and logical reasoning in a mutually beneficial way. Zhou first proposed the concept of ABL ____. Dai and Zhou elaborated the framework of ABL and applied ABL to the task of recognizing handwritten formulas with good results ____. Huang developed a similarity-based consistency measure for abduction called ABLSim ____, which takes the idea that samples in the same category are similar in feature space. Huang and Li presented an attempt called SS-ABL ____ which combines semi-supervised learning and abductive learning and applied it to theft judicial sentencing with good results. In order to alleviate the problem of low efficiency and high cost of the knowledge base in ABL, Huang presented ABL-KG ____ which enables abductive learning to exploit general knowledge graph. In recent years, this field has made some research advancements.
% and applied abductive learning to the task of recognizing handwritten formulas with good results.
% This innovative methodology involves two distinct components that independently interpret sub-symbolic information and perform symbolic reasoning. Notably, these components operate interactively. Through logical abduction coupled with consistency optimization, ABL (Abductive Learning) enhances the machine learning model and acquires logical theories within a unified framework. 
\subsection{Derivative-Free Optimization} 
Most of the optimization algorithms used in ABL are Derivative-Free Optimization. Derivative-free optimization algorithms are a class of optimization methods that do not rely on gradient information to find the minimum or maximum of a function. RACOS ____ is a proposed classification-based derivative-free optimization algorithm. Unlike other derivative-free optimization algorithms, the sampling region of RACOS is learned by a simple classifier. Two improving methods mentioned in ZOOpt are SRACOS ____ and ASRACOS ____,  respectively are the sequential and asynchronous versions of RACOS. POSS ____ is another derivative-free optimization approach that employs evolutionary Pareto optimization to find a small-sized subset with good performance. POSS treats the subset selection task as a bi-objective optimization problem that simultaneously optimizes some given criterion and the subset size. POSS has been proven with the best so far approximation quality on these problems. PPOSS ____ is the parallel version of the POSS algorithm.
Yu released a toolbox ZOOpt ____ with effective derivative-free optimization algorithms.