%-----------------------------------------------------------------------
% Template File for Science China Information Sciences
% Downloaded from http://scis.scichina.com
% Please compile the tex file using LATEX or PDF-LATEX or CCT-LATEX
%-----------------------------------------------------------------------
\documentclass{SCIS2025}
% \usepackage{natbib}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\bibliographystyle{plainnat}
% \bibliography{references}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{graphicx}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{multicol}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
% \newtheorem{assumption}{Assumption}
\usepackage{cleveref}
\usepackage{caption}
% \newtheorem{theorem}{Theorem}
\begin{document}
\ArticleType{RESEARCH PAPER}
%\SpecialTopic{}
%\luntan
\Year{2025}
\Month{}
\Vol{68}
\No{}
\DOI{}
\ArtNo{000000}
\ReceiveDate{}
\ReviseDate{}
\AcceptDate{}
\OnlineDate{}
\title{A Smooth Transition Between Induction and Deduction: Fast Abductive Learning Based on Probabilistic Symbol Perception}{A Smooth Transition Between Induction and Deduction: Fast Abductive Learning Based on Probabilistic Symbol Perception}
\author[]{Lin-Han Jia}{}
\author[]{Si-Yu Han}{}
\author[]{Lan-Zhe Guo}{{guolz@lamda.nju.edu.cn}}
\author[]{Zhi Zhou}{}
\author[]{Zhao-Long Li}{}
\author[]{Yu-Feng Li}{{liyf@lamda.nju.edu.cn}}
\author[]{Zhi-Hua Zhou}{}
\AuthorMark{Lin-Han Jia}
\AuthorCitation{Lin-Han Jia, Si-Yu Han, Lan-Zhe Guo, Zhi Zhou, Zhao-Long Li, Yu-Feng Li and Zhi-Hua Zhou}
\address[]{National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China}
\abstract{Abductive learning (ABL) that integrates strengths of machine learning and logical reasoning to improve the learning generalization, has been recently shown effective. However, its efficiency is affected by the transition between numerical induction and symbolical deduction, leading to high computational costs in the worst-case scenario. Efforts on this issue remain to be limited. In this paper, we identified three reasons why previous optimization algorithms for ABL were not effective: insufficient utilization of prediction, symbol relationships, and accumulated experience in successful abductive processes, resulting in redundant calculations to the knowledge base. To address these challenges, we introduce an optimization algorithm named as Probabilistic Symbol Perception (PSP), which makes a smooth transition between induction and deduction and keeps the correctness of ABL unchanged. We leverage probability as a bridge and present an efficient data structure, achieving the transfer from a continuous probability sequence to discrete Boolean sequences with low computational complexity. Experiments demonstrate the promising results.}
\keywords{Abductive Learning, Neuro-Symbolic Learning, }
\maketitle
\section{Introduction}
The relationship between empirical inductive learning and rational deductive logic has been a frequently discussed philosophical question throughout human history. In recent years, this issue has also emerged in the field of artificial intelligence  (AI), manifesting as the challenge of integrating machine learning and logical reasoning, two relatively independent technologies.
%, at a low cost. 
% In the development of artificial intelligence, the perception system based on machine learning and the symbolic system based on logical reasoning have developed independently. They represent induction and deduction, important components of human intelligence, respectively and neither of them can completely replace the role of the other.

Many efforts have focused on this integration issue.
%, but they also encounter some challenging problems that are difficult to resolve. 
Neuro-symbolic (NeSy) learning ~\cite{mao2019neuro} proposes to enhance neural networks with symbolic reasoning. However, it requires lots of labeled training data and is difficult to extrapolate. Probabilistic Logic Program (PLP) ~\cite{de2015probabilistic} is a heavy-reasoning light-learning way because the most workload is to be finished by logical reasoning though some elements of machine learning are introduced. Statistical Relational Learning (SRL) ~\cite{getoor2007introduction} is a heavy-learning light-reasoning way in opposite. DeepProbLog ~\cite{manhaeve2018deepproblog}, which unifies probabilistic logical inference with neural networks but with exponential complexity of probabilistic distributions on the Herbrand base.
%by gradient descent
 % Most of them usually make one subsume to the other. Additionally, the majority require direct semantic-level input and are difficult to be applied to raw input data.
 
In order to better integrate the advantages of both fields, Abductive Learning (ABL) ~\cite{zhou2019abductive} is introduced to allow to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts. It is a recent generic and effective framework that bridges any kind of machine learning algorithms and logical reasoning by minimizing the inconsistency between the pseudo labels obtained from machine learning and logical reasoning. The inconsistency value is calculated by a designed distance function.

%strikes a good balance between machine learning and logical reasoning. It simulates the process of humans combining induction and deduction more reasonably. It first uses a perception model to transform concrete entities into abstract symbols by induction, corresponding to the classification problem in machine learning. Then it uses deduction on abstract symbols for logical reasoning to modify the mistakes made in induction. Finally, the mistakes are corrected and the results are fed back to the perception model for updates. 
However, the efficiency in previous ABL studies as well as NeSy approaches is affected by the transition between numerical induction and symbolical deduction, leading to high computational costs in the worst-case scenario. A smooth transition to bridge the calculations in two fields will be important. In ABL, the logic reasoning module takes the unreliable parts of the symbols predicted by the machine learning model as variables, while treating the reliable parts as constants. Connecting the rules in the knowledge base, it then uses the reliable parts to infer the corrected symbols for the unreliable parts and feeds these back to the machine learning model for updating. 
% the predicted results of machine learning are the symbols that need to be determined as the variable or constant in logical reasoning. Connecting the rules in the knowledge base, logical reasoning derives the determined value of the variable symbols through constant value. 
Therefore, this can be seen as an \textit{optimization problem}, where the optimization variables are Boolean variables that determine which predicted symbols should be variables and the others should be constants in reasoning. The optimization objective is the number of final correct logical reasoning results. In fact, the essence of this optimization problem is to simulate human meta-reasoning ability based on symbol systems, which is also called symbol sensitivity, or abstract perception.
%Both the induction and the deduction programs have achieved much progress in their own fields. However, the key issue lies in how to build a bridge between the two fields in order to make a smooth transition. 
%For machine learning, its prediction results are uncertain constant symbols (i.e., categories in classification). For logical reasoning, it is necessary to determine the variable and constant symbols in reasoning and connect the rules in the knowledge base to deduce the values of variable symbols from constant symbols. 
%Therefore, if they are combined, it naturally requires judgment on the uncertain symbols after perception, treating some symbols as constants and others as variables. This can be treated as an optimization problem, where what is to be optimized is the Boolean value that determines whether a symbol is a variable or a constant. The optimization goal is the number of correct final logical reasoning results. In fact, the essence of this optimization problem is to simulate the meta-reasoning ability of humans based on symbolic systems, which is referred to as symbol sensitivity, or abstract perception.
 % to be input into the logical reasoning program
% The above-mentioned optimization problem serves as a crucial bridge between machine learning-based entity perception and first-order logic (FOL)-based symbolic reasoning. 

Due to the fact that this optimization problem involves both numerical and symbolical values, there is still a lack of efficient optimization techniques. It is worth mentioning that throughout the optimization process, the number of executions in machine learning does not correspond one-to-one mapping with the number of logical reasoning executions, but rather \textit{one-to-many}, resulting in logical reasoning occupying most of the time, as shown in \cref{Abductive}, which typically requires hundreds of logical reasoning to correspond to a single machine learning process. To this end, significantly reducing the number of attempts on Boolean variables in logical reasoning denoted as $T_{ac}$ can help improve the efficiency of ABL and potentially even enhance performance.
%It is precisely due to the incomplete resolution of this optimization problem that ABL faces the challenge of excessive time consumption in practical applications. Throughout the optimization process, the number of executions in machine learning is not one-to-one with the number of executions of logical reasoning, but rather one-to-many, resulting in logical reasoning occupying the majority of the time in ABL as shown in \cref{Abductive}. Due to the low efficiency of optimization methods, typically hundreds of logical inferences were required to correspond to a single machine-learning process. Only by significantly reducing the number of attempts with Boolean variables is it possible to make ABL feasible for practical applications.
\begin{figure*}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.45]{Fig/Abductive.png}}
\caption{The process diagram of ABL reveals a one-to-many relationship between the inductive learning and logical reasoning modules.}
\label{Abductive}
\end{center}
\vskip -0.2in
\end{figure*}

% both the machine learning and logical reasoning components operate in a mostly deterministic manner. 
% The only uncertain aspect lies in their bridging component, determining which symbols are treated as constants and which as variables. This decision corresponds to a Boolean sequence, which serves as the variable to be optimized. 
% During the optimization process, multiple sets of values need to be generated for this Boolean sequence. Each set of values corresponds to different logical reasoning processes. This mismatch between the number of runs in the machine learning module and the logical reasoning module results in a one-to-many relationship, primarily caused by multiple accesses to the knowledge base. 
% As illustrated in Figure 1, assuming fixed runtimes for individual machine learning and logical reasoning operations, the fundamental solution lies in strictly limiting the number of attempts for different Boolean variable values during the optimization process. This approach aims to achieve optimal performance within a finite number of attempts while minimizing the time spent generating variable values during the transition between the two modules.
% 实际应用中反绎学习面临着一个巨大的挑战，即所需的优化时间过长。我们深入分析了其中的原因，发现在整个优化过程中，无论是机器学习部分，还是逻辑推理部分，他们的运作都是基本确定的，而唯一没有确定的是他们的桥梁部分，即将哪些符号作为常量，哪些符号作为变量，这一决策对应着一个布尔序列，而这一布尔序列作为待优化的变量，优化过程中需要生成多组取值，而每一组取值对应了不同的逻辑推理过程，这造成了机器学习模块的运行次数和逻辑推理模块的运行次数并非一对一的，而是一对多的，正是对于知识库的多次访问造成了反绎学习效率过低的问题。参考图1，在假设单次机器学习和逻辑推理运行时间固定的情况下，究其根本我们需要严格限制优化过成中对不同布尔变量取值的尝试次数，以求在有限的次数下达到尽可能优秀的性能，同时尽可能减少二者转换阶段产生变量取值的时间。
In order to address this optimization problem, ABL used general gradient-free optimization methods, such as search-based algorithms POSS ~\cite{NIPS2015_b4d168b4, Liu_2022} and sample-based algorithms RACOS ~\cite {yu2016derivative}. All of these are different from the way human experts combine induction and reasoning, and perform many ineffective operations, leading to resource waste. Specifically, compared to human experts, the reasons for the inefficiency of existing algorithms are related to three aspects: starting in a random state without utilizing predictive information, failing to utilize the relationships between symbols, and performing zero experience optimization every time without accumulating successful ABL process experience.

%To handle this optimization problem, ABL used to adopt general gradient-free optimization methods for example search-based algorithms like POSS\~\cite{NIPS2015_b4d168b4, Liu_2022} and sample-based algorithms like RACOS\~\cite{yu2016derivative}. All of them are vastly different from the way human experts combine induction and deduction, and perform lots of ineffective operations, leading to severe resource wastage. Throughout the history of science, any significant scientific discovery has been based on the precise intuition of scientists accumulated from extensive experience, rather than redundant searching or sampling. Proposing, proving, or refuting a conclusion relies on flexible thinking underpinned by sensitivity to symbols, not on extensive trial and error. Cognitive science also reveals that human logical reasoning abilities depend on perception based on symbolic systems. Therefore, it is urgently needed to address the three major deficiencies of optimization algorithms compared to human experts: initiating with random states without utilizing perceptual information, failing to exploit the relationships between symbols, and conducting zero-experience optimization each time without accumulating experiences from successful ABL processes. This paper attempts to tackle the aforementioned issues to endow AI with meta-reasoning capabilities based on symbol sensitivity.
% similar to humans'
% These gradient-free optimization algorithms are essentially random and inefficient search algorithms, which has a bad influence on the effectiveness and training speed of abductive learning. The reason lies in the fact that these gradient-free optimization algorithms are universally applicable to all problems and do not consider the uniqueness of the abductive learning task. They perform a lot of ineffective searches, leading to severe resource wastage. The optimization methods are vastly different from the way humans combine induction and deduction. On one hand, they do not integrate with the perceptual results of machine learning, severely lacking effective utilization of machine learning predictions. On the other hand, each time they face a new optimization task, they start the search from scratch and are unable to accumulate the experience from previous successful abductive learning.

% In contrast, human experts' logical reasoning greatly differs, as it is an action heavily relying on intuition and closely related to perception, rather than a rigid search process.
% Moreover, this intuition can be accumulated through experience. In other words, humans posses perception based on not only entities but also symbols. Humans can quickly identify potentially erroneous symbols through intuition and combine this with logic to correct and complete the abduction process. Max Planck's intuition led him to deduce the Planck formula, starting a new era of quantum mechanics. Kurt Gödel, relying on intuition, constructed a refutation that challenged the completeness of the axiomatic mathematical system. Throughout the history of science, any significant scientific discovery is based on the precise intuition of scientists accumulated from extensive experience, rather than redundant mechanized searching. Proposing, proving, or refuting a conclusion relies on flexible thinking underpinned by extensive experience, not merely on searching for symbols. 
% 

% Currently, no one has proposed a specialized optimization algorithm specifically for bridging perception and symbolic reasoning. Previous works have largely overlooked the crucial layer that separates perception based on entities and reasoning based on symbols: perception based on symbols. As a result, perception based on symbols has become a search based on symbols, leading to much redundancy. 
% In the abduction process, the important task of perception based on symbols is to transform continuous perceptual results into discrete symbols. We find that this transformation from continuous to discrete variables has an extremely important natural tool: probability. Probability is continuous in its values but represents the possibility of each discrete event occurring, making it the smoothest transition between continuous perception and discrete reasoning. 
We propose an optimization algorithm for ABL named Probabilistic Symbol Perception (PSP) that can respond quickly with few trial and error attempts. The intermediate process from machine learning to logical reasoning inevitably involves a transition from continuous to discrete. Probability is a natural tool because its values are continuous, but it represents the degree to which each discrete event occurs, making it the smoothest transition. Therefore, our algorithm consists of two steps: the first step involves a neural network capable of processing sequences, which takes probability predictions from machine learning models as input. It combines the structural information of the symbol sequence and outputs the probability that each position in the output sequence needs to be corrected as a variable. The second step involves an algorithm for generating $T_{ac}$ discrete Boolean sequences from the continuous probability sequence, determining which symbols are variables in logical reasoning. In the case of a sequence length of $l$, the complexity of traditional calculation methods is $O(2^l\log 2^l)$ in the worst-case scenarios. In contrast, the proposed new algorithm only requires $O (l \log l+T_{ac} \log T_{ac}+lT_{ac})$ time complexity. Similar to traditional ABL, we keep the strict correctness to be unchanged. 
% the most likely discrete Boolean sequence of $K$ from 
In addition, we pass the Boolean sequence corresponding to the optimal result to the sequence neural network, enabling it to accumulate successful experiences.
%select the optimal result to provide feedback to the machine model after completing all inference. 

%We propose an ABL algorithm that can respond quickly with very few trial-and-error attempts. We consider that the intermediate process from machine learning to logical reasoning inevitably involves a transition from continuous to discrete. Probability is a natural tool because it is continuous in its values but represents the extent of each discrete event occurring, making it the smoothest transition. Therefore, our algorithm consists of two steps: The first step involves utilizing a neural network capable of handling sequences, which takes the probabilistic predictions of the machine learning model as input. It combines the structural information of symbol sequences to output the probability of each position in the output sequence needing correction as a variable. The second step involves utilizing continuous probability sequences to generate the $K$ most likely discrete Boolean sequences, which serve as the basis for determining whether symbols are variables or constants in logical reasoning. In cases where the sequence length is $l$, conventional computation methods have a complexity of $O(2^l\log 2^l)$. However, we have designed a new continuous-to-discrete algorithm that requires only $O(l\log l+K\log K +lK)$ time complexity. Similar to traditional ABL, we select the optimal result for feedback to the machine model after all reasoning is completed. Additionally, we pass the Boolean sequences corresponding to the optimal result to the sequence neural network to enable it to accumulate successful experiences.
% It also designs a low-complexity method for quickly converting probabilities into discrete Boolean squeezes, enabling the fastest acquisition of the most likely Boolean squeezes. These high-probability Boolean squeezes are then used as the basis for determining whether symbols are variables or constants in logical reasoning. Finally, the most consistent result according to the abduction outcome is chosen as the corrected symbol to be fed back to the perceptual system for updates.
% This algorithm utilizes global information to obtain the probability of correctness for each symbol's predicted result by adding a symbolic perception neural network to the entity perception neural network. 
\textbf{Our contribution.} 
Our contribution can be summarized in three main aspects. First, we identified the core issue that leads to slow ABL optimization: reducing the number of attempts on Boolean variables in logical reasoning.
%the one-to-many relationship between machine learning and logical reasoning.

We also identified three main shortcomings of past optimization algorithms. Second, we propose a solution to alleviate these drawbacks with low complexity. Finally, experiments demonstrate the promising results of our proposal in terms of good accuracy with reduced attempts.

%by introducing an entity aware symbol perception algorithm based on sequence neural networks. Thirdly, we have solved the basic problem of converting from continuous sequences to discrete sequences and proposed an algorithm with extremely low complexity. This greatly reduces the interaction time between machine learning and logical reasoning.

%Our contributions can be summarized in three main aspects. First, we identified the core issue leading to the slow ABL optimization: the one-to-many relationship between machine learning and logical reasoning. We also identified the three major drawbacks of past optimization algorithms. Second, we proposed a solution to alleviate these drawbacks by introducing an algorithm based on sequence neural networks for symbol perception, building upon entity perception. Third, we addressed the problem of the conversion from continuous to discrete sequences, presenting an algorithm with significantly low complexity. This greatly reduces the interaction time between machine learning and logical reasoning.
\section{Related Works}
\subsection{Integration of Perception and Reasoning} 
% Probabilistic Logic Program (PLP)  and Statistical Relational Learning (SRL) are two typical paradigms for integrating logical reasoning and machine learning.  Various novel approaches have been proposed in recent years, including DeepProbLog [Manhaeve et al., 2018], Abductive Learning [Zhou, 2019] and NGS [Li et al., 2020].
 % 
 % because human beings generally solve problems based on the leverage of both perception and reasoning
Bridging machine learning and logical reasoning is a well-known holy grail problem in artificial intelligence. NeSy learning ~\cite{mao2019neuro} proposes to enhance machine learning with symbolic reasoning. It tries to learn the ability for both perception from the environment and reasoning from what has been perceived. However, it requires lots of labeled training data and is difficult to extrapolate. PLP ~\cite{de2015probabilistic} extends first-order logic (FOL) to accommodate probabilistic groundings and conduct probabilistic inference. SRL ~\cite{getoor2007introduction} tries to construct a probabilistic graphical model based on domain knowledge expressed in FOL. PLP and SRL are different from the way human beings solve problems as human beings can use perception and reasoning seamlessly while PLP and SRL focus on one side more. DeepProbLog ~\cite{manhaeve2018deepproblog} unifies probabilistic logical inference with neural network training by gradient descent. However, the probabilistic inference in these methods could be inefficient for complicated tasks because of exponential complexity. Formal Logic Deduction (FLD) ~\cite{morishita2023learning} tries to grant language models with reasoning abilities through logic system, where the language model is the main body of the system. Multi-layer perception mixer model (MLP-Mixer) ~\cite{amayuelas2022neural} is quite the opposite, utilizing neural networks to study logic.

% due to the exponential complexity of the probabilistic distribution on the Herbrand base, 
% Previous work attempted to extract some elements from one approach into another, for example, Probabilistic Logic Programming which is a heavy-reasoning light-learning way and Statistical Relational Learning which is a heavy-learning light-reasoning way. 
% However, both methods are still significantly 
Different from the works above, ABL tries to bridge machine learning and logical reasoning in a mutually beneficial way. Zhou first proposed the concept of ABL ~\cite{zhou2019abductive}. Dai and Zhou elaborated the framework of ABL and applied ABL to the task of recognizing handwritten formulas with good results ~\cite{dai2019bridging}. Huang developed a similarity-based consistency measure for abduction called ABLSim ~\cite{huang2021fast}, which takes the idea that samples in the same category are similar in feature space. Huang and Li presented an attempt called SS-ABL ~\cite{huang2020semi} which combines semi-supervised learning and abductive learning and applied it to theft judicial sentencing with good results. In order to alleviate the problem of low efficiency and high cost of the knowledge base in ABL, Huang presented ABL-KG ~\cite{huang2023enabling} which enables abductive learning to exploit general knowledge graph. In recent years, this field has made some research advancements.
% and applied abductive learning to the task of recognizing handwritten formulas with good results.
% This innovative methodology involves two distinct components that independently interpret sub-symbolic information and perform symbolic reasoning. Notably, these components operate interactively. Through logical abduction coupled with consistency optimization, ABL (Abductive Learning) enhances the machine learning model and acquires logical theories within a unified framework. 
\subsection{Derivative-Free Optimization} 
Most of the optimization algorithms used in ABL are Derivative-Free Optimization. Derivative-free optimization algorithms are a class of optimization methods that do not rely on gradient information to find the minimum or maximum of a function. RACOS ~\cite{yu2016derivative} is a proposed classification-based derivative-free optimization algorithm. Unlike other derivative-free optimization algorithms, the sampling region of RACOS is learned by a simple classifier. Two improving methods mentioned in ZOOpt are SRACOS ~\cite{hu2017sequential} and ASRACOS ~\cite{liu2019asynchronous},  respectively are the sequential and asynchronous versions of RACOS. POSS ~\cite{NIPS2015_b4d168b4} is another derivative-free optimization approach that employs evolutionary Pareto optimization to find a small-sized subset with good performance. POSS treats the subset selection task as a bi-objective optimization problem that simultaneously optimizes some given criterion and the subset size. POSS has been proven with the best so far approximation quality on these problems. PPOSS ~\cite{qian2016parallel} is the parallel version of the POSS algorithm.
Yu released a toolbox ZOOpt ~\cite{Liu_2022} with effective derivative-free optimization algorithms.

\section{Problem Setting and Formulation}
\subsection{The Machine Learning Module}
In ABL, we are given a set of $L$ training labeled data $D=\{(X_1,Y_1),(X_2,Y_2),\dots,(X_L,Y_L)\}$, where $X_i=[x_{i1},x_{i2},\dots,x_{il_i}],x_{ij}\in \mathcal{R}^d$ represents a sample sequence of length $l_i$, with $d$ denoting the feature dimension. Each sample corresponds to a symbol sequence of length $l_i$, denoted as $S_i=[s_{i1},s_{i2},\dots,s_{il_i}],s_{ij}\in SYM$, where $SYM$ represents the set of all symbols. However, the true symbol sequence is unknown and needs to be predicted jointly through machine learning and logical reasoning. The label $Y_i\in \{False, True\}$ serves as a Boolean variable indicating whether the symbol sequence $S_i$ corresponding to the sample sequence $X_i$ complies with the logic.

% 在反绎学习中，我们被给定了一批总量为$L$数据$D=\{(X_1,Y_1),(X_2,Y_2),\dots,(X_L,Y_L)\}$，其中$X_i=[x_{i1},x_{i2},\dots,x_{il_i}],x_{ij}\in \mathcal{R}^d$是长度为$l_i$的样本序列，其中d表示样本的维度，并对应了长度为$l_i$的符号序列$S=[s_{i1},s_{i2},\dots,s_{il_i}],s_{ij}\in SB$，其中SB表示所有符号组成的集合，然而真实的符号序列是不可知的，需要通过结合机器学习与逻辑推理共同预测，而标注$Y_i\in\{False,True\}$作为一个布尔类型的变量表示样本序列$X_i$对应的符号序列$S_i$是否符合逻辑。
During ABL, we need to train a perception model, denoted as $g$, for symbol induction, which classifies samples into corresponding symbols. Typically, the perception model outputs the probabilities of a sample belonging to different symbols. This yields a probability sequence $P_i=[p_{i1},p_{i2},\dots,p_{il_i}]$, where $p_{ij}=g(x_{ij})$, and $p_{ij}\in\mathcal{R}^{|SYM|}$, with $|SYM|$ representing the number of all symbols. Based on the probability sequence, we obtain the prediction sequence $O_i=[o_{i1},o_{i2},\dots,o_{il_i}]$ for the sample sequence using a machine learning classifier based on the perception model. Here, $o_{ij}=f(x_{ij})=\arg\max_k p_{ij}[k]$, where $f$ represents the machine learning classifier obtained from the perception model $g$.
% 在反绎学习的过程中，我们需要训练一个感知模型g用于进行符号归纳，该分类器可以将样本与符号进行对应。通常感知模型对于一个样本会输出该样本属于不同符号的概率,得到概率序列$P_i=[p_{i1},p_{i2},\dots,p_{il_i}]$，$p_{ij}=g(x_{ij})$，其中$p_{ij}\in\mathcal{R}^{|SB|}$，|SB|表示所有符号的数量。而根据概率序列可以得到基于感知模型的机器学习分类器对样本序列的预测序列$O_i=[o_{i1},o_{i2},\dots,o_{il_i}],o_{ij}=f(x_{ij})=\argm\max_kp_{ij}[k]$，其中f即为根据感知模型g得到的机器学习分类器。
\subsection{The Logical Reasoning Module}
The logical reasoning module contains a knowledge base (KB) used to store logical rules formed from human knowledge. Its function is to deduce the values of variables based on inputs of both variables and constants, using the constants and rules stored in the KB. To integrate the machine learning module with the logical reasoning module, we need to determine whether the prediction sequence $O_i$ is compatible with the knowledge base KB. If the sequence is compatible, the prediction sequence does not need modification. However, in cases where it is not compatible, parts of the observation results in the prediction sequence $O_i$, which are all constants, need to be modified to variables. Thus, we denote a Boolean sequence $B_i=[b_{i1},b_{i2},\dots,b_{il_i}], b_{ij}\in\{False,True\}$, where if $b_{ij}=False$, it means keeping $o_{ij}$ as a constant, and if $b_{ij}=True$, it means replacing $o_{ij}$ with a variable, which is symbolized as `$\_$' in Prolog. For example, for a prediction sequence $O_i=[o_{i1},o_{i2},o_{i3},o_{i4},o_{i5}]$ of length 5, if we have the sequence $B_i=[False,True,False,True,False]$, then the observation sequence is replaced with ${O’}_i=[o_{i1},\_,o_{i3},\_,o_{i5}]$. By inputting this into the knowledge base KB, we obtain a new observation sequence $\Delta(O_i)=[o_{i1},\delta(o_{i2}),o_{i3},\delta(o_{i4}),o_{i5}]$, where $\delta(o_{i2})$ and $\delta(o_{i4})$ are the correction results obtained by combining $o_{i1}$, $o_{i3}$, and $o_{i5}$ with the KB through logical reasoning. Providing the corrected result $\Delta(O_i)$ to the machine learning model $f$ allows for further updating of the model, gradually integrating the knowledge from the knowledge base into the machine learning model. Let $\models$ denote logical entailment. $\psi$ represents the update to model $f$. The above process can be symbolized as:
% 逻辑推理模块包含一个知识库KB用于存储有人类知识形成的逻辑规则，其作用为通过同时输入一些变量和一些常量，并根据常量和KB中的规则推导出变量的取值。而为了将机器学习模块和逻辑推理模块结合起来，我们就需判断预测序列$O_i$与知识库KB是否兼容，如果兼容预测序列自然不需要被修改，然而在不兼容的情况下则需要将全是常量的预测序列$O_i$中的部分观测结果修改为待推理变量，因此我们定义一个布尔序列$B_i=[b_{i1},b_{i2},\dots,b_{il_i}], b_{ij}\in\{False,True\}$，其中如果$b_{ij}=False$表示使$o_{ij}$保持为常量，$b_{ij}=True$表示使$o_{ij}$替换为变量，在prolog中用符号'$\_$'表示。例如对于一个长度为5的预测序列$O_i=[o_{i1},o_{i2},o_{i3},o_{i4},o_{i5}]$，如果取序列$B_i=[False,True,False,True,False]$，则观测序列被替换为${O’}_i=[o_{i1},\_,o_{i3},\_,o_{i5}]$，将其输入知识库KB，即可得到经逻辑修正后的新的观测序列$\Delta(O_i)=[o_{i1},\delta(o_{i2}),o_{i3},\delta(o_{i4}),o_{i5}]$，其中\delta(o_{i2})和\delta(o_{i4})就是o_{i1}、o_{i3}、o_{i5}与KB结合通过逻辑推理得到的修正结果。将修正结果\Delta(O_i)反馈给机器学习模型f就可以对模型进一步更新，从而潜移默化地将知识库中的知识融入机器学习模型。上述过程可符号化为：
\begin{align*}
    &(X_i, f) \triangleright O_i\\
    s.t. &(KB, O_i) \models Y_i, or (KB,\Delta(O_i))\models Y_i, f\leftarrow \psi(f,\Delta(O_i))
\end{align*}
\subsection{The Optimization Problem}
In the aforementioned ABL process, the procedures of machine learning and logical reasoning are predetermined, and the only uncertainty lies in the bridge between the two, which is the Boolean sequence $B$ used to determine whether symbols should be treated as constants or variables. The essence of selecting $B$ is to identify the current prediction results that are more likely to be erroneous and need correction. This selection process is crucial in the ABL process because if correctly predicted variables are designated as constants, logical reasoning may fail due to information loss. Conversely, if incorrectly predicted variables are designated as constants, even successful logical reasoning may lead to erroneous abduction. Therefore, ABL algorithms need to optimize the selection of the Boolean sequence $B$, with the optimization goal being the number of samples where the prediction results match the knowledge base, i.e.:
% 其中$\models$ denotes logical entailment. $\psi$表示对模型f的更新。
% 在上述反绎学习过程中，机器学习和逻辑推理的过程都是已经确定的，而唯一不确定的就是二者之间的桥梁，即用于确定符号使作为常量还是作为变量的布尔序列B。对于B的选取本质上是要确认更可能错误需要被更正的当前预测结果，这一选择过程是反绎学习过程中至关重要的部分，因为如果预测正确的变量被定为变量，则逻辑推理很可能由于信息缺失而失败；如果预测错误的变量被定为常量，则即使逻辑推理成功也会造成错误的反绎结果。因此反绎学习算法都需要对于布尔序列B的选择进行优化，而优化目标为预测结果与知识库匹配的样本数量，即：
\begin{align*}
&\max_B\max_{D_c\subset D} |D_c|\\
s.t.&\forall (X_i,Y_i)\in D_c, (X_i, f) \triangleright O_i,\\&
(KB,B_i,O_i) \triangleright \Delta(O_i), (KB,\Delta(O_i))\models Y_i\\
\end{align*}
% &\forall x_ij
For such an optimization problem, the variable to be optimized is the Boolean sequence $B$, which constitutes a discrete optimization problem. Conventional numerical optimization algorithms cannot be applied to this problem; only gradient-free ones are suitable. Additionally, we observe that the search space for the variables in this problem grows exponentially with the sequence length. It is challenging to obtain the global optimal solution by traversing every possible combination.
\subsection{Time Consumption}
% From the perspective of the entire ABL process, assuming the execution times of the machine learning module and the logical reasoning module are fixed, denoted as $T_{ml}$ and $T_{lr}$ respectively, and if the optimization algorithm produces $T_{ac}$ Boolean sequences $B$, 
%  Although, in practice, the time consumption for each model training, the time taken by different optimization algorithms to generate sequences, and the time consumed by different symbolic logic reasoning processes may vary, they are generally within the same order of magnitude. Therefore, 
% We denote $T_{ml}$ as the time taken by the machine learning model, including both forward and backward propagation. We denote $T_{lr}$ as the time needed to obtain a sequence $B$ and complete logical reasoning based on $B$. We assume $T_{ml}$ and $T_{lr}$ to be constant.
From the perspective of the entire ABL process, assuming the execution times of the machine learning module and the logical reasoning module are fixed, denoted as $T_{ml}$ and $T_{lr}$ respectively. 
% Given the one-to-many relationship between machine learning and logical reasoning, 
We denote $T_{ac}$ as the number of times the knowledge base needs to be accessed for logical reasoning after each machine learning iteration. It deduces the total time $T_{total}$ consumed by one ABL process:
$T_{total}=T_{ml}+T_{ac}\cdot T_{lr}$. In normal situations, $T_{lr} >> T_{ml}$.
This makes the number of trial iterations, $T_{ac}$, the key factor in determining the total time consumption of ABL. Traditional optimization algorithms often set $T_{ac}$ to hundreds or more. However, such overhead is impractical in applications. Therefore, we are committed to addressing this challenge by completing ABL with a smaller $T_{ac}$.
% have struggled to converge to satisfactory solutions under strict constraints on $K$. Consequently, they 
% For each set of values of $B$, we need to access the knowledge base KB and perform logical reasoning to evaluate the consistency between the results of the ABL under that set of values and the knowledge base. However, in practical applications, accessing the knowledge base for logical reasoning consumes a significant amount of time. This means restricting the number of attempts $K$ for the values of $B$ during the optimization process is important. While ensuring that $K$ attempts achieve satisfactory results, we can also optimize $T_{op}(K)$ as much as possible, even though this part of the time is not critical.
% This leads to slow response times of abductive learning in real-world applications. Therefore, we have to limit the number of accesses to the knowledge base during inductive learning. 
% This limitation allows the algorithm to achieve satisfactory results within a finite time frame, considering the substantial time consumed by logical reasoning during each access to the knowledge base.
% 对于这样一个优化问题，待优化的变量为布尔序列B，这是一个明显的离散优化问题，无法用常规的数值优化算法，只有无梯度优化算法才能适用于这一问题。另外我们发现这一问题中变量的搜索空间是随序列长度呈指数级增长的，因此也很难通过遍历每一种取值获取全局最优解，而对于每一组B的取值，我们都需要对知识库KB进行一次访问后进行一次逻辑推理，从而对该取值下反绎学习的结果和知识库的一致性进行评估。然而实际应用中访问知识库进行逻辑推理消耗的时间是巨大的，这导致了反绎学习在实际应用中响应过慢，因此我们不得不限制反绎学习中对知识库的访问次数，即限制优化过程中对于B的取值的尝试次数$K$，是算法在有限的时间内达到较好的反绎效果。

\section{Methodology}
We propose an optimization algorithm for ABL that can respond quickly with very few trial-and-error attempts. Our algorithm mainly consists of two steps: In the first step, we achieved a smooth transition from sample perception to symbol perception, addressing the shortcomings of previous algorithms from three aspects. In the second step, we realized a smooth transition from symbol perception to symbol reasoning, proposing an efficient algorithm for converting the continuous probability sequence into the top $T_{ac}$ discrete Boolean sequences. 
\subsection{From Sample Perception to Symbolic Perception}
Previous optimization algorithms fail to perform well when the number of trial-and-error attempts is strictly limited. It can be attributed to their inability to effectively narrow down the search space. This limitation is primarily manifested in the following three aspects:
\begin{itemize}
    \item \textbf{Underutilization of Prediction:} Previous optimization algorithms often start at a random state and fail to leverage the information provided by the prediction.
    \item \textbf{Underutilization of Symbol Relationship:} They tend to overlook the relational information inherent in the structure of symbol sequences.
    \item \textbf{Underutilization to Accumulate Experience:} They cannot accumulate experience from successful abductive processes. Instead, they restart from scratch with zero experience each time they encounter new data.
\end{itemize}

Addressing these limitations is crucial for developing optimization algorithms that can perform well under finite conditions and efficiently handle symbol perception tasks.
% 以往的优化算法需要大量搜索和评估而无法在有限条件下表现好的关键原因在于它们无法有效缩小搜索空间，主要体现为如下三点：1.以往的优化算法没有充分利用感知部分提供的信息；2.以往的优化算法没有利用符号序列结构本身蕴含的符号间关系信息.3.以往的算法无法从成功的反绎过程中积累经验，每次面对新的数据都需要重新开始零经验优化。
\subsubsection{Utilization of Perceptual Information}
% providing ample information for 
Machine learning models typically first produce probabilistic predictions, and then select the symbol with the highest probability as the definitive result. Previous ABL algorithms directly searched for suitable Boolean variables $B$ based on the absolute results, which resulted in a loss of a significant amount of perceptual information. In cases where samples are insufficient, the perceptual results of machine learning algorithms themselves are ambiguous and uncertain. Preserving and utilizing their probability information is advantageous for determining which symbol predictions are unreliable, and identifying symbols that may have been predicted incorrectly. Therefore, we use the probabilistic symbols obtained from the machine learning model as input to the symbol perception module. For a sample sequence $X_i=[x_{i1},x_{i2},\dots,x_{il_i}],x_{ij}\in \mathcal{R}^d$ and a machine model $g$, it's easy to obtain the probabilistic symbols $P_i=[p_{i1},p_{i2},\dots,p_{il_i}]$, where $p_{ij}=g(x_{ij}), p_{ij}\in\mathcal{R}^{|SYM|}$.
\subsubsection{Utilization of Symbol Structure}
% 机器学习模型通常会先得到概率化的测结果，之后再选择概率最高的符号作为绝对的结果，之前的反绎学习算法直接基于绝对的结果搜索合适的布尔变量B，这一过程损失了大量感知信息，且没有有效利用符号与符号间的关系。在样本不充分的情况下，机器学习算法的感知结果本身就是模糊不可确定的，保留并利用其概率信息对于判断哪些符号的预测是不可靠的是有利的，为找到可能预测错误的符号提供了大量信息。因此，我们将机器学习模型得到的概率化符号作为符号感知模块的输入。
\begin{figure*}[htbp]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.47]{Fig/Perception.png}}
\caption{The schematic diagram of ABL transitioning from sample to symbolic perception.}
\label{Perception}
\end{center}
% \vskip -0.2in
\end{figure*}
In the process of understanding symbols, the meaning of an individual symbol is typically limited and requires connections with other symbols to better understand its significance. Moreover, logical relationships are formed among multiple symbols, which are then induced into knowledge. Therefore, symbol-level perception is based on symbol sequences. At the sample perception level, we have obtained probabilistic symbol sequences through machine learning models. The subsequent requirement is to combine the probabilistic predictions of individual symbols from machine learning with the relationships between symbols within the symbol sequence to predict $B$, determining whether each symbol should be treated as a variable or a constant in the logical reasoning process as shown in \cref{Perception}. In current machine learning technologies, neural networks designed for sequence processing, such as bidirectional recurrent neural networks ~\cite{schuster1997bidirectional} and Transformer ~\cite{vaswani2017attention} architectures, are well-suited for this task. They can output the probability of each symbol being predicted correctly or incorrectly $PB_i=[pb_{i1},pb_{i2},\dots,pb_{il_i}]$, which can serve as the basis for obtaining $B$. For a bidirectional sequence neural network $BSNN$:
\begin{align*}[pb_{i1},pb_{i2},\dots,pb_{il_i}]=BSNN([p_{i1},p_{i2},\dots,p_{il_i}])
\end{align*}
% 在对符号的理解过程中，单个符号的意义通常是有限的，需要和其他符号联系起来才能更好地理解其意义。并且只有多个符号间才能形成逻辑关系，并被归纳成为知识，因此符号级别的感知是基于符号序列的。在实体感知层面，我们已经通过机器学习模型得到了概率化的符号序列，后续的需求是同时结合机器学习对单一符号的概率预测与符号序列中符号与符号间的关系，去预测B，即每一符号在逻辑推理过程中是作为变量还是作为常量。而在现在的机器学习技术中，可以用于处理序列的神经网络如双向循环神经网络、Transformer架构，正好可以适用于这一需求，可以用于输出每一符号预测正确或错误的概率，并将其作为获取B的依据。
\subsubsection{Accumulation of Experience}
Furthermore, neural networks designed for sequence processing can accumulate experience from each inference process, without wasting successful inference cases. After the inference process, we can use the Boolean variables that achieved the highest consistency between the knowledge base and the data during the evaluation as supervisory information. This information can be used to update the BSNN. Consequently, the successful experiences of inference can be effectively accumulated, enabling the neural network to possess symbol-based intuition akin to human scientists. This intuition plays a crucial role in significantly reducing unnecessary search iterations. 

More importantly, the sequence neural network can be pre-trained. This means that by considering only partially replacing symbols in originally logically correct sequences and allowing the sequence neural network to predict the positions to be replaced, we can accumulate sufficient experience very simply before the start of ABL. This process enables the model to converge faster and perform better.
% 并且处理序列的神经网络是可以从每次反绎中积累经验的，不会导致对反绎成功的案例的浪费。在推理结束后，我们将在评估过程中知识库与数据一致性最高时的布尔变量作为监督信息，可以反向更新序列神经网络，这样反绎成功的经验就可以被成功积累，从而该神经网络拥有了像人类科学家一样的基于符号的直觉，这种直觉对大幅减少不必要的搜索迭代起到了关键性的作用。更重要的是，该序列神经网络是可以被预训练的，即只需考虑将原本逻辑正确的序列中的部分符号进行替换，让序列神经网络预测其被替换的位置，即可非常简单地在反绎学习开始之前就积累足够多的经验，使模型性能更快收敛。
\subsection{From Symbolic Perception to Symbolic Reasoning}
\subsubsection{From Continuous to Discrete}
Taking symbol perception into further consideration, we aim to obtain $T_{ac}$ Boolean variables $B$ for querying the knowledge base based on the results of symbol perception $PB$, the probability of each symbol being predicted correctly or incorrectly. Our objective is to use a probability sequence to obtain the $T_{ac}$ most likely discrete Boolean variables, and the probabilities of the sequence follow the multiplication rule. For example, given the probability sequence $PB_i=[0.1, 0.2, 0.6]$, where the probability of the first symbol being predicted incorrectly is 0.1 (i.e., $b_{i1}$ is True with a probability of $0.1$ and False with a probability of 0.9), and so forth. The probability of $B_i$ taking the value [False, False, True] is the highest, at $0.9 * 0.8 * 0.6 = 0.432$. The probability of $B_i$ taking the value $[False, False, False]$ is the second highest, at $0.9 * 0.8 * 0.4 = 0.288$, and the probability of $B_i$ taking the value [False, True, False] is the third highest, at $0.9 * 0.2 * 0.6 = 0.108$. We want to select the $T_{ac}$ most probable Boolean sequences for querying the knowledge base. However, using the simplest approach, if the sequence length is $l$, computing all possible solutions' probabilities and sorting them to select the top $T_{ac}$ would have a complexity exceeding exponential time, $O(2^l \log2^l)$. We have designed an extremely rapid method that allows us to obtain the $T_{ac}$ most likely sequences based on the probability sequence in a very short time, requiring only $O(l \log l + T_{ac} \log T_{ac} +lT_{ac})$ computational complexity to achieve this goal.
% 进一步考虑符号感知，我们希望根据符号感知的结果，即每个符号预测正确或错误的概率，获取K次用于查询知识库的布尔变量B。即我们的目标是利用一个概率序列获取最有可能出现的K种离散布尔变量，而概率与概率之间是满足乘法法则的。例如对于给定的概率序列$[0.1,0.2,0.6]$,表示第一个符号预测错误的概率为0.1，即b_1以0.1的概率取True，以0.9的概率取False，以此类推。最终$B_i$的取值为False,False,True]的概率最大，为0.9*0.8*0.6=0.432，取值为[False,False,False]的概率次大，为0.9*0.8*0.4=0.288，取值为[False,True,False]的概率第三大，为0.9*0.2*0.6=0.108。我们的算法选择可能性最大的K种组合作为用于查询知识库的变量。然而以最简单的思路，若序列长度为$l$，对所有可能的解求出概率并排序取前K大，复杂度是超过指数级别的O(2^l log2^l)。而我们设计了一种极为快速的方法能够根据概率序列以极短的时间消耗获取最有可能的k种组合，仅需O(l \log l+k\log k)的计算复杂度即可达成目的。
\subsubsection{Initialize}
Our approach is as follows: Initially, the sequence with the highest probability can be directly obtained. We simply assign True to elements in the probability sequence greater than $0.5$, and False otherwise. Then, based on this initial sequence, we generate subsequent sequences. To simplify the description, for each position in a sequence, if it remains consistent with the initial state, we call it the original state; if it differs, we call it the flipped state. The transition from the initial state to the flipped state is referred to as flipping, while the transition from the flipped state back to the initial state is referred to as unflipping. Furthermore, we have proven a theorem:
% 我们的思路是：首先可能性最大的序列是可以直接获取的，只需直接将概率序列中大于0.5的元素直接取为True，小于0.5 的元素直接取为False，然后就可以基于初始取值序列产生排名在之后的序列。接下来为了简化描述，对于一个取值序列的每一位置，如果其与初始状态一致，我们称其为原始状态，如果不一致，则称其为反转状态。我们称由初始状态到反转状态的过程为反转，由反转状态到初始状态的过程为逆反转
% \begin{theorem}
% \label{theorem1}
% Let $a>1$, after identifying the probability of the top $a-1$ sequences, at least one sequence among the top $a-1$ solutions does not require any positions to transition from flipped to initial states to obtain the $a_{th}$ top sequence. Additionally, only one position needs to transition from the original to the flipped state.
% \end{theorem}
% \begin{proof}
% Firstly, we prove that among the top $a-1$ sequences, there exists at least one sequence that does not need any positions to transition from flipped to original states to obtain the $a_{th}$ top sequence: 

% This is evident since the top 1 sequence is included among the top $a$ sequences, and it does not require any positions to transition from flipped to initial states, thus, the proof is direct.

% Secondly, we prove that among the top $a-1$ sequences, there exists at least one sequence that requires only one position to transition from the initial to flipped state to obtain the $a_{th}$ top sequence.

% Let's $a>b>0$, we assume that the $b_{th}$ sequence among the top $a-1$ sequences does not require unflipping and requires the least number of flips to the $a_{th}$ top sequence, and the required flip count is $c$ where $c>1$. We can flip any one of the $c$ flip positions. If the obtained new sequence is not among the top $a-1$, its probability must be greater than that of the $a_{th}$ top sequence. Thus, the current $a_{th}$ top sequence is not the true $a_{th}$ top sequence, which contradicts the assumption. If the obtained new sequence is among the top $a-1$, then the flip count required from this sequence to the $a_{th}$ top sequence is $c-1$, contradicting the fact that the $b_{th}$ sequence among the top $a-1$ does not require unflipping and requires the minimum flip count of c. Therefore, by contradiction, we can conclude that among the top $a$ sequences, there exists at least one sequence that only requires flipping one position from the initial to flipped state to obtain the $a_{th}$ top sequence.
% \end{proof}
\begin{theorem}
\label{theorem1}
Let \( a > 1 \). After identifying the probabilities of the top \( a-1 \) sequences, it holds that at least one sequence among the top \( a-1 \) solutions does not require any positions to transition from flipped to initial states in order to obtain the \( a \)-th top sequence. Additionally, only one position needs to transition from the initial to the flipped state.
\end{theorem}
\begin{proof}
\textbf{Part 1:}  
We first prove that among the top \( a-1 \) sequences, there exists at least one sequence that does not require any positions to transition from flipped to initial states in order to obtain the \( a \)-th top sequence. This is evident because the top sequence (i.e., the first sequence) is included among the top \( a \) sequences, and it does not require any transitions from flipped to initial states. Hence, the proof for this part is straightforward.

\textbf{Part 2:}  
Next, we prove that among the top \( a-1 \) sequences, there exists at least one sequence that requires only one position to transition from the initial to the flipped state to obtain the \( a \)-th top sequence.
Let \( a > b > 0 \), and suppose that the \( b \)-th sequence among the top \( a-1 \) sequences does not require any unflipping and requires the minimum number of flips, denoted by \( c \), where \( c > 1 \), to reach the \( a \)-th top sequence. Consider the case where one of the \( c \) flip positions is changed.
\begin{itemize}
  \item If the new sequence obtained is not among the top \( a-1 \) sequences, its probability must be greater than that of the \( a \)-th top sequence. This would imply that the current \( a \)-th top sequence is not truly the \( a \)-th top sequence, which contradicts our assumption.
  \item If the new sequence is among the top \( a-1 \), then the number of flips required to transition from this new sequence to the \( a \)-th top sequence is \( c-1 \). This contradicts the assumption that the \( b \)-th sequence requires the minimum number of flips, \( c \), without unflipping. 
\end{itemize}
Thus, by contradiction, we conclude that among the top \( a-1 \) sequences, there exists at least one sequence that requires only one position to transition from the initial to the flipped state in order to obtain the \( a \)-th top sequence.
\end{proof}
\subsubsection{Search Based on Max Heap}
 % in $O(l \log l + K \log K)$ time complexity
According to the theorem above, we have already proven that the $a-th$ solutions can be obtained from one of the top $a-1$ solutions through a single flip, and in this process, all unflips are unnecessary. Hence, each flip only needs to choose the minimal-cost option that minimizes the total probability loss. For a position $u$, flipping reduces the total probability by a factor of:
$V_u = \frac{min(pb_u, 1 - pb_u)}{max(pb_u, 1 - pb_u)}$, where $V$ is fixed for each position. Therefore, we can pre-sort each position in descending order based on $V$ and simply choose the unflipped position with the highest rank during each flip. The sorting time complexity is $O(l \log l)$, as shown in \cref{Sort}. 
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.7]{Fig/Sort.png}}
\caption{Ranking positions based on overall probability loss.}
\label{Sort}
\end{center}
\vskip -0.2in
\end{figure}
After obtaining a new sequence, we can easily determine its successor sequence obtained by flipping the least costly position and its total probability. We only need to maintain a max-heap as shown in \cref{Bridge} to store the top $a$ sequences found so far based on the probability of their next successor. This allows us to directly query the heap top to find the sequence with the highest successor probability with time complexity of $O(1)$. This successor has the highest probability among all sequences not selected. After identifying the top sequence in the heap, we need to locate the successor sequence of its current successor sequence. Subsequently, we store the current successor sequence into the heap. Upon finding the next successor sequence of the top sequence, we update its position in the heap. In the absence of conflicts, the total time complexity for $T_{ac}$ solutions is $O(2T_{ac}\log T_{ac})$,  as the time complexity for adjusting the position of a node in the heap once is $\log T_{ac}$.
\begin{figure*}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.32]{Fig/Bridge1.png}}
\caption{The schematic diagram of the search algorithm for finding the $T_{ac}$ discrete Boolean sequences $B$ with the highest probability based on continuous probability sequence $PB$.}
\label{Bridge}
\end{center}
\vskip -0.2in
\end{figure*}
\subsubsection{Handling Conflicts}
In some cases, the current successor sequence may duplicate the previously obtained successor sequence, such as [False, False, True] and [False, True, False], both resulting in [False, True, True]. When we encounter conflicts, the current sequence needs to skip its next successor sequence and instead find the successor sequence with a lower rank to avoid the conflict. In other words, rather than selecting the position with the minimum loss among the positions that can be flipped, we choose the second smallest position. 
% Each conflict check requires an additional time consumption of $O(l)$. This kind of conflict introduces uncertainty into the computational complexity. However, we resolved this uncertainty through theorem proving.
% 其次我们证明了一个定理：在已经找到概率前a-1（a>1）大的取值序列后，前a-1大的解中至少存在一个序列，不需要将任何位置由反转状态转换为初始状态，且只需将一个位置的初始状态转换为反转状态即可得到概率第a大的序列。
% 证明：
% 首先证明前a-1（a>1）大序列中一定存在不需要将任何位置由反转状态转换为初始状态即可得到第a大序列的序列。这一点由于第1大序列被包含在前a大序列中，其向任何其他序列转换都不需要在任何位置由反转状态转换为初始状态，可直接得证。
% 其次证明前a-1（a>1）大序列中不需要将任何位置由反转状态转换为初始状态的序列中，至少存在一个序列只需将一个位置的初始状态转换为反转状态即可得到第a大的序列。
% 假设第b(a>b>0)大的序列是前a-1个序列中不需要逆反转且距离第a大的序列所需反转次数最少的，且需要的反转次数为c>1。我们只反转c个反转位置中的任意一个，如果得到的新序列不在前a-1大里，则其概率一定大于第a大地序列，则当前的第a大序列并非真实的第a大序列，矛盾；如果得到的新序列在前a-1大里已经存在，则这一序列到第a大序列所需的反转次数为c-1，与第b大的序列是前a-1个序列中不需要逆反转且距离第a大序列所需反转次数最少且最少次数为c相矛盾。因此反证可得前a（a>0）大取值组合中不需要将任何位置由反转状态转换为初始状态的序列中，至少存在一个序列只需将一个位置的初始状态转换为反转状态即可得到概率第i大的序列。
% 最后，根据上述定理，可以在O(l logl+k logk)的时间复杂度下实现利用概率序列直接获取概率前K大的取值组合。步骤如下：已证明第a大的解都可以从前a-1大的解通过一次反转得到，且在这一过程中所有逆反转都是不被需要的，就可以发现每次反转都只需选择代价最小的方案，即总概率损失最小的方案。而对于位置u的反转，会使总概率减小为原先v=min(p_u.1-p_u)/max(p_u,1-p_u)倍，而各位置的v是固定的，我们只需按照v提前对各位置进行降序排序，并且每次翻转时直接选择当前排名最靠前的未翻转位置即可，排序耗时为O(l logl)。在每获得一个新序列后，我们可以很轻易地找到其翻转一个损失最小的位置得到的后继序列及后继序列的总概率是多少，我们只需维护一个最大堆，用于按后继序列的总概率大小存储目前找到的前a大序列，这样就可以直接查询得到当前后继序列总概率值最大的序列并更新状态找到下一个后继并用log(k)的时间更新最大堆即可，在没有重复的情况下总耗时为O(klogk)。对于所有节点，维护其后继的复杂度是O(2k)的，这是由于可以通过维护一个链表，存储当前节点可以翻转的位置，在较为极端的情况下，可能会出现当前后继组合与先前已经得到的后继组合重复的情况，如[False,False,True]和[False,True,False]的后继都是[False,True,True]，但是由于对于每次产生新解的过程，这种重复最多只会出现1次，因此每次产生新解只需O(2)的时间消耗。总体时间消耗为O(l log l+2k log k )，可最终结算为O(l log l+k log k)
% \begin{theorem}
% There will be at most one conflict each time a new successor sequence is generated.
% \label{theorem2}
% \end{theorem}
% For instance, suppose [False, False, True] and [False, True, False] are both in the max-heap storing the top $a-1 (a > 2)$ sequences and their successor sequences are both [False, True, True]. Due to $V_2 < V_3$, the probability of [False, False, True] is greater than that of [False, True, False], which is extracted first from the heap, generating [False, True, True] as the $a-th$ top sequence. When [False, True, False] is extracted, its successor [False, True, True] is already in the heap, requiring a new successor to be determined as [True, True, False]. This successor will not be present in the heap of the top $a$ sequences with additional conflicts because the only sequence that could potentially duplicate this successor, [True, False, False], would have a successor of [True, False, True] due to $V_3 > V_2$. Hence, each time a new sequence is generated, there will be at most one conflict.

Since each search for a new solution requires finding the next successor of the current top sequence as well as the successor of its current successor, at most $2l$ conflicts occur. The time required for conflict detection is $O(2lT_{ac})$. So the total time complexity is $O(l \log l + 2T_{ac} \log T_{ac} + 2lT_{ac})$, which can be settled as $O(l \log l + T_{ac} \log T_{ac} +lT_{ac})$.
% 接下来我们证明每次产生新序列时最多只会有一次重复：
% 若前a-1（a>2）序列的最大堆中，序列$B_j$从堆中取出时发现其后继已经存在于堆中，此时说明一定之前一定存在一个序列$B_i$产生了与$B_j$相同的后继，$B_i$和$B_j$两个序列除了在$u_i$和$u_j$两个位置不一致，其他位置全部一致，其中$B_i$的后继为反转位置$u_i$，$B_j$的后继为反转位置$u_j$，且$v_{u_i}<v_{u_j}$，这才造成$B_i$的概率大于$B_j$先从堆中被取出，产生后继序列被存入堆中，之后$B_j$被取出时其后继已存在于堆中，发生一次重复，需要产生新的后继，反转位置由$u_j$更改为$u_j'$，这时考察与$B_j$有两个位置不一致且$u_j'$位置已反转的序列$B_k$，可知其存在一个位置$u_k$是未反转但的$B_j$中已反转的，我们需要证明的是$B_k$有两种情况：要么不会在$B_j$前从堆中取出，要么在$B_j$前从堆中取出但是其后继不与$B_j$冲突。在第一种情况下，$u_k$是在$B_i$和$B_j$中共同反转过的位置，此时由于已知$B_j$的第一后继由$B_i$最后一次反转$v_{u_i}$产生，可知$v_{u_i}<v_{u_k}$，另外由于$B_i$的后继为反转$v_{u_i}$而非$v_{u_j'}$，可证$v_{u_j'}<v_{u_i}<v_{u_k}$，可证此时$v_{u_k}$尚未反转的$B_k$相较$v_{u_j'}$还未反转的$B_j$晚一些从堆中取出；在第二种情况下$u_k$可能就是$u_i$，此时由于$v_{u_i}<v_{u_j}$，而$u_j$是$B_j$和$B_k$都未反转的位置，因此即使$B_k$在$B_j$前出堆也不会产生冲突。
% 例如：假设[False,False,True]和[False,True,False]都在存储前a-1（a>2）序列的最大堆中，且它们的后继序列都是[False,True,True]，且由于$v_2<v_3$，则[False,False,True]的概率大于[False,True,False]，先从堆中被取出，产生后继[False,True,True]为第a大序列,之后[False,True,False]被取出时其后继[False,True,True]已在堆中，需要重新确定其新的后继为[True,True,False]，则这一后继一定不在前a大序列的堆中。且不再可能产生重复，这是因为唯一可能与这一后继重复的序列[True,False,False]，然而由于$v_3>v_2$，[True,False,False]的后继只能是[True,False,True]，而非[True,True,False]，因此每次产生新序列最多只会有一次重复。
\subsubsection{Feedback}
Through the aforementioned method, we efficiently obtain the most probable $T_{ac}$ sets of Boolean sequences. We then conduct knowledge base queries based on each of them to find the sequence with the highest consistency with the knowledge base, which serves as the final result of the inference. The logic-revised results are used to provide feedback to the machine learning perception model, and the final Boolean sequence is used as feedback for the BSNN. This approach allows for the accumulation of successful experiences from each inference process, enhancing both sample-level and symbol-level perception. Consequently, it aligns the model's perception results more closely with the knowledge base, subtly integrating knowledge into the machine learning model.
% 通过上述方式，我们快速获取了最具可能性的K组布尔序列，分别以它们为基础进行知识库访问找到它们之中数据与知识库一致性最高的序列即可作为最终的反绎结果，并将逻辑推理修正过的结果用于对机器学习感知模型的反馈，并将最终的布尔序列作为序列神经网络的反馈，这样无论是实体级别的感知还是符号级别的感知，都能从每一次反绎的过程中积累成功的经验，并更大程度使模型的感知结果与知识库匹配，从而潜移默化地将知识融入机器学习模型。
\section{Experiments} 
\subsection{About the Evaluations}
% Consequently, the variables deduced from constant symbols cannot guarantee their correctness. This situation leads to even incorrect prediction results being compatible with the knowledge base, generating new erroneous rules.
We evaluated the proposed algorithm through diverse approaches. Initially, in preliminary experiments, we used the overall accuracy of ABL on the target task as the performance metric and the number of accesses to the knowledge base $T_{ac}$ as the time metric. We demonstrated the efficiency of algorithms in two forms: comparing target performance with fixed time consumption and comparing time consumption under fixed target performance. However, we later found that this evaluation method is not sufficiently scientific. This is because in practical applications, knowledge bases are typically incomplete. The inputted constant symbols to the knowledge base may neither be provable nor disprovable. As a result, the performance of ABL is significantly affected by the accidental factor of whether the rules derived from the knowledge base are correct. This inner problem cannot be resolved by the outer optimization algorithm and can lead to learning failures even if the optimization module is optimized to the extreme. Moreover, in cases of poor knowledge base quality, worse performance on the target task can actually prove the superiority of the optimization algorithm. 

To address the fairness issue mentioned above, we propose two methods to independently and objectively evaluate the performance of optimization algorithms of ABL, excluding external accidental factors. The first method involves evaluating under the condition of having a complete knowledge base. However, this comparison method is utopian in practical applications. The second method, and the one we advocate the most, is to return to the original goal of the optimization algorithm: to integrate knowledge from the knowledge base into the machine learning model. Hence, what we should primarily evaluate is the degree of consistency between the knowledge base and the machine learning model under the influence of the optimization algorithm.
% Traditionally, ABL requires continual correction and restart of erroneous training processes using labeled validation data during the iterative process. However, this requires additional resources and can make effective and fair evaluations difficult. To address the fairness issue mentioned above, we propose two methods to independently and objectively evaluate the performance of the ABL optimization algorithm, excluding external accidental factors.

% The first method involves evaluating under the condition of having a complete knowledge base. Some related experiments have been conducted using such complete knowledge bases in prior work\cite{huang2021fast}, where all possible facts and conclusions are stored in the knowledge base, thereby eliminating the influence of randomness in the reasoning process on performance. However, this comparison method is utopian in practical applications, and if a complete knowledge base truly exists, then the optimization algorithm is no longer needed, and instead, matching algorithms would suffice, leading to a logical paradox: when we can evaluate the optimization algorithm, it means we no longer need it; when we need the optimization algorithm, it means we cannot evaluate it.

% The second method, and the one we advocate the most, is to return to the original goal of the optimization algorithm: to integrate knowledge from the knowledge base into the machine learning model. Hence, what we should primarily evaluate is the degree of consistency between the knowledge base and the machine learning model under the influence of the optimization algorithm. Specifically, after the ABL process concludes, we combine the data with the knowledge base in batches to form symbols through perception and generate new rules. We measure the degree to which the knowledge base is absorbed by the perception model by the number of successfully generated new rules. This evaluation strategy allows the optimization algorithm in ABL to be evaluated independently of machine learning and logical reasoning and can serve as the most important and objective basis for evaluating algorithm performance.
\subsection{Experimental Setup}
% We conducted experiments on three classic ABL datasets, Digital Binary Additive (DBA), Random Symbol Binary Additive (RBA) and Handwritten Math Symbols (HMS). The experimental settings can be found in the Experimental Setup section.
\label{Experimental Setup}
We conducted experiments on three datasets ~\cite{dai2019bridging}, Digital Binary Additive (DBA), Random Symbol Binary Additive (RBA) and Handwritten Math Symbols (HMS). The first two datasets are from ~\cite{dai2019bridging}, while HMS comes from ~\cite{licloses}, and is constructed in the same way as DBA and RBA in our experiments. Unlike previous works on ABL that typically impose restrictions on accessing the knowledge base to hundreds of times or more, we strictly limited the number of accesses to the knowledge base to ${T_{ac}}=5$. We set the length of individual sample sequences to be between 5 and 10. Additionally, we specified that every 3 sample sequences form a group for ABL. At the end of the ABL phase, similarly, we combine 3 sequences into one group and incorporate knowledge-based learning to derive a set of rules. Then, based on the satisfaction of each rule set by the sample sequences and the labels $Y$ indicating whether it conforms to logic, we utilize a Multi-Layer Perceptron (MLP) model for supervised training. This MLP model is eventually employed for predicting the test data combined with rule sets. We utilized LeNet5 ~\cite{lecun1998gradient} as the machine learning model for the perceptual part of entities and a bidirectional Long Short Term Memory Networks (LSTM) ~\cite{hochreiter1997long} model with a hidden layer dimension of 10 as the sequence neural network for the symbolic perception part. The length of the LSTM sequences is set to 10. For sequences shorter than 10, we pad them with leading zeros. At any stage, the number of epochs for all neural networks is set to 10. We set the total number of iterations for conducting ABL to be 150.

Before ABL begins, we employed self-supervised learning approaches to pretrain the neural networks. For LeNet5, it needs to be converted into an autoencoder before pretraining, where the intermediate layer dimension is set to $|SYM|$, representing the number of symbols. Since after pretraining, we do not know the mapping between each dimension of the intermediate layer and specific symbols, we choose to retrieve the knowledge base and select the mapping with the highest match. For LSTM, the pretraining method involves taking originally logically correct sequences and randomly replacing less than half of the samples. The LSTM then determines whether each position has been replaced or not. We constructed the knowledge base using the SWI-Prolog ~\cite{wielemaker2012swi} tool and implemented the deep learning code using the TensorFlow-based Keras framework ~\cite{abadi2016tensorflow}. All experiments were conducted on 4 A800 GPUs.
% \subsection{Evaluation Metrics}
\subsection{Comparison Methods}
% We compared five gradient-free optimization algorithms, namely RACOS, SRACOS, SSRACOS, POSS, PONSS and ABL-REFL which is an ABL paradigm that directly predicts the Boolean list, in the same framework. Detailed introductions can be found in the Comparison Methods section.
\label{Comparison Methods}
We compared our proposed optimization algorithm with five gradient-free optimization algorithms, namely RACOS, SRACOS, SSRACOS, POSS, PONSS and ABL-REFL which is an ABL paradigm that directly predicts the Boolean list, in the same framework. They are:
\begin{itemize}
\item \textbf{RACOS}: RACOS is a proposed classification-based derivative-free optimization algorithm. Unlike other derivative-free optimization algorithms, the sampling region of RACOS is learned by a simple classifier.
\item \textbf{SRACOS}: SRACOS is the sequential version of RACOS.
\item \textbf{SSRACOS}: SSRACOS is a noise handling variant of SRACOS. 
\item \textbf{POSS}: POSS is another derivative-free optimization approach that employs evolutionary Pareto optimization to find a small-sized subset with good performance.
\item \textbf{PONSS}: PONSS is a noise handling variant of POSS. 
\item \textbf{ABL-REFL}: ABL-REFL is a paradigm that directly utilizes neural networks to predict Boolean lists. 
\end{itemize}
\subsection{Experiments with Incomplete Knowledge Base}
Based on the analysis, the time consumed by ABL is primarily determined by the number of accesses $T_{ac}$ to the knowledge base. Therefore, there are two ways to evaluate algorithm efficiency: one is to compare performance with a fixed number of accesses, and the other is to compare the required number of accesses for fixed performance. 

For the former form, We primarily evaluate the final performance $ACC_{IT_{total}}$ after iteration completion, the best performance $ACC_{best}$ during iteration, and the final convergence rate $CR$, where $IT_{total}=150$ is the number of iterations. For the last form, we compare the number of accesses ${T_{ac}}_a$ needed for the algorithm to achieve and never fall below a fixed accuracy $a$ and the number of accesses ${T_{ac}}_c$ needed to achieve a fixed convergence rate $c$. The fixed accuracy $a$ is taken as $65\%$, while the fixed convergence rate $c$ is set at $3\%$ because most discrete optimization algorithms struggle to converge. We use `$-$' to indicate algorithms that ultimately fail to reach the objective, with the required number of accesses remaining unknown. The evaluation precision of all metrics will be influenced by the evaluation interval $\Delta_{IT}=10$ and the number of accesses $T_{ac}=5$ per iteration. In particular, the convergence rate $CR$ is defined as:
\begin{align*}
CR=\frac{|ACC_{IT_{total}}-ACC_{IT_{total}-\Delta_{IT}}|}{\Delta_{IT}}. 
\end{align*}
The variation of accuracy with the number of iterations on the three datasets can be seen in \cref{DBA_Incompleted,RBA_Incompleted,HMS_Incompleted} and the evaluation results are shown in \cref{DBA_T,RBA_T,HMS_T}.
% The experimental results are shown in \cref{DBA_Incompleted,RBA_Incompleted,HMS_Incompleted} and \cref{DBA_T,RBA_T,HMS_T}.
% The experimental results indicate ABL-PSP enhances the efficiency of ABL, demonstrating better performance compared to other optimization algorithms when the number of attempts is restricted. 
% More detailed experimental descriptions and additional experimental results can be found in the Experiments with Incomplete Knowledge Base section in appendix.

%  % with the number of iterations
% The variation of accuracy with the number of iterations on the RBA dataset can be seen in \cref{RBA_Incompleted} and the evaluation results are shown in \cref{RBA_T}. 
% % with the number of iterations 
\begin{table}[htbp]
    \centering
    \captionof{table}{The evaluation results on the DBA dataset.}
    \label{DBA_T}
    \begin{tabular}{c c c c c c}
    \hline\hline
    Method &$ACC_{IT_{total}}$&$ACC_{best}$&$CR$&${T_{ac}}_{a}$&${T_{ac}}_{c}$\\
        \hline\hline
        RACOS&61.22&73.56&0.29&$>750$&700\\
        SRACOS&77.33&77.33&2.46&750&$>750$\\
        SSRACOS&88.33&72.67&1.57&300&$>750$\\
        POSS&73.00&76.56&0.27&700&700\\
        PONSS&65.56&74.78&0.58&150&$>750$\\
        ABL-REFL&71.50&74.33&0.35&100&$>750$\\
        ABL-PSP&\textbf{96.67}&\textbf{96.67}&\textbf{0.00}&\textbf{50}&\textbf{550}\\
        \hline\hline
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    % \begin{minipage}{0.5\textwidth} % 第一列，占页面宽度的50%
    \centering
    \includegraphics[scale=1.0]{Fig/DBA_Incompleted.png} % 请替换为你的图片文件名
    \caption{The variation of accuracy on the DBA dataset with the incomplete knowledge base.}
    \label{DBA_Incompleted}
    % \end{minipage}%
    % \begin{minipage}{0.5\textwidth} % 第二列，占页面宽度的50%
    % \end{minipage}
\end{figure}

% ABL-PSP maximizes the utilization of all available information and past experience to reduce the uncertainty of solutions, it achieves decent results with very few attempts.
% The experimental results indicate ABL-PSP enhances the efficiency of ABL, demonstrating better performance compared to other optimization algorithms when the number of trial and error attempts is restricted. Based on the results, it is not difficult to find that, when the number of accesses is fixed, most optimization algorithms tend to perform relatively poorly. However, since ABL-PSP maximizes the utilization of all available information and past experience to reduce the uncertainty of solutions, it achieves decent results with very few attempts. When the learning objective is fixed, ABL-PSP stands out as one of the rare algorithms that achieve the target without the need for extensive trials, and the time saved by ABL-PSP is incalculable.

% \label{Incomplete}

% \begin{figure}[h]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[scale=0.54]{Fig/DBA_Incompleted.png}}
% \caption{The variation of accuracy on the DBA dataset with Incomplete Knowledge Base.}
% \label{DBA_Incompleted}
% \end{center}
% \vskip -0.2in
% \end{figure}

\begin{table}[htbp]
\centering
\caption{The evaluation results on the RBA dataset.}
\label{RBA_T}
\vskip 0.15in
\begin{tabular}{c c c c c c}
\hline\hline
Method &$ACC_{IT_{total}}$&$ACC_{best}$&$CR$&${T_{ac}}_{a}$&${T_{ac}}_{c}$\\
    \hline\hline
    RACOS&62.17&67.17&0.53&$>750$&$>750$\\
    SRACOS&65.33&66.83&1.27&750&$>750$\\
    SSRACOS&49.11&68.33&1.09&$>750$&$>750$\\
    POSS&62.67&67.67&0.23&$>750$&600\\
    PONSS&68.67&69.33&0.38&750&$>750$\\
    ABL-REFL&63.67&69.50&0.40&$>750$&$>750$\\
    ABL-PSP&\textbf{71.50}&\textbf{71.50}&\textbf{0.05}&\textbf{300}&\textbf{500}\\
    \hline\hline
\end{tabular}
\end{table}
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/RBA_Incompleted.png}}
\caption{The variation of accuracy on the RBA dataset with the incomplete knowledge base.}
\label{RBA_Incompleted}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}[htbp]
\centering
\caption{The evaluation results on the HMS dataset.}
\label{HMS_T}
\vskip 0.15in
\begin{tabular}{c c c c c c}
\hline\hline
Method &$ACC_{IT_{total}}$&$ACC_{best}$&$CR$&${T_{ac}}_{a}$&${T_{ac}}_{c}$\\
    \hline\hline
    RACOS&61.50&68.67&1.02&$>750$&$>750$\\
    SRACOS&50.00&71.00&\textbf{0.05}&$>750$&650\\
    SSRACOS&69.00&70.33&1.90&750&$>750$\\
    POSS&71.00&71.00&0.15&700&700\\
    PONSS&50.00&\textbf{72.33}&0.08&$>750$&600\\
    ABL-REFL&71.33&\textbf{72.33}&\textbf{0.05}&700&700\\
    ABL-PSP&\textbf{72.00}&72.00&\textbf{0.05}&\textbf{200}&\textbf{200}\\
    \hline\hline
\end{tabular}
\end{table}
The experimental results indicate that ABL-PSP enhances the efficiency of ABL, demonstrating better performance compared to other optimization algorithms when the number of trial and error attempts is restricted. Based on the results, it is not difficult to find that, when the number of accesses is fixed, most optimization algorithms tend to perform relatively poorly. However, since ABL-PSP maximizes the utilization of all available information and past experience to reduce the uncertainty of solutions, it achieves decent results with very few attempts. When the learning objective is fixed, ABL-PSP stands out as one of the rare algorithms that achieve the target without the need for extensive trials, and the time saved by ABL-PSP is incalculable. 
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/HMS_Incompleted.png}}
\caption{The variation of accuracy on the HMS dataset with the incomplete knowledge base.}
\label{HMS_Incompleted}
\end{center}
\vskip -0.2in
\end{figure}
\subsection{Experiments with Complete Knowledge Base}
Experimenting on the aforementioned incomplete knowledge base can partially reflect the performance ceiling of optimization algorithms when inference succeeds. However, it cannot be guaranteed that every inference will succeed in cases where the knowledge base is incomplete and labeled information is lacking. Previously, ABL used a validation set to determine whether to retrain the model midway through iterations, which is unfair to the evaluation of optimization algorithms\cite{dai2019bridging}. Therefore, we need to consider how to evaluate optimization algorithms under the premise of excluding interference from the knowledge base. Our first approach is to use a complete knowledge base for evaluation. This minimizes the influence of the knowledge base itself and the sequence of sample inputs on the evaluation process, thereby maintaining consistency between the algorithm's ability to incorporate the knowledge base into the ML model and its evaluation performance. Some related experiments have been conducted using such complete knowledge bases in prior work ~\cite{huang2021fast}, where all possible facts and conclusions are stored in the knowledge base, thereby eliminating the influence of randomness in the reasoning process on performance. However, this comparison method is utopian in practical applications due to substantial costs to construct a complete knowledge base, which is impossible on complex tasks. And if a complete knowledge base truly exists, then the optimization algorithm is no longer needed, and instead, just string matching algorithms would suffice, leading to a logical paradox: when we can evaluate the optimization algorithm, it means we no longer need it; when we need the optimization algorithm, it means we cannot evaluate it.
% This evaluation method incurs .
% , and we conducted evaluations on the DBA and HMS datasets. More detailed experimental descriptions and additional experimental results can be found in the Experiments with Complete Knowledge Base section in appendix.

% \label{Complete}
% The first method involves evaluating under the condition of having a complete knowledge base.
We have pre-built complete knowledge bases for the DBA, RBA, and HMS tasks respectively and then conducted the experiments. The experimental results are shown in \cref{DBA_Complete,RBA_Complete,HMS_Completed}.
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/DBA_Completed.png}}
\caption{The variation of accuracy on the DBA dataset with the completed knowledge base.}
\label{DBA_Complete}
\end{center}
\vskip -0.2in
\end{figure}
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/RBA_Completed.png}}
\caption{The variation of accuracy on the RBA dataset with the completed knowledge base.}
\label{RBA_Complete}
\end{center}
\vskip -0.2in
\end{figure}
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/HMS_Completed.png}}
\caption{The variation of accuracy on the HMS dataset with the completed knowledge base.}
\label{HMS_Completed}
\end{center}
\vskip -0.2in
\end{figure}
\subsection{Experiments on Rule Generation}
While a complete knowledge base may alleviate some fairness issues, it is ineffective for most incomplete scenarios. Therefore, we have opted for new metrics that no longer measure the algorithm's merits solely based on final performance but rather assess how well optimization can align the ML model with the knowledge base. 
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/DBA_Incompleted_Rules.png}}
\caption{The variation of the number of generated rules on the DBA dataset with the incomplete knowledge base.}
\label{DBA_Incompleted_Rule}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/RBA_Incompleted_Rules.png}}
\caption{The variation of the number of generated rules on the RBA dataset with the incomplete knowledge base.}
\label{RBA_Incompleted_Rule}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=1.0]{Fig/HMS_Incompleted_Rules.png}}
\caption{The variation of the number of generated rules on the HMS dataset with the incomplete knowledge base.}
\label{HMS_Incompleted_Rules}
\end{center}
\vskip -0.2in
\end{figure}
Hence, what we should primarily evaluate is the degree of consistency between the knowledge base and the machine learning model under the influence of the optimization algorithm. Specifically, after the ABL process concludes, we combine the data with the knowledge base in batches to form symbols through perception and generate new rules. We measure the degree to which the knowledge base is absorbed by the perception model by the number of successfully generated new rules. This is precisely the essence of optimization algorithms. 

We group every $s$ sample sequences and input their perceptual results into the knowledge base to count how many new rules can be generated, thereby evaluating the efficiency of the optimization algorithm. We conducted experiments with $s=3$ on 3 datasets.

% The second method, and the one we advocate the most, is to return to the original goal of the optimization algorithm: to integrate knowledge from the knowledge base into the machine learning model. 
% \label{Rules}
This evaluation strategy allows the optimization algorithm in ABL to be evaluated independently of machine learning and logical reasoning and can serve as the most important and objective basis for evaluating the performance of optimization algorithms. The experimental results are shown in \cref{DBA_Incompleted_Rule,RBA_Incompleted_Rule,HMS_Incompleted_Rules}.

The results indicate that ABL-PSP, can integrate external knowledge into machine learning models at the fastest rate.
% \section{Experimental Setup}
% 
% \section{Comparison Methods}

% \section{Proof of \cref{theorem2}}
% Suppose that in the max-heap containing the top $a-1 (a > 2)$ sequences, when sequence $B_j$ is extracted from the heap and its successor is found to already exist in the heap, it implies that there must have been a previous sequence $B_i$ that generated the same successor as $B_j$. The sequences $B_i$ and $B_j$ are identical in all positions except for $u_i$ and $u_j$, where $B_i$ flips at position $u_i$ and $B_j$ flips at position $u_j$. Additionally, $B_i$ has a lower flip value $V_{u_i}$ than $B_j$.
% This situation leads to $B_i$ having a higher probability than $B_j$, causing $B_i$ to be extracted from the heap before $B_j$, resulting in a repetition when $B_j$ is extracted and its successor is already in the heap. To resolve the repetition, a new successor needs to be generated, and the flip position changes from $u_j$ to $u_j'$. Then, consider the sequence $B_k$ with two differences from $B_j$ and $u_j'$ already flipped. It is crucial to demonstrate that $B_k$ falls into one of two categories: either it is not extracted from the heap before $B_j$, or if it is, its successor does not conflict with $B_j$.
% In the first scenario, $u_k$ is a position flipped by both $B_i$ and $B_j$. As $B_j$'s first successor is generated by the last flip $V_{u_i}$ of $B_i$, it follows that $V_{u_i} < V_{u_k}$. Moreover, since $B_i$ flips at $V_{u_i}$ instead of $v_{u_j'}$, it can be proven that $V_{u_j'} < V_{u_i} < V_{u_k}$. Thus, the sequence $B_k$ with $V_{u_k}$ not yet flipped is extracted from the heap later than $B_j$ with $V_{u_j'}$ not yet flipped.
% In the second scenario, $u_k$ might coincide with $u_i$. Here, since $V_{u_i} < V_{u_j}$ and $u_j$ remains unflipped in both $B_j$ and $B_k$, even if $B_k$ is extracted before $B_j$, there would be no conflict.
% \section{Experiments with Incomplete Knowledge Base}

% \begin{table}[tbp]
% \centering
% \caption{The evaluation results on the HMS dataset.}
% \label{HMS_T}
% \vskip 0.15in
% \begin{tabular}{c c c c c c}
% \hline\hline
%     \hline\hline
%     RACOS&58.22&68.67&0.19&-&550\\
%     SRACOS&59.11&67.11&0.23&-&650\\
%     SSRACOS&49.11&68.33&1.09&-&-\\
%     POSS&62.67&67.67&0.51&-&-\\
%     PONSS&65.22&68.44&0.08&750&\textbf{350}\\
%     ABL-PSP&\textbf{69.67}&\textbf{72.67}&\textbf{0.06}&\textbf{350}&\textbf{350}\\
%     \hline\hline
% \end{tabular}
% \end{table}
% \section{Experiments with Complete Knowledge Base}
% \section{Experiments on Generated Rules}
\section{Conclusion}
In this paper, we confront two major challenges in integrating perception and reasoning models: the problem of extensive trial and error and the issue of converting continuous and discrete variables. Building upon ABL, we propose a solution that alleviates past algorithmic deficiencies in perception information, symbol relationships, and experience accumulation through sequence-based symbol perception built upon sample perception. This approach endows ABL with meta-reasoning capability based on symbol sensitivity akin to human experts, achieving promising results with only a few trial-and-error attempts. Additionally, we introduce probability as a bridge between continuous and discrete variable conversion, presenting an efficient and rigorously complete algorithm for converting a continuous probability sequence into discrete Boolean sequences.

\Acknowledgements{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Supplements{Appendix A.}
% \section{Proof of \cref{theorem1}}
% \label{proof}
\begin{thebibliography}{99}

% \bibitem{1} Author A, Author B, Author C. Reference title. Journal, 2024, 38: 13--28

% \bibitem{2} Author A, Author B, Author C, et al. Reference title. In: Proceedings of Conference, Place, 2024. 6--12
\bibitem{NEURIPS2019_9c19a2aa} Dai, W, Xu, Q, Yu, Y, et al. Bridging Machine Learning and Logical Reasoning by Abductive Learning. In: Advances in Neural Information Processing Systems, 2019. 2811-2822.

\bibitem{amayuelas2022neural} Amayuelas, A, Zhang, S, Rao, S X, et al. Neural Methods for Logical Reasoning over Knowledge Graphs. In: Proceedings of the 10th International Conference on Learning Representations, 2022.

\bibitem{morishita2023learning} Morishita, T, Morio, G, Yamaguchi, A, et al. Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. In: Proceedings of the 40th International Conference on Machine Learning, 2023. 25254-25274.

\bibitem{Liu_2022} Liu, Y-R, Hu, Y-Q, Qian, H, et al. ZOOpt: a toolbox for derivative-free optimization. Science China Information Sciences, 2022, 65(10).

\bibitem{NIPS2015_b4d168b4} Qian, C, Yu, Y, Zhou, Z. Subset Selection by Pareto Optimization. In: Advances in Neural Information Processing Systems, 2015. 1774-1782.

\bibitem{zhou2019abductive} Zhou, Z. Abductive learning: towards bridging machine learning and logical reasoning. Science China Information Sciences, 2019, 62: 1-3.

\bibitem{dai2019bridging} Dai, W-Z, Xu, Q, Yu, Y, et al. Bridging machine learning and logical reasoning by abductive learning. In: Advances in Neural Information Processing Systems, 2019. 2811-2822.

\bibitem{huang2021fast} Huang, Y-X, Dai, W-Z, Cai, L-W, et al. Fast abductive learning by similarity-based consistency optimization. In: Advances in Neural Information Processing Systems, 2021. 26574-26584.

\bibitem{huang2020semi} Huang, Y-X, Dai, W-Z, Yang, J, et al. Semi-supervised abductive learning and its application to theft judicial sentencing. In: Proceedings of the 20th International Conference on Data Mining, 2020. 1070-1075.

\bibitem{huang2023enabling} Huang, Y-X, Sun, Z, Li, G, et al. Enabling abductive learning to exploit knowledge graph. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence, 2023. 3839-3847.

\bibitem{de2015probabilistic} De Raedt, L, Kimmig, A. Probabilistic (logic) programming concepts. Machine Learning, 2015, 100: 5-47.

\bibitem{getoor2007introduction} Getoor, L, Taskar, B. Introduction to statistical relational learning. Cambridge: MIT Press, 2007.

\bibitem{mao2019neuro} Mao J, Gan C, Kohli P, et al. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In: Proceedings of the 7th International Conference on Learning Representations, 2019.

\bibitem{manhaeve2018deepproblog} Manhaeve, R, Dumancic, S, Kimmig, A, et al. Deepproblog: Neural probabilistic logic programming. In: Advances in Neural Information Processing Systems, 2018. 3753-3763.

\bibitem{wang2018noisy} Wang, H, Qian, H, Yu, Y. Noisy derivative-free optimization with value suppression. In: Proceedings of the 32nd Conference on Artificial Intelligence, 2018. 1447-1454.

\bibitem{hu2017sequential} Hu, Y-Q, Qian, H, Yu, Y. Sequential classification-based optimization for direct policy search. In: Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. 2029-2035.

\bibitem{liu2019asynchronous} Liu, Y-R, Hu, Y-Q, Qian, H, et al. Asynchronous classification-based optimization. In: Proceedings of the 1st International Conference on Distributed Artificial Intelligence, 2019. 1-8.

\bibitem{qian2016parallel} Qian, C, Shi, J-C, Yu, Y, et al. Parallel Pareto Optimization for Subset Selection. In: Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016. 1939-1945.

\bibitem{yu2016derivative} Yu, Y, Qian, H, Hu, Y-Q. Derivative-free optimization via classification. In: Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2016. 2286-2292.

\bibitem{vaswani2017attention} Vaswani, A, Shazeer, N, Parmar, N, et al. Attention is all you need. In: Advances in Neural Information Processing Systems, 2017. 5998-6008.

\bibitem{wielemaker2012swi} Wielemaker, J, Schrijvers, T, Triska, M, et al. Swi-prolog. Theory and Practice of Logic Programming, 2012, 12(1-2): 67-96.

\bibitem{hochreiter1997long} Hochreiter, S, Schmidhuber, J. Long short-term memory. Neural Computation, 1997, 9(8): 1735-1780.

\bibitem{schuster1997bidirectional} Schuster, M, Paliwal, K K. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 1997, 45(11): 2673-2681.

\bibitem{abadi2016tensorflow} Abadi, M, Agarwal, A, Barham, P, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint, 2016.

\bibitem{lecun1998gradient} LeCun, Y, Bottou, L, Bengio, Y, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998, 86(11): 2278-2324.

\bibitem{licloses} Li, Q, Huang, S, Hong, Y, et al. Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning. In: Proceedings of the 37th International Conference on Machine Learning, 2020. 5884-5894.

\bibitem{yu2023survey} Yu, D, Yang, B, Liu, D, et al. A survey on neural-symbolic learning systems. Neural Networks, 2023, 166: 105-126.

\bibitem{zhang2020efficient} Zhang, Y, Chen, X, Yang, Y, et al. Efficient Probabilistic Logic Reasoning with Graph Neural Networks. In: Proceedings of the 8th International Conference on Learning Representations, ICLR 2020.

\bibitem{fioretto2018distributed} Fioretto, F, Pontelli, E, Yeoh, W. Distributed constraint optimization problems and applications: A survey. Journal of Artificial Intelligence Research, 2018, 61: 623-698.

\bibitem{hu2024efficient} Hu, W-C, Dai, W-Z, Jiang, Y, et al. Efficient Rectification of Neuro-Symbolic Reasoning Inconsistencies by Abductive Reflection. In: Proceedings of the 38th AAAI Conference on Artificial Intelligence, 2024.


\end{thebibliography}
\bibliography{scis_paper}


\end{document}
