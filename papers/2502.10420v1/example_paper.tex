%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Stop Acting Like Language Model Agents Are Normal Agents}
%\icmltitlerunning{Position: Language Model Agents Are Not (Normal) Agents. Stop Acting Like They Are.}

\begin{document}
%\onecolumn
\twocolumn%[
%\icmltitle{Position: Language Model Agents Are Not (Normal) Agents. Stop Acting Like They Are.}
\icmltitle{Position: Stop Acting Like Language Model Agents Are Normal Agents}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Elija Perrier}{comp,equal,yyy}
\icmlauthor{Michael Timothy Bennett}{equal}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Centre for Quantum Software and Information, UTS, Sydney}
\icmlaffiliation{equal}{Australian National University, Canberra, Australia}
\icmlaffiliation{comp}{Stanford Center for Responsible Quantum Technology, Stanford University, United States}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Elija Perrier}{elija dot perrier at gmail dot com}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Agents, Identity, Ontology}

\vskip 0.3in
%]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. 
We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.
\end{abstract}


\section{Introduction}
Language Model Agents (LMAs) \cite{xi2023rise} are agentic systems based upon large language models (LLMs). They appear to be able to reason \cite{wei_chain--thought_2023} and plan \cite{innermonologue,momennejad_evaluating_2023} in natural language, which allows them to interact autonomously \cite{kinniment_evaluating_2023} with a wide range of human systems. This overcomes some persistent limitations of classical agents \cite{bennettmaruyama2022b}. LMAs are already being used in finance \cite{han2024enhancing,zhou2024finrobot,yu2023finmem,zhang2024stockagent}, politics \cite{Yin2023-mf}, software engineering \cite{chowdhury_introducing_2024,jimenez_swe-bench_2024}, healthcare \cite{schmidgall2024agentclinic,Mehandru_Miao_Almaraz_Sushil_Butte_Alaa_2024,Tu_Palepu_Schaekermann_Saab_Freyberg_Tanno_Wang_Li_Amin_Tomasev_etal._2024}, customer service, legal services and insurance for claims management. They are being talked about as the bedrock of new economies \cite{weforum2024ai,reuters2024autonomous} in which autonomous or semi-autonomous agents transact \cite{wsj2024aiagents}, negotiate, organise and act at scale and speed. As a result, frontier AI laboratories \cite{altman_ai_killer_function} and technology companies \cite{venturebeat2024microsoft_ai_agents,fortune2025nvidia} are becoming increasingly focused on the use of LMAs \cite{altman_level3_agents,pwc2024agentic,forbes2024agentic_ai,forbes2024navigating_shift}. 

LMAs are expected by some become the default interface or `control layer' between humans and the cyberphysical systems with which we interact \cite{a16z_control_layer,ganapathy2021,bennett2025a}. Agency is a necessary step in the direction of artificial general intelligence \cite{goertzel2021,thorisson2012,wang2006rigid}. If an LMA acts like an agent, humans will tend to treat it as if it is \cite{kotrschal2015,bennett2023d}. Yet an LMA might only mimic human-like behaviour \cite{floridi2020,bennettmaruyama2022a}. It may not be agentic in the normal sense \cite{maes_agents_1994,maes_artificial_1995,lieberman_autonomous_1997,jennings_roadmap_1998,johnson_software_2011,sutton_reinforcement_2018,russell_artificial_2021,chan_harms_2023,wu_autogen_2023,openai_openai_2018,gabriel_ethics_2024,kolt_governing_2024,lazar_frontier_2024}. We will argue the extent to which LMAs are truly agentic is a significant factor in how and where they are useful. We identify pathologies of agency intrinsic to LMAs, and argue these pathologies must be acknowledged and measured if they are to be mitigated. We conclude that LMAs are not agents in the \textit{normal} sense, and by acting as if they are we limit their usefulness. Their uncanny nature need not be an impediment.

% Research and policy effort is being focused on how to govern agentic LLMs to mitigate their risks \cite{bengio_international_2024,DBLP:journals/arxiv/abs-2108-07258,chan_harms_2023,gabriel_ethics_2024}. LMA governance proposals are based upon imposing rules or normative criteria which LMAs should follow. These include adapting existing regulatory regimes that govern agents \cite{kolt_governing_2024}, mandating visibility \cite{chan_visibility_2024} into and legibility of their reasoning processes \cite{shavit_practices_2023,anthropic_model_2024}, constraining their action spaces, monitoring and tracking their activity. Research proposals are also emerging on how to govern agentic LLMs to mitigate their risks \cite{bengio_international_2024,DBLP:journals/arxiv/abs-2108-07258,chan_harms_2023,gabriel_ethics_2024}. These include adapting practices for governing agents \cite{kolt_governing_2024}, mandating visibility into and legibility of their reasoning processes \cite{chan_visibility_2024}, constraining their action spaces, monitoring and tracking their activity \cite{shavit_practices_2023,anthropic_model_2024}.

\subsection{Differentiation from Related Work}
Considerable attention has been paid to how LLMs hallucinate \cite{Rawte_Chakraborty_Pathak_Sarkar_Tonmoy_Chadha_Sheth_Das_2023}, provide incorrect information \cite{Mündler_He_Jenko_2023} or otherwise act inconsistently. Emerging research is starting to address reliability of LMA capabilities \cite{kapoor_ai_2024,DBLP:journals/arxiv/abs-2308-03688,liu_agentbench_2023,mialon_gaia_2023,lu_toolsandbox_2024,zhang_cybench_2024,jimenez_swe-bench_2024,wu_introducing_2024,chowdhury_introducing_2024,gur_real-world_2024,multion_multion_2024,gu2024agent}. But comparatively little research exists on how the nature of LLMs affects the classical \textit{agentic} properties of LMAs upon which their widespread use relies. Most proposals for LMAs assume they identifiable, can be distinguished from their environments, persist continuously over time and act consistently. These assumptions are problematic. LMAs are built upon LLMs which are stateless, stochastic, semantically sensitive and linguistically intermediated. These properties or `pathologies' make identifying continuous, persistent and consistent LMAs and their boundaries difficult. This problematises claims that LMAs reliably satisfy agency criteria. In effect, LMAs suffer from an identity crisis. This has consequences for their reliability, utility and trustworthiness.

\subsection{Position}
Here we argue that LMAs face an identity crisis. This stems from the LLMs around which they are constructed. LMAs are not \textit{normal} agents. They have intrinsic pathologies, which we enumerate here. %Yes, there is already some research on LLM-related problems affecting LMAs (such as jailbreaking agents \cite{gu2024agent}). 
Yet many LMA proposals treat LMAs as something like normal agents. This is both incorrect and normatively undesirable. It engenders a false sense of utility and trustworthiness in LMAs. We do not argue against the use of LMAs. Quite the contrary. Our position is that the identity crisis underpinning LMAs requires attention and, perhaps most importantly, means of scientifically evaluating the extent to which LMAs are ontologically robust and actually satisfy agentic criteria. The pathologies of LMAs should be acknowledged, measured and mitigated. We discuss these issues and sketch out our proposal for agentic evaluation below.




% Yet the core architecture of LLMs, which underlies every LMA, was built for next-token prediction rather than durable agency. Classical views on agency highlight enduring traits such as persistent identity, autonomy, consistent goals, and clear boundaries from the environment \cite{wooldridge_introduction_2009,Schlosser_2019}. LLMs, by contrast, exhibit \emph{statelessness}, \emph{stochastic outputs}, \emph{semantic hyper-sensitivity}, and a form of \emph{linguistic intermediation} that can rewrite context on the fly. These so-called ``pathologies'' \cite{shanahan2024talking,wei_chain--thought_2023} are no mere glitches: they are the engines of LLMs’ creativity and breadth, but they also undermine traditional agentic qualities.

% From the outset, we suggest that LMAs combine two conflicting imperatives. On one hand, they inherit an LLM’s power to generate fluid, contextually rich behaviours across varied tasks. On the other hand, these same dynamics erode stable identity conditions and consistent intentionality, complicating claims that LMAs are truly agentic. Much of contemporary LMA design centres on adding scaffolding—external memory, tool handlers, multi-step planners—to approximate the traits we expect of ``agents.'' While these additions improve real-world utility, they cannot excise the underlying next-token mechanism that drives both LLM adaptability and its pathological brittleness. This paper offers a perspective on how these tensions manifest and why they remain inescapable for current-generation LLM-based systems.

\section{Agentic Identity}
\subsection{What is a Language Model Agent?}
To govern an agent of any sort we need to specify what it is, and is not. We need identity criteria. Entities which satisfy identity criteria (in whole or part) are classified as agents. Theories of agency describe such criteria, providing an ontology of agents: what types of agent there are, the classification of their properties and how they may vary. While there is no universal, agreed upon definition of what constitutes an agent e.g. \cite{franklin1997agent, Schlosser_2019}, there are common features. At minimum, an agent must be distinguished from its environment. It must be capable of action, pursuing goals and interacting with the environment responsively in accordance with plans, practical reasoning or intentional states \cite{russel_norvig,wooldridge1995intelligent}. A variety of detailed criteria also exist, such as functionalist, cognitive and legal where agency is a matter of degree rather than a binary property of systems. Different types of agentic systems satisfy agentic criteria to greater or lesser degrees. Our claim in this Position paper is that regardless of which criteria of agency is chosen, LMAs struggle to satisfy it. This is a somewhat contrarian view. LMAs appear on their face to satisfy these criteria in abundance. They seem to exhibit independence, autonomy, reactivity, reasoning capabilities and the capability to act in ways that far exceed traditional computational agents. We are able to easily and readily interact with these systems as if they were agents who can converse, generate plans and execute upon them. It is therefore understandable why the use of LLMs is often framed in terms of agentic concepts, and why it seems natural to do so. 

\subsection{Ontological Identity Conditions for Agents}
However, as we argue in this Position paper, the appearance of LMAs satisfying criteria of agency is upon deeper investigation problematic. In both philosophical and computational treatments of agency, how we identify agents and their boundaries is critical (see \cite{Olson_2024} for a review). So too is the ability to demarcate and re-identify the same agent and its properties as persisting over time. Identification of agents is generally treated as prerequisite for attributing higher-level agentic properties to a system, such as the ability to reason, deliberate, plan and execute tasks. We denote the these ontological \textit{agent identity conditions} as they consist of conditions upon the manner in which agents and their properties are identified. 


\subsubsection{Identifiability}
The first condition is that LMAs be \textit{identifiable}. We must be able to (synchronically) identify what an agent is and what it is not. Identifiability requires: (i) a legible and well-formed set of criteria by which to identity an agent (\textit{identity criteria}); and (ii) a means of determining whether any system or phenomena satisfies this criteria (\textit{satisfiability criteria}). Identifiability also implies that an agent must be causally \textit{distinguishable} from its environment and other agents \cite{bennett2022b}. In other words an observer must be able to construct a means of discriminating between the environment, and those parts of that environment we call the agent: `causal identities' classifying the agent's causal interventions, and the 1st order effects of agentic properties such as autonomy, planning, reasoning, perception \cite{bennett2023c,bennettwelshciaunica2025}. 
% These requirements are implicit in identity condition, because to identify an agent it must be distinguished from that which it is not and by doing so it may be individuated.

% \item \textit{Continuity}. 
\subsubsection{Continuity}
LMAs should maintain their (diachronic) identity over time, even a very short time \cite{Bratman2000}, throughout their instantiation. In the extreme case, an agent that exhibits no continuity from one time step to another is impossible to identify. But an agent also ought to be adaptable. It must change. An immutable system is not agentic. How continuous and which properties ought to be continuous about an agent depends on context. But it is clear that at least a de minimis degree of continuity is required to underpin anything agentic about an LMA system, including its ability to take action, their coherence and accountability. If we have continuity then an observer should, in theory, be able to construct a reasonably specific classifier or causal identity denoting a given instantiation of the agent, based upon its behaviour.  

\subsubsection{Persistence}
LMA ontology conditions also imply a degree of persistence (be it intentional \cite{dennett_intentional_1971} or psychological \cite{Parfit1984}). We distinguish persistence from continuity as the maintenance of an LMA's identity across different instantiations (distinct from a single session or instance). Persistence also implies that the properties of an agent and its identity ought not to be so sensitive to perturbations as to dissolve or radically alter. If LMAs do not exhibit persistence (e.g. where the same prompt sequencing and scaffolding lead to quite different outputs), it calls into question whether an instance of an agent was anything more than the stochastic output of the LLM itself. If we have persistence then an observer should, in theory, be able to construct a reasonably specific classifier or causal identity denoting the LMA \textit{across} instantiations, based upon its behaviour\footnote{Persistence requires a so-called `weaker' \cite{bennett2023b} causal identity than continuity.}.

% \item \textit{Consistency}.
\subsubsection{Consistency}
Consistency refers to the coherence of an agent’s description and actions according to which it is identified. An agent's state description - and that of any of its properties - should not be contradictorily described. It should not be described by a predicate and its negation, or by inconsistent outputs. An LMA prompt or trace riddled with contradictions would undermine most attempts to identify it as an agent. Consistency also has another sense in terms of consistency of objectives and actions with intentions. 
 
The ontological identity conditions above are not unique to agents. Nor are they sufficient to constitute something as an agent. However, they are necessary preconditions of agency. For human agents, satisfaction of identity conditions is anchored in physical embodiment and cognitive unity\cite{Parfit1984,dennett_intentional_1971}. For artificial agents which lack physical embodiment, such as corporations \cite{ListPettit2011}, it occurs by way of stable institutional practices, such as the law. For classical computational agents \cite{wooldridge_introduction_2009}, such as those based upon formal systems, it occurs via their relatively fixed ontologies of limited scope. But these necessary conditions are no longer guaranteed to be satisfied in case of LMAs. This in turn means that whether LMAs satisfy the properties that causes us to classify them as normal agents - such as reasoning, planning, autonomy, reactivity - may be called into question. This can be seen by considering how LMAs are constituted using LLMs and the structural scaffolding, such as memory, tool use and infrastructure, that supports them.

\section{LLM Pathologies} 
%\label{section:Language Model Agents}
% \subsection{LMAs and LLMs}
The foundation of an LMA is an LLM that takes prompts and outputs text (or other data modalities) conditioned on those queries. This procedure is undertaken post-training, at inference stage. While LMAs can be described at different levels of abstraction — from the micro (model) to macro (output) level - behaviour that appears recognisably agentic only arises at the macro scale. We cannot reliably identify agentic behaviour at the level of activations for example. We are therefore primarily reliant upon on macro-level outputs of LLMs and scaffolding around them to infer whether these systems meet agentic criteria and identity conditions.

Because of the way transformer models are designed \cite{radford2019language}, LLMs that form the bedrock of LMAs exhibit exhibit distinct characteristics of being stateless, stochastic, semantically sensitive and linguistically intermediated. These properties are integral to their computational power and versatility. But they also are the source of instability and uncertainty when it comes to LMA identity. For this reason and in the context of their effect on LMA identity, we denote them LLM pathologies.

\subsection{Statelessness}
First, LLMs are stateless. They do not store a persistent record of prompts and outputs from one interaction to the next \cite{merrill2024illusion,vaswani2017attention}. A query and response exist in isolation unless additional context is explicitly included. Consequently, there is no classical concept of state transition within an LLM at inference. This is in sharp distinction to traditional agents whose evolution over time is represented via changes of state. LLMs are often discussed in terms of inhering world models \cite{DBLP:journals/arxiv/abs-2305-14992,DBLP:conf/icml/NottinghamAS0H023,DBLP:journals/arxiv/abs-2306-12672,brooks_video_2024}, but this framing is problematic. LLMs do allow construction of representations of worlds, but lack the usual concept of state integral to their continuity and persistence. This ephemeral nature of LLM facilitates broad reusability: the same model can handle a plethora if different queries without being restricted to a particular context. Yet this statelessness compromises persistence of identity or memory and can give rise to inconsistencies. If the user does not reinsert context, the LLM will not recall commitments  or preferences from earlier exchanges \cite{merrill2024illusion}. This is in contrast to traditional agents which are stateful in some way (such as being described by states). Even formal computational agents usually have an identifiable system state at the level of computation (e.g. the configuration of a circuit at a point in time). 

\subsection{Stochasticity}
Second, LLMs are stochastic \cite{Bender_Gebru_McMillan-Major_Shmitchell_2021,li2023transformers,cui2024bayesian}. Repeating the same query can yield different or incorrect outputs \cite{ferrando2024do}. This unpredictability makes it difficult to distinguish stable traits that might serve as evidence of a coherent agent across time. Attempts to dampen this variability, such as adjusting temperature parameters, can have mixed results. LLMs generate output by sampling from probability distributions over tokens \cite{vaswani2017attention}. This stochasticity facilitates creativity and divergent problem-solving, but it also means repeated prompts might yield contradictory outputs. While agentic theory expects a measure of autonomy, it typically presupposes consistency over identical conditions, which is difficult to guarantee when the system’s next token is partially determined by random sampling.

\subsection{Semantic Sensitivity}
Third, LLMs exhibit high semantic sensitivity. Small perturbations in input wording can accumulate into significantly divergent outputs \cite{Wang2023,Zhu2023-vz}. Saturating an LMA with different context can alter its core properties in ways that do not occur with normal agents. This phenomenon is seen in jailbreaking, adversarial attacks \cite{mcdermott2023robustifying,moradi2021evaluating,wang2023kgpa} or contradictory responses \cite{zhang2024measuring} where slight prompt modifications can produce unexpected or inconsistent behaviour despite safeguards \cite{Mei2023}. It is also discernible in what we can denote as \textit{context attrition} \cite{shi2023large,leng2024long}, the fact that as we layer more and more context into query, the weight ascribed to features (such as agentic features) can diminish. Seemingly trivial changes in phrasing can swing an LLM’s response dramatically \cite{moradi2021evaluating,mcdermott2023robustifying}. An LLM's training on extensive text corpora can render it highly reactive to minor context shifts. Although this property can be harnessed for precise prompt engineering, it also endows LLMs with a sensitivity that can undermine the stability of its agentic properties, such as goals, reasoning, planning and execution which can all be modified by subtle textual cues.

\subsection{Linguistic Intermediation}
Fourth, LLMs are linguistically intermediated. Everything is filtered through to tokens of text and embedding-space representations \cite{shanahan2024talking}: agent definitions, environment descriptions, actions, events, and prompts. This is a form of computational dualism, meaning interaction between the LLM and the LMA's environment is filtered through an `abstraction layer' and subject to its interpretation \cite{bennett2024a}. In interactive settings this can undermine constraints upon or claims regarding behaviour. For example, the general reinforcement learning agent AIXI was initially thought to be Pareto optimal \cite{hutter2010}. This claim was undermined when the agent's performance was shown to hinge upon a choice of universal Turing machine \cite{leike2015}. Likewise, natural language constitutes an additional abstraction layer that separates the software `mind' of an LMA from the environment in which it pursues goals. All information is expressed in tokens. This is certainly nothing like how interaction occurs traditionally where agents perceive, adapt and act. Nor does it resemble how biological self-organising systems enact cognition within the world \cite{thompson2007,ciaunica2023,friston2023,bennett2025a}. Rather than sensing objects or states, the LLM receives tokenized descriptions of them and responds in kind. Some information may be lost. Adaptation is separated from embodiment by an abstraction layer, which can potentially reduce efficiency \cite{bennett2024a,bennett2025a}. Unintended ambiguities or adversarial phrasing thus can shift the LLM’s perceived reality, undermining agentic boundaries and consistent situational awareness. 




\subsubsection{LLM-Only LMAs} We can see how LLM pathologies problematise LMAs by considering the simplest model of an LMA: an LLM-Only agent which operates purely via repeated calls to an underlying language model, with no cumulative context, nor external memory, nor tools. In realistic LMA scenarios, they are equipped by scaffolding, but this scenario is useful to illustrate our point. In a single prompt-response situation (with no memory and no caching as is common on LLM platforms like ChatGPT), there is minimal data to establish identity criteria upon which to identify an agent at all: only a solitary trace in the form of a query-output tuple. The boundaries of any purported agent are inseparable from this record. Individuating an agent using text data alone is difficult. It is therefore unclear how to distinguish such an agent from (i) the LLM itself, (ii) its user prompts, or (iii) the environment described by the text. 
% None of these are satisfactory options. Equating the LMA with the entire LLM is highly problematic. While its weights are persistent, the inferential outputs are stateless. LLMs can produce contradictory responses and inconsistencies that make identifying anything like a consistent agent challenging.  

Because the LLM is stateless, the system lacks any built-in mechanism for maintaining continuity of decisions or output over time. Nor do we see meaningful guarantees of consistency through repeated interactions. An essential element of any identity criteria is that it provides a means of being able to identify the same thing by its repeated application. But repeating the same prompt may yield different outputs. This is due to the stateless and stochastic nature of LLMs in concert with linguistic intermediation. For traditional agents, consistent responses arise because of a state which is unaffected by the act of querying. This might be physical or ontological, in the sense of classes and rules which constrain the agent. %the permitted configurations of an agent in some object language but which remain external to such language. 
This is not the case for LLMs where the query can instrumentally affect the ontology of the LMA in ways very different from normal agents. 

Semantically similar inputs (e.g. schemas for designating an LMA) which vary slightly may lead to large differences in output, upon which an LMA's properties are inferred. If two queries are constructed with the intention of referring to the same agent, there is no way to determine if they refer to the same underlying LMA, or two different instantiations. From an observer's perspective, there is no `causal identity' denoting interventions by a particular agent \cite{bennett2023c,pearl2018}. Multiple LMAs could be running on top of the same LLM, making it difficult to distinguish an LMA from the LLM, or different LMAs from each other. We might try to unify multiple outputs into a coherent narrative of a single agent. But the inherent stochasticity of LLMs hinders consistency across queries. These features of LLMs challenge the identifiability of LMAs. They make it uncertain whether we can confidently infer that the same agent is persisting from one interaction to the next. 

\subsubsection{LLMs with Context} Consider the next simplest model: an LLM-only LMA but where context is added. This is a common strategy in response for achieving the semblance of persistent agency and continuous interactivity. Context includes the history or summary of previous outputs \cite{zhang2019consistent,gekhman2023robustness}. Adding context does enhance the consistency of responses across multiple prompts and enable more coherent conversations. Yet the underlying LLM remains stateless. The prompts merely carry forward relevant text from earlier exchanges. It does not solve the deeper problems of stochasticity and semantic sensitivity. Outputs remain probabilistic. Small perturbations in context can produce large and unpredictable changes in output. Consequently, any apparent persistence of an LMA is affected by how a user curates, summarises, or appends prior outputs, rather than a property of the LLM itself. If the appended history is incomplete or semantically altered, previous decisions might be lost or reversed. Text-based context can be rearranged or truncated, making it difficult to track the same agent’s boundaries across repeated interactions. Likewise, the distinguishability of LMAs is rendered uncertain when many agents share overlapping contexts. These problems undermine any strong notion of agentic continuity simply by adding context.

\section{LMA Scaffolding}
\label{section:Scaffolding}
In an attempt to overcome these limitations, architectural scaffolding can be used:
\begin{enumerate}
    \item \textit{Memory} \cite{wang_augmenting_2023,DBLP:journals/arxiv/abs-2308-01542,zhang2024survey}, such as in the form of browser caching, databases or other information registers which enable the retention of information; and
    \item \textit{Tools} \cite{schick_toolformer_2023,lu_toolsandbox_2024}, which serve as extensions that enable the LMA to effectually act via interaction with other external systems, such as via executing code, or control physical devices.
    \item \textit{Planning} \cite{huang2024understanding} as a separate and distinct module (planning is in practice manifest via a combination of memory, tool use and prompting).
    \item  \textit{Infrastructure} \cite{a16z2023emerging,chan2025infrastructure}, this may include containerised instances of LMAs e.g. via Docker \cite{docker2023llmeverywhere}, or ecosystems such as cloud technology stacks, or distributed networks.
\end{enumerate}
The purpose of scaffolding is twofold: to overcome underlying LLM pathologies and to provide LMAs with agentic capabilities, such as being able to use tools to perform tasks, or long-term memory for planning and reasoning. The modularity nature of scaffolding means there are numerous possible configurations of LMA architectures. But as we show below, despite considerable improvement in robustness, versatility and utility, scaffolding does not fully address the underlying effect of LLM pathologies in LMA identity.

\subsection{Memory Mechanisms}
Firstly, we examine the effect of memory scaffolding on LMA ontology. Consider an LMA constituted by an LLM with external memory modules or additional storage \cite{zhang2024survey}. This is a common approach in attempting to overcome the statelessness of LLMs. Memory may take the form of browser caches, external databases, or specialised vector stores that maintain relevant text, summaries of previous actions, and user interactions \cite{wang_augmenting_2023,zhong2024memorybank}. Memory mechanisms aim to instantiate a degree of persistence, allowing the LMA to reference past states or decisions. This may be to enable the LMA to perform multi-turn tasks or maintain context across longer interactions, as is required for chain-of-thought reasoning \cite{wei_chain--thought_2023}. However, the LLM itself is never truly updated by these memory modules. Memory is just another form of context. The LLM simply ingests more data as part of each query. As a result, continuity and distinguishability of LMAs rely heavily on how that memory is orchestrated. A memory store might contain a detailed record of previous interactions, but any subsequent LLM-generated output can still deviate substantially from that record. With minor prompt alterations, context attrition in long-term memory \cite{dannenhauer2023memory} or noisy retrieval, LLM outputs may contradict prior statements \cite{Mündler_He_Jenko_2023} or lose saliency, compromising the consistency of the LMA over time. 

External-memory also complicates LMA identifiability. Two or more LMAs can share the same LLM, but point to different (or partially overlapping) memory stores. In this case, it is unclear whether we have multiple distinct LMAs or just different views of the same underlying system. A single LMA can dynamically switch or shuffle memory modules depending on relevance. This can undermine its identity by causing it to become reconfigured in ways that break continuity. Memory scaffolding can improve user-facing coherence of LMAs, particularly for extended, multi-turn applications. But it falls short of guaranteeing the stable boundaries and consistent identity demanded by the traditional criteria of agency.

\subsection{Tool Use and API Integration}
The second cornerstone of LMA scaffolding is tool use and integration. When LMAs gain the ability to invoke external tools, they extend their reach into broader environments. Tools allow LLM textual outputs (e.g. code) to trigger real actions \cite{toolformer,bran2023chemcrow,DBLP:journals/arxiv/abs-2308-03427} and interact with the environment. Tool use can improve LMA performance \cite{DBLP:journals/arxiv/abs-2305-11738}. It typically requires adherence to schemas or formal inputs which can also enhance predictability. In many cases, tool use can be readily traced. If an LMA calls an API with specific parameters, we can record that event in more structured logs. This can yield a partial audit trail \cite{Waiwitlikhit2024-gv,mokander_auditing_2023}, or `trace' of actions. This is often how practical agentic applications frame LMA identification \cite{Chase-LangChain-2022,wu_autogen_2023}. LLMs can also autonomously accumulate tools when paired with actuating environments, like SDEs, for access to external services or code execution \cite{schick_toolformer_2023,lu_toolsandbox_2024}. This can include creating full applications and orchestrations among multiple applications. The agent can call an API, parse the response, and incorporate the result into its output. This leaves a considerable depth of trace data which is often used to identify LMAs and often fosters the appearance of agentic autonomy.

Yet tool use (or creation by LLMs) remains linguistically intermediated and subject to stochastic generation and semantic fragility. An inadvertent or malicious prompt can steer the model to misuse tools or produce nonsensical commands. The model’s environment is mediated entirely by language, leaving it susceptible to manipulations that exploit linguistic oversights. In any case, tool use does not solve the fundamental problems facing LMA identity. The LLM remains free to generate varying tools or contradictory tool-calling directives. Tool use may be of varying accuracy or quality \cite{lu_toolsandbox_2024,furuta2023language}. The same textual instruction that guided a prior tool operation might in another context trigger some other action, eroding consistency of the LMA's actions over time. Multiple LMAs (e.g. within the same session or instantiated in a single prompt) might share tools, further blurring where one LMA ends and another begins. The policies and tools which are used by LMAs may also be ambiguous. The action space of an LMA may be difficult to discern. Complicated coding structures integrated within the LMA itself may make tools difficult to distinguish from the LMA itself. Semantic sensitivity can mean that tool use can be subject to prompt injection \cite{zhan_injecagent_2024} and adversarial attacks in unexpected ways. As such, sophisticated tool integration does not eliminate the deeper problems of a lack of LMA continuity or how to unify the LMA into a single identifiable agent with stable boundaries.

\subsection{Cognitive Architectures and Planning Modules}
More elaborate scaffolding frameworks introduce chain-of-thought prompting, hierarchical planning modules, or meta-level reflection \cite{wei_chain--thought_2023,valmeekam_can_2023}. These can yield more systematic reasoning steps, reduce shallow guesswork, and encourage the LLM to “explain” intermediate decisions. Chains of thought are usually claimed to mirror an agent’s deliberative processes \cite{wei_chain--thought_2023}, but this is known to be problematic \cite{r93}. Despite impressive gains in reliability, these methods still revolve around the LLM and remain exposed to its pathologies. A single contradictory token can unravel the entire plan. Where standard agents update an internal state following each step, an LLM may simply produce textual placeholders of state, which, if inconsistent or corrupted, can lead to inconsistent reasoning.

These characteristics of memory integration mean that proposed cognitive architectures such as COALA \cite{sumers_cognitive_2023} which promise more elaborate internal processes for reasoning and planning (such as short-term working memory, long-term semantic and episodic stores, and procedural modules) are problematised. LMAs have no mental states per se in any traditional sense of the word. Memory scaffolding doesn't change this. An LMA's mental state is just an inference made upon using the cumulative trace of its outputs, something we infer as resembling a state of mind. 
% This is similarly the case for descriptions of LMA systems in terms of behaviour. 

\section{Alternative Views}
Our arguments above are premised upon a close analysis of how the features of LLMs that underpin LMAs give rise to pathological effects which propagate in ways that challenge the claim that LMAs are normal agents. Yet there are a range of alternative views and challenges to our claims, ranging from questioning whether the same criticisms might be levelled at traditional agents, to whether LLM properties such as statelessness and stochasticity may be present within normal agents also. We focus on two primary alternative views below.
\subsection{LLM pathologies are not unique}
An alternative view to the one we offer is that the problems of LMA identity are not unique to LMAs and could be said of almost any agent to some degree. For example, it may be objected that LLM weights preserve information in essentially the same way as memory and that querying via prompts approximates the process of memory itself. Responses to queries are not usually wildly chaotic. They might exhibit some deviations in edge cases, but they are reliably salient. The retention of information via weights is clearly true of LLMs. But it is currently difficult or impossible to discern at the activation exactly where or what memories reside. Information may be held in superpositions \cite{henighan2023superposition,elhage2022toy} that are difficult to disentangle and impose boundaries upon. And even then such information can still be unreliable or subject to hallucination \cite{ferrando2024do}. The reliable saliency of outputs can be relatively easily undermined. The same is true of representations of agency. LLM weights do not provide the sort of stable basis of memory we would expect of typical agents. While internal and external memory can and does improve planning abilities, LMAs can still exhibit inconsistency and unpredictability between plans and task execution \cite{mallen2023when,valmeekam_can_2023,valmeekam_large_2023}. Cognitive structuring such as COALA is therefore at base simply more intricate context architecture. It does not address the confounding of LMA identity and boundaries that arise from LLM pathologies.

\subsection{LMA identity problems are inconsequential}
A second objection to our position is that the LMA identity and ontology are largely irrelevant because all that matters is functionality. The extent to which LLM pathologies confound the identity conditions underpinning agency criteria is largely an empirical question. It may be that in the future models are developed to overcome such issues e.g. embedding statefulness in some way. In certain cases, these problems may be of limited consequence. But for complex planning tasks, especially where stakes are high, the consequences may be considerable. It may be objected that variation in how an agent is identified is inconsequential. After all, humans, corporations and other traditional agents exhibit variation in their attributes, yet their agency is not called into question. While it is true that traditional agents do indeed vary across their properties and states, the way in which they satisfy identity conditions is not grounded in anything like an LLM. A human's persistence is irrespective of how they are described. A corporation's persistence depends upon persistent reliable legal practices. A classical computational agent upon formal logically-instantiated code.



% \section{An Identity Crisis}

\subsection{Scaffolding can mitigate but not cure the LMA identity crisis}
Although scaffolding strategies partially mask or mitigate the pathologies, they cannot eliminate them without compromising the generative breadth that make LLMs so versatile in the first place. Memory modules, APIs, and multi-step reasoning each rely upon textual input and output; any change in assumptions encoded within scaffolding can expose underlying unpredictability. Consequently, LMAs remain constrained by the same design choices that underlie their flexible creativity. In our view, this actually highlights a feature that we conjecture is generic about LLMs and LMAs in general: that there is a necessary trade-off between the ontological stability of such systems on the one hand and their power on the other. The more ontologically rigid a system, by definition the less variance. In this sense, the conundrum of LMAs is in analogous spirit with no free lunch theorems or typical bias-variance trade-off where the more deterministic a system, the more identifiable, continuous and consistent it is, but at the cost of less expressivity, generalisability or versatility.

\section{Consequences and Responses}
\subsection{Reliability and Predictability}
There is usually little emphasis upon these foundational identity challenges faced by LMAs arising from their instantiation upon LLMs. Often uncertainty over LMAs is referred to in the small print or relegated to discussion of edge cases. The paradigm of LMAs for developers and consumers remains that of normal agents. But LMAs are not normal agents. And the effects of LLM pathologies are not edge cases.  They are inherent to any LMA architecture and it is very unclear whether they can be remedied, because they stem from transformer models on which they are based. 

\subsubsection{Interference with LMA utility}
The consequences of the unstable basis of LMA identity are significant. Without a grounding in robust, persistent identity, LMAs and any systems relying or built upon them will be subject to irreducible uncertainty. For low-risk uses this may be of little concern. But for high-stakes and high-impact decision-making, the tenuous grounding of LMAs mean that it is difficult to envisage circumstances in which the promise of truly autonomous and reliable LMA systems is achievable. LLM pathologies have been shown to confound attempts at aligning LLMs \cite{anwar2024foundational}. Attempts to govern LMAs by constructing their identity in a certain way, such as via prompt engineering constitutions \cite{DBLP:journals/arxiv/abs-2212-08073,huang_collective_2024}, imposing internal rules \cite{Schuett2024-ps} or schema, or by imposing external rules, infrastructure or environments, do not address the underlying problem of LMA identity. This lack of persistent grounding of LLMs and LMAs also manifests in well-studied behaviour such as hallucinations \cite{Azamfirei_Kudchadkar_Fackler_2023} and other problems with transformer models. From an AI safety perspective, we know little to nothing of the distribution of these failure modes. Modern model evaluation techniques being far from resembling anything like a science or possessed of the rigour expected in truly high-stakes and high-impact decision-making.

\subsubsection{Undermining trustworthiness of LMAs}
The crisis of LMA identity thus has direct pragmatic consequences for their utility and trustworthiness and the brave new world of agentic AI that relies on such assurances. LMAs may fail to deliver reliable, agentic performance on complex tasks requiring strict consistency. Over-reliance on apparent agenticness of LMAS can mask the fact that, despite sophisticated scaffolding, they may lack the persistence, continuity and predictability expected of normal agents. This interferes with their utility in production chains and workflows which require consistent, predictable, output. And it is obviously a particular problem for safety-critical applications. While scaffolding can reduce the odds of contradictory or drifting outputs, the underlying randomness and prompt sensitivity never vanish. Software integrators and domain experts must account for the possibility of erratic shifts, even after thorough testing \cite{momennejad_evaluating_2023,shavit_practices_2023}.


\subsection{A Response: Agentic Evaluations}
To address these foundational ontological dilemmas and consequences of the LMA identity crisis, our position is that researchers and developers ought to consider the following: 
\begin{itemize}
    \item \textit{Mechanistic Agentic Interpretability}. Internal to LMAs, we should aim for mechanistic interpretability tools embedded within models which can identify structures or processes occurring at the micro (model) level which correlate or causally relate to agentic properties observed of LMAs at higher levels of abstraction. Doing so would have multiple benefits. Firstly it would allow us to holistically understand in more detail how the underlying LLM architecture affects LMA agency and identity. Secondly, it ought to provide a steer towards how agents may be made more robust, or at least shed light on why this may not be possible.
    \item \textit{Agent Identity Evaluations}. External to LMAs, we ought to foster a risk-based way of quantifiably measuring and monitoring the degree of a system's \textit{agentic identity}. This we argue should include rigorous scientific evaluations of how effective LMA scaffolding configurations are at preserving the ontology and properties of agents. Agent identity evaluations could in principle be deployed at all stages of the LMA life-cycle, during LLM training, testing, inference and scaffolding stages. Quantifying the variance of agenticness using risk models would fit within how modern risk analysis works, allowing decision-makers such as boards, governments and stakeholders assess how likely agentic systems are to meet sought after criteria (e.g. reasoning, autonomy and so on).
\end{itemize}
These measures come down to assurance of AI agentic identity. They are a means of ensuring the LMA we are interacting with is actually the same LMA over time; that it actually does exhibit concrete agentic properties and that it will remain that way to an acceptable degree. Our call to re-evaluate how we approach the ontology of LMAs does not seek to address the causes of the LMA identity crisis. We hold those are intrinsic to LLMs and their versatility. However, from a practical perspective what matters is the distribution of those pathologies, failure modes and edge cases.


\section{Conclusion}
In this Position paper, we have argued that LMAs are not \textit{normal} agents, and that to deploy them most effectively we should stop treating them as if they are. While LMAs clearly demonstrate potential and can convincingly simulate agentic behaviour, they rest upon an unstable foundation presaged by the inherent pathologies of LLMs identified above. The same core characteristics that enable LLMs’ creativity and adaptability also undermine continuity of goals, autonomy, and reliability. Contemporary scaffolding approaches can patch over many surface deficiencies but as we have argued they are necessarily limited in their ability to align LMAs due to the inherent underlying computational model of LLMs themselves.

Recognising this tension from the start may encourage more pragmatic deployment strategies. Rather than presuming that LMAs are stable, we have argued that developers and researchers as a first step ought to design and apply statistical, scientifically-driven, ways of measuring the degree to which systems are agents. By doing so, the extent to which those systems - LLMs with scaffolding configurations - are ontologically robust or not can be quantified. This in turn can assist efforts in how both internal and external measures may be tuned to address these pathologies, or provide more insight into why the trade-off between LMA bias and variance is inevitable. Evaluating agentic identity also ought to provide a greater handle on how we manage the risks of using LMA systems in a way to maximise their utility and trustworthiness. With the appropriate evaluation and response, although LMAs might never be normal agents, they might not need to be.

\subsection*{Impact Statement}
Our work aims to bring a more quantitative and evidence-based methodology to AI agent research and deployment, particularly in order to heighten awareness of the risks and unique character of LMAs.

\bibliography{example_paper,library,refs-technical-governance,refs-control,refs-agentinfra,refs-examples}
\bibliographystyle{icml2025}
% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% % In the unusual situation where you want a paper to appear in the
% % references without citing it in the main text, use \nocite
% \nocite{langley00}

% \bibliography{example_paper}
% \bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
