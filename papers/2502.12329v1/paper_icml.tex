%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
 %\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}

\newcommand{\artem}[1]{\todo[inline]{\textbf{Artem: }#1}}
\newcommand{\ahmed}[1]{\todo[inline]{\textbf{Ahmed: }#1}}
\newcommand{\peter}[1]{\todo[inline]{\textbf{Peter: }#1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Novel Unified Parametric Assumption for Nonconvex Optimization}

\begin{document}

\twocolumn[
\icmltitle{A Novel Unified Parametric Assumption for Nonconvex Optimization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Artem Riabinin}{yyy}
\icmlauthor{Ahmed Khaled}{comp}
\icmlauthor{Peter Richt{\'a}rik}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia}
\icmlaffiliation{comp}{Princeton University, Princeton, USA}

\icmlcorrespondingauthor{Artem Riabinin}{artem.riabinin@kaust.edu.sa}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}

\begin{abstract}
Nonconvex optimization is central to modern machine learning, but the general framework of nonconvex optimization yields weak convergence guarantees that are too pessimistic compared to practice. On the other hand, while convexity enables efficient optimization, it is of limited applicability to many practical problems. To bridge this gap and better understand the practical success of optimization algorithms in nonconvex settings, we introduce a novel unified parametric assumption. Our assumption is general enough to encompass a broad class of nonconvex functions while also being specific enough to enable the derivation of a unified convergence theorem for gradient-based methods. Notably, by tuning the parameters of our assumption, we demonstrate its versatility in recovering several existing function classes as special cases and in identifying functions amenable to efficient optimization. We derive our convergence theorem for both deterministic and stochastic optimization, and conduct experiments to verify that our assumption can hold practically over optimization trajectories.
\end{abstract}

\section{Introduction}

There is a large disconnect between the theory and practice of nonconvex optimization with first-order methods. The theory for nonconvex optimization allows us only to guarantee convergence to a stationary point, or at most, a higher-order stationary point~\citep{carmon17_lower_bound_findin_station_point_i,carmon17_lower_bound_findin_station_point_ii}. In practice, neural scaling laws show smooth decreases in the loss function value as the number of training steps increases~\citep{kaplan20_scalin_laws_neural_languag_model}. In contrast, convex optimization theory typically allows us to derive tight guarantees on the function value~\citep{nesterov2018introductory}, but is too restrictive to apply to nonconvex models directly.  This discrepancy has motivated researchers to develop intermediate theoretical frameworks that allow us to obtain stronger convergence guarantees without losing too much applicability. These developments include star convexity~\cite{star-convexity}, quasi-convexity~\cite{QCvx, bu2020notenesterovsacceleratedmethod}, the Polyak-\L{o}jasiewicz (PL) condition~\cite{PL1,PL2}, Aiming~\cite{liu2023aiming}, and the ${\alpha}$-${\beta}$ conditions \cite{islamov2024losslandscapecharacterizationneural}.

\textbf{Problem statement.} We are primarily concerned with the minimization problem
$$
\min _{x \in \mathbb{R}^d} f(x),
$$
where \( f(x): \mathbb{R}^d \to \mathbb{R} \) is a differentiable objective function. We focus on variants of gradient descent of the form
$$
x^{k+1}=x^k-\gamma^k \nabla f\left(x^k\right),
$$
where \( \gamma^k > 0 \) is a stepsize, and \( \nabla f(x^k) \) represents the gradient of the function $f$ at the current point \( x^k \). Our analysis also extends to stochastic gradient descent.

\paragraph{A novel unified assumption.} We build on this line of work by introducing a new assumption that allows us to obtain convergence guarantees for nonconvex optimization. Our unified framework is broadly applicable-- it subsumes prior assumptions on nonconvex optimization and allows for unified analysis of convex and nonconvex objectives. The main idea of our framework is that even in complex nonconvex landscapes, effective optimization algorithms rely on the gradient possessing a degree of directional alignment towards the set of solutions. To formalize this, we first make the assumption that a set of solutions exists.

\begin{assumption}
\label{ass:1}
The function \( f \) is continuously differentiable and has a nonempty set \( S \subseteq \mathbb{R}^d \) of global minimizers. Let \( f^\star \) denote the minimum value of the function \( f \).
\end{assumption}

We now introduce our main assumption, an inequality that relates the gradient at any point $x$ to its projection onto a subset $\tilde{S}$ of optimal solutions, using a progress function $P(x;\tilde{S})$ to quantify proximity to this set.

\begin{assumption}
\label{ass:2}
There exists constants $c_1>0$ and $c_2\geq0$ such that for all $x \in \mathbb{R}^d$,
$$
\left\langle\nabla f\left(x\right), x-\operatorname{proj}_{\tilde{S}}(x)\right\rangle \geq c_1 P(x;{\tilde{S}}) - c_2,
$$
where $\tilde{S} \subseteq S$, \( S \subseteq \mathbb{R}^d \) is a set of global minimizers of $f$, \(\tilde{S} \neq \emptyset\), $\operatorname{proj}_{\tilde{S}}(x) \in \arg\min_{y \in \tilde{S}} \|x - y\|^2$, and $P(x;\tilde{S})$ is a nonnegative function of the argument $x \in \mathbb{R}^d$.
\end{assumption}

\Cref{ass:2} has a clear and intuitive interpretation: the progress function controls how ``informative'' the gradient is in pointing us towards the set of minimizers, while the constants $c_1$ and $c_2$ control how stringent this information is.

\paragraph{Our contributions.} We develop a new framework for analyzing gradient descent under Assumption~\ref{ass:2}. We demonstrate that our framework recovers classical convergence guarantees for convex optimization as a special case, and also subsumes several existing assumptions in nonconvex optimization (such as quasiconvexity and the aiming condition). We provide convergence analysis under this new assumption for gradient descent (\Cref{thm:1}) and stochastic gradient descent (\Cref{thm:2}) and demonstrate the flexibility of these theorems in deriving new convergence guarantees. Finally, we provide experimental validation for how applicable our assumption is in half-space learning with the sigmoid, training MLPs on Fashion-MNIST, and training convolutional neural networks on CIFAR-10.

\subsection{Brief literature review}

The challenge of bridging the gap between theory and practice in nonconvex optimization has spurred significant research into developing more refined analytical frameworks.  Classical convex optimization theory provides strong convergence guarantees, but its assumptions are often too restrictive for modern machine learning. Conversely, standard nonconvex optimization results guarantee convergence to stationary points and do not reflect the empirical success of first-order methods in deep learning. Convexity can be seen as controlling the lower curvature of a function while smoothness controls the upper curvature. The literature has explored generalizations and alternatives to both.

\textbf{Alternatives to convexity.} The Polyak-\L{o}jasiewicz (PL) condition \cite{PL1,PL2} is a prominent example that relates the function value to the gradient norm, provides a lower bound on the function growth, and ensures linear convergence under certain conditions.  Quasi-convexity \cite{QCvx} and star-convexity \cite{star-convexity} represent other relaxations of convexity that have been studied in optimization. More recently, conditions like the Aiming property \cite{liu2023aiming} and the ${\alpha}$-${\beta}$ conditions \cite{islamov2024losslandscapecharacterizationneural} have emerged as tools to characterize the loss landscapes of neural networks and analyze the convergence of optimization algorithms in these settings.

\textbf{Alternatives to smoothness.} Recent work has explored alternatives to smoothness that may more accurately describe neural network optimization, e.g. generalized smoothness~\citep{zhang19_why_gradien_clipp_accel_train,xie2024trust}, directional sharpness or smoothness~\citep{pan22_adam_sgd,mishkin24_direc_smoot_gradien_method}, and local smoothness~\citep{berahas23_non_unifor_smoot_gradien_descen}.

\textbf{Assumptions on the stochastic gradients.} Another line of work has considered the various properties of the stochastic gradient noise, and its effect on the convergence of gradient-based methods, see e.g.~\citep{khaled2020bettertheorysgdnonconvex,faw22_power_adapt_sgd,zhang19_why_are_adapt_method_good_atten_model}. Our work is primarily aimed at relaxing convexity and is therefore orthogonal to these results.


\section{Main Theory \& Results}

In this section, we first discuss further Assumption~\ref{ass:2} and its implications, then present our convergence theory for gradient descent under this assumption, followed by stochastic gradient descent.

% \clearpage
% \paragraph{Problem formulation.} In this work, we wish to solve the following deterministic optimization problem
% $$
% \min _{x \in \mathbb{R}^d} f(x),
% $$
% where \( f(x): \mathbb{R}^d \to \mathbb{R} \) is a differentiable objective function.

% \paragraph{Method.} To tackle this problem, we use the widely known gradient descent method, which relies on the following update rule
% $$
% x^{k+1}=x^k-\gamma^k \nabla f\left(x^k\right),
% $$
% where \( \gamma^k > 0 \) is a stepsize, and \( \nabla f(x^k) \) represents the gradient of the function $f$ at the current point \( x^k \).

% The convergence properties of gradient-based methods have been extensively studied in optimization, with a primary focus on convex and nonconvex problems. Traditional optimization theory often relies on the assumption of convexity to analyze convergence \cite{nesterov2018introductory}, which is well-suited for many problems but overly optimistic for modern machine learning models, especially neural networks \cite{pmlr-v38-choromanska15}. On the other hand, assumptions such as smoothness and nonconvexity are commonly used for nonconvex optimization problems, but these assumptions tend to be too pessimistic when it comes to analyzing the convergence behavior. Specifically, the smoothness and nonconvexity assumptions provide only weak convergence guarantees to a first-order critical point \cite{Ghadimi2013,Orabona2019,khaled2020bettertheorysgdnonconvex,zhou2024on}. Therefore, there is a need for a framework that strikes a balance—providing a more realistic characterization of optimization landscapes, without the excessive optimism of convexity or the overly conservative view of nonconvexity.

% To address the challenges posed by nonconvexity, recent work has focused on deriving weaker assumptions for the loss function that are more applicable to the analysis of gradient-based optimizers in the context of machine learning. These assumptions relax convexity while still allowing for convergence analysis and include star convexity \cite{star-convexity}, weak quasi-convexity \cite{QCvx}, strongly weak quasi-convexity \cite{bu2020notenesterovsacceleratedmethod}, Polyak-\L{o}jasiewicz (PL) \cite{PL1,PL2}, Aiming \cite{liu2023aiming}, and ${\alpha}$-${\beta}$ conditions \cite{islamov2024losslandscapecharacterizationneural}.

% This paper aims to bridge this gap by proposing a new set of assumptions that encompasses some of the existing ones as special cases, thereby providing a more balanced framework between the overly optimistic view of convexity and the overly pessimistic view of nonconvexity.

% \section{Assumption}
% \label{sec:Assumption}

% \paragraph{Assumptions.} To facilitate our convergence analysis, we make several assumptions on the function $f$. In this context, Assumption~\ref{ass:1} is standard for analyzing optimization algorithms in unconstrained optimization, while Assumption~\ref{ass:2} is novel and introduced in this work.

% \begin{assumption}
% \label{ass:1}
% The function \( f \) is continuously differentiable and has a nonempty set \( S \subseteq \mathbb{R}^d \) of global minimizerss. Let \( f^\star \) denote the minimum value of the function \( f \).
% \end{assumption}

% \begin{assumption}
% \label{ass:2}
% The function $f$ is such that for all $x \in \mathbb{R}^d$ and some constants $c_1>0,$ $c_2\geq0$
% $$
% \left\langle\nabla f\left(x\right), x-\operatorname{proj}_{\tilde{S}}(x)\right\rangle \geq c_1 P(x;{\tilde{S}}) - c_2,
% $$
% where $\tilde{S} \subseteq S$, \( S \subseteq \mathbb{R}^d \) is a set of global minimizers of $f$, $\operatorname{proj}_{\tilde{S}}(x) := \arg\min_{y \in \tilde{S}} \|x - y\|^2$, and $P(x;\tilde{S})$ is a nonnegative function of the argument $x \in \mathbb{R}^d$.
% \end{assumption}

\subsection{Discussion of Assumption~\ref{ass:2}}\label{sec:main-1}
To analyze Assumption~\ref{ass:2}, we start by considering the simpler setting \( c_2 = 0 \). In this case, Assumption~\ref{ass:2} takes the form
\[
\left\langle \nabla f(x), x - \operatorname{proj}_{\tilde{S}}(x) \right\rangle \geq c_1 P(x; \tilde{S}) \geq 0 \ \ \text{for all} \ x \in \mathbb{R}^d.
\]  
This means that the negative gradient \( -\nabla f(x) \) points toward \( \tilde{S} \) in the sense that \( -\nabla f(x) \) is nontrivially correlated with the direction \( \operatorname{proj}_{\tilde{S}}(x) - x \). The term \( c_1 P(x; \tilde{S}) \) can tighten or relax this correlation depending on the choices of $c_1$ and $P(x; \tilde{S})$, leading to narrower or wider classes of functions. Introducing \( c_2 \) relaxes the correlation, possibly allowing the inner product to be negative at certain points.

Now, consider the case where $x \in \mathbb{R}^d$ is a stationary point of $f$, i.e., $\nabla f(x)=0$. From Assumption~\ref{ass:2} we have that $P(x;\tilde{S}) \leq \frac{c_2}{c_1}$. This implies, in terms of the measure $P(x;\tilde{S})$, the stationary point $x$ is not too far from the set $\tilde{S}$.

A specific, natural choice for the progress function in Assumption~\ref{ass:2} is $P(x;\tilde{S})=f(x)-f^{\star}$, as an example. We define the constants \( c_1 = 1 \), \( c_2 = 0 \), and set \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \). With these choices, Assumption~\ref{ass:2} becomes
\[
\left\langle \nabla f(x), x - x^{\star} \right\rangle \geq f(x) - f^{\star} \quad \text{for all} \ x \in \mathbb{R}^d,
\]
which is a simple consequence of the convexity of \( f \) from standard convex analysis.

For additional examples of various classes of functions derived from Assumption~\ref{ass:2} that yield meaningful convergence results, please refer to Section~\ref{sec:Special Cases} and~\ref{sec:Extensions}, where by adjusting the parameters of Assumption~\ref{ass:2}, we can recover many well-known function classes as special cases, including convex, strongly convex, weak quasi-convex functions \cite{QCvx}, strongly weak quasi-convex functions \cite{bu2020notenesterovsacceleratedmethod}, and functions satisfying the Aiming \cite{liu2023aiming} property or the ${\alpha}$-${\beta}$ condition \cite{islamov2024losslandscapecharacterizationneural}. Moreover, this framework also reveals entirely new classes of functions.

\paragraph{Role of the parameters \( ( c_1, c_2, \tilde{S} ) \).} Now, let us examine how the flexibility of the choices \( ( c_1, c_2, \tilde{S} ) \) in Assumption~\ref{ass:2} leads to wider classes of functions for the particular choice of \( P(x, \tilde{S}) = f(x) - f^\star \), where we assume without loss of generality that \( f^\star = 0 \). We include examples of how our assumption subsumes existing conditions and allow for relaxed ones in Table~\ref{table:1}, including different examples of functions \( f(x) \), \( x \in \mathbb{R} \) (Figure~\ref{plot:1}) (see \Cref{sec:examples of functions} for details).
\begin{align*}
f_1 &= x^2, \\
f_2 &=
\begin{cases}
f_1, & x \geq -1 \\
4\sqrt{-x}-3, & x < -1
\end{cases}, \\
f_3 &= \frac{x^4}{2} - x^2 + \frac{1}{2}, \\
f_4 &= x^4 - \frac{10}{3}x^3+3x^2, \\
f_5 &=
\begin{cases}
f_4, & x \geq 0 \\
f_2, & x < 0
\end{cases}.
\end{align*} 

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{plots}}
\caption{Examples of the function $f(x)$, $x \in \mathbb{R}$.}
\label{plot:1}
\end{center}
\end{figure}

\begin{table}[ht]
\caption{Examples of classes of functions described by Assumption~\ref{ass:2} for \( P(x, \tilde{S}) = f(x) - f^\star \), where \( f^\star = 0 \), \( \tilde S \subseteq S \), and for different choices of \( ( c_1, c_2, \tilde{S} ) \). Here, $S \subseteq \mathbb{R}$ is a set of global minimizers of $f$, $x^{\star} \in S$.}
\label{table:1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
$c_1$, $\tilde{S}$ & $c_2 = 0$ & $c_2 \geq 0$ \\
\midrule
$c_1=1$, & consequence of & \textcolor{green}{New} \\
$\tilde{S}=\{x^{\star}\}$ & convexity &  \\
\scriptsize{Examples:} & $f_1$ & $f_1$, $f_3$, $f_4$ \\
\midrule
$c_1>0$, & weak quasi-convexity & \textcolor{green}{New} \\
$\tilde{S}=\{x^{\star}\}$ & \cite{QCvx} &  \\
\scriptsize{Examples:} & $f_1$, $f_2$ & $f_1$, $f_2$, $f_3$, $f_4$, $f_5$\\
\midrule
$c_1>0$, & aiming condition& \textcolor{green}{New} \\
$\tilde{S} = S$ & \cite{liu2023aiming} & \\
\scriptsize{Examples:} & $f_1$, $f_2$ & $f_1$, $f_2$, $f_3$, $f_4$, $f_5$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

We can observe that incorporating constants into Assumption~\ref{ass:2}, allowing \( c_1 \neq 1 \) and \( c_2 \neq 0 \), leads to broader classes of functions. When \( c_2 \neq 0 \), Assumption~\ref{ass:2} can describe functions with local minima and saddle points.

Note that for certain functions, choosing $c_1 \neq 1$ and $\tilde{S}=S$ allows us to satisfy Assumption~\ref{ass:2} with a smaller constant $c_2$:
\begin{itemize} 

\item Specifically, for a fixed $c_1=1$, if $\tilde{S} = \{x^\star\}$, $x^\star=1$, then $f_3$ satisfies Assumption~\ref{ass:2} with $c_2 \approx 1.437$, and if $\tilde{S}=S$, then $f_3$ satisfies Assumption~\ref{ass:2} with $c_2 = 0.5$. In both of these cases, we choose the smallest $c_2$ for the given $c_1$.

\item For the function \( f_4 \), it can be shown that \( f_4 \) satisfies Assumption~\ref{ass:2} with \( c_1 = 1 \), \( c_2 \approx 1.013 \), or \( c_1 = 0.1 \), \( c_2 \approx 0.467 \). Also, if \( f_3 \) is considered with \( \tilde{S} = S \), it satisfies Assumption~\ref{ass:2} with \( c_1 = 1 \), \( c_2 \approx 0.5 \), or \( c_1 = 0.1 \), \( c_2 = 0.05 \). In all these examples, we select the smallest \( c_2 \) for the given \( c_1 \).  
\end{itemize}

\subsection{Main Convergence Theorem}
\label{sec:Theorem}

In this section, we examine the convergence guarantees we can obtain under the proposed Assumption~\ref{ass:2}.

\begin{theorem}
\label{thm:1} Let Assumptions~\ref{ass:1} and \ref{ass:2} be satisfied. Further assume that the stepsize $\gamma^k$ satisfies the relations
$$
0 < \gamma^k \leq (2-\alpha) \frac{\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + c_2 + \beta^k}{\left\|\nabla f\left(x^k\right)\right\|^2}
$$
that holds for all $k \geq 0$, where $0<\alpha<2$, $\beta^k>0$, $\gamma_{\star}>0$, $x_p := \operatorname{proj}_{\tilde{S}}(x)$. Then we have the following descent inequality that holds for all $k \geq 0$
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 &\leq \left\|x^k-x^k_p\right\|^2 - \alpha c_1 \gamma^k P(x^k;\tilde{S})\\ 
& \quad + (2-\alpha)\beta^k \gamma^k + 2 c_2 \gamma^k, 
\end{align*}
and
\begin{align*}
\min_{k \in \{0, \dots, K\}} P(x^k;\tilde{S}) & \leq \frac{\sum_{k=0}^K \gamma^k P(x^k;\tilde{S})}{\sum_{k=0}^K \gamma^k} \\
& \leq \frac{\left\|x^{0}-x^0_p\right\|^2}{\alpha c_1\sum_{k=0}^K \gamma^k} + C^{K},
\end{align*}
where $C^{K} := \frac{\sum_{k=0}^K \gamma^k (2-\alpha)\beta^k}{\alpha c_1 \sum_{k=0}^K \gamma^k} + \frac{2c_2}{\alpha c_1}$.
\end{theorem}

Theorem~\ref{thm:1} provides convergence guarantees for \(P(x;\tilde{S})\) within a neighborhood \(C^K\), given that the sum of the stepsizes, \(\sum_{k=0}^K \gamma^k\), is sufficiently large. However, achieving a precise convergence rate to the neighborhood requires additional assumptions on \(P(x;\tilde{S})\), the function \(f\), or the stepsizes \(\gamma^k\).

For instance, under assumptions such as \(P(x;\tilde{S}) = f(x) - f^\star\) and smoothness (Corollary~\ref{cor:1}), bounded gradients (Corollary~\ref{cor:2}), or decreasing stepsizes (Corollary~\ref{cor:3}), we can establish convergence to a neighborhood within the framework of Theorem~\ref{thm:1}. However, the latter two results---bounded gradients and decreasing stepsizes---are only meaningful if \(P(x;\tilde{S})\) satisfies certain regularity properties, which are discussed in detail in Section~\ref{sec:Special Cases}.

\begin{corollary} 
\label{cor:1}
Under the assumptions of Theorem~\ref{thm:1} with $P(x;\tilde{S})=f(x)-f^{\star}$, if we additionally assume that $f$ is $L$-smooth, and choose $\gamma^k=\frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$, $\alpha=1$, $\beta^k=0$, then we obtain
$$
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} \leq \frac{2L \left\|x^{0}-x^0_p\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1}.
$$
\end{corollary}

Note that for \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \), $c_1=1$, $c_2=0$, Corollary~\ref{cor:1} presents a well-known result from standard convex analysis for the Polyak stepsize \cite{polyak1987book}.

\begin{corollary} 
\label{cor:2}
Under the assumptions of Theorem~\ref{thm:1}, if we additionally assume that $f$ has bounded gradients, i.e., $\| \nabla f\left(x\right) \| \leq G$ for all $x \in \mathbb{R}^d$, and choose $\gamma^k = \frac{c_1 P(x^k;\tilde{S}) + \beta^k}{\left\|\nabla f\left(x^k\right)\right\|^2}$, then we obtain
$$
\min_{k \in \{0, \dots, K\}} P(x^k;\tilde{S}) \leq \frac{G \left\|x^{0}-x^0_p\right\|}{\sqrt{(2-\alpha) \alpha} c_1} \frac{1}{\sqrt{K+1}} + C^K,
$$
where $C^{K} := \frac{\sum_{k=0}^K \gamma^k (2-\alpha)\beta^k}{\alpha c_1 \sum_{k=0}^K \gamma^k} + \frac{2c_2}{\alpha c_1}$.
\end{corollary}

\begin{corollary} 
\label{cor:3}
Under the assumptions of Theorem~\ref{thm:1}, if we additionally assume that $\gamma^k \leq \gamma^{k-1}$ for $k=1,\dots, K$, then we obtain 
\begin{align*}
\min_{k \in \{0, \dots, K\}}P(x^k;\tilde{S}) \leq \frac{D_{\max}^2}{\alpha c_1 \gamma^K (K+1)} + \tilde{C}^K,
\end{align*}
where $\tilde{C}^K:=\frac{2-\alpha}{\alpha c_1 (K+1)} \sum_{k=0}^K \beta^k + \frac{2 c_2}{\alpha c_1}$ and $D_{\max}^2 := \max_{k \in \{0, \dots, K\}} \left\|x^{k}-x^k_p\right\|^2$.
\end{corollary}

\subsubsection{Special Cases}
\label{sec:Special Cases}

% \subsection{Examples of stepsizes}

Let us consider some examples of stepsizes that satisfy Theorem~\ref{thm:1} for a specific choice of \( P(x;\tilde{S}) = f(x) - f^\star \) and \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \). These results are summarized in Table~\ref{table:2}. From the table, we observe that for various stepsizes of the Polyak type \cite{polyak1987book,StochPolyak2021,Orvieto2022}, convergence is achieved up to a neighborhood under the assumptions of Theorem~\ref{thm:1}, along with additional conditions such as the smoothness of the function \( f \) or the boundedness of its gradients, i.e., \( \| \nabla f(x) \| \leq G \) for all $x \in \mathbb{R}^d$ (see Section~\ref{sec:proofs for special cases} for details).

\begin{table}[ht]
\caption{Examples of stepsizes that satisfy Theorem~\ref{thm:1} for \(\alpha=1 \), \( P(x, \tilde{S}) = f(x) - f^\star \), where \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \). Here, $S \subseteq \mathbb{R}$ is a set of global minimizers of $f$, $l^{\star} \leq f^{\star}$, $c^k=\sqrt{k+1}$, $\sigma^2:=f^{\star} - l^{\star}$.}
\label{table:2}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
Stepsize, $\gamma^k$ & Extra & Convergence \\
 & Assumption & Rate \\ 
\midrule
$\frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$ & \scriptsize{smoothness} & $\mathcal{O}\left(\frac{1}{K}\right) + \frac{2c_2}{c_1}$\\
 & \scriptsize{bounded} $\nabla f$ & $\mathcal{O}\left(\frac{1}{\sqrt{K}}\right) + \frac{2c_2}{c_1}$\\
\midrule
$\frac{c_1 \left( f(x^k) - l^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$ & \scriptsize{smoothness} & $\mathcal{O}\left(\frac{1}{K}\right) + \frac{2c_2}{c_1}$\\
 & &\quad \quad $ \ \ \ + \ \sigma^2$ \\
 & \scriptsize{bounded} $\nabla f$ & $\mathcal{O}\left(\frac{1}{\sqrt{K}}\right) + \frac{2c_2}{c_1}$\\
 & &\quad \quad \quad $ \ \ \ + \ \sigma^2$ \\
\midrule
$\min \left\{ \frac{\tilde \gamma_b^k}{c^k}, \frac{\gamma^{k-1} c^{k-1}}{c^k} \right\},$ & \scriptsize{smoothness} & $\mathcal{O}\left(\frac{1}{\sqrt{K}}\right) + \frac{2c_2}{c_1}$\\
$\tilde \gamma_b^k:=\frac{c_1 \left( f(x^k) - l^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$ & & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Examples of function classes}

Next, let us consider some choices of \( P(x; \tilde{S}) \), \( c_1 \), and \( c_2 \) in Assumption~\ref{ass:2} that describe specific classes of functions and lead to meaningful convergence results. Our first example is one we have already mentioned  before.

\paragraph{\textit{Example 1.}} Let $P(x;\tilde{S}) = f(x) - f^{\star}$.  

Note that if \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \), $c_2=0$, then Assumption~\ref{ass:2} is equivalent to the definition of $c_1$-weak quasi-convex functions \cite{QCvx}. If additionally $c_1=1$, then Assumption~\ref{ass:2} follows from the convexity of the function $f$.

Consider using the Polyak stepsize $\gamma^k = \frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2}=\frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$, with $\alpha=1$, and $\beta^k=0$. If we additionally assume that $f$ is $L$-smooth, then from Corollary~\ref{cor:1} we get the following convergence result
$$
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} \leq \frac{2L \left\|x^{0}-x^0_p\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1}.
$$
If $c_2=0$, then we obtain an $\mathcal{O}\left(\frac{1}{K}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star}$ under Assumptions~\ref{ass:1},~\ref{ass:2}, and the smoothness of $f$.

If, instead of the smoothness of $f$, we assume that $f$ has bounded gradients, then from  Corollary~\ref{cor:2}, we get the following convergence result
$$
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} \leq \frac{G \left\|x^{0}-x^0_p\right\|}{ c_1 \sqrt{K+1}} + \frac{2c_2}{c_1}.
$$

If $c_2=0$, then we obtain an $\mathcal{O}\left(\frac{1}{\sqrt{K}}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star}$ under Assumptions~\ref{ass:1},~\ref{ass:2}, and the boundedness of the gradients of $f$.

\paragraph{\textit{Example 2.}} Let $P(x;\tilde{S}) = f(x) - f^{\star} + \frac{\mu}{2} \|x-x_p\|^2$, $\mu>0$.

Note that if \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \), $c_2=0$, then Assumption~\ref{ass:2} is equivalent to the definition of $\mu$-strongly $c_1$-weak quasi-convex functions \cite{bu2020notenesterovsacceleratedmethod}. If additionally $c_1=1$, then Assumption~\ref{ass:2} follows from the $\mu$-strong convexity of the function $f$.

Let us choose $\gamma^k =\frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \leq \frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2}$. Then, by setting $\alpha=1$, $\beta^k=0$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
$$
0<\gamma^k \leq \frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2} \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + c_2}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$

Similar to the previous example, assuming that $f$ is $L$-smooth, we can show that
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} + \frac{\mu}{2} \|x^k-x^k_p\|^2 \\ \leq \frac{2L \left\|x^{0}-x^k_p\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1},
\end{align*}
and assuming that $f$ has bounded gradients, we get
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} + \frac{\mu}{2} \|x^k-x^k_p\|^2 \\ \leq \frac{G \left\|x^{0}-x^k_p\right\|}{ c_1 \sqrt{K+1}} + \frac{2c_2}{c_1}.
\end{align*}
If $c_2=0$, then we obtain an $\mathcal{O}\left(\frac{1}{K}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} + \frac{\mu}{2} \|x^k-x^k_p\|^2$ under Assumptions~\ref{ass:1},~\ref{ass:2}, and the smoothness of $f$, and an $\mathcal{O}\left(\frac{1}{\sqrt{K}}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} + \frac{\mu}{2} \|x^k-x^k_p\|^2$ under Assumptions~\ref{ass:1},~\ref{ass:2}, and the boundedness of the gradients of $f$.

We can also establish the linear rate of convergence for this class of functions (see Section~\ref{sec:proofs for special cases} for details)
$$
\|x^K - x^K_p \|^2 \leq \left( 1 - \frac{c^2_1 \mu}{4 L} \right)^K \left\|x^0-x^0_p\right\|^2 + \frac{8 c_2 L \gamma_{\text{max}}}{c^2_1 \mu},
$$
where $\gamma_{\text{max}} := \max_{k \in \{0, \dots, K\}} \gamma^k$.

If $c_2=0$, then we obtain a linear convergence rate for $\|x^K - x^K_p \|^2$ under Assumptions~\ref{ass:1},~\ref{ass:2}, and the smoothness of $f$.

\paragraph{\textit{Example 3.}} Let $P(x;\tilde{S}) = \frac{1}{L}\|\nabla f\left(x\right)\|^2$, $L>0$.

Note that if \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \), $c_1=1$, $c_2=0$, then Assumption~\ref{ass:2} follows from the convexity and $L$-smoothness of the function $f$. Here, we used the fact that $f$ is $L$-smooth and convex , which is equivalent to the property that for all $x, y \in \mathbb{R}^d$
$$
\frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^2 \leq\langle\nabla f(x)-\nabla f(y), x-y\rangle.
$$

Let us choose $\gamma^k=\frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2}=\frac{c_1}{L}$. Then, by setting $\alpha=1$, $\beta^k=0$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
$$
0<\gamma^k \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + c_2}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$
Therefore, from Theorem~\ref{thm:1} we get the following convergence result
$$
\min_{k \in \{0, \dots, K\}}\|\nabla f\left(x^k\right)\|^2 \leq \frac{L^2 \left\|x^{0}-x^0_p\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1}.
$$ 
If $c_2=0$, then we obtain an $\mathcal{O}\left(\frac{1}{K}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}}\|\nabla f\left(x^k\right)\|^2$ under Assumptions~\ref{ass:1},~\ref{ass:2}.

\paragraph{\textit{Example 4.}} Let $P(x;\tilde{S}) = f(x) - f^{\star} + \frac{1}{2L}\|\nabla f\left(x\right)\|^2$, $L>0$.

Note that if \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \), $c_1=1$, $c_2=0$, then Assumption~\ref{ass:2} follows from the convexity and $L$-smoothness of the function $f$. Here, we used the fact that $f$ is $L$-smooth and convex , which is equivalent to the property that for all $x, y \in \mathbb{R}^d$
$$
\frac{1}{2L}\|\nabla f(x)-\nabla f(y)\|^2 \leq f(x) - f(y) - \langle \nabla f(y), x-y\rangle.
$$

Let us choose $\gamma^k=\frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2}=\frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}+\frac{c_1}{2L}$. Then, by setting $\alpha=1$, $\beta^k=0$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
$$
0<\gamma^k \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + c_2}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$
Therefore, using the fact that $\sum_{k=0}^K \gamma^k \geq \frac{c_1}{2L} (K+1)$, from Theorem~\ref{thm:1} we get the following convergence result
\begin{align*}
\min_{k \in \{0, \dots, K\}} f(x^k) - f^{\star} + \frac{1}{2L}\|\nabla f\left(x^k\right)\|^2 \\ \leq \frac{2L\left\|x^{0}-x^0_p\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1}.
\end{align*}
If $c_2=0$, then we obtain an $\mathcal{O}\left(\frac{1}{K}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}} f(x^k) - f^{\star} + \frac{1}{2L}\|\nabla f\left(x^k\right)\|^2$ under Assumptions~\ref{ass:1},~\ref{ass:2}.

\paragraph{\textit{Example 5.}} Let $P(x;\tilde{S})=f(x)$, $f^{\star}=0$.


Note that if $\tilde{S}=S$, where $S$ is a nonempty set, $c_2=0$, then Assumption~\ref{ass:2} is equivalent to the Aiming condition \cite{liu2023aiming}.

Let us choose $\gamma^k = \frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2}=\frac{c_1 f(x^k) }{\left\|\nabla f\left(x^k\right)\right\|^2}$. Then, by setting $\alpha=1$, $\beta^k=0$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
$$
0<\gamma^k \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + c_2}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$

Similar to the previous examples, assuming that $f$ is $L$-smooth, we can show that
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) \leq \frac{2L \left\|x^{0}-x^0_p\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1},
\end{align*}
and assuming that $f$ has bounded gradients, we get
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) \leq \frac{G \left\|x^{0}-x^0_p\right\|}{ c_1 \sqrt{K+1}} + \frac{2c_2}{c_1}.
\end{align*}

If \( c_2 = 0 \), then we obtain an \( \mathcal{O}\left( \frac{1}{K} \right) \) convergence rate for \( \min_{k \in \{0, \dots, K\}} f(x^k) \) under Assumptions~\ref{ass:1},~\ref{ass:2}, and the smoothness of \( f \). Also, if \( c_2 = 0 \), then we get an \( \mathcal{O}\left( \frac{1}{\sqrt{K}} \right) \) convergence rate for \( \min_{k \in \{0, \dots, K\}} f(x^k) \) under Assumptions~\ref{ass:1},~\ref{ass:2}, and the boundedness of the gradients of \( f \).

\subsection{Extension to the stochastic setting}
\label{sec:Extensions}

\paragraph{Problem formulation.} In this subsection, we extend our results to the stochastic optimization problem
$$
\min _{x \in \mathbb{R}^d}\left\{f(x):=\mathrm{E}_{\xi \sim \mathcal{D}}\left[f_{\xi}(x)\right]\right\},
$$
where $\xi$ are samples from some distribution $\mathcal{D}$. We consider the stochastic gradient method
$$
x^{k+1}=x^k-\gamma^k \nabla f_{\xi}\left(x^k\right),
$$
where $\gamma^k>0$ is a stepsize. 

\paragraph{Assumptions.} To facilitate our convergence analysis, we make the following assumption on $f_{\xi}$.

\begin{assumption} 
\label{ass:3}
The function $f_{\xi}$ is such that for all $x \in \mathbb{R}^d$ and some constants ${c_1}_{\xi}>0,$ ${c_2}_{\xi}\geq0$ 
\begin{align*}
\left\langle\nabla f_{\xi}\left(x\right), x-\operatorname{proj}_S(x)\right\rangle \geq {c_1}_{\xi} P_{\xi}(x;\tilde{S}) - {c_2}_{\xi},
\end{align*}
where $\tilde{S} \subseteq S$, \( S \subseteq \mathbb{R}^d \) is a set of global minimizers of $f$, \(\tilde{S} \neq \emptyset\), $\operatorname{proj}_{\tilde{S}}(x) \in \arg\min_{y \in \tilde{S}} \|x - y\|^2$, and $P_{\xi}(x;\tilde{S})$ is a nonnegative function of the argument $x \in \mathbb{R}^d$.
\end{assumption}

\begin{theorem}
\label{thm:2} Let Assumptions~\ref{ass:1} and \ref{ass:3} be satisfied. Further assume that the stepsize $\gamma^k = \min \{\tilde \gamma^k, \gamma_b\}$, where $\tilde \gamma^k$ satisfies the relations
$$
\gamma_{\star} \leq \tilde \gamma^k \leq (2-\alpha) \frac{\left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle + {c_2}_{\xi} + \beta_{\xi}^k}{\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2}
$$
that holds for all $k \geq 0$, where $0<\alpha<2$, $\beta_{\xi}^k>0$, $\gamma_{\star}>0$, $\gamma_b>0$, $x_p := \operatorname{proj}_{\tilde{S}}(x)$. Then we have the following descent inequality that holds for all $k \geq 0$
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 &\leq \left\|x^k-x^k_p\right\|^2 - \alpha \gamma^k  {c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S}) \\
& \quad + (2-\alpha) \gamma^k \beta_{\xi}^k + 2 \gamma^k {c_{2}}_{\xi},    
\end{align*}
and
\begin{align*}
\min_{k \in \{0, \dots, K\}} \mathrm{E} \left[{c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S})\right] \leq \frac{\mathrm{E} \left[ \left\|x^{0}-x^{0}_p\right\|^2\right]}{\alpha \gamma_{\min} (K+1)} + C_{\text{stoc}}^K,
\end{align*}
where $C_{\text{stoc}}^K := \frac{(2-\alpha)\gamma_b}{\alpha \gamma_{\min} (K+1)} \sum_{k=0}^K \mathrm{E} \left[\beta_{\xi}^k\right] + \frac{2 \gamma_b \mathrm{E} \left[{c_{2}}_{\xi} \right]}{\alpha \gamma_{\min}}$, $\gamma_{\min} := \min\{\gamma_{\star}, \gamma_b \}$.
\end{theorem}

\begin{corollary} 
\label{cor:4}
Under the assumptions of Theorem~\ref{thm:2} with $P(x;\tilde{S})=f_{\xi}(x)-f_{\xi}(x_p)$, $c_{1\xi}=c_1>0$, if we additionally assume that $f_{\xi}$ are bounded from below, i.e, $f_{\xi}^{\star}:=\min _x f_{\xi}(x)>0$, $f_{\xi}$ are $L$-smooth, and choose $\tilde \gamma^k=\frac{c_1 \left( f_{\xi}(x^k) - f_{\xi}^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$, $\alpha=1$, $\beta_{\xi}^k=c_1(f_{\xi}(x_p)-f_{\xi}^{\star})$, then we obtain
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} &\leq \frac{\mathrm{E} \left[ \left\|x^{0}-x^{0}_p\right\|^2\right]}{c_1 \gamma_{\min} (K+1)} \\
& \quad + \frac{\sigma^2 \gamma_b}{\gamma_{\min}} + \frac{2\gamma_b \mathrm{E} \left[{c_{2}}_{\xi} \right]}{c_1 \gamma_{\min}},
\end{align*}
where $\gamma_{\min}:=\min\{\frac{c_1}{2 L}, \gamma_b\}$, $\sigma^2:=\mathrm{E} \left[ f_{\xi}(x_p)-f_{\xi}^{\star} \right]$.
\end{corollary}

\begin{corollary} 
\label{cor:5}
Under the assumptions of Theorem~\ref{thm:2}, if we additionally assume that $\tilde \gamma^k \leq \tilde \gamma^{k-1}$ for $k=1,\dots, K$, then we obtain 
\begin{align*}
\min_{k \in \{0, \dots, K\}} \mathrm{E} \left[c_{1 \xi} P(x^k;\tilde{S})\right] \leq \mathrm{E} \left[ \frac{D_{\max}^2}{\alpha \gamma^K } \right] \frac{1}{K+1} + \tilde{C}_{\text{stoc}}^K.
\end{align*}
where $\tilde{C}_{\text{stoc}}^K:=\frac{2-\alpha}{\alpha (K+1)} \sum_{k=0}^K \mathrm{E} \left[\beta_{\xi}^k\right] + \frac{2 \mathrm{E} \left[ c_{2 \xi} \right]}{\alpha}$ and $D_{\max}^2 := \max_{k \in \{0, \dots, K\}} \left\|x^{k}-x^k_p\right\|^2$.
\end{corollary}

Let us consider some examples of the choices of \(P_{\xi}(x;\tilde{S})\), ${c_1}_{\xi}$, ${c_2}_{\xi}$ in Assumption~\ref{ass:2} that describe certain classes of functions and lead to meaningful convergence results.

\paragraph{\textit{Example 6.}} Let $P_{\xi}(x;\tilde{S}) = f_{\xi}(x) - f_{\xi}(x_p)$, ${c_1}_{\xi}=c_1>0$, where we assume that functions $f_{\xi}$ are bounded from
below, i.e, $f_{\xi}^{\star}:=\min _x f_{\xi}(x)>0$. 

Note that if $\tilde{S}=S$, $S$ is a nonempty set, ${c_1}=\tilde \alpha - \tilde \beta$, ${c_2}_{\xi}=\tilde \beta (f_{\xi}(x_p) - f^{\star}_{\xi})$, where $\tilde \alpha> \tilde \beta>0$, then Assumption~\ref{ass:2} is equivalent to the definition of the $\tilde \alpha$-$\tilde \beta$ condition \cite{islamov2024losslandscapecharacterizationneural}.

Let us choose $\tilde \gamma^k = \frac{c_1 \left( f_{\xi}(x^k) - f_{\xi}^{\star} \right)}{\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2}$. If we additionally assume that functions $f_{\xi}$ is $L$-smooth, then we can show that $\tilde \gamma^k \geq \frac{c_1}{2L}$. Then, by setting $\alpha=1$, $\beta_{\xi}^k=c_1(f_{\xi}(x_p)-f_{\xi}^{\star})$, from  Corollary~\ref{cor:4}, we get the following convergence result
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} &\leq \frac{\mathrm{E} \left[ \left\|x^{0}-x^{0}_p\right\|^2\right]}{c_1 \gamma_{\min} (K+1)} \\
& \quad + \frac{\sigma^2 \gamma_b}{\gamma_{\min}} + \frac{2\gamma_b \mathrm{E} \left[{c_{2}}_{\xi} \right]}{c_1 \gamma_{\min}},
\end{align*}
where $\gamma_{\min}:=\min\{\frac{c_1}{2 L}, \gamma_b\}$, $\sigma^2:=\mathrm{E} \left[ f_{\xi}(x_p)-f_{\xi}^{\star} \right]$.

If ${c_2}_{\xi} = 0$ (under $\tilde \alpha$-$\tilde \beta$ condition either $\tilde \beta = 0$, or in the interpolation regime), then we obtain an $\mathcal{O}\left(\frac{1}{K}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}} f(x^k) - f^{\star}$ under Assumption~\ref{ass:1},~\ref{ass:3}, and the smoothness of $f_{\xi}$.

\paragraph{\textit{Example 7.}} Let $P_{\xi}(x;\tilde{S}) = \frac{1}{L}\| \nabla f_{\xi}(x) \|^2$, $L>0$, ${c_1}_{\xi}=c_1>0$. 

Note that if \( \tilde{S} = \{x^{\star} \} \), \( x^{\star} \in S \), $c_1=1$, ${c_2}_{\xi}=0$, then Assumption~\ref{ass:2} follows from the convexity and $L$-smoothness of functions $f_{\xi}$.

Let us choose $\tilde \gamma^k = \frac{c_1 P_{\xi}(x^k;\tilde{S})}{\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2}=\frac{c_1}{L}$. Then, by setting $\alpha=1$, $\beta_{\xi}^k=0$, $\tilde \gamma^k$ satisfies the relations of Theorem~\ref{thm:2}
$$
0<\gamma_{\star} = \frac{c_1}{L} \leq \tilde \gamma^k \stackrel{\ref{ass:3}}{\leq} \frac{\left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle + {c_2}_{\xi}}{\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2}. 
$$

Finally, from  Theorem~\ref{thm:2}, using Jensen's inequality
$$
\|\underbrace{\mathrm{E} \left[\nabla f_{\xi}(x^k)\right]}_{=\nabla f(x^k)} \|^2 \leq \mathrm{E} \left[ \| \nabla f_{\xi}(x^k) \|^2 \right],
$$
we get the following convergence result
\begin{align*}
\min_{k \in \{0, \dots, K\}} \| \nabla f(x^k) \|^2 \leq \frac{L^2 \mathrm{E} \left[ \left\|x^{0}-x^0_p\right\|^2\right]}{c^2_1 (K+1)} \\ + \frac{2L^2 \gamma_b \mathrm{E} \left[{c_{2}}_{\xi} \right]}{c^2_1}.
\end{align*}

If ${c_2}_{\xi} = 0$, then we obtain an $\mathcal{O}\left(\frac{1}{K}\right)$ convergence rate for $\min_{k \in \{0, \dots, K\}} \| \nabla f(x^k) \|^2$ Assumption~\ref{ass:1},~\ref{ass:3}, and the smoothness of $f_{\xi}$.

\section{Experiments}

In this section, we consider experiments to test whether our new assumption holds for two specific choices of functions, defined by the progress functions \( P_{\xi}(x;\tilde{S}) = f_{\xi}(x) - f_{\xi}(x_p) \) and \( P_{\xi}(x;\tilde{S}) = \| \nabla f_{\xi}(x)\|^2 \), with $c_{1 \xi} = c_1 > 0$. In experiments, we approximately assume that \( \tilde{S} = \{x^{\star} \} \), \(x^K \approx x^{\star} \in S \) is the set of all minimizers. For all experiments, we use $3$ different random seeds and plot the mean, along with the maximum and minimum fluctuations.

\subsection{Half space learning problem}
In the first experiment, we consider the following half-space learning problem
$$
\min_{x \in \mathbb{R}^d} \left\{ f(x):=\frac{1}{n} \sum_{i=1}^n \sigma\left(-b_i x^{\top} a_i\right)+\frac{\lambda}{2}\|x\|^2 \right\},
$$
where $\left\{a_i, b_i\right\}_{i=1}^n, a_i \in \mathbb{R}^d, b_i \in\{0,1\}$ is a given dataset, $\lambda=10^{-5}$, and $\sigma$ is a sigmoid function. We draw \( n/2 = 20 \) samples \( a_i \in \mathbb{R}^4 \) from two multivariate Gaussian distributions with different means and the same variance of $2$, and assign the labels \( b_i \in \{0,1\} \) accordingly. We use SGD with a learning rate of $0.05$ and a batch size of $1$ for minimization problem. 

The results of the experiment are presented in Figure~\ref{plot:2}. The problem is nonconvex \cite{pmlr-v80-daneshmand18a}, and we observe that the gradient norm becomes near zero early, indicating that the SGD trajectory passes through saddle points or local minima. From the plots, you can observe that for different functions \( P_{\xi}(x;\tilde{S}) \), $\mathrm{E} \left[ c_{2\xi} \right]$ remains close to zero and Assumption~\ref{ass:3} holds with relatively small constants \( c_{2\xi} \) along the gradient trajectories, when $c_1$ is fixed. Specifically, when \( P_{\xi}(x;\tilde{S}) = f_{\xi}(x) - f_{\xi}(x_p) \) and \( c_1 = 1 \), it follows that \( c_{2\xi} \leq 0.221 \), and when \( P_{\xi}(x;\tilde{S}) = \|\nabla f_{\xi}(x)\|^2 \) and \( c_1 = 0.1 \), it follows that \( c_{2\xi} \leq 0.612 \).

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{plot1}}
\caption{Training the half-space learning problem.}
\label{plot:2}
\end{center}
\end{figure}

\subsection{MLP architecture}
In the second experiment, we train an MLP model with $3$ fully connected layers and ReLU activation functions (the second layer has a size of $64$) on the Fashion-MNIST dataset \cite{xiao2017fashionmnistnovelimagedataset}, using SGD with a learning rate of $0.05$ and a batch size of $128$.

The experimental results are shown in Figure~\ref{plot:3}. The plots indicate that for different functions \( P_{\xi}(x;\tilde{S}) \), $\mathrm{E} \left[ c_{2\xi} \right]$ remains close to zero and Assumption~\ref{ass:3} holds with relatively small constants \( c_{2\xi} \) along the gradient trajectories when \( c_1\) is fixed. Specifically, when \( P_{\xi}(x;\tilde{S}) = f_{\xi}(x) - f_{\xi}(x_p) \) and \( c_1 = 1 \), it follows that \( c_{2\xi} \leq 0.402 \), and when \( P_{\xi}(x;\tilde{S}) = \|\nabla f_{\xi}(x)\|^2 \) and \( c_1 = 0.01 \), it follows that \( c_{2\xi} \leq 0.072 \).

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{plot2}}
\caption{Training the MLP model with $3$ fully connected layers.}
\label{plot:3}
\end{center}
\end{figure}

\subsection{ResNet architecture}

In the last experiment, we adopted the ResNet architecture~\cite{7780459} with a batch size of \( 128 \), training on the CIFAR-10 dataset~\cite{Krizhevsky} using the Adam optimizer with a learning rate of \( 0.001 \). Our implementation is based on the open-source \textit{cifar10-fast-simple} repository, available at \href{https://github.com/99991/cifar10-fast-simple.git}{https://github.com/99991/cifar10-fast-simple.git}.  

From Figure~\ref{plot:4}, we observe that Assumption~\ref{ass:3} can once again be satisfied for fixed values of \( c_1 \), with values of \( c_{2\xi} \) remaining relatively close to zero along gradient trajectories.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.5\textwidth]{plot3}}
\caption{Training the ResNet model.}
\label{plot:4}
\end{center}
\end{figure}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\section{Impact Statement}
Our contribution is primarily theoretical and we do not expect any negative impacts.

\bibliography{paper_icml}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\part*{Appendix}

\section{Proofs from Section~\ref{sec:main-1}}
\label{sec:examples of functions}

We consider different examples of functions \( f(x) \), \( x \in \mathbb{R} \) (see Figure~\ref{plot:1}):
\begin{align*}
f_1 &= x^2, \\
f_2 &=
\begin{cases}
f_1, & x \geq -1 \\
4\sqrt{-x}-3, & x < -1
\end{cases}, \\
f_3 &= \frac{x^4}{2} - x^2 + \frac{1}{2}, \\
f_4 &= x^4 - \frac{10}{3}x^3+3x^2, \\
f_5 &=
\begin{cases}
f_4, & x \geq 0 \\
f_2, & x < 0
\end{cases}, 
\end{align*}
that belong to a particular class of functions. We denote each class of function in Table~\ref{table:2} as $F_1$, $F_2$, $F_3$, $F_4$, $F_5$, and $F_6$.

\begin{table}[ht]
\caption{Assumption~\ref{ass:2} for \( P(x, \tilde{S}) = f(x) - f^\star \), where \( f^\star = 0 \), $\tilde S \subseteq S$, and for different choices of \( ( c_1, c_2, \tilde{S} ) \). Here, $S \subseteq \mathbb{R}$ is a set of global minimizers of $f$, $x^{\star} \in S$.}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
$c_1$, $\tilde{S}$ & $c_2 = 0$ & $c_2 \geq 0$ \\
\midrule
$c_1=1$, & $f_1 \in F_1$ & $f_1, \ f_3, \ f_4 \in F_2$ \\
$\tilde{S}=\{x^{\star}\}$ & & \\
\midrule
$c_1>0$, & $f_1, \ f_2, \ \in F_3$ & $f_1, \, f_2, \, f_3, \, f_4, \, f_5 \in F_4$ \\
$\tilde{S}=\{x^{\star}\}$ & & \\
\midrule
$c_1>0$, & $f_1, \ f_2, \ \in F_5$ & $f_1, \, f_2, \, f_3, \, f_4, \, f_5 \in F_6$  \\
$\tilde{S} = S$    & & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

1. Obviously, since \( f_1 \) is a convex function, we have \( f_1 \in F_i \) for \( i = 1, 2, 3, 4, 5, 6 \).

2. The function \( f_2 \in F_i \) for \( i = 3, 4, 5, 6 \), since
\[
\left\langle \nabla f_2(x), x \right\rangle = 2 \sqrt{-x} \geq c_1 f(x) = \underbrace{c_1}_{=1/2} (4\sqrt{-x} - 3), \quad \text{for} \quad x < -1.
\]
With \( c_1 = 1 \), it can be shown that it is not possible to satisfy this inequality by choosing any constant \( c_2 \geq 0 \).

3. The function \( f_3 \in F_i \) for \( i = 2, 4, 6 \). It is easy to show $f_3$ has two global minima: a global minimum at $x=1$ with $f^{\star}=0$ and a global minimum at $x=-1$ with $f^{\star}=0$. Then, choosing $c_1=1$ and $c_2 = 0.5$ (it is the smallest $c_2$ when $c_1=1$), we can show that
\[
\left\langle \nabla f_3(x), x - 1 \right\rangle - c_1 f_3(x) = (2x^3-2x)(x-1) - c_1 \left(\frac{x^4}{2} - x^2 + \frac{1}{2}\right) \geq -c_2, \quad \text{for} \quad x \geq 0,
\]
\[
\left\langle \nabla f_3(x), x + 1 \right\rangle - c_1 f_3(x) = (2x^3-2x)(x+1) - c_1 \left(\frac{x^4}{2} - x^2 + \frac{1}{2}\right) \geq -c_2, \quad \text{for} \quad x < 0.
\]
With \( c_2 = 0 \), it can be shown that it is not possible to satisfy these inequalities by choosing any constant \( c_1 > 0 \). 

By choosing $c_1=1$ and $c_2 \approx 1.437$ (it is the smallest $c_2$ when $c_1=1$), we have
\[
\left\langle \nabla f_3(x), x - 1 \right\rangle - c_1 f_3(x) = (2x^3-2x)(x-1) - c_1 \left(\frac{x^4}{2} - x^2 + \frac{1}{2}\right) \geq -c_2, \quad \text{for} \quad x \in \mathbb{R}.
\]
With \( c_2 = 0 \), it can be shown that it is not possible to satisfy this inequality by choosing any constant \( c_1 > 0 \).

4. The function \( f_4 \in F_i \) for \( i = 2, 4, 6 \). It is easy to show that \( f_4 \) has two minima: a global minimum at \( x = 0 \) with \( f^{\star} = 0 \), and a local minimum at \( x = 1.5 \). Then, choosing $c_1=1$ and $c_2 \approx 1.013$ (it is the smallest $c_2$ when $c_1=1$), we can show that
$$
\left\langle\nabla f_4\left(x\right), x\right\rangle - c_1 f_4(x) = (4-c_1)x^4-(10-\frac{10}{3}c_1)x^3+(6-3c_1)x^2 \geq - c_2, \quad \text{for} \quad x \in \mathbb{R}.
$$
With \( c_2 = 0 \), it can be shown that it is not possible to satisfy this inequality by choosing any constant \( c_1 > 0 \).

5. The function \( f_5 \in F_i \) for \( i = 4, 6 \). It is simply a piecewise function composed of \( f_2 \) and \( f_4 \). This statement can be easily proven using the proofs for \( f_2 \) and \( f_4 \).

\section{Proofs from Section~\ref{sec:Theorem}}

\subsection{Proof of Theorem~\ref{thm:1}}

\begin{proof}

By the definition of $x_p$ and the gradient update, we have
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 & \leq \| x^{k+1}-x^k_p\|^2 \\
& = \left\|x^k-\gamma^k \nabla f\left(x^k\right)-x^k_p\right\|^2 \\
& = \left\|x^k-x^k_p\right\|^2-2 \gamma^k\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle+\left(\gamma^k\right)^2\left\|\nabla f\left(x^k\right)\right\|^2.
\end{align*}

Since $0 < \gamma^k \leq (2-\alpha) \frac{\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + c_2 + \beta^k}{\left\|\nabla f\left(x^k\right)\right\|^2}$, we have
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 & \leq \left\|x^k-x^k_p\right\|^2-\alpha \gamma^{k} \left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + (2-\alpha)\beta^k \gamma^k + (2-\alpha)c_2 \gamma^k \\
& \stackrel{\ref{ass:2}}{\leq} \left\|x^k-x^k_p\right\|^2 - \alpha c_1 \gamma^k P(x^k;\tilde{S}) + (2-\alpha)\beta^k \gamma^k + 2 c_2 \gamma^k. 
\end{align*}

After telescoping the last inequality, we get
$$
\min_{k \in \{0, \dots, K\}} P(x^k;\tilde{S}) \leq \frac{\sum_{k=0}^K \gamma^k P(x^k;\tilde{S})}{\sum_{k=0}^K \gamma^k} \leq \frac{\left\|x^{0}-x^{0}_p\right\|^2}{\alpha c_1\sum_{k=0}^K \gamma^k} + \frac{\sum_{k=0}^K \gamma^k (2-\alpha)\beta^k}{\alpha c_1 \sum_{k=0}^K \gamma^k} + \frac{2c_2}{\alpha c_1}.
$$

\end{proof}

\subsection{Proof of Corollary~\ref{cor:1}}

\begin{proof}

If $P(x;\tilde{S}) = f(x) - f^{\star}$ and $\gamma^k = \frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2}=\frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$, then, by setting $\alpha=1$, $\beta^k=0$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
$$
0<\gamma^k \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^{\star}\right\rangle + c_2}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$

If we additionally assume that $f$ is $L$-smooth function, we can show
$$
\gamma^k = \frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \geq \frac{c_1 \frac{1}{2L} \left\|\nabla f\left(x^k\right)\right\|^2}{\left\|\nabla f\left(x^k\right)\right\|^2} \geq \frac{c_1}{2L}.
$$
Indeed, consider function $ \varphi(y) = f(y) - f^{\star}$, then $\varphi(y) \geq 0$ for all $y \in \mathbb{R}^d$. Using smoothness of $\varphi$ and choosing $y=x-\frac{1}{L} \nabla f(x)$, we get
\begin{align*}
0 \leq \varphi(y) & \leq \varphi(x) + \left\langle\nabla \varphi\left(x\right), y-x\right\rangle + \frac{L}{2} \|y-x\|^2 \\
& = f(x)-f^{\star} -  \frac{1}{2L} \| \nabla f(x) \|^2.
\end{align*} 

Therefore, from Theorem~\ref{thm:2} we get the following convergence result
$$
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} \leq \frac{2L \left\|x^{0}-x^0_p\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1}.
$$

\end{proof}

\subsection{Proof of Corollary~\ref{cor:2}}

\begin{proof}

Let us choose $\gamma^k = (2-\alpha)\frac{c_1 P(x^k;\tilde{S}) + \beta^k}{\left\|\nabla f\left(x^k\right)\right\|^2}$, then
$$
0<\gamma^k \stackrel{\ref{ass:2}}{\leq} (2-\alpha) \frac{\left\langle\nabla f\left(x^k\right), x^k-x^k_p\right\rangle + c_2 + \beta^k}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$

From Theorem~\ref{thm:1} we have 
$$
\frac{\sum_{k=0}^K \gamma^k P(x^k;\tilde{S})}{\sum_{k=0}^K \gamma^k} \leq \frac{\left\|x^{0}-x^{0}_p\right\|^2}{\alpha c_1\sum_{k=0}^K \gamma^k} + C^K,
$$
or equivalently
$$
\sum_{k=0}^K \gamma^k \left(P(x^k;\tilde{S}) - C^K \right) \leq \frac{\left\|x^{0}-x^{0}_p\right\|^2}{\alpha c_1}.
$$
Since $\gamma^k \geq (2-\alpha) \frac{c_1 P(x^k;\tilde{S})}{\left\|\nabla f\left(x^k\right)\right\|^2}$, we have
$$
\sum_{k=0}^K \frac{P(x^k;\tilde{S}) \left(P(x^k;\tilde{S})-C^K\right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \leq \frac{\left\|x^{0}-x^0_p\right\|^2}{(2-\alpha) \alpha c_1^2}.
$$

Let us assume that $P(x^k;\tilde{S}) > C^K$ for $k=0, \dots, K$, otherwise, $\min_{k \in \{0, \dots, K\}} P(x^k;\tilde{S}) \leq C^K$. If we also assume that $f$ has bounded gradients, i.e., $\| \nabla f\left(x\right) \| \leq G$ for all $x \in \mathbb{R}^d$, then we get
\begin{align*}
\sum_{k=0}^K \frac{\left(P(x^k;\tilde{S})-C^K\right)^2}{G^2} & \leq \sum_{k=0}^K \frac{P(x^k;\tilde{S}) \left(P(x^k;\tilde{S})-C^K\right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \leq \frac{\left\|x^{0}-x^0_p\right\|^2}{(2-\alpha) \alpha c_1^2},
\end{align*}
consequently,
$$
\min_{k \in \{0, \dots, K\}} \left(P(x^k;\tilde{S})-C^K\right)^2 \leq \frac{G^2 \left\|x^{0}-x^0_p\right\|^2}{(2-\alpha) \alpha c_1^2} \frac{1}{K+1},
$$
or equivalently
$$
\min_{k \in \{0, \dots, K\}} P(x^k;\tilde{S}) \leq \frac{G \left\|x^{0}-x^0_p\right\|}{\sqrt{(2-\alpha) \alpha} c_1} \frac{1}{\sqrt{K+1}} + C^K.
$$

\end{proof}

\subsection{Proof of Corollary~\ref{cor:3}}

\begin{proof}

From Theorem~\ref{thm:1} we have the following descent inequality
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 \leq \left\|x^k-x^k_p\right\|^2 - \alpha c_1 \gamma^k P(x^k;\tilde{S}) + (2-\alpha)\beta^k \gamma^k + 2 c_2 \gamma^k. 
\end{align*}

If $\gamma^k \leq \gamma^{k-1}$, instead of immediate telescoping the descent inequality, we can divide it by $\alpha c_1 \gamma^k$ and then telescope
\begin{align*}
\sum_{k=0}^K P(x^k;\tilde{S}) &\leq \sum_{k=0}^K \frac{\left\|x^{k}-x^k_p\right\|^2}{\alpha c_1 \gamma^k} - \sum_{k=0}^K \frac{\left\|x^{k+1}-x^{k+1}_p\right\|^2}{\alpha c_1 \gamma^{k}} + \sum_{k=0}^K \frac{(2-\alpha)\beta^k + 2 c_2}{\alpha c_1} \\
& \leq \frac{\left\|x^{0}-x^0_p\right\|^2}{\alpha c_1 \gamma^0} + \sum_{k=1}^K \frac{\left\|x^{k}-x^k_p\right\|^2}{\alpha c_1 \gamma^k} - \sum_{k=1}^{K} \frac{\left\|x^{k}-x^k_p\right\|^2}{\alpha c_1 \gamma^{k-1}} + \sum_{k=0}^K \frac{(2-\alpha)\beta^k + 2 c_2}{\alpha c_1}  \\
& \stackrel{\gamma^k \leq \gamma^{k-1}}{\leq} \frac{D_{\max}^2}{\alpha c_1} \left( \frac{1}{\gamma^0} + \sum_{k=1}^{K} \left( \frac{1}{\gamma^k} - \frac{1}{\gamma^{k-1}} \right) \right) + \sum_{k=0}^K \frac{(2-\alpha)\beta^k + 2 c_2}{\alpha c_1} \\
& = \frac{D_{\max}^2}{\alpha c_1 \gamma^K } + \sum_{k=0}^K \frac{(2-\alpha)\beta^k + 2 c_2}{\alpha c_1} ,
\end{align*}
where $D_{\max}^2 := \max_{k \in \{0, \dots, K\}} \left\|x^{k}-x^k_p\right\|^2$.

Therefore, we obtain
\begin{align*}
\min_{k \in \{0, \dots, K\}}P(x^k;\tilde{S}) \leq \frac{D_{\max}^2}{\alpha c_1 \gamma^K (K+1)} + \frac{2-\alpha}{\alpha c_1 (K+1)} \sum_{k=0}^K \beta^k + \frac{2 c_2}{\alpha c_1}.
\end{align*}

\end{proof}

\section{Proofs from Section~\ref{sec:Special Cases}}
\label{sec:proofs for special cases}

\subsection{Proofs for examples of stepsizes}

1. Let us choose $\gamma^k = \frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$. Then, by setting $\alpha=1$, $\beta^k = 0$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
$$
0<\gamma^k \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^{\star}\right\rangle + c_2}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$
If we assume that $f$ is $L$-smooth function, we can show
$$
\gamma^k = \frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \geq \frac{c_1 \frac{1}{2L} \left\|\nabla f\left(x^k\right)\right\|^2}{\left\|\nabla f\left(x^k\right)\right\|^2} \geq \frac{c_1}{2L}.
$$
Indeed, consider function $ \varphi(y) = f(y) - f^{\star}$, then $\nabla \varphi(x^{\star})=0$ and $\varphi(y) \geq \varphi(x^{\star}) = 0$ for all $y \in \mathbb{R}^d$. Using smoothness of $\varphi$ and choosing $y=x-\frac{1}{L} \nabla f(x)$, we get
\begin{align*}
0 \leq \varphi(y) & \leq \varphi(x) + \left\langle\nabla \varphi\left(x\right), y-x\right\rangle + \frac{L}{2} \|y-x\|^2 \\
& = f(x)-f^{\star} -  \frac{1}{2L} \| \nabla f(x) \|^2.
\end{align*}
Therefore, the convergence rate will be
$$
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} \leq \frac{2L \left\|x^{0}-x^{\star}\right\|^2}{c_1^2 (K+1)} + \frac{2c_2}{c_1}.
$$

2. Let us choose $\gamma^k = \frac{c_1 \left( f(x^k) - l^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}$, $l^{\star} \leq f^{\star}$. Then, by setting $\alpha=1$, $\beta^k = c_1\left(f^{\star} - l^{\star}\right)$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
$$
0<\gamma^k \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^{\star}\right\rangle + c_2 + \beta^k}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
$$
If we assume that $f$ is $L$-smooth function, we can show
$$
\gamma^k = \frac{c_1 \left( f(x^k) - l^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \geq \frac{c_1 \left( f(x^k) - f^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \geq \frac{c_1}{2L}.
$$
Therefore, the convergence rate will be
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} \leq \frac{2L \left\|x^{0}-x^{\star}\right\|^2}{c_1^2 (K+1)} + {\sigma^2} + \frac{2c_2}{c_1},
\end{align*}
where $\sigma^2:=f^{\star} - l^{\star}$.

3. Let us choose 
$$
\gamma^k = \frac{1}{c^k} \min \left\{ \frac{c_1 \left( f(x^k) - l^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2}, \gamma^{k-1} c^{k-1} \right\},
$$
where $l^{\star} \leq f^{\star}$, $\{c^k\}$ is any non-decreasing sequence such that $c^k \geq 1$, $c^{-1}=c^0$, $\gamma^{-1}=\gamma^0>0$. First, note that $\gamma^k \leq \gamma^{k-1}$ holds. Then, by setting $\alpha=1$, $\beta^k = \frac{c_1}{c^k}\left(f^{\star} - l^{\star}\right)$, $\gamma^k$ satisfies the relations of Theorem~\ref{thm:1}
\begin{align*}
0<\gamma^k & \leq \frac{1}{c^k} \frac{c_1 \left( f(x^k) - l^{\star} \right)}{\left\|\nabla f\left(x^k\right)\right\|^2} \stackrel{\ref{ass:2}}{\leq} \frac{\left\langle\nabla f\left(x^k\right), x^k-x^{\star}\right\rangle + c_2 + \beta^k}{\left\|\nabla f\left(x^k\right)\right\|^2}. 
\end{align*}
If we assume that $f$ is $L$-smooth function, we can show recursively that
\begin{align*}
\gamma^{K} &= \frac{1}{c^K} \min \left\{ \frac{c_1 \left( f(x^K) - l^{\star} \right)}{\left\|\nabla f\left(x^K\right)\right\|^2}, \gamma^{K-1} c^{K-1} \right\} \\ & \geq \min \left\{ \frac{c_1 \left( f(x^K) - f^{\star} \right)}{c^K \left\|\nabla f\left(x^K\right)\right\|^2}, \frac{\gamma^{K-1} c^{K-1}}{c^K} \right\} \\
&\geq \min \left\{ \frac{c_1}{2 c^K L}, \frac{\gamma^{K-1} c^{K-1}}{c^K} \right\} \\
&\geq \min \left\{ \frac{c_1}{2 c^K L}, \dots, \frac{c_1}{2 c^0 L},  \frac{\gamma^{0} c^{0}}{c^K} \right\} \\
& \dots \\
& \geq \min \left\{ \frac{c_1}{2 c^K L}, \frac{\gamma^{0} c^{0}}{c^K} \right\} = \frac{c_1}{2 c^K \tilde L},
\end{align*}
where $\tilde L = \max \left\{ L , \frac{c_1}{2 \gamma^0 c^0} \right\}$.

Therefore, the convergence rate will be
\begin{align*}
\min_{k \in \{0, \dots, K\}}f(x^k) - f^{\star} & \leq \frac{D^2}{c_1 \gamma^K (K+1)} + \frac{1}{ c_1 (K+1)} \underbrace{\sum_{k=0}^K \beta^k}_{=\sum_{k=0}^K \frac{c_1 \sigma^2}{c^k}} + \frac{2 c_2}{c_1} \\
& \leq \frac{2D^2 c^K \tilde L}{c_1^2 (K+1)} + \frac{\sigma^2}{K+1}\sum_{k=0}^K \frac{1}{c^k} + \frac{2 c_2}{c_1} \\
& \stackrel{c^k=\sqrt{1+k}}{\leq} \frac{2D^2 c^K \tilde L}{c_1^2 (K+1)} + \frac{2\sigma^2}{ \sqrt{K+1}} + \frac{2 c_2}{c_1}.
\end{align*}
where $\sigma^2=f^{\star} - l^{\star}$.

\section{Proofs from Section~\ref{sec:Extensions}}

\subsection{Proof of Theorem~\ref{thm:2}}

\begin{proof}

By the definition of $x_p$ and the gradient update, we have
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 & \leq \| x^{k+1}-x^k_p\|^2 \\
& = \left\|x^k-\gamma^k \nabla f_{\xi}\left(x^k\right)-x^k_p\right\|^2 \\
& = \left\|x^k-x^k_p\right\|^2-2 \gamma^k\left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle+\left(\gamma^k\right)^2\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2.
\end{align*}

Since $\gamma_{\star} \leq \tilde \gamma^k \leq (2-\alpha) \frac{\left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle + {c_{2}}_{\xi} + \beta_{\xi}^k}{\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2}$ and $\gamma^k = \min\{\tilde \gamma^k, \gamma_b\}$, we have
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 & \leq \left\|x^k-x^k_p\right\|^2 - 2 \gamma^k\left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle \mid x^k + \gamma^k \tilde \gamma^k\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2 \\
& \leq \left\|x^k-x^k_p\right\|^2 - 2 \gamma^k\left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle \\
& \quad + (2-\alpha) \gamma^{k} \left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle + (2-\alpha)\left(\gamma^k \beta_{\xi}^k + \gamma^k {c_{2}}_{\xi} \right) \\
& = \left\|x^k-x^k_p\right\|^2 - \alpha \gamma^k\left\langle\nabla f_{\xi}\left(x^k\right), x^k-x^k_p\right\rangle + (2-\alpha)\left(\gamma^k \beta_{\xi}^k + \gamma^k {c_{2}}_{\xi} \right) \\ 
& \stackrel{\ref{ass:3}}{\leq} \left\|x^k-x^k_p\right\|^2 - \alpha \gamma^k {c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S})+\alpha \gamma^k {c_{2}}_{\xi} + (2-\alpha) \left(\gamma^k \beta_{\xi}^k + \gamma^k {c_{2}}_{\xi} \right) \\  
& = \left\|x^k-x^k_p\right\|^2- \alpha \gamma^k {c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S}) + (2-\alpha) \gamma^k \beta_{\xi}^k + 2 \gamma^k {c_{2}}_{\xi} \\
& \leq \left\|x^k-x^k_p\right\|^2 - \alpha \gamma_{\min}  {c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S}) + (2-\alpha) \gamma_b \beta_{\xi}^k + 2 \gamma_b {c_{2}}_{\xi},
\end{align*}
where $\gamma_{\min} := \min\{\gamma_{\star}, \gamma_b \}$.

By taking expectation, we have
\begin{align*}
\mathrm{E} \left[ \left\|x^{k+1}-x^{k+1}_p\right\|^2\right] \leq \mathrm{E} \left[ \left\|x^k-x^k_p\right\|^2 \right]-\alpha \gamma_{\min} \mathrm{E} \left[ {c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S})\right] + (2-\alpha) \gamma_b \mathrm{E} \left[\beta_{\xi}^k\right] + 2 \gamma_b \mathrm{E} \left[ {c_{2}}_{\xi} \right].
\end{align*}

After telescoping the last inequality, we get
$$
\min_{k \in \{0, \dots, K\}} \mathrm{E} \left[{c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S})\right] \leq \frac{\mathrm{E} \left[ \left\|x^{0}-x^{0}_p\right\|^2\right]}{\alpha \gamma_{\min} (K+1)} + \frac{(2-\alpha)\gamma_b}{\alpha \gamma_{\min} (K+1)} \sum_{k=0}^K \mathrm{E} \left[\beta_{\xi}^k\right] + \frac{2 \gamma_b \mathrm{E} \left[{c_{2}}_{\xi} \right]}{\alpha \gamma_{\min}}.
$$

\end{proof}

\subsection{Proof of Corollary~\ref{cor:4}}

\begin{proof}

It follows straightforwardly from Theorem~\ref{thm:2} and using the smoothness of $f_{\xi}$, since $\tilde{\gamma}^k = \frac{c_1 \left( f_{\xi}(x^k) - f_{\xi}^{\star} \right)}{\left\|\nabla f_{\xi}\left(x^k\right)\right\|^2} \geq \frac{c_1}{2L} = \gamma_{\star}$.

\end{proof}

\subsection{Proof of Corollary~\ref{cor:5}}

\begin{proof}

From Theorem~\ref{thm:2} we have the following descent inequality
\begin{align*}
\left\|x^{k+1}-x^{k+1}_p\right\|^2 &\leq \left\|x^k-x^k_p\right\|^2 - \alpha \gamma^k  {c_{1}}_{\xi} P_{\xi}(x^k;\tilde{S}) + (2-\alpha) \gamma^k \beta_{\xi}^k + 2 \gamma^k {c_{2}}_{\xi}.    
\end{align*}

If $\tilde \gamma^k \leq \tilde \gamma^{k-1}$, then $\gamma^k \leq \gamma^{k-1}$ and instead of immediate telescoping the descent inequality, we can divide it by $\alpha \gamma^k$ and then telescope
\begin{align*}
\sum_{k=0}^K c_{1 \xi} P_{\xi}(x^k;\tilde{S}) &\leq \sum_{k=0}^K \frac{\left\|x^{k}-x^k_p\right\|^2}{\alpha \gamma^k} - \sum_{k=0}^K \frac{\left\|x^{k+1}-x^{k+1}_p\right\|^2}{\alpha \gamma^{k}} + \sum_{k=0}^K \frac{(2-\alpha)\beta_{\xi}^k + 2 c_{2 \xi}}{\alpha} \\
& \leq \frac{\left\|x^{0}-x^0_p\right\|^2}{\alpha \gamma^0} + \sum_{k=1}^K \frac{\left\|x^{k}-x^k_p\right\|^2}{\alpha \gamma^k} - \sum_{k=1}^{K} \frac{\left\|x^{k}-x^k_p\right\|^2}{\alpha \gamma^{k-1}} + \sum_{k=0}^K \frac{(2-\alpha)\beta_{\xi}^k + 2 c_{2 \xi}}{\alpha}  \\
& \stackrel{\gamma^k \leq \gamma^{k-1}}{\leq} \frac{D_{\max}^2}{\alpha} \left( \frac{1}{\gamma^0} + \sum_{k=1}^{K} \left( \frac{1}{\gamma^k} - \frac{1}{\gamma^{k-1}} \right) \right) + \sum_{k=0}^K \frac{(2-\alpha)\beta_{\xi}^k + 2 c_{2 \xi}}{\alpha} \\
& = \frac{D_{\max}^2}{\alpha \gamma^K } + \sum_{k=0}^K \frac{(2-\alpha)\beta_{\xi}^k + 2 c_{2 \xi}}{\alpha} ,
\end{align*}
where $D_{\max}^2 := \max_{k \in \{0, \dots, K\}} \left\|x^{k}-x^k_p\right\|^2$.

After taking expectation, we have
\begin{align*}
\sum_{k=0}^K \mathrm{E} \left[ c_{1 \xi} P_{\xi}(x^k;\tilde{S}) \right] \leq \mathrm{E} \left[ \frac{D_{\max}^2}{\alpha \gamma^K } \right] + \sum_{k=0}^K \frac{(2-\alpha)\mathrm{E} \left[\beta_{\xi}^k\right] + 2 \mathrm{E} \left[ c_{2 \xi} \right]}{\alpha} ,
\end{align*}

Therefore, we obtain
\begin{align*}
\min_{k \in \{0, \dots, K\}} \mathrm{E} \left[c_{1 \xi} P(x^k;\tilde{S})\right] \leq \mathrm{E} \left[ \frac{D_{\max}^2}{\alpha \gamma^K } \right] \frac{1}{K+1} + \frac{2-\alpha}{\alpha (K+1)} \sum_{k=0}^K \mathrm{E} \left[\beta_{\xi}^k\right] + \frac{2 \mathrm{E} \left[ c_{2 \xi} \right]}{\alpha}.
\end{align*}

\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
