\section{Related Work}
\label{sec:relatedworks}

\subsection{Video to Audio Generation}
% LoVA **Zhu, "Foley"** use a DiT for V2A tasks, specifically targeting 10-second long videos. 
In this paper, we focus on ``Foley'' audio,\footnote{\href{https://www.youtube.com/watch?v=UO3N_PRIgX0}{YouTube: The Magic of Making Sound}} which refers to sound effects added during post-production to enrich the auditory experience of multimedia ____ such as the crunch of leaves or the clinking of glass bottles. Earlier AI-based Foley generation methods were conditioned on class labels **Hsu, "Text-to-Speech Synthesis with Class Labels"** or text prompts ____ . Building on this, recent work has expanded video-to-audio generation. SpecVQGAN **Choi et al., "SpecVQGAN: Spectrogram-Based VQ-VAE for Audio Generation"** adopts a transformer that generates high-fidelity spectrograms from video frames using a VQGAN-based codebook and perceptual audio loss. ____ requires both silent video and conditional audio to produce candidate tracks, which are filtered using an audio-visual synchronization model. Diff-Foley **Chen et al., "Diff-Foley: Diffusion-Based Video-to-Audio Generation with Contrastive Pretraining"** only requires silent video as input. It first aligns audio-visual features through contrastive pretraining (CAVP), then trains a diffusion model on spectrogram latents conditioned on CAVP features. FoleyCrafter **Kim et al., "FoleyCrafter: A Unified Framework for Video-to-Audio Generation with Text Prompts"** introduces optional text prompts for finer control and incorporates a semantic adapter and temporal controller for improved alignment. V-AURA **Shen et al., "V-AURA: Vision-Aware Audio Representation Learning"** proposes an autoregressive model with high-frame-rate visual encoding and cross-modal fusion for fine-grained temporal precision. VATT ____ presents a multi-modal system that generates audio from video with optional text prompts. Departing from GANs and diffusion, FRIEREN **Wang et al., "FRIEREN: Flow Matching for Video-to-Audio Generation"** uses flow matching as its generative backbone. To better preserve temporal structure, it introduces a non-autoregressive vector field estimator without temporal downsampling.



\subsection{Audio to Video Generation}
% AADiff ____ is an early example of an audio-aligned diffusion framework that uses both text and audio as input. 
 % Meanwhile, AVSyncD ____ proposes and addresses a novel task: given an image, it uses an audio clip to animate the objects within the image accordingly.
Given the high information density of video, video-to-audio generation tasks typically treat video as the primary input, with text serving as an auxiliary cue. In contrast, audio to video generation mainly relies on audio alignment. Due to the limited semantic context provided by audio itself, it is difficult for both humans and machines to distinguish (e.g., distinguishing a given audio clip corresponds to climbing stairs or tap dancing). As a result, audio-to-video generation often depends on text or images to supply the missing context, and dedicated efforts in this direction remain relatively limited. ____ learns to generate semantically aligned video from audio. It maps audio into the StyleGAN latent space and refines the output using CLIP-based multimodal embeddings to reinforce audio-visual coherence. TPoS **Zhou et al., "TPoS: Text-to-Video Synthesis with Audio Adaptor"** follows a stage-wise strategy: it first generates an initial frame from a text prompt, then progressively adapts the visuals based on the audio input. ____ proposes a lightweight adaptor that translates audio features into the input format expected by a pre-trained text-to-video model, enabling audio-driven video generation with minimal changes to the backbone.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.80\textwidth]{fig/overview.pdf}
    \caption{Overview of the proposed UniForm. Vision tokens and audio tokens are integrated and processed within a unified latent space using a DiT model to learn their representations. During training, one of three tasks is randomly selected in each iteration, with task tokens guiding the learning of the DiT. The text encoder, the encoder-decoder for video and audio, and the audio vocoder are all pre-trained models that remain frozen throughout.}
    \label{fig:overview}
\end{figure*}
\subsection{Diffusion-based Generation}\label{subsec:diffusion}
Diffusion models, as a class of probabilistic generative models, have received growing attention for their remarkable performance in diverse domains such as image generation **Ho et al., "DALL-E: Diffusion Models for Deep Image Synthesis"** and audio synthesis ____ . The majority of existing approaches are built upon Denoising Diffusion Probabilistic Models (DDPMs) ____ , which form the foundation of this paradigm. The key idea of diffusion modeling is to define a forward process that progressively perturbs data into Gaussian noise through a sequence of noise-adding steps. The model is then trained to approximate the reverse process, which starts from pure noise and performs iterative denoising steps to recover samples that approximate the original data distribution. To reduce the computational cost of operating in high-dimensional spaces, Latent Diffusion Models (LDMs) ____ shift the diffusion process into a lower-dimensional latent space, enabling more efficient generation. ____ explore replacing the previously used U-Net backbone with a transformer operating in latent space. Their results show that, given sufficient compute, the Diffusion Transformer (DiT) produces samples that closer to the original data distribution. In this work, we adopt DiT as the backbone of our multi-task architecture, leveraging its scalability and strong performance in modeling a unified latent space across audio and visual modalities.