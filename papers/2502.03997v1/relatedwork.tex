\section{Related Works}
\label{related_work}  
\noindent\textbf{CAD Generation.}  
Parametric CAD, defined by its sketch-and-extrude operations, is central to mechanical design due to its ability to retain modeling history, which facilitates both editing and manufacturing. 
Recent large-scale CAD datasets~\citep{wu2021deepcad} have fueled the development of generative models.
\citet{wu2021deepcad} explores unconditional generation, where a random latent vector is used as input to generate CAD models. 
\citet{xu2022skexgen, xu2023hierarchical, zhang2024flexcad} focus on design variation generation, which randomly modifies parts of an existing CAD model. 
\citet{khan2024cad} studies text-based CAD generation, which transforms textual descriptions into CAD models.
Our work differs in two key aspects.
First, we target a distinct task, unlike prior work, which either lacks text-based control~\citep{wu2021deepcad, xu2022skexgen, xu2023hierarchical, zhang2024flexcad} or disregards existing CAD models as constraints~\citep{khan2024cad}.
Second, we introduce a novel locate-then-infill framework based on LLMs to handle the composite nature of text-based CAD editing.
Previous approaches either rely on a VAE-based framework~\citep{wu2021deepcad, xu2022skexgen, xu2023hierarchical} or use LLMs without accounting for task-specific needs~\citep{zhang2024flexcad,khan2024cad}.


\noindent\textbf{Large Language Models (LLMs).} 
Recently, LLMs like GPT~\citep{achiam2023gpt, OpenAI2023} and LLaMA~\citep{touvron2023llama} have distinguished themselves from smaller models through remarkable emergent capabilities. 
Beyond excelling in natural language processing, LLMs have transforms generative tasks in other domains, e.g., motion generation~\citep{zhang2024motiongpt} and material generation~\citep{gruverfine}. 
These advancements inspire us to adopt LLMs as the backbone for sub-tasks in our locate-then-infill framework.
Moreover, LLMs and Large Vision-Language Models (LVLMs) are increasingly utilized for data synthesis to enhance training~\citep{xu2024wizardlm, yumetamath}. 
Specifically, \citet{khantext2cad} leverage LLMs/LVLMs to synthesize data for text-based CAD generation. 
However, our task of text-based CAD editing presents distinct challenges.
First, unlike \citet{khan2024cad}, who generate two-tuple data (a text prompt and a CAD model), our task involves creating triplet data: an editing instruction, an original CAD model, and an edited CAD model. 
We address this by combining design variation models with LLMs/LVLMs.
Second, while \citet{khan2024cad} focus on captioning single CAD models, our task requires summarizing differences between two CAD models. 
We handle this by introducing a stepwise captioning strategy.


\noindent\textbf{Text-based Editing in Other Domains.}
Text-based editing has been widely explored across various domains, e.g., 3D editing~\citep{mikaeili2023sked}, image editing~\citep{meng2021sdedit, brooks2023instructpix2pix}, and video editing~\citep{chai2023stablevideo, ceylan2023pix2video}.
It enables users to specify and modify particular objects or attributes with precision and flexibility.
Inspired by these advancements, we introduce text-based editing in the CAD domain for the first time.

\begin{figure*}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.98\textwidth]{img/data_generation.pdf}  
\end{center}
\caption{
Illustration of automated data synthesis pipeline.
}
\label{fig:data_generation}
\end{figure*}