\section{\ours: Generative Flow Models for Visual Creation}

In this section, we present three core components of \ours, the image-video joint VAE~\citep{yang2024cogvideox}, the \ours Transformer architecture, and the rectified flow formulation. These components are designed to work synergistically, forming a cohesive and scalable framework for joint image and video generation. During training, each raw video input $x\in \mathbb{R}^{T\times H \times W \times3}$ (with images treated as a special case where $T=1$ ) is encoded from the pixel space to a latent space using a 3D image-video joint VAE~(\Cref{sec:vae}). The encoded latents are then organized into mini-batches containing both video and image representations, facilitating the learning of a unified cross-modal representation. Subsequently, the rectified flow formulation~(\Cref{sec:method-flow}) is applied to these latents, leveraging a series of Transformer blocks~(\Cref{sec:trans-block}) to model complex temporal and spatial dependencies effectively.

 
\subsection{Image-Video Joint VAE}\label{sec:vae}
Earlier research \citep{he2022latent,rombach2022high,esser2021taming} demonstrates that diffusion and flow-based models can significantly improve efficiency and performance by modeling in latent space through a Variational Auto-Encoder (VAE)~\citep{esser2021taming, kingma2013auto}. Inspired by Sora~\citep{videoworldsimulators2024}, the open-source community has introduced 3D-VAE to explore spatio-temporal compression within latent spaces for video generation tasks~\citep{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109, opensora, yang2024cogvideox}. To extend the advantages of latent space modeling across multiple media formats, including images and videos, we adopt a jointly trained Image-Video VAE~\citep{yang2024cogvideox} that handles both image and video data within a unified framework. Specifically, for videos, we apply a compression stride of $8 \times 8 \times 4$ across height, width, and temporal dimensions, respectively, while for images, the compression stride is set to $8 \times 8$ in spatial dimensions.



\subsection{Transformer Architectures}\label{sec:trans-block}

The design of the \ours Transformer block builds upon GenTron~\citep{chen2024gentron}, an extension of the class-conditioned diffusion transformer~\citep{peebles2023scalable} for text-to-image/video tasks. It includes a self-attention module for capturing inter-token correlations, a cross-attention layer to integrate textual conditional embeddings (extracted via the Flan-T5 language model~\citep{chung2024scaling}), a feed-forward network~(FFN) for feature projection, and a layer-wise adaLN-Zero block that incorporates timestep information to guide feature transformations. Additionally, we introduce several recent design enhancements to improve model performance and training stability, as detailed below.

\begin{table}[t]
    \centering
    \tablestyle{8pt}{1.15}
    \begin{tabular}{ccccc}
    \toprule
    Model & Layer & Model Dimension & FFN Dimension & Attention Heads  \\
    \midrule
    \ours-1B & 28 & 1152 & 4608  & 16 \\
    \ours-2B & 28 & 1792 & 7168  & 28 \\
    \ours-8B & 40 & 3072 & 12288 & 48 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Architecture configurations for \ours Models.} \ours-1B model is only used for pilot experiments in \Cref{sec:method-flow}}\label{tab:model-config}
\end{table}

\paragraph{Plain Full Attention.} In Transformer-based video generative models, previous approaches~\citep{chen2024gentron, wu2023tune, singer2023makeavideo, blattmann2023align} typically combine temporal attention with spatial attention to extend text-to-image generation to video. While this method reduces computational cost, it is sub-optimal for modeling complex temporal motions, as highlighted in prior work~\citep{yang2024cogvideox, polyak2024movie}. In \ours, we adopt full attention to model multi-modal tokens (image and video) within a unified network. Given the large number of video tokens remaining after VAE processing—particularly for high-frame-rate, long-duration videos—we leverage \texttt{FlashAttention}~\citep{shah2024flashattention, dao2023flashattention2} and sequence parallelism~\citep{li2021sequence} to optimize both GPU memory usage and computational efficiency.

\paragraph{Patch n’ Pack.} To enable joint training on images and videos of varying aspect ratios and lengths, we follow the approach from NaViT~\citep{dehghani2024patch}, packing both modalities into a single minibatch along the sequence dimension. This method allows flexible mixing of training instances with different sequence lengths into a single batch, eliminating the need for data buckets~\citep{podell2023sdxl}.

\paragraph{3D RoPE Position Embedding.} Rotary Position Embedding (RoPE)~\citep{su2024roformer} has demonstrated effectiveness in LLMs by enabling greater sequence length flexibility and reducing inter-token dependencies as relative distances increase. During joint training, we apply 3D RoPE embeddings to image and video tokens. In our joint training framework, we extend 3D RoPE embeddings to image and video tokens, leveraging their extrapolation capability to accommodate varying resolutions. This adaptability makes RoPE particularly suited for handling diverse resolutions and video lengths. Furthermore, our empirical analysis revealed that RoPE converges faster than sinusoidal positional embeddings during transitions across different training stages

\paragraph{Q-K Normalization.} Training large-scale Transformers can occasionally result in loss spikes, which may lead to model corruption, manifesting as severe artifacts or even pure noise in generated images or videos. To mitigate this issue, we incorporate query-key normalization~\citep{dehghani2023scaling} to stabilize the training process. Specifically, we apply RMSNorm~\citep{zhang2019root} to each query-key feature prior to attention computation, ensuring smoother and more reliable training dynamics.





The overall Transformer model is constructed by stacking a sequence of blocks as described above. To address varying computational demands and performance requirements, we design three model variants, summarized in \Cref{tab:model-config}. The \ours-1B model serves as a lightweight option for pilot experiments. The \ours-2B variant consists of 28 layers, each with a model dimension of 1792 and 28 attention heads, providing a balance between computational efficiency and expressive capacity. In contrast, the larger \ours-8B variant features 40 layers, a model dimension of 3072, and 48 attention heads, delivering superior modeling capacity aimed at achieving high generation quality.


\subsection{Flow-based Training}\label{sec:method-flow}
Our flow-based formulation is rooted in the rectified flow~(RF) algorithm~\citep{albergo2023building, lipman2023flow, liu2023flow}, where a sample is progressively transformed from a prior distribution, such as a standard normal distribution, to the target data distribution. This transformation is achieved by defining the forward process as a series of linear interpolations between the prior and target distributions. Specifically, given a real data sample $\mathbf{x}_1$ from the target distribution and a noise sample $\mathbf{x}_0 \sim \mathcal{N}(0, 1)$ from the prior distribution, a training example is constructed through linear interpolation:

\begin{equation}\label{eq:flow-formulation}
\mathbf{x}_t = t \cdot \mathbf{x}_1 + (1 - t) \cdot \mathbf{x}_0,
\end{equation}

\noindent where $t \in [0, 1]$ represents the interpolation coefficient. The model is trained to predict the velocity, defined as the time derivative of $\mathbf{x}_t$, $\mathbf{v}_t = \frac{d\mathbf{x}_t}{dt}$, which guides the transformation of intermediate samples $\mathbf{x}_t$ towards the real data $\mathbf{x}_1$ during inference. By establishing a direct, linear interpolation between data and noise, RF simplifies the modeling process, providing improved theoretical properties, conceptual clarity, and faster convergence across data distributions.

\ours takes a pioneering step by adopting a flow-based formulation for joint image-and-video generation.
We conduct a pilot experiment to validate the rapid convergence of flow-based training by performing class-conditional generation with \ours-1B a model specifically designed for these proof-of-concept experiments, on ImageNet-1K ($256 \times 256$)~\citep{deng2009imagenet}. The model is configured with 28 layers, an attention dimension of 1152, and 16 attention heads. To evaluate performance, we compare key metrics, such as FID-50K and Inception Score~(IS), for models trained using the denoising diffusion probabilistic model (DDPM)~\citep{ho2020denoising} and rectified flow.  As shown in \Cref{tab:toy-fid}, RF demonstrates  faster convergence than DDPM. For instance, \ours-1B~(RF) achieves a lower FID-50K after 400k training steps compared to \ours-1B~(DDPM), which requires 1000k steps to reach a similar level of performance.


\subsection{Training Details}\label{sec:training}
\paragraph{Multi-stage Training.} Directly optimizing joint image-and-video training poses significant challenges, as the network must simultaneously learn spatial semantics critical for images and temporal motion dynamics essential for videos. To tackle this complexity, we introduce a decomposed, multi-stage training strategy that progressively enhances the model’s capabilities, ensuring effective and robust learning across both modalities.

\begin{table}[t]
    \centering
    \tablestyle{9pt}{1.12}
    \begin{tabular}{cccccccc}
    \toprule
    Loss & Steps & FID $\downarrow$ & sFID $\downarrow$ & IS $\uparrow$ & Precision $\uparrow$ & Recall $\uparrow$ \\
    \midrule
    DDPM & 200k & 3.0795 & 4.3498 & 226.4783 & 0.8387 & 0.5317  \\
    DDPM & 400k & 2.5231 & 4.3821 & 265.0612 & 0.8399 & 0.5591 \\
    DDPM & 1000k & 2.2568 & 4.4887 & 286.5601 & 0.8319 & 0.5849  \\
    \midrule
    Rectified Flow & 200k & 2.7472 & 4.6416 & 232.3090 & 0.8239 & 0.5590  \\
    Rectified Flow & 400k & 2.1572 & 4.5022 & 261.1203 & 0.8210 & 0.5871  \\

    \bottomrule
    \end{tabular}
    \caption{\textbf{Proof-of-concept experiments of class-conditional generation on ImageNet 256$\times$256.} Rectified flow achieves faster convergency compared to DDPM.}\label{tab:toy-fid}
\end{table}

\begin{itemize}[align=parleft, left=0pt, labelsep=0.5em]
    \item \textbf{Stage-1: Text-Semantic Pairing.} In the initial stage, we focus on establishing a solid understanding of text-to-image relationships by pretraining \ours on text-to-image tasks. This step is critical for grounding the model in basic semantic comprehension, enabling it to learn to associate textual prompts with high-level visual semantics. Through this process, the model develops a reliable capacity for representing visual concepts essential for both image and video generation, such as object attributes, spatial configurations, and contextual coherence.

    \item \textbf{Stage-2: Image-and-video joint learning.} Building on the foundational capabilities of text-to-semantic pairing, we extend \ours to joint learning across both image and video data. This stage leverages the unified framework of \ours, which employs a global attention mechanism adaptable to both images and videos. Besides, acquiring a substantial volume of high-quality video data is generally more resource-intensive compared to obtaining a similar amount of high-quality image data. To address this disparity, our framework integrates images and videos into unified token sequences during training, enabling the rich information inherent in high-quality images to enhance the generation of video frames~\citep{chen2024gentron}. By curating a carefully balanced dataset of images and videos, \ours not only gains the capability to generate both high-quality images and videos but also enhances the visual quality of videos by leveraging the rich information from high-quality image data.

    \item \textbf{Stage-3: Modality-specific finetuning.} In the final stage, we fine-tune \ours for each specific modality to further enhance its output quality. For text-to-image generation, we implement image-centric adjustments aimed at producing more visually compelling images. For text-to-video generation, we focus on adjustments that improve temporal smoothness, motion continuity, and stability across frames, resulting in realistic and fluid video outputs.

\end{itemize}

\paragraph{Cascaded Resolution Training.} In the second stage of joint training, we adopt a cascade resolution strategy to optimize the learning process. Initially, training is conducted on low-resolution image and video data ($288\times512$), enabling the model to efficiently focus on fundamental text-semantic-motion relationships at reduced computational costs. Once these core interactions are well-established, the resolution of the training data is progressively increased, transitioning from $480\times864$ to $720\times1280$. This stepwise resolution enhancement allows \ours to refine its understanding of intricate details and improve overall image fidelity, ultimately leading to superior generation quality for both images and videos.


\subsection{Image-to-Video}
To extend \ours for adapting an \emph{image} as an additional condition for video generation, we employ a widely used strategy by using the first frame of each clip as the reference image~\citep{girdhar2023emu, blattmann2023stable, yang2024cogvideox}. The corresponding image tokens are broadcasted and concatenated with the paired noised video tokens along the channel dimension. To fully leverage the pretrained knowledge during fine-tuning, we introduce a single MLP layer for channel alignment, while preserving the rest of the model architecture identical to \ours-T2V.


\section{Infrastructure Optimization} 

To achieve scalable and efficient training of \ours, we first adopt advanced parallelism strategies~(\Cref{sec:infra-model-parallel}), to handle the challenges of long-context, large-scale models. To further optimize memory usage and balance computation with communication, we implement fine-grained Activation Checkpointing~(\Cref{sec:infra-ac}). Additionally, we integrate robust fault tolerance mechanisms from MegaScale, enabling automated fault detection and recovery with minimal disruption~(\Cref{sec:infra-robust-training}). Finally, ByteCheckpoint is utilized to ensure efficient and scalable saving and loading of training states, supporting flexibility across diverse hardware configurations~(\Cref{sec:infra-byted-ckpt}). The details of these optimizations are introduced below.

\subsection{Model Parallelism Strategies}\label{sec:infra-model-parallel}
The substantial model size and the exceptionally long sequence length (exceeding 220K tokens for the longest sequence) necessitate the adoption of multiple parallelism strategies to ensure efficient training. Specifically, we employ 3D parallelism to achieve scalability across three axes: input sequences, data, and model parameters.


\paragraph{Sequence-Parallelism (SP)}~\citep{korthikanti2023reducing, li2021sequence, jacobs2023deepspeed} slices the input across the sequence dimension for independent layers (\eg, LayerNorm) to eliminate redundant computations, reduce memory usage, and support padding for non-conforming input. We adopt \textit{Ulysses}~\citep{jacobs2023deepspeed} as our implementation, which shards samples across the sequence parallel group from the start of the training loop. During attention computation, it uses all-to-all communication to distribute query, key, and value shards, allowing each worker to process the full sequence but only a subset of attention heads. After parallel computation of attention heads, another all-to-all communication aggregates the results, recombining all heads and the sharded sequence dimension.

\paragraph{Fully Sharded Data Parallelism~(FSDP)}~\citep{pytorch_fsdp} partitions all parameters, gradients and optimizer states across the data parallel ranks. Instead of all-reduce in Distributed Data Parallelism, FSDP performs all-gather for parameters and reduce-scatter for gradients, enabling overlap with forward and backward computations to potentially reduce communication overhead. In our case, we adopt the \texttt{HYBRID\_SHARD} strategy, which combines \texttt{FULL\_SHARD} within a \emph{shard group} and parameter replication across such groups, which effectively implements data parallelism (DP). This approach minimizes communication costs by limiting all-gather and reduce-scatter operations.


\subsection{Activation Checkpointing}\label{sec:infra-ac}

While the parallelism methods discussed in \Cref{sec:infra-model-parallel} provide significant memory savings and enable large-scaling training with long sequences, they inevitably introduce communication overhead among ranks, which can lead to suboptimal overall performance. To address this issue and better balance the computation and communication by maximizing their overlap in the profiling trace, we designed a fine-grained Activation Checkpointing~(AC)~\citep{chen2016training} strategy. Specifically, we implemented selective activation checkpointing to minimize the number of layers requiring activation storage while maximizing GPU utilization.

\subsection{Cluster Fault Tolerance}\label{sec:infra-robust-training}

Scaling \ours training to large-scale GPU clusters inevitably introduces fault scenarios, which can reduce training efficiency. The likelihood of encountering failures increases with the number of nodes, as larger systems have a higher probability of at least one node failing. These disruptions can extend training time and increase costs. To enhance stability and efficiency at scale, we adopted fault tolerance techniques from MegaScale~\citep{jiang2024megascale}, including self-check diagnostics, multi-level monitoring, and fast restart/recovery mechanisms. These strategies effectively mitigate the impact of interruptions, enabling \ours to maintain robust performance in large-scale generative modeling tasks.

 
\subsection{Saving and Loading Training Stages}\label{sec:infra-byted-ckpt}
Checkpointing training states—such as model parameters, exponential moving average (EMA) parameters, optimizer states, and random states—is crucial for training large-scale models, particularly given the increased likelihood of cluster faults. Reloading checkpointed states ensures reproducibility, which is essential for model reliability and debugging potential issues, including those caused by unintentional errors or malicious attacks.

To support scalable large-scale training, we adopt ByteCheckpoint~\citep{wan2024bytecheckpoint} as our checkpointing solution. It not only enables parallel saving and loading of partitioned checkpoints with high I/O efficiency but also supports resharding distributed checkpoints. This flexibility allows seamless switching between different training scales, accommodating varying numbers of ranks and diverse storage backends. 
In our setup, checkpointing an 8B model across over thousands of GPUs blocks training for less than 4 seconds, which is negligible compared to the overall forward and backward computation time per iteration.

