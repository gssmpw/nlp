\section{Conclusion}
In this work, we presented \ours, a novel model for joint image-and-video generation for industry-standard performance. Through an advanced data curation process and a robust model architecture, \ours delivers high-quality outputs by ensuring both fine-grained data selection and effective integration of image and video modalities. Key components, such as the image-video joint VAE and the application of rectified flow, facilitate seamless token interaction across modalities, establishing a shared latent space that enhances model adaptability and attention across tokens. Empirical results highlight \ours's superiority in commercial-grade visual generation quality.

\subsection*{Acknowledgements}
We sincerely appreciate the support of our collaborators at ByteDance who contributed to this work. Xibin Wu, Chongxi Wang, Yina Tang, Fangzhou Ai, Yi Ren, Wei Wang, Chen Chen, Colin Young, Bobo Zeng, Ge Bai, Yi Fu, Ruoyu Guo, Prasanna Raghav, Weiguo Feng, Xugang Ye, Adithya Sampath, Aaron Shen, Da Tang, Yuan Fang, Qijun Gan, Chen Zhang, Zhenhui Ye, Pan Xie, Houmin Wei, Gaohong Liu, Zherui Liu, Chenyuan Wang, Yun Zhang, Kaihua Jiang, Zhuo Jiang, Yang Bai, Weiqiang Lou, Hongkai Li, Xi Yang, Shuguang Wang, Junru Zheng, Zuquan Song, Zixian Du, Jingzhe Tang, Yongqiang Zhang, Mingji Han, Heng Zhang, Li Han,  Sophie Xie, Shuo Li, Xinzhi Yao, Peng Li, Lianke Qin, Dongyang Wang, Yang Cheng, Chundian Liu, Wenhao Hao, Haibin Lin, Xin Liu 
