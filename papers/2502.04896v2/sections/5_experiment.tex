\section{Experiments}

\subsection{Text-to-Image Results}


\begin{table}[t]
    \centering
    \tablestyle{1.5pt}{1.12}
        \begin{tabular}{l|c|c|ccc|c}
            \toprule
            \multirow{2}{*}{Method}&   & \multicolumn{1}{c|}{GenEval}  &  \multicolumn{3}{c|}{T2I-CompBench}  &  \multicolumn{1}{c}{DPG-Bench}  \\
             & Text Enc. & Overall &  Color  & Shape & Texture &  Average   \\
            \midrule
            SDv1.5~\citep{rombach2022high}       &  CLIP ViT-L/14     & 0.43 & 0.3730 & 0.3646 & 0.4219 & 63.18   \\
            DALL-E 2~\citep{ramesh2022hierarchical} & CLIP ViT-H/16  & 0.52 & 0.5750 & 0.5464 & 0.6374 & -   \\
            SDv2.1~\citep{rombach2022high}          &  CLIP ViT-H/14  & 0.50 & 0.5694 & 0.4495 &  0.4982 & -   \\
            SDX~\citep{podell2023sdxl}             &  CLIP ViT-bigG  & 0.55 & 0.6369 & 0.5408 & 0.5637 & 74.65 \\
            PixArt-$\alpha$~\citep{chen2023pixart} &  Flan-T5-XXL  & 0.48 & 0.6886 & 0.5582 & 0.7044 & 71.11   \\
            DALL-E 3~\citep{betker2023improving}       & Flan-T5-XXL   &~~0.67$^\dagger$  & 0.8110$^\dagger$ & 0.6750$^\dagger$ & 0.8070$^\dagger$ & 83.50$^\dagger$    \\ 
            GenTron~\citep{chen2024gentron} & CLIP T5XXL &  -  & 0.7674 & 0.5700 & 0.7150  & - \\
            SD3~\citep{esser2024scaling} &  Flan-T5-XXL & 0.74 & - & - & - & -  \\
            Show-o~\citep{xie2024show}      & Phi-1.5   & 0.53 & - & - & - & - \\
            Transfusion~\citep{zhou2024transfusion}  & -  & 0.63 & - & - & - &  - \\
            Chameleon~\citep{lu2024chameleon}   & -  & 0.39 & - & - & - & - \\
            LlamaGen~\citep{sun2024autoregressive}   & FLAN-T5 XL  & 0.32  & - & - & - & - \\
            Emu 3~\citep{wang2024emu3}             & -    &~~0.66$^\dagger$ & 0.7913$^\dagger$ & 0.5846$^\dagger$ & 0.7422$^\dagger$ & 80.60  \\
            \midrule
            \ours-T2I (2B)  & \multirow{2}{*}{FLAN-T5 XL}   & 0.70 & 0.7521 & 0.4832 & 0.6691 &  \multirow{2}{*}{83.65}  \\
            \ours-T2I (2B)$^\dagger$   &  & 0.76$^\dagger$ & 0.7561$^\dagger$ & 0.5759$^\dagger$ & 0.7071$^\dagger$ &    \\
            \bottomrule
        \end{tabular}
    \caption{\textbf{Comparison with state-of-the-art models on image generation benchmarks.} We evaluate on GenEval~\citep{ghosh2024geneval}; T2I-CompBench~\citep{huang2023t2i-compbench} and DPG-Bench~\citep{hu2024ella_dbgbench}. Following ~\citep{wang2024emu3}, we use $^\dagger$ to indicate the result with prompt rewriting.}\label{tab:text2image_evaluation}
\end{table}


 we conduct a comprehensive quantitative evaluation of \ours-T2I on widely recognized image generation benchmarks, including GenEval~\citep{ghosh2024geneval}, T2I-CompBench~\citep{huang2023t2i-compbench}, and DPG-Bench~\citep{hu2024ella_dbgbench}. Details of these benchmarks could be found in \Cref{appendix:bench-config}. The results are summarized in \Cref{tab:text2image_evaluation}.


\paragraph{Performance on GenEval.} To assess text-image alignment comprehensively, we employ the GenEval benchmark, which evaluates the correspondence between textual descriptions and visual content. Since \ours-T2I is primarily trained on dense generative captions, it exhibits a natural advantage when handling detailed prompts. To further explore this, we expand the original short prompts in GenEval with ChatGPT-4o, preserving their semantics while enhancing descriptive detail. As shown in \Cref{tab:text2image_evaluation}, \ours-T2I achieves strong performance with the original short prompts, surpassing most state-of-the-art models. With the rewritten prompts, \ours-T2I attains the highest score (0.76), demonstrating its exceptional capability in aligning detailed textual descriptions with generated images.

\paragraph{Performance on T2I-CompBench.} We further evaluate the alignment between generated images and textual conditions using the T2I-CompBench benchmark, which focuses on various object attributes such as color, shape, and texture. As illustrated in \Cref{tab:text2image_evaluation}, \ours-T2I consistently outperforms several strong baselines, including PixArt-$\alpha$~\citep{chen2023pixart}, SDXL~\citep{podell2023sdxl}, and DALL-E 2~\citep{mishkin2022dall}. Notably, the inclusion of prompt rewriting leads to improved performance across all attributes, further highlighting \ours-T2I's robustness in text-image alignment.

\paragraph{Performance on DPG-Bench.} While the aforementioned benchmarks primarily evaluate text-image alignment with short prompts, DPG-Bench is designed to test model performance on dense prompt following. This challenging benchmark includes 1,000 detailed prompts, providing a rigorous test of a model's ability to generate visually accurate outputs for complex textual inputs. As shown in the last column of \Cref{tab:text2image_evaluation}, \ours-T2I achieves the highest performance with an average score of 83.65, surpassing PixArt-$\alpha$~\citep{chen2023pixart} (71.11), DALL-E 3~\citep{betker2023improving} (83.50), and EMU3~\citep{wang2024emu3} (80.60). These results highlight \ours-T2I's superior ability to handle dense prompts and maintain high fidelity in text-image alignment.

\subsection{Text-to-Video Results}


\begin{table}[t]
\tablestyle{8pt}{1.12}
\centering
    \begin{tabular}{l|c c c}
    \toprule
    Method & Resolution & FVD ($\downarrow$) & IS ($\uparrow$ )\\
    \hline
    CogVideo (Chinese)~\citep{hong2022cogvideo} & 480$\times$480 & 751.34 & 23.55 \\
    CogVideo (English)~\citep{hong2022cogvideo} & 480$\times$480 & 701.59 & 25.27 \\
    Make-A-Video~\citep{singer2023makeavideo} &  256$\times$256 & 367.23 & 33.00 \\
    VideoLDM~\citep{blattmann2023align} & - &  550.61 & 33.45 \\
    LVDM~\citep{he2022latent}           & 256$\times$256 & 372.00 & - \\
    MagicVideo~\citep{zhou2022magicvideo} & - &  655.00 & - \\ 
    PixelDance~\citep{zeng2024make}     & -  & 242.82 & 42.10 \\
    PYOCO~\citep{ge2023preserve}        & - & 355.19 & 47.76 \\
    Emu-Video~\citep{girdhar2023emu}    & 256$\times$256 & 317.10 & 42.7 \\
    SVD~\citep{blattmann2023stable}     & 240$\times$360 & 242.02 & - \\
    \hline
    \ours-2B (ours) & 256$\times$256 & 246.17 & 45.77 $\pm$ 1.10 \\
    \ours-2B (ours) & 240$\times$360 & 254.47 & 46.64 $\pm$ 1.08 \\
    \ours-2B (ours) & 128$\times$128 & 217.24 & 42.30 $\pm$ 1.03 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Zero-shot text-to-video performance on UCF-101}. We generate videos of different resolutions, including 256$\times$256, 240$\times$360, 128$\times$128, for comprehensive comparisons.}\label{tab:ucf101-fvd}
\end{table}

\paragraph{Performance on UCF-101.} 
We conduct experiments on UCF-101~\citep{ucf101} using zero-shot text-to-video setting. As UCF-101 only has \emph{class} labels, we utilize an video-language model, Tarsier-34B~\citep{wang2024tarsier}, to generate detailed captions for all UCF-101 videos. These captions are then used to synthesize videos with \ours. Finally, we generated 13,320 videos at different resolutions with \ours-2B model for evaluation, including  256$\times$256, 240$\times$360 and 128$\times$128. Following standard practice~\citep{skorokhodov2022stylegan}, we use the I3D model, pre-trained on Kinetics-400~\citep{carreira2017quo}, as the feature extractor. Based on the extracted features, we calculated Fr\'echet Video Distance (FVD)~\citep{unterthiner2018towards} to evaluate the fidelity of the generated videos. The results in \Cref{tab:ucf101-fvd} demonstrate that \ours consistently generates videos with lower FVD and higher IS. For instance, at a resolution of 128$\times$128, the FVD of videos generated by \ours is 217.24, achieving state-of-the-art performance and highlighting significant advantages over other methods.



\paragraph{Performance on VBench.}

As presented in \Cref{tab:vbench-mini}, we evaluate \ours-T2V against state-of-the-art models on VBench~\citep{huang2024vbench}, a comprehensive benchmark designed to assess video generation quality across 16 dimensions. \ours-T2V achieves state-of-the-art overall performance on VBench, showcasing its ability to generate high-quality videos across diverse attributes and scenarios.

Among the key metrics, \ours-T2V demonstrates notable strength in human action representation, dynamic degree, and multiple object generation, reflecting its capacity for handling complex and diverse video content. Additionally, it achieves competitive results in appearance style, quality score, and semantic alignment, highlighting its balanced performance across multiple aspects.


\begin{table}[t]
    \tablestyle{3.5pt}{1.12}
    \centering
    \begin{tabular}{lcccccccc}
   \toprule
        \multirow{2}{*}{Models} & Human  & \multirow{2}{*}{Scene} &  Dynamic  & Multiple  & Appear.  & Quality & Semantic & \multirow{2}{*}{\textbf{Overall}}  \\ 
         & Action &  & Degree & Objects & Style  & Score & Score &  \\
        \midrule
\href{https://github.com/guoyww/AnimateDiff}{AnimateDiff-V2} & 92.60 & 50.19 & 40.83 & 36.88 & 22.42 & 82.90 & 69.75 & 80.27 \\
\href{https://github.com/AILab-CVC/VideoCrafter}{VideoCrafter-2.0} & 95.00 & 55.29 & 42.50 & 40.66 & \textbf{25.13} & 82.20 & 73.42 & 80.44 \\
\href{https://huggingface.co/hpcai-tech/OpenSora-STDiT-v3}{OpenSora V1.2} & 85.80 & 42.47 & 47.22 & 58.41 & 23.89 & 80.71 & 73.30 & 79.23 \\
\href{https://github.com/showlab/Show-1}{Show-1} & 95.60 & 47.03 & 44.44 & 45.47 & 23.06 & 80.42 & 72.98 & 78.93 \\
\href{https://runwayml.com/research/introducing-gen-3-alpha}{Gen-3} & 96.40 & 54.57 & 60.14 & 53.64 & 24.31 & 84.11 & 75.17 & 82.32 \\
\href{https://pika.art}{Pika-1.0} & 86.20 & 49.83 & 47.50 & 43.08 & 22.26 & 82.92 & 71.77 & 80.69 \\
\href{https://github.com/THUDM/CogVideo}{CogVideoX-5B} & 99.40 & 53.20 & 70.97 & 62.11 & 24.91 & 82.75 & 77.04 & 81.61 \\
\href{https://klingai.kuaishou.com/}{Kling} & 93.40 & 50.86 & 46.94 & 68.05 & 19.62 & 83.39 & 75.68 & 81.85 \\
\href{https://github.com/mira-space/Mira}{Mira} & 63.80 & 16.34 & 60.33 & 12.52 & 21.89 & 78.78 & 44.21 & 71.87 \\
\href{https://causvid.github.io/}{CausVid} & \textbf{99.80} & 56.58 & \textbf{92.69} & 72.15 & 24.27 & \textbf{85.65} & 78.75 & 84.27 \\
\href{https://lumalabs.ai/dream-machine}{Luma} & 96.40 & \textbf{58.98} & 44.26 & \textbf{82.63} & 24.66 & 83.47 & \textbf{84.17} & 83.61 \\
\href{https://github.com/Tencent/HunyuanVideo}{HunyuanVideo} & 94.40 & 53.88 & 70.83 & 68.55 & 19.80 & 85.09 & 75.82 & 83.24 \\
\midrule
\textbf{Goku} (ours) & 97.60 & 57.08 & 76.11 & 79.48 & 23.08 & 85.60 & 81.87 & \textbf{84.85} \\
\bottomrule
    \end{tabular}
    \caption{\textbf{Comparison with leading T2V models on VBench.} \ours achieves state-of-the-art overall performance. Detailed results across all 16 evaluation dimensions are provided in \Cref{tab:vbench-full} in the Appendix.}\label{tab:vbench-mini}
\end{table}

For detailed results on all 16 evaluation dimensions, we refer readers to \Cref{tab:vbench-full} in the Appendix. This comprehensive analysis underscores \ours-T2V's superiority in video generation compared to prior approaches.



\subsection{Image-to-Video}

\begin{figure}[ht]
\centering
\includegraphics[width=0.96\linewidth]{figures/i2v_samples.pdf} \\
\caption{\textbf{Samples of \ours-I2V.} Reference images are presented in the leftmost columns. We omitted redundant information from the long prompts, displaying only the key details in each one. Key words are highlighted in \textcolor{red}{\textbf{RED}}.}\label{fig:i2v}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/scaling_ablation.pdf} 
        \caption{Model Scaling}
        \label{fig:ablation_1}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\linewidth}
        \centering
    \includegraphics[width=0.7\linewidth]{figures/joint_ablation.pdf} 
        \caption{Joint Training}
        \label{fig:ablation_2}
    \end{subfigure}
    \caption{\textbf{Ablation Studies of Model Scaling and Joint Training.} Fig. (a) shows the comparison between \ours-T2V(2B) and \ours-T2V(8B). Fig. (b) shows the comparison  between whether joint training is adopted or not. }
    \label{fig:video-tag}
\end{figure}

We finetune \ours-I2V from the T2V initialization with approximate 4.5M text-image-video triplets, sourced from diverse domains to ensure robust generalization. Despite the relatively small number of fine-tuning steps (10k), our model demonstrates remarkable efficiency in animating reference image while maintaining strong alignment with the accompanying text. As illustrated in \Cref{fig:i2v}, the generated videos exhibit high visual quality and temporal coherence, effectively capturing the semantic nuances described in the text. 

\begin{figure}[t]
\centering
\includegraphics[width=0.96\textwidth]{figures/video_comparisons.pdf} \\
\caption{\textbf{Qualitative comparisons with state-of-the-art (SoTA) video generation models.} This figure showcases comparisons with leading models, including~\citep{yang2024cogvideox}, Open-Sora Plan~\citep{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109}, Pika~\citep{pika}, DreamMachine~\citep{DreamMachine}, Vidu~\citep{bao2024vidu}, and Kling v1.5~\citep{kuaishou2024klingai}.}
\label{fig:video-sota}
\end{figure}

\subsection{Image and Video Qualitative Visualizations}\label{sec:overall-qualitative-results}


For intuitive comparisons, we conduct qualitative assessments and present sampled results in \Cref{fig:video-sota}. The evaluation includes open-source models, such as CogVideoX~\citep{yang2024cogvideox} and Open-Sora-Plan~\citep{opensora}, alongside closed-source commercial products, including DreamMachine~\citep{DreamMachine}, Pika~\citep{pika}, Vidu~\citep{bao2024vidu}, and Kling~\citep{kuaishou2024klingai}. The results reveal that some commercial models struggle to generate critical video elements when handling complex prompts. For instance, models like Pika, DreamMachine, and Vidu (rows 3–5) fail to render the skimming drone over water. While certain models succeed in generating the target drone, they often produce distorted subjects (rows 1–2) or static frames lacking motion consistency (row 6). In contrast, \ours-T2V (8B) demonstrates superior performance by accurately incorporating all details from the prompt, creating a coherent visual output with smooth motion. Additional comparisons are provided in the appendix for a more comprehensive evaluation. Furthermore, more video examples are available at \href{https://saiyan-world.github.io/goku/}{the goku homepage}.

\subsection{Ablation Studies}

\paragraph{Model Scaling.} We compared \ours-T2V models with 2B and 8B parameters. Results in \cref{fig:ablation_1} indicate that model scaling helps mitigate the generation of distorted object structures, such as the arm in \cref{fig:ablation_1} (row 1) and the wheel in \cref{fig:ablation_1} (row 2). This aligns with findings observed in large multi-modality models.





\paragraph{Joint Training.} We further examine the impact of joint image-and-video training. Starting from the same pretrained \ours-T2I (8B) weights, we fine-tuned \ours-T2V (8B) on 480p videos for an equal number of training steps, with and without joint image-and-video training. As shown in \cref{fig:ablation_2}, \ours-T2V without joint training tends to generate low-quality video frames, while the model with joint training more consistently produces photorealistic frames.


