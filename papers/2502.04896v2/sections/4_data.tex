\section{Data Curation Pipeline}

We unblock the data volume that is utilized for industry-grade video/image generation models. Our data curation pipeline, illustrated in \Cref{fig:data-pipeline}, consists of five main stages: (1) image and video collection, (2) video extraction and clipping, (3) image and video filtering, (4) captioning, and (5) data distribution balancing. We describe the details of data curation procedure below.

\subsection{Data Overview}
We collet raw image and video data from a variety of sources, including publicly available academic datasets, internet resources, and proprietary datasets obtained through partnerships with collaborating organizations. After rigorous filtering, the final training dataset for \ours consists of approximately 160M image-text pairs and 36M video-text pairs, encompassing both publicly available datasets and internally curated proprietary datasets. The detailed composition of these resources is outlined as follows:
\begin{itemize}[align=parleft, left=0pt, labelsep=0.5em]
    \item \textbf{Text-to-Image Data.} Our text-to-image training dataset includes 100M public samples from LAION~\citep{schuhmann2022laion} and 60M high-quality, internal samples. We use public data for pre-training and internal data for fine-tuning. 
    \item \textbf{Text-to-Video Data. } Our T2V training dataset includes 11M public clips and 25M in-house clips. The former include Panda-70M~\citep{chen2024panda}, InternVid~\citep{wang2023internvid}, OpenVid-1M~\citep{nan2024openvid}, and Pexels~\citep{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109}. Rather than directly using these datasets, we apply a data curation pipeline to keep high-quality samples. 
\end{itemize}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/datapipe.pdf}
    \caption{\textbf{The data curation pipeline in \ours.} Given a large volume of video/image data collected from Internet, we generate high-quality video/image-text pairs through a series of data filtering, captioning and balancing steps.}
    \label{fig:data-pipeline}
\end{figure}


\subsection{Data Processing and Filtering}
To construct a high-quality video dataset, we implement a comprehensive processing pipeline comprising several key stages. Raw videos are first preprocessed and standardized to address inconsistencies in encoding formats, durations, and frame rates. Next, a two-stage video clipping method segments videos into meaningful and diverse clips of consistent length. Additional filtering processes are applied, including visual aesthetic filtering to retain photorealistic and visually rich clips, OCR filtering to exclude videos with excessive text, and motion filtering to ensure balanced motion dynamics. In addition, the multi-level training data is segmented based on resolution and corresponding filtering thresholds for DINO similarity, aesthetic score, OCR text coverage, and motion score, as summarized in \Cref{tab:multi_stage_data}. We provide the details of each processing step as follows.



\Cref{tab:video-basic-parameters} presents the key parameters and their corresponding thresholds used for video quality assessment. Each parameter is essential in ensuring the generation and evaluation of high-quality videos. The Duration parameter specifies that raw video lengths should be at least 4 seconds to capture meaningful temporal dynamics. The Resolution criterion ensures that the minimum dimension (either height or width) of the video is no less than 480 pixels, maintaining adequate visual clarity. The Bitrate, which determines the amount of data processed per second during playback, requires a minimum of 500 kbps to ensure sufficient quality, clarity, and manageable file size. Videos with low bitrate typically correspond to content with low complexity, such as static videos or those featuring pure color backgrounds. Finally, the Frame Rate enforces a standard of at least 24 frames per second (film standard) or 23.976 frames per second (NTSC standard) to guarantee smooth motion and prevent visual artifacts. These thresholds collectively establish a baseline for evaluating and generating high-quality video content.

\begin{table}[t]
    \centering
    \begin{tabular}{m{0.15\linewidth} m{0.4\linewidth} m{0.35\linewidth}}
        \toprule
        Parameter & Description & Threshold  \\ \midrule
        Duration & Raw video length & $\geq$ 4 seconds  \\ \midrule
        Resolution & Width and height of the video & $min$\{ height, width\} $\geq$ 480 \\ \midrule
        Bitrate & Amount of data processed per second during playback, which impacts the video's quality, clarity, and file size & $\geq$ 500 kbps \\ \midrule
        Frame Rate & Frames displayed per second & $\geq$ 24 FPS (Film Standard) / 23.976 FPS (NTSC Standard) \\ 
        \bottomrule
        % Color Depth & Bits per color channel & $\geq$ 8-bit \\ \midrule
        % Color Space & Organization and representation of colors &  Rec. 709 (Standard), Rec. 2020 (Wide Color Gamut) \\ \hline
        \
    \end{tabular}
    \caption{\textbf{Summary of video quality parameters and their thresholds for preprocessing.} The table outlines the criteria used to filter and standardize raw videos based on essential attributes, ensuring uniformity and compatibility in the dataset.}\label{tab:video-basic-parameters}
\end{table}



\begin{itemize}[align=parleft, left=0pt, labelsep=0.5em]
    \item \textbf{Preprocessing and Standardization of Raw Videos.} Videos collected from the internet often require extensive preprocessing to address variations in encoding formats, durations, and frame rates. Initially, we perform a primary filtering step based on fundamental video attributes such as duration, resolution, bitrate. The specific filtering criteria and corresponding thresholds are detailed in \Cref{tab:video-basic-parameters}. This initial filtering step is computationally efficient compared to more advanced, model-based filtering approaches, such as aesthetic~\citep{schuhmann2022laion} evaluation models. Following this stage, the raw videos are standardized to a consistent coding format, H.264~\citep{wiegand2003overview}, ensuring uniformity across the dataset and facilitating subsequent processing stages.
    
    \item \textbf{Video Clips Extraction.}
     We employ a two-stage video clipping method for this stage. First, we use PySceneDetect~\citep{Castellano_PySceneDetect} for shot boundary detection, resulting coarse-grained video clips from raw videos. Next, we further refine the video clips by sampling one frame per second, generating DINOv2~\citep{oquab2023dinov2} features and calculating cosine similarity between adjacent frames. When similarity falls below a set threshold, we mark a shot change and further divide the clip. Specifically, as shown in \Cref{tab:multi_stage_data}, for video resolutions around $480 \times 864$, we segmented the video clips where the DINO similarity between adjacent frames exceeds 0.85. For resolutions greater than $720 \times 1280$, the threshold is set at 0.9.
    Besides, to standardize length, we limit clips to a maximum of 10 seconds. Furthermore, we consider the similarity between different clips derived from the same source video to ensure diversity and maintain quality. Specifically, we compute the perceptual hashing~\citep{imagehash} values of keyframes from each clip and compare them. If two clips have similar hash values, indicating significant overlap, we retain the clip with a higher aesthetic score. This ensures that the final dataset includes diverse and high-quality video clips.
    
    \item \textbf{Visual Aesthetic Filtering.} To assess the visual quality of the videos, we utilize aesthetic models~\citep{schuhmann2022laion} to evaluate the keyframes. The aesthetic scores of the keyframes are averaged to obtain an overall aesthetic score for each video. For videos with resolutions around $480 \times 864$, those with an aesthetic score below 4.3 are discarded, while for resolutions exceeding $720 \times 1280$, the threshold is raised to 4.5. This filtering process ensures that the selected clips are photorealistic, visually rich, and of high aesthetic quality.
    
    \item \textbf{OCR Filtering.} To exclude videos with excessive text, we employ an internal OCR model to detect text within the keyframes. The OCR model identifies text regions, and we calculate the text coverage ratio by dividing the area of the largest bounding box detected by the total area of the keyframe. Videos with a text coverage ratio exceeding predefined thresholds are discarded. Specifically, for videos with resolutions around $480 \times 864$, the threshold is set at 0.02, while for resolutions exceeding $720 \times 1280$, the threshold is reduced to 0.01. This process effectively filters out videos with excessive text content.
    
    \item \textbf{Motion Filtering.} Unlike images, videos require additional filtering based on motion characteristics. To achieve this, we utilize RAFT~\citep{teed2020raft} to compute the mean optical flow of video clips, which is then used to derive a motion score. For videos with resolutions around $480 \times 864$, clips with motion scores below 0.3 (indicating low motion) or above 20.0 (indicating excessive motion) are excluded. For resolutions exceeding $720 \times 1280$, the thresholds are adjusted to 0.5 and 15.0, respectively. Furthermore, to enhance motion control, the motion score is appended to each caption.
\end{itemize}

\begin{table}[t]
\centering
\tablestyle{5pt}{1.12}
\begin{tabular}{ccccccc}
\toprule
Stage & Amount & Resolution & DINO-Sim.  & Aesthetic & OCR & Motion \\
\midrule
480p & 36M & $\geq$ 480$\times$864 &$\geq$0.85  &$\geq$ 4.3 & <= 0.02 &  0.3 $\leq$ score $\leq$ 20.0 \\
720p & 24M & $\geq$ 720$\times$1280  &$\geq$0.90   &$\geq$ 4.5 & <= 0.01 & 0.5 $\leq$ score $\leq$ 15.0 \\
1080p & 7M &$\geq$ 1080$\times$1920  &$\geq$0.90   &$\geq$ 4.5 & <= 0.01  & 0.5 $\leq$ score $\leq$ 8.0  \\
\bottomrule
\end{tabular}
\caption{\textbf{Overview of multi-stage training data}.This table summarizes the thresholds for each filtering criterion, including resolution, DINO similarity, aesthetic score, OCR text coverage, motion score, and the corresponding data quantities.}\label{tab:multi_stage_data}
\end{table}

\subsection{Captioning}\label{sec:captions}
Detailed captions are essential for enabling the model to generate text-aligned images/videos precisely. For images, we use InternVL2.0~\citep{chen2024far} to generate dense captions for each sample. To caption video clips, we start with InternVL2.0~\citep{chen2024far} for keyframe captions, followed by Tarsier2~\citep{yuan2025tarsier2} for video-wide captions. Note that the Tarsier2 model can inherently describe camera motion types (\eg, \emph{zoom in}, \emph{pan right}) in videos, eliminating the need for a separate prediction model and simplifying the overall pipeline compared to previous work such as \citep{polyak2024movie}.
Next, we utilize Qwen2~\citep{yang2024qwen2} to merge the keyframe and video captions. Besides, we also empirically found that adding the motion score (calculated by RAFT~\citep{teed2020raft}) to the captions improves motion control for video generation. This approach enables users to specify different motion scores in prompts to guide the model in generating videos with varied motion dynamics. 


\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/label_distribution_bar.png} 
        \caption{Semantic distribution of video clips.}
        \label{fig:data-semantic-distribution}
    \end{subfigure}
    \vspace{5pt}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/distribution_pie.pdf} 
        \caption{The balanced semantic distribution of subcategories.}\label{fig:data-balanced}
    \end{subfigure}
    \caption{\textbf{Training data distributions.} The balanced semantic distribution of primary categories and subcategories are shown in (a) and (b), respectively.}
    \label{fig:video-data-distribution}
\end{figure}

\subsection{Training Data Balancing}\label{sec:balance}
The model’s performance are significantly influenced by the data distribution, especially for video data. To balance the video training data, we first use an internal video classification model to generate semantic tags for the videos. We then adjust the data distribution based on these semantic tags to ensure a balanced representation across categories.


\begin{itemize}[align=parleft, left=0pt, labelsep=0.5em]
    \item \textbf{Data Semantic Distribution.} The video classification model assigns a semantic tag to each video based on four evenly sampled keyframes. The model categorizes videos into 9 primary classes (\eg, human, scenery, animals, food) and 86 subcategories (\eg, half-selfie, kid, dinner, wedding). \Cref{fig:data-semantic-distribution} presents the semantic distribution across our filtered training clips, with humans, scenery, food, urban life, and animals as the predominant categories.
    \item \textbf{Data Balancing.} The quality of the generated videos is closely tied to the semantic distribution of the training data. Videos involving humans pose greater modeling challenges due to the extensive diversity in appearances, whereas animals and landscapes exhibit more visual consistency and are relatively easier to model. To address this disparity, we implement a data-balancing strategy that emphasizes human-related content while ensuring equitable representation across subcategories within each primary category. Overrepresented subcategories are selectively down-sampled, whereas underrepresented ones are augmented through artificial data generation and oversampling techniques. Balanced data distribution is shown in \Cref{fig:data-balanced}.
\end{itemize}
