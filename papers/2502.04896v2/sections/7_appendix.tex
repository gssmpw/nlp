% \clearpage
% \appendix
% \setcounter{page}{1}

\clearpage
% \setcounter{page}{1}
% \maketitlesupplementary
\appendix

\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thesection}{Appendix \Alph{section}}


\section{Benchmark Configurations}\label{appendix:bench-config}

\paragraph{T2I-Compbench~\citep{huang2023t2i-compbench}}  We evaluate the alignment between the generated images and text conditions using T2I-Compbench, a comprehensive benchmark for assessing compositional text-to-image generation capabilities. Specifically, we report scores for color binding, shape binding, and texture binding. To evaluate these results, we employ the Disentangled BLIP-VQA model. For each attribute, we generate 10 images per prompt, with a total of 300 prompts in each category.


\paragraph{GenEval~\citep{ghosh2024geneval}} GenEval is an object-focused framework designed to evaluate compositional image properties, such as object co-occurrence, position, count, and color. For evaluation, we generate a total of 2,212 images across 553 prompts. The final score is reported as the average across tasks.

\paragraph{DPG-Bench~\citep{hu2024ella_dbgbench}} Compared to the aforementioned benchmarks, DPGBench offers longer prompts with more detailed information, making it effective for evaluating compositional generation in text-to-image models. For this evaluation, we generate a total of 4,260 images across 1,065 prompts, with the final score reported as the average across tasks.


\paragraph{VBench~\citep{huang2024vbench}} VBench is a benchmark suite for evaluating video generative models. It provides a structured Evaluation Dimension Suite that breaks down ``video generation quality" into precise dimensions for detailed assessment. Each dimension and content category includes a carefully crafted Prompt Suite and samples Generated Videos from various models. 



\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.82\textwidth]{figures/t2i_supp.pdf} \\
\end{tabular}
\end{center}
\caption{\textbf{Qualitative samples of \ours-T2I.} Key words are highlighted in \textcolor{red}{RED}.}
\label{fig:t2i_supp}
\end{figure}


\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=1\textwidth]{figures/t2i_supp_zoom.pdf} \\
\end{tabular}
\end{center}
\caption{\textbf{Qualitative samples of \ours-T2I.} Key words are highlighted in \textcolor{red}{RED}. For clarity, we zoom in on specific regions to enhance visualization.}
\label{fig:t2i_zoom_supp}
\end{figure}

\section{More Visualization Examples}
\subsection{\ours-T2I Samples Visualization}\label{sec:more-t2i-example}


We present more generated image samples with their text prompts in \Cref{fig:t2i_supp}.  The prompts are randomly selected from the Internet \footnote{\url{https://promptlibrary.org/}}.  \ours-T2I achieves strong performance in both visual quality and text-image alignment. It can interpret visual elements and their interactions from complex natural language descriptions. Notably, in \Cref{fig:t2i_zoom_supp}, \ours-T2I exhibits impressive abilities on generating images with rich details, for example, the clear textures of leaves and berries.


\subsection{\ours-T2V Samples Visualization}
In \Cref{fig:t2v_supp} we show more examples generated by \ours-T2V, in both \emph{landscape} (\eg, rows one through five) and \emph{portrait} mode (\eg, the last row). \ours-T2V is capable of generating high-motion videos (\eg, skiing) and realistic scenes (\eg, forests). All videos are configured with a duration of 4 seconds, a frame rate of 24 FPS, and a resolution of 720p. For visualization, we uniformly sample five frames in temporal sequence.


\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=1\textwidth]{figures/t2v_supp.pdf} \\
\end{tabular}
\end{center}
\caption{\textbf{Qualitative samples of \ours-T2V.} Key words are highlighted in \textcolor{red}{\textbf{RED}}.}
\label{fig:t2v_supp}
\end{figure}



\subsection{\ours-T2V Comparisons with Prior Arts}

\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.9\textwidth]{figures/supp_v_comparisons_1.pdf} \\
\end{tabular}
\end{center}
\caption{\textbf{Qualitative comparisons of \ours-T2V with SOTA video generation methods.} Key words are highlighted in \textcolor{red}{\textbf{RED}}.}
\label{fig:video_comparisons_supp1}
\end{figure}


\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.9\textwidth]{figures/supp_v_comparisons_2.pdf} \\
\end{tabular}
\end{center}
\caption{\textbf{Qualitative comparisons of \ours-T2V with SOTA video generation methods.} Key words are highlighted in \textcolor{red}{\textbf{RED}}.}
\label{fig:video_comparisons_supp2}
\end{figure}

Additional comparisons with state-of-the-art text-to-video generation models are presented in \Cref{fig:video_comparisons_supp1} and \Cref{fig:video_comparisons_supp2}. These results demonstrate the strong performance of \ours when evaluated against both open-source models \citep{yang2024cogvideox,opensora} and commercial products \citep{pika,kuaishou2024klingai,bao2024vidu,DreamMachine}. For instance, in \Cref{fig:video_comparisons_supp2}, \ours successfully generates smooth motion and accurately incorporates the specified low-angle shot. In contrast, other models, such as CogVideoX~\citep{yang2024cogvideox}, Vidu~\citep{bao2024vidu}, and Kling~\citep{kuaishou2024klingai}, often produce incorrect objects or improper camera views.


\begin{sidewaystable}
\centering
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1}
\begin{tabular}{l|lllllllllllllllllll}
     Method  & 
    \makecell[bc]{\rotatebox{75}{Total Score}} & 
    \makecell[bc]{\rotatebox{75}{Quality Score}} & 
    \makecell[bc]{\rotatebox{75}{Semantic Score}} & 
    \makecell[bc]{\rotatebox{75}{subject consistency}} & 
    \makecell[bc]{\rotatebox{75}{background consistency}} & 
    \makecell[bc]{\rotatebox{75}{temporal flickering}} & 
    \makecell[bc]{\rotatebox{75}{motion smoothness}} & 
    \makecell[bc]{\rotatebox{75}{dynamic degree}} & 
    \makecell[bc]{\rotatebox{75}{aesthetic quality}} & 
    \makecell[bc]{\rotatebox{75}{imaging quality}} & 
    \makecell[bc]{\rotatebox{75}{object class}} & 
    \makecell[bc]{\rotatebox{75}{multiple objects}} & 
    \makecell[bc]{\rotatebox{75}{human action}} & 
    \makecell[bc]{\rotatebox{75}{color}} & 
    \makecell[bc]{\rotatebox{75}{spatial relationship}} & 
    \makecell[bc]{\rotatebox{75}{scene}} & 
    \makecell[bc]{\rotatebox{75}{appearance style}} & 
    \makecell[bc]{\rotatebox{75}{temporal style}} & 
    \makecell[bc]{\rotatebox{75}{overall consistency}} \\
\href{https://github.com/guoyww/AnimateDiff}{AnimateDiff-V2} & 80.27 & 82.90 & 69.75 & 95.30 & 97.68 & 98.75 & 97.76 & 40.83 & 67.16 & 70.10 & 90.90 & 36.88 & 92.60 & 87.47 & 34.60 & 50.19 & 22.42 & 26.03 & 27.04 \\
\href{https://github.com/AILab-CVC/VideoCrafter}{VideoCrafter-2.0} & 80.44 & 82.20 & 73.42 & 96.85 & \textbf{98.22} & 98.41 & 97.73 & 42.50 & 63.13 & 67.22 & 92.55 & 40.66 & 95.00 & \textbf{92.92} & 35.86 & 55.29 & \textbf{25.13} & 25.84 & \textbf{28.23} \\
\href{https://huggingface.co/hpcai-tech/OpenSora-STDiT-v3}{OpenSora V1.2} & 79.23 & 80.71 & 73.30 & 94.45 & 97.90 & 99.47 & 98.20 & 47.22 & 56.18 & 60.94 & 83.37 & 58.41 & 85.80 & 87.49 & 67.51 & 42.47 & 23.89 & 24.55 & 27.07 \\
\href{https://github.com/showlab/Show-1}{Show-1} & 78.93 & 80.42 & 72.98 & 95.53 & 98.02 & 99.12 & 98.24 & 44.44 & 57.35 & 58.66 & 93.07 & 45.47 & 95.60 & 86.35 & 53.50 & 47.03 & 23.06 & 25.28 & 27.46 \\
\href{https://runwayml.com/research/introducing-gen-3-alpha}{Gen-3} & 82.32 & 84.11 & 75.17 & 97.10 & 96.62 & 98.61 & 99.23 & 60.14 & 63.34 & 66.82 & 87.81 & 53.64 & 96.40 & 80.90 & 65.09 & 54.57 & 24.31 & 24.71 & 26.69 \\
\href{https://pika.art}{Pika-1.0} & 80.69 & 82.92 & 71.77 & 96.94 & 97.36 & \textbf{99.74} & \textbf{99.50} & 47.50 & 62.04 & 61.87 & 88.72 & 43.08 & 86.20 & 90.57 & 61.03 & 49.83 & 22.26 & 24.22 & 25.94 \\
\href{https://github.com/THUDM/CogVideo}{CogVideoX-5B} & 81.61 & 82.75 & 77.04 & 96.23 & 96.52 & 98.66 & 96.92 & 70.97 & 61.98 & 62.90 & 85.23 & 62.11 & 99.40 & 82.81 & 66.35 & 53.20 & 24.91 & 25.38 & 27.59 \\
\href{https://klingai.kuaishou.com/}{Kling} & 81.85 & 83.39 & 75.68 & \textbf{98.33} & 97.60 & 99.30 & 99.40 & 46.94 & 61.21 & 65.62 & 87.24 & 68.05 & 93.40 & 89.90 & 73.03 & 50.86 & 19.62 & 24.17 & 26.42 \\
\href{https://github.com/mira-space/Mira}{Mira} & 71.87 & 78.78 & 44.21 & 96.23 & 96.92 & 98.29 & 97.54 & 60.33 & 42.51 & 60.16 & 52.06 & 12.52 & 63.80 & 42.24 & 27.83 & 16.34 & 21.89 & 18.77 & 18.72 \\
\href{https://causvid.github.io/}{CausVid} & 84.27 & \textbf{85.65} & 78.75 & 97.53 & 97.19 & 96.24 & 98.05 & \textbf{92.69} & 64.15 & 68.88 & 92.99 & 72.15 & \textbf{99.80} & 80.17 & 64.65 & 56.58 & 24.27 & 25.33 & 27.51 \\
\href{https://lumalabs.ai/dream-machine}{Luma} & 83.61 & 83.47 & \textbf{84.17} & 97.33 & 97.43 & 98.64 & 99.35 & 44.26 & 65.51 & 66.55 & \textbf{94.95} & \textbf{82.63} & 96.40 & 92.33 & 83.67 & \textbf{58.98} & 24.66 & \textbf{26.29} & 28.13 \\
\href{https://github.com/Tencent/HunyuanVideo}{HunyuanVideo} & 83.24 & 85.09 & 75.82 & 97.37 & 97.76 & 99.44 & 98.99 & 70.83 & 60.36 & 67.56 & 86.10 & 68.55 & 94.40 & 91.60 & 68.68 & 53.88 & 19.80 & 23.89 & 26.44 \\
\href{here}{Goku} & \textbf{84.85} & 85.60 & 81.87 & 95.55 & 96.67 & 97.71 & 98.50 & 76.11 & \textbf{67.22} & \textbf{71.29} & 94.40 & 79.48 & 97.60 & 83.81 & \textbf{85.72} & 57.08 & 23.08 & 25.64 & 27.35 \\


\bottomrule 
\end{tabular}
\caption{\textbf{Comparison with state-of-the-art models on video generation benchmarks.} We evaluate on VBench~\citep{huang2024vbench} and compare with Gen-3~\citep{runway2023gen2}, Vchitect-2.0~\citep{Vchitect2}, VEnhancer~\citep{he2024venhancer}, Kling~\citep{kuaishou2024klingai}, LaVie-2~\citep{wang2023lavie}, CogVideoX~\citep{yang2024cogvideox}, Emu3~\citep{wang2024emu3}.}
\label{tab:vbench-full}
\end{sidewaystable}

\subsection{\ours-I2V Samples Visualization}


\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.9\textwidth]{figures/i2v_supp.pdf} \\
\end{tabular}
\end{center}
\caption{\textbf{Qualitative samples of \ours-I2V.} Key words are highlighted in \textcolor{red}{\textbf{RED}}.}
\label{fig:i2v_supp}
\end{figure}


We present additional visualization of generated samples from \ours-I2V in \Cref{fig:i2v_supp}, which further validate the effectiveness and versatility of our approach. As shown in the figure, \ours-I2V demonstrates an impressive ability to synthesize coherent and visually compelling videos from diverse reference images, maintaining consistency in motion and scene semantics.

For instance, in the first row, the model successfully captures the dynamic and high-energy nature of water boxing, generating fluid and natural movements of splashes synchronized with the subject’s motions. In the second row, the sequence of a child riding a bike through a park illustrates the model’s proficiency in creating smooth and realistic forward motion while preserving environmental consistency. Finally, the third row showcases the model’s ability to handle creative and imaginative scenarios, as seen in the detailed depiction of pirate ships battling atop a swirling coffee cup. The photorealistic rendering and accurate motion trajectories underscore the model’s robustness in both realism and creativity.

These examples highlight \ours-I2V’s capacity to generalize across a wide range of inputs, reinforcing its potential for applications in video generation tasks requiring high fidelity and adaptability.
