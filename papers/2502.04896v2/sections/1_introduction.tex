\section{Introduction}

Video generation has garnered significant attention owing to its transformative potential across a wide range of applications, such media content creation~\citep{polyak2024movie}, advertising~\citep{zhang2024virbo,bacher2021advert}, video games~\citep{yang2024playable,valevski2024diffusion, oasis2024}, and world model simulators~\citep{ha2018world, videoworldsimulators2024, agarwal2025cosmos}. Benefiting from advanced generative algorithms~\citep{goodfellow2014generative, ho2020denoising, liu2023flow, lipman2023flow}, scalable model architectures~\citep{vaswani2017attention, peebles2023scalable}, vast amounts of internet-sourced data~\citep{chen2024panda, nan2024openvid, ju2024miradata}, and ongoing expansion of computing capabilities~\citep{nvidia2022h100, nvidia2023dgxgh200, nvidia2024h200nvl}, remarkable advancements have been achieved in the field of video generation~\citep{ho2022video, ho2022imagen, singer2023makeavideo, blattmann2023align, videoworldsimulators2024, kuaishou2024klingai, yang2024cogvideox, jin2024pyramidal, polyak2024movie, kong2024hunyuanvideo, ji2024prompt}.


In this work, we present \textbf{\ours}, a family of rectified flow~\citep{lipman2023flow, liu2023flow} transformer models designed for joint image and video generation, establishing a pathway toward industry-grade performance. This report centers on four key components: data curation, model architecture design, flow formulation, and training infrastructure optimizationâ€”each rigorously refined to meet the demands of high-quality, large-scale video generation.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.82\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/t2i_1024.pdf}
        \caption{Text-to-Image Samples}\label{fig:main-demo-t2i}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.82\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/t2v_samples.pdf}
        \caption{Text-to-Video Samples}\label{fig:main-demo-t2v}
    \end{subfigure}
\caption{\textbf{Generated samples from \ours.} Key components are highlighted in \textcolor{red}{\textbf{RED}}.}\label{fig:main-demo}
\end{figure}


First, we present a comprehensive data processing pipeline designed to construct large-scale, high-quality image and video-text datasets. The pipeline integrates multiple advanced techniques, including video and image filtering based on aesthetic scores, OCR-driven content analysis, and subjective evaluations, to ensure exceptional visual and contextual quality. Furthermore, we employ multimodal large language models~(MLLMs)~\citep{yuan2025tarsier2} to generate dense and contextually aligned captions, which are subsequently refined using an additional large language model~(LLM)~\citep{yang2024qwen2} to enhance their accuracy, fluency, and descriptive richness. As a result, we have curated a robust training dataset comprising approximately 36M video-text pairs and 160M image-text pairs, which are proven sufficient for training industry-level generative models.

Secondly, we take a pioneering step by applying rectified flow formulation~\citep{lipman2023flow} for joint image and video generation, implemented through the \ours model family, which comprises Transformer architectures with 2B and 8B parameters. At its core, the \ours framework employs a 3D joint image-video variational autoencoder (VAE) to compress image and video inputs into a shared latent space, facilitating unified representation. This shared latent space is coupled with a full-attention~\citep{vaswani2017attention} mechanism, enabling seamless joint training of image and video. This architecture delivers high-quality, coherent outputs across both images and videos, establishing a unified framework for visual generation tasks.


Furthermore, to support the training of \ours at scale, we have developed a robust infrastructure tailored for large-scale model training. Our approach incorporates advanced parallelism strategies~\citep{jacobs2023deepspeed, pytorch_fsdp} to manage memory efficiently during long-context training. Additionally, we employ ByteCheckpoint~\citep{wan2024bytecheckpoint} for high-performance checkpointing and integrate fault-tolerant mechanisms from MegaScale~\citep{jiang2024megascale} to ensure stability and scalability across large GPU clusters. These optimizations enable \ours to handle the computational and data challenges of generative modeling with exceptional efficiency and reliability.


We evaluate \ours on both text-to-image and text-to-video benchmarks to highlight its competitive advantages. For text-to-image generation, \ours-T2I demonstrates strong performance across multiple benchmarks, including T2I-CompBench~\citep{huang2023t2i-compbench}, GenEval~\citep{ghosh2024geneval}, and DPG-Bench~\citep{hu2024ella_dbgbench}, excelling in both visual quality and text-image alignment. In text-to-video benchmarks, \ours-T2V achieves state-of-the-art performance on the UCF-101~\citep{ucf101} zero-shot generation task. Additionally, \ours-T2V attains an impressive score of \textbf{84.85} on VBench~\citep{huang2024vbench}, securing the top position on the leaderboard (as of 2025-01-25) and surpassing several leading commercial text-to-video models. Qualitative results, illustrated in \Cref{fig:main-demo}, further demonstrate the superior quality of the generated media samples. These findings underscore \ours's effectiveness in multi-modal generation and its potential as a high-performing solution for both research and commercial applications.