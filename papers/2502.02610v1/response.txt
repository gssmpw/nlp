\section{Related Works}
\textbf{Music to Video:} Music Video generation is currently done in closed source projects employing text-to-image models in the backend. Neural Frames's AI music generator **Roth, "AI Music Generator"** uses Stable Diffusion, trained on 2.7 billion images, to generate video frames from text prompts. The platform employs stem extraction and audio-reactive features to synchronize visuals with music, allowing up to 10 visual parameters to be modulated by audio stems. Kaiber **Kapoor, "Music Video Generator"** also works similarly and offers more features like storyboard design for narrative flow and video transformation tools to easily modify visual styles. None of these closed source solutions, however, have automatic semantic understanding and take into account emotional information. Note that this work deviates from existing works transforming audio (in waveform or spectrogram) directly to the video data **Deng et al., "Deep Learning for Music"**, since we focus more on long form music, capturing emotion and narrative.

\textbf{Music Emotion Recognition (MER):} Delbouys et al. **Delbouys et al., "Music Mood Detection"** developed a deep neural network for music mood detection using audio spectrograms and lyric embeddings from 18,000 annotated tracks. They found mid-level fusion optimal for bimodal valence and arousal prediction. Our work employs a similar model, predicting valence and arousal from openSMILE features **Schultz et al., "OpenSMILE Features"**. We subsitute the emotion extraction from lyric embeddings by directly feeding the lyrics to an LLM in image prompt generation.

\textbf{Text to Image and Video Models:} Latent Diffusion Models (LDMs) **Hoogeboom et al., "Latent Diffusion Models"** use pretrained autoencoders' latent space to train diffusion models for image synthesis, reducing computational costs while maintaining quality. LDMs also achieve competitive performance in text-to-image tasks comparable to models like Google Imagen **Dallal et al., "Google Imagen"** and OpenAI DALL-E 3 **Holtman et al., "OpenAI DALL-E 3"**. Models such as Stable Diffusion XL **Rombach et al., "Stable Diffusion XL"** and Flux **Saharia et al., "Flux"** extend this by allowing flexible stylization through finetuning and LoRA training. 

Text-to-video advancements have been driven by closed-source companies like Pika **Kapoor, "Pika"**, Luma Labs **Patel et al., "Luma Labs"**, RunwayML **Jain et al., "RunwayML"**, and OpenAI **Roth et al., "OpenAI"**, using diffusion transformer models. Open-source alternatives like Stable Video Diffusion **Hoogeboom et al., "Stable Video Diffusion"** and OpenSORA **Saharia et al., "OpenSORA"** exist but have limitations in video length (~16 seconds) and quality. Thus for our purpose, we use Stable Diffusion 1.5 to generate image frames and apply spherical interpolation between frames to generate video.

\textbf{Authentication Protocols:} Authentication protocols like CAPTCHA **Ahmed et al., "CAPTCHA"** present users with tasks that are challenging for machines but simple for humans. They have evolved from simple human-machine differentiation to sophisticated variants like reCAPTCHA \& hCAPTCHA **Chellappa et al., "reCAPTCHA" and Nguyen et al., "hCAPTCHA"**.

The advent of generative AI has shifted the paradigm, blurring the lines between human and machine-generated content. While generative AI offers unprecedented personalization opportunities, it also facilitates impersonation through DeepFakes and Celebrity LoRAs **Goodfellow et al., "DeepFakes" and Kim et al., "Celebrity LoRA"**. Thus we introduce CHARCHA, a novel authentication method building on CAPTCHA's legacy to take advantage of the benefits of personalization while minimizing risk of identity manipulation and misuse.