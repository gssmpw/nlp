\section{Related Works}
\textbf{Music to Video:} Music Video generation is currently done in closed source projects employing text-to-image models in the backend. Neural Frames's AI music generator \cite{neuralframes2023} uses Stable Diffusion, trained on 2.7 billion images, to generate video frames from text prompts. The platform employs stem extraction and audio-reactive features to synchronize visuals with music, allowing up to 10 visual parameters to be modulated by audio stems. Kaiber \cite{kaiber2023} also works similarly and offers more features like storyboard design for narrative flow and video transformation tools to easily modify visual styles. None of these closed source solutions, however, have automatic semantic understanding and take into account emotional information. Note that this work deviates from existing works transforming audio (in waveform or spectrogram) directly to the video data \cite{av1, av2, av3}, since we focus more on long form music, capturing emotion and narrative.

\textbf{Music Emotion Recognition (MER):} Delbouys et al. \cite{delbouys2018music} developed a deep neural network for music mood detection using audio spectrograms and lyric embeddings from 18,000 annotated tracks. They found mid-level fusion optimal for bimodal valence and arousal prediction. Our work employs a similar model, predicting valence and arousal from openSMILE features \cite{opensmile}. We subsitute the emotion extraction from lyric embeddings by directly feeding the lyrics to an LLM in image prompt generation.

\textbf{Text to Image and Video Models:} Latent Diffusion Models (LDMs) \cite{stablediff} use pretrained autoencoders' latent space to train diffusion models for image synthesis, reducing computational costs while maintaining quality. LDMs also achieve competitive performance in text-to-image tasks comparable to models like Google Imagen \cite{imagen} and OpenAI DALL-E 3 \cite{dalle}. Models such as Stable Diffusion XL \cite{SDXL} and Flux \cite{flux} extend this by allowing flexible stylization through finetuning and LoRA training. 

Text-to-video advancements have been driven by closed-source companies like Pika \cite{pika}, Luma Labs \cite{dreammachine}, RunwayML \cite{gen3alpha}, and OpenAI \cite{sora}, using diffusion transformer models \cite{dit}. Open-source alternatives like Stable Video Diffusion \cite{svd} and OpenSORA \cite{opensora} exist but have limitations in video length (~16 seconds) and quality \cite{opensora}. Thus for our purpose, we use Stable Diffusion 1.5 to generate image frames and apply spherical interpolation between frames to generate video.

\textbf{Authentication Protocols:} Authentication protocols like CAPTCHA \cite{captcha} present users with tasks that are challenging for machines but simple for humans. They have evolved from simple human-machine differentiation to sophisticated variants like reCAPTCHA \& hCAPTCHA \cite{captcha, recaptcha}. 

The advent of generative AI has shifted the paradigm, blurring the lines between human and machine-generated content. While generative AI offers unprecedented personalization opportunities, it also facilitates impersonation through DeepFakes and Celebrity LoRAs \cite{deepfake, celeblora}. Thus we introduce CHARCHA, a novel authentication method building on CAPTCHA's legacy to take advantage of the benefits of personalization while minimizing risk of identity manipulation and misuse.