\section{Related Works}
\textbf{Music to Video:} Music Video generation is currently done in closed source projects employing text-to-image models in the backend. Neural Frames's AI music generator ____ uses Stable Diffusion, trained on 2.7 billion images, to generate video frames from text prompts. The platform employs stem extraction and audio-reactive features to synchronize visuals with music, allowing up to 10 visual parameters to be modulated by audio stems. Kaiber ____ also works similarly and offers more features like storyboard design for narrative flow and video transformation tools to easily modify visual styles. None of these closed source solutions, however, have automatic semantic understanding and take into account emotional information. Note that this work deviates from existing works transforming audio (in waveform or spectrogram) directly to the video data ____, since we focus more on long form music, capturing emotion and narrative.

\textbf{Music Emotion Recognition (MER):} Delbouys et al. ____ developed a deep neural network for music mood detection using audio spectrograms and lyric embeddings from 18,000 annotated tracks. They found mid-level fusion optimal for bimodal valence and arousal prediction. Our work employs a similar model, predicting valence and arousal from openSMILE features ____. We subsitute the emotion extraction from lyric embeddings by directly feeding the lyrics to an LLM in image prompt generation.

\textbf{Text to Image and Video Models:} Latent Diffusion Models (LDMs) ____ use pretrained autoencoders' latent space to train diffusion models for image synthesis, reducing computational costs while maintaining quality. LDMs also achieve competitive performance in text-to-image tasks comparable to models like Google Imagen ____ and OpenAI DALL-E 3 ____. Models such as Stable Diffusion XL ____ and Flux ____ extend this by allowing flexible stylization through finetuning and LoRA training. 

Text-to-video advancements have been driven by closed-source companies like Pika ____, Luma Labs ____, RunwayML ____, and OpenAI ____, using diffusion transformer models ____. Open-source alternatives like Stable Video Diffusion ____ and OpenSORA ____ exist but have limitations in video length (~16 seconds) and quality ____. Thus for our purpose, we use Stable Diffusion 1.5 to generate image frames and apply spherical interpolation between frames to generate video.

\textbf{Authentication Protocols:} Authentication protocols like CAPTCHA ____ present users with tasks that are challenging for machines but simple for humans. They have evolved from simple human-machine differentiation to sophisticated variants like reCAPTCHA \& hCAPTCHA ____. 

The advent of generative AI has shifted the paradigm, blurring the lines between human and machine-generated content. While generative AI offers unprecedented personalization opportunities, it also facilitates impersonation through DeepFakes and Celebrity LoRAs ____. Thus we introduce CHARCHA, a novel authentication method building on CAPTCHA's legacy to take advantage of the benefits of personalization while minimizing risk of identity manipulation and misuse.