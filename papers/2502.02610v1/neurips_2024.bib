@misc{(Deforum)_2023, title={How to make a video with stable diffusion (Deforum)}, url={https://stable-diffusion-art.com/deforum/}, journal={Stable Diffusion Art}, author={(Open Source), Deforum}, year={2023}, month={Dec}} 

@misc{whisper,
  doi = {10.48550/ARXIV.2212.04356},
  url = {https://arxiv.org/abs/2212.04356},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{wang2023generative,
      title={Generative Powers of Ten}, 
      author={Xiaojuan Wang and Janne Kontkanen and Brian Curless and Steve Seitz and Ira Kemelmacher and Ben Mildenhall and Pratul Srinivasan and Dor Verbin and Aleksander Holynski},
      year={2023},
      eprint={2312.02149},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{customizing,
      title={Customizing Motion in Text-to-Video Diffusion Models}, 
      author={Joanna Materzynska and Josef Sivic and Eli Shechtman and Antonio Torralba and Richard Zhang and Bryan Russell},
      year={2023},
      eprint={2312.04966},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Jafarian_2021_CVPR_TikTok,
    author    = {Jafarian, Yasamin and Park, Hyun Soo},
    title     = {Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12753-12762}} 

@misc{tseng2022edge,
      title={EDGE: Editable Dance Generation From Music}, 
      author={Jonathan Tseng and Rodrigo Castellon and C. Karen Liu},
      year={2022},
      eprint={2211.10658},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}


@inproceedings{aist-dance-db,
           author = {Shuhei Tsuchida and Satoru Fukayama and Masahiro Hamasaki and Masataka Goto}, 
           title = {AIST Dance Video Database: Multi-genre, Multi-dancer, and Multi-camera Database for Dance Information Processing}, 
           booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference, {ISMIR} 2019},
           address = {Delft, Netherlands}, 
           year = 2019, 
           month = nov }

@misc{stablediff,
  doi = {10.48550/ARXIV.2112.10752},
  
  url = {https://arxiv.org/abs/2112.10752},
  
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{delbouys2018music,
      title={Music Mood Detection Based On Audio And Lyrics With Deep Neural Net}, 
      author={Rémi Delbouys and Romain Hennequin and Francesco Piccoli and Jimena Royo-Letelier and Manuel Moussallam},
      year={2018},
      eprint={1809.07276},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@misc{giffusion,
	author = {Dhruv Nair},
	title = {{M}ake {Y}our {A}rt {M}ove with {S}table {D}iffusion {A}nimations --- towardsdatascience.com},
	howpublished = {\url{https://towardsdatascience.com/make-your-art-move-with-stable-diffusion-animations-80de62eec633}},
	year = {},
	note = {[Accessed 02-May-2023]},
}

@misc{controlnet,
      title={Adding Conditional Control to Text-to-Image Diffusion Models}, 
      author={Lvmin Zhang and Anyi Rao and Maneesh Agrawala},
      year={2023},
      eprint={2302.05543},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{hu2023animate,
      title={Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation}, 
      author={Li Hu and Xin Gao and Peng Zhang and Ke Sun and Bang Zhang and Liefeng Bo},
      year={2023},
      eprint={2311.17117},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{brooks2023instructpix2pix,
      title={InstructPix2Pix: Learning to Follow Image Editing Instructions}, 
      author={Tim Brooks and Aleksander Holynski and Alexei A. Efros},
      year={2023},
      eprint={2211.09800},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vist,
      title={Visual Storytelling}, 
      author={Ting-Hao and Huang and Francis Ferraro and Nasrin Mostafazadeh and Ishan Misra and Aishwarya Agrawal and Jacob Devlin and Ross Girshick and Xiaodong He and Pushmeet Kohli and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh and Lucy Vanderwende and Michel Galley and Margaret Mitchell},
      year={2016},
      eprint={1604.03968},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{laion,
  doi = {10.48550/ARXIV.2210.08402},
  
  url = {https://arxiv.org/abs/2210.08402},
  
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LAION-5B: An open large-scale dataset for training next generation image-text models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{clip,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@software{Ryu_Low-rank_adaptation_for,
author = {Ryu, Simo},
title = {{Low-rank adaptation for fast text-to-image diffusion fine-tuning}},
url = {https://github.com/cloneofsimo/lora},
version = {0.0.1}
}

@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Civitai, url={https://civitai.com/}, journal={Civitai}} 

@misc{ruiz2023dreambooth,
      title={DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation}, 
      author={Nataniel Ruiz and Yuanzhen Li and Varun Jampani and Yael Pritch and Michael Rubinstein and Kfir Aberman},
      year={2023},
      eprint={2208.12242},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{textual_inversion,
      title={An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion}, 
      author={Rinon Gal and Yuval Alaluf and Yuval Atzmon and Or Patashnik and Amit H. Bermano and Gal Chechik and Daniel Cohen-Or},
      year={2022},
      eprint={2208.01618},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhong2024multilora,
      title={Multi-LoRA Composition for Image Generation}, 
      author={Ming Zhong and Yelong Shen and Shuohang Wang and Yadong Lu and Yizhu Jiao and Siru Ouyang and Donghan Yu and Jiawei Han and Weizhu Chen},
      year={2024},
      eprint={2402.16843},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{toonyou, title={Toonyou - beta 6: Stable diffusion checkpoint}, url={https://civitai.com/models/30240/toonyou}, journal={Civitai}, author={Bradcatt}, year={2023}, month={Jun}} 

@misc{real_vis, title={Realistic vision V6.0 B1 - V6.0 B1 (VAE): Stable diffusion checkpoint}, url={https://civitai.com/models/4201?modelVersionId=245598}, journal={Civitai}, author={SG\_161222}, year={2023}, month={Jan}} 

@misc{kohya, title={Bmaltais/KOHYA\_SS}, url={https://github.com/bmaltais/kohya\_ss?tab=readme-ov-file}, journal={GitHub}, author={Bmaltais}} 

@misc{kony_tutorial, title={Tutorial: Konyconi-style Lora - Konyconi - [training data]: Stable diffusion other}, url={https://civitai.com/models/52697/tutorial-konyconi-style-lora}, journal={Civitai}, author={Civitai}, year={2024}, month={Jan}} 

@misc{li2021learn,
      title={Learn to Dance with AIST++: Music Conditioned 3D Dance Generation}, 
      author={Ruilong Li and Shan Yang and David A. Ross and Angjoo Kanazawa},
      year={2021},
      eprint={2101.08779},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{aist-dance-db,
           author = {Shuhei Tsuchida and Satoru Fukayama and Masahiro Hamasaki and Masataka Goto}, 
           title = {AIST Dance Video Database: Multi-genre, Multi-dancer, and Multi-camera Database for Dance Information Processing}, 
           booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference, {ISMIR} 2019},
           address = {Delft, Netherlands}, 
           year = 2019, 
           month = nov }

@misc{https://doi.org/10.48550/arxiv.2104.13478,
  doi = {10.48550/ARXIV.2104.13478},
  
  url = {https://arxiv.org/abs/2104.13478},
  
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computational Geometry (cs.CG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{laion,
  doi = {10.48550/ARXIV.2210.08402},
  
  url = {https://arxiv.org/abs/2210.08402},
  
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LAION-5B: An open large-scale dataset for training next generation image-text models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{webvid,
  doi = {10.48550/ARXIV.2104.00650},
  
  url = {https://arxiv.org/abs/2104.00650},
  
  author = {Bain, Max and Nagrani, Arsha and Varol, Gül and Zisserman, Andrew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{msrvtt,
author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},
year = {2016},
month = {June},
abstract = {While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content.

In this paper we present MSR-VTT (standing for “MSR Video to Text”) which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. We present a detailed analysis of MSR-VTT in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches. We also provide an extensive evaluation of these approaches on this dataset, showing that the hybrid Recurrent Neural Networkbased approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on MSR-VTT.},
publisher = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
url = {https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/},
}

@misc{lsvtd,
  doi = {10.48550/ARXIV.1903.03299},
  
  url = {https://arxiv.org/abs/1903.03299},
  
  author = {Cheng, Zhanzhan and Lu, Jing and Niu, Yi and Pu, Shiliang and Wu, Fei and Zhou, Shuigeng},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {You Only Recognize Once: Towards Fast Video Text Spotting},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{howto100m,
  doi = {10.48550/ARXIV.1906.03327},
  
  url = {https://arxiv.org/abs/1906.03327},
  
  author = {Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{millionsong,
  author = {Thierry Bertin-Mahieux and Daniel P.W. Ellis and Brian Whitman and Paul Lamere},
  title = {The Million Song Dataset},
  booktitle = {{Proceedings of the 12th International Conference on Music Information
	Retrieval ({ISMIR} 2011)}},
  year = {2011},
  owner = {thierry},
  timestamp = {2010.03.07}
}

@article{mugen,
  title={MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration},
  author={Hayes, Thomas and Zhang, Songyang and Yin, Xi and Pang, Guan and Sheng, Sasha and Yang, Harry and Ge, Songwei and Hu, Qiyuan and Parikh, Devi},
  journal={arXiv preprint arXiv:2204.08058},
  year={2022}
}

@misc{imagen,
  doi = {10.48550/ARXIV.2205.11487},
  
  url = {https://arxiv.org/abs/2205.11487},
  
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dalle,
  doi = {10.48550/ARXIV.2204.13807},
  
  url = {https://arxiv.org/abs/2204.13807},
  
  author = {Marcus, Gary and Davis, Ernest and Aaronson, Scott},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A very preliminary analysis of DALL-E 2},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@misc{dreambooth,
  doi = {10.48550/ARXIV.2208.12242},
  
  url = {https://arxiv.org/abs/2208.12242},
  
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{ediffi,
  doi = {10.48550/ARXIV.2211.01324},
  
  url = {https://arxiv.org/abs/2211.01324},
  
  author = {Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and Catanzaro, Bryan and Karras, Tero and Liu, Ming-Yu},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{midjourney,
  doi = {10.48550/ARXIV.2210.00586},
  
  url = {https://arxiv.org/abs/2210.00586},
  
  author = {Borji, Ali},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{phenaki,
  doi = {10.48550/ARXIV.2210.02399},
  
  url = {https://arxiv.org/abs/2210.02399},
  
  author = {Villegas, Ruben and Babaeizadeh, Mohammad and Kindermans, Pieter-Jan and Moraldo, Hernan and Zhang, Han and Saffar, Mohammad Taghi and Castro, Santiago and Kunze, Julius and Erhan, Dumitru},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Phenaki: Variable Length Video Generation From Open Domain Textual Description},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{makeavid,
  doi = {10.48550/ARXIV.2209.14792},
  
  url = {https://arxiv.org/abs/2209.14792},
  
  author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Make-A-Video: Text-to-Video Generation without Text-Video Data},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{tuneavid,
  doi = {10.48550/ARXIV.2212.11565},
  
  url = {https://arxiv.org/abs/2212.11565},
  
  author = {Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Weixian and Gu, Yuchao and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{text2live,
  doi = {10.48550/ARXIV.2204.02491},
  
  url = {https://arxiv.org/abs/2204.02491},
  
  author = {Bar-Tal, Omer and Ofri-Amar, Dolev and Fridman, Rafail and Kasten, Yoni and Dekel, Tali},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Text2LIVE: Text-Driven Layered Image and Video Editing},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{zeroeggs,
  doi = {10.48550/ARXIV.2209.07556},
  
  url = {https://arxiv.org/abs/2209.07556},
  
  author = {Ghorbani, Saeed and Ferstl, Ylva and Holden, Daniel and Troje, Nikolaus F. and Carbonneau, Marc-André},
  
  keywords = {Graphics (cs.GR), Machine Learning (cs.LG), Sound (cs.SD), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@misc{vtoonify,
  doi = {10.48550/ARXIV.2209.11224},
  
  url = {https://arxiv.org/abs/2209.11224},
  
  author = {Yang, Shuai and Jiang, Liming and Liu, Ziwei and Loy, Chen Change},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {VToonify: Controllable High-Resolution Portrait Video Style Transfer},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2209.15264,
  doi = {10.48550/ARXIV.2209.15264},
  
  url = {https://arxiv.org/abs/2209.15264},
  
  author = {Kwon, Gihyun and Ye, Jong Chul},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Diffusion-based Image Translation using Disentangled Style and Content Representation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}




@misc{BLIP,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      eprint={2201.12086},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{Posner2005TheCM,
  title={The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology},
  author={Jonathan Posner and James A. Russell and Bradley S. Peterson},
  journal={Development and Psychopathology},
  year={2005},
  volume={17},
  pages={715 - 734}
}

@inproceedings{opensmile,
author = {Eyben, Florian and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn},
title = {Opensmile: The Munich Versatile and Fast Open-Source Audio Feature Extractor},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874246},
doi = {10.1145/1873951.1874246},
abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1459–1462},
numpages = {4},
keywords = {music, speech, signal processing, emotion, audio feature extraction, statistical functionals},
location = {Firenze, Italy},
series = {MM '10}
}

@article{plp,
author = {Grosche, P. and Muller, M.},
title = {Extracting Predominant Local Pulse Information From Music Recordings},
year = {2011},
issue_date = {August 2011},
publisher = {IEEE Press},
volume = {19},
number = {6},
issn = {1558-7916},
url = {https://doi.org/10.1109/TASL.2010.2096216},
doi = {10.1109/TASL.2010.2096216},
abstract = {The extraction of tempo and beat information from music recordings constitutes a challenging task in particular for non-percussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation that captures musically meaningful local pulse information even for the case of complex music. Our main idea is to derive for each time position a sinusoidal kernel that best explains the local periodic nature of a previously extracted note onset representation. Then we employ an overlap-add technique accumulating all these kernels over time to obtain a single function that reveals the predominant local pulse (PLP). Our concept introduces a high degree of robustness to noise and distortions resulting from weak and blurry onsets. Furthermore, the resulting PLP curve reveals the local pulse information even in the presence of continuous tempo changes and indicates a kind of confidence in the periodicity estimation. As further contribution, we show how our PLP concept can be used as a flexible tool for enhancing tempo estimation and beat tracking. The practical relevance of our approach is demonstrated by extensive experiments based on music recordings of various genres.},
journal = {Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {1688–1701},
numpages = {14}
}

@INPROCEEDINGS{librosa,
  author={Raguraman, Preeth and R., Mohan and Vijayan, Midhula},
  booktitle={2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)}, 
  title={LibROSA Based Assessment Tool for Music Information Retrieval Systems}, 
  year={2019},
  volume={},
  number={},
  pages={109-114},
  doi={10.1109/MIPR.2019.00027}}


@article{DEAM,
author = {Alajanki, Anna and Yang, Yi-Hsuan and Soleymani, Mohammad},
title = {Benchmarking music emotion recognition systems},
journal = {PLOS ONE},
year = {2016},
note= {under review}
}

 @misc{sketch, title={Queratogray Sketch (eddiemauro-mix) - v1.0 | Stable Diffusion Checkpoint | Civitai}, url={https://civitai.com/models/80357/queratogray-sketch-eddiemauro-mix}, journal={Civitai.com}, author={eddiemauro}, year={2023} }

 @misc{west, title={Western Animation Diffusion - v1 | Stable Diffusion Checkpoint | Civitai}, url={https://civitai.com/models/86546/western-animation-diffusion}, journal={Civitai.com}, author={Lykkon}, year={2023} }

@misc{SDXL,
      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, 
      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Müller and Joe Penna and Robin Rombach},
      year={2023},
      eprint={2307.01952},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.01952}, 
}

 @misc{flux, title={GitHub - black-forest-labs/flux: Official inference repo for FLUX.1 models}, url={https://github.com/black-forest-labs/flux}, journal={GitHub}, author={black-forest-labs}, year={2024} }

 @misc{pika, title={Pika}, url={https://pika.art/}, journal={pika.art}, author={Pika} }

 @misc{dreammachine, title={Luma Dream Machine}, url={https://lumalabs.ai/dream-machine}, journal={Luma Dream Machine}, author={AI, Luma} }

 @misc{gen3alpha, title={Runway Research | Introducing Gen-3 Alpha: A New Frontier for Video Generation}, url={https://runwayml.com/research/introducing-gen-3-alpha}, journal={Runwayml.com}, author={Runway ML}, year={2024} }

@misc{sora,
      title={Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models}, 
      author={Yixin Liu and Kai Zhang and Yuan Li and Zhiling Yan and Chujie Gao and Ruoxi Chen and Zhengqing Yuan and Yue Huang and Hanchi Sun and Jianfeng Gao and Lifang He and Lichao Sun},
      year={2024},
      eprint={2402.17177},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.17177}, 
}

@misc{dit,
      title={Scalable Diffusion Models with Transformers}, 
      author={William Peebles and Saining Xie},
      year={2023},
      eprint={2212.09748},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.09748}, 
}

@misc{svd,
      title={Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets}, 
      author={Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
      year={2023},
      eprint={2311.15127},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.15127}, 
}

 @misc{opensora, title={Open-Sora/docs/report\_03.md at main · hpcaitech/Open-Sora}, url={https://github.com/hpcaitech/Open-Sora/blob/main/docs/report\_03.md}, journal={GitHub}, author={hpcaitech}, year={2024} }

@inproceedings{captcha,
author = {Ahn, Luis Von and Blum, Manuel and Hopper, Nicholas J. and Langford, John},
title = {CAPTCHA: using hard AI problems for security},
year = {2003},
isbn = {3540140395},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We introduce captcha, an automated test that humans can pass, but current computer programs can't pass: any program that has high success over a captcha can be used to solve an unsolved Artificial Intelligence (AI) problem. We provide several novel constructions of captchas. Since captchas have many applications in practical security, our approach introduces a new class of hard problems that can be exploited for security purposes. Much like research in cryptography has had a positive impact on algorithms for factoring and discrete log, we hope that the use of hard AI problems for security purposes allows us to advance the field of Artificial Intelligence. We introduce two families of AI problems that can be used to construct captchas and we show that solutions to such problems can be used for steganographic communication. captchas based on these AI problem families, then, imply a win-win situation: either the problems remain unsolved and there is a way to differentiate humans from computers, or the problems are solved and there is a way to communicate covertly on some channels.},
booktitle = {Proceedings of the 22nd International Conference on Theory and Applications of Cryptographic Techniques},
pages = {294–311},
numpages = {18},
location = {Warsaw, Poland},
series = {EUROCRYPT'03}
}

@misc{recaptcha,
      title={An Empirical Study \& Evaluation of Modern CAPTCHAs}, 
      author={Andrew Searles and Yoshimichi Nakatsuka and Ercan Ozturk and Andrew Paverd and Gene Tsudik and Ai Enkoji},
      year={2023},
      eprint={2307.12108},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2307.12108}, 
}

@article{deepfake,
   title={Deep learning for deepfakes creation and detection: A survey},
   volume={223},
   ISSN={1077-3142},
   url={http://dx.doi.org/10.1016/j.cviu.2022.103525},
   DOI={10.1016/j.cviu.2022.103525},
   journal={Computer Vision and Image Understanding},
   publisher={Elsevier BV},
   author={Nguyen, Thanh Thi and Nguyen, Quoc Viet Hung and Nguyen, Dung Tien and Nguyen, Duc Thanh and Huynh-The, Thien and Nahavandi, Saeid and Nguyen, Thanh Tam and Pham, Quoc-Viet and Nguyen, Cuong M.},
   year={2022},
   month=oct, pages={103525} }

 @misc{celeblora, title={celebrity Stable Diffusion AI Models | Civitai}, url={https://civitai.com/tag/celebrity}, journal={Civitai.com}, author={civitai}, year={2024} }

 @book{imagineme, title={Imagine yourself: Tuning-Free Personalized Image Generation}, url={https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/455202613_905334511631908_8424078344857058009_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=hvfJltR6G6gQ7kNvgHSKNuK&_nc_ht=scontent-sjc3-1.xx&oh=00_AYAc2szhaBbdcOGu1YRQPSLo6GC6ayMimiurCj-NLOGryA&oe=66C77D43}, 
publisher={Meta AI}, author={He, Zecheng and Sun, Bo and Juefei-Xu, Felix and Ma, Haoyu and Ramchandani, Ankit and Cheung, Vincent and Shah, Siddharth and Kalia, Anmol and Zhang, Ning and Zhang, Peizhao and Sumbaly, Roshan and Vajda, Peter and Sinha, Animesh and Genai, Meta}, year={2024}, month={Aug} }

@misc{lia_paper,
      title={Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset}, 
      author={Zhixuan Liu and Youeun Shin and Beverley-Claire Okogwu and Youngsik Yun and Lia Coleman and Peter Schaldenbrand and Jihie Kim and Jean Oh},
      year={2023},
      eprint={2301.12073},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2301.12073}, 
}

@misc{mediapipe,
      title={MediaPipe: A Framework for Building Perception Pipelines}, 
      author={Camillo Lugaresi and Jiuqiang Tang and Hadon Nash and Chris McClanahan and Esha Uboweja and Michael Hays and Fan Zhang and Chuo-Ling Chang and Ming Guang Yong and Juhyun Lee and Wan-Teh Chang and Wei Hua and Manfred Georg and Matthias Grundmann},
      year={2019},
      eprint={1906.08172},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1906.08172}, 
}

@INPROCEEDINGS{opencv,
  author={Culjak, Ivan and Abram, David and Pribanic, Tomislav and Dzapo, Hrvoje and Cifrek, Mario},
  booktitle={2012 Proceedings of the 35th International Convention MIPRO}, 
  title={A brief introduction to OpenCV}, 
  year={2012},
  volume={},
  number={},
  pages={1725-1730},
  keywords={Libraries;Image edge detection;Cameras;Computer vision;Histograms;Low pass filters;Detectors},
  doi={}}

@misc{vggface,
      title={VGGFace2: A dataset for recognising faces across pose and age}, 
      author={Qiong Cao and Li Shen and Weidi Xie and Omkar M. Parkhi and Andrew Zisserman},
      year={2018},
      eprint={1710.08092},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1710.08092}, 
}

@article{deepface1,
  title         = {A Benchmark of Facial Recognition Pipelines and Co-Usability Performances of Modules},
  author        = {Serengil, Sefik Ilkin and Ozpinar, Alper},
  journal       = {Bilisim Teknolojileri Dergisi},
  volume        = {17},
  number        = {2},
  pages         = {95-107},
  year          = {2024},
  doi           = {10.17671/gazibtd.1399077},
  url           = {https://dergipark.org.tr/en/pub/gazibtd/issue/84331/1399077},
  publisher     = {Gazi University}
}

@inproceedings{deepface2,
  title        = {LightFace: A Hybrid Deep Face Recognition Framework},
  author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},
  booktitle    = {2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},
  pages        = {23-27},
  year         = {2020},
  doi          = {10.1109/ASYU50717.2020.9259802},
  url          = {https://ieeexplore.ieee.org/document/9259802},
  organization = {IEEE}
}

 @misc{deepfacelive, title={iperov/DeepFaceLive}, url={https://github.com/iperov/DeepFaceLive}, journal={GitHub}, author={iperov}, year={2022}, month={Nov} }

@inproceedings{clipscore,
    title = "{CLIPS}core: A Reference-free Evaluation Metric for Image Captioning",
    author = "Hessel, Jack  and
      Holtzman, Ari  and
      Forbes, Maxwell  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.595",
    doi = "10.18653/v1/2021.emnlp-main.595",
    pages = "7514--7528",
    abstract = "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",
}

@misc{clipeval,
      title={Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy and Novel Ensemble Method}, 
      author={Uri Berger and Gabriel Stanovsky and Omri Abend and Lea Frermann},
      year={2024},
      eprint={2408.04909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.04909}, 
}

@misc{neuralframes2023,
  author = {{NeuralFrames}},
  title = {AI Music Video Generator},
  year = {2023},
  url = {https://www.neuralframes.com/ai-music-video-generator},
  note = {Accessed: 2024-08-16}
}

@misc{kaiber2023,
  author = {{Kaiber AI}},
  title = {Kaiber AI Product},
  year = {2023},
  url = {https://kaiber.ai/product},
  note = {Accessed: 2024-08-16}
}

@misc{gpt4o,
  author = {{OpenAI}},
  title = {Hello GPT-4o},
  year = {2024},
  url = {https://openai.com/index/hello-gpt-4o/},
  note = {Accessed: 2024-08-16}
}

@misc{av1,
      title={Sound2Sight: Generating Visual Dynamics from Sound and Context}, 
      author={Anoop Cherian and Moitreya Chatterjee and Narendra Ahuja},
      year={2020},
      eprint={2007.12130},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2007.12130}, 
}

@misc{av2,
      title={The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion}, 
      author={Yujin Jeong and Wonjeong Ryoo and Seunghyun Lee and Dabin Seo and Wonmin Byeon and Sangpil Kim and Jinkyu Kim},
      year={2023},
      eprint={2309.04509},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2309.04509}, 
}

@misc{av3,
      title={MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation}, 
      author={Ludan Ruan and Yiyang Ma and Huan Yang and Huiguo He and Bei Liu and Jianlong Fu and Nicholas Jing Yuan and Qin Jin and Baining Guo},
      year={2023},
      eprint={2212.09478},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.09478}, 
}