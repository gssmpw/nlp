\section{Related Work}
\label{sec:literature}
Due to the lack of continual pretraining work on LLMs for Urdu, this section examines the closest related works. This includes models developed for Asian languages and low-resource languages built using the LLaMA framework.

Tamil-Llama \citep{tamilllama}, an Asian language model built on LLaMA 2 \citep{llama2}, incorporates 16,000 Tamil tokens and utilizes the Low-Rank Adaptation (LoRA) \citep{lora} technique for efficient training on Tamil datasets. The model was trained on an Nvidia A100 GPU with 80GB of VRAM for 48 hours, followed by instruct fine tuning on translated Alpaca datasets \citep{alpaca} and a custom subset of the OpenOrca \citep{openorca} dataset for 60 hours using Microsoft Azureâ€™s Standard NC24 ads A100v4 instance. Performance evaluations indicate significant improvements in Tamil text generation, with the Tamil-Llama 13B model outperforming OpenAI's GPT-3.5-turbo on Tamil language tasks.

Taiwan-LLM \citep{18}, an LLM for Traditional Chinese, underwent continual pretraining on LLaMA 2 \citep{llama2} using 35.1 billion tokens and a diverse instruction set derived from 17 fine tuning datasets, including 20,000 user feedback instances. The training process leveraged the Transformer Reinforcement Learning (TRL) library \citep{15}, along with DeepSpeed ZeRO-2 \citep{16} and FlashAttention-2 \citep{17} to optimize memory usage and enhance training efficiency. Utilizing up to 48 NVIDIA H100 Tensor Core GPUs, Taiwan-LLM demonstrated superior performance in understanding and generating Traditional Chinese text, surpassing models such as GPT-4 and Claude-2.1.

PersianLLaMA \citep{persianllama}, the first large-scale Persian language model, was trained from scratch on 184 million tokens from Persian Wikipedia and 9 billion tokens from the OSCAR dataset \citep{19}. The training process leveraged DeepSpeed \citep{deepspeed} and TencentPretrain \citep{20}, two advanced frameworks for optimizing deep learning, utilizing two A100 GPUs with 80GB of VRAM over 12 days. Additionally, they conducted an experiment using LoRA \citep{lora} with the original English LLaMA weights, training on a single A100 GPU with 80GB of VRAM for over 70 hours. Their evaluations indicate that PersianLLaMA significantly outperformed its competitors in both understanding and generating Persian text. 

Airavata \citep{21} is an instruction-tuned model for Hindi, built by fine tuning OpenHathi \citep{22}, on 404k instruction instances from diverse Hindi instruction-tuning datasets. OpenHathi \citep{22} is again a model built on the LLaMA 2 7B architecture. The training employed both full fine tuning and supervised fine tuning using LoRA \citep{lora}. Their results demonstrated that Airavata significantly outperforms OpenHathi on most tasks, highlighting the effectiveness of fine tuning in aligning the base model to a variety of tasks. The details regarding their training infrastructure were not provided in their paper.

SeaLLMs \citep{seallm} is an innovative series of language models focused on Southeast Asian (SEA) languages. Built upon LLaMA 2 \citep{llama2} and Mistral 7B \citep{mistral7b}, SeaLLMs underwent continued pretraining with an extended vocabulary, followed by a hybrid approach for instruction and alignment tuning. Their evaluation claims that SeaLLMs significantly outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins, while remaining lightweight and cost-effective to operate. The authors did not provide detailed information regarding the training infrastructure in their paper.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{urdullmpaperdataset4.png} 
    \caption{Development of UrduLLaMA 1.0}
    \label{fig:example-image}
\end{figure*} 


A study related to Chinese LLaMA \citep{24} extended different variants of LLaMA 2 \citep{llama2} by adding 20,000 Chinese tokens to the existing vocabulary. The model was pre-trained using LoRA \citep{lora} and fine tuned on Chinese instruction datasets formatted according to Alpaca \citep{alpaca}. Training was conducted on A40 GPUs (48GB VRAM), with up to 48 GPUs used depending on the model size. The parameter-efficient training with LoRA was carried out using the PEFT library\footnote{\url{https://github.com/huggingface/peft}}. Additionally, DeepSpeed \citep{deepspeed} was employed to optimize memory efficiency during training. Experimental results demonstrate that the newly proposed model significantly improves the original LLaMA's ability to understand and generate Chinese content.

Another study \citep{25} conducted a two-stage continual pretraining of LLaMA 3 8B \citep{llama3} for Chinese. Initially, they performed experiments on TinyLLaMA \citep{tinyllama}, and then applied their findings to train LLaMA 3 using 100B tokens, followed by fine tuning on Synthetic Scientific QA Data. The experiments were implemented using Hugging Face Transformers \citep{26}, incorporating Flash Attention and DeepSpeed ZeRO \citep{deepspeed} to optimize training efficiency. The study leveraged computing resources provided by the Public Computing Cloud at Renmin University of China. Their extensive experiments on a number
of evaluation benchmarks show that their approach can largely improve the performance of the backbone models, including both the general abilities and the scientific reasoning abilities without hurting the original capacities.

Latxa \citep{27} is a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2 \citep{llama2}, which they continue pretraining on their own Basque corpus comprising of 4.2B tokens.  The training of Latxa has been conducted using the GPT-Neox \citep{28} library. As infrastructure, they have leveraged the CINECA HPC Leonardo computing cluster located in Italy, which is powered by 3,456 nodes each containing 4x custom A100 64GB GPUs. They claimed that Latxa outperforms all previous open models they compared to by a large margin. 

\begin{table*}[ht]
    \centering
    \caption{Summary of Token Count Reduction Across Different Data Sources for Urdu-LLaMA}
    \label{tab:token_reduction}
    \renewcommand{\arraystretch}{1.5} % Increase the row height by 50%
    \resizebox{\textwidth}{!}{ % Scale the table to the width of the page
        \begin{tabular}{lcccc}
            \hline
            \textbf{Source} & \textbf{Original Token Count} & \textbf{Token Count After Processing} & \textbf{Reduction} & \textbf{Percentage Reduction (\%)} \\
            \hline
            Publically Available Resources & 798,260,573 & 541,151,638 & 257,108,935 & 32.20 \\
            Inhouse          & 639,786,525 & 606,053,446 & 33,733,079  & 5.30  \\
            \hline

        UrduLLaMA 1.0 Dataset   & 1,438,047,098 & 1,147,205,084 & 290,842,014 & \textbf{-} \\
            \textbf{UrduLLaMA 1.0 Dataset  (in Billion)} & \textbf{1.43} & \textbf{1.14} & \textbf{0.29} & \textbf{-} \\
            \hline
        \end{tabular}
    }
\end{table*}
VinaLLaMA \citep{31}, an open-weight, state-of-the-art (SOTA) Large Language Model for the Vietnamese language, was built upon LLaMA 2 \citep{llama2} with an additional 800 billion trained tokens followed by fine tuning on 1 million sample instruction of Vietnamese and English. For our pretraining phase, they utilized a cluster consisting of eight nodes, each equipped with 8x Intel Habana Gaudi2 Processors. This phase was completed over the course of one week. In contrast, the fine tuning phase was conducted more rapidly, utilizing a single node of Google Cloud TPU v5e, and completed within a single day. They claim to achieve state-of-the-art results on different key benchmarks, showcasing fluency in Vietnamese and a deep understanding of their culture.

VI-MISTRAL-X \citep{29} is another LLM designed expressly for the Vietnamese language. It performed continual pretraining on 8 billion tokens selected from CulturaX \citep{7}, on the Mistral architecture, using 8 Nvidia H100 80GB SXM5
GPUs. Following the pretraining phase, vi-mistral-x underwent a series of fine tuning processes aimed at aligning the model
with specific NLP tasks. Through comprehensive testing
on various benchmarks, they have shown that vi-mistral-x has  outperformd existing Vietnamese LLMs in several
key areas.

This literature review presents the absence of dedicated Urdu LLMs highlights a significant need for such resources. This paper introduces the first continual pretraining followed by fine tuning of Llama-3.1-8B-Instruct model for Urdu, leveraging a dataset of 128 million tokens and using the LoRA \citep{lora} training approach for instruct tuning. This pioneering effort aims to pave the way for future research and development of more sophisticated Urdu language models.