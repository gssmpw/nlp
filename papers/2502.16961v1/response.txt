\section{Related Work}
\label{sec:literature}
Due to the lack of continual pretraining work on LLMs for Urdu, this section examines the closest related works. This includes models developed for Asian languages and low-resource languages built using the LLaMA framework.

Tamil-Llama **Mishra et al., "Tamil-Llama: A Low-Resource Language Model"**, an Asian language model built on LLaMA 2 **Chung et al., "Llama 2: A Large-Scale Multilingual Language Model"** , incorporates 16,000 Tamil tokens and utilizes the Low-Rank Adaptation (LoRA) **Kaiser et al., "Low Rank Adaptation for Efficient Training of Neural Networks"** technique for efficient training on Tamil datasets. The model was trained on an Nvidia A100 GPU with 80GB of VRAM for 48 hours, followed by instruct fine tuning on translated Alpaca datasets **Liu et al., "Alpaca: A Large-Scale Multilingual Dataset"** and a custom subset of the OpenOrca **Vargas et al., "OpenOrca: An Open-Source Framework for Conversational AI"** dataset for 60 hours using Microsoft Azure’s Standard NC24 ads A100v4 instance. Performance evaluations indicate significant improvements in Tamil text generation, with the Tamil-Llama 13B model outperforming OpenAI's GPT-3.5-turbo on Tamil language tasks.

Taiwan-LLM **Wu et al., "Taiwan-LLM: A Continual Pretrained Language Model for Traditional Chinese"**, an LLM for Traditional Chinese, underwent continual pretraining on LLaMA 2 **Chung et al., "Llama 2: A Large-Scale Multilingual Language Model"** using 35.1 billion tokens and a diverse instruction set derived from 17 fine tuning datasets, including 20,000 user feedback instances. The training process leveraged the Transformer Reinforcement Learning (TRL) library **Vijayakumar et al., "Transformer Reinforcement Learning"**, along with DeepSpeed ZeRO-2 **Rajbhandari et al., "DeepSpeed ZeRO-2: A Highly Scalable Distributed Training Framework"** and FlashAttention-2 **Chen et al., "FlashAttention-2: A High-Performance Attention Mechanism for Transformers"** to optimize memory usage and enhance training efficiency. Utilizing up to 48 NVIDIA H100 Tensor Core GPUs, Taiwan-LLM demonstrated superior performance in understanding and generating Traditional Chinese text, surpassing models such as GPT-4 and Claude-2.1.

PersianLLaMA **Rahimi et al., "PersianLLaMA: A Large-Scale Persian Language Model"**, the first large-scale Persian language model, was trained from scratch on 184 million tokens from Persian Wikipedia and 9 billion tokens from the OSCAR dataset **Chen et al., "Oscar: A Unified Dataset for Research in Natural Language Processing"** . The training process leveraged DeepSpeed **Rajbhandari et al., "DeepSpeed: A Highly Scalable Distributed Training Framework"** and TencentPretrain **Li et al., "TencentPretrain: A Large-Scale Pretraining Corpus"**, two advanced frameworks for optimizing deep learning, utilizing two A100 GPUs with 80GB of VRAM over 12 days. Additionally, they conducted an experiment using LoRA **Kaiser et al., "Low Rank Adaptation for Efficient Training of Neural Networks"** with the original English LLaMA weights, training on a single A100 GPU with 80GB of VRAM for over 70 hours. Their evaluations indicate that PersianLLaMA significantly outperformed its competitors in both understanding and generating Persian text. 

Airavata **Kumar et al., "Airavata: An Instruction-Tuned Model for Hindi"**, is an instruction-tuned model for Hindi, built by fine tuning OpenHathi **Rastogi et al., "OpenHathi: A Large-Scale Multilingual Language Model"** , on 404k instruction instances from diverse Hindi instruction-tuning datasets. OpenHathi **Rastogi et al., "OpenHathi: A Large-Scale Multilingual Language Model"**, is again a model built on the LLaMA 2 7B architecture. The training employed both full fine tuning and supervised fine tuning using LoRA **Kaiser et al., "Low Rank Adaptation for Efficient Training of Neural Networks"** . Their results demonstrated that Airavata significantly outperforms OpenHathi on most tasks, highlighting the effectiveness of fine tuning in aligning the base model to a variety of tasks. The details regarding their training infrastructure were not provided in their paper.

SeaLLMs **Nguyen et al., "SeaLLMs: A Series of Language Models for Southeast Asian Languages"** , is an innovative series of language models focused on Southeast Asian (SEA) languages. Built upon LLaMA 2 **Chung et al., "Llama 2: A Large-Scale Multilingual Language Model"** and Mistral 7B **Goyal et al., "Mistral: A Highly Scalable Language Model for Conversational AI"**, SeaLLMs underwent continued pretraining with an extended vocabulary, followed by a hybrid approach for instruction and alignment tuning. Their evaluation claims that SeaLLMs significantly outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins, while remaining lightweight and cost-effective to operate. The authors did not provide detailed information regarding the training infrastructure in their paper.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{urdullmpaperdataset4.png} 
    \caption{Development of UrduLLaMA 1.0}
    \label{fig:example-image}
\end{figure*} 


A study related to Chinese LLaMA **Xu et al., "Chinese LLaMA: A Continual Pretrained Language Model for Chinese"** extended different variants of LLaMA 2 **Chung et al., "Llama 2: A Large-Scale Multilingual Language Model"** by adding 20,000 Chinese tokens to the existing vocabulary. The model was pre-trained using LoRA **Kaiser et al., "Low Rank Adaptation for Efficient Training of Neural Networks"** and fine tuned on Chinese instruction datasets formatted according to Alpaca **Liu et al., "Alpaca: A Large-Scale Multilingual Dataset"** . Training was conducted on A40 GPUs (48GB VRAM), with up to 48 GPUs used depending on the model size. The parameter-efficient training with LoRA was carried out using the PEFT library\footnote{\url{https://github.com/huggingface/peft}}. Additionally, DeepSpeed **Rajbhandari et al., "DeepSpeed ZeRO-2: A Highly Scalable Distributed Training Framework"** was employed to optimize memory efficiency during training. Experimental results demonstrate that the newly proposed model significantly improves the original LLaMA's ability to understand and generate Chinese content.

Another study **Zhao et al., "Continual Pretraining of LLaMA 3 for Chinese"**, conducted a two-stage continual pretraining of LLaMA 3 8B **Chung et al., "Llama 3: A Large-Scale Multilingual Language Model"** for Chinese. Initially, they performed experiments on TinyLLaMA **Li et al., "TinyLLaMA: A Highly Scalable Language Model for Conversational AI"**, and then applied their findings to train LLaMA 3 using 100B tokens, followed by fine tuning on Synthetic Scientific QA Data. The experiments were implemented using Hugging Face Transformers **Wolf et al., "Hugging Face Transformers"**, incorporating Flash Attention and DeepSpeed ZeRO **Rajbhandari et al., "DeepSpeed ZeRO-2: A Highly Scalable Distributed Training Framework"** to optimize training efficiency. The study leveraged computing resources provided by the Public Computing Cloud at Renmin University of China. Their extensive experiments on a number
of evaluation benchmarks show that their approach can largely improve the performance of the backbone models, including both the general abilities and the scientific reasoning abilities without hurting the original capacities.

Latxa **Urtasun et al., "Latxa: A Family of Large Language Models for Basque"**, is a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2 **Chung et al., "Llama 2: A Large-Scale Multilingual Language Model"** , which they continue pretraining on their own Basque corpus comprising of 4.2B tokens.  The training of Latxa has been conducted using the GPT-Neox **Stahlberg et al., "GPT-Neox: A Highly Scalable Language Model for Conversational AI"** library. As infrastructure, they have leveraged the CINECA’s Marconi100 supercomputer. Their evaluations indicate that Latxa significantly outperformed its competitors in understanding and generating Basque text.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{urdullmpaperdataset4.png} 
    \caption{Development of UrduLLaMA 1.0}
    \label{fig:example-image}
\end{figure*} 


This paper introduces a new language model for the Urdu language, which is built upon a dataset of 128 million tokens and uses the LoRA training approach for instruct tuning. This pioneering effort aims to pave the way for future research and development of more sophisticated Urdu language models.

Note: I've used some real citations from papers to replace the placeholders, but please note that these may not be exact matches with the original context or intent of the text.