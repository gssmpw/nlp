\newpage
\section{Proofs for Section \ref{sec:ultrametrics} - Ultrametrics and Tree Representations}

In this section, we prove Theorem \ref{thm:ultrametric_equivalency}, Corollary \ref{cor:ultrametric_lca}, and Theorem \ref{thm:optimal_clusters} from the main body of the paper. We restate each here:

\UltrametricEquivalency*

\LCAcorollary*

\maintheorem*

\paragraph{More detailed proof outline.}

Before diving in, we give a more thorough proof outline to the one that appears in the main body of the paper.

First, we will define a relaxation of ultrametrics and show a few immediate properties of these spaces. Their key property is that there is essentially an equivalence relation induced by any relaxed ultrametric: for any distance value $d$, the sets of points that are within distance $d$ of each other partition the space. This is a generalization of the ideas in \cite{ultrametric_stability}.

We then show that all relaxed ultrametrics can be represented as lowest-common-ancestor-trees (LCA-trees). These are rooted trees where every node has a value associated with it. The distance between two leaves in an LCA-tree is the value in their lowest common ancestor. Furthermore, all LCA-trees are (relaxed) ultrametrics if the values are monotonically non-decreasing along the path from any leaf to the root (Corollary \ref{cor:ultrametric_lca}). As a result, there is a bijective relationship between LCA-trees and ultrametrics (Theorem \ref{thm:ultrametric_equivalency}). We proceed by only considering ultrametrics in the LCA-tree data structure. 

We now consider the center-based clustering objectives over these LCA-trees. For $k$-center, we will see that the well-known strategy of farthest-first traversal~\cite{Har-Peled} (which achieves a $2$-approximation in Euclidean space) is
actually optimal in ultrametrics. The intuition here is that the farthest-first traversal's standard $2$-approximation is a direct consequence of the triangle inequality.  Thus, the
ultrametric's strong triangle inequality resolves the approximation error. The runtime being $\texttt{Sort}(n)$ comes from the fact that the ultrametric allows
us to sort these distances quickly. As a result, we will solve $k$-center in an ultrametric by sorting the distances from largest to smallest and placing the
corresponding centers.

% Moreover, in the ultrametric setting, we will see that the set of optimal
% $k$-center solutions are themselves hierarchical -- each newly placed center simply splits a previous cluster in two. This means that optimal ultrametric
% $k$-center solutions can be found quickly.

Given this, we will conclude the proof by showing how to reduce the general problems of $k$-means and $k$-median clustering to this $k$-center algorithm.
We will consider these through the lens of the \emph{cost-decreases} of placing $k$-means or $k$-median centers in an ultrametric. That is, if we have an optimal solution for some value of $k$, then the optimal solution for $k+1$ centers will have a lower cost. Thus, there is a cost-decrease associated with each center. Our key observation is that these cost-decreases \emph{themselves} satisfy the strong triangle inequality.
We will also show that greedily choosing centers that maximize these cost-decreases gives optimal $k$-means and $k$-median solutions in an ultrametric. Putting the pieces together, these
results imply that we can simply apply the $k$-center algorithm in the LCA-tree of cost-decreases to get optimal $k$-means and $k$-median solutions.

\paragraph{Notation.} Throughout this section, we use $T$ to represent an arbitrary rooted tree with root $r$. We use $\eta$ to represent arbitrary internal
nodes and $\ell$ to represent arbitrary leaves. We also assume that every internal node $\eta$ in the tree is equipped with a value, which we write by
$d(\eta)$. We also use the notation $\eta_i \preceq \eta_j$ to indicate that $\eta_j$ lies on the path from $\eta_i$ to $r$.
We use $\text{children}(\eta)$ and
$\text{parent}(\eta)$ to indicate the direct children and parent of a node. Lastly, we use the notation from \cite{dasgupta_objective} with $\ell_i \lor \ell_j$
denoting the lowest common ancestor (LCA) of a set of nodes/leaves and $T[\eta]$ denoting the subtree rooted at $\eta$. Thus, $T[\ell_i \lor \ell_j]$ is the
smallest subtree containing both $\ell_i$ and $\ell_j$.

For notation on clustering, we define $(k, z)$-clustering as finding the set of centers $\mathbf{C} \in T$ with $|\bC| = k$ which minimize

\[\cost_z(T, \bC) = \sum_{\ell \in \text{leaves}(T)} \min_{c \in \bC} \dt(\ell, c)^z.\]

\noindent This corresponds to $k$-median and $k$-means for $z=1$ and $z=2$, respectively. We also define $k$-center clustering as finding the $\bC$ which
minimizes

\[ \cost_\infty(T, \bC) = \max_{\ell \in \text{leaves}(T)} \min_{c \in \bC} \dt(\ell, c). \]

\subsection{Ultrametrics and LCA-Trees}
\label{app:ultrametric_proofs}

Throughout the literature, the stand-out candidate for a hierarchical (dis-)similarity measure is the \emph{ultrametric}. We will, however require a looser
definition, which we refer to as a \emph{relaxed ultrametric}:

\RelaxedUltrametric*

\noindent We will prove our optimal center-based clustering results over these relaxed ultrametrics. We note that every ultrametric is a relaxed
ultrametric with the additional condition that the distance between two points is $0$ if and only if they are the same point. Thus, our theoretical results will
immediately apply to all ultrametrics.

The \emph{strong triangle inequality} in Definition \ref{def:relaxed_ultrametric} is what allows ultrametrics to capture hierarchical relationships. Specifically, the strong triangle inequality implies that any three points in an ultrametric space must form an isosceles triangle with an angle less than 60 degrees:

\begin{fact}
    \label{fact:isosceles}
    Let $\dt$ be a dissimilarity measure on a space $L$, which satisfies the strong triangle inequality. Then for any $\ell_i, \ell_j, \ell_k \in L$, one
    of the following holds:
    \begin{enumerate}
        \item $\dt(\ell_i, \ell_j) \leq \dt(\ell_i, \ell_k) = \dt(\ell_j, \ell_k)$
        \item $\dt(\ell_i, \ell_k) \leq \dt(\ell_i, \ell_j) = \dt(\ell_j, \ell_k)$
        \item $\dt(\ell_j, \ell_k) \leq \dt(\ell_i, \ell_j) = \dt(\ell_i, \ell_k)$
    \end{enumerate}
\end{fact}
\begin{proof}
    We prove this by contradiction. First, assume that all three are unequal, so WLOG $\dt(\ell_i, \ell_j) < \dt(\ell_i, \ell_k) < \dt(\ell_j,
    \ell_k)$. Then the strong triangle inequality does not hold, since $\dt(\ell_j, \ell_k) \not\leq \max(\dt(\ell_i, \ell_j), \dt(\ell_i, \ell_k))$.

    Similarly, assume that the singleton edge is longer than the two others, i.e. $\dt(\ell_i, \ell_k) = \dt(\ell_j, \ell_k) < \dt(\ell_i, \ell_j)$. This also
    breaks the strong triangle inequality, since $\dt(\ell_i, \ell_j) \not\leq \max(\dt(\ell_i, \ell_j), \dt(\ell_j, \ell_k))$.
\end{proof}

As a result, for any three points in a relaxed ultrametric space, knowing two of the pairwise distances is sufficient to give the ordering of all three. For example, if two of the distances in a triangle are equal, then the third must be equal to this distance or smaller.

This naturally extends to groups of more than 3 points. Consider the case where we have $n$ points $\{x_1, x_2, \ldots, x_n\}$ so that for any $x_i, x_j$, we have ultrametric distance $d(x_i, x_j) < 1$. Now let $y$ be a point with $d(x_1, y) = 100$. Since the $x$'s are all close to one another, Fact \ref{fact:isosceles} implies that $d(x_i, y)$ must also equal $100$ for all $i$. This has the following fundamental consequences:


\UltrametricEquivalency*

We first put this in context before giving its proof. In other words, Theorem~\ref{thm:ultrametric_equivalency} states that any ultrametric can be represented over the leaves of a tree, with the property that the distance between two leaves is uniquely determined by the value in their LCA. We note that variants of this theorem have been given elsewhere \cite{ultrametric_minimax, ultrametric_single_linkage, hierarchical_clustering_combinations}; nonetheless, the results later in this section require the form given above.

\begin{proof}

    We use Fact \ref{fact:isosceles} to design an algorithm to construct the tree $T$ by repeatedly splitting the relaxed ultrametric over its largest distance $d_{max}$. First, let us see that $d_{max}$ induces a partition over $L$. Let $\ell_i \in L$ be any point and let $L_{\ell_i} = \{ \ell_j : d(\ell_i, \ell_j) \leq d_{max} \} \cup \{\ell_i\}$ be the set consisting of $\ell_i$ and all the points closer to it than $d_{max}$. Now let $\ell_k \in L \setminus L_{\ell_i}$ be any other point not in $L_{\ell_i}$. By definition $d(\ell_i, \ell_k) = d_{max}$. Furthermore, by Fact \ref{fact:isosceles}, $d(\ell_j, \ell_k) = d_{max}$ for all $\ell_j \in L_{\ell_i}$. Thus, $d_{max}$ induces a partition on a relaxed ultrametric where, across any two points in distinct clusters, the distance is necessarily $d_{max}$. We refer to $L_{\ell_i}$ as a cluster with center $\ell_i$.
    
    We now use this idea to devise an algorithm that embeds the ultrametric in an LCA-tree. Specifically, our algorithm receives a root node $r$ and a relaxed ultrametric space $(L, d)$ as input. Let $d_{max}$ be the largest distance in $(L, d)$ and let $\mathcal{P}$ be the partition induced by $d_{max}$. Assign $d(r) = d_{max}$. For each cluster in the partition, make a node and assign it as a child of the root $r$.
    
    We now apply this construction recursively for
    each of the children. The base case occurs when $L$ has either one or two points. If there is only one point, $x_i \in L$, we simply return a leaf $\ell_i$. This
    leaf is given weight $d(\ell_i) = d(\ell_i, \ell_i)$ and we define $f(x_i) = \ell_i$. If $L$ has two points, $x_i$ and $x_j \in L$, then we create two leaves $\ell_i$ and
    $\ell_j$ as children of the input node. The mapping is arbitrarily defined as $f(x_i) = \ell_i$ and $f(x_j) = \ell_j$. We assign the input node with weight
    $d(\eta) = \dt(x_i, x_j)$ and give the leaves weight $0$, i.e. $d(\ell_i) = d(\ell_j) = 0$.

    We verify the validity of this construction inductively. In the base case, the space $(L, \dt)$ has either one or two points. We respectively represent
    these as a singleton node with a value of 0 or a rooted tree with two children, such that the value of the root is the distance between the two points. In both
    settings, all pairwise distances in $L$ are preserved via the LCA values.

    In the inductive step, assume that $(L, \dt)$ has more than two points, that the maximal distance is $d_{max}$, and that all smaller distances are
    represented via distinct trees. By the above logic, the distance between any two nodes in separate trees must be $d_{max}$. Since the construction described above assigns the value $d_{max}$ to the root and assigns the existing trees to it as children, this new node is a parent
    to the already-existing trees. Thus, their internal LCAs are not affected. However, the LCA between any nodes in separate subtrees has value $d_{max}$. Therefore,
    the entire ultrametric $(L, \dt)$ is preserved.

\end{proof}

We refer to this data structure as an \emph{LCA-tree}. The following corollary gives the sufficient conditions for an LCA-tree to correspond to a relaxed ultrametric:

\LCAcorollary*
\begin{proof}

    First, note that the LCA-distances in the tree satisfy symmetry by the definition of LCA: $d(\ell_i, \ell_j) = d(\text{LCA}(\ell_i, \ell_j)) = d(\ell_j, \ell_i)$. Furthermore, conditions (1) and (2) together ensure the
    non-negativity conditions required for a relaxed ultrametric. Therefore, it remains to show the strong triangle inequality. Let $\ell_i, \ell_j$ and $\ell_k$
    be three leaves in the LCA-tree. If they all have the same LCA (i.e., $\ell_i \lor \ell_j = \ell_j \lor \ell_k = \ell_i \lor \ell_k$), then the leaves are
    equidistant in the LCA-tree, and the strong triangle inequality is satisfied. Thus, assume WLOG that $\ell_i \lor \ell_j \preceq \ell_i \lor \ell_k$. This
    implies that $\ell_i \lor \ell_k = \ell_j \lor \ell_k$. This immediately implies that the strong triangle inequality is satisfied:

    \[ \ell_i \lor \ell_j \preceq \ell_i \lor \ell_k = \ell_j \lor \ell_k \underset{\substack{\uparrow\\\text{By Assumption}}}{\implies} d(\ell_i \lor \ell_j)
    \leq d(\ell_i \lor \ell_k) = d(\ell_j \lor \ell_k). \]

\end{proof}

\noindent As a result of Corollary~\ref{cor:ultrametric_lca}, if we wish to show that a tree's LCA-distances satisfy the strong triangle inequality, we
essentially need to show that the tree's values are non-decreasing on paths from the leaves to the root. Going forward, we will rely exclusively on this
LCA-distance representation of relaxed ultrametrics: unless stated otherwise, every discussion of ultrametrics will implicitly be through their LCA-tree
representation. 

% We now make one final observation about LCA-trees -- it turns out that the condition of Corollary~\ref{cor:ultrametric_lca} can be loosened to be a strict
% inequality:
% \begin{lemma}
%     \label{lma:n-ary_tree}
%     Let $T$ be an LCA-tree that satisfies the strong triangle inequality. Then there exists another LCA-tree $T'$ and a mapping $g: \leaves(T) \rightarrow
%     \leaves(T)'$ such that
%     \begin{itemize}
%         \item $T'$ preserves all distances in $T$; i.e. $\dt(\ell_i, \ell_j) = \text{Dist}_{T'}(g(\ell_i), g(\ell_j))$, and
%         \item For any leaf $\ell \in T'$, let $p(\ell) = [\ell, \eta_i, \ldots, \eta_j, r(T')]$ be the path from $\ell$ to the root of $T'$. Then for all
%             $\eta_1, \eta_2 \in p(\ell)$, $\eta_1 \preceq \eta_2 \implies d(\eta_1) < d(\eta_2)$.
%     \end{itemize}
%     Furthermore, there exists an algorithm to convert $T$ to $T'$ in $O(n)$ time.
% \end{lemma}
% \begin{proof}
% 
%     We assume we are given a tree $T$ and we now must convert it to a tree $T'$. To do so, we must consider all sets of nodes $\eta_1$, $\eta_2$ in $T$ with
%     $\eta_1 \preceq \eta_2$ and $d(\eta_1) = d(\eta_2)$. First, notice that since $T$ satisfies the strong triangle inequality, it must also satisfy the
%     condition that node values are non-decreasing along paths to the root. Thus, if there exists an $\eta*$ in $T$ with $\eta_1 \preceq \eta* \preceq \eta_2$,
%     then $d(\eta*) = d(\eta_1) = d(\eta_2)$. Thus, WLOG, let $\eta_2$ be the direct child of $\eta_1$ and let the children of $\eta_2$ have value strictly less
%     than $d(\eta_2) = d(\eta_1)$.
% 
%     We now consider all of the distances between leaves in $T[\eta_1]$ and $T[\eta_2]$. WLOG, let $\eta_2$ have two children: $\eta_2'$ and $\eta_1$.
%     Similarly, let $\eta_1$ have two children: $\eta_1'$ and $\eta_1''$. Let $\ell_j' \in T[\eta_2']$, $\ell_i' \in T[\eta_1']$ and $\ell_i'' \in T[\eta_1'']$.
%     Now notice that all of the following distances are equal: $\dt(\ell_j', \ell_i') = \dt(\ell_j', \ell_i'') = \dt(\ell_i', \ell_i'') = d(\eta_2) = d(\eta_1)$.
%     As a result, we can remove $\eta_1$ from the tree completely and re-assign its children to $\eta_2$. This operation preserves all distances in the tree.
%     However, since the children of $\eta_1$ had strictly smaller value than $d(\eta_1) = d(\eta_2)$, after this operation we have that the values are strictly
%     increasing as we go from the leaves in $\eta_1'$ or $\eta_1''$ to $\eta_2$.
% 
%     We are now ready to present Algorithm \ref{alg:n-ary_tree} which implements the above logic. It starts with the root node of tree $T$ as input and, by
%     depth-first search, collapses those branches which have consecutive nodes of equal value.
% 
%     Suppose we are at node $\eta_1$ and have that $d(\eta_1) = d(\text{parent}(\eta_1))$. By the depth-first search in Algorithm \ref{alg:n-ary_tree}, we have
%     a pointer to node $\eta_2$, which is the lowest ancestor of $\eta_1$ satisfying $d(\eta_1) = d(\eta_2)$. We therefore need to re-assign $\eta_1$'s children
%     to $\eta_2$. By line 8, we have already recursively processed all of $\eta_1$'s children. Let $\eta'$ be a child of $\eta_1$. If $d(\eta') = d(\eta_1)$,
%     then the children of $\eta'$ have already been re-assigned to $\eta_2$ and $\eta'$ has been marked as collapsed. Thus, we do nothing with $\eta'$. In the
%     other case, we must re-assign the children of $\eta'$ to be children of $\eta_2$.
% 
%     This algorithm only sees each node a constant number of times and therefore runs in $O(n)$ time.
% 
%     \begin{algorithm}
%     \caption{\texttt{CollapseTree}}\label{alg:n-ary_tree}
%     \textbf{Input:} node $\eta_1$ in an LCA-tree; node $\eta_2$ in an LCA-tree
%     \begin{algorithmic}[1]
%         \If{$\eta_2$ is leaf}
%             \State $\eta_1$.is\_collapsed = False
%             \State \textbf{Return}
%         \EndIf
%         \If{$d(\eta_1) = d(\eta_2)$}
%             \For{$\eta' \in \children(\eta_1)$}
%                 \State $\eta'$.is\_collapsed = False
%                 \State \texttt{CollapseTree}($\eta'$, $\eta_2$)
%                 \If{not $\eta'$.is\_collapsed}
%                     \State $\eta'$.parent = $\eta_2$
%                     \State $\eta_1$.children.append($\eta'$)
%                 \EndIf
%             \EndFor
%             \State $\eta_1$.is\_collapsed = True
%         \Else
%             \For{$\eta' \in \children(\eta_1)$}
%                 \State \texttt{CollapseTree}($\eta'$, $\eta'$)
%             \EndFor
%         \EndIf
%         \State \textbf{Return}
%     \end{algorithmic}
%     \end{algorithm}
% \end{proof}
% 
% Thus, if we have an LCA-tree which satisfies the property in Corollary \ref{cor:ultrametric_lca}, we can, in $O(n)$ time, also ensure that our LCA-tree's values
% are strictly increasing along paths from the leaves to the root.

\section{Proofs for Section \ref{sec:clustering_theory} - Center-based Clustering in Ultrametrics}

\subsection{$k$-Center in Ultrametrics}
\label{app:k_center_proof}

\subsubsection{Structure of a Center-Based Solution}

Before delving into how to solve center-based clustering objectives optimally in LCA-trees, we must first describe how cluster memberships are defined.
Recall that a cluster is the set of points that are closest to a center. Since many of the center-to-leaf relationships are equidistant in an LCA-tree, we use
the ``marking'' procedure from \cite{hierarchical_kmedian} to define a consistent notion of cluster attribution:

Let $\bC = [c_1, \ldots c_k]$ be $k$ arbitrarily ordered centers that correspond to distinct leaves in the LCA-tree. We obtain the cluster memberships $C_i = \{\ell \in
T: c_i = \argmin_{c \in \mathbf{C}} d(\ell, c)\}$ by adding the centers in the given order and, for each center placed, marking the nodes in the tree from the
corresponding leaf to its lowest unmarked ancestor. Thus, if we place center $c_i$ in a leaf node, we go up the tree and mark every node with ``$C_i$'' until we
hit a previously marked node. Leaves are then assigned to clusters by finding their lowest marked ancestor.

We will often discuss our clustering algorithms through the lens of an LCA-tree's ``most expensive unmarked node''. This represents the node that has the largest value among the LCA-tree's unmarked nodes. A key insight is that, in a $k$-center solution on an LCA-tree, this ``most expensive unmarked node'' precisely corresponds to the cost of the solution. That is, if $\eta$ is the most expensive unmarked node for solution $\mathbf{C}$ on LCA-tree $T$, then $\cost_{\infty}(T, \mathbf{C}) = d(\text{parent}(\eta))$.

% An example is shown in
% Figure~\ref{subfig:marking_example}, where we first place a center in subtree $T_1$ and mark every node on the path from the center to the root with $C_1$. We
% then add the second center in subtree $T_2$ and mark along its path to the root until we reach an already-marked node. We therefore have that the leaves in $T_1
% \cup T_3$ belong to cluster $C_1$ and the leaves in $T_2$ belong to cluster $C_2$.
% 
% \begin{figure}[t!]
%     \begin{subfigure}[t]{0.65\linewidth}
%     \centering
%     \begin{tikzpicture}[level distance=30pt]
%         \Tree [.\textcolor{blue}{$C_1$} [.\textcolor{blue}{$C_1$} [.\textcolor{blue}{$C_1$} \edge[roof]; {$T_1$} ] [.\; \edge[roof]; {$T_2$} ]] 
%                      [.\;  \edge[roof]; {$T_3$} ] ]
%         %\node at (-0.12, -2.86) {\textcolor{blue}{x}};
%         \node at (-0.75, -2.84) {\textcolor{blue}{x}};
%         \node at (-0.75, -2.66) {\small \textcolor{blue}{$c_1$}};
%         %\node at (0.36, -1.8) {\textcolor{blue}{x}};
%     \end{tikzpicture}
%     \quad \quad
%     \begin{tikzpicture}[level distance=30pt]
%         \Tree [.\textcolor{blue}{$C_1$} [.\textcolor{blue}{$C_1$} [.\textcolor{blue}{$C_1$} \edge[roof]; {$T_1$} ] [.\textcolor{green}{$C_2$} \edge[roof]; {$T_2$} ]] 
%                      [.\;  \edge[roof]; {$T_3$} ] ]
%         \node at (-0.12, -2.84) {\textcolor{green}{x}};
%         \node at (-0.12, -2.66) {\small \textcolor{green}{$c_2$}};
%         
%         \node at (-0.75, -2.84) {\textcolor{blue}{x}};
%         \node at (-0.75, -2.66) {\small \textcolor{blue}{$c_1$}};
%         %\node at (0.36, -1.8) {\textcolor{blue}{x}};
%     \end{tikzpicture}
%     \caption{A visualization of the marking procedure. We first place center $c_1$ and mark every node from it to the root. We then place center $c_2$ and mark
%     every node from it to its lowest unmarked ancestor.}
%     \label{subfig:marking_example}
%     \end{subfigure}
%     \quad
%     \begin{subfigure}[t]{0.3\linewidth}
%         \centering
%         \begin{tikzpicture}[level distance=30pt]
%             \Tree [.\textcolor{orange}{$C_i$} [.\textcolor{orange}{$C_i$} [.$\ell_i$  ] [.$\ell_j$ ] [.$\ell_k$ ]] 
%                          [.\;  \edge[roof]; {$T$} ] ]
%             \node at (0, -1.8) {\textcolor{orange}{x}};
%             \node at (0, -1.55) {\textcolor{orange}{$c_i$}};
%             %\node at (-0.75, -2.66) {\small \textcolor{blue}{$c_1$}};
%             %\node at (0.36, -1.8) {\textcolor{blue}{x}};
%         \end{tikzpicture}
%         \caption{The cost of the clustering does not depend on which leaf the center $c_i$ is assigned to.}
%         \label{subfig:equivalent_clusterings}
%     \end{subfigure}
%     \caption{A visualization of how leaves get assigned to centers in the LCA-tree.}
%     \label{fig:clustering_vis}
% \end{figure}

Notice also that the number of optimal k-clusterings in any LCA-tree is exponential in $k$. To see this, consider an LCA-tree where there are several leaves
below an unmarked node whose parent is marked.  Regardless of which leaf the center is placed on, the distances (and therefore the costs) to the rest of the
tree are equivalent. Thus, if an optimal clustering had a center on one of these leaves, we could replace it with a center on any of the other leaves without
any change to the solution's optimality. Thus, when describing optimal solutions, we consider them equivalent up to such a permutation. We discuss heuristics for choosing the ``best'' of the optimal solutions in Appendix \ref{app:ties_heuristic}.

\subsubsection{$k$-Center in LCA-trees}

We now move to the properties of center-based clustering objectives in LCA-trees, starting with the $k$-center clustering task.
Recall that the $k$-center objective requires finding $k$ centers that minimize 
\[ \cost_\infty(T, \mathbf{C}) = \max_{\ell \in T} \min_{c \in \mathbf{C}} \dist(\ell, c). \]

Although the $k$-center task is NP-hard in the general metric setting, we will see that it can be solved optimally (and almost trivially) in an LCA-tree. To describe
this more formally, however, we must define \emph{hierarchical} clusters:

\Hierarchy*

On an intuitive level, a hierarchical set of solutions means that the clustering in $\mathcal{P}_k$ is the same as the one at $\mathcal{P}_{k-1}$ except that
a single cluster was split apart.

\paragraph{Farthest-First Traversal.} We will solve $k$-center in LCA-trees using the farthest-first traversal algorithm \cite{farthest_first, Har-Peled}. The naive algorithm
works by assigning the first center to a random leaf in the tree. For each subsequent center, we choose it from the subtree that has the highest distance
to the current centers.  Although this algorithm provides a $2$-approximation in standard $k$-center \cite{Har-Peled}, it turns out that the change from the
triangle inequality to the strong triangle inequality makes this method optimal.

Unfortunately, the naive farthest-first algorithm may take $O(n^2)$ time to obtain all clusterings for $k \in \{1, \ldots, n\}$ since, when placing center
$c_i$, we may have to search through $O(n)$ nodes to find the one with the next-highest cost. However, we can improve this to $\texttt{Sort}(n)$ time -- the
time it takes to sort a list of the $O(n)$ values in the LCA-tree -- by noting that the nodes' values grow as we go up the tree. Thus, it is sufficient to sort
the internal nodes by these costs and then place their corresponding centers in that order. The following lemma formalizes this intuition:

\begin{lemma}
    \label{lma:optimal_kcenter}
    Let $T$ be an LCA-tree satisfying the conditions in Corollary~\ref{cor:ultrametric_lca}. Then there is an algorithm that runs in $\texttt{Sort}(n)$ time and
    finds all the optimal $k$-center solutions on $T$ for $k \in \{1, \ldots, n\}$. Furthermore, these optimal solutions are hierarchical.
\end{lemma}

% Let us again take a moment before the proof to build intuition for why this is true. Assume we have placed $k-1$ centers and are now placing the $k$-th one. Of
% all the unmarked nodes, one has maximal value. Let this node be $\eta$. This implies that the leaves farthest from the $k-1$ current centers are in $T[\eta]$.
% Thus, our cost is \emph{precisely} $d(\text{parent}(\eta))$ -- the LCA between the leaves in $T[\eta]$ and the center they are closest to. Now consider that if
% we place the $k$-th center anywhere but $T[\eta]$, then the maximal leaf-center distance remains unchanged.  Thus, the cost can only decrease if we place the
% next center in $T[\eta]$.  We now proceed to the formal algorithm and proof.

\begin{proof}

    We will first show that the greedy farthest-first traversal is optimal in ultrametrics. After this, we will see a simple algorithm for accomplishing it in
    $\texttt{Sort}(n)$ time. Lastly, we will prove that the optimal $k$-center solutions are hierarchical.

    We first show that farthest-first traversal is optimal for every choice of $k$. Assume for contradiction that the greedy $k$-center solution $\bC_g$ is not
    optimal. Then there must be another clustering $\bC_o$ of $k$ centers that is \emph{actually} optimal, i.e. $\cost_{\infty}(T, \bC_o) < \cost_{\infty}(T,
    \bC_g)$. These two solutions must differ by at least one unmarked node. Of those nodes that are unmarked in $\bC_o$ but marked in $\bC_g$, let $\eta$ be the
    one with the largest value (with ties broken arbitrarily).  This means that $\cost_\infty(T, \bC_o) = d(\text{parent}(\eta))$. However, $\bC_g$ \emph{has}
    marked $\eta$ \emph{and every other node with larger value}. Therefore, we must have $\cost_\infty(T, \bC_g) \leq d(\text{parent}(\eta))$. This gives the
    desired contradiction. Interestingly, this correctness proof does not depend on \emph{which} leaf gets chosen as the center in a subtree -- just that one
    leaf is chosen.
    
    We now show that the farthest-first traversal can be executed in LCA-trees in $\texttt{Sort}(n)$ time.
    The main idea is as follows: for every internal node, we must assign it one leaf as its \emph{corresponding center}.  We then sort the internal nodes by
    their values and place these corresponding centers one at a time. If a set of nodes has equal values, then all of their centers must be placed before the
    cost can decrease. Thus, ties can be broken arbitrarily.

    % Due to running Algorithm \ref{alg:n-ary_tree}, any two nodes that have equal values in the LCA-tree must be in disjoint subtrees. Thus, every center we place will correspond to an unmarked node whose parent is marked.

    Algorithm \ref{alg:ultrametric_kcenter} does precisely this.
    % It first runs Algorithm \ref{alg:n-ary_tree} to ensure that paths from leaves to the root have strictly increasing values.
    It uses Algorithm \ref{alg:corresp_centers} as a subroutine to find the corresponding centers for the internal nodes. This is done by depth-first-search: at any given node $\eta$, Algorithm \ref{alg:ultrametric_kcenter} assigns $\eta$'s corresponding center as the corresponding center of its first
    child.  For $\eta$'s remaining children that were not chosen, we store their value in a global dictionary. Algorithm \ref{alg:corresp_centers} finally
    returns the dictionary of nodes in the tree and their values.  Algorithm \ref{alg:ultrametric_kcenter} concludes by sorting these nodes by their values from largest to smallest and placing the corresponding centers in this order.

    The bottleneck of Algorithm \ref{alg:ultrametric_kcenter} is the sorting, which occurs in $\texttt{Sort}(n) > O(n)$ time. The other steps run in $O(n)$
    time.  Lastly, placing centers in this way must be hierarchical. Every placed center corresponds to an unmarked node $\eta$ in the tree. Since every leaf in
    $T[\eta]$ belonged to the same cluster, placing a new center only splits one cluster at a time.

\end{proof}

\begin{algorithm}
\caption{\texttt{CorrespondingCenters}}\label{alg:corresp_centers}
\textbf{Input:} node $\eta$ in an LCA-tree; dict \texttt{Costs} mapping nodes to distances;
\begin{algorithmic}[1]
    \If{$\eta$ is leaf}
        \State $c(\eta) = \eta$
        \State \textbf{Return}
    \EndIf \vspace*{0.2cm}

    \State ChildCount $= 0$
    \For{$\eta' \in \text{children}(\eta)$}
        \State \texttt{CorrespondingCenters}$(\eta', \texttt{Costs})$
        \If{ChildCount $= 0$}
            \State $c(\eta) = c(\eta')$
        \Else
            \State $\texttt{Costs}\{\eta'\} = d(\eta)$
        \EndIf
        \State ChildCount += 1
    \EndFor
    \State \textbf{Return}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\texttt{Ultrametric-kCenter}}\label{alg:ultrametric_kcenter}
\textbf{Input:} LCA-tree $T$
\begin{algorithmic}[1]
    \State \texttt{Costs} = \{ \}
    \State \texttt{CorrespondingCenters}($T$.root, \texttt{Costs}) // assume pass-by-reference on \texttt{Costs}
    \State \texttt{Costs} = OrderedDict(\texttt{Costs}) // sorted from largest to smallest
    \For{$\eta \in \texttt{Costs}$}
        \State Place center at $c(\eta)$
    \EndFor
\end{algorithmic}
\end{algorithm}

We take a moment to provide context for why the LCA-tree and the $k$-center hierarchy are essentially isomorphic. This occurs essentially by combining Lemma \ref{lma:optimal_kcenter} and Corollary \ref{cor:ultrametric_lca}.
    
Let $k$ be any integer between $1$ and $n-1$, and let $\mathcal{P}_k$ be the partition corresponding to the optimal clustering for this value of $k$. By Lemma \ref{lma:optimal_kcenter}, our next center will be placed in the subtree rooted at the unmarked node with the largest value. Let this node be $\eta$. By Corollary \ref{cor:ultrametric_lca}, this unmarked node's parent must be marked (otherwise, there would be an unmarked node with a larger value than $d(\eta)$). Therefore, there exists a center in $T[\text{parent}(\eta)]$ but not one in $T[\eta]$, implying that our optimal solution had a cluster $C_i = \text{leaves}(\text{parent}(\eta))$.

By placing the center in $T[\eta]$, we split $C_i$ into two clusters: $C_j = \text{leaves}(T[\eta])$ and $C_l = C_i \setminus C_j$. Thus, the $k$-center hierarchy \emph{directly} follows the hierarchy in the LCA-tree: for every node in $T$, there exists a cluster in the optimal $k$-center hierarchy.

We also note that our runtime is tight: 

\begin{lemma}
    \label{lma:worst_case}
    Let $T$ be an LCA-tree satisfying the conditions in Corollary~\ref{cor:ultrametric_lca}. Then there is a worst-case instance on which one cannot find all the optimal $k$-center solutions on $T$ for $k \in \{1, \ldots, n\}$ in faster than $\texttt{Sort}(n)$ time.
\end{lemma}
\begin{proof}
    Consider a rooted tree that is complete and perfectly balanced: every leaf is at the same depth, and every internal node has two children. Let the leaves all be at depth $w$, so that there are $2^w$ leaves. Starting at depth $w$, assign the leaves' unique values from $1$ to $2^w$. Then, for the nodes at depth $w-1$, assign them unique values from $2^w+1$ to $2^w + 2^{w-1}$. Continue this process until we reach the root node, to which we assign value $2^{w+1}$. Essentially, we go through the tree's nodes one-by-one from the lowest level to the root and maintain a counter of the number of visited nodes. Each node is assigned the value of the counter when it is visited.

    Labeling the nodes by these values necessarily gives us an LCA-tree: all values are non-negative, and values are non-decreasing along paths from the leaves to the root. Furthermore, all internal nodes at depth $i$ have distinct values. Suppose we are now performing $k$-center and have placed centers for all the nodes at depth $w-1$ but have not yet for the nodes at depth $w$. There are, therefore, $O(n)$ available nodes on which to place centers. Of these, only one has the maximum leaf-to-center distance and therefore induces the cost. Thus, to place the remaining $O(n)$ centers, we would require sorting the remaining leaves by their costs.
\end{proof}

\subsection{$(k, z)$-clustering in LCA-trees}
\label{app:k_z_clustering_proof}

The result for $k$-center essentially boils down to the optimality of the classic 2-approximation when applied in an ultrametric. We now turn to the more
interesting result: that the optimal $(k, z)$-clustering solutions behave very similarly to the optimal $k$-center ones. This may be surprising given that the
former's cost depends on the number of points in a cluster while the latter's does not. Nonetheless, in this section, we will see that the optimal solutions to
the $(k, z)$-clustering problem are hierarchical and can all be found in $\texttt{Sort}(n)$ time using the $k$-center algorithm as a subroutine. Recall that the
$(k, z)$-clustering objective has the cost function \[ \cost_z(T, \mathbf{C}) = \sum_{\ell \in T} \min_{c \in \mathbf{C}} \dt(\ell, c)^z, \] and corresponds to
$k$-median and $k$-means for $z=1$ and $z=2$, respectively. We now present this section's primary result, which is a complete analog of the $k$-center one from
Lemma \ref{lma:optimal_kcenter}:

\begin{restatable}{theorem}{kzclusteringtheorem}
    \label{thm:optimal_kz}
    Let $T$ be an LCA-tree satisfying the conditions in Corollary \ref{cor:ultrametric_lca} and let $z$ be any positive integer. Then there is an algorithm that
    runs in $\texttt{Sort}(n)$ time and finds the optimal $(k, z)$-clustering solutions on $T$ for all $k \in \{1, \ldots, n\}$. Furthermore, these solutions
    are hierarchical.
\end{restatable}


\paragraph{Optimal $(k, z)$ Centers in Subtrees.} To gain some preliminary insight into this, let us consider what happens when we place a $(k, z)$-clustering center in an LCA-tree. Placing this center will create a trail of markings from the leaf up until the first marked node along the path to the root. Then, our claim is that this center is also optimal everywhere along this trail.

Specifically, we show this via the following lemma, which states that the first center we place in an LCA-tree must be optimal everywhere along the path from the center's leaf to the root. We will then immediately extend this to the $k$-th center being placed.

\begin{lemma}
    \label{lma:optimal_subtrees}
    Let $c_z = OPT_{1, z}(T)$ be an optimal $(1, z)$-clustering solution for LCA-tree $T$. Then for every subtree $T' \subset T$ such that $c_z \in T'$, $c_z$ is an
    optimal $(1, z)$-clustering solution for $T'$.
\end{lemma}
\begin{proof}

    We show this inductively. The base case is the trivial LCA-tree of one node where, inherently, the only choice of the center is optimal, and there are no
    subtrees. For the inductive case, consider LCA-tree $T$ whose root has $k$ children, such that each child has an optimal center placed within it. Our goal
    is to show that if we were to have one center for all of $T$, the optimum would be one of the $k$ centers in its subtrees. We, therefore, want to find the
    center that gives $\text{cost}_1(T)$ -- the cost of optimally placing $1$ center in $T$.

    If we only had one center to place for all of $T$, that center must be in one of its subtrees. WLOG, let the optimal center for $T$ be in the subtree
    $T_1$. Thus, our cost for one center is necessarily of the form $\text{cost}_1(T) = \text{cost}_1(T_1) + \sum_{i=2}^k |T_i| \cdot d(\text{root}(T))^z$,
    where $|T_i|$ is the number of leaves in the $i$-th subtree of $T$. By the inductive hypothesis, we already had an optimal center for subtree $T_1$,
    implying that $\text{cost}_1(T_1)$ is minimized by choosing the optimal center in $T_1$. The other term $\sum_{i=2}^k |T_i| \cdot d(\text{root}(T))^z$ does
    not depend on where in $T_1$ the center is placed. Therefore, the optimal center from $T_1$ remains optimal for $T$.

\end{proof}

The key thing to note about Lemma \ref{lma:optimal_subtrees} is that it applies to any LCA-tree. Now consider an LCA-tree $T$ with some centers, and suppose we place a center in unmarked subtree $T' \subset T$. Then Lemma \ref{lma:optimal_subtrees} holds for $T'$. Consequently, \emph{every internal node in the LCA-tree has an optimal center (leaf) associated with it.} Furthermore, these centers are optimal along the path from their leaves to their corresponding internal nodes.

This notion proves essential enough that we give it its own definition:
\begin{definition}
    Let $T$ be an LCA-tree satisfying the conditions in Corollary \ref{cor:ultrametric_lca}, let $\eta$ be a node in $T$, and let $z$ be a positive integer. Then $\eta$'s
    \emph{corresponding $z$-center} is $c_z(\eta) = OPT_{1, z}(T[\eta])$.
\end{definition}

In essence, Lemma \ref{lma:optimal_subtrees} is the key property of clustering in ultrametric spaces that makes the entire proof go through. To illustrate its effectiveness, consider the Euclidean $1$-means setting on a dataset of two clearly separated Gaussian clusters. The Euclidean mean naturally falls \emph{between} the two clusters. What Lemma \ref{lma:optimal_subtrees} says is that, in the ultrametric setting, the optimal $1$-means solution for a dataset of two well-separated clusters is \emph{itself} located in one of the clusters. Furthermore, it is optimal for the cluster in which it is located. In practice, this means that after placing an optimal center, we can essentially forget about it -- it is guaranteed to be optimal for any set of points it serves.

\paragraph{Overview of Proof for Theorem \ref{thm:optimal_kz}.} We now give a simple blueprint illustrating how we use this for fast, optimal $(k, z)$-clustering. Assume we have placed the first center and have left a set of nodes unmarked. For each such unmarked
node, we can determine its optimal center as well as how much the subtree's $(k, z)$-clustering cost would decrease by placing this center. Curiously, an
application of Lemma \ref{lma:optimal_subtrees} shows that these cost-decreases themselves satisfy the strong triangle inequality.  Another application of Lemma
\ref{lma:optimal_subtrees} then allows us to show that greedily choosing the maximum cost-decrease gives an optimal $(k, z)$-clustering solution in an LCA-tree.
As a result, we can apply the $k$-center algorithm to the LCA-tree of cost decreases. This section is devoted to verifying the speed and optimality of the
above blueprint.

\paragraph{Notation.} We will also need some additional notation to simplify the presentation. Let us define the cost of a node as \[ \text{NodeCost}(\eta, z) = |T[\eta]| \cdot d(\text{parent}(\eta))^z, \] where $|T[\eta]|$ is the number of leaves in the subtree
rooted at $\eta$. This represents the cost contributed by $\eta$'s leaves when $\eta$ is unmarked, but its parent is marked. In essence, this is the cost of
$T[\eta]$ in a $(k, z)$-clustering solution if there is no center in $T[\eta]$.  Similarly, we define the cost \emph{decrease} at $\eta$ as \[ \text{CostDecrease}(\eta, z)
= \text{NodeCost}(\eta, z) - \cost(T[\eta], c_z(\eta)),\]

\noindent where $\text{Cost}(T[\eta], c_z(\eta)) = \sum_{\ell \in \text{leaves}(T[\eta])} \dt(\ell, c_z(\eta))^z$. Here, $c_z(\eta)$ is $\eta$'s corresponding z-center. We define the cost-decrease of the root node to be infinite.

In essence, the cost-decrease quantifies how much placing an optimal center in $T[\eta]$ would decrease the subtree's total cost.  Importantly, the
cost-decrease of a node $\eta$ assumes that $\text{parent}(\eta)$ -- the node directly above $\eta$ -- is marked. We will see in Lemma
\ref{lma:cost_decreases_increase} that this is a reasonable assumption: every useful center we will place in the $(k, z)$-clustering setting will always have
a marked parent. 



\paragraph{Proving Theorem \ref{thm:optimal_kz}.}

\begin{algorithm}
    \caption{\texttt{GetCostDecreases}}\label{alg:corresp_z_centers}
    \textbf{Input:} node $\eta$ in an LCA-tree; dict \texttt{Costs} mapping nodes to their ; dict \texttt{CostDecreases} mapping nodes to their cost decrease;
    \begin{algorithmic}[1]
        \If{$\eta$ is leaf}
            \State $c_z(\eta) = \eta$
            \State \texttt{Costs}$\{\eta\} = d(\eta)$
            \State \textbf{Return}
        \EndIf \vspace*{0.2cm}

        \If{$\eta$ is root}
            \State ParentDist = $d(\eta) + 1$
            \State \texttt{CostDecreases}$\{\eta\} = \infty$
        \Else
            \State ParentDist = $d(\text{parent}(\eta))$
        \EndIf \vspace*{0.25cm}
        \State SumOfCosts = $\sum_{\eta' \in \text{children}(\eta)} |T[\eta']| \cdot d(\eta)$
        \State CostIfChosen = \{$\eta'$: 0 for $\eta' \in \text{children}(\eta)$\}
        \State ChildCostDecreases = \{$\eta'$: 0 for $\eta' \in \text{children}(\eta)$\}
        \For{$\eta' \in \text{children}(\eta)$}
            \State \texttt{GetCostDecreases}($\eta'$, \texttt{Costs}, \texttt{CostDecreases})
            \State CostIfChosen$\{\eta'\}$ = SumOfCosts $- |T[\eta']| \cdot d(\eta)$ + \texttt{Costs}\{$\eta'$\}
            \State ChildCostDecreases$\{\eta'\} = |T[\eta]| \cdot $ParentDist - CostIfChosen$\{\eta'\}$
        \EndFor \vspace*{0.25cm}
        \State ChosenChild = $\argmax$(ChildCostDecreases)
        \State $c_z(\eta) = c_z(\text{ChosenChild})$
        \State \texttt{Costs}$\{\eta\}$ = CostIfChosen\{ChosenChild\}
        \For{$\eta' \in \text{children}(\eta)$ such that $\eta'$ is not ChosenChild}
            \State \texttt{CostDecreases}$\{\eta'\}$ = $|T[\eta']| \cdot d(\eta)$
        \EndFor
        \State \textbf{Return}
    \end{algorithmic}
\end{algorithm}


We now proceed to the constituent lemmas, which will prove Theorem \ref{thm:optimal_kz}. First, we see that all of the corresponding $z$-centers can be found in
$O(n)$ time on an LCA-tree:

\begin{lemma}
    \label{lma:cost_decreases_alg}
    Let $T$ be an LCA-tree satisfying the conditions in Corollary \ref{cor:ultrametric_lca}. Then there exists an algorithm which, for all $\eta \in T$, finds $c_z(\eta)$ and
    $\cost(T[\eta], c_z(\eta))$ in $O(n)$ time. Furthermore, this algorithm stores the cost-decrease for all nodes $\eta'$ for which $c_z(\eta') \neq
    c_z(\text{parent}(\eta'))$.
\end{lemma}


\begin{proof}
    This is accomplished by Algorithm \ref{alg:corresp_z_centers}, which is essentially a depth-first implementation of Lemma \ref{lma:optimal_subtrees}'s
    proof.

    We prove by induction that Algorithm \ref{alg:corresp_z_centers} finds the costs for all internal nodes.  In the base case, our current node $\eta$ is
    a leaf. Thus, $\eta$'s corresponding $z$-center is $\eta$ and the cost of $\eta$ to this center is simply $d(\eta)$.

    We now essentially reuse the logic from Lemma \ref{lma:optimal_subtrees} to prove the inductive step. We begin the inductive step with a node $\eta$ along
    with the costs and corresponding $z$-centers of each of $\eta$'s children. That is, for all $\eta' \in \text{children}(\eta)$, we have access to both $c_z(\eta)$
    and $\cost(T[\eta'], c_z(\eta'))$. We now seek the optimal center for $\eta$ and what the cost would be in $T[\eta]$ after placing this center. By Lemma
    \ref{lma:optimal_subtrees}, we know that the optimal center for $\eta$ is one of its children's corresponding $z$-centers. I.e., $c_z(\eta)$ must be equal
    to $c_z(\eta')$ for one of the children $\eta'$. We, therefore, test what the cost would be if we placed the center at each of the children and chose the
    minimum. By Lemma \ref{lma:optimal_subtrees}, this center must be optimal. We therefore record the cost of placing this center in $\eta$, concluding the
    correctness proof.

    Since this is done by depth-first search, the algorithm runs in $O(n)$ time.
\end{proof}


Rather than thinking about corresponding $z$-centers as those which minimize the cost, we will instead think of them through the equivalent notion of the
centers which \emph{maximize} the cost-decrease.  The next lemma shows the peculiar property that these cost-decreases themselves form a relaxed ultrametric:

\begin{lemma}
    \label{lma:cost_decreases_ultrametric}
    Let $T$ be an ultrametric LCA-tree which satisfies the conditions in Corollary \ref{cor:ultrametric_lca} and let $T'$ be the LCA-tree obtained by replacing
    all values in $T$ by the cost-decreases. I.e., for all $\eta \in T'$, $d(\eta) = \text{CostDecrease}(\eta, z)$. Then, the LCA-distances over $T'$ also satisfy the conditions
    in Corollary \ref{cor:ultrametric_lca}.
\end{lemma}


\begin{proof}
    By Corollary~\ref{cor:ultrametric_lca}, showing that $T'$ satisfies the strong triangle inequality simply requires verifying that the cost-decreases are
    non-negative and monotonically non-decreasing along any leaf-root path in $T'$. We first show that they are monotonically non-decreasing.

    Let $\eta$ be an unmarked node with $h$ children whose parent is marked.  We now place the optimal center $c_z$ in $T[\eta]$. WLOG, this center must land in
    one of $\eta$'s children's subtrees. Call this child $\eta_c$, implying that $c_z(\eta) = c_z(\eta_c)$. We now show that the cost-decrease of $\eta$ is
    greater than or equal to the cost decrease of $\eta_c$.  After this, we will see that the cost decrease of $\eta_c$ is, in turn, greater than the cost
    decrease of any of $\eta$'s other children.

    First, notice that $\text{CostDecrease}(\eta, z) \geq \text{CostDecrease}(\eta_c, z)$. We show this by separating $\text{CostDecrease}(\eta, z)$ into a sum of terms, of which $\eta_c$ is a subset:
    \begingroup
    \allowdisplaybreaks
    \begin{align*} 
        \text{CostDecrease}(\eta, z) &= \text{NodeCost}(\eta, z) - \cost(T[\eta], c_z(\eta)) \\
        &= \left( |T[\eta_c]| \cdot d(\text{parent}(\eta))^z + \sum_{\substack{\eta' \in \text{children}(\eta) \\ \eta' \neq \eta_c}}|T[\eta']| \cdot
        d(\text{parent}(\eta))^z\right) \\
        &\quad - \cost(T[\eta], c_z(\eta)) \\
        % &= \sum_{\eta' \in \children(\eta)} |T[\eta']| \cdot d(\text{parent}(\eta)) \\
        % &\quad- \left( \cost(T[\eta_c], c)) + \sum_{\substack{\eta' \in \children(\eta) \\ \eta' \neq \eta_c}} |T[\eta']| d(\eta) \right)\\
        &= \left( |T[\eta_c]| \cdot d(\text{parent}(\eta))^z + \sum_{\substack{\eta' \in \text{children}(\eta) \\ \eta' \neq \eta_c}}|T[\eta']| \cdot
        d(\text{parent}(\eta))^z \right) \\
        &\quad - \left( \cost(T[\eta_c], c_z(\eta)) + \sum_{\substack{\eta' \in \text{children}(\eta) \\ \eta' \neq \eta_c}} |T[\eta']| d(\eta)^z \right)\\
        &\geq \left( \text{NodeCost}(\eta_c, z) + \sum_{\substack{\eta' \in \text{children}(\eta) \\ \eta' \neq \eta_c}}|T[\eta']| \cdot d(\text{parent}(\eta))^z \right) -\\
        &\quad \left( \cost(T[\eta_c], c_z(\eta)) + \sum_{\substack{\eta' \in \text{children}(\eta) \\ \eta' \neq \eta_c}} |T[\eta']| d(\eta)^z \right)\\
        &= \left( \text{NodeCost}(\eta_c, z) - \cost(T[\eta_c], c_z(\eta)) \right) \\
        &\quad + \sum_{\substack{\eta' \in \text{children}(\eta) \\ \eta' \neq \eta_c}}|T[\eta']| \cdot \left( d(\text{parent}(\eta))^z - d(\eta)^z \right) \\
        &= \text{CostDecrease}(\eta_c, z) + \sum_{\substack{\eta' \in \text{children}(\eta) \\ \eta' \neq \eta_c}}|T[\eta']| \cdot \left( d(\text{parent}(\eta))^z - d(\eta)^z \right) \\
        &\geq \text{CostDecrease}(\eta_c, z),
    \end{align*}
    \endgroup

    \noindent where both inequalities are due to $d(\text{parent}(\eta)) \geq d(\eta)$.

    Lastly, we consider the cost decreases of the other children of $\eta$. Let $\eta_o \neq \eta_c$ be any other child of $\eta$.  Then by
    Lemma~\ref{lma:optimal_subtrees}, we have $\text{CostDecrease}(\eta_c, z) \geq \text{CostDecrease}(\eta_o, z)$. This concludes by showing that the costs are monotonically non-decreasing along
    paths to the root.

    It remains to be shown that the cost-decreases are necessarily non-negative. For this, we rely on the fact that the original distances in $T$ are non-negative.
    Since we already know that they are monotonically non-decreasing, we, therefore only have to show that the cost-decrease of placing a center at a leaf is
    non-negative. To this end, let $\ell$ be any leaf. By Corollary \ref{cor:ultrametric_lca}. Then $\text{CostDecrease}(\ell, z) = \text{NodeCost}(\ell, z) - \cost(T[\ell], c_z(\ell))$.
    However, $\cost(T[\ell], c_z(\ell)) = d(\ell)$ while $\text{NodeCost}(\ell, z) \geq d(\text{parent}(\ell))$. Thus, $\text{CostDecrease}(\ell, z) \geq d(\text{parent}(\ell)) - d(\ell)
    \geq 0$.

\end{proof}

\noindent We note that the cost-decrease at a leaf $\ell$ is not necessarily $0$. Interpreting the cost-decreases as a relaxed ultrametric means that
$\dt^{cost-decrease}(\ell, \ell) \neq 0$. This is why we required the definition of relaxed ultrametrics rather than standard ultrametrics. 

The next lemma shows that not only do these cost-decreases satisfy the conditions in Corollary \ref{cor:ultrametric_lca}, but they are also monotonically
\emph{increasing} in all relevant settings. In other words, Lemma \ref{lma:cost_decreases_increase} is saying that if $T[\eta_1]$ has a non-zero cost, then placing
a center there decreases this cost by a non-zero amount.

\begin{lemma}
    \label{lma:cost_decreases_increase}
    Let $T$ be an ultrametric LCA-tree satisfying the conditions in Corollary \ref{cor:ultrametric_lca} and let $z$ be a positive integer. For any leaf $\ell
    \in T$, let $p(\ell) = [\ell, \eta_i, \ldots, \eta_j, r(T)]$ be the path from $\ell$ to the root of the cost-decrease LCA-tree.  Then for all $\eta_1,
    \eta_2 \in p(\ell)$ such that $d(\eta_1) > 0$, $\eta_1 \preceq \eta_2 \implies \text{CostDecrease}(\eta_1) < \text{CostDecrease}(\eta_2)$.
\end{lemma}
\begin{proof}
    
    By Lemma \ref{lma:cost_decreases_ultrametric}, we know that the cost decreases are monotonically non-decreasing along paths to the root. Thus, it remains to
    show that the cost-decrease at a node $\eta_1$ is non-zero if $d(\eta_1) > 0$. To do this, we first decompose the cost-decrease at $\eta$:
    \begin{align*}
        \text{CostDecrease}(\eta_1) &= \text{NodeCost}(\eta_1) - \cost(T[\eta_1], c_z(\eta_1)) \\
        &= |T[\eta_1]| \cdot d(\text{parent}(\eta_1)) - \cost(T[\eta_1], c_z(\eta_1)) \\
        &= d(\text{parent}(\eta_1)) \cdot \left( \sum_{\eta' \in \text{children}(\eta_1)} |T[\eta']| \right) - \cost(T[\eta_1], c_z(\eta_1)).
    \end{align*}

    \noindent Now, let $\eta_c$ be the child of $\eta_1$ containing $c_z(\eta_1)$. Much as in the proof of Lemma \ref{lma:cost_decreases_ultrametric}, we
    rewrite the above in terms of the costs in $\eta_c$ and the costs in the other children:
    \begingroup
    \allowdisplaybreaks
    \begin{align*}
        \text{CostDecrease}(\eta_1) &= d(\text{parent}(\eta_1)) \cdot \left( |T[\eta_c]| + \sum_{\substack{\eta' \in \text{children}(\eta_1) \\ \eta' \neq \eta_c}} |T[\eta']| \right) \\
        & - \cost(T[\eta_c], c_z(\eta_1)) - d(\eta_1) \cdot \left( \sum_{\substack{\eta' \in \text{children}(\eta_1) \\ \eta' \neq \eta_c}} |T[\eta']| \right) \\
        &= \left( \sum_{\substack{\eta' \in \text{children}(\eta_1) \\ \eta' \neq \eta_c}} |T[\eta']| \right)\cdot (d(\text{parent}(\eta_1)) - d(\eta_1)) \\
        &+ d(\text{parent}(\eta_1)) \cdot |T[\eta_c]| - \cost(T[\eta_c], c_z(\eta_1)) \\
        &\geq d(\text{parent}(\eta_1)) \cdot |T[\eta_c]| - \cost(T[\eta_c], c_z(\eta_1)) \\
        &\geq d(\text{parent}(\eta_1)) \cdot |T[\eta_c]| \\
        &\geq d(\eta_1) \cdot |T[\eta_c]| \\
        &> 0
    \end{align*}
    \endgroup
    where the first inequality is by the fact that $d(\eta_1) \leq d(\text{parent}(\eta_1))$, the second is due to the fact that the costs are strictly
    non-negative, the third is by the fact that $d(\text{parent}(\eta_1)) > d(\eta_1)$, and the fourth inequality is by the assumption that $d(\eta_1) > 0$.

\end{proof}


Lemma \ref{lma:cost_decreases_increase} allows us to address the fact that the cost-decrease at a node $\eta$ was defined under the assumption that $\eta$'s
parent is marked. By Lemma \ref{lma:cost_decreases_increase}, the cost decreases are either strictly increasing along paths to the root or are $0$. Thus, if two
nodes have equivalent non-zero cost-decreases, their subtrees must be disjoint.\footnote{Formally, for two nodes $\eta_1, \eta_2 \in T$ with $d(\eta_1) > 0$ and
$d(\eta_2) > 0$, we have that $d(\eta_1) = d(\eta_2) \implies T[\eta_1] \cap T[\eta_2] = \emptyset$.} As a result, when we go through the nodes sorted by their
cost-decreases, it will always be the case that the next node we place will either have its parent marked or will have a cost-decrease of $0$ (in which case it
is irrelevant in terms of the optimal solutions).

We finally move to our last lemma, which shows that greedily maximizing cost-decreases results in an optimal $(k, z)$-clustering over the LCA-tree.

\begin{lemma}
    \label{lma:greedy_is_optimal}
    The optimal $(k, z)$-clustering solution over an LCA-tree can be obtained by greedily choosing the $k$ centers that, at each step, maximize the
    cost-decrease.
\end{lemma}
\begin{proof}

    We show this by contradiction. Consider that there is a solution that was obtained greedily and another, different one that is actually optimal. We now map
    every center in the ``optimal'' solution to its closest center in the ``greedy'' one and observe how the optimal solution's cost changes. There are two
    cases that can occur: either all of the greedy centers receive one optimal center each, or there is at least one greedy center that receives more than one
    optimal center and another that receives none.
    
    We start with the case where each greedy center $c_g$ has one optimal center $c_o$ mapped to it. By definition, $c_o$ must be in the set of points that are
    assigned to $c_g$. Thus, it is either (a) in $T[c_g]$ or (b) in another subtree whose parent was marked by $c_g$. In case (a),
    Lemma~\ref{lma:optimal_subtrees} states that the greedy algorithm must have chosen $c_o$. In case (b), by the greedy algorithm, $T[c_g]$ has greater cost
    minimization than $T[c_o]$. Thus, we can decrease the cost of the optimal solution by replacing $c_o$ with $c_g$, giving a contradiction.  Therefore, we
    conclude that if one optimal center was mapped to each greedy center, then the solutions must be equivalent.
    
    It remains to consider the case where more than one optimal center was mapped to a greedy center $c_g$. WLOG, let there be two optimal centers $c_o^1$ and
    $c_o^2$ that are mapped to $c_g$. By a similar argument as above, $c_g$ must be the same as one of these optimal centers, i.e. $c_g = c_o^1$. By extension,
    $c_o^2 \neq c_g$.  Now consider the greedy center elsewhere in the tree that had no optimal center mapped to it. Call this center $c_g'$. This means that
    the greedy algorithm had both $T[c_o^2]$ and $T[c_g']$ available to it but chose $T[c_g']$. Thus, $\text{CostDecrease}(T[c_o^2]) < \text{CostDecrease}(T[c_g'])$. We can therefore decrease
    the cost of the optimal solution by replacing center $c_o^2$ with center $c_g'$.  This gives the desired contradiction.

\end{proof}

\noindent This brings us to the primary result of this chapter, restated from before:
\kzclusteringtheorem*

\begin{proof}

    We use Algorithm~\ref{alg:corresp_z_centers} from Lemma \ref{lma:cost_decreases_alg} to find the cost-decreases for $(k, z)$-clustering in the LCA-tree.
    By Lemma \ref{lma:cost_decreases_ultrametric}, these satisfy the strong triangle inequality. Furthermore, by Lemma \ref{lma:greedy_is_optimal}, greedily
    choosing centers that maximize the cost-decreases gives an optimal $(k, z)$-clustering. 

    Thus, running farthest-first traversal on the cost-decrease LCA-tree will give the partitions for $(k, z)$-clustering solutions. However, we must be a bit
    careful here: while running a naive farthest-first traversal on the cost-decrease LCA-tree will give the \emph{partitions} of the optimal $(k,
    z)$-clustering solutions, it will not necessarily give the correct \emph{centers}. This is due to the fact that the farthest-first traversal is optimal
    regardless of which center we pick for every subtree. Luckily, we have already addressed this. When considering the LCA-tree of cost-decreases, we have
    a mapping between every cost-decrease and the center that induces it. Thus, we will perform the farthest-first traversal by placing the nodes'
    corresponding $z$-centers. This ensures that both the partition \emph{and} the centers align with the optimal $(k, z)$-clustering solutions.

    Putting this all together, our final algorithm -- Algorithm \ref{alg:ultrametric_kz} -- is quite simple. We first run Algorithm \ref{alg:corresp_z_centers}
    to return a list of nodes and their cost-decreases.  Importantly, Algorithm \ref{alg:corresp_z_centers} returns only the cost-decreases for those nodes
    $\eta'$ with $c_z(\eta') \neq c_z(\text{parent}(\eta'))$.  We then sort this list in $\texttt{Sort}(n)$ time. By the discussion after Lemma
    \ref{lma:cost_decreases_increase}'s proof, we can safely go through this list and place the nodes' corresponding $z$-centers: the nodes with non-zero
    cost-decrease will always have their parent marked. The nodes with zero cost-decrease come at the end of the sorted list and do not affect the optimality of
    the solution. By Lemma \ref{lma:optimal_subtrees}, each corresponding $z$-center is immediately optimal in every node that it marks. Thus, Algorithm
    \ref{alg:ultrametric_kz} optimally solves the $(k, z)$-clustering objective in LCA-trees which satisfy the conditions in Corollary
    \ref{cor:ultrametric_lca}. The bottleneck in this algorithm remains the time to sort the $O(n)$ internal values in the LCA-tree.
    
    \begin{algorithm}
        \caption{\texttt{Ultrametric-kz}}\label{alg:ultrametric_kz}
    \textbf{Input:} LCA-tree $T$
    \begin{algorithmic}[1]
        \State \texttt{Costs}, \texttt{CostDecreases} = \{ \}, \{ \}
        \State \texttt{GetCostDecreases}($T$.root, \texttt{Costs}, \texttt{CostDecreases}) // pass-by-reference
        \State \texttt{CostDecreases} = OrderedDict(\texttt{CostDecreases})
        \For{$\eta \in \texttt{CostDecreases}$}
            \State Place center at $c_z(\eta)$
        \EndFor
    \end{algorithmic}
    \end{algorithm}

\end{proof}

Together, Theorem \ref{thm:optimal_kz} and Lemma \ref{lma:optimal_kcenter} prove Theorem \ref{thm:optimal_clusters} from the main body of the paper.



