\section{Center-based clustering in ultrametrics}
\label{sec:clustering_theory}
%
Our primary theoretical result is that the $k$-means, $k$-median, and $k$-center objectives can be solved optimally in a relaxed ultrametric. Furthermore, these solutions are hierarchical and can all be found in $\texttt{Sort}(n)$ time.\footnote{ I.e., the time required to sort $O(n)$ distances in the metric. Although sorting is thought of as requiring $O(n \log n)$ time, in many settings, it only requires $O(n \log \log n)$ time \citep{sort_time}.}
%
\paragraph{Notation.} We define the \emph{$(k, z)$-clustering} and the \emph{$k$-center clustering} objectives over $T$ as finding the set of centers $\mathbf{C} \subseteq \text{leaves}(T)$ with $|\bC| = k$ which minimize, respectively, 
\[\cost_z(T, \bC) = \underbrace{ \sum_{\ell \in \text{leaves}(T)} \min_{c \in \bC} \dt(\ell, c)^z}_{(k, z)\text{-clustering objective}},\]
%
\[\cost_\infty(T, \bC) = \underbrace{\max_{\ell \in \text{leaves}(T)} \min_{c \in \bC} \dt(\ell, c)}_{k\text{-center clustering objective}}.\]
%
Note that the $(k, z)$-clustering task gives us the well-known $k$-median and $k$-means tasks for $z=1$ and $z=2$. Let us now define what it means for a clustering to be hierarchical. Given a set of points $L$, we define a \emph{cluster} $C$ as any subset of $L$. We then define a \emph{partition} $\mathcal{P}_k = \{C_1, \ldots, C_k\}$ as any \emph{non-overlapping} set of $k$ clusters, i.e., for all $C_i, C_j \in \mathcal{P}$, $C_i \cap C_j = \emptyset$. We now define cluster hierarchies:%This brings us to the following definition for cluster hierarchies (visualized in box 2 of \Cref{fig:overview}):


\begin{restatable}{definition}{Hierarchy}[\citet{hierarchical_center_based}]
    \label{def:hierarchical_clusters}
     A \emph{cluster hierarchy} $\mathcal{H} = \{\mathcal{P}_1, \ldots, \mathcal{P}_n\}$ is a set of partitions with
    \begin{enumerate}[topsep=0pt,itemsep=0ex,partopsep=0ex,parsep=1ex]
        \item for $k=1$, $\mathcal{P}_1 \subseteq L$, and
        \item for $1 < k \leq n$, $\mathcal{P}_k = \left( \mathcal{P}_{k-1} \symbol{92} C_i \right) \cup \{C_j, C_l\}$, such that $C_i = C_j \cup C_l \in \mathcal{P}_{k-1}$ with $C_j \cap C_l = \emptyset$ and $i \neq j \neq l$.
    \end{enumerate}
    We say a cluster $C_i \in \mathcal{H}$ if $\exists \; \mathcal{P}_j \in \mathcal{H}$ with $C_i \in \mathcal{P}_j$.
    %We write $\mathcal{H}[C_i]$ to denote the cluster hierarchy rooted at cluster $C_i$.
    %We say $\mathcal{P}'$ \textbf{indirectly} belongs to $\mathcal{H}$ if $\mathcal{P}' \subset \bigcup_{\mathcal{P}_i \in \mathcal{H}} \mathcal{P}_i$.
\end{restatable}
\noindent This means that the partition $\mathcal{P}_k$ is obtained by splitting a cluster from $\mathcal{P}_{k-1}$ in two. 
Thus, all cluster hierarchies can be represented as a rooted tree. We depict this in \cref{fig:overview} (middle), where the hierarchy is obtained by subdividing the clusters. We can now state our primary theoretical result:
%
\begin{restatable}{theorem}{maintheorem}
    \label{thm:optimal_clusters}
    Let $(L, d)$ be a finite relaxed ultrametric space represented over an LCA-tree $T$. Let $n = |L|$ and $z \in \mathbb{Z}_{>0}$.
    Then, for both the $k$-center and $(k, z)$-clustering objectives on $T$, there exists an algorithm which finds the optimal solutions $\{\mathbf{C}_1, \ldots, \mathbf{C}_n\}$ for all $k \in \mathbb{N}_n$ in $\texttt{Sort}(n)$ time. Furthermore, the respective partitions $\mathcal{H} = \{\mathcal{P}_1, \ldots, \mathcal{P}_n\}$ obtained by assigning all leaves in $T$ to their closest center satisfy Definition \ref{def:hierarchical_clusters}.
\end{restatable}
%
Theorem \ref{thm:optimal_clusters} states that given any LCA-tree over a relaxed ultrametric, it takes $\texttt{Sort}(n)$ time to find the optimal solutions for $k$-means, $k$-median or $k$-center across all values of $k$ and that these solutions are hierarchical. While subsets of this are known for specific ultrametrics \cite{hierarchical_kmedian, beer2023connecting}, our runtime improves over the previous state of the art, and we are not aware of a comprehensive treatment of this subject over all ultrametrics and center-based clustering objectives.
%For example, \cite{beer2023connecting} showed that one can optimally solve $k$-center given the minimax distances in $O(n^2)$ time. Similarly, \cite{hierarchical_kmedian} showed that $k$-median can be solved optimally in $O(n\log^2(n^2 + \Delta^2))$ time given a hierarchically well-separated tree (HST).\footnote{Here, $\log \Delta$ is the depth of the HST.} Nonetheless, our runtime improves over those listed above, and we are not aware of a comprehensive treatment of this subject over all ultrametrics and center-based clustering objectives. 
%
\paragraph{Proof Overview.} The $k$-center part of this result is an extension of the farthest-first traversal algorithm where one picks each subsequent center as the point that is farthest from the current set of centers \citep{farthest_first, Har-Peled}. In essence, Appendix \ref{app:k_center_proof} shows that the farthest-first traversal algorithm becomes optimal under the strong triangle inequality. Then, we show in Appendix \ref{app:k_z_clustering_proof} that the $(k, z)$-clustering objectives in a relaxed ultrametric can be reduced to the $k$-center one. Specifically, given an LCA-tree $T$ on which to do $(k, z)$-clustering, there exists a \emph{new} LCA-tree $T'$ such that an optimal $k$-center solution on $T'$ \emph{is} the optimal $(k, z)$-clustering solution on $T$. Interestingly, even if this reduction is applied in a standard ultrametric, it requires the definition of relaxed ultrametrics to go through.
Finally, the runtime bottleneck for $k$-center lies in sorting the $O(n)$ unique distances in the ultrametric so that we may apply farthest-first traversal. The reduction from $(k, z)$-clustering only requires $O(n)$ time. Thus, the bottleneck remains unchanged for $(k, z)$-clustering. We verify by a worst-case example that our runtime is tight: one cannot find the optimal centers for all values of $k \in \mathbb{N}_n$ faster than in $\texttt{Sort}(n)$ time (Lemma \ref{lma:worst_case}).

\paragraph{Takeaway.} An interesting consequence of Corollary \ref{cor:ultrametric_lca} and Theorem \ref{thm:optimal_clusters} is that these center-based cluster hierarchies \emph{themselves} constitute relaxed ultrametrics. That is, let $\mathcal{H}$ be a hierarchy of optimal $k$-center or $(k, z)$-clustering solutions from Theorem \ref{thm:optimal_clusters}. For each cluster $C$ in this hierarchy, consider the cost $d_{cost}(C)$ of assigning all of its points to a single optimal center. As we traverse the tree towards the root, these costs will necessarily grow (see Lemma \ref{lma:optimal_subtrees} in Appendix \ref{app:k_z_clustering_proof}). Thus, by Corollary \ref{cor:ultrametric_lca}, the hierarchy $\mathcal{H}$ with LCA-distances $d_{cost}$ \emph{must} be a relaxed ultrametric. This is precisely what facilitates our reduction from the $(k, z)$-clustering setting to the $k$-center one. Indeed, any LCA-tree satisfying the properties in Corollary \ref{cor:ultrametric_lca} \emph{is} its own optimal $k$-center hierarchy (non-binary LCA-trees can be binarized while preserving the LCA-distances to recover the equivalence to the $k$-center hierarchy).
