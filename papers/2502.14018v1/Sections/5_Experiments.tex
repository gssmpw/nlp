\section{From Theory to Practice}

Sections \ref{sec:ultrametrics}-\ref{sec:best_clustering} describe what we refer to as the SHIP (Similarity-HIerarchy-Partition) clustering framework: fitting an ultrametric, choosing a hierarchy, and partitioning the data. Table \ref{tbl:combinations} outlines various options for each component. We use the remainder of this paper to demonstrate that the SHIP framework includes diverse, effective clustering strategies and enables quickly finding the many clusterings contained in any ultrametric. We note that this is a particularly valuable feature for exploratory data analysis, where quickly switching between clustering methods is beneficial.

\input{tables/combinations}


\subsection{Ultrametrics}\label{ssec:ultrametrics}

\paragraph{HSTs.}\label{ssec:kd_tree}


Hierarchically well-separated trees (HSTs) are embeddings of a metric space into a tree structure so that distances between points are approximated using the tree's path distances. There is a spectrum of available HST constructions, from those that minimize the distortion of the original distances (with the best possible distortion being $O(\log n)$ \citep{HST_2}) to those that are quick to construct \citep{hierarchical_kmedian}. 
These are used regularly within computer graphics \citep{kd_tree_graphics_1} and nearest-neighbor search \citep{kd_tree_nn_search_2, kd_tree_nn_search_3}. Thus, in all these use cases, we provide access to a suite of optimal clusterings.

In HSTs, every internal node is equidistant to all its descendant leaves \citep{hst_1}. Thus, an HST can be transformed to an LCA-tree in $O(n)$ time\footnote{For every internal node $\eta$ in the HST with leaf $\ell$ below it, $d_{H\!S\!T}(\eta, \ell)$ is known. Then, an LCA-tree can preserve the HST's path distances by simply assigning $d(\eta) = 2 d_{H\!S\!T}(\eta, \ell)$.}.
In our experiments, we use the well-known KD trees~\cite{kd_tree}, Cover trees~\cite{cover_tree}, and `theoretically optimal' HST-DPOs~\cite{hst_modern}. We use the SHIP clustering framework to evaluate their differences in Figure \ref{fig:exp_ablation_ultrametrics}.

Each of these HSTs works by recursively subdividing the metric space into a set of nested cells. Cover trees are defined so that points $p \in C_{i-1}$ and $q \in C_i$ (where $C_i \subset C_{i-1}$) satisfy $d(p,q) < 2^i$ under ambient metric $d$. This can result in non-convex shapes, as evidenced by the optimal $k$-median solutions for the ground-truth value of $k$ in Figure \ref{fig:exp_ablation_ultrametrics}. The KD tree, on the other hand, works by simply subdividing the space into nested hypercubes. Consequently, the KD tree's $k$-median solutions in Figure \ref{fig:exp_ablation_ultrametrics} are the axis-aligned rectangles that best fit the data. These HSTs are fast to construct, requiring $O(n \log n)$ time assuming constant dimensionality, but have poor distance preservation guarantees. Alternatively, the HST-DPO ultrametric achieves optimal distance preservation at the expense of an $O(n^2)$ runtime. This works by the principle from \citet{HST_2}, wherein one recursively claims points by placing metric balls. Indeed, one can see this ball structure in Figure \ref{fig:exp_ablation_ultrametrics}'s HST-DPO $k$-median solutions on the Boxes dataset. Our remaining experiments use Cover~trees, with full comparisons in the appendix.


\paragraph{Density-Connectivity Distance (dc-dist).}
\label{ssec:dc_experiments}
The second ultrametric we use, the dc-dist, is the basis of density-connected clustering algorithms such as DBSCAN~\citep{dbscan} and HDBSCAN~\citep{hdbscan}. The dc-dist builds on mutual reachabilities between points to characterize local density:  

\begin{definition}[Mutual reachability distance \citep{dbscan}]
    Let $(L, d')$ be a metric space, $x$ and $y$ be any two points in $L$, and $\mu \in \mathbb{Z}_{>0}$. Let $\kappa_{\mu}(x)$ be the distance from $x$ to its $\mu$-th closest neighbor in $L$. Then the mutual reachability between $x$ and $y$ is $m_{\mu}(x, y) = \max(d'(x, y), \kappa_{\mu}(x), \kappa_{\mu}(y))$.
\end{definition}
Note that usually $m_\mu(x,x)\gneq 0$. 
%the mutual reachability of a point to itself is usually not 0. 
The dc-dist is then the minimax distance over the pairwise mutual reachabilities:

\begin{definition}[dc-dist \citep{beer2023connecting}]
    Let $(L, d')$ be a metric space, $x$ and $y$ be any two points in $L$, and $\mu \in \mathbb{Z}_{>0}$. Let $T$ be an MST over $L$'s pairwise mutual reachabilities. Let $p(x, y)$ be the path in $T$ from $x$ to $y$ given by edges $\{e_i, \ldots, e_j\}$ in $T$. Lastly, let $|e|$ be the weight of any edge $e$ in $T$. Then the dc-dist between $x$ and $y$ is defined as
    % \[ d_{dc}(x, y) = \begin{cases}
    %     \max_{e \in p(x, y)} |e| & \text{if } x \neq y \\
    %     m_{\mu}(x, y) & \text{if } x = y.
    % \end{cases}\]
    \[ d_{dc}(x, y) = \max_{e \in p(x, y)} |e| \text{ if } x \neq y; \,\, \text{else } \, m_{\mu}(x, x).\]
\end{definition}

Note that this definition slightly differs from the one in \citet{beer2023connecting} since they defined $d_{dc}(x, x) = 0$ in order for it to be an ultrametric. However, our definition allows for the following proposition (proof in Appendix \ref{prf:dc_dist_ultrametric}):

\begin{restatable}{proposition}{DcDistUltra}
    \label{fact:dc_dist_ultrametric}
    Let $(L, d')$ be a metric space. Then, the dc-dist over $L$ is a relaxed ultrametric.
\end{restatable}

 (without border points)\mbox{DC~tree$$}\mbox{DC~tree$$} (see Appendix \ref{app:cluster_merging_rules}). 
W the

%%% RUNTIME TABLE
\begin{table*}[b!]
    \centering
    \caption{
        Runtimes of our SHIP framework's components (first three groups of columns) and competitors (last column group) in minutes, seconds, and milliseconds [min:sec.ms].
        Computation times of the ultrametrics are comparable to the runtimes of our competitors. Computation of the cluster hierarchies and partitioning methods take only \emph{milliseconds} even on large and high-dimensional datasets. Thus, once the ultrametric is computed, we get arbitrarily many hierarchies and partitions in negligible time. Notably, while density-based methods (blue) are sometimes slower than others, building the DC tree is generally faster than its counterparts, AMD-DBSCAN or DPC.
    }
    \label{tbl:runtimes}
    \vspace{0.5em}
    \input{tables/runtimes_small}
\end{table*}
%%% RUNTIME TABLE



\begin{figure}[t]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tikzpicture}
        \node[inner sep=0pt] (img) at (0, 0) {\includegraphics[trim={1.75cm, 0.05cm, 0.05cm, 1.68cm}, clip, width=0.9\linewidth]{figures/DCFramework_Multiple_Clusterings.jpg}};
        \node (labels) at (-2.8, 3.43) {\textcolor{darkgray}{HDBSCAN}};
        \node (labels) at (-0.9, 3.6) {\textcolor{darkgray}{Eucl.}};
        \node (labels) at (-0.9, 3.25) {\textcolor{darkgray}{$k$-means}};
        \node (labels) at (0.9, 3.43) {\textcolor{darkgray}{Ward}};
        \node (labels) at (2.8, 3.6) {\textcolor{darkgray}{DC tree/}};
        \node (labels) at (2.8, 3.25) {\small \textcolor{darkgray}{$k$-median/GT}};
        \node () at (-4, 2) {\textcolor{darkgray}{\rotatebox{90}{Moons}}};
        \node () at (-4, 0) {\textcolor{darkgray}{\rotatebox{90}{Compound}}};
        \node () at (-4, -2) {\textcolor{darkgray}{\rotatebox{90}{Iris}}}; 
    \end{tikzpicture}
    }
    \caption{Clusterings on toy datasets indicated by color.}
    \label{fig:exp_adapt}
\end{figure}



\subsection{Hierarchies and Partitioning methods}\label{ssec:partitioning}


We now give an intuition for several hierarchy/partition combinations. First, recall that the \emph{hierarchies} emphasize different criteria when merging clusters together (e.g., sum of squared distances for $k$-means vs. maximum distance for $k$-center). Thus, the different hierarchies offer macro-level control over the available clusterings. From this, the \textit{partitioning} methods then extract the hierarchy's clustering, which optimizes for a specific goal, such as the number of clusters or the clusters' stability.



Our experiments focus on the $k$-center/stability, $k$-median/\\MoE, and $k$-means/elbow combinations as illustrative examples. These are depicted over the dc-dist in Figure \ref{fig:exp_ablation_partitioning}. In the first column, the $k$-center hierarchy focuses solely on maximal cluster distances, merging clusters based on density-connectivity regardless of their number of points. The stability objective finds the sets of clusters that persist for the longest amount of time while allowing for noise points (a full description is in Appendix \ref{app:cluster_merging_rules}). Thus, the $k$-center/stability clustering in the left column of Figure \ref{fig:exp_ablation_partitioning} separates the D31 dataset but combines the two red boxes as there is a density-connected path between them.

It may initially seem unintuitive to use a $k$-median or $k$-means objective over the dc-dist. However, it is a very natural notion. Consider that the $k$-means objective sums over per-point distances, and distance to a center under the dc-dist is the largest step away from the center's densely-connected region. Thus, a sum over a cluster's dc-dists is large if there are many points that are all spread out. To visualize this, notice that $k$-median/MoE and $k$-means/elbow separate the two red boxes, which were merged under $k$-center in the top row of Figure \ref{fig:exp_ablation_partitioning}: these two boxes had too many spread-out points to be in a single $k$-median or $k$-means cluster. Furthermore, the $k$-median hierarchy does not penalize large distances as severely as $k$-means. Consequently, notice that $k$-median/MoE merges some sufficiently density-connected clusters in D31, which $k$-means does not.


Lastly, we verify that our introduced hierarchies provide the ability to find previously unattainable clusterings with standard techniques. Specifically, \Cref{fig:exp_adapt} shows several toy datasets where we see that HDBSCAN, Euclidean $k$-means, and Ward agglomerative clustering are all unable to correctly partition the data. However, each dataset's correct clustering exists within the dc-dist's $k$-median hierarchy.


\section{Experiments}\label{sec:experiments}

%%% ARI TABLE
\begin{table*}[thb]
    \centering
    \caption{
    ARI values indicate that our framework achieves high clustering quality with an automated selection of $k$, matching even competitors' performance that relies on the GT number of clusters ($k$-means, SCAR, Ward).
    (Full version: Table \ref{tbl:ari_full} in the Appendix.) % Noise points were handled as singleton clusters).
    }
    \label{tbl:experiments_ari}
    \vspace{0.5em}
    \input{tables/ari_small}
\end{table*}
%%% ARI TABLE

We now show the practical utility of the SHIP clustering framework: our new ultrametric/hierarchy/partition combinations are competitive with standard clustering methods, and once the ultrametric is constructed, one can switch between hierarchies and partitions in essentially no time.

Table \ref{tbl:dataset_overview} gives an overview of the datasets we use. We evaluate the clustering quality with the adjusted rand index (ARI) \citep{ari}, treating points labeled as noise as singleton clusters. NMI \citep{nmi} results can be found in Appendix \ref{app:experiments}.
Our framework is implemented in C++ and provides a Python interface using pybind11. All experiments were performed with Python 3.12 on a Linux workstation with 2x Intel 6326 with 16 hyperthreaded cores each and 512GB RAM. Our code is available at \url{https://anonymous.4open.science/r/SHIP-42B1/}.

\subsection{Runtimes}\label{ssec:runtime}

Clustering is an exploratory data mining task that, in practice, requires several runs of different methods or using different hyperparameter settings. Especially when done sequentially, this requires substantial computation time and resources.
In contrast, our SHIP clustering framework requires just a single upfront ultrametric computation, after which we can generate multiple clustering solutions with negligible additional runtime.
Table \ref{tbl:runtimes} and \Cref{fig:runtime_barplot} highlight this efficiency. The first column group in \Cref{tbl:runtimes} shows that computing our ultrametrics (Cover tree and DC tree) has a runtime comparable to that of the corresponding standard clustering algorithms (last column group). 
Specifically, we show on the right under competitors the time required for Euclidean $k$-means with $k=10$, Euclidean $k$-means with $k=500$,\footnote{We note that the SHIP clustering framework simultaneously obtains the optimal solutions for every value of $k$.} agglomerative clustering~\cite{hierarchical_clustering_og}, an adaptive multi-density DBSCAN version (AMD-DBSCAN)~\cite{Wang2022AMDDBSCANAA}, density peaks clustering (DPC)~\cite{density-peaks}, and an accelerated version of spectral clustering (SCAR)~\cite{hohma2022scar}. 
Computing the ultrametrics takes time comparable to that of clustering algorithms following similar notions, e.g., density-based methods (shown in blue).
However, once the ultrametric has been computed, generating different hierarchies (column group 2) and partitions (column group 3) takes on the order of \emph{milliseconds}. Thus, users can explore many different clustering solutions in essentially the same time it takes to run a traditional clustering algorithm.



\subsection{Clustering Quality}\label{sec:exp:high_quality_benchmarks}



Our runtime efficiency enables exploring diverse clustering approaches, which is particularly valuable given the results shown in Table~\ref{tbl:experiments_ari}. Here, we study the quality of $k$-center/stability (which is HDBSCAN if done on the dc-dist), $k$-median/MoE, and $k$-means/elbow combinations on the dc-dist and Cover trees and compare them against the competitors from Table \ref{tbl:runtimes}. The ARI scores reveal that the SHIP clustering framework on the dc-dist produces clusterings that outperform competitor algorithms in most cases, even without knowing the ground truth number of clusters. Importantly, $k$-means, Ward, and SCAR are all given the ground-truth value of $k$ to maximize their competitiveness. Furthermore, we point out that Ward agglomerative clustering is ultrametric in nature and, therefore, falls under the umbrella of our framework \citep{ultrametric_single_linkage}.
We also see that Cover~tree combinations achieve slightly worse performance than Euclidean $k$-means.

Notably, no single hierarchy/partition combination emerges as universally superior, with different combinations excelling on different datasets. This underscores the value of rapidly switching between methods and allowing users to identify the best clustering strategy for their data.