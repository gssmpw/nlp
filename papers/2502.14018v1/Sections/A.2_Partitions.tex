\section{Further Details for Section \ref{sec:best_clustering} - Choosing a Partition}
\label{app:best_clustering}

\subsection{Ultrametric Elbow Method}
\label{app:ultrametric_elbow}


\begin{figure*}[hbt]
    \centering
    \includegraphics[width=\textwidth, trim={0.75cm 6.75cm 1.2cm 0.4cm}, clip]{figures/hierarchy_explainer.pdf}
    \caption{\emph{Left}: an example hierarchy $\mathcal{H} = \{\mathcal{P}_1, \mathcal{P}_2, \ldots \}$; $\mathcal{P}' \not\in \mathcal{H}$ is then an example partition which \emph{belongs} to $\mathcal{H}$. \emph{Right}: $\mathcal{P}''$ is \emph{not} a partition since $\ell_2$ and $\ell_3$ each belong to clusters $C_2$ and $C_5$.}
    \label{fig:elbow_weakness}
\end{figure*}


We begin by quickly showing Corollary \ref{cor:elbow_plot}, which holds as a direct consequence of Lemma \ref{lma:cost_decreases_increase} in Section \ref{app:k_z_clustering_proof}:
\ElbowPlotCor*

\begin{proof}
    Lemma \ref{lma:cost_decreases_increase} states that the cost-decreases associated with nodes in an LCA-tree monotonically increase along paths from any leaf to the root or are all 0. By Lemma \ref{lma:greedy_is_optimal}, we solve $(k, z)$-clustering using farthest-first traversal over the LCA-tree of cost-decreases. I.e., we start at the root and greedily pick the cluster that gives the maximal cost-decrease at that step. Consequently, each cost-decrease we pick must be smaller than the previous one.
    
    Thus, let $\Delta_k' = \mathcal{L}_k - \mathcal{L}_{k+1} = -\Delta_k$. By Lemma \ref{lma:cost_decreases_increase}, we have $\Delta_k' > \Delta_{k+1}' \geq 0$ or $\Delta_k' = \Delta_{k+1} = 0$. Plugging in $\Delta = -\Delta'$ completes the proof.
\end{proof}

\paragraph{Choosing the elbow}
Although there are many methods for finding the elbow index, we are in the privileged setting where a single elbow exists and is clearly delineated. Inspired by \citet{elbow_angle}, we simply define the elbow as the index where there most strongly appears to be a right angle. Namely, let $\vec{v}_i = (i, \mathcal{L}_i)$ be the $(x, y)$ position of the $i$-th point in the elbow plot. Then, for every index $k \in \{2, \ldots, n-1\}$, let $\theta_k$ be the angle induced by the vectors\footnote{In practice, we normalize the $k$ values and the costs to be in $[0, 1]$ so that the scales are comparable.} $(\vec{v}_1 - \vec{v}_k)$ and $(\vec{v}_k - \vec{v}_n)$. We define the elbow as being at the index $k$ where $\theta_k$ is closest to 90 degrees. Since Theorem \ref{thm:optimal_clusters} gives us all of the partitions for $k \in \{1, \ldots, n\}$ simultaneously, this index can be found $O(n)$ time given the cluster hierarchy.

We note that Figure \ref{fig:elbow_plot} only plots the elbow curves for values of $k$ up to 100. This is because plotting until $k=n$ makes the plot look in practice like a vertical line followed by a horizontal line, as seen in Figure \ref{fig:elbow_plot_until_n}. However, we use all values of $k$ from $1$ to $n$ when choosing the elbow.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/elbow_plot_3100.pdf}
    \caption{The same elbow plot as in Figure \ref{fig:elbow_plot} with values of $k$ from $1$ to $n$. The chosen elbows are higlighted with circles.}
    \label{fig:elbow_plot_until_n}
\end{figure}

\subsection{Agglomerative Clustering Algorithms under our Framework}

Corollary \ref{cor:ultrametric_lca} and Theorem \ref{thm:optimal_clusters} imply that a large set of agglomerative clustering algorithms can be interpreted as $k$-center over various relaxed ultrametrics. As an example, consider the complete linkage algorithm \citep{agglomerative_clusterings}. Here, one starts with every point in its own cluster and recursively merges those clusters $C_i$, $C_j$ which have the smallest merge distance $\argmin_{C_i, C_j} \max_{x_i \in C_i, x_j \in C_j} d(x_i, x_j)$. Importantly, with each subsequent merge, the merge distances are monotonically non-decreasing. Thus, by Corollary \ref{cor:ultrametric_lca}, labeling each cluster in the hierarchy by its merge distance gives us a relaxed ultrametric and, by Theorem \ref{thm:optimal_clusters}, $k$-center over this ultrametric gives us the complete-linkage hierarchy. Indeed, this is true of \emph{any} agglomerative clustering method where the merge distances progressively grow as we approach the root cluster.

\subsection{Thresholding}
\label{app:thresholding}

We quickly explain how one can partition a cluster hierarchy by thresholding. We assume that the cluster hierarchy $\mathcal{H}$ has every internal node labeled by its cluster's cost. As discussed in the main body of the paper, this constitutes a relaxed ultrametric.

Now let $\varepsilon$ be any threshold value. We can return the set of clusters that have cost less than $\varepsilon$ by depth-first search in $O(n)$ time. This is precisely what DBSCAN* does on the dc-dist relaxed ultrametric. Namely, DBSCAN* returns clusters of core points that are within $\varepsilon$ of each other under the dc-dist. Under the dc-dist's relaxed ultrametric definition, this is specifically the set of nodes with a value less than $\varepsilon$.

\subsection{Cluster Value Functions}
\label{app:cluster_merging_rules}

The idea is a generalization of the excess-of-mass measure in HDBSCAN: we have a user-defined function $v(C)$ which assigns a non-negative value to each cluster in the hierarchy. I.e., $v(C) \geq 0$ for all clusters $C \in \mathcal{H}$. Then the \emph{best} partition maximizes the sum of these values:

\begin{definition}
    Given a value function $v$, we define the \textbf{best} partition under $v$ as the partition that maximizes the sum of valuations. I.e. $\mathcal{P}_v(\mathcal{H}) = \argmax_{\mathcal{P} \text{ belonging to } \mathcal{H}} \sum_{C \in \mathcal{P}} v(C)$.
\end{definition}

This brings us to the following result:

\begin{restatable}{theorem}{BestClustering}
    \label{thm:best_clustering}
    Let $\mathcal{H}$ be a cluster hierarchy. Let $v$ be a function such that, for all $C \in \mathcal{H}$, $v(C)$ can be obtained in $O(1)$ time. Then there exists an algorithm to find the best partition $\mathcal{P}_v(\mathcal{H})$ in $O(n)$ time.
\end{restatable}
\begin{proof}
    The algorithm for finding the best clustering is essentially Algorithm 3 from \citet{hdbscan}. We provide our own version of it in Algorithm \ref{alg:best_clustering} for completeness' sake.\footnote{Note, we write $B_v(C)$ to refer to the best clustering of the cluster hierarchy rooted at $C$.} It works by depth-first search where, at each node, we simply calculate its value and compare it against the sum of the values of its children. Since calculating the value takes $O(1)$ time and there are $O(n)$ nodes in the cluster hierarchy, the algorithm therefore runs in $O(n)$ time.

    We prove its correctness inductively. In the base case, we have a single leaf whose best clustering is simply the leaf itself. In the inductive case, we are given a cluster $C$ in the hierarchy and have the best clusterings of $C$'s children. We seek to find $B_v(C)$. By the non-overlapping requirement, if we include $C$ in $B_v(C)$, then we cannot include any of its children. Similarly, since the values of the children are non-negative, if we include one child, then we may as well include all of them. Thus, $B_v(C)$ is either $\{C\}$ or $\{C' : C' \in \text{children}(C)\}$.

    \begin{algorithm}
    \caption{\texttt{BestClustering}}\label{alg:best_clustering}
    \textbf{Input:} node $C$ in an cluster hierarchy;\\
    \textbf{Output:} the best clustering under this node $B_v(C)$, the value of the clustering $v(C)$;
    \begin{algorithmic}[1]
        \If{$C$ is leaf}
            \State $B_v(C) = \{C\}$
            \State \textbf{Return} $B_v(C)$
        \EndIf \vspace*{0.2cm}

        \State ChildValues $= 0$
        \State ChildClusterings $= \{ \}$
        \For{$C' \in \text{children}(C)$}
            \State $B_v(C'), v(C') = \texttt{BestClustering}(C')$
            \State ChildValues += $v(C')$
            \State ChildClusterings.append($B_v(C')$)
        \EndFor
        \If{$v(C) >$ ChildValues}
            \State \textbf{Return} $\{C\}$, $v(C)$
        \EndIf
        \State \textbf{Return} ChildClusterings, ChildValues
    \end{algorithmic}
    \end{algorithm}

\end{proof}

% The algorithm for finding the best clustering is essentially Algorithm 3 from \citet{hdbscan}. It works by depth-first search where, at each cluster, we calculate its value and compare it against the sum of the values of its children.


\paragraph{The \emph{stability} cluster value function.} 
We now introduce the stability cluster value function from \citet{hdbscan}. Let $C$ be any cluster in a hierarchy and let $C'$ be $C$'s parent in that hierarchy. Then the stability objective can be roughly phrased as the following:
\begin{equation}
    \label{eq:EoM}
    v_{E}(C) = |C| \cdot \left( \frac{1}{\mathcal{L}(C)} - \frac{1}{\mathcal{L}(C')} \right)\vspace*{-0.2cm}
\end{equation}

We note that our description of the stability criterium differs slightly from the original function discussed in \cite{hdbscan, hdbscan_long, acceleratedHDBSCAN}. Namely, we omit here the notion that singleton points may fall out of the clustering as it is not easy to represent in our notation and the differences are negligible for the purposes of this discussion. Our implemented stability criterion is the original (correct) one. For a full discussion of the original function, we refer the reader to the referenced literature.

In either case, the stability value function can be interpreted as emphasizing those clusters that have a large number of points and have significantly lower costs than their parent. While we find that the stability criterion performs well in the $k$-center hierarchy, it can produce sub-par partitions when applied in the $(k, z)$-clustering hierarchies. This is because the per-point cost in the $(k, z)$-clustering task is comparable to the per-cluster cost in the $k$-center objective. 

To be consistent with the literature \cite{hdbscan}, we utilize the stability cluster value function in the noisy setting. Given a user defined parameter $\mu$ representing the minimum cluster size, we first prune the tree so that all nodes with fewer than $\mu$ children are removed. We then run the stability value function over the remaining points. This selects a set of internal nodes to represent the clusters. Finally, we re-introduce the points which were pruned away. If the re-introduced points are in the sub-tree of a cluster, we assign them to the cluster. If, instead, they are not below a cluster found by the stability function, we label them as ``noise''.

\subsection{Further details on the dc-dist ultrametric}

We begin by proving Proposition \ref{fact:dc_dist_ultrametric}:

\DcDistUltra*

\begin{proof}\label{prf:dc_dist_ultrametric}
    Note that it is known that the minimax distances over a space constitute an ultrametric \citep{minimax_distance}. Thus, we have that $d_{dc}(\ell_i, \ell_j) = d(\ell_j, \ell_i)$ and that $d_{dc}(\ell_i, \ell_j) \geq 0$ for all $\ell_i, \ell_j$.
    
    To show that the dc-dist is a relaxed ultrametric, we will leverage that the only difference between it and the minimax distance is that the minimax distance of a point to itself is 0 while the dc-dist of a point to itself is its mutual reachability. Thus, we will prove that the dc-dist is a relaxed ultrametric by showing that the distance to itself is non-negative and that it inherits the strong-triangle inequality from the minimax ultrametric. Together, these imply the two properties for Corollary \ref{cor:ultrametric_lca}.
    
    First, note that $d_{dc}(\ell_i, \ell_i) = \max(||\ell_i - \ell_i||, \kappa_{\mu}(\ell_i), \kappa_{\mu}(\ell_i)) = \max(0, \kappa_{\mu}(\ell_i)) = \kappa_{\mu}(\ell_i)$. Thus, the dc-dist of a point to itself is the distance of $\ell_i$ to its $\mu$-th nearest neighbor in the ambient metric. This is necessarily non-negative. Similarly, the mutual-reachabilities for any other pair of points are also necessarily non-negative. Because the dc-dist is the minimax distance over the pairwise mutual reachabilities and the pairwise mutual reachabilities are all non-negative, the dc-dist must also be.

    Let us now show that the dc-dist satisfies the second property of Corollary \ref{cor:ultrametric_lca}. First, notice that the dc-dist of a point to itself is necessarily less than or equal to the dc-dists of that point to any other point in the set, i.e., $d_{dc}(\ell_i, \ell_i) \leq \max(d_{dc}(\ell_i, \ell_j), d_{dc}(\ell_j, \ell_i))$ for all $\ell_j$. To see this, consider the mutual reachability of $\ell_i$ to any other point $\ell_j$ is necessarily greater than $m_{\mu}(\ell_i, \ell_i)$. Namely, $m_{\mu}(\ell_i, \ell_j) = \max(d'(\ell_i, \ell_j), \kappa_{\mu}(\ell_i), \kappa_{\mu}(\ell_j)) \geq \kappa_{\mu}(\ell_i)$. As a result, any step in the mutual reachability MST that originates at $\ell_i$ will be at least as large as $m_{\mu}(\ell_i, \ell_i) = d_{dc}(\ell_i, \ell_i)$. Put simply, a point's closet point under the dc-dist is itself. Pairing this with the inheritance of the strong triangle inequality from the minimax distance gives us the second property of Corollary \ref{cor:ultrametric_lca}.
    
    Thus, we have shown the two properties for Corollary \ref{cor:ultrametric_lca}: the dc-dists are non-negative and are non-decreasing as we traverse the DC tree from any leaf to the root. Consequently, it is a relaxed ultrametric.
\end{proof}

\paragraph{Relationship to DBSCAN and HDBSCAN.} First, \citet{hdbscan} showed that one can obtain DBSCAN* partitions using the single-linkage hierarchy over the mutual reachabilities. Rephrasing into this paper's notation, they showed that one can obtain DBSCAN* embeddings by thresholding the dc-dist's LCA-tree at a user-defined value $\varepsilon$; i.e., removing all nodes $\eta$ in the LCA-tree with $d(\eta) > \varepsilon$. 
Subsequently, \citet{beer2023connecting} proved that $k$-center can be optimally solved over the dc-dist and that these partitions correspond to single-linkage over the mutual reachabilities. 
In this sense, one can think of $k$-center on the minimax distances as equivalent to single-linkage clustering. 
Lastly, \citet{hdbscan} showed that, given the dc-dists's LCA-tree, one can choose a partition from it by optimizing the EoM cluster merging criterium over the pruned tree.
