\section{Introduction}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth, bb=0 0 356.7 118.5, trim=20 0 20 28, clip]{figures/ClusterFrameworkOverview_final.pdf}
    \caption{Sec. \ref{sec:ultrametrics} introduces relaxed ultrametrics and how they correspond to lowest common ancestor (LCA) trees. Based on LCA trees, Sec. \ref{sec:clustering_theory} shows how various objectives lead to cluster hierarchies. Sec. \ref{sec:best_clustering} shows how to partition these hierarchies into desirable clusterings.
    }
    \label{fig:overview}
\end{figure*}


Hierarchical clustering is a fundamental technique for exploratory data analysis \citep{hierarchical_clustering_og, hierarchical_clustering_og_2}.
The key idea is that having a tree of clusterings over a given dataset allows users to choose any partition from this hierarchy that best suits the users' needs. These notions go beyond standard agglomerative clustering algorithms to also include density-based clustering techniques like DBSCAN \cite{dbscan} and HDBSCAN \cite{hdbscan}. Even non-hierarchical clustering algorithms like $k$-means can be solved efficiently by leveraging hierarchical representations \cite{mettu_plaxton, fast_kmeans, hierarchical_kmedian}.

The central underlying concept of hierarchical clustering methods is that they can be modeled via ultrametrics: distance functions satisfying the strong triangle inequality $d(x, z) \leq \max(d(x, y), d(y, z))$ for all $x, y, z$.
Originally described for agglomerative clustering tasks such as single-linkage clustering \cite{ultrametric_single_linkage}, the depth of this connection between ultrametrics and hierarchical clustering has inspired multiple subfields of clustering theory focused on finding the best-fitting ultrametric for a given dataset \cite{hst_1, hst_3, dasgupta_objective, HC_objectives, ultrametric_grad_descent}.

In this paper, we prove an elegant property of ultrametrics: \emph{all} standard center-based clustering tasks, including $k$-means, $k$-median, and $k$-center, can be solved optimally in an ultrametric. This improves on recent work in center-based clustering in trees \citep{hierarchical_kmedian, beer2023connecting, cover_tree_k_means}. Remarkably, our algorithm is extremely efficient: finding optimal solutions for all $k \in \mathbb{N}_n = \{1, \ldots, n\}$ on a dataset of 
$n$ points requires only the time to sort $O(n)$ values. %(and we show this to be tight). 
Moreover, these partitions are inherently hierarchical: the optimal center-based clustering solutions in an ultrametric themselves form a cluster-tree.

As a result, this significantly expands the versatility of hierarchical clustering. Not only can one choose partitions from a given cluster hierarchy, we show that one can also efficiently obtain \emph{new} hierarchies from it. We showcase this expanded utility by studying our results under two common ultrametrics: hierarchically well-separated trees (\citealp{hst_1}) and density-connectivity \citep{beer2023connecting}. The former is instrumental to accelerating countless machine learning problems \citep{kd_tree_nn_search_1, fast_emst, tsne_fast}. The latter is the backbone of density-based clustering algorithms like DBSCAN or HDBSCAN.

We conclude by verifying the utility of these ultrametric, hierarchy, and partition combinations. As an example, our framework can fully reproduce HDBSCAN. However, once this HDBSCAN clustering has been produced, one also has access to \emph{any} additional hierarchy and \emph{any} additional partition, essentially for free. We show that many of our new combinations indeed yield competitive clustering results. Thus, our approach is ideally suited for exploratory data analysis. 
\newpage

Our main contributions are: 
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item A theoretical derivation that all center-based clustering objectives can be solved optimally in ultrametrics for all values of $k$ in the time it takes to sort $n$ values.
    \item A generalization of hierarchical clustering that produces a diverse set of known and novel algorithms essentially simultaneously.
    \item A thorough experimental evaluation of our framework across ultrametrics and datasets verifying its utility. 
\end{enumerate}

\Cref{fig:overview} shows the structure of this paper: We introduce ultrametrics and their hierarchical representations in Section~\ref{sec:ultrametrics}. 
Section~\ref{sec:clustering_theory} contains our primary theoretical contribution that center-based clustering tasks like $k$-means can be solved optimally in ultrametrics, resulting in new cluster hierarchies.
Section~\ref{sec:best_clustering} shows ways to choose a partition from those hierarchies. 
We conclude with a thorough experimental analysis highlighting the speed and versatility of our proposed SHIP (Similarity-HIerarchy-Partition) clustering framework.%[ultrametric, hierarchy, partition] combinations.

