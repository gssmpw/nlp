\section{Choosing a Partition}\label{sec:best_clustering}

Theorem \ref{thm:optimal_clusters} gives us the optimal center-based clustering solutions for all values of $k$ at once. If we simply require the solution for a specific $k$ then this is sufficient. However, what if we are instead interested in the ``best'' clustering from this hierarchy? To this end, this section discusses how known techniques for obtaining a partition can be applied to Section \ref{sec:clustering_theory}'s hierarchies in $O(n)$ time.

\subsection{Cluster Selection Criteria}

\subsubsection{Feasibility of the Elbow Method}\label{sec:elbow}

One of the most common methods for choosing a ``best'' clustering is the elbow method \citep{elbow_original}. Here, one is given a set of values of $k$, $[k_1, k_2, \ldots, k_f]$, and a set of corresponding partitions $\mathcal{P} = [\mathcal{P}_{1}, \mathcal{P}_{2}, \ldots, \mathcal{P}_{f}]$. Each partition incurs a cost $\mathcal{L} = [\mathcal{L}_{1}, \mathcal{L}_{2}, \ldots, \mathcal{L}_{f}]$ with respect to the clustering objective, where $\mathcal{L}_i = \sum_{C \in \mathcal{P}_i} \mathcal{L}(C)$. This gives us a plot of costs over the different values of $k$. Informally, the \emph{elbow method} chooses the partition $\mathcal{P}_i$ whose cost $\mathcal{L}_i$ looks to be at a sharp point in this curve.

Due to the NP-hardness of $k$-means clustering \cite{kmeans_hardness_1}, standard elbow plots can only compute approximate solutions, traditionally done sequentially for each $k$ \cite{elbow_issues}. However, the elbow method is surprisingly viable in the ultrametric setting: Theorem \ref{thm:optimal_clusters} yields optimal clusterings for all $k$ values at once, eliminating both computational overhead and approximation errors. Moreover, we show that the relaxed ultrametric's elbow plot for $(k, z)$-clustering is guaranteed to be convex:

\begin{restatable}{corollary}{ElbowPlotCor}
    \label{cor:elbow_plot}
    Let $\mathcal{P}$ and $\mathcal{L}$ correspond to the $n$ partitions and losses obtained in accordance with Theorem \ref{thm:optimal_clusters} for the $(k, z$)-clustering objective. Let $\Delta_i = \mathcal{L}_{i+1} - \mathcal{L}_i$. Then either $\Delta_{i} < \Delta_{i+1} \leq 0$ or $\Delta_{i} = \Delta_{i+1} = 0$ for all $i \in [n-1]$.
\end{restatable}

The idea here is that $\Delta_i$ represents the elbow plot's first derivative at $k=i$. Thus, Corollary \ref{cor:elbow_plot} states that the elbow plot's slope is steepest at $k=1$ and monotonically levels out to 0 as $k \rightarrow n$. We prove this in Appendix \ref{app:ultrametric_elbow}, where we also specify how we determine the index of the elbow. We depict relaxed ultrametric elbow plots in the $(k, z)$-clustering setting for various values of $z$ in Figure \ref{fig:elbow_plot}, where we see that the plots are indeed convex.

\input{figures/elbow_plot}


\subsubsection{Thresholding the Tree}

While Corollary \ref{cor:elbow_plot} shows that the elbow method is reasonable in the ultrametric setting, it still has two main drawbacks. \emph{First}, the choice of the elbow is arbitrary, and it is unclear whether the best technique exists \citep{elbow_issues}. \emph{Second}, the elbow method is restricted to a single partition for each value of $k$. Although the cluster hierarchy can be cut in many ways to obtain $k$ clusters, the elbow method can only produce one of these partitions for any value of $k$.

One alternative is to threshold the cluster hierarchy, as done in single-linkage clustering or DBSCAN (see \citet{hdbscan}). Specifically, let $\varepsilon$ be any user-defined threshold parameter. We now (1) label the nodes of our cluster hierarchy by their costs as discussed at the end of Section \ref{sec:clustering_theory} and (2) report all clusters whose cost is less than $\varepsilon$, both in $O(n)$ time.
We further discuss this in \Cref{app:thresholding}.


\subsubsection{Cluster Value Functions} \label{sec:clustervaluefunction}

Rather than picking clusters based on their costs, one can also assign a \emph{new} value function to clusters and choose the partition that maximizes the sum of these values. Importantly, such a partition may not be attainable by choosing a value of $k$ or thresholding the clusters' costs in the hierarchy. For example, HDBSCAN uses the \emph{stability} objective to choose the clusters from a hierarchy that persist for the largest range of threshold values. Given any cluster hierarchy obtained via Theorem \ref{thm:optimal_clusters} and any reasonable value function, one can find the partition that maximizes this value function in $O(n)$ time. We detail this in Appendix \ref{app:cluster_merging_rules}. 


\subsection{Generalizing to Noisy Settings}

For a given LCA-tree $T$ and any node $\eta \in T$, one can remove the subtree rooted at $\eta$ from $T$ without affecting Corollary \ref{cor:ultrametric_lca}.
This allows handling additional noise points consistently, e.g., \citet{k-center-q-coverage, hdbscan}.

% To be consistent with the literature treating additional noise points in the data \citep{k-center-q-coverage, hdbscan}, we point out that one can prune nodes away from an LCA-tree without compromising our theoretical results. 
% Namely, given an LCA-tree $T$ and any node $\eta \in T$, one can remove the subtree rooted at $\eta$ from $T$ without affecting Corollary \ref{cor:ultrametric_lca}. Thus, pruning away subtrees does not affect any of the tree's ultrametric properties.

For example, consider a minimum-cluster-size parameter $\mu \in \mathbb{Z}_{+}$ and a cluster hierarchy $\mathcal{H}$ obtained via Theorem \ref{thm:optimal_clusters}. Then, for some cluster $C_i$ with $|C_i| < \mu$, we can prune the hierarchy by removing the cluster trees rooted at $C_i$ in $O(n)$ time \citep{hdbscan}. We can, therefore, obtain any desired clustering over this pruned hierarchy, guaranteeing that every cluster in the returned partition will have a size greater than or equal to the minimum cluster size. 


\subsection{Integrating Multiple Partition Methods} \label{sec:integrating_multiple_partition_methods}

Lastly, we note that we can combine results of several partitions without notable impact on runtime. As Section \ref{sec:experiments} confirms, fitting an ultrametric essentially always requires longer than $O(n \log n)$ time. Given such an ultrametric, we can find hierarchies of optimal center-based clustering solutions in $\texttt{Sort}(n) \leq O(n \log n)$ time (Section \ref{sec:clustering_theory}). Furthermore, we presented several $O(n) < \texttt{Sort}(n)$-time methods for choosing a partition. As Figure \ref{fig:runtime_barplot} highlights, the time it takes to fit the density-connectivity ultrametric dwarfs the time for finding a hierarchy or partition.
%
\input{figures/runtimes_barplot}
%
Consequently, once we have fit an ultrametric, we can calculate multiple hierarchies and partitions and integrate their information in negligible time. To illustrate this, we introduce a method for choosing the number of clusters that we find to work well in practice.
Given an ultrametric, our \emph{Median-of-Elbows} (MoE) algorithm starts by fitting the $(k, z)$-cluster hierarchies for $z = 1, 2, 3, 4\text{, and }5$. Intuitively, each of these hierarchies has a different penalty for large distances. Applying the elbow method to each hierarchy gives us a set of five $k$ values, each representing its hierarchy's `best' number of clusters. 
The MoE method picks the median $k$ from this set, which is essentially the $k$ that gives stable clusterings across values of $z$. Figure \ref{fig:elbow_plot} visualizes this process on the D31 dataset. Here, MoE selects $k=14$ (see also the D31 $k$-median/MoE cell in Figure \ref{fig:exp_ablation_partitioning}).


\input{figures/visualizations}
