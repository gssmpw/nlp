\section{Runtime Speedups / Efficient Operations}

% The code of our framework is available at \url{https://anonymous.4open.science/r/SHIP-42B1/}.  % This is already mentioned in the main paper. 

We now give an overview of how we implement the theory from Section \ref{app:k_center_proof} and \ref{app:k_z_clustering_proof} in practice in our codebase. We assume that we have already computed an ultrametric so that queries to the ultrametric require $O(1)$ time. From this, we describe how we build an LCA-tree, how to transform it to other clustering hierarchies, and how we extract partitions from it. We center this discussion on the dc-dist's dc-tree as a point of reference since we believe that this is the most common use case of our framework. However, we note that the discussion applies immediately to any relaxed ultrametric.

\subsection{Building and utilizing the tree efficiently (theoretical complexities)}
The overall structure of our implementation is the following:
\begin{outline}[enumerate]
    \1 \textbf{Building the tree}
        \2 Compute annotations over the dc-tree bottom up. These contain information that we will use for creating hierarchies.
        \2 Sort the annotations.
        \2 Create the hierarchy in a way that takes O(n) time utilizing the parent pointers in the annotations and in-place pointer updating. 
            
    \1 \textbf{Get each solution in O(n)}
        \2 Annotate each node in the tree with the k that resulted in creating this node, given that the $k$-th center is chosen.
        \2 Top-down (or bottom-up) algorithm that cuts all edges in the tree going from $k$-annotation $> k$ to $k$-annotation $\leq k$. The result is all nodes above the cut.
        \2 Small edge case to deal with for nodes with $> 2$ children.
    \1 \textbf{Initialize smart pointer access} to the leaves in the tree so that internal nodes can get their leaves in constant time.
\end{outline}
Now, each step in more detail:

\subsubsection{Building the tree}
How we build the tree in $O(n \cdot log(n))$.

Two main functions:
\begin{outline}
    \1 Annotate tree
    \1 Create hierarchy
\end{outline}

\textbf{Annotating the tree}\\
Annotate tree creates the annotations for each internal node in the tree. Each annotation is the following:
$A_k = [cost\_decrease, center, parent\_pointer, tree\_node]$.
\begin{idea}
    Only maximal annotations for a given center will be picked with the greedy algorithm.
\end{idea}
By maximal annotations, we mean those highest in the tree corresponding to a specific center, i.e., the annotation for that center with the highest annotated cost-decreases. This is very easy to see, as only maximal annotations will be children of marked paths of other, already picked, centers in the tree.

\begin{idea}
    The cluster corresponding to the parent center $p'$ of the maximal annotation of $p$ is exactly the cluster/set of points from which choosing $p$ will exclusively take points.
\end{idea}
This comes from the fact that each annotation is maximal in its corresponding subtree, which means that the maximal subtree above always will have chosen that center first. Also, any other center $p''$ chosen within that subtree of the maximal annotation of $p'$ will have taken disjoint points from that center $p'$'s cluster that $p$ cannot otherwise a contradiction and $p''$ should have been the parent annotation of $p$.

We formally define the parent center as the single center in solution $k-1$ that contains all points of the $k$'th center, which will get assigned for $k$. We now get the following insights needed to make building the tree efficient: 

1: We only need to store one annotation for each center, where we just store the highest / best annotation cost-decrease for that center. This can be updated as the annotations are computed. \\
2: Each annotation/center can contain a pointer to the parent center. 

Having these, we now have all the information required to build the tree - for each new center, we have exactly the cost-decrease corresponding to picking it and the parent center. We can sort the list of annotations, and each k solution will correspond to the first k centers picked in that order.


\textbf{Building the tree from the annotations}\\
First, we sort the annotations. Picking the first annotation corresponds to the root node and cluster of all points with that center. Picking a subsequent new center will always correspond to a split in the tree, splitting up the parent center's cluster. One side of the split will be what is left over from the parent cluster, and the other side will be the new cluster for this $k$. The only caveat is that if there are multiple, some number $a$, annotations with the same cost-decrease and parent, we only get $a+1$ nodes from this multi-split. We get a node for each of the $a$ new centers and the $a+1$'st node for what is left over in the partition of the parent node's cluster. 

Now, the key is that the cost-decreases are decreasing in the sorted order. This means that any subsequent annotations with a parent that already has been split up will always create a new split at the lowest possible node/cluster corresponding to that parent center, and all nodes above that also correspond to that parent center will never get new splits from them. This means that we can maintain for each center the current lowest/active node, which will be where any new center pointing to it should split from. This can easily be managed within the annotations with a pointer that is updated as the tree is created. 

The steps are then generally the following at a given $k$'th annotation for some center $c_k$ in the traversal of the sorted list:

Case 1: If the parent node $p$ corresponding to some center $c_p$ has no offspring, add two new nodes below. One for the new annotation and $p'$ for the old center representing that nodes have been taken from it. For the new node for $c_k$, update the annotation to point to it.  

Case 2: If the parent $p$ already has offspring associated with a higher cost-decrease, then add the new split nodes below $p'$  that then have to exist as that other offspring will have created that in case 1. Update the parent center's annotation to now point to $p'$ instead of $p$, as we can now be sure everything pointing to that center should never be added to $p$ but instead to $p'$ or further below at a later point. For the new node for $c_k$, update the annotation to point to it. 

Case 3: The other offspring of the parent node has the same cost-decrease associated with the current annotation. This means the $p'$ has already been created, so just add a singular new node as a child of $p$ corresponding to center $c_k$. For the new node for $c_k$, update the annotation to point to it. 



\textbf{Complexity}
Computing the annotations is a single-pass bottom-up algorithm over the tree that does constant work in each tree node. As there are $O(n)$ nodes in the tree, this step has a worst-case complexity of $O(n)$.

Sorting the list of annotations has a worst-case complexity of $O(n \cdot log(n))$ as there are $n$ annotations to sort. 

Computing the tree from the sorted annotations does a single scan over the list of annotations, with constant work in any iteration and, therefore, a worst-case complexity of $O(n)$. 

Therefore, the total worst-case complexity is $O(n \cdot log(n))$.




\subsubsection{Optimizing the tree: Resolving ties better} \label{app:ties_heuristic}

An important observation is that due to the number of equidistant points under the relaxed ultrametric, many centers will often have tied distances to points between them. For example, consider that we have placed our first center $c_1$ and it marks every node along the path to the root. Now let there be two subtrees $T_i$ and $T_j$ which are \emph{not} marked but whose parent node \emph{is} marked. We now place a center $c_2$ in $T_i$. This creates a new path of markings that stops at the root of $T_i$.

Now notice that \emph{every} node in $T_j$ is equidistant to center $c_1$ and to center $c_2$. Thus, we can leave $T_j$ assigned to $c_1$, or we can re-assign it to $c_2$, and the cost remains \emph{unchanged}. Over the course of placing $k$ clusters, there will inevitably be many such ties, implying that there are an exponential number of optimal solutions!

The previous theory described this setting by simply utilizing a ``first come, first served'' principle and never re-assigning points to new clusters. However, under the dc-dist and other minimax path distances, this may result in unintuitive clusterings. To see this, consider a set of 10 copies of the same cluster which are equally spaced apart. Let there be two centers assigned to these: $c_1$ is placed in the first copy of the cluster, and $c_2$ is placed in the second copy of the cluster. The remaining 8 clusters must now be assigned to either $c_1$ or $c_2$. However, due to them being equally spaced apart, we can assign each of these 8 clusters to either center and the cost will remain the same.

Our recommendation on how to resolve this is to introduce a secondary heuristic for tie-breaking. Namely, if a subtree is equidistant to two centers under the ultrametric, assign it to the center which it is closest to in the data's original metric. I.e., if our ambient distance metric is Euclidean, then we use the Euclidean distance as a tie-breaker. This provides the intuitive clustering that one expects when looking at groups of points.

In practice, we achieve this by giving each internal node in the tree a representative point - the Euclidean mean of its leaves. Each annotation not yet marked (and thus chosen at a later k) maintains the closest center based on the Euclidean distance to its representative, which is resolved between any center that marks its parent. At the k where this annotation itself is chosen, we have found the best annotation, and thus, the internal node that this annotation and sub-tree of nodes itself should become a subtree of. We update all annotation pointers in this way by placing the centers one by one, following the path from the center to the leaf checking distances to unmarked sub-trees, and updating the pointer if the new center is closer than other tied centers that had already been marked. 

As this is just a processing step over the annotations before the tree is actually constructed, it can easily be plugged in/out based on a boolean flag. Furthermore, the choice of heuristic can also easily be changed, but here it should be kept in mind that it might influence the running time if any complex function is used. 

The choice of using the mean point as a representative and comparing Euclidean distances is based on the following: under the dc-dist, one often gets one cluster which consists of multiple equidistant sub-clusters. These equidistant sub-clusters often follow snaky paths throughout the ambient space. Let us now suppose our large cluster gets split into two, implying that its sub-clusters must get assigned to one of these two new groups. Since the sub-clusters are all equidistant, they can be arbitrarily assigned to either of the two groups. By breaking ties so that the sub-clusters are closest to each other in the ambient space, we dramatically increase the chance of the sub-clusters getting re-assigned in an intuitive manner.

\textbf{Complexity}\\
This process requires traversing the tree from the leaf to the root for all centers placed. To construct the full hierarchy/dendrogram/tree of solutions, we do constant time calculations (scales with dimensionality) at each visit of an internal node, as it only requires computing the Euclidean distance of two points. This means that the worst-case complexity becomes $O(n^2)$ in the case of a fully unbalanced tree, but in practice and expectation, this will be much closer to $O(n \cdot log(n))$.




\subsubsection{Find solution for a given $k$}
To find the solution for a given $k$ efficiently, we start out by annotating each node of the new tree with additional information. Generally, the insight is that going from $k$ to $k+1$ corresponds to splitting up a single node/cluster into two new nodes in the tree. A slight detail is that some nodes will have multiple children, where the "split" is implicit as all the children split from the same node. We can mark at which $k$ each node is split from the parent in constructing the tree, where $k$ is just the current iteration. We simply mark all new nodes created in an annotation with that $k$.

To then recover a solution for a given $k$, all that is required is to cut all edges in the tree going from $k' \geq k$ to $k' < k$. The solution is the clusters associated with all the nodes above the cut. The only note is that if a leaf is reached and no cut is found in that path, the leaf is just part of the solution. 

The correctness of this algorithm comes from the fact that any clusters below this cut only existed at a higher $k''$ than the solution for $k$ we are recovering.


\textbf{Complexity}\\
Marking the tree is done with a constant factor added to the complexity of the list traversal of building the tree, so $O(n)$. 

Finding the cut can also be done in a single traversal of the tree top-down, simply stopping the traversal of a given branch when a cut is found and returning the nodes. So this is also $O(n)$.

Furthermore, the solution for a given $k$ can be stored, and the algorithm to find another $k$ can be "resumed" from this solution, making finding similar $k$ values very fast, almost constant. 





\subsubsection{Find leaves for a given node quickly / labeling a solution}
We create an array of the id's in the leaves, where each id is inserted in postfix order. This means that looking at the tree, each internal node of the tree will correspond to a continuous area of that array. If we store the bounds of those segments in each internal node, the nodes corresponding to a node can be returned in constant time by just returning that continuous segment of data of the new array. Only a single postfix order traversal of the tree is required to set up this array and pointers in the internal node, which then takes $O(n)$.

However, if we want to recover the labels in the standard form of each label being in the order that the points were provided, a traversal of the recovered solution is required, putting the corresponding label in the right place in an output array of labels. Simply put, the label value of point $p$ has to be inserted in place $p$ of the output array of labels. This requires worst-case $O(n)$ complexity for a given $k$ solution, as it is just a linear scan with constant work for each of the $n$ points.  



\section{The Algorithms}

We now describe how these algorithms are implemented. We use the terminology $n$-ary dc-tree to refer to a non-binary LCA-tree storing dc-dist relationships. Again, this immediately transfers to all relaxed ultrametrics.


Algorithm \ref{alg:k-centroid-annotation} describes how we find the corresponding $z$-centers and the costs associated with them in practice. Algorithm \ref{alg:k-centroid-hierarchy} describes how we use this information to obtain the corresponding $(k, z)$-clustering hierarchy. Algorithm \ref{alg:k-centroid-cluster} shows how we extract clusterings for a specific value of $k$ from a hierarchy. Lastly, Algorithm \ref{alg:opt_annos} describes how we implement the tie-breaking heuristic assuming Euclidean distance.



\renewcommand{\cost}{\text{cost}} %Field names
\newcommand{\id}{\text{id}}
\newcommand{\parent}{\text{parent}} 
\renewcommand{\children}{\text{children}}
\renewcommand{\dist}{\text{dist}}
\newcommand{\size}{\text{size}}
\newcommand{\tree}{\text{tree}}
\newcommand{\cent}{\text{center}}
\newcommand{\add}{\text{add}}
\newcommand{\costDec}{costDecrease}



\begin{algorithm}[bht]
    \newcommand{\bestCost}{bestCost}
    \newcommand{\bestCenter}{bestCenter}
    \newcommand{\currCost}{currCost}
    \newcommand{\currCenter}{currCenter}
    
    \caption{$k$-centroid-annotation}\label{alg:k-centroid-annotation}
    \textbf{Input:} An $n$-ary dc-tree $T$ over the dataset $\mathbf{X}$, power $z$, array $A$
    \begin{algorithmic}[1]
        \If{$|T| = 1$}
            \State $A[T.\id] = \{ (T.\parent.\dist)^z, T.\id, nullPtr\}$
            \State \textbf{return} 0, $T.\id$
        \Else
            \State $\bestCost = \infty$, $\bestCenter = -1$
            \For{$C \in T.children $}
                \State $subCost, subCenter = k\text{-centroid-annotation}\left( C, z \right)$
                \State $\currCost  = subCost + (T.\dist)^z \cdot (T.\size - C.\size)$
                \If{$\currCost < \bestCost$}
                    \State $\bestCost =\currCost$, $\bestCenter = \currCenter$
                \EndIf
            \EndFor
            
            \For{$C \in T.\children $}
                \State $A[C.\id].\parent = \bestCenter$
            \EndFor
            
            \State $\costDec = (T.\parent.\dist^z \cdot T.\size - \bestCost$
            \State $A[T.\id] = \{ \costDec, \bestCenter, nullPtr\}$
            \State \textbf{return} $\bestCost, \bestCenter$
        \EndIf
    \end{algorithmic}
\end{algorithm}

\newpage

\newcommand{\Anno}{A} %Annotation names
\newcommand{\ParentAnno}{a_{par}}
\newcommand{\CurrAnno}{a_{cur}} 
\newcommand{\Node}{\text{Node}} %Tree node names
\newcommand{\node}{N}
\newcommand{\NewNode}{\node_{new}}
\newcommand{\NewParent}{\node_{par}}
\newcommand{\ParentResidual}{\node'_{par}}

\begin{algorithm}[h]
    \caption{$k$-centroid-hierarchy}\label{alg:k-centroid-hierarchy}
    \textbf{Input:} An $n$-ary dc-tree $T$ over the dataset $\mathbf{X}$, power $z$
    \begin{algorithmic}[1]
        \State $\Anno = k\text{-centroid-annotation}\left( T, z, [T.size] \right)$
        \State $\text{Sort}(\Anno)$
        \State Root $ = \Node(\parent = nullPtr, \cost = -1, \id = \Anno[0].\cent)$
        \State $\Anno[0].\tree = $ Root
        
        \For{$\CurrAnno \in \Anno[1]...\Anno[n-1]$}
            \State $\NewNode = \Node(\parent = nullPtr,\cost = -1, \id = \CurrAnno.\cent)$
            \State $\CurrAnno.\tree = \NewNode$

            \State $\ParentAnno = A[\CurrAnno.\parent], c_{p} = \ParentAnno.\tree.\cost $
            \If{$c_p \neq \CurrAnno.\cost \land c_p \geq 0$} \Comment{Parent has added children with higher cost}
                \State $\NewParent = \ParentAnno.\tree.\children[0]$ 
                \State $\NewParent.\cost = \CurrAnno.\costDec$
                \State $\NewNode.\parent = \NewParent$
                \State $\ParentResidual = \Node(\parent = \NewParent, \cost = -1, \id = \NewParent.\id)$
                \State $\NewParent.\children.\add(\ParentResidual)$ \Comment{Add same center node as first child}
                \State $\NewParent.\children.add(\NewNode)$
                \State $\ParentAnno.\tree = \NewParent$ \Comment{Update annotation to point to lowest corresponding node}
                
            \ElsIf{$c_p < 0$} \Comment{Parent has no children added}
                \State $\NewNode.\parent = \ParentAnno.\tree$
                \State $\ParentResidual = \Node(\parent = \ParentAnno.\tree, \cost = -1, \id = \ParentAnno.\id)$
                \State $\ParentAnno.\tree.\children.\add(\ParentResidual)$
                \State $\ParentAnno.\tree.\children.\add(\NewNode)$
                
            \Else \Comment{Parent has children added with same cost}
                \State $\ParentAnno.\tree.\children.\add(\NewNode)$
                \State $\NewNode.\parent = \ParentAnno.\tree$
            \EndIf
        \EndFor
        \State \textbf{return} Root
    \end{algorithmic}
\end{algorithm}


\newpage


\begin{algorithm}[h]
    \newcommand{\searchK}{searchK}
    \newcommand{\minK}{minK}
    \newcommand{\maxK}{maxK}

    \caption{$k$-centroid-cluster}\label{alg:k-centroid-cluster}
    \textbf{Input:} An annotated $k$-centroid hierarchy-tree $T$ over the dataset $\mathbf{X}$, $\searchK$
    \begin{algorithmic}[1]
        \If{$|T| = 1$}
            \State \textbf{Output} $T$            
        \EndIf
        
        \State $\maxK = \text{maxK}(T.\children), \minK = \text{minK}(T.\children)$

        \If{$\maxK \leq \searchK$} \Comment{Every edge should be cut}
            \For{$C \in T.\children $}
                \State $k\text{-centroid-cluster}(C, \searchK)$
            \EndFor
        \Else \Comment{Only some edges should be cut}
            \State $A = []$
            \For{$C \in T.\children $}
                \If{$C.k > \searchK \lor C.\text{is\_orig\_cluster}$} \Comment{Non-cut edges merge with original}
                    \State $A.\add(C)$
                \Else
                    \State \textbf{Output} $C$
                \EndIf
            \EndFor
            
            \State \textbf{Output} $\text{Merge}(A)$
        \EndIf
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
    \caption{Optimize Annotations}\label{alg:opt_annos}
    \textbf{Input:} Sorted list of annotations in decreasing order $\Anno$, tree $T$ annotated with representatives 
    \begin{algorithmic}[1]
       \For{$\CurrAnno \in \Anno$} \Comment{For each annotation}
            \State $\node = \CurrAnno.leaf$
            \While{$\node \neq null$} \Comment{Traverse from center leaf to root}
                \State $\node.mark(\CurrAnno.center)$ 
                \For{$C \in N.children$}
                    \If{$C.mark = null$} \Comment{Any unmarked children of marked path}
                        \State $C.anno.bestParent = min\{C.anno.bestParent, Euclid(C.rep, Anno.center)\}$
                    \EndIf
                \EndFor
                \State $\node = \node.parent$
            \EndWhile
       \EndFor
       \State \textbf{Return} $\Anno$ \Comment{Return list of annotations with updated pointers}
    \end{algorithmic}
\end{algorithm}
