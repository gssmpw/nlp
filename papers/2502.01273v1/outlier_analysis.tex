\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper, portrait, lmargin=0.5in, rmargin=0.5in, tmargin=1in,bmargin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{physics}
\usepackage{mathtools}
\usepackage{amsthm} 
\usepackage{amsmath} 
\usepackage[]{amssymb} 
\usepackage{hyperref}
\usepackage{bm}
\usepackage{systeme} 
\usepackage{enumerate} 
\usepackage{framed} 
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{listings}
\usepackage{graphics}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{tcolorbox}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{forest}
\usepackage{listings}
\usepackage{pgfplots}
\usepackage{float}
\setlength{\parindent}{0pt}
\usepackage{pgfplots}

\usepackage{xcolor}

\lstset{style=mystyle}
% Augmented matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
    \hskip -\arraycolsep
    \let\@ifnextchar\new@ifnextchar
    \array{#1}}
\makeatother

\pagestyle{fancy}
\rhead{\thepage}

\title{LLM Usage for Software Engineering}

\begin{document}

\maketitle



\section*{Outlier Analysis}

The analysis evaluates four key metrics to assess the quality and complexity of students’ software engineering codes: Total Lines of Code, Cyclomatic Complexity, Maximum Control Flow Graph (CFG) Depth, and Halstead Metrics for Effort. Each metric provides a different perspective on the structure, maintainability, and cognitive effort required to understand the code. These metrics collectively offer a comprehensive overview of how intricate or straightforward the student-generated code is across different sources and milestones.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/outliers_iden.png}
    \caption{Boxplot of Original Data to show heavy incluence of Outliers }
    \label{fig:enter-label}
\end{figure}

Across the three project milestones (MS1, MS2, and MS3) and code sources (student repositories, GPT-generated code, and Copilot-generated code), we observed a significant presence of outliers. Most distributions were heavily skewed, with many outliers, as determined using Tukey’s statistical definition of outliers based on the interquartile range (IQR). This skewness indicates that while most students’ codes follow relatively simple patterns, there are distinct cases where the code deviates sharply from the norm, either due to excessive complexity or volume.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/density_comp_outliers.png}
    \caption{Density Plots for the 4 Metrics separated for Outliers and Non Outliers}
    \label{fig:enter-label}
\end{figure}

The outliers consistently appeared at the positive extremes for all four metrics. In practical terms, this means that while the majority of student codes (around 80\%) demonstrated low values across these metrics—indicating straightforward code structures—outliers exhibited much higher values, sometimes 40 to 50 times greater than the median. This suggests instances where a few teams may have heavily relied on AI tools like GPT and Copilot, leading to excessively complex, verbose, or over-engineered code. 
\newline
\newline
To improve the analysis and derive more meaningful insights, the data was divided into two parts: normal data and outliers. We first identified the outliers for each source (Repository, GPT, and Copilot) and milestone (MS1, MS2, and MS3) based on Tukey’s definition across all four metrics. Once identified, the distributions were studied separately for the outlier and non-outlier groups. This approach was essential for several reasons: it allowed a clearer understanding of the typical coding patterns among students, isolated cases of AI overuse, and provided a fair comparison between the different sources and milestones. This separation ensures that the analysis accurately reflects both the average student performance and the unique instances where AI tools significantly impacted code quality.


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/outliers_count.png}
    \caption{Outlier Counts}
    \label{fig:enter-label}
\end{figure}

Our analysis also investigates where outliers are most prevalent and reveals that the majority of outliers occur in Copilot-generated codes. This indicates that Copilot often produces code with significantly higher complexity and volume, far exceeding the median values across all four metrics. The trend is consistent across all metrics and project milestones, suggesting that Copilot’s generative behavior frequently results in code that is overly complex or verbose. GPT-generated code also shows a notable number of outliers, but fewer than Copilot, implying that while GPT can create intricate code, it does so less aggressively than Copilot. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/ouliers_dom_1.png}
    \caption{Outlier Counts}
    \label{fig:enter-label}
\end{figure}

We further explored whether the same set of outputs consistently qualifies as outliers across multiple metrics. To examine this, we graphed the number of AI-generated outputs that were flagged as outliers across the 4 metrics. The results indicate that many outputs were outliers in two to four metrics simultaneously, suggesting a correlation between metrics: when a piece of code is identified as an outlier for one metric, it is more likely to be flagged as an outlier in others. This trend points to an inherent relationship between different aspects of code quality, such as complexity, size, and cognitive effort.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/outliers_pp.png}
    \caption{Outlier Counts}
    \label{fig:enter-label}
\end{figure}

To deepen our understanding of these relationships, we plotted pairwise comparisons across the four metrics to study the probability of a conversation or code being an outlier in one metric relative to another. Specifically, we calculated the proportion of AI-generated code in each conversation marked as an outlier for each metric and analyzed the relationships between them. The pairwise plots reveal that some relationships are particularly strong—for example, between Cyclomatic Complexity and Halstead Metrics for Effort, or between Maximum CFG Depth and Cyclomatic Complexity. This suggests that codes with high logical complexity tend to also demand more cognitive effort to understand and have deeper control flow structures. In contrast, Total Lines of Code does not exhibit such strong correlations with the other metrics, implying that while code length can vary widely, it does not necessarily correspond to high complexity or effort. This analysis underscores how AI-generated outputs tend to amplify multiple aspects of complexity simultaneously, particularly logical intricacy and control depth.


\section*{Repo and GPT Code Comparisons}


To better understand how students modify and integrate GPT-generated code into their repositories, we compared the GPT-generated outputs with the corresponding student repository code. For each conversation, we identified the segment of GPT code with the highest average similarity to the repository code and used it as a reference point. Then, for all instances where code from GPT was utilized, we calculated the average differences between the repository code and the original GPT-generated code across the four metrics. These differences were plotted on a density plot, with a vertical line representing zero difference for reference. Note that since the values were log-transformed, the zero point does not align directly with zero on the plot. A vertical line has been added to show the zero points post log transformation allowing for clearer insights into the data distribution.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/dens_rvsg.png}
    \caption{GPT vs Repo Codes}
    \label{fig:enter-label}
\end{figure}
The plot shows that a substantial portion of the observations fall on the positive side of the deviation, meaning that for most cases, the repository code is more complex than the original GPT-generated version across all four metrics—Total Lines of Code, Cyclomatic Complexity, Max CFG Depth, and Halstead Effort. This trend indicates that students frequently modify AI-generated code before including it in their repositories, often increasing its complexity. The modifications are not minor tweaks but significant changes, as evidenced by the higher values in multiple complexity metrics. This suggests that students tend to enhance the initial AI-generated code by adding more logic, deeper control structures, or additional lines, making the final version more intricate.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/boxplot_rvsg.png}
    \caption{GPT vs Repo Codes}
    \label{fig:enter-label}
\end{figure}
These insights reveal that AI tools like GPT provide a foundational base that students use and build upon, often adapting it to meet specific project requirements. The tendency to increase complexity could reflect students’ efforts to customize the code for better functionality or alignment with project goals, or it could also indicate over-engineering, where students inadvertently introduce unnecessary complexity. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{outlier_analysis_imgs/AVGSIMms.png}
    \caption{Code Similarity Over Milestones}
    \label{fig:enter-label}
\end{figure}

Lastly, we compare the average similarity between the Repo and corresponding GPT code over the milestones. The plot shows a progressive increase in average similarity between the generated and repository code across the milestones (MS). In MS1, the similarity is generally low, with a broad range and many outliers, indicating that participants significantly modified the AI-generated code early on, experimenting with custom implementations. By MS2, the median similarity improves, suggesting that participants began relying more on the provided code while still introducing some variations. The spread remains wide, showing that some still deviated considerably, but others followed the original code more closely. In MS3, the similarity peaks, with a higher median and fewer outliers. This suggests that participants leaned more on the AI-generated code toward the later stages, likely to save time or because they became more comfortable integrating it directly into their projects. Additionally, MS1 has the most upper-end outliers, indicating that, even in the initial stages, some participants used AI-generated code with minimal changes. MS3 shows more consistency overall, with fewer outliers and a trend toward higher similarity, likely reflecting a more refined and stabilized workflow in later stages. The differences between the milestones reflect the learning curve of the participants: as they become more comfortable with AI-generated code, they opt for more direct usage in their final implementations.

\section*{Code Complexity Over Conversations}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/conv_var_metric.png}
    \caption{Code Complexity Throughout Conversation}
    \label{fig:enter-label}
\end{figure}

The above plot tracks the mean values of four complexity metrics (Cyclomatic Complexity, Halstead Metrics for Effort, Max CFG Depth, and Total Lines) over the first 20 conversations.

\begin{itemize}
    \item Cyclomatic Complexity: At the beginning of conversations, the complexity tends to be high, especially in MS1 (purple). However, the trend shows a gradual decrease as conversations progress, indicating that the code generated or discussed becomes less complex over time. This reduction could reflect initial brainstorming or experimentation with complex logic, which is simplified in later stages. 
    \item Halstead Metrics for Effort: This metric, which measures the cognitive effort required to understand the code, is highest in the early stages of MS1 and MS2 but decreases rapidly as the conversations proceed. The initial peaks suggest that students may require more effort to create or modify code early on, possibly as they get familiar with the problem requirements or AI-generated outputs. Interestingly, MS3 shows a generally lower effort throughout, hinting that students might have become more comfortable working with the code by this stage, or the problems they were addressing became more streamlined.
    \item Max CFG Depth: The control flow depth shows more fluctuations across milestones, with MS1 displaying some of the highest peaks. This suggests that students initially used deeper or more complex logical flows (possibly experimenting with AI-generated code) but later shifted to more straightforward implementations as discussions progressed. Both MS2 and MS3 maintain a lower and more stable CFG depth, reflecting a shift towards simpler control structures as students refined their work over time.
    \item Total Lines of Code: Across all milestones, the number of lines of code decreases steadily with each conversation, suggesting that students likely start with larger or more verbose code and gradually refactor it into more concise forms. This trend aligns with good coding practices, where initial drafts may be bulky and later iterations more efficient.
\end{itemize}

\newpage

\section*{Prompt Analysis}

\subsection*{Sentiment Analysis}

To gain insights into user interactions with LLMs across the three milestones, we performed sentiment analysis on the user prompts using the VADER (Valence Aware Dictionary and sEntiment Reasoner) tool. VADER is well-suited for analyzing the sentiment of short texts, like prompts, as it captures nuances in the language often found in user-generated content. For each milestone, sentiment scores—ranging from positive, neutral, to negative—were calculated for prompts associated with different hat IDs. This approach allows us to observe changes in user sentiment over time as users become more accustomed to interacting with LLMs or as they encounter different challenges across milestones. By analyzing these sentiment scores, we can determine whether users generally felt positive, neutral, or frustrated during their interactions at each milestone. This analysis can help us understand user engagement patterns and the evolution of their expectations and comfort levels with LLM assistance over the project timeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/prompts/vader_sc_distr.png}
    \caption{Distribution of Vader Scores Across Milestones}
    \label{fig:enter-label}
\end{figure}

The above plot illustrates the evolution of sentiment distributions across three milestones, as measured by the VADER sentiment analysis scores for user prompts. In the compound sentiment distribution, we observe a progression from a wide and relatively balanced distribution in MS1-3, centered around zero, but with heavier positive tails indicating that there were more positive than negative interactions overall across all 3 milestones. 

In the negative sentiment distribution, most scores are clustered close to zero, indicating that strong negative expressions are relatively rare, with only occasional outliers showing higher negative sentiment. The neutral distribution, by contrast, has a strong peak near 1, suggesting that a large portion of prompts are neutral in tone. Finally, the positive sentiment distribution shows a moderate density at lower positive values, suggesting that while positive sentiment is present, it is not typically intense.

Overall, these sentiment distributions imply that user interactions are mostly neutral or slightly positive, with infrequent negative or highly positive expressions. This pattern suggests users tend to maintain a balanced or cautious tone in their interactions, with a generally favorable leaning.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/prompts/varder_var.png}
    \caption{Variation of Vader Scores Over Conversation: Boxplot}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/prompts/vader_sc_distr_line.png}
    \caption{Variation of Vader Scores Over Conversation: Smooth Plot (LOESS)}
    \label{fig:enter-label}
\end{figure}


This above plot reveals sentiment fluctuations throughout the progression of user messages within a single milestone. Each box plot represents the compound VADER score for different message numbers in a sequence, from the start to the end of the conversation (filtered to focus on the first 10 messages). In each milestone, the first few messages generally exhibit higher positive sentiment scores, which could indicate users beginning interactions on a constructive or hopeful note. As the conversation advances, sentiment tends to vary more widely, with both positive and negative scores appearing across different messages. This pattern suggests that user sentiment becomes more dynamic as they engage deeper, potentially reflecting the iterative nature of seeking help or resolving issues.

The middle messages (around messages 4–7) tend to show the widest range of compound scores, encompassing both highly positive and somewhat negative sentiments. This could result from users facing challenges or expressing frustration while interacting with the system. The variability here reflects points of potential struggle or complexity in communication, where users may express mixed sentiments as they clarify questions or seek further assistance.

Towards the end of the conversation (messages 8–10), sentiment generally stabilizes with lower overall variability, which might suggest resolution or closure. Positive scores are still present, likely indicating successful interactions or satisfactory answers, while negative scores diminish, hinting at fewer frustrations. This overall shape—starting positively, fluctuating in the middle, and stabilizing toward the end—may point to a natural rhythm in conversation, with users starting optimistically, experiencing challenges, and then concluding with a settled outlook as their issues are addressed.


\subsection*{Number of Lines}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{outlier_analysis_imgs/prompts/lines_distr.png}
    \caption{Distribution of number of Lines Across Milestones}
    \label{fig:enter-label}
\end{figure}

The aboce plot shows the distribution of the number of lines in user prompts across three milestones, revealing a similar shape in each. Each milestone demonstrates a prominent peak at lower line counts, indicating that most prompts are relatively concise. The sharp rise at the beginning, with a high concentration around shorter prompts, suggests that users primarily engage with brief requests or instructions. After the peak, the distribution gradually tails off as the line count increases, with a few instances of significantly longer prompts. This elongated tail indicates occasional, more detailed prompts but emphasizes that such extended interactions are relatively rare. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{outlier_analysis_imgs/prompts/lines_var_smooth.png}
    \caption{Variation of Number of Lines Over Conversation: Smooth Plot (LOESS)}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/prompts/lines_var_boxplot.png}
    \caption{Variation of Number of Lines Over Conversation: Boxplot (without outliers)}
    \label{fig:enter-label}
\end{figure}


This plot illustrates how the number of lines in prompts varies throughout the progression of a conversation within each milestone. At the start of each conversation (Message 1), we see significantly higher line counts, suggesting that initial prompts are often more detailed or require elaborate setup, perhaps to provide necessary context or instructions. As the conversation advances, the number of lines generally decreases and stabilizes, indicating a shift to shorter, more direct interactions. This pattern likely reflects a common dialogue structure where users start with comprehensive, information-rich prompts and then rely on concise follow-up questions or clarifications as the conversation progresses. Notably, spikes in line counts occasionally occur mid-conversation (e.g., Message 8 in Milestone 1), possibly due to the need for additional explanations or adjustments in specific scenarios. This trend reflects an largely efficient conversational dynamic, where the initial setup is followed by streamlined, targeted interactions.









\end{document}
