\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper, portrait, lmargin=0.5in, rmargin=0.5in, tmargin=1in,bmargin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{physics}
\usepackage{mathtools}
\usepackage{amsthm} 
\usepackage{amsmath} 
\usepackage[]{amssymb} 
\usepackage{hyperref}
\usepackage{bm}
\usepackage{systeme} 
\usepackage{enumerate} 
\usepackage{framed} 
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{listings}
\usepackage{graphics}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{tcolorbox}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{forest}
\usepackage{listings}
\usepackage{pgfplots}
\usepackage{float}
\setlength{\parindent}{0pt}
\usepackage{pgfplots}

\usepackage{xcolor}

\lstset{style=mystyle}
% Augmented matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
    \hskip -\arraycolsep
    \let\@ifnextchar\new@ifnextchar
    \array{#1}}
\makeatother

\pagestyle{fancy}
\rhead{\thepage}

\title{LLM Usage for Software Engineering}
\begin{document}

\maketitle

\tableofcontents

\section{Methodology}

The goal of this study is to explore the ways computer science students engage with LLMs like ChatGPT and GitHub Copilot in software engineering projects, particularly in the context of code generation and adaptation over time. By examining how students interact with these models, adapt generated code, and refine their prompting strategies, we aim to reveal the dynamics of human-AI collaboration in SE education. Here we unpack=a multi-layered analysis that spans the complexity of generated code, the processes of adaptation within student repositories, and the evolving conversational interactions between students and LLMs across three distinct project milestones (MS1, MS2, MS3).
\newline
\newline
To capture the intricacies of LLM-generated code quality, we conducted an extensive analysis using four complexity metrics: total lines of code (LOC), cyclomatic complexity, maximum control flow graph (CFG) depth, and the Halstead effort metric. Each of these metrics highlights a unique aspect of code sophistication for a clearer understanding of the structural and logical complexity embedded in LLM outputs. Total lines of code provide a surface-level measure of verbosity, with unusually lengthy blocks hinting at potential over-reliance on AI-generated solutions. Cyclomatic complexity, on the other hand, delves into the logical intricacies by counting linearly independent paths, which can reflect maintainability concerns in more intricate LLM-generated snippets. Similarly, examining the maximum depth of control flow reveals how deeply nested or hierarchically complex the code becomes, while the Halstead effort metric offers a cognitive perspective, estimating the mental load needed to understand and adapt the generated code. A preliminary look at these metrics across the milestones revealed interesting contrasts, with a large portion of students producing moderately complex code, while certain outliers surfaced. Approximately 20\% of the cases exhibited extreme values in one or more metrics, suggesting either intentional complexity for specific project requirements or potential overuse of LLM-generated solutions. By identifying these outliers, we observed patterns that may signify excessive dependency on LLM outputs, which forms the basis for further inquiry into the adaptive processes students use when integrating LLM-generated code into their repositories.
\newline
\newline
To explore the relation between LLM-generated code and the code found in each student's repository, we performed a comparative analysis to understand how students modified the AI-generated snippets to meet project requirements. Using the same complexity metrics, we observed whether the original structure was simplified, expanded, or restructured during integration: ranging from direct adoption of LLM outputs to significant manual refinements, with some students reconfiguring the AI-generated code to enhance functionality or maintainability. The statistical analysis of these differences provided deeper insights into how students’ technical goals aligned—or diverged—from the raw LLM suggestions. To further dissect the modifications made, we applied Tree-Sitter, a syntax parsing tool, to calculate syntactic similarity between LLM-generated code snippets and final repository code. This process allowed us to evaluate how directly students used the AI outputs versus how much they altered them to suit specific project requirements. High similarity scores suggested minimal modification, indicative of instances where the AI-generated solution met the students’ needs directly. In contrast, low similarity scores hinted at extensive customization, often aligning with cases where students used the initial LLM output as a starting framework but transformed it substantially through iterative refinement. These patterns provide a window into the decision-making processes underlying student-LMM interaction across different milestones of the project.
\newline
\newline
Analyzing conversation history added a qualitative layer to our understanding by focusing on the interactions students had with the LLMs across milestones. By examining how students crafted prompts, responded to LLM-generated suggestions, and evolved their conversational strategies, we captured a dynamic view of the human-AI exchange. To understand how complexity unfolded over the course of a conversation, we tracked complexity metrics at different stages within each dialogue. For example, we divided conversations based on whether students engaged with a single LLM output or iterated through three or more outputs to see how code complexity fluctuated with increased iteration. In addition to complexity progression, we examined sequential similarity between LLM outputs within a single conversation. This similarity analysis helped clarify whether students iteratively refined the LLM-generated code, maintaining a base framework but adapting it through successive prompts. 
\newline
\newline
Finally, to further understand students' prompting behavior, we also analyzed the characteristics of their prompts over time, including sentiment, length, and structure. Sentiment analysis using the VADER tool allowed us to capture shifts in tone as students became more comfortable with the LLMs, reflecting a gradual shift in how they perceived the LLM’s role in assisting their projects. Prompt length and structure also provided insights into students’ evolving strategies for guiding the LLMs. Prompt length, in particular, served as a marker for specificity, with longer prompts often correlating with more detailed requests as students learned to fine-tune their instructions for the AI. This adaptation in prompt construction suggests a developing sophistication in how students approached AI-driven assistance, seeking either more concise outputs for simpler tasks or detailed, layered responses for complex queries. 


\section{Results}

\subsection{Analysis of LLM-Generated Code Quality}

In evaluating the quality of AI-generated code, our analysis centered on how students utilized large language models (LLMs) such as ChatGPT and Copilot across three distinct project milestones (MS1, MS2, and MS3). By examining code snippets generated by students with both LLMs, we sought to understand not only the code’s structural and logical intricacies but also its cognitive complexity, offering a lens into how AI tools may influence software engineering processes at the student level. 

\subsubsection{Outliers and Skewness in Complexity Metrics} Analysis of metric distributions (Total Lines of Code, Cyclomatic Complexity, Maximum Control Flow Graph (CFG) Depth, and Halstead Effort Metric (E)) revealed a heavily skewed dataset with significant outliers, particularly at the positive extremes of all four metrics. Outliers were identified using Tukey's statistical definition based on the interquartile range (IQR):
\[
[Q1- 1.5 \times \text{IQR}, \; Q3 + 1.5 \times \text{IQR}]
\]
The presence of these outliers suggested that while the majority of students generated relatively simple code structures, certain students or teams, representing approximately 20\% of cases, produced code with far higher complexity, often due to heavy reliance on AI-generated code. This was evident from boxplots (Figure \ref{fig:fig1}) illustrating the extreme values and density plots (Figure \ref{fig:fig2}) that clearly separated the distributions from outliers.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{outlier_analysis_imgs/outliers_iden.png}
    \caption{Boxplot of Original Data to show heavy incluence of Outliers }
    \label{fig:fig1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{outlier_analysis_imgs/density_comp_outliers.png}
    \caption{Density Plots for the 4 Metrics separated for Outliers and Non Outliers}
    \label{fig:fig2}
\end{figure}

The outliers consistently appeared at the high end across all four metrics, meaning that while the average complexity of student-generated code remained moderate, some AI-assisted outputs were notably complex, occasionally reaching values 40 to 50 times above the median. This distribution underscores cases where AI-generated code, while potentially effective, could lead to verbose or over-engineered solutions if applied without modification. To deepen our insights, we divided the data into outliers and non-outliers, allowing a fair comparison across different sources (Repository, GPT, Copilot) and milestones. This separation illuminated average student coding patterns while isolating cases where AI assistance contributed to unusually high code complexity. 


\subsubsection{Outlier Distribution in Copilot and GPT} To examine this phenomenon more closely, we conducted a deeper investigation into the prevalence of outliers across Copilot and GPT-generated code. This analysis revealed that the majority of outliers were concentrated in Copilot’s outputs, suggesting that its generative process may tend towards complexity more frequently than GPT’s. By focusing on these outliers, we gained insights into the differences in how each tool influences code generation.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/outliers_overall.png}
    \caption{Outlier Counts}
    \label{fig:fig5}
\end{figure}

This trend suggests that Copilot’s code often exceeds standard complexity and volume, potentially reflecting its generative approach. GPT, while still capable of producing intricate code, resulted in fewer outliers than Copilot, suggesting that it generally aligns more closely with median complexity values. The greater complexity in Copilot code may point to its tendency to produce extensive, perhaps over-detailed code responses, impacting both code volume and logical structure. In addition, by mapping outliers across all four metrics, we observed instances where single AI-generated outputs consistently exhibited high values across multiple complexity measures. For example, numerous Copilot-generated snippets appeared as outliers simultaneously in two, three, or all four metrics. This overlap suggests a correlation between different aspects of code complexity: when a snippet was identified as an outlier in one metric, it was likely to be flagged as an outlier in others. This pattern highlights interrelated facets of code quality, such as logical complexity, hierarchical depth, and cognitive load, which together contribute to high-complexity code outputs.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/outliers_pp.png}
    \caption{Outlier Relationships across 4 metrics}
    \label{fig:fig6}
\end{figure}

To explore these relationships further, we conducted pairwise comparisons between the metrics, where we identified particularly strong relationships between Cyclomatic Complexity and Halstead Effort, as well as between Maximum CFG Depth and Cyclomatic Complexity. This indicates that code with higher logical complexity tends to demand more cognitive effort and exhibits deeper control flow structures. Interestingly, Total Lines of Code did not strongly correlate with other metrics, suggesting that while verbosity varies widely, it does not necessarily imply high complexity or effort. These relationships underline the multifaceted nature of AI-generated code, where factors like logical structure, cognitive demand, and control depth are often amplified concurrently in high-complexity outputs.

\subsubsection{Conversations Across Milestones} 
Lastly, we also examined the patterns of LLM interaction across milestones to understand how students’ use of AI support evolved: the length and frequency of LLM conversations provided additional context for understanding AI interaction. We observed a decline in conversation length and frequency from MS1 to MS3, suggesting that as students grew more accustomed to the project, their reliance on detailed prompts or extensive exchanges with LLMs decreased. This trend could indicate an improvement in students’ confidence and coding independence or a shift toward more focused, refined prompt-crafting strategies. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{outlier_analysis_imgs/boxplot_length_ms.png}
    \caption{Box Plots for Conversation Lengths Across Milestones}
    \label{fig:fig3}
\end{figure}



\subsection{Comparative Analysis of Repository and GPT-Generated Code}

Our second analysis focuses on how students modify and integrate GPT-generated code into their project repositories, examining the differences between the AI-generated outputs and the final repository versions across multiple metrics. This comparison offers insights into the patterns of student adaptations, highlighting whether and how students enhanced or simplified the GPT-generated base code.

\subsubsection{Repo vs GPT Code: Metrics and Distribution Analysis}

To assess the extent of modification, we compared each student’s repository code with the corresponding GPT-generated code for each conversation. Specifically, we identified the segment of GPT code with the highest average similarity to the repository version and used it as a reference. For each instance where GPT code was incorporated, we calculated the average differences between the repository code and the original GPT output across four primary metrics: Total Lines of Code, Cyclomatic Complexity, Maximum Control Flow Graph (CFG) Depth, and Halstead Effort. These differences were visualized on a density plot with a vertical line representing zero deviation post-log transformation to enhance interpretability.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/dens_rvsg.png}
    \caption{GPT vs Repo Codes}
    \label{fig:fig7}
\end{figure}

The density plot (Figure \ref{fig:fig7}) reveals that a significant proportion of deviations are positive, indicating that repository code is frequently more complex than the original GPT version. This trend is observed across all four metrics—suggesting that students often alter GPT-generated code by increasing its complexity, whether by adding more lines, enhancing logical structures, or deepening control flows. Such modifications go beyond minor tweaks, as evidenced by higher values across multiple metrics in the repository versions, indicating substantial enhancement or customization of the initial AI output. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/boxplot_rvsg.png}
    \caption{GPT vs Repo Codes}
    \label{fig:fig8}
\end{figure}

To further investigate this trend, we visualized the differences in a boxplot format (Figure \ref{fig:fig8}) to capture the distribution and outliers. The boxplot reiterates that most modifications lean towards increased complexity, with outliers appearing predominantly in the positive range across all metrics. This pattern implies that students generally use GPT-generated code as a starting point, enhancing its complexity to align better with specific project requirements or goals. This tendency could reflect intentional customization for project needs, but it may also suggest occasional over-engineering, where students inadvertently introduce unnecessary complexity in the modification process.


\subsubsection{Code Similarity Over Milestones: Tree-Sitter Analysis} 

To gain further insight into these modification patterns, we quantified logical and structural similarity using Tree-Sitter, focusing on comparing logic and function definitions between the repository and GPT-generated code across milestones. The similarity scores over time provide insights into students' evolving relationship with AI-generated code, reflecting changes in adaptation patterns as they progressed through the project.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{outlier_analysis_imgs/AVGSIMms.png}
    \caption{Code Similarity Over Milestones}
    \label{fig:fig9}
\end{figure}

The similarity trends are illustrated in Figure \ref{fig:fig9}, showing an increase in average similarity between repository and GPT code over the project milestones. In Milestone 1 (MS1), similarity levels were generally low, with a wide range of scores and many outliers. This broad distribution suggests that students were more experimental in MS1, heavily modifying AI-generated code and opting for custom implementations. By Milestone 2 (MS2), the median similarity increased, indicating a shift towards greater reliance on the original GPT outputs, though with some remaining variations. The range of similarity scores, while still relatively wide, showed that some students continued to significantly customize the AI code while others followed it more closely. In Milestone 3 (MS3), the trend peaks: similarity scores reach higher median values with fewer outliers, suggesting that students relied more directly on GPT-generated code in the later stages. This increasing reliance likely reflects a maturing familiarity with the AI tool, as students became more comfortable incorporating GPT-generated code directly into their projects. The progressive increase in similarity across milestones suggests a learning curve. Early on, students were more exploratory, modifying AI outputs significantly. As they gained confidence in integrating AI-generated content, they leaned towards more direct usage, likely as a time-saving measure or due to increased comfort with the AI’s quality. Notably, while MS1 contained the highest number of high outliers—indicating that some students used the AI code with minimal alteration from the outset—MS3 displayed greater consistency and less deviation, likely representing a stabilized workflow and a more refined approach to AI integration in the project’s final stages.

\subsection{In-depth Analysis of Conversations}

In our next analysis, we delve deeper into the complexity and similarity metrics of code generated by GPT during individual conversations across milestones. This exploration reveals how students interact with GPT outputs, iteratively refining or adjusting them to fit their projects.

\subsubsection{Complexity of GPT-Generated Code Within Conversations}

First, to assess complexity patterns, we analyzed where the most intricate code snippets generated by GPT appeared within each conversation. For each session with at least one GPT output, we identified the conversation stage (measured as a percentage) at which the most complex code was generated. For example, if a conversation consisted of five outputs and the fourth was the most complex, it was marked as occurring at 80\% of the conversation. A similar marking at 100\% completion was made for conversations where the most complex code appeared in the final output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/tree_sitter_complexity_conversation_min_group-1-12052024.png}
    \caption{Complexity of generated code in chatGPT chats with at least 1 LLM output (i.e. min\_group\_1)}
    \label{fig:fig10}
    \vspace*{-1\baselineskip}
\end{figure}

In conversations where students received one or more GPT responses, the most complex code often appeared towards the end. However, there were notable spikes at both 50\% and 100\%, mainly due to many shorter conversations with only one or two outputs. This limitation prompted us to explore patterns within longer interactions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/tree_sitter_complexity_conversation_min_group-3-12052024.png}
    \caption{Complexity of generated code in chatGPT chats with at least 3 LLM outputs (i.e. min\_group\_3)}
    \label{fig:fig11}
    \vspace*{-1\baselineskip}
\end{figure}
 
In conversations with at least three GPT outputs, we observed a trend where the most complex outputs (particularly in total lines and Halstead Effort) appeared at later stages. This suggests that as students seek satisfactory results, they tend to request increasingly complex outputs towards the end of their sessions. Interestingly, maximum CFG depth—a measure of control flow complexity—often peaked near the start of conversations, implying that early attempts included deeper logical structures which were later refined. In longer conversations where there are at least 3 outputs from the LLM, we observe a clear pattern in the total lines and Halstead effort metrics for code complexity, indicating that more complex code are generated towards the end of a chat in chatGPT, and are then getting adopted by the students for integration into their codebases. The pattern in max CFG depth is also similar to the overall CFG trends, as fewer chats had the most nested code appear at the end than chats that had them at the start, indicated by the higher frequency towards 0 on the MaxCfgDepth graph.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/tree_sitter_conversation_similarity_evolution_min_group-1-12052024.png}
    \caption{Similarity of generated code in chatGPT chats with at least 1 LLM output (i.e. min\_group\_1)}
    \label{fig:fig12}
    \vspace*{-1\baselineskip}
\end{figure}

Additional measurements were performed on the captured chatGPT conversations, to determine if the final version of the generated code used in the codebase was further modified by the students, and also if the LLM had been tasked with improving the quality of the output. Several criteria such as the average similarity score, code snippet similarity, number of total lines, and function lines were measured, and the results were once again filtered with a minimum conversation count of 3, to remove the effect of conversations containing only 1 or 2 outputs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/tree_sitter_conversation_similarity_evolution_min_group-3-12052024.png}
    \caption{Similarity of generated code in chatGPT chats with at least 3 LLM outputs (i.e. min\_group\_3)}
    \label{fig:fig13}
    \vspace*{-1\baselineskip}
\end{figure}

From both groups of data, where the first set contains the results of all conversations, and the second set containing the results of conversations with at least 3 outputs from the LLMs, we observe that there exist little to no conversations where the generated code was used directly in the codebase. This suggests that in the vast majority of cases, there exist significant amounts of human intervention performed by the students, which may range from changing the names of variables, reordering the structure of a function, or rewriting the code entirely while using the LLM output as a guide. Taking n as the index of the output within a conversation with the LLM, apart from several individual outliers where the conversation extended beyond 10 outputs, the value of n that yielded the closest match between the generated code and the tagged code in the codebase was n=2 for chats with at least 1 output, and n=5 for chats with at least 3 outputs. These values indicate that in most conversations, the final version of the code snippet used was generated in the early stages, with further attempts to prompt the LLM into improving the output proving unsuccessful, as is also reflected by the decreasing trend in similarity scores as n increases.
\newline
\newline
These results suggest that while LLMs are capable of generating code, the process of actually integrating the outputs into an existing codebase remains a complex task and requires human intervention. In the majority of conversations, the similarity of the output actually decreased as the conversation went on, which means that the output generated is becoming less and less alike the final version implemented in the codebase. In most cases, the peak in similarity score at a smaller value of n suggests that an earlier version of the output was chosen by the students as the basis upon which further modifications were performed. Instead of replacing the programming work entirely, the LLM was used by students as a pair-programmer for the creation of a code snippet that fulfilled parts of the requirements laid out by the prompt, before students performed their modifications to get the output to work in the larger codebase, with a larger context than what the LLM was able to access.


\subsubsection{Code Complexity Throughout Conversations}

Building on the above iterative modification process, we finally explore how code complexity metrics evolve as students engage with GPT across multiple interactions. By tracking cyclomatic complexity, Halstead Effort, maximum CFG (Control Flow Graph) depth, and total lines of code over the first 20 conversations, we gain insights into how students refine their code and adapt GPT-generated suggestions.



\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/conv_var_metric.png}
    \caption{Code Complexity Throughout Conversation}
    \label{fig:fig14}
\end{figure}


Initially, we observe that cyclomatic complexity tends to be highest at the beginning of conversations, particularly in milestone MS1. This suggests that students are experimenting with intricate logic and control structures as they brainstorm potential solutions. However, as conversations progress, the complexity generally decreases, indicating a shift towards more streamlined and effective code. Similarly, Halstead Effort—a measure of the cognitive load required to understand the code—reveals a similar pattern. It peaks early on, particularly in MS1 and MS2, and then declines rapidly. This trend implies that students initially face greater challenges in understanding and manipulating the code but become more adept as they gain familiarity with the problem requirements and the capabilities of the AI.
\newline
\newline
The maximum CFG depth metric shows fluctuations throughout the conversations. In the early stages, particularly in MS1, we see higher peaks, indicating that students are exploring deeper logical flows. However, as discussions continue, there is a clear movement toward simpler and more efficient implementations, especially in milestones MS2 and MS3. Finally, the total lines of code metric consistently decreases with each conversation. This trend indicates that students typically begin with more verbose drafts, which they later refine into concise forms. This progression aligns with best coding practices, where initial iterations are often expanded upon before being optimized.
\newline
\newline
These trends suggest that students begin with a broader, more experimental approach, which evolves into more efficient and streamlined code as conversations and milestones advance. The initial complexity, followed by simplification, reflects both the learning curve and the refinement process of integrating AI-generated code effectively into a larger project.


\subsection{Prompt Analysis}

Finally, to gain insights into user interactions with LLMs across the three milestones, we conducted a sentiment analysis of user prompts utilizing the VADER (Valence Aware Dictionary and sentiment Reasoner) tool. VADER is particularly effective for analyzing the sentiment of short texts, such as prompts, as it captures the subtleties often present in user-generated content, which enables us to determine whether users generally felt positive, neutral, or frustrated during their interactions, providing a clearer picture of user engagement patterns and their evolving expectations and comfort levels with LLM assistance throughout the project timeline.

\subsubsection{Sentiment Analysis Results}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/prompts/vader_sc_distr.png}
    \caption{Distribution of Vader Scores Across Milestones}
    \label{fig:fig15}
\end{figure}

The sentiment distribution across milestones reveals a noteworthy progression, as illustrated in Figure \ref{fig:fig15}. The compound sentiment scores indicate a relatively balanced distribution centered around zero, with a slight inclination towards positive sentiment across all three milestones. This suggests that, while users may express a range of emotions, positive interactions tend to outnumber negative ones. In the negative sentiment distribution, most scores cluster close to zero, indicating that strong negative expressions are infrequent, with only occasional outliers showing higher negativity. In contrast, the neutral sentiment distribution peaks strongly near zero, suggesting a significant portion of prompts are neutral in tone. The positive sentiment distribution, however, displays moderate density at lower positive values, indicating that while positive sentiment is present, it typically lacks intensity. Overall, these distributions imply that user interactions are predominantly neutral or slightly positive, reflecting a cautious but generally favorable tone. To delve deeper into the sentiment landscape, we also analyze fluctuations in sentiment throughout individual conversations within each milestone.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/prompts/varder_var.png}
    \caption{Variation of Vader Scores Over Conversation: Boxplot}
    \label{fig:fig16}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{outlier_analysis_imgs/prompts/vader_sc_distr_line.png}
    \caption{Variation of Vader Scores Over Conversation: Smooth Plot (LOESS)}
    \label{fig:fig17}
\end{figure}

Figure \ref{fig:fig16} and Figure \ref{fig:fig17} further explore sentiment fluctuations throughout individual conversations within each milestone. The box plots illustrate the compound VADER scores for user messages, focusing on the first ten messages in each sequence. Initially, the first few messages generally exhibit higher positive sentiment scores, indicating that users often begin interactions with a constructive or hopeful outlook. However, as conversations progress, sentiment becomes more dynamic, with both positive and negative scores appearing throughout different messages. The middle segment of the conversations (around messages 4–7) displays the widest range of compound scores, reflecting a mix of highly positive and somewhat negative sentiments. This variability suggests that users may encounter challenges or express frustrations during interactions, highlighting moments of potential struggle as they clarify questions or seek further assistance. As the conversation nears its conclusion (messages 8–10), sentiment stabilizes with lower variability. This suggests a resolution or closure, as positive scores persist, indicating successful interactions, while negative scores diminish, pointing to fewer frustrations. The overall trajectory—starting positively, fluctuating in the middle, and stabilizing towards the end—illustrates a natural rhythm in conversations, where users begin optimistically, navigate challenges, and ultimately conclude with a sense of resolution.

\subsubsection{Analysis of Prompt Lengths}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{outlier_analysis_imgs/prompts/lines_distr.png}
    \caption{Distribution of number of Lines Across Milestones}
    \label{fig:fig18}
\end{figure}

In addition to sentiment, we examined the distribution of the number of lines in user prompts across milestones, as shown in Figure \ref{fig:fig18}. Each milestone demonstrates a prominent peak at lower line counts, suggesting that most prompts are relatively concise. The sharp rise at the beginning, centered around shorter prompts, indicates that users primarily engage with brief requests or instructions. The distribution gradually tails off, indicating that while longer prompts do occur, they are relatively rare.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{outlier_analysis_imgs/prompts/lines_var_boxplot.png}
    \caption{Variation of Number of Lines Over Conversation: Boxplot (without outliers)}
    \label{fig:fig20}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{outlier_analysis_imgs/prompts/lines_var_smooth.png}
    \caption{Variation of Number of Lines Over Conversation: Smooth Plot (LOESS)}
    \label{fig:fig19}
\end{figure}


Just as we did for sentiments, in Figure \ref{fig:fig19} and Figure \ref{fig:fig20}, we depict how the number of lines in prompts varies throughout the progression of a conversation within each milestone. At the start of conversations (Message 1), significantly higher line counts suggest that initial prompts are often more detailed, providing necessary context or elaborate instructions. As conversations advance, the number of lines typically decreases and stabilizes, reflecting a transition to shorter, more direct interactions. This pattern likely mirrors common dialogue structures, where users begin with comprehensive, information-rich prompts and subsequently rely on concise follow-up questions or clarifications. Notably, spikes in line counts occasionally arise mid-conversation (e.g., Message 8 in Milestone 1), similar to the results from sentiments, indicating moments when additional explanations or adjustments are needed. This trend highlights an efficient conversational dynamic, wherein initial setups are followed by streamlined and targeted interactions, allowing for effective communication between users and the LLM.












\end{document}
