% some summary of data
% e.g. total number of generated snippets (count), total number of chat IDs (count), histogram of ms 1-3 (distribution) showing how students have used llms across milestones   

% also number of students, teams, etc




\section{Results}\label{secres}

% At the start of each section, mention exactly each data used. 
% descriptive captions for all figures and tables

\subsection{Analysis of LLM Usage}
We first analyzed the code snippets generated using LLMs across each milestone for each team. Table~\ref{tab:complete_teams_model_usage} captures the cumulative model usage within each team. 5 teams did not use any LLMs for code generation tasks despite providing premium access for the project. 
Out of the remaining 16 teams, 12 used LLMs to generate a moderate number ($>$10) of code snippets. Among these, 6 primarily relied on Copilot, 5 heavily utilized ChatGPT, and 1 team used both tools equally. 

Analyzing across milestones significant decline can be observed in usage of both ChatGPT and Copilot by all teams. This suggests the student teams heavily on AI assistants to generate code earlier in the course but reduced in the latter stages of the course. For some of the teams, we observe a decline in the cumulative number of code snippets from the first to the last milestone. Notably, teams 5 and 8 generated fewer Copilot snippets in the third milestone compared to the second. A similar trend is evident for ChatGPT-generated code in teams 10, 13, and 17. This suggests that some AI-generated code from earlier milestones was either refactored or removed entirely by the end of the project.

% \input{tables/lines-per-snippet}



% From our collected data on snippets and lines of code for LLM-generated code across milestones, our findings align with those of \cite{Rasnayaka2024}. Specifically, later milestones show a smaller delta in the absolute number of snippets but demonstrate a higher number of lines per snippet.


\input{tables/llm-usage}





\subsection{Analysis of LLM Generated Code}

We analyzed 730 code snippets generated using ChatGPT and Copilot. Table~\ref{tab:code_test_summary} summarizes the types of code snippets produced by both tools, categorized into those for testing purposes and functionality implementation. The analysis shows that students primarily relied on AI assistants for functionality implementation, with moderate usage for generating test cases.

\input{tables/code-breakout}

We further analyzed the complexity of AI-generated code across the three project milestones (MS1, MS2, and MS3) using metrics such as lines of code and cyclomatic complexity. Analysis of AI-generated code revealed a trend towards higher complexity, particularly in code generated by Copilot, as shown by skewed density plots in Figure \ref{fig:fig1.1.1}. This suggests that AI assistance may lead to more complex solutions, although the majority of student-generated code remained moderately complex. Although the average complexity (cyclomatic complexity and total lines) of student-generated code remained moderate, the analysis revealed that AI assistance, particularly Copilot, occasionally produced highly complex solutions, sometimes exceeding the average values by 40 to 50 times. This suggests that AI-generated code, while often effective, has the potential to introduce unnecessary complexity if adopted without careful review and refinement.


\begin{figure}[b]
    \centering
    \includegraphics[width=0.9\linewidth]{final_imgs/1_1_1_0.png}
    \caption{Density Plot for measured key metrics}
    \label{fig:fig1.1.1}
\end{figure}

Copilot generated significantly more outliers than GPT across all complexity metrics, indicating a tendency toward producing more complex and verbose code. This difference likely stems from Copilot's auto-completion approach, which favors extensive code generation based on common patterns, potentially leading to inflated complexity compared to GPT's more concise and conversationally guided output.
%Specifically, we found out Copilot produced 265 outliers for Cyclomatic Complexity, 197 for Halstead Effort, 268 for Maximum CFG Depth, and 136 for Total Lines of Code. In comparison, GPT had considerably fewer outliers, with 61 for Cyclomatic Complexity, 66 for Halstead Effort, 50 for Maximum CFG Depth, and 23 for TotalLines, about half of as much as Copilot across all metrics. By focusing on these outliers, we gained insights into the differences in how each tool influences code generation. This trend suggests that Copilot’s code often exceeds standard levels of complexity and volume, potentially reflecting its generative approach. While GPT also produces intricate code, it results in fewer outliers than Copilot, generally aligning more closely with median complexity values. The greater complexity observed in Copilot’s outputs may stem from its tendency to generate extensive, sometimes over-detailed responses, which can inflate both the code volume and its logical structure. This aligns with Copilot’s completion-based mechanism, where it auto-generates code directly within the development environment based on typical programming patterns, without the nuanced adjustments that a conversational interaction might facilitate.

GPT's conversational interface allows for iterative refinement of code, enabling students to guide the model towards simpler and more maintainable solutions. Conversely, Copilot's auto-completion approach, while efficient, can lead to overly complex code due to the lack of nuanced interaction. Additionally, the study's analysis of GPT-generated code is more precise due to the ability to track exact model outputs, while Copilot's contributions are assessed through student modifications, highlighting a difference in how interactions with each tool are measured.
%In contrast, GPT’s conversational interface enables iterative refinement of code through prompt adjustments. Students can interact with GPT to gradually simplify or streamline the code, tailoring the output to emphasize clarity and maintainability—qualities that are particularly valued in this course’s objective of producing straightforward code solutions to complex tasks. This flexibility allows students to guide GPT’s responses towards specific needs, enabling adjustments in code length, complexity, and readability, which are not as readily achievable with Copilot’s auto-completion model. Copilot, while efficient in generating plausible code solutions quickly, may lack the nuanced guidance inherent in a conversational exchange, making its outputs prone to verbosity or over-engineering. Additionally, there is a significant difference in how student interactions with each tool are recorded and assessed. For GPT, exact code outputs generated by the model are tracked, allowing for precise analysis of its contributions. However, with Copilot, this granular tracking is not available, meaning we don’t capture the precise code Copilot produces. Instead, we rely on a student intervention measure, which reflects how much modification students make to Copilot-generated code.

We also analyzed students' efforts to integrate AI-generated code into the project based on reported manual intervention ratings. For Copilot-generated code, the majority (53.6\%) required minor intervention (level 1), while a significant portion (30.0\%) required moderate intervention (level 2), indicating a higher demand for user input to refine or simplify the code. Only 15.2\% of Copilot-generated code required no intervention. In contrast, ChatGPT-generated code more often aligned with user expectations, with 26 \% requiring no manual modification and 22.9\% needing moderate intervention, suggesting ChatGPT-generated code generally met project requirements with minimal refinement.


We further analyzed the code snippets to understand the difference in complexity between ChatGPT and Copilot-generated code. While GPT and Copilot can achieve similar levels of code complexity, GPT generally does so with less code and lower cognitive effort, as measured by Halstead effort (figure \ref{fig:fig1.1.2}). This suggests that while ChatGPT-generated code shares a similar level of complexity with Copilot's, it is often more concise and easier to understand. ChatGPT's conversational interface enables users to iteratively refine prompts, resulting in more efficient code generation. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{final_imgs/1_1_2.png}
    \caption{Comparison of ChatGPT and Copilot Complexity Across Various Complexity Measures}
    \label{fig:fig1.1.2}
\end{figure}

Analyzing interactions with ChatGPT highlights how this iterative process reduces code complexity. Figure \ref{fig:fig1.1.3} shows the analysis of average code complexity across each conversation. This reveals a consistent trend:  as students converse with ChatGPT, the average complexity of the generated code decreases, particularly in terms of cognitive effort as captured using Halstead Effort. This demonstrates that the interactive nature of ChatGPT allows for iterative refinement and simplification, ultimately supporting the development of more manageable and effective code solutions.  

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{final_imgs/1_1_3.png}
    \caption{Variation of ChatGPT generated code in a conversation}
    \label{fig:fig1.1.3}
\end{figure}



% \begin{longtable}{|c|p{0.35\textwidth}|}
% \hline
% \textbf{Message No.} & \textbf{Message Content} \\
% \hline
% 1 & Given a string of an expression following these grammar rules: ... Give a function in C++ to convert the string which may not have all of these tokens separated by a whitespace, into a string where all these tokens are separated by a single whitespace. \\
% \hline
% 2 & Variable names or constant values can be multi-char. \\
% \hline
% 3 & Shorten the code, abstracting into functions if needed. \\
% \hline
% 4 & Shorten the code. \\
% \hline
% 5 & Simplify the code for addWhitespace. \\
% \hline
% \end{longtable}
% make it a figure

For example, consider the conversation shown in Chat Listing 1. The student begins by asking ChatGPT to create a C++ function to format an expression string according to specific grammar rules. Over a series of messages, the student requests to simplify the code, asking GPT to “shorten the code” and then to further “abstract into functions if needed.” Each subsequent request leads to a more streamlined and modular version of the code, showing how GPT’s responses become progressively aligned with the student’s preference for conciseness. 

\begin{chatgroup}{A sequence of prompts used by a student to instruct ChatGPT to generate and iteratively improve the generated code.}
    \begin{chatbubble}
     \scriptsize
    Given a string of an expression following these grammar rules: ... Give a function in C++ to convert the string which may not have all of these tokens separated by a whitespace, into a string where all these tokens are separated by a single whitespace.
    \end{chatbubble}
    \begin{chatbubble}
     \scriptsize
    Variable names or constant values can be multi-char.
    \end{chatbubble}
    \begin{chatbubble}
     \scriptsize
    Shorten the code, abstracting it into functions if needed.
    \end{chatbubble}
    \begin{chatbubble}
     \scriptsize
    Shorten the code.
    \end{chatbubble}
    \begin{chatbubble}
     \scriptsize
    Simplify the code for addWhitespace.
    \end{chatbubble}
\end{chatgroup}

This example illustrates how GPT's conversational interface empowers students to iteratively refine code, achieving both reduced complexity and improved maintainability. This adaptability makes GPT well-suited for educational contexts that prioritize clear and concise coding practices. While GitHub Copilot also includes a chat feature, it was not extensively used during the project timeline, as students primarily utilized Copilot for code completion and debugging rather than conversational interactions. Therefore, the study focuses solely on GPT-generated code to further investigate how conversational interactions can enhance code simplicity and efficiency, aligning with the course's objectives.
%This example illustrate how GPT’s conversational setup allows students to interactively guide the AI toward producing code that meets both complexity and maintainability needs. This adaptive process showcases GPT’s ability to respond to iterative feedback, leading to more efficient and understandable code – an aspect that aligns well with educational goals that emphasize clarity and simplicity in coding practices. Together, these findings form the basis for our decision to concentrate solely on GPT-generated code in the subsequent analyses. By focusing on GPT, we can delve into the effects of prompt refinement and conversational context on code complexity, gaining insights into how students might leverage AI’s guidance to create effective solutions. This approach aligns closely with the course’s objectives, offering a pathway to explore how conversational interactions can foster code simplicity and clarity, ultimately enhancing coding efficiency.

\subsection{Generated Code vs Integrated Code}
Our next analysis focuses on how students modify and integrate ChatGPT-generated code into their project repositories. Copilot-generated code is excluded, as it lacks the conversational context that evolves the code. This analysis aims to uncover patterns in student adaptations, examining whether they enhance, simplify, or otherwise alter the initial code provided by ChatGPT. We compared each team's repository code with the corresponding ChatGPT-generated code for each conversation. We identified the segment of ChatGPT code with the highest average similarity to the repository and used it as a reference. Our analysis revealed multiple instances of reuse of ChatGPT-generated code snippets in various parts of the team repository. While most students used a generated code once, some used it multiple times, demonstrating its adaptability.


In 95 instances the generated code was used only once, indicating that the majority of students found LLMs useful for generating task-specific code. In 23 instances students reused the generated code twice in the repository and in 5 cases, three times. Additionally, in 3 cases the generated code was reused four times, indicating that the code became a key tool for certain tasks. One notable example of code reuse involved a team who reused a GPT-generated code segment 13 times across different predicates, demonstrating its modularity. The code was adapted for various predicates like UsesPredicate and ParentPredicate, with minor adjustments for logic and parameter types. This highlights the flexibility of the generated code and its strategic use across different parts of the repository.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{final_imgs/2_1_1.png}
    \caption{Distribution of Difference Complexity Measures between Repo and GPT Code with Log Transformed x-axis}
    \label{fig:fig7}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{final_imgs/2_2_1.png}
    \caption{Similarity of generated and integrated code}
    \label{fig:fig9}
\end{figure}
To further assess modification, we compared each instance of repository code with the original ChatGPT code. We calculated the average differences across four metrics: Total Lines of Code, Cyclomatic Complexity, Maximum Control Flow Graph (CFG) Depth, and Halstead Effort. The density plot (Figure \ref{fig:fig7}) shows the distribution of deviations, with a vertical black line marking the point where the difference is zero. The x-axis of the plot was log-transformed for better visualization. The shifted zero point is indicated by the black line, highlighting instances where no change was made to the GPT-generated code. The plot reveals that a significant proportion of deviations are positive, indicating that repository code is frequently more complex than the original GPT version. This trend is consistent across all metrics and milestones, suggesting that students often modify GPT-generated code by increasing its complexity—whether by adding more lines, enhancing logical structures, or deepening control flows. 
%Such modifications go beyond minor tweaks, as evidenced by higher values across the two metrics shown above in the repository versions, indicating substantial enhancement or customization of the initial AI output. 
%This tendency could reflect intentional customization for project needs.%, but it may also suggest occasional over-engineering, where students inadvertently introduce unnecessary complexity in the modification process.

% \subsubsection{Code Similarity Over Milestones: Tree-Sitter Analysis}



%To gain deeper insight into modification patterns, we analyzed the logical and structural similarity using Tree-Sitter, focusing on comparing logic and function definitions between the repository and GPT-generated code across project milestones. The similarity scores over time reveal students’ evolving engagement with AI-generated code, reflecting shifts in adaptation patterns as they progressed through the project. As shown in Figure \ref{fig:fig9}, the similarity between the repository and GPT code generally increased across milestones. In Milestone 1 (MS1), similarity levels were relatively low, with wide variability and several outliers. This suggests that students were highly experimental in MS1, heavily modifying AI-generated code or opting for custom implementations. However, by Milestone 2 (MS2), median similarity had increased, indicating a shift towards greater reliance on GPT-generated outputs. Some students continued to customize AI code significantly, while others followed it more closely. In Milestone 3 (MS3), the trend peaked, with similarity scores reaching higher median values and fewer outliers. This pattern suggests that students relied more directly on GPT-generated code in later stages, possibly reflecting increased familiarity and comfort with the AI tool.

% To better understand modification patterns, we analyzed the logical and structural similarity between the repository and ChatGPT-generated code. To compute the similarity between the repository and ChatGPT-generated code, we used Jaccard similarity to measure the overlap between the sets of relevant information extracted using Tree-sitter Each string pair was compared using the Longest Common Subsequence (LCS) method to determine their similarity, allowing strings with more than 90\% similarity to be considered equivalent. The Jaccard similarity was then calculated as the ratio of the intersection to the union of these Tree-sitter-extracted sets. The similarity scores over time highlight students' evolving engagement with AI-generated code across project milestones. As shown in Figure \ref{fig:fig9}, similarity generally increased as students progressed. In Milestone 1 (MS1), similarity was low and highly variable, indicating that students were experimenting and modifying AI-generated code before using it. By Milestone 2 (MS2), median similarity increased, suggesting a shift toward greater reliance on ChatGPT outputs, with few students making significant modifications. By Milestone 3 (MS3), similarity peaked with fewer outliers, reflecting increased reliance on generated code.

To understand modification patterns, we analyzed the logical and structural similarity between repository code and ChatGPT-generated code. We used Jaccard similarity to measure overlap in relevant information extracted via Tree-sitter. Each string pair was assessed with the Longest Common Subsequence (LCS) method, considering pairs over 90\% similar as equivalent. The Jaccard similarity was the ratio of intersection to union of Tree-sitter-extracted sets. Over time, similarity scores highlighted students' evolving use of AI-generated code. As shown in Figure \ref{fig:fig9}, similarity increased across project milestones. In Milestone 1 (MS1), similarity was low and variable, indicating experimentation with AI code. By Milestone 2 (MS2), median similarity rose, suggesting increased reliance on ChatGPT outputs with fewer modifications. At Milestone 3 (MS3), similarity peaked with fewer outliers, reflecting a stronger dependency on generated code.

\begin{chatgroup}{Prompts by teams 5 and 13, during MS1}
    \begin{chatbubble}
    \scriptsize
    After extracting source SIMPLE program into tokens, how do I validate that a cond\_expr is syntactically valid according to the grammar rules?
    \end{chatbubble}
    \begin{chatbubble}
     \scriptsize
    Generate a PKB stub class that I can assign the return value through the constructor. You can work with this:…
    \end{chatbubble}
\end{chatgroup}

An important factor in the increasing similarity scores is the improvement in students' prompting techniques. As they gained experience with ChatGPT, their prompts likely became more refined, leading to more accurate and task-specific outputs. These enhanced outputs could have made the AI-generated code easier to incorporate with minimal changes, contributing to the rising similarity scores. This trend, combined with qualitative observations, has key pedagogical implications. The exploratory behavior in MS1 suggests an active learning phase where students experiment and modify AI-generated solutions, deepening their understanding. As students gained experience, they began using ChatGPT more efficiently, refining prompts to produce high-quality code. By MS3, the workflow stabilized, with consistent similarity scores reflecting a seamless integration of AI into their process.


Chat Listing 2 captures prompts used in MS1, which tend to be straightforward and limited in scope, often yielding basic outputs that require further customization to meet students’ needs. 


\begin{chatgroup}{Improved prompting by teams 5 and 13, at the end of the semester in MS2 and MS3}
    \begin{chatbubble}
     \scriptsize
    I need to implement a semantic checker to check that there are no cyclic procedure calls before I build the AST. How should I add on to that for my SemanticValidator class? This is how my SemanticValidator class looks like for now. It receives lines of tokenized code from Tokenizer ..
    \end{chatbubble}
    
    \begin{chatbubble}
     \scriptsize
    Swap the inner sections with the outer ones. E.g., in the section for ‘Contains pair when table is empty,’ have 4 subsections for each combination of $<$int, int$>$, $<$int, string$>$, and so on.
    \end{chatbubble}
\end{chatgroup}

In MS3, students’ prompts become more advanced, specifying examples, constraints, and project-specific contexts. This more strategic prompting enables GPT to produce outputs closely aligned with project requirements, reducing the need for extensive modifications. Thus, the increase in similarity scores over milestones reflects not only students’ growing reliance on GPT but also their refinement in AI interaction, signaling a maturity in prompt engineering that enhances productivity and code quality. 
For educators, this implies the importance of teaching effective prompting techniques and encouraging initial experimentation to ensure that students can critically assess and adapt AI-generated code.

\subsection{In-depth Analysis of Conversations}
% In the previous section, we examined trends in complexity and similarity at a high level across milestones, highlighting variations over time. This section focus on a prompt-level analysis within individual conversations. By tracking changes in code similarity during conversations, we can understand how students iteratively refine GPT-generated code to fit their projects. 

% \subsubsection{Similarity of GPT-Generated Code Within Conversations}
Similarity measurements on the ChatGPT conversations were used to determine how the generated code evolved during a conversation and ultimately integrated. The histogram in Figure \ref{fig:fig12} reveals that most conversations typically consist of just one or two messages, with a smaller number extending beyond 15 messages. The longest conversation was 50 messages. This distribution shows an overall downward trend, indicating that longer conversations are less frequent. 
%It’s important to note that we did not solely analyze the histogram of indices of maximum similarity, as this would be heavily influenced by the many short conversations, which tend to dominate the results.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{final_imgs/3_1_1.png}
    \caption{Histogram of Conversation Lengths (filtered for conversations with less than 20 messages)}
    \label{fig:fig12}
\end{figure}


\begin{figure}[b]
    \centering
    \includegraphics[width=0.7\linewidth]{final_imgs/3_1_2.png}
    \caption{ 
    Mean Index of the Generated Code Most Similar to the Repository Code for Conversations of Different Lengths}
    \label{fig:fig13}
\end{figure}

% For each code referenced in the repository, we identified the index of the ChatGPT conversation that led to that code. This was done by assessing code similarity between GPT-generated code snippets within each conversation and the tagged code snippet in the final repository. To minimize the impact of short conversations, we analyzed conversations of varying lengths separately. This approach was necessary because conversations with shorter lengths naturally tended to have smaller indices where maximum similarity was achieved. As a result, visualizing the maximum similarity index separately for conversations of different lengths helped avoid a peak near smaller indices, which were mainly attributed to the larger number of shorter conversations. For each conversation of length m  (where $m < 20$), we calculated the mean index of the code with the highest similarity to the tagged code in the repository. We excluded cases where the same code was reused multiple times across conversations, as this would artificially inflate the values. Additionally, we omitted conversations with an average similarity of zero, as they likely involved substantial code modifications or less relevant outputs from the LLM. In cases where multiple stages of conversation yielded the same maximum similarity, we selected the first occurrence to resolve ties, focusing on the initial prompt that produced the highest similarity. 

For each code snippet in the repository, we identified the ChatGPT conversation that generated it by comparing the similarity of GPT-produced code snippets within conversations to the tagged repository code. Conversations were analyzed separately based on varying lengths to account for the tendency of shorter conversations to show high similarity at smaller indices. This separate analysis helped prevent a skew towards smaller indices. For conversations shorter than 20 interactions, we calculated the average index of the code with the highest similarity to the repository code, excluding reused code to avoid skewed values. Conversations averaging zero similarity, suggesting significant modifications or irrelevant outputs, were omitted. In cases of ties in maximum similarity across conversation stages, we prioritized the first occurrence to highlight the initial prompt responsible for the highest similarity.


The results in Figure \ref{fig:fig13} show a general upward trend in the index of the generated code used in the repository as conversations continue. This indicates that as conversations progress, the code ultimately included in the final repository is often generated during the later stages of the dialogue. This trend suggests that students leverage iterative back-and-forth interactions with the LLM to refine and improve the code. However, the mean position of the final code within the conversation is consistently lower than the total conversation length. This implies that the final version of the code does not always originate from the last prompt. Instead, students may opt for earlier outputs that better suit their needs or seek clarification on specific portions of the generated code to enhance their understanding. 

This shows that while LLM-generated code provides valuable starting points, students often interact with the model over several iterations, modifying and adapting the code before integrating it into the final codebase. The increasing index of similarity as conversations progress suggests that students can effectively prompt to make nuanced modifications and refinements to the generated code as required by their use case.


\subsection{Prompt Analysis}
We conducted sentiment analysis for student prompts utilizing the VADER (Valence Aware Dictionary and Sentiment Reasoner) tool \cite{Hutto_Gilbert_2014}. VADER is effective for analyzing the sentiment of short texts, such as prompts, which enables us to determine whether users generally felt positive, neutral, or frustrated during their interactions with the LLM.


% \begin{figure}[b]
%     \centering
%     \includegraphics[width=\linewidth]{final_imgs/4_1_1.png}
%     \caption{Distribution of Sentiment Scores (Compound  Vader Scores) Across Milestones}
%     \label{fig:fig15}
% \end{figure}


% Figure \ref{fig:fig15} illustrates the distribution of sentiment scores across milestones. The compound sentiment scores center around zero, indicating a mix of emotions, with positive sentiments slightly outweighing negative ones. Most negative scores are close to zero, suggesting infrequent strong negativity, while neutral sentiments dominate, as seen by the peak around zero. Positive sentiment is present but tends to be mild. Further analysis will explore sentiment fluctuations within individual conversations.


%The sentiment distribution across milestones reveals a noteworthy progression, as illustrated in Figure \ref{fig:fig15}. The compound sentiment scores indicate a relatively balanced distribution centered around zero, with a slight inclination towards positive sentiment across all three milestones. This suggests that, while users may express a range of emotions, positive interactions tend to outnumber negative ones. In the negative sentiment distribution, most scores cluster close to zero, indicating that strong negative expressions are infrequent, with only occasional outliers showing higher negativity. In contrast, the neutral sentiment distribution peaks strongly near zero, suggesting a significant portion of prompts are neutral in tone. The positive sentiment distribution, however, displays moderate density at lower positive values, indicating that while positive sentiment is present, it typically lacks intensity. Overall, these distributions imply that user interactions are predominantly neutral or slightly positive, reflecting a cautious but generally favorable tone. To delve deeper into the sentiment landscape, we also analyze fluctuations in sentiment throughout individual conversations within each milestone.  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{final_imgs/4_1_2.png}
    \caption{Variation of Compound Vader Scores Over a Conversation: Estimated using LOESS (locally estimated scatterplot smoothing)}
    \label{fig:fig17}
\end{figure}% describe the convos, show some examples

Figure \ref{fig:fig17} explores sentiment fluctuations throughout individual conversations within each milestone. Initially, the first few messages generally exhibit higher positive sentiment scores, indicating that users often begin interactions with a constructive or hopeful outlook. However, as conversations progress, sentiment tends to show a steady downward trend suggesting that users may encounter challenges or express frustrations as interactions progress, highlighting moments of potential struggle as they clarify questions or seek further assistance. As the conversation nears its conclusion, sentiment stabilizes and ends with a slight uptick of sentiment. This suggests a conclusion to conversations with a sense of resolution.

For example, in one conversation involving unit testing for the AssignParsingStrategy::parse function, the initial message begins optimistically, with the user providing detailed code for context and a clear prompt for assistance. As the conversation progresses, subsequent messages reflect increasing frustration as the user struggles to refine test cases and address specific errors (e.g., “it gives an error saying ‘SyntaxError’ does not refer to a value”). The sentiment recovers slightly toward the end as the issue is resolved, illustrating the characteristic fluctuations in sentiment we observed in many conversations

%Figure \ref{fig:fig17} further explores sentiment fluctuations throughout individual conversations within each milestone. Initially, the first few messages generally exhibit higher positive sentiment scores, indicating that users often begin interactions with a constructive or hopeful outlook. However, as conversations progress, sentiment becomes more dynamic, with both positive and negative scores appearing throughout different messages. The middle segment of the conversations displays the widest range of compound scores, reflecting a mix of highly positive and somewhat negative sentiments. This variability suggests that users may encounter challenges or express frustrations during interactions, highlighting moments of potential struggle as they clarify questions or seek further assistance. As the conversation nears its conclusion, sentiment stabilizes with lower variability. This suggests a resolution or closure, as positive scores persist, indicating successful interactions, while negative scores diminish, pointing to fewer frustrations. The overall trajectory—starting positively, fluctuating in the middle, and stabilizing towards the end—illustrates a natural rhythm in conversations, where users begin optimistically, navigate challenges, and ultimately conclude with a sense of resolution.






