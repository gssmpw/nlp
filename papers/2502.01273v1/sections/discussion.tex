
\section{Threats to Validity}
% The analysis of LLM usage may not accurately capture the entire usage due to students not correctly tagging AI-generated code snippets. 

% A single conversation could have multiple tasks, so the evolution analysis and sentiment analysis can be noisy

Our analysis is based on voluntarily collected student self-reports, which may include underreporting or selective disclosure, introducing potential bias. Although GitHub Copilot offers a chat feature, it was not widely used; it primarily served for code completion and debugging. The chat functionality was not significant. Moreover, the VADER tool for sentiment analysis often misclassifies technical terms as neutral, resulting in many prompts receiving scores near zero due to frequent technical language. Despite these limitations, the analysis offers valuable insights into sentiment trends and the emotional tone of user interactions.


% removed for double blind review - to return to this discussion for camera ready paper
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\linewidth]{images/tree_sitter_gpt_copilot_average_complexity-sem1.png}
%     \caption{Metrics of LLM generated code in Semester 1 (Previous study)}
%     \label{fig:sem1_metrics}
%     \vspace*{-1\baselineskip}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\linewidth]{images/tree_sitter_gpt_copilot_average_complexity-sem2.png}
%     \caption{Metrics of LLM generated code in Semester 2 (This study)}
%     \label{fig:sem2_metrics}
%     \vspace*{-1\baselineskip}
% \end{figure}
% % can combine

% Comparing the data from Semester 1 (our previous study) and Semester 2 (this study), we can see a similar increasing trend in the complexity of the LLM-generated code, corresponding to the growth in project codebase complexity as students develop their SPA programs from scratch. As complexity grew, so did the effort needed to comprehend and maintain the LLM-generated code, which is indicated by the Halstead Metric Effort's evident growth across the milestones for both semesters.

% One noticeable difference between the two semesters is the significantly lower relative MaxCfgDepth in SEM1MS1 - being the lowest in that semester, when compared to the relative MaxCfgDepth in SEM2MS1 - being the highest in that semester. As the student cohort attending the course was different in the semesters, Semester 2's students started the project amidst an academic climate which saw wider incorporation of LLMs into school work and projects, especially for programming-related courses. 

% Apart from the trends over milestones, the absolute values in average total lines of generated code snippets were also higher in semester 2, which can be attributed to the more advanced LLM models used in Copilot and ChatGPT 4, in addition to the semester 2 cohort's familiarity and smoother learning curve in utilizing LLMs. A similar conclusion could be arrived at when evaluating the Halstead Metric Effort, where the value for SEM2MS1 is nearly twice as high as that for SEM1MS3, indicating that the LLM-generated code at the start of the project in Semester 2 is already likely much more complicated and require more effort to maintain than at the end of the project in Semester 1, despite the requirements of the overall project for the SPA remaining largely unchanged.