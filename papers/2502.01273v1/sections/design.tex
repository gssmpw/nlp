\section{Methodology}\label{sec-case}



% - 21 teams of 6 students each, total of 126 students in the module
% - 3x model dist graph - dist ms1-ms3
%  - observations:
%  - 3 (out of 21) teams favored copilot across all 3 MS.
%  - other teams favored gpt (some of these teams did not use copilot at all)
% - raw numbers of usage:
%  - ms1: 535 code snippets (335 copilot, 200 gpt)
%  - ms2: 699 code snippets (488 copilot, 211 gpt)
%  - ms3: 730 code snippets (507 copilot, 223 gpt)

% Data on General number of full conversations in chatGPT (on both the chat.openai and platform.openai sites):

% MS1: 36 chat, 46 platform, 82 total
% MS2: 37 chat, 55 platform, 92 total
% MS3: 41 chat, 59 platform, 100 total

% can bring cite prev paper in line w that observation - MS1 initially more, 2 and 3 fewer in delta


% %% data for the 100 conversations and 223 snippets

% 223 snippets 
% = 68 snippets with no tag (blank)
% + 3 snippets with invalid tag ('dash', 'i can't get the link')
% + 152 snippets with correct tag



% for the 100 conversations:

% 100 conversations
% = 70 conversations with 1 snippet each
% + 17 conversations with 2 snippets each
% + 4 conversations with 3 snippets each
% + 3 conversations with 4 snippets each
% + 1 convo w 5 snippets
% + 1 convo w 6 snippets
% + 1 convo w 13 snippets
% + 3 remaining convo not tagged to snippet (suspect these map to the invalid tagging snippets)


% also, the 152 correctly tagged snippets:

% 152 = 70*1 + 17*2 + 4*3 + 3*4 + 5 + 6 + 13


% So the numbers do tally





% ToDo: Joseph to rewrite - reduce this section, cite first work and mention we followed similar course and setup
% \cite{Rasnayaka2024} when referring to prev work
% 	1. Summarize course description, can use same terms like 'SPA', 'SP'
% 	2. Quote first paper and say we follow same course structure, similar to that paper's tagging and extraction of LLM generated code
% 	3. Mention paid accounts were provided, chatGPT through school funded access, copilot through students' GitHub school accounts
% 	4. Anonymize


\textbf{Project Description:}
In compliance with institutional guidelines, approval for our research was obtained from the Departmental Ethics Review Committee (DERC) before conducting the study. The undergraduate level software engineering course within which this study is conducted involves a 13-week long, robust software development project, where 126 students are tasked with building a Static Program Analyzer (SPA), with three distinct milestones (MS1, MS2, MS3) where the delivery of a functional SPA is expected. The SPA is capable of performing analysis on a course specific custom programming language. The structure of the project is similar to the project used in \cite{Rasnayaka2024}, where the SPA is further subdivided into:
\begin{itemize}
    \item A Source Parser (SP) which analyzes the custom language to extract abstractions such as design entities.
    \item A Program Knowledge Base (PKB), responsible for storing the extracted information.
    \item A Query Processing Subsystem (QPS), which is able to handle queries written in an SQL-like language for querying the PKB, and provide responses to the user.
\end{itemize}

Throughout the development phase, students were granted organizational access to the paid versions of ChatGPT via both the ``Chat'' and ``Playground'' interfaces, enabling close monitoring of their usage. Additionally, students were also able to access  GitHub Copilot features through their institutional GitHub Pro accounts. Access to both of these LLM code generators is funded by the university. Students are also actively encouraged to utilize LLMs and integrate them into their development cycle, and the usage of their organizational access was also reserved strictly for the purposes of this project. Through this setup, we are able to obtain data regarding students' interactions with LLMs, and also the conversational history and information about prompts that were used on ChatGPT.

\textbf{Code extraction and ChatGPT conversations:} Following our initial work~\cite{Rasnayaka2024} we extracted LLM-generated code snippets used by the students at each milestone, which was achieved by requiring students to tag the LLM-generated code utilized in their project with the following information:

\begin{itemize}
    \item Generator used to obtain the output code.
    \item Level of human intervention required to modify the code.
    \item Link to the conversation (only for ChatGPT)
\end{itemize}

The tagging and collection of student data, as well as the definitions of human intervention levels (0, 1, and 2), follow our previous work in \cite{Rasnayaka2024}: level 0 (no changes), level 1 (10\% or fewer lines changed), and level 2 (more than 10\% of the lines changed). This paper introduces a new aspect by including links to student-LLM conversations.

% The collected and extracted data at each milestone are cumulative, as students build upon previous iterations of their SPA as they work through the semester. At each milestone, we also gathered data on students' usage of ChatGPT platforms through their organizational access, obtaining both the prompts used and the generated code, which enables the analysis of the effect of conversational-based interactions on the quality and usability of LLM-generated code.

The collected data at each milestone is cumulative, reflecting students' iterative development of their SPA over the semester. We also gathered data on students' use of ChatGPT, including the prompts and generated code, to analyze how conversational interactions affect the quality and usability of LLM-generated code.

\textbf{Overview on Analysis:}
This study explores how students in a software engineering course interact with LLMs like ChatGPT and GitHub Copilot, to use the LLM generator tools to help them in their development process, particularly in the context of how approaches to code generation and the generated outputs evolve. By examining how students interact with these LLMs, adapt generated code, and refine their prompting strategies, we aim to reveal the dynamics of human-AI collaboration in SE education. Here we unpack a multi-layered analysis that spans the quality and complexity of LLM-generated code, the processes of integrating LLM-generated code within student repositories, and the evolving conversational interactions between students and LLMs across milestones.

To comprehensively analyze the quality of LLM-generated code, we used a set of four distinct metrics: total lines of code (LOC), cyclomatic complexity, maximum control flow graph (CFG) depth, and the Halstead effort metric. These metrics provide insights into the sophistication and structural intricacies of the code produced by LLMs. 

\begin{itemize} 
\item \textbf{Total Lines of Code (LOC)}: Serves as a basic indicator of code verbosity and has been used to estimate the programming productivity of a developer.

\item \textbf{Cyclomatic Complexity}: Measures the number of linearly independent paths within the code, and evaluates its logical complexity. Higher cyclomatic complexity can be indicative of maintainability challenges.

\item \textbf{Maximum Control Flow Graph (CFG) Depth}: Measures the depth of nested structures within the code. Increased CFG depth can reflect the presence of deeply nested loops or conditional statements, which may complicate code comprehension and maintenance.

\item \textbf{Halstead Effort}: Estimates the mental effort required to understand and modify the generated code. Higher values suggest that the code may be more challenging to understand and maintain.
\end{itemize}

This work extends our previous research \cite{Rasnayaka2024} by adding a new dimension of sentiment analysis enabled by the collected prompts, providing insights into student - AI interactions. We also introduced new metrics, offering deeper analysis of how code quality and usability vary across different generation approaches.
% To explore the relation between LLM-generated code and the code found in each student's repository, we performed a comparative analysis to understand how students modified the AI-generated snippets to meet project requirements. Using the same complexity metrics, we observed whether the original structure was simplified, expanded, or restructured during integration: ranging from direct adoption of LLM outputs to significant manual refinements, with some students reconfiguring the AI-generated code to enhance functionality or maintainability. The statistical analysis of these differences provided deeper insights into how students’ technical goals aligned—or diverged—from the raw LLM suggestions. To further dissect the modifications made, we applied Tree-Sitter, a syntax parsing tool, to calculate syntactic similarity between LLM-generated code snippets and final repository code. This process allowed us to evaluate how directly students used the AI outputs versus how much they altered them to suit specific project requirements. High similarity scores suggested minimal modification, indicative of instances where the AI-generated solution met the students’ needs directly. In contrast, low similarity scores hinted at extensive customization, often aligning with cases where students used the initial LLM output as a starting framework but transformed it substantially through iterative refinement. These patterns provide a window into the decision-making processes underlying student-LMM interaction across different milestones of the project.

% Analyzing conversation history added a qualitative layer to our understanding by focusing on the interactions students had with the LLMs across milestones. By examining how students crafted prompts, responded to LLM-generated suggestions, and evolved their conversational strategies, we captured a dynamic view of the human-AI exchange. To understand how complexity unfolded over the course of a conversation, we tracked complexity metrics at different stages within each dialogue. For example, we divided conversations based on whether students engaged with a single LLM output or iterated through three or more outputs to see how code complexity fluctuated with increased iteration. In addition to complexity progression, we examined sequential similarity between LLM outputs within a single conversation. This similarity analysis helped clarify whether students iteratively refined the LLM-generated code, maintaining a base framework but adapting it through successive prompts. 

% Finally, to further understand students' prompting behavior, we also analyzed the characteristics of their prompts over time, including sentiment and structure. Sentiment analysis using the VADER tool allowed us to capture shifts in tone as students became more comfortable with the LLMs, reflecting a gradual shift in how they perceived the LLM’s role in assisting their projects. Prompt length and structure also provided insights into students’ evolving strategies for guiding the LLMs. Prompt length, in particular, served as a marker for specificity, with longer prompts often correlating with more detailed requests as students learned to fine-tune their instructions for the AI. This adaptation in prompt construction suggests a developing sophistication in how students approached AI-driven assistance, seeking either more concise outputs for simpler tasks or detailed, layered responses for complex queries. 
