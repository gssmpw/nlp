\section{Introduction}
% Generative large language models (LLMs) have become pivotal in educational settings, demonstrating promising results in tasks as diverse as solving mathematical problems~\cite{Enkelejda2023}, dialog-based tutoring~\cite{Park2024}, and assisting software engineering student projects~\cite{Rasnayaka2024}. LLM-driven assistance for educational purposes has garnered significant attention across diverse settings due to its exceptional versatility in performing various tasks to assist students in the learning process. Particularly in the software engineering context, LLMs have shown greater promise in various tasks such as code summarization~\cite{Ahmed2023}, test generation~\cite{Chen2024}, program analysis~\cite{Zhang2024}, code review~\cite{Lu2023}, bug fixing~\cite{Jin2023}, and code generation~\cite{Bairi2024}. While there is a growing interest in exploring the use of AI for education, research on the actual usage of AI in student learning environments for software engineering remains limited, particularly in understanding how students utilize LLMs for open-ended tasks in a software engineering project. 

Generative large language models (LLMs) have become crucial in education, excelling in tasks from math problem-solving~\cite{Enkelejda2023} to dialog-based tutoring~\cite{Park2024} and aiding software engineering projects~\cite{Rasnayaka2024}. Their versatility has made them highly sought after in educational settings. In software engineering, LLMs particularly excel in tasks like code summarization~\cite{Ahmed2023}, test generation~\cite{Chen2024}, program analysis~\cite{Zhang2024}, code review~\cite{Lu2023}, bug fixing~\cite{Jin2023}, and code generation~\cite{Bairi2024}. Despite growing interest in AI for education, research remains limited on how students use LLMs for open-ended tasks in software engineering projects.

% In this study, we analyze the interaction between undergraduate students and AI assistants in a software engineering course. The students are encouraged to utilize an AI assistant for a software engineering project to develop a Static Program Analyzer (SPA) for programs written in a custom programming language. Multiple teams of 6 students engage in various tasks to build a program analyzer, from requirement engineering to user acceptance testing, spanning a 13-week semester. Students were given premium access to Microsoft CoPilot and OpenAI ChatGPT with unlimited access, for the duration of the course. At the end of the semester, we collected all conversations, code, and artifacts generated using AI, and student-annotated code which captures meta-data for our analysis. We examine the collected data to answer the following research questions:

In this work we examine the interaction between undergraduate students and AI assistants in a software engineering course. Students were tasked with using AI to develop a Static Program Analyzer (SPA) for a custom programming language. Over a 13-week semester, teams of six students undertook various tasks, from requirement engineering to user acceptance testing. They received unlimited premium access to Microsoft CoPilot and OpenAI ChatGPT. At semester's end, we collected all AI-driven conversations, code, and artifacts, along with student-annotated code metadata, for analysis. We examine the collected data to answer the following research questions:

\begin{itemize}
    % \item Is there a significant difference between code generated using ChatGPT and CoPilot? $\rightarrow$ We analyze code generated from each AI assistant and compare the complexities using various metrics. 
     \item Is there a significant difference between code generated by ChatGPT and CoPilot? $\rightarrow$ We compare code complexities using various metrics.
    \item How does the code evolve during a conversation between a student and AI? $\rightarrow$ We analyze conversation logs and extract code for each conversation. 
    \item What is the impact of using AI assistant on their learning outcomes? $\rightarrow$ We analyze the conversation volume, final code output, and evolution of the prompting techniques. 
    \item Does the interaction between the student and AI result in a positive engagement? $\rightarrow$ We perform sentimental analysis across each conversation. 
\end{itemize}

A total of 126 undergraduate students in 21 groups, generated 730 code snippets (172 tests and 558 functionality implementations) using CoPilot and ChatGPT. We also collected 62 ChatGPT conversations that generated code, amounting to 318 messages between students and ChatGPT. Of the total 582,117 lines of code across all teams, 40,482 lines of code (6.95\%) were produced with an LLM's help. 

% Analyzing the code snippets, we identified that Copilot-generated code has more lines of code compared to ChatGPT-generated code, and Copilot-generated code has higher code complexity leading to difficulty in interpretation. We initially hypothesized that ChatGPT-generated code was easy to integrate into the project compared to Copilot. However, student feedback on manual intervention levels does not show a significant difference in the effort required to integrate Copilot-generated code compared to ChatGPT-generated code.

Upon analysis, Copilot-generated code is longer and more complex (i.e. higher Halstead Complexity) than ChatGPT's, making it harder to interpret. Despite initial assumptions, student feedback shows no significant difference in the integration effort required for both Copilot and ChatGPT-generated code. Furthre analyzing the conversation logs, we identified that through feedback ChatGPT generated code meets project needs with minimal refinement. Sentiment analysis of the conversation reveals on average the conversation ends on a positive note. Indicating conversational-based assistance generate code requiring minimal manual refinement. Over the semester, we also observed a noticeable improvement in the quality of the prompts generated by students, demonstrating their growing ability to craft more effective and precise prompts for better outcomes.

Based on the observations from our study, we discuss design considerations for a future educational course tailored to using AI assistants for software engineering. These considerations include promoting students to learn better prompting strategies and evolving the use of AI assistants beyond merely being a tool for code generation. Our contribution lies in providing an in-depth analysis of how students use ChatGPT in a project-based software engineering course.  