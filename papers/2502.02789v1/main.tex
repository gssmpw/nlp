%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{booktabs} % for professional tables
\usepackage{colortbl} % For coloring table cells
\usepackage[table, dvipsnames]{xcolor}
\usepackage{soul}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Speculative Prefill}

\begin{document}

\twocolumn[
\icmltitle{Speculative Prefill: Turbocharging TTFT with\\ Lightweight and Training-Free Token Importance Estimation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jingyu Liu}{uchi}
\icmlauthor{Beidi Chen}{cmu}
\icmlauthor{Ce Zhang}{uchi}
\end{icmlauthorlist}

\icmlaffiliation{uchi}{Department of Computer Science, The University of Chicago, Chicago, IL, USA}
\icmlaffiliation{cmu}{Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA}

\icmlcorrespondingauthor{Jingyu Liu}{jingyu6@uchicago.edu}
\icmlcorrespondingauthor{Ce Zhang}{cez@uchicago.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% define commands
\newcommand{\ours}{\textsc{SpecPrefill}}
\newcommand{\baseline}[1]{\texttt{Llama-#1-Inst}}

% \url{https://github.com/Jingyu6/speculative_prefill}

\begin{abstract}
% problem we want to solve
Improving time-to-first-token (TTFT) is an essentially important objective in modern large language model (LLM) inference engines. Because optimizing TTFT directly results in higher maximal QPS and meets the requirements of many critical applications. 
% why is it important and hard
However, boosting TTFT is notoriously challenging since it is purely compute-bounded and the performance bottleneck shifts from the self-attention to the MLP part. 
% what is our insight
We present \ours{}\footnote{The code with experiment reproduction is available at \textit{\url{https://github.com/Jingyu6/speculative_prefill}}. }, a training free framework that accelerates the inference TTFT for both long and medium context queries based on the following insight: LLMs are generalized enough to still preserve the quality given only a \textit{carefully chosen} subset of prompt tokens. 
% how did we do it?
At its core, \ours{} leverages a lightweight model to speculate locally important tokens based on the context. These tokens, along with the necessary positional information, are then sent to the main model for processing. 
% how did we evaluate it?
We evaluate \ours{} with a diverse set of tasks, followed by a comprehensive benchmarking of performance improvement both in a real end-to-end setting and ablation studies. 
% what is the result we get?
\ours{} manages to serve \texttt{Llama-3.1-405B-Instruct-FP8} with up to $7\times$ maximal end-to-end QPS on real downstream tasks and $7.66\times$ TTFT improvement during benchmarking.

\end{abstract}

\input{sections/intro}
\input{sections/background}
\input{sections/method}
\input{sections/experiment}
\input{sections/limitation}
\input{sections/impact_statement}
\input{sections/conclusion}

\newpage
\bibliography{main}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Standard Short Task Performance of \ours{}}
\label{app:standard}

In Figure~\ref{fig:standard}, we report \ours{} when applied to \texttt{Llama-3.1-70B-Instruct} on standard short tasks as discussed in Sec~\ref{sec:standard}. It is worth noting that for shorter tasks, the queries are more likely to become information dense, rendering \ours{} less effective especially for certain tasks. 

\section{Comparing \ours{} with RAG Based Systems}
\label{app:rag}

In this section, we first detail the algorithm behind two of our RAG baselines and present results comparing \ours{} against them. Both variants split the context of the prompt into sentences using \textit{nltk} library~\cite{loper2002nltknaturallanguagetoolkit}. After splitting the context, all sentences are encoded using pretrained sentence embedding models~\cite{reimers-2019-sentence-bert}. A specially chosen query is used to select relevant sentences based on similarity scores without exceeding the predefined budget. Finally, the new context is re-assembled and fed to the main model. We highlight the key differences in various steps of the pipeline in the following table~\ref{tab:rag_spec}: 

\begin{table}[ht]
\centering
\begin{tabular}{l|lll}
\toprule
\textbf{Model Name} & \textbf{Embedding Model} & \textbf{RAG Query} & \textbf{Reassemble Method} \\
\midrule
\midrule
\textsc{RAG-Llama LS} & \texttt{gtr-t5-large} & Last sentence in full prompt & Original order \\
\textsc{RAG-Llama EQ} & \texttt{all-mpnet-base-v2} & Provided by the dataset & Ordered by relevance \\
\bottomrule
\end{tabular}
\caption{\textbf{RAG Baseline Specification:} We implemented two RAG baselines with different trade-offs to compare \ours{}. }
\label{tab:rag_spec}
\end{table}

In Figure~\ref{fig:longbench_rag}, we compare \texttt{Llama-3.1-70B-Instruct} with \textsc{Rag-Llama-70B LS} and \textsc{Rag-Llama-70B EQ}. Since both RAG variants are based on sentence chunkation, and hence we calculate the final real token keep percentage for visualization and a fair comparison. 

It is worth noting that both two RAG variants can in principle fall short under given tasks due to the fact that their strategy for selecting the query for retrieval is not flexible enough. For example, \textsc{Rag-Llama-70B LS} will become less effective when the real query is not placed at the end of the prompt, and \textsc{Rag-Llama-70B EQ} not only assumes that the real query is separated from the context and given to the model but also needs special catering when it is not obvious how to design the query for certain tasks (e.g. summarization). Therefore, we consider RAG systems and \ours{} to be useful for different cases with varying degrees of requirements for efficiency, cost, performance, and generality. 

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{figures/standard.pdf}}
\caption{\textbf{Standard Short Tasks on Llama 70B:} We include results on popular regular context tasks ranging from common knowledge, math, reasoning, and coding ability. Unlike prior works on token eviction and prompt compression, we wish to give a comprehensive evaluation on domains where \ours{} becomes less effective due to the fact that short and knowledge rich prompts are less compressible. }
\label{fig:standard}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/long_bench_70b_rag.pdf}}
\caption{\textbf{LongBench Baseline Comparison:} We compare \ours{} against the baseline and \textsc{Rag-Llama} in this separate figure for clarity. \textsc{Rag-Llama} could be effective on certain tasks but for the majority of the tasks, but \ours{} does not require any prior knowledge about the prompt while still being accurate with a finer-control. }
\label{fig:longbench_rag}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/long_bench_70b.pdf}}
\caption{\textbf{LongBench Results on 70B Model:} We supplement the evaluation of \texttt{Llama-3.1-70B-Instruct}, under the same setup as in Figure~\ref{fig:longbench_405b} and~\ref{fig:longbench_rag}.}
\label{fig:longbench_70b}
\end{center}
\vskip -0.2in
\end{figure*}


\section{Experiment Details}
\label{app:experiment}
There are some details for our experiments that we wish to give some accounts for: 

\begin{enumerate}
    \item For standard short task evaluation in Sec~\ref{sec:standard}, we use \textsc{lm-eval-harness}~\cite{eval-harness} and \textsc{eval-plus}~\cite{evalplus}. For several tasks in \textsc{lm-eval-harness}, we include task configuration files for \texttt{Llama-3.1} based on its templates in our code base, which should be placed in the right place for reproducing experimental results. 
    
    \item In the QPS experiment in Sec~\ref{sec:efficiency}, we add an extra 5 seconds to the timeout in order to avoid potential system instability. The final results are reported with the original timeout. We set the number of samples for each category based on the maximum QPS we want to evaluate, which makes sure we have a constant QPS during the duration of querying. 

    \item When running all experiments in vLLM (0.6.3.post1), we set \hl{enforce\_eager=True} and \hl{enable\_chunked\_prefill=False} to avoid any unexpected behaviors. 
\end{enumerate}

\section{System Specification}
\label{app:system}

In Table~\ref{tab:system}, we list the detailed specification of the system on which we test the efficiency of models. 

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
        \toprule
        \textbf{System Hardware/Software Name} & \textbf{Value} \\
        \midrule
        \midrule
        CUDA Version & 12.7 \\
        vLLM Version & 0.6.3.post1 \\
        \midrule
        GPU Type & 8 $\times$ NVIDIA H200 \\
        Total GPU TFLOPS & 428.2 \\
        Total RAM & 1123.2 GB \\
        Per GPU Memory Bandwidth & 4052.8 GB/s \\
        Per GPU NVLink Bandwidth & 478.1 GB/s \\
        Per GPU PCIe Bandwidth & 52.8 GB/s \\
        Per GPU PCIe Lanes & 16 $\times$ PCIe 5.0 \\
        \midrule
        Disk Bandwidth & 4730 MB/s \\
        Internet Upload Speed & 605.5 Mbps \\
        Internet Download Speed & 733.4 Mbps \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{System Specification for Efficiency Benchmarking:} Efficiency scores can vary when benchmarked on different platforms, and therefore, we list the detailed specification of the system we're using for better reproducibility and understanding. }
    \label{tab:system}
\end{table}

\section{Overhead Analysis}
\label{app:overhead}

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        \toprule
        \textbf{Parameters} & \textbf{Value} \\
        \midrule
        \midrule
        Number of layers & $L$ \\
        Hidden Size & $D$ \\
        Intermediate Size & $I$ \\
        Number of Query Heads & $H$ \\
        Number of KV Heads & $H'$ \\
        Vocabulary Size & $V$ \\
        \midrule
        Sequence Length & $S$ \\
        Batch Size & $B$ \\
        \midrule
        MLP FLOPS &  $3BSDI$ \\
        QKVO Projections FLOPS & $BSD^2(2 + 2H'/H)$ \\
        Self-Attention FLOPS & $2BS^2D$\\
        LM-Head FLOPS & $BSDV$ \\
        \midrule
        Total FLOPS & $LBSD(3I + D(2 + 2H'/H) + 2S) + BSDV$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{FLOPS Estimation of Llama Models:} We estimate the FLOPS (MACS) for Llama model with a given configuration. We ignored lower order terms of computations such as vector addition, RMSNorm, RoPE, and treat all matrix operation as FMA for simplicity. }
    \label{tab:flops}
\end{table}

\begin{figure*}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\linewidth]{figures/flops.pdf}}
\caption{\textbf{Theoretical TTFT Speedup of \ours{}:} We show the theoretical upper-bound of the TTFT speedup that \ours{} can achieve assuming no other system overhead. The percentage of each bar is the percentage of FLOPS spent on speculation w.r.t. the total FLOPS. Comparing it with results in Figure~\ref{fig:efficiency}, our real measurement has very little gap to the theoretical maximum, which suggests the efficiency of the implementation. }
\label{fig:flops}
\end{center}
\vskip -0.2in
\end{figure*}

\ours{} uses a smaller model as a speculator to help accelerate the larger model. Although proven to be effective, there is no free lunch. In this section, we analyze and quantify the overhead incurred by the speculator so that practitioners are more informed when they choose to use it. 

We start by calculating the FLOPS of a transformer model based on a standard implementation, and then compute the theoretical overhead when using a specific speculator for a specific main model with the official Llama model configuration. Since the prefill phase is mostly compute-bound, it is fair to use the FLOPS ratio as a decent approximation of latency improvement. We introduce several parameters for an Llama-like transformer architecture and we calculate the FLOPS of each module separately using these parameters. We ignore less essential computations (e.g. normalization, RoPE, etc) and the formula are shown in Table~\ref{tab:flops}. 

In Figure~\ref{fig:flops}, we calculate the theoretical upper-bound of the TTFT speedup for \ours{} with sequence length being $32K$ and batch size equal to 16. As we can see from the theoretical analysis, the real speedup we obtain from the implementation is very close to it, as shown in Figure~\ref{fig:efficiency}, which suggests that our implementation is highly efficient (i.e. measured $7.66\times$ compared to analyzed $7.72\times$). On each bar, we annotate the overhead of the speculation process, which we define as: 
\begin{align*}
    overhead(\alpha) := \frac{FLOPS(spec)}{FLOPS(spec) + \alpha * FLOPS(base)}, \forall \alpha \in (0, 1]
\end{align*}

Within our expectation, the overhead is higher when we have a lower keep rate and lower when we have a higher keep rate. Table~\ref{tab:flop_ratio} reports the theoretical relative FLOPS between the speculator and the base model. 

\begin{table*}
    \centering
    \begin{tabular}{c|c}
        \toprule
        \textbf{Base Model Size} & \textbf{$FLOPS_{spec}/{FLOPS_{base}}$} \\
        \midrule
        \midrule
        70B & 14.24\% \\
        405B & 2.96\% \\ 
        \bottomrule
    \end{tabular}
    \label{tab:flop_ratio}
    \caption{\textbf{Relative FLOPS of \ours{}:} We calculate the theoretical FLOP ratio between the speculator model of size 8B and the base model. }
\end{table*}

\section{LongBench Task Length Distribution}

We visualize the LongBench suite's average length for each task, which, when coupled with the token keep rate, provides a more clear picture of the model's efficiency gain. 

\begin{figure}[h]
\begin{center}
\vskip -0.2in
\centerline{\includegraphics[width=0.65\columnwidth]{figures/longbench_len.pdf}}
\caption{\textbf{LongBench Prompt Token Lengths:} We visualize the average token lengths of prompts for each task spanning the five major categories in LongBench. }
\label{fig:longbench_len}
\end{center}
\vskip -0.3in
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2025 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
