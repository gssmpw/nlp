\section{Speculative Prefill}
In this section, we present \ours{} by first describing its high-level algorithm, followed by several design choices that mitigate various biases, and a detailed implementation account. Finally, we touch a bit on how to integrate \ours{} to speculative decoding, forming a full small-model-assisted inference paradigm. 

\subsection{Overall Architecture}

\ours{} follows a conceptually simple architecture where a usually less expensive model is chosen as the speculator model that predicts contextually important tokens given a prompt. The speculated tokens, alone with the original position information, are then fed to the main model for processing. In the following section, we will discuss two central design choices in more details, namely the token estimation algorithm and the selection strategy. Note that \ours{} can be seamlessly integrated with speculative decoding in which the small model can work both in the prefill stage for token selection and the decoding stage for drafting proposals, making our approach almost free to integrate and deploy.  

\subsection{Token Importance Speculation}
\label{sec:token_importance}

The goal here is to select which tokens are contextually important for a given query and send those along with necessary positional information for the main model. The procedure starts with calculating the attention scores from the speculator, which uses the last token's attention score w.r.t. the context as the surrogate for measuring token importance:

\begin{equation*}
    a_{ij} := \text{Softmax}(Q_{M + j}K^T))_i, \forall 0\leq i < M, 0 \leq j < N
\end{equation*}

where $M$ is the context length, $N$ is the number of look-ahead steps, and $a_{ij}$ is the attention score for the $i$th token in the prompt w.r.t. the $j$th decoded token, assuming we're looking at a particular layer. 

We build on top of this by aggregating the scores over the whole speculator model (Section~\ref{sec:attn_agg}) with potential look-ahead (Section~\ref{sec:look_ahead}) and select tokens based on chunks (Section~\ref{sec:chunk_select}). The subset of chosen tokens with their original positional information (Section~\ref{sec:position_ids}) will then be used for the main model's inference. 

\subsubsection{Mitigate Position Bias via Look-Ahead}
\label{sec:look_ahead}
Prior works have shown that there are many biases for attention scores, such as the sink phenomenon~\cite{xiao2024efficientstreaminglanguagemodels} (the first couple of tokens tend to have higher weights) and the proximity bias (tokens closer to the output tend to have higher weights~\cite{lv2024critiprefillsegmentwisecriticalitybasedapproach}). To mitigate these issues, instead of relying on the attention score of the last token alone, we further decode the speculator by $N$ steps and obtain the attention information from the new $N$ tokens~\cite{wan2024lookmlookonceoptimizationkv}. $N$ here serves as a trade-off between bias and budget, which can substantially increase the performance for shorter context queries. 

\subsubsection{Aggregated Attention Score as Token Importance}
\label{sec:attn_agg}
Given the full attention scores of the speculator, we decide to use a max-mean aggregation strategy to map to scalar token importance. Formally, given an attention score tensor of shape $[N, L, S, H]$ where $N$ is the number of look-ahead tokens, $L$ is the number of layers, $S$ is the sequence length, and $H$ is the number of heads, we take the maximum over $H$ and $L$ dimension to make salient tokens stand out, and average over $N$ to account for fair token contribution. 

\subsubsection{Denoise Attention Scores by Chunk Selection and Pooling}
\label{sec:chunk_select}
It has also been observed in concurrent works~\cite{lv2024critiprefillsegmentwisecriticalitybasedapproach} that tokens that are positioned nearby share similarity in importance. We take this insights to select tokens by chunks in order to reduce the variance of our token importance estimation. Specifically, we chunk the context contiguously and average the token score within each block, and then we select the Top-K blocks. In order to eliminate the artifacts of chunkation, we apply a 1D average pooling before this to smooth the cross block scores. 

\subsubsection{Restoration of Position IDs}
\label{sec:position_ids}
Finally, when we select the subset of tokens based on our compute budget and query compressibility, we also need to restore the position information which are also sent to the main model. Basically, instead of using a contiguous position ids as before, we send a potentially non-continuously increasing position ids which are obtained from tokens' positions in the original context. In addition to that, we also need to explicitly set the decoding token position to the context length in case we dropped tokens before the first decoding token. An example is shown below with ten prompt tokens and three decoding tokens (bold): 
\begin{align*}
    \text{Original Pos Ids:}\ &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \\
    \text{Speculated Pos Ids:}\ &[0, 1, 3, 6, 7] \\
    \text{Decoding Pos Ids:}\ &[0, 1, 3, 6, 7, \textbf{10, 11, 12, ...}]
\end{align*}
where the \textbf{bold indices} are the decoding positions which are offset based on the original position information. We found this design choice to be crucially essential, especially for position-sensitive tasks such as synthetic tasks involving retrieval and counting. 

\subsection{Implementation Details}

We describe both the high-level procedure and the implementation details of \ours{} in this section. In Algorithm~\ref{algo:spec_prefill}, we list the high-level steps of conducting \ours{}. Our implementation is based on creating a monkey patch on top of vLLM~\cite{li2024snapkvllmknowslooking} which only needs a few line of code along with a configuration file to enable \ours{}. The KV cache is not necessary if we do not need to look-ahead for our speculator, which can save lots of memory allocation. However, we do need to explicitly store the queries of the decoded tokens (including the last token of the input query) which we later retrieve to compute the attention score. Note that a specific mapping (e.g. slot mapping in vLLM) might be kept track of to retrieve the right data. For batched look-ahead, we only consider tokens that are valid by checking afterwards whether they are equal to the EOS tokens. Finally, we want to mention that despite being being a sequential implementation, we can actually split the process of speculation into a separate procedure and decouple from the inference of the main model by adding a new layer of scheduling, which we leave as a future work. 

\subsection{Relation to Speculative Decoding}
Speculative decoding has been proven to be extremely successful at accelerating the decoding TPS. \ours{} can be seamlessly combined with speculative decoding by sharing the same draft model. Since speculative decoding itself requires a full forward pass of the context, \ours{} will provide the necessary KV information required for subsequent decoding speculation, hence amortizing the overhead. This will open-up a huge space of possibilities, and leading to the first paradigm of an inference system that is fully aided by smaller speculators. 

\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE Base model $M$, speculator $S$, look-ahead steps $N$, batch of mixed requests $B$, base model QKV cache $C_b$, speculator KV cache $C_s$ 
\STATE $B_p, B_d \gets$ \textit{split\_prefill\_decode\_requests} ($B$)

\STATE \COMMENT{Section~\ref{sec:look_ahead}}
\FOR{$i = 1$ to $N$}
    \STATE $B_p' \gets$ \textit{model\_forward} ($S$, $B_p$, $C_s$, \textit{store\_q}=True)
    \STATE $B_p \gets$ \textit{update\_requests} ($B_p$, $B_p'$)
    \STATE $B_p \gets$ \textit{check\_for\_eos} ($B_p$)
\ENDFOR

\IF{\textit{is\_tensor\_paralleled} ()}
    \STATE \textit{tp\_gather\_qk} ($C_s$)
\ENDIF

\STATE $Q, K \gets$ \textit{retrieve\_qk} ($B_p$, $C_s$)
\STATE $A \gets$ \textit{compute\_attention\_score} ($Q$, $K$)
\STATE \COMMENT{Section~\ref{sec:attn_agg}}
\STATE $A \gets$ \textit{aggregate\_attention\_score} ($A$) 
\STATE \COMMENT{Section~\ref{sec:chunk_select}}
\STATE $T \gets$ \textit{chunk\_select\_from\_smoothed\_attention} ($A$) 
\STATE \COMMENT{Section~\ref{sec:position_ids}}
\STATE $P \gets$ \textit{restore\_pos\_ids} ($T$, $B_p$)
\STATE $B \gets$ \textit{merge\_requests} ($T$, $P$, $B_p$, $B_d$)
\STATE \textbf{Return} \textit{model\_forward} ($M$, $B$, $C_b$)
\end{algorithmic}
\caption{Speculative Prefill}
\label{algo:spec_prefill}
\end{algorithm}
