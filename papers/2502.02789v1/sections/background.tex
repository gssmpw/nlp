\section{Background}

\subsection{Inference Bottlenecks}

Improving LLM inference efficiency has been extensively studied in prior work~\cite{miao2023efficientgenerativelargelanguage, yuan2024llminferenceunveiledsurvey}. We review works that focus on different aspects when dealing with real serving systems where the bottlenecks are quite different under various serving requirements (e.g. long context domains, latency sensitive applications, etc)~\cite{kwon2023efficient, zheng2024sglangefficientexecutionstructured}. 

LLM inference can be roughly divided into two major procedures, namely the prefill phase where the model computes the KV necessary for producing the output based on the query and the decoding phase where the model predicts new token auto-regressively. 

\subsection{Decoding Acceleration}
The decoding phase is mostly memory-bounded, and therefore reducing the amount of data to move around will effectively help improve the latency. As a result, explicitly manipulating the KV cache has been extremely successful with many strategies: H2O~\cite{zhang2023h2oheavyhitteroracleefficient} and StreamingLLM~\cite{xiao2024efficientstreaminglanguagemodels} idenfitied key insights to KV dynamics which is used to evict less essential KV caches during decoding. CacheGen~\cite{liu2024cachegenkvcachecompression}, Q-Hitter~\cite{qhitter}, and ShadowKV~\cite{sun2024shadowkvkvcacheshadows} apply efficient techniques to compress/quantize, store, and transmit KV caches to reduce memory overhead. Speculative decoding relies on the insight that hides the memory latency by concurrently verifying several speculated tokens from either a draft model or itself~\cite{leviathan2023fastinferencetransformersspeculative, zhang2024draftverifylossless, xia2024unlockingefficiencylargelanguage}.  

Despite being crucially important, decoding speed is not the only factor that influences the overall inference pipeline and we will review why \textit{sometimes the prefill time optimization is even more essential} in many cases. 

\subsection{Prefill Acceleration}

The time-to-first-token (TTFT) is crucially important both from a user experience but also the system serving perspective (Sec~\ref{sec:efficiency}). Unlike the decoding phase, the prefill phase is usually compute-bounded and its bottleneck can shift between the attention module and the MLP based on the prompt sequence length. Moreover, the input token length can often eclipse that of the generation tokens (e.g. 10:1 ratio) in real traffic~\cite{qiao2024swiftkvfastprefilloptimizedinference}. 

Many works have explored ways to make self-attention faster: Flash-attention series~\cite{dao2022flashattentionfastmemoryefficientexact, dao2023flashattention2fasterattentionbetter} computes the exact attention using carefully designed hardware-aware algorithms. Special attention masks are designed for sparse calculation such as LongFormer~\cite{beltagy2020longformerlongdocumenttransformer}, MInference~\cite{jiang2024minference10acceleratingprefilling} and Duo-attention~\cite{xiao2024duoattentionefficientlongcontextllm}. However, when the sequence lengths become shorter, the MLP computation quickly becomes dominant and the gains from attention acceleration are deflating~\cite{xiong2023effectivelongcontextscalingfoundation}. 

Orthogonal to techniques such as prompt compression/rewrite~\cite{jiang2023llmlinguacompressingpromptsaccelerated, jiang2024longllmlinguaacceleratingenhancingllms}, layer dropping~\cite{Elhoushi_2024}, and weight quantization methods~\cite{lin2024awqactivationawareweightquantization}, we explore selecting \textit{important} prompt tokens to skip the full forward computation. GemFilter~\cite{shi2024discoveringgemsearlylayers} uses an extra pass to get a model's own middle layer attention information that decides on what tokens to keep for the real forward. Contrast to this, we apply a separate and cheaper model to speculate locally important tokens via token transferability, which can scale more efficiently than GemFilter. Concurrent to ours, SwiftKV~\cite{qiao2024swiftkvfastprefilloptimizedinference} learns to skip later layers by reusing the past layers' KV, which achieves up to $50\%$ TTFT reduction (\ours{} can reach up to $87\%$ TTFT reduction). Unlike our zero-shot requirement, they require extra light-weight fine-tuning due to modified model behavior. It is worth noting that our method approaches the problem in a different way, which makes them complimentary to each other. Finally, akin to our motivation, KV Prediction~\cite{horton2024kvpredictionimprovedtime} proposes to adapt a cheaper system (i.e. a learned auxiliary network and a KV predictor) to predict the KV cache of the base model, thus bypassing the original KV computation. We show that \ours{} can accomplish better TTFT reduction than theirs, without introducing extra overhead when coupled with speculative decoding~\cite{leviathan2023fastinferencetransformersspeculative} while maintaining competitive quality. 
