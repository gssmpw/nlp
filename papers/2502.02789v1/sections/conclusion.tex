\section{Conclusion}

In this work, we introduce \ours{}, a training-free framework for accelerating the LLM inference by speculating what tokens to drop with the help of a smaller speculator model. Leveraging the insight that models of different sizes within the same family can usually transfer token importance, \ours{} not only achieves substantial improvement on TTFT, which leads to $7\times$ maximal supported QPS of an inference system, but also reduces the memory required. \ours{} can also be readily combined with other techniques such as speculative decoding, the combination of which could result in the first unified small-model-assisted inference pipeline. With extensive evaluations, we believe that \ours{} will be one of the practical answers to large-scale LLM inference systems. 