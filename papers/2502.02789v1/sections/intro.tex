\section{Introduction}

\begin{figure*}[t]
\vskip -0.1in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/qps.pdf}}
\vskip -0.2in
\caption{\textbf{Speculative Prefill QPS Improvement:} In an end-to-end server-client setting with real world datasets, we benchmark the average query latency under a given fixed timeout when sending queries at a constant QPS. \ours{} significantly improves the maximum QPS supported by the vLLM server as well as the latency compared to not using it. When we reach low keep rate, we can even serve the 405B model with \ours{} to run more efficiently than the 70B model. As the base model size increases and keep rate drops, we can get 7$\times$ end-to-end QPS boost while only occurring $<5\%$ accuracy. }
\label{fig:qps}
\end{center}
\vskip -0.3in
\end{figure*}

Large Language Models (LLMs) represent a transformative innovation in artificial intelligence, enabling machines to understand and generate human-like languages~\cite{bubeck2023sparksartificialgeneralintelligence, wei2022emergentabilitieslargelanguage, feng2024faragillmsneed}. Many SOTA models have been developed, such as GPT-4~\cite{openai2024gpt4technicalreport}, the Llama family~\cite{grattafiori2024llama3herdmodels}, DeepSeek R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, Mistral~\cite{jiang2023mistral7b}, Gemini~\cite{geminiteam2024geminifamilyhighlycapable}, and Qwen2~\cite{yang2024qwen2technicalreport}, to meet the increasing expectations of users. In order to broaden their real-world applications, one essential requirement is to build an efficient serving engine that can satisfy various requirements~\cite{miao2023efficientgenerativelargelanguage, kwon2023efficient, zheng2024sglangefficientexecutionstructured, shoeybi2020megatronlmtrainingmultibillionparameter}. 

% Why is improving TTFT important?
There are several fundamental reasons why TTFT stands so pivotal: 1) many applications require a fast response time that directly influences how users perceive the responsiveness of the system and 2) more importantly, TTFT determines the scaling of maximal QPS an inference engine can support as shown in Figure~\ref{fig:qps}. 
% Why is optimizing it hard?
However, optimizing TTFT is an arduous task mostly because the prefill stage is largely compute-bounded and the computational bottleneck can change depending on the prompt length and batch size. For example, many works focus on improving the self-attention speed~\cite{dao2022flashattentionfastmemoryefficientexact, jiang2024minference10acceleratingprefilling}, but in reality, there is still a huge traffic of large-batch short to medium context queries where it is the MLP part that clogs the whole system. 
% why is it not solved well before
Despite achieving impressive results, prior works that target the prefill phase either require a post-training adaptation~\cite{qiao2024swiftkvfastprefilloptimizedinference,horton2024kvpredictionimprovedtime} or scale less efficiently~\cite{shi2024discoveringgemsearlylayers}. 

% what is our insights and how did we create a method on top of that? key components. 
Inspired by those work, we found a key insight that LLMs can retain most of its performance when given only \textit{a carefully chosen subset} of tokens from the prompt, and the model is able to adapt to that in a zero-shot manner. \ours{} optimizes the TTFT by leveraging a secondary lightweight model to speculate locally important tokens. Only these tokens are sent later to the base model. It reduces the total FLOPS by a factor proportional to the percentage of token drop. \ours{} does not require any fine-tuning and is ready to be deployed and scaled to larger models. We summarize our key contributions: 

\begin{itemize}
    \item We present a conceptually simple, effective, and noval framework called \ours{} that significantly improve the prefill phase, hence the maximal QPS, of LLM inference without any fine-tuning or adaptation. 
    \item We conducted comprehensive evaluations both on real and synthetic datasets to demonstrate its effectiveness and limitations, giving a full picture of the expected benefits when deployed to productions. 
    \item We implemented our method on industry standard serving engines, and benchmark its performance in both an end-to-end fashion and ablation experiments. The end result is a system that can serve \texttt{Llama-3.1-405B-Instruct-FP8} with up to $7\times$ maximal QPS under the same system specification and $7.66\times$ reduced TTFT while maintaining decent accuracy. 
    \item Our method can be easily combined with techniques from quantization, KV eviction, and speculative decoding, making it an ideal add-on to existing engines. 
\end{itemize}