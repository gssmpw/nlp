\section{Experiments}
In this section, we start with our experiment setup for reproducibility, followed by categorizing prompt compressibility of different queries. We evaluate \ours{} on downstream long context, synthetic context probing, and standard short tasks. Finally, we conclude with a comprehensive efficiency measurement of our system under the real end-to-end setting. 

\subsection{Setup}
We implement \ours{} in vLLM that supports tensor parallelism with the same degree as the main model\footnote{We expose the API so that it only takes a few line of code to apply \ours{} before initializing vLLM engines.}. Due to its token dropping nature, we focus on evaluating \textit{generative} tasks in this section and include a comprehensive range of benchmarks to fully present its applicability and potential pitfalls. We run all of experiments using a tensor parallelism of 8 for both the speculator and the base model across either 8 NVIDIA H100s or H200s (full system specification in Appendix~\ref{app:system} and guidance on reproducing results in Appendix~\ref{app:experiment}). We choose \textsc{Llama-3.1-8B-Instruct}~\cite{grattafiori2024llama3herdmodels} with BF16 precision as our speculator for a balance of efficiency and attention transferability and couple it with either \texttt{Llama-3.1-70B-Instruct} (BF16) or \texttt{Llama-3.1-405B-Instruct-FP8}\footnote{\url{https://huggingface.co/neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8}. } (fully quantized FP8) as the base model. In terms of token keep rate, we use a fixed percentage (i.e. the ratio of chunks when we do chunk selection) for a given task. In practice, we might devise more adaptive strategy for how many tokens to keep based on the query compressibility discussed next, or delegate the decision to users based on their needs. We leave all these possibilities for prospective applications. 

\subsection{Query Context Compressibility}
We empirically found three types of queries during our evaluations based on the quality difference before and after applying \ours{}: 
\begin{enumerate}
    \item \textit{Information-dense queries}: These queries usually are short and information dense, which naturally makes token dropping less effective because there is no redundancy in the prompt. 
    \item \textit{Compressible queries}: These queries are those that do not get degradation after removing a significant amount of tokens, often seen in long context tasks. 
    \item \textit{Noisy queries}: These queries, perhaps surprisingly, get better results after dropping some ``noisy'' tokens. We hypothesize the reason behind the improvement might be that \ours{} helps remove noisy and distracting tokens in the prompt, hence projecting the prompt to the space where the main model performs better. 
\end{enumerate}
We will see examples in each categories in the following evaluations. It is worth noting that we used a fixed keep percentage for our evaluation and it can be tremendously helpful to automatically decide on the percentage based on the query, pushing the limit of \ours{}, which we leave as a future work. 

\subsection{Real Long Context Tasks: LongBench}
\label{sec:longbench}

\begin{figure*}[t]
% \vskip -0.1in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figures/long_bench_405b.pdf}}
\caption{\textbf{LongBench Main Result on Llama 405B:} In this figure, we showcase the effectiveness of \ours{} on LongBench, which consists of six categories of long context downstream tasks. In each plot, the dash lines are the results of baseline \texttt{Llama-3.1-405B-Instruct-FP8} for each subtask and we benchmark \ours{} with increasing token keep rates. We observe different behaviors such as quality preservation, degradation, and improvement based on the task type. }
\label{fig:longbench_405b}
\end{center}
\vskip -0.2in
\end{figure*}

We start with long context tasks using LongBench~\cite{bai2024longbench}, which consists of six different categories focusing on various aspects of long context modeling abilities. For this benchmark, we compare two versions of our techniques against three baselines to showcase the trade-offs between using more speculative computation and quality: 

\begin{itemize}
    \item \textit{Baseline}: the original Llama model we evaluate. 
    \item \textit{Sentence RAG}: \ours{} can be framed as a special case of retrieval-augmented (RAG) LLM with the granularity of tokens or blocks and the relevance metric controlled by the speculator's internal knowledge~\cite{li2024retrievalaugmentedgenerationlongcontext, gao2024retrievalaugmentedgenerationlargelanguage, lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}. Therefore, we implemented two simple sentence-level RAG baselines to compare \ours{} against, which we call \textsc{RAG-Llama LS} and \textsc{Rag-Llama EQ} (more detailed description in Appendix~\ref{app:rag}). 
    \item \textit{\ours{}}: \ours{} with raw attention scores and ignoring the techniques we discussed in Section~\ref{sec:look_ahead} and~\ref{sec:chunk_select}. 
    \item \textit{\ours{} Full}: \ours{} with all techniques but no look-ahead.
    \item \textit{\ours{} Full LAH}: \ours{} with all techniques with 8-step look-ahead\footnote{We empirically found that going beyond 16 look-ahead gives minimal performance gain. }.
\end{itemize}

In Figure~\ref{fig:longbench_405b}, we report the main results on LongBench for \texttt{Llama-3.1-405B-Instruct-FP8}, whose length information is visualized in Appendix Figure~\ref{fig:longbench_len}. To compliment it, we also include the performance of \texttt{Llama-3.1-70B-Instruct} in Appendix Figure~\ref{fig:longbench_70b}. We vary the token keep percentage starting from $10\%$ to $90\%$ and draw the baseline model quality using the dash lines for each subtask. To ablate the effect of our design choices, we compare \textit{\ours{}} and \textit{\ours{} Full LAH} with the 70B models in Appendix Figure~\ref{fig:longbench_70b}, and \textit{\ours{} Full} and \textit{\ours{} Full LAH} with the 405B model in Figure~\ref{fig:longbench_405b}. 

As we can observe, for categories such as Single-Document QA, Multi-Document QA, Few-Shot Learning, \ours{} can preserve most of the quality up to keeping only $10\%$ tokens. For Summarization, we expect to see some degradation in performance as we drop more. Perhaps surprisingly, for the smaller 70B model, we can achieve better quality after we remove some tokens on tasks like Code Completion. As the model size increases, the quality gap between applying \ours{} or not becomes smaller, which indicates that bigger models adapt better with our speculated subset of tokens. 

To ablate the effectiveness of techniques discussed in Section~\ref{sec:token_importance}, we compare them separately in Figure~\ref{fig:longbench_405b} and ~\ref{fig:longbench_70b} to avoid crowdedness. In both cases, we can see consistent improvement and the benefits of look-ahead are more consistent in shorter context tasks (more details in Sec~\ref{sec:standard}) . 

In Appendix Figure~\ref{fig:longbench_rag}, we compare \ours{} against our implemented ad hoc baseline \textsc{RAG-Llama EQ} and \textsc{RAG-Llama LS}. Both RAG models split the context into sentences, encode these sentences into embeddings, and retrieve relevant context from a given input query, the design choice of which differentiates \textsc{RAG-Llama EQ} and \textsc{RAG-Llama LS}. Both two variants of \textsc{RAG-Llama} are not as generic as \ours{} and does not support a precise token drop rate in its naive form. Therefore, we plot its performance with the true post-calculated keep rate. Within our expectation, the embeddings can in principle capture semantic similarity well but often omit more nuance relationships. Despite achieving decent scores on specific tasks, \textsc{RAG-Llama} is eclipsed by \ours{} in most tasks under the same keep rate, further supporting the effectiveness and flexibility of \ours{}. We discuss more details of the baselines, trade-offs and their relationships in Appendix~\ref{app:rag}. 

\subsection{Synthetic Context Probing: RULER}

\begin{table*}[t]
\vskip -0.1in
\caption{\textbf{RULER Results on Llama 70B:} We present results of \ours{} with 10\% token keep rate on the effective context probing suite RULER with varying context length. \ours{} can preserve the performance of all except for aggregation tasks, which are \textit{less compressible} due to the problem nature as each word in the prompt is important to reason about word frequency and commonality. }
\begin{center}
\begin{scriptsize}
\begin{tabular}{lccccccc}

\toprule
\multicolumn{1}{c}{\textbf{Model Name}} & \multicolumn{1}{c}{\textbf{Task Length}} & \multicolumn{1}{c}{\textbf{Retrieval}} & \multicolumn{1}{c}{\textbf{Multi-hop Tracking}} & \multicolumn{1}{c}{\textbf{QA}} & \multicolumn{1}{c}{\textbf{Aggregation}} & \multicolumn{1}{c}{\textbf{Average}} \\ 
&  & \multicolumn{1}{l}{Niah Variants} & \multicolumn{1}{l}{Variable Checking} & \multicolumn{1}{l}{SQuAD \& HotpotQA} & \multicolumn{1}{l}{CWE \& FWE} & \multicolumn{1}{l}{w/o Aggregation}\\
\midrule
\multirow{4}{*}{\baseline{70B}} & 4k & 100.0 & 100.0 & 76.9 & 99.7 & \underline{92.3} \\
& 8k & 99.9 & 100.0 & 74.7 & 98.0 & 91.5 \\
& 16k & 99.8 & 100.0 & 72.0 & 97.8 & 90.6 \\
& 32k & 99.6 & 100.0 & 69.8 & 96.9 & 89.8 \\
\midrule
\rowcolor{lightgray!40} & 4k & 99.7 & 89.6 & 75.2 & 77.9 & 88.2 \\
\rowcolor{lightgray!40} & 8k & 99.6 & 100.0 & 75.6 & 79.7 & \underline{91.7} \\
\rowcolor{lightgray!40} & 16k & 99.5 & 99.1 & 75.3 & 78.5 & \underline{91.3} \\
\rowcolor{lightgray!40} \multirow{-4}{*}{\ours{} with 10\% Keep Rate} & 32k & 99.7 & 100.0 & 72.6 & 70.0 & \underline{90.8} \\
\bottomrule

\end{tabular}
\end{scriptsize}
\label{tab:ruler}
\end{center}
\vskip -0.2in
\end{table*}

In addition to LongBench, we also evaluate \ours{} on a synthetic context probing task to see if \ours{} can preserve effective context lengths. RULER~\cite{hsieh2024ruler} is a suite of synthetically created tasks with controllable lengths, which ranges from retrieval, multi-hop tracking, real QA datasets, and context aggregation tasks. In Table~\ref{tab:ruler}, we include the results for the 70B model with \ours{} that keeps 10\% context. As we can observe, \ours{} preserves the quality despite only using one tenth of the tokens except for aggregation tasks, which we believe belonging to the category of information-dense queries that are not our main target application. Take CWE from aggregation category for example: CWE asks for the common words presented in the prompt, which becomes challenging to answer by token dropping. We hope to explore in the future ways of potentially rewriting the queries instead of directly dropping the tokens to mitigate this type of limitation~\cite{jiang2023llmlinguacompressingpromptsaccelerated, jiang2024longllmlinguaacceleratingenhancingllms}. Averaging scores without the aggregation task, we can see that in most context lengths, \ours{} even helps improve the quality\footnote{For 4k Multi-hop Tracking, since we only keep around 400 tokens, we might unintentionally ignore some essential information. But to keep experiment setup more consistent, we list the results here for clarity. }, suggesting 1) that \ours{} provides both efficiency and performance gains at the same time, and 2) the fact that there are lots of potential redundancy and noise in these synthetic tasks. 

\subsection{Standard Short Tasks} 
\label{sec:standard}

Unlike prior works on prefill token dropping techniques~\cite{lv2024critiprefillsegmentwisecriticalitybasedapproach, shi2024discoveringgemsearlylayers} that do not include regular short context task evaluation, we present a wide range of standard tasks to show the full spectrum of \ours{}'s performance and potential caveats. We select tasks spanning general knowledge (Generative MMLU~\cite{hendrycks2021measuringmassivemultitasklanguage} and Instruction Following Evaluation~\cite{zhou2023instructionfollowing}), math (GSM8K 8 Shots~\cite{cobbe2021training}), coding (HumanEval~\cite{chen2021evaluatinglargelanguagemodels} and MBPP~\cite{austin2021programsynthesislargelanguage}), and reasoning abilities (Arc Challenge~\cite{Clark2018ThinkYH} and GPQA 8 Shots~\cite{rein2023gpqa}). 

In Figure~\ref{fig:standard}, we showcase the performance of \texttt{Llama-3.1-70B-Instruct} on these tasks. Non-surprisingly, prompts from standard tasks without few shot examples are very information dense, making \ours{} less effective with low token keep rate. However, for certain tasks (e.g. MBPP and GPQA), we do observe improved performance when dropping certain tokens. On average, \ours{} can maintain and even surpass the baseline when choosing the right token keep rate. 

\subsection{Efficiency Benchmarking}
\label{sec:efficiency}

\ours{} offers great improvement to TTFT, a speedup almost proportional to the percentage of tokens we drop from the speculator, with almost ignorable overhead as we increase the base model size. In this section, we benchmark both the 70B and 405B models under two settings: 1) understanding the average query latency and QPS dynamics with real downstream datasets, and 2) evaluating TTFT with varying sequence lengths on synthetic data. We used one node consisting of eight NVIDIA H200s for all experiments (full system specification is listed in Table~\ref{tab:system} from the Appendix~\ref{app:system}). 

\subsubsection{Average Query Latency under Different QPS with Real Downstream Datasets} 

We want to measure the \textit{real} performance gain we can create in an end-to-end fashion. To do this, we launch a vLLM server with a given model, and an OpenAI API client~\footnote{\url{https://github.com/openai/openai-python}} that sends asynchronous requests at a constant QPS with queries from datasets in LongBench. We measure the client-side per query latency that consists of the prefill stage with several decoding steps based on the maximum budget defined by the task. In Figure~\ref{fig:qps}, we increase the QPS of our client and calculate the  average query latency with a given fixed timeout to simulate real-world user needs. For each task category, we draw samples randomly from each subtask and shuffle them before starting the querying, making sure the same set of queries is used over all QPS. As we can observe, all models will follow a standard three-stage pattern: 1) the initial constant stage where latency remains almost unchanged as all queries can be finished before receiving new ones, 2) the middle linear stage where TTFT is small enough but the decoding step might not finish fast enough, and 3) the final timeout stage where the server can not even finish the prefill stage before new requests and all subsequent queries are jammed thereafter. Since the maximum QPS a system can support is $\mathcal{O}(1/TTFT)$\footnote{If we have finitely large timeout, we would expect \ours{} to support around $N$ times larger maximal QPS if we reduce TTFT by $N$ times.} given finite timeout, the acceleration from \ours{} will drastically increases the maximal QPS under a fixed timeout, which pushes the transition from stage 2 to stage 3 further later. 
    
We show that with the help of \ours{}, the 405B model can convert to 7$\times$ QPS improvement on a Multi-Doc QA suite from LongBench while maintaining $>95\%$ accuracy. The results vary as we change the model FLOPS ratio and drop rate. We believe that this type of analysis provides invaluable insights and tangible benefits of \ours{} when deployed to real world systems. 
    
\subsubsection{TTFT Improvement over Different Batch Size $\times$ Sequence Length Products}  

\begin{figure}[]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/efficiency.pdf}}
\vskip -0.1in
\caption{\textbf{Speculative Prefill TTFT Improvement:} We present prefill TTFT speed-up using \ours{} under different settings over Llama-3.1-70B-Instruct and Llama-3.1-405B-Instruct-FP8 (achieving up to 7.66x faster TTFT when keeping 10\% tokens for the 405B model). }
\label{fig:efficiency}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/efficiency_lah.pdf}}
\vskip -0.1in
\caption{\textbf{Speculative Prefill with look-ahead TTFT Improvement:} Complimentary to Figure~\ref{fig:efficiency}, we also show the relative speedup when using a look-ahead = 8 steps for both the 70B and 405B model. }
\end{center}
\vskip -0.2in
\label{fig:efficiency_lah}
\end{figure}

We try to understand the dynamics of \ours{} under different batch-size-sequence-length products and keep percentage while isolating the advantage of TTFT. We use the official script from vLLM for latency benchmarking. In Figure~\ref{fig:efficiency} and~\ref{fig:efficiency_lah} (with look-ahead), we highlight the TTFT speedup against the vanilla base model without \ours{}, which we produce by setting the maximum decoding step to be 1. As we can see, for both the 70B and 405B models, not only do we see more direct effects of \ours{} but also imply the increasing scaling along the sequence dimension. As the relative FLOPS ratio between the speculator and main model becomes larger, the overhead of speculation starts to become more negligible, which leads to more substantial improvement. In order to better understand the whole system, we include theoretical analysis of the overhead and the performance gains in Appendix~\ref{app:overhead}. 
