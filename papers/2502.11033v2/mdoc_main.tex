
% \doparttoc %
% \faketableofcontents %

\begin{abstract}
    Modern policy optimization methods roughly follow the policy mirror descent (PMD) algorithmic template, for which there are by now numerous theoretical convergence results.
    However, most of these either target tabular environments, or can be applied effectively only when the class of policies being optimized over satisfies strong closure conditions, which is typically not the case when working with parametric policy classes in large-scale environments. 
    In this work, we develop a theoretical framework for PMD for general policy classes where we replace the closure conditions with a strictly weaker variational gradient dominance assumption, and obtain upper bounds on the rate of convergence to the best-in-class policy. Our main result leverages a novel notion of smoothness with respect to a local norm induced by the occupancy measure of the current policy, and casts PMD as a particular instance of smooth non-convex optimization in non-Euclidean space.
\end{abstract}
\section{Introduction}
Modern policy optimization algorithms 
\citep{peters2006policy,peters2008reinforcement,lillicrap2015continuous, schulman2015trust, schulman2017proximal} operate by solving a sequence of stochastic optimization problems, each of which being roughly equivalent to:
\begin{align}\label{def:pmd_step}
    % k=1, \ldots K: \quad 
    \pi^{k+1}
    \gets 
    \argmin_{\pi\in \Pi}\E_{s \sim \mu^{k}}\sbr{
        \abr[b]{\widehat Q^{k}_s, \pi_s} 
        + \frac1\eta B(\pi_s, \pi_s^k)
    },
\end{align}
where $\mu^k$ is a state probability measure (typically related, or equal to, the occupancy measure of the current policy $\pi^k$) from which sampling is granted through interaction with the environment; $\widehat Q^k$ is an estimate of the action-value function of $\pi^k$, and $B$ is a distance-like function employed to regularize the update so as to not stray too far from $\pi^k$.
The solution to \cref{def:pmd_step} is usually produced by optimizing a parametric neural network model $\pi_\theta$ (known as the actor, or policy network) via multiple steps of stochastic gradient descent, and consequently, the policy class $\Pi$ is the set of policies representable by the model; $\Pi = \cbr{\pi_\theta \mid \theta \in \R^p}$, where $p$ denotes the number of parameters in the network.

Contemporary theoretical analyses of this algorithm \citep{shani2020adaptive,agarwal2021theory,xiao2022convergence,ju2022policy,zhan2023policy, yuan2023loglinear,alfano2023novel} all have their roots in the online Markov decision process (MDP) framework, and roughly build on decomposing \cref{def:pmd_step} state-wise and casting the problem as a collection of independent online mirror descent 
% \citep{nemirovskij1983problem,beck2003mirror} 
steps~\citep{even2009online}. The disadvantage of such an approach lies in the requirement that the update step be exact (or almost exact) in each state independently, effectively limiting the applicability of such analyses to policy classes that are \emph{complete}, (i.e., $\Pi = \Delta(\cA)^\cS$), or otherwise satisfy strong closure conditions.

Largely, papers that develop convergence upper bounds for algorithms following \cref{def:pmd_step}, commonly known as Policy Mirror Descent (PMD; \citealp{tomar2020mirror,xiao2022convergence,lan2023policy}), fall into two main categories. The first category includes studies that target the tabular setup (e.g., \citealp{geist2019theory,shani2020adaptive,agarwal2021theory,xiao2022convergence,johnson2023optimal,lan2023policy,zhan2023policy}), where no sampling distribution $\mu^k$ is involved (or it has no effect) and updates are performed in a per-state manner. The second category consists of papers that consider parametric policy classes (e.g., \citealp{agarwal2021theory,alfano2022linear,ju2022policy,yuan2023loglinear,alfano2023novel,xiong2024dual}) often building---either directly or indirectly---on the compatible function approximation framework~\citep{sutton1999policy}. 
As such, these works essentially assume that the update in \cref{def:pmd_step} remains ``close'' to the one that would have been performed over the complete policy class (see  \cref{sec:intro_discussion} for further discussion).
% 
This state of affairs is (at least partially) due to the fact that policy gradient methods in the general policy class setting are prone to local optima~\citep{bhandari2024global}, and as a result, structural assumptions are necessary to establish global optimality guarantees.

The present paper aims to establish \emph{best-in-class} convergence of PMD (\cref{def:pmd_step}) for general policy classes, relaxing the stringent closure conditions and assuming instead a \emph{variational gradient dominance} (VGD) condition \citep{bhandari2024global,agarwal2021theory,xiao2022convergence}.
It can be shown that a general form of closure conditions implies VGD and that the converse does not hold, hence it is a strict relaxation of the setup assumptions (see detailed discussion in \cref{sec:intro_discussion,sec:vgd_discussion}).
%
Our main result features a novel analysis technique that casts \cref{def:pmd_step} as a particular instance of smooth non-convex optimization in a non-Euclidean space, where the smoothness of the objective is w.r.t.~a \emph{local} norm induced by the current policy occupancy measure. Importantly, this approach leads to rates independent of the cardinality of the state space.
% 
In contrast, previous results that establish convergence of gradient based methods (though not of PMD; e.g., \citealp{agarwal2021theory,bhandari2024global,xiao2022convergence}) that are applicable in our setting, lead to bounds that depend on the size of the state-space, thus rendering them useful only in tabular setups.



\subsection{Main results}
We consider the problem of finding an (approximately) optimal policy in a
discounted MDP $\cM = (\cS, \cA, \P, r, \gamma, \rho_0)$ within a general policy class $\Pi\subset \Delta(\cA)^{\cS}$.
We assume the action set is finite 
$A\eqq |\cA|$, and denote the effective horizon by $H\eqq \frac{1}{1-\gamma}$.
Our goal is to minimize the value $V(\pi)$, defined as the long term discounted cost (we interpret $r\colon\cS \times \cA \to [0,1]$ as measuring regret, or cost).
Our central structural assumption, that replaces and relaxes specific closure conditions, is the following.
\begin{definition}[Variational Gradient Dominance]\label{def:vgd_mdp}
    We say that $\Pi$ satisfies a $(C_\star, \epsvgd)$-variational gradient dominance (VGD) condition w.r.t.~$\cM$, 
    if there exist constants $C_\star, \epsvgd>0$, such that for any policy $\pi\in \Pi$:
    \begin{align}\label{eq:M_vgd}
        V(\pi) - V^\star(\Pi) 
        \leq 
        C_\star \max_{\tilde \pi \in \Pi}\abr{\nabla V(\pi), \pi - \tilde \pi} + \epsvgd.
    \end{align}
\end{definition}
We note that any policy class satisfies the above conditions with some $C_\star \geq 1, \epsvgd \leq H$, and that the complete policy class is $\br{H\norm{\fraci{\mu^\star}{\rho_0}}_\infty, 0}$-VGD w.r.t.~any MDP (see \citealp{bhandari2024global,agarwal2021theory}, and \cref{lem:vgd_complete} for completeness).
Our main result is the following.
\begin{theorem*}[informal]
    Let $\Pi\subset \Delta(\cA)^\cS$ be convex and assume it satisfies $(C_\star, \epsvgd)$-VGD w.r.t.~$\cM$. 
    Suppose further that the actor and critic are approximately optimal up to some error $\epsstatM > 0$.
    Then, with well tuned $\varepsilon$-greedy exploration and learning rate $\eta$, we have that
    the PMD method (\cref{def:pmd_step}) converges as follows.
    With Euclidean regularization,
    %$V(\pi^k) - \min_{\pi^\star\in \Pi}V(\pi^\star)=$ %icml_edit
    \begin{align*}
            V(\pi^k) - \min_{\pi^\star\in \Pi}V(\pi^\star)=
            \bigO\br{
            \frac{C_\star^2 H^3 A^{3/2} }{k^{2/3}}
            + \br{C_\star + A H^2 k^{1/6}}\sqrt{\epsstatM}
            + \epsvgd},
        \end{align*}
    and with negative Entropy regularization, we have that 
    %$V(\pi^k) - \min_{\pi^\star\in \Pi}V(\pi^\star)=$ %icml_edit
        \begin{align*}
        V(\pi^k) - \min_{\pi^\star\in \Pi}V(\pi^\star)=
        \bigO\br{
                \frac{C_\star^2 H^3A^{3/2}  }{k^{2/7}}
                + \br{C_\star + A^2 H^3 k^{4/7}}\sqrt{\epsstatM} 
                + \epsvgd},
        \end{align*}
    where the big-O only suppresses constant numerical factors.
\end{theorem*}

To obtain our main result, our analysis casts
PMD as a proximal point algorithm 
% \citep{rockafellar1976monotone} 
in a non-Euclidean setting (see \citealp{teboulle2018simplified} for a review), where the proximal operator uses a regularizer that adapts to \emph{local} smoothness of the objective. As we demonstrate in \cref{lem:value_local_smoothness}, the approximation error of the linearization of the objective $V(\cdot)$ at $\pi^k$ can be bounded w.r.t.~the local norm $\norm{\cdot}_{L^2(\mu^k)}$; crucially, a norm according to which the decision set $\Pi$ has diameter independent of the cardinality of the state-space. 
This significantly deviates from the commonly used smoothness of the value function w.r.t.~the Euclidean norm \citep{agarwal2021theory}, which assigns a diameter of $|\cS|$ to $\Pi$, and therefore leads to rates that have merit only in tabular environments.


\subsection{Discussion: VGD vs.~Closure}
\label{sec:intro_discussion}
Our work establishes best-in-class convergence subject to the VGD condition presented in the previous section. This is a substantially different starting point than that of the prevalent closure conditions based on the compatible function approximation approach \citep{sutton1999policy} assumed in recent works on parametric policy classes \citep{agarwal2021theory,yuan2023loglinear,alfano2023novel,xiong2024dual}. 
The assumptions employed in these works fall into two main categories; The first and more general one is that of a bounded \emph{approximation} error \citep[e.g.,][]{alfano2023novel,yuan2023loglinear}, which essentially requires that the update step in \cref{def:pmd_step} be close (up to a small error) to the update that would have been performed over the complete policy class $\Pi_{\rm all} \eqq \Delta(\cA)^\cS$.
The second is that of bounded \emph{transfer} error \citep[e.g.,][]{agarwal2021theory,yuan2023loglinear}, which roughly requires that the update be accurate (up to a small error) when accuracy is measured over the optimal policy occupancy measure. This assumption is commonly employed in the specific log-linear policy class setup; to the best of our knowledge, there do not exist results that employ these conditions in a fully general policy class setting (\citealp{agarwal2021theory} consider a non-PMD method in a bounded transfer error setup where the policy class satisfies additional smoothness assumptions). 

The relation between closure and VGD is subtle, primarily because closure conditions are algorithm dependent. Typically, they relate to one or more of the following three elements; step-size range, action regularizer, and the particular algorithmic approach employed to solve \cref{def:pmd_step}.
At the same time, the VGD condition is algorithm independent, as it relates only to the policy class-MDP combination.
Nonetheless, as we discuss next, a general form of closure conditions for a large enough step-size range imply variational gradient dominance, effectively establishing closure $\Rightarrow$ VGD.
We further demonstrate that the converse does not hold; that there exist simple examples where the VGD condition holds whereas closure does not take place.


Before discussing the aforementioned arguments, we make two additional remarks. Closure conditions generally imply realizability, thus under this assumption convergence to the true optimal policy is possible. We do not assume realizability and therefore prove convergence to the optimal \emph{in-class} policy.
Finally, our results require convexity of the policy class $\Pi$, while there exist setups where closure conditions hold and convexity does not. Hence, strictly speaking, these assumptions are incomparable. However, in a general setting where the MDP does not satisfy specific structural assumptions, it seems improbable that closure conditions hold unless the policy class is approximately complete, and therefore satisfies the VGD condition.
For additional discussions we refer the reader to \cref{sec:vgd_discussion}.

\paragraph{Closure implies VGD.}
As we demonstrate in \cref{sec:closure_implies_vgd},
a general form of closure conditions  \citep{alfano2023novel} implies VGD with similar error floors (the $\epsvgd$ parameter) as those exhibited in convergence guarantees established subject to these conditions.
At a high level, for any convex action regularizer and any policy $\pi\in \Pi$, there exists a sufficiently large step-size so that the PMD step produces a policy that is an approximate greedification of $Q^\pi$. The quality of this approximation is governed by the closure conditions approximation error, hence, when this error is small it follows that the PMD step policy $\pi^+\in \Pi$ is a ``VGD witness'' that can be plugged into the RHS of \cref{eq:M_vgd} playing the role of $\tilde \pi$.
This is in fact an elaborate generalization of a similar claim made in \citet{bhandari2024global}, that closure to a policy improvement step implies VGD.

\paragraph{VGD does not imply closure.}
We present an example where the VGD condition holds but closure does not, and as a result existing analyses fail to establish convergence of PMD.
We consider the MDP depicted in \cref{fig:simple_mdp} 
 with the log-linear policy class $\Pi$ induced by the state-action feature vectors shown in the diagram. 
 For simplicity we assume there are no statistical errors in the execution of the algorithm ($\epsstatM=0$). In this example the value landscape is convex (in state-action space) over $\Pi$, and thus $\Pi$ is $(1,0)$-VGD and convergence of PMD follows by our main theorem:
\begin{align*}
        V(\pi^{K}) - &\min_{\pi^\star\in \Pi}V(\pi^\star)
        \xrightarrow[K\to \infty]{} 0.
    \end{align*}
At the same time, results based on closure imply convergence to an error floor that is larger than $H$.
% In particular, this includes results that relate to the  same algorithm\footnote{To be precise, our result relates to PMD with $\epsilon$-greedy exploration, though for the example in question convergence follows also without it.}; PMD instantiated with negative-entropy regularization.
For instance, by Theorem 1 of \citet{yuan2023loglinear}:
\begin{align*}
        V(\pi^{K}) - &\min_{\pi^\star\in \Pi}V(\pi^\star)
        \xrightarrow[K\to \infty]{}
        2H \nu_0 \sqrt{A C_0 \epsbiasM}
        \geq 10 H,
    \end{align*}
    where $\epsbiasM=\Omega(1)$, $\nu_0 \eqq H\norm{\frac{\mu^\star}{\rho_0}}_\infty$, and $C_0$ is a certain concentrability coefficient larger than $1$. Here, both the transfer error and approximation error are $\Omega(\epsbiasM)$.
We refer the reader to \cref{sec:vgd_not_imply_closure} for a  rigorous analysis of this example.
Recent papers such as \citet{alfano2023novel,xiong2024dual} accommodate more general policy parameterizations but still include
the log-linear setup as a special case (see discussion in \citealp{alfano2023novel} and \cref{sec:dual_pc}). The error floor in their results is also larger than $H$ for the example in question for exactly the same reasons; their results depend on the approximation error, which for this example as mentioned behaves the same as the transfer error.
\begin{figure}[ht!]
    \centering
\begin{tikzpicture}[
    state/.style={circle, draw, minimum size=2cm, node distance=2cm},
    every node/.style={font=\sffamily},
    >={Stealth[round]}
]

% States
\node[state] (S0) at (0, 0) {$S_0$};
\node[state] (S1) at (-2, -3) {$S_1$};
\node[state] (S2) at (2, -3) {$S_2$};

% Arrows
\draw[->, color=orange] (S0) -- 
		node[pos=0, anchor=south, font=\scriptsize, shift={(0.15, -0.1)}] 
		{\tiny $\actA$} 
	(S1) node[midway, above] {};
\draw[->, color=violet] (S0) -- 
		node[pos=0, anchor=south, font=\scriptsize, shift={(-0.1, -0.1)}] 
		{\tiny $\actB$} 
	(S2) node[midway, left] {};
	
	
\draw[->, color=violet] (S1) to[bend right] 
	node[pos=0, anchor=north east, font=\scriptsize, shift={(0.2, 0.15)}] 
		{\tiny $\actB$}
	(S0) node[midway, above] {};
\draw[->, color=orange] (S2) to[bend left] 
		node[pos=0, anchor=north west, font=\scriptsize, shift={(-0.2, 0.1)}] 
		{\tiny $\actA$}
	(S0) node[midway, below] {};

\draw[->, very thick, color=orange] (S1) 
	to[bend left] node[midway, left] {1} 
		node[pos=0, anchor=north, font=\scriptsize, shift={(0, 0.05)}] 
		{\tiny $\actA$}
	(S0) node[midway, above] {};

\draw[->, very thick, color=violet] (S2) 
	to[bend right] node[midway, right] {$1$}
		node[pos=0, anchor=north, font=\scriptsize, shift={(0, 0.05)}] 
		{\tiny $\actB$}
	(S0) node[midway, below] {};
\end{tikzpicture}
    \caption{A simple MDP with a convex value landscape. Each action represented by a (feature-vector, edge) pair leads deterministically to the state at the other end of the edge. The two outer bold edges labeled $1$ inflict a cost of $1$, the others have cost $0$.}
    \label{fig:simple_mdp}
\end{figure}

\subsection{Additional Related work}
\label{sec:related_work}
\paragraph{PMD with non-tabular policy classes.} Most closely related to our work are papers that study convergence of PMD in setups where the policy class is given by function approximators \citep{vaswani22ageneral,ju2022policy,grudzien2022mirror,alfano2023novel,xiong2024dual}. 
The motivation of \citet{alfano2023novel,xiong2024dual} is somewhat related to ours but they address a different aspect of the problem in question. These works focus on the approximation errors in the update step (thus essentially assuming closure) and propose algorithmic mechanisms to ensure it is small, but obtain meaningful upper bounds only when it is indeed small w.r.t.~the exact steps over the complete policy class (as discussed in the previous section). 
There is a long line of works on parametric policy classes and specific instantiations of PMD such as the Natural Policy Gradient (NPG; \citealp{kakade2001natural}); which is the focus of, e.g., \citet{alfano2022linear,yuan2023loglinear,cayci2024convergence} as well as \citet{agarwal2021theory}. Many works also study convergence dynamics induced by particular policy classes, e.g., \citet{liu2019neural,wang2020neural,liu2020improved}; we refer the reader to \citet{alfano2023novel} for an excellent and more detailed account of these works.

Several prior works have made the observation that PMD is a mirror descent step on the linearization of the value function with a dynamically weighted regularization term \citep{shani2020adaptive,tomar2020mirror,vaswani22ageneral,xiao2022convergence}, which is the starting point of our work. In particular, this perspective is the focus of \citet{vaswani22ageneral}; however this work did not establish any convergence guarantees.

\paragraph{PMD in the tabular setting.}
The modern analysis approach for PMD in the generic (agnostic to the regularizer) tabular setup is due to \citet{xiao2022convergence}. Additional works that study the tabular setup include \citet{geist2019theory,lan2023policy,johnson2023optimal,zhan2023policy}.
As in the function approximation case, many works study convergence of the prototypical PMD instantiation; the NPG or its derivatives TRPO \citep{schulman2015trust} and PPO \citep{schulman2017proximal} in tabular or softmax-tabular settings, e.g., \citet{agarwal2021theory,shani2020adaptive,cen2022fast,bhandari2021linear, khodadadian2021linear, khodadadian2022linear}.



\paragraph{Policy Gradients in parameter space.} 
There is a rich line of work into policy gradient algorithms that take gradient steps \emph{in parameter space}, both in the tabular and non-tabular setups \citep{zhang2020global,mei2020global, mei2021leveraging,yuan2022general,mu2024second}. This class of algorithms are a special case of on-policy PMD only in the case of the direct parametrization, but are not PMD algorithms in general. Most of the results in the case of non-tabular, generic parameterizations characterize convergence in terms of conditions on the parametric representation. We refer the reader to \citet{yuan2022general} for further review.

\paragraph{Bregman proximal point methods.}
As mentioned, our analysis builds on realizing PMD as an instance of a Bregman proximal point algorithm --- roughly, this is a proximal point algorithm \cite{rockafellar1976monotone} in a non-Euclidean setting (see \citet{teboulle2018simplified} for a review).
There are numerous studies that investigate non-Euclidean proximal point methods for both convex and non-convex objective functions
(e.g., \citealp{tseng2010approximation,ghadimi2016mini,bauschke2017descent,lu2018relatively,zhang2018convergence,fatkhullin2024taming}; see also \citealp{beck2017first})
, although none of them accommodate the particular setup that PMD fits into (see \cref{sec:opt_analysis} for details).
Our analysis for the proximal point method presented in \cref{sec:opt_main} is mostly inspired by the work of \citet{xiao2022convergence}; specifically, their upper bounds for projected gradient descent, where they apply a proximal point analysis in the euclidean setting. 


\section{Preliminaries}
\paragraph{Discounted MDPs.}
A discounted MDP $\cM$ is defined by a tuple
	$\cM = (\cS, \cA, \P, r, \gamma, \rho_0)$,
where $\cS$ denotes the state-space, $\cA$ the action set, $\P\colon \cS \times \cA \to \Delta(\cS)$ the transition dynamics, $r \colon \cS \times \cA \to [0, 1]$ the reward function, $0<\gamma< 1$ the discount factor, and $\rho_0\in \Delta(\cS)$ the initial state distribution.
For notational convenience, for $s,a\in \cS\times\cA$ we let $\P_{s, a} \eqq \P(\cdot \mid s, a) \in \Delta(\cS)$ denote the next state probability measure.

We assume the action set is finite with $A\eqq |\cA|$, and identify $\R^A$ with $\R^\cA$. We additionally assume, for clarity of exposition and in favor of simplified technical arguments, that the state space is finite with $S\eqq |\cS|$, and identify $\R^\cS$ with $\R^S$.
We emphasize that all our arguments may be extended to the infinite state-space setting with additional technical work.
An agent interacting with the MDP is modeled by a policy $\pi \colon \cS \to \Delta(\cA)$, for which we let $\pi_s \in \Delta(\cA)\subset\R^A$ denote the action probability vector at $s$ and $\pi_{s, a}\in [0,1]$ denote the probability of taking action $a$ at $s$. We denote the \emph{value} of $\pi$ when starting from a state $s\in \cS$ by $V_s(\pi)$:
\begin{align*}
    V_s(\pi) \eqq \E\sbr{\sum_{t=0}^\infty \gamma^t r(s_t, a_t) \mid s_0 = s, \pi},
\end{align*}
and more generally for any $\rho\in \Delta(\cS)$,
$V_\rho(\pi) \eqq \E_{s \sim \rho}V_{s}(\pi)$. When the subscript is omitted, $V(\pi)$ denotes value of $\pi$ when starting from the initial state distribution $\rho_0$:
\begin{align*}
    V(\pi) \eqq V_{\rho_0}(\pi) =  \E\sbr{\sum_{t=0}^\infty \gamma^t r(s_t, a_t) \mid s_0 \sim \rho_0, \pi}.
\end{align*}
For any state action pair $s,a\in \cS\times \cA$, the action-value function of $\pi$, or $Q$-function, measures the value of $\pi$ when starting from $s$, taking action $a$, and then following $\pi$ for the reset of the interaction:
\begin{align*}
    Q^\pi_{s, a} \eqq \E\sbr{\sum_{t=0}^\infty \gamma^t r(s_t, a_t) \mid s_0 = s, a_0=a, \pi}
\end{align*}
We further denote the discounted state-occupancy measure of $\pi$ induced by any start state distribution $\rho\in \Delta(\cS)$ by $\mu^\pi_\rho$:
\begin{align*}
    \mu^\pi_{\rho}(s) \eqq 
    \br{1-\gamma}\sum_{t=0}^\infty \gamma^t \Pr(s_t = s \mid s_0 \sim \rho, \pi).
\end{align*}
It is easily verified that $\mu^\pi\in \Delta(\cS)$ is indeed a state probability measure.
In the sake of brevity, we take the MDP true start state distribution $\rho_0$ as the default in case one is not specified:
\begin{align}
    \mu^\pi \eqq \mu^\pi_{\rho_0}.
\end{align}


\paragraph{Learning objective.}
In the conventional formulation of MDPs, the objective is to maximize the discounted total reward, i.e., $\max_\pi V(\pi)$. In this paper, we follow \citet{xiao2022convergence} and adopt a minimization formulation in order to better align with
conventions in the optimization literature. To this end, we regard each $r(s, a) \in [0, 1]$ as a value measuring regret, or cost, rather than reward. Given any reward function $r$, we may reset $r(s, a) \gets 1 - r(s, a)$ for all $s,a\in \cS \times \cA$ to transform it into a regret function.
With this in mind, we consider the problem of finding an approximately optimal policy within a given policy class $\Pi\subset \Delta(\cA)^\cS$:
\begin{align}\label{def:rl_opt_problem}
    \argmin_{\pi \in \Pi} V(\pi).
\end{align}
To avoid ambiguity, we denote the optimal value attainable by an in-class policy (a solution to \cref{def:rl_opt_problem}) by $\VstarPi$, and the optimal value attainable by any policy by $V^\star$:
\begin{align}
    \VstarPi \eqq \argmin_{\pi^\star \in \Pi}V(\pi^\star);
    % \; %icml_edit
    \quad
    V^\star \eqq \argmin_{\pi^\star \in \Delta(\cA)^\cS}V(\pi^\star).
\end{align}
We note that we do not make any explicit structural assumptions about $\cM$. We will however make some assumptions about the policy class $\Pi$, which will be made clear in the statements of our theorems.

\subsection{Problem Setup}
In this work, we focus on the PMD method \cref{alg:pmd} for solving \cref{def:rl_opt_problem} in the case that the policy class is non-complete, $\Pi\neq \Delta(\cA)^\cS$.
\begin{algorithm}[tb]
   \caption{Policy Mirror Descent (on-policy)}
   \label{alg:pmd}
\begin{algorithmic}
   \STATE {\bfseries Input:} learning rate $\eta > 0$, regularizer $R\colon\R^\cA \to \R$
   \STATE Initialize $\pi^1 \in \Pi$
   \FOR{$k=1$ {\bfseries to} $K$}
   \STATE Set $\mu^k \eqq \mu^{\pi^k}$; $\widehat Q^k \eqq \widehat Q^{\pi^k}$.
   \STATE 
   \begin{aligni*}
        \pi^{k+1}
        \gets 
        \argmin\limits_{\pi\in \Pi}\E_{s\sim \mu^k}\sbr{
            \abr{\widehat Q^{k}_s, \pi_s} 
            + \frac1\eta B_R(\pi_s, \pi_s^k)
        }
   \end{aligni*}
   \ENDFOR
\end{algorithmic}
\end{algorithm}
In each iteration, PMD solves a stochastic optimization sub-problem formed by an estimate of the current policy $Q$-function and a Bregman divergence term which is defined below.
\begin{definition}[Bregman divergence]\label{def:bregman_divergenvce}
    Given a convex differentiable regularizer $R \colon \R^\cA \to \R$, the Bregman divergence w.r.t.~$R$ is:
    \begin{align*}
        B_R(u, v) \eqq R(u) - R(v) - \abr{\nabla R(v), u -v}.
    \end{align*}
\end{definition}
Throughout, we make the following assumptions regarding the solutions to the sub-problems and the $Q$-function estimates \cref{alg:pmd}. 
\begin{assumption}[Sub-problem optimization oracle]
\label{assm:opt_oracle}
    We assume that for all $k$, $\pi^{k+1}$ is approximately optimal, in the sense that constrained optimality conditions hold up to error $\epsactM$:
    \begin{align*}
        \forall \pi \in \Pi, 
        \abr{\nabla \phi_k(\pi^{k+1}), \pi - \pi^{k+1}} \geq -\epsactM,
    \end{align*}
    where
    \begin{aligni*}
        \phi_k(\pi) \eqq \E_{s\sim \mu^k}\sbr{
            \abr{\widehat Q^{k}_s, \pi_s} 
            + \frac1\eta B_R(\pi_s, \pi_s^k)}.
    \end{aligni*}
\end{assumption}
\begin{assumption}[Q-function oracle]
\label{assm:Q_oracle}
    We assume that for all $\pi$, 
    \begin{align*}
        \E_{s \sim \mu^\pi}\sbr{\norm[b]{\widehat Q_{s}^\pi - Q_{s}^\pi}_2^2} \leq \epscritM.
    \end{align*}
\end{assumption}
    We remark that our results can be easily adapted to 
    somewhat weaker conditions on the critic error; we defer the discussion to \cref{sec:discuss_critic_error}.
    
\paragraph{Additional notation.}
Given a state probability measure $\mu\in \Delta(\cS)$ and an action space norm $\norm{\cdot}_\circ\colon \R^A \to \R$, we define the induced state-action weighted $L^p$ norm $\norm{\cdot}_{L^p(\mu), \circ}\colon \R^{SA}\to \R$ as follows:
\begin{align*}
	\norm{u}_{L^p(\mu), \circ}
	&\eqq \br{\E_{s \sim \mu}\norm{u_s}_\circ^p}^{1/p}.
\end{align*}
For any norm $\norm{\cdot}$, we let $\norm{\cdot}^*$ denote its dual. When discussing a generic norm and there is no risk of confusion, we may use $\norm{\cdot}_*$ to refer to its dual.
We repeat the following notation that is used throughout the paper for convenience:
\begin{align*}
    \mu^\pi \eqq \mu^\pi_{\rho_0}, \quad 
    S \eqq |\cS|, \quad
    A \eqq |\cA|, \quad
    H \eqq \frac{1}{1-\gamma}.
\end{align*}

\subsection{Optimization preliminaries}
We proceed with several basic definitions before concluding the setup.
\begin{definition}[Lipschitz Gradient]\label{def:Lipschitz_gradient}
We say a function $h\colon\Omega \to \R$, $\Omega\subseteq \R^d$ has an $L$-Lipschitz gradient or is $L$-smooth w.r.t.~a norm $\norm{\cdot}$ if for all $x,y\in \Omega$:
\begin{align*}
    \norm{\nabla h(x) - \nabla h(y)}_* \leq L\norm{x - y}.
\end{align*}
\end{definition}

\begin{definition}[Gradient Dominance]\label{def:gradient_dominance}
    We say $f\colon \cX \to \R$ satisfies the variational gradient dominance condition with parameters $(C_\star, \delta)$, or that $f$ is $(C_\star, \delta)$-VGD, if
    here exist constants $C_\star, \delta>0$, such that for any $x \in \cX$, it holds that:
    \begin{align*}
        f(x) - \argmin_{x^\star\in \cX}f(x^\star)
        \leq 
        C_\star \max_{\tilde x \in \cX}\abr{\nabla f(x), x - \tilde x} + \delta.
    \end{align*}
\end{definition}

\begin{definition}[Local Norm]\label{def:local_norm}
    We define a \emph{local} norm over a set $\cX\subseteq \R^d$ by a mapping $x \mapsto \norm{\cdot}_x$ such that $\norm{\cdot}_x$ is a norm for all $x\in \cX$.
    We may denote a local norm by $\norm{\cdot}_{(\cdot)}$ or by $x \mapsto \norm{\cdot}_x$.
\end{definition}

\begin{definition}[Local Smoothness]\label{def:local_smoothness}
    We say $f\colon \cX \to \R$ is
    $\beta$-\emph{locally} smooth w.r.t.~a local norm $x \mapsto \norm{\cdot}_x$ if for all $x, y\in \cX$:
    \begin{align*}
    \av{f(y) - f(x) - \abr{\nabla f(x), y - x}} \leq 
    \frac\beta2\norm{y-x}_x^2 .
    \end{align*}
\end{definition}
% From the above we also have that a $\beta$-smooth function $f$ admits:
% \begin{align*}
%     \forall y, x\in \cX, \quad
%     \abr{\nabla f(x) - \nabla f (y), x - y} 
%     % &= 
%     % \abr{\nabla f(x), x - y} 
%     % + 
%     % \abr{\nabla f(y), y - x} 
%     % \\
%     &\leq \beta\norm{y-x}^2.
% \end{align*}


\section{Best-in-class Convergence of Policy Mirror Descent}
In this section, we present our main results which establish convergence rates for the PMD method in the non-complete class setting we consider. 
Our main theorem, given below, provides convergence rates for two classic instantiations of PMD; with Euclidean regularization and negative entropy regularization.
Our results require that $\epsilon$-greedy exploration be incorporated into the policy class.
To that end, let $\Pi^\epsilon$ denote the policy class obtained by adding $\epsilon$-greedy exploration to $\Pi$:
\begin{align*}
    \Pi^\epsilon \eqq \cbr{(1-\epsilon)\pi + \epsilon u \mid \pi \in \Pi},
    \text{ where }u_{s, a} \equiv 1/A.
\end{align*}
We have the following.
\begin{theorem}\label{thm:pmd_main}
    Let $\Pi\subset \Delta(\cA)^\cS$ be convex and assume it is $(C_\star, \epsvgd)$-VGD w.r.t.~$\cM$. 
    Consider the on-policy PMD method \cref{alg:pmd}  when run over $\Pi^\epsexpl$.
    Then, assuming $\epsactM + \epscritM \leq \epsstatM$, and with proper tuning of $\eta, \epsexpl$, it holds that:
    
    % \begin{enumerate}[label=\roman*., leftmargin=0.4cm] %icml_edit
    \begin{enumerate}[label=\roman*.] 
        \item If $R(p) = \frac12\norm{p}_2^2$ is the Euclidean action-regularizer, we have  
        % $V(\pi^K) - \VstarPi=$ %icml_edit
        \begin{align*}
            V(\pi^K) - \VstarPi=
            \bigO\br{
            \frac{C_\star^2 A^{3/2} H^3}{K^{2/3}}
            + \br{C_\star + A H^2 K^{1/6}}\sqrt{\epsstatM}
            + \epsvgd}
        \end{align*}

        \item If $R(p) = \sum_i p_i\log p_i$ is the negative entropy action-regularizer, we have  
        % $V(\pi^K) - \VstarPi=$ %icml_edit
        \begin{align*}
            V(\pi^K) - \VstarPi=
            \bigO\br{
                \frac{C_\star^2 A^{3/2} H^3 }{K^{2/7}}
                + \br{C_\star + A^2 H^3 K^{4/7}}\sqrt{\epsstatM} 
                + \epsvgd}.
        \end{align*}
        
    \end{enumerate}
    In both cases, 
    big-O notation suppresses only constant factors.
\end{theorem}
To our knowledge, \cref{thm:pmd_main} is the first result to establish best-in-class convergence (at any rate) of PMD without closure conditions. Two additional comments are in order:
(1) Our current analysis technique requires the action regularizer to be smooth. This is also the source of the degraded rate in the negative entropy case.
(2) The greedy exploration stems from the smoothness parameter we establish for the value function, and leads to worse rates in the Euclidean case (for negative entropy, it actually implies smoothness of the regularizer, though this is not the primary reason for which it is introduced). We discuss this point further in \cref{sec:value_smooth_main}.

\paragraph{Analysis overview.}
The analysis leading up to \cref{thm:pmd_main} builds on casting PMD as an instance of a Bregman proximal point (or equivalently, a mirror descent) algorithm. This follows by demonstrating PMD proceeds by optimizing subproblems formed by linear approximations of the value function and a proximity term that adapts to \emph{local} smoothness of the objective, as measured by the norm induced by the current policy occupancy measure.


In fact, it has already been previously observed \citep[e.g.,][]{shani2020adaptive, xiao2022convergence} that the on-policy PMD update step is completely equivalent to a mirror descent step w.r.t.~the value function gradient equipped with a dynamically weighted proximity term. For any two policies $\pi$ and $\pi^k$, by the policy gradient theorem (\citealp{sutton1999policy}, see also \cref{lem:value_pg} in \cref{sec:aux_lemmas}):
\begin{align}\label{eq:pmd_omd_equiv}
    \E_{s \sim \mu^{k}}\Big[
        &\abr{Q^{k}_s, \pi_s} 
        + \frac1\eta B_R(\pi_s, \pi_s^k)
    \Big]
    % \notag \\%icml_edit
    % &
    =
        \abr{\nabla V(\pi^k), \pi} 
        + \frac1\eta B_{\pi^k}(\pi, \pi^k),
\end{align}
where we denote $\mu^k \eqq \mu^{\pi^k}, Q^k \eqq Q^{\pi^k}$, and $B_{\pi^k}(u, v) \eqq \E_{s \sim \mu^k}B_R(u_s, v_s)$. 
However, these prior observations did not yield new convergence results, as the algorithm in question significantly deviates from a standard instantiation of mirror descent; a priori, it is unclear how the regularizer associated with $B_{\pi^k}$ relates to the objective in optimization terms.

The high level components of our analysis are outlined next. In \cref{sec:value_smooth_main} we establish local smoothness of the value function (\cref{lem:value_local_smoothness}), which is the key element in establishing convergence of PMD through a proximal point algorithm perspective.
Then, in \cref{sec:opt_main} we introduce the optimization setup that accommodates proximal point methods that adapt to local smoothness of the objective, and present the  convergence guarantees for this class of algorithms. Finally, we return to prove \cref{thm:pmd_main} in \cref{sec:proof:thm:pmd_main}, where we apply both \cref{lem:value_local_smoothness} and the result of \cref{sec:opt_main} to establish convergence of PMD.




\subsection{Local smoothness of the value function}
\label{sec:value_smooth_main}
The principal element of our approach builds on smoothness of the value function w.r.t.~the local norm induced by the occupancy measure of the policy at which we take the linear approximation, given by the below lemma. We defer the proof to \cref{sec:proof:lem:value_local_smoothness}.
\begin{lemma}\label{lem:value_local_smoothness}
    Let $\pi\colon \cS \to \Delta(\cA)$ be any policy such that $\epsilon\eqq \min_{s, a}\cbr{\pi_{sa}} > 0$.
    Then, for any $\tilde \pi \in \cS \to \Delta(\cA)$, we have:
    \begin{align*}
        &\av{V(\tilde \pi) - V(\pi) - \abr{\nabla V(\pi), \tilde \pi - \pi}}
        % \\ &\leq %icml_edit
        \leq 
        \min\cbr{
        \frac{ H^3}{\sqrt \epsilon}
            \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}^2,
        \frac{ A H^3}{\sqrt \epsilon}
            \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 2}^2
        }.
    \end{align*}
\end{lemma}
It is instructive to consider
\cref{lem:value_local_smoothness} in the context of the more standard non-weighted $L^2$ smoothness property established in \citet{agarwal2021theory}.
% \begin{itemize}[topsep=0pt, leftmargin=0.3cm] %icml_edit
\begin{itemize}
    \item \textbf{Dependence on $\boldsymbol S$:} The standard $L^2$ smoothness leads to rates that scale with $\norm{\pi^1 - \pi^\star}_2$, which scales with $S$ in general.  Indeed, prior works that exploit smoothness of the value function (e.g., \citealp{agarwal2021theory,xiao2022convergence}) derive bounds for PGD (i.e., mirror descent with non-local, euclidean regularization) that do in fact hold in the  setting we consider here, but inevitably lead to convergence rates that scale with the cardinality of the state-space. This is while the diameter assigned to the decision set $\Pi$ by $\norm{\cdot}_{L^2(\mu^\pi), \circ}$, for any $\pi$, depends only on the diameter assigned to $\Delta(\cA)$ by $\norm{\cdot}_\circ$, and thus is independent of $S$.
    \item \textbf{Relation to PMD:} The standard $L^2$ smoothness does not naturally integrate with the PMD framework, and leads to algorithms (such as vanilla projected gradient descent) where the update step cannot be framed as a solution to a stochastic optimization problem induced by some policy occupancy measure. As such, these do not admit a formulation that is easily implemented in practical applications.
    \item \textbf{Smoothness parameter:} The smoothness parameter in \cref{lem:value_local_smoothness} depends on the minimum action probability assigned by the policy at which we linearize the value function (and as we discuss in \cref{sec:discuss_value_local_smoothness}, this is not an artifact of our analysis). A simple resolution for this is given by adding $\epsilon$-greedy exploration. Notably, the relatively large $\bigO(1/\sqrt\epsilon)$ smoothness constant ultimately leads to a rate that is worse than the $\bigO(1/K)$ achievable with the standard $L^2$ smoothness (but that crucially, does not scale with $S$).
\end{itemize}


\subsection{Digression: Constrained non-convex optimization for locally smooth objectives}
\label{sec:opt_main}
In this section, we consider the constrained optimization problem:
\begin{align}\label{opt:objective}
	\min_{x\in \cX} f(x),
\end{align}
where the decision set $\cX\subseteq \R^d$ is convex and endowed with a local norm $x \mapsto \norm{\cdot}_x$ (see \cref{def:local_norm}), and $f$ is differentiable over an open domain that contains $\cX$. We assume access to the objective is granted through an approximate first order oracle, as defined next.
\begin{assumption}\label{assm:opt_grad_oracle}
We have first order access to $f$ through an $\epsgrad$-approximate gradient oracle; For all $x\in \cX$, we have \begin{align*}
\norm{\hgrad{f}{x} - \nabla f(x)}_x^* \leq \epsgrad \leq 1.
\end{align*}
\end{assumption}
\cref{thm:opt_main} given below establishes convergence rates for the algorithm we describe next.
Given an initialization $x_1\in \cX$, learning rate $\eta > 0$, and local regularizer $R_x\colon \R^d \to \R$ for all $x\in \cX$, iterate for $k=1, \ldots, K:$
\begin{align}\label{eq:alg_opt_omd}
    &x_{k+1} = \argmin_{y\in \cX} 
    		\cbr{\abr{\hgrad{f}{x}, y} + \frac1\eta B_{R_x}(y, x)} .
\end{align}
The above algorithm can be viewed as either a mirror descent algorithm \citep{nemirovskij1983problem,beck2003mirror}  or a proximal point algorithm \citep{rockafellar1976monotone} in a non-Euclidean setup (see \citealp{teboulle2018simplified} for a review), where the non-smooth term is the decision set indicator function.
Our analysis (detailed in \cref{sec:opt_analysis}) hinges on a descent property of the algorithm, thus naturally takes the proximal point perspective. We prove the following.
\begin{theorem}\label{thm:opt_main}
    Suppose that $f$ is $(C_\star, \epsvgd)$-VGD as per \cref{def:gradient_dominance}, and that $f^\star \eqq \min_{x\in \cX}f(x) > -\infty$. 
    Assume further that:
    \begin{enumerate}[label=(\roman*)]
        \item The local regularizer $R_x$ is $1$-strongly convex and has an $L$-Lipschitz gradient w.r.t.~$\norm{\cdot}_x$ for all $x\in \cX$.
        
        \item For all $x\in \cX$, 
        $\max_{u,v\in \cX}\norm{u - v}_x\leq D$, and
        $\norm{\nabla f(x)}^*_x \leq M$.
        
        \item $f$ is $\beta$-locally smooth w.r.t.~$x \mapsto \norm{\cdot}_{x}$. 
    \end{enumerate}
    Then, assuming $x^{k+1}$ are $\epsopt$-approximately optimal (in the same sense of \cref{assm:opt_oracle}),
    the proximal point algorithm \cref{eq:alg_opt_omd}
	has the following guarantee when $\eta\leq 1/(2\beta)$:
    %icml_edit
 %    \begin{align*}
	% 	f(x_{K+1}) - f^\star 
	% 	&= O\br{\frac{C_\star^2 L^2 c_1^2}{\eta K}
 %            + \cE_{\rm err}
 %            + \epsvgd
 %            }
 %        \end{align*}
	% where $c_1 \eqq D + \eta M$ and
 %    \begin{align*}
 %        \cE_{\rm err} \eqq \br{C_\star D + c_1 L^2}\epsgrad
 %        + C_\star \epsopt 
 %        + c_1 L \sqrt{\epsopt/\eta}.
 %    \end{align*}
    \begin{align*}
		f(x_{K+1}) - f^\star 
		&= O\br{\frac{C_\star^2 L^2 c_1^2}{\eta K}
            + \br{C_\star D + c_1 L^2}\epsgrad
                + C_\star \epsopt 
                + c_1 L \sqrt{\epsopt/\eta}
            + \epsvgd
            }
        \end{align*}
	where $c_1 \eqq D + \eta M$.
    
    
\end{theorem}
The proof of \cref{thm:opt_main} as well as additional technical details for this section are provided in 
\cref{sec:opt_analysis}.
\subsection{Proof of main result}
\label{sec:proof_main}
To prove our main result, we begin with a lemma that essentially ``maps'' the PMD setup into the optimization framework of \cref{sec:opt_main}. The proof consists of showing that the appropriate assumptions on actor, critic, and action regularizer translate to the conditions of \cref{thm:opt_main} for locally smooth optimization.
\begin{lemma}\label{lem:pmd_main}
    Let $\Pi$ be a convex policy class that is $\br{C_\star, \epsvgd}$-VGD w.r.t.~the MDP $\cM$.
    Consider the on-policy PMD method \cref{alg:pmd}, and assume that the following conditions hold:
    \begin{enumerate}[label=(\roman*)]
        \item \label{thmassm:pmd_main_R} $R\colon \R^{\cA} \to \R$ is $1$-strongly convex and has an $L$-Lipschitz gradient w.r.t.~an action-space norm $\norm{\cdot}_\circ$.
        
        \item \label{thmassm:pmd_main_DM} $\max_{p,q\in \Delta(\cA)}\norm{p  - q}_\circ\leq D$, and $\norm{Q_s^\pi}_\circ^*\leq M$ for all $s \in \cS, \pi\in \Pi$.
        
        \item \label{thmassm:pmd_main_smoothness} The value function is $\beta$-locally smooth over $\Pi$ w.r.t.~the local norm $\norm{\cdot}_\pi \eqq \norm{\cdot}_{L^2(\mu^\pi), \circ}$.
    \end{enumerate}
    Then, we have the following guarantee:
    \begin{align*}
        V(\pi^{K}) - \VstarPi
        = \bigO\br{
        \frac{C_\star^2 L^2 c_1^2}{\eta K}
        + \cE_{\rm stat}
        + \epsvgd
        }
    \end{align*}
    where $c_1 \eqq D + \eta M$, and
    \begin{align*}
        \cE_{\rm stat} = \br{C_\star D + c_1 L^2}\sqrt \epscritM
        + C_\star \epsactM
        + c_1 L \sqrt{\epsactM/\eta}.
    \end{align*}
\end{lemma}
\begin{proof}[\unskip\nopunct]
    For $\mu\in \R^S, Q\in \R^{SA}$, we define the state to state-action element-wise product $\mu \circ Q \in \R^{SA}$ by 
\begin{aligni*}
	\br{\mu \circ Q}_{s, a} \eqq \mu(s)Q_{s, a}.
\end{aligni*}
        Observe that for all $k$, it holds that
        \begin{align*}
            \E_{s \sim \mu^{k}}\Big[
                \abr{\widehat Q^{k}_s, \pi_s}& 
                + \frac1\eta B_R(\pi_s, \pi_s^k)
            \Big]
            % \\&= %icml_edit
            =
            \abr{\hgrad{V}{\pi^k}, \pi} 
            + \frac1\eta B_{\pi^k}(\pi, \pi^k),
        \end{align*}
        with:
        \begin{aligni*}
        B_{\pi^k}(\pi, \tilde \pi) \eqq \E_{s\sim \mu^k} B_R(\pi_s, \tilde \pi_s),\;
        \hgrad{V}{\pi} \eqq \mu^\pi \circ \widehat Q^\pi.
        \end{aligni*}
        Next, we demonstrate PMD is an instance of the optimization algorithm \cref{eq:alg_opt_omd}, and verify that all of the conditions in \cref{thm:opt_main} hold w.r.t.~the local norm $\pi \mapsto \norm{\cdot}_{L^2(\mu^\pi), \circ}$.
        First, to see that the gradient error is bounded by $\sqrt{\epscritM}$, observe:
        %icml_edit
        % \begin{align*}
        %     \Big\Vert\hgrad{V}{\pi}
        %     - \nabla V(\pi)\Big\Vert_{L^2(\mu^\pi), \circ}^*
        %     &=
        %     \norm{\mu^\pi \circ \br{\widehat Q^\pi - Q^\pi}}_{L^2(\mu^\pi), \circ}^*
        %     \\
        %     &= 
        %     \sqrt{\E_{s \sim \mu^\pi}\br{\norm{\widehat Q^\pi - Q^\pi}_\circ^*}^2}
        %     \\
        %     &\leq \sqrt{\epscritM},
        % \end{align*}
        \begin{align*}
            \Big\Vert\hgrad{V}{\pi}
            - \nabla V(\pi)\Big\Vert_{L^2(\mu^\pi), \circ}^*
            =
            \norm{\mu^\pi \circ \br{\widehat Q^\pi - Q^\pi}}_{L^2(\mu^\pi), \circ}^*
            = 
            \sqrt{\E_{s \sim \mu^\pi}\br{\norm{\widehat Q^\pi - Q^\pi}_\circ^*}^2}
            \leq \sqrt{\epscritM},
        \end{align*}
        where second inequality follows from \cref{lem:wnorm_dual_mu} and the inequality from \cref{assm:Q_oracle}.
        Further:
        % \begin{enumerate}[leftmargin=0.5cm] %icml_edit
        \begin{enumerate} 
            \item By a simple relation (\cref{lem:reg_transform}) between $R$ and the state-action it regularizer it induces defined below,
        \begin{align*}
            R_{\pi^k}(\pi)\eqq \E_{s \sim \mu^k}R(\pi_s),
        \end{align*}
        we have that $B_{\pi^k}(\cdot, \cdot)$ is the Bregman divergence of $R_{\pi^k}$, and further using
        \ref{thmassm:pmd_main_R} that 
        $R_{\pi^k}$ is $1$-strongly convex and has an $L$-Lipschitz gradient w.r.t.~$\norm{\cdot}_{L^2(\mu^k), \circ}$.
            \item For all $\pi, \pi', \tilde \pi$, by \ref{thmassm:pmd_main_DM},
        \begin{align*}
            \norm{\pi' - \tilde \pi}_{L^2(\mu^\pi), \circ}
            =\sqrt{\E_{s\sim \mu^\pi}\norm{\pi'_s - \tilde \pi_s}^2}
            \leq D.
        \end{align*}
        In addition by \ref{thmassm:pmd_main_DM} and the dual norm expression (\cref{lem:wnorm_dual_mu}), for any $\pi$:
        %icml_edit
        % \begin{align*}
        %     \norm{\nabla V(\pi)}_{L^2(\mu^\pi), \circ}^*
        %     &=
        %     \norm{\mu^\pi \circ Q^\pi}_{L^2(\mu^\pi), \circ}^*
        %     \\
        %     &=
        %     \sqrt{\E_{s \sim \mu^\pi} \br{\norm{Q^\pi_s}_\circ^*}^2}
        %     \leq M.
        % \end{align*}
        \begin{align*}
            \norm{\nabla V(\pi)}_{L^2(\mu^\pi), \circ}^*
            =
            \norm{\mu^\pi \circ Q^\pi}_{L^2(\mu^\pi), \circ}^*
            =
            \sqrt{\E_{s \sim \mu^\pi} \br{\norm{Q^\pi_s}_\circ^*}^2}
            \leq M.
        \end{align*}
            \item Finally, the objective is $\beta$-locally smooth by assumption \ref{thmassm:pmd_main_smoothness}.
        \end{enumerate}
        The result now follows from \cref{thm:opt_main}.
    \end{proof}
    We conclude with a proof sketch of \cref{thm:pmd_main} for the Euclidean case; the full technical details are provided in \cref{sec:proof:thm:pmd_main}.
    \begin{proof}[Proof sketch of \cref{thm:pmd_main} (Euclidean case)]
    The first step is showing that the $\epsexpl$-greedy exploration introduces an error term that scales with $\delta \eqq \epsexpl C_\star H^2 A$ (see \cref{lem:epsgreedy_vgd}). This implies that $\Pi^{\epsexpl}$ is $(C_\star, \epsvgd + \delta)$-VGD w.r.t.~$\cM$.
    In addition, by definition of $\Pi^{\epsexpl}$ we have
    $\min_{s, a} \cbr{\pi_{s, a}} \geq \epsexpl/A$
    for all $\pi\in \Pi^\epsexpl$.
    We now argue the following:
    % \begin{enumerate}[nosep,leftmargin=0.5cm] %icml_edit
    \begin{enumerate}
        \item The action regularizer $R(p)=\frac12\norm{p}_2^2$ is $1$-strongly convex and has $1$-Lipschitz gradient w.r.t.~$\norm{\cdot}_2$.
        \item $\forall s, \norm{\pi_s - \tilde \pi_s}_2\leq D = 2$, $\norm{Q_s}_2\leq M=\sqrt{A H}$.
        \item By \cref{lem:value_local_smoothness}, the value function is 
        $\br{\beta \eqq \frac{A^{3/2} H^3}{\sqrt \epsexpl}}$-locally smooth w.r.t.~$\pi \mapsto \norm{\cdot}_{L^2(\mu^\pi), 2}$.
    \end{enumerate}
    The result now follows from \cref{lem:pmd_main} with $\eta=1/(2\beta)$ and $\epsexpl=K^{-2/3}$.
\end{proof}



\section{Conclusions and Outlook}
In this work, we introduced a novel theoretical framework and established best-in-class convergence of PMD for general policy classes, subject to an algorithm independent variational gradient dominance condition instead of a closure condition. In addition, we discussed the relation between VGD and closure thoroughly, and demonstrated closure implies VGD but not the other way around (\cref{sec:intro_discussion,sec:vgd_discussion}).
We conclude by outlining two directions for valuable (in our view) future research.
% \begin{itemize}[noitemsep, topsep=0pt, leftmargin=0.3cm]
% \begin{itemize}[leftmargin=0.4cm] %icml_edit
\begin{itemize}
    \item \textbf{$\epsilon$-greedy exploration.} Our approach builds on ensuring descent on each iteration, 
    which we establish by demonstrating local smoothness holds \emph{globally}, for any reference policy $\tilde \pi$. 
    As we discuss in \cref{sec:discuss_value_local_smoothness}, it seems that this technique cannot yield better results. However, when the multiplicative ratio $|\pi_{s,a}/ \tilde \pi_{s,a}-1|$ is bounded, arguments similar to those given in \cref{sec:proof:lem:value_local_smoothness} demonstrate a somewhat weaker notion of smoothness --- but \emph{without} dependence on the exploration parameter.
    Furthermore, an analysis approach that combines with the classic mirror descent analysis might do without the per iteration descent property.
    
    \item \textbf{Relaxing the VGD condition.} 
    It remains unclear whether a non-vanilla version of PMD, or an entirely different algorithm, can do without the VGD assumption. The VGD condition encapsulates both an exploration assumption (as it implicitly assumes bounded distribution mismatch coefficient), and an optimization assumption (as it guarantees a loss landscape that does not have suboptimal stationary points). It is interesting to consider to what extent this assumption can be relaxed or decomposed, possibly given additional structural assumptions on the MDP.
    
\end{itemize}


