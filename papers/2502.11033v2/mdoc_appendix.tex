% \part{Appendix}

% \parttoc
\section{Variational Gradient Dominance and Closure Conditions}
\label{sec:vgd_discussion}
In this section, we include detailed discussions regarding the VGD and closure conditions.
In \cref{sec:closure_implies_vgd}, we demonstrate closure $\Rightarrow$ VGD for a general form of closure conditions; 
%
In \cref{sec:vgd_not_imply_closure}, we provide the full details of the example presented in \cref{sec:intro_discussion}, establishing VGD $\not\Rightarrow$ closure;
%
Finally, in \cref{sec:vgd_discussion_additional}, we conclude with several general remarks.
\subsection{Closure implies VGD}
\label{sec:closure_implies_vgd}
In this section, we demonstrate a general form of (bounded approximation error) closure conditions implies a VGD condition with similar error floors. 
The work of \citet{bhandari2024global} demonstrated that closure to policy improvement implies VGD, and further observed there is also a connection between bounded approximation error and VGD (Lemma 16, Appendix B in their work).
The arguments we give below may be considered an elaborate generalization of those in \citet{bhandari2024global} relating to the connection between closure and VGD.
We consider the conditions proposed in the recent work of \citet{alfano2023novel}.

\begin{definition}[$\eta$-closure]\label{def:closure}
    For any policy $\pi$ and step-size $\eta > 0$, 
    denote $v\eqq\mu^\pi\circ \pi$ and define:
\begin{align*}
	f^+ \eqq f^{+}(\pi, \eta) &\eqq \argmin_{f\in \cF}
	\norm{f - \br{\eta^{-1}\nabla R(\pi) -  Q^\pi}}_{L^2(v)}^2
	\\
	\pi^+ \eqq \pi^+(\pi, \eta) &\eqq P_R(\eta f^+),
\end{align*}
where $P_R(\eta f)_{s} \eqq \Pi_{\Delta(\cA)}^R(\nabla R^*(\eta f_s))$.
We say that the policy class $\Pi$ satisfies $\eta$-closure if for all $\pi\in \Pi$, the following holds:
\begin{align*}
  	\norm{f^+  - \br{\eta^{-1}\nabla R(\pi) - Q^\pi}}_{L^2(v)}^2	\leq \epsapproxM,
	\tag{A1}	  
\end{align*}
and for $\tilde \pi\in\cbr{\pi, \pi^+, \pi^\star}, \tilde \mu\in \cbr{\mu^\pi, \mu^\star}$:
\begin{align*}
    \E_{s,a \sim v}\sbr{\br{\frac{\tilde \mu(s)\tilde \pi_{s, a}}{v(s, a)}}^2}
	\leq C_v,
	\tag{A2}
\end{align*}
and finally,
\begin{align*}
    \sup_s \frac{\mu^\star(s)}{\mu^\pi(s)}
    \leq \nu_\star.
	\tag{A3}
\end{align*}
\end{definition}

Before proving our claim, we will also need to define a certain regularity condition required from the policy class.

\begin{definition}\label{assm:Pi_regular}
    We say $\Pi$ is strictly stochastic $\inf_{\pi\in \Pi, s\in\cS,a\in \cA}\cbr{\pi_{s,a}}>0$.
\end{definition}
Next, we state and prove our claim, and conclude with a few remarks afterwards.
\begin{lemma}[Closure $\implies$ VGD]
\label{lem:closure_implies_VGD}
Let $R\colon \R^A \to \R$ be an action regularizer. Assume $\Pi \subseteq \Delta(\cA)^\cS$ is a policy class that is strictly stochastic (\cref{assm:Pi_regular}) and satisfies closure conditions w.r.t.~$R$ for any $\eta>0$ (\cref{def:closure}).
Then $\Pi$ satisfies $\br{\nu_\star, 5H\br{\nu_\star+1}\sqrt{C_v \epsapproxM}}$-VGD.
\end{lemma}
\begin{proof}
Fix $\pi \in \Pi$, and define
\begin{align*}
	\widehat Q^\pi
	&\eqq \eta^{-1}\nabla R(\pi) - f^+
	\\
	\implies
	f^+ 
	&= \eta^{-1}\nabla R(\pi) - \widehat Q^\pi,
\end{align*}
which implies that:
\begin{align*}
	\forall s, \;
	\pi^+_s &= \argmin_{p\in \Delta(\cA)}
		\abr{\widehat Q^\pi_s, p} + \frac1\eta B_R(p, \pi_s)
\end{align*}
    We first show the approximation error is bounded owed to (A1), (A2), using arguments similar to those in \citet{alfano2023novel}.
    Then, we argue that for a sufficiently large step-size, $\pi^+$ is close to being greedy w.r.t.~$Q^\pi$, which will imply variational gradient dominance at $\pi$.
    \paragraph{Approximation error.}
    For any policy $\tilde \pi$ and state-occupancy $\tilde \mu$, recalling that $v\eqq (\mu^\pi\circ \pi)$, we have:
$$
\E_{s\sim \tilde \mu}\abr{Q^\pi_s - \widehat Q^\pi_s, \tilde \pi_s}
=
\abr{Q^\pi - \widehat Q^\pi, \tilde \mu\circ \tilde \pi}
\leq
\norm{Q^\pi - \widehat Q^\pi}_{L^2(v)} 
\norm{\tilde \mu \circ \tilde \pi}_{L^2(v)}^*
$$
To bound the first term,
$$
\norm{Q^\pi - \widehat Q^\pi}_{L^2(v)} 
=
\norm{f^+ -\br{\eta^{-1}\nabla R(\pi)-Q^\pi}}_{L^2(\mu^\pi\circ\pi)} 
\leq \sqrt{\epsapproxM}.
$$
For the second term,
$$
\norm{\tilde \mu \circ \tilde \pi}_{L^2(\mu^\pi\circ\pi)}^*
=
\sqrt{\E_{s, a \sim v}
\br{\frac{\tilde \mu(s)\tilde \pi_{s, a}}{v(s, a)}}^2}
\leq \sqrt{C_v},
$$
where the last inequality follows for $\tilde \pi\in\cbr{\pi, \pi^+, \pi^\star}, \tilde \mu\in \cbr{\mu^\pi, \mu^\star}$ by assumption (A2). Now, for such $\tilde \mu, \tilde \pi$, we have:
\begin{align*}
	\av{\E_{s\sim \tilde \mu}\abr{Q^\pi_s - \widehat Q^\pi_s, \pi_s - \tilde \pi_s}}
	\leq
	\av{\E_{s\sim \tilde \mu}\abr{Q^\pi_s - \widehat Q^\pi_s, \pi_s}}
	+ \av{\E_{s\sim \tilde \mu}\abr{Q^\pi_s - \widehat Q^\pi_s, \tilde \pi_s}}
	\leq
	2\sqrt{C_v\epsapproxM}.
\end{align*}
In summary,
\begin{align*}
	\av{\E_{s\sim \mu^\pi}\abr{Q^\pi_s - \widehat Q^\pi_s, \pi_s - \pi_s^+}}
	&\leq
	2\sqrt{C_v\epsapproxM},
	\\
	\av{\E_{s\sim \mu^\star}\abr{Q^\pi_s - \widehat Q^\pi_s, \pi_s - \pi_s^\star}}
	&\leq
	2\sqrt{C_v\epsapproxM}.
\end{align*}
\paragraph{Greedification.}
Set $\epsgreedy = \sqrt{C_v\epsapproxM}$, $\tilde \epsilon = \epsgreedy/(2 H)$, and 
denote the effective minimal probability $p_{\min} \eqq 
\min\cbr{\inf_{\pi\in \Pi, s\in\cS, a\in\cA}\cbr{\pi_{s, a}}, \tilde \epsilon/A}$.
Consider the set 
$\cX = \cbr{p\in \Delta(\cA) \mid \forall i, p_i \geq p_{\min}}$, and note that $B_R(x, z)$ is continuous over $x, z\in  \cX\times \cX$, and further that $\cX\times \cX$ is compact. Hence $\max_{x,z\in \cX}B_R(x, z)<\infty$ is attained,
and setting $\eta=\max_{x,z\in \cX}B_R(x, z)/\epsgreedy$, implies that, by \cref{lem:omd_to_greedy};
$$
	\forall s,\; \abr{\widehat Q^\pi_s, \pi^+_s}
	\leq 
	\min_a \widehat Q^\pi_{s,a} + \epsgreedy.
$$
Now, 
\begin{align*}
	\E_{s\sim \mu^\pi}\abr{Q^\pi_s, \pi_s - \pi^+_s}
	&= \E_{s\sim \mu^\pi}\abr{\widehat Q^\pi_s, \pi_s - \pi^+_s}
	+ \E_{s\sim \mu^\pi}\abr{Q^\pi_s - \widehat Q^\pi_s, \pi_s - \pi^+_s}
	\\
	&\geq
	\E_{s\sim \mu^\pi}\max_p\abr{\widehat Q^\pi_s, \pi_s - p}
	-\epsgreedy
	- 2\sqrt{C_v \epsapproxM}
        \\
	&=
	\E_{s\sim \mu^\pi}\max_p\abr{\widehat Q^\pi_s, \pi_s - p}
	- 3\sqrt{C_v \epsapproxM}
        ,
\end{align*}
therefore by \cref{lem:value_diff} (value difference),
\begin{align*}
	\frac{1}{H}\br{V(\pi) - V(\pi^\star)}
	&= \E_{s\sim \mu^\star}\abr{Q^\pi_s, \pi - \pi^\star}
	\\
	&= \E_{s\sim \mu^\star}\abr{\widehat Q^\pi_s, \pi - \pi^\star}
	+ \E_{s\sim \mu^\star}\abr{Q^\pi_s - \widehat Q^\pi_s, \pi_s - \pi^\star_s}
	\\
	&\leq 
	\E_{s\sim \mu^\star}\max_{p_s\in\Delta(\cA)}\abr{\widehat Q^\pi_s, \pi - p_s}
	+ 2\sqrt{C_v \epsapproxM}
	\\
	&\leq 
	\norm{\frac{\mu^\star}{\mu^\pi}}_\infty\E_{s\sim \mu^\pi}\max_{p_s\in\Delta(\cA)}\abr{\widehat Q^\pi_s, \pi - p_s}
	+ 2\sqrt{C_v \epsapproxM}
	\\
	&\leq 
	\norm{\frac{\mu^\star}{\mu^\pi}}_\infty
		\E_{s\sim \mu^\pi}\abr{Q^\pi_s, \pi_s - \pi^+_s}
         + 5\br{\norm{\frac{\mu^\star}{\mu^\pi}}_\infty+1}\sqrt{C_v \epsapproxM}
	\\
        %
	&=\nu_\star
		\E_{s\sim \mu^\pi}\abr{Q^\pi_s, \pi_s - \pi^+_s}
         + 5\br{\nu_\star+1}\sqrt{C_v \epsapproxM}
	\\
        %
	&=\frac{\nu_\star}{H} \abr{\nabla V(\pi), \pi - \pi^+}
         + 5\br{\nu_\star+1}\sqrt{C_v \epsapproxM}
	\\
        %
	&\leq 
    	\frac{\nu_\star}{H}
		\max_{\tilde \pi\in \Pi}\abr{\nabla V(\pi), \pi - \tilde \pi}
		+ 5\br{\nu_\star+1}\sqrt{C_v \epsapproxM}
	,
\end{align*}
which completes the proof after multiplying by $H$.
\end{proof}

\begin{lemma}\label{lem:omd_to_greedy}
  Let $\epsilon>0$, $R\colon \R^A \to \R$ a convex regularizer differentiable over ${\rm relint}(\Delta(A))$, and
$g\in [0, H]^A$ a linear objective.  
Denote $a^\star=\argmin_a g_a$ and define $\tilde x^\star \eqq (1-\tilde \epsilon)e_{a^\star} + \tilde \epsilon u$ where $u_i=1/A$ for all $i$, and $\tilde \epsilon=\epsilon/(2H)$.
Then, for any  $x\in {\rm relint} (\Delta(A))$, if $\eta\geq 2 B_R(\tilde x^\star, x)/\epsilon$, we have:
$$
	x^+ = \argmin_{z\in \Delta(A)} \cbr{\abr{g,z} 
            + \frac1\eta B_R(z, x)}
	\implies
	g(x^+) \leq g_{a^\star} + \epsilon.
$$
\end{lemma}
\begin{proof}
    By optimality of $x^+$:
    \begin{align*}
        g(x^+) 
        &\leq 
        g(\tilde x^\star) + \frac1\eta B_R(\tilde x^\star, x)
        - \frac1\eta B_R(x^+, x)
        \\
        &\leq 
        g_{a^\star}
        +\tilde \epsilon H/A + \frac1\eta B_R(\tilde x^\star, x)
        \\
        &= 
        g_{a^\star}
        +\epsilon,
    \end{align*}
    and the result follows.
\end{proof}

Before concluding this section, we make a few remarks regarding the assumptions of \cref{lem:closure_implies_VGD} and the fundamental log-linear setup.

\paragraph{Strict stochasticity.}
The role of this condition is to ensure that there exists a finite step-size that will produce the greedifying policy $\pi^+$. This condition does not really limit the applicability of the result, as any policy class can always be ``shrank'' in the analysis so that its policies are bounded away from being deterministic. Indeed, any mirror descent algorithm with a standard action regularizer will not visit near deterministic policies $\pi$ such that $\inf_{s, a}\pi_{s, a}=0$, hence this shrinkage has no actual effect.


\paragraph{Closure conditions on algorithm iterates.}
    Most works based on closure conditions (including \citealp{alfano2023novel}) assume the conditions hold for the iterates produced by the optimization algorithm. To simplify the comparison, we consider a ``global''  variant of closure conditions (i.e., closure holds at any $\pi\in \Pi$) and show it implies $\Pi$ satisfies VGD.
    We note however that our analysis for \cref{thm:pmd_main} only requires variational gradient dominance to hold at the iterates produced by the algorithm, thus the comparison would follow through also for the variant of the conditions that consider only algorithm iterates. 


\paragraph{The log-linear setup.}
It is not hard to show that our above theorem includes to the log-linear setup considered in \citet{agarwal2021theory,yuan2023loglinear} as a special case. In this setup, the policy class is naturally closed w.r.t.~any step-size $\eta$; indeed, in \citet{agarwal2021theory,yuan2023loglinear} the approximation error relates only to the approximability of the Q-functions (see also discussion in \cref{sec:dual_pc}). Here, approximability of the $Q$ functions immediately implies $\eta$-closure for all $\eta>0$.


\subsection{VGD does not imply closure}
\label{sec:vgd_not_imply_closure}
In this section, we discuss in detail the example presented in the introduction (\cref{fig:simple_mdp}); the exact same diagram is included here in \cref{fig:simple_mdp_2} for convenience.
\begin{figure}[ht!]
    \centering
\begin{tikzpicture}[
    state/.style={circle, draw, minimum size=2cm, node distance=2cm},
    every node/.style={font=\sffamily},
    >={Stealth[round]}
]

% States
\node[state] (S0) at (0, 0) {$S_0$};
\node[state] (S1) at (-2, -3) {$S_1$};
\node[state] (S2) at (2, -3) {$S_2$};

% Arrows
\draw[->, color=orange] (S0) -- 
		node[pos=0, anchor=south, font=\scriptsize, shift={(0.15, -0.1)}] 
		{\tiny $\actA$} 
	(S1) node[midway, above] {};
\draw[->, color=violet] (S0) -- 
		node[pos=0, anchor=south, font=\scriptsize, shift={(-0.1, -0.1)}] 
		{\tiny $\actB$} 
	(S2) node[midway, left] {};
	
	
\draw[->, color=violet] (S1) to[bend right] 
	node[pos=0, anchor=north east, font=\scriptsize, shift={(0.2, 0.15)}] 
		{\tiny $\actB$}
	(S0) node[midway, above] {};
\draw[->, color=orange] (S2) to[bend left] 
		node[pos=0, anchor=north west, font=\scriptsize, shift={(-0.2, 0.1)}] 
		{\tiny $\actA$}
	(S0) node[midway, below] {};

\draw[->, very thick, color=orange] (S1) 
	to[bend left] node[midway, left] {1} 
		node[pos=0, anchor=north, font=\scriptsize, shift={(0, 0.05)}] 
		{\tiny $\actA$}
	(S0) node[midway, above] {};

\draw[->, very thick, color=violet] (S2) 
	to[bend right] node[midway, right] {$1$}
		node[pos=0, anchor=north, font=\scriptsize, shift={(0, 0.05)}] 
		{\tiny $\actB$}
	(S0) node[midway, below] {};
\end{tikzpicture}
    \caption{A simple MDP with a convex value landscape. Each action represented by a (feature-vector, edge) pair leads deterministically to the state at the other end of the edge. The two outer bold edges labeled $1$ inflict a cost of $1$, the others have cost $0$.}
    \label{fig:simple_mdp_2}
\end{figure}
In what follows, we demonstrate:
\begin{enumerate}
    \item The loss landscape is convex, hence the VGD condition holds with error floor $\epsvgd=0$ and \cref{thm:pmd_main} guarantees convergence of PMD w.r.t.~the best in-class policy.
    \item Both the transfer error and approximation error in the compatible function approximation framework are $O(1)$, and as a result closure based analyses lead to upper bounds with error floors that are larger than $H$.
\end{enumerate}
We note that the example is not realizable and the discussion focuses on best-in class convergence as the objective.
If we were to look for convergence w.r.t.~the true optimal policy, our \cref{thm:pmd_main} establishes convergence to an error floor of $V(\Pi^\star) - V^\star \approx H/2$, while closure based analyses suffer from the same $\geq H$ error floor.
In all that follows, we focus on the transfer error $\epsbiasM$; the argument for the approximation error is the same.

\subsubsection{Overview}
The transfer error term, usually denoted $\epsbiasM$, essentially accounts for the discrepancy between $\Pi$ and the complete policy class $\Pi_{\rm all} = \Delta(\cA)^{\cS}$ ``w.r.t.~the optimal policy occupancy measure''. 
It is not hard to find examples where $\epsbiasM = \Omega(1)$ when operating over a policy class that does not satisfy closure. At the same time, the error floors in such results typically contain a term that scales with (at least) $H\norm{\fraci{\mu^\star}{\rho_0}}_\infty \sqrt \epsbiasM$. 
For instance, Theorem 1 of \citet{yuan2023loglinear} guarantees that, when there are no statistical errors:
\begin{align}\label{eq:thm_yuan}
        V(\pi^{K}) - &\min_{\pi^\star\in \Pi}V(\pi^\star)
        \lesssim 
        2H\br{1- \frac{1}{\nu_0}}^{K}
        + 2H \nu_0 \sqrt{A C_0 \epsbiasM},
    \end{align}
    where $\nu_0 \eqq H\norm{\fraci{\mu^\star}{\rho_0}}_\infty$ and $C_0$ is a certain concentrability coefficient larger than $1$.
We consider the MDP depicted in \cref{fig:simple_mdp_2}, with the log-linear policy class $\Pi_{\rm loglin}$ induced by the state-action feature vectors $\cbr{\phi_{s,a}}$ shown in the diagram. 
As we show below, the value landscape is convex over $\Pi_{\rm loglin}$, thus convergence of PMD follows by \cref{thm:pmd_main}, which in this case guarantees the sub-optimality of $\pi^K$ tends to $0$ as $K$ grows (since there is no error floor).

 At the same time, already for a moderate effective horizon (say, $\gamma\geq0.9$), it follows that $\epsbiasM=\Omega(1)$. On step $k$ the algorithm performs an update using:
\begin{align*}
w^{(k)}_\star
&\eqq \argmin_{w}	\E_{s \sim \mu^k, a\sim \pi^k_s}\sbr{
    \br{\phi_{s, a}\T w - Q^k_{s, a}}^2
},
\end{align*}
and ``pays'' a bias / transfer error of:
\begin{align*}
    \epsilon_{\rm bias}
	&\geq  \E_{s \sim \mu^\star, a\sim \Unif(\cA)}\sbr{
		\br{\phi_{s, a}\T w_\star^{(k)} - Q^k_{s, a}}^2
	},
\end{align*}
where $\mu^\star$ denotes the occupancy measure of a comparator policy. Now the issue is that the Q function cannot be well approximated from all states using the feature vectors $\cbr{\phi_{s,a}}$, since these obfuscate which state the agent is occupying. Thus, the error upper bounded by $\epsbiasM$ is $\Omega(1)$ for \emph{any} vector $w_{\star}^{(k)}$ and therefore for \emph{any} policy $\pi^k$.
It can now be shown that as long as the start state distribution $\rho_0$ assigns most of the weight to $S_0$ compared to the effective horizon (say, $\rho_0(S_0) \geq \gamma/10$), it holds that,
\begin{align*}
	H\norm{\frac{\mu^\star}{\rho_0}}_\infty\sqrt{\epsilon_{\rm bias}}
	\geq
	10 H.
\end{align*}

\subsubsection{Analysis}
We denote the actions:
\begin{align*}
	u \eqq  \actA,
	\quad 
	b \eqq  \actB,
\end{align*}
and the state-action features, for all $s$:
\begin{align*}
	\phi_{s, 1} \eqq \actA,
	\quad 
	\phi_{s, 2} \eqq \actB,
	\quad 
	\phi_{s} 
	\eqq (\phi_{s, 1}, \phi_{s, 2})
	= (\actA \actB)
	\in \R^{2\times 2}.
\end{align*}
In favor of conciseness, we will let
\begin{align*}
	\phi_{i, \cdot} \eqq \phi_{S_i, \cdot}.
\end{align*}
For $\theta \in \R^2$, we denote the log-linear policy $\pi^\theta_{s}\eqq \sigma(\phi_{s}\T \theta)$, where $\sigma$ is the softmax function:
\begin{align*}
	\sigma(u)_i \eqq \frac{e^{u_i}}{\sum_{j}e^{u_j}}.
\end{align*}
This gives rise to the log-linear policy class:
\begin{align*}
	\Pi \eqq \cbr{\pi^\theta \mid \theta \in \R^2}.
\end{align*}
Since such a policy $\pi^\theta$ in this MDP must select actions independent of the state, we let $\alpha$ denote the probability it chooses $u$ and $1-\alpha$ the probability it chooses $b$; $\alpha\eqq\pi^\theta_{s, u} \implies 1-\alpha = \pi^\theta_{s, b}$.
Now, denote $V_i^\alpha \eqq V_{S_i}\br{\pi^\theta}, Q^\alpha_{i, \cdot}\eqq Q_{S_i, \cdot}^{\pi^\theta}$, and observe that by direct computation:
\begin{align*}
	V_0(\alpha) 
	&= \frac{\gamma}{(1-\gamma)(1+\gamma)}\br{\alpha^2 + (1-\alpha)^2}
	=: \widetilde H\br{\alpha^2 + (1-\alpha)^2}
	\\
	V_1(\alpha) &= \alpha + \gamma \widetilde H\br{\alpha^2 + (1-\alpha)^2}
	\\
	V_2(\alpha) &= (1-\alpha) + \gamma \widetilde H\br{\alpha^2 + (1-\alpha)^2}.
\end{align*}
and,
\begin{alignat*}{2}
	Q^\alpha_{0,u} &= \gamma V_1(\alpha),
	\quad &
	Q^\alpha_{0,b} &= \gamma V_2(\alpha);
	\\
	Q^\alpha_{1,u} &= 1 + \gamma V_0(\alpha),
	\quad &
	Q^\alpha_{1,b} &= \gamma V_0(\alpha);
	\\
	Q^\alpha_{2,u} &= \gamma V_0(\alpha),
	\quad &
	Q^\alpha_{2,b} &= 1 + \gamma V_0(\alpha).
\end{alignat*}
Let $\rho_0(S0)=1-p,\rho_0(S1)=\rho_0(S2)=p/2$ for some $p\in[0,1)$. Then
\begin{align*}
	V(\alpha) 
	&= (1-p+\gamma p)\widetilde H\br{\alpha^2 + (1-\alpha)^2}
	+ p/2.
\end{align*}

\paragraph{The VGD condition holds.}
It is not hard to verify the value function is convex (in state-action space) over this policy class. Indeed, we have
\begin{align*}
	\abr{\nabla_{\pi^\theta} V(\pi^\theta), \pi^{\tilde \theta} - \pi^\theta}
	= \frac{\partial V^\alpha}{\partial \alpha}\br{\tilde \alpha - \alpha},
\end{align*}
and therefore convexity of $V^\alpha$ w.r.t.~$\alpha$ implies convexity in the direct parametrization over $\Pi$.
Hence in particular, $\Pi$ is $(1, 0)$-VGD w.r.t.~the MDP in question. Thus, convergence of PMD follows by \cref{thm:pmd_main}, which in this case guarantees the sub-optimality of $\pi^K$ tends to $0$ as $K$ grows (since there is no error floor).

\paragraph{Closure does not hold, and the error floor in closure based analyses is $\geq H=\frac{1}{1-\gamma}$.}
Let $\mu^\alpha \eqq \mu^{\pi^\theta}$, then
\begin{align*}
	\mu^\alpha(S_0) 
	&= \frac{(1-\gamma)(1-p) + \gamma}{1+\gamma}
	= \frac{1-p + \gamma H}{(1+\gamma) H}
	\\
	\mu^\alpha(S_1) &= (1-\gamma)p + \gamma \alpha \mu^\alpha(S_0)
	\\
	\mu^\alpha(S_2) &= (1-\gamma)p + \gamma (1-\alpha) \mu^\alpha(S_0).
\end{align*}
It is immediate that the optimal in-class policy is given by $\theta^\star\eqq (1, 1), \alpha^\star = 1/2$, and satisfies,
\begin{align*}
	\mu^\star(S_0) = \frac{1-p + \gamma H}{(1+\gamma)H}, \quad 
	\mu^\star(S_1) = \frac{H + p -1}{2(1+\gamma)H}, \quad 
	\mu^\star(S_2) = \frac{H + p -1}{2(1+\gamma)H}.
\end{align*}
Now suppose that $\gamma\geq0.99$ and $p\leq 1/100$, then by direct computation,
\begin{align*}
	\mu^\star(S_0) 
	\approx \frac12, \quad
	\mu^\star(S_1) \approx \frac{1}{4}, \quad
	\mu^\star(S_2) \approx \frac{1}{4},
\end{align*}
where the approximation is correct up to error of $1/100$.
Recall that for a policy $\pi^{(k)}$, in the NPG update step \cite{agarwal2021theory,yuan2023loglinear}
\begin{align*}
	w^{(k)}_\star
	&\eqq \argmin_{w}	\E_{s \sim \mu^k, a\sim \pi^k_s}\sbr{
		\br{\phi_{s, a}\T w - Q^k_{s, a}}^2
	}
\end{align*}
Meanwhile, by definition
\begin{align*}
	\epsilon_{\rm bias}
	&\geq  \E_{s \sim \mu^\star, a\sim {\rm Unif}(\cA)}\sbr{
		\br{\phi_{s, a}\T w_\star^{(k)} - Q^k_{s, a}}^2
	}
	\\
	&\geq \frac12\argmin_{w_1}\E_{s \sim \mu^\star}\sbr{
		\br{w_1 - Q^k_{s, u}}^2
	}
	\\
	&\approx  \frac12\argmin_{w_1}
	\cbr{
		\frac12\br{w_1 - \gamma V_1(\alpha)}^2
		+
		\frac14\br{w_1 - 1 - \gamma V_0(\alpha)}^2
		+
		\frac14\br{w_1 - \gamma V_0(\alpha)}^2
	}
	\\
	&\geq
	\frac18\argmin_{w_1}
	\cbr{
		\br{w_1 - 1 - \gamma V_0(\alpha)}^2
		+
		\br{w_1 - \gamma V_0(\alpha)}^2
	}
	\\
	&= \frac{1}{32}
\end{align*}
Now the bias term in \cref{eq:thm_yuan} is at least as large as
\begin{align*}
	H\norm{\frac{\mu^\star}{\rho_0}}_\infty\sqrt{\epsilon_{\rm bias}}
	\gtrsim H\frac{1}{p}\sqrt{\frac{1}{32}}	
	\geq
	10 H.
\end{align*}


\subsection{Additional Remarks}
\label{sec:vgd_discussion_additional}
In this section we include several additional points for consideration regarding closure and VGD conditions.

\paragraph{On-policy PMD is prone to local optima.}
The necessity of some structural assumption
    (whether VGD or closure) is motivated in the introduction by the fact that policy gradient methods over non-complete policy classes $\Pi \neq \Delta(\cA)^\cS$ are prone to local optima \citep{bhandari2024global}.
     While PMD and vanilla policy gradients are not the same algorithm, the example given in \citet{bhandari2024global} (Example 1) also applies to PMD with Euclidean regularization, as we explain next.
    A vanilla policy gradient update in the direct parametrization case is equivalent to:
    \begin{align*}
        \pi^{k+1} = \argmin_{\pi\in \Pi}\sbr{\E_{s\sim \mu^k}\sbr{\abr{Q_s^k, \pi_s}} + \frac{1}{2\eta}\norm[b]{\pi - \pi^k}^2_2},
    \end{align*}
    which is an ``unweighted regularization'' version of Euclidean PMD.
    While this is equivalent to PMD for $\Pi = \Delta(\cA)^\cS$ (in the error free case), it is indeed not equivalent in general.
	However, Example 1 of \citet{bhandari2024global} indeed also applies to Euclidean PMD because the policy class in question contains only policies $\pi$ such that $\pi_{s, a} = \pi_{s', a}$ for all $s,s',a$. Hence, for any two policies $\pi, \pi^k\in \Pi$,
    $\norm{\pi_s - \pi^k_s}^2_2 = \norm{\pi_{s'} - \pi^k_{s'}}^2_2$ for all $s, s'$, and
    $\norm{\pi - \pi_k}^2_2 = S\E_{s\sim \mu^k}\norm{\pi_s - \pi^k_s}^2_2 = 2\E_{s\sim \mu^k}\norm{\pi_s - \pi^k_s}^2_2$. Thus, for the example in question the two algorithms are equivalent up to scaling of the step-size by a constant factor.

\paragraph{Closure conditions in practice.}
Closure conditions (that are based on bounded approximation error) roughly stipulate the policy class is closed to a soft policy improvement step.
This has a flavor that is similar to Bellman completeness \citep{munos2008finite,chen2019information,zanette2020learning,zanette2023realizability}, a property of a $Q$-function class that says the class is closed to a Bellman backup step.  
Bellman completeness is widely considered too strong a condition to hold in practice, the reasoning being that increasing capacity of a function class that violates completeness inadvertently introduces new functions for which completeness needs to be satisfied. Therefore, an increase in capacity may actually cause completeness to be further violated.
The same can be argued for closure conditions, with one important difference being that the complete policy class $\Delta(\cA)^\cS$ is naturally closed to any policy improvement step. However, in a large scale environment setting, the complete policy class is typically too large to be well approximated by reasonably sized neural network architectures.

\paragraph{PMD and VGD from the optimization perspective.}
Standard arguments from optimization literature are insufficient to establish convergence of PMD with the VGD condition. First, PMD is not an algorithm that has (prior to our work) a formulation within a purely optimization-based framework.
    Second, convergence in a smooth non-convex setting typically scales with the distance to the optimal solution, measured by the norm induced by smoothness of the objective. Prior works that establish convergence of gradient descent type methods (though not of PMD; e.g., \citealp{agarwal2021theory,bhandari2024global,xiao2022convergence}) exploit smoothness of the value function w.r.t.~the Euclidean norm (established  in \citealp{agarwal2021theory}), and as a result obtain bounds that scale with the cardinality of the state-space.

\paragraph{Divergence of Policy Iteration.}
Our setup with the VGD condition is general enough to accommodate examples where the policy iteration algorithm does not converge (the same example we discuss in \cref{sec:vgd_not_imply_closure} demonstrates this). Here, since the policy class is non-complete, the policy improvement step is performed over the current policy occupancy measure (see \citealp{bhandari2024global} who introduce this natural adaptation). Arguably, it should not be expected that policy iteration converges for real world, large-scale problems, as it is a very ``non-regularized'' algorithm from an optimization perspective.
    At the same time,
    in setups where closure conditions based on bounded approximation error hold, in particular, closure to policy improvement as studied in \citet{bhandari2024global}, the policy iteration algorithm converges at a linear rate. Thus it is not immediately clear why should we employ more sophisticated algorithms such as PMD in such settings.
    
\paragraph{Approximability of the $Q$-functions.}
    Our results (as do most if not all of results in other works) require that the $Q$-functions are estimable. 
    Importantly, in any setup, regardless of expressibility of the policy class, we may estimate the $Q$-functions through a pointwise statistical estimation procedure. 
    Moreover, considering a dual policy parametrization rich enough to approximate the $Q$-functions still does not imply closure in general (although it does in the log-linear setup; see \cref{sec:dual_pc}).
    Therefore, there is no ``contradiction'' in assuming access to approximate $Q$-functions and having a policy class that does not satisfy closure.


\paragraph{Convergence beyond the VGD condition.}
    Using our framework, it can be shown that PMD converges to a stationary point regardless of any VGD condition; see \cref{sec:prox_convergence_stationary_point}.

\section{Deferred Discussions}

\subsection{Assumption on the critic error}
\label{sec:discuss_critic_error}
Our results can be easily adapted to 
    the (generally weaker) assumption that 
    \begin{align*}
        \E_{s \sim \mu^\pi}
        \norm[b]{\widehat Q_{s}^{\pi} - Q_{s}^{\pi}}_2 
        \leq \epscritM.
    \end{align*}
    (In which case the bounds would depend on $\epscritM$ rather than $\sqrt \epscritM$.) \cref{assm:Q_oracle} in its current form simplifies presentation, since it allows working with the weighted $L^2$ norm for both smoothness and errors in the gradient approximation. Also noteworthy, when working with the negative entropy regularizer, approximation w.r.t.~the $\norm{\cdot}_\infty$ norm would suffice. Since the statistical errors are not the focus of this work, we make these concessions in favor of a more streamlined and clear presentation. 
    
\subsection{Local smoothness of the value function requires greedy exploration}
\label{sec:discuss_value_local_smoothness}
In this section we discuss why the dependence on $\epsilon$ in the bound of \cref{lem:value_local_smoothness} cannot be improved in general.
We consider the MDP in \cref{fig:mdp_smoothness_lb}, for which we can show \cref{lem:value_local_smoothness} has tight dependence on the $\epsilon$-exploration parameter.
\begin{figure}[ht!]
    \centering
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

%Shape: Ellipse [id:dp47233526940748916] 
\draw   (244,99.93) .. controls (244,82.21) and (259.67,67.85) .. (279,67.85) .. controls (298.33,67.85) and (314,82.21) .. (314,99.93) .. controls (314,117.64) and (298.33,132) .. (279,132) .. controls (259.67,132) and (244,117.64) .. (244,99.93) -- cycle ;
%Curve Lines [id:da0442407466355883] 
\draw    (244,99.93) .. controls (148.48,108.81) and (219.28,-35.69) .. (278.11,66.3) ;
\draw [shift={(279,67.85)}, rotate = 240.67] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Ellipse [id:dp7845506873835677] 
\draw   (344,199.93) .. controls (344,182.21) and (359.67,167.85) .. (379,167.85) .. controls (398.33,167.85) and (414,182.21) .. (414,199.93) .. controls (414,217.64) and (398.33,232) .. (379,232) .. controls (359.67,232) and (344,217.64) .. (344,199.93) -- cycle ;
%Curve Lines [id:da2757137889402381] 
\draw    (279,132) .. controls (298.8,186.3) and (348,122.16) .. (378.09,166.48) ;
\draw [shift={(379,167.85)}, rotate = 237.45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da8298973695413794] 
\draw    (379,232) .. controls (444.67,284.59) and (490.54,224.46) .. (415.15,200.29) ;
\draw [shift={(414,199.93)}, rotate = 17.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da7770546305742225] 
\draw    (379,232) .. controls (348.16,276.63) and (246.03,240.22) .. (342.53,200.52) ;
\draw [shift={(344,199.93)}, rotate = 158.04] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (269,90) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle S_{0}$};
% Text Node
\draw (369,190) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle S_{1}$};
% Text Node
\draw (144,104) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \pi _{0,0}^{\alpha } =1-\alpha $};
% Text Node
\draw (265,154) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \pi _{0,1}^{\alpha } =\alpha $};
% Text Node
\draw (439,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \pi _{1,1}^{\alpha } =?$};
% Text Node
\draw (254,254) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \pi _{1,0}^{\alpha } =?$};
% Text Node
\draw (176,49) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a_{0}$};
% Text Node
\draw (281,221) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a_{0}$};
% Text Node
\draw (459,225) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a_{1}$};
% Text Node
\draw (319,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a_{1}$};

\end{tikzpicture}
\caption{A two state deterministic MDP, with $\rho_0(S_0)=1$. Each edge is labeled with an action ($a\in \cbr{a_0, a_1}$) that takes the agent to the state at the other end. A policy $\pi^\alpha$, $\alpha\in [0,1]$ takes actions in $S_0$ with the probabilities displayed in the diagram next to the relevant action. The probabilities $\pi^\alpha$ assigns to actions in $S_1$ denoted by $?$ are unrelated to $\alpha$ and left for later.}
    \label{fig:mdp_smoothness_lb}
\end{figure}
Let $p\in(0,1/2)$ and $0< \epsilon < p$. Define:
\begin{align*}
	\pi \eqq \pi^\epsilon, \quad \pi_{1,0}=1,\pi_{1,0}=0,
	\\
	\tilde \pi \eqq \pi^p, \quad  \tilde \pi_{1,0}=0, \tilde \pi_{1,0}=1.
\end{align*}
\paragraph{Idea.}
Think of $\epsilon$ as much smaller than $p$. When measuring distance with the local norm $\norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}$, the large difference $\norm{\tilde \pi_1 - \pi_1}_1^2$ gets little weight: $\mu^\pi(S_1) \approx \epsilon$.
Meanwhile, the error of the linear approximation at $\pi$ behaves like (see proof of \cref{lem:value_local_smoothness} in \cref{sec:proof:lem:value_local_smoothness}):
\begin{align*}
	\av{\sum_s \mu^\pi(s) \sum_{a}
    		\br{\tilde \pi_{sa} - \pi_{sa}}
    		\br{\sum_{s'}\mu^\pi_{\P_{sa}}(s')\norm{\tilde \pi_{s'}- \pi_{s'}}_1}
    },
\end{align*}
where the weight assigned to $\norm{\tilde \pi_1 - \pi_1}_1^2$ is approximately $(\tilde \pi_{0, 1} - \pi_{0, 1}) = p-\epsilon$.
Hence, if $\epsilon=p^2$,
\begin{align*}
	\av{V(\tilde \pi) - V(\pi) - \abr{\nabla V(\pi), \tilde \pi - \pi}}
	&\approx p,
	\\
	\norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}^2 
	&\approx p^2,
\end{align*}
so
\begin{align*}
	\av{V(\tilde \pi) - V(\pi) - \abr{\nabla V(\pi), \tilde \pi - \pi}}
	\gtrsim  \frac{1}{\sqrt \epsilon}\norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}^2.
\end{align*}

\paragraph{Computations.}
The term that is equal to the linearization error, up to constant factors, is the following:
\begin{align*}
\av{\sum_s \mu^\pi(s) \sum_{a}
    		\br{\tilde \pi_{sa} - \pi_{sa}}
    		\br{\sum_{s'}\mu^\pi_{\P_{sa}}(s')\abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}}
    }.
\end{align*}
Assume $p > \epsilon$.
By choosing a cost function $r(s,i)=i$ for $s\in \cbr{S_0, S_1}$, $i\in \cbr{0,1}$ we have that for all $s$,
\begin{align*}
	\abr{Q^{\tilde \pi}_{s}, \tilde \pi_{s}- \pi_{s}}
	= \Omega(\norm{\tilde \pi_s - \pi_s}_1),
\end{align*}
hence we focus on lower bounding
\begin{align*}
	(*) \eqq \av{\sum_s \mu^\pi(s) \sum_{a}
    		\br{\tilde \pi_{sa} - \pi_{sa}}
    		\br{\sum_{s'}\mu^\pi_{\P_{sa}}(s')\norm{\tilde \pi_{s'}- \pi_{s'}}_1}
    }.
\end{align*}
By direct computation,
\begin{align*}
	\mu^\pi(S_0) = \frac{1}{1+\gamma\epsilon}, \quad
	\mu^\pi(S_1) = \frac{\gamma\epsilon}{(1+\gamma\epsilon)(1-\gamma)}
\end{align*}
and 
\begin{align*}
\norm{\tilde \pi_0 - \pi_0}_1=2|p-\epsilon|,
\quad \norm{\tilde \pi_1 - \pi_1}_1=2.
\end{align*}
Thus,
\begin{align*}
	(\tilde \pi_{0,0} - \pi_{0,0})\sum_{s'}\mu^\pi_{\P_{0,0}}(s')\norm{\tilde \pi_{s'}- \pi_{s'}}_1
	&\approx
	(1-\epsilon)(\epsilon - p)|p-\epsilon| + \epsilon
	\geq - p^2 + \epsilon
	\\
	(\tilde \pi_{0,1} - \pi_{0,1})\sum_{s'}\mu^\pi_{\P_{0,1}}(s')\norm{\tilde \pi_{s'}- \pi_{s'}}_1
	&\approx
	p-\epsilon,
\end{align*}
and further,
\begin{align*}
\av{\sum_{a}
    		\br{\tilde \pi_{1,a} - \pi_{1,a}}
    		\br{\sum_{s'}\mu^\pi_{\P_{1,a}}(s')\norm{\tilde \pi_{s'}- \pi_{s'}}_1}
    } = 0.
\end{align*}
We obtain
\begin{align*}
	(*) 
	\gtrsim \mu^\pi(S_0) \br{p - p^2}
	\approx p - p^2.
\end{align*}
Meanwhile,
\begin{align*}
	\norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}^2 
	=
	\frac{4(p-\epsilon)^2}{1+\gamma\epsilon}
	+ \frac{4\gamma\epsilon}{(1+\gamma\epsilon)(1-\gamma)}
	\approx (p-\epsilon)^2 + \epsilon.
\end{align*}
Now,  
\begin{align*}
	\frac{\av{V(\tilde \pi) - V(\pi) - \abr{\nabla V(\pi), \tilde \pi - \pi}}}
	{\norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}^2 }
	\approx
		\frac{(*)}
	{\norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}^2 }
	\approx \frac{p - p^2}{(p-\epsilon)^2 + \epsilon}.
\end{align*}
Now, for $\epsilon\eqq p^2, p< 1/2$, we obtain 
\begin{align*}
	\frac{p - p^2}{(p-\epsilon)^2 + \epsilon}
	=
	\frac{p - p^2}{(p-p^2)^2 + p^2}
	\geq \frac{p}{4 p^2} = \frac{1}{4 p} 
		= \frac{1}{4\sqrt \epsilon}.
\end{align*}
\section{State-weighted state-action space: Basic Facts}
Given a state probability measure $\mu\in \Delta(\cS)$, and an action space norm $\norm{\cdot}_\circ\colon \R^A \to \R$, we define the induced state-action weighted $L^p$ norm $\norm{\cdot}_{L^p(\mu), \circ}\colon \R^{SA}\to \R$:
\begin{align}
	\norm{u}_{L^p(\mu), \circ}
	&\eqq \br{\E_{s \sim \mu}\norm{u_s}_\circ^p}^{1/p}.
\end{align}
In addition, for $\mu\in \R^S, Q\in \R^{SA}$, we define the state to state-action element-wise product $\mu \circ Q \in \R^{SA}$:
\begin{align}
	\br{\mu \circ Q}_{s, a} \eqq \mu(s)Q_{s, a}.
\end{align}
\begin{lemma}\label{lem:weighted_norm_dual}
	For any strictly positive measure $\mu\in \R_{++}^S$,
	the dual norm of $\norm{\cdot}_{L^2(\mu),\circ}$ is given by
	\begin{align}
		\norm{z}_{L^2(\mu), \circ}^*
		&= \sqrt{\int \mu(s)^{-1}\br{\norm{z_s}_\circ^*}^2 {\rm d}s}
	\end{align}
\end{lemma}
\begin{proof}
	First denote
$$\begin{aligned}
	z_s^* &\eqq \argmax_{u_s \in \R^A, \norm{u_s}_\circ\leq 1}\abr{u_s, z_s}
	\\
	\implies
	\norm{z_s}^*_\circ &= \abr{z_s^*, z_s},
	\text{ and } \norm{z_s^*}_\circ = 1.
\end{aligned}$$
Now let $x\in \R^{SA}$ be defined by $x_s \eqq \frac{\norm{z_s}_\circ^*}{\mu(s)} z_s^*$, then
$$\begin{aligned}
	\abr{x, z}
	= \int \frac{\norm{z_s}_\circ^*}{\mu(s)}\abr{z_s^*, z_s} \rmd s
	= \int \frac{1}{\mu(s)} \br{\norm{z_s}_\circ^*}^2 {\rm d}s.
\end{aligned}$$
Now, note that
$$
	\norm{x}_{L^2(\mu), \circ}
	= \int \mu(s)\br{\frac{\norm{z_s}_\circ^*}{\mu(s)}}^2\norm{z_s^*}_\circ^2
	= \int \frac{1}{\mu(s)}\br{\norm{z_s}_\circ^*}^2
	= \abr{x, z},
$$
hence, for $\bar x \eqq x/\norm{x}_{L^2(\mu), \circ}$ we have $\norm{\bar x}_{L^2(\mu), \circ}=1$, and
$$
	\abr{\bar x, z} = \sqrt{\int \frac{1}{\mu(s)} \br{\norm{z_s}_\circ^*}^2 \rmd s.}
$$
On the other hand, for any $v$ such that $\norm{v}_{L^2(\mu), \circ} \leq 1$, we have 
\begin{align*} \abr{v, z}
	= \int \abr{v_s, z_s} \rmd s
	&= \int \abr{\mu(s)v_s, \mu(s)^{-1}z_s} \rmd s
	\\
	&\leq \int \norm{\sqrt{\mu(s)}v_s}_\circ
		\norm{\sqrt{\mu(s)^{-1}}z_s}_\circ^* \rmd s
	\\
	&\leq \sqrt{\int \mu(s)\norm{v_s}_\circ^2 \rmd s}
	 \sqrt{\int \mu(s)^{-1}\br{\norm{z_s}_\circ^*}^2 \rmd s}
	\\
	&\leq 
	 \sqrt{\int \mu(s)^{-1}\br{\norm{z_s}_\circ^*}^2 \rmd s},
\end{align*}
and the proof is complete.
\end{proof}
\begin{lemma}\label{lem:wnorm_dual_mu}
	Let $\mu\in \Delta(\cS)$, and consider the state-action norm $\norm{\cdot}_{L^2(\mu), \circ}$. For any $W\in \R^{SA}$, we have
	\begin{align*}
		\norm{\mu \circ W}_{L^2(\mu), \circ}^*
		= \sqrt{\E_{s\sim \mu} \br{\norm{W_s}_\circ^*}^2}
	\end{align*}
\end{lemma}
\begin{proof}
	By \cref{lem:weighted_norm_dual}, 
	\begin{align*}
		\norm{\mu \circ W}_{L^2(\mu), \circ}^*
		&= \sqrt{\int \mu(s)^{-1} \br{\norm{\mu(s)W_s}_\circ^*}^2}
		= \sqrt{\E_{s\sim \mu} \br{\norm{W_s}_\circ^*}^2}
		. \qedhere
	\end{align*}
	
\end{proof}

\begin{lemma}\label{lem:reg_transform}
	Assume $h\colon \R^A \to \R$ is $1$-strongly convex and has $L$-Lipschitz gradient w.r.t.~$\norm{\cdot}$.
	Let  $\mu\in \Delta(\cS)$, and define $R_\mu(\pi) \eqq \E_{s\sim \mu}[h(\pi_s)]$.
	Then
	\begin{enumerate}
		\item $B_{R_\mu}(\pi, \tilde \pi) = \E_{s \sim \mu} B_R(\pi_s, \tilde \pi_s)$.
		\item $R_\mu$ is $1$-strongly convex and has an $L$-Lipschitz gradient w.r.t. $\norm{\cdot}_{L^2(\mu), \circ}$. 
	\end{enumerate}

\end{lemma}
\begin{proof}
We have
\begin{align*}
	\forall s, \nabla R_\mu(\pi)_{s} &= \mu(s) \nabla R(\pi_s) \in \R^A
	\\
	\implies
	B_{R_\mu}(\pi, \tilde \pi) 
	&=
	R_\mu(\pi) - R_\mu(\tilde \pi) - \abr{\nabla R_\mu(\tilde \pi), \pi - \tilde \pi}
	\\
	&=
	\E_{s \sim \mu}\sbr{
		R(\pi_s) - R(\tilde \pi_s) - \abr{\nabla R(\tilde \pi_s), \pi_s - \tilde \pi_s}
	}
	\\
	&=
	\E_{s \sim \mu} B_R(\pi_s, \tilde \pi_s)
	.
\end{align*}
Further, $1$-strongly convexity follows by
\begin{align*}
		\E_{s \sim \mu} B_R(\pi_s, \tilde \pi_s) \geq 
		\frac12\E_{s \sim \mu} \norm{\pi_s - \tilde \pi_s}_\circ^2,
\end{align*}
and the Lipschitz gradient condition from \cref{lem:wnorm_dual_mu}:
\begin{align*}
	\norm{\nabla R_\mu(\pi) - \nabla R_\mu(\pi^+)}_{L^2(\mu), \circ}^*
	&=
	\norm{\mu\circ\br{\nabla h(\pi_s) - \nabla h(\pi_s^+)}}_{L^2(\mu), \circ}^*
	\\
	&=
	\sqrt{\E_{s \sim \mu}
	\br{\norm{\nabla h(\pi_s) - \nabla h(\pi_s^+)}_\circ^*}^2}
	\\
	&\leq
	L\sqrt{\E_{s \sim \mu}
	\norm{\pi_s - \pi_s^+}_\circ^2}
	\\
	&=
	L
	\norm{\pi - \pi^+}_{L^2(\mu),\circ},
\end{align*}
which completes the proof.
\end{proof}





\section{Deferred proofs}

\subsection{Auxiliary Lemmas}
\label{sec:aux_lemmas}
\begin{lemma}[Value difference; \citealp{kakade2002approximately}]\label{lem:value_diff}
    For any $\rho\in \Delta(\cS)$,
    \begin{align*}
        V_\rho\br{\tilde \pi} -  V_\rho\br{ \pi}
        = \frac{1}{1-\gamma}
        \E_{s\sim \mu_\rho^\pi}\abr{Q^{\tilde \pi}_s, \tilde \pi_s - \pi_s}.
    \end{align*}
\end{lemma}

\begin{lemma}[Policy gradient theorem; \citealp{sutton1999policy}]\label{lem:value_pg}
    For any $\rho\in \Delta(\cS)$,
    \begin{align*}
        \br{\nabla V_\rho(\pi)}_{s, a}
        &= \frac{1}{1-\gamma} \mu^\pi_\rho(s) Q^\pi_{s, a},
        \\
        \abr{\nabla V_\rho(\pi), \tilde \pi - \pi}
        &= \frac{1}{1-\gamma} \E_{s\sim \mu^\pi_\rho} 
        \abr{Q^\pi_s, \tilde \pi_s - \pi_s}.
    \end{align*}
\end{lemma}


The following lemma can be found in e.g., \cite{bhandari2024global, agarwal2021theory}. The proof below is provided for convenience.

\begin{lemma}\label{lem:vgd_complete}
	Let $\Pi\subset \Delta(\cA)^\cS, \Pi_{\rm all} \eqq \Delta(\cA)^\cS$ and suppose that for any policy $\pi\in \Pi$, we have 
	\begin{align*}
		\max_{\pi^+ \in \Pi}\E_{s\sim \mu^\pi}\abr{Q^\pi, \pi - \pi^+}
		\geq 
		\max_{\pi' \in \Pi_{\rm all}}\E_{s\sim \mu^\pi}\abr{Q^\pi, \pi - \pi'} - \epsilon.
	\end{align*}
	Then $\Pi$ is $(H\nu_0, \epsilon H^2 \nu_0)$-VGD w.r.t.~$\cM$, for $\nu_0\eqq \norm{\frac{\mu^{\star}}{\rho_0}}_\infty$.
\end{lemma}
\begin{proof}
	Let $\pi^\star\in \argmin_{\pi\in \Pi} V(\pi)$.
    By value difference \cref{lem:value_diff},
    \begin{align*}
	V(\pi) - V(\pi^\star)
	&= H \E_{s \sim \mu^{\star}}\sbr{\abr{Q^\pi_s, \pi_s - \pi_s^\star}}
	\\
	&\leq H \max_{\pi'\in \Pi_{\rm all}}\E_{s \sim \mu^{\star}}\sbr{
		\abr{Q^\pi_s, \pi_s - \pi'_s}}
	\\
	&\overset{(*)}{\leq} H \norm{\frac{\mu^{\star}}{\mu^\pi}}_\infty \max_{\pi'\in \Pi_{\rm all}}
		\E_{s \sim \mu^\pi}\sbr{\abr{Q^\pi_s, \pi_s - \pi'_s}}
	\\
	&\leq H \norm{\frac{\mu^{\star}}{\mu^\pi}}_\infty \max_{\pi^+\in \Pi}
		\E_{s \sim \mu^\pi}\sbr{\abr{Q^\pi_s, \pi_s - \pi^+_s}}
		+ \epsilon H\norm{\frac{\mu^{\star}}{\mu^\pi}}_\infty
	\\
	&= \norm{\frac{\mu^{\star}}{\mu^\pi}}_\infty \max_{\pi^+\in \Pi}
		\abr{\nabla V^\pi, \pi - \pi^+}
		+ \epsilon H\norm{\frac{\mu^{\star}}{\mu^\pi}}_\infty
	\\
	&\overset{(**)}{\leq} H \norm{\frac{\mu^{\star}}{\rho_0}}_\infty \max_{z\in \Pi}
		\abr{\nabla V^\pi, \pi - z}
		+ \epsilon H^2\norm{\frac{\mu^{\star}}{\rho_0}}_\infty
	.
\end{align*}
    To explain the transitions above, $(*)$ follows by the fact that within the complete policy class we may choose $\pi'$ to be greedy w.r.t. $Q^\pi$, which means $\abr{Q^\pi_s, \pi_s - \pi'_s} \geq 0$ for all $s\in \cS$. The last transition $(**)$ follows from the fact that:
    \begin{align*}
        \mu^\pi(s) 
        &= \frac1H \sum_{t=0}^\infty \Pr(s_t = s \mid \rho_0, \pi)
        = \frac1H\rho_0(s) + \sum_{t=1}^\infty \Pr(s_t = s \mid \rho_0, \pi)
        \geq 
        \frac1H \rho_0(s).
        \qedhere
    \end{align*}
\end{proof}

\begin{lemma}\label{lem:Qvalue_diff}
	For any policy $\pi\colon \cS \to \Delta(\cA)$, $s, a \in \cS \times \cA$:
	\begin{align*}
	Q_{s, a}^{\tilde \pi} - Q_{s, a}^\pi
	&= \gamma H\E_{s' \sim \mu^\pi_{\P_{s, a}}} 
	\abr{Q^{\tilde\pi}_{s'}, \tilde \pi_{s'} - \pi_{s'}}.
    \end{align*}
\end{lemma}
\begin{proof}
	By \cref{lem:value_diff},
	we have:
	\begin{align*}
	Q_{s, a}^{\tilde \pi} - Q_{s, a}^\pi
	&= \gamma \E_{s' \sim \P_{s, a}}\sbr{V^{\tilde \pi}(s') - V^\pi(s')}
	\\
	&= \gamma H\E_{s' \sim \P_{s, a}}
            \sbr{\E_{s''\sim \mu^\pi_{s'}}\abr{Q^{\tilde\pi}_{s''}, \tilde \pi_{s''} - \pi_{s''}}}
	\\
	&= \gamma H\sum_{s'} \P(s' |s, a)
		\sum_{s''} \mu^\pi_{s'}(s'')
	\abr{Q^{\tilde\pi}_{s''}, \tilde \pi_{s''} - \pi_{s''}}
	\\
	&= \gamma H\sum_{s''} \sum_{s'} \P(s' |s, a)
            \mu^\pi_{s'}(s'')
	\abr{Q^{\tilde\pi}_{s''}, \tilde \pi_{s''} - \pi_{s''}}
	\\
	&= \gamma H\sum_{s''} \mu^\pi_{\P_{s, a}}(s'')
	\abr{Q^{\tilde\pi}_{s''}, \tilde \pi_{s''} - \pi_{s''}}
        \\
	&= \gamma H\E_{s'' \sim \mu^\pi_{\P_{s, a}}} 
	\abr{Q^{\tilde\pi}_{s''}, \tilde \pi_{s''} - \pi_{s''}}.
    \qedhere
    \end{align*}
\end{proof}
\begin{lemma}\label{lem:negent_smooth}
	Let $h\colon \R^A \to \R$ be the negative entropy regularizer $h(p)\eqq \sum_i p_i\log p_i$, and assume $\Delta_\epsilon (\cA)\subset \Delta(\cA)$ is such that $p_i \geq \epsilon$ for all $p\in \Delta_\epsilon (\cA)$. Then $h$ has $1/\epsilon$-Lipschitz gradient w.r.t.~$\norm{\cdot}_1$ over $\Delta_\epsilon (\cA)$.
\end{lemma}
\begin{proof}
Let $p, \tilde p\in \Delta_\epsilon(\cA)$, and note,
$$
	\norm{\nabla h(p) - \nabla h(\tilde p)}^*_1
	=
	\norm{\nabla h(p) - \nabla h(\tilde p)}_\infty.
$$
Let $i\in \cA$, and observe that by the mean value theorem, for some $\alpha\in [p_i, \tilde p_i]$,
$$
	\av{\log(p_i) - \log(\tilde p_i)}
	= \av{\frac{\partial \log(x)}{\partial x}}_{x=\alpha}\av{p_i - \tilde p_i}
	= \frac{1}{\alpha}\av{p_i - \tilde p_i}
	\leq \frac{1}{\epsilon}\av{p_i - \tilde p_i}
	\leq \frac{1}{\epsilon}\norm{p - \tilde p}_1,
$$
since $p_i, \tilde p_i \geq \epsilon$.
\end{proof}




\subsection{Proof of \cref{lem:value_local_smoothness}}
\label{sec:proof:lem:value_local_smoothness}
\begin{proof}[\unskip\nopunct]
    We have, by \cref{lem:value_diff,lem:value_pg},
    \begin{align}
	\av{V^{\tilde \pi} - V^{\pi} - \abr{\nabla V^\pi, \tilde \pi - \pi}}
	&= \av{H\E_{s \sim \mu^\pi}\abr{Q^{\tilde \pi}_s, \tilde \pi_s - \pi_s}
	- H\E_{s \sim \mu^\pi}\abr{Q^{\pi}_s, \tilde \pi_s - \pi_s}}
	\nonumber \\
        &= H\av{\E_{s \sim \mu^\pi}\abr{Q^{\tilde \pi}_s - Q^\pi_s, \tilde \pi_s - \pi_s}}
	\nonumber.
    \end{align}
    Applying \cref{lem:Qvalue_diff} yields,
    \begin{align*}
    &\frac{1}{\gamma H^2}\av{V^{\tilde \pi} - V^{\pi} - \abr{\nabla V^\pi, \tilde \pi - \pi}}
    \\
    &=\av{
     \E_{s\sim \mu^\pi}\sbr{\sum_a \br{
    	\E_{s' \sim \mu^\pi_{\P_{s, a}}} 
    	\abr{Q^{\tilde\pi}_{s'}, \tilde \pi_{s'} - \pi_{s'}}
        }
    \br{\tilde \pi_{sa} - \pi_{sa}}}
    }
    \\
    &= \av{\sum_s \mu^\pi(s) \sum_{a}
    		\br{\tilde \pi_{sa} - \pi_{sa}}
    		\br{\sum_{s'}\mu^\pi_{\P_{sa}}(s')\abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}}
    }
    \\
    &= \av{\sum_{s,a} \sqrt{\mu^\pi(s)}
    \br{\tilde \pi_{sa} - \pi_{sa}}
    \br{\sqrt{\mu^\pi(s)}\sum_{s'}\mu^\pi_{\P_{sa}}(s')\abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}}
    } \\
    &\leq 
    \sqrt{\sum_{s,a} \mu^\pi(s)
    \br{\tilde \pi_{sa} - \pi_{sa}}^2}
    \sqrt{\sum_{s,a}\mu^\pi(s)
        \br{\sum_{s'}\mu^\pi_{\P_{sa}}(s')\abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}}^2}
    \tag{Cauchy-Schwarz}
    \\
    &\leq
    \sqrt{\sum_{s,a} \mu^\pi(s)
    \br{\tilde \pi_{sa} - \pi_{sa}}^2}
    \sqrt{\sum_{s, a}\mu^\pi(s)
        \sum_{s'}\mu^\pi_{\P_{sa}}(s')\abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}^2}
    \tag{Jensen}
    \\
    &=
    \sqrt{\sum_{s} \mu^\pi(s)
    \norm{\tilde \pi_{s} - \pi_{s}}_2^2}
    \sqrt{\sum_{s'}
        \br{\sum_{s, a}\frac{1}{\pi_{sa}}\mu^\pi(s)\pi_{sa}\mu^\pi_{\P_{sa}}(s')}
        \abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}^2}
    \\
    &\leq
    \frac{1}{\sqrt \epsilon}
    \sqrt{\sum_{s} \mu^\pi(s)
    \norm{\tilde \pi_{s} - \pi_{s}}_2^2}
    \sqrt{\sum_{s'}
        \br{\sum_{s, a}\mu^\pi(s)\pi_{sa}\mu^\pi_{\P_{sa}}(s')}
        \abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}^2}
    ,
    \end{align*}
    for $\epsilon \eqq \min_{s, a}\cbr{\pi_{s a}}$.
    Now, by the law of total probability (applied on the discounted probability measure $\mu^\pi$):
    \begin{align*}
	\sum_{s,a}\mu^\pi(s)\pi_{sa}\mu^\pi_{\P_{sa}}(s')
	&= \sum_{s,a}\mu^\pi(s \mid s_0 \sim \rho_0) \pi(a|s)\mu^\pi(s' \mid s_0' \sim \P_{sa})
	\\
	&= \sum_{s,a}\mu^\pi(s, a \mid s_0 \sim \rho_0)\mu^\pi(s' \mid s_0' \sim \P_{sa})
	\\
	&= \mu^\pi(s' \mid s_0 \sim \rho_0)
	\\
	&= \mu^\pi(s').
    \end{align*}
    Combining with our previous inequality, we obtain
    \begin{align*}
    \av{V^{\tilde \pi} - V^{\pi} - \abr{\nabla V^\pi, \tilde \pi - \pi}}
    &\leq\frac{\gamma H^2}{\sqrt \epsilon}
    \sqrt{\sum_{s} \mu^\pi(s)
    \norm{\tilde \pi_{s} - \pi_{s}}_2^2}
    \sqrt{\sum_{s'}\mu^\pi(s')
        \abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}^2}
    \\
    &=
    \frac{\gamma H^2}{\sqrt \epsilon}
    \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 2}
    \sqrt{\sum_{s'}\mu^\pi(s')
        \abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}^2}.
    \end{align*}
    Further,
    \begin{align*}
        \sqrt{\sum_{s'}\mu^\pi(s')
        \abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}^2}
        \leq 
        \sqrt{\sum_{s'}\mu^\pi(s')
        \norm{Q^{\tilde \pi}_{s'}}_\infty^2
        \norm{\tilde \pi_{s'}- \pi_{s'}}_1^2}
        \leq H
        \norm{\tilde \pi- \pi}_{L^2(\mu^\pi), 1},
    \end{align*}
    and 
    \begin{align*}
        \sqrt{\sum_{s'}\mu^\pi(s')
        \abr{Q^{\tilde \pi}_{s'}, \tilde \pi_{s'}- \pi_{s'}}^2}
        \leq 
        \sqrt{\sum_{s'}\mu^\pi(s')
        \norm{Q^{\tilde \pi}_{s'}}_2^2
        \norm{\tilde \pi_{s'}- \pi_{s'}}_2^2}
        \leq A H
        \norm{\tilde \pi- \pi}_{L^2(\mu^\pi), 2}.
    \end{align*}
    The first inequality above gives
    \begin{align*}
    \av{V^{\tilde \pi} - V^{\pi} - \abr{\nabla V^\pi, \tilde \pi - \pi}}
    \leq \frac{\gamma H^3}{\sqrt \epsilon}
    \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 2}
    \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}
    \leq \frac{\gamma H^3}{\sqrt \epsilon}
    \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 1}^2,
    \end{align*}
    which proves the first claim, and the second one 
    \begin{align*}
    \av{V^{\tilde \pi} - V^{\pi} - \abr{\nabla V^\pi, \tilde \pi - \pi}}
    \leq \frac{\gamma A H^3}{\sqrt \epsilon}
    \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 2}
    \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 2}
    = \frac{\gamma A H^3}{\sqrt \epsilon}
    \norm{\tilde \pi - \pi}_{L^2(\mu^\pi), 2}^2,
    \end{align*}
    which proves the second and completes the proof.
\end{proof}




\subsection{Proof of \cref{thm:pmd_main}}
\label{sec:proof:thm:pmd_main}
The theorem makes use of the following.

\begin{lemma}\label{lem:epsgreedy_vgd}
	Assume $\Pi$ is $(C_\star, \epsvgd)$-VGD w.r.t. $\cM$, and consider the $\epsilon$-greedy exploratory version of $\Pi$, $\Pi^\epsilon \eqq \cbr{(1-\epsilon)\pi + \epsilon u \mid \pi \in \Pi}$, where $u_{s,a} \equiv 1/A$. Then $\Pi^\epsilon$ is $(C_\star, \delta)$-VGD with $\delta \eqq \epsvgd + 12 C_\star H^2 A \epsilon $. Concretely, for any $\pi^\epsilon \in \Pi^\epsilon$, we have:
	\begin{align*}
	C_\star \max_{\tilde \pi^\epsilon \in \Pi^\epsilon}\abr{\nabla V(\pi^\epsilon), \tilde \pi^\epsilon - \pi^\epsilon}
	\geq V\br{\pi^\epsilon} - V^\star(\Pi^\epsilon)
		- \epsvgd - 12 \epsilon C_\star H^2 A
	.
\end{align*}
\end{lemma}
We now prove our corollary and return to prove the above lemma later in \cref{sec:lem:epsgreedy_vgd}.
\begin{proof}[Proof of \cref{thm:pmd_main}]
    By \cref{lem:epsgreedy_vgd}, we have that $\Pi^\epsexpl$ is $(C_\star, \delta)$-VGD with $\delta=\epsvgd + 12 \epsexpl C_\star H^2 A$.
    Therefore, under the conditions of \cref{thm:opt_main} and the value difference \cref{lem:value_diff},
    \begin{align*}
		V(\pi^{K+1}) - V^\star(\Pi)
		&\leq V(\pi^{K+1}) - V^\star(\Pi^\epsexpl) 
			+ \av{V^\star(\Pi^\epsexpl) - V^\star(\Pi)}
		\\
		&= O\br{
            \frac{C_\star^2 L^2 c_1^2}{\eta K}
            + \br{C_\star D + c_1 L^2} \sqrt \epscritM
            + C_\star \epsactM
            + c_1 L \eta^{-1/2}\sqrt \epsactM
            + \delta
            },
	\end{align*}
    where $c_1 \eqq D + \eta M$.
    Next we apply \cref{lem:pmd_main} in the both cases considered, using the fact that for all $\pi\in \Pi^\epsexpl$, we have $\min_{s, a} \cbr{\pi_{s, a}} \geq \epsexpl/A$.
    In the euclidean case, we argue the following:
    \begin{enumerate}
        \item $R$ is $1$-strongly convex and has $1$-Lipschitz gradient w.r.t.~$\norm{\cdot}_2$.
        \item $\forall s, \norm{\pi_s - \tilde \pi_s}_2\leq D = 2$, $\norm{Q_s}_2\leq M=\sqrt{A H}$.
        \item The value function is 
        $\br{\beta \eqq \frac{A^{3/2} H^3}{\sqrt \epsexpl}}$-locally smooth w.r.t.~$\pi \mapsto \norm{\cdot}_{L^2(\mu^\pi), 2}$.
    \end{enumerate}
    Hence, $c_1=O(1)$, and \cref{lem:pmd_main} gives:
    \begin{align*}
        V(\pi^{K+1}) - V^\star(\Pi)
        &\lesssim \frac{C_\star^2}{\eta K}
        + C_\star\br{\sqrt{\epscritM} + \epsactM}
        + \eta^{-1/2}\sqrt \epsactM
        + \delta
        \\
        &=
        \frac{2 A^{3/2}H^3 C_\star^2}{\sqrt{\epsexpl} K}
        + C_\star\br{\sqrt{\epscritM} + \epsactM}
        + \frac{\sqrt{2 A^{3/2}H^3}}{\epsexpl^{1/4}}
        \sqrt \epsactM
        + \delta.
    \end{align*}
    Setting $\epsexpl = K^{-2/3}$, we obtain
    \begin{align*}
        V(\pi^{K+1}) - V^\star(\Pi)
        = O\br{
        \frac{C_\star^2 A^{3/2} H^3}{K^{2/3}}
        + C_\star\br{\sqrt{\epscritM} + \epsactM}
        + A H^2 K^{1/6} \sqrt{\epsactM}
        + \epsvgd}.
    \end{align*}
    In the negative-entropy case, we have the following.
    \begin{enumerate}
        \item $R$ is $1$-strongly convex and has a $\br{A/\epsexpl}$-Lipschitz gradient w.r.t.~$\norm{\cdot}_1$ (by Pinsker's inequality and \cref{lem:negent_smooth}).
        \item $\forall s, \norm{\pi_s - \tilde \pi_s}_1\leq D = 2$, $\norm{Q_s}_1\leq M=H$.
        \item The value function is 
        $\br{\beta \eqq \frac{A^{1/2} H^3}{\sqrt \epsexpl}}$-locally smooth w.r.t.~$\pi \mapsto \norm{\cdot}_{L^2(\mu^\pi), 1}$.
    \end{enumerate}
    Hence, $c_1=O(1)$, and \cref{lem:pmd_main} gives:
    \begin{align*}
        V(\pi^{K+1}) - V^\star(\Pi)
        &\lesssim \frac{C_\star^2 A^2}{\epsexpl^2 \eta K}
        + \br{C_\star + \frac{A^2}{\epsexpl^2}}\sqrt{\epscritM} 
        + C_\star\epsactM
        + \frac{A}{\epsexpl\sqrt\eta}\sqrt \epsactM
        + \delta
        \\
        &=
        \frac{2 A^{5/2}H^3 C_\star^2}{\epsexpl^{5/2} K}
        + \br{C_\star + \frac{A^2}{\epsexpl^2}}\sqrt{\epscritM} 
        + C_\star\epsactM
        + \frac{A^{3/2}H^3}{\epsexpl^{5/4}}\sqrt \epsactM
        + \delta.
    \end{align*}
    We now set $\epsexpl = K^{-2/7} A^{2/5}$ in order to balance the terms,
    \begin{align*}
        \frac{2 A^{5/2}H^3 C_\star^2}{\epsexpl^{5/2} K}
        + C_\star H^2 A \epsexpl,
    \end{align*}
    which yields,
    \begin{align*}
        &V(\pi^{K+1}) - V^\star(\Pi)
        \\
        &=O\br{
        \frac{C_\star^2 A^{3/2} H^3 }{K^{2/7}}
        + \br{C_\star + A^2 K^{4/7}}\sqrt{\epscritM} 
        + C_\star\epsactM
        + A^{3/2}H^3 K^{5/14}\sqrt \epsactM
        + \epsvgd},
    \end{align*}
    and completes the proof.
\end{proof}





\subsection{Proof of \cref{lem:epsgreedy_vgd}}
\label{sec:lem:epsgreedy_vgd}
\begin{lemma}\label{lem:om_valuediff}
	For any MDP $\cM=(\cS, \cA, \P, \l, \gamma, \rho_0)$ and two policies $\pi, \tilde \pi\colon \cS \to \Delta(\cA)$, we have:
	\begin{align*}
\norm{\mu^{\tilde \pi} - \mu^\pi}_1
	\leq H \norm{\tilde \pi - \pi}_{L^1(\mu^\pi), 1}.
	\end{align*}
\end{lemma}
\begin{proof}
	Consider the MDP $\cM_x = (\cS, \cA, \P, r_x, \gamma, \rho_0)$; i.e., the same MDP $\cM$ but with reward function defined by $r_x(s, a) \eqq \I\cbr{s = x}$.
	Let $V_{\cdot;r_x}, Q_{\cdot, \cdot; r_x}$ denote its value and action-value functions, respectively.
	We have 
\begin{align*}
	Q^{\tilde \pi}_{s,a;r_x} 
	&= \E\sbr{\sum_{t=0}^\infty \gamma^t \I\cbr{s_t = x} \mid s_0=s, a_0=a, \tilde \pi}
	\\
	&= \sum_{t=0}^\infty \gamma^t \Pr\br{s_t = x \mid s_0=s, a_0=a, \tilde \pi}
	\\
	&= \I\cbr{s=x} + \sum_{t=1}^\infty \gamma^t \Pr\br{s_t = x \mid s_0=s, a_0=a, \tilde \pi}
	\\
	&= \I\cbr{s=x} + \gamma \sum_{t=1}^\infty \gamma^{t-1} \Pr\br{s_t = x \mid s_1 \sim \P_{sa}, \tilde \pi}
	\\
	&= \I\cbr{s=x} + \gamma \mu_{\P_{sa}}^{\tilde \pi}(x).
\end{align*}
Hence,
\begin{align*}
	\mu^{\tilde \pi}(x) - \mu^\pi(x)
	&= V^{\tilde \pi}_{\rho_0; r_x} - V_{\rho_0; r_x}^\pi
	\\
	&= H \E_{s \sim \mu^\pi}
		\abr{Q_{s;r_x}^{\tilde \pi}, \tilde \pi_s -\pi_s}
	\tag{\cref{lem:value_diff}}
	\\
	&= H \E_{s \sim \mu^\pi}\sbr{
		\sum_a \br{\I\cbr{s=x} + \gamma \mu_{\P_{sa}}^{\tilde \pi}(x)}
		\br{\tilde \pi_{sa} -\pi_{sa}}}
	\\
	&= H \E_{s \sim \mu^\pi}\sbr{
	\sum_a \I\cbr{s=x} 
		\br{\tilde \pi_{sa} -\pi_{sa}}
		+
		\gamma\sum_a \mu_{\P_{sa}}^{\tilde \pi}(x)
		\br{\tilde \pi_{sa} -\pi_{sa}}}
	\\
	&= \gamma H\E_{s\sim \mu^\pi}\sbr{\sum_a \mu_{\P_{sa}}^{\tilde \pi}(x)
		\br{\tilde \pi_{sa} -\pi_{sa}}}.
\end{align*}
Therefore, 
\begin{align*}
	\sum_x \av{\mu^{\tilde \pi}(x) - \mu^\pi(x)}
	&=
	\gamma H \sum_x \av{\E_{s\sim \mu^\pi}\sbr{\sum_a \mu_{\P_{sa}}^{\tilde \pi}(x)
		\br{\tilde \pi_{sa} -\pi_{sa}}}}
	\\
	&\leq 
	\gamma H \sum_x 
		\E_{s\sim \mu^\pi}\sbr{\sum_a \mu_{\P_{sa}}^{\tilde \pi}(x)
		\av{\tilde \pi_{sa} -\pi_{sa}}}
	\\
	&=
	\gamma H  
		\E_{s\sim \mu^\pi}\sbr{\sum_a \br{\sum_x\mu_{\P_{sa}}^{\tilde \pi}(x)}
		\av{\tilde \pi_{sa} -\pi_{sa}}}
	\\
	&=
	\gamma H  
		\E_{s\sim \mu^\pi}\sbr{\sum_a
		\av{\tilde \pi_{sa} -\pi_{sa}}}
	\\
	&=
	\gamma H \norm{\tilde \pi -\pi}_{L^1(\mu^\pi), 1},
\end{align*}
and the proof is complete.
\end{proof}


\begin{proof}[Proof of \cref{lem:epsgreedy_vgd}]
Let $\pi^\epsilon \in \Pi^\epsilon$, and set $\pi\in \Pi$ to be the non-exploratory version of $\pi^\epsilon$. We have, by \cref{lem:value_diff}:
\begin{align}\label{eq:appxvgd_valuediff}
	V^\pi - V^{\pi^\epsilon}
	= \E_{s\sim\mu^\pi}\abr{Q^{\pi^\epsilon}_s, \pi_s - \pi^\epsilon_s}
%	\\
%	&= \E_{s\sim\mu^\pi}\abr{Q^{\pi^\epsilon}_s, \pi_s - (1-\epsilon)\pi_s - \epsilon u}
%	\\
	= \epsilon \E_{s\sim\mu^\pi}\abr{Q^{\pi^\epsilon}_s, \pi_s - u}
%	\\
	\leq 2 \epsilon H.
\end{align}
In addition,
\begin{align*}
	\norm{\nabla V\br{\pi^\epsilon} - \nabla V(\pi)}_1
	&=
	\sum_s \norm{\mu^{\pi^\epsilon}(s) Q^{\pi^\epsilon}_s
	- \mu^{\pi}(s) Q^{\pi}_s}_1
	\\
	&\leq 
	\sum_s \norm{Q^{\pi^\epsilon}_s}_1
		\av{\mu^{\pi^\epsilon}(s) - \mu^\pi(s)} 
	+ \sum_s \mu^{\pi}(s) \norm{Q^{\pi}_s - Q^{\pi^\epsilon}_s}_1
	\\
	&\leq 
	A H \norm{\mu^{\pi^\epsilon} - \mu^\pi}_1
	+ \sum_s \mu^{\pi}(s) \norm{Q^{\pi}_s - Q^{\pi^\epsilon}_s}_1.
\end{align*}
To bound the first term, apply \cref{lem:om_valuediff}:
\begin{align*}
AH\norm{\mu^{\pi^\epsilon} - \mu^\pi}_1
	\leq A H^2 \norm{\pi^\epsilon - \pi}_{L^1(\mu^\pi), 1}
	\leq \epsilon A H^2 \norm{\pi - u}_{L^1(\mu^\pi), 1}
	\leq 2 \epsilon A H^2.
\end{align*}
To bound the second term, we have for any $\tilde \pi$:
\begin{align*}
	\sum_s \mu^{\pi}(s) \norm{Q^{\pi}_s - Q^{\tilde \pi}_s}_1
	&\leq
	H^2\sum_s \mu^\pi(s)\sum_a\sum_{s'} \mu^\pi_{\P_{s, a}}(s')
	\norm{\tilde \pi_{s'} - \pi_{s}}_1
	\\
	&=
	H^2 A\sum_{s'} \sum_{s,a} \mu^\pi(s)\frac1A\mu^\pi_{\P_{s, a}}(s')
	\norm{\tilde \pi_{s'} - \pi_{s}}_1
	\\
	&=
	H^2 A \norm{\tilde \pi - \pi}_{L^1(\nu), 1},
\end{align*}
where $\nu\in \R^\cS$ is defined by 
$$\nu(s') = \sum_{s,a} \mu^\pi(s)\frac1A\mu^\pi_{\P_{s, a}}(s').$$
By the law of total probability, $\nu\in \Delta(\cS)$ is in fact a state probability measure. Hence, we obtain 
\begin{align*}
	\sum_s \mu^{\pi}(s) \norm{Q^{\pi}_s - Q^{\pi^\epsilon}_s}_1
	&\leq
	H^2 A \norm{\pi^\epsilon - \pi}_{L^1(\nu), 1}
	=
	\epsilon H^2 A \norm{\pi - u}_{L^1(\nu), 1}
	\leq 
	2 \epsilon H^2 A.
\end{align*}
The bounds on both terms, combined with the previous display now yields
\begin{align}\label{eq:appxvgd_graddiff}
	\norm{\nabla V\br{\pi^\epsilon} - \nabla V(\pi)}_1
	&\leq 
	4\epsilon A H^2.
\end{align}
We now turn to apply \cref{eq:appxvgd_valuediff,eq:appxvgd_graddiff} to establish the claimed VGD condition.
Let $\pi^\epsilon\in \Pi^\epsilon$ be an arbitrary $\epsilon$-greedy policy and $\pi \in \Pi$ the non-exploratory version of $\pi^\epsilon$. The assumption that $\Pi$ is $(C_\star, \epsvgd)$-VGD implies
\begin{align*}
	\max_{\tilde \pi \in \Pi}\abr{\nabla V(\pi), \tilde \pi - \pi}
	\geq \frac1{C_\star}\br{V(\pi) - V^\star(\Pi) - \epsvgd}.
\end{align*}
Let $\tilde \pi\in \Pi$ be the policy maximizing the LHS, and $\tilde \pi^\epsilon=(1-\epsilon)\tilde \pi + \epsilon u\in \Pi^\epsilon$ its corresponding greedy exploration policy. We have,
\begin{align*}
	\abr{\nabla V(\pi^\epsilon), \tilde \pi^\epsilon - \pi^\epsilon}	
	&=(1-\epsilon)
		\abr{\nabla V(\pi^\epsilon), \tilde \pi - \pi}	
	\\
	&=
	(1-\epsilon)
		\abr{\nabla V(\pi), \tilde \pi - \pi}
	+
	(1-\epsilon)
		\abr{\nabla V(\pi^\epsilon) - \nabla V(\pi), \tilde \pi - \pi}
	\\
	&\geq
	\frac{1-\epsilon}{C_\star}\br{V(\pi) - V^\star(\Pi) - \epsvgd}
	+
	(1-\epsilon)
		\abr{\nabla V(\pi^\epsilon) - \nabla V(\pi), \tilde \pi - \pi}
	\\
	&\geq
	\frac{1}{C_\star}\br{V(\pi) - V^\star(\Pi) - \epsvgd}
	-
	2\norm{\nabla V(\pi^\epsilon) - \nabla V(\pi)}_1
	\\
	&\geq
	\frac{1}{C_\star}\br{V(\pi) - V^\star(\Pi) - \epsvgd}
	-
	8\epsilon H^2 A
	\tag{\cref{eq:appxvgd_graddiff}}
	\\
	&\geq
	\frac1{C_\star}
	\br{V(\pi^\epsilon) - V^\star(\Pi) - \epsvgd - \av{V(\pi^\epsilon) - V(\pi)}}
	-
	8\epsilon H^2 A
	\\
	&\geq
	\frac1{C_\star}
	\br{V(\pi^\epsilon) - V^\star(\Pi) - \epsvgd - 2\epsilon H}
	-
	8\epsilon H^2 A
	\tag{\cref{eq:appxvgd_valuediff}}
	\\
	&\geq
	\frac1{C_\star}
	\br{V(\pi^\epsilon) - V^\star(\Pi^\epsilon) - \epsvgd - 4\epsilon H}
	-
	8\epsilon H^2 A.
	\tag{\cref{eq:appxvgd_valuediff}}
\end{align*}
(Indeed, we pay for the difference $V^\star(\Pi^\epsilon)-V^\star(\Pi)$ here, only to pay it again in the other direction later, but it is cleaner this way and results in only an extra constant numerical factor.)
Therefore, 
\begin{align*}
	C_\star \max_{\hat \pi^\epsilon \in \Pi^\epsilon}\abr{\nabla V(\pi^\epsilon), \hat \pi^\epsilon - \pi^\epsilon}
	&\geq V(\pi^\epsilon) - V^\star(\Pi^\epsilon) - \epsvgd - 12 \epsilon C_\star H^2 A
	,
\end{align*}
which completes the proof.
\end{proof}


\section{Constrained non-convex optimization for locally smooth objectives: Analysis}
\label{sec:opt_analysis}
In this section, we provide the full technical details for \cref{sec:opt_main}. Recall that we consider the constrained optimization problem:
\begin{align}%\label{opt:objective}
	\min_{x\in \cX} f(x),
\end{align}
where the decision set $\cX\subseteq \R^d$ is convex and endowed with a local norm $x \mapsto \norm{\cdot}_x$ (see \cref{def:local_norm}), and access to the objective is granted through an approximate first order oracle, as defined in \cref{assm:opt_grad_oracle}.
We assume $f\colon \cX \to \R$ is differentiable and defined over an open domain $\dom f\subseteq \R^d$ that contains $\cX$.
We consider an approximate version of the algorithm described in \cref{eq:alg_opt_omd}, hence for the sake of rigor, we introduce some additional notation.
Given any convex regularizer $h\colon \R^d \to \R$, we define a Bregman proximal point update with step-size $\eta > 0$ by:
\begin{align}
	T_{\eta}(x; h) \eqq 
        \argmin_{y\in \cX} 
    		\cbr{\abr{\hgrad{f}{x}, y} + \frac1\eta B_{h}(y, x)},
\end{align}
and the set of $\epsopt$-approximate solutions by:
\begin{align}\label{def:bregman_prox_approx}
	T_{\eta}^{\epsopt}(x; h) 
        \eqq 
	\cbr{x^+ \in \cX \mid \forall z\in \cX: 
		\abr{\hgrad{f}{x} + \frac1\eta\nabla B_h(x^+, x), z - x^+} \geq -\epsopt}.
\end{align}
Now, the approximate version of our algorithm is given by:
\begin{align}\label{alg:opt_omd}
    k=1, \ldots, K: \quad 
    		x_{k+1} &\in T_{\eta}^{\epsopt}(x_k; R_{x_k}).
\end{align}
We recall our main theorem below.
\begin{theorem*}[restatement of \cref{thm:opt_main}]%\label{thm:opt_main}
    Suppose that $f$ is $(C_\star, \epsvgd)$-VGD as per \cref{def:gradient_dominance}, and that $f^\star \eqq \min_{x\in \cX}f(x) > -\infty$. 
    Assume further that:
    \begin{enumerate}[label=(\roman*)]
        \item The local regularizer $R_x$ is $1$-strongly convex and has an $L$-Lipschitz gradient w.r.t.~$\norm{\cdot}_x$ for all $x\in \cX$.
        
        \item For all $x\in \cX$, 
        $\max_{u,v\in \cX}\norm{u - v}_x\leq D$, and
        $\norm{\nabla f(x)}^*_x \leq M$.
        
        \item $f$ is $\beta$-locally smooth w.r.t.~$x \mapsto \norm{\cdot}_{x}$. 
    \end{enumerate}
    Then, for the algorithm described in \cref{alg:opt_omd} we have following guarantee when $\eta\leq 1/(2\beta)$:
	\begin{align*}
		f(x_{K+1}) - f^\star 
		&= O\br{\frac{C_\star^2 L^2 c_1^2}{\eta K}
            + \br{C_\star D + c_1 L^2}\epsgrad
        + C_\star \epsopt 
        + c_1 L \eta^{-\frac12}\sqrt{\epsopt}
            + \epsvgd
            }
        \end{align*}
	where $c_1 \eqq D + \eta M$.
\end{theorem*}
Evidently, since the objective is not convex, standard mirror descent analyses are inadequate, and our analysis takes the proximal point update view of \cref{alg:opt_omd}. While there are numerous prior works that investigate non-euclidean proximal point methods for both convex and non-convex objective functions
(e.g., \citealp{tseng2010approximation,ghadimi2016mini,bauschke2017descent,lu2018relatively,zhang2018convergence,fatkhullin2024taming}; see also \citealp{beck2017first})
, non of them fit into the specific setting we study here. The notable differences being the use of \emph{local} smoothness (\cref{def:local_smoothness}), and the goal of seeking convergence in function values for a non-convex objective by exploiting variational gradient dominance. 

Our approach may be best described as one that adapts the work of \citet{xiao2022convergence} to the non-euclidean (and, ``local'') setup, but without relying on the objective having a Lipschitz gradient (note that we do not claim our definition of local smoothness implies a Lipschitz gradient condition). Since \citet{xiao2022convergence} relies on global smoothness of the objective w.r.t.~the euclidean norm (as was established by \citealp{agarwal2021theory}), their bounds inevitably scale with the size of the state-space $S$, which we want to avoid.
Given any convex regularizer $h\colon \R^d \to \R$, we define Bregman gradient mapping by:
\begin{align}\label{def:breg_gm}
    G_\eta(x, x^+; h)
    \eqq 
    \frac1\eta\br{\nabla h(x) - \nabla h(x^+)},
\end{align}
where $x^+\in \R^d$ should be interpreted as an approximate proximal point update step, i.e., $x^+ \in T_\eta^{\epsopt}(x; h)$.


\subsection{Bregman prox: Descent and Stationarity}
In this section we provide basic results relating to proximal point descent and stationarity conditions.
% We let $\cX \subseteq \R^d$ be convex, and consider an objective function
% $f\colon \cX \to \R$ 
% which is assumed to be differentiable and defined over an open domain $\dom f\subseteq \R^d$ that contains $\cX$. 
% For convenience, we recall below the definitions of (approximate) Bregman proximal point \cref{def:bregman_prox_approx} and Bregman gradient mapping \cref{def:breg_gm} w.r.t.~a regularizer $h$:
% \begin{align*}
% 	T_{\eta}^{\epsopt}(x; h) 
% 	&\eqq 
% 	\cbr{x^+ \in \cX \mid \forall z\in \cX: 
% 		\abr{\hgrad{f}{x} + \frac1\eta\nabla B_h(x^+, x), z - x^+} \geq -\epsopt}.
%     		\\
%     G_{\eta}(x, x^+; h)
%     &\eqq 
%     \frac1\eta\br{\nabla h(x) - \nabla h(x^+)}
% \end{align*}
% We remark that our definition of the gradient mapping above is slightly non-standard, and is such so as to accommodate errors in the update step. When the update is error free, i.e., $x^+ = T_\eta(x; h)$, the definition coincides with the standard one.
Our first lemma is (roughly) a non-euclidean version of a similar lemma given in \cite{nesterov2013gradient} for the euclidean case.
\begin{lemma}[Bregman proximal step descent]
\label{lem:prox_descent}
        Let $\norm{\cdot}$ be a norm, and 
	suppose $x\in \cX$ is such that
    \begin{align*}
        \forall y\in \cX: \av{f(y) - f(x) - \abr{\nabla f(x), y - x}}
        \leq 
        \frac\beta2\norm{y - x}^2.
    \end{align*}
    Assume further that:
    \begin{enumerate}
    	\item $0 < \eta \leq 1/(2\beta)$,
    	\item $h\colon \R^d \to \R$ is $1$-strongly convex and has an $L_h$-Lipschitz gradient, w.r.t. $\norm{\cdot}$.
    	\item $\norm{\hgrad{f}{x} - \grad{f}{x}}_*\leq \epsgrad$.
    \end{enumerate}
	Then, for $x^+ \in T_\eta^\epsopt(x; h)$ we have that:
	\begin{align*}
		f(x^+) \leq f(x) 
		-\frac{\eta}{ 2 L_h^2}\norm{G_\eta(x, x^+; h)}_*^2
	        +\eta \epsgrad \norm{G_\eta(x, x^+; h)}_* + \epsopt.	
	\end{align*}
\end{lemma}
\begin{proof}
Observe,
\begin{align*}
	f(x^+) 
	&\leq 
	f(x) + \abr{\nabla f(x), x^+ - x}
		+ \frac{\beta}{2}\norm{x^+ - x}^2
	\\
	&= 
	f(x) + \abr{\hgrad{f}{x}, x^+ - x}
		+ \frac{\beta}{2}\norm{x^+ - x}^2
		+ \abr{\grad{f}{x} - \hgrad{f}{x}, x^+ - x}
	\\
	&\leq 
	f(x) + \abr{\hgrad{f}{x}, x^+ - x}
		+ \frac{\beta}{2}\norm{x^+ - x}^2
		+ \epsgrad\norm{x^+ - x}
        \\
	&\leq 
	f(x) + \abr{\hgrad{f}{x}, x^+ - x}
		+ \frac{\beta}{2}\norm{x^+ - x}^2
		+ \eta\epsgrad\norm{G_\eta(x, x^+; h)}_*
        \tag{\cref{lem:breg_to_gm}}
        .
\end{align*}
Further, since $x^+\in T_\eta^\epsopt(x; h)$, for any $z\in \cX$, 
$$
	\abr{\hgrad{f}{x}, x^+ - z} 
	\leq 
	\abr{\frac1\eta\br{\nabla h(x^+) - \nabla h(x)}, z - x^+}
	+\epsopt.
$$
Hence,
\begin{align*}
	&f(x) + \abr{\hgrad{f}{x}, x^+ - x}
		+ \frac{\beta}{2}\norm{x^+ - x}^2
	\\
	&\leq
	f(x) + \frac1\eta\abr{\nabla h(x) -\nabla h(x^+), x^+ - x}
		+ \frac{\beta}{2}\norm{x^+ - x}^2
		+\epsopt
	\\
	&=
	f(x) 
        - \frac1\eta\br{B_h(x^+, x) + B_h(x, x^+)}
	 	+ \frac{\beta}{2}\norm{x^+ - x}^2
	 	+ \epsopt
	\\
	&\leq
	f(x) - \frac1\eta\br{B_h(x^+, x) + B_h(x, x^+)}
		+ \beta B_h(x^+, x)
		+ \epsopt
	\\
	&\leq
	f(x) - \frac1{2\eta}
        \br{B_h(x^+, x) + B_h(x, x^+)} + \epsopt,
\end{align*}
where the last line inequality follows from $\eta\leq 1/(2\beta)$.
Combining with the previous derivation, we now have
\begin{align}\label{eq:prox_descent_main}
	f(x^+)
	\leq 
		f(x) 
		- \frac1{2\eta}
        \br{B_h(x^+, x) + B_h(x, x^+)}
        + \epsopt
		+\eta \epsgrad \norm{G_\eta(x, x^+; h)}_*.
\end{align}
Finally, the assumption that $h$ has an $L_h$-Lipschitz gradient implies that 
\begin{align*}
	\eta^2\norm{G_\eta(x, x^+; h)}_*^2
	=
	\norm{\nabla h(x^+) - \nabla h(x)}_*^2
	\leq 
	L_h^2 \norm{x^+ - x}^2
	\leq 2L_h^2 B_h(x^+, x),
\end{align*}
and similarly $\eta^2\norm{G_\eta(x, x^+; h)}_*^2\leq 2L_h^2 B_h(x, x^+)$.
Hence,
\begin{align*}
	- \frac1{2\eta}
        \br{B_h(x^+, x) + B_h(x, x^+)}
        \leq 	
        - \frac{\eta}{ 2L_h^2}\norm{G_\eta(x, x^+; h)}_*^2,
\end{align*}
which completes the proof after combining with \cref{eq:prox_descent_main}.
\end{proof}

Our second lemma bounds the error in optimality conditions at any point $x\in \cX$ w.r.t.~the gradient mapping dual norm. We remark that here we do not assume a Lipschitz gradient condition holds for the objective function, as commonly done in similar arguments (e.g., \citealp{nesterov2013gradient,xiao2022convergence}).
\begin{lemma}\label{lem:prox_optcond}
	Let $\norm{\cdot}$ be a norm, and $x\in \cX$. Assume that:
	\begin{enumerate}
		\item $h\colon \R^d \to \R$ is $1$-strongly convex and has an $L_h$-Lipschitz gradient, w.r.t. $\norm{\cdot}$.
    	\item $\norm{\hgrad{f}{x} - \grad{f}{x}}_*\leq \epsgrad$,
    	\item $D>0$ upper bounds the diameter of $\cX$: $\max_{z,y\in \cX}\norm{z - y}\leq D$
    	\item $M>0$ upper bounds the gradient dual norm at $x$: $\norm{\nabla f(x)}_*\leq M$.
    \end{enumerate}
	Then, if $x^+ \in  T_\eta^\epsopt(x; h)$, it holds that:
	\begin{align*}
		\forall y\in \cX:
		\abr{\nabla f(x), x - y} 
		\leq 
		(D + \eta M)\norm{G_{\eta}(x, x^+; h)}_* + \epsgrad D + \epsopt.
	\end{align*}

\end{lemma}
\begin{proof}
		By assumption, we have for all $y\in \cX$,
	\begin{align*}
		\abr{\hgrad{f}{x} - G_{\eta}(x, x^+;h), y - x^+} 
		&\geq -\epsopt
		\\
		\iff
		\abr{\nabla f(x), x^+ - y} 
		&\leq 
		\abr{G_{\eta}(x, x^+;h), y - x^+}
		+ 
		\abr{\nabla f(x) - \hgrad{f}{x}, x^+ - y}
		+ \epsopt
		\\
		&\leq 
		\norm{G_{\eta}(x, x^+;h)}_* D + \epsgrad D
		+ \epsopt
		.
	\end{align*}
	Further,
	\begin{align*}
		\abr{\nabla f(x), x - y}
		&= 
		\abr{\nabla f(x), x^+ - y}
		+
		\abr{\nabla f(x), x - x^+}
		\\
		&\leq
		\norm{G_{\eta}(x, x^+;h)}_* D + \epsgrad D + \epsopt
		+ \abr{\nabla f(x), x - x^+}
		\\
		&\leq
		\norm{G_{\eta}(x, x^+;h)}_* D + \epsgrad D + \epsopt
		+ M \norm{x - x^+}
		\\
		&\leq
		\norm{G_{\eta}(x, x^+;h)}_* D + \epsgrad D + \epsopt
		+ \eta M \norm{G_\eta(x, x^+;h)}_*
		\tag{\cref{lem:breg_to_gm}}
		\\
		&\leq
		\br{D + \eta M}\norm{G_{\eta}(x, x^+;h)}_*  + \epsgrad D + \epsopt
		,
	\end{align*}
    which completes the proof.
\end{proof}

    \begin{lemma}\label{lem:breg_to_gm}
        For any norm $\norm{\cdot}$,
        and any $x, x^+\in \cX$, we have
        $
    	\norm{x-x^+}
    	\leq 
    	\eta \norm{G_{\eta}(x, x^+;h)}_*.
    	$
    \end{lemma}
    \begin{proof}
        For any $u,v$ it holds that (see e.g., \citealp{hiriart2004fundamentals}), 
    \begin{align*}
    	\frac12\norm{u-v}^2 \leq B_h(u, v)
    	= B_{h^*}(\nabla h(u), \nabla h(v))
    	\leq 
    	\frac12\norm{\nabla h(u) - \nabla h(v)}_*^2.
    \end{align*}
    The result now follows by the definition of $G_\eta(x, x^+; h)$.
\end{proof}

\subsection{Proof of \cref{thm:opt_main}}
We begin by establishing the objective satisfies a weak gradient mapping domination condition similar (but not identical, due to the differences mentioned above) to that considered in  \citet{xiao2022convergence}. 
\begin{definition}\label{def:weakgm_local}
	 We say that $f\colon \cX \to \R$ satisfies a weak gradient mapping domination condition w.r.t.~a local regularizer $R$ 
	 	if there exist $\delta, \omega > 0$ such that for all $x\in \cX$:
	 \begin{align*}
	 	\norm{G_{\eta}(x, x^+; h)}_{x}^*
	 	\geq \sqrt{2\omega}(f(x) - f^\star - \delta)
	 \end{align*}
\end{definition}
The lemma below establishes our objective function satisfies \cref{def:weakgm_local} with a suitable choice of parameters.
\begin{lemma}\label{lem:weakgm}
    Suppose that $f$ is $(C_\star, \epsvgd)$-VGD as per \cref{def:gradient_dominance}, and that $f^\star \eqq \min_{x\in \cX}f(x) > -\infty$. 
    Assume further that $R_x$ is $1$-strongly convex and has an $L$-Lipschitz gradient w.r.t. $\norm{\cdot}_x$ for all $x\in \cX$. Then, we have the following weak gradient mapping domination condition; 
    for all $x\in \cX, x^+\in T_{\eta}^{\epsopt}(x; R_x)$:
	\begin{align*}
		\norm{G_{\eta}(x, x^+; R_x)}_x^*
		\geq \sqrt{2\omega}\br{f(x) - f^\star - \delta},
	\end{align*}
	for $\omega\eqq \frac12\br{C_\star(D + \eta M)}^{-2}$, 
	$\delta\eqq \epsvgd + \epsopt C_\star + \epsgrad C_\star D$.
\end{lemma}
\begin{proof}
	Let $x\in \cX$, and
	apply \cref{lem:prox_optcond} with $\norm{\cdot}=\norm{\cdot}_x$ and $h=R_x$, to obtain:
	\begin{align*}
		\forall y\in \cX:
		\abr{\nabla f(x), x - y} 
		\leq (D+\eta M)\norm{G_{\eta}(x, x^+; R_x)}_x^*
            +\epsgrad D + \epsopt
		.
	\end{align*}
	Further, since $f$ is $(C_\star,\epsvgd)$-VGD, we have
	\begin{align*}
            \max_{y \in \cX}\abr{\nabla f(x), x - y}
        \geq \frac1{C_\star}\br{f(x) - f^\star - \epsvgd} .
        \end{align*}
	Combining both inequalities, the result follows.
\end{proof}
We are now ready to prove \cref{thm:opt_main}.
\begin{proof}[Proof of \cref{thm:opt_main}]
	In the sake of notational clarity, define:
	\begin{align}
		\cG_k \eqq \norm{G_\eta(x_k, x_{k+1}; R_{x_k})}^*_{x_k}.
	\end{align}
We begin by applying \cref{lem:prox_descent} for every $k \in [K]$ with 
$\norm{\cdot}=\norm{\cdot}_{x_k}$ and $h=R_{x_k}$, which implies,
\begin{align}\label{eq:opt_descent_main}
    f(x_{k+1}) - f(x_k)
    \leq  
    -\frac{\eta}{2L^2}\cG_k^2
        +\eta \epsgrad \cG_k
        +\epsopt.
\end{align}
Let us first assume that for all $k\in [K]$:
\begin{align}\label{eq:opt_small_grad_error}
    8L^2\epsgrad + \frac{4 L}{\sqrt\eta} \sqrt{\epsopt} \leq \cG_k.
\end{align}
Then \cref{eq:opt_descent_main} along with  \cref{lem:weakgm} gives
	\begin{align*}
		f(x_{k+1}) - f(x_k)
        \leq 
	-\frac{\eta}{4L_h^2}\cG_k^2
        \leq -\frac{\eta \omega}{ 4L^2}\br{f(x_k) - f^\star - \delta}^2,
	\end{align*}
	with $\omega\eqq \frac12\br{C_\star (D + \eta M)}^{-2}$ 
	and $\delta\eqq \epsvgd + \epsopt C_\star + \epsgrad C_\star D$.
    We proceed to define $E_k \eqq f(x_k) - f^\star$, and note that the above display implies $E_{k+1} \leq E_k$.
    Hence, we may assume that $E_k \geq 2\delta$ for all $k\in [K]$, otherwise the claim holds trivially.
    With this in mind, we now have,
	\begin{align*}
		E_{k+1} - E_k
		&\leq - \frac{\eta \omega}{4 L^2} (E_k - \delta)^2
            \leq 
            - \frac{\eta \omega}{16 L^2} E_k^2.
	\end{align*}
        Dividing both sides by $E_k E_{k+1}$ yields
	\begin{align*}
		\frac1{E_k} - \frac1{E_{k+1}}
		&\leq - \frac{\eta \omega}{16 L^2} \frac{E_k}{E_{k+1}}.
	\end{align*}
	Summing over $k=1, \ldots, K$ and telescoping the sum on the LHS, we obtain
	\begin{align*}
		\frac1{E_1} - \frac1{E_{K+1}}
		&\leq - \frac{\eta \omega}{16 L^2} \sum_{k=1}^K\frac{E_k}{E_{k+1}}
		\\
		\iff
		E_{K+1} - E_1
		&\leq - \frac{\eta \omega}{16 L^2} \br{E_{K+1}E_1}\sum_{k=1}^K\frac{E_k}{E_{k+1}}
		\leq - \frac{\eta \omega}{16 L^2} \br{E_{K+1}E_1}K,
	\end{align*}
	where the last inequality follows from the descent property $E_{k+1}\leq E_k$. Rearranging, we now have	
		\begin{align*}
			0\leq E_{K+1}
			&\leq E_1 \br{1- \frac{\eta \omega}{16 L^2} E_{K+1}K}
			\\
			\implies
			E_{K+1} &\leq \frac{16 L^2}{\eta \omega  K}
			=\frac{32 C_\star^2 L^2 \br{D + \eta M}^2}{\eta K},
		\end{align*}
        which completes the proof for the case that \cref{eq:opt_small_grad_error} holds for all $k\in [K]$.
        Assume now that this is not the case, and let $k_0\in [K]$ be the last iteration such that 
        \begin{align*}
            \cG_{k_0}
            < 8L^2\epsgrad + \frac{4 L}{\sqrt\eta} \sqrt{\epsopt}.
        \end{align*}
        Then by \cref{lem:prox_optcond},
        \begin{align*}
            E_{k_0} \leq 
            (D + \eta M)\cG_{k_0}
            + \epsgrad D + \epsopt 
            \leq 
            8(D + \eta M) \br{L^2 \epsgrad + L\sqrt{\epsopt/\eta}}
            + \epsgrad D + \epsopt,
        \end{align*}
        and therefore by \cref{eq:opt_descent_main},
        \begin{align*}
            E_{k_0+1} \leq E_{k_0} + \eta \epsgrad \cG_{k_0}
            = O\br{ (D + \eta M)\br{L^2 \epsgrad + L\sqrt{\epsopt/\eta}} }.
        \end{align*}
        Now, if $k_0 = K$ we are done. Otherwise, by the definition of $k_0$ we have that \cref{eq:opt_small_grad_error} holds for all $k\in [k_0+1, K]$, hence $E_{k+1} \leq E_k$ for all $k\geq k_0+1$. This implies that $E_{K+1} \leq E_{k_0 + 1}$, which completes the proof.
\end{proof}

\subsection{Convergence to stationary point without a VGD condition}
\label{sec:prox_convergence_stationary_point}
In this section, we include a proof that the proximal point algorithm we consider converges to a stationary point, also without assuming a VGD condition. The proof follows from standard arguments and is given for completeness; for simplicity, we provide analysis only for the error free case.
As an implication, we have that PMD converges to a stationary point in any MDP; this follows by combining the below theorem with \cref{lem:pmd_main} and \cref{lem:value_local_smoothness}, and proceeding with an argument similar to that of \cref{thm:pmd_main}.

\begin{theorem}\label{thm:opt_stationary_point}
    Suppose that $f^\star \eqq \min_{x\in \cX}f(x) > -\infty$, and assume:
    \begin{enumerate}[label=(\roman*)]
        \item The local regularizer $R_x$ is $1$-strongly convex and has an $L$-Lipschitz gradient w.r.t.~$\norm{\cdot}_x$ for all $x\in \cX$.
        
        \item For all $x\in \cX$, 
        $\max_{u,v\in \cX}\norm{u - v}_x\leq D$, and
        $\norm{\nabla f(x)}^*_x \leq M$.
        
        \item $f$ is $\beta$-locally smooth w.r.t.~$x \mapsto \norm{\cdot}_{x}$. 
    \end{enumerate}
	Consider an exact version of the proximal point algorithm \cref{eq:alg_opt_omd} with $\eta=1/(2\beta)$ where $\epsgrad=0$ and $x^{k+1}=T_\eta(x_k; R_{x_k})$ for all $k$.
	Then, after $K$ iterations, there exists $k^\star\in [K]$
	such that:
    \begin{align*}
    	\forall y\in \cX, \quad \abr{\nabla f(x_{k^\star}), y - x_{k^\star}}
    	\geq - \frac{2 (D+ \eta M)L\sqrt{\beta\br{f(x_1) - f(x^\star)}}}{\sqrt{K}},
    \end{align*}
\end{theorem}
\begin{proof}
In the sake of notational clarity, define:
	\begin{align*}
		\cG_k \eqq \norm{G_\eta(x_k, x_{k+1}; R_{x_k})}^*_{x_k}.
	\end{align*}
We begin by applying \cref{lem:prox_descent} for every $k \in [K]$ with 
$\norm{\cdot}=\norm{\cdot}_{x_k}$ and $h=R_{x_k}$, which implies,
\begin{align}
    f(x_{k+1}) - f(x_k)
    \leq  
    -\frac{\eta}{2L^2}\cG_k^2.
\end{align}
	Now,
    \begin{align*}
        f(x_{K+1}) - f(x_1)
        = 
        \sum_{k=1}^K
        f(x_{k+1}) - f(x_k)
        \leq - \frac{\eta}{2 L^2}\sum_{k=1}^K \cG_k^2,
    \end{align*}
    thus, rearranging and bounding $f(x_{K+1}) \geq f(x^\star)$ gives
    \begin{align*}
        \frac1K\sum_{t=1}^T \cG_k^2
        \leq 
        \frac{2 L^2\br{f(x_1) - f(x^\star)}}{\eta K}.
    \end{align*}
    Hence, it must hold for $k^\star\eqq \argmin_{k}\cG_k^2$; 
    \begin{align*}
	\cG_{k^\star}^2
      \leq  \frac{2 L^2\br{f(x_1) - f(x^\star)}}{\eta K}.
    \end{align*}
    We now apply \cref{lem:prox_optcond} to conclude,
    \begin{align*}
    	\forall y\in \cX, \quad \abr{\nabla f(x_{k^\star}), x_{k^\star} - y}
    	\leq \frac{ (D+ \eta M)L\sqrt{2\br{f(x_1) - f(x^\star)}}}{\sqrt{\eta K}},
    \end{align*}
    which implies the required result.
    
\end{proof}




\section{Policy Classes with Dual Parametrizations}
\label{sec:dual_pc}
In general, solving the following OMD problem in some state $s\in \cS$,
\begin{align}\label{eq:dualpc_1}
	\pi^{k+1}_s \gets \argmin_{p\in \Delta(\cA)} \abr{Q^k_s, p} + \frac1\eta B_R(p, \pi^k_s)
\end{align}
is equivalent to the following two updates:
\begin{align*}
	\nabla R(\tilde \pi_s^{k+1}) &\gets \nabla R(\pi^k_s) - \eta Q^k_s
	\\
	\pi^{k+1}_s &= \Pi_{\Delta(\cA)}^R\br{\tilde \pi_s^{k+1}}.
\end{align*}
Let us denote the composition of the dual-to-primal mirror-map and the projection:
$$
	P_R(y) \eqq \Pi_{\Delta(\cA)}^R\br{\nabla R^*(y)},
$$
and note that
$$
	\pi_s^{k+1} = P_R(\nabla R(\tilde \pi_s^{k+1})).
$$

When we are in a non-tabular setup and have a non-complete policy class $\Pi\neq \Delta(\cA)^\cS$, we cannot update each state independently according to \cref{eq:dualpc_1}.
%
There are however a number of places we can "intervene" in the policy class representation to derive slightly different update procedures based on the dual variables. The PMD step in its general form is given by:
\begin{align}\label{eq:dualpc_2}
	\pi^{k+1} \gets \argmin_{\pi \in \Pi} \E_{s\sim\mu^k}\sbr{\abr{Q^k_s, \pi_s} + \frac1\eta B_R(\pi_s, \pi^k_s)}
\end{align}

Without making any assumptions regarding the parametric form of $\Pi$, we cannot decompose \cref{eq:dualpc_2} into meaningful dual space steps. We discuss next two types of policy class parameterizations and the update steps associated with them.

\subsection{Generic dual parameterizations}
This is the approach taken in \cite{alfano2023novel} (see also the followup \citealp{xiong2024dual}), and perhaps the most general one that allows for an explicit dual space update as well as leads to an approximate solution of \cref{eq:dualpc_2} that satisfies approximate optimality conditions in the complete-class setting. Consider a parametric function class $\cF_\Theta \eqq \cbr{f_\theta \in \R^{SA} \mid \theta \in \Theta}$, and the policy class:
$$
	\Pi(\cF) \eqq \cbr{ \pi^f \mid  f \in \cF_\Theta},
	\quad \text{ where } \pi^f_s \eqq P_R(f_s),\;\forall s\in \cS.
$$
Then, to solve \cref{eq:dualpc_2} we can proceed by:
\begin{align*}
	f^{k+1} &\gets \argmin_{f \in \cF}\E_{s\sim \mu^k}\sbr{
		\norm{f_s - \nabla R(\pi^k_s) - \eta Q^k_s}_2^2}
	\\
	\pi^{k+1} &\gets \text{ the policy defined by }	\pi^{k+1}_s = P_R(f^{k+1}_s)
	\tag{A}
\end{align*}
\subsection{The log-linear policy class}
This is a special case of the one discussed in the previous sub-section. In general, when we try to approximate the true solution of the unconstrained mirror descent step in a specific state:
$$
	f_s \approx \nabla R(\pi^k_s) - \eta Q_s^k,
$$
we need to overcome two sources of error; one from the previous policy dual variable and one from the $Q$ function. More specifically, in general we have $\nabla R(\pi^k) \notin \cF$ and $Q^k \notin \cF$. (For $\pi\in \R^{SA}$ we define $\nabla R(\pi)_s \eqq \nabla R(\pi_s)$.)
In the special case that our function class $\cF$ can represent $\nabla R(\pi)$ perfectly for all $\pi\in \Pi(\cF)$ and is closed to linear combinations, we can focus our attention on approximating the $Q$ function. Now, we may proceed by the following special case of $(A)$:
\begin{align*}
	\widehat Q^k &\gets \argmin_{\widehat Q \in \cF}\E_{s, a\sim \mu^k}\sbr{
		\br{\widehat Q_{s, a} - Q^k_{s, a}}^2}
	\\
	f^{k+1} &\gets \nabla R(\pi^k) - \eta \widehat Q^k
	\\
	\pi^{k+1} &\gets \text{ the policy defined by }	\pi^{k+1}_s = P_R(f^{k+1}_s).
	\tag{B}
\end{align*}
Let $\phi_{s, a} \in \R^p$ be given feature vectors, and let $\phi_{s} \eqq [\phi_{s, a_1} \cdots \phi_{s, a_A}]\in \R^{p \times A}$, and consider
the log-linear policy class:
\begin{align*}
	\Pi &\eqq \cbr{\pi^\theta \mid \theta \in \R^p}
	\\
	 \text{ where }\forall s\in \cS,\; \pi^\theta_s &\eqq P_R(\phi_s\T\theta)
	 = \frac{e^{\phi_s\T \theta}}{\sum_{a}e^{\phi_{s, a}\T \theta}}.
\end{align*}
Note that:
\begin{enumerate}
    \item This is the class $\Pi(\cF)$ for $\cF = \cbr{\theta \mapsto \br{(s, a) \mapsto \phi_{s,a}\T \theta}}$.
    \item This is precisely a case where $\cF$ can model $\nabla R(\pi)$ if $R$ is the negative entropy regularizer. 
\end{enumerate}
Here, we may proceed as follows:
\begin{align*}
	w^{k} &\gets \argmin_{w\in \R^p}
		\E_{s, a \sim \mu^k}\sbr{\br{\phi_{s, a}\T w - Q_{s,a}^k}^2}
	\\
	\theta^{k+1} &\gets \theta^k - \eta w^k
	\\
	\pi^{k+1} &\gets \text{ the log-linear policy defined by } \theta^{k+1}
\end{align*}
The above can be seen as a special case of $(B)$, by considering the induced updates in state-action space:
\begin{align*}
\widehat Q^{k} &=
	\argmin_{\widehat Q \in \cF}
		\E_{s, a \sim \mu^k}\sbr{\br{\widehat Q_{s, a}  - Q_{s,a}^k}^2}
		= (s, a) \mapsto \phi_{s, a}\T w^k
	\\
	f^{k+1}_s
	&= \nabla R(\pi^k_s) - \eta \widehat Q^k_s
	\\
	&= \log(e^{\phi_s\T\theta^k}) - \log(Z^k_s)\boldsymbol 1 - \eta \widehat Q^k_s
	\\
	&= \phi_s\T\theta^k - \eta \widehat Q^k_s - \log(Z^k_s)\boldsymbol 1
	\\
	\pi^{k+1} &\gets  \text{ the policy defined by }\pi^{k+1}_s 
	= P_R(f^{k+1}_s) 
	= \frac{e^{\phi_s\T \theta^{k+1}}}{\sum_{a}e^{\phi_{s, a}\T \theta^{k+1}}}.
\end{align*}
Note that in the above,
$$
	f^{k+1}_{s, a} = \phi_{s, a}\T \theta^k - \eta \phi_{s, a}\T w^k - \log (Z^k_s),
$$
and that $Z^k_s$ is the same for all actions in $s$, hence makes no difference after the projection step..




