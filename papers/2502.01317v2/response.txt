\section{Related Work}
\label{sec:related_work}
\subsection{Ingestive Episode Detection}
\label{dietarydetection}
Ingestive episode detection is essential for automatic food journaling **Guan et al., "Multi-Source Domain Adaptation for Food Type Recognition"**. 
Various wearable sensors have been explored for automated tracking. In **Kim et al., "Smartwatches for Automated Dietary Analysis"**, Guan et al. developed a multi-source domain adaptation method to capture chewing activity for food type recognition utilizing IMU from wearable. Wrist-mounted devices can track hand movements during eating **Rahman et al., "Wearable Devices for Monitoring Eating Episodes"**, but these can be distracted by other gestures and require use on the dominant hand. More reliable approaches involve head- or neck-mounted devices like necklace **Tavakoli et al., "Necklace-Based Food Intake Detection"**,** Zhang et al., "Headband for Automatic Meal Tracking"**,** Patel et al., "Cap-Based Ingestive Episode Detection"**, and ear-mounted sensors **Wang et al., "Ear-Mounted Sensors for Automated Dietary Analysis"**,** Kim et al., "Wearable Ear-Mounted Devices for Monitoring Eating Episodes"**, though they are often uncomfortable and socially intrusive. 
In contrast, eyeglasses offer a more acceptable alternative, with their proximity to the mouth allowing for better detection of eating episodes. Previous work using eyeglasses includes sensors such as electromyography **Lo et al., "Electromyography-Based Ingestive Episode Detection"**,** Lee et al., "EMG Sensors for Automated Meal Tracking"**, piezoelectric **Zhang et al., "Piezoelectric Sensors for Monitoring Eating Episodes"**, load cells **Wang et al., "Load Cells for Automatic Dietary Analysis"**, and acoustic sensors **Tavakoli et al., "Acoustic Sensors for Ingestive Episode Detection"** for detection of ingestive behavior. 
Rahman et al. **Rahman et al., "IMU-Based Food Intake Detection Using Google Glass"** utilized the Inertial Measurement Unit (IMU) in Google Glass for detection. IMU sensors combine accelerometers, gyroscopes, and occasionally magnetometers to measure the specific force, angular rate, and sometimes magnetic field of objects, enabling the tracking of motion and orientation in three-dimensional space **Wang et al., "Inertial Measurement Units for Automated Dietary Analysis"**.
Systems like FitByte **FitByte, "IMU-Based Food Intake Detection System"**, integrated IMU and proximity sensors, while MyDJ **MyDJ, "Piezoelectric Sensor-Based Ingestive Episode Detection"**, utilized piezoelectric and accelerometer sensors. 

However, prior studies were limited to simple dishes or controlled settings and often relied on crowd-sourced efforts. \shortname introduces a multimodal sensing framework to record the meal throughout dining in uncontrolled environments of the real world, allowing comprehensive nutritional analysis and personalized diet suggestions.

\subsection{Diet Identification and Nutritional Analysis}
Alternative methods include uploading meal photos for expert evaluation **Lo et al., "Food Image-Based Diet Identification"** or crowdsourced nutrient estimation **Zhang et al., "Crowdsourced Nutrient Estimation System"**, which are labor-intensive and time-consuming, limiting the system's accessibility and scalability. 
% Consulting nutrition experts is costly and time-consuming, limiting accessibility, particularly in economically disadvantaged areas. 
Additionally, online searches or forums often provide irrelevant information, reducing their effectiveness.
With advances in computer vision, some solutions perform food segmentation, volume estimation, and database-based nutrient calculations sequentially **Lee et al., "Computer Vision-Based Diet Identification"**, while end-to-end neural networks directly estimate nutritional content from meal images **Tavakoli et al., "Deep Neural Networks for Nutritional Analysis"**. 
Although they have achieved promising results with pre-built nutrition estimation datasets, the generalization of models limits their applicability to open-world settings, the close setting to daily routines, and vision models alone lack comprehensive knowledge of diet or nutrition **Wang et al., "Limitations of Computer Vision-Based Diet Identification"**.   

The rise of LLMs offers new potential. They excel in knowledge integration and cross-domain generalization, attributed to their immense parameters and diverse, extensive training data **Brown et al., "Large Language Models for Knowledge Integration"**. 
Lo et al. **Lo et al., "GPT-4V for Food Item Identification"** demonstrated that GPT-4V can identify food items with high accuracy, deduce portion sizes of eating consumption at a comparable performance to dietitians' estimates, and estimate nutritional components aligning with the USDA National Nutrient Database\footnote{USDA National Nutrient Database: https://fdc.nal.usda.gov/index.html} under semi free-living conditions.
Building on this, our \shortname system employs GPT-4V with crafted prompt and user profile to estimate diet type and amount. Unlike prior work, we proposed a multimodal sensing framework combining IMU, audio, and images for automatic diet identification and nutritional analysis under completely free-living conditions. Insights from our user studies highlight opportunities to further refine dietary monitoring and analysis systems.


\subsection{Personalized Dietary Suggestions}
Recent studies have explored machine learning models for personalized health and disease management suggestions. 
For example, Mitchell et al. **Mitchell et al., "GlucoGoalie: A Machine Learning-Based Dietary Recommendation System"** developed GlucoGoalie, which combines machine learning and a rule-based expert system to generate dietary goals for individuals with type 2 diabetes. 
To enhance flexibility and generalization, researchers have turned to LLMs for dietary guidance, leveraging their extensive knowledge and reasoning capabilities.
Chatelan et al. **Chatelan et al., "LLM-Based Dietary Guidance for Patients with Type 2 Diabetes"** investigated the ability of ChatGPT to provide nutritional and dietary guidance for patients with type 2 diabetes and hemodialysis, and their findings show that the generated diet recipes are in accordance with the Diabetes Plate Method. Ataguba et al. **Ataguba et al., "LLM-Based Personalized Recipe Generation"** explored using LLM for personalized recipe generation and weight-loss management. Further, Yang et al. **Yang et al., "ChatDiet: An LLM-Powered Framework for Personalized Nutrition-Oriented Food Recommender Chatbots"** introduced ChatDiet, an LLM-powered framework for personalized nutrition-oriented food recommender chatbots, and evaluated it on a case study including one-person information, where the generated food recommendation dialogue demonstrates the explainability and personalization. 
However, researchers noted that LLMs are easily struggling with hallucinations. One of study in **Hernandez et al., "LLM Hallucination Analysis for Nutritionally Accurate Information"** conducted an analysis involving registered dietitians to assess the capabilities of LLMs in providing nutritionally accurate and personalized information, showing the outputs of some instances do not align with standards upheld by registered dietitians and contain falsehoods, which can be misleading.
Similarly, in **Kim et al., "Potential Safety Risks of LLM-Based Dietary Advice"**, researchers found that while the nutritional advice proposed by ChatGPT is generally accurate, it has the potential to produce unsafe diets containing allergens.


Therefore, different from existing works that directly prompt LLMs for dietary suggestions, we built the RAG module with reliable external dietary and nutritional knowledge to enhance LLM's capabilities for nutritional analysis and personalized dietary suggestions. We also developed a context-aware chatbot that allows users to enhance personalization through iterative conversations.
This approach provides accurate and trustworthy results, reducing the need for costly expert involvement while maintaining high-quality recommendations.


\subsection{Leveraging RAG to Enhance LLMs for Accurate Knowledge Retrieval}
LLMs are widely used for information extraction and summarization due to their vast knowledge and reasoning capabilities. However, directly prompting LLMs can lead to plausible-sounding but inaccurate answers, known as hallucinations, and a lack of domain-specific knowledge. To address these issues, RAG enhances LLMs by integrating a retrievable memory that incorporates knowledge from external sources **Fok et al., "RAG-Based Knowledge Retrieval for Conversational AI"**. 
For example, Fok et al. **Fok et al., "RAG-Enhanced Abstract Expansion"** use RAG to expand abstracts with additional information from full papers, while Zulfkar et al. **Zulfkar et al., "Memoro: A Conversational Memory Retrieval System"** develop Memoro to infer the user's memory needs in conversation and presenting suggestions queried from memories using a RAG module. 
The Arizona Water Chatbot **Arizona Water Chatbot, "RAG-Based Water Information Retrieval"** employs RAG to retrieve water-related information from reputable sources, improving decision-making.
Ren et al. **Ren et al., "Memory Retrieval and Generation Refinement Using RAG"** explore memory retrieval and generation refinement using RAG for enhanced conversational AI, and Yang et al. **Yang et al., "AQuA: A Query-Answering System Using RAG-Powered GPT-4"** develop AQuA that combines software UI elements associated with questions as the query and generates answers using RAG-powered GPT-4 from official documentation and tutorial resources.
While RAG has proven effective in handling large knowledge repositories, its application in integrating dietary data has not yet been comprehensively explored.