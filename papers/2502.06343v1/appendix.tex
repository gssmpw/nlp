\section{Proofs}
\label{sec:proofs}

\subsection{Proof of Theorem \ref{th:feasibility}}

\begin{theorembox}
\begin{theorem*}
Let $\hat u$ and $\hat e$ be the nuisance function estimators such that they satisfy: $||\hat{\mu} - \mu \| \cdot \| \hat{e} - e \| = o_{\mathbb P}(n^{-1/2})$. Further, assume that positivity holds, i.e. there exists $ \epsilon > 0 $ such that $ \epsilon \leq e_0(W_i) \leq 1 - \epsilon $ for all $ W_i $ on the target experiment. Then, if the factual outcome model $g$ satisfies Equation \ref{eq:unbiased}, the AIPW estimator over the prediction-powered target experiment is asymptotically normal---i.e. it holds that:
$$
\sqrt n (\hat \tau - \tau) \to \mathcal N (0, V), 
$$
where $V$ denotes the asymptotic variance. 



    
\end{theorem*}
\end{theorembox}
\begin{proof}
    %Prove first strong convergence (combining PPI + AIPW + cross validation + Invariance assumption and probably some additional guarantees on the factual outcome estimation),  and then provide valid confidence interval using CLT. \pd{first sketch below}
%\todo{(i) add proof that alg. 1 enforces reference fairness and unconfounding on the reference population; (ii) replace target oracle g with a generic g satisfying Condition \ref{eq:eff+unconf} (it comes from the previous point and Th. \ref{alg:ppci}), (iii)
%adapt to new variables definition.}
Our goal is to prove that the following estimator is asymptotically normal:
\begin{equation*}
\begin{split}
    \hat{\tau} = \frac{1}{n^{e_2}} \sum_{i \in \mathcal{D}^{e_2}} \Bigg[ \hat{\mu}(W_i, 1) - \hat{\mu}(W_i, 0) &+ \frac{T_i}{\hat{e}(W_i)}(g(X_i) - \hat{\mu}(W_i, 1)) \\&- \frac{1-T_i}{1-\hat{e}(W_i)}(g(X_i) - \hat{\mu}(W_i, 0)) \Bigg]
\end{split}
\end{equation*}
where $\hat{\mu}(w, t)$ is an estimator of $\EE_{\mathbb{P}^{e_2}}[ g(X) |W=w, T=t]$ over the prediction-powered target sample $\mathcal{D}^{e_2}$, $\hat{e}(w)$ is an estimator of the propensity score $\mathbb{P}_{\mathbb{P}^{e_2}}(T=1|W=w)$ over the target sample $\mathcal{D}^{e_2}$, and without ambiguity we dropped the superscript $e_2$ to the random variables to simplify the language.

Let us define the influence function of the estimator $$\phi(O_i; \mu, e, g) = \mu(W_i, 1) - \mu(W_i, 0) + \frac{T_i}{e(W_i)}(g(X_i) - \mu(W_i, 1)) - \frac{1-T_i}{1-e(W_i)}(g(X_i) - \mu(W_i, 0)),$$ where $O_i = (W_i, X_i, T_i, Y_i)$. Then $\hat{\tau} = \frac{1}{n^{e_2}} \sum_{i \in \mathcal{D}^{e_2}} \phi(O_i; \hat{\mu}, \hat{e}, g)$.



We can rewrite our estimator as:
$$\hat{\tau} - \tau_0 =  \frac{1}{n^{e_2}} \sum_{i \in \mathcal{D}^{e_2}} \left[ \phi(O_i; \mu, e, g) - \tau_0 \right] + \frac{1}{n^{e_2}} \sum_{i \in \mathcal{D}^{e_2}} \underbrace{\left[ \phi(O_i; \hat{\mu}, \hat{e}, g) - \phi(O_i; \mu, e, g) \right]}_{\Delta_i} $$

Assuming that the second moment of the random variable $\phi$ is bounded, by a standard central limit theorem argument, the second term satisfies  $$\sqrt{n^{e_2}} \left( \frac{1}{n^{e_2}} \sum_{i \in \mathcal{D}^{e_2}} \phi(O_i; \mu_0, e_0, g_0) - \tau_0 \right) \xrightarrow{d} \mathcal{N}(0, \mathbb{E}[\phi^2]).$$ It remains to show that the first term multiplied by $\sqrt{n^{e_2}}$ goes to zero in probability, i.e. it is asymptotically negligible.

To do so, observe that we can rewrite the second term as
$$
\frac{1}{n^{e_2}} \sum_{i=1} \Delta_i = (\mathbb P_n - \mathbb P)(\Delta_i) + \mathbb P(\Delta_i),
$$
where $\mathbb P$ and $\mathbb P_n$ are the true and empirical target measures; $\mathbb P(\cdot) = \EE[\cdot]$ as it is standard in empirical process theory.
Our goal is therefore to show that both of these terms 
$$
\underbrace{(\mathbb P_n- \mathbb P)(\Delta_i)}_{T_1} + \underbrace{\mathbb P(\Delta_i)}_{T_2} = o_{\mathbb P}(n^{-1/2}).
$$
\paragraph{Controlling the term $T_1$}The first term $T_1$ is easy to control, as it follows directly from the following lemma.

\begin{lemma} \label{lemma:kennedy} \citep{kennedy2020sharp}
Let $\widehat{f}(z)$ be a function estimated from a sample $Z^N = (Z_{n+1}, \ldots, Z_N)$, and let $\mathbb{P}_n$ denote the empirical measure over $(Z_1, \ldots, Z_n)$, which is independent of $Z^N$. Then
\[
(\mathbb{P}_n - \mathbb{P})(\widehat{f} - f) = O_{\mathbb{P}} \left(\frac{\|\widehat{f} - f\|}{\sqrt{n}}\right).
\]
\end{lemma}
Since we have from assumptions that $||\phi(\cdot;\hat \mu,  \hat e, g) -\phi(\cdot;\mu, e,  g)  ||_2^2=o_{\mathbb P}(1)$, it holds that $T_1 = o_{\mathbb P}(n^{-1/2}).$ 


\paragraph{Controlling the term $T_2$}The second term requires some care. We will focus on the term involving $ T_i = 1 $; the case for $ T_i = 0 $ follows by symmetry.
For \( T_i = 1 \), after some simple calculations, we have:
\begin{align*}
\Delta_i &= \left( \hat{\mu}(W_i, 1) - \mu(W_i, 1) \right) + \frac{1}{\hat{e}(W_i)} \left( g(X_i) - \hat{\mu}(W_i, 1) \right) - \frac{1}{e(W_i)} \left( g(X_i) - \mu(W_i, 1) \right).
\end{align*}
Note that we can drop the third term since, by assumption, $g$ and $\mu$ are equal on average. Therefore, we can write:
\begin{align*}
\mathbb P(\Delta_i) = \EE[\Delta_i] = \EE[\hat{\mu}(W_i, 1) - \mu(W_i, 1) + \frac{1}{\hat{e}(W_i)} \left( g(X_i) - \hat{\mu}(W_i, 1) \right)].
\end{align*}
Under our assumption that $\EE[ g(X) \mid W, T]  = \mu(W,T), \mathbb{P}^{e_2}-\text{almost~surely}$, we can group terms as follows:
\begin{align*}
 \EE\Big [\hat{\mu}(W_i, 1) - \mu(W_i, 1) 
 &+ \frac{1}{\hat{e}(W_i)} \left( g(X_i) - \hat{\mu}(W_i, 1) \right) \Big ] \\
 &= \EE\Big [\hat{\mu}(W_i, 1) - \mu(W_i, 1) + \frac{1}{\hat{e}(W_i)} \left( \mu(W,1) - \hat{\mu}(W_i, 1) \right) \Big ] \\ 
 &=  \EE\Big[(e(W)/\hat e(W) -1) ( \mu(W, 1) - \hat \mu(W,1) )\Big ] \\
 &\leq \frac{1}{\epsilon}|| e - \hat e||_2 || \mu - \hat \mu||_2 = o_{\mathbb P}(n^{-1/2}).
\end{align*}
\end{proof}

\newpage
\section{Algorithm}
\label{sec:algorithm}
We provide here a detailed description of our pipeline for 0-shot Generalization for PPCI, Causal Lifting a foundational model via DERM (Algorithm \ref{alg:clfull}). 
Without loss of generality, we focus on applications with binary treatment estimating the Average Treatment Effect via AIPW estimator (as suggested by Theorem \ref{th:feasibility}), but the same procedure with different Causal Inference estimators can be considered when the treatment is discrete or continuous.


\setcounter{algorithm}{0}
\begin{algorithm*}[h!]
\caption{0-shot Generalization for PPCI (\textit{Causal Lifting})}
\begin{algorithmic}[1] % The [1] allows for line numbering
\label{alg:clfull}
    \STATE \textbf{Input:} 
    \begin{itemize}
    \setlength\itemsep{0em}
        \item a PPCI problem, i.e, $\mathcal{D}^{e_1}=\{(T^{e_1}_i, \bm{W}^{e_1}_i, Y_i, \bm{X}_i)\}_{i=1}^{n^{e_1}}$  and  $\mathcal{D}^{e_2}=\{(T^{e_2}_i, \bm{W}^{e_2}_i, \_, \bm{X}_i)\}_{i=1}^{n^{e_2}}$,
        \item a pre-trained encoder $\phi^*: \mathcal{X} \rightarrow\mathbb{R}^d$, i.e., foundational model, and an hypothesis space $\mathcal{H}$ of candidate classification heads $h:\mathbb{R}^d\rightarrow \mathcal{Y}$, i.e., model architecture,
        \item an optimizer, e.g. Stochastic Gradient Descent,
        \item a (potential) outcome and a propensity score estimator for AIPW, e.g., XGBoost \citep{chen2016xgboost}.
    \end{itemize}
    \medskip
    \STATE \textbf{Output:} Average Treatment Effect Inference on the target experiment, i.e., estimate
    \begin{equation}
        \tau:=\mathbb{E}_{\mathbb{P}^{e_2}}[Y|do(T^{e_2}=1)]-\mathbb{E}_{\mathbb{P}^{e_2}}[Y|do(T^{e_2}=0)]
    \end{equation}
    
    \bigskip
    \STATE \textbf{Procedure:} 
    \STATE \qquad \textbf{Factual Outcome Model}
    Solve
    \begin{equation}
        \hat{h}:=\arg \min_{h \in \mathcal{H}} \frac{1}{n^{e_1}}\sum_{i\in \mathcal{D}^{e_1}} \underbrace{w_i}_{\text{unconfoundness}}\cdot\underbrace{\mathcal{L}^{\text{task}}(h \circ \phi^*(\bm{x}_i), y_i)}_{\text{sufficiency}}
    \end{equation}
    using the given optimizer, where the weights:
    \begin{equation}
            w_i :=\underbrace{\frac{1}{\widehat{\mathbb{P}}^{e_1}(Y=y_i, \bm{Z}^{e_1}=\bm{z}_i)}}_{\text{reference distribution}} \cdot \overbrace{\frac{\widehat{\text{Var}}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z_i})}{\displaystyle\sum_{\bm{z}'\in \mathcal{Z}^{e_1}} \widehat{\text{Var}}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z}')}}^{\text{fictitious distribution s.t. $\bm{Z} \indep Y$}}
        \end{equation}
    are computed una tantum before training. The (conditional) variance and joint distribution in the weights are estimated via sample variance and frequency, respectively, over the reference experiment and the experiment settings are discretized if continuous.

    \STATE \qquad \textbf{Causal Inference} Via AIPW estimator on the prediction-powered target sample $\mathcal{D}^{e_2}$ by $\hat{g}:=\hat{h}\circ \phi^*$:  
    \begin{equation}
    \label{eq:alg1aipw}
    \begin{split}
       \widehat{\tau} := \frac{1}{n^{e_2}} \sum_{i \in \mathcal{D}^{e_2}} \Bigg[ \widehat{\mu}(\bm{W}_i^{e_2},1) - \widehat{\mu}(\bm{W}_i^{e_2},0) 
        &+ \frac{T_i^{e_2}}{\widehat{e}(\bm{W}_i^{e_2})}  ( \hat{g}(\bm{X}_i) - %\\
        \widehat{\mu}(\bm{W}_i^{e_2},1) ) 
        \\ & - \frac{1-T_i^{e_2}}{1-\widehat{e}(\bm{W}_i^{e_2})} \left( \hat{g}(\bm{X}_i) - \widehat{\mu}(\bm{W}_i^{e_2},0) \right) \Bigg] 
    \end{split}
    \end{equation}

    where the propensity score model $\hat{e}: \mathcal{W}\rightarrow \mathcal{T}$ and the potential outcomes model $\hat{\mu}: \mathcal{W}\times \mathcal{T}\rightarrow \mathcal{Y}$ are also trained on the prediction powered target experiment. 
    Confidence Intervals, leveraging the asymptotic normality from Thm. \ref{th:feasibility}:
    \begin{equation}
        C_{\alpha}(\tau) = \left[\hat{\tau} \pm z_{1-\frac{\alpha}{2}} \sqrt{\frac{\widehat{\text{Var}}(\hat{\tau}_i)}{n^{e_2}}}\right]
    \end{equation}
    where $\hat{\tau}_i$ are the addends in Equation \ref{eq:alg1aipw}.
\end{algorithmic}
\end{algorithm*}

\newpage
We additionally describe the pipeline if proposing to train such a model from scratch (Algorithm \ref{alg:ppcifull}).

\begin{algorithm*}[h!]
\caption{0-shot Generalization for PPCI (\textit{from scratch})}
\begin{algorithmic}[1] % The [1] allows for line numbering
\label{alg:ppcifull}
    \STATE \textbf{Input:} 
    \begin{itemize}
    \setlength\itemsep{0em}
        \item a PPCI problem, i.e, $\mathcal{D}^{e_1}=\{(T^{e_1}_i, \bm{W}^{e_1}_i, Y_i, \bm{X}_i)\}_{i=1}^{n^{e_1}}$  and  $\mathcal{D}^{e_2}=\{(T^{e_2}_i, \bm{W}^{e_2}_i, \_, \bm{X}_i)\}_{i=1}^{n^{e_2}}$,
        \item an hypothesis space $\mathcal{G}$ of candidate classification model $g:\mathcal{X}\rightarrow \mathcal{Y}$, i.e., model architecture,
        \item an optimizer, e.g. ADAM \citep{kingma2014adam},
        \item a (potential) outcome and a propensity score estimator for AIPW, e.g., XGBoost \citep{chen2016xgboost}.
    \end{itemize}
    \medskip
    \STATE \textbf{Output:} Average Treatment Effect Inference on the target experiment, i.e., estimate
    \begin{equation}
        \tau:=\mathbb{E}_{\mathbb{P}^{e_2}}[Y|do(T^{e_2}=1)]-\mathbb{E}_{\mathbb{P}^{e_2}}[Y|do(T^{e_2}=0)]
    \end{equation}
    
    \bigskip
    \STATE \textbf{Procedure:} 
    \STATE \qquad \textbf{Factual Outcome Model}
    Solve
    \begin{equation}
        \hat{g}:=\arg \min_{g \in \mathcal{G}} \frac{1}{n^{e_1}}\sum_{i\in \mathcal{D}^{e_1}} \underbrace{w_i}_{\text{unconfoundness}}\cdot\underbrace{\mathcal{L}^{\text{task}}(g(\bm{x}_i), y_i)}_{\text{sufficiency}}
    \end{equation}
    using the given optimizer, where the weights:
     \begin{equation}
            w_i :=\underbrace{\frac{1}{\widehat{\mathbb{P}}^{e_1}(Y=y_i, \bm{Z}^{e_1}=\bm{z}_i)}}_{\text{reference distribution}} \cdot \overbrace{\frac{\widehat{\text{Var}}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z_i})}{\displaystyle\sum_{\bm{z}'\in \mathcal{Z}^{e_1}} \widehat{\text{Var}}_{\mathbb{P}^{e_1}}(Y|\bm{Z}^{e_1}=\bm{z}')}}^{\text{fictitious distribution s.t. $\bm{Z} \indep Y$}}
        \end{equation}
    are computed una tantum before training. The (conditional) variance and joint distribution in the weights are estimated via sample variance and frequency, respectively, over the reference experiment and the experiment settings are discretized if continuous.

    \STATE \qquad \textbf{Causal Inference} Via AIPW estimator on the prediction-powered target sample $\mathcal{D}^{e_2}$ by $\hat{g}$:  
    \begin{equation}
    \label{eq:alg2aipw}
    \begin{split}
       \widehat{\tau} := \frac{1}{n^{e_2}} \sum_{i \in \mathcal{D}^{e_2}} \Bigg[ \widehat{\mu}(\bm{W}_i^{e_2},1) - \widehat{\mu}(\bm{W}_i^{e_2},0) 
        &+ \frac{T_i^{e_2}}{\widehat{e}(\bm{W}_i^{e_2})}  ( \hat{g}(\bm{X}_i) - %\\
        \widehat{\mu}(\bm{W}_i^{e_2},1) ) 
        \\ & - \frac{1-T_i^{e_2}}{1-\widehat{e}(\bm{W}_i^{e_2})} \left( \hat{g}(\bm{X}_i) - \widehat{\mu}(\bm{W}_i^{e_2},0) \right) \Bigg] 
    \end{split}
    \end{equation}

    where the propensity score model $\hat{e}: \mathcal{W}\rightarrow \mathcal{T}$ and the potential outcomes model $\hat{\mu}: \mathcal{W}\times \mathcal{T}\rightarrow \mathcal{Y}$ are also trained on the prediction powered target experiment. 
    Confidence Intervals, leveraging the asymptotic normality from Thm. \ref{th:feasibility}:
    \begin{equation}
        C_{\alpha}(\tau) = \left[\hat{\tau} \pm z_{1-\frac{\alpha}{2}} \sqrt{\frac{\widehat{\text{Var}}(\hat{\tau}_i)}{n^{e_2}}}\right]
    \end{equation}
    where $\hat{\tau}_i$ are the addends in Equation \ref{eq:alg2aipw}.
\end{algorithmic}
\end{algorithm*}

\newpage
\section{ISTAnt}
\label{sec:istantextra}
In this Section, we discuss our procedure to generate a dataset similar to the ISTAnt experiment and its analysis, i.e., model details, hyper-parameters, fine-tuning, etc.
\subsection{Reference Experiment and Data Recording}
\label{ssec:replica}
We run an experiment very much alike the ISTAnt experiment with triplets of worker ants following the step-by-step design described in Appendix C in \citet{cadei2024smoke}. We recorded 5 batches of 9 simultaneously run replicates, producing 45 original videos, of which one had to be excluded for experimental problems, leaving 44 analyzable videos. 
We used a comparable experimental setup (i.e., camera set-up, random treatment assignment, etc.) except for the following, obtaining a \textit{similar} experiment belonging to the same class of SCMs (see Problem Formulation in Section \ref{sec:problem}). 

\begin{itemize}
    \item \textbf{Treatments:} Whereas ISTAnt used two micro-particle applications \footnote{By author correspondence.}, our experimental treatments also constitute micro-particle application in two different treatments (n=15 each), but also one treatment completely free of micro-particles (control, n=14), all applied to the focal ant. The three treatments of the ants are visually indistinguishable, independent of micro-particle application.
    \item \textbf{Light conditions:} We created a lower-quality illumination of the nests by implementing a ring of light around the experiment container, resulting in more inhomogeneous lighting and a high-lux (“cold”) light effect, compared to the light diffusion by a milky plexiglass sheet proposed in the original experiment. Also, our ant nests had a higher rim from the focal plane where the ants were placed, causing some obscuring of ant observation along the walls. See a comparison of the filming set-up and an example of the resulting recording in Figure \ref{fig:examples}. We also considered a slightly lower resolution, i.e., 700x700 pixels.
    \item \textbf{Longer Videos}: Whereas ISTAnt annotated 10 min long videos, we here annotated 30 min long videos. Ant activity generally decreases with time from the first exposure to a new environment. Our videos were recorded at 30fps, totaling 158400 annotated frames in the 44 videos.
    \item \textbf{Other potential distribution shifts}: Other sources of variations from the original experiment are:
    \begin{itemize}
        \item Whereas ISTAnt used orange and blue color dots, we used yellow and blue.
        \item Whereas in ISTAnt, grooming presence or absence was annotated for each frame, we here annotated a single grooming event even if the ant stopped grooming for up to one second but then kept grooming after that, with no other behaviors being performed in between. This means that intermediate frames between grooming frames were also annotated as grooming despite the ant pausing its behavior. Such less exact grooming annotations are faster to perform for the human annotator.
        \item The person performing annotation in this experiment was different from the annotators in the ISTAnt dataset, leading to some possible observer effects. 
    \end{itemize}
\end{itemize}

% \begin{figure}[h]{}  % Adjust width as needed
%         \hfill
%         \begin{subfigure}[b]{0.48\textwidth}
%             \centering
%             \includegraphics[height=6cm,] %trim={0mm 22mm 0mm 6mm}, clip
%             {figures/istantfs.jpg}  
%             \caption{Homogeneous lighting (original ISTAnt)}
%             \label{fig:istantfs}
%         \end{subfigure}
%         \hfill
%         \begin{subfigure}[b]{0.48\textwidth}
%             \centering
%             \includegraphics[height=6cm,]% trim={0mm 22mm 0mm 13mm}, clip]
%             {figures/replicafs.jpg} 
%             \caption{Inhomogeneous lighting (our experiment)}
%             \label{fig:replicafs}
%         \end{subfigure}
%         \hfill
%     \caption{Comparison of the filming box set-up between the ISTAnt and the current experiment.}
%     \label{fig:experiment_setup}
% \end{figure}

Despite all the experiment variants, we still rely on the assumption that an invariant factual outcome model for behavior classification $g^*$ exists, as well as we are making when considering ground truth human experts manually annotating frame by frame.
Let's observe that our work models the general pipeline in experimental ecologists, where multiple experiment variants are recorded over time, e.g., upgrading the data acquisition technique, and we aim to generalize from a lower to higher quality \textit{similar} experiment. 

\subsection{Analysis}
\label{ssec:ISTAexp}
We considered our dataset as a Reference Experiment in a PPCI problem trying to generalize a factual outcome model to the original ISTAnt dataset, i.e., Target Experiment, causal lifting of a foundational model.
For each pre-trained encoder -- ViT-B \citep{dosovitskiy2020image}, ViT-L \citep{zhai2023sigmoid}, CLIP-ViT-B,-L \citep{radford2021learning}, DINOv2 \citep{oquab2023dinov2}, we fine-tuned a multi-layer perception head (2 hidden layers with 256 nodes each and ReLU activation) on top of its \textit{class} token via Adam optimizer ($\beta_1=0.9, \beta_2=0.9,  \epsilon=10^{-8}$) for ERM, vREx (finetuning the invariance constraint in $\{0.01,0.1,1,10\}$ and DERM (ours) for 15 epochs and batch size 256. So, we fine-tuned the learning rates in $[0.0005, 0.5]$, selecting the best-performing hyper-parameters for each model-method, minimizing the Treatment Effect Bias on the reference sample. 
We computed the ATE at the video level (aggregating the predictions per frame) via the AIPW estimator. We used XGBoost for the model outcome and estimated the propensity score via sample mean (constant) since the treatment assignments are randomized, i.e., RCT. For the outcome model, we consider the following experiment settings for controlling: experiment day, time of the day, batch, position in the batch, and annotator.



\section{CausalMNIST}
\label{sec:CausalMNIST}
To fully validate our method, we replicated the comparison in a controlled setting, manipulating the MNIST dataset with coloring, allowing us to (i) cheaply replicate fictitious experiments several times and bootstrapping confidence intervals and (ii) control the underlying causal effects.

\subsection{Data Generating Process}
We define the (universal) Structural Equations as follows:
\begin{align}
    \bm{Z} &:= [T^1,T^2, T, W, U]=n_{\bm{Z}} \\
    Y &=  W \cdot \text{Unif}(\{0,1,2,3\}) + T^1 \cdot \text{Unif}(\{0,1,2,3\}) + U \cdot \text{Unif}(\{0,1,2,3\})\\
    X &:= f_{\bm{X}}(T,W,Y, U,n_{\bm{X}})
\end{align}
where $f_X$ is a deterministic manipulation of a random digit image $n_{\bm{X}}$ from MNIST dataset enforcing the background color $W$ (red or green) and pen color $T$ (black or white) and a padding size $U$. 

For the reference experiment (RCT) we intervene on the experimental settings such that:
\begin{align}
    T^1 &:= Be(0.5)\\
    T^2 &:= 0\\
    T &:= \max(T^1, T^2) \\
    W &:= Be(p_W) \\
    U &:= Be(p_U)
\end{align}
with $\bm{Z} := [\underbrace{T^2, T}_{U^{e_1}}, \underbrace{T^1, \underbrace{W, U}_{W^{e_1}}}_{Z^{e_1}}]$.
While for the target experiment (OS):
\begin{align}
    T^1 &:= 0\\
    T^2 &:= Be(0.1)\cdot (1-W) + Be(0.9)\cdot W\\
    T &:= \max(T^1, T^2) \\
    W &:= Be(p_W) \\
    U &:= Be(p_U)
\end{align}
with $\bm{Z} := [\underbrace{T^1, T}_{U^{e_2}}, \underbrace{T^2, \underbrace{W, U}_{W^{e_2}}}_{Z^{e_2}}]$.

By definition, whatever the experiment setting distribution, the ATE on the reference experiment is 1.5, while on target, it is 0. 
We especially considered a reference sample $\mathcal{D}^{e_1}$ with $n_{e_1}=10\ 000$, $p_U=0.02$ and $p_W=0.5$; a target sample $\mathcal{D}^{e_2}$ with $n_{e_2}=10\ 000$, $p_U=0.05$ and $p_W=0.05$; and additional target sample $\mathcal{D}^{e_3}$ with $n_{e_3}=10\ 000$ and $p_U=0.5$ and $p_W=0.5$. Six examples of colored handwritten digits from CausalMNIST are reported in Figure \ref{fig:causalmnist}.

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.8\linewidth]{figures/causalmnist.png} 
    \caption{Random samples from a CausalMNIST sample.}
    \label{fig:causalmnist}
\end{figure}

\subsection{Analysis}
We fully replicated the modeling choices for CausalMNIST proposed in \citet{cadei2024smoke} and described in Appendix E.2 (without relying on pre-trained models). We replicated the same hyper-parameter tuning (for training) and ATE inference from our experiments on ISTAnt generalization. We replicated each experiment 50 times, including resampling the data, and bootstrapped the confidence interval of the ATE estimates via AIPW.